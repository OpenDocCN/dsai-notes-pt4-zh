- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应性集成微调变换器用于LLM生成文本检测
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.13335](https://ar5iv.labs.arxiv.org/html/2403.13335)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.13335](https://ar5iv.labs.arxiv.org/html/2403.13335)
- en: Zhixin Lai Electrical and Computer Engineering
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 黎智新 电气与计算机工程
- en: Cornell University Ithaca, NY, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学，伊萨卡，纽约，美国
- en: zl768@cornell.edu    Xuesheng Zhang Recommendation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zl768@cornell.edu    张学胜 推荐
- en: Meituan Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 美团，北京，中国
- en: xueshengz503@gmail.com    Suiyao Chen Industrial and Management Systems Engineering
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: xueshengz503@gmail.com    陈随尧 工业与管理系统工程
- en: University of South Florida Tampa, FL, USA
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 南佛罗里达大学，坦帕，佛罗里达，美国
- en: suiyaochen@usf.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: suiyaochen@usf.edu
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have reached human-like proficiency in generating
    diverse textual content, underscoring the necessity for effective fake text detection
    to avoid potential risks such as fake news in social media. Previous research
    has mostly tested single models on in-distribution datasets, limiting our understanding
    of how these models perform on different types of data for LLM-generated text
    detection task. We researched this by testing five specialized transformer-based
    models on both in-distribution and out-of-distribution datasets to better assess
    their performance and generalizability. Our results revealed that single transformer-based
    classifiers achieved decent performance on in-distribution dataset but limited
    generalization ability on out-of-distribution dataset. To improve it, we combined
    the individual classifiers models using adaptive ensemble algorithms, which improved
    the average accuracy significantly from 91.8% to 99.2% on an in-distribution test
    set and from 62.9% to 72.5% on an out-of-distribution test set. The results indicate
    the effectiveness, good generalization ability, and great potential of adaptive
    ensemble algorithms in LLM-generated text detection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在生成多样化文本内容方面达到了类似人类的水平，这突显了有效假文本检测的必要性，以避免社交媒体中的假新闻等潜在风险。以往研究大多在分布内数据集上测试单一模型，这限制了我们对这些模型在LLM生成文本检测任务中对不同类型数据的表现的理解。我们通过在分布内和分布外数据集上测试五种专门的基于变换器的模型来研究这一问题，以更好地评估它们的性能和泛化能力。我们的结果揭示，单一的基于变换器的分类器在分布内数据集上表现良好，但在分布外数据集上的泛化能力有限。为了改善这一点，我们使用适应性集成算法结合了个体分类器模型，显著提高了分布内测试集的平均准确率，从91.8%提高到99.2%，以及分布外测试集从62.9%提高到72.5%。结果表明，适应性集成算法在LLM生成文本检测中的有效性、良好的泛化能力以及巨大潜力。
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: LLM-generated text detection, Adaptive assemble algorithm, Transformer-based
    classifier, Generalization ability, in-distribution dataset, out-of-distribution
    dataset
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成文本检测，适应性集成算法，基于变换器的分类器，泛化能力，分布内数据集，分布外数据集
- en: I Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一、引言
- en: Recently, LLM has experienced rapid development, and its text generation ability
    is comparable to that of human writing [[1](#bib.bib1), [2](#bib.bib2)]. LLM has
    penetrated into various aspects of daily life and plays a crucial role in many
    professional workflows such as forecasting and anomaly detection [[3](#bib.bib3)],
    text production [[4](#bib.bib4)] and across various domains [[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)], promoting tasks such as advertising slogan creation,
    news writing [[9](#bib.bib9)], story generation, and code generation. In addition,
    their impact has significantly influenced the development of many sectors and
    disciplines, including education [[10](#bib.bib10)], law [[11](#bib.bib11)], biology
    [[12](#bib.bib12)], and medicine [[13](#bib.bib13)]. However, the use of GPTs
    also brings various risks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LLM经历了快速发展，其文本生成能力已与人类写作相媲美[[1](#bib.bib1), [2](#bib.bib2)]。LLM已渗透到日常生活的各个方面，并在许多专业工作流程中发挥了重要作用，如预测和异常检测[[3](#bib.bib3)]、文本生成[[4](#bib.bib4)]以及多个领域[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]，推动了广告标语创作、新闻写作[[9](#bib.bib9)]、故事生成和代码生成等任务。此外，它们的影响已显著影响到许多行业和学科，包括教育[[10](#bib.bib10)]、法律[[11](#bib.bib11)]、生物学[[12](#bib.bib12)]和医学[[13](#bib.bib13)]。然而，GPT的使用也带来了各种风险。
- en: Firstly, GPT and other high-level language generation models can generate realistic
    text, which may be used to create and disseminate misleading information [[14](#bib.bib14)],
    fake news, or harmful content. This may not only affect the public’s understanding
    of the facts, but also manipulate key areas such as political elections, the stock
    market, and public health. Secondly, LLMs can generate text with a similar style
    to existing content, which may lead to copyright infringement and intellectual
    property disputes [[15](#bib.bib15)]. For example, a model may replicate the style
    of a specific author in creating literary works, which may infringe upon the copyright
    of the original author. In addition, the content generated by LLMs may be used
    to produce pirated books, articles, or other media content, thereby damaging the
    economic interests and intellectual property rights of original content creators.
    Thirdly, LLM-generated content on social media platforms can be exploited to create
    false identities, enabling the manipulation of online conversations and public
    opinion. Automated accounts (bots) can leverage GPT-like models to generate a
    large volume of realistic comments, posts, or messages with deceptive, harassing,
    or persuasive intentions [[16](#bib.bib16), [17](#bib.bib17)]. In addition, the
    widespread use of models such as GPTs may lead to dishonest behavior in academic
    and educational fields. Students may use these tools to automatically generate
    papers and assignments, thereby undermining academic integrity [[18](#bib.bib18),
    [19](#bib.bib19)]. Lastly, many traditional industries are still cautiously adopting
    the power of LLM for text generation due to critical risk concerns such as cybersecurity
    [[20](#bib.bib20), [21](#bib.bib21)], healthcare [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)], transportation
    [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)] and sophistication and
    reliability requirements for manufacturing [[31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)], environment [[35](#bib.bib35)], agriculture
    [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)] and energy [[39](#bib.bib39)],
    etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，GPT 和其他高级语言生成模型可以生成逼真的文本，这些文本可能被用来创建和传播误导性信息[[14](#bib.bib14)]、虚假新闻或有害内容。这不仅可能影响公众对事实的理解，还可能操控政治选举、股市和公共卫生等关键领域。其次，LLMs
    可以生成与现有内容风格相似的文本，这可能导致版权侵犯和知识产权纠纷[[15](#bib.bib15)]。例如，一个模型可能在创作文学作品时复制特定作者的风格，这可能会侵犯原作者的版权。此外，LLMs
    生成的内容可能被用于制作盗版书籍、文章或其他媒体内容，从而损害原内容创作者的经济利益和知识产权。第三，LLM 生成的社交媒体内容可能被利用来创建虚假身份，从而操控在线对话和公众舆论。自动化账户（机器人）可以利用类似
    GPT 的模型生成大量逼真的评论、帖子或消息，具有欺骗、骚扰或劝说的意图[[16](#bib.bib16), [17](#bib.bib17)]。此外，GPT
    等模型的广泛使用可能导致学术和教育领域的不诚实行为。学生可能使用这些工具自动生成论文和作业，从而破坏学术诚信[[18](#bib.bib18), [19](#bib.bib19)]。最后，由于网络安全[[20](#bib.bib20),
    [21](#bib.bib21)]、医疗[[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)]、交通[[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]以及制造业对复杂性和可靠性的要求[[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]、环境[[35](#bib.bib35)]、农业[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)]和能源[[39](#bib.bib39)]等方面的关键风险担忧，许多传统行业仍在谨慎采用
    LLM 的文本生成能力。
- en: However, telling machine-generated text from manually written text has proven
    to be a challenging task, with human performance only slightly surpassing chance
    levels [[40](#bib.bib40)]. Consequently, the development of efficient automated
    methods for identifying machine-generated text and mitigating its potential misuse
    is important. Even prior to the widespread adoption of ChatGPT, research on the
    detection of machine-generated text had garnered attention, particularly in the
    recognition of deeply forged texts [[41](#bib.bib41)]. Nowadays, with ChatGPT’s
    increasing prevalence, the primary focus of machine-generated text detection has
    shifted towards distinguishing between text generated by LLMs and text composed
    by humans [[42](#bib.bib42)]. For instance, Guo et al. have undertaken the task
    of detecting whether a given text, in both English and Chinese, originates from
    ChatGPT or has been written by a human across various domains [[43](#bib.bib43)].
    While the use of pre-trained language models (LMs) as classifiers has been established
    as an effective approach for detecting text generated by Large Language Models
    (LLMs), prior research, often confined to single-model evaluations on familiar
    datasets, provides limited insights into their broader applicability and generalization
    capabilities. Individual classifiers may exhibit instability and struggle to generalize
    when applied to unfamiliar data contexts [[44](#bib.bib44)]. In contrast, ensemble
    learning methods, such as the random forest algorithm [[45](#bib.bib45)], excel
    in such situations by combining multiple models to create a more accurate and
    robust predictor. Recognizing this, we integrated ensemble learning algorithms
    to amalgamate our trained transformer-based classifiers, aiming to enhance their
    performance and generalization capacity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，区分机器生成的文本和人工编写的文本被证明是一项具有挑战性的任务，人类的表现仅略微超出偶然水平[[40](#bib.bib40)]。因此，开发高效的自动化方法来识别机器生成的文本并减轻其潜在的误用至关重要。即使在ChatGPT广泛应用之前，关于机器生成文本的检测研究已受到关注，特别是在识别深度伪造文本方面[[41](#bib.bib41)]。如今，随着ChatGPT的日益普及，机器生成文本检测的主要关注点已转向区分LLM生成的文本和人工编写的文本[[42](#bib.bib42)]。例如，Guo等人承担了检测给定文本是否来源于ChatGPT或由人类编写的任务，涵盖了多个领域的英语和中文文本[[43](#bib.bib43)]。虽然使用预训练语言模型（LMs）作为分类器已被确立为检测大语言模型（LLMs）生成文本的有效方法，但之前的研究通常局限于在熟悉数据集上的单模型评估，提供了有限的广泛适用性和泛化能力的见解。个别分类器可能在应用于不熟悉的数据上下文时表现不稳定，难以泛化[[44](#bib.bib44)]。相比之下，集成学习方法，如随机森林算法[[45](#bib.bib45)]，通过结合多个模型创建更准确且更强大的预测器，在这种情况下表现优越。认识到这一点，我们集成了集成学习算法，将我们训练的基于变换器的分类器融合在一起，旨在提升它们的性能和泛化能力。
- en: 'The principal contributions of this work are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献如下：
- en: $\bullet$
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We achieved the LLMs generated text detection task by training five distinct
    transformer-based classifiers, each pre-trained on different datasets. .
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过训练五个不同的基于变换器的分类器来完成LLMs生成文本检测任务，每个分类器都在不同的数据集上进行了预训练。
- en: $\bullet$
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We noticed variations in the accuracy of transformer-based classifiers and identified
    a notable constraint in their generalizability by evaluating the classifiers on
    different datasets.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们注意到基于变换器的分类器在准确性上的差异，并通过在不同数据集上评估分类器，识别出了它们在泛化能力方面的一个显著限制。
- en: $\bullet$
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We employed both non-adaptive and adaptive ensemble techniques to improve the
    accuracy of LLM generated text detection. The adaptive ensemble method demonstrated
    the highest accuracy when tested on both in-distribution and out-of-distribution
    datasets, underscoring its superior effectiveness in identifying LLM-generated
    text.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们采用了非自适应和自适应集成技术来提高LLM生成文本检测的准确性。自适应集成方法在测试中展示了最高的准确率，无论是在分布内还是分布外的数据集上，这突显了它在识别LLM生成文本方面的优越效果。
- en: II Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: The LLM-generated text detection task is framed as a binary classification problem.
    Transformer-based classifiers, which has been extensively explored in various
    domains [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50)], serve as one of the most popular and effective approaches for
    this classification task. Additionally, we recognized that ensemble learning algorithms
    offer an efficient means of enhancing classification performance. In this paper,
    we combined individual classifiers with ensemble algorithms, resulting in a significant
    improvement in the performance of LLM-generated text detection on both in-distribution
    and out-of-distribution datasets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成文本检测任务被框定为二分类问题。基于变换器的分类器在各种领域中得到了广泛探索[[46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]，是这个分类任务中最流行和有效的方法之一。此外，我们认识到集成学习算法提供了一种有效的方式来提升分类性能。在本文中，我们将个体分类器与集成算法相结合，在内部分布和外部分布数据集上的LLM生成文本检测性能得到了显著提升。
- en: II-A Algorithms for machine-generated text detection
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 机器生成文本检测算法
- en: 'Guo et al, applied two methods: logistic regression with GLTR features and
    an end-to-end RoBerta classifier to distinguish whether a text was generated by
    ChatGPT or humans across multiple fields [[43](#bib.bib43)]. Shijaku and Canhasi
    detected TOEFL essays using XGBoost with manually extracted 244 lexical and semantic
    features [[51](#bib.bib51)]. There are also widely-used off-the-shelf GPT detectors,
    such as the OpenAI detection classifier, GPTZero and ZeroGPT [[52](#bib.bib52)].
    OpenAI’s AI text classifier is fine-tuned on the output of an already trained
    language model. They used text generated by 34 models pre-trained by five different
    organizations, and then trained their models on samples from multiple sources
    of human writing and language model-generated text. GPTZero is trained on an extensive
    and diverse corpus of text created by humans and artificial intelligence, with
    a primary focus on English. As a classification model, GPTzero predicts whether
    a given text fragment is generated by a LLM with different text granularities,
    including sentence, paragraph, and entire document levels. These classifiers are
    all based on the transformer structure [[53](#bib.bib53)], providing us with some
    reference for the algorithm of selecting classifiers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Guo等人应用了两种方法：带有GLTR特征的逻辑回归和端到端的RoBerta分类器，用于区分文本是由ChatGPT还是人类生成的，覆盖多个领域[[43](#bib.bib43)]。Shijaku和Canhasi使用XGBoost检测TOEFL作文，手动提取了244个词汇和语义特征[[51](#bib.bib51)]。还有广泛使用的现成GPT检测器，如OpenAI检测分类器、GPTZero和ZeroGPT[[52](#bib.bib52)]。OpenAI的AI文本分类器在一个已经训练的语言模型输出上进行微调。他们使用了34个由五个不同组织预训练的模型生成的文本，然后在来自多个来源的人类写作和语言模型生成文本的样本上训练了他们的模型。GPTZero在一个广泛而多样化的人类和人工智能创建的文本语料库上进行训练，主要集中在英语上。作为一个分类模型，GPTZero预测给定文本片段是否由LLM生成，涵盖不同的文本粒度，包括句子、段落和整篇文档。这些分类器都基于变换器结构[[53](#bib.bib53)]，为我们选择分类器的算法提供了一些参考。
- en: II-B Ensemble Learning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 集成学习
- en: Ensemble learning refers to the machine learning paradigm where multiple learners
    (also known as models or predictors) are trained to solve the same problem. The
    main advantage of ensemble learning lies in its ability to improve model generalization
    ability. By combining multiple models, it can effectively reduce the risk of overfitting
    and underfitting. In addition, the diversity of different models can improve the
    overall prediction accuracy. Ensemble learning typically performs well in various
    machine learning competitions and practical applications [[54](#bib.bib54), [55](#bib.bib55)].
    We call algorithms without parameter updates in ensemble learning as ”non-adaptive
    ensemble algorithm”, like the hard voting ensemble [[56](#bib.bib56)]. And we
    call algorithms with parameter updates in ensemble learning as ”adaptive ensemble
    algorithm”, like the neural network ensemble and random forest algorithm. Usually,
    adaptive classifier detection performs better by adaptively integrating the outputs
    of different classifiers, assigning dynamic weights to each classifier’s performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习指的是一种机器学习范式，其中多个学习者（也称为模型或预测器）被训练以解决同一问题。集成学习的主要优点在于其能够提高模型的泛化能力。通过组合多个模型，它可以有效减少过拟合和欠拟合的风险。此外，不同模型的多样性可以提高整体预测准确性。集成学习通常在各种机器学习竞赛和实际应用中表现良好[[54](#bib.bib54),
    [55](#bib.bib55)]。我们称集成学习中没有参数更新的算法为“非自适应集成算法”，例如硬投票集成[[56](#bib.bib56)]。我们称集成学习中有参数更新的算法为“自适应集成算法”，例如神经网络集成和随机森林算法。通常，自适应分类器检测通过自适应地整合不同分类器的输出，为每个分类器的性能分配动态权重，从而表现更好。
- en: II-C Ensemble Learning for LLM-generated Text Detection Task
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C LLM生成文本检测任务中的集成学习
- en: Specifically for LLM-generated text detection, since this is a unified data
    framework, ensemble learning will be a good fit to combine multiple models, instead
    of using fusion mechanisms [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]
    or model selection techniques [[60](#bib.bib60)]. LLM-Blender [[61](#bib.bib61)]
    is an ensemble framework designed to attain consistently superior performance
    by leveraging the diverse strengths of multiple open-source large language models
    (LLMs). The study [[62](#bib.bib62)] presents ensemble neural models utilizing
    probabilities from multiple pre-trained Large Language Models (LLMs) as features
    for Traditional Machine Learning (TML) classifiers to distinguish between AI-generated
    and human-written text, achieving competitive performance in both English and
    Spanish languages and ranking first in model attribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于LLM生成文本检测，由于这是一个统一的数据框架，集成学习将非常适合结合多个模型，而不是使用融合机制[[57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59)]或模型选择技术[[60](#bib.bib60)]。LLM-Blender [[61](#bib.bib61)] 是一个集成框架，旨在通过利用多个开源大语言模型（LLMs）的多样化优势，获得一致优越的性能。研究[[62](#bib.bib62)]提出了利用来自多个预训练的大语言模型（LLMs）的概率作为特征的集成神经模型，以便传统机器学习（TML）分类器区分AI生成和人工编写的文本，在英语和西班牙语中都取得了具有竞争力的性能，并在模型归因中排名第一。
- en: III Dataset
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集
- en: We utilized the DAIGT dataset for training and in-distribution testing, consisting
    of a 2:1 ratio of human-generated to LLM-generated text. The LLM-generated text
    includes outputs from various LLMs, such as ChatGPT and Llama-70b [[63](#bib.bib63)].
    We divided the dataset into training and testing sets in an 80%/20% ratio, as
    shown in Table [I](#S3.T1 "TABLE I ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned
    Transformers for LLM-Generated Text Detection").
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了DAIGT数据集进行训练和分布内测试，该数据集由人类生成文本与LLM生成文本以2:1的比例组成。LLM生成文本包括来自各种LLM的输出，如ChatGPT和Llama-70b
    [[63](#bib.bib63)]。我们将数据集按80%/20%的比例分为训练集和测试集，如表[I](#S3.T1 "TABLE I ‣ III Dataset
    ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection")所示。
- en: 'TABLE I: The item number of training and testing datasets'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 训练和测试数据集的项目数量'
- en: '| Dataset | Functionality | Human-generated | LLM-generated | Total |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 功能性 | 人工生成 | LLM生成 | 总计 |'
- en: '| DAIGT | Training | 23833 | 11531 | 35364 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| DAIGT | 训练 | 23833 | 11531 | 35364 |'
- en: '| DAIGT | In-dist.^a testing | 5959 | 2883 | 8842 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| DAIGT | 分布内^a 测试 | 5959 | 2883 | 8842 |'
- en: '| Deepfake | Out-of-dist.^a testing | 800 | 762 | 1562 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Deepfake | 分布外^a 测试 | 800 | 762 | 1562 |'
- en: '| ^adist.: distribution |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ^adist.: 分布 |'
- en: 'TABLE II: Test average word length of training and testing Datasets'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 训练和测试数据集的平均单词长度'
- en: '|      Dataset |      Human-written |      Machine-generated |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|      数据集 |      人工编写 |      机器生成 |'
- en: '| --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|      DAIGT |      466.82 |      369.09 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|      DAIGT |      466.82 |      369.09 |'
- en: '|      Deepfake |      279.43 |      284.33 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|      Deepfake |      279.43 |      284.33 |'
- en: 'To assess the generalizability of our text detection methods, we introduced
    the Deepfake dataset as out-of-distribution test set. The Deepfake, generated
    by a range of LLMs, encompasses broader domains including open statements, news
    articles, and scientific texts [[44](#bib.bib44)]. We compared the two datasets
    from the following aspects:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们文本检测方法的泛化能力，我们引入了 Deepfake 数据集作为分布外测试集。Deepfake 由多种 LLM 生成，涵盖了更广泛的领域，包括公开声明、新闻文章和科学文本
    [[44](#bib.bib44)]。我们从以下几个方面比较了这两个数据集：
- en: $\bullet$
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Firstly, the DAIGT dataset has longer text lengths than the Deepfake dataset
    from Table [II](#S3.T2 "TABLE II ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned
    Transformers for LLM-Generated Text Detection").
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，从表[II](#S3.T2 "表 II ‣ III 数据集 ‣ 自适应集成微调的变换器用于 LLM 生成文本检测")可以看出，DAIGT 数据集的文本长度比
    Deepfake 数据集长。
- en: $\bullet$
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Secondly, we used the Latent Dirichlet Allocation (LDA) to analyze the dataset
    topic distribution, shown in Fig. [1](#S3.F1 "Figure 1 ‣ III Dataset ‣ Adaptive
    Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection"). The topic
    distribution indicates the dissimilarity in topic type and distribution between
    DAIGT and Deepfake datasets, indicating their divergence in content.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，我们使用潜在狄利克雷分配（LDA）分析了数据集主题分布，如图[1](#S3.F1 "图 1 ‣ III 数据集 ‣ 自适应集成微调的变换器用于 LLM
    生成文本检测")所示。主题分布显示 DAIGT 和 Deepfake 数据集在主题类型和分布上的不一致，表明它们在内容上的差异。
- en: $\bullet$
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Thirdly, from Fig. [2](#S3.F2 "Figure 2 ‣ III Dataset ‣ Adaptive Ensembles of
    Fine-Tuned Transformers for LLM-Generated Text Detection"), part-of-speech distribution
    in human-written and machine-generated text within the same dataset closely aligns,
    suggesting similar linguistic structures. However, when we compare DAIGT with
    Deepfake, a more notable disparity emerges in their part-of-speech distribution.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，从图[2](#S3.F2 "图 2 ‣ III 数据集 ‣ 自适应集成微调的变换器用于 LLM 生成文本检测")可以看出，在相同数据集中的人工撰写文本和机器生成文本的词性分布紧密一致，表明具有相似的语言结构。然而，当我们将
    DAIGT 与 Deepfake 进行比较时，其词性分布差异更加显著。
- en: The analyses from three different aspects of text length, topic, and part-of-speech
    distribution shows great difference between the DAIGT and Deepfake datasets. Therefore,
    we are confident to use Deepfake as an out-of-distribution dataset to test generalizability
    of different models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本长度、主题和词性分布三个不同的方面进行的分析显示 DAIGT 和 Deepfake 数据集之间存在很大差异。因此，我们有信心将 Deepfake
    作为分布外数据集来测试不同模型的泛化能力。
- en: '![Refer to caption](img/048c28144f33ce474906a17502d91550.png)![Refer to caption](img/cb290ded0434eb9a20bdfe85ac04ac32.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/048c28144f33ce474906a17502d91550.png)![参见说明](img/cb290ded0434eb9a20bdfe85ac04ac32.png)'
- en: 'Figure 1: Datasets topic distribution'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：数据集主题分布
- en: '![Refer to caption](img/6a602dfe128ba5b6a833a84ce72b61c5.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a602dfe128ba5b6a833a84ce72b61c5.png)'
- en: 'Figure 2: Dataset part-of-speech tag.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：数据集词性标签。
- en: '![Refer to caption](img/ef6dd8996af4bf840371502341d21b66.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef6dd8996af4bf840371502341d21b66.png)'
- en: 'Figure 3: Structure of single classifer detection.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：单一分类器检测的结构。
- en: '![Refer to caption](img/5bbed0c3a7de619371ae3ad071ca78b7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5bbed0c3a7de619371ae3ad071ca78b7.png)'
- en: 'Figure 4: Structure of assemble detection.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：集成检测的结构。
- en: '![Refer to caption](img/3eb02fe14f5365507f34af5e360ef0a2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3eb02fe14f5365507f34af5e360ef0a2.png)'
- en: 'Figure 5: Average accuracy of different methods.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同方法的平均准确率。
- en: 'TABLE III: Detailed detection performance on in-distribution dataset'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：在分布内数据集上的详细检测性能
- en: '|  |  | Metrics |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 指标 |'
- en: '| Strategy | Method | Human-generated text | LLM-generated text | Global |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 方法 | 人工生成文本 | LLM 生成文本 | 全球 |'
- en: '|  |  | Recall | Precision | F1 | Recall | Precision | F1 | Accuracy |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 召回率 | 精确度 | F1 | 召回率 | 精确度 | F1 | 准确率 |'
- en: '|  | DistilBert | 0.863 | 0.997 | 0.925 | 0.994 | 0.780 | 0.875 | 0.906 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | DistilBert | 0.863 | 0.997 | 0.925 | 0.994 | 0.780 | 0.875 | 0.906 |'
- en: '|  | DeBERTaV3 | 0.967 | 0.995 | 0.981 | 0.990 | 0.936 | 0.962 | 0.974 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | DeBERTaV3 | 0.967 | 0.995 | 0.981 | 0.990 | 0.936 | 0.962 | 0.974 |'
- en: '| Single Classifier | FNet | 0.961 | 0.862 | 0.909 | 0.685 | 0.896 | 0.776
    | 0.870 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 单一分类器 | FNet | 0.961 | 0.862 | 0.909 | 0.685 | 0.896 | 0.776 | 0.870 |'
- en: '|  | Albert | 0.959 | 0.929 | 0.943 | 0.85 | 0.909 | 0.879 | 0.923 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | Albert | 0.959 | 0.929 | 0.943 | 0.85 | 0.909 | 0.879 | 0.923 |'
- en: '|  | XLMRoberta | 0.879 | 0.996 | 0.934 | 0.993 | 0.801 | 0.887 | 0.917 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | XLMRoberta | 0.879 | 0.996 | 0.934 | 0.993 | 0.801 | 0.887 | 0.917 |'
- en: '| Non-adaptive Ensemble | Hard Voting | 0.960 | 0.995 | 0.977 | 0.991 | 0.924
    | 0.956 | 0.970 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 非自适应集成 | 硬投票 | 0.960 | 0.995 | 0.977 | 0.991 | 0.924 | 0.956 | 0.970 |'
- en: '|  | Neural Network | 0.996 | 0.992 | 0.994 | 0.984 | 0.992 | 0.988 | 0.992
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 神经网络 | 0.996 | 0.992 | 0.994 | 0.984 | 0.992 | 0.988 | 0.992 |'
- en: '| Adaptive Ensemble | Random Forest | 0.997 | 0.992 | 0.994 | 0.983 | 0.993
    | 0.988 | 0.992 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 自适应集成 | 随机森林 | 0.997 | 0.992 | 0.994 | 0.983 | 0.993 | 0.988 | 0.992 |'
- en: '|  | GBDT | 0.995 | 0.992 | 0.993 | 0.983 | 0.990 | 0.987 | 0.992 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | GBDT | 0.995 | 0.992 | 0.993 | 0.983 | 0.990 | 0.987 | 0.992 |'
- en: 'TABLE IV: Accuracy Comparison between in-distribution and out-of-distribution
    datasets'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：分布内数据集与分布外数据集的准确性比较
- en: '|  |  | Accuracy |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 准确性 |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Method | In-dist. testing | Out-of-dist. testing |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 分布内测试 | 分布外测试 |'
- en: '|  |  | DAIGT | Deepfake |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DAIGT | Deepfake |'
- en: '|  | DistilBert | 0.906 | 0.567 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | DistilBert | 0.906 | 0.567 |'
- en: '|  | DeBERTaV3 | 0.974 | 0.616 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | DeBERTaV3 | 0.974 | 0.616 |'
- en: '| Single Classifier | FNet | 0.870 | 0.659 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 单一分类器 | FNet | 0.870 | 0.659 |'
- en: '|  | Albert | 0.923 | 0.699 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | Albert | 0.923 | 0.699 |'
- en: '|  | XLMRoberta | 0.917 | 0.602 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | XLMRoberta | 0.917 | 0.602 |'
- en: '| Non-adaptive Ensemble | Hard Voting | 0.970 | 0.651 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 非自适应集成 | 硬投票 | 0.970 | 0.651 |'
- en: '|  | Neural Network | 0.992 | 0.736 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 神经网络 | 0.992 | 0.736 |'
- en: '| Adaptive Ensemble | Random Forest | 0.992 | 0.722 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 自适应集成 | 随机森林 | 0.992 | 0.722 |'
- en: '|  | GBDT | 0.992 | 0.718 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | GBDT | 0.992 | 0.718 |'
- en: IV Method
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 方法
- en: In this study, we firstly developed single classifier models leveraging pretrained
    transformers. Also, we aggregated the output of the classifiers with non-adaptive
    ensemble and adaptive ensemble methods. The performance and generalizability of
    these models and methods was subsequently evaluated on DAIGT and Deepfake datasets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们首先开发了利用预训练变换器的单一分类器模型。我们还使用非自适应集成和自适应集成方法聚合了这些分类器的输出。随后对这些模型和方法在DAIGT和Deepfake数据集上的性能和泛化能力进行了评估。
- en: IV-A Single Classifier Detection
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 单一分类器检测
- en: The classifiers are pre-trained transformer-based LMs, which were subsequently
    fine-tuned for the LLM-generated text detection task. As illustrated in Fig. [3](#S3.F3
    "Figure 3 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"), each classifier is associated with a classification head, comprising
    dropout and dense layers, integrated with the backbone of a transformer-based
    LM. We employed five distinct transformer-based LMs as the backbones for five
    individual classifiers. The pre-trained layers of the LM are frozen (not updated
    during training) to preserve their learned features. Only the classifier “head”
    (the last layer) is trained. In addition, we use cross-entropy loss and Adam optimizer
    with a learning rate of 5e-4\. During model training, each text sample is truncated
    to a maximum of 256 tokens, the classifier is trained for 8841 steps and each
    training batch contains 4 samples.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类器是基于预训练变换器的语言模型（LM），并随后针对LLM生成文本检测任务进行了微调。如图[3](#S3.F3 "图 3 ‣ III 数据集 ‣
    自适应集成的微调变换器用于LLM生成文本检测")所示，每个分类器都与一个分类头相关联，分类头包含了dropout和密集层，并与变换器LM的主干集成。我们使用了五种不同的变换器LM作为五个单独分类器的主干。LM的预训练层被冻结（在训练过程中不更新），以保留其学到的特征。只有分类器“头”（最后一层）进行训练。此外，我们使用交叉熵损失和Adam优化器，学习率为5e-4。在模型训练过程中，每个文本样本被截断为最多256个标记，分类器训练8841步，每个训练批次包含4个样本。
- en: 'The summary of the pre-trained LMs is provided below: 1) DistilBert: DistilBERT
    is developed through the distillation of the BERT base model, is notably smaller
    and faster [[64](#bib.bib64)]. We use the “distil_bert_base_en_uncased” as pretrained
    backbone weight. 2) DeBERTaV3: DeBERTaV3 is an enhanced pre-trained language model
    that outperforms its predecessor by implementing replaced token detection (RTD)
    and introducing a gradient-disentangled embedding sharing method [[65](#bib.bib65)].
    We use the “deberta_v3_base_en” as pretrained backbone weight. 3) FNet: FNet introduces
    a novel approach to speed up Transformer encoders by replacing self-attention
    sublayers with linear transformations, achieving 92-97% of BERT’s accuracy on
    the GLUE benchmark while training significantly faster [[66](#bib.bib66)]. We
    use the “f_net_base_en” as pretrained backbone weight. 4) Albert: Albert presents
    two parameter-reduction techniques that lower memory usage and accelerate training
    of BERT, achieving superior scaling and performance [[67](#bib.bib67)]. We use
    the “albert_base_en_uncased” as pretrained backbone weight. 5) XLMRoberta: XLMRoberta
    is a large-scale multilingual transformer-based LM pretrained on a diverse set
    of one hundred languages using extensive CommonCrawl data [[68](#bib.bib68)].
    We use the “xlm_roberta_base_multi” as pretrained backbone weight.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型的总结如下：1) DistilBert：DistilBERT是通过对BERT基础模型进行蒸馏开发的，显著更小更快[[64](#bib.bib64)]。我们使用“distil_bert_base_en_uncased”作为预训练骨干权重。2)
    DeBERTaV3：DeBERTaV3是一个增强的预训练语言模型，通过实现替换标记检测（RTD）并引入梯度解耦嵌入共享方法，超越了其前身[[65](#bib.bib65)]。我们使用“deberta_v3_base_en”作为预训练骨干权重。3)
    FNet：FNet通过用线性变换替代自注意力子层，引入了一种加速Transformer编码器的新方法，在GLUE基准上实现了92-97%的BERT准确度，同时训练速度显著提高[[66](#bib.bib66)]。我们使用“f_net_base_en”作为预训练骨干权重。4)
    Albert：Albert提出了两种参数减少技术，以降低内存使用和加速BERT训练，具有优越的扩展性和性能[[67](#bib.bib67)]。我们使用“albert_base_en_uncased”作为预训练骨干权重。5)
    XLMRoberta：XLMRoberta是一个大规模的多语言Transformer-based语言模型，使用广泛的CommonCrawl数据对一百种语言进行预训练[[68](#bib.bib68)]。我们使用“xlm_roberta_base_multi”作为预训练骨干权重。
- en: IV-B Non-Adaptive Ensemble
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 非自适应集成
- en: '1) Hard Voting Ensemble: The hard voting ensemble integrates the outputs of
    the distinct classifiers through a hard voting mechanism. Shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"), this ensemble technique operates by aggregating the predictions
    from each individual classifier without extra model training, where each classifier
    contributes a “vote” towards the final decision. The prediction receiving the
    majority of votes is then selected as the ensemble’s output.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 硬投票集成：硬投票集成通过硬投票机制整合不同分类器的输出。如图[4](#S3.F4 "Figure 4 ‣ III Dataset ‣ Adaptive
    Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection")所示，这种集成技术通过汇总每个单独分类器的预测来操作，而不需要额外的模型训练，每个分类器贡献一个“投票”来决定最终结果。获得大多数投票的预测被选为集成的输出。
- en: IV-C Adaptive Ensemble
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 自适应集成
- en: 'Adaptive ensemble presents adaptive approaches of classifier aggregation, including
    neural network ensemble random forest ensemble, and GBDT ensemble. Shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"), the adaptive ensemble methods assign dynamic weights to the
    output of each classifier based on their performance, enabling a more effective
    and context-sensitive combination of predictions. 1) Neural Network Ensemble:
    The neural network ensemble integrates with two ReLU-activated dense layers of
    32 and 16 neurons, each followed by 50% dropout for regularization, following
    the output from the five transformer-based classifiers, shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"). This structure employs dense layers to effectively synthesize
    the classifiers’ outputs, while the dropout layer aids in preventing overfitting,
    thus enhancing the model’s generalization capabilities. During the model training
    for the Neural Network, the five classifiers are frozen. The model was trained
    for 10 epochs using a batch size of 128 with the Adam optimizer and cross-entropy
    loss. The training process took just around 5 seconds. 2) Random Forest Ensemble:
    The random forest algorithm [[45](#bib.bib45)] with 100 estimators is used to
    adaptively aggregate the outputs of the distinct classifiers, shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"). During the model training for Random Forest, the five classifiers
    are frozen. This method capitalizes on the inherent adaptability and decision-making
    prowess of random forests to dynamically integrate classifier outputs, thereby
    enabling a more nuanced and context-sensitive ensemble decision-making process.
    3) GBDT (Gradient Boosting Decision Trees) Ensemble: GBDT [31] with 100 estimators
    adaptively aggregates outputs from the five classifiers, shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"). During the model training for the GBDT, the five classifiers
    are frozen. The approach ensures a dynamic and context-sensitive integration of
    diverse classifier predictions.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应集成展示了分类器聚合的自适应方法，包括神经网络集成、随机森林集成和GBDT集成。如图[4](#S3.F4 "Figure 4 ‣ III Dataset
    ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection")所示，自适应集成方法根据每个分类器的表现分配动态权重，从而实现更有效且具有上下文敏感性的预测组合。1)
    神经网络集成：神经网络集成与两个ReLU激活的全连接层（分别为32个和16个神经元）集成，每层后跟50%的dropout以进行正则化，跟随来自五个基于变换器的分类器的输出，如图[4](#S3.F4
    "Figure 4 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection")所示。该结构利用全连接层有效合成分类器的输出，而dropout层有助于防止过拟合，从而增强模型的泛化能力。在神经网络的模型训练过程中，五个分类器保持冻结。模型使用Adam优化器和交叉熵损失进行了10个周期的训练，批量大小为128。训练过程仅花费了大约5秒钟。2)
    随机森林集成：随机森林算法[[45](#bib.bib45)]使用100个估计器来自适应地聚合不同分类器的输出，如图[4](#S3.F4 "Figure 4
    ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection")所示。在随机森林的模型训练过程中，五个分类器保持冻结。该方法利用随机森林固有的适应性和决策能力，动态集成分类器的输出，从而实现更细致且具有上下文敏感性的集成决策过程。3)
    GBDT（梯度提升决策树）集成：GBDT [31]使用100个估计器自适应地聚合来自五个分类器的输出，如图[4](#S3.F4 "Figure 4 ‣ III
    Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text
    Detection")所示。在GBDT的模型训练过程中，五个分类器保持冻结。该方法确保了多样分类器预测的动态和具有上下文敏感性的集成。
- en: V Results
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结果
- en: 'We employed the standard metrics conventionally utilized in the domain of text
    classification: recall, precision, F1, and accuracy [[69](#bib.bib69)]. From Table [III](#S3.T3
    "TABLE III ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection") and [IV](#S3.T4 "TABLE IV ‣ III Dataset ‣ Adaptive Ensembles
    of Fine-Tuned Transformers for LLM-Generated Text Detection"), the adaptive ensemble
    methods have superior performance compared to single classifier models and non-adaptive
    ensemble. The adaptive ensemble methods achieve the best F1 scores for both LLM-generated
    and human-written texts, as well as the top accuracy across both in-distribution
    and out-of-distribution datasets. The findings highlight the adaptive ensemble
    methods’ robustness and their enhanced ability to generalize in detecting LLM-generated
    text.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了在文本分类领域常用的标准指标：召回率、精准率、F1 值和准确率 [[69](#bib.bib69)]。从表 [III](#S3.T3 "TABLE
    III ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection") 和 [IV](#S3.T4 "TABLE IV ‣ III Dataset ‣ Adaptive Ensembles of
    Fine-Tuned Transformers for LLM-Generated Text Detection") 可以看出，自适应集成方法在性能上优于单一分类器模型和非自适应集成。自适应集成方法在
    LLM 生成文本和人工编写文本的 F1 分数上都达到了最佳值，并且在分布内和分布外数据集上均表现出最高的准确率。这些发现突显了自适应集成方法的鲁棒性及其在检测
    LLM 生成文本时增强的泛化能力。
- en: V-A Single Model Detection
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 单模型检测
- en: As indicated in Table [III](#S3.T3 "TABLE III ‣ III Dataset ‣ Adaptive Ensembles
    of Fine-Tuned Transformers for LLM-Generated Text Detection"), the single classifiers
    demonstrate a predilection for higher Precision and F1 scores when classifying
    human-generated text, mainly attributed to the imbalanced dataset. The recall
    rates for human-generated versus LLM-generated text differ significantly among
    classifiers, underscoring the distinctiveness of each model. This diversity suggests
    that these models are well-suited for combination through ensemble algorithms,
    potentially enhancing overall classification performance. In Table [IV](#S3.T4
    "TABLE IV ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated
    Text Detection"), compared to in-distribution test set DAIGT, there is a significant
    accuracy regression on out-of-distribution test set Deepfake. Single classifier
    has bad generalization capabilities for unseen dataset. Also, DeBERTaV3 performs
    best in DAIGT while Albert performs best in Deepfake, which approves the generalization
    capabilities variance among transformer-based classifiers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [III](#S3.T3 "TABLE III ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned
    Transformers for LLM-Generated Text Detection") 所示，单一分类器在分类人工生成文本时倾向于较高的精准率和 F1
    分数，这主要归因于数据集的不平衡。分类器对人工生成文本与 LLM 生成文本的召回率差异显著，突显了每个模型的独特性。这种多样性表明，这些模型非常适合通过集成算法组合，可能会提升整体分类性能。在表
    [IV](#S3.T4 "TABLE IV ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers
    for LLM-Generated Text Detection") 中，相较于分布内测试集 DAIGT，分布外测试集 Deepfake 上的准确率有显著回退。单一分类器在未见数据集上的泛化能力较差。此外，DeBERTaV3
    在 DAIGT 上表现最佳，而 Albert 在 Deepfake 上表现最佳，这证实了基于变换器的分类器在泛化能力上的差异。
- en: V-B Non-Adaptive Ensemble Detection
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 非自适应集成检测
- en: The hard voting ensemble method achieved an accuracy of 0.970, surpassing most
    single classifier models, though it slightly underperforms the highest-performing
    DeBERTaV3 classifier, which recorded an accuracy of 0.974 on in-distribution dataset.
    This outcome aligns with expectations, as hard voting is a non-adaptive ensemble
    approach, and its performance is typically near that of the best-performing individual
    model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 硬投票集成方法达到了 0.970 的准确率，超越了大多数单一分类器模型，尽管略微低于最高表现的 DeBERTaV3 分类器，在分布内数据集上记录的准确率为
    0.974。这个结果符合预期，因为硬投票是一种非自适应的集成方法，其性能通常接近于最佳单一模型的性能。
- en: V-C Adaptive Ensemble Detection
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 自适应集成检测
- en: In Table [III](#S3.T3 "TABLE III ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned
    Transformers for LLM-Generated Text Detection"), the adaptive ensemble detection
    approach achieved an accuracy of 0.992 on the in-distribution test set, outperforming
    single classifier models and non-adaptive ensemble method. It also achieved the
    highest F1 scores for both detecting human and LLM-generated texts, demonstrating
    superior classification capabilities. Table [IV](#S3.T4 "TABLE IV ‣ III Dataset
    ‣ Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection")
    shows that adaptive ensemble methods also excel on out-of-distribution datasets,
    with the Neural Network Ensemble reaching the highest accuracy of 0.736, evidencing
    better generalization capabilities than single transformer-based classifier. In
    Fig. [5](#S3.F5 "Figure 5 ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned Transformers
    for LLM-Generated Text Detection"), adaptive assemble methods have the average
    accuracy of 0.992 on DAIGT and 0.725 on Deepfake, while transformer-based classifiers
    have 0.918 on DAIGT and 0.629 on Deepfake. In summary, the adaptive ensemble methods,
    integrated with its constituent transformer-based classifiers, significantly enhances
    the performance and generalization ability of LLM-generated text detection task,
    based on the results cross different datasets.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [III](#S3.T3 "TABLE III ‣ III Dataset ‣ Adaptive Ensembles of Fine-Tuned
    Transformers for LLM-Generated Text Detection") 中，适应性集成检测方法在分布内测试集上的准确率达到了 0.992，优于单一分类器模型和非适应性集成方法。它还在检测人类和
    LLM 生成文本的 F1 分数上均达到了最高值，展示了更卓越的分类能力。表 [IV](#S3.T4 "TABLE IV ‣ III Dataset ‣ Adaptive
    Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection") 显示，适应性集成方法在分布外数据集上也表现出色，其中神经网络集成达到了
    0.736 的最高准确率，证明其比单一基于变换器的分类器具有更好的泛化能力。在图 [5](#S3.F5 "Figure 5 ‣ III Dataset ‣
    Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection")
    中，适应性集成方法在 DAIGT 上的平均准确率为 0.992，在 Deepfake 上为 0.725，而基于变换器的分类器在 DAIGT 上为 0.918，在
    Deepfake 上为 0.629。总之，适应性集成方法结合其组成的基于变换器的分类器，显著提升了 LLM 生成文本检测任务的性能和泛化能力，基于不同数据集的结果。
- en: VI Conclusion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: This paper has successfully demonstrated the significant advantages of adaptive
    ensemble methods in the field of text classification in distinguishing between
    text generated by humans and large language models. This paper revealed that while
    individual transformer-based classifiers can tell LLM generated text, they exhibit
    constraints in their capacity to generalize when confronted with out-of-distribution
    data. The implementation of adaptive ensemble strategies has proven to be instrumental
    in mitigating the limitation. The adaptive ensemble not only improved accuracy
    in in-distribution dataset significantly but also showed better generalization
    ability out-of-distribution dataset. This dual enhancement in accuracy and generalizability
    makes the adaptive ensemble method as a robust, excellent tool in the ongoing
    challenge of LLM-generated text detection.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本文成功展示了适应性集成方法在文本分类领域中区分人类生成文本和大型语言模型生成文本的显著优势。本文揭示了虽然单个基于变换器的分类器可以识别 LLM 生成的文本，但当面对分布外数据时，它们在泛化能力上存在局限性。实施适应性集成策略在缓解这一限制方面发挥了重要作用。适应性集成不仅显著提高了分布内数据集的准确性，还在分布外数据集上表现出更好的泛化能力。这种准确性和泛化能力的双重提升，使适应性集成方法成为应对
    LLM 生成文本检测挑战的强大且优秀的工具。
- en: Acknowledgment
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Sincere thanks to Zelun Wang for paper review.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 真诚感谢 Zelun Wang 对论文的审阅。
- en: References
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*，“GPT-4 技术报告，” *arXiv 预印本 arXiv:2303.08774*，2023。'
- en: '[2] Anthropic, “Model card and evaluations for claude models,” Anthropic, Tech.
    Rep., 2023\. [Online]. Available: https://www-files.anthropic.com/production/
    images/Model-Card-Claude-2.pdf'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Anthropic，“Claude 模型的模型卡和评估，” Anthropic，技术报告，2023\. [在线]. 可用: https://www-files.anthropic.com/production/
    images/Model-Card-Claude-2.pdf'
- en: '[3] J. Su, C. Jiang, X. Jin, Y. Qiao, T. Xiao, H. Ma, R. Wei, Z. Jing, J. Xu,
    and J. Lin, “Large language models for forecasting and anomaly detection: A systematic
    literature review,” *arXiv preprint arXiv:2402.10350*, 2024.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Su, C. Jiang, X. Jin, Y. Qiao, T. Xiao, H. Ma, R. Wei, Z. Jing, J. Xu,
    和 J. Lin，“用于预测和异常检测的大型语言模型：系统文献综述，” *arXiv 预印本 arXiv:2402.10350*，2024。'
- en: '[4] V. Veselovsky, M. H. Ribeiro, and R. West, “Artificial artificial artificial
    intelligence: Crowd workers widely use large language models for text production
    tasks,” *arXiv preprint arXiv:2306.07899*, 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] V. Veselovsky, M. H. Ribeiro, 和 R. West, “人工的人工智能：众包工作者广泛使用大型语言模型进行文本生产任务，”
    *arXiv预印本 arXiv:2306.07899*，2023年。'
- en: '[5] T. Liu, I. Škrjanec, and V. Demberg, “Temperature-scaling surprisal estimates
    improve fit to human reading times–but does it do so for the “right reasons”?”
    in *ICLR 2024 Workshop on Representational Alignment*, 2024.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Liu, I. Škrjanec, 和 V. Demberg, “温度缩放惊讶估计改进对人类阅读时间的拟合——但它是否出于‘正确的原因’？”
    载于 *ICLR 2024 代表性对齐研讨会*，2024年。'
- en: '[6] D. Li, J. You, K. Funakoshi, and M. Okumura, “A-tip: attribute-aware text
    infilling via pre-trained language model,” in *Proceedings of the 29th International
    Conference on Computational Linguistics*, 2022, pp. 5857–5869.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Li, J. You, K. Funakoshi, 和 M. Okumura, “A-tip：通过预训练语言模型进行属性感知的文本填充，”
    载于 *第29届国际计算语言学会议论文集*，2022年，页码5857–5869。'
- en: '[7] Y. Zhou, X. Li, Q. Wang, and J. Shen, “Visual in-context learning for large
    vision-language models,” *arXiv preprint arXiv:2402.11574*, 2024.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Zhou, X. Li, Q. Wang, 和 J. Shen, “大规模视觉语言模型的视觉上下文学习，” *arXiv预印本 arXiv:2402.11574*，2024年。'
- en: '[8] Y. Zhou, X. Geng, T. Shen, C. Tao, G. Long, J.-G. Lou, and J. Shen, “Thread
    of thought unraveling chaotic contexts,” *arXiv preprint arXiv:2311.08734*, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Zhou, X. Geng, T. Shen, C. Tao, G. Long, J.-G. Lou, 和 J. Shen, “思维线索揭示混乱的上下文，”
    *arXiv预印本 arXiv:2311.08734*，2023年。'
- en: '[9] T. Liu, C. Xu, Y. Qiao, C. Jiang, and W. Chen, “News recommendation with
    attention mechanism,” *Journal of Industrial Engineering and Applied Science*,
    vol. 2, no. 1, pp. 21–26, 2024.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Liu, C. Xu, Y. Qiao, C. Jiang, 和 W. Chen, “具有注意力机制的新闻推荐，” *工业工程与应用科学期刊*，第2卷，第1期，页码21–26，2024年。'
- en: '[10] T. Susnjak, “Chatgpt: The end of online exam integrity?” *arXiv preprint
    arXiv:2212.09292*, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. Susnjak, “ChatGPT：在线考试诚信的终结？” *arXiv预印本 arXiv:2212.09292*，2022年。'
- en: '[11] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source legal
    large language model with integrated external knowledge bases,” *arXiv preprint
    arXiv:2306.16092*, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Cui, Z. Li, Y. Yan, B. Chen, 和 L. Yuan, “Chatlaw：具有集成外部知识库的开源法律大型语言模型，”
    *arXiv预印本 arXiv:2306.16092*，2023年。'
- en: '[12] S. R. Piccolo, P. Denny, A. Luxton-Reilly, S. Payne, and P. G. Ridge,
    “Many bioinformatics programming tasks can be automated with chatgpt,” *arXiv
    preprint arXiv:2303.13528*, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. R. Piccolo, P. Denny, A. Luxton-Reilly, S. Payne, 和 P. G. Ridge, “许多生物信息学编程任务可以通过ChatGPT自动化，”
    *arXiv预印本 arXiv:2303.13528*，2023年。'
- en: '[13] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.
    Tan, and D. S. W. Ting, “Large language models in medicine,” *Nature medicine*,
    vol. 29, no. 8, pp. 1930–1940, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.
    Tan, 和 D. S. W. Ting, “医学中的大型语言模型，” *自然医学*，第29卷，第8期，页码1930–1940，2023年。'
- en: '[14] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,”
    *ACM Computing Surveys*, vol. 55, no. 12, pp. 1–38, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, 和 P. Fung, “自然语言生成中的幻觉调查，” *ACM计算机调查*，第55卷，第12期，页码1–38，2023年。'
- en: '[15] J. Lee, T. Le, J. Chen, and D. Lee, “Do language models plagiarize?” in
    *Proceedings of the ACM Web Conference 2023*, 2023, pp. 3637–3647.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Lee, T. Le, J. Chen, 和 D. Lee, “语言模型是否会抄袭？” 载于 *ACM网页会议2023论文集*，2023年，页码3637–3647。'
- en: '[16] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang,
    M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh *et al.*, “Ethical and social risks
    of harm from language models,” *arXiv preprint arXiv:2112.04359*, 2021.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang,
    M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh *等*，“语言模型带来的伦理和社会风险，” *arXiv预印本 arXiv:2112.04359*，2021年。'
- en: '[17] N. Ayoobi, S. Shahriar, and A. Mukherjee, “The looming threat of fake
    and llm-generated linkedin profiles: Challenges and opportunities for detection
    and prevention,” in *Proceedings of the 34th ACM Conference on Hypertext and Social
    Media*, 2023, pp. 1–10.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. Ayoobi, S. Shahriar, 和 A. Mukherjee, “伪造和LLM生成的LinkedIn个人资料的迫在眉睫的威胁：检测和预防的挑战与机遇，”
    载于 *第34届ACM超文本与社交媒体会议论文集*，2023年，页码1–10。'
- en: '[18] C. Stokel-Walker, “Ai bot chatgpt writes smart essays-should academics
    worry?” *Nature*, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Stokel-Walker, “AI机器人ChatGPT撰写智能论文——学术界应该担忧吗？” *自然*，2022年。'
- en: '[19] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer,
    U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier *et al.*, “Chatgpt for good?
    on opportunities and challenges of large language models for education,” *Learning
    and individual differences*, vol. 103, p. 102274, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer,
    U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier *等*，“ChatGPT 的善用？关于大语言模型在教育中的机遇与挑战，”
    *学习与个体差异*，第 103 卷，页 102274，2023 年。'
- en: '[20] Y. Liang, X. Wang, Y. C. Wu, H. Fu, and M. Zhou, “A study on blockchain
    sandwich attack strategies based on mechanism design game theory,” *Electronics*,
    vol. 12, no. 21, p. 4417, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Liang, X. Wang, Y. C. Wu, H. Fu, and M. Zhou, “基于机制设计博弈理论的区块链三明治攻击策略研究，”
    *电子学*，第 12 卷，第 21 期，页 4417，2023 年。'
- en: '[21] J. Tian, C. Shen, B. Wang, X. Xia, M. Zhang, C. Lin, and Q. Li, “Lesson:
    Multi-label adversarial false data injection attack for deep learning locational
    detection,” *IEEE Transactions on Dependable and Secure Computing*, 2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Tian, C. Shen, B. Wang, X. Xia, M. Zhang, C. Lin, and Q. Li, “Lesson:
    多标签对抗性虚假数据注入攻击用于深度学习位置检测，” *IEEE 可靠性与安全计算期刊*，2024 年。'
- en: '[22] J. Liu, T. Hu, Y. Zhang, X. Gai, Y. Feng, and Z. Liu, “A chatgpt aided
    explainable framework for zero-shot medical image diagnosis,” *arXiv preprint
    arXiv:2307.01981*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Liu, T. Hu, Y. Zhang, X. Gai, Y. Feng, and Z. Liu, “一种 ChatGPT 辅助的可解释框架用于零样本医学图像诊断，”
    *arXiv 预印本 arXiv:2307.01981*，2023 年。'
- en: '[23] S. Chen, N. Kong, X. Sun, H. Meng, and M. Li, “Claims data-driven modeling
    of hospital time-to-readmission risk with latent heterogeneity,” *Health care
    management science*, vol. 22, pp. 156–179, 2019.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Chen, N. Kong, X. Sun, H. Meng, and M. Li, “基于索赔数据的医院再入院风险建模与潜在异质性，”
    *医疗管理科学*，第 22 卷，页 156–179，2019 年。'
- en: '[24] Y. Li, W. Wang, X. Yan, M. Gao, and M. Xiao, “Research on the application
    of semantic network in disease diagnosis prompts based on medical corpus,” *International
    Journal of Innovative Research in Computer Science & Technology*, vol. 12, no. 2,
    pp. 1–9, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Y. Li, W. Wang, X. Yan, M. Gao, and M. Xiao, “基于医学语料库的疾病诊断提示中语义网络应用的研究，”
    *计算机科学与技术国际创新研究杂志*，第 12 卷，第 2 期，页 1–9，2024 年。'
- en: '[25] S. Chen, W. D. Kearns, J. L. Fozard, and M. Li, “Personalized fall risk
    assessment for long-term care services improvement,” in *2017 Annual Reliability
    and Maintainability Symposium (RAMS)*.   IEEE, 2017, pp. 1–7.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Chen, W. D. Kearns, J. L. Fozard, and M. Li, “长期护理服务改进的个性化跌倒风险评估，”
    见 *2017 年年会可靠性与可维护性研讨会 (RAMS)*。IEEE，2017 年，页 1–7。'
- en: '[26] W. Dai, J. Tao, X. Yan, Z. Feng, and J. Chen, “Addressing unintended bias
    in toxicity detection: An lstm and attention-based approach,” in *2023 5th International
    Conference on Artificial Intelligence and Computer Applications (ICAICA)*.   IEEE,
    2023, pp. 375–379.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W. Dai, J. Tao, X. Yan, Z. Feng, and J. Chen, “解决毒性检测中的非预期偏见：一种基于 LSTM
    和注意力的方式，” 见 *2023 年第五届国际人工智能与计算机应用大会 (ICAICA)*。IEEE，2023 年，页 375–379。'
- en: '[27] J. Liu, T. Hu, Y. Zhang, Y. Feng, J. Hao, J. Lv, and Z. Liu, “Parameter-efficient
    transfer learning for medical visual question answering,” *IEEE Transactions on
    Emerging Topics in Computational Intelligence*, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Liu, T. Hu, Y. Zhang, Y. Feng, J. Hao, J. Lv, and Z. Liu, “医学视觉问答的参数高效迁移学习，”
    *IEEE 新兴计算智能期刊*，2023 年。'
- en: '[28] T. Liu, C. Xu, Y. Qiao, C. Jiang, and J. Yu, “Particle filter slam for
    vehicle localization,” *Journal of Industrial Engineering and Applied Science*,
    vol. 2, no. 1, pp. 27–31, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Liu, C. Xu, Y. Qiao, C. Jiang, and J. Yu, “车辆定位的粒子滤波 SLAM，” *工业工程与应用科学杂志*，第
    2 卷，第 1 期，页 27–31，2024 年。'
- en: '[29] S. Chen, L. Lu, Y. Xiang, Q. Lu, and M. Li, “A data heterogeneity modeling
    and quantification approach for field pre-assessment of chloride-induced corrosion
    in aging infrastructures,” *Reliability Engineering & System Safety*, vol. 171,
    pp. 123–135, 2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Chen, L. Lu, Y. Xiang, Q. Lu, and M. Li, “用于老化基础设施氯化物诱导腐蚀现场预评估的数据异质性建模与量化方法，”
    *可靠性工程与系统安全*，第 171 卷，页 123–135，2018 年。'
- en: '[30] J. Tian, B. Wang, R. Guo, Z. Wang, K. Cao, and X. Wang, “Adversarial attacks
    and defenses for deep-learning-based unmanned aerial vehicles,” *IEEE Internet
    of Things Journal*, vol. 9, no. 22, pp. 22 399–22 409, 2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Tian, B. Wang, R. Guo, Z. Wang, K. Cao, and X. Wang, “深度学习无人机的对抗攻击与防御，”
    *IEEE 物联网期刊*，第 9 卷，第 22 期，页 22 399–22 409，2021 年。'
- en: '[31] S. Chen, L. Lu, and M. Li, “Multi-state reliability demonstration tests,”
    *Quality Engineering*, vol. 29, no. 3, pp. 431–445, 2017.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Chen, L. Lu, and M. Li, “多状态可靠性演示测试，” *质量工程*，第 29 卷，第 3 期，页 431–445，2017
    年。'
- en: '[32] J. Wu, N. Hovakimyan, and J. Hobbs, “Genco: An auxiliary generator from
    contrastive learning for enhanced few-shot learning in remote sensing,” *arXiv
    preprint arXiv:2307.14612*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Wu, N. Hovakimyan, 和 J. Hobbs，“Genco：来自对比学习的辅助生成器，用于增强遥感中的少样本学习，” *arXiv预印本arXiv:2307.14612*，2023年。'
- en: '[33] S. Chen, L. Lu, Q. Zhang, and M. Li, “Optimal binomial reliability demonstration
    tests design under acceptance decision uncertainty,” *Quality Engineering*, vol. 32,
    no. 3, pp. 492–508, 2020.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Chen, L. Lu, Q. Zhang, 和 M. Li，“在接受决策不确定性下的最优二项可靠性演示测试设计，” *质量工程*，第32卷，第3期，第492–508页，2020年。'
- en: '[34] B. Wang, L. Lu, S. Chen, and M. Li, “Optimal test design for reliability
    demonstration under multi-stage acceptance uncertainties,” *Quality Engineering*,
    vol. 0, no. 0, pp. 1–14, 2023\. [Online]. Available: https://doi.org/10.1080/08982112.2023.2249188'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] B. Wang, L. Lu, S. Chen, 和 M. Li，“在多阶段接受不确定性下的最优测试设计，” *质量工程*，第0卷，第0期，第1–14页，2023年。[在线]。可用链接：https://doi.org/10.1080/08982112.2023.2249188'
- en: '[35] S. Chen, K. Li, H. Fu, Y. C. Wu, and Y. Huang, “Sea ice extent prediction
    with machine learning methods and subregional analysis in the arctic,” *Atmosphere*,
    vol. 14, no. 6, p. 1023, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Chen, K. Li, H. Fu, Y. C. Wu, 和 Y. Huang，“利用机器学习方法和北极次区域分析预测海冰范围，”
    *大气*，第14卷，第6期，第1023页，2023年。'
- en: '[36] R. Tao, P. Zhao, J. Wu, N. F. Martin, M. T. Harrison, C. Ferreira, Z. Kalantari,
    and N. Hovakimyan, “Optimizing crop management with reinforcement learning and
    imitation learning,” *arXiv preprint arXiv:2209.09991*, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] R. Tao, P. Zhao, J. Wu, N. F. Martin, M. T. Harrison, C. Ferreira, Z.
    Kalantari, 和 N. Hovakimyan，“利用强化学习和模仿学习优化作物管理，” *arXiv预印本arXiv:2209.09991*，2022年。'
- en: '[37] J. Wu, R. Tao, P. Zhao, N. F. Martin, and N. Hovakimyan, “Optimizing nitrogen
    management with deep reinforcement learning and crop simulations,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2022,
    pp. 1712–1720.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Wu, R. Tao, P. Zhao, N. F. Martin, 和 N. Hovakimyan，“利用深度强化学习和作物模拟优化氮管理，”
    收录于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第1712–1720页。'
- en: '[38] J. Wu, D. Pichler, D. Marley, D. Wilson, N. Hovakimyan, and J. Hobbs,
    “Extended agriculture-vision: An extension of a large aerial image dataset for
    agricultural pattern analysis,” *arXiv preprint arXiv:2303.02460*, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Wu, D. Pichler, D. Marley, D. Wilson, N. Hovakimyan, 和 J. Hobbs，“扩展农业视觉：用于农业模式分析的大型航拍图像数据集的扩展，”
    *arXiv预印本arXiv:2303.02460*，2023年。'
- en: '[39] J.-Y. Shi, L.-T. Ling, F. Xue, Z.-J. Qin, Y.-J. Li, Z.-X. Lai, and T. Yang,
    “Combining incremental conductance and firefly algorithm for tracking the global
    mpp of pv arrays,” *Journal of Renewable and Sustainable Energy*, vol. 9, no. 2,
    2017.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J.-Y. Shi, L.-T. Ling, F. Xue, Z.-J. Qin, Y.-J. Li, Z.-X. Lai, 和 T. Yang，“结合增量电导和萤火虫算法跟踪光伏阵列的全球MPP，”
    *可再生与可持续能源期刊*，第9卷，第2期，2017年。'
- en: '[40] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn, “Detectgpt:
    Zero-shot machine-generated text detection using probability curvature,” in *International
    Conference on Machine Learning*.   PMLR, 2023, pp. 24 950–24 962.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, 和 C. Finn，“Detectgpt：使用概率曲率进行零样本机器生成文本检测，”
    收录于*国际机器学习会议*，PMLR，2023年，第24,950–24,962页。'
- en: '[41] J. Pu, Z. Sarwar, S. M. Abdullah, A. Rehman, Y. Kim, P. Bhattacharya,
    M. Javed, and B. Viswanath, “Deepfake text detection: Limitations and opportunities,”
    in *2023 IEEE Symposium on Security and Privacy (SP)*.   IEEE, 2023, pp. 1613–1630.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Pu, Z. Sarwar, S. M. Abdullah, A. Rehman, Y. Kim, P. Bhattacharya,
    M. Javed, 和 B. Viswanath，“深伪文本检测：局限性与机遇，” 收录于*2023 IEEE安全与隐私研讨会 (SP)*，IEEE，2023年，第1613–1630页。'
- en: '[42] G. Jawahar, M. Abdul-Mageed, and L. V. Lakshmanan, “Automatic detection
    of machine generated text: A critical survey,” *arXiv preprint arXiv:2011.01314*,
    2020.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Jawahar, M. Abdul-Mageed, 和 L. V. Lakshmanan，“机器生成文本的自动检测：关键综述，” *arXiv预印本arXiv:2011.01314*，2020年。'
- en: '[43] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu,
    “How close is chatgpt to human experts? comparison corpus, evaluation, and detection,”
    *arXiv preprint arXiv:2301.07597*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, 和 Y. Wu，“ChatGPT与人类专家有多接近？比较语料库、评估与检测，”
    *arXiv预印本arXiv:2301.07597*，2023年。'
- en: '[44] Y. Li, Q. Li, L. Cui, W. Bi, L. Wang, L. Yang, S. Shi, and Y. Zhang, “Deepfake
    text detection in the wild,” *arXiv preprint arXiv:2305.13242*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Li, Q. Li, L. Cui, W. Bi, L. Wang, L. Yang, S. Shi, 和 Y. Zhang，“野外深伪文本检测，”
    *arXiv预印本arXiv:2305.13242*，2023年。'
- en: '[45] L. Breiman, “Random forests,” *Machine learning*, vol. 45, pp. 5–32, 2001.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] L. Breiman，“随机森林，” *机器学习*，第45卷，第5–32页，2001年。'
- en: '[46] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像价值 16x16 个词：用于大规模图像识别的变换器，”
    *arXiv 预印本 arXiv:2010.11929*，2020。'
- en: '[47] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “Bert: 用于语言理解的深度双向变换器预训练，”
    *arXiv 预印本 arXiv:1810.04805*，2018。'
- en: '[48] J. Wu, S. Chen, Q. Zhao, R. Sergazinov, C. Li, S. Liu, C. Zhao, T. Xie,
    H. Guo, C. Ji *et al.*, “Switchtab: Switched autoencoders are effective tabular
    learners,” *arXiv preprint arXiv:2401.02013*, 2024.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Wu, S. Chen, Q. Zhao, R. Sergazinov, C. Li, S. Liu, C. Zhao, T. Xie,
    H. Guo, C. Ji *等*，“Switchtab: 切换自动编码器是有效的表格学习器，” *arXiv 预印本 arXiv:2401.02013*，2024。'
- en: '[49] S. Chen, J. Wu, N. Hovakimyan, and H. Yao, “Recontab: Regularized contrastive
    representation learning for tabular data,” *arXiv preprint arXiv:2310.18541*,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] S. Chen, J. Wu, N. Hovakimyan, 和 H. Yao, “Recontab: 规整对比表示学习用于表格数据，” *arXiv
    预印本 arXiv:2310.18541*，2023。'
- en: '[50] X. Li, X. Wang, X. Chen, Y. Lu, H. Fu, and Y. C. Wu, “Unlabeled data selection
    for active learning in image classification,” *Scientific Reports*, vol. 14, no. 1,
    p. 424, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Li, X. Wang, X. Chen, Y. Lu, H. Fu, 和 Y. C. Wu, “图像分类中的主动学习未标记数据选择，”
    *科学报告*，第14卷，第1期，第424页，2024。'
- en: '[51] R. Shijaku and E. Canhasi, “Chatgpt generated text detection,” *Publisher:
    Unpublished*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Shijaku 和 E. Canhasi, “Chatgpt 生成文本检测，” *出版商：未出版*，2023。'
- en: '[52] Y. Wang, J. Mansurov, P. Ivanov, J. Su, A. Shelmanov, A. Tsvigun, C. Whitehouse,
    O. M. Afzal, T. Mahmoud, A. F. Aji *et al.*, “M4: Multi-generator, multi-domain,
    and multi-lingual black-box machine-generated text detection,” *arXiv preprint
    arXiv:2305.14902*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Wang, J. Mansurov, P. Ivanov, J. Su, A. Shelmanov, A. Tsvigun, C. Whitehouse,
    O. M. Afzal, T. Mahmoud, A. F. Aji *等*，“M4: 多生成器、多领域和多语言黑箱机器生成文本检测，” *arXiv 预印本
    arXiv:2305.14902*，2023。'
- en: '[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “Attention is all you need，” *神经信息处理系统进展*，第30卷，2017。'
- en: '[54] M. Zhu, Y. Zhang, Y. Gong, K. Xing, X. Yan, and J. Song, “Ensemble methodology:
    Innovations in credit default prediction using lightgbm, xgboost, and localensemble,”
    *arXiv preprint arXiv:2402.17979*, 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Zhu, Y. Zhang, Y. Gong, K. Xing, X. Yan, 和 J. Song, “集成方法论：使用 lightgbm、xgboost
    和 localensemble 在信用违约预测中的创新，” *arXiv 预印本 arXiv:2402.17979*，2024。'
- en: '[55] Z. Hu, J. Zhang, H. Wang, S. Liu, and S. Liang, “Leveraging relational
    graph neural network for transductive model ensemble,” in *Proceedings of the
    29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, 2023, pp.
    775–787.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Hu, J. Zhang, H. Wang, S. Liu, 和 S. Liang, “利用关系图神经网络进行传导模型集成，” 见 *第29届
    ACM SIGKDD 知识发现与数据挖掘会议论文集*，2023，页 775–787。'
- en: '[56] R. Delgado, “A semi-hard voting combiner scheme to ensemble multi-class
    probabilistic classifiers,” *Applied Intelligence*, vol. 52, no. 4, pp. 3653–3677,
    2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] R. Delgado, “一种半硬投票组合方案用于集成多类概率分类器，” *应用智能*，第52卷，第4期，页 3653–3677，2022。'
- en: '[57] W. Weimin, L. Yufeng, Y. Xu, X. Mingxuan, and G. Min, “Enhancing liver
    segmentation: A deep learning approach with eas feature extraction and multi-scale
    fusion,” *International Journal of Innovative Research in Computer Science & Technology*,
    vol. 12, no. 1, pp. 26–34, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] W. Weimin, L. Yufeng, Y. Xu, X. Mingxuan, 和 G. Min, “增强肝脏分割：一种具有易于提取特征和多尺度融合的深度学习方法，”
    *国际计算机科学与技术创新研究期刊*，第12卷，第1期，页 26–34，2024。'
- en: '[58] D. Li, Y. Wang, K. Funakoshi, and M. Okumura, “Joyful: Joint modality
    fusion and graph contrastive learning for multimodal emotion recognition,” *arXiv
    preprint arXiv:2311.11009*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Li, Y. Wang, K. Funakoshi, 和 M. Okumura, “Joyful: 联合模态融合和图对比学习用于多模态情感识别，”
    *arXiv 预印本 arXiv:2311.11009*，2023。'
- en: '[59] Y. Wang, D. Li, K. Funakoshi, and M. Okumura, “Emp: emotion-guided multi-modal
    fusion and contrastive learning for personality traits recognition,” in *Proceedings
    of the 2023 ACM International Conference on Multimedia Retrieval*, 2023, pp. 243–252.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Wang, D. Li, K. Funakoshi, 和 M. Okumura, “Emp: 基于情感引导的多模态融合和对比学习用于个性特征识别，”
    见 *2023年 ACM 国际多媒体检索会议论文集*，2023，页 243–252。'
- en: '[60] Z. Hu, J. Zhang, Y. Yu, Y. Zhuang, and H. Xiong, “How many validation
    labels do you need? exploring the design space of label-efficient model ranking,”
    *arXiv preprint arXiv:2312.01619*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Hu, J. Zhang, Y. Yu, Y. Zhuang 和 H. Xiong，“你需要多少验证标签？探索标签高效模型排名的设计空间，”
    *arXiv预印本 arXiv:2312.01619*，2023年。'
- en: '[61] J. H. Friedman, “Greedy function approximation: a gradient boosting machine,”
    *Annals of statistics*, pp. 1189–1232, 2001.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. H. Friedman，“贪婪函数逼近：一种梯度提升机器，” *统计年鉴*，第1189–1232页，2001年。'
- en: '[62] H. Abburi, M. Suesserman, N. Pudota, B. Veeramani, E. Bowen, and S. Bhattacharya,
    “Generative ai text classification using ensemble llm approaches,” *arXiv preprint
    arXiv:2309.07755*, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Abburi, M. Suesserman, N. Pudota, B. Veeramani, E. Bowen 和 S. Bhattacharya，“使用集成
    LLM 方法的生成 AI 文本分类，” *arXiv预印本 arXiv:2309.07755*，2023年。'
- en: '[63] D. Kleczek, *DAIGT Proper Train Dataset*, Kaggle, 2013\. [Online]. Available:
    https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] D. Kleczek，*DAIGT Proper Train Dataset*，Kaggle，2013年。 [在线]。 可用网址：https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset'
- en: '[64] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
    version of bert: smaller, faster, cheaper and lighter,” *arXiv preprint arXiv:1910.01108*,
    2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] V. Sanh, L. Debut, J. Chaumond 和 T. Wolf，“Distilbert，bert的精简版：更小、更快、更便宜、更轻便，”
    *arXiv预印本 arXiv:1910.01108*，2019年。'
- en: '[65] P. He, J. Gao, and W. Chen, “Debertav3: Improving deberta using electra-style
    pre-training with gradient-disentangled embedding sharing,” *arXiv preprint arXiv:2111.09543*,
    2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. He, J. Gao 和 W. Chen，“Debertav3：通过带有梯度解耦嵌入共享的 electra 风格预训练改进 deberta，”
    *arXiv预印本 arXiv:2111.09543*，2021年。'
- en: '[66] J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon, “Fnet: Mixing tokens
    with fourier transforms,” *arXiv preprint arXiv:2105.03824*, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Lee-Thorp, J. Ainslie, I. Eckstein 和 S. Ontanon，“Fnet：使用傅里叶变换混合标记，”
    *arXiv预印本 arXiv:2105.03824*，2021年。'
- en: '[67] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert:
    A lite bert for self-supervised learning of language representations,” *arXiv
    preprint arXiv:1909.11942*, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma 和 R. Soricut，“Albert：用于自监督语言表示学习的轻量级
    bert，” *arXiv预印本 arXiv:1909.11942*，2019年。'
- en: '[68] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán,
    E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, “Unsupervised cross-lingual
    representation learning at scale,” *arXiv preprint arXiv:1911.02116*, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán,
    E. Grave, M. Ott, L. Zettlemoyer 和 V. Stoyanov，“大规模无监督跨语言表示学习，” *arXiv预印本 arXiv:1911.02116*，2019年。'
- en: '[69] F. Sebastiani, “Machine learning in automated text categorization,” *ACM
    computing surveys (CSUR)*, vol. 34, no. 1, pp. 1–47, 2002.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] F. Sebastiani，“自动化文本分类中的机器学习，” *ACM计算机调查（CSUR）*，第34卷，第1期，第1–47页，2002年。'
