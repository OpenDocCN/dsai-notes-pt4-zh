- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用大型语言模型进行零样本和少样本提示：对孟加拉语情感分析的微调模型进行比较研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.10783](https://ar5iv.labs.arxiv.org/html/2308.10783)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.10783](https://ar5iv.labs.arxiv.org/html/2308.10783)
- en: Md. Arid Hasan ¹, Shudipta Das ², Afiyat Anjum ², Firoj Alam ³,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Md. Arid Hasan ¹、Shudipta Das ²、Afiyat Anjum ²、Firoj Alam ³，
- en: Anika Anjum ², Avijit Sarker ², Sheak Rashed Haider Noori ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Anika Anjum ²、Avijit Sarker ²、Sheak Rashed Haider Noori ²
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid expansion of the digital world has propelled sentiment analysis into
    a critical tool across diverse sectors such as marketing, politics, customer service,
    and healthcare. While there have been significant advancements in sentiment analysis
    for widely spoken languages, low-resource languages, such as Bangla, remain largely
    under-researched due to resource constraints. Furthermore, the recent unprecedented
    performance of Large Language Models (LLMs) in various applications highlights
    the need to evaluate them in the context of low-resource languages. In this study,
    we present a sizeable manually annotated dataset encompassing 33,605 Bangla news
    tweets and Facebook comments. We also investigate zero- and few-shot in-context
    learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering
    a comparative analysis against fine-tuned models. Our findings suggest that monolingual
    transformer-based models consistently outperform other models, even in zero and
    few-shot scenarios. To foster continued exploration, we intend to make this dataset
    and our research tools publicly available to the broader research community. In
    the spirit of further research, we plan to make this dataset and our experimental
    resources publicly accessible to the wider research community.¹¹1https://anonymous.com/
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数字世界的快速扩展使得情感分析成为市场营销、政治、客户服务和医疗保健等多个领域的重要工具。尽管在广泛使用的语言中，情感分析取得了显著进展，但由于资源限制，低资源语言如孟加拉语仍然处于研究不足的状态。此外，大型语言模型（LLMs）在各种应用中的前所未有的表现突显了在低资源语言背景下评估它们的必要性。在这项研究中，我们呈现了一个大规模的手动注释数据集，包括
    33,605 条孟加拉语新闻推文和 Facebook 评论。我们还研究了包括 Flan-T5、GPT-4 和 Bloomz 在内的多种语言模型的零样本和少样本上下文学习，提供了与微调模型的比较分析。我们的发现表明，单语言的基于变压器的模型在零样本和少样本场景下始终优于其他模型。为了促进持续探索，我们打算将该数据集和我们的研究工具公开，以便更广泛的研究社区使用。为了进一步研究，我们计划将该数据集和我们的实验资源公开，以便更广泛的研究社区使用。¹¹1https://anonymous.com/
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Sentiment analysis is an influential sub-area of NLP that deals with sentiment,
    emotions, affect and stylistic analysis in language. There has been significant
    research effort for sentiment analysis due to its need in various fields, such
    as business, finance, politics, education, and services (Cui et al. [2023](#bib.bib20)).
    The analysis typically has been done different types of content – domains (news,
    blog post, customer reviews, social media posts), modalities (textual and multimodal)
    (Hussein [2018](#bib.bib26); Dashtipour et al. [2016](#bib.bib21)). The surge
    in user-generated content on social media platforms has become a significant phenomenon,
    as individuals increasingly voice their opinions on a wide array of topics through
    comments and tweets. As a result, these platforms have garnered considerable research
    attention as valuable sources of data for sentiment analysis (Yue et al. [2019](#bib.bib50)).
    Leveraging such data resources (Dashtipour et al. [2016](#bib.bib21)), substantial
    progress has been achieved for the sentiment analysis in English. The advancements
    range from quantifying sentiment polarity to tackling more complex challenges
    like identifying aspects (Chen et al. [2022](#bib.bib15)), multimodal sentiment
    detection (Liang et al. [2022](#bib.bib33)), explainability (Cambria et al. [2022](#bib.bib13)),
    and multilingual sentiment analysis (Barbieri, Espinosa Anke, and Camacho-Collados
    [2022](#bib.bib8); Galeshchuk, Qiu, and Jourdan [2019](#bib.bib23)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是自然语言处理（NLP）的一个重要子领域，涉及语言中的情感、情绪、情感影响及风格分析。由于其在商业、金融、政治、教育和服务等多个领域的需求，情感分析已经吸引了大量研究努力（Cui
    et al. [2023](#bib.bib20)）。该分析通常涵盖不同类型的内容——领域（新闻、博客文章、客户评价、社交媒体帖子）、模态（文本和多模态）（Hussein
    [2018](#bib.bib26); Dashtipour et al. [2016](#bib.bib21)）。社交媒体平台上用户生成内容的激增已成为一个重要现象，个人越来越多地通过评论和推文表达对各种话题的看法。因此，这些平台成为了情感分析的重要数据来源，吸引了大量研究关注（Yue
    et al. [2019](#bib.bib50)）。利用这些数据资源（Dashtipour et al. [2016](#bib.bib21)），英语情感分析取得了显著进展。这些进展涵盖了从量化情感极性到应对更复杂的挑战，如识别方面（Chen
    et al. [2022](#bib.bib15)）、多模态情感检测（Liang et al. [2022](#bib.bib33)）、可解释性（Cambria
    et al. [2022](#bib.bib13)）以及多语言情感分析（Barbieri, Espinosa Anke, 和 Camacho-Collados
    [2022](#bib.bib8); Galeshchuk, Qiu, 和 Jourdan [2019](#bib.bib23)）。
- en: There has been a growing research interest over time in sentiment analysis for
    low-resource languages (Batanović, Nikolić, and Milosavljević [2016](#bib.bib9);
    Nabil, Aly, and Atiya [2015](#bib.bib37); Muhammad et al. [2023](#bib.bib35)).
    Similar to other low resources languages, the research for the sentiment analysis
    for Bangla has been limited (Islam et al. [2021](#bib.bib27), [2023](#bib.bib28)).
    A study conducted by Alam et al. ([2021a](#bib.bib2)) emphasized the primary challenges
    associated with Bangla sentiment analysis, specifically issues of duplicate instances
    in the data, inadequate reporting of annotation agreement, and generalization.
    These challenges were also highlighted in (Islam et al. [2021](#bib.bib27)), further
    emphasizing the need to address them for effective sentiment analysis in Bangla.
    To further facilitate sentiment analysis research in Bangla, we have created a
    multi-platform sentiment analysis dataset in this study. The dataset has undergone
    multiple rounds of pre-processing and validation to ensure its suitability for
    both sentiment analysis tasks and qualitative investigations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对低资源语言的情感分析研究随着时间的推移逐渐增加（Batanović, Nikolić, 和 Milosavljević [2016](#bib.bib9);
    Nabil, Aly, 和 Atiya [2015](#bib.bib37); Muhammad et al. [2023](#bib.bib35)）。与其他低资源语言类似，孟加拉语的情感分析研究也有限（Islam
    et al. [2021](#bib.bib27), [2023](#bib.bib28)）。Alam et al. ([2021a](#bib.bib2))
    的研究强调了孟加拉语情感分析面临的主要挑战，特别是数据中重复实例的问题、标注一致性的报告不足以及泛化问题。这些挑战在（Islam et al. [2021](#bib.bib27)）中也有所提及，进一步强调了有效进行孟加拉语情感分析的必要性。为了进一步推动孟加拉语情感分析研究，我们在本研究中创建了一个多平台情感分析数据集。该数据集经过多轮预处理和验证，以确保其适用于情感分析任务和定性研究。
- en: 'We provide a comparative analysis using various pre-trained language models,
    including zero and few-shot settings with Flan-T5, GPT-4 and Bloomz as presented
    in [1](#Sx1.F1 "Figure 1 ‣ Introduction ‣ Zero- and Few-Shot Prompting with LLMs:
    A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis"). The
    performance clearly shows that LLMs outperforms random and majority baselines.
    More details are discussed in the Results section.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了使用各种预训练语言模型的比较分析，包括 Flan-T5、GPT-4 和 Bloomz 在零样本和少样本设置中的表现，如 [1](#Sx1.F1
    "图 1 ‣ 介绍 ‣ 使用 LLMs 的零样本和少样本提示：对孟加拉语情感分析的微调模型的比较研究") 中所述。性能明显表明，大语言模型优于随机和多数基线。更多细节在结果部分讨论。
- en: '![Refer to caption](img/511a7072da4286b757af7ce919ba923e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/511a7072da4286b757af7ce919ba923e.png)'
- en: 'Figure 1: Performance comparisons with baselines (random and majority), fine-tuned
    models and LLMs (GPT and Bloomz).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：与基线（随机和多数）、微调模型以及大语言模型（GPT 和 Bloomz）的性能比较。
- en: 'Our contributions can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Development of one of the largest manually annotated datasets for sentiment
    analysis.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发展了一个最大的手工标注的情感分析数据集。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Investigation into zero-shot and few-shot learning using various LLMs. We are
    the first to provide such a comprehensive evaluation for Bangla sentiment analysis.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对使用各种大语言模型（LLMs）进行零样本和少样本学习的调查。我们首次为孟加拉语情感分析提供了如此全面的评估。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comparative analysis of performance differences between in-context learning
    and fine-tuned models.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对上下文学习和微调模型之间性能差异的比较分析。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Investigation of how different prompting variations affect performance in in-context
    learning.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调查不同提示变体如何影响上下文学习中的性能。
- en: 'Based on our extensive experiments our findings are summarized below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们广泛的实验，以下是我们的发现总结：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine-tuned models yield better results compared to both zero- and few-shot in-context
    learning setups.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调模型在零样本和少样本学习设置中表现更佳。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine-tuned models using monolingual text (BanglaBERT) demonstrate superior performance.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用单语文本（BanglaBERT）的微调模型表现优越。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There is little to no performance difference between zero- and few-shot learning
    with the GPT-4 model.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 GPT-4 模型的零样本和少样本学习之间几乎没有性能差异。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For the majority of zero- and few-shot runs, Bloomz yielded better performance
    than GPT-4.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于大多数零样本和少样本实验，Bloomz 的表现优于 GPT-4。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While Bloomz failed to predict the neutral class, GPT-4 struggled with positive
    class prediction.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然 Bloomz 无法预测中性类别，但 GPT-4 在积极类别预测方面表现挣扎。
- en: 'The remainder of the paper is structured as follows: Section Related Work provides
    an overview of relevant literature. The Dataset section delves into the dataset
    used, along with an analysis of its contents. In Section Methodology, we detail
    the models and experiments. The Results and Discussion section presents and discusses
    our findings. Lastly, Section Conclusion wraps up the paper.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分结构如下：相关工作部分概述了相关文献。数据集部分**深入探讨**了使用的数据集及其内容分析。在方法论部分，我们详细说明了模型和实验。结果与讨论部分呈现并讨论了我们的发现。最后，结论部分总结了论文内容。
- en: Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: 'In the realm of sentiment classification for Bangla, the current state-of-the-art
    research focuses on two key aspects: resource development and tackling model development
    challenges. Earlier work in this area has encompassed rule-based methodologies
    as well as classical machine learning approaches and recently the use of pre-trained
    models has received a wider attention.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在孟加拉语情感分类领域，目前的最前沿研究集中在两个关键方面：资源开发和解决模型开发挑战。早期的工作涵盖了基于规则的方法以及经典的机器学习方法，最近预训练模型的使用也受到广泛关注。
- en: Datasets
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: Over time there has been several resources developed including manual and semi-supervised
    labeling approach (Chowdhury and Chowdhury [2014](#bib.bib17); Alam et al. [2021a](#bib.bib2);
    Islam et al. [2021](#bib.bib27), [2023](#bib.bib28); Kabir et al. [2023](#bib.bib30)).
    Chowdhury and Chowdhury ([2014](#bib.bib17)) developed a dataset using semi-supervised
    approaches and trained models SVM and Maximum Entropy. The study of Kabir et al.
    ([2023](#bib.bib30)) proposed an annotated sentiment corpus comprising 158,065
    reviews collected from online bookstores. The annotations were primarily based
    on the rating of the reviews, with the majority (89.6%) being in the positive
    class. The study also evaluated classical and BERT-based models for training and
    performance assessment. The skewness of this dataset makes it particularly challenging.
    SentiGold (Islam et al. [2023](#bib.bib28))²²2Note that this dataset is not publicly
    available. is a well-balanced sentiment dataset containing 70K entries from 30
    domains. It was gathered from various sources, including YouTube, Facebook, newspapers,
    blogs, etc., and labeled into five classes. The reported inter-annotator agreement
    stands at 0.88.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，已经开发出多个资源，包括手动和半监督标注方法 (Chowdhury 和 Chowdhury [2014](#bib.bib17); Alam
    等人 [2021a](#bib.bib2); Islam 等人 [2021](#bib.bib27), [2023](#bib.bib28); Kabir
    等人 [2023](#bib.bib30))。Chowdhury 和 Chowdhury ([2014](#bib.bib17)) 使用半监督方法开发了一个数据集，并训练了
    SVM 和最大熵模型。Kabir 等人 ([2023](#bib.bib30)) 的研究提出了一个注释情感语料库，包含 158,065 条来自在线书店的评论。注释主要基于评论的评分，其中大多数（89.6%）属于正面类别。该研究还评估了经典和基于
    BERT 的模型用于训练和性能评估。该数据集的偏斜性使其特别具有挑战性。SentiGold (Islam 等人 [2023](#bib.bib28))²²2注意，此数据集不公开。是一个平衡良好的情感数据集，包含来自
    30 个领域的 70K 条条目。它从 YouTube、Facebook、报纸、博客等多个来源收集，并标注为五个类别。报告的注释者一致性达到 0.88。
- en: Rahman and Kumar Dey ([2018](#bib.bib41)) labeled 5,700 instances as positive,
    negative, or neutral for two aspect-based sentiment analysis (ABSA) tasks, specifically
    extracting aspect categories and polarity. The authors curated two new datasets
    from cricket and restaurants domains. Islam et al. ([2021](#bib.bib27)) developed
    the SentNoB dataset, comprising 15,000 manually annotated comments collected from
    the comments section of Bangla news articles and videos across 13 diverse domains.
    The experimental findings using this dataset indicate that lexical feature combinations
    outperform neural models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Rahman 和 Kumar Dey ([2018](#bib.bib41)) 为两个基于方面的情感分析（ABSA）任务标记了 5,700 个实例，分类为正面、负面或中性，具体任务包括提取方面类别和极性。作者从板球和餐馆领域策划了两个新的数据集。Islam
    等人 ([2021](#bib.bib27)) 开发了 SentNoB 数据集，该数据集包含 15,000 条手动注释的评论，这些评论来自于 13 个不同领域的孟加拉语新闻文章和视频的评论区。使用该数据集的实验结果表明，词汇特征组合优于神经网络模型。
- en: Models
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: Various classical algorithms have been employed in different studies for sentiment
    classification in Bangla. These include Bernoulli Naive Bayes (BNB), Decision
    Tree, Support Vector Machine (SVM), Maximum Entropy (ME), and Multinomial Naive
    Bayes (MNB) (Rahman and Hossen [2019](#bib.bib40); Banik and Rahman [2018](#bib.bib7);
    Chowdhury et al. [2019](#bib.bib16)). Islam et al. ([2016](#bib.bib29)) developed
    a sentiment classification system for textual movie reviews in Bangla. The authors
    utilized two machine learning algorithms, Naive Bayes (NB) and SVM, and provided
    comparative results. Additionally, Islam et al. ([2016](#bib.bib29)) employed
    NB with rules for sentiment detection in Bengali Facebook statuses.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在孟加拉语的情感分类研究中，采用了多种经典算法。这些算法包括 Bernoulli 朴素贝叶斯（BNB）、决策树、支持向量机（SVM）、最大熵（ME）和多项式朴素贝叶斯（MNB）(Rahman
    和 Hossen [2019](#bib.bib40); Banik 和 Rahman [2018](#bib.bib7); Chowdhury 等人 [2019](#bib.bib16))。Islam
    等人 ([2016](#bib.bib29)) 开发了一种针对孟加拉语文本电影评论的情感分类系统。作者使用了两种机器学习算法，朴素贝叶斯（NB）和 SVM，并提供了比较结果。此外，Islam
    等人 ([2016](#bib.bib29)) 使用了带规则的 NB 进行孟加拉语 Facebook 状态的情感检测。
- en: Deep learning algorithms have been extensively explored in the context of Bangla
    sentiment analysis (Hassan et al. [2016](#bib.bib25); Aziz Sharfuddin, Nafis Tihami,
    and Saiful Islam [2018](#bib.bib5); Tripto and Ali [2018](#bib.bib47); Ashik,
    Shovon, and Haque [2019](#bib.bib4); Karim et al. [2020](#bib.bib31); Sazzed [2021](#bib.bib44);
    Sharmin and Chakma [2021](#bib.bib46)). In the study conducted by Tripto and Ali
    ([2018](#bib.bib47)), the authors utilized Long Short-Term Memory (LSTM) networks
    and Convolutional Neural Networks (CNNs) with an embedding layer to identify both
    sentiment and emotion in YouTube comments. Ashik, Shovon, and Haque ([2019](#bib.bib4))
    conducted a comparative analysis of classical algorithms, such as Support Vector
    Machines (SVM), alongside deep learning algorithms, including LSTM and CNN, for
    sentiment classification of Bangla news comments. Karim et al. ([2020](#bib.bib31))
    integrated word embeddings into a Multichannel Convolutional-LSTM (MConv-LSTM)
    network, enabling the prediction of various types of hate speech, document classification,
    and sentiment analysis in the Bangla language. Another aspect explored in sentiment
    analysis is the utilization of LSTM models due to the prevalence of romanized
    Bangla texts in social media. Hassan et al. ([2016](#bib.bib25)); Aziz Sharfuddin,
    Nafis Tihami, and Saiful Islam ([2018](#bib.bib5)) employed LSTM models to design
    and evaluate their sentiment analysis models, taking into account the unique characteristics
    of romanized Bangla texts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法在孟加拉语情感分析的背景下得到了广泛的探索（Hassan等 [2016](#bib.bib25)；Aziz Sharfuddin、Nafis
    Tihami 和 Saiful Islam [2018](#bib.bib5)；Tripto 和 Ali [2018](#bib.bib47)；Ashik、Shovon
    和 Haque [2019](#bib.bib4)；Karim等 [2020](#bib.bib31)；Sazzed [2021](#bib.bib44)；Sharmin
    和 Chakma [2021](#bib.bib46)）。在 Tripto 和 Ali ([2018](#bib.bib47)) 进行的研究中，作者利用了长短期记忆（LSTM）网络和卷积神经网络（CNN）及嵌入层来识别
    YouTube 评论中的情感和情绪。Ashik、Shovon 和 Haque ([2019](#bib.bib4)) 对经典算法，如支持向量机（SVM），与包括
    LSTM 和 CNN 在内的深度学习算法进行了比较分析，以进行孟加拉新闻评论的情感分类。Karim 等 ([2020](#bib.bib31)) 将词嵌入整合到多通道卷积-LSTM（MConv-LSTM）网络中，能够预测各种类型的仇恨言论、文档分类和孟加拉语言的情感分析。情感分析中探索的另一个方面是由于社交媒体中罗马化孟加拉文本的普遍存在而利用
    LSTM 模型。Hassan 等 ([2016](#bib.bib25))；Aziz Sharfuddin、Nafis Tihami 和 Saiful Islam
    ([2018](#bib.bib5)) 采用了 LSTM 模型来设计和评估他们的情感分析模型，考虑了罗马化孟加拉文本的独特特征。
- en: In the study conducted by Hasan et al. ([2020](#bib.bib24)), a comprehensive
    comparison was performed on various annotated sentiment datasets consisting of
    Bangla content from social media sources. The research investigated the effectiveness
    of both classical algorithms, such as SVM, and deep learning algorithms, including
    CNN and transformer models. Notably, the deep learning algorithm XLMRoBERTa exhibited
    superior performance with an accuracy of 0.671, surpassing the classical algorithm
    SVM, which achieved an accuracy of 0.581.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hasan 等 ([2020](#bib.bib24)) 进行的研究中，对来自社交媒体源的孟加拉内容的各种注释情感数据集进行了全面比较。研究调查了经典算法，如
    SVM，与包括 CNN 和 transformer 模型在内的深度学习算法的有效性。值得注意的是，深度学习算法 XLMRoBERTa 表现出更优的性能，准确率为
    0.671，超越了经典算法 SVM 的 0.581 的准确率。
- en: In a review article by Alam et al. ([2021a](#bib.bib2)), the authors investigated
    nine NLP tasks, including sentiment analysis. They reported that transformer-based
    models, particularly XLM-RoBERTa-large, are more suitable for Bangla text categorization
    problems than other machine learning techniques such as LSTM, BERT, and CNN.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Alam 等 ([2021a](#bib.bib2)) 的综述文章中，作者研究了九项 NLP 任务，包括情感分析。他们报告称，基于 transformer
    的模型，特别是 XLM-RoBERTa-large，比其他机器学习技术，如 LSTM、BERT 和 CNN，更适合孟加拉文本分类问题。
- en: 'Our Study:'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的研究：
- en: We developed a largest MUltiplatform BAngla SEntiment (MUBASE) social media
    dataset, consisting of Facebook posts and tweets. Following the recommendations
    outlined in Alam et al. ([2021a](#bib.bib2)), we ensured that the dataset is clean,
    free of duplicates, and possesses high annotation quality with an annotation agreement
    score of 0.84\. We have made the dataset publicly available for the community.
    We conducted experiments that goes beyond traditional approaches and smaller transformer-based
    models. Specifically, we investigated the effectiveness of advanced models such
    as Flan-T5, GPT-4 and Bloomz in both zero- and few-shot settings.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个最大的 MUltiplatform BAngla SEntiment (MUBASE) 社交媒体数据集，包括 Facebook 帖子和推文。根据
    Alam et al. ([2021a](#bib.bib2)) 的建议，我们确保数据集干净、没有重复，并且具有高标注质量，标注一致性评分为 0.84。我们已将数据集公开提供给社区。我们进行了超越传统方法和较小的变换器模型的实验。具体来说，我们研究了先进模型如
    Flan-T5、GPT-4 和 Bloomz 在零样本和少样本设置中的有效性。
- en: Dataset
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: Data Collection
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'We collected tweets and comments from both Facebook posts and Twitter. To collect
    tweets, we focused on user accounts associated with the following news media sources:
    BBC Bangla, Prothom Alo, and BD24Live. For the comments from the Facebook posts,
    we selected public pages belonging to several news media outlets. Our selection
    of news media was based on the availability of a substantial number of comments.
    In total, we collected approximately 35,000 posts/comments associated with various
    Bangla news portals. Then we removed all the posts, which contains only emojis
    and URLs as well as duplicate data and filtered tweets while collecting through
    API. We also removed all the Banglish comments from our initial dataset. These
    filtering and duplicate-removal steps resulted in $33,606$ entries. In the rest
    of the paper, we will use the term post to refer to post and comments.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Facebook 帖子和 Twitter 收集了推文和评论。为了收集推文，我们重点关注了与以下新闻媒体来源相关的用户账户：BBC Bangla、Prothom
    Alo 和 BD24Live。对于 Facebook 帖子的评论，我们选择了属于几个新闻媒体机构的公共页面。我们的新闻媒体选择基于评论数量的充足性。总的来说，我们收集了大约
    35,000 条与各种 Bangla 新闻门户相关的帖子/评论。然后我们移除了所有仅包含表情符号和网址的帖子以及重复的数据，并在通过 API 收集推文时进行了筛选。我们还从初始数据集中移除了所有
    Banglish 评论。这些筛选和去重步骤的结果是 $33,606$ 条记录。在本文的其余部分，我们将使用“帖子”一词来指代帖子和评论。
- en: 'Table [1](#Sx3.T1 "Table 1 ‣ Data Collection ‣ Dataset ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis") presents the distribution of the number of posts and comments associated
    with each social media source. Our preliminary study reveals that Twitter users
    post both positive and negative sentiments, while showing fewer neutral expressions.
    On the other hand, Facebook users post more negative sentiments. Overall, the
    distribution of posts with negative sentiment is higher in the dataset. We further
    analyzed the distribution of sentences by the number of words associated with
    each class label, as shown in Figure [2](#Sx3.F2 "Figure 2 ‣ Data Collection ‣
    Dataset ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"). We created different ranges of sentence
    length buckets in order to understand and define the sequence length while training
    the transformer based models. It appears that more than 80% of the posts lie within
    twenty words, which is expected with social media posts, as observed in previous
    studies (Alam et al. [2021b](#bib.bib3)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [1](#Sx3.T1 "Table 1 ‣ Data Collection ‣ Dataset ‣ Zero- and Few-Shot Prompting
    with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis")
    展示了与每个社交媒体来源相关的帖子和评论的数量分布。我们的初步研究表明，Twitter 用户发布了积极和消极的情感，但中性表达较少。另一方面，Facebook
    用户发布了更多的消极情感。总体而言，数据集中具有消极情感的帖子分布较高。我们进一步分析了按类别标签的单词数量分布，如图 [2](#Sx3.F2 "Figure
    2 ‣ Data Collection ‣ Dataset ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative
    Study with Fine-tuned Models for Bangla Sentiment Analysis") 所示。我们创建了不同范围的句子长度区间，以便在训练基于变换器的模型时理解和定义序列长度。结果显示，超过
    80% 的帖子在二十个单词以内，这与以往研究（Alam et al. [2021b](#bib.bib3)）观察到的社交媒体帖子情况相符。'
- en: '| Class | Facebook | Twitter | Total |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | Facebook | Twitter | 总计 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Positive | 2,245 | 8,315 | 10,560 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 2,245 | 8,315 | 10,560 |'
- en: '| Neutral | 4,866 | 1,331 | 6,197 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 中性 | 4,866 | 1,331 | 6,197 |'
- en: '| Negative | 9,078 | 7,771 | 16,849 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 消极 | 9,078 | 7,771 | 16,849 |'
- en: '| Total | 16,189 | 17,417 | 33,606 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 16,189 | 17,417 | 33,606 |'
- en: 'Table 1: Class label distribution across different sources of the dataset.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：数据集不同来源的类别标签分布。
- en: '![Refer to caption](img/fb270332b82a20638b5fda123d82b290.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fb270332b82a20638b5fda123d82b290.png)'
- en: 'Figure 2: The distribution sentence length (number of words) associated with
    each sentiment label.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：与每种情感标签相关联的句子长度（词数）分布。
- en: Annotation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: 'To perform the annotation, we developed an annotation guideline based on previous
    studies (Mukta et al. [2021](#bib.bib36)). For the Bangla sentiment polarity annotation,
    Mukta et al. ([2021](#bib.bib36)) proposed a classification with five labels:
    strongly negative, weakly negative, neutral, strongly positive, and weakly positive.
    However, due to the difficulty in distinguishing between strong and weak labels,
    we opted for a simplified approach with three labels: negative, neutral, and positive.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行标注，我们基于以前的研究（Mukta et al. [2021](#bib.bib36)）制定了标注指南。在孟加拉情感极性标注中，Mukta et
    al. ([2021](#bib.bib36)) 提出了五个标签的分类：强烈负面、弱负面、中立、强烈正面和弱正面。然而，由于区分强标签和弱标签的难度，我们选择了三种标签的简化方法：负面、中立和正面。
- en: Each post was independently annotated by three annotators, all of whom are native
    speakers of Bangla. The annotators consisted of both male and female undergraduate
    students studying computer science. The final label for each post was determined
    based on the majority agreement among the annotators. However, in cases where
    there was disagreement among the annotators, a consensus meeting was organized
    to resolve any discrepancies and reach a final decision. Note that annotators
    are also the authors of the paper, hence, there has not been any payment for the
    annotation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每条帖子由三名注释员独立标注，这些注释员都是孟加拉语的母语者。注释员包括男性和女性计算机科学本科生。每条帖子的最终标签是根据注释员之间的多数意见确定的。然而，在注释员之间存在分歧的情况下，会组织一次共识会议以解决任何不一致并达成最终决定。请注意，注释员也是论文的作者，因此标注没有获得任何报酬。
- en: Inter-Annotation Agreement
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释一致性
- en: The quality of the annotations was assessed by calculating the inter-annotator
    agreement. As mentioned previously, three annotators independently annotated each
    post, adhering to the provided annotation instructions. We calculated the Fleiss
    Kappa ($\kappa$, indicating a perfect agreement among the annotators.³³3Note that
    values of Kappa of and 0.81–1.0 correspond perfect agreement as reported in (Landis
    and Koch [1977](#bib.bib32)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注释质量通过计算注释员之间的一致性来评估。如前所述，三名注释员独立标注了每条帖子，遵循提供的标注说明。我们计算了Fleiss Kappa（$\kappa$），它表示注释员之间的完美一致性。³³3请注意，Kappa值为0.81–1.0表示完美一致性，如(Landis
    and Koch [1977](#bib.bib32))中报告。
- en: Data Split
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据划分
- en: 'For our experiments, we divided the dataset into training, development, and
    test sets, comprising 70%, 10%, and 20% of the data, respectively. To ensure a
    balanced class label distribution across the sets, we employed stratified sampling
    (Sechidis, Tsoumakas, and Vlahavas [2011](#bib.bib45)). The distribution of the
    data split is provided in Table [2](#Sx3.T2 "Table 2 ‣ Data Split ‣ Dataset ‣
    Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models
    for Bangla Sentiment Analysis").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将数据集分为训练集、开发集和测试集，分别占数据的70%、10%和20%。为了确保各集合之间标签分布的平衡，我们采用了分层抽样（Sechidis,
    Tsoumakas, and Vlahavas [2011](#bib.bib45)）。数据划分的分布如表[2](#Sx3.T2 "表2 ‣ 数据划分 ‣
    数据集 ‣ 使用LLMs的零样本和少样本提示：与微调模型进行孟加拉情感分析的比较研究")所示。
- en: '| Class | Train | Dev | Test | Total |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 训练集 | 开发集 | 测试集 | 总计 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Positive | 7,342 | 1,126 | 2,092 | 10,560 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 7,342 | 1,126 | 2,092 | 10,560 |'
- en: '| Neutral | 4,319 | 601 | 1,277 | 6,197 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 中立 | 4,319 | 601 | 1,277 | 6,197 |'
- en: '| Negative | 11,811 | 1,700 | 3,338 | 16,849 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 负面 | 11,811 | 1,700 | 3,338 | 16,849 |'
- en: '| Total | 23,472 | 3,427 | 6,707 | 33,606 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 23,472 | 3,427 | 6,707 | 33,606 |'
- en: 'Table 2: Class label distribution of the dataset.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：数据集的类别标签分布。
- en: Methodology
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: Data Pre-processing
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: The content shared on social media is mostly noisy and includes emoticons, usernames,
    hashtags, URLs, invisible characters, and symbols. To clean the data, we removed
    the noisy portion (emoticons, usernames, hashtags, URLs, invisible characters,
    etc.) of the data. Then we applied tokenization and removed the stopwords from
    the data. Identifying usernames in Facebook posts is more challenging than in
    tweets. While tweets precede usernames with an ‘@’ symbol, Facebook posts have
    no such distinguishing pattern. To address this, we removed English text from
    Facebook posts since most usernames are in English. However, for usernames in
    Bangla text, removal was challenging due to the absence of a consistent pattern
    or a comprehensive Bangla name dictionary.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体上分享的内容大多是噪音，包含表情符号、用户名、标签、网址、不可见字符和符号。为了清理数据，我们移除了数据中的噪音部分（表情符号、用户名、标签、网址、不可见字符等）。然后，我们进行了分词处理，并去除了数据中的停用词。在Facebook帖子中识别用户名比在推文中更具挑战性。虽然推文中的用户名前面有‘@’符号，Facebook帖子则没有这样的区分模式。为了解决这个问题，我们从Facebook帖子中删除了英文文本，因为大多数用户名都是英文的。然而，对于孟加拉文中的用户名，由于缺乏一致的模式或完整的孟加拉姓名词典，删除是具有挑战性的。
- en: Evaluation Measures
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: For the performance measure for all different experimental settings we compute
    accuracy, and weighted preision, recall and F[1] score. We choose to use weighted
    version of the metric as it takes into account class imbalance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有不同实验设置的性能衡量，我们计算了准确率、加权精确度、召回率和F[1]分数。我们选择使用加权版本的指标，因为它考虑了类别不平衡问题。
- en: Training and Evaluation Setup
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练与评估设置
- en: For all experiments, except for LLMs (as detailed below), we trained the models
    using the training set, fine-tuned the parameters with the development set, and
    assessed the model’s performance on the test set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了LLMs（详见下文）外，在所有实验中，我们使用训练集训练模型，用开发集微调参数，并在测试集上评估模型的表现。
- en: Models
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: We conducted our experiments using classical models, small and large language
    models. Note that we use the definition of small and large models discussed in
    (Zhao et al. [2023](#bib.bib51)). LLMs is used to refer to the models containing
    tens or hundreds of billions of parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用经典模型、小型和大型语言模型进行了实验。注意，我们使用了（Zhao et al. [2023](#bib.bib51)）中讨论的小型和大型模型的定义。LLMs用于指代包含数十亿或数百亿参数的模型。
- en: We conducted our experiments using classical models as well as both small and
    large language models. It is worth noting that we follow the definitions of ‘small’
    and ‘large’ models discussed in (Zhao et al. [2023](#bib.bib51)). The term ‘LLMs’
    refers to models encompassing tens or hundreds of billions of parameters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了使用经典模型以及小型和大型语言模型的实验。值得注意的是，我们遵循了（Zhao et al. [2023](#bib.bib51)）中讨论的“小型”和“大型”模型的定义。‘LLMs’一词指的是包含数十亿或数百亿参数的模型。
- en: Baseline
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准
- en: As baselines, we used both a majority (i.e., the class with the highest frequency)
    and a random approach. These methods have been widely used as baseline techniques
    in numerous studies, for example, (Rosenthal, Farra, and Nakov [2019](#bib.bib43)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，我们使用了多数类（即，频率最高的类别）和随机方法。这些方法在许多研究中被广泛用作基准技术，例如（Rosenthal, Farra, 和 Nakov
    [2019](#bib.bib43)）。
- en: Classical Models
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经典模型
- en: While classical models such as SVM (Platt [1998](#bib.bib39)) and Random Forest (Breiman
    [2001](#bib.bib11)) have been widely used in prior studies and remain in use in
    many low-resource production settings, we also wanted to assess their performance.
    To prepare the data for these models, we transformed the text into a tf-idf representation.
    During our experiments with SVM and RF, we used standard parameter settings.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经典模型如SVM（Platt [1998](#bib.bib39)）和随机森林（Breiman [2001](#bib.bib11)）在之前的研究中被广泛使用，并且在许多低资源生产环境中仍在使用，我们也希望评估它们的性能。为了准备这些模型的数据，我们将文本转换为tf-idf表示。在使用SVM和RF的实验中，我们使用了标准的参数设置。
- en: Small Language Models (SLMs)
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小型语言模型（SLMs）
- en: 'Large-scale pre-trained transformer models (PLMs) have achieved state-of-the-art
    performance across numerous NLP tasks. In our study, we fine-tuned several of
    these models. These included the monolingual transformer model BanglaBERT (Bhattacharjee
    et al. [2022](#bib.bib10)) and multilingual transformers such as multilingual
    BERT (mBERT) (Devlin et al. [2019](#bib.bib22)), XLM-RoBERTa (XLM-r) (Conneau
    et al. [2020](#bib.bib19)), Bloomz (560m and 1.7B parameters models) (Muennighoff
    et al. [2022](#bib.bib34)). We used the Transformer toolkit (Wolf et al. [2020](#bib.bib48))
    for the experiment. Following the guidelines outlined in (Devlin et al. [2019](#bib.bib22)),
    we fine-tuned each model using the default settings over three epochs. Due to
    instability, we performed ten reruns for each experiment using different random
    seeds, and we picked the model that performed best on the development set. We
    provided the details of the parameters settings in Appendix [A](#A1 "Appendix
    A Details of the experiments ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative
    Study with Fine-tuned Models for Bangla Sentiment Analysis").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练的变换器模型（PLMs）在众多 NLP 任务中实现了最先进的性能。在我们的研究中，我们微调了几个这样的模型。这些模型包括单语变换器模型 BanglaBERT（Bhattacharjee
    等人 [2022](#bib.bib10)）和多语言变换器，例如多语言 BERT（mBERT）（Devlin 等人 [2019](#bib.bib22)）、XLM-RoBERTa（XLM-r）（Conneau
    等人 [2020](#bib.bib19)）、Bloomz（560m 和 1.7B 参数模型）（Muennighoff 等人 [2022](#bib.bib34)）。我们使用了
    Transformer 工具包（Wolf 等人 [2020](#bib.bib48)）进行实验。根据（Devlin 等人 [2019](#bib.bib22)）中概述的指南，我们使用默认设置对每个模型进行了三轮微调。由于不稳定性，我们对每个实验进行了十次重复实验，使用不同的随机种子，并选择了在开发集上表现最好的模型。我们在附录
    [A](#A1 "附录 A 实验细节 ‣ 零样本和少样本提示与 LLMs：针对孟加拉情感分析的微调模型的比较研究") 中提供了参数设置的详细信息。
- en: GPT Embedding
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT 嵌入
- en: For many downstram NLP tasks, embedding extracted from pre-trained models followed
    by fine-tuning a feed-forward network provided a reasonable results and also reasonable
    setup for a low-resource production setting. Hence, we wanted to see the performance
    of this setting. We first extract the embeddings using OpenAI’s text-embedding-ada-002
    model for each data split. We then fine-tune a feed-forward network on the embeddings
    extracted from the training set to train our model. Our feed-forward model utilizes
    the Rectified Linear Unit (ReLU) activation function. We have set the learning
    rate to 0.001 and the hidden layer size to 500\. We validate our model using the
    validation set and finally, we evaluate the model using the test set.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多下游 NLP 任务，从预训练模型中提取的嵌入，再通过微调一个前馈网络，提供了合理的结果，并且对于低资源生产环境来说也是合理的设置。因此，我们想看看这种设置的表现。我们首先使用
    OpenAI 的 text-embedding-ada-002 模型为每个数据拆分提取嵌入。然后，我们在从训练集中提取的嵌入上微调前馈网络以训练我们的模型。我们的前馈模型使用了修正线性单元（ReLU）激活函数。我们将学习率设置为
    0.001，隐藏层大小设置为 500。我们使用验证集来验证模型，最后，我们使用测试集评估模型。
- en: Large Language Models (LLMs)
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）
- en: 'For the LLMs, we investigate their performance with in-context zero- and few-shot
    learning setting without any specific training. It involve prompting and post-processing
    of output to extract the expected content. Therefore, for each task, we experimented
    with a number of prompts, guided by the same instruction and format as recommended
    in the OpenAI Chat playground, and PromptSource (Bach et al. [2022](#bib.bib6)).
    We used following models: Flan-T5 (large and XL) (Chung et al. [2022](#bib.bib18)),
    Bloomz (1.7B, 3B, 7.1B, 176B-8bit) (Muennighoff et al. [2022](#bib.bib34)) and
    GPT-4 (OpenAI [2023](#bib.bib38)). To ensure a deterministic predictions, we set
    the temperatures to zero of all these models.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLMs），我们研究了它们在上下文零样本和少样本学习设置下的表现，而没有任何特定的训练。这涉及到提示和输出的后处理以提取预期内容。因此，对于每个任务，我们使用了多个提示，并遵循了
    OpenAI Chat playground 推荐的相同指令和格式，以及 PromptSource（Bach 等人 [2022](#bib.bib6)）。我们使用了以下模型：Flan-T5（large
    和 XL）（Chung 等人 [2022](#bib.bib18)）、Bloomz（1.7B、3B、7.1B、176B-8bit）（Muennighoff
    等人 [2022](#bib.bib34)）和 GPT-4（OpenAI [2023](#bib.bib38)）。为了确保确定性预测，我们将所有这些模型的温度设置为零。
- en: Prompting Strategy
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示策略
- en: LLMs produces varied responses depending of the prompt design, which is a complex
    and iterative process that present challenges due to the unknown representation
    of information within and different LLMs. The instructions expressed in our prompts
    include both native (Bangla) and English languages with the content in Bangla.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 根据提示设计产生不同的响应，这一过程复杂且迭代，因信息在不同 LLMs 中的未知表示而带来挑战。我们提示中表达的指令包括本地（孟加拉语）和英语，并且内容为孟加拉语。
- en: Zero-shot
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零样本
- en: 'We employ zero-shot prompting, providing natural language instructions that
    describe the task and specify the expected output. This approach enables the LLMs
    to construct a context that refines the inference space, yielding a more accurate
    output. In Figure [3](#Sx4.F3 "Figure 3 ‣ Zero-shot ‣ Prompting Strategy ‣ Methodology
    ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"), we provide an example of a zero-shot prompt,
    emphasizing the instructions and placeholders for both input and label. Given
    that GPT-4 has the capability to play a role, threefore, we also provide a role
    for it as an “expert annotator” Along with the instruction we provide the labels
    to guide the LLMs. Within the instruction, we we provide information of how the
    LLMs should present their output, aiming to eliminate the need for post-processing.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了零-shot 提示方法，提供自然语言指令来描述任务并指定期望的输出。这种方法使得大语言模型（LLMs）能够构建一个上下文，以优化推理空间，从而产生更准确的输出。在图
    [3](#Sx4.F3 "图 3 ‣ Zero-shot ‣ 提示策略 ‣ 方法论 ‣ Zero-和Few-Shot提示与LLMs的比较研究：用于孟加拉情感分析的微调模型")
    中，我们提供了一个零-shot 提示的示例，强调了输入和标签的指令及占位符。考虑到 GPT-4 能够扮演角色，因此，我们还为其提供了一个“专家标注员”的角色。除了指令外，我们还提供了标签来引导LLMs。在指令中，我们提供了
    LLMs 应如何呈现输出的信息，旨在消除后处理的需要。
- en: 'In our initial set of experiments with Bloomz, we observed that it did not
    respond as effectively to the same instructions as GPT-4\. Therefore, we used
    more straightforward instructions for Bloomz, as illustrated in Figure [4](#Sx4.F4
    "Figure 4 ‣ Zero-shot ‣ Prompting Strategy ‣ Methodology ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"). For the other versions of Bloomz and Flan-T5, we used same promopt
    as Bloomz.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们与 Bloomz 的初步实验中，我们观察到其对相同指令的响应效果不如 GPT-4。因此，我们为 Bloomz 使用了更简单的指令，如图 [4](#Sx4.F4
    "图 4 ‣ Zero-shot ‣ 提示策略 ‣ 方法论 ‣ Zero-和Few-Shot提示与LLMs的比较研究：用于孟加拉情感分析的微调模型") 所示。对于
    Bloomz 和 Flan-T5 的其他版本，我们使用了与 Bloomz 相同的提示。
- en: '![Refer to caption](img/4e64812e597d3b7ce92d117bb68fc11a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e64812e597d3b7ce92d117bb68fc11a.png)'
- en: 'Figure 3: Zero-shot prompt example for GPT-4.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: GPT-4 的零-shot 提示示例。'
- en: '![Refer to caption](img/cd04b4e898784e88a2af88a66a0d4e80.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cd04b4e898784e88a2af88a66a0d4e80.png)'
- en: 'Figure 4: Zero-shot prompt example for Bloomz.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: Bloomz 的零-shot 提示示例。'
- en: Few-shot
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Few-shot
- en: The seminal work by Brown et al. ([2020](#bib.bib12)) demonstrated that few-shot
    learning offers superior performance when compared to the zero-shot learning setup.
    This has also proven by numerous benchmarking studies (e.g., (Ahuja et al. [2023](#bib.bib1))).
    In our study, we conducted few-shot experiments using GPT-4 and Bloomz. For few-shot
    learning, we selected examples from the available training data. We used maximal
    marginal relevance-based (MMR) selection to construct example sets that are deemed
    relevant and diverse (Carbonell and Goldstein [1998](#bib.bib14)). This approach
    has been demonstrated as a successful method for selecting few-shot examples by
    Ye et al. ([2022](#bib.bib49)). The MMR technique calculates the similarity between
    a test example and the training dataset, subsequently selecting $m$ setting will
    be explored in out future study. Note that our experiments of few-shot with Bloomz
    was worse than zero-shot, which might require further investigation. Therefore,
    in this study, we do not further discuss the Bloomz experiments with few-shot.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Brown 等人 ([2020](#bib.bib12)) 的开创性研究表明，与零-shot 学习设置相比，few-shot 学习提供了更优越的性能。这也通过众多基准测试研究得到了证明（例如，（Ahuja
    等人 [2023](#bib.bib1)））。在我们的研究中，我们使用 GPT-4 和 Bloomz 进行了 few-shot 实验。对于 few-shot
    学习，我们从现有的训练数据中选择了示例。我们使用基于最大边际相关性的（MMR）选择方法来构建被认为相关且多样的示例集（Carbonell 和 Goldstein
    [1998](#bib.bib14)）。这种方法已被 Ye 等人 ([2022](#bib.bib49)) 证明是一种成功的 few-shot 示例选择方法。MMR
    技术计算测试示例与训练数据集之间的相似性，然后选择 $m$ 设置将在我们的未来研究中探讨。请注意，我们的 Bloomz few-shot 实验结果比零-shot
    更差，这可能需要进一步调查。因此，在本研究中，我们不再讨论 Bloomz 的 few-shot 实验。
- en: 'In Figure [5](#Sx4.F5 "Figure 5 ‣ Few-shot ‣ Prompting Strategy ‣ Methodology
    ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"), we present an example of a few-shot prompt
    for GPT-4\. The few-shot prompt distinguishes itself from the zero-shot in several
    ways: (i) We provided additional information for the role, (ii) We simplified
    the instructions, and (iii) We included $m$-shot examples.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [5](#Sx4.F5 "图 5 ‣ 少样本 ‣ 提示策略 ‣ 方法论 ‣ 零样本和少样本提示与 LLMs: 针对孟加拉情感分析的微调模型比较研究")
    中，我们展示了 GPT-4 的一个少样本提示示例。少样本提示与零样本的不同之处在于： (i) 我们提供了额外的角色信息， (ii) 我们简化了指令， (iii)
    我们包括了 $m$-shot 示例。'
- en: Our choices of prompts was based on our extensive experiments on similar tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的提示基于我们在类似任务上的广泛实验。
- en: '![Refer to caption](img/df0a73a48df7021ca672e4ca0279cf00.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df0a73a48df7021ca672e4ca0279cf00.png)'
- en: 'Figure 5: Few-shot prompt example for GPT-4.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：GPT-4 的少样本提示示例。
- en: Result and Discussion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: '| Exp | Acc | P | R | F1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | 准确率 | 精确率 | 召回率 | F1 |'
- en: '| Baseline |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 基线 |'
- en: '| Random | 33.56 | 38.31 | 33.56 | 33.56 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Random | 33.56 | 38.31 | 33.56 | 33.56 |'
- en: '| Majority | 49.77 | 24.77 | 49.77 | 49.77 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Majority | 49.77 | 24.77 | 49.77 | 49.77 |'
- en: '| Classic Models |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 经典模型 |'
- en: '| SVM | 55.81 | 53.33 | 55.81 | 52.39 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| SVM | 55.81 | 53.33 | 55.81 | 52.39 |'
- en: '| RF | 56.75 | 54.61 | 56.75 | 52.62 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| RF | 56.75 | 54.61 | 56.75 | 52.62 |'
- en: '| Fine-tuning |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 微调 |'
- en: '| Embedding (GPT) | 57.79 | 57.30 | 57.79 | 57.46 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Embedding (GPT) | 57.79 | 57.30 | 57.79 | 57.46 |'
- en: '| Bloomz-560m | 61.71 | 63.08 | 61.97 | 63.08 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-560m | 61.71 | 63.08 | 61.97 | 63.08 |'
- en: '| Bloomz-1.7B | 61.16 | 59.76 | 61.16 | 59.95 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-1.7B | 61.16 | 59.76 | 61.16 | 59.95 |'
- en: '| BERT-m | 64.95 | 64.92 | 64.95 | 64.90 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| BERT-m | 64.95 | 64.92 | 64.95 | 64.90 |'
- en: '| XLM-r (base) | 66.63 | 66.24 | 66.63 | 66.28 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| XLM-r (base) | 66.63 | 66.24 | 66.63 | 66.28 |'
- en: '| XLM-r (large) | 66.33 | 65.63 | 66.33 | 65.79 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| XLM-r (large) | 66.33 | 65.63 | 66.33 | 65.79 |'
- en: '| BanglaBERT | 69.08 | 67.61 | 69.08 | 67.98 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| BanglaBERT | 69.08 | 67.61 | 69.08 | 67.98 |'
- en: '| BanglaBERT* | 70.33 | 69.13 | 70.33 | 69.39 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| BanglaBERT* | 70.33 | 69.13 | 70.33 | 69.39 |'
- en: '| Zero- and Few-shot on LLMs |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLMs 上的零样本和少样本 |'
- en: '| Open Models - 0-shot |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 开放模型 - 0-shot |'
- en: '| Flan-T5 (large) | 41.28 | 20.23 | 13.77 | 20.23 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5 (large) | 41.28 | 20.23 | 13.77 | 20.23 |'
- en: '| Flan-T5 (xl) | 49.42 | 29.46 | 18.18 | 29.46 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5 (xl) | 49.42 | 29.46 | 18.18 | 29.46 |'
- en: '| Bloomz-1.7B | 58.33 | 49.38 | 58.33 | 50.38 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-1.7B | 58.33 | 49.38 | 58.33 | 50.38 |'
- en: '| Bloomz-3B | 59.73 | 50.98 | 59.73 | 51.53 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-3B | 59.73 | 50.98 | 59.73 | 51.53 |'
- en: '| Bloomz-7.1B | 62.83 | 50.92 | 62.83 | 56.24 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz-7.1B | 62.83 | 50.92 | 62.83 | 56.24 |'
- en: '| Bloomz 176B (8bit) | 61.84 | 51.16 | 61.84 | 55.54 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz 176B (8bit) | 61.84 | 51.16 | 61.84 | 55.54 |'
- en: '| Bloomz Majority | 61.97 | 51.32 | 61.97 | 61.97 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Bloomz Majority | 61.97 | 51.32 | 61.97 | 61.97 |'
- en: '| Closed Models - $m$-shot |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 封闭模型 - $m$-shot |'
- en: '| GPT-4: 0-Shot | 60.21 | 61.65 | 60.21 | 59.99 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4: 0-Shot | 60.21 | 61.65 | 60.21 | 59.99 |'
- en: '| GPT-4: 0-Shot (BN inst.) | 60.70 | 61.71 | 60.70 | 59.96 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4: 0-Shot (BN inst.) | 60.70 | 61.71 | 60.70 | 59.96 |'
- en: '| GPT-4: 3-Shot | 59.14 | 64.80 | 59.14 | 59.68 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4: 3-Shot | 59.14 | 64.80 | 59.14 | 59.68 |'
- en: '| GPT-4: 5-Shot | 59.25 | 63.94 | 59.25 | 59.67 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4: 5-Shot | 59.25 | 63.94 | 59.25 | 59.67 |'
- en: '| GPT-4 Majority | 59.74 | 63.26 | 59.74 | 59.74 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Majority | 59.74 | 63.26 | 59.74 | 59.74 |'
- en: 'Table 3: Performance of different sets of experiments. * indicates trained
    on combined MUBASE, SentiNoB(Islam et al. [2021](#bib.bib27)), and Alam et al.
    ([2021a](#bib.bib2)). BN Ins. refers that instruction is provided in native Bangla
    language.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同实验组的性能。* 表示训练时使用了合并的 MUBASE、SentiNoB（Islam et al. [2021](#bib.bib27)）和
    Alam et al. ([2021a](#bib.bib2)) 数据集。BN Ins. 表示指令以本地孟加拉语提供。
- en: 'In Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"), we reported the results of our experiments.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [3](#Sx5.T3 "表 3 ‣ 结果与讨论 ‣ 零样本和少样本提示与 LLMs: 针对孟加拉情感分析的微调模型比较研究") 中，我们报告了我们的实验结果。'
- en: 'Comparison with Baselines:'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与基线的比较：
- en: All experimental setup outperformed random and majority baselines except Flan-T5.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Flan-T5 外，所有实验设置均优于随机和多数基线。
- en: Performance of Classic Models
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典模型的性能
- en: The performance of the SVM and Random Forest better than baseline, however,
    worse than other except Flan-T5.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 和随机森林的性能优于基线，但除了 Flan-T5 外不如其他模型。
- en: Fine-tuning
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: Fine-tuned models consistently outperform across various settings. Results using
    GPT embeddings are superior to classical models, though not as effective as some
    other approaches. Although multilingual models such as BERT-m, XLM-r, and Bloomz
    show promising direction, however, models trained on monolingual text ultimately
    achieve superior performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型在各种设置中始终表现优越。使用GPT嵌入的结果优于经典模型，但不如其他一些方法有效。尽管诸如BERT-m、XLM-r和Bloomz等多语言模型显示了有前途的方向，但最终在单语文本上训练的模型表现更为卓越。
- en: Given the superior performance of monolingual models across various settings,
    we chose to augment our training data. By integrating the SentiNoB training set
    with the MUBASE training set and fine-tuning with BanglaBERT, we managed to boost
    performance by an additional 1.41% of F1.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于单语模型在各种设置中的优越表现，我们选择增加我们的训练数据。通过将SentiNoB训练集与MUBASE训练集整合，并用BanglaBERT进行微调，我们成功提高了额外1.41%的F1性能。
- en: When comparing the smaller Bloomz model (560m) to the larger one (1.7B), the
    smaller model performs better. This suggests that more training data might be
    required to effectively train such a large model. A similar pattern is observed
    with the XLM-r model when comparing its base and large versions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较较小的Bloomz模型（560m）和较大的模型（1.7B）时，较小的模型表现更好。这表明可能需要更多的训练数据来有效地训练这样的大型模型。类似的模式也出现在XLM-r模型的基础版和大型版之间的比较中。
- en: Zero- and Few-shot Prompt-Based Results
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本和少样本提示结果
- en: 'Bloomz:'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Bloomz:'
- en: 'As can be seen in Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero-
    and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for
    Bangla Sentiment Analysis"), the performance of zero- and few-shot approaches
    is promising, though there is a significant difference compared to the best monolingual
    fine-tuned transformer-based model. When comparing different parameter sizes of
    Bloomz, we observe that performance increases from 1.7B to 7.1B. However, we see
    a lower performance with Bloomz 176B compared to 7.1B, which might be due to the
    8-bit precision.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero- and Few-Shot Prompting
    with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis")所示，零样本和少样本方法的表现是有前景的，但与最佳的单语微调变换器模型相比存在显著差异。当比较不同参数大小的Bloomz时，我们观察到从1.7B到7.1B的性能有所提升。然而，我们发现Bloomz
    176B的性能低于7.1B，这可能与8位精度有关。'
- en: 'Ensemble:'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成：
- en: We hypothesized that predictions from different models might vary, and an ensemble
    of their outputs might provide better results. Therefore, we opted to use a majority-based
    ensemble method, resulting in a 5.73% improvement in weighted F1.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设不同模型的预测可能会有所不同，多个模型的输出集成可能会提供更好的结果。因此，我们选择使用基于多数投票的集成方法，结果在加权F1上提高了5.73%。
- en: 'GPT4:'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'GPT4:'
- en: 'The performance of GPT-4 is higher than that of other LLMs. Our experiments
    with different types of prompting did not yield a clear improvement, as can be
    seen in Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"). While prior studies on other tasks and languages showed a clear performance
    gain with a few-shot setup, in our study, we did not find such a gain, only slight
    differences in precision. Therefore, our future studies will include further investigation
    of few-shot learning setups.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-4的表现优于其他大型语言模型。我们在不同类型提示的实验中没有明显的改进，如表[3](#Sx5.T3 "Table 3 ‣ Result and
    Discussion ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with
    Fine-tuned Models for Bangla Sentiment Analysis")所示。虽然以往在其他任务和语言上的研究显示出少量样本设置有明显的性能提升，但在我们的研究中，我们没有发现这样的提升，只有精度的轻微差异。因此，我们未来的研究将进一步探讨少量样本学习设置。'
- en: Our experiments revealed that native language instructions achieved performance
    comparable to that of English instructions. This indicates the potential for using
    native language prompts for Bangla sentiment analysys.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验揭示，使用母语指令的性能与英语指令相当。这表明使用母语提示进行孟加拉语情感分析是有潜力的。
- en: While the ensemble of different Bloomz settings improved performance, it did
    not help for GPT-4.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不同Bloomz设置的集成提高了性能，但对GPT-4并没有帮助。
- en: 'Error Analysis on the Output of Prompts:'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示输出的误差分析：
- en: Further analysis the results of the LLMs outputs we observed that (i) Flan-T5
    (xl) labeled only 5 posts as negative, and Flan-T5 (large) labeled only 45 posts
    as negative, (ii) Bloomz completely failed to label posts as neutral, and (i)
    GPT-4 struggled to predict positive class.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析 LLMs 输出的结果时，我们观察到 (i) Flan-T5 (xl) 仅将 5 个帖子标记为负面，Flan-T5 (large) 仅将 45
    个帖子标记为负面，(ii) Bloomz 完全未能将帖子标记为中立，(i) GPT-4 难以预测正面类别。
- en: Conclusion
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this study, we present our evaluation of LLMs using zero and few-shot prompting.
    We offer a detailed comparison with fine-tuned models. Our experiments were conducted
    on a newly developed dataset named ”MUBASE”, for which we provide an in-depth
    analysis. Our results indicate that while LLMs represent a promising research
    direction, the smaller versions of fine-tuned pre-trained models outperform them.
    The performance of LLMs suggests that sentiment analysis in a new domain is feasible
    with reasonable accuracy without the need to develop a new dataset or train a
    new model. Future research directions include using other recently released datasets
    and providing a comparative analysis with LLMs. Additionally, further study on
    few-shot learning represents another promising avenue.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们展示了对 LLMs 使用零样本和少样本提示的评估。我们提供了与微调模型的详细比较。我们的实验是在一个新开发的数据集 ”MUBASE” 上进行的，我们提供了深入分析。我们的结果表明，尽管
    LLMs 代表了一个有前景的研究方向，但较小的微调预训练模型在性能上超越了它们。LLMs 的性能表明，在新领域中进行情感分析是可行的，具有合理的准确性，而无需开发新数据集或训练新模型。未来的研究方向包括使用其他最近发布的数据集，并提供与
    LLMs 的比较分析。此外，对少样本学习的进一步研究代表了另一条有前景的途径。
- en: References
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahuja et al. (2023) Ahuja, K.; Hada, R.; Ochieng, M.; Jain, P.; Diddee, H.;
    Maina, S.; Ganu, T.; Segal, S.; Axmed, M.; Bali, K.; et al. 2023. MEGA: Multilingual
    Evaluation of Generative AI. *arXiv preprint arXiv:2303.12528*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahuja 等 (2023) Ahuja, K.; Hada, R.; Ochieng, M.; Jain, P.; Diddee, H.; Maina,
    S.; Ganu, T.; Segal, S.; Axmed, M.; Bali, K.; 等. 2023. MEGA: 多语言生成 AI 评估。 *arXiv
    preprint arXiv:2303.12528*.'
- en: Alam et al. (2021a) Alam, F.; Hasan, M. A.; Alam, T.; Khan, A.; Tajrin, J.;
    Khan, N.; and Chowdhury, S. A. 2021a. A review of bangla natural language processing
    tasks and the utility of transformer models. *arXiv preprint arXiv:2107.03844*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alam 等 (2021a) Alam, F.; Hasan, M. A.; Alam, T.; Khan, A.; Tajrin, J.; Khan,
    N.; 和 Chowdhury, S. A. 2021a. 孟加拉自然语言处理任务和变换器模型的实用性综述。 *arXiv preprint arXiv:2107.03844*.
- en: 'Alam et al. (2021b) Alam, F.; Qazi, U.; Imran, M.; and Ofli, F. 2021b. Humaid:
    Human-annotated disaster incidents data from twitter with deep learning benchmarks.
    In *Proceedings of the International AAAI Conference on Web and social media*,
    volume 15, 933–942.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alam 等 (2021b) Alam, F.; Qazi, U.; Imran, M.; 和 Ofli, F. 2021b. Humaid: 带有深度学习基准的
    Twitter 人工标注灾难事件数据。在 *Proceedings of the International AAAI Conference on Web
    and Social Media*, volume 15, 933–942.'
- en: Ashik, Shovon, and Haque (2019) Ashik, M. A.-U.-Z.; Shovon, S.; and Haque, S.
    2019. Data Set For Sentiment Analysis On Bengali News Comments And Its Baseline
    Evaluation. In *Proc. of ICBSLP*, 1–5\. IEEE.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashik, Shovon, 和 Haque (2019) Ashik, M. A.-U.-Z.; Shovon, S.; 和 Haque, S. 2019.
    用于孟加拉新闻评论的情感分析数据集及其基线评估。在 *Proc. of ICBSLP*, 1–5\. IEEE.
- en: Aziz Sharfuddin, Nafis Tihami, and Saiful Islam (2018) Aziz Sharfuddin, A.;
    Nafis Tihami, M.; and Saiful Islam, M. 2018. A Deep Recurrent Neural Network with
    BiLSTM model for Sentiment Classification. In *2018 International Conference on
    Bangla Speech and Language Processing (ICBSLP)*, 1–4\. IEEE.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aziz Sharfuddin, Nafis Tihami 和 Saiful Islam (2018) Aziz Sharfuddin, A.; Nafis
    Tihami, M.; 和 Saiful Islam, M. 2018. 一种深度递归神经网络与 BiLSTM 模型用于情感分类。在 *2018 International
    Conference on Bangla Speech and Language Processing (ICBSLP)*, 1–4\. IEEE.
- en: 'Bach et al. (2022) Bach, S.; Sanh, V.; Yong, Z. X.; Webson, A.; Raffel, C.;
    Nayak, N. V.; Sharma, A.; Kim, T.; Bari, M. S.; Févry, T.; et al. 2022. PromptSource:
    An Integrated Development Environment and Repository for Natural Language Prompts.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, 93–104.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bach 等 (2022) Bach, S.; Sanh, V.; Yong, Z. X.; Webson, A.; Raffel, C.; Nayak,
    N. V.; Sharma, A.; Kim, T.; Bari, M. S.; Févry, T.; 等. 2022. PromptSource: 一个集成的开发环境和自然语言提示库。在
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:
    System Demonstrations*, 93–104.'
- en: Banik and Rahman (2018) Banik, N.; and Rahman, M. H. H. 2018. Evaluation of
    Naïve Bayes and Support Vector Machines on Bangla Textual Movie Reviews. In *Proc.
    of ICBSLP*, 1–6\. IEEE.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banik 和 Rahman (2018) Banik, N.; 和 Rahman, M. H. H. 2018. 对孟加拉文本电影评论中朴素贝叶斯和支持向量机的评估。在
    *Proc. of ICBSLP*, 1–6\. IEEE.
- en: 'Barbieri, Espinosa Anke, and Camacho-Collados (2022) Barbieri, F.; Espinosa Anke,
    L.; and Camacho-Collados, J. 2022. XLM-T: Multilingual Language Models in Twitter
    for Sentiment Analysis and Beyond. In *Proceedings of the Thirteenth Language
    Resources and Evaluation Conference*, 258–266\. Marseille, France: European Language
    Resources Association.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barbieri, Espinosa Anke 和 Camacho-Collados (2022) Barbieri, F.; Espinosa Anke,
    L.; 和 Camacho-Collados, J. 2022. XLM-T：Twitter中的多语言模型用于情感分析及其他应用。在 *第十三届语言资源与评估会议论文集*，258–266。法国，马赛：欧洲语言资源协会。
- en: 'Batanović, Nikolić, and Milosavljević (2016) Batanović, V.; Nikolić, B.; and
    Milosavljević, M. 2016. Reliable baselines for sentiment analysis in resource-limited
    languages: The serbian movie review dataset. In *Proceedings of the Tenth International
    Conference on Language Resources and Evaluation (LREC’16)*, 2688–2696.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batanović, Nikolić 和 Milosavljević (2016) Batanović, V.; Nikolić, B.; 和 Milosavljević,
    M. 2016. 资源有限语言中情感分析的可靠基线：塞尔维亚电影评论数据集。在 *第十届语言资源与评估国际会议（LREC’16）论文集*，2688–2696。
- en: 'Bhattacharjee et al. (2022) Bhattacharjee, A.; Hasan, T.; Ahmad, W.; Mubasshir,
    K. S.; Islam, M. S.; Iqbal, A.; Rahman, M. S.; and Shahriyar, R. 2022. BanglaBERT:
    Language Model Pretraining and Benchmarks for Low-Resource Language Understanding
    Evaluation in Bangla. In *Findings of the Association for Computational Linguistics:
    NAACL 2022*, 1318–1327\. Seattle, United States: Association for Computational
    Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhattacharjee 等人 (2022) Bhattacharjee, A.; Hasan, T.; Ahmad, W.; Mubasshir,
    K. S.; Islam, M. S.; Iqbal, A.; Rahman, M. S.; 和 Shahriyar, R. 2022. BanglaBERT：用于孟加拉语低资源语言理解评估的语言模型预训练和基准测试。在
    *计算语言学协会年会论文集：NAACL 2022*，1318–1327。美国，华盛顿：计算语言学协会。
- en: 'Breiman (2001) Breiman, L. 2001. Random forests. *Machine learning*, 45(1):
    5–32.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breiman (2001) Breiman, L. 2001. 随机森林。*机器学习*，45(1): 5–32。'
- en: Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. *Advances in Neural
    Information Processing Systems*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;
    Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.;
    Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.
    M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;
    Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.;
    和 Amodei, D. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*。
- en: 'Cambria et al. (2022) Cambria, E.; Liu, Q.; Decherchi, S.; Xing, F.; and Kwok,
    K. 2022. SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable
    sentiment analysis. In *Proceedings of the Thirteenth Language Resources and Evaluation
    Conference*, 3829–3839.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cambria 等人 (2022) Cambria, E.; Liu, Q.; Decherchi, S.; Xing, F.; 和 Kwok, K.
    2022. SenticNet 7：一个基于常识的神经符号AI框架，用于可解释的情感分析。在 *第十三届语言资源与评估会议论文集*，3829–3839。
- en: Carbonell and Goldstein (1998) Carbonell, J.; and Goldstein, J. 1998. The use
    of MMR, diversity-based reranking for reordering documents and producing summaries.
    In *Proceedings of the 21st annual international ACM SIGIR conference on Research
    and development in information retrieval*, 335–336.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carbonell 和 Goldstein (1998) Carbonell, J.; 和 Goldstein, J. 1998. 使用MMR，多样性基础的重新排序用于重新排序文档和生成摘要。在
    *第21届国际ACM SIGIR信息检索研究与开发年会论文集*，335–336。
- en: 'Chen et al. (2022) Chen, C.; Teng, Z.; Wang, Z.; and Zhang, Y. 2022. Discrete
    opinion tree induction for aspect-based sentiment analysis. In *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2051–2064.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2022) Chen, C.; Teng, Z.; Wang, Z.; 和 Zhang, Y. 2022. 基于方面的情感分析的离散意见树诱导。在
    *第60届计算语言学协会年会（第1卷：长篇论文）*，2051–2064。
- en: Chowdhury et al. (2019) Chowdhury, R. R.; Hossain, M. S.; Hossain, S.; and Andersson,
    K. 2019. Analyzing sentiment of movie reviews in Bangla by applying machine learning
    techniques. In *Proc. of (ICBSLP)*, 1–6\. IEEE.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury 等人 (2019) Chowdhury, R. R.; Hossain, M. S.; Hossain, S.; 和 Andersson,
    K. 2019. 通过应用机器学习技术分析孟加拉语电影评论的情感。在 *（ICBSLP）会议论文集*，1–6。IEEE。
- en: Chowdhury and Chowdhury (2014) Chowdhury, S.; and Chowdhury, W. 2014. Performing
    sentiment analysis in Bangla microblog posts. In *2014 International Conference
    on Informatics, Electronics Vision (ICIEV)*, 1–6.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury 和 Chowdhury (2014) Chowdhury, S.; 和 Chowdhury, W. 2014. 在孟加拉微型博客帖子中进行情感分析。在
    *2014年信息学、电子学视觉国际会议（ICIEV）*，1–6。
- en: Chung et al. (2022) Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus,
    W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等（2022）Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.;
    Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; 等. 2022. 扩展指令微调语言模型。*arXiv预印本 arXiv:2210.11416*。
- en: 'Conneau et al. (2020) Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.;
    Wenzek, G.; Guzmán, F.; Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V.
    2020. Unsupervised Cross-lingual Representation Learning at Scale. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    ACL ’20, 8440–8451\. Online: Association for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau等（2020）Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek,
    G.; Guzmán, F.; Grave, E.; Ott, M.; Zettlemoyer, L.; 和 Stoyanov, V. 2020. 大规模无监督跨语言表示学习。发表于*第58届计算语言学协会年会论文集*，ACL
    ’20, 8440–8451. 在线：计算语言学协会。
- en: 'Cui et al. (2023) Cui, J.; Wang, Z.; Ho, S.-B.; and Cambria, E. 2023. Survey
    on sentiment analysis: evolution of research methods and topics. *Artificial Intelligence
    Review*, 1–42.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui等（2023）Cui, J.; Wang, Z.; Ho, S.-B.; 和 Cambria, E. 2023. 情感分析调查：研究方法和主题的演变。*人工智能评论*，1–42。
- en: 'Dashtipour et al. (2016) Dashtipour, K.; Poria, S.; Hussain, A.; Cambria, E.;
    Hawalah, A. Y.; Gelbukh, A.; and Zhou, Q. 2016. Multilingual sentiment analysis:
    state of the art and independent comparison of techniques. *Cognitive computation*,
    8: 757–771.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dashtipour等（2016）Dashtipour, K.; Poria, S.; Hussain, A.; Cambria, E.; Hawalah,
    A. Y.; Gelbukh, A.; 和 Zhou, Q. 2016. 多语言情感分析：最新进展和技术的独立比较。*认知计算*，8: 757–771。'
- en: 'Devlin et al. (2019) Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, NAACL-HLT ’19, 4171–4186\.
    Minneapolis, Minnesota, USA: Association for Computational Linguistics.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin等（2019）Devlin, J.; Chang, M.-W.; Lee, K.; 和 Toutanova, K. 2019. BERT：用于语言理解的深度双向变换器的预训练。发表于*2019年北美计算语言学协会会议：人类语言技术*，NAACL-HLT
    ’19, 4171–4186. 美国明尼苏达州明尼阿波利斯：计算语言学协会。
- en: 'Galeshchuk, Qiu, and Jourdan (2019) Galeshchuk, S.; Qiu, J.; and Jourdan, J.
    2019. Sentiment Analysis for Multilingual Corpora. In *Proceedings of the 7th
    Workshop on Balto-Slavic Natural Language Processing*, 120–125\. Florence, Italy:
    Association for Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galeshchuk、Qiu和Jourdan（2019）Galeshchuk, S.; Qiu, J.; 和 Jourdan, J. 2019. 多语言语料库的情感分析。发表于*第七届巴尔托-斯拉夫自然语言处理研讨会论文集*，120–125.
    意大利佛罗伦萨：计算语言学协会。
- en: 'Hasan et al. (2020) Hasan, M. A.; Tajrin, J.; Chowdhury, S. A.; and Alam, F.
    2020. Sentiment classification in Bangla textual content: a comparative study.
    In *2020 23rd International Conference on Computer and Information Technology
    (ICCIT)*, 1–6\. IEEE.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan等（2020）Hasan, M. A.; Tajrin, J.; Chowdhury, S. A.; 和 Alam, F. 2020. 孟加拉语文本内容的情感分类：一项比较研究。发表于*2020年第23届计算机与信息技术国际会议（ICCIT）*，1–6.
    IEEE。
- en: Hassan et al. (2016) Hassan, A.; Amin, M. R.; Al Azad, A. K.; and Mohammed,
    N. 2016. Sentiment Analysis on Bangla and Romanized Bangla Text using Deep Recurrent
    Models. In *2016 International Workshop on Computational Intelligence (IWCI)*,
    51–56\. IEEE.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassan等（2016）Hassan, A.; Amin, M. R.; Al Azad, A. K.; 和 Mohammed, N. 2016. 使用深度递归模型对孟加拉语和罗马化孟加拉语文本进行情感分析。发表于*2016年国际计算智能研讨会（IWCI）*，51–56.
    IEEE。
- en: 'Hussein (2018) Hussein, D. M. E.-D. M. 2018. A survey on sentiment analysis
    challenges. *Journal of King Saud University-Engineering Sciences*, 30(4): 330–338.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hussein（2018）Hussein, D. M. E.-D. M. 2018. 情感分析挑战的综述。*沙特国王大学工程科学期刊*，30(4):
    330–338。'
- en: 'Islam et al. (2021) Islam, K. I.; Kar, S.; Islam, M. S.; and Amin, M. R. 2021.
    SentNoB: A dataset for analysing sentiment on noisy Bangla texts. In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, 3265–3271.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam等（2021）Islam, K. I.; Kar, S.; Islam, M. S.; 和 Amin, M. R. 2021. SentNoB：一个用于分析噪声孟加拉语文本情感的数据集。发表于*计算语言学协会会议：EMNLP
    2021*，3265–3271。
- en: 'Islam et al. (2023) Islam, M. E.; Chowdhury, L.; Khan, F. A.; Hossain, S.;
    Hossain, S.; Rashid, M. M. O.; Mohammed, N.; and Amin, M. R. 2023. SentiGOLD:
    A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation.
    *arXiv preprint arXiv:2306.06147*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Islam等（2023）Islam, M. E.; Chowdhury, L.; Khan, F. A.; Hossain, S.; Hossain,
    S.; Rashid, M. M. O.; Mohammed, N.; 和 Amin, M. R. 2023. SentiGOLD: 一个大型孟加拉语金标准多领域情感分析数据集及其评估。*arXiv预印本
    arXiv:2306.06147*。'
- en: Islam et al. (2016) Islam, M. S.; Islam, M. A.; Hossain, M. A.; and Dey, J. J.
    2016. Supervised approach of sentimentality extraction from Bengali facebook status.
    In *2016 19th International Conference on Computer and Information Technology
    (ICCIT)*, 383–387\. IEEE.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam et al. (2016) Islam, M. S.; Islam, M. A.; Hossain, M. A.; and Dey, J.
    J. 2016. 从孟加拉语Facebook状态中提取情感的监督方法。载于 *2016年第19届计算机与信息技术国际会议（ICCIT）*，383–387。IEEE。
- en: 'Kabir et al. (2023) Kabir, M.; Mahfuz, O. B.; Raiyan, S. R.; Mahmud, H.; and
    Hasan, M. K. 2023. BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis
    from Book Reviews. *arXiv preprint arXiv:2305.06595*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kabir et al. (2023) Kabir, M.; Mahfuz, O. B.; Raiyan, S. R.; Mahmud, H.; and
    Hasan, M. K. 2023. BanglaBook: 一个大规模孟加拉语数据集，用于书评情感分析。*arXiv 预印本 arXiv:2305.06595*。'
- en: Karim et al. (2020) Karim, M. R.; Chakravarthi, B. R.; McCrae, J. P.; and Cochez,
    M. 2020. Classification Benchmarks for Under-resourced Bengali Language based
    on Multichannel Convolutional-LSTM Network. *CoRR*, abs / 2004.07807.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karim et al. (2020) Karim, M. R.; Chakravarthi, B. R.; McCrae, J. P.; and Cochez,
    M. 2020. 基于多通道卷积-LSTM网络的资源不足的孟加拉语分类基准。*CoRR*, abs / 2004.07807。
- en: Landis and Koch (1977) Landis, J. R.; and Koch, G. G. 1977. The measurement
    of observer agreement for categorical data. *biometrics*, 159–174.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landis and Koch (1977) Landis, J. R.; and Koch, G. G. 1977. 类别数据观察者一致性的测量。*biometrics*,
    159–174。
- en: 'Liang et al. (2022) Liang, Y.; Meng, F.; Xu, J.; Chen, Y.; and Zhou, J. 2022.
    MSCTD: A Multimodal Sentiment Chat Translation Dataset. In *Proceedings of the
    60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, 2601–2613.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang et al. (2022) Liang, Y.; Meng, F.; Xu, J.; Chen, Y.; and Zhou, J. 2022.
    MSCTD: 一种多模态情感聊天翻译数据集。载于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，2601–2613。'
- en: Muennighoff et al. (2022) Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts,
    A.; Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.; Schoelkopf,
    H.; et al. 2022. Crosslingual generalization through multitask finetuning. *arXiv
    preprint arXiv:2211.01786*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff et al. (2022) Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts,
    A.; Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.; Schoelkopf,
    H.; et al. 2022. 通过多任务微调实现跨语言泛化。*arXiv 预印本 arXiv:2211.01786*。
- en: 'Muhammad et al. (2023) Muhammad, S. H.; Abdulmumin, I.; Ayele, A. A.; Ousidhoum,
    N.; Adelani, D. I.; Yimam, S. M.; Ahmad, I. S.; Beloucif, M.; Mohammad, S.; Ruder,
    S.; et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african
    languages. *arXiv preprint arXiv:2302.08956*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Muhammad et al. (2023) Muhammad, S. H.; Abdulmumin, I.; Ayele, A. A.; Ousidhoum,
    N.; Adelani, D. I.; Yimam, S. M.; Ahmad, I. S.; Beloucif, M.; Mohammad, S.; Ruder,
    S.; et al. 2023. Afrisenti: 用于非洲语言的推特情感分析基准。*arXiv 预印本 arXiv:2302.08956*。'
- en: 'Mukta et al. (2021) Mukta, M. S. H.; Islam, M. A.; Khan, F. A.; Hossain, A.;
    Razik, S.; Hossain, S.; and Mahmud, J. 2021. A comprehensive guideline for Bengali
    sentiment annotation. *Transactions on Asian and Low-Resource Language Information
    Processing*, 21(2): 1–19.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mukta et al. (2021) Mukta, M. S. H.; Islam, M. A.; Khan, F. A.; Hossain, A.;
    Razik, S.; Hossain, S.; and Mahmud, J. 2021. 孟加拉语情感注释的综合指南。*亚洲及低资源语言信息处理期刊*, 21(2):
    1–19。'
- en: 'Nabil, Aly, and Atiya (2015) Nabil, M.; Aly, M.; and Atiya, A. 2015. Astd:
    Arabic sentiment tweets dataset. In *Proceedings of the 2015 conference on empirical
    methods in natural language processing*, 2515–2519.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nabil, Aly, and Atiya (2015) Nabil, M.; Aly, M.; and Atiya, A. 2015. Astd:
    阿拉伯语情感推文数据集。载于 *2015年自然语言处理实证方法会议论文集*，2515–2519。'
- en: OpenAI (2023) OpenAI, R. 2023. GPT-4 technical report. *arXiv*, 2303–08774.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI, R. 2023. GPT-4 技术报告。*arXiv*, 2303–08774。
- en: Platt (1998) Platt, J. 1998. *Fast Training of Support Vector Machines using
    Sequential Minimal Optimization*. MIT Press.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Platt (1998) Platt, J. 1998. *使用序列最小优化的支持向量机快速训练*。麻省理工学院出版社。
- en: Rahman and Hossen (2019) Rahman, A.; and Hossen, M. S. 2019. Sentiment Analysis
    on Movie Review Data Using Machine Learning Approach. In *2019 International Conference
    on Bangla Speech and Language Processing (ICBSLP)*, 1–4.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman and Hossen (2019) Rahman, A.; and Hossen, M. S. 2019. 使用机器学习方法对电影评论数据进行情感分析。载于
    *2019年国际孟加拉语语音与语言处理会议（ICBSLP）*，1–4。
- en: Rahman and Kumar Dey (2018) Rahman, M. A.; and Kumar Dey, E. 2018. Datasets
    for Aspect-Based Sentiment Analysis in Bangla and Its Baseline Evaluation. *Data*,
    3(2).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman and Kumar Dey (2018) Rahman, M. A.; and Kumar Dey, E. 2018. 孟加拉语基于方面的情感分析数据集及其基线评估。*Data*,
    3(2)。
- en: 'Reimers and Gurevych (2019) Reimers, N.; and Gurevych, I. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, 3982–3992.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych (2019) Reimers, N.; 和 Gurevych, I. 2019. Sentence-BERT：使用
    Siamese BERT 网络的句子嵌入。发表于 *2019 年自然语言处理经验方法会议及第 9 届国际联合自然语言处理会议 (EMNLP-IJCNLP)*,
    3982–3992。
- en: 'Rosenthal, Farra, and Nakov (2019) Rosenthal, S.; Farra, N.; and Nakov, P.
    2019. SemEval-2017 task 4: Sentiment analysis in Twitter. *arXiv preprint arXiv:1912.00741*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenthal, Farra, 和 Nakov (2019) Rosenthal, S.; Farra, N.; 和 Nakov, P. 2019.
    SemEval-2017 任务 4：Twitter 情感分析。*arXiv 预印本 arXiv:1912.00741*。
- en: Sazzed (2021) Sazzed, S. 2021. Improving sentiment classification in low-resource
    bengali language utilizing cross-lingual self-supervised learning. In *International
    Conference on Applications of Natural Language to Information Systems*, 218–230\.
    Springer.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sazzed (2021) Sazzed, S. 2021. 利用跨语言自监督学习提升低资源孟加拉语的情感分类。发表于 *国际自然语言应用于信息系统会议*，218–230。Springer。
- en: 'Sechidis, Tsoumakas, and Vlahavas (2011) Sechidis, K.; Tsoumakas, G.; and Vlahavas,
    I. 2011. On the Stratification of Multi-label Data. In Gunopulos, D.; Hofmann,
    T.; Malerba, D.; and Vazirgiannis, M., eds., *Machine Learning and Knowledge Discovery
    in Databases*, ECML-PKDD ’11, 145–158\. Berlin, Heidelberg: Springer Berlin Heidelberg.
    ISBN 978-3-642-23808-6.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sechidis, Tsoumakas, 和 Vlahavas (2011) Sechidis, K.; Tsoumakas, G.; 和 Vlahavas,
    I. 2011. 关于多标签数据的分层。发表于 Gunopulos, D.; Hofmann, T.; Malerba, D.; 和 Vazirgiannis,
    M., 主编，*机器学习与数据库中的知识发现*，ECML-PKDD ’11, 145–158。柏林，海德堡：Springer Berlin Heidelberg。ISBN
    978-3-642-23808-6。
- en: 'Sharmin and Chakma (2021) Sharmin, S.; and Chakma, D. 2021. Attention-based
    convolutional neural network for Bangla sentiment analysis. *Ai & Society*, 36(1):
    381–396.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharmin 和 Chakma (2021) Sharmin, S.; 和 Chakma, D. 2021. 基于注意力的卷积神经网络用于孟加拉语情感分析。*Ai
    & Society*, 36(1): 381–396。'
- en: Tripto and Ali (2018) Tripto, N. I.; and Ali, M. E. 2018. Detecting multilabel
    sentiment and emotions from Bangla youtube comments. In *2018 International Conference
    on Bangla Speech and Language Processing (ICBSLP)*, 1–6\. IEEE.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tripto 和 Ali (2018) Tripto, N. I.; 和 Ali, M. E. 2018. 从孟加拉语 YouTube 评论中检测多标签情感和情绪。发表于
    *2018 国际孟加拉语语音与语言处理会议 (ICBSLP)*, 1–6。IEEE。
- en: 'Wolf et al. (2020) Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
    Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer,
    S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger,
    S.; Drame, M.; Lhoest, Q.; and Rush, A. 2020. Transformers: State-of-the-Art Natural
    Language Processing. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: System Demonstrations*, EMNLP ’20, 38–45. Online:
    Association for Computational Linguistics.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等 (2020) Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi,
    A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.;
    von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger, S.;
    Drame, M.; Lhoest, Q.; 和 Rush, A. 2020. Transformers: 先进的自然语言处理技术。发表于 *2020 年自然语言处理会议：系统演示论文集*，EMNLP
    ’20, 38–45。在线：计算语言学协会。'
- en: Ye et al. (2022) Ye, X.; Iyer, S.; Celikyilmaz, A.; Stoyanov, V.; Durrett, G.;
    and Pasunuru, R. 2022. Complementary Explanations for Effective In-Context Learning.
    *arXiv preprint arXiv:2211.13892*.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等 (2022) Ye, X.; Iyer, S.; Celikyilmaz, A.; Stoyanov, V.; Durrett, G.; 和
    Pasunuru, R. 2022. 有效的上下文学习的补充解释。*arXiv 预印本 arXiv:2211.13892*。
- en: 'Yue et al. (2019) Yue, L.; Chen, W.; Li, X.; Zuo, W.; and Yin, M. 2019. A survey
    of sentiment analysis in social media. *Knowledge and Information Systems*, 60:
    617–663.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yue 等 (2019) Yue, L.; Chen, W.; Li, X.; Zuo, W.; 和 Yin, M. 2019. 社交媒体情感分析综述。*知识与信息系统*，60:
    617–663。'
- en: Zhao et al. (2023) Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.;
    Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language
    models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2023) Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min,
    Y.; Zhang, B.; Zhang, J.; Dong, Z.; 等 2023. 大型语言模型综述。*arXiv 预印本 arXiv:2303.18223*。
- en: Appendix
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Details of the experiments
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验细节
- en: For the experiments with transformer models, we adhered to the following hyper-parameters
    during the fine-tuning process. Additionally, we have released all our scripts
    for the reproducibility.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用变换器模型的实验，我们在微调过程中遵循了以下超参数。此外，我们已发布所有脚本以确保可重复性。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch size: 8;'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：8；
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning rate (Adam): 2e-5;'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率 (Adam)：2e-5；
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of epochs: 10;'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练轮数：10；
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Max seq length: 256.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大序列长度：256。
- en: 'Models and Parameters:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和参数：
- en: •
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BanglaBERT (csebuetnlp/banglabert):L=12, H=768, A=12, total parameters: 110M;
    where L is the number of layers (i.e., Transformer blocks), H is the hidden size,
    and A is the number of self-attention heads; (110M);'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BanglaBERT (csebuetnlp/banglabert): L=12, H=768, A=12，总参数数量：110M；其中L为层数（即Transformer块），H为隐藏层大小，A为自注意力头数；（110M）。'
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'XLM-RoBERTa (xlm-roberta-base): L=24, H=1027, A=16; the total number of parameters
    is 355M.'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'XLM-RoBERTa (xlm-roberta-base): L=24, H=1027, A=16；总参数数量为355M。'
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bloomz (bigscience/bloom-560m): L=24, H=1024, A=16; the total number of parameters
    is 560M.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Bloomz (bigscience/bloom-560m): L=24, H=1024, A=16；总参数数量为560M。'
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bloomz (bigscience/bloom-1b7): L=24, H=2048, A=16; the total number of parameters
    is 1.7B.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Bloomz (bigscience/bloom-1b7): L=24, H=2048, A=16；总参数数量为1.7B。'
