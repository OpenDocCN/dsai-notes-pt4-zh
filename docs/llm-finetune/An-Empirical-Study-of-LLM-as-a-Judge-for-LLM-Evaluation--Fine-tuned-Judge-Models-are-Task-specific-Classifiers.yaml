- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一项关于LLM作为评判者的实证研究：微调的评判模型是任务特定分类器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02839](https://ar5iv.labs.arxiv.org/html/2403.02839)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02839](https://ar5iv.labs.arxiv.org/html/2403.02839)
- en: Hui Huang¹²²2Contribution during internship at Baidu Inc., Yingqi Qu², Jing
    Liu², Muyun Yang¹³³3Corresponding Authors., Tiejun Zhao¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hui Huang¹²²2在百度公司实习期间的贡献，Yingqi Qu²，Jing Liu²，Muyun Yang¹³³3通讯作者，Tiejun Zhao¹
- en: ¹Faculty of Computing, Harbin Institute of Technology, Harbin, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹哈尔滨工业大学计算学院，哈尔滨，中国
- en: ²Baidu Inc., Beijing, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²百度公司，北京，中国
- en: huanghui@stu.hit.edu.cn, {quyingqi, liujing46}@baidu.com,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: huanghui@stu.hit.edu.cn，{quyingqi, liujing46}@baidu.com，
- en: '{yangmuyun, tjzhao}@hit.edu.cn;'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{yangmuyun, tjzhao}@hit.edu.cn；'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, there has been a growing trend of utilizing Large Language Model (LLM)
    to evaluate the quality of other LLMs. Many studies have employed proprietary
    close-source models, especially GPT4, as the evaluator. Alternatively, other works
    have fine-tuned judge models based on open-source LLMs as the evaluator. In this
    study, we conduct an empirical study of different judge models on their evaluation
    capability. Our findings indicate that although the fine-tuned judge models achieve
    high accuracy on in-domain test sets, even surpassing GPT4, they are inherently
    task-specific classifiers, and their generalizability and fairness severely underperform
    GPT4.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用大语言模型（LLM）评估其他LLMs的质量的趋势日益增长。许多研究使用专有的闭源模型，特别是GPT4，作为评估者。另一些研究则基于开源LLMs微调评判模型作为评估者。在本研究中，我们对不同的评判模型在评估能力方面进行了实证研究。我们的发现表明，尽管微调的评判模型在领域内测试集上达到高准确率，甚至超过GPT4，但它们本质上是任务特定的分类器，其通用性和公平性远逊于GPT4。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently, the evaluation for Large-scale Language Models (LLMs) has drawn considerate
    attention of research community Liang et al. ([2022](#bib.bib8)); Chang et al.
    ([2023](#bib.bib2)). As the capabilities of LLMs continue to develop across various
    tasks, it is essential to evaluate them from comprehensive perspectives Qin et al.
    ([2023](#bib.bib11)). However, traditional evaluation metrics for generative models,
    such as BLEU and ROUGE, only capture limited aspects of a model’s performance
    Mathur et al. ([2020](#bib.bib9)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，对大规模语言模型（LLMs）的评估引起了研究界的广泛关注 Liang et al. ([2022](#bib.bib8)); Chang et al.
    ([2023](#bib.bib2))。随着LLMs在各种任务上的能力不断发展，从全面的角度进行评估变得尤为重要 Qin et al. ([2023](#bib.bib11))。然而，传统的生成模型评估指标，如BLEU和ROUGE，仅捕捉了模型表现的有限方面
    Mathur et al. ([2020](#bib.bib9))。
- en: '![Refer to caption](img/3bb320e41d4aa9c44b6487826c1399c0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3bb320e41d4aa9c44b6487826c1399c0.png)'
- en: 'Figure 1: The general training and inference procedure of fine-tuned judge
    models.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：微调的评判模型的通用训练和推断过程。
- en: Some research has proposed LLM-as-a-Judge Li et al. ([2023b](#bib.bib7)); Zheng
    et al. ([2023](#bib.bib19)), namley utilizing proprietary LLMs, especially GPT4
    Achiam et al. ([2023](#bib.bib1)), to evaluate the LLM’s response. By defining
    evaluation schemes in the prompt template, LLMs can leverage their instruction-following
    ability to provide reliable evaluation. For example, Li et al. ([2023b](#bib.bib7))
    constructed a test set containing 805 questions and used the win rate compared
    with text-davinci-003 as the evaluation result, which is determined by GPT4\.
    Zheng et al. ([2023](#bib.bib19)) developed 80 multi-round test questions covering
    eight common areas, and then automatically scored the model’s answers using GPT4\.
    Their results reveal that strong LLM-based evaluators can achieve a high agreement
    rate among human experts, establishing a foundation for LLM-as-a-Judge.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究提出了LLM-as-a-Judge Li et al. ([2023b](#bib.bib7)); Zheng et al. ([2023](#bib.bib19))，即利用专有LLMs，特别是GPT4
    Achiam et al. ([2023](#bib.bib1))，来评估LLM的响应。通过在提示模板中定义评估方案，LLMs可以利用其指令跟随能力提供可靠的评估。例如，Li
    et al. ([2023b](#bib.bib7))构建了一个包含805个问题的测试集，并使用与text-davinci-003相比的胜率作为评估结果，这一结果由GPT4决定。Zheng
    et al. ([2023](#bib.bib19))开发了80个多轮测试问题，涵盖八个常见领域，然后使用GPT4自动评分模型的答案。他们的结果表明，强大的LLM基础评估器可以在专家之间实现高一致性，为LLM-as-a-Judge奠定了基础。
- en: '| Model | Foundation | Instruction | Response | Annotation | Evaluation Scheme
    | Testset |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 基础 | 指令 | 响应 | 注释 | 评估方案 | 测试集 |'
- en: '| JudgeLM |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| JudgeLM |'
- en: '| (Zhu et al., [2023b](#bib.bib21)) | Vicuna | Instruct Datasets |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| (Zhu et al., [2023b](#bib.bib21)) | Vicuna | 指令数据集 |'
- en: '| (Alpaca-GPT4, |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| (Alpaca-GPT4, |'
- en: '| Dolly-15K…) | 11 models |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Dolly-15K…) | 11 模型 |'
- en: '| (Alpaca,Vicuna…) | GPT4 | Pairwise Grading | GPT4 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| (Alpaca, Vicuna…) | GPT4 | 成对评分 | GPT4 |'
- en: '| PandaLM |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| PandaLM |'
- en: '| (Wang et al., [2024](#bib.bib16)) | LLaMA | Alpaca 52K | 5 models |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| (Wang et al., [2024](#bib.bib16)) | LLaMA | Alpaca 52K | 5 模型 |'
- en: '| (LLaMA, Bloom…) | GPT3.5 | Pairwise Selection | Human |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| (LLaMA, Bloom…) | GPT3.5 | 成对选择 | 人工 |'
- en: '| Auto-J |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Auto-J |'
- en: '| (Li et al., [2023a](#bib.bib6)) | LLaMA2-chat | Preference Datasets |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| (Li et al., [2023a](#bib.bib6)) | LLaMA2-chat | 偏好数据集 |'
- en: '| (Chatbot Arena, |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (Chatbot Arena, |'
- en: '| OpenAI WebGPT…) | Preference Datasets | Human | Pairwise Selection |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI WebGPT…) | 偏好数据集 | 人工 | 成对选择 |'
- en: '| Pointwise Grading | Human |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 单点评分 | 人工 |'
- en: '| Prometheus |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Prometheus |'
- en: '| (Kim et al., [2023](#bib.bib5)) | LLaMA2-chat | GPT4 Generated | GPT4 Generated
    | GPT4 | Pointwise Grading | GPT4 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (Kim et al., [2023](#bib.bib5)) | LLaMA2-chat | GPT4 生成的 | GPT4 生成的 | GPT4
    | 单点评分 | GPT4 |'
- en: 'Table 1: Detailed statistics of the four fine-tuned judge models, which is
    the foundation of our cross-validation. All the four models are open-source, with
    their training and test data also publicly released.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：四个微调判断模型的详细统计数据，这是我们交叉验证的基础。这四个模型都是开源的，其训练和测试数据也公开发布。
- en: 'However, relying on external API for evaluation may introduce consideration
    about privacy leakage, and the opacity of API models also challenges the evaluation
    reproducibility. To address these issues, several fine-tuned judge models are
    proposed, relying on open-source foundation models and data constructed from either
    GPT4 or human annotation, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"). For instance, PandaLM Wang et al. ([2024](#bib.bib16))
    constructs data based on Alpaca instructions and GPT3.5 annotation, and then fine-tunes
    LLaMA-7B Touvron et al. ([2023](#bib.bib14)) as a judge model. JudgeLM Zhu et al.
    ([2023b](#bib.bib21)) constructs data from GPT4 annotations and fine-tunes a scalable
    judge model. Auto-J Li et al. ([2023a](#bib.bib6)) constructs judgement data upon
    multiple scenarios to train a generative judge model, which can provide both judgement
    and critic. Prometheus Kim et al. ([2023](#bib.bib5)) defines thousands of evaluation
    criteria and finetunes a fine-grained judge model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，依赖外部API进行评估可能会引入隐私泄露的问题，而API模型的不透明性也挑战了评估的可重复性。为了解决这些问题，提出了几种微调的判断模型，这些模型依赖于开源基础模型和从GPT4或人工标注中构建的数据，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation:
    Fine-tuned Judge Models are Task-specific Classifiers")所示。例如，PandaLM Wang et al.
    ([2024](#bib.bib16)) 基于Alpaca指令和GPT3.5标注构建数据，然后微调LLaMA-7B Touvron et al. ([2023](#bib.bib14))作为判断模型。JudgeLM
    Zhu et al. ([2023b](#bib.bib21)) 从GPT4标注中构建数据并微调了一个可扩展的判断模型。Auto-J Li et al. ([2023a](#bib.bib6))
    在多个场景下构建判断数据以训练一个生成性判断模型，该模型可以提供判断和评论。Prometheus Kim et al. ([2023](#bib.bib5))
    定义了数千个评估标准，并微调了一个细粒度的判断模型。'
- en: 'In this paper, we conduct an empirical study of the evaluation capabilities
    of judge models. We conduct extrapolated evaluations among available judge models
    and meta-evaluation testsets. Experiment results indicate that while the fine-tuned
    judge models achieve superior accuracy on their respective in-domain test sets,
    they still exhibit limitations in the following aspects:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对判断模型的评估能力进行了实证研究。我们在现有的判断模型和元评估测试集之间进行了外推评估。实验结果表明，尽管微调的判断模型在各自的领域测试集上取得了优异的准确性，但它们在以下方面仍然存在局限性：
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The fine-tuned judge model is inherently a classification model;
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调的判断模型本质上是一个分类模型；
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The fine-tuned judge model is overfitted to specific evaluation schemes;
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调的判断模型过于适应特定的评估方案；
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The fine-tuned judge model is biased towards superficial quality;
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调的判断模型偏向于表面质量；
- en: To draw a conclusion, the fine-tuned judge models should be used only in similar
    evaluation scenarios, and can not serve as a general substitution for GPT4 in
    terms of LLM evaluation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，微调的判断模型应仅在类似的评估场景中使用，不能作为LLM评估中GPT4的一般替代品。
- en: 2 How Far can Fine-tuned Judges Go?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 微调判断模型能达到多远？
- en: 'The typical process for finetuning a judge model consists of the following
    three steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 微调判断模型的典型过程包括以下三个步骤：
- en: 'Step 1: Data Collection. The training data generally comprises three components:
    instructions, responses and evaluations. The instructions are typically obtained
    from instruction datasets, with the responses generated by various representative
    models, and the evaluations can be derived from either GPT4 or human annotation;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1：数据收集。训练数据通常包括三个组成部分：指令、响应和评估。指令通常来自指令数据集，响应由各种代表性模型生成，评估可以来源于GPT4或人工注释；
- en: 'Step 2: Prompt Designing. The prompt template can be structured in various
    ways depending on the evaluation scheme, such as pairwise selection (which aims
    to select the better one from a pair of responses), pointwise grading (which aims
    to score a single reference), etc.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：提示设计。提示模板可以根据评估方案的不同而结构化，例如成对选择（旨在从一对响应中选择更好的一个）、逐点评分（旨在对单一参考进行评分）等。
- en: 'Step 3: Model Fine-tuning. Using the designed prompt and collected data, the
    training process of the judge model typically follows the instruction fine-tuning
    paradigm Ouyang et al. ([2022](#bib.bib10)). The model is fed with a instruction
    alongside answer(s) for yielding output comprising evaluation results.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：模型微调。使用设计的提示和收集的数据，评判模型的训练过程通常遵循指令微调范式 Ouyang 等人（[2022](#bib.bib10)）。模型输入指令和答案以生成包括评估结果的输出。
- en: 'While current judge models mostly self-claim exceeding the evaluation capability
    of GPT4, in this work, we make an cross validation based on four representative
    works as listed in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ An Empirical Study
    of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific
    Classifiers"). Our findings are presented in the following sections.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前的评判模型大多数自称超越了GPT4的评估能力，但在这项工作中，我们基于表[1](#S1.T1 "表 1 ‣ 1 介绍 ‣ LLM作为评判者进行LLM评估的实证研究：微调的评判模型是任务特定的分类器")中列出的四项代表性工作进行了交叉验证。我们的发现将在以下部分中呈现。
- en: '| Model | Train | JudgeLM-test | PandaLM-test | Auto-J-test | Prometheus-test
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练 | JudgeLM-test | PandaLM-test | Auto-J-test | Prometheus-test |'
- en: '| accuracy | F1 | accuracy | F1 | agreement | pearson-ind | pearson-ood |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | F1 | 准确度 | F1 | 一致性 | pearson-ind | pearson-ood |'
- en: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 0.636 | 0.563 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 0.636 | 0.563 |'
- en: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 0.742 | 0.743 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 0.742 | 0.743 |'
- en: '| Released Models^† | 79.02 | 71.87 | 67.57 | 57.49 | 54.6 | 0.864 | 0.869
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 发布的模型^† | 79.02 | 71.87 | 67.57 | 57.49 | 54.6 | 0.864 | 0.869 |'
- en: '| Vicuna-7B | generation^‡ | 82.44 | 71.77 | 72.37 | 60.78 | 47.6 | 0.826 |
    0.815 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B | 生成^‡ | 82.44 | 71.77 | 72.37 | 60.78 | 47.6 | 0.826 | 0.815 |'
- en: '| Vicuna-7B | classification^‡ | 82.16 | 70.07 | 70.87 | 60.34 | 46.8 | 0.846
    | 0.831 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B | 分类^‡ | 82.16 | 70.07 | 70.87 | 60.34 | 46.8 | 0.846 | 0.831 |'
- en: '| DeBERTa | classification^‡ | 81.30 | 68.34 | 72.27 | 51.75 | 31.7 | 0.835
    | 0.813 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa | 分类^‡ | 81.30 | 68.34 | 72.27 | 51.75 | 31.7 | 0.835 | 0.813 |'
- en: 'Table 2: Comparison of evaluators trained with different architectures. Results
    with ^† are from evaluating the four publicly released models on their respective
    testsets, and results with ^‡ are from evaluating models trained by us. Notice
    all our LLM-based evaluators are trained from Vicuna-7B Chiang et al. ([2023](#bib.bib3)).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用不同架构训练的评估器比较。带有^†的结果来自对四个公开发布模型在其各自测试集上的评估，带有^‡的结果来自我们训练的模型。请注意，我们所有基于LLM的评估器均从Vicuna-7B
    Chiang 等人（[2023](#bib.bib3)）进行训练。
- en: 2.1 Inherently a Classification Model
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 本质上是一个分类模型
- en: 'If we do not consider critic generation (which is seldom used in system-level
    evaluation), the LLM evaluation is inherently a classification (or regression)
    task. In contrast to the current judge models trained in a generation-style, we
    train four classification (regression) models based the four groups of data in
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")¹¹1The
    training settings and prompt templates are presented in Appendix [A.1](#A1.SS1
    "A.1 Training Settings ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") due
    to space limitation.. We also train four classification models based on DeBERTaV3-large
    He et al. ([2023](#bib.bib4)) on the same data, which is 20 times smaller than
    the 7B version of LLaMA.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们不考虑批评生成（在系统级评估中很少使用），LLM评估本质上是一个分类（或回归）任务。与当前训练为生成风格的评估模型相比，我们基于表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation:
    Fine-tuned Judge Models are Task-specific Classifiers")¹¹1由于篇幅限制，训练设置和提示模板见附录[A.1](#A1.SS1
    "A.1 Training Settings ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")。训练了四个分类（回归）模型。我们还基于DeBERTaV3-large
    He et al. ([2023](#bib.bib4))在相同的数据上训练了四个分类模型，该数据量比7B版本的LLaMA小20倍。'
- en: 'As shown in Table [2](#S2.T2 "Table 2 ‣ 2 How Far can Fine-tuned Judges Go?
    ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"), the classification model performs equally well
    as the generation model for both pairwise selection and pointwise grading. The
    powerful generation ability of LLMs hardly brings any improvement to the evaluation
    accuracy, as they are trained on the same data for the same objective. Moreover,
    the DeBERTa-based evaluator achieves comparable performance with the LLM-based
    evaluators²²2The only exception is on Auto-J-test, which is possibly due to a
    large proportion of the test data exceeds 512 (the maximum context length of DeBERTa).,
    which might be argued for that the encoder-only architecture is more suitable
    for classification.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](#S2.T2 "Table 2 ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical
    Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific
    Classifiers")所示，分类模型在对比选择和逐点评分方面的表现与生成模型一样出色。LLM的强大生成能力几乎没有提高评估准确性，因为它们在相同的数据上训练，目标相同。此外，基于DeBERTa的评估器与基于LLM的评估器达到了可比的性能²²2唯一的例外是Auto-J-test，这可能是由于大量测试数据超过512（DeBERTa的最大上下文长度）。，这可能说明编码器仅架构更适合分类。'
- en: 'We also analyze the correlation between different predictions made by different
    evaluators. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Inherently a Classification
    Model ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") and
    [3](#S2.F3 "Figure 3 ‣ 2.1 Inherently a Classification Model ‣ 2 How Far can Fine-tuned
    Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
    Judge Models are Task-specific Classifiers"), the correlation among different
    classification models is much closer than their correlation with GPT4\. Different
    as they are in architectures, all three models are inherently classifiers fitting
    to the same set of supervision, leading to similar evaluation outcomes.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还分析了不同评估者做出的不同预测之间的相关性。如图[2](#S2.F2 "Figure 2 ‣ 2.1 Inherently a Classification
    Model ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")和[3](#S2.F3
    "Figure 3 ‣ 2.1 Inherently a Classification Model ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers")所示，不同分类模型之间的相关性远高于它们与GPT4的相关性。尽管它们在架构上不同，所有三个模型本质上都是适应相同监督集的分类器，从而导致类似的评估结果。'
- en: '![Refer to caption](img/79767ba21d1f44665a1bdd6ee354c7f6.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79767ba21d1f44665a1bdd6ee354c7f6.png)'
- en: 'Figure 2: The F1 score between the predictions of different evaluators on JudgeLM
    testset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同评估者在JudgeLM测试集上的预测之间的F1分数。
- en: '![Refer to caption](img/a1487d8392bc7307cd990606a7dd6163.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1487d8392bc7307cd990606a7dd6163.png)'
- en: 'Figure 3: The pearson coefficient between the predictions of different evaluators
    on Prometheus testset.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：不同评估者在Prometheus测试集上的预测之间的皮尔逊系数。
- en: 2.2 Overfitting to Evaluation Scheme
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 过拟合到评估方案
- en: '| Model | JudgeLM-test | PandaLM-test | Auto-J-test | Average |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | JudgeLM-test | PandaLM-test | Auto-J-test | 平均值 |'
- en: '| accuracy | F1 | accuracy | F1 | agreement |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | F1 | 准确率 | F1 | 一致性 |'
- en: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 59.83 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 | 73.83 | 52.85 | 62.96 | 58.20 | 42.7 | 59.83 |'
- en: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 73.42 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT 4-0613 | 85.28 | 76.87 | 78.68 | 73.24 | 56.3 | 73.42 |'
- en: '| JudgeLM-7B | 79.02 | 71.87 | 70.97 | 67.59 | 46.6 | 65.53 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| JudgeLM-7B | 79.02 | 71.87 | 70.97 | 67.59 | 46.6 | 65.53 |'
- en: '| PandaLM-7B | 65.24 | 47.42 | 67.57 | 57.49 | 40.0 | 57.61 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| PandaLM-7B | 65.24 | 47.42 | 67.57 | 57.49 | 40.0 | 57.61 |'
- en: '| Auto-J-13B | 72.86 | 57.60 | 71.47 | 61.01 | 54.6 | 66.31 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Auto-J-13B | 72.86 | 57.60 | 71.47 | 61.01 | 54.6 | 66.31 |'
- en: '| Prometheus-13B | 54.24 | 50.04 | 45.25 | 43.58 | 47.8 | 49.10 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Prometheus-13B | 54.24 | 50.04 | 45.25 | 43.58 | 47.8 | 49.10 |'
- en: '| w/o trans | 24.58 | 23.39 | 29.03 | 27.92 | 16.2 | 23.26 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 未翻译 | 24.58 | 23.39 | 29.03 | 27.92 | 16.2 | 23.26 |'
- en: 'Table 3: Results of evaluators on pairwise selection. Notice Prometheus can
    be transformed for pairwise selection by grading two answers twice and compare
    the scores, therefore we release both results with and without transformation.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：评估者在配对选择中的结果。请注意，Prometheus 可以通过对两个答案进行两次评分并比较分数来转换为配对选择，因此我们发布了转换前后的结果。
- en: '| Model | Prometheus-test-ind | Prometheus-test-ood | Average |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Prometheus测试-内 | Prometheus测试-外 | 平均 |'
- en: '| pearson | kendalltau | spearman | pearson | kendalltau | spearman |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| pearson | kendalltau | spearman | pearson | kendalltau | spearman |'
- en: '| GPT 3.5 | 0.636 | 0.536 | 0.617 | 0.563 | 0.453 | 0.521 | 0.600 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 | 0.636 | 0.536 | 0.617 | 0.563 | 0.453 | 0.521 | 0.600 |'
- en: '| GPT 4-0613 | 0.742 | 0.659 | 0.747 | 0.743 | 0.660 | 0.747 | 0.743 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPT 4-0613 | 0.742 | 0.659 | 0.747 | 0.743 | 0.660 | 0.747 | 0.743 |'
- en: '| Prometheus-13B | 0.864 | 0.788 | 0.863 | 0.869 | 0.789 | 0.869 | 0.867 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Prometheus-13B | 0.864 | 0.788 | 0.863 | 0.869 | 0.789 | 0.869 | 0.867 |'
- en: '| JudgeLM-7B | 0.649 | 0.647 | 0.739 | 0.610 | 0.602 | 0.690 | 0.630 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| JudgeLM-7B | 0.649 | 0.647 | 0.739 | 0.610 | 0.602 | 0.690 | 0.630 |'
- en: '| w/o trans | 0.398 | 0.371 | 0.416 | 0.384 | 0.371 | 0.419 | 0.391 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 未翻译 | 0.398 | 0.371 | 0.416 | 0.384 | 0.371 | 0.419 | 0.391 |'
- en: '| PandaLM-7B | 0.417 | 0.368 | 0.423 | 0.386 | 0.333 | 0.383 | 0.402 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| PandaLM-7B | 0.417 | 0.368 | 0.423 | 0.386 | 0.333 | 0.383 | 0.402 |'
- en: '| Auto-J-13B | 0.614 | 0.526 | 0.608 | 0.591 | 0.504 | 0.580 | 0.603 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Auto-J-13B | 0.614 | 0.526 | 0.608 | 0.591 | 0.504 | 0.580 | 0.603 |'
- en: 'Table 4: Results of evaluators on pointwise grading. Notice JudgeLM can be
    transformed for pointwise grading by adding the reference as the first answer,
    therefore we release both results with and without transformation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：评估者在逐点评分中的结果。请注意，JudgeLM 可以通过将参考答案作为第一个答案来转换为逐点评分，因此我们发布了转换前后的结果。
- en: '| Model | MTBench |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MTBench |'
- en: '| --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| accuracy | precision | recall | F1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 精确度 | 召回率 | F1 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT 4-0613 | 66.9 | 63.8 | 62.2 | 61.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPT 4-0613 | 66.9 | 63.8 | 62.2 | 61.9 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| JudgeLM-7B | 48.7 | 52.0 | 49.7 | 48.7 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| JudgeLM-7B | 48.7 | 52.0 | 49.7 | 48.7 |'
- en: '| PandaLM-7B | 55.2 | 52.6 | 49.4 | 46.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| PandaLM-7B | 55.2 | 52.6 | 49.4 | 46.8 |'
- en: '| Auto-J-13B | 51.7 | 50.2 | 46.8 | 43.7 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Auto-J-13B | 51.7 | 50.2 | 46.8 | 43.7 |'
- en: '| Prometheus-13B | 53.2 | 49.6 | 48.4 | 47.1 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Prometheus-13B | 53.2 | 49.6 | 48.4 | 47.1 |'
- en: 'Table 5: Results of evaluators on multi-turn evaluation.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：评估者在多轮评估中的结果。
- en: 'One of the most appealing attribute of LLMs is their generalization ability,
    enabling them to execute various task defined by various instructions Zhu et al.
    ([2023a](#bib.bib20)). Under the case of LLM evaluation, the instruction can also
    be formed in various schemes: pairwise selection, pointwise grading, etc. Since
    different judge models are fine-tuned on different schemes, we could verify their
    evaluation capability under the scheme defined by others. Specifically, we cross-validate
    the judge models on each other’s testset³³3We applying their publicly released
    checkpoints with predefined prompts. For details please refer to Appendix [A.2](#A1.SS2
    "A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers")..'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）最吸引人的特点之一是它们的泛化能力，使它们能够执行由各种指令定义的各种任务（Zhu et al. ([2023a](#bib.bib20))）。在LLM评估的情况下，指令也可以采用各种方案：配对选择、逐点评分等。由于不同的评判模型在不同的方案上进行了微调，我们可以验证它们在其他人定义的方案下的评估能力。具体而言，我们在相互的测试集上对评判模型进行交叉验证³³3我们应用了它们公开发布的检查点和预定义的提示。详细信息请参见附录
    [A.2](#A1.SS2 "A.2 提示模板 ‣ 附录 A 附录 ‣ LLM作为评审的实证研究：微调评审模型是任务特定的分类器")。。
- en: 'As shown in Table [3](#S2.T3 "Table 3 ‣ 2.2 Overfitting to Evaluation Scheme
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") and [4](#S2.T4
    "Table 4 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers"), all four models perform the best on their
    own training settings, respectivly, with results comparable with GPT4\. However,
    if we employ a model on evaluation schemes where it is not trained on⁴⁴4For example,
    using a pairwise model (such as PandaLM or JudgeLM) for pointwise grading (such
    as Promethues testset), or using a pointwise model (such as Promethues) for pairwise
    selection (such as PandaLM or JudgeLM testsets)., the evaluation performance would
    drop by a large margin. On the contrary, GPT4 consistently exhibits superior performance
    across various evaluation schemes.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [3](#S2.T3 "Table 3 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can
    Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation:
    Fine-tuned Judge Models are Task-specific Classifiers") 和 [4](#S2.T4 "Table 4
    ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges Go? ‣
    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers") 所示，所有四个模型在各自的训练设置中表现最佳，结果与 GPT4 相当。然而，如果我们在模型未经过训练的评估方案上使用该模型⁴⁴例如，将成对模型（如
    PandaLM 或 JudgeLM）用于点对点评分（如 Promethues 测试集），或将点对点模型（如 Promethues）用于成对选择（如 PandaLM
    或 JudgeLM 测试集）。，评估性能将大幅下降。相反，GPT4 在各种评估方案中始终表现优越。'
- en: 'We also validate the judge models on MT-bench Zheng et al. ([2023](#bib.bib19)),
    which is a multi-turn meta-evaluation dataset. As can be seen in Table [5](#S2.T5
    "Table 5 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers"), while the four models are all trained
    for single-turn evaluation, they underperforms GPT4 on MT-bench by a large margin.
    This demonstrates that the finetuned judge models are overfitted to their respective
    evaluation schemes and lack generalibility.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在 MT-bench Zheng 等人 ([2023](#bib.bib19)) 上验证了评判模型，该数据集是一个多轮元评估数据集。正如表 [5](#S2.T5
    "Table 5 ‣ 2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges
    Go? ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
    Models are Task-specific Classifiers") 中所示，虽然这四个模型都是为单轮评估训练的，但它们在 MT-bench 上的表现远远落后于
    GPT4。这表明，微调后的评判模型对各自的评估方案过拟合，且缺乏泛化能力。'
- en: 2.3 Biased Towards Superficial Quality
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 偏向表面质量
- en: 'Recently, there has been a few researches on the bias of LLM-based evaluators,
    namely the evaluator would favor more verbose answers, or answers with similar
    format Wang et al. ([2023](#bib.bib15)); Saito et al. ([2023](#bib.bib13)). To
    address this issue, Zeng et al. ([2023](#bib.bib18)) proposed LLMBar as a testbed
    for the fairness of evaluators. It comprises one natural testset (Natural) and
    four adversarial testsets (Neighbor, Manual, GPTOut, GPTInst), and the adversarial
    testsets consist of paired outputs with a correct answer and a smooth, coherent
    but inconsistent answer. We evaluate the judge models on LLMBar\@footnotemark,
    and the results are shown in Table [6](#S2.T6 "Table 6 ‣ 2.3 Biased Towards Superficial
    Quality ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，关于基于 LLM 的评估者偏差的研究有所增加，即评估者倾向于偏好更详细的回答或格式类似的回答 Wang 等人 ([2023](#bib.bib15));
    Saito 等人 ([2023](#bib.bib13))。为解决这个问题，Zeng 等人 ([2023](#bib.bib18)) 提出了 LLMBar
    作为评估公平性的测试平台。它包括一个自然测试集（Natural）和四个对抗性测试集（Neighbor、Manual、GPTOut、GPTInst），对抗性测试集由一个正确答案和一个流畅、一致但不一致的答案组成。我们在
    LLMBar\@footnotemark 上评估了评判模型，结果如表 [6](#S2.T6 "Table 6 ‣ 2.3 Biased Towards Superficial
    Quality ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers") 所示。'
- en: It seems that the fine-tuned judge models are severely biased to superficial
    quality such as formality or verbosity, while neglecting crucial properties such
    as instruction following, resulting an accuracy even worse than random guess on
    the adversarial testsets. On the other hand, GPT4 does not over-rely on the superficial
    features and achieves decent accuracy on all the testsets. This also verifies
    that the fine-tuned judge models are inherently classifiers overfitted to the
    training data, relying on spurious statistical features for prediction.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎经过微调的判断模型严重偏向于表面质量，如形式性或冗长，而忽略了诸如遵循指令等关键属性，导致在对抗性测试集上的准确率甚至比随机猜测还要差。另一方面，GPT4并不过度依赖表面特征，并在所有测试集上都达到了不错的准确率。这也验证了经过微调的判断模型本质上是过拟合于训练数据的分类器，依赖虚假的统计特征进行预测。
- en: It deserves noticing that the DeBERTa-based evaluator also outperforms the LLM-based
    evaluator by a large margin in terms of fairness. This inspires us that the bias
    of LLM-based evaluator may come from the casual language modeling process. While
    the model is trained to generate fluent and verbose responses, it also tends to
    prefer fluent and verbose response when employed for evaluation, even if it is
    not aligned with the instruction.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，基于DeBERTa的评估者在公平性方面也远超基于LLM的评估者。这启示我们，LLM-based评估者的偏差可能源于随意的语言建模过程。虽然模型被训练以生成流畅和冗长的响应，但在评估时，它也倾向于偏好流畅和冗长的响应，即使这与指令不一致。
- en: '| Model | LLMBar |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLMBar |'
- en: '| --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Natu. | Neig. | GPTI. | GPTO. | Manu. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Natu. | Neig. | GPTI. | GPTO. | Manu. |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT 4-0613 | 93.5 | 64.2 | 76.6 | 76.6 | 75.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GPT 4-0613 | 93.5 | 64.2 | 76.6 | 76.6 | 75.0 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| JudgeLM-7B | 62.0 | 23.1 | 26.1 | 46.8 | 28.3 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| JudgeLM-7B | 62.0 | 23.1 | 26.1 | 46.8 | 28.3 |'
- en: '| PandaLM-7B | 59.0 | 16.5 | 21.7 | 42.6 | 26.1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| PandaLM-7B | 59.0 | 16.5 | 21.7 | 42.6 | 26.1 |'
- en: '| Auto-J-13B | 70.0 | 20.9 | 21.7 | 46.8 | 23.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Auto-J-13B | 70.0 | 20.9 | 21.7 | 46.8 | 23.9 |'
- en: '| Prometheus-7B | 53.0 | 22.4 | 17.4 | 27.7 | 32.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Prometheus-7B | 53.0 | 22.4 | 17.4 | 27.7 | 32.6 |'
- en: '| DeBERTa | 62.0 | 26.9 | 42.4 | 55.3 | 34.8 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa | 62.0 | 26.9 | 42.4 | 55.3 | 34.8 |'
- en: 'Table 6: Accuracy of evaluators on bias evaluation.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：评估者在偏差评估上的准确率。
- en: 3 Conclusion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结论
- en: In this work, we empirically study on the judge models for LLM evaluation. As
    revealed in our experiments, despite achieving superior evaluation performance
    on the in-domain testset, the fine-tuned judge models underperforms GPT4 in terms
    of generalibility and fairness by a large margin, which we believe cannot be amended
    by simply finetuning on more evaluation data. It is advisable to exercise caution
    when leveraging fine-tuned judge models for evaluation in real applications, watching
    for the overlap between the evaluation scenario and the training process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们实证研究了用于LLM评估的判断模型。实验表明，尽管在领域内测试集上取得了优异的评估性能，但经过微调的判断模型在通用性和公平性方面远远不如GPT4，我们认为仅通过在更多评估数据上进行微调无法弥补这一差距。建议在实际应用中使用经过微调的判断模型时要谨慎，注意评估场景与训练过程之间的重叠。
- en: Limitations
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Our work still has some limitations: 1) Due to the lack of related work, we
    primarily relied on cross-validation to assess the generalizability of the four
    fine-tuned judge models. To conduct a more thorough evaluation of the generalizability,
    it would be beneficial to incorporate additional independent testsets encompassing
    a broader range of evaluation schemes, such as reference augmentation and domain-specific
    evaluation. 2) The work of Zeng et al. ([2023](#bib.bib18)) is only a general
    assessment of evaluator bias, and we did not include fine-grained assessment for
    different biases, such as formality bias, verbosity bias, etc. 3) Due to time
    and resource constraints, we did not incorporate manual inspection into the meta-evaluation
    process. Including human evaluators would enhance the credibility of our claims.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作仍有一些局限性：1）由于相关工作缺乏，我们主要依赖交叉验证来评估四种微调判断模型的通用性。为了更彻底地评估通用性，纳入额外的独立测试集，涵盖更广泛的评估方案（如参考增强和领域特定评估）将是有益的。2）Zeng等人（[2023](#bib.bib18)）的工作仅对评估者偏差进行了总体评估，我们没有包括对不同偏差（如形式偏差、冗长偏差等）的细粒度评估。3）由于时间和资源限制，我们没有在元评估过程中纳入人工检查。加入人工评估者将提高我们结论的可信度。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat等。2023年。《Gpt-4技术报告》。*arXiv预印本 arXiv:2303.08774*。
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang等人（2023）Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie
    Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang等。2023年。《大型语言模型评估综述》。*ACM智能系统与技术交易*。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang等人（2023）Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao
    Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing。2023年。[Vicuna：一种开源聊天机器人，以90%* ChatGPT质量打动GPT-4](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'He et al. (2023) Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. [DeBERTav3:
    Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled
    embedding sharing](https://openreview.net/forum?id=sE7-XhLxHA). In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2023）Pengcheng He, Jianfeng Gao, 和 Weizhu Chen。2023年。[DeBERTav3：使用ELECTRA风格的预训练和梯度解耦嵌入共享改进deBERTa](https://openreview.net/forum?id=sE7-XhLxHA)。收录于*第十一届学习表征国际会议*。
- en: 'Kim et al. (2023) Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre,
    Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023.
    Prometheus: Inducing fine-grained evaluation capability in language models. *arXiv
    preprint arXiv:2310.08491*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等人（2023）Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran
    Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne等。2023年。《Prometheus：在语言模型中引入细粒度评估能力》。*arXiv预印本
    arXiv:2310.08491*。
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023a）Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, 和 Pengfei
    Liu。2023a年。《用于评估对齐的生成性判断》。*arXiv预印本 arXiv:2310.05470*。
- en: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023b）Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani,
    Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto。2023b年。《Alpacaeval：一种自动评估指令跟随模型的工具》。[https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（2022）Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar等。2022年。《语言模型的整体评估》。*arXiv预印本
    arXiv:2211.09110*。
- en: 'Mathur et al. (2020) Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.
    [Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation
    evaluation metrics](https://doi.org/10.18653/v1/2020.acl-main.448). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 4984–4997, Online. Association for Computational Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mathur等人（2020）Nitika Mathur, Timothy Baldwin, 和 Trevor Cohn。2020年。[纠结于BLEU：重新评估自动机器翻译评估指标](https://doi.org/10.18653/v1/2020.acl-main.448)。收录于*第58届计算语言学协会年会论文集*，第4984–4997页，在线。计算语言学协会。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2022）Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray等。2022年。《训练语言模型以遵循人类反馈的指令》。*神经信息处理系统进展*，35:27730–27744。
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing
    task solver? *arXiv preprint arXiv:2302.06476*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, 和 Diyi Yang. 2023. ChatGPT 是一个通用自然语言处理任务求解器吗？*arXiv 预印本 arXiv:2302.06476*。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, 和 Yuxiong
    He. 2020. Deepspeed: 系统优化使得训练超过 1000 亿参数的深度学习模型成为可能。在 *第 26 届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*，第
    3505–3506 页。'
- en: Saito et al. (2023) Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto.
    2023. Verbosity bias in preference labeling by large language models. *arXiv preprint
    arXiv:2310.10076*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saito et al. (2023) Keita Saito, Akifumi Wachi, Koki Wataoka, 和 Youhei Akimoto.
    2023. 大型语言模型中的冗长偏差。*arXiv 预印本 arXiv:2310.10076*。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023. Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。'
- en: Wang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, 和 Zhifang Sui. 2023. 大型语言模型不是公平的评估者。*ArXiv*，abs/2305.17926。
- en: 'Wang et al. (2024) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, and Yue Zhang. 2024. Pandalm: An automatic evaluation benchmark for llm
    instruction tuning optimization.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, 和 Yue Zhang. 2024. Pandalm: 一种用于 llm 指令调整优化的自动评估基准。'
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander M. Rush. 2020. [Transformers: 最先进的自然语言处理](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。在
    *2020 年自然语言处理实证方法会议：系统演示*，第 38–45 页，在线。计算语言学协会。'
- en: Zeng et al. (2023) Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal,
    and Danqi Chen. 2023. Evaluating large language models at evaluating instruction
    following. *arXiv preprint arXiv:2310.07641*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. (2023) Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal,
    和 Danqi Chen. 2023. 评估大型语言模型在指令跟随评估中的表现。*arXiv 预印本 arXiv:2310.07641*。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, 等. 2023.
    使用 mt-bench 和聊天机器人竞技场评估 llm 作为评审的能力。*arXiv 预印本 arXiv:2306.05685*。
- en: 'Zhu et al. (2023a) Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing
    Xie. 2023a. Promptbench: A unified library for evaluation of large language models.
    *arXiv preprint arXiv:2312.07910*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023a) Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, 和 Xing
    Xie. 2023a. Promptbench: 评估大型语言模型的统一库。*arXiv 预印本 arXiv:2312.07910*。'
- en: 'Zhu et al. (2023b) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023b. Judgelm:
    Fine-tuned large language models are scalable judges. *arXiv preprint arXiv:2310.17631*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023b) Lianghui Zhu, Xinggang Wang, 和 Xinlong Wang. 2023b. Judgelm:
    微调的大型语言模型是可扩展的评审者。*arXiv 预印本 arXiv:2310.17631*。'
- en: Appendix A Appendix
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Training Settings
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 训练设置
- en: 'As mentioned in Section [2.1](#S2.SS1 "2.1 Inherently a Classification Model
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), we fine-tune
    our own judge models based on the four groups of data (JudgeLM Zhu et al. ([2023b](#bib.bib21)),
    PandaLM Wang et al. ([2024](#bib.bib16)), Auto-J Li et al. ([2023a](#bib.bib6)),
    Prometheus Kim et al. ([2023](#bib.bib5))), both in generation-style and in classification-style,
    for the purpose of comparison. We train all the models on NVIDIA A100-80GB GPUs
    with Huggingface-transformers Wolf et al. ([2020](#bib.bib17)) and DeepSpeed Rasley
    et al. ([2020](#bib.bib12)). Detailed hyper-parameters are presented in Table
    [7](#A1.T7 "Table 7 ‣ A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical
    Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific
    Classifiers"). Notice when comparing generation and classification models, we
    adopt the same prompt template and same hyper-parameters, with the only difference
    lies in the prediction method. For generation model, the prediction head reused
    the pretrained language model head, and is trained akin to the process of language
    modeling. For classification (regression) model, the prediction head is newly
    initialized as a linear projection layer, and is decoupled from the language modeling
    process⁵⁵5Please refer to the class AutoModelForSequence Classification in Huggingface
    library for more details., as shown in Figure [4](#A1.F4 "Figure 4 ‣ A.2 Prompt
    Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge for LLM
    Evaluation: Fine-tuned Judge Models are Task-specific Classifiers").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.1节](#S2.SS1 "2.1 本质上是分类模型 ‣ 2 调整后的评判模型的极限 ‣ LLM作为评判者的经验研究：调整后的评判模型是任务特定分类器")所述，我们基于四组数据（JudgeLM
    Zhu et al. ([2023b](#bib.bib21))，PandaLM Wang et al. ([2024](#bib.bib16))，Auto-J
    Li et al. ([2023a](#bib.bib6))，Prometheus Kim et al. ([2023](#bib.bib5))) 对我们自己的评判模型进行微调，涵盖生成风格和分类风格，以便进行比较。我们在NVIDIA
    A100-80GB GPU上使用Huggingface-transformers Wolf et al. ([2020](#bib.bib17))和DeepSpeed
    Rasley et al. ([2020](#bib.bib12))训练所有模型。详细的超参数见表[7](#A1.T7 "表7 ‣ A.2 提示模板 ‣ 附录A
    ‣ LLM作为评判者的经验研究：调整后的评判模型是任务特定分类器")。注意，在比较生成模型和分类模型时，我们采用相同的提示模板和相同的超参数，唯一的区别在于预测方法。对于生成模型，预测头重用了预训练语言模型头，并按照语言建模的过程进行训练。对于分类（回归）模型，预测头作为线性投影层重新初始化，并且与语言建模过程解耦，如图[4](#A1.F4
    "图4 ‣ A.2 提示模板 ‣ 附录A ‣ LLM作为评判者的经验研究：调整后的评判模型是任务特定分类器")所示。
- en: A.2 Prompt Templates
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 提示模板
- en: 'As mentioned in Section [2.1](#S2.SS1 "2.1 Inherently a Classification Model
    ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge for
    LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), [2.2](#S2.SS2
    "2.2 Overfitting to Evaluation Scheme ‣ 2 How Far can Fine-tuned Judges Go? ‣
    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers") and [2.3](#S2.SS3 "2.3 Biased Towards Superficial
    Quality ‣ 2 How Far can Fine-tuned Judges Go? ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"), we
    take the publicly released checkpoints of the four models and validate their performance.
    We use the same prompt templates as defined by the four open-source models, as
    presented from Figure [5](#A1.F5 "Figure 5 ‣ A.2 Prompt Templates ‣ Appendix A
    Appendix ‣ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
    Judge Models are Task-specific Classifiers") to Figure [12](#A1.F12 "Figure 12
    ‣ A.2 Prompt Templates ‣ Appendix A Appendix ‣ An Empirical Study of LLM-as-a-Judge
    for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers"). For
    JudgeLM and PandaLM, their predefined prompts are in the form of pairwise selection,
    and we make slight modifications to apply them on pointwise grading. For Prometheus,
    the predefined prompt is in the form of pointwise grading, and we make slight
    modifications to apply it on pairwise selection. For Auto-J, they predefined prompts
    both for pairwise selection and pointwise grading.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如[2.1](#S2.SS1 "2.1 本质上是分类模型 ‣ 2 微调的评估模型能走多远？ ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")、[2.2](#S2.SS2
    "2.2 过拟合评估方案 ‣ 2 微调的评估模型能走多远？ ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")和[2.3](#S2.SS3
    "2.3 偏向表面质量 ‣ 2 微调的评估模型能走多远？ ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")中提到的，我们使用四个模型公开发布的检查点来验证其性能。我们使用与四个开源模型定义的相同提示模板，如图[5](#A1.F5
    "图5 ‣ A.2 提示模板 ‣ 附录A 附录 ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")到图[12](#A1.F12 "图12
    ‣ A.2 提示模板 ‣ 附录A 附录 ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")。对于JudgeLM和PandaLM，它们的预定义提示形式为成对选择，我们稍作修改以应用于逐点评分。对于Prometheus，预定义提示形式为逐点评分，我们稍作修改以应用于成对选择。对于Auto-J，它们的预定义提示既包括成对选择也包括逐点评分。
- en: '| Configuration | Vicuna-based | DeBERTa-based |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 基于Vicuna | 基于DeBERTa |'
- en: '| base model | Vicuna-7B | DeBERTaV3-large |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 | Vicuna-7B | DeBERTaV3-large |'
- en: '| max length | 2048 | 512 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 最大长度 | 2048 | 512 |'
- en: '| learning rate | 2e-5 | 2e-5 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 2e-5 | 2e-5 |'
- en: '| learning rate schedule | cosine decay | cosine decay |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 学习率计划 | 余弦衰减 | 余弦衰减 |'
- en: '| optimizer | AdamW | AdamW |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | AdamW |'
- en: '| AdamW beta1 | 0.9 | 0.9 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| AdamW beta1 | 0.9 | 0.9 |'
- en: '| AdamW beta2 | 0.999 | 0.98 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| AdamW beta2 | 0.999 | 0.98 |'
- en: '| weight decay | 0.0 | 0.0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.0 | 0.0 |'
- en: '| GPU nums | 8 | 2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GPU数量 | 8 | 2 |'
- en: '| training epochs | 3 | 3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | 3 | 3 |'
- en: '| batch size | 128 | 128 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 128 | 128 |'
- en: '| warmup ratio | 0.003 | 0.003 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 预热比例 | 0.003 | 0.003 |'
- en: '| numerical precision | bf16, tf32 | fp16 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 数值精度 | bf16, tf32 | fp16 |'
- en: '| ZeRO optimizer | stage 2 | None |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ZeRO优化器 | 阶段2 | 无 |'
- en: 'Table 7: Configurations of the judge models fine-tuned by us in Section [2.1](#S2.SS1
    "2.1 Inherently a Classification Model ‣ 2 How Far can Fine-tuned Judges Go? ‣
    An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models
    are Task-specific Classifiers"). Both classification and generation judge models
    leverage the same group of configs based on their foundation model.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：我们在[2.1](#S2.SS1 "2.1 本质上是分类模型 ‣ 2 微调的评估模型能走多远？ ‣ LLM作为评估工具的实证研究：微调的评估模型是任务特定的分类器")中微调的评估模型配置。分类和生成评估模型基于其基础模型使用相同的配置组。
- en: '![Refer to caption](img/b879619b9d6774d8bef4568a3e5ec0de.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b879619b9d6774d8bef4568a3e5ec0de.png)'
- en: 'Figure 4: The architecture of classification-based judge model. The major difference
    lies in the prediction head, where a new classification (regression) head is initialized
    for predicting the result.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基于分类的评估模型架构。主要区别在于预测头，在预测结果时初始化了新的分类（回归）头。
- en: '![Refer to caption](img/d845900076de53da237e0f85bc760a90.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d845900076de53da237e0f85bc760a90.png)'
- en: 'Figure 5: Prompt template for JudgeLM applied for pairwise selection.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：用于成对选择的JudgeLM提示模板。
- en: '![Refer to caption](img/22c367ebf8e4d8e173ead67fd45c638d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22c367ebf8e4d8e173ead67fd45c638d.png)'
- en: 'Figure 6: Prompt template for JudgeLM applied for pointwise grading.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：用于逐点评分的JudgeLM提示模板。
- en: '![Refer to caption](img/a5f5b575d89ef89b5be8ddd3cc0b765f.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5f5b575d89ef89b5be8ddd3cc0b765f.png)'
- en: 'Figure 7: Prompt template for PandaLM applied for pairwise selection.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：用于配对选择的PandaLM提示模板。
- en: '![Refer to caption](img/d589becaf202d696cc55f94d8f145a38.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d589becaf202d696cc55f94d8f145a38.png)'
- en: 'Figure 8: Prompt template for PandaLM applied for pointwise grading.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：用于逐点评分的PandaLM提示模板。
- en: '![Refer to caption](img/1ee3823c2c3e1615e0307d9360f9fea0.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ee3823c2c3e1615e0307d9360f9fea0.png)'
- en: 'Figure 9: Prompt template for Auto-J applied for pairwise selection.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：用于配对选择的Auto-J提示模板。
- en: '![Refer to caption](img/28cd1c709a6c10de05edd83b86183864.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28cd1c709a6c10de05edd83b86183864.png)'
- en: 'Figure 10: Prompt template for Auto-J applied for pointwise grading.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：用于逐点评分的Auto-J提示模板。
- en: '![Refer to caption](img/dcdbee053c57d6607ec9c4c12b46f80b.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dcdbee053c57d6607ec9c4c12b46f80b.png)'
- en: 'Figure 11: Prompt template for Prometheus applied for pairwise selection.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于配对选择的Prometheus提示模板。
- en: '![Refer to caption](img/5bc2fd1f3f1300dae9acdda4dc7f35b8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5bc2fd1f3f1300dae9acdda4dc7f35b8.png)'
- en: 'Figure 12: Prompt template for Prometheus applied for pointwise grading.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：用于逐点评分的Prometheus提示模板。
