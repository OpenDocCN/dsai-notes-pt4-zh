- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:37:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CRE-LLM：一个基于微调大型语言模型的专域中文关系抽取框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18085](https://ar5iv.labs.arxiv.org/html/2404.18085)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18085](https://ar5iv.labs.arxiv.org/html/2404.18085)
- en: Zhengpeng Shi¹    Haoran Luo² ¹College of Statistics and Mathematics, Zhejiang
    Gongshang University, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郑鹏¹    罗浩然² ¹浙江工商大学统计与数学学院，中国
- en: ²School of Computer Science, Beijing University of Posts and Telecommunications,
    China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²计算机科学学院，北京邮电大学，中国
- en: shizhengpeng@outlook.com, luohaoran@bupt.edu.cn  Corresponding author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: shizhengpeng@outlook.com, luohaoran@bupt.edu.cn  通讯作者。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Domain-Specific Chinese Relation Extraction (DS-CRE) aims to extract relations
    between entities from domain-specific Chinese text. Despite the rapid development
    of PLMs in recent years, especially LLMs, DSCRE still faces three core challenges:
    complex network structure design, poor awareness, and high consumption of fine-tuning.
    Given the impressive performance of large language models (LLMs) in natural language
    processing, we propose a new framework called CRE-LLM. This framework is based
    on fine-tuning open-source LLMs, such as Llama-2, ChatGLM2, and Baichuan2\. CRE-LLM
    enhances the logic-awareness and generative capabilities of the model by constructing
    an appropriate prompt and utilizing open-source LLMs for instruction-supervised
    fine-tuning. And then it directly extracts the relations of the given entities
    in the input textual data, which improving the CRE approach. To demonstrate the
    effectiveness of the proposed framework, we conducted extensive experiments on
    two domain-specific CRE datasets, FinRE and SanWen. The experimental results show
    that CRE-LLM is significantly superior and robust, achieving state-of-the-art
    (SOTA) performance on the FinRE dataset. This paper introduces a novel approach
    to domain-specific relation extraction (DSCRE) tasks that are semantically more
    complex by combining LLMs with triples. Our code is publicly available¹¹1[https://github.com/SkyuForever/CRE-LLM](https://github.com/SkyuForever/CRE-LLM).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 专域中文关系抽取（DS-CRE）旨在从专域中文文本中提取实体间的关系。尽管近年来预训练语言模型（PLMs），尤其是大型语言模型（LLMs）迅速发展，DSCRE
    仍面临三个核心挑战：复杂的网络结构设计、较差的意识和高昂的微调消耗。鉴于大型语言模型（LLMs）在自然语言处理中的出色表现，我们提出了一个新的框架，称为 CRE-LLM。该框架基于对开源
    LLM（如 Llama-2、ChatGLM2 和 Baichuan2）的微调。CRE-LLM 通过构建适当的提示并利用开源 LLM 进行指令监督微调，增强了模型的逻辑意识和生成能力。然后，它直接提取输入文本数据中给定实体的关系，从而改进
    CRE 方法。为了验证所提框架的有效性，我们在两个专域 CRE 数据集 FinRE 和 SanWen 上进行了大量实验。实验结果表明，CRE-LLM 显著优越且稳健，在
    FinRE 数据集上实现了最先进的（SOTA）性能。本文介绍了一种将 LLM 与三元组相结合的新方法，用于处理语义更复杂的专域关系抽取（DSCRE）任务。我们的代码公开可用¹¹1[https://github.com/SkyuForever/CRE-LLM](https://github.com/SkyuForever/CRE-LLM)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/2818aee3b5a2aa4812e039690547d84a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2818aee3b5a2aa4812e039690547d84a.png)'
- en: 'Figure 1: An example of Domain-specific CRE Task.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：专域 CRE 任务的示例。
- en: '![Refer to caption](img/042d889f91ebb37850c7779b167564a7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/042d889f91ebb37850c7779b167564a7.png)'
- en: 'Figure 2: Illustration of 4 different paradigms for solving CRE task. As shown
    in Figure 2a, entities and texts from the RE datasets are inputted separately
    into the PLM. And the PLM is combined with the Relation Set and output the relation
    with the highest probability as result. As shown in Figure 2b, prompts are constructed
    based on the texts and Relation Set from the RE dataset and input them into the
    LLM to generate relation. As shown in Figure 2c, the RE dataset is employed to
    construct the prompts and input them into the LLM to generate preliminary results,
    which are subsequently retrieved with the Relation Set to obtain relation extraction
    results. As shown in Figure 2d, our method directly utilizes a fine-tuning dataset
    constructed from the RE dataset to fine-tune the LLM and then generate accurate
    relation extraction results.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：解决 CRE 任务的 4 种不同范式的示例。如图 2a 所示，将 RE 数据集中的实体和文本分别输入到 PLM 中。然后将 PLM 与关系集结合，并输出具有最高概率的关系作为结果。如图
    2b 所示，根据 RE 数据集中的文本和关系集构建提示，并将其输入到 LLM 中生成关系。如图 2c 所示，使用 RE 数据集构建提示，并将其输入到 LLM
    中生成初步结果，随后使用关系集进行检索以获得关系抽取结果。如图 2d 所示，我们的方法直接利用从 RE 数据集中构建的微调数据集来微调 LLM，然后生成准确的关系抽取结果。
- en: 'Domain-Specific Chinese Relation Extraction (DSCRE) is a crucial task in the
    field of Natural Language Processing (NLP). Its objective is to extract relations
    between given entities from domain-specific unstructured Chinese text. Examples
    of such relations include financial and biomedical relations. The main difficulties
    of this task are due to the fact that Chinese datasets specific to certain domains
    are mostly private, with limited informative data and resources. Additionally,
    dealing with the diversity of linguistic expressions and potential ambiguities
    in the text is a challenge. Furthermore, the limitations of the Chinese corpus
    and the low use of dummy words and lexemes in Chinese, which makes it challenging
    to extract relation between entities from domain-specific Chinese texts. A specific
    example as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CRE-LLM: A
    Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large Language
    Model").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '特定领域的中文关系抽取（DSCRE）是自然语言处理（NLP）领域中的一个关键任务。其目标是从特定领域的非结构化中文文本中提取给定实体之间的关系。这些关系的例子包括金融关系和生物医学关系。这个任务的主要困难在于，特定领域的中文数据集大多数是私有的，信息数据和资源有限。此外，处理语言表达的多样性和文本中的潜在歧义也是一大挑战。此外，中文语料库的局限性以及中文中虚词和词素的使用较少，使得从特定领域的中文文本中提取实体之间的关系具有挑战性。具体例子见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")。'
- en: 'The field of deep learning has rapidly developed and achieved significant success
    in relation extraction tasks. Pre-trained models (shown in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework
    with Fine-tuned Large Language Model")a), such as BERT Devlin et al. ([2019](#bib.bib4))
    and T5 Raffel et al. ([2019](#bib.bib18)), have been widely used in this area.
    For instance, when dealing with domain-specific Chinese relation extraction, BERT-PAGG
    Xu et al. ([2023](#bib.bib26)) combines the positional information of entities
    with local features extracted by the PAGG module and entity vector representations
    outputted by BERT to achieve relation extraction. Similarly, MoVE Yang et al.
    ([2023b](#bib.bib28)) achieves this by dynamically learning multi-view features.
    LLMs can also be used for domain-specific relation extraction (shown in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")b), such as ERNIE 3.0 Sun et al.
    ([2021](#bib.bib21)) and ERNIE 3.0 TITAN Wang et al. ([2021](#bib.bib24)), the
    continuous learning semantic understanding frameworks were developed based on
    knowledge augmentation. It utilized self-supervised contrastive learning pre-training
    technique of word-mixing and self-adversarial fine-tuning with word-mixing data
    augmentation to improve RE tasks. Additionally, GPT-FinRE Rajpoot and Parikh ([2023](#bib.bib19))
    can also be used for this purpose (shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model")c). It uses OpenAI models under the ICL framework and incorporates
    retrieval mechanisms to achieve financial relation extraction, has strong potential
    and performance in general.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习领域迅速发展，并在关系提取任务中取得了显著成功。预训练模型（如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model")a）如 BERT Devlin et al. ([2019](#bib.bib4)) 和 T5 Raffel et
    al. ([2019](#bib.bib18)) 已在该领域被广泛使用。例如，在处理领域特定的中文关系提取时，BERT-PAGG Xu et al. ([2023](#bib.bib26))
    将实体的位置信息与 PAGG 模块提取的局部特征以及 BERT 输出的实体向量表示相结合，以实现关系提取。类似地，MoVE Yang et al. ([2023b](#bib.bib28))
    通过动态学习多视角特征来实现这一目标。大语言模型（LLMs）也可用于领域特定的关系提取（如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model")b），如 ERNIE 3.0 Sun et al. ([2021](#bib.bib21)) 和 ERNIE 3.0
    TITAN Wang et al. ([2021](#bib.bib24))，这些基于知识增强的持续学习语义理解框架被开发出来。它利用了自监督对比学习预训练技术和带词汇混合的数据增强的自对抗微调技术，以改进关系提取任务。此外，GPT-FinRE
    Rajpoot 和 Parikh ([2023](#bib.bib19)) 也可用于此目的（如图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model")c）。它在 ICL 框架下使用 OpenAI 模型，并结合检索机制来实现金融关系提取，具有强大的潜力和整体性能。'
- en: Several approaches have been proposed for this task, resulting in significant
    performance improvements. However, three main challenges and difficulties remain.
    (1) Complex network design is required. Methods such as BERT-PAGG Xu et al. ([2023](#bib.bib26)),
    MoVE Yang et al. ([2023b](#bib.bib28)), and other BERT-type pre-trained models
    have been proposed, which require the construction of complex networks and the
    combination of external and internal information to improve model performance.
    (2) Poor perception is a challenge. Direct use of large models for event extraction
    may not perceive internal Relations, especially in the domain-specific of Chinese
    text. A sentence contains semantic information from different perspectives, including
    words, structure and contextual semantic information, as well as domain-specific
    proper nouns. These difficulties can impede the understanding of the model, so
    direct use of the model for relation extraction often yields unsatisfactory results.
    (3) Fine-tuning the model to achieve the domain-specific CRE task results in significant
    memory consumption. GPT-3 Brown et al. ([2020](#bib.bib2)) demonstrates that expanding
    the pre-trained models can further exploit its potential. However, fine-tuning
    large-scale pre-trained models such as ERNIE 3.0 Sun et al. ([2021](#bib.bib21))
    and ERNIE 3.0 TITAN Wang et al. ([2021](#bib.bib24)) to achieve domain-specific
    Chinese relation extraction requires a large amount of memory consumption, which
    may be difficult for a typical team to achieve.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为这个任务已经提出了几种方法，导致了性能的显著提升。然而，仍然存在三大主要挑战和困难。（1）需要复杂的网络设计。提出了如 BERT-PAGG Xu 等人（[2023](#bib.bib26)）、MoVE
    Yang 等人（[2023b](#bib.bib28)）等 BERT 类型的预训练模型，这些方法需要构建复杂的网络，并结合外部和内部信息以提高模型性能。（2）感知能力差是一个挑战。直接使用大型模型进行事件提取可能无法感知内部关系，特别是在中文文本的领域特定语境中。一个句子包含来自不同角度的语义信息，包括词汇、结构和上下文语义信息，以及领域特定的专有名词。这些困难可能阻碍模型的理解，因此直接使用模型进行关系提取通常会得到不尽如人意的结果。（3）微调模型以实现领域特定的
    CRE 任务会消耗大量内存。GPT-3 Brown 等人（[2020](#bib.bib2)）表明，扩展预训练模型可以进一步挖掘其潜力。然而，微调大规模预训练模型如
    ERNIE 3.0 Sun 等人（[2021](#bib.bib21)）和 ERNIE 3.0 TITAN Wang 等人（[2021](#bib.bib24)）以实现领域特定的中文关系提取需要大量内存消耗，这可能对典型团队来说是一个挑战。
- en: 'To address the aforementioned challenges, we introduce CRE-LLM (shown in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")d), a framework for extracting
    domain-specific Chinese relations using fine-tuned open-source large models such
    as Llama-2-7B Touvron et al. ([2023](#bib.bib22)), ChatGLM2-6B Du et al. ([2022](#bib.bib5)),
    and Baichuan2-7B Yang et al. ([2023a](#bib.bib27)). CRE-LLM proposes a direct
    and concise method for relation extraction by calling a fine-tuned open-source
    large model and constructing an appropriate prompt. (1) The method selects a pre-trained
    open-source large model and constructs an appropriate prompt to directly complete
    relation extraction. This eliminates the need to design and build complex network
    structures. Instead, the PEFT Mangrulkar et al. ([2022](#bib.bib14)) framework
    can be used to achieve end-to-end question and answer quickly and efficiently
    Luo et al. ([2023](#bib.bib13)). Additionally, larger models typically have more
    parameters and often yield better relation extraction results. (2) In order to
    enable the model to perceive and comprehend the internal relations between the
    given sentences and entities, we simultaneously utilize instruction-supervised
    fine-tuning methods. This enhances its logical perception and generation capabilities,
    resulting in appropriate extraction outcomes. (3) To leverage the enhanced performance
    of large-scale language models, we apply the PEFT Mangrulkar et al. ([2022](#bib.bib14))
    framework to fine-tune LLMs. This substantially decreases memory consumption and
    enhances training efficiency, enabling typical projects and teams to harness LLMs
    for domain-specific CRE tasks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对上述挑战，我们引入了CRE-LLM（见图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ CRE-LLM: 一种使用微调大型语言模型的领域特定中文关系抽取框架")d），这是一个用于提取领域特定中文关系的框架，使用了如Llama-2-7B
    Touvron等人（[2023](#bib.bib22)）、ChatGLM2-6B Du等人（[2022](#bib.bib5)）和Baichuan2-7B
    Yang等人（[2023a](#bib.bib27)）等微调的开源大型模型。CRE-LLM提出了一种直接而简洁的关系抽取方法，通过调用微调的开源大型模型并构建适当的提示。（1）该方法选择一个预训练的开源大型模型，并构建一个适当的提示来直接完成关系抽取。这避免了设计和构建复杂网络结构的需求。相反，PEFT
    Mangrulkar等人（[2022](#bib.bib14)）框架可以用于快速而高效地实现端到端的问题和答案 Luo等人（[2023](#bib.bib13)）。此外，更大的模型通常具有更多的参数，通常会产生更好的关系抽取结果。（2）为了使模型能够感知和理解给定句子和实体之间的内部关系，我们同时利用了指令监督的微调方法。这增强了其逻辑感知和生成能力，从而产生了适当的抽取结果。（3）为了利用大规模语言模型的增强性能，我们应用PEFT
    Mangrulkar等人（[2022](#bib.bib14)）框架来微调LLMs。这大大减少了内存消耗并提高了训练效率，使得典型的项目和团队能够利用LLMs进行领域特定的CRE任务。'
- en: 'To evaluate the performance of our proposed framework, we applied it to datasets
    of CRE tasks from two different domains: FinRE Li et al. ([2019](#bib.bib10))
    and SanWen Li et al. ([2019](#bib.bib10)). The experimental results demonstrate
    the excellent performance of CRE-LLM on the domain-specific CRE tasks, while achieving
    new state-of-the-art (SOTA) performance on FinRE. We also conducted additional
    experiments to verify whether our relation extraction framework improves the relation
    extraction accuracy and efficiency. Finally, we also discuss how the insights
    from this framework allow us to envision future combinations of LLMs and domain-specific
    CRE.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们提出的框架的性能，我们将其应用于来自两个不同领域的CRE任务数据集：FinRE Li等人（[2019](#bib.bib10)）和SanWen
    Li等人（[2019](#bib.bib10)）。实验结果表明，CRE-LLM在领域特定的CRE任务上表现出色，同时在FinRE上达到了新的最先进（SOTA）性能。我们还进行了额外的实验，以验证我们的关系抽取框架是否提高了关系抽取的准确性和效率。最后，我们还讨论了这个框架的洞察如何使我们能够展望未来LLMs与领域特定CRE的结合。
- en: 2 Related Work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Domain-Specific RE with PLMs
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 使用PLMs进行领域特定的RE
- en: Categorical relation extraction based on pre-trained language models (PLMs)
    such as BERT Devlin et al. ([2019](#bib.bib4)) and T5 Raffel et al. ([2019](#bib.bib18))
    is currently one of the mainstream methods for extracting relations in specific
    domains. The two main pre-trained language models used in the biomedical domain
    are BioBERT Peng et al. ([2019](#bib.bib17)) and PubMedBERT Gu et al. ([2022](#bib.bib8)).
    These models are suitable for real-world tasks such as entity and relation extraction.
    In the financial domain, T5-base Zhao et al. ([2019](#bib.bib30)) is a commonly
    used pre-trained language model. It is important to note that technical term abbreviations
    should always be explained when first used. In order to improve the overall level
    of Chinese financial natural language processing (NLP) and promote the development
    of information extraction, several models have been proposed, including FinBERT
    Araci ([2019](#bib.bib1)) and Mengzi-BERT-base-fin Zhang et al. ([2021](#bib.bib29)).
    Furthermore, in the domain-specific CRE task, BERT-PAGG Xu et al. ([2023](#bib.bib26))
    has developed the PAGG module to enable the model to synthesize multiple pieces
    of information about entities. Additionally, MoVE Yang et al. ([2023b](#bib.bib28))
    has integrated different view representations through hybrid viewpoints and expert
    mechanisms, which effectively filter out noisy information and improve the efficiency
    of model training and inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预训练语言模型（PLMs）如 BERT Devlin 等人 ([2019](#bib.bib4)) 和 T5 Raffel 等人 ([2019](#bib.bib18))
    的分类关系抽取目前是特定领域中提取关系的主流方法之一。生物医学领域中使用的两个主要预训练语言模型是 BioBERT Peng 等人 ([2019](#bib.bib17))
    和 PubMedBERT Gu 等人 ([2022](#bib.bib8))。这些模型适用于实体和关系抽取等实际任务。在金融领域，T5-base Zhao
    等人 ([2019](#bib.bib30)) 是常用的预训练语言模型。需要注意的是，技术术语的缩写在首次使用时应始终解释。为了提升中文金融自然语言处理（NLP）的整体水平并促进信息抽取的发展，提出了几个模型，包括
    FinBERT Araci ([2019](#bib.bib1)) 和 Mengzi-BERT-base-fin Zhang 等人 ([2021](#bib.bib29))。此外，在领域特定的
    CRE 任务中，BERT-PAGG Xu 等人 ([2023](#bib.bib26)) 开发了 PAGG 模块，使模型能够综合多个关于实体的信息。此外，MoVE
    Yang 等人 ([2023b](#bib.bib28)) 通过混合视角和专家机制整合了不同的视图表示，这有效地过滤了噪声信息，并提高了模型训练和推理的效率。
- en: This paper introduces CRE-LLM, a framework for DSCRE that deviates from the
    traditional classification-based approach for relation extraction and instead
    uses a generative method. Fine-tuned open-source LLMs are utilized to directly
    discern relations between given entities through the generation process. The framework
    aims to solve the complex network structure design problem of previous pre-trained
    models like BERT Devlin et al. ([2019](#bib.bib4)) and T5 Raffel et al. ([2019](#bib.bib18)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 CRE-LLM，一个针对 DSCRE 的框架，偏离了传统的基于分类的关系抽取方法，而采用了生成方法。经过微调的开源 LLMs 被用来通过生成过程直接识别给定实体之间的关系。该框架旨在解决之前预训练模型如
    BERT Devlin 等人 ([2019](#bib.bib4)) 和 T5 Raffel 等人 ([2019](#bib.bib18)) 的复杂网络结构设计问题。
- en: 2.2 Domain-Specific RE with LLMs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 面向领域的关系抽取与大型语言模型（LLMs）
- en: The emergence phenomenon has been demonstrated by the introduction of ChatGPT
    Ouyang et al. ([2022](#bib.bib16)) and GPT-4 OpenAI ([2023](#bib.bib15)), which
    are decoder-only Large Language Models (LLMs) with a large number of parameters.
    These models have shown strong performance on natural language problems, making
    many traditional NLP tasks easier Zhao et al. ([2023](#bib.bib31)). The emergence
    of open-source large models such as Llama2-7B Touvron et al. ([2023](#bib.bib22)),
    ChatGLM2-6B Du et al. ([2022](#bib.bib5)), and Baichuan2-7B Yang et al. ([2023a](#bib.bib27))
    have made the use of LLMs more convenient. For example, GCRE-GPT Wang et al. ([2023](#bib.bib25))
    uses a generative approach to extract complex multiple comparison relations from
    input text. On the other hand, GPT-RE Wan et al. ([2023](#bib.bib23)) focuses
    on entity and relation information and use gold label induced reasoning to bridge
    the performance gap of RE. GPT-FinRE Rajpoot and Parikh ([2023](#bib.bib19)),
    which has strong potential to implement financial relation extraction using OpenAI
    models and combining retrieval mechanisms under the ICL framework. Therefore,
    leveraging powerful semantic parsing capabilities of the LLMs to design and implement
    relation extraction through multiple rounds of dialogue or retrieval is a promising
    approach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 出现现象已经通过ChatGPT Ouyang et al. ([2022](#bib.bib16))和GPT-4 OpenAI ([2023](#bib.bib15))的引入得到了证明，这些模型是仅解码器的大型语言模型（LLMs），具有大量的参数。这些模型在自然语言问题上表现出强大的性能，使得许多传统的NLP任务变得更容易
    Zhao et al. ([2023](#bib.bib31))。开源大型模型如Llama2-7B Touvron et al. ([2023](#bib.bib22))、ChatGLM2-6B
    Du et al. ([2022](#bib.bib5))和Baichuan2-7B Yang et al. ([2023a](#bib.bib27))的出现使得使用LLM更加便利。例如，GCRE-GPT
    Wang et al. ([2023](#bib.bib25))采用生成方法从输入文本中提取复杂的多重比较关系。另一方面，GPT-RE Wan et al.
    ([2023](#bib.bib23))关注实体和关系信息，并使用金标诱导推理来弥补RE的性能差距。GPT-FinRE Rajpoot and Parikh
    ([2023](#bib.bib19))具有强大的潜力，通过使用OpenAI模型并结合ICL框架下的检索机制实现金融关系抽取。因此，利用LLM强大的语义解析能力，通过多轮对话或检索来设计和实现关系抽取是一种有前景的方法。
- en: In this paper, our proposed CRE-LLM is a novel approach to relation extraction.
    It employs a fine-tuned open-source LLMs and leverages the powerful capabilities
    of LLMs in text understanding, generation and generalization. This enables the
    extraction of relations between specified entities in a simple and direct end-to-end
    method from unstructured Chinese text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出的CRE-LLM是一种新颖的关系抽取方法。它采用了经过微调的开源LLM，并利用了LLM在文本理解、生成和概括方面的强大能力。这使得能够通过一种简单直接的端到端方法从非结构化中文文本中提取指定实体之间的关系。
- en: 2.3 Fine-Tuning for Large Pre-Trained Models
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 大型预训练模型的微调
- en: With the open-source LLMs like Llama-2-7B Touvron et al. ([2023](#bib.bib22)),
    ChatGLM2-6B Du et al. ([2022](#bib.bib5)) and Baichuan2-7B Yang et al. ([2023a](#bib.bib27))
    emerging. It can be supervised fine-tuning (SFT) by using Parameter-Efficient
    Fine-Tuning (PEFT) technologies Mangrulkar et al. ([2022](#bib.bib14)) such as
    LoRA Hu et al. ([2021](#bib.bib9)), QLoRA Dettmers et al. ([2023](#bib.bib3)),
    P-Tuning v2 Liu et al. ([2021](#bib.bib11)), and Freeze Geva et al. ([2021](#bib.bib7)),
    enhancing the capabilities of LLMs for specific tasks. For instance, ERNIE 3.0
    Sun et al. ([2021](#bib.bib21)) and ERNIE 3.0 TITAN Wang et al. ([2021](#bib.bib24))
    achieved significant performance improvements on domain-specific CRE datasets
    through full fine-tuning. Additionally, the fine-tuning approach was used to construct
    the BBT-FinT5 Lu et al. ([2023](#bib.bib12)) model based on the T5 model, which
    promotes the development of natural language processing (NLP) in Chinese finance.
    In conclusion, the implementation of domain-specific CRE tasks through fine-tuning
    LLMs is highly effective and significant. This enables more general projects and
    teams to deploy implementations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着开源LLM如Llama-2-7B Touvron et al. ([2023](#bib.bib22))、ChatGLM2-6B Du et al.
    ([2022](#bib.bib5))和Baichuan2-7B Yang et al. ([2023a](#bib.bib27))的出现，可以通过使用参数高效微调（PEFT）技术Mangrulkar
    et al. ([2022](#bib.bib14))来进行监督微调（SFT），如LoRA Hu et al. ([2021](#bib.bib9))、QLoRA
    Dettmers et al. ([2023](#bib.bib3))、P-Tuning v2 Liu et al. ([2021](#bib.bib11))和Freeze
    Geva et al. ([2021](#bib.bib7))，增强LLM在特定任务中的能力。例如，ERNIE 3.0 Sun et al. ([2021](#bib.bib21))和ERNIE
    3.0 TITAN Wang et al. ([2021](#bib.bib24))通过全面微调在领域特定的CRE数据集上取得了显著的性能提升。此外，微调方法还用于基于T5模型构建BBT-FinT5
    Lu et al. ([2023](#bib.bib12))模型，推动了中文金融领域自然语言处理（NLP）的发展。总之，通过微调LLM实施领域特定的CRE任务是非常有效且重要的。这使得更多的通用项目和团队能够部署实现。
- en: The CRE-LLM integrates the semantic parsing capability of LLMs with the advantages
    of instruction-supervised fine-tuning. This provides convenience and possibility
    for LLMs to be deployed in DSCRE tasks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CRE-LLM 将 LLMs 的语义解析能力与指令监督微调的优势相结合。这为 LLMs 在 DSCRE 任务中的部署提供了便利和可能性。
- en: '![Refer to caption](img/edfb8a3ce39fe8c5830f9dcaeb509293.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/edfb8a3ce39fe8c5830f9dcaeb509293.png)'
- en: 'Figure 3: The overview of CRE-LLM for domain-specific Chinese relation extraction
    method with supervised fine-tuned LLMs by using Parameter-Efficient Fine-Tuning
    (PEFT) technologies (e.g. LoRA).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用参数高效微调（PEFT）技术（例如 LoRA）对领域特定的中文关系抽取方法进行监督微调的 CRE-LLM 概述。
- en: 3 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基础知识
- en: 3.1 Problem Definition
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: 'Given an input sentence of $n$, an entity e is a consecutive span of words
    where $e=\{xi,xi+1,...,x_{j}\}$. For each sentence s, the output of the CRE-LLM
    is a set of facts where each fact consists of a relation triplet. A relation triplet
    consists of the relation $r\in R$ and tail entity $e_{tail}$. A specific example
    is shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Fine-Tuning for Large Pre-Trained
    Models ‣ 2 Related Work ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入句子 $n$，实体 e 是一系列连续的词，其中 $e=\{xi,xi+1,...,x_{j}\}$。对于每个句子 s，CRE-LLM 的输出是一组事实，每个事实由一个关系三元组组成。一个关系三元组由关系
    $r\in R$ 和尾实体 $e_{tail}$ 组成。具体示例如图 [3](#S2.F3 "图 3 ‣ 2.3 大规模预训练模型的微调 ‣ 2 相关工作
    ‣ CRE-LLM：一个领域特定的中文关系抽取框架，使用微调的大型语言模型")。
- en: 3.2 Parameter-Efficient Fine-Tuning Methods
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 参数高效微调方法
- en: 'The current trend in pre-trained language models (PLMs), represented by models
    like ChatGPT Ouyang et al. ([2022](#bib.bib16)), is towards increasing scale.
    This leads to a growing cost for full fine-tuning. PEFT Mangrulkar et al. ([2022](#bib.bib14))
    methods address this challenge by freezing the majority of model parameters and
    fine-tuning only a small or additional set of parameters. This approach achieves
    comparable performance to full fine-tuning while significantly reducing the overall
    fine-tuning costs. LoRA Hu et al. ([2021](#bib.bib9)) , as a form of PEFT Mangrulkar
    et al. ([2022](#bib.bib14)), which employs low-rank approximation, introducing
    low-rank matrix modules labeled as A and B. This approach helps to reduce memory
    consumption during fine-tuning of LLMs by minimizing changes in weights associated
    with the parameters of the model. For instance, considering the weight matrix
    $W_{0}$ and $A$. The specific update is expressed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当前预训练语言模型（PLMs）的趋势，以 ChatGPT Ouyang 等人（[2022](#bib.bib16)）等模型为代表，正在向大规模发展。这导致完全微调的成本不断增长。PEFT
    Mangrulkar 等人（[2022](#bib.bib14)）方法通过冻结大部分模型参数，仅微调少量或额外的参数来应对这一挑战。这种方法在性能上与完全微调相当，同时显著降低了总体微调成本。LoRA
    Hu 等人（[2021](#bib.bib9)），作为一种 PEFT Mangrulkar 等人（[2022](#bib.bib14)）的方法，采用低秩近似，引入了标记为
    A 和 B 的低秩矩阵模块。这种方法通过最小化与模型参数相关的权重变化，帮助减少在微调 LLMs 过程中的内存消耗。例如，考虑权重矩阵 $W_{0}$ 和
    $A$。具体的更新表达如下：
- en: '|  | $\displaystyle h=$ |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h=$ |  | (1) |'
- en: 'Therefore, LoRA Hu et al. ([2021](#bib.bib9)) framework offers the following
    advantages: generalization comparable to full fine-tuning, no additional inference
    latency, and reduced consumption of memory and storage resources.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LoRA Hu 等人（[2021](#bib.bib9)）的框架提供了以下优点：与完全微调相当的泛化能力，无额外的推理延迟，以及减少内存和存储资源的消耗。
- en: 4 Methodology
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: 'In this section, we first give a brief overview of CRE-LLM and introduced by
    the following three main sections: Instruction design, Efficient Fine-Tuning on
    LLMs and Discussion of Methods. We explain how the fine-tuning dataset is constructed,
    why we use the PEFT Mangrulkar et al. ([2022](#bib.bib14)) framework to fine-tune
    LLMs and comparison of CRE-LLM to other methods.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们首先简要概述了 CRE-LLM，并由以下三个主要部分介绍：指令设计、LLMs 的高效微调和方法讨论。我们解释了微调数据集的构建方式，为什么使用
    PEFT Mangrulkar 等人（[2022](#bib.bib14)）框架来微调 LLMs，以及 CRE-LLM 与其他方法的比较。
- en: 4.1 Overview of CRE-LLM
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 CRE-LLM 概述
- en: 'CRE-LLM is a framework for domain-specific CRE based on generative question-answering,
    utilizing open-source LLMs fine-tuned with instruction supervision. Firstly, CRE-LLM
    involves constructing effective instructions based on natural language and specified
    entities in CRE datasets. Simultaneously, it sets up reasonable input and output
    configurations to enable the model to better understand and accomplish the task.
    Subsequently, leverage PEFT Mangrulkar et al. ([2022](#bib.bib14)) framework to
    achieve efficient fine-tuning of LLMs. And then, from the given Chinese text data,
    the fine-tuned LLMs are employed to generate inference results in the form of
    triplets by means of prompts. Finally, the framework employs a direct extraction
    process to derive the relations from the generated triplets, thereby elucidating
    the relations between the specified entities in the Chinese text. The detailed
    framework description is illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Fine-Tuning
    for Large Pre-Trained Models ‣ 2 Related Work ‣ CRE-LLM: A Domain-Specific Chinese
    Relation Extraction Framework with Fine-tuned Large Language Model").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'CRE-LLM是一个基于生成问答的领域特定CRE框架，利用经过指令监督微调的开源LLMs。首先，CRE-LLM涉及根据自然语言和CRE数据集中的指定实体构造有效的指令。同时，设立合理的输入和输出配置，使模型能够更好地理解和完成任务。随后，利用PEFT
    Mangrulkar等人（[2022](#bib.bib14)）的框架实现LLMs的高效微调。然后，通过提示，从给定的中文文本数据中，微调后的LLMs生成三元组形式的推断结果。最后，该框架采用直接提取过程，从生成的三元组中推导关系，从而阐明中文文本中指定实体之间的关系。框架的详细描述见图[3](#S2.F3
    "Figure 3 ‣ 2.3 Fine-Tuning for Large Pre-Trained Models ‣ 2 Related Work ‣ CRE-LLM:
    A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large
    Language Model")。'
- en: 4.2 Instruction Design
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 指令设计
- en: To construct an instruction fine-tuning dataset, we need to design instruction,
    input and output. In this context, we set the “instruction” for inputting LLMs
    as “Please extract the relation based on the given sentence and entities.” And
    the entities in each instance of the relation extraction dataset (e.g., “Shuanghui”)
    are labeled with the given entities using “[]” (e.g., “[Shuanghui]”) to help LLMs
    understand the meaning of the concept referred to by the term “given entities”
    and better focus on and comprehend the relation between the given entities.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构造一个指令微调数据集，我们需要设计指令、输入和输出。在这种情况下，我们将LLMs的“指令”设置为“请根据给定的句子和实体提取关系。”并且关系提取数据集中的每个实例中的实体（例如，“双汇”）使用“[]”标注（例如，“[双汇]”），以帮助LLMs理解“给定实体”一词所指的概念，并更好地关注和理解给定实体之间的关系。
- en: 'Subsequently, to facilitate LLMs understanding of the task requirements and
    ensure the accuracy of inferring relation extraction, we append a triplet and
    the format is that $([e_{head}],?,[e_{tail}])$ (e.g., “([Shuanghui International],
    ? ,[Shuanghui])”), where “?” represents the relation between the given entities.
    The specific format of the instance data is detailed in Figure [3](#S2.F3 "Figure
    3 ‣ 2.3 Fine-Tuning for Large Pre-Trained Models ‣ 2 Related Work ‣ CRE-LLM: A
    Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large Language
    Model"). Combining the relations between given entities, we construct triplets
    (e.g., “([Shuanghui International], analysis ,[Shuanghui])”) as both “input” and
    “output,” and finally, the previously constructed instruction is added to complete
    an instance in the fine-tuning dataset.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '为了便于LLMs理解任务要求并确保关系提取推断的准确性，我们附加了一个三元组，格式为$([e_{head}],?,[e_{tail}])$（例如，“([双汇国际],
    ? ,[双汇])”），其中“？”表示给定实体之间的关系。实例数据的具体格式详见图[3](#S2.F3 "Figure 3 ‣ 2.3 Fine-Tuning
    for Large Pre-Trained Models ‣ 2 Related Work ‣ CRE-LLM: A Domain-Specific Chinese
    Relation Extraction Framework with Fine-tuned Large Language Model")。结合给定实体之间的关系，我们构造三元组（例如，“([双汇国际],
    分析 ,[双汇])”）作为“输入”和“输出”，最终将之前构造的指令添加到微调数据集中以完成一个实例。'
- en: Additionally, if multiple relation exist between entities in a sentence, we
    list all relation in the inference results, separated by “,” (e.g.“([Shuanghui
    International], self, [Shuanghui]), ([Shuanghui], analysis, [Shuanghui International])”).
    Following this structured process, the instruction fine-tuning training dataset
    for open-source LLMs is constructed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果句子中存在多个关系，我们将在推断结果中列出所有关系，用“,”分隔（例如“([双汇国际], 自身, [双汇]), ([双汇], 分析, [双汇国际])”）。按照这一结构化过程，构建了用于开源LLMs的指令微调训练数据集。
- en: 4.3 Efficient Fine-Tuning on LLMs
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 高效微调LLMs
- en: To reduce the substantial cost of fine-tuning LLMs, overcome the limitations
    of the available information resources for domain-specific CRE tasks and ensure
    that LLMs can generate standardized relation extraction results, it is necessary
    to implement a solution that addresses these issues. The CRE-LLM employs the PEFT
    Mangrulkar et al. ([2022](#bib.bib14)) framework to address these challenges and
    minimize the number of fine-tuning parameters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少微调 LLMs 的巨大成本，克服特定领域 CRE 任务可用信息资源的局限性，并确保 LLMs 能够生成标准化的关系提取结果，需要实施一个解决方案来应对这些问题。CRE-LLM
    使用 PEFT Mangrulkar 等人 ([2022](#bib.bib14)) 框架来解决这些挑战，并最小化微调参数的数量。
- en: 'The CRE-LLM allows switching between all the aforementioned fine-tuning methods
    and open-source LLMs, such as Llama-2-7B Touvron et al. ([2023](#bib.bib22)),
    ChatGLM2-6B Du et al. ([2022](#bib.bib5)), and Baichuan2-7B Yang et al. ([2023a](#bib.bib27)).
    As shown in Figure 3, for these large-parameter-only decoder LLMs, CRE-LLM adopts
    PEFT Mangrulkar et al. ([2022](#bib.bib14)) technology. It fine-tunes the $Q$
    parts of the input in the GQA section using LoRA Hu et al. ([2021](#bib.bib9)),
    adds them to the $K$ part, and then calculates the attention using the following
    formula:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CRE-LLM 允许在所有上述微调方法和开源 LLMs 之间切换，如 Llama-2-7B Touvron 等人 ([2023](#bib.bib22))、ChatGLM2-6B
    Du 等人 ([2022](#bib.bib5)) 和 Baichuan2-7B Yang 等人 ([2023a](#bib.bib27))。如图 3 所示，对于这些仅含大参数的解码器
    LLMs，CRE-LLM 采用了 PEFT Mangrulkar 等人 ([2022](#bib.bib14)) 技术。它使用 LoRA Hu 等人 ([2021](#bib.bib9))
    微调 GQA 部分的 $Q$ 部分，将其添加到 $K$ 部分，然后使用以下公式计算注意力：
- en: '|  | $\displaystyle Attention(Q,K,V)=$ |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Attention(Q,K,V)=$ |  | (2) |'
- en: 'Then, through a series of network architecture layers, the relations of the
    given entities in the input text are generated. The underlying mechanism can be
    outlined by the following formula:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过一系列网络结构层生成输入文本中给定实体的关系。其底层机制可以通过以下公式概述：
- en: '|  | $\displaystyle p_{\theta}(\mathcal{Y}&#124;\mathcal{X},\mathcal{P})=$
    |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{\theta}(\mathcal{Y}&#124;\mathcal{X},\mathcal{P})=$
    |  | (3) |'
- en: where $\mathcal{X}=[x_{1},x_{2},...,x_{n}]$ is the target sequence, and $\mathcal{P}$
    is the prompt. Through PEFT Mangrulkar et al. ([2022](#bib.bib14)) framework,
    the problem of poor internal perceptual capabilities in general open-source LLMs
    is addressed. It simultaneously enhances the generation understanding and generalization
    abilities of LLMs with respect to texts in this domain specific. This can be widely
    applied to domain-specific CRE tasks, generating more accurate results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{X}=[x_{1},x_{2},...,x_{n}]$ 是目标序列，$\mathcal{P}$ 是提示。通过 PEFT Mangrulkar
    等人 ([2022](#bib.bib14)) 框架，解决了通用开源 LLMs 内部感知能力差的问题。它同时增强了 LLMs 在特定领域文本中的生成理解和泛化能力。这可以广泛应用于特定领域的
    CRE 任务，生成更准确的结果。
- en: 4.4 Discussion of Methods
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 方法讨论
- en: 'Based on the explanation and process description of the CRE-LLM method above,
    let’s discuss and compare it with existing CRE methods:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述对 CRE-LLM 方法的解释和过程描述，我们来讨论并与现有的 CRE 方法进行比较：
- en: (1) Comparison with Classify based PLMs. The PLMs, such as BERT Devlin et al.
    ([2019](#bib.bib4)), T5 Raffel et al. ([2019](#bib.bib18)), ERNIE 3.0 Sun et al.
    ([2021](#bib.bib21)) and ERNIE 3.0 TITAN Wang et al. ([2021](#bib.bib24)) primarily
    implement CRE using classification-based methods. In contrast, the CRE-LLM utilizes
    PEFT framework to fine-tune LLMs and employs a generative approach for CRE.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 与基于分类的 PLMs 的比较。PLMs，如 BERT Devlin 等人 ([2019](#bib.bib4))、T5 Raffel 等人 ([2019](#bib.bib18))、ERNIE
    3.0 Sun 等人 ([2021](#bib.bib21)) 和 ERNIE 3.0 TITAN Wang 等人 ([2021](#bib.bib24))
    主要通过基于分类的方法实现 CRE。相比之下，CRE-LLM 利用 PEFT 框架来微调 LLMs，并采用生成方法进行 CRE。
- en: (2) Comparison with Classify-then-Extract based LLMs. The direct invocation
    of LLMs, such as ChatGPT Ouyang et al. ([2022](#bib.bib16)) and GPT4 OpenAI ([2023](#bib.bib15))
    involves constructing a prompt based on the Classify-then-Extract approach. This
    prompt must include the Relation Set as options for the relation between the given
    entities. In contrast, CRE-LLM employs a fine-tuned LLM that has incorporated
    the knowledge of the Relation Set, eliminating the need for an excessively lengthy
    prompt while achieving the domain-specific CRE task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 与基于分类-再提取的 LLMs 的比较。直接调用 LLMs，如 ChatGPT Ouyang 等人 ([2022](#bib.bib16)) 和
    GPT4 OpenAI ([2023](#bib.bib15)) 涉及基于分类-再提取方法构建提示。这些提示必须包括作为给定实体之间关系的选项的关系集。相比之下，CRE-LLM
    使用了一个经过微调的 LLM，该 LLM 已经纳入了关系集的知识，从而避免了过于冗长的提示，同时实现了特定领域的 CRE 任务。
- en: (3) Comparison with Generate-then-Retrieval based LLMs. Regarding the Generate-then-Retrieval
    method, LLMs generate the relation between given entities directly and then align
    it with the Relation Set through retrieval. In contrast, CRE-LLM does not require
    additional retrieval alignment with the Relation Set. It can directly infer and
    generate more accurate relation extraction results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 与生成-再检索基础的 LLMs 的比较。关于生成-再检索方法，LLMs 直接生成给定实体之间的关系，然后通过检索与关系集合对齐。相比之下，CRE-LLM
    不需要额外的检索对齐到关系集合。它可以直接推断并生成更准确的关系提取结果。
- en: 5 Experiments
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: 'This section covers experimental configurations, results, and analysis. We
    address the following research questions (RQs): RQ1: Is CRE-LLM superior to other
    CRE methods? RQ2: Is fine-tuning of LLMs effective? RQ3: Is the design of the
    prompts for LLMs reasonable? RQ4: Why use fine-tuned open-source LLMs instead
    of directly invoking ChatGPT for CRE? RQ5: Does LoRA fine-tuning reduce GPU memory
    consumption and environmental configuration requirements, and does it improve
    training efficiency? RQ6: How about error analysis?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了实验配置、结果和分析。我们探讨了以下研究问题（RQs）：RQ1：CRE-LLM 是否优于其他 CRE 方法？RQ2：LLM 的微调是否有效？RQ3：LLM
    的提示设计是否合理？RQ4：为什么使用微调的开源 LLM 而不是直接调用 ChatGPT 进行 CRE？RQ5：LoRA 微调是否减少了 GPU 内存消耗和环境配置要求，并且是否提高了训练效率？RQ6：错误分析如何？
- en: 5.1 Experimental Setup
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Datasets. All experiments were conducted on two Chinese datasets from different
    domains: FinRE Li et al. ([2019](#bib.bib10)), based on 2647 financial news articles
    from Sina Finance, containing 44 distinguished relations, including the special
    relation “NA” indicating no relation between the marked entity pairs. The dataset
    was split into 26,971, 2,977, and 7,453 relation extraction instances for training,
    validation and testing respectively. SanWen Li et al. ([2019](#bib.bib10)) is
    based on 837 Chinese literary works, comprising 10 distinguishable relations,
    also including the special relation “NA.” It was respectively divided into 17,227,
    1,793 and 2,220 relation extraction instances for training, validation, and testing.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。所有实验均在来自不同领域的两个中文数据集上进行：FinRE Li 等人（[2019](#bib.bib10)），基于来自新浪财经的 2647 篇金融新闻文章，包含
    44 种区分关系，其中包括特殊关系“NA”，表示标记实体对之间没有关系。该数据集被划分为 26,971 个训练实例、2,977 个验证实例和 7,453 个测试实例。SanWen
    Li 等人（[2019](#bib.bib10)）基于 837 部中文文学作品，包含 10 种可区分的关系，也包括特殊关系“NA”。它被分别划分为 17,227
    个训练实例、1,793 个验证实例和 2,220 个测试实例。
- en: 'Baselines. CRE-LLM was compared with several CRE baselines, including ERNIE
    3.0 TITAN Wang et al. ([2021](#bib.bib24)), Bert-PAGG Xu et al. ([2023](#bib.bib26)),
    MoVE Yang et al. ([2023b](#bib.bib28)), and other CRE methods mentioned in Section
    [2](#S2 "2 Related Work ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '基准线。CRE-LLM 与几个 CRE 基准进行了比较，包括 ERNIE 3.0 TITAN Wang 等人（[2021](#bib.bib24)）、Bert-PAGG
    Xu 等人（[2023](#bib.bib26)）、MoVE Yang 等人（[2023b](#bib.bib28)）和第 [2](#S2 "2 Related
    Work ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model") 节中提到的其他 CRE 方法。'
- en: Evaluation Metrics. Following previous work Sun et al. ([2019](#bib.bib20),
    [2021](#bib.bib21)); Wang et al. ([2021](#bib.bib24)); Lu et al. ([2023](#bib.bib12));
    Xu et al. ([2023](#bib.bib26)); Yang et al. ([2023b](#bib.bib28)), we used Precision,
    Recall, and F1-Score to evaluate the performance of methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。遵循之前的工作 Sun 等人（[2019](#bib.bib20)，[2021](#bib.bib21)）；Wang 等人（[2021](#bib.bib24)）；Lu
    等人（[2023](#bib.bib12)）；Xu 等人（[2023](#bib.bib26)）；Yang 等人（[2023b](#bib.bib28)），我们使用精确度、召回率和
    F1 分数来评估方法的性能。
- en: Hyperparameters and Environment. For the FinRE dataset Li et al. ([2019](#bib.bib10)),
    we fine-tuned the LLM for 5 epochs with a learning rate of 5e-5\. For SanWen Li
    et al. ([2019](#bib.bib10)), fine-tuning was performed for 10 epochs with a learning
    rate of 5e-4\. The batch size was set to 4, and the gradient accumulation steps
    were 5e-5\. All experiments were conducted on a single NVIDIA A40 GPU (48 GB),
    and the results are averages from five experiments with different random seeds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数和环境。对于 FinRE 数据集，Li 等人（[2019](#bib.bib10)）我们对 LLM 进行了 5 个周期的微调，学习率为 5e-5。对于
    SanWen 数据集，Li 等人（[2019](#bib.bib10)）的微调则进行了 10 个周期，学习率为 5e-4。批处理大小设置为 4，梯度累积步数为
    5e-5。所有实验都在单个 NVIDIA A40 GPU（48 GB）上进行，结果是来自五次不同随机种子的实验的平均值。
- en: '| Method | FinRE | SanWen |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | FinRE | SanWen |'
- en: '| Dev | Test | Dev | Test |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Dev | Test | Dev | Test |'
- en: '| T5-base | - | 54.93 | - | - |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| T5-base | - | 54.93 | - | - |'
- en: '| ERNIE 2.0 | 63.33 | 60.60 | 79.92 | 77.97 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE 2.0 | 63.33 | 60.60 | 79.92 | 77.97 |'
- en: '| FinBERT-base | - | 55.33 | - | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| FinBERT-base | - | 55.33 | - | - |'
- en: '| ERNIE 3.0 | 64.87 | 62.88 | 81.32 | 82.59 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE 3.0 | 64.87 | 62.88 | 81.32 | 82.59 |'
- en: '| Mengzi-BERT-base-fin | - | 58.25 | - | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 孟子-BERT-base-fin | - | 58.25 | - | - |'
- en: '| ERNIE 3.0 TITAN | 65.27 | 63.15 | 83.07 | 82.70 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ERNIE 3.0 TITAN | 65.27 | 63.15 | 83.07 | 82.70 |'
- en: '| BBT-FinT5-base | - | 60.62 | - | - |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| BBT-FinT5-base | - | 60.62 | - | - |'
- en: '| BBT-FinT5-large | - | 61.88 | - | - |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| BBT-FinT5-large | - | 61.88 | - | - |'
- en: '| Bert-PAGG | - | 53.01 | - | 73.83 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Bert-PAGG | - | 53.01 | - | 73.83 |'
- en: '| BERT+MultiView | - | 53.89 | - | 72.98 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| BERT+MultiView | - | 53.89 | - | 72.98 |'
- en: '| CRE-LLM(ours) | 69.17 | 67.37 | 81.30 | 82.74 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CRE-LLM（我们） | 69.17 | 67.37 | 81.30 | 82.74 |'
- en: 'Table 1: The F1 score comparison of CRE-LLM with other baselines on FinRE Li
    et al. ([2019](#bib.bib10)) and SanWen Li et al. ([2019](#bib.bib10)) datasets.
    The results are mainly taken from their original paper. For our proposed CRE-LLM,
    we display the results of the best setup on FinRE and SanWen. The results of the
    FinRE and SanWen datasets in RQ1 are all attained utilising Baichuan2-13B, which
    is fine-tuned with LoRA. The best results in each metric are in bold.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：CRE-LLM在FinRE Li et al. ([2019](#bib.bib10))和SanWen Li et al. ([2019](#bib.bib10))数据集上的F1分数与其他基线的比较。结果主要取自他们的原始论文。对于我们提出的CRE-LLM，我们展示了在FinRE和SanWen上的最佳设置结果。RQ1中的FinRE和SanWen数据集的结果均使用百川2-13B微调，采用LoRA方法。每个指标中的最佳结果用**粗体**显示。
- en: 5.2 Main Result (RQ1)
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果 (RQ1)
- en: 'For the DSCRE task, to validate that CRE-LLM outperforms other CRE methods
    as proposed in this paper, we conducted inference tests through designed experiments.
    The experimental results, listed in Table [1](#S5.T1 "Table 1 ‣ 5.1 Experimental
    Setup ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model"), include fine-tuning Baichuan2-13B
    Yang et al. ([2023a](#bib.bib27)) on the FinRE Li et al. ([2019](#bib.bib10))
    and SanWen Li et al. ([2019](#bib.bib10)) using LoRA Hu et al. ([2021](#bib.bib9)).
    Additionally, we compared CRE-LLM with other baseline models. From Table [1](#S5.T1
    "Table 1 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model"),
    it can be observed that CRE-LLM outperforms the majority of baseline models on
    the SanWen Li et al. ([2019](#bib.bib10)) and demonstrates significant improvements
    over all existing CRE methods on the FinRE Li et al. ([2019](#bib.bib10)). Comparing
    with the previous best scores, CRE-LLM increased F1 score on the FinRE Li et al.
    ([2019](#bib.bib10)) validation set and test set by approximately 5.98% and 6.68%,
    respectively. This reflects the state-of-the-art performance of CRE-LLM in domain-specific
    CRE capabilities.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DSCRE任务，为验证CRE-LLM是否优于本文提出的其他CRE方法，我们通过设计实验进行了推理测试。实验结果列在表[1](#S5.T1 "表 1
    ‣ 5.1 实验设置 ‣ 5 实验 ‣ CRE-LLM：一种领域特定的中文关系抽取框架，结合微调的大型语言模型")中，包括对FinRE Li et al.
    ([2019](#bib.bib10))和SanWen Li et al. ([2019](#bib.bib10))使用LoRA Hu et al. ([2021](#bib.bib9))微调百川2-13B
    Yang et al. ([2023a](#bib.bib27))。此外，我们还将CRE-LLM与其他基线模型进行了比较。从表[1](#S5.T1 "表 1
    ‣ 5.1 实验设置 ‣ 5 实验 ‣ CRE-LLM：一种领域特定的中文关系抽取框架，结合微调的大型语言模型")中可以观察到，CRE-LLM在SanWen
    Li et al. ([2019](#bib.bib10))上优于大多数基线模型，并且在FinRE Li et al. ([2019](#bib.bib10))上显著改善了所有现有CRE方法。与之前的最佳得分相比，CRE-LLM在FinRE
    Li et al. ([2019](#bib.bib10))验证集和测试集上的F1分数分别提高了约5.98%和6.68%。这反映了CRE-LLM在领域特定CRE能力上的**最先进**表现。
- en: 5.3 Effectiveness of LLM’s Fine-Tuning (RQ2)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 LLM微调的有效性 (RQ2)
- en: To validate the effectiveness of fine-tuning LLMs, we chose to use the FinRE
    dataset Li et al. ([2019](#bib.bib10)). We randomly selected 20%, 40%, 60%, and
    80% of the training data and fine-tuned Baichuan2-13B Yang et al. ([2023a](#bib.bib27))
    using LoRA Hu et al. ([2021](#bib.bib9)) on each subset. We then compared their
    inference test results with the results of the model fine-tuned on the complete
    dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证微调大型语言模型（LLMs）的有效性，我们选择使用FinRE数据集Li et al. ([2019](#bib.bib10))。我们随机选择了20%、40%、60%和80%的训练数据，并对每个子集使用LoRA
    Hu et al. ([2021](#bib.bib9))微调百川2-13B Yang et al. ([2023a](#bib.bib27))。然后，我们将其推理测试结果与在完整数据集上微调的模型结果进行了比较。
- en: '| Fine-Tuning Setting | Precision | Recall | F1 score |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 微调设置 | 精度 | 召回率 | F1分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Baichuan2-13B &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 百川2-13B &#124;'
- en: '&#124;       +20%Training Data &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       +20%训练数据 &#124;'
- en: '| 63.25 | 60.04 | 61.10 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 63.25 | 60.04 | 61.10 |'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Baichuan2-13B &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 百川2-13B &#124;'
- en: '&#124;       +40%Training Data &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       +40%训练数据 &#124;'
- en: '| 66.97 | 63.74 | 64.73 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 66.97 | 63.74 | 64.73 |'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Baichuan2-13B &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 百川2-13B &#124;'
- en: '&#124;       +60%Training Data &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       +60%训练数据 &#124;'
- en: '| 69.44 | 66.25 | 67.20 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 69.44 | 66.25 | 67.20 |'
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Baichuan2-13B &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Baichuan2-13B &#124;'
- en: '&#124;       +80%Training Data &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;       +80%训练数据 &#124;'
- en: '| 69.71 | 66.89 | 67.65 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 69.71 | 66.89 | 67.65 |'
- en: '| Baichuan2-13B | 69.43 | 66.65 | 67.37 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B | 69.43 | 66.65 | 67.37 |'
- en: 'Table 2: Ablation study for LLM’s Fine-Tuning on FinRE.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：关于LLM微调的消融研究。
- en: 'As shown in the Table [2](#S5.T2 "Table 2 ‣ 5.3 Effectiveness of LLM’s Fine-Tuning
    (RQ2) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model"), the performance of the model
    on domain-specific CRE tasks gradually improves with an increase in the training
    data, demonstrating the effectiveness of fine-tuning. Additionally, we observed
    that when using only 40% of the training data for fine-tuning, the F1 score already
    surpassed the original best performance. This indicates that fine-tuning allows
    LLMs to learn effectively from a limited dataset, achieving commendable performance.
    Moreover, when fine-tuning with 60% or more of the training data, there is minimal
    improvement in the F1 score. This suggests that instruction fine-tuning exhibits
    good generalization capabilities, emphasizing the importance of the quality of
    training samples over quantity.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S5.T2 "Table 2 ‣ 5.3 LLM微调的有效性（RQ2） ‣ 5 实验 ‣ CRE-LLM：一个基于微调的大型语言模型的领域特定中文关系抽取框架")所示，模型在领域特定CRE任务上的表现随着训练数据的增加而逐渐提高，展示了微调的有效性。此外，我们观察到，当仅使用40%的训练数据进行微调时，F1分数已超过原来的最佳表现。这表明微调使LLM能够从有限的数据集中有效学习，取得了可观的性能。此外，当微调使用60%或更多的训练数据时，F1分数的提升非常有限。这表明教学微调展示了良好的泛化能力，强调了训练样本质量相对于数量的重要性。
- en: 5.4 The Impact of Instruction Design (RQ3)
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 教学设计的影响（RQ3）
- en: 'To validate the effectiveness and rationality of the dataset constructed for
    the DSCRE task using our proposed fine-tuning-based generative approach with open-source
    LLMs, we conducted ablation experiments on various components. Table [3](#S5.T3
    "Table 3 ‣ 5.4 The Impact of Instruction Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM:
    A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large
    Language Model") shows the effectiveness of the different components of Instruction
    Design on FinRE Li et al. ([2019](#bib.bib10)).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证使用我们提出的基于微调的生成方法与开源LLM构建的DSCRE任务数据集的有效性和合理性，我们对各种组件进行了消融实验。表[3](#S5.T3 "Table
    3 ‣ 5.4 教学设计的影响（RQ3） ‣ 5 实验 ‣ CRE-LLM：一个基于微调的大型语言模型的领域特定中文关系抽取框架")展示了教学设计中不同组件在FinRE中的有效性（Li
    et al. ([2019](#bib.bib10))）。
- en: '| Fine-Tuning Setting | Precision | Recall | F1 score |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 微调设置 | 精确度 | 召回率 | F1分数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Baichuan2-13B w/o EM | 61.09 | 61.54 | 61.24 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B w/o EM | 61.09 | 61.54 | 61.24 |'
- en: '| Baichuan2-13B w/o AT | 68.67 | 65.13 | 66.23 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B w/o AT | 68.67 | 65.13 | 66.23 |'
- en: '| Baichuan2-13B w/o TR | 69.28 | 67.07 | 67.50 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B w/o TR | 69.28 | 67.07 | 67.50 |'
- en: '| Baichuan2-13B w/o AT+TR | 67.44 | 64.42 | 65.22 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B w/o AT+TR | 67.44 | 64.42 | 65.22 |'
- en: '| Baichuan2-13B | 69.43 | 66.65 | 67.37 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B | 69.43 | 66.65 | 67.37 |'
- en: 'Table 3: Ablation study for Instruction Design on FinRE.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：关于FinRE的教学设计的消融研究。
- en: 'Effectiveness of Using “[]” Entity Markers (EM). As shown in Tabel [3](#S5.T3
    "Table 3 ‣ 5.4 The Impact of Instruction Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM:
    A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large
    Language Model"), the model when fine-tuned without utilizing “[]” entity markers
    in the dataset resulted in a decrease in Precision, Recall and F1 score by 13.65%,
    8.30% and 10.11% respectively. Using “[]” entity markers proves to be effective.
    It can facilitate LLMs in locating and understanding the given entities in the
    sentence, as well as aligning entities during the inference process. This helps
    improve the performance of CRE .'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“[]”实体标记（EM）的有效性。如表[3](#S5.T3 "Table 3 ‣ 5.4 教学设计的影响（RQ3） ‣ 5 实验 ‣ CRE-LLM：一个基于微调的大型语言模型的领域特定中文关系抽取框架")所示，当模型在微调过程中未使用“[]”实体标记时，精确度、召回率和F1分数分别下降了13.65%、8.30%和10.11%。使用“[]”实体标记被证明是有效的。它可以帮助LLM定位和理解句子中的实体，并在推理过程中对齐实体。这有助于提高CRE的表现。
- en: 'Effectiveness of Adding Triplets (AT) After the Input Text Tabel [3](#S5.T3
    "Table 3 ‣ 5.4 The Impact of Instruction Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM:
    A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned Large
    Language Model") demonstrates that removing the triplet part leads to a decrease
    in Precision, Recall, F1 score by 1.11%, 2.33% and 1.81% respectively. The absence
    of the triplet in the input text during the fine-tuning phase makes the model
    only observe the positional relation between entities from the output part. It
    hinders the learning process and reducing the performance of CRE.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '添加三元组（AT）后的有效性。表[3](#S5.T3 "Table 3 ‣ 5.4 The Impact of Instruction Design
    (RQ3) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")显示，移除三元组部分会导致精确度、召回率、F1分数分别下降1.11%、2.33%和1.81%。在微调阶段，输入文本中缺少三元组使得模型仅能观察到输出部分实体之间的位置信息。这阻碍了学习过程，降低了CRE的性能。'
- en: 'Effectiveness of the Triplet Results(TR) As shown in Table [3](#S5.T3 "Table
    3 ‣ 5.4 The Impact of Instruction Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model"),
    from the results of the ablation experiments, it can be observed that TR does
    not significantly improve Precision, Recall and F1 Score. However, in the absence
    of AT, TR can respectively increase Precision, Recall and F1 Score by 2.95%, 3.46%,
    and 3.39%. This is because in such a scenario, demanding direct output of relation
    extraction results from LLM may lead to insufficient understanding of entities
    and their relation reasoning. The setting of TR provides partial informative cues
    and aligns with entities indicated in the input, leveraging the well-established
    learning patterns of LLM. This allows for better understanding of task requirements,
    resulting in superior performance on domain-specific CRE tasks. Therefore, we
    consider TR to be effective, contributing to enhanced robustness of LLM in domain-specific
    CRE performance.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '三元组结果（TR）的有效性。如表[3](#S5.T3 "Table 3 ‣ 5.4 The Impact of Instruction Design
    (RQ3) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")所示，从消融实验结果可以观察到，TR对精确度、召回率和F1分数的提升并不显著。然而，在没有AT的情况下，TR分别可以提高精确度、召回率和F1分数2.95%、3.46%和3.39%。这是因为在这种情况下，要求LLM直接输出关系提取结果可能导致对实体及其关系推理的理解不足。TR的设置提供了部分信息线索，并与输入中指示的实体对齐，利用LLM已建立的学习模式。这有助于更好地理解任务要求，从而在特定领域的CRE任务中表现更佳。因此，我们认为TR是有效的，增强了LLM在特定领域CRE性能的鲁棒性。'
- en: 'Comparison among all components. The results of ablation experiments for all
    components are shown in Table [3](#S5.T3 "Table 3 ‣ 5.4 The Impact of Instruction
    Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model"), where the settings of EM and
    AT significantly improve Precision, Recall and F1 score. Particularly, EM has
    the most significant impact on the performance of the model in CRE. For the TR
    part, its influence on the performance of the model is substantial without AT
    settings. However, with AT settings, its impact becomes very limited. Consequently,
    AT has a stronger effect on LLM’s performance in CRE than TR, with TR playing
    a role similar to AT but to a lesser extent. In summary, EM has the most significant
    impact on LLM’s performance in DSCRE, followed by AT, and TR has the smallest
    impact.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '所有组件之间的比较。所有组件的消融实验结果见表[3](#S5.T3 "Table 3 ‣ 5.4 The Impact of Instruction
    Design (RQ3) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction
    Framework with Fine-tuned Large Language Model")，其中EM和AT的设置显著提高了精确度、召回率和F1分数。特别是，EM对CRE模型的性能影响最为显著。对于TR部分，其在没有AT设置时对模型性能的影响显著。然而，在有AT设置的情况下，其影响变得非常有限。因此，AT对LLM在CRE中的性能影响比TR更强，TR的作用与AT类似但程度较小。总之，EM对LLM在DSCRE中的性能影响最大，其次是AT，而TR的影响最小。'
- en: 5.5 Comparison with ChatGPT (RQ4)
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 与ChatGPT的比较（RQ4）
- en: 'To illustrate why CRE-LLM chooses to fine-tune open-source LLMs for relation
    extraction, instead of directly using ChatGPT to accomplish this task, experiments
    were designed to compare these two approaches. Therefore, we conducted experiments
    on FinRE Li et al. ([2019](#bib.bib10)) for relation extraction. Table [4](#S5.T4
    "Table 4 ‣ 5.5 Comparison with ChatGPT (RQ4) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model") shows
    the Comparison with ChatGPTOuyang et al. ([2022](#bib.bib16)).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '为了说明为什么 CRE-LLM 选择微调开源 LLM 进行关系提取，而不是直接使用 ChatGPT 完成这一任务，设计了实验以比较这两种方法。因此，我们在
    FinRE Li 等（[2019](#bib.bib10)）上进行了关系提取实验。表 [4](#S5.T4 "表 4 ‣ 5.5 与 ChatGPT 的比较（RQ4）
    ‣ 5 实验 ‣ CRE-LLM: 一种领域特定的中文关系提取框架与微调的大型语言模型") 显示了与 ChatGPT 的比较，Ouyang 等（[2022](#bib.bib16)）。'
- en: '| Method | Precision | Recall | F1 score |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 精度 | 召回率 | F1 分数 |'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ChatGPT &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ChatGPT &#124;'
- en: '&#124;      +Classify-then-Extract &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;      +分类后提取 &#124;'
- en: '| 27.59 | 25.86 | 26.44 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 27.59 | 25.86 | 26.44 |'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ChatGPT &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ChatGPT &#124;'
- en: '&#124;      +Generate-then-Retrieval &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;      +生成后检索 &#124;'
- en: '| 25.86 | 24.14 | 24.71 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 25.86 | 24.14 | 24.71 |'
- en: '| Baichuan2-13B+LoRA | 69.43 | 66.65 | 67.37 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B+LoRA | 69.43 | 66.65 | 67.37 |'
- en: 'Table 4: CRE result comparison of CRE-LLM with ChatGPT on FinRE. For our proposed
    CRE-LLM, we display the results of the basic setup on FinRE. The best results
    in each metric are in bold.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: CRE-LLM 与 ChatGPT 在 FinRE 上的 CRE 结果比较。对于我们提出的 CRE-LLM，我们展示了在 FinRE 上的基本设置结果。每个指标的最佳结果以**粗体**显示。'
- en: 'Comparison with Classify-then-Extract. This method involves constructing a
    Relation Set containing all distinguished relations from the RE dataset. The prompt
    is modified to introduce the Relation Set, which allows ChatGPT Ouyang et al.
    ([2022](#bib.bib16)) to select the appropriate relation between the given entities
    from the Relation Set. As shown in Table [4](#S5.T4 "Table 4 ‣ 5.5 Comparison
    with ChatGPT (RQ4) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation
    Extraction Framework with Fine-tuned Large Language Model"), despite ChatGPT Ouyang
    et al. ([2022](#bib.bib16)) having a larger number of model parameters, it is
    not open-source and cannot be fine-tuned, posing a challenge for generating standard
    relation extraction results directly. The challenge presented by the dataset is
    the necessity to provide 44 distinguished relations for ChatGPT Ouyang et al.
    ([2022](#bib.bib16)) to differentiate. This necessitates the input prompt being
    longer and more challenging for the model to understand, consequently reducing
    the performance of CRE.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '与分类后提取的比较。该方法涉及构建一个包含所有从 RE 数据集中区分出的关系的关系集。提示语被修改以引入关系集，这使 ChatGPT Ouyang 等（[2022](#bib.bib16)）能够从关系集中选择给定实体之间的适当关系。如表
    [4](#S5.T4 "表 4 ‣ 5.5 与 ChatGPT 的比较（RQ4） ‣ 5 实验 ‣ CRE-LLM: 一种领域特定的中文关系提取框架与微调的大型语言模型")
    所示，尽管 ChatGPT Ouyang 等（[2022](#bib.bib16)）的模型参数更多，但由于其不是开源的，无法进行微调，这给直接生成标准的关系提取结果带来了挑战。数据集带来的挑战是需要为
    ChatGPT Ouyang 等（[2022](#bib.bib16)）提供 44 种区分关系。这要求输入提示更长，更具挑战性，导致模型理解困难，从而降低了
    CRE 的性能。'
- en: 'Comparison with Generate-then-Retrieval. This method involves using ChatGPT
    Ouyang et al. ([2022](#bib.bib16)) to directly infer and generate relations. In
    contrast to the Classify-then-Extract approach, this method does not necessitate
    the input of all special relations, simplifying the prompt significantly. However,
    the generated results are more diverse. SimCSE Gao et al. ([2021](#bib.bib6))
    is employed to align the results with the Relation Set for relation extraction.
    As shown in Table [4](#S5.T4 "Table 4 ‣ 5.5 Comparison with ChatGPT (RQ4) ‣ 5
    Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework
    with Fine-tuned Large Language Model"), due to the lack of fine-tuning, ChatGPT
    Ouyang et al. ([2022](#bib.bib16)) exhibits weaker internal understanding and
    logical reasoning capabilities for domain-specific Chinese text. It fails to generate
    results with precise meanings. Consequently, the performance of relation extraction
    is naturally not ideal after retrieval alignment through SimCSE Gao et al. ([2021](#bib.bib6)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '与生成-再检索方法的比较。这种方法涉及使用 ChatGPT Ouyang 等人 ([2022](#bib.bib16)) 直接推断和生成关系。与分类-提取方法相比，这种方法不需要输入所有特殊关系，从而显著简化了提示。然而，生成的结果更为多样。SimCSE
    Gao 等人 ([2021](#bib.bib6)) 被用于将结果与关系集对齐以进行关系抽取。如表 [4](#S5.T4 "表 4 ‣ 5.5 与 ChatGPT
    的比较 (RQ4) ‣ 5 实验 ‣ CRE-LLM: 一个领域特定的中文关系抽取框架，使用微调的大型语言模型") 所示，由于缺乏微调，ChatGPT Ouyang
    等人 ([2022](#bib.bib16)) 在处理领域特定的中文文本时显示出较弱的内部理解和逻辑推理能力，未能生成具有准确含义的结果。因此，通过 SimCSE
    Gao 等人 ([2021](#bib.bib6)) 进行检索对齐后，关系抽取的性能自然不理想。'
- en: 5.6 Analysis of Efficiency (RQ5)
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 效率分析 (RQ5)
- en: 'For domain-specific CRE tasks, training efficiency and costs also require significant
    attention. To illustrate that CRE-LLM achieves higher efficiency and lower cost,
    our experimental results as shown in Table [5](#S5.T5 "Table 5 ‣ 5.6 Analysis
    of Efficiency (RQ5) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific Chinese Relation
    Extraction Framework with Fine-tuned Large Language Model").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '对于领域特定的 CRE 任务，训练效率和成本也需要显著关注。为了说明 CRE-LLM 实现了更高的效率和更低的成本，我们的实验结果如表 [5](#S5.T5
    "表 5 ‣ 5.6 效率分析 (RQ5) ‣ 5 实验 ‣ CRE-LLM: 一个领域特定的中文关系抽取框架，使用微调的大型语言模型") 所示。'
- en: '| Method | Trainable params↓ | Training Time↓ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 可训练参数↓ | 训练时间↓ |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ERNIE 3.0 &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ERNIE 3.0 &#124;'
- en: '&#124;     +Progressive Learing &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     +渐进学习 &#124;'
- en: '| 100M |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 100M |'
- en: '&#124; 11h30m &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 11小时30分钟 &#124;'
- en: '&#124; 4h &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4小时 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Baichuan2-13B+LoRA &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Baichuan2-13B+LoRA &#124;'
- en: '&#124;     +40%Training Data &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;     +40%训练数据 &#124;'
- en: '| 0.0655M | 1h08m |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 0.0655M | 1小时08分钟 |'
- en: '| Baichuan2-13B+LoRA | 2h32m |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan2-13B+LoRA | 2小时32分钟 |'
- en: 'Table 5: Comparison of CRE-LLM with ERNIE 3.0 on FinRE.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: CRE-LLM 与 ERNIE 3.0 在 FinRE 上的比较。'
- en: 'The specific experimental results are presented in Table [5](#S5.T5 "Table
    5 ‣ 5.6 Analysis of Efficiency (RQ5) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model").
    Although ERNIE 3.0 Sun et al. ([2021](#bib.bib21)) exhibits superior performance
    after fine-tuning on the SanWen Li et al. ([2019](#bib.bib10)), Table [5](#S5.T5
    "Table 5 ‣ 5.6 Analysis of Efficiency (RQ5) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model") reveals
    that the number of parameters and GPU memory consumption required for general
    NLP tasks after fine-tuning ERNIE 3.0 is significantly more than for CRE-LLM.
    For instance, during the fine-tuning phase, ERNIE 3.0 would need at least eight
    32GB V100 GPUs, indicating higher requirements for environmental configuration.
    Additionally, the fine-tuning training duration is also substantially longer.
    In contrast, CRE-LLM, which employs PEFT Mangrulkar et al. ([2022](#bib.bib14))
    framework, achieves efficient parameter fine-tuning for LLMs. This results in
    a significant reduction in GPU memory usage and a lowering of environmental configuration
    demands, thereby enabling more general teams and projects to adopt it.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '具体实验结果见表[5](#S5.T5 "Table 5 ‣ 5.6 Analysis of Efficiency (RQ5) ‣ 5 Experiments
    ‣ CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with Fine-tuned
    Large Language Model")。尽管ERNIE 3.0 Sun等人（[2021](#bib.bib21)）在SanWen Li等人（[2019](#bib.bib10)）上的微调后表现优越，但表[5](#S5.T5
    "Table 5 ‣ 5.6 Analysis of Efficiency (RQ5) ‣ 5 Experiments ‣ CRE-LLM: A Domain-Specific
    Chinese Relation Extraction Framework with Fine-tuned Large Language Model")显示，ERNIE
    3.0在微调后对一般NLP任务所需的参数数量和GPU内存消耗显著高于CRE-LLM。例如，在微调阶段，ERNIE 3.0至少需要八个32GB V100 GPU，说明对环境配置的要求较高。此外，微调训练的持续时间也显著较长。相比之下，采用PEFT
    Mangrulkar等人（[2022](#bib.bib14)）框架的CRE-LLM实现了对LLMs的高效参数微调。这大大降低了GPU内存使用量和环境配置需求，从而使更多的一般团队和项目能够采用。'
- en: 5.7 Error Analysis (RQ6)
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误分析（RQ6）
- en: 'We analyzed instances in the FinRE Li et al. ([2019](#bib.bib10)) test set
    where CRE-LLM failed to achieve correct CRE, and conducted a statistical analysis
    of the errors. The specific findings are summarized as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了FinRE Li等人（[2019](#bib.bib10)）测试集中CRE-LLM未能正确完成CRE的实例，并对这些错误进行了统计分析。具体发现总结如下：
- en: Entity Relation Understanding Errors (52.21%). The primary source of errors
    in relation extraction is a misunderstanding of relations between entities. This
    is mainly due to the inherent difficulty of relation extraction in the dataset
    and the model’s limited ability to precisely infer relations between entities
    in domain-specific natural language texts.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实体关系理解错误（52.21%）。关系抽取中的错误主要来源于对实体之间关系的误解。这主要是由于数据集中关系抽取的固有难度以及模型在领域特定自然语言文本中精确推断实体关系的能力有限。
- en: Errors with Multiple Relations between Entities (26.69%). Another category of
    errors arises when there are multiple relations between entities. For instance,
    in sentences like “With the establishment of [Ant Financial], [Alibaba]’s layout
    in the financial business has been officially clarified,” , and the relations
    of the given entities are “ownership” and “establishment”. The model may overlook
    one of the relations while generating the relation extraction result, leading
    to lower the performance of CRE.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 多关系实体错误（26.69%）。另一类错误发生在实体之间存在多个关系时。例如，在句子“随着[蚂蚁金融]的建立，[阿里巴巴]在金融业务中的布局已得到正式明确”中，给定实体的关系包括“所有权”和“建立”。模型在生成关系抽取结果时可能会忽略其中一个关系，从而降低CRE的性能。
- en: “NA” Relation Errors (20.47%). The FinRE Li et al. ([2019](#bib.bib10)) dataset
    contains a special “NA” relation that is challenging to express in the text. Even
    for general readers, understanding the relation between the given entities may
    be difficult. This complexity poses a challenge for the model to achieve accurate
    CRE.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: “NA”关系错误（20.47%）。FinRE Li等人（[2019](#bib.bib10)）数据集中包含一个特殊的“NA”关系，这在文本中难以表达。即使对于普通读者，理解给定实体之间的关系也可能很困难。这种复杂性给模型实现准确的CRE带来了挑战。
- en: Nonexistent Relation Errors (0.622%). Since CRE-LLM does not provide any relation
    options and relies on learning from training data to obtain the Relation Set of
    the dataset. Consequently, there may be instances where the model, despite fine-tuning,
    generates relation extraction results that do not exist in the Relation Set. This
    is due to the fine-tuned LLMs still retain certain independent capabilities in
    text understanding, generation, and generalization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不存在的关系错误（0.622%）。由于 CRE-LLM 不提供任何关系选项，并依赖于从训练数据中学习以获得数据集的关系集。因此，尽管模型经过微调，仍可能生成在关系集中不存在的关系提取结果。这是因为微调后的
    LLM 仍保留某些独立的文本理解、生成和概括能力。
- en: 6 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we propose CRE-LLM, a large language model framework for domain-specific
    Chinese relation extraction (DSCRE) based on fine-tunned open-source large models.
    This method represents a significant shift from traditional approaches. It employs
    the PEFT Mangrulkar et al. ([2022](#bib.bib14)) framework and open-source LLMs
    with numerous parameters to achieve a simple and efficient end-to-end generative
    relation extraction. It addresses inherent challenges such as complex network
    structure design, poor perception and high consumption of fine-tuning.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了 CRE-LLM，一个基于微调开源大模型的领域特定中文关系提取（DSCRE）的大型语言模型框架。该方法代表了从传统方法的重大转变。它采用了
    PEFT Mangrulkar 等人 ([2022](#bib.bib14)) 框架和具有大量参数的开源 LLM，以实现简单而高效的端到端生成关系提取。它解决了复杂网络结构设计、感知差和微调消耗高等固有挑战。
- en: Our experimental results, based on two standard domain-specific CRE benchmarks,
    namely FinRE Li et al. ([2019](#bib.bib10)) and SanWen Li et al. ([2019](#bib.bib10)),
    demonstrate that CRE-LLM achieves state-of-the-art(SOTA) performance on the DSCRE
    tasks. Moreover, the simplicity, flexibility, and especially the efficiency of
    our framework make it a promising direction for applying LLMs to DSCRE tasks that
    involve stronger domain specificity and more challenging semantic understanding.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果基于两个标准领域特定 CRE 基准，即 FinRE Li 等人 ([2019](#bib.bib10)) 和 SanWen Li 等人 ([2019](#bib.bib10))，表明
    CRE-LLM 在 DSCRE 任务上达到了最先进的（SOTA）性能。此外，我们框架的简单性、灵活性，特别是高效性，使其成为将 LLM 应用于具有更强领域特定性和更具挑战性语义理解的
    DSCRE 任务的有前景方向。
- en: Acknowledgment
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported in part by BUPT Excellent Ph.D. Students Foundation (No.
    CX2023133).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分由 BUPT 优秀博士生基金（编号 CX2023133）资助。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Araci [2019] Dogu Araci. Finbert: Financial sentiment analysis with pre-trained
    language models. CoRR, abs/1908.10063, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Araci [2019] Dogu Araci. Finbert: 使用预训练语言模型进行金融情感分析。CoRR, abs/1908.10063, 2019.'
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 语言模型是少样本学习者。CoRR, abs/2005.14165, 2020.
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. CoRR, abs/2305.14314,
    2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora: 高效的量化 LLM 微调。CoRR, abs/2305.14314, 2023.'
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    4171–4186, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    BERT: 深度双向变换器的预训练用于语言理解。在 Jill Burstein, Christy Doran, 和 Thamar Solorio 编辑的《2019年北美计算语言学协会会议：人类语言技术论文集》，第1卷（长篇和短篇论文），第4171–4186页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。'
- en: 'Du et al. [2022] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 320–335, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. [2022] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, 和 Jie Tang. Glm: 使用自回归空白填充进行通用语言模型预训练。在《第60届计算语言学协会年会论文集（第1卷：长论文）》中，第320–335页，2022年。'
- en: 'Gao et al. [2021] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple
    contrastive learning of sentence embeddings. In Conference on Empirical Methods
    in Natural Language Processing, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. [2021] Tianyu Gao, Xingcheng Yao, 和 Danqi Chen. Simcse: 简单的对比学习句子嵌入。发表于自然语言处理经验方法会议，2021年。'
- en: Geva et al. [2021] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    Transformer feed-forward layers are key-value memories. In Marie-Francine Moens,
    Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
    Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 5484–5495\.
    Association for Computational Linguistics, 2021.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva et al. [2021] Mor Geva, Roei Schuster, Jonathan Berant, 和 Omer Levy. Transformer
    前馈层是关键-值记忆。在 Marie-Francine Moens, Xuanjing Huang, Lucia Specia, 和 Scott Wen-tau
    Yih 编辑的《2021年自然语言处理经验方法会议论文集》，EMNLP 2021，虚拟会议 / 多米尼加共和国蓬塔卡纳，2021年11月7-11日，第5484–5495页。计算语言学协会，2021年。
- en: Gu et al. [2022] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,
    Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific
    language model pretraining for biomedical natural language processing. ACM Trans.
    Comput. Heal., 3(1):2:1–2:23, 2022.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2022] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,
    Xiaodong Liu, Tristan Naumann, Jianfeng Gao, 和 Hoifung Poon. 生物医学自然语言处理的领域特定语言模型预训练。ACM计算健康学报，3(1):2:1–2:23,
    2022。
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. CoRR, abs/2106.09685, 2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配。CoRR, abs/2106.09685,
    2021。'
- en: 'Li et al. [2019] Ziran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng, and Ying Shen.
    Chinese relation extraction with multi-grained information and external linguistic
    knowledge. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings
    of the 57th Conference of the Association for Computational Linguistics, ACL 2019,
    Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4377–4386\.
    Association for Computational Linguistics, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2019] Ziran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng, 和 Ying Shen.
    使用多粒度信息和外部语言知识进行中文关系抽取。在 Anna Korhonen, David R. Traum, 和 Lluís Màrquez 编辑的《第57届计算语言学协会会议论文集》，ACL
    2019，意大利佛罗伦萨，2019年7月28日至8月2日，第1卷：长论文，第4377–4386页。计算语言学协会，2019年。
- en: 'Liu et al. [2021] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks. CoRR, abs/2110.07602, 2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2021] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    和 Jie Tang. P-tuning v2: 提示调优在不同规模和任务中可与微调相媲美。CoRR, abs/2110.07602, 2021。'
- en: 'Lu et al. [2023] Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He,
    Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. Bbt-fin: Comprehensive
    construction of chinese financial domain pre-trained language model, corpus and
    benchmark. CoRR, abs/2302.09432, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu et al. [2023] Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He,
    Yipeng Geng, Mengkun Han, Yingsi Xin, 和 Yanghua Xiao. Bbt-fin: 综合构建中文金融领域的预训练语言模型、语料库和基准。CoRR,
    abs/2302.09432, 2023。'
- en: 'Luo et al. [2023] Haoran Luo, E. Haihong, Zichen Tang, Shiyao Peng, Yikai Guo,
    Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, and Wei Lin. Chatkbqa: A
    generate-then-retrieve framework for knowledge base question answering with fine-tuned
    large language models. ArXiv, abs/2310.08975, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. [2023] Haoran Luo, E. Haihong, Zichen Tang, Shiyao Peng, Yikai Guo,
    Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, 和 Wei Lin. Chatkbqa: 一种生成-检索框架，用于基于知识库的问答，使用微调的大型语言模型。ArXiv,
    abs/2310.08975, 2023。'
- en: 'Mangrulkar et al. [2022] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient
    fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mangrulkar et al. [2022] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, 和 Benjamin Bossan. Peft: 最先进的参数高效微调方法。 [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022。'
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Gpt-4 技术报告，2023。
- en: 'Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed,
    A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
    Information Processing Systems 35: Annual Conference on Neural Information Processing
    Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022,
    2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,
    Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, 和 Ryan Lowe. 训练语言模型以跟随人类反馈的指示。在
    Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, 和 A. Oh 主编的《神经信息处理系统年会
    35：神经信息处理系统大会 2022，NeurIPS 2022》，美国路易斯安那州新奥尔良，2022年11月28日 - 12月9日，2022。
- en: 'Peng et al. [2019] Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning
    in biomedical natural language processing: An evaluation of BERT and elmo on ten
    benchmarking datasets. In Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou,
    and Junichi Tsujii, editors, Proceedings of the 18th BioNLP Workshop and Shared
    Task, BioNLP@ACL 2019, Florence, Italy, August 1, 2019, pages 58–65\. Association
    for Computational Linguistics, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. [2019] Yifan Peng, Shankai Yan, 和 Zhiyong Lu. 生物医学自然语言处理中的迁移学习：BERT
    和 ELMo 在十个基准数据集上的评估。在 Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou,
    和 Junichi Tsujii 主编的《第18届 BioNLP 工作坊与共享任务论文集，BioNLP@ACL 2019》，意大利佛罗伦萨，2019年8月1日，页码
    58–65。计算语言学协会，2019。
- en: Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. CoRR,
    abs/1910.10683, 2019.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 探索统一文本到文本转换器的迁移学习极限。CoRR,
    abs/1910.10683, 2019。
- en: 'Rajpoot and Parikh [2023] Pawan Kumar Rajpoot and Ankur Parikh. Gpt-finre:
    In-context learning for financial relation extraction using large language models.
    CoRR, abs/2306.17519, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajpoot and Parikh [2023] Pawan Kumar Rajpoot 和 Ankur Parikh. Gpt-finre: 使用大型语言模型进行金融关系提取的上下文学习。CoRR,
    abs/2306.17519, 2023。'
- en: 'Sun et al. [2019] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian,
    Hua Wu, and Haifeng Wang. ERNIE 2.0: A continual pre-training framework for language
    understanding. CoRR, abs/1907.12412, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. [2019] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian,
    Hua Wu, 和 Haifeng Wang. ERNIE 2.0: 语言理解的持续预训练框架。CoRR, abs/1907.12412, 2019。'
- en: 'Sun et al. [2021] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang,
    Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua
    Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang,
    Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 3.0: Large-scale knowledge
    enhanced pre-training for language understanding and generation. CoRR, abs/2107.02137,
    2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. [2021] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang,
    Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua
    Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang,
    Dianhai Yu, Hao Tian, Hua Wu, 和 Haifeng Wang. ERNIE 3.0: 大规模知识增强的语言理解与生成预训练。CoRR,
    abs/2107.02137, 2021。'
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen,
    Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
    Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini,
    Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann,
    A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
    Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar
    Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
    Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian,
    Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen,
    Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
    Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini,
    Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann,
    A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
    Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar
    Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
    Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian,
    Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom. Llama 2: 开放基础和微调聊天模型。ArXiv，abs/2307.09288，2023年。'
- en: 'Wan et al. [2023] Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song,
    Jiwei Li, and Sadao Kurohashi. GPT-RE: in-context learning for relation extraction
    using large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,
    Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
    EMNLP 2023, Singapore, December 6-10, 2023, pages 3534–3547\. Association for
    Computational Linguistics, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan et al. [2023] Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song,
    Jiwei Li, 和 Sadao Kurohashi. GPT-RE: 使用大型语言模型进行关系抽取的上下文学习。在 Houda Bouamor、Juan
    Pino 和 Kalika Bali 编辑的《2023年自然语言处理经验方法会议论文集》，EMNLP 2023，新加坡，2023年12月6-10日，第3534–3547页。计算语言学协会，2023年。'
- en: 'Wang et al. [2021] Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding,
    Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu,
    Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao,
    Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng,
    Ge Li, Wen Gao, and Haifeng Wang. ERNIE 3.0 titan: Exploring larger-scale knowledge
    enhanced pre-training for language understanding and generation. CoRR, abs/2112.12731,
    2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2021] Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding,
    Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu,
    Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao,
    Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng,
    Ge Li, Wen Gao 和 Haifeng Wang. ERNIE 3.0 titan: 探索更大规模知识增强的预训练用于语言理解和生成。CoRR，abs/2112.12731，2021年。'
- en: 'Wang et al. [2023] Yequan Wang, Hengran Zhang, Aixin Sun, and Xuying Meng.
    GCRE-GPT: A generative model for comparative relation extraction. CoRR, abs/2303.08601,
    2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023] Yequan Wang, Hengran Zhang, Aixin Sun 和 Xuying Meng. GCRE-GPT:
    一种用于比较关系抽取的生成模型。CoRR，abs/2303.08601，2023年。'
- en: 'Xu et al. [2023] Bin Xu, Shuai Li, Zhaowu Zhang, and Tongxin Liao. BERT-PAGG:
    a chinese relationship extraction model fusing PAGG and entity location information.
    PeerJ Comput. Sci., 9:e1470, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. [2023] Bin Xu, Shuai Li, Zhaowu Zhang 和 Tongxin Liao. BERT-PAGG:
    融合 PAGG 和实体位置信息的中文关系抽取模型。PeerJ Comput. Sci.，9:e1470，2023年。'
- en: 'Yang et al. [2023a] Ai Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian,
    Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang,
    Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang,
    Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song,
    Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
    Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen,
    Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin
    Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou,
    and Zhiying Wu. Baichuan 2: Open large-scale language models. ArXiv, abs/2309.10305,
    2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2023a] Ai Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian,
    Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang,
    Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang,
    Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song,
    Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
    Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen,
    Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin
    Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou,
    和 Zhiying Wu. Baichuan 2: 开放的大规模语言模型. ArXiv, abs/2309.10305, 2023.'
- en: Yang et al. [2023b] Jing Yang, Bin Ji, Shasha Li, Jun Ma, Long Peng, and Jie
    Yu. Dynamic multi-view fusion mechanism for chinese relation extraction. In Hisashi
    Kashima, Tsuyoshi Idé, and Wen-Chih Peng, editors, Advances in Knowledge Discovery
    and Data Mining - 27th Pacific-Asia Conference on Knowledge Discovery and Data
    Mining, PAKDD 2023, Osaka, Japan, May 25-28, 2023, Proceedings, Part I, volume
    13935 of Lecture Notes in Computer Science, pages 405–417\. Springer, 2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023b] Jing Yang, Bin Ji, Shasha Li, Jun Ma, Long Peng, 和 Jie Yu. 中文关系抽取的动态多视图融合机制.
    在 Hisashi Kashima, Tsuyoshi Idé, 和 Wen-Chih Peng 编辑的《知识发现与数据挖掘进展 - 第27届亚太知识发现与数据挖掘会议，PAKDD
    2023，2023年5月25-28日，大阪，日本，会议论文集，第I部分，计算机科学讲义笔记第13935卷，第405–417页》。Springer, 2023.
- en: 'Zhang et al. [2021] Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo,
    Jingyun Hua, Yulong Wang, and Ming Zhou. Mengzi: Towards lightweight yet ingenious
    pre-trained models for chinese. CoRR, abs/2110.06696, 2021.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2021] Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun
    Hua, Yulong Wang, 和 Ming Zhou. Mengzi: 朝向轻量且巧妙的中文预训练模型. CoRR, abs/2110.06696,
    2021.'
- en: 'Zhao et al. [2019] Zhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu, Wei
    Lu, Xi Chen, Haotang Deng, Qi Ju, and Xiaoyong Du. UER: an open-source toolkit
    for pre-training models. In Sebastian Padó and Ruihong Huang, editors, Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019 - System Demonstrations, pages 241–246\.
    Association for Computational Linguistics, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人 [2019] Zhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu, Wei Lu,
    Xi Chen, Haotang Deng, Qi Ju, 和 Xiaoyong Du. UER: 一个用于预训练模型的开源工具包. 在 Sebastian
    Padó 和 Ruihong Huang 编辑的《2019年自然语言处理经验方法会议和第九届国际联合自然语言处理会议，EMNLP-IJCNLP 2019，2019年11月3-7日，香港，中国，系统演示论文集，第241–246页》。计算语言学协会,
    2019.'
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of
    large language models. CoRR, abs/2303.18223, 2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
    Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
    Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,
    Zikang Liu, Peiyu Liu, Jian-Yun Nie, 和 Ji-Rong Wen. 大型语言模型综述. CoRR, abs/2303.18223,
    2023.
