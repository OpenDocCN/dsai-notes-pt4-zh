- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:35:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ObfuscaTune: 对专有大型语言模型在私有数据集上的模糊离线微调与推断'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02960](https://ar5iv.labs.arxiv.org/html/2407.02960)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02960](https://ar5iv.labs.arxiv.org/html/2407.02960)
- en: Ahmed Frikha  Nassim Walha
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmed Frikha  Nassim Walha
- en: Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing Zhou   Huawei Munich
    Research Center
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing Zhou   华为慕尼黑研究中心
- en: ahmed.frikha1@huawei.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ahmed.frikha1@huawei.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This work addresses the timely yet underexplored problem of performing inference
    and finetuning of a proprietary LLM owned by a model provider entity on the confidential/private
    data of another data owner entity, in a way that ensures the confidentiality of
    both the model and the data. Hereby, the finetuning is conducted offsite, i.e.,
    on the computation infrastructure of a third-party cloud provider. We tackle this
    problem by proposing *ObfuscaTune*, a novel, efficient and fully utility-preserving
    approach that combines a simple yet effective obfuscation technique with an efficient
    usage of confidential computing (only $~{}5\%$ of the model parameters are placed
    on TEE). We empirically demonstrate the effectiveness of *ObfuscaTune* by validating
    it on GPT-2 models with different sizes on four NLP benchmark datasets. Finally,
    we compare to a naive version of our approach to highlight the necessity of using
    random matrices with low condition numbers in our approach to reduce errors induced
    by the obfuscation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解决了一个时效性强但未被充分探索的问题，即在确保模型和数据机密性的情况下，对由模型提供方拥有的专有 LLM 在另一个数据拥有方的机密/私有数据上进行推断和微调。为此，微调在异地进行，即在第三方云计算基础设施上。我们通过提出
    *ObfuscaTune*，一种新颖、高效且完全保留效用的方法，来解决这一问题，该方法将简单而有效的模糊技术与高效的机密计算（仅 $~{}5\%$ 的模型参数放置在
    TEE 中）相结合。我们通过在四个 NLP 基准数据集上对不同尺寸的 GPT-2 模型进行验证，实证展示了 *ObfuscaTune* 的有效性。最后，我们与一种简单的版本进行比较，以突出在我们的方法中使用低条件数随机矩阵以减少模糊引起的错误的必要性。
- en: '*ObfuscaTune*: Obfuscated Offsite Fine-tuning and Inference of Proprietary
    LLMs on Private Datasets'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*ObfuscaTune*: 对专有大型语言模型在私有数据集上的模糊离线微调与推断'
- en: Ahmed Frikha  Nassim Walha Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing
    Zhou   Huawei Munich Research Center ahmed.frikha1@huawei.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmed Frikha  Nassim Walha Ricardo Mendes  Krishna Kanth Nakka  Xue Jiang Xuebing
    Zhou   华为慕尼黑研究中心 ahmed.frikha1@huawei.com
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) such as GPT-4 Achiam et al. ([2023](#bib.bib1))
    are increasingly used due to their state-of-the-art performance in diverse tasks
    and productivity benefits Noy and Zhang ([2023](#bib.bib23)). While LLMs excel
    in zero-shot and few-shot predictions with in-context learning Mann et al. ([2020](#bib.bib20)),
    finetuning them on domain-specific data can significantly outperform foundation
    models in tasks like chip designThakur et al. ([2023](#bib.bib26)); Wu et al.
    ([2024](#bib.bib29)); Liu et al. ([2023](#bib.bib16)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如 GPT-4 Achiam 等（[2023](#bib.bib1)），由于其在各种任务中的先进性能和生产力优势，正被越来越广泛地使用
    Noy 和 Zhang（[2023](#bib.bib23)）。虽然 LLMs 在上下文学习中表现出色，能进行零样本和少样本预测 Mann 等（[2020](#bib.bib20)），但在特定领域数据上微调它们可以在芯片设计等任务中显著超越基础模型
    Thakur 等（[2023](#bib.bib26)）；Wu 等（[2024](#bib.bib29)）；Liu 等（[2023](#bib.bib16)）。
- en: Model providers keep their proprietary models private due to the exorbitant
    costs of training them¹¹1Training GPT-4 costed more than $\$100$M Knight ([2023](#bib.bib12)).
    To enable their users to customize or apply the proprietary models to their data,
    model owners provide finetuning and inference services, e.g., OpenAI finetuning
    API²²2https://platform.openai.com/docs/guides/fine-tuning and GitHub Copilot³³3https://docs.github.com/en/copilot
    respectively. Hereby, the users have to share their data with the model owners
    to use these services. Due to concerns of privacy leakage and competitive disadvantage,
    several users and commercial entities are not willing to share their private or
    confidential data. For e.g., Samsung banned the usage of ChatGPT after sensitive
    code was leaked Ray ([2023](#bib.bib25)). Hence, approaches that enable the inference
    and finetuning of proprietary LLMs of one stakeholder on the confidential/private
    data of another stakeholder in a privacy-preserving way are crucially needed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提供者由于训练其专有模型的费用过高而将其保持私密¹¹1训练 GPT-4 的成本超过了 \$100M Knight ([2023](#bib.bib12))。为了使用户能够自定义或将专有模型应用于其数据，模型所有者提供了微调和推理服务，例如，OpenAI
    微调 API²²2https://platform.openai.com/docs/guides/fine-tuning 和 GitHub Copilot³³3https://docs.github.com/en/copilot
    分别。因此，用户必须与模型所有者共享其数据才能使用这些服务。由于担忧隐私泄露和竞争劣势，一些用户和商业实体不愿意共享他们的私人或机密数据。例如，三星在敏感代码泄露后禁止使用
    ChatGPT Ray ([2023](#bib.bib25))。因此，迫切需要能够在保护隐私的方式下，让一个利益相关者对另一个利益相关者的机密/私人数据进行推理和微调的方案。
- en: 'We define the following requirement that potential methods addressing this
    problem must fulfill: (a) Model confidentiality: prevent leakage of the proprietary
    model parameters, (b) Data confidentiality: prevent data leakage, (c) Utility:
    the performance and results of the inference and finetuning should be comparable
    with and without protection, (d) Efficiency: the computational time, memory footprint
    and communication should remain acceptable. To the best of our knowledge, no prior
    work fulfill all of these requirements simultaneously. In the following, we discuss
    different categories of prior works.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了潜在方法必须满足的以下要求：(a) 模型保密性：防止专有模型参数泄露，(b) 数据保密性：防止数据泄露，(c) 实用性：推理和微调的性能和结果应与有无保护时相当，(d)
    效率：计算时间、内存占用和通信应保持在可接受范围内。根据我们所知，目前尚无工作能够同时满足所有这些要求。接下来，我们将讨论不同类别的现有工作。
- en: Prior approaches based on differential privacy (DP) for inference Igamberdiev
    and Habernal ([2023](#bib.bib10)); Majmudar et al. ([2022](#bib.bib19)) and finetuning
    Yu et al. ([2021](#bib.bib31)) focus on protecting the data. However, they do
    not provide any protection for the model parameters and incur significant utility
    losses (Req. (a) and (c) are not fulfilled). Another line of work uses cryptographic
    techniques, e.g., multi-party computation (MPC) and homomorphic encryption (HE)
    Li et al. ([2022](#bib.bib14)); Liu and Liu ([2023](#bib.bib17)). While the confidentiality
    of both the model and the data can be ensured, their substantial slowdown and
    communication costs are not suitable for real-time applications (Req. (d) is not
    fulfilled). Another proposal Xiao et al. ([2023](#bib.bib30)) considers sending
    a distilled version of the model to the client where adapter layers are finetuned
    on the confidential data. At inference time, the finetuned adapter are used in
    combination with the proprietary model on the server side. This approach does
    not protect inference data and leads to utility losses of up to $6\%$ (Req. (b)
    and (c) are not fulfilled). The closest approach to the present work combines
    Trusted Execution Environments (TEE) with a lightweight encryption to address
    federated learning settings Huang et al. ([2024](#bib.bib9)). However, such proposal
    protects only the finetuned LoRA parameters by using the TEE and deploys the proprietary
    LLM on the client-side fully or partially (Req. (a) is not fulfilled).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以差分隐私（DP）为基础的先前方法，如Igamberdiev和Habernal（[2023](#bib.bib10)）；Majmudar等（[2022](#bib.bib19)）以及微调Yu等（[2021](#bib.bib31)），专注于保护数据。然而，它们并未对模型参数提供任何保护，且导致显著的效用损失（Req.
    (a)和(c)未满足）。另一类方法使用加密技术，例如，多方计算（MPC）和同态加密（HE），如Li等（[2022](#bib.bib14)）；Liu和Liu（[2023](#bib.bib17)）。尽管可以确保模型和数据的保密性，但它们的显著延迟和通信成本不适合实时应用（Req.
    (d)未满足）。另一种提案Xiao等（[2023](#bib.bib30)）考虑将模型的精简版本发送给客户端，其中适配层在机密数据上进行微调。在推断时，微调后的适配层与服务器端的专有模型结合使用。这种方法无法保护推断数据，导致高达$6\%$的效用损失（Req.
    (b)和(c)未满足）。与目前工作最接近的方法结合了受信执行环境（TEE）与轻量加密，以应对联邦学习设置，如Huang等（[2024](#bib.bib9)）。然而，这种提案仅通过使用TEE保护微调的LoRA参数，并将专有LLM完全或部分部署在客户端（Req.
    (a)未满足）。
- en: Our contribution in the present work is threefold. First, we propose *ObfuscaTune*,
    a novel and efficient approach that combines TEE with a simple yet effective obfuscation
    technique. Our proposed approach enables finetuning and inference of LLMs in a
    way that preserves the confidentiality of the model and the data with no utility
    loss and acceptable efficiency loss, fulfilling all aforementioned requirements.
    Second, we empirically demonstrate the effectiveness of our method by validating
    it on GPT-2 models with different sizes on four NLP benchmark datasets. Hereby,
    only $5\%$ of the model parameters are placed on TEE. Finally, we highlight the
    necessity of our obfuscation technique by comparing it to a naive obfuscation
    method.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前工作的贡献有三方面。首先，我们提出了*ObfuscaTune*，一种新颖且高效的方法，将TEE与简单而有效的混淆技术结合。我们提出的方法使得在不损失效用且具有可接受的效率损失的情况下，进行LLM的微调和推断，从而满足所有上述要求。其次，我们通过在四个NLP基准数据集上验证不同尺寸的GPT-2模型，实证展示了我们方法的有效性。在此，仅有$5\%$的模型参数被放置在TEE上。最后，我们通过与一种简单混淆方法进行比较，强调了我们混淆技术的必要性。
- en: 2 Method
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 'We consider a problem setting involving three stakeholders: the model provider,
    the data owner and the cloud provider. The objective is to perform inference and
    finetuning of the proprietary LLM of the model provider on the confidential/private
    data of the data owner, in a way that ensures the confidentiality of both the
    model and the data. Due to the high computation and hardware costs required, we
    assume that the finetuning and/or inference is performed offsite, i.e., on the
    computational infrastructure of the cloud provider. We assume that the cloud provider
    is honest-but-curious, i.e., they will perform their task correctly but will try
    to find extra information about the other parties assets and data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个涉及三方利益相关者的问题设置：模型提供者、数据所有者和云提供商。目标是对模型提供者的专有LLM在数据所有者的机密/私人数据上进行推理和微调，以确保模型和数据的保密性。由于所需的高计算和硬件成本，我们假设微调和/或推理是在现场外进行的，即在云提供商的计算基础设施上。我们假设云提供商是诚实但好奇的，即他们会正确执行任务，但会尝试获取其他方的资产和数据的额外信息。
- en: 'To tackle this problem, we propose *ObfuscaTune*, an approach that addresses
    this problem by combining TEE and a simple yet effective obfuscation technique,
    ensuring model and data confidentiality while preserving utility. Following prior
    works, we consider the TEE as an isolated secure zone on a potentially adversary
    host where the data, code and computation processes used are inaccessible from
    outside Hou et al. ([2021](#bib.bib7)); Huang et al. ([2024](#bib.bib9)). Figure [1](#A1.F1
    "Figure 1 ‣ Appendix A Hyperparameters ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning
    and Inference of Proprietary LLMs on Private Datasets")⁴⁴4Will be part of the
    additional page in the camera ready version upon paper acceptance. presents an
    overview of the *ObfuscaTune* approach, which we detail next.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，我们提出了*ObfuscaTune*，这是一种通过结合TEE和简单而有效的混淆技术来解决该问题的方法，确保模型和数据的保密性，同时保持实用性。根据以往的研究，我们将TEE视为一个可能存在对抗性主机上的隔离安全区，在这里数据、代码和计算过程对外部不可访问（Hou
    et al. ([2021](#bib.bib7))；Huang et al. ([2024](#bib.bib9))）。图[1](#A1.F1 "Figure
    1 ‣ Appendix A Hyperparameters ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning and
    Inference of Proprietary LLMs on Private Datasets")⁴⁴4将在论文接受后的相机准备版本中作为附加页面的一部分展示，提供了*ObfuscaTune*方法的概述，我们将进一步详细说明。'
- en: 'The model protection is ensured as follows: the model provider sends the proprietary
    model to the TEE on the cloud provider infrastructure. Within the TEE, the highly
    parameterized attention and MLP layers are protected using our obfuscation technique
    that we detail later and then sent outside the TEE. Since large models do not
    fit inside the TEE, the model layers can be sent there batchwise to be protected
    before leaving it. The remaining low-parameterized layers, e.g., the input, output,
    normalization and dropout layers, are kept on the TEE. After these steps, all
    model parameters are protected, either by TEE or by the obfuscation, and the majority
    of model parameters are outside of the TEE. We note that the TEE is controlled
    by authentication that ensures that only the data owner can query the model. This
    prevents the cloud provider from querying the model to perform model stealing
    Carlini et al. ([2024](#bib.bib5)) or embedding inversion attacks Li et al. ([2023](#bib.bib15));
    Morris et al. ([2023](#bib.bib22)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模型保护的方式如下：模型提供者将专有模型发送到云提供商基础设施上的TEE。在TEE内部，具有高参数化的注意力层和MLP层使用我们稍后详细说明的混淆技术进行保护，然后发送到TEE外部。由于大型模型无法完全放入TEE，因此模型层可以分批发送到TEE进行保护后再离开。其余的低参数化层，例如输入层、输出层、归一化层和丢弃层，保留在TEE内。经过这些步骤，所有模型参数都得到了保护，无论是通过TEE还是混淆方法，大部分模型参数都在TEE外部。我们注意到，TEE由身份验证控制，确保只有数据所有者可以查询模型。这防止了云提供商通过查询模型进行模型窃取（Carlini
    et al. ([2024](#bib.bib5))）或嵌入反演攻击（Li et al. ([2023](#bib.bib15))；Morris et al.
    ([2023](#bib.bib22))）。
- en: 'The data protection in *ObfuscaTune*is conducted as follows: The data owner
    sends an encrypted batch of data directly to the TEE where it is first decrypted
    and then embedded using the model input layer. The resulting embedding is protected
    by our obfuscation method before leaving the TEE. The text tokenization can be
    conducted either before or after transmitting the data on the data owner side
    or in the TEE, respectively.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在*ObfuscaTune*中，数据保护的流程如下：数据所有者将加密的数据批次直接发送到TEE，在那里首先解密，然后使用模型输入层进行嵌入。生成的嵌入在离开TEE之前会通过我们的混淆方法进行保护。文本标记化可以在数据所有者端或TEE中，分别在数据传输之前或之后进行。
- en: 'The obfuscated feedforward pass through one transformer block is executed as
    follows: Outside the TEE, the obfuscated data embedding is passed through the
    obfuscated model layers yielding an obfuscated intermediate embedding that is
    sent back to the TEE. The latter is then de-obfuscated and passed through the
    corresponding model layers on the TEE, depending on the model architecture. Subsequently,
    the resulting embedding is obfuscated again and leaves the TEE to be fed to the
    next transformer block. Finally, the output layer is applied in the TEE and the
    model output is sent back to the data owner (inference case) or used to computed
    the loss on the TEE and perform backpropagation and parameter updates (finetuning
    case).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆的前馈通过一个变换器块的执行如下：在TEE外部，混淆的数据嵌入通过混淆的模型层，产生一个混淆的中间嵌入，然后被送回TEE。后者随后在TEE中解混淆，并通过TEE上的相应模型层，具体取决于模型架构。随后，得到的嵌入再次被混淆并离开TEE，供下一个变换器块使用。最后，输出层在TEE中应用，模型输出被送回数据所有者（推断情况）或用于计算TEE上的损失，并进行反向传播和参数更新（微调情况）。
- en: Our obfuscation method obfuscates the model parameters and data embeddings by
    multiplying them with random matrices that minimize numerical errors. We begin
    by introducing the obfuscation method and later explain how we limit the numerical
    errors. Let’s consider a multi-head attention layer and first focus on a single
    attention head with key, query, value layers parameterized by $W_{k}$ as its input.
    We obfuscate the embedding $X$, $W_{q}$, $W_{q}^{*}$, $W_{q}^{*}$ and $V$, of
    the original non-obfuscated operations (Eq. 1-3). All obfuscation operations are
    applied inside the TEE. The remaining aforementioned operations are performed
    outside of the TEE.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混淆方法通过将模型参数和数据嵌入与随机矩阵相乘来混淆它们，以最小化数值误差。我们首先介绍混淆方法，然后解释如何限制数值误差。考虑一个多头注意力层，首先关注一个单一的注意力头，其输入由$W_{k}$参数化的key、query、value层组成。我们对原始未混淆操作的嵌入$X$、$W_{q}$、$W_{q}^{*}$、$W_{q}^{*}$和$V$进行混淆（公式
    1-3）。所有混淆操作都在TEE内部应用。前述剩余的操作在TEE外部进行。
- en: 'The output $H$ that are obfuscated by another randomly generated random matrix
    $R_{b}$. The bias term of this last projection layer has to be added after de-obfuscation
    and is therefore kept unobfuscated on the TEE. The obfuscation of the MLP layers
    of the proprietary LLM is conducted in an analogous manner to the obfuscation
    of the multi-head attention layers. Fig. [2](#A1.F2 "Figure 2 ‣ Appendix A Hyperparameters
    ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets") shows an overview of all operations conducted in GPT-2 Radford
    et al. ([2019](#bib.bib24)) with annotations of which operations are performed
    inside or outside the TEE and on obfuscated or de-obfuscated variables.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输出$H$被另一个随机生成的随机矩阵$R_{b}$混淆。这个最后投影层的偏置项必须在解混淆后添加，因此在TEE上保持未混淆。专有LLM的MLP层的混淆方式与多头注意力层的混淆方式类似。图
    [2](#A1.F2 "图 2 ‣ 附录 A 超参数 ‣ ObfuscaTune：在私有数据集上对专有LLM进行混淆的离线微调和推断") 显示了GPT-2
    Radford等人（[2019](#bib.bib24)）中进行的所有操作的概述，并标注了哪些操作在TEE内部或外部以及在混淆或解混淆的变量上执行。
- en: Note that using the same or different random matrices to obfuscate different
    transformer blocks does not impact our method. Note that the layers that are kept
    on TEE involve non-linearities, e.g., layer-norm, and therefore cannot be applied
    to obfuscated variables since the subsequent de-obfuscation would not yield the
    same result. These layers have a low number of parameters compared to the attention
    and MLP layers placed outside of TEE, e.g., only ca. $5\%$ are obfuscated and
    placed outside of TEE, in our experiments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用相同或不同的随机矩阵来混淆不同的变换器块不会影响我们的方法。注意，保留在TEE上的层涉及非线性操作，例如layer-norm，因此不能应用于混淆的变量，因为随后的解混淆不会产生相同的结果。这些层的参数数量相比于放置在TEE外的注意力和MLP层要少，例如，在我们的实验中，只有约$5\%$的参数被混淆并放置在TEE外。
- en: '|  | $\displaystyle Q$ |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q$ |  | (1) |'
- en: '|  | $\displaystyle K$ |  | (2) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K$ |  | (2) |'
- en: '|  | $\displaystyle V$ |  | (3) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle V$ |  | (3) |'
- en: '|  | $\displaystyle H$ |  | (4) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H$ |  | (4) |'
- en: '|  | $\displaystyle O^{*}$ |  | (5) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle O^{*}$ |  | (5) |'
- en: '|  | $\displaystyle O$ |  | (6) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle O$ |  | (6) |'
- en: Note that all data embeddings and parameters that are accessible to the adversary,
    i.e., the ones that are processed outside of the TEE, are obfuscated, except for
    the intermediate embeddings $Q$ and the model parameters $W_{k}$, while having
    access to only 4 equations involving them (Eq. 1-3 and Eq. 5). Hence, it is not
    possible to compute them analytically. For an additional layer of protection,
    model obfuscation with new randomly generated matrices can be conducted regularly,
    e.g., every day or every hour, although we believe this is not required. The model
    obfuscation can be performed very efficiently (ca. 10 seconds on a middle range
    GPU for a GPT2-XL model).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有对对手可访问的数据嵌入和参数，即在TEE外部处理的那些，都会被混淆，除了中间嵌入$Q$和模型参数$W_{k}$，同时仅能访问涉及它们的4个方程（方程1-3和方程5）。因此，无法进行解析计算。为了增加额外的保护，可以定期进行模型混淆，例如每天或每小时，尽管我们认为这不是必要的。模型混淆可以非常高效地进行（例如，在中档GPU上对GPT2-XL模型大约需要10秒）。
- en: The minimal error property of our obfuscation method is designed to limit numerical
    errors resulting from the inverse computations of the random matrices as well
    as errors resulting from matrix multiplication between the random matrix and the
    data embeddings or model parameters. We use only orthogonal random matrices, as
    they have the minimum condition number of $1$ matrix computed by applying a $QR$
    is always orthogonal. In this case, the inverse computation is fully error-free
    since the inverse of an orthogonal matrix is its transposed version which is an
    error-free operation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混淆方法具有最小误差特性，旨在限制由于随机矩阵的逆运算以及随机矩阵与数据嵌入或模型参数之间的矩阵乘法导致的数值误差。我们仅使用正交随机矩阵，因为它们的条件数最小为$1$，通过应用$QR$计算的矩阵总是正交的。在这种情况下，逆运算完全无误，因为正交矩阵的逆是其转置矩阵，这是一种无误差的操作。
- en: 3 Experimental evaluation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验评估
- en: 'The conducted experiments aim to address the following key questions: (a) What
    is the impact of applying *ObfuscaTune* on utility, i.e., how do models finetuned
    with *ObfuscaTune* compare to the normally finetuned models? (b) How does our
    obfuscation method using orthogonal random matrices compare to naively using any
    random matrices?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 进行的实验旨在解决以下关键问题：（a）应用*ObfuscaTune*对实用性的影响是什么，即使用*ObfuscaTune*微调的模型与正常微调的模型相比如何？（b）我们的混淆方法使用正交随机矩阵与简单使用任何随机矩阵相比如何？
- en: We apply our method to GPT2 Radford et al. ([2019](#bib.bib24)) models with
    different sizes, ranging from 117 million to 1.5 billion parameters. We implement
    *ObfuscaTune* on top of the nanoGPT implementation Karpathy ([2023](#bib.bib11)).
    All our experiments perform LoRA-finetuning Hu et al. ([2022](#bib.bib8)). Hereby,
    the LoRA parameters are randomly initialized and placed outside of the TEE. We
    apply LoRA to all linear and attention layers. Further hyperparameters are specified
    in the appendix.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法应用于不同大小的GPT2 Radford 等人 ([2019](#bib.bib24)) 模型，参数范围从1.17亿到15亿。我们在nanoGPT实现
    Karpathy ([2023](#bib.bib11)) 上实现了*ObfuscaTune*。我们所有的实验都执行了LoRA微调 Hu 等人 ([2022](#bib.bib8))。在此，LoRA参数是随机初始化的，并放置在TEE外部。我们将LoRA应用于所有线性和注意力层。进一步的超参数在附录中指定。
- en: In each *ObfuscaTune* experiment, we use 2 GPU devices, one that is placed outside
    of TEE and another that simulates the TEE. We believe this is reasonable since
    high-end GPUs have TEE support Apsey et al. ([2023](#bib.bib2)). We evaluate the
    finetuning with *ObfuscaTune* and with a naive version that uses any random matrices
    on 4 question-answering benchmark datasets, including WebQuestions (WebQs) Berant
    et al. ([2013](#bib.bib3)), OpenBookQA (OBQA) Mihaylov et al. ([2018](#bib.bib21)),
    PIQA Bisk et al. ([2020](#bib.bib4)) and SciQ Welbl et al. ([2017](#bib.bib28)).
    We evaluate all models using lm-eval-harness⁵⁵5https://github.com/EleutherAI/lm-evaluation-harness.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个*ObfuscaTune*实验中，我们使用2个GPU设备，一个放置在TEE外部，另一个模拟TEE。我们认为这是合理的，因为高端GPU支持TEE Apsey
    等人 ([2023](#bib.bib2))。我们在4个问答基准数据集上评估*ObfuscaTune*的微调效果以及使用任何随机矩阵的简单版本，包括 WebQuestions
    (WebQs) Berant 等人 ([2013](#bib.bib3))，OpenBookQA (OBQA) Mihaylov 等人 ([2018](#bib.bib21))，PIQA
    Bisk 等人 ([2020](#bib.bib4)) 和 SciQ Welbl 等人 ([2017](#bib.bib28))。我们使用lm-eval-harness⁵⁵5https://github.com/EleutherAI/lm-evaluation-harness评估所有模型。
- en: '| Setting | WebQs | OBQA | PIQA | SciQ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | WebQs | OBQA | PIQA | SciQ |'
- en: '| GPT2-Small |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-Small |'
- en: '| Unprotected | 16.0 | 23.0 | 64.1 | 91.1 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 未保护 | 16.0 | 23.0 | 64.1 | 91.1 |'
- en: '| Protected (random) | 0.0 | 15.4 | 53.1 | 19.7 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（随机） | 0.0 | 15.4 | 53.1 | 19.7 |'
- en: '| Protected (ours) | 16.8 | 23.6 | 64.8 | 91.7 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（我们的） | 16.8 | 23.6 | 64.8 | 91.7 |'
- en: '| GPT2-Medium |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-Medium |'
- en: '| Unprotected | 24.1 | 29.2 | 69.1 | 92.2 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 未保护 | 24.1 | 29.2 | 69.1 | 92.2 |'
- en: '| Protected (random) | 0.0 | 14.4 | 52.0 | 20.0 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（随机） | 0.0 | 14.4 | 52.0 | 20.0 |'
- en: '| Protected (ours) | 24.5 | 28.6 | 68.9 | 92.4 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（我们的） | 24.5 | 28.6 | 68.9 | 92.4 |'
- en: '| GPT2-Large |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-Large |'
- en: '| Unprotected | 30.0 | 35.0 | 72.1 | 93.3 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 未保护 | 30.0 | 35.0 | 72.1 | 93.3 |'
- en: '| Protected (random) | 0.0 | 14.4 | 52.0 | 19.7 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（随机） | 0.0 | 14.4 | 52.0 | 19.7 |'
- en: '| Protected (ours) | 29.7 | 32.2 | 72.3 | 93.0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（我们的） | 29.7 | 32.2 | 72.3 | 93.0 |'
- en: '| GPT2-XL |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL |'
- en: '| Unprotected | 32.4 | 34.2 | 74.1 | 93.5 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 未保护 | 32.4 | 34.2 | 74.1 | 93.5 |'
- en: '| Protected (random) | 0.0 | 14.8 | 52.5 | 20.5 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（随机） | 0.0 | 14.8 | 52.5 | 20.5 |'
- en: '| Protected (ours) | 32.6 | 33.2 | 73.9 | 93.6 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 受保护（我们的） | 32.6 | 33.2 | 73.9 | 93.6 |'
- en: 'Table 1: Test accuracy results (%) yielded by normally finetuned models (unprotected)
    and models which are protected by *ObfuscaTune* as well as a naive version of
    our method that uses an arbitrary random matrix with a non-optimized condition
    number (random).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：正常微调（未保护）模型和通过 *ObfuscaTune* 保护的模型以及使用任意随机矩阵（随机）且条件数未优化的简单方法微调的模型的测试准确率结果（%）。
- en: '| CN | 1 | 8 | 32 | 128 | 160 | random |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| CN | 1 | 8 | 32 | 128 | 160 | 随机 |'
- en: '| Accuracy | 16.8 | 15.5 | 15.2 | 14.7 | 0.3 | 0.0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 16.8 | 15.5 | 15.2 | 14.7 | 0.3 | 0.0 |'
- en: 'Table 2: Test accuracy results (%) yielded by GPT2-small models finetuned on
    WebQs with *ObfuscaTune* using matrices with different condition numbers (CN).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：GPT2-small 模型在 WebQs 上经过 *ObfuscaTune* 微调后的测试准确率结果（%），使用了不同条件数（CN）的矩阵。
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3 Experimental evaluation ‣ ObfuscaTune: Obfuscated
    Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets") presents
    our main experimental results. We find that models finetuned with our method achieve
    a performance comparable to models finetuned without model and data protection.
    This observation is consistent across all model sizes and benchmark datasets.
    Besides, models that are finetuned with a naive method that uses arbitrary random
    matrices incur substantial utility loss due to the high accumulation of errors.
    Furthermore, we evaluate the impact of using random matrices with different condition
    numbers and empirically confirm that higher condition numbers deteriorate performance
    (Tab. [2](#S3.T2 "Table 2 ‣ 3 Experimental evaluation ‣ ObfuscaTune: Obfuscated
    Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets"), details
    in Appendix B).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "表 1 ‣ 3 实验评估 ‣ ObfuscaTune：针对私有数据集的模糊离线微调和推理") 展示了我们的主要实验结果。我们发现，使用我们方法微调的模型表现与未进行模型和数据保护的模型相当。这个观察在所有模型规模和基准数据集中都是一致的。此外，使用任意随机矩阵进行简单方法微调的模型，由于错误积累过多，会导致显著的效用损失。此外，我们评估了使用不同条件数的随机矩阵的影响，并通过实验证实较高的条件数会恶化性能（表
    [2](#S3.T2 "表 2 ‣ 3 实验评估 ‣ ObfuscaTune：针对私有数据集的模糊离线微调和推理")，详细见附录 B）。
- en: We also measure the percentage of model parameters present on TEE after model
    obfuscation to be $\boldsymbol{5.2\%}$ using MPC Knott et al. ([2021](#bib.bib13))
    and $10^{5}$ using HE Lou and Jiang ([2021](#bib.bib18)) with significantly smaller
    models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测量了模型模糊处理后在 TEE 上的模型参数百分比，使用 MPC Knott et al. ([2021](#bib.bib13)) 为 $\boldsymbol{5.2\%}$，使用
    HE Lou 和 Jiang ([2021](#bib.bib18)) 为 $10^{5}$，其中模型显著较小。
- en: 4 Conclusion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: This work tackled the timely but underexplored problem of performing offsite
    inference and finetuning of a proprietary LLM owned by a model provider entity
    on the confidential/private data of another data owner entity, in a way that ensures
    the confidentiality of both the model and the data. Our proposed approach, ObfuscaTune,
    achieves this by combining a simple yet effective obfuscation technique with an
    efficient usage of confidential computing (only $~{}5\%$ of the model parameters
    are placed on TEE). Our extensive empirical evaluation on four NLP benchmark datasets
    and different models highlights the effectiveness of our method and emphasizes
    the importance of using random matrices with low condition numbers for preserving
    high utility. In future work, we will investigate the effectiveness of our approach
    to RAG-systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作解决了一个及时但尚未充分探索的问题，即在确保模型和数据机密性的情况下，对模型提供方拥有的专有LLM进行异地推理和微调。我们提出的方法ObfuscaTune通过将简单有效的混淆技术与高效的机密计算（仅有$~{}5\%$的模型参数放置在TEE中）结合来实现这一目标。我们在四个NLP基准数据集和不同模型上的广泛实证评估突显了我们方法的有效性，并强调了使用低条件数随机矩阵以保持高效用的重要性。在未来的工作中，我们将调查我们方法在RAG系统中的有效性。
- en: 5 Limitations
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 限制
- en: One potential limitation of our work is that despite testing on different models
    and datasets, we focused on the same model architecture, i.e., GPT2\. However,
    most of the other LLMs are composed on the same building blocks, which makes the
    application of our method to them straightforward. Another limitation might be
    that while the slowdown incured by *ObfuscaTune* is substantially lower than other
    technologies, e.g., MPC and HE, it might still be unsuitable for some applications
    where efficiency has a higher importance than privacy
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的一个潜在限制是，尽管在不同的模型和数据集上进行了测试，但我们专注于相同的模型架构，即GPT2。不过，大多数其他LLM也由相同的构建模块组成，这使得我们的方法在它们上的应用变得直接。另一个限制可能是，虽然*ObfuscaTune*造成的延迟明显低于其他技术，如MPC和HE，但在一些对效率比隐私更重要的应用中，它可能仍不适用。
- en: References
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. 2023. GPT-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: Apsey et al. (2023) Emily Apsey, Phil Rogers, Michael O’Connor, and Rob Nertney.
    2023. [Confidential computing on nvidia h100 gpus for secure and trustworthy ai,
    august 2023](https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apsey 等（2023）Emily Apsey, Phil Rogers, Michael O’Connor, 和 Rob Nertney. 2023.
    [针对安全和可信赖的AI在nvidia h100 gpu上的机密计算，2023年8月](https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/)。
- en: Berant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
    2013. Semantic parsing on freebase from question-answer pairs. In *Proceedings
    of the 2013 conference on empirical methods in natural language processing*, pages
    1533–1544.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berant 等（2013）Jonathan Berant, Andrew Chou, Roy Frostig, 和 Percy Liang. 2013.
    从问题-回答对中进行Freebase的语义解析。在*2013年自然语言处理经验方法会议论文集*中，第1533–1544页。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等. 2020. Piqa：在自然语言中推理物理常识。在*AAAI人工智能会议论文集*，第34卷，第7432–7439页。
- en: Carlini et al. (2024) Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham,
    Thomas Steinke, Jonathan Hayase, A Feder Cooper, Katherine Lee, Matthew Jagielski,
    Milad Nasr, Arthur Conmy, et al. 2024. Stealing part of a production language
    model. *arXiv preprint arXiv:2403.06634*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等（2024）Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham,
    Thomas Steinke, Jonathan Hayase, A Feder Cooper, Katherine Lee, Matthew Jagielski,
    Milad Nasr, Arthur Conmy 等. 2024. 盗取部分生产语言模型。*arXiv预印本 arXiv:2403.06634*。
- en: Golub and Van Loan (2013) Gene H Golub and Charles F Van Loan. 2013. *Matrix
    computations*. JHU press.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golub 和 Van Loan（2013）Gene H Golub 和 Charles F Van Loan. 2013. *矩阵计算*。JHU出版社。
- en: 'Hou et al. (2021) Jiahui Hou, Huiqi Liu, Yunxin Liu, Yu Wang, Peng-Jun Wan,
    and Xiang-Yang Li. 2021. Model protection: Real-time privacy-preserving inference
    service for model privacy at the edge. *IEEE Transactions on Dependable and Secure
    Computing*, 19(6):4270–4284.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. (2021) Jiahui Hou, Huiqi Liu, Yunxin Liu, Yu Wang, Peng-Jun Wan,
    和 Xiang-Yang Li. 2021. 模型保护：边缘计算中模型隐私的实时隐私保护推理服务。*IEEE 可靠与安全计算学报*，19(6):4270–4284。
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2022. [LoRA: 大型语言模型的低秩适配](https://openreview.net/forum?id=nZeVKeeFYf9)。发表于
    *国际学习表征会议*。'
- en: Huang et al. (2024) Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan
    Yu, and Lei Wang. 2024. A fast, performant, secure distributed training framework
    for large language model. *arXiv preprint arXiv:2401.09796*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2024) Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan
    Yu, 和 Lei Wang. 2024. 一种快速、高效、安全的大型语言模型分布式训练框架。*arXiv 预印本 arXiv:2401.09796*。
- en: Igamberdiev and Habernal (2023) Timour Igamberdiev and Ivan Habernal. 2023.
    Dp-bart for privatized text rewriting under local differential privacy. *arXiv
    preprint arXiv:2302.07636*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Igamberdiev 和 Habernal (2023) Timour Igamberdiev 和 Ivan Habernal. 2023. Dp-bart
    用于局部差分隐私下的隐私化文本重写。*arXiv 预印本 arXiv:2302.07636*。
- en: Karpathy (2023) Andrej Karpathy. 2023. [nanogpt](https://github.com/karpathy/nanoGPT).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy (2023) Andrej Karpathy. 2023. [nanogpt](https://github.com/karpathy/nanoGPT)。
- en: Knight (2023) Will Knight. 2023. [Openai’s ceo says the age of giant ai models
    is already over, april 2023](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knight (2023) Will Knight. 2023. [Openai 的 CEO 说大型 AI 模型的时代已经结束，2023 年 4 月](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)。
- en: 'Knott et al. (2021) Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta,
    Mark Ibrahim, and Laurens van der Maaten. 2021. Crypten: Secure multi-party computation
    meets machine learning. *Advances in Neural Information Processing Systems*, 34:4961–4973.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Knott et al. (2021) Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta,
    Mark Ibrahim, 和 Laurens van der Maaten. 2021. Crypten: 安全多方计算与机器学习的结合。*神经信息处理系统进展*，34:4961–4973。'
- en: 'Li et al. (2022) Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P Xing,
    and Hao Zhang. 2022. Mpcformer: fast, performant and private transformer inference
    with mpc. *arXiv preprint arXiv:2211.01452*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022) Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P Xing,
    和 Hao Zhang. 2022. Mpcformer: 具有 mpc 的快速、高效和隐私保护的变换器推理。*arXiv 预印本 arXiv:2211.01452*。'
- en: 'Li et al. (2023) Haoran Li, Mingshi Xu, and Yangqiu Song. 2023. Sentence embedding
    leaks more information than you expect: Generative embedding inversion attack
    to recover the whole sentence. *arXiv preprint arXiv:2305.03010*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Haoran Li, Mingshi Xu, 和 Yangqiu Song. 2023. 句子嵌入泄漏的信息超出你的预期：生成嵌入反演攻击以恢复整个句子。*arXiv
    预印本 arXiv:2305.03010*。
- en: 'Liu et al. (2023) Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng,
    Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee,
    Ismet Bayraktaroglu, et al. 2023. Chipnemo: Domain-adapted llms for chip design.
    *arXiv preprint arXiv:2311.00176*.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng,
    Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee,
    Ismet Bayraktaroglu, 等人. 2023. Chipnemo: 适应领域的 llms 用于芯片设计。*arXiv 预印本 arXiv:2311.00176*。'
- en: 'Liu and Liu (2023) Xuanqi Liu and Zhuotao Liu. 2023. Llms can understand encrypted
    prompt: Towards privacy-computing friendly transformers. *arXiv preprint arXiv:2305.18396*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Liu (2023) Xuanqi Liu 和 Zhuotao Liu. 2023. Llms 可以理解加密提示：迈向隐私计算友好的变换器。*arXiv
    预印本 arXiv:2305.18396*。
- en: 'Lou and Jiang (2021) Qian Lou and Lei Jiang. 2021. Hemet: a homomorphic-encryption-friendly
    privacy-preserving mobile neural network architecture. In *International conference
    on machine learning*, pages 7102–7110\. PMLR.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lou 和 Jiang (2021) Qian Lou 和 Lei Jiang. 2021. Hemet: 一种同态加密友好的隐私保护移动神经网络架构。发表于
    *国际机器学习会议*，第 7102–7110 页。PMLR。'
- en: Majmudar et al. (2022) Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami
    Smaili, Rahul Gupta, and Richard Zemel. 2022. Differentially private decoding
    in large language models. *arXiv preprint arXiv:2205.13621*.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majmudar et al. (2022) Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami
    Smaili, Rahul Gupta, 和 Richard Zemel. 2022. 大型语言模型中的差分隐私解码。*arXiv 预印本 arXiv:2205.13621*。
- en: Mann et al. (2020) Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan,
    P Shyam, G Sastry, A Askell, S Agarwal, et al. 2020. Language models are few-shot
    learners. *arXiv preprint arXiv:2005.14165*.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mann 等（2020）Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan,
    P Shyam, G Sastry, A Askell, S Agarwal 等。2020年。《语言模型是少量学习者》。*arXiv 预印本 arXiv:2005.14165*。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open
    book question answering. *arXiv preprint arXiv:1809.02789*.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等（2018）Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal。2018年。《一套盔甲能导电吗？一个用于开放书籍问答的新数据集》。*arXiv
    预印本 arXiv:1809.02789*。
- en: Morris et al. (2023) John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and
    Alexander M Rush. 2023. Text embeddings reveal (almost) as much as text. *arXiv
    preprint arXiv:2310.06816*.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morris 等（2023）John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, 和 Alexander
    M Rush。2023年。《文本嵌入揭示（几乎）与文本一样多的信息》。*arXiv 预印本 arXiv:2310.06816*。
- en: Noy and Zhang (2023) Shakked Noy and Whitney Zhang. 2023. Experimental evidence
    on the productivity effects of generative artificial intelligence. *Science*,
    381(6654):187–192.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noy 和 Zhang（2023）Shakked Noy 和 Whitney Zhang。2023年。《生成型人工智能对生产力影响的实验证据》。*科学*，381（6654）：187–192。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei,
    和 Ilya Sutskever。2019年。《语言模型是无监督的多任务学习者》。
- en: Ray (2023) Siladitya Ray. 2023. [Samsung bans chatgpt among employees after
    sensitive code leak, may 2023](https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray（2023）Siladitya Ray。2023年。[三星因敏感代码泄露禁止员工使用 ChatGPT，2023年5月](https://www.forbes.com/sites/siladityaray/2023/05/02/samsung-bans-chatgpt-and-other-chatbots-for-employees-after-sensitive-code-leak/)。
- en: Thakur et al. (2023) Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce,
    Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023. Benchmarking
    large language models for automated verilog rtl code generation. In *2023 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*, pages 1–6\. IEEE.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur 等（2023）Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin
    Tan, Ramesh Karri, Brendan Dolan-Gavitt, 和 Siddharth Garg。2023年。《大规模语言模型自动化 Verilog
    RTL 代码生成的基准测试》。在 *2023 欧洲设计、自动化与测试会议及展览（DATE）*，第1–6页。IEEE。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin。2017年。《注意力机制是你所需的一切》。*神经信息处理系统进展*，30。
- en: Welbl et al. (2017) Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing
    multiple choice science questions. *arXiv preprint arXiv:1707.06209*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welbl 等（2017）Johannes Welbl, Nelson F Liu, 和 Matt Gardner。2017年。《众包选择题科学问题》。*arXiv
    预印本 arXiv:1707.06209*。
- en: 'Wu et al. (2024) Haoyuan Wu, Zhuolun He, Xinyun Zhang, Xufeng Yao, Su Zheng,
    Haisheng Zheng, and Bei Yu. 2024. Chateda: A large language model powered autonomous
    agent for eda. *IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2024）Haoyuan Wu, Zhuolun He, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng
    Zheng, 和 Bei Yu。2024年。《Chateda：一种基于大规模语言模型的EDA自主代理》。*IEEE 集成电路与系统计算机辅助设计学报*。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-tuning:
    Transfer learning without full model. *arXiv preprint arXiv:2302.04870*.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）Guangxuan Xiao, Ji Lin, 和 Song Han。2023年。《离线调优：无需完整模型的迁移学习》。*arXiv
    预印本 arXiv:2302.04870*。
- en: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A
    Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    et al. 2021. Differentially private fine-tuning of language models. *arXiv preprint
    arXiv:2110.06500*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2021）Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan,
    Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz
    等。2021年。《语言模型的差分隐私微调》。*arXiv 预印本 arXiv:2110.06500*。
- en: Appendix A Hyperparameters
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 超参数
- en: We train all models for 10 epochs. We perform validation at the end of every
    epoch and use early stopping with a patience of 3\. We use a learning rate of
    ${3e-5}$. We did not perform hyperparameter tuning, which highlights the robustness
    of our method. We did all experiments on middle-range GPUs. Each experiment took
    between less than 1 and 8 GPU hours, depending on he model size and dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练所有模型共10个周期。我们在每个周期结束时进行验证，并使用耐心为3的早停法。我们使用的学习率为${3e-5}$。我们没有进行超参数调优，这突显了我们方法的鲁棒性。所有实验均在中档GPU上进行。每个实验耗时在不到1小时到8小时的GPU时间之间，具体取决于模型大小和数据集。
- en: '![Refer to caption](img/b242ce36f31b38604d888a340fee2820.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b242ce36f31b38604d888a340fee2820.png)'
- en: 'Figure 1: Overview of the proposed *ObfuscaTune*, composed by the three stakeholders:
    model provider, which seeks to keep the model confidential, data owner, which
    uses the model (finetuning or inference) while preserving privacy of their data,
    and cloud provider which provides the computation infrastructure, while potentially
    trying to eavesdrop on the data or steal the model. *ObfuscaTune* provides the
    necessary protection by keeping very few components of the model within a TEE,
    and obfuscating the remaining ones, effectively and efficiently preventing data
    or model stealing. This Figure will be part of the additional page in the camera
    ready version upon paper acceptance.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：所提出的*ObfuscaTune*的概述，由三个利益相关者组成：模型提供者，旨在保持模型的机密性；数据所有者，使用模型（微调或推理）同时保护其数据隐私；云提供者提供计算基础设施，同时可能试图窃听数据或窃取模型。*ObfuscaTune*通过将模型的极少数组件保留在TEE内，并对其余部分进行混淆，有效且高效地防止数据或模型被窃取。该图将作为论文接受后的最终版本的附加页面的一部分。
- en: '![Refer to caption](img/f63f0c1be14a8237b441159ec9d06f9c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f63f0c1be14a8237b441159ec9d06f9c.png)'
- en: 'Figure 2: Detailed architecture of the GPT-2 with M layers using *ObfuscaTune*.
    Diagram blocks in green are within the TEE, while the orange are outside the TEE.
    This diagram illustrates how the data is successfully sent from and to the TEE,
    while being obfuscated while outside the TEE. Note that both the input text and
    output text are always within the TEE to prevent inversion attacks. Note that
    the non-activation applied after the first MLP (bottom) is applied on the de-obfuscated
    embedding. The same applies for the softmax non-linear function.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用*ObfuscaTune*的GPT-2的详细架构。绿色的方块表示在TEE内，而橙色的方块表示在TEE外。该图示说明了数据如何成功地从TEE发送到外部，同时在TEE外部时进行混淆。注意，输入文本和输出文本始终在TEE内，以防止反演攻击。注意，第一次MLP（底部）之后应用的非激活操作应用于去混淆的嵌入。softmax非线性函数也是如此。
- en: Appendix B Effect of the condition number
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 条件数的影响
- en: The condition number $\kappa$ measures how much the mapping induced by that
    matrix can stretch vectors and $m=\min{\frac{\|Ax\|}{\|x\|}}$ measures how much
    it can shrink vectors. It determines how much a relative error in the input reflects
    on the output for solving linear systems, matrix inversion or matrix-vector multiplication
    Golub and Van Loan ([2013](#bib.bib6)). Such numerical errors get accumulated
    and increase with the number of sequential matrix multiplication operations, i.e.,
    the deeper the model the higher the accumulated error. We minimize the numerical
    errors by minimizing the condition number of the random matrix.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 条件数$\kappa$衡量矩阵诱导的映射可以将向量拉伸多少，$m=\min{\frac{\|Ax\|}{\|x\|}}$衡量它可以缩小向量多少。它决定了在求解线性系统、矩阵反演或矩阵-向量乘法时输入的相对误差在输出中的反映程度（Golub
    和 Van Loan [2013](#bib.bib6)）。这种数值误差会积累并随着连续矩阵乘法操作的数量增加而增加，即模型越深，累积误差越高。我们通过最小化随机矩阵的条件数来减少数值误差。
- en: In this work, we consider the condition number w.r.t the $\ell_{2}$ for every
    orthogonal matrix $A$. On the other side, from the definition we see that the
    lowest possible $\kappa$ is 1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们考虑了每个正交矩阵$A$相对于$\ell_{2}$的条件数。另一方面，从定义中我们可以看到，最低的$\kappa$值为1。
- en: 'Let $\sigma_{max}(A)$-induced operator norm norm the following holds :'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$\sigma_{max}(A)$-诱导的算子范数，以下条件成立：
- en: '|  | $\&#124;A\&#124;=\max{\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}}=\sigma_{max}(A).$
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;A\&#124;=\max{\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}}=\sigma_{max}(A).$
    |  |'
- en: On the other hand, for A square and non-singular
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于一个方阵且非奇异的矩阵
- en: '|  | $\displaystyle\min_{x}\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}$ |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{x}\frac{\&#124;Ax\&#124;}{\&#124;x\&#124;}$ |  |'
- en: '|  |  | $\displaystyle=\frac{1}{\max_{y}\frac{\&#124;A^{-1}y\&#124;}{\&#124;y\&#124;}}$
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{\max_{y}\frac{\&#124;A^{-1}y\&#124;}{\&#124;y\&#124;}}$
    |  |'
- en: '|  |  | $\displaystyle=\frac{1}{\&#124;A^{-1}\&#124;}$ |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{\&#124;A^{-1}\&#124;}$ |  |'
- en: '|  |  | $\displaystyle=\frac{1}{\sigma_{max}(A^{-1})}=\sigma_{min}(A).$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{\sigma_{max}(A^{-1})}=\sigma_{min}(A).$ |  |'
- en: 'Finally we get for every square and non-singular matrix $A$:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，对于每一个方阵和非奇异矩阵$A$，我们得到：
- en: '|  | $\kappa(A)=\frac{\sigma_{max}(A)}{\sigma_{min}(A)}$ |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\kappa(A)=\frac{\sigma_{max}(A)}{\sigma_{min}(A)}$ |  |'
- en: 'The last equation makes it possible to generate random matrices $R$ using the
    standard normal distribution. We then apply QR-decomposition on $A$. We then choose
    a random positive value for the largest singular value of the final matrix $R$.
    Then we construct the diagonal matrix $S$ to be having the following singular
    value decomposition:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的方程使得可以使用标准正态分布生成随机矩阵$R$。然后我们对$A$应用QR分解。我们选择一个随机正值作为最终矩阵$R$的最大奇异值。接着，我们构造对角矩阵$S$，使其具有以下奇异值分解：
- en: '|  | $R=Q_{A}SQ_{B}.$ |  | (7) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $R=Q_{A}SQ_{B}.$ |  | (7) |'
- en: 'And can calculate $R^{-1}=Q_{B}^{T}S^{-1}Q_{A}^{T}$ with minimal rounding errors.
    We use this approach to generate random matrices of a given condition number and
    monitor the effect of the condition number on the test accuracy of the final model.
    The results are showcased in table [2](#S3.T2 "Table 2 ‣ 3 Experimental evaluation
    ‣ ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs
    on Private Datasets") show indeed that it is curcial to have a low condition number,
    otherwise the training degenerates.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '可以计算$R^{-1}=Q_{B}^{T}S^{-1}Q_{A}^{T}$，并且最小化舍入误差。我们使用这种方法生成具有给定条件数的随机矩阵，并监测条件数对最终模型测试精度的影响。结果展示在表格
    [2](#S3.T2 "Table 2 ‣ 3 Experimental evaluation ‣ ObfuscaTune: Obfuscated Offsite
    Fine-tuning and Inference of Proprietary LLMs on Private Datasets") 中，确实显示低条件数是至关重要的，否则训练会退化。'
