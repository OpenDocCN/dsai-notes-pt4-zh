- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:35:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: On the Client Preference of LLM Fine-tuning in Federated Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于联邦学习中 LLM 微调的客户端偏好
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03038](https://ar5iv.labs.arxiv.org/html/2407.03038)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03038](https://ar5iv.labs.arxiv.org/html/2407.03038)
- en: Feijie Wu¹, Xiaoze Liu¹, Haoyu Wang¹, Xingchen Wang¹, Jing Gao¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Feijie Wu¹, Xiaoze Liu¹, Haoyu Wang¹, Xingchen Wang¹, Jing Gao¹
- en: ¹Purdue University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹普渡大学
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large
    language model (LLM) using preference datasets, enabling the LLM to generate outputs
    that align with human preferences. Given the sensitive nature of these preference
    datasets held by various clients, there is a need to implement RLHF within a federated
    learning (FL) framework, where clients are reluctant to share their data due to
    privacy concerns. To address this, we introduce a feasible framework in which
    clients collaboratively train a binary selector with their preference datasets
    using our proposed FedBis. With a well-trained selector, we can further enhance
    the LLM that generates human-preferred completions. Meanwhile, we propose a novel
    algorithm, FedBiscuit, that trains multiple selectors by organizing clients into
    balanced and disjoint clusters based on their preferences. Compared to the FedBis,
    FedBiscuit demonstrates superior performance in simulating human preferences for
    pairwise completions. Our extensive experiments on federated human preference
    datasets – marking the first benchmark to address heterogeneous data partitioning
    among clients – demonstrate that FedBiscuit outperforms FedBis and even surpasses
    traditional centralized training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工反馈强化学习（RLHF）通过偏好数据集对预训练的大型语言模型（LLM）进行微调，使 LLM 能够生成符合人类偏好的输出。由于这些偏好数据集由多个客户端持有且具有敏感性，因此需要在联邦学习（FL）框架内实施
    RLHF，因为客户端由于隐私问题不愿共享数据。为了解决这个问题，我们引入了一个可行的框架，其中客户端使用我们提出的 FedBis 在其偏好数据集上协作训练一个二元选择器。通过一个训练良好的选择器，我们可以进一步提升生成符合人类偏好的
    LLM。同时，我们提出了一种新算法 FedBiscuit，该算法通过将客户端组织成基于其偏好的平衡且不重叠的集群来训练多个选择器。与 FedBis 相比，FedBiscuit
    在模拟成对完成的人类偏好方面表现出更好的性能。我们在联邦人类偏好数据集上的广泛实验——这是首个针对客户端异质数据分区的基准——证明了 FedBiscuit
    优于 FedBis，甚至超过了传统的集中式训练。
- en: On the Client Preference of LLM Fine-tuning in Federated Learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于联邦学习中 LLM 微调的客户端偏好
- en: Feijie Wu¹, Xiaoze Liu¹, Haoyu Wang¹, Xingchen Wang¹, Jing Gao¹ ¹Purdue University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Feijie Wu¹, Xiaoze Liu¹, Haoyu Wang¹, Xingchen Wang¹, Jing Gao¹ ¹普渡大学
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have demonstrated remarkable capacities in responding
    to a wide range of open-ended instructions as professionally as human beings.
    This achievement is attributed to reinforcement learning with human feedback (RLHF)
    Ziegler et al. ([2019](#bib.bib54)); Christiano et al. ([2017](#bib.bib6)); Ouyang
    et al. ([2022](#bib.bib32)), a method that aligns an LLM with human preferences.
    Specifically, it trains a reward model (a.k.a. preference model) with the help
    of a preference dataset, which includes the comparisons among various completions
    of given instructions. Then, it fine-tunes the LLM towards generating completions
    that closely match human preference, evaluated by the reward model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在回应各种开放性指令时表现出卓越的能力，表现得如同人类一样专业。这一成就归功于人工反馈强化学习（RLHF）Ziegler 等人
    ([2019](#bib.bib54)); Christiano 等人 ([2017](#bib.bib6)); Ouyang 等人 ([2022](#bib.bib32))，这是一种使
    LLM 与人类偏好对齐的方法。具体而言，它利用偏好数据集训练奖励模型（即偏好模型），该数据集包括对给定指令的各种完成选项的比较。然后，它通过奖励模型评估，将
    LLM 微调为生成与人类偏好紧密匹配的完成选项。
- en: While empirical studies have validated the effectiveness of RLHF in enhancing
    LLM performance, RLHF faces a challenge regarding preference data collection.
    There are two approaches for constructing preference datasets, namely, human efforts
    Bai et al. ([2022](#bib.bib4)); Ganguli et al. ([2022](#bib.bib15)); Stiennon
    et al. ([2020](#bib.bib37)) and ChatGPT generation Dubois et al. ([2024](#bib.bib10)).
    The former gathers the preference data from a team of labelers, who rank the completions
    of each instruction from best to worst. In contrast, the latter entails a set
    of instructions together with pairwise completions, with ChatGPT Achiam et al.
    ([2023](#bib.bib1)) tasked with selecting the superior completion for each instruction.
    As LLMs are deployed to serve diverse clients, a preference gap may occur between
    clients and labelers/ChatGPT, impeding the LLM’s ability to generate responses
    that satisfy real clients’ tastes. Therefore, there is a demand for a preference
    dataset that accurately mirrors clients’ preferences in order to facilitate LLM
    performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实证研究验证了RLHF在提升LLM性能方面的有效性，但RLHF在偏好数据收集方面面临挑战。构建偏好数据集有两种方法，即人工努力 Bai et al.
    ([2022](#bib.bib4)); Ganguli et al. ([2022](#bib.bib15)); Stiennon et al. ([2020](#bib.bib37))
    和 ChatGPT生成 Dubois et al. ([2024](#bib.bib10))。前者从标签团队那里收集偏好数据，这些标签员将每个指令的完成结果从最好到最差进行排序。相反，后者则包括一组指令以及成对的完成结果，由ChatGPT
    Achiam et al. ([2023](#bib.bib1)) 负责为每个指令选择更优的完成结果。由于LLMs被部署以服务不同的客户，可能会出现客户和标签员/ChatGPT之间的偏好差距，阻碍LLM生成满足真实客户口味的响应。因此，需要一个准确反映客户偏好的数据集，以促进LLM性能。
- en: One approach to meeting the demand is to collect client preference data and
    build a huge preference dataset, with which an LLM can be fine-tuned on a central
    entity (a.k.a. server). It is noticed that this approach has been applied to a
    recent open project named OASST Köpf et al. ([2024](#bib.bib22)). However, this
    approach may be infeasible because most clients refuse the disclosure of their
    preference data out of privacy concerns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这一需求的一种方法是收集客户偏好数据并构建一个庞大的偏好数据集，用于对LLM进行中央实体（即服务器）上的微调。注意到这种方法已被应用于最近一个名为OASST的开放项目
    Köpf et al. ([2024](#bib.bib22))。然而，这种方法可能不可行，因为大多数客户出于隐私 concerns 拒绝披露他们的偏好数据。
- en: '![Refer to caption](img/86982ecd104e8378ab0c84c5d7a89f6e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/86982ecd104e8378ab0c84c5d7a89f6e.png)'
- en: 'Figure 1: An outline of RLHF in federated learning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：联邦学习中RLHF的概述。
- en: 'To this end, we adopt FedAvg, a conventional federated learning (FL) algorithm
    Konečnỳ et al. ([2016](#bib.bib21)); McMahan et al. ([2017](#bib.bib30)) that
    avoids the collection of clients’ raw data. Specifically, each client trains a
    reward model with their preference data, and the server aggregates the reward
    models into a global model. While the process seems effective, we observe the
    following two limitations during training:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们采用了FedAvg，一种传统的联邦学习（FL）算法 Konečnỳ et al. ([2016](#bib.bib21)); McMahan
    et al. ([2017](#bib.bib30))，它避免了收集客户的原始数据。具体而言，每个客户端使用其偏好数据训练奖励模型，服务器将奖励模型聚合为一个全球模型。尽管这个过程看似有效，但我们在训练过程中观察到以下两个限制：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Excessive Computation Overhead: Conventionally, the reward model is designed
    to output a scalar value for a given prompt and completion Stiennon et al. ([2020](#bib.bib37)).
    Its optimization is based on reward differences between preferred and dispreferred
    completions, i.e., the preferred completions should earn greater rewards than
    the dispreferred ones. As a result, when the optimization involves a data sample
    in training the model, it should simultaneously retain two computation graphs
    in the forward and backward pass, leading to significant computation overhead
    and intensive GPU requirements.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过度的计算开销：传统上，奖励模型被设计为对给定的提示和完成输出一个标量值 Stiennon et al. ([2020](#bib.bib37))。其优化基于偏好和非偏好完成之间的奖励差异，即偏好的完成应获得比非偏好完成更高的奖励。因此，当优化涉及训练模型中的数据样本时，它应同时保留两个计算图进行前向和反向传播，这导致了显著的计算开销和密集的GPU需求。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Degraded Performance: Preference data are heterogeneous among the clients in
    view that the preferences vary among the clients. Consequently, each client trains
    a reward model towards a local minima, deviating from the global optima and resulting
    in a longer training time to converge when compared to the centralized training.
    Moreover, the method is likely to suffer from reward hacking, a phenomenon where
    the reward model heads to a poor performance after several training rounds Askell
    et al. ([2021](#bib.bib2)); Michaud et al. ([2020](#bib.bib31)); Tien et al. ([2022](#bib.bib41));
    Skalse et al. ([2022](#bib.bib36)).'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能下降：由于客户端间的偏好数据异质性，各客户端的偏好不同。因此，每个客户端训练一个奖励模型以达到局部最小值，这与全局最优值偏离，导致相比于集中训练，需要更长时间才能收敛。此外，该方法可能会受到奖励作弊的影响，即奖励模型在经过几轮训练后表现不佳
    Askell et al. ([2021](#bib.bib2)); Michaud et al. ([2020](#bib.bib31)); Tien et
    al. ([2022](#bib.bib41)); Skalse et al. ([2022](#bib.bib36)).
- en: In this paper, we propose to address these two limitations and propose effective
    and computationally efficient methods for preference collection and subsequent
    fine-tuning. We start with a solution that addresses the first limitation. The
    key idea is to train a binary selector, which chooses a superior response between
    two completions under a given instruction. Compared with preference ranking, the
    binary selector requires much less computation during training. Casting binary
    selector training into a federated learning setting, we thus propose the federated
    binary selector training (FedBis), as depicted in the first stage of Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ On the Client Preference of LLM Fine-tuning in Federated
    Learning"). Each client independently trains the selector with local preference
    dataset, and the server aggregates the selectors into a global one and broadcasts
    it to the clients. Afterwards, we utilize the binary selector to enhance the performance
    of LLM. Specifically, we assume the server holds a set of instructions, together
    with pairwise responses generated by an LLM. Then, we build a preference dataset
    with the help of the binary selector and boost the LLM by means of direct preference
    optimization (DPO) Rafailov et al. ([2023](#bib.bib33)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了解决这两个限制的方法，并提出了有效且计算效率高的偏好收集和随后的微调方法。我们从解决第一个限制的方案开始。关键思想是训练一个二元选择器，该选择器在给定的指令下选择两个完成结果中的一个优越响应。与偏好排序相比，二元选择器在训练过程中需要的计算量要少得多。将二元选择器训练转化为联邦学习设置，我们因此提出了联邦二元选择器训练（FedBis），如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning") 的第一阶段所示。每个客户端使用本地偏好数据集独立训练选择器，服务器将选择器汇总为一个全局选择器并广播给客户端。随后，我们利用二元选择器来增强
    LLM 的性能。具体而言，我们假设服务器持有一组指令，以及 LLM 生成的成对响应。然后，我们在二元选择器的帮助下构建一个偏好数据集，并通过直接偏好优化（DPO）Rafailov
    et al. ([2023](#bib.bib33)) 来提升 LLM。
- en: To further address the performance deterioration due to preference heterogeneity
    and reward hacking, we propose a method named FedBis with cluster-wise aggregation
    (FedBiscuit). This method ensembles multiple binary selectors, each trained by
    the clients possessing similar preferences. In light of privacy concerns, which
    prevent explicit sharing of clients’ data, the server intermittently collects
    client losses on all binary selectors. Subsequently, clients are organized into
    disjoint clusters, and when comparing two completions, the one selected by the
    majority of binary selectors is deemed better. The proposed method has two main
    advantages. Firstly, clients with similar preferences jointly train a binary selector,
    moderating data heterogeneity and mitigating performance deterioration. Secondly,
    the method alleviates reward hacking by having numerous binary selectors jointly
    decide on optimal completions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解决因偏好异质性和奖励作弊造成的性能下降，我们提出了一种名为 FedBis 的方法，具有集群聚合功能（FedBiscuit）。该方法集合了多个二元选择器，每个选择器由具有相似偏好的客户端训练。在隐私问题的考虑下，防止明确共享客户端数据，服务器会间歇性地收集所有二元选择器的客户端损失。随后，客户端被组织成不相交的集群，在比较两个完成结果时，由多数二元选择器选择的结果被认为更好。该方法有两个主要优点。首先，相似偏好的客户端共同训练一个二元选择器，从而缓解数据异质性和性能下降。其次，该方法通过多个二元选择器共同决定最优完成结果，缓解了奖励作弊的问题。
- en: Contributions.
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'In this paper, our contributions are highlighted as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的贡献如下：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first feasible framework to achieve
    RLHF in FL. In detail, the framework trains binary selector(s) with clients’ local
    datasets, distills the selector(s) toward an LLM, and boosts LLM performance in
    the meantime. Under this framework, we introduce two methods, i.e., FedBis and
    FedBiscuit.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一个在 FL 中实现 RLHF 的可行框架。具体而言，该框架通过客户端的本地数据集训练二进制选择器，将选择器提炼为 LLM，并在此过程中提升
    LLM 的性能。在此框架下，我们引入了两种方法，即 FedBis 和 FedBiscuit。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Previous works offer a number of human preference datasets, but none of them
    address the FL setting. This is the first work to discuss the possible data partition
    approaches to build a heterogeneous human preference dataset. To this end, we
    introduce a benchmark that includes several human preference datasets suitable
    for FL.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以往的工作提供了许多人类偏好数据集，但没有一个处理 FL 设置。这是首次讨论可能的数据划分方法以构建异质的人类偏好数据集。为此，我们引入了一个基准，其中包括几个适用于
    FL 的人类偏好数据集。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments to demonstrate the performance of the proposed
    FedBis and FedBiscuit. As expected, FedBiscuit demonstrates superior performance
    over FedBis and even surpasses traditional centralized training. Meanwhile, we
    present some insights from the empirical studies.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行广泛的实验以展示所提出的 FedBis 和 FedBiscuit 的性能。正如预期的那样，FedBiscuit 展现出比 FedBis 更优的性能，甚至超越了传统的集中式训练。同时，我们展示了一些来自实证研究的见解。
- en: 2 Related Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM Fine-tuning in FL.
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 在 FL 中的微调。
- en: Recent studies have increasingly focused on fine-tuning large language models
    (LLMs) using federated datasets Sun et al. ([2024](#bib.bib39)); Ye et al. ([2024](#bib.bib49));
    Zhang et al. ([2023a](#bib.bib51)); Yi et al. ([2023](#bib.bib50)); Zhang et al.
    ([2023b](#bib.bib52)). However, these approaches often suffer from high computation
    and communication costs due to the necessity of training and synchronizing the
    model with clients. To mitigate these issues, lightweight methods such as black-box
    fine-tuning Sun et al. ([2023](#bib.bib38)); Lin et al. ([2023](#bib.bib27)) and
    offsite-tuning Wu et al. ([2024](#bib.bib46)); Kuang et al. ([2023](#bib.bib23))
    have emerged. Despite their advancements, these methods primarily focus on fine-tuning
    LLMs for specific downstream tasks, neglecting user preferences in the generated
    responses. To address this gap, our work aims to align LLMs with human preferences
    and introduces a feasible training framework in federated learning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究越来越多地关注于使用联邦数据集对大型语言模型（LLMs）进行微调 Sun 等人 ([2024](#bib.bib39))；Ye 等人 ([2024](#bib.bib49))；Zhang
    等人 ([2023a](#bib.bib51))；Yi 等人 ([2023](#bib.bib50))；Zhang 等人 ([2023b](#bib.bib52))。然而，这些方法通常由于需要与客户端一起训练和同步模型，导致高计算和通信成本。为了缓解这些问题，出现了诸如黑盒微调
    Sun 等人 ([2023](#bib.bib38))；Lin 等人 ([2023](#bib.bib27)) 和离线微调 Wu 等人 ([2024](#bib.bib46))；Kuang
    等人 ([2023](#bib.bib23)) 等轻量化方法。尽管有这些进展，这些方法主要关注于针对特定下游任务的 LLM 微调，忽略了生成响应中的用户偏好。为了解决这个问题，我们的工作旨在使
    LLM 与人类偏好对齐，并提出了一个在联邦学习中可行的训练框架。
- en: Reinforcement Learning with Human Feedback (RLHF).
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有人类反馈的强化学习（RLHF）。
- en: RLHF typically involves supervised fine-tuning, reward modeling, and reward
    optimization, initially popularized by Christiano et al. ([2017](#bib.bib6)).
    Proximal Policy Optimization (PPO) Schulman et al. ([2017](#bib.bib35)) is a common
    RLHF algorithm, yet it struggles with instability, inefficiency, and high resource
    demands Choshen et al. ([2019](#bib.bib5)); Engstrom et al. ([2020](#bib.bib12)).
    These challenges have led to the development of alternative methods, such as Direct
    Preference Optimization (DPO) Rafailov et al. ([2023](#bib.bib33)) and others
    Dong et al. ([2023](#bib.bib9)); Zhao et al. ([2023](#bib.bib53)); Azar et al.
    ([2024](#bib.bib3)); Ethayarajh et al. ([2024](#bib.bib14)); Gulcehre et al. ([2023](#bib.bib18)),
    which offer more stable and efficient solutions. However, these methods typically
    operate within a centralized training framework, where the LLM owner retains control
    over the preference data. In contrast, our work seeks to expand data sources and
    incorporate real user preferences during the fine-tuning of the LLM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF通常涉及监督微调、奖励建模和奖励优化，最初由Christiano等人（[2017](#bib.bib6)）推广。近端策略优化（PPO）由Schulman等人（[2017](#bib.bib35)）提出，是一种常见的RLHF算法，但它在不稳定性、低效率和高资源需求方面存在问题（Choshen等人，[2019](#bib.bib5)；Engstrom等人，[2020](#bib.bib12)）。这些挑战促使了替代方法的发展，例如直接偏好优化（DPO）由Rafailov等人（[2023](#bib.bib33)）提出，以及其他方法（Dong等人，[2023](#bib.bib9)；Zhao等人，[2023](#bib.bib53)；Azar等人，[2024](#bib.bib3)；Ethayarajh等人，[2024](#bib.bib14)；Gulcehre等人，[2023](#bib.bib18)），这些方法提供了更稳定和高效的解决方案。然而，这些方法通常在集中训练框架内运行，LLM拥有者控制着偏好数据。相比之下，我们的工作旨在扩展数据来源，并在LLM微调过程中纳入真实用户的偏好。
- en: '3 FedBis: A Feasible Framework for Achieving RLHF in FL'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 FedBis: 一种在联邦学习中实现RLHF的可行框架'
- en: 'The objective of RLHF is to align a pretrained language model with human preferences.
    RLHF comprises two phases: (i) preference modeling and (ii) reinforcement-learning
    fine-tuning. The first phase aims to develop a model that simulates human preferences
    to select the superior options from numerous pairwise completions. Subsequently,
    the second phase enhances the language model’s performance by creating a preference
    dataset, enabling the model to generate responses preferred by humans. In the
    following, We describe the proposed FedBis that achieves RLHF in FL in the first
    two subsections, followed by a brief discussion of its limitations that motivate
    the proposed FedBiscuit presented in Section [4](#S4 "4 FedBiscuit: FedBis with
    Cluster-wise Aggregation ‣ On the Client Preference of LLM Fine-tuning in Federated
    Learning").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'RLHF的目标是将预训练语言模型与人类偏好对齐。RLHF包括两个阶段：（i）偏好建模和（ii）强化学习微调。第一阶段旨在开发一个模拟人类偏好的模型，以从大量成对完成中选择优越的选项。随后，第二阶段通过创建一个偏好数据集来提升语言模型的性能，使模型能够生成被人类偏好的回应。接下来，我们在前两个小节中描述了提出的FedBis，它在FL中实现了RLHF，然后简要讨论了其限制，这些限制促使了第[4](#S4
    "4 FedBiscuit: FedBis with Cluster-wise Aggregation ‣ On the Client Preference
    of LLM Fine-tuning in Federated Learning")节中提出的FedBiscuit。'
- en: 3.1 Preference Modeling
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 偏好建模
- en: 3.1.1 Problem Formulation.
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 问题定义。
- en: 'In preference modeling, our objective is to train a binary selector using data
    from multiple clients. Consider an FL system with $M$, and we aim to optimize
    the following objectives:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏好建模中，我们的目标是利用来自多个客户端的数据训练一个二元选择器。考虑一个具有$M$个客户端的FL系统，我们旨在优化以下目标：
- en: '|  | $\min_{\phi\in\mathbb{R}^{d}}\quad F(\mathbf{\phi})\overset{\triangle}{=}\sum_{m\in[M]}p_{m}F_{m}(\mathbf{\phi})$
    |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\phi\in\mathbb{R}^{d}}\quad F(\mathbf{\phi})\overset{\triangle}{=}\sum_{m\in[M]}p_{m}F_{m}(\mathbf{\phi})$
    |  | (1) |'
- en: where $F_{m}(\phi)$ holds a set of pairwise data with the size of $n_{m}$ is
    the preferred completion out of the pair of $y_{i,w}$ for training, in which each
    contains the prompt, a pair of completions and preference selection. Apparently,
    this dataset eliminates the position effects, and we can train the selector as
    a classification task. Therefore, we utilize cross-entropy (CE) loss $\ell_{CE}$
    to optimize the selector and formulate the expected loss as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$F_{m}(\phi)$包含一组成对数据，大小为$n_{m}$，这是训练中的成对$y_{i,w}$中的优选完成，其中每对包含提示、成对完成和偏好选择。显然，这个数据集消除了位置效应，我们可以将选择器训练成分类任务。因此，我们利用交叉熵（CE）损失$\ell_{CE}$来优化选择器，并将期望损失表示为
- en: '|  | $1$2 |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: Next, we will discuss how to optimize the selector $\phi$ under the FL scenario.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何在FL场景下优化选择器$\phi$。
- en: 3.1.2 Algorithm Design
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 算法设计
- en: We consider a practical and efficient FL scenario where not all clients but
    only a sampled subsets of clients participate in each communication round Yang
    et al. ([2020](#bib.bib48)). Before the commencement of FL training, we initialize
    the binary selector with a pretrained LLM such as LLaMA-2 Touvron et al. ([2023](#bib.bib42)),
    and set the hyperparameters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个实际且高效的 FL 场景，其中并非所有客户端，而只是每轮通信中采样的子集客户端参与 Yang et al. ([2020](#bib.bib48))。在
    FL 训练开始之前，我们使用预训练的 LLM，如 LLaMA-2 Touvron et al. ([2023](#bib.bib42))，来初始化二元选择器，并设置超参数。
- en: An FL algorithm requires multiple communication rounds and consists of three
    phases in each round, i.e., model broadcast, local training, and global aggregation.
    Following this paradigm, we design FedBis and optimize the selector $\phi$, as
    discussed as follows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 FL 算法需要多轮通信，并且每轮包括三个阶段，即模型广播、本地训练和全局聚合。按照这一范式，我们设计了 FedBis，并优化了选择器 $\phi$，具体讨论如下。
- en: 'Step 1: Model Broadcast.'
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第一步：模型广播。
- en: The server uniformly samples $A$-th communication round, and the server broadcasts
    it to the sampled clients.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器均匀地采样 $A$-th 通信轮次，并将其广播到被采样的客户端。
- en: 'Step 2: Local Training.'
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第二步：本地训练。
- en: 'At this step, client $m\in\mathcal{A}$ iterations, where the update rule between
    consecutive iterations follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，客户端 $m\in\mathcal{A}$ 进行迭代，其中连续迭代之间的更新规则如下：
- en: '|  | $\displaystyle\phi_{r,k+1}^{m}=\phi_{r,k}^{m}-\eta\nabla F_{m}(\phi_{r,k}^{m}),k\in[K]$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi_{r,k+1}^{m}=\phi_{r,k}^{m}-\eta\nabla F_{m}(\phi_{r,k}^{m}),k\in[K]$
    |  | (3) |'
- en: where the gradient $\nabla F_{m}(\phi_{r,k}^{m})$ back to the server.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中梯度 $\nabla F_{m}(\phi_{r,k}^{m})$ 返回到服务器。
- en: 'Step 3: Global Aggregation.'
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第三步：全局聚合。
- en: 'After receiving the local selectors from the sampled clients $\mathcal{A}$,
    the server updates the global selector:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到采样客户端 $\mathcal{A}$ 的本地选择器后，服务器更新全局选择器：
- en: '|  | $\displaystyle\phi_{r+1}=\frac{M}{A}\sum_{m\in\mathcal{A}}p_{m}\phi_{r,K}^{m}.$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi_{r+1}=\frac{M}{A}\sum_{m\in\mathcal{A}}p_{m}\phi_{r,K}^{m}.$
    |  | (4) |'
- en: 'This aggregation method, based on Li et al. ([2019](#bib.bib25)) where the
    clients are uniformly sampled to train a global model, ensures consistency with
    Problem ([1](#S3.E1 "In 3.1.1 Problem Formulation. ‣ 3.1 Preference Modeling ‣
    3 FedBis: A Feasible Framework for Achieving RLHF in FL ‣ On the Client Preference
    of LLM Fine-tuning in Federated Learning")) in mathematical expectation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 Li et al. ([2019](#bib.bib25)) 的这种聚合方法，其中客户端被均匀采样以训练全局模型，确保了与问题 ([1](#S3.E1
    "In 3.1.1 Problem Formulation. ‣ 3.1 Preference Modeling ‣ 3 FedBis: A Feasible
    Framework for Achieving RLHF in FL ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning")) 的一致性，在数学期望上。'
- en: After $R$ that reflects the overall preferences of all clients. The selector
    can then be used to enhance the performance of the LLM, as discussed in the next
    section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $R$ 之后，反映了所有客户的整体偏好。选择器可以用于提升 LLM 的性能，下一节中将讨论。
- en: 3.2 Reinforcement-learning Fine-tuning
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 强化学习微调
- en: 3.2.1 Problem Formulation.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 问题表述。
- en: Traditionally, reinforcement-learning fine-tuning adopts PPO algorithm Schulman
    et al. ([2017](#bib.bib35)) to enhance the performance of an LLM using a reward
    model that can rate a completion Stiennon et al. ([2020](#bib.bib37)); Ouyang
    et al. ([2022](#bib.bib32)); Dai et al. ([2023](#bib.bib8)), which does not fit
    the proposed framework with a binary selector.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，强化学习微调采用 PPO 算法 Schulman et al. ([2017](#bib.bib35)) 来增强 LLM 的性能，使用一个可以对完成项进行评分的奖励模型
    Stiennon et al. ([2020](#bib.bib37)); Ouyang et al. ([2022](#bib.bib32)); Dai
    et al. ([2023](#bib.bib8))，但这不符合使用二元选择器的建议框架。
- en: One practical approach to aligning the LLM with clients’ preferences is to create
    a preference dataset with the help of the binary selector. Suppose the server
    holds a set of instructions $\hat{\mathcal{D}}$ are two completions generated
    by the LLM $\theta$. With this generated dataset, we apply the Direct Preference
    Optimization (DPO) algorithm Rafailov et al. ([2023](#bib.bib33)) to optimize
    the LLM consistent with clients’ preferences, which is formulated as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LLM 与客户偏好对齐的一种实际方法是利用二元选择器创建一个偏好数据集。假设服务器持有一组指令 $\hat{\mathcal{D}}$ 和 LLM
    $\theta$ 生成的两个完成项。使用这个生成的数据集，我们应用 Direct Preference Optimization (DPO) 算法 Rafailov
    et al. ([2023](#bib.bib33)) 来优化 LLM，使其与客户的偏好一致，这个过程可以表示为
- en: '|  | $1$2 |  | (5) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where the DPO loss is $1$2. Next we discuss the specifics of the preference
    data generation and LLM optimization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: DPO 损失为 $1$2。接下来，我们讨论偏好数据生成和 LLM 优化的具体细节。
- en: 3.2.2 Algorithm Design
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 算法设计
- en: 'The reinforcement-learning fine-tuning takes place on the server and includes
    two phases: 1) a preference dataset is created with a pretrained LLM $\theta_{0}$
    from FedBis. 2) LLM is optimized according to the objective defined in Equation
    ([5](#S3.E5 "In 3.2.1 Problem Formulation. ‣ 3.2 Reinforcement-learning Fine-tuning
    ‣ 3 FedBis: A Feasible Framework for Achieving RLHF in FL ‣ On the Client Preference
    of LLM Fine-tuning in Federated Learning")) with the generated dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习微调发生在服务器上，包括两个阶段：1）使用来自 FedBis 的预训练 LLM $\theta_{0}$ 创建一个偏好数据集。2）根据方程 ([5](#S3.E5
    "在 3.2.1 问题表述。 ‣ 3.2 强化学习微调 ‣ 3 FedBis：实现 RLHF 在 FL 中的可行框架 ‣ 客户端对 LLM 微调的偏好"))
    中定义的目标优化 LLM，使用生成的数据集。
- en: 'Step 1: Preference Dataset Generation.'
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 步骤 1：偏好数据集生成。
- en: Suppose the server holds a set of instructions $\hat{\mathcal{D}}$ completions
    $(y_{0},\dots,y_{n-1})\sim\pi_{\theta_{0}}(y|x)$ where $0\leq j<l\leq n-1$ otherwise.
    This process builds the preference dataset $\mathcal{D}_{gen}$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设服务器持有一组指令 $\hat{\mathcal{D}}$ 完成 $(y_{0},\dots,y_{n-1})\sim\pi_{\theta_{0}}(y|x)$，其中
    $0\leq j<l\leq n-1$。这个过程构建了偏好数据集 $\mathcal{D}_{gen}$。
- en: 'Step 2: LLM Fine-tuning.'
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 步骤 2：LLM 微调。
- en: 'With the constructed preference dataset $\mathcal{D}_{gen}$ from $\mathcal{D}_{gen}$,
    and update the LLM using the following rule:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用从 $\mathcal{D}_{gen}$ 构建的偏好数据集 $\mathcal{D}_{gen}$，并使用以下规则更新 LLM：
- en: '|  | $\displaystyle\theta_{t+1}=\theta_{t}-\eta\nabla\mathcal{L}_{DPO}\left(\theta_{t}&#124;x,y_{0},y_{1},i\right),$
    |  | (6) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{t+1}=\theta_{t}-\eta\nabla\mathcal{L}_{DPO}\left(\theta_{t}&#124;x,y_{0},y_{1},i\right),$
    |  | (6) |'
- en: where $\eta$ is given by Rafailov et al. ([2023](#bib.bib33)). In a nutshell,
    we distill the binary selector’s preferences into the LLM, allowing it to function
    as a binary selector itself implicitly.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 由 Rafailov et al. ([2023](#bib.bib33)) 给出。简而言之，我们将二元选择器的偏好提炼到 LLM
    中，使其能够隐式地作为一个二元选择器功能。
- en: 3.3 Discussion
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 讨论
- en: We discuss the limitations of FedBis which motivate us to propose FedBiscuit.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 FedBis 的局限性，这促使我们提出 FedBiscuit。
- en: Preference Heterogeneity.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏好异质性。
- en: A performance gap between FedBis and centralized training could arise from data
    heterogeneity among clients, a common issue in FL. Different from centralized
    training that aggregates all the clients’ data and samples an i.i.d. batch in
    each training round, FedBis samples a subset of clients in each round, with each
    client independently optimizing the model based on their local data. This could
    result in a global aggregation that diverges from the global optimum Karimireddy
    et al. ([2020](#bib.bib20)); Wu et al. ([2023](#bib.bib45)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: FedBis 和集中训练之间的性能差距可能源于客户端之间的数据异质性，这是联邦学习中的一个常见问题。与集中训练不同，集中训练将所有客户端的数据汇总，并在每次训练轮次中采样
    i.i.d. 批次，而 FedBis 在每轮中采样客户端的一个子集，每个客户端根据其本地数据独立优化模型。这可能导致全球汇总偏离全球最优 Karimireddy
    et al. ([2020](#bib.bib20))；Wu et al. ([2023](#bib.bib45))。
- en: Reward Hacking.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 奖励黑客行为。
- en: As demonstrated in experiments, FedBis’s performance improves first but may
    later decline with the increase of training rounds. This phenomenon, known as
    reward hacking, is discussed by Skalse et al. ([2022](#bib.bib36)) as an inevitable
    issue in training a reward proxy model, which is used to enhance the performance
    of a policy model (e.g., LLM). However, we can mitigate this impact by delaying
    the inflection point, allowing the reward proxy model to continue improving performance
    for more training rounds and ultimately achieve a higher rating.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，FedBis 的性能最初会提升，但随着训练轮次的增加可能会下降。这种现象称为奖励黑客行为，Skalse et al. ([2022](#bib.bib36))
    讨论了这是训练奖励代理模型中一个不可避免的问题，该模型用于提高策略模型（例如 LLM）的性能。然而，我们可以通过推迟拐点来缓解这一影响，使奖励代理模型在更多的训练轮次中继续提升性能，并最终达到更高的评分。
- en: '4 FedBiscuit: FedBis with Cluster-wise Aggregation'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 FedBiscuit：具有集群聚合的 FedBis
- en: In this section, we aim to address the aforementioned limitations of FedBis.
    To tackle reward hacking, Eisenstein et al. ([2023](#bib.bib11)) and Coste et al.
    ([2024](#bib.bib7)) introduce a promising approach that trains multiple reward
    models at the same time because aggregation over multiple reward model outputs
    can provide a more robust reward estimate. Furthermore, recognizing that some
    clients may share similar preferences, we employ clustered FL Sattler et al. ([2020](#bib.bib34));
    Ghosh et al. ([2020](#bib.bib16)); Ma et al. ([2023](#bib.bib29)) to group clients
    with similar preferences for joint model training. Notably, these two approaches
    complement each other, inspiring us to combine them into a novel algorithm FedBiscuit
    that simultaneously combats reward hacking and preference heterogeneity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在解决FedBis的上述局限性。为了解决奖励作弊问题，Eisenstein等人 ([2023](#bib.bib11)) 和 Coste等人
    ([2024](#bib.bib7)) 提出了一种有前景的方法，该方法同时训练多个奖励模型，因为对多个奖励模型输出的聚合可以提供更稳健的奖励估计。此外，鉴于一些客户端可能具有相似的偏好，我们采用了聚类联邦学习方法
    Sattler等人 ([2020](#bib.bib34)); Ghosh等人 ([2020](#bib.bib16)); Ma等人 ([2023](#bib.bib29))
    将具有相似偏好的客户端分组进行联合模型训练。值得注意的是，这两种方法相互补充，启发我们将其结合成一种新算法FedBiscuit，以同时应对奖励作弊和偏好异质性。
- en: Problem Formulation.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题表述。
- en: 'In this work, we consider training multiple binary selectors of $U$. To ensure
    that all selectors are trained without bias towards a small specific group, we
    mandate that these selectors be trained using evenly disjoint clusters of clients.
    Additionally, a client’s preference should align more closely with those within
    the same cluster than with those in different clusters. To this end, we can formulate
    the following objective:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们考虑训练多个二元选择器$U$。为了确保所有选择器在训练过程中没有偏向于一个小的特定群体，我们要求这些选择器使用均匀不重叠的客户端簇进行训练。此外，客户端的偏好应与同一簇内的其他客户端更为一致，而与不同簇的客户端相比。为此，我们可以制定以下目标：
- en: '|  | $1$2 |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle s.t.\quad\max\{&#124;M_{u}&#124;\}_{u\in[U]}-\min\{&#124;M_{u}&#124;\}_{u\in[U]}\leq
    1$ |  | (7) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\quad\max\{&#124;M_{u}&#124;\}_{u\in[U]}-\min\{&#124;M_{u}&#124;\}_{u\in[U]}\leq
    1$ |  | (7) |'
- en: where the function $F_{m}$ means a set of clients using the $u$.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中函数$F_{m}$表示使用$u$的一组客户端。
- en: 'Next we explore how the proposed FedBiscuit optimizes Equation ([7](#S4.E7
    "In Problem Formulation. ‣ 4 FedBiscuit: FedBis with Cluster-wise Aggregation
    ‣ On the Client Preference of LLM Fine-tuning in Federated Learning")).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们深入探讨提出的FedBiscuit如何优化方程 ([7](#S4.E7 "问题表述。 ‣ 4 FedBiscuit: 带有簇聚合的 FedBis
    ‣ 联邦学习中 LLM 微调的客户端偏好"))。'
- en: 4.1 Algorithm Design
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 算法设计
- en: 'Section [3.1](#S3.SS1 "3.1 Preference Modeling ‣ 3 FedBis: A Feasible Framework
    for Achieving RLHF in FL ‣ On the Client Preference of LLM Fine-tuning in Federated
    Learning") mentions that a client $m\in[M]$ and a validation set $\mathcal{D}_{m,val}$.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '第[3.1节](#S3.SS1 "3.1 偏好建模 ‣ 3 FedBis: 实现 RLHF 的可行框架 ‣ 联邦学习中 LLM 微调的客户端偏好")提到一个客户端$m\in[M]$和一个验证集$\mathcal{D}_{m,val}$。'
- en: 'The proposed FedBiscuit consists of two phases: 1) We train each selector for
    a couple of rounds so that all $U$ and train each binary selector with a specific
    cluster.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的FedBiscuit包含两个阶段：1）我们训练每个选择器若干轮，使所有$U$和每个二元选择器与一个特定的簇进行训练。
- en: 'Phase 1: Warm-up.'
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第一阶段：预热。
- en: 'In the beginning, we initialize each binary selector $\phi_{u}(u\in[U])$ consecutive
    communication rounds following the steps of FedBis: In each communication round,
    the server samples a subset of client $\mathcal{A}$ iterations using the dataset
    $\mathcal{D}_{m,train}$ by repeating the above steps until all selectors are trained.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们按照FedBis的步骤初始化每个二元选择器$\phi_{u}(u\in[U])$连续通信轮次：在每轮通信中，服务器使用数据集$\mathcal{D}_{m,train}$采样一个客户端子集$\mathcal{A}$，通过重复上述步骤直到所有选择器都训练完成。
- en: The selectors are trained with different data distributions because the clients
    participating in each training round are randomly selected. Consequently, all
    the selectors $\phi_{[U]}$ have distinct model parameters, leading to varied performance
    in terms of final logit output when given an instruction and a pair of completions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器在不同的数据分布下进行训练，因为参与每轮训练的客户端是随机选择的。因此，所有选择器$\phi_{[U]}$具有不同的模型参数，导致在给定指令和一对完成时的最终logit输出表现各异。
- en: 'Phase 2: Clustered FL Training.'
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第二阶段：聚类联邦学习训练。
- en: 'After the first phase, we obtain $U$ using the following four steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段后，我们使用以下四个步骤获得$U$：
- en: 'Step 2.1: Client Grouping. This step is executed every $\tau$. During this
    step, the server broadcasts all selectors $\phi_{[U],r}$ using local validation
    set via $\tau$. The server thereby collects all these losses and adopts a greedy
    clustering approach Sattler et al. ([2020](#bib.bib34)); Ma et al. ([2023](#bib.bib29))
    to assign each client to the selector where they achieve the minimum loss. However,
    an obvious deficiency is an imbalance where some selectors are chosen by many
    clients and others by few. It is noted that the selectors trained with more clients
    achieve remarkable performance, while some may be overfitted to a specific group
    of clients. Therefore, the greedy clustering approach negatively impacts the overall
    performance when building a global preference dataset. To tackle the limitation,
    we propose to balance the clusters using the following steps repeatedly until
    the clients are evenly distributed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第2.1步：客户分组。此步骤每隔$\tau$执行一次。在此步骤中，服务器使用本地验证集通过$\tau$广播所有选择器$\phi_{[U],r}$。服务器因此收集所有这些损失，并采用贪婪的聚类方法Sattler
    et al. ([2020](#bib.bib34)); Ma et al. ([2023](#bib.bib29))将每个客户分配给使其损失最小的选择器。然而，明显的缺陷是某些选择器被许多客户选择，而其他选择器则被少数选择。需要注意的是，训练有更多客户的选择器表现出色，但有些可能过度拟合于特定客户组。因此，贪婪的聚类方法在构建全球偏好数据集时对整体性能产生负面影响。为了解决这一局限性，我们提出通过以下步骤反复平衡集群，直到客户均匀分布：
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Choose the cluster selected by the most clients.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择被最多客户选择的集群。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the cluster can accommodate $n$ clients and reassign the rest to other clusters
    where they achieve suboptimal loss.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果集群可以容纳$n$个客户，将其余客户重新分配到其他集群中，在这些集群中他们达到次优损失。
- en: Finally, we obtain balanced and disjoint clusters. Let a client $m$ rounds.
    After client grouping step, the proposed method proceeds to the following three
    steps as outlined in FedBis.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们获得平衡且不重叠的集群。让一个客户$m$轮次。在客户分组步骤之后，所提方法继续执行FedBis中概述的以下三步。
- en: 'Step 2.2: Model Broadcast. Similar to FedBis, the server samples $A$, the server
    transmits the selector $\phi_{U_{m},r}$ and $\cap_{u\in[U]}\mathcal{A}_{u}=\emptyset$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第2.2步：模型广播。类似于FedBis，服务器采样$A$，服务器传输选择器$\phi_{U_{m},r}$和$\cap_{u\in[U]}\mathcal{A}_{u}=\emptyset$。
- en: 'Step 2.3: Local Training. The client $m\in\mathcal{A}$, and the client pushes
    it to the server.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第2.3步：本地训练。客户$m\in\mathcal{A}$，并将其推送到服务器。
- en: 'Step 2.4: Global Aggregation. The server collects updated selectors from all
    participants $\mathcal{A}$ follows'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第2.4步：全球聚合。服务器从所有参与者$\mathcal{A}$处收集更新的选择器，遵循如下：
- en: '|  | $\displaystyle\phi_{u,r+1}=\left(1-\sum_{m\in\mathcal{A}_{u}}p_{m}\right)\phi_{u,r}+\sum_{m\in\mathcal{A}_{u}}p_{m}\phi_{u,r,K}^{m}$
    |  | (8) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi_{u,r+1}=\left(1-\sum_{m\in\mathcal{A}_{u}}p_{m}\right)\phi_{u,r}+\sum_{m\in\mathcal{A}_{u}}p_{m}\phi_{u,r,K}^{m}$
    |  | (8) |'
- en: 'It is noted that performance degradation occurs when a model is trained by
    clients with time-varying sizes in FedAvg Gu et al. ([2021](#bib.bib17)); Wang
    and Ji ([2023](#bib.bib44)). In other words, Equation ([4](#S3.E4 "In Step 3:
    Global Aggregation. ‣ 3.1.2 Algorithm Design ‣ 3.1 Preference Modeling ‣ 3 FedBis:
    A Feasible Framework for Achieving RLHF in FL ‣ On the Client Preference of LLM
    Fine-tuning in Federated Learning")) is no longer suitable for multi-selector
    aggregation due to the fluctuation in the number of clients training a selector
    in each communication round. Therefore, FedBiscuit adopts a new aggregation rule
    as formulated in Equation ([8](#S4.E8 "In Phase 2: Clustered FL Training. ‣ 4.1
    Algorithm Design ‣ 4 FedBiscuit: FedBis with Cluster-wise Aggregation ‣ On the
    Client Preference of LLM Fine-tuning in Federated Learning")).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '需要注意的是，当模型由具有时间变化大小的客户训练时，性能会下降，FedAvg Gu et al. ([2021](#bib.bib17)); Wang
    and Ji ([2023](#bib.bib44))。换句话说，由于每次通信轮次中训练选择器的客户数量波动，方程([4](#S3.E4 "第3步：全球聚合。
    ‣ 3.1.2算法设计 ‣ 3.1 偏好建模 ‣ 3 FedBis: 实现RLHF的可行框架 ‣ 联邦学习中的LLM微调客户偏好"))不再适用于多选择器聚合。因此，FedBiscuit采用了在方程([8](#S4.E8
    "第2阶段：集群化FL训练。 ‣ 4.1算法设计 ‣ 4 FedBiscuit：具有集群级聚合的FedBis ‣ 联邦学习中的LLM微调客户偏好"))中制定的新聚合规则。'
- en: FedBiscuit finally produces a set of well-trained selectors $\phi_{[U],R}$ and
    he subsequent objective is to enhance LLM performance with the help of these selectors,
    as explored below.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: FedBiscuit最终生成一组训练良好的选择器$\phi_{[U],R}$，随后目标是利用这些选择器提高LLM的性能，如下所述。
- en: Reinforcement-learning Fine-tuning with Multiple Selectors.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强化学习微调与多个选择器。
- en: 'We can leverage the methodology mentioned in Section [3.2](#S3.SS2 "3.2 Reinforcement-learning
    Fine-tuning ‣ 3 FedBis: A Feasible Framework for Achieving RLHF in FL ‣ On the
    Client Preference of LLM Fine-tuning in Federated Learning"), and one of the key
    steps involves constructing a preference dataset incorporating multiple selectors.
    For this, we employ a strategy of majority voting. Given an instruction $x\in\hat{\mathcal{D}}$,
    where $i_{u}\in\{0,1\}$ is favored by the most clients.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以利用第 [3.2](#S3.SS2 "3.2 Reinforcement-learning Fine-tuning ‣ 3 FedBis: A
    Feasible Framework for Achieving RLHF in FL ‣ On the Client Preference of LLM
    Fine-tuning in Federated Learning") 节中提到的方法，其中一个关键步骤是构建一个包含多个选择器的偏好数据集。为此，我们采用了多数投票策略。给定一个指令
    $x\in\hat{\mathcal{D}}$，其中 $i_{u}\in\{0,1\}$ 是最受客户端青睐的。'
- en: '4.2 Discussion: Integration with LoRA'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 讨论：与 LoRA 集成
- en: As all binary selectors are LLM, training them may consume significant communication
    and computation overheads. Besides, multiple LLMs lead to considerable storage
    burdens shouldered by the server. To reduce the costs, we adopt a parameter-efficient
    fine-tuning approach LoRA Hu et al. ([2021](#bib.bib19)), where all binary selectors
    share the same base model while using different adapters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有二元选择器都是 LLM，训练它们可能会消耗大量的通信和计算开销。此外，多个 LLM 导致服务器承担大量存储负担。为了减少成本，我们采用了一种参数高效的微调方法
    LoRA Hu 等人 ([2021](#bib.bib19))，其中所有二元选择器共享相同的基础模型，同时使用不同的适配器。
- en: In comparison with FedBis, FedBiscuit requires extra costs, i.e., $O(MU\lfloor
    R/\tau\rfloor\cdot C)$ is the communication cost of a selector. This is because
    FedBiscuit involves client grouping periodically, unilaterally transferring all
    selectors from the server to the clients. Despite the extra costs, extensive experiments
    demonstrate non-trivial improvement by comparing FedBiscuit with FedBis.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与 FedBis 相比，FedBiscuit 需要额外的成本，即 $O(MU\lfloor R/\tau\rfloor\cdot C)$ 是选择器的通信成本。这是因为
    FedBiscuit 涉及定期的客户端分组，单方面将所有选择器从服务器转移到客户端。尽管存在额外成本，大量实验表明，通过将 FedBiscuit 与 FedBis
    进行比较，效果有显著提升。
- en: 5 Federated Human Preference Benchmark
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 联邦人类偏好基准
- en: This section mainly focuses on how we prepare federated human preference datasets,
    while the next section introduces the experimental setup and analyzes numerical
    results. Specifically, we cover two of the most common NLP tasks, i.e., summarization
    and question-answering. All two datasets are partitioned based on the public datasets,
    and the following subsections will include the details. We will release these
    datasets on HuggingFace soon.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要关注我们如何准备联邦人类偏好数据集，而下一节介绍实验设置并分析数值结果。具体来说，我们涵盖了两种最常见的 NLP 任务，即摘要和问答。所有两个数据集都基于公开数据集进行划分，以下小节将包含详细信息。我们将很快在
    HuggingFace 上发布这些数据集。
- en: Summarization.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: Stiennon et al. ([2020](#bib.bib37)) introduces a summarization dataset that
    consists of Reddit posts with human-written tl;dr Völske et al. ([2017](#bib.bib43)).
    This dataset consists of two parts, one is a pretrained dataset, while the other
    is a dataset with human preference. As suggested by Ouyang et al. ([2022](#bib.bib32)),
    we ensure a post does not appear in both datasets. We assume the pretrained dataset
    is stored on the server side, and 60% of data are served for model pertaining
    such that the model can perform well on summarization. The remaining 40% are used
    for the RLHF process to improve the LLM performance and generate human-preferred
    content. Since the human-preference dataset contains the worker ID, we partition
    the dataset based on the worker ID so that the dataset can be partitioned into
    53 workers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Stiennon 等人 ([2020](#bib.bib37)) 介绍了一个摘要数据集，该数据集包括带有人类撰写的 tl;dr 的 Reddit 帖子
    Völske 等人 ([2017](#bib.bib43))。该数据集分为两部分，一部分是预训练数据集，另一部分是具有人工偏好的数据集。正如 Ouyang
    等人 ([2022](#bib.bib32)) 所建议的，我们确保一个帖子不会出现在两个数据集中。我们假设预训练数据集存储在服务器端，60% 的数据用于模型训练，以便模型在摘要任务上表现良好。剩余的
    40% 用于 RLHF 过程，以提高 LLM 性能并生成人类偏好的内容。由于人类偏好数据集包含工人 ID，我们根据工人 ID 划分数据集，以便将数据集划分为
    53 个工人。
- en: Question-Answering (QA).
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问答 (QA)。
- en: We reconstruct the public dataset SHP, which comprises numerous questions from
    Reddit posts and their corresponding user answers Ethayarajh et al. ([2022](#bib.bib13)).
    The preference indicator is based on the number of likes an answer receives. Given
    that the dataset spans 18 domains, we partition the dataset using a Dirichlet
    distribution with a parameter of 0.3, ensuring that no questions overlap between
    clients. In our experiment, we prepare 300 clients, and Figure [2](#S5.F2 "Figure
    2 ‣ Question-Answering (QA). ‣ 5 Federated Human Preference Benchmark ‣ On the
    Client Preference of LLM Fine-tuning in Federated Learning") visualizes the data
    distribution on the selected clients. For the RLHF process, we use a set of 2.6K
    Reddit questions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重建了公共数据集 SHP，该数据集包含大量来自 Reddit 帖子的提问及其对应的用户回答 Ethayarajh 等人（[2022](#bib.bib13)）。偏好指示基于回答获得的点赞数量。由于数据集跨越了
    18 个领域，我们使用参数为 0.3 的 Dirichlet 分布对数据集进行划分，确保客户端之间的问题不重叠。在我们的实验中，我们准备了 300 个客户端，图
    [2](#S5.F2 "图 2 ‣ 问答（QA）。 ‣ 5 联邦人类偏好基准 ‣ 联邦学习中 LLM 微调的客户端偏好") 可视化了选定客户端上的数据分布。对于
    RLHF 过程，我们使用了一组 2.6K Reddit 问题。
- en: '![Refer to caption](img/1b6a1406c543d9bcafb64bda63208045.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b6a1406c543d9bcafb64bda63208045.png)'
- en: 'Figure 2: Data distribution across different question domains on the selected
    clients.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同问题领域在选定客户端上的数据分布。
- en: 6 Experiments
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 6.1 Experimental Setup
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设置
- en: Model and computation environment.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型和计算环境。
- en: 'We initialize the binary selector(s) using the pretrained LLaMA-2-7B Touvron
    et al. ([2023](#bib.bib42)), configuring the final layer to produce binary outputs
    "A" and "B" only. The LLM chosen for content generation depends on the tasks:
    (i) For the summarization task, we start with LLaMA-2-7B and fine-tune it using
    a pretrained dataset; (ii) For the QA task, we initialize the LLM with Alpaca-7B
    Taori et al. ([2023](#bib.bib40)). To reduce computation efforts, we employ LoRA
    to fine-tune the models. Our implementation, built upon FederatedScope Xie et al.
    ([2023](#bib.bib47)); Kuang et al. ([2023](#bib.bib23)), will soon be available
    on GitHub. The experiments are conducted on machines equipped with two Nvidia
    A100 GPU cards, Intel Xeon Platinum 8369B CPUs, and 256GB RAM.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预训练的 LLaMA-2-7B Touvron 等人（[2023](#bib.bib42)）初始化二分类选择器，将最终层配置为仅生成二分类输出
    "A" 和 "B"。内容生成所选择的 LLM 根据任务的不同而异：（i）对于总结任务，我们以 LLaMA-2-7B 为起点，并使用预训练的数据集进行微调；（ii）对于问答任务，我们使用
    Alpaca-7B Taori 等人（[2023](#bib.bib40)）初始化 LLM。为了减少计算工作量，我们采用 LoRA 对模型进行微调。我们的实现建立在
    FederatedScope Xie 等人（[2023](#bib.bib47)）；Kuang 等人（[2023](#bib.bib23)）的基础上，将很快在
    GitHub 上提供。实验在配备两张 Nvidia A100 GPU 卡、Intel Xeon Platinum 8369B CPU 和 256GB RAM
    的机器上进行。
- en: Evaluation.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估。
- en: 'We evaluate two models produced by our proposed FedBis and FedBiscuit: a binary
    selector and an LLM. We employ different strategies to assess each model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了由我们提出的 FedBis 和 FedBiscuit 生成的两个模型：一个二分类选择器和一个 LLM。我们采用不同的策略来评估每个模型：
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Binary selector: The evaluation includes two metrics: agreement and best-of-$n$
    generated by a task-specific LLM. We then evaluate the average rating for the
    selector’s choices using Auto-J Li et al. ([2023a](#bib.bib24)).'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 二分类选择器：评估包括两个指标：一致性和由特定任务的 LLM 生成的最佳 $n$。然后，我们使用 Auto-J Li 等人（[2023a](#bib.bib24)）评估选择器选择的平均评分。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM: After reinforcement-learning fine-tuning, the LLM is evaluated on its
    ability to generate human-preferred content. This means we can assess the quality
    of the generated texts. For example, given an instruction set, the LLM produces
    one completion per instruction, and Auto-J evaluates the average rating of these
    completions. Furthermore, we compare the generated completions with a reference
    set of responses, annotated by humans or ChatGPT, and calculate a win rate based
    on how often the generated response is superior to the reference one.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM：经过强化学习微调后，LLM 被评估其生成符合人类偏好的内容的能力。这意味着我们可以评估生成文本的质量。例如，给定一组指令，LLM 为每个指令生成一个完成，Auto-J
    评估这些完成的平均评分。此外，我们将生成的完成与一组由人类或 ChatGPT 注释的参考回应进行比较，并基于生成回应优于参考回应的频率计算胜率。
- en: Due to the space limit, the hyperparameter settings are presented in Appendix
    [A](#A1 "Appendix A More Implementation Details ‣ On the Client Preference of
    LLM Fine-tuning in Federated Learning"). Moreover, Appendix [B](#A2 "Appendix
    B More Numerical Results and Analysis ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning") provides real cases to demonstrate the performance of
    the proposed FedBis and FedBiscuit. In particular, Appendix [B.1](#A2.SS1 "B.1
    Numerical Results on QA ‣ Appendix B More Numerical Results and Analysis ‣ On
    the Client Preference of LLM Fine-tuning in Federated Learning") discusses the
    results under the QA task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，超参数设置详见附录 [A](#A1 "附录 A 更多实施细节 ‣ 关于联邦学习中 LLM 微调的客户端偏好")。此外，附录 [B](#A2
    "附录 B 更多数值结果和分析 ‣ 关于联邦学习中 LLM 微调的客户端偏好") 提供了实际案例来展示提出的 FedBis 和 FedBiscuit 的性能。特别地，附录
    [B.1](#A2.SS1 "B.1 QA 任务的数值结果 ‣ 附录 B 更多数值结果和分析 ‣ 关于联邦学习中 LLM 微调的客户端偏好") 讨论了 QA
    任务下的结果。
- en: 6.2 Numerical Results on Summarization
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 摘要的数值结果
- en: '|  | Selector |  | LLM |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 选择器 |  | LLM |'
- en: '|  | Agreement | Best-of-$n$ |  | Rating | Win Rate |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 一致性 | 最佳-$n$ |  | 评分 | 胜率 |'
- en: '| SFT | - | - |  | 5.028 | 29.71% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| SFT | - | - |  | 5.028 | 29.71% |'
- en: '| Centralized | 73.10% | 5.302 |  | 5.688 | 78.89% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 集中式 | 73.10% | 5.302 |  | 5.688 | 78.89% |'
- en: '| FedBis | 70.44% | 5.274 |  | 5.661 | 71.35% |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| FedBis | 70.44% | 5.274 |  | 5.661 | 71.35% |'
- en: '| FedBiscuit | 70.52% | 5.305 |  | 5.703 | 80.65% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| FedBiscuit | 70.52% | 5.305 |  | 5.703 | 80.65% |'
- en: 'Table 1: Performce under summarization task. All values here indicate their
    best performance within 500 communication rounds of training.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：摘要任务下的表现。这里的所有数值均表示在 500 次训练轮次内的最佳表现。
- en: 'In this section, the evaluation data originates from the TL;DR dataset, as
    mentioned in Section [5](#S5 "5 Federated Human Preference Benchmark ‣ On the
    Client Preference of LLM Fine-tuning in Federated Learning"). The dataset comprises
    two disjoint parts: one ranked by a group of labelers for model-generated responses,
    and the other written by users to summarize the key content of a post. We use
    the former to compute the consistency between the selector and the human annotator.
    For the latter, we apply various metrics, including best-of-$n$, rating, and win
    rate. The results are presented in Table [1](#S6.T1 "Table 1 ‣ 6.2 Numerical Results
    on Summarization ‣ 6 Experiments ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning") and Figure [3](#S6.F3 "Figure 3 ‣ 6.2 Numerical Results
    on Summarization ‣ 6 Experiments ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning").'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，评估数据来源于 TL;DR 数据集，如第 [5](#S5 "5 联邦人类偏好基准 ‣ 关于联邦学习中 LLM 微调的客户端偏好") 节所述。该数据集包括两个不重叠的部分：一个由标注员对模型生成的响应进行排序，另一个由用户编写，以总结帖子中的关键内容。我们使用前者来计算选择器与人工标注员之间的一致性。对于后者，我们应用了多种指标，包括最佳-$n$、评分和胜率。结果呈现在表
    [1](#S6.T1 "表 1 ‣ 6.2 摘要的数值结果 ‣ 6 实验 ‣ 关于联邦学习中 LLM 微调的客户端偏好") 和图 [3](#S6.F3 "图
    3 ‣ 6.2 摘要的数值结果 ‣ 6 实验 ‣ 关于联邦学习中 LLM 微调的客户端偏好") 中。
- en: The table shows that conventional centralized training outperforms the proposed
    FedBiscuit in terms of agreement. This is because the agreement evaluation data
    have a similar distribution to the training dataset, as their outputs are generated
    from the same language models and labeled by the same group of labelers Stiennon
    et al. ([2020](#bib.bib37)). Consequently, centralized training performs better
    than the proposed FedBis and FedBiscuit, which are affected by data heterogeneity.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示，传统的集中式训练在一致性方面优于提出的 FedBiscuit。这是因为一致性评估数据的分布与训练数据集相似，因为它们的输出来自相同的语言模型，并由同一组标注员
    Stiennon 等人（[2020](#bib.bib37)）标注。因此，集中式训练的表现优于提出的 FedBis 和 FedBiscuit，因为这两者都受到数据异质性的影响。
- en: However, when evaluating the selectors with datasets generated by a supervised
    fine-tuning model, the proposed FedBiscuit slightly outperforms centralized training.
    These results suggest that a centrally trained selector performs poorly in terms
    of generalization and is prone to overfitting to a specific dataset distribution.
    In contrast, comparing FedBiscuit with FedBis, we find that FedBiscuit mitigates
    data heterogeneity and produces a more robust selection of completion pairs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在用由监督微调模型生成的数据集评估选择器时，提出的 FedBiscuit 的表现略优于集中式训练。这些结果表明，集中式训练的选择器在泛化能力方面表现较差，容易过拟合到特定的数据集分布。相比之下，FedBiscuit
    与 FedBis 的比较发现，FedBiscuit 缓解了数据异质性，并产生了更为稳健的完成对选择。
- en: '![Refer to caption](img/b322d47ac6d9258b86cd13c1c49db945.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b322d47ac6d9258b86cd13c1c49db945.png)'
- en: 'Figure 3: Auto-J rating of best-of-$n$ against communication rounds under summarization
    tasks.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在总结任务中，最佳的$ n $的Auto-J评分与通信轮次的关系。
- en: 'Figure [3](#S6.F3 "Figure 3 ‣ 6.2 Numerical Results on Summarization ‣ 6 Experiments
    ‣ On the Client Preference of LLM Fine-tuning in Federated Learning") illustrates
    the performance trend across communication rounds. As discussed in Section [3.3](#S3.SS3
    "3.3 Discussion ‣ 3 FedBis: A Feasible Framework for Achieving RLHF in FL ‣ On
    the Client Preference of LLM Fine-tuning in Federated Learning"), training a binary
    selector can lead to reward hacking. For both centralized training and FedBis,
    which train a single selector, we observe an inflection point where the selector’s
    performance begins to decline. However, this inflection point has not yet appeared
    in FedBiscuit, allowing it to continuously improve and eventually surpass the
    best performance of centralized training. It is important to note that the warmup
    rounds are included in the communication rounds, which explains FedBiscuit’s initial
    poor performance.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S6.F3 "Figure 3 ‣ 6.2 Numerical Results on Summarization ‣ 6 Experiments
    ‣ On the Client Preference of LLM Fine-tuning in Federated Learning")展示了在通信轮次中的性能趋势。如[3.3](#S3.SS3
    "3.3 Discussion ‣ 3 FedBis: A Feasible Framework for Achieving RLHF in FL ‣ On
    the Client Preference of LLM Fine-tuning in Federated Learning")节中讨论的，训练一个二元选择器可能会导致奖励劫持。对于集中式训练和FedBis（训练单个选择器）而言，我们观察到选择器的性能开始下降的拐点。然而，这一拐点在FedBiscuit中尚未出现，使其能够持续改进并最终超越集中式训练的最佳表现。需要注意的是，热身轮次包含在通信轮次中，这解释了FedBiscuit最初较差的性能。'
- en: 7 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**7 结论**'
- en: 'In this work, we explore a feasible framework to achieve RLHF in FL. Specifically,
    we train a binary selector across different clients using their local preference
    datasets, and then use the well-trained selector to align an LLM with human preferences.
    We propose two approaches to enable selector training: FedBis and FedBiscuit.
    FedBis provides a framework to train a single selector, while FedBiscuit ensembles
    multiple selectors to more robustly simulate human preferences. With the proposed
    federated human preference datasets, we conduct empirical studies to validate
    our statements and demonstrate the superiority of FedBiscuit.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们探索了一个实现FL中RLHF的可行框架。具体来说，我们使用不同客户的本地偏好数据集训练一个二元选择器，然后利用经过良好训练的选择器使LLM与人类偏好对齐。我们提出了两种方法来实现选择器训练：FedBis和FedBiscuit。FedBis提供了一个训练单个选择器的框架，而FedBiscuit则集合了多个选择器，以更稳健地模拟人类偏好。通过提出的联邦人类偏好数据集，我们进行了实证研究以验证我们的陈述，并展示了FedBiscuit的优越性。
- en: Ethics Statement
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**伦理声明**'
- en: This paper investigates clients’ preferences using a publicly available dataset,
    ensuring that all data sources are appropriately cited to maintain academic integrity
    and transparency. By leveraging this public dataset, we avoid using private or
    sensitive client data, thus upholding ethical standards in data usage and research
    practices. Furthermore, this work prioritizes the protection of clients’ privacy
    and strictly avoids any disclosure of local data. When clients utilize their own
    data to fine-tune the model, robust privacy measures are in place to ensure that
    no other clients can access or infer any information related to their data. This
    approach not only safeguards individual privacy but also fosters trust and security
    in the application of the model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本文利用一个公开的数据集调查客户的偏好，确保所有数据来源都得到适当引用，以保持学术诚信和透明度。通过利用这个公开的数据集，我们避免使用私人或敏感的客户数据，从而维护数据使用和研究实践的伦理标准。此外，本研究优先保护客户的隐私，严格避免任何地方数据的泄露。当客户利用自己的数据来微调模型时，采用了强有力的隐私保护措施，以确保其他客户无法访问或推测与其数据相关的任何信息。这种方法不仅保护了个人隐私，而且在模型应用中建立了信任和安全。
- en: Limitations
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**局限性**'
- en: One notable limitation of our work lies in the construction of the preference
    dataset, which relies solely on publicly available data rather than gathering
    information directly from real clients. By doing so, we miss out on the nuances
    and intricacies of individual preferences that can only be captured through firsthand
    data collection. As a result, our dataset may lack the depth and breadth necessary
    to fully comprehend the true heterogeneity of preferences among clients. Without
    access to authentic client data, we may inadvertently overlook important variations
    in preferences, potentially limiting the applicability and robustness of our findings.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的一个显著局限在于偏好数据集的构建，它完全依赖于公开可用的数据，而不是直接从真实客户那里收集信息。这样做导致我们错过了只能通过一手数据收集捕捉的个人偏好细微差别和复杂性。因此，我们的数据集可能缺乏全面理解客户偏好的真正异质性所需的深度和广度。没有真实客户数据，我们可能无意中忽略了偏好的重要变异，这可能限制了我们发现的适用性和稳健性。
- en: Another limitation pertains to the use of a task-specific dataset rather than
    a more generalized one encompassing a broader spectrum of tasks. While task-specific
    datasets offer advantages such as focused analysis and tailored insights, they
    may also restrict the scope of our research and hinder its generalizability. By
    incorporating a more diverse range of tasks into our dataset, we could gain a
    more comprehensive understanding of clients’ preferences across various domains,
    thereby enhancing the versatility and validity of our findings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个局限在于使用特定任务的数据集，而不是包含更广泛任务的通用数据集。尽管特定任务的数据集提供了如聚焦分析和量身定制见解等优点，但它们也可能限制了我们研究的范围并阻碍其普遍性。通过将更多样化的任务纳入我们的数据集，我们可以更全面地了解客户在各种领域的偏好，从而提升我们发现的多样性和有效性。
- en: Additionally, our work employs a binary selector that implicitly assumes one
    response is superior to another, overlooking scenarios where responses may exhibit
    similar levels of quality. This oversimplified approach fails to leverage valuable
    data that could provide valuable insights into subtle differences and nuances
    in preferences. By adopting a more nuanced and inclusive framework that acknowledges
    and incorporates variations in response quality, we could extract richer insights
    and make more informed decisions regarding client preferences. Addressing these
    limitations could bolster the robustness and validity of our research, ultimately
    enhancing its relevance and impact in real-world applications.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的工作采用了一个二元选择器，隐含地假设一个响应优于另一个，忽视了响应可能表现出类似质量水平的情况。这种过于简化的方法未能利用可能提供微妙差异和偏好细微差别的宝贵数据。通过采用一个更加细致和包容的框架，承认并纳入响应质量的变化，我们可以提取更丰富的见解，并对客户偏好做出更明智的决策。解决这些局限性可以增强我们研究的稳健性和有效性，*最终*提升其在现实应用中的相关性和影响力。
- en: References
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat等。2023. GPT-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: Askell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al.
    2021. A general language assistant as a laboratory for alignment. *arXiv preprint
    arXiv:2112.00861*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell等（2021）Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli,
    Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma等。2021. 将通用语言助手作为对齐实验室。*arXiv预印本
    arXiv:2112.00861*。
- en: Azar et al. (2024) Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot,
    Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. 2024. A general
    theoretical paradigm to understand learning from human preferences. In *International
    Conference on Artificial Intelligence and Statistics*, pages 4447–4455\. PMLR.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azar等（2024）Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos,
    Mark Rowland, Michal Valko, 和 Daniele Calandriello. 2024. 理解来自人类偏好的学习的通用理论范式。发表于*国际人工智能与统计会议*，第4447–4455页。PMLR。
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    2022. Training a helpful and harmless assistant with reinforcement learning from
    human feedback. *arXiv preprint arXiv:2204.05862*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等.
    2022. 通过人类反馈的强化学习训练一个有用且无害的助手。*arXiv 预印本 arXiv:2204.05862*。
- en: Choshen et al. (2019) Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend.
    2019. On the weaknesses of reinforcement learning for neural machine translation.
    *arXiv preprint arXiv:1907.01752*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choshen et al. (2019) Leshem Choshen, Lior Fox, Zohar Aizenbud, 和 Omri Abend.
    2019. 关于神经机器翻译中强化学习的弱点。*arXiv 预印本 arXiv:1907.01752*。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, 和 Dario Amodei. 2017. 从人类偏好中进行深度强化学习。*神经信息处理系统进展*，30。
- en: Coste et al. (2024) Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
    2024. [Reward model ensembles help mitigate overoptimization](https://openreview.net/forum?id=dcjtMYkpXx).
    In *The Twelfth International Conference on Learning Representations*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coste et al. (2024) Thomas Coste, Usman Anwar, Robert Kirk, 和 David Krueger.
    2024. [奖励模型集成帮助减轻过度优化](https://openreview.net/forum?id=dcjtMYkpXx)。在 *第十二届国际学习表征会议*。
- en: 'Dai et al. (2023) Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu,
    Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023. Safe rlhf: Safe reinforcement
    learning from human feedback. *arXiv preprint arXiv:2310.12773*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai et al. (2023) Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu,
    Mickel Liu, Yizhou Wang, 和 Yaodong Yang. 2023. Safe rlhf: 从人类反馈中安全地进行强化学习。*arXiv
    预印本 arXiv:2310.12773*。'
- en: 'Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie
    Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft:
    Reward ranked finetuning for generative foundation model alignment. *arXiv preprint
    arXiv:2304.06767*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie
    Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, 和 Tong Zhang. 2023. Raft:
    奖励排序微调用于生成基础模型对齐。*arXiv 预印本 arXiv:2304.06767*。'
- en: 'Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang,
    Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.
    2024. Alpacafarm: A simulation framework for methods that learn from human feedback.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang,
    Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, 和 Tatsunori B Hashimoto.
    2024. Alpacafarm: 一个从人类反馈中学习的方法的仿真框架。*神经信息处理系统进展*，36。'
- en: Eisenstein et al. (2023) Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, et al. 2023. Helping or herding? reward model ensembles mitigate
    but do not eliminate reward hacking. *arXiv preprint arXiv:2312.09244*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenstein et al. (2023) Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, 等. 2023. 帮助还是驱使？奖励模型集成减轻但未消除奖励黑客攻击。*arXiv 预印本 arXiv:2312.09244*。
- en: 'Engstrom et al. (2020) Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris
    Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. 2020. Implementation
    matters in deep policy gradients: A case study on ppo and trpo. *arXiv preprint
    arXiv:2005.12729*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engstrom et al. (2020) Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris
    Tsipras, Firdaus Janoos, Larry Rudolph, 和 Aleksander Madry. 2020. 在深度策略梯度中，实施至关重要：以ppo和trpo为案例研究。*arXiv
    预印本 arXiv:2005.12729*。
- en: Ethayarajh et al. (2022) Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.
    2022. Understanding dataset difficulty with $\mathcal{V}$-usable information.
    In *International Conference on Machine Learning*, pages 5988–6008\. PMLR.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ethayarajh et al. (2022) Kawin Ethayarajh, Yejin Choi, 和 Swabha Swayamdipta.
    2022. 使用$\mathcal{V}$-可用信息理解数据集难度。在 *国际机器学习会议*，第5988–6008页。PMLR。
- en: 'Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan
    Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization.
    *arXiv preprint arXiv:2402.01306*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ethayarajh et al. (2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan
    Jurafsky, 和 Douwe Kiela. 2024. Kto: 作为前景理论优化的模型对齐。*arXiv 预印本 arXiv:2402.01306*。'
- en: 'Ganguli et al. (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell,
    Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
    et al. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors,
    and lessons learned. *arXiv preprint arXiv:2209.07858*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganguli 等人（2022） Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell,
    Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
    等人. 2022. 针对语言模型进行红队测试以减少危害：方法、扩展行为和经验教训。*arXiv 预印本 arXiv:2209.07858*。
- en: Ghosh et al. (2020) Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran.
    2020. An efficient framework for clustered federated learning. *Advances in Neural
    Information Processing Systems*, 33:19586–19597.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh 等人（2020） Avishek Ghosh, Jichan Chung, Dong Yin, 和 Kannan Ramchandran.
    2020. 一个高效的集群联邦学习框架。*神经信息处理系统进展*, 33:19586–19597。
- en: Gu et al. (2021) Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang.
    2021. Fast federated learning in the presence of arbitrary device unavailability.
    *Advances in Neural Information Processing Systems*, 34:12052–12064.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2021） Xinran Gu, Kaixuan Huang, Jingzhao Zhang, 和 Longbo Huang. 2021.
    在任意设备不可用的情况下的快速联邦学习。*神经信息处理系统进展*, 34:12052–12064。
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulcehre 等人（2023） Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia
    Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen
    Wang, Chenjie Gu, 等人. 2023. 语言建模的强化自训练（rest）。*arXiv 预印本 arXiv:2308.08998*。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2021） Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2021. Lora：大语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*。
- en: 'Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
    Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic
    controlled averaging for federated learning. In *International Conference on Machine
    Learning*, pages 5132–5143\. PMLR.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimireddy 等人（2020） Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank
    Reddi, Sebastian Stich, 和 Ananda Theertha Suresh. 2020. Scaffold：用于联邦学习的随机控制平均。*国际机器学习大会*,
    页码 5132–5143\. PMLR。
- en: 'Konečnỳ et al. (2016) Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
    Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies for
    improving communication efficiency. *arXiv preprint arXiv:1610.05492*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konečnỳ 等人（2016） Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
    Ananda Theertha Suresh, 和 Dave Bacon. 2016. 联邦学习：提高通信效率的策略。*arXiv 预印本 arXiv:1610.05492*。
- en: Köpf et al. (2024) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. 2024. Openassistant conversations-democratizing
    large language model alignment. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等人（2024） Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
    Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd
    Nagyfi, 等人. 2024. Openassistant Conversations——民主化的大语言模型对齐。*神经信息处理系统进展*, 36。
- en: 'Kuang et al. (2023) Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei
    Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.
    Federatedscope-llm: A comprehensive package for fine-tuning large language models
    in federated learning. *arXiv preprint arXiv:2309.00363*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuang 等人（2023） Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao,
    Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, 和 Jingren Zhou. 2023. Federatedscope-llm:
    一个全面的联邦学习大语言模型微调包。*arXiv 预印本 arXiv:2309.00363*。'
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a） Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, 和 Pengfei
    Liu. 2023a. 用于评估对齐的生成性判别器。*arXiv 预印本 arXiv:2310.05470*。
- en: Li et al. (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua
    Zhang. 2019. On the convergence of fedavg on non-iid data. *arXiv preprint arXiv:1907.02189*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2019） Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, 和 Zhihua Zhang.
    2019. 关于 FedAvg 在非独立同分布数据上的收敛性。*arXiv 预印本 arXiv:1907.02189*。
- en: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Xuechen Li、Tianyi Zhang、Yann Dubois、Rohan Taori、Ishaan Gulrajani、Carlos
    Guestrin、Percy Liang 和 Tatsunori B. Hashimoto。2023b年。Alpacaeval：一个自动评估指令跟随模型的工具。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。
- en: Lin et al. (2023) Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen,
    and Dacheng Tao. 2023. Efficient federated prompt tuning for black-box large pre-trained
    models. *arXiv preprint arXiv:2310.03123*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) Zihao Lin、Yan Sun、Yifan Shi、Xueqian Wang、Lifu Huang、Li Shen
    和 Dacheng Tao。2023年。针对黑箱大规模预训练模型的高效联邦提示调优。*arXiv 预印本 arXiv:2310.03123*。
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter (2017) Ilya Loshchilov 和 Frank Hutter。2017年。解耦权重衰减正则化。*arXiv
    预印本 arXiv:1711.05101*。
- en: Ma et al. (2023) Jie Ma, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi
    Zhang. 2023. Structured federated learning through clustered additive modeling.
    *Advances in Neural Information Processing Systems*, 36.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Jie Ma、Tianyi Zhou、Guodong Long、Jing Jiang 和 Chengqi Zhang。2023年。通过集群加法建模的结构化联邦学习。*神经信息处理系统进展*，第36卷。
- en: McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*, pages 1273–1282\.
    PMLR.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan et al. (2017) Brendan McMahan、Eider Moore、Daniel Ramage、Seth Hampson
    和 Blaise Aguera y Arcas。2017年。从去中心化数据中高效通信学习深度网络。发表于 *人工智能与统计*，第1273–1282页。PMLR。
- en: Michaud et al. (2020) Eric J Michaud, Adam Gleave, and Stuart Russell. 2020.
    Understanding learned reward functions. *arXiv preprint arXiv:2012.05862*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michaud et al. (2020) Eric J Michaud、Adam Gleave 和 Stuart Russell。2020年。理解学习的奖励函数。*arXiv
    预印本 arXiv:2012.05862*。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等。2022年。训练语言模型以跟随指令和人类反馈。*神经信息处理系统进展*，35:27730–27744。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2023. [Direct preference optimization:
    Your language model is secretly a reward model](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 36, pages 53728–53741\.
    Curran Associates, Inc.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2023) Rafael Rafailov、Archit Sharma、Eric Mitchell、Christopher
    D Manning、Stefano Ermon 和 Chelsea Finn。2023年。 [直接偏好优化：你的语言模型实际上是奖励模型](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf)。发表于
    *神经信息处理系统进展*，第36卷，第53728–53741页。Curran Associates, Inc.
- en: 'Sattler et al. (2020) Felix Sattler, Klaus-Robert Müller, and Wojciech Samek.
    2020. Clustered federated learning: Model-agnostic distributed multitask optimization
    under privacy constraints. *IEEE transactions on neural networks and learning
    systems*, 32(8):3710–3722.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sattler et al. (2020) Felix Sattler、Klaus-Robert Müller 和 Wojciech Samek。2020年。集群化联邦学习：在隐私约束下的模型无关分布式多任务优化。*IEEE
    神经网络与学习系统汇刊*，32(8):3710–3722。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2017) John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford
    和 Oleg Klimov。2017年。近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*。
- en: Skalse et al. (2022) Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and
    David Krueger. 2022. Defining and characterizing reward gaming. *Advances in Neural
    Information Processing Systems*, 35:9460–9471.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skalse et al. (2022) Joar Skalse、Nikolaus Howe、Dmitrii Krasheninnikov 和 David
    Krueger。2022年。定义和特征化奖励游戏。*神经信息处理系统进展*，35:9460–9471。
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020.
    Learning to summarize from human feedback. In *NeurIPS*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon et al. (2020) Nisan Stiennon、Long Ouyang、Jeff Wu、Daniel M. Ziegler、Ryan
    Lowe、Chelsea Voss、Alec Radford、Dario Amodei 和 Paul Christiano。2020年。通过人类反馈学习总结。发表于
    *NeurIPS*。
- en: 'Sun et al. (2023) Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu,
    Yiran Chen, and Holger R Roth. 2023. Fedbpt: Efficient federated black-box prompt
    tuning for large language models. *arXiv preprint arXiv:2310.01467*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等（2023） Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran
    Chen, 和 Holger R Roth. 2023. Fedbpt: 高效的联邦黑箱提示调优方法。*arXiv 预印本 arXiv:2310.01467*。'
- en: Sun et al. (2024) Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024. Improving
    lora in privacy-preserving federated learning. *arXiv preprint arXiv:2403.12313*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2024） Youbang Sun, Zitao Li, Yaliang Li, 和 Bolin Ding. 2024. 改进隐私保护的联邦学习中的
    lora。*arXiv 预印本 arXiv:2403.12313*。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori等（2023） Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. Stanford alpaca:
    一种指令跟随型的 llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: Tien et al. (2022) Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D
    Dragan, and Daniel S Brown. 2022. Causal confusion and reward misidentification
    in preference-based reward learning. *arXiv preprint arXiv:2204.06601*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tien等（2022） Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D Dragan,
    和 Daniel S Brown. 2022. 在基于偏好奖励学习中的因果混淆和奖励错误识别。*arXiv 预印本 arXiv:2204.06601*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等（2023） Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Völske et al. (2017) Michael Völske, Martin Potthast, Shahbaz Syed, and Benno
    Stein. 2017. Tl; dr: Mining reddit to learn automatic summarization. In *Proceedings
    of the Workshop on New Frontiers in Summarization*, pages 59–63.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Völske等（2017） Michael Völske, Martin Potthast, Shahbaz Syed, 和 Benno Stein.
    2017. Tl; dr: 从 Reddit 中挖掘以学习自动摘要。 在 *总结新前沿研讨会论文集*，第59–63页。'
- en: Wang and Ji (2023) Shiqiang Wang and Mingyue Ji. 2023. A lightweight method
    for tackling unknown participation probabilities in federated averaging. *arXiv
    preprint arXiv:2306.03401*.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang和Ji（2023） Shiqiang Wang 和 Mingyue Ji. 2023. 处理联邦平均中未知参与概率的轻量级方法。*arXiv 预印本
    arXiv:2306.03401*。
- en: Wu et al. (2023) Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing
    Gao. 2023. Anchor sampling for federated learning with partial client participation.
    In *International Conference on Machine Learning*, pages 37379–37416\. PMLR.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2023） Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, 和 Jing Gao.
    2023. 用于部分客户端参与的联邦学习的锚采样。 在 *国际机器学习会议*，第37379–37416页。PMLR。
- en: 'Wu et al. (2024) Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, and Jing Gao.
    2024. Fedbiot: Llm local fine-tuning in federated learning without full model.
    *arXiv preprint arXiv:2406.17706*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu等（2024） Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, 和 Jing Gao. 2024. Fedbiot:
    在联邦学习中没有完整模型的本地微调。*arXiv 预印本 arXiv:2406.17706*。'
- en: 'Xie et al. (2023) Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. Federatedscope:
    A flexible federated learning platform for heterogeneity. *Proceedings of the
    VLDB Endowment*, 16(5):1059–1072.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie等（2023） Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui
    Kuang, Yaliang Li, Bolin Ding, 和 Jingren Zhou. 2023. Federatedscope: 面向异质性的灵活联邦学习平台。*VLDB
    纪要论文集*，16(5):1059–1072。'
- en: Yang et al. (2020) Haibo Yang, Minghong Fang, and Jia Liu. 2020. Achieving linear
    speedup with partial worker participation in non-iid federated learning. In *International
    Conference on Learning Representations*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2020） Haibo Yang, Minghong Fang, 和 Jia Liu. 2020. 在非独立同分布的联邦学习中，通过部分工人参与实现线性加速。
    在 *国际学习表征会议*。
- en: 'Ye et al. (2024) Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda
    Xu, Yaxin Du, Yanfeng Wang, and Siheng Chen. 2024. Openfedllm: Training large
    language models on decentralized private data via federated learning. *arXiv preprint
    arXiv:2402.06954*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye等（2024） Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin
    Du, Yanfeng Wang, 和 Siheng Chen. 2024. Openfedllm: 通过联邦学习在去中心化私有数据上训练大语言模型。*arXiv
    预印本 arXiv:2402.06954*。'
- en: 'Yi et al. (2023) Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023. Fedlora:
    Model-heterogeneous personalized federated learning with lora tuning. *arXiv preprint
    arXiv:2310.13283*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi等（2023） Liping Yi, Han Yu, Gang Wang, 和 Xiaoguang Liu. 2023. Fedlora: 通过
    lora 调优进行模型异质性的个性化联邦学习。*arXiv 预印本 arXiv:2310.13283*。'
- en: 'Zhang et al. (2023a) Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,
    Ruiyi Zhang, Guoyin Wang, and Yiran Chen. 2023a. Towards building the federated
    gpt: Federated instruction tuning. *arXiv preprint arXiv:2305.05644*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023a）Jianyi Zhang、Saeed Vahidian、Martin Kuo、Chunyuan Li、Ruiyi Zhang、Guoyin
    Wang 和 Yiran Chen。2023a。构建联邦 GPT 的方向：联邦指令调整。 *arXiv 预印本 arXiv:2305.05644*。
- en: 'Zhang et al. (2023b) Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu,
    Lizhen Qu, and Zenglin Xu. 2023b. Fedpetuning: When federated learning meets the
    parameter-efficient tuning methods of pre-trained language models. In *Annual
    Meeting of the Association of Computational Linguistics 2023*, pages 9963–9977\.
    Association for Computational Linguistics (ACL).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023b）Zhuo Zhang、Yuanhang Yang、Yong Dai、Qifan Wang、Yue Yu、Lizhen Qu
    和 Zenglin Xu。2023b。Fedpetuning: 当联邦学习遇上预训练语言模型的参数高效调整方法。在 *2023 年计算语言学协会年会*，第
    9963–9977 页。计算语言学协会（ACL）。'
- en: 'Zhao et al. (2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad
    Saleh, and Peter J Liu. 2023. Slic-hf: Sequence likelihood calibration with human
    feedback. *arXiv preprint arXiv:2305.10425*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2023）Yao Zhao、Rishabh Joshi、Tianqi Liu、Misha Khalman、Mohammad Saleh
    和 Peter J Liu。2023。Slic-hf: 基于人类反馈的序列似然校准。 *arXiv 预印本 arXiv:2305.10425*。'
- en: Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning
    language models from human preferences. *arXiv preprint arXiv:1909.08593*.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2019）Daniel M Ziegler、Nisan Stiennon、Jeffrey Wu、Tom B Brown、Alec Radford、Dario
    Amodei、Paul Christiano 和 Geoffrey Irving。2019。根据人类偏好微调语言模型。 *arXiv 预印本 arXiv:1909.08593*。
- en: Appendix A More Implementation Details
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更多实施细节
- en: In this section, we include various settings, such as the prompt and the hyperparameters.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们包括了各种设置，例如提示词和超参数。
- en: A.1 Hyperparameter Settings
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 超参数设置
- en: 'In our work, we fine-tune all models using LoRA, which is consistently set
    to rank 8, $\alpha=16$, and the dropout rate 0.0\. For the generation, we apply
    with these parameters:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们使用 LoRA 对所有模型进行微调，其中 LoRA 一致设置为等级 8，$\alpha=16$，以及 dropout 率 0.0\.
    对于生成任务，我们应用这些参数：
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If it is required to generate multiple completions, then we set the temperature
    to 1.0.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果需要生成多个完成项，则将温度设置为 1.0。
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If it is required to generate a single completion, then we adopt greedy search
    by setting the temperature to 0.0.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果需要生成单个完成项，则通过将温度设置为 0.0 来采用贪婪搜索。
- en: 'In the following part, we show the hyperparameter setting for different tasks:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分，我们展示了不同任务的超参数设置：
- en: '|  | SFT | Selector Training | RLFT |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | SFT | 选择器训练 | RLFT |'
- en: '| Participation Rate | - | 5/53 | - |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 参与率 | - | 5/53 | - |'
- en: '| Local Iterations | 30 | 30 | 30 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 本地迭代次数 | 30 | 30 | 30 |'
- en: '| Batch Size | 32 | 16 | 32 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 | 16 | 32 |'
- en: '| Rounds | 1000 | 500 | 500 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 1000 | 500 | 500 |'
- en: '| Optimizer | AdamW | AdamW | RMSprop |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | AdamW | RMSprop |'
- en: '| Hyperparameters | (0.9, 0.95) | (0.9, 0.95) | – |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | (0.9, 0.95) | (0.9, 0.95) | – |'
- en: '| Learning rate | $1e-4$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $1e-4$ |'
- en: 'Table 2: Hyperparameter Settings for the Summarization Task'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 摘要任务的超参数设置'
- en: '|  | Selector Training | RLFT |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | 选择器训练 | RLFT |'
- en: '| Participation Rate | 10/300 | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 参与率 | 10/300 | - |'
- en: '| Local Iterations | 10 | 10 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 本地迭代次数 | 10 | 10 |'
- en: '| Batch Size | 16 | 16 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 16 | 16 |'
- en: '| Rounds | 200 | 200 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 200 | 200 |'
- en: '| Optimizer | AdamW | RMSprop |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | RMSprop |'
- en: '| Hyperparameters | (0.9, 0.95) | – |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | (0.9, 0.95) | – |'
- en: '| Learning rate | $1e-5$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $1e-5$ |'
- en: 'Table 3: Hyperparameter Settings for the QA Task'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: QA 任务的超参数设置'
- en: Special Setting for FedBiscuit
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FedBiscuit 的特殊设置
- en: For the above two tasks, we ensemble three binary selectors (i.e., LoRAs). In
    the warmup round, we train the selector for 50 rounds under an FL framework. FedBiscuit
    performs regrouping every 50 rounds in the summarization task, while regrouping
    every 100 rounds in the QA task.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述两个任务，我们集成了三个二元选择器（即 LoRAs）。在预热轮次中，我们在 FL 框架下训练选择器 50 轮。FedBiscuit 在摘要任务中每
    50 轮进行一次分组，而在 QA 任务中每 100 轮进行一次分组。
- en: A.2 Instruction Tuning Prompt
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 指令调整提示词
- en: 'In this section, we highlight the prompts used to fine-tune the summarization
    tasks and the QA task:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们突出显示了用于微调摘要任务和 QA 任务的提示词：
- en: Summarization.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要。
- en: For pertaining (SFT) and the later reinforcement-learning fine-tuning (RLFT),
    it follows the prompt below
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相关的（SFT）和后续的强化学习微调（RLFT），遵循如下提示词
- en: '| Below is a forum post. Write a precise and concise summary that includes
    the most important points of the post.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '| 以下是一个论坛帖子。写一个简明扼要的总结，包括帖子的最重要要点。 |'
- en: 'SUBREDDIT: r/{subreddit}'
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'SUBREDDIT: r/{subreddit}'
- en: 'TITLE: {title}'
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'TITLE: {title}'
- en: 'POST: {post}'
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'POST: {post}'
- en: 'TL;DR: |'
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'TL;DR: |'
- en: 'For comparison:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 供比较：
- en: '| Below is a forum post followed by two summaries. Pick a more precise and
    concise one that summarizes the most important points in the given forum post,
    without including unimportant or irrelevant details. State your choice with a
    single capital letter, i.e., Äïf SUMMARY A is better, B̈ïf SUMMARY B is better.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '| 以下是一个论坛帖子，后跟两个摘要。选择一个更精确、简洁的摘要，总结给定论坛帖子中最重要的点，而不包括不重要或无关的细节。用一个大写字母表示你的选择，即，**A**如果SUMMARY
    A更好，**B**如果SUMMARY B更好。'
- en: 'SUBREDDIT: r/{subreddit}'
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'SUBREDDIT: r/{subreddit}'
- en: 'TITLE: {title}'
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'TITLE: {title}'
- en: 'POST: {post}'
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'POST: {post}'
- en: 'SUMMARY A: {output_A}'
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'SUMMARY A: {output_A}'
- en: 'SUMMARY B: {output_B}'
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'SUMMARY B: {output_B}'
- en: 'YOUR CHOICE: |'
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的选择：|
- en: QA.
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: QA。
- en: As the QA utilizes a pretrained model named Alpaca-7B, we follow its pretrained
    format
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于QA使用了名为Alpaca-7B的预训练模型，我们遵循其预训练格式
- en: '| Below is an instruction that describes a task, paired with an input that
    provides further context. Write a response that appropriately completes the request.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '| 以下是描述任务的指令，并附有提供更多背景的信息。写一个适当完成请求的回应。'
- en: 'Instruction:'
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令：
- en: '{instruction}'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '{instruction}'
- en: 'Input:'
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入：
- en: '{input}'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '{input}'
- en: 'Response: |'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回应：|
- en: 'For comparison between the two responses:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个回应：
- en: '| Below is a query followed by two responses. Pick a helpful response that
    is precise, concise, and casual. State your choice with a single capital letter,
    i.e., Äïf RESPONSE A is better, B̈ïf RESPONSE B is better.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '| 以下是一个查询，后跟两个回应。选择一个有帮助、精确、简洁且随意的回应。用一个大写字母表示你的选择，即，**A**如果RESPONSE A更好，**B**如果RESPONSE
    B更好。'
- en: 'QUERY: {instruction}'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'QUERY: {instruction}'
- en: 'RESPONSE A: {output_A}'
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'RESPONSE A: {output_A}'
- en: 'RESPONSE B: {output_B}'
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'RESPONSE B: {output_B}'
- en: 'YOUR CHOICE: |'
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的选择：|
- en: Appendix B More Numerical Results and Analysis
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 更多数值结果和分析
- en: B.1 Numerical Results on QA
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 QA的数值结果
- en: 'In this section, the test dataset comes from AlpacaFarm Dubois et al. ([2024](#bib.bib10));
    Li et al. ([2023b](#bib.bib26)). The results are as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，测试数据集来自AlpacaFarm Dubois等人（[2024](#bib.bib10)）；Li等人（[2023b](#bib.bib26)）。结果如下：
- en: '|  | 2-completion |  | 4-completion |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-completion |  | 4-completion |'
- en: '|  | Rating |  | Rating |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | 评分 |  | 评分 |'
- en: '| Alpaca-7B | 3.752 |  | - |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca-7B | 3.752 |  | - |'
- en: '| FedBis | 4.140 |  | 4.113 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| FedBis | 4.140 |  | 4.113 |'
- en: '| FedBiscuit | 4.094 |  | 3.830 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| FedBiscuit | 4.094 |  | 3.830 |'
- en: 'Table 4: Performce under QA.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在QA下的表现。
- en: 'As presented in Section [4](#S4 "4 FedBiscuit: FedBis with Cluster-wise Aggregation
    ‣ On the Client Preference of LLM Fine-tuning in Federated Learning"), the LLM
    owner will generate a set of responses to a given instruction before building
    a preference dataset. Therefore, the column "2-completion" means the owner prepares
    2 completions for each instruction, while "4-completion" means 4 completions for
    each instruction and forms 6 pairs. The row "Alpaca-7B" acts as a baseline to
    help us understand the performance of the proposed FedBiscuit and FedBis. All
    the rating comes from Auto-J Li et al. ([2023a](#bib.bib24)), which would be different
    from the ratings reported by Li et al. ([2023b](#bib.bib26)) because it evaluates
    with GPT-4 Achiam et al. ([2023](#bib.bib1)).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[4](#S4 "4 FedBiscuit: FedBis with Cluster-wise Aggregation ‣ On the Client
    Preference of LLM Fine-tuning in Federated Learning")节所述，LLM所有者会在构建偏好数据集之前生成一组对给定指令的回应。因此，“2-completion”列表示所有者为每个指令准备了2个完成，而“4-completion”表示每个指令准备了4个完成并形成了6对。行“Alpaca-7B”作为基准，帮助我们理解提议的FedBiscuit和FedBis的性能。所有评分来自Auto-J
    Li等人（[2023a](#bib.bib24)），这些评分与Li等人（[2023b](#bib.bib26)）报告的评分不同，因为它是用GPT-4 Achiam等人（[2023](#bib.bib1)）进行评估的。'
- en: The table above may lead to conclusions different from those drawn from the
    summarization task. First, FedBis achieves better performance than FedBiscuit.
    This is within our expectations. First, these selectors are trained for a total
    of 200 rounds. As presented in Figure [3](#S6.F3 "Figure 3 ‣ 6.2 Numerical Results
    on Summarization ‣ 6 Experiments ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning"), FedBiscuit surpasses FedBis after 300 communication rounds.
    This is because the selectors of FedBiscuit are trained for 100 rounds only, while
    the selector of FedBis has been fully trained for 200 rounds. When the inflection
    point appears in FedBis, we can hypothesize that the dominance of FedBiscuit still
    exists.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 上表可能导致与总结任务得出的结论不同。首先，**FedBis**的表现优于**FedBiscuit**。这在我们的预期之内。首先，这些选择器总共训练了200轮。正如图[3](#S6.F3
    "Figure 3 ‣ 6.2 Numerical Results on Summarization ‣ 6 Experiments ‣ On the Client
    Preference of LLM Fine-tuning in Federated Learning")所示，**FedBiscuit**在300轮通信后超越了**FedBis**。这是因为**FedBiscuit**的选择器只训练了100轮，而**FedBis**的选择器已完全训练了200轮。当**FedBis**出现拐点时，我们可以假设**FedBiscuit**的优势仍然存在。
- en: Another comparison arises between different numbers of generations to a given
    prompt. From the table, we notice that "2-completion" can achieve better performance
    than "4-completion," meaning that the performance may not be relevant to the size
    of the RLFT training set. Instead, it may rely on the quality of the training
    data. As we can see, Alpaca-7B hardly generates high-quality data, leading to
    limited improvement with training with these generated data. In other words, if
    we generate more low-quality completions, the improvement of the model would be
    more limited. At the same time, we can hypothesize that if the generated data
    are of high quality, "4-completion" may outperform "2-completion" in terms of
    the final rating.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个比较涉及对给定提示的不同生成次数。从表中我们注意到，“2-completion”比“4-completion”能实现更好的性能，这意味着性能可能与RLFT训练集的大小无关。相反，它可能依赖于训练数据的质量。正如我们所见，**Alpaca-7B**几乎无法生成高质量的数据，导致使用这些生成的数据训练的改进有限。换句话说，如果我们生成更多低质量的完成，模型的改进将会更加有限。同时，我们可以假设，如果生成的数据质量很高，“4-completion”可能在最终评分上优于“2-completion”。
- en: B.2 Examples on the Final RLHF Results
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 关于最终RLHF结果的示例
- en: In this section, we prepare two examples to illustrate the performance of our
    proposed FedBis and FedBiscuit and compare them with other baselines. Refer to
    Table [5](#A2.T5 "Table 5 ‣ B.2 Examples on the Final RLHF Results ‣ Appendix
    B More Numerical Results and Analysis ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning") and Table [6](#A2.T6 "Table 6 ‣ B.2 Examples on the Final
    RLHF Results ‣ Appendix B More Numerical Results and Analysis ‣ On the Client
    Preference of LLM Fine-tuning in Federated Learning") for the demonstration of
    the examples.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们准备了两个示例来说明我们提出的**FedBis**和**FedBiscuit**的表现，并将它们与其他基准进行比较。有关示例的演示，请参阅表[5](#A2.T5
    "Table 5 ‣ B.2 Examples on the Final RLHF Results ‣ Appendix B More Numerical
    Results and Analysis ‣ On the Client Preference of LLM Fine-tuning in Federated
    Learning")和表[6](#A2.T6 "Table 6 ‣ B.2 Examples on the Final RLHF Results ‣ Appendix
    B More Numerical Results and Analysis ‣ On the Client Preference of LLM Fine-tuning
    in Federated Learning")。
- en: '|  Subreddit: r/relationships Title: Me [25F] with my SO [27 M] of 3.5 years,
    I went through his texts and can’t stop thinking about something I saw. Post:
    I admit that going through his text messages was wrong and I do feel incredibly
    guilty about it now but the bigger issue for me is that I found something that
    I can’t stop thinking about. He was talking to his good female friend and I saw
    a text that said "am I being an idiot for leaving [my name]"? . I stopped reading
    after that because I started having a panic attack. The idea of breaking up was
    not even on my radar and I thought he was more than happy. Again, I know I shouldn’t
    have looked but whats done is done and now I can’t stop worrying about when it’s
    going to happen. And now i’m being clingy just because of my worry which i’m sure
    doesn’t help. I love him and don’t want to lose him. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  Subreddit: r/relationships 标题: 我 [25F] 与我的SO [27M] 交往了3.5年，我查看了他的短信，无法停止对我看到的内容的思考。帖子:
    我承认查看他的短信是不对的，我现在确实感到非常内疚，但对我来说更大的问题是我发现了我无法停止思考的东西。他在和他的好女性朋友聊天，我看到一条短信写着“我离开[我的名字]是不是很傻”？我看到这里就停止了，因为我开始出现惊恐发作。分手的想法甚至没有出现在我的雷达上，我以为他非常幸福。再次，我知道我不该看，但已成事实，现在我无法停止担心什么时候会发生。现在我因为担忧变得粘人，我确信这没有帮助。我爱他，不想失去他。
    |'
- en: '| Human Summary: I saw a text from my SO to his friend about leaving me, can’t
    stop freaking out. Need serious advice. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 人类总结：我看到 SO 发给他朋友的短信，内容提到离开我，无法停止恐慌。需要认真建议。 |'
- en: '| SFT: I went through my SO’s texts and found something I wasn’t supposed to.
    Now I’m being clingy and I don’t want to lose him. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| SFT: 我查看了 SO 的短信，发现了不该看到的东西。现在我变得依赖，且不想失去他。 |'
- en: '| Centralized: went through my boyfriends texts and found something that i’m
    now worrying about when/if he’s going to break up with me. I love him and don’t
    want to lose him. Help. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 中心化：查看了我男朋友的短信，发现了一些让我担心的内容，现在我担心他是否会和我分手。我爱他，不想失去他。帮帮我。 |'
- en: '| FedBis: went through SO’s texts and found a text that said "am I being an
    idot for leaving [my name]"? and now I’m worried he’s going to break up with me
    and can’t stop thinking about it. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| FedBis: 查看了 SO 的短信，发现了一条写着“我离开[我的名字]是否很愚蠢？”的短信，现在我担心他会和我分手，无法停止思考。 |'
- en: '| FedBiscuit: went through SO’s texts, found something that i’m now worrying
    about when the inevitable breakup is going to happen. Now being clingy and can’t
    stop thinking about it. Help! |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| FedBiscuit: 查看了 SO 的短信，发现了一些让我担心不可避免的分手时刻。现在变得依赖，无法停止思考。救命！ |'
- en: '|   |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: 'Table 5: An example for summarization task. Caution: The example data that
    may be offensive or harmful.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：总结任务的示例。警告：示例数据可能会冒犯或造成伤害。
- en: '|  List the layers of the TCP/IP model and for each layer identify one protocol.
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 列出 TCP/IP 模型的各层，并为每一层标识一个协议。 |'
- en: '| Alpaca-7B: The TCP/IP model consists of four layers: the application layer,
    transport layer, internet layer and network access layer. Application Layer: HTTP
    Transport Layer: TCP'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '| Alpaca-7B: TCP/IP 模型由四个层次组成：应用层、传输层、互联网层和网络接入层。应用层：HTTP 传输层：TCP |'
- en: 'Internet Layer: IP'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网层：IP
- en: 'Network Access Layer: Ethernet  |'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 网络接入层：以太网  |
- en: '| FedBis: The TCP/IP model has four layers: the application layer, transport
    layer, internet layer and network access layer. At the application layer, the
    protocol is TCP. At the transport layer, the protocol is UDP. At the internet
    layer, the protocol is IP. At the network access layer, the protocol is Ethernet.
    1\. Application layer: TCP 2\. Transport layer: UDP'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '| FedBis: TCP/IP 模型有四个层次：应用层、传输层、互联网层和网络接入层。在应用层，协议是 TCP。在传输层，协议是 UDP。在互联网层，协议是
    IP。在网络接入层，协议是以太网。1\. 应用层：TCP 2\. 传输层：UDP |'
- en: '3\. Internet layer: IP'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 互联网层：IP
- en: '4\. Network access layer: Ethernet |'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 网络接入层：以太网 |
- en: '| FedBiscuit: The TCP/IP model has four layers: the application layer, transport
    layer, internet layer and network access layer. At the application layer, the
    HTTP protocol is used. The transport layer uses the TCP protocol. At the internet
    layer, the IP protocol is used. Finally, the network access layer uses the Ethernet
    protocol. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| FedBiscuit: TCP/IP 模型有四个层次：应用层、传输层、互联网层和网络接入层。在应用层，使用 HTTP 协议。在传输层，使用 TCP
    协议。在互联网层，使用 IP 协议。最后，网络接入层使用以太网协议。 |'
- en: '|   |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: 'Table 6: An example for QA task. Caution: The example data that may be offensive
    or harmful.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：QA 任务的示例。警告：示例数据可能会冒犯或造成伤害。
