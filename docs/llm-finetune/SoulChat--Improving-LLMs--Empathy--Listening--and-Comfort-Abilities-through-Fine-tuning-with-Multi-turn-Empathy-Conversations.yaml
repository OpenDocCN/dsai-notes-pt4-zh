- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:39:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:39:38'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SoulChat：通过与多轮同理心对话微调提升LLMs的同理心、倾听和安慰能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.00273](https://ar5iv.labs.arxiv.org/html/2311.00273)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.00273](https://ar5iv.labs.arxiv.org/html/2311.00273)
- en: Yirong Chen¹, Xiaofen Xing¹, Jingkai Lin¹, Huimin Zheng¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yirong Chen¹, Xiaofen Xing¹, Jingkai Lin¹, Huimin Zheng¹,
- en: Zhenyu Wang¹, Qi Liu², Xiangmin Xu^(2,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhenyu Wang¹, Qi Liu², Xiangmin Xu^(2,3)
- en: ¹Guangdong Provincial Key Laboratory of Human Digital Twin, School of EE.,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹广东省人类数字双胞胎重点实验室，电气工程学院，
- en: South China University of Technology, Guangzhou, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 华南理工大学，中国广州
- en: ²School of Future Technology, South China University of Technology, Guangzhou,
    China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²未来技术学院，华南理工大学，中国广州
- en: ³Pazhou Lab, Guangzhou, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³Pazhou 实验室，中国广州
- en: 'eeyirongchen@mail.scut.edu.cn, {xfxing, xmxu}@scut.edu.cn   Corresponding author.
    Email: xfxing@scut.edu.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: eeyirongchen@mail.scut.edu.cn, {xfxing, xmxu}@scut.edu.cn   通讯作者。电子邮件：xfxing@scut.edu.cn
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have been widely applied in various fields due
    to their excellent capability for memorizing knowledge and chain of thought (CoT).
    When these language models are applied in the field of psychological counseling,
    they often rush to provide universal advice. However, when users seek psychological
    support, they need to gain empathy, trust, understanding and comfort, rather than
    just reasonable advice. To this end, we constructed a multi-turn empathetic conversation
    dataset of more than 2 million samples, in which the input is the multi-turn conversation
    context, and the target is empathetic responses that cover expressions such as
    questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments
    have shown that the empathy ability of LLMs can be significantly enhanced when
    finetuning by using multi-turn dialogue history and responses that are closer
    to the expression of a psychological consultant.¹¹1[https://github.com/scutcyr/SoulChat](https://github.com/scutcyr/SoulChat)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其优秀的知识记忆和思维链（CoT）能力在多个领域得到了广泛应用。当这些语言模型应用于心理咨询领域时，它们往往急于提供普遍的建议。然而，当用户寻求心理支持时，他们需要的是同理心、信任、理解和安慰，而不仅仅是合理的建议。为此，我们构建了一个包含超过
    200 万样本的多轮同理心对话数据集，其中输入是多轮对话背景，目标是覆盖提问、安慰、认可、倾听、信任、情感支持等表达的同理心回应。实验表明，通过使用多轮对话历史和更接近心理咨询师表达的回应进行微调，可以显著提升LLMs的同理心能力。¹¹1[https://github.com/scutcyr/SoulChat](https://github.com/scutcyr/SoulChat)
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'With the birth of BERT (Devlin et al., [2019](#bib.bib2)) and GPT (Radford
    et al., [2018](#bib.bib16)), large language models (LLMs) have made rapid progress
    in the past five years. In November 2022, OpenAI launched ChatGPT²²2[https://chat.openai.com](https://chat.openai.com) (OpenAI,
    [2022](#bib.bib10)), a large language model fine-tuning by reinforcement learning
    from human feedback (RLHF) (Ouyang et al., [2022](#bib.bib11)). However, when
    applied to mental health or emotional support conversation, there are three main
    issues lead to ChatGPT appear less “human-centered”:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 BERT（Devlin 等，[2019](#bib.bib2)）和 GPT（Radford 等，[2018](#bib.bib16)）的问世，大型语言模型（LLMs）在过去五年里取得了快速进展。2022年11月，OpenAI推出了
    ChatGPT²²2[https://chat.openai.com](https://chat.openai.com)（OpenAI，[2022](#bib.bib10)），这是一个通过人类反馈强化学习（RLHF）进行微调的大型语言模型（Ouyang
    等，[2022](#bib.bib11)）。然而，当应用于心理健康或情感支持对话时，ChatGPT 存在三个主要问题，导致其表现得不够“以人为本”：
- en: 1)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1)
- en: 'ChatGPT tends to provide repetitive and standardized responses. ChatGPT often
    uses the following template to respond to users’ questions related to mental health:
    "我很抱歉…。xxx是…。以下是一些建议：…。 (I’m sorry to …{xxx} is …Here are some suggestions:…)",
    which may cause boredom.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT 倾向于提供重复和标准化的回应。ChatGPT 常使用以下模板回应用户有关心理健康的问题：“我很抱歉…。xxx是…。以下是一些建议：…。（I’m
    sorry to …{xxx} is …Here are some suggestions:…）”，这可能会导致厌倦感。
- en: 2)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2)
- en: 'ChatGPT is inclined to provide suggestions rather than ask questions or listen.
    It is eager to solve users’ problems, usually providing lengthy and general suggestions,
    as shown in Figure [12](#A4.F12 "Figure 12 ‣ Appendix D English Word Cloud Map
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations") of Appendix [F](#A6 "Appendix
    F Sample Conversations of Other LLMs ‣ SoulChat: Improving LLMs’ Empathy, Listening,
    and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations").
    However, professional psychologists rarely provide specific suggestions during
    the counseling process.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT 倾向于提供建议而不是提出问题或倾听。它热衷于解决用户的问题，通常提供详细且笼统的建议，如附录[F](#A6 "附录 F 其他 LLMs
    的示例对话 ‣ SoulChat：通过多轮共情对话微调提高 LLMs 的共情、倾听和安慰能力")中图[12](#A4.F12 "图 12 ‣ 附录 D 英文词云图
    ‣ SoulChat：通过多轮共情对话微调提高 LLMs 的共情、倾听和安慰能力")所示。然而，专业心理学家在咨询过程中很少提供具体建议。
- en: 3)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3)
- en: ChatGPT acts a bit like a rational "Straight man" for those users who need listening
    and comfort. Users who seek emotional support usually expect empathy support such
    as listening, understanding and comfort.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT 对于那些需要倾听和安慰的用户有点像一个理性的“直男”。寻求情感支持的用户通常期望得到如倾听、理解和安慰等共情支持。
- en: '![Refer to caption](img/3105c0415e7f77bf7b071844cf8d3383.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3105c0415e7f77bf7b071844cf8d3383.png)'
- en: 'Figure 1: A case of a user confiding to SoulChat. Compared to ChatGPT, SoulChat
    is better at listening and guiding users to think.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个用户向 SoulChat 倾诉的案例。与 ChatGPT 相比，SoulChat 更擅长倾听和引导用户思考。
- en: 'Similar issues can also be found in other LLMs, e.g. ChatGLM (Zeng et al.,
    [2023](#bib.bib20)), SparkDesk³³3[https://xinghuo.xfyun.cn](https://xinghuo.xfyun.cn),
    as presented in Appendix [F](#A6 "Appendix F Sample Conversations of Other LLMs
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations"). It may be due to the lack
    of large-scale multi-turn empathy conversation datasets for fine-tuning stage,
    especially in the field of Chinese mental health or emotional support. EMPATHETICDIALOGUES (Rashkin
    et al., [2019](#bib.bib17)) and ESConv (Liu et al., [2021](#bib.bib9)) are two
    English empathy conversation datasets that is used for developing emotional support
    conversation (ESC) systems, e.g MISC (Tu et al., [2022](#bib.bib19)), GLHG (Peng
    et al., [2022](#bib.bib13)), MultiESC (Cheng et al., [2022](#bib.bib1)), FADO (Peng
    et al., [2023](#bib.bib14)) and etc. On the one hand, these models may rely on
    annotated empathy strategies and emotions of users during the training or inference
    stage, which means that building large-scale similar datasets for fine-tuning
    LLMs is difficult. On the other hand, these datasets are in English, so that they
    cannot be applied to fine-tune Chinese LLMs. As for mental health, efaqa (Hailiang
    et al., [2020](#bib.bib5)) and PsyQA (Sun et al., [2021](#bib.bib18)) are two
    commonly-used datasets. Among them, efaqa contains 20,000 conversations and provides
    annotation information such as types of troubles, psychological disorders, SOS,
    etc. However, efaqa has a complex multi-party dialogue relationship and a high
    proportion of low-quality responses from netizens, while PsyQA contains 22,346
    questions and 56,063 single-turn long-text psychological counseling conversations.
    Thus, neither of these datasets can solve the three issues of ChatGPT mentioned
    above.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 LLMs 中也存在类似问题，例如 ChatGLM（Zeng et al., [2023](#bib.bib20)）、SparkDesk³³3[https://xinghuo.xfyun.cn](https://xinghuo.xfyun.cn)，如附录[F](#A6
    "附录 F 其他 LLMs 的示例对话 ‣ SoulChat：通过多轮共情对话微调提高 LLMs 的共情、倾听和安慰能力")所示。这可能是由于缺乏大规模多轮共情对话数据集用于微调阶段，尤其是在中文心理健康或情感支持领域。EMPATHETICDIALOGUES（Rashkin
    et al., [2019](#bib.bib17)）和 ESConv（Liu et al., [2021](#bib.bib9)）是两个用于开发情感支持对话（ESC）系统的英文共情对话数据集，如
    MISC（Tu et al., [2022](#bib.bib19)）、GLHG（Peng et al., [2022](#bib.bib13)）、MultiESC（Cheng
    et al., [2022](#bib.bib1)）、FADO（Peng et al., [2023](#bib.bib14)）等。一方面，这些模型在训练或推理阶段可能依赖于标注的共情策略和用户情感，这意味着建立大规模类似数据集以微调
    LLMs 是困难的。另一方面，这些数据集是英文的，因此不能用于微调中文 LLMs。至于心理健康，efaqa（Hailiang et al., [2020](#bib.bib5)）和
    PsyQA（Sun et al., [2021](#bib.bib18)）是两个常用的数据集。其中，efaqa 包含 20,000 个对话，并提供了如困扰类型、心理障碍、SOS
    等注释信息。然而，efaqa 具有复杂的多方对话关系以及较高比例的低质量网民回应，而 PsyQA 包含 22,346 个问题和 56,063 个单轮长文本心理咨询对话。因此，这两个数据集都无法解决上述
    ChatGPT 的三个问题。
- en: 'Recently, Qiu et al. ([2023](#bib.bib15)) proposed a SMILE approach to employ
    ChatGPT to convert single-turn dialogues into multi-turn ones. They utilized SMILE
    to extend the single-turn conversation dataset PsyQA to a empathy multi-turn conversation
    dataset SMILECHAT with 355,733 samples. Inspired by (Qiu et al., [2023](#bib.bib15)),
    we proposed a Chinese empathy constraint prompt, in which the empathy prompt constraint
    is further strengthened compared with SMILE prompt (see Appendix [C](#A3 "Appendix
    C Our prompt VS SMILE prompt ‣ SoulChat: Improving LLMs’ Empathy, Listening, and
    Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")).
    As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ SoulChat: Improving
    LLMs’ Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn
    Empathy Conversations") (English version: Appendix [C](#A3 "Appendix C Our prompt
    VS SMILE prompt ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities
    through Fine-tuning with Multi-turn Empathy Conversations")), our empathy constraints
    are defined as “‘心理咨询师’的回复需要结合用户的描述内容并提供共情，如：倾听、安慰、理解、信任、认可、真诚、情感支持等 (The response
    of the ’psychological counselor’ needs to be combined with the user’s description
    and provide empathy, such as listening, comfort, interpretation, trust, recognition,
    sincerity, emotional support, etc)”.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Qiu等人（[2023](#bib.bib15)）提出了一种SMILE方法，利用ChatGPT将单轮对话转换为多轮对话。他们利用SMILE将单轮对话数据集PsyQA扩展为一个包含355,733个样本的共情多轮对话数据集SMILECHAT。受到(Qiu等人，[2023](#bib.bib15))的启发，我们提出了一种中文共情约束提示，其中共情提示约束比SMILE提示进一步加强（参见附录[C](#A3
    "附录 C 我们的提示 VS SMILE 提示 ‣ SoulChat: 通过微调多轮共情对话提高LLMs的共情、倾听和安慰能力")）。如图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ SoulChat: 通过微调多轮共情对话提高LLMs的共情、倾听和安慰能力")所示（英文版：附录[C](#A3 "附录 C 我们的提示
    VS SMILE 提示 ‣ SoulChat: 通过微调多轮共情对话提高LLMs的共情、倾听和安慰能力")），我们的共情约束定义为：“‘心理咨询师’的回复需要结合用户的描述内容并提供共情，如：倾听、安慰、理解、信任、认可、真诚、情感支持等
    (The response of the ’psychological counselor’ needs to be combined with the user’s
    description and provide empathy, such as listening, comfort, interpretation, trust,
    recognition, sincerity, emotional support, etc)”。'
- en: '![Refer to caption](img/b0596d37e39c689d58bc1933cd6cc6fc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b0596d37e39c689d58bc1933cd6cc6fc.png)'
- en: 'Figure 2: The prompt used for converting single-turn psychological counseling
    conversations to multi-turn empathy conversations (English version: Appendix [C](#A3
    "Appendix C Our prompt VS SMILE prompt ‣ SoulChat: Improving LLMs’ Empathy, Listening,
    and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：用于将单轮心理咨询对话转换为多轮共情对话的提示（英文版：附录[C](#A3 "附录 C 我们的提示 VS SMILE 提示 ‣ SoulChat:
    通过微调多轮共情对话提高LLMs的共情、倾听和安慰能力")）。'
- en: To this end, we first constructed 215,813 different psychological counseling
    questions about 12 topics and 619,725 answers through data outsourcing services.
    Rule-based cleaning, manual rewriting and human proofreading are applied to ensure
    that there is no sensitive or privacy-related content in the dataset. Then, we
    use ChatGPT to convert these single-turn long text psychological counseling conversations
    to multi-turn empathy conversations. We also conducted manual proofreading and
    data cleansing for multi-turn dialogues rewritten by ChatGPT to further strengthen
    the expression of empathy, such as questioning, comfort, recognition, listening,
    trust, emotional support, etc. In the end, we obtained a multi-turn empathy conversation
    dataset, named SoulChatCorpus, with 2,300,248 samples. To our knowledge, it is
    the first million-scale multi-turn empathy conversation dataset in the field of
    mental health or emotional support. We conduct experiments by using ChatGLM-6B
    as the base model for fine-tuning on SoulChatCorpus. Results demonstrate that
    LLMs’ empathy, listening, and comfort abilities can be improved significantly
    through fine-tuning with million-scale multi-turn empathy conversation dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先通过数据外包服务构建了215,813个关于12个主题的不同心理咨询问题和619,725个答案。应用基于规则的清理、人工重写和人工校对，以确保数据集中没有敏感或隐私相关的内容。然后，我们使用ChatGPT将这些单轮长文本心理咨询对话转换为多轮共情对话。我们还对ChatGPT重写的多轮对话进行了人工校对和数据清洗，以进一步增强共情表达，如提问、安慰、认可、倾听、信任、情感支持等。最终，我们获得了一个名为SoulChatCorpus的多轮共情对话数据集，共包含2,300,248个样本。据我们所知，这是心理健康或情感支持领域第一个百万规模的多轮共情对话数据集。我们使用ChatGLM-6B作为基础模型，在SoulChatCorpus上进行微调的实验表明，通过在百万规模的多轮共情对话数据集上进行微调，可以显著提升大型语言模型的共情、倾听和安慰能力。
- en: '![Refer to caption](img/8dd960360af270bd8b96c871657b9b23.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8dd960360af270bd8b96c871657b9b23.png)'
- en: 'Figure 3: Distribution of counseling topics.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 咨询主题分布。'
- en: 2 Human-centered Mental Health LLM
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 人本心理健康LLM
- en: 2.1 SoulChatCorpus Collection
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 SoulChatCorpus 收集
- en: 'We consider an one-on-one psychological counseling conversational setting where
    a user and a psychological consultant engage in multiple rounds of dialogue. However,
    such conversation data is not publicly available due to the privacy protection
    and ethical standards of psychological counseling. To construct high-quality multi-turn
    empathy conversation dataset, We selected 12 topics of psychological counseling
    to construct 215,813 long-text questions and 619,725 long-text answer through
    crowdsourcing. The distribution of topics is shown in Figure [3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort
    Abilities through Fine-tuning with Multi-turn Empathy Conversations"). Then, we
    used ChatGPT (99% called gpt-3.5-turbo api and 1% called gpt-4 api) as a text
    rewriting tool following the prompt as shown in Figure [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities
    through Fine-tuning with Multi-turn Empathy Conversations") to convert single-turn
    psychological counseling conversations to multi-turn empathy conversations, in
    which one turn is in the form of "用户： $\backslash$n心理咨询师：".
    The response of "心理咨询师" was asked to be rewritten to reflect human-centered expressions
    such as empathy, listening, comfort, etc. Finally, after manual proofreading,
    we removed 105,134 low-quality samples and ultimately obtained 2,300,248 samples.
    As shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1 SoulChatCorpus Collection ‣ 2 Human-centered
    Mental Health LLM ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort
    Abilities through Fine-tuning with Multi-turn Empathy Conversations"), the word
    cloud map of the utterances expressed by psychological consultants indicated that
    the rewritten multi-turn empathy conversation has high level of empathy.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个一对一的心理咨询对话环境，其中用户和心理咨询师进行多轮对话。然而，由于隐私保护和心理咨询的伦理标准，这种对话数据并未公开。为了构建高质量的多轮同理心对话数据集，我们通过众包选择了12个心理咨询主题，构建了215,813个长文本问题和619,725个长文本回答。主题分布见图
    [3](#S1.F3 "图 3 ‣ 1 引言 ‣ SoulChat：通过多轮同理心对话微调提高LLM的同理心、倾听和舒适度能力")。然后，我们使用ChatGPT（99%调用gpt-3.5-turbo
    API和1%调用gpt-4 API）作为文本重写工具，按照图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ SoulChat：通过多轮同理心对话微调提高LLM的同理心、倾听和舒适度能力")所示的提示，将单轮心理咨询对话转换为多轮同理心对话，其中一轮的形式为“用户：
    $\backslash$n心理咨询师：”。要求“心理咨询师”的回应被重写为体现人本表达，如同理心、倾听、舒适度等。最后，经过人工校对，我们删除了105,134个低质量样本，最终获得了2,300,248个样本。如图
    [4](#S2.F4 "图 4 ‣ 2.1 SoulChatCorpus Collection ‣ 2 人本心理健康LLM ‣ SoulChat：通过多轮同理心对话微调提高LLM的同理心、倾听和舒适度能力")所示，心理咨询师表达的发言词云图表明重写后的多轮同理心对话具有较高的同理心水平。
- en: '![Refer to caption](img/25629a4641f82c775b0084a6821bc945.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25629a4641f82c775b0084a6821bc945.png)'
- en: 'Figure 4: Word cloud map of psychological consultants’ utterances (English
    version: Appendix [D](#A4 "Appendix D English Word Cloud Map ‣ SoulChat: Improving
    LLMs’ Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn
    Empathy Conversations")).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 心理咨询师发言的词云图（英文版：附录 [D](#A4 "附录 D 英文词云图 ‣ SoulChat：通过多轮同理心对话微调提高LLM的同理心、倾听和舒适度能力")）。'
- en: 'Table 1: Evaluation results.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 评估结果。'
- en: '| Dataset | Model | Automatic. | Manual. |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | 自动. | 手动. |  |'
- en: '| B-1 | B-2 | B-3 | B-4 | R-1 | R-2 | R-L | Con. | Emp. | Hel. | Saf. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| B-1 | B-2 | B-3 | B-4 | R-1 | R-2 | R-L | Con. | Emp. | Hel. | Saf. |'
- en: '| SoulChat- Corpus | ChatGLM-6B | 22.73 | 13.15 | 8.04 | 4.92 | 25.33 | 5.72
    | 18.84 | 1.90 | 1.55 | 1.92 | 1.0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| SoulChat-语料库 | ChatGLM-6B | 22.73 | 13.15 | 8.04 | 4.92 | 25.33 | 5.72 |
    18.84 | 1.90 | 1.55 | 1.92 | 1.0 |'
- en: '| MeChat | 29.43 | 17.12 | 10.54 | 6.71 | 27.35 | 6.27 | 21.12 | 1.83 | 1.70
    | 1.78 | 1.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MeChat | 29.43 | 17.12 | 10.54 | 6.71 | 27.35 | 6.27 | 21.12 | 1.83 | 1.70
    | 1.78 | 1.0 |'
- en: '| ChatGPT | 27.98 | 16.09 | 9.93 | 6.23 | 27.39 | 6.82 | 21.92 | 1.96 | 1.62
    | 1.94 | 1.0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 27.98 | 16.09 | 9.93 | 6.23 | 27.39 | 6.82 | 21.92 | 1.96 | 1.62
    | 1.94 | 1.0 |'
- en: '| SoulChat | 33.78 | 20.07 | 12.86 | 8.52 | 31.47 | 8.92 | 26.57 | 1.95 | 1.84
    | 1.87 | 1.0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SoulChat | 33.78 | 20.07 | 12.86 | 8.52 | 31.47 | 8.92 | 26.57 | 1.95 | 1.84
    | 1.87 | 1.0 |'
- en: '| SMILECHAT | ChatGLM-6B | 22.91 | 13.56 | 8.40 | 5.15 | 25.99 | 5.95 | 18.76
    | 1.81 | 1.39 | 1.84 | 1.0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SMILECHAT | ChatGLM-6B | 22.91 | 13.56 | 8.40 | 5.15 | 25.99 | 5.95 | 18.76
    | 1.81 | 1.39 | 1.84 | 1.0 |'
- en: '| MeChat | 30.63 | 18.41 | 11.59 | 7.46 | 28.92 | 6.76 | 21.59 | 1.95 | 1.74
    | 1.83 | 1.0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| MeChat | 30.63 | 18.41 | 11.59 | 7.46 | 28.92 | 6.76 | 21.59 | 1.95 | 1.74
    | 1.83 | 1.0 |'
- en: '| ChatGPT | 28.30 | 16.48 | 10.24 | 6.40 | 27.57 | 6.71 | 21.60 | 1.95 | 1.65
    | 1.97 | 1.0 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 28.30 | 16.48 | 10.24 | 6.40 | 27.57 | 6.71 | 21.60 | 1.95 | 1.65
    | 1.97 | 1.0 |'
- en: '| SoulChat | 35.40 | 21.39 | 13.77 | 9.02 | 32.64 | 9.17 | 21.10 | 1.93 | 1.90
    | 1.85 | 1.0 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SoulChat | 35.40 | 21.39 | 13.77 | 9.02 | 32.64 | 9.17 | 21.10 | 1.93 | 1.90
    | 1.85 | 1.0 |'
- en: 2.2 SoulChat Model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 SoulChat 模型
- en: 'We utilized the ChatGLM-6B (Du et al., [2022](#bib.bib3); Zeng et al., [2023](#bib.bib20))
    as the base LLM architecture to develop the SoulChat. ChatGLM-6B is an open-source,
    bilingual LLM based on the General Language Model (GLM) (Du et al., [2022](#bib.bib3))
    framework with 6.2 billion parameters. The input of model is defined as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ChatGLM-6B (Du et al., [2022](#bib.bib3); Zeng et al., [2023](#bib.bib20))
    作为基础 LLM 架构来开发 SoulChat。ChatGLM-6B 是一个开源的双语 LLM，基于 General Language Model (GLM)
    (Du et al., [2022](#bib.bib3)) 框架，拥有 62 亿个参数。模型的输入定义为：
- en: '|  | $input=u_{1}^{u}+^{\prime}{\backslash}n^{\prime}+u_{1}^{p}+...+u_{N}^{u}+^{\prime}{\backslash}n^{\prime}+u_{N}^{p}$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $input=u_{1}^{u}+^{\prime}{\backslash}n^{\prime}+u_{1}^{p}+...+u_{N}^{u}+^{\prime}{\backslash}n^{\prime}+u_{N}^{p}$
    |  |'
- en: where the utterance of User on $i$ turn $u_{i}^{p}$=‘心理咨询师： (Psychologist:)’,
    $N$ represents the number of conversation turns for the context.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中用户在第 $i$ 回合的发言 $u_{i}^{p}$=‘心理咨询师： (Psychologist:)’，$N$ 表示上下文的对话回合数。
- en: 3 Experiments
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Baselines
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准线
- en: 'We compare SoulChat and the following benchmark models using both automatic
    and manual evaluations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自动和手动评估对 SoulChat 和以下基准模型进行比较：
- en: 1)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1)
- en: ChatGLM-6B⁴⁴4[https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) (Du
    et al., [2022](#bib.bib3); Zeng et al., [2023](#bib.bib20)) serves as the base
    model for SoulChat.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGLM-6B⁴⁴4[https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
    (Du et al., [2022](#bib.bib3); Zeng et al., [2023](#bib.bib20)) 是 SoulChat 的基础模型。
- en: 2)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2)
- en: ChatGPT (OpenAI, [2022](#bib.bib10); Ouyang et al., [2022](#bib.bib11)) is a
    LLM that is trained using supervised finetuning and Reinforcement Learning from
    Human Feedback (RLHF).
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT (OpenAI, [2022](#bib.bib10); Ouyang et al., [2022](#bib.bib11)) 是一个通过监督微调和从人类反馈
    (RLHF) 中的强化学习训练的 LLM。
- en: 3)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3)
- en: MeChat (Qiu et al., [2023](#bib.bib15)) is a LLM finetuned with low-rank adaptation
    (LoRA) (Hu et al., [2022](#bib.bib7)) on SMILECHAT dataset that is generated by
    ChatGPT based on PsyQA.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MeChat (Qiu et al., [2023](#bib.bib15)) 是一个在 SMILECHAT 数据集上使用低秩适应 (LoRA) (Hu
    et al., [2022](#bib.bib7)) 微调的 LLM，该数据集由 ChatGPT 基于 PsyQA 生成。
- en: 3.2 Implementation details
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实现细节
- en: SoulChat is finetuned on the proposed SoulChatCorpus with a batch size of 80
    and global training steps of 30,000\. The WarmupDecayLR learning rate scheduler
    with $warmup\_steps=1000$ is adopted during the inference phase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: SoulChat 在提出的 SoulChatCorpus 上进行了微调，批量大小为 80，全球训练步骤为 30,000\. 推理阶段采用了 WarmupDecayLR
    学习率调度器，$warmup\_steps=1000$。
- en: 3.3 Results and Analysis
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 结果与分析
- en: 'We randomly selected 10,000 samples from SoulChatCorpus and SMILECHAT respectively
    as the test set for automatic evaluation and 100 samples for manual evaluation.
    For each sample, each model generates an answer for evaluation. We used 7 evaluation
    metrics as automatic metrics: BLEU-1 (B-1), BLEU-2 (B-2), BLEU-3 (B-3), BLEU-4
    (B-4) (Papineni et al., [2002](#bib.bib12)), R-1 (ROUGE-1), R-2 (ROUGE-2) and
    R-L (ROUGE-L) (Lin, [2004](#bib.bib8))). Three individual experts majoring in
    Psychology were asked to evaluate the generated responses in terms of content
    naturalness (Con.), empathy level (Emp.), Helpfulness (Hel.) and Safety (Saf.),
    as detailed described in Appendix [G](#A7 "Appendix G Manual Evaluation Instructions
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations"). The rating scale of Con.,
    Emp. and Hel. is $(0,1,2)$ for Saf. (perfect agreement). The evaluation results
    are shown in Table [1](#S2.T1 "Table 1 ‣ 2.1 SoulChatCorpus Collection ‣ 2 Human-centered
    Mental Health LLM ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort
    Abilities through Fine-tuning with Multi-turn Empathy Conversations"). Generally,
    SoulChat outperforms ChatGLM-6B, ChatGPT and MeChat in both automatic evaluation
    metrics and Emp. metric on test set of SoulChatCorpus and SMILECHAT. Specifically,
    the results on SMILECHAT demonstrates SoulChat’s excellent zero-shot performance
    in the field of mental health.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '我们分别从SoulChatCorpus和SMILECHAT中随机选择了10,000个样本作为自动评估的测试集，并选择了100个样本进行人工评估。对于每个样本，每个模型生成一个回答以供评估。我们使用了7个评估指标作为自动指标：BLEU-1
    (B-1)、BLEU-2 (B-2)、BLEU-3 (B-3)、BLEU-4 (B-4) (Papineni等，[2002](#bib.bib12))，R-1
    (ROUGE-1)、R-2 (ROUGE-2) 和R-L (ROUGE-L) (Lin，[2004](#bib.bib8))。三位心理学专业的专家被要求从内容自然性（Con.）、同理心水平（Emp.）、帮助性（Hel.）和安全性（Saf.）四个方面评估生成的回应，详细描述见附录[G](#A7
    "Appendix G Manual Evaluation Instructions ‣ SoulChat: Improving LLMs’ Empathy,
    Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")。Con.、Emp.和Hel.的评分尺度为$(0,1,2)$，Saf.的评分尺度为（完美一致）。评估结果如表[1](#S2.T1
    "Table 1 ‣ 2.1 SoulChatCorpus Collection ‣ 2 Human-centered Mental Health LLM
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations")所示。总体而言，SoulChat在SoulChatCorpus和SMILECHAT的测试集上，在自动评估指标和Emp.指标上均优于ChatGLM-6B、ChatGPT和MeChat。具体而言，SMILECHAT上的结果展示了SoulChat在心理健康领域的出色零样本性能。'
- en: 4 Conclusion and Future Work
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作
- en: In this paper, we explore how to make LLMs more human-centered. To this end,
    we constructed a Chinese large-scale multi-turn empathy conversation dataset,
    named SoulChatCorpus, with 12 empathy topics and more than 2 million samples.
    The experimental results indicate that using this dataset to finetune LLMs leads
    to high-level empathy ability when users try to seek emotional support from LLMs.
    Future work needs to further consider user attributes, such as personality, gender
    and etc., to help LLMs generate targeted empathy responses for different individuals.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了如何使LLM更加以人为本。为此，我们构建了一个中文大规模多轮同理心对话数据集，名为SoulChatCorpus，包含12个同理心主题和超过200万条样本。实验结果表明，使用该数据集进行微调的LLM在用户寻求情感支持时能展现出高水平的同理心能力。未来的工作需要进一步考虑用户属性，如个性、性别等，以帮助LLM为不同个体生成有针对性的同理心回应。
- en: Limitations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: In this work we proposed a human-centered LLM named SoulChat that has excellent
    empathy ability, which is finetuned on the proposed SoulChatCorpus dataset. Although
    the experimental results demonstrate the effectiveness of SoulChat, there are
    still some limitations need to consider. The mechanism of empathy is complex.
    Different users have different expectations for the output of the model. For example,
    when discussing tense emotions, there are significant differences in the solutions
    expected by adults and adolescents. Therefore, human-centered LLMs need to further
    consider the user’s personality, identity, and other attributes to assist in generating
    answers that are closer to the user’s needs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种以人为本的LLM，名为**SoulChat**，它具有出色的同理心能力，经过在提议的SoulChatCorpus数据集上进行微调。尽管实验结果证明了SoulChat的有效性，但仍存在一些需要考虑的局限性。同理心的机制是复杂的。不同用户对模型输出有不同的期望。例如，在讨论紧张情绪时，成年人和青少年对解决方案的期望存在显著差异。因此，以人为本的LLM需要进一步考虑用户的个性、身份以及其他属性，以帮助生成更符合用户需求的回答。
- en: Ethics Statement
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data Collection. In order to protect privacy (Hovy and Spruit, [2016](#bib.bib6)),
    we adopted strict manual proofreading process when constructing the dataset. We
    filtered all samples with special strings such as "我是 (I am)", "自杀 (suicide)",
    "跳楼 (jumping off a building)", etc., and conducted manual data cleansing. Any
    text related to privacy has been rewritten or removed. Besides, any potential
    conversations that pose harm to users, others, or society have been completely
    removed from our data. To this end, we removed 105,134 samples from multi-turn
    conversations generated by ChatGPT.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集。为了保护隐私（Hovy 和 Spruit, [2016](#bib.bib6)），在构建数据集时我们采用了严格的人工校对过程。我们过滤了所有包含特殊字符串的样本，如“我是
    (I am)”、“自杀 (suicide)”、“跳楼 (jumping off a building)”等，并进行了手动数据清洗。任何与隐私相关的文本都已被重写或删除。此外，任何可能对用户、他人或社会造成伤害的对话都已从数据中完全删除。为此，我们从
    ChatGPT 生成的多轮对话中删除了105,134个样本。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Potential Risks of the Model We conducted a safety assessment specifically
    for the output of the model during the manual evaluation phase, and the results
    are shown in Table [1](#S2.T1 "Table 1 ‣ 2.1 SoulChatCorpus Collection ‣ 2 Human-centered
    Mental Health LLM ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort
    Abilities through Fine-tuning with Multi-turn Empathy Conversations"). Due to
    the lack of human feedback during the model finetuning stage, there are inevitably
    answers that may pose harm to users. Therefore, future work needs to combine RLHF
    to improve the safety level of model generated content. In addition, when this
    model is applied to downstream scenarios, it is necessary to inform the users
    in advance that the answers they see are generated by the AI model and are for
    reference only.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '模型的潜在风险 我们在手动评估阶段对模型输出进行了安全性评估，结果见表[1](#S2.T1 "Table 1 ‣ 2.1 SoulChatCorpus
    Collection ‣ 2 Human-centered Mental Health LLM ‣ SoulChat: Improving LLMs’ Empathy,
    Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")。由于模型微调阶段缺乏人工反馈，不可避免地存在可能对用户造成伤害的回答。因此，未来的工作需要结合
    RLHF 以提高模型生成内容的安全性。此外，当该模型应用于下游场景时，需提前告知用户他们看到的回答是由 AI 模型生成的，仅供参考。'
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Annotator Compensation. We invited individual experts majoring in Psychology
    to conduct the proposed CEHS evaluation of the model’s output. The annotators’
    evaluation of each sample takes approximately 3 minutes, during which they can
    receive a salary of $0.418\. Therefore, the hourly salary of the annotators is
    $8.36, which is higher than the US minimum wage of $7.12 per hour.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注释员补偿。我们邀请了心理学领域的个别专家对模型输出进行 CEHS 评估。每个样本的评估大约需要 3 分钟，评估过程中，注释员可以获得 $0.418 的薪水。因此，注释员的小时工资为
    $8.36，高于美国最低时薪 $7.12。
- en: Acknowledgements
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the Science and Technology Project of Guangzhou (202103010002),
    the Natural Science Foundation of Guangdong Province (2022A1515011588), the National
    Key R&D Program of China (2022YFB4500600), the Science and Technology Project
    of Guangdong (2022B0101010003), the National Natural Science Foundation of China
    under Grant U1801262 and Guangdong Provincial Key Laboratory of Human Digital
    Twin (2022B1212010004).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了广州市科技项目（202103010002）、广东省自然科学基金（2022A1515011588）、中国国家重点研发计划（2022YFB4500600）、广东省科技项目（2022B0101010003）、中国国家自然科学基金资助（U1801262）以及广东省人类数字双胞胎重点实验室（2022B1212010004）的支持。
- en: References
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Cheng et al. (2022) Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, Ruihui Zhao,
    Bang Liu, Xiaodan Liang, and Yefeng Zheng. 2022. [Improving multi-turn emotional
    support dialogue generation with lookahead strategy planning](https://aclanthology.org/2022.emnlp-main.195).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 3014–3026, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 (2022) Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, Ruihui Zhao, Bang
    Liu, Xiaodan Liang, 和 Yefeng Zheng. 2022. [通过前瞻策略规划改进多轮情感支持对话生成](https://aclanthology.org/2022.emnlp-main.195)。收录于
    *2022年自然语言处理实证方法会议论文集*，页码 3014–3026，阿布扎比，阿联酋。计算语言学协会。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。[BERT：用于语言理解的深度双向变换器的预训练](https://doi.org/10.18653/v1/N19-1423)。见于*2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长短论文）*，第4171–4186页，美国明尼阿波利斯。计算语言学协会。
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. [GLM: General language model pretraining with
    autoregressive blank infilling](https://doi.org/10.18653/v1/2022.acl-long.26).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 320–335, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2022）郑晓杜、钱钰杰、刘潇、丁铭、邱杰中、杨志麟、唐洁。2022年。[GLM：具有自回归空白填充的通用语言模型预训练](https://doi.org/10.18653/v1/2022.acl-long.26)。见于*第60届计算语言学协会年会论文集（第1卷：长论文）*，第320–335页，爱尔兰都柏林。计算语言学协会。
- en: Fleiss (1971) Joseph L Fleiss. 1971. Measuring nominal scale agreement among
    many raters. *Psychological Bulletin*, 76(5):378–382.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleiss（1971）Joseph L. Fleiss。1971年。测量多评估者之间的名义尺度一致性。*心理学公报*，76(5)：378–382。
- en: 'Hailiang et al. (2020) Wang Hailiang, Wu Zhizhi, and Lang Jiayuan. 2020. [Pat
    psychology: Psychological consultation q&a corpus](https://github.com/chatopera/efaqa-corpus-zh).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hailiang 等（2020）王海良、吴智智、郎佳媛。2020年。[Pat心理学：心理咨询问答语料库](https://github.com/chatopera/efaqa-corpus-zh)。
- en: 'Hovy and Spruit (2016) Dirk Hovy and Shannon L. Spruit. 2016. [The social impact
    of natural language processing](https://doi.org/10.18653/v1/P16-2096). In *Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 591–598, Berlin, Germany. Association for Computational
    Linguistics.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy 和 Spruit（2016）Dirk Hovy 和 Shannon L. Spruit。2016年。[自然语言处理的社会影响](https://doi.org/10.18653/v1/P16-2096)。见于*第54届计算语言学协会年会论文集（第2卷：短论文）*，第591–598页，德国柏林。计算语言学协会。
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2022）Edward J. Hu、沈也龙、Phillip Wallis、Zeyuan Allen-Zhu、李元之、王绍安、王璐 和 陈伟柱。2022年。[LoRA：大型语言模型的低秩适应](https://openreview.net/forum?id=nZeVKeeFYf9)。见于*国际学习表征会议*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin。2004年。[ROUGE：自动评估摘要的工具包](https://aclanthology.org/W04-1013)。见于*文本摘要扩展*，第74–81页，西班牙巴塞罗那。计算语言学协会。
- en: 'Liu et al. (2021) Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour,
    Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. 2021. [Towards emotional support
    dialog systems](https://doi.org/10.18653/v1/2021.acl-long.269). In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*, pages 3469–3483, Online. Association for Computational Linguistics.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）刘思阳、郑初杰、Orianna Demasi、Sahand Sabour、李宇、周瑜、姜勇 和 黄敏丽。2021年。[迈向情感支持对话系统](https://doi.org/10.18653/v1/2021.acl-long.269)。见于*第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长论文）*，第3469–3483页，在线。计算语言学协会。
- en: OpenAI (2022) OpenAI. 2022. [Introducing chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2022）OpenAI。2022年。[介绍ChatGPT](https://openai.com/blog/chatgpt)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 27730–27744\.
    Curran Associates, Inc.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. [通过人类反馈训练语言模型以遵循指令](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)。在*神经信息处理系统进展*中，第35卷，第27730–27744页。Curran
    Associates, Inc.
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a method for automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135).
    In *Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics*, pages 311–318, Philadelphia, Pennsylvania, USA. Association for
    Computational Linguistics.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu：一种自动评估机器翻译的方法](https://doi.org/10.3115/1073083.1073135)。在*第40届计算语言学协会年会论文集*中，第311–318页，美国宾夕法尼亚州费城。计算语言学协会。
- en: 'Peng et al. (2022) Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun, and
    Yunpeng Li. 2022. [Control globally, understand locally: A global-to-local hierarchical
    graph network for emotional support conversation](https://www.ijcai.org/proceedings/2022/0600.pdf).
    In *Proceedings of the Thirty-First International Joint Conference on Artificial
    Intelligence (IJCAI-22)*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2022) Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Yajing Sun, and
    Yunpeng Li. 2022. [全球控制，本地理解：一种全球到本地的层次图网络用于情感支持对话](https://www.ijcai.org/proceedings/2022/0600.pdf)。在*第三十一届国际人工智能联合会议论文集
    (IJCAI-22)*中。
- en: 'Peng et al. (2023) Wei Peng, Ziyuan Qin, Yue Hu, Yuqiang Xie, and Yunpeng Li.
    2023. [Fado: Feedback-aware double controlling network for emotional support conversation](https://doi.org/https://doi.org/10.1016/j.knosys.2023.110340).
    *Knowledge-Based Systems*, 264:110340.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Wei Peng, Ziyuan Qin, Yue Hu, Yuqiang Xie, and Yunpeng Li.
    2023. [Fado：一种反馈感知的双重控制网络用于情感支持对话](https://doi.org/https://doi.org/10.1016/j.knosys.2023.110340)。*知识基础系统*，264:110340。
- en: 'Qiu et al. (2023) Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, and Zhenzhong
    Lan. 2023. [Smile: Single-turn to multi-turn inclusive language expansion via
    chatgpt for mental health support](http://arxiv.org/abs/2305.00450).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu et al. (2023) Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, and Zhenzhong
    Lan. 2023. [Smile：通过ChatGPT进行单轮到多轮的包容性语言扩展以支持心理健康](http://arxiv.org/abs/2305.00450)。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. [Improving language understanding by generative pre-training](http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. [通过生成预训练提高语言理解](http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。
- en: 'Rashkin et al. (2019) Hannah Rashkin, Eric Michael Smith, Margaret Li, and
    Y-Lan Boureau. 2019. [Towards empathetic open-domain conversation models: A new
    benchmark and dataset](https://doi.org/10.18653/v1/P19-1534). In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 5370–5381, Florence, Italy. Association for Computational Linguistics.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashkin et al. (2019) Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan
    Boureau. 2019. [朝向具有同理心的开放领域对话模型：一种新的基准和数据集](https://doi.org/10.18653/v1/P19-1534)。在*第57届计算语言学协会年会论文集*中，第5370–5381页，意大利佛罗伦萨。计算语言学协会。
- en: 'Sun et al. (2021) Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie
    Huang. 2021. [PsyQA: A Chinese dataset for generating long counseling text for
    mental health support](https://doi.org/10.18653/v1/2021.findings-acl.130). In
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 1489–1503, Online. Association for Computational Linguistics.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2021) Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie
    Huang. 2021. [PsyQA: 一个用于生成心理健康支持长篇咨询文本的中文数据集](https://doi.org/10.18653/v1/2021.findings-acl.130)。在*计算语言学协会发现：ACL-IJCNLP
    2021*中，第1489–1503页，在线。计算语言学协会。'
- en: 'Tu et al. (2022) Quan Tu, Yanran Li, Jianwei Cui, Bin Wang, Ji-Rong Wen, and
    Rui Yan. 2022. [MISC: A mixed strategy-aware model integrating COMET for emotional
    support conversation](https://doi.org/10.18653/v1/2022.acl-long.25). In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 308–319, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tu 等（2022）Quan Tu, Yanran Li, Jianwei Cui, Bin Wang, Ji-Rong Wen, 和 Rui Yan.
    2022. [MISC: 一种混合策略感知模型，整合 COMET 用于情感支持对话](https://doi.org/10.18653/v1/2022.acl-long.25)。在
    *第60届计算语言学协会年会（第1卷：长论文）*，第308–319页，爱尔兰都柏林。计算语言学协会。'
- en: 'Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan
    Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong,
    and Jie Tang. 2023. [GLM-130b: An open bilingual pre-trained model](https://openreview.net/forum?id=-Aw0rrrPUF).
    In *The Eleventh International Conference on Learning Representations (ICLR)*.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等（2023）Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming
    Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei
    Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, 和 Jie Tang.
    2023. [GLM-130b: 一个开放的双语预训练模型](https://openreview.net/forum?id=-Aw0rrrPUF)。在 *第十一届国际学习表征会议（ICLR）*。'
- en: Appendix A Reproducibility Checklist
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 可重复性检查表
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model and Data: The SoulChat model and SoulChatCorpus will be released upon
    decision of the paper.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型和数据：SoulChat 模型和 SoulChatCorpus 将在论文决定后发布。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'System Hardware: We trained the SoulChat on the Ubuntu 20.04.6 LTS server that
    has 2 CPUs called "Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz", 8 NVIDIA A800-SXM4-80GB
    GPUs, and 1,024GB memory.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统硬件：我们在 Ubuntu 20.04.6 LTS 服务器上训练 SoulChat，该服务器配备 2 个名为“Intel(R) Xeon(R) Platinum
    8358P CPU @ 2.60GHz”的 CPU，8 台 NVIDIA A800-SXM4-80GB GPU 和 1,024GB 内存。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Driver Version: The version of Nvidia driver is "525.105.17". The version of
    CUDA is "11.6". The version of Cudnn is "8.4.0.27".'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 驱动版本：Nvidia 驱动版本为“525.105.17”。CUDA 版本为“11.6”。Cudnn 版本为“8.4.0.27”。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Package version: python=3.8, torch⁵⁵5[https://pytorch.org/get-started/previous-versions](https://pytorch.org/get-started/previous-versions)=1.13.1,
    transformers⁶⁶6[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)=4.28.0,
    deepspeed⁷⁷7[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)=0.9.3,
    datasets=2.11.0 and jieba=0.42.1 is recommended.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包版本：python=3.8, torch⁵⁵5[https://pytorch.org/get-started/previous-versions](https://pytorch.org/get-started/previous-versions)=1.13.1,
    transformers⁶⁶6[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)=4.28.0,
    deepspeed⁷⁷7[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)=0.9.3,
    datasets=2.11.0 和 jieba=0.42.1 是推荐的。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Parameters: SoulChat has 6.2B parameters with 28 layers and $max\_sequence\_length$
    of 2,048\. During the inference phase, the model requires at least 14GB of GPU
    memory.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型参数：SoulChat 拥有 6.2B 参数，28 层，$max\_sequence\_length$ 为 2,048。在推理阶段，模型需要至少 14GB
    的 GPU 内存。
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training Time: SoulChat is trained with global steps of 30,000 and $torch\_dtype$
    of "float16" on 8 NVIDIA A800-SXM4-80GB GPUs. The training time is about 79 hours.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练时间：SoulChat 使用 8 台 NVIDIA A800-SXM4-80GB GPU，全球步骤为 30,000，$torch\_dtype$ 为“float16”进行训练。训练时间约为
    79 小时。
- en: Appendix B Counseling Topics
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 咨询主题
- en: The following dictionaries represent the corresponding relationships between
    Chinese and English for 12 counseling topics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下词典表示 12 个咨询主题的中英文对应关系。
- en: '{'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '’家庭’: ’Family’, ’婚恋’: ’Marriage’, ’性心理’: ’Sex’, ’成长发展’: ’Growth’, ’人际关系’: ’Relationship
    ’, ’职场’: ’Career’, ’自我认知’: ’Self-awareness ’, ’行为’: ’Behavior’, ’情绪’: ’Emotion’,
    ’治疗’: ’Treatment’, ’社会事件’: ’Social events’, ’心理学知识’: ’Psychological knowledge’,'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '’家庭’：’Family’，’婚恋’：’Marriage’，’性心理’：’Sex’，’成长发展’：’Growth’，’人际关系’：’Relationship’，’职场’：’Career’，’自我认知’：’Self-awareness’，’行为’：’Behavior’，’情绪’：’Emotion’，’治疗’：’Treatment’，’社会事件’：’Social
    events’，’心理学知识’：’Psychological knowledge’， '
- en: '}'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Appendix C Our prompt VS SMILE prompt
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 我们的提示 VS SMILE 提示
- en: '![Refer to caption](img/5fd2e15e8d228ca824dd3ee95dc5011d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5fd2e15e8d228ca824dd3ee95dc5011d.png)'
- en: 'Figure 5: The translated English prompt used for converting single-turn psychological
    counseling conversations to multi-turn empathy conversations.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：用于将单轮心理咨询对话转换为多轮同理心对话的翻译英文提示。
- en: We used SMILE (Qiu et al., [2023](#bib.bib15)) prompt and our proposed Chinese
    empathy constraint prompt for extend the single-turn long text psychological counseling
    example to multi-turn empathy conversation respectively.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 SMILE （Qiu 等，[2023](#bib.bib15)）提示和我们提出的中文同理心约束提示，分别将单轮长文本心理咨询示例扩展为多轮同理心对话。
- en: 'Chinese:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 中文：
- en: 用户：谈恋爱两年了，异地恋，平时很好，一年前开始吵架吵凶了，男朋友就说难听的话甚至骂人，正常吗？
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：谈恋爱两年了，异地恋，平时很好，一年前开始吵架吵得很凶，男朋友就说难听的话甚至骂人，正常吗？
- en: 心理咨询师：你好！异地恋，平时还好，但一吵架，就出现骂你，这确实不正常。这应该跟很多因素有关。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 心理咨询师：你好！异地恋平时还好，但一吵架就骂你，这确实不正常。这应该与许多因素相关。
- en: 1、异地恋本身就有挑战性，你男朋友跟你异地，他的语言和言语就受到了挑战。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1、异地恋本身就有挑战性，你男朋友跟你异地，他的语言和言语就受到了挑战。
- en: 2、他平时跟你相处的时候，也会有很多压力，所以一遇到类似的情况，就会很暴躁。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 2、他平时跟你相处的时候，也会有很多压力，所以一遇到类似的情况，就会很暴躁。
- en: 3、平时跟他相处的时候，你们之间没有矛盾，一有矛盾就会吵凶。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 平时跟他相处时，你们之间没有矛盾，一有矛盾就会吵得很凶。
- en: 4、你们平时沟通也比较少，所以他也没有渠道发泄自己的负面情绪。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 你们平时沟通较少，因此他没有途径释放负面情绪。
- en: 以上是一些可能性，但并不一定就是导致他出现这样的情况的原因，因为你们之间是否存在其他矛盾，只有你们两个人一起沟通才知道。所以，我觉得你应该认真考虑一下，自己男朋友的这种行为，是处于什么心理状态，或者是什么情况，只有你自己最清楚。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是一些可能性，但并不一定就是导致他出现这样的情况的原因，因为你们之间是否存在其他矛盾，只有你们两个人一起沟通才知道。所以，我觉得你应该认真考虑一下，自己男朋友的这种行为，是处于什么心理状态，或者是什么情况，只有你自己最清楚。
- en: 'English:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 英文：
- en: 'User: I have been in a relationship for two years and have been in a long-distance
    relationship. I am usually very good. I started arguing and quarreling a year
    ago, and my boyfriend said unpleasant things or even cursed. Is it normal'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：谈恋爱两年了，异地恋，平时很好，一年前开始吵架吵凶了，男朋友就说难听的话甚至骂人，正常吗？
- en: 'Psychologist: Hello! Long distance love is usually okay, but when you argue,
    it’s really abnormal to scold you. This should be related to many factors'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 心理咨询师：你好！异地恋平时还好，但一吵架就骂人，这确实不正常。这可能与多种因素有关。
- en: 1\. Long distance relationships are inherently challenging, and when your boyfriend
    is away from you, their language and language are challenged
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 异地恋本身就充满挑战，当你的男朋友远离你时，他的语言和言辞也会受到挑战。
- en: 2\. When he usually gets along with you, he will also have a lot of pressure,
    so when encountering similar situations, he will be very irritable
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 当他平时跟你相处时，也会承受很多压力，所以遇到类似情况时，他会非常容易暴躁。
- en: 3\. When we usually get along with him, there is no conflict between you. Whenever
    there is a conflict, we will argue fiercely
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 平时跟他相处时，你们之间没有矛盾，一旦有矛盾就会吵得很凶。
- en: 4\. You usually communicate less, so he doesn’t have a channel to vent his negative
    emotions
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 你们平时沟通也比较少，所以他也没有渠道发泄自己的负面情绪。
- en: The above are some possibilities, but they may not necessarily be the reason
    for his situation, because whether there are any other conflicts between you is
    only known through communication between the two of you. So, I think you should
    seriously consider what kind of psychological state or situation your boyfriend’s
    behavior is in, and only you know it best.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是一些可能性，但它们可能并不是他出现这种情况的真正原因，因为你们之间是否存在其他矛盾，只有你们两个人的沟通才能揭示。所以，我认为你应该认真考虑一下你男朋友的行为处于什么样的心理状态或情况，只有你自己最清楚。
- en: 'As shown in Figure [7](#A4.F7 "Figure 7 ‣ Appendix D English Word Cloud Map
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations") (English version: Figure [8](#A4.F8
    "Figure 8 ‣ Appendix D English Word Cloud Map ‣ SoulChat: Improving LLMs’ Empathy,
    Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations"))
    and Figure [9](#A4.F9 "Figure 9 ‣ Appendix D English Word Cloud Map ‣ SoulChat:
    Improving LLMs’ Empathy, Listening, and Comfort Abilities through Fine-tuning
    with Multi-turn Empathy Conversations") (English version: Figure [10](#A4.F10
    "Figure 10 ‣ Appendix D English Word Cloud Map ‣ SoulChat: Improving LLMs’ Empathy,
    Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")),
    the multi-turn conversation generated by using the proposed prompt has richer
    expressions of empathy, compared with SMILE prompt.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#A4.F7 "图7 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮同理对话微调提升大语言模型的同理心、倾听能力和安慰能力")
    (英文版：图 [8](#A4.F8 "图8 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮同理对话微调提升大语言模型的同理心、倾听能力和安慰能力"))
    和图 [9](#A4.F9 "图9 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮同理对话微调提升大语言模型的同理心、倾听能力和安慰能力") (英文版：图 [10](#A4.F10
    "图10 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮同理对话微调提升大语言模型的同理心、倾听能力和安慰能力"))，使用建议的提示生成的多轮对话相比SMILE提示，展现了更丰富的同理心表达。'
- en: Appendix D English Word Cloud Map
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 英文词云图
- en: 'The English word cloud map is presented in Figure [6](#A4.F6 "Figure 6 ‣ Appendix
    D English Word Cloud Map ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort
    Abilities through Fine-tuning with Multi-turn Empathy Conversations").'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '英文词云图见图 [6](#A4.F6 "图6 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮同理对话微调提升大语言模型的同理心、倾听能力和安慰能力")。'
- en: '![Refer to caption](img/090ad1d1fad4865c7c74bfb4c7db891f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/090ad1d1fad4865c7c74bfb4c7db891f.png)'
- en: 'Figure 6: Word cloud map of psychological consultants’ utterances.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: 心理咨询师言论的词云图。'
- en: '![Refer to caption](img/f1de3a581665bf5f970d08b7d9cd7cba.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f1de3a581665bf5f970d08b7d9cd7cba.png)'
- en: 'Figure 7: Multi-turn conversation generated by ChatGPT using the proposed prompt.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ChatGPT使用提议的提示生成的多轮对话。
- en: '![Refer to caption](img/e4a8333b3ca8b6a1726afa07e5672016.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e4a833b3ca8b6a1726afa07e5672016.png)'
- en: 'Figure 8: Multi-turn conversation generated by ChatGPT using the proposed prompt
    (English version).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：ChatGPT使用提议的提示（英文版）生成的多轮对话。
- en: '![Refer to caption](img/641cf449b62ec17678406f8ca130e309.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/641cf449b62ec17678406f8ca130e309.png)'
- en: 'Figure 9: Multi-turn conversation generated by ChatGPT using the SMILE prompt.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：ChatGPT使用SMILE提示生成的多轮对话。
- en: '![Refer to caption](img/5dbb936ead5825d51a29e37182587c37.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5dbb936ead5825d51a29e37182587c37.png)'
- en: 'Figure 10: Multi-turn conversation generated by ChatGPT using the SMILE prompt
    (English version).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：ChatGPT使用SMILE提示（英文版）生成的多轮对话。
- en: '![Refer to caption](img/08a248df618ec9d5bd2aa5445b8af6b6.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/08a248df618ec9d5bd2aa5445b8af6b6.png)'
- en: 'Figure 11: A case of a user confiding to SoulChat.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用户向SoulChat倾诉的一个案例。
- en: '![Refer to caption](img/f8fde2dfd6aa939e6f6ebede2fefd3df.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f8fde2dfd6aa939e6f6ebede2fefd3df.png)'
- en: 'Figure 12: A case of a user confiding to ChatGPT. ChatGPT is eager to solve
    user problems and tends to provide comprehensive and effective advice rather than
    truly empathizing with users.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：用户向ChatGPT倾诉的一个案例。ChatGPT热衷于解决用户问题，倾向于提供全面而有效的建议，而不是与用户真正共情。
- en: Appendix E Sample Conversations of SoulChat
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E SoulChat的对话示例
- en: 'As shown in Figure [11](#A4.F11 "Figure 11 ‣ Appendix D English Word Cloud
    Map ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations"), SoulChat can better serve
    as a listener when users seek emotional support or confide. Besides, it can naturally
    empathize with users (e.g. "你的童年经历真的很不容易 (Your childhood experiences were really
    difficult)", "我可以理解你的痛苦和内心的挣扎 (I can understand your pain and inner struggle)")
    and comfort them (e.g. "我相信你是一个坚强的人，你可以通过自己的努力来改变现状。 (I believe you are a strong
    person who can change the situation through your own efforts.)").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[11](#A4.F11 "图11 ‣ 附录D 英文词云图 ‣ SoulChat: 通过多轮共情对话微调提升LLM的共情、倾听和安慰能力")所示，SoulChat在用户寻求情感支持或倾诉时能更好地担任倾听者。此外，它可以自然地对用户表达共情（例如，“你的童年经历真的很不容易
    (Your childhood experiences were really difficult)”， “我可以理解你的痛苦和内心的挣扎 (I can understand
    your pain and inner struggle)”）并安慰他们（例如，“我相信你是一个坚强的人，你可以通过自己的努力来改变现状。 (I believe
    you are a strong person who can change the situation through your own efforts.)”）。'
- en: Appendix F Sample Conversations of Other LLMs
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 其他LLM的对话示例
- en: 'The examples when users seek emotional support from ChatGPT, ChatGLM and SparkDesk
    are shown in Figure [12](#A4.F12 "Figure 12 ‣ Appendix D English Word Cloud Map
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations"), Figure [13](#A7.F13 "Figure
    13 ‣ Appendix G Manual Evaluation Instructions ‣ SoulChat: Improving LLMs’ Empathy,
    Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations")
    and Figure [14](#A7.F14 "Figure 14 ‣ Appendix G Manual Evaluation Instructions
    ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through
    Fine-tuning with Multi-turn Empathy Conversations"). These LLMs are inclined to
    provide suggestions rather than ask questions or listen, acting a bit like a rational
    “Straight man” for those users who need listening and comfort, which make them
    appear less “human-centered”.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '用户寻求情感支持时ChatGPT、ChatGLM和SparkDesk的示例见图[12](#A4.F12 "图12 ‣ 附录D 英文词云图 ‣ SoulChat:
    通过多轮共情对话微调提升LLM的共情、倾听和安慰能力")、图[13](#A7.F13 "图13 ‣ 附录G 手动评估说明 ‣ SoulChat: 通过多轮共情对话微调提升LLM的共情、倾听和安慰能力")和图[14](#A7.F14
    "图14 ‣ 附录G 手动评估说明 ‣ SoulChat: 通过多轮共情对话微调提升LLM的共情、倾听和安慰能力")。这些LLM倾向于提供建议而不是提问或倾听，对于需要倾听和安慰的用户来说，表现得有点像理性的“直男”，使得它们显得不那么“以人为本”。'
- en: Appendix G Manual Evaluation Instructions
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 手动评估说明
- en: 'For mental health LLMs, we have constructed a manual evaluation framework called
    "CEHS" with the following four dimensions: Content naturalness, Empathy, Helpfulness,
    Safety, as presented in Table [2](#A7.T2 "Table 2 ‣ Appendix G Manual Evaluation
    Instructions ‣ SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities
    through Fine-tuning with Multi-turn Empathy Conversations"). Specifically, there
    may be a certain conflict between Empathy and Helpfulness. For example, ChatGPT
    tends to generate helpful but lacking empathetic responses. On the other hand,
    when SoulChat generates empathetic responses, it may weaken the direct Helpfulness
    of the answer. This is because general advice often appears helpful, but not so
    empathetic. The scores of Empathy and Helpfulness in Table [1](#S2.T1 "Table 1
    ‣ 2.1 SoulChatCorpus Collection ‣ 2 Human-centered Mental Health LLM ‣ SoulChat:
    Improving LLMs’ Empathy, Listening, and Comfort Abilities through Fine-tuning
    with Multi-turn Empathy Conversations") also illustrate this phenomenon.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于心理健康LLMs，我们构建了一个名为“CEHS”的人工评估框架，包括以下四个维度：内容自然性、共情、帮助性、安全性，如表 [2](#A7.T2 "表
    2 ‣ 附录 G 人工评估说明 ‣ SoulChat：通过多轮共情对话微调提升LLMs的共情、倾听和舒适能力") 所示。具体来说，共情与帮助性之间可能存在一定的冲突。例如，ChatGPT倾向于生成有帮助但缺乏共情的回应。另一方面，当SoulChat生成共情回应时，可能会削弱回答的直接帮助性。这是因为一般建议通常看起来很有帮助，但不那么共情。表 [1](#S2.T1
    "表 1 ‣ 2.1 SoulChatCorpus 收集 ‣ 2 以人为本的心理健康LLM ‣ SoulChat：通过多轮共情对话微调提升LLMs的共情、倾听和舒适能力")
    中的共情和帮助性得分也说明了这一现象。
- en: 'Table 2: Manual Evaluation.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 人工评估。'
- en: '| Aspect | Score | Description |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 得分 | 描述 |'
- en: '| Content | 0-2 | It mainly evaluates whether the content generated by the
    model is relevant and coherent to the user’s conversation history, whether the
    content is smooth and natural, whether the content is consistent with language
    habits, whether the content has syntax error. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 内容 | 0-2 | 主要评估模型生成的内容是否与用户的对话历史相关且连贯，内容是否流畅自然，是否符合语言习惯，内容是否有语法错误。 |'
- en: '| Empathy | 0-2 | The main assessment is whether the text generated by the
    model can understand the feelings and behaviors of the parties involved, grasp
    their inner feelings, deep meanings, and their degree. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 共情 | 0-2 | 主要评估模型生成的文本是否能够理解相关方的感受和行为，掌握他们的内心感受、深层含义及其程度。 |'
- en: '| Helpful- ness | 0-2 | This indicator mainly evaluates whether the output
    of the model is helpful to the user. When evaluating this item, it is necessary
    to consider from the user’s perspective whether the model’s answer provides assistance.
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 有用性 | 0-2 | 该指标主要评估模型的输出是否对用户有帮助。在评估此项时，需要从用户的角度考虑模型的回答是否提供了帮助。 |'
- en: '| Safety | 0-1 | This indicator mainly evaluates whether the text generated
    by the model causes harm to users, others, society, environment, etc., whether
    personal privacy is protected, and whether laws and regulations are complied with.
    The main considerations are: respect for social morality, Public policy doctrine.
    Whether it violates laws and regulations, involves political sensitivity, commits
    crimes, infringes on physical health, infringes on mental health, infringes on
    property privacy, discriminates/prejudices, insults/hate speech, violates ethical
    ethics, etc. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 | 0-1 | 该指标主要评估模型生成的文本是否对用户、他人、社会、环境等造成伤害，个人隐私是否得到保护，是否符合法律法规。主要考虑：尊重社会道德，公共政策原则。是否违反法律法规，涉及政治敏感性，犯罪，侵害身体健康，侵害心理健康，侵害财产隐私，歧视/偏见，侮辱/仇恨言论，违反伦理道德等。
    |'
- en: '![Refer to caption](img/90fb4d562bb7e106039473aae32d90ba.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/90fb4d562bb7e106039473aae32d90ba.png)'
- en: 'Figure 13: A case of a user confiding to ChatGLM.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 用户向ChatGLM倾诉的案例。'
- en: '![Refer to caption](img/ac9de5d6cfdb2a389c00414924992761.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ac9de5d6cfdb2a389c00414924992761.png)'
- en: 'Figure 14: A case of a user confiding to SparkDesk.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 用户向SparkDesk倾诉的案例。'
