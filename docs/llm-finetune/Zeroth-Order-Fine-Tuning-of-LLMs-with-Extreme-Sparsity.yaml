- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:36:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:36:11'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零阶微调 LLM 的极端稀疏性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02913](https://ar5iv.labs.arxiv.org/html/2406.02913)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02913](https://ar5iv.labs.arxiv.org/html/2406.02913)
- en: Wentao Guo wg0420@princeton.edu, Princeton University Jikai Long {jlong1,yran1,xyu38,zxu79}@stevens.edu,
    Stevens Institute of Technology Yimeng Zeng {yimengz,jacobrg,obastani}@seas.upenn.edu,
    University of Pennsylvania Zirui Liu zl105@rice.edu, Rice University Xinyu Yang
    {xinyuya2,beidic}@andrew.cmu.edu, Carnegie Mellon University Yide Ran {jlong1,yran1,xyu38,zxu79}@stevens.edu,
    Stevens Institute of Technology Jacob R. Gardner {yimengz,jacobrg,obastani}@seas.upenn.edu,
    University of Pennsylvania Osbert Bastani {yimengz,jacobrg,obastani}@seas.upenn.edu,
    University of Pennsylvania Christopher De Sa cdesa@cs.cornell.edu, Cornell University
    Xiaodong Yu {jlong1,yran1,xyu38,zxu79}@stevens.edu, Stevens Institute of Technology
    Beidi Chen {xinyuya2,beidic}@andrew.cmu.edu, Carnegie Mellon University Zhaozhuo
    Xu {jlong1,yran1,xyu38,zxu79}@stevens.edu, Stevens Institute of Technology
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郭文涛 wg0420@princeton.edu, 普林斯顿大学 龙季开 {jlong1,yran1,xyu38,zxu79}@stevens.edu,
    史蒂文斯理工学院 曾一萌 {yimengz,jacobrg,obastani}@seas.upenn.edu, 宾夕法尼亚大学 刘紫睿 zl105@rice.edu,
    赖斯大学 杨心瑜 {xinyuya2,beidic}@andrew.cmu.edu, 卡内基梅隆大学 冉艺德 {jlong1,yran1,xyu38,zxu79}@stevens.edu,
    史蒂文斯理工学院 雅各布·R·加德纳 {yimengz,jacobrg,obastani}@seas.upenn.edu, 宾夕法尼亚大学 奥斯伯特·巴斯塔尼
    {yimengz,jacobrg,obastani}@seas.upenn.edu, 宾夕法尼亚大学 克里斯托弗·德·萨 cdesa@cs.cornell.edu,
    康奈尔大学 余晓东 {jlong1,yran1,xyu38,zxu79}@stevens.edu, 史蒂文斯理工学院 陈北笛 {xinyuya2,beidic}@andrew.cmu.edu,
    卡内基梅隆大学 徐兆卓 {jlong1,yran1,xyu38,zxu79}@stevens.edu, 史蒂文斯理工学院
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning
    Large Language Models using only forward passes. However, the application of ZO
    fine-tuning in memory-constrained settings such as mobile phones and laptops is
    still challenging since full precision forward passes are infeasible. In this
    study, we address this limitation by integrating sparsity and quantization into
    ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of fine-tuning
    an extremely small subset of LLM parameters using ZO. This approach allows the
    majority of un-tuned parameters to be quantized to accommodate the constraint
    of limited device memory. Our findings reveal that the pre-training process can
    identify a set of “sensitive parameters” that can guide the ZO fine-tuning of
    LLMs on downstream tasks. Our results demonstrate that fine-tuning 0.1% sensitive
    parameters in the LLM with ZO can outperform the full ZO fine-tuning performance,
    while offering wall-clock time speedup. Additionally, we show that ZO fine-tuning
    targeting these 0.1% sensitive parameters, combined with 4 bit quantization, enables
    efficient ZO fine-tuning of an Llama2-7B model on a GPU device with less than
    8GiB of memory and notably reduced latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 零阶优化（ZO）是一种内存高效的策略，用于仅通过前向传递来微调大型语言模型（LLM）。然而，在如手机和笔记本电脑等内存受限的环境中应用 ZO 微调仍然具有挑战性，因为全精度前向传递是不可行的。在本研究中，我们通过将稀疏性和量化集成到
    LLM 的 ZO 微调中来解决这一限制。具体而言，我们研究了使用 ZO 微调 LLM 参数的一个极小子集的可行性。这种方法允许将大多数未微调的参数量化，以适应有限设备内存的约束。我们的研究结果表明，预训练过程可以识别一组“敏感参数”，这些参数可以指导
    LLM 在下游任务中的 ZO 微调。我们的结果表明，使用 ZO 微调 LLM 中 0.1% 的敏感参数可以超越完整 ZO 微调的性能，同时提供墙钟时间加速。此外，我们还展示了，针对这些
    0.1% 敏感参数的 ZO 微调，结合 4 位量化，可以在内存少于 8GiB 的 GPU 设备上高效地微调 Llama2-7B 模型，并显著降低延迟。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Large language models (LLMs) have demonstrated superior performance in general-purpose
    language generation [[1](#bib.bib1), [35](#bib.bib35), [22](#bib.bib22)]. Despite
    their success, it remains necessary to fine-tune LLMs for specific tasks to achieve
    optimal results. However, fine-tuning LLMs often requires much more memory compared
    to the inference process. Specifically, there are mainly four parts that occupy
    the memory during fine-tuning LLMs: (1) the weight parameter itself; (2) the optimizer
    state, which contains the information about the past gradient [[16](#bib.bib16)];
    (3) the weight gradient used to update the parameters; (4) the activation cached
    to calculate the weight gradient [[25](#bib.bib25)]; In previous work like QLoRA
    [[7](#bib.bib7)], it can reduce both (1) and (2) by combining weight quantization
    and low-rank adaption [[12](#bib.bib12)], which enables fine-tuning huge LLMs
    under data-center level GPUs. However, under more memory-constraint hardware like
    cell phones, the memory of caching (3) weight gradient and (4) activation required
    by backpropagation still cannot be overlooked. The disparity between the demand
    of LLM fine-tuning and hardware capacity limits the adaptability of LLMs, especially
    when personalizing them for edge devices.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在通用语言生成中表现出色[[1](#bib.bib1), [35](#bib.bib35), [22](#bib.bib22)]。尽管如此，为了获得**最佳**结果，仍需对LLMs进行针对特定任务的微调。然而，相较于推理过程，微调LLMs通常需要更多的内存。具体来说，微调LLMs过程中主要有四个部分占用内存：（1）权重参数本身；（2）优化器状态，包含过去梯度的信息[[16](#bib.bib16)];
    （3）用于更新参数的权重梯度；（4）缓存的激活，用于计算权重梯度[[25](#bib.bib25)]。在之前的工作中，例如QLoRA[[7](#bib.bib7)]，通过结合权重量化和低秩适应[[12](#bib.bib12)]，可以减少（1）和（2），这使得在数据中心级别的GPU上微调大型LLMs成为可能。然而，在更具内存限制的硬件如手机上，反向传播所需的（3）权重梯度和（4）激活的缓存仍然不容忽视。LLM微调的需求与硬件容量之间的差距限制了LLMs的适应性，尤其是在为边缘设备个性化时。
- en: Exploring Zeroth-Order Optimization in LLM Fine-Tuning. Recently, there has
    been a resurging interest in zeroth-order (ZO) optimization methods for LLM fine-tuning
    [[27](#bib.bib27), [23](#bib.bib23), [3](#bib.bib3)]. ZO optimization method perturbs
    model parameters in random directions and utilize the loss value difference to
    compute the gradient direction for parameter update. One advantage of ZO methods
    in LLM fine-tuning is that they do not require backpropagation procedures, which
    significantly saves the computation and memory. In this way, ZO is backpropagation-free
    and does not need to cache (3) weight gradients and (4) activations during fine-tuning.
    In practice, ZO methods have demonstrated the potential to achieve performance
    comparable to first-order methods in LLM fine-tuning, which opens the doors for
    various efficient LLM adaptation strategies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 探索LLM微调中的零阶优化。最近，对零阶（ZO）优化方法在LLM微调中的兴趣有所复苏[[27](#bib.bib27), [23](#bib.bib23),
    [3](#bib.bib3)]。ZO优化方法通过在随机方向上扰动模型参数，并利用损失值差异计算梯度方向以更新参数。ZO方法在LLM微调中的一个优势是它们不需要反向传播过程，这显著节省了计算和内存。因此，ZO不需要在微调过程中缓存（3）权重梯度和（4）激活。实际上，ZO方法已展示了与一阶方法相媲美的性能潜力，为各种高效的LLM适应策略打开了大门。
- en: '![Refer to caption](img/5fbd4cac51b7d68c96e4880d443af3af.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5fbd4cac51b7d68c96e4880d443af3af.png)'
- en: 'Figure 1: Training & inference speed of Llama2-7B. As the sensitive sparse
    fine-tuning method achieves great performance via optimizing only 0.1% parameters
    (performance comparable to ZO full fine-tuning and 10% random subsets), during
    inference we achieve an end-to-end $1.49\times$ speedup at sparse operations.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Llama2-7B的训练与推理速度。由于敏感稀疏微调方法通过仅优化0.1%的参数（性能可与ZO全微调和10%的随机子集相媲美），在推理过程中，我们在稀疏操作中实现了$1.49\times$的速度提升。
- en: 'Efficient ZO LLM Fine-Tuning with Sparsity. Although ZO methods remove the
    need for backpropagation, a significant drawback of these methods is the slow
    convergence rate [[51](#bib.bib51), [23](#bib.bib23)]. A recent approach addresses
    this by fine-tuning with a sparse mask [[23](#bib.bib23), [50](#bib.bib50)], achieving
    approximately $\sim 75\%$ sparsity is still comparable to that of dense matrix
    operations. This latency increase can greatly impact user experience on applications
    such as personal assistants, where even a twofold increase in latency is perceptible.
    In addition, merging the sparse weights back into the base model is impractical
    on these devices due to memory constraints prohibiting dequantization and quantization.
    Empirical evidence suggests that higher sparsity levels can significantly decrease
    the time required for sparse matrix operations, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity").
    This raises the question:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的 ZO LLM 稀疏微调。虽然 ZO 方法去除了反向传播的需求，但这些方法的一个显著缺点是收敛速度较慢[[51](#bib.bib51), [23](#bib.bib23)]。一种最新的方法通过使用稀疏掩码进行微调[[23](#bib.bib23),
    [50](#bib.bib50)]，即使达到大约 $\sim 75\%$ 的稀疏性，其性能仍可与稠密矩阵操作相媲美。这种延迟增加可能会极大地影响个人助理等应用的用户体验，在这些应用中，即使延迟增加两倍也是可以察觉的。此外，由于内存限制禁止去量化和量化，将稀疏权重合并回基础模型在这些设备上是不切实际的。实证证据表明，更高的稀疏级别可以显著减少稀疏矩阵操作所需的时间，如图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 极端稀疏的 LLM 零阶微调") 所示。这提出了以下问题：
- en: '*Is it possible to leverage the benefits of higher sparsity levels in reducing
    inference latency while preserving performance on downstream tasks? If so, how
    far can sparsity be pushed in this context?*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*是否可以利用更高的稀疏级别在减少推理延迟的同时保持下游任务的性能？如果可以，这种稀疏性在这种情况下可以推进到什么程度？*'
- en: 'Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity.
    In this paper, we answer the raised research question by proposing an efficient
    sparse ZO LLM fine-tuning strategy. We observe an extreme sparsity pattern in
    LLM parameters: a subset, determined by selecting the top $k$ magnitude entries
    from the empirical Fisher information matrix, is effective for ZO fine-tuning.
    Moreover, we find this sparsity pattern can be obtained through LLM’s continuous
    pre-training process and be transferred to various downstream tasks without modification.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提案：基于 Fisher 信息的稀疏 ZO LLM 微调。在本文中，我们通过提出一种高效的稀疏 ZO LLM 微调策略来回答提出的研究问题。我们观察到
    LLM 参数中的极端稀疏模式：由从经验 Fisher 信息矩阵中选择的前 $k$ 个幅度条目决定的子集，对 ZO 微调有效。此外，我们发现这种稀疏模式可以通过
    LLM 的持续预训练过程获得，并在不修改的情况下转移到各种下游任务。
- en: 'Summary of Contributions. Building on these insights, our work proposes a comprehensive
    framework for ZO fine-tuning, making the following contributions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献总结。基于这些见解，我们的工作提出了一个全面的 ZO 微调框架，并做出以下贡献：
- en: $\bullet$
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We identify that only an extremely small portion (0.1%) of LLM parameters should
    be updated during ZO LLM fine-tuning. Moreover, we utilize this insight to guide
    the memory-efficient on-device personalization of LLMs by low-bit quantization
    of model parameters.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现，在 ZO LLM 微调过程中，只有极小部分（0.1%）的 LLM 参数应被更新。此外，我们利用这一见解，通过对模型参数进行低比特量化来指导设备上内存高效的
    LLM 个性化。
- en: $\bullet$
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We observe the sparsity pattern observed in LLM pre-training can be transferred
    across different downstream tasks while still maintaining good ZO performance.
    Based on this observation, we develop a computational framework to perform parameter-efficient
    ZO fine-tuning of LLMs.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们观察到，在 LLM 预训练过程中观察到的稀疏模式可以在不同的下游任务中转移，同时保持良好的 ZO 性能。基于这一观察，我们开发了一个计算框架来执行
    LLM 的参数高效 ZO 微调。
- en: $\bullet$
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We conduct extensive experiments across various LLMs and demonstrate that our
    method achieves competitive performance across various downstream tasks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在各种 LLM 上进行了广泛的实验，证明我们的方法在各种下游任务中表现出竞争力。
- en: 2 Background and Related works
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: In this section, we present the formulation for ZO optimization. We also discuss
    related works about sparsity in LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 ZO 优化的公式。我们还讨论了关于 LLM 中稀疏性的相关工作。
- en: 2.1 Zeroth-Order Optimization
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 零阶优化
- en: ZO surrogate gradient estimator. ZO optimizers have been studied widely in the
    machine learning community. Given a dataset $\mathcal{D}=\{(\mathbf{x}_{1},y_{1}),\dots,(\mathbf{x}_{n},y_{n})\}$
    via ZO surrogate gradient estimator. Simultaneous Perturbation Stochastic Approximation
    (SPSA) [[39](#bib.bib39)] is such an estimator that would first sample a random
    vector $\mathbf{z}\in\mathbb{R}^{d}$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ZO 替代梯度估计器。ZO 优化器在机器学习社区中得到了广泛研究。给定数据集 $\mathcal{D}=\{(\mathbf{x}_{1},y_{1}),\dots,(\mathbf{x}_{n},y_{n})\}$，使用
    ZO 替代梯度估计器。同步扰动随机逼近 (SPSA) [[39](#bib.bib39)] 是这样的一个估计器，它首先采样一个随机向量 $\mathbf{z}\in\mathbb{R}^{d}$。
- en: Definition 1  (Simultaneous Perturbation Stochastic Approximation (SPSA) [[39](#bib.bib39)]).
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1 （同时扰动随机逼近 (SPSA) [[39](#bib.bib39)]）。
- en: 'SPSA estimates the gradient w.r.t. $\mathbf{w}$ as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SPSA 根据如下公式估计关于 $\mathbf{w}$ 的梯度：
- en: '|  | $\hat{g}(\mathbf{w},(\mathbf{x},y),\mathbf{z})=\dfrac{f(\mathbf{w}+\epsilon\mathbf{z};(\mathbf{x},y))-f(\mathbf{w}-\epsilon\mathbf{z};(\mathbf{x},y))}{2\epsilon}\mathbf{z}$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{g}(\mathbf{w},(\mathbf{x},y),\mathbf{z})=\dfrac{f(\mathbf{w}+\epsilon\mathbf{z};(\mathbf{x},y))-f(\mathbf{w}-\epsilon\mathbf{z};(\mathbf{x},y))}{2\epsilon}\mathbf{z}$
    |  | (1) |'
- en: There are other ZO surrogate gradient estimators available [[21](#bib.bib21),
    [31](#bib.bib31)], but in practice SPSA achieves good performance in ZO optimization,
    particularly when fine-tuning LLMs. In addition, other ZO algorithms such as DeepZero
    [[3](#bib.bib3)] would utilize the parameter-wise finite difference of loss values
    to derive parameter-wise update directions. This would yield $O(d)$ query costs
    per training step even when combining with certain sparse masking methods and
    not practical for LLM fine-tuning scenarios. We therefore select SPSA with random
    Gaussian perturbation as our ZO gradient estimator.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他 ZO 替代梯度估计器 [[21](#bib.bib21), [31](#bib.bib31)]，但在实际应用中，SPSA 在 ZO 优化中表现良好，特别是在微调
    LLMs 时。此外，其他 ZO 算法如 DeepZero [[3](#bib.bib3)] 会利用损失值的参数级有限差分来推导参数级更新方向。这会在每个训练步骤中产生
    $O(d)$ 查询成本，即使与某些稀疏掩蔽方法结合也不适用于 LLM 微调场景。因此，我们选择带有随机高斯扰动的 SPSA 作为我们的 ZO 梯度估计器。
- en: 'ZO-SGD algorithm. ZO-SGD is an optimizer similar to SGD but replaces the FO
    gradient with ZO surrogate gradient estimate per training step, as defined below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD 算法。ZO-SGD 是一种类似于 SGD 的优化器，但将每个训练步骤中的 FO 梯度替换为 ZO 替代梯度估计，如下所定义：
- en: Definition 2  (ZO-SGD update rule).
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2 （ZO-SGD 更新规则）。
- en: 'ZO-SGD is an optimizer that uses ZO surrogate gradient to update parameters
    $\mathbf{w}_{t}$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD 是一种优化器，使用 ZO 替代梯度来更新参数 $\mathbf{w}_{t}$：
- en: '|  | $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t}\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),\mathbf{z}_{t})$
    |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t}\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),\mathbf{z}_{t})$
    |  | (2) |'
- en: MeZO [[27](#bib.bib27)] is a ZO-SGD algorithm that uses the "random seed trick"
    to save the need of caching ZO surrogate gradient. The choice of optimizer (SGD)
    is orthogonal to ZO optimization techniques, but in our preliminary experiments
    we find adaptive optimizers such as Adam [[16](#bib.bib16)] would not necessarily
    accelerate ZO convergence in LLM fine-tuning scenarios. There are other ZO optimizers
    aware of the parameter-wise heterogeneity of loss curvatures to accelerate the
    optimization convergence [[51](#bib.bib51)], and we leave how to combine our method
    with theirs as future works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MeZO [[27](#bib.bib27)] 是一种 ZO-SGD 算法，使用“随机种子技巧”来节省缓存 ZO 替代梯度的需求。优化器的选择 (SGD)
    与 ZO 优化技术是正交的，但在我们的初步实验中，我们发现自适应优化器如 Adam [[16](#bib.bib16)] 不一定能加速 ZO 在 LLM 微调中的收敛。有其他
    ZO 优化器考虑了损失曲率的参数级异质性以加速优化收敛 [[51](#bib.bib51)]，我们将如何将我们的方法与他们的方法结合留待未来工作。
- en: 2.2 Sparsity in LLMs
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs 中的稀疏性
- en: Sparsity-driven techniques are widely adopted in improving ML model’s efficiency
    [[42](#bib.bib42), [46](#bib.bib46), [24](#bib.bib24), [33](#bib.bib33), [8](#bib.bib8)]
    and robustness [[53](#bib.bib53), [52](#bib.bib52)]. Frankle and Carbin [[8](#bib.bib8)]
    showed that within large feed-forward networks, there exists a subnetwork that,
    when trained in isolation, can achieve test accuracy comparable to that of the
    original network. In the foundation models era, Liu et al. [[24](#bib.bib24)]
    demonstrated that transformer-based models, such as OPT [[49](#bib.bib49)], exhibit
    great sparsity ($\geq 95\%$ of that achieved by full fine-tuning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动稀疏性的技术被广泛采用来提高机器学习模型的效率[[42](#bib.bib42), [46](#bib.bib46), [24](#bib.bib24),
    [33](#bib.bib33), [8](#bib.bib8)]和鲁棒性[[53](#bib.bib53), [52](#bib.bib52)]。Frankle和Carbin[[8](#bib.bib8)]展示了在大型前馈网络中，存在一个子网络，当其在隔离训练时，可以达到与原始网络相当的测试准确率。在基础模型时代，Liu等人[[24](#bib.bib24)]展示了基于变压器的模型，如OPT[[49](#bib.bib49)]，表现出极大的稀疏性（$\geq
    95\%$的全精调所达到的稀疏性）。
- en: In the context of ZO optimization, Liu et al. [[23](#bib.bib23)] and Zhang et al.
    [[50](#bib.bib50)] also suggest that sparsity would potentially accelerate ZO
    optimization convergence. We believe that ZO has an intrinsic need for sparse
    training, as the procedure of ZO gradient estimator usually requires nearly uniform
    coordinate-wise scale (in expectation) perturbation which grows with $d$ serves
    as a Hessian-informed preconditioner) [[48](#bib.bib48), [51](#bib.bib51)]. However,
    they do not provide a comprehensive investigation on massive parameter models
    like LLMs. In particular, we also observe that during first-order (FO) fine-tuning
    of LLMs, the FO gradient can be quite sparse. We will elaborate more on this insight
    in the following section (see Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Extreme Sparsity
    Pattern in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") and Figure [7](#A3.F7 "Figure 7 ‣
    C.2 Gradient Sparsity During LLM Fine-Tuning ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")). We would
    like to explore how sparsity can benefit the ZO LLM fine-tuning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在ZO优化的背景下，Liu等人[[23](#bib.bib23)]和Zhang等人[[50](#bib.bib50)]也建议稀疏性可能会加速ZO优化的收敛。我们相信ZO本质上需要稀疏训练，因为ZO梯度估计器的过程通常需要几乎均匀的坐标尺度（期望中）扰动，这随着$d$的增长，作为Hessian信息预处理器[[48](#bib.bib48),
    [51](#bib.bib51)]。然而，他们并没有对像LLM这样的大规模参数模型进行全面的调查。特别是，我们还观察到在LLM的一级（FO）微调过程中，FO梯度可能非常稀疏。我们将在下面的部分中详细阐述这一见解（见图[2](#S3.F2
    "Figure 2 ‣ 3.1 Extreme Sparsity Pattern in LLM ‣ 3 Chasing Extreme Sparsity in
    ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")和图[7](#A3.F7
    "Figure 7 ‣ C.2 Gradient Sparsity During LLM Fine-Tuning ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")）。我们希望探索稀疏性如何有利于ZO
    LLM的微调。
- en: 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 在ZO LLM微调中追求极端稀疏性
- en: In this section, we describe the extreme sparsity pattern we observed in LLMs
    and how we utilize it for efficient ZO fine-tuning including on-device personalization
    of LLMs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们描述了我们在LLM中观察到的极端稀疏模式，以及我们如何利用它进行高效的ZO微调，包括LLM的设备个性化。
- en: 3.1 Extreme Sparsity Pattern in LLM
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLM中的极端稀疏模式
- en: '![Refer to caption](img/d11bdfad39bfde7625a64e0b066611d2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d11bdfad39bfde7625a64e0b066611d2.png)'
- en: 'Figure 2: Cumulative normalized sum of coordinate-wise gradient square $[\nabla\mathcal{F}(\mathbf{w})]_{i}^{2}$
    std of all blue curves. More similar figures are in Figure [7](#A3.F7 "Figure
    7 ‣ C.2 Gradient Sparsity During LLM Fine-Tuning ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). We observe
    that roughly 0.1% parameters in all linear layers contribute about 50% gradient
    norm square.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：所有蓝色曲线的坐标梯度平方$[\nabla\mathcal{F}(\mathbf{w})]_{i}^{2}$标准化累积和。更多类似的图见图[7](#A3.F7
    "Figure 7 ‣ C.2 Gradient Sparsity During LLM Fine-Tuning ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")。我们观察到，在所有线性层中，大约0.1%的参数贡献了约50%的梯度范数平方。
- en: ZO optimization with sensitive parameters. Given model parameters $\mathbf{w}$,
    sensitive parameters are defined as parameters whose corresponding FO coordinate-wise
    gradient square values are maximized.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ZO优化与敏感参数。给定模型参数$\mathbf{w}$，敏感参数被定义为其对应的FO坐标梯度平方值被最大化的参数。
- en: Definition 3  (Sensitive parameter mask).
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义3（敏感参数掩码）。
- en: A sensitive sparse mask $\mathbf{m}_{k}\in\{0,1\}^{d}$) is defined as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感稀疏掩码$\mathbf{m}_{k}\in\{0,1\}^{d}$的定义是
- en: '|  | $\mathbf{m}_{k}=\text{argmax}_{\mathbf{m}}\&#124;\mathbf{m}\odot\nabla
    f(\mathbf{w};(\mathbf{x},y))\&#124;_{2}^{2}.$ |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{m}_{k}=\text{argmax}_{\mathbf{m}}\&#124;\mathbf{m}\odot\nabla
    f(\mathbf{w};(\mathbf{x},y))\&#124;_{2}^{2}.$ |  | (3) |'
- en: 'In the context of ZO optimization, we will update sensitive parameters only.
    Denote that ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{z}}\odot\mathbf{m}_{k}$,
    and accordingly:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在ZO优化的背景下，我们将仅更新敏感参数。设${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{z}}\odot\mathbf{m}_{k}$，并因此：
- en: Definition 4  (Sensitive sparse ZO-SGD update rule).
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义4（敏感稀疏ZO-SGD更新规则）。
- en: '|  | $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t}\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}_{t}})$
    |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta_{t}\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}_{t}})$
    |  | (4) |'
- en: 'The theoretical support of sensitive parameters can be derived from the lens
    of SPSA gradient estimator and Fisher information matrix as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感参数的理论支持可以从SPSA梯度估计器和Fisher信息矩阵的角度推导如下：
- en: $\bullet$
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Maximum zeroth-order loss value changes, from the lens of SPSA estimator.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从SPSA估计器的角度来看，最大零阶损失值变化。
- en: 'The square (account for negativity) of loss value difference for $\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}_{t}})$
    is as follows:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\hat{g}_{\mathbf{w}}(\mathbf{w}_{t},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}_{t}})$的损失值差异的平方（考虑负值）如下：
- en: '|  | $\displaystyle\mathbb{E}_{{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}}}\{f(\mathbf{w}+\epsilon{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}};(\mathbf{x},y))-f(\mathbf{w}-\epsilon{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}};(\mathbf{x},y))\}^{2}$
    |  | (5) |'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}}}\{f(\mathbf{w}+\epsilon{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}};(\mathbf{x},y))-f(\mathbf{w}-\epsilon{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\bar{\mathbf{z}}};(\mathbf{x},y))\}^{2}$
    |  | (5) |'
- en: '|  |  | $\displaystyle=4\epsilon^{2}\&#124;{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{m}_{k}\odot}\,\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))\&#124;^{2}$
    |  | (6) |'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=4\epsilon^{2}\&#124;{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{m}_{k}\odot}\,\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))\&#124;^{2}$
    |  | (6) |'
- en: Since by Definition [3](#Thmdefinition3 "Definition 3 (Sensitive parameter mask).
    ‣ 3.1 Extreme Sparsity Pattern in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") our sensitive mask
    would maximize $\|\mathbf{m}_{k}\odot\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))\|^{2}$
    for a given sparsity ratio, we would expect our sensitive mask to maximize the
    magnitude of the loss value difference for any given sparsity ratio.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于根据定义[3](#Thmdefinition3 "定义 3（敏感参数掩码）。 ‣ 3.1 LLM中的极端稀疏模式 ‣ 3 ZO LLM微调中的极端稀疏追踪
    ‣ 带有极端稀疏的LLM的零阶微调")我们的敏感掩码将最大化$\|\mathbf{m}_{k}\odot\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))\|^{2}$，对于给定的稀疏比率，我们期望我们的敏感掩码最大化任何给定稀疏比率下的损失值差异的幅度。
- en: $\bullet$
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Maximum coverage of Hessian diagonal, from the lens of Fisher matrix.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从Fisher矩阵的角度来看，Hessian对角线的最大覆盖范围。
- en: 'LLMs are often pre-trained on large text corpus to reach low perplexity before
    entering the fine-tuning stage. In this case, we would assume $p_{\text{LLM}}(y|\mathbf{x})\sim
    p_{\mathcal{D}}(y|\mathbf{x})$ as follows:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs通常在大规模文本语料库上进行预训练，以达到低困惑度，然后进入微调阶段。在这种情况下，我们将假设$p_{\text{LLM}}(y|\mathbf{x})\sim
    p_{\mathcal{D}}(y|\mathbf{x})$如下：
- en: '|  | $\displaystyle\mathbf{F}$ |  | (7) |'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{F}$ |  | (7) |'
- en: '|  |  | $1$2 |  | (8) |'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (8) |'
- en: As we assume the empirical Fisher matrix approximates Fisher, which also approximates
    the Hessian, and empirical Fisher’s diagonal is equal to the coordinate-wise gradient
    square vector when computing with downstream task-specific loss, our sensitive
    parameters would cover a large fraction of the largest Hessian diagonal entries.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们假设经验Fisher矩阵近似于Fisher矩阵，而Fisher矩阵也近似于Hessian矩阵，且在计算下游任务特定损失时，经验Fisher的对角线等于坐标方向梯度平方向量，我们的敏感参数将覆盖Hessian对角线项中的较大部分。
- en: 'This idea of sensitive parameters has been studied in the quantization community
    [[15](#bib.bib15), [11](#bib.bib11)] and FO optimization [[40](#bib.bib40)]. However,
    we are the first one to leverage the extremely sparse sensitive parameters in
    LLM fine-tuning to accelerate ZO fine-tuning with LLMs. When we have perturbation
    and updating in the scale of billion parameters, finding which parameters to fine-tune
    would be important for improving ZO performance. Notice that here we use sensitive
    masks $\mathbf{m}_{k}$ for understanding purposes. In Section [3.4](#S3.SS4 "3.4
    Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity ‣
    3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of
    LLMs with Extreme Sparsity"), we will discuss how to transform Definition [4](#Thmdefinition4
    "Definition 4 (Sensitive sparse ZO-SGD update rule). ‣ 3.1 Extreme Sparsity Pattern
    in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") to a parameter-efficient optimization pipeline
    by optimizing fixed sensitive parameters.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '这种敏感参数的想法已经在量子化领域[[15](#bib.bib15), [11](#bib.bib11)]和FO优化[[40](#bib.bib40)]中进行了研究。然而，我们是第一个利用LLM微调中的极度稀疏敏感参数来加速LLM的ZO微调的方法。当我们在数十亿参数的范围内进行扰动和更新时，找出哪些参数需要微调对于提高ZO性能至关重要。请注意，这里我们使用敏感掩码$\mathbf{m}_{k}$是为了理解的目的。在第[3.4](#S3.SS4
    "3.4 Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")节中，我们将讨论如何通过优化固定的敏感参数将定义[4](#Thmdefinition4 "Definition
    4 (Sensitive sparse ZO-SGD update rule). ‣ 3.1 Extreme Sparsity Pattern in LLM
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")转化为一个参数高效的优化管道。'
- en: 3.2 Theoretical Convergence Rate
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 理论收敛速率
- en: We would investigate the theoretical convergence of sensitive sparse ZO-SGD
    on sensitive parameters under the non-convex optimization settings. Our assumptions
    are included in Appendix [B.2](#A2.SS2 "B.2 Proof for Equation 9, Theorem 1 ‣
    Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity").
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究在非凸优化设置下对敏感参数的敏感稀疏ZO-SGD的理论收敛性。我们的假设包含在附录[B.2](#A2.SS2 "B.2 Proof for Equation
    9, Theorem 1 ‣ Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")中。
- en: Theorem 1  (Convergence rate of sensitive sparse ZO-SGD (Definition [4](#Thmdefinition4
    "Definition 4 (Sensitive sparse ZO-SGD update rule). ‣ 3.1 Extreme Sparsity Pattern
    in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"))).
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（敏感稀疏ZO-SGD的收敛速率（定义[4](#Thmdefinition4 "Definition 4 (Sensitive sparse ZO-SGD
    update rule). ‣ 3.1 Extreme Sparsity Pattern in LLM ‣ 3 Chasing Extreme Sparsity
    in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")））。
- en: If we pick $\eta_{t}=1/(L(k+2))$, under Assumptions [1](#Thmassumption1 "Assumption
    1 (Bounded stochastic gradient errors). ‣ B.1 Assumptions ‣ Appendix B Theoretical
    Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") (bounded
    gradient error), [2](#Thmassumption2 "Assumption 2 (Lipschitz smoothness). ‣ B.1
    Assumptions ‣ Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") (Lipschitz smoothness), and [4](#Thmassumption4
    "Assumption 4 (Sensitive parameters are sparse). ‣ B.1 Assumptions ‣ Appendix
    B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") (sparse sensitive parameters), we would have
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择$\eta_{t}=1/(L(k+2))$，在假设[1](#Thmassumption1 "Assumption 1 (Bounded stochastic
    gradient errors). ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence Rate
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")（有界随机梯度误差）、[2](#Thmassumption2
    "Assumption 2 (Lipschitz smoothness). ‣ B.1 Assumptions ‣ Appendix B Theoretical
    Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")（Lipschitz平滑性）和[4](#Thmassumption4
    "Assumption 4 (Sensitive parameters are sparse). ‣ B.1 Assumptions ‣ Appendix
    B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")（敏感参数稀疏性）下，我们会得到
- en: '|  |  | $1$2 |  | (9) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (9) |'
- en: Moreover, if we still pick $\eta_{t}=1/(L(k+2))$, with an extra Assumption [3](#Thmassumption3
    "Assumption 3 (PL inequality). ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence
    Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") (P.L. condition),
    we would have
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们仍然选择$\eta_{t}=1/(L(k+2))$，并且有一个额外的假设[3](#Thmassumption3 "Assumption 3
    (PL inequality). ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence Rate ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")（P.L.条件），我们会得到
- en: '|  |  | $1$2 |  | (10) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (10) |'
- en: The proof for Inequality [9](#S3.E9 "Equation 9 ‣ Theorem 1 (Convergence rate
    of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") is in Appendix [B.2](#A2.SS2 "B.2 Proof for Equation
    9, Theorem 1 ‣ Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") and the proof for Inequality [10](#S3.E10 "Equation
    10 ‣ Theorem 1 (Convergence rate of sensitive sparse ZO-SGD (Definition 4)). ‣
    3.2 Theoretical Convergence Rate ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") is in Appendix [B.3](#A2.SS3
    "B.3 Proof for Equation 10, Theorem 1 ‣ Appendix B Theoretical Convergence Rate
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). If we choose $k=d$.
    As we assume $c\gg k/d$ are much lower than $O(d/T)+O(\text{constant})$ that zeroth-order
    method will yield.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不等式[9](#S3.E9 "Equation 9 ‣ Theorem 1 (Convergence rate of sensitive sparse
    ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")的证明在附录[B.2](#A2.SS2 "B.2 Proof for Equation 9, Theorem 1 ‣ Appendix
    B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")中，不等式[10](#S3.E10 "Equation 10 ‣ Theorem 1 (Convergence rate of sensitive
    sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")的证明在附录[B.3](#A2.SS3 "B.3 Proof for Equation 10, Theorem 1 ‣
    Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")中。如果我们选择 $k=d$。因为我们假设 $c\gg k/d$ 远低于 $O(d/T)+O(\text{constant})$
    的零阶方法所能得到的值。
- en: 'We want to emphasize that our contributions are more on empirical LLM fine-tuning
    instead of general machine learning tasks, and in Section [4.1](#S4.SS1 "4.1 RQ1:
    Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") we extensively compare
    our sparse ZO methods with other sparse ZO methods and we demonstrate its superiority
    during LLM fine-tuning. We do not use the strict “local $r$-effective rank” assumption
    that Malladi et al. [[27](#bib.bib27)] uses, and our Assumption [4](#Thmassumption4
    "Assumption 4 (Sensitive parameters are sparse). ‣ B.1 Assumptions ‣ Appendix
    B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") can be easily observed empirically in Figure [2](#S3.F2 "Figure 2 ‣
    3.1 Extreme Sparsity Pattern in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). Liu et al. [[23](#bib.bib23)]
    and Ohta et al. [[31](#bib.bib31)] also provide similar analysis on the convergence.
    However, they do not include our sensitive sparse mask in their studies.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '我们要强调的是，我们的贡献更多集中在经验性的LLM细化上，而不是一般的机器学习任务。在第[4.1](#S4.SS1 "4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")节中，我们广泛比较了我们的方法与其他稀疏ZO方法，并展示了其在LLM细化过程中的优越性。我们没有使用Malladi等人[[27](#bib.bib27)]所使用的严格的“局部
    $r$-有效秩”假设，我们的假设[4](#Thmassumption4 "Assumption 4 (Sensitive parameters are sparse).
    ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")可以在图[2](#S3.F2 "Figure 2 ‣ 3.1 Extreme Sparsity
    Pattern in LLM ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")中通过经验观察到。刘等人[[23](#bib.bib23)]和Ohta等人[[31](#bib.bib31)]也对收敛性进行了类似分析。然而，他们的研究中没有包含我们的敏感稀疏掩码。'
- en: 3.3 Transferability of LLM Pre-Training Sparsity Pattern in ZO Fine-Tuning
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM预训练稀疏模式在ZO细化中的可转移性
- en: Sparse fine-tuning with fixed sensitive parameters. Our Theorem [1](#Thmtheorem1
    "Theorem 1 (Convergence rate of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2
    Theoretical Convergence Rate ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") focuses on dynamic
    sparse fine-tuning. However, Panigrahi et al. [[32](#bib.bib32)] notice that in
    real LLM fine-tuning scenario, the fine-tuning performance could be attributed
    to a sparse subset of weights ($\sim 0.01\%$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 带有固定敏感参数的稀疏细化。我们的定理[1](#Thmtheorem1 "Theorem 1 (Convergence rate of sensitive
    sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity") 侧重于动态稀疏细化。然而，Panigrahi等人[[32](#bib.bib32)]注意到，在实际的LLM细化场景中，细化性能可能归因于一个稀疏的权重子集（$\sim
    0.01\%$）。
- en: 'The similarity of gradient features during fine-tuning would imply that we
    do not need to re-select our sensitive parameters during fine-tuning i.e. select
    once before fine-tuning should be sufficient. This hypothesis can be validated
    by Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Transferability of LLM Pre-Training Sparsity
    Pattern in ZO Fine-Tuning ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") and Figure [5](#S4.F5
    "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters
    ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). In
    Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Transferability of LLM Pre-Training Sparsity
    Pattern in ZO Fine-Tuning ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), the fact that “task
    grad, static” does not vanish and still has a large ratio over “task grad, dyn.”
    at the end of training demonstrate that we can select parameters before fine-tuning.
    We also include similar figures for Mistral-7B and OPT-6.7B in Figure [8](#A3.F8
    "Figure 8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets
    to Downstream Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") in Appendix [C.3](#A3.SS3 "C.3 Transferability
    of Gradient Features from Pre-Training Datasets to Downstream Tasks ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity"). We will describe Figure [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") in Section [4.3](#S4.SS3 "4.3 RQ3:
    On-Device Personalization ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '在微调过程中梯度特征的相似性意味着我们不需要在微调过程中重新选择我们的敏感参数，即在微调前选择一次应该就足够了。这个假设可以通过图 [3](#S3.F3
    "图 3 ‣ 3.3 零阶微调中 LLM 预训练稀疏模式的可迁移性 ‣ 3 追踪零阶 LLM 微调中的极端稀疏 ‣ 极端稀疏的零阶微调") 和图 [5](#S4.F5
    "图 5 ‣ 4.1 RQ1: 稀疏零阶微调对敏感参数的有效性 ‣ 4 实验 ‣ 极端稀疏的零阶微调") 进行验证。在图 [3](#S3.F3 "图 3 ‣
    3.3 零阶微调中 LLM 预训练稀疏模式的可迁移性 ‣ 3 追踪零阶 LLM 微调中的极端稀疏 ‣ 极端稀疏的零阶微调") 中，“task grad, static”
    并没有消失，并且在训练结束时仍然在“task grad, dyn.” 之上占据较大比例，这表明我们可以在微调前选择参数。我们还在图 [8](#A3.F8 "图
    8 ‣ C.3 预训练数据集梯度特征向下游任务的可迁移性 ‣ 附录 C 补充实验细节 ‣ 极端稀疏的零阶微调") 的附录 [C.3](#A3.SS3 "C.3
    预训练数据集梯度特征向下游任务的可迁移性 ‣ 附录 C 补充实验细节 ‣ 极端稀疏的零阶微调") 中包括了 Mistral-7B 和 OPT-6.7B 的类似图示。我们将在章节
    [4.3](#S4.SS3 "4.3 RQ3: 设备上的个性化 ‣ 4 实验 ‣ 极端稀疏的零阶微调") 中描述图 [5](#S4.F5 "图 5 ‣ 4.1
    RQ1: 稀疏零阶微调对敏感参数的有效性 ‣ 4 实验 ‣ 极端稀疏的零阶微调")。'
- en: '![Refer to caption](img/f3a0c9704d4a8d77c142271f8028cf84.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f3a0c9704d4a8d77c142271f8028cf84.png)'
- en: 'Figure 3: Cumulative normalized gradient square values of Llama2-7B model’s
    linear layers during fine-tuning. For each line, the colors represent the fraction
    of parameters and the line style represents the category. “task grad, dyn.” refers
    to the sensitive parameters selected at the given timestep (x-axis), and “task
    grad, static” refers to the sensitive parameters selected before fine-tuning.
    “C4 grad, static” refers to the sensitive parameters selected with gradients taken
    from causal language modeling on C4 datasets [[36](#bib.bib36)], and we keep it
    unchanged during fine-tuning. More similar figures are in Figure [8](#A3.F8 "Figure
    8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets to Downstream
    Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Llama2-7B 模型在线性层微调期间的累积归一化梯度平方值。每条线的颜色表示参数的比例，线型表示类别。“task grad, dyn.” 指的是在给定时间步（x轴）选择的敏感参数，而“task
    grad, static”指的是在微调前选择的敏感参数。“C4 grad, static”指的是在 C4 数据集上进行因果语言建模时获得的敏感参数[[36](#bib.bib36)]，在微调过程中保持不变。更多类似的图示见图
    [8](#A3.F8 "图 8 ‣ C.3 预训练数据集梯度特征向下游任务的可迁移性 ‣ 附录 C 补充实验细节 ‣ 极端稀疏的零阶微调")。
- en: Surrogate sensitive sparse mask from pre-training datasets. Another observation
    from Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Transferability of LLM Pre-Training Sparsity
    Pattern in ZO Fine-Tuning ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") is that the sensitive
    parameters derived from pre-training datasets (C4) would still cover a large fraction
    of model sensitivity. Therefore, we could use it as a surrogate sensitive sparse
    mask when gradients on downstream tasks are unavailable, particularly in scenario
    of on-device personalization. ¹¹1Obtaining gradients of LLMs on edge devices is
    expensive, and we usually cannot transfer data from edge devices to the cloud
    to compute the gradient on downstream tasks on cloud. In this case we would need
    some surrogate gradient information to derive sensitive sparse masks on cloud.
    We will discuss this in Section [3.5](#S3.SS5 "3.5 An Opportunity for On-Device
    LLM Personalization ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity").
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练数据集中得到的替代敏感稀疏掩码。另一个观察来自图[3](#S3.F3 "Figure 3 ‣ 3.3 Transferability of LLM
    Pre-Training Sparsity Pattern in ZO Fine-Tuning ‣ 3 Chasing Extreme Sparsity in
    ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")是，从预训练数据集（C4）推导出的敏感参数仍然会覆盖模型敏感性的很大一部分。因此，当下游任务的梯度不可用时，我们可以将其作为替代敏感稀疏掩码，特别是在设备上个性化的场景中。¹¹1在边缘设备上获取LLM的梯度是昂贵的，我们通常不能将数据从边缘设备传输到云端以计算下游任务的梯度。在这种情况下，我们需要一些替代梯度信息来推导云端的敏感稀疏掩码。我们将在第[3.5节](#S3.SS5
    "3.5 An Opportunity for On-Device LLM Personalization ‣ 3 Chasing Extreme Sparsity
    in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")中讨论这个问题。
- en: '3.4 Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 我们的提议：基于Fisher信息的、可转移的稀疏性ZO LLM微调
- en: 'The sparse optimization on fixed parameters can be implemented as a parameter-efficient
    optimization workflow, which will reduce the perturbation and updating time during
    ZO optimization. Suppose we have derived a sensitive sparse mask $\mathbf{m}_{k}$
    and extract the nonzero parts as below:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 固定参数上的稀疏优化可以实现为一种参数高效的优化工作流，这将减少在ZO优化过程中扰动和更新的时间。假设我们已经推导出一个敏感稀疏掩码$\mathbf{m}_{k}$并提取了非零部分，如下所示：
- en: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}=\mathbf{w}\odot\mathbf{m}_{k},\quad\mathbf{w}_{\text{dense}}=\mathbf{w}\odot(\mathbf{1}_{d}-\mathbf{m}_{k})$
    |  | (11) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}=\mathbf{w}\odot\mathbf{m}_{k},\quad\mathbf{w}_{\text{dense}}=\mathbf{w}\odot(\mathbf{1}_{d}-\mathbf{m}_{k})$
    |  | (11) |'
- en: 'Denote $\mathbf{z}_{k,t}\sim\mathcal{N}(\mathbf{0}_{k},\mathbf{I}_{k})$ only
    and leave $\mathbf{w}_{\text{dense}}$ frozen during fine-tuning. In this case,
    our sensitive sparse ZO-SGD update rule will become:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仅表示$\mathbf{z}_{k,t}\sim\mathcal{N}(\mathbf{0}_{k},\mathbf{I}_{k})$并在微调过程中保持$\mathbf{w}_{\text{dense}}$不变。在这种情况下，我们的敏感稀疏ZO-SGD更新规则将变为：
- en: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t+1}}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t}}-\eta_{t}\hat{g}({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t}},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{z}_{k,t}})$
    |  | (12) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t+1}}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t}}-\eta_{t}\hat{g}({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse},t}},(\mathbf{x}_{t},y_{t}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{z}_{k,t}})$
    |  | (12) |'
- en: In Section [3.5](#S3.SS5 "3.5 An Opportunity for On-Device LLM Personalization
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"), we describe how this decomposition would seamlessly
    combine with existing post-training quantization (PTQ) methods, which creates
    an opportunity for on-device personalization. In Appendix [C.6](#A3.SS6 "C.6 Implementation
    of Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we discuss efficient
    implementations of linear layers after our decomposition.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3.5节](#S3.SS5 "3.5 An Opportunity for On-Device LLM Personalization ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")中，我们描述了这种分解如何与现有的后训练量化（PTQ）方法无缝结合，这为设备上的个性化提供了机会。在附录[C.6](#A3.SS6
    "C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")中，我们讨论了在分解后线性层的高效实现。
- en: 3.5 An Opportunity for On-Device LLM Personalization
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 设备上LLM个性化的机会
- en: '![Refer to caption](img/6974f68286fa9ee28ecf6b06873e6ffb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6974f68286fa9ee28ecf6b06873e6ffb.png)'
- en: 'Figure 4: On-device LLM personalization workflow via integrating sensitive
    sparse ZO optimization with quantization.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过将敏感稀疏ZO优化与量化集成的设备上LLM个性化工作流程。
- en: As LLMs are often pre-trained with user-agnostic public datasets, personalizing
    LLMs with individual user’s preferences and meet user’s specific needs before
    real-world deployment are vital. [[41](#bib.bib41), [26](#bib.bib26)] However,
    transferring the user-specific data to upstream cloud before fine-tuning LLMs
    would raise privacy concerns. [[47](#bib.bib47)] On the other hand, personal devices
    usually have less computational budget and are more memory-constrained than the
    cloud [[54](#bib.bib54)], and performing full fine-tuning would easily exceed
    the device memory budget.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs通常使用与用户无关的公共数据集进行预训练，在实际应用之前，个性化LLMs以满足个体用户的偏好和具体需求至关重要。[[41](#bib.bib41),
    [26](#bib.bib26)] 然而，在微调LLMs之前将用户特定数据传输到上游云端可能会引发隐私问题。[[47](#bib.bib47)] 另一方面，个人设备的计算预算通常较少，内存约束也较大，与云相比，全面微调容易超出设备内存预算。
- en: 'If we want to fine-tune a 7B-level model (like Llama2-7B) on memory-constrained
    devices, we need to reduce the memory consumption on model weights, gradients,
    forward activations, and optimizer states:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在内存受限的设备上微调一个7B级模型（如Llama2-7B），我们需要减少对模型权重、梯度、前向激活和优化器状态的内存消耗：
- en: $\bullet$
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Model weights. We would quantize the $\mathbf{w}_{\text{dense}}$ to 4 bits,
    which reduces the model size of a Llama2-7B model from 13.5 to 3.4 GiB.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型权重。我们会将$\mathbf{w}_{\text{dense}}$量化为4位，这将Llama2-7B模型的大小从13.5 GiB减少到3.4 GiB。
- en: $\bullet$
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Forward activations. ZO optimization already saves the need of caching activations.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前向激活。ZO优化已经省去了缓存激活的需求。
- en: $\bullet$
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Gradients. We would use the “random seed trick” same as MeZO [[27](#bib.bib27)]
    to reproduce layer-wise gradients instead of caching them.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度。我们会使用与MeZO相同的“随机种子技巧”[[27](#bib.bib27)]来重现逐层梯度，而不是缓存它们。
- en: $\bullet$
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Optimizer states. We use SGD. Our method can also be implemented as a parameter-efficient
    optimization method which is also memory-efficient with other optimizers (even
    with Adam).
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化器状态。我们使用SGD。我们的方法也可以作为一种参数高效的优化方法来实现，这在其他优化器（即使是Adam）中也具有内存效率。
- en: 'As a result, our memory consumption is nearly minimum: we can fine-tune a Llama2-7B
    model under 8 GiB GPU memory without any offloading. This would satisfy the memory
    constraint by a wide range of edge or mobile devices as illustrated in Table [3](#A3.T3
    "Table 3 ‣ C.1 On-Device Memory Constraints ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们的内存消耗几乎达到最低：我们可以在8 GiB GPU内存下微调Llama2-7B模型而无需任何卸载。这将满足广泛边缘或移动设备的内存约束，如表[3](#A3.T3
    "Table 3 ‣ C.1 On-Device Memory Constraints ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")所示。
- en: 'Integration with quantization. In Section [3.4](#S3.SS4 "3.4 Our Proposal:
    ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity"), we know that we can obtain surrogate sensitive sparse masks before
    fine-tuning. We would first decompose sensitive $\mathbf{w}$. During this process,
    we will use surrogate gradient information that many PTQ algorithms already have:
    they need gradients to calibrate their quantization errors.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '与量化的集成。在第[3.4](#S3.SS4 "3.4 Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed,
    Transferable Sparsity ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")节中，我们知道在微调之前可以获得替代敏感稀疏掩码。我们会首先分解敏感的$\mathbf{w}$。在此过程中，我们将使用许多PTQ算法已经拥有的替代梯度信息：它们需要梯度来校准量化误差。'
- en: 'Our method also does not put strict constraints on specific choices of quantization
    algorithms since any algorithm [[2](#bib.bib2), [30](#bib.bib30), [9](#bib.bib9),
    [20](#bib.bib20), [15](#bib.bib15)] that aims to minimize the quantization error
    term or its variant would suffice:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法也没有对量化算法的具体选择施加严格的限制，因为任何旨在最小化量化误差项或其变体的算法[[2](#bib.bib2), [30](#bib.bib30),
    [9](#bib.bib9), [20](#bib.bib20), [15](#bib.bib15)]都足够。
- en: '|  | $Q(\mathbf{w})=\text{argmin}_{Q(\mathbf{w})}\mathbb{E}_{\mathbf{x}}\&#124;(\mathbf{w}-Q(\mathbf{w}))\mathbf{x}\&#124;_{2}^{2}$
    |  | (13) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(\mathbf{w})=\text{argmin}_{Q(\mathbf{w})}\mathbb{E}_{\mathbf{x}}\&#124;(\mathbf{w}-Q(\mathbf{w}))\mathbf{x}\&#124;_{2}^{2}$
    |  | (13) |'
- en: On-device personalization workflow. The workflow is illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ 3.5 An Opportunity for On-Device LLM Personalization ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity"). The high-level overview is that we use surrogate gradient information
    from pre-training datasets $\nabla_{\mathbf{w}}p_{\text{LLM}}(y|\mathbf{x})$ (Step
    1-4). We send $\mathbf{w}_{\text{sparse}}$ (Step 6).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 设备端个性化工作流程。工作流程如图 [4](#S3.F4 "图 4 ‣ 3.5 设备端 LLM 个性化的机会 ‣ 3 追求 ZO LLM 微调中的极端稀疏性
    ‣ 极端稀疏的 LLM 零阶微调") 所示。高层次概述是我们使用来自预训练数据集的替代梯度信息 $\nabla_{\mathbf{w}}p_{\text{LLM}}(y|\mathbf{x})$（步骤
    1-4）。我们发送 $\mathbf{w}_{\text{sparse}}$（步骤 6）。
- en: 4 Experiments
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we want to validate the effectiveness of our sensitive sparse
    ZO optimization method. We also investigate the effectiveness of our on-device
    personalization recipe in Figure [4](#S3.F4 "Figure 4 ‣ 3.5 An Opportunity for
    On-Device LLM Personalization ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). There are a few research
    questions we want to answer:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们希望验证我们在敏感稀疏 ZO 优化方法的有效性。我们还研究了图 [4](#S3.F4 "图 4 ‣ 3.5 设备端 LLM 个性化的机会
    ‣ 3 追求 ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏的 LLM 零阶微调") 中设备端个性化方案的有效性。我们有几个研究问题想要回答：
- en: $\bullet$
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'RQ1: Is optimizing sensitive parameters more effective than optimizing other
    subset of parameters during ZO fine-tuning? Can we optimize surrogate sensitive
    sparse parameters when downstream gradient information is unavailable?'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ1: 在 ZO 微调过程中，优化敏感参数是否比优化其他参数子集更有效？当下游梯度信息不可用时，我们能否优化替代的敏感稀疏参数？'
- en: $\bullet$
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'RQ2: Can optimizing extremely sparse and fixed parameters (Equation [12](#S3.E12
    "Equation 12 ‣ 3.4 Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable
    Sparsity ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")) lead to iteration-wise and total wall-clock time
    speedup?'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ2: 优化极端稀疏和固定参数（见方程 [12](#S3.E12 "方程 12 ‣ 3.4 我们的提议：带有 Fisher 知识的 ZO LLM 微调
    ‣ 3 追求 ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏的 LLM 零阶微调")）能否带来每次迭代和总墙钟时间的加速？'
- en: $\bullet$
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'RQ3: Can we match the full performance of ZO full fine-tuning by employing
    our on-device personalization recipe (Figure [4](#S3.F4 "Figure 4 ‣ 3.5 An Opportunity
    for On-Device LLM Personalization ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"))?'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ3: 我们能否通过采用我们的设备端个性化方案（见图 [4](#S3.F4 "图 4 ‣ 3.5 设备端 LLM 个性化的机会 ‣ 3 追求 ZO
    LLM 微调中的极端稀疏性 ‣ 极端稀疏的 LLM 零阶微调")）来匹配 ZO 完整微调的性能？'
- en: We focus on 7B LLM models (Llama2-7B [[43](#bib.bib43)], Mistral-7B [[13](#bib.bib13)],
    OPT-6.7B [[49](#bib.bib49)]) as they would fit with common on-device memory constraints
    (8 GiB) listed on Table [3](#A3.T3 "Table 3 ‣ C.1 On-Device Memory Constraints
    ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs
    with Extreme Sparsity") after applying quantization. We use SST-2 [[38](#bib.bib38)],
    RTE [[44](#bib.bib44)], CB [[6](#bib.bib6)], BoolQ [[4](#bib.bib4)], WSC [[17](#bib.bib17)],
    WiC [[34](#bib.bib34)], and COPA [[37](#bib.bib37)] datasets. We follow standard
    ZO fine-tuning settings and use the same codebases as in Malladi et al. [[27](#bib.bib27)].
    More details of our experiments (hyperparameters, task-specific prompts, etc.)
    are in Appendix [C](#A3 "Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity").
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注 7B LLM 模型（Llama2-7B [[43](#bib.bib43)]，Mistral-7B [[13](#bib.bib13)]，OPT-6.7B
    [[49](#bib.bib49)]），因为它们适合于表 [3](#A3.T3 "表 3 ‣ C.1 设备端内存限制 ‣ 附录 C 补充实验细节 ‣ 极端稀疏的
    LLM 零阶微调") 列出的常见设备端内存限制（8 GiB），在应用量化后。我们使用 SST-2 [[38](#bib.bib38)]，RTE [[44](#bib.bib44)]，CB
    [[6](#bib.bib6)]，BoolQ [[4](#bib.bib4)]，WSC [[17](#bib.bib17)]，WiC [[34](#bib.bib34)]
    和 COPA [[37](#bib.bib37)] 数据集。我们遵循标准的 ZO 微调设置，并使用与 Malladi 等人 [[27](#bib.bib27)]
    相同的代码库。我们实验的更多细节（超参数、任务特定提示等）见附录 [C](#A3 "附录 C 补充实验细节 ‣ 极端稀疏的 LLM 零阶微调")。
- en: '4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.1 RQ1: 稀疏 ZO 微调在敏感参数上的有效性'
- en: 'We first investigate the performance of optimizing our sensitive parameters
    versus other subsets of parameters. Our baseline sparsity methods are random subsets
    and weight outliers. As illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1:
    Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we can find that ZO
    fine-tuning would benefit from sparse optimization, as all methods would achieve
    higher than ZO full fine-tuning at 90% sparsity. However, only sensitive parameters
    would maintain its performance as we move to the extreme sparsity region  less parameters compared
    with random and weight outliers and still get same performance.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先调查优化敏感参数与其他参数子集的性能。我们的基准稀疏方法是随机子集和权重离群值。如图[5](#S4.F5 "Figure 5 ‣ 4.1 RQ1:
    Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")所示，我们发现 ZO 微调会从稀疏优化中受益，因为所有方法在
    90% 的稀疏度下都能达到高于 ZO 完整微调的效果。然而，只有敏感参数在我们进入极端稀疏区域时仍能保持其性能，与随机和权重离群值相比，参数更少，但性能保持一致。'
- en: 'We also validate whether optimizing fixed and surrogate sensitive parameters
    should still yield satisfactory performance. In Figure [5](#S4.F5 "Figure 5 ‣
    4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we compare the performance
    of optimizing sensitive parameters with C4 gradients with its theoretical upper
    bound: fixed sensitive parameters derived from task-specific gradients as the
    solid line and its dynamic version as the dash-dotted line. We also include the
    fixed and dynamic random subset parameters as a baseline. We can find that the
    gap of sensitive parameters between deriving from C4 gradients and task-specific
    gradients at sparsity level 99.9% is small and blue line is still far above the
    random and full fine-tuning baseline. We also present a summary of our approaches
    with 99.9% sparsity on various datasets and models in Table [1](#S4.T1 "Table
    1 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣
    4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还验证了优化固定和替代敏感参数是否仍然可以获得令人满意的性能。在图[5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")中，我们比较了使用 C4 梯度优化敏感参数的性能与其理论上限：从任务特定梯度得到的固定敏感参数作为实线，以及其动态版本作为虚线点线。我们还包括了固定和动态的随机子集参数作为基准。我们发现，敏感参数在稀疏度为
    99.9% 时，从 C4 梯度和任务特定梯度得出的性能差距很小，蓝线仍远高于随机和完整微调基准。我们还在表[1](#S4.T1 "Table 1 ‣ 4.1
    RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")中总结了我们在各种数据集和模型上进行 99.9%
    稀疏度的处理方法。'
- en: '![Refer to caption](img/a7a566966705fbd4ab41af91cc901b5e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a7a566966705fbd4ab41af91cc901b5e.png)'
- en: (a) Optimizing sensitive parameters with C4 gradients versus optimizing weights
    with largest magnitude (weight outliers) and random subsets of weights. The trainable
    parameters are all determined before fine-tuning and other parameters are kept
    unchanged.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用 C4 梯度优化敏感参数与使用最大幅度的权重（权重离群值）和权重的随机子集进行优化的比较。可训练参数在微调前已全部确定，其他参数保持不变。
- en: '![Refer to caption](img/6b27809cd99c9fc38b02ad96b7de2759.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b27809cd99c9fc38b02ad96b7de2759.png)'
- en: (b) Optimizing sensitive parameters with C4 gradients versus task-specific gradients.
    “Static” means the parameters to optimize are determined before fine-tuning and
    other parameters are kept unchanged during fine-tuning. “Dyn.” means the parameters
    to optimize will be updated every 100 training steps.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用 C4 梯度优化敏感参数与使用任务特定梯度进行优化的比较。“静态”表示优化的参数在微调前已确定，其他参数在微调过程中保持不变。“动态”表示每
    100 个训练步骤更新一次优化的参数。
- en: 'Figure 5: Performance of optimizing sensitive parameters in Llama2-7B fine-tuning
    on RTE, WiC, and COPA tasks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 RTE、WiC 和 COPA 任务中，Llama2-7B 微调中优化敏感参数的性能。
- en: '![Refer to caption](img/f4ad5017b8ef6aff11ee38def02bb876.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f4ad5017b8ef6aff11ee38def02bb876.png)'
- en: 'Figure 6: Iteration-wise & wall-clock convergence time of sensitive sparse
    fine-tuning on fixed parameters (“Sensitive”) versus ZO full fine-tuning (“Full”)
    for Llama2-7B. Here we use the 16-bit model as the base model for fine-tuning.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Llama2-7B 的敏感稀疏微调在固定参数（“敏感”）与 ZO 完整微调（“完整”）下的每次迭代和实际时钟收敛时间。这里我们使用 16 位模型作为微调的基础模型。
- en: 'Table 1: Performance of difference methods on Llama2-7B fine-tuning tasks.
    In the first column, “Q” means the full model is quantized with 4-bit quantization
    method (SqueezeLLM [[15](#bib.bib15)]), and “ZO” means the model is fine-tuned
    with ZO-SGD optimizer. For each cell, we use the same hyperparameters and repeat
    it with 3 random seeds. We report the average and standard deviation of test set
    accuracy in the format of $\text{mean}_{\text{std}}$. In last 2 columns, “Acc”
    means the average test set accuracy and “Rank” means the average rank among all
    methods across tasks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：Llama2-7B 微调任务中不同方法的性能。在第一列，“Q”表示完整模型使用4位量化方法（SqueezeLLM [[15](#bib.bib15)]），而“ZO”表示模型使用ZO-SGD优化器进行微调。每个单元格我们使用相同的超参数，并用3个随机种子进行重复。我们报告测试集准确率的平均值和标准差，格式为$\text{mean}_{\text{std}}$。最后两列，“Acc”表示测试集准确率的平均值，“Rank”表示所有方法在各任务中的平均排名。
- en: (a) Llama2-7B
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7B
- en: '|  | Methods | SST-2 | RTE | CB | BoolQ | WSC | WiC | COPA | Acc | Rank |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | SST-2 | RTE | CB | BoolQ | WSC | WiC | COPA | 准确率 | 排名 |'
- en: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.7}_{0.4}$ | $57.4_{3.9}$ | 75.2
    | 2.43 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.7}_{0.4}$ | $57.4_{3.9}$ | 75.2
    | 2.43 |'
- en: '|  | LoRA | $93.8_{0.6}$ | $\textbf{61.5}_{2.1}$ | 72.9 | 4.29 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA | $93.8_{0.6}$ | $\textbf{61.5}_{2.1}$ | 72.9 | 4.29 |'
- en: '|  | Prefix | $80.5_{4.3}$ | $54.5_{11.4}$ | 69.2 | 5.86 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | Prefix | $80.5_{4.3}$ | $54.5_{11.4}$ | 69.2 | 5.86 |'
- en: '| ZO | Sensitive (task, static) | $\textbf{94.8}_{0.1}$ | $57.4_{4.7}$ | 75.2
    | 2.29 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ZO | Sensitive (task, static) | $\textbf{94.8}_{0.1}$ | $57.4_{4.7}$ | 75.2
    | 2.29 |'
- en: '|  | Random (static) | $94.1_{0.3}$ | $\textbf{59.6}_{3.6}$ | 73.1 | 4.14 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | Random (static) | $94.1_{0.3}$ | $\textbf{59.6}_{3.6}$ | 73.1 | 4.14 |'
- en: '|  | Full fine-tuning | $94.6_{0.5}$ | $58.0_{4.3}$ | 74.2 | 3.57 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | Full fine-tuning | $94.6_{0.5}$ | $58.0_{4.3}$ | 74.2 | 3.57 |'
- en: '|  | Zero-shot | $89.0_{0.0}$ | $50.2_{0.0}$ | 59.2 | 7.29 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | Zero-shot | $89.0_{0.0}$ | $50.2_{0.0}$ | 59.2 | 7.29 |'
- en: '|  | ICL | $94.8_{0.2}$ | $53.2_{1.1}$ | 74.0 | 3.43 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL | $94.8_{0.2}$ | $53.2_{1.1}$ | 74.0 | 3.43 |'
- en: (b) Mistral-7B
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral-7B
- en: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.0}_{0.3}$ | $59.6_{4.9}$ | 74.7
    | 2.86 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.0}_{0.3}$ | $59.6_{4.9}$ | 74.7
    | 2.86 |'
- en: '|  | LoRA | $\textbf{94.0}_{0.4}$ | $\textbf{60.9}_{3.7}$ | 72.1 | 3.57 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA | $\textbf{94.0}_{0.4}$ | $\textbf{60.9}_{3.7}$ | 72.1 | 3.57 |'
- en: '|  | Prefix | $86.9_{2.1}$ | $60.3_{4.6}$ | 65.8 | 4.86 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | Prefix | $86.9_{2.1}$ | $60.3_{4.6}$ | 65.8 | 4.86 |'
- en: '| ZO | Sensitive (task, static) | $\textbf{94.7}_{0.3}$ | $\textbf{58.0}_{4.3}$
    | 75.4 | 1.86 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ZO | Sensitive (task, static) | $\textbf{94.7}_{0.3}$ | $\textbf{58.0}_{4.3}$
    | 75.4 | 1.86 |'
- en: '|  | Random (static) | $87.9_{1.9}$ | $57.6_{1.4}$ | 66.0 | 5.29 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | Random (static) | $87.9_{1.9}$ | $57.6_{1.4}$ | 66.0 | 5.29 |'
- en: '|  | Full fine-tuning | $94.6_{0.1}$ | $54.8_{6.2}$ | 74.3 | 2.86 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | Full fine-tuning | $94.6_{0.1}$ | $54.8_{6.2}$ | 74.3 | 2.86 |'
- en: '|  | Zero-shot | $54.8_{0.0}$ | $50.8_{0.0}$ | 50.6 | 7.00 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | Zero-shot | $54.8_{0.0}$ | $50.8_{0.0}$ | 50.6 | 7.00 |'
- en: '|  | ICL | $60.7_{16.7}$ | $50.4_{0.6}$ | 57.0 | 5.43 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL | $60.7_{16.7}$ | $50.4_{0.6}$ | 57.0 | 5.43 |'
- en: (c) OPT-6.7B
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (c) OPT-6.7B
- en: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.9}_{0.5}$ | $\textbf{59.3}_{5.3}$
    | 75.5 | 1.29 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | Sensitive (C4, static) | $\textbf{94.9}_{0.5}$ | $\textbf{59.3}_{5.3}$
    | 75.5 | 1.29 |'
- en: '|  | LoRA | $94.2_{0.2}$ | $57.1_{9.1}$ | 71.4 | 4.57 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA | $94.2_{0.2}$ | $57.1_{9.1}$ | 71.4 | 4.57 |'
- en: '|  | Prefix | $93.3_{0.4}$ | $62.5_{2.4}$ | 72.5 | 4.14 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | Prefix | $93.3_{0.4}$ | $62.5_{2.4}$ | 72.5 | 4.14 |'
- en: '| ZO | Sensitive (task, static) | $\textbf{94.5}_{0.4}$ | $\textbf{57.4}_{5.2}$
    | 75.1 | 2.14 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ZO | Sensitive (task, static) | $\textbf{94.5}_{0.4}$ | $\textbf{57.4}_{5.2}$
    | 75.1 | 2.14 |'
- en: '|  | Random (static) | $87.3_{2.0}$ | $58.0_{7.0}$ | 69.4 | 5.71 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | Random (static) | $87.3_{2.0}$ | $58.0_{7.0}$ | 69.4 | 5.71 |'
- en: '|  | Full fine-tuning | $94.4_{0.3}$ | $\textbf{57.4}_{4.6}$ | 74.1 | 3.29
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | Full fine-tuning | $94.4_{0.3}$ | $\textbf{57.4}_{4.6}$ | 74.1 | 3.29
    |'
- en: '|  | Zero-shot | $61.0_{0.0}$ | $55.5_{0.0}$ | 56.1 | 7.71 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | Zero-shot | $61.0_{0.0}$ | $55.5_{0.0}$ | 56.1 | 7.71 |'
- en: '|  | ICL | $74.0_{14.6}$ | $53.2_{1.7}$ | 62.5 | 6.57 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL | $74.0_{14.6}$ | $53.2_{1.7}$ | 62.5 | 6.57 |'
- en: '4.2 RQ2: Wall-Clock Time Efficiency'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.2 RQ2: 实时时间效率'
- en: 'By employing parameter-efficient ZO fine-tuning with extreme sparsity, we also
    achieve 1.2 - 2.5$\times$ wall-clock time convergence speedup compared with ZO
    full fine-tuning as we nearly eliminate the ZO perturbation and optimizer update
    time, as Figure [6](#S4.F6 "Figure 6 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity") shows. This also boosts the GPU utilization rate as large-batched
    ZO forward is often compute-bounded while the perturbation and optimization steps
    are often memory-bounded. Furthermore, the reduced memory footprint of parameter-efficient
    ZO fine-tuning allows for training larger models on the same hardware, potentially
    leading to even better performance. As a result, we answer this question that
    optimizing extremely sparse and fixed parameters leads to substantial iteration-wise
    and total wall-clock time improvements.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '通过采用参数高效的 ZO 微调并实现极端稀疏，我们还实现了 1.2 - 2.5$\times$ 的墙钟时间收敛加速，与 ZO 全量微调相比，几乎消除了
    ZO 扰动和优化器更新时间，如图[6](#S4.F6 "Figure 6 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")所示。这也提高了 GPU 的利用率，因为大批量的 ZO 前向传播通常是计算瓶颈，而扰动和优化步骤通常是内存瓶颈。此外，参数高效的
    ZO 微调减少了内存占用，使得在相同硬件上训练更大的模型成为可能，从而可能带来更好的性能。因此，我们回答了这个问题：优化极端稀疏和固定参数带来了显著的迭代时间和总墙钟时间改进。'
- en: '4.3 RQ3: On-Device Personalization'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.3 RQ3: 设备端个性化'
- en: 'We validate whether our sensitive sparse ZO optimization method would fit with
    on-device personalization pipeline described in Section [3.5](#S3.SS5 "3.5 An
    Opportunity for On-Device LLM Personalization ‣ 3 Chasing Extreme Sparsity in
    ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")
    with Table [1](#S4.T1 "Table 1 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity"). We follow the exact recipe as described Figure [4](#S3.F4
    "Figure 4 ‣ 3.5 An Opportunity for On-Device LLM Personalization ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") to report a number as “Sensitive (C4, static)”, where we only optimize
    0.1% sensitive parameters on top of a 4-bit quantized model. As ZO fine-tuning
    happens after model is quantized, ablating on extracting 0.1% random subsets of
    parameters would produce a different quantized model. So we choose to report the
    result for optimizing a fixed random subset on top of the 16-bit model as the
    “Random (static)”.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '我们验证了我们敏感的稀疏 ZO 优化方法是否适合于第[3.5](#S3.SS5 "3.5 An Opportunity for On-Device LLM
    Personalization ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")节中描述的设备端个性化流水线，与[1](#S4.T1 "Table 1
    ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4
    Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")表中的内容一致。我们按照图[4](#S3.F4
    "Figure 4 ‣ 3.5 An Opportunity for On-Device LLM Personalization ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")中描述的精确方法报告了一个数字为“Sensitive (C4, static)”，其中我们只优化了在 4 位量化模型上的 0.1% 敏感参数。由于
    ZO 微调发生在模型量化之后，提取 0.1% 随机子集的参数会产生不同的量化模型。因此，我们选择报告在 16 位模型上优化固定随机子集的结果，作为“Random
    (static)”。'
- en: We also compare with optimizing with LoRA [[12](#bib.bib12)] and Prefix Tuning
    [[19](#bib.bib19)] with ZO-SGD optimizer on top of the same quantized model. We
    follow the LoRA $r$ and prefix length shown in Malladi et al. [[27](#bib.bib27)],
    and for LoRA, we add it to all linear layers same as where our sensitive parameters
    are extracted. We find that integrating sensitive sparse ZO optimization with
    on-device personalization pipelines would still yield good performance exceeding
    all baselines across models and tasks. Particularly, the performance is higher
    than ICL, and ZO full fine-tuning in 16 bits. In addition, we have surpassed other
    ZO-PEFT methods and random sparse ZO fine-tuning methods. This demonstrates the
    superiority of optimizing sensitive parameters only in ZO fine-tuning recipes.
    We also notice that optimizing sensitive parameters derived from C4 gradients
    still produce close results as from task-specific gradients (in average less than
    1% accuracy difference). This indicates optimizing surrogate sensitive parameters
    is still empirically successful.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还与 LoRA [[12](#bib.bib12)] 和 Prefix Tuning [[19](#bib.bib19)] 进行比较，使用 ZO-SGD
    优化器对相同的量化模型进行优化。我们遵循 Malladi 等 [[27](#bib.bib27)] 中的 LoRA $r$ 和前缀长度，对于 LoRA，我们将其添加到所有线性层中，正如我们的敏感参数提取的地方一样。我们发现，将敏感稀疏
    ZO 优化与设备上的个性化管道集成，仍能提供超过所有基准的良好性能。特别是，性能高于 ICL 和 16 位的 ZO 全微调。此外，我们还超越了其他 ZO-PEFT
    方法和随机稀疏 ZO 微调方法。这展示了在 ZO 微调方案中仅优化敏感参数的优越性。我们还注意到，优化从 C4 梯度派生的敏感参数仍然产生与任务特定梯度相近的结果（平均差异不到
    1%）。这表明，优化替代的敏感参数在经验上仍然成功。
- en: 5 Conclusion
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We have shown that the sensitive parameters provided by the pre-training process
    can effectively assist in ZO LLMs fine-tuning. Our experiments suggest that the
    ZO fine-tuning guided by 0.1% sensitive parameters in the LLM can even perform
    better than the full parameter ZO fine-tuning. The experiment results also demonstrate
    that the quantization of parameters other than sensitive parameters allows us
    to perform ZO fine-tuning of an LLM on limited memory devices.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了预训练过程提供的敏感参数可以有效地辅助 ZO LLMs 的微调。我们的实验表明，使用 LLM 中 0.1% 的敏感参数进行 ZO 微调，甚至可以比使用全参数的
    ZO 微调效果更佳。实验结果还表明，除了敏感参数之外的参数量化允许我们在有限内存设备上执行 LLM 的 ZO 微调。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等 [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher M De
    Sa。Quip: 2 位量化的大型语言模型及其保证。*神经信息处理系统进展*，36，2024。'
- en: 'Chen et al. [2024] Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer,
    Jiancheng Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura,
    and Sijia Liu. Deepzero: Scaling up zeroth-order optimization for deep model training.
    In *International Conference on Learning Representations*, 2024. doi: 10.48550/arXiv.2310.02025.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2024] Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer,
    Jiancheng Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura
    和 Sijia Liu。Deepzero: 扩展零阶优化用于深度模型训练。发表于 *国际学习表示会议*，2024。doi: 10.48550/arXiv.2310.02025。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers)*, pages 2924–2936, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等 [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins 和 Kristina Toutanova。Boolq: 探索自然是/否问题的意外难度。发表于 *2019 年北美计算语言学协会会议论文集:
    人类语言技术，卷 1（长文与短文）*，第 2924–2936 页，2019。'
- en: 'Dao [2023] Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao [2023] Tri Dao。Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分区。*arXiv 预印本 arXiv:2307.08691*，2023。'
- en: 'De Marneffe et al. [2019] Marie-Catherine De Marneffe, Mandy Simons, and Judith
    Tonhauser. The commitmentbank: Investigating projection in naturally occurring
    discourse. In *Proceedings of Sinn und Bedeutung*, pages 107–124, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'De Marneffe 等人 [2019] Marie-Catherine De Marneffe、Mandy Simons 和 Judith Tonhauser。The
    commitmentbank: Investigating projection in naturally occurring discourse。载于 *Proceedings
    of Sinn und Bedeutung*，第 107–124 页，2019 年。'
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural
    Information Processing Systems*, volume 36, pages 10088–10115\. Curran Associates,
    Inc., 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2023] Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。Qlora:
    Efficient finetuning of quantized llms。载于 A. Oh、T. Naumann、A. Globerson、K. Saenko、M.
    Hardt 和 S. Levine 主编的 *Advances in Neural Information Processing Systems*，第 36
    卷，第 10088–10115 页。Curran Associates, Inc.，2023 年。'
- en: 'Frankle and Carbin [2019] Jonathan Frankle and Michael Carbin. The lottery
    ticket hypothesis: Finding sparse, trainable neural networks. In *International
    Conference on Learning Representations*, 2019. doi: 10.48550/arXiv.1803.03635.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frankle 和 Carbin [2019] Jonathan Frankle 和 Michael Carbin。The lottery ticket
    hypothesis: Finding sparse, trainable neural networks。载于 *International Conference
    on Learning Representations*，2019 年。doi: 10.48550/arXiv.1803.03635。'
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 [2022] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq:
    Accurate post-training quantization for generative pre-trained transformers。*arXiv
    preprint arXiv:2210.17323*，2022 年。'
- en: Gim and Ko [2022] In Gim and JeongGil Ko. Memory-efficient dnn training on mobile
    devices. In *Proceedings of the 20th Annual International Conference on Mobile
    Systems, Applications and Services*, pages 464–476, 2022.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gim 和 Ko [2022] In Gim 和 JeongGil Ko。Memory-efficient dnn training on mobile
    devices。载于 *Proceedings of the 20th Annual International Conference on Mobile
    Systems, Applications and Services*，第 464–476 页，2022 年。
- en: 'Guo et al. [2023] Han Guo, Philip Greengard, Eric Xing, and Yoon Kim. Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    In *The Twelfth International Conference on Learning Representations*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人 [2023] Han Guo、Philip Greengard、Eric Xing 和 Yoon Kim。Lq-lora: Low-rank
    plus quantized matrix decomposition for efficient language model finetuning。载于
    *The Twelfth International Conference on Learning Representations*，2023 年。'
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 [2021] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。Lora: Low-rank adaptation of large language
    models。*arXiv preprint arXiv:2106.09685*，2021 年。'
- en: Jiang et al. [2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2023] Albert Q Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier 等人。Mistral 7b。*arXiv preprint arXiv:2310.06825*，2023 年。
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等人 [2020] Jared Kaplan、Sam McCandlish、Tom Henighan、Tom B Brown、Benjamin
    Chess、Rewon Child、Scott Gray、Alec Radford、Jeffrey Wu 和 Dario Amodei。Scaling laws
    for neural language models。*arXiv preprint arXiv:2001.08361*，2020 年。
- en: 'Kim et al. [2023] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 [2023] Sehoon Kim、Coleman Hooper、Amir Gholami、Zhen Dong、Xiuyu Li、Sheng
    Shen、Michael W Mahoney 和 Kurt Keutzer。Squeezellm: Dense-and-sparse quantization。*arXiv
    preprint arXiv:2306.07629*，2023 年。'
- en: 'Kingma and Ba [2015] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In *International Conference on Learning Representations*, 2015.
    doi: 10.48550/arXiv.1412.6980.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Ba [2015] Diederik P Kingma 和 Jimmy Ba。Adam: A method for stochastic
    optimization。载于 *International Conference on Learning Representations*，2015 年。doi:
    10.48550/arXiv.1412.6980。'
- en: Levesque et al. [2012] Hector Levesque, Ernest Davis, and Leora Morgenstern.
    The winograd schema challenge. In *Thirteenth international conference on the
    principles of knowledge representation and reasoning*, 2012.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levesque 等人 [2012] Hector Levesque、Ernest Davis 和 Leora Morgenstern。The winograd
    schema challenge。载于 *Thirteenth international conference on the principles of
    knowledge representation and reasoning*，2012 年。
- en: 'Li et al. [2024] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and
    Qin Xie. Transformer-lite: High-efficiency deployment of large language models
    on mobile phone gpus. *arXiv preprint arXiv:2403.20041*, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2024] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang 和 Qin Xie。Transformer-lite：在手机
    GPU 上高效部署大型语言模型。*arXiv 预印本 arXiv:2403.20041*，2024。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang [2021] Xiang Lisa Li 和 Percy Liang。Prefix-tuning：优化生成的连续提示。*arXiv
    预印本 arXiv:2101.00190*，2021。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang 和 Song
    Han。AWQ：用于 LLM 压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023。
- en: 'Liu et al. [2020] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang,
    Alfred O Hero III, and Pramod K Varshney. A primer on zeroth-order optimization
    in signal processing and machine learning: Principals, recent advances, and applications.
    *IEEE Signal Processing Magazine*, 37(5):43–54, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2020] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred
    O Hero III 和 Pramod K Varshney。信号处理和机器学习中的零阶优化入门：原理、近期进展及应用。*IEEE 信号处理杂志*，37(5):43–54，2020。
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。Roberta：一种稳健优化的
    BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*，2019。
- en: 'Liu et al. [2024a] Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui
    Hsieh, and Yang You. Sparse mezo: Less parameters for better performance in zeroth-order
    llm fine-tuning. *arXiv preprint arXiv:2402.15751*, 2024a.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2024a] Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh
    和 Yang You。Sparse mezo：用于零阶 LLM 微调的更少参数以获得更好性能。*arXiv 预印本 arXiv:2402.15751*，2024a。
- en: 'Liu et al. [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al.
    Deja vu: Contextual sparsity for efficient llms at inference time. In *International
    Conference on Machine Learning*, pages 22137–22176\. PMLR, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re 等。Deja vu：在推理时实现高效
    LLM 的上下文稀疏性。发表于 *国际机器学习大会*，第 22137–22176 页。PMLR，2023。
- en: Liu et al. [2024b] Zirui Liu, Guanchu Wang, Shaochen Henry Zhong, Zhaozhuo Xu,
    Daochen Zha, Ruixiang Ryan Tang, Zhimeng Stephen Jiang, Kaixiong Zhou, Vipin Chaudhary,
    Shuai Xu, et al. Winner-take-all column row sampling for memory efficient adaptation
    of language model. *Advances in Neural Information Processing Systems*, 36, 2024b.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2024b] Zirui Liu, Guanchu Wang, Shaochen Henry Zhong, Zhaozhuo Xu, Daochen
    Zha, Ruixiang Ryan Tang, Zhimeng Stephen Jiang, Kaixiong Zhou, Vipin Chaudhary,
    Shuai Xu 等。Winner-take-all 列行采样用于语言模型的内存高效适配。*神经信息处理系统进展*，36，2024b。
- en: Mairittha et al. [2020] Nattaya Mairittha, Tittaya Mairittha, and Sozo Inoue.
    Improving activity data collection with on-device personalization using fine-tuning.
    In *Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive
    and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium
    on Wearable Computers*, pages 255–260, 2020.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairittha 等 [2020] Nattaya Mairittha, Tittaya Mairittha 和 Sozo Inoue。通过微调进行设备个性化以改进活动数据收集。发表于
    *2020 年 ACM 国际联合计算与 2020 年 ACM 国际可穿戴计算研讨会附录会议论文*，第 255–260 页，2020。
- en: Malladi et al. [2023a] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just
    forward passes. *Advances in Neural Information Processing Systems*, 36:53038–53075,
    2023a.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi 等 [2023a] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D Lee, Danqi Chen 和 Sanjeev Arora。仅通过前向传播微调语言模型。*神经信息处理系统进展*，36:53038–53075，2023a。
- en: Malladi et al. [2023b] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen,
    and Sanjeev Arora. A kernel-based view of language model fine-tuning. In *International
    Conference on Machine Learning*, pages 23610–23641\. PMLR, 2023b.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi 等 [2023b] Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen 和
    Sanjeev Arora。语言模型微调的核方法视角。发表于 *国际机器学习大会*，第 23610–23641 页。PMLR，2023b。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *International Conference on Learning
    Representations*, 2016.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] 斯蒂芬·梅里提、蔡名雄、詹姆斯·布拉德伯里和理查德·索彻。《指针哨兵混合模型》。发表于 *国际学习表征会议*，2016年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2020] 马库斯·纳戈尔、拉娜·阿里·阿姆贾德、马特·范·巴伦、克里斯托斯·卢伊佐斯和蒂杰门·布兰克沃特。《上升还是下降？用于后训练量化的自适应舍入》。发表于
    *国际机器学习会议*，第7197–7206页。PMLR，2020年。
- en: 'Ohta et al. [2020] Mayumi Ohta, Nathaniel Berger, Artem Sokolov, and Stefan
    Riezler. Sparse perturbations for improved convergence in stochastic zeroth-order
    optimization. In *Machine Learning, Optimization, and Data Science: 6th International
    Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers,
    Part II 6*, pages 39–64\. Springer, 2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohta et al. [2020] 大田真由美、内森尼尔·伯杰、阿尔忒弥斯·索科洛夫和斯特凡·里茨勒。《用于改进随机零阶优化收敛性的稀疏扰动》。发表于
    *机器学习、优化与数据科学：第六届国际会议，LOD 2020，意大利锡耶纳，2020年7月19–23日，修订版选定论文，第II部分6*，第39–64页。施普林格，2020年。
- en: Panigrahi et al. [2023] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and
    Sanjeev Arora. Task-specific skill localization in fine-tuned language models.
    In *International Conference on Machine Learning*, pages 27011–27033\. PMLR, 2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panigrahi et al. [2023] 阿比舍克·帕尼格拉希、尼昆杰·桑希、浩宇·赵和桑吉夫·阿罗拉。《在微调语言模型中的任务特定技能定位》。发表于
    *国际机器学习会议*，第27011–27033页。PMLR，2023年。
- en: Peng et al. [2013] Zhimin Peng, Ming Yan, and Wotao Yin. Parallel and distributed
    sparse optimization. In *2013 Asilomar conference on signals, systems and computers*,
    pages 659–646\. IEEE, 2013.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. [2013] 彭志敏、闫鸣和尹渥涛。《并行和分布式稀疏优化》。发表于 *2013年阿西洛玛信号、系统和计算机会议*，第659–646页。IEEE，2013年。
- en: 'Pilehvar and Camacho-Collados [2019] Mohammad Taher Pilehvar and Jose Camacho-Collados.
    Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 1267–1273, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pilehvar and Camacho-Collados [2019] 穆罕默德·塔赫尔·皮莱瓦尔和何塞·卡马乔-科拉多斯。《WIC：用于评估上下文敏感意义表示的词语上下文数据集》。发表于
    *2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，第1267–1273页，2019年。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2019] 亚历克·拉德福德、杰弗里·吴、雷温·查伊尔、戴维·卢安、达里奥·阿莫代伊、伊利亚·苏茨克维尔等。《语言模型是无监督的多任务学习者》。*OpenAI博客*，1(8):9，2019年。
- en: Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *arXiv
    e-prints*, 2019.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2019] 科林·拉费尔、诺亚姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、阎奇·周、魏莉和彼得·J·刘。《使用统一文本到文本转换器探索迁移学习的极限》。*arXiv
    电子印刷本*，2019年。
- en: 'Roemmele et al. [2011] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S
    Gordon. Choice of plausible alternatives: An evaluation of commonsense causal
    reasoning. In *2011 AAAI Spring Symposium Series*, 2011.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele et al. [2011] 梅丽莎·罗梅尔、科斯敏·阿德里安·贝詹和安德鲁·S·戈登。《可行备选方案的选择：对常识因果推理的评估》。发表于
    *2011年AAAI春季研讨会系列*，2011年。
- en: Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 conference on empirical methods in natural language processing*, pages 1631–1642,
    2013.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. [2013] 理查德·索彻、亚历克斯·佩雷尔金、简·吴、杰森·张、克里斯托弗·D·曼宁、安德鲁·Y·吴和克里斯托弗·波茨。《递归深度模型用于情感树库的语义组合性》。发表于
    *2013年自然语言处理实证方法会议论文集*，第1631–1642页，2013年。
- en: Spall [1992] James C Spall. Multivariate stochastic approximation using a simultaneous
    perturbation gradient approximation. *IEEE transactions on automatic control*,
    37(3):332–341, 1992.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spall [1992] 詹姆斯·C·斯帕尔。《使用同步扰动梯度逼近的多变量随机逼近》。*IEEE自动控制汇刊*，37(3):332–341，1992年。
- en: Sung et al. [2021] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural
    networks with fixed sparse masks. *Advances in Neural Information Processing Systems*,
    34:24193–24205, 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung et al. [2021] 宋怡霖、瓦伦·奈尔和科林·A·拉费尔。《使用固定稀疏掩码训练神经网络》。*神经信息处理系统进展*，34:24193–24205，2021年。
- en: Tan et al. [2024a] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing
    Yin, and Meng Jiang. Democratizing large language models via personalized parameter-efficient
    fine-tuning. *arXiv preprint arXiv:2402.04401*, 2024a.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人 [2024a] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin
    和 Meng Jiang。通过个性化参数高效微调实现大型语言模型的民主化。*arXiv 预印本 arXiv:2402.04401*，2024a 年。
- en: Tan et al. [2024b] Zhen Tan, Tianlong Chen, Zhenyu Zhang, and Huan Liu. Sparsity-guided
    holistic explanation for llms with interpretable inference-time intervention.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 21619–21627,
    2024b.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等人 [2024b] Zhen Tan, Tianlong Chen, Zhenyu Zhang 和 Huan Liu。基于稀疏性的全局解释方法：具有可解释推理时干预的
    LLM。见于 *Proceedings of the AAAI Conference on Artificial Intelligence*，第 21619–21627
    页，2024b 年。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。Llama 2: 开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023 年。'
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop
    BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353–355,
    2018.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy 和 Samuel Bowman。Glue: 自然语言理解的多任务基准和分析平台。见于 *Proceedings of the 2018 EMNLP
    Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*，第 353–355
    页，2018 年。'
- en: Xi et al. [2023] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training
    transformers with 4-bit integers. *Advances in Neural Information Processing Systems*,
    36:49146–49168, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi 等人 [2023] Haocheng Xi, Changhao Li, Jianfei Chen 和 Jun Zhu。使用 4 位整数训练变换器。*Advances
    in Neural Information Processing Systems*，36:49146–49168，2023 年。
- en: 'Xia et al. [2023] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu
    Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling
    cost-effective and highly-efficient large generative model inference with unstructured
    sparsity. In *Proceedings of the VLDB Endowment, Vol. 17, No. 2*, 2023. doi: 10.14778/3626292.3626303.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等人 [2023] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou,
    Xiafei Qiu, Yong Li, Wei Lin 和 Shuaiwen Leon Song。Flash-llm: 通过无结构稀疏性实现成本效益高且高效的大型生成模型推理。见于
    *Proceedings of the VLDB Endowment, Vol. 17, No. 2*，2023 年。doi: 10.14778/3626292.3626303。'
- en: 'Xu et al. [2018] Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, and Xuanzhe
    Liu. Deeptype: On-device deep learning for input personalization service with
    minimal privacy concern. *Proceedings of the ACM on Interactive, Mobile, Wearable
    and Ubiquitous Technologies*, 2(4):1–26, 2018.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2018] Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang 和 Xuanzhe Liu。Deeptype:
    一种具有最小隐私关注的设备端深度学习输入个性化服务。*Proceedings of the ACM on Interactive, Mobile, Wearable
    and Ubiquitous Technologies*，2(4):1–26，2018 年。'
- en: Ye et al. [2018] Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li, and
    Tong Zhang. Hessian-aware zeroth-order optimization for black-box adversarial
    attack. *arXiv preprint arXiv:1812.11377*, 2018.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人 [2018] Haishan Ye, Zhichao Huang, Cong Fang, Chris Junchi Li 和 Tong Zhang。对黑箱对抗攻击的海森矩阵感知零阶优化。*arXiv
    预印本 arXiv:1812.11377*，2018 年。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等。Opt:
    开放的预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022 年。'
- en: 'Zhang et al. [2024] Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng
    Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D Lee, Wotao Yin, Mingyi Hong, et al.
    Revisiting zeroth-order optimization for memory-efficient llm fine-tuning: A benchmark.
    *arXiv preprint arXiv:2402.11592*, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang,
    Wenqing Zheng, Pin-Yu Chen, Jason D Lee, Wotao Yin, Mingyi Hong 等。重新审视零阶优化以进行内存高效的
    LLM 微调：一个基准。*arXiv 预印本 arXiv:2402.11592*，2024 年。
- en: 'Zhao et al. [2024] Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian,
    and Ivor W Tsang. Second-order fine-tuning without pain for llms: A hessian informed
    zeroth-order optimizer. *arXiv preprint arXiv:2402.15173*, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2024] Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian 和 Ivor
    W Tsang。无痛的二阶微调 LLM：一种海森矩阵信息的零阶优化器。*arXiv 预印本 arXiv:2402.15173*，2024 年。
- en: Zhong et al. [2021] Shaochen Zhong, Guanqun Zhang, Ningjia Huang, and Shuai
    Xu. Revisit kernel pruning with lottery regulated grouped convolutions. In *International
    Conference on Learning Representations*, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等[2021] Shaochen Zhong, Guanqun Zhang, Ningjia Huang, 和 Shuai Xu. 重新审视带有彩票调节的分组卷积的内核修剪。发表于
    *国际学习表示大会*，2021。
- en: 'Zhong et al. [2024] Shaochen Henry Zhong, Zaichuan You, Jiamu Zhang, Sebastian
    Zhao, Zachary LeClaire, Zirui Liu, Daochen Zha, Vipin Chaudhary, Shuai Xu, and
    Xia Hu. One less reason for filter pruning: Gaining free adversarial robustness
    with structured grouped kernel pruning. *Advances in Neural Information Processing
    Systems*, 36, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等[2024] Shaochen Henry Zhong, Zaichuan You, Jiamu Zhang, Sebastian Zhao, Zachary
    LeClaire, Zirui Liu, Daochen Zha, Vipin Chaudhary, Shuai Xu, 和 Xia Hu. 筛选修剪的一个减少理由：通过结构化分组卷积内核修剪获得免费对抗鲁棒性。*神经信息处理系统进展*，第
    36 卷，2024。
- en: 'Zhu et al. [2023] Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen
    Wang, Chuang Gan, and Song Han. Pockengine: Sparse and efficient fine-tuning in
    a pocket. In *Proceedings of the 56th Annual IEEE/ACM International Symposium
    on Microarchitecture*, pages 1381–1394, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等[2023] Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang
    Gan, 和 Song Han. Pockengine: 一种便携的稀疏且高效的微调方法。发表于 *第56届 IEEE/ACM 国际微架构年会论文集*，第
    1381–1394 页，2023。'
- en: Appendix
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: In Section [A](#A1 "Appendix A Notations ‣ Zeroth-Order Fine-Tuning of LLMs
    with Extreme Sparsity") we describe all notations used in this paper. In Section [B](#A2
    "Appendix B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity"), we include the assumption and exact proof on the convergence
    rate (Theorem [1](#Thmtheorem1 "Theorem 1 (Convergence rate of sensitive sparse
    ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing Extreme
    Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")). In Section [C](#A3 "Appendix C Supplementary Experiment Details ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we describe all details
    in our experiments and provide a high-level recommendation on how to efficiently
    implement our sensitive sparse ZO fine-tuning in forward passes of linear layers
    with existing quantization methods or training / inference workflow.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在[A](#A1 "附录 A 术语 ‣ 极端稀疏条件下的 LLMs 的零阶微调")节中，我们描述了本文中使用的所有术语。在[B](#A2 "附录 B 理论收敛速率
    ‣ 极端稀疏条件下的 LLMs 的零阶微调")节中，我们包含了关于收敛速率的假设和精确证明（定理[1](#Thmtheorem1 "定理 1（敏感稀疏 ZO-SGD
    的收敛速率（定义 4））。 ‣ 3.2 理论收敛速率 ‣ 3 极端稀疏追踪中的 ZO LLM 微调 ‣ 极端稀疏条件下的 LLMs 的零阶微调")）。在[C](#A3
    "附录 C 补充实验细节 ‣ 极端稀疏条件下的 LLMs 的零阶微调")节中，我们描述了实验中的所有细节，并提供了如何在现有量化方法或训练/推理工作流中高效实现我们的敏感稀疏
    ZO 微调的高级建议。
- en: Appendix A Notations
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 术语
- en: We present the notations used in this work as follows.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下介绍本工作中使用的术语。
- en: 'Table 2: Notations'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 术语'
- en: '| Term/Symbol |  | Explanation |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 术语/符号 |  | 解释 |'
- en: '| $f$ |  | loss function |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| $f$ |  | 损失函数 |'
- en: '| $t$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| $t$ |'
- en: '| $(\mathbf{x}_{t},y_{t})$ as a pair of input vector and training target |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| $(\mathbf{x}_{t},y_{t})$ 作为输入向量和训练目标的一对 |'
- en: '| $\mathbf{w}_{t}\in\mathbb{R}^{d}$ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{w}_{t}\in\mathbb{R}^{d}$ |'
- en: '| $f(\mathbf{w};(\mathbf{x},y))$ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| $f(\mathbf{w};(\mathbf{x},y))$ |'
- en: '| $\mathcal{F}(\mathbf{w})$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}(\mathbf{w})$ |'
- en: '| $\epsilon$ |  | a small perturbation scaling constant (close to 0) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| $\epsilon$ |  | 小扰动缩放常数（接近 0） |'
- en: '| $\mathbf{z}_{t}\in\mathbb{R}^{d}$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{z}_{t}\in\mathbb{R}^{d}$ |'
- en: '| $\hat{g}(\mathbf{w},(\mathbf{x},y),\mathbf{z})$ (Definition [1](#Thmdefinition1
    "Definition 1 (Simultaneous Perturbation Stochastic Approximation (SPSA) [39]).
    ‣ 2.1 Zeroth-Order Optimization ‣ 2 Background and Related works ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{g}(\mathbf{w},(\mathbf{x},y),\mathbf{z})$ （定义[1](#Thmdefinition1 "定义
    1（同时扰动随机逼近（SPSA）[39]）。 ‣ 2.1 零阶优化 ‣ 2 背景和相关工作 ‣ 极端稀疏条件下的 LLMs 的零阶微调")） |'
- en: '| $\eta_{t}$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| $\eta_{t}$ |'
- en: '| $\mathbf{m}_{k}\in\{0,1\}^{d}$ nonzero entries (Definition [3](#Thmdefinition3
    "Definition 3 (Sensitive parameter mask). ‣ 3.1 Extreme Sparsity Pattern in LLM
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{m}_{k}\in\{0,1\}^{d}$ 非零条目（定义[3](#Thmdefinition3 "定义 3（敏感参数掩码）。
    ‣ 3.1 LLM 中的极端稀疏模式 ‣ 3 极端稀疏追踪中的 ZO LLM 微调 ‣ 极端稀疏条件下的 LLMs 的零阶微调")） |'
- en: '| $\mathbf{m}_{k,t}\in\{0,1\}^{d}$. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{m}_{k,t}\in\{0,1\}^{d}$。 |'
- en: '| $\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}}=\mathbf{m}_{k}\mathbf{m}_{k}^{\top}$
    with main diagonal masked by $\mathbf{m}_{k}$. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| $\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}}=\mathbf{m}_{k}\mathbf{m}_{k}^{\top}$
    其主对角线被 $\mathbf{m}_{k}$ 遮蔽。 |'
- en: '| $\bar{\mathbf{z}}_{t}=\mathbf{z}_{t}\odot\mathbf{m}_{k}$. Notice that $\bar{\mathbf{z}}$
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| $\bar{\mathbf{z}}_{t}=\mathbf{z}_{t}\odot\mathbf{m}_{k}$。注意 $\bar{\mathbf{z}}$
    |'
- en: '| $\mathbf{1}_{d}$ with all entries equal to 1 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{1}_{d}$ 全部条目等于 1 |'
- en: '| $\operatorname{Tr}$ |  | trace operation |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{Tr}$ |  | 跟踪操作 |'
- en: '| $Q(\mathbf{w})$ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| $Q(\mathbf{w})$ |'
- en: '| $\mathbf{F}$ |  | (true) Fisher information matrix |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{F}$ |  | （真实的）费舍尔信息矩阵 |'
- en: '| $\hat{\mathbf{F}}$ |  | empirical Fisher information matrix |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{\mathbf{F}}$ |  | 实证费舍尔信息矩阵 |'
- en: '| $p_{\text{LLM}}$ |  | LLM as a probabilistic model |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| $p_{\text{LLM}}$ |  | 作为概率模型的 LLM |'
- en: '| $p_{\mathcal{D}}$ |  | true data distribution |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| $p_{\mathcal{D}}$ |  | 真实数据分布 |'
- en: '| $\mathbf{w}_{\text{sparse}}=\mathbf{w}\odot\mathbf{m}_{k}$ (Equation [11](#S3.E11
    "Equation 11 ‣ 3.4 Our Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable
    Sparsity ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{w}_{\text{sparse}}=\mathbf{w}\odot\mathbf{m}_{k}$ (公式 [11](#S3.E11
    "公式 11 ‣ 3.4 我们的提议：基于费舍尔信息的零阶大规模语言模型微调 ‣ 3 追求极端稀疏性 ‣ 零阶微调大规模语言模型的极端稀疏性")) |'
- en: '| $L$ |  | Lipschitz constant in Assumption [2](#Thmassumption2 "Assumption
    2 (Lipschitz smoothness). ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence
    Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| $L$ |  | 假设 [2](#Thmassumption2 "假设 2（利普希茨平滑性）。 ‣ B.1 假设 ‣ 附录 B 理论收敛率 ‣ 零阶微调大规模语言模型的极端稀疏性")
    中的利普希茨常数 |'
- en: '| $\mu$ |  | PL condition number in Assumption [3](#Thmassumption3 "Assumption
    3 (PL inequality). ‣ B.1 Assumptions ‣ Appendix B Theoretical Convergence Rate
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| $\mu$ |  | 假设 [3](#Thmassumption3 "假设 3（PL 不等式）。 ‣ B.1 假设 ‣ 附录 B 理论收敛率 ‣
    零阶微调大规模语言模型的极端稀疏性") 中的 PL 条件数 |'
- en: '| $\sigma^{2}$ |  | stochastic gradient error term in Assumption [1](#Thmassumption1
    "Assumption 1 (Bounded stochastic gradient errors). ‣ B.1 Assumptions ‣ Appendix
    B Theoretical Convergence Rate ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| $\sigma^{2}$ |  | 假设 [1](#Thmassumption1 "假设 1（有界随机梯度误差）。 ‣ B.1 假设 ‣ 附录 B
    理论收敛率 ‣ 零阶微调大规模语言模型的极端稀疏性") 中的随机梯度误差项 |'
- en: '| $W_{K}$ in attention layers |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| $W_{K}$ 在注意力层中 |'
- en: '| $W_{V}$ in attention layers |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| $W_{V}$ 在注意力层中 |'
- en: Appendix B Theoretical Convergence Rate
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 理论收敛率
- en: B.1 Assumptions
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 假设
- en: 'We start with listing standard assumptions in nonconvex optimization literature:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从列出非凸优化文献中的标准假设开始：
- en: Assumption 1  (Bounded stochastic gradient errors).
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 1（有界随机梯度误差）。
- en: For any data example $(\mathbf{x},y)\in\mathcal{D}$, we have
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何数据示例 $(\mathbf{x},y)\in\mathcal{D}$，我们有
- en: '|  | $\&#124;\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))-\nabla_{\mathbf{w}}\mathcal{F}(\mathbf{w})\&#124;^{2}\leq\sigma^{2}.$
    |  | (14) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))-\nabla_{\mathbf{w}}\mathcal{F}(\mathbf{w})\&#124;^{2}\leq\sigma^{2}.$
    |  | (14) |'
- en: Assumption 2  (Lipschitz smoothness).
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 2（利普希茨平滑性）。
- en: We assume that $f(\mathbf{w},\mathbf{x})$,
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设 $f(\mathbf{w},\mathbf{x})$，
- en: '|  | $\&#124;\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))-\nabla_{\mathbf{w}}f(\mathbf{w}^{\prime};(\mathbf{x},y))\&#124;\leq
    L\&#124;\mathbf{w}-\mathbf{w}^{\prime}\&#124;.$ |  | (15) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\nabla_{\mathbf{w}}f(\mathbf{w};(\mathbf{x},y))-\nabla_{\mathbf{w}}f(\mathbf{w}^{\prime};(\mathbf{x},y))\&#124;\leq
    L\&#124;\mathbf{w}-\mathbf{w}^{\prime}\&#124;.$ |  | (15) |'
- en: Assumption 3  (PL inequality).
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 3（PL 不等式）。
- en: We assume that $\mathcal{F}(\mathbf{w})$
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设 $\mathcal{F}(\mathbf{w})$
- en: '|  | $1$2 |  | (16) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (16) |'
- en: Inspired by Figure [7](#A3.F7 "Figure 7 ‣ C.2 Gradient Sparsity During LLM Fine-Tuning
    ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs
    with Extreme Sparsity"), we would assume the sensitive parameters of $\mathbf{w}$
    are sparse.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 灵感来源于图 [7](#A3.F7 "图 7 ‣ C.2 LLM 微调过程中的梯度稀疏性 ‣ 附录 C 补充实验细节 ‣ 零阶微调大规模语言模型的极端稀疏性")，我们假设
    $\mathbf{w}$ 的敏感参数是稀疏的。
- en: Assumption 4  (Sensitive parameters are sparse).
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 4（敏感参数是稀疏的）。
- en: We assume at timestep $t$ such that
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设在时间步 $t$ 使得
- en: '|  | $1$2 |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Here we assume $c\gg k/d$.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们假设 $c\gg k/d$。
- en: B.2 Proof for Equation [9](#S3.E9 "Equation 9 ‣ Theorem 1 (Convergence rate
    of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"), Theorem [1](#Thmtheorem1 "Theorem 1 (Convergence
    rate of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence
    Rate ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 证明方程 [9](#S3.E9 "方程 9 ‣ 定理 1 (敏感稀疏 ZO-SGD 的收敛速率（定义 4）). ‣ 3.2 理论收敛速率 ‣ 3 追寻
    ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏性的 LLM 的零阶微调")，定理 [1](#Thmtheorem1 "定理 1 (敏感稀疏 ZO-SGD 的收敛速率（定义
    4）). ‣ 3.2 理论收敛速率 ‣ 3 追寻 ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏性的 LLM 的零阶微调")
- en: Lemma 1  (Sparse ZO surrogate gradient covariance).
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1  （稀疏 ZO 替代梯度协方差）。
- en: '|  |  | $\displaystyle~{}\mathbb{E}_{\bar{\mathbf{z}}}\hat{g}(\mathbf{w},(\mathbf{x},y),\bar{\mathbf{z}})\hat{g}(\mathbf{w},(\mathbf{x},y),y),\bar{\mathbf{z}})^{\top}$
    |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\mathbb{E}_{\bar{\mathbf{z}}}\hat{g}(\mathbf{w},(\mathbf{x},y),\bar{\mathbf{z}})\hat{g}(\mathbf{w},(\mathbf{x},y),y),\bar{\mathbf{z}})^{\top}$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: Proof for Equation [9](#S3.E9 "Equation 9 ‣ Theorem 1 (Convergence rate of sensitive
    sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity"), Theorem [1](#Thmtheorem1 "Theorem 1 (Convergence rate of sensitive
    sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity").
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明方程 [9](#S3.E9 "方程 9 ‣ 定理 1 (敏感稀疏 ZO-SGD 的收敛速率（定义 4）). ‣ 3.2 理论收敛速率 ‣ 3 追寻
    ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏性的 LLM 的零阶微调")，定理 [1](#Thmtheorem1 "定理 1 (敏感稀疏 ZO-SGD 的收敛速率（定义
    4）). ‣ 3.2 理论收敛速率 ‣ 3 追寻 ZO LLM 微调中的极端稀疏性 ‣ 极端稀疏性的 LLM 的零阶微调")。
- en: 'Denote $\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}}\in\mathbb{R}^{d\times d}$ nonzero
    entries:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}}\in\mathbb{R}^{d\times d}$ 非零元素：
- en: '|  | $\displaystyle\text{diag}(\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}})$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{diag}(\tilde{\mathbf{I}}_{d,\mathbf{m}_{k}})$ |  |'
- en: '|  | $\displaystyle f(\mathbf{w}_{t+1},\mathbf{x}_{t})$ |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathbf{w}_{t+1},\mathbf{x}_{t})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}f(\mathbf{w}_{t+1},\mathbf{x}_{t})$
    |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}f(\mathbf{w}_{t+1},\mathbf{x}_{t})$
    |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}f(\mathbf{w}_{t+1},\mathbf{x}_{t})$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}f(\mathbf{w}_{t+1},\mathbf{x}_{t})$
    |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
- en: Denote $\alpha=Lc(k+2)$, we will have
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\alpha=Lc(k+2)$，我们将得到
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
- en: Set $\eta_{t}<\dfrac{c}{\alpha}=\dfrac{1}{L(k+2)}$, we have
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\eta_{t}<\dfrac{c}{\alpha}=\dfrac{1}{L(k+2)}$，我们有
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
- en: If we apply our sparse ZO update rule recursively for $T$ steps,
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们递归应用我们的稀疏 ZO 更新规则 $T$ 步，
- en: '|  | $\displaystyle\dfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\&#124;\nabla_{\mathbf{w}}\mathcal{F}(\mathbf{w}_{t})\&#124;^{2}$
    |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\dfrac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\&#124;\nabla_{\mathbf{w}}\mathcal{F}(\mathbf{w}_{t})\&#124;^{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq\dfrac{2\alpha}{Tc^{2}}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+(2\sigma^{2}+\sigma^{2})$
    |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\dfrac{2\alpha}{Tc^{2}}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+(2\sigma^{2}+\sigma^{2})$
    |  |'
- en: '|  |  | $\displaystyle\leq\dfrac{2L(k+2)}{c}\dfrac{1}{T}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+3\sigma^{2}$
    |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\dfrac{2L(k+2)}{c}\dfrac{1}{T}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+3\sigma^{2}$
    |  |'
- en: '|  |  | $\displaystyle\leq O\left(\dfrac{k}{c}\cdot\dfrac{L}{T}\right)(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+3\sigma^{2}$
    |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq O\left(\dfrac{k}{c}\cdot\dfrac{L}{T}\right)(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+3\sigma^{2}$
    |  |'
- en: ∎
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: B.3 Proof for Equation [10](#S3.E10 "Equation 10 ‣ Theorem 1 (Convergence rate
    of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"), Theorem [1](#Thmtheorem1 "Theorem 1 (Convergence
    rate of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence
    Rate ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity")
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 方程[10](#S3.E10 "方程10 ‣ 定理1（敏感稀疏ZO-SGD的收敛速率（定义4）） ‣ 3.2 理论收敛速率 ‣ 3 极端稀疏下的ZO
    LLM微调 ‣ 极端稀疏条件下的LLM零阶微调")的证明，定理[1](#Thmtheorem1 "定理1（敏感稀疏ZO-SGD的收敛速率（定义4）） ‣ 3.2
    理论收敛速率 ‣ 3 极端稀疏下的ZO LLM微调 ‣ 极端稀疏条件下的LLM零阶微调")
- en: Lemma 2  (Sparse ZO surrogate gradient norm).
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理2（稀疏ZO替代梯度范数）。
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}[\&#124;\hat{g}(\mathbf{w}_{t},\mathbf{x}_{t},\bar{\mathbf{z}}_{t})\&#124;^{2}]$
    |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}}}[\&#124;\hat{g}(\mathbf{w}_{t},\mathbf{x}_{t},\bar{\mathbf{z}}_{t})\&#124;^{2}]$
    |  |'
- en: Proof for Equation [10](#S3.E10 "Equation 10 ‣ Theorem 1 (Convergence rate of
    sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence Rate ‣
    3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of
    LLMs with Extreme Sparsity"), Theorem [1](#Thmtheorem1 "Theorem 1 (Convergence
    rate of sensitive sparse ZO-SGD (Definition 4)). ‣ 3.2 Theoretical Convergence
    Rate ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity").
  id: totrans-294
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程[10](#S3.E10 "方程10 ‣ 定理1（敏感稀疏ZO-SGD的收敛速率（定义4）） ‣ 3.2 理论收敛速率 ‣ 3 极端稀疏下的ZO LLM微调
    ‣ 极端稀疏条件下的LLM零阶微调")的证明，定理[1](#Thmtheorem1 "定理1（敏感稀疏ZO-SGD的收敛速率（定义4）） ‣ 3.2 理论收敛速率
    ‣ 3 极端稀疏下的ZO LLM微调 ‣ 极端稀疏条件下的LLM零阶微调")。
- en: Denote $\kappa$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为$\kappa$。
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\mathcal{F}(\mathbf{w}_{t+1})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{t+1})-\mathcal{F}^{*}\}$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{t+1})-\mathcal{F}^{*}\}$
    |  |'
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{t+1})-\mathcal{F}^{*}\}$
    |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{t+1})-\mathcal{F}^{*}\}$
    |  |'
- en: Plugging in $\eta_{t}\leq\dfrac{c}{\alpha}$ iterations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 插入$\eta_{t}\leq\dfrac{c}{\alpha}$迭代次数。
- en: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{T})-\mathcal{F}^{*}\}$
    |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\bar{\mathbf{z}},(\mathbf{x},y)}\{\mathcal{F}(\mathbf{w}_{T})-\mathcal{F}^{*}\}$
    |  |'
- en: '|  |  | $\displaystyle\leq(1-\dfrac{c\kappa}{(k+2)})^{T}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+\dfrac{3\sigma^{2}c}{2L(k+2)}$
    |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq(1-\dfrac{c\kappa}{(k+2)})^{T}(\mathcal{F}(\mathbf{w}_{0})-\mathcal{F}^{*})+\dfrac{3\sigma^{2}c}{2L(k+2)}$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: ∎
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Appendix C Supplementary Experiment Details
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 补充实验细节
- en: C.1 On-Device Memory Constraints
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 设备内存限制
- en: We include a table of common memory constraints imposed by edge or mobile devices
    as Table [3](#A3.T3 "Table 3 ‣ C.1 On-Device Memory Constraints ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity").
    We can find that a wide range of these devices impose a memory constraint of 8
    GiB as our main constraint that we consider when we develop our on-device personalization
    recipe in Section [3.5](#S3.SS5 "3.5 An Opportunity for On-Device LLM Personalization
    ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity").
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[3](#A3.T3 "表3 ‣ C.1 设备内存限制 ‣ 附录C 补充实验细节 ‣ 极端稀疏条件下的LLM零阶微调")中包含了一张边缘或移动设备常见内存限制的表格。我们可以发现，这些设备的广泛范围中，大多数设备的内存限制为8
    GiB，这是我们在第[3.5节](#S3.SS5 "3.5 设备内LLM个性化的机会 ‣ 3 极端稀疏下的ZO LLM微调 ‣ 极端稀疏条件下的LLM零阶微调")开发设备个性化方案时所考虑的主要限制。
- en: 'Table 3: Device memory of some mobile devices or consumer-graded GPUs.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：一些移动设备或消费级GPU的设备内存。
- en: '| Devices | Memory |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 设备 | 内存 |'
- en: '| Nvidia GeForce GTX 1080 Ti | 11 GiB |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Nvidia GeForce GTX 1080 Ti | 11 GiB |'
- en: '| Nvidia GeForce RTX 3060 Ti | 8 GiB |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Nvidia GeForce RTX 3060 Ti | 8 GiB |'
- en: '| Nvidia Jetson TX2 | 8 GiB |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Nvidia Jetson TX2 | 8 GiB |'
- en: '| OPPO Find X7 Ultra [[18](#bib.bib18)] | 12 GiB |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| OPPO Find X7 Ultra [[18](#bib.bib18)] | 12 GiB |'
- en: '| Samsung Galaxy S10 with Mali-G76 GPU [[10](#bib.bib10)] | 8 GiB |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Samsung Galaxy S10 配备Mali-G76 GPU [[10](#bib.bib10)] | 8 GiB |'
- en: C.2 Gradient Sparsity During LLM Fine-Tuning
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 LLM微调期间的梯度稀疏性
- en: In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Extreme Sparsity Pattern in LLM ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity"), we explore the FO gradient sparsity of Llama2-7B during fine-tuning
    (at Epoch 5). Here we follow the identical setting and plot the FO gradient sparsity
    for Llama2-7B, Mistral-7B, and OPT-6.7B during epoch 1, 5, and 10 (end of fine-tuning).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S3.F2 "图 2 ‣ 3.1 LLM 的极端稀疏模式 ‣ 3 追踪 ZO LLM 微调中的极端稀疏 ‣ 极端稀疏的 LLM 零阶微调")
    中，我们探索了 Llama2-7B 在微调期间（第 5 个周期）的 FO 梯度稀疏性。这里我们采用相同的设置，并绘制了 Llama2-7B、Mistral-7B
    和 OPT-6.7B 在第 1、5 和 10 个周期（微调结束时）的 FO 梯度稀疏性。
- en: We observe that the gradient sparsity is exhibited throughout the fine-tuning
    with slightly increasing towards the end. OPT-6.7B which uses ReLU as the activation
    function would demonstrate greater sparsity across tasks compared with Llama2-7B
    and Mistral-7B which uses SwiGLU and SiLU respectively. Nevertheless, the gradient
    sparsity pattern holds across architectures, tasks, and fine-tuning time in general.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，梯度稀疏性在微调过程中始终存在，并在最后略有增加。与使用 ReLU 作为激活函数的 OPT-6.7B 相比，使用 SwiGLU 和 SiLU
    的 Llama2-7B 和 Mistral-7B 在各任务中的稀疏性会更大。然而，梯度稀疏性模式在不同架构、任务和微调时间上基本保持一致。
- en: '![Refer to caption](img/db45c71c2845fff2d6497e14c4d8f9d8.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/db45c71c2845fff2d6497e14c4d8f9d8.png)'
- en: (a) Llama2-7B
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7B
- en: '![Refer to caption](img/0a267da58b54ee9abca12d6bafa66735.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0a267da58b54ee9abca12d6bafa66735.png)'
- en: (b) Mistral-7B
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral-7B
- en: '![Refer to caption](img/5850fc073b659522447c062dea8e31de.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5850fc073b659522447c062dea8e31de.png)'
- en: (c) OPT-6.7B
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: (c) OPT-6.7B
- en: 'Figure 7: Cumulative normalized sum of coordinate-wise gradient square $[\nabla\mathcal{F}(\mathbf{w})]_{i}^{2}$
    std of all blue curves.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：所有蓝色曲线的坐标方向梯度平方 $[\nabla\mathcal{F}(\mathbf{w})]_{i}^{2}$ 的标准累计归一化和。
- en: C.3 Transferability of Gradient Features from Pre-Training Datasets to Downstream
    Tasks
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 从预训练数据集到下游任务的梯度特征迁移性
- en: In Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Transferability of LLM Pre-Training Sparsity
    Pattern in ZO Fine-Tuning ‣ 3 Chasing Extreme Sparsity in ZO LLM Fine-Tuning ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we explore the transferability
    of gradient features from pre-training datasets (C4) to downstream tasks, and
    here we will also validate this phenomenon across models, as shown in Figure [8](#A3.F8
    "Figure 8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets
    to Downstream Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity"). As there are no solid lines (top-(1e-2,1e-3,1e-4))
    parameters with C4 gradient entries prior to fine-tuning) vanish to 0, we know
    the transferability of gradient features from C4 datasets to downstream datasets
    hold across models and downstream tasks. In this case, sensitive parameters determined
    from C4 gradients would still be similar to sensitive parameters determined from
    downstream task-specific gradients across models.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3](#S3.F3 "图 3 ‣ 3.3 LLM 预训练稀疏模式在 ZO 微调中的迁移性 ‣ 3 追踪 ZO LLM 微调中的极端稀疏 ‣ 极端稀疏的
    LLM 零阶微调") 中，我们探索了从预训练数据集（C4）到下游任务的梯度特征的迁移性，并将在图 [8](#A3.F8 "图 8 ‣ C.3 从预训练数据集到下游任务的梯度特征迁移性
    ‣ 附录 C 补充实验细节 ‣ 极端稀疏的 LLM 零阶微调") 中验证这一现象。由于在微调之前 C4 梯度条目没有实线（top-(1e-2,1e-3,1e-4)）消失到
    0，我们知道从 C4 数据集到下游数据集的梯度特征的迁移性在不同模型和下游任务中保持一致。在这种情况下，从 C4 梯度确定的敏感参数与从下游任务特定梯度确定的敏感参数在不同模型中仍然相似。
- en: '![Refer to caption](img/5a8cacf776f5baaa8925a3be7642969e.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5a8cacf776f5baaa8925a3be7642969e.png)'
- en: (a) Llama2-7B
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7B
- en: '![Refer to caption](img/8d96f39e3f6cdf0642e92c715a3e4e6b.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d96f39e3f6cdf0642e92c715a3e4e6b.png)'
- en: (b) Mistral-7B
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral-7B
- en: '![Refer to caption](img/649c25c64d71b92682b81336ee72ea35.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/649c25c64d71b92682b81336ee72ea35.png)'
- en: (c) OPT-6.7B
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (c) OPT-6.7B
- en: 'Figure 8: Cumulative normalized gradient square values of Llama2-7B (subfigure [8](#A3.F8
    "Figure 8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets
    to Downstream Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")), Mistral-7B (subfigure [8](#A3.F8
    "Figure 8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets
    to Downstream Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")), and OPT-6.7B (subfigure [8](#A3.F8
    "Figure 8 ‣ C.3 Transferability of Gradient Features from Pre-Training Datasets
    to Downstream Tasks ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity"))’s linear layers during FO fine-tuning.
    For a given model and training checkpoint, we report the average value across
    all linear layers as a line in each subfigure. For each line, the colors represent
    the fraction of parameters (1e-2,1e-3,1e-4) and the line style represents the
    category. “task grad, dyn.” refers to the sensitive parameters selected at the
    given timestep (x-axis), and “task grad, static” refers to the sensitive parameters
    selected before fine-tuning. “C4 grad, static” refers to the sensitive parameters
    selected with gradients taken from causal language modeling on C4 datasets, and
    we keep it unchanged during fine-tuning.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：Llama2-7B（子图 [8](#A3.F8 "图 8 ‣ C.3 从预训练数据集到下游任务的梯度特征的可迁移性 ‣ 附录 C 补充实验细节 ‣
    极端稀疏条件下的LLMs零阶微调")）、Mistral-7B（子图 [8](#A3.F8 "图 8 ‣ C.3 从预训练数据集到下游任务的梯度特征的可迁移性
    ‣ 附录 C 补充实验细节 ‣ 极端稀疏条件下的LLMs零阶微调")）和OPT-6.7B（子图 [8](#A3.F8 "图 8 ‣ C.3 从预训练数据集到下游任务的梯度特征的可迁移性
    ‣ 附录 C 补充实验细节 ‣ 极端稀疏条件下的LLMs零阶微调")）在FO微调过程中线性层的累积归一化梯度平方值。对于给定的模型和训练检查点，我们报告所有线性层的平均值作为每个子图中的一条线。每条线的颜色表示参数的比例（1e-2,1e-3,1e-4），线条样式表示类别。“任务梯度，动态”指的是在给定时间步（x轴）选择的敏感参数，“任务梯度，静态”指的是微调前选择的敏感参数。“C4梯度，静态”指的是从C4数据集上的因果语言建模中获取的敏感参数，我们在微调过程中保持不变。
- en: C.4 Hyperparameters in Experiments
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 实验中的超参数
- en: For all experiments, we use 20,000 training steps with ZO-SGD optimizer (Definition [2](#Thmdefinition2
    "Definition 2 (ZO-SGD update rule). ‣ 2.1 Zeroth-Order Optimization ‣ 2 Background
    and Related works ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")).
    We will save a model checkpoint every 500 steps, and load the checkpoint with
    the lowest loss on the validation set at the end of the training, and report its
    test set accuracy as result. Usually, the training/validation set will be sampled
    from the original dataset with size 1000/500 respectively and the test set is
    of size $\min(1000,|\text{original test set}|)$, except for CB and COPA that we
    use 100 for the validation set size. For all ZO experiments (Table [4](#A3.T4
    "Table 4 ‣ C.4 Hyperparameters in Experiments ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") and Table [5](#A3.T5
    "Table 5 ‣ C.4 Hyperparameters in Experiments ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")), we use batch
    size of 16\. This experiment setting is identical to Malladi et al. [[27](#bib.bib27)].
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们使用20,000个训练步骤和ZO-SGD优化器（定义 [2](#Thmdefinition2 "定义 2 (ZO-SGD更新规则)。
    ‣ 2.1 零阶优化 ‣ 2 背景及相关工作 ‣ 极端稀疏条件下的LLMs零阶微调")）。我们会每500步保存一个模型检查点，并在训练结束时加载验证集上损失最低的检查点，并报告其测试集准确度作为结果。通常，训练集/验证集将从原始数据集中分别抽样1000/500大小，测试集大小为$\min(1000,|\text{original
    test set}|)$，CB和COPA的验证集大小为100。对于所有ZO实验（表 [4](#A3.T4 "表 4 ‣ C.4 实验中的超参数 ‣ 附录 C
    补充实验细节 ‣ 极端稀疏条件下的LLMs零阶微调")和表 [5](#A3.T5 "表 5 ‣ C.4 实验中的超参数 ‣ 附录 C 补充实验细节 ‣ 极端稀疏条件下的LLMs零阶微调")），我们使用批量大小16。该实验设置与Malladi等人[[27](#bib.bib27)]相同。
- en: 'Table 4: The chosen hyperparameters for experiments in Table [1](#S4.T1 "Table
    1 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣
    4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). We repeat
    each hyperparameters for 3 random trials and report the average and standard deviation
    in Table [1](#S4.T1 "Table 1 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity").'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：表格[1](#S4.T1 "表 1 ‣ 4.1 RQ1: 稀疏 ZO 微调在敏感参数上的有效性 ‣ 4 实验 ‣ 极端稀疏的 LLMs 的零阶微调")中的实验选择的超参数。我们对每个超参数进行
    3 次随机试验，并在表[1](#S4.T1 "表 1 ‣ 4.1 RQ1: 稀疏 ZO 微调在敏感参数上的有效性 ‣ 4 实验 ‣ 极端稀疏的 LLMs 的零阶微调")中报告了平均值和标准差。'
- en: (a) Llama2-7B
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7B
- en: '|  | Methods | SST-2 | RTE | CB | BoolQ | WSC | WiC | COPA |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | SST-2 | RTE | CB | BoolQ | WSC | WiC | COPA |'
- en: '| Q, ZO | Sensitive (C4, static) ($\epsilon=$1e-3) | 5e-7 | 1e-6 | 1e-6 | 1e-6
    | 5e-7 | 1e-6 | 1e-6 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | 敏感（C4，静态）($\epsilon=$1e-3) | 5e-7 | 1e-6 | 1e-6 | 1e-6 | 5e-7 | 1e-6
    | 1e-6 |'
- en: '|  | LoRA ($\epsilon=$1e-3) | 1e-5 | 5e-5 | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 1e-5
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA ($\epsilon=$1e-3) | 1e-5 | 5e-5 | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 1e-5
    |'
- en: '|  | Prefix ($\epsilon=$1e-2) | 1e-4 | 2e-4 | 5e-4 | 5e-4 | 1e-4 | 5e-4 | 2e-4
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | 前缀 ($\epsilon=$1e-2) | 1e-4 | 2e-4 | 5e-4 | 5e-4 | 1e-4 | 5e-4 | 2e-4
    |'
- en: '| ZO | Sensitive (task, static) ($\epsilon=$1e-3) | 5e-7 | 1e-6 | 1e-6 | 1e-6
    | 1e-6 | 1e-6 | 2e-6 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| ZO | 敏感（任务，静态）($\epsilon=$1e-3) | 5e-7 | 1e-6 | 1e-6 | 1e-6 | 1e-6 | 1e-6
    | 2e-6 |'
- en: '|  | Random (static) ($\epsilon=$1e-3) | 2e-4 | 5e-4 | 2e-4 | 5e-4 | 2e-4 |
    5e-4 | 5e-4 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机（静态）($\epsilon=$1e-3) | 2e-4 | 5e-4 | 2e-4 | 5e-4 | 2e-4 | 5e-4 | 5e-4
    |'
- en: '|  | Full fine-tuning ($\epsilon=$1e-3) | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 2e-7
    | 5e-7 | 5e-7 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | 全量微调 ($\epsilon=$1e-3) | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 2e-7 | 5e-7 | 5e-7
    |'
- en: '|  | ICL (#examples) | 16 | 16 | 16 | 8 | 16 | 8 | 8 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL（#例子） | 16 | 16 | 16 | 8 | 16 | 8 | 8 |'
- en: (b) Mistral-7B
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral-7B
- en: '| Q, ZO | Sensitive (C4, static) ($\epsilon=$1e-4) | 2e-8 | 5e-8 | 2e-8 | 2e-8
    | 1e-8 | 2e-8 | 2e-8 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | 敏感（C4，静态）($\epsilon=$1e-4) | 2e-8 | 5e-8 | 2e-8 | 2e-8 | 1e-8 | 2e-8
    | 2e-8 |'
- en: '|  | LoRA ($\epsilon=$1e-4) | 2e-6 | 5e-6 | 2e-6 | 2e-6 | 2e-6 | 2e-6 | 2e-6
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA ($\epsilon=$1e-4) | 2e-6 | 5e-6 | 2e-6 | 2e-6 | 2e-6 | 2e-6 | 2e-6
    |'
- en: '|  | Prefix ($\epsilon=$1e-3) | 1e-3 | 2e-3 | 1e-3 | 1e-2 | 5e-4 | 1e-3 | 5e-4
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | 前缀 ($\epsilon=$1e-3) | 1e-3 | 2e-3 | 1e-3 | 1e-2 | 5e-4 | 1e-3 | 5e-4
    |'
- en: '| ZO | Sensitive (task, static) ($\epsilon=$1e-4) | 5e-8 | 5e-8 | 2e-8 | 2e-8
    | 2e-8 | 2e-8 | 2e-8 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| ZO | 敏感（任务，静态）($\epsilon=$1e-4) | 5e-8 | 5e-8 | 2e-8 | 2e-8 | 2e-8 | 2e-8
    | 2e-8 |'
- en: '|  | Random (static) ($\epsilon=$1e-4) | 1e-5 | 2e-6 | 5e-6 | 1e-5 | 1e-6 |
    2e-6 | 2e-5 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机（静态）($\epsilon=$1e-4) | 1e-5 | 2e-6 | 5e-6 | 1e-5 | 1e-6 | 2e-6 | 2e-5
    |'
- en: '|  | Full fine-tuning ($\epsilon=$1e-4) | 2e-8 | 2e-8 | 1e-8 | 1e-8 | 1e-8
    | 1e-8 | 2e-8 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | 全量微调 ($\epsilon=$1e-4) | 2e-8 | 2e-8 | 1e-8 | 1e-8 | 1e-8 | 1e-8 | 2e-8
    |'
- en: '|  | ICL (#examples) | 4 | 8 | 4 | 16 | 4 | 4 | 8 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL（#例子） | 4 | 8 | 4 | 16 | 4 | 4 | 8 |'
- en: (c) OPT-6.7B
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: (c) OPT-6.7B
- en: '| Q, ZO | Sensitive (C4, static) ($\epsilon=$1e-3) | 2e-7 | 5e-7 | 5e-7 | 5e-7
    | 2e-7 | 5e-7 | 2e-7 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Q, ZO | 敏感（C4，静态）($\epsilon=$1e-3) | 2e-7 | 5e-7 | 5e-7 | 5e-7 | 2e-7 | 5e-7
    | 2e-7 |'
- en: '|  | LoRA ($\epsilon=$1e-3) | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 2e-5
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA ($\epsilon=$1e-3) | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 1e-5 | 2e-5 | 2e-5
    |'
- en: '|  | Prefix ($\epsilon=$1e-2) | 2e-3 | 1e-2 | 1e-3 | 5e-3 | 5e-3 | 1e-2 | 5e-3
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | 前缀 ($\epsilon=$1e-2) | 2e-3 | 1e-2 | 1e-3 | 5e-3 | 5e-3 | 1e-2 | 5e-3
    |'
- en: '| ZO | Sensitive (task, static) ($\epsilon=$1e-3) | 2e-7 | 5e-7 | 5e-7 | 2e-7
    | 2e-7 | 5e-7 | 2e-7 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| ZO | 敏感（任务，静态）($\epsilon=$1e-3) | 2e-7 | 5e-7 | 5e-7 | 2e-7 | 2e-7 | 5e-7
    | 2e-7 |'
- en: '|  | Random (static) ($\epsilon=$1e-3) | 1e-4 | 5e-5 | 2e-5 | 5e-5 | 2e-4 |
    5e-5 | 5e-5 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机（静态）($\epsilon=$1e-3) | 1e-4 | 5e-5 | 2e-5 | 5e-5 | 2e-4 | 5e-5 | 5e-5
    |'
- en: '|  | Full fine-tuning ($\epsilon=$1e-3) | 2e-7 | 2e-7 | 2e-7 | 2e-7 | 2e-7
    | 2e-7 | 5e-7 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | 全量微调 ($\epsilon=$1e-3) | 2e-7 | 2e-7 | 2e-7 | 2e-7 | 2e-7 | 2e-7 | 5e-7
    |'
- en: '|  | ICL (#examples) | 16 | 4 | 16 | 16 | 16 | 8 | 16 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | ICL（#例子） | 16 | 4 | 16 | 16 | 16 | 8 | 16 |'
- en: 'Table 5: The chosen hyperparameters for experiments in Figure [5](#S4.F5 "Figure
    5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣
    4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") and Figure [5](#S4.F5
    "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters
    ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). We
    repeat each hyperparameters for 3 random trials and report the average to draw
    a line in Figure [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity") and Figure [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness of
    Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"), and we use Llama2-7B for all experiments. For
    each subtable, we include the fraction to optimize on its header and report the
    chosen learning rate on each cell.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：图 [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning
    on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity") 和图 [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse
    ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") 中选择的超参数。我们对每个超参数进行3次随机试验，并报告平均值，以便在图 [5](#S4.F5
    "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters
    ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") 和图 [5](#S4.F5
    "Figure 5 ‣ 4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters
    ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") 中绘制线条，我们使用
    Llama2-7B 进行所有实验。对于每个子表格，我们在标题中包括优化的分数，并报告每个单元格中选择的学习率。'
- en: (a) RTE
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RTE
- en: '| Methods | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
- en: '| Sensitive (C4, static) ($\epsilon=$1e-3) | 1e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（C4，静态）（$\epsilon=$1e-3） | 1e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Sensitive (task-specific, static) ($\epsilon=$1e-3) | 1e-5 | 1e-6 | 1e-6
    | 1e-6 | 1e-6 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，静态）（$\epsilon=$1e-3） | 1e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Sensitive (task-specific, dynamic) ($\epsilon=$1e-3) | 1e-5 | 1e-6 | 1e-6
    | 1e-6 | 1e-6 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，动态）（$\epsilon=$1e-3） | 1e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Random (static) ($\epsilon=$1e-3) | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-5 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 随机（静态）（$\epsilon=$1e-3） | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-5 |'
- en: '| Random (dynamic) ($\epsilon=$1e-3) | 2e-2 | 5e-3 | 2e-4 | 5e-5 | 5e-6 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 随机（动态）（$\epsilon=$1e-3） | 2e-2 | 5e-3 | 2e-4 | 5e-5 | 5e-6 |'
- en: '| Weight outliers (static) ($\epsilon=$1e-3) | 2e-3 | 1e-3 | 2e-4 | 5e-5 |
    1e-5 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 权重离群点（静态）（$\epsilon=$1e-3） | 2e-3 | 1e-3 | 2e-4 | 5e-5 | 1e-5 |'
- en: (b) WiC
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: (b) WiC
- en: '| Methods | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
- en: '| Sensitive (C4, static) ($\epsilon=$1e-3) | 1e-5 | 2e-6 | 1e-6 | 1e-6 | 1e-6
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（C4，静态）（$\epsilon=$1e-3） | 1e-5 | 2e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Sensitive (task-specific, static) ($\epsilon=$1e-3) | 1e-5 | 2e-6 | 1e-6
    | 1e-6 | 1e-6 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，静态）（$\epsilon=$1e-3） | 1e-5 | 2e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Sensitive (task-specific, dynamic) ($\epsilon=$1e-3) | 1e-5 | 2e-6 | 1e-6
    | 1e-6 | 1e-6 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，动态）（$\epsilon=$1e-3） | 1e-5 | 2e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Random (static) ($\epsilon=$1e-3) | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-6 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 随机（静态）（$\epsilon=$1e-3） | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-6 |'
- en: '| Random (dynamic) ($\epsilon=$1e-3) | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-6 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 随机（动态）（$\epsilon=$1e-3） | 2e-2 | 5e-3 | 5e-4 | 5e-5 | 5e-6 |'
- en: '| Weight outliers (static) ($\epsilon=$1e-3) | 1e-3 | 5e-4 | 2e-4 | 1e-4 |
    2e-5 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 权重离群点（静态）（$\epsilon=$1e-3） | 1e-3 | 5e-4 | 2e-4 | 1e-4 | 2e-5 |'
- en: (c) COPA
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: (c) COPA
- en: '| Methods | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1e-5 | 1e-4 | 1e-3 | 1e-2 | 1e-1 |'
- en: '| Sensitive (C4, static) ($\epsilon=$1e-3) | 5e-6 | 1e-6 | 1e-6 | 1e-6 | 5e-7
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（C4，静态）（$\epsilon=$1e-3） | 5e-6 | 1e-6 | 1e-6 | 1e-6 | 5e-7 |'
- en: '| Sensitive (task-specific, static) ($\epsilon=$1e-3) | 5e-6 | 2e-6 | 2e-6
    | 1e-6 | 1e-6 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，静态）（$\epsilon=$1e-3） | 5e-6 | 2e-6 | 2e-6 | 1e-6 | 1e-6 |'
- en: '| Sensitive (task-specific, dynamic) ($\epsilon=$1e-3) | 5e-6 | 1e-6 | 1e-6
    | 1e-6 | 1e-6 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 敏感（任务特定，动态）（$\epsilon=$1e-3） | 5e-6 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| Random (static) ($\epsilon=$1e-3) | 1e-2 | 2e-3 | 5e-4 | 5e-5 | 5e-6 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 随机（静态）（$\epsilon=$1e-3） | 1e-2 | 2e-3 | 5e-4 | 5e-5 | 5e-6 |'
- en: '| Random (dynamic) ($\epsilon=$1e-3) | 2e-3 | 1e-3 | 2e-4 | 2e-5 | 2e-6 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 随机（动态）（$\epsilon=$1e-3） | 2e-3 | 1e-3 | 2e-4 | 2e-5 | 2e-6 |'
- en: '| Weight outliers (static) ($\epsilon=$1e-3) | 1e-3 | 5e-4 | 5e-4 | 1e-4 |
    1e-5 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 权重离群点（静态）（$\epsilon=$1e-3） | 1e-3 | 5e-4 | 5e-4 | 1e-4 | 1e-5 |'
- en: 'Our hyperparameters (learning rate $\eta$ throughout our experiments. We also
    report the chosen hyperparameter for Figure [5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") and Figure [5](#S4.F5 "Figure 5 ‣
    4.1 RQ1: Effectiveness of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") in Table [5](#A3.T5
    "Table 5 ‣ C.4 Hyperparameters in Experiments ‣ Appendix C Supplementary Experiment
    Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"). For LoRA,
    we always add to all linear layers with $r=8$ with length as 5, as what Malladi
    et al. [[27](#bib.bib27)] uses.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的超参数（学习率$\eta$）贯穿我们的实验。我们还报告了图[5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")和图[5](#S4.F5 "Figure 5 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")中的选择超参数在表[5](#A3.T5 "Table 5 ‣ C.4
    Hyperparameters in Experiments ‣ Appendix C Supplementary Experiment Details ‣
    Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")。对于LoRA，我们总是添加到所有线性层中，$r=8$，长度为5，就像Malladi等人[[27](#bib.bib27)]所使用的那样。'
- en: C.5 Task-Specific Prompts in Experiments
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 实验中的任务特定提示
- en: We describe our task templates in Table [6](#A3.T6 "Table 6 ‣ C.5 Task-Specific
    Prompts in Experiments ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity").
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[6](#A3.T6 "Table 6 ‣ C.5 Task-Specific Prompts in Experiments ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")中描述了我们的任务模板。
- en: 'Table 6: Task templates for all experiments. On the left column we include
    the task name and the model name, and on the right column we describe the exact
    prompt with answer candidates.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：所有实验的任务模板。在左侧列中，我们包含任务名称和模型名称，在右侧列中，我们描述了准确的提示和答案候选。
- en: '| Task |  | Prompts |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |  | 提示 |'
- en: '| SST-2 (Llama2-7B) |  | ### Sentence:  ### Sentiment: negative/positive
    |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 (Llama2-7B) |  | ### 句子： ### 情感：负面/正面 |'
- en: '| SST-2 (Mistral-7B, OPT-6.7B) |  |  It was terrible/great |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 (Mistral-7B, OPT-6.7B) |  |  这很糟糕/很棒 |'
- en: '| RTE (Llama2-7B) |  | Suppose "" Can we infer that ""?
    Yes or No? Yes/No |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| RTE (Llama2-7B) |  | 假设"" 我们可以推断出""吗？ 是还是否？ 是/否 |'
- en: '| RTE (Mistral-7B, OPT-6.7B) |  |  Does this mean that ""
    is true? Yes or No?'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '| RTE (Mistral-7B, OPT-6.7B) |  |  这是否意味着""是真的？ 是还是否？'
- en: Yes/No |
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 是/否 |
- en: '| CB (Llama2-7B, Mistral-7B, OPT-6.7B) |  | Suppose  Can we infer
    that ""? Yes, No, or Maybe? Yes/No/Maybe |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| CB (Llama2-7B, Mistral-7B, OPT-6.7B) |  | 假设  我们可以推断出""吗？
    是、否，还是可能？ 是/否/可能 |'
- en: '| BoolQ (Llama2-7B) |  |  ? Yes/No |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ (Llama2-7B) |  |  ? 是/否 |'
- en: '| BoolQ (Mistral-7B, OPT-6.7B) |  |  ? Yes/No |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ (Mistral-7B, OPT-6.7B) |  |  ? 是/否 |'
- en: '| WSC (Llama2-7B, Mistral-7B, OPT-6.7B) |  |  In the previous sentence,
    does the pronoun "" refer to ? Yes or No?'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '| WSC (Llama2-7B, Mistral-7B, OPT-6.7B) |  |  在前一句中，代词""是否指代?
    是还是否？'
- en: Yes/No |
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 是/否 |
- en: '| WiC (Llama2-7B, Mistral-7B, OPT-6.7B) |  | Does the word "" have the
    same meaning in these two sentences? Yes, No? '
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '| WiC (Llama2-7B, Mistral-7B, OPT-6.7B) |  | 词语""在这两个句子中的意思是否相同？ 是、否？
    '
- en: 
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: Yes/No |
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 是/否 |
- en: '| COPA (Llama2-7B, Mistral-7B, OPT-6.7B) |  |  so/because 
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| COPA (Llama2-7B, Mistral-7B, OPT-6.7B) |  |  因此/因为  |'
- en: C.6 Implementation of Sparse Operations in Linear Layers
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.6 稀疏操作在线性层中的实现
- en: 'Linear layers in LLMs often contribute most parameters [[14](#bib.bib14)].
    Since from Equation [11](#S3.E11 "Equation 11 ‣ 3.4 Our Proposal: ZO LLM Fine-Tuning
    with Fisher-Informed, Transferable Sparsity ‣ 3 Chasing Extreme Sparsity in ZO
    LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") we
    know'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM中的线性层通常贡献了大部分参数[[14](#bib.bib14)]。由于从方程[11](#S3.E11 "Equation 11 ‣ 3.4 Our
    Proposal: ZO LLM Fine-Tuning with Fisher-Informed, Transferable Sparsity ‣ 3 Chasing
    Extreme Sparsity in ZO LLM Fine-Tuning ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")我们知道'
- en: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}=\mathbf{w}\odot\mathbf{m}_{k},\quad\mathbf{w}_{\text{dense}}=\mathbf{w}\odot(\mathbf{1}_{d}-\mathbf{m}_{k}),\quad\mathbf{w}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}+\mathbf{w}_{\text{dense}}$
    |  | (17) |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}=\mathbf{w}\odot\mathbf{m}_{k},\quad\mathbf{w}_{\text{dense}}=\mathbf{w}\odot(\mathbf{1}_{d}-\mathbf{m}_{k}),\quad\mathbf{w}={\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}+\mathbf{w}_{\text{dense}}$
    |  | (17) |'
- en: 'and since $\mathbf{w}_{\text{dense}}$ to improve the computational efficiency
    of linear layers after extracting the sparse parameters. In this case, we would
    have two different methods to implement the forward pass of linear layers (with
    induced sparse operation colored in red):'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\mathbf{w}_{\text{dense}}$ 提高了提取稀疏参数后线性层的计算效率。在这种情况下，我们将有两种不同的方法来实现线性层的前向传播（红色标记的诱发稀疏操作）：
- en: '|  | $\displaystyle\ \mathbf{w}\mathbf{x}$ |  | (18) |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ \mathbf{w}\mathbf{x}$ |  | (18) |'
- en: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}(\mathrm{DenseMM}(\mathbf{w}_{\text{dense}},\mathbf{x}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{x})$
    | faster with token generation |  | (19) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}(\mathrm{DenseMM}(\mathbf{w}_{\text{dense}},\mathbf{x}),{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{x})$
    | 使用令牌生成更快 |  | (19) |'
- en: '|  |  | $\displaystyle=(\mathbf{w}_{\text{dense}}{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}+\mathbf{w}_{\text{sparse}}})\mathbf{x}$
    |  | (20) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\mathbf{w}_{\text{dense}}{\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}+\mathbf{w}_{\text{sparse}}})\mathbf{x}$
    |  | (20) |'
- en: '|  |  | $\displaystyle=\mathrm{DenseMM}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAdd}}({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{w}_{\text{dense}}),\mathbf{x})$
    | faster with ZO training |  | (21) |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathrm{DenseMM}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAdd}}({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{w}_{\text{dense}}),\mathbf{x})$
    | 使用 ZO 训练更快 |  | (21) |'
- en: '![Refer to caption](img/9518e7e597646bb27839a3c196936b30.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9518e7e597646bb27839a3c196936b30.png)'
- en: 'Figure 9: Time of SparseAdd (Equation [21](#A3.E21 "Equation 21 ‣ C.6 Implementation
    of Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")) versus SparseAddMM
    (Equation [19](#A3.E19 "Equation 19 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")) in Llama2-7B ZO training forward
    & inference. In subfigure 1 and 3, we use Nvidia RTX A6000 and Intel Xeno Gold
    6342 CPUs, with PyTorch version 2.2, HuggingFace version 4.36, and CUDA 12.2\.
    In subfigure 2 and 4, we use Nvidia A100-SXM4 (40 GiB) and AMD EPYC 7543P 32-Core
    CPU with PyTorch version 2.1, HuggingFace version 4.38.2, and CUDA 12.2\. We use
    Flash Attention 2 [[5](#bib.bib5)] for all 4 subfigures.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：SparseAdd (方程式 [21](#A3.E21 "方程式 21 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 补充实验细节 ‣ 极端稀疏性下的LLMs零阶微调"))
    与 SparseAddMM (方程式 [19](#A3.E19 "方程式 19 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 补充实验细节 ‣ 极端稀疏性下的LLMs零阶微调"))
    在 Llama2-7B ZO 训练的前向传播和推理中的时间。在子图 1 和 3 中，我们使用 Nvidia RTX A6000 和 Intel Xeno Gold
    6342 CPUs，PyTorch 版本 2.2，HuggingFace 版本 4.36，以及 CUDA 12.2。 在子图 2 和 4 中，我们使用 Nvidia
    A100-SXM4 (40 GiB) 和 AMD EPYC 7543P 32 核 CPU，PyTorch 版本 2.1，HuggingFace 版本 4.38.2，以及
    CUDA 12.2。我们在所有 4 个子图中使用 Flash Attention 2 [[5](#bib.bib5)]。
- en: 'The specific choice of employing Equation [19](#A3.E19 "Equation 19 ‣ C.6 Implementation
    of Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") or Equation [21](#A3.E21
    "Equation 21 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") needs careful consideration and benchmarking, but here we can provide
    a general guideline based on the size of input vector (or arithmetic intensity)
    and potential integration with weight quantization method:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方程式 [19](#A3.E19 "方程式 19 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 补充实验细节 ‣ 极端稀疏性下的LLMs零阶微调")
    或方程式 [21](#A3.E21 "方程式 21 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 补充实验细节 ‣ 极端稀疏性下的LLMs零阶微调")
    需要仔细考虑和基准测试，但我们可以根据输入向量的大小（或算术强度）和潜在的权重量化方法的集成提供一般的指导：
- en: Size of input vectors $\mathbf{x}$ would be large enough such that Equation [19](#A3.E19
    "Equation 19 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity") would induce large computational overhead, as shown in subfigure 1
    of Figure [9](#A3.F9 "Figure 9 ‣ C.6 Implementation of Sparse Operations in Linear
    Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity"). In contrast, the computational complexity of
    Equation [21](#A3.E21 "Equation 21 ‣ C.6 Implementation of Sparse Operations in
    Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") is independent of $\mathbf{x}$ is large, we would
    expect Equation [21](#A3.E21 "Equation 21 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") is much faster than Equation [19](#A3.E19
    "Equation 19 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity"). As an example, we use sequence length of 512 and batch size 16 sampled
    from WikiText-2 dataset [[29](#bib.bib29)] as a representative computational intensity
    for ZO training in subfigures 1 and 2 in Figure [9](#A3.F9 "Figure 9 ‣ C.6 Implementation
    of Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity").
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量$\mathbf{x}$的大小可能足够大，以至于方程式[19](#A3.E19 "方程式 19 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录
    C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")会引起较大的计算开销，如图[9](#A3.F9 "图 9 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录
    C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")中子图1所示。相比之下，方程式[21](#A3.E21 "方程式 21 ‣ C.6 稀疏操作在线性层中的实现
    ‣ 附录 C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")的计算复杂度与$\mathbf{x}$的大小无关，因此我们预期方程式[21](#A3.E21
    "方程式 21 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")比方程式[19](#A3.E19 "方程式
    19 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")要快得多。例如，我们在图[9](#A3.F9 "图
    9 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")的子图1和2中，使用了从WikiText-2数据集中采样的序列长度为512和批量大小为16作为ZO训练的代表性计算强度。
- en: However, during autoregressive token generation, on each step we would only
    append a single token to the previously cached embeddings, and in this case $\mathbf{x}$
    is already sparse. This is also illustrated in subfigure 3 and 4 in Figure [9](#A3.F9
    "Figure 9 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity"). However, we note that the specific implementation choice is hardware
    and task dependent and requires thorough benchmarking and we will leave it as
    a future work.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在自回归标记生成过程中，每一步我们只会将一个标记附加到先前缓存的嵌入中，在这种情况下$\mathbf{x}$已经是稀疏的。这一点在图[9](#A3.F9
    "图 9 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")的子图3和4中也有说明。然而，我们指出，具体的实现选择依赖于硬件和任务，需要经过彻底的基准测试，我们将把这作为未来的工作。
- en: We recommend using Equation [21](#A3.E21 "Equation 21 ‣ C.6 Implementation of
    Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") during large-batched
    ZO training and Equation [19](#A3.E19 "Equation 19 ‣ C.6 Implementation of Sparse
    Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") during small-batched autoregressive
    token generation.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在大批量ZO训练过程中使用方程式[21](#A3.E21 "方程式 21 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 附加实验细节 ‣
    极端稀疏的LLM的零阶微调")，在小批量自回归标记生成过程中使用方程式[19](#A3.E19 "方程式 19 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录
    C 附加实验细节 ‣ 极端稀疏的LLM的零阶微调")。
- en: 'In light of this observation, in our Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we implement both
    “SparseAdd” and “SparseAddMM” methods for “Sensitive (0.1%)” and “Random (10%)”.
    For each method we report the lowest time out of these 2 implementations: for
    “Sensitive (0.1%)” training and “Random (10%)” training and inference, we use
    “SparseAdd” approach. For “Sensitive (0.1%)” inference, we use the “SparseAddMM”
    approach.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一观察，在我们的图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 极端稀疏 LLM 的零阶微调") 中，我们为“Sensitive (0.1%)”和“Random
    (10%)”实施了“SparseAdd”和“SparseAddMM”两种方法。对于每种方法，我们报告这两种实现中的最低时间：对于“Sensitive (0.1%)”训练和“Random
    (10%)”训练及推理，我们使用“SparseAdd”方法。对于“Sensitive (0.1%)”推理，我们使用“SparseAddMM”方法。
- en: 'Integration with weight quantization method. Weight quantization algorithms
    can be categorized into 2 categories: uniform quantization method and non-uniform
    quantization method. For uniform quantization method, Xi et al. [[45](#bib.bib45)]
    indicates that we could use integer matrix multiplication to compute $Q(\mathbf{w}_{\text{dense}})\mathbf{x}$.
    In this case, we also have 3 different implementations:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 与权重量化方法的集成。权重量化算法可以分为两类：均匀量化方法和非均匀量化方法。对于均匀量化方法，Xi 等人 [[45](#bib.bib45)] 指出我们可以使用整数矩阵乘法来计算
    $Q(\mathbf{w}_{\text{dense}})\mathbf{x}$。在这种情况下，我们还具有 3 种不同的实现方式：
- en: '|  | $\displaystyle\ Q(\mathbf{w})\mathbf{x}$ |  | (22) |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ Q(\mathbf{w})\mathbf{x}$ |  | (22) |'
- en: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}\Bigl{(}\mathrm{Dequantize}\bigl{(}\mathrm{IntMM}(Q(\mathbf{w}_{\text{dense}}),\mathbf{x})\bigr{)},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{x}\Bigr{)}\quad\text{fits
    with integer matmul }$ |  | (23) |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}\Bigl{(}\mathrm{Dequantize}\bigl{(}\mathrm{IntMM}(Q(\mathbf{w}_{\text{dense}}),\mathbf{x})\bigr{)},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathbf{x}\Bigr{)}\quad\text{符合整数矩阵乘法
    }$ |  | (23) |'
- en: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}\Bigl{(}\text{Dequantize}(Q(\mathbf{w}_{\text{dense}})),\mathbf{x},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}\Bigr{)}$
    |  | (24) |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle={\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAddMM}}\Bigl{(}\text{Dequantize}(Q(\mathbf{w}_{\text{dense}})),\mathbf{x},{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}}\Bigr{)}$
    |  | (24) |'
- en: '|  |  | $\displaystyle=(\mathrm{Dequantize}(Q(\mathbf{w}_{\text{dense}})){\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}+}{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}})\mathbf{x}$
    |  | (25) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\mathrm{Dequantize}(Q(\mathbf{w}_{\text{dense}})){\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}+}{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}})\mathbf{x}$
    |  | (25) |'
- en: '|  |  | $\displaystyle=\mathrm{DenseMM}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAdd}}\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathrm{Dequantize}(Q(\mathbf{w}_{\text{dense}})),\mathbf{x}\right)$
    |  | (26) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathrm{DenseMM}({\color[rgb]{1,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\mathrm{SparseAdd}}\left({\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\mathbf{w}_{\text{sparse}}},\mathrm{Dequantize}(Q(\mathbf{w}_{\text{dense}})),\mathbf{x}\right)$
    |  | (26) |'
- en: Equation [23](#A3.E23 "Equation 23 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") would compute $\mathrm{IntMM}(Q(\mathbf{w}_{\text{dense}}),\mathbf{x})$
    in last paragraph. We will leave a practical implementation and thorough benchmarking
    into a future work.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [23](#A3.E23 "方程 23 ‣ C.6 稀疏操作在线性层中的实现 ‣ 附录 C 补充实验细节 ‣ 极端稀疏 LLM 的零阶微调") 将计算上一段中的
    $\mathrm{IntMM}(Q(\mathbf{w}_{\text{dense}}),\mathbf{x})$。我们将把实际实现和全面基准测试留到未来工作中。
- en: We recommend using Equation [23](#A3.E23 "Equation 23 ‣ C.6 Implementation of
    Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity") when we use efficient
    integer matmul to compute $Q(\mathbf{w}_{\text{dense}})\mathbf{x}$ and in other
    cases, using Equation [24](#A3.E24 "Equation 24 ‣ C.6 Implementation of Sparse
    Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity") or Equation [26](#A3.E26 "Equation
    26 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")
    follows our previous recommendation based on the size of input vectors.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在使用高效整数矩阵乘法计算$Q(\mathbf{w}_{\text{dense}})\mathbf{x}$时使用方程[23](#A3.E23 "Equation
    23 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")，而在其他情况下，使用方程[24](#A3.E24
    "Equation 24 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix
    C Supplementary Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme
    Sparsity")或方程[26](#A3.E26 "Equation 26 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")，根据输入向量的大小遵循我们的先前建议。
- en: C.7 Hardware, Platform, Libraries, and Other Details for Benchmarking
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.7 硬件、平台、库和基准测试的其他细节
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Zeroth-Order Fine-Tuning of
    LLMs with Extreme Sparsity"), Figure [6](#S4.F6 "Figure 6 ‣ 4.1 RQ1: Effectiveness
    of Sparse ZO Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity"), and Figure [9](#A3.F9 "Figure 9 ‣
    C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")
    (subfigure 1 and 3) are trained and evaluated on an internal cluster with 8 Nvidia
    RTX A6000 GPUs and 2 Intel Xeon Gold 6342 CPUs, with PyTorch version 2.2, HuggingFace
    version 4.36, and CUDA 12.2\. In subfigure 2 and 4 in Figure [9](#A3.F9 "Figure
    9 ‣ C.6 Implementation of Sparse Operations in Linear Layers ‣ Appendix C Supplementary
    Experiment Details ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"),
    we use Nvidia A100-SXM4 (40 GiB) and AMD EPYC 7543P 32-Core CPU with PyTorch version
    2.1, HuggingFace version 4.38.2, and CUDA 12.2\. We use Flash Attention 2 [[5](#bib.bib5)]
    throughout our experiments, and the base model for benchmarking is always Llama2-7B
    with Float16 datatype.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")、图[6](#S4.F6 "Figure 6 ‣ 4.1 RQ1: Effectiveness of Sparse ZO
    Fine-Tuning on Sensitive Parameters ‣ 4 Experiments ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") 和图[9](#A3.F9 "Figure 9 ‣ C.6 Implementation of
    Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity")（子图1和3）在一个包含8个Nvidia
    RTX A6000 GPU和2个Intel Xeon Gold 6342 CPU的内部集群上进行训练和评估，使用PyTorch 2.2版本、HuggingFace
    4.36版本和CUDA 12.2。在图[9](#A3.F9 "Figure 9 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")的子图2和4中，我们使用Nvidia A100-SXM4（40 GiB）和AMD
    EPYC 7543P 32核心CPU，PyTorch 2.1版本、HuggingFace 4.38.2版本和CUDA 12.2。我们在所有实验中使用Flash
    Attention 2 [[5](#bib.bib5)]，基准测试的基础模型始终是Float16数据类型的Llama2-7B。'
- en: 'In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Zeroth-Order Fine-Tuning
    of LLMs with Extreme Sparsity") and Figure [9](#A3.F9 "Figure 9 ‣ C.6 Implementation
    of Sparse Operations in Linear Layers ‣ Appendix C Supplementary Experiment Details
    ‣ Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"), we use sequence length
    of 512 and batch size 16 sampled from WikiText-2 dataset [[29](#bib.bib29)] as
    a representative computational intensity for ZO training, and for inference we
    generate 128 tokens with top-$p$) sampling from the prompt “Please describe the
    effect of sparse zeroth-order optimization methods on memory-efficient LLM fine-tuning:
    ”.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Zeroth-Order Fine-Tuning of LLMs with
    Extreme Sparsity")和图[9](#A3.F9 "Figure 9 ‣ C.6 Implementation of Sparse Operations
    in Linear Layers ‣ Appendix C Supplementary Experiment Details ‣ Zeroth-Order
    Fine-Tuning of LLMs with Extreme Sparsity")中，我们使用长度为512的序列和从WikiText-2数据集[[29](#bib.bib29)]中抽取的批量大小为16的样本作为ZO训练的代表性计算强度，对于推理，我们从提示“请描述稀疏零阶优化方法对内存高效LLM微调的影响”中生成128个标记，使用top-$p$采样。
