- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ReALLM: A general framework for LLM compression and fine-tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReALLM：一种通用的LLM压缩和微调框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13155](https://ar5iv.labs.arxiv.org/html/2405.13155)
- en: \newaliascnt
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newaliascnt
- en: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: lemmatheorem \aliascntresetthelemma \newaliascntcorollarytheorem \aliascntresetthecorollary
    \newaliascntpropositiontheorem \aliascntresettheproposition \newaliascntdefinitiontheorem
    \aliascntresetthedefinition \newaliascntremarktheorem \aliascntresettheremark
- en: Louis Leconte
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 路易斯·勒孔特
- en: Lisite, Isep, Sorbonne University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Lisite，Isep，索邦大学
- en: Math. and Algo. Sciences Lab, Huawei Tech
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室，华为技术
- en: louis.leconte@ens-paris-saclay.fr
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: louis.leconte@ens-paris-saclay.fr
- en: '&Lisa Bedin^∗'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Lisa Bedin^∗'
- en: CMAP, Ecole Polytechnique, France
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP，巴黎高科，法国
- en: lisa.bedin@polytechnique.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: lisa.bedin@polytechnique.edu
- en: Van Minh Nguyen
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 范敏阮
- en: Math. and Algo. Sciences Lab, Huawei Tech.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与算法科学实验室，华为技术。
- en: '&Eric Moulines'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&Eric Moulines'
- en: CMAP, Ecole Polytechnique, France equal contribution
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CMAP，巴黎高科，法国，等贡献
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce ReALLM, a novel approach for compression and memory-efficient adaptation
    of pre-trained language models that encompasses most of the post-training quantization
    and fine-tuning methods for a budget of $ patches extracted from pre-trained LLM matrices, and we use the HNeRV
    Chen et al., ([2023](#bib.bib6)) autoencoder model. For more details on the practical
    aspect of decoder training, see [Section A.2](#A1.SS2 "A.2 Autoencoder computational
    limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM: A general
    framework for LLM compression and fine-tuning").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReALLM 的超参数集合为：$r$ 解码器的参数数量和位数。我们进行了广泛的实验以寻找合适的配置；然而，我们无法测试大解码器大小的配置。例如，对于小的嵌入
    ($e_{0}e_{1}e_{2} 从预训练的 LLM 矩阵中提取的补丁，我们使用 HNeRV Chen
    et al., ([2023](#bib.bib6)) 自编码器模型。有关解码器训练的实际细节，请参见[第 A.2 节](#A1.SS2 "A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning")。'
- en: We have experimentally discovered two sets of optimal combinations of hyperparameters
    that depend on the type and shape of the pre-trained matrix. Some pre-trained
    matrices, especially those closer to the input tokens, compress better with small
    latent representations ($e_{0}e_{1}e_{2}<1024$) with low bit precision ($b\ll
    8$).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验性地发现了两组最佳超参数组合，这些组合依赖于预训练矩阵的类型和形状。一些预训练矩阵，特别是那些更接近输入标记的矩阵，压缩效果更好，使用较小的潜在表示
    ($e_{0}e_{1}e_{2}<1024$) 和较低的比特精度 ($b\ll 8$)。
- en: '![Refer to caption](img/2a3b8c73103df94f28a4dac4b780901b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a3b8c73103df94f28a4dac4b780901b.png)'
- en: (a) Mistral-7B (Jiang et al.,, [2023](#bib.bib22))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral-7B (Jiang et al., [2023](#bib.bib22))
- en: '![Refer to caption](img/56a4889e163c5d185e777e5271277b5e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56a4889e163c5d185e777e5271277b5e.png)'
- en: (b) Llama2-7B (Touvron et al.,, [2023](#bib.bib44))
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama2-7B (Touvron et al., [2023](#bib.bib44))
- en: 'Figure 3: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks. Quip# (Tseng et al.,, [2024](#bib.bib46)) does not take advantage of the
    structures in the first blocks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：所有块中“Q”类型层的重建（Frobenius 范数）误差。Quip# (Tseng et al., [2024](#bib.bib46)) 未利用前几个块中的结构。
- en: 'In [Figure 3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general
    framework for LLM compression and fine-tuning") ReALLM achieves the lowest Frobenius
    norm quantization error. We perform ablation experiments with this metric to decouple
    the effects of VQ and permutation preprocessing of ReALLM on the final performance.
    For example, in block $8$, while permutation alone (i.e. with SQ) leads to an
    error of $2.88$. Quip# rotates the matrices randomly, causing all patterns in
    the initial blocks to be lost.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在[图3](#S3.F3 "In ReALLM: a new LLM format. ‣ 3 Method ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")中，ReALLM 实现了最低的 Frobenius 范数量化误差。我们用这个指标进行消融实验，以解耦
    VQ 和 ReALLM 的置换预处理对最终性能的影响。例如，在块 $8$ 中，仅置换（即带有 SQ）导致误差为 $2.88$。Quip# 随机旋转矩阵，导致初始块中的所有模式丢失。'
- en: Input : Number of end-to-end fine-tuning steps $T$), Number of weights in the
    decoder $c$, Rank $r$* do7       $B_{j}=\{W^{q},W^{k},W^{v},W^{o},W^{gate},W^{up},W^{down}\}[block=j]$
    ;11             $W^{l}_{j}=W^{l}_{j}-L1^{l}_{j}(L2^{l}_{j})^{t}$ /* with NF-normalization
    ([Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18)) */12            
    $codebook^{l}_{j}=Kmeans({\mathcal{E}_{\psi}}(W^{l}_{j}),b,d)$;14                        $dora^{l}_{j}=DoRA(W^{l}_{j},L1^{l}_{j},L2^{l}_{j})$* do19            
    Optimize $\{dora^{l}_{j},L1^{l}_{j},L2^{l}_{j}\}_{l\geq 0}$ with gradient descent
    ;26      27 end for
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '输入 : 端到端微调步骤数 $T$)，解码器中的权重数量 $c$，秩 $r$* do7 $B_{j}=\{W^{q},W^{k},W^{v},W^{o},W^{gate},W^{up},W^{down}\}[block=j]$
    ;11 $W^{l}_{j}=W^{l}_{j}-L1^{l}_{j}(L2^{l}_{j})^{t}$ /* 使用 NF-归一化 ([Dettmers et
    al., 2023a,](#bib.bib11) ; Guo et al., [2023](#bib.bib18)) */12 $codebook^{l}_{j}=Kmeans({\mathcal{E}_{\psi}}(W^{l}_{j}),b,d)$;14
    $dora^{l}_{j}=DoRA(W^{l}_{j},L1^{l}_{j},L2^{l}_{j})$* do19 优化 $\{dora^{l}_{j},L1^{l}_{j},L2^{l}_{j}\}_{l\geq
    0}$ 使用梯度下降 ;26 27 end for'
- en: Algorithm 2 Pseudo-code for ReALLM with block-wise and end-to-end fine-tuning
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 ReALLM 的伪代码，带有块级和端到端微调。
- en: 4 Experimental validation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验验证
- en: We test ReALLM on the LLaMA-2 (Touvron et al.,, [2023](#bib.bib44)) family models
    (with $7$ bits per coordinate. We partially reused code from the implementations
    of LQ-LoRA¹¹1https://github.com/HanGuo97/lq-lora/tree/main, AQLM ²²2https://github.com/Vahe1994/AQLM
    and HNeRV³³3https://github.com/haochen-rye/HNeRV. On an Nvidia A40 GPU (with $46$
    hours for a LLaMA2-7B model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LLaMA-2 (Touvron et al., [2023](#bib.bib44)) 家族模型上测试 ReALLM（每个坐标 $7$ 位）。我们部分重用了
    LQ-LoRA¹¹1https://github.com/HanGuo97/lq-lora/tree/main、AQLM ²²2https://github.com/Vahe1994/AQLM
    和 HNeRV³³3https://github.com/haochen-rye/HNeRV 的实现代码。在 Nvidia A40 GPU 上（LLaMA2-7B
    模型需要 $46$ 小时）。
- en: Language Generation Tasks.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言生成任务。
- en: For continual language modeling, we train on a single partition of the C4 (Raffel
    et al.,, [2020](#bib.bib37)) dataset for half an epoch and use a sequence length
    of $4096$ (all except (Egiazarian et al.,, [2024](#bib.bib13)) follow this rule).
    Therefore, we use a sequence length of size $2048$ for both WikiText-2 (Merity
    et al.,, [2016](#bib.bib34)) and C4 evaluation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续语言建模，我们在 C4 (Raffel et al., [2020](#bib.bib37)) 数据集的单一分区上训练半个周期，并使用 $4096$
    的序列长度（除 (Egiazarian et al., [2024](#bib.bib13)) 外，所有遵循此规则）。因此，我们对 WikiText-2 (Merity
    et al., [2016](#bib.bib34)) 和 C4 评估使用 $2048$ 的序列长度。
- en: 'Our main baselines are LQ-LoRA (Guo et al.,, [2023](#bib.bib18)), Quip# (Tseng
    et al.,, [2024](#bib.bib46)), and AQLM (Egiazarian et al.,, [2024](#bib.bib13)).
    However, we also report the performance of popular quantization approaches GPTQ
    (Frantar et al.,, [2022](#bib.bib14)), AWQ (Lin et al.,, [2023](#bib.bib29)),
    Omniquant (Shao et al.,, [2023](#bib.bib39)), as well as the performance of recent
    work ApiQ (Liao and Monz,, [2024](#bib.bib28)) and QuaRot (Ashkboos et al.,, [2024](#bib.bib4)).
    In the results below, we present the target bits per parameter that takes into
    account quantized weights and include parameters kept in high precision (head
    layer, scales, codebooks, permutations in $16$ bits precision) similarly to the
    related work. The exact bit budget is detailed in [Table 5](#A1.T5 "In A.2 Autoencoder
    computational limitations ‣ Appendix A Appendix / supplemental material ‣ ReALLM:
    A general framework for LLM compression and fine-tuning") in the Appendix.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要基准是 LQ-LoRA (Guo et al., [2023](#bib.bib18))、Quip# (Tseng et al., [2024](#bib.bib46))
    和 AQLM (Egiazarian et al., [2024](#bib.bib13))。不过，我们还报告了流行的量化方法的性能，如 GPTQ (Frantar
    et al., [2022](#bib.bib14))、AWQ (Lin et al., [2023](#bib.bib29))、Omniquant (Shao
    et al., [2023](#bib.bib39))，以及最近的工作 ApiQ (Liao and Monz, [2024](#bib.bib28)) 和
    QuaRot (Ashkboos et al., [2024](#bib.bib4))。在下面的结果中，我们展示了每个参数的目标位数，考虑了量化权重，并包括保持高精度的参数（头层、缩放、代码本、$16$
    位精度的排列），类似于相关工作。确切的位预算详见附录中的[表 5](#A1.T5 "在 A.2 自编码器计算限制 ‣ 附录 A 附录 / 补充材料 ‣ ReALLM：LLM
    压缩和微调的一般框架")。
- en: In our experiments, following [Dettmers et al., 2023a](#bib.bib11) ; Guo et al.,
    ([2023](#bib.bib18)), we take a DoRA (Liu et al.,, [2024](#bib.bib31)) rank of
    $r=64$ to $(16,16,16)$ for $2000$, and a learning rate of $2\cdot e^{-5}$. As
    far as we know, we have also developed the first VQ code (available in the supplementary
    material) that makes efficient use of PyTorch’s “torch dispatch” functionality
    (Ansel et al.,, [2024](#bib.bib2)), which is known to be as fast as dedicated
    CUDA kernels (Guo et al.,, [2023](#bib.bib18)). This allows us to overload PyTorch
    operations to perform just-in-time dequantization.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，按照[Dettmers et al., 2023a](#bib.bib11)；Guo et al., ([2023](#bib.bib18))的做法，我们采用DoRA
    (Liu et al.,, [2024](#bib.bib31))排名$r=64$，设置为$(16,16,16)$，训练$2000$步，学习率为$2\cdot
    e^{-5}$。据我们了解，我们还开发了第一个VQ代码（见补充材料），有效利用了PyTorch的“torch dispatch”功能（Ansel et al.,,
    [2024](#bib.bib2)），其速度与专用CUDA内核（Guo et al.,, [2023](#bib.bib18)）相当。这使我们能够重载PyTorch操作以进行即时去量化。
- en: 'In [Tables 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") and [2](#S4.T2
    "Table 2 ‣ Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A
    general framework for LLM compression and fine-tuning") we evaluate the perplexity
    of ReALLM on the respective validation datasets of C4 and WikiText-2 for a single
    run. During fine-tuning (on a single partition of the C4 dataset), we only update
    the DoRA components (scales and low-rank matrices). For each dataset, we provide
    three sets of results in [Table 1](#S4.T1 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"):
    Perplexity without any fine-tuning (only low-rank and VQ autoencoder decomposition),
    perplexity with only block-wise fine-tuning, and perplexities with end-to-end
    fine-tuning (in addition to the block-wise fine-tuning process).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在[表1](#S4.T1 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")和[2](#S4.T2 "表2
    ‣ 语言生成任务。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")中，我们评估了ReALLM在C4和WikiText-2的验证数据集上的困惑度。微调过程中（在C4数据集的单一分区上），我们只更新DoRA组件（尺度和低秩矩阵）。对于每个数据集，我们在[表1](#S4.T1
    "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")中提供了三组结果：没有任何微调（仅低秩和VQ自编码器分解）的困惑度，仅块状微调的困惑度，以及端到端微调（加上块状微调过程）的困惑度。'
- en: 'Table 1: Perplexity $(\downarrow)$'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 排名 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $6.21$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ | $6.21$ |'
- en: '| ReALLM (block-wise) | $3$ | $6.01$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状训练) | $3$ | $6.01$ |'
- en: '| ReALLM (40% training) | $3$ | $5.80$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ | $5.80$ |'
- en: '| ReALLM (full training) | $3$ | $5.79$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $3$ | $5.79$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $6.10$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $3$ | $6.10$ |'
- en: '| ReALLM (block-wise) | $3$ | $5.92$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状) | $3$ | $5.92$ |'
- en: '| ReALLM (40% training) | $3$ | $5.78$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $3$ | $5.78$ |'
- en: '| ReALLM (full training) | $3$ | $5.77$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $3$ | $5.77$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $51.74$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ | $51.74$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ | $16.95$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状50轮) | $2$ | $16.95$ |'
- en: '| ReALLM (block-wise 200 epochs) | $2$ | $8.31$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状200轮) | $2$ | $8.31$ |'
- en: '| ReALLM (40% training) | $2$ | $6.95$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ | $6.95$ |'
- en: '| ReALLM (full training) | $2$ | $6.91$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $2$ | $6.91$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $40.85$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (无微调) | $2$ | $40.85$ |'
- en: '| ReALLM (block-wise 50 epochs) | $2$ | 15.74 | 12.08 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (块状50轮) | $2$ | 15.74 | 12.08 |'
- en: '| ReALLM (40% training) | $2$ | $6.74$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (40% 训练) | $2$ | $6.74$ |'
- en: '| ReALLM (full training) | $2$ | $6.69$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM (完全训练) | $2$ | $6.69$ |'
- en: 'Our *data-free* version of ReALLM (no fine-tuning; see [Table 1](#S4.T1 "In
    Language Generation Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework
    for LLM compression and fine-tuning")) achieves state-of-the-art metrics for $3$
    has minimal effect on the final perplexity result, while halving the number of
    parameters that need to be tuned. Moreover, a larger VQ dimension $d=4$ bits are
    needed to store the codebook). Additional results for other models are available
    in the Appendix.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的*无数据*版本ReALLM（无微调；见[表1](#S4.T1 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用框架用于LLM压缩和微调")）在$3$的情况下达到了最先进的指标，对最终困惑度结果的影响极小，同时将需要调整的参数数量减少了一半。此外，存储代码本需要更大的VQ维度$d=4$位。其他模型的附加结果见附录。'
- en: 'Table 2: Perplexity $(\downarrow)$'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：困惑度 $(\downarrow)$
- en: '| Method | Number of bits | C4 $(\downarrow)$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | C4 $(\downarrow)$ |'
- en: '|  |  | 7B | 13B | 7B | 13B |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 7B | 13B | 7B | 13B |'
- en: '| LLaMA2 (Touvron et al.,, [2023](#bib.bib44)) | $16$ | $4.48$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 (Touvron et al., [2023](#bib.bib44)) | $16$ | $4.48$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $3$ | $5.42$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ (Frantar et al., [2022](#bib.bib14)) | $3$ | $5.42$ |'
- en: '| AWQ (Lin et al.,, [2023](#bib.bib29)) | $3$ | $5.32$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AWQ (Lin et al., [2023](#bib.bib29)) | $3$ | $5.32$ |'
- en: '| Omniquant (Shao et al.,, [2023](#bib.bib39)) | $3$ | $5.28$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant (Shao et al., [2023](#bib.bib39)) | $3$ | $5.28$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $3$ | $-$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA (Guo et al., [2023](#bib.bib18)) | $3$ | $-$ |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $3$ | $5.13$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li et al., [2023](#bib.bib27)) | $3$ | $5.13$ |'
- en: '| ApiQ[PTQ] (Liao and Monz,, [2024](#bib.bib28)) | $3$ | $5.18$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ[PTQ] (Liao and Monz, [2024](#bib.bib28)) | $3$ | $5.18$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $3$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | $3$ |'
- en: '| QuaRot[A16W3] (Ashkboos et al.,, [2024](#bib.bib4)) | $3$ | $5.37$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot[A16W3] (Ashkboos et al., [2024](#bib.bib4)) | $3$ | $5.37$ |'
- en: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $3$ | 7.27 | 6.69 | 5.77 | 5.14 |'
- en: '| LoftQ (Li et al.,, [2023](#bib.bib27)) | $2$ | $7.69$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ (Li et al., [2023](#bib.bib27)) | $2$ | $7.69$ |'
- en: '| ApiQ (Liao and Monz,, [2024](#bib.bib28)) | $2$ | $6.29$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ApiQ (Liao and Monz, [2024](#bib.bib28)) | $2$ | $6.29$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $2$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | $2$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | $2$ | 6.64 | 5.65 |'
- en: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $2$ | 8.28 | 7.50 | 6.69 | 5.72 |'
- en: 'In [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental validation
    ‣ ReALLM: A general framework for LLM compression and fine-tuning") we compare
    ReALLM with end-to-end fine-tuning, and the best performing PTQ approaches. All
    the methods cited in [Table 2](#S4.T2 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning")
    also uses a calibration dataset. It is interesting to note that ReALLM with $2$
    bits precision) during the layer-wise fine-tuning. This does not only slow down
    the PTQ process (as gradients must be store for all weights in the given block),
    but it also means Quip# has to store learnable vectors and also quantized weights
    for *each* fine-tuning task.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [表 2](#S4.T2 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用的 LLM 压缩与微调框架") 中，我们将 ReALLM
    与端到端微调以及表现最佳的 PTQ 方法进行了比较。[表 2](#S4.T2 "在语言生成任务中。 ‣ 4 实验验证 ‣ ReALLM: 一种通用的 LLM
    压缩与微调框架") 中引用的所有方法也使用了校准数据集。有趣的是，ReALLM 在层级微调过程中使用 $2$ 位精度。这不仅会减慢 PTQ 过程（因为梯度必须存储在给定块中的所有权重中），还意味着
    Quip# 必须为*每个*微调任务存储可学习的向量和量化的权重。'
- en: 'Table 3: Accuracy $(\uparrow)$ in LM Eval (acc, not acc_norm).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LM Eval 中的准确率 $(\uparrow)$（acc，而非 acc_norm）。'
- en: '| Method | Size | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | Average
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 大小 | #bits | ARC-challenge | ARC-easy | PiQA | Winogrande | 平均值 |'
- en: '| LLaMA-2 | 7B | $16$ | $69.22$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 7B | $16$ | $69.22$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 7B | $2$ | $64.61$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | 7B | $2$ | $64.61$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 7B | $2$ | $64.89$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | 7B | $2$ | $64.89$ |'
- en: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 7B | $2$ | 35.15 | 68.56 | 75.73 | 66.46 | 61.47 |'
- en: '| LLaMA-2 | 13B | $16$ | $72.13$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2 | 13B | $16$ | $72.13$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | 13B | $3$ | $67.56$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AQLM (Egiazarian et al., [2024](#bib.bib13)) | 13B | $3$ | $67.56$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | 13B | $3$ | $69.13$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Quip# (Tseng et al., [2024](#bib.bib46)) | 13B | $3$ | $69.13$ |'
- en: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | 13B | $3$ | 47.01 | 75.96 | 78.67 | 70.96 | 68.15 |'
- en: Zero-Shot Tasks.
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Zero-Shot 任务。
- en: 'Following HuggingFace’s Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,
    and the literature (Frantar et al.,, [2022](#bib.bib14); Guo et al.,, [2023](#bib.bib18)),
    we also measure zero-shot accuracy on ARC (Clark et al.,, [2018](#bib.bib7)),
    PiQA (Tata and Patel,, [2003](#bib.bib42)), and Winogrande (Sakaguchi et al.,,
    [2021](#bib.bib38)), via the LM Evalaluation Harness (Gao et al.,, [2021](#bib.bib15)).
    We report results in [Table 3](#S4.T3 "In Language Generation Tasks. ‣ 4 Experimental
    validation ‣ ReALLM: A general framework for LLM compression and fine-tuning"),
    and compute the average on the 4 mentioned tasks. For all LLM sizes, ReALLM provides
    a notable advantage (between $0.5$ bits) on the zero-shot tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '根据HuggingFace的Open LLM Leaderboard⁴⁴4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard以及文献（Frantar等人，[2022](#bib.bib14)；Guo等人，[2023](#bib.bib18)），我们还测量了ARC（Clark等人，[2018](#bib.bib7)）、PiQA（Tata和Patel，[2003](#bib.bib42)）和Winogrande（Sakaguchi等人，[2021](#bib.bib38)）的零样本准确率，通过LM
    Evalaluation Harness（Gao等人，[2021](#bib.bib15)）。我们在[表3](#S4.T3 "In Language Generation
    Tasks. ‣ 4 Experimental validation ‣ ReALLM: A general framework for LLM compression
    and fine-tuning")中报告结果，并计算了4项任务的平均值。对于所有LLM尺寸，ReALLM在零样本任务上提供了显著优势（在$0.5$位之间）。'
- en: 5 Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present ReALLM, a weight-only PTQ method that achieves state-of-the-art results
    on LLMs at $2$ GB of RAM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了ReALLM，这是一种仅限权重的PTQ方法，在$2$ GB RAM的LLMs上实现了最先进的结果。
- en: Large context sequence lengths result in large $KV$ matrices. We are currently
    studying how to adapt ReALLM to $KV$-cache quantization, and how to combine it
    with activation quantization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 大上下文序列长度会导致较大的$KV$矩阵。我们目前正在研究如何将ReALLM适应$KV$-缓存量化，并如何将其与激活量化结合。
- en: 6 Societal impact
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 社会影响
- en: This paper presents work whose goal is to advance the field of LLM compression
    and fine-tuning. There are many potential societal consequences of our work, in
    particular malicious usage of LLMs for spams or language generation on edge devices.
    However, this negative societal impact is not limited to ReALLM, but to the field
    of LLM in general.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了旨在推动LLM压缩和微调领域的工作。我们的工作有许多潜在的社会影响，特别是LLM在垃圾邮件或边缘设备语言生成中的恶意使用。然而，这种负面的社会影响不仅限于ReALLM，而是对LLM领域的普遍现象。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almazrouei et al., (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli,
    A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic,
    Q., et al. (2023). The falcon series of open language models. arXiv preprint arXiv:2311.16867.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei等人，（2023）Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A.,
    Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q.,
    等人。（2023）。Falcon系列开放语言模型。arXiv预印本arXiv:2311.16867。
- en: 'Ansel et al., (2024) Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A.,
    Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia,
    A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J.,
    Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano,
    M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso,
    M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang,
    X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G.,
    Wu, P., and Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic
    Python Bytecode Transformation and Graph Compilation. In 29th ACM International
    Conference on Architectural Support for Programming Languages and Operating Systems,
    Volume 2 (ASPLOS ’24). ACM.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansel等人，（2024）Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky,
    M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable,
    W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M.,
    Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang,
    Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim,
    M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W.,
    Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., 和Chintala,
    S.（2024）。PyTorch 2：通过动态Python字节码转换和图形编译加速机器学习。见第29届ACM国际编程语言和操作系统体系结构支持会议，第2卷（ASPLOS
    ’24）。ACM。
- en: 'Arthur et al., (2007) Arthur, D., Vassilvitskii, S., et al. (2007). k-means++:
    The advantages of careful seeding. In Soda, volume 7, pages 1027–1035.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arthur等人，（2007）Arthur, D., Vassilvitskii, S., 等人。（2007）。k-means++：精心初始化的优势。见Soda，第7卷，第1027–1035页。
- en: 'Ashkboos et al., (2024) Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B.,
    Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. (2024). Quarot: Outlier-free
    4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos等人，（2024）Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Jaggi,
    M., Alistarh, D., Hoefler, T., 和Hensman, J.（2024）。Quarot：在旋转LLMs中进行无异常4位推理。arXiv预印本arXiv:2404.00456。
- en: Bengio, (2013) Bengio, Y. (2013). Estimating or propagating gradients through
    stochastic neurons. arXiv preprint arXiv:1305.2982.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio (2013) Bengio, Y. (2013). 通过随机神经元估计或传播梯度。arXiv 预印本 arXiv:1305.2982。
- en: 'Chen et al., (2023) Chen, H., Gwilliam, M., Lim, S.-N., and Shrivastava, A.
    (2023). Hnerv: A hybrid neural representation for videos. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10270–10279.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023) Chen, H., Gwilliam, M., Lim, S.-N., 和 Shrivastava, A. (2023).
    Hnerv: 一种用于视频的混合神经表示。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，第 10270–10279 页。'
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O. (2018). 觉得你已经解决了问答问题？试试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457。
- en: 'Courbariaux et al., (2015) Courbariaux, M., Bengio, Y., and David, J.-P. (2015).
    Binaryconnect: Training deep neural networks with binary weights during propagations.
    Advances in neural information processing systems, 28.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Courbariaux 等人 (2015) Courbariaux, M., Bengio, Y., 和 David, J.-P. (2015). Binaryconnect:
    在传播过程中训练具有二进制权重的深度神经网络。神经信息处理系统进展，28。'
- en: Dao et al., (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., and Ré, C. (2019).
    Learning fast algorithms for linear transforms using butterfly factorizations.
    In International conference on machine learning, pages 1517–1527\. PMLR.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人 (2019) Dao, T., Gu, A., Eichhorn, M., Rudra, A., 和 Ré, C. (2019). 使用蝶形分解学习快速算法进行线性变换。在国际机器学习会议上，第
    1517–1527 页。PMLR。
- en: 'Dettmers et al., (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. (2022). Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale.
    Advances in Neural Information Processing Systems, 35:30318–30332.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L.
    (2022). Gpt3\. int8 (): 用于变换器的 8 位矩阵乘法。神经信息处理系统进展，35:30318–30332。'
- en: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. (2023a).
    Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(11) Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer, L. (2023a). Qlora:
    量化 LLM 的高效微调。神经信息处理系统进展，36。'
- en: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. (2023b). Spqr: A
    sparse-quantized representation for near-lossless llm weight compression. In The
    Twelfth International Conference on Learning Representations.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(12) Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar,
    E., Ashkboos, S., Borzunov, A., Hoefler, T., 和 Alistarh, D. (2023b). Spqr: 一种用于近乎无损的
    LLM 权重压缩的稀疏量化表示。在第十二届国际学习表示大会上。'
- en: Egiazarian et al., (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., and Alistarh, D. (2024). Extreme compression of large language
    models via additive quantization. arXiv preprint arXiv:2401.06118.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人 (2024) Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar,
    E., Babenko, A., 和 Alistarh, D. (2024). 通过加法量化对大型语言模型进行极限压缩。arXiv 预印本 arXiv:2401.06118。
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. (2022).
    Gptq: 生成预训练变换器的准确后训练量化。arXiv 预印本 arXiv:2210.17323。'
- en: Gao et al., (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for
    few-shot language model evaluation.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2021) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. (2021). 少样本语言模型评估框架。
- en: Gholami et al., (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. (2022). A survey of quantization methods for efficient neural
    network inference. In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami 等人 (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., 和
    Keutzer, K. (2022). 高效神经网络推断的量化方法调查。在《低功耗计算机视觉》中，第 291–326 页。Chapman and Hall/CRC。
- en: 'Guo et al., (2021) Guo, D., Rush, A. M., and Kim, Y. (2021). Parameter-efficient
    transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等, (2021) Guo, D., Rush, A. M., 和 Kim, Y. (2021). 具有差异修剪的参数高效迁移学习。见于第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集，第4884–4896页。
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E., and Kim, Y. (2023). Lq-lora:
    Low-rank plus quantized matrix decomposition for efficient language model finetuning.
    In The Twelfth International Conference on Learning Representations.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等, (2023) Guo, H., Greengard, P., Xing, E., 和 Kim, Y. (2023). Lq-lora:
    低秩加量化矩阵分解用于高效语言模型微调。第十二届国际学习表征会议。'
- en: Han et al., (2015) Han, S., Pool, J., Tran, J., and Dally, W. (2015). Learning
    both weights and connections for efficient neural network. Advances in neural
    information processing systems, 28.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等, (2015) Han, S., Pool, J., Tran, J., 和 Dally, W. (2015). 为高效神经网络学习权重和连接。神经信息处理系统进展,
    28。
- en: 'Hooper et al., (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W.,
    Shao, Y. S., Keutzer, K., and Gholami, A. (2024). Kvquant: Towards 10 million
    context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper 等, (2024) Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao,
    Y. S., Keutzer, K., 和 Gholami, A. (2024). Kvquant: 通过KV缓存量化实现1000万上下文长度的LLM推断。arXiv
    预印本 arXiv:2401.18079。'
- en: 'Hu et al., (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., et al. (2021). Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等, (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W., 等. (2021). Lora: 大语言模型的低秩适配。国际学习表征会议。'
- en: Jiang et al., (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. (2023). Mistral 7b. arXiv preprint arXiv:2310.06825.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等, (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等.
    (2023). Mistral 7b。arXiv 预印本 arXiv:2310.06825。
- en: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., and Lee,
    D. (2023a). Memory-efficient fine-tuning of compressed large language models via
    sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Kim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon, S. J., 和 Lee,
    D. (2023a). 通过子4位整数量化的压缩大语言模型的内存高效微调。arXiv 预印本 arXiv:2305.14152。
- en: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., and Keutzer, K. (2023b). Squeezellm: Dense-and-sparse quantization. arXiv
    preprint arXiv:2306.07629.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., 和 Keutzer, K. (2023b). Squeezellm: 密集与稀疏量化。arXiv 预印本 arXiv:2306.07629。'
- en: 'Kwan et al., (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., and Bull, D.
    (2024). Hinerv: Video compression with hierarchical encoding-based neural representation.
    Advances in Neural Information Processing Systems, 36.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwan 等, (2024) Kwan, H. M., Gao, G., Zhang, F., Gower, A., 和 Bull, D. (2024).
    Hinerv: 基于层次编码的神经表示视频压缩。神经信息处理系统进展, 36。'
- en: 'Li and Liang, (2021) Li, X. L. and Liang, P. (2021). Prefix-tuning: Optimizing
    continuous prompts for generation. In Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang, (2021) Li, X. L. 和 Liang, P. (2021). Prefix-tuning: 优化生成的连续提示。见于第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集，第4582–4597页。'
- en: 'Li et al., (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen,
    W., and Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large
    language models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等, (2023) Li, Y., Yu, Y., Liang, C., Karampatziakis, N., He, P., Chen, W.,
    和 Zhao, T. (2023). Loftq: 适用于大语言模型的Lora微调感知量化。第十二届国际学习表征会议。'
- en: 'Liao and Monz, (2024) Liao, B. and Monz, C. (2024). Apiq: Finetuning of 2-bit
    quantized large language model. arXiv preprint arXiv:2402.05147.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao 和 Monz, (2024) Liao, B. 和 Monz, C. (2024). Apiq: 2位量化大语言模型的微调。arXiv 预印本
    arXiv:2402.05147。'
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等, (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. (2023).
    Awq: 激活感知权重量化用于LLM压缩和加速。arXiv 预印本 arXiv:2306.00978。'
- en: 'Liu et al., (2023) Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., and Zhuang,
    B. (2023). Qllm: Accurate and efficient low-bitwidth quantization for large language
    models. In The Twelfth International Conference on Learning Representations.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Liu, J., Gong, R., Wei, X., Dong, Z., Cai, J., 和 Zhuang, B.（2023）。Qllm：针对大型语言模型的准确高效低位宽量化。在第十二届国际学习表示大会上。
- en: 'Liu et al., (2024) Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., and Chen, M.-H. (2024). Dora: Weight-decomposed low-rank adaptation.
    arXiv preprint arXiv:2402.09353.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F.,
    Cheng, K.-T., 和 Chen, M.-H.（2024）。Dora：权重分解的低秩适应。arXiv 预印本 arXiv:2402.09353。
- en: Liu et al., (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell,
    T., and Xie, S. (2022). A convnet for the 2020s. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 11976–11986.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., 和 Xie,
    S.（2022）。2020 年代的卷积网络。在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页面 11976–11986。
- en: 'Liu et al., (2020) Liu, Z., Shen, Z., Savvides, M., and Cheng, K.-T. (2020).
    Reactnet: Towards precise binary neural network with generalized activation functions.
    In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, pages 143–159\. Springer.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）Liu, Z., Shen, Z., Savvides, M., 和 Cheng, K.-T.（2020）。Reactnet：朝着具有广义激活函数的精确二进制神经网络前进。在计算机视觉–ECCV
    2020：第 16 届欧洲会议，英国格拉斯哥，2020 年 8 月 23–28 日，论文集，第 XIV 部分，第 16 页，页面 143–159。Springer。
- en: Merity et al., (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016).
    Pointer sentinel mixture models. In International Conference on Learning Representations.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等（2016）Merity, S., Xiong, C., Bradbury, J., 和 Socher, R.（2016）。指针哨兵混合模型。在国际学习表示大会上。
- en: Nrusimha et al., (2024) Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda,
    R., and Kim, Y. (2024). Mitigating the impact of outlier channels for language
    model quantization with activation regularization. arXiv preprint arXiv:2404.03605.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nrusimha 等（2024）Nrusimha, A., Mishra, M., Wang, N., Alistarh, D., Panda, R.,
    和 Kim, Y.（2024）。通过激活正则化缓解离群通道对语言模型量化的影响。arXiv 预印本 arXiv:2404.03605。
- en: 'Park et al., (2022) Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B.,
    Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). Lut-gemm: Quantized matrix
    multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2022）Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B., Kwon, S.
    J., Kim, B., Lee, Y., 和 Lee, D.（2022）。Lut-gemm：基于 LUT 的量化矩阵乘法，用于大规模生成语言模型的高效推理。arXiv
    预印本 arXiv:2206.09557。
- en: Raffel et al., (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits
    of transfer learning with a unified text-to-text transformer. Journal of machine
    learning research, 21(140):1–67.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J.（2020）。使用统一的文本到文本变换器探索迁移学习的极限。《机器学习研究期刊》，21(140)：1–67。
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2021）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y.（2021）。Winogrande：大规模对抗性
    Winograd 模式挑战。《ACM 通讯》，64(9)：99–106。
- en: 'Shao et al., (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2023). Omniquant: Omnidirectionally
    calibrated quantization for large language models. In The Twelfth International
    Conference on Learning Representations.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等（2023）Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang,
    K., Gao, P., Qiao, Y., 和 Luo, P.（2023）。Omniquant：大型语言模型的全方位标定量化。在第十二届国际学习表示大会上。
- en: Shi et al., (2016) Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P.,
    Bishop, R., Rueckert, D., and Wang, Z. (2016). Real-time single image and video
    super-resolution using an efficient sub-pixel convolutional neural network. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1874–1883.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2016）Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P., Bishop,
    R., Rueckert, D., 和 Wang, Z.（2016）。使用高效的亚像素卷积神经网络进行实时单图像和视频超分辨率。在 IEEE 计算机视觉与模式识别会议论文集中，页面
    1874–1883。
- en: Soro et al., (2024) Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., and
    Hwang, S. J. (2024). Diffusion-based neural network weights generation. arXiv
    preprint arXiv:2402.18153.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soro 等（2024）Soro, B., Andreis, B., Lee, H., Chong, S., Hutter, F., 和 Hwang,
    S. J.（2024）。基于扩散的神经网络权重生成。arXiv 预印本 arXiv:2402.18153。
- en: 'Tata and Patel, (2003) Tata, S. and Patel, J. M. (2003). Piqa: An algebra for
    querying protein data sets. In 15th International Conference on Scientific and
    Statistical Database Management, 2003., pages 141–150\. IEEE.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tata和Patel，（2003）Tata, S. 和 Patel, J. M.（2003）。Piqa: 查询蛋白质数据集的代数。第15届国际科学与统计数据库管理会议，2003，第141–150页。IEEE。'
- en: 'Team et al., (2024) Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju,
    S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. (2024).
    Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team等人，（2024）Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S.,
    Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., 等。（2024）。Gemma: 基于双子座研究和技术的开放模型。arXiv预印本
    arXiv:2403.08295。'
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人，（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。（2023）。Llama:
    开放而高效的基础语言模型。arXiv预印本 arXiv:2302.13971。'
- en: Trukhanov and Soloveychik, (2024) Trukhanov, N. and Soloveychik, I. (2024).
    Accurate block quantization in llms with outliers. arXiv preprint arXiv:2403.20137.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trukhanov和Soloveychik，（2024）Trukhanov, N. 和 Soloveychik, I.（2024）。带异常值的LLM中准确的块量化。arXiv预印本
    arXiv:2403.20137。
- en: 'Tseng et al., (2024) Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa,
    C. (2024). Quip#: Even better llm quantization with hadamard incoherence and lattice
    codebooks. arXiv preprint arXiv:2402.04396.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng等人，（2024）Tseng, A., Chee, J., Sun, Q., Kuleshov, V., 和 De Sa, C.（2024）。Quip#:
    通过Hadamard不相干性和晶格代码本改进的llm量化。arXiv预印本 arXiv:2402.04396。'
- en: Van Den Oord et al., (2017) Van Den Oord, A., Vinyals, O., et al. (2017). Neural
    discrete representation learning. Advances in neural information processing systems,
    30.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Den Oord等人，（2017）Van Den Oord, A., Vinyals, O., 等。（2017）。神经离散表示学习。神经信息处理系统进展，30。
- en: Vaswani et al., (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人，（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I.（2017）。注意力机制就是你所需的一切。神经信息处理系统进展，30。
- en: Viazovska, (2017) Viazovska, M. S. (2017). The sphere packing problem in dimension
    8. Annals of mathematics, pages 991–1015.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viazovska，（2017）Viazovska, M. S.（2017）。维度8的球体堆积问题。数学年刊，第991–1015页。
- en: Wang et al., (2024) Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu,
    Z., and You, Y. (2024). Neural network diffusion. arXiv preprint arXiv:2402.13144.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人，（2024）Wang, K., Xu, Z., Zhou, Y., Zang, Z., Darrell, T., Liu, Z., 和 You,
    Y.（2024）。神经网络扩散。arXiv预印本 arXiv:2402.13144。
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人，（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.（2023）。Smoothquant:
    大型语言模型的准确且高效的后训练量化。在国际机器学习会议上，第38087–38099页。PMLR。'
- en: 'Yao et al., (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.,
    and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人，（2022）Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., 和 He,
    Y.（2022）。Zeroquant: 大规模变换器的高效且经济的后训练量化。神经信息处理系统进展，35:27168–27183。'
- en: Appendix A Appendix / supplemental material
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录/补充材料
- en: A.1 Structures in pre-trained matrices
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 预训练矩阵中的结构
- en: 'Interestingly, the blocks that show some visual structures in LLaMA and Mistral
    models are not the same for Gemma LLMs. For instance in [Figure 4](#A1.F4 "In
    A.1 Structures in pre-trained matrices ‣ Appendix A Appendix / supplemental material
    ‣ ReALLM: A general framework for LLM compression and fine-tuning"), we can see
    that Gemma2b (Team et al.,, [2024](#bib.bib43))’s matrices keep some internal
    patterns in all blocks, not only at the very first blocks. Note this has no negative
    impact on ReALLM, as the shape of the encoder is experimentally adapted to each
    block.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，LLaMA和Mistral模型中显示一些视觉结构的块与Gemma LLMs不同。例如在[图4](#A1.F4 "在A.1 预训练矩阵中的结构
    ‣ 附录A附加材料 ‣ ReALLM: 一般的LLM压缩和微调框架")中，我们可以看到Gemma2b（Team等人，[2024](#bib.bib43)）的矩阵在所有块中保留了一些内部模式，而不仅仅是在最初的块中。请注意，这对ReALLM没有负面影响，因为编码器的形状会根据每个块进行实验性适配。'
- en: '![Refer to caption](img/bf9fe57040bc062c570f73b31d6925da.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bf9fe57040bc062c570f73b31d6925da.png)'
- en: 'Figure 4: Reconstruction (Frobenius norm) error for layer of type “Q” for all
    blocks of Gemma2b LLM.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Gemma2b LLM 所有块的“Q”类型层的重构（Frobenius 范数）误差。
- en: A.2 Autoencoder computational limitations
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 自编码器计算限制
- en: Our GPU can not directly work on LLM pre-trained matrices with large sizes (typically
    of shape $4096\times 4096$ bits using straight through estimator Bengio, ([2013](#bib.bib5)).
    We also tested a post training quantization method where the weight of the decoder
    are quantized with a round to nearest (RTN) approache, at the end of the decoder
    training steps.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GPU 不能直接处理具有大尺寸的 LLM 预训练矩阵（通常为 $4096\times 4096$ 位，使用直通估计器 Bengio，[2013](#bib.bib5)）。我们还测试了一种后训练量化方法，其中解码器的权重在解码器训练步骤结束时采用四舍五入到最近（RTN）的方法进行量化。
- en: 'Table 4: Reconstruction (Frobenius norm) error for layer of type “Q” inside
    the first block of Mistral-7b model, for patches of size $512\times 512$, and
    a varying quantization strategy (during the decoder training, i.e. QAT, or after
    the training, i.e. PTQ).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Mistral-7b 模型中第一块内的“Q”类型层的重构（Frobenius 范数）误差，适用于大小为 $512\times 512$ 的补丁，以及不同的量化策略（在解码器训练期间，即
    QAT，或在训练后，即 PTQ）。
- en: '| Error | # parameters c ($\times 10^{6}$ | bit budget | quantization |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 错误 | 参数数量 c ($\times 10^{6}$ | 位预算 | 量化 |'
- en: '| $0.84$ | NF3(Guo et al.,, [2023](#bib.bib18)) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $0.84$ | NF3（Guo 等，[2023](#bib.bib18)） |'
- en: '| $1.78$ | PTQ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $1.78$ | PTQ |'
- en: '| $1.19$ | PTQ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| $1.19$ | PTQ |'
- en: '| $1.61$ | QAT |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $1.61$ | QAT |'
- en: '| $1.24$ | QAT |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $1.24$ | QAT |'
- en: '| $0.69$ | QAT |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| $0.69$ | QAT |'
- en: We vary the number of parameters $c$ and their respective bit precision $b_{\phi}$),
    ReALLM yields a smaller quantization error compared to the scalar quantization
    NF3 ([Dettmers et al., 2023a,](#bib.bib11) ; Guo et al.,, [2023](#bib.bib18)).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改变参数数量 $c$ 及其相应的位精度 $b_{\phi}$，与标量量化 NF3（[Dettmers 等，2023a,](#bib.bib11)；Guo
    等，[2023](#bib.bib18)）相比，ReALLM 产生了更小的量化误差。
- en: 'Table 5: Comparison of several LLM format for $m$ parameters trained on $b_{\phi}$.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：比较多种 LLM 格式在 $m$ 参数上训练的 $b_{\phi}$。
- en: '| Method | LoRA | VQ only (like AQLM) | ReALLM |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LoRA | 仅 VQ（如 AQLM） | ReALLM |'
- en: '| Matrix representation | $(p\times q)\cdot 16$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 矩阵表示 | $(p\times q)\cdot 16$ |'
- en: '| Codebook | $-$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 代码本 | $-$ |'
- en: '| Decoder | $-$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | $-$ |'
- en: '| Low-rank | $(2\times r\times\min(p,q))\cdot 16$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 低秩 | $(2\times r\times\min(p,q))\cdot 16$ |'
- en: '| Total bit cost | $16(pq+2r\min(p,q))\cdot m$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 总位成本 | $16(pq+2r\min(p,q))\cdot m$ |'
- en: 'Table 6: Quantization and fine-tuning approaches as particular case of ReALLM (with
    a rank $r$.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：量化和微调方法作为 ReALLM 的特例（带有一个排名 $r$）。
- en: '| Method | rank $r$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 排名 $r$ |'
- en: '| LoRA (Hu et al.,, [2021](#bib.bib21)) | $64$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| LoRA（Hu 等，[2021](#bib.bib21)） | $64$ |'
- en: '| GPTQ (Frantar et al.,, [2022](#bib.bib14)) | $0$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ（Frantar 等，[2022](#bib.bib14)） | $0$ |'
- en: '| QLoRA ([Dettmers et al., 2023a,](#bib.bib11) ) | $64$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| QLoRA（[Dettmers 等，2023a,](#bib.bib11)） | $64$ |'
- en: '| LQ-LoRA (Guo et al.,, [2023](#bib.bib18)) | $64$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| LQ-LoRA（Guo 等，[2023](#bib.bib18)） | $64$ |'
- en: '| Quip# (Tseng et al.,, [2024](#bib.bib46)) | $0$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Quip#（Tseng 等，[2024](#bib.bib46)） | $0$ |'
- en: '| AQLM (Egiazarian et al.,, [2024](#bib.bib13)) | $0$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| AQLM（Egiazarian 等，[2024](#bib.bib13)） | $0$ |'
- en: '| ReALLM | $64$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM | $64$ |'
- en: A.3 Permutations
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 排列
- en: In ReALLM, we compute permutations on sets of vectors in dimension $128$. We
    could work with smaller blocks, but it induces more memory dedicated to the permutation
    storage (one permutation for each block).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ReALLM 中，我们在维度为 $128$ 的向量集上计算排列。我们可以使用更小的块，但这会导致更多的内存用于存储排列（每个块一个排列）。
- en: 'We start from the first vector (i.e. the first column of the initial matrix
    shrunk to a dimension $d=128$ vectors. The process is then iterated. Details are
    given in [Algorithm 1](#algorithm1 "In Quantization pre-processing. ‣ 3 Method
    ‣ ReALLM: A general framework for LLM compression and fine-tuning").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第一个向量（即初始矩阵的第一列，缩小到维度 $d=128$ 向量）开始。然后迭代这个过程。详细信息见 [算法 1](#algorithm1 "在量化预处理。
    ‣ 3 方法 ‣ ReALLM：用于 LLM 压缩和微调的通用框架")。
- en: A.4 Broader impacts and Safeguards
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 更广泛的影响与保障
- en: Our computing unit seriously restricts the size of the decoder models we can
    train. We are not able to train one decoder model for a given LLM, but we work
    layer-wise and train a single decoder model for all patches extracted from the
    given layer. This layer-wise training forms the main limitation of ReALLM w.r.t. standard
    post-training quantization methods, such as round to nearest (RTN).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算单元严重限制了我们可以训练的解码器模型的大小。我们不能为特定的 LLM 训练一个解码器模型，而是逐层工作，并为从给定层提取的所有补丁训练一个解码器模型。这种逐层训练是
    ReALLM 相对于标准后训练量化方法（如四舍五入到最近（RTN））的主要限制。
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了旨在推动机器学习领域发展的工作。我们的工作具有许多潜在的社会影响，但我们认为没有必要在此特别突出。
- en: 'Table 7: Perplexity $(\downarrow)$'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：困惑度 $(\downarrow)$
- en: '| Method | #bits | rank $r$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #bits | 排名 $r$ |'
- en: '| ReALLM (no fine-tuning) | $3$ | $5.27$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $3$ | $5.27$ |'
- en: '| ReALLM (30% training) | $3$ | $5.14$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（30% 训练） | $3$ | $5.14$ |'
- en: '| ReALLM (no fine-tuning) | $2$ | $8.15$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（无微调） | $2$ | $8.15$ |'
- en: '| ReALLM (10% training) | $2$ | $5.99$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ReALLM（10% 训练） | $2$ | $5.99$ |'
