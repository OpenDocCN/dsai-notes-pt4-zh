- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Breaking Free Transformer Models: Task-specific Context Attribution Promises
    Improved Generalizability Without Fine-tuning Pre-trained LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 破除限制的 Transformer 模型：任务特定上下文归因承诺在不微调预训练大模型的情况下提升泛化能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.16638](https://ar5iv.labs.arxiv.org/html/2401.16638)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.16638](https://ar5iv.labs.arxiv.org/html/2401.16638)
- en: Stepan Tytarenko,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 斯捷潘·季塔连科，
- en: Fordham University, New York
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约福坦莫大学
- en: stytarenko@fordham.edu \AndDr. Mohammad Ruhul Amin,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: stytarenko@fordham.edu \AndDr. Mohammad Ruhul Amin,
- en: Fordham University, New York
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约福坦莫大学
- en: mamin17@fordham.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: mamin17@fordham.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning large pre-trained language models (LLMs) on particular datasets
    is a commonly employed strategy in Natural Language Processing (NLP) classification
    tasks. However, this approach usually results in a loss of models’ generalizability.
    In this paper, we present a framework that allows for maintaining generalizability,
    and enhances the performance on the downstream task by utilizing task-specific
    context attribution. We show that a linear transformation of the text representation
    from any transformer model using the task-specific concept operator results in
    a projection onto the latent concept space, referred to as context attribution
    in this paper. The specific concept operator is optimized during the supervised
    learning stage via novel loss functions. The proposed framework demonstrates that
    context attribution of the text representation for each task objective can improve
    the capacity of the discriminator function and thus achieve better performance
    for the classification task. Experimental results on three datasets, namely HateXplain,
    IMDB reviews, and Social Media Attributions, illustrate that the proposed model
    attains superior accuracy and generalizability. Specifically, for the non-fine-tuned
    BERT on the HateXplain dataset, we observe 8% improvement in accuracy and 10%
    improvement in F1-score. Whereas for the IMDB dataset, fine-tuned state-of-the-art
    XLNet is outperformed by 1% for both accuracy and F1-score. Furthermore, in an
    out-of-domain cross-dataset test, DistilBERT fine-tuned on the IMDB dataset in
    conjunction with the proposed model improves the F1-score on the HateXplain dataset
    by 7%. For the Social Media Attributions dataset of YouTube comments, we observe
    5.2% increase in F1-metric. The proposed framework is implemented with PyTorch
    and provided open-source on GitHub¹¹1https://github.com/StepanTita/space-model.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在特定数据集上微调大型预训练语言模型（LLMs）是一种在自然语言处理（NLP）分类任务中常用的策略。然而，这种方法通常会导致模型的泛化能力下降。本文提出了一个框架，允许保持泛化能力，并通过利用任务特定上下文归因来提高下游任务的性能。我们展示了使用任务特定概念操作符对来自任何
    Transformer 模型的文本表示进行线性变换，结果是投影到潜在概念空间，这在本文中被称为上下文归因。特定的概念操作符在监督学习阶段通过新颖的损失函数进行优化。该框架表明，为每个任务目标进行文本表示的上下文归因可以提高判别函数的能力，从而在分类任务中取得更好的性能。对三个数据集的实验结果，即
    HateXplain、IMDB 评价和社交媒体归因，表明该模型在准确性和泛化能力方面优于其他模型。具体来说，对于未微调的 BERT 在 HateXplain
    数据集上，我们观察到准确率提高了 8%，F1 分数提高了 10%。而在 IMDB 数据集中，微调的最先进 XLNet 在准确率和 F1 分数上均被超越 1%。此外，在跨域的交叉数据集测试中，与提出的模型结合使用的微调
    DistilBERT 在 HateXplain 数据集上提高了 7% 的 F1 分数。对于 YouTube 评论的社交媒体归因数据集，我们观察到 F1 指标提高了
    5.2%。该框架使用 PyTorch 实现，并在 GitHub 上开源¹¹1https://github.com/StepanTita/space-model。
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Currently, the domain of Language Models is one of the most rapidly developing
    areas of machine learning. Transformer architecture (?) has proven itself as a
    state-of-the-art approach towards the absolute majority of Natural Language Processing
    (NLP) domains (?). A particular strength of the language models is their generalizability
    (?), (?).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，语言模型领域是机器学习中发展最快的领域之一。Transformer 架构 (?) 已被证明是自然语言处理（NLP）领域绝大多数问题的最先进方法（?）。语言模型的一个特别强项是它们的泛化能力（?），（?）。
- en: By pre-training the model on a big chunk of semi-structured data and then fine-tuning
    with task-specific labeled data, we may obtain state-of-the-art performance in
    the problems of classification, regression, language translation, and more. However,
    the important note here is that the most crucial pre-training stage is usually
    costly, and repeating it for every new task is computationally inefficient. At
    the same time, the fine-tuning only downstream task specific head of pre-trained
    models is time efficient and requires much less labeled data, preserving models’
    generalizability. On the other hand, this part of the pipeline might be a bottleneck
    to the process. Usually, single fine-tuning does not produce results on par with
    the complete model adaptation via training (?). Explaining and adapting the results
    to the various downstream tasks is also tricky. To avoid any confusion in this
    paper, we are going to use term ”training” or ”model adaptation” referring to
    the process of full model retraining (adapting all of the weights), while for
    the head-only adaptation we are going to use term ”fine-tuning”. Fine-tuning is
    usually easier, faster and requires less data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在大量半结构化数据上进行预训练，然后用特定任务的标记数据进行微调，我们可以在分类、回归、语言翻译等问题上获得最先进的性能。然而，需要注意的是，最关键的预训练阶段通常成本高昂，而且对每个新任务重复进行预训练在计算上效率低下。同时，仅微调预训练模型的下游任务头在时间上效率较高，并且需要更少的标记数据，从而保持模型的泛化能力。另一方面，这部分管道可能会成为流程的瓶颈。通常，单次微调的结果无法与通过训练（？）进行的完整模型适应相媲美。解释和调整结果以适应各种下游任务也很棘手。为避免本文中的任何混淆，我们将使用术语“训练”或“模型适应”来指代完整模型的重新训练（调整所有权重），而对于仅头部的适应我们将使用术语“微调”。微调通常更简单、更快速，并且需要更少的数据。
- en: As a potential solution to the problem, we propose a novel method of model fine-tuning.
    We call this approach the Space Model. The whole idea is to replace the classification
    head of the transformer model with a set of conceptual operators, projecting the
    contextual embeddings of the model to the set of concept spaces referred to as
    context attributions. The Space model is an additional model framework that plays
    the role of the pipeline’s original downstream task head. In this work, we limit
    ourselves to the review of the classification capabilities of the proposed approach,
    but generally, this is not a limitation to the technique in any way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决这一问题的潜在方案，我们提出了一种新的模型微调方法。我们称这种方法为**空间模型**。整个思路是用一组概念操作符替换变换模型的分类头，将模型的上下文嵌入投影到被称为上下文归因的概念空间集合中。**空间模型**是一个额外的模型框架，充当管道的原始下游任务头。在这项工作中，我们将自己限制在对所提方法的分类能力进行回顾，但通常这并不是对该技术的限制。
- en: The model is designed in a way that a set of concepts describes different classes;
    such a set is called the “context attribution”. It is worth noting that we do
    not limit these concepts in terms of overlapping. Some context attributions might
    overlap if that makes sense in terms of the problem solved. This paper reviews
    one such task where overlapping context attributions are entirely appropriate.
    What we would like to avoid is allowing multiple concepts to converge to the same
    representation. For that type of regularization, we introduce an additional loss
    called Intra-Space loss. Its goal is to make sure concepts in the context attribution
    are disjoint.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的设计方式是通过一组概念来描述不同的类别；这样的集合称为“上下文归因”。值得注意的是，我们并不限制这些概念之间的重叠。如果上下文归因在解决问题时有意义，那么它们可能会重叠。本文回顾了一个这样的任务，其中重叠的上下文归因是完全合适的。我们希望避免的是让多个概念收敛到相同的表示上。为了实现这种正则化，我们引入了一个额外的损失，称为**空间内损失**。其目标是确保上下文归因中的概念是互斥的。
- en: As was stated previously, the Space Model is an external framework with a set
    of operators on top of the transformer model. Generally speaking, this is not
    limited to the transformer architecture either. Potentially, any technique that
    can produce embeddings may be used as the Space model’s base model, such as Word2Vec
    (?), Glove (?), or RNN (?), (?). Further in this paper, whenever we refer to the
    base model, we mean the model that produces the embeddings for the space model.
    Some of the base models tested in this paper include BERT (?), DistilBERT (?),
    and XLNet (?).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，空间模型是一个外部框架，在变换器模型之上具有一组操作符。一般来说，这不仅限于变换器架构。潜在地，任何能够生成嵌入的技术都可以作为空间模型的基础模型，例如
    Word2Vec (？)、Glove (？) 或 RNN (？)、（？）。在本文中，每当我们提到基础模型时，指的是为空间模型生成嵌入的模型。本文测试的一些基础模型包括
    BERT (？)、DistilBERT (？) 和 XLNet (？)。
- en: The benchmarking and evaluation of the proposed solution are done with various
    configurations of the base models and across multiple datasets. We test performance
    for the specific task, fine-tuning the Space model for that particular downstream
    task, and we also test the performance of the model on the task that is related
    to the original semantically; however, it uses different data. The baseline for
    the comparison is mainly the performance of the original base model fine-tuned
    for the downstream task. During the experiments, we prove that besides an evident
    performance boost, the Space model also stabilizes the training process and generalizes
    better for the semantically close tasks. We also prove that the space model can
    achieve a significant performance boost even when using a smaller number of parameters
    than the base model fine-tuned on the downstream task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对所提解决方案的基准测试和评估是通过不同配置的基础模型和多个数据集进行的。我们测试了针对特定任务的性能，为该特定下游任务微调空间模型，同时也测试了模型在与原始语义相关但使用不同数据的任务上的性能。比较的基准主要是原始基础模型针对下游任务微调后的性能。在实验过程中，我们证明了除了明显的性能提升外，空间模型还稳定了训练过程，并且在语义上接近的任务中具有更好的泛化能力。我们还证明了即使使用比下游任务微调的基础模型更少的参数，空间模型也能实现显著的性能提升。
- en: The datasets used for benchmarking are HateXplain (?), IMDB reviews sentiment
    dataset (?), and Social Media Attributions dataset of YouTube comments, related
    to Chennai water crisis (?). The main reason for choosing corresponding datasets
    is that HateXplain is considered a very complex dataset, with imbalanced data,
    and labels “offensive” and “hateful” are conceptually very close. On the other
    hand, IMDB sentiment reviews are a semantically close dataset to the former one
    and are reasonably easily interpretable. Such a relation is essential since we
    would like to test the generalizability of the proposed technique. Besides, in
    the Social Media Attributions paper, the authors apply a very similar approach
    to the one proposed in this paper, however, with additional manual labeling of
    the concepts and multiple runs. We would like to show that our approach achieves
    superior performance without additional manual labeling and via a single pass.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用于基准测试的数据集包括 HateXplain (？)、IMDB 评论情感数据集 (？) 和与钦奈水危机相关的 YouTube 评论的社交媒体归因数据集
    (？)。选择这些数据集的主要原因是 HateXplain 被认为是一个非常复杂的数据集，具有不平衡的数据，并且“冒犯性”和“仇恨性”标签在概念上非常接近。另一方面，IMDB
    情感评论数据集与前者语义上非常接近，且解释起来相对容易。这种关系非常重要，因为我们希望测试所提出技术的泛化能力。此外，在社交媒体归因论文中，作者采用了与本文提出的非常相似的方法，但进行了额外的手动概念标记和多次运行。我们希望展示我们的方法在没有额外手动标记且仅通过一次运行的情况下，实现了更优的性能。
- en: 'The impact and novelty of this paper include:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的影响和创新点包括：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel framework for Language Model fine-tuning, which outperforms the baseline
    score of the base models such as BERT, DistilBERT, and XLNet
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种用于语言模型微调的新框架，其性能超越了 BERT、DistilBERT 和 XLNet 等基础模型的基准分数
- en: –
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For the non-pretrained BERT (only trained context attribution operator) we observe
    8% improvement in accuracy and 10% in F1-score on HateXplain data
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于未预训练的 BERT（仅训练了上下文归因操作符），我们观察到在 HateXplain 数据上准确率提高了 8%，F1 分数提高了 10%
- en: –
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For the IMDB with XLNet base model, we observe an improvement of around 1% after
    full model adaptation compared to fully trained vanilla XLNet
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于使用 XLNet 基础模型的 IMDB，我们观察到与完全训练的普通 XLNet 相比，经过完整模型适应后，性能提高了约 1%
- en: –
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Space-model with the base model as DistilBERT non-pretrained (only trained context
    attribution operator) on IMDB dataset, in a zero-shot manner outperforms basic
    DistilBERT in the same manner (only head fine-tuning) by 7% on F1-score
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于DistilBERT的未预训练基础模型（仅训练了上下文归属操作符）的Space-model在IMDB数据集上以零-shot的方式比基本DistilBERT（仅调整头部）在F1-score上提高了7%。
- en: –
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Compared to the results from the Social Attribution paper (with manual supervision),
    we observe an improvement of 5.2% with our model without additional supervision
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与Social Attribution论文（手动监督）的结果相比，我们的模型在没有额外监督的情况下提高了5.2%。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel loss function that improves the generalization and stabilization of
    the training process, improving the zero-shot capacity of the transformers
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种新的损失函数，改善了训练过程的泛化能力和稳定性，提高了变换器的零-shot能力。
- en: Related work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: The task of effective fine-tuning is one of the main tasks in the modern NLP.
    Cheaper and faster results of great quality are very appealing and a current trend
    in the domain. However, we are sourcing the inspiration for our framework not
    only from the latest NLP findings. The core idea has a root in a Psychological
    Belief Attribution Theory (?), (?). The theory revolves around the idea of attribution
    of certain concepts with corresponding behavior patterns. The concepts (sometimes
    also referred to as factors) may be external and internal. These factors are usually
    related to personal beliefs, and they affect the decisions and behavior of an
    individual. Researchers have also classified people based on these factors (e.g.,
    pessimistic attribution, optimistic attribution, hostile attribution). We try
    to apply the same idea to language modeling, attributing certain concepts with
    class labels. In general, the idea of measuring and researching the belief attribution
    of language models is not novel. The authors of (?) have not only proved that
    certain language models possess the beliefs, but they have also provided metrics
    to measure such beliefs and a tool to update these beliefs, as well as visualization
    of beliefs graph.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有效微调的任务是现代NLP中的主要任务之一。低成本、高质量且更快速的结果非常吸引人，并且是该领域的当前趋势。然而，我们为我们的框架提供灵感的来源不仅仅是最新的NLP发现。核心思想源于心理信念归属理论（？），（？）。该理论围绕将某些概念与相应行为模式的归属展开。这些概念（有时也称为因素）可能是外部的或内部的。这些因素通常与个人信念相关，并影响个体的决策和行为。研究人员还根据这些因素对人们进行分类（例如，悲观归属、乐观归属、敌对归属）。我们尝试将相同的思想应用于语言建模，将某些概念归属到类别标签上。一般而言，衡量和研究语言模型的信念归属的思想并不新颖。作者（？）不仅证明了某些语言模型具有信念，而且提供了测量这些信念的指标、更新这些信念的工具以及信念图的可视化。
- en: It is very natural that semi-supervised solutions are mentioned when it comes
    to fine-tuning with the least resources. These also mainly include ensembling
    to achieve regularization when working with unsupervised data. One of the first
    such approaches addressing this issue is the COREG (?). The technique uses two
    k-nearest-neighbor regressors with different distance metrics to label the data.
    The distance metric, in that case, would serve as the confidence label. This approach
    uses a fundamental idea that some features in some spaces are aligned with similar
    class labels and are further apart from the different class labels. This is an
    essential fact that is reused in the Space model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到用最少资源进行微调时，提到半监督解决方案是非常自然的。这些方法主要包括在处理无监督数据时进行集成以实现正则化。解决这个问题的第一个方法之一是COREG（？）。该技术使用两个具有不同距离度量的k近邻回归器来标记数据。在这种情况下，距离度量将作为置信度标签。该方法利用一个基本思想，即某些空间中的特征与相似的类别标签对齐，而与不同类别标签的距离更远。这是Space模型中重用的一个重要事实。
- en: Another later technique involves minimal supervision for the labeling of the
    concept space, and then, based on this concept space, the model can autonomously
    label the unlabelled data (?). The key idea here is the knowledge extraction from
    the manually labeled concept space. It is claimed in the work that labeling a
    set of concepts and then running an algorithm on a set of documents to label them
    based on these supervised concepts is a superior technique. Our main takeaway
    from there is that we can extract knowledge from the supervised concept space
    for unlabelled data. Furthermore, what we would like to propose is testing if
    this concept space can help us make a prediction at the inference stage rather
    than during labeling.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种较新的技术涉及对概念空间进行最小监督标记，然后基于这个概念空间，模型可以自主标记未标记的数据（？）。这里的关键思想是从手动标记的概念空间中提取知识。该工作中声称，标记一组概念然后在一组文档上运行算法，以基于这些监督概念对其进行标记是一种更优的技术。我们从中得到的主要启示是，我们可以从监督的概念空间中提取知识，用于未标记的数据。此外，我们希望提出的是测试这个概念空间是否能帮助我们在推理阶段而不是标记阶段进行预测。
- en: Social Media attributions in the Context of Water Crisis paper (?) is accomplishing
    a task very close to the one we are dealing with. However, unlike our approach,
    same as the previous one, their technique requires supervised sub-concept labeling.
    Besides, they measure the similarities between sub-concepts and the attention
    of the Language Model by feeding the sentence to the model multiple times, each
    time with a new sub-concept. However, they are using the similarity measure to
    find the concept sub-space that best describes the given sentence and make the
    decision based on that. Our approach does this all in one pass and in an automated
    manner. We do not require manual labeling of the concept sub-spaces. We expect
    to learn them during the fine-tuning phase.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 《水危机背景下的社交媒体归因》一文（？）完成了一个与我们处理的任务非常接近的工作。然而，与我们的方法不同，正如之前所述，他们的技术需要监督的子概念标签。此外，他们通过将句子多次输入模型，每次使用一个新的子概念，来测量子概念之间的相似性和语言模型的注意力。然而，他们使用相似性度量来找到最能描述给定句子的概念子空间，并基于此做出决定。我们的方法则在一次处理和自动化的方式下完成所有这些。我们不需要手动标记概念子空间。我们期望在微调阶段学习这些子空间。
- en: In the paper on Interacting Conceptual Spaces (?), the authors create all of
    the necessary mathematical background required to formulate the knowledge extraction
    process from the concept space. They converge the optimization task to the convex
    relations and prove that by means of optimizing the conceptual space and merging
    multiple concepts (or even spaces) together, one can extract new knowledge practical
    for the downstream task. They also provide an algebra language on how concepts
    are organized and interact and what it means mathematically when several concepts
    (or concept spaces) are combined. They put the conceptual representations in different
    compacts and explore the vectors’ behavior there. This is one of the ideas we
    are adopting in our paper, which we believe helps regularize the network. The
    concept spaces are encapsulated into a compact hypercube with the side 2\. This
    is achieved due to the utilization of the $tanh$ activation, which we will review
    in more detail in the methodology section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在《交互概念空间》一文中（？），作者创建了从概念空间中制定知识提取过程所需的所有数学背景。他们将优化任务收敛到凸关系，并证明通过优化概念空间并将多个概念（或甚至空间）合并在一起，可以提取对下游任务有用的新知识。他们还提供了一种代数语言，解释了概念是如何组织和交互的，以及多个概念（或概念空间）合并时在数学上的含义。他们将概念表示放在不同的紧凑体中，并探讨了向量在其中的行为。这是我们在论文中采用的思想之一，我们相信这有助于规范化网络。概念空间被封装在一个边长为2的紧凑超立方体中。这是通过利用$tanh$激活函数实现的，我们将在方法论部分对此进行更详细的回顾。
- en: Methodology
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: We are going to use the transformer architecture to extract the contextual embeddings
    from the input text. However, the methodology is not limited to transformers and
    may be reused with any architecture producing some kind of embeddings. In this
    specific research, we are focusing on the BERT family models (and some variations
    such as XLNet).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用变换器架构从输入文本中提取上下文嵌入。然而，这种方法论不仅限于变换器，也可以与任何生成某种嵌入的架构一起使用。在这项具体研究中，我们专注于BERT系列模型（以及一些变体如XLNet）。
- en: Context Attribution
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文归因
- en: Context attribution - is a projection of a collection of contextual embeddings
    (vectors) or, simply, a matrix. Projection is done via a concept operator. When
    we train the model, we ensure that concept operators project disjoint concepts
    far away from similar concepts. We project the sentence in multiple context attributions
    and then find the similarity between the original sentence and the conceptual
    projections. This similarity tells how to classify the instance correctly. Note
    that in the actual implementation, we do not do the pairwise comparisons of the
    similarities or any other type of processing. Instead, we concatenate obtained
    projections into a single tensor and feed it to the classification layer. Thus,
    instead of manually defining the classification criteria, we specify it as a set
    of trainable parameters.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文归因 - 是对一组上下文嵌入（向量）或简单地说是矩阵的投影。投影通过一个概念操作符完成。当我们训练模型时，我们确保概念操作符将不相干的概念与相似的概念远离。我们将句子投影到多个上下文归因中，然后找到原始句子和概念投影之间的相似性。这种相似性指示如何正确地对实例进行分类。请注意，在实际实现中，我们不会进行相似性的逐对比较或任何其他类型的处理。相反，我们将获得的投影拼接成一个单一的张量，并将其输入到分类层。因此，我们不手动定义分类标准，而是将其指定为一组可训练的参数。
- en: Contextual word embeddings
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文词嵌入
- en: As it was already stated, the only assumption we impose on the base model is
    that it can create (contextual) embeddings from the input. Let $N_{s}$ be the
    sequence length, $d$ be the dimensionality of the contextualized embedding of
    the model. $E=[e_{1},e_{2},...,e_{N_{s}}]\in R^{N_{S}\times d}$, $E_{N_{s}\times
    d}\in R^{N_{S}\times d}$ is the contexual embedding matrix.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，我们对基础模型施加的唯一假设是它能够从输入创建（上下文）嵌入。设 $N_{s}$ 为序列长度，$d$ 为模型的上下文化嵌入的维度。$E=[e_{1},e_{2},...,e_{N_{s}}]\in
    R^{N_{S}\times d}$，$E_{N_{s}\times d}\in R^{N_{S}\times d}$ 是上下文嵌入矩阵。
- en: In our research, we assume that the BERT-like models produce this embedding.
    So $N_{s}$ would be defined in the range between 256 and 512 (as a maximum sequence
    length used by the BERT architecture), and $d$ would be 768 for all of the base
    models and 1024 for the XLNet large.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们假设类似BERT的模型产生这种嵌入。因此 $N_{s}$ 将被定义在 256 到 512 之间（作为 BERT 架构使用的最大序列长度），而
    $d$ 将为所有基础模型定义为 768，为 XLNet large 定义为 1024。
- en: Conceptual projections
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概念投影
- en: 'For each of the classes in the classification problem, we assume a single concept
    space operator. This concept space operator transforms (projects) the contextual
    embeddings to the context attribution and produces new conceptual embeddings.
    The obtained representation of the embeddings is also called a latent representation.
    This representation’s dimensionality is defined as the latent space (target space
    for the projection). Let $m$ be the dimensionality of the latent space. We first
    define the projection operator as a matrix with trainable parameters: $P_{d\times
    m}\in R^{d\times m}$. Thus obtained projection matrix (context attribution) $C_{N_{s}\times
    m}\in R^{N_{s}\times m}$.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题中的每个类别，我们假设一个单一的概念空间操作符。这个概念空间操作符将上下文嵌入转换（投影）到上下文归因，并生成新的概念嵌入。获得的嵌入表示也称为潜在表示。这个表示的维度被定义为潜在空间（投影的目标空间）。设
    $m$ 为潜在空间的维度。我们首先将投影操作符定义为一个具有可训练参数的矩阵：$P_{d\times m}\in R^{d\times m}$。因此，获得的投影矩阵（上下文归因）
    $C_{N_{s}\times m}\in R^{N_{s}\times m}$。
- en: Basically, context attribution is a new representation of the embeddings in
    the latent space, where the transformation operator is trained during fine-tuning.
    However, since we want to obtain proximity of the contextualized embedding to
    the context attribution, we introduce previously defined $tanh$ operation as a
    similarity measure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，上下文归因是在潜在空间中对嵌入的一个新表示，其中变换操作符在微调过程中进行训练。然而，由于我们希望获得上下文化嵌入与上下文归因的接近度，我们引入了先前定义的
    $tanh$ 操作作为相似性度量。
- en: '|  | $C_{N_{s}\times m}=tanh(E_{N_{s}\times d}\times P_{d\times m})$ |  | (1)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{N_{s}\times m}=tanh(E_{N_{s}\times d}\times P_{d\times m})$ |  | (1)
    |'
- en: $tanh$ is applied element-wise. In that case, our conceptual matrix is a representation
    of how close a certain sentence is to the concept from the target context attribution
    (1 is very close, and -1 is from an orthogonal attribution). As an example, when
    we feed the word “terrible” to the context attribution that was predefined as
    “positive”, we expect to see -1 in the conceptual representation and 1 for a word
    like “great”.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $tanh$ 按元素应用。在这种情况下，我们的概念矩阵表示某个句子与目标上下文归属的概念的接近程度（1表示非常接近，-1表示正交归属）。例如，当我们将词语“terrible”输入到预定义为“positive”的上下文归属时，我们期望在概念表示中看到
    -1，而对于词语“great”则期望看到 1。
- en: The training objective of the model is, by taking into account multiple projections
    of the input embeddings, to find the projection that is most aligned with the
    sentence content. The similarity measure we are using is a slight modification
    of the cosine similarity, where normalizing the value by the vectors’ norms is
    replaced with the $tanh$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练目标是，通过考虑输入嵌入的多个投影，找到与句子内容最对齐的投影。我们使用的相似度度量是对余弦相似度的轻微修改，其中通过向量的范数进行归一化的操作被
    $tanh$ 替代。
- en: '|  | $tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ |  | (2) |'
- en: Similarly to the original paper introducing LSTM (?) , we use $tanh$ to control
    the flow of the information in the network. It squashes the range, centers the
    values around zero, and introduces non-linearity. This has also proven to be an
    excellent regularization technique, which reduces the instability, improves models’
    generalizability, and improves the results. This aspect is discussed in more detail
    in the results section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于原论文介绍 LSTM (?)，我们使用 $tanh$ 来控制网络中信息的流动。它压缩了范围，将值中心化到零附近，并引入了非线性。这也被证明是一种优秀的正则化技术，减少了不稳定性，提高了模型的泛化能力，并改善了结果。这个方面在结果部分进行了更详细的讨论。
- en: '![Refer to caption](img/06b7653646618e3df4a4baf16858997e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/06b7653646618e3df4a4baf16858997e.png)'
- en: 'Figure 1: 3D projection of the space embeddings for the 2-class classification.
    After projecting the sentence onto different concept spaces, we expect these projections
    to be orthogonal if the classes are completely divergent. For the case between
    positive and negative sentiment, we expect that positive class projection would
    be orthogonal to the negative class projection.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：2类分类的空间嵌入的3D投影。将句子投影到不同的概念空间后，我们期望这些投影在类完全分离时是正交的。对于积极和消极情感的情况，我们期望积极类的投影与消极类的投影是正交的。
- en: As a good side-effect of the $tanh$ we add additional non-linearity and squashing
    effect to the model. Thus, no additional normalization of values is required.
    Besides, we shrink our problem to the compact (hypercube with side 2, from -1
    to 1). In Figure 1, one can find a benefit from such an approach. We can now easily
    interpret the outcome of the binary classification model. The visualization provided
    is the contextual embedding of the BERT model into 3-dimensional context attribution
    space for the IMDB classification task (this is done for test examples, so the
    model is not overfitted, and what we clearly see is the orthogonality of the negative
    and positive sentiment concepts).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $tanh$ 的一个好副作用是为模型增加了额外的非线性和压缩效果。因此，不需要额外的值归一化。此外，我们将问题缩小到紧凑的（边长为2的超立方体，从 -1
    到 1）。在图1中，可以发现这种方法的好处。我们现在可以轻松解释二分类模型的结果。提供的可视化是 BERT 模型在 3 维上下文归属空间中的上下文嵌入，针对
    IMDB 分类任务（这是针对测试示例进行的，因此模型没有过拟合，我们清晰地看到负面和积极情感概念的正交性）。
- en: 'According to our definition, every target class has a unique context attribution
    for itself. So for $n$ classes classification problem:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的定义，每个目标类都有一个独特的上下文归属。因此，对于 $n$ 类分类问题：
- en: '|  | $C_{N_{s}\times m}^{i}=tanh(E_{N_{s}\times d}\times P_{d\times m}^{i})$
    |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{N_{s}\times m}^{i}=tanh(E_{N_{s}\times d}\times P_{d\times m}^{i})$
    |  | (3) |'
- en: for $i\leq n$. Where $C^{i}$ and $P^{i}$ are namely $i$-th context attribution
    and concept projection operator.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $i\leq n$。其中 $C^{i}$ 和 $P^{i}$ 分别为第 $i$ 个上下文归属和概念投影操作符。
- en: Classification
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: After we have projected the embeddings to all of the context attributions, we
    need to perform classification. In that case, since every projection is a set
    of vectors (where each vector is a conceptual embedding with latent size $m$)
    we would find the centroid of this representation for each context attribution
    and then concatenate these representations. This concatenated representation is
    then fed to the single linear layer for classification. This basically identifies
    the proximity of the embedding to the corresponding context attribution. Let $k_{i}$
    represent $i$-th context attribution centroid, and $c_{i,j}$ $j$-th conceptual
    embedding vector ($j$-column) of the $i$-th context attribution $C_{N_{s}\times
    m}^{i}$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将嵌入映射到所有上下文归属后，我们需要进行分类。在这种情况下，由于每个投影都是一组向量（每个向量是具有潜在大小 $m$ 的概念嵌入），我们将找到每个上下文归属的这个表示的质心，然后将这些表示连接起来。然后将这种连接表示传递给单个线性层进行分类。这基本上是识别嵌入与相应上下文归属的接近程度。令
    $k_{i}$ 表示第 $i$ 个上下文归属质心，$c_{i,j}$ 表示第 $i$ 个上下文归属 $C_{N_{s}\times m}^{i}$ 的第 $j$
    个概念嵌入向量（第 $j$ 列）。
- en: '|  | $k_{i}=\frac{1}{N_{s}}\cdot\sum_{j=0}^{N_{s}}c_{i,j}$ |  | (4) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $k_{i}=\frac{1}{N_{s}}\cdot\sum_{j=0}^{N_{s}}c_{i,j}$ |  | (4) |'
- en: Loss function
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss that we are optimizing is primarily the Cross-Entropy loss. To ensure
    that the conceptual embeddings don’t converge to the same embedding inside the
    conceptual space, we introduce an intra-space loss. This also adds additional
    regularization and improves generalization. This is proved during the experiments.
    Controlling the weight of this loss compared to the cross entropy loss is another
    hyperparameter fine-tuning task. The intra-space loss is basically an inverse
    of the variance of the vectors inside the context attribution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们优化的损失主要是交叉熵损失。为了确保概念嵌入不会在概念空间内收敛到相同的嵌入，我们引入了一个内部空间损失。这也增加了额外的正则化并提高了泛化能力。这在实验中得到了验证。与交叉熵损失相比，控制这种损失的权重是另一个超参数调整任务。内部空间损失基本上是上下文归属内向量方差的倒数。
- en: '|  | $\sigma^{2}=\sum_{i=1}^{m}\frac{1}{m}\cdot(c_{i}-\hat{c})^{2}$ |  | (5)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{2}=\sum_{i=1}^{m}\frac{1}{m}\cdot(c_{i}-\hat{c})^{2}$ |  | (5)
    |'
- en: where $c_{i}$ is $i$-th embedding ($i$-th column of the context attribution
    matrix) $C_{N_{s}\times m}$ and $\hat{c}$ is the mean vector of conceptual embedding
    matrix (column-wise).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c_{i}$ 是第 $i$ 个嵌入（上下文归属矩阵的第 $i$ 列）$C_{N_{s}\times m}$，$\hat{c}$ 是概念嵌入矩阵的均值向量（按列）。
- en: Results
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'We evaluate our framework using 3 base models in 3 benchmarks with 3 different
    datasets. With benchmarks, we want to measure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 3 个基模型在 3 个基准测试中对 3 个不同的数据集进行评估。通过基准测试，我们希望衡量：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performance of the proposed Space Model
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提议的空间模型的性能
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generalization property of the novel technique
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新技术的泛化性质
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How does it compare with existing context attribution solutions which involve
    a manual process
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与涉及手动过程的现有上下文归属解决方案相比如何
- en: To achieve that, we are going to use 3 datasets, namely HateXplain, IMDB reviews
    sentiment dataset, and Social Media Attributions dataset of YouTube comments related
    to the Chennai water crisis. IMDB sentiment reviews is a semantically close dataset
    to the HateXplain and is reasonably easily interpretable. Such a relation is essential
    since we would like to test the generalizability of the proposed technique. Besides,
    in the Social Media Attributions paper, the authors do manual labeling of the
    concepts and measure similarity with the so-called Social Media Attributions;
    we would like to show that our approach achieves superior performance without
    additional manual labeling and via a single pass.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们将使用 3 个数据集，即 HateXplain、IMDB 影评情感数据集和与钦奈水危机相关的 YouTube 评论的社交媒体归属数据集。IMDB
    影评情感数据集在语义上与 HateXplain 接近且相对容易解释。这种关系至关重要，因为我们希望测试所提技术的泛化能力。此外，在社交媒体归属论文中，作者对概念进行手动标注并与所谓的社交媒体归属进行相似性测量；我们希望展示我们的方法在没有额外手动标注的情况下，通过一次传递实现了优越的性能。
- en: The experiments are structured in a way that we have basic experiments with
    smaller models and simpler tasks. Additionally, we conducted experiments to compare
    the Space Model to the state-of-the-art model of the IMDB dataset. We also investigate
    and analyze various properties of the Space Model and explore some of the hyperparameters’
    usage, with their respectful effect on the model performance. We explore the generalization
    property of the model by cross-testing it on the unseen dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结构安排为基础实验，包括较小的模型和更简单的任务。此外，我们还进行了实验，将空间模型与 IMDB 数据集的最先进模型进行比较。我们还研究和分析了空间模型的各种属性，并探索了一些超参数的使用及其对模型性能的影响。我们通过在未见过的数据集上进行交叉测试，探索模型的泛化属性。
- en: '| Dataset | Model | Train Params | Accuracy | F1-score (macro) | Recall | Intra-Space
    weight |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | 训练参数 | 准确率 | F1-score（宏观） | 召回率 | 内部空间权重 |'
- en: '| IMDB (training) | DistilBERT | 592130 | 0.7852 | 0.7819 | 0.6614 | N/A |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| IMDB（训练） | DistilBERT | 592130 | 0.7852 | 0.7819 | 0.6614 | N/A |'
- en: '|  | Space Model | 197122 | 0.7917 | 0.7916 | 0.7728 | 0.001 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间模型 | 197122 | 0.7917 | 0.7916 | 0.7728 | 0.001 |'
- en: '|  | Space Model | 197122 | 0.8322 | 0.8320 | 0.8663 | 0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间模型 | 197122 | 0.8322 | 0.8320 | 0.8663 | 0 |'
- en: '| HateXplain (zero-shot testing) | DistilBERT | 592130 | 0.6013 | 0.4450 |
    0.0869 | N/A |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| HateXplain（零样本测试） | DistilBERT | 592130 | 0.6013 | 0.4450 | 0.0869 | N/A
    |'
- en: '|  | Space Model | 197122 | 0.5821 | 0.5187 | 0.2698 | 0.001 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间模型 | 197122 | 0.5821 | 0.5187 | 0.2698 | 0.001 |'
- en: '|  | Space Model | 197122 | 0.5977 | 0.5040 | 0.2007 | 0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间模型 | 197122 | 0.5977 | 0.5040 | 0.2007 | 0 |'
- en: 'Table 1: Comparative table of the results of the Space Model in different configurations
    with DistilBERT on the IMDB dataset and HateXplain dataset'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同配置下空间模型与 DistilBERT 在 IMDB 数据集和 HateXplain 数据集上的结果对比表
- en: Preprocessing and settings
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理和设置
- en: BERT is a standard model that we use as a reference and a baseline. We only
    fully train the weights of this model once when compared with the Chennai water
    crisis data. For all of the other experiments, we preserve all of the generalizability
    and do not spend time on training. XLNet is the current state-of-the-art transformer
    for multiple benchmarks; in this specific work, we focus on the IMDB sentiment
    analysis dataset. By using this model and comparing the results with it, we want
    to prove that attaching the Space-model head to virtually any current state-of-the-art
    transformer would significantly boost performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT** 是一个标准模型，我们将其用作参考和基准。在与钦奈水危机数据对比时，我们只对该模型的权重进行了一次完全训练。在其他所有实验中，我们保留了所有的泛化能力，并且没有花时间进行训练。**XLNet**
    是当前在多个基准上表现最好的变换器模型；在这项特定工作中，我们专注于 IMDB 情感分析数据集。通过使用该模型并与其结果进行比较，我们希望证明将空间模型头附加到几乎任何当前最先进的变换器模型上将显著提升性能。'
- en: We are not conducting any data preprocessing for either of the datasets. We
    use cased models for all of the expriments except for the Social Media Attributions
    comparison. For the space model, the key idea is the contextual embedding generation.
    The entity doing this in our framework is called a base model; virtually any transformer
    model can play this role. We use cased DistliBERT, cased BERT, and cased XLNet.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对这两个数据集，我们没有进行任何数据预处理。除了社交媒体归因比较之外，我们对所有实验使用了区分大小写的模型。对于空间模型，关键思想是上下文嵌入生成。在我们的框架中，执行这项任务的实体被称为基础模型；几乎任何变换器模型都可以担任这个角色。我们使用了区分大小写的
    DistilBERT、区分大小写的 BERT 和区分大小写的 XLNet。
- en: '![Refer to caption](img/75b387ef16dfb56d07d6f683c94fdeaf.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/75b387ef16dfb56d07d6f683c94fdeaf.png)'
- en: 'Figure 2: 3D projection of the space embeddings for the 3-class classification
    (HateXplain). For the 3-class, similar to the 2-class, we expect to have 3 orthogonal
    projections. Here, we observe that if we review this image in multiple projections
    - some projections are clearly orthogonal, and some are more aligned. This is
    the effect that we have discussed previously, that contextual attributions might
    have overlapping concepts.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：3 类分类（HateXplain）的空间嵌入的 3D 投影。对于 3 类数据，与 2 类数据类似，我们期望有 3 个正交投影。在这里，我们观察到，如果我们在多个投影中查看此图像——一些投影显然是正交的，一些则更为对齐。这是我们之前讨论的效果，即上下文归因可能具有重叠的概念。
- en: We use the base model configuration for all of the experiments except for the
    state-of-the-art establishment (12 layers for BERT and XLNet and 6 layers for
    DistilBERT). For the state-of-the-art performance, we trained large (24 layers)
    XLNet, which was used for reporting the results in the original paper. We use
    the Adam optimizer with a learning rate of $2\cdot 10^{-4}$ for all experiments,
    except for the state-of-the-art establishment, since the original paper states
    that $10^{-5}$ was used to achieve the best results. Maximum sequence length and
    batch size is 256 for all of the basic experiments and is replaced with 512 and
    4, respectively, for the XLNet large state-of-the-art results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最新技术的建立（BERT和XLNet的12层，DistilBERT的6层）外，我们对所有实验使用基础模型配置。对于最新技术表现，我们训练了大型（24层）XLNet，这在原始论文中用于报告结果。我们对所有实验使用Adam优化器，学习率为$2\cdot
    10^{-4}$，除了最新技术建立，因为原始论文指出使用了$10^{-5}$以获得最佳结果。所有基础实验的最大序列长度和批量大小为256，而对于XLNet
    large最新技术结果则替换为512和4。
- en: Since the original paper recommends using 32 as the batch size for the IMDB
    benchmark for the best results, and we could not fit that to the GPU memory, we
    used 8 gradient accumulation steps and adjusted the number of training steps accordingly.
    Even though the result does not precisely reproduce the original outcome, it is
    close, and the evident performance boost from the space model is transparent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始论文推荐使用32作为IMDB基准的批量大小以获得最佳结果，而我们无法适应GPU内存，因此我们使用了8个梯度累积步骤，并相应调整了训练步骤的数量。尽管结果未能完全重现原始结果，但接近，并且从空间模型中明显的性能提升是显而易见的。
- en: For the number of latent spaces, we use three for most experiments since this
    is enough to outperform significantly and is easy to visualize. As discussed previously,
    when we project the contextual embeddings onto the context attribution, we expect
    these projections to be orthogonal if the classes are different. That is what
    we observe in Figure 1 and Figure 2.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于潜在空间的数量，我们在大多数实验中使用三个，因为这足以显著超越并且易于可视化。如前所述，当我们将上下文嵌入投影到上下文归因上时，我们期望这些投影在类别不同的情况下是正交的。这就是我们在图1和图2中观察到的情况。
- en: For the comparison with the Social Media Attributions, use the latent size of
    64\. For the state-of-the-art results using XLNet, we use 128 as the latent space
    size. We use a single Nvidia A5000 GPU for our training. Our model with various
    configurations may take from 30 seconds per epoch with DistilBERT to 25 minutes
    with XLNet large. A standard number of fine-tuning epochs is set to 5; however,
    for the XLNet large state-of-the-art results, we used only one epoch of training
    with one epoch of head fine-tuning to prevent overfitting.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与社交媒体归因的比较中，使用64的潜在空间大小。对于使用XLNet的最新技术成果，我们使用128作为潜在空间大小。我们使用一块Nvidia A5000
    GPU进行训练。我们的模型在不同配置下，可能需要从DistilBERT每个周期30秒到XLNet large每个周期25分钟。标准的微调周期数设为5；然而，为了达到XLNet
    large的最新技术成果，我们仅使用了一个周期的训练和一个周期的头部微调以防止过拟合。
- en: Evaluation Metrics
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: Since we are evaluating the model between multiple benchmarks simultaneously,
    we want to adjust to both a perfectly balanced IMDB dataset and a less balanced
    HateXplain dataset. So, we report accuracy and f1-macro score. Our loss throughout
    the experiments is Cross-Entropy loss, sometimes combined with intra-space loss
    for better regularization. We also report the weight of the Intra-space loss in
    the experiments. This is usually set to a very low number to avoid dominance over
    the cross-entropy loss.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们同时在多个基准上评估模型，我们希望调整到一个完全平衡的IMDB数据集和一个不太平衡的HateXplain数据集。因此，我们报告准确率和f1-macro分数。我们在整个实验中的损失是交叉熵损失，有时结合了空间内部损失以获得更好的正则化。我们还报告了实验中空间内部损失的权重。这通常设置为一个非常低的数值，以避免对交叉熵损失的主导。
- en: Experimental Results
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验结果
- en: Fine-tuning Space Model
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调空间模型
- en: First, we ran a set of experiments on the IMDB benchmark dataset with the DistilBERT
    model as a base model (Table 1). We observe that the Space model is superior for
    both accuracy and f1-macro score. We also explore the number of trained parameters.
    With 3-time fewer parameters, the performance boost is already around 5% for both
    metrics. We also observe that with around 128 times fewer trainable parameters,
    the space model performs better by almost 2%.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 IMDB 基准数据集上使用 DistilBERT 模型作为基础模型进行了多组实验（见表 1）。我们观察到 Space 模型在准确率和 f1-macro
    分数上都表现更优。我们还探讨了训练参数的数量。参数减少三倍的情况下，两个指标的性能提升已接近 5%。我们还观察到，在训练参数减少约 128 倍的情况下，Space
    模型的表现提升了近 2%。
- en: '| Metric | Space Model | BERT |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | Space 模型 | BERT |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | 0.5296 | 0.4485 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.5296 | 0.4485 |'
- en: '| F1-score (macro) | 0.4304 | 0.3314 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| F1-score（宏） | 0.4304 | 0.3314 |'
- en: '| Precision | 0.5431 | 0.4471 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 精确率 | 0.5431 | 0.4471 |'
- en: '| Recall | 0.5296 | 0.4485 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 0.5296 | 0.4485 |'
- en: 'Table 2: BERT HateXplain (3-class) evaluation'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：BERT HateXplain（三类）评估
- en: Then, we compare these results with the experiments for a much more complex
    HateXplain benchmark. The choice of the datasets is non-arbitrary in that case.
    We want data to have the evident polarization between classes, which is aligned
    cross-datasets, to prove the zero-short generalization component of our approach.
    BERT, DistilBERT, and XLNet are all evaluated with this benchmark against the
    Space Model in a 3-class and 2-class setting.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些结果与针对更复杂的 HateXplain 基准的实验进行比较。在这种情况下，数据集的选择并非任意。我们希望数据在类别之间具有明显的极化，并且跨数据集一致，以证明我们方法的零样本泛化组件。BERT、DistilBERT
    和 XLNet 都在三类和两类设置下与 Space 模型进行了评估。
- en: Generalizability
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 泛化能力
- en: We take corresponding models and evaluate them on the HateXplain benchmark in
    a zero-shot manner (Table 1). For the sentiment analysis, negative labels are
    encoded as 0 and positive as 1; for the HateXplain, we encode Hateful and offensive
    labels as 0 and normal labels as 1\. Here, we see that the Space model with intra-space
    loss is a top model in terms of f1-score, while the accuracy is the highest for
    the DistilBERT. However, accounting for the dataset imbalance, we see that DistilBERT
    is worse in terms of f1 by at least 6-7% and almost 4 times worse in terms of
    recall. Next, we compare the BERT model with the Space model and base BERT on
    the HateXplain benchmark with 3 classes. Here, we only train the classification
    head for BERT and contextual attribution operators for the Space model (as discussed
    previously). The results in Table 2 clearly show that the Space Model is superior
    in all of the metrics by at least 8%.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相应的模型在 HateXplain 基准上以零样本方式进行了评估（见表 1）。对于情感分析，负标签编码为 0，正标签编码为 1；对于 HateXplain，我们将仇恨和攻击性标签编码为
    0，将正常标签编码为 1。在这里，我们看到，具有 intra-space 损失的 Space 模型在 f1-score 上是顶尖模型，而 DistilBERT
    的准确率最高。然而，考虑到数据集的不平衡，我们看到 DistilBERT 的 f1-score 至少低 6-7%，在召回率方面几乎低了 4 倍。接下来，我们将
    BERT 模型与 Space 模型和基础 BERT 进行了比较，在 3 类的 HateXplain 基准上进行评估。在这里，我们仅对 BERT 训练分类头，对
    Space 模型训练上下文归因操作符（如前所述）。表 2 的结果清楚地显示，Space 模型在所有指标上都至少优于 8%。
- en: We then fine-tune the classification head and the space model with XLNet and
    BERT base models for the same HateXplain benchmark (Table 5) and observe that
    for BERT, the performance gap with identical training settings and identical base
    model is more than 16% on the f1-macro score. In comparison, for XLNet, this gap
    is around 6%.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对分类头和 Space 模型进行了微调，使用 XLNet 和 BERT 基础模型在相同的 HateXplain 基准数据集上（见表 5），并观察到对于
    BERT，在相同的训练设置和基础模型下，f1-macro 分数的性能差距超过 16%。相比之下，XLNet 的差距约为 6%。
- en: To further prove the effect of the performance boost using the space model,
    we do the full training of the XLNet, a state-of-the-art model for the IMDB benchmark
    (Table 3). With almost identical settings to the original paper, we obtain a 0.9386
    f1-score, while training the space model with the exact same settings gives us
    0.9487 (all of the other metrics are also superior for the space model, except
    for the recall, which is again very different with precision for the vanilla model,
    and very close for the space model). We observe that the space model surpasses
    the state-of-the-art models in the tasks and is much more tolerant to the imbalanced
    data. The precision-recall trade-off is evident in most of the experiments. To
    prove this point further, we conducted the ablation study and researched how the
    space model stabilizes performance during training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步证明使用空间模型提升性能的效果，我们对IMDB基准测试中的最先进模型XLNet进行了全面训练（表3）。在几乎与原论文相同的设置下，我们获得了0.9386的F1-score，而使用完全相同设置训练空间模型则得到了0.9487（其他所有指标也优于空间模型，除了召回率，在传统模型中与精确率差异较大，而在空间模型中则非常接近）。我们观察到空间模型在任务中超越了最先进的模型，并且对不平衡数据的容忍度更高。大多数实验中精确率与召回率的权衡是显而易见的。为了进一步证明这一点，我们进行了消融研究，并研究了空间模型在训练过程中如何稳定性能。
- en: '| Metric | Space Model | XLNet |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 空间模型 | XLNet |'
- en: '| --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | 0.9488 | 0.9387 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.9488 | 0.9387 |'
- en: '| F1-score (macro) | 0.9487 | 0.9386 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| F1-score（宏观）| 0.9487 | 0.9386 |'
- en: '| Precision | 0.9463 | 0.9106 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 精确率 | 0.9463 | 0.9106 |'
- en: '| Recall | 0.9516 | 0.9731 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 0.9516 | 0.9731 |'
- en: 'Table 3: State-of-the-art XLNet on IMDB'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：IMDB上的最先进XLNet
- en: Social Media Attribution
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 社交媒体归因
- en: We compare our Space Model with the Social Media Attribution and observe 5.2%
    F1-score improvement on our own reproducing experiment and almost 2% F1-score
    improvement compared to the best-reported score from the original paper. The best
    result there was obtained on the adapted Indian BERT, while we used base uncased
    BERT without any adaptation, so this performance boost is not exhaustive.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的空间模型与社交媒体归因进行了比较，发现我们的重现实验中F1-score提高了5.2%，而与原论文中报告的最佳成绩相比提高了近2%。那里获得的最佳结果是基于适应后的印度BERT，而我们使用的是未适应的基础uncased
    BERT，因此这一性能提升并非穷尽所有可能。
- en: '| Metric | Space-model | BERT (uncased) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 空间模型 | BERT（uncased） |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Accuracy | 0.8309 | 0.8220 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.8309 | 0.8220 |'
- en: '| F1-score (macro) | 0.8006 | 0.7484 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| F1-score（宏观）| 0.8006 | 0.7484 |'
- en: '| Precision | 0.7126 | 0.8876 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 精确率 | 0.7126 | 0.8876 |'
- en: '| Recall | 0.7337 | 0.4674 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 0.7337 | 0.4674 |'
- en: 'Table 4: Social Media Attribution BERT-uncased'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：社交媒体归因BERT-uncased
- en: '| Metric | Train Params | Accuracy | F1-score (macro) | Precision | Recall
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 训练参数 | 准确率 | F1-score（宏观）| 精确率 | 召回率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Space-model (XLNet) | 4622 | 0.8798 | 0.8797 | 0.8764 | 0.8824 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Space-model（XLNet）| 4622 | 0.8798 | 0.8797 | 0.8764 | 0.8824 |'
- en: '| XLNet-base-cased | 1538 | 0.8160 | 0.8156 | 0.8421 | 0.7750 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| XLNet-base-cased | 1538 | 0.8160 | 0.8156 | 0.8421 | 0.7750 |'
- en: '| Space-model (BERT) | 4622 | 0.8110 | 0.8108 | 0.8227 | 0.7899 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Space-model（BERT）| 4622 | 0.8110 | 0.8108 | 0.8227 | 0.7899 |'
- en: '| BERT-base-cased | 1538 | 0.6588 | 0.6555 | 0.6919 | 0.5649 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| BERT-base-cased | 1538 | 0.6588 | 0.6555 | 0.6919 | 0.5649 |'
- en: 'Table 5: BERT and XLNet Comparison on HateXplain Dataset (2-class)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：BERT与XLNet在HateXplain数据集（2类）上的比较
- en: Regularization effect on fine-tuning
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化对微调的影响
- en: '![Refer to caption](img/41f3cee64adc57c4782a1fd3adfc5d9a.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/41f3cee64adc57c4782a1fd3adfc5d9a.png)'
- en: 'Figure 3: DistilBERT (upper part) vs DistilBERT and Space Model (lower part)
    stabilization comparison'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：DistilBERT（上部分）与DistilBERT和Space Model（下部分）的稳定性比较
- en: During the experiments, we observed that the space model has a much better ratio
    of recall/precision, which means that it handles imbalanced data much more efficiently.
    Another observation is that adding just a space model stabilizes the results during
    training, not allowing the performance to vary a lot between iterations. Find
    the visualization of the ablation study in Figure 3. Additionally, intra-space
    loss adds more regularization and stabilization and ensures that the concepts
    in the context attribution will not converge to a single vector.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验过程中，我们观察到空间模型具有更好的召回率/精确率比，这意味着它对不平衡数据的处理效率更高。另一个观察结果是，仅添加一个空间模型就能在训练过程中稳定结果，不允许性能在迭代间变化太大。图3中展示了消融研究的可视化结果。此外，内部空间损失增加了更多的正则化和稳定性，并确保上下文归因中的概念不会收敛到单一向量。
- en: Conclusion and discussion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论与讨论
- en: In conclusion, this research focused on the novel methodology towards conceptual
    embedding for classification with Language models. As an outcome of this research,
    we have conducted a set of experiments to empirically prove the efficiency of
    the proposed technique. We have also created the implementation of the proposed
    framework via PyTorch and provided an open-source GitHub repository to incivate
    and simplify future collaboration and exploration. We believe that the potential
    of this approach is yet to be discovered, and the goal of this paper was to provide
    some baseline ideas and understanding.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本研究集中于语言模型分类的概念嵌入的新方法。作为研究成果，我们进行了一系列实验，以实证证明所提技术的有效性。我们还通过PyTorch实现了所提框架，并提供了一个开源的GitHub代码库，以促进未来的合作与探索。我们相信这种方法的潜力尚待发现，本文的目标是提供一些基础性思想和理解。
- en: We anticipate improvements by adding more complex transformations after the
    conceptual projection phase. We also believe that this technique should in no
    way be limited to classification problems only. The formulation of the regression
    problem is quite straightforward but needs to be additionally researched. With
    that, we also expect that 1-to-1 correspondence of the context attribution to
    the target class is an artificial limitation that we hold in this paper for the
    simplicity of interpretation. However, if domain knowledge suggests that having
    multiple context attributions (more than the number of classes) for the task makes
    sense - then this should also be an option. We would also like to explore further
    the potential of the interpretation capabilities of the framework and how we can
    use it to extract knowledge from the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预期在概念投影阶段后通过添加更复杂的转换来改进。此外，我们还认为这种技术不应仅限于分类问题。回归问题的表述相当简单，但需要进一步研究。我们也期望将上下文归因与目标类别的1对1对应关系是我们在本文中为简化解释而设定的一个人为限制。然而，如果领域知识表明对任务拥有多个上下文归因（多于类别数量）是合理的，那么这也应成为一个选项。我们还希望进一步探索框架的解释能力的潜力以及如何利用它从模型中提取知识。
- en: References
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Bem 1972] Bem, D. J. 1972. Self-perception theory11development of self-perception
    theory was supported primarily by a grant from the national science foundation
    (gs 1452) awarded to the author during his tenure at carnegie-mellon university.
    volume 6 of Advances in Experimental Social Psychology. Academic Press. 1–62.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bem 1972] Bem, D. J. 1972. 《自我感知理论》11自我感知理论的开发主要得到国家科学基金（gs 1452）资助，该基金在作者在卡内基梅隆大学任职期间颁发。
    《实验社会心理学进展》第6卷。学术出版社。1–62。'
- en: '[Blackledge and Atapour-Abarghouei 2021] Blackledge, C., and Atapour-Abarghouei,
    A. 2021. Transforming fake news: Robust generalisable news classification using
    transformers. In 2021 IEEE International Conference on Big Data (Big Data), 3960–3968.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Blackledge 和 Atapour-Abarghouei 2021] Blackledge, C., 和 Atapour-Abarghouei,
    A. 2021. 《转变假新闻: 使用变换器的稳健可泛化新闻分类》。在2021 IEEE国际大数据会议（Big Data），3960–3968。'
- en: '[Bolt et al. 2019] Bolt, J.; Coecke, B.; Genovese, F.; Lewis, M.; Marsden,
    D.; and Piedeleu, R. 2019. Interacting Conceptual Spaces I: Grammatical Composition
    of Concepts. Cham: Springer International Publishing. 151–181.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bolt et al. 2019] Bolt, J.; Coecke, B.; Genovese, F.; Lewis, M.; Marsden,
    D.; 和 Piedeleu, R. 2019. 《交互式概念空间 I: 概念的语法组合》。Cham: Springer International Publishing.
    151–181.'
- en: '[Chenthamarakshan et al. 2011] Chenthamarakshan, V.; Melville, P.; Sindhwani,
    V.; and Lawrence, R. 2011. Concept labeling: Building text classifiers with minimal
    supervision. 1225–1230.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chenthamarakshan et al. 2011] Chenthamarakshan, V.; Melville, P.; Sindhwani,
    V.; 和 Lawrence, R. 2011. 《概念标记: 使用最少监督构建文本分类器》。1225–1230。'
- en: '[Devlin et al. 2019] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    In Burstein, J.; Doran, C.; and Solorio, T., eds., Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. Minneapolis,
    Minnesota: Association for Computational Linguistics.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Devlin et al. 2019] Devlin, J.; Chang, M.-W.; Lee, K.; 和 Toutanova, K. 2019.
    《BERT: 用于语言理解的深度双向变换器的预训练》。在 Burstein, J.; Doran, C.; 和 Solorio, T., 编，《2019年北美计算语言学协会年会会议录:
    人类语言技术，第1卷（长篇和短篇论文）》，4171–4186。明尼阿波利斯, 明尼苏达州: 计算语言学协会。'
- en: '[Ghosh et al. 2016] Ghosh, S.; Vinyals, O.; Strope, B.; Roy, S.; Dean, T.;
    and Heck, L. 2016. Contextual lstm (clstm) models for large scale nlp tasks. ArXiv
    abs/1602.06291.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ghosh et al. 2016] Ghosh, S.; Vinyals, O.; Strope, B.; Roy, S.; Dean, T.;
    和 Heck, L. 2016. 用于大规模NLP任务的上下文LSTM（CLSTM）模型。ArXiv abs/1602.06291。'
- en: '[Hase et al. 2023] Hase, P.; Diab, M.; Celikyilmaz, A.; Li, X.; Kozareva, Z.;
    Stoyanov, V.; Bansal, M.; and Iyer, S. 2023. Methods for measuring, updating,
    and visualizing factual beliefs in language models. In Vlachos, A., and Augenstein,
    I., eds., Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics, 2714–2731. Dubrovnik, Croatia: Association for
    Computational Linguistics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hase et al. 2023] Hase, P.; Diab, M.; Celikyilmaz, A.; Li, X.; Kozareva, Z.;
    Stoyanov, V.; Bansal, M.; 和 Iyer, S. 2023. 测量、更新和可视化语言模型中事实信念的方法。在Vlachos, A.,
    和 Augenstein, I., 主编，《第17届欧洲计算语言学协会年会论文集》，2714–2731。克罗地亚杜布罗夫尼克：计算语言学协会。'
- en: '[Hochreiter and Schmidhuber 1997] Hochreiter, S., and Schmidhuber, J. 1997.
    Long short-term memory. Neural Computation 9(8):1735–1780.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hochreiter and Schmidhuber 1997] Hochreiter, S., 和 Schmidhuber, J. 1997. 长短期记忆。神经计算
    9(8):1735–1780。'
- en: '[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y.;
    and Potts, C. 2011. Learning word vectors for sentiment analysis. In Proceedings
    of the 49th Annual Meeting of the Association for Computational Linguistics: Human
    Language Technologies, 142–150. Portland, Oregon, USA: Association for Computational
    Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A.
    Y.; 和 Potts, C. 2011. 词向量学习用于情感分析。在《计算语言学协会第49届年会论文集：人类语言技术》，142–150。美国俄勒冈州波特兰：计算语言学协会。'
- en: '[Maia et al. 2021] Maia, M.; Sales, J. E.; Freitas, A.; Handschuh, S.; and
    Endres, M. 2021. A comparative study of deep neural network models on multi-label
    text classification in finance. In 2021 IEEE 15th International Conference on
    Semantic Computing (ICSC), 183–190.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Maia et al. 2021] Maia, M.; Sales, J. E.; Freitas, A.; Handschuh, S.; 和 Endres,
    M. 2021. 在金融领域多标签文本分类中对深度神经网络模型的比较研究。在2021 IEEE第15届国际语义计算会议（ICSC），183–190。'
- en: '[Mathew et al. 2021] Mathew, B.; Saha, P.; Yimam, S. M.; Biemann, C.; Goyal,
    P.; and Mukherjee, A. 2021. Hatexplain: A benchmark dataset for explainable hate
    speech detection. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 35, 14867–14875.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mathew et al. 2021] Mathew, B.; Saha, P.; Yimam, S. M.; Biemann, C.; Goyal,
    P.; 和 Mukherjee, A. 2021. Hatexplain：一个用于解释性仇恨言论检测的基准数据集。在《AAAI人工智能会议论文集》，第35卷，14867–14875。'
- en: '[Mikolov et al. 2013] Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013.
    Efficient estimation of word representations in vector space. CoRR abs/1301.3781.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mikolov et al. 2013] Mikolov, T.; Chen, K.; Corrado, G.; 和 Dean, J. 2013.
    高效的词表示向量空间估计。CoRR abs/1301.3781。'
- en: '[Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning,
    C. 2014. GloVe: Global vectors for word representation. In Moschitti, A.; Pang,
    B.; and Daelemans, W., eds., Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), 1532–1543. Doha, Qatar: Association for
    Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; 和 Manning,
    C. 2014. GloVe: 全球词向量表示。In Moschitti, A.; Pang, B.; 和 Daelemans, W., 主编，《2014年自然语言处理经验方法会议论文集（EMNLP）》，1532–1543。卡塔尔多哈：计算语言学协会。'
- en: '[Peters, Ruder, and Smith 2019] Peters, M. E.; Ruder, S.; and Smith, N. A.
    2019. To tune or not to tune? adapting pretrained representations to diverse tasks.
    In Augenstein, I.; Gella, S.; Ruder, S.; Kann, K.; Can, B.; Welbl, J.; Conneau,
    A.; Ren, X.; and Rei, M., eds., Proceedings of the 4th Workshop on Representation
    Learning for NLP (RepL4NLP-2019), 7–14. Florence, Italy: Association for Computational
    Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Peters, Ruder, and Smith 2019] Peters, M. E.; Ruder, S.; 和 Smith, N. A. 2019.
    调整还是不调整？将预训练表示适应于多样任务。在Augenstein, I.; Gella, S.; Ruder, S.; Kann, K.; Can, B.;
    Welbl, J.; Conneau, A.; Ren, X.; 和 Rei, M., 主编，《第4届自然语言处理表示学习研讨会论文集（RepL4NLP-2019）》，7–14。意大利佛罗伦萨：计算语言学协会。'
- en: '[Rumelhart, Hinton, and Williams 1986] Rumelhart, D. E.; Hinton, G. E.; and
    Williams, R. J. 1986. Learning Internal Representations by Error Propagation.
    Cambridge, MA, USA: MIT Press. 318–362.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rumelhart, Hinton, and Williams 1986] Rumelhart, D. E.; Hinton, G. E.; 和 Williams,
    R. J. 1986. 通过误差传播学习内部表示。美国马萨诸塞州剑桥：麻省理工学院出版社。318–362。'
- en: '[Sanh et al. 2019] Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv abs/1910.01108.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sanh et al. 2019] Sanh, V.; Debut, L.; Chaumond, J.; 和 Wolf, T. 2019. Distilbert，一种简化版BERT：更小、更快、更便宜、更轻便。ArXiv
    abs/1910.01108。'
- en: '[Sarkar et al. 2020] Sarkar, R.; Mahinder, S.; Sarkar, H.; and KhudaBukhsh,
    A. 2020. Social media attributions in the context of water crisis. In Webber,
    B.; Cohn, T.; He, Y.; and Liu, Y., eds., Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing (EMNLP), 1402–1412. Online: Association
    for Computational Linguistics.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sarkar 等人 2020] Sarkar, R.; Mahinder, S.; Sarkar, H.; 和 KhudaBukhsh, A. 2020.
    社交媒体在水危机背景下的归因研究。收录于 Webber, B.; Cohn, T.; He, Y.; 和 Liu, Y. 编辑的《2020 年自然语言处理经验方法会议论文集
    (EMNLP)》，第 1402–1412 页。在线：计算语言学协会。'
- en: '[Spilka, Shaver, and Kirkpatrick 1985] Spilka, B.; Shaver, P.; and Kirkpatrick,
    L. A. 1985. A general attribution theory for the psychology of religion. Journal
    for the Scientific Study of Religion 24(1):1–20.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spilka, Shaver, 和 Kirkpatrick 1985] Spilka, B.; Shaver, P.; 和 Kirkpatrick,
    L. A. 1985. 宗教心理学的普遍归因理论。科学宗教研究杂志 24(1):1–20。'
- en: '[Swamy, Jamatia, and Gambäck 2019] Swamy, S. D.; Jamatia, A.; and Gambäck,
    B. 2019. Studying generalisability across abusive language detection datasets.
    In Bansal, M., and Villavicencio, A., eds., Proceedings of the 23rd Conference
    on Computational Natural Language Learning (CoNLL), 940–950. Hong Kong, China:
    Association for Computational Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Swamy, Jamatia, 和 Gambäck 2019] Swamy, S. D.; Jamatia, A.; 和 Gambäck, B. 2019.
    跨滥用语言检测数据集的泛化研究。收录于 Bansal, M., 和 Villavicencio, A. 编辑的《第 23 届计算自然语言学习会议论文集 (CoNLL)》，第
    940–950 页。中国香港：计算语言学协会。'
- en: '[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;
    Jones, L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017. Attention is
    all you need. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.;
    Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing
    Systems, volume 30. Curran Associates, Inc.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vaswani 等人 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
    L.; Gomez, A. N.; Kaiser, L. u.; 和 Polosukhin, I. 2017. 注意力机制才是关键。收录于 Guyon, I.;
    Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; 和 Garnett,
    R. 编辑的《神经信息处理系统进展》，第 30 卷。Curran Associates, Inc.'
- en: '[Yang et al. 2019] Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
    R. R.; and Le, Q. V. 2019. Xlnet: Generalized autoregressive pretraining for language
    understanding. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d''Alché-Buc,
    F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Processing
    Systems, volume 32. Curran Associates, Inc.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang 等人 2019] Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.
    R.; 和 Le, Q. V. 2019. Xlnet: 用于语言理解的广义自回归预训练。收录于 Wallach, H.; Larochelle, H.;
    Beygelzimer, A.; d''Alché-Buc, F.; Fox, E.; 和 Garnett, R. 编辑的《神经信息处理系统进展》，第 32
    卷。Curran Associates, Inc.'
- en: '[Zhou and Li 2005] Zhou, Z.-H., and Li, M. 2005. Semi-supervised regression
    with co-training. In Proceedings of the 19th International Joint Conference on
    Artificial Intelligence, IJCAI’05, 908–913. San Francisco, CA, USA: Morgan Kaufmann
    Publishers Inc.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zhou 和 Li 2005] Zhou, Z.-H., 和 Li, M. 2005. 具有协同训练的半监督回归。收录于第 19 届国际人工智能联合会议论文集，IJCAI’05，第
    908–913 页。美国加利福尼亚州旧金山：Morgan Kaufmann Publishers Inc.'
