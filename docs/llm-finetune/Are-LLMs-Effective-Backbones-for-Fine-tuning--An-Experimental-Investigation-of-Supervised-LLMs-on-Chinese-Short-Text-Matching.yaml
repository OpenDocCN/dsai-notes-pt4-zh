- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**大型语言模型（LLMs）**在细化时是否有效？对中文短文本匹配的监督学习模型的实验调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.19930](https://ar5iv.labs.arxiv.org/html/2403.19930)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.19930](https://ar5iv.labs.arxiv.org/html/2403.19930)
- en: Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang**'
- en: Machine Learning Platform Department
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习平台部
- en: Tencent TEG. Beijing, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 腾讯 TEG，北京，中国
- en: '{forestliu, doublecxu, paulhliu, maxwellyu, rigorosyang}@tencent.com'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{forestliu, doublecxu, paulhliu, maxwellyu, rigorosyang}@tencent.com'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The recent success of Large Language Models (LLMs) has garnered significant
    attention in both academia and industry. Prior research on LLMs has primarily
    focused on enhancing or leveraging their generalization capabilities in zero-
    and few-shot settings. However, there has been limited investigation into effectively
    fine-tuning LLMs for a specific natural language understanding task in supervised
    settings. In this study, we conduct an experimental analysis by fine-tuning LLMs
    for the task of Chinese short text matching. We explore various factors that influence
    performance when fine-tuning LLMs, including task modeling methods, prompt formats,
    and output formats.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型（LLMs）**的最近成功在学术界和工业界引起了广泛关注。此前对LLMs的研究主要集中在增强或利用其在零样本和少样本设置中的泛化能力。然而，对于如何在监督学习设置中有效地细化LLMs以应对特定的自然语言理解任务的研究较少。在本研究中，我们通过细化LLMs来处理中文短文本匹配任务进行实验分析。我们探讨了在细化LLMs时影响性能的各种因素，包括任务建模方法、提示格式和输出格式。'
- en: Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型（LLMs）**在细化时是否有效？对中文短文本匹配的监督学习模型的实验调查'
- en: Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang Machine Learning Platform
    Department Tencent TEG. Beijing, China {forestliu, doublecxu, paulhliu, maxwellyu,
    rigorosyang}@tencent.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang** 机器学习平台部 腾讯 TEG，北京，中国
    {forestliu, doublecxu, paulhliu, maxwellyu, rigorosyang}@tencent.com'
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The recent success of Large Language Models (LLMs), such as GPT-3Brown et al.
    ([2020](#bib.bib1)), LLaMATouvron et al. ([2023](#bib.bib16)) and PaLMChowdhery
    et al. ([2023](#bib.bib4)), has garnered significant attention in both academia
    and industry. LLMs have demonstrated remarkable generalization capabilities in
    zero- and few-shot settings, particularly in natural language generation (NLG)
    tasks. Substantial efforts have been made to enhance and utilizing such generalization
    capabilitiesXu et al. ([2023](#bib.bib20)); Saad-Falcon et al. ([2023](#bib.bib15));
    Yun et al. ([2023](#bib.bib22)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型（LLMs）**的最近成功，例如**GPT-3**（Brown et al. ([2020](#bib.bib1)））、**LLaMA**（Touvron
    et al. ([2023](#bib.bib16)））和**PaLM**（Chowdhery et al. ([2023](#bib.bib4)）），在学术界和工业界引起了广泛关注。LLMs在零样本和少样本设置中展现了卓越的泛化能力，特别是在自然语言生成（NLG）任务中。大量努力已被投入以增强和利用这些泛化能力（Xu
    et al. ([2023](#bib.bib20)）; Saad-Falcon et al. ([2023](#bib.bib15)）; Yun et al.
    ([2023](#bib.bib22)））。'
- en: However, for natural language understanding (NLU) tasks, zero- and few-shot
    LLMs struggle to achieve satisfactory performanceNie et al. ([2022](#bib.bib12));
    Wei et al. ([2023](#bib.bib19)); Li et al. ([2023a](#bib.bib7), [b](#bib.bib8))
    compared to fine-tuned small models (e.g., Bert baseDevlin et al. ([2018](#bib.bib5))).
    Our experimental results on the task of Chinese short text matching also confirm
    this phenomenon. As presented in Section[3.1](#S3.SS1 "3.1 Generative vs. Discriminative
    Models ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching"),
    fine-tuned Bert achieves an accuracy of 84.5% on the BQChen et al. ([2018](#bib.bib2))
    corpus, while GPT-4¹¹1The metrics are measured by utilizing OpenAI API., one of
    the most successful LLMs, only attains an accuracy score of 52.9% in zero-shot
    and 77.9% in few-shot settings. There has been limited investigation into effectively
    tuning LLMs for a specific NLU task in supervised settings. In this paper, we
    explore various factors affecting the performance of LLMs for Chinese short text
    matching task, including task modeling methods, prompt formats, and output formats.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于自然语言理解（NLU）任务，零样本和少样本LLM难以实现令人满意的性能Nie et al. ([2022](#bib.bib12)); Wei
    et al. ([2023](#bib.bib19)); Li et al. ([2023a](#bib.bib7), [b](#bib.bib8))，相较于微调的小模型（如Bert
    baseDevlin et al. ([2018](#bib.bib5)))。我们在中文短文本匹配任务上的实验结果也证实了这一现象。如第[3.1节](#S3.SS1
    "3.1 Generative vs. Discriminative Models ‣ 3 Experiments and Results ‣ Are LLMs
    Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised
    LLMs on Chinese Short Text Matching")所示，微调后的Bert在BQChen et al. ([2018](#bib.bib2))语料库上的准确率为84.5%，而GPT-4¹¹1这些指标通过利用OpenAI
    API测量，作为最成功的LLM之一，在零样本情况下仅达到52.9%的准确率，在少样本情况下达到77.9%。对有效微调LLM以应对特定NLU任务的监督设置的研究还很有限。本文探索了影响LLM在中文短文本匹配任务表现的各种因素，包括任务建模方法、提示格式和输出格式。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task modeling methods: In this study, we examine the impacts of modeling this
    task as both a generative task and a discriminative classification task, respectively.
    (1) Generative Task: LLMs uniformly model all tasks as generative tasks. Following
    this principle, we organize the given pair of sentences into a single text as
    input and make the model generate the target label (equivalent or inequivalent).
    (2) Discriminative Classification Task: Motivated by the efficacy of fine-tuning
    Bert for text matchingChen et al. ([2020](#bib.bib3)); Qi et al. ([2022](#bib.bib14)),
    we concatenate the given pair of texts as input, extract vector representations
    from the final LLM layer as features, and perform binary classifications based
    on the extracted features.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务建模方法：在本研究中，我们分别考察了将此任务建模为生成任务和判别分类任务的影响。(1) 生成任务：LLM将所有任务统一建模为生成任务。根据这一原则，我们将给定的句子对组织成单一文本作为输入，并让模型生成目标标签（等效或不等效）。(2)
    判别分类任务：受到Bert在文本匹配中微调效果的启发Chen et al. ([2020](#bib.bib3)); Qi et al. ([2022](#bib.bib14))，我们将给定的文本对拼接为输入，从最终的LLM层提取向量表示作为特征，并基于提取的特征进行二分类。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Formats: Prompt design is crucial for LLMs in zero- and few-shot settingsGu
    et al. ([2021](#bib.bib6)); Liu et al. ([2023](#bib.bib9)). However, the importance
    of prompts in supervised settings has not been explored. In this paper, we compare
    two completely different styles of prompts. One is concise, directly concatenating
    the given pair of sentences without any explanation of the target task. The other
    organizes the prompt through complex instructions, including not only the given
    sentences but also a detail description of the target task.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示格式：提示设计对于零样本和少样本设置中的LLM至关重要Gu et al. ([2021](#bib.bib6)); Liu et al. ([2023](#bib.bib9))。然而，提示在监督设置中的重要性尚未被探讨。本文比较了两种完全不同风格的提示。一种是简洁的，直接将给定的句子对拼接在一起，而不对目标任务做任何解释。另一种则通过复杂的指令组织提示，包括不仅仅是给定的句子，还包括对目标任务的详细描述。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output Formats: Incorporating the Chain of Thought (CoT) into prompts has been
    shown to significantly enhance performance in reasoning and complex tasks in zero-
    and few-shot settingsWei et al. ([2022](#bib.bib18)); Wang et al. ([2022](#bib.bib17)).
    Nevertheless, the impact of CoT on matching tasks in supervised settings has yet
    to be examined. In this study, we address this gap by incorporating CoT into the
    output part of training samples.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出格式：将思维链 (CoT) 纳入提示中已被证明能显著提升在零样本和少量样本设置中的推理和复杂任务性能 Wei 等人 ([2022](#bib.bib18))；Wang
    等人 ([2022](#bib.bib17))。然而，CoT 对于监督设置中的匹配任务的影响尚未被研究。在本研究中，我们通过将 CoT 纳入训练样本的输出部分来填补这一空白。
- en: We conduct experiments on two widely-used Chinese short text matching datasets,
    LCQMC Liu et al. ([2018a](#bib.bib10)) and BQ Chen et al. ([2018](#bib.bib2)).
    All experiments are carried out based on CLLM-7B, which is a Chinese-enhanced
    model based on LLaMA-2-7B. Our preliminary results demonstrate that the fine-tuned
    CLLM-7B outperforms both fine-tuned BERT and few-shot GPT-4\. Furthermore, the
    results indicate that the generative paradigm surpasses the discriminative approach,
    especially when training data is limited. Lastly, our experiments reveal that
    CoT is also beneficial for the matching task in supervised settings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个广泛使用的中文短文本匹配数据集上进行实验，LCQMC Liu 等人 ([2018a](#bib.bib10)) 和 BQ Chen 等人 ([2018](#bib.bib2))。所有实验都基于
    CLLM-7B 进行，该模型是基于 LLaMA-2-7B 的中文增强模型。我们的初步结果显示，经过微调的 CLLM-7B 优于经过微调的 BERT 和少量示例的
    GPT-4。此外，结果还表明，在训练数据有限的情况下，生成范式超越了判别方法。最后，我们的实验还揭示了 CoT 对于监督环境中的匹配任务也是有益的。
- en: 2 Backgrounds
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: In this section, we provide a brief overview of the Chinese short text matching
    task and the datasets employed in this study.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要概述了中文短文本匹配任务及本研究中使用的数据集。
- en: 2.1 Task Definition
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 任务定义
- en: Chinese short text matching, often regarded as a task of identifying sentence
    semantic equivalence, is a fundamental task of natural language processing. Given
    a pair of sentences, the goal of a matching model is to ascertain their semantic
    equivalence. Short text matching is extensively utilized in a range of NLP tasks,
    such as question answering Liu et al. ([2018b](#bib.bib11)) and dialogue systems
    Pang et al. ([2008](#bib.bib13)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 中文短文本匹配，通常被视为识别句子语义等价性的任务，是自然语言处理中的一个基础任务。给定一对句子，匹配模型的目标是确定它们的语义等价性。短文本匹配在问答系统
    Liu 等人 ([2018b](#bib.bib11)) 和对话系统 Pang 等人 ([2008](#bib.bib13)) 等多种 NLP 任务中得到广泛应用。
- en: 2.2 Datasets and Metrics
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据集与指标
- en: 'We conduct experiments on two widely-used Chinese short text matching corpora:
    LCQMC Liu et al. ([2018a](#bib.bib10)) and BQ Chen et al. ([2018](#bib.bib2)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个广泛使用的中文短文本匹配语料库上进行实验：LCQMC Liu 等人 ([2018a](#bib.bib10)) 和 BQ Chen 等人 ([2018](#bib.bib2))。
- en: LCQMC is a large-scale, open-domain question matching corpus. It comprises 260,068
    Chinese search query pairs, including 238,766 training samples, 8,802 development
    samples, and 12,500 test samples. Each pair is annotated with a binary label indicating
    whether the two queries share the same intention.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LCQMC 是一个大规模的开放领域问题匹配语料库。它包括 260,068 对中文搜索查询，其中 238,766 对为训练样本，8,802 对为开发样本，12,500
    对为测试样本。每对查询都被标注了一个二元标签，表示这两个查询是否具有相同的意图。
- en: BQ is a domain-specific, large-scale corpus for bank question matching. It consists
    of 120,000 Chinese sentence pairs, including 100,000 training samples, 10,000
    development samples, and 10,000 test samples. Each pair is also annotated with
    a binary label indicating whether the two sentences convey the same meaning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: BQ 是一个用于银行问题匹配的领域特定大规模语料库。它包含 120,000 对中文句子，其中 100,000 对为训练样本，10,000 对为开发样本，10,000
    对为测试样本。每对句子也被标注了一个二元标签，表示这两个句子是否传达相同的意思。
- en: We employ accuracy (ACC.) as the evaluation metric, which is the percentage
    of correctly predicted examples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用准确率 (ACC.) 作为评估指标，它是正确预测示例的百分比。
- en: '![Refer to caption](img/546f437fb1487ccce54944a0fbe321dd.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/546f437fb1487ccce54944a0fbe321dd.png)'
- en: 'Figure 1: Model structures of modeling text matching as generative and discriminant
    task.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：将文本匹配建模为生成任务和判别任务的模型结构。
- en: 3 Experiments and Results
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验与结果
- en: In this section, we outline the experimental configurations and present the
    results. We examine the influence of the three factors discussed in Section [1](#S1
    "1 Introduction ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching") through the
    following experiments. We tune models via full-model fine-tuning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了实验配置并展示了结果。我们通过以下实验检查了第 [1](#S1 "1 Introduction ‣ Are LLMs Effective
    Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on
    Chinese Short Text Matching") 节中讨论的三个因素的影响。我们通过全模型微调来调整模型。
- en: 3.1 Generative vs. Discriminative Models
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 生成模型与判别模型
- en: We first outline our approach to fine-tuning LLMs by modeling the matching task
    as both a generative task and a discriminative task. Subsequently, we present
    the results and provide an analysis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先概述了通过将匹配任务建模为生成任务和判别任务来微调 LLMs 的方法。随后，我们展示了结果并提供了分析。
- en: 'Modeling as A Generative Task: LLMs consistently treat all tasks as generative
    tasks. In line with this principle, we merge the provided pair of sentences with
    instructions into a single text input and prompt the model to generate the target
    label. We refer to this model as CLLM-7B-GEN. Figure [1](#S2.F1 "Figure 1 ‣ 2.2
    Datasets and Metrics ‣ 2 Backgrounds ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")(b)
    illustrates the model structure. We optimize it by maximizing the generation probability
    of the target label.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生成任务建模：LLMs 一贯将所有任务视为生成任务。根据这一原则，我们将提供的句子对与指令合并为一个文本输入，并提示模型生成目标标签。我们将该模型称为
    CLLM-7B-GEN。图 [1](#S2.F1 "Figure 1 ‣ 2.2 Datasets and Metrics ‣ 2 Backgrounds
    ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching")(b) 说明了模型结构。我们通过最大化目标标签的生成概率来优化它。
- en: 'Modeling as A Discriminative Task: Inspired by the effectiveness of fine-tuning
    BERT for text matching tasks (see Figure [1](#S2.F1 "Figure 1 ‣ 2.2 Datasets and
    Metrics ‣ 2 Backgrounds ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching")(a)), we concatenate
    the given pair of texts as input, extract vector representations from the final
    LLM layer as features, and perform binary classification based on the extracted
    features. We refer to this model as CLLM-7B-CLS. Figure [1](#S2.F1 "Figure 1 ‣
    2.2 Datasets and Metrics ‣ 2 Backgrounds ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")(c)
    demonstrates the model structure.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为判别任务建模：受到微调 BERT 在文本匹配任务中效果的启发（见图 [1](#S2.F1 "Figure 1 ‣ 2.2 Datasets and
    Metrics ‣ 2 Backgrounds ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching")(a)），我们将给定的文本对作为输入进行拼接，从最终
    LLM 层提取向量表示作为特征，并根据提取的特征进行二分类。我们将该模型称为 CLLM-7B-CLS。图 [1](#S2.F1 "Figure 1 ‣ 2.2
    Datasets and Metrics ‣ 2 Backgrounds ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")(c)
    展示了模型结构。
- en: '![Refer to caption](img/b5bd2a7635059e3ea5ba149e17a82fcb.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b5bd2a7635059e3ea5ba149e17a82fcb.png)'
- en: 'Figure 2: The results of models trained on 5,000, 20,000, 80,000 samples as
    well as trained on the entire training set.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：训练在 5,000、20,000 和 80,000 个样本上的模型结果，以及在整个训练集上的训练结果。
- en: 'We validated the performance of generative and discriminative models on training
    sets of different scales. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Generative vs. Discriminative
    Models ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")
    shows the experimental results, where the 2-shot GPT-4 results are measured by
    calling the official OpenAI API. Figure [6](#A1.F6 "Figure 6 ‣ Appendix A Appendix
    ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching") and Figure [7](#A1.F7 "Figure
    7 ‣ Appendix A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching") in Appendix
    [A](#A1 "Appendix A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An
    Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")
    illustrate the 2-shot prompts for LCQMC and BQ, respectively. From the results,
    we observe that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了生成模型和判别模型在不同规模训练集上的表现。图 [2](#S3.F2 "Figure 2 ‣ 3.1 Generative vs. Discriminative
    Models ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")
    显示了实验结果，其中 2-shot GPT-4 的结果通过调用官方 OpenAI API 进行测量。图 [6](#A1.F6 "Figure 6 ‣ Appendix
    A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching") 和图 [7](#A1.F7 "Figure 7 ‣
    Appendix A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching") 在附录 [A](#A1
    "Appendix A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching") 中分别展示了 LCQMC
    和 BQ 的 2-shot 提示。通过结果，我们观察到：
- en: 1) When the number of training samples is less than 20,000, CLLM-GEN significantly
    outperforms discriminative models, including BERT and CLLM-CLS, on both LCQMC
    and BQ. This phenomenon is quite intuitive, as the generative approach aligns
    with the pre-training procedure, making it easier to activate the knowledge acquired
    by the model during pre-training. Furthermore, due to the massive amount of data
    used in the pre-training phase of LLMs, the issue of evaluation data leakage cannot
    be ignored Yang et al. ([2023](#bib.bib21)); Zhou et al. ([2023](#bib.bib23)).
    To determine whether CLLM-7B has a data leakage problem, we conducted zero-shot
    experiments on it. The model achieves an accuracy of 52.1% on LCQMC and 52.9%
    on BQ, slightly better than the 50% expected from random guessing. Consequently,
    we believe that both BQ and LCQMC are not included in CLLM-7B’s pre-training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 当训练样本少于 20,000 时，CLLM-GEN 在 LCQMC 和 BQ 上显著优于包括 BERT 和 CLLM-CLS 在内的判别模型。这一现象相当直观，因为生成方法与预训练过程一致，使得激活模型在预训练期间获得的知识更容易。此外，由于
    LLM 预训练阶段使用的数据量巨大，评估数据泄露问题不容忽视（Yang et al. ([2023](#bib.bib21)); Zhou et al. ([2023](#bib.bib23))）。为了确定
    CLLM-7B 是否存在数据泄露问题，我们对其进行了零-shot 实验。该模型在 LCQMC 上的准确率为 52.1%，在 BQ 上为 52.9%，略高于随机猜测的
    50%。因此，我们认为 BQ 和 LCQMC 并未包含在 CLLM-7B 的预训练数据中。
- en: 2) The performance of 2-shot GPT-4 on BQ is much worse than that of supervised
    models. This is mainly because BQ is a dataset of real customer service questions
    from WeBank Inc., and a full understanding of the sentences’ meaning requires
    background information about this bank. For example, questions in BQ usually mention
    specific products or a particular function in the bank’s app. This background
    knowledge is unknown to CLLM and is also impossible to provide entirely in the
    prompt.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 2-shot GPT-4 在 BQ 上的表现远逊于监督模型。这主要是因为 BQ 是来自 WeBank Inc. 的真实客服问题数据集，对句子意义的全面理解需要关于该银行的背景信息。例如，BQ
    中的问题通常提及银行应用中的特定产品或功能。这些背景知识对 CLLM 来说是未知的，并且在提示中也无法完全提供。
- en: 3) CLLM-GEN trained on the whole training corpus on LCQMC outperforms BERT.
    However, it fails on the BQ corpus. We believe the reason is that CLLM-7B, like
    BERT, also lack knowledge of WeBank, and such knowledge can only be obtained from
    the training data. Therefore, compared to BERT, CLLM-7B does not have an advantage
    on this dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 在 LCQMC 上使用整个训练语料库进行训练的 CLLM-GEN 优于 BERT。然而，它在 BQ 语料库上表现不佳。我们认为原因是 CLLM-7B
    像 BERT 一样缺乏 WeBank 的知识，这种知识只能从训练数据中获得。因此，与 BERT 相比，CLLM-7B 在这个数据集上没有优势。
- en: The above experiments demonstrate that generative paradigm is better for supervised
    LLMs. Therefore, all subsequent experiments will be conducted following this paradigm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实验表明，生成范式对监督式 LLM 更为有效。因此，所有后续实验将遵循这一范式。
- en: 3.2 Concise vs. Complex Prompts
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 简洁提示 vs. 复杂提示
- en: '![Refer to caption](img/a56b708a210b3624b50d4726929f2a8d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a56b708a210b3624b50d4726929f2a8d.png)'
- en: 'Figure 3: The results of concise and complex prompts.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：简洁与复杂提示的结果。
- en: Prompt design is crucial for LLMs in zero- and few-shot settings. However, the
    significance of prompts in supervised settings remains unexplored. In this subsection,
    we compare two distinct styles of prompts. The concise prompt involves directly
    concatenating the given text pairs without any explanation of the target task,
    while the complex prompt organizes the prompt with detailed instructions, incorporating
    not only the given texts but also a specific description of the target task. Examples
    of these prompts can be found in Figure [8](#A1.F8 "Figure 8 ‣ Appendix A Appendix
    ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching") in Appendix [A](#A1 "Appendix
    A Appendix ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching").
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计对LLM在零样本和少样本设置下至关重要。然而，提示在监督设置中的重要性仍未被探索。在本小节中，我们比较了两种不同风格的提示。简洁提示直接将给定的文本对连接在一起，没有关于目标任务的任何解释，而复杂提示则用详细的说明组织提示，结合了给定的文本和对目标任务的具体描述。这些提示的示例可以在附录[A](#A1
    "附录A ‣ LLM是否有效的骨干？对中文短文本匹配的监督LLM的实验研究")中的图[8](#A1.F8 "图8 ‣ 附录A ‣ LLM是否有效的骨干？对中文短文本匹配的监督LLM的实验研究")中找到。
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Concise vs. Complex Prompts ‣ 3 Experiments
    and Results ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
    of Supervised LLMs on Chinese Short Text Matching") presents the results, showing
    that models separately trained by concise and complex prompts achieve comparable
    performance. This observation suggests that supervised LLMs are not sensitive
    to prompts. The primary function of a complex prompt is to enhance the model’s
    comprehension of the target task. In supervised scenarios, the model can learn
    the task definition more accurately from the training data, rendering the prompt
    design less impactful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S3.F3 "图3 ‣ 3.2 简洁与复杂提示 ‣ 3 实验与结果 ‣ LLM是否有效的骨干？对中文短文本匹配的监督LLM的实验研究")展示了结果，表明通过简洁和复杂提示分别训练的模型达到了相似的性能。这一观察结果表明，监督LLM对提示不敏感。复杂提示的主要功能是增强模型对目标任务的理解。在监督场景下，模型可以从训练数据中更准确地学习任务定义，从而使提示设计的影响力较小。
- en: 3.3 Effects of CoT
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 CoT的效果
- en: '![Refer to caption](img/ba69e440111fbe19f6a8ecd82b997d25.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ba69e440111fbe19f6a8ecd82b997d25.png)'
- en: 'Figure 4: Illustration of how to obtain CoT via GPT-4\. All original texts
    in this figure are in Chinese. For ease of reading, we translated them. The original
    version is illustrated in Figure [9](#A1.F9 "Figure 9 ‣ Appendix A Appendix ‣
    Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of
    Supervised LLMs on Chinese Short Text Matching") in Appendix.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过GPT-4获得CoT的说明。本图中的所有原始文本均为中文。为了便于阅读，我们进行了翻译。原始版本在附录中的图[9](#A1.F9 "图9 ‣
    附录A ‣ LLM是否有效的骨干？对中文短文本匹配的监督LLM的实验研究")中有所展示。
- en: CoT has demonstrated its effectiveness in reasoning and complex tasks within
    zero- and few-shot settings. However, its efficacy for language understanding
    tasks in supervised settings remains unexplored.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CoT在零样本和少样本设置下展示了其在推理和复杂任务中的有效性。然而，它在监督设置下对语言理解任务的有效性仍未被探索。
- en: We have already demonstrated in Section [3.2](#S3.SS2 "3.2 Concise vs. Complex
    Prompts ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones for Fine-tuning?
    An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")
    that adding additional information to the prompt does not improve performance
    in the supervised setting. Therefore, unlike in zero/few-shot settings, we did
    not include CoT in the prompt, but instead added it to the output section. Figure
    [10](#A1.F10 "Figure 10 ‣ Appendix A Appendix ‣ Are LLMs Effective Backbones for
    Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short
    Text Matching") in Appendix A presents a training sample with CoT.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第 [3.2](#S3.SS2 "3.2 简洁与复杂提示 ‣ 3 实验与结果 ‣ LLM 是否有效作为微调的核心？对中文短文本匹配的监督 LLM
    实验研究") 节中展示了，向提示中添加额外信息不会改善监督设置中的表现。因此，与零样本/少量样本设置不同，我们没有在提示中包含 CoT，而是将其添加到输出部分。附录
    A 中的图 [10](#A1.F10 "图 10 ‣ 附录 A 附录 ‣ LLM 是否有效作为微调的核心？对中文短文本匹配的监督 LLM 实验研究") 展示了一个包含
    CoT 的训练样本。
- en: Matching datasets provide labels without CoT. To obtain CoT for the training
    set, we enlist GPT-4 to determine whether a given pair of texts is equivalent,
    while also providing explanations for its decision. For samples where GPT-4’s
    judgment aligns with the golden label, we utilize the explanation as the CoT.
    Conversely, for inconsistent samples, we retain only golden label. Figure [4](#S3.F4
    "Figure 4 ‣ 3.3 Effects of CoT ‣ 3 Experiments and Results ‣ Are LLMs Effective
    Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on
    Chinese Short Text Matching") depicts the designed prompt and response generated
    by GPT-4\. Note that only the output portion of the training samples requires
    the addition of CoT. Figure [10](#A1.F10 "Figure 10 ‣ Appendix A Appendix ‣ Are
    LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised
    LLMs on Chinese Short Text Matching") in Appendix presents a training sample that
    includes CoT. During the evaluation process, we disregard the CoT generated by
    the model, focusing solely on the label "same" or "different".
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配数据集提供了没有 CoT 的标签。为了获取训练集的 CoT，我们让 GPT-4 判断给定的文本对是否等价，同时提供其决定的解释。对于 GPT-4 的判断与真实标签一致的样本，我们将解释作为
    CoT。相反，对于不一致的样本，我们只保留真实标签。图 [4](#S3.F4 "图 4 ‣ 3.3 CoT 效果 ‣ 3 实验与结果 ‣ LLM 是否有效作为微调的核心？对中文短文本匹配的监督
    LLM 实验研究") 展示了设计的提示和 GPT-4 生成的回应。请注意，仅训练样本的输出部分需要添加 CoT。附录中的图 [10](#A1.F10 "图
    10 ‣ 附录 A 附录 ‣ LLM 是否有效作为微调的核心？对中文短文本匹配的监督 LLM 实验研究") 展示了一个包含 CoT 的训练样本。在评估过程中，我们忽略了模型生成的
    CoT，仅关注标签“相同”或“不同”。
- en: In order to reduce the cost, we did not obtain CoT for the entire training set.
    Instead, we separately sampled 10,000 instances from each dataset and requested
    GPT-4 to generate CoT. After filtering samples with inconsistent judgments, approximately
    86% of samples in LCQMC and 78% in BQ retained CoT.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低成本，我们没有为整个训练集获取 CoT。相反，我们从每个数据集中分别抽取了 10,000 个实例，并请求 GPT-4 生成 CoT。在筛选掉判断不一致的样本后，LCQMC
    中约 86% 的样本和 BQ 中 78% 的样本保留了 CoT。
- en: We conducted experiments on training sets of varying scales. Figure [5](#S3.F5
    "Figure 5 ‣ 3.3 Effects of CoT ‣ 3 Experiments and Results ‣ Are LLMs Effective
    Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on
    Chinese Short Text Matching") displays the results, from which we observe that
    CoT improves performance on both LCQMC and BQ. Furthermore, the BQ dataset is
    more challenging than LCQMC, and CLLM-GEN-CoT achieved a more substantial improvement
    on BQ. This finding suggests that CoT may be particularly effective for difficult
    tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同规模的训练集上进行了实验。图 [5](#S3.F5 "图 5 ‣ 3.3 CoT 效果 ‣ 3 实验与结果 ‣ LLM 是否有效作为微调的核心？对中文短文本匹配的监督
    LLM 实验研究") 显示了结果，我们观察到 CoT 在 LCQMC 和 BQ 上都提高了性能。此外，BQ 数据集比 LCQMC 更具挑战性，CLLM-GEN-CoT
    在 BQ 上取得了更显著的改善。这一发现表明，CoT 对于困难任务可能特别有效。
- en: '![Refer to caption](img/73f58089a338caff651a0e2b524cc8d3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/73f58089a338caff651a0e2b524cc8d3.png)'
- en: 'Figure 5: Results of models trained with CoT.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 CoT 训练模型的结果。
- en: 4 Conclusions
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In this work, we conduct an experimental study by fine-tuning LLMs on the task
    of Chinese short text matching. We investigate various factors affecting performance
    in tuning LLMs, including task modeling methods, prompt formats, and the chain
    of thought. We systematically carry out experiments on two widely used datasets.
    The results reveal several insights. First, the fine-tuned CLLM-7B outperforms
    both fine-tuned BERT and few-shot GPT-4, indicating that LLMs serve as effective
    backbones in supervised scenarios. Moreover, the generative paradigm is superior
    to the discriminative one, particularly when training data is limited. Second,
    supervised LLMs are insensitive to prompts, unlike zero- and few-shot LLMs. Third,
    CoT is also beneficial for supervised text matching. Although our experiments
    focus on the task of text matching, the observations may be applicable to other
    NLU tasks, such as text classification.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们通过对LLMs进行微调以处理中文短文本匹配任务，进行了一项实验研究。我们调查了影响LLMs性能的各种因素，包括任务建模方法、提示格式和思维链。我们系统地在两个广泛使用的数据集上进行了实验。结果揭示了几个见解。首先，微调后的CLLM-7B在性能上优于微调后的BERT和少样本GPT-4，表明LLMs在监督场景中作为有效的骨干。此外，生成范式优于判别范式，特别是在训练数据有限时。其次，监督LLMs对提示不敏感，这与零样本和少样本LLMs不同。第三，CoT对监督文本匹配也有益。尽管我们的实验集中在文本匹配任务上，但这些观察可能适用于其他自然语言理解任务，如文本分类。
- en: Limitations
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'This study has two primary limitations: (1) Prompt engineering is crucial for
    zero- and few-shot LLMs. We assessed the few-shot performance of GPT-4, as depicted
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Generative vs. Discriminative Models ‣ 3
    Experiments and Results ‣ Are LLMs Effective Backbones for Fine-tuning? An Experimental
    Investigation of Supervised LLMs on Chinese Short Text Matching"). Despite our
    meticulous design of the few-shot prompts, the prompt designs remain subjective
    and may not necessarily represent the most optimal choices. (2) This study concentrates
    on the text matching task. Additional experiments might be required to adequately
    demonstrate if the conclusions drawn in this article are applicable to other NLU
    tasks (e.g. text classification).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究有两个主要限制：（1）提示工程对于零样本和少样本LLM至关重要。我们评估了GPT-4的少样本表现，如图[2](#S3.F2 "Figure 2 ‣
    3.1 Generative vs. Discriminative Models ‣ 3 Experiments and Results ‣ Are LLMs
    Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised
    LLMs on Chinese Short Text Matching")所示。尽管我们精心设计了少样本提示，但提示设计仍然是主观的，可能不一定代表最优选择。（2）本研究集中于文本匹配任务。可能需要额外的实验来充分展示本文得出的结论是否适用于其他自然语言理解任务（例如文本分类）。
- en: References
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in
    neural information processing systems*, 33:1877–1901.
- en: 'Chen et al. (2018) Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu,
    and Buzhou Tang. 2018. The bq corpus: A large-scale domain-specific chinese corpus
    for sentence semantic equivalence identification. In *Proceedings of the 2018
    conference on empirical methods in natural language processing*, pages 4946–4951.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2018) Jing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu,
    and Buzhou Tang. 2018. The bq corpus: A large-scale domain-specific chinese corpus
    for sentence semantic equivalence identification. In *Proceedings of the 2018
    conference on empirical methods in natural language processing*, pages 4946–4951.'
- en: Chen et al. (2020) Lu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi Chen, Su Zhu,
    and Kai Yu. 2020. Neural graph matching networks for chinese short text matching.
    In *Proceedings of the 58th annual meeting of the Association for Computational
    Linguistics*, pages 6152–6158.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Lu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi Chen, Su
    Zhu, and Kai Yu. 2020. Neural graph matching networks for chinese short text matching.
    In *Proceedings of the 58th annual meeting of the Association for Computational
    Linguistics*, pages 6152–6158.
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等（2018）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2018年。《Bert:
    深度双向变换器的预训练用于语言理解》。*arXiv 预印本 arXiv:1810.04805*。'
- en: 'Gu et al. (2021) Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt:
    Pre-trained prompt tuning for few-shot learning. *arXiv preprint arXiv:2109.04332*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等（2021）Yuxian Gu、Xu Han、Zhiyuan Liu 和 Minlie Huang。2021年。《Ppt: 预训练提示调优用于少样本学习》。*arXiv
    预印本 arXiv:2109.04332*。'
- en: 'Li et al. (2023a) Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen
    Zhao, and Shikun Zhang. 2023a. Evaluating chatgpt’s information extraction capabilities:
    An assessment of performance, explainability, calibration, and faithfulness. *arXiv
    preprint arXiv:2304.11633*.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023a）Bo Li、Gexiang Fang、Yang Yang、Quansen Wang、Wei Ye、Wen Zhao 和 Shikun
    Zhang。2023a年。《评估 ChatGPT 的信息提取能力：对性能、可解释性、校准和忠实性的评估》。*arXiv 预印本 arXiv:2304.11633*。
- en: Li et al. (2023b) Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li,
    Fu-lee Wang, Qing Li, and Xiaoqin Zhong. 2023b. Label supervised llama finetuning.
    *arXiv preprint arXiv:2310.01208*.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023b）Zongxi Li、Xianming Li、Yuzhang Liu、Haoran Xie、Jing Li、Fu-lee Wang、Qing
    Li 和 Xiaoqin Zhong。2023b年。《标签监督的 Llama 微调》。*arXiv 预印本 arXiv:2310.01208*。
- en: 'Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Pengfei Liu、Weizhe Yuan、Jinlan Fu、Zhengbao Jiang、Hiroaki Hayashi
    和 Graham Neubig。2023年。《预训练、提示和预测：自然语言处理中的提示方法系统综述》。*ACM 计算机调查*，55(9)：1–35。
- en: 'Liu et al. (2018a) Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen,
    Dongfang Li, and Buzhou Tang. 2018a. Lcqmc: A large-scale chinese question matching
    corpus. In *Proceedings of the 27th international conference on computational
    linguistics*, pages 1952–1962.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2018a）Xin Liu、Qingcai Chen、Chong Deng、Huajun Zeng、Jing Chen、Dongfang
    Li 和 Buzhou Tang。2018a年。《lcqmc: 大规模中文问题匹配语料库》。在 *第27届国际计算语言学会议论文集*，第1952–1962页。'
- en: Liu et al. (2018b) Yang Liu, Wenge Rong, and Zhang Xiong. 2018b. Improved text
    matching by enhancing mutual information. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 32.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018b）Yang Liu、Wenge Rong 和 Zhang Xiong。2018b年。《通过增强互信息改进文本匹配》。在 *AAAI
    人工智能大会论文集*，第32卷。
- en: Nie et al. (2022) Feng Nie, Meixi Chen, Zhirui Zhang, and Xu Cheng. 2022. Improving
    few-shot performance of language models via nearest neighbor calibration. *arXiv
    preprint arXiv:2212.02216*.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等（2022）Feng Nie、Meixi Chen、Zhirui Zhang 和 Xu Cheng。2022年。《通过最近邻校准提高语言模型的少样本性能》。*arXiv
    预印本 arXiv:2212.02216*。
- en: Pang et al. (2008) Bo Pang, Lillian Lee, et al. 2008. Foundations and trends®
    in information retrieval. *Foundations and Trends® in Information Retrieval*,
    2(1-2):1–135.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等（2008）Bo Pang、Lillian Lee 等。2008年。《信息检索的基础和趋势®》。*信息检索的基础和趋势®*，2(1-2)：1–135。
- en: 'Qi et al. (2022) Le Qi, Yu Zhang, Qingyu Yin, Guidong Zheng, Wen Junjie, Jinlong
    Li, and Ting Liu. 2022. All information is valuable: Question matching over full
    information transmission network. In *Findings of the Association for Computational
    Linguistics: NAACL 2022*, pages 1431–1440.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2022）Le Qi、Yu Zhang、Qingyu Yin、Guidong Zheng、Wen Junjie、Jinlong Li 和 Ting
    Liu。2022年。《所有信息都是有价值的：在全面信息传输网络上的问题匹配》。在 *计算语言学协会发现：NAACL 2022*，第1431–1440页。
- en: 'Saad-Falcon et al. (2023) Jon Saad-Falcon, Omar Khattab, Keshav Santhanam,
    Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Sultan, and Christopher
    Potts. 2023. [UDAPDR: Unsupervised domain adaptation via LLM prompting and distillation
    of rerankers](https://doi.org/10.18653/v1/2023.emnlp-main.693). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    11265–11279, Singapore. Association for Computational Linguistics.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saad-Falcon 等（2023）Jon Saad-Falcon、Omar Khattab、Keshav Santhanam、Radu Florian、Martin
    Franz、Salim Roukos、Avirup Sil、Md Sultan 和 Christopher Potts。2023年。[UDAPDR: 通过
    LLM 提示和重新排序器的蒸馏进行无监督领域适应](https://doi.org/10.18653/v1/2023.emnlp-main.693)。在 *2023年自然语言处理实证方法会议论文集*，第11265–11279页，新加坡。计算语言学协会。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。2023年。《Llama: 开放和高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*。'
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自一致性提升语言模型中的思维链推理。 *arXiv
    预印本 arXiv:2203.11171*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等人。2022. 思维链提示在大型语言模型中引发推理。 *神经信息处理系统进展*，35:24824–24837。
- en: Wei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,
    Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023. Zero-shot
    information extraction via chatting with chatgpt. *arXiv preprint arXiv:2302.10205*.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,
    Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, 等人。2023. 通过与 ChatGPT
    对话进行零样本信息提取。 *arXiv 预印本 arXiv:2302.10205*。
- en: Xu et al. (2023) Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, Meng-Hsi
    Wu, Sina Semnani, and Monica Lam. 2023. [Fine-tuned LLMs know more, hallucinate
    less with few-shot sequence-to-sequence semantic parsing over Wikidata](https://doi.org/10.18653/v1/2023.emnlp-main.353).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 5778–5791, Singapore. Association for Computational Linguistics.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, Meng-Hsi
    Wu, Sina Semnani, 和 Monica Lam. 2023. [经过微调的大语言模型在 Wikidata 上进行少量样本序列到序列的语义解析时，了解更多、虚构更少](https://doi.org/10.18653/v1/2023.emnlp-main.353)。在
    *2023 年自然语言处理经验方法会议论文集*，页码 5778–5791，新加坡。计算语言学协会。
- en: Yang et al. (2023) Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez,
    and Ion Stoica. 2023. Rethinking benchmark and contamination for language models
    with rephrased samples. *arXiv preprint arXiv:2311.04850*.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez,
    和 Ion Stoica. 2023. 重新思考语言模型的基准和污染问题，通过改写样本。 *arXiv 预印本 arXiv:2311.04850*。
- en: Yun et al. (2023) Hye Yun, Iain Marshall, Thomas Trikalinos, and Byron Wallace.
    2023. [Appraising the potential uses and harms of LLMs for medical systematic
    reviews](https://doi.org/10.18653/v1/2023.emnlp-main.626). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    10122–10139, Singapore. Association for Computational Linguistics.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun et al. (2023) Hye Yun, Iain Marshall, Thomas Trikalinos, 和 Byron Wallace.
    2023. [评估大语言模型在医学系统评价中的潜在应用和风险](https://doi.org/10.18653/v1/2023.emnlp-main.626)。在
    *2023 年自然语言处理经验方法会议论文集*，页码 10122–10139，新加坡。计算语言学协会。
- en: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don’t make your
    llm an evaluation benchmark cheater. *arXiv preprint arXiv:2311.01964*.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, 和 Jiawei Han. 2023. 不要让你的大语言模型成为评估基准的作弊者。
    *arXiv 预印本 arXiv:2311.01964*。
- en: Appendix A Appendix
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: '![Refer to caption](img/c67d97c03d7fc55d6be6ce4b7056160f.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c67d97c03d7fc55d6be6ce4b7056160f.png)'
- en: 'Figure 6: An illustration of 2-shot prompt for LCQMC.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LCQMC 的 2-shot 提示示意图。
- en: '![Refer to caption](img/90ac1e0b1ccdd4d2381cfc0abc2215cb.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/90ac1e0b1ccdd4d2381cfc0abc2215cb.png)'
- en: 'Figure 7: An illustration of 2-shot prompt for BQ.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：BQ 的 2-shot 提示示意图。
- en: '![Refer to caption](img/4e9e9c805c22e3d217abccdb09bbf21d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e9e9c805c22e3d217abccdb09bbf21d.png)'
- en: 'Figure 8: Examples of complex and simple prompts in Section[3.2](#S3.SS2 "3.2
    Concise vs. Complex Prompts ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones
    for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short
    Text Matching")'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：第 [3.2](#S3.SS2 "3.2 简洁 vs. 复杂提示 ‣ 3 实验与结果 ‣ 大语言模型在微调中的有效性？对中文短文本匹配的监督大语言模型的实验研究")
    节中的复杂和简单提示示例。
- en: '![Refer to caption](img/e9aa6a124a84a98dac18704745acc3db.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9aa6a124a84a98dac18704745acc3db.png)'
- en: 'Figure 9: The Chinese version of texts in Figure [4](#S3.F4 "Figure 4 ‣ 3.3
    Effects of CoT ‣ 3 Experiments and Results ‣ Are LLMs Effective Backbones for
    Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short
    Text Matching")'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：图 [4](#S3.F4 "图 4 ‣ 3.3 CoT 的影响 ‣ 3 实验与结果 ‣ 大语言模型在微调中的有效性？对中文短文本匹配的监督大语言模型的实验研究")
    中文本的中文版本。
- en: '![Refer to caption](img/55d56f694d41de64efa938935928b07e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/55d56f694d41de64efa938935928b07e.png)'
- en: 'Figure 10: An example of training sample with CoT.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 带有 CoT 的训练样本示例。'
