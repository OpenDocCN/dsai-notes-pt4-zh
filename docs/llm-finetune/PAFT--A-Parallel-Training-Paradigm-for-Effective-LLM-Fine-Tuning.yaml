- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:35:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PAFT：一种有效的LLM微调的并行训练范式
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17923](https://ar5iv.labs.arxiv.org/html/2406.17923)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17923](https://ar5iv.labs.arxiv.org/html/2406.17923)
- en: Shiva Kumar Pentyala^*, Zhichao Wang^*, Bin Bi^*, Kiran Ramnath,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 希瓦·库马尔·彭提亚^*、志超·王^*、斌·毕^*、基兰·拉姆纳特，
- en: Xiang-Bo Mao, Regunathan Radhakrishnan, Sitaram Asur, Na (Claire) Cheng
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 蔡博毛、雷贡纳坦·拉达克里希南、斯塔拉姆·阿苏尔、娜（克莱尔）程
- en: Salesforce
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Salesforce
- en: '{shivakumar.pentyala, zhichaowang, bin.bi, k.ramnath,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{shivakumar.pentyala, zhichaowang, bin.bi, k.ramnath,'
- en: xmao, rradhakrishnan, sasur, claire.cheng}@salesforce.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: xmao, rradhakrishnan, sasur, claire.cheng}@salesforce.com
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have shown remarkable abilities in diverse natural
    language processing (NLP) tasks. The LLMs generally undergo supervised fine-tuning
    (SFT) followed by preference alignment to be usable in downstream applications.
    However, this sequential training pipeline leads to alignment tax that degrades
    the LLM performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出卓越的能力。这些LLMs通常经过监督微调（SFT），随后进行偏好对齐，以便用于下游应用。然而，这种顺序训练流程会导致对齐税，进而降低LLM的性能。
- en: 'This paper introduces PAFT, a new PArallel training paradigm for effective
    LLM Fine-Tuning, which independently performs SFT and preference alignment (e.g.,
    DPO and ORPO, etc.) with the same pre-trained model on respective datasets. The
    model produced by SFT and the model from preference alignment are then merged
    into a final model by parameter fusing for use in downstream applications. This
    work reveals important findings that preference alignment like DPO naturally results
    in a sparse model while SFT leads to a natural dense model which needs to be sparsified
    for effective model merging. This paper introduces an effective interference resolution
    which reduces the redundancy by sparsifying the delta parameters. The LLM resulted
    from the new training paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard¹¹1[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    Comprehensive evaluation shows the effectiveness of the parallel training paradigm.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了PAFT，一种新型的并行训练范式，用于有效的LLM微调，该范式在各自的数据集上独立执行SFT和偏好对齐（例如，DPO和ORPO等），使用相同的预训练模型。SFT生成的模型与偏好对齐生成的模型通过参数融合合并为最终模型，以用于下游应用。这项工作揭示了重要的发现：像DPO这样的偏好对齐自然会产生一个稀疏模型，而SFT会导致一个自然的密集模型，这需要被稀疏化以进行有效的模型合并。本文引入了一种有效的干扰解决方案，通过稀疏化增量参数减少冗余。新的训练范式所产生的LLM在HuggingFace
    Open LLM Leaderboard¹¹1[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)上取得了第1名的排名。全面评估显示了并行训练范式的有效性。
- en: 'PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PAFT：一种有效的LLM微调的并行训练范式
- en: Shiva Kumar Pentyala^*, Zhichao Wang^*, Bin Bi^*, Kiran Ramnath, Xiang-Bo Mao,
    Regunathan Radhakrishnan, Sitaram Asur, Na (Claire) Cheng Salesforce {shivakumar.pentyala,
    zhichaowang, bin.bi, k.ramnath, xmao, rradhakrishnan, sasur, claire.cheng}@salesforce.com
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 希瓦·库马尔·彭提亚^*、志超·王^*、斌·毕^*、基兰·拉姆纳特、蔡博毛、雷贡纳坦·拉达克里希南、斯塔拉姆·阿苏尔、娜（克莱尔）程 Salesforce
    {shivakumar.pentyala, zhichaowang, bin.bi, k.ramnath, xmao, rradhakrishnan, sasur,
    claire.cheng}@salesforce.com
- en: '^*^*footnotetext: These authors contributed equally to this work'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注：这些作者对本工作贡献相同
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, large language models (LLMs) have emerged as the standard
    approach to addressing natural language processing (NLP) tasks. The typical way
    of building an LLM for downstream applications generally follows a sequential
    training pipeline consisting of two phases: 1\. Supervised Fine-tuning (SFT),
    where the pre-trained LLM is fine-tuned with the language modelling loss on demonstrations
    of the desired behaviour. 2\. Alignment with human preference, where the model
    produced by the SFT phase is further fine-tuned with an alignment algorithm like
    Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization
    (DPO), etc. While this sequential pipeline has been used to seemingly great success,
    how the SFT and the preference alignment work better with each other is underexplored.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）已成为处理自然语言处理（NLP）任务的标准方法。构建LLM以用于下游应用的典型方式通常遵循一个包括两个阶段的顺序训练流程：1\.
    监督微调（SFT），在此阶段，预训练的LLM通过语言建模损失在期望行为的示例上进行微调。2\. 与人类偏好的对齐，在此阶段，SFT阶段产生的模型通过类似于强化学习（RLHF）或直接偏好优化（DPO）的对齐算法进一步微调。虽然这种顺序流程似乎取得了很大的成功，但SFT和偏好对齐如何更好地协同工作仍未被充分探讨。
- en: Recent studies OpenAI ([2023](#bib.bib23)); Askell et al. ([2021](#bib.bib1));
    Song et al. ([2023](#bib.bib29)) have found that the preference alignment phase
    can cause the LLM to forget the diverse capabilities that it has acquired from
    earlier phases, despite aligning the LLM with human expectation. This phenomenon,
    also known as the *alignment tax* in the literature Ouyang et al. ([2022](#bib.bib24)),
    has accumulated substantial attention from both academia and industry. The alignment
    tax inherently results from catastrophic forgetting present in the staged training.
    To reduce catastrophic forgetting and thus alignment tax, this paper introduces
    a new parallel training paradigm for LLM fine-tuning, named PAFT, which independently
    performs SFT and preference alignment with the same pre-trained model on respective
    datasets, instead of sequentially conducting SFT followed by preference alignment.
    The model from SFT and the model from preference alignment are then merged into
    a final model by parameter fusing for use in downstream applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 OpenAI ([2023](#bib.bib23)); Askell et al. ([2021](#bib.bib1)); Song et
    al. ([2023](#bib.bib29))发现，尽管将LLM与人类期望对齐，但偏好对齐阶段可能会导致LLM遗忘从早期阶段获得的多样化能力。这一现象在文献中也称为*对齐代价*
    Ouyang et al. ([2022](#bib.bib24))，已引起了学术界和工业界的广泛关注。对齐代价本质上源于分阶段训练中的灾难性遗忘。为减少灾难性遗忘，从而降低对齐代价，本文提出了一种新的并行训练范式用于LLM微调，称为PAFT，该方法在各自的数据集上独立执行SFT和偏好对齐，而不是顺序进行SFT后再进行偏好对齐。然后，将SFT模型和偏好对齐模型通过参数融合合并成最终模型，以供下游应用使用。
- en: As discovered by prior work Yadav et al. ([2023](#bib.bib38)); Yu et al. ([2023](#bib.bib39)),
    direct model merging causes the parameter values to interfere across models, thereby
    harming the performance of the final model. The interference, which reduces parameter
    magnitudes in the merged model and eliminates subtle distinctions among values,
    can attribute to the redundant *delta parameters*, i.e., the differences in values
    between fine-tuned and pre-trained parameters, resulted from fine-tuning. Previous
    studies on model pruning Hoefler et al. ([2021](#bib.bib12)); Thimm and Fiesler
    ([1995](#bib.bib32)) have shown that during fine-tuning, many model parameters
    can change over the course of fine-tuning but only have a small impact on performance.
    However, when merging a parameter that is influential for one model but redundant
    (i.e. not influential) for other models, the influential value may be obscured
    by the redundant values, lowering the overall model performance. This work reveals
    the dense properties of the delta parameters resulted from SFT. To mitigate the
    dense property of SFT, we propose an effective interference resolution which reduces
    the redundancy by sparsifying the delta parameters by adding a L1-norm penalty
    to the original SFT loss function. The existing findings indicate that the inclusion
    of the L1 term enhances the sparsity of the SFT. This method of implicitly inducing
    sparsity has been evaluated against a technique that introduces sparsity explicitly,
    i.e., DARE Yu et al. ([2023](#bib.bib39)), demonstrating the advantages of employing
    the L1-norm on LLM’s performances in downstream tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Yadav等人（[2023](#bib.bib38)）和Yu等人（[2023](#bib.bib39)）的研究，直接模型合并导致参数值在模型之间干扰，从而损害最终模型的性能。这种干扰减少了合并模型中的参数大小，并消除了值之间的微妙区别，可能归因于冗余的*增量参数*，即微调和预训练参数之间的值差异，由微调引起。关于模型剪枝的先前研究（Hoefler等人（[2021](#bib.bib12)）；Thimm和Fiesler（[1995](#bib.bib32)））表明，在微调过程中，许多模型参数可能会发生变化，但对性能的影响很小。然而，当合并对一个模型有影响的参数但对其他模型冗余（即不重要）的参数时，影响值可能会被冗余值掩盖，从而降低整体模型性能。本工作揭示了SFT产生的增量参数的稠密特性。为了减轻SFT的稠密特性，我们提出了一种有效的干扰解决方案，通过将L1范数惩罚添加到原始SFT损失函数中来减少冗余。这些现有发现表明，包含L1项可以增强SFT的稀疏性。这种隐式引入稀疏性的方法与一种显式引入稀疏性的方法，即DARE（Yu等人（[2023](#bib.bib39)））进行了比较，展示了在下游任务中使用L1范数对LLM性能的优势。
- en: Finally, the sparse delta parameters from SFT and preference alignment are merged
    into a single stronger model. Different merging methods are assessed, and TIES
    and Task Arithmetic are shown to be the best model merging methods, depending
    on base models. The method of Parallel $\text{SFT}_{\text{sparse}}$+DPO consistently
    outperforms Parallel SFT+DPO across all model merging methods, showing the effectiveness
    and robustness of the PAFT training paradigm.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，来自SFT和偏好对齐的稀疏增量参数被合并成一个更强大的模型。评估了不同的合并方法，TIES和任务算术被证明是最好的模型合并方法，具体取决于基础模型。Parallel
    $\text{SFT}_{\text{sparse}}$+DPO 方法在所有模型合并方法中始终优于Parallel SFT+DPO，显示了PAFT训练范式的有效性和鲁棒性。
- en: 'The contributions of this paper are threefold:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献有三方面：
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Evidence is presented that parallel training of SFT and preference alignment
    outperforms sequential training, effectively reducing the alignment tax.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证据表明，SFT和偏好对齐的并行训练优于顺序训练，有效地减少了对齐成本。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The significance of sparse model integration is highlighted as a mean to prevent
    model conflict while preserving the full capability of each model. We demonstrate
    the superiority of the L1-norm over DARE as a more effective and higher-quality
    method for promoting sparsity in model training across various model merging techniques.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稀疏模型集成的重要性被突出为一种防止模型冲突，同时保留每个模型的全部能力的方法。我们展示了L1范数在促进模型训练中的稀疏性方面相对于DARE的优越性，作为一种更有效、更高质量的模型合并技术。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We conduct comprehensive evaluation of PAFT on well-known public benchmarks
    including Open LLM Leaderboard and AlpacaEval. The PAFT-ed 7B model achieved Rank
    #1 in the 7B/8B model category on the Open LLM Leaderboard, and the PAFT-ed 70B
    model topped the Leaderboard globally.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对PAFT在知名公共基准上进行了全面评估，包括Open LLM Leaderboard和AlpacaEval。PAFT处理的7B模型在Open LLM
    Leaderboard的7B/8B模型类别中获得第1名，而PAFT处理的70B模型在全球Leaderboards中排名第一。
- en: '![Refer to caption](img/fd4902e3e9393bd7377ba4b0a5499d7c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd4902e3e9393bd7377ba4b0a5499d7c.png)'
- en: (a) Staged training
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 分阶段训练
- en: '![Refer to caption](img/0faf29eb93addad93b0cd3d4aca3e1e8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0faf29eb93addad93b0cd3d4aca3e1e8.png)'
- en: (b) Parallel training
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 并行训练
- en: 'Figure 1: Comparison of training paradigms'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：训练范式比较
- en: 2 Methodology
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 2.1 Problem Setting
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题设置
- en: Given a pre-trained LLM, such as Mistral and Llama, we aim to optimize the model
    for a wide range of downstream tasks by fine-tuning it either fully or with parameter-efficient
    tuning such as LoRA Hu et al. ([2022](#bib.bib15)), using SFT and preference alignment.
    Throughout this paper, $\theta$ denotes the parameters of the model fine-tuned
    with preference alignment, such as PPO Schulman et al. ([2017](#bib.bib28)); Ziegler
    et al. ([2020](#bib.bib41)), DPO Rafailov et al. ([2023](#bib.bib26)) and ORPO Hong
    et al. ([2024](#bib.bib13)), etc.; $\delta_{\mathrm{sft}}=\theta_{\mathrm{sft}}-\theta_{\mathrm{pre}}$
    denotes the delta parameters between the preference-aligned model and the pre-trained
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个预训练的LLM，例如Mistral和Llama，我们的目标是通过完全微调或使用诸如LoRA Hu et al. ([2022](#bib.bib15))等参数高效的调优方法来优化模型，以适应广泛的下游任务，采用SFT和偏好对齐。在本文中，$\theta$表示使用偏好对齐微调的模型的参数，例如PPO
    Schulman et al. ([2017](#bib.bib28))；Ziegler et al. ([2020](#bib.bib41))，DPO Rafailov
    et al. ([2023](#bib.bib26))和ORPO Hong et al. ([2024](#bib.bib13))等；$\delta_{\mathrm{sft}}=\theta_{\mathrm{sft}}-\theta_{\mathrm{pre}}$表示偏好对齐模型和预训练模型之间的参数差异。
- en: 2.2 Parallel Training
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 并行训练
- en: SFT and preference alignment are two distinct methodologies designed to enhance
    the capabilities of pre-trained LLMs for specific applications. SFT focuses on
    boosting the performance of LLMs on downstream tasks by fine-tuning them with
    datasets that closely resemble the target task. This process tailors the model’s
    responses to be more accurate and relevant for a specific use-case. In contrast,
    preference alignment, such as RLHF, DPO and ORPO, etc., is a methodology that
    refines a model’s outputs based on human preferences. It generally fine-tunes
    the model on pairs of responses to an input query, one of which is preferred over
    the other one. Preference alignment uses such feedback signal to guide the model
    towards generating outputs that align with human expectation and ethical standards.
    This approach is particularly valuable for addressing the ethical considerations
    that arise when deploying LLMs in real-world scenarios.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SFT和偏好对齐是两种旨在提升预训练LLM在特定应用中的能力的方法。SFT专注于通过使用与目标任务相似的数据集微调LLM，从而提升LLM在下游任务中的表现。这一过程使模型的响应更加准确和相关，适用于特定的用例。相比之下，偏好对齐（例如RLHF、DPO和ORPO等）是一种基于人类偏好来优化模型输出的方法。它通常通过对输入查询的响应对进行微调，其中一个响应被偏好于另一个。偏好对齐利用这种反馈信号来指导模型生成符合人类期望和伦理标准的输出。这种方法在解决将LLM应用于现实世界场景时出现的伦理问题时尤为重要。
- en: Nowadays, researchers have applied SFT to enhance the performance of LLMs on
    targeted tasks, and then employed preference alignment to further align the models
    with human preferences. However, this sequential application of SFT followed by
    preference alignment has often led to a compromise in task-specific performance
    - a phenomenon referred to as the alignment tax. This occurs because the distinct
    objectives of SFT and preference alignment can sometimes be at odds, with the
    alignment process potentially undoing some of the task-specific optimizations
    achieved through SFT.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，研究人员已经应用SFT来提升LLM在特定任务上的性能，然后采用偏好对齐进一步使模型符合人类偏好。然而，这种先使用SFT然后进行偏好对齐的顺序应用，往往会导致任务特定性能的妥协——这种现象被称为对齐税。这是因为SFT和偏好对齐的不同目标有时可能会冲突，对齐过程可能会撤销通过SFT实现的一些任务特定优化。
- en: 'We address the challenge of the alignment tax by a novel approach that involves
    SFT and preference alignment concurrently using adapter training, such as LoRA Hu
    et al. ([2022](#bib.bib15)). This method takes full advantages and strengths of
    both SFT and preference alignment without sacrificing performance in either one,
    i.e., ensuring that the resulting model maintains high performance in downstream
    tasks while also being aligned with human preferences, thus overcoming the limitations
    associated with the alignment tax. During the training process specifically, based
    on the same pre-trained model $\theta_{\mathrm{pre}}$ and $\delta_{\mathrm{xpo}}$
    in an effective way of avoiding feature interference. Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning")
    compares the typical staged training pipeline and our parallel training pipeline
    PAFT.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过一种新颖的方法来解决对齐税问题，该方法涉及 SFT 和偏好对齐，同时使用适配器训练，例如 LoRA Hu 等（[2022](#bib.bib15)）。该方法充分发挥了
    SFT 和偏好对齐的优势和强项，而不牺牲其中任何一种的性能，即确保生成的模型在下游任务中保持高性能，同时也与人类偏好保持一致，从而克服了与对齐税相关的局限性。在训练过程中，特别是基于相同的预训练模型
    $\theta_{\mathrm{pre}}$ 和 $\delta_{\mathrm{xpo}}$，有效避免了特征干扰。图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning")
    比较了典型的阶段训练流程和我们的并行训练流程 PAFT。'
- en: 2.3 Sparse Merging
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 稀疏合并
- en: 'The integration of dense neural network models often results in a suboptimal
    combined model due to the phenomenon of parameter interference. This challenge
    has led researchers to explore alternative strategies. Our investigations reveal
    that by increasing sparsity of a fine-tuned adapter, the performance of merging
    the adapter with the base model can be improved. Specifically, the parameter $\delta_{\mathrm{xpo}}$,
    we propose the incorporation of an L1 regularization term during the SFT process.
    This modification to the fine-tuning procedure is expressed mathematically as
    follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 稠密神经网络模型的整合通常会导致子优化的组合模型，这主要是由于参数干扰现象。这一挑战促使研究人员探索替代策略。我们的研究表明，通过增加微调适配器的稀疏性，可以提高将适配器与基础模型合并的性能。具体来说，对于参数
    $\delta_{\mathrm{xpo}}$，我们建议在 SFT 过程中加入 L1 正则化项。这一对微调过程的修改在数学上表示如下：
- en: '|  | $L_{\text{$\text{SFT}_{\text{sparse}}$}}=L_{\text{SFT}}+\lambda\cdot\&#124;\delta_{\mathrm{sft}}\&#124;_{1}$
    |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{$\text{SFT}_{\text{sparse}}$}}=L_{\text{SFT}}+\lambda\cdot\&#124;\delta_{\mathrm{sft}}\&#124;_{1}$
    |  | (1) |'
- en: 'Here, $L_{\text{SFT}}$, with sparsity levels over 90%, as illustrated by the
    SFT_sparse in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Sparse Merging ‣ 2 Methodology
    ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$L_{\text{SFT}}$，如图 [2](#S2.F2 "Figure 2 ‣ 2.3 Sparse Merging ‣ 2 Methodology
    ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning") 所示，具有超过 90%
    的稀疏级别。'
- en: 'Given sparse representations for adapters of both SFT and preference alignment,
    the challenge is to effectively merge these delta parameters, $\delta_{\mathrm{sft}}$,
    while preserving the performance benefits of SFT and preference alignment. The
    merging process can be formalized by the equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 SFT 和偏好对齐的适配器的稀疏表示，挑战在于有效地合并这些 delta 参数 $\delta_{\mathrm{sft}}$，同时保持 SFT
    和偏好对齐的性能优势。合并过程可以通过以下方程形式化：
- en: '|  | $\theta_{\mathrm{merge}}=f(\theta_{\mathrm{pre}},\delta_{\mathrm{dpo}},\delta_{\mathrm{sft}})$
    |  | (2) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{\mathrm{merge}}=f(\theta_{\mathrm{pre}},\delta_{\mathrm{dpo}},\delta_{\mathrm{sft}})$
    |  | (2) |'
- en: In our study, we explore a variety of merging methods proposed in the literature,
    including SLERP, Task Arithmetic, TIES, DARE TIES, and Linear. Detailed discussions
    of these merging methods are provided in the Related Work section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们探讨了文献中提出的各种合并方法，包括 SLERP、任务算术、TIES、DARE TIES 和线性方法。有关这些合并方法的详细讨论请参见相关工作部分。
- en: '![Refer to caption](img/c1364b2749710b9ff970dbfad5dcf79d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c1364b2749710b9ff970dbfad5dcf79d.png)'
- en: 'Figure 2: Adapter sparsity for SFT and DPO. The sparsity levels are computed
    by first merging the parameters from LoRA matrices $\delta_{A}$ that are less
    than a threshold of $1\times e^{-5}$, indicating the proportion of weights approaching
    zero. The reported sparsity is the average across all layers.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：SFT 和 DPO 的适配器稀疏性。稀疏级别是通过首先合并 LoRA 矩阵 $\delta_{A}$ 中小于 $1\times e^{-5}$
    的参数来计算的，这表示权重接近零的比例。报告的稀疏性是所有层的平均值。
- en: .
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 3 Experiments
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Evaluation Settings
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 评估设置
- en: In this study, we conduct comprehensive evaluation on both the Open LLM leaderboard
    provided by HuggingFace and the AlpacaEval benchmark. The Open LLM Leaderboard
    benchmark suite encompasses a diverse set of six benchmark tasks, namely ARC,
    HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K, along with their aggregated
    performance metrics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们对HuggingFace提供的Open LLM排行榜和AlpacaEval基准进行了全面评估。Open LLM排行榜基准套件涵盖了六个基准任务，即ARC、HellaSwag、MMLU、TruthfulQA、Winogrande和GSM8K，以及它们的汇总性能指标。
- en: 'In our experiments, we employ two state-of-the-art pre-trained models: Mistral-7B Jiang
    et al. ([2023](#bib.bib17)) and Llama-3-8B²²2Note that while the Llama 3 model
    is referenced in our work, the official documentation for this model has not been
    released at the time of writing, and thus we cite its official GitHub site as
    a proxy: [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3).
    This section presents the experimental results of merging the delta parameters
    obtained through SFT and DPO using the LoRA technique. We also study another preference
    alignment method ORPO for PAFT, which results in the same observations and conclusions
    as those from DPO. It shows the generalizability of PAFT to different preference
    alignment techniques. Due to space limit, we put the experimental results for
    ORPO in the appendix.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用了两种最先进的预训练模型：Mistral-7B Jiang et al. ([2023](#bib.bib17))和Llama-3-8B²²2请注意，尽管我们在工作中引用了Llama
    3模型，但在撰写时该模型的官方文档尚未发布，因此我们引用了其官方GitHub站点作为代理：[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)。本节展示了使用LoRA技术合并通过SFT和DPO获得的增量参数的实验结果。我们还研究了另一种首选对齐方法ORPO用于PAFT，结果与DPO得出的观察和结论相同。这显示了PAFT对不同首选对齐技术的通用性。由于篇幅限制，我们将ORPO的实验结果放在附录中。
- en: Following the Zephyr work Tunstall et al. ([2023](#bib.bib34)), we use the UltraChat Ding
    et al. ([2023](#bib.bib8)) dataset for SFT and the UltraFeedback Tunstall et al.
    ([2023](#bib.bib34)) dataset for DPO. UltraChat is a self-refinement dataset consisting
    of 200K multi-turn dialogues generated by GPT-3.5-Turbo over 30 topics and 20
    different types of text material. UltraFeedback consists of 64k prompts, each
    of which have four LLM responses that are rated by GPT-4 according to criteria
    like instruction-following, honesty, and helpfulness.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循Zephyr工作 Tunstall et al. ([2023](#bib.bib34))，我们使用UltraChat Ding et al. ([2023](#bib.bib8))数据集进行SFT，使用UltraFeedback
    Tunstall et al. ([2023](#bib.bib34))数据集进行DPO。UltraChat是一个自我改进数据集，包含200K个由GPT-3.5-Turbo生成的多轮对话，覆盖30个主题和20种不同类型的文本材料。UltraFeedback包含64k个提示，每个提示有四个LLM响应，这些响应根据指令遵循、诚实性和有用性等标准由GPT-4进行评分。
- en: We meticulously explore a spectrum of merging methods, including SLERP, Task
    Arithmetic, TIES, DARE-enhanced TIES, and Linear combination. Each of these merging
    strategies is scrutinized to determine its efficacy in integrating the sparsity-induced
    parameters from LoRA with the original pre-trained models. The goal is to ascertain
    which method most effectively preserves the performance enhancements attributed
    to SFT and DPO, thereby contributing to the advancement of model merging methods
    in LLM research. For training individual adapters, we have used the same settings
    as in the *zephyr-7b-beta* development³³3[https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-7b-beta](https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-7b-beta).
    Our evaluation is conducted using the EleutherAI’s LM Evaluation Harness framework Gao
    et al. ([2023](#bib.bib10)). We adhere to the same branch (b281b09) used by the
    HuggingFace Open LLM Leaderboard Beeching et al. ([2023](#bib.bib3)), and evals
    are run with batch size 1 on an A100 GPU.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们细致地探索了一系列融合方法，包括SLERP、任务算术、TIES、DARE增强TIES以及线性组合。对这些融合策略进行详细审查，以确定其在将LoRA引起的稀疏性参数与原始预训练模型进行整合时的有效性。目标是确定哪种方法最有效地保持了SFT和DPO带来的性能提升，从而推动LLM研究中模型融合方法的进展。对于训练单独的适配器，我们使用了与*zephyr-7b-beta*开发相同的设置³³3[https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-7b-beta](https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-7b-beta)。我们的评估使用了EleutherAI的LM
    Evaluation Harness框架 Gao et al. ([2023](#bib.bib10))。我们遵循了HuggingFace Open LLM
    Leaderboard Beeching et al. ([2023](#bib.bib3))使用的相同分支（b281b09），并在A100 GPU上以批量大小1运行评估。
- en: The hyper parameter $\lambda$ are validated in our experiments to achieve reasonable
    sparsity.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，超参数$\lambda$经过验证，以实现合理的稀疏性。
- en: '| Base Model: Mistral-7B-v0.1 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型：Mistral-7B-v0.1 |'
- en: '| --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Method | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *AVERAGE*
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *平均值* |'
- en: '| PAFT ($\text{SFT}_{\text{sparse}}$+DPO) |  |  |  |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| PAFT ($\text{SFT}_{\text{sparse}}$+DPO) |  |  |  |  |  |'
- en: '| SLERP | 0.6391 | 0.8464 | 0.63961 | 0.5123 | 0.794 | 0.4223 | 0.64228 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.6391 | 0.8464 | 0.63961 | 0.5123 | 0.794 | 0.4223 | 0.64228 |'
- en: '| Task Arithmetic | 0.6519 | 0.8477 | 0.63325 | 0.563 | 0.794 | 0.4071 | 0.64949
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 任务算术 | 0.6519 | 0.8477 | 0.63325 | 0.563 | 0.794 | 0.4071 | 0.64949 |'
- en: '| TIES | 0.6519 | 0.8551 | 0.63927 | 0.5453 | 0.7946 | 0.4284 | 0.65243 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.6519 | 0.8551 | 0.63927 | 0.5453 | 0.7946 | 0.4284 | 0.65243 |'
- en: '| DARE TIES | 0.6493 | 0.8526 | 0.63444 | 0.5454 | 0.7964 | 0.4094 | 0.64792
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.6493 | 0.8526 | 0.63444 | 0.5454 | 0.7964 | 0.4094 | 0.64792
    |'
- en: '| Linear | 0.6348 | 0.8451 | 0.64275 | 0.505 | 0.7932 | 0.4246 | 0.64091 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 0.6348 | 0.8451 | 0.64275 | 0.505 | 0.7932 | 0.4246 | 0.64091 |'
- en: '| Parallel SFT+DPO |  |  |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 并行 SFT+DPO |  |  |  |  |  |'
- en: '| SLERP | 0.6391 | 0.8479 | 0.63937 | 0.5031 | 0.7924 | 0.4124 | 0.63904 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.6391 | 0.8479 | 0.63937 | 0.5031 | 0.7924 | 0.4124 | 0.63904 |'
- en: '| Task Arithmetic | 0.651 | 0.851 | 0.62998 | 0.5397 | 0.8011 | 0.4117 | 0.64741
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 任务算术 | 0.651 | 0.851 | 0.62998 | 0.5397 | 0.8011 | 0.4117 | 0.64741 |'
- en: '| TIES | 0.5956 | 0.8319 | 0.61651 | 0.3993 | 0.7853 | 0.3071 | 0.58928 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.5956 | 0.8319 | 0.61651 | 0.3993 | 0.7853 | 0.3071 | 0.58928 |'
- en: '| DARE TIES | 0.5922 | 0.8244 | 0.60471 | 0.3801 | 0.7577 | 0.2767 | 0.57263
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.5922 | 0.8244 | 0.60471 | 0.3801 | 0.7577 | 0.2767 | 0.57263
    |'
- en: '| Linear | 0.6391 | 0.846 | 0.63935 | 0.4946 | 0.7995 | 0.4314 | 0.64166 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 0.6391 | 0.846 | 0.63935 | 0.4946 | 0.7995 | 0.4314 | 0.64166 |'
- en: '| Sequential |  |  |  |  |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 |  |  |  |  |  |'
- en: '| $\text{SFT}_{\text{sparse}}$+DPO | 0.6391 | 0.8464 | 0.63461 | 0.5103 | 0.7894
    | 0.4123 | 0.63868 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| $\text{SFT}_{\text{sparse}}$+DPO | 0.6391 | 0.8464 | 0.63461 | 0.5103 | 0.7894
    | 0.4123 | 0.63868 |'
- en: '| SFT+DPO | 0.656 | 0.8459 | 0.62634 | 0.5079 | 0.7884 | 0.3836 | 0.63469 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| SFT+DPO | 0.656 | 0.8459 | 0.62634 | 0.5079 | 0.7884 | 0.3836 | 0.63469 |'
- en: '| Individual |  |  |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 个人 |  |  |  |  |  |'
- en: '| $\text{SFT}_{\text{sparse}}$-alone | 0.6126 | 0.8233 | 0.6421 | 0.4124 |
    0.7711 | 0.3715 | 0.6055 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| $\text{SFT}_{\text{sparse}}$-单独 | 0.6126 | 0.8233 | 0.6421 | 0.4124 | 0.7711
    | 0.3715 | 0.6055 |'
- en: '| SFT-alone | 0.6101 | 0.8216 | 0.6263 | 0.4486 | 0.7798 | 0.3525 | 0.6065
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| SFT-单独 | 0.6101 | 0.8216 | 0.6263 | 0.4486 | 0.7798 | 0.3525 | 0.6065 |'
- en: '| DPO-alone | 0.6314 | 0.8487 | 0.6423 | 0.4496 | 0.7932 | 0.4344 | 0.6333
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DPO-单独 | 0.6314 | 0.8487 | 0.6423 | 0.4496 | 0.7932 | 0.4344 | 0.6333 |'
- en: '| Mistral-7B-v0.1 | 0.6049 | 0.8320 | 0.6369 | 0.4259 | 0.7814 | 0.37 | 0.6085
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | 0.6049 | 0.8320 | 0.6369 | 0.4259 | 0.7814 | 0.37 | 0.6085
    |'
- en: '| Base Model: Llama-3-8B |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型：Llama-3-8B |'
- en: '| Method | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *AVERAGE*
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *平均值*'
- en: '| PAFT ($\text{SFT}_{\text{sparse}}$+DPO) |  |  |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| PAFT ($\text{SFT}_{\text{sparse}}$+DPO) |  |  |  |  |  |'
- en: '| SLERP | 0.6067 | 0.8367 | 0.66995 | 0.5297 | 0.7837 | 0.5095 | 0.65604 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.6067 | 0.8367 | 0.66995 | 0.5297 | 0.7837 | 0.5095 | 0.65604 |'
- en: '| Task Arithmetic | 0.6118 | 0.8411 | 0.66858 | 0.5552 | 0.7806 | 0.5208 |
    0.66301 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 任务算术 | 0.6118 | 0.8411 | 0.66858 | 0.5552 | 0.7806 | 0.5208 | 0.66301 |'
- en: '| TIES | 0.6101 | 0.8414 | 0.67098 | 0.5313 | 0.7891 | 0.5185 | 0.66023 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.6101 | 0.8414 | 0.67098 | 0.5313 | 0.7891 | 0.5185 | 0.66023 |'
- en: '| DARE TIES | 0.6067 | 0.8398 | 0.66945 | 0.5232 | 0.7885 | 0.5163 | 0.65732
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.6067 | 0.8398 | 0.66945 | 0.5232 | 0.7885 | 0.5163 | 0.65732
    |'
- en: '| Linear | 0.6049 | 0.8329 | 0.67059 | 0.5168 | 0.7837 | 0.5011 | 0.65166 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 0.6049 | 0.8329 | 0.67059 | 0.5168 | 0.7837 | 0.5011 | 0.65166 |'
- en: '| Parallel SFT+DPO |  |  |  |  |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 并行 SFT+DPO |  |  |  |  |  |'
- en: '| SLERP | 0.6152 | 0.8347 | 0.66248 | 0.5149 | 0.7869 | 0.5171 | 0.65521 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.6152 | 0.8347 | 0.66248 | 0.5149 | 0.7869 | 0.5171 | 0.65521 |'
- en: '| Task Arithmetic | 0.6254 | 0.837 | 0.66089 | 0.5266 | 0.7869 | 0.5133 | 0.65835
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 任务算术 | 0.6254 | 0.837 | 0.66089 | 0.5266 | 0.7869 | 0.5133 | 0.65835 |'
- en: '| TIES | 0.5879 | 0.8092 | 0.65863 | 0.4283 | 0.7545 | 0.4291 | 0.61127 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.5879 | 0.8092 | 0.65863 | 0.4283 | 0.7545 | 0.4291 | 0.61127 |'
- en: '| DARE TIES | 0.6007 | 0.8061 | 0.65702 | 0.4233 | 0.7609 | 0.4049 | 0.60882
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.6007 | 0.8061 | 0.65702 | 0.4233 | 0.7609 | 0.4049 | 0.60882
    |'
- en: '| Linear | 0.6152 | 0.8331 | 0.66614 | 0.5082 | 0.7845 | 0.5095 | 0.65277 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 0.6152 | 0.8331 | 0.66614 | 0.5082 | 0.7845 | 0.5095 | 0.65277 |'
- en: '| Sequential |  |  |  |  |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 |  |  |  |  |  |'
- en: '| $\text{SFT}_{\text{sparse}}$+DPO | 0.5648 | 0.7984 | 0.62204 | 0.4049 | 0.7766
    | 0.3692 | 0.58932 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\text{SFT}_{\text{sparse}}$+DPO | 0.5648 | 0.7984 | 0.62204 | 0.4049 | 0.7766
    | 0.3692 | 0.58932 |'
- en: '| SFT+DPO | 0.5623 | 0.7976 | 0.62258 | 0.4057 | 0.7719 | 0.3662 | 0.58771
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SFT+DPO | 0.5623 | 0.7976 | 0.62258 | 0.4057 | 0.7719 | 0.3662 | 0.58771
    |'
- en: '| Individual |  |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 个人 |  |  |  |  |  |'
- en: '| $\text{SFT}_{\text{sparse}}$-alone | 0.5862 | 0.8177 | 0.66328 | 0.4834 |
    0.7719 | 0.4473 | 0.6283 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\text{SFT}_{\text{sparse}}$-单独 | 0.5862 | 0.8177 | 0.66328 | 0.4834 | 0.7719
    | 0.4473 | 0.6283 |'
- en: '| SFT-alone | 0.6084 | 0.8135 | 0.65325 | 0.4469 | 0.7648 | 0.4637 | 0.62509
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SFT-单独 | 0.6084 | 0.8135 | 0.65325 | 0.4469 | 0.7648 | 0.4637 | 0.62509 |'
- en: '| DPO-alone | 0.6152 | 0.8412 | 0.6682 | 0.5273 | 0.7845 | 0.4849 | 0.65355
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DPO-单独 | 0.6152 | 0.8412 | 0.6682 | 0.5273 | 0.7845 | 0.4849 | 0.65355 |'
- en: '| Llama-3-8B | 0.5947 | 0.8209 | 0.66603 | 0.4391 | 0.7719 | 0.4587 | 0.62522
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B | 0.5947 | 0.8209 | 0.66603 | 0.4391 | 0.7719 | 0.4587 | 0.62522
    |'
- en: 'Table 1: Results of compared methods on the six benchmark tasks'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 六项基准任务中比较方法的结果'
- en: 3.2 Parallel Training vs. Sequential Training
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 平行训练 vs. 顺序训练
- en: 'To demonstrate the advantages of parallel training PAFT, we conducted empirical
    comparison of parallel, sequential and standalone training approaches on the six
    benchmark tasks using the two pre-trained models: Mistral-7B and Llama-3-8B. The
    results are given in Table [1](#S3.T1 "Table 1 ‣ 3.1 Evaluation Settings ‣ 3 Experiments
    ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning"). In the Mistral-7B
    model section, training with DPO alone improves the average score over the base
    model, while training with SFT alone doesn’t show an improvement. This result
    reveals that SFT, while focusing on downstream tasks, inadvertently undermines
    performance due to a lack of alignment with human preferences. Conversely, DPO
    aims to harmonize the outputs of LLMs with human preferences, resulting in a noticeable
    improvement in the average score.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示平行训练PAFT的优势，我们在使用两个预训练模型：Mistral-7B和Llama-3-8B的六项基准任务上进行了平行、顺序和单独训练方法的实证比较。结果见表[1](#S3.T1
    "Table 1 ‣ 3.1 Evaluation Settings ‣ 3 Experiments ‣ PAFT: A Parallel Training
    Paradigm for Effective LLM Fine-Tuning")。在Mistral-7B模型部分，仅用DPO训练使平均得分有所提高，而仅用SFT训练则没有显示出改进。这一结果揭示了SFT在关注下游任务时，由于缺乏与人类偏好的对齐，意外地削弱了性能。相反，DPO旨在将LLM的输出与人类偏好协调，从而显著提高了平均得分。'
- en: Furthermore, we evaluated the sequential training of SFT with L1 regularization
    followed by DPO, which gave an average score of 0.6387\. This score marginally
    surpasses that of standalone DPO, setting the stage for a comparison with parallel
    training outcomes. This outcome aligns with our initial hypothesis that during
    the DPO phase the model appears to discard much of the knowledge acquired in the
    SFT stage, i.e., alignment tax. Consequently, its performance exhibits only a
    marginal improvement over the training with DPO-alone.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们评估了SFT与L1正则化的顺序训练，然后是DPO，其平均得分为0.6387。这一得分略微超出了单独DPO的得分，为与平行训练结果的比较奠定了基础。这一结果与我们最初的假设一致，即在DPO阶段，模型似乎抛弃了SFT阶段获得的许多知识，即对齐税。因此，其性能仅比单独DPO训练有轻微提升。
- en: Additionally, we performed side-by-side evaluations of $\text{SFT}_{\text{sparse}}$
    and DPO in sequence. This outcome can be explained by a notable drawback of sequential
    training which is its tendency to overlook much of the knowledge gained during
    the SFT stage, suggesting a suboptimal use of SFT data. In contrast, parallel
    training effectively combines the benefits from SFT and DPO by processing them
    concurrently. The benefits are mostly preserved during model merging, ensuring
    efficient utilization of both SFT and DPO data. Our work underscores the enhanced
    efficacy of the parallel training approach PAFT, which not only maintains the
    distinct advantages of SFT and DPO, but also outperforms these techniques when
    they are used separately or sequentially.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们对$\text{SFT}_{\text{sparse}}$和DPO进行了顺序的逐一评估。这一结果可以通过顺序训练的一个显著缺陷来解释，即它倾向于忽视SFT阶段获得的许多知识，这表明SFT数据的使用不够理想。相比之下，平行训练通过并行处理SFT和DPO的优点有效地结合了它们。这些好处在模型合并过程中大多被保留，确保了对SFT和DPO数据的有效利用。我们的工作强调了平行训练方法PAFT的增强效果，它不仅保持了SFT和DPO的独特优势，还在它们单独使用或顺序使用时表现更佳。
- en: 3.3 Sparse Merging vs. Dense Merging
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 稀疏合并 vs. 密集合并
- en: 'Our study has demonstrated the advantages of incorporating sparsity into fine-tuned
    models. In the context of sequential training, the inclusion of L1 regularization
    has yielded a modest yet notable improvement. Specifically, in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Evaluation Settings ‣ 3 Experiments ‣ PAFT: A Parallel Training
    Paradigm for Effective LLM Fine-Tuning"), the average score for the sequential
    $\text{SFT}_{\text{sparse}}$+DPO stands at 0.6387, surpassing the sequential SFT+DPO
    without L1 regularization, with a score of 0.6347\. Although the improvement is
    marginal, it underscores the value of integrating the L1-norm to induce sparsity.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究展示了将稀疏性引入微调模型的优势。在顺序训练的背景下，L1正则化的引入带来了适度但显著的改善。具体而言，在表 [1](#S3.T1 "Table
    1 ‣ 3.1 Evaluation Settings ‣ 3 Experiments ‣ PAFT: A Parallel Training Paradigm
    for Effective LLM Fine-Tuning")中，顺序的$\text{SFT}_{\text{sparse}}$+DPO的平均得分为0.6387，超过了没有L1正则化的顺序SFT+DPO的0.6347。尽管改进幅度很小，但它突显了整合L1范数以引入稀疏性的价值。'
- en: '| LLM | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *AVERAGE*
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLM | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *AVERAGE*
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PAFT (Ein-70B) | 0.7986 | 0.9149 | 0.7805 | 0.7514 | 0.8777 | 0.7544 | 0.8129
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PAFT (Ein-70B) | 0.7986 | 0.9149 | 0.7805 | 0.7514 | 0.8777 | 0.7544 | 0.8129
    |'
- en: '| Mixtral-8x22B-Instruct | 0.727 | 0.8908 | 0.7777 | 0.6814 | 0.8516 | 0.8203
    | 0.7915 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x22B-Instruct | 0.727 | 0.8908 | 0.7777 | 0.6814 | 0.8516 | 0.8203
    | 0.7915 |'
- en: '| Llama-3-70B-Instruct | 0.7142 | 0.8569 | 0.8006 | 0.6181 | 0.8287 | 0.8544
    | 0.7788 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct | 0.7142 | 0.8569 | 0.8006 | 0.6181 | 0.8287 | 0.8544
    | 0.7788 |'
- en: '| PAFT (TextBase-7B) | 0.7389 | 0.9027 | 0.6478 | 0.7813 | 0.8603 | 0.6793
    | 0.7684 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| PAFT (TextBase-7B) | 0.7389 | 0.9027 | 0.6478 | 0.7813 | 0.8603 | 0.6793
    | 0.7684 |'
- en: '| Cohere-Command-R+ | 0.7099 | 0.8856 | 0.7573 | 0.563 | 0.854 | 0.7074 | 0.7462
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-Command-R+ | 0.7099 | 0.8856 | 0.7573 | 0.563 | 0.854 | 0.7074 | 0.7462
    |'
- en: '| DBRX-132B-Instruct | 0.6783 | 0.8885 | 0.7372 | 0.6702 | 0.8208 | 0.6732
    | 0.7447 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| DBRX-132B-Instruct | 0.6783 | 0.8885 | 0.7372 | 0.6702 | 0.8208 | 0.6732
    | 0.7447 |'
- en: '| OpenChat-3.5 | 0.6604 | 0.8293 | 0.6504 | 0.519 | 0.8177 | 0.6816 | 0.693
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| OpenChat-3.5 | 0.6604 | 0.8293 | 0.6504 | 0.519 | 0.8177 | 0.6816 | 0.693
    |'
- en: '| Llama-3-8B-Instruct | 0.6075 | 0.7855 | 0.6707 | 0.5165 | 0.7451 | 0.6869
    | 0.6687 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 0.6075 | 0.7855 | 0.6707 | 0.5165 | 0.7451 | 0.6869
    | 0.6687 |'
- en: '| Mistral-7B-Instruct-v0.2 | 0.6314 | 0.8488 | 0.6078 | 0.6826 | 0.7719 | 0.4003
    | 0.6571 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-v0.2 | 0.6314 | 0.8488 | 0.6078 | 0.6826 | 0.7719 | 0.4003
    | 0.6571 |'
- en: '| Gemma-7B | 0.6109 | 0.8247 | 0.6603 | 0.4491 | 0.7845 | 0.5277 | 0.6429 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B | 0.6109 | 0.8247 | 0.6603 | 0.4491 | 0.7845 | 0.5277 | 0.6429 |'
- en: 'Table 2: Comparison with state-of-the-art LLMs on Open LLM Leaderboard (All
    the scores are obtained from the Leaderboard)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：与Open LLM排行榜上的最先进LLMs的比较（所有得分均来自排行榜）
- en: '| LLM | LC WinRate | WinRate |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM | LC WinRate | WinRate |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 Preview | 50.0% | 50.0% |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Preview | 50.0% | 50.0% |'
- en: '| Claude 3 Opus | 40.5% | 29.1% |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 40.5% | 29.1% |'
- en: '| PAFT 70B | 38.6% | 26.5% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| PAFT 70B | 38.6% | 26.5% |'
- en: '| GPT-4 (03/14) | 35.3% | 22.1% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (03/14) | 35.3% | 22.1% |'
- en: '| Claude 3 Sonnet | 34.9% | 25.6% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 34.9% | 25.6% |'
- en: '| Llama 3 70B Instruct | 34.4% | 33.2% |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 70B Instruct | 34.4% | 33.2% |'
- en: '| Mixtral 8x22B v0.1 | 30.9% | 22.2% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral 8x22B v0.1 | 30.9% | 22.2% |'
- en: '| PAFT 7B | 30.6% | 22.8% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| PAFT 7B | 30.6% | 22.8% |'
- en: '| DBRX Instruct | 25.4% | 18.4% |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| DBRX Instruct | 25.4% | 18.4% |'
- en: '| Mixtral 8x7B v0.1 | 23.7% | 18.3% |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral 8x7B v0.1 | 23.7% | 18.3% |'
- en: '| Llama 3 8B Instruct | 22.9% | 22.6% |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8B Instruct | 22.9% | 22.6% |'
- en: '| GPT 3.5 Turbo | 22.7% | 14.1% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 Turbo | 22.7% | 14.1% |'
- en: '| Mistral 7B v0.2 | 17.1% | 14.7% |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7B v0.2 | 17.1% | 14.7% |'
- en: 'Table 3: Comparison with state-of-the-art LLMs on the AlpacaEval benchmark
    using GPT-4 as a judge'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用GPT-4作为裁判的AlpacaEval基准上与最先进LLMs的比较
- en: The impact of sparsity becomes more pronounced when examining parallel training
    scenarios. Across all considered model merging techniques, Parallel $\text{SFT}_{\text{sparse}}$+DPO)
    scores 0.6479, outstripping Parallel SFT+DPO’s 0.5726\. This substantial margin
    illustrates the robustness of L1-norm sparsity for various merging methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当审视并行训练场景时，稀疏性的影响变得更加明显。在所有考虑的模型融合技术中，Parallel $\text{SFT}_{\text{sparse}}$+DPO得分为0.6479，超越了Parallel
    SFT+DPO的0.5726。这一显著差距展示了L1范数稀疏性在各种融合方法中的鲁棒性。
- en: 'The same insights as given in the Mistral-7B section can be gained from the
    Llama-3-8B section in Table [1](#S3.T1 "Table 1 ‣ 3.1 Evaluation Settings ‣ 3
    Experiments ‣ PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning").
    PAFT on Llama-3-8B significantly outperforms Parallel SFT+DPO, sequential training
    and standalone training. The experimental results confirm the generalizability
    of PAFT to various pre-trained models.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '从表[1](#S3.T1 "Table 1 ‣ 3.1 Evaluation Settings ‣ 3 Experiments ‣ PAFT: A Parallel
    Training Paradigm for Effective LLM Fine-Tuning")中的Llama-3-8B部分可以获得与Mistral-7B部分相同的见解。PAFT在Llama-3-8B上显著优于Parallel
    SFT+DPO、顺序训练和独立训练。实验结果确认了PAFT对各种预训练模型的通用性。'
- en: When comparing different model merging strategies, TIES generally performs better
    than other methods on both Mistral-7B and Llama-3-8B, exhibiting superior performance
    over DARE TIES. DARE, which stands for "Drop And REscale", is a method that explicitly
    increases sparsity by eliminating elements below a certain threshold and rescaling
    the remaining parameters. In contrast, the L1-norm introduces sparsity implicitly
    by integrating it into the objective function. Consequently, the impact of the
    eliminated terms is less pronounced in the final results compared to DARE. This
    comparison reveals the advantages of the L1-norm’s explicit sparsity induction
    over the implicit approach employed by DARE.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同的模型合并策略时，TIES通常在Mistral-7B和Llama-3-8B上表现优于其他方法，相较于DARE TIES表现出更好的性能。DARE，即“Drop
    And REscale”，是一种通过消除低于某个阈值的元素并重新调整剩余参数来显著增加稀疏度的方法。相比之下，L1范数通过将稀疏度集成到目标函数中，隐式地引入了稀疏度。因此，与DARE相比，被消除的项在最终结果中的影响较小。这种比较揭示了L1范数的显式稀疏性引入相对于DARE采用的隐式方法的优势。
- en: 3.4 Comparison with State-of-the-art LLMs
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 与最先进的LLMs的比较
- en: 'On the online Open LLM Leaderboard, we performed PAFT on the Neurotic-7B⁴⁴4[https://huggingface.co/liminerity/Neurotic-Jomainotrik-7b-slerp](https://huggingface.co/liminerity/Neurotic-Jomainotrik-7b-slerp)
    and MoMo-70B⁵⁵5[https://huggingface.co/leejunhyeok/MoMo-70B-LoRA-V1.2_1](https://huggingface.co/leejunhyeok/MoMo-70B-LoRA-V1.2_1)
    base models. The two PAFT-ed models significantly improved over the respective
    base models, and achieved Rank #1 in the 7B/8B model category and globally on
    the online Open LLM Leaderboard, respectively, showing the effectiveness of PAFT
    on various base models. Table [2](#S3.T2 "Table 2 ‣ 3.3 Sparse Merging vs. Dense
    Merging ‣ 3 Experiments ‣ PAFT: A Parallel Training Paradigm for Effective LLM
    Fine-Tuning") gives the results of our PAFT-ed models and the existing state-of-the-art
    models on the Leaderboard.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '在在线Open LLM排行榜上，我们对Neurotic-7B⁴⁴4[https://huggingface.co/liminerity/Neurotic-Jomainotrik-7b-slerp](https://huggingface.co/liminerity/Neurotic-Jomainotrik-7b-slerp)和MoMo-70B⁵⁵5[https://huggingface.co/leejunhyeok/MoMo-70B-LoRA-V1.2_1](https://huggingface.co/leejunhyeok/MoMo-70B-LoRA-V1.2_1)基础模型进行了PAFT。两个PAFT-ed模型在各自的基础模型上显著改进，并在7B/8B模型类别和全球在线Open
    LLM排行榜上分别获得第1名，展示了PAFT对各种基础模型的有效性。表[2](#S3.T2 "Table 2 ‣ 3.3 Sparse Merging vs.
    Dense Merging ‣ 3 Experiments ‣ PAFT: A Parallel Training Paradigm for Effective
    LLM Fine-Tuning")展示了我们PAFT-ed模型与现有最先进模型在排行榜上的结果。'
- en: 'Additionally, we compared the two PAFT-ed models with existing state-of-the-art
    LLMs on the AlpacaEval benchmark Li et al. ([2023](#bib.bib21)), where every model
    generates responses to 805 questions on different topics, mostly focused on helpfulness.
    The models are judged by GPT-4, and the final metric is the pairwise win-rate
    against GPT-4\. As shown in Table [3](#S3.T3 "Table 3 ‣ 3.3 Sparse Merging vs.
    Dense Merging ‣ 3 Experiments ‣ PAFT: A Parallel Training Paradigm for Effective
    LLM Fine-Tuning"), the PAFT-ed 70B model outperforms existing state-of-the-art
    LLMs, except *GPT-4 Preview* and *Claude 3 Opus* in LC (Length-controlled) Win-Rate.
    While the GPT-4 judge favors its own GPT model family, the PAFT-ed 70B model performs
    better than *GPT-4 (03/14)* and *GPT 3.5 Turbo* do. On the other hand, the PAFT-ed
    7B model outperforms all the 7B/8B and smaller models on AlpacaEval. It even beats
    some larger models, such as *DBRX Instruct* and *Mixtral 8x7B*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们将两个PAFT-ed模型与现有的最先进LLMs在AlpacaEval基准测试中进行了比较Li等（[2023](#bib.bib21)），每个模型生成对805个不同主题的问题的回答，主要关注有用性。模型由GPT-4进行评判，最终指标是与GPT-4的配对胜率。如表[3](#S3.T3
    "Table 3 ‣ 3.3 Sparse Merging vs. Dense Merging ‣ 3 Experiments ‣ PAFT: A Parallel
    Training Paradigm for Effective LLM Fine-Tuning")所示，PAFT-ed 70B模型在LC（长度控制）胜率方面优于现有的最先进LLMs，除了*GPT-4
    Preview*和*Claude 3 Opus*。虽然GPT-4评判偏向于其自身的GPT模型系列，但PAFT-ed 70B模型的表现优于*GPT-4 (03/14)*和*GPT
    3.5 Turbo*。另一方面，PAFT-ed 7B模型在AlpacaEval上优于所有7B/8B及更小的模型。它甚至超过了一些更大的模型，例如*DBRX
    Instruct*和*Mixtral 8x7B*。'
- en: 4 Related Work
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: 4.1 SFT and Human Preference Alignment
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 SFT 和人类偏好对齐
- en: The groundbreaking achievements of BERT Devlin et al. ([2019](#bib.bib7)) and
    GPT OpenAI ([2023](#bib.bib23)) have underscored the significance of pretraining
    and supervised fine-tuning (SFT) techniques. To mitigate ethical concerns and
    ensure such language model outputs are aligned with human values, a subsequent
    alignment step employs human feedback to enhance the efficacy of pretraining Christiano
    et al. ([2023](#bib.bib6)), fine-tuning Ziegler et al. ([2020](#bib.bib41)), and
    adaptability for scaling purposes Leike et al. ([2018](#bib.bib20)). Kreutzer
    et al. ([2018](#bib.bib19)) found that implicit task feedback often outperforms
    explicit user feedback, leading to other high-quality datasets of human-generated
    summaries to compare with those produced by LLMs, resulting in superior quality
    outputs compared to SFT and human benchmarks Stiennon et al. ([2022](#bib.bib30)).
    Recent advancements by models such as GPT OpenAI ([2023](#bib.bib23)), Claude
    Bai et al. ([2022](#bib.bib2)), Llama Touvron et al. ([2023](#bib.bib33)), and
    Gemini Team ([2024](#bib.bib31)) have all leveraged human comparison feedback
    to refine output quality through alignment, a method also known as reinforcement
    learning from human feedback (RLHF).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Devlin 等人（[2019](#bib.bib7)）和 GPT OpenAI（[2023](#bib.bib23)）的开创性成就强调了预训练和监督微调（SFT）技术的重要性。为了缓解伦理问题并确保语言模型输出符合人类价值观，随后的对齐步骤通过人类反馈来增强预训练
    Christiano 等人（[2023](#bib.bib6)）、微调 Ziegler 等人（[2020](#bib.bib41)）和规模适应性 Leike
    等人（[2018](#bib.bib20)）的效果。Kreutzer 等人（[2018](#bib.bib19)）发现，隐性任务反馈通常优于显性用户反馈，这导致了与
    LLMs 生成的总结进行比较的高质量人类生成总结数据集，从而产生比 SFT 和人类基准 Stiennon 等人（[2022](#bib.bib30)）更优质的输出。最近由
    GPT OpenAI（[2023](#bib.bib23)）、Claude Bai 等人（[2022](#bib.bib2)）、Llama Touvron
    等人（[2023](#bib.bib33)）和 Gemini Team（[2024](#bib.bib31)）等模型进行的进展都利用了人类比较反馈来通过对齐改进输出质量，这种方法也称为基于人类反馈的强化学习（RLHF）。
- en: RLHF models employ the Bradley-Terry model to develop a reward function that
    emulates human preferences between two candidate responses Bradley and Terry ([1952](#bib.bib4)).
    This reward model lays the groundwork for applying reinforcement learning to LLMs,
    drawing inspiration from Proximal Policy Optimization (PPO) techniques Schulman
    et al. ([2017](#bib.bib28)). Direct Preference Optimization (DPO) streamlines
    the alignment process by integrating reward training with LLM alignment, thereby
    simplifying the training regimen through a direct relationship between the reward
    function and policy in reinforcement learning Rafailov et al. ([2023](#bib.bib26)).
    However, the efficacy of DPO in practice remains an area for further exploration
    Xu et al. ([2024](#bib.bib37)). Odds-ratio Preference Optimization (ORPO) Hong
    et al. ([2024](#bib.bib13)) is an alternative alignment paradigm that aims to
    replace sequential SFT + DPO with a single monolithic optimization algorithm.
    It directly optimizes for preferences between two candidate generations by maximizing
    the ratio of odds of the winning generation w.r.t. losing generation to simultaneously
    reward logits of desired tokens and penalize logits of undesired tokens.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 模型采用 Bradley-Terry 模型来开发模拟人类偏好的奖励函数 Bradley 和 Terry（[1952](#bib.bib4)）。该奖励模型为将强化学习应用于
    LLMs 奠定了基础，受到 Proximal Policy Optimization（PPO）技术的启发 Schulman 等人（[2017](#bib.bib28)）。Direct
    Preference Optimization（DPO）通过将奖励训练与 LLM 对齐集成，简化了对齐过程，从而通过奖励函数和强化学习中的策略之间的直接关系简化了训练方案
    Rafailov 等人（[2023](#bib.bib26)）。然而，DPO 在实践中的有效性仍然是进一步探索的领域 Xu 等人（[2024](#bib.bib37)）。Odds-ratio
    Preference Optimization（ORPO）Hong 等人（[2024](#bib.bib13)）是另一种对齐范式，旨在用单一的整体优化算法替代顺序的
    SFT + DPO。它通过最大化胜利生成相对于失败生成的赔率比，直接优化两候选生成之间的偏好，同时奖励所需标记的 logits 并惩罚不需要的标记的 logits。
- en: SFT and Human Preference Alignment serve distinct objectives and should be approached
    as components of a multi-objective optimization problem. SFT focused on enhancing
    the performance of LLMs in downstream tasks, whereas alignment seeks to address
    ethical concerns. Prior research on RLHF often treats alignment as a compromise
    that could potentially degrade the model’s output quality while address ethical
    problems Ouyang et al. ([2022](#bib.bib24)). Consequently, SFT and alignment are
    typically implemented in a sequential manner to ensure the safety of LLMs while
    accepting some degree of capability loss Hou et al. ([2024](#bib.bib14)). In contrast,
    Bai et al. have claimed that ’Smaller models experience severe ‘alignment taxes’
    – their performance on a wide variety of evaluations declines after RLHF training.
    However, we find a variety of alignment bonuses, with our 13B and 52B RLHF-trained
    models performing better at zero-shot NLP evaluations, and the same at few-shot
    evaluations’ Bai et al. ([2022](#bib.bib2)). This divergence in findings motivates
    further exploration into the interplay between SFT and alignment. Specifically,
    there is a strong interest in devising a method to integrate SFT and alignment
    in such a manner that yields an ’alignment bonus.’
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 和人类偏好对齐服务于不同的目标，应作为多目标优化问题的组成部分来处理。SFT 专注于提高 LLMs 在下游任务中的表现，而对齐旨在解决伦理问题。以前的
    RLHF 研究通常将对齐视为一种折中，这可能会降低模型的输出质量，同时解决伦理问题 Ouyang et al. ([2022](#bib.bib24))。因此，SFT
    和对齐通常以顺序方式实施，以确保 LLMs 的安全性，同时接受一定程度的能力损失 Hou et al. ([2024](#bib.bib14))。相比之下，Bai
    et al. 宣称“较小的模型会经历严重的‘对齐税’——它们在各种评估中的表现会在 RLHF 训练后下降。然而，我们发现各种对齐奖励，我们的 13B 和 52B
    RLHF 训练模型在零样本 NLP 评估中表现更好，在少样本评估中表现相同” Bai et al. ([2022](#bib.bib2))。这一发现的差异促使了对
    SFT 和对齐之间相互作用的进一步探索。具体而言，开发一种方法来整合 SFT 和对齐，从而获得“对齐奖励”，引起了强烈的兴趣。
- en: 4.2 Sparsity for LLMs
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLMs 的稀疏性
- en: As the size of LLMs continues to increase, the importance of compression becomes
    crucial for deploying them on edge devices. This is done to reduce costs and improve
    inference speed Zhu et al. ([2023](#bib.bib40)). Various compression strategies
    for LLMs exist, with a focus on pruning Han et al. ([2015](#bib.bib11)) and Low
    Rank Adapters (LoRA) Hu et al. ([2022](#bib.bib15)). Pruning involves creating
    sparsity through pretraining, magnitude-based pruning, and fine-tuning the remaining
    weights Han et al. ([2015](#bib.bib11)). LoRA suggests representing a matrix as
    the product of two low-rank matrices to reduce memory storage requirements Hu
    et al. ([2022](#bib.bib15)). Recent research has shown that the magnitudes of
    parameters trained by LoRA in SFT process are relatively small. A strategy has
    been developed where random pruning is applied to these small SFT parameters with
    a ratio $p$ to enhance model performance Yu et al. ([2023](#bib.bib39)). Merging
    sparsity models trained on different tasks has led to significant improvements
    in downstream tasks like AlpacaEval and GSM8K. This method involves applying pruning
    to introduce more sparsity in SFT using LoRA. Other methods for inducing sparsity
    in SFT parameters exist like incorporating the L1 norm in the loss function, similar
    to techniques used in Lasso regression Santosa and Symes ([1986](#bib.bib27))
    and compressed sensing Candes et al. ([2006](#bib.bib5)). A Bayesian interpretation
    of the L1-norm on the weights amounts to assuming a standard Laplacian prior on
    the parameters which is centered more closely around mean of zero. This concept
    will guide the research in this paper.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLMs 的规模不断扩大，压缩的重要性变得至关重要，以便在边缘设备上部署它们。这是为了降低成本和提高推理速度 Zhu et al. ([2023](#bib.bib40))。存在各种
    LLMs 压缩策略，重点包括剪枝 Han et al. ([2015](#bib.bib11)) 和低秩适配器 (LoRA) Hu et al. ([2022](#bib.bib15))。剪枝涉及通过预训练、基于幅度的剪枝以及微调剩余权重来创建稀疏性
    Han et al. ([2015](#bib.bib11))。LoRA 建议将矩阵表示为两个低秩矩阵的乘积，以减少内存存储要求 Hu et al. ([2022](#bib.bib15))。最近的研究表明，在
    SFT 过程中通过 LoRA 训练的参数幅度相对较小。已经开发出一种策略，其中将随机剪枝应用于这些小的 SFT 参数，比例为 $p$，以提高模型性能 Yu
    et al. ([2023](#bib.bib39))。将不同任务上训练的稀疏模型合并已经在下游任务中，如 AlpacaEval 和 GSM8K，取得了显著的改进。这种方法涉及应用剪枝以引入更多稀疏性，在
    SFT 中使用 LoRA。还有其他方法可以在 SFT 参数中引入稀疏性，如在损失函数中加入 L1 范数，类似于在 Lasso 回归 Santosa 和 Symes
    ([1986](#bib.bib27)) 和压缩感知 Candes et al. ([2006](#bib.bib5)) 中使用的技术。L1 范数在权重上的贝叶斯解释相当于假设在参数上有一个标准的拉普拉斯先验，该先验更靠近于零均值。这一概念将指导本文的研究。
- en: 4.3 Model Merging
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型合并
- en: 'Combining skills learnt from different types of datasets in a single model
    provides multiple benefits like better in-domain performance Poth et al. ([2021](#bib.bib25)),
    out-of-domain generalization Wang et al. ([2020](#bib.bib35)), and a more parameter
    efficient model w.r.t. specialized models. Joint multi-task learning is one way
    to achieve this, but it has several difficulties: it is costly to train a single
    model across all tasks and it is non-trivial to find the correct task-mix to ensure
    a jointly optimal performance across all tasks Fifty et al. ([2021](#bib.bib9)).
    A wide variety of model merging methods to combine specialized models into a stronger
    merged model have emerged as an alternative to multi-task training. Wortsman et al.
    ([2022](#bib.bib36)) introduced the paradigm of averaging model weights from separate
    fine-tuned models to create a stronger merged model in ModelSoup, achieving SOTA
    in several different benchmarks. Fisher merging from Matena and Raffel ([2022](#bib.bib22))
    proposed to improve upon naively averaging all model weights by instead using
    a weighted average of the parameters. They identified the importance of each individual
    parameter based on its Fisher Information to use as the coefficient in the weighted
    average. Ilharco et al. ([2023](#bib.bib16)) further showed that one could influence
    the merged model’s performance in several ways via task-arithmetic on task-vectors
    (additive weight adaptors): forgetting undesired traits via negation, learning
    tasks by addition, or learning entirely new tasks by analogies. Jin et al. ([2023](#bib.bib18))
    proposed RegMean where they solve a local closed-form linear-regression problem
    to estimate the merged model parameters for each individual linear layer. Yadav
    et al. ([2023](#bib.bib38)) demonstrated that the phenomenon of parameter interference
    during model-merging leads to performance degradation in merged models. They cited
    this interference to two main sources - redundant parameter-updates, i.e. updates
    not crucial to a model’s prediction, and sign disagreement between different parameter-updates.
    To overcome such destructive interference, they proposed TIES-Merging which has
    two filtering steps before model-merging. First, only the top-k% updates by magnitude
    are retained in each task-vector. Next, the dominant sign is chosen as $\text{sgn}(\Sigma_{i}(\text{sgn}(\theta_{i})))$
    and only those updates whose sign agrees with the dominant sign are finally averaged
    and merged.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将从不同类型的数据集中学到的技能结合到一个模型中，带来了多种好处，例如更好的领域内性能（Poth et al. ([2021](#bib.bib25))）、领域外的泛化能力（Wang
    et al. ([2020](#bib.bib35))），以及相对于专门模型的更高参数效率。联合多任务学习是实现这一目标的一种方式，但它面临几种困难：训练一个跨所有任务的单一模型成本高，而且找到正确的任务组合以确保所有任务的联合最佳性能并非易事（Fifty
    et al. ([2021](#bib.bib9))）。为结合专门模型而形成更强的合并模型，已经出现了各种模型合并方法，作为多任务训练的替代方案。Wortsman
    et al. ([2022](#bib.bib36)) 引入了从单独微调模型中平均模型权重的范式，以在ModelSoup中创建更强的合并模型，并在多个不同的基准测试中达到了SOTA。Matena
    和 Raffel ([2022](#bib.bib22)) 提出的 Fisher 合并方案，通过使用参数的加权平均来改进简单平均所有模型权重的方法。他们根据每个参数的
    Fisher 信息来识别其重要性，并将其用作加权平均中的系数。Ilharco et al. ([2023](#bib.bib16)) 进一步表明，可以通过对任务向量进行任务算术（加性权重适配器）以几种方式影响合并模型的性能：通过否定来忘记不需要的特征，通过加法学习任务，或通过类比学习全新的任务。Jin
    et al. ([2023](#bib.bib18)) 提出了 RegMean，其中他们通过解决局部封闭形式的线性回归问题来估计每个单独线性层的合并模型参数。Yadav
    et al. ([2023](#bib.bib38)) 证明了模型合并过程中参数干扰现象会导致合并模型的性能下降。他们将这种干扰归因于两个主要来源 - 冗余的参数更新，即对模型预测并不关键的更新，以及不同参数更新之间的符号不一致。为了克服这种破坏性干扰，他们提出了
    TIES-Merging，在模型合并之前有两个过滤步骤。首先，在每个任务向量中仅保留按大小排序的前k%更新。接下来，选择主导符号作为 $\text{sgn}(\Sigma_{i}(\text{sgn}(\theta_{i})))$，并且只有那些符号与主导符号一致的更新才最终被平均和合并。
- en: 5 Conclusions
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: LLM fine-tuning generally undergoes a two-stage training process, with SFT applied
    initially, followed by preference alignment. Yet, research indicates that this
    sequential approach incurs an "alignment tax", compromising the LLM’s overall
    performance. To counteract this, we advocate for a parallel training strategy
    PAFT which preserves the advantages of both SFT and preference alignment without
    incurring the alignment tax associated with sequential training. A significant
    hurdle in parallel training is the potential for conflict during the model merging
    phase, where the merging of different adapters can lead to diminished performance.
    In this paper, we propose the integration of an L1 regularization to the training
    loss during the SFT phase to induce sparsity, thereby reducing interference between
    models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: LLM微调通常经历两个阶段的训练过程，首先应用SFT，然后进行偏好对齐。然而，研究表明这种顺序方法会产生“对齐税”，影响LLM的整体性能。为了解决这个问题，我们提倡采用并行训练策略PAFT，这种方法保留了SFT和偏好对齐的优点，同时避免了顺序训练带来的对齐税。并行训练中的一个重大障碍是在模型合并阶段可能出现的冲突，其中不同适配器的合并可能导致性能下降。本文提出在SFT阶段将L1正则化整合到训练损失中以引发稀疏性，从而减少模型间的干扰。
- en: Our experimental results demonstrate the efficacy of incorporating an L1-norm
    into the SFT process for sparsification and utilizing a parallel training framework
    over the typical sequential approach. When combining all of them together, i.e.
    Parallel $\text{SFT}_{\text{sparse}}$+DPO achieves the state-of-art results on
    both the LLM leaderboard by HuggingFace and the AlpacaEval benchmark. The ORPO
    experimental results given in the appendix show the same patterns, demonstrating
    the generalizability of our PAFT to various preference alignment methods. This
    comprehensive strategy highlights how the methods of integrating SFT with preference
    alignment can greatly enhance LLM fine-tuning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果证明，将L1范数纳入SFT过程以实现稀疏化并利用并行训练框架比传统的顺序方法更为有效。当将所有这些方法结合在一起时，即并行$\text{SFT}_{\text{sparse}}$+DPO，在HuggingFace的LLM排行榜和AlpacaEval基准上都达到了最先进的结果。附录中的ORPO实验结果显示出相同的模式，证明了我们PAFT方法对各种偏好对齐方法的普适性。这一综合策略突显了将SFT与偏好对齐方法整合可以大大提升LLM微调效果。
- en: 6 Limitations
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个限制
- en: There are a couple of limitations of the parallel training of SFT and preference
    alignment. Firstly, we have found that sparsity aids in model merging, though
    the reasons behind this benefit and why DPO initially induces sparsity in the
    adapter remain unanswered.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SFT和偏好对齐的并行训练，有几个限制。首先，我们发现稀疏性有助于模型合并，但这种好处的原因以及DPO为何最初会在适配器中引发稀疏性仍未解答。
- en: Moreover, sparsity can reduce model interference during merging, but the scalability
    of this approach is still in question. If a merged model deployed in production
    fails in some cases, it is underexplored how to improve the model responses in
    these cases. Directly performing SFT on the merged model may lead to catastrophic
    forgetting of what it learned earlier. On the other hand, parallel training necessitates
    merging a new SFT-ed model with the existing merged model, adding complexity to
    the process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，稀疏性可以减少模型合并期间的干扰，但这种方法的可扩展性仍然存在疑问。如果一个在生产环境中部署的合并模型在某些情况下失败，如何改善这些情况下的模型响应尚未深入探讨。直接在合并模型上执行SFT可能会导致之前学习内容的灾难性遗忘。另一方面，并行训练需要将新的SFT-ed模型与现有合并模型进行合并，这增加了过程的复杂性。
- en: The primary risk associated with this paper pertains to its data usage. Currently,
    UltraChat data is employed for SFT, while UltraFeedback data is used for preference
    alignment. UltraChat consists solely of multi-round dialogue data, which inherently
    limits its format diversity. To enhance the robustness and applicability of the
    model, it is crucial to incorporate a wider variety of data types beyond dialogue
    data. Additionally, UltraFeedback relies on annotations generated by GPT-4, which
    inevitably include errors and in-accurate feedback. To mitigate these risks, higher-quality
    datasets are needed in the future.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要风险与数据使用相关。目前，SFT使用的是UltraChat数据，而偏好对齐使用的是UltraFeedback数据。UltraChat仅包含多轮对话数据，这在本质上限制了其格式多样性。为了增强模型的鲁棒性和适用性，必须融入更多类型的数据，超越对话数据。此外，UltraFeedback依赖于GPT-4生成的注释，这些注释不可避免地包含错误和不准确的反馈。为了减轻这些风险，未来需要更高质量的数据集。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Askell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma,
    Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse,
    Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher
    Olah, and Jared Kaplan. 2021. A general language assistant as a laboratory for
    alignment. *ArXiv*, abs/2112.00861.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma,
    Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse,
    Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher
    Olah, 和 Jared Kaplan. 2021. 一般语言助手作为对齐实验室。*ArXiv*，abs/2112.00861。
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. [Training
    a helpful and harmless assistant with reinforcement learning from human feedback](https://arxiv.org/abs/2204.05862).
    *Preprint*, arXiv:2204.05862.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, 和 Jared Kaplan. 2022. [通过人类反馈的强化学习训练有用且无害的助手](https://arxiv.org/abs/2204.05862)。*预印本*，arXiv:2204.05862。
- en: Beeching et al. (2023) Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. 2023. Open llm leaderboard. [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching et al. (2023) Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, 和 Thomas
    Wolf. 2023. Open llm 排行榜。 [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)。
- en: 'Bradley and Terry (1952) Ralph Allan Bradley and Milton E. Terry. 1952. [Rank
    analysis of incomplete block designs: I. the method of paired comparisons](http://www.jstor.org/stable/2334029).
    *Biometrika*, 39(3/4):324–345.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley and Terry (1952) Ralph Allan Bradley 和 Milton E. Terry. 1952. [不完全区组设计的等级分析：I.
    配对比较的方法](http://www.jstor.org/stable/2334029)。*生物统计学*，39(3/4):324–345。
- en: 'Candes et al. (2006) E.J. Candes, J. Romberg, and T. Tao. 2006. [Robust uncertainty
    principles: exact signal reconstruction from highly incomplete frequency information](https://doi.org/10.1109/TIT.2005.862083).
    *IEEE Transactions on Information Theory*, 52(2):489–509.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Candes et al. (2006) E.J. Candes, J. Romberg, 和 T. Tao. 2006. [稳健不确定性原理：从高度不完整的频率信息中精确重建信号](https://doi.org/10.1109/TIT.2005.862083)。*IEEE
    信息理论交易*，52(2):489–509。
- en: Christiano et al. (2023) Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2023. [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741).
    *Preprint*, arXiv:1706.03741.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. (2023) Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
    Shane Legg, 和 Dario Amodei. 2023. [从人类偏好中进行深度强化学习](https://arxiv.org/abs/1706.03741)。*预印本*，arXiv:1706.03741。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](https://arxiv.org/abs/1810.04805). *Preprint*, arXiv:1810.04805.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. [Bert: 深度双向变换器的预训练用于语言理解](https://arxiv.org/abs/1810.04805)。*预印本*，arXiv:1810.04805。'
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu,
    Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. [Enhancing chat language models
    by scaling high-quality instructional conversations](https://openreview.net/forum?id=oEsYs3WRc3).
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu,
    Zhiyuan Liu, Maosong Sun, 和 Bowen Zhou. 2023. [通过扩展高质量的指导对话来增强聊天语言模型](https://openreview.net/forum?id=oEsYs3WRc3)。在
    *2023 年自然语言处理经验方法会议* 中。
- en: Fifty et al. (2021) Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan
    Anil, and Chelsea Finn. 2021. [Efficiently identifying task groupings for multi-task
    learning](https://api.semanticscholar.org/CorpusID:237485414). In *Neural Information
    Processing Systems*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fifty et al. (2021) Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan
    Anil, 和 Chelsea Finn. 2021. [高效识别多任务学习的任务分组](https://api.semanticscholar.org/CorpusID:237485414)。在
    *神经信息处理系统* 中。
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le
    Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
    Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,
    Anish Thite, Ben Wang, Kevin Wang 和 Andy Zou. 2023. [少样本语言模型评估框架](https://doi.org/10.5281/zenodo.10256836)。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William J. Dally. 2015.
    [Learning both weights and connections for efficient neural networks](https://arxiv.org/abs/1506.02626).
    *Preprint*, arXiv:1506.02626.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Song Han, Jeff Pool, John Tran 和 William J. Dally. 2015. [学习权重和连接以提高神经网络的效率](https://arxiv.org/abs/1506.02626)。*预印本*，arXiv:1506.02626。
- en: 'Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden,
    and Alexandra Peste. 2021. Sparsity in deep learning: pruning and growth for efficient
    inference and training in neural networks. *J. Mach. Learn. Res.*, 22(1).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoefler et al. (2021) Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden
    和 Alexandra Peste. 2021. 深度学习中的稀疏性：神经网络中高效推理和训练的剪枝与增长。*J. Mach. Learn. Res.*，22(1)。
- en: 'Hong et al. (2024) Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic
    preference optimization without reference model. *arXiv preprint arXiv:2403.07691*,
    2(4):5.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. (2024) Jiwoo Hong, Noah Lee 和 James Thorne. 2024. Orpo: 无参考模型的单体偏好优化。*arXiv
    预印本 arXiv:2403.07691*，2(4):5。'
- en: 'Hou et al. (2024) Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao
    Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao
    Dong. 2024. [Chatglm-rlhf: Practices of aligning large language models with human
    feedback](https://arxiv.org/abs/2404.00934). *Preprint*, arXiv:2404.00934.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. (2024) Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu,
    Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang 和 Yuxiao Dong.
    2024. [Chatglm-rlhf：使大型语言模型与人类反馈对齐的实践](https://arxiv.org/abs/2404.00934)。*预印本*，arXiv:2404.00934。
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang 和 Weizhu Chen. 2022. [LoRA：大规模语言模型的低秩适配](https://openreview.net/forum?id=nZeVKeeFYf9)。在*国际学习表征会议*上。
- en: Ilharco et al. (2023) Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman,
    Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. [Editing models with
    task arithmetic](https://openreview.net/forum?id=6t0Kwf8-jrj). In *The Eleventh
    International Conference on Learning Representations*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilharco et al. (2023) Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman,
    Ludwig Schmidt, Hannaneh Hajishirzi 和 Ali Farhadi. 2023. [用任务算术编辑模型](https://openreview.net/forum?id=6t0Kwf8-jrj)。在*第十一届国际学习表征会议*上。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](https://arxiv.org/abs/2310.06825). *Preprint*,
    arXiv:2310.06825.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix 和 William
    El Sayed. 2023. [Mistral 7b](https://arxiv.org/abs/2310.06825)。*预印本*，arXiv:2310.06825。
- en: Jin et al. (2023) Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang
    Cheng. 2023. [Dataless knowledge fusion by merging weights of language models](https://openreview.net/forum?id=FCnohuR6AnM).
    In *The Eleventh International Conference on Learning Representations*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2023) Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro 和 Pengxiang Cheng.
    2023. [通过合并语言模型的权重进行无数据知识融合](https://openreview.net/forum?id=FCnohuR6AnM)。在*第十一届国际学习表征会议*上。
- en: 'Kreutzer et al. (2018) Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and
    Stefan Riezler. 2018. [Can neural machine translation be improved with user feedback?](https://doi.org/10.18653/v1/N18-3012)
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 3 (Industry
    Papers)*, pages 92–105, New Orleans - Louisiana. Association for Computational
    Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreutzer 等（2018）Julia Kreutzer, Shahram Khadivi, Evgeny Matusov 和 Stefan Riezler。2018年。[用户反馈能否改善神经机器翻译？](https://doi.org/10.18653/v1/N18-3012)
    在 *2018年北美计算语言学协会年会论文集：人类语言技术，卷3（行业论文）*，页码92–105，新奥尔良 - 路易斯安那州。计算语言学协会。
- en: 'Leike et al. (2018) Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal
    Maini, and Shane Legg. 2018. [Scalable agent alignment via reward modeling: a
    research direction](https://arxiv.org/abs/1811.07871). *Preprint*, arXiv:1811.07871.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leike 等（2018）Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini
    和 Shane Legg。2018年。[通过奖励建模实现可扩展的代理对齐：一种研究方向](https://arxiv.org/abs/1811.07871)。*预印本*，arXiv:1811.07871。
- en: 'Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023）Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani,
    Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto。2023年。Alpacaeval: 一种自动评估指令跟随模型的工具。[https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。'
- en: Matena and Raffel (2022) Michael Matena and Colin Raffel. 2022. [Merging models
    with fisher-weighted averaging](https://arxiv.org/abs/2111.09832). *Preprint*,
    arXiv:2111.09832.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matena 和 Raffel（2022）Michael Matena 和 Colin Raffel。2022年。[通过费舍尔加权平均合并模型](https://arxiv.org/abs/2111.09832)。*预印本*，arXiv:2111.09832。
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。GPT-4技术报告。*ArXiv*，abs/2303.08774。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155).
    *Preprint*, arXiv:2203.02155.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike 和 Ryan Lowe。2022年。[通过人类反馈训练语言模型以遵循指令](https://arxiv.org/abs/2203.02155)。*预印本*，arXiv:2203.02155。
- en: Poth et al. (2021) Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna Gurevych.
    2021. [What to pre-train on? Efficient intermediate task selection](https://doi.org/10.18653/v1/2021.emnlp-main.827).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 10585–10605, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poth 等（2021）Clifton Poth, Jonas Pfeiffer, Andreas Rücklé 和 Iryna Gurevych。2021年。[预训练什么？有效的中间任务选择](https://doi.org/10.18653/v1/2021.emnlp-main.827)。在
    *2021年自然语言处理实证方法会议论文集*，页码10585–10605，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. [Direct preference optimization:
    Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290).
    *Preprint*, arXiv:2305.18290.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2023）Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,
    Christopher D. Manning 和 Chelsea Finn。2023年。[直接偏好优化：你的语言模型实际上是一个奖励模型](https://arxiv.org/abs/2305.18290)。*预印本*，arXiv:2305.18290。
- en: Santosa and Symes (1986) Fadil Santosa and William W. Symes. 1986. [Linear inversion
    of band-limited reflection seismograms](https://doi.org/10.1137/0907087). *SIAM
    Journal on Scientific and Statistical Computing*, 7(4):1307–1330.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santosa 和 Symes（1986）Fadil Santosa 和 William W. Symes。1986年。[带宽限制反射地震图的线性反演](https://doi.org/10.1137/0907087)。*SIAM科学与统计计算期刊*，7(4):1307–1330。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347).
    *Preprint*, arXiv:1707.06347.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov。2017年。[近端策略优化算法](https://arxiv.org/abs/1707.06347)。*预印本*，arXiv:1707.06347。
- en: Song et al. (2023) Ziang Song, Tianle Cai, Jason D. Lee, and Weijie J. Su. 2023.
    [Reward collapse in aligning large language models](https://arxiv.org/abs/2305.17608).
    *arXiv*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2023）**Ziang Song**、**Tianle Cai**、**Jason D. Lee** 和 **Weijie J. Su**。2023。
    [Reward collapse in aligning large language models](https://arxiv.org/abs/2305.17608)。*arXiv*。
- en: Stiennon et al. (2022) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022.
    [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325).
    *Preprint*, arXiv:2009.01325.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon 等（2022）**Nisan Stiennon**、**Long Ouyang**、**Jeff Wu**、**Daniel M. Ziegler**、**Ryan
    Lowe**、**Chelsea Voss**、**Alec Radford**、**Dario Amodei** 和 **Paul Christiano**。2022。
    [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)。*预印本*，arXiv:2009.01325。
- en: 'Team (2024) Gemini Team. 2024. [Gemini: A family of highly capable multimodal
    models](https://arxiv.org/abs/2312.11805). *Preprint*, arXiv:2312.11805.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team（2024）**Gemini Team**。2024。 [Gemini: A family of highly capable multimodal
    models](https://arxiv.org/abs/2312.11805)。*预印本*，arXiv:2312.11805。'
- en: Thimm and Fiesler (1995) Georg Thimm and Emile Fiesler. 1995. Evaluating pruning
    methods.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thimm 和 Fiesler（1995）**Georg Thimm** 和 **Emile Fiesler**。1995。评估修剪方法。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）**Hugo Touvron**、**Louis Martin**、**Kevin Stone**、**Peter Albert**、**Amjad
    Almahairi**、**Yasmine Babaei**、**Nikolay Bashlykov**、**Soumya Batra**、**Prajjwal
    Bhargava**、**Shruti Bhosale**、**Dan Bikel**、**Lukas Blecher**、**Cristian Canton
    Ferrer**、**Moya Chen**、**Guillem Cucurull**、**David Esiobu**、**Jude Fernandes**、**Jeremy
    Fu**、**Wenyin Fu**、**Brian Fuller**、**Cynthia Gao**、**Vedanuj Goswami**、**Naman
    Goyal**、**Anthony Hartshorn**、**Saghar Hosseini**、**Rui Hou**、**Hakan Inan**、**Marcin
    Kardas**、**Viktor Kerkez**、**Madian Khabsa**、**Isabel Kloumann**、**Artem Korenev**、**Punit
    Singh Koura**、**Marie-Anne Lachaux**、**Thibaut Lavril**、**Jenya Lee**、**Diana
    Liskovich**、**Yinghai Lu**、**Yuning Mao**、**Xavier Martinet**、**Todor Mihaylov**、**Pushkar
    Mishra**、**Igor Molybog**、**Yixin Nie**、**Andrew Poulton**、**Jeremy Reizenstein**、**Rashi
    Rungta**、**Kalyan Saladi**、**Alan Schelten**、**Ruan Silva**、**Eric Michael Smith**、**Ranjan
    Subramanian**、**Xiaoqing Ellen Tan**、**Binh Tang**、**Ross Taylor**、**Adina Williams**、**Jian
    Xiang Kuan**、**Puxin Xu**、**Zheng Yan**、**Iliyan Zarov**、**Yuchen Zhang**、**Angela
    Fan**、**Melanie Kambadur**、**Sharan Narang**、**Aurelien Rodriguez**、**Robert Stojnic**、**Sergey
    Edunov** 和 **Thomas Scialom**。2023。 [Llama 2: Open foundation and fine-tuned chat
    models](https://arxiv.org/abs/2307.09288)。*预印本*，arXiv:2307.09288。'
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. 2023. [Zephyr: Direct distillation of lm alignment](https://arxiv.org/abs/2310.16944).
    *Preprint*, arXiv:2310.16944.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunstall 等（2023）**Lewis Tunstall**、**Edward Beeching**、**Nathan Lambert**、**Nazneen
    Rajani**、**Kashif Rasul**、**Younes Belkada**、**Shengyi Huang**、**Leandro von Werra**、**Clémentine
    Fourrier**、**Nathan Habib**、**Nathan Sarrazin**、**Omar Sanseviero**、**Alexander
    M. Rush** 和 **Thomas Wolf**。2023。 [Zephyr: Direct distillation of lm alignment](https://arxiv.org/abs/2310.16944)。*预印本*，arXiv:2310.16944。'
- en: Wang et al. (2020) Jing Wang, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2020.
    [Multi-domain named entity recognition with genre-aware and agnostic inference](https://doi.org/10.18653/v1/2020.acl-main.750).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8476–8488, Online. Association for Computational Linguistics.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）**Jing Wang**、**Mayank Kulkarni** 和 **Daniel Preotiuc-Pietro**。2020。
    [Multi-domain named entity recognition with genre-aware and agnostic inference](https://doi.org/10.18653/v1/2020.acl-main.750)。在
    *第58届计算语言学协会年会论文集*，第8476–8488页，在线。计算语言学协会。
- en: 'Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,
    Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi,
    Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. [Model soups: averaging
    weights of multiple fine-tuned models improves accuracy without increasing inference
    time](https://proceedings.mlr.press/v162/wortsman22a.html). In *Proceedings of
    the 39th International Conference on Machine Learning*, volume 162 of *Proceedings
    of Machine Learning Research*, pages 23965–23998\. PMLR.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman 等（2022）Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca
    Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi,
    Yair Carmon, Simon Kornblith 和 Ludwig Schmidt. 2022. [模型汤：对多个微调模型的权重进行平均提高准确性而不增加推理时间](https://proceedings.mlr.press/v162/wortsman22a.html)。在
    *第39届国际机器学习大会论文集*，*机器学习研究论文集*第162卷，23965–23998页。PMLR。
- en: Xu et al. (2024) Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu
    Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024. [Is dpo superior to ppo for llm alignment?
    a comprehensive study](https://arxiv.org/abs/2404.10719). *Preprint*, arXiv:2404.10719.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2024）Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei,
    Guangju Wang, Chao Yu 和 Yi Wu. 2024. [DPO 是否优于 PPO 进行 LLM 对齐？全面研究](https://arxiv.org/abs/2404.10719)。*预印本*，arXiv:2404.10719。
- en: 'Yadav et al. (2023) Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel,
    and Mohit Bansal. 2023. TIES-merging: Resolving interference when merging models.
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yadav 等（2023）Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel 和 Mohit
    Bansal. 2023. TIES-merging：合并模型时解决干扰。在 *第37届神经信息处理系统会议*。
- en: 'Yu et al. (2023) Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2023.
    Language models are super mario: Absorbing abilities from homologous models as
    a free lunch. *arXiv preprint arXiv:2311.03099*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2023）Le Yu, Bowen Yu, Haiyang Yu, Fei Huang 和 Yongbin Li. 2023. 语言模型是超级玛丽：从同源模型中吸收能力作为免费午餐。*arXiv
    预印本 arXiv:2311.03099*。
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023.
    [A survey on model compression for large language models](https://arxiv.org/abs/2308.07633).
    *Preprint*, arXiv:2308.07633.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）Xunyu Zhu, Jian Li, Yong Liu, Can Ma 和 Weiping Wang. 2023. [大规模语言模型的模型压缩调查](https://arxiv.org/abs/2308.07633)。*预印本*，arXiv:2308.07633。
- en: Ziegler et al. (2020) Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
    Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020.
    [Fine-tuning language models from human preferences](https://arxiv.org/abs/1909.08593).
    *Preprint*, arXiv:1909.08593.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2020）Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano 和 Geoffrey Irving. 2020. [根据人类偏好微调语言模型](https://arxiv.org/abs/1909.08593)。*预印本*，arXiv:1909.08593。
- en: Appendix A PAFT Performance with a Different Preference Optimization Algorithm
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A PAFT 使用不同的偏好优化算法的性能
- en: 'The stronger performance of PAFT is also confirmed with a different choice
    of preference alignment algorithm. Table [4](#A1.T4 "Table 4 ‣ Appendix A PAFT
    Performance with a Different Preference Optimization Algorithm ‣ PAFT: A Parallel
    Training Paradigm for Effective LLM Fine-Tuning") shows experimental results with
    ORPO as the preference alignment method alongside SFT with the Llama-3-8B base
    model. We observe a similar trend where finetuning the LLM sequentially via SFT
    followed by ORPO underperforms all the parallelly trained variants. Even simple
    model merging methods such as Task Arithmetic and Linear merging perform strongly,
    outperforming more complicated methods like DARE TIES in both experiment settings.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: PAFT 的更强性能也在使用不同的偏好对齐算法时得到了确认。表 [4](#A1.T4 "表 4 ‣ 附录 A PAFT 使用不同的偏好优化算法的性能 ‣
    PAFT：一种有效 LLM 微调的并行训练范式") 显示了使用 ORPO 作为偏好对齐方法的实验结果，同时与 Llama-3-8B 基础模型的 SFT 进行比较。我们观察到类似的趋势，其中通过
    SFT 进行 LLM 顺序微调，然后是 ORPO 的表现逊色于所有并行训练的变体。即使是简单的模型合并方法，如任务算术和线性合并，表现也很强劲，在两种实验设置中都优于更复杂的方法如
    DARE TIES。
- en: '| Base Model: Meta-Llama-3-8B |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型：Meta-Llama-3-8B |'
- en: '| --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Method | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *AVERAGE*
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ARC | HellaSwag | MMLU | TruthfulQA | Winograde | GSM8K | *平均* |'
- en: '| PAFT ($\text{SFT}_{\text{sparse}}$+ORPO) |  |  |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PAFT ($\text{SFT}_{\text{sparse}}$+ORPO) |  |  |  |  |  |'
- en: '| SLERP | 0.599 | 0.8217 | 0.665 | 0.4926 | 0.7845 | 0.4898 | 0.6421 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.599 | 0.8217 | 0.665 | 0.4926 | 0.7845 | 0.4898 | 0.6421 |'
- en: '| Task Arithmetic | 0.5964 | 0.8214 | 0.6655 | 0.4995 | 0.783 | 0.4814 | 0.6412
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 任务算术 | 0.5964 | 0.8214 | 0.6655 | 0.4995 | 0.783 | 0.4814 | 0.6412 |'
- en: '| TIES | 0.5947 | 0.8226 | 0.66358 | 0.4931 | 0.783 | 0.4852 | 0.64036 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.5947 | 0.8226 | 0.66358 | 0.4931 | 0.783 | 0.4852 | 0.64036 |'
- en: '| DARE TIES | 0.593 | 0.8224 | 0.6637 | 0.4921 | 0.783 | 0.4738 | 0.638 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.593 | 0.8224 | 0.6637 | 0.4921 | 0.783 | 0.4738 | 0.638 |'
- en: '| Linear | 0.5964 | 0.8206 | 0.6654 | 0.4923 | 0.7814 | 0.4905 | 0.6411 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Linear | 0.5964 | 0.8206 | 0.6654 | 0.4923 | 0.7814 | 0.4905 | 0.6411 |'
- en: '| Parallel SFT+ORPO |  |  |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Parallel SFT+ORPO |  |  |  |  |  |'
- en: '| SLERP | 0.6049 | 0.8227 | 0.668 | 0.4905 | 0.783 | 0.4951 | 0.644 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| SLERP | 0.6049 | 0.8227 | 0.668 | 0.4905 | 0.783 | 0.4951 | 0.644 |'
- en: '| Task Arithmetic | 0.6152 | 0.8209 | 0.6621 | 0.4908 | 0.7845 | 0.4989 | 0.6454
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Task Arithmetic | 0.6152 | 0.8209 | 0.6621 | 0.4908 | 0.7845 | 0.4989 | 0.6454
    |'
- en: '| TIES | 0.593 | 0.8139 | 0.6633 | 0.4446 | 0.768 | 0.467 | 0.6250 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| TIES | 0.593 | 0.8139 | 0.6633 | 0.4446 | 0.768 | 0.467 | 0.6250 |'
- en: '| DARE TIES | 0.5981 | 0.8101 | 0.66 | 0.4398 | 0.7632 | 0.4534 | 0.6208 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| DARE TIES | 0.5981 | 0.8101 | 0.66 | 0.4398 | 0.7632 | 0.4534 | 0.6208 |'
- en: '| Linear | 0.6067 | 0.8222 | 0.6685 | 0.4868 | 0.783 | 0.4989 | 0.6444 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Linear | 0.6067 | 0.8222 | 0.6685 | 0.4868 | 0.783 | 0.4989 | 0.6444 |'
- en: '| Sequential |  |  |  |  |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Sequential |  |  |  |  |  |'
- en: '| $\text{SFT}_{\text{sparse}}$+ORPO | 0.5563 | 0.8018 | 0.62116 | 0.4068 |
    0.7719 | 0.3662 | 0.58736 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| $\text{SFT}_{\text{sparse}}$+ORPO | 0.5563 | 0.8018 | 0.62116 | 0.4068 |
    0.7719 | 0.3662 | 0.58736 |'
- en: '| SFT+ORPO | 0.5589 | 0.8021 | 0.62142 | 0.4092 | 0.7711 | 0.3677 | 0.5884
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SFT+ORPO | 0.5589 | 0.8021 | 0.62142 | 0.4092 | 0.7711 | 0.3677 | 0.5884
    |'
- en: '| Llama-3-8B | 0.5947 | 0.8209 | 0.64854 | 0.4391 | 0.7719 | 0.4587 | 0.62231
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B | 0.5947 | 0.8209 | 0.64854 | 0.4391 | 0.7719 | 0.4587 | 0.62231
    |'
- en: 'Table 4: Results of compared methods with ORPO on the six benchmark tasks'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在六个基准任务中与ORPO比较的方法结果
