- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:35:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoLoRA：针对有效的权重量化和激活量化进行旋转后的无异常值LLMs微调
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08044](https://ar5iv.labs.arxiv.org/html/2407.08044)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08044](https://ar5iv.labs.arxiv.org/html/2407.08044)
- en: Xijie Huang¹, Zechun Liu², Shih-yang Liu¹, Kwang-Ting Cheng¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xijie Huang¹, Zechun Liu², Shih-yang Liu¹, Kwang-Ting Cheng¹
- en: ¹Hong Kong University of Science and Technology, ²Meta Reality Labs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学，²Meta Reality Labs
- en: '{xhuangbs,sliuau}@connect.ust.hk, zechunliu@meta.com,timcheng@ust.hk All the
    work was done within HKUST and Zechun Liu served an advisory role.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{xhuangbs,sliuau}@connect.ust.hk, zechunliu@meta.com,timcheng@ust.hk 所有工作均在HKUST完成，Zechun
    Liu担任顾问角色。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning
    (PEFT) method, significantly enhances the training efficiency by updating only
    a small portion of the weights in Large Language Models (LLMs). Recently, weight-only
    quantization techniques have also been applied to LoRA methods to reduce the memory
    footprint of fine-tuning. However, applying weight-activation quantization to
    the LoRA pipeline is under-explored, and we observe substantial performance degradation
    primarily due to the presence of activation outliers. In this work, we propose
    RoLoRA, the first LoRA-based scheme for effective weight-activation quantization.
    RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning
    to preserve the outlier-free characteristics in rotated LLMs. Experimental results
    show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization
    robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B,
    LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation
    quantized LLaMA2-13B on commonsense reasoning tasks compared to LoRA baseline.
    We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B).
    Codes are available at [https://github.com/HuangOwen/RoLoRA](https://github.com/HuangOwen/RoLoRA)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA），作为一种代表性的参数高效微调（PEFT）方法，通过仅更新大语言模型（LLMs）中一小部分权重，显著提高了训练效率。最近，权重量化技术也被应用于LoRA方法，以减少微调的内存占用。然而，将权重量化和激活量化应用于LoRA流程的研究尚不充分，我们观察到由于激活异常值的存在，性能明显下降。在这项工作中，我们提出了RoLoRA，这是首个针对有效的权重量化和激活量化的基于LoRA的方案。RoLoRA利用旋转来消除异常值，并提出了旋转感知微调，以保持旋转后的LLMs中的无异常值特性。实验结果表明，RoLoRA在权重量化和激活量化设置中
    consistently 提高了低比特LoRA的收敛性和训练后的量化鲁棒性。我们在LLaMA2-7B/13B、LLaMA3-8B模型上评估了RoLoRA，与LoRA基线相比，在常识推理任务中，4比特权重量化LLaMA2-13B的绝对准确度提高了最高29.5%。我们进一步展示了其在大型多模态模型（LLaVA-1.5-7B）上的有效性。代码可在
    [https://github.com/HuangOwen/RoLoRA](https://github.com/HuangOwen/RoLoRA) 获取
- en: 'RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RoLoRA：针对有效的权重量化和激活量化进行旋转后的无异常值LLMs微调
- en: 'Xijie Huang¹, Zechun Liu²^†^†thanks: All the work was done within HKUST and
    Zechun Liu served an advisory role., Shih-yang Liu¹, Kwang-Ting Cheng¹ ¹Hong Kong
    University of Science and Technology, ²Meta Reality Labs {xhuangbs,sliuau}@connect.ust.hk,
    zechunliu@meta.com,timcheng@ust.hk'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Xijie Huang¹, Zechun Liu²^†^†感谢：所有工作均在HKUST完成，Zechun Liu担任顾问角色。, Shih-yang Liu¹,
    Kwang-Ting Cheng¹ ¹香港科技大学，²Meta Reality Labs {xhuangbs,sliuau}@connect.ust.hk,
    zechunliu@meta.com,timcheng@ust.hk
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: While we have witnessed the success of Large Language Models (LLMs) such as
    GPT-4 Achiam et al. ([2023](#bib.bib1)) and LLaMA Touvron et al. ([2023](#bib.bib44))
    across various tasks in recent years, the massive model size and expanding training
    cost for LLMs have necessitated the design of model compression and Parameter-Efficient
    Fine-Tuning (PEFT) methods. Low-rank Adaption (LoRA) Hu et al. ([2021](#bib.bib21)),
    as the most favored PEFT method, significantly enhances the fine-tuning efficiency
    of LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在最近几年见证了如GPT-4 Achiam et al. ([2023](#bib.bib1)) 和 LLaMA Touvron et al.
    ([2023](#bib.bib44)) 等大型语言模型（LLMs）在各种任务中的成功，但LLMs的庞大模型规模和不断扩大的训练成本已迫使我们设计模型压缩和参数高效微调（PEFT）方法。低秩适应（LoRA）
    Hu et al. ([2021](#bib.bib21)) 作为最受青睐的PEFT方法，显著提高了LLMs的微调效率。
- en: Recently, quantization techniques, which convert high-precision parameters into
    lower-bit formats such as INT4, have been integrated with LoRA methods Dettmers
    et al. ([2024](#bib.bib14)); Li et al. ([2024](#bib.bib27)); Xu et al. ([2024](#bib.bib47));
    Qin et al. ([2024](#bib.bib35)). Existing quantization-LoRA schemes can save memory
    costs during fine-tuning, and some schemes Li et al. ([2024](#bib.bib27)); Xu
    et al. ([2024](#bib.bib47)) can also reduce inference costs by producing quantized
    LLMs directly. However, these methods only perform weight-only quantization, while
    LoRA weight-activation quantization is under-explored. Quantizing both weights
    and activations in low-bit further saves run-time GPU memory and accelerates compute-intensive
    matrix-multiplication operations. We observe that 4-bit or 6-bit weight-activation
    quantization with LoRA finetuning still incurs a high accuracy degradation in
    LLMs, attributing to the outliers in weight and activation distribution, which
    stretch the quantization range and increase the quantization error.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，将高精度参数转换为低位格式（如INT4）的量化技术已与LoRA方法集成 Dettmers et al. ([2024](#bib.bib14))；李等人
    ([2024](#bib.bib27))；徐等人 ([2024](#bib.bib47))；秦等人 ([2024](#bib.bib35))。现有的量化-LoRA方案可以在微调过程中节省内存成本，一些方案 李等人
    ([2024](#bib.bib27))；徐等人 ([2024](#bib.bib47)) 还可以通过直接生成量化的LLM来减少推理成本。然而，这些方法仅执行权重量化，而LoRA权重-激活量化尚未充分探索。在低位格式中量化权重和激活进一步节省运行时GPU内存并加速计算密集型矩阵乘法操作。我们观察到，尽管使用LoRA微调的4位或6位权重-激活量化仍会导致LLM的高准确度下降，这归因于权重和激活分布中的离群值，这些离群值扩展了量化范围并增加了量化误差。
- en: 'Existing methods in the post-training quantization research community have
    endeavored to tackle the outlier challenge by mixed-precision subgrouping Zhao
    et al. ([2024](#bib.bib50)); Chee et al. ([2024](#bib.bib9)) or shifting outliers
    from activation to weight Xiao et al. ([2023](#bib.bib46)); Shao et al. ([2024](#bib.bib40)).
    More recently, applying rotation Ashkboos et al. ([2024](#bib.bib5)); Liu et al.
    ([2024c](#bib.bib33)) to the weight matrices of LLMs has demonstrated effectiveness
    in eliminating activation outliers and keeping computational invariance Ashkboos
    et al. ([2023a](#bib.bib3)). However, all these methods solve the problems from
    a post-training perspective, ignoring that outliers will emerge and change distribution
    during pre-training and fine-tuning Bondarenko et al. ([2021](#bib.bib7)). In
    this work, we take a step further to utilize the rotation for outliers-removal
    in LoRA fine-tuning setting and investigate the optimal solution for dynamically
    integrating rotation with LoRA to preserve the outlier-free characteristics and
    improve weight-activation quantization. Motivated by this target, we propose Rotated
    outlier-free Low-Rank Adaptation (RoLoRA), which initially apply in-block and
    between-block rotation to the pre-trained LLMs, and then utilize rotation-aware
    fine-tuning to produce outlier-free fine-tuned LLMs as shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization"). We explore the optimal rotation-aware
    fine-tuning scheme based on approximation error analysis.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在后训练量化研究领域，现有方法尝试通过混合精度子分组来应对离群值挑战 赵等人 ([2024](#bib.bib50))；张等人 ([2024](#bib.bib9))
    或者将离群值从激活转移到权重 肖等人 ([2023](#bib.bib46))；邵等人 ([2024](#bib.bib40))。最近，将旋转应用于LLM的权重矩阵 Ashkboos
    et al. ([2024](#bib.bib5))；刘等人 ([2024c](#bib.bib33)) 已被证明在消除激活离群值和保持计算不变性方面有效 Ashkboos
    et al. ([2023a](#bib.bib3))。然而，这些方法都从后训练的角度解决问题，忽视了离群值在预训练和微调过程中会出现并改变分布 Bondarenko
    et al. ([2021](#bib.bib7))。在这项工作中，我们进一步利用旋转在LoRA微调设置中移除离群值，并研究将旋转与LoRA动态集成的最佳解决方案，以保持无离群值的特性并改进权重-激活量化。受到这一目标的激励，我们提出了旋转无离群值低秩适配（RoLoRA），初步在块内和块间应用旋转于预训练LLM，然后利用旋转感知微调生成无离群值的微调LLM，如图 [1](#S2.F1
    "图 1 ‣ 2 相关工作 ‣ RoLoRA：针对有效权重-激活量化的旋转无离群值LLM微调")所示。我们基于近似误差分析探索了最佳的旋转感知微调方案。
- en: 'Extensive experimental results prove the effectiveness of RoLoRA across diverse
    LLMs, tasks, and quantization settings. RoLoRA improves the 4-bit quantization
    for weights and activations (W4A4) performance up to 14.6 points on the MMLU benchmark
    compared to LoRA. Compared with existing low-bit LoRA methods, RoLoRA outperforms
    previous SOTA IR-QLoRA Qin et al. ([2024](#bib.bib35)) with up to 6.0 points on
    the MMLU benchmark. The proposed RoLoRA is highly efficient with negligible fine-tuning
    overhead compared to LoRA in the same setting. RoLoRA can also improve the quantization
    robustness of Large Multimodal Models (LMMs) such as LLaVA Liu et al. ([2024a](#bib.bib30)),
    and we observe the multimodal understanding is largely retained even after W4A4
    quantization as shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '大量实验证据证明了RoLoRA在各种LLMs、任务和量化设置下的有效性。与LoRA相比，RoLoRA将4位量化（W4A4）在MMLU基准测试中的性能提升了多达14.6分。与现有的低位LoRA方法相比，RoLoRA在MMLU基准测试中超越了先前的SOTA
    IR-QLoRA Qin等人（[2024](#bib.bib35)）高达6.0分。所提出的RoLoRA在相同设置下，与LoRA相比具有极高的效率，几乎没有微调开销。RoLoRA还可以提高大型多模态模型（LMMs）的量化鲁棒性，如LLaVA
    Liu等人（[2024a](#bib.bib30)），我们观察到即使在W4A4量化后，多模态理解也基本保留，具体见表[1](#S1.T1 "Table 1
    ‣ 1 Introduction ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization")。'
- en: 'In summary, our work contributes as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的工作贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose RoLoRA, the first method that explores the feasibility of integrating
    rotation in LoRA with quantization settings.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了RoLoRA，这是首个探索将LoRA中的旋转与量化设置集成的可行性的方法。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RoLoRA enables robust weight-activation quantization of fine-tuned LLMs, especially
    in low-bit settings such as W4A4 and W6A6.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RoLoRA实现了对微调LLMs的鲁棒权重-激活量化，特别是在低位设置下，如W4A4和W6A6。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify the effectiveness of RoLoRA on the LLaMA series (2-7B, 2-13B, 3-8B)
    across quantizers (RTN/GPTQ), bitwidth (W4A4/W6A6), and benchmarks (Zero-shot
    commonsense, MMLU). We also demonstrate that RoLoRA can be applied to LMMs.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们验证了RoLoRA在LLaMA系列（2-7B, 2-13B, 3-8B）上的有效性，涵盖了量化器（RTN/GPTQ）、位宽（W4A4/W6A6）以及基准测试（零-shot常识、MMLU）。我们还展示了RoLoRA可以应用于LMMs。
- en: | ![[Uncaptioned image]](img/f213fe2c0aa6c3c82e6dee4f1b787fe3.png)  |  |
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: | ![[无标题图像]](img/f213fe2c0aa6c3c82e6dee4f1b787fe3.png)  |  |
- en: '| Question: User: Explain the visual content of the image in great detail.
    (category: “detail”) |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 问题：用户：详细解释图像的视觉内容。（类别：“详细”） |  |'
- en: '| GPT-4 Response: The image displays three open pizza boxes with pizzas inside.
    The first pizza box, situated to the left, contains a smaller pizza. The second
    pizza box, positioned in the middle, features a larger pizza topped with tortilla
    chips. The third pizza box, located to the right, contains another smaller pizza.
    Beside the large pizza, there are two side orders of sauce in bowls, accompanying
    the delicious meal. The arrangement showcases a delicious spread of pizzas, inviting
    everyone to enjoy their different flavors and sizes. |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4回应：图像展示了三个打开的披萨盒，里面有披萨。第一个披萨盒位于左侧，包含一个较小的披萨。第二个披萨盒位于中间，里面有一个较大的披萨，上面撒有玉米片。第三个披萨盒位于右侧，包含另一个较小的披萨。在大披萨旁边，有两个碗装的配菜酱料，伴随着美味的餐点。这个排列展示了一系列美味的披萨，邀请大家享受它们不同的口味和尺寸。
    |  |'
- en: '| LoRA$\xrightarrow{\text{GPTQ}}$W4A4: The image shows a pizza. |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| LoRA$\xrightarrow{\text{GPTQ}}$W4A4: 该图像显示了一个披萨。 |  |'
- en: '| RoLoRA$\xrightarrow{\text{GPTQ}}$W4A4: The image features a pizza with a
    variety of toppings, including cheese, pepperonies, and chicken. There are three
    pizzas in total, with one of them being a pizza with chicken. The pizzas are placed
    in a cardboard box, which is open and ready for the pizzas. The pizzas are served
    in a box, making it a convenient and portable meal option. |  |'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '| RoLoRA$\xrightarrow{\text{GPTQ}}$W4A4: 该图像展示了各种配料的比萨，包括奶酪、意大利辣香肠和鸡肉。总共有三块比萨，其中一块是鸡肉比萨。比萨放在一个打开的纸板箱中，准备好放置比萨。比萨被放在一个箱子里，方便携带和食用。
    |  |'
- en: 'Table 1: LLaVA-1.5-7B W4A4 quantization evaluation on the example from LLaVA-Bench.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: LLaVA-1.5-7B W4A4 量化评估，示例来自 LLaVA-Bench。'
- en: 2 Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Quantization Quantization methods are powerful tools for improving training
    and inference efficiency. The core insight is replacing full-precision weights
    and activations with lower-precision representation. Most existing LLM quantization
    techniques fall in the category of post-training quantization (PTQ) (Liu et al.,
    [2023b](#bib.bib31); Frantar et al., [2023](#bib.bib16); Lin et al., [2024](#bib.bib28);
    Shang et al., [2024](#bib.bib39); Chee et al., [2024](#bib.bib9)) that directly
    quantize the model without extensive training. Among these LLM PTQ methods, most
    of them apply weight-only quantization while few methods explore weight-activation
    quantization Xiao et al. ([2023](#bib.bib46)); Shao et al. ([2024](#bib.bib40));
    Zhao et al. ([2024](#bib.bib50)); Ashkboos et al. ([2024](#bib.bib5)). Compared
    to the weight-only quantization, quantizing both weights and activations enables
    low-precision multiply-accumulation (MAC) units. The core challenge is that outliers
    in activations cause high quantization errors. This work focuses on the weight-activation
    quantization in the LoRA pipeline.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 量化方法是提高训练和推理效率的强大工具。其核心思想是将全精度权重和激活替换为低精度表示。大多数现有的 LLM 量化技术属于后训练量化（PTQ）类别（Liu
    et al., [2023b](#bib.bib31); Frantar et al., [2023](#bib.bib16); Lin et al., [2024](#bib.bib28);
    Shang et al., [2024](#bib.bib39); Chee et al., [2024](#bib.bib9)），直接对模型进行量化，而无需大量训练。在这些
    LLM PTQ 方法中，大多数采用仅量化权重的方法，而很少有方法探讨权重-激活量化（Xiao et al. ([2023](#bib.bib46)); Shao
    et al. ([2024](#bib.bib40)); Zhao et al. ([2024](#bib.bib50)); Ashkboos et al.
    ([2024](#bib.bib5)））。与仅量化权重相比，量化权重和激活使得低精度的乘加（MAC）单元成为可能。核心挑战在于激活中的异常值会导致较高的量化误差。本工作专注于
    LoRA 流程中的权重-激活量化。
- en: LoRA Considering that full parameter fine-tuning becomes computationally impractical
    as the scale of LLM continues to grow, Parameter-Efficient Fine-Tuning (PEFT)
    methods Li and Liang ([2021](#bib.bib26)); Hu et al. ([2023](#bib.bib22)); Zhang
    et al. ([2023](#bib.bib49)) are designed to reduce the cost by training a relatively
    small subset of parameters. Low-Rank Adaptation (LoRA) Hu et al. ([2021](#bib.bib21))
    is the most adopted PEFT method, considering its flexibility and efficiency. More
    recently, LoRA variants Kopiczko et al. ([2024](#bib.bib24)); Liu et al. ([2024b](#bib.bib32));
    Hayou et al. ([2024](#bib.bib18)) emerged to improve the effectiveness and efficiency
    of LoRA. Combining LoRA and quantization Dettmers et al. ([2024](#bib.bib14))
    has also been a promising direction as quantization can further save the GPU memory
    in LoRA finetuning. To further reduce the information distortion of low-bit finetuning,
    various improvements of QLoRA have been proposed Xu et al. ([2024](#bib.bib47));
    Li et al. ([2024](#bib.bib27)); Qin et al. ([2024](#bib.bib35)). However, these
    methods only apply quantization to the weight during fine-tuning to reduce memory
    consumption. This work is the first quantized LoRA scheme that considers the robustness
    to weight-activation quantization.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 由于全参数微调在 LLM 规模不断增长的情况下变得计算上不可行，参数高效微调（PEFT）方法（Li and Liang ([2021](#bib.bib26));
    Hu et al. ([2023](#bib.bib22)); Zhang et al. ([2023](#bib.bib49)）旨在通过训练相对较小的参数子集来降低成本。低秩适应（LoRA）（Hu
    et al. ([2021](#bib.bib21)）是最常用的 PEFT 方法，考虑到其灵活性和效率。最近，LoRA 的变体（Kopiczko et al.
    ([2024](#bib.bib24)); Liu et al. ([2024b](#bib.bib32)); Hayou et al. ([2024](#bib.bib18)）出现，以提高
    LoRA 的有效性和效率。将 LoRA 和量化（Dettmers et al. ([2024](#bib.bib14)）结合起来也是一个有前途的方向，因为量化可以进一步节省
    LoRA 微调中的 GPU 内存。为了进一步减少低位微调的信息失真，提出了各种 QLoRA 的改进（Xu et al. ([2024](#bib.bib47));
    Li et al. ([2024](#bib.bib27)); Qin et al. ([2024](#bib.bib35)））。然而，这些方法仅在微调过程中对权重应用量化以减少内存消耗。本工作是首个考虑到权重-激活量化鲁棒性的量化
    LoRA 方案。
- en: '![Refer to caption](img/527580942bd74c4ae748573bb7f7a5c1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/527580942bd74c4ae748573bb7f7a5c1.png)'
- en: 'Figure 1: Activation distribution before and after rotation. The visualized
    input activations are selected from layers.1.self_attn.q_proj in LLaMA2-7B.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：旋转前后的激活分布。可视化的输入激活值选自 LLaMA2-7B 中的 layers.1.self_attn.q_proj 层。
- en: '![Refer to caption](img/72ccaa02e52bed59ba611368cf904c90.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72ccaa02e52bed59ba611368cf904c90.png)'
- en: 'Figure 2: Overview of the proposed Rotated outlier-free LoRA (RoLoRA)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：提出的旋转无异常值 LoRA（RoLoRA）概述
- en: 3 Preliminary and Motivation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步研究及动机
- en: 3.1 Low-Rank Adaptation (LoRA)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 低秩适配（LoRA）
- en: 'For a pre-trained weight matrix $W_{0}\in\mathbb{R}^{d\times k}$ and $B\in\mathbb{R}^{r\times
    k}$ can be represented as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个预训练的权重矩阵 $W_{0}\in\mathbb{R}^{d\times k}$ 和 $B\in\mathbb{R}^{r\times k}$
    可以表示为：
- en: '|  | $W^{\prime}=W_{0}+\Delta W=W_{0}+AB,$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{\prime}=W_{0}+\Delta W=W_{0}+AB,$ |  | (1) |'
- en: where $W_{0}$ in advance of deployment, and given that both $W^{\prime}$, LoRA
    and its related variants do not introduce any extra latency during the inference
    compared to the original model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练权重矩阵 $W_{0}$ 在部署前，且考虑到 $W^{\prime}$、LoRA 及其相关变体在推理过程中不会引入额外的延迟，相较于原始模型。
- en: 3.2 Outlier in Transformer
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 变换器中的异常值
- en: 'Starting from small-scale transformer models such as BERT and ViT, researchers
    have revealed that outliers exist within the weight and activation distribution Huang
    et al. ([2023](#bib.bib23)); Wei et al. ([2022](#bib.bib45)). Their existence
    in LLMs is also observed in various studies. As shown in the left side of Figure. [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization"), activation outliers are distributed
    per channel. While these outliers improve the representative capacity of the transformers Sun
    et al. ([2024](#bib.bib42)), they bring non-trivial challenges for quantization Xiao
    et al. ([2023](#bib.bib46)); Liu et al. ([2023b](#bib.bib31)).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '从小规模的变换器模型如 BERT 和 ViT 开始，研究人员已经揭示出权重和激活分布中存在异常值 Huang 等 ([2023](#bib.bib23))；Wei
    等 ([2022](#bib.bib45))。在 LLM 中的存在也在各种研究中被观察到。如图左侧所示，[1](#S2.F1 "Figure 1 ‣ 2 Related
    Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization")，激活异常值按通道分布。这些异常值虽然提升了变换器的表示能力 Sun 等 ([2024](#bib.bib42))，但对量化带来了不容忽视的挑战
    Xiao 等 ([2023](#bib.bib46))；Liu 等 ([2023b](#bib.bib31))。'
- en: 'Most previous solutions to this outlier problem in quantization can be categorized
    into three types: (1) isolating these outlier values in a sub-group with higher
    precision, such as LLM.int8 Dettmers et al. ([2022](#bib.bib13)), Atom Zhao et al.
    ([2024](#bib.bib50)), QuiK Ashkboos et al. ([2023b](#bib.bib4)), and AdaDim Heo
    et al. ([2024](#bib.bib20)). However, there is non-trivial overhead for the grouping
    and mixed-precision. (2) shifting the challenge of quantization from activations
    to weights, such as SmoothQuant Xiao et al. ([2023](#bib.bib46)) and OmniQuant Shao
    et al. ([2024](#bib.bib40)). However, these methods negatively influence the weight
    quantization robustness and fail at W4A4 scenarios. (3) rotating activation or
    weight matrices to remove outliers, such as QuaRot Ashkboos et al. ([2024](#bib.bib5))
    and SpinQuant Liu et al. ([2024c](#bib.bib33)). Among these methods, recent rotation-based
    solutions demonstrate superior effectiveness. However, previous rotation-based
    methods tackle the outlier challenge from a post-training perspective and have
    not been explored under PEFT settings.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以往解决量化中异常值问题的方法可以分为三种类型：（1）将这些异常值隔离到一个高精度的子组中，例如 LLM.int8 Dettmers 等 ([2022](#bib.bib13))、Atom
    Zhao 等 ([2024](#bib.bib50))、QuiK Ashkboos 等 ([2023b](#bib.bib4)) 和 AdaDim Heo
    等 ([2024](#bib.bib20))。然而，这种分组和混合精度存在不容忽视的开销。（2）将量化挑战从激活值转移到权重上，例如 SmoothQuant
    Xiao 等 ([2023](#bib.bib46)) 和 OmniQuant Shao 等 ([2024](#bib.bib40))。然而，这些方法对权重量化的鲁棒性产生负面影响，并在
    W4A4 场景下失效。（3）旋转激活或权重矩阵以去除异常值，例如 QuaRot Ashkboos 等 ([2024](#bib.bib5)) 和 SpinQuant
    Liu 等 ([2024c](#bib.bib33))。在这些方法中，最近的基于旋转的解决方案表现出更优的效果。然而，先前的基于旋转的方法从后训练的角度解决异常值问题，并未在
    PEFT 设置下进行探索。
- en: 'Thus, it leads to a question: Can we preserve the outlier-free characteristics
    of rotated LLMs and benefit from them during PEFT? We show in this work that we
    can achieve such a target and step further to investigate the most promising rotation-based
    fine-tuning solutions in this work.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，出现了一个问题：我们能否在 PEFT 过程中保持旋转 LLM 的无异常值特性，并从中受益？我们在这项工作中展示了我们可以实现这一目标，并进一步研究最有前景的基于旋转的微调解决方案。
- en: 3.3 Eliminating Outlier with Rotation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 通过旋转消除异常值
- en: 'A rotation matrix $R$. If the entries of $R$ entries¹¹1For the $n\neq 2^{k}$
    efficiently. based on the Hadamard transform (also known as the Walsh–Hadamard
    transform Ritter ([1996](#bib.bib37)) as an example of a generalized class of
    Fourier transforms):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个旋转矩阵 $R$。如果 $R$ 的元素为 $n \neq 2^{k}$，则基于 Hadamard 变换（也称为 Walsh–Hadamard 变换，Ritter
    ([1996](#bib.bib37)) 作为广义傅里叶变换的一种示例）可以高效地处理。
- en: '|  | $1$2 |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $\otimes$ operations. Previous research Ashkboos et al. ([2023a](#bib.bib3))
    has revealed that applying rotation on the weights of pre-norm transformers can
    retain its computational consistency and further lead to fewer outliers in the
    weight and activation distribution Ashkboos et al. ([2024](#bib.bib5)); Liu et al.
    ([2024c](#bib.bib33)). Concretely, the multiplication of weight matrices with
    a rotation matrix statistically blends weights with large and small magnitudes
    together into a more Gaussian-like distribution, thus producing activations with
    fewer outliers and easier to quantize. The outlier elimination effect of rotation
    is also theoretically proved in Chee et al. ([2024](#bib.bib9)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\otimes$ 操作。先前的研究 Ashkboos 等 ([2023a](#bib.bib3)) 显示，在预归一化变换器的权重上应用旋转可以保持其计算一致性，并进一步导致权重和激活分布中的异常值更少，Ashkboos
    等 ([2024](#bib.bib5)); Liu 等 ([2024c](#bib.bib33))。具体而言，将权重矩阵与旋转矩阵相乘，可以将大幅度和小幅度的权重统计混合成更类似于高斯分布的形式，从而产生激活异常值更少且更易于量化。旋转的异常值消除效果在
    Chee 等 ([2024](#bib.bib9)) 中也有理论证明。
- en: 4 Method
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: Motivated by existing challenges of activation outliers and the success of rotation-based
    solutions Ashkboos et al. ([2024](#bib.bib5)); Liu et al. ([2024c](#bib.bib33)),
    we introduce Rotated outlier-free Low-Rank Adaptation (RoLoRA). RoLoRA initially
    apply in-block and between-block rotation to the pre-trained LLMs, and rotation-aware
    fine-tuning on the rotated LLMs will retain the optimal outlier-free characteristic,
    producing fine-tuned LLMs highly robust to weight-activation quantization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 受激活异常值的现有挑战和基于旋转的解决方案成功的启发，Ashkboos 等 ([2024](#bib.bib5)); Liu 等 ([2024c](#bib.bib33))，我们引入了旋转异常值自由低秩适应（RoLoRA）。RoLoRA
    初步在预训练的 LLM 中应用块内和块间旋转，并在旋转后的 LLM 上进行旋转感知的微调，将保留最佳的异常值自由特性，生成对权重-激活量化高度鲁棒的微调 LLM。
- en: 4.1 Applying Rotation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 应用旋转
- en: Before starting fine-tuning with rotation, we first modify the model to keep
    computational invariance before and after rotation. First, we need to ensure no
    scaling operation in the normalization module. For the LLaMA series, this can
    be implemented by absorbing the RMSNorm scale parameters $\alpha$ into the weight
    matrix right after the RMSNorm layer Elhage et al. ([2023](#bib.bib15)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始进行带旋转的微调之前，我们首先修改模型以保持旋转前后的计算不变性。首先，我们需要确保在归一化模块中没有缩放操作。对于 LLaMA 系列，这可以通过将
    RMSNorm 缩放参数 $\alpha$ 吸收到 RMSNorm 层之后的权重矩阵中来实现，Elhage 等 ([2023](#bib.bib15))。
- en: 'Then, we perform between-block rotation to make sure that the outliers in between-block
    activation are eliminated. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Related
    Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we classify the weight matrices in LLMs into two groups: left-side
    weights, including $W_{q},W_{k},W_{v}$ in self-attention modules and $W_{down}$
    in feed-forward network modules. For the weights of these two groups, we adopt
    different rotation strategies with'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行块间旋转，以确保消除块间激活中的异常值。如图 [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ RoLoRA：微调旋转异常值自由的 LLM
    以有效权重-激活量化") 所示，我们将 LLM 中的权重矩阵分为两组：左侧权重，包括自注意力模块中的 $W_{q},W_{k},W_{v}$ 和前馈网络模块中的
    $W_{down}$。对于这两组权重，我们采用不同的旋转策略。
- en: '|  | $W^{R}_{\text{left}}\leftarrow RW_{\text{left}},W^{R}_{\text{right}}\leftarrow
    W_{\text{right}}R^{-1},$ |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{R}_{\text{left}}\leftarrow RW_{\text{left}},W^{R}_{\text{right}}\leftarrow
    W_{\text{right}}R^{-1},$ |  | (3) |'
- en: where the rotation $R$ after lm_head with $Y\leftarrow RY$, the final output
    of the model will be identical to the original model. To avoid overflow issues
    in the rotation process, we converted the FP16 weights to FP64 and converted them
    back after the multiplication. These rotations are applied before any training
    and inference, which indicates that there will be no overhead after the merging
    to original weights.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转 $R$ 应用于 lm_head 后，$Y\leftarrow RY$，模型的最终输出将与原始模型完全相同。为了避免旋转过程中出现溢出问题，我们将
    FP16 权重转换为 FP64，并在乘法操作后再转换回 FP16。这些旋转在任何训练和推理之前应用，这表明合并回原始权重后不会有额外开销。
- en: 'The rotation that directly applies to weights effectively reduces the outlier
    in between-block activation, and we refer to the operation as Between Block Rotation
    (BBR). Figure. [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization") demonstrates
    the effect of applying BBR as the activation distribution is smoother and de-centralized.
    However, another challenge remains that the activation in these modules still
    suffers from outliers, especially prevalent in FFN as discussed in previous research Bondarenko
    et al. ([2024](#bib.bib8)). We cannot directly apply rotation similar to BBR because
    of the non-linear operations such as SwiGLU Shazeer ([2020](#bib.bib41)) in FFN.
    To solve this, we adopt the online rotation node before inputting the activation
    input to $W_{down}$. This online rotation is implemented following the fast Hadamard
    kernel Chee et al. ([2024](#bib.bib9)); Ashkboos et al. ([2024](#bib.bib5)), which
    can be seen as a layer dynamically rotating the activation. This online rotation
    operation is highly efficient, and the overhead is negligible during training
    and inference. It is referred to as in-block rotation (IBR). Note that IBR can
    also be applied to the self-attention module, but we observe in the experiments
    of Table [6](#S5.T6 "Table 6 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") that there is no performance improvement with this rotation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 直接对权重应用旋转可以有效减少块间激活的异常值，我们将这种操作称为块间旋转（BBR）。图 [1](#S2.F1 "图 1 ‣ 2 相关工作 ‣ RoLoRA：微调旋转的无异常
    LLM 以实现有效的权重激活量化") 展示了应用 BBR 的效果，因为激活分布更平滑且去中心化。然而，另一个挑战是这些模块中的激活仍然存在异常值，尤其在 FFN
    中，这在之前的研究 Bondarenko 等人 ([2024](#bib.bib8)) 中有所讨论。由于 FFN 中的非线性操作，如 SwiGLU Shazeer
    ([2020](#bib.bib41))，我们不能直接应用类似 BBR 的旋转。为了解决这个问题，我们在将激活输入到 $W_{down}$ 之前，采用了在线旋转节点。这种在线旋转是按照快速
    Hadamard 核 Chee 等人 ([2024](#bib.bib9)); Ashkboos 等人 ([2024](#bib.bib5)) 实现的，可以看作是一个动态旋转激活的层。此在线旋转操作效率很高，训练和推理过程中开销可以忽略不计，称为块内旋转（IBR）。注意，IBR
    也可以应用于自注意力模块，但我们在表 [6](#S5.T6 "表 6 ‣ 5.4 消融研究与分析 ‣ 5 实验 ‣ RoLoRA：微调旋转的无异常 LLM
    以实现有效的权重激活量化") 的实验中观察到，这种旋转并未改善性能。
- en: 4.2 Rotation-aware Fine-tuning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 旋转感知微调
- en: After performing both BBR and IBR, the between-block and in-block activation
    outliers are eliminated. This characteristic can lower the quantization error
    during QLoRA training, enabling a more accurate gradient estimation and smoother
    optimization for fine-tuning. However, existing research Bondarenko et al. ([2021](#bib.bib7));
    Kovaleva et al. ([2021](#bib.bib25)) revealed that outliers will change distribution
    or emerge during fine-tuning and pre-training. This poses a new challenge of dynamically
    integrating rotation into LoRA to effectively maintain outlier-free characteristics.
    To design the optimal rotation-aware fine-tuning scheme, we first analyze the
    approximation difficulty when rotation is applied. We assume that the optimal
    weight distribution for specific downstream tasks is $W^{*}$. The optimization
    of LoRA fine-tuning could be indicated as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行了 BBR 和 IBR 后，块间和块内激活的异常值被消除。这一特性可以降低 QLoRA 训练中的量化误差，从而实现更准确的梯度估计和更平滑的微调优化。然而，现有研究
    Bondarenko 等人 ([2021](#bib.bib7)); Kovaleva 等人 ([2021](#bib.bib25)) 揭示了异常值会在微调和预训练过程中改变分布或出现。这提出了将旋转动态集成到
    LoRA 中以有效保持无异常特性的新的挑战。为了设计最佳的旋转感知微调方案，我们首先分析了应用旋转时的逼近难度。我们假设针对特定下游任务的最优权重分布是 $W^{*}$。LoRA
    微调的优化可以表示为
- en: '|  | $\displaystyle\underset{A,B}{\min}\&#124;W^{*}-(W_{0}+AB)\&#124;_{F},$
    |  | (4) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{A,B}{\min}\&#124;W^{*}-(W_{0}+AB)\&#124;_{F},$
    |  | (4) |'
- en: 'where the $\|\cdot\|_{F}$ denotes the Frobenious norm. To insert the LoRA module
    in the rotated models, we propose two rotation-aware fine-tuning schemes, namely
    LoRA After Rotation (LAR) and LoRA Before Rotation (LBR), as shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|\cdot\|_{F}$ 表示弗罗贝尼乌斯范数。为了在旋转模型中插入 LoRA 模块，我们提出了两种旋转感知微调方案，即旋转后 LoRA（LAR）和旋转前
    LoRA（LBR），如图 [3](#S4.F3 "图 3 ‣ 4.2 旋转感知微调 ‣ 4 方法 ‣ RoLoRA：微调旋转的无异常 LLM 以实现有效的权重激活量化")
    所示。
- en: '![Refer to caption](img/ab0b3bc1156acab3135a01bd1cce216b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ab0b3bc1156acab3135a01bd1cce216b.png)'
- en: 'Figure 3: Two schemes for performing rotation-aware fine-tuning: (a) LAR and
    (b) LBR.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：执行旋转感知细化的两种方案：（a）LAR和（b）LBR。
- en: 'In LAR, we first merge the rotation matrix with pre-trained weights and then
    use $R_{1}W_{0}+AB$, and the optimization for these two schemes becomes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在LAR中，我们首先将旋转矩阵与预训练权重合并，然后使用$R_{1}W_{0}+AB$，这两个方案的优化变为：
- en: '|  | LAR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LAR}}\&#124;_{F},O_{\text{LAR}}=W_{FT}-R_{1}W_{0}$
    |  | (5) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | LAR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LAR}}\&#124;_{F},O_{\text{LAR}}=W_{FT}-R_{1}W_{0}$
    |  | (5) |'
- en: '|  | LBR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LBR}}\&#124;_{F},O_{\text{LBR}}=R_{1}^{-1}W_{FT}-W_{0}$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | LBR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LBR}}\&#124;_{F},O_{\text{LBR}}=R_{1}^{-1}W_{FT}-W_{0}$
    |  |'
- en: 'the final optimization is very different. We apply SVD of the approximation
    target $O_{\text{LAR}},O_{\text{LBR}}\in\mathbb{R}^{d\times k}$ as $A\in\mathbb{R}^{m\times
    r}$:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的优化方法非常不同。我们应用了近似目标$O_{\text{LAR}},O_{\text{LBR}}\in\mathbb{R}^{d\times k}$的SVD，作为$A\in\mathbb{R}^{m\times
    r}$：
- en: '|  | $A=U_{[:,:r]}\,S_{[:r,:r]}^{1/2}\in\mathbb{R}^{d\times r},B=S_{[:r,:r]}^{1/2}\,V_{[:,:r]}^{T}\in\mathbb{R}^{r\times
    k}.$ |  | (6) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $A=U_{[:,:r]}\,S_{[:r,:r]}^{1/2}\in\mathbb{R}^{d\times r},B=S_{[:r,:r]}^{1/2}\,V_{[:,:r]}^{T}\in\mathbb{R}^{r\times
    k}.$ |  | (6) |'
- en: 'We verify the approximation error of different rank choices $r$ for the experiments.
    which is shown in Figure. [4](#S4.F4 "Figure 4 ‣ 4.2 Rotation-aware Fine-tuning
    ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"). Based on the results, LAR outperforms LBR in low-rank settings
    with lower approximation error, suggesting LAR is the better design for rotation-aware
    fine-tuning. The better approximation indicates that after the two-stage merging
    with rotation matrices and LoRA weights, the final weights can still retain the
    outlier-free property, which is further validated by ablation experiments in Section [5.4](#S5.SS4
    "5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '我们验证了不同秩选择$r$的近似误差，这在图[4](#S4.F4 "图 4 ‣ 4.2 旋转感知细化 ‣ 4 方法 ‣ RoLoRA: 细化旋转无异常值LLMs以实现有效的权重-激活量化")中展示。根据结果，LAR在低秩设置中具有更低的近似误差，表明LAR是旋转感知细化的更佳设计。更好的近似表明，在经过旋转矩阵和LoRA权重的两阶段合并后，最终权重仍能保留无异常值属性，这一点在第[5.4](#S5.SS4
    "5.4 消融研究与分析 ‣ 5 实验 ‣ RoLoRA: 细化旋转无异常值LLMs以实现有效的权重-激活量化")节的消融实验中得到了进一步验证。'
- en: '![Refer to caption](img/5a96802b6da7a85ea2770cc742757c51.png)![Refer to caption](img/08eb5bf4bc0be31201b53330784887ce.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5a96802b6da7a85ea2770cc742757c51.png)![参见标题](img/08eb5bf4bc0be31201b53330784887ce.png)'
- en: 'Figure 4: SVD approximation error of optimization targets with different LoRA-rotation
    integration schemes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同LoRA-旋转集成方案的优化目标的SVD近似误差。
- en: 'As a result of the optimal rotation-aware fine-tuning scheme under the LAR
    setting, we can effectively retain the outlier-free characteristic during LLM
    fine-tuning, as shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.3 Visual Instruction
    Tuning ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '作为LAR设置下的最优旋转感知细化方案的结果，我们可以有效地保留LLM细化过程中的无异常值特性，如图[5](#S5.F5 "图 5 ‣ 5.3 视觉指令细化
    ‣ 5 实验 ‣ RoLoRA: 细化旋转无异常值LLMs以实现有效的权重-激活量化")所示。'
- en: 5 Experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Table 2: Comparison of the averaged accuracy on seven Zero-shot Common Sense
    Reasoning (ZCSR) tasks and MMLU benchmark across LLaMA series. The detailed accuracy
    for each tasks are listed in Table [9](#A1.T9 "Table 9 ‣ Appendix A Detailed Evaluation
    Results ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") and Table [10](#A1.T10 "Table 10 ‣ Appendix A Detailed Evaluation
    Results ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '表2：LLaMA系列在七个零-shot常识推理（ZCSR）任务和MMLU基准上的平均准确率比较。每个任务的详细准确率列在表[9](#A1.T9 "表
    9 ‣ 附录 A 详细评估结果 ‣ RoLoRA: 细化旋转无异常值LLMs以实现有效的权重-激活量化")和表[10](#A1.T10 "表 10 ‣ 附录
    A 详细评估结果 ‣ RoLoRA: 细化旋转无异常值LLMs以实现有效的权重-激活量化")中。'
- en: '| #Bits | Quantizer | Method | LLaMA-2 7B | LLaMA-2 13B | LLaMA-3 8B |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| #位数 | 量化器 | 方法 | LLaMA-2 7B | LLaMA-2 13B | LLaMA-3 8B |'
- en: '| ZCSR⁷ Avg. | MMLU⁴ Avg. | ZCSR⁷ Avg. | MMLU⁴ Avg. | ZCSR⁷ Avg. | MMLU⁴ Avg.
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ZCSR⁷ 平均 | MMLU⁴ 平均 | ZCSR⁷ 平均 | MMLU⁴ 平均 | ZCSR⁷ 平均 | MMLU⁴ 平均 |'
- en: '| FP16 | - | LoRA | 68.4 | 43.5 | 70.5 | 52.4 | 70.0 | 62.7 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 68.4 | 43.5 | 70.5 | 52.4 | 70.0 | 62.7 |'
- en: '| W4A4 | RTN | LoRA | 35.8 | 23.5 | 34.4 | 24.2 | 36.7 | 23.3 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 35.8 | 23.5 | 34.4 | 24.2 | 36.7 | 23.3 |'
- en: '| RoLoRA | 54.1 ($\uparrow$6.3) | 50.0 ($\uparrow$8.8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 54.1 ($\uparrow$6.3) | 50.0 ($\uparrow$8.8) |'
- en: '| GPTQ | LoRA | 37.0 | 23.5 | 34.4 | 24.4 | 36.6 | 23.9 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 37.0 | 23.5 | 34.4 | 24.4 | 36.6 | 23.9 |'
- en: '| RoLoRA | 62.3 ($\uparrow$14.5) | 56.6 ($\uparrow$14.6) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 62.3 ($\uparrow$14.5) | 56.6 ($\uparrow$14.6) |'
- en: '| W6A6 | RTN | LoRA | 65.3 | 35.9 | 67.3 | 47.3 | 67.7 | 55.3 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 65.3 | 35.9 | 67.3 | 47.3 | 67.7 | 55.3 |'
- en: '| RoLoRA | 66.8 ($\uparrow$0.4) | 67.8 ($\uparrow$4.1) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 66.8 ($\uparrow$0.4) | 67.8 ($\uparrow$4.1) |'
- en: '| GPTQ | LoRA | 65.5 | 35.7 | 68.0 | 47.6 | 67.8 | 54.3 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 65.5 | 35.7 | 68.0 | 47.6 | 67.8 | 54.3 |'
- en: '| RoLoRA | 67.1 ($\uparrow$0.3) | 68.1 ($\uparrow$5.1) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 67.1 ($\uparrow$0.3) | 68.1 ($\uparrow$5.1) |'
- en: 5.1 Settings
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Model, LoRA, Quantizer The models for our experiments include LLaMA2-7B/13B Touvron
    et al. ([2023](#bib.bib44)) and LLaMA3-8B AI@Meta ([2024](#bib.bib2)). We follow
    the settings in LLaMA-Factory Zheng et al. ([2024](#bib.bib51)) to implement the
    training pipeline. The dataset for fine-tuning is Alpaca Taori et al. ([2023](#bib.bib43))
    with 52K samples. The weight PTQ methods are the baseline Round-To-Nearest (RTN)
    and widely used GPTQ Frantar et al. ([2023](#bib.bib16)), and the activation quantizer
    is RTN across all experiments.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、LoRA、量化器 我们实验中使用的模型包括LLaMA2-7B/13B Touvron等人 ([2023](#bib.bib44)) 和 LLaMA3-8B
    AI@Meta ([2024](#bib.bib2))。我们遵循LLaMA-Factory Zheng等人 ([2024](#bib.bib51)) 的设置来实现训练管道。微调的数据集是Alpaca
    Taori等人 ([2023](#bib.bib43)) 提供的52K样本。权重PTQ方法为基线的Round-To-Nearest (RTN) 和广泛使用的GPTQ
    Frantar等人 ([2023](#bib.bib16))，激活量化器在所有实验中均为RTN。
- en: Tasks Our RoLoRA was verified on seven zero-shot commonsense reasoning tasks
    using EleutherAI evaluation harness Gao et al. ([2021](#bib.bib17)). These tasks
    include BoolQ (Clark et al., [2019](#bib.bib11)), PIQA (Bisk et al., [2020](#bib.bib6)),
    HellaSwag (Zellers et al., [2019](#bib.bib48)), WinoGrande (Sakaguchi et al.,
    [2021](#bib.bib38)), ARC-easy and ARC-challenge (Clark et al., [2018](#bib.bib12)),
    and OBQA (Mihaylov et al., [2018](#bib.bib34)). Additionally, we also report the
    accuracy of Massively Multitask Language Understanding (MMLU) benchmark Hendrycks
    et al. ([2020](#bib.bib19)) for our evaluation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 我们的RoLoRA在使用EleutherAI评估工具箱 Gao等人 ([2021](#bib.bib17)) 的七个零样本常识推理任务上进行了验证。这些任务包括BoolQ
    (Clark等人，[2019](#bib.bib11))、PIQA (Bisk等人，[2020](#bib.bib6))、HellaSwag (Zellers等人，[2019](#bib.bib48))、WinoGrande
    (Sakaguchi等人，[2021](#bib.bib38))、ARC-easy 和 ARC-challenge (Clark等人，[2018](#bib.bib12))，以及OBQA
    (Mihaylov等人，[2018](#bib.bib34))。此外，我们还报告了Massively Multitask Language Understanding
    (MMLU) 基准 Hendrycks等人 ([2020](#bib.bib19)) 的准确性。
- en: Baselines We consider two settings for experiments. The first is conducting
    FP16 fine-tuning with RoLoRA, where we compare the W4A4 and W6A6 quantization
    results with LoRA. The second is conducting RoLoRA fine-tuning with 4-bit weight
    quantization, which we refer to as QRoLoRA, and comparing the W4A4 performance
    with other low-bit LoRA methods including QLoRA Dettmers et al. ([2024](#bib.bib14)),
    LoftQ Li et al. ([2024](#bib.bib27)), and IR-LoRA Qin et al. ([2024](#bib.bib35)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 我们考虑了两种实验设置。第一种是在FP16微调下进行RoLoRA微调，在这种设置下我们将W4A4和W6A6量化结果与LoRA进行比较。第二种是在4位权重量化下进行RoLoRA微调，我们称之为QRoLoRA，并将W4A4性能与其他低位LoRA方法（包括QLoRA
    Dettmers等人 ([2024](#bib.bib14))、LoftQ Li等人 ([2024](#bib.bib27)) 和 IR-LoRA Qin等人
    ([2024](#bib.bib35))）进行比较。
- en: 5.2 Main Results
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'We first evaluate RoLoRA against LoRA in FP16 fine-tuning and then apply weight-activation
    PTQ to the fine-tuned LLMs. To ensure a fair comparison, both RoLoRA and LoRA
    use the same settings (rank, epoch, learning rate, etc.). As listed in Table [2](#S5.T2
    "Table 2 ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"), RoLoRA enhances the quantization robustness
    of the LLaMA series across various quantization settings on zero-shot commonsense
    reasoning and MMLU benchmarks. Specifically for the W4A4 low-bit setting, RoLoRA
    outperforms LoRA with an absolute up to 29.5% and 14.6% on ZCSR and MMLU, respectively.
    Furthermore, RoLoRA makes it feasible for near-lossless W6A6 quantization of the
    LLaMa series.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在FP16微调下评估RoLoRA与LoRA的表现，然后将权重激活PTQ应用于微调后的LLMs。为了确保公平比较，RoLoRA和LoRA都使用相同的设置（秩、轮次、学习率等）。如表 [2](#S5.T2
    "表 2 ‣ 5 实验 ‣ RoLoRA：为有效的权重-激活量化微调旋转的无异常LLMs")所示，RoLoRA在零样本常识推理和MMLU基准测试中增强了LLaMA系列在各种量化设置下的量化鲁棒性。特别是在W4A4低位设置下，RoLoRA在ZCSR和MMLU上分别比LoRA高出绝对值29.5%和14.6%。此外，RoLoRA使得LLaMa系列的近无损W6A6量化成为可能。
- en: 'Table 3: Comparison of the averaged accuracy of different Low-bit LoRA methods
    on Zero-shot Common Sense Reasoning tasks and MMLU benchmark on LLaMA2-7B.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同低位 LoRA 方法在零样本常识推理任务和 MMLU 基准测试中对 LLaMA2-7B 的平均准确率比较。
- en: '| #Bits | Quantizer | Method | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c
    | OBQA | Avg. | Hums. | STEM | Social | Other | Avg. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| #位数 | 量化器 | 方法 | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c | OBQA |
    平均 | 人文学科 | STEM | 社会 | 其他 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '|  | RTN | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 47.1 | 51.5 | 27.5
    | 49.1 | 28.4 | 24.6 | 25.4 | 36.2 | 24.1 | 24.7 | 22.9 | 21.8 | 23.5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 47.1 | 51.5 | 27.5
    | 49.1 | 28.4 | 24.6 | 25.4 | 36.2 | 24.1 | 24.7 | 22.9 | 21.8 | 23.5 |'
- en: '|  | LoftQ Li et al. ([2024](#bib.bib27)) | 51.5 | 50.8 | 26.6 | 50.4 | 27.5
    | 26.0 | 25.0 | 36.8 | 23.9 | 24.0 | 22.2 | 22.2 | 23.2 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | LoftQ Li et al. ([2024](#bib.bib27)) | 51.5 | 50.8 | 26.6 | 50.4 | 27.5
    | 26.0 | 25.0 | 36.8 | 23.9 | 24.0 | 22.2 | 22.2 | 23.2 |'
- en: '| W4A16 | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 45.5 | 49.7 | 26.7 | 50.6
    | 25.7 | 26.8 | 26.8 | 36.0 | 24.3 | 24.6 | 23.9 | 21.9 | 23.7 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 45.5 | 49.7 | 26.7 | 50.6
    | 25.7 | 26.8 | 26.8 | 36.0 | 24.3 | 24.6 | 23.9 | 21.9 | 23.7 |'
- en: '| $\downarrow$0.8) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\downarrow$0.8) |'
- en: '| W4A4 | GPTQ | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 51.4 | 51.6 |
    27.7 | 51.9 | 29.6 | 25.3 | 26.4 | 37.7 | 24.9 | 24.0 | 22.2 | 22.5 | 23.6 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | GPTQ | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 51.4 | 51.6 |
    27.7 | 51.9 | 29.6 | 25.3 | 26.4 | 37.7 | 24.9 | 24.0 | 22.2 | 22.5 | 23.6 |'
- en: '| LoftQ Li et al. ([2024](#bib.bib27)) | 55.9 | 49.2 | 27.2 | 49.1 | 26.6 |
    26.1 | 24 | 36.9 | 24.1 | 23.8 | 23.3 | 22.7 | 23.6 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ Li et al. ([2024](#bib.bib27)) | 55.9 | 49.2 | 27.2 | 49.1 | 26.6 |
    26.1 | 24 | 36.9 | 24.1 | 23.8 | 23.3 | 22.7 | 23.6 |'
- en: '|  | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 51.1 | 49.8 | 27.6 | 49.3 |
    27.6 | 24.6 | 27.4 | 36.8 | 24.6 | 24.8 | 22.9 | 22.7 | 23.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 51.1 | 49.8 | 27.6 | 49.3 |
    27.6 | 24.6 | 27.4 | 36.8 | 24.6 | 24.8 | 22.9 | 22.7 | 23.9 |'
- en: '|  | RoLoRA | 68.7 | 73.1 | 66.8 | 61.3 | 61.2 | 37.8 | 38.2 | 58.2 ($\uparrow$6.0)
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | RoLoRA | 68.7 | 73.1 | 66.8 | 61.3 | 61.2 | 37.8 | 38.2 | 58.2 ($\uparrow$6.0)
    |'
- en: 'We further evaluate RoLoRA against QLoRA Dettmers et al. ([2024](#bib.bib14))
    and serval baseline methods, including LoftQ Li et al. ([2024](#bib.bib27)), IR-QLoRA Qin
    et al. ([2024](#bib.bib35)), on 4-bit fine-tuning and then apply W4A4 PTQ to the
    low-bit fine-tuned LLaMA2-7B. The performance across seven commonsense reasoning
    tasks and four MMLU subtasks is detailed in Table [3](#S5.T3 "Table 3 ‣ 5.2 Main
    Results ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"). We can see that RoLoRA consistently improves
    the performance of the quantized model using the same quantizer. In particular,
    for W4A4 GPTQ, RoLoRA exceeds QLoRA by 20.5% on the average accuracy of commonsense
    reasoning tasks. Across the experiments on both FP16 and 4-bit fine-tuning, we
    observe that RoLoRA achieves higher performance improvement on the LLMs quantized
    by GPTQ Frantar et al. ([2023](#bib.bib16)) in general. This observation supports
    our claim that RoLoRA retains the outlier-free activation in fine-tuning as GPTQ
    only helps lower the quantization error of weights but not for activation.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步将 RoLoRA 与 QLoRA Dettmers et al. ([2024](#bib.bib14)) 和多个基线方法进行比较，包括 LoftQ Li
    et al. ([2024](#bib.bib27))、IR-QLoRA Qin et al. ([2024](#bib.bib35))，在 4 位微调后，应用
    W4A4 PTQ 于低位微调的 LLaMA2-7B。表 [3](#S5.T3 "Table 3 ‣ 5.2 Main Results ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") 中详细列出了七个常识推理任务和四个 MMLU 子任务的表现。我们可以看到，RoLoRA 一致提高了使用相同量化器的量化模型的表现。特别是，对于
    W4A4 GPTQ，RoLoRA 在常识推理任务的平均准确率上超过了 QLoRA 20.5%。在 FP16 和 4 位微调的实验中，我们观察到 RoLoRA
    在通过 GPTQ Frantar et al. ([2023](#bib.bib16)) 量化的 LLMs 上普遍获得了更高的性能提升。这一观察支持了我们的观点：RoLoRA
    在微调中保留了无异常值的激活，而 GPTQ 仅有助于降低权重的量化误差，而非激活。'
- en: 5.3 Visual Instruction Tuning
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 视觉指令微调
- en: 'We further verify the effectiveness of RoLoRA on visual instruction tuning
    tasks with LLaVA-1.5-7B Liu et al. ([2023a](#bib.bib29)), which consists of a
    language model, Vicuna-7B Chiang et al. ([2023](#bib.bib10)), and a vision encoder
    CLIP ViT-L-336px Radford et al. ([2021](#bib.bib36)). We finetune the LLaVA-1.5-7B
    on LLaVA-Instruct-150K²²2https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K.
    We only perform quantization on the language model and evaluate the LLaVA with
    quantized Vicuna and full-precision vision encoder on LLaVA-bench (COCO) Liu et al.
    ([2024a](#bib.bib30)) with GPT-4 Achiam et al. ([2023](#bib.bib1)). The relative
    score across the conversation, detail description, and complex reasoning are reported
    in Table. [4](#S5.T4 "Table 4 ‣ 5.3 Visual Instruction Tuning ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), where we can observe from the results that RoLoRA help improve
    the quantization robustness and keep the multi-modal ability during PTQ to the
    better extent with an increase up to 18.9 overall scores. We also provide an example
    of the detail description task on a given image shown in Table. [1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"). While the W4A4 LoRA model only gives a rough
    superficial description of the images, our W4A4 RoLoRA model fully elaborates
    the details, such as the toppings and containers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步验证了 RoLoRA 在 LLaVA-1.5-7B Liu et al. ([2023a](#bib.bib29)) 视觉指令调优任务上的有效性，该模型包括一个语言模型
    Vicuna-7B Chiang et al. ([2023](#bib.bib10)) 和一个视觉编码器 CLIP ViT-L-336px Radford
    et al. ([2021](#bib.bib36))。我们在 LLaVA-Instruct-150K²²2https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
    上微调了 LLaVA-1.5-7B。我们只对语言模型进行量化，并在 LLaVA-bench (COCO) Liu et al. ([2024a](#bib.bib30))
    上评估了量化的 Vicuna 和全精度视觉编码器，使用 GPT-4 Achiam et al. ([2023](#bib.bib1))。表中报告了对话、详细描述和复杂推理的相对评分。我们可以从结果中观察到，RoLoRA
    帮助提高了量化的鲁棒性，并在 PTQ 过程中更好地保持了多模态能力，总体评分最多提高了 18.9。我们还提供了表中给定图像的详细描述任务的示例。[1](#S1.T1
    "表 1 ‣ 1 引言 ‣ RoLoRA：旋转无离群值 LLM 的微调以有效的权重-激活量化")。虽然 W4A4 LoRA 模型仅对图像提供了粗略的表面描述，但我们的
    W4A4 RoLoRA 模型全面阐述了细节，例如配料和容器。
- en: 'Table 4: Comparison of the W4A4 quantization performance on LLaVA-Bench of
    LLaVA-1.5-7B.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLaVA-1.5-7B 上的 W4A4 量化性能比较。
- en: '| #Bits | Quantizer | Method | Conv. | Detail | Reas. | Overall |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 量化器 | 方法 | Conv. | Detail | Reas. | Overall |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4A4 | RTN | LoRA | 43.2 | 29.6 | 31.6 | 34.9 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 43.2 | 29.6 | 31.6 | 34.9 |'
- en: '| RoLoRA | 68.8 | 40.5 | 51.9 | 53.8 ($\uparrow$18.9) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 68.8 | 40.5 | 51.9 | 53.8 ($\uparrow$18.9) |'
- en: '| GPTQ | LoRA | 70.6 | 41.8 | 47.9 | 53.5 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 70.6 | 41.8 | 47.9 | 53.5 |'
- en: '| RoLoRA | 67.5 | 48.3 | 66.2 | 60.8 ($\uparrow$7.3) | ![Refer to caption](img/d1555b770ca3b2cd3857fd6df3a43ead.png)![Refer
    to caption](img/50cc5ac8683797f4d364ffc0ce763737.png)![Refer to caption](img/60d0898aef6d4eebddbb4cec896e3558.png)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '| RoLoRA | 67.5 | 48.3 | 66.2 | 60.8 ($\uparrow$7.3) | ![参见说明](img/d1555b770ca3b2cd3857fd6df3a43ead.png)![参见说明](img/50cc5ac8683797f4d364ffc0ce763737.png)![参见说明](img/60d0898aef6d4eebddbb4cec896e3558.png)'
- en: 'Figure 5: Left: The training dynamics of the average Kurtosis of activations,
    Middle: The distribution of Kurtosis of activations across all layers in the final
    model after fine-tuning with LoRA and RoLoRA, Right: The accumulative quantization
    error of W4A4 GPTQ across all layers in the final model after fine-tuning with
    LoRA and RoLoRA.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：左：激活的平均峰度的训练动态，中：最终模型中所有层的激活峰度分布（经过 LoRA 和 RoLoRA 微调后），右：最终模型中所有层的 W4A4
    GPTQ 累积量化误差（经过 LoRA 和 RoLoRA 微调后）。
- en: 5.4 Ablation Study and Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究与分析
- en: 'When to Apply Rotation? Different from the Rotation-Aware Fine-tuning (RAF)
    scheme that rotates the LLMs before LoRA fine-tuning, we can also directly apply
    rotation on an already-finetuned LoRA model. This possible paradigm of LoRA$\rightarrow$PTQ
    is referred to as post-training rotation. We evaluate post-training rotation using
    the same training setting as RoLoRA across the LLaMA series. The W4A4 GPTQ performance
    on seven zero-shot commonsense reasoning tasks are listed in Table [5](#S5.T5
    "Table 5 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA: Fine-tuning
    Rotated Outlier-free LLMs for Effective Weight-Activation Quantization"). The
    results indicate that applying rotation before LoRA can consistently enhance the
    quantization robustness of the fine-tuned LLMs.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '何时应用旋转？不同于在LoRA微调之前旋转LLMs的旋转感知微调（RAF）方案，我们还可以直接在已经微调的LoRA模型上应用旋转。这种LoRA$\rightarrow$PTQ的可能范式称为后训练旋转。我们使用与RoLoRA相同的训练设置评估后训练旋转，跨越LLaMA系列。表
    [5](#S5.T5 "Table 5 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")列出了七个零样本常识推理任务中的W4A4
    GPTQ性能。结果表明，在LoRA之前应用旋转可以持续提升微调LLMs的量化鲁棒性。'
- en: 'Table 5: Ablation on when to apply rotation.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：关于何时应用旋转的消融实验。
- en: '| Method | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B |'
- en: '| RoLoRA | 62.3 | 63.9 | 56.6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 62.3 | 63.9 | 56.6 |'
- en: '| Post-Training Rotation | 58.7 ($\downarrow$1.4) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 后训练旋转 | 58.7 ($\downarrow$1.4) |'
- en: 'Where to Apply Rotation? In Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣
    RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we introduce two types of rotation in our pipeline, namely Between-Block
    Rotation applied on all weight matrices and In-Block Rotation applied on down_proj
    in FFN. As discussed in Section [4.1](#S4.SS1 "4.1 Applying Rotation ‣ 4 Method
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we can also apply a similar head-wise IBR $R_{3}$ in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization") by $W^{R}_{v}\leftarrow W_{v}R_{3},W^{R}_{o}\leftarrow
    R_{3}^{-1}W_{o}$ is the best option to eliminate outliers.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '何时应用旋转？在图 [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization")中，我们在流程中引入了两种旋转类型，即应用于所有权重矩阵的块间旋转（Between-Block
    Rotation）和应用于FFN中的down_proj的块内旋转（In-Block Rotation）。如第 [4.1](#S4.SS1 "4.1 Applying
    Rotation ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization")节所讨论，我们还可以在图 [2](#S2.F2 "Figure 2 ‣ 2 Related
    Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization")中应用类似的头部级 IBR $R_{3}$，其中 $W^{R}_{v}\leftarrow W_{v}R_{3},W^{R}_{o}\leftarrow
    R_{3}^{-1}W_{o}$ 是消除异常值的最佳选择。'
- en: 'Table 6: Ablation on where to apply rotation.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：关于应用旋转的消融实验。
- en: '| Method | Rotation | ZCSR⁷ Avg. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 旋转 | ZCSR⁷ 平均值 |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RoLoRA | $R_{1},R_{2}$ | 54.1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | $R_{1},R_{2}$ | 54.1 |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $(-)$13.7) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $(-)$13.7) |'
- en: '| $(-)$4.4) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $(-)$4.4) |'
- en: '| $(+)$0.3) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $(+)$0.3) |'
- en: 'How to Apply LoRA? In Section [4.2](#S4.SS2 "4.2 Rotation-aware Fine-tuning
    ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we propose two rotation-aware fine-tuning schemes LoRA After Rotation
    (LAR) and LoRA Before Rotation (LBR) shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2
    Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free
    LLMs for Effective Weight-Activation Quantization"). We prove that LAR is the
    better paradigm based on the approximation error analysis compared with full-finetuning.
    In Table [7](#S5.T7 "Table 7 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we quantitatively compare the W4A4 quantization performance of
    two schemes on the fine-tuning of the LLaMA2-7B. The LAR scheme demonstrates better
    effectiveness, which corresponds to the approximation analysis shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2 Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如何应用LoRA？在第[4.2节](#S4.SS2 "4.2 旋转感知微调 ‣ 4 方法 ‣ RoLoRA：针对有效权重-激活量化的旋转无异常值LLM微调")中，我们提出了两种旋转感知微调方案LoRA
    After Rotation (LAR)和LoRA Before Rotation (LBR)，如图[3](#S4.F3 "图3 ‣ 4.2 旋转感知微调
    ‣ 4 方法 ‣ RoLoRA：针对有效权重-激活量化的旋转无异常值LLM微调")所示。通过对比分析，我们证明LAR是更好的范式，相较于完全微调。我们在表[7](#S5.T7
    "表7 ‣ 5.4 消融研究与分析 ‣ 5 实验 ‣ RoLoRA：针对有效权重-激活量化的旋转无异常值LLM微调")中定量比较了两种方案在LLaMA2-7B微调上的W4A4量化性能。LAR方案表现出更好的效果，这与图[4](#S4.F4
    "图4 ‣ 4.2 旋转感知微调 ‣ 4 方法 ‣ RoLoRA：针对有效权重-激活量化的旋转无异常值LLM微调")中的近似分析相符。
- en: 'Table 7: Ablation on how to apply LoRA.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：如何应用LoRA的消融研究。
- en: '| #Bits-Quantizer | Method | ZCSR⁷ Avg. | MMLU⁴ Avg. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| #Bits-Quantizer | 方法 | ZCSR⁷ 平均 | MMLU⁴ 平均 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| W4A4-GPTQ | LAR | 62.3 | 31.0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| W4A4-GPTQ | LAR | 62.3 | 31.0 |'
- en: '| LBR | 61.1 ($\downarrow$0.6) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LBR | 61.1 ($\downarrow$0.6) |'
- en: 'Outliers Retaining the outlier-free characteristic during LLM fine-tuning is
    the most important motivation for RoLoRA. To quantitatively validate the effect
    of outlier elimination, we use kurtosis $\kappa=\frac{\sum_{i}^{k}(\mathbf{x}_{i}-\mu)^{4}}{\sigma^{4}+\epsilon}$
    across all layers is significantly reduced, which further gives rise to the low
    quantization error compared to the LoRA baseline. We also compare the activation
    distribution of RoLoRA against LoRA across layers in Figure [7](#A3.F7 "Figure
    7 ‣ Appendix C Activation Distribution Visualization ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization") in the Appendix.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值 保持无异常值特征在LLM微调过程中是RoLoRA的最重要动机。为了定量验证异常值消除的效果，我们使用所有层的峰度$\kappa=\frac{\sum_{i}^{k}(\mathbf{x}_{i}-\mu)^{4}}{\sigma^{4}+\epsilon}$显著减少，这进一步导致相较于LoRA基线的低量化误差。我们还在附录图[7](#A3.F7
    "图7 ‣ 附录C 激活分布可视化 ‣ RoLoRA：针对有效权重-激活量化的旋转无异常值LLM微调")中比较了RoLoRA和LoRA在各层的激活分布。
- en: LoRA rank settings We explore the robustness of LoRA and RoLoRA towards various
    rank settings $r\in\{4,8,16,32,64\}$ when fine-tuning LLaMA2-7B and evaluated
    on zero-shot commonsense reasoning tasks. The optimal rank setting for RoLoRA
    and LoRA are 16 and 32, respectively. The lower optimal rank indicates the potential
    of our RoLoRA to save trainable parameters. Overall, RoLoRA consistently outperforms
    LoRA regardless of the rank setting, demonstrating its robustness.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA排名设置 我们探讨了LoRA和RoLoRA在不同排名设置$r\in\{4,8,16,32,64\}$下的鲁棒性，微调LLaMA2-7B并在零-shot常识推理任务上进行评估。RoLoRA和LoRA的最佳排名设置分别为16和32。较低的最佳排名显示出我们RoLoRA在节省可训练参数方面的潜力。总体而言，RoLoRA在任何排名设置下始终优于LoRA，显示出其鲁棒性。
- en: '![Refer to caption](img/2f83175753160c38d5d60fce916239e5.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f83175753160c38d5d60fce916239e5.png)'
- en: 'Figure 6: Average accuracy of W4A4 LLaMA2-7B fine-tuned with RoLoRA for varying
    ranks $r$.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：W4A4 LLaMA2-7B在不同排名$r$下，经过RoLoRA微调后的平均准确率。
- en: 'Efficiency For the fine-tuning efficiency of RoLoRA, the additional training
    time is only incurred by the online rotation operation ($R_{2}$, batch size as
    8, 3 total epochs) in Table [8](#S5.T8 "Table 8 ‣ 5.4 Ablation Study and Analysis
    ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"), where RoLoRA significantly improve W4A4 quantized
    LLaMA2-7B performance with extremely low additional overhead.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 RoLoRA 的微调效率，额外的训练时间仅由在线旋转操作 ($R_{2}$, 批量大小为 8, 总共 3 个周期) 产生，如表 [8](#S5.T8
    "Table 8 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA: Fine-tuning
    Rotated Outlier-free LLMs for Effective Weight-Activation Quantization") 所示，其中
    RoLoRA 在极低的额外开销下显著提高了 W4A4 量化 LLaMA2-7B 的性能。'
- en: 'Table 8: The fine-tuning costs comparison on LLaMA2-7B with batch size as 8
    on NVIDIA H800 80G GPUs.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在 NVIDIA H800 80G GPU 上，批量大小为 8 时 LLaMA2-7B 的微调成本比较。
- en: '| Method | Training Time | GPU Memory | ZCSR⁷ Avg. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 训练时间 | GPU 内存 | ZCSR⁷ 平均值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LoRA | 3.55 h | 23.0 GB | 37.0 (GPTQ) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 3.55 h | 23.0 GB | 37.0 (GPTQ) |'
- en: '| RoLoRA | 3.65 h | 23.1 GB | 62.3 (GPTQ) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 3.65 h | 23.1 GB | 62.3 (GPTQ) |'
- en: 6 Conclusion
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presents RoLoRA, the first work to explore the feasibility of weight-activation
    quantization in LoRA. RoLoRA applies rotation for eliminating outliers in activation
    distribution and performs rotation-aware fine-tuning to preserve the outlier-free
    characteristics. We theoretically and empirically investigate how to integrate
    rotation into LoRA. RoLoRA improves the performance of W4A4 and W6A6 LLMs by a
    great margin across various tasks with the same training cost. Moreover, RoLoRA
    can also help visual instruction tuning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了 RoLoRA，这是第一个探索 LoRA 中权重-激活量化可行性的工作。RoLoRA 通过旋转消除激活分布中的异常值，并执行旋转感知微调以保留无异常值的特性。我们从理论和实证上探讨了如何将旋转集成到
    LoRA 中。RoLoRA 在各种任务中以相同的训练成本大幅提升了 W4A4 和 W6A6 LLM 的性能。此外，RoLoRA 还可以帮助视觉指令微调。
- en: Limitation
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: In this work, we propose a rotation-based fine-tuning method that can effectively
    improve quantization robustness to low-bit weight-activation PTQ via retaining
    the outlier-free characteristics. The fine-tuning is conducted on NVIDIA H800
    GPUs, while the recent NVIDIA Blackwell-architecture GPUs with 4-bit floating
    point support may further improve the efficiency. We will take the limitations
    into account and improve in future work.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种基于旋转的微调方法，该方法能够有效提高量化鲁棒性，通过保持无异常值的特性来实现对低位权重-激活 PTQ 的改进。微调在 NVIDIA
    H800 GPU 上进行，而最近的 NVIDIA Blackwell 架构 GPU 支持 4 位浮点运算，可能会进一步提高效率。我们将考虑这些限制，并在未来的工作中进行改进。
- en: Acknowledgement
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This research is supported by HKSAR RGC General Research Fund (GRF) #16208823.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究得到了香港特别行政区研究资助局 (RGC) 一般研究基金 (GRF) #16208823 的资助。'
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等 (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等。2023. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta (2024) AI@Meta. 2024. [Llama 3 模型卡](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'Ashkboos et al. (2023a) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari
    do Nascimento, Torsten Hoefler, and James Hensman. 2023a. Slicegpt: Compress large
    language models by deleting rows and columns. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等 (2023a) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento,
    Torsten Hoefler, 和 James Hensman. 2023a. Slicegpt: 通过删除行和列压缩大型语言模型。发表于 *第十二届国际学习表征会议*。'
- en: Ashkboos et al. (2023b) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023b. Towards
    end-to-end 4-bit inference on generative large language models. *arXiv preprint
    arXiv:2310.09259*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等 (2023b) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong,
    Xincheng Wang, Jie Ren, Torsten Hoefler, 和 Dan Alistarh. 2023b. 迈向生成大型语言模型的端到端
    4 位推理。*arXiv 预印本 arXiv:2310.09259*。
- en: 'Ashkboos et al. (2024) Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    2024. Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等 (2024) Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci,
    Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, 和 James Hensman. 2024. Quarot:
    在旋转 LLM 中进行无异常值 4 位推理。*arXiv 预印本 arXiv:2404.00456*。'
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人.
    2020. Piqa: 论自然语言中的物理常识推理. 在 *AAAI 人工智能会议论文集*，第34卷，第7432–7439页。'
- en: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2021. Understanding and overcoming the challenges of efficient transformer quantization.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 7947–7969.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel 和 Tijmen Blankevoort.
    2021. 理解和克服高效变压器量化的挑战. 在 *2021 年自然语言处理实证方法会议论文集*，第7947–7969页。
- en: 'Bondarenko et al. (2024) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2024. Quantizable transformers: Removing outliers by helping attention heads do
    nothing. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko et al. (2024) Yelysei Bondarenko, Markus Nagel 和 Tijmen Blankevoort.
    2024. 可量化的变压器：通过帮助注意力头什么都不做来去除异常值. *神经信息处理系统进展*, 36。
- en: 'Chee et al. (2024) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. 2024. Quip: 2-bit quantization of large language models with guarantees.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee et al. (2024) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher
    M De Sa. 2024. Quip: 大型语言模型的 2 位量化及其保证. *神经信息处理系统进展*, 36。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica 和 Eric P. Xing. 2023. [Vicuna: 一个开源聊天机器人以 90%* chatgpt 质量打动 gpt-4](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins 和 Kristina Toutanova. 2019. Boolq: 探索自然是/否问题的惊人难度. *arXiv 预印本
    arXiv:1905.10044*。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick 和 Oyvind Tafjord. 2018. 认为你已经解决了问题回答？试试 arc，ai2 推理挑战.
    *arXiv 预印本 arXiv:1803.05457*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 大规模变压器的 8 位矩阵乘法. *神经信息处理系统进展*, 35:30318–30332。'
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman 和 Luke
    Zettlemoyer. 2024. Qlora: 高效的量化大语言模型微调. *神经信息处理系统进展*, 36。'
- en: Elhage et al. (2023) Nelson Elhage, Robert Lasenby, and Christopher Olah. 2023.
    Privileged bases in the transformer residual stream. *Transformer Circuits Thread*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage et al. (2023) Nelson Elhage, Robert Lasenby 和 Christopher Olah. 2023.
    变压器残差流中的特权基. *变压器电路线程*。
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan
    Alistarh. 2023. Gptq: 生成预训练变压器的精确后训练量化. 在 *第十一届国际学习表征会议*。'
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. 2021. A framework for few-shot language model evaluation. *Version v0\.
    0.1\. Sept*, page 8.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff
    等人. 2021. 少样本语言模型评估框架. *版本 v0\. 0.1\. 九月*, 第8页。
- en: 'Hayou et al. (2024) Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024. Lora+:
    Efficient low rank adaptation of large models. *arXiv preprint arXiv:2402.12354*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hayou 等（2024）Soufiane Hayou、Nikhil Ghosh 和 Bin Yu。2024年。《Lora+: 大型模型的高效低秩适应》。*arXiv
    预印本 arXiv:2402.12354*。'
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2020）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song 和 Jacob Steinhardt。2020年。《衡量大规模多任务语言理解》。发表于 *国际学习表征会议*。
- en: Heo et al. (2024) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim,
    Se Jung Kwon, and Dongsoo Lee. 2024. Rethinking channel dimensions to isolate
    outliers for low-bit weight quantization of large language models. In *The Twelfth
    International Conference on Learning Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heo 等（2024）Jung Hwan Heo、Jeonghoon Kim、Beomseok Kwon、Byeongwook Kim、Se Jung
    Kwon 和 Dongsoo Lee。2024年。《重新思考通道维度以隔离异常值，进行大型语言模型的低位权重量化》。发表于 *第十二届国际学习表征会议*。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Edward J Hu、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean Wang、Lu
    Wang、Weizhu Chen 等。2021年。《Lora: 大型语言模型的低秩适应》。发表于 *国际学习表征会议*。'
- en: 'Hu et al. (2023) Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
    Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. Llm-adapters:
    An adapter family for parameter-efficient fine-tuning of large language models.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2023）Zhiqiang Hu、Lei Wang、Yihuai Lan、Wanyu Xu、Ee-Peng Lim、Lidong Bing、Xing
    Xu、Soujanya Poria 和 Roy Ka-Wei Lee。2023年。《Llm-adapters: 一种用于大型语言模型参数高效微调的适配器家族》。发表于
    *2023年自然语言处理实证方法会议*。'
- en: Huang et al. (2023) Xijie Huang, Zhiqiang Shen, and Kwang-Ting Cheng. 2023.
    Variation-aware vision transformer quantization. *arXiv preprint arXiv:2307.00331*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2023）Xijie Huang、Zhiqiang Shen 和 Kwang-Ting Cheng。2023年。《关注变化的视觉变换器量化》。*arXiv
    预印本 arXiv:2307.00331*。
- en: 'Kopiczko et al. (2024) Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano.
    2024. VeRA: Vector-based random matrix adaptation. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kopiczko 等（2024）Dawid Jan Kopiczko、Tijmen Blankevoort 和 Yuki M Asano。2024年。《VeRA:
    基于向量的随机矩阵适应》。发表于 *第十二届国际学习表征会议*。'
- en: 'Kovaleva et al. (2021) Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and
    Anna Rumshisky. 2021. Bert busters: Outlier dimensions that disrupt transformers.
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kovaleva 等（2021）Olga Kovaleva、Saurabh Kulshreshtha、Anna Rogers 和 Anna Rumshisky。2021年。《Bert
    busters: 扰乱变换器的异常维度》。*计算语言学协会发现：ACL-IJCNLP 2021*。'
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 4582–4597.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang（2021）Xiang Lisa Li 和 Percy Liang。2021年。《Prefix-tuning: 优化生成的连续提示》。发表于
    *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，第4582–4597页。'
- en: 'Li et al. (2024) Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. 2024. Loftq: Lora-fine-tuning-aware quantization
    for large language models. In *The Twelfth International Conference on Learning
    Representations*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2024）Yixiao Li、Yifan Yu、Chen Liang、Nikos Karampatziakis、Pengcheng He、Weizhu
    Chen 和 Tuo Zhao。2024年。《Loftq: 针对大型语言模型的 Lora 微调感知量化》。发表于 *第十二届国际学习表征会议*。'
- en: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.
    Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
    *Proceedings of Machine Learning and Systems*, 6:87–100.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等（2024）Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Wei-Ming Chen、Wei-Chen
    Wang、Guangxuan Xiao、Xingyu Dang、Chuang Gan 和 Song Han。2024年。《Awq: 激活感知的权重量化用于设备上
    LLM 压缩和加速》。*机器学习与系统会议论文集*，6:87–100。'
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.
    Improved baselines with visual instruction tuning. *arXiv preprint arXiv:2310.03744*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）Haotian Liu、Chunyuan Li、Yuheng Li 和 Yong Jae Lee。2023a年。《通过视觉指令调整改进基线》。*arXiv
    预印本 arXiv:2310.03744*。
- en: Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2024a. Visual instruction tuning. *Advances in neural information processing systems*,
    36.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee. 2024a. 视觉指令调整.
    *神经信息处理系统进展*, 36。
- en: 'Liu et al. (2023b) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. 2023b. Llm-fp4: 4-bit floating-point quantized transformers.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023b) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong 和 Kwang-Ting
    Cheng. 2023b. Llm-fp4: 4 位浮点量化变换器. 见 *2023 年自然语言处理实证方法会议*。'
- en: 'Liu et al. (2024b) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov,
    Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024b. Dora: Weight-decomposed
    low-rank adaptation. *arXiv preprint arXiv:2402.09353*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2024b) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
    Frank Wang, Kwang-Ting Cheng 和 Min-Hung Chen. 2024b. Dora: 权重分解低秩适配. *arXiv 预印本
    arXiv:2402.09353*。'
- en: Liu et al. (2024c) Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv
    Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen
    Blankevoort. 2024c. Spinquant–llm quantization with learned rotations. *arXiv
    preprint arXiv:2405.16406*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2024c) Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv
    Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian 和 Tijmen Blankevoort.
    2024c. Spinquant–llm 量化与学习旋转. *arXiv 预印本 arXiv:2405.16406*。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open
    book question answering. *arXiv preprint arXiv:1809.02789*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等 (2018) Todor Mihaylov, Peter Clark, Tushar Khot 和 Ashish Sabharwal.
    2018. 一套盔甲能导电吗？一个新的开放式书籍问答数据集. *arXiv 预印本 arXiv:1809.02789*。
- en: Qin et al. (2024) Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang,
    Shouda Liu, Jie Luo, Xianglong Liu, and Michele Magno. 2024. Accurate lora-finetuning
    quantization of llms via information retention. *arXiv preprint arXiv:2402.05445*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2024) Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang,
    Shouda Liu, Jie Luo, Xianglong Liu 和 Michele Magno. 2024. 精确的 lora 微调量化通过信息保留.
    *arXiv 预印本 arXiv:2402.05445*。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*, pages 8748–8763\.
    PMLR.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark 等. 2021. 从自然语言监督中学习可转移的视觉模型. 见 *国际机器学习会议*, 页 8748–8763。PMLR。
- en: 'Ritter (1996) Terry Ritter. 1996. Walsh-hadamard transforms: A literature survey.
    *Research Comments from Cipers by Ritter*, page 10.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ritter (1996) Terry Ritter. 1996. Walsh-Hadamard 变换: 文献综述. *Ritter 的 Cipers
    研究评论*, 第 10 页。'
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi 等 (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和
    Yejin Choi. 2021. Winogrande: 大规模对抗性 Winograd 语料库挑战. *ACM 通讯*, 64(9):99–106。'
- en: 'Shang et al. (2024) Yuzhang Shang, Zhihang Yuan, and Zhen Dong. 2024. Pb-llm:
    Partially binarized large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shang 等 (2024) Yuzhang Shang, Zhihang Yuan 和 Zhen Dong. 2024. Pb-llm: 部分二值化的大型语言模型.
    见 *第十二届国际学习表征会议*。'
- en: 'Shao et al. (2024) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. Omniquant:
    Omnidirectionally calibrated quantization for large language models. In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等 (2024) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao,
    Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao 和 Ping Luo. 2024. Omniquant: 面向大型语言模型的全方位校准量化.
    见 *第十二届国际学习表征会议*。'
- en: Shazeer (2020) Noam Shazeer. 2020. Glu variants improve transformer. *arXiv
    preprint arXiv:2002.05202*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer (2020) Noam Shazeer. 2020. Glu 变体改进变换器. *arXiv 预印本 arXiv:2002.05202*。
- en: Sun et al. (2024) Mingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu.
    2024. Massive activations in large language models. *arXiv preprint arXiv:2402.17762*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2024) Mingjie Sun, Xinlei Chen, J. Zico Kolter 和 Zhuang Liu. 2024. 大型语言模型中的大规模激活.
    *arXiv 预印本 arXiv:2402.17762*。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: 一种指令跟随型的 llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等等。2023. Llama 2: 开放基础和微调聊天模型。 *arXiv 预印本 arXiv:2307.09288*。'
- en: 'Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2022. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu, 和 Xianglong Liu. 2022. 异常值抑制：推动低位变压器语言模型的极限。 *神经信息处理系统进展*，35:17402–17414。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. 2023. Smoothquant: 大型语言模型的准确高效后训练量化。在 *国际机器学习会议* 上，第 38087–38099 页。PMLR。'
- en: 'Xu et al. (2024) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
    Zhang, Zhengsu Chen, XIAOPENG ZHANG, and Qi Tian. 2024. Qa-lora: Quantization-aware
    low-rank adaptation of large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2024) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
    Zhang, Zhengsu Chen, XIAOPENG ZHANG, 和 Qi Tian. 2024. Qa-lora: 大型语言模型的量化感知低秩适配。在
    *第十二届国际学习表征会议* 上。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. 2019. Hellaswag: 机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*。'
- en: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu,
    Yu Qiao, Hongsheng Li, and Peng Gao. 2023. Llama-adapter: Efficient fine-tuning
    of large language models with zero-initialized attention. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu,
    Yu Qiao, Hongsheng Li, 和 Peng Gao. 2023. Llama-adapter: 使用零初始化注意力的高效大型语言模型微调。在
    *第十二届国际学习表征会议* 上。'
- en: 'Zhao et al. (2024) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2024.
    Atom: Low-bit quantization for efficient and accurate llm serving. *Proceedings
    of Machine Learning and Systems*, 6:196–209.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2024) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, 和 Baris Kasikci. 2024.
    Atom: 高效且准确的低位量化 LLM 服务。 *机器学习与系统会议论文集*，6:196–209。'
- en: 'Zheng et al. (2024) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan
    Luo, and Yongqiang Ma. 2024. [Llamafactory: Unified efficient fine-tuning of 100+
    language models](http://arxiv.org/abs/2403.13372). *arXiv preprint arXiv:2403.13372*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2024) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan
    Luo, 和 Yongqiang Ma. 2024. [Llamafactory: 100+ 语言模型的统一高效微调](http://arxiv.org/abs/2403.13372)。
    *arXiv 预印本 arXiv:2403.13372*。'
- en: Appendix A Detailed Evaluation Results
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 详细评估结果
- en: 'Table [9](#A1.T9 "Table 9 ‣ Appendix A Detailed Evaluation Results ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")
    and Table [10](#A1.T10 "Table 10 ‣ Appendix A Detailed Evaluation Results ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")
    listed the full evaluation results on zero-shot commonsense reasoning tasks and
    MMLU benchmarks, respectively. We use the ‘acc_norm’ in the evaluation report
    given by EleutherAI evaluation harness Gao et al. ([2021](#bib.bib17)) as the
    accuracy if there are such metrics. Otherwise, we use ‘acc’.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [9](#A1.T9 "表 9 ‣ 附录 A 详细评估结果 ‣ RoLoRA：微调旋转无异常值 LLM 以实现有效的权重-激活量化")和表 [10](#A1.T10
    "表 10 ‣ 附录 A 详细评估结果 ‣ RoLoRA：微调旋转无异常值 LLM 以实现有效的权重-激活量化")列出了零-shot 常识推理任务和 MMLU
    基准上的完整评估结果。我们使用 EleutherAI 评估工具 Gao 等人（[2021](#bib.bib17)）提供的评估报告中的‘acc_norm’作为准确率指标，如果存在该指标。否则，我们使用‘acc’。
- en: 'Table 9: Full accuracy comparison on zero-shot commonsense reasoning tasks
    of LLaMA series.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：LLaMA 系列在零-shot 常识推理任务上的完整准确率比较。
- en: '| #Bits | Quantizer | Method | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c
    | OBQA | Avg. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | Quantizer | Method | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c
    | OBQA | Avg. |'
- en: '| LLaMA2-7B |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B |'
- en: '| FP16 | - | LoRA | 81.2 | 79.8 | 78.6 | 70.6 | 73.9 | 47.7 | 46.8 | 68.4 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 81.2 | 79.8 | 78.6 | 70.6 | 73.9 | 47.7 | 46.8 | 68.4 |'
- en: '| W4A4 | RTN | LoRA | 46.0 | 49.5 | 27.0 | 49.6 | 27.8 | 24.2 | 26.8 | 35.8
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 46.0 | 49.5 | 27.0 | 49.6 | 27.8 | 24.2 | 26.8 | 35.8
    |'
- en: '| RoLoRA | 67.1 | 67.7 | 59.7 | 56.9 | 58.3 | 35.0 | 34.2 | 54.1 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 67.1 | 67.7 | 59.7 | 56.9 | 58.3 | 35.0 | 34.2 | 54.1 |'
- en: '| GPTQ | LoRA | 52.3 | 52.5 | 26.9 | 50.4 | 28.6 | 25.3 | 22.8 | 37.0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 52.3 | 52.5 | 26.9 | 50.4 | 28.6 | 25.3 | 22.8 | 37.0 |'
- en: '| RoLoRA | 73.5 | 76.2 | 71.8 | 64.1 | 67.7 | 42.2 | 40.4 | 62.3 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 73.5 | 76.2 | 71.8 | 64.1 | 67.7 | 42.2 | 40.4 | 62.3 |'
- en: '| W6A6 | RTN | LoRA | 76.3 | 78.0 | 75.3 | 69.2 | 71.2 | 45.7 | 41.6 | 65.3
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 76.3 | 78.0 | 75.3 | 69.2 | 71.2 | 45.7 | 41.6 | 65.3
    |'
- en: '| RoLoRA | 77.9 | 79.1 | 76.3 | 68.5 | 74.8 | 47.3 | 43.6 | 66.8 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 77.9 | 79.1 | 76.3 | 68.5 | 74.8 | 47.3 | 43.6 | 66.8 |'
- en: '| GPTQ | LoRA | 76.3 | 78.2 | 75.4 | 69.5 | 72.1 | 46.1 | 40.8 | 65.5 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 76.3 | 78.2 | 75.4 | 69.5 | 72.1 | 46.1 | 40.8 | 65.5 |'
- en: '| RoLoRA | 77.4 | 79.1 | 76.5 | 70.4 | 75.2 | 47.2 | 44.0 | 67.1 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 77.4 | 79.1 | 76.5 | 70.4 | 75.2 | 47.2 | 44.0 | 67.1 |'
- en: '| LLaMA2-13B |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B |'
- en: '| FP16 | - | LoRA | 83.9 | 81.2 | 80.9 | 74.2 | 74.4 | 51.3 | 47.6 | 70.5 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 83.9 | 81.2 | 80.9 | 74.2 | 74.4 | 51.3 | 47.6 | 70.5 |'
- en: '| W4A4 | RTN | LoRA | 39.8 | 52.1 | 26.1 | 45.7 | 25.9 | 25.8 | 25.4 | 34.4
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 39.8 | 52.1 | 26.1 | 45.7 | 25.9 | 25.8 | 25.4 | 34.4
    |'
- en: '| RoLoRA | 70.6 | 73.9 | 67.2 | 59.6 | 66.8 | 38.7 | 34.2 | 58.7 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 70.6 | 73.9 | 67.2 | 59.6 | 66.8 | 38.7 | 34.2 | 58.7 |'
- en: '| GPTQ | LoRA | 38.0 | 50.2 | 26.0 | 49.0 | 25.9 | 26.4 | 25.4 | 34.4 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 38.0 | 50.2 | 26.0 | 49.0 | 25.9 | 26.4 | 25.4 | 34.4 |'
- en: '| RoLoRA | 74.0 | 77.2 | 73.9 | 66.0 | 73.3 | 43.9 | 38.8 | 63.9 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 74.0 | 77.2 | 73.9 | 66.0 | 73.3 | 43.9 | 38.8 | 63.9 |'
- en: '| W6A6 | RTN | LoRA | 80.8 | 78.1 | 77.8 | 70.3 | 73.0 | 49.2 | 42.2 | 67.3
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 80.8 | 78.1 | 77.8 | 70.3 | 73.0 | 49.2 | 42.2 | 67.3
    |'
- en: '| RoLoRA | 80.3 | 78.8 | 78.0 | 71.1 | 77.6 | 49.6 | 43.2 | 68.4 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 80.3 | 78.8 | 78.0 | 71.1 | 77.6 | 49.6 | 43.2 | 68.4 |'
- en: '| GPTQ | LoRA | 81.9 | 79.2 | 78.5 | 69.3 | 74.3 | 51.5 | 41.2 | 68.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 81.9 | 79.2 | 78.5 | 69.3 | 74.3 | 51.5 | 41.2 | 68.0 |'
- en: '| RoLoRA | 80.6 | 79.3 | 78.1 | 72.5 | 77.4 | 49.4 | 44.0 | 68.8 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 80.6 | 79.3 | 78.1 | 72.5 | 77.4 | 49.4 | 44.0 | 68.8 |'
- en: '| LLaMA3-8B |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B |'
- en: '| FP16 | - | LoRA | 64.6 | 82.4 | 81.4 | 75.1 | 81.8 | 56.5 | 48.0 | 70.0 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 64.6 | 82.4 | 81.4 | 75.1 | 81.8 | 56.5 | 48.0 | 70.0 |'
- en: '| W4A4 | RTN | LoRA | 46.7 | 52.2 | 29.7 | 47.6 | 29.3 | 24.7 | 26.6 | 36.7
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 46.7 | 52.2 | 29.7 | 47.6 | 29.3 | 24.7 | 26.6 | 36.7
    |'
- en: '| RoLoRA | 58.0 | 67.3 | 57.7 | 56.0 | 49.0 | 30.2 | 31.8 | 50.0 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 58.0 | 67.3 | 57.7 | 56.0 | 49.0 | 30.2 | 31.8 | 50.0 |'
- en: '| GPTQ | LoRA | 42.5 | 54.4 | 29.4 | 49.0 | 31.1 | 22.5 | 27.0 | 36.6 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 42.5 | 54.4 | 29.4 | 49.0 | 31.1 | 22.5 | 27.0 | 36.6 |'
- en: '| RoLoRA | 63.2 | 71.1 | 66.7 | 60.2 | 60.3 | 38.2 | 36.8 | 56.6 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 63.2 | 71.1 | 66.7 | 60.2 | 60.3 | 38.2 | 36.8 | 56.6 |'
- en: '| W6A6 | RTN | LoRA | 75.5 | 78.3 | 77.4 | 70.8 | 76.4 | 51.2 | 44.0 | 67.7
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 75.5 | 78.3 | 77.4 | 70.8 | 76.4 | 51.2 | 44.0 | 67.7
    |'
- en: '| RoLoRA | 78.6 | 79.5 | 76.7 | 71.1 | 77.6 | 49.8 | 40.8 | 67.8 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 78.6 | 79.5 | 76.7 | 71.1 | 77.6 | 49.8 | 40.8 | 67.8 |'
- en: '| GPTQ | LoRA | 77.9 | 78.3 | 77.9 | 71.3 | 75.2 | 50.5 | 43.2 | 67.8 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 77.9 | 78.3 | 77.9 | 71.3 | 75.2 | 50.5 | 43.2 | 67.8 |'
- en: '| RoLoRA | 78.1 | 79.3 | 76.8 | 71.9 | 76.7 | 50.9 | 42.8 | 68.1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 78.1 | 79.3 | 76.8 | 71.9 | 76.7 | 50.9 | 42.8 | 68.1 |'
- en: 'Table 10: Full accuracy on MMLU Benchmark of LLaMA series.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：LLaMA 系列在 MMLU 基准上的完整准确率。
- en: '| #Bits | Quantizer | Method | Hums. | Other | Social | STEM | Avg. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | Quantizer | Method | Hums. | Other | Social | STEM | Avg. |'
- en: '| LLaMA2-7B |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B |'
- en: '| FP16 | - | LoRA | 41.5 | 50.8 | 48.2 | 34.7 | 43.5 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 41.5 | 50.8 | 48.2 | 34.7 | 43.5 |'
- en: '| W4A4 | RTN | LoRA | 24.2 | 24.8 | 22.7 | 21.7 | 23.5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 24.2 | 24.8 | 22.7 | 21.7 | 23.5 |'
- en: '| RoLoRA | 24.7 | 26.2 | 27.2 | 25.7 | 25.8 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 24.7 | 26.2 | 27.2 | 25.7 | 25.8 |'
- en: '| GPTQ | LoRA | 24.3 | 24.5 | 23.0 | 22.0 | 23.5 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 24.3 | 24.5 | 23.0 | 22.0 | 23.5 |'
- en: '| RoLoRA | 30.1 | 33.0 | 32.0 | 29.4 | 31.0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 30.1 | 33.0 | 32.0 | 29.4 | 31.0 |'
- en: '| W6A6 | RTN | LoRA | 35.4 | 40.6 | 37.5 | 30.4 | 35.9 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 35.4 | 40.6 | 37.5 | 30.4 | 35.9 |'
- en: '| RoLoRA | 38.2 | 45.4 | 44.7 | 35.2 | 40.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 38.2 | 45.4 | 44.7 | 35.2 | 40.5 |'
- en: '| GPTQ | LoRA | 34.2 | 39.4 | 39.4 | 30.6 | 35.7 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 34.2 | 39.4 | 39.4 | 30.6 | 35.7 |'
- en: '| RoLoRA | 37.8 | 46.1 | 46.2 | 34.9 | 40.8 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 37.8 | 46.1 | 46.2 | 34.9 | 40.8 |'
- en: '| LLaMA2-13B |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B |'
- en: '| FP16 | - | LoRA | 49.6 | 59.2 | 59.9 | 42.8 | 52.4 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 49.6 | 59.2 | 59.9 | 42.8 | 52.4 |'
- en: '| W4A4 | RTN | LoRA | 25.0 | 25.7 | 23.4 | 22.4 | 24.2 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 25.0 | 25.7 | 23.4 | 22.4 | 24.2 |'
- en: '| RoLoRA | 28.9 | 32.5 | 33.2 | 28.4 | 30.5 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 28.9 | 32.5 | 33.2 | 28.4 | 30.5 |'
- en: '| GPTQ | LoRA | 25.5 | 24.2 | 24.1 | 23.4 | 24.4 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 25.5 | 24.2 | 24.1 | 23.4 | 24.4 |'
- en: '| RoLoRA | 37.7 | 42.3 | 43.7 | 32.7 | 38.9 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 37.7 | 42.3 | 43.7 | 32.7 | 38.9 |'
- en: '| W6A6 | RTN | LoRA | 44.3 | 52.8 | 55.0 | 38.6 | 47.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 44.3 | 52.8 | 55.0 | 38.6 | 47.3 |'
- en: '| RoLoRA | 45.0 | 52.9 | 55.2 | 39.1 | 47.7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 45.0 | 52.9 | 55.2 | 39.1 | 47.7 |'
- en: '| GPTQ | LoRA | 44.8 | 54.7 | 53.8 | 39.0 | 47.6 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 44.8 | 54.7 | 53.8 | 39.0 | 47.6 |'
- en: '| RoLoRA | 45.6 | 53.7 | 55.2 | 38.7 | 47.9 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 45.6 | 53.7 | 55.2 | 38.7 | 47.9 |'
- en: '| LLaMA3-8B |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B |'
- en: '| FP16 | - | LoRA | 57.4 | 70.7 | 72.8 | 52.7 | 62.7 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 57.4 | 70.7 | 72.8 | 52.7 | 62.7 |'
- en: '| W4A4 | RTN | LoRA | 23.6 | 24.3 | 23.7 | 21.8 | 23.3 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 23.6 | 24.3 | 23.7 | 21.8 | 23.3 |'
- en: '| RoLoRA | 30.8 | 34.5 | 33.5 | 30.5 | 32.1 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 30.8 | 34.5 | 33.5 | 30.5 | 32.1 |'
- en: '| GPTQ | LoRA | 24.6 | 23.0 | 23.4 | 24.3 | 23.9 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 24.6 | 23.0 | 23.4 | 24.3 | 23.9 |'
- en: '| RoLoRA | 36.0 | 42.2 | 43.6 | 33.5 | 38.5 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 36.0 | 42.2 | 43.6 | 33.5 | 38.5 |'
- en: '| W6A6 | RTN | LoRA | 49.7 | 63.0 | 64.4 | 47.2 | 55.3 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 49.7 | 63.0 | 64.4 | 47.2 | 55.3 |'
- en: '| RoLoRA | 52.7 | 67.5 | 70.0 | 51.1 | 59.4 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 52.7 | 67.5 | 70.0 | 51.1 | 59.4 |'
- en: '| GPTQ | LoRA | 48.8 | 61.8 | 63.9 | 45.7 | 54.3 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 48.8 | 61.8 | 63.9 | 45.7 | 54.3 |'
- en: '| RoLoRA | 52.9 | 68.3 | 69.6 | 50.4 | 59.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 52.9 | 68.3 | 69.6 | 50.4 | 59.4 |'
- en: Appendix B Hyper-parameters for Reproduction
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 重现的超参数
- en: 'In Table [11](#A2.T11 "Table 11 ‣ Appendix B Hyper-parameters for Reproduction
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we list the detailed hyper-parameters for reproducing RoLoRA and
    LoRA results. We do not apply searches on any hyperparameters for better accuracy,
    all the settings for the LLaMA series and LLaVA align with the default settings
    of Zheng et al. ([2024](#bib.bib51)) and Liu et al. ([2024a](#bib.bib30)).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [11](#A2.T11 "表 11 ‣ 附录 B 超参数 ‣ RoLoRA: 细化旋转无异常值的 LLM 以有效量化权重-激活") 中，我们列出了重现
    RoLoRA 和 LoRA 结果的详细超参数。我们没有对任何超参数进行搜索以提高准确性，所有 LLaMA 系列和 LLaVA 的设置都与 Zheng 等人
    ([2024](#bib.bib51)) 和 Liu 等人 ([2024a](#bib.bib30)) 的默认设置一致。'
- en: 'Table 11: Detailed hyper-parameters for fine-tuning different LLMs and LMMs.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 细化不同 LLM 和 LMM 的详细超参数。'
- en: '| Model | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B | LLaVA-1.5-7B |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B | LLaVA-1.5-7B |'
- en: '| Epoch | 3 | 3 | 3 | 1 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 3 | 3 | 3 | 1 |'
- en: '| Learning Rate | 3 | 3 | 3 | 1 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 3 | 3 | 3 | 1 |'
- en: '| Batch Size (Per GPU) | 8 | 4 | 8 | 2 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小（每 GPU） | 8 | 4 | 8 | 2 |'
- en: '| Gradient Accumulation | 1 | 2 | 1 | 64 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积 | 1 | 2 | 1 | 64 |'
- en: '| Warmup Ratio | 0.01 | 0.01 | 0.01 | 0.03 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 热身比例 | 0.01 | 0.01 | 0.01 | 0.03 |'
- en: '| Optimizer | AdamW | AdamW | AdamW | AdamW |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | AdamW | AdamW | AdamW |'
- en: '| LoRA Rank $r$ | 16 | 16 | 16 | 128 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 秩 $r$ | 16 | 16 | 16 | 128 |'
- en: '| LoRA Dropout | 0 | 0 | 0 | 0.05 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LoRA Dropout | 0 | 0 | 0 | 0.05 |'
- en: '| LoRA Target | $W_{q},W_{v}$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 目标 | $W_{q},W_{v}$ |'
- en: '| Learning Rate | 1$e^{-4}$ |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1$e^{-4}$ |'
- en: Appendix C Activation Distribution Visualization
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 激活分布可视化
- en: 'We visualize the magnitude of the activation of fine-tuned LLaMA2-7B using
    LoRA and RoLoRA in Figure [7](#A3.F7 "Figure 7 ‣ Appendix C Activation Distribution
    Visualization ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"). The visualizations reveal a noticeable amount of outliers presented
    in the LoRA fine-tuned model, but are highly eliminated in RoLoRA counterpart.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [7](#A3.F7 "图 7 ‣ 附录 C 激活分布可视化 ‣ RoLoRA: 细化旋转无异常值的 LLM 以有效量化权重-激活") 中可视化了细化的
    LLaMA2-7B 的激活幅度。可视化揭示了 LoRA 细化模型中出现的明显异常值，但在 RoLoRA 对应模型中已被高度消除。'
- en: '![Refer to caption](img/7e0d1683e37aac51b1dc395391226a29.png)![Refer to caption](img/761c6c2a6c68947cef0f467d528f8516.png)![Refer
    to caption](img/a995724a91e279744e7292e080aaf73b.png)![Refer to caption](img/0afb8891443f47fbed639199c8a8439c.png)![Refer
    to caption](img/3a0864878a767aad5a00b0ed87070e5e.png)![Refer to caption](img/6fe34c22d67859aa4bae1c9ce1d6bc93.png)![Refer
    to caption](img/00e6386ec3fccc7b6987824fc63b689e.png)![Refer to caption](img/1cef0b939bd4a95b44b0cb73932a06b6.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e0d1683e37aac51b1dc395391226a29.png)![参见说明](img/761c6c2a6c68947cef0f467d528f8516.png)![参见说明](img/a995724a91e279744e7292e080aaf73b.png)![参见说明](img/0afb8891443f47fbed639199c8a8439c.png)![参见说明](img/3a0864878a767aad5a00b0ed87070e5e.png)![参见说明](img/6fe34c22d67859aa4bae1c9ce1d6bc93.png)![参见说明](img/00e6386ec3fccc7b6987824fc63b689e.png)![参见说明](img/1cef0b939bd4a95b44b0cb73932a06b6.png)'
- en: 'Figure 7: Final activation distribution of the fine-tuned model produced using
    RoLoRA and LoRA. We select the output activation of q_proj across layers with
    the index of 0, 1, 6, 11, 16, 21, 26, 31.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：使用RoLoRA和LoRA微调模型产生的最终激活分布。我们选择了层索引为0、1、6、11、16、21、26、31的q_proj输出激活。
