- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:35:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:35:11'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based
    Summarization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发挥LLMs的力量：一种高质量基于方面的摘要微调方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02584](https://ar5iv.labs.arxiv.org/html/2408.02584)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02584](https://ar5iv.labs.arxiv.org/html/2408.02584)
- en: ¹Ankan Mullick   ¹Sombit Bose   ¹Rounak Saha   ²Ayan Kumar Bhowmick
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹安坎·穆利克   ¹宋比特·博斯   ¹罗纳克·萨哈   ²阿扬·库马尔·博姆尼克
- en: ² Aditya Vempaty    ¹Pawan Goyal    ¹ Niloy Ganguly    ²Prasenjit Dey    ²Ravi
    Kokku
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ² 阿迪提亚·温帕提    ¹帕万·戈亚尔    ¹尼洛伊·甘古利    ²普拉森吉特·德伊    ²拉维·科库
- en: '{ankanm, sbcs.sombit, runk20}@kgpian.iitkgp.ac.in'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{ankanm, sbcs.sombit, runk20}@kgpian.iitkgp.ac.in'
- en: '{pawang, niloy}@cse.iitkgp.ac.in,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{pawang, niloy}@cse.iitkgp.ac.in,'
- en: '{ayan, aditya, prasenjit, ravi}@merlyn.org'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{ayan, aditya, prasenjit, ravi}@merlyn.org'
- en: ¹Computer Science and Engineering Department, IIT Kharagpur, India. ²Emergence
    AI
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ¹印度伊特坎普尔理工学院计算机科学与工程系。 ²Emergence AI
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ever-increasing volume of digital information necessitates efficient methods
    for users to extract key insights from lengthy documents. Aspect-based summarization
    offers a targeted approach, generating summaries focused on specific aspects within
    a document. Despite advancements in aspect-based summarization research, there
    is a continuous quest for improved model performance. Given that large language
    models (LLMs) have demonstrated the potential to revolutionize diverse tasks within
    natural language processing, particularly in the problem of summarization, this
    paper explores the potential of fine-tuning LLMs for the aspect-based summarization
    task. We evaluate the impact of fine-tuning open-source foundation LLMs, including
    Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect
    based summary dataset. We hypothesize that this approach will enable these models
    to effectively identify and extract aspect-related information, leading to superior
    quality aspect-based summaries compared to the state-of-the-art. We establish
    a comprehensive evaluation framework to compare the performance of fine-tuned
    LLMs against competing aspect-based summarization methods and vanilla counterparts
    of the fine-tuned LLMs. Our work contributes to the field of aspect-based summarization
    by demonstrating the efficacy of fine-tuning LLMs for generating high-quality
    aspect-based summaries. Furthermore, it opens doors for further exploration of
    using LLMs for targeted information extraction tasks across various NLP domains.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数字信息量的不断增长要求用户采用高效的方法从长篇文档中提取关键见解。基于方面的摘要提供了一种有针对性的方法，生成关注文档中特定方面的摘要。尽管基于方面的摘要研究取得了进展，但对改进模型性能的追求仍在持续。鉴于大型语言模型（LLMs）在自然语言处理的各种任务中展示了颠覆性的潜力，特别是在摘要问题上，本文探讨了对LLMs进行微调以处理基于方面的摘要任务的潜力。我们评估了对开源基础LLMs，包括Llama2、Mistral、Gemma和Aya，进行微调对公开的领域特定基于方面的摘要数据集的影响。我们假设这种方法将使这些模型能够有效地识别和提取方面相关的信息，从而生成比当前最先进技术更优质的基于方面的摘要。我们建立了一个综合评价框架，以比较微调后的LLMs与竞争的基于方面的摘要方法以及微调前的LLMs的性能。我们的工作通过展示微调LLMs生成高质量基于方面的摘要的有效性，贡献了基于方面的摘要领域。此外，这也为在各种NLP领域中使用LLMs进行有针对性的信息提取任务开辟了进一步探索的机会。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The ever-growing volume of information in various digital formats presents a
    significant challenge for users who need to efficiently extract key insights from
    large documents. Automatic text summarization has emerged as a valuable tool to
    address this challenge, providing concise and informative representations of textual
    documents - El-Kassas et al. ([2021](#bib.bib11)); Gambhir & Gupta ([2017](#bib.bib15));
    Tas & Kiyani ([2007](#bib.bib38)). While traditional summarization techniques
    aim to capture the overall gist of a document, aspect-based summarization offers
    a more focused approach.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 各种数字格式中信息量的不断增加给需要从大文档中高效提取关键信息的用户带来了重大挑战。自动文本摘要已经成为应对这一挑战的有价值工具，提供了文本文档的简明和信息丰富的表示
    - El-Kassas et al. ([2021](#bib.bib11)); Gambhir & Gupta ([2017](#bib.bib15));
    Tas & Kiyani ([2007](#bib.bib38))。虽然传统的摘要技术旨在捕捉文档的整体要点，但基于方面的摘要提供了一种更为集中化的方法。
- en: Aspect-based summarization goes beyond generic summarization by targeting specific
    aspects or topics within a document - Frermann & Klementiev ([2019](#bib.bib13));
    Coavoux et al. ([2019](#bib.bib8)); Mukherjee et al. ([2020](#bib.bib31)). This
    targeted approach is particularly valuable for large documents, such as research
    papers, product reviews, or news articles, where specific information about certain
    aspects might be crucial for the reader. Aspect-based summarization allows users
    to delve deeper into a document by quickly providing summaries that cater to their
    specific information needs. For instance, consider a researcher reviewing a medical
    study. Using aspect-based summarization, they can prioritize summaries that highlight
    the methodology and results sections. Conversely, a customer reading online reviews
    for a new phone might prioritize summaries emphasizing aspects like battery life
    or camera performance - Li et al. ([2020](#bib.bib27)); Kunneman et al. ([2018](#bib.bib24)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方面的摘要通过针对文档中的特定方面或主题超越了通用摘要 - Frermann & Klementiev ([2019](#bib.bib13));
    Coavoux et al. ([2019](#bib.bib8)); Mukherjee et al. ([2020](#bib.bib31))。这种针对性的
    approach 对于大型文档尤为重要，如研究论文、产品评论或新闻文章，其中关于某些方面的具体信息可能对读者至关重要。基于方面的摘要使用户能够更深入地了解文档，通过快速提供符合其具体信息需求的摘要。例如，考虑一位研究人员审阅医学研究。使用基于方面的摘要，他们可以优先关注突出方法论和结果部分的摘要。相反，一位顾客在阅读新手机的在线评论时，可能会优先关注强调电池寿命或相机性能等方面的摘要
    - Li et al. ([2020](#bib.bib27)); Kunneman et al. ([2018](#bib.bib24))。
- en: Hence, effective generation of aspect-based summaries presents a unique challenge.
    Unlike generic summarization, which focuses on capturing the overall gist of a
    document, aspect-based summarization requires models to not only comprehend the
    document’s content but also identify and extract information pertinent to specific
    aspects. This necessitates models that can not only understand the semantics of
    the text but also possess the ability to discern and prioritize aspect-related
    information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有效生成基于方面的摘要是一个独特的挑战。与关注捕捉文档整体要点的通用摘要不同，基于方面的摘要要求模型不仅理解文档的内容，还需识别和提取与特定方面相关的信息。这要求模型不仅能够理解文本的语义，还具备辨别和优先处理与方面相关信息的能力。
- en: Despite the significant strides made in aspect-based summarization research,
    there remains an ongoing quest for models capable of generating even higher quality
    summaries. While several state-of-the-art methods like Falcon, BART, Pegasus,
    T5, and LED [Lewis et al. ([2020](#bib.bib26)); Penedo et al. ([2023](#bib.bib33));
    Wan & Bansal ([2022](#bib.bib43)); Guo et al. ([2022](#bib.bib17))] among others
    have yielded promising results, there is a continual exploration for novel approaches
    that can elevate the quality of aspect-based summaries. This quest motivates the
    exploration of new approaches, such as fine-tuning large language models (LLMs)
    (Huang et al. ([2022](#bib.bib21)); Ding et al. ([2023](#bib.bib10))) for the
    task of aspect-based summarization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在基于方面的摘要研究方面取得了显著进展，但仍然在不断寻求能够生成更高质量摘要的模型。尽管像 Falcon、BART、Pegasus、T5 和 LED
    [Lewis et al. ([2020](#bib.bib26)); Penedo et al. ([2023](#bib.bib33)); Wan &
    Bansal ([2022](#bib.bib43)); Guo et al. ([2022](#bib.bib17))] 等几个最先进的方法已取得了令人鼓舞的结果，但对能够提升基于方面摘要质量的新方法的探索仍在继续。这种探索促使我们研究新方法，例如对大型语言模型（LLMs）的微调（Huang
    et al. ([2022](#bib.bib21)); Ding et al. ([2023](#bib.bib10)))，以应对基于方面的摘要任务。
- en: LLMs represent a transformative paradigm shift in natural language processing
    (NLP). These powerful models, trained on massive datasets of text and code, have
    demonstrated exceptional capabilities in various NLP tasks, including text generation,
    translation, and question answering among others (Wei et al. ([2022](#bib.bib45));
    Hoffmann et al. ([2022](#bib.bib20))). The ability of LLMs to capture intricate
    linguistic patterns and relationships within text(Yang et al. ([2023](#bib.bib48)))
    makes them a compelling candidate for enhancing the performance of aspect-based
    summarization task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 代表了自然语言处理（NLP）领域的一次变革性范式转变。这些强大的模型在大量文本和代码数据集上进行训练，展示了在各种 NLP 任务中的卓越能力，包括文本生成、翻译和问答等
    (Wei et al. ([2022](#bib.bib45)); Hoffmann et al. ([2022](#bib.bib20)))。LLMs 捕捉文本中复杂语言模式和关系的能力（Yang
    et al. ([2023](#bib.bib48))) 使其成为提升基于方面的摘要任务表现的有力候选者。
- en: 'In this paper, we aim to study the impact of finetuning LLMs (Yang et al. ([2024](#bib.bib46)))
    for the task of aspect-based summarization and demonstrate the improvement in
    the quality of generated aspect-based summaries over vanilla LLMs. Our work centers
    around the concept of fine-tuning recent open-source foundation LLMs, including
    Llama2 (Touvron et al. ([2023](#bib.bib40))), Mistral (Jiang et al. ([2023](#bib.bib23))),
    Gemma (Team et al. ([2024](#bib.bib39))) and Aya (Üstün et al. ([2024](#bib.bib41))).
    Precisely, we investigate the potential of fine-tuning such open-source foundation
    LLMs on a dataset specifically tailored for the task of aspect-based summarization.
    By fine-tuning these LLMs on aspect-based summarization datasets, we aim to equip
    them with the necessary expertise to effectively identify, extract, and generate
    summaries that focus on user-specified aspects within a document such that the
    fine-tuned LLMs can achieve superior performance compared to existing methods.
    In this paper, we seek to address the following research questions through making
    contributions related to the field of aspect-based summarization:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们旨在研究微调LLM（Yang et al. ([2024](#bib.bib46))) 对于基于方面的总结任务的影响，并展示与原始LLM相比生成的基于方面的总结质量的改进。我们的工作围绕微调最近的开源基础LLM的概念，包括Llama2
    (Touvron et al. ([2023](#bib.bib40))), Mistral (Jiang et al. ([2023](#bib.bib23))),
    Gemma (Team et al. ([2024](#bib.bib39)))和Aya (Üstün et al. ([2024](#bib.bib41)))。具体来说，我们调查了在专门针对基于方面总结任务的数据集上微调这些开源基础LLM的潜力。通过在基于方面总结数据集上微调这些LLM，我们旨在使它们具备有效识别、提取和生成专注于文档中用户指定方面的总结的必要专业知识，从而使微调后的LLM能相比现有方法实现更优的性能。在本文中，我们通过对基于方面总结领域的贡献，力求解决以下研究问题：
- en: 1\. Does fine-tuning LLMs provide a significant benefit for aspect-based summarization
    tasks?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 微调LLM是否对基于方面的总结任务提供了显著的益处？
- en: 2\. How effective are fine-tuned LLMs compared to vanilla LLMs and other state-of-the-art
    methods for aspect-based summarization?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 微调后的LLM与原始LLM和其他最新方法相比，在基于方面的总结任务中的效果如何？
- en: 3\. Does the effectiveness of fine-tuning LLMs vary depending on the base model
    architecture?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 微调LLM的有效性是否取决于基础模型架构？
- en: 4\. How robust is the fine-tuned LLM for variations in dataset and domains for
    aspect-based summarization?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 微调后的LLM对于数据集和领域变化的鲁棒性如何？
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'In this section, we perform a survey of the state-of-the-art literature on
    summarization and discuss the literature on different types of summarization as
    follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对最新的总结文献进行了调查，并讨论了不同类型总结的文献，如下所述：
- en: 2.1 Generic Summarization
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 泛型总结
- en: We focus on brief literature survey on generic summarization, which encompasses
    a broad approach to summarizing text without focusing on specific aspects, queries,
    or goals, using abstractive or extractive approaches. Among abstractive approaches,
    Chopra et al. ([2016](#bib.bib6)) introduced an abstractive summarization model
    using attentive recurrent neural networks and discuss the challenges of generating
    coherent and informative summaries while avoiding redundancy. Based on pointer-generator
    network framework, See et al. ([2017](#bib.bib35)) presents a model that combines
    extractive and abstractive techniques for summarization by effectively incorporating
    source information into the generated summaries. On the other hand, among purely
    extractive approaches, earlier researchers used graph-based approaches like TextRank
    (Mihalcea & Tarau ([2004](#bib.bib30))) and LexRank (Erkan & Radev ([2004](#bib.bib12))).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重点关注泛型总结的简要文献综述，该综述涵盖了对文本进行总结的广泛方法，而不关注特定的方面、查询或目标，使用抽象或提取方法。在抽象方法中，Chopra
    et al. ([2016](#bib.bib6)) 介绍了一种使用注意力递归神经网络的抽象总结模型，并讨论了生成连贯且信息丰富的总结而避免冗余的挑战。基于指针生成网络框架，See
    et al. ([2017](#bib.bib35)) 提出了一个结合了提取和抽象技术的总结模型，通过有效地将源信息纳入生成的总结中。另一方面，在纯提取方法中，早期研究人员使用了基于图的方法，如TextRank
    (Mihalcea & Tarau ([2004](#bib.bib30))) 和LexRank (Erkan & Radev ([2004](#bib.bib12)))。
- en: 2.2 Aspect-based Summarization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于方面的总结
- en: Hayashi et al. ([2021](#bib.bib19)) employed a method for aspect-based summarization
    focusing on multiple domains while (Coavoux et al. ([2019](#bib.bib8))) focused
    on aspect-based multi-document abstractive summarization with an unsupervised
    approach. Few works have also explored domain-specific aspect-based summarization
    such as (Mukherjee et al. ([2020](#bib.bib31))) that focus on data from tourist
    review domain and (Akhtar et al. ([2017](#bib.bib1))) that focus on dataset of
    hotel reviews. (Tang et al. ([2016](#bib.bib37))) developed a deep memory network
    for aspect-level sentiment classification, emphasizing the extraction of aspects
    within a document and these are relevant for aspect-based summarization. Again,
    (Wang et al. ([2016](#bib.bib44))) proposed an attention-based LSTM model which
    helps identify and emphasize important aspects and these are used for aspect-based
    summarization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Hayashi et al. ([2021](#bib.bib19)) 采用了一种关注多个领域的基于方面的总结方法，而 (Coavoux et al.
    ([2019](#bib.bib8))) 专注于基于方面的多文档抽象总结，并采用了无监督的方法。一些研究还探讨了领域特定的基于方面的总结，例如 (Mukherjee
    et al. ([2020](#bib.bib31))) 关注于旅游评论领域的数据，(Akhtar et al. ([2017](#bib.bib1)))
    则关注于酒店评论的数据集。 (Tang et al. ([2016](#bib.bib37))) 开发了一个深度记忆网络，用于方面级情感分类，强调了从文档中提取方面，这些方法对基于方面的总结非常相关。再次，(Wang
    et al. ([2016](#bib.bib44))) 提出了一个基于注意力的 LSTM 模型，帮助识别和强调重要方面，这些方面被用于基于方面的总结。
- en: 2.3 Use of LLMs for summary evaluation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 使用 LLMs 进行摘要评估
- en: LLMs are recently emerging as alternatives to traditional metrics and human
    evaluation for evaluating NLP tasks. Recent work has explored LLM-based NLG evaluation
    methods (Gao et al. ([2024](#bib.bib16))) while (Chan et al. ([2023](#bib.bib5)))
    assessed the quality of generated responses from different models on open-ended
    questions. (Zhou et al. ([2023](#bib.bib52))) have proposed guidelines for LLM
    use for evaluations while few works have proposed techniques to improve LLM evaluation
    performance [Hasanbeig et al. ([2023](#bib.bib18)); Liu et al. ([2023](#bib.bib29))]
    and (Huang et al. ([2023](#bib.bib22))) have also investigated the explainability
    of LLMs in evaluation contexts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 最近作为评估 NLP 任务的传统指标和人工评估的替代方案逐渐出现。最近的工作探讨了基于 LLM 的 NLG 评估方法 (Gao et al.
    ([2024](#bib.bib16)))，而 (Chan et al. ([2023](#bib.bib5))) 评估了不同模型在开放式问题上的生成回应质量。
    (Zhou et al. ([2023](#bib.bib52))) 提出了 LLM 用于评估的指南，而一些工作提出了提高 LLM 评估性能的技术 [Hasanbeig
    et al. ([2023](#bib.bib18)); Liu et al. ([2023](#bib.bib29))]，(Huang et al. ([2023](#bib.bib22)))
    也研究了 LLM 在评估背景下的可解释性。
- en: In this paper, our focus is on analysing the impact of fine-tuning open-source
    foundation LLMs on the performance of the aspect-based summarization task and
    determine the type of LLMs that can help to generate high quality aspect-based
    summaries either using the pre-trained version or after fine-tuning on relevant
    datasets. We also use LLMs (GPT4) to evaluate summaries on different conventions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们专注于分析微调开源基础 LLMs 对基于方面的总结任务性能的影响，并确定哪种类型的 LLMs 可以帮助生成高质量的基于方面的摘要，无论是使用预训练版本还是在相关数据集上微调后。我们还使用
    LLMs (GPT4) 来评估不同惯例下的摘要。
- en: 3 Dataset
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集
- en: We leverage the publicly available benchmark dataset, Open Aspect-based Summarization
    (OASUM) Yang et al. ([2022](#bib.bib47)), for both fine-tuning open-source foundation
    LLMs and evaluating their performance. OASUM offers a rich collection of over
    3.6 million document-aspect-summary triplets, featuring diverse aspects across
    various domains¹¹1https://github.com/tencent-ailab/OASum. There are 1M unique
    aspects in the entire dataset. The average token count for the documents and aspect-based
    summaries are $1612$ respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用公开的基准数据集 Open Aspect-based Summarization (OASUM) Yang et al. ([2022](#bib.bib47))，用于对开源基础
    LLMs 进行微调以及评估其性能。OASUM 提供了超过 360 万个文档-方面-摘要三元组的丰富集合，涵盖了各个领域的多样化方面¹¹1https://github.com/tencent-ailab/OASum。整个数据集中有
    100 万个独特方面。文档和基于方面的摘要的平均令牌数量分别为 $1612$。
- en: Domain Aspect set HealthCare Death, Diagnosis, Differential diagnosis, Diagnosis-Classification
    Education History, Geography, Taxonomy, Education Life and Career Career, Political
    Career, Personal Life, Life and career Music Production, Composition, Soundtrack,
    Track Listing
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 领域方面集 健康护理 死亡、诊断、鉴别诊断、诊断-分类 教育 历史、地理、分类学、教育 生活与职业 职业、政治职业、个人生活、生活与职业 音乐 制作、作曲、原声带、曲目列表
- en: 'Table 1: Domain-wise breakdown of aspects in OASUM dataset'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：OASUM 数据集中方面的领域分布
- en: 'Data Preprocessing and Variations: To facilitate targeted training and analysis,
    we prepared several variations of the OASUM dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理和变体：为了促进有针对性的训练和分析，我们准备了 OASUM 数据集的几种变体：
- en: '1\. Domain-Wise Split: We selected 16 aspects from four popular domains (Healthcare,
    Music, Education, Life & Career) resulting in a domain-specific dataset of 14,279
    training instances.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 领域特定划分：我们从四个流行领域（医疗保健、音乐、教育、生活与职业）中选择了 16 个方面，结果得到了一个领域特定的数据集，共有 14,279
    个训练实例。
- en: '2\. High-Frequency Aspects: We created the variation OASUM-Hi by choosing the
    top-50 most frequent aspects (based on document count) and randomly selecting
    1,000 documents for each. This dataset investigates the impact of fine-tuning
    on well-represented aspects.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 高频方面：我们通过选择前 50 个最频繁出现的方面（基于文档计数）并随机选择每个方面的 1,000 个文档，创建了变体 OASUM-Hi。这个数据集研究了微调对代表性较强的方面的影响。
- en: '3\. Low-Frequency Aspects: In contrast, the variation OASUM-Lo focuses on the
    long tail of the dataset. We selected the 50 least frequent aspects ($1-4$ document
    occurrences) with 1,000 documents each. This explores fine-tuning performance
    on less common aspects (aka long-tails).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 低频方面：相比之下，变体 OASUM-Lo 关注数据集的长尾部分。我们选择了出现频率最低的 50 个方面（$1-4$ 次文档出现），每个方面有
    1,000 个文档。这探讨了在较少见的方面（即长尾）上的微调性能。
- en: '4\. Random Aspect Selection: The variation OASUM-Ra comprises a randomly selected
    set of 50,000 document-aspect-summary triplets for a domain-agnostic evaluation.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 随机方面选择：变体 OASUM-Ra 包含一个随机选择的 50,000 个文档-方面-摘要三元组集合，用于领域无关的评估。
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3 Dataset ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization") summarizes the key statistics
    for each dataset variation, including aspects, training/validation/test split
    sizes.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S3.T2 "Table 2 ‣ 3 Dataset ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization") 总结了每个数据集变体的关键统计信息，包括方面、训练/验证/测试划分大小。'
- en: Dataset Aspect Train Validation Test OASUM-domain wise 16 14279 500 2544 OASUM-Hi
    50 50000 500 500 OASUM-Lo 8995 50000 500 500 OASUM-Ra 7320 50000 500 500
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 方面 训练 验证 测试 OASUM-domain wise 16 14279 500 2544 OASUM-Hi 50 50000 500 500
    OASUM-Lo 8995 50000 500 500 OASUM-Ra 7320 50000 500 500
- en: 'Table 2: Different OASUM Datasets distribution'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同 OASUM 数据集的分布
- en: 4 Proposed Framework
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提出的框架
- en: In this section, we detail our framework for fine-tuning open-source foundation
    LLMs on the OASUM dataset to obtain corresponding fine-tuned domain-specific LLMs
    specialized for the downstream task of aspect-based summarization. We describe
    the fine-tuning process, the LLM architectures employed, and the baseline models
    used for comparison.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了我们的框架，以便对 OASUM 数据集上的开源基础 LLMs 进行微调，从而获得相应的微调领域特定 LLMs，这些模型专注于面向方面的摘要下游任务。我们描述了微调过程、所采用的
    LLM 架构以及用于比较的基准模型。
- en: 4.1 Model architecture for fine-tuning LLMs
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 微调 LLMs 的模型架构
- en: Our training process consists of employing different open-source foundation
    LLMs for fine-tuning on the training set of OASUM dataset described above. Specifically,
    we leverage supervised fine-tuning (Zhang et al. ([2023](#bib.bib51))) on the
    OASUM training dataset to transform pre-trained foundation LLMs into domain-specific
    models suited to perform aspect-based summarization. This involves utilizing prompt-completion
    pairs to guide the pre-trained models towards generating aspect-based summaries.
    Each training instance comprises a document paired with an instruction to generate
    a summary based on a specific aspect. The corresponding completion is the relevant
    aspect-based summary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练过程包括使用不同的开源基础 LLMs 对上述 OASUM 数据集的训练集进行微调。具体而言，我们利用监督微调（Zhang et al. ([2023](#bib.bib51)))
    对 OASUM 训练数据集进行微调，将预训练的基础 LLMs 转换为适合进行面向方面摘要的领域特定模型。这包括利用提示-完成对来引导预训练模型生成面向方面的摘要。每个训练实例包括一个文档和生成基于特定方面的摘要的指令。相应的完成是相关的面向方面的摘要。
- en: 'To enhance the fine-tuning process, we incorporate advanced techniques like
    Quantized Low-Rank Adaptation (QLoRA) Dettmers et al. ([2023](#bib.bib9)) and
    PEFT (Parameter-Efficient Fine-Tuning) Fu et al. ([2023](#bib.bib14)) to optimize
    training efficiency. Following fine-tuning, these models (referred to as "*FT")
    acquire the ability to generate aspect-based summaries for corresponding documents
    based on the specified aspect within the prompt. Following is a summary of the
    open-source foundation LLMs we fine-tuned on OASUM:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强微调过程，我们采用了先进的技术，如量化低秩适应（QLoRA）（Dettmers et al. ([2023](#bib.bib9))) 和 PEFT（参数高效微调）（Fu
    et al. ([2023](#bib.bib14))) 来优化训练效率。微调后，这些模型（称为 "*FT"）获得了根据提示中的指定方面生成面向方面摘要的能力。以下是我们在
    OASUM 上微调的开源基础 LLMs 的摘要：
- en: '1\. Llama2:²²2https://ai.meta.com/llama/ We use two different versions of Llama2
    - vanilla: with sizes of 7b, 13b and 70b (Touvron et al. ([2023](#bib.bib40)))
    and fine-tuned: using models Llama2-7b and 13b. We have referred the Llama2-7b
    and Llama2-13b fine-tuned version as Lm7b-FT and Lm13b-FT.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. Llama2:²²2https://ai.meta.com/llama/ 我们使用了两个不同版本的 Llama2——原始版本：7b、13b 和
    70b（Touvron et al. ([2023](#bib.bib40))) 和微调版本：使用 Llama2-7b 和 13b 模型。我们将 Llama2-7b
    和 Llama2-13b 微调版本称为 Lm7b-FT 和 Lm13b-FT。
- en: '2\. Mistral: We fine-tuned the Mistral-7b decoder-only Transformer model (Jiang
    et al. ([2023](#bib.bib23))) from Mistral AI, obtaining Mistral-7b-FT (abbreviated
    as Mis7b-Va for vanilla and Mis7b-FT for finetune).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. Mistral: 我们微调了 Mistral-7b 解码器仅模型（Jiang et al. ([2023](#bib.bib23)))，由 Mistral
    AI 提供，获得了 Mistral-7b-FT（缩写为 Mis7b-Va 表示原始版本，Mis7b-FT 表示微调版本）。'
- en: '3\. Gemma: We use Gemma which is a family of lightweight, state-of-the-art
    open models (Team et al. ([2024](#bib.bib39))) developed by Google DeepMind from
    the same technology used to create the Gemini models. Specifically, we finetune
    the Gemma-2b version to obtain the finetuned version referred to as Gemma-FT.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '3\. Gemma: 我们使用 Gemma，这是一系列轻量级的最先进的开源模型（Team et al. ([2024](#bib.bib39)))，由
    Google DeepMind 开发，使用了与创建 Gemini 模型相同的技术。具体来说，我们微调了 Gemma-2b 版本，以获得称为 Gemma-FT
    的微调版本。'
- en: '4\. Aya: We use the Aya Model (Üstün et al. ([2024](#bib.bib41))), a massively
    multilingual 13 billion parameter language model capable of following instructions
    in 101 languages that is developed by Cohere and fine-tune the pre-trained version
    to obtain the fine-tuned model referred to as Aya-FT.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '4\. Aya: 我们使用 Aya 模型（Üstün et al. ([2024](#bib.bib41)))，这是一个拥有 130 亿参数的大型多语言语言模型，能够在
    101 种语言中执行指令，由 Cohere 开发，并微调了预训练版本以获得称为 Aya-FT 的微调模型。'
- en: For performance comparison, we also include the vanilla pre-trained versions
    of each LLM (referred to as "*VA"). These include Llama2-7b-VA (Lm7b-VA), Llama2-13b-VA
    (Lm13b-VA), Llama2-70b-VA (Lm70b-VA), Mistral-7b-VA, Gemma-VA, and Aya-VA.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行性能比较，我们还包括了每个 LLM 的原始预训练版本（称为 "*VA"）。这些包括 Llama2-7b-VA（Lm7b-VA）、Llama2-13b-VA（Lm13b-VA）、Llama2-70b-VA（Lm70b-VA）、Mistral-7b-VA、Gemma-VA
    和 Aya-VA。
- en: 4.2 Baseline models
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基线模型
- en: 'We use the following state-of-the-art competing baselines for comparing the
    performance of aspect-based summarization task against the fine-tuned LLMs and
    their vanilla counterparts:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下最先进的竞争基线来比较面向方面的摘要任务在微调的 LLMs 和它们的原始对比模型上的性能：
- en: '1\. LongFormer: The Longformer (Beltagy et al. ([2020](#bib.bib3))) is a transformer-based
    model designed to handle long documents efficiently using an attention pattern
    that effectively combines local and global information, enabling to handle long
    inputs. We use Longformer-base (LED-ba) and large (LED-La) model with 149 million
    and 439 million parameters.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '1\. LongFormer: Longformer（Beltagy et al. ([2020](#bib.bib3))) 是一种基于变换器的模型，旨在通过使用一种有效结合局部和全局信息的注意力模式高效处理长文档，从而能够处理长输入。我们使用了
    Longformer-base（LED-ba）和 large（LED-La）模型，分别拥有 1.49 亿和 4.39 亿参数。'
- en: '2\. T5 (Text-to-Text Transfer Transformer): This model Raffel et al. ([2020](#bib.bib34))
    leverages transfer learning for summarization tasks by converting them into a
    text-to-text format. We fine-tune the T5-3b version (T5-FT) with 3 billion parameters
    to generate aspect-based summaries.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. T5（Text-to-Text Transfer Transformer）：该模型（Raffel et al. ([2020](#bib.bib34)))
    通过将总结任务转换为文本到文本格式，利用迁移学习进行摘要任务。我们微调了 T5-3b 版本（T5-FT），该版本具有 30 亿参数，用于生成面向方面的摘要。
- en: '3\. Flan T5: Flan-T5 Chung et al. ([2022](#bib.bib7)) instruction fine-tuned
    approach highlights the benefits of fine-tuning across various models, prompting
    setups, and evaluation tasks. We finetune the Flan T5 XL model (Fl-T5-FT).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '3\. Flan T5: Flan-T5（Chung et al. ([2022](#bib.bib7))) 指令微调方法突出了在各种模型、提示设置和评估任务中的微调好处。我们对
    Flan T5 XL 模型（Fl-T5-FT）进行了微调。'
- en: '4\. BART (Bidirectional and Autoregressive Transformer): This denoising autoencoder Lewis
    et al. ([2019](#bib.bib25)) is used for pre-training sequence-to-sequence models.
    We employ the instruction-prompted BART-large model with 406 million parameters,
    pre-trained on English and fine-tuned for summarization on the CNN Daily Mail
    news dataset.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. BART（双向自回归变换器）：该去噪自编码器 Lewis et al. ([2019](#bib.bib25)) 用于对序列到序列模型进行预训练。我们使用带有406百万参数的指令提示BART-large模型，预训练于英语，并在CNN
    Daily Mail新闻数据集上进行摘要微调。
- en: '5\. Pegasus: We utilize the instruction-tuned Pegasus model Zhang et al. ([2020](#bib.bib49))
    with 571 million parameters for generating aspect-based summaries.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. Pegasus：我们利用指令调整的Pegasus模型 Zhang et al. ([2020](#bib.bib49))，具有571百万参数，用于生成基于方面的摘要。
- en: '6\. Falcon: The Falcon 7b-instruction-tuned model Penedo et al. ([2023](#bib.bib33))
    is used for generating aspect-based summaries.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '6\. Falcon: Falcon 7b指令调整模型 Penedo et al. ([2023](#bib.bib33)) 用于生成基于方面的摘要。'
- en: '7\. TLDR: We apply state-of-the-art approach ‘TLDR-CATTS-XSUM’ (TLDR) Cachola
    et al. ([2020](#bib.bib4)) for extreme summarization to obtain crisp summary of
    the document.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. TLDR：我们应用最先进的方法‘TLDR-CATTS-XSUM’ (TLDR) Cachola et al. ([2020](#bib.bib4))
    进行极端总结，以获得文档的简洁摘要。
- en: 5 Experimental evaluation and Results
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验评估和结果
- en: In this section, we evaluate the performance of our different fine-tuned LLM
    models in terms of the quality of the generated aspect-based summaries for documents
    in the OASUM domain wise test set and compare against their vanilla counterparts
    as well as the competing baseline models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们评估了不同微调的LLM模型在OASUM领域测试集中的基于方面的摘要的质量，并与其原始对照模型以及竞争基准模型进行比较。
- en: 5.1 Evaluation metrics and experimental settings
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 评估指标和实验设置
- en: 'Our evaluation relies on two different approaches:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估依赖于两种不同的方法：
- en: '1\. Traditional: Here we check the comptenece of different models with traditional
    evaluation metrics like (i) Rouge 1 (R1), Rouge 2 (R2) and Rouge L (RL) (Lin ([2004](#bib.bib28))),
    (ii) Meteor (Mt) (Banerjee & Lavie ([2005](#bib.bib2))), (iii) Bleu (Bl) Papineni
    et al. ([2002](#bib.bib32))), and (iv) BERTScore F1 (BeF1) (Zhang et al. ([2019](#bib.bib50)))
    to assess the quality of generated summaries.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 传统：在这里，我们检查不同模型在传统评估指标下的表现，如（i）Rouge 1 (R1)、Rouge 2 (R2) 和 Rouge L (RL)
    (Lin ([2004](#bib.bib28)))，（ii）Meteor (Mt) (Banerjee & Lavie ([2005](#bib.bib2)))，（iii）Bleu
    (Bl) Papineni et al. ([2002](#bib.bib32)))，以及（iv）BERTScore F1 (BeF1) (Zhang et al.
    ([2019](#bib.bib50))) 以评估生成摘要的质量。
- en: '2\. GPT-4 Critique: Here, we use the GPT-4 LLM as a critique Valmeekam et al.
    ([2023](#bib.bib42)); Sun et al. ([2024](#bib.bib36)) to evaluate the quality
    of the model generated aspect-based summaries against the gold standard aspect-based
    summaries in the test set of the OASUM dataset variations from different dimensions.
    Specifically, we provide suitable critique based prompts to GPT-4 where we evaluate
    the summaries based on a set of five predefined criterias (termed as GPT-4 criteria)
    defined below:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. GPT-4 批评：在这里，我们使用GPT-4 LLM作为批评工具 Valmeekam et al. ([2023](#bib.bib42));
    Sun et al. ([2024](#bib.bib36)) 来评估模型生成的基于方面的摘要的质量，与OASUM数据集中不同维度的金标准方面摘要进行比较。具体而言，我们向GPT-4提供合适的批评提示，并基于以下五个预定义标准（称为GPT-4标准）来评估摘要：
- en: 'a. Relevance (Re): The extent to which the generated summary is relevant to
    the specific aspect-based summary of the document.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: a. 相关性（Re）：生成的摘要与文档的特定方面摘要的相关程度。
- en: 'b. Coverage (Cv): The extent to which the generated aspect-based summary correctly
    covers all the important key points described in the gold standard aspect-based
    summary of the document.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: b. 覆盖率（Cv）：生成的基于方面的摘要正确覆盖文档的金标准方面摘要中描述的所有重要要点的程度。
- en: 'c. Impurity (Im): The extent to which the aspect-based summary does not contain
    information specific to any other aspect.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: c. 杂质（Im）：基于方面的摘要中不包含任何其他方面特定信息的程度。
- en: 'd. Rating (Ra): Scores how well the summary captures the target aspect with
    the score reflecting if the summary is good, average or bad. A good summary is
    clear, concise, accurate, and engaging. An average summary conveys the main points
    but might lack detail. A bad summary is inaccurate, unclear, non-coherent or overly
    verbose. (Details are in Appendix)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: d. 评分（Ra）：评估摘要捕捉目标方面的效果，评分反映摘要的质量是好、一般还是差。一个好的摘要是清晰、简洁、准确且吸引人的。一个一般的摘要传达了主要观点，但可能缺乏细节。一个差的摘要是不准确、不清晰、不连贯或过于冗长的。（详细信息见附录）
- en: 'e. Goodness (Gd): Extending from 4, we manually verify the goodness of the
    summary.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: e. 优良度（Gd）：从 4 扩展，我们手动验证摘要的优良度。
- en: This combined evaluation strategy allows us to assess performance from both
    a similarity and quality perspective, leveraging established metrics and leveraging
    the capabilities of GPT-4 for in-depth analysis.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种综合评估策略使我们能够从相似性和质量的角度评估性能，利用既定指标和 GPT-4 的能力进行深入分析。
- en: 'Experimental Settings: We use 80GB A100 GPU, 210MHz clock cycle and 6 epochs
    for all experiments (Details are in Appendix). We have used NLTK, Spacy, openai(version=0.28),
    huggingface_hub, torch and transformers python packages for all experiments³³3Code/Data
    are in http://tiny.cc/zjelxz.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置：我们使用 80GB A100 GPU，210MHz 时钟周期和 6 轮训练进行所有实验（详细信息见附录）。我们在所有实验中使用了 NLTK、Spacy、openai（版本=0.28）、huggingface_hub、torch
    和 transformers python 包³³3代码/数据在 http://tiny.cc/zjelxz。
- en: Model R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd Llama2-7b-FT 39.4 23.9 35.9 32.7 14.7
    80.0 65.8 45.2 96.6 55.2 37.7 Llama2-13b-FT 41.5 25.9 37.8 35.5 16.8 80.7 68.3
    48.9 96.7 58.8 42.3 Mistral-7b-FT 36.1 19.8 31.6 30.8 11.8 78.8 67.7 46.2 83.5
    61.4 56.0 Gemma-FT 17.3 2.4 10.9 8.7 0.7 62.4 59.7 37.1 79.0 48.1 20.0 Aya-FT
    22.9 10.6 20.1 15.9 4.2 68.2 35.2 27.0 57.8 41.1 40.0 Falcon 17.2 4.8 12.5 22.2
    1.2 71.6 61.5 42.1 87.5 55.1 40.2 BART 23.9 8.5 17.6 27.5 3.3 74.6 62.4 43.1 86.8
    52.1 22.1 Pegasus 19.8 5.5 14.2 21.9 1.9 71.9 50.9 37.0 87.0 45.5 30.7 T5-FT 35.2
    18.2 31.1 29.5 10.1 78.7 63.3 42.7 95.1 53.7 24.0 Fl-T5-FT 35.8 19.1 31.6 30.6
    10.9 79.1 64.4 44.1 94.8 54.9 25.5 LED-ba 28.2 16.6 26.1 24.6 9.9 71.9 54.2 38.0
    83.5 48.6 22.8 LED-la 34.2 18.5 30.9 27.7 10.9 75.9 62.1 40.9 85.7 42.9 39.6 TLDR
    28.2 12.1 23.8 21.8 4.1 76.2 52.4 48.1 80.8 49.1 22.1
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd Llama2-7b-FT 39.4 23.9 35.9 32.7 14.7
    80.0 65.8 45.2 96.6 55.2 37.7 Llama2-13b-FT 41.5 25.9 37.8 35.5 16.8 80.7 68.3
    48.9 96.7 58.8 42.3 Mistral-7b-FT 36.1 19.8 31.6 30.8 11.8 78.8 67.7 46.2 83.5
    61.4 56.0 Gemma-FT 17.3 2.4 10.9 8.7 0.7 62.4 59.7 37.1 79.0 48.1 20.0 Aya-FT
    22.9 10.6 20.1 15.9 4.2 68.2 35.2 27.0 57.8 41.1 40.0 Falcon 17.2 4.8 12.5 22.2
    1.2 71.6 61.5 42.1 87.5 55.1 40.2 BART 23.9 8.5 17.6 27.5 3.3 74.6 62.4 43.1 86.8
    52.1 22.1 Pegasus 19.8 5.5 14.2 21.9 1.9 71.9 50.9 37.0 87.0 45.5 30.7 T5-FT 35.2
    18.2 31.1 29.5 10.1 78.7 63.3 42.7 95.1 53.7 24.0 Fl-T5-FT 35.8 19.1 31.6 30.6
    10.9 79.1 64.4 44.1 94.8 54.9 25.5 LED-ba 28.2 16.6 26.1 24.6 9.9 71.9 54.2 38.0
    83.5 48.6 22.8 LED-la 34.2 18.5 30.9 27.7 10.9 75.9 62.1 40.9 85.7 42.9 39.6 TLDR
    28.2 12.1 23.8 21.8 4.1 76.2 52.4 48.1 80.8 49.1 22.1
- en: 'Table 3: Traditional and GPT-4 based evaluation on OASUM domain-wise dataset
    variation'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：OASUM 领域数据集变异的传统和 GPT-4 基于评估
- en: 5.2 Results and discussion
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果与讨论
- en: 'In this section, we analyze the results presented in Table [3](#S5.T3 "Table
    3 ‣ 5.1 Evaluation metrics and experimental settings ‣ 5 Experimental evaluation
    and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality
    Aspect-Based Summarization") as well as Figure [1](#S5.F1 "Figure 1 ‣ 5.2.1 How
    effective is fine-tuning LLMs for aspect-based summarization based on traditional
    evaluation metrics? ‣ 5.2 Results and discussion ‣ 5 Experimental evaluation and
    Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality
    Aspect-Based Summarization") based on values of traditional metrics and GPT-4
    criteria respectively to understand how different models perform and gain insights
    into the effectiveness of fine-tuning LLMs for aspect-based summarization.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们根据传统指标和 GPT-4 标准分析了表格 [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation metrics and
    experimental settings ‣ 5 Experimental evaluation and Results ‣ Leveraging the
    Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization")
    和图 [1](#S5.F1 "Figure 1 ‣ 5.2.1 How effective is fine-tuning LLMs for aspect-based
    summarization based on traditional evaluation metrics? ‣ 5.2 Results and discussion
    ‣ 5 Experimental evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization") 中呈现的结果，以了解不同模型的表现，并深入探讨微调
    LLMs 在基于方面的摘要中的有效性。'
- en: 5.2.1 How effective is fine-tuning LLMs for aspect-based summarization based
    on traditional evaluation metrics?
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 微调 LLMs 在基于方面的摘要中基于传统评价指标的效果如何？
- en: 'In Figure [1](#S5.F1 "Figure 1 ‣ 5.2.1 How effective is fine-tuning LLMs for
    aspect-based summarization based on traditional evaluation metrics? ‣ 5.2 Results
    and discussion ‣ 5 Experimental evaluation and Results ‣ Leveraging the Power
    of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization"),
    we can see comparison between vanilla and fine-tuned LLMs based on values for
    traditional metrics like ROUGE and BERTScore. Here, we can observe a significant
    performance boost for fine-tuned LLMs (particularly Llama2-7b-FT, Llama2-13b-FT,
    Mistral-7b-FT) compared to their vanilla counterparts (Llama2-7b-VA, Llama2-13b-VA,
    Mistral-7b-VA) across all metrics. This indicates that fine-tuning successfully
    tailors these models to the task of aspect-based summarization, enabling them
    to generate summaries that better match the gold-standard summaries in terms of
    n-gram overlap and semantic similarity.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1](#S5.F1 "图 1 ‣ 5.2.1 微调LLM在基于传统评估指标的方面摘要任务中的有效性 ‣ 5.2 结果与讨论 ‣ 5 实验评估与结果
    ‣ 利用LLM的力量：一种用于高质量基于方面的摘要的微调方法") 中，我们可以看到原始LLM和微调LLM在ROUGE和BERTScore等传统指标上的比较。在这里，我们可以观察到微调后的LLM（特别是Llama2-7b-FT、Llama2-13b-FT、Mistral-7b-FT）在所有指标上的性能显著提升，相比于其原始版本（Llama2-7b-VA、Llama2-13b-VA、Mistral-7b-VA）。这表明微调成功地将这些模型调整为适应基于方面的摘要任务，使其生成的摘要在n-gram重叠和语义相似性方面更好地匹配黄金标准摘要。
- en: 'Among the fine-tuned LLMs, Llama2-13b-FT consistently achieves the highest
    scores across all traditional metrics compared to competing baseline models (as
    seen from Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation metrics and experimental
    settings ‣ 5 Experimental evaluation and Results ‣ Leveraging the Power of LLMs:
    A Fine-Tuning Approach for High-Quality Aspect-Based Summarization")), suggesting
    that its larger parameter size provides an advantage in capturing the nuances
    of aspect-based information. Interestingly, among the latest released LLMs, Aya-VA
    demonstrates an expected performance gain upon fine-tuning, suggesting its potential
    suitability for aspect-based summarization tasks. However, Gemma-VA degrades in
    BeF1 score, highlighting the importance of model architecture and suitability
    for aspect-based summarization task beyond parameter size. In summary, all models
    might NOT gain performance upon finetuning.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调的LLM中，Llama2-13b-FT在所有传统指标上均 consistently 达到最高分，相比之下，竞争基准模型表现较差（见表格 [3](#S5.T3
    "表格 3 ‣ 5.1 评估指标与实验设置 ‣ 5 实验评估与结果 ‣ 利用LLM的力量：一种用于高质量基于方面的摘要的微调方法")），这表明其更大的参数规模在捕捉基于方面的信息细微差别方面具有优势。有趣的是，在最新发布的LLM中，Aya-VA在微调后显示出预期的性能提升，表明其可能适用于基于方面的摘要任务。然而，Gemma-VA在BeF1评分上有所下降，这突显了模型架构和适用于基于方面的摘要任务的重要性，而不仅仅是参数规模。总之，并非所有模型在微调后都会提升性能。
- en: '![Refer to caption](img/4a4cc32fde056c314818d10772dc4cf5.png)![Refer to caption](img/5b7ceb95d5fd647fc6c876ed960e3e52.png)![Refer
    to caption](img/7fae8eef94691dcdd8b70a9da1979b09.png)![Refer to caption](img/c1a506e12016a9f8d8212a6a65953f1e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a4cc32fde056c314818d10772dc4cf5.png)![参见说明](img/5b7ceb95d5fd647fc6c876ed960e3e52.png)![参见说明](img/7fae8eef94691dcdd8b70a9da1979b09.png)![参见说明](img/c1a506e12016a9f8d8212a6a65953f1e.png)'
- en: 'Figure 1: Comparison between vanilla and fine-tuned versions of different LLMs
    for Rouge1, Bert-Score F1, Relevance and Coverage'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同LLM的原始版本和微调版本在Rouge1、Bert-Score F1、相关性和覆盖度上的比较
- en: 5.2.2 How does fine-tuning LLMs impact the quality of summaries based on GPT-4
    critiquing?
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 微调LLM如何影响基于GPT-4批评的摘要质量？
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation metrics and experimental settings
    ‣ 5 Experimental evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization") and Fig. [1](#S5.F1 "Figure
    1 ‣ 5.2.1 How effective is fine-tuning LLMs for aspect-based summarization based
    on traditional evaluation metrics? ‣ 5.2 Results and discussion ‣ 5 Experimental
    evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization") also unveil a deeper perspective
    on summary quality through the lens of GPT-4 critiquing. Here, we evaluate summaries
    based on five criteria: relevance, key point coverage, aspect-specificity, overall
    quality, and manually verified goodness. Consistent with the traditional metrics,
    fine-tuned LLMs (Llama2-7b-FT, Llama2-13b-FT, Mistral-7b-FT) significantly outperform
    vanilla models (Llama2-7b-VA, Llama2-13b-VA) across all criteria as further supported
    by corresponding plots comparing vanilla and fine-tuned LLMs based on values of
    relevance and coverage in Figure [1](#S5.F1 "Figure 1 ‣ 5.2.1 How effective is
    fine-tuning LLMs for aspect-based summarization based on traditional evaluation
    metrics? ‣ 5.2 Results and discussion ‣ 5 Experimental evaluation and Results
    ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based
    Summarization"). This reinforces the effectiveness of fine-tuning in generating
    summaries that are not only similar to the gold standard but also capture the
    essence of the specific aspect and deliver clear, concise information.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation metrics and experimental settings ‣
    5 Experimental evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization")和图 [1](#S5.F1 "Figure 1
    ‣ 5.2.1 How effective is fine-tuning LLMs for aspect-based summarization based
    on traditional evaluation metrics? ‣ 5.2 Results and discussion ‣ 5 Experimental
    evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization")也从GPT-4批评的角度揭示了总结质量的更深层次视角。在这里，我们根据五个标准评估总结：相关性、关键点覆盖、方面特异性、整体质量和人工验证的优良程度。与传统指标一致，微调后的LLMs（Llama2-7b-FT、Llama2-13b-FT、Mistral-7b-FT）在所有标准中显著超越了原始模型（Llama2-7b-VA、Llama2-13b-VA），这进一步通过图 [1](#S5.F1
    "Figure 1 ‣ 5.2.1 How effective is fine-tuning LLMs for aspect-based summarization
    based on traditional evaluation metrics? ‣ 5.2 Results and discussion ‣ 5 Experimental
    evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization")中原始模型和微调模型在相关性和覆盖度值上的比较图得到了支持。这进一步巩固了微调在生成不仅类似于金标准的总结，而且能够捕捉特定方面的本质并提供清晰、简洁信息的有效性。'
- en: 'Llama2-13b-FT achieves best performance again compared to baseline methods,
    achieving the highest scores in most criteria, particularly in key point coverage
    and overall quality (see Table [3](#S5.T3 "Table 3 ‣ 5.1 Evaluation metrics and
    experimental settings ‣ 5 Experimental evaluation and Results ‣ Leveraging the
    Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization")).
    This suggests that its larger size allows for a more comprehensive understanding
    of the document and the target aspect, leading to summaries that effectively capture
    the crucial aspect-based details. However, size is not necessarily an indicator
    of getting the best performance since Aya is also 13b moel but has the least performance
    for this task among the models considered. This indicates that specific models
    are optimized for specific tasks. Also, similar to our observations in the previous
    sections, Gemma-FT has degraded performance upon fine-tuning, indicating that
    fine-tuning LLMs is not always a good thing for all LLMs and tasks.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'Llama2-13b-FT再次在性能上超越了基线方法，在大多数标准中取得了最高分，特别是在关键点覆盖和整体质量方面（见表格 [3](#S5.T3 "Table
    3 ‣ 5.1 Evaluation metrics and experimental settings ‣ 5 Experimental evaluation
    and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality
    Aspect-Based Summarization")）。这表明其较大的模型规模允许对文档和目标方面有更全面的理解，从而生成能够有效捕捉关键方面细节的总结。然而，规模并不一定是获得最佳性能的指示，因为Aya虽然也是13b模型，但在所考虑的模型中在此任务上表现最差。这表明特定模型是针对特定任务优化的。此外，与我们在前面章节中的观察类似，Gemma-FT在微调后性能下降，表明微调LLMs并不总是对所有LLMs和任务都有利。'
- en: 5.2.3 Which LLMs achieve the best performance on fine-tuning?
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 哪些LLMs在微调中表现最佳？
- en: By combining the insights from both traditional metrics and GPT-4 critiquing
    results, Llama2-13b-FT emerges as the clear winner for generating aspect-based
    summaries, consistently demonstrating superior performance in terms of similarity,
    key point coverage, relevance, and overall quality. Its larger parameter size
    appears to be instrumental in achieving this level of performance for the aspect-based
    summarization task, along with its superior architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合传统指标和GPT-4的批评结果，Llama2-13b-FT脱颖而出，成为生成基于方面的摘要的明显赢家，一直展示出在相似度、关键点覆盖、相关性和整体质量方面的卓越表现。其较大的参数大小似乎对在基于方面的摘要任务中取得这一水平的表现起到了重要作用，同时其优越的架构也起到了关键作用。
- en: These findings significantly strengthen the case for fine-tuning LLMs for aspect-based
    summarization, for most of the base models. Fine-tuning not only improves the
    similarity of generated summaries to the gold standard but also enhances their
    ability to capture the essence of the target aspect and deliver clear, concise
    information. While parameter size plays a role, model architecture also plays
    a crucial part, as evidenced by Gemma-VA’s limitations with fine-tuning not improving
    its performance and the marginal improvement of Aya-FT over its vanilla counterpart.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现显著加强了对大多数基础模型进行精细调整以进行基于方面的摘要的理由。精细调整不仅提高了生成摘要与黄金标准的相似度，还增强了其捕捉目标方面的本质并提供清晰、简洁信息的能力。虽然参数大小起一定作用，但模型架构也扮演着关键角色，例如Gemma-VA的限制在于精细调整未能改善其性能，而Aya-FT相比于其原始版本略有改进。
- en: Data Approach R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd Hi VA Lm7b-VA 18.5 5.4 13.9
    20.8 1.5 70.0 42.1 36.5 55.1 40.1 26.4 Lm13b-VA 19.2 5.5 14.1 21.6 1.8 68.5 43.2
    37.2 56.3 43.6 28.8 Mis7b-VA 22.1 6.6 15.8 25.2 2.1 73.1 59.7 38.1 85.3 50.6 24.0
    FT Lm7b-FT 33.8 18.3 30.7 28.3 10.0 78.4 53.7 43.8 69.1 49.2 35.3 Lm13b-FT 36.9
    21.9 33.0 30.3 11.7 81.1 63.2 44.8 88.1 52.3 42.5 Mis7b-FT 32.4 15.9 27.6 26.6
    7.1 78.1 59.4 42.3 90.3 47.3 42.0 Lo VA Lm7b-VA 14.3 4.4 10.9 17.0 1.0 65.3 29.1
    27.5 48.3 30.5 20.6 Lm13b-VA 20.1 5.3 14.6 20.3 1.6 70.2 31.5 25.2 48.9 29.6 20.1
    Mis7b-VA 21.5 5.7 15.2 20.1 1.6 73.2 46.0 30.5 55.6 42.5 18.0 FT Lm7b-FT 21.9
    7.2 16.4 22.1 3.5 72.1 34.1 32.7 50.2 31.7 25.2 Lm13b-FT 29.2 13.3 25.1 22.5 6.3
    78.8 48.2 36.6 66.8 49.9 44.8 Mis7b-FT 25.3 8.8 20.3 19.4 2.7 76.1 47.5 32.3 62.8
    43.8 42.0 Ra VA Lm7b-VA 15.5 4.9 11.7 19.7 1.4 68.2 34.6 30.2 49.0 32.7 21.9 Lm13b-VA
    19.6 5.3 14.2 21.3 1.6 70.3 35.2 31.8 52.4 33.3 23.3 Mis7b-VA 21.8 5.9 15.6 24.8
    1.8 70.2 52.2 30.7 80.9 39.2 22.0 FT Lm7b-FT 27.8 13.9 27.2 26.1 7.8 73.0 48.9
    35.4 62.3 34.3 29.0 Lm13b-FT 30.4 14.6 28.1 28.3 9.2 75.3 55.8 39.0 88.9 42.5
    33.9 Mis7b-FT 28.6 13.1 24.8 24.1 5.3 72.3 53.7 33.6 86.8 40.0 38.0
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据 方法 R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd Hi VA Lm7b-VA 18.5 5.4 13.9 20.8 1.5
    70.0 42.1 36.5 55.1 40.1 26.4 Lm13b-VA 19.2 5.5 14.1 21.6 1.8 68.5 43.2 37.2 56.3
    43.6 28.8 Mis7b-VA 22.1 6.6 15.8 25.2 2.1 73.1 59.7 38.1 85.3 50.6 24.0 FT Lm7b-FT
    33.8 18.3 30.7 28.3 10.0 78.4 53.7 43.8 69.1 49.2 35.3 Lm13b-FT 36.9 21.9 33.0
    30.3 11.7 81.1 63.2 44.8 88.1 52.3 42.5 Mis7b-FT 32.4 15.9 27.6 26.6 7.1 78.1
    59.4 42.3 90.3 47.3 42.0 Lo VA Lm7b-VA 14.3 4.4 10.9 17.0 1.0 65.3 29.1 27.5 48.3
    30.5 20.6 Lm13b-VA 20.1 5.3 14.6 20.3 1.6 70.2 31.5 25.2 48.9 29.6 20.1 Mis7b-VA
    21.5 5.7 15.2 20.1 1.6 73.2 46.0 30.5 55.6 42.5 18.0 FT Lm7b-FT 21.9 7.2 16.4
    22.1 3.5 72.1 34.1 32.7 50.2 31.7 25.2 Lm13b-FT 29.2 13.3 25.1 22.5 6.3 78.8 48.2
    36.6 66.8 49.9 44.8 Mis7b-FT 25.3 8.8 20.3 19.4 2.7 76.1 47.5 32.3 62.8 43.8 42.0
    Ra VA Lm7b-VA 15.5 4.9 11.7 19.7 1.4 68.2 34.6 30.2 49.0 32.7 21.9 Lm13b-VA 19.6
    5.3 14.2 21.3 1.6 70.3 35.2 31.8 52.4 33.3 23.3 Mis7b-VA 21.8 5.9 15.6 24.8 1.8
    70.2 52.2 30.7 80.9 39.2 22.0 FT Lm7b-FT 27.8 13.9 27.2 26.1 7.8 73.0 48.9 35.4
    62.3 34.3 29.0 Lm13b-FT 30.4 14.6 28.1 28.3 9.2 75.3 55.8 39.0 88.9 42.5 33.9
    Mis7b-FT 28.6 13.1 24.8 24.1 5.3 72.3 53.7 33.6 86.8 40.0 38.0
- en: 'Table 4: Traditional and LLM based evaluations on three different variations
    of OASUM dataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 基于传统和LLM的三种不同OASUM数据集变异的评估'
- en: 5.2.4 How robust is the fine-tuned LLM for variations in dataset and domains
    for aspect-based summarization?
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 精细调整的LLM在数据集和领域变异的情况下对基于方面的摘要有多鲁棒？
- en: To answer this question, we pick our best perfoming fine-tuned model from the
    results in the previous section, the Llama2-13b-FT, and evaluate it on variations
    in the dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们从上一部分的结果中选择表现最佳的精细调整模型——Llama2-13b-FT，并在数据集变异上对其进行评估。
- en: 'Different Types of OASUM Data:'
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同类型的OASUM数据：
- en: 'To check the effectiveness of oir fine-tuned models, we experiment on different
    types of OASUM data: OASUM-Hi, OASUM-Lo and OASUM-Ra as shown in Table [4](#S5.T4
    "Table 4 ‣ 5.2.3 Which LLMs achieve the best performance on fine-tuning? ‣ 5.2
    Results and discussion ‣ 5 Experimental evaluation and Results ‣ Leveraging the
    Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization").
    By employing multiple dataset variations, we aim to achieve a comprehensive evaluation
    of fine-tuned LLMs for aspect-based summarization, taking into account various
    data characteristics and potential shortcomings in existing summaries. As expected,
    evaluation outcomes are best for OASUM-Hi and least for OASUM-Lo since number
    of aspects for OASUM-Hi is much lesser than OASUM-Lo. OASUM-Ra exhibit results
    better than OASUM-Lo due to presence of lesser aspects than OASUM-Lo. Llama2-13b-FT
    performs best for almost all scenrios across different evaluation metrics.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们微调模型的有效性，我们在不同类型的OASUM数据上进行实验：OASUM-Hi、OASUM-Lo和OASUM-Ra，如表[4](#S5.T4
    "表 4 ‣ 5.2.3 哪些LLM在微调中表现最佳？ ‣ 5.2 结果与讨论 ‣ 5 实验评估与结果 ‣ 利用LLM的力量：一种高质量基于方面总结的微调方法")所示。通过采用多种数据集变体，我们旨在对微调后的LLM进行全面评估，考虑各种数据特征和现有总结中的潜在缺陷。正如预期的那样，OASUM-Hi的评估结果最佳，而OASUM-Lo的结果最差，因为OASUM-Hi的方面数量远少于OASUM-Lo。由于OASUM-Ra的方面数量少于OASUM-Lo，因此其结果优于OASUM-Lo。Llama2-13b-FT在几乎所有场景下的表现都最佳。
- en: Domain R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd Healthcare 32.8 17.3 29.3 27.4 9.4
    77.8 69.9 47.0 97.4 57.0 42.5 Education 44.9 28.2 41.1 38.1 18.2 81.3 68.2 51.0
    97.6 58.6 45.3 Life and Career 39.4 23.9 35.5 32.7 14.1 80.4 69.9 48.5 96.7 58.8
    41.1 Music 41.9 27.6 38.6 37.7 20.4 81.0 66.2 0.47 94.9 56.6 40.3 Average 41.5
    25.9 37.8 35.5 16.8 80.7 68.3 48.9 96.7 58.8 42.3
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 领域 R1 R2 RL Mt Bl BeF1 Re Cv Im Ra Gd 医疗 32.8 17.3 29.3 27.4 9.4 77.8 69.9 47.0
    97.4 57.0 42.5 教育 44.9 28.2 41.1 38.1 18.2 81.3 68.2 51.0 97.6 58.6 45.3 生活与职业
    39.4 23.9 35.5 32.7 14.1 80.4 69.9 48.5 96.7 58.8 41.1 音乐 41.9 27.6 38.6 37.7
    20.4 81.0 66.2 0.47 94.9 56.6 40.3 平均 41.5 25.9 37.8 35.5 16.8 80.7 68.3 48.9
    96.7 58.8 42.3
- en: 'Table 5: Evaluations of fine-tuned Llama2-13b on different domains of OASUM'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在OASUM不同领域上对微调后的Llama2-13b的评估
- en: 'Evaluations for different Domains:'
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同领域的评估：
- en: 'In Table [5](#S5.T5 "Table 5 ‣ Different Types of OASUM Data: ‣ 5.2.4 How robust
    is the fine-tuned LLM for variations in dataset and domains for aspect-based summarization?
    ‣ 5.2 Results and discussion ‣ 5 Experimental evaluation and Results ‣ Leveraging
    the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization"),
    we show five different traditional metric and GPT4 critique scores for the best
    performing Llama2-13b Finetuned model of OASUM data for 4 different domains -
    Healthcare, Education, Life and Career and Music. It shows consistent performance
    of Llama2-13b Finetuned model for different domains.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[5](#S5.T5 "表 5 ‣ 不同类型的OASUM数据： ‣ 5.2.4 微调后的LLM在数据集和领域变化下的鲁棒性如何？ ‣ 5.2 结果与讨论
    ‣ 5 实验评估与结果 ‣ 利用LLM的力量：一种高质量基于方面总结的微调方法")中，我们展示了OASUM数据的Llama2-13b微调模型在4个不同领域（医疗、教育、生活与职业和音乐）上的五种不同传统指标和GPT4评估分数。这显示了Llama2-13b微调模型在不同领域中的一致表现。
- en: Different Evaluation Parameter Settings
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同评估参数设置
- en: 'We evaluate outcomes of various models with different parameter setting during
    GPT4 critique - max-new-token and temperature. Best results are obtained when
    max-new-token size is 80 (as shown in Fig. [2](#S5.F2 "Figure 2 ‣ Different Evaluation
    Parameter Settings ‣ 5.2.4 How robust is the fine-tuned LLM for variations in
    dataset and domains for aspect-based summarization? ‣ 5.2 Results and discussion
    ‣ 5 Experimental evaluation and Results ‣ Leveraging the Power of LLMs: A Fine-Tuning
    Approach for High-Quality Aspect-Based Summarization")) and GPT4 critique’s temperature
    is 0.0.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了在GPT4评估中，使用不同参数设置（如max-new-token和温度）时各种模型的结果。当max-new-token大小为80时（如图[2](#S5.F2
    "图 2 ‣ 不同评估参数设置 ‣ 5.2.4 微调后的LLM在数据集和领域变化下的鲁棒性如何？ ‣ 5.2 结果与讨论 ‣ 5 实验评估与结果 ‣ 利用LLM的力量：一种高质量基于方面总结的微调方法")所示），且GPT4评估的温度为0.0时，结果最佳。
- en: 'Varying Training Size Dataset: To understand the effect of training data size
    on the performance, we vary the OASUM Domain-Wise Split training data for the
    Llama2-13b model - taking 10%, 40% and 70% of the initial training data, and finetune
    the Llama2-13b model with same parameter and hyper-parameter settings and the
    five criterias of GPT4-Critique outcome (in %) are shown in Fig [2](#S5.F2 "Figure
    2 ‣ Different Evaluation Parameter Settings ‣ 5.2.4 How robust is the fine-tuned
    LLM for variations in dataset and domains for aspect-based summarization? ‣ 5.2
    Results and discussion ‣ 5 Experimental evaluation and Results ‣ Leveraging the
    Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization").
    We see that with increasing the dataset size, the performance of Llama2-13b improves
    in terms of different GPT4 critique metrics: Relevance (Re), Coverage (Cv), Impurity
    (Im), Rating (Ra) and Goodness (Gd). Even at 40% of the dataset, the model is
    able to achieve a decent performance. It shows the effectiveness of the Llama2-13b
    model. It also infers that even with very little amount of data with 10% Llama2-13b
    can able to generate appropriate aspect based summary.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集大小变化：为了了解训练数据大小对性能的影响，我们调整了Llama2-13b模型的OASUM领域划分训练数据——采用初始训练数据的10%、40%和70%，并以相同的参数和超参数设置对Llama2-13b模型进行微调，GPT4-Critique结果（以%表示）的五个标准显示在图[2](#S5.F2
    "图2 ‣ 不同评估参数设置 ‣ 5.2.4 微调LLM在数据集和领域变化下的鲁棒性 ‣ 5.2 结果与讨论 ‣ 5 实验评估与结果 ‣ 利用LLMs的力量：高质量方面总结的微调方法")中。我们看到随着数据集大小的增加，Llama2-13b在不同的GPT4评估指标上表现有所提升：相关性（Re）、覆盖度（Cv）、杂质（Im）、评分（Ra）和优良度（Gd）。即使在数据集的40%时，模型也能够实现相当好的性能。这显示了Llama2-13b模型的有效性。它还表明，即使只有10%的数据，Llama2-13b也能够生成适当的方面总结。
- en: '![Refer to caption](img/8761c5da5a0102104bee81f5ccc7195e.png)![Refer to caption](img/64fd6a4f76be124a2ae5c2aed9be6585.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8761c5da5a0102104bee81f5ccc7195e.png)![参见说明](img/64fd6a4f76be124a2ae5c2aed9be6585.png)'
- en: 'Figure 2: GPT4 Criteria Performance comparison of Llama2-13b-FT model w.r.t.
    training data variation (left) and max-new-token size (right)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GPT4标准下Llama2-13b-FT模型在训练数据变异（左）和最大新令牌大小（右）方面的性能比较
- en: 'Assessment of Llama2-13b Model To further investigate the potency of Llama2-13b-FT
    model, we extract 50 different OASUM articles and provide aspect based summaries
    of OASUM (Ground Truth vs Llama2) to two annotators (with domain knowledge and
    proficiency in English) to label: (i) whether Llama2-13b-FT is better, (ii) Both
    are Good, (iii) Ground Truth is better and (iv) Both are bad. We found that 20%
    cases Llama2-13b-FT is better, 50% cases both are good, 24% cases ground truth
    is better and 6% cases both are bad. So, overall Llama2 provides 70% good summaries.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Llama2-13b模型评估 为进一步探究Llama2-13b-FT模型的效能，我们提取了50篇不同的OASUM文章，并提供了OASUM的方面总结（真实数据与Llama2对比）给两位标注员（具有领域知识和英语能力）进行标注：（i）Llama2-13b-FT是否更好，（ii）两者都很好，（iii）真实数据更好，（iv）两者都差。我们发现20%的案例中Llama2-13b-FT更好，50%的案例中两者都好，24%的案例中真实数据更好，6%的案例中两者都差。因此，总体上Llama2提供了70%的好总结。
- en: These findings reinforce the claim of superiority of finetuning approach and
    utilization of LLM as an alternative evaluation criteria. They also show that
    the approach is robust to variations in type, domain, and quantity of datasets
    for the given task.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现强化了微调方法的优越性及利用LLM作为替代评估标准的主张。它们还表明，该方法对任务所涉及的数据集类型、领域和数量的变化具有鲁棒性。
- en: 6 Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we addressed the ever-growing challenge of efficiently extracting
    key insights from voluminous documents in the digital age. We explored the potential
    of fine-tuning large language models (LLMs) to enhance the performance of aspect-based
    summarization task. Our work centered around fine-tuning open-source foundation
    LLMs, including Llama2, Gemma, Mistral, and Aya, on aspect-based summarization
    datasets. We hypothesized that this approach would enable these models to excel
    at identifying and extracting information relevant to user-specified aspects within
    a document, ultimately leading to superior quality aspect-based summaries.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们解决了在数字时代有效提取大量文档中的关键洞察这一日益增长的挑战。我们探索了微调大型语言模型（LLMs）的潜力，以提升方面总结任务的表现。我们的工作集中在对包括Llama2、Gemma、Mistral和Aya在内的开源基础LLMs进行方面总结数据集的微调。我们假设这种方法能使这些模型在识别和提取与用户指定方面相关的信息方面表现出色，从而最终生成优质的方面总结。
- en: Through a comprehensive evaluation framework, we compared the performance of
    fine-tuned LLMs against state-of-the-art aspect-based summarization methods and
    vanilla counterparts of the fine-tuned LLMs, and demonstrated significant improvement
    in quality of generated summaries as a result of fine-tuning. Our findings not
    only contribute towards the advancement of aspect-based summarization techniques
    but also hold significant implications for the broader field of NLP. By demonstrating
    the effectiveness of fine-tuning LLMs for targeted information extraction tasks
    like aspect-based summarization, we open doors for further exploration and potential
    applications in various NLP domains requiring focused information retrieval and
    summarization, ultimately empowering users to navigate the ever-expanding sea
    of information with greater efficiency and precision.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个全面的评估框架，我们将经过微调的LLMs的性能与最先进的基于方面的摘要方法以及未经微调的LLMs进行了比较，并展示了微调显著提高生成摘要的质量。我们的发现不仅有助于推动基于方面的摘要技术的发展，还对自然语言处理（NLP）领域具有重要的意义。通过展示微调LLMs在针对性信息提取任务（如基于方面的摘要）的有效性，我们为进一步探索和潜在应用在需要专注信息检索和摘要的各种NLP领域打开了大门，*最终使用户能够以更高的效率和精确度在不断扩展的信息海洋中导航*。
- en: Limitations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Our datasets are not multilingual and multimodal. We plan to capture aspects
    involving multimodal content, such as images or videos, limiting their comprehensiveness.
    LLMs may face challenges in adapting to domain-specific jargon, resulting in less
    informative summaries for aspects containing specialized terminology. So, we need
    to explore how to correct these - which we aim to do as a part of future work.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集不是多语言和多模态的。我们计划捕捉涉及多模态内容（如图像或视频）的方面，但这限制了其全面性。LLMs在适应领域特定术语时可能面临挑战，导致对于包含专业术语的方面的摘要信息较少。因此，我们需要探索如何纠正这些问题——这是我们计划作为未来工作的部分内容。
- en: Ethics Statement
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: Our work does not reveal any personal sensitive information and we use publicly
    available benchmarked datasets and models in different contexts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作未揭示任何个人敏感信息，我们在不同的背景下使用了公开可用的基准数据集和模型。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Akhtar et al. (2017) Nadeem Akhtar, Nashez Zubair, Abhishek Kumar, and Tameem
    Ahmad. Aspect based sentiment oriented summarization of hotel reviews. *Procedia
    computer science*, 115:563–571, 2017.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akhtar等（2017）Nadeem Akhtar, Nashez Zubair, Abhishek Kumar, 和Tameem Ahmad。基于方面的情感导向酒店评论摘要。*Procedia计算机科学*，115:563–571，2017。
- en: 'Banerjee & Lavie (2005) Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
    metric for mt evaluation with improved correlation with human judgments. In *Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization*, pp.  65–72, 2005.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee & Lavie（2005）Satanjeev Banerjee 和Alon Lavie。Meteor: 一种用于机器翻译评估的自动指标，与人工判断的相关性更高。在*ACL机器翻译和/或摘要评估措施研讨会论文集*，第65–72页，2005。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy等（2020）Iz Beltagy, Matthew E Peters, 和Arman Cohan。Longformer: 长文档变换器。*arXiv预印本arXiv:2004.05150*，2020。'
- en: 'Cachola et al. (2020) Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S Weld.
    Tldr: Extreme summarization of scientific documents. *arXiv preprint arXiv:2004.15011*,
    2020.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cachola等（2020）Isabel Cachola, Kyle Lo, Arman Cohan, 和Daniel S Weld。Tldr: 科学文档的极端摘要。*arXiv预印本arXiv:2004.15011*，2020。'
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv preprint arXiv:2308.07201*, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan等（2023）Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang
    Zhang, Jie Fu, 和Zhiyuan Liu。Chateval: 通过多代理辩论改进LLM基础评估器。*arXiv预印本arXiv:2308.07201*，2023。'
- en: 'Chopra et al. (2016) Sumit Chopra, Michael Auli, and Alexander M Rush. Abstractive
    sentence summarization with attentive recurrent neural networks. In *Proceedings
    of the 2016 conference of the North American chapter of the association for computational
    linguistics: human language technologies*, pp.  93–98, 2016.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chopra等（2016）Sumit Chopra, Michael Auli, 和Alexander M Rush。使用注意力递归神经网络的抽象句子摘要。在*2016年北美计算语言学协会人类语言技术会议论文集*，第93–98页，2016。
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma
    等。扩展指令微调语言模型。*arXiv 预印本 arXiv:2210.11416*，2022年。
- en: Coavoux et al. (2019) Maximin Coavoux, Hady Elsahar, and Matthias Gallé. Unsupervised
    aspect-based multi-document abstractive summarization. In *Proceedings of the
    2nd Workshop on New Frontiers in Summarization*, pp.  42–47, 2019.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coavoux et al. (2019) Maximin Coavoux, Hady Elsahar 和 Matthias Gallé。无监督的方面多文档抽象摘要。在
    *第二届总结新前沿研讨会论文集*，第 42–47 页，2019年。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman 和 Luke Zettlemoyer。Qlora：量化LLMs的高效微调。*arXiv
    预印本 arXiv:2305.14314*，2023年。
- en: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3):220–235, 2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen 等。大规模预训练语言模型的参数高效微调。*自然机器智能*，5(3):220–235，2023年。
- en: 'El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,
    and Hoda K Mohamed. Automatic text summarization: A comprehensive survey. *Expert
    systems with applications*, 165:113679, 2021.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea 和
    Hoda K Mohamed。自动文本摘要：综合调查。*应用专家系统*，165:113679，2021年。
- en: 'Erkan & Radev (2004) Günes Erkan and Dragomir R Radev. Lexrank: Graph-based
    lexical centrality as salience in text summarization. *Journal of artificial intelligence
    research*, 22:457–479, 2004.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erkan & Radev (2004) Günes Erkan 和 Dragomir R Radev。Lexrank：基于图的词汇中心性作为文本摘要中的显著性。*人工智能研究期刊*，22:457–479，2004年。
- en: Frermann & Klementiev (2019) Lea Frermann and Alexandre Klementiev. Inducing
    document structure for aspect-based summarization. In *Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics*, pp.  6263–6273,
    2019.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frermann & Klementiev (2019) Lea Frermann 和 Alexandre Klementiev。为基于方面的摘要引入文档结构。在
    *第57届计算语言学协会年会论文集*，第 6263–6273 页，2019年。
- en: Fu et al. (2023) Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong
    Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37,
    pp.  12799–12807, 2023.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong
    Bing 和 Nigel Collier。关于参数高效微调的有效性。在 *AAAI 人工智能会议论文集*，第 37 卷，第 12799–12807 页，2023年。
- en: 'Gambhir & Gupta (2017) Mahak Gambhir and Vishal Gupta. Recent automatic text
    summarization techniques: a survey. *Artificial Intelligence Review*, 47(1):1–66,
    2017.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gambhir & Gupta (2017) Mahak Gambhir 和 Vishal Gupta。最近的自动文本摘要技术：综述。*人工智能评论*，47(1):1–66，2017年。
- en: 'Gao et al. (2024) Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan.
    Llm-based nlg evaluation: Current status and challenges. *arXiv preprint arXiv:2402.01383*,
    2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2024) Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu 和 Xiaojun Wan。基于LLM的NLG评估：当前状况与挑战。*arXiv
    预印本 arXiv:2402.01383*，2024年。
- en: 'Guo et al. (2022) Mandy Guo, Joshua Ainslie, David C Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer
    for long sequences. In *Findings of the Association for Computational Linguistics:
    NAACL 2022*, pp.  724–736, 2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2022) Mandy Guo, Joshua Ainslie, David C Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung 和 Yinfei Yang。Longt5：高效的长序列文本到文本变换器。在 *计算语言学协会年会论文集：NAACL
    2022*，第 724–736 页，2022年。
- en: 'Hasanbeig et al. (2023) Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe
    Vieira Frujeri, and Ida Momennejad. Allure: Auditing and improving llm-based evaluation
    of text using iterative in-context-learning. *arXiv e-prints*, pp.  arXiv–2309,
    2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasanbeig et al. (2023) Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe
    Vieira Frujeri 和 Ida Momennejad。Allure：审计和改进基于LLM的文本评估的迭代上下文学习。*arXiv 电子预印本*，第
    arXiv–2309 页，2023年。
- en: 'Hayashi et al. (2021) Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson,
    Raj Neervannan, and Graham Neubig. Wikiasp: A dataset for multi-domain aspect-based
    summarization. *Transactions of the Association for Computational Linguistics*,
    9:211–225, 2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hayashi et al. (2021) Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson,
    Raj Neervannan 和 Graham Neubig。Wikiasp：一个多领域方面摘要的数据集。*计算语言学协会会刊*，9:211–225，2021年。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍夫曼等（2022）乔丹·霍夫曼、塞巴斯蒂安·博尔戈德、亚瑟·门施、埃琳娜·布赫茨卡娅、特雷弗·蔡、伊丽莎·拉瑟福德、迭戈·德·拉斯·卡萨斯、丽莎·安妮·亨德里克斯、约翰内斯·韦布尔、艾丹·克拉克等。训练计算优化的大型语言模型。*arXiv
    预印本 arXiv:2203.15556*，2022年。
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. *arXiv
    preprint arXiv:2210.11610*, 2022.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等（2022）贾欣·黄、石祥·肖恩·顾、乐侯、岳欣·吴、薛志旺、洪坤·余和贾伟·韩。大型语言模型可以自我改进。*arXiv 预印本 arXiv:2210.11610*，2022年。
- en: Huang et al. (2023) Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun
    Zhou, and Leilani H Gilpin. Can large language models explain themselves? a study
    of llm-generated self-explanations. *arXiv preprint arXiv:2310.11207*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等（2023）施远·黄、悉达多·马米丹娜、施里德哈尔·姜、易伦·周和莱拉妮·H·吉尔平。大语言模型能否自我解释？对LLM生成自我解释的研究。*arXiv
    预印本 arXiv:2310.11207*，2023年。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等（2023）阿尔伯特·Q·蒋、亚历山大·萨布雷罗勒斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦杰尔、吉约姆·兰普尔、露西尔·索尔尼耶等。Mistral
    7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Kunneman et al. (2018) Florian Kunneman, Sander Wubben, Antal van den Bosch,
    and Emiel Krahmer. Aspect-based summarization of pros and cons in unstructured
    product reviews. In *COLING*, pp.  2219–2229, 2018.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库内曼等（2018）弗洛里安·库内曼、桑德尔·乌本、安塔尔·范·登·博斯赫和埃米尔·克拉默。基于方面的非结构化产品评论的优缺点摘要。见于 *COLING*，第2219–2229页，2018年。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising
    sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*, 2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2019）迈克·刘易斯、尹汉·刘、纳曼·戈亚尔、马尔詹·加兹维尼贾德、阿卜杜勒拉赫曼·穆罕默德、奥梅尔·莱维、维斯·斯托扬诺夫和卢克·泽特尔莫耶。Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练。*arXiv
    预印本 arXiv:1910.13461*，2019年。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In *Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics*, pp.  7871–7880, 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）迈克·刘易斯、尹汉·刘、纳曼·戈亚尔、马尔詹·加兹维尼贾德、阿卜杜勒拉赫曼·穆罕默德、奥梅尔·莱维、维塞林·斯托扬诺夫和卢克·泽特尔莫耶。Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练。见于
    *第58届计算语言学协会年会论文集*，第7871–7880页，2020年。
- en: Li et al. (2020) Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, and
    Bowen Zhou. Aspect-aware multimodal summarization for chinese e-commerce products.
    In *Proceedings of the AAAI conference on artificial intelligence*, volume 34,
    pp.  8188–8195, 2020.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2020）李浩然、袁鹏、宋旭、吴有正、贺晓东和周博文。面向方面的多模态摘要在中文电子商务产品中的应用。见于 *AAAI人工智能会议论文集*，第34卷，第8188–8195页，2020年。
- en: 'Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pp.  74–81, 2004.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林（2004）林振耀。Rouge：一个自动评估摘要的软件包。见于 *文本摘要扩展*，第74–81页，2004年。
- en: Liu et al. (2023) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Calibrating llm-based evaluator.
    *arXiv preprint arXiv:2309.13308*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023）刘钰轩、杨天池、黄少涵、张子涵、黄海震、魏富如、邓炜炜、孙峰和张琦。校准基于LLM的评估器。*arXiv 预印本 arXiv:2309.13308*，2023年。
- en: 'Mihalcea & Tarau (2004) Rada Mihalcea and Paul Tarau. Textrank: Bringing order
    into text. In *Proceedings of the 2004 conference on empirical methods in natural
    language processing*, pp.  404–411, 2004.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米哈尔切亚 & 塔劳（2004）拉达·米哈尔切亚和保罗·塔劳。Textrank：将秩序引入文本。见于 *2004年自然语言处理实证方法会议论文集*，第404–411页，2004年。
- en: 'Mukherjee et al. (2020) Rajdeep Mukherjee, Hari Chandana Peruri, Uppada Vishnu,
    Pawan Goyal, Sourangshu Bhattacharya, and Niloy Ganguly. Read what you need: Controllable
    aspect-based opinion summarization of tourist reviews. In *Proceedings of the
    43rd international ACM SIGIR conference on research and development in information
    retrieval*, pp.  1825–1828, 2020.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukherjee et al. (2020) Rajdeep Mukherjee, Hari Chandana Peruri, Uppada Vishnu,
    Pawan Goyal, Sourangshu Bhattacharya, 和 Niloy Ganguly. 读取你需要的内容：可控的基于方面的旅游评论总结。在*第43届国际ACM
    SIGIR信息检索研究与开发会议论文集*，第1825–1828页，2020。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pp.  311–318, 2002.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. Bleu：一种自动评估机器翻译的方法。在*第40届计算语言学协会年会论文集*，第311–318页，2002。
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated
    corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    和 Julien Launay. Falcon LLM的refinedweb数据集：用网络数据超越精心挑选的语料库，仅使用网络数据。*arXiv 预印本 arXiv:2306.01116*，2023。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 利用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*，21(1):5485–5551，2020。
- en: 'See et al. (2017) Abigail See, Peter J Liu, and Christopher D Manning. Get
    to the point: Summarization with pointer-generator networks. *arXiv preprint arXiv:1704.04368*,
    2017.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See et al. (2017) Abigail See, Peter J Liu, 和 Christopher D Manning. 直达要点：使用指针生成网络进行摘要。*arXiv
    预印本 arXiv:1704.04368*，2017。
- en: Sun et al. (2024) Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie
    Li, and Pengfei Liu. The critique of critique. *arXiv preprint arXiv:2401.04518*,
    2024.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2024) Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie
    Li, 和 Pengfei Liu. 批评的批评。*arXiv 预印本 arXiv:2401.04518*，2024。
- en: Tang et al. (2016) Duyu Tang, Bing Qin, and Ting Liu. Aspect level sentiment
    classification with deep memory network. *arXiv preprint arXiv:1605.08900*, 2016.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2016) Duyu Tang, Bing Qin, 和 Ting Liu. 基于深度记忆网络的方面级情感分类。*arXiv
    预印本 arXiv:1605.08900*，2016。
- en: Tas & Kiyani (2007) Oguzhan Tas and Farzad Kiyani. A survey automatic text summarization.
    *PressAcademia Procedia*, 5(1):205–213, 2007.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tas & Kiyani (2007) Oguzhan Tas 和 Farzad Kiyani. 自动文本摘要的调查。*PressAcademia Procedia*，5(1):205–213，2007。
- en: 'Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, 等。Gemma：基于双子座研究和技术的开放模型。*arXiv 预印本 arXiv:2403.08295*，2024。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等。Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。
- en: 'Üstün et al. (2024) Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko,
    Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi,
    Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual
    language model. *arXiv preprint arXiv:2402.07827*, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Üstün et al. (2024) Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko,
    Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi,
    Amr Kayid, 等。Aya模型：一种经过指令微调的开放访问多语言语言模型。*arXiv 预印本 arXiv:2402.07827*，2024。
- en: Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati.
    Can large language models really improve by self-critiquing their own plans? *arXiv
    preprint arXiv:2310.08118*, 2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, 和 Subbarao Kambhampati.
    大型语言模型通过自我批评其计划真的可以改进吗？*arXiv 预印本 arXiv:2310.08118*，2023。
- en: 'Wan & Bansal (2022) David Wan and Mohit Bansal. Factpegasus: Factuality-aware
    pre-training and fine-tuning for abstractive summarization. In *Proceedings of
    the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pp.  1010–1028, 2022.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan & Bansal (2022) 大卫·万和莫希特·班萨尔。Factpegasus：面向事实的预训练和微调用于抽象总结。在*2022年北美计算语言学协会会议：人类语言技术会议论文集*中，第1010–1028页，2022年。
- en: Wang et al. (2016) Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. Attention-based
    lstm for aspect-level sentiment classification. In *Proceedings of the 2016 conference
    on empirical methods in natural language processing*, pp.  606–615, 2016.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) 叶全·王、敏蕾·黄、晓燕·朱和李·赵。基于注意力的lstm用于方面级情感分类。在*2016年自然语言处理经验方法会议论文集*中，第606–615页，2016年。
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*,
    2022.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) 杰森·魏、伊·泰、瑞希·博马萨尼、科林·拉费尔、巴雷特·佐夫、塞巴斯蒂安·博尔戈德、达尼·瑜伽塔马、马滕·博斯马、丹尼·周、唐纳德·梅茨勒等。大语言模型的涌现能力。*arXiv预印本
    arXiv:2206.07682*，2022年。
- en: Yang et al. (2024) Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann
    Heng, and Wai Lam. Unveiling the generalization power of fine-tuned large language
    models. *arXiv preprint arXiv:2403.09162*, 2024.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) 郝然·杨、玉梦·张、佳琪·徐、洪源·陆、芬·安·亨和韦·林。揭示微调大语言模型的泛化能力。*arXiv预印本 arXiv:2403.09162*，2024年。
- en: 'Yang et al. (2022) Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang,
    Xiaoman Pan, Linda Petzold, and Dong Yu. Oasum: Large-scale open domain aspect-based
    summarization. *arXiv preprint arXiv:2212.09233*, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2022) 先军·杨、凯强·宋、尚宇·赵、晓阳·王、小曼·潘、琳达·佩佐尔德和董·余。Oasum：大规模开放领域的基于方面的总结。*arXiv预印本
    arXiv:2212.09233*，2022年。
- en: Yang et al. (2023) Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei
    Cheng. Exploring the limits of chatgpt for query or aspect-based text summarization.
    *arXiv preprint arXiv:2302.08081*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) 先军·杨、颜莉、辛璐·张、海峰·陈和伟·程。探索chatgpt在查询或基于方面的文本总结中的极限。*arXiv预印本
    arXiv:2302.08081*，2023年。
- en: 'Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu.
    Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.
    In *International Conference on Machine Learning*, pp.  11328–11339\. PMLR, 2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) 京庆·张、姚赵、穆罕默德·萨雷赫和彼得·刘。Pegasus：使用提取的缺失句子进行抽象总结的预训练。在*国际机器学习大会*上，第11328–11339页。PMLR，2020年。
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *arXiv preprint
    arXiv:1904.09675*, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019) 天一·张、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格和尤亚夫·阿茨。Bertscore：使用bert评估文本生成。*arXiv预印本
    arXiv:1904.09675*，2019年。
- en: 'Zhang et al. (2023) Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun Ma, Yingtong
    Bu, Xun Zhou, and Liang Zhao. Balancing specialized and general skills in llms:
    The impact of modern tuning and data strategy. *arXiv preprint arXiv:2310.04945*,
    2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) 郑张、陈郑、大唐、柯孙、玉坤·马、颖彤·卜、寻周和梁赵。平衡llms中的专业技能与通用技能：现代调优和数据策略的影响。*arXiv预印本
    arXiv:2310.04945*，2023年。
- en: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don’t make your llm an
    evaluation benchmark cheater. *arXiv preprint arXiv:2311.01964*, 2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) 俊·周、玉涛·朱、志鹏·陈、文彤·陈、韦恩·辛·赵、徐·陈、燕凯·林、纪荣·温和佳伟·韩。不要让你的llm成为评估基准的作弊者。*arXiv预印本
    arXiv:2311.01964*，2023年。
- en: Appendix
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Prompts
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 提示
- en: We use prompting in two stages - finetune-inference and critique. There are
    two kinds of prompts - system prompt and user prompt.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个阶段使用提示 - 微调-推理和评论。有两种类型的提示 - 系统提示和用户提示。
- en: A.1 Finetune and Inference prompt
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 微调和推理提示
- en: 'system: You are an AI assistant who is to generate the summary of a textual
    document specific to a certain aspect.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'system: 你是一个AI助手，负责生成针对特定方面的文本文档摘要。'
- en: 'user prompt - Summarize the textual document given below from the perspective
    of aspect:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: user prompt - 从方面的角度总结下面的文本文档：
- en: '$\#\#\#$ Document: document'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: $\#\#\#$ 文档：文档
- en: A.2 Critique
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评论
- en: 'system: You are an AI assistant who is to evaluate the summary of a textual
    document specific to a certain aspect. You need to return a score between 0 and
    1 reflecting the quality of the generated summary based on some criteria.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 系统：你是一个AI助手，负责评估特定方面的文本总结。你需要返回一个介于0和1之间的分数，反映生成的总结质量的标准。
- en: user:You are given a textual document and the corresponding summary of the document
    generated from the respective of an aspect {aspect} predicted by a language model
    as follows.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：你将获得一个文本文件和从语言模型生成的关于特定方面{aspect}的相应总结。
- en: 'Document: {document}'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 文档：{document}
- en: 'Ground truth summary : {label summary}'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 真实总结：{label summary}
- en: 'Summary with respect to an aspect {aspect}: {model generated summary}'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 针对方面{aspect}的总结：{模型生成的总结}
- en: 'Evaluate the above aspect based summary for the document in terms of each of
    the following criteria and return only a score between 0 and 1 without any explanation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下每一标准评估上述基于方面的文档总结，仅返回一个介于0和1之间的分数，不需要解释：
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The extent to which the generated summary is relevant to a specific aspect {aspect}
    based summary of the document.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的总结在多大程度上与基于特定方面{aspect}的文档总结相关。
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The extent to which the generated aspect-based summary correctly covers all
    the important key points described in the aspect {aspect} based summary of the
    document.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的基于方面的总结在多大程度上正确涵盖了方面{aspect}基于总结中描述的所有重要要点。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The extent to which the summary does not contain information specific to all
    other possible aspects {aspect_set_in_a_domain - aspect} based summary.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结在多大程度上不包含针对所有其他可能方面{aspect_set_in_a_domain - aspect}的具体信息。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rate the summary from the point of view of the aspect – whether the summary
    is good, average, or bad. A good summary effectively captures the essential points,
    presenting them clearly and concisely. It maintains accuracy, encourages reader
    engagement, and serves as a compelling introduction to the content. An average
    summary conveys the main points but may lack some clarity or detail, presenting
    a decent overview without standing out in terms of conciseness or precision. It
    provides a basic understanding but might benefit from a more refined focused summary
    fails to accurately convey the main points, containing inaccuracies or misinterpretations.
    It is either overly verbose or lacks coherence, making it difficult for the reader
    to grasp the core information effectively.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从该方面的角度对总结进行评级——总结是否良好、一般或差。一个好的总结能够有效地捕捉要点，清晰且简明地呈现。它保持准确性，鼓励读者参与，并作为内容的引人入胜的介绍。一个一般的总结传达了主要点，但可能缺乏一些清晰度或细节，提供了一个不错的概述，但在简洁性或准确性上没有突出。它提供了基本理解，但可能需要更精炼的总结。一个不合格的总结未能准确传达主要点，包含不准确或误解的信息。它要么过于冗长，要么缺乏连贯性，使读者难以有效掌握核心信息。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Calculated summary from the point of view of the aspect [Good/Bad/Average] [Calculated
    from 4 with the help of manual annotation]
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从该方面的角度计算的总结 [好/差/一般] [通过人工标注从4计算]
- en: Appendix B Time and GPU
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 时间和GPU
- en: 'We experiment on 80GB A100 GPU with GPU clock cycle 210 MHz. The finetuning
    and inference time of our finetuned models are in Table [6](#A2.T6 "Table 6 ‣
    Appendix B Time and GPU ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在80GB A100 GPU上进行实验，GPU时钟周期为210 MHz。我们微调模型的微调和推理时间见表[6](#A2.T6 "Table 6 ‣
    Appendix B Time and GPU ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization")。'
- en: '| Model | Finetune Time | Inference Time |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 微调时间 | 推理时间 |'
- en: '| --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Llama2-7b | 22 hrs | 2hrs 10 mins |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b | 22小时 | 2小时10分钟 |'
- en: '| Llama2-13b | 44 hrs | 3hrs 10 mins |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 44小时 | 3小时10分钟 |'
- en: '| Mistral-7b | 36 hrs | 2hrs 44 mins |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b | 36小时 | 2小时44分钟 |'
- en: '| Aya | 38 hrs | 2hrs 40mins |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Aya | 38小时 | 2小时40分钟 |'
- en: '| Gemma | 50 hrs | 4hrs 40mins |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | 50小时 | 4小时40分钟 |'
- en: 'Table 6: Model Training Time [using 80GB A100 GPU]'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：模型训练时间 [使用80GB A100 GPU]
- en: Appendix C Examples
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 示例
- en: 'An examples of OASUM aspect based summary is shown in Fig [3](#A3.F3 "Figure
    3 ‣ Appendix C Examples ‣ Leveraging the Power of LLMs: A Fine-Tuning Approach
    for High-Quality Aspect-Based Summarization") . Example of human annotation interface
    is shown in Fig [4](#A3.F4 "Figure 4 ‣ Appendix C Examples ‣ Leveraging the Power
    of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: OASUM 方面的总结示例如图 [3](#A3.F3 "图 3 ‣ 附录 C 示例 ‣ 利用 LLM 的力量：高质量方面总结的微调方法") 所示。人类标注界面的示例如图
    [4](#A3.F4 "图 4 ‣ 附录 C 示例 ‣ 利用 LLM 的力量：高质量方面总结的微调方法") 所示。
- en: '![Refer to caption](img/0f13114e2acfeb6a26a893c0b81d02a4.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0f13114e2acfeb6a26a893c0b81d02a4.png)'
- en: 'Figure 3: OASUM summary example snapshot'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：OASUM 总结示例快照
- en: '![Refer to caption](img/451b8aa1a1acab04254a1ef1e347a485.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/451b8aa1a1acab04254a1ef1e347a485.png)'
- en: 'Figure 4: original summary and Llama2-13b finetune comparison experiment example
    snapshot'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：原始总结和 Llama2-13b 微调对比实验示例快照
