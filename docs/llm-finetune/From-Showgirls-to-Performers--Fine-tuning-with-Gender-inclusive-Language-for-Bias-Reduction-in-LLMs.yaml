- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:35:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:35:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From Showgirls to Performers: Fine-tuning with Gender-inclusive Language for
    Bias Reduction in LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Showgirls到表演者：通过性别包容性语言微调以减少LLMs中的偏见
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.04434](https://ar5iv.labs.arxiv.org/html/2407.04434)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.04434](https://ar5iv.labs.arxiv.org/html/2407.04434)
- en: Marion Bartl    Susan Leavy
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Marion Bartl    Susan Leavy
- en: Insight SFI Research Centre for Data Analytics
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Insight SFI 研究中心数据分析
- en: School of Information and Communication Studies
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 信息与通信研究学院
- en: University College Dublin
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 都柏林大学
- en: marion.bartl@insight-centre.org
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: marion.bartl@insight-centre.org
- en: susan.leavy@ucd.ie
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: susan.leavy@ucd.ie
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Gender bias is not only prevalent in Large Language Models (LLMs) and their
    training data, but also firmly ingrained into the structural aspects of language
    itself. Therefore, adapting linguistic structures within LLM training data to
    promote gender-inclusivity can make gender representations within the model more
    inclusive. The focus of our work are gender-exclusive affixes in English, such
    as in showgirl or man-cave, which can perpetuate gender stereotypes and binary
    conceptions of gender. We use an LLM training dataset to compile a catalogue of
    692 gender-exclusive terms along with gender-neutral variants and from this, develop
    a gender-inclusive fine-tuning dataset, the Tiny Heap. Fine-tuning three different
    LLMs with this dataset, we observe an overall reduction in gender-stereotyping
    tendencies across the models. Our approach provides a practical method for enhancing
    gender inclusivity in LLM training data and contributes to incorporating queer-feminist
    linguistic activism in bias mitigation research in NLP.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 性别偏见不仅普遍存在于大型语言模型（LLMs）及其训练数据中，还深深根植于语言本身的结构方面。因此，调整LLM训练数据中的语言结构以促进性别包容性，可以使模型中的性别表现更加包容。我们工作的重点是英语中的性别专属词缀，例如在showgirl或man-cave中，这些词缀可能会延续性别刻板印象和二元性别观念。我们使用LLM训练数据集编制了一个包含692个性别专属术语及其性别中立变体的目录，并基于此开发了一个性别包容的微调数据集，即Tiny
    Heap。通过使用该数据集对三种不同的LLM进行微调，我们观察到模型中的性别刻板印象整体减少。我们的方法提供了一种在LLM训练数据中增强性别包容性的实际方法，并有助于将酷儿女性主义语言行动融入NLP偏见缓解研究中。
- en: 'From Showgirls to Performers: Fine-tuning with Gender-inclusive Language for
    Bias Reduction in LLMs'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从Showgirls到表演者：通过性别包容性语言微调以减少LLMs中的偏见
- en: Marion Bartl  and Susan Leavy Insight SFI Research Centre for Data Analytics
    School of Information and Communication Studies University College Dublin marion.bartl@insight-centre.org
    susan.leavy@ucd.ie
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Marion Bartl 和 Susan Leavy Insight SFI 研究中心数据分析学院 都柏林大学信息与通信研究学院 marion.bartl@insight-centre.org
    susan.leavy@ucd.ie
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have become ubiquitous in Natural Language Processing
    (NLP) due to their impressive capabilities in a variety of tasks. However, they
    also carry risks arising from social biases incorporated into models from the
    training data (Bender et al., [2021](#bib.bib7)). Well-documented among these
    are harmful gender biases such as reliance on stereotypes and erasure of non-binary
    gender identities (Cao and Daumé, [2021](#bib.bib12); Ovalle et al., [2023](#bib.bib35),
    a.o.). Structural aspects of language itself and linguistic norms can reflect
    as well as shape societal concepts of gender (Pauwels, [2003](#bib.bib36); Whorf
    and Carroll, [1956](#bib.bib51)). Within the context of LLMs, encoded representations
    of gender inform language generation and classification decisions, thereby having
    the potential to influence societal concepts of gender (Bommasani et al., [2022](#bib.bib10)).
    It is vital therefore, to ensure that LLMs are evaluated and trained to minimize
    gender bias and promote equitable representation of all genders.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在各种任务中的出色能力而在自然语言处理（NLP）中变得无处不在。然而，它们也存在来自训练数据中社会偏见的风险（Bender
    等， [2021](#bib.bib7)）。其中广为记录的有有害的性别偏见，例如对刻板印象的依赖和对非二元性别身份的抹除（Cao 和 Daumé，[2021](#bib.bib12)；Ovalle
    等，[2023](#bib.bib35)等）。语言自身的结构方面和语言规范也能反映以及塑造社会对性别的概念（Pauwels，[2003](#bib.bib36)；Whorf
    和 Carroll，[1956](#bib.bib51)）。在LLMs的背景下，性别的编码表示影响语言生成和分类决策，从而有可能影响社会对性别的概念（Bommasani
    等，[2022](#bib.bib10)）。因此，确保LLMs被评估和训练以最小化性别偏见并促进所有性别的公平表现是至关重要的。
- en: In English, linguistic structures have a long history of reinforcing traditional
    gender roles and the concept of male gender as the default (Mills, [2012](#bib.bib29)).
    Examples include the use of man to mean all humans, the indication of women’s
    marital status in terms of address (Miss, Mrs., Ms.), or the marking of deviation
    from gendered norms (male nurse, girl boss). Sexist and gender-exclusive linguistic
    constructions have been discouraged in official style guides (APA, [2020](#bib.bib1))
    and their use has been in decline (Baker, [2010b](#bib.bib4)). However, the nature
    of language change is slow, with new and traditional variations existing simultaneously.
    Given the scale of LLM training data (Bender et al., [2021](#bib.bib7)) and the
    disproportionate representation of men within textual data Baker ([2010a](#bib.bib3)),
    language models have the potential to proliferate and reinforce stereotypical
    and traditional views of gender.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，语言结构有着长期强化传统性别角色和男性性别作为默认概念的历史（Mills，[2012](#bib.bib29)）。例如，使用man表示所有人类，使用称谓（Miss，Mrs.，Ms.）来指示女性的婚姻状态，或标记性别规范的偏离（male
    nurse，girl boss）。官方风格指南（APA，[2020](#bib.bib1)）已不鼓励使用性别歧视和排他性的语言结构，这些用法也在减少（Baker，[2010b](#bib.bib4)）。然而，语言变化的本质是缓慢的，新旧变体同时存在。鉴于LLM训练数据的规模（Bender
    et al.，[2021](#bib.bib7)）以及文本数据中男性的不成比例代表（Baker，[2010a](#bib.bib3)），语言模型可能会扩散和强化刻板印象和传统性别观念。
- en: Approaches to mitigating bias in LLMs have included fine-tuning with gender-inclusive
    language (Thakur et al., [2023](#bib.bib47)). Data interventions with gender-inclusive
    text aim to reduce the use of binary gender terms in cases where gender is irrelevant
    (for example, a chairman and chairwoman do the same job) and thereby allow for
    association of a term with all genders (chairperson). However, the replacement
    of sexist and gender-exclusive terminology often relies on limited lists of gender-neutral
    terms (Ghanbarzadeh et al., [2023](#bib.bib18); Thakur et al., [2023](#bib.bib47)),
    and often focuses on professions (Fatemi et al., [2023](#bib.bib15)). Additionally,
    previous works on fine-tuning LLMs with gender-inclusive data have primarily carried
    out experiments with masked language models such as BERT (Devlin et al., [2019](#bib.bib14))
    and its derivatives (Vashishtha et al., [2023](#bib.bib50)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 减少LLMs偏见的方法包括使用性别包容性语言的微调（Thakur et al.，[2023](#bib.bib47)）。性别包容性文本的数据干预旨在减少在性别无关的情况下使用二元性别术语（例如，chairman和chairwoman做同样的工作），从而允许将一个术语与所有性别相关联（chairperson）。然而，替换性别歧视和排他性术语通常依赖于有限的性别中立术语列表（Ghanbarzadeh
    et al.，[2023](#bib.bib18)；Thakur et al.，[2023](#bib.bib47)），且通常集中于职业（Fatemi et
    al.，[2023](#bib.bib15)）。此外，之前关于使用性别包容性数据对LLMs进行微调的研究主要在如BERT（Devlin et al.，[2019](#bib.bib14)）及其衍生模型（Vashishtha
    et al.，[2023](#bib.bib50)）等掩蔽语言模型上进行实验。
- en: In this research, we focused on expanding the coverage of gender-exclusive terminology
    and experimented with fine-tuning both causal and masked LLMs. We first exploited
    structural elements of English that relate to gender discrimination and exclusion
    in order to generate a larger catalogue of words that are unnecessarily gendered
    along with gender neutral alternatives. We extracted nouns with gender-marking
    prefixes and suffixes from a common training corpus, OpenWebText2 (Gao et al.,
    [2020](#bib.bib17)), which was used to train LLMs like Meta’s Llama2 (Thakur et al.,
    [2023](#bib.bib47)) and Microsoft’s MT-NLG (Smith et al., [2022](#bib.bib43)).
    The distribution of extracted gender-marking nouns demonstrated clear androcentric
    tendencies within the corpus. We compiled gender-neutral variants for each term
    with a gender-marking affix to form a catalogue of 692 term pairs. This resource
    is just over three times larger than the size of previously available resources
    and could be used in assessments of gender skew within LLM training corpora as
    well as in the replacement gender-exclusive terminology. We also developed a small-scale,
    multi-domain fine-tuning corpus, using our catalogue to replace gender-exclusive
    with gender-neutral words. We also employed the NeuTral Rewriter (Vanmassenhove
    et al., [2021](#bib.bib49)) to replace gendered pronouns (he, she, himself etc.)
    with singular they. The resulting corpus was used to fine-tune three different
    (masked and causal) LLMs. The results of this process of fine-tuning with gender
    inclusive terminology demonstrated an overall tendency towards reduction in gender-stereotyping
    exhibited by the models as well as a reduction in the generation of harmful language
    in gendered contexts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们专注于扩大性别排他性术语的覆盖范围，并尝试对因果和掩蔽语言模型进行微调。我们首先利用英语中与性别歧视和排斥相关的结构元素，以生成一个包含不必要性别标记的词汇表及性别中性替代词。我们从一个常用的训练语料库
    OpenWebText2（Gao et al., [2020](#bib.bib17)）中提取了带有性别标记前缀和后缀的名词，该语料库用于训练如 Meta
    的 Llama2（Thakur et al., [2023](#bib.bib47)）和微软的 MT-NLG（Smith et al., [2022](#bib.bib43)）等语言模型。提取的性别标记名词的分布展示了语料库中明显的男性中心倾向。我们为每个带有性别标记的词汇编制了性别中性变体，形成了一个包含
    692 对术语的目录。这个资源比以前可用的资源大约大三倍，可以用于评估语言模型训练语料库中的性别偏差以及替换性别排他性术语。我们还开发了一个小规模、多领域的微调语料库，利用我们的目录将性别排他性词汇替换为性别中性词汇。我们还使用了
    NeuTral Rewriter（Vanmassenhove et al., [2021](#bib.bib49)）将性别代词（如 he、she、himself
    等）替换为单数 they。生成的语料库被用于微调三种不同（掩蔽和因果）语言模型。通过使用性别包容性术语的微调过程的结果表明，模型的性别刻板印象总体上有所减少，并且在性别化背景下生成有害语言的现象也有所减少。
- en: Contributions
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show clear androcentric tendencies within a commonly used LLM training corpus.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在一个常用语言模型训练语料库中明显的男性中心倾向。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We construct a catalogue of 692 term pairs, consisting of a gender-exclusive
    terms and neutral alternatives, which we release for public use¹¹1[https://github.com/marionbartl/affixed_words](https://github.com/marionbartl/affixed_words).
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们构建了一个包含 692 对术语的目录，其中包括性别排他性术语及其中性替代词，我们将其公开发布供公众使用¹¹1[https://github.com/marionbartl/affixed_words](https://github.com/marionbartl/affixed_words)。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that automatically generated gender-inclusive English is effective in
    reducing gender stereotyping in LLMs through fine-tuning²²2[https://github.com/marionbartl/performers](https://github.com/marionbartl/performers).
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了自动生成的性别包容性英语在通过微调减少语言模型中的性别刻板印象方面的有效性²²2[https://github.com/marionbartl/performers](https://github.com/marionbartl/performers)。
- en: 2 Bias Statement
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 偏见声明
- en: The focus of this work is gender-inclusive language, and its counterpart, sexist
    language. Sexist language, following [Frye](#bib.bib16)’s ([1983](#bib.bib16))
    definition of sexism, can be defined as language that clearly divides between
    two genders, in which one gender (masculine) is treated as hierarchically superior
    to the other (feminine). This superiority is expressed, for example, through the
    generic use of masculine gendered expressions (e.g. use of terms such as mankind,
    chairman to refer to people of any gender).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的重点是性别包容性语言及其对立面，性别歧视语言。性别歧视语言，按照 [Frye](#bib.bib16)（[1983](#bib.bib16)）的定义，可以被定义为明确区分两个性别的语言，其中一个性别（男性）被视为相对于另一个性别（女性）在等级上更优越。这种优越感通过使用男性性别表达的通用用法（例如使用
    mankind、chairman 等术语来指代任何性别的人）来表达。
- en: Our work is based on the assumption that sexist language in training data is
    one of the sources of gender bias in LLMs. Specifically, we would expect models
    to favor masculine expressions over gender-neutral alternatives, creating a representational
    harm for people of non-masculine gender (Blodgett et al., [2020](#bib.bib8)).
    Sexist expressions additionally reinforce traditional gender roles (e.g. male
    nurse), therefore we would also expect models to favor gender-stereotypical expressions.
    Moreover, since sexist language is based on a binary model of gender, we expect
    models to default to this. This can lead to misrepresentation and erasure of non-binary
    genders in downstream applications, creating allocational and representational
    harms for non-binary users of these systems (Blodgett et al., [2020](#bib.bib8)).
    Not adjusting LLMs to accurately represent the variety of genders that exist in
    society will contribute to the ongoing marginalization of people identifying as
    gender-queer (Ovalle et al., [2023](#bib.bib35)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作基于这样的假设：训练数据中的性别歧视语言是导致大型语言模型（LLMs）性别偏见的一个来源。具体而言，我们预期模型会倾向于使用男性表达方式而非性别中立的替代方式，从而对非男性性别的人群造成表现上的伤害（Blodgett
    等，[2020](#bib.bib8)）。性别歧视的表达还会强化传统的性别角色（例如男性护士），因此我们也预期模型会倾向于使用性别刻板的表达方式。此外，由于性别歧视语言基于二元性别模型，我们预期模型会默认这种模式。这可能导致在下游应用中对非二元性别的误表现和抹除，对这些系统的非二元用户造成分配和表现上的伤害（Blodgett
    等，[2020](#bib.bib8)）。如果不调整LLMs以准确表示社会中存在的各种性别，将助长性别酷儿人群的持续边缘化（Ovalle 等，[2023](#bib.bib35)）。
- en: 3 Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: 'Large Language Models (LLMs) have been shown to encode a variety of social
    biases contained in their training data (Gupta et al., [2023](#bib.bib20); Salinas
    et al., [2023](#bib.bib41)), among them gender bias (Stanczak and Augenstein,
    [2021](#bib.bib44)). Due to the current prevalence of transfer learning in NLP,
    in which a pre-trained model is fine-tuned with task-specific data, transfer learning
    has recently also been adapted by works that aimed to reduce gender bias in LLMs (Lauscher
    et al., [2021](#bib.bib24); Ghanbarzadeh et al., [2023](#bib.bib18)). In this
    approach, an LLM is fine-tuned with data that has undergone interventions to increase
    gender fairness. This approach is supported by the finding that biases in fine-tuning
    data have a greater influence on downstream model behavior than biases in the
    pre-training data (Steed et al., [2022](#bib.bib45)). Previous interventions to
    fine-tuning data include Counterfactual Data Augmentation (CDA), in which masculine
    and feminine pronouns and gendered nouns are swapped for the respective other (Ghanbarzadeh
    et al., [2023](#bib.bib18); Vashishtha et al., [2023](#bib.bib50); Fatemi et al.,
    [2023](#bib.bib15)). Another intervention replaces gendered words for gender-neutral
    words (fire fighter for fireman) or phrases containing both masculine and feminine
    genders (he and she for he; [Thakur et al.](#bib.bib47), [2023](#bib.bib47)).
    This kind of intervention is not new: it rests upon a longstanding tradition of
    research and advocacy the field of feminist linguistics, which has been promoting
    changes in the lexicon to reduce gender stereotyping and masculine-default language
    since the 1970s (Kramer, [2016](#bib.bib21); Mills, [2012](#bib.bib29); Lakoff,
    [1973](#bib.bib22)). More recently such changes to the language, also called feminist
    language reform, have incorporated ways of adapting language to include non-binary
    and trans gender identities, such as the third person singular (neo)pronouns (they,
    xe, ze, etc.). The usage and possible modelling of this extended lexicon of pronouns
    within the context of NLP was analyzed by Lauscher et al. ([2022](#bib.bib23)).
    Lund et al. ([2023](#bib.bib27)) also showed that training on data containing
    singular they can reduce gender bias in grammatical error correction. Furthermore, Vanmassenhove
    et al. ([2021](#bib.bib49)) and Sun et al. ([2021](#bib.bib46)) developed rule-based
    and neutral machine translation-based models to modify English text to render
    it gender-neutral. [Vanmassenhove et al.](#bib.bib49)’s ([2021](#bib.bib49)) NeuTral
    Rewriter replaces gendered pronouns with singular they and a list of gendered
    nouns with neutral variants. However, while the amount of NLP research incorporating
    and exploring strategies of feminist language reform has grown, the queer-feminist
    linguistic research it is based on is, with some exceptions (Devinney et al.,
    [2022](#bib.bib13); Piergentili et al., [2023a](#bib.bib37); Seaborn et al., [2023](#bib.bib42)),
    rarely acknowledged and even less often informs the research itself.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已显示出它们在训练数据中编码了各种社会偏见（Gupta 等，[2023](#bib.bib20)；Salinas 等，[2023](#bib.bib41)），其中包括性别偏见（Stanczak
    和 Augenstein，[2021](#bib.bib44)）。由于当前在自然语言处理（NLP）中普遍采用迁移学习，即利用预训练模型并对其进行任务特定的数据微调，近期的研究也开始将迁移学习应用于减少LLMs中的性别偏见（Lauscher
    等，[2021](#bib.bib24)；Ghanbarzadeh 等，[2023](#bib.bib18)）。在这种方法中，LLM通过经过干预以提高性别公平的数据进行微调。这种方法得到了这样一种发现的支持：在微调数据中的偏见对下游模型行为的影响大于预训练数据中的偏见（Steed
    等，[2022](#bib.bib45)）。以前的微调数据干预包括反事实数据增强（CDA），在这种方法中，男性和女性代词及性别化名词会被交换（Ghanbarzadeh
    等，[2023](#bib.bib18)；Vashishtha 等，[2023](#bib.bib50)；Fatemi 等，[2023](#bib.bib15)）。另一种干预则是将性别化词汇替换为性别中性词汇（如将
    fireman 替换为 fire fighter）或包含男性和女性性别的短语（如将 he and she 替换为 he；[Thakur 等](#bib.bib47)，[2023](#bib.bib47)）。这种干预并不新鲜：它基于女性语言学领域长期以来的研究和倡导传统，该领域自1970年代以来一直推动词汇上的变更，以减少性别刻板印象和男性默认语言（Kramer，[2016](#bib.bib21)；Mills，[2012](#bib.bib29)；Lakoff，[1973](#bib.bib22)）。最近，这种语言变更，也称为女性主义语言改革，已纳入了适应语言以包括非二元和跨性别身份的方式，例如第三人称单数（neo）代词（they，xe，ze等）。Lauscher
    等人（[2022](#bib.bib23)）分析了这种扩展代词词汇在NLP背景下的使用及其可能的建模。Lund 等人（[2023](#bib.bib27)）还表明，使用包含单数
    they 的数据进行训练可以减少语法错误纠正中的性别偏见。此外，Vanmassenhove 等人（[2021](#bib.bib49)）和 Sun 等人（[2021](#bib.bib46)）开发了基于规则和中性机器翻译的模型，以将英文文本修改为性别中性。[Vanmassenhove
    等人](#bib.bib49)（[2021](#bib.bib49)）的 NeuTral Rewriter 将性别化代词替换为单数 they，并将性别化名词列表替换为中性变体。然而，尽管包含和探索女性主义语言改革策略的NLP研究数量有所增长，但它所基于的酷儿女性主义语言学研究，除了一些例外（Devinney
    等，[2022](#bib.bib13)；Piergentili 等，[2023a](#bib.bib37)；Seaborn 等，[2023](#bib.bib42)），很少得到承认，更少作为研究的基础。
- en: 4 Method
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: Gender bias in the English language is reflected in features such as masculine
    generics and is captured in datasets through, for example, skewed distributions
    of pronouns and profession words in the same context. However, it is also contained
    in structural elements of the language itself, such as gender-marking affixes.
    The most frequent are suffixes such as -man in spokesman, but gender can also
    be marked with a prefix, such as in man-power or girlboss. Words marked with masculine
    suffixes have traditionally been used in a generic sense (e.g. Madam Chairman),
    however, with the emergence of feminist language reform, style guides have advised
    against their use (Piergentili et al., [2023b](#bib.bib38)). In English, the most
    common replacement strategy for gendered generics is neutralisation (chairperson),
    because all gender identities, not just male and female, can be referred to by
    gender-neutral nouns. In NLP, research using gender-neutral language in the context
    of English LLMs has mainly relied on lists of common gender-neutral replacements (Vanmassenhove
    et al., [2021](#bib.bib49); Thakur et al., [2023](#bib.bib47)), without taking
    structural processes such as affixation into account in order to broaden the coverage
    of these lists.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 英语中的性别偏见反映在如男性通用词等特征中，并通过例如在相同语境中代词和职业词汇的分布不均来捕捉。然而，它也存在于语言本身的结构元素中，例如性别标记词缀。最常见的是像发言人中的-man这样的后缀，但性别也可以通过前缀标记，例如在man-power或girlboss中。带有男性后缀的词汇传统上用于通用意义（例如，女主席），然而，随着女性主义语言改革的出现，风格指南建议避免使用这些词汇（Piergentili
    et al., [2023b](#bib.bib38)）。在英语中，性别通用词最常见的替代策略是中性化（chairperson），因为所有性别身份，不仅仅是男性和女性，都可以用性别中性名词来指代。在自然语言处理（NLP）中，使用性别中性语言的研究主要依赖于常见性别中性替代词的列表（Vanmassenhove
    et al., [2021](#bib.bib49); Thakur et al., [2023](#bib.bib47)），而没有考虑到如词缀化等结构过程，以扩大这些列表的覆盖范围。
- en: 'In this section we first outline the process of extracting unnecessarily gendered
    words based on gender-marking affixes (§[4.1](#S4.SS1 "4.1 Word Catalogue ‣ 4
    Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language
    for Bias Reduction in LLMs")). We then describe the gender-neutralizing interventions
    to our fine-tuning data (§[4.2](#S4.SS2 "4.2 Fine-Tuning Data ‣ 4 Method ‣ From
    Showgirls to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction
    in LLMs")) as well as the models (§[4.3](#S4.SS3 "4.3 Models and Fine-tuning ‣
    4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language
    for Bias Reduction in LLMs")) and bias measurements used (§[4.4](#S4.SS4 "4.4
    Bias Evaluation Metrics ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs")).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先概述了基于性别标记词缀提取不必要的性别化词汇的过程（§[4.1](#S4.SS1 "4.1 词汇目录 ‣ 4 方法 ‣ 从表演女郎到演员：使用性别包容语言微调以减少LLMs中的偏见")）。然后，我们描述了对微调数据的性别中立干预（§[4.2](#S4.SS2
    "4.2 微调数据 ‣ 4 方法 ‣ 从表演女郎到演员：使用性别包容语言微调以减少LLMs中的偏见")），以及使用的模型（§[4.3](#S4.SS3 "4.3
    模型与微调 ‣ 4 方法 ‣ 从表演女郎到演员：使用性别包容语言微调以减少LLMs中的偏见")）和偏见测量方法（§[4.4](#S4.SS4 "4.4 偏见评估指标
    ‣ 4 方法 ‣ 从表演女郎到演员：使用性别包容语言微调以减少LLMs中的偏见")）。
- en: 4.1 Word Catalogue
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 词汇目录
- en: '| affix |  | round 1 | round 2 | round 3 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 词缀 |  | 第一轮 | 第二轮 | 第三轮 |'
- en: '| prefix | woman- | 10 | 4 | 4 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 前缀 | woman- | 10 | 4 | 4 |'
- en: '|  | girl- | 30 | 13 | 10 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | girl- | 30 | 13 | 10 |'
- en: '|  | man- | 87 | 47 | 49 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | man- | 87 | 47 | 49 |'
- en: '|  | boy- | 59 | 11 | 7 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | boy- | 59 | 11 | 7 |'
- en: '|  | total | 186 | 75 | 70 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 总计 | 186 | 75 | 70 |'
- en: '| suffix | -woman | 42 | 37 | 35 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 后缀 | -woman | 42 | 37 | 35 |'
- en: '|  | -girl | 47 | 24 | 14 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | -girl | 47 | 24 | 14 |'
- en: '|  | -man | 271 | 238 | 180 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | -man | 271 | 238 | 180 |'
- en: '|  | -boy | 62 | 41 | 24 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | -boy | 62 | 41 | 24 |'
- en: '|  | -womanship | 2 | 2 | 2 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | -womanship | 2 | 2 | 2 |'
- en: '|  | -manship | 53 | 32 | 30 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | -manship | 53 | 32 | 30 |'
- en: '|  | total | 477 | 342 | 285 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | 总计 | 477 | 342 | 285 |'
- en: '| TOTAL |  | 663 | 417 | 355 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 总计 |  | 663 | 417 | 355 |'
- en: '| PERCENT |  | 100% | 62.9% | 53.54% |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 百分比 |  | 100% | 62.9% | 53.54% |'
- en: 'Table 1: Number of singular nouns with gender-marking affixes extracted from
    subsection of OpenWebText2 corpus throughout verification process.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：从OpenWebText2语料库的子部分中提取的具有性别标记词缀的单数名词数量，经过验证过程。
- en: '| -man | # | -woman | # | -boy | # | -girl | # |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| -man | # | -woman | # | -boy | # | -girl | # |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| spokesman | 44,004 | spokeswoman | 14,044 | cowboy | 1167 | showgirl | 46
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 发言人 | 44,004 | 女发言人 | 14,044 | 牛仔 | 1167 | 表演女郎 | 46 |'
- en: '| congressman | 4,551 | congresswoman | 419 | fanboy | 388 | fangirl | 42 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 国会议员 | 4,551 | 女国会议员 | 419 | 粉丝男孩 | 388 | 粉丝女孩 | 42 |'
- en: '| businessman | 3,830 | businesswoman | 231 | playboy | 374 | cowgirl | 39
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 商人 | 3,830 | 女商人 | 231 | 花花公子 | 374 | 牛仔女孩 | 39 |'
- en: '| policeman | 3,015 | policewoman | 151 | tomboy | 199 | playgirl | 6 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 警察 | 3,015 | 女警察 | 151 | 男孩气 | 199 | 女孩气 | 6 |'
- en: '| freshman | 1,055 | anchorwoman | 40 | busboy | 71 | babygirl | 4 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 新生 | 1,055 | 女主播 | 40 | 清洁工 | 71 | 小女孩 | 4 |'
- en: '| fisherman | 991 | forewoman | 33 | paperboy | 69 | ballgirl | 4 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 渔夫 | 991 | 女工 | 33 | 纸男孩 | 69 | 球女孩 | 4 |'
- en: '| cameraman | 910 | everywoman | 30 | homeboy | 47 | camgirl | 4 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 摄影师 | 910 | 每位女性 | 30 | 家乡男孩 | 47 | 自拍女孩 | 4 |'
- en: '| statesman | 671 | noblewoman | 21 | plowboy | 32 | papergirl | 4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 政治家 | 671 | 贵族女性 | 21 | 农场男孩 | 32 | 纸女孩 | 4 |'
- en: '| defenseman | 571 | spokewoman | 19 | bellboy | 16 | tomgirl | 3 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 防守者 | 571 | 女发言人 | 19 | 行李员 | 16 | 男孩气 | 3 |'
- en: '| madman | 505 | charwoman | 16 | callboy | 13 | schoolgirl | 3 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 疯子 | 505 | 女仆 | 16 | 招待员 | 13 | 校女孩 | 3 |'
- en: 'Table 2: Top 10 words with gender-denoting suffixes after second round of verification
    and their frequencies within 200-million token subset of OpenWebText2'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：第二轮验证后带有性别标识后缀的前 10 个词汇及其在 2 亿标记子集中的频率
- en: We extracted words with the suffixes -man, -manship, -woman, -womanship, -boy,
    -girl and words with the prefixes man-³³3Words with man- prefixes were only included
    if they also had the dash (-) following man, because otherwise the false positive
    rate (manager, mandate, etc.) would have been too high., woman-, boy- and girl-.
    We used a 200 million token random subsection of the OpenWebText2 corpus (Gao
    et al., [2020](#bib.bib17)) for extraction. The words were extracted using regular
    expressions within Python. We additionally filtered the words to include only
    English singular nouns. We only filtered for singular nouns to reduce the amount
    of redundant extractions, and to simplify the dictionary verification later on.
    Plurals for all verified words were added after the third round of verification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取了带有后缀 -man、-manship、-woman、-womanship、-boy、-girl 的词汇和带有前缀 man-³³3 词汇。只有在
    man 后跟连字符（-）的词汇才被纳入，因为否则误报率（例如 manager、mandate 等）会太高。woman-、boy- 和 girl-。我们使用了
    OpenWebText2 语料库的 2 亿个标记的随机子集（Gao et al.，[2020](#bib.bib17)）进行提取。使用 Python 中的正则表达式提取了词汇。我们额外筛选了词汇，只包括英文单数名词。我们仅筛选了单数名词，以减少冗余提取的数量，并简化后续的词典验证。所有经过验证的词汇的复数形式在第三轮验证后被添加。
- en: 'The first round of verification of extracted affixed terms generally followed
    a human-in-the-loop approach, meaning that after 20 files, each 1MB in size, the
    extracted words were manually checked for validity. This eliminated a variety
    of false positives such as words in which affixes did not denote gender (german,
    ramen), spelling errors (camerman, sopkesman), surnames (zimmerman), and other
    word creations (heythereman, mrfredman). In total, 663 words were extracted in
    the first round (ref. Table [1](#S4.T1 "Table 1 ‣ 4.1 Word Catalogue ‣ 4 Method
    ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language for
    Bias Reduction in LLMs")).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '提取的词汇的第一轮验证通常遵循“人机结合”的方法，这意味着在每处理20个文件（每个文件1MB大小）后，手动检查提取的词汇的有效性。这消除了各种误报，例如词汇中词缀并不表示性别（german，ramen）、拼写错误（camerman，sopkesman）、姓氏（zimmerman）以及其他词汇创造（heythereman，mrfredman）。总共提取了663个词汇（参见表
    [1](#S4.T1 "Table 1 ‣ 4.1 Word Catalogue ‣ 4 Method ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs")）。'
- en: 'After extraction, the terms were verified in the second round using the API
    of the BabelNet encyclopedic dictionary (Navigli and Ponzetto, [2012](#bib.bib31)).
    BabelNet was chosen due to its broad coverage of lexical resources; its search
    engine combines entries from WordNet, Wikidata and Wikipedia among others. Terms
    that did not return an entry in BabelNet were disregarded in order to eliminate
    less established terms, slang and sexually charged terminology. If a term contained
    a dash, such as in man-bun, but could not be found in BabelNet, we also searched
    for the term with a space instead of the dash to not disregard terms due to spelling
    differences. Table [2](#S4.T2 "Table 2 ‣ 4.1 Word Catalogue ‣ 4 Method ‣ From
    Showgirls to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction
    in LLMs") shows the top ten words containing the four simple gender-marking suffixes
    and their frequency. The highest frequent words with gendered prefixes, and words
    with -wo/manship suffixes are shown in Table [6](#A1.T6 "Table 6 ‣ Appendix A
    Appendix ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language
    for Bias Reduction in LLMs") and [7](#A1.T7 "Table 7 ‣ Appendix A Appendix ‣ From
    Showgirls to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction
    in LLMs") in the Appendix, respectively.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '提取后，术语在第二轮中通过BabelNet百科全书API进行了验证（Navigli和Ponzetto，[2012](#bib.bib31)）。选择BabelNet是由于其广泛的词汇资源覆盖；其搜索引擎结合了WordNet、Wikidata和Wikipedia等多个来源。未在BabelNet中返回条目的术语被忽略，以排除不太确定的术语、俚语和带有性暗示的术语。如果一个术语包含破折号，如man-bun，但在BabelNet中无法找到，我们还会用空格代替破折号搜索，以避免因拼写差异而忽略术语。表[2](#S4.T2
    "Table 2 ‣ 4.1 Word Catalogue ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs")显示了包含四种简单性别标记后缀的前十个词及其频率。含有性别前缀的高频词，以及带有-wo/manship后缀的词分别显示在附录中的表[6](#A1.T6
    "Table 6 ‣ Appendix A Appendix ‣ From Showgirls to Performers: Fine-tuning with
    Gender-inclusive Language for Bias Reduction in LLMs")和[7](#A1.T7 "Table 7 ‣ Appendix
    A Appendix ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language
    for Bias Reduction in LLMs")中。'
- en: Following the BabelNet verification, words were manually filtered in the third
    round to exclude words not related to gender (e.g. boycott, boyne), and proper
    names such as surnames or words related to pop culture (batgirl, rainman). Furthermore,
    terms that occurred with a feminine suffix (noblewoman) but did not have a masculine
    equivalent (nobleman) were added as their masculine variant to the list, because
    we treat gender-marking suffixes as exchangeable to mark a different gender. The
    third round left 353 singular affixed nouns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据BabelNet验证，词汇在第三轮中被手动筛选，以排除与性别无关的词（例如boycott, boyne），以及专有名词如姓氏或流行文化相关的词（batgirl,
    rainman）。此外，出现了带有女性后缀（noblewoman）但没有男性等价词（nobleman）的术语也被作为其男性变体添加到列表中，因为我们将性别标记后缀视为可互换的标记不同性别。第三轮留下了353个单数附加名词。
- en: 4.1.1 Gender-neutral variants
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 性别中立变体
- en: Gender-neutral variants were manually compiled for all extracted words with
    gender-marking affixes. A single variant was added for all items in the list to
    simplify the replacement process. The final gender-neutral variants were discussed
    and agreed upon by the researchers. The proposed replacements are not intended
    to be definitive substitutes for their gender-marked counterparts. Instead, they
    were developed for the present experiments to provide gender-neutral terms, as
    no official list exists.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有提取出的带有性别标记后缀的词汇，手动编制了性别中立变体。列表中的所有项目都添加了一个单一变体，以简化替换过程。最终的性别中立变体由研究人员讨论并达成一致。提出的替换并非旨在成为其性别标记对应词的最终替代品。相反，它们是为了当前实验开发的，以提供性别中立的术语，因为不存在官方列表。
- en: Suffixes
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后缀
- en: 'Some gender-marking suffix could simply be exchanged for one that is gender
    neutral, such as in the common neutralisation of chair-man/-woman to chairperson.
    However, this simple replacement does not always work. For example, some frequent
    terms already have gender-neutral replacements such as fire fighter for fireman
    or police officer for policeman. In these cases, *fireperson or *policeperson
    would be ungrammatical⁴⁴4As per linguistic convention we mark ungrammatical terms
    with a leading asterisk (*).. A similar case can be made for less frequent words
    for which more elegant solutions are available than simply replacing -man/-woman
    with -person. One approach is to find more fitting suffixes or compound nouns,
    such as in the neutralisation of crewman with crew member. Another approach is
    to replace a word with a gender-neutral synonym, such as in the replacement of
    hitman with assassin. A third approach applies to words containing a verb as their
    root, such as the word huntsman, which has the root hunt. Here, the word can be
    replaced by a nominalization: hunter.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一些性别标记后缀可以简单地替换为性别中立的，例如将 chair-man/-woman 中立化为 chairperson。然而，这种简单的替换并不总是有效。例如，一些常见术语已经有性别中立的替换词，如
    fire fighter 替换 fireman 或 police officer 替换 policeman。在这些情况下，*fireperson 或 *policeperson
    会是不合语法的⁴⁴4根据语言学惯例，我们用前缀星号 (*) 标记不合语法的术语。对于不太常见的词，存在比简单地将 -man/-woman 替换为 -person
    更优雅的解决方案。一种方法是寻找更合适的后缀或复合名词，例如将 crewman 替换为 crew member。另一种方法是用性别中立的同义词替换词汇，例如将
    hitman 替换为 assassin。第三种方法适用于包含动词根的词汇，如 huntsman，其根词为 hunt。在这种情况下，可以用名词化形式：hunter。
- en: Prefixes
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前缀
- en: In the case of words with gender-marking prefixes, gender-neutral variants can
    be constructed by removing the prefix. For example, the word man-crush can be
    neutralised to crush.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带有性别标记前缀的词，可以通过去除前缀来构建性别中立的变体。例如，单词 man-crush 可以中立化为 crush。
- en: 'Once the list of singular word pairs was fixed, the plural version of every
    word-pair was added to the final list. The plurals were obtained using the inflect
    library in Python (version 7.0.0). After adding plurals, we performed one last
    round of manual verification to ensure all plurals were formed correctly. The
    final list contains 692 term pairs. For comparison, Vanmassenhove et al. ([2021](#bib.bib49))
    used a list of 91 term pairs. A sample of our final list can be found in Table
    [8](#A1.T8 "Table 8 ‣ Appendix A Appendix ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs") in the Appendix.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦确定了单数词对的列表，就将每个词对的复数版本添加到最终列表中。复数形式使用 Python 的 inflect 库（版本 7.0.0）获得。添加复数后，我们进行了最后一轮手动验证，以确保所有复数形式都正确形成。最终列表包含
    692 对术语。作为比较，Vanmassenhove 等人 ([2021](#bib.bib49)) 使用了 91 对术语。我们的最终列表的样本可以在附录中的表
    [8](#A1.T8 "Table 8 ‣ Appendix A Appendix ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs") 中找到。'
- en: 4.2 Fine-Tuning Data
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 微调数据
- en: '|  |  | Heap | Small Heap | Tiny Heap |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 堆积 | 小堆积 | 微小堆积 |'
- en: '| dataset | original weight | # tokens |  |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 原始权重 | # 令牌 |  |  |'
- en: '| OWT2 | 50% | 125M | 25M | 162k |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| OWT2 | 50% | 125M | 25M | 162k |'
- en: '| CC-News | 30% | 75M | 15M | 240k |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| CC-News | 30% | 75M | 15M | 240k |'
- en: '| English Wikipedia | 20% | 50M | 10M | 112k |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 英文维基百科 | 20% | 50M | 10M | 112k |'
- en: '| TOTAL | 100% | 250M | 50M | 514k |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 100% | 250M | 50M | 514k |'
- en: 'Table 3: Composition of Heap corpora; OWT2 = OpenWebText2, CC-News = Common
    Crawl News'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：堆积语料库的组成；OWT2 = OpenWebText2，CC-News = Common Crawl News
- en: '| original sentence | He told newsmen at the scene that unknown criminals vandalised
    MD metres and armoured cables of the transformer. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 原始句子 | 他告诉现场的新闻工作者，不明犯罪分子破坏了变压器的 MD 米和装甲电缆。 |'
- en: '| after word replacement | He told reporters at the scene that unknown criminals
    vandalised MD metres and armoured cables of the transformer. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 替换后 | 他告诉现场的记者，不明犯罪分子破坏了变压器的 MD 米和装甲电缆。 |'
- en: '| after rewriting and word replacement | They told reporters at the scene that
    unknown criminals vandalised MD metres and armoured cables of the transformer.
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 重写和单词替换后 | 他们告诉现场的记者，不明犯罪分子破坏了变压器的 MD 米和装甲电缆。 |'
- en: 'Table 4: Example of sentences in fine-tuning data at different stages of gender-neutral
    rewriting and replacement'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同阶段性别中立重写和替换的数据示例句子
- en: 'To create a fine-tuning corpus with gender-neutral interventions, we assembled
    a base corpus, which needed to have several features: (1) The configuration should
    be similar to current LLM pre-training data, meaning that it should contain a
    diverse set of sources. However, we excluded data that was too domain-specific,
    such as code and scientific publications in order to demonstrate methodology for
    general-purpose English. In the same line of reasoning, (2) the corpus should
    only contain English data, because the focus of this work is English, and the
    NeuTral Rewriter (Vanmassenhove et al., [2021](#bib.bib49)), which replaces gendered
    pronouns with singular they does also only exist for English. (3) Finally, since
    we do not aim to worsen the performance of the LLM through fine-tuning, the corpus
    should only include high-quality text.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个具有性别中立干预的微调语料库，我们组装了一个基础语料库，该语料库需要具备几个特点：（1）配置应类似于当前LLM预训练数据，即应包含多样化的来源。然而，我们排除了过于领域特定的数据，如代码和科学出版物，以展示一般用途英语的方法论。在同一推理下，（2）语料库应仅包含英语数据，因为这项工作的重点是英语，而NeuTral
    Rewriter（Vanmassenhove et al., [2021](#bib.bib49)）也仅存在于英语中。（3）最后，由于我们不希望通过微调来恶化LLM的性能，因此语料库应仅包括高质量的文本。
- en: 'The final composition of our base corpus was inspired by the composition of
    GPT-3’s training data (Brown et al., [2020](#bib.bib11)) as well as The Pile corpus (Gao
    et al., [2020](#bib.bib17)) and is shown in Table [3](#S4.T3 "Table 3 ‣ 4.2 Fine-Tuning
    Data ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive
    Language for Bias Reduction in LLMs"). Our original download has a size of 250
    million tokens, which is approximately 1.5 GB of data. Since this is substantially
    smaller than The Pile (825GB), we called our dataset The Heap. The dataset was
    downloaded using the Huggingface datasets library (version 1.18.3; [Wolf et al.](#bib.bib52), [2020](#bib.bib52))
    and tokenized with the stanza library (version 1.7.0; [Qi et al.](#bib.bib39), [2020](#bib.bib39)).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们基础语料库的最终组成受到了GPT-3训练数据（Brown et al., [2020](#bib.bib11)）以及The Pile语料库（Gao
    et al., [2020](#bib.bib17)）组成的启发，并在表[3](#S4.T3 "Table 3 ‣ 4.2 Fine-Tuning Data
    ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language
    for Bias Reduction in LLMs")中展示。我们原始下载的大小为2.5亿个标记，约1.5GB的数据。由于这远小于The Pile（825GB），我们将我们的数据集称为The
    Heap。数据集使用Huggingface datasets库（版本1.18.3；[Wolf et al.](#bib.bib52)，[2020](#bib.bib52)）下载，并使用stanza库（版本1.7.0；[Qi
    et al.](#bib.bib39)，[2020](#bib.bib39)）进行标记化。'
- en: 'The fine-tuning data were adjusted for gender-neutral wording in two rounds.
    Firstly, we used our own list of extracted affixed words combined with [Vanmassenhove
    et al.](#bib.bib49)’s ([2021](#bib.bib49)) list to replace sexist with gender-inclusive
    terms. Their list covers additional word pairs like stewardess–flight attendant
    or waitress–server. Words that were part of named entities were not replaced.
    Secondly, feminine and masculine singular pronouns (he, she, himself, etc.) were
    re-written into the respective variants of singular they using [Vanmassenhove
    et al.](#bib.bib49)’s ([2021](#bib.bib49)) NeuTral Rewriter. Table [4](#S4.T4
    "Table 4 ‣ 4.2 Fine-Tuning Data ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs") illustrates this re-writing
    process and provides an example sentence within the different variants of the
    corpus: normal, with replacements, and rewritten with replacements.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '微调数据在两轮中调整了性别中立的措辞。首先，我们使用了自己提取的附加词列表，并结合了[Vanmassenhove et al.](#bib.bib49)（[2021](#bib.bib49)）的列表，将带有性别偏见的术语替换为性别包容的术语。他们的列表涵盖了额外的词对，如空姐–航班服务员或女服务员–服务员。作为命名实体的一部分的词未被替换。其次，阴性和阳性单数代词（he,
    she, himself等）被改写成单数they的相应变体，使用了[Vanmassenhove et al.](#bib.bib49)（[2021](#bib.bib49)）的NeuTral
    Rewriter。表[4](#S4.T4 "Table 4 ‣ 4.2 Fine-Tuning Data ‣ 4 Method ‣ From Showgirls
    to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction in
    LLMs")展示了这一改写过程，并提供了不同变体中的示例句子：正常、带有替换和带有替换的改写。'
- en: 'We then reduced the final dataset, because fine-tuning a model with the entire
    250 million word corpus would have gone beyond computational resources available
    to us and good fine-tuning results can be achieved with considerably less data (Thakur
    et al., [2023](#bib.bib47); Zhou et al., [2023](#bib.bib53)). We first reduced
    the Heap corpus to a smaller dataset of 50 million tokens (the Small Heap, ~300MB),
    and finally only extracted lines containing word replacements. The composition
    of the final dataset, Tiny Heap, can be seen in Table [3](#S4.T3 "Table 3 ‣ 4.2
    Fine-Tuning Data ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive
    Language for Bias Reduction in LLMs").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '我们随后减少了最终数据集，因为用整个 2.5 亿字语料库进行模型微调超出了我们的计算资源，且用显著更少的数据也能获得良好的微调结果 (Thakur et
    al., [2023](#bib.bib47); Zhou et al., [2023](#bib.bib53))。我们首先将 Heap 语料库减少到 5000
    万标记的小数据集 (Small Heap，约 300MB)，最终仅提取了包含单词替换的行。最终数据集 Tiny Heap 的组成可以在表 [3](#S4.T3
    "Table 3 ‣ 4.2 Fine-Tuning Data ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs") 中看到。'
- en: 4.3 Models and Fine-tuning
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型与微调
- en: 'We ran our experiments on three models: GPT-2 (Radford et al., [2019](#bib.bib40)),
    RoBERTa-large (Liu et al., [2019](#bib.bib26)) and PHI-1.5 (Li et al., [2023](#bib.bib25)).
    These models were chosen because they (1) cover both causal and masked language
    modelling architectures, (2) feature in previous research (GPT-2 and RoBERTa),
    and (3) have small parameter sizes and thus require less resources to fine-tune.
    Microsoft’s PHI-1.5 was chosen, because it reached one of the highest performances
    within the 1.5 billion parameter category of pre-trained models in Huggingface’s
    OpenLeaderboard⁵⁵5[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    at the time we conducted our experiments.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个模型上进行了实验：GPT-2 (Radford et al., [2019](#bib.bib40))、RoBERTa-large (Liu
    et al., [2019](#bib.bib26)) 和 PHI-1.5 (Li et al., [2023](#bib.bib25))。这些模型的选择是因为它们
    (1) 涵盖了因果和掩码语言建模架构，(2) 出现在以往的研究中 (GPT-2 和 RoBERTa)，以及 (3) 参数规模较小，因此需要较少的资源进行微调。微软的
    PHI-1.5 被选择是因为在我们进行实验时，它在 Huggingface 的 OpenLeaderboard⁵⁵5[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    的 15 亿参数类别中达到了最高性能之一。
- en: The models were fine-tuned for each one and three epochs (batch size 2) on an
    NVIDIA A100-SXM4-40GB GPU on Google Colaboratory, using 30 GPU hours in total
    for all models. The two fine-tuning datasets used were Tiny Heap with gender-neutral
    replacements (tiny-heap-rep) and gender-neutral replacements and rewriting with
    [Vanmassenhove et al.](#bib.bib49)’s ([2021](#bib.bib49)) NeuTral Rewriter (tiny-heap-rep-neutral).
    The learning rate was set to $2\mathrm{e}{-5}$ with a weight decay of 0.01\. We
    used the Trainer class of the Huggingface transformers library in python (version
    4.38.0.dev0; [Wolf et al.](#bib.bib52), [2020](#bib.bib52)) and kept all other
    hyperparameters at their default values.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在 Google Colaboratory 的 NVIDIA A100-SXM4-40GB GPU 上进行了每个模型三轮的微调 (批量大小 2)，所有模型共使用了
    30 个 GPU 小时。使用的两个微调数据集是 Tiny Heap 具有性别中立替换 (tiny-heap-rep) 和性别中立替换及 [Vanmassenhove
    et al.](#bib.bib49) ([2021](#bib.bib49)) 的 NeuTral Rewriter (tiny-heap-rep-neutral)。学习率设置为
    $2\mathrm{e}{-5}$，权重衰减为 0.01。我们使用了 Huggingface transformers 库中的 Trainer 类 (python
    版本 4.38.0.dev0; [Wolf et al.](#bib.bib52), [2020](#bib.bib52))，其余超参数保持默认值。
- en: 4.4 Bias Evaluation Metrics
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 偏差评估指标
- en: We utilise three established metrics for quantifying bias. CrowS-Pairs (Nangia
    et al., [2020](#bib.bib30)) and RedditBias (Barikeri et al., [2021](#bib.bib5))
    were selected because they are not based on artificial templates but are crowdsourced
    and extracted from naturally occurring data, respectively. The third benchmark,
    HONEST (Nozza et al., [2021](#bib.bib32), [2022](#bib.bib33)), was selected as
    an extrinsic metric because it relies on prompt completion. In addition to measuring
    bias along the binary male-female axis, both RedditBias and HONEST support gender
    bias evaluation in relation to LGBTQ+ (Lesbian, Gay, Binary, Trans and Queer or
    Questioning) terminology.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了三个已建立的偏差量化指标。CrowS-Pairs (Nangia et al., [2020](#bib.bib30)) 和 RedditBias
    (Barikeri et al., [2021](#bib.bib5)) 被选择是因为它们不基于人工模板，而是分别从众包和自然发生的数据中提取的。第三个基准
    HONEST (Nozza et al., [2021](#bib.bib32), [2022](#bib.bib33)) 被选为外在指标，因为它依赖于提示完成。除了沿二元男性-女性轴测量偏差外，RedditBias
    和 HONEST 都支持与 LGBTQ+（女同性恋、男同性恋、二元、跨性别和酷儿或疑问）术语相关的性别偏差评估。
- en: CrowS-Pairs
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CrowS-Pairs
- en: (Nangia et al., [2020](#bib.bib30)) is a benchmark comprised of crowdsourced
    minimal sentence pairs differing in words that are related to a variety of social
    categories, such as race, ability and gender. Since we are interested in gender
    bias, we run our experiments on only the gender-dimension of the dataset, which
    contains 262 sentence pairs. Each sentence pair consists of a stereotypical setting
    (Women don’t know how to drive.), and an anti-stereotypical setting (Men know
    how to drive.), in which the term referring to the relevant social group (gender
    in this case) is changed. We use [Meade et al.](#bib.bib28)’s ([2022](#bib.bib28))
    implementation of the sentence scoring, which measures the likelihood of the changed,
    instead of the unchanged, tokens within a sentence. The CrowS-Pairs metric measures
    the percentage of cases in which a model gives a higher likelihood to a more stereotypical
    or less anti-stereotypical sentence. The metric’s ideal value is 50, meaning that
    the model does not show a clear preference for stereotypical sentences.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (Nangia 等人，[2020](#bib.bib30)) 是一个基准数据集，由众包的最小句子对组成，这些句子对在与多种社会类别相关的词汇上有所不同，如种族、能力和性别。由于我们关注性别偏见，我们只对数据集中的性别维度进行实验，该维度包含
    262 对句子。每对句子包括一个刻板印象设置（女性不知道如何开车。），以及一个反刻板印象设置（男性知道如何开车。），其中涉及相关社会群体（在这种情况下为性别）的术语被更改。我们使用[Meade
    等人](#bib.bib28)（[2022](#bib.bib28)）的句子评分实现，该实现测量句子中更改过的代币的可能性，而不是未更改的代币。CrowS-Pairs
    指标测量模型给出更高可能性的刻板印象更强或反刻板印象更弱的句子的百分比。该指标的理想值是 50，意味着模型对刻板印象句子没有明显偏好。
- en: RedditBias
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RedditBias
- en: '(Barikeri et al., [2021](#bib.bib5)) also contains minimal sentence pairs expressing
    stereotypes for different demographic dimensions: religion, race, gender and queerness.
    Due to our focus on gender, we only calculate scores for the gender and queerness
    dimensions, which contain 253 and 235 sentence pairs, respectively. The sentences
    in RedditBias were extracted from the Reddit social network forum and contain
    both a target term identifying a social demographic (boldface in Example [4.4](#S4.SS4.SSS0.Px2
    "RedditBias ‣ 4.4 Bias Evaluation Metrics ‣ 4 Method ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs")) as well
    as an attribute term that expresses a (negative) stereotype related to the group
    (underlined in Example [4.4](#S4.SS4.SSS0.Px2 "RedditBias ‣ 4.4 Bias Evaluation
    Metrics ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive
    Language for Bias Reduction in LLMs")). \eenumsentence'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '(Barikeri 等人，[2021](#bib.bib5)) 还包含表达不同人口统计维度的刻板印象的最小句子对：宗教、种族、性别和酷儿性。由于我们关注性别，我们只计算性别和酷儿维度的分数，这两个维度分别包含
    253 和 235 对句子。RedditBias 中的句子提取自 Reddit 社交网络论坛，包含一个识别社会人口统计特征的目标术语（在示例 [4.4](#S4.SS4.SSS0.Px2
    "RedditBias ‣ 4.4 Bias Evaluation Metrics ‣ 4 Method ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs") 中加粗）以及一个表达（负面）刻板印象的属性术语（在示例
    [4.4](#S4.SS4.SSS0.Px2 "RedditBias ‣ 4.4 Bias Evaluation Metrics ‣ 4 Method ‣
    From Showgirls to Performers: Fine-tuning with Gender-inclusive Language for Bias
    Reduction in LLMs") 中下划线）。'
- en: his (mom | dad) is seriously devoted to cleaning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 他的（妈妈 | 爸爸）非常专注于清洁。
- en: those (gays | straights) are so dramatic.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那些（同性恋 | 异性恋）太戏剧化了。
- en: Stereotyping in a model is quantified through calculating the perplexity of
    the model for the sentence pairs and performing the student’s t-test on the perplexity
    pairs. Negative values of $t$ indicates statistical significance of the perplexity
    differences.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的刻板印象通过计算句子对的困惑度并对困惑度对进行学生t检验来量化。$t$的负值表示困惑度差异的统计显著性。
- en: HONEST
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 诚实
- en: 'differs from the first two measures in that it does not measure gender stereotyping
    but the presence of hurtful language in LLM sentence completions. The original
    HONEST benchmark consists of prompts containing binary masculine and feminine
    terms (Nozza et al., [2021](#bib.bib32); Example [4.4](#S4.SS4.SSS0.Px3 "HONEST
    ‣ 4.4 Bias Evaluation Metrics ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning
    with Gender-inclusive Language for Bias Reduction in LLMs")). This was later extended
    with prompts containing LGBTQ+ terms (Nozza et al., [2022](#bib.bib33); Example
    [4.4](#S4.SS4.SSS0.Px3 "HONEST ‣ 4.4 Bias Evaluation Metrics ‣ 4 Method ‣ From
    Showgirls to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction
    in LLMs")). \eenumsentence'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与前两项测量不同，它不测量性别刻板印象，而是测量 LLM 句子补全中的伤害性语言。原始的 HONEST 基准包括包含二元男性和女性术语的提示（Nozza
    等人，[2021](#bib.bib32)；示例 [4.4](#S4.SS4.SSS0.Px3 "HONEST ‣ 4.4 偏见评估指标 ‣ 4 方法 ‣
    从脱衣舞者到表演者：使用性别包容语言微调以减少 LLMs 中的偏见")）。后来扩展为包含 LGBTQ+ 术语的提示（Nozza 等人，[2022](#bib.bib33)；示例
    [4.4](#S4.SS4.SSS0.Px3 "HONEST ‣ 4.4 偏见评估指标 ‣ 4 方法 ‣ 从脱衣舞者到表演者：使用性别包容语言微调以减少 LLMs
    中的偏见")）。\eenumsentence
- en: the sister should work as a          .
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 姐妹应该担任          。
- en: The transwoman was hired as a          . HONEST uses the HurtLex lexicon of
    harmful language (Bassignana et al., [2018](#bib.bib6)) to measure the hurtfulness
    of words contained sentence completions. HurtLex provides a classification of
    hurtful language into nine categories such as animals or derogatory words. The
    HONEST score is calculated for each of these categories and subsequently averaged
    into a global score that represents the percentage of overall hurtful completions.
    An ideal model that does not generate hurtful output will therefore have a score
    of zero. For our experiments, we used $k=20$ completions for PHI-1.5 in order
    to shorten the runs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 变性女性被聘为          。HONEST 使用 HurtLex 词汇表来测量句子补全中词汇的伤害性（Bassignana 等人，[2018](#bib.bib6)）。HurtLex
    将伤害性语言分类为九类，如动物或贬损性词汇。HONEST 分数是为每一类计算的，随后平均为一个全球分数，代表整体伤害性补全的百分比。因此，理想的模型不会生成伤害性输出，分数应为零。在我们的实验中，我们使用了
    $k=20$ 次补全以缩短运行时间。
- en: 5 Results and Discussion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果与讨论
- en: '| model | epochs | FT | RedditBias |  | CrowsPairs (in%) |  |  | HONEST |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练轮次 | FT | RedditBias |  | CrowsPairs（%） |  |  | HONEST |  |'
- en: '|  |  |  | t[gender] | t[queerness] | metric | stereo | anti-st. | binary |
    queer |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | t[性别] | t[酷儿性] | 指标 | 刻板印象 | 反刻板印象 | 二元 | 酷儿 |'
- en: '| GPT-2 | 0 | baseline | -1.28 | -1.65 | 56.87 | 53.46 | 62.14 | 0.140 | 0.146
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 0 | 基线 | -1.28 | -1.65 | 56.87 | 53.46 | 62.14 | 0.140 | 0.146 |'
- en: '|  | 1 | replacement | -2.01* | -0.39 | 54.96 | 51.57 | 60.19 | 0.101 | 0.112
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 1 | replacement | -2.01* | -0.39 | 54.96 | 51.57 | 60.19 | 0.101 | 0.112
    |'
- en: '|  |  | rep+neutral | -0.77 | -0.69 | 54.96 | 58.94 | 49.51 | 0.107 | 0.119
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | rep+neutral | -0.77 | -0.69 | 54.96 | 58.94 | 49.51 | 0.107 | 0.119
    |'
- en: '|  | 3 | replacement | -1.54 | -0.81 | 54.58 | 49.69 | 62.14 | 0.110 | 0.120
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | replacement | -1.54 | -0.81 | 54.58 | 49.69 | 62.14 | 0.110 | 0.120
    |'
- en: '|  |  | rep+neutral | -1.54 | -1.09 | 54.2 | 56.60 | 50.49 | 0.124 | 0.126
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | rep+neutral | -1.54 | -1.09 | 54.2 | 56.60 | 50.49 | 0.124 | 0.126
    |'
- en: '| PHI-1.5 | 0 | baseline | -1.83 | -0.34 | 55.73 | 62.26 | 45.63 | 0.079 |
    0.142 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| PHI-1.5 | 0 | 基线 | -1.83 | -0.34 | 55.73 | 62.26 | 45.63 | 0.079 | 0.142
    |'
- en: '|  | 1 | replacement | -2.06* | -2.32* | 51.15 | 51.57 | 50.49 | 0.109 | 0.114
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 1 | replacement | -2.06* | -2.32* | 51.15 | 51.57 | 50.49 | 0.109 | 0.114
    |'
- en: '|  |  | rep+neutral | -2.26* | -2.42* | 50.76 | 55.35 | 43.69 | 0.123 | 0.154
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |  | rep+neutral | -2.26* | -2.42* | 50.76 | 55.35 | 43.69 | 0.123 | 0.154
    |'
- en: '|  | 3 | replacement | -2.72* | -2.87* | 51.91 | 53.46 | 49.51 | 0.084 | 0.135
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | replacement | -2.72* | -2.87* | 51.91 | 53.46 | 49.51 | 0.084 | 0.135
    |'
- en: '|  | 3 | rep+neutral | -2.71* | -2.16 | 51.91 | 55.97 | 45.63 | 0.093 | 0.129
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | rep+neutral | -2.71* | -2.16 | 51.91 | 55.97 | 45.63 | 0.093 | 0.129
    |'
- en: '| RoBERTa | 0 | baseline | -0.50 | 1.50 | 60.15 | 72.15 | 42.16 | 0.035 | 0.05
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 0 | 基线 | -0.50 | 1.50 | 60.15 | 72.15 | 42.16 | 0.035 | 0.05 |'
- en: '|  | 1 | replacement | -0.56 | 1.42 | 50.19 | 58.23 | 38.24 | 0.044 | 0.066
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 1 | replacement | -0.56 | 1.42 | 50.19 | 58.23 | 38.24 | 0.044 | 0.066
    |'
- en: '|  |  | rep+neutral | -2.62* | -0.06 | 56.32 | 62.26 | 46.06 | 0.040 | 0.054
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |  | rep+neutral | -2.62* | -0.06 | 56.32 | 62.26 | 46.06 | 0.040 | 0.054
    |'
- en: '|  | 3 | replacement | -1.61 | 0.47 | 52.87 | 60.38 | 41.18 | 0.012 | 0.035
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 3 | replacement | -1.61 | 0.47 | 52.87 | 60.38 | 41.18 | 0.012 | 0.035
    |'
- en: '|  |  | rep+neutral | 0.22 | 2.18* | 49.04 | 54.72 | 40.20 | 0.028 | 0.041
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |  | rep+neutral | 0.22 | 2.18* | 49.04 | 54.72 | 40.20 | 0.028 | 0.041
    |'
- en: 'Table 5: Gender-stereotyping (RedditBias, CrowsPairs) and hurtful language
    generation (HONEST) results for different interventions to fine-tuning (FT) data,
    divided by baseline model, one, and three epochs of fine-tuning; RedditBias results
    marked * significant with $p<0.05$. rep+neutral = gender-neutral replacements
    + neutral rewriting; anti-st = anti-stereotypical setting'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同干预措施对微调（FT）数据的性别刻板印象（RedditBias, CrowsPairs）和伤害性语言生成（HONEST）结果，按基线模型、一个和三个微调周期划分；RedditBias结果标记为*显著，$p<0.05$。rep+neutral
    = 性别中立替换 + 中立改写；anti-st = 反刻板印象设置
- en: 5.1 Gender-marking affixes
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 性别标记词缀
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.1 Word Catalogue ‣ 4 Method ‣ From Showgirls
    to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction in
    LLMs") illustrates the number of affixed word extractions for three rounds of
    verification. This process of finding words with gender-exclusive affixes also
    serves as a frequency analysis of the distribution of gender-marking words within
    English text. Overall, it can be clearly seen in Table [1](#S4.T1 "Table 1 ‣ 4.1
    Word Catalogue ‣ 4 Method ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive
    Language for Bias Reduction in LLMs") that gender-marking through suffixation
    is more common than prefixation. Regarding the distribution of gender, more words
    with masculine than feminine affixes were extracted. In fact, of all gender-marking
    affixes within our final catalogue, feminine affixes only make up roughly one
    fifth. This skewed distribution demonstrates a tendency within English text to
    over-represent masculine gender. This over-representation could be one of the
    origins of gender bias towards masculine forms in LLMs. Our generated list of
    words with gendered affixes can be used in future research to analyze the distributions
    of gendered words within NLP training and fine-tuning corpora to get a better
    insight into how gender distributions in the training data might affect representations
    of gender in downstream models.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S4.T1 "表 1 ‣ 4.1 词汇目录 ‣ 4 方法 ‣ 从舞女到表演者：使用性别包容语言进行微调以减少LLMs中的偏见")展示了三轮验证中附加词提取的数量。这个寻找具有性别排他性词缀的过程也作为对英语文本中性别标记词分布的频率分析。总的来说，可以在表[1](#S4.T1
    "表 1 ‣ 4.1 词汇目录 ‣ 4 方法 ‣ 从舞女到表演者：使用性别包容语言进行微调以减少LLMs中的偏见")中清晰地看到，通过后缀标记性别的情况比前缀标记更为常见。关于性别分布，提取到的带有男性词缀的词汇比女性词缀的更多。实际上，在我们最终目录中的所有性别标记词缀中，女性词缀仅占大约五分之一。这种不均衡的分布显示了英语文本中男性性别的过度表现。这种过度表现可能是LLMs中对男性形式性别偏见的来源之一。我们生成的带有性别词缀的词汇列表可以在未来的研究中用于分析NLP训练和微调语料库中的性别词汇分布，以更好地了解训练数据中的性别分布如何影响下游模型中的性别表现。
- en: 5.2 Fine-tuning
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 微调
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs") shows
    how fine-tuning impacted three different bias metrics for the three LLMs we tested.
    Each model was fine-tuned for one and three epochs, using fine-tuning data with
    gender-exclusive replaced by gender-neutral wording using our own gender-neutral
    catalogue (cf. Section [4.1](#S4.SS1 "4.1 Word Catalogue ‣ 4 Method ‣ From Showgirls
    to Performers: Fine-tuning with Gender-inclusive Language for Bias Reduction in
    LLMs")) as well as [Vanmassenhove et al.](#bib.bib49)’s ([2021](#bib.bib49)) list
    (replacement). In addition, gender-neutral rewriting (Vanmassenhove et al., [2021](#bib.bib49))
    was performed on the fine-tuning data (rep+neutral).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#S5.T5 "表 5 ‣ 5 结果与讨论 ‣ 从舞女到表演者：使用性别包容语言进行微调以减少LLMs中的偏见")展示了微调对我们测试的三种LLMs的三种不同偏见指标的影响。每个模型在一个和三个训练周期内进行微调，使用的微调数据中性别排他性用词被我们的性别中立目录（参见第[4.1](#S4.SS1
    "4.1 词汇目录 ‣ 4 方法 ‣ 从舞女到表演者：使用性别包容语言进行微调以减少LLMs中的偏见")节）中的性别中立用词所替代，以及[Vanmassenhove等](#bib.bib49)（[2021](#bib.bib49)）的列表（替换）。此外，还对微调数据进行了性别中立改写（Vanmassenhove等，[2021](#bib.bib49)）（rep+neutral）。
- en: For RedditBias (Barikeri et al., [2021](#bib.bib5)), we report the values of
    the $t$) by fine-tuning in the case of GPT-2 and RoBERTa. We reach the least binary
    gender bias when fine-tuning with data that contains both gender-neutral pronouns
    and gender-neutral replacements for one epoch for GPT-2 and three epochs for RoBERTa.
    Fine-tuning PHI-1.5 achieves opposite results, increasing the binary bias metric.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RedditBias (Barikeri et al., [2021](#bib.bib5))，我们报告了 GPT-2 和 RoBERTa 在微调情况下的
    $t$ 值。我们在对 GPT-2 进行一个周期的微调和对 RoBERTa 进行三个周期的微调时，达到了最小的二元性别偏见。微调 PHI-1.5 得到相反的结果，增加了二元偏见指标。
- en: Measuring queerness bias, GPT-2 exhibits the most stereotypical bias, followed
    by PHI-1.5, which shows a low negative value of $t_{queerness}$ (1.5). Fine-tuning
    again has positive effects for both GPT-2 and RoBERTa, but exacerbates bias for
    PHI-1.5\. Again, GPT-2 shows bias decreases after one epoch, while RoBERTa’s best
    results are achieved after three epochs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量性取向偏见时，GPT-2 展示了最刻板的偏见，其次是 PHI-1.5，后者显示了较低的 $t_{queerness}$ 值（1.5）。微调对 GPT-2
    和 RoBERTa 都有积极的效果，但加剧了 PHI-1.5 的偏见。再次，GPT-2 在一个周期后显示偏见减少，而 RoBERTa 的最佳结果是在三个周期后获得的。
- en: 'For CrowS-Pairs (Nangia et al., [2020](#bib.bib30)), we report the percentage
    of cases in which a model assigns higher likelihood to gendered target terms within
    a sentence expressing a stereotype (‘stereo’ column in Table [5](#S5.T5 "Table
    5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers: Fine-tuning with
    Gender-inclusive Language for Bias Reduction in LLMs")) or a lower probability
    to target terms in sentences expressing an anti-stereotype (‘anti-st.’ column
    in Table [5](#S5.T5 "Table 5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs")). The
    ‘metric’ column contains the overall stereotype score. For all three LLMs, the
    overall CrowS-Pairs metric shows a reduction in gender stereotyping, i.e. results
    that are lower than the baseline and approach a value of 50%. This result is mostly
    in line or goes beyond of what Thakur et al. ([2023](#bib.bib47)) reported for
    their methods of fine-tuning with gender-inclusive text; they showed a maximum
    reduction of the CrowS-Pairs score of approximately 2.7% for RoBERTa-base. Our
    RoBERTa-large model trained for 3 epochs on data with gender-neutral pronouns
    and replacements shows the largest reduction (difference of 11%) to a value even
    less than the ideal of 50 percent likelihood of preferring a stereotyped sentence.
    GPT-2 shows the best result (54.2%) for this setting as well, while PHI shows
    the best results for fine-tuning only one epoch. Moreover, for GPT-2 there is
    a tendency for fine-tuning in the replacement setting to lower the stereotype
    score, while the replacement+neutral setting lowers the anti-stereotype score.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 CrowS-Pairs (Nangia et al., [2020](#bib.bib30))，我们报告了模型在表达刻板印象的句子中（‘stereo’
    列在表 [5](#S5.T5 "Table 5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs")）将更高的可能性分配给性别目标词的情况百分比，或者在表达反刻板印象的句子中（‘anti-st.’
    列在表 [5](#S5.T5 "Table 5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers:
    Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs")）将较低的可能性分配给目标词的情况百分比。‘metric’
    列包含总体刻板印象评分。对于所有三个 LLM，整体 CrowS-Pairs 指标显示性别刻板印象的减少，即结果低于基线，接近 50% 的值。这个结果大致与
    Thakur et al. ([2023](#bib.bib47)) 对他们使用性别包容文本的微调方法所报告的结果一致或超出；他们显示 RoBERTa-base
    的 CrowS-Pairs 评分最大减少约 2.7%。我们对 RoBERTa-large 模型在具有性别中性代词和替换的数据上训练 3 个周期显示出最大的减少（差异
    11%），接近低于 50% 的理想值。GPT-2 在此设置下也显示了最佳结果（54.2%），而 PHI 在仅微调一个周期的情况下表现最好。此外，对于 GPT-2，替换设置有降低刻板印象评分的趋势，而替换+中性设置降低了反刻板印象评分。'
- en: 'The HONEST scores contain the percentage of sentence completions for sentences
    containing a term referring to binary or queer gender were completed with hurtful
    language. The two baseline causal LLMs GPT-2 and PHI-1.5 generate hurtful sentence
    completions around 15% of the time in the queer setting, while RoBERTa has a much
    lower starting point with only 5% hurtful completions. Table [5](#S5.T5 "Table
    5 ‣ 5 Results and Discussion ‣ From Showgirls to Performers: Fine-tuning with
    Gender-inclusive Language for Bias Reduction in LLMs") shows that our method of
    fine-tuning language models can be used to reduce the number of hurtful completions.
    All models show that best results are achieved when fine-tuning on data with only
    gender-neutral replacements in both queer and binary setting. However, depending
    on the model and the setting (binary vs. queer), the best results are either achieved
    for one or three epochs of fine/tuning. Similar to results for RedditBias, our
    method could not reduce the HONEST score for PHI-1.5 in the binary setting.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'HONEST 分数包含了含有指代二元性别或酷儿性别术语的句子中，有害语言的完成百分比。两个基线因果 LLMs GPT-2 和 PHI-1.5 在酷儿设置下生成有害句子的完成率大约为
    15%，而 RoBERTa 的起点则低得多，仅为 5% 的有害完成。表 [5](#S5.T5 "Table 5 ‣ 5 Results and Discussion
    ‣ From Showgirls to Performers: Fine-tuning with Gender-inclusive Language for
    Bias Reduction in LLMs") 显示了我们微调语言模型的方法可以用来减少有害完成的数量。所有模型显示，最佳结果是在仅用性别中立替换的微调数据上进行微调时获得的，无论是酷儿设置还是二元设置。然而，根据模型和设置（双性别
    vs. 酷儿），最佳结果要么在微调/训练一轮，要么在三轮时获得。与 RedditBias 的结果类似，我们的方法无法在二元设置下减少 PHI-1.5 的 HONEST
    分数。'
- en: Overall, our results echo those of  Aribandi et al. ([2021](#bib.bib2)) who
    found that bias metrics within the NLP literature often do not correlate. While
    we could demonstrate a reduction in stereotyping as measured by CrowS-Pairs as
    well as a reduction in the generation of hurtful language, the RedditBias metric
    did not show a bias reduction for all models. Moreover, the fact that different
    models proved to be susceptible to bias reduction in different settings, such
    as level of gender-neutralisation in fine-tuning data or number of fine-tuning
    epochs, additionally shows that model specifications such as architecture and
    model size need to be taken into account when choosing a bias mitigation strategy.
    For instance, RoBERTa generally shows a larger bias reduction when fine-tuning
    for three epochs, while the best number of epochs for PHI-1.5 and GPT-2 depends
    on the fine-tuning data. Furthermore, we demonstrated that a newer model, PHI-1.5 (Li
    et al., [2023](#bib.bib25)), which was released in 2023 as opposed to RoBERTa (Liu
    et al., [2019](#bib.bib26)) and GPT-2 (Radford et al., [2019](#bib.bib40)) in
    2019, was less susceptible to gender bias reduction through fine-tuning. However,
    the baseline PHI-1.5 did not necessarily tend to exhibit less stereotyping or
    hurtful language generation than the older models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的结果与 Aribandi 等人（[2021](#bib.bib2)）的研究相呼应，他们发现 NLP 文献中的偏见指标往往不相关。尽管我们能够展示出通过
    CrowS-Pairs 测量的刻板印象减少以及有害语言生成的减少，但 RedditBias 指标并未对所有模型显示出偏见的减少。此外，不同模型在不同设置下对偏见减少的敏感性，例如在微调数据中的性别中立程度或微调轮数，进一步表明在选择偏见缓解策略时需要考虑模型的具体规格，如架构和模型大小。例如，RoBERTa
    在微调三轮时通常显示出较大的偏见减少，而 PHI-1.5 和 GPT-2 的最佳轮数则依赖于微调数据。此外，我们还展示了一个更新的模型 PHI-1.5（Li
    等人，[2023](#bib.bib25)），该模型于 2023 年发布，而 RoBERTa（Liu 等人，[2019](#bib.bib26)）和 GPT-2（Radford
    等人，[2019](#bib.bib40)）则是在 2019 年发布，PHI-1.5 在通过微调减少性别偏见方面不那么敏感。然而，基线模型 PHI-1.5
    并不一定比旧模型更少表现出刻板印象或有害语言生成。
- en: 6 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Gender-inclusive language has a long history of development and advocacy within
    the field of feminist linguistics, but it has only recently entered gender bias
    research in NLP. This direction of interdisciplinary research is important, because
    not only do the linguistic structures used in LLM training data shape gender representations
    in the model, but the language generated by the model also has the potential to
    influence societal norms and cognitive patterns. In this paper, we presented a
    method of semi-automatically extracting gender-exclusive nouns based on the presence
    of gender-marking affixes. We then extended this list with gender-neutral variants,
    presenting a catalogue of 692 gender-exclusive vs. -inclusive pairs, which we
    make available for future research.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 性别包容语言在女性主义语言学领域有着悠久的发展和倡导历史，但它最近才进入了 NLP 中的性别偏见研究。这种跨学科的研究方向很重要，因为不仅 LLM 训练数据中使用的语言结构塑造了模型中的性别表征，而且模型生成的语言也有可能影响社会规范和认知模式。在这篇论文中，我们提出了一种基于性别标记词缀存在的半自动提取性别排斥名词的方法。然后，我们用性别中立的变体扩展了这个列表，提供了
    692 对性别排斥与性别包容的名词对，供未来研究使用。
- en: We further performed fine-tuning experiments on three LLMs. To create a fine-tuning
    corpus we used our catalogue to replace gender-exclusive with gender-neutral nouns.
    We also re-wrote gendered pronouns with the respective variants of singular they.
    Fine-tuning with gender-neutral data showed an overall reduction in gender stereotyping
    as measured by likelihood of gendered word generation in stereotyped settings,
    as well as a reduction in the generation of harmful language when prompted with
    sentences containing words related to binary gender as well as the LGBTQ+ community.
    However, we also showed that optimal bias reduction is dependent on model architecture
    and number of fine-tuning epochs, which need to be considered in deployment. We
    hope that our work will inspire further research into the effects of gender-inclusive
    terminology within large language models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对三种 LLM 进行了微调实验。为了创建微调语料库，我们使用我们的目录将性别排斥名词替换为性别中立名词。我们还将性别化代词重新编写为单数 they
    的相应变体。使用性别中立数据进行微调显示，性别刻板印象总体减少，通过在刻板环境中生成性别化词汇的可能性测量，以及在包含二元性别和 LGBTQ+ 社区相关词汇的句子中生成有害语言的减少。然而，我们也表明，最佳的偏见减少效果依赖于模型架构和微调轮次，这需要在部署时加以考虑。我们希望我们的工作能激发对性别包容术语在大型语言模型中效果的进一步研究。
- en: 7 Limitations
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: 'This study is limited by four main factors:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究受到四个主要因素的限制：
- en: Firstly, our study is limited to English specifically. We did not include other
    languages in this particular piece of research, because we wanted to pursue an
    approach tailored to English, targeting words and terms that have largely been
    overlooked but are still relevant to the aims of gender-fair language activism
    in this language. Therefore, the resources we developed and utilised, i.e. our
    catalogue of term-pairs, the Tiny Heap corpus, and [Vanmassenhove et al.](#bib.bib49)’s
    ([2021](#bib.bib49)) NeuTral Rewriter, are monolingual. Still, we hope that (parts
    of) our approach can be transferred to other languages, in which efforts at exploring
    the interplay of LLMs and feminist linguistic activism are undertaken and we are
    open for future collaborations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的研究仅限于英语。我们没有包括其他语言，因为我们希望采用一种专门针对英语的方法，关注那些在性别公平语言倡导中被忽视但仍然相关的词汇和术语。因此，我们开发和利用的资源，即我们的术语对目录、Tiny
    Heap 语料库，以及[Vanmassenhove 等人](#bib.bib49)（[2021](#bib.bib49)）的 NeuTral Rewriter，都是单语的。不过，我们希望我们的方法（部分）可以转移到其他语言中，进行大规模语言模型与女性主义语言倡导之间的交互探索，我们也欢迎未来的合作。
- en: 'Secondly, we performed naive replacements within our fine-tuning data: words
    found in our catalogue of gendered words were replaced with gender-neutral variants
    without regard for the sentence context. The only restriction posed was that the
    word not be part of a named entity. This might have created ungrammatical or nonsensical
    constructions, impacting the quality of the text and in turn model performance.
    Here, we come upon a trade-off between the quality of the generated text and the
    level of achievable automation. This is an important consideration when scaling
    up to larger amounts of data. Additionally, gender-exclusive terms were only replaced
    by a single neutral term; however, for some words several variations are possible,
    such as chairperson or chair for chairman/-woman. Managing this variation presents
    an interesting avenue for future research.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们在微调数据中进行了简单的替换：我们在性别化词汇表中的词语被替换为性别中立的变体，而不考虑句子的上下文。唯一的限制是这些词不能是命名实体的一部分。这可能会造成不符合语法或无意义的构造，影响文本质量，从而影响模型性能。在这里，我们面临生成文本质量和可实现的自动化水平之间的权衡。这在扩大数据量时是一个重要的考虑因素。此外，性别排他性术语只被替换为一个中性术语；然而，对于某些词语，可能有几种变体，例如“chairperson”或“chair”作为“chairman/-woman”的替代。管理这种变异为未来的研究提供了一个有趣的方向。
- en: Thirdly, there is an increasing number of bias metrics to measure gender bias,
    and a growing body of work critiquing them (Goldfarb-Tarrant et al., [2023](#bib.bib19);
    Orgad and Belinkov, [2022](#bib.bib34)). For example, Blodgett et al. ([2021](#bib.bib9))
    found several pitfalls in the CrowS-Pairs benchmark (Nangia et al., [2020](#bib.bib30)),
    which we used in this paper. This means that just because our metrics report a
    reduction in stereotyping in the models, it does not ensure a bias-free model
    but should rather be interpreted as a tendency toward decreased stereotyping.
    We tried to pick a diverse range of metrics to measure gender bias without relying
    solely on a binary conceptualisation of gender. However, our choice of metrics
    was also limited by ease of use and interpretation. Besides issues with the bias
    metrics themselves, future work could additionally explore whether our fine-tuning
    approach impacts the performance of the models on NLU tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，越来越多的偏差度量指标被用来测量性别偏差，同时也有越来越多的工作对这些指标提出批评（Goldfarb-Tarrant 等，[2023](#bib.bib19)；Orgad
    和 Belinkov，[2022](#bib.bib34)）。例如，Blodgett 等人（[2021](#bib.bib9)）发现 CrowS-Pairs
    基准（Nangia 等人，[2020](#bib.bib30)）存在几个问题，而我们在本文中使用了这个基准。这意味着，尽管我们的指标报告了模型中刻板印象的减少，但这并不能确保模型没有偏差，而应被解读为趋向于减少刻板印象。我们尝试选择多样化的指标来测量性别偏差，而不仅仅依赖于性别的二元概念。然而，我们选择的指标也受到易用性和解释性的限制。除了偏差指标本身的问题，未来的工作还可以进一步探讨我们的微调方法是否会影响模型在自然语言理解任务上的表现。
- en: Lastly, our study was limited to language models of relatively small size. The
    largest models we used (GPT-2 and PHI-1.5) each have 1.5 billion parameters, which
    is significantly smaller than for example the smallest (seven billion parameter)
    model in the Llama suite of LLMs (Touvron et al., [2023](#bib.bib48)), which reaches
    state-of-the-art performance using an open-source approach. We already demonstrated
    that the benefits of our approach differ based on the model used, which is why
    it would be interesting to see how fine-tuning with gender-neutral data impacts
    state-of-the-art models. However, our research institute does not have the resources
    to perform a study with models of state-of-the-art scale at the level of detail
    we provided here. Therefore, we leave experimentation with larger models to future
    research.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的研究局限于相对较小的语言模型。我们使用的最大模型（GPT-2 和 PHI-1.5）每个模型有 15 亿个参数，这显著小于例如 Llama 系列中最小的（70
    亿参数）模型（Touvron 等人，[2023](#bib.bib48)），后者通过开源方法达到最先进的性能。我们已经展示了我们的方法的好处会因所使用的模型不同而有所差异，这就是为什么了解微调性别中立数据如何影响最先进模型会很有趣。然而，我们的研究机构没有资源进行如此详细的最先进规模模型的研究。因此，我们将对更大模型的实验留给未来的研究。
- en: Acknowledgements
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We acknowledge the Research IT HPC Service at University College Dublin for
    providing computational facilities and support that contributed to the research
    results reported in this paper. This publication has emanated from research conducted
    with the financial support of Science Foundation Ireland under Grant number 12/RC/2289_P2\.
    For the purpose of Open Access, the authors have applied a CC BY public copyright
    licence to any Author Accepted Manuscript version arising from this submission.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢都柏林大学学院的研究IT HPC服务提供计算设施和支持，这对本文报告的研究结果做出了贡献。本出版物源于爱尔兰科学基金会资助的研究，资助编号12/RC/2289_P2。为了开放获取，作者对任何因本提交产生的作者接受稿版本申请了CC
    BY公共版权许可。
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'APA (2020) APA. 2020. *Publication Manual of the American Psychological Association:
    the Official Guide to Apa Style*, 7th edition. Book, Whole. American Psychological
    Association.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: APA (2020) APA. 2020. *《美国心理学会出版手册：APA风格官方指南》*，第7版。书籍，整体。美国心理学会。
- en: 'Aribandi et al. (2021) Vamsi Aribandi, Yi Tay, and Donald Metzler. 2021. [How
    Reliable are Model Diagnostics?](https://doi.org/10.18653/v1/2021.findings-acl.155)
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 1778–1785, Online. Association for Computational Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aribandi et al. (2021) Vamsi Aribandi, Yi Tay, 和 Donald Metzler. 2021. [《模型诊断有多可靠？》](https://doi.org/10.18653/v1/2021.findings-acl.155)
    载于*计算语言学协会年会论文集：ACL-IJCNLP 2021*，第1778–1785页，在线。计算语言学协会。
- en: Baker (2010a) Paul Baker. 2010a. [*Sociolinguistics and Corpus Linguistics*](http://ebookcentral.proquest.com/lib/ucd/detail.action?docID=536982).
    Edinburgh University Press, Edinburgh, UNITED KINGDOM.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker (2010a) Paul Baker. 2010a. [*《社会语言学与语料库语言学》*](http://ebookcentral.proquest.com/lib/ucd/detail.action?docID=536982)。爱丁堡大学出版社，爱丁堡，英国。
- en: Baker (2010b) Paul Baker. 2010b. [Will Ms ever be as frequent as Mr? A corpus-based
    comparison of gendered terms across four diachronic corpora of British English](https://doi.org/10.1558/genl.v4i1.125).
    *Gender and Language*, 4(1):125–149.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker (2010b) Paul Baker. 2010b. [《Ms会否变得像Mr一样频繁？基于语料库的英国英语四个历史语料库中的性别术语比较》](https://doi.org/10.1558/genl.v4i1.125)。*性别与语言*，4(1):125–149。
- en: 'Barikeri et al. (2021) Soumya Barikeri, Anne Lauscher, Ivan Vulić, and Goran
    Glavaš. 2021. [RedditBias: A Real-World Resource for Bias Evaluation and Debiasing
    of Conversational Language Models](https://doi.org/10.18653/v1/2021.acl-long.151).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 1941–1955, Online. Association for Computational
    Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barikeri et al. (2021) Soumya Barikeri, Anne Lauscher, Ivan Vulić, 和 Goran Glavaš.
    2021. [《RedditBias：用于评估和去偏见对话语言模型的现实世界资源》](https://doi.org/10.18653/v1/2021.acl-long.151)
    载于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，第1941–1955页，在线。计算语言学协会。
- en: 'Bassignana et al. (2018) Elisa Bassignana, Valerio Basile, and Viviana Patti.
    2018. [Hurtlex: A Multilingual Lexicon of Words to Hurt](https://doi.org/10.4000/books.aaccademia.3085).
    In *CEUR Workshop Proceedings*, volume 2253\. Accademia University Press.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bassignana et al. (2018) Elisa Bassignana, Valerio Basile, 和 Viviana Patti.
    2018. [《Hurtlex：伤害词汇的多语言词典》](https://doi.org/10.4000/books.aaccademia.3085)。载于*CEUR工作坊论文集*，第2253卷。Accademia
    University Press。
- en: 'Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. 2021. [On the dangers of stochastic parrots: Can language
    models be too big?](https://doi.org/10.1145/3442188.3445922) In *FAccT 2021 -
    Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency*,
    pages 610–623. Conference Proceedings.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major,
    和 Shmargaret Shmitchell. 2021. [《随机鹦鹉的危险：语言模型会不会太大？》](https://doi.org/10.1145/3442188.3445922)
    载于*FAccT 2021 - 2021年ACM公平性、问责制与透明度会议论文集*，第610–623页。会议论文集。
- en: 'Blodgett et al. (2020) Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna
    Wallach. 2020. Language (Technology) is Power: A Critical Survey of “Bias” in
    NLP. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5454–5476.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blodgett et al. (2020) Su Lin Blodgett, Solon Barocas, Hal Daumé III, 和 Hanna
    Wallach. 2020. 语言（技术）是力量：对NLP中“偏见”的批判性调查。载于*第58届计算语言学协会年会论文集*，第5454–5476页。
- en: 'Blodgett et al. (2021) Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
    Robert Sim, and Hanna Wallach. 2021. [Stereotyping Norwegian Salmon: An Inventory
    of Pitfalls in Fairness Benchmark Datasets](https://doi.org/10.18653/v1/2021.acl-long.81).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 1004–1015, Online. Association for Computational
    Linguistics.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blodgett 等人 (2021) **Su Lin Blodgett**, **Gilsinia Lopez**, **Alexandra Olteanu**,
    **Robert Sim** 和 **Hanna Wallach**。2021年。[刻板印象挪威三文鱼：公平性基准数据集中的陷阱清单](https://doi.org/10.18653/v1/2021.acl-long.81)。在
    *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，第1004–1015页，在线。计算语言学协会。
- en: Bommasani et al. (2022) Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
    Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky,
    Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy,
    Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan
    Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson,
    John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
    Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte
    Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi,
    Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,
    Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir
    Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak
    Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian
    Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech,
    Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda
    Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori
    Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori,
    Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun
    Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia,
    Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou,
    and Percy Liang. 2022. [On the Opportunities and Risks of Foundation Models](https://doi.org/10.48550/arXiv.2108.07258).
    ArXiv:2108.07258 [cs].
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani 等人 (2022) **Rishi Bommasani**, **Drew A. Hudson**, **Ehsan Adeli**,
    **Russ Altman**, **Simran Arora**, **Sydney von Arx**, **Michael S. Bernstein**,
    **Jeannette Bohg**, **Antoine Bosselut**, **Emma Brunskill**, **Erik Brynjolfsson**,
    **Shyamal Buch**, **Dallas Card**, **Rodrigo Castellon**, **Niladri Chatterji**,
    **Annie Chen**, **Kathleen Creel**, **Jared Quincy Davis**, **Dora Demszky**,
    **Chris Donahue**, **Moussa Doumbouya**, **Esin Durmus**, **Stefano Ermon**, **John
    Etchemendy**, **Kawin Ethayarajh**, **Li Fei-Fei**, **Chelsea Finn**, **Trevor
    Gale**, **Lauren Gillespie**, **Karan Goel**, **Noah Goodman**, **Shelby Grossman**,
    **Neel Guha**, **Tatsunori Hashimoto**, **Peter Henderson**, **John Hewitt**,
    **Daniel E. Ho**, **Jenny Hong**, **Kyle Hsu**, **Jing Huang**, **Thomas Icard**,
    **Saahil Jain**, **Dan Jurafsky**, **Pratyusha Kalluri**, **Siddharth Karamcheti**,
    **Geoff Keeling**, **Fereshte Khani**, **Omar Khattab**, **Pang Wei Koh**, **Mark
    Krass**, **Ranjay Krishna**, **Rohith Kuditipudi**, **Ananya Kumar**, **Faisal
    Ladhak**, **Mina Lee**, **Tony Lee**, **Jure Leskovec**, **Isabelle Levent**,
    **Xiang Lisa Li**, **Xuechen Li**, **Tengyu Ma**, **Ali Malik**, **Christopher
    D. Manning**, **Suvir Mirchandani**, **Eric Mitchell**, **Zanele Munyikwa**, **Suraj
    Nair**, **Avanika Narayan**, **Deepak Narayanan**, **Ben Newman**, **Allen Nie**,
    **Juan Carlos Niebles**, **Hamed Nilforoshan**, **Julian Nyarko**, **Giray Ogut**,
    **Laurel Orr**, **Isabel Papadimitriou**, **Joon Sung Park**, **Chris Piech**,
    **Eva Portelance**, **Christopher Potts**, **Aditi Raghunathan**, **Rob Reich**,
    **Hongyu Ren**, **Frieda Rong**, **Yusuf Roohani**, **Camilo Ruiz**, **Jack Ryan**,
    **Christopher Ré**, **Dorsa Sadigh**, **Shiori Sagawa**, **Keshav Santhanam**,
    **Andy Shih**, **Krishnan Srinivasan**, **Alex Tamkin**, **Rohan Taori**, **Armin
    W. Thomas**, **Florian Tramèr**, **Rose E. Wang**, **William Wang**, **Bohan Wu**,
    **Jiajun Wu**, **Yuhuai Wu**, **Sang Michael Xie**, **Michihiro Yasunaga**, **Jiaxuan
    You**, **Matei Zaharia**, **Michael Zhang**, **Tianyi Zhang**, **Xikun Zhang**,
    **Yuhui Zhang**, **Lucia Zheng**, **Kaitlyn Zhou** 和 **Percy Liang**。2022年。[基础模型的机会与风险](https://doi.org/10.48550/arXiv.2108.07258)。ArXiv:2108.07258
    [cs]。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165).
    ArXiv:2005.14165 [cs].
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) **Tom B. Brown**, **Benjamin Mann**, **Nick Ryder**, **Melanie
    Subbiah**, **Jared Kaplan**, **Prafulla Dhariwal**, **Arvind Neelakantan**, **Pranav
    Shyam**, **Girish Sastry**, **Amanda Askell**, **Sandhini Agarwal**, **Ariel Herbert-Voss**,
    **Gretchen Krueger**, **Tom Henighan**, **Rewon Child**, **Aditya Ramesh**, **Daniel
    M. Ziegler**, **Jeffrey Wu**, **Clemens Winter**, **Christopher Hesse**, **Mark
    Chen**, **Eric Sigler**, **Mateusz Litwin**, **Scott Gray**, **Benjamin Chess**,
    **Jack Clark**, **Christopher Berner**, **Sam McCandlish**, **Alec Radford**,
    **Ilya Sutskever** 和 **Dario Amodei**。2020年。[语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。ArXiv:2005.14165
    [cs]。
- en: 'Cao and Daumé (2021) Yang Trista Cao and Hal Daumé, III. 2021. [Toward Gender-Inclusive
    Coreference Resolution: An Analysis of Gender and Bias Throughout the Machine
    Learning Lifecycle*](https://doi.org/10.1162/coli_a_00413). *Computational Linguistics*,
    47(3):615–661.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao and Daumé (2021) Yang Trista Cao 和 Hal Daumé III。2021年。[《面向性别包容的核心指代解决：对机器学习生命周期中性别和偏见的分析》](https://doi.org/10.1162/coli_a_00413)。*《计算语言学》*，47(3)：615-661。
- en: Devinney et al. (2022) Hannah Devinney, Jenny Björklund, and Henrik Björklund.
    2022. Theories of "Gender" in NLP Bias Research. In *ACM FAccT Conference 2022,
    Conference on Fairness, Accountability, and Transparency, Hybrid via Seoul, Soth
    Korea, June 21-14, 2022*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devinney et al. (2022) Hannah Devinney, Jenny Björklund, 和 Henrik Björklund。2022年。《NLP
    偏见研究中的“性别”理论》。发表于 *ACM FAccT 2022 会议，公平性、问责性与透明性会议，混合模式，地点：首尔，韩国，2022年6月21-14日*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Lee Kenton, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of NAACL-HLT*, pages 4171–4186.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Lee Kenton, 和 Kristina Toutanova。2019年。BERT：用于语言理解的深度双向变换器的预训练。发表于
    *NAACL-HLT 会议论文集*，第4171-4186页。
- en: 'Fatemi et al. (2023) Zahra Fatemi, Chen Xing, Wenhao Liu, and Caimming Xiong.
    2023. [Improving gender fairness of pre-trained language models without catastrophic
    forgetting](https://doi.org/10.18653/v1/2023.acl-short.108). In *Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 1249–1262, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fatemi et al. (2023) Zahra Fatemi, Chen Xing, Wenhao Liu, 和 Caimming Xiong。2023年。[《在不发生灾难性遗忘的情况下改善预训练语言模型的性别公平性》](https://doi.org/10.18653/v1/2023.acl-short.108)。发表于
    *《第61届计算语言学协会年会论文集（第2卷：简短论文）》* 中，第1249-1262页，多伦多，加拿大。计算语言学协会。
- en: 'Frye (1983) Marilyn Frye. 1983. Sexism. *The politics of reality: Essays in
    feminist theory*, pages 17–40.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frye (1983) Marilyn Frye。1983年。《性别歧视》。收录在 *《现实的政治：女性主义理论中的论文》* 中，第17-40页。
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. 2020. [The Pile: An 800GB Dataset of Diverse Text for
    Language Modeling](https://doi.org/10.48550/arXiv.2101.00027). ArXiv:2101.00027
    [cs].'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, 和 Connor Leahy。2020年。[《The Pile：一个包含多样文本的800GB语言建模数据集》](https://doi.org/10.48550/arXiv.2101.00027)。ArXiv:2101.00027
    [cs]。
- en: 'Ghanbarzadeh et al. (2023) Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi,
    Radames Cruz Moreno, and Hamed Khanpour. 2023. [Gender-tuning: Empowering fine-tuning
    for debiasing pre-trained language models](https://doi.org/10.18653/v1/2023.findings-acl.336).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    5448–5458, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghanbarzadeh et al. (2023) Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames
    Cruz Moreno, 和 Hamed Khanpour。2023年。[《性别调整：赋能细化调整以去偏见预训练语言模型》](https://doi.org/10.18653/v1/2023.findings-acl.336)。发表于
    *《计算语言学协会发现：ACL 2023》* 中，第5448-5458页，多伦多，加拿大。计算语言学协会。
- en: 'Goldfarb-Tarrant et al. (2023) Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma
    Balkir, and Su Lin Blodgett. 2023. [This prompt is measuring \textlessmask\textgreater:
    evaluating bias evaluation in language models](https://doi.org/10.18653/v1/2023.findings-acl.139).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    2209–2225, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldfarb-Tarrant et al. (2023) Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma
    Balkir, 和 Su Lin Blodgett。2023年。[《这个提示正在测量 \textlessmask\textgreater：评估语言模型中的偏见评估》](https://doi.org/10.18653/v1/2023.findings-acl.139)。发表于
    *《计算语言学协会发现：ACL 2023》* 中，第2209-2225页，多伦多，加拿大。计算语言学协会。
- en: Gupta et al. (2023) Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, and
    Rebecca J. Passonneau. 2023. [Survey on Sociodemographic Bias in Natural Language
    Processing](https://doi.org/10.48550/arXiv.2306.08158). ArXiv:2306.08158 [cs].
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. (2023) Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, 和 Rebecca
    J. Passonneau。2023年。[《自然语言处理中的社会人口偏见调查》](https://doi.org/10.48550/arXiv.2306.08158)。ArXiv:2306.08158
    [cs]。
- en: Kramer (2016) Elise Kramer. 2016. [Feminist Linguistics and Linguistic Feminisms](https://go.exlibris.link/J2p0HbgK).
    In Ellen Lewin and Leni M. Silverstein, editors, *Mapping Feminist Anthropology
    in the Twenty-First Century*, page 65\. Rutgers University Press.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kramer (2016) Elise Kramer。2016年。[《女性主义语言学与语言学女性主义》](https://go.exlibris.link/J2p0HbgK)。收录在
    Ellen Lewin 和 Leni M. Silverstein 编辑的 *《21世纪女性主义人类学的绘图》* 中，第65页。罗格斯大学出版社。
- en: 'Lakoff (1973) Robin Lakoff. 1973. [Language and Woman’s Place](http://www.jstor.org/stable/4166707).
    *Language in Society*, 2(1):45–80. Publisher: Cambridge University Press.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakoff (1973) Robin Lakoff. 1973. [语言与女性的位置](http://www.jstor.org/stable/4166707)。*《社会语言学》*，2(1):45–80。出版商：剑桥大学出版社。
- en: 'Lauscher et al. (2022) Anne Lauscher, Archie Crowley, and Dirk Hovy. 2022.
    [Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language
    Processing beyond Gender](http://arxiv.org/abs/2202.11923). *arXiv:2202.11923
    [cs]*. ArXiv: 2202.11923.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lauscher et al. (2022) Anne Lauscher, Archie Crowley, 和 Dirk Hovy. 2022. [欢迎来到现代代词世界：超越性别的身份包容自然语言处理](http://arxiv.org/abs/2202.11923)。*arXiv:2202.11923
    [cs]*。ArXiv: 2202.11923。'
- en: 'Lauscher et al. (2021) Anne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.
    [Sustainable Modular Debiasing of Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.411).
    In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages
    4782–4797, Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lauscher et al. (2021) Anne Lauscher, Tobias Lueken, 和 Goran Glavaš. 2021. [语言模型的可持续模块化去偏见](https://doi.org/10.18653/v1/2021.findings-emnlp.411)。见于
    *计算语言学协会发现：EMNLP 2021*，页码4782–4797，蓬塔卡纳，多米尼加共和国。计算语言学协会。
- en: 'Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. 2023. [Textbooks Are All You Need II: phi-1.5
    technical report](https://doi.org/10.48550/arXiv.2309.05463). ArXiv:2309.05463
    [cs].'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, 和 Yin Tat Lee. 2023. [教科书就是你所需 II: phi-1.5 技术报告](https://doi.org/10.48550/arXiv.2309.05463)。ArXiv:2309.05463
    [cs]。'
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://doi.org/10.48550/arXiv.1907.11692).
    ArXiv:1907.11692 [cs].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. 2019.
    [RoBERTa: 一种强健优化的BERT预训练方法](https://doi.org/10.48550/arXiv.1907.11692)。ArXiv:1907.11692
    [cs]。'
- en: Lund et al. (2023) Gunnar Lund, Kostiantyn Omelianchuk, and Igor Samokhin. 2023.
    [Gender-inclusive grammatical error correction through augmentation](https://doi.org/10.18653/v1/2023.bea-1.13).
    In *Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational
    Applications (BEA 2023)*, pages 148–162, Toronto, Canada. Association for Computational
    Linguistics.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lund et al. (2023) Gunnar Lund, Kostiantyn Omelianchuk, 和 Igor Samokhin. 2023.
    [通过增强实现性别包容的语法错误纠正](https://doi.org/10.18653/v1/2023.bea-1.13)。见于 *第18届自然语言处理教育应用创新研讨会（BEA
    2023）*，页码148–162，多伦多，加拿大。计算语言学协会。
- en: 'Meade et al. (2022) Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy. 2022.
    [An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained
    Language Models](https://aclanthology.org/2022.acl-long.132). In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 1878–1898, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meade et al. (2022) Nicholas Meade, Elinor Poole-Dayan, 和 Siva Reddy. 2022.
    [预训练语言模型去偏见技术效果的实证调查](https://aclanthology.org/2022.acl-long.132)。见于 *计算语言学协会第60届年会（第1卷：长篇论文）*，页码1878–1898，都会，爱尔兰。计算语言学协会。
- en: 'Mills (2012) Sara Mills. 2012. *Gender matters : feminist linguistic analysis*.
    Equinox Publishing Ltd, London.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mills (2012) Sara Mills. 2012. *性别问题：女性主义语言分析*。Equinox Publishing Ltd, 伦敦。
- en: 'Nangia et al. (2020) Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R.
    Bowman. 2020. [CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in
    Masked Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.154). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 1953–1967, Online. Association for Computational Linguistics.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nangia et al. (2020) Nikita Nangia, Clara Vania, Rasika Bhalerao, 和 Samuel
    R. Bowman. 2020. [CrowS-Pairs: 测量掩蔽语言模型中的社会偏见的挑战数据集](https://doi.org/10.18653/v1/2020.emnlp-main.154)。见于
    *2020年自然语言处理经验方法会议（EMNLP）*，页码1953–1967，在线。计算语言学协会。'
- en: 'Navigli and Ponzetto (2012) Roberto Navigli and Simone Paolo Ponzetto. 2012.
    [BabelNet: The automatic construction, evaluation and application of a wide-coverage
    multilingual semantic network](https://doi.org/10.1016/j.artint.2012.07.001).
    *Artificial Intelligence*, 193:217–250.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Navigli and Ponzetto (2012) Roberto Navigli 和 Simone Paolo Ponzetto. 2012.
    [BabelNet: 广覆盖多语言语义网络的自动构建、评估和应用](https://doi.org/10.1016/j.artint.2012.07.001)。*《人工智能》*，193:217–250。'
- en: 'Nozza et al. (2021) Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. [HONEST:
    Measuring Hurtful Sentence Completion in Language Models](https://doi.org/10.18653/v1/2021.naacl-main.191).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2398–2406,
    Online. Association for Computational Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nozza 等人（2021）Debora Nozza、Federico Bianchi 和 Dirk Hovy。2021。 [HONEST：衡量语言模型中的伤害性句子完成](https://doi.org/10.18653/v1/2021.naacl-main.191)。在
    *2021 年北美计算语言学协会会议：人类语言技术论文集*，第2398–2406页，在线。计算语言学协会。
- en: Nozza et al. (2022) Debora Nozza, Federico Bianchi, Anne Lauscher, and Dirk
    Hovy. 2022. [Measuring harmful sentence completion in language models for LGBTQIA+
    individuals](https://doi.org/10.18653/v1/2022.ltedi-1.4). In *Proceedings of the
    Second Workshop on Language Technology for Equality, Diversity and Inclusion*,
    pages 26–34, Dublin, Ireland. Association for Computational Linguistics.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nozza 等人（2022）Debora Nozza、Federico Bianchi、Anne Lauscher 和 Dirk Hovy。2022。
    [衡量语言模型中对 LGBTQIA+ 个体的有害句子完成](https://doi.org/10.18653/v1/2022.ltedi-1.4)。在 *第二次语言技术与平等、多样性和包容性研讨会论文集*，第26–34页，都柏林，爱尔兰。计算语言学协会。
- en: 'Orgad and Belinkov (2022) Hadas Orgad and Yonatan Belinkov. 2022. [Choose Your
    Lenses: Flaws in Gender Bias Evaluation](https://doi.org/10.18653/v1/2022.gebnlp-1.17).
    In *Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing
    (GeBNLP)*, pages 151–167, Seattle, Washington. Association for Computational Linguistics.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orgad 和 Belinkov（2022）Hadas Orgad 和 Yonatan Belinkov。2022。 [选择你的视角：性别偏见评估中的缺陷](https://doi.org/10.18653/v1/2022.gebnlp-1.17)。在
    *第4届自然语言处理性别偏见研讨会（GeBNLP）论文集*，第151–167页，西雅图，华盛顿。计算语言学协会。
- en: 'Ovalle et al. (2023) Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary Jaggers,
    Kai-Wei Chang, Aram Galstyan, Richard Zemel, and Rahul Gupta. 2023. [“I’m fully
    who I am”: Towards Centering Transgender and Non-Binary Voices to Measure Biases
    in Open Language Generation](https://doi.org/10.1145/3593013.3594078). In *Proceedings
    of the 2023 ACM Conference on Fairness, Accountability, and Transparency*, FAccT
    ’23, pages 1246–1266, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ovalle 等人（2023）Anaelia Ovalle、Palash Goyal、Jwala Dhamala、Zachary Jaggers、Kai-Wei
    Chang、Aram Galstyan、Richard Zemel 和 Rahul Gupta。2023。 [“我完全就是我自己”：向中心化跨性别和非二元声音以衡量开放语言生成中的偏见](https://doi.org/10.1145/3593013.3594078)。在
    *2023 年 ACM 公平、问责制和透明度会议论文集*，FAccT ’23，第1246–1266页，纽约，NY，美国。计算机协会。
- en: Pauwels (2003) Anne Pauwels. 2003. [Linguistic Sexism and Feminist Linguistic
    Activism](https://doi.org/10.1002/9780470756942.ch24). In Janet Holmes and Miriam
    Meyerhoff, editors, *The Handbook of Language and Gender*, pages 550–570\. Blackwell
    Publishing Ltd, Oxford, UK.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pauwels（2003）Anne Pauwels。2003。 [语言性别歧视与女性主义语言行动主义](https://doi.org/10.1002/9780470756942.ch24)。在
    Janet Holmes 和 Miriam Meyerhoff 编著的 *语言与性别手册* 中，第550–570页。Blackwell Publishing
    Ltd，牛津，英国。
- en: 'Piergentili et al. (2023a) Andrea Piergentili, Dennis Fucci, Beatrice Savoldi,
    Luisa Bentivogli, and Matteo Negri. 2023a. [Gender neutralization for an inclusive
    machine translation: from theoretical foundations to open challenges](https://aclanthology.org/2023.gitt-1.7).
    In *Proceedings of the First Workshop on Gender-Inclusive Translation Technologies*,
    pages 71–83, Tampere, Finland. European Association for Machine Translation.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piergentili 等人（2023a）Andrea Piergentili、Dennis Fucci、Beatrice Savoldi、Luisa
    Bentivogli 和 Matteo Negri。2023a。 [性别中立化以实现包容性机器翻译：从理论基础到开放挑战](https://aclanthology.org/2023.gitt-1.7)。在
    *第一次性别包容翻译技术研讨会论文集*，第71–83页，坦佩雷，芬兰。欧洲机器翻译协会。
- en: 'Piergentili et al. (2023b) Andrea Piergentili, Dennis Fucci, Beatrice Savoldi,
    Luisa Bentivogli, and Matteo Negri. 2023b. [Gender Neutralization for an Inclusive
    Machine Translation: from Theoretical Foundations to Open Challenges](https://doi.org/10.48550/arXiv.2301.10075).
    ArXiv:2301.10075 [cs].'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piergentili 等人（2023b）Andrea Piergentili、Dennis Fucci、Beatrice Savoldi、Luisa
    Bentivogli 和 Matteo Negri。2023b。 [性别中立化以实现包容性机器翻译：从理论基础到开放挑战](https://doi.org/10.48550/arXiv.2301.10075)。ArXiv:2301.10075
    [cs]。
- en: 'Qi et al. (2020) Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D.
    Manning. 2020. [Stanza: A Python Natural Language Processing Toolkit for Many
    Human Languages](https://doi.org/10.48550/arXiv.2003.07082). ArXiv:2003.07082
    [cs].'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等人（2020）Peng Qi、Yuhao Zhang、Yuhui Zhang、Jason Bolton 和 Christopher D. Manning。2020。
    [Stanza：一个用于多种人类语言的 Python 自然语言处理工具包](https://doi.org/10.48550/arXiv.2003.07082)。ArXiv:2003.07082
    [cs]。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019） Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei
    和 Ilya Sutskever。2019。《语言模型是无监督的多任务学习者》。
- en: 'Salinas et al. (2023) Abel Salinas, Parth Shah, Yuzhong Huang, Robert McCormack,
    and Fred Morstatter. 2023. [The Unequal Opportunities of Large Language Models:
    Examining Demographic Biases in Job Recommendations by ChatGPT and LLaMA](https://doi.org/10.1145/3617694.3623257).
    In *Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms,
    Mechanisms, and Optimization*, EAAMO ’23, pages 1–15, New York, NY, USA. Association
    for Computing Machinery.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salinas 等（2023） Abel Salinas、Parth Shah、Yuzhong Huang、Robert McCormack 和 Fred
    Morstatter。2023。[《大型语言模型的不平等机会：通过 ChatGPT 和 LLaMA 检查招聘推荐中的人口统计学偏见》](https://doi.org/10.1145/3617694.3623257)。在
    *第3届 ACM 算法、机制与优化公平性与访问会议论文集*，EAAMO ’23，第 1–15 页，纽约，NY，美国。计算机协会。
- en: 'Seaborn et al. (2023) Katie Seaborn, Shruti Chandra, and Thibault Fabre. 2023.
    [Transcending the “Male Code”: Implicit Masculine Biases in NLP Contexts](https://doi.org/10.1145/3544548.3581017).
    In *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*,
    pages 1–19, Hamburg Germany. ACM.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seaborn 等（2023） Katie Seaborn、Shruti Chandra 和 Thibault Fabre。2023。[《超越“男性代码”：NLP
    环境中的隐性男性偏见》](https://doi.org/10.1145/3544548.3581017)。在 *2023 年 CHI 计算机系统人因会议论文集*，第
    1–19 页，德国汉堡。ACM。
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,
    Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan
    Catanzaro. 2022. [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B,
    A Large-Scale Generative Language Model](http://arxiv.org/abs/2201.11990). ArXiv:2201.11990
    [cs].
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等（2022） Shaden Smith、Mostofa Patwary、Brandon Norick、Patrick LeGresley、Samyam
    Rajbhandari、Jared Casper、Zhun Liu、Shrimai Prabhumoye、George Zerveas、Vijay Korthikanti、Elton
    Zhang、Rewon Child、Reza Yazdani Aminabadi、Julie Bernauer、Xia Song、Mohammad Shoeybi、Yuxiong
    He、Michael Houston、Saurabh Tiwary 和 Bryan Catanzaro。2022。[《使用 DeepSpeed 和 Megatron
    训练 Megatron-Turing NLG 530B，一个大规模生成语言模型》](http://arxiv.org/abs/2201.11990)。ArXiv:2201.11990
    [cs]。
- en: 'Stanczak and Augenstein (2021) Karolina Stanczak and Isabelle Augenstein. 2021.
    [A Survey on Gender Bias in Natural Language Processing](http://arxiv.org/abs/2112.14168).
    *arXiv:2112.14168 [cs]*. ArXiv: 2112.14168.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stanczak 和 Augenstein（2021） Karolina Stanczak 和 Isabelle Augenstein。2021。[《自然语言处理中的性别偏见调查》](http://arxiv.org/abs/2112.14168)。*arXiv:2112.14168
    [cs]*。ArXiv: 2112.14168。'
- en: 'Steed et al. (2022) Ryan Steed, Swetasudha Panda, Ari Kobren, and Michael Wick.
    2022. [Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis
    in Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.247).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3524–3542, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steed 等（2022） Ryan Steed、Swetasudha Panda、Ari Kobren 和 Michael Wick。2022。[《上游缓解措施并非全能：测试预训练语言模型中的偏见转移假设》](https://doi.org/10.18653/v1/2022.acl-long.247)。在
    *第60届计算语言学协会年会（卷1：长篇论文）*，第 3524–3542 页，爱尔兰都柏林。计算语言学协会。
- en: 'Sun et al. (2021) Tony Sun, Kellie Webster, Apu Shah, William Yang Wang, and
    Melvin Johnson. 2021. [They, Them, Theirs: Rewriting with Gender-Neutral English](http://arxiv.org/abs/2102.06788).
    *arXiv:2102.06788 [cs]*. ArXiv: 2102.06788.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等（2021） Tony Sun、Kellie Webster、Apu Shah、William Yang Wang 和 Melvin Johnson。2021。[《他们、她们、他们的：用性别中立英语改写》](http://arxiv.org/abs/2102.06788)。*arXiv:2102.06788
    [cs]*。ArXiv: 2102.06788。'
- en: 'Thakur et al. (2023) Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu
    Liang, and Louis-Philippe Morency. 2023. [Language models get a gender makeover:
    Mitigating gender bias with few-shot data interventions](https://doi.org/10.18653/v1/2023.acl-short.30).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 340–351, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur 等（2023） Himanshu Thakur、Atishay Jain、Praneetha Vaddamanu、Paul Pu Liang
    和 Louis-Philippe Morency。2023。[《语言模型性别再造：通过少量样本数据干预缓解性别偏见》](https://doi.org/10.18653/v1/2023.acl-short.30)。在
    *第61届计算语言学协会年会（卷2：短篇论文）*，第 340–351 页，加拿大多伦多。计算语言学协会。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [LLaMA: Open and Efficient Foundation Language Models](https://doi.org/10.48550/arXiv.2302.13971).
    ArXiv:2302.13971 [cs].'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。2023。[LLaMA：开放且高效的基础语言模型](https://doi.org/10.48550/arXiv.2302.13971)。ArXiv:2302.13971
    [cs]。
- en: 'Vanmassenhove et al. (2021) Eva Vanmassenhove, Chris Emmery, and Dimitar Shterionov.
    2021. [NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting
    into Gender Neutral Alternatives](https://aclanthology.org/2021.emnlp-main.704).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 8940–8948, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanmassenhove 等（2021）Eva Vanmassenhove、Chris Emmery 和 Dimitar Shterionov。2021。[NeuTral
    Rewriter：基于规则和神经的方法用于自动改写为性别中立替代](https://aclanthology.org/2021.emnlp-main.704)。见于
    *2021 年自然语言处理实证方法会议论文集*，第 8940–8948 页，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Vashishtha et al. (2023) Aniket Vashishtha, Kabir Ahuja, and Sunayana Sitaram.
    2023. [On evaluating and mitigating gender biases in multilingual settings](https://doi.org/10.18653/v1/2023.findings-acl.21).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    307–318, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vashishtha 等（2023）Aniket Vashishtha、Kabir Ahuja 和 Sunayana Sitaram。2023。 [在多语言环境中评估和缓解性别偏见](https://doi.org/10.18653/v1/2023.findings-acl.21)。见于
    *计算语言学协会会议成果：ACL 2023*，第 307–318 页，加拿大多伦多。计算语言学协会。
- en: 'Whorf and Carroll (1956) Benjamin Lee Whorf and John Bissell Carroll. 1956.
    *Language, thought and reality: selected writings of Benjamin Lee Whorf*. M.I.T.
    Press, Cambridge [Mass].'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Whorf 和 Carroll（1956）Benjamin Lee Whorf 和 John Bissell Carroll。1956。*语言、思想与现实：本杰明·李·沃尔夫的精选著作*。麻省理工学院出版社，剑桥
    [马萨诸塞州]。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-Art Natural Language
    Processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等（2020）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement Delangue、Anthony
    Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz、Joe Davison、Sam Shleifer、Patrick
    von Platen、Clara Ma、Yacine Jernite、Julien Plu、Canwen Xu、Teven Le Scao、Sylvain
    Gugger、Mariama Drame、Quentin Lhoest 和 Alexander M. Rush。2020。[变压器：最先进的自然语言处理](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。见于
    *2020 年自然语言处理实证方法会议：系统演示*，第 38–45 页，在线。计算语言学协会。
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. [LIMA: Less Is More for Alignment](http://arxiv.org/abs/2305.11206).
    ArXiv:2305.11206 [cs].'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Chunting Zhou、Pengfei Liu、Puxin Xu、Srini Iyer、Jiao Sun、Yuning Mao、Xuezhe
    Ma、Avia Efrat、Ping Yu、Lili Yu、Susan Zhang、Gargi Ghosh、Mike Lewis、Luke Zettlemoyer
    和 Omer Levy。2023。[LIMA：少即是多用于对齐](http://arxiv.org/abs/2305.11206)。ArXiv:2305.11206
    [cs]。
- en: Appendix A Appendix
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: '| man- | # | woman- | # | boy- | # | girl- | # |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| man- | # | woman- | # | boy- | # | girl- | # |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| man-made | 181 | womankind | 45 | boyfriend | 5,333 | girlfriend | 7,442
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 人造 | 181 | 女性 | 45 | 男朋友 | 5,333 | 女朋友 | 7,442 |'
- en: '| man-child | 24 | womanism | 12 | boyish | 32 | girlish | 20 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 男孩气 | 24 | 女性主义 | 12 | 男孩气质 | 32 | 女孩气质 | 20 |'
- en: '| man-eating | 17 | womanist | 9 | boyband | 13 | girliness | 17 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 吃人 | 17 | 女性主义 | 9 | 男孩乐队 | 13 | 女孩气质 | 17 |'
- en: '| man-eater | 11 | womanly | 2 | boyscout | 3 | girlfight | 5 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 吃人 | 11 | 女性化 | 2 | 男童军 | 3 | 女孩打架 | 5 |'
- en: '| man-crush | 10 |  |  | boyism | 3 | girllove | 4 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 男人崇拜 | 10 |  |  | 男孩主义 | 3 | 女孩爱 | 4 |'
- en: '| man power | 10 |  |  | boyishly | 1 | girldom | 2 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 男人力 | 10 |  |  | 少年气 | 1 | 女孩气 | 2 |'
- en: '| man-boobs | 9 |  |  | boytoy | 1 | girlification | 2 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 男胸 | 9 |  |  | 玩具男孩 | 1 | 女孩化 | 2 |'
- en: '| man-hater | 9 |  |  |  |  | girlfag | 1 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 厌男者 | 9 |  |  |  |  | 女同性恋者 | 1 |'
- en: '| man-hating | 7 |  |  |  |  | girlishly | 1 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 厌男 | 7 |  |  |  |  | 娇女生 | 1 |'
- en: '| manstopper | 7 |  |  |  |  | girlpower | 1 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 男止 | 7 |  |  |  |  | 女孩力量 | 1 |'
- en: 'Table 6: Top 10 words with gender-denoting prefixes after second round of verification
    and their frequencies within 200-million token subset of OpenWebText2; empty rows
    indicate that $<10$ instances were found.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在第二轮验证后的前 10 个带有性别标识前缀的词及其在 2 亿词汇子集中的频率；空行表示找到的实例少于 $<10$ 个。
- en: '| -manship | # |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| -manship | # |'
- en: '| --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| chairmanship | 693 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 主席职位 | 693 |'
- en: '| craftsmanship | 424 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 工艺精湛 | 424 |'
- en: '| workmanship | 174 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 工艺水平 | 174 |'
- en: '| sportsmanship | 155 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 体育精神 | 155 |'
- en: '| statesmanship | 154 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 政治家风范 | 154 |'
- en: '| showmanship | 149 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 表演艺术 | 149 |'
- en: '| marksmanship | 149 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 精准射击 | 149 |'
- en: '| gamesmanship | 147 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 游戏技巧 | 147 |'
- en: '| brinkmanship | 119 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 边缘政策 | 119 |'
- en: '| upmanship | 118 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 优越感 | 118 |'
- en: '| salesmanship | 105 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 销售技巧 | 105 |'
- en: '| brinksmanship | 73 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 威慑政策 | 73 |'
- en: '| penmanship | 62 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 笔迹 | 62 |'
- en: '| seamanship | 31 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 航海技术 | 31 |'
- en: '| swordsmanship | 28 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 剑术 | 28 |'
- en: '| airmanship | 21 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 航空技术 | 21 |'
- en: '| draftsmanship | 13 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 草图工艺 | 13 |'
- en: '| horsemanship | 12 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 骑术 | 12 |'
- en: '| craftmanship | 6 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 工艺水平 | 6 |'
- en: '| draughtsmanship | 5 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 草图工艺 | 5 |'
- en: '| -womanship | # |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| -womanship | # |'
- en: '| stateswomanship | 2 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 女州长 | 2 |'
- en: '| workwomanship | 2 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 女工艺员 | 2 |'
- en: 'Table 7: Top 20 words with -manship suffix and the two words with -womanship
    suffix after second round of verification and their frequencies within 200-million
    token subset of OpenWebText2'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在第二轮验证后的前 20 个含 -manship 后缀的词以及两个含 -womanship 后缀的词及其在 2 亿词汇子集中的频率
- en: '| suffix: -woman |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 后缀：-woman |'
- en: '| ambulancewoman::emergency medical technician, anchorwoman::anchorperson,
    anti-woman::misogynist, antiwoman::misogynist, bogeywoman::monster, bondwoman::slave,
    businesswoman::businessperson, cavewoman::caveperson, charwoman::cleaner, congresswoman::congressperson,
    craftswoman::craftsoerson, everywoman::ordinary person, fisherwoman::fisher, forewoman::foreperson,
    frontierswoman::explorer, frontwoman::frontperson, gentlewoman::refined person,
    hitwoman::assassin, horsewoman::equestrian, madwoman::maniac |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 女急救员::急救医务人员，女主播::新闻主播，厌女者::厌女者，女恶棍::怪物，女奴::奴隶，女商人::商人，穴居女::穴居者，清洁女工::清洁工，女议员::议员，女工匠::工艺工人，普通女子::普通人，女渔民::渔夫，女工头::工头，女探险者::探险者，女前卫::前卫，女绅士::优雅的人，女杀手::刺客，女骑手::骑手，疯女人::疯子
    |'
- en: '| suffix: -womanship |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 后缀：-womanship |'
- en: '| stateswomanship::statespersonship, workwomanship::workpersonship |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 女州长::州长职位，女工艺员::工艺职位 |'
- en: '| suffix: -girl |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 后缀：-girl |'
- en: '| babygirl::baby, ballgirl::ball person, bargirl::bartender, callgirl::sex
    worker, cavegirl::caveperson, cowgirl::cow herder, fangirl::fan, farmgirl::farm
    worker, papergirl::newspaper delivery person, playgirl::player, showgirl::performer,
    slavegirl::slave, snowgirl::snowperson, tomgirl::timid child |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 小女孩::婴儿，球女郎::球员，酒吧女郎::酒吧招待，陪酒女郎::性工作者，穴居女孩::穴居者，牛仔女孩::牛群牧者，粉丝女孩::粉丝，农场女孩::农场工人，报纸女孩::报纸递送员，玩乐女孩::玩家，秀女::表演者，奴隶女孩::奴隶，雪女孩::雪人，胆小女孩::胆小的孩子
    |'
- en: '| suffix: -man |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 后缀：-man |'
- en: '| adman::advertiser, almsman::medical social worker, ambulanceman::emergency
    medical technician, anchorman::anchorperson, artilleryman::cannoneer, assemblyman::assembly
    member, assman::assperson, backwoodsman::explorer, bagman::travelling salesperson,
    bargeman::barge operator, barman::bartender, baseman::baseperson, batsman::batter,
    bellman::bellhop, binman::garbage collector, bluesman::bluesperson, boatman::boater,
    bogeyman::monster, bondman::slave, bondsman::slave |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 广告商::广告商，施药员::医疗社会工作者，男急救员::急救医务人员，男主播::新闻主播，炮手::火炮手，议员::议会成员，屁股男::屁股人，边远地区人::探险者，袋装销售员::旅行推销员，驳船工::驳船操作员，酒保::酒吧招待，垒球员::垒球员，击球手::击球员，行李员::行李员，垃圾工::垃圾收集员，布鲁斯歌手::布鲁斯歌手，船夫::船员，怪物男::怪物，债主::奴隶，债务人::奴隶
    |'
- en: '| suffix: -manship |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 后缀：-manship |'
- en: '| airmanship::aerial skill, batsmanship::batting skill, brinkmanship::extreme
    strategy, brinksmanship::extreme strategy, chairmanship::chairpersonship, churchmanship
    ::churchpersonship, craftmanship::craftpersonship, craftsmanship::craftspersonship,
    draftsmanship::draftspersonship, draughtsmanship::draughtspersonship, foremanship::forepersonship,
    gamesmanship::unsporting tactic, gentlemanship::refinedness, grantsmanship::grant
    acquisition expertise, handcraftsmanship::handcraftspersonship, horsemanship::equestrian
    skill, journeymanship::artisanship, manship::courage, marksmanship::sharpshooting
    skill, oarsmanship::rowing skill |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| airmanship::空中技能, batsmanship::击球技巧, brinkmanship::极限策略, brinksmanship::极限策略,
    chairmanship::主席职责, churchmanship::教会职责, craftmanship::工艺技能, craftsmanship::工艺技能,
    draftsmanship::制图技能, draughtsmanship::制图技能, foremanship::工头职责, gamesmanship::不正当策略,
    gentlemanship::绅士风度, grantsmanship::资助申请专业知识, handcraftsmanship::手工艺技能, horsemanship::骑术,
    journeymanship::工艺技能, manship::勇气, marksmanship::射击技巧, oarsmanship::划船技巧 |'
- en: '| suffix: -boy |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 后缀: -boy |'
- en: '| ballboy::ball person, batboy::bat person, bellboy::bellhop, busboy::restaurant
    attendant, callboy::sex worker, copyboy::junior newspaper worker, cowboy::cow
    herder, doughboy::foot soldier, fanboy::fan, farmboy::farm worker, femboy::effeminate
    person, fisherboy::young fisher, fratboy::fraternity member, headboy::student
    leader, homeboy::fellow member, houseboy::domestic worker, ladyboy::genderqueer
    person, nancyboy::nancy, newsboy::newspaper delivery person, paperboy::newspaper
    delivery person |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ballboy::球童, batboy::击球童, bellboy::行李员, busboy::餐厅服务员, callboy::性工作者, copyboy::初级新闻工作者,
    cowboy::牧牛工, doughboy::步兵, fanboy::粉丝, farmboy::农场工人, femboy::女性化的人, fisherboy::年轻渔民,
    fratboy::兄弟会成员, headboy::学生领袖, homeboy::伙伴, houseboy::家务工, ladyboy::性别酷儿, nancyboy::娘娘腔,
    newsboy::报童, paperboy::报纸投递员 |'
- en: '| prefix: woman- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 前缀: woman- |'
- en: '| womanism::feminism, womanist::feminist, womankind::humankind, womanly::feminine
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| womanism::女性主义, womanist::女性主义者, womankind::人类, womanly::女性化的 |'
- en: '| prefix: girl- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 前缀: girl- |'
- en: '| girldom::feminine sphere, girlfag::woman attracted to gay men, girlfight::fight,
    girlfriend::partner, girlification::feminization, girliness::femininity, girlish::feminine,
    girlishly::childishly, girllove::love, girlpower::power |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| girldom::女性领域, girlfag::对同性恋男性有吸引力的女性, girlfight::打斗, girlfriend::伴侣, girlification::女性化,
    girliness::女性特质, girlish::女性化的, girlishly::像小女孩一样, girllove::爱, girlpower::力量
    |'
- en: '| prefix: man- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 前缀: man- |'
- en: '| man cave::sanctuary, man hater::hater, man hating::misandry, man hug::pound
    hug, man hunt::organized search, man magnet::attractive person, man marking::marking,
    man servant::servant, man up::adult up, man-ass::ass, man-bag::handbag, man-boobs::boobs,
    man-cave::sanctuary, man-cession::recession, man-child::child, man-crush::crush,
    man-eater::cannibal, man-eating::human-eating, man-friend::friend, man-hater::hater
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| man cave::避风港, man hater::厌男者, man hating::厌男, man hug::拥抱, man hunt::有组织的搜索,
    man magnet::吸引人的人, man marking::标记, man servant::仆人, man up::成熟起来, man-ass::屁股,
    man-bag::手袋, man-boobs::胸部, man-cave::避风港, man-cession::衰退, man-child::孩子, man-crush::迷恋,
    man-eater::食人者, man-eating::食人的, man-friend::朋友, man-hater::厌男者 |'
- en: '| prefix: boy- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 前缀: boy- |'
- en: '| boyband::band, boyfriend::partner, boyish::childish, boyishly::childishly,
    boyism::childism, boyscout::scout, boytoy::toy |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| boyband::男孩乐队, boyfriend::伴侣, boyish::孩子气的, boyishly::孩子气地, boyism::儿童主义,
    boyscout::童子军, boytoy::玩具 |'
- en: 'Table 8: Example terms (SG) from catalogue of gender-exclusive terms and gender-inclusive
    replacements; each category contains 20 example pairs or the number of pairs in
    the catalogue if there are $<20$ singular pairs'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 性别排他性术语和性别包容性替代术语的示例条目（SG）；每个类别包含20对示例，或目录中的对数（如果少于20对单数对）'
