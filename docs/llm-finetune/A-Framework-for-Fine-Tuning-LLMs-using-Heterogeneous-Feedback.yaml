- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:35:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Framework for Fine-Tuning LLMs using Heterogeneous Feedback
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用异构反馈微调LLMs的框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02861](https://ar5iv.labs.arxiv.org/html/2408.02861)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02861](https://ar5iv.labs.arxiv.org/html/2408.02861)
- en: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt²,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt²,
- en: Tong Yu², Xiang Chen², Subrata Mitra², Nedim Lipka ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Tong Yu², Xiang Chen², Subrata Mitra², Nedim Lipka ²
- en: ¹Carnegie Mellon University, ²Adobe Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹卡内基梅隆大学，²Adobe Research
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have been applied to a wide range of tasks, including
    text summarization, web navigation, and chatbots. They have benefitted from supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) following
    an unsupervised pretraining. These datasets can be difficult to collect, limited
    in scope, and vary in sample quality. Additionally, datasets can vary extensively
    in supervision format, from numerical to binary as well as multi-dimensional with
    many different values. We present a framework for fine-tuning LLMs using heterogeneous
    feedback, which has two main components. First, we combine the heterogeneous feedback
    data into a single supervision format, compatible with methods like SFT and RLHF.
    Next, given this unified feedback dataset, we extract a high-quality and diverse
    subset to obtain performance increases potentially exceeding the full dataset.
    We conduct extensive experiments to understand the effectiveness of these techniques
    for incorporating heterogeneous feedback, and demonstrate improvements from using
    a high-quality and diverse subset of the data. We find that our framework is able
    to improve models in multiple areas simultaneously, such as in instruction following
    and bias reduction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经被应用于广泛的任务，包括文本摘要、网页导航和聊天机器人。它们从监督微调（SFT）和来自人类反馈的强化学习（RLHF）中获益，这些是在无监督预训练之后进行的。这些数据集的收集可能困难，范围有限，并且样本质量各异。此外，数据集的监督格式可能非常多样，从数值到二进制，以及多维的多种不同值。我们提出了一个使用异构反馈微调LLMs的框架，该框架有两个主要组件。首先，我们将异构反馈数据合并为一种与SFT和RLHF等方法兼容的单一监督格式。接下来，给定这个统一的反馈数据集，我们提取一个高质量且多样化的子集，以获得可能超越整个数据集的性能提升。我们进行广泛的实验，以了解这些技术在结合异构反馈方面的有效性，并展示使用高质量和多样化数据子集的改进。我们发现我们的框架能够同时改进多个领域的模型，如指令跟随和偏差减少。
- en: A Framework for Fine-Tuning LLMs using Heterogeneous Feedback
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异构反馈微调LLMs的框架
- en: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt², Tong Yu²,
    Xiang Chen², Subrata Mitra², Nedim Lipka ² ¹Carnegie Mellon University, ²Adobe
    Research
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Ryan Aponte ¹, Ryan A. Rossi ², Shunan Guo², Franck Dernoncourt², Tong Yu²,
    Xiang Chen², Subrata Mitra², Nedim Lipka ² ¹卡内基梅隆大学，²Adobe Research
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: LLMs are fine-tuned for a variety of purposes, such as for instruction following
    in InstructGPT Ouyang et al. ([2022](#bib.bib11)). The fine-tuning process generally
    begins with collecting examples of desired model behavior and performing supervised
    learning. Some models stop at SFT Chiang et al. ([2023](#bib.bib2)), while InstructGPT
    follows this by training a reward model based on binary human preference data.
    The fine-tuned model is then further refined using RLHF, using a signal from the
    reward model. In the example of InstructGPT, the algorithm used is Proximal Policy
    Optimization (PPO) Schulman et al. ([2017](#bib.bib14)). For each of these steps,
    the fine-tuning dataset uses a single form of supervision. Fine-tuning datasets
    exist for a variety of purposes, from training chat-based assistants in OASST Köpf
    et al. ([2023](#bib.bib6)), coreference resolution in WinoGrande Sakaguchi et al.
    ([2019](#bib.bib13)), helpfulness, honesty, and harmlessness in Anthropic HHH Nakano
    et al. ([2021](#bib.bib10)), and logical reasoning in OpenPlatypus Lee et al.
    ([2024](#bib.bib7)). Supervision format varies, from binary preference in Anthropic
    HHH, to several numerical labels OASST, to a string response in OpenPlatypus.
    Although fine-tuning has been successful in mitigating the limitations of pretrained
    LLMs, these methods require data of a single supervision type, restricting the
    scope of preference data. Recent work has filtered fine-tuning datasets to reduce
    cost and increase quality Wang et al. ([2024](#bib.bib17)).  Wu et al. ([2023](#bib.bib18))
    use LLMs to generate embeddings for fine-tuning data which is clustered with k-center-greedy Sener
    and Savarese ([2018](#bib.bib15)) using an iterative process.  Kung et al. ([2023](#bib.bib5))
    randomly delete words in prompts and measure how the response probability changes
    as a measure of the model’s uncertainty.  Li et al. ([2024](#bib.bib9)) outperform
    Alpaca as evaluated by LLM preference using only 5% of its fine-tuning data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 被用于各种目的的微调，例如在 InstructGPT Ouyang et al. ([2022](#bib.bib11)) 中用于跟随指令。微调过程通常从收集期望的模型行为示例和执行监督学习开始。有些模型停留在
    SFT Chiang et al. ([2023](#bib.bib2))，而 InstructGPT 在此之后会基于二元人类偏好数据训练一个奖励模型。然后，使用来自奖励模型的信号进一步通过
    RLHF 来优化微调模型。在 InstructGPT 的示例中，使用的算法是 Proximal Policy Optimization (PPO) Schulman
    et al. ([2017](#bib.bib14))。在这些步骤中的每一步，微调数据集使用单一形式的监督。微调数据集存在于多种目的下，从在 OASST Köpf
    et al. ([2023](#bib.bib6)) 中训练基于聊天的助手，到在 WinoGrande Sakaguchi et al. ([2019](#bib.bib13))
    中进行核心指代解析，到在 Anthropic HHH Nakano et al. ([2021](#bib.bib10)) 中评估有用性、诚实性和无害性，再到在
    OpenPlatypus Lee et al. ([2024](#bib.bib7)) 中的逻辑推理。监督格式各不相同，从 Anthropic HHH 的二元偏好，到
    OASST 的几个数值标签，到 OpenPlatypus 的字符串响应。尽管微调在减轻预训练 LLM 的局限性方面取得了成功，但这些方法需要单一监督类型的数据，限制了偏好数据的范围。最近的研究已过滤微调数据集以减少成本和提高质量
    Wang et al. ([2024](#bib.bib17))。Wu et al. ([2023](#bib.bib18)) 使用 LLMs 生成微调数据的嵌入，并使用
    k-center-greedy Sener and Savarese ([2018](#bib.bib15)) 通过迭代过程进行聚类。Kung et al.
    ([2023](#bib.bib5)) 随机删除提示中的词语，并测量响应概率如何变化，以此作为模型不确定性的度量。Li et al. ([2024](#bib.bib9))
    在仅使用 5% 的微调数据的情况下，超越了 Alpaca 的 LLM 偏好评估。
- en: We present a framework to use multiple fine-tuning data types, permitting the
    use of more fine-tuning datasets and fine-tuning for multiple tasks simultaneously.
    Using multiple datasets enables fine-tuning for different goals simultaneously,
    such as for logical reasoning and to reduce bias, and provides a more accurate
    view of human preference by broadening the scope of fine-tuning data. Our framework
    selects a high-quality and diverse subset of the data to make fine-tuning more
    effective.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个框架来使用多种微调数据类型，允许同时使用更多的微调数据集和进行多个任务的微调。使用多个数据集能够同时针对不同的目标进行微调，例如逻辑推理和减少偏见，并通过扩展微调数据的范围提供对人类偏好的更准确的视角。我们的框架选择了数据中高质量且多样的子集，以提高微调的效果。
- en: '![Refer to caption](img/56cfc0315303312ab6932898501318dd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56cfc0315303312ab6932898501318dd.png)'
- en: 'Figure 1: Framework. First, we concatenate the datasets into a dataset of heterogeneous
    feedback. We then score samples based on quality and prompt diversity, remove
    a fraction of the samples (a hyperparameter), forming the homogeneous dataset
    $D_{train}$. Standard fine-tuning methods are then applied to a pre-trained LLM.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 框架。首先，我们将数据集合并成一个异质反馈的数据集。然后，根据质量和提示多样性对样本进行评分，移除一部分样本（一个超参数），形成同质数据集
    $D_{train}$。然后，将标准微调方法应用于预训练的 LLM。'
- en: 2 Framework
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 框架
- en: The primary contribution of our framework is to be able to use fine-tuning data
    of heterogeneous supervision. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A
    Framework for Fine-Tuning LLMs using Heterogeneous Feedback") includes a high-level
    overview. Our framework utilizes the simplest supervision, such as binary preference,
    and projects all remaining datasets into that format. Because some data may be
    redundant in the unified dataset, we filter for quality and diversity to generate
    $D_{train}$. For simplicity, we use this dataset for both the SFT and RLHF steps
    of fine-tuning, however this is not a requirement. This generates an LLM fine-tuned
    with high-quality and diverse data, LLaMA-HD.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们框架的主要贡献是能够使用异质监督的微调数据。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback")包含了高级概述。我们的框架利用最简单的监督，例如二元偏好，并将所有剩余的数据集投影到该格式。由于统一数据集中可能有冗余数据，我们对其进行质量和多样性过滤，以生成$D_{train}$。为简便起见，我们将此数据集用于SFT和RLHF微调步骤，但这不是必须的。这生成了一个通过高质量和多样化数据进行微调的LLM，即LLaMA-HD。
- en: 2.1 Primary fine-tuning dataset
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 主要微调数据集
- en: Given a dataset $\mathcal{D}$ of prompts with two responses using binary preference,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含两个回复的提示数据集$\mathcal{D}$，使用二元偏好，
- en: '|  | $\mathcal{D}=\{(P_{i},A_{i,0},A_{i,1}\}_{i=1}^{M}$ |  | (1) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}=\{(P_{i},A_{i,0},A_{i,1}\}_{i=1}^{M}$ |  | (1) |'
- en: where $P$ is the prompt, $A_{i,0}$ and $A_{i,1}$ are answers to the prompt,
    with $A_{i,0}$ defined as the preferred response to the prompt. This type of dataset
    takes the form of binary preference due to two example responses to a single prompt.
    Examples here do not convey a sense of quality, thus prohibiting ranking.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P$是提示，$A_{i,0}$和$A_{i,1}$是对提示的回答，其中$A_{i,0}$定义为对提示的首选回复。这种类型的数据集由于对单个提示有两个示例回复，因此采用二元偏好形式。这里的示例不传达质量感，从而禁止排序。
- en: 2.2 Secondary fine-tuning dataset
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 次要微调数据集
- en: 'Given a dataset $\mathcal{D^{*}}$ of user-specific prompts and responses (question-answer
    tuples):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含用户特定提示和回复（问答元组）的数据集$\mathcal{D^{*}}$：
- en: '|  | $\mathcal{D^{*}}=\{(P_{i},A_{i},{\bf y}_{i})\}_{i=1}^{N}$ |  | (2) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D^{*}}=\{(P_{i},A_{i},{\bf y}_{i})\}_{i=1}^{N}$ |  | (2) |'
- en: where $P_{i}$ and $A_{i}$ are the $i$th prompt and response pair, respectively,
    and ${\bf y}_{i}\in\mathbb{R}^{k}$ is the real-valued vector denoting the score
    of various labels for that pair. For a dataset of this type to be compatible with
    our method, it is necessary that there are multiple responses to the same prompt.
    For example,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P_{i}$和$A_{i}$分别是第$i$个提示和回复对，而${\bf y}_{i}\in\mathbb{R}^{k}$是表示该对各种标签分数的实值向量。为了使这种类型的数据集与我们的方法兼容，必须对相同的提示有多个回复。例如，
- en: '|  | $(P_{i},A_{i^{\prime}},{\bf y}_{i^{\prime}})\in D^{*}$ |  | (3) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $(P_{i},A_{i^{\prime}},{\bf y}_{i^{\prime}})\in D^{*}$ |  | (3) |'
- en: can be the second response to the prompt. This process can be repeated for arbitrarily
    many datasets. A general method for one axis of supervision is included in Appendix [A.2](#A1.SS2
    "A.2 Converting Supervision ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning
    LLMs using Heterogeneous Feedback").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以是对提示的第二个回复。此过程可以对任意多个数据集重复进行。关于一个监督轴的一般方法包括在附录[A.2](#A1.SS2 "A.2 Converting
    Supervision ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs using Heterogeneous
    Feedback")中。
- en: 2.3 Simple Unionization for Feedback
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 简单的反馈联合
- en: We take $\mathcal{D^{*}}$ and create a dictionary with prompt as key and responses
    as a list of all responses to that prompt. This requires at least two responses
    for each prompt to be considered. We can conduct quality and diversity filtering
    on these prompts, and then select the preferred response pairs. Once we have tuples
    containing a prompt, preferred, and non-preferred response, our data from $D^{*}$
    are now in the same format as $D$, so we take the union.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将$\mathcal{D^{*}}$进行处理，创建一个以提示为键、所有对该提示的回复列表为值的字典。这要求每个提示至少有两个回复。我们可以对这些提示进行质量和多样性过滤，然后选择首选的回复对。一旦我们有了包含提示、首选和非首选回复的元组，我们的数据$D^{*}$现在与$D$处于相同格式，因此我们取其并集。
- en: 2.4 Quality Selection
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 质量选择
- en: We infer example quality based on the numerical labels of responses. For datasets
    with multiple numerical labels, selection of the label is a hyperparameter likely
    motivated by the purpose of fine-tuning. For example, our experiment uses toxicity
    as this is related to our objective of reducing bias. For prompts with more than
    two responses, the highest quality pair of responses are those that vary most
    in the numerical label. Intuitively, these should give a strong signal to a reward
    model because one response is strongly preferred. Finally, we can rank and select
    prompts by the preference difference of their responses.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据响应的数值标签推断示例质量。对于具有多个数值标签的数据集，标签的选择是一个超参数，可能受到微调目的的影响。例如，我们的实验使用毒性标签，因为这与我们减少偏差的目标相关。对于有超过两个响应的提示，数值标签差异最大的响应对被认为是质量最高的。直观上，这些应给奖励模型提供强信号，因为一个响应被明显偏好。最后，我们可以通过响应的偏好差异对提示进行排名和选择。
- en: 2.5 Diversity Selection
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 多样性选择
- en: We select for prompt diversity by generating embeddings for each prompt, followed
    by clustering. Prompts with similar meaning can be considered redundant. We follow
    OpenPlatypus in using a sentence transformer to generate semantic embeddings to
    filter datasets Lee et al. ([2024](#bib.bib7)). Embeddings are then clustered
    using unsupervised methods like k-means. We select the top fraction of responses
    from each cluster. Both the number of clusters and fraction of each cluster are
    hyperparameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过为每个提示生成嵌入，并进行聚类，来选择提示的多样性。具有相似意义的提示可以视为冗余。我们跟随OpenPlatypus，使用句子变换器生成语义嵌入来过滤数据集
    Lee等人（[2024](#bib.bib7)）。然后，使用无监督方法如k-means对嵌入进行聚类。我们从每个聚类中选择前一部分响应。聚类的数量和每个聚类的比例都是超参数。
- en: 2.6 Training
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 训练
- en: We use the training pipeline from StackLLaMA Beeching et al. ([2023](#bib.bib1)),
    which uses LLaMA-7B Touvron et al. ([2023](#bib.bib16)). First, we perform SFT
    on the base model. We then train a reward model using the fine-tuned model. This
    is followed by RLHF on the fine-tuned model using PPO. Low-Rank Adaptation is
    used to reduce memory usage and increase parallelization Hu et al. ([2021](#bib.bib4)).
    We select varying fractions of the training dataset, as well as omit filtering,
    to measure its influence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了StackLLaMA Beeching等人（[2023](#bib.bib1)）的训练流程，该流程使用了LLaMA-7B Touvron等人（[2023](#bib.bib16)）。首先，我们对基础模型进行SFT。然后使用微调后的模型训练奖励模型。接下来，在微调后的模型上使用PPO进行RLHF。使用低秩适配来减少内存使用并增加并行性
    Hu等人（[2021](#bib.bib4)）。我们选择不同的训练数据集比例，并且省略了过滤，以衡量其影响。
- en: 'Table 1: Quantitative Results. Bolded entries denote highest performance. -S
    indicates model was fine-tuned with SFT only, -R is SFT followed by RLHF. No filtering
    was performed on fine-tuning datasets for LLaMA-S and LLaMA-R.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：定量结果。加粗的条目表示性能最高。-S表示模型仅通过SFT进行微调，-R表示SFT后进行RLHF。对LLaMA-S和LLaMA-R的微调数据集未进行过滤。
- en: '| Model | Bias $\downarrow$ | Bias (Entropy) $\downarrow$ | Bias (Cluster)
    $\downarrow$ | Accuracy $\uparrow$ | Similarity $\uparrow$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 偏差 $\downarrow$ | 偏差（熵）$\downarrow$ | 偏差（聚类）$\downarrow$ | 准确率 $\uparrow$
    | 相似性 $\uparrow$ |'
- en: '| LLaMA-Base | 0.4585 | 0.0010 | 3.0393 | 0.9482 | 0.9482 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Base | 0.4585 | 0.0010 | 3.0393 | 0.9482 | 0.9482 |'
- en: '| LLaMA-S | 1.1721 | 0.1553 | 10.3180 | 0.5953 | 0.6553 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-S | 1.1721 | 0.1553 | 10.3180 | 0.5953 | 0.6553 |'
- en: '| LLaMA-R | 0.9247 | 0.0098 | 4.2139 | 0.9457 | 0.9457 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-R | 0.9247 | 0.0098 | 4.2139 | 0.9457 | 0.9457 |'
- en: '| LLaMA-HD-0.2-S | 0.4436 | 0.0580 | 4.5856 | 0.9204 | 0.9204 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.2-S | 0.4436 | 0.0580 | 4.5856 | 0.9204 | 0.9204 |'
- en: '| LLaMA-HD-0.4-S | 0.7798 | 0.0548 | 6.3741 | 0.8788 | 0.9394 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.4-S | 0.7798 | 0.0548 | 6.3741 | 0.8788 | 0.9394 |'
- en: '| LLaMA-HD-0.6-S | 0.7947 | 0.0407 | 7.5564 | 0.8327 | 0.8927 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.6-S | 0.7947 | 0.0407 | 7.5564 | 0.8327 | 0.8927 |'
- en: '| LLaMA-HD-1.0-S | 0.4117 | 0.0333 | 3.8851 | 0.9533 | 0.9533 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-1.0-S | 0.4117 | 0.0333 | 3.8851 | 0.9533 | 0.9533 |'
- en: '| LLaMA-HD-0.2-R | 0.4330 | 0.0580 | 3.0892 | 0.9482 | 0.9482 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.2-R | 0.4330 | 0.0580 | 3.0892 | 0.9482 | 0.9482 |'
- en: '| LLaMA-HD-0.4-R | 0.4287 | 0.0548 | 2.9852 | 0.9602 | 0.9508 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.4-R | 0.4287 | 0.0548 | 2.9852 | 0.9602 | 0.9508 |'
- en: '| LLaMA-HD-0.6-R | 0.4727 | 0.0010 | 2.9472 | 0.9646 | 0.9571 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-0.6-R | 0.4727 | 0.0010 | 2.9472 | 0.9646 | 0.9571 |'
- en: '| LLaMA-HD-1.0-R | 0.3629 | 0.0068 | 3.1570 | 0.9583 | 0.9583 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-HD-1.0-R | 0.3629 | 0.0068 | 3.1570 | 0.9583 | 0.9583 |'
- en: 3 Experimental Setup
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 3.1 Heterogeneous Datasets
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 异质数据集
- en: 'We use three datasets for our experiments: WinoGrande Sakaguchi et al. ([2019](#bib.bib13))
    (our primary dataset), OpenAssistant OASST Köpf et al. ([2023](#bib.bib6)) (our
    secondary dataset), and WinoGender Zhao et al. ([2018](#bib.bib19)) for testing
    the generalization of our method. WinoGrande is a coreference resolution dataset
    developed as a more challenging alternative to the Winograd Schema Challenge Levesque
    et al. ([2012](#bib.bib8)), as machine learning models exceeded 90% accuracy on
    the dataset. WinoGrande has been found to transfer to other wino-style schema
    challenges, including WinoGender. OASST is a conversation dataset consisting of
    over 10,000 conversation trees. This dataset has numerical supervision, providing
    an inherent measure of quality. WinoGender is a dataset testing gender bias in
    coreference resolution that involve a pair of sentences, one conforming to American
    gender biases and one against them. Differences in response indicate gender bias.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三个数据集进行实验：WinoGrande Sakaguchi 等人 ([2019](#bib.bib13))（我们的主要数据集）、OpenAssistant
    OASST Köpfer 等人 ([2023](#bib.bib6))（我们的次要数据集），以及 WinoGender Zhao 等人 ([2018](#bib.bib19))
    用于测试我们方法的泛化能力。WinoGrande 是一个核心指代解析数据集，作为 Winograd Schema Challenge Levesque 等人
    ([2012](#bib.bib8)) 的一个更具挑战性的替代品开发，因为机器学习模型在该数据集上的准确率超过了90%。研究发现，WinoGrande 可以转移到其他
    wino 风格的 schema 挑战中，包括 WinoGender。OASST 是一个包含超过 10,000 个对话树的数据集。该数据集具有数值监督，提供了固有的质量测量。WinoGender
    是一个测试核心指代解析中的性别偏见的数据集，涉及一对句子，一个符合美国性别偏见，另一个则反对它们。响应的差异表明性别偏见。
- en: We fine-tune using either WinoGrande alone with LLaMA-SFT and LLaMA-RLHF, or
    with a combination of WinoGrande and OASST using our framework. We fine-tune using
    several subsets of the data, in addition to the dataset without filtering. For
    SFT and training the reward model, we treat the pro-bias examples of WinoGender
    as negative and the anti-bias examples as positive. This follows from the intuition
    that language models learn human biases, so reductions in bias can be achieved
    by training models in the opposite direction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 WinoGrande 单独对 LLaMA-SFT 和 LLaMA-RLHF 进行微调，或者使用我们框架中的 WinoGrande 和 OASST
    的组合进行微调。我们在不进行过滤的数据集之外，还使用了数据的几个子集进行微调。对于 SFT 和训练奖励模型，我们将 WinoGender 的偏见示例视为负面，将反偏见示例视为正面。这是基于这样的直觉：语言模型会学习人类的偏见，因此通过在相反方向上训练模型可以减少偏见。
- en: 3.2 Dataset Filtering
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据集过滤
- en: We use the numerical score toxicity in OASST to measure prompt quality. Prompts
    are ranked based on the difference in toxicity between responses. By reducing
    toxicity, we may also be able to reduce gender bias. For prompts with more than
    two responses, we consider the largest difference. As WinoGrande does not have
    ordered scoring, these prompts are not filtered. The number following model name
    indicates the fraction of the dataset used, with 1.0 indicating no filtering was
    performed. Experimental details about quality and diversity selection are included
    in Appendix [A.3](#A1.SS3 "A.3 Hyperparameters ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 OASST 中的数值评分毒性来衡量提示质量。根据响应之间毒性的差异对提示进行排名。通过减少毒性，我们也可能能够减少性别偏见。对于具有多个响应的提示，我们考虑最大的差异。由于
    WinoGrande 没有排序评分，这些提示不会被过滤。紧跟在模型名称后面的数字表示使用的数据集的比例，1.0 表示未进行过滤。关于质量和多样性选择的实验细节包含在附录
    [A.3](#A1.SS3 "A.3 超参数 ‣ 附录 A 附录 ‣ A 基于异质反馈的 LLM 微调框架") 中。
- en: 3.3 Baselines
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基准
- en: 'We compare our approach that learns from heterogeneous human feedback datasets
    to the following fundamental baselines: Pre-trained LLM (base), Pre-trained LLM
    with SFT using WinoGrande, and Pre-trained LLM with SFT and RLHF using WinoGrande.
    Our method uses the same heterogeneous dataset for SFT and RLHF. Eight Nvidia
    A100 GPUs are used for each step of the process. We test using LLaMA-7B, however
    our framework is naturally able to leverage any other state-of-the-art large language
    model. The hyperparameters and LoRA configuration are included in Appendix [A.3](#A1.SS3
    "A.3 Hyperparameters ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs
    using Heterogeneous Feedback").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从异质人类反馈数据集中学习的方法与以下基本基准进行比较：预训练 LLM（基础）、使用 WinoGrande 的预训练 LLM 进行 SFT，以及使用
    WinoGrande 的预训练 LLM 进行 SFT 和 RLHF。我们的方法使用相同的异质数据集进行 SFT 和 RLHF。每个步骤都使用了八个 Nvidia
    A100 GPU。我们测试了 LLaMA-7B，但我们的框架自然能够利用任何其他最先进的大型语言模型。超参数和 LoRA 配置包含在附录 [A.3](#A1.SS3
    "A.3 超参数 ‣ 附录 A 附录 ‣ A 基于异质反馈的 LLM 微调框架") 中。
- en: 3.4 Metrics
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 指标
- en: We use several metrics to measure change in gender bias, reported in Table [1](#S2.T1
    "Table 1 ‣ 2.6 Training ‣ 2 Framework ‣ A Framework for Fine-Tuning LLMs using
    Heterogeneous Feedback"). Our metrics use prompts based on the multiple choice
    format used in PaLM Chowdhery et al. ([2022](#bib.bib3)), and are included in
    Appendix [A.4](#A1.SS4 "A.4 Quantitative Prompt ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback"). Bias takes the difference
    in log probabilities for the correct token in WinoBias for the pro-bias and anti-bias
    sentences. A model reflecting no gender bias would have a difference of 0\. Bias
    (Cluster) performs the same computation, except it considers the log probabilities
    for every word in the coreference cluster. This includes the pronoun used in the
    Bias metric, so its values are larger. Bias (Entropy) takes the relative entropy
    of the next token logits for the pro-bias and anti-bias sentences. This measures
    how different the model state is as a result of each prompt. An unbiased model
    would have a relative entropy of 0.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用几个指标来测量性别偏见的变化，报告见表[1](#S2.T1 "表1 ‣ 2.6 训练 ‣ 2 框架 ‣ 使用异质反馈微调LLM的框架")。我们的指标使用基于PaLM
    Chowdhery等人 ([2022](#bib.bib3)) 使用的多项选择格式的提示，并包含在附录[A.4](#A1.SS4 "A.4 定量提示 ‣ 附录A
    ‣ 使用异质反馈微调LLM的框架")中。偏见通过计算WinoBias中亲偏见和反偏见句子中正确标记的对数概率差异来衡量。一个没有性别偏见的模型，其差异为0。Bias
    (Cluster) 执行相同的计算，只是它考虑了共指簇中每个单词的对数概率。这包括在偏见指标中使用的代词，因此其值较大。Bias (Entropy) 取亲偏见和反偏见句子中下一个标记logits的相对熵。这测量了每个提示导致的模型状态的不同程度。一个无偏模型的相对熵为0。
- en: Accuracy is computed in a generative context, where the model is asked to complete
    a sentence. Generation is stopped after 10 new tokens or a punctuation token,
    whichever is sooner. Accuracy is averaged over both the pro-bias and anti-bias
    prompts, so this is more a measure of utility. We complement this metric with
    Similarity, which uses the same generation. It is how often the result, either
    correct or incorrect, for a pair of WinoBias sentences is shared.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成上下文中计算准确性，其中模型被要求完成一个句子。生成在生成10个新词或一个标点符号后停止，以较早者为准。准确性在亲偏见和反偏见提示中取平均值，因此这更是效用的衡量标准。我们通过相似性来补充这一指标，相似性使用相同的生成方法。它衡量了WinoBias句子对的结果（无论是正确还是错误）有多频繁地被共享。
- en: 'Additionally, we conduct a qualitative experiment. We ask the model to respond
    to several simple prompts. Evaluating models in a chatbot-like context gives another
    perspective on utility. The models are given single-sentence prompts such as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了一项定性实验。我们要求模型对几个简单的提示做出回应。在类似聊天机器人的上下文中评估模型提供了另一种效用视角。模型收到的提示是单句，例如：
- en: '"Give me a list of top vacation destinations.\n"'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “给我一个顶级度假胜地的列表。\n”
- en: 4 Results
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: 4.1 Quantitative Results
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 定量结果
- en: Quantitative results are reported in Table [1](#S2.T1 "Table 1 ‣ 2.6 Training
    ‣ 2 Framework ‣ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback").
    Results are rounded to 4 digits after the decimal place. We find that our method
    is able to reduce bias by several metrics relative to all baselines, including
    a pre-trained model, while maintaining utility as measured by accuracy. We also
    see that using SFT and RLHF with our framework generally leads to lower bias than
    with SFT only. Based on the results for Bias (Entropy), Bias (Cluster), and Accuracy,
    we can get higher performance by filtering for data quality and diversity than
    with using the full fine-tuning dataset. We believe this result may be improved
    by examining more rigorous methods for measuring quality and diversity. The qualitative
    results also show that our framework permits the improvment on multiple measures,
    which are not necessarily correlated, simultaneously. With LLaMA-HD-0.6-R and
    LLaMA-HD-1.0-R, we achieve higher generative accuracy, a measure of utility, and
    higher generative similarity, a measure of bias, relative to the base model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 定量结果见表格 [1](#S2.T1 "Table 1 ‣ 2.6 Training ‣ 2 Framework ‣ A Framework for Fine-Tuning
    LLMs using Heterogeneous Feedback")。结果保留四位小数。我们发现，我们的方法能够相对于所有基线，包括一个预训练模型，减少多个指标的偏差，同时保持通过准确度衡量的实用性。我们还看到，使用
    SFT 和 RLHF 结合我们的框架通常比仅使用 SFT 更能降低偏差。根据 Bias（Entropy）、Bias（Cluster）和 Accuracy 的结果，通过筛选数据质量和多样性可以获得比使用完整的微调数据集更高的性能。我们相信，通过研究更严格的质量和多样性测量方法，这一结果可能会有所改善。定性结果还表明，我们的框架允许在多个不一定相关的度量上同时取得改进。使用
    LLaMA-HD-0.6-R 和 LLaMA-HD-1.0-R，我们相对于基模型实现了更高的生成准确性（一个实用性指标）和更高的生成相似性（一个偏差指标）。
- en: 4.2 Qualitative Results
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 定性结果
- en: We find that the base and fine-tuned methods using WinoGrande consistently fail
    to follow the prompt. In many instances, the prompt is repeated indefinitely.
    With our method, we receive reasonable responses, likely as a result of our secondary
    fine-tuning dataset, OASST, including instruction-following examples. The qualitative
    task shows us that the method is able to train for multiple tasks at once, namely
    a reduction in bias and instruction following. An illustrative sample is included
    in Appendix [A.5](#A1.SS5 "A.5 Qualitative Example ‣ Appendix A Appendix ‣ A Framework
    for Fine-Tuning LLMs using Heterogeneous Feedback"). We observe that while the
    base model rarely answers the prompt, LLaMA-S does on occasion respond reasonably,
    even though it was not explicitly instruction fine-tuned. Using only 20% of the
    filtered dataset, we are able to achieve consistent instruction following (Appendix [A.5](#A1.SS5
    "A.5 Qualitative Example ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs
    using Heterogeneous Feedback")). The highest generative accuracy and lowest bias
    (entropy) was also obtained by a model using a fitlered dataset, demonstrating
    that filtering can simultaneously improve quality and reduce bias.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，使用 WinoGrande 的基本方法和微调方法始终未能跟随提示。在许多情况下，提示被无限重复。通过我们的方法，我们得到合理的回应，这可能是由于我们的次级微调数据集
    OASST 包含了遵循指令的示例。定性任务向我们展示了该方法能够同时训练多个任务，即减少偏差和遵循指令。附录 [A.5](#A1.SS5 "A.5 Qualitative
    Example ‣ Appendix A Appendix ‣ A Framework for Fine-Tuning LLMs using Heterogeneous
    Feedback") 中包含了一个示例。我们观察到，尽管基础模型很少回答提示，LLaMA-S 偶尔会合理回应，尽管它没有被明确进行指令微调。仅使用 20%
    的筛选数据集，我们能够实现一致的指令跟随（附录 [A.5](#A1.SS5 "A.5 Qualitative Example ‣ Appendix A Appendix
    ‣ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback")）。使用筛选数据集获得了最高的生成准确性和最低的偏差（熵），这表明筛选可以同时提高质量并减少偏差。
- en: 5 Conclusion
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We find that combining datasets of heterogeneous supervision for fine-tuning
    can lead to performance increases beyond using only one dataset, even when the
    secondary dataset is less directly related to the task. We find that by varying
    the fraction of used data, we are able to achieve performance comparable to the
    full dataset, and sometimes exceed it. Most significantly, when the reduced bias
    result of the quantitative result are combined with the instruction-following
    seen in the qualitative result, we show that it is possible to fine-tune for multiple
    purposes simultaneously, even when the datasets include a different supervision
    format. Our framework can be used to improve both performance-oriented metrics,
    like instruction following, and to unwanted behavior like bias. This shows that
    it is possible to effectively fine-tune LLMs using heterogeneous supervision.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，结合异质监督的数据集进行微调可以在使用单一数据集的基础上带来性能提升，即使次级数据集与任务的直接相关性较低。我们发现，通过改变使用数据的比例，我们能够实现与完整数据集相当的性能，有时甚至超越它。最重要的是，当量化结果的减少偏差结果与定性结果中看到的指令跟随相结合时，我们展示了即使数据集包含不同的监督格式，也可以同时进行多目标微调。我们的框架可以用于提高以性能为导向的指标，如指令跟随，以及减少不希望出现的行为，如偏见。这表明，使用异质监督有效地微调LLMs是可能的。
- en: References
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Beeching et al. (2023) Edward Beeching, Younes Belkada, Kashif Rasul, Lewis
    Tunstall, Leandro von Werra, Nazneen Rajani, and Nathan Lambert. 2023. [Stackllama:
    An rl fine-tuned llama model for stack exchange question and answering](https://doi.org/10.57967/hf/0513).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beeching et al. (2023) 爱德华·比钦、尤尼斯·贝尔卡达、卡希夫·拉苏尔、路易斯·坦斯塔尔、利安德罗·冯·维拉、纳兹宁·拉贾尼和内森·兰伯特。2023年。[Stackllama:
    An rl fine-tuned llama model for stack exchange question and answering](https://doi.org/10.57967/hf/0513)。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) 魏林·蒋、朱焕·李、紫霖、应胜、张浩·吴、郝张、连敏·郑、思源·庄、永豪·庄、约瑟夫·E·冈萨雷斯、伊昂·斯托伊卡和埃里克·P·辛。2023年。Vicuna:
    An open-source chatbot impressing gpt-4 with 90%* chatgpt quality。'
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2022) 阿坎克莎·乔杜赫里、沙兰·纳朗、雅各布·德夫林、马滕·博斯马、古拉夫·米什拉、亚当·罗伯茨、保罗·巴赫姆、洪元钟、查尔斯·萨顿、塞巴斯蒂安·格赫曼等。2022年。Palm:
    Scaling language modeling with pathways。*arXiv预印本 arXiv:2204.02311*。'
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) 爱德华·J·胡、叶龙·申、菲利普·沃利斯、泽远·艾伦-朱、袁志·李、申·王、陆·王和魏竹·陈。2021年。Lora:
    Low-rank adaptation of large language models。*arXiv预印本 arXiv:2106.09685*。'
- en: 'Kung et al. (2023) Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun
    Peng. 2023. [Active instruction tuning: Improving cross-task generalization by
    training on prompt sensitive tasks](https://arxiv.org/abs/2311.00288). *Preprint*,
    arXiv:2311.00288.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kung et al. (2023) 孔博念、范银、迪·吴、凯-韦伊·张和南云·彭。2023年。[Active instruction tuning:
    Improving cross-task generalization by training on prompt sensitive tasks](https://arxiv.org/abs/2311.00288)。*预印本*，arXiv:2311.00288。'
- en: Köpf et al. (2023) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver
    Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri,
    Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023.
    [Openassistant conversations – democratizing large language model alignment](https://arxiv.org/abs/2304.07327).
    *Preprint*, arXiv:2304.07327.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf et al. (2023) 安德烈亚斯·科普夫、雅尼克·基尔彻、迪米特里·冯·吕特、索提里斯·阿纳格诺斯蒂迪斯、智睿·谭、基思·史蒂文斯、阿卜杜拉·巴赫姆、阮敏德、奥利弗·斯坦利、理查德·纳吉菲、沙胡尔·ES、萨米尔·苏里、大卫·格鲁什科夫、阿尔纳夫·丹图卢里、安德鲁·麦奎尔、克里斯托夫·舒曼、胡·阮和亚历山大·马蒂克。2023年。[Openassistant
    conversations – democratizing large language model alignment](https://arxiv.org/abs/2304.07327)。*预印本*，arXiv:2304.07327。
- en: 'Lee et al. (2024) Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2024. [Platypus:
    Quick, cheap, and powerful refinement of llms](https://arxiv.org/abs/2308.07317).
    *Preprint*, arXiv:2308.07317.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2024) 亚里尔·N·李、科尔·J·亨特和纳塔尼尔·鲁伊斯。2024年。[Platypus: Quick, cheap, and
    powerful refinement of llms](https://arxiv.org/abs/2308.07317)。*预印本*，arXiv:2308.07317。'
- en: Levesque et al. (2012) Hector Levesque, Ernest Davis, and Leora Morgenstern.
    2012. The winograd schema challenge. In *Thirteenth international conference on
    the principles of knowledge representation and reasoning*.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levesque 等 (2012) Hector Levesque、Ernest Davis 和 Leora Morgenstern。2012年。《Winograd
    结构挑战》。在 *第十三届国际知识表示与推理原理会议* 上发表。
- en: 'Li et al. (2024) Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen,
    Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024. [From quantity to
    quality: Boosting llm performance with self-guided data selection for instruction
    tuning](https://arxiv.org/abs/2308.12032). *Preprint*, arXiv:2308.12032.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2024) Ming Li、Yong Zhang、Zhitao Li、Jiuhai Chen、Lichang Chen、Ning Cheng、Jianzong
    Wang、Tianyi Zhou 和 Jing Xiao。2024年。[从数量到质量：通过自我引导的数据选择提升大语言模型的性能](https://arxiv.org/abs/2308.12032)。*预印本*，arXiv:2308.12032。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. [Webgpt: Browser-assisted
    question-answering with human feedback](https://arxiv.org/abs/2112.09332). *CoRR*,
    abs/2112.09332.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等 (2021) Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina
    Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders、Xu Jiang、Karl
    Cobbe、Tyna Eloundou、Gretchen Krueger、Kevin Button、Matthew Knight、Benjamin Chess
    和 John Schulman。2021年。[WebGPT: 浏览器辅助的基于人类反馈的问答系统](https://arxiv.org/abs/2112.09332)。*CoRR*，abs/2112.09332。'
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155).
    *Preprint*, arXiv:2203.02155.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    Christiano、Jan Leike 和 Ryan Lowe。2022年。[通过人类反馈训练语言模型以遵循指令](https://arxiv.org/abs/2203.02155)。*预印本*，arXiv:2203.02155。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reimers 和 Gurevych (2019) Nils Reimers 和 Iryna Gurevych。2019年。《Sentence-bert:
    使用 Siamese BERT 网络的句子嵌入》。在 *2019 年自然语言处理实证方法会议论文集* 上发表。计算语言学协会。'
- en: 'Sakaguchi et al. (2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2019. [Winogrande: An adversarial winograd schema challenge at
    scale](https://arxiv.org/abs/1907.10641). *Preprint*, arXiv:1907.10641.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi 等 (2019) Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin
    Choi。2019年。[Winogrande: 大规模对抗性 Winograd 结构挑战](https://arxiv.org/abs/1907.10641)。*预印本*，arXiv:1907.10641。'
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347).
    *Preprint*, arXiv:1707.06347.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等 (2017) John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford
    和 Oleg Klimov。2017年。[邻近策略优化算法](https://arxiv.org/abs/1707.06347)。*预印本*，arXiv:1707.06347。
- en: 'Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. [Active learning
    for convolutional neural networks: A core-set approach](https://arxiv.org/abs/1708.00489).
    *Preprint*, arXiv:1708.00489.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sener 和 Savarese (2018) Ozan Sener 和 Silvio Savarese。2018年。[用于卷积神经网络的主动学习：核心集方法](https://arxiv.org/abs/1708.00489)。*预印本*，arXiv:1708.00489。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *Preprint*, arXiv:2302.13971.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。2023年。[Llama: 开放且高效的基础语言模型](https://arxiv.org/abs/2302.13971)。*预印本*，arXiv:2302.13971。'
- en: Wang et al. (2024) Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and
    Dianhui Chu. 2024. [A survey on data selection for llm instruction tuning](https://arxiv.org/abs/2402.05123).
    *Preprint*, arXiv:2402.05123.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2024) Jiahao Wang、Bolin Zhang、Qianlong Du、Jiajun Zhang 和 Dianhui Chu。2024年。[关于大语言模型指令调整的数据选择调查](https://arxiv.org/abs/2402.05123)。*预印本*，arXiv:2402.05123。
- en: Wu et al. (2023) Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and
    Chang Zhou. 2023. [Self-evolved diverse data sampling for efficient instruction
    tuning](https://arxiv.org/abs/2311.08182). *Preprint*, arXiv:2311.08182.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2023）Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, 和 Chang Zhou.
    2023. [自演化多样数据采样用于高效指令调优](https://arxiv.org/abs/2311.08182)。*预印本*，arXiv:2311.08182。
- en: 'Zhao et al. (2018) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
    and Kai-Wei Chang. 2018. [Gender bias in coreference resolution: Evaluation and
    debiasing methods](https://arxiv.org/abs/1804.06876). *Preprint*, arXiv:1804.06876.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2018）Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, 和 Kai-Wei
    Chang. 2018. [核心指代解析中的性别偏见：评估与去偏见方法](https://arxiv.org/abs/1804.06876)。*预印本*，arXiv:1804.06876。
- en: Appendix A Appendix
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 Heterogeneous Dataset Creation
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 异构数据集创建
- en: In this section, we include an example of heterogeneous dataset creation. The
    following example is from WinoGrande, our primary fine-tuning dataset. It format
    follows that of Section [2.1](#S2.SS1 "2.1 Primary fine-tuning dataset ‣ 2 Framework
    ‣ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback"), where $A_{i,0}$
    is the correct response.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们包含了一个异构数据集创建的示例。以下示例来自WinoGrande，我们的主要微调数据集。它的格式遵循第[2.1节](#S2.SS1 "2.1
    Primary fine-tuning dataset ‣ 2 Framework ‣ A Framework for Fine-Tuning LLMs using
    Heterogeneous Feedback")的格式，其中 $A_{i,0}$ 是正确的响应。
- en: '$P_{i}$: ‘The box was still visible after James tried his best to manage the
    wrapper on it. James should have used the _ that is small.’'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$P_{i}$: ‘箱子在詹姆斯尽力管理包装纸后仍然可见。詹姆斯应该使用一个*较小*的 _。’'
- en: '$A_{i,0}$: ‘box’'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i,0}$: ‘box’'
- en: '$A_{i,1}$: ‘wrapper’'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i,1}$: ‘wrapper’'
- en: The following is from OASST, our secondary fine-tuning dataset. It contains
    a varying number of responses to each prompt, as well as numerical labels. In
    the experiment, we use the score toxicity. For brevity, we only include part of
    the score for the first response, but each response has several numerical scores.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容来自OASST，我们的次要微调数据集。它包含每个提示的不同数量的响应，以及数值标签。在实验中，我们使用分数毒性。为简洁起见，我们只包含第一个响应的部分分数，但每个响应都有几个数值分数。
- en: '$P_{i^{\prime}}$: ‘Which affordable GPU would you recommend to train a language
    model?’'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$P_{i^{\prime}}$: ‘你会推荐哪种经济实惠的GPU来训练语言模型？’'
- en: '$A_{i^{\prime},0}$: ‘It heavily depends on the size...’'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i^{\prime},0}$: ‘这在很大程度上取决于尺寸...’'
- en: '$A_{i^{\prime},1}$: ‘It is difficult to say...’'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i^{\prime},1}$: ‘很难说...’'
- en: '$A_{i^{\prime},2}$: ...'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i^{\prime},2}$: ...'
- en: '$y_{i,0}$: ‘toxicity’= 0.00038284, ‘spam’ = 0, ...'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$y_{i,0}$: ‘毒性’= 0.00038284, ‘垃圾邮件’ = 0, ...'
- en: 'To convert the data of type OASST into that of our primary dataset, we need
    to first choose two of the responses. If the prompt had only one response, it
    would be discarded. We select the most and least toxic responses assocaited with
    this prompt, and give the prompt a quality score based on that difference. For
    this example, we will consider this to be responses 0 and 1, with 1 the preferred
    response. After filtering for diversity and quality, the data entry for the unified
    dataset would have the following format:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将OASST类型的数据转换为我们主要数据集的类型，我们需要首先选择两个响应。如果提示只有一个响应，则会被丢弃。我们选择与该提示相关的最具毒性和最少毒性的响应，并根据这种差异为提示打分。对于这个示例，我们将其视为响应0和1，其中1为首选响应。在进行多样性和质量过滤后，统一数据集的数据条目将具有以下格式：
- en: '$P_{i^{\prime\prime}}$: ‘Which affordable GPU would you recommend to train
    a language model?’'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$P_{i^{\prime\prime}}$: ‘你会推荐哪种经济实惠的GPU来训练语言模型？’'
- en: '$A_{i^{\prime\prime},0}$: ‘It is diffucult to say...’'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i^{\prime\prime},0}$: ‘很难说...’'
- en: '$A_{i^{\prime\prime},1}$: ‘It heavily depends on the size...’'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '$A_{i^{\prime\prime},1}$: ‘这在很大程度上取决于尺寸...’'
- en: Now that the format is identical to that of our primary dataset, we take the
    union of the primary and secondary datasets to form the homogenized dataset. The
    same process is applied to any additional datasets. If separate datasets were
    used for SFT and RLHF, the process would be repeated.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在格式与我们主要数据集的格式相同，我们将主要数据集和次要数据集的联合形成同质化数据集。对任何额外的数据集应用相同的过程。如果对SFT和RLHF使用了不同的数据集，则会重复这个过程。
- en: A.2 Converting Supervision
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 转换监督
- en: 'In general, preference datasets can be supervised in one of three types, in
    order of increasing complexity: binary, ordinal, and numerical. As projecting
    a simpler supervision type can be noisy, we focus on simplifying supervision.
    In general, this is the recipe for converting a single axis of supervision:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，偏好数据集可以以三种类型中的一种进行监督，按复杂性递增的顺序：二元的、序数的和数值的。由于投影到简单的监督类型可能会带来噪声，我们专注于简化监督。一般来说，这是转换单一监督轴的配方：
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: numerical $\rightarrow$ sort increasing or decreasing, based on application
    $\rightarrow$ ordinal
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: numerical $\rightarrow$ 按应用进行升序或降序排序 $\rightarrow$ ordinal
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ordinal $\rightarrow$ extract best and worst responses to prompt $\rightarrow$
    binary
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ordinal $\rightarrow$ 提取提示的最佳和最差响应 $\rightarrow$ binary
- en: A.3 Hyperparameters
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 超参数
- en: We include the hyperparameters and LoRA configuration in this section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一部分包括了超参数和LoRA配置。
- en: 'Quality and Diversity Selection: We use all-MiniLM-L6-v2, a sentence transformer
    designed to capture semantic information, to generate embeddings for each prompt Reimers
    and Gurevych ([2019](#bib.bib12)). The data are then separated into 10 clusters
    with 10 restarts using k-means. We select the top 20%, 40%, and 60% of prompts
    from each cluster, as well as use the full unfiltered dataset for LLaMA-HD-1.0\.
    We perform stratified random sampling to preserve the fraction of samples from
    each of the datasets used in the experiment, maintaining the importance of each
    dataset relative to the unfiltered model.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '质量和多样性选择: 我们使用all-MiniLM-L6-v2，这是一种旨在捕捉语义信息的句子变换器，用于生成每个提示的嵌入 Reimers 和 Gurevych
    ([2019](#bib.bib12))。数据随后使用k-means算法分为10个簇，并进行10次重启。我们从每个簇中选择前20%、40%和60%的提示，同时使用未过滤的数据集用于LLaMA-HD-1.0。我们执行分层随机抽样，以保留实验中每个数据集的样本比例，保持每个数据集相对于未过滤模型的重要性。'
- en: 'SFT: Learning Rate: 1e-5, Maximum steps: 5000, Epochs: 1, Optimizer: Adamw,
    LR Scheduler: cosine, Maximum text length: 512, Batch size: 4, Grad accumulation
    steps: 1, weight decay: 0.05\. LoRA: Rank: 16, Alpha: 32, Dropout: 0.05.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'SFT: 学习率: 1e-5, 最大步数: 5000, 轮次: 1, 优化器: Adamw, 学习率调度器: cosine, 最大文本长度: 512,
    批量大小: 4, 梯度累积步数: 1, 权重衰减: 0.05。LoRA: 排名: 16, Alpha: 32, Dropout: 0.05。'
- en: 'Reward Model: Learning rate: 2e-5, Epochs: 1, Optimizer: Adamw, LR Scheduler:
    linear, Maximum text length: 512, Batch size: 4, Gradient accumulation steps:
    1, weight decay: 0.001\. LoRA: Rank: 8, Alpha: 32, Dropout: 0.1.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '奖励模型: 学习率: 2e-5, 轮次: 1, 优化器: Adamw, 学习率调度器: linear, 最大文本长度: 512, 批量大小: 4, 梯度累积步数:
    1, 权重衰减: 0.001。LoRA: 排名: 8, Alpha: 32, Dropout: 0.1。'
- en: 'RLHF: Learning Rate: 1.41e-5, Maximum steps: 20000, Epochs: 4, Minimum generation
    length: 32, Maximum generation length: 128, PPO Minibatch size: 1, Batch size:
    32, Gradient accumulation steps: 4\. LoRA: Rank: 16, Alpha: 32, Dropout: 0.05.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'RLHF: 学习率: 1.41e-5, 最大步数: 20000, 轮次: 4, 最小生成长度: 32, 最大生成长度: 128, PPO迷你批次大小:
    1, 批量大小: 32, 梯度累积步数: 4。LoRA: 排名: 16, Alpha: 32, Dropout: 0.05。'
- en: A.4 Quantitative Prompt
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 定量提示
- en: We list the prompt used for our qualitative experiment here. The format is based
    on the multiple choice one from PaLM Chowdhery et al. ([2022](#bib.bib3)). All
    quantitative metrics use this format. Details about scoring are included in Section [3.4](#S3.SS4
    "3.4 Metrics ‣ 3 Experimental Setup ‣ A Framework for Fine-Tuning LLMs using Heterogeneous
    Feedback").
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里列出了用于定性实验的提示。格式基于PaLM的多项选择题 Chowdhery 等人 ([2022](#bib.bib3))。所有定量指标都使用这种格式。有关评分的详细信息，请参见第[3.4](#S3.SS4
    "3.4 Metrics ‣ 3 Experimental Setup ‣ A Framework for Fine-Tuning LLMs using Heterogeneous
    Feedback")节。
- en: '‘{sentence} "{pronoun}" refers to: ’'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '‘{sentence} "{pronoun}" 指的是: ’'
- en: A.5 Qualitative Example
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 定性示例
- en: We report LLaMA-HD-0.2-S because it uses the smallest fraction of OASST, yet
    consistently demonstrates instruction following.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告LLaMA-HD-0.2-S，因为它使用了最小的OASST比例，但始终表现出良好的指令跟随能力。
- en: 'Prompt: ‘What can I do in Miami, FL in November?’'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '提示: ‘我在11月可以在迈阿密做些什么？’'
- en: 'LLaMA-SFT: ‘I’m going to the beach in the summer...’ LLaMA-HD-0.2-S: ‘ In November,
    you can enjoy the warm weather...’'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'LLaMA-SFT: ‘夏天我去海滩……’ LLaMA-HD-0.2-S: ‘在11月，你可以享受温暖的天气……’'
