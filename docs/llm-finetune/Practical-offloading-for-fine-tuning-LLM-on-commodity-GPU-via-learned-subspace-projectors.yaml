- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:35:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于学习子空间投影器的商品GPU微调LLM的实际卸载
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10181](https://ar5iv.labs.arxiv.org/html/2406.10181)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10181](https://ar5iv.labs.arxiv.org/html/2406.10181)
- en: Siyuan Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 陈思源
- en: Carnegie Mellon University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: siyuanc3@andrew.cmu.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: siyuanc3@andrew.cmu.edu
- en: '&Zelong Guan'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&关泽龙'
- en: Carnegie Mellon University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: zelongg@andrew.cmu.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zelongg@andrew.cmu.edu
- en: Yudong Liu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 刘玉栋
- en: Carnegie Mellon University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: yudongltech@gmail.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: yudongltech@gmail.com
- en: '&Phillip B. Gibbons'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&菲利普·B·吉本斯'
- en: Carnegie Mellon University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: gibbons@cs.cmu.edu Correspondence author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: gibbons@cs.cmu.edu 通讯作者。
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning large language models (LLMs) requires significant memory, often
    exceeding the capacity of a single GPU. A common solution to this memory challenge
    is offloading compute and data from the GPU to the CPU. However, this approach
    is hampered by the limited bandwidth of commodity hardware, which constrains communication
    between the CPU and GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLMs）需要大量内存，通常超出了单个GPU的容量。应对这一内存挑战的常见解决方案是将计算和数据从GPU转移到CPU。然而，这种方法受到商品硬件有限带宽的制约，这限制了CPU和GPU之间的通信。
- en: In this paper, we present an offloading framework, LSP-Offload, that enables
    near-native speed LLM fine-tuning on commodity hardware through learned subspace
    projectors. Our data-driven approach involves learning an efficient sparse compressor
    that minimizes communication with minimal precision loss. Additionally, we introduce
    a novel layer-wise communication schedule to maximize parallelism between communication
    and computation. As a result, our framework can fine-tune a 1.3 billion parameter
    model on a 4GB laptop GPU and a 7 billion parameter model on an NVIDIA RTX 4090
    GPU with 24GB memory, achieving only a 31% slowdown compared to fine-tuning with
    unlimited memory. Compared to state-of-the-art offloading frameworks, our approach
    increases fine-tuning throughput by up to 3.33 times and reduces end-to-end fine-tuning
    time by 33.1% 62.5% when converging to the same accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种卸载框架LSP-Offload，通过学习子空间投影器，在商品硬件上实现接近本地速度的LLM微调。我们基于数据的方法涉及学习一种高效的稀疏压缩器，以最小化通信并减少精度损失。此外，我们引入了一种新颖的逐层通信调度，以最大化通信和计算之间的并行性。结果是，我们的框架可以在4GB笔记本GPU上微调一个13亿参数的模型，在具有24GB内存的NVIDIA
    RTX 4090 GPU上微调一个70亿参数的模型，且与无限内存微调相比，只有31%的减速。与最先进的卸载框架相比，我们的方法将微调吞吐量提高了最多3.33倍，并在达到相同准确度时减少了33.1%至62.5%的端到端微调时间。
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Recent years have highlighted the remarkable success of billion scale LLMs.
    Hand-to-hand with task performance improvement are the ever-growing model sizes
    and the strong demand for powerful computing resources that are only available
    in high-end clusters. Fortunately, fine-tuning provides common ML practitioners
    the accessibility to LLMs by allowing them to adapt the pre-trained model to downstream
    tasks using less onerous computational effort. However, fine-tuning’s memory and
    compute demand are still daunting. For example, under a default fine-tuning configuration
    that uses the fp32 data type with the Adam optimizer [kingma2014adam](#bib.bib7)
    , the memory footprint is 16 $\times$ #Parameter, which makes the best consumer
    GPU devices (e.g., NVIDIA 4090 GPU and AMD 7900XTX with 24GB memory each) only
    able to hold the smallest LLM (1.5B parameters).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，亿级规模的LLM取得了显著的成功。随着任务性能的提升，模型规模不断扩大，并且对强大计算资源的需求也在增长，而这些资源仅在高端集群中可用。幸运的是，微调使得普通机器学习从业者能够通过较少的计算努力来适应下游任务，从而接触到LLM。然而，微调的内存和计算需求仍然令人畏惧。例如，在使用fp32数据类型和Adam优化器[kingma2014adam](#bib.bib7)的默认微调配置下，内存占用为16
    $\times$ #Parameter，这使得最好的消费级GPU设备（如NVIDIA 4090 GPU和AMD 7900XTX，每个24GB内存）只能容纳最小的LLM（1.5B参数）。'
- en: A variety of techniques have been proposed to reduce the memory demand during
    fine-tuning. A typical solution from system researchers is to offload part of
    the compute and memory from GPU to CPU, leveraging the fact that commodity laptop
    CPUs typically have 4x the memory of laptop GPUs and commodity server CPUs can
    provide 4TBs of memory (per socket). Although offloading is able to scale the
    trainable model size, large batch sizes are essential to remain efficient despite
    the limited PCIe bandwidth between CPU and GPU [zero-inifity](#bib.bib11) . In
    fact, we show that training with offloading is inherently bounded by either the
    CPU-GPU communication or the compute on CPU, especially in the consumer setting
    characterizing slower CPUs and batch sizes that are constrained by the limited
    GPU memory. Therefore, offloading itself can hardly save us from the scaling challenge.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 系统研究人员提出了各种技术来减少微调期间的内存需求。来自系统研究人员的典型解决方案是将部分计算和内存从GPU卸载到CPU，利用消费级笔记本CPU通常具有的内存是笔记本GPU的4倍以及商用服务器CPU可以提供4TB内存（每个插槽）。虽然卸载可以扩大可训练模型的大小，但大型批次对于保持高效至关重要，尽管CPU和GPU之间的PCIe带宽有限 [zero-inifity](#bib.bib11)
    。事实上，我们展示了使用卸载进行训练本质上受限于CPU-GPU通信或CPU上的计算，特别是在消费者设置中，特征是较慢的CPU和受限于有限GPU内存的批次大小。因此，卸载本身几乎无法解决规模扩展的挑战。
- en: Meanwhile, another promising method from ML researchers for memory-reduction
    is parameter-efficient fine-tuning (PEFT). The key idea of PEFT is to limit the
    trainable parameters to a carefully designed subspace (e.g., low rank subspace [hu2021lora](#bib.bib4)
    ; [zhao2024galore](#bib.bib18) , part of the model [guo2020parameter](#bib.bib3)
    ), so the GPU can train the model without offloading as long as it can hold the
    parameters and minimal optimizer states for the trainable parameters. However,
    though more memory-efficient, PEFT methods can suffer from slow convergence or
    sub-optimal training results due to their overly constrained space for parameter
    updates.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，机器学习研究人员提出的另一种有前景的内存减少方法是参数高效微调（PEFT）。PEFT的关键思想是将可训练参数限制在精心设计的子空间中（例如，低秩子空间 [hu2021lora](#bib.bib4)
    ; [zhao2024galore](#bib.bib18) ，模型的一部分 [guo2020parameter](#bib.bib3) ），因此，只要GPU能够容纳这些参数及其最小优化器状态，GPU就可以在没有卸载的情况下训练模型。然而，尽管PEFT方法更节省内存，但由于其对参数更新的空间限制过多，可能会导致收敛缓慢或训练结果不理想。
- en: In this paper, we show how to mitigate the memory challenge by combining both
    types of approaches. We present LSP-Offload, a novel fine-tuning framework that
    mitigates the bottlenecks in prior offloading approaches via a new approach for
    refactoring the offloading process but also trains efficiently via a new approach
    to constraining the optimization space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了通过结合两种方法来缓解内存挑战的方法。我们提出了LSP-Offload，这是一种新型的微调框架，它通过重新构建卸载过程的新方法来缓解先前卸载方法中的瓶颈，同时还通过限制优化空间的新方法来实现高效训练。
- en: Specifically, to alleviate the compute pressure on CPU as well as the communication
    overhead back-and-forth between CPU and GPU, we constrain the updates to happen
    on a periodically changing subspace. Since the updates from different subspaces
    are projected back and accumulate together in the original space, the model is
    able to update in the full-rank optimization space. Current approaches [hu2021lora](#bib.bib4)
    ; [zhao2024galore](#bib.bib18) for constraining the parameter update space suffer
    from linear memory and compute complexity that limits them from optimizing in
    large subspaces. We solve this problem by the introduction of $d$-sparse projectors,
    sparse embedding matrices that represent a subspace but whose size is independent
    of the subspace’s size. In this way, given same memory budget as PEFT, we are
    able to optimize in an arbitrary-size subspace. To further boost the compression
    quality of the subspace, we adopt a data-driven approach that adapts the subspace
    to the gradient matrices, which is empirically proven necessary for fast convergence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，为了缓解CPU的计算压力以及CPU和GPU之间的来回通信开销，我们将更新限制在周期性变化的子空间中。由于来自不同子空间的更新会被投影回并在原始空间中累积，因此模型能够在全秩优化空间中更新。当前的 [hu2021lora](#bib.bib4)
    ; [zhao2024galore](#bib.bib18) 方法在限制参数更新空间方面遭遇线性内存和计算复杂性的问题，这限制了它们在大子空间中的优化能力。我们通过引入$d$-稀疏投影器来解决这个问题，这些稀疏嵌入矩阵表示一个子空间，但其大小与子空间的大小无关。通过这种方式，给定与PEFT相同的内存预算，我们能够在任意大小的子空间中进行优化。为了进一步提升子空间的压缩质量，我们采用了一种数据驱动的方法，该方法将子空间适配到梯度矩阵上，经验上证明这是快速收敛所必需的。
- en: Moreover, on the system level, we identify limited parallelism between communication
    and compute in the SOTA offloading framework [zero](#bib.bib10) when run on consumer
    devices, since their limited GPU memory relative to model size implies that only
    small batch sizes can be trained efficiently. We improve the SOTA schedule by
    performing fine-grained communication on the granularity of layers and communicating
    components of the gradient ahead of time. The new schedule enables us to explore
    the full parallelism between CPU compute, GPU compute, CPU-to-GPU communication
    and GPU-to-CPU communication.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在系统层面，我们识别出在SOTA卸载框架 [zero](#bib.bib10)上运行时，通信和计算之间的并行性有限，因为相对于模型大小，其有限的GPU内存意味着只能有效地训练小批量。我们通过在层的粒度上进行精细通信，并提前传递梯度的组件，改进了SOTA调度。新的调度使我们能够探索CPU计算、GPU计算、CPU到GPU通信和GPU到CPU通信之间的完整并行性。
- en: 'In summary, our paper makes following contributions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的论文做出了以下贡献：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We do an analysis for training LLMs on the single commodity hardware to show
    that current offloading workflows are fundamentally bounded by either the communication
    or the CPU’s compute.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对在单一普通硬件上训练LLM进行分析，展示当前的卸载工作流程在通信或CPU计算上存在根本性限制。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design LSP-Offload to enable near-native-speed fine-tuning on commodity hardware.
    The system is built on the key idea of learned subspace projectors, which allows
    us to optimize on high-dimensional subspaces with constant memory and compute
    overhead.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了LSP-Offload，使得在普通硬件上能够实现接近原生速度的微调。该系统基于学习的子空间投影器的关键理念，这使得我们能够在高维子空间上进行优化，同时保持恒定的内存和计算开销。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify that LSP-Offload can converge at the same rate with native training
    on the GLUE dataset. Also, in the end-to-end comparison with SOTA offloading framework
    on the instruction-tuning task, we are able to achieve upto 3.33x higher training
    throughput and can converge to the same accuracy with 33.1% to 62.5% less of time.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们验证了LSP-Offload在GLUE数据集上能够以与原生训练相同的速度收敛。此外，在与SOTA卸载框架进行的端到端比较中，我们能够实现高达3.33倍的训练吞吐量提升，并且在时间上减少33.1%到62.5%而达到相同的准确率。
- en: 2 Background and Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: Memory breakdown for training large language models.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大型语言模型训练的内存分解。
- en: Training a deep learning model requires memory for parameters, activations,
    and optimizer states. Activations include the intermediate results used in backward
    propagation. The optimizer states are used by the optimizer to update the parameters.
    Out of the three, memory for parameters ($M_{param}$ bytes, which easily exceeds
    the single GPU’s memory for billion scale models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型需要内存来存储参数、激活和优化器状态。激活包括在反向传播中使用的中间结果。优化器状态由优化器用来更新参数。在这三者中，参数的内存（$M_{param}$字节）很容易超出单个GPU的内存，尤其是对于亿级规模的模型。
- en: 'Table 1: Configurations and timings for training/fine-tuning the llama-7B Model
    on the RTX 4090 and AMD Ryzen Threadripper 3970X CPU. For the Update stage, we
    measure the fused Adam kernel with thread-level parallelism and SIMD optimizations.
    For the Bandwidth, we measure the PCIe bandwidth with a pinned memory buffer.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在RTX 4090和AMD Ryzen Threadripper 3970X CPU上训练/微调llama-7B模型的配置和时间。对于更新阶段，我们测量了具有线程级并行和SIMD优化的融合Adam内核。对于带宽，我们测量了带有固定内存缓冲区的PCIe带宽。
- en: '| Parameters | Optimizer State | Activations | CPU-GPU Bandwidth | #Layer |
    GPU Memory |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 优化器状态 | 激活 | CPU-GPU带宽 | #层 | GPU内存 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 14GB | 42GB | 8GB | 10$\sim$20GB/s | 32 | 24GB |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 14GB | 42GB | 8GB | 10$\sim$20GB/s | 32 | 24GB |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FWD on CPU | BWD on CPU | UPD on CPU | FWD on GPU | BWD on GPU | UPD on GPU
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| CPU上的前向 | CPU上的反向 | CPU上的更新 | GPU上的前向 | GPU上的反向 | GPU上的更新 |'
- en: '| 1.61s/layer | 3.30s/layer | 0.06s/layer | 1.7ms/layer | 3.5ms/layer | 1ms/layer
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 1.61s/层 | 3.30s/层 | 0.06s/层 | 1.7ms/层 | 3.5ms/层 | 1ms/层 |'
- en: Memory offloading.
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存卸载。
- en: These techniques [g10](#bib.bib17) ; [swapadvisor](#bib.bib5) ; [zero](#bib.bib10)
    ; [zero-inifity](#bib.bib11) ; [zero-offload](#bib.bib12) scale up the training
    by extending with external memory like the CPU and the disk. Out of these approaches,
    Zero series are the state-of-the-art approaches for fine-tuning large models.
    Zero-Offload [zero-offload](#bib.bib12) offloads the optimizer states and the
    update step onto the CPU. Compared to other approaches that only offload the memory
    to CPU and do all computations on GPU, Zero-Offload achieves the optimal communication
    volume. Though optimal with regard to the communication volume, we found that
    Zero’s training is severely bottlenecked by the communication (see § [3](#S3 "3
    Motivation ‣ Practical offloading for fine-tuning LLM on commodity GPU via learned
    subspace projectors")). Our work is built on top of the Zero series offloading
    schedule to make it practical for single GPU training with minimal communication
    overhead.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术 [g10](#bib.bib17) ; [swapadvisor](#bib.bib5) ; [zero](#bib.bib10) ; [zero-inifity](#bib.bib11)
    ; [zero-offload](#bib.bib12) 通过扩展外部内存，如CPU和磁盘，来提升训练规模。在这些方法中，Zero系列是微调大型模型的最先进方法。Zero-Offload [zero-offload](#bib.bib12)
    将优化器状态和更新步骤卸载到CPU上。与其他仅将内存卸载到CPU并在GPU上进行所有计算的方法相比，Zero-Offload 实现了最佳的通信量。尽管在通信量方面是最佳的，但我们发现Zero的训练受到通信的严重瓶颈影响（见§ [3](#S3
    "3 Motivation ‣ Practical offloading for fine-tuning LLM on commodity GPU via
    learned subspace projectors")）。我们的工作建立在Zero系列卸载计划之上，使其在单GPU训练中具有最小的通信开销。
- en: Parameter-efficient fine-tuning.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数高效的微调。
- en: PEFT enables pre-trained models to rapidly adapt to downstream tasks with minimal
    extra memory required. LoRA [hu2021lora](#bib.bib4) is among the most popular
    PEFT techniques by constraining the updates to weight matrices that lie in a low-rank
    space. However, recent works [lialin2023relora](#bib.bib8) ; [valipour2022dylora](#bib.bib15)
    found LoRA is sensitive to hyperparameter tuning and can struggle with tasks requiring
    significant change to the base model. To break the low-dimensional constraint
    of LoRA, Galore [zhao2024galore](#bib.bib18) recently explores a similar idea
    to ours that periodically changes the subspace computed by the singular-value-decomposition.
    However, both LoRA and Galore have the limitation that their algorithms require
    extra memory and compute linear with the subspace’s size (rank), which inherently
    prevent them from tuning on a higher dimensional subspace. Our work mitigates
    this problem via novel subspace projectors whose compute and memory demands are
    independent of the subspace size, enabling us to achieve better model accuracy
    by tuning in a larger subspace.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT使预训练模型能够快速适应下游任务，且所需额外内存极少。LoRA [hu2021lora](#bib.bib4) 是最受欢迎的PEFT技术之一，通过将更新限制在低秩空间中的权重矩阵来实现。然而，近期的研究 [lialin2023relora](#bib.bib8)
    ; [valipour2022dylora](#bib.bib15) 发现LoRA对超参数调整非常敏感，并且在需要显著更改基础模型的任务中可能会遇到困难。为了解决LoRA的低维约束，Galore [zhao2024galore](#bib.bib18)
    最近探索了类似的想法，通过周期性地改变由奇异值分解计算的子空间。然而，LoRA和Galore都有一个限制，即它们的算法需要额外的内存，并且随着子空间的大小（秩）线性计算，这本质上阻止了它们在更高维度的子空间上进行微调。我们的工作通过新颖的子空间投影器来缓解这个问题，其计算和内存需求与子空间大小无关，从而使我们能够通过在更大子空间中微调来实现更好的模型准确性。
- en: Other methods for memory-efficient training.
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他内存高效训练的方法。
- en: Various approaches such as quantization [dettmers2024qlora](#bib.bib2) and gradient
    checkpointing [chen2016training](#bib.bib1) have been proposed to reduce the memory
    demand for training/fine-tuning LLMs. The quantization approach uses data types
    with fewer bits for training, and is fully compatible with our techniques. Meanwhile,
    the gradient checkpointing technique trades computation for memory by recomputing
    activations during the backward pass. We include this technique in our implementation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 各种方法如量化 [dettmers2024qlora](#bib.bib2) 和梯度检查点 [chen2016training](#bib.bib1)
    已被提出以减少训练/微调LLMs的内存需求。量化方法使用更少位的数据类型进行训练，并且与我们的技术完全兼容。同时，梯度检查点技术通过在反向传递过程中重新计算激活来用计算换取内存。我们在实现中包括了这一技术。
- en: 3 Motivation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机
- en: 3.1 Numerical Analysis for Training/Fine-tuning on a Single GPU
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 单GPU上的训练/微调数值分析
- en: In this section, we motivate our work by analyzing the fundamental limits of
    vanilla offloading on a single consumer-level GPU. We use the example of training/fine-tuning
    a llama-7B model on an Nvidia RTX 4090 GPU, which can provide only $24/(14+42+8)=37.5\%$
    of the required memory (Table [1](#S2.T1 "Table 1 ‣ Memory breakdown for training
    large language models. ‣ 2 Background and Related Work ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors")).¹¹1A similar
    analysis, with the same general conclusions, can be done for the GPT2-1.3B model
    on a laptop GPU, based on Table [4](#S9.T4 "Table 4 ‣ 9 Appendix ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors") in the
    Appendix.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们通过分析传统离线在单个消费者级 GPU 上的基本限制来激励我们的工作。我们以在 Nvidia RTX 4090 GPU 上训练/微调 llama-7B
    模型为例，它只能提供 $24/(14+42+8)=37.5\%$ 的所需内存（表 [1](#S2.T1 "Table 1 ‣ Memory breakdown
    for training large language models. ‣ 2 Background and Related Work ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors")）。¹¹1
    可以基于附录中表 [4](#S9.T4 "Table 4 ‣ 9 Appendix ‣ Practical offloading for fine-tuning
    LLM on commodity GPU via learned subspace projectors") 对笔记本 GPU 上的 GPT2-1.3B 模型进行类似分析，得出相同的总体结论。
- en: 'Current offloading techniques can be categorized into two classes: 1) those
    that offload only memory to CPU, and 2) those that offload both memory and compute
    to CPU. The first type is represented by [swapadvisor](#bib.bib5) ; [g10](#bib.bib17)
    which perform all compute on GPU while swapping in and out memory on the fly.
    An example of this type of schedule is shown in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1
    Numerical Analysis for Training/Fine-tuning on a Single GPU ‣ 3 Motivation ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors").c.
    However, this type of offloading schedule is inherently bounded by the communication
    under the following observation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的离线技术可以分为两类：1）仅将内存离线到 CPU 的技术，和 2）将内存和计算都离线到 CPU 的技术。第一类由 [swapadvisor](#bib.bib5)；[g10](#bib.bib17)
    代表，它们在 GPU 上执行所有计算，同时在运行时交换内存。此类调度的一个示例如图 [1](#S3.F1 "Figure 1 ‣ 3.1 Numerical
    Analysis for Training/Fine-tuning on a Single GPU ‣ 3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors").c 所示。然而，这种类型的离线调度在以下观察下本质上受限于通信：
- en: Observation. Training a model demanding $M_{tot}$ of communication per iteration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 观察。训练一个模型每次迭代需要 $M_{tot}$ 的通信量。
- en: With our example, we need approximately 2.67s communication every iteration
    for each of CPU-to-GPU and GPU-to-CPU communication, which adds 3.2x overhead
    compared to the compute on GPU even if the compute and communication are fully
    overlapped.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的例子来看，每次迭代我们需要大约 2.67 秒的通信时间用于 CPU 到 GPU 和 GPU 到 CPU 的通信，即使计算和通信完全重叠，也会比
    GPU 上的计算增加 3.2 倍的开销。
- en: The second type of offloading schedule splits the workload across CPU and GPU.
    Among the forward pass, backward pass, and parameter update, because of CPU’s
    poor computing power, only the parameter update step (UPD) is suitable to run
    on the CPU. For example, assigning the forward+backward pass of just one layer
    to the CPU directly adds 4.9s overhead, which is already 3.21x the GPU compute.
    Moreover, offloading UPD to CPU²²2More specifically, the computation of $\Delta
    W$ to the CPU—applying these deltas to the model parameters remains on the GPU.
    means that the 42GB optimizer state can reside on the CPU, enabling larger models
    like llama-7B to fit in the GPU memory.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种离线调度将工作负载分配到 CPU 和 GPU 之间。在前向传播、反向传播和参数更新中，由于 CPU 的计算能力较差，只有参数更新步骤（UPD）适合在
    CPU 上运行。例如，将单层的前向传播+反向传播直接分配到 CPU 上会增加 4.9 秒的开销，这已经是 GPU 计算的 3.21 倍。此外，将 UPD 离线到
    CPU²²2 更具体地说，将 $\Delta W$ 的计算放到 CPU 上——这些增量应用到模型参数上仍然在 GPU 上。这意味着 42GB 的优化器状态可以驻留在
    CPU 上，从而使得像 llama-7B 这样的大型模型能够适应 GPU 内存。
- en: Offloading UPD to CPU was first realized in Zero-Offload [zero-offload](#bib.bib12)
    , whose schedule is displayed in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1 Numerical Analysis
    for Training/Fine-tuning on a Single GPU ‣ 3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors").a. In their
    schedule, $M_{param}$ communication happens every iteration (gradients to CPU,
    deltas to GPU), which brings the communication overhead to 0.93s, but is still
    1.11x the GPU compute time. When there is no overlap between CPU compute and GPU
    compute, the training slowdown can reach 2.11x.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 卸载 UPD 到 CPU 首次在 Zero-Offload [zero-offload](#bib.bib12) 中实现，其调度如图 [1](#S3.F1
    "图 1 ‣ 3.1 单 GPU 上的训练/微调的数值分析 ‣ 3 动机 ‣ 通过学习子空间投影器对消费级 GPU 进行微调的实际卸载") 中所示。在他们的调度中，$M_{param}$
    通信每次迭代发生（梯度到 CPU，增量到 GPU），这使得通信开销达到 0.93 秒，但仍是 GPU 计算时间的 1.11 倍。当 CPU 计算与 GPU
    计算之间没有重叠时，训练减速可达到 2.11 倍。
- en: Moreover, the CPU computation can become the bottleneck for Zero’s schedule.
    In our example, it takes approximately 1.92s per iteration for parameter update
    on the CPU. Therefore, when CPU’s compute is not paralleled with the GPU, this
    slows down the training by 2.14x.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CPU 计算可能成为 Zero 调度的瓶颈。在我们的例子中，参数更新在 CPU 上大约需要 1.92 秒每次迭代。因此，当 CPU 计算未与 GPU
    并行时，这会使训练减速 2.14 倍。
- en: This analysis shows that training/fine-tuning with offloading is computationally
    inefficient on a consumer device due to fundamental bottlenecks in communication
    and/or CPU compute. This motivates us to design a lossy (PEFT) algorithm for reduced
    communication/compute overheads.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这项分析表明，由于通信和/或 CPU 计算的基本瓶颈，使用卸载进行训练/微调在消费级设备上计算效率低下。这促使我们设计了一种有损（PEFT）算法，以减少通信/计算开销。
- en: '![Refer to caption](img/d9ce200119658a5602793563467e593a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9ce200119658a5602793563467e593a.png)'
- en: 'Figure 1: Comparison between current offloading pipelines and LSP-Offload’s
    overlapped pipeline.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：当前卸载管道与 LSP-Offload 的重叠管道的比较。
- en: Algorithm 1 Zero-Offload’s Pseudo-code
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Zero-Offload 的伪代码
- en: 'Input: $M$: Datafor $t\leftarrow 1$$\triangleright$$\triangleright$ on GPU'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$M$: 数据为 $t\leftarrow 1$$\triangleright$$\triangleright$ 在 GPU 上'
- en: 3.2 Case Study on Zero’s Schedule
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Zero 调度的案例研究
- en: '![Refer to caption](img/9a576ba35a5f88465ec882e7437830b0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9a576ba35a5f88465ec882e7437830b0.png)'
- en: 'Figure 2: Normalized slowdown of Zero’s schedule. The breakdown for communication
    (COMM) depicts the additional slowdown due to communication that is not overlapped
    with GPU compute. Similarly, the CPU compute and Other are additional non-overlapped
    overheads. The experiments are done using precision fp16 under maximum allowed
    batch size with gradient checkpointing.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Zero 调度的归一化减速。通信（COMM）的细分显示了由于与 GPU 计算不重叠的通信带来的额外减速。同样，CPU 计算和其他是额外的非重叠开销。实验是在最大允许批次大小下使用
    fp16 精度和梯度检查点进行的。
- en: 'Moreover, prior offload schedules are suboptimal. Here we profile Zero-Offload’s
    schedule for a more comprehensive view of its performance. We chose two settings
    for profiling: 1) training a GPT2 model on a 4GB GPU representing the personal
    laptop, and 2) training a llama model on a 24GB GPU representing the workstation.
    The slowdown normalized by the GPU compute time is shown in Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Case Study on Zero’s Schedule ‣ 3 Motivation ‣ Practical offloading for
    fine-tuning LLM on commodity GPU via learned subspace projectors"). Under both
    configurations, Zero’s schedule lowers the training speed by 1.73x to 4.28x. We
    ascribe the slowdown to the following two reasons.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以前的卸载调度是次优的。在这里，我们对 Zero-Offload 的调度进行性能的更全面分析。我们选择了两种配置进行分析：1）在 4GB GPU
    上训练 GPT2 模型，代表个人笔记本电脑；2）在 24GB GPU 上训练 llama 模型，代表工作站。图 [2](#S3.F2 "图 2 ‣ 3.2
    Zero 调度的案例研究 ‣ 3 动机 ‣ 通过学习子空间投影器对消费级 GPU 进行微调的实际卸载") 中显示了 GPU 计算时间归一化的减速。在这两种配置下，Zero
    的调度将训练速度降低了 1.73 倍到 4.28 倍。我们将减速归因于以下两个原因。
- en: Communication and CPU compute overhead.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通信和 CPU 计算开销。
- en: The primary source of overhead comes from the unavoidable high communication
    volume and slow CPU compute as demonstrated in our previous analysis. Shown in
    Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Case Study on Zero’s Schedule ‣ 3 Motivation ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), though Zero is able to overlap part of the GPU/CPU compute with
    communication, the non-overlapped communication brings 0.61x to 2.09x added slowdown
    compared to the GPU compute time. For each GPU, the situation is worse for the
    larger model because the maximum available batch size decreases. When training
    a 1.3B model on a 4GB GPU, the non-overlapped communication and CPU compute are
    2.09x, 0.63x the GPU compute respectively.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的开销来源于不可避免的高通信量和慢速的CPU计算，这在我们之前的分析中已有展示。图[2](#S3.F2 "图 2 ‣ 3.2 零的调度案例 ‣ 3
    动机 ‣ 通过学习的子空间投影仪在普通GPU上进行微调的实际卸载")显示，尽管Zero能够将部分GPU/CPU计算与通信重叠，未重叠的通信带来了0.61倍到2.09倍的额外慢速，与GPU计算时间相比。对于每个GPU来说，模型越大情况越糟，因为可用的最大批量大小减少。当在4GB
    GPU上训练1.3B模型时，未重叠的通信和CPU计算分别是GPU计算的2.09倍和0.63倍。
- en: Limited parallelism between CPU and GPU, communication and compute.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CPU和GPU之间有限的并行性，通信和计算。
- en: 'The second source of overhead comes from Zero’s limited parallelism between
    compute and communication. Fig. [1](#S3.F1 "Figure 1 ‣ 3.1 Numerical Analysis
    for Training/Fine-tuning on a Single GPU ‣ 3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors").a shows
    Zero’s standard training pipeline, which is sub-optimal for two reasons: 1) The
    forward and backward pass on the GPU is not overlapped with the CPU’s compute.
    This results in significant slowdown when the CPU compute is around the same scale
    as the GPU’s compute. 2) No overlap exists between the GPU-to-CPU communication
    and CPU-to-GPU communication. As a result, the duplex PCIe channel, which is able
    to send and receive data between CPU and GPU in both directions at full bandwidth
    at the same time, is at least 50% under utilized.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个开销来源于Zero在计算和通信之间有限的并行性。图[1](#S3.F1 "图 1 ‣ 3.1 单GPU上的训练/微调的数值分析 ‣ 3 动机 ‣
    通过学习的子空间投影仪在普通GPU上进行微调的实际卸载").a展示了Zero的标准训练流程，这在两个方面是次优的：1) GPU上的前向和反向传递与CPU的计算未重叠。这在CPU计算与GPU计算规模相近时会导致显著的慢速。2)
    GPU到CPU通信和CPU到GPU通信之间没有重叠。因此，双工PCIe通道能够同时在两个方向以满带宽发送和接收数据，却至少有50%未被利用。
- en: 'Thus, the per-iteration time is:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每次迭代的时间为：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: To mitigate the first issue, Zero proposed delayed parameter updates (Fig. [1](#S3.F1
    "Figure 1 ‣ 3.1 Numerical Analysis for Training/Fine-tuning on a Single GPU ‣
    3 Motivation ‣ Practical offloading for fine-tuning LLM on commodity GPU via learned
    subspace projectors").b), which use stale parameter values to calculate current
    gradients, allowing the CPU to perform the previous step’s update at the same
    time the GPU performs the current step’s forward and backward passes. Though increasing
    throughput, this method can hurt training accuracy. Also, in order to not incur
    additional memory for buffering communication, the CPU-to-GPU communication and
    GPU-to-CPU communication cannot be parallelized.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解第一个问题，Zero提出了延迟参数更新（图[1](#S3.F1 "图 1 ‣ 3.1 单GPU上的训练/微调的数值分析 ‣ 3 动机 ‣ 通过学习的子空间投影仪在普通GPU上进行微调的实际卸载").b），该方法使用过时的参数值来计算当前的梯度，使CPU能够在GPU执行当前步骤的前向和反向传递时进行上一步的更新。虽然这种方法增加了吞吐量，但可能会影响训练准确性。此外，为了不增加缓冲通信的额外内存，CPU到GPU通信和GPU到CPU通信不能并行化。
- en: These limitations motivate us for a layer-wise schedule that enables maximal
    parallelism between CPU compute, GPU compute, CPU-to-GPU communication and GPU-to-CPU
    communication.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制促使我们设计了一种逐层调度的方法，以实现CPU计算、GPU计算、CPU到GPU通信以及GPU到CPU通信之间的最大并行性。
- en: 4 LSP-Offload’s Approach
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LSP-Offload的方法
- en: In this section, we present LSP-Offload, a practical offload framework for a
    commodity GPU that mitigates the problems identified in §[3](#S3 "3 Motivation
    ‣ Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"). We will first introduce our lossy training algorithm for reducing
    the communication and compute pressure of the gradient update step. Afterwards,
    we will illustrate our new schedule design for maximized parallelism in the offloading’s
    schedule.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了 LSP-Offload，这是一个实用的商品 GPU 卸载框架，可以缓解在 §[3](#S3 "3 Motivation ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors")
    中识别的问题。我们将首先介绍我们的有损训练算法，以减少梯度更新步骤中的通信和计算压力。之后，我们将阐述我们新设计的调度，以实现卸载调度中的最大并行性。
- en: 4.1 Communication/Compute-efficient Parameter Update via Learned Subspace Projectors
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 通过学习的子空间投影器实现通信/计算高效的参数更新
- en: 'As observed in the prior section, the communication and compute complexity
    of current gradient updates cannot be improved without changes to the training
    algorithm. Therefore, a “lossy” approach is a necessity for practical offloading.
    Ideally, the new training algorithm should: 1) reduce the communication amount,
    2) reduce the computational complexity for the parameter update on CPU, and 3)
    converge at the similar rate with the standard training algorithm.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，当前梯度更新的通信和计算复杂度无法在不改变训练算法的情况下得到改善。因此，“有损”方法是实际卸载的必要条件。理想情况下，新的训练算法应该：1)
    减少通信量，2) 减少 CPU 上参数更新的计算复杂度，3) 与标准训练算法以类似的速度收敛。
- en: 'To obtain such an algorithm, we borrow the key idea from the PEFT field that
    constrains the optimization in a subspace. Specifically, we consider the matrix
    multiplication operations in the network, which cover over $90\%$, where $s\ll\min(m,n)$,
    the subspace and approximated gradient are given by:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这样的算法，我们借鉴了 PEFT 领域中的关键思想，即在子空间中约束优化。具体来说，我们考虑网络中的矩阵乘法操作，这些操作覆盖了超过 $90\%$
    的部分，其中 $s\ll\min(m,n)$，子空间和近似梯度由下式给出：
- en: '|  | $1$2 |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In this way, we constrain the parameter updates to the subspace $S$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们将参数更新约束在子空间 $S$ 中。
- en: 'Table 2: Additional time and space complexity of different subspace training
    methods. $m,n$ is the number of nonzero values per row in our projectors.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同子空间训练方法的额外时间和空间复杂度。$m,n$ 是我们投影器中每行的非零值数量。'
- en: '|  | LoRA [hu2021lora](#bib.bib4) | Galore [zhao2024galore](#bib.bib18) | LSP-Offload
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA [hu2021lora](#bib.bib4) | Galore [zhao2024galore](#bib.bib18) | LSP-Offload
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Time Complexity | $O(mns)$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 时间复杂度 | $O(mns)$ |'
- en: '| Space Complexity | $O((m+n)s)$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 空间复杂度 | $O((m+n)s)$ |'
- en: Next, we need to come up with good bases to represent the subspace, which is
    our key contribution. As discussed in §[3](#S3 "3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors") and shown
    in Table [2](#S4.T2 "Table 2 ‣ 4.1 Communication/Compute-efficient Parameter Update
    via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors"), both LoRA
    and Galore have $O(mns)$ of storage in half precision, which is an unignorable
    $3/14=21\%$ of the parameter memory.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要提出良好的基准来表示子空间，这是我们主要的贡献。正如在 §[3](#S3 "3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors") 中讨论的以及在表
    [2](#S4.T2 "Table 2 ‣ 4.1 Communication/Compute-efficient Parameter Update via
    Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors") 中所示，LoRA
    和 Galore 在半精度下的存储为 $O(mns)$，这占用了不可忽视的 $3/14=21\%$ 的参数内存。
- en: Algorithm 2 LSP-Offload’s training/fine-tuning with learned sparse projectors
    (simplified version without layer-wise scheduling)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 LSP-Offload 的训练/微调与学习的稀疏投影器（简化版，不包含逐层调度）
- en: 'HyperParam: $s$. $CheckFreq,threshold$: previous projectors)     $P,Q\leftarrow
    Initialize(d)$     return $P,Q$: Weights, $M,V\in\mathbb{R}^{s\times s}:$         if $t\mod
    CheckFreq=0$.              if $\|PP^{T}\nabla f_{S}(W)QQ^{T}-\nabla f_{S}(W)\|_{F}^{2}\geq
    threshold\cdot\|\nabla f_{S}(W)\|$, $Q$ FWD+BWD on GPU         $grad\leftarrow
    SendToCPU(P^{T}\nabla f_{x}(W)Q)$ UPD on CPU and delta upload         $W\leftarrow
    W+\eta_{t}P\Delta_{W}Q^{T}$ Decompress and apply deltas on GPU'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 'HyperParam: $s$. $CheckFreq,threshold$: 之前的投影器) $P,Q\leftarrow Initialize(d)$
    返回 $P,Q$: 权重, $M,V\in\mathbb{R}^{s\times s}:$ 如果 $t\mod CheckFreq=0$。如果 $\|PP^{T}\nabla
    f_{S}(W)QQ^{T}-\nabla f_{S}(W)\|_{F}^{2}\geq threshold\cdot\|\nabla f_{S}(W)\|$，$Q$
    在 GPU 上进行前向和后向传播 $grad\leftarrow SendToCPU(P^{T}\nabla f_{x}(W)Q)$ 在 CPU 上更新和增量上传
    $W\leftarrow W+\eta_{t}P\Delta_{W}Q^{T}$ 在 GPU 上解压和应用增量'
- en: In this work, we propose to solve this problem by using a sparse projector similar
    to the JL-sparse embedding matrix [kane2014sparser](#bib.bib6) whose time and
    space complexities are independent of the size of the subspace.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出通过使用类似于JL-稀疏嵌入矩阵 [kane2014sparser](#bib.bib6)的稀疏投影器来解决这个问题，其时间和空间复杂度与子空间的大小无关。
- en: Definition 1  (Sparse Projector).
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1 （稀疏投影器）。
- en: We call the projection bases $P\in\mathbb{R}^{n\times s},Q\in\mathbb{R}^{m\times
    s}$ nonzeros values per row.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称投影基为$P\in\mathbb{R}^{n\times s},Q\in\mathbb{R}^{m\times s}$的每行非零值。
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 Communication/Compute-efficient
    Parameter Update via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), a pair of $d$ time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [2](#S4.T2 "表 2 ‣ 4.1 通过学习的子空间投影器进行的通信/计算高效参数更新 ‣ 4 LSP-Offload的方案 ‣ 使用学习的子空间投影器在商品GPU上对LLM进行细调的实际卸载")所示，一对$d$时间。
- en: Next, we describe our training algorithm with the $d$ as $f_{S}(W):=\frac{1}{|\mathcal{S}|}\Sigma_{x\in\mathcal{S}}f_{x}(W)$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们描述了我们的训练算法，其中$d$表示为$f_{S}(W):=\frac{1}{|\mathcal{S}|}\Sigma_{x\in\mathcal{S}}f_{x}(W)$。
- en: To find a good $d$. As shown in Alg. [2](#alg2 "Algorithm 2 ‣ 4.1 Communication/Compute-efficient
    Parameter Update via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), we periodically check the quality of the empirical bias on a sub
    sampled dataset. When the bias exceeds a certain threshold, we update the subspace
    to reduce the empirical bias.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到一个好的$d$。如算法 [2](#alg2 "算法 2 ‣ 4.1 通过学习的子空间投影器进行的通信/计算高效参数更新 ‣ 4 LSP-Offload的方案
    ‣ 使用学习的子空间投影器在商品GPU上对LLM进行细调的实际卸载")所示，我们定期检查子采样数据集上经验偏差的质量。当偏差超过某个阈值时，我们更新子空间以减少经验偏差。
- en: Compared to Galore [zhao2024galore](#bib.bib18) who perform the update step
    periodically, our check-and-update mechanism guarantees the quality of the subspace
    while minimizes the frequency of the update operation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与定期执行更新步骤的Galore [zhao2024galore](#bib.bib18)相比，我们的检查和更新机制在保证子空间质量的同时，最小化了更新操作的频率。
- en: In terms of convergence, we characterize the quality of the subspace by Assumption [3](#Thmassumption3
    "Assumption 3 (Effectiveness of the subspace). ‣ 4.1 Communication/Compute-efficient
    Parameter Update via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"). Under the assumptions on the norm bound and sparsity of the bias,
    we are able to show the convergence of our algorithm in The. [1](#Thmtheorem1
    "Theorem 1\. ‣ 4.1 Communication/Compute-efficient Parameter Update via Learned
    Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣ Practical offloading for fine-tuning
    LLM on commodity GPU via learned subspace projectors") following the analysis
    in  [stich2020analysis](#bib.bib13) . Compared to vanilla training, the bound
    for time-to-convergence is increased by a factor of $\frac{1}{1-2c^{2}\alpha^{2}}$.
    Moreover, only logarithmic-scale samples are needed in the sub-sampled dataset
    for convergence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在收敛性方面，我们通过假设 [3](#Thmassumption3 "假设 3（子空间的有效性）。 ‣ 4.1 通过学习的子空间投影器进行的通信/计算高效参数更新
    ‣ 4 LSP-Offload的方案 ‣ 使用学习的子空间投影器在商品GPU上对LLM进行细调的实际卸载")来刻画子空间的质量。在对偏差的范数界限和稀疏性的假设下，我们能够在定理 [1](#Thmtheorem1
    "定理 1。 ‣ 4.1 通过学习的子空间投影器进行的通信/计算高效参数更新 ‣ 4 LSP-Offload的方案 ‣ 使用学习的子空间投影器在商品GPU上对LLM进行细调的实际卸载")中展示我们算法的收敛性，依据 [stich2020analysis](#bib.bib13)的分析。与普通训练相比，收敛时间的界限增加了$\frac{1}{1-2c^{2}\alpha^{2}}$倍。此外，只需对子采样数据集进行对数规模的样本。
- en: Assumption 1  (Bounded Bias).
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 1 （有界偏差）。
- en: There exists $$\gamma>.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 存在$$\gamma>。
- en: Assumption 2  (Sparse Bias).
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 2 （稀疏偏差）。
- en: There exists constant , under Assumptions [1](#Thmassumption1 $, with probability
    $1-\delta$,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何$\beta>$，在假设 [1](#Thmassumption1 $，以概率$1-\delta$，
- en: '|  | $T=\mathcal{O}(\frac{1}{\epsilon})\cdot\frac{LF}{(1-2c^{2}\alpha^{2})}$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $T=\mathcal{O}(\frac{1}{\epsilon})\cdot\frac{LF}{(1-2c^{2}\alpha^{2})}$
    |  |'
- en: iterations are sufficient to obtain $1$2.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数足以获得$1$2。
- en: During the update, we initialize the sparse embedding by first choosing non-zero
    positions in each row, and then assigning values to them under normal distribution
    $\mathcal{N}(0,\frac{1}{\sqrt{d}})$, which yields an unbiased estimation of the
    gradient.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新过程中，我们通过首先选择每一行中的非零位置，然后在正态分布 $\mathcal{N}(0,\frac{1}{\sqrt{d}})$ 下分配值，从而初始化稀疏嵌入，这样可以获得梯度的无偏估计。
- en: 'Afterwards, we fix the non-zero positions and optimize the values in the sparse
    embedding matrix to minimize the empirical bias. We define our loss function as
    the bias with norm regularization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们固定非零位置并优化稀疏嵌入矩阵中的值，以最小化经验偏差。我们将损失函数定义为带有范数正则化的偏差：
- en: '|  | $loss:=\&#124;PP^{T}GQQ^{T}-G\&#124;_{F}^{2}+\gamma\cdot(\&#124;P\&#124;_{F}^{2}+\&#124;Q\&#124;_{F}^{2})$
    |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $loss:=\&#124;PP^{T}GQQ^{T}-G\&#124;_{F}^{2}+\gamma\cdot(\&#124;P\&#124;_{F}^{2}+\&#124;Q\&#124;_{F}^{2})$
    |  |'
- en: Finally, when trained with Adam optimizer, we need to project the old momentum
    and velocity tensor to the new subspace.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当使用 Adam 优化器进行训练时，我们需要将旧的动量和速度张量投影到新的子空间。
- en: Because we are periodically learning a new pair of $d$-sparse projectors as
    the training/fine-tuning proceeds, we can fine-tune to higher accuracies than
    approaches like LoRA that constrain the possible adjustments to the base model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在训练/微调过程中定期学习一对新的 $d$-稀疏投影器，我们可以微调到比像 LoRA 这样的方案更高的准确度，这些方案限制了对基础模型的可能调整。
- en: Algorithm 3 Layer-wise Scheduling
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 层级调度
- en: 'Hyperparameter: $TransitionLayer:$     for $l$ The forward pass happens after
    the parameter gets updated         $x_{l}\leftarrow forward(x_{l-1},l,W_{l})$
    in $reversed(layers)$         $e_{l}\leftarrow AsyncSchedule(SchMode,\nabla_{W_{l}},Stream_{G2C})$'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数：$TransitionLayer:$     对于 $l$ 参数更新后进行前向传递         $x_{l}\leftarrow forward(x_{l-1},l,W_{l})$
    在 $reversed(layers)$ 中         $e_{l}\leftarrow AsyncSchedule(SchMode,\nabla_{W_{l}},Stream_{G2C})$
- en: 4.2 Layer-wise Schedule for Maximal Parallelism
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 最大并行度的层级调度
- en: 'On the system level, we propose a new schedule that solves both issues in Zero’s
    schedule based on the observation that there is no dependency between different
    layer’s optimizer update steps. Because of this, we are able to overlap different
    layers’ GPU compute, CPU-GPU communication in both directions, and the parameter
    update on the CPU. Alg.[3](#alg3 "Algorithm 3 ‣ 4.1 Communication/Compute-efficient
    Parameter Update via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors") presents pseudo-code for our new highly-parallel layer-wise schedule.
    The key idea and its benefits are illustrated in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1
    Numerical Analysis for Training/Fine-tuning on a Single GPU ‣ 3 Motivation ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors").d.
    We split the GPU-to-CPU, CPU update, CPU-to-GPU communication into small blocks
    to unlock the parallelism between layers without the accuracy loss of Zero’s use
    of stale parameter values. We parallelize the CPU’s and GPU’s compute by executing
    the deeper layers’ update step on CPU while doing the backward pass of shallower
    layers on GPU. We also parallelize the double-sided communication by executing
    deeper layer’s upload step while doing the shallower layer’s offload step. Thus,
    in our schedule, the critical path of the training can be characterized by:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统层面上，我们提出了一种新的调度方法，解决了 Zero 的调度中的两个问题，这基于观察到不同层的优化器更新步骤之间没有依赖关系。因此，我们能够重叠不同层的
    GPU 计算、CPU-GPU 双向通信和 CPU 上的参数更新。算法 [3](#alg3 "算法 3 ‣ 4.1 通过学习的子空间投影器进行通信/计算高效的参数更新
    ‣ 4 LSP-Offload 的方法 ‣ 通过学习的子空间投影器在商用 GPU 上进行微调的实际卸载") 展示了我们新的高并行层级调度的伪代码。关键思想及其好处在图
    [1](#S3.F1 "图 1 ‣ 3.1 在单 GPU 上训练/微调的数值分析 ‣ 3 动机 ‣ 通过学习的子空间投影器在商用 GPU 上进行微调的实际卸载")d
    中有所展示。我们将 GPU 到 CPU、CPU 更新、CPU 到 GPU 的通信分解为小块，以解锁层之间的并行性，而不会因为 Zero 使用过时参数值而导致准确度损失。我们通过在
    CPU 上执行较深层的更新步骤，同时在 GPU 上进行较浅层的反向传递来并行化 CPU 和 GPU 的计算。我们还通过在执行较深层的上传步骤时进行较浅层的卸载步骤来并行化双向通信。因此，在我们的调度中，训练的关键路径可以通过以下方式描述：
- en: '|  | $1$2 |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Compared to Eqn. [1](#S3.E1 "In Limited parallelism between CPU and GPU, communication
    and compute. ‣ 3.2 Case Study on Zero’s Schedule ‣ 3 Motivation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors"), LSP-Offload
    is able to reduce the CPU’s involvement in the critical path from the entire parameter
    update step to the update for only one layer, a 32x improvement for the llama-7B
    model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与公式 [1](#S3.E1 "CPU 和 GPU 之间有限的并行性，通信和计算。 ‣ 3.2 零调度案例研究 ‣ 3 动机 ‣ 通过学习子空间投影器在普通
    GPU 上微调 LLM 的实际卸载") 相比，LSP-Offload 能够将 CPU 在关键路径中的参与度从整个参数更新步骤减少到仅更新一层，这为 llama-7B
    模型带来了 32 倍的改进。
- en: 'Further, to avoid the deeper layer’s workload from blocking the shallower layer’s
    computation which involves earlier in the next iteration, we use a heuristic to
    switch between two schedule mode: $FirstComeFirstServe$, which is the deepest
    layer that may block the computation of the first layer.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了避免更深层次的工作负载阻塞较浅层次的计算（这些计算在下一次迭代中涉及），我们使用启发式方法在两种调度模式之间切换：$FirstComeFirstServe$，这是最深的层次，可能会阻塞第一层的计算。
- en: 5 Implementation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实现
- en: We propotyped LSP-Offload as a Python library built on top of Pytorch. LSP-Offload
    can automatically detect the matrix multiplication modules and replace it with
    the offloaded version without user’s interference. To achieve best performance,
    we implemented the fused Adam kernel in Zero-Offload to accelerate the parameter
    update on CPU. Also, we used the Pinned Memory buffer on CPU to enable fast communication,
    and used CUDA streams for paralleled communication and computation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 LSP-Offload 原型化为一个基于 Pytorch 的 Python 库。LSP-Offload 可以自动检测矩阵乘法模块，并用卸载版本替换它，而无需用户干预。为了实现最佳性能，我们在
    Zero-Offload 中实现了融合的 Adam 核心，以加速 CPU 上的参数更新。此外，我们使用了 CPU 上的固定内存缓冲区以实现快速通信，并使用了
    CUDA 流进行并行通信和计算。
- en: 6 Evaluation
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: In evaluation, we first verify the convergence of the LSP training approach
    on the GLUE dataset and then evaluate the end-to-end training performance on the
    instruction-tuning task. Detailed configurations for the experiments are described
    in the §[9.2](#S9.SS2 "9.2 Experiment Configurations ‣ 9 Appendix ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors").
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中，我们首先验证了 LSP 训练方法在 GLUE 数据集上的收敛性，然后评估了在指令调优任务上的端到端训练性能。实验的详细配置在 §[9.2](#S9.SS2
    "9.2 实验配置 ‣ 9 附录 ‣ 通过学习子空间投影器在普通 GPU 上微调 LLM 的实际卸载") 中描述。
- en: Convergence and Accuracy validation of LSP training on GLUE
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LSP 训练在 GLUE 上的收敛性和准确性验证
- en: First of all, we verify the convergence and accuracy of Alg. [2](#alg2 "Algorithm
    2 ‣ 4.1 Communication/Compute-efficient Parameter Update via Learned Subspace
    Projectors ‣ 4 LSP-Offload’s Approach ‣ Practical offloading for fine-tuning LLM
    on commodity GPU via learned subspace projectors") by fine-tuning pre-trained
    RobertA-base[liu2019roberta](#bib.bib9) (117M) model on the GLUE [wang2018glue](#bib.bib16)
    dataset, which is a language understanding task sets that are widely adopted in
    the fine-tuning’ s evaluation[hu2021lora](#bib.bib4) ; [zhao2024galore](#bib.bib18)
    . For hyper parameters, We set both the rank of Galore’ s projector and the non-zero
    entries per row in the LSP algorithm to be 16\. We set the projection space of
    LSP to be 512\. As both Galore and LSP need additional profiling time, we make
    an end-to-end comparison that allow all candidates to train under an hour’s time
    budget.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过在 GLUE [wang2018glue](#bib.bib16) 数据集上微调预训练的 RobertA-base[liu2019roberta](#bib.bib9)
    (117M) 模型，验证了算法 [2](#alg2 "算法 2 ‣ 4.1 通过学习子空间投影器的通信/计算高效参数更新 ‣ 4 LSP-Offload 的方法
    ‣ 通过学习子空间投影器在普通 GPU 上微调 LLM 的实际卸载") 的收敛性和准确性，该数据集是广泛应用于微调评估的语言理解任务集 [hu2021lora](#bib.bib4)
    ; [zhao2024galore](#bib.bib18)。对于超参数，我们将 Galore 投影器的秩和 LSP 算法中的每行非零条目都设置为 16。我们将
    LSP 的投影空间设置为 512。由于 Galore 和 LSP 需要额外的分析时间，我们进行了一次端到端的比较，允许所有候选者在一个小时的时间预算内训练。
- en: Shown in Fig. [3](#S6.F3 "Figure 3 ‣ Convergence and Accuracy validation of
    LSP training on GLUE ‣ 6 Evaluation ‣ Practical offloading for fine-tuning LLM
    on commodity GPU via learned subspace projectors"), for all cases in GLUE, the
    LSP algorithm is able to converge at the same rate with the full parameter tuning.
    In fact, as displayed in Tab. [3](#S6.T3 "Table 3 ‣ Convergence and Accuracy validation
    of LSP training on GLUE ‣ 6 Evaluation ‣ Practical offloading for fine-tuning
    LLM on commodity GPU via learned subspace projectors"), LSP is able to outperform
    full parameter tuning, which indicates constraining the update space can help
    with the performance. Meanwhile, compared to Galore, we are able to achieve 1%
    higher averaged accuracy. We attribute this to the LSP algorithm’s larger parameter
    update space, which is 1024x for this experiment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S6.F3 "图 3 ‣ LSP 在 GLUE 上的收敛性和准确性验证 ‣ 6 评估 ‣ 通过学习的子空间投影器在通用 GPU 上进行
    LLM 微调的实际卸载") 所示，在 GLUE 的所有情况下，LSP 算法的收敛速度与全参数调优相同。实际上，如表 [3](#S6.T3 "表 3 ‣ LSP
    在 GLUE 上的收敛性和准确性验证 ‣ 6 评估 ‣ 通过学习的子空间投影器在通用 GPU 上进行 LLM 微调的实际卸载") 所示，LSP 能够超越全参数调优，这表明限制更新空间有助于性能。同时，与
    Galore 相比，我们能够实现高出 1% 的平均准确率。我们将此归因于 LSP 算法的更新空间更大，本实验中为 1024 倍。
- en: '![Refer to caption](img/595deb946cea8c8fbcdbe39f7ce17113.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/595deb946cea8c8fbcdbe39f7ce17113.png)'
- en: 'Figure 3: Convergence Validation of LSP by finetuning pre-trained RoBertA-base
    model on GLUE.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：通过在 GLUE 上微调预训练 RoBertA-base 模型来验证 LSP 的收敛性。
- en: 'Table 3: Accuracy Validation of LSP by finetuning pre-trained RoBertA-base
    model on GLUE'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：通过在 GLUE 上微调预训练 RoBertA-base 模型来验证 LSP 的准确性
- en: '|  | Memory | #Trainable | MNLI | SST2 | MRPC | CoLA | QNLI | QQP | SST2 |
    STS-B | Avg |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 内存 | 可训练参数数 | MNLI | SST2 | MRPC | CoLA | QNLI | QQP | SST2 | STS-B |
    平均 |'
- en: '| Full Parameter | 747M | 747M | 0.8111 | 0.934 | 0.866 | 0.55 | 0.904 | 0.808
    | 0.933 | 0.884 | 0.8362625 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 全参数 | 747M | 747M | 0.8111 | 0.934 | 0.866 | 0.55 | 0.904 | 0.808 | 0.933
    | 0.884 | 0.8362625 |'
- en: '| Galore (Rank = 16) | 253M | 18K | 0.83 | 0.92 | 0.88 | 0.567 | 0.881 | 0.852
    | 0.92 | 0.9 | 0.84375 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Galore (Rank = 16) | 253M | 18K | 0.83 | 0.92 | 0.88 | 0.567 | 0.881 | 0.852
    | 0.92 | 0.9 | 0.84375 |'
- en: '| LSP (S: 512, d: 16) | 253M | 18M | 0.814 | 0.917 | 0.911 | 0.6165 | 0.9178
    | 0.8339 | 0.922 | 0.91 | 0.855275 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LSP (S: 512, d: 16) | 253M | 18M | 0.814 | 0.917 | 0.911 | 0.6165 | 0.9178
    | 0.8339 | 0.922 | 0.91 | 0.855275 |'
- en: End-to-end evaluation of the LSP-Offload on Alpaca.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 Alpaca 上对 LSP-Offload 的端到端评估。
- en: 'Next, we evaluate the end-to-end performance of LSP-Offload by fine-tuning
    on the instruction-tuning dataset Alpaca [alpaca](#bib.bib14) . We perform our
    evaluation in two settings: 1) fine-tuning the GPT2 (774M) model on a laptop with
    Nvidia A1000 Laptop GPU (4GB) and Intel Core-i7 12800H CPU (32GB), and 2) fine-tuning
    a Llama-3B model on commodity workstation with Nvidia RTX 4090 GPU (24 GB) and
    AMD Ryzen Threadripper 3970X CPU (252GB). We compared LSP-Offload with Zero-Offload
    for full-parameter tuning, as well as LoRA for PEFT fine-tuning.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过在指令调优数据集 Alpaca [alpaca](#bib.bib14) 上微调来评估 LSP-Offload 的端到端性能。我们在两个设置中进行评估：1）在配备
    Nvidia A1000 笔记本 GPU（4GB）和 Intel Core-i7 12800H CPU（32GB）的笔记本电脑上微调 GPT2（774M）模型；2）在配备
    Nvidia RTX 4090 GPU（24 GB）和 AMD Ryzen Threadripper 3970X CPU（252GB）的通用工作站上微调 Llama-3B
    模型。我们将 LSP-Offload 与 Zero-Offload（用于全参数微调）以及 LoRA（用于 PEFT 微调）进行比较。
- en: '![Refer to caption](img/47a8bfb24d4015b22ab8effdffab99e0.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47a8bfb24d4015b22ab8effdffab99e0.png)'
- en: (a) Evaluation PPL. of fine-tuning GPT2-774M w/ the laptop GPU.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用笔记本 GPU 对 GPT2-774M 进行微调的评估 PPL。
- en: '![Refer to caption](img/0f9509d14b0b0d4303a022bae16f65ce.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f9509d14b0b0d4303a022bae16f65ce.png)'
- en: (b) Evaluation PPL. of fine-tuning the Llama-3B model w/ the workstation GPU.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用工作站 GPU 对 Llama-3B 模型进行微调的评估 PPL。
- en: '![Refer to caption](img/d7e50398d164d378e06deab77f7dee14.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7e50398d164d378e06deab77f7dee14.png)'
- en: (c) Training Throughput Comparison
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 训练吞吐量比较
- en: 'Figure 4: End-to-end evaluation of LSP-Offload by finetuning LLM on the Alpaca
    Dataset. For Fig. [4(a)](#S6.F4.sf1 "In Figure 4 ‣ End-to-end evaluation of the
    LSP-Offload on Alpaca. ‣ 6 Evaluation ‣ Practical offloading for fine-tuning LLM
    on commodity GPU via learned subspace projectors"), Fig. [4(b)](#S6.F4.sf2 "In
    Figure 4 ‣ End-to-end evaluation of the LSP-Offload on Alpaca. ‣ 6 Evaluation
    ‣ Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), rolling average is applied for drawing the curve. The fianted area
    around the line shows the standard deviation.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：通过在 Alpaca 数据集上微调 LLM 来进行 LSP-Offload 的端到端评估。对于图 [4(a)](#S6.F4.sf1 "在图 4
    ‣ Alpaca 上的 LSP-Offload 端到端评估。 ‣ 6 评估 ‣ 通过学习的子空间投影器进行商用 GPU 的微调实践")，图 [4(b)](#S6.F4.sf2
    "在图 4 ‣ Alpaca 上的 LSP-Offload 端到端评估。 ‣ 6 评估 ‣ 通过学习的子空间投影器进行商用 GPU 的微调实践")，曲线图应用了滚动平均。线周围的阴影区域表示标准差。
- en: Shown in Fig. [4(a)](#S6.F4.sf1 "In Figure 4 ‣ End-to-end evaluation of the
    LSP-Offload on Alpaca. ‣ 6 Evaluation ‣ Practical offloading for fine-tuning LLM
    on commodity GPU via learned subspace projectors") and Fig. [4(b)](#S6.F4.sf2
    "In Figure 4 ‣ End-to-end evaluation of the LSP-Offload on Alpaca. ‣ 6 Evaluation
    ‣ Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), compared to Zero-Offload, LSP-Offload uses around 62.5% and 33.1%
    less time when converging to the same accuracy. For example, when training on
    the Laptop GPU, LSP-Offload achieves the evaluation perplexity of 1.82 after 2
    hours of training, while reaching the same perplexity takes 4.5 hours with Zero-Offload.
    In terms of the training accuracy, LSP-Offload converges to the perplexity of
    1.63 after 12 hours, which is achieved by Zero-Offload after 20 hours. One thing
    we want to mention that training all parameters as in Zero-Offload does converge
    to lower perplexity of 1.59\. However, the training time which takes about approximately
    30 hours makes the performance gain less favorable.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4(a)](#S6.F4.sf1 "在图 4 ‣ Alpaca 上的 LSP-Offload 端到端评估。 ‣ 6 评估 ‣ 通过学习的子空间投影器进行商用
    GPU 的微调实践") 和图 [4(b)](#S6.F4.sf2 "在图 4 ‣ Alpaca 上的 LSP-Offload 端到端评估。 ‣ 6 评估 ‣
    通过学习的子空间投影器进行商用 GPU 的微调实践") 所示，与 Zero-Offload 相比，LSP-Offload 在收敛到相同精度时使用了大约 62.5%
    和 33.1% 更少的时间。例如，在 Laptop GPU 上训练时，LSP-Offload 在 2 小时的训练后达到了 1.82 的评估困惑度，而 Zero-Offload
    达到相同的困惑度需要 4.5 小时。在训练精度方面，LSP-Offload 在 12 小时后收敛到 1.63 的困惑度，而 Zero-Offload 需要
    20 小时才能达到同样的困惑度。需要提到的是，训练所有参数如同 Zero-Offload 确实可以收敛到更低的 1.59 的困惑度。然而，训练时间约为 30
    小时，使得性能提升不够理想。
- en: Moreover, compared to LoRA, LSP-Offload is able to achieve higher training accuracy.
    For example, LoRA converges to the perplexity of 2.15 and 2.05 in the laptop and
    workstation setting respectively, which are 30% and 13% higher than the final
    perplexity of LSP-Offload. This finding verifies the intuition that LSP-Offload
    can have better performance by optimizing in larger subspace.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与 LoRA 相比，LSP-Offload 能够实现更高的训练精度。例如，LoRA 在笔记本电脑和工作站设置下的困惑度分别为 2.15 和 2.05，比
    LSP-Offload 的最终困惑度高出 30% 和 13%。这一发现验证了 LSP-Offload 通过在更大的子空间中优化可以获得更好性能的直觉。
- en: Training Throughput Comparison.
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练吞吐量比较。
- en: In Fig. [4(c)](#S6.F4.sf3 "In Figure 4 ‣ End-to-end evaluation of the LSP-Offload
    on Alpaca. ‣ 6 Evaluation ‣ Practical offloading for fine-tuning LLM on commodity
    GPU via learned subspace projectors"), we show the comparison of training throughput
    for different configurations. When trained on a subspace of size $512\times 512$,
    we are able to achieve 2.03, 3.09, 2.04, 3.33 times higher training throughput
    for the 4 test cases listed Fig. [4(c)](#S6.F4.sf3 "In Figure 4 ‣ End-to-end evaluation
    of the LSP-Offload on Alpaca. ‣ 6 Evaluation ‣ Practical offloading for fine-tuning
    LLM on commodity GPU via learned subspace projectors"). Compared to the native
    training without offloading, LSP-Offload slows down on average 10.6%, 16.7%, 38.3%
    with the subspace of size 256, 512, 1024\. Specifically, when trained on the workstation
    GPU with subspace size of smaller or equal to 512, LSP-Offload is able to obtain
    2% higher throughput as compared to native training due to the fully paralleled
    optimizer update step on CPU. Lastly, applying the layer-wise schedule to Zero’s
    schedule yields on average 18% increase in the throughput.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4(c)](#S6.F4.sf3 "图 4 ‣ LSP-Offload 在 Alpaca 上的端到端评估。 ‣ 6 评估 ‣ 通过学习到的子空间投影器在普通
    GPU 上对 LLM 进行实用的卸载")中，我们展示了不同配置下训练吞吐量的比较。当在大小为$512\times 512$的子空间上训练时，我们能够实现4个测试案例在图[4(c)](#S6.F4.sf3
    "图 4 ‣ LSP-Offload 在 Alpaca 上的端到端评估。 ‣ 6 评估 ‣ 通过学习到的子空间投影器在普通 GPU 上对 LLM 进行实用的卸载")中列出的训练吞吐量分别提高2.03倍、3.09倍、2.04倍、3.33倍。与没有卸载的本地训练相比，LSP-Offload
    在子空间大小为256、512、1024的情况下，平均降低了10.6%、16.7%、38.3%。具体来说，当在工作站 GPU 上训练，子空间大小小于或等于512时，由于CPU上完全并行的优化器更新步骤，LSP-Offload能够获得比本地训练高2%的吞吐量。最后，将逐层调度应用于
    Zero 的调度，吞吐量平均提高了18%。
- en: Hyper parameters for LSP training.
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LSP 训练的超参数。
- en: We empirically compare the performance of different subspace sizes and the number
    of non-zero values per row in the sparse projector. While smaller subspace sizes
    limit the optimization space, we found too large subspace can lead to low accuracy
    because of over fitting. Shown in Fig. [4(b)](#S6.F4.sf2 "In Figure 4 ‣ End-to-end
    evaluation of the LSP-Offload on Alpaca. ‣ 6 Evaluation ‣ Practical offloading
    for fine-tuning LLM on commodity GPU via learned subspace projectors"), training
    with subspace of size 512 outperform training with either 256 and 1024\. But the
    training loss is 0.61 for the subspace of size 1024 and 0.72 for the subspace
    of size 512.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经验性地比较了不同子空间大小和稀疏投影器每行的非零值数量的性能。虽然较小的子空间大小限制了优化空间，但我们发现过大的子空间会因过拟合导致低准确率。如图[4(b)](#S6.F4.sf2
    "图 4 ‣ LSP-Offload 在 Alpaca 上的端到端评估。 ‣ 6 评估 ‣ 通过学习到的子空间投影器在普通 GPU 上对 LLM 进行实用的卸载")所示，子空间大小为512的训练优于子空间大小为256和1024的训练。但是，子空间大小为1024的训练损失为0.61，而子空间大小为512的训练损失为0.72。
- en: 7 Limitation
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: While efficient, LSP-Offload introduces a few hyper parameters which may need
    careful selection for the best performance (Fig. [4(a)](#S6.F4.sf1 "In Figure
    4 ‣ End-to-end evaluation of the LSP-Offload on Alpaca. ‣ 6 Evaluation ‣ Practical
    offloading for fine-tuning LLM on commodity GPU via learned subspace projectors"),
    [4(b)](#S6.F4.sf2 "In Figure 4 ‣ End-to-end evaluation of the LSP-Offload on Alpaca.
    ‣ 6 Evaluation ‣ Practical offloading for fine-tuning LLM on commodity GPU via
    learned subspace projectors")), including the choice of the d-sparse matrix, the
    frequency to update the subspace, the threshold for the subspace update, etc.
    Moreover, the current prototype of LSP-Offload does not include the quantization
    technique, which is fully compatible with our approach and we leave for the future
    work.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然高效，但 LSP-Offload 引入了一些可能需要仔细选择以获得最佳性能的超参数（图[4(a)](#S6.F4.sf1 "图 4 ‣ LSP-Offload
    在 Alpaca 上的端到端评估。 ‣ 6 评估 ‣ 通过学习到的子空间投影器在普通 GPU 上对 LLM 进行实用的卸载")、[4(b)](#S6.F4.sf2
    "图 4 ‣ LSP-Offload 在 Alpaca 上的端到端评估。 ‣ 6 评估 ‣ 通过学习到的子空间投影器在普通 GPU 上对 LLM 进行实用的卸载")），包括
    d-sparse 矩阵的选择、更新子空间的频率、子空间更新的阈值等。此外，当前 LSP-Offload 原型未包含量化技术，而该技术与我们的方法完全兼容，我们将其留待未来工作中解决。
- en: 8 Conclusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we observed that in the commodity setting, current offloading
    frameworks are fundamentally bottle necked by the expensive communication or the
    compute on CPU. Motivated by the PEFT method, we designed LSP-Offload to enable
    near-native speed fine-tuning by constraining the parameter update onto a subspace.
    Technically, we projected the gradient onto a subspace using a sparse projector,
    and boosted its performance by minimizing the empirical bias. Compared to the
    prior PEFT approaches (Galore, LORA), with the same amount of additional memory
    on GPU, we are able to optimize in subspace of arbitrary subspace. In evaluation,
    we verified that LSP training can converge at the same rate with native training
    on the GLUE dataset. Also, in the end-to-end comparison with SOTA offloading framework
    on instruction-tuning task, we are able to achieve up to 3.33x higher training
    throughput and converge to the same accuracy with 37.5% to 66.9% of time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们观察到在商品化环境下，当前的卸载框架在CPU上由于昂贵的通信或计算而存在根本瓶颈。受到PEFT方法的启发，我们设计了LSP-Offload，通过将参数更新约束到一个子空间来实现接近本地速度的微调。从技术上讲，我们使用稀疏投影器将梯度投影到子空间，并通过最小化经验偏差来提升其性能。与先前的PEFT方法（Galore,
    LORA）相比，在相同的GPU附加内存下，我们能够在任意子空间进行优化。在评估中，我们验证了LSP训练在GLUE数据集上可以与本地训练以相同的速度收敛。此外，在与SOTA卸载框架在指令调优任务上的端到端比较中，我们能够实现最高3.33倍的训练吞吐量，并在37.5%到66.9%的时间内收敛到相同的准确率。
- en: References
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep
    nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 田啟，邢冰，张赤源，和卡洛斯·戈斯特林。《以亚线性内存成本训练深度网络》。arXiv预印本 arXiv:1604.06174，2016年。'
- en: '[2] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 蒂姆·德特梅斯，阿尔蒂多罗·帕尼奥尼，阿里·霍尔茨曼，和卢克·泽特尔莫耶。《Qlora：量化llms的高效微调》。神经信息处理系统进展，第36卷，2024年。'
- en: '[3] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer
    learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 德米·郭，亚历山大·M·拉什，和尹金。《通过差异修剪的参数高效转移学习》。arXiv预印本 arXiv:2012.07463，2020年。'
- en: '[4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 爱德华·J·胡，叶龙·申，菲利普·沃利斯，泽元·艾伦-朱，袁志·李，肖恩·王，陆·王，和魏柱·陈。《Lora：大型语言模型的低秩适应》。arXiv预印本
    arXiv:2106.09685，2021年。'
- en: '[5] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning
    beyond the gpu memory limit via smart swapping. In Proceedings of the Twenty-Fifth
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, pages 1341–1355, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 黄建钦，顾进，和金阳·李。《Swapadvisor：通过智能交换突破GPU内存限制》。在第二十五届国际编程语言和操作系统体系结构支持会议论文集中，第1341–1355页，2020年。'
- en: '[6] Daniel M Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms.
    Journal of the ACM (JACM), 61(1):1–23, 2014.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 丹尼尔·M·凯恩和杰拉尼·尼尔森。《更稀疏的约翰逊-林登斯特劳斯变换》。ACM期刊（JACM），61(1)：1–23，2014年。'
- en: '[7] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 迪德里克·P·金马和吉米·巴。《Adam：一种随机优化方法》。arXiv预印本 arXiv:1412.6980，2014年。'
- en: '[8] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.
    Relora: High-rank training through low-rank updates. In Workshop on Advancing
    Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization
    (WANT@ NeurIPS 2023), 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 弗拉基斯拉夫·利亚林，谢林·穆卡塔拉，南拉塔·希瓦贡德，和安娜·鲁姆希斯基。《Relora：通过低秩更新进行高秩训练》。在《推动神经网络训练：计算效率、可扩展性和资源优化研讨会》（WANT@
    NeurIPS 2023）上，2023年。'
- en: '[9] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 刘银汉，迈尔·奥特，纳曼·戈亚尔，静妃·杜，曼达尔·乔希，丹琪·陈，奥梅尔·莱维，迈克·刘易斯，卢克·泽特尔莫耶，和维塞林·斯托扬诺夫。《Roberta：一种强健优化的bert预训练方法》。arXiv预印本
    arXiv:1907.11692，2019年。'
- en: '[10] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero:
    Memory optimizations toward training trillion parameter models. In SC20: International
    Conference for High Performance Computing, Networking, Storage and Analysis, pages
    1–16\. IEEE, 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 萨姆扬·拉吉班达里，杰夫·拉斯利，奥拉图吉·鲁瓦塞，和余雄·赫。《Zero：用于训练万亿参数模型的内存优化》。在SC20：国际高性能计算、网络、存储与分析会议上，第1–16页。IEEE，2020年。'
- en: '[11] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong
    He. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning.
    In Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis, pages 1–14, 2021.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Samyam Rajbhandari、Olatunji Ruwase、Jeff Rasley、Shaden Smith 和 Yuxiong
    He。Zero-infinity：打破极大规模深度学习的 GPU 内存壁垒。见《国际高性能计算、网络、存储与分析会议论文集》，页码 1–14，2021 年。'
- en: '[12] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,
    Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. $\{$ model training. In
    2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551–564, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 任洁、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyan
    Yang、张敏佳、董立和 Yuxiong He。$\{$ 模型训练。见 2021 USENIX 年度技术会议 (USENIX ATC 21)，页码 551–564，2021
    年。'
- en: '[13] Ahmad Ajalloeian1 Sebastian U Stich. Analysis of sgd with biased gradient
    estimators. arXiv preprint arXiv:2008.00051, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ahmad Ajalloeian1 Sebastian U Stich。带有偏差梯度估计器的 SGD 分析。arXiv 预印本 arXiv:2008.00051，2020
    年。'
- en: '[14] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Rohan Taori、Ishaan Gulrajani、张天逸、Yann Dubois、李雪辰、Carlos Guestrin、Percy
    Liang 和 Tatsunori B. Hashimoto。斯坦福 alpaca：一种遵循指令的 llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023
    年。'
- en: '[15] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi.
    Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free
    low-rank adaptation. arXiv preprint arXiv:2210.07558, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Mojtaba Valipour、Mehdi Rezagholizadeh、Ivan Kobyzev 和 Ali Ghodsi。Dylora：通过动态搜索-free
    低秩适应对预训练模型进行参数高效调优。arXiv 预印本 arXiv:2210.07558，2022 年。'
- en: '[16] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
    Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural
    language understanding. arXiv preprint arXiv:1804.07461, 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Alex Wang、Amanpreet Singh、Julian Michael、Felix Hill、Omer Levy 和 Samuel
    R Bowman。Glue：一个用于自然语言理解的多任务基准和分析平台。arXiv 预印本 arXiv:1804.07461，2018 年。'
- en: '[17] Haoyang Zhang, Yirui Eric Zhou, Yuqi Xue, Yiqi Liu, and Jian Huang. G10:
    Enabling an efficient unified gpu memory and storage architecture with smart tensor
    migrations. arXiv preprint arXiv:2310.09443, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 张浩阳、周逸瑞、薛宇奇、刘亦琪和黄健。G10：通过智能张量迁移实现高效统一的 GPU 内存和存储架构。arXiv 预印本 arXiv:2310.09443，2023
    年。'
- en: '[18] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar,
    and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank
    projection. arXiv preprint arXiv:2403.03507, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 赵佳伟、张振宇、陈贝迪、张阳、Anima Anandkumar 和 田远东。Galore：通过梯度低秩投影实现内存高效的 LLM 训练。arXiv
    预印本 arXiv:2403.03507，2024 年。'
- en: 9 Appendix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 附录
- en: 'Table 4: Configurations and timings for training/fine-tuning the GPT2-1.3B
    Model on the Nvidia A1000 Laptop GPU (4GB) and Intel Core-i7 12800H CPU (32GB).
    For the Update stage, we measure the fused Adam kernel with thread-level parallelism
    and SIMD optimizations. For the Bandwidth, we measure the PCIe bandwidth with
    a pinned memory buffer.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 Nvidia A1000 笔记本 GPU (4GB) 和 Intel Core-i7 12800H CPU (32GB) 上训练/微调 GPT2-1.3B
    模型的配置和时间。对于更新阶段，我们测量了带有线程级并行和 SIMD 优化的融合 Adam 内核。对于带宽，我们测量了带有固定内存缓冲区的 PCIe 带宽。
- en: '| Parameters | Optimizer State | Activations | CPU-GPU Bandwidth | #Layer |
    GPU Memory |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 优化器状态 | 激活 | CPU-GPU 带宽 | 层数 | GPU 内存 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 2.6GB | 7.8GB | 0.5GB | 10$\sim$15GB/s | 40 | 4GB |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2.6GB | 7.8GB | 0.5GB | 10$\sim$15GB/s | 40 | 4GB |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FWD on CPU | BWD on CPU | UPD on CPU | FWD on GPU | BWD on GPU | UPD on GPU
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| CPU 上的前向传播 | CPU 上的反向传播 | CPU 上的更新 | GPU 上的前向传播 | GPU 上的反向传播 | GPU 上的更新 |'
- en: '| 0.16s/layer | 0.27s/layer | 0.08s/layer | 4.5ms/layer | 8.7ms/layer | 7.9ms/layer
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 0.16s/层 | 0.27s/层 | 0.08s/层 | 4.5ms/层 | 8.7ms/层 | 7.9ms/层 |'
- en: 9.1 Proof of theorem 1
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 定理 1 的证明
- en: Before proving Theorem [1](#Thmtheorem1 "Theorem 1\. ‣ 4.1 Communication/Compute-efficient
    Parameter Update via Learned Subspace Projectors ‣ 4 LSP-Offload’s Approach ‣
    Practical offloading for fine-tuning LLM on commodity GPU via learned subspace
    projectors"), we listed the lemmas used in the proof.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在证明定理 [1](#Thmtheorem1 "定理 1\. ‣ 4.1 通过学习子空间投影器实现通信/计算高效的参数更新 ‣ 4 LSP-Offload
    的方法 ‣ 实用的 LLM 微调方法通过学习子空间投影器在商用 GPU 上") 之前，我们列出了证明中使用的引理。
- en: Lemma 1  (Matrix Chernoff).
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1（矩阵切尔诺夫）。
- en: Let $M_{1},...,M_{t}$ holds almost surely for all $i\in\{1,...,t\}$,
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $M_{1},...,M_{t}$ 对所有 $i\in\{1,...,t\}$ 几乎确定成立，
- en: '|  | $$Pr(\&#124;\frac{1}{t}\Sigma_{i}M_{i}\&#124;_{2}> |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $$Pr(\&#124;\frac{1}{t}\Sigma_{i}M_{i}\&#124;_{2}> |  |'
- en: Lemma 2.
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2。
- en: For a subspace compressor $\mathcal{C}$, we can bound the bias by the empirical
    bias on a random sub-sampled dataset $S$,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个子空间压缩器 $\mathcal{C}$，我们可以通过在随机子样本数据集 $S$ 上的经验偏差来界定偏差，
- en: '|  | $1$2 |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We prove by Matrix Chernoff Bound. For data $x\in S$. Also,
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过矩阵切尔诺夫界限证明。对于数据 $x\in S$。此外，
- en: '|  | $\displaystyle\frac{1}{&#124;S&#124;}\Sigma_{x\in S}M_{x}$ |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{&#124;S&#124;}\Sigma_{x\in S}M_{x}$ |  |'
- en: '|  |  | $\displaystyle=\frac{1}{&#124;S&#124;}\Sigma_{x\in S}(\mathcal{C}(\nabla
    f_{x}(W)))-\nabla f_{S}(W)-\textbf{b}(W)$ |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{&#124;S&#124;}\Sigma_{x\in S}(\mathcal{C}(\nabla
    f_{x}(W)))-\nabla f_{S}(W)-\textbf{b}(W)$ |  |'
- en: '|  |  | $\displaystyle=\mathcal{C}(\frac{1}{&#124;S&#124;}\Sigma_{x\in S}\nabla
    f_{x}(W))-\nabla f_{S}(W)-\textbf{b}(W)$ |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathcal{C}(\frac{1}{&#124;S&#124;}\Sigma_{x\in S}\nabla
    f_{x}(W))-\nabla f_{S}(W)-\textbf{b}(W)$ |  |'
- en: '|  |  | $\displaystyle=\mathcal{C}(\nabla f_{S}(W))-\nabla f_{S}(W)-\textbf{b}(W)$
    |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathcal{C}(\nabla f_{S}(W))-\nabla f_{S}(W)-\textbf{b}(W)$
    |  |'
- en: . By Matrix Chernoff, we have that for 
    |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $$Pr(\&#124;\mathcal{C}(\nabla f_{S}(W))-\nabla f_{S}(W)-\textbf{b}(W)\&#124;_{2}>
    |  |'
- en: . Therefore, with probability $1-\delta$,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: . 因此，概率为 $1-\delta$，
- en: '|  | $\displaystyle\&#124;\textbf{b}(W)\&#124;_{2}$ |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\textbf{b}(W)\&#124;_{2}$ |  |'
- en: '|  |  | $\displaystyle\leq\&#124;\mathcal{C}(\nabla f_{S}(W))-\nabla f_{S}(W)\&#124;_{2}+\epsilon$
    |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;\mathcal{C}(\nabla f_{S}(W))-\nabla f_{S}(W)\&#124;_{2}+\epsilon$
    |  |'
- en: ∎
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Theorem 2.
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2。
- en: '[[13](#bib.bib13)] For any $\epsilon>, and stepsize ，和步长 ,'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ', 通过将其代入定理 [2](#Thmtheorem2 "定理 2\. ‣ 9.1 定理 1 的证明 ‣ 9 附录 ‣ 通过学习的子空间投影器在商品
    GPU 上进行微调 LLM 的实际卸载") 对于所有步骤从 1 到 T，我们得到对于 ,'
- en: '|  | $T=\mathcal{O}(\frac{1}{\epsilon})\cdot\frac{LF}{(1-2c^{2}\alpha^{2})}$
    |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $T=\mathcal{O}(\frac{1}{\epsilon})\cdot\frac{LF}{(1-2c^{2}\alpha^{2})}$
    |  |'
- en: iterations are sufficient to obtain $\min_{t\in[T]}\mathbb{E}\|\nabla f(W_{t})\|^{2}=\mathcal{O}(\epsilon+\frac{2c^{2}\beta^{2}(1+\alpha)^{2}}{1-2c^{2}\alpha^{2}})$
    concludes the proof. ∎
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数足以得到 $\min_{t\in[T]}\mathbb{E}\|\nabla f(W_{t})\|^{2}=\mathcal{O}(\epsilon+\frac{2c^{2}\beta^{2}(1+\alpha)^{2}}{1-2c^{2}\alpha^{2}})$
    证明完毕。∎
- en: 9.2 Experiment Configurations
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 实验配置
- en: 9.2.1 The GLUE Experiment
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1 GLUE 实验
- en: For the GLUE experiment, we use the batch size of 16 and set the learning rate
    of 1e-4 for the baseline and 1e-5 for LSP-Offload. For LSP-Offload, we update
    the subspace at the beginning of each epoch or every 1000 iterations. The threshold
    for the compression is set to be 0.3.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GLUE 实验，我们使用批量大小为 16，基准学习率设置为 1e-4，LSP-Offload 的学习率设置为 1e-5。对于 LSP-Offload，我们在每个纪元开始时或每
    1000 次迭代时更新子空间。压缩的阈值设置为 0.3。
- en: 9.2.2 The Instruction Fine-tuning Experiment
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2 指令微调实验
- en: For the instruction fine-tuning experiment, we use the batch size of 4 for the
    GPT2-774M model and 16 for the Llama-3B model, which is the largest without exceeding
    the GPU memory. The learning rate the best from $1e-4,1e-5,1e-6$, which is 1e-4
    for LSP-Offload, and 1e-5 for both LoRA and Zero-Offload. For LSP-Offload, we
    update the subspace every 1000 iterations. The threshold for the compression is
    set to be 0.5.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指令微调实验，我们对 GPT2-774M 模型使用批量大小为 4，对 Llama-3B 模型使用批量大小为 16，这是不超过 GPU 内存的最大值。学习率从
    $1e-4,1e-5,1e-6$ 中选择最佳值，其中 LSP-Offload 为 1e-4，LoRA 和 Zero-Offload 均为 1e-5。对于 LSP-Offload，我们每
    1000 次迭代更新一次子空间。压缩的阈值设置为 0.5。
