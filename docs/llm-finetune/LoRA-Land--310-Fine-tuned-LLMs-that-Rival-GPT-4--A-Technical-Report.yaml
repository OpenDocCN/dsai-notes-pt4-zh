- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:37:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:15
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LoRA Land: 310 个微调的 LLM 与 GPT-4 竞争，技术报告'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00732](https://ar5iv.labs.arxiv.org/html/2405.00732)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.00732](https://ar5iv.labs.arxiv.org/html/2405.00732)
- en: Justin Zhao, Timothy Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Justin Zhao, Timothy Wang
- en: Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky,
- en: Piero Molino, Travis Addair, Devvret Rishi
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Piero Molino, Travis Addair, Devvret Rishi
- en: Predibase
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Predibase
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods
    for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA
    reduces the number of trainable parameters and memory usage while achieving comparable
    performance to full fine-tuning. We aim to assess the viability of training and
    serving LLMs fine-tuned with LoRA in real-world applications. First, we measure
    the quality of LLMs fine-tuned with quantized low rank adapters across 10 base
    models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned
    models outperform base models by 34 points and GPT-4 by 10 points on average.
    Second, we investigate the most effective base models for fine-tuning and assess
    the correlative and predictive capacities of task complexity heuristics in forecasting
    the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency
    capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates
    the deployment of multiple LoRA fine-tuned models on a single GPU using shared
    base model weights and dynamic adapter loading. LoRAX powers [LoRA Land](https://predibase.com/lora-land),
    a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA
    A$100$GB memory. LoRA Land highlights the quality and cost-effectiveness of employing
    multiple specialized LLMs over a single, general-purpose LLM.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配（LoRA）已成为参数高效微调（PEFT）大型语言模型（LLMs）最广泛采用的方法之一。LoRA 减少了可训练参数的数量和内存使用，同时实现了与完全微调相媲美的性能。我们旨在评估在实际应用中训练和服务经过
    LoRA 微调的 LLMs 的可行性。首先，我们测量了使用量化低秩适配器对 10 个基础模型和 31 个任务进行微调的 LLM 的质量，总计 310 个模型。我们发现，4-bit
    LoRA 微调模型在平均上超越了基础模型 34 分，超越了 GPT-4 10 分。其次，我们调查了最有效的基础模型进行微调，并评估了任务复杂性启发式方法在预测微调结果中的相关性和预测能力。最后，我们评估了
    LoRAX 的延迟和并发能力，LoRAX 是一个开源的 Multi-LoRA 推理服务器，可以在单个 GPU 上使用共享基础模型权重和动态适配器加载来部署多个
    LoRA 微调模型。LoRAX 支持 [LoRA Land](https://predibase.com/lora-land)，这是一个在单个 NVIDIA
    A$100$GB 内存上托管 25 个 LoRA 微调 Mistral-7B LLM 的 Web 应用。LoRA Land 突出了使用多个专业化 LLM 相比于单一通用
    LLM 的质量和成本效益。
- en: '![Refer to caption](img/e4891fdc95cb3d988a86b588051aff1b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4891fdc95cb3d988a86b588051aff1b.png)'
- en: 'Figure 1: Average model performance for GPT-3.5, GPT-4, and 310 LLMs, before
    and after fine-tuning with LoRA, across 31 different tasks and 10 different base
    models. Zephyr-7b and Mistral-7b models exhibit the best performance after LoRA-based
    fine-tuning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GPT-3.5、GPT-4 和 310 个 LLM 在 LoRA 微调前后的平均模型性能，涵盖 31 个不同任务和 10 个不同基础模型。Zephyr-7b
    和 Mistral-7b 模型在 LoRA 基于微调后表现最佳。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Fine-tuning Large Language Models (LLMs) [[23](#bib.bib23), [3](#bib.bib3)]
    is a highly effective way to improve their performance, and add desirable or remove
    undesirable behaviors [[28](#bib.bib28), [12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29)].
    Low Rank Adaptation (LoRA) [[14](#bib.bib14)] is one of the most widely adopted
    methods for fine-tuning LLMs, showing significant promise for enabling smaller,
    specialized models to outperform larger, more general models on specific tasks,
    with a fraction of trainable parameters, challenging the notion that bigger general
    models always outperform smaller ones.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）进行微调 [[23](#bib.bib23), [3](#bib.bib3)] 是一种非常有效的提升性能的方法，能够增加所需行为或去除不必要的行为
    [[28](#bib.bib28), [12](#bib.bib12), [13](#bib.bib13), [29](#bib.bib29)]。低秩适配（LoRA）
    [[14](#bib.bib14)] 是目前最广泛采用的微调 LLMs 的方法之一，显示出能够使较小、专业化的模型在特定任务中超越更大、更通用的模型的巨大潜力，只需少量的可训练参数，挑战了较大通用模型总是优于较小模型的观念。
- en: Despite the rapid advancement and release of new base models, such as Gemma [[36](#bib.bib36)],
    Llama [[37](#bib.bib37)], and Mistral [[15](#bib.bib15)], which claim ease of
    fine-tuning across various tasks, comprehensive evaluations of these models remain
    scarce. Broad knowledge and reasoning-based benchmarks like MMLU [[11](#bib.bib11)]
    and HellaSwag [[44](#bib.bib44)] are commonly used in leaderboards like the Open
    LLM Leaderboard [[2](#bib.bib2)], however, this is not necessarily representative
    of task-specific performance, before or after fine-tuning. Technical reports [[36](#bib.bib36),
    [37](#bib.bib37), [15](#bib.bib15), [26](#bib.bib26), [35](#bib.bib35)] often
    leave training configurations unspecified, with claims of ease of fine-tuning
    left unmeasured. While the effectiveness of fine-tuning has been broadly demonstrated [[17](#bib.bib17),
    [45](#bib.bib45)], the lack of large-scale experimentation leaves several pivotal
    questions unanswered, particularly regarding the consistency and predictability
    of performance improvements through fine-tuning, and the impact of model size,
    base model, and task complexity.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管新基础模型如 Gemma [[36](#bib.bib36)]、Llama [[37](#bib.bib37)] 和 Mistral [[15](#bib.bib15)]
    迅速发展并发布，这些模型声称在各种任务上微调简便，但对这些模型的全面评估仍然稀缺。像 MMLU [[11](#bib.bib11)] 和 HellaSwag
    [[44](#bib.bib44)] 这样的广泛知识和推理基准常用于如 Open LLM Leaderboard [[2](#bib.bib2)] 等排行榜中，但这不一定代表任务特定的表现，无论是微调之前还是之后。技术报告
    [[36](#bib.bib36), [37](#bib.bib37), [15](#bib.bib15), [26](#bib.bib26), [35](#bib.bib35)]
    通常未明确训练配置，且关于微调简便性的声明未进行测量。尽管微调的有效性已经得到广泛证明 [[17](#bib.bib17), [45](#bib.bib45)]，但缺乏大规模实验使得几个关键问题未得到解答，特别是关于微调性能改进的一致性和可预测性，以及模型规模、基础模型和任务复杂度的影响。
- en: Evaluations are sensitive to prompting, and there are significant variation
    in the formulations used in publications and libraries¹¹1https://github.com/openai/simple-evals.
    Technical reports often showcase model performance using specialized, dataset-specific
    prompting strategies such as role-playing prompts (e.g. "Assume you are an expert"),
    maj@k voting [[40](#bib.bib40)], varied n-shot [[34](#bib.bib34)], MedPrompt [[25](#bib.bib25)],
    and chain-of-thought [[43](#bib.bib43)] prompting. While these methods are intended
    to highlight the optimal capabilities of models, the use of such diverse prompting
    techniques can make direct comparisons across models and tasks challenging.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 评估对提示非常敏感，出版物和库中使用的提示形式存在显著差异¹¹1https://github.com/openai/simple-evals。技术报告经常展示模型性能时使用专门的、数据集特定的提示策略，例如角色扮演提示（例如“假设你是一个专家”）、maj@k
    投票 [[40](#bib.bib40)]、不同的 n-shot [[34](#bib.bib34)]、MedPrompt [[25](#bib.bib25)]
    和 chain-of-thought [[43](#bib.bib43)] 提示。虽然这些方法旨在突出模型的最佳能力，但使用如此多样的提示技术可能使得不同模型和任务之间的直接比较变得具有挑战性。
- en: In this work, we seek to bridge these gaps by conducting an extensive analysis
    of LoRA-based fine-tuning across 10 base models and 31 tasks, totaling 310 LLMs
    fine-tuned with LoRA. We deliberately maintain that all LLMs are fine-tuned with
    the same training parameters and emphasize querying with zero or single-shot,
    completion-style prompts, with simple instructions like "Solve the following multiple
    choice problem". Altogether, this provides a standardized framework to compare
    and assess the intrinsic capabilities of different base models when fine-tuned
    with LoRA under consistent conditions, across specific tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们试图通过对 10 个基础模型和 31 个任务中的 LoRA 微调进行广泛分析来弥补这些空白，共计 310 个 LLM 使用 LoRA
    进行微调。我们故意保持所有 LLM 使用相同的训练参数，并强调使用零-shot 或单-shot 的完成式提示，简单指令如“解决以下多项选择题”。总体而言，这提供了一个标准化的框架，用于在一致条件下比较和评估不同基础模型在使用
    LoRA 微调时的内在能力，跨特定任务进行评估。
- en: We also aim to explore the viability of serving multiple LoRA models in a real-world
    production application. LoRAX  [[1](#bib.bib1)] enables serving multiple LoRA
    models simultaneously on a single GPU by leveraging shared base model weights
    and dynamic adapter loading [12]. We measure latency and concurrency metrics of
    this library. We use LoRAX to deploy 25 fine-tuned LLM served on a single A100²²2https://www.nvidia.com/en-us/data-center/a100/
    in the LoRA Land web application. Our successful implementation showcases the
    economic efficiency of serving multiple LoRA-adapted LLMs for specialized tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还旨在探索在实际生产应用中服务多个LoRA模型的可行性。LoRAX[[1](#bib.bib1)]通过利用共享基础模型权重和动态适配器加载[12]，使得在单个GPU上同时服务多个LoRA模型成为可能。我们测量了该库的延迟和并发性指标。我们使用LoRAX在单个A100²²2https://www.nvidia.com/en-us/data-center/a100/上部署25个微调的LLM，在LoRA
    Land网络应用中服务。我们的成功实施展示了服务多个LoRA适配的LLM以执行专门任务的经济效率。
- en: Finally, we release all 25 of the fine-tuned models on the LoRA Land web application
    and their training recipes on ([Hugging Face](https://huggingface.co/predibase))
    to allow further analysis and replication by the community.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在LoRA Land网络应用上发布了所有25个微调模型及其训练配方，并在([Hugging Face](https://huggingface.co/predibase))上发布，以便社区进一步分析和复制。
- en: 2 Related work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Parameter-Efficient Fine-Tuning (PEFT) methods are designed to reduce the high
    expense of fine-tuning large-scale models. They achieve this by training a relatively
    small subset of parameters, compared to the total number of parameters, for adapting
    to downstream tasks. Existing PEFT strategies can be divided into two categories:
    Prompt-based methods add extra soft tokens (prompts) to the initial input and
    focus solely on fine-tuning these trainable vectors [[19](#bib.bib19), [31](#bib.bib31),
    [42](#bib.bib42)]. Adapter-based methods introduce additional trainable modules
    into the original frozen backbone [[12](#bib.bib12), [32](#bib.bib32), [30](#bib.bib30),
    [33](#bib.bib33)]. LoRA [[14](#bib.bib14)] expands upon adapter-based fine-tuning
    by adding a small number of trainable low-rank matrices alongside layers of frozen
    weights, which introduces a negligible inference overhead. Variants of LoRA include
    works like [[22](#bib.bib22)], which employs SVD decomposition to prune less significant
    singular values for more efficient updates. Another variation, DoRA  [[21](#bib.bib21)],
    decomposes pre-trained weights into magnitude and direction components while applying
    LoRA the latter. QLoRA [[8](#bib.bib8)] optimizes LoRA’s design one step further,
    using 4-bit NF4 weights, double quantization to reduce the memory footprint, and
    paged optimizers to alleviate memory spikes. In our experiments, we focus on the
    original implementation of LoRA with 4-bit quantization.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法旨在降低微调大规模模型的高费用。它们通过训练相对于总参数数目较小的一部分参数来适应下游任务。现有的PEFT策略可以分为两类：基于提示的方法通过在初始输入中添加额外的软标记（提示）来专注于微调这些可训练的向量[[19](#bib.bib19),
    [31](#bib.bib31), [42](#bib.bib42)]。基于适配器的方法在原始冻结骨干中引入额外的可训练模块[[12](#bib.bib12),
    [32](#bib.bib32), [30](#bib.bib30), [33](#bib.bib33)]。LoRA[[14](#bib.bib14)]在基于适配器的微调基础上扩展，通过在冻结权重的层旁边添加少量可训练的低秩矩阵，从而引入了微不足道的推理开销。LoRA的变体包括像[[22](#bib.bib22)]这样的工作，它采用SVD分解来修剪不太重要的奇异值，以实现更高效的更新。另一种变体，DoRA[[21](#bib.bib21)]，将预训练权重分解为幅度和方向分量，同时对后者应用LoRA。QLoRA[[8](#bib.bib8)]进一步优化了LoRA的设计，使用4位NF4权重、双重量化来减少内存占用，并采用分页优化器来减轻内存峰值。在我们的实验中，我们专注于4位量化的LoRA原始实现。
- en: 'Efficient serving of LoRA models. The main challenges for serving multiple
    fine-tuned models efficiently are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 高效服务LoRA模型。高效服务多个微调模型的主要挑战包括：
- en: '1.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Scalability: As the demand for model inference grows, the system must scale
    efficiently to handle the increased load. This involves not just scaling up the
    computational resources but also managing the load distribution among models to
    maintain performance.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可扩展性：随着模型推理需求的增加，系统必须高效地扩展以处理增加的负载。这不仅涉及到计算资源的扩展，还包括在模型之间管理负载分配以保持性能。
- en: '2.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Cost: The computational resources required to serve multiple fine-tuned models
    can lead to significant costs. Efficiently managing these costs while maintaining
    high performance and availability is a major challenge.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本：为服务多个微调模型所需的计算资源可能导致显著的成本。有效管理这些成本，同时保持高性能和可用性是一个主要挑战。
- en: Techniques like Segmented Gather Matrix-Vector Multiplication (SGMV) [[4](#bib.bib4)]
    aim to address these challenges by optimizing the way computations are performed
    and resources are used. Open source tools like DeepSpeed³³3https://github.com/microsoft/DeepSpeed,
    FasterTransformer⁴⁴4https://github.com/NVIDIA/FasterTransformer, and vLLM [[18](#bib.bib18)]
    also aim to enable cost-effective and scalable serving of fine-tuned models. In
    this paper, we use LoRAX⁵⁵5https://github.com/predibase/lorax, which is specifically
    designed for the efficient serving of LLMs fine-tuned with LoRA. LoRAX supports
    dynamic adapter loading so adapters can be downloaded asynchronously during inference,
    multiple model families like Llama [[37](#bib.bib37)] and Mistral [[15](#bib.bib15)],
    and bitsandbytes⁶⁶6https://github.com/TimDettmers/bitsandbytes-quantized models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 像分段矩阵-向量乘法（SGMV） [[4](#bib.bib4)]等技术旨在通过优化计算执行方式和资源使用来应对这些挑战。开源工具如DeepSpeed³³3https://github.com/microsoft/DeepSpeed、FasterTransformer⁴⁴4https://github.com/NVIDIA/FasterTransformer和vLLM [[18](#bib.bib18)]也旨在实现经济高效且可扩展的微调模型服务。在本文中，我们使用LoRAX⁵⁵5https://github.com/predibase/lorax，它专门设计用于高效服务经过LoRA微调的LLMs。LoRAX支持动态适配器加载，使适配器可以在推理过程中异步下载，支持多个模型系列，如Llama [[37](#bib.bib37)]和Mistral [[15](#bib.bib15)]，以及bitsandbytes⁶⁶6https://github.com/TimDettmers/bitsandbytes-quantized模型。
- en: 3 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Task selection
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务选择
- en: In selecting datasets and tasks for our study, we prioritize those that are
    widely accessible via Kaggle⁷⁷7https://www.kaggle.com and HuggingFace⁸⁸8https://huggingface.co
    and those that are commonly used for benchmarking such as those on the Open LLM
    Leaderboard [[2](#bib.bib2)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择我们研究的数据集和任务时，我们优先考虑那些通过Kaggle⁷⁷7https://www.kaggle.com和HuggingFace⁸⁸8https://huggingface.co广泛可用的，以及那些常用于基准测试的数据集，如Open
    LLM Leaderboard上的数据集 [[2](#bib.bib2)]。
- en: 'Our selection includes datasets like MMLU [[11](#bib.bib11)] for broad domain
    knowledge, Jigsaw [[6](#bib.bib6)] for content moderation, WikiSQL [[46](#bib.bib46)]
    for SQL generation, and GLUE benchmarks [[39](#bib.bib39)]. We categorize the
    tasks encompassed by these datasets into 5 types:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的选择包括MMLU [[11](#bib.bib11)]用于广泛领域知识，Jigsaw [[6](#bib.bib6)]用于内容审查，WikiSQL [[46](#bib.bib46)]用于SQL生成，以及GLUE基准测试 [[39](#bib.bib39)]。我们将这些数据集所涵盖的任务分类为5种类型：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Classic NLP: Tasks derived from common NLP datasets published between 2018
    and 2022 covering tasks like named entity recognition, data-to-text, and headline
    generation.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经典NLP：来源于2018至2022年间发布的常见NLP数据集的任务，涵盖命名实体识别、数据到文本和标题生成等任务。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Coding: SQL query generation, and Python programming questions, which are mostly
    centered on algorithms and object-oriented design.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码：SQL查询生成和Python编程问题，主要集中在算法和面向对象设计上。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Knowledge: Knowledge-based multiple choice questions.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识：基于知识的选择题。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning: Reasoning-based multiple choice questions.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理：基于推理的选择题。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Math: Numerical, math-based word problems.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数学：数值型、基于数学的问题。
- en: '| Category | Task Name | Task Description | Dataset Link | Metric | Range #
    Tokens | P95 # Tokens | # examples | Split Used for Evaluation |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 任务名称 | 任务描述 | 数据集链接 | 指标 | 范围 # 令牌 | P95 # 令牌 | # 示例 | 用于评估的拆分 |'
- en: '| train | validation | test |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 验证 | 测试 |'
- en: '| Classic NLP | bc5cdr | Chemical and disease recognition | hf://tner/bc5cdr
    | rouge | 143 - 570 | 226 | 5228 | 5330 | 5865 | validation |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 经典NLP | bc5cdr | 化学和疾病识别 | hf://tner/bc5cdr | rouge | 143 - 570 | 226 | 5228
    | 5330 | 5865 | 验证 |'
- en: '| conllpp | Named entity recognition | hf://conllpp | rouge | 110 - 401 | 170
    | 14041 | 3250 | 3453 | test |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| conllpp | 命名实体识别 | hf://conllpp | rouge | 110 - 401 | 170 | 14041 | 3250
    | 3453 | test |'
- en: '| e2e_nlg | Translation from meaning representation to natural language | hf://e2e_nlg
    | rouge | 92 - 213 | 153 | 42061 | 4672 | 4693 | test |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| e2e_nlg | 从意义表示到自然语言的翻译 | hf://e2e_nlg | rouge | 92 - 213 | 153 | 42061 |
    4672 | 4693 | test |'
- en: '| tldr_content_gen | Content generation given a headline | hf://JulesBelveze/tldr_news
    | rouge | 46 - 425 | 204 | 7138 | – | 794 | test |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| tldr_content_gen | 给定标题的内容生成 | hf://JulesBelveze/tldr_news | rouge | 46 -
    425 | 204 | 7138 | – | 794 | test |'
- en: '| tldr_headline_gen | Headline generation given news content | hf://JulesBelveze/tldr_news
    | rouge | 41 - 420 | 199 | 7138 | – | 794 | test |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| tldr_headline_gen | 给定新闻内容的标题生成 | hf://JulesBelveze/tldr_news | rouge | 41
    - 420 | 199 | 7138 | – | 794 | test |'
- en: '| viggo | Translation of video game meaning representations to natural language
    | hf://GEM/viggo | rouge | 151 - 304 | 240 | 5103 | 714 | 1083 | test |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| viggo | 视频游戏含义表示的自然语言翻译 | hf://GEM/viggo | rouge | 151 - 304 | 240 | 5103
    | 714 | 1083 | test |'
- en: '| webnlg | Translation of triples to natural language | hf://web_nlg (release_v3.0_en)
    | rouge | 88 - 345 | 215 | 13211 | 1667 | 5713 | test |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| webnlg | 将三元组翻译成自然语言 | hf://web_nlg (release_v3.0_en) | rouge | 88 - 345
    | 215 | 13211 | 1667 | 5713 | test |'
- en: '| Coding | magicoder | Coding tasks in multiple languages | hf://ise-uiuc/Magicoder-OSS-Instruct-75K
    | humaneval | 141 - 1661 | 805 | 75197 | – | – | (human_eval) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Coding | magicoder | 多语言编码任务 | hf://ise-uiuc/Magicoder-OSS-Instruct-75K |
    humaneval | 141 - 1661 | 805 | 75197 | – | – | (human_eval) |'
- en: '| wikisql | SQL generation given a table and question | hf://wikisql | rouge
    | 198 - 72472 | 1941 | 56355 | 8421 | 15878 | test |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| wikisql | 给定表格和问题生成 SQL | hf://wikisql | rouge | 198 - 72472 | 1941 | 56355
    | 8421 | 15878 | test |'
- en: '| Knowledge | boolq | Knowledge-based yes/no questions. | hf://google/boolq
    | accuracy | 30 - 898 | 271 | 9427 | 3270 | – | validation |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Knowledge | boolq | 基于知识的是/否问题 | hf://google/boolq | accuracy | 30 - 898
    | 271 | 9427 | 3270 | – | validation |'
- en: '| dbpedia | Topic extraction from a news article and title | hf://fancyzhx/dbpedia_14
    | accuracy | 102 - 387 | 211 | 560000 | – | 70000 | test |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| dbpedia | 从新闻文章和标题中提取主题 | hf://fancyzhx/dbpedia_14 | accuracy | 102 - 387
    | 211 | 560000 | – | 70000 | test |'
- en: '| customer_support | Customer support call classification given call transcript
    | github://cricketclub/gridspace-stanford-harper-valley | accuracy | 151 - 679
    | 377 | 1245 | 245 | 391 | test |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| customer_support | 给定通话记录的客户支持电话分类 | github://cricketclub/gridspace-stanford-harper-valley
    | accuracy | 151 - 679 | 377 | 1245 | 245 | 391 | test |'
- en: '| glue_qnli | Does the response answer the question? | hf://glue/viewer/qnli
    | accuracy | 52 - 350 | 123 | 104743 | 5463 | 5463 | validation |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| glue_qnli | 回答是否解答了问题？ | hf://glue/viewer/qnli | accuracy | 52 - 350 | 123
    | 104743 | 5463 | 5463 | validation |'
- en: '| glue_stsb | How similar are the sentences? | hf://glue/viewer/stsb | mae
    | 74 - 187 | 124 | 5749 | 1500 | 1379 | validation |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| glue_stsb | 句子之间有多相似？ | hf://glue/viewer/stsb | mae | 74 - 187 | 124 | 5749
    | 1500 | 1379 | validation |'
- en: '| legal | Legal document classification | kaggle://bahushruth/legalclausedataset
    | rouge | 143 - 885 | 489 | 17000 | 2000 | 1000 | test |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| legal | 法律文档分类 | kaggle://bahushruth/legalclausedataset | rouge | 143 - 885
    | 489 | 17000 | 2000 | 1000 | test |'
- en: '| reuters | Topic extraction from Reuters news articles | hf://reuters21578/viewer/ModLewis
    (modlewis) | rouge | 51 - 2056 | 637 | 13625 | – | 6188 | test |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| reuters | 从路透社新闻文章中提取主题 | hf://reuters21578/viewer/ModLewis (modlewis) |
    rouge | 51 - 2056 | 637 | 13625 | – | 6188 | test |'
- en: '| mmlu | General domain multiple-choice questions | hf://cais/mmlu/viewer/all
    | accuracy | 47 - 1491 | 578 | 99842 | 1531 | 14042 | validation |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| mmlu | 一般领域的选择题 | hf://cais/mmlu/viewer/all | accuracy | 47 - 1491 | 578
    | 99842 | 1531 | 14042 | validation |'
- en: '| Reasoning | winogrande | Common sense 2-option task | hf://winogrande | accuracy
    | 48 - 75 | 63 | 9248 | 1767 | 1267 | test |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning | winogrande | 常识性 2 选项任务 | hf://winogrande | accuracy | 48 - 75
    | 63 | 9248 | 1767 | 1267 | test |'
- en: '| arc_combined | Multiple-choice science questions | hf://allenai/ai2_arc |
    accuracy | 68 - 232 | 143 | 3370 | 869 | 3548 | test |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| arc_combined | 多选题科学问题 | hf://allenai/ai2_arc | accuracy | 68 - 232 | 143
    | 3370 | 869 | 3548 | test |'
- en: '| glue_cola | Grammar and syntax acceptability | hf://glue/viewer/cola | accuracy
    | 45 - 87 | 59 | 8551 | 1043 | 1063 | validation |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| glue_cola | 语法和句法可接受性 | hf://glue/viewer/cola | accuracy | 45 - 87 | 59 |
    8551 | 1043 | 1063 | validation |'
- en: '| glue_mnli | Does the hypothesis entail the premise? | hf://nyu-mll/glue/viewer/mnli
    | accuracy | 64 - 339 | 128 | 392702 | 19647 | 19643 | validation |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| glue_mnli | 假设是否蕴含前提？ | hf://nyu-mll/glue/viewer/mnli | accuracy | 64 - 339
    | 128 | 392702 | 19647 | 19643 | validation |'
- en: '| glue_mrpc | Do the sentences have the same meaning? | hf://glue/viewer/mrpc
    | accuracy | 67 - 157 | 123 | 3668 | 408 | 1725 | validation |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| glue_mrpc | 这些句子的含义是否相同？ | hf://glue/viewer/mrpc | accuracy | 67 - 157 |
    123 | 3668 | 408 | 1725 | validation |'
- en: '| glue_qqp | Do the questions have the same meaning? | hf://glue/viewer/qqp
    | accuracy | 60 - 351 | 102 | 363846 | 40430 | 390965 | validation |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| glue_qqp | 这些问题的含义是否相同？ | hf://glue/viewer/qqp | accuracy | 60 - 351 | 102
    | 363846 | 40430 | 390965 | validation |'
- en: '| glue_sst2 | Binary sentiment detection | hf://glue/viewer/sst2 | accuracy
    | 33 - 91 | 63 | 67349 | 872 | 1821 | validation |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| glue_sst2 | 二元情感检测 | hf://glue/viewer/sst2 | accuracy | 33 - 91 | 63 | 67349
    | 872 | 1821 | validation |'
- en: '| glue_wnli | Pronoun resolution | hf://glue/viewer/wnli | accuracy | 73 -
    160 | 134 | 635 | 71 | 146 | validation |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| glue_wnli | 代词解析 | hf://glue/viewer/wnli | accuracy | 73 - 160 | 134 | 635
    | 71 | 146 | validation |'
- en: '| covid | Sentiment detection of COVID-19 tweets | kaggle://datatattle/covid-19-nlp-text-classification
    | accuracy | 131 - 292 | 223 | 37361 | – | 3798 | test |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| covid | 对 COVID-19 推文的情感检测 | kaggle://datatattle/covid-19-nlp-text-classification
    | accuracy | 131 - 292 | 223 | 37361 | – | 3798 | test |'
- en: '| hellaswag | Multiple-choice sentence completion | hf://Rowan/hellaswag |
    accuracy | 120 - 407 | 341 | 39905 | 10003 | 10042 | validation |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag | 多选题句子填空 | hf://Rowan/hellaswag | accuracy | 120 - 407 | 341 |
    39905 | 10003 | 10042 | validation |'
- en: '| hellaswag_processed | Sentence completion | hf://Rowan/hellaswag | rouge
    | 75 - 205 | 185 | 39905 | 10003 | 10042 | validation |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag_processed | 句子完成 | hf://Rowan/hellaswag | rouge | 75 - 205 | 185
    | 39905 | 10003 | 10042 | 验证 |'
- en: '| jigsaw | Toxic comment classification | kaggle://c/jigsaw-unintended-bias-in-toxicity-classification
    | accuracy | 409 - 715 | 601 | 159571 | – | 153164 | test |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| jigsaw | 有毒评论分类 | kaggle://c/jigsaw-unintended-bias-in-toxicity-classification
    | 准确率 | 409 - 715 | 601 | 159571 | – | 153164 | 测试 |'
- en: '| drop | Question answering given a passage | hf://drop | rouge | 87 - 2275
    | 571 | 77400 | 9535 | – | validation |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| drop | 给定一段文本的问答 | hf://drop | rouge | 87 - 2275 | 571 | 77400 | 9535 | –
    | 验证 |'
- en: '| Math | gsm8k | Grade school math problems | hf://gsm8k (main) | accuracy
    | 58 - 465 | 276 | 7473 | – | 1319 | test |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Math | gsm8k | 小学数学问题 | hf://gsm8k (main) | 准确率 | 58 - 465 | 276 | 7473 |
    – | 1319 | 测试 |'
- en: 'Table 1: Tasks and datasets used. tldr_news and hellaswag datasets are used
    for multiple tasks. The length of the texts vary substantially across tasks. Many
    tasks and datasets exhibit a long-tail distribution, where a small number of examples
    have significantly longer sequences than the average. Token counts are based on
    the tiktoken package [[27](#bib.bib27)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用的任务和数据集。tldr_news和hellaswag数据集用于多个任务。文本长度在不同任务之间差异很大。许多任务和数据集表现出长尾分布，其中少量示例的序列长度显著长于平均值。标记计数基于tiktoken包[[27](#bib.bib27)]。
- en: 3.2 Prompt selection
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提示选择
- en: '![Refer to caption](img/4c737acea748addd9ee3d209d6131672.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4c737acea748addd9ee3d209d6131672.png)'
- en: 'Figure 2: Examples of different styles of prompting. To maintain using the
    same prompts when comparing models and to ensure the highest likelihood of success
    amongst all types of models (fine-tuned, auto-complete, or instruction-tuned),
    all of our prompts adhere to completion style.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同风格提示的示例。为了在比较模型时保持使用相同的提示，并确保所有类型模型（微调、自动完成或指令调整）中的成功概率最高，我们所有的提示都遵循完成式风格。
- en: Previous studies have demonstrated the potential of leveraging prompt engineering
    techniques, such as the use of majority voting [48], the inclusion of multiple
    in-context examples (n-shot) [[34](#bib.bib34)], MedPrompt [[25](#bib.bib25)],
    chain-of-thought prompting [[43](#bib.bib43)], etc., to enhance model performance
    on specific tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究已经证明了利用提示工程技术的潜力，例如使用多数投票[48]、包含多个上下文示例（n-shot）[[34](#bib.bib34)]、MedPrompt[[25](#bib.bib25)]、思维链提示[[43](#bib.bib43)]等，以提高模型在特定任务上的表现。
- en: 'In our evaluations, we consciously choose not to employ additional prompt engineering
    or tuning strategies for any specific dataset, task, or model. Although using
    more in-context examples or a more selective approach in n-shot prompting might
    yield better results, we prioritize reproducibility and the minimization of biases
    that could arise from customized in-context learning. Instead, we opt to use simple
    zero or single-shot completion-style prompts for all tasks. Our prompts are written
    in the completion style, described in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Prompt
    selection ‣ 3 Methodology ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A
    Technical Report"), to provide a fair comparison across fine-tuned, instruction-tuned,
    and auto-complete models. For classification tasks, the prompt includes all possible
    classes to inform the model’s responses. For more specialized tasks, where describing
    the expected output format is challenging, we use a single in-context example
    – the first example from the published training split – to guide the model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的评估中，我们刻意选择不为任何特定的数据集、任务或模型使用额外的提示工程或调整策略。虽然使用更多的上下文示例或在n-shot提示中采取更具选择性的方法可能会产生更好的结果，但我们优先考虑可重复性和最小化可能因定制上下文学习而产生的偏差。相反，我们选择对所有任务使用简单的零-shot或单-shot完成式提示。我们的提示采用完成式风格，如图[2](#S3.F2
    "Figure 2 ‣ 3.2 Prompt selection ‣ 3 Methodology ‣ LoRA Land: 310 Fine-tuned LLMs
    that Rival GPT-4, A Technical Report")所示，以便在微调、指令调整和自动完成模型之间进行公平比较。对于分类任务，提示包括所有可能的类别，以便告知模型的响应。对于更专业的任务，描述预期输出格式具有挑战性时，我们使用单个上下文示例——来自已发布训练集的第一个示例——来指导模型。'
- en: '![[Uncaptioned image]](img/e32d81d18506e166b50991c3a3928f7a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![[无说明图片]](img/e32d81d18506e166b50991c3a3928f7a.png)'
- en: 'Table 2: Examples of prompts that are used in this study, all written in completion
    style. For more specialized tasks, where describing the expected output format
    is challenging (e.g. bc5cdr), we use a single in-context example — the first example
    from the published training split — to guide the model.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：本研究中使用的提示示例，所有示例均以完成风格书写。对于更专业的任务（如bc5cdr），描述预期输出格式较为困难，我们使用一个单一的上下文示例——来自发布训练拆分的第一个示例——来引导模型。
- en: 'Finally, we follow prescribed prompt tagging conventions for each model, as
    outlined in the respective model’s documentation on HuggingFace, to ensure proper
    querying of pre-trained and instruction-tuned base models. This includes using
    "[INST] … [/INST]" for prompts intended for Mistral Instruct, and "user
    … " for Gemma’s instruction-tuned models. For
    detailed information on the exact prompt templates applied to each task and model,
    please see Appendix [A](#A1 "Appendix A Prompts for all tasks ‣ LoRA Land: 310
    Fine-tuned LLMs that Rival GPT-4, A Technical Report").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们遵循每个模型规定的提示标记约定，按照HuggingFace上相应模型的文档中概述的规范，以确保对预训练和指令调优的基础模型进行适当查询。这包括对Mistral
    Instruct使用"[INST] … [/INST]"，对Gemma的指令调优模型使用"user … "。有关应用于每个任务和模型的确切提示模板的详细信息，请参见附录[A](#A1
    "附录 A 所有任务的提示 ‣ LoRA Land: 310种精调的LLM与GPT-4竞争，技术报告")。'
- en: 3.3 Base models
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基础模型
- en: 'All base models are listed in Table [3](#S3.T3 "Table 3 ‣ 3.3 Base models ‣
    3 Methodology ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report").
    We use GPT-4 (gpt-4-0613) and GPT-3.5-Turbo (gpt-3.5-turbo-0125) as two strong
    LLM baselines. Our selection of these ten base models was guided by several key
    considerations, including their widespread adoption within the AI community, availability
    with permissive licenses, and availability of technical reports. We specifically
    choose base models with $\leq 8$ billion parameters to ensure that each model
    can be efficiently trained within the resource limits of a single A10G GPU.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '所有基础模型列在表[3](#S3.T3 "表 3 ‣ 3.3 基础模型 ‣ 3 方法论 ‣ LoRA Land: 310种精调的LLM与GPT-4竞争，技术报告")中。我们使用GPT-4（gpt-4-0613）和GPT-3.5-Turbo（gpt-3.5-turbo-0125）作为两个强大的LLM基线。我们选择这十种基础模型的主要考虑因素包括它们在AI社区中的广泛应用、具有宽松许可证的可用性和技术报告的可用性。我们特别选择了参数≤8亿的基础模型，以确保每个模型可以在单个A10G
    GPU的资源限制内高效训练。'
- en: '| Model Name | Creator | # of Parameters | Date Released |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 创造者 | 参数数量 | 发布日期 |'
- en: '| Llama-2-7b | Meta | 7B | July 18, 2023 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7b | Meta | 7B | 2023年7月18日 |'
- en: '| Llama-2-7b-chat | Meta | 7B | July 18, 2023 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7b-chat | Meta | 7B | 2023年7月18日 |'
- en: '| Mistral-7b-v0.1 | Mistral AI | 7.24B | September 20, 2023 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b-v0.1 | Mistral AI | 7.24B | 2023年9月20日 |'
- en: '| Mistral-7b-Instruct-v0.1 | Mistral AI | 7.24B | September 27, 2023 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b-Instruct-v0.1 | Mistral AI | 7.24B | 2023年9月27日 |'
- en: '| Zephyr-7b | Hugging Face | 7.24B | October 26, 2023 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Zephyr-7b | Hugging Face | 7.24B | 2023年10月26日 |'
- en: '| Phi-2b | Microsoft | 2.78B | December 13, 2023 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Phi-2b | 微软 | 2.78B | 2023年12月13日 |'
- en: '| Gemma-2b | Google | 2.51B | February 21, 2024 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2b | 谷歌 | 2.51B | 2024年2月21日 |'
- en: '| Gemma-2b-it | Google | 2.51B | February 21, 2024 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2b-it | 谷歌 | 2.51B | 2024年2月21日 |'
- en: '| Gemma-7b | Google | 8.54B | February 21, 2024 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7b | 谷歌 | 8.54B | 2024年2月21日 |'
- en: '| Gemma-7b-it | Google | 8.54B | February 21, 2024 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7b-it | 谷歌 | 8.54B | 2024年2月21日 |'
- en: 'Table 3: Base models used in LoRA-based fine-tuning experiments. To train all
    models on A10G hardware, all chosen base models are  7B parameters or smaller.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：用于LoRA基础调整实验的基础模型。为了在A10G硬件上训练所有模型，所选择的基础模型参数均为7B或更少。
- en: 3.4 Training parameters
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 训练参数
- en: Each model is trained with published train splits⁹⁹9customer_support and legal
    are the only two tasks in our list without official splits. The exact splits for
    these datasets are published on ¡[github.com/predibase/lora-bakeoff](github.com/predibase/lora-bakeoff)¿..
    Each model is trained for $40000$ warm-up fraction ($1200$.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都使用发布的训练拆分进行训练⁹⁹9，客户支持和法律是我们列表中唯一没有官方拆分的两个任务。这些数据集的确切拆分已发布在¡[github.com/predibase/lora-bakeoff](github.com/predibase/lora-bakeoff)¿..
    每个模型的训练时间为$40000$暖启动阶段（$1200$）。
- en: These training parameters, combined with gradient checkpointing, allow each
    LLM to be fine-tuned on a single A10 GPU with 24 GB of memory. For tasks where
    training on the full sequence lengths would still produce a GPU Out-Of-Memory
    (OOM) error, we first truncate example inputs to a maximum sequence length set
    as the 95th percentile of all task inputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练参数结合梯度检查点，使每个LLM能够在具有24GB内存的单个A10 GPU上进行微调。对于在完整序列长度上训练仍会导致GPU内存溢出（OOM）错误的任务，我们首先将示例输入截断到最大序列长度，该长度设置为所有任务输入的第95百分位数。
- en: To guarantee a consistent and straightforward basis of comparison across models,
    no additional hyperparameter tuning is applied to any specific dataset, task,
    or base model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证模型之间比较的一致性和简洁性，没有对任何特定的数据集、任务或基础模型应用额外的超参数调优。
- en: 'Training recipes for each model are provided as Ludwig [[24](#bib.bib24)] configurations
    for each of the fine-tuned LLMs and can be found at [https://huggingface.co/predibase](https://huggingface.co/predibase).
    Figure [3](#S3.F3 "Figure 3 ‣ 3.4 Training parameters ‣ 3 Methodology ‣ LoRA Land:
    310 Fine-tuned LLMs that Rival GPT-4, A Technical Report") shows an example of
    a config.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '每个模型的训练方案作为Ludwig [[24](#bib.bib24)]配置提供，适用于每个微调后的LLM，可在[https://huggingface.co/predibase](https://huggingface.co/predibase)找到。图[3](#S3.F3
    "Figure 3 ‣ 3.4 Training parameters ‣ 3 Methodology ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report")展示了一个配置示例。'
- en: '![Refer to caption](img/1e144c6847c6cec5e262a803208f1d23.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1e144c6847c6cec5e262a803208f1d23.png)'
- en: 'Figure 3: Example LLM model training configuration for LoRA-based fine-tuning.
    Based on Ludwig [[24](#bib.bib24)].'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于LoRA的微调示例LLM模型训练配置。基于Ludwig [[24](#bib.bib24)]。
- en: 3.5 Evaluation
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 评估
- en: 'As specified in Table [1](#S3.T1 "Table 1 ‣ 3.1 Task selection ‣ 3 Methodology
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report"), models
    are evaluated on the test split if it exists and is labeled, or the validation
    set otherwise^(10)^(10)10MMLU has a published test set with labels, however, we
    use validation split to be consistent with the HELM benchmark [[20](#bib.bib20)].
    We employ a tailored set of evaluation metrics to accurately assess the performance
    across all of the tasks. We use accuracy for classification tasks, (1 - mean absolute
    error) for regression tasks^(11)^(11)11Mean absolute error (MAE) is used because
    the range of target values are integer-like and small., and rouge-L^(12)^(12)12Text
    generation tasks are complicated to evaluate automatically [[16](#bib.bib16)].
    ROUGE-L is a widely adopted proxy metric that focuses on the longest common subsequence
    between the generated text and the reference text, which captures the semantic
    similarity between the generated and reference texts rather than relying solely
    on exact word matches. ROUGE-L may not fully capture aspects like fluency, coherence
    and should be used in conjunction with other metrics and human evaluations to
    provide a fuller assessment of text generation quality. for generation tasks.
    The WikiSQL dataset has its own [evaluation suite](https://github.com/salesforce/WikiSQL/tree/master),
    however due to challenges integrating the WikiSQL evaluation suite, we have adopted
    the ROUGE metric as a proxy for assessing query quality^(13)^(13)13Although ROUGE
    is not tailored for SQL queries, it offers a viable alternative for gauging the
    alignment between generated and target queries.. For coding, we use HumanEval [[5](#bib.bib5)].
    For GSM8K [[7](#bib.bib7)], a regex-based heuristic [[9](#bib.bib9)] is used to
    extract the mathematical answer to be consistent with the Open LLM Leaderboard [[2](#bib.bib2)].
    All metrics are on a 0 to 1 scale, where 0 is the worst possible score, and 1
    the best possible score.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S3.T1 "表 1 ‣ 3.1 任务选择 ‣ 3 方法论 ‣ LoRA Land: 310 个对标 GPT-4 的精调 LLMs, 技术报告")中所述，如果存在并标记了测试集，则在测试集上评估模型，否则在验证集上进行评估^(10)^(10)10MMLU发布了带标签的测试集，但为了与HELM基准保持一致，我们使用验证集[[20](#bib.bib20)]。我们采用量身定制的评估指标集，以准确评估所有任务的性能。对于分类任务使用准确率，对于回归任务使用（1
    - 平均绝对误差）^(11)^(11)11平均绝对误差（MAE）用于因为目标值范围类似整数且较小。，对于生成任务使用rouge-L^(12)^(12)12文本生成任务的自动评估很复杂[[16](#bib.bib16)]。ROUGE-L是一种广泛采用的代理指标，关注生成文本与参考文本之间的最长公共子序列，这捕捉了生成文本与参考文本之间的语义相似性，而不仅仅依赖于完全匹配的单词。ROUGE-L可能无法完全捕捉流畅性、连贯性等方面，因此应与其他指标和人工评估结合使用，以提供更全面的文本生成质量评估。WikiSQL数据集有自己的[评估套件](https://github.com/salesforce/WikiSQL/tree/master)，然而由于集成WikiSQL评估套件的挑战，我们采用ROUGE指标作为评估查询质量的代理^(13)^(13)13尽管ROUGE并非专门针对SQL查询，但它提供了一个衡量生成查询与目标查询对齐度的可行替代方案..
    对于编码，我们使用HumanEval[[5](#bib.bib5)]。对于GSM8K[[7](#bib.bib7)]，使用基于正则表达式的启发式方法[[9](#bib.bib9)]提取数学答案，以与Open
    LLM Leaderboard[[2](#bib.bib2)]保持一致。所有指标的范围是0到1，其中0是最差分数，1是最佳分数。'
- en: Non-fine-tuned models often generate more varied outputs, including unintended
    artifacts such as additional words or explanations not specified in the prompt.
    For classification tasks, sometimes these models will generate the actual class
    string like "Yes/No", "positive/negative" or "True/False" spelled out, instead
    of the true "1/0" label in the dataset even when instructed. To minimize metric
    deductions due to response parsing strictness, we first use a regex-based extraction
    step to map the model’s response to the ground truth vocabulary. If there are
    multiple matches in the generated text, the first valid match is used. The code
    for regex-based pre-metric response extractions are available at [github.com/predibase/lora-bakeoff](https://www.github.com/predibase/lora-bakeoff).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 未经过精细调优的模型通常生成更多样化的输出，包括一些未在提示中指定的意外产物，如额外的单词或解释。对于分类任务，这些模型有时会生成实际的类别字符串，如“是/否”、“正面/负面”或“真/假”，而不是数据集中真实的“1/0”标签，即使在指示时也是如此。为了最小化由于响应解析严格性带来的指标扣分，我们首先使用基于正则表达式的提取步骤，将模型的响应映射到真实词汇。如果生成的文本中有多个匹配项，则使用第一个有效匹配项。基于正则表达式的预指标响应提取代码可在[github.com/predibase/lora-bakeoff](https://www.github.com/predibase/lora-bakeoff)找到。
- en: Financial constraints associated with LLM APIs are not trivial. For example,
    using GPT-4 to assess the complete WikiSQL test set of 15,878 examples would cost
    approximately $400, considering the average input (805) and output (16) token
    counts per example. Such costs can be prohibitive, especially for organizations
    or researchers operating on limited budgets.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM API相关的财务限制并非微不足道。例如，使用GPT-4评估包含15,878个样本的完整WikiSQL测试集的费用约为400美元，考虑到每个样本的平均输入（805）和输出（16）标记计数。这些费用可能是禁止性的，尤其对于预算有限的组织或研究人员来说。
- en: To manage costs while maintaining rigor, we restrict evaluations to the first
    1000 examples for datasets with evaluation splits larger than 1000 examples. We
    acknowledge that this method may introduce selection bias and affect the generalizability
    of our findings. We recommend that future research considers more expansive evaluations
    as resources permit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在保持严格性的同时管理成本，我们将数据集评估限制在前1000个样本，对于评估拆分大于1000个样本的数据集尤为如此。我们承认这种方法可能会引入选择性偏差并影响我们发现的普遍性。我们建议未来的研究在资源允许的情况下考虑更广泛的评估。
- en: 4 Results
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: '![Refer to caption](img/f79042e35a49942dcf38ab3faafe3bad.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f79042e35a49942dcf38ab3faafe3bad.png)'
- en: 'Figure 4: Performance lift from the best fine-tuned LLM over 1) the best base
    model (<= 7B) (in blue) and GPT-4 (in red) across 31 tasks, in absolute points.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：最佳微调LLM在31个任务中相对于1) 最佳基础模型（<= 7B）（蓝色）和GPT-4（红色）的性能提升，以绝对分数表示。
- en: 'LoRA fine-tuning provides a consistent and significant boost from fine-tuning
    across base models and tasks, as seen in Figure [4](#S4.F4 "Figure 4 ‣ 4 Results
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report"). Before
    fine-tuning, GPT-4 and GPT-3.5 have the strongest performance out of the box compared
    to all other base models, with 0.599 and 0.661 overall scores, respectively. Performance
    boosts from fine-tuning range from +26.3 to +51.2 points of improvement depending
    on the base model, and +38.7 on average (Table [4](#S4.T4 "Table 4 ‣ 4 Results
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")). Depending
    on the task, the best fine-tuned LLM outperforms the best base model from +8.3
    to +67.5 points, +25.0 points on average (Table [5](#S4.T5 "Table 5 ‣ 4 Results
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA微调提供了从基础模型和任务中微调的一致且显著的提升，如图[4](#S4.F4 "图 4 ‣ 4 结果 ‣ LoRA领域：310个微调LLM与GPT-4匹敌，技术报告")所示。在微调之前，GPT-4和GPT-3.5在所有其他基础模型中表现最强，整体得分分别为0.599和0.661。根据基础模型的不同，微调带来的性能提升范围从+26.3到+51.2分不等，平均提升为+38.7分（表[4](#S4.T4
    "表 4 ‣ 4 结果 ‣ LoRA领域：310个微调LLM与GPT-4匹敌，技术报告")）。根据任务的不同，最佳微调LLM超越最佳基础模型的得分从+8.3到+67.5分不等，平均提升为+25.0分（表[5](#S4.T5
    "表 5 ‣ 4 结果 ‣ LoRA领域：310个微调LLM与GPT-4匹敌，技术报告")）。
- en: '| Task | Metric | Best BM | Best FT | GPT-4 | Lift over BM | Lift over GPT-4
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 指标 | 最佳 BM | 最佳 FT | GPT-4 | 超越 BM 的提升 | 超越 GPT-4 的提升 |'
- en: '| magicoder | humaneval | 0.201 | 0.433 | 0.829 | 0.232 | -0.396 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| magicoder | humaneval | 0.201 | 0.433 | 0.829 | 0.232 | -0.396 |'
- en: '| mmlu | accuracy | 0.506 | 0.589 | 0.774 | 0.083 | -0.185 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| mmlu | accuracy | 0.506 | 0.589 | 0.774 | 0.083 | -0.185 |'
- en: '| glue_wnli | accuracy | 0.437 | 0.873 | 0.93 | 0.436 | -0.057 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| glue_wnli | accuracy | 0.437 | 0.873 | 0.93 | 0.436 | -0.057 |'
- en: '| arc_combined | accuracy | 0.673 | 0.915 | 0.947 | 0.242 | -0.032 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| arc_combined | accuracy | 0.673 | 0.915 | 0.947 | 0.242 | -0.032 |'
- en: '| wikisql | rouge | 0.301 | 0.898 | 0.909 | 0.597 | -0.011 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| wikisql | rouge | 0.301 | 0.898 | 0.909 | 0.597 | -0.011 |'
- en: '| boolq | accuracy | 0.764 | 0.909 | 0.911 | 0.145 | -0.002 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| boolq | accuracy | 0.764 | 0.909 | 0.911 | 0.145 | -0.002 |'
- en: '| customer_support | accuracy | 0.850 | 1.000 | 1.000 | 0.150 | 0.000 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| customer_support | accuracy | 0.850 | 1.000 | 1.000 | 0.150 | 0.000 |'
- en: '| glue_cola | accuracy | 0.797 | 0.872 | 0.864 | 0.075 | 0.008 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| glue_cola | accuracy | 0.797 | 0.872 | 0.864 | 0.075 | 0.008 |'
- en: '| winogrande | accuracy | 0.576 | 0.84 | 0.832 | 0.264 | 0.008 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| winogrande | accuracy | 0.576 | 0.84 | 0.832 | 0.264 | 0.008 |'
- en: '| glue_sst2 | accuracy | 0.933 | 0.961 | 0.942 | 0.028 | 0.019 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| glue_sst2 | accuracy | 0.933 | 0.961 | 0.942 | 0.028 | 0.019 |'
- en: '| dbpedia | accuracy | 0.868 | 0.988 | 0.965 | 0.120 | 0.023 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| dbpedia | accuracy | 0.868 | 0.988 | 0.965 | 0.120 | 0.023 |'
- en: '| hellaswag | accuracy | 0.393 | 0.834 | 0.805 | 0.441 | 0.029 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag | accuracy | 0.393 | 0.834 | 0.805 | 0.441 | 0.029 |'
- en: '| glue_qnli | accuracy | 0.743 | 0.931 | 0.902 | 0.188 | 0.029 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| glue_qnli | accuracy | 0.743 | 0.931 | 0.902 | 0.188 | 0.029 |'
- en: '| e2e_nlg | rouge | 0.482 | 0.552 | 0.513 | 0.070 | 0.039 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| e2e_nlg | rouge | 0.482 | 0.552 | 0.513 | 0.070 | 0.039 |'
- en: '| glue_qqp | accuracy | 0.708 | 0.883 | 0.841 | 0.175 | 0.042 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| glue_qqp | accuracy | 0.708 | 0.883 | 0.841 | 0.175 | 0.042 |'
- en: '| bc5cdr | rouge | 0.703 | 0.972 | 0.89 | 0.269 | 0.082 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| bc5cdr | rouge | 0.703 | 0.972 | 0.89 | 0.269 | 0.082 |'
- en: '| glue_mnli | accuracy | 0.455 | 0.899 | 0.803 | 0.444 | 0.096 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Glue_MNLI | 准确率 | 0.455 | 0.899 | 0.803 | 0.444 | 0.096 |'
- en: '| webnlg | rouge | 0.563 | 0.681 | 0.583 | 0.118 | 0.098 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| WebNLG | Rouge | 0.563 | 0.681 | 0.583 | 0.118 | 0.098 |'
- en: '| tldr_content_gen | rouge | 0.183 | 0.23 | 0.125 | 0.047 | 0.105 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| TLDR_Content_Gen | Rouge | 0.183 | 0.23 | 0.125 | 0.047 | 0.105 |'
- en: '| glue_mrpc | accuracy | 0.694 | 0.887 | 0.777 | 0.193 | 0.11 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Glue_MRPC | 准确率 | 0.694 | 0.887 | 0.777 | 0.193 | 0.11 |'
- en: '| jigsaw | accuracy | 0.704 | 0.867 | 0.754 | 0.163 | 0.113 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 拼图 | 准确率 | 0.704 | 0.867 | 0.754 | 0.163 | 0.113 |'
- en: '| hellaswag_processed | rouge | 0.146 | 0.261 | 0.134 | 0.115 | 0.127 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Hellaswag_Processed | Rouge | 0.146 | 0.261 | 0.134 | 0.115 | 0.127 |'
- en: '| viggo | rouge | 0.374 | 0.505 | 0.374 | 0.131 | 0.131 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Viggo | Rouge | 0.374 | 0.505 | 0.374 | 0.131 | 0.131 |'
- en: '| glue_stsb | mae | 0.814 | 0.913 | 0.773 | 0.099 | 0.14 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Glue_STSB | MAE | 0.814 | 0.913 | 0.773 | 0.099 | 0.14 |'
- en: '| gsm8k | accuracy | 0.364 | 0.569 | 0.373 | 0.205 | 0.196 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 准确率 | 0.364 | 0.569 | 0.373 | 0.205 | 0.196 |'
- en: '| conllpp | rouge | 0.733 | 0.989 | 0.742 | 0.256 | 0.247 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| CoNLLPP | Rouge | 0.733 | 0.989 | 0.742 | 0.256 | 0.247 |'
- en: '| tldr_headline_gen | rouge | 0.174 | 0.441 | 0.175 | 0.267 | 0.266 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| TLDR_Headline_Gen | Rouge | 0.174 | 0.441 | 0.175 | 0.267 | 0.266 |'
- en: '| drop | rouge | 0.066 | 0.741 | 0.393 | 0.675 | 0.348 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 丢弃 | Rouge | 0.066 | 0.741 | 0.393 | 0.675 | 0.348 |'
- en: '| legal | rouge | 0.158 | 0.683 | 0.305 | 0.525 | 0.378 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 法律 | Rouge | 0.158 | 0.683 | 0.305 | 0.525 | 0.378 |'
- en: '| reuters | rouge | 0.010 | 0.479 | 0.014 | 0.469 | 0.465 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 路透社 | Rouge | 0.010 | 0.479 | 0.014 | 0.469 | 0.465 |'
- en: '| covid | accuracy | 0.322 | 0.843 | 0.309 | 0.521 | 0.534 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| COVID | 准确率 | 0.322 | 0.843 | 0.309 | 0.521 | 0.534 |'
- en: '| Average |  | 0.506 | 0.756 | 0.661 | 0.250 | 0.095 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 |  | 0.506 | 0.756 | 0.661 | 0.250 | 0.095 |'
- en: 'Table 4: Best model performance for each task, before and after fine-tuning,
    compared to GPT-4.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：每个任务的最佳模型性能，微调前后，与GPT-4的比较。
- en: 'After fine-tuning, 301/310 models surpass their base model counterpart^(14)^(14)14Most
    instances where fine-tuning was worse than the base model were in the family of
    Gemma models. This is possibly due to the bugs with the Gemma family of models
    as identified by Unsloth[[10](#bib.bib10)], which were not accounted for when
    benchmarks were collected., while 224/310 fine-tuned LLMs surpass the benchmark
    set by GPT-4 (Table [5](#S4.T5 "Table 5 ‣ 4 Results ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report")). Gemma-2b is the worst performing
    base model after fine-tuning, but also experiences the largest lift from fine-tuning
    overall, which suggests that models with lower initial scores stand to benefit
    the most from fine-tuning (Figure 1).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '微调后，301/310个模型超越了其基础模型^(14)^(14)14在微调效果不如基础模型的实例中，大多数都在Gemma模型系列中。这可能是由于Unsloth[[10](#bib.bib10)]识别出的Gemma模型系列的缺陷，在收集基准数据时未被考虑到。)，而224/310个微调的LLM超越了GPT-4设定的基准（表
    [5](#S4.T5 "表 5 ‣ 4 结果 ‣ LoRA Land: 310 个微调 LLM 与 GPT-4 竞争的技术报告")）。Gemma-2b是微调后的表现最差的基础模型，但也在微调后获得了最大的提升，这表明初始得分较低的模型在微调中可能会受益最多（图1）。'
- en: By overall average across all tasks, all fine-tuned models perform better than
    GPT-3.5, and all 7B fine-tuned models perform better than GPT-4, except for gemma-7b
    and gemma-7b-it. Phi-2, with as few as 2 billion parameters, exhibits performance
    competitive with GPT-4 after fine-tuning, consistent with the findings of the
    Phi-2 technical report [46].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 按所有任务的整体平均值来看，所有微调模型的表现都优于GPT-3.5，所有7B的微调模型都优于GPT-4，除了gemma-7b和gemma-7b-it。Phi-2在经过微调后，其性能与GPT-4相当，尽管只有20亿参数，这与Phi-2技术报告的发现一致[46]。
- en: 'Averaged over 31 tasks, the overall performance of the best fine-tuned LLMs
    (0.756) are significantly higher than GPT-4 (0.661) (Table [5](#S4.T5 "Table 5
    ‣ 4 Results ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")).
    A detailed breakdown of performance per model, per task, can be found in Appendix
    [C](#A3 "Appendix C Full Results Tables ‣ LoRA Land: 310 Fine-tuned LLMs that
    Rival GPT-4, A Technical Report").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '在31个任务的平均值上，最佳微调LLM的整体表现（0.756）明显高于GPT-4（0.661）（表 [5](#S4.T5 "表 5 ‣ 4 结果 ‣
    LoRA Land: 310 个微调 LLM 与 GPT-4 竞争的技术报告")）。每个模型每个任务的详细表现分解可以在附录 [C](#A3 "附录 C 完整结果表
    ‣ LoRA Land: 310 个微调 LLM 与 GPT-4 竞争的技术报告")中找到。'
- en: '| Base Model | No FT | With FT |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 | 未微调 | 微调后 |'
- en: '&#124; Average lift &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均提升 &#124;'
- en: '&#124; from FT &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自微调 &#124;'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Average lift &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均提升 &#124;'
- en: '&#124; from FT &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自微调 &#124;'
- en: '&#124; vs. GPT-4 &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; vs. GPT-4 &#124;'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Frequency &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 频率 &#124;'
- en: '&#124; FT >No FT &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FT > 未微调 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Frequency &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 频率 &#124;'
- en: '&#124; FT >GPT-4 &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FT >GPT-4 &#124;'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Frequency &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 频率 &#124;'
- en: '&#124; FT = max(task) &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FT = 最大值(task) &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| gpt-3.5-turbo | 0.599 | — | — | — | — | — | 0/31 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo | 0.599 | — | — | — | — | — | 0/31 |'
- en: '| gemma-2b-instruct | 0.326 | 0.645 | 0.319 | -0.016 | 96.7% (30/31) | 64.5%
    (20/31) | 0/31 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| gemma-2b-instruct | 0.326 | 0.645 | 0.319 | -0.016 | 96.7% (30/31) | 64.5%
    (20/31) | 0/31 |'
- en: '| gemma-7b | 0.187 | 0.645 | 0.458 | -0.016 | 93.5% (29/31) | 64.5% (20/31)
    | 1/31 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| gemma-7b | 0.187 | 0.645 | 0.458 | -0.016 | 93.5% (29/31) | 64.5% (20/31)
    | 1/31 |'
- en: '| gemma-7b-instruct | 0.377 | 0.656 | 0.279 | -0.005 | 83.8% (26/31) | 64.5%
    (20/31) | 0/31 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| gemma-7b-instruct | 0.377 | 0.656 | 0.279 | -0.005 | 83.8% (26/31) | 64.5%
    (20/31) | 0/31 |'
- en: '| gemma-2b | 0.145 | 0.657 | 0.512 | -0.004 | 100.0% (31/31) | 67.7% (21/31)
    | 0/31 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| gemma-2b | 0.145 | 0.657 | 0.512 | -0.004 | 100.0% (31/31) | 67.7% (21/31)
    | 0/31 |'
- en: '| gpt-4 | 0.661 | — | — | — | — | — | 6/31 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 0.661 | — | — | — | — | — | 6/31 |'
- en: '| phi-2 | 0.274 | 0.677 | 0.403 | 0.016 | 100.0% (31/31) | 71.0% (22/31) |
    1/31 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| phi-2 | 0.274 | 0.677 | 0.403 | 0.016 | 100.0% (31/31) | 71.0% (22/31) |
    1/31 |'
- en: '| llama-2-7b | 0.252 | 0.696 | 0.444 | 0.035 | 96.7% (30/31) | 67.7% (21/31)
    | 0/31 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-7b | 0.252 | 0.696 | 0.444 | 0.035 | 96.7% (30/31) | 67.7% (21/31)
    | 0/31 |'
- en: '| llama-2-7b-chat | 0.370 | 0.708 | 0.337 | 0.047 | 100.0% (31/31) | 74.2%
    (23/31) | 0/31 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-7b-chat | 0.370 | 0.708 | 0.337 | 0.047 | 100.0% (31/31) | 74.2%
    (23/31) | 0/31 |'
- en: '| mistral-7b-instruct | 0.462 | 0.724 | 0.263 | 0.063 | 100.0% (31/31) | 77.4%
    (24/31) | 3/31 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b-instruct | 0.462 | 0.724 | 0.263 | 0.063 | 100.0% (31/31) | 77.4%
    (24/31) | 3/31 |'
- en: '| mistral-7b | 0.271 | 0.732 | 0.461 | 0.071 | 100.0% (31/31) | 83.8% (26/31)
    | 10/31 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b | 0.271 | 0.732 | 0.461 | 0.071 | 100.0% (31/31) | 83.8% (26/31)
    | 10/31 |'
- en: '| zephyr-7b-beta | 0.350 | 0.742 | 0.392 | 0.081 | 100.0% (31/31) | 87.1% (27/31)
    | 8/31 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| zephyr-7b-beta | 0.350 | 0.742 | 0.392 | 0.081 | 100.0% (31/31) | 87.1% (27/31)
    | 8/31 |'
- en: '| Average | 0.301 | 0.688 | 0.387 | 0.027 | 97.1% (301/310) | 72.3% (224/310)
    |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.301 | 0.688 | 0.387 | 0.027 | 97.1% (301/310) | 72.3% (224/310) |  |'
- en: 'Table 5: Model performance by base model averaged over 31 tasks, before and
    after fine-tuning.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：基础模型在31个任务中的表现，微调前后的平均值。
- en: 5 Discussion and Analysis
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与分析
- en: 5.1 Which Base Model is the best for LoRA Fine-tuning?
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 哪种基础模型最适合 LoRA 微调？
- en: 'Mistral-7B and Zephyr-7b-beta emerge as leaders, albeit in different categories.
    Mistral-7B frequently achieves top performance across the most number of tasks
    (10/31), suggesting a high adaptability (Figure [5](#S5.F5 "Figure 5 ‣ 5.1 Which
    Base Model is the best for LoRA Fine-tuning? ‣ 5 Discussion and Analysis ‣ LoRA
    Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")). Conversely,
    Zephyr boasts the highest overall average performance (0.731). Mistral-7b, Mistral-7b-instruct,
    and Zephyr-7b-beta (which is itself based on Mistral-7b-instruct [[38](#bib.bib38)])
    lead the pack for LoRA fine-tuning performance, ahead of Llama, Phi, and Gemma
    families.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral-7B 和 Zephyr-7b-beta 脱颖而出，尽管在不同类别中。Mistral-7B 在最多数量的任务中经常取得最佳表现（10/31），表明其高适应性（图
    [5](#S5.F5 "图 5 ‣ 5.1 哪种基础模型最适合 LoRA 微调？ ‣ 5 讨论与分析 ‣ LoRA Land：310 个微调的 LLM 与
    GPT-4 竞争的技术报告")）。相对而言，Zephyr 拥有最高的总体平均性能（0.731）。Mistral-7b、Mistral-7b-instruct
    和 Zephyr-7b-beta（本身基于 Mistral-7b-instruct [[38](#bib.bib38)]）在 LoRA 微调表现中领先，超过了
    Llama、Phi 和 Gemma 系列。
- en: '![Refer to caption](img/aa21750c0a8662bcdf93b1ff42a29d3b.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa21750c0a8662bcdf93b1ff42a29d3b.png)'
- en: 'Figure 5: Frequency of base models (with fine-tuning) as the top performer
    for a task. Ties, namely for the customer_support task where most models attain
    100% perfect scores, are excluded.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基础模型（经过微调）在任务中的顶级表现频率。排除了例如客户支持任务等大多数模型都能达到100%满分的平局情况。
- en: 5.2 Does size matter for LoRA fine-tuning? 2B vs. 7B
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LoRA 微调的大小是否重要？2B vs. 7B
- en: The 2B parameter Phi-2 model, after fine-tuning, outperforms all of the 2B and
    7B Gemma models by overall overage, and is only 1.9 points behind the next highest
    performing 7B model, Llama-2-7b (0.677 vs. 0.696). Despite this, we find that
    fine-tuned 7B models are almost always better than fine-tuned 2B models (29/31
    tasks). Among 2B parameter models in particular (Phi and Gemma), we see that all
    Gemma instruct models were better than Phi out of the box, however, Phi-2 performs
    better than all other Gemma models after fine-tuning.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 经过微调的 2B 参数 Phi-2 模型在总体平均表现上超过了所有 2B 和 7B Gemma 模型，仅落后于下一个表现最好的 7B 模型 Llama-2-7b（0.677
    vs. 0.696）。尽管如此，我们发现经过微调的 7B 模型几乎总是优于经过微调的 2B 模型（29/31 任务）。特别是在 2B 参数模型（Phi 和
    Gemma）中，我们看到所有 Gemma instruct 模型在开箱即用时都优于 Phi，然而 Phi-2 在微调后表现优于所有其他 Gemma 模型。
- en: 5.3 Is fine-tuning better with Instruction-tuned or Auto-complete models?
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 微调时使用指令调优还是自动完成模型更好？
- en: 'In Figure [6](#S5.F6 "Figure 6 ‣ 5.3 Is fine-tuning better with Instruction-tuned
    or Auto-complete models? ‣ 5 Discussion and Analysis ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report"), we observe that before fine-tuning,
    instruction-tuned models outperform auto-complete models, despite using completion
    style prompts. A qualitative analysis shows that auto-complete models were much
    more likely to "go off the rails", and generate long irrelevant text sequences,
    and instruction-tuned models demonstrate a higher consistency in correctly attempting
    the imminent task.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[6](#S5.F6 "图6 ‣ 5.3 微调是使用指令微调还是自动完成模型更好？ ‣ 5 讨论与分析 ‣ LoRA领域：310个微调LLMs对抗GPT-4，技术报告")中，我们观察到，在微调前，指令微调模型优于自动完成模型，尽管使用的是完成样式的提示。定性分析显示，自动完成模型更容易“跑偏”，生成长的无关文本序列，而指令微调模型在正确尝试即将到来的任务时表现出更高的一致性。
- en: After fine-tuning, the performance disparities between the models narrow. The
    average instruction-tuned model slightly outperforms the average auto-complete
    model by a margin of +0.009, however the reverse is true when comparing the best
    fine-tuned instruction-tuned model and the best fine-tuned auto-complete model
    (-0.002). Auto-complete models, possibly due to their broader and less specialized
    knowledge base, may be inherently more adaptable to a variety of tasks. However,
    with adequate fine-tuning, both types of models achieve comparable performance
    levels. We encourage further research to explore how the foundational design of
    instruction-tuned models influences their adaptability and effectiveness in task-specific
    fine-tuning.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后，模型之间的性能差距变小。平均指令微调模型略微优于平均自动完成模型，差距为+0.009，而在比较最佳微调指令模型和最佳微调自动完成模型时则相反（-0.002）。自动完成模型可能由于其更广泛且不那么专门化的知识基础，可能天生更能适应各种任务。然而，经过充分微调，两种类型的模型都能达到相当的性能水平。我们鼓励进一步研究以探索指令微调模型的基础设计如何影响其在任务特定微调中的适应性和有效性。
- en: '![Refer to caption](img/cbbd30f2bb36fecf05450b7f643dc7ec.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cbbd30f2bb36fecf05450b7f643dc7ec.png)'
- en: 'Figure 6: Comparison of auto-complete vs. instruction-tuned base models, before
    and after fine-tuning.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：自动完成与指令微调基础模型的比较，微调前后的对比。
- en: 5.4 When does GPT-4 consistently outperform fine-tuned models?
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 GPT-4何时始终优于微调模型？
- en: We observe a distinct advantage for fine-tuned LLMs on narrowly-scoped tasks,
    such as those within the GLUE benchmarks. These tasks, primarily classification-oriented,
    saw fine-tuned LLMs achieve near 90% accuracy, outperforming GPT-4\. GPT-4 continues
    to outperform fine-tuned models in 6 out of 31 tasks, particularly in broader,
    more complex domains such as Python coding and MMLU.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到在狭窄范围任务上，经过微调的LLMs有明显的优势，比如在GLUE基准测试中的任务。这些任务主要是分类导向的，经过微调的LLMs达到了接近90%的准确率，超过了GPT-4。GPT-4在31个任务中的6个任务上继续优于微调模型，特别是在更广泛、更复杂的领域，如Python编程和MMLU。
- en: 5.5 Quantifying the relationship between fine-tuning quality lift and task complexity
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 量化微调质量提升与任务复杂性之间的关系
- en: If fine-tuned models perform better on specialized "narrow" tasks and worse
    on "broader" tasks, can we establish a predictive relationship between the complexity
    of a task and the efficacy of LoRA fine-tuning? Identifying such a relationship
    could provide a valuable predictive tool for assessing the potential benefits
    of fine-tuning enhancements on new tasks before the fine-tuning process begins.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果微调模型在专门的“狭窄”任务上表现更好，而在“广泛”任务上表现更差，我们能否建立任务复杂性与LoRA微调有效性之间的预测关系？识别这样的关系可能提供一个有价值的预测工具，用于评估在微调过程开始之前微调增强对新任务的潜在好处。
- en: 5.5.1 Heuristics for fine-tuning quality, quality lift, and task complexity
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 微调质量、质量提升和任务复杂性的启发式
- en: 'To quantify task complexity, we use several heuristics:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化任务复杂性，我们使用了几种启发式方法：
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Number of training examples
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练示例数量
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lengths of inputs and outputs ($\mu$, and 95th percentile).
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入和输出的长度（$\mu$和95百分位数）。
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compressibility^(15)^(15)15https://docs.python.org/3/library/gzip.html ($\mu$)
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 压缩性^(15)^(15)15https://docs.python.org/3/library/gzip.html ($\mu$)
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Diversity of content, which we approximate by measuring the rouge-L similarity
    between inputs and outputs) [[41](#bib.bib41)] ($\mu$).
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容多样性，我们通过测量输入和输出之间的rouge-L相似性来近似（$\mu$）。
- en: For task complexity heuristic,
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任务复杂性启发式，
- en: 'For model quality measurements, we track:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型质量测量，我们跟踪：
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Baseline GPT-4 score
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基线GPT-4得分
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lift from the best fine-tuned model vs. GPT-4 ("Max GPT-4 Lift")
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从最佳微调模型到 GPT-4 的提升（"最大 GPT-4 提升"）
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Average fine-tuning lift over the base model
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均微调提升（相对于基础模型）
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Best base model score without fine-tuning
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未微调的最佳基础模型得分
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Average base model score without fine-tuning
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均基础模型得分（未微调）
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Best fine-tuned model score
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳微调模型得分
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Average fine-tuned model score
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均微调模型得分
- en: 'Refer to Table [6](#S5.T6 "Table 6 ‣ 5.5.1 Heuristics for fine-tuning quality,
    quality lift, and task complexity ‣ 5.5 Quantifying the relationship between fine-tuning
    quality lift and task complexity ‣ 5 Discussion and Analysis ‣ LoRA Land: 310
    Fine-tuned LLMs that Rival GPT-4, A Technical Report") for a complete example.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '请参阅表格 [6](#S5.T6 "Table 6 ‣ 5.5.1 Heuristics for fine-tuning quality, quality
    lift, and task complexity ‣ 5.5 Quantifying the relationship between fine-tuning
    quality lift and task complexity ‣ 5 Discussion and Analysis ‣ LoRA Land: 310
    Fine-tuned LLMs that Rival GPT-4, A Technical Report") 获取完整示例。'
- en: '|  | Metric | arc_combined | bc5cdr | boolq |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | 指标 | arc_combined | bc5cdr | boolq |'
- en: '| Model quality measurements | Max GPT-4 Lift | -0.03 | 0.08 | 0.00 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 模型质量测量 | 最大 GPT-4 提升 | -0.03 | 0.08 | 0.00 |'
- en: '| Average Base Model Lift | 0.32 | 0.75 | 0.19 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 平均基础模型提升 | 0.32 | 0.75 | 0.19 |'
- en: '| Best Base Model Score | 0.67 | 0.70 | 0.76 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 最佳基础模型得分 | 0.67 | 0.70 | 0.76 |'
- en: '| Average Base Model Score | 0.41 | 0.22 | 0.64 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 平均基础模型得分 | 0.41 | 0.22 | 0.64 |'
- en: '| Best Fine-tuned Score | 0.92 | 0.97 | 0.91 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 最佳微调得分 | 0.92 | 0.97 | 0.91 |'
- en: '| Average Fine-Tuned Score | 0.73 | 0.97 | 0.82 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 平均微调得分 | 0.73 | 0.97 | 0.82 |'
- en: '| Task complexity heuristics | Input length p95 | 143.00 | 175.00 | 270.70
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 任务复杂性启发式 | 输入长度 p95 | 143.00 | 175.00 | 270.70 |'
- en: '| Input length $\mu$ | 102.89 | 142.15 | 145.23 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 输入长度 $\mu$ | 102.89 | 142.15 | 145.23 |'
- en: '| Input length $\sigma$ | 21.68 | 19.17 | 69.03 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 输入长度 $\sigma$ | 21.68 | 19.17 | 69.03 |'
- en: '| Output length p95 | 1.00 | 58.00 | 1.00 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 输出长度 p95 | 1.00 | 58.00 | 1.00 |'
- en: '| Output length $\mu$ | 1.00 | 37.11 | 1.00 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 输出长度 $\mu$ | 1.00 | 37.11 | 1.00 |'
- en: '| Output length $\sigma$ | 0.00 | 11.27 | 0.00 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 输出长度 $\sigma$ | 0.00 | 11.27 | 0.00 |'
- en: '| Example length $\mu$ | 102.92 | 178.26 | 146.23 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 示例长度 $\mu$ | 102.92 | 178.26 | 146.23 |'
- en: '| Example length p95 | 143.00 | 226.05 | 271.70 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 示例长度 p95 | 143.00 | 226.05 | 271.70 |'
- en: '| Example length $\sigma$ | 21.66 | 27.84 | 69.03 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 示例长度 $\sigma$ | 21.66 | 27.84 | 69.03 |'
- en: '| I/O rougeL similarity $\mu$ | 0.03 | 0.19 | 0.00 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| I/O rougeL 相似度 $\mu$ | 0.03 | 0.19 | 0.00 |'
- en: '| I/O rougeL similarity $\sigma$ | 0.01 | 0.03 | 0.00 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| I/O rougeL 相似度 $\sigma$ | 0.01 | 0.03 | 0.00 |'
- en: '| Compressibility $\mu$ | 0.64 | 0.55 | 0.60 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 压缩性 $\mu$ | 0.64 | 0.55 | 0.60 |'
- en: '| Compressibility $\sigma$ | 0.06 | 0.01 | 0.07 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 压缩性 $\sigma$ | 0.06 | 0.01 | 0.07 |'
- en: '| # training examples | 3370 | 5228 | 9427 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 训练样本数量 | 3370 | 5228 | 9427 |'
- en: 'Table 6: Model quality measurements and task complexity heuristics for 3 different
    tasks (example). Refer to the Appendix [C](#A3 "Appendix C Full Results Tables
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report"). for all
    measurements and heuristics for all 31 tasks.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 3 个不同任务的模型质量测量和任务复杂性启发式（示例）。有关所有 31 个任务的所有测量和启发式，请参见附录 [C](#A3 "Appendix
    C Full Results Tables ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical
    Report")。'
- en: 5.5.2 Correlating fine-tuning quality and quality lift with task complexity
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 微调质量和质量提升与任务复杂性的相关性
- en: 'We find several intriguing correlations suggesting significant interactions
    between our task complexity heuristics and measurements of model performance.
    Key observations include:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现几个有趣的相关性，表明任务复杂性启发式和模型性能测量之间存在显著交互。主要观察包括：
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compressibility exhibited a dual influence, correlating positively with both
    best and average base model scores (0.36), while correlating negatively with these
    scores when the variance in compressibility increased (-0.37). This indicates
    that while uniform compressibility supports model performance, higher variability
    in compressibility tends to degrade it.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 压缩性表现出双重影响，与最佳和平均基础模型得分呈正相关（0.36），而在压缩性方差增加时与这些得分呈负相关（-0.37）。这表明，虽然均匀的压缩性支持模型性能，但压缩性的较高变异性往往会降低模型性能。
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input and Output Lengths: Longer and more varied output lengths correlated
    positively with the maximum lift from GPT-4 fine-tuning, suggesting that tasks
    with extended and more varied outputs are not detrimental for fine-tuning. Conversely,
    longer and more varied input and output lengths negatively correlate with absolute
    base and fine-tuned model scores.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入和输出长度：较长和更具变异性的输出长度与 GPT-4 微调的最大提升呈正相关，表明输出较长且多样的任务对微调没有不利影响。相反，较长且多样的输入和输出长度与基础和微调模型的绝对得分呈负相关。
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input and Output Rouge-L Similarity: A higher standard deviation in input/output
    Rouge-L similarity correlates negatively with both base and fine-tuned model scores.
    This suggests that greater variability in content similarity within a dataset
    may pose difficulties for model learning.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入和输出 Rouge-L 相似度：输入/输出 Rouge-L 相似度的标准差较高与基础模型和微调模型的得分均呈负相关。这表明数据集中内容相似度的较大变异性可能对模型学习造成困难。
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of training examples: No significant correlation was found with the
    number of training examples, pointing to the possibility that once a sufficient
    sample size is achieved, additional examples do not necessarily contribute to
    improved fine-tuning efficacy.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练样本数量：未发现与训练样本数量的显著相关性，这表明一旦达到足够的样本量，额外的样本不一定能提高微调的效果。
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model quality inter-correlations reveal that better average scores (both base
    and fine-tuned) strongly predict the best scores obtained, suggesting a general
    consistency in model performance across different training instances.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型质量的互相关性揭示了更好的平均得分（包括基础和微调模型）强烈预测最佳得分，这表明在不同训练实例中的模型性能具有一般的一致性。
- en: Overall, these observations are consistent with our hypothesis that narrower
    easier tasks are more likely to see success with fine-tuned adapters.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些观察结果与我们的假设一致，即较窄且较简单的任务更容易通过微调适配器取得成功。
- en: '![Refer to caption](img/3a6b629cdafbfcdf435df0696bd46add.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3a6b629cdafbfcdf435df0696bd46add.png)'
- en: 'Figure 7: Correlations between dataset complexity and model quality correlations
    for 310 LLMs across 31 tasks, before and after LoRA-based fine-tuning.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：310 个 LLM 在 31 个任务中数据集复杂度与模型质量的相关性，LoRA 基于微调前后的比较。
- en: 5.5.3 Predicting fine-tuning quality and quality lift given task complexity
    heuristics
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3 预测微调质量和任务复杂度启发式方法下的质量提升
- en: 'We train linear regression models to predict the quality lift achievable through
    adapter-based fine-tuning, using z-score normalized dataset complexity heuristics
    (described in Table [6](#S5.T6 "Table 6 ‣ 5.5.1 Heuristics for fine-tuning quality,
    quality lift, and task complexity ‣ 5.5 Quantifying the relationship between fine-tuning
    quality lift and task complexity ‣ 5 Discussion and Analysis ‣ LoRA Land: 310
    Fine-tuned LLMs that Rival GPT-4, A Technical Report")) as predictors. Results
    are summarized in Table [7](#S5.T7 "Table 7 ‣ 5.5.3 Predicting fine-tuning quality
    and quality lift given task complexity heuristics ‣ 5.5 Quantifying the relationship
    between fine-tuning quality lift and task complexity ‣ 5 Discussion and Analysis
    ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report"), where
    we find that linear models yield root mean squared errors (RMSE) of 0.166 to 0.092,
    depending on the model quality metric in question.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '我们训练线性回归模型以预测通过基于适配器的微调所能实现的质量提升，使用 z-score 标准化的数据集复杂度启发式方法（在表 [6](#S5.T6 "Table
    6 ‣ 5.5.1 Heuristics for fine-tuning quality, quality lift, and task complexity
    ‣ 5.5 Quantifying the relationship between fine-tuning quality lift and task complexity
    ‣ 5 Discussion and Analysis ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4,
    A Technical Report") 中描述）作为预测因子。结果总结在表 [7](#S5.T7 "Table 7 ‣ 5.5.3 Predicting
    fine-tuning quality and quality lift given task complexity heuristics ‣ 5.5 Quantifying
    the relationship between fine-tuning quality lift and task complexity ‣ 5 Discussion
    and Analysis ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")
    中，我们发现线性模型的均方根误差（RMSE）在 0.166 到 0.092 之间，具体取决于模型质量指标。'
- en: Incorporating the score of the average base model without fine tuning as an
    additional feature improves prediction accuracy for all model quality metrics
    (+0.004 to +0.069). This demonstrates some predictive power in knowing base model
    performance for anticipating potential gains from fine-tuning. RMSE errors are
    rather low, suggesting that upfront heuristics-based measurements of dataset complexity
    can be reasonable indicators of positive fine-tuning impact.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 将未经过微调的平均基础模型得分作为额外特征纳入进来，可以提高所有模型质量指标的预测准确性（+0.004 到 +0.069）。这表明了解基础模型的性能对预测微调可能带来的潜在收益具有一定的预测能力。RMSE
    错误较低，表明基于启发式的前期数据集复杂度测量可以作为积极微调影响的合理指示器。
- en: '| Model Quality Metric |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 模型质量指标 |'
- en: '&#124; With average base model score &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于平均基础模型得分 &#124;'
- en: '&#124; as a feature &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作为一个特征 &#124;'
- en: '&#124; (RMSE) &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (RMSE) &#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; With average base model score &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于平均基础模型得分 &#124;'
- en: '&#124; as a feature &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作为一个特征 &#124;'
- en: '&#124; (RMSE) &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (RMSE) &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| GPT-4 Score | 0.140 | 0.121 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 得分 | 0.140 | 0.121 |'
- en: '| Max GPT-4 Lift | 0.092 | 0.085 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 最大 GPT-4 提升 | 0.092 | 0.085 |'
- en: '| Average Base Model Score | 0.099 | N/A (0.000) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 平均基础模型得分 | 0.099 | 不适用 (0.000) |'
- en: '| Best Base Model Score | 0.166 | 0.097 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 最佳基础模型得分 | 0.166 | 0.097 |'
- en: '| Average Base Model Lift | 0.099 | 0.095 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 平均基础模型提升 | 0.099 | 0.095 |'
- en: '| Average Fine-Tuned Score | 0.119 | 0.095 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 平均微调得分 | 0.119 | 0.095 |'
- en: '| Best Fine-tuned Score | 0.097 | 0.091 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 最佳微调得分 | 0.097 | 0.091 |'
- en: 'Table 7: The performance of linear regression models predicting model quality
    heuristics before and after fine-tuning, given z-score normalized dataset complexity
    heuristics, with and without a representative base model score.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：线性回归模型在微调前后预测模型质量启发式的性能，给定 z-score 归一化数据集复杂度启发式，有和没有代表性基础模型得分。
- en: 6 Performance Benchmarks of LoRAX Deployments
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 LoRAX 部署的性能基准测试
- en: To assess the viability of serving many LoRA fine-tuned LLMs simultaneously
    in a real-world application, we launch LoRA Land. LoRA Land is a web application
    that serves 25 fine-tuned Mistral-7b LLMs served to thousands of users from a
    single A100 GPU.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估在实际应用中同时服务多个 LoRA 微调 LLM 的可行性，我们推出了 LoRA Land。LoRA Land 是一个 web 应用程序，从单个
    A100 GPU 向数千名用户提供 25 个微调的 Mistral-7b LLM。
- en: '![Refer to caption](img/1d93b1c5d6a4640af3f7eb9cdb13997b.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d93b1c5d6a4640af3f7eb9cdb13997b.png)'
- en: 'Figure 8: The LoRA Land web application that serves 25 fine-tuned LLMs on a
    single A100\. The application is available at [https://predibase.com/lora-land](https://predibase.com/lora-land).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：LoRA Land web 应用程序在单个 A100 上提供 25 个微调的 LLM。该应用程序可以在 [https://predibase.com/lora-land](https://predibase.com/lora-land)
    上访问。
- en: 6.1 LoRAX in a Nutshell
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 LoRAX 简介
- en: 'LoRA Exchange (LoRAX) [[1](#bib.bib1)] is an open source Multi-LoRA inference
    server specifically designed for serving many fine-tuned models at once using
    a shared set of GPU resources. Compared with conventional dedicated LLM deployments,
    LoRAX consists of three novel components:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA Exchange (LoRAX) [[1](#bib.bib1)] 是一个开源的 Multi-LoRA 推理服务器，专门设计用于利用共享的 GPU
    资源一次性服务多个微调模型。与传统的专用 LLM 部署相比，LoRAX 包含三个新颖的组件：
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dynamic Adapter Loading, allowing each set of fine-tuned LoRA weights to be
    loaded from storage just-in-time as requests come in at runtime, without blocking
    concurrent requests.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态适配器加载，允许每组微调的 LoRA 权重在请求到达时即时从存储中加载，而不会阻塞并发请求。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Continuous Multi-Adapter Batching, a fair scheduling policy for optimizing aggregate
    throughput of the system that extends the popular continuous batching strategy
    to work across multiple sets of LoRA adapters in parallel.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连续多适配器批处理，一种公平的调度策略，用于优化系统的总吞吐量，该策略将流行的连续批处理策略扩展到多个 LoRA 适配器集并行工作。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tiered Weight Caching, to support fast exchanging of LoRA adapters between requests,
    and offloading of adapter weights to CPU and disk to avoid out-of-memory errors.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层权重缓存，以支持在请求之间快速交换 LoRA 适配器，并将适配器权重卸载到 CPU 和磁盘，以避免内存不足错误。
- en: '![Refer to caption](img/0482b267a9641e429dae10b28c0628d4.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0482b267a9641e429dae10b28c0628d4.png)'
- en: 'Figure 9: Dynamic adapter loading (left) enables multiple concurrent fine-tuned
    models to process requests. User 3’s model (green) is loaded in the background
    while the other requests proceed as usual. Continuous Multi-Adapter Batching (right):
    Multiple adapters are decoded in a single batch. Masks ensure that only the right
    adapter is used for processing each element of the batch.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：动态适配器加载（左）使多个并发微调模型能够处理请求。用户 3 的模型（绿色）在后台加载，而其他请求按常规进行。连续多适配器批处理（右）：多个适配器在一个批次中解码。掩码确保每个批次的每个元素仅使用正确的适配器进行处理。
- en: 6.2 Benchmarking Results
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 基准测试结果
- en: 'We run benchmarks in order to understand the impact of serving multiple adapters
    on the relevant metrics, described below. We also test the scalability of the
    system with respect to the following factors:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行基准测试以了解同时服务多个适配器对相关指标的影响，如下所述。我们还测试了系统在以下因素方面的可扩展性：
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Number of concurrent users submitting LLM prompts
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提交 LLM 提示的并发用户数量
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Number of adapters concurrently being queried
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并发查询的适配器数量
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Number of input tokens
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入 token 数量
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Number of output tokens
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出 token 数量
- en: 'LLM serving performance metrics include: time to first token (TFTT), total
    request time, token streaming time, and throughput (tokens per second). We run
    our benchmarks from a t3.2xlarge EC2 instance in the AWS zone us-west-2\. All
    benchmarks are based on the Mistral-7b-instruct LLM, deployed on an A100 GPU with
    80GB of RAM. The script used to benchmark LLM serving performance can be found
    in Appendix [B](#A2 "Appendix B LoRAX benchmarking scripts ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report").'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 服务性能指标包括：第一个标记的时间（TFTT）、总请求时间、标记流时间和吞吐量（每秒标记数）。我们从 AWS 区域 us-west-2 的 t3.2xlarge
    EC2 实例上运行基准测试。所有基准测试都基于 Mistral-7b-instruct LLM，部署在具有 80GB RAM 的 A100 GPU 上。用于基准测试
    LLM 服务性能的脚本可以在附录 [B](#A2 "附录 B LoRAX 基准测试脚本 ‣ LoRA Land: 310 个调整过的 LLM 对标 GPT-4
    的技术报告") 中找到。'
- en: 'The following is a summary of relevant terminology:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相关术语的总结：
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total request time (ms): total time from when the request is sent to when the
    last token is streamed back to the client.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总请求时间（毫秒）：从发送请求到最后一个标记返回客户端的总时间。
- en: •
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Time to first token, TTFT (ms): time from when the request is sent to the first
    token is received by the client'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个标记的时间，TTFT（毫秒）：从发送请求到客户端收到第一个标记的时间
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Token streaming time (ms): time from when the first token is received by the
    client to when the last token is received by the client.'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记流时间（毫秒）：从客户端收到第一个标记到收到最后一个标记的时间。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Throughput (token/s): number of tokens generated per seconds, computed by (Token
    streaming time (ms) / number of output tokens)'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 吞吐量（标记/秒）：每秒生成的标记数量，通过（标记流时间（毫秒）/ 输出标记数量）计算得出
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Concurrent users: number of users that make requests to the LLM, wait until
    they receive a full response, then make another request until the end of testing
    time.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并发用户：发起请求的用户数量，等待直到收到完整的响应，然后再发起另一个请求，直到测试结束。
- en: 6.3 Latency from adapter switching and concurrent users
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 适配器切换和并发用户的延迟
- en: The following reported benchmarks come from 2-minute runs that continuously
    stream requests to the LLM deployment. Our experiments indicate that a duration
    of two minutes provides an adequate volume of data to obtain stable and reliable
    metrics.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下报告的基准来自连续流请求到 LLM 部署的 2 分钟运行。我们的实验表明，两分钟的持续时间提供了足够的数据量，以获得稳定可靠的指标。
- en: 'Table [8](#S6.T8 "Table 8 ‣ 6.3 Latency from adapter switching and concurrent
    users ‣ 6 Performance Benchmarks of LoRAX Deployments ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report") shows the impact LLM query performance
    isolated to adapter switching mechanics. In the multi-adapter, multi-user case,
    we see that the token streaming time is the same, but the total request time differs
    by 7.21ms which illustrates the cost of handling requests from 100 concurrent
    users that lead to switching between 25 adapters.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#S6.T8 "表 8 ‣ 6.3 适配器切换和并发用户的延迟 ‣ 6 LoRAX 部署性能基准 ‣ LoRA Land: 310 个调整过的
    LLM 对标 GPT-4 的技术报告") 显示了 LLM 查询性能仅受适配器切换机制的影响。在多适配器、多用户的情况下，我们看到标记流时间相同，但总请求时间相差
    7.21 毫秒，这表明处理来自 100 个并发用户的请求而导致在 25 个适配器之间切换的成本。'
- en: '|  | 0 adapters (base model), 1 concurrent user | 25 adapters (base model),
    100 concurrent user |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 个适配器（基础模型），1 个并发用户 | 25 个适配器（基础模型），100 个并发用户 |'
- en: '|  | Average | p90 | Average | p90 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | p90 | 平均值 | p90 |'
- en: '| Total request time (ms) | 191.81 | 192.3 | 199.02 | 201.82 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 总请求时间（毫秒） | 191.81 | 192.3 | 199.02 | 201.82 |'
- en: '| Time to first token, TTFT (ms) | 122.19 | 191.16 | 128.79 | 199.11 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 第一个标记的时间，TTFT（毫秒） | 122.19 | 191.16 | 128.79 | 199.11 |'
- en: '| Token streaming time (ms) | 70 | 92.38 | 70.14 | 96.62 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 标记流时间（毫秒） | 70 | 92.38 | 70.14 | 96.62 |'
- en: 'Table 8: Measuring LLM querying metrics from adapter switching mechanics only.
    To eliminate extra, non-adapter-switching factors related to input and generation,
    simulated requests contain 1 input token and max_new_tokens is capped at 1\. Throughput
    metrics are excluded since only 1 output token is generated.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：仅测量适配器切换机制中的 LLM 查询指标。为了消除与输入和生成相关的额外非适配器切换因素，模拟请求包含 1 个输入标记，max_new_tokens
    被限制为 1。吞吐量指标被排除，因为只生成了 1 个输出标记。
- en: '| # concurrent users |  | 1 | 5 | 10 | 20 | 50 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| # 并发用户 |  | 1 | 5 | 10 | 20 | 50 |'
- en: '| Total request time (ms) | average | 943.03 | 1165.71 | 1359.39 | 2004.9 |
    2981.66 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 总请求时间（毫秒） | 平均值 | 943.03 | 1165.71 | 1359.39 | 2004.9 | 2981.66 |'
- en: '| p90 | 1567.66 | 1925.96 | 2147.84 | 3287.21 | 4673.52 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 1567.66 | 1925.96 | 2147.84 | 3287.21 | 4673.52 |'
- en: '| Time to first token, TTFT (ms) | average | 121.84 | 121.80 | 143.68 | 135.43
    | 136.17 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Time to first token, TTFT (ms) | average | 121.84 | 121.80 | 143.68 | 135.43
    | 136.17 |'
- en: '| p90 | 191.08 | 195.85 | 199.98 | 199.76 | 199.54 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 191.08 | 195.85 | 199.98 | 199.76 | 199.54 |'
- en: '| Token streaming time (ms) | average | 821.09 | 1043.79 | 1215.6 | 1869.36
    | 2845.38 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Token streaming time (ms) | average | 821.09 | 1043.79 | 1215.6 | 1869.36
    | 2845.38 |'
- en: '| p90 | 1468.76 | 1804.16 | 2007.89 | 3130.72 | 4544.64 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 1468.76 | 1804.16 | 2007.89 | 3130.72 | 4544.64 |'
- en: 'Table 9: Benchmarking base LLM deployments on 1xA100 with queries that simulate
    real load.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 表格9：在1xA100上基于真实负载的基线LLM部署基准测试。
- en: 'To simulate realistic traffic payloads, we generate random payloads with 30-500
    input tokens and 1-120 output tokens, modeled off of the tasks defined in Table
    [1](#S3.T1 "Table 1 ‣ 3.1 Task selection ‣ 3 Methodology ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report"). We vary the number of concurrent
    users from 1 to 50, and payloads are issued randomly between 25 different adapter
    endpoints.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '为了模拟真实的流量负载，我们生成了30-500个输入标记和1-120个输出标记的随机负载，基于表格[1](#S3.T1 "Table 1 ‣ 3.1
    Task selection ‣ 3 Methodology ‣ LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4,
    A Technical Report")中定义的任务进行建模。我们将并发用户数量从1到50进行变化，负载在25个不同的适配器端点之间随机发放。'
- en: When scaling from 1 to 50 concurrent users, which also increases load by 50X,
    the average time to first token (TTFT) is slightly affected (+21.84ms or 17.9%
    increase). We see a 3.46X decrease in throughput for the same 50X increase in
    load.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 当并发用户从1增加到50时，负载也增加了50倍，首次标记时间（TTFT）略有影响（+21.84ms或17.9%增加）。我们观察到相同50倍负载增加导致吞吐量减少了3.46倍。
- en: '| # concurrent users |  | 1 | 5 | 10 | 20 | 50 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| # concurrent users |  | 1 | 5 | 10 | 20 | 50 |'
- en: '| Total request time (ms) | average | 956.56 | 1272.16 | 1528.99 | 1896.1 |
    3336.27 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Total request time (ms) | average | 956.56 | 1272.16 | 1528.99 | 1896.1 |
    3336.27 |'
- en: '| p90 | 1758.53 | 2164.08 | 2612.05 | 3222.73 | 5330.84 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 1758.53 | 2164.08 | 2612.05 | 3222.73 | 5330.84 |'
- en: '| Time to first token, TTFT (ms) | average | 170.62 | 148.14 | 157.49 | 167.28
    | 153.89 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Time to first token, TTFT (ms) | average | 170.62 | 148.14 | 157.49 | 167.28
    | 153.89 |'
- en: '| p90 | 199.36 | 198.98 | 199.41 | 200.99 | 200.2 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 199.36 | 198.98 | 199.41 | 200.99 | 200.2 |'
- en: '| Token streaming time (ms) | average | 785.82 | 1123.91 | 1371.39 | 1728.71
    | 3182.27 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Token streaming time (ms) | average | 785.82 | 1123.91 | 1371.39 | 1728.71
    | 3182.27 |'
- en: '| p90 | 1594.65 | 2023.33 | 2468.87 | 3047.92 | 5169.05 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 1594.65 | 2023.33 | 2468.87 | 3047.92 | 5169.05 |'
- en: 'Table 10: Benchmarking 25 adapters on 1xA100 with queries that simulate real
    load.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 表格10：在1xA100上基于真实负载的25个适配器的基准测试。
- en: 'Table [10](#S6.T10 "Table 10 ‣ 6.3 Latency from adapter switching and concurrent
    users ‣ 6 Performance Benchmarks of LoRAX Deployments ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report") shows that there’s no significant
    difference between querying the base LLM vs. the 25 adapters when it comes to
    TTFT or throughput. The cost of adapter switching is overshadowed by the time
    it takes to generate tokens once requests come in. Comparing average case numbers
    vs. p90 numbers for TTFT, the largest disparity is between 121.8ms (average) and
    195.95ms (p90) for a 60.87% increase. Additionally, we consistently see that TTFT
    is at or under the 200ms mark.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[10](#S6.T10 "Table 10 ‣ 6.3 Latency from adapter switching and concurrent
    users ‣ 6 Performance Benchmarks of LoRAX Deployments ‣ LoRA Land: 310 Fine-tuned
    LLMs that Rival GPT-4, A Technical Report")显示，在TTFT或吞吐量方面，查询基线LLM与25个适配器之间没有显著差异。适配器切换的成本被请求到达后生成标记所需的时间所掩盖。对于TTFT，比较平均值与p90值，最大差异为121.8ms（平均）和195.95ms（p90），增长了60.87%。此外，我们始终观察到TTFT在200ms以内或更短。'
- en: On throughput, we observe that it takes between 12 and 13.5ms to generate a
    single token on an A100 GPU both for base deployments and deployments where adapter
    weights have been added. This means that the aggregate throughput for the LLM
    deployment on that GPU is between 74 tokens/s and 83 tokens/s.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在吞吐量方面，我们观察到，无论是基线部署还是添加了适配器权重的部署，在A100 GPU上生成一个标记的时间都在12到13.5ms之间。这意味着在该GPU上的LLM部署的总吞吐量在74
    tokens/s到83 tokens/s之间。
- en: 6.4 Analyzing the performance impact of additional deployment replicas
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 分析额外部署副本的性能影响
- en: 'In Table [11](#S6.T11 "Table 11 ‣ 6.4 Analyzing the performance impact of additional
    deployment replicas ‣ 6 Performance Benchmarks of LoRAX Deployments ‣ LoRA Land:
    310 Fine-tuned LLMs that Rival GPT-4, A Technical Report"), we run benchmarks
    for 25 adapters queried concurrently by 50 users, with a LoRAX deployment on 1
    replica. We then run benchmarks where we scale the LoRAX deployment to 2 replicas
    placed behind a round robin load balancer to route equal amounts of traffic to
    each replica, while also scaling the load to 100 concurrent users. We see that
    the numbers are stable across the board, signifying that replicas can be scaled
    linearly with load to achieve comparable metrics.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[11](#S6.T11 "Table 11 ‣ 6.4 Analyzing the performance impact of additional
    deployment replicas ‣ 6 Performance Benchmarks of LoRAX Deployments ‣ LoRA Land:
    310 Fine-tuned LLMs that Rival GPT-4, A Technical Report")中，我们对25个适配器在50个用户同时查询下进行了基准测试，使用1个副本的LoRAX部署。然后，我们对将LoRAX部署扩展到2个副本的情况进行了基准测试，这些副本通过轮询负载均衡器进行流量分配，同时将负载扩展到100个并发用户。我们发现各项数据稳定，表明副本可以随着负载线性扩展以实现可比的指标。'
- en: '|  |  | 50 Concurrent users, 1 replica | 100 Concurrent users, 2 replicas |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 50并发用户，1个副本 | 100并发用户，2个副本 |'
- en: '| Total request time (ms) | average | 3336.27 | 3368.53 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 总请求时间 (ms) | 平均值 | 3336.27 | 3368.53 |'
- en: '| p90 | 5330.84 | 5382.61 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 5330.84 | 5382.61 |'
- en: '| Time to first token, TTFT (ms) | average | 153.89 | 161.97 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 到第一个Token的时间 (TTFT) (ms) | 平均值 | 153.89 | 161.97 |'
- en: '| p90 | 200.2 | 199.83 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 200.2 | 199.83 |'
- en: '| Token streaming time (ms) | average | 3182.27 | 3206.46 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Token流时间 (ms) | 平均值 | 3182.27 | 3206.46 |'
- en: '| p90 | 5169.05 | 5248.97 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| p90 | 5169.05 | 5248.97 |'
- en: 'Table 11: Benchmarking 25 adapters on 1 LoRAX replica vs. 2 replicas with queries
    that simulate real load.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：在1个LoRAX副本与2个副本上基准测试25个适配器，查询模拟实际负载。
- en: 7 Limitations
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: 'Our experimental design has many limitations, including:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验设计存在许多限制，包括：
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Restricted Evaluation Scope: Our evaluations are limited to the first 1000
    examples of datasets with larger evaluation splits to manage costs while maintaining
    rigor. This may introduce selection bias and limit the generalizability of our
    findings. Future research should consider more comprehensive evaluations as resources
    allow.'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 限制评估范围：我们的评估仅限于前1000个示例的数据集，评估分割较大的数据集，以管理成本同时保持严格性。这可能引入选择偏差并限制我们发现的普遍性。未来的研究应考虑在资源允许的情况下进行更全面的评估。
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Engineering Constraints: Our study does not employ advanced prompt engineering
    techniques such as majority voting, n-shot prompting, or specialized tuning methods
    like MedPrompt or chain-of-thought prompting. In this study, we prioritize reproducibility
    and minimize biases from selective example choice by using simple zero or single-shot
    prompts across all tasks, however these techniques have shown potential in enhancing
    task-specific performance.'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程约束：我们的研究没有使用先进的提示工程技术，如多数投票、n-shot提示或专门的调优方法，如MedPrompt或链式思维提示。在本研究中，我们优先考虑可重复性，并通过在所有任务中使用简单的零或单次提示来最小化选择性示例选择的偏差，尽管这些技术在提高任务特定性能方面表现出潜力。
- en: •
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training Constraints: All LLMs are fine-tuned with the same Models are trained
    with consistent parameters: 40K examples, batch size of 1, 4-bit quantization,
    and a LoRA rank of 8, using an adam optimizer and a cosine learning rate scheduler
    with specific settings. Training is conducted on a single A10 GPU, using gradient
    checkpointing to manage memory limitations. For datasets where full sequence lengths
    induce memory overflow, we truncate sequences to the 95th percentile length. This
    approach may impact the thoroughness of model training, particularly on datasets
    where 40K steps do not complete even one full epoch. Expanding hardware capabilities,
    increasing batch sizes, or adjusting hyperparameters like the learning rate or
    scheduler could potentially enhance outcomes.'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练约束：所有LLMs都经过相同的微调，模型训练参数一致：40K示例、批量大小为1、4位量化、LoRA秩为8，使用adam优化器和带有特定设置的余弦学习率调度器。训练在单个A10
    GPU上进行，使用梯度检查点以管理内存限制。对于那些完整序列长度会导致内存溢出的数据集，我们将序列截断到第95百分位长度。这种方法可能会影响模型训练的全面性，特别是在40K步骤甚至无法完成一个完整周期的数据集上。扩展硬件能力、增加批量大小或调整学习率或调度器等超参数可能会提高结果。
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited Model Variety: Our experiments are limited to LoRA fine-tuning on two
    model sizes, 2B and 7B. Exploring a broader range of model sizes, including larger
    models such as 13B or 70B, could provide insights into the scalability and effectiveness
    of fine-tuning across different computational capacities.'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型多样性有限：我们的实验仅限于对两种模型大小（2B和7B）进行LoRA微调。探索更广泛的模型尺寸，包括像13B或70B这样的较大模型，可能会提供关于不同计算能力下微调的可扩展性和有效性的见解。
- en: We maintain that LoRA Land successfully demonstrates the practical efficiency
    of training and serving several task-specialized LLMs that rival GPT-4 in a production
    application powered by LoRAX, despite these limitations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，尽管存在这些限制，LoRA Land成功展示了在生产应用中训练和服务几个任务专用LLM的实际效率，这些LLM在LoRAX驱动的生产应用中可以与GPT-4媲美。
- en: 8 Conclusion
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this study, we assess the efficacy of Low Rank Adaptation (LoRA) for fine-tuning
    Large Language Models (LLMs) across a broad range of tasks and models and the
    viability of serving multiple fine-tuned LoRA LLMs in production.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们评估了低秩适配（LoRA）在广泛任务和模型中的微调效果，以及在生产环境中服务多个微调的LoRA大语言模型（LLM）的可行性。
- en: On model quality, our results confirm that LoRA fine-tuning significantly enhances
    LLM performance, surpassing non-fine-tuned bases and GPT-4\. The standout performance
    of models like Mistral-7B across multiple tasks highlights the importance of base
    model selection in fine-tuning success. We find that dataset complexity heuristics
    can be reasonably leveraged as potential predictors of fine-tuning success, suggesting
    that the nature of the task plays an important role in the effectiveness of fine-tuning.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型质量，我们的结果确认了LoRA微调显著提升了LLM的性能，超过了未微调的基础模型和GPT-4。像Mistral-7B这样的模型在多个任务中的出色表现突显了基础模型选择在微调成功中的重要性。我们发现，数据集复杂性启发式可以合理地作为微调成功的潜在预测因子，表明任务的性质在微调效果中扮演了重要角色。
- en: Despite these outcomes, limitations such as the scale of evaluations, training
    constraints, and the simplicity of our prompt engineering approaches suggest areas
    for future improvement. We release all of our models and training setups for further
    community validation and experimentation.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些结果，诸如评估规模、训练限制以及我们提示工程方法的简单性等限制性因素表明了未来改进的方向。我们发布了所有的模型和训练设置，以便进行进一步的社区验证和实验。
- en: On serving, we demonstrate the practical deployment of these models using the
    LoRAX framework through the LoRA Land web application. We provide benchmarks for
    time to first token (TFTT), total request time, and token streaming time, and
    measure LoRAX’s latency robustness to up to 100 concurrent users.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务方面，我们展示了通过LoRAX框架在LoRA Land web应用中实际部署这些模型。我们提供了首次令牌时间（TFTT）、总请求时间和令牌流时间的基准，并测量了LoRAX对最多100名并发用户的延迟鲁棒性。
- en: Altogether, LoRA Land emphasizes the quality and cost-effectiveness of employing
    multiple specialized LLMs over a single, general-purpose LLM.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，LoRA Land强调了采用多个专用LLM相较于单一通用LLM的质量和成本效益。
- en: 9 Acknowledgements
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: Justin Zhao led the research and wrote the paper. Justin Zhao and Timothy Wang
    designed the experiments, created the evaluation harness, ran experiments, and
    analyzed the data. Wael Abid led LoRAX performance benchmarks and wrote section
    6 of the paper. Piero Molino was an early advocate for the idea and provided feedback
    on the writing, experiments, and data analysis. We thank Martin Davis, Kabir Brar,
    and Jackie Ho for designing and developing the LoRA Land web application. We thank
    Travis Addair, Geoffrey Angus, Magdy Saleh, Noah Yoshida, Jeffrey Tang, and open
    source contributors for developing LoRAX. We thank Noah Yoshida and Gyanesh Mishra
    for supporting deployments. We thank Arnav Garg, Geoffrey Angus, Arnav Garg, Jeff
    Kinnison, Alex Shertinsky, Travis Addair, Piero Molino, and open source contributors
    for Ludwig. We thank Will Gorman, Michael Gonzales, and Devvret Rishi for support,
    discussion, and feedback.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Justin Zhao主导了研究并撰写了论文。Justin Zhao和Timothy Wang设计了实验，创建了评估工具，进行了实验并分析了数据。Wael
    Abid主导了LoRAX性能基准测试并撰写了第6节。Piero Molino是这个想法的早期倡导者，并对写作、实验和数据分析提供了反馈。我们感谢Martin
    Davis、Kabir Brar和Jackie Ho设计和开发了LoRA Land web应用。我们感谢Travis Addair、Geoffrey Angus、Magdy
    Saleh、Noah Yoshida、Jeffrey Tang以及开源贡献者开发了LoRAX。我们感谢Noah Yoshida和Gyanesh Mishra对部署的支持。我们感谢Arnav
    Garg、Geoffrey Angus、Arnav Garg、Jeff Kinnison、Alex Shertinsky、Travis Addair、Piero
    Molino和开源贡献者对Ludwig的支持。我们感谢Will Gorman、Michael Gonzales和Devvret Rishi的支持、讨论和反馈。
- en: References
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Addair and Angus [2023] Travis Addair and Geoffrey Angus. LoRA Exchange (LoRAX):
    Serve 100s of Fine-Tuned LLMs for the Cost of 1 - Predibase — predibase.com. [https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one](https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one),
    2023. [Accessed 15-04-2024].'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Addair 和 Angus [2023] Travis Addair 和 Geoffrey Angus。LoRA Exchange (LoRAX):
    以 1 的成本服务于 100 个微调的 LLMs - Predibase — predibase.com。 [https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one](https://predibase.com/blog/lora-exchange-lorax-serve-100s-of-fine-tuned-llms-for-the-cost-of-one)，2023。
    [访问时间 2024 年 4 月 15 日]。'
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching 等人 [2023] Edward Beeching、Clémentine Fourrier、Nathan Habib、Sheon Han、Nathan
    Lambert、Nazneen Rajani、Omar Sanseviero、Lewis Tunstall 和 Thomas Wolf。开放 LLM 排行榜。
    [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，2023。
- en: Bommasani et al. [2021] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S.
    Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris
    Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin
    Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan
    Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter
    Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F.
    Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff
    Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,
    Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,
    Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D.
    Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika
    Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed
    Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung
    Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert
    Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher
    R’e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram
    Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E.
    Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro
    Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang,
    Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities
    and risks of foundation models. *ArXiv*, 2021. URL [https://crfm.stanford.edu/assets/report.pdf](https://crfm.stanford.edu/assets/report.pdf).
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani 等人 [2021] Rishi Bommasani、Drew A. Hudson、Ehsan Adeli、Russ Altman、Simran
    Arora、Sydney von Arx、Michael S. Bernstein、Jeannette Bohg、Antoine Bosselut、Emma
    Brunskill、Erik Brynjolfsson、S. Buch、Dallas Card、Rodrigo Castellon、Niladri S. Chatterji、Annie
    S. Chen、Kathleen A. Creel、Jared Davis、Dora Demszky、Chris Donahue、Moussa Doumbouya、Esin
    Durmus、Stefano Ermon、John Etchemendy、Kawin Ethayarajh、Li Fei-Fei、Chelsea Finn、Trevor
    Gale、Lauren E. Gillespie、Karan Goel、Noah D. Goodman、Shelby Grossman、Neel Guha、Tatsunori
    Hashimoto、Peter Henderson、John Hewitt、Daniel E. Ho、Jenny Hong、Kyle Hsu、Jing Huang、Thomas
    F. Icard、Saahil Jain、Dan Jurafsky、Pratyusha Kalluri、Siddharth Karamcheti、Geoff
    Keeling、Fereshte Khani、O. Khattab、Pang Wei Koh、Mark S. Krass、Ranjay Krishna、Rohith
    Kuditipudi、Ananya Kumar、Faisal Ladhak、Mina Lee、Tony Lee、Jure Leskovec、Isabelle
    Levent、Xiang Lisa Li、Xuechen Li、Tengyu Ma、Ali Malik、Christopher D. Manning、Suvir
    P. Mirchandani、Eric Mitchell、Zanele Munyikwa、Suraj Nair、Avanika Narayan、Deepak
    Narayanan、Benjamin Newman、Allen Nie、Juan Carlos Niebles、Hamed Nilforoshan、J. F.
    Nyarko、Giray Ogut、Laurel Orr、Isabel Papadimitriou、Joon Sung Park、Chris Piech、Eva
    Portelance、Christopher Potts、Aditi Raghunathan、Robert Reich、Hongyu Ren、Frieda
    Rong、Yusuf H. Roohani、Camilo Ruiz、Jack Ryan、Christopher R’e、Dorsa Sadigh、Shiori
    Sagawa、Keshav Santhanam、Andy Shih、Krishna Parasuram Srinivasan、Alex Tamkin、Rohan
    Taori、Armin W. Thomas、Florian Tramèr、Rose E. Wang、William Wang、Bohan Wu、Jiajun
    Wu、Yuhuai Wu、Sang Michael Xie、Michihiro Yasunaga、Jiaxuan You、Matei A. Zaharia、Michael
    Zhang、Tianyi Zhang、Xikun Zhang、Yuhui Zhang、Lucia Zheng、Kaitlyn Zhou 和 Percy Liang。关于基础模型的机会与风险。*ArXiv*，2021。网址
    [https://crfm.stanford.edu/assets/report.pdf](https://crfm.stanford.edu/assets/report.pdf)。
- en: 'Chen et al. [2023] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze,
    and Arvind Krishnamurthy. Punica: Multi-tenant lora serving, 2023.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2023] Lequn Chen、Zihao Ye、Yongji Wu、Danyang Zhuo、Luis Ceze 和 Arvind
    Krishnamurthy。Punica: 多租户 lora 服务，2023。'
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code, 2021.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人 [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, 和 Wojciech
    Zaremba. 评估基于代码训练的大型语言模型，2021。
- en: cjadams et al. [2019] cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas
    Dixon, Lucy Vasserman, and nithum. Jigsaw unintended bias in toxicity classification,
    2019. URL [https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification](https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cjadams等人 [2019] cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas
    Dixon, Lucy Vasserman, 和 nithum. Jigsaw 无意的毒性分类偏见，2019。网址 [https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification](https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)。
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems, 2021.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人 [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证器以解决数学词题，2021。
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等人 [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora: 高效微调量化语言模型，2023。'
- en: Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model
    evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人 [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black,
    Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, 和 Andy Zou. 少样本语言模型评估框架，2023年12月。网址 [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: Han and Han [2024] Daniel Han and Michael Han. Unsloth Fixing Gemma bugs — unsloth.ai.
    [https://unsloth.ai/blog/gemma-bugs](https://unsloth.ai/blog/gemma-bugs), 2024.
    [Accessed 15-04-2024].
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han和Han [2024] Daniel Han 和 Michael Han. 修复Gemma错误 — unsloth.ai. [https://unsloth.ai/blog/gemma-bugs](https://unsloth.ai/blog/gemma-bugs)，2024。
    [访问日期：2024年4月15日]。
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding, 2020.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人 [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解，2020。
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp, 2019.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby等人 [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain Gelly.
    参数高效的迁移学习在自然语言处理中的应用，2019。
- en: Howard and Ruder [2018] Jeremy Howard and Sebastian Ruder. Universal language
    model fine-tuning for text classification, 2018.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard和Ruder [2018] Jeremy Howard 和 Sebastian Ruder. 通用语言模型微调用于文本分类，2018。
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. Lora：大型语言模型的低秩适应，2021。
- en: Jiang et al. [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2023] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix 和 William
    El Sayed. Mistral 7b，2023。
- en: 'Kocmi et al. [2021] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin
    Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. To ship or not to ship:
    An extensive evaluation of automatic metrics for machine translation, 2021.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocmi 等人 [2021] Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt,
    Hitokazu Matsushita 和 Arul Menezes. 该发还是不该发：自动评估指标在机器翻译中的广泛评估，2021。
- en: Kohút and Hradiš [2023] Jan Kohút and Michal Hradiš. Finetuning is a surprisingly
    effective domain adaptation baseline in handwriting recognition, 2023.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohút 和 Hradiš [2023] Jan Kohút 和 Michal Hradiš. 微调是手写识别中一个出乎意料的有效领域适应基线，2023。
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang 和 Ion Stoica. 用分页注意力高效管理大型语言模型的内存。在
    *ACM SIGOPS 第29届操作系统原则研讨会论文集* 中，2023。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning, 2021.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等人 [2021] Brian Lester, Rami Al-Rfou 和 Noah Constant. 参数高效提示调优的规模效应，2021。
- en: Liang et al. [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
    Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric
    Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
    Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,
    Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian
    Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
    Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen
    Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language
    models. Published in Transactions on Machine Learning Research (TMLR), 2023, 2022.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2022] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
    Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher
    D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman,
    Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav
    Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim,
    Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan
    Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,
    Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan
    Mai, Yuhui Zhang 和 Yuta Koreeda. 语言模型的整体评估。发表在《机器学习研究汇刊》(TMLR)，2023，2022。
- en: 'Liu et al. [2024] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov,
    Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed
    low-rank adaptation, 2024.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
    Frank Wang, Kwang-Ting Cheng 和 Min-Hung Chen. Dora：权重分解的低秩适应，2024。
- en: 'Meng et al. [2024] Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang
    Wu, Xiaochen Wang, Peiyi Wang, Qingxiu Dong, Liang Chen, and Zhifang Sui. Periodiclora:
    Breaking the low-rank bottleneck in lora optimization, 2024.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人 [2024] Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu,
    Xiaochen Wang, Peiyi Wang, Qingxiu Dong, Liang Chen 和 Zhifang Sui. Periodiclora：打破
    Lora 优化中的低秩瓶颈，2024。
- en: 'Minaee et al. [2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey,
    2024.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等人 [2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain 和 Jianfeng Gao. 大型语言模型：综述，2024。
- en: 'Molino et al. [2019] Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala.
    Ludwig: a type-based declarative deep learning toolbox, 2019.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molino 等人 [2019] Piero Molino, Yaroslav Dudin 和 Sai Sumanth Miryala. Ludwig：一种基于类型的声明性深度学习工具箱，2019。
- en: Nori et al. [2023] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin,
    Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models
    outcompete special-purpose tuning? case study in medicine, 2023.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nori et al. [2023] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin,
    Naoto Usuyama, Chris White, 和 Eric Horvitz. 通用基础模型能否超越专用调优？医学案例研究，2023年。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. GPT-4 技术报告，2023年。
- en: 'OpenAI [2024] OpenAI. GitHub - openai/tiktoken: tiktoken is a fast BPE tokeniser
    for use with OpenAI’s models. — github.com. [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken),
    2024. [Accessed 15-04-2024].'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI [2024] OpenAI. GitHub - openai/tiktoken: tiktoken 是用于 OpenAI 模型的快速 BPE
    分词器。 — github.com. [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)，2024年。[访问日期
    2024年4月15日]。'
- en: Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll
    L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,
    Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe. 通过人类反馈训练语言模型以跟随指令，2022年。
- en: Peters et al. [2019] Matthew E. Peters, Sebastian Ruder, and Noah A. Smith.
    To tune or not to tune? adapting pretrained representations to diverse tasks,
    2019.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters et al. [2019] Matthew E. Peters, Sebastian Ruder, 和 Noah A. Smith. 调优还是不调优？将预训练表示适应不同任务，2019年。
- en: 'Pfeiffer et al. [2020] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun
    Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer
    learning. Proceedings of EACL 2021, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer et al. [2020] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun
    Cho, 和 Iryna Gurevych. Adapterfusion: 任务组合的非破坏性迁移学习。EACL 2021 论文集，2020年。'
- en: 'Razdaibiedina et al. [2023] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian
    Khabsa, Mike Lewis, and Amjad Almahairi. Progressive prompts: Continual learning
    for language models, 2023.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razdaibiedina et al. [2023] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian
    Khabsa, Mike Lewis, 和 Amjad Almahairi. 渐进式提示：语言模型的持续学习，2023年。
- en: Rebuffi et al. [2017] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
    Learning multiple visual domains with residual adapters, 2017.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi et al. [2017] Sylvestre-Alvise Rebuffi, Hakan Bilen, 和 Andrea Vedaldi.
    通过残差适配器学习多个视觉领域，2017年。
- en: 'Rücklé et al. [2020] Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck,
    Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency
    of adapters in transformers, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rücklé et al. [2020] Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck,
    Jonas Pfeiffer, Nils Reimers, 和 Iryna Gurevych. Adapterdrop: 变压器中适配器的效率，2020年。'
- en: 'Song et al. [2022] Yisheng Song, Ting Wang, Subrota K Mondal, and Jyoti Prakash
    Sahoo. A comprehensive survey of few-shot learning: Evolution, applications, challenges,
    and opportunities, 2022.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2022] Yisheng Song, Ting Wang, Subrota K Mondal, 和 Jyoti Prakash
    Sahoo. 少样本学习的综合调查：演变、应用、挑战和机遇，2022年。
- en: 'Team [2023] Gemini Team. Gemini: A family of highly capable multimodal models,
    2023.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team [2023] Gemini Team. Gemini: 一系列高性能的多模态模型，2023年。'
- en: 'Team [2024] Gemma Team. Gemma: Open models based on gemini research and technology,
    2024.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team [2024] Gemma Team. Gemma: 基于 Gemini 研究和技术的开放模型，2024年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和
    Guillaume Lample. Llama: 开放且高效的基础语言模型，2023年。'
- en: 'Tunstall et al. [2023] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunstall et al. [2023] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, 和
    Thomas Wolf. Zephyr: 语言模型对齐的直接蒸馏，2023年。'
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding, 2018.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, 和 Samuel R. Bowman. Glue: 一种用于自然语言理解的多任务基准和分析平台，2018。'
- en: Wang et al. [2022a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models, 2022a.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 自一致性提升语言模型的思维链推理，2022a。
- en: 'Wang et al. [2022b] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning
    language models with self-generated instructions, 2022b.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2022b] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. Self-instruct: 将语言模型与自生成指令对齐，2022b。'
- en: Wang et al. [2021] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi
    Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning
    to prompt for continual learning, 2021.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2021] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi
    Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, 和 Tomas Pfister. 学习为持续学习提示，2021。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models, 2022.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 思维链提示引发大型语言模型中的推理，2022。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    和 Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？, 2019。'
- en: Zheng et al. [2024] Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui
    Liang, and Shikai Wu. Fine-tuning large language models for domain-specific machine
    translation, 2024.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2024] Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui
    Liang, 和 Shikai Wu. 针对领域特定机器翻译的语言模型微调，2024。
- en: 'Zhong et al. [2017] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql:
    Generating structured queries from natural language using reinforcement learning.
    *CoRR*, abs/1709.00103, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. [2017] Victor Zhong, Caiming Xiong, 和 Richard Socher. Seq2sql:
    使用强化学习从自然语言生成结构化查询。*CoRR*, abs/1709.00103, 2017。'
- en: Appendix A Prompts for all tasks
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 所有任务的提示
- en: The preprocessing code, prompts, configuration, and splits used for all experiments
    can be found at [https://github.com/predibase/lora_bakeoff](https://github.com/predibase/lora_bakeoff).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验中使用的预处理代码、提示、配置和分割可以在 [https://github.com/predibase/lora_bakeoff](https://github.com/predibase/lora_bakeoff)
    找到。
- en: Appendix B LoRAX benchmarking scripts
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LoRAX 基准测试脚本
- en: The load testing script and instructions can be found at [https://github.com/predibase/lora_bakeoff](https://github.com/predibase/lora_bakeoff).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 负载测试脚本和说明可以在 [https://github.com/predibase/lora_bakeoff](https://github.com/predibase/lora_bakeoff)
    找到。
- en: Appendix C Full Results Tables
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 完整结果表
- en: '| Category | Task | Metric | Microsoft | Google | Meta | Mistral | Hugging
    Face | OpenAI |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 任务 | 指标 | 微软 | 谷歌 | Meta | Mistral | Hugging Face | OpenAI |'
- en: '| phi-2 | gemma-2b | gemma-2b-instruct | gemma-7b | gemma-7b-instruct | llama-2-7b
    | llama-2-7b-chat | mistral-7b | mistral-7b-instruct | zephyr-7b-beta | gpt-3.5-turbo
    | gpt-4 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| phi-2 | gemma-2b | gemma-2b-instruct | gemma-7b | gemma-7b-instruct | llama-2-7b
    | llama-2-7b-chat | mistral-7b | mistral-7b-instruct | zephyr-7b-beta | gpt-3.5-turbo
    | gpt-4 |'
- en: '| Classic NLP | bc5cdr | rouge | 0.172 | 0.013 | 0.494 | 0.075 | 0.198 | 0.185
    | 0.024 | 0.177 | 0.703 | 0.146 | 0.732 | 0.890 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Classic NLP | bc5cdr | rouge | 0.172 | 0.013 | 0.494 | 0.075 | 0.198 | 0.185
    | 0.024 | 0.177 | 0.703 | 0.146 | 0.732 | 0.890 |'
- en: '| conllpp | rouge | 0.101 | 0.011 | 0.647 | 0.085 | 0.120 | 0.108 | 0.115 |
    0.148 | 0.733 | 0.088 | 0.810 | 0.742 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| conllpp | rouge | 0.101 | 0.011 | 0.647 | 0.085 | 0.120 | 0.108 | 0.115 |
    0.148 | 0.733 | 0.088 | 0.810 | 0.742 |'
- en: '| e2e_nlg | rouge | 0.132 | 0.174 | 0.281 | 0.152 | 0.434 | 0.087 | 0.442 |
    0.167 | 0.482 | 0.122 | 0.467 | 0.513 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| e2e_nlg | rouge | 0.132 | 0.174 | 0.281 | 0.152 | 0.434 | 0.087 | 0.442 |
    0.167 | 0.482 | 0.122 | 0.467 | 0.513 |'
- en: '| tldr_content_gen | rouge | 0.158 | 0.117 | 0.160 | 0.089 | 0.141 | 0.148
    | 0.183 | 0.153 | 0.163 | 0.164 | 0.173 | 0.125 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| tldr_content_gen | rouge | 0.158 | 0.117 | 0.160 | 0.089 | 0.141 | 0.148
    | 0.183 | 0.153 | 0.163 | 0.164 | 0.173 | 0.125 |'
- en: '| tldr_headline_gen | rouge | 0.169 | 0.034 | 0.155 | 0.063 | 0.152 | 0.078
    | 0.174 | 0.071 | 0.171 | 0.120 | 0.195 | 0.175 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| tldr_headline_gen | rouge | 0.169 | 0.034 | 0.155 | 0.063 | 0.152 | 0.078
    | 0.174 | 0.071 | 0.171 | 0.120 | 0.195 | 0.175 |'
- en: '| viggo | rouge | 0.133 | 0.093 | 0.237 | 0.123 | 0.313 | 0.141 | 0.356 | 0.044
    | 0.374 | 0.193 | 0.372 | 0.374 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| viggo | rouge | 0.133 | 0.093 | 0.237 | 0.123 | 0.313 | 0.141 | 0.356 | 0.044
    | 0.374 | 0.193 | 0.372 | 0.374 |'
- en: '| webnlg | rouge | 0.120 | 0.055 | 0.312 | 0.257 | 0.453 | 0.148 | 0.563 |
    0.091 | 0.541 | 0.512 | 0.581 | 0.583 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| webnlg | rouge | 0.120 | 0.055 | 0.312 | 0.257 | 0.453 | 0.148 | 0.563 |
    0.091 | 0.541 | 0.512 | 0.581 | 0.583 |'
- en: '| Coding | magicoder | humaneval | 0.012 | 0.037 | 0.024 | 0.030 | 0.018 |
    0.012 | 0.134 | 0.201 | 0.152 | 0.049 | 0.683 | 0.829 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Coding | magicoder | humaneval | 0.012 | 0.037 | 0.024 | 0.030 | 0.018 |
    0.012 | 0.134 | 0.201 | 0.152 | 0.049 | 0.683 | 0.829 |'
- en: '| wikisql | rouge | 0.143 | 0.030 | 0.301 | 0.036 | 0.244 | 0.043 | 0.093 |
    0.265 | 0.134 | 0.080 | 0.887 | 0.909 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| wikisql | rouge | 0.143 | 0.030 | 0.301 | 0.036 | 0.244 | 0.043 | 0.093 |
    0.265 | 0.134 | 0.080 | 0.887 | 0.909 |'
- en: '| Knowledge | boolq | accuracy | 0.691 | 0.447 | 0.661 | 0.300 | 0.735 | 0.645
    | 0.759 | 0.669 | 0.764 | 0.683 | 0.870 | 0.911 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Knowledge | boolq | accuracy | 0.691 | 0.447 | 0.661 | 0.300 | 0.735 | 0.645
    | 0.759 | 0.669 | 0.764 | 0.683 | 0.870 | 0.911 |'
- en: '| dbpedia | dbpedia | 0.268 | 0.018 | 0.086 | 0.021 | 0.089 | 0.043 | 0.868
    | 0.036 | 0.313 | 0.578 | 0.853 | 0.965 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| dbpedia | dbpedia | 0.268 | 0.018 | 0.086 | 0.021 | 0.089 | 0.043 | 0.868
    | 0.036 | 0.313 | 0.578 | 0.853 | 0.965 |'
- en: '| customer_support | accuracy | 0.250 | 0.120 | 0.380 | 0.100 | 0.850 | 0.110
    | 0.630 | 0.030 | 0.730 | 0.540 | 1.000 | 1.000 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| customer_support | accuracy | 0.250 | 0.120 | 0.380 | 0.100 | 0.850 | 0.110
    | 0.630 | 0.030 | 0.730 | 0.540 | 1.000 | 1.000 |'
- en: '| glue_qnli | accuracy | 0.496 | 0.439 | 0.444 | 0.463 | 0.685 | 0.510 | 0.736
    | 0.533 | 0.743 | 0.569 | 0.829 | 0.902 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| glue_qnli | accuracy | 0.496 | 0.439 | 0.444 | 0.463 | 0.685 | 0.510 | 0.736
    | 0.533 | 0.743 | 0.569 | 0.829 | 0.902 |'
- en: '| glue_stsb | mae | 0.682 | 0.197 | 0.590 | 0.537 | 0.729 | 0.651 | 0.680 |
    0.672 | 0.723 | 0.814 | 0.857 | 0.773 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| glue_stsb | mae | 0.682 | 0.197 | 0.590 | 0.537 | 0.729 | 0.651 | 0.680 |
    0.672 | 0.723 | 0.814 | 0.857 | 0.773 |'
- en: '| legal | rouge | 0.008 | 0.010 | 0.037 | 0.019 | 0.053 | 0.009 | 0.026 | 0.001
    | 0.158 | 0.039 | 0.266 | 0.305 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| legal | rouge | 0.008 | 0.010 | 0.037 | 0.019 | 0.053 | 0.009 | 0.026 | 0.001
    | 0.158 | 0.039 | 0.266 | 0.305 |'
- en: '| reuters | rouge | 0.003 | 0.001 | 0.010 | 0.001 | 0.009 | 0.003 | 0.010 |
    0.004 | 0.010 | 0.005 | 0.026 | 0.014 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| reuters | rouge | 0.003 | 0.001 | 0.010 | 0.001 | 0.009 | 0.003 | 0.010 |
    0.004 | 0.010 | 0.005 | 0.026 | 0.014 |'
- en: '| mmlu | accuracy | 0.339 | 0.160 | 0.279 | 0.302 | 0.460 | 0.189 | 0.349 |
    0.402 | 0.446 | 0.506 | 0.504 | 0.774 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| mmlu | accuracy | 0.339 | 0.160 | 0.279 | 0.302 | 0.460 | 0.189 | 0.349 |
    0.402 | 0.446 | 0.506 | 0.504 | 0.774 |'
- en: '| Reasoning | winogrande | accuracy | 0.380 | 0.309 | 0.515 | 0.390 | 0.576
    | 0.503 | 0.515 | 0.498 | 0.546 | 0.532 | 0.569 | 0.832 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning | winogrande | accuracy | 0.380 | 0.309 | 0.515 | 0.390 | 0.576
    | 0.503 | 0.515 | 0.498 | 0.546 | 0.532 | 0.569 | 0.832 |'
- en: '| arc_combined | accuracy | 0.323 | 0.180 | 0.254 | 0.272 | 0.657 | 0.304 |
    0.379 | 0.573 | 0.673 | 0.497 | 0.926 | 0.947 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| arc_combined | accuracy | 0.323 | 0.180 | 0.254 | 0.272 | 0.657 | 0.304 |
    0.379 | 0.573 | 0.673 | 0.497 | 0.926 | 0.947 |'
- en: '| glue_cola | accuracy | 0.463 | 0.152 | 0.642 | 0.062 | 0.749 | 0.691 | 0.691
    | 0.691 | 0.797 | 0.788 | 0.843 | 0.864 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| glue_cola | accuracy | 0.463 | 0.152 | 0.642 | 0.062 | 0.749 | 0.691 | 0.691
    | 0.691 | 0.797 | 0.788 | 0.843 | 0.864 |'
- en: '| glue_mnli | accuracy | 0.328 | 0.053 | 0.347 | 0.213 | 0.272 | 0.315 | 0.293
    | 0.327 | 0.455 | 0.348 | 0.588 | 0.803 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| glue_mnli | accuracy | 0.328 | 0.053 | 0.347 | 0.213 | 0.272 | 0.315 | 0.293
    | 0.327 | 0.455 | 0.348 | 0.588 | 0.803 |'
- en: '| glue_mrpc | accuracy | 0.652 | 0.265 | 0.664 | 0.654 | 0.652 | 0.679 | 0.674
    | 0.684 | 0.694 | 0.676 | 0.689 | 0.777 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| glue_mrpc | accuracy | 0.652 | 0.265 | 0.664 | 0.654 | 0.652 | 0.679 | 0.674
    | 0.684 | 0.694 | 0.676 | 0.689 | 0.777 |'
- en: '| glue_qqp | accuracy | 0.327 | 0.138 | 0.337 | 0.316 | 0.396 | 0.345 | 0.340
    | 0.327 | 0.708 | 0.340 | 0.830 | 0.841 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| glue_qqp | accuracy | 0.327 | 0.138 | 0.337 | 0.316 | 0.396 | 0.345 | 0.340
    | 0.327 | 0.708 | 0.340 | 0.830 | 0.841 |'
- en: '| glue_sst2 | accuracy | 0.487 | 0.407 | 0.719 | 0.187 | 0.682 | 0.306 | 0.695
    | 0.115 | 0.933 | 0.706 | 0.933 | 0.942 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| glue_sst2 | accuracy | 0.487 | 0.407 | 0.719 | 0.187 | 0.682 | 0.306 | 0.695
    | 0.115 | 0.933 | 0.706 | 0.933 | 0.942 |'
- en: '| glue_wnli | accuracy | 0.437 | 0.183 | 0.437 | 0.366 | 0.437 | 0.423 | 0.437
    | 0.437 | 0.437 | 0.437 | 0.521 | 0.930 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| glue_wnli | accuracy | 0.437 | 0.183 | 0.437 | 0.366 | 0.437 | 0.423 | 0.437
    | 0.437 | 0.437 | 0.437 | 0.521 | 0.930 |'
- en: '| covid | accuracy | 0.207 | 0.154 | 0.317 | 0.169 | 0.322 | 0.162 | 0.212
    | 0.191 | 0.297 | 0.243 | 0.334 | 0.309 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| covid | accuracy | 0.207 | 0.154 | 0.317 | 0.169 | 0.322 | 0.162 | 0.212
    | 0.191 | 0.297 | 0.243 | 0.334 | 0.309 |'
- en: '| hellaswag | accuracy | 0.371 | 0.117 | 0.023 | 0.112 | 0.201 | 0.381 | 0.264
    | 0.246 | 0.249 | 0.393 | 0.622 | 0.805 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag | accuracy | 0.371 | 0.117 | 0.023 | 0.112 | 0.201 | 0.381 | 0.264
    | 0.246 | 0.249 | 0.393 | 0.622 | 0.805 |'
- en: '| hellaswag_processed | rouge | 0.037 | 0.056 | 0.146 | 0.109 | 0.143 | 0.044
    | 0.089 | 0.038 | 0.134 | 0.040 | 0.140 | 0.134 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag_processed | rouge | 0.037 | 0.056 | 0.146 | 0.109 | 0.143 | 0.044
    | 0.089 | 0.038 | 0.134 | 0.040 | 0.140 | 0.134 |'
- en: '| jigsaw | accuracy | 0.491 | 0.490 | 0.482 | 0.233 | 0.520 | 0.486 | 0.545
    | 0.475 | 0.704 | 0.472 | 0.735 | 0.754 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| jigsaw | accuracy | 0.491 | 0.490 | 0.482 | 0.233 | 0.520 | 0.486 | 0.545
    | 0.475 | 0.704 | 0.472 | 0.735 | 0.754 |'
- en: '| drop | rouge | 0.018 | 0.013 | 0.034 | 0.024 | 0.042 | 0.010 | 0.047 | 0.011
    | 0.066 | 0.023 | 0.119 | 0.393 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| drop | rouge | 0.018 | 0.013 | 0.034 | 0.024 | 0.042 | 0.010 | 0.047 | 0.011
    | 0.066 | 0.023 | 0.119 | 0.393 |'
- en: '| Math | gsm8k | accuracy | 0.083 | 0.026 | 0.082 | 0.039 | 0.364 | 0.051 |
    0.160 | 0.114 | 0.275 | 0.133 | 0.622 | 0.373 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | gsm8k | 准确率 | 0.083 | 0.026 | 0.082 | 0.039 | 0.364 | 0.051 | 0.160
    | 0.114 | 0.275 | 0.133 | 0.622 | 0.373 |'
- en: 'Table 12: Base model performance for every task and base model, before fine-tuning.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 所有任务和基础模型的基准性能，未进行微调之前。'
- en: '| Category | Task | Metric | Microsoft | Google | Meta | Mistral | Hugging
    Face | OpenAI |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 任务 | 指标 | Microsoft | Google | Meta | Mistral | Hugging Face | OpenAI
    |'
- en: '| phi-2 | gemma-2b | gemma-2b-instruct | gemma-7b | gemma-7b-instruct | llama-2-7b
    | llama-2-7b-chat | mistral-7b | mistral-7b-instruct | zephyr-7b-beta | gpt-3.5-turbo
    | gpt-4 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| phi-2 | gemma-2b | gemma-2b-instruct | gemma-7b | gemma-7b-instruct | llama-2-7b
    | llama-2-7b-chat | mistral-7b | mistral-7b-instruct | zephyr-7b-beta | gpt-3.5-turbo
    | gpt-4 |'
- en: '| Classic NLP | bc5cdr | rouge | 0.950 (+0.778) | 0.961 (+0.948) | 0.956 (+0.462)
    | 0.969 (+0.894) | 0.969 (+0.771) | 0.967 (+0.782) | 0.967 (+0.943) | 0.972 (+0.795)
    | 0.971 (+0.268) | 0.969 (+0.823) | 0.732 | 0.890 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 经典 NLP | bc5cdr | rouge | 0.950 (+0.778) | 0.961 (+0.948) | 0.956 (+0.462)
    | 0.969 (+0.894) | 0.969 (+0.771) | 0.967 (+0.782) | 0.967 (+0.943) | 0.972 (+0.795)
    | 0.971 (+0.268) | 0.969 (+0.823) | 0.732 | 0.890 |'
- en: '| conllpp | rouge | 0.950 (+0.849) | 0.976 (+0.965) | 0.975 (+0.328) | 0.989
    (+0.904) | 0.989 (+0.869) | 0.977 (+0.869) | 0.980 (+0.865) | 0.986 (+0.838) |
    0.987 (+0.254) | 0.984 (+0.896) | 0.810 | 0.742 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| conllpp | rouge | 0.950 (+0.849) | 0.976 (+0.965) | 0.975 (+0.328) | 0.989
    (+0.904) | 0.989 (+0.869) | 0.977 (+0.869) | 0.980 (+0.865) | 0.986 (+0.838) |
    0.987 (+0.254) | 0.984 (+0.896) | 0.810 | 0.742 |'
- en: '| e2e_nlg | rouge | 0.516 (+0.384) | 0.543 (+0.369) | 0.543 (+0.262) | 0.549
    (+0.397) | 0.550 (+0.116) | 0.541 (+0.454) | 0.538 (+0.096) | 0.552 (+0.385) |
    0.551 (+0.069) | 0.543 (+0.421) | 0.467 | 0.513 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| e2e_nlg | rouge | 0.516 (+0.384) | 0.543 (+0.369) | 0.543 (+0.262) | 0.549
    (+0.397) | 0.550 (+0.116) | 0.541 (+0.454) | 0.538 (+0.096) | 0.552 (+0.385) |
    0.551 (+0.069) | 0.543 (+0.421) | 0.467 | 0.513 |'
- en: '| tldr_content_gen | rouge | 0.201 (+0.043) | 0.204 (+0.087) | 0.202 (+0.042)
    | 0.217 (+0.128) | 0.194 (+0.053) | 0.219 (+0.071) | 0.220 (+0.037) | 0.227 (+0.074)
    | 0.226 (+0.063) | 0.230 (+0.066) | 0.173 | 0.125 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| tldr_content_gen | rouge | 0.201 (+0.043) | 0.204 (+0.087) | 0.202 (+0.042)
    | 0.217 (+0.128) | 0.194 (+0.053) | 0.219 (+0.071) | 0.220 (+0.037) | 0.227 (+0.074)
    | 0.226 (+0.063) | 0.230 (+0.066) | 0.173 | 0.125 |'
- en: '| tldr_headline_gen | rouge | 0.343 (+0.174) | 0.404 (+0.370) | 0.385 (+0.230)
    | 0.394 (+0.331) | 0.391 (+0.239) | 0.432 (+0.354) | 0.429 (+0.255) | 0.434 (+0.363)
    | 0.419 (+0.248) | 0.441 (+0.321) | 0.195 | 0.175 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| tldr_headline_gen | rouge | 0.343 (+0.174) | 0.404 (+0.370) | 0.385 (+0.230)
    | 0.394 (+0.331) | 0.391 (+0.239) | 0.432 (+0.354) | 0.429 (+0.255) | 0.434 (+0.363)
    | 0.419 (+0.248) | 0.441 (+0.321) | 0.195 | 0.175 |'
- en: '| viggo | rouge | 0.445 (+0.312) | 0.504 (+0.411) | 0.497 (+0.260) | 0.474
    (+0.351) | 0.441 (+0.128) | 0.469 (+0.328) | 0.463 (+0.107) | 0.483 (+0.439) |
    0.505 (+0.131) | 0.477 (+0.284) | 0.372 | 0.374 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| viggo | rouge | 0.445 (+0.312) | 0.504 (+0.411) | 0.497 (+0.260) | 0.474
    (+0.351) | 0.441 (+0.128) | 0.469 (+0.328) | 0.463 (+0.107) | 0.483 (+0.439) |
    0.505 (+0.131) | 0.477 (+0.284) | 0.372 | 0.374 |'
- en: '| webnlg | rouge | 0.634 (+0.514) | 0.652 (+0.597) | 0.649 (+0.337) | 0.673
    (+0.416) | 0.664 (+0.211) | 0.666 (+0.518) | 0.673 (+0.110) | 0.681 (+0.590) |
    0.672 (+0.131) | 0.677 (+0.165) | 0.581 | 0.583 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| webnlg | rouge | 0.634 (+0.514) | 0.652 (+0.597) | 0.649 (+0.337) | 0.673
    (+0.416) | 0.664 (+0.211) | 0.666 (+0.518) | 0.673 (+0.110) | 0.681 (+0.590) |
    0.672 (+0.131) | 0.677 (+0.165) | 0.581 | 0.583 |'
- en: '| Coding | magicoder | humaneval | 0.384 (+0.372) | 0.079 (+0.042) | 0.152
    (+0.128) | 0.433 (+0.403) | 0.329 (+0.311) | 0.122 (+0.110) | 0.152 (+0.018) |
    0.335 (+0.134) | 0.341 (+0.189) | 0.317 (+0.268) | 0.683 | 0.829 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | magicoder | humaneval | 0.384 (+0.372) | 0.079 (+0.042) | 0.152 (+0.128)
    | 0.433 (+0.403) | 0.329 (+0.311) | 0.122 (+0.110) | 0.152 (+0.018) | 0.335 (+0.134)
    | 0.341 (+0.189) | 0.317 (+0.268) | 0.683 | 0.829 |'
- en: '| wikisql | rouge | 0.680 (+0.537) | 0.890 (+0.860) | 0.885 (+0.584) | 0.894
    (+0.858) | 0.893 (+0.649) | 0.898 (+0.855) | 0.893 (+0.800) | 0.669 (+0.404) |
    0.651 (+0.517) | 0.896 (+0.816) | 0.887 | 0.909 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| wikisql | rouge | 0.680 (+0.537) | 0.890 (+0.860) | 0.885 (+0.584) | 0.894
    (+0.858) | 0.893 (+0.649) | 0.898 (+0.855) | 0.893 (+0.800) | 0.669 (+0.404) |
    0.651 (+0.517) | 0.896 (+0.816) | 0.887 | 0.909 |'
- en: '| Knowledge | boolq | accuracy | 0.863 (+0.172) | 0.811 (+0.364) | 0.776 (+0.115)
    | 0.664 (+0.364) | 0.665 (-0.070) | 0.884 (+0.239) | 0.872 (+0.113) | 0.909 (+0.240)
    | 0.891 (+0.127) | 0.897 (+0.214) | 0.870 | 0.911 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 知识 | boolq | 准确率 | 0.863 (+0.172) | 0.811 (+0.364) | 0.776 (+0.115) | 0.664
    (+0.364) | 0.665 (-0.070) | 0.884 (+0.239) | 0.872 (+0.113) | 0.909 (+0.240) |
    0.891 (+0.127) | 0.897 (+0.214) | 0.870 | 0.911 |'
- en: '| dbpedia | accuracy | 0.988 (+0.720) | 0.960 (+0.942) | 0.961 (+0.875) | 0.964
    (+0.943) | 0.971 (+0.882) | 0.975 (+0.932) | 0.980 (+0.112) | 0.981 (+0.945) |
    0.970 (+0.657) | 0.963 (+0.385) | 0.853 | 0.965 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| dbpedia | 准确率 | 0.988 (+0.720) | 0.960 (+0.942) | 0.961 (+0.875) | 0.964
    (+0.943) | 0.971 (+0.882) | 0.975 (+0.932) | 0.980 (+0.112) | 0.981 (+0.945) |
    0.970 (+0.657) | 0.963 (+0.385) | 0.853 | 0.965 |'
- en: '| customer_support | accuracy | 1.000 (+0.750) | 1.000 (+0.880) | 1.000 (+0.620)
    | 1.000 (+0.900) | 1.000 (+0.150) | 1.000 (+0.890) | 1.000 (+0.370) | 1.000 (+0.970)
    | 1.000 (+0.270) | 1.000 (+0.460) | 1.000 | 1.000 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| customer_support | 准确率 | 1.000 (+0.750) | 1.000 (+0.880) | 1.000 (+0.620)
    | 1.000 (+0.900) | 1.000 (+0.150) | 1.000 (+0.890) | 1.000 (+0.370) | 1.000 (+0.970)
    | 1.000 (+0.270) | 1.000 (+0.460) | 1.000 | 1.000 |'
- en: '| glue_qnli | accuracy | 0.892 (+0.396) | 0.872 (+0.433) | 0.887 (+0.443) |
    0.897 (+0.434) | 0.876 (+0.191) | 0.860 (+0.350) | 0.925 (+0.189) | 0.931 (+0.398)
    | 0.906 (+0.163) | 0.928 (+0.359) | 0.829 | 0.902 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| glue_qnli | 准确率 | 0.892 (+0.396) | 0.872 (+0.433) | 0.887 (+0.443) | 0.897
    (+0.434) | 0.876 (+0.191) | 0.860 (+0.350) | 0.925 (+0.189) | 0.931 (+0.398) |
    0.906 (+0.163) | 0.928 (+0.359) | 0.829 | 0.902 |'
- en: '| glue_stsb | mae | 0.888 (+0.206) | 0.875 (+0.678) | 0.895 (+0.305) | 0.704
    (+0.167) | 0.893 (+0.164) | 0.912 (+0.261) | 0.907 (+0.227) | 0.913 (+0.241) |
    0.911 (+0.188) | 0.911 (+0.097) | 0.857 | 0.773 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| glue_stsb | 平均绝对误差 | 0.888 (+0.206) | 0.875 (+0.678) | 0.895 (+0.305) | 0.704
    (+0.167) | 0.893 (+0.164) | 0.912 (+0.261) | 0.907 (+0.227) | 0.913 (+0.241) |
    0.911 (+0.188) | 0.911 (+0.097) | 0.857 | 0.773 |'
- en: '| legal | rouge | 0.404 (+0.396) | 0.503 (+0.493) | 0.451 (+0.414) | 0.586
    (+0.567) | 0.580 (+0.527) | 0.668 (+0.659) | 0.602 (+0.576) | 0.602 (+0.601) |
    0.666 (+0.508) | 0.683 (+0.644) | 0.266 | 0.305 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| legal | ROUGE | 0.404 (+0.396) | 0.503 (+0.493) | 0.451 (+0.414) | 0.586
    (+0.567) | 0.580 (+0.527) | 0.668 (+0.659) | 0.602 (+0.576) | 0.602 (+0.601) |
    0.666 (+0.508) | 0.683 (+0.644) | 0.266 | 0.305 |'
- en: '| reuters | rouge | 0.149 (+0.146) | 0.458 (+0.457) | 0.465 (+0.455) | 0.475
    (+0.474) | 0.477 (+0.468) | 0.475 (+0.472) | 0.475 (+0.465) | 0.431 (+0.427) |
    0.455 (+0.445) | 0.479 (+0.474) | 0.026 | 0.014 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| reuters | ROUGE | 0.149 (+0.146) | 0.458 (+0.457) | 0.465 (+0.455) | 0.475
    (+0.474) | 0.477 (+0.468) | 0.475 (+0.472) | 0.475 (+0.465) | 0.431 (+0.427) |
    0.455 (+0.445) | 0.479 (+0.474) | 0.026 | 0.014 |'
- en: '| mmlu | accuracy | 0.530 (+0.191) | 0.446 (+0.286) | 0.432 (+0.153) | 0.248
    (-0.054) | 0.243 (-0.217) | 0.519 (+0.330) | 0.526 (+0.177) | 0.561 (+0.159) |
    0.558 (+0.112) | 0.589 (+0.083) | 0.504 | 0.774 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| mmlu | 准确率 | 0.530 (+0.191) | 0.446 (+0.286) | 0.432 (+0.153) | 0.248 (-0.054)
    | 0.243 (-0.217) | 0.519 (+0.330) | 0.526 (+0.177) | 0.561 (+0.159) | 0.558 (+0.112)
    | 0.589 (+0.083) | 0.504 | 0.774 |'
- en: '| Reasoning | winogrande | accuracy | 0.741 (+0.361) | 0.493 (+0.184) | 0.494
    (-0.021) | 0.493 (+0.103) | 0.493 (-0.083) | 0.493 (-0.010) | 0.754 (+0.239) |
    0.840 (+0.342) | 0.818 (+0.272) | 0.825 (+0.293) | 0.569 | 0.832 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning | Winogrande | 准确率 | 0.741 (+0.361) | 0.493 (+0.184) | 0.494 (-0.021)
    | 0.493 (+0.103) | 0.493 (-0.083) | 0.493 (-0.010) | 0.754 (+0.239) | 0.840 (+0.342)
    | 0.818 (+0.272) | 0.825 (+0.293) | 0.569 | 0.832 |'
- en: '| arc_combined | accuracy | 0.915 (+0.592) | 0.768 (+0.588) | 0.745 (+0.491)
    | 0.269 (-0.003) | 0.258 (-0.399) | 0.832 (+0.528) | 0.843 (+0.464) | 0.915 (+0.342)
    | 0.857 (+0.184) | 0.909 (+0.412) | 0.926 | 0.947 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| arc_combined | 准确率 | 0.915 (+0.592) | 0.768 (+0.588) | 0.745 (+0.491) | 0.269
    (-0.003) | 0.258 (-0.399) | 0.832 (+0.528) | 0.843 (+0.464) | 0.915 (+0.342) |
    0.857 (+0.184) | 0.909 (+0.412) | 0.926 | 0.947 |'
- en: '| glue_cola | accuracy | 0.843 (+0.380) | 0.828 (+0.676) | 0.777 (+0.135) |
    0.691 (+0.629) | 0.691 (-0.058) | 0.837 (+0.146) | 0.860 (+0.169) | 0.845 (+0.154)
    | 0.849 (+0.052) | 0.872 (+0.084) | 0.843 | 0.864 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| glue_cola | 准确率 | 0.843 (+0.380) | 0.828 (+0.676) | 0.777 (+0.135) | 0.691
    (+0.629) | 0.691 (-0.058) | 0.837 (+0.146) | 0.860 (+0.169) | 0.845 (+0.154) |
    0.849 (+0.052) | 0.872 (+0.084) | 0.843 | 0.864 |'
- en: '| glue_mnli | accuracy | 0.871 (+0.543) | 0.833 (+0.780) | 0.837 (+0.490) |
    0.882 (+0.669) | 0.874 (+0.602) | 0.877 (+0.562) | 0.870 (+0.577) | 0.893 (+0.566)
    | 0.887 (+0.432) | 0.899 (+0.551) | 0.588 | 0.803 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| glue_mnli | 准确率 | 0.871 (+0.543) | 0.833 (+0.780) | 0.837 (+0.490) | 0.882
    (+0.669) | 0.874 (+0.602) | 0.877 (+0.562) | 0.870 (+0.577) | 0.893 (+0.566) |
    0.887 (+0.432) | 0.899 (+0.551) | 0.588 | 0.803 |'
- en: '| glue_mrpc | accuracy | 0.858 (+0.206) | 0.850 (+0.585) | 0.870 (+0.206) |
    0.740 (+0.086) | 0.684 (+0.032) | 0.797 (+0.118) | 0.870 (+0.196) | 0.887 (+0.203)
    | 0.885 (+0.191) | 0.870 (+0.194) | 0.689 | 0.777 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| glue_mrpc | 准确率 | 0.858 (+0.206) | 0.850 (+0.585) | 0.870 (+0.206) | 0.740
    (+0.086) | 0.684 (+0.032) | 0.797 (+0.118) | 0.870 (+0.196) | 0.887 (+0.203) |
    0.885 (+0.191) | 0.870 (+0.194) | 0.689 | 0.777 |'
- en: '| glue_qqp | accuracy | 0.875 (+0.548) | 0.877 (+0.739) | 0.863 (+0.526) |
    0.872 (+0.556) | 0.673 (+0.277) | 0.868 (+0.523) | 0.874 (+0.534) | 0.870 (+0.543)
    | 0.883 (+0.175) | 0.867 (+0.527) | 0.830 | 0.841 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| glue_qqp | 准确率 | 0.875 (+0.548) | 0.877 (+0.739) | 0.863 (+0.526) | 0.872
    (+0.556) | 0.673 (+0.277) | 0.868 (+0.523) | 0.874 (+0.534) | 0.870 (+0.543) |
    0.883 (+0.175) | 0.867 (+0.527) | 0.830 | 0.841 |'
- en: '| glue_sst2 | accuracy | 0.946 (+0.459) | 0.954 (+0.547) | 0.919 (+0.200) |
    0.919 (+0.732) | 0.943 (+0.261) | 0.948 (+0.642) | 0.956 (+0.261) | 0.959 (+0.844)
    | 0.958 (+0.025) | 0.961 (+0.255) | 0.933 | 0.942 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| glue_sst2 | 准确率 | 0.946 (+0.459) | 0.954 (+0.547) | 0.919 (+0.200) | 0.919
    (+0.732) | 0.943 (+0.261) | 0.948 (+0.642) | 0.956 (+0.261) | 0.959 (+0.844) |
    0.958 (+0.025) | 0.961 (+0.255) | 0.933 | 0.942 |'
- en: '| glue_wnli | accuracy | 0.676 (+0.239) | 0.563 (+0.380) | 0.563 (+0.126) |
    0.563 (+0.197) | 0.563 (+0.126) | 0.718 (+0.295) | 0.775 (+0.338) | 0.873 (+0.436)
    | 0.803 (+0.366) | 0.831 (+0.394) | 0.521 | 0.930 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| glue_wnli | 准确率 | 0.676 (+0.239) | 0.563 (+0.380) | 0.563 (+0.126) | 0.563
    (+0.197) | 0.563 (+0.126) | 0.718 (+0.295) | 0.775 (+0.338) | 0.873 (+0.436) |
    0.803 (+0.366) | 0.831 (+0.394) | 0.521 | 0.930 |'
- en: '| covid | accuracy | 0.692 (+0.485) | 0.827 (+0.673) | 0.832 (+0.515) | 0.830
    (+0.661) | 0.843 (+0.521) | 0.751 (+0.589) | 0.727 (+0.515) | 0.770 (+0.579) |
    0.811 (+0.514) | 0.776 (+0.533) | 0.334 | 0.309 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| covid | 准确率 | 0.692 (+0.485) | 0.827 (+0.673) | 0.832 (+0.515) | 0.830 (+0.661)
    | 0.843 (+0.521) | 0.751 (+0.589) | 0.727 (+0.515) | 0.770 (+0.579) | 0.811 (+0.514)
    | 0.776 (+0.533) | 0.334 | 0.309 |'
- en: '| hellaswag | accuracy | 0.714 (+0.343) | 0.397 (+0.280) | 0.252 (+0.229) |
    0.252 (+0.140) | 0.252 (+0.051) | 0.741 (+0.360) | 0.736 (+0.472) | 0.834 (+0.588)
    | 0.730 (+0.481) | 0.828 (+0.435) | 0.622 | 0.805 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag | 准确率 | 0.714 (+0.343) | 0.397 (+0.280) | 0.252 (+0.229) | 0.252
    (+0.140) | 0.252 (+0.051) | 0.741 (+0.360) | 0.736 (+0.472) | 0.834 (+0.588) |
    0.730 (+0.481) | 0.828 (+0.435) | 0.622 | 0.805 |'
- en: '| hellaswag_processed | rouge | 0.223 (+0.186) | 0.235 (+0.179) | 0.214 (+0.068)
    | 0.222 (+0.113) | 0.208 (+0.065) | 0.253 (+0.209) | 0.249 (+0.160) | 0.261 (+0.223)
    | 0.254 (+0.120) | 0.260 (+0.220) | 0.140 | 0.134 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag_processed | rouge | 0.223 (+0.186) | 0.235 (+0.179) | 0.214 (+0.068)
    | 0.222 (+0.113) | 0.208 (+0.065) | 0.253 (+0.209) | 0.249 (+0.160) | 0.261 (+0.223)
    | 0.254 (+0.120) | 0.260 (+0.220) | 0.140 | 0.134 |'
- en: '| jigsaw | accuracy | 0.824 (+0.333) | 0.852 (+0.362) | 0.845 (+0.363) | 0.824
    (+0.591) | 0.789 (+0.269) | 0.847 (+0.361) | 0.832 (+0.287) | 0.849 (+0.374) |
    0.867 (+0.163) | 0.866 (+0.394) | 0.735 | 0.754 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| jigsaw | 准确率 | 0.824 (+0.333) | 0.852 (+0.362) | 0.845 (+0.363) | 0.824 (+0.591)
    | 0.789 (+0.269) | 0.847 (+0.361) | 0.832 (+0.287) | 0.849 (+0.374) | 0.867 (+0.163)
    | 0.866 (+0.394) | 0.735 | 0.754 |'
- en: '| drop | rouge | 0.549 (+0.531) | 0.506 (+0.493) | 0.410 (+0.376) | 0.693 (+0.669)
    | 0.602 (+0.560) | 0.670 (+0.660) | 0.667 (+0.620) | 0.705 (+0.694) | 0.677 (+0.611)
    | 0.741 (+0.718) | 0.119 | 0.393 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| drop | rouge | 0.549 (+0.531) | 0.506 (+0.493) | 0.410 (+0.376) | 0.693 (+0.669)
    | 0.602 (+0.560) | 0.670 (+0.660) | 0.667 (+0.620) | 0.705 (+0.694) | 0.677 (+0.611)
    | 0.741 (+0.718) | 0.119 | 0.393 |'
- en: '| Math | gsm8k | accuracy | 0.441 (+0.358) | 0.258 (+0.232) | 0.240 (+0.158)
    | 0.569 (+0.530) | 0.505 (+0.141) | 0.339 (+0.288) | 0.323 (+0.163) | 0.520 (+0.406)
    | 0.488 (+0.213) | 0.503 (+0.370) | 0.622 | 0.373 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | gsm8k | 准确率 | 0.441 (+0.358) | 0.258 (+0.232) | 0.240 (+0.158) | 0.569
    (+0.530) | 0.505 (+0.141) | 0.339 (+0.288) | 0.323 (+0.163) | 0.520 (+0.406) |
    0.488 (+0.213) | 0.503 (+0.370) | 0.622 | 0.373 |'
- en: 'Table 13: Performance of 310 fine-tuned models across 10 base models and 31
    tasks. The value in parentheses is the absolute improvement compared to the base
    model. Fine-tuning scores were not obtained for GPT-3.5-Turbo or GPT-4.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 310个微调模型在10个基础模型和31个任务上的表现。括号中的值是与基础模型相比的绝对改进。GPT-3.5-Turbo 或 GPT-4 的微调分数未获得。'
- en: '| Task |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Max &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最大值 &#124;'
- en: '&#124; GPT-4 &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPT-4 &#124;'
- en: '&#124; Lift &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提升 &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Average &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值 &#124;'
- en: '&#124; Base &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础 &#124;'
- en: '&#124; Model &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '&#124; Lift &#124;'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提升 &#124;'
- en: '|'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Best &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最佳 &#124;'
- en: '&#124; Base &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础 &#124;'
- en: '&#124; Model &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '&#124; Score &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分数 &#124;'
- en: '|'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Average &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值 &#124;'
- en: '&#124; Base &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础 &#124;'
- en: '&#124; Model &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '&#124; Score &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分数 &#124;'
- en: '|'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Best &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最佳 &#124;'
- en: '&#124; Fine-tuned &#124;'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '&#124; Score &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分数 &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Average &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均值 &#124;'
- en: '&#124; Fine-Tuned &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '&#124; Score &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分数 &#124;'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '&#124; length &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; p95 &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; p95 &#124;'
- en: '|'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input &#124;'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '&#124; length &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\mu$ &#124;'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mu$ &#124;'
- en: '|'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Input &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '&#124; length &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\sigma$ &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sigma$ &#124;'
- en: '|'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Output &#124;'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出 &#124;'
- en: '&#124; length &#124;'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; p95 &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; p95 &#124;'
- en: '|'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Output &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出 &#124;'
- en: '&#124; length &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\mu$ &#124;'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mu$ &#124;'
- en: '|'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Output &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出 &#124;'
- en: '&#124; length &#124;'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\sigma$ &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sigma$ &#124;'
- en: '|'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Example &#124;'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 示例 &#124;'
- en: '&#124; length &#124;'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\mu$ &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mu$ &#124;'
- en: '|'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Example &#124;'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 示例 &#124;'
- en: '&#124; length &#124;'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; p95 &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; p95 &#124;'
- en: '|'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Example &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 示例 &#124;'
- en: '&#124; length &#124;'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度 &#124;'
- en: '&#124; $\sigma$ &#124;'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sigma$ &#124;'
- en: '|'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; I/O &#124;'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入/输出 &#124;'
- en: '&#124; rougeL &#124;'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; rougeL &#124;'
- en: '&#124; similarity &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似度 &#124;'
- en: '&#124; $\mu$ &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mu$ &#124;'
- en: '|'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; I/O &#124;'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入/输出 &#124;'
- en: '&#124; rougeL &#124;'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; rougeL &#124;'
- en: '&#124; similarity &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似度 &#124;'
- en: '&#124; $\sigma$ &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sigma$ &#124;'
- en: '|'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compr. &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 综合 &#124;'
- en: '&#124; $\mu$ &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mu$ &#124;'
- en: '|'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compr. &#124;'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 综合 &#124;'
- en: '&#124; $\sigma$ &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sigma$ &#124;'
- en: '|'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; # &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; # &#124;'
- en: '&#124; training &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; examples &#124;'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 示例 &#124;'
- en: '|'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| arc_combined | -0.032 | 0.320 | 0.673 | 0.411 | 0.915 | 0.731 | 143 | 102.89
    | 21.68 | 1 | 1.00 | 0.00 | 102.92 | 143.00 | 21.659 | 0.034 | 0.009 | 0.644 |
    0.064 | 3370 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| arc_combined | -0.032 | 0.320 | 0.673 | 0.411 | 0.915 | 0.731 | 143 | 102.89
    | 21.68 | 1 | 1.00 | 0.00 | 102.92 | 143.00 | 21.659 | 0.034 | 0.009 | 0.644 |
    0.064 | 3370 |'
- en: '| bc5cdr | 0.082 | 0.746 | 0.703 | 0.219 | 0.972 | 0.965 | 175 | 142.15 | 19.17
    | 58 | 37.11 | 11.27 | 178.26 | 226.05 | 27.839 | 0.191 | 0.026 | 0.547 | 0.014
    | 5228 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| bc5cdr | 0.082 | 0.746 | 0.703 | 0.219 | 0.972 | 0.965 | 175 | 142.15 | 19.17
    | 58 | 37.11 | 11.27 | 178.26 | 226.05 | 27.839 | 0.191 | 0.026 | 0.547 | 0.014
    | 5228 |'
- en: '| boolq | -0.002 | 0.188 | 0.764 | 0.635 | 0.909 | 0.823 | 270.7 | 145.23 |
    69.03 | 1 | 1.00 | 0.00 | 146.23 | 271.70 | 69.031 | 0.000 | 0.003 | 0.596 | 0.066
    | 9427 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| boolq | -0.002 | 0.188 | 0.764 | 0.635 | 0.909 | 0.823 | 270.7 | 145.23 |
    69.03 | 1 | 1.00 | 0.00 | 146.23 | 271.70 | 69.031 | 0.000 | 0.003 | 0.596 | 0.066
    | 9427 |'
- en: '| conllpp | 0.247 | 0.764 | 0.733 | 0.216 | 0.989 | 0.979 | 137 | 111.88 |
    13.17 | 38 | 24.88 | 7.58 | 135.76 | 170.00 | 18.647 | 0.126 | 0.031 | 0.583 |
    0.013 | 14041 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| conllpp | 0.247 | 0.764 | 0.733 | 0.216 | 0.989 | 0.979 | 137 | 111.88 |
    13.17 | 38 | 24.88 | 7.58 | 135.76 | 170.00 | 18.647 | 0.126 | 0.031 | 0.583 |
    0.013 | 14041 |'
- en: '| covid | 0.534 | 0.559 | 0.322 | 0.227 | 0.843 | 0.786 | 222 | 189.89 | 19.85
    | 3 | 1.58 | 0.91 | 190.18 | 223.00 | 19.910 | 0.020 | 0.007 | 0.570 | 0.012 |
    37361 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| covid | 0.534 | 0.559 | 0.322 | 0.227 | 0.843 | 0.786 | 222 | 189.89 | 19.85
    | 3 | 1.58 | 0.91 | 190.18 | 223.00 | 19.910 | 0.020 | 0.007 | 0.570 | 0.012 |
    37361 |'
- en: '| customer_support | 0.000 | 0.626 | 0.850 | 0.374 | 1.000 | 1.000 | 376 |
    274.02 | 57.26 | 3 | 2.13 | 0.34 | 275.15 | 377.00 | 57.160 | 0.023 | 0.007 |
    0.472 | 0.034 | 1245 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| customer_support | 0.000 | 0.626 | 0.850 | 0.374 | 1.000 | 1.000 | 376 |
    274.02 | 57.26 | 3 | 2.13 | 0.34 | 275.15 | 377.00 | 57.160 | 0.023 | 0.007 |
    0.472 | 0.034 | 1245 |'
- en: '| dbpedia | 0.023 | 0.739 | 0.868 | 0.232 | 0.988 | 0.971 | 210 | 162.20 |
    30.93 | 4 | 1.77 | 1.00 | 162.83 | 211.00 | 31.021 | 0.023 | 0.006 | 0.617 | 0.030
    | 560000 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| dbpedia | 0.023 | 0.739 | 0.868 | 0.232 | 0.988 | 0.971 | 210 | 162.20 |
    30.93 | 4 | 1.77 | 1.00 | 162.83 | 211.00 | 31.021 | 0.023 | 0.006 | 0.617 | 0.030
    | 560000 |'
- en: '| drop | 0.348 | 0.593 | 0.066 | 0.029 | 0.741 | 0.622 | 570 | 335.17 | 150.52
    | 5 | 2.05 | 1.58 | 337.16 | 571.00 | 150.431 | 0.009 | 0.012 | 0.518 | 0.039
    | 77400 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| drop | 0.348 | 0.593 | 0.066 | 0.029 | 0.741 | 0.622 | 570 | 335.17 | 150.52
    | 5 | 2.05 | 1.58 | 337.16 | 571.00 | 150.431 | 0.009 | 0.012 | 0.518 | 0.039
    | 77400 |'
- en: '| e2e_nlg | 0.039 | 0.295 | 0.482 | 0.247 | 0.552 | 0.543 | 116 | 104.18 |
    7.38 | 40 | 25.08 | 8.33 | 128.12 | 153.00 | 14.427 | 0.173 | 0.050 | 0.513 |
    0.018 | 42061 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| e2e_nlg | 0.039 | 0.295 | 0.482 | 0.247 | 0.552 | 0.543 | 116 | 104.18 |
    7.38 | 40 | 25.08 | 8.33 | 128.12 | 153.00 | 14.427 | 0.173 | 0.050 | 0.513 |
    0.018 | 42061 |'
- en: '| glue_cola | 0.008 | 0.237 | 0.797 | 0.573 | 0.872 | 0.809 | 58 | 50.34 |
    4.08 | 2 | 1.10 | 0.30 | 51.34 | 59.00 | 4.075 | 0.062 | 0.006 | 0.646 | 0.010
    | 8551 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| glue_cola | 0.008 | 0.237 | 0.797 | 0.573 | 0.872 | 0.809 | 58 | 50.34 |
    4.08 | 2 | 1.10 | 0.30 | 51.34 | 59.00 | 4.075 | 0.062 | 0.006 | 0.646 | 0.010
    | 8551 |'
- en: '| glue_mnli | 0.096 | 0.577 | 0.455 | 0.295 | 0.899 | 0.872 | 127 | 94.73 |
    18.76 | 1 | 1.00 | 0.00 | 95.73 | 128.00 | 18.763 | 0.031 | 0.007 | 0.558 | 0.023
    | 392702 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| glue_mnli | 0.096 | 0.577 | 0.455 | 0.295 | 0.899 | 0.872 | 127 | 94.73 |
    18.76 | 1 | 1.00 | 0.00 | 95.73 | 128.00 | 18.763 | 0.031 | 0.007 | 0.558 | 0.023
    | 392702 |'
- en: '| glue_mrpc | 0.110 | 0.202 | 0.694 | 0.629 | 0.887 | 0.831 | 122 | 100.78
    | 13.18 | 1 | 1.00 | 0.00 | 101.78 | 123.00 | 13.179 | 0.029 | 0.004 | 0.539 |
    0.038 | 3668 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| glue_mrpc | 0.110 | 0.202 | 0.694 | 0.629 | 0.887 | 0.831 | 122 | 100.78
    | 13.18 | 1 | 1.00 | 0.00 | 101.78 | 123.00 | 13.179 | 0.029 | 0.004 | 0.539 |
    0.038 | 3668 |'
- en: '| glue_qnli | 0.029 | 0.336 | 0.743 | 0.562 | 0.931 | 0.897 | 122 | 88.49 |
    18.44 | 1 | 1.04 | 0.20 | 89.49 | 123.00 | 18.444 | 0.032 | 0.006 | 0.621 | 0.030
    | 104743 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| glue_qnli | 0.029 | 0.336 | 0.743 | 0.562 | 0.931 | 0.897 | 122 | 88.49 |
    18.44 | 1 | 1.04 | 0.20 | 89.49 | 123.00 | 18.444 | 0.032 | 0.006 | 0.621 | 0.030
    | 104743 |'
- en: '| glue_qqp | 0.042 | 0.495 | 0.708 | 0.357 | 0.883 | 0.852 | 101 | 77.35 |
    12.61 | 2 | 1.49 | 0.50 | 78.35 | 102.00 | 12.612 | 0.038 | 0.006 | 0.603 | 0.030
    | 363846 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| glue_qqp | 0.042 | 0.495 | 0.708 | 0.357 | 0.883 | 0.852 | 101 | 77.35 |
    12.61 | 2 | 1.49 | 0.50 | 78.35 | 102.00 | 12.612 | 0.038 | 0.006 | 0.603 | 0.030
    | 363846 |'
- en: '| glue_sst2 | 0.019 | 0.423 | 0.933 | 0.524 | 0.961 | 0.946 | 62 | 42.33 |
    9.40 | 1 | 1.03 | 0.16 | 43.33 | 63.00 | 9.403 | 0.059 | 0.011 | 0.652 | 0.019
    | 67349 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| glue_sst2 | 0.019 | 0.423 | 0.933 | 0.524 | 0.961 | 0.946 | 62 | 42.33 |
    9.40 | 1 | 1.03 | 0.16 | 43.33 | 63.00 | 9.403 | 0.059 | 0.011 | 0.652 | 0.019
    | 67349 |'
- en: '| glue_stsb | 0.140 | 0.253 | 0.814 | 0.628 | 0.913 | 0.881 | 121 | 89.99 |
    13.95 | 4 | 3.16 | 0.37 | 92.99 | 124.00 | 13.946 | 0.038 | 0.025 | 0.576 | 0.027
    | 5749 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| glue_stsb | 0.140 | 0.253 | 0.814 | 0.628 | 0.913 | 0.881 | 121 | 89.99 |
    13.95 | 4 | 3.16 | 0.37 | 92.99 | 124.00 | 13.946 | 0.038 | 0.025 | 0.576 | 0.027
    | 5749 |'
- en: '| glue_wnli | -0.057 | 0.290 | 0.437 | 0.403 | 0.873 | 0.693 | 133 | 96.20
    | 17.81 | 2 | 1.17 | 0.38 | 97.20 | 134.00 | 17.809 | 0.030 | 0.005 | 0.560 |
    0.031 | 635 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| glue_wnli | -0.057 | 0.290 | 0.437 | 0.403 | 0.873 | 0.693 | 133 | 96.20
    | 17.81 | 2 | 1.17 | 0.38 | 97.20 | 134.00 | 17.809 | 0.030 | 0.005 | 0.560 |
    0.031 | 635 |'
- en: '| gsm8k | 0.196 | 0.286 | 0.364 | 0.133 | 0.569 | 0.419 | 106 | 65.30 | 21.13
    | 186 | 100.70 | 43.79 | 165.77 | 276.00 | 57.679 | 0.272 | 0.081 | 0.545 | 0.073
    | 7473 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| gsm8k | 0.196 | 0.286 | 0.364 | 0.133 | 0.569 | 0.419 | 106 | 65.30 | 21.13
    | 186 | 100.70 | 43.79 | 165.77 | 276.00 | 57.679 | 0.272 | 0.081 | 0.545 | 0.073
    | 7473 |'
- en: '| hellaswag | 0.029 | 0.338 | 0.393 | 0.236 | 0.834 | 0.574 | 339 | 253.99
    | 71.38 | 3 | 2.66 | 0.75 | 256.48 | 341.00 | 71.366 | 0.009 | 0.006 | 0.524 |
    0.027 | 39905 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag | 0.029 | 0.338 | 0.393 | 0.236 | 0.834 | 0.574 | 339 | 253.99
    | 71.38 | 3 | 2.66 | 0.75 | 256.48 | 341.00 | 71.366 | 0.009 | 0.006 | 0.524 |
    0.027 | 39905 |'
- en: '| hellaswag_processed | 0.127 | 0.154 | 0.146 | 0.084 | 0.261 | 0.238 | 142
    | 111.15 | 20.86 | 56 | 30.85 | 15.46 | 140.97 | 185.00 | 33.774 | 0.111 | 0.040
    | 0.564 | 0.023 | 39905 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| hellaswag_processed | 0.127 | 0.154 | 0.146 | 0.084 | 0.261 | 0.238 | 142
    | 111.15 | 20.86 | 56 | 30.85 | 15.46 | 140.97 | 185.00 | 33.774 | 0.111 | 0.040
    | 0.564 | 0.023 | 39905 |'
- en: '| jigsaw | 0.113 | 0.350 | 0.704 | 0.490 | 0.867 | 0.839 | 600 | 475.45 | 58.46
    | 1 | 1.00 | 0.00 | 476.45 | 601.00 | 58.457 | 0.006 | 0.001 | 0.486 | 0.006 |
    159571 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| jigsaw | 0.113 | 0.350 | 0.704 | 0.490 | 0.867 | 0.839 | 600 | 475.45 | 58.46
    | 1 | 1.00 | 0.00 | 476.45 | 601.00 | 58.457 | 0.006 | 0.001 | 0.486 | 0.006 |
    159571 |'
- en: '| legal | 0.378 | 0.539 | 0.158 | 0.036 | 0.683 | 0.575 | 485.05 | 246.96 |
    107.88 | 6 | 2.92 | 1.73 | 249.88 | 489.00 | 107.919 | 0.012 | 0.013 | 0.499 |
    0.040 | 17000 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| legal | 0.378 | 0.539 | 0.158 | 0.036 | 0.683 | 0.575 | 485.05 | 246.96 |
    107.88 | 6 | 2.92 | 1.73 | 249.88 | 489.00 | 107.919 | 0.012 | 0.013 | 0.499 |
    0.040 | 17000 |'
- en: '| magicoder | -0.396 | 0.198 | 0.201 | 0.067 | 0.433 | 0.264 | 473 | 305.39
    | 91.88 | 436 | 231.40 | 110.12 | 535.80 | 805.00 | 151.769 | 0.253 | 0.089 |
    0.366 | 0.046 | 75197 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| magicoder | -0.396 | 0.198 | 0.201 | 0.067 | 0.433 | 0.264 | 473 | 305.39
    | 91.88 | 436 | 231.40 | 110.12 | 535.80 | 805.00 | 151.769 | 0.253 | 0.089 |
    0.366 | 0.046 | 75197 |'
- en: '| mmlu | -0.185 | 0.122 | 0.506 | 0.343 | 0.589 | 0.465 | 577 | 377.20 | 153.00
    | 1 | 1.00 | 0.00 | 378.20 | 578.00 | 152.998 | 0.010 | 0.012 | 0.526 | 0.076
    | 99842 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| mmlu | -0.185 | 0.122 | 0.506 | 0.343 | 0.589 | 0.465 | 577 | 377.20 | 153.00
    | 1 | 1.00 | 0.00 | 378.20 | 578.00 | 152.998 | 0.010 | 0.012 | 0.526 | 0.076
    | 99842 |'
- en: '| reuters | 0.465 | 0.428 | 0.010 | 0.006 | 0.479 | 0.434 | 635 | 239.80 |
    186.43 | 8 | 2.99 | 3.18 | 242.24 | 637.05 | 187.038 | 0.003 | 0.008 | 0.625 |
    0.087 | 13625 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| reuters | 0.465 | 0.428 | 0.010 | 0.006 | 0.479 | 0.434 | 635 | 239.80 |
    186.43 | 8 | 2.99 | 3.18 | 242.24 | 637.05 | 187.038 | 0.003 | 0.008 | 0.625 |
    0.087 | 13625 |'
- en: '| tldr_content_gen | 0.105 | 0.066 | 0.183 | 0.148 | 0.230 | 0.214 | 53 | 44.38
    | 5.97 | 159 | 95.09 | 36.51 | 138.33 | 204.45 | 38.846 | 0.128 | 0.040 | 0.576
    | 0.037 | 7138 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| tldr_content_gen | 0.105 | 0.066 | 0.183 | 0.148 | 0.230 | 0.214 | 53 | 44.38
    | 5.97 | 159 | 95.09 | 36.51 | 138.33 | 204.45 | 38.846 | 0.128 | 0.040 | 0.576
    | 0.037 | 7138 |'
- en: '| tldr_headline_gen | 0.266 | 0.289 | 0.174 | 0.119 | 0.441 | 0.407 | 184 |
    120.96 | 36.50 | 22 | 13.53 | 5.98 | 133.34 | 199.45 | 38.845 | 0.086 | 0.050
    | 0.588 | 0.041 | 7138 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| tldr_headline_gen | 0.266 | 0.289 | 0.174 | 0.119 | 0.441 | 0.407 | 184 |
    120.96 | 36.50 | 22 | 13.53 | 5.98 | 133.34 | 199.45 | 38.845 | 0.086 | 0.050
    | 0.588 | 0.041 | 7138 |'
- en: '| viggo | 0.131 | 0.275 | 0.374 | 0.201 | 0.505 | 0.476 | 193 | 169.05 | 13.10
    | 49 | 27.68 | 11.54 | 196.48 | 240.00 | 23.486 | 0.112 | 0.042 | 0.512 | 0.016
    | 5103 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| viggo | 0.131 | 0.275 | 0.374 | 0.201 | 0.505 | 0.476 | 193 | 169.05 | 13.10
    | 49 | 27.68 | 11.54 | 196.48 | 240.00 | 23.486 | 0.112 | 0.042 | 0.512 | 0.016
    | 5103 |'
- en: '| webnlg | 0.098 | 0.359 | 0.563 | 0.305 | 0.681 | 0.664 | 176 | 125.11 | 27.61
    | 51 | 20.85 | 17.15 | 145.67 | 215.05 | 37.522 | 0.129 | 0.092 | 0.530 | 0.033
    | 13211 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| webnlg | 0.098 | 0.359 | 0.563 | 0.305 | 0.681 | 0.664 | 176 | 125.11 | 27.61
    | 51 | 20.85 | 17.15 | 145.67 | 215.05 | 37.522 | 0.129 | 0.092 | 0.530 | 0.033
    | 13211 |'
- en: '| wikisql | -0.011 | 0.688 | 0.301 | 0.137 | 0.898 | 0.825 | 1921 | 805.07
    | 1668.51 | 26 | 15.60 | 5.72 | 819.66 | 1941.10 | 1669.119 | 0.050 | 0.030 |
    0.387 | 0.080 | 56355 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| wikisql | -0.011 | 0.688 | 0.301 | 0.137 | 0.898 | 0.825 | 1921 | 805.07
    | 1668.51 | 26 | 15.60 | 5.72 | 819.66 | 1941.10 | 1669.119 | 0.050 | 0.030 |
    0.387 | 0.080 | 56355 |'
- en: '| winogrande | 0.008 | 0.168 | 0.576 | 0.476 | 0.840 | 0.644 | 62 | 54.32 |
    4.02 | 1 | 1.00 | 0.00 | 55.32 | 63 | 4.017 | 0.052 | 0.004 | 0.748 | 0.024 |
    9248 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| winogrande | 0.008 | 0.168 | 0.576 | 0.476 | 0.840 | 0.644 | 62 | 54.32 |
    4.02 | 1 | 1.00 | 0.00 | 55.32 | 63 | 4.017 | 0.052 | 0.004 | 0.748 | 0.024 |
    9248 |'
- en: 'Table 14: Task and Dataset complexity heuristics and model quality measurements,
    across all tasks.'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：所有任务中的任务和数据集复杂性启示及模型质量测量。
- en: Appendix D Other
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 其他
- en: '![Refer to caption](img/377e6bf64290e3dde9500191f965eefc.png)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/377e6bf64290e3dde9500191f965eefc.png)'
- en: 'Figure 10: An esoteric visual representation of 310 fine-tuned LLMs.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：310 个微调 LLM 的深奥可视化表示。
