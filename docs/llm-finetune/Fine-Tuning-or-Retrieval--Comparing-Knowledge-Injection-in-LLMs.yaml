- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调还是检索？比较 LLMs 中的知识注入
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.05934](https://ar5iv.labs.arxiv.org/html/2312.05934)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.05934](https://ar5iv.labs.arxiv.org/html/2312.05934)
- en: Oded Ovadia Corresponding author.Equal contribution. {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Oded Ovadia 通讯作者。等贡献。{odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
- en: 'Microsoft, Israel Menachem Brief⁰⁰footnotemark: 0 {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '微软，以色列 Menachem Brief⁰⁰footnotemark: 0 {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com'
- en: Microsoft, Israel Moshik Mishaeli {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软，以色列 Moshik Mishaeli {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
- en: Microsoft, Israel Oren Elisha {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微软，以色列 Oren Elisha {odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
- en: Microsoft, Israel
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 微软，以色列
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) encapsulate a vast amount of factual information
    within their pre-trained weights, as evidenced by their ability to answer diverse
    questions across different domains. However, this knowledge is inherently limited,
    relying heavily on the characteristics of the training data. Consequently, using
    external datasets to incorporate new information or refine the capabilities of
    LLMs on previously seen information poses a significant challenge. In this study,
    we compare two common approaches: unsupervised fine-tuning and retrieval-augmented
    generation (RAG). We evaluate both approaches on a variety of knowledge-intensive
    tasks across different topics. Our findings reveal that while unsupervised fine-tuning
    offers some improvement, RAG consistently outperforms it, both for existing knowledge
    encountered during training and entirely new knowledge. Moreover, we find that
    LLMs struggle to learn new factual information through unsupervised fine-tuning,
    and that exposing them to numerous variations of the same fact during training
    could alleviate this problem.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在其预训练权重中包含了大量的事实信息，这从它们能够回答不同领域的各种问题中可以得到证实。然而，这些知识本质上是有限的，严重依赖于训练数据的特征。因此，使用外部数据集来整合新信息或在之前见过的信息上提升LLMs的能力是一个巨大的挑战。在本研究中，我们比较了两种常见的方法：无监督微调和检索增强生成（RAG）。我们在各种知识密集型任务中评估了这两种方法，涵盖了不同的主题。我们的发现表明，虽然无监督微调提供了一些改进，但RAG始终优于前者，无论是对训练中遇到的现有知识还是对全新知识。此外，我们发现LLMs
    在通过无监督微调学习新事实信息方面存在困难，暴露于训练中的相同事实的多种变体可能会缓解这一问题。
- en: 'Keywords: LLMs, NLP, Fine-Tuning vs. RAG, Knowledge and Factuality.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：LLMs, NLP, 微调与RAG, 知识与事实性。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) are able to capture vast amounts of factual information
    (Petroni et al., [2019](#bib.bib32); Cohen et al., [2023](#bib.bib8); Hu et al.,
    [2023](#bib.bib13)). LLMs exhibit a remarkable level of knowledge in various domains
    due to their massive pre-training datasets. However, there are two significant
    limitations to this knowledge. First, it is static and does not update with time.
    Second, it is non-specific and thus may lack nuanced expertise in particular domains.
    While these are two different problems, they are deeply related since their solution
    is the same: enhancing the model’s knowledge.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）能够捕捉大量的事实信息（Petroni et al., [2019](#bib.bib32); Cohen et al., [2023](#bib.bib8);
    Hu et al., [2023](#bib.bib13)）。LLMs 在各个领域展现了显著的知识水平，这得益于其庞大的预训练数据集。然而，这些知识存在两个重大限制。首先，它是静态的，无法随着时间的推移进行更新。其次，它是非特定的，因此可能在某些领域缺乏细致的专业知识。虽然这两个问题不同，但它们紧密相关，因为它们的解决方案是相同的：增强模型的知识。
- en: Recently, the idea of adapting LLMs to particular domains and updating their
    knowledge has become increasingly common (Yu et al., [2022](#bib.bib54)). Various
    models have been suggested to improve factual knowledge and capabilities in diverse
    fields such as healthcare (Singhal et al., [2023a](#bib.bib39), [b](#bib.bib40);
    Wu et al., [2023a](#bib.bib50)), finance (Wu et al., [2023b](#bib.bib51); Yang
    et al., [2023](#bib.bib53)), and law (Huang et al., [2023](#bib.bib14); Nguyen,
    [2023](#bib.bib28)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，针对特定领域调整LLMs并更新其知识的想法变得越来越普遍（Yu et al., [2022](#bib.bib54)）。已经提出了各种模型来改善在医疗（Singhal
    et al., [2023a](#bib.bib39), [b](#bib.bib40); Wu et al., [2023a](#bib.bib50)）、金融（Wu
    et al., [2023b](#bib.bib51); Yang et al., [2023](#bib.bib53)）和法律（Huang et al.,
    [2023](#bib.bib14); Nguyen, [2023](#bib.bib28)）等领域的事实知识和能力。
- en: In this work, we focus on the evaluation of a model’s knowledge and its ability
    to memorize, understand, and retrieve factual data. We aim to understand the concept
    of knowledge injection (Wang et al., [2020](#bib.bib48); Chen et al., [2022](#bib.bib4);
    Liu et al., [2020](#bib.bib22); Lauscher et al., [2020](#bib.bib20)). Given some
    knowledge base in the form of a text corpus, what is the best way to teach a pre-trained
    model this knowledge?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于评估模型的知识及其记忆、理解和检索事实数据的能力。我们的目标是理解知识注入的概念（Wang et al., [2020](#bib.bib48);
    Chen et al., [2022](#bib.bib4); Liu et al., [2020](#bib.bib22); Lauscher et al.,
    [2020](#bib.bib20)）。在某种形式的文本语料库中给定一些知识库，教给预训练模型这些知识的最佳方式是什么？
- en: One way to add knowledge to a pre-trained model is through fine-tuning. With
    fine-tuning, we continue the model’s training process and adapt it using task-specific
    data. By exposing the model to a specific knowledge base, we expect the model
    weights to adapt accordingly. This process is meant to optimize the model for
    targeted applications, enhancing its performance and contextual relevance in specialized
    domains.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 向预训练模型添加知识的一种方法是通过微调。通过微调，我们继续模型的训练过程，并使用特定任务的数据对其进行调整。通过让模型接触特定的知识库，我们期望模型权重相应地调整。这个过程旨在优化模型以适应特定应用，提高其在专业领域的表现和上下文相关性。
- en: Another method to enhance a model’s knowledge base is through the use of in-context
    learning (ICL) (Chen et al., [2021](#bib.bib5); Radford et al., [2019](#bib.bib33);
    Min et al., [2021](#bib.bib24); Lampinen et al., [2022](#bib.bib19)). The main
    idea behind ICL is to improve the performance of pre-trained LLMs on new tasks
    by modifying the input query to the model without directly changing the weights
    of the model. One form of ICL is retrieval augmented generation (RAG) (Lewis et al.,
    [2020](#bib.bib21); Neelakantan et al., [2022](#bib.bib27)). RAG uses information
    retrieval techniques to enable LLMs to obtain relevant information from a knowledge
    source and incorporate it into generated text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 增强模型知识库的另一种方法是通过使用上下文学习（ICL）（Chen et al., [2021](#bib.bib5); Radford et al.,
    [2019](#bib.bib33); Min et al., [2021](#bib.bib24); Lampinen et al., [2022](#bib.bib19)）。ICL的主要思想是通过修改模型的输入查询来提高预训练LLMs在新任务上的表现，而不直接改变模型的权重。ICL的一种形式是检索增强生成（RAG）（Lewis
    et al., [2020](#bib.bib21); Neelakantan et al., [2022](#bib.bib27)）。RAG使用信息检索技术，使LLMs能够从知识源中获取相关信息并将其融入生成的文本中。
- en: This study aims to evaluate the knowledge injection capabilities of LLMs through
    a comparison of fine-tuning and RAG. To illustrate the rationale, let us use an
    analogy. Consider three college students taking a test on a specific topic. All
    had access to class materials but didn’t know the topic beforehand. The first
    student had the textbook only during the test, the second had pre-test access
    and studied, and the third lost access upon the test announcement. Who would probably
    perform better?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究旨在通过比较微调和RAG来评估LLMs的知识注入能力。为了说明其理由，让我们用一个类比。假设三名大学生在某一特定主题上进行考试。他们都能访问课堂材料，但事先不知道这个主题。第一名学生在考试期间只有教科书，第二名学生在考试前获得了材料并进行了学习，而第三名学生在考试公告后失去了访问权限。谁可能表现更好？
- en: 2 Background
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/ac4610debc6f6fb498143dc0241a92ae.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac4610debc6f6fb498143dc0241a92ae.png)'
- en: 'Figure 1: A visualization of the knowledge injection framework.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：知识注入框架的可视化。
- en: To assess knowledge injection, we must first understand what knowledge means
    for LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估知识注入，我们必须首先理解知识对大型语言模型（LLMs）意味着什么。
- en: Knowledge and Language Models  Defining knowledge is a complex philosophical
    task far beyond the scope of this research. However, we can examine what factual
    knowledge means in the context of language models. If a model knows a fact, it
    can accurately and consistently answer questions about it. Furthermore, it can
    reliably distinguish between true and false statements related to this fact. We
    can then extend this definition to a whole knowledge base, not just a single fact.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 知识和语言模型  定义知识是一个复杂的哲学任务，远超出本研究的范围。然而，我们可以在语言模型的背景下探讨什么是事实知识。如果一个模型知道一个事实，它可以准确且一致地回答有关该事实的问题。此外，它还可以可靠地区分与该事实相关的真实和虚假陈述。然后，我们可以将这个定义扩展到整个知识库，而不仅仅是一个单一的事实。
- en: Mathematically, let $\mathcal{Q}=\{q_{n}\}_{n=1}^{N}$ be the corresponding set
    of possible answers, and $\mathcal{C}=\{c_{n}\}_{n=1}^{N}$ be the correct answers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，设$\mathcal{Q}=\{q_{n}\}_{n=1}^{N}$为可能答案的对应集合，$\mathcal{C}=\{c_{n}\}_{n=1}^{N}$为正确答案。
- en: Let $\mathcal{M}$-th question.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑第$\mathcal{M}$-个问题。
- en: 'We define the knowledge score $\mathcal{L}$ to be the standard accuracy score:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义知识得分$\mathcal{L}$为标准准确度得分：
- en: '|  | $\mathcal{L}_{\mathcal{M},\mathcal{Q}}:=\frac{\#\{q_{n}&#124;\;\mathcal{M}(q_{n})=c_{n}\}}{N}.$
    |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathcal{M},\mathcal{Q}}:=\frac{\#\{q_{n}&#124;\;\mathcal{M}(q_{n})=c_{n}\}}{N}.$
    |  | (1) |'
- en: 'We say that the model $\mathcal{M}$ if the following holds:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说模型$\mathcal{M}$如果以下条件成立：
- en: '|  | $$\mathcal{L}_{\mathcal{M},\mathcal{Q}}> |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\mathcal{L}_{\mathcal{M},\mathcal{Q}}> |  | (2) |'
- en: In simpler terms, the model can consistently give correct answers, outperforming
    a simple random guessing baseline. Naturally, if the knowledge score $\mathcal{L}_{\mathcal{M},\mathcal{Q}}$
    compared to the latter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，模型可以始终给出正确的答案，表现优于简单的随机猜测基准。自然地，如果知识得分$\mathcal{L}_{\mathcal{M},\mathcal{Q}}$与后者相比。
- en: Previously Seen Knowledge  One important distinction to make is between knowledge
    that the model has been exposed to before during pre-training as opposed to entirely
    new facts. Considering the size of modern LLM training sets, they cover a vast
    amount of information available through web-sourced text. As a result, even in
    niche domains, the goal of knowledge injection is not necessarily to teach the
    model entirely new facts but rather to ”refresh” its memory by inducing a bias
    toward a particular domain.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以前见过的知识  一个重要的区分是模型在预训练期间接触过的知识与完全新的事实之间的区别。考虑到现代大规模语言模型（LLM）训练集的大小，它们涵盖了通过网络来源文本获得的大量信息。因此，即使在小众领域，知识注入的目标也不一定是教会模型完全新的事实，而是通过引入对特定领域的偏向来“刷新”其记忆。
- en: Knowledge and Reasoning  We emphasize that this knowledge evaluation framework
    for LLMs is imperfect. Importantly, it doesn’t address other quality metrics influencing
    a model’s response. Creating a purely knowledge-intensive dataset without involving
    some level of reasoning is challenging. Consequently, a model with robust reasoning
    abilities might excel on unfamiliar knowledge-intensive tasks by making ”educated
    guesses” in a multiple-choice exam. Therefore, any evaluation of knowledge in
    LLMs should consider this, with results seen as part of a broader range of benchmarks
    for reasoning (Sakaguchi et al., [2021](#bib.bib35)), reading comprehension (Dua
    et al., [2019](#bib.bib9)), and general language abilities (Srivastava et al.,
    [2022](#bib.bib41)). However, this evaluation framework still strongly emphasizes
    factual information above all else.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 知识与推理  我们强调，这种针对LLMs的知识评估框架是不完美的。重要的是，它没有考虑影响模型响应的其他质量指标。创建一个纯粹以知识为主的 dataset
    而不涉及某种程度的推理是具有挑战性的。因此，具有强大推理能力的模型可能在不熟悉的知识密集型任务中表现出色，通过在多项选择考试中进行“有根据的猜测”来取得好成绩。因此，对LLMs的任何知识评估都应考虑到这一点，结果应视为推理（Sakaguchi
    et al., [2021](#bib.bib35)）、阅读理解（Dua et al., [2019](#bib.bib9)）和一般语言能力（Srivastava
    et al., [2022](#bib.bib41)）等更广泛基准的一部分。然而，这一评估框架仍然强烈强调事实信息。
- en: 'Causes for Factual Errors  There are many possible reasons for the failure
    of models to answer factual questions accurately. In (Wang et al., [2023](#bib.bib47)),
    Wang et al. introduce a taxonomy of five main model-level causes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 事实错误的原因  模型无法准确回答事实性问题的原因有很多。在(Wang et al., [2023](#bib.bib47))中，Wang等人介绍了五种主要的模型级原因分类：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain knowledge deficit: A language model may lack comprehensive expertise
    in a specific domain to which it has not been exposed. For example, a model trained
    exclusively on texts written by William Shakespeare would perform poorly when
    asked about the works of Mark Twain.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域知识不足：一个语言模型可能在某个特定领域缺乏全面的专业知识，因为它没有接触过该领域。例如，一个只在威廉·莎士比亚的文本上训练的模型在被问及马克·吐温的作品时表现不佳。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Outdated Information: LLMs invariably have a cutoff date determined by their
    training dataset. Consequently, any events, discoveries, or changes occurring
    after the last training update will not be within the model’s knowledge without
    access to external sources.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过时信息：LLMs不可避免地有一个由其训练数据集确定的截止日期。因此，任何在最后一次训练更新之后发生的事件、发现或变化都不会在模型的知识范围内，除非访问外部来源。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Immemorization: Sometimes, a model is exposed to knowledge during its training
    process but does not retain it. This is especially true for rare facts that appear
    in the training dataset only scarcely (Kandpal et al., [2023](#bib.bib17)).'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 忘记：有时，模型在训练过程中接触到知识但没有保留它。这在训练数据集中出现的稀有事实中尤其如此（Kandpal et al., [2023](#bib.bib17)）。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Forgetting: Language models often undergo additional training after the pre-training
    phase (fine-tuning). In some cases, this might lead to a phenomenon called catastrophic
    forgetting (Kirkpatrick et al., [2017](#bib.bib18); Goodfellow et al., [2013](#bib.bib11);
    Chen et al., [2020](#bib.bib3); Luo et al., [2023](#bib.bib23)), where models
    lose some of the knowledge they had prior to the fine-tuning process.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遗忘：语言模型在预训练阶段之后通常会进行额外训练（微调）。在某些情况下，这可能导致一种称为灾难性遗忘的现象（Kirkpatrick et al., [2017](#bib.bib18);
    Goodfellow et al., [2013](#bib.bib11); Chen et al., [2020](#bib.bib3); Luo et
    al., [2023](#bib.bib23)），即模型会丧失在微调过程之前获得的一部分知识。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning Failure: In certain instances, a language model might possess relevant
    knowledge about a fact but fail to utilize it properly. This is particularly evident
    in complex multi-step reasoning tasks (Tan et al., [2023](#bib.bib42)) or when
    posed with different questions about the same fact, resulting in disparate outcomes
    (Berglund et al., [2023](#bib.bib2)).'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理失败：在某些情况下，语言模型可能掌握有关某个事实的相关知识，但未能正确利用它。这在复杂的多步骤推理任务（Tan et al., [2023](#bib.bib42)）中尤为明显，或者当面对关于同一事实的不同问题时，可能会产生不同的结果（Berglund
    et al., [2023](#bib.bib2)）。
- en: We observe that most of these issues arise during the pre-training phase, with
    catastrophic forgetting being the notable exception. Hence, many LLMs will suffer
    from factual errors of this kind regardless of any post-training process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，大多数这些问题发生在预训练阶段，灾难性遗忘是一个显著的例外。因此，无论后续训练过程如何，许多大型语言模型都会遭遇这种事实错误。
- en: 3 Injecting Knowledge to Language Models
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 将知识注入语言模型
- en: Following the background given in [Section 2](#S2 "2 Background ‣ Fine-Tuning
    or Retrieval? Comparing Knowledge Injection in LLMs"), it is clear that general
    pre-training is insufficient for many knowledge-intensive tasks. To solve this,
    an additional post-processing step is essential to augment the knowledge of a
    pre-trained model. This step is often reffered to as knowledge injection (Wang
    et al., [2020](#bib.bib48); Chen et al., [2022](#bib.bib4); Liu et al., [2020](#bib.bib22);
    Lauscher et al., [2020](#bib.bib20)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [第2节](#S2 "2 背景 ‣ 微调还是检索？比较 LLM 中的知识注入") 中提供的背景，很明显一般预训练对于许多知识密集型任务是不够的。为了解决这个问题，额外的后处理步骤对于增强预训练模型的知识是必不可少的。这个步骤通常被称为知识注入（Wang
    et al., [2020](#bib.bib48); Chen et al., [2022](#bib.bib4); Liu et al., [2020](#bib.bib22);
    Lauscher et al., [2020](#bib.bib20)）。
- en: 'In this section, we examine two widely used frameworks for knowledge injection:
    fine-tuning (FT) and retrieval augmented generation (RAG). We begin by formulating
    the knowledge injection problem, aiming to explain both methods using consistent
    terminology.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了两种广泛使用的知识注入框架：微调（FT）和检索增强生成（RAG）。我们首先制定知识注入问题的表述，旨在使用一致的术语解释这两种方法。
- en: 3.1 Problem formulation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: In [Equations 1](#S2.E1 "In 2 Background ‣ Fine-Tuning or Retrieval? Comparing
    Knowledge Injection in LLMs") and [2](#S2.E2 "Equation 2 ‣ 2 Background ‣ Fine-Tuning
    or Retrieval? Comparing Knowledge Injection in LLMs"), we presented a formulation
    for knowledge in language models through the lens of question-answering (Q&A).
    We now extend this formulation to the problem of knowledge injection using the
    same terminology.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [方程 1](#S2.E1 "在 2 背景 ‣ 微调还是检索？比较 LLM 中的知识注入") 和 [2](#S2.E2 "方程 2 ‣ 2 背景 ‣
    微调还是检索？比较 LLM 中的知识注入") 中，我们通过问答（Q&A）的视角展示了语言模型中知识的表述。我们现在扩展这一表述到使用相同术语的知识注入问题上。
- en: Given a set of factual questions, there exists some text corpus containing information
    that is relevant to these questions. The central assumption of knowledge injection
    is that given full access to this corpus, it could serve as an auxiliary knowledge
    base and improve the model’s performance on this set of questions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组事实性问题，存在某些文本语料库包含与这些问题相关的信息。知识注入的核心假设是，给定对这个语料库的完全访问权限，它可以作为一个辅助知识库，并提高模型在这一组问题上的表现。
- en: 'Mathematically, let $\mathcal{M}$, that, when applied, would enhance the knowledge
    about $\mathcal{Q}$:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，设定 $\mathcal{M}$，它在应用时将增强对 $\mathcal{Q}$ 的知识：
- en: '|  | , to demarcate the original chunks’ beginnings and ends
    to preserve the documents’ structure.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 训练设置  我们使用 [第 3.2 节](#S3.SS2 "3.2 微调 ‣ 3 向语言模型注入知识 ‣ 微调还是检索？比较 LLMs 中的知识注入")
    中描述的无监督训练过程训练了所有模型。对于每个数据集，我们将辅助知识库划分为 $256$EOS$$> 的大小相等的块，以标记原始块的开始和结束，从而保留文档的结构。
- en: The models were trained using learning rates between $1\times{10}^{-6}$, which
    were found through a hyperparameter search. All models were trained on 4 NVIDIA
    A-100 GPUs for a maximum of 5 epochs and a batch size of 64.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 模型使用 $1\times{10}^{-6}$ 之间的学习率进行训练，这些学习率是通过超参数搜索找到的。所有模型均在 4 个 NVIDIA A-100
    GPU 上训练，最多进行 5 个周期，批次大小为 64。
- en: 'Evaluation method  All evaluations were done by appending each of the multiple-choice
    options to the question, followed by passing the concatenation through the model
    to get a log probability score per option. The highest score was interpreted as
    the model’s choice and used for accuracy calculation. More formally, this means
    that in [Equation 1](#S2.E1 "In 2 Background ‣ Fine-Tuning or Retrieval? Comparing
    Knowledge Injection in LLMs") we say that $\mathcal{M}(q_{n})=c_{n}$ if:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法  所有评估都是通过将每个多选项附加到问题上，然后将拼接后的内容通过模型以获取每个选项的对数概率分数。最高的分数被解释为模型的选择，并用于计算准确性。更正式地说，这意味着在 [公式 1](#S2.E1
    "在 2 背景 ‣ 微调还是检索？比较 LLMs 中的知识注入")中，我们说如果$\mathcal{M}(q_{n})=c_{n}$：
- en: '|  | $c_{n}=\operatorname*{arg\,max}_{l}\{\mathcal{M}(q_{n}\&#124;a^{1}_{n}),\ldots,\mathcal{M}(q_{n}\&#124;a^{L}_{n})\},$
    |  | (4) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{n}=\operatorname*{arg\,max}_{l}\{\mathcal{M}(q_{n}\&#124;a^{1}_{n}),\ldots,\mathcal{M}(q_{n}\&#124;a^{L}_{n})\},$
    |  | (4) |'
- en: where $\mathcal{M}(q_{n}\|a^{l}_{n})=\log P_{\mathcal{M}}(q_{n}\|a^{l}_{n})$.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}(q_{n}\|a^{l}_{n})=\log P_{\mathcal{M}}(q_{n}\|a^{l}_{n})$。
- en: 'MMLU Results  For each task and model, we compared four approaches: using just
    the base model, RAG, FT, and finally combining FT and RAG by using the fine-tuned
    model as the generator. Furthermore, we tested the MMLU tasks using both 0-shot
    and 5-shot scenarios. The full results are shown in  [Table 1](#S4.T1 "In 4 Knowledge
    Base Creation ‣ Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs").
    An aggregation of the relative accuracy gain, i.e.,'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU 结果  对于每个任务和模型，我们比较了四种方法：仅使用基础模型、RAG、FT，最后将 FT 和 RAG 结合起来，使用微调模型作为生成器。此外，我们测试了
    MMLU 任务的 0-shot 和 5-shot 场景。完整结果如 [表 1](#S4.T1 "在 4 知识库创建 ‣ 微调还是检索？比较 LLMs 中的知识注入")
    中所示。相对准确性增益的汇总，即，
- en: '|  | $(\mathcal{L}_{\mathcal{M^{\prime}},\mathcal{Q}}-\mathcal{L}_{\mathcal{M},\mathcal{Q}})/{\mathcal{L}_{\mathcal{M},\mathcal{Q}}},$
    |  | (5) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\mathcal{L}_{\mathcal{M^{\prime}},\mathcal{Q}}-\mathcal{L}_{\mathcal{M},\mathcal{Q}})/{\mathcal{L}_{\mathcal{M},\mathcal{Q}}},$
    |  | (5) |'
- en: where $\mathcal{M}$ is the knowledge-injected model, is shown in  [Figure 2](#S5.F2
    "In 5 Experiments and Results ‣ Fine-Tuning or Retrieval? Comparing Knowledge
    Injection in LLMs").
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}$ 是知识注入模型，如在 [图 2](#S5.F2 "在 5 实验和结果 ‣ 微调还是检索？比较 LLMs 中的知识注入")中所示。
- en: In all cases, RAG performed significantly better compared to the base models.
    Furthermore, using RAG with the base model as the generator was consistently better
    than only fine-tuning. In some cases, using the fine-tuned model instead of the
    base model as the generator in the RAG pipeline improved results even further.
    However, this is not consistent and thus demonstrates the inherent instability
    of fine-tuning. Additionally, we found that the 5-shot approach boosts the results
    by a small margin in most cases, with a similar trend being observed in all of
    the different approaches.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，与基础模型相比，RAG表现显著更好。此外，使用 RAG 作为生成器的基础模型始终比仅进行微调效果更佳。在某些情况下，使用经过微调的模型而不是基础模型作为
    RAG 流水线中的生成器，进一步改善了结果。然而，这并不一致，因此展示了微调的固有不稳定性。此外，我们发现5-shot 方法在大多数情况下略微提高了结果，所有不同方法中都观察到了类似的趋势。
- en: Current Events Results  The evaluation on the current events task is shown in [Table 2](#S4.T2
    "In 4 Knowledge Base Creation ‣ Fine-Tuning or Retrieval? Comparing Knowledge
    Injection in LLMs"). RAG proves particularly effective due to the one-to-one correspondence
    between the questions and the auxiliary dataset (see [Section 4.3](#S4.SS3 "4.3
    Current Events Task Creation ‣ 4 Knowledge Base Creation ‣ Fine-Tuning or Retrieval?
    Comparing Knowledge Injection in LLMs")). Fine-tuning is not competitive with
    RAG. However, fine-tuning with multiple paraphrases still provides a significant
    improvement over the baseline. We note that combining RAG with fine-tuning shows
    inferior performance compared to RAG alone.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当前事件结果 当前事件任务的评估结果见 [表 2](#S4.T2 "在 4 知识库创建 ‣ 微调或检索？比较 LLM 中的知识注入")。RAG 由于问题与辅助数据集之间的一对一对应关系，证明了其特别有效（见
    [第 4.3 节](#S4.SS3 "4.3 当前事件任务创建 ‣ 4 知识库创建 ‣ 微调或检索？比较 LLM 中的知识注入")）。微调与 RAG 不具竞争力。然而，使用多重同义句的微调仍然相较于基线有显著提升。我们注意到，将
    RAG 与微调结合的性能逊色于单独使用 RAG 的表现。
- en: It is worth noting that although the questions are based on information the
    models were not exposed to during training, the results of the base models surpass
    $\frac{1}{L}=0.25$. This can partially be explained by the models using reasoning
    and/or pre-existing knowledge when answering questions that are not independent
    of the past information. Some examples of this can be found in [Appendix C](#A3
    "Appendix C Current Events Existing Knowledge Examples ‣ Fine-Tuning or Retrieval?
    Comparing Knowledge Injection in LLMs").
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管这些问题基于模型在训练过程中未接触过的信息，但基础模型的结果超过了 $\frac{1}{L}=0.25$。这可以部分通过模型在回答与过去信息不独立的问题时使用推理和/或现有知识来解释。有关这方面的一些示例可以在
    [附录 C](#A3 "附录 C 当前事件现有知识示例 ‣ 微调或检索？比较 LLM 中的知识注入") 中找到。
- en: 'Fine-Tuning vs. RAG: In the results of both the MMLU and current events tasks,
    a significant advantage for RAG over fine-tuning is evident. While fine-tuning
    improved results compared to the base model in most cases, it was not competitive
    with the RAG approach.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 微调 vs. RAG：在 MMLU 和当前事件任务的结果中，RAG 相比微调显现出显著的优势。虽然微调在大多数情况下比基础模型有所改善，但它与 RAG
    方法相比仍显得不够竞争力。
- en: Several factors might contribute to this behavior. Firstly, RAG not only adds
    knowledge to a model but also incorporates context relevant to the question, a
    feature lacking in fine-tuning. Additionally, fine-tuning may impact other capabilities
    of the model due to a degree of catastrophic forgetting. Finally, it’s plausible
    that unsupervised fine-tuned models might benefit from further alignment through
    supervised or RL-based fine-tuning, as evidenced by the vastly improved performance
    of Orca2 over the base Llama2.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 几个因素可能导致这种行为。首先，RAG 不仅向模型添加知识，还融入了与问题相关的上下文，而这是微调所缺乏的。此外，微调可能会因灾难性遗忘影响模型的其他能力。最后，未监督的微调模型可能通过监督或基于
    RL 的微调受益，如 Orca2 相较于基础 Llama2 的显著改进所示。
- en: 6 The Importance of Repetition
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 重复的重要性
- en: Unlike the other tasks, where the model has been exposed to aspects related
    to the topic during pretraining, current events includes new information. In this
    case, standard regular fine-tuning not only did not improve the performance of
    Llama2 but also significantly degraded it. To improve the fine-tuning results,
    we explored augmentation of the data using paraphrases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他任务不同，在预训练期间模型已接触到与主题相关的方面，而当前事件任务包含了新信息。在这种情况下，标准的常规微调不仅未能改善 Llama2 的表现，还显著恶化了它。为提高微调结果，我们探索了使用同义句对数据进行扩充的方法。
- en: '![Refer to caption](img/b7f39962a4deb2e18dfbd0be754f0eb1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b7f39962a4deb2e18dfbd0be754f0eb1.png)'
- en: 'Figure 3: Training loss over time for Mistral-7B.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Mistral-7B 的训练损失随时间变化。
- en: '![Refer to caption](img/e282bea20d16547b9a7bddac5751980c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e282bea20d16547b9a7bddac5751980c.png)'
- en: 'Figure 4: Model accuracy on the current events task as a function of the number
    of paraphrases.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：模型在当前事件任务上的准确率与同义句数量的关系。
- en: Data Augmentation Data augmentation is a well-established method for enhancing
    the performance of language models and has been surveyed extensively (Shorten
    et al., [2021](#bib.bib38)). Using generative models for augmentations has also
    been used successfully to improve classification models in the past (Sharma et al.,
    [2022](#bib.bib37)). An example of data augmentation using paraphrasing can be
    found in  [Appendix B](#A2 "Appendix B Paraphrase Examples ‣ Fine-Tuning or Retrieval?
    Comparing Knowledge Injection in LLMs").
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强 数据增强是一种提高语言模型性能的成熟方法，并已被广泛调查 (Shorten et al., [2021](#bib.bib38))。使用生成模型进行增强也曾成功地改善分类模型 (Sharma
    et al., [2022](#bib.bib37))。数据增强的一个例子可以在 [附录B](#A2 "附录 B 释义示例 ‣ 微调还是检索？比较LLMs中的知识注入")中找到。
- en: Monotonic Improvement This approach resulted in notable improvements in our
    results, showcasing a direct correlation between the number of paraphrases utilized
    and the models’ accuracy. Our experimentation revealed a compelling trend, shown
    in [Figure 4](#S6.F4 "In 6 The Importance of Repetition ‣ Fine-Tuning or Retrieval?
    Comparing Knowledge Injection in LLMs"). For all models tested, the accuracy was
    a monotonically increasing function of the number of paraphrases used. This observation
    strongly suggests the positive impact of paraphrase augmentation, yielding information
    repetition, on the model’s ability to comprehend and generalize new knowledge
    from limited data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 单调改进 这种方法在我们的结果中取得了显著的改进，展示了所使用的释义数量与模型准确度之间的直接关系。我们的实验揭示了一个引人注目的趋势，如 [图4](#S6.F4
    "在 6 重复的重要性 ‣ 微调还是检索？比较LLMs中的知识注入")所示。所有测试的模型中，准确度都是所用释义数量的单调递增函数。这一观察结果强烈暗示了释义增强对模型理解和从有限数据中概括新知识的能力的积极影响。
- en: Learning New Information In [Figure 3](#S6.F3 "In 6 The Importance of Repetition
    ‣ Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs"), we can see
    an interesting phenomenon observed throughout our experiments. After each epoch,
    i.e., completing another iteration over the entire dataset, the training loss
    drops significantly. This is consistent with what is known about LLMs memorizing
    the data during training and overfitting (Tirumala et al., [2022](#bib.bib44)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 学习新信息 在 [图3](#S6.F3 "在 6 重复的重要性 ‣ 微调还是检索？比较LLMs中的知识注入")中，我们可以看到一个有趣的现象，这一现象在我们的实验中得到了一致的观察。每个epoch之后，即完成整个数据集的另一轮迭代后，训练损失显著下降。这与已知的LLMs在训练期间记忆数据并过拟合的现象一致 (Tirumala
    et al., [2022](#bib.bib44))。
- en: 'Our hypothesis is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设如下：
- en: In order to teach pre-trained LLMs new knowledge, the knowledge must be repeated
    in numerous ways.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了教会预训练的语言模型新的知识，必须以多种方式重复这些知识。
- en: This is well known for LLM pre-training (Kandpal et al., [2023](#bib.bib17)),
    and we see in this case that this holds for fine-tuning as well. The rationale
    for this hypothesis is that mere memorization of sentences does not entail knowledge
    of their content, as was already shown in (Berglund et al., [2023](#bib.bib2)).
    By providing the information in numerous forms (like the data augmentation process
    we used), the various relationships in the data (e.g., <math id=$$ in general,
    as well as ameliorate Berglund et al.’s Reversal Curse. While promising, this
    result still warrants further research.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在LLM预训练中是众所周知的 (Kandpal et al., [2023](#bib.bib17))，我们在这里看到微调的情况也是如此。这一假设的理由是，仅仅记忆句子并不意味着了解其内容，正如 (Berglund
    et al., [2023](#bib.bib2))中已经显示的那样。通过以多种形式提供信息（如我们使用的数据增强过程），数据中的各种关系（例如，<math
    id=$$ 一般来说，以及改善Berglund等人的逆转诅咒）。虽然这一结果很有前景，但仍需进一步研究。
- en: 7 Conclusion and Future Work
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: 'Large language models possess vast amounts of knowledge on various topics.
    In this work, we tested their capability to adapt to new knowledge: both specialized
    and completely unseen. This is among the first studies to compare two prominent
    approaches in this domain, namely fine-tuning and retrieval augmented generation.
    While fine-tuning can be useful for many use-cases, we found that RAG is a more
    reliable choice for knowledge injection.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型拥有各种主题的海量知识。在这项工作中，我们测试了它们适应新知识的能力：包括专业知识和完全未见过的知识。这是首次比较该领域两种主要方法的研究，即微调和检索增强生成。虽然微调对许多用例很有用，但我们发现RAG是知识注入的更可靠选择。
- en: Some aspects of this work still warrant further research. For example, we focused
    on unsupervised training as our primary fine-tuning method, as opposed to instruction-tuning
    or RL-based methods. Researching combinations of various techniques, with diverse
    auxiliary knowledge bases, may yield improved results. This approach, combined
    with our hypothesis from [Section 6](#S6 "6 The Importance of Repetition ‣ Fine-Tuning
    or Retrieval? Comparing Knowledge Injection in LLMs"), could further enhance our
    understanding of knowledge injection via FT.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的一些方面仍然值得进一步研究。例如，我们将无监督训练作为主要的微调方法，而非指令调整或基于RL的方法。研究各种技术的组合，以及多样的辅助知识库，可能会产生更好的结果。这个方法结合我们在[第6节](#S6
    "6 The Importance of Repetition ‣ Fine-Tuning or Retrieval? Comparing Knowledge
    Injection in LLMs")中的假设，可能进一步增强我们对通过FT注入知识的理解。
- en: While we believe that this work further enhances our understanding of knowledge
    in LLMs, there is a lot more work to be done in this field. Specifically, more
    research is required regarding the question of knowledge representation in LLMs,
    especially from a theoretical perspective.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们相信这项工作进一步增强了我们对LLM中知识的理解，但在这一领域仍有大量工作需要完成。具体来说，关于LLM中知识表示的问题，尤其是从理论角度，需要更多研究。
- en: Finally, further efforts are needed to measure knowledge in LLMs. While we employed
    an empirical approach as described in [Equation 2](#S2.E2 "In 2 Background ‣ Fine-Tuning
    or Retrieval? Comparing Knowledge Injection in LLMs"), it is important to explore
    other definitions and perspectives on knowledge as well, and extend upon this
    work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，仍需要进一步努力来衡量LLM中的知识。尽管我们采用了[方程2](#S2.E2 "In 2 Background ‣ Fine-Tuning or
    Retrieval? Comparing Knowledge Injection in LLMs")中描述的经验方法，但探索知识的其他定义和视角也很重要，并在此基础上扩展工作。
- en: 8 Limitations
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: As in all machine learning applications, the choice of hyperparameters significantly
    impacts the results. We therefore strongly recommend optimizing all relevant hyperparameters
    for specific cases.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如同所有机器学习应用一样，超参数的选择对结果有显著影响。因此，我们强烈建议对特定情况优化所有相关超参数。
- en: We have supported our claims by running the experiments on three different models.
    However, generalization to other LLMs should be tested thoroughly. For example,
    GPT-4 achieves near perfect accuracy for some MMLU tasks (Nori et al., [2023](#bib.bib29)),
    and thus further improvement is not applicable.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在三个不同的模型上运行实验来支持我们的主张。然而，应该对其他LLM进行彻底的泛化测试。例如，GPT-4 在一些 MMLU 任务中达到了近乎完美的准确度（Nori
    等，[2023](#bib.bib29)），因此进一步改进并不适用。
- en: Finally, while we chose various topics for the knowledge bases, all of our sources
    came from Wikipedia. Other datasets may yield different results, and must be evaluated
    carefully.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，虽然我们为知识库选择了各种主题，但所有来源均来自维基百科。其他数据集可能会产生不同结果，必须仔细评估。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Attardi (2015) Attardi, G. Wikiextractor. [https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor),
    2015.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Attardi (2015) Attardi, G. Wikiextractor. [https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor)，2015。
- en: 'Berglund et al. (2023) Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland,
    A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on” a is b”
    fail to learn” b is a”. *arXiv preprint arXiv:2309.12288*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berglund 等（2023） Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland,
    A. C., Korbak, T., 和 Evans, O. 反转诅咒：训练于“a 是 b”的LLM无法学习“b 是 a”。*arXiv 预印本 arXiv:2309.12288*，2023。
- en: 'Chen et al. (2020) Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., and Yu, X.
    Recall and learn: Fine-tuning deep pretrained language models with less forgetting.
    *arXiv preprint arXiv:2004.12651*, 2020.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., 和 Yu, X. 召回与学习：在较少遗忘的情况下微调深度预训练语言模型。*arXiv
    预印本 arXiv:2004.12651*，2020。
- en: 'Chen et al. (2022) Chen, X., Zhang, N., Xie, X., Deng, S., Yao, Y., Tan, C.,
    Huang, F., Si, L., and Chen, H. Knowprompt: Knowledge-aware prompt-tuning with
    synergistic optimization for relation extraction. In *Proceedings of the ACM Web
    conference 2022*, pp.  2778–2788, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2022）Chen, X., Zhang, N., Xie, X., Deng, S., Yao, Y., Tan, C., Huang,
    F., Si, L., 和 Chen, H. Knowprompt: 具有协同优化的知识感知提示调整用于关系提取。在*ACM Web 会议论文集 2022*，第
    2778–2788 页，2022。'
- en: Chen et al. (2021) Chen, Y., Zhong, R., Zha, S., Karypis, G., and He, H. Meta-learning
    via language model in-context tuning. *arXiv preprint arXiv:2110.07814*, 2021.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Chen, Y., Zhong, R., Zha, S., Karypis, G., 和 He, H. 通过语言模型的上下文调优进行元学习。*arXiv
    预印本 arXiv:2110.07814*，2021。
- en: 'Chia et al. (2023) Chia, Y. K., Hong, P., Bing, L., and Poria, S. Instructeval:
    Towards holistic evaluation of instruction-tuned large language models. *arXiv
    preprint arXiv:2306.04757*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chia 等（2023）Chia, Y. K., Hong, P., Bing, L., 和 Poria, S. Instructeval: 朝着全面评估指令调整的大型语言模型迈进。*arXiv
    预印本 arXiv:2306.04757*，2023年。'
- en: Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*, 2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2022）Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W.,
    Li, Y., Wang, X., Dehghani, M., Brahma, S., 等。扩展指令微调语言模型。*arXiv 预印本 arXiv:2210.11416*，2022年。
- en: Cohen et al. (2023) Cohen, R., Geva, M., Berant, J., and Globerson, A. Crawling
    the internal knowledge-base of language models. *arXiv preprint arXiv:2301.12810*,
    2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等（2023）Cohen, R., Geva, M., Berant, J., 和 Globerson, A. 爬取语言模型的内部知识库。*arXiv
    预印本 arXiv:2301.12810*，2023年。
- en: 'Dua et al. (2019) Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S.,
    and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning
    over paragraphs. *arXiv preprint arXiv:1903.00161*, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dua 等（2019）Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., 和 Gardner,
    M. Drop: 一个需要对段落进行离散推理的阅读理解基准。*arXiv 预印本 arXiv:1903.00161*，2019年。'
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, September 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C.,
    Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L.,
    Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 一个用于少样本语言模型评估的框架，2021年9月。网址
    [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: Goodfellow et al. (2013) Goodfellow, I. J., Mirza, M., Xiao, D., Courville,
    A., and Bengio, Y. An empirical investigation of catastrophic forgetting in gradient-based
    neural networks. *arXiv preprint arXiv:1312.6211*, 2013.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2013）Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., 和 Bengio,
    Y. 在基于梯度的神经网络中对灾难性遗忘的实证研究。*arXiv 预印本 arXiv:1312.6211*，2013年。
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    *Proceedings of the International Conference on Learning Representations (ICLR)*,
    2021.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解。*国际学习表征会议（ICLR）论文集*，2021年。
- en: Hu et al. (2023) Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., and Li, J. A survey
    of knowledge enhanced pre-trained language models. *IEEE Transactions on Knowledge
    and Data Engineering*, 2023.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2023）Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., 和 Li, J. 知识增强预训练语言模型的综述。*IEEE
    知识与数据工程交易*，2023年。
- en: Huang et al. (2023) Huang, Q., Tao, M., An, Z., Zhang, C., Jiang, C., Chen,
    Z., Wu, Z., and Feng, Y. Lawyer llama technical report. *arXiv preprint arXiv:2305.15062*,
    2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2023）Huang, Q., Tao, M., An, Z., Zhang, C., Jiang, C., Chen, Z., Wu,
    Z., 和 Feng, Y. 律师 llama 技术报告。*arXiv 预印本 arXiv:2305.15062*，2023年。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等。Mistral
    7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Johnson et al. (2019) Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity
    search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等（2019）Johnson, J., Douze, M., 和 Jégou, H. 基于GPU的大规模相似性搜索。*IEEE 大数据交易*，7(3):535–547，2019年。
- en: Kandpal et al. (2023) Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel,
    C. Large language models struggle to learn long-tail knowledge. In *International
    Conference on Machine Learning*, pp.  15696–15707\. PMLR, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandpal 等（2023）Kandpal, N., Deng, H., Roberts, A., Wallace, E., 和 Raffel, C.
    大型语言模型在学习长尾知识方面的挑战。见 *国际机器学习会议*，页码 15696–15707，PMLR，2023年。
- en: Kirkpatrick et al. (2017) Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness,
    J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska,
    A., et al. Overcoming catastrophic forgetting in neural networks. *Proceedings
    of the national academy of sciences*, 114(13):3521–3526, 2017.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirkpatrick 等（2017）Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J.,
    Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska,
    A., 等。克服神经网络中的灾难性遗忘。*国家科学院院刊*，114(13):3521–3526，2017年。
- en: Lampinen et al. (2022) Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson,
    K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang, J. X., and Hill, F.
    Can language models learn from explanations in context? *arXiv preprint arXiv:2204.02329*,
    2022.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lampinen 等（2022）Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson, K.,
    Tessler, M. H., Creswell, A., McClelland, J. L., Wang, J. X., 和 Hill, F. 语言模型是否可以从上下文中的解释中学习？*arXiv
    预印本 arXiv:2204.02329*，2022年。
- en: Lauscher et al. (2020) Lauscher, A., Majewska, O., Ribeiro, L. F., Gurevych,
    I., Rozanov, N., and Glavaš, G. Common sense or world knowledge? investigating
    adapter-based knowledge injection into pretrained transformers. *arXiv preprint
    arXiv:2005.11787*, 2020.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lauscher 等（2020）Lauscher, A., Majewska, O., Ribeiro, L. F., Gurevych, I., Rozanov,
    N., 和 Glavaš, G. 常识还是世界知识？研究基于适配器的知识注入到预训练的变换器中。*arXiv 预印本 arXiv:2005.11787*，2020年。
- en: Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,
    V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented
    generation for knowledge-intensive nlp tasks. *Advances in Neural Information
    Processing Systems*, 33:9459–9474, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal,
    N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., 等。检索增强生成用于知识密集型自然语言处理任务。*神经信息处理系统进展*，33:9459–9474，2020年。
- en: 'Liu et al. (2020) Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H.,
    and Wang, P. K-bert: Enabling language representation with knowledge graph. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pp. 
    2901–2908, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., 和 Wang,
    P. K-bert：通过知识图谱实现语言表示。在 *AAAI 人工智能会议论文集*，第34卷，第2901–2908页，2020年。
- en: Luo et al. (2023) Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang,
    Y. An empirical study of catastrophic forgetting in large language models during
    continual fine-tuning. *arXiv preprint arXiv:2308.08747*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2023）Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., 和 Zhang, Y. 大型语言模型在持续微调期间的灾难性遗忘的实证研究。*arXiv
    预印本 arXiv:2308.08747*，2023年。
- en: 'Min et al. (2021) Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl:
    Learning to learn in context. *arXiv preprint arXiv:2110.15943*, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等（2021）Min, S., Lewis, M., Zettlemoyer, L., 和 Hajishirzi, H. Metaicl：在上下文中学习如何学习。*arXiv
    预印本 arXiv:2110.15943*，2021年。
- en: Mishra et al. (2021) Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.
    Cross-task generalization via natural language crowdsourcing instructions. *arXiv
    preprint arXiv:2104.08773*, 2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等（2021）Mishra, S., Khashabi, D., Baral, C., 和 Hajishirzi, H. 通过自然语言众包指令进行跨任务泛化。*arXiv
    预印本 arXiv:2104.08773*，2021年。
- en: 'Mitra et al. (2023) Mitra, A., Del Corro, L., Mahajan, S., Codas, A., Simoes,
    C., Agrawal, S., Chen, X., Razdaibiedina, A., Jones, E., Aggarwal, K., et al.
    Orca 2: Teaching small language models how to reason. *arXiv preprint arXiv:2311.11045*,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitra 等（2023）Mitra, A., Del Corro, L., Mahajan, S., Codas, A., Simoes, C., Agrawal,
    S., Chen, X., Razdaibiedina, A., Jones, E., Aggarwal, K., 等。Orca 2：教小型语言模型如何推理。*arXiv
    预印本 arXiv:2311.11045*，2023年。
- en: Neelakantan et al. (2022) Neelakantan, A., Xu, T., Puri, R., Radford, A., Han,
    J. M., Tworek, J., Yuan, Q., Tezak, N. A., Kim, J. W., Hallacy, C., Heidecke,
    J., Shyam, P., Power, B., Nekoul, T. E., Sastry, G., Krueger, G., Schnurr, D. P.,
    Such, F. P., Hsu, K. S.-K., Thompson, M., Khan, T., Sherbakov, T., Jang, J., Welinder,
    P., and Weng, L. Text and code embeddings by contrastive pre-training. *ArXiv*,
    abs/2201.10005, 2022. URL [https://api.semanticscholar.org/CorpusID:246275593](https://api.semanticscholar.org/CorpusID:246275593).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neelakantan 等（2022）Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M.,
    Tworek, J., Yuan, Q., Tezak, N. A., Kim, J. W., Hallacy, C., Heidecke, J., Shyam,
    P., Power, B., Nekoul, T. E., Sastry, G., Krueger, G., Schnurr, D. P., Such, F.
    P., Hsu, K. S.-K., Thompson, M., Khan, T., Sherbakov, T., Jang, J., Welinder,
    P., 和 Weng, L. 通过对比预训练的文本和代码嵌入。*ArXiv*，abs/2201.10005，2022年。网址 [https://api.semanticscholar.org/CorpusID:246275593](https://api.semanticscholar.org/CorpusID:246275593)。
- en: 'Nguyen (2023) Nguyen, H.-T. A brief report on lawgpt 1.0: A virtual legal assistant
    based on gpt-3. *arXiv preprint arXiv:2302.05729*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen（2023）Nguyen, H.-T. 关于 lawgpt 1.0 的简要报告：基于 GPT-3 的虚拟法律助手。*arXiv 预印本 arXiv:2302.05729*，2023年。
- en: Nori et al. (2023) Nori, H., King, N., McKinney, S. M., Carignan, D., and Horvitz,
    E. Capabilities of gpt-4 on medical challenge problems. *ArXiv*, abs/2303.13375,
    2023. URL [https://api.semanticscholar.org/CorpusID:257687695](https://api.semanticscholar.org/CorpusID:257687695).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nori 等（2023）Nori, H., King, N., McKinney, S. M., Carignan, D., 和 Horvitz, E.
    GPT-4 在医学挑战问题上的能力。*ArXiv*，abs/2303.13375，2023年。网址 [https://api.semanticscholar.org/CorpusID:257687695](https://api.semanticscholar.org/CorpusID:257687695)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
    URL [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。Gpt-4 技术报告。*ArXiv*，abs/2303.08774，2023年。网址 [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815)。
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in Neural Information
    Processing Systems*, 35:27730–27744, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., 等人。利用人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744，2022年。
- en: Petroni et al. (2019) Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A.,
    Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? *arXiv
    preprint arXiv:1909.01066*, 2019.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni et al. (2019) Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A.,
    Wu, Y., Miller, A. H., 和 Riedel, S. 语言模型作为知识库？*arXiv 预印本 arXiv:1909.01066*，2019年。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 1(8):9, 2019.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., 等人。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019年。
- en: 'Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,
    C. D., and Finn, C. Direct preference optimization: Your language model is secretly
    a reward model. *arXiv preprint arXiv:2305.18290*, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,
    C. D., 和 Finn, C. 直接偏好优化：你的语言模型实际上是一个奖励模型。*arXiv 预印本 arXiv:2305.18290*，2023年。
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi,
    Y. Winogrande：大规模对抗性 Winograd 语义挑战。*计算机协会通讯*，64(9):99–106，2021年。
- en: Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*,
    2017.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    和 Klimov, O. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*，2017年。
- en: Sharma et al. (2022) Sharma, S., Joshi, A., Mukhija, N., Zhao, Y., Bhathena,
    H., Singh, P., Santhanam, S., and Biswas, P. Systematic review of effect of data
    augmentation using paraphrasing on named entity recognition. In *NeurIPS 2022
    Workshop on Synthetic Data for Empowering ML Research*, 2022. URL [https://openreview.net/forum?id=rc2h1h89aDi](https://openreview.net/forum?id=rc2h1h89aDi).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma et al. (2022) Sharma, S., Joshi, A., Mukhija, N., Zhao, Y., Bhathena,
    H., Singh, P., Santhanam, S., 和 Biswas, P. 关于使用同义改写的数据增强对命名实体识别影响的系统评价。*NeurIPS
    2022 研讨会：赋能 ML 研究的合成数据*，2022年。网址 [https://openreview.net/forum?id=rc2h1h89aDi](https://openreview.net/forum?id=rc2h1h89aDi)。
- en: Shorten et al. (2021) Shorten, C., Khoshgoftaar, T. M., and Furht, B. Text data
    augmentation for deep learning. *Journal of Big Data*, 8, 2021. URL [https://api.semanticscholar.org/CorpusID:236096559](https://api.semanticscholar.org/CorpusID:236096559).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten et al. (2021) Shorten, C., Khoshgoftaar, T. M., 和 Furht, B. 深度学习的文本数据增强。*大数据期刊*，8，2021年。网址
    [https://api.semanticscholar.org/CorpusID:236096559](https://api.semanticscholar.org/CorpusID:236096559)。
- en: Singhal et al. (2023a) Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei,
    J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., et al. Large
    language models encode clinical knowledge. *Nature*, 620(7972):172–180, 2023a.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal et al. (2023a) Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei,
    J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S., 等人。大型语言模型编码临床知识。*自然*，620(7972):172–180，2023a年。
- en: Singhal et al. (2023b) Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn,
    E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., et al. Towards expert-level
    medical question answering with large language models. *arXiv preprint arXiv:2305.09617*,
    2023b.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal et al. (2023b) Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn,
    E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., 等人。利用大型语言模型实现专家级医学问答。*arXiv
    预印本 arXiv:2305.09617*，2023b年。
- en: 'Srivastava et al. (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M.,
    Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A.,
    et al. Beyond the imitation game: Quantifying and extrapolating the capabilities
    of language models. *arXiv preprint arXiv:2206.04615*, 2022.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava et al. (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A.
    M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso,
    A., 等人。超越模仿游戏：量化和外推语言模型的能力。*arXiv 预印本 arXiv:2206.04615*，2022年。
- en: Tan et al. (2023) Tan, Y., Min, D., Li, Y., Li, W., Hu, N., Chen, Y., and Qi,
    G. Can chatgpt replace traditional kbqa models? an in-depth analysis of the question
    answering performance of the gpt llm family. In *International Semantic Web Conference*,
    pp.  348–367\. Springer, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谭等（2023）谭，Y.，敏，D.，李，Y.，李，W.，胡，N.，陈，Y.，和齐，G. ChatGPT能否替代传统的KBQA模型？对GPT LLM家族问答性能的深入分析。发表于*国际语义网会议*，第348–367页。Springer，2023。
- en: 'Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
    Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: A strong, replicable instruction-following
    model. *Stanford Center for Research on Foundation Models. https://crfm. stanford.
    edu/2023/03/13/alpaca. html*, 3(6):7, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等（2023）Taori，R.，古尔拉贾尼，I.，张，T.，杜布瓦，Y.，李，X.，古斯特林，C.，梁，P.，和哈希莫托，T. B. Alpaca:
    一种强大且可复制的指令跟随模型。*斯坦福基础模型研究中心。https://crfm.stanford.edu/2023/03/13/alpaca.html*，3(6):7，2023。'
- en: 'Tirumala et al. (2022) Tirumala, K., Markosyan, A. H., Zettlemoyer, L., and
    Aghajanyan, A. Memorization without overfitting: Analyzing the training dynamics
    of large language models. *ArXiv*, abs/2205.10770, 2022. URL [https://api.semanticscholar.org/CorpusID:248986465](https://api.semanticscholar.org/CorpusID:248986465).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tirumala 等（2022）Tirumala，K.，Markosyan，A. H.，Zettlemoyer，L.，和 Aghajanyan，A. 无过拟合的记忆化：分析大型语言模型的训练动态。*ArXiv*，abs/2205.10770，2022。网址
    [https://api.semanticscholar.org/CorpusID:248986465](https://api.semanticscholar.org/CorpusID:248986465)。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Touvron，H.，马丁，L.，斯通，K.，阿尔贝特，P.，阿尔马海里，A.，巴巴伊，Y.，巴什利科夫，N.，巴特拉，S.，巴尔加瓦，P.，博萨尔，S.，等。Llama
    2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。'
- en: 'Tunstall et al. (2023) Tunstall, L., Beeching, E., Lambert, N., Rajani, N.,
    Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et al.
    Zephyr: Direct distillation of lm alignment. *arXiv preprint arXiv:2310.16944*,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunstall 等（2023）Tunstall，L.，比钦，E.，兰伯特，N.，拉贾尼，N.，拉苏尔，K.，贝尔卡达，Y.，黄，S.，冯·维拉，L.，弗雷，C.，哈比布，N.，等。Zephyr:
    直接蒸馏语言模型对齐。*arXiv 预印本 arXiv:2310.16944*，2023。'
- en: 'Wang et al. (2023) Wang, C., Liu, X., Yue, Y., Tang, X., Zhang, T., Jiayang,
    C., Yao, Y., Gao, W., Hu, X., Qi, Z., et al. Survey on factuality in large language
    models: Knowledge, retrieval and domain-specificity. *arXiv preprint arXiv:2310.07521*,
    2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023）王，C.，刘，X.，岳，Y.，唐，X.，张，T.，佳杨，C.，姚，Y.，高，W.，胡，X.，齐，Z.，等。大型语言模型中的真实性调查：知识、检索和领域特异性。*arXiv
    预印本 arXiv:2310.07521*，2023。
- en: 'Wang et al. (2020) Wang, R., Tang, D., Duan, N., Wei, Z., Huang, X., Cao, G.,
    Jiang, D., Zhou, M., et al. K-adapter: Infusing knowledge into pre-trained models
    with adapters. *arXiv preprint arXiv:2002.01808*, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等（2020）王，R.，唐，D.，段，N.，魏，Z.，黄，X.，曹，G.，江，D.，周，M.，等。K-adapter: 用适配器将知识注入预训练模型。*arXiv
    预印本 arXiv:2002.01808*，2020。'
- en: 'Wang et al. (2022) Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei,
    A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al.
    Super-naturalinstructions: Generalization via declarative instructions on 1600+
    nlp tasks. *arXiv preprint arXiv:2204.07705*, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等（2022）王，Y.，米什拉，S.，阿里普尔摩拉巴希，P.，科尔迪，Y.，米尔扎伊，A.，阿伦库马尔，A.，阿肖克，A.，达纳塞卡兰，A. S.，奈克，A.，斯塔普，D.，等。Super-naturalinstructions:
    通过声明性指令在1600多个NLP任务上的泛化。*arXiv 预印本 arXiv:2204.07705*，2022。'
- en: 'Wu et al. (2023a) Wu, C., Zhang, X., Zhang, Y., Wang, Y., and Xie, W. Pmc-llama:
    Further finetuning llama on medical papers. *arXiv preprint arXiv:2304.14454*,
    2023a.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等（2023a）吴，C.，张，X.，张，Y.，王，Y.，和谢，W. Pmc-llama: 进一步在医学论文上微调llama。*arXiv 预印本 arXiv:2304.14454*，2023a。'
- en: 'Wu et al. (2023b) Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann,
    S., Kambadur, P., Rosenberg, D., and Mann, G. Bloomberggpt: A large language model
    for finance. *arXiv preprint arXiv:2303.17564*, 2023b.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等（2023b）吴，S.，Irsoy，O.，卢，S.，达布拉沃尔斯基，V.，德雷泽，M.，格尔曼，S.，卡姆巴杜尔，P.，罗森伯格，D.，和曼，G.
    Bloomberggpt: 一种用于金融的大型语言模型。*arXiv 预印本 arXiv:2303.17564*，2023b。'
- en: 'Xiao et al. (2023) Xiao, S., Liu, Z., Zhang, P., and Muennighoff, N. C-pack:
    Packaged resources to advance general chinese embedding, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '萧等（2023）萧，S.，刘，Z.，张，P.，和穆宁霍夫，N. C-pack: 包装资源以推进通用中文嵌入，2023。'
- en: 'Yang et al. (2023) Yang, H., Liu, X.-Y., and Wang, C. D. Fingpt: Open-source
    financial large language models. *arXiv preprint arXiv:2306.06031*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等（2023）杨，H.，刘，X.-Y.，和王，C. D. Fingpt: 开源金融大型语言模型。*arXiv 预印本 arXiv:2306.06031*，2023。'
- en: Yu et al. (2022) Yu, W., Zhu, C., Li, Z., Hu, Z., Wang, Q., Ji, H., and Jiang,
    M. A survey of knowledge-enhanced text generation. *ACM Computing Surveys*, 54(11s):1–38,
    2022.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) Yu, W., Zhu, C., Li, Z., Hu, Z., Wang, Q., Ji, H., and Jiang,
    M. 知识增强文本生成的调查。*ACM Computing Surveys*, 54(11s):1–38, 2022.
- en: 'Zhou et al. (2023) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma,
    X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. *arXiv
    preprint arXiv:2305.11206*, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma,
    X., Efrat, A., Yu, P., Yu, L., 等。Lima: 对齐中的“少即是多”。*arXiv预印本 arXiv:2305.11206*,
    2023.'
- en: Appendix A RAG Ablation Study
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A RAG消融研究
- en: As mentioned in [Section 5](#S5 "5 Experiments and Results ‣ Fine-Tuning or
    Retrieval? Comparing Knowledge Injection in LLMs"), we compared various values
    of $K\in\{0,\ldots,5\}$ consistently, there seems to be no patterns that aid in
    predicting the performance per $K$s can be large.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第5节](#S5 "5 实验和结果 ‣ 微调还是检索？比较LLM中的知识注入")中提到的，我们一致比较了不同的$K\in\{0,\ldots,5\}$值，似乎没有模式可以帮助预测每个$K$的性能。
- en: Unfortunately, we must conclude that this additional hyperparameter is unstable.
    This is a downside of using RAG in practice, and the choice of $K$ cannot be ignored.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们必须得出结论，这个额外的超参数是不稳定的。这是实践中使用RAG的一个缺点，$K$的选择不能被忽视。
- en: '| Task | Model | # Retrieved documents ($k$) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 模型 | 检索文档数量（$k$） |'
- en: '| --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 2 | 3 | 4 | 5 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Anatomy (0-shot) | Mistral 7B | 0.615 | 0.681 | 0.630 | 0.644 | 0.622 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学（0-shot） | Mistral 7B | 0.615 | 0.681 | 0.630 | 0.644 | 0.622 |'
- en: '| Llama2 7B | 0.444 | 0.489 | 0.467 | 0.474 | 0.481 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.444 | 0.489 | 0.467 | 0.474 | 0.481 |'
- en: '| Orca2 7B | 0.607 | 0.637 | 0.600 | 0.585 | 0.637 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.607 | 0.637 | 0.600 | 0.585 | 0.637 |'
- en: '| Anatomy (5-shot) | Mistral 7B | 0.659 | 0.667 | 0.659 | 0.681 | 0.674 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学（5-shot） | Mistral 7B | 0.659 | 0.667 | 0.659 | 0.681 | 0.674 |'
- en: '| Llama2 7B | 0.496 | 0.563 | 0.541 | 0.526 | 0.526 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.496 | 0.563 | 0.541 | 0.526 | 0.526 |'
- en: '| Orca2 7B | 0.630 | 0.659 | 0.600 | 0.600 | 0.600 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.630 | 0.659 | 0.600 | 0.600 | 0.600 |'
- en: '| Astronomy (0-shot) | Mistral 7B | 0.651 | 0.678 | 0.678 | 0.664 | 0.664 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 天文学（0-shot） | Mistral 7B | 0.651 | 0.678 | 0.678 | 0.664 | 0.664 |'
- en: '| Llama2 7B | 0.447 | 0.434 | 0.447 | 0.434 | 0.467 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.447 | 0.434 | 0.447 | 0.434 | 0.467 |'
- en: '| Orca2 7B | 0.711 | 0.730 | 0.730 | 0.750 | 0.730 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.711 | 0.730 | 0.730 | 0.750 | 0.730 |'
- en: '| Astronomy (5-shot) | Mistral 7B | 0.704 | 0.684 | 0.658 | 0.684 | 0.724 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 天文学（5-shot） | Mistral 7B | 0.704 | 0.684 | 0.658 | 0.684 | 0.724 |'
- en: '| Llama2 7B | 0.461 | 0.447 | 0.474 | 0.428 | 0.454 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.461 | 0.447 | 0.474 | 0.428 | 0.454 |'
- en: '| Orca2 7B | 0.730 | 0.737 | 0.750 | 0.743 | 0.763 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.730 | 0.737 | 0.750 | 0.743 | 0.763 |'
- en: '| Biology (0-shot) | Mistral 7B | 0.736 | 0.722 | 0.757 | 0.743 | 0.736 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 生物学（0-shot） | Mistral 7B | 0.736 | 0.722 | 0.757 | 0.743 | 0.736 |'
- en: '| Llama2 7B | 0.438 | 0.472 | 0.493 | 0.479 | 0.472 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.438 | 0.472 | 0.493 | 0.479 | 0.472 |'
- en: '| Orca2 7B | 0.639 | 0.618 | 0.639 | 0.625 | 0.639 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.639 | 0.618 | 0.639 | 0.625 | 0.639 |'
- en: '| Biology (5-shot) | Mistral 7B | 0.722 | 0.778 | 0.778 | 0.771 | 0.743 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 生物学（5-shot） | Mistral 7B | 0.722 | 0.778 | 0.778 | 0.771 | 0.743 |'
- en: '| Llama2 7B | 0.500 | 0.521 | 0.507 | 0.465 | 0.472 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.500 | 0.521 | 0.507 | 0.465 | 0.472 |'
- en: '| Orca2 7B | 0.625 | 0.639 | 0.625 | 0.660 | 0.660 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.625 | 0.639 | 0.625 | 0.660 | 0.660 |'
- en: '| Chemistry (0-shot) | Mistral 7B | 0.450 | 0.470 | 0.470 | 0.500 | 0.470 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 化学（0-shot） | Mistral 7B | 0.450 | 0.470 | 0.470 | 0.500 | 0.470 |'
- en: '| Llama2 7B | 0.320 | 0.320 | 0.300 | 0.380 | 0.360 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.320 | 0.320 | 0.300 | 0.380 | 0.360 |'
- en: '| Orca2 7B | 0.370 | 0.420 | 0.400 | 0.410 | 0.440 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.370 | 0.420 | 0.400 | 0.410 | 0.440 |'
- en: '| Chemistry (5-shot) | Mistral 7B | 0.540 | 0.490 | 0.500 | 0.510 | 0.470 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 化学（5-shot） | Mistral 7B | 0.540 | 0.490 | 0.500 | 0.510 | 0.470 |'
- en: '| Llama2 7B | 0.280 | 0.320 | 0.340 | 0.340 | 0.380 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.280 | 0.320 | 0.340 | 0.340 | 0.380 |'
- en: '| Orca2 7B | 0.390 | 0.430 | 0.400 | 0.430 | 0.470 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.390 | 0.430 | 0.400 | 0.430 | 0.470 |'
- en: '| Prehistory (0-shot) | Mistral 7B | 0.728 | 0.725 | 0.750 | 0.735 | 0.728
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 史前时代（0-shot） | Mistral 7B | 0.728 | 0.725 | 0.750 | 0.735 | 0.728 |'
- en: '| Llama2 7B | 0.481 | 0.460 | 0.457 | 0.457 | 0.429 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.481 | 0.460 | 0.457 | 0.457 | 0.429 |'
- en: '| Orca2 7B | 0.648 | 0.645 | 0.660 | 0.670 | 0.679 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.648 | 0.645 | 0.660 | 0.670 | 0.679 |'
- en: '| Prehistory (5-shot) | Mistral 7B | 0.710 | 0.750 | 0.759 | 0.756 | 0.762
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 史前时代（5-shot） | Mistral 7B | 0.710 | 0.750 | 0.759 | 0.756 | 0.762 |'
- en: '| Llama2 7B | 0.512 | 0.485 | 0.525 | 0.519 | 0.531 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 7B | 0.512 | 0.485 | 0.525 | 0.519 | 0.531 |'
- en: '| Orca2 7B | 0.660 | 0.688 | 0.685 | 0.698 | 0.688 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Orca2 7B | 0.660 | 0.688 | 0.685 | 0.698 | 0.688 |'
- en: 'Table 3: RAG ablation study.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: RAG消融研究。'
- en: Appendix B Paraphrase Examples
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 改写示例
- en: 'Below is the prompt we used to generate paraphrases with GPT-4:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们用来生成GPT-4改写的提示：
- en: Your task is to paraphrase a text paragraph. The paragraph is given below.
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的任务是改写一个文本段落。以下是该段落：
- en: ''
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Make sure to keep the same meaning but change the wording. Do not change any
    factual information.
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确保保持相同的含义但更改措辞。不要更改任何事实信息。
- en: ''
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Try to keep roughly the same length of the original text.
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽量保持与原文大致相同的长度。
- en: ''
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Give NUM _ PARAPHRASES different paraphrases for each text.
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为每个文本提供NUM _ PARAPHRASES个不同的改述。
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These paraphrases should be as different from each other as possible.
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些改述应尽可能不同。
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Return a JSON formatted string with one key, called ’paraphrases’, and a list
    of paraphrases.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 返回一个JSON格式的字符串，其中包含一个名为’paraphrases’的键，以及一个改述列表。
- en: ''
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Input paragraph:'
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 输入段落：
- en: 'Below are some examples. First, an example taken from the following Wikipedia
    page: ”51st International Emmy Awards”.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例。首先是取自维基百科页面的示例：“第51届国际艾美奖”。
- en: Paraphrase I
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 I
- en: ''
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fifty-first edition of the International Emmy Awards Competition accepted
    submissions across all categories from December 7, 2022, until the closing date
    of February 16, 2023.
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第五十一届国际艾美奖比赛接受了从2022年12月7日到2023年2月16日的所有类别的提交。
- en: ''
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paraphrase II
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 II
- en: ''
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The International Emmy Awards Competition, now in its 51st year, accepted submissions
    across all categories starting from December 7, 2022, until the deadline on February
    16, 2023.
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 国际艾美奖比赛，第五十一届，从2022年12月7日开始接受所有类别的提交，直到2023年2月16日的截止日期。
- en: ''
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paraphrase III
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 III
- en: ''
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fifty-first edition of the International Emmy Awards Competition accepted
    submissions across all categories from December 7, 2022, until the closing date
    of February 16, 2023.
  id: totrans-281
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第五十一届国际艾美奖比赛接受了从2022年12月7日到2023年2月16日的所有类别的提交。
- en: 'The next example was taken from the following Wikipedia page: ”2023 Indianapolis
    mayoral election”.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例取自维基百科页面：“2023年印第安纳波利斯市长选举”。
- en: Paraphrase I The mayoral election in Indianapolis took place on November 7,
    2023, with preliminary elections occurring on May 2\. The sitting mayor, Democrat
    Joe Hogsett, successfully ran for a third term. Both Hogsett and his Republican
    opponent, Jefferson Shreve, moved on to the main election.
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 I 印第安纳波利斯的市长选举于2023年11月7日举行，初选发生在5月2日。现任市长、民主党人乔·霍格塞特成功竞选了第三个任期。霍格塞特和他的共和党对手杰斐逊·施里夫进入了主要选举阶段。
- en: ''
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paraphrase II On November 7, 2023, citizens of Indianapolis cast their votes
    to elect their Mayor, following primary elections on May 2\. Joe Hogsett, the
    Democrat already in office, won his bid for a third term. Hogsett and the Republican
    candidate, Jefferson Shreve, were the two contenders in the final electoral round.
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 II 2023年11月7日，印第安纳波利斯的市民投票选举市长，初选在5月2日举行。现任民主党市长乔·霍格塞特成功竞选第三个任期。霍格塞特和共和党候选人杰斐逊·施里夫是最终选举回合中的两个竞争者。
- en: ''
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paraphrase III
  id: totrans-287
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 改述 III
- en: ''
  id: totrans-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The mayoral election in Indianapolis took place on the 7th of November, 2023,
    following primary elections that occurred on the 2nd of May. Joe Hogsett, the
    incumbent Democrat, successfully ran for a third term. Both Hogsett and his Republican
    challenger, Jefferson Shreve, made it through to the final round of the election.
  id: totrans-289
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 印第安纳波利斯的市长选举于2023年11月7日举行，此前的初选在5月2日进行。现任市长、民主党人乔·霍格塞特成功竞选第三个任期。霍格塞特和他的共和党对手杰斐逊·施里夫都进入了选举的最终回合。
- en: Appendix C Current Events Existing Knowledge Examples
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 当前事件 现有知识示例
- en: To give a better understanding of how a model might be able to answer questions
    about new information, with better than random success, we present three possible
    scenarios as examples. These scenarios show how models with stronger reasoning
    skills can infer the correct answer even for unseen information.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解模型如何能够回答有关新信息的问题，并且成功率高于随机水平，我们提供了三个可能的示例场景。这些场景展示了具有更强推理能力的模型如何即使面对未知信息也能推断出正确答案。
- en: The first scenario involves questions about previously unseen information, where
    basic reasoning abilities allow a model to make an educated guess.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个场景涉及有关之前未见信息的问题，在这种情况下，基本的推理能力可以使模型做出有根据的猜测。
- en: 'Question: What was a key issue that led to the 2023 United Auto Workers strike?'
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：导致2023年美国汽车工人罢工的关键问题是什么？
- en: ''
  id: totrans-294
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answers:'
  id: totrans-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：
- en: ''
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-297
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: ''
  id: totrans-298
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Dissatisfaction with the quality of cafeteria food.
  id: totrans-299
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于自助餐厅食物质量的不满。
- en: ''
  id: totrans-300
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-301
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-302
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: ''
  id: totrans-303
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Disagreements over employee dress codes.
  id: totrans-304
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对员工着装规范的分歧。
- en: ''
  id: totrans-305
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-306
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-307
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: ''
  id: totrans-308
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Discontent with stagnant wages and tiered employment systems.
  id: totrans-309
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于工资停滞和分层就业系统的不满。
- en: ''
  id: totrans-310
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-311
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-312
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: ''
  id: totrans-313
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Debates over the color scheme of the factories.
  id: totrans-314
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对工厂色彩方案的辩论。
- en: In this case it is easy to guess that the third option is the most likely, even
    without knowledge of this specific strike.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，即使没有具体的袭击知识，也容易猜测第三个选项是最可能的。
- en: A second scenario involves questions where prior knowledge about a topic may
    aid a model in answering.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情景涉及的问题是，事先了解某个主题可能有助于模型回答。
- en: 'Question: What environmental concern was raised by some scientists as a result
    of the 2023 Hawaii wildfires?'
  id: totrans-317
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：一些科学家因2023年夏威夷的野火提出了什么环境担忧？
- en: ''
  id: totrans-318
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answers:'
  id: totrans-319
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：
- en: ''
  id: totrans-320
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-321
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1。
- en: ''
  id: totrans-322
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Rising temperatures.
  id: totrans-323
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 气温上升。
- en: ''
  id: totrans-324
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-325
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-326
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2。
- en: ''
  id: totrans-327
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Melting ice caps.
  id: totrans-328
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 冰盖融化。
- en: ''
  id: totrans-329
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-330
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-331
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3。
- en: ''
  id: totrans-332
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Charred soils running off into the shoreline.
  id: totrans-333
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灼烧的土壤流入海岸线。
- en: ''
  id: totrans-334
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-335
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-336
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4。
- en: ''
  id: totrans-337
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Increased air pollution.
  id: totrans-338
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加的空气污染。
- en: In this case, knowing the geography of Hawaii, as well as immediate effects
    of wildfires, enables a model to give the first two options a lower likelihood.
    This process of elimination increases the probability of choosing one of the remaining
    options (the third option is the correct answer).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，了解夏威夷的地理环境以及野火的直接影响，使模型能够给出前两个选项较低的可能性。这个排除过程增加了选择剩余选项之一的概率（第三个选项是正确答案）。
- en: A third scenario arises due to the automatic question generation process, some
    questions strongly rely on pre-existing knowledge.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自动生成问题的过程产生了第三种情景，有些问题强烈依赖于已有知识。
- en: 'Question: What event in 2021 was compared to the September 2023 New York floods?'
  id: totrans-341
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：2021年与2023年9月纽约洪水相比的事件是什么？
- en: ''
  id: totrans-342
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answers:'
  id: totrans-343
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：
- en: ''
  id: totrans-344
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-345
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1。
- en: ''
  id: totrans-346
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Hurricane Katrina.
  id: totrans-347
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 飓风卡特里娜。
- en: ''
  id: totrans-348
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-349
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-350
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2。
- en: ''
  id: totrans-351
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Hurricane Ida.
  id: totrans-352
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 飓风艾达。
- en: ''
  id: totrans-353
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-354
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-355
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3。
- en: ''
  id: totrans-356
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Hurricane Sandy.
  id: totrans-357
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 飓风桑迪。
- en: ''
  id: totrans-358
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-359
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-360
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4。
- en: ''
  id: totrans-361
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Hurricane Harvey.
  id: totrans-362
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 飓风哈维。
- en: Since only one of these events occurred in 2021 (Hurricane Ida), and all the
    models tested have been exposed to events from 2021 during pre-training, this
    question can potentially be answered without using additional current information.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有一个事件在2021年发生（飓风艾达），且所有测试过的模型在预训练过程中都接触过2021年的事件，因此这个问题有可能在不使用额外当前信息的情况下回答。
- en: 'Finally, to demonstrate why it is reasonable to assume that models cannot generally
    answer questions about new information, with better than random success, look
    at the following example:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，为了展示为何合理假设模型通常无法对新信息做出比随机更好的回答，请看以下示例：
- en: 'Question: How did Matthew Belk, a National Weather Service meteorologist, describe
    the September 2023 northeastern U.S. floods?'
  id: totrans-365
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：国家气象局气象学家马修·贝尔克如何描述2023年9月美国东北部的洪水？
- en: ''
  id: totrans-366
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answers:'
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案：
- en: ''
  id: totrans-368
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-369
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1。
- en: ''
  id: totrans-370
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 50-year event.
  id: totrans-371
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 50年一遇的事件。
- en: ''
  id: totrans-372
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-373
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-374
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2。
- en: ''
  id: totrans-375
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 100-year event.
  id: totrans-376
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 100年一遇的事件。
- en: ''
  id: totrans-377
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-378
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-379
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3。
- en: ''
  id: totrans-380
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 200-year event.
  id: totrans-381
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 200年一遇的事件。
- en: ''
  id: totrans-382
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-383
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-384
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4。
- en: ''
  id: totrans-385
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 500-year event.
  id: totrans-386
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
  zh: 500年一遇的事件。
- en: Even with some knowledge about floods and their statistical properties, it would
    be very difficult to guess that this specific meteorologist would call the flood
    a ‘200-year event’. This is especially true if the model was not exposed to information
    about the details of the flood.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对洪水及其统计特性有一定了解，也很难猜测这位特定的气象学家会将洪水称为‘200年一遇的事件’。如果模型没有接触到洪水详细信息，这种情况尤为真实。
