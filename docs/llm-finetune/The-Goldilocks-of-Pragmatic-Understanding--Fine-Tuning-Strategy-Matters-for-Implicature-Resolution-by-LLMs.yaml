- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:40:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语用理解的金发姑娘：LLMs的微调策略对隐含意义解决的重要性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2210.14986](https://ar5iv.labs.arxiv.org/html/2210.14986)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2210.14986](https://ar5iv.labs.arxiv.org/html/2210.14986)
- en: \extrafloats
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \extrafloats
- en: '100'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '100'
- en: Laura Ruis
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 劳拉·鲁伊斯
- en: University College London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Akbir Khan
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阿克比尔·汗
- en: University College London
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Stella Biderman
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 斯特拉·比德曼
- en: EleutherAI, Booz Allen Hamilton
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: EleutherAI，Booz Allen Hamilton
- en: Sara Hooker
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 萨拉·胡克
- en: Cohere for AI
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere for AI
- en: Tim Rocktäschel
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提姆·罗克塔谢尔
- en: University College London
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Edward Grefenstette
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 爱德华·格雷芬斯泰特
- en: University College London Correspondence to laura.ruis.21@ucl.ac.uk
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院，联系邮箱：laura.ruis.21@ucl.ac.uk
- en: Abstract
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Despite widespread use of LLMs as conversational agents, evaluations of performance
    fail to capture a crucial aspect of communication: interpreting language *in context*—incorporating
    its pragmatics. Humans interpret language using beliefs and prior knowledge about
    the world. For example, we intuitively understand the response “I wore gloves”
    to the question “Did you leave fingerprints?” as meaning “No”. To investigate
    whether LLMs have the ability to make this type of inference, known as an *implicature*,
    we design a simple task and evaluate four categories of widely used state-of-the-art
    models. We find that, despite only evaluating on utterances that require a binary
    inference (yes or no), models in three of these categories perform close to random.
    However, LLMs instruction-tuned at the example-level perform significantly better.
    These results suggest that certain fine-tuning strategies are far better at inducing
    pragmatic understanding in models. We present our findings as the starting point
    for further research into evaluating how LLMs interpret language in context and
    to drive the development of more pragmatic and useful models of human discourse.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）作为对话代理的应用广泛，但性能评估未能捕捉到沟通中的一个关键方面：*在上下文中*解释语言——即包含其语用学。人类通过对世界的信念和先验知识来解释语言。例如，我们直观地理解对问题“你留下了指纹吗？”的回答“我戴了手套”为“不”。为了研究LLMs是否具备这种类型的推理能力，称为*隐含意义*，我们设计了一个简单任务并评估了四类广泛使用的最先进模型。我们发现，尽管仅对需要二元推理（是或否）的发言进行评估，这三类模型的表现接近随机。然而，经过示例级别微调的LLMs表现显著更好。这些结果表明，某些微调策略在诱导模型的语用理解方面要好得多。我们将我们的发现作为进一步研究LLMs如何在上下文中解释语言的起点，并推动更具语用性和实用性的人类话语模型的发展。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'User: “Have you seen my phone?”'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用户：“你看过我的手机吗？”
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'GPT-3: “Yes, I have seen your phone.”'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-3：“是的，我看过你的手机。”
- en: 'GPT-3’s response¹¹1Appendix [D](#A4 "Appendix D Opener example with InstructGPT
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") contains details on how this completion was obtained
    from text-davinci-002 is a perfectly fine answer to the question, but a human
    might answer differently. They might respond “it’s in your bag," bypassing the
    obvious follow-up question (“where is it?”). Giving such a helpful and efficient
    answer is an example of pragmatic language use that goes beyond the mere production
    of semantically plausible and consistent utterances. Meaning is not only determined
    by a combination of words, but also context, beliefs, and social institutions
    (Wittgenstein,, [1953](#bib.bib69); Grice,, [1975](#bib.bib26); Huang,, [2017](#bib.bib29)).
    Consider another exchange where Esther asks her friend Juan “Can you come to my
    party on Friday?” and Juan responds “I have to work”. We resolve Juan’s response
    as him declining the invitation by using the contextual commonsense knowledge
    that having to work on a Friday night precludes attendance. Both these exchanges
    contain an implicature—utterances that convey something other than their literal
    meaning.²²2In Appendix [E](#A5 "Appendix E Background on implicature ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") we present an introduction to implicature. Implicatures illustrate how
    context contributes to meaning; distinguishing writing and speaking from communicating
    (Green,, [1996](#bib.bib24)). We cannot fully understand utterances without understanding
    their implications. Indeed, the term “communication” presupposes the speaker’s
    implications are understood by the addressee. Being able to resolve completely
    novel implicatures and, more broadly, engage in pragmatic understanding constitutes
    an essential and ubiquitous aspect of our every day use of language.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的响应¹¹1附录 [D](#A4 "附录 D InstructGPT 的开启示例 ‣ 实用理解的金发女孩：微调策略对 LLM 解决隐含意义的重要性")
    包含了如何从 text-davinci-002 获得这个完成的详细信息，这个答案对问题来说完全可以，但人类可能会给出不同的回答。他们可能会回答“它在你的包里”，绕过明显的后续问题（“它在哪里？”）。给出这样一个有帮助且高效的回答是实用语言使用的一个例子，它超越了仅仅生成语义上合理且一致的话语。意义不仅由词汇的组合决定，还包括上下文、信念和社会机构（Wittgenstein，[1953](#bib.bib69)；Grice，[1975](#bib.bib26)；Huang，[2017](#bib.bib29)）。考虑另一个对话，Esther
    问她的朋友 Juan “你能在星期五来我的派对吗？”而 Juan 回答“我得工作”。我们通过使用上下文常识知识（即在星期五晚上工作排除了出席）来将 Juan
    的回应解读为拒绝邀请。这些对话都包含了隐含意义——传达其他含义的言辞。²²2在附录 [E](#A5 "附录 E 隐含意义背景 ‣ 实用理解的金发女孩：微调策略对
    LLM 解决隐含意义的重要性") 中，我们介绍了隐含意义。隐含意义说明了上下文如何对意义产生影响；区分写作和讲话与沟通（Green，[1996](#bib.bib24)）。我们不能完全理解言辞而不理解其隐含意义。确实，术语“沟通”预设了听者理解说话者的隐含意义。能够解决全新的隐含意义，更广泛地说，进行实用理解，是我们日常语言使用中的一个重要而普遍的方面。
- en: '![Refer to caption](img/fff215109fcf197cf84f56e501f9e2ae.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fff215109fcf197cf84f56e501f9e2ae.png)'
- en: 'Figure 1: A schematic depiction of the protocol we propose to evaluate whether
    language models can resolve implicatures. Each example in the test set gets wrapped
    in templates and transformed into an *incoherent* example by swapping “yes” and
    “no”. The model is said to resolve the implicature if it assigns a higher likelihood
    to the coherent text than the incoherent text.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提议的协议示意图，用于评估语言模型是否能够解决隐含意义。测试集中的每个例子都被包装在模板中，通过交换“yes”和“no”被转化为*不连贯*的例子。如果模型给连贯文本分配的概率高于不连贯文本，则认为模型解决了隐含意义。
- en: Large language models (LLMs) have demonstrated remarkable ability on a variety
    of downstream tasks such as planning (Huang et al.,, [2022](#bib.bib28)), commonsense
    reasoning (Kojima et al.,, [2022](#bib.bib37)), information retrieval (Lewis et al.,,
    [2020](#bib.bib40); Kim et al.,, [2022](#bib.bib36)) and code completion (Austin
    et al.,, [2021](#bib.bib4); Biderman and Raff,, [2022](#bib.bib8)), to name a
    few. When fine-tuned with human feedback, LLMs obtain higher ratings on desiderata
    like helpfulness (Ouyang et al.,, [2022](#bib.bib47); Bai et al.,, [2022](#bib.bib6)),
    and are proposed as conversational agents (Thoppilan et al.,, [2022](#bib.bib63)).
    Despite the widespread use of LLMs as conversational agents, there has been limited
    evaluation of their ability to navigate contextual commonsense knowledge.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种下游任务上展现了显著的能力，例如规划（Huang et al., [2022](#bib.bib28)）、常识推理（Kojima
    et al., [2022](#bib.bib37)）、信息检索（Lewis et al., [2020](#bib.bib40); Kim et al.,
    [2022](#bib.bib36)）和代码完成（Austin et al., [2021](#bib.bib4); Biderman and Raff,
    [2022](#bib.bib8)）等。通过人类反馈进行微调时，LLMs 在有用性（Ouyang et al., [2022](#bib.bib47); Bai
    et al., [2022](#bib.bib6)）等期望指标上获得了更高的评分，并被提议作为对话代理（Thoppilan et al., [2022](#bib.bib63)）。尽管
    LLMs 作为对话代理的使用已相当广泛，但对其在处理上下文常识知识方面的能力评估仍然有限。
- en: 'This raises an important question: to what extent can large language models
    resolve conversational implicature? To answer this question we use a public dataset
    of conversational implicatures and propose an evaluation protocol on top of it
    (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")). We evaluate
    a range of state-of-the-art models that can be categorised into four groups; large-scale
    pre-trained models, like OPT (Zhang et al.,, [2022](#bib.bib71)), LLMs fine-tuned
    on conversational data, like BlenderBot (Ng et al.,, [2019](#bib.bib46)), LLMs
    fine-tuned on common NLP benchmarks with natural instructions for each benchmark,
    like Flan-T5 (Chung et al.,, [2022](#bib.bib12)), and LLMs fine-tuned on tasks
    with natural instructions for each example, e.g. versions of OpenAI’s InstructGPT-3
    series³³3The precise method is unpublished and differs from the original instructGPT
    (Ouyang et al.,, [2022](#bib.bib47)).. Our results show that implicature resolution
    is a challenging task for LLMs. All pre-trained models obtain close to random
    zero-shot accuracy (around 60%), whereas humans obtain 86%. However, our results
    suggest that instruction-tuning at the example level is important for pragmatic
    understanding. Models fine-tuned with this method perform much better than others,
    and analysis of different model sizes shows that they have the best scaling properties.
    We further push performance for these models with chain-of-thought prompting,
    and find that one model in the group (GPT-4) reaches human-level performance.
    In summary, we conclude that pragmatic understanding has not yet arisen from large-scale
    pre-training *on its own*, but scaling analysis shows that it might for much larger
    scale. Fine-tuning on conversational data or benchmark-level instructions does
    not produce models with pragmatic understanding. However, fine-tuning on instructions
    at the example-level is a fruitful path towards more useful models of human discourse.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '这提出了一个重要的问题：大型语言模型在解决会话含义方面能够做到多大程度？为回答这个问题，我们使用了一个公共的会话含义数据集，并在其基础上提出了一种评估协议（图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")）。我们评估了一系列最先进的模型，这些模型可以分为四组：大规模预训练模型，如
    OPT（Zhang et al., [2022](#bib.bib71)），在对话数据上微调的 LLMs，如 BlenderBot（Ng et al., [2019](#bib.bib46)），在自然指令的常见
    NLP 基准上微调的 LLMs，如 Flan-T5（Chung et al., [2022](#bib.bib12)），以及在每个示例上有自然指令的任务上微调的
    LLMs，例如 OpenAI 的 InstructGPT-3 系列版本³³3该方法的具体细节未公开，且与原始的 instructGPT（Ouyang et
    al., [2022](#bib.bib47)）有所不同。我们的结果表明，含义解析对于 LLMs 来说是一项具有挑战性的任务。所有预训练模型的零-shot
    准确率接近随机（约 60%），而人类的准确率为 86%。然而，我们的结果表明，在示例级别的指令微调对于语用理解至关重要。采用这种方法微调的模型表现远好于其他模型，且不同模型规模的分析显示它们具有最佳的扩展特性。我们进一步通过思维链提示来提升这些模型的性能，并发现其中一个模型（GPT-4）达到了人类水平的表现。总之，我们得出的结论是，语用理解尚未通过大规模预训练*自身*产生，但扩展分析表明，针对更大规模的预训练可能会实现。对话数据或基准级别指令的微调并未产生具备语用理解的模型。然而，在示例级别上的指令微调是一条通向更有用的人类话语模型的有前途的路径。'
- en: 'The main contributions of this work are: i) we motivate implicature understanding
    as a crucial aspect of communication that is currently mostly missing from evaluations
    of LLMs, ii) we design an implicature resolution task and propose a comprehensive
    evaluation protocol on which we evaluate both humans and LLMs to find that it
    poses a significant challenge for SotA LLMs, and iii) we provide a thorough analysis
    of the results and identify one fine-tuning strategy (instruction-tuning at the
    example-level) as a promising method that produces models with more pragmatic
    understanding.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献包括：i) 我们将隐含意义理解作为交流中的一个关键方面进行探讨，这在当前的LLM评估中大多缺失；ii) 我们设计了一个隐含意义解决任务，并提出了一个全面的评估协议，在此协议下我们对人类和LLM进行评估，发现这对现有最先进的LLM构成了重大挑战；iii)
    我们对结果进行了深入分析，并确定一种微调策略（示例级指令微调）作为一种有前景的方法，这种方法能生成具有更多实用理解的模型。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs have demonstrated remarkable performance on tasks for which they were not
    explicitly trained (Brown et al.,, [2020](#bib.bib11)). Building on the hypothesis
    that these abilities arise due to implicit multitask learning (Radford et al.,,
    [2019](#bib.bib52)), the recent works of Sanh et al., ([2022](#bib.bib57)) and
    Wei et al., ([2022](#bib.bib67)) explicitly train LLMs in a supervised multitask
    fashion, leading to models that are better zero-shot learners with fewer parameters.
    Besides rapidly saturating language understanding benchmarks (Kiela et al.,, [2021](#bib.bib34)),
    these advancements make LLMs beneficial foundations for agents performing a plethora
    of tasks (Adolphs et al.,, [2022](#bib.bib1); Reed et al.,, [2022](#bib.bib54)).
    The trend towards using these models as agents brings along with it increased
    urgency for alignment with human values (Kenton et al.,, [2021](#bib.bib33)).
    However, larger models trained with next-word prediction are generally more toxic
    and unhelpful (Gehman et al.,, [2020](#bib.bib20); Bender et al.,, [2021](#bib.bib7);
    Lin et al.,, [2022](#bib.bib42)). Recent work mitigates this with methods like
    prompting and finetuning on human-annotated outputs (Askell et al.,, [2021](#bib.bib3);
    Ouyang et al.,, [2022](#bib.bib47); Thoppilan et al.,, [2022](#bib.bib63)). The
    produced models are more aligned on desiderata such as informativeness when evaluated
    by dedicated benchmarks and humans. We argue, however, that there is still something
    missing in these benchmarks. What is helpful and informative, as Kasirzadeh and
    Gabriel, ([2022](#bib.bib31)) also point out, depends on the context in which
    a conversation is held. Consequently, any application that requires communicating
    with humans will rely on pragmatic communication skills—something that is not
    explicitly captured by the benchmarks used to evaluate the alignment of LLMs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在未明确训练的任务上表现出了卓越的性能（Brown et al., [2020](#bib.bib11)）。基于这些能力因隐式多任务学习（Radford
    et al., [2019](#bib.bib52)）而产生的假设，Sanh et al. ([2022](#bib.bib57)) 和 Wei et al.
    ([2022](#bib.bib67)) 的近期工作明确地以监督多任务的方式训练LLM，从而生成了具有更少参数的更佳零-shot学习模型。除了迅速饱和语言理解基准（Kiela
    et al., [2021](#bib.bib34)），这些进展使得LLM成为执行大量任务的代理的有益基础（Adolphs et al., [2022](#bib.bib1)；Reed
    et al., [2022](#bib.bib54)）。将这些模型作为代理的趋势带来了与人类价值观对齐的紧迫性（Kenton et al., [2021](#bib.bib33)）。然而，使用下一词预测训练的大型模型通常更具毒性和无帮助（Gehman
    et al., [2020](#bib.bib20)；Bender et al., [2021](#bib.bib7)；Lin et al., [2022](#bib.bib42)）。近期工作通过提示和对人类标注输出的微调等方法来缓解这一问题（Askell
    et al., [2021](#bib.bib3)；Ouyang et al., [2022](#bib.bib47)；Thoppilan et al.,
    [2022](#bib.bib63)）。这些生成的模型在通过专门基准和人类评估时在信息量等期望方面表现得更为对齐。然而，我们认为这些基准中仍然存在一些缺失。如Kasirzadeh和Gabriel
    ([2022](#bib.bib31)) 所指出的，什么是有用和信息丰富的取决于对话发生的背景。因此，任何需要与人类沟通的应用将依赖于实用的沟通技巧——这是评估LLM对齐的基准中没有明确捕捉到的。
- en: There is a large body of work that investigates the interplay between pragmatics
    and computational modeling (Cianflone et al.,, [2018](#bib.bib13); Schuster et al.,,
    [2020](#bib.bib59); Louis et al.,, [2020](#bib.bib44); Kim et al.,, [2021](#bib.bib35);
    Li et al.,, [2021](#bib.bib41); Jeretic et al.,, [2020](#bib.bib30); Parrish et al.,,
    [2021](#bib.bib48); Hosseini et al.,, [2023](#bib.bib27)). Cianflone et al., ([2018](#bib.bib13))
    introduce the task of predicting adverbial presupposition triggers, which are
    words like ‘again’ that trigger the unspoken presupposition that an event has
    happened before. Schuster et al., ([2020](#bib.bib59)) study the ability of computational
    models to do scalar inferences, finding that models use linguistic features to
    make pragmatic inferences. Kim et al., ([2021](#bib.bib35)) find that a substantial
    part of question-answering datasets contain questions that are unanswerable due
    to false presuppositions (i.e. “which linguist invented the lightbulb”). Hosseini
    et al., ([2023](#bib.bib27)) present a dataset for selecting entities with indirect
    answers, and find that language models adapted for this task get reasonable accuracy,
    but that there is room for improvement. The difference with this body of work
    and ours is that we look at the emergence of pragmatic understanding from large-scale
    language modeling. Jeretic et al., ([2020](#bib.bib30)); Parrish et al., ([2021](#bib.bib48))
    are early works investigating the emergence of pragmatic understanding in pretrained
    language models, but they only look at scalar implicatures and presuppositions.
    Zheng et al., ([2021](#bib.bib72)) are the first to evaluate pretrained language
    models on conversational implicatures. This is important pioneering work highlighting
    the difficulty of implicature for language models, but their evaluations require
    task-specific training and the models they evaluate are relatively small. In contrast,
    our evaluation protocol is applicable out-of-the-box and is much more comprehensive,
    evaluating models up to 176 billion parameters and using in-context prompting.
    Additionally, Zheng et al., ([2021](#bib.bib72)) benchmark synthetic data whereas
    this work evaluates performance on naturally occurring implicatures (George and
    Mamidi,, [2020](#bib.bib21)). We believe this to be a better representation of
    the true distribution of implicatures in natural dialogue.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量的研究探讨了语用学与计算建模之间的相互作用（Cianflone et al., [2018](#bib.bib13); Schuster et al.,
    [2020](#bib.bib59); Louis et al., [2020](#bib.bib44); Kim et al., [2021](#bib.bib35);
    Li et al., [2021](#bib.bib41); Jeretic et al., [2020](#bib.bib30); Parrish et
    al., [2021](#bib.bib48); Hosseini et al., [2023](#bib.bib27)）。Cianflone et al.,
    ([2018](#bib.bib13)) 引入了预测副词性预设触发词的任务，这些词如“again”会引发一个事件之前发生过的未言明预设。Schuster et
    al., ([2020](#bib.bib59)) 研究了计算模型进行量化推理的能力，发现模型使用语言特征来进行语用推理。Kim et al., ([2021](#bib.bib35))
    发现相当大一部分问答数据集包含由于虚假预设而无法回答的问题（例如“哪位语言学家发明了电灯”）。Hosseini et al., ([2023](#bib.bib27))
    提出了一个选择间接答案的实体数据集，并发现适应于此任务的语言模型获得了合理的准确率，但仍有改进空间。这些研究与我们的不同之处在于，我们关注的是大规模语言建模中语用理解的出现。Jeretic
    et al., ([2020](#bib.bib30)); Parrish et al., ([2021](#bib.bib48)) 是早期研究预训练语言模型中语用理解出现的工作，但仅限于量化暗示和预设。Zheng
    et al., ([2021](#bib.bib72)) 是首个对对话暗示进行评估的预训练语言模型的研究。这是重要的开创性工作，突显了暗示对于语言模型的难度，但其评估需要特定任务的训练，并且他们评估的模型相对较小。相比之下，我们的评估协议是开箱即用的，并且更为全面，评估高达1760亿参数的模型并使用上下文提示。此外，Zheng
    et al., ([2021](#bib.bib72)) 基准测试了合成数据，而本研究评估了自然发生的暗示（George and Mamidi, [2020](#bib.bib21)）。我们认为这更能代表自然对话中暗示的真实分布。
- en: The standard set of benchmarks LLMs are evaluated on covers many tasks, but
    even though implicature is one of the most important aspects of language pragmatics
    (Levinson,, [1983](#bib.bib39)), it is only evaluated as part of BIG-bench (Srivastava
    et al.,, [2022](#bib.bib61)). Unfortunately, the methodology used by the BIG-bench
    implicature task contributors has limitations, which call into question the validity
    of their claims. Firstly, the task contributors discard a subset of the data that
    is ambiguous according to them. In our view this defeats the point of the benchmark.
    Implicatures are a type of non-literal, ambiguous language the intended meaning
    of which humans often easily interpret; comparing the way humans and models do
    this is precisely what we are interested in. In turn, we expect performance on
    the BIG-bench task to overestimate the ability of LLMs to resolve naturally occurring
    implicatures. We keep this challenging subset of the data and instead use human
    evaluation to deal with examples that are too ambiguous to understand. Secondly,
    the difference in performance between their average and best rater is 18%, whereas
    for our evaluations this difference is 6%. This indicates their human evaluation
    is of low quality, but it is impossible to verify because there are no details
    available on how the annotation is done. Finally, BIG-bench uses only base LLMs
    and no SotA fine-tuning methods. In summary, we use a more challenging dataset,
    and in turn at least six times more evaluations per model, we provide higher-quality
    human annotations, and evaluate four different categories of LLMs to investigate
    which aspects of LLMs contribute to their performance on implicature understanding.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs评估的标准基准涵盖了许多任务，但尽管含义是语言语用学中最重要的方面之一（Levinson,, [1983](#bib.bib39)），它仅作为BIG-bench的一部分进行评估（Srivastava
    et al.,, [2022](#bib.bib61)）。不幸的是，BIG-bench含义任务贡献者使用的方法存在局限性，这引发了对其声明有效性的质疑。首先，任务贡献者丢弃了他们认为模糊的数据子集。在我们看来，这违背了基准的初衷。含义是一种非字面、模糊的语言，人类通常可以轻松解读其意图的含义；比较人类和模型如何处理这种情况正是我们感兴趣的地方。因此，我们预计BIG-bench任务的表现会高估LLMs解决自然发生的含义的能力。我们保留这部分具有挑战的数据子集，改用人工评估来处理那些过于模糊以至于难以理解的示例。其次，他们的平均评估者与最佳评估者之间的表现差异为18%，而我们评估的差异为6%。这表明他们的人工评估质量较低，但由于没有关于注释过程的细节，因此无法验证。最后，BIG-bench只使用基础LLMs，没有使用最先进的微调方法。总之，我们使用了更具挑战性的数据集，并对每个模型进行了至少六倍的评估，提供了更高质量的人工注释，并评估了四种不同类别的LLMs，以调查哪些方面的LLMs有助于它们在含义理解上的表现。
- en: 3 Evaluation Protocol
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 评估协议
- en: 'Here we outline the evaluation protocol we use to answer the research question
    “To what extent can LLMs resolve conversational implicature?”. We focus on binary
    implicatures that imply “yes” or “no” (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")). We say a model resolves an implicature correctly
    if it assigns higher likelihood to a coherent utterance than a similar but incoherent
    one, detailed below.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们概述了用于回答研究问题“LLMs在多大程度上能够解决会话含义？”的评估协议。我们关注的是暗示“yes”或“no”的二元含义（见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")）。我们说一个模型正确地解决了含义，如果它给一个连贯的言论分配了比一个类似但不连贯的言论更高的可能性，详细情况见下文。'
- en: 'Zero-shot evaluation. Consider the example from the introduction packed into
    a single utterance:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot评估。考虑将介绍中的示例整合到一个话语中的情况：
- en: Esther asked “Can you come to my party on Friday?” and Juan responded “I have
    to work”, which means no.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Esther问：“你能来我周五的聚会吗？”而Juan回答：“我得工作”，这意味着不行。
- en: 'We can transform this example to be *pragmatically incoherent* (in the sense
    that it will become pragmatically inconsistent with expected use) by replacing
    the word “no” with “yes”:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将“no”替换为“yes”来将此示例转变为*实用上不连贯*（即它将与预期用途不一致）：
- en: Esther asked “Can you come to my party on Friday?” and Juan responded “I have
    to work”, which means yes.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Esther问：“你能来我周五的聚会吗？”而Juan回答：“我得工作”，这意味着可以。
- en: 'To resolve the implicature, the model should assign higher likelihood to the
    first of the two sentences above, namely the most coherent one. Importantly, both
    sentences have exactly the same words except for the binary implicature “yes”
    or “no”, making the assigned likelihood scores directly comparable. Formally,
    let the coherent prompt be $\mathbf{x}$. We say a model correctly resolves an
    example $\mathbf{x}$. This is equivalent to evaluating whether the model assigns
    a higher likelihood to the correct continuation of the two options. Note that
    this is a more lenient evaluation protocol than sometimes used for language models,
    where models are evaluated on on their ability to generate the correct continuation,
    in this case “no”. The greedy decoding approach (evaluating whether “yes” or “no”
    is generated) is also captured by our approach, but we additionally label an example
    correct if “no” is not the highest assigned likelihood, but still higher than
    “yes”. We did not opt for greedy decoding because “no” is not the only coherent
    continuation here, and marginalising over all possible correct continuations is
    intractable. The more lenient evaluation does capture implicature resolution,
    because the choice of “no” versus “yes” is only determined by the resolution of
    the implicature. We guide the models to output “yes” or “no” explicitly in three
    of the six prompt templates with instructions, such that we can estimate the effect
    of this guidance on performance. For two model classes (i.e. GPT-3.5-turbo and
    GPT-4) we do not have access to likelihoods, and for these models we take the
    greedy decoding approach, guiding the model to output “yes” or “no” explicitly
    in all prompts (see Table [6](#A6.T6 "Table 6 ‣ Appendix F Detailed prompt templates
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") in Appendix [F](#A6 "Appendix F Detailed prompt
    templates ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解析含义，模型应为上述两个句子中的第一个句子，即最连贯的那个，分配更高的可能性。重要的是，这两个句子除了二元含义“是”或“否”外，完全相同，因此分配的可能性分数可以直接比较。形式上，设连贯的提示为$\mathbf{x}$。我们说模型正确解析了一个示例$\mathbf{x}$。这等同于评估模型是否为两个选项中正确的继续分配了更高的可能性。请注意，这是一种比有时用于语言模型的评估协议更宽松的评估方法，其中模型会根据生成正确继续的能力进行评估，在本例中是“否”。贪婪解码方法（评估是否生成“是”或“否”）也被我们的方法所涵盖，但我们额外标记一个示例为正确，如果“否”不是最高分配的可能性，但仍高于“是”。我们没有选择贪婪解码，因为“否”并不是唯一的连贯延续，考虑所有可能的正确延续是不可行的。更宽松的评估确实捕捉到含义解析，因为“否”与“是”的选择仅由含义的解析决定。我们在六个提示模板中的三个模板中明确指导模型输出“是”或“否”，以便估计这种指导对性能的影响。对于两个模型类别（即GPT-3.5-turbo和GPT-4），我们无法访问可能性，因此对于这些模型我们采取贪婪解码方法，在所有提示中明确指导模型输出“是”或“否”（见表[6](#A6.T6
    "表 6 ‣ 附录 F 详细提示模板 ‣ 语用理解的金发姑娘：对含义解析的细调策略对LLM的影响")在附录[F](#A6 "附录 F 详细提示模板 ‣ 语用理解的金发姑娘：对含义解析的细调策略对LLM的影响")）。
- en: 'We use a dataset of conversational implicatures curated by George and Mamidi,
    ([2020](#bib.bib21))⁴⁴4Published under a CC BY 4.0 license.. It contains implicatures
    that, like in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Goldilocks of
    Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs"), are presented in utterance-response-implicature tuples. Of these, 718
    are binary implicatures that we can convert into an incoherent sentence. We randomly
    sample 600 examples for the test set and keep the remaining 118 as a development
    set to improve implicature resolution after pre-training through in-context prompting
    or fine-tuning.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了由George和Mamidi策划的会话含义数据集（[2020](#bib.bib21)）⁴⁴4，发布在CC BY 4.0许可证下。该数据集包含了类似于图[1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 语用理解的金发姑娘：对含义解析的细调策略对LLM的影响")中呈现的言语-回应-含义三元组的含义。在这些三元组中，有718个是二元含义，我们可以将它们转换为无意义的句子。我们随机抽取600个示例作为测试集，其余118个作为开发集，以便通过上下文提示或细调在预训练后改进含义解析。
- en: 'Few-shot in-context evaluation. We add $k$:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 少量样本上下文评估。我们添加$k$：
- en: 'Esther asked “Have you found him yet?” and Juan responded “They’re still looking”,
    which means no. Esther asked “Are you having fun?” and Juan responded “Is the
    pope Catholic?”, which means yes. Finish the following sentence: Esther asked
    “Can you come to my party on Friday?” and Juan responded “I have to work”, which
    means no.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Esther 问道：“你找到他了吗？”Juan 回答：“他们还在找”，这意味着没有。Esther 问道：“你玩得开心吗？”Juan 回答：“教皇是天主教徒吗？”，这意味着是的。完成以下句子：Esther
    询问：“你能来我周五的派对吗？”Juan 回答：“我必须工作”，这意味着不能。
- en: We evaluate the models’ $k$ examples from the development set. This requires
    $\frac{118!}{(118-k)!}$ evaluations for each test example, which is intractable.
    Instead, we estimate performance per test example by randomly sampling from the
    development set. In this way we control for some of the variance in performance,
    but avoid extra evaluations. We ensure each model sees the same few-shot examples
    per test example.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从开发集中评估模型的 $k$ 个示例。这需要对每个测试示例进行 $\frac{118!}{(118-k)!}$ 次评估，这在实际中不可行。相反，我们通过从开发集中随机抽样来估计每个测试示例的性能。通过这种方式，我们控制了一些性能的方差，但避免了额外的评估。我们确保每个模型在每个测试示例中看到相同的少量示例。
- en: 'Controlling for prompt sensitivity. It has been shown language models are sensitive
    to prompt wording (Efrat and Levy,, [2020](#bib.bib17); Tan et al.,, [2021](#bib.bib62);
    [Reynolds and McDonell, 2021a,](#bib.bib55) ; Webson and Pavlick,, [2021](#bib.bib66)).
    To control for this factor of randomness we manually curate six different template
    prompts and measure performance across these. One of the templates has been presented
    above, namely “Esther asked  and Juan responded , which means
    ”. Another template is: “Question: , response: ,
    meaning: ”. The former we call natural prompts and the latter structured
    prompts. Each group has three templates that only differ slightly in wording.
    This grouping allows us to look at the variance due to slight changes in wording
    as well as performance difference due to a completely different way of presenting
    the example. The full list of prompts can be found in Appendix [F](#A6 "Appendix
    F Detailed prompt templates ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 控制提示敏感性。已有研究表明，语言模型对提示词的措辞非常敏感（Efrat 和 Levy，， [2020](#bib.bib17)；Tan 等，， [2021](#bib.bib62)；[Reynolds
    和 McDonell, 2021a,](#bib.bib55)；Webson 和 Pavlick，， [2021](#bib.bib66)）。为了控制这种随机因素，我们手动策划了六种不同的模板提示，并在这些提示下测量性能。其中一个模板如上所示，即“Esther
    询问 ，Juan 回应 ，这意味着 ”。另一个模板是：“问题：，回应：，意义：”。前者我们称为自然提示，后者称为结构化提示。每组有三个模板，措辞略有不同。这种分组允许我们观察由于措辞细微变化引起的方差，以及由于完全不同的示例呈现方式引起的性能差异。完整的提示列表可以在附录
    [F](#A6 "附录 F 详细提示模板 ‣ 实用理解的金发姑娘：微调策略对 LLM 的隐含意义解析很重要") 中找到。
- en: 4 Experiments
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'The set of large language model classes we evaluate can be grouped into four
    distinct categories:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估的大型语言模型类别可以分为四个不同的类别：
- en: '1.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Base models: large-scale pre-trained models; RoBERTa (Liu et al.,, [2019](#bib.bib43)),
    BERT (Devlin et al.,, [2018](#bib.bib16)), GPT-2 (Radford et al.,, [2019](#bib.bib52)),
    EleutherAI (Wang and Komatsuzaki,, [2021](#bib.bib65); Black et al.,, [2022](#bib.bib10)),
    BLOOM (BigScience,, [2022](#bib.bib9)), OPT (Zhang et al.,, [2022](#bib.bib71)),
    Cohere’s base models, and GPT-3 (Brown et al.,, [2020](#bib.bib11))'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础模型：大规模预训练模型；RoBERTa（Liu 等，， [2019](#bib.bib43)），BERT（Devlin 等，， [2018](#bib.bib16)），GPT-2（Radford
    等，， [2019](#bib.bib52)），EleutherAI（Wang 和 Komatsuzaki，， [2021](#bib.bib65)；Black
    等，， [2022](#bib.bib10)），BLOOM（BigScience，， [2022](#bib.bib9)），OPT（Zhang 等，， [2022](#bib.bib71)），Cohere
    的基础模型，以及 GPT-3（Brown 等，， [2020](#bib.bib11)）
- en: '2.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Dialogue FT: LLMs fine-tuned on dialogue, BlenderBot (Ng et al.,, [2019](#bib.bib46)).'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话微调：对话微调的 LLM，BlenderBot（Ng 等，， [2019](#bib.bib46)）。
- en: '3.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Benchmark IT: LLMs fine-tuned on tasks with natural instructions for each benchmark
    or “benchmark-level instruction-tuned models”; T0 (Sanh et al.,, [2022](#bib.bib57))
    and Flan-T5 (Chung et al.,, [2022](#bib.bib12)).'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基准 IT：对每个基准任务进行自然指令微调的 LLM，或“基准级别指令微调模型”；T0（Sanh 等，， [2022](#bib.bib57)）和 Flan-T5（Chung
    等，， [2022](#bib.bib12)）。
- en: '4.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Example IT: LLMs fine-tuned on tasks with natural instructions for each example
    or “example-level instruction-tuned models”; a subset of OpenAI’s API models and
    Cohere’s API models).'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例 IT：对每个示例进行自然指令微调的 LLM，或“示例级别指令微调模型”；OpenAI 的 API 模型子集和 Cohere 的 API 模型。
- en: 'For Benchmark IT models, annotators write a single instruction for an entire
    dataset. The models are then fine-tuned on each example from the dataset with
    the same instruction. We distinguish this from example-level IT; for that type
    of fine-tuning each example in a dataset gets a new instruction, resulting in
    a more diverse dataset. Each group contains model classes for which we evaluate
    a range of sizes. A detailed categorization of the models and their attributes
    can be found in appendix [G](#A7 "Appendix G Model categorization ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs").⁵⁵5Note that there are several important aspects unknown for models
    behind APIs, like OpenAI’s model sizes. We make use of the OpenAI and Cohere APIs
    as well as the pretrained models in the transformers library (Wolf et al.,, [2020](#bib.bib70))
    and EleutherAI’s framework to evaluate them (Gao et al.,, [2021](#bib.bib19)).
    All code used for this paper can be found on GitHub⁶⁶6[https://github.com/LauraRuis/do-pigs-fly](https://github.com/LauraRuis/do-pigs-fly)
    and the dataset is made publicly available on HuggingFace⁷⁷7[https://huggingface.co/datasets/UCL-DARK/ludwig](https://huggingface.co/datasets/UCL-DARK/ludwig).
    Below, we present zero-shot and few-shot results, discussing patterns of performance
    of the models in the four different groups. We further look at the results for
    different model sizes of each model class and the variance over the prompt templates.
    We contrast the models’ performance with human performance. To this end, each
    test example gets annotated by five humans. We split the test set in four and
    assign each annotator a subset, leaving us with twenty annotators in total. The
    average human performance is 86.2%, and the best performance is 92%. Some of the
    errors humans make uncover examples that have multiple interpretations, and others
    uncover annotation errors. The nature of the task of implicature resolution means
    we do not expect models to perform better than human best performance. Details
    on the human experiment can be found in the Appendix [H](#A8 "Appendix H Human
    evaluation ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") (also containing an analysis of human errors),
    and detailed results per model and prompt template in Appendix [K.10](#A11.SS10
    "K.10 Detailed results per model ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs"). We also test for spurious correlations present in the benchmark (like
    lexical cues the model can rely on), and find no indication (Appendix [K.8](#A11.SS8
    "K.8 Testing for spurious correlations ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基准 IT 模型，标注者为整个数据集编写单一指令。然后，这些模型会在数据集中的每个示例上进行相同指令的微调。我们将其与示例级别 IT 区分开来；对于这种类型的微调，数据集中的每个示例都得到一个新指令，导致数据集更加多样化。每个组包含我们评估各种大小的模型类别。模型及其属性的详细分类可以在附录
    [G](#A7 "附录 G 模型分类 ‣ 实用理解的金发女孩：微调策略对含意解析的影响") 中找到。⁵⁵5请注意，像 OpenAI 模型大小这样的模型背后的重要方面尚未知晓。我们使用了
    OpenAI 和 Cohere APIs 以及 transformers 库中的预训练模型（Wolf 等，[2020](#bib.bib70)）和 EleutherAI
    的框架来进行评估（Gao 等，[2021](#bib.bib19)）。本文中使用的所有代码都可以在 GitHub⁶⁶6[https://github.com/LauraRuis/do-pigs-fly](https://github.com/LauraRuis/do-pigs-fly)
    找到，数据集在 HuggingFace⁷⁷7[https://huggingface.co/datasets/UCL-DARK/ludwig](https://huggingface.co/datasets/UCL-DARK/ludwig)
    上公开。以下是 zero-shot 和 few-shot 结果，我们讨论了模型在四个不同组中的表现模式。我们进一步查看了每个模型类别的不同模型大小和提示模板的方差。我们将模型的表现与人类表现进行对比。为此，每个测试示例由五位人类标注。我们将测试集分为四部分，并为每位标注者分配一个子集，总共留下二十位标注者。人类的平均表现为
    86.2%，最佳表现为 92%。一些人类犯的错误揭示了多种解释的示例，其他则揭示了注释错误。由于含意解析任务的性质，我们不期望模型的表现优于人类的最佳表现。有关人类实验的详细信息可以在附录
    [H](#A8 "附录 H 人类评价 ‣ 实用理解的金发女孩：微调策略对含意解析的影响")（也包含人类错误分析）中找到，以及每个模型和提示模板的详细结果在附录
    [K.10](#A11.SS10 "K.10 每个模型的详细结果 ‣ 附录 K 附加结果 ‣ 实用理解的金发女孩：微调策略对含意解析的影响") 中找到。我们还测试了基准中的虚假相关性（如模型可能依赖的词汇提示），未发现任何指示（附录
    [K.8](#A11.SS8 "K.8 测试虚假相关性 ‣ 附录 K 附加结果 ‣ 实用理解的金发女孩：微调策略对含意解析的影响")）。
- en: 'Table 1: The k-shot accuracy ($k\in\{0,1,5\}$ means size unknown.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：k-shot 准确率（$k\in\{0,1,5\}$ 意味着大小未知）。
- en: '|  | Model | 0-shot | 1-shot | 5-shot |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 0-shot | 1-shot | 5-shot |'
- en: '| Baselines and Toplines | Random | 50% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 基线和顶线 | 随机 | 50% |'
- en: '| Human avg. | 86.2% $\pm$ 2.3 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 人类平均 | 86.2% $\pm$ 2.3 |'
- en: '| Base models | BERT-110M | 54.8% $\pm$ 2.2 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 | BERT-110M | 54.8% $\pm$ 2.2 |'
- en: '| RoBERTa-355M | 55.6% $\pm$ 1.5 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa-355M | 55.6% $\pm$ 1.5 |'
- en: '| GPT-2-xl | 51.3% $\pm$ 1.1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2-xl | 51.3% $\pm$ 1.1 |'
- en: '| EleutherAI-20B | 57.5% $\pm$ 4.9 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20B | 57.5% $\pm$ 4.9 |'
- en: '| BLOOM-176B | 54.2% $\pm$ 3.4 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176B | 54.2% $\pm$ 3.4 |'
- en: '| OPT-13B | 61.0% $\pm$ 2.1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | 61.0% $\pm$ 2.1 |'
- en: '| Cohere-52B | 58.5% $\pm$ 2.9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52B | 58.5% $\pm$ 2.9 |'
- en: '|  | GPT-3-175B | 57.7% $\pm$ 1.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-3-175B | 57.7% $\pm$ 1.5 |'
- en: '| Dialogue FT | BlenderBot-2.7B | 53.4% $\pm$ 0.1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 对话 FT | BlenderBot-2.7B | 53.4% $\pm$ 0.1 |'
- en: '| Benchmark IT | T0-11B | 55.6% $\pm$ 0.2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 基准 IT | T0-11B | 55.6% $\pm$ 0.2 |'
- en: '| Flan-T5-11B | 60.8% $\pm$ 4.8 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11B | 60.8% $\pm$ 4.8 |'
- en: '| Example IT | text-davinci-001-$\star$ 1.0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 示例 IT | text-davinci-001-$\star$ 1.0 |'
- en: '| text-davinci-002-$\star$ 2.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-$\star$ 2.0 |'
- en: '|  | text-davinci-003-$\star$ 0.6 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | text-davinci-003-$\star$ 0.6 |'
- en: '|  | ChatGPT-$\star$ 6.3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | ChatGPT-$\star$ 6.3 |'
- en: '|  | GPT-4-$\star$  1.7 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4-$\star$  1.7 |'
- en: '|  | Cohere-command-52B | 60.2% $\pm$ 1.8 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | Cohere-command-52B | 60.2% $\pm$ 1.8 |'
- en: '![Refer to caption](img/2bb1bd805908ce51438adfd9d1c70f27.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2bb1bd805908ce51438adfd9d1c70f27.png)'
- en: 'Figure 2: The few-shot accuracy for the best model of each class (e.g. the
    best performing model in the class Cohere-command is the 52b model, whereas the
    best model in the class OPT is the 13b model). The bars show the group means.
    Models fine-tuned on example-level instructions perform better than most other
    models, especially for $$k>. For all models there is a significant gap
    between best accuracy and human accuracy (which is 86.2%). * means size unknown.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每个类别最佳模型的少样本准确率（例如，类别 Cohere-command 中表现最佳的模型是 52b 模型，而类别 OPT 中最佳的模型是 13b
    模型）。条形图显示了组均值。在示例级别指令上微调的模型表现优于大多数其他模型，特别是对于 $$k>。对于所有模型，最佳准确率与人类准确率（86.2%）之间存在显著差距。*
    表示大小未知。
- en: 'Insight 1: Models instruction-tuned at the example level outperform all others.
    Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs") shows the best
    0-, 1-, and 5-shot accuracy each model class achieved on the implicature task.
    The best overall accuracy is achieved by GPT-4 (the size of this model is unknown)
    at $82.3\%\pm 1.4$ and 68.7% by GPT-3-175b at $k=5$. We hypothesise that the lower
    performance which Cohere-command-52b achieves 0-shot is not due to a lack of implicature
    understanding, but due to a failure to calibrate the yes/no likelihoods without
    examples. For this model, we observe a sharp rise in performance from $k=0$ (see
    Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs") or Figure [2](#S4.F2
    "Figure 2 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")). Since it is unlikely that
    one example of an implicature induces pragmatic understanding, we hypothesise
    that few-shot prompting mostly serves to clarify the task format. We test this
    hypothesis in Appendix [K.6](#A11.SS6 "K.6 Experiment with random in-context labels
    ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") by repeating the 1- and
    5-shot experiment with random labels for Cohere-command-52B and text-davinci-001\.
    We find that the performance does not degrade, which confirms that the few-shot
    examples mainly serve to prime the model towards producing outputs following the
    yes/no structure.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '见解 1：在示例级别进行指令调优的模型优于所有其他模型。表格 [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") 显示了每种模型类别在隐含意义任务上的最佳 0-shot、1-shot 和 5-shot 准确率。总体最佳准确率由 GPT-4（该模型的大小未知）在
    $82.3\%\pm 1.4$ 达到，GPT-3-175b 在 $k=5$ 时的准确率为 68.7%。我们假设 Cohere-command-52b 在 0-shot
    下较低的表现并非由于对隐含意义理解不足，而是由于在没有示例的情况下未能校准是/否的可能性。对于该模型，我们观察到从 $k=0$ 开始性能急剧提升（见表 [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 或图 [2](#S4.F2 "Figure 2
    ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs")）。由于不太可能仅通过一个隐含意义的示例就能引发务实的理解，我们假设少量示例的提示主要用于明确任务格式。我们在附录
    [K.6](#A11.SS6 "K.6 Experiment with random in-context labels ‣ Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") 中通过对 Cohere-command-52B 和 text-davinci-001
    进行随机标签的 1-shot 和 5-shot 实验来测试这一假设。我们发现性能没有下降，这确认了少量示例主要用于使模型产生符合是/否结构的输出。'
- en: 'Insight 2: The results are robust to different prompt templates. As detailed
    in Section 3, each example in the test set is wrapped in six different prompt
    templates. The standard deviation in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") and in Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") shows the sensitivity to different prompt wording.
    The standard deviation ranges from 0.3 for BlenderBot to 7.0 for T0-11B. All in
    all, the sensitivity to prompt wording does not seem to be a problem for this
    task; when taking into account the confidence intervals the result remains that
    models in the group Example IT perform significantly better than all other models,
    but worse than humans. In Appendix [K.4](#A11.SS4 "K.4 The effect of in-context
    examples on sensitivity to prompt wording ‣ Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs") another analysis is presented that shows how different prompt
    templates benefit from in-context examples. The takeaway from the analysis is
    that in-context prompting can mitigate the fact that some models are better at
    natural prompts and others better at structured prompts by improving performance
    on the type of prompt the model struggles with zero-shot. Again, when only looking
    at the best prompt type for each model class (i.e. structured or natural), the
    results remain that models in the group Example IT perform best.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '洞察 2：结果对不同的提示模板具有鲁棒性。如第 3 节详细介绍的那样，测试集中每个示例都被包装在六种不同的提示模板中。表 [1](#S4.T1 "Table
    1 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs") 和图 [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") 中的标准差显示了对不同提示措辞的敏感性。标准差范围从 BlenderBot 的 0.3 到
    T0-11B 的 7.0。总的来说，对提示措辞的敏感性似乎对这个任务没有问题；考虑到置信区间的结果仍然表明，示例 IT 组中的模型在所有其他模型中表现显著更好，但仍不如人类。在附录
    [K.4](#A11.SS4 "K.4 The effect of in-context examples on sensitivity to prompt
    wording ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs") 中提供了另一项分析，展示了不同提示模板如何从上下文示例中受益。这项分析的结论是，上下文提示可以通过改善模型在零样本中遇到的提示类型的性能，从而减轻一些模型在自然提示上表现更好而其他模型在结构化提示上表现更好的事实。再次强调，当仅考虑每个模型类别的最佳提示类型（即结构化或自然提示）时，结果仍然表明示例
    IT 组中的模型表现最佳。'
- en: 'Insight 3: Models instruction-tuned at the example-level have the most favourable
    scaling properties, but some base models also show positive correlation with scale.
    Figure [3](#S4.F3 $$ we observe significant positive correlation with size for
    the models in the Example IT class for which we have multiple sizes (Cohere-command
    and ‘text--001’) as well as some models in the base model class. Not only
    do the models in the Example IT class exhibit higher performance for the same
    model size, these models also have a steeper performance increase with size than
    the base models. Comparing the scaling properties of the best base model (GPT-3)
    with Cohere-command, we see that the increase in performance from the second-largest
    to the largest model is 0.04% per billion parameters from GPT-3-6.7B to GPT-3-175B
    and 0.15% per billion parameters for Cohere-command-6B to Cohere-command-52b (exact
    numbers used to calculate the slope can be found in Appendix [K.10](#A11.SS10
    "K.10 Detailed results per model ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs")). If performance is linearly extrapolated from this curve GPT-3 reaches
    human-level performance at 642b parameters where Cohere-command would need 125b
    parameters.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '洞察 3：在示例级别上经过指令调优的模型具有最有利的扩展属性，但一些基础模型也显示出与规模的正相关性。图 [3](#S4.F3 $$ 我们观察到在示例
    IT 类模型中，模型大小与性能之间有显著的正相关性，这些模型有多个大小（Cohere-command 和 ‘text--001’），以及一些基础模型类中的模型。不仅示例
    IT 类模型在相同模型大小下表现更好，这些模型的性能增长速度也比基础模型更陡峭。将最佳基础模型（GPT-3）与 Cohere-command 进行比较，我们发现从第二大模型到最大模型的性能提升为每十亿参数
    0.04%，从 GPT-3-6.7B 到 GPT-3-175B，而 Cohere-command-6B 到 Cohere-command-52b 为每十亿参数
    0.15%（用于计算斜率的精确数字可以在附录 [K.10](#A11.SS10 "K.10 Detailed results per model ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 中找到）。如果根据这条曲线进行线性外推，GPT-3
    在 642b 参数时达到人类水平的性能，而 Cohere-command 则需要 125b 参数。'
- en: '![Refer to caption](img/57e5ddfae92d1ebb78dbf6b4350e4190.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57e5ddfae92d1ebb78dbf6b4350e4190.png)'
- en: 'Figure 3: Scaling results for the model classes of which we know the number
    of non-embedding parameters. The error bars show standard deviation over prompt
    templates. Cohere’s command models instruction-tuned at the example-level perform
    better than all other models. For all models there is still a significant gap
    between best accuracy and human accuracy.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对于我们知道非嵌入参数数量的模型类别的扩展结果。误差条表示在提示模板上的标准偏差。Cohere 的指令调优命令模型在示例级别上表现优于所有其他模型。所有模型之间仍存在最佳准确率和人类准确率之间的显著差距。
- en: 'Table 2: Scaling results for OpenAI’s text--001-series, for which we
    do not know the number of non-embedding parameters but do know the ordering in
    terms of size. The colors indicate whether going up in size (from left-to-right)
    increases performance significantly or not.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：OpenAI 的 text--001 系列的扩展结果，对于该系列我们不知道非嵌入参数的数量，但知道其大小排序。颜色指示了在大小上升（从左到右）是否显著提高性能。
- en: '| Engine | Ada | Babbage | Curie | Davinci |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 引擎 | Ada | Babbage | Curie | Davinci |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0-shot | 56.5% $\pm$ 2.8 (+3.3%) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 0-shot | 56.5% $\pm$ 2.8 (+3.3%) |'
- en: '| 5-shot | 57.6% $\pm$ 1.0 (+4.0%) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 5-shot | 57.6% $\pm$ 1.0 (+4.0%) |'
- en: 'Insight 4: GPT-4 reaches average human-level performance with chain-of-thought
    prompting. For the model groups that benefit from in-context examples, we attempt
    to push performance further with chain-of-thought prompting. We manually write
    a five-shot chain-of-thought prompt for all six prompt templates, and evaluate
    model performance using this prompt. One of the six chain-of-thought prompts can
    be found in Table 4 in Appendix [F](#A6 "Appendix F Detailed prompt templates
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs"), and the other five are provided in the supplementary
    material. We only present the results for the group Example IT here, since CoT
    prompting did not improve performance for two of the base model classes we tried
    (see Appendix [K.7](#A11.SS7 "K.7 Chain-of-thought on base models ‣ Appendix K
    Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs")). Consequently, we decided not to
    apply this experiment to the other models in the base group to save compute costs.
    The results of are shown in Table [3](#S4.T3 "Table 3 ‣ 4 Experiments ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs"). We find that chain-of-thought prompting does not help for all models,
    but is nonetheless able to boost performance of GPT-4 to 86.5% $\pm$ 1.0\. This
    is on-par with average human-level performance, and slightly below human best
    performance at 89.8%. To illustrate how explicit reasoning helps implicature understanding,
    we highlight a CoT generated by GPT-4 for an example from the dataset that models
    persistently get wrong. “A: Is there a bus I can get to the station? B: You can’t
    rely on it”. The implicature is yes, there is a bus, you just cannot rely on it.
    GPT-4 five-shot gets this wrong for all six templates. With CoT it gets it right
    for five of six templates. The generated CoT for one template is the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 见解 4：GPT-4 在链式思维提示下达到了平均人类水平的性能。对于从上下文示例中受益的模型组，我们尝试通过链式思维提示进一步提高性能。我们为所有六个提示模板手动编写了一个五次链式思维提示，并使用该提示评估模型性能。一个链式思维提示见表
    4 附录 [F](#A6 "附录 F 详细提示模板 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")，另外五个提示在补充材料中提供。我们仅在此展示示例
    IT 组的结果，因为 CoT 提示没有提高我们尝试的两个基础模型类别的性能（见附录 [K.7](#A11.SS7 "K.7 基础模型上的链式思维 ‣ 附录
    K 额外结果 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")）。因此，我们决定不将此实验应用于基础组中的其他模型以节省计算成本。结果见表 [3](#S4.T3
    "表 3 ‣ 4 实验 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")。我们发现链式思维提示并不适用于所有模型，但仍能将 GPT-4 的性能提升至
    86.5% $\pm$ 1.0。这与平均人类水平的性能相当，略低于 89.8% 的人类最佳性能。为了说明显性推理如何帮助隐含意义理解，我们突出显示了 GPT-4
    为数据集中模型持续出错的一个示例生成的 CoT。“A：是否有公交车可以到车站？B：你不能依赖它。”隐含意义是：是的，有公交车，只是你不能依赖它。GPT-4
    五次提示在所有六个模板中都出错。使用 CoT，它在六个模板中的五个上得出正确答案。一个模板生成的 CoT 如下：
- en: 'Alice says ‘You can’t rely on it.’ Alice must be implying that there is a bus,
    but it may not be dependable or timely. This means the response to Bob’s question
    is yes, but with a caution about reliability. Answer: yes'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 爱丽丝说“你不能依赖它。”爱丽丝可能暗示有一辆公交车，但它可能不可靠或不准时。这意味着对鲍勃问题的回答是肯定的，但需要注意可靠性。回答：是
- en: 'More completions can be found in Appendix [J](#A10 "Appendix J Chain-of-thought
    completions by GPT-4 ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的补充内容可以在附录 [J](#A10 "附录 J GPT-4 的链式思维补充 ‣ 实用理解的金发女孩：微调策略对隐含义的解决至关重要") 中找到。
- en: 'Table 3: Results of the chain-of-thought (CoT) experiment for models in the
    group Example IT. The numbers between brackets show the difference in performance
    with the number on the same row one column to the left. Most models benefit from
    CoT-prompting, but not all. Additionally, GPT-4 reaches average human-level performance
    with CoT prompting. $\star$ means size unknown.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：示例 IT 组中模型的链式思维 (CoT) 实验结果。括号中的数字显示与同一行左侧一列的数字之间的性能差异。大多数模型从 CoT 提示中受益，但并非所有模型。除此之外，GPT-4
    在 CoT 提示下达到了平均人类水平的表现。$\star$ 表示大小未知。
- en: '| Model | 0-shot | 5-shot | 5-shot CoT |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 0-shot | 5-shot | 5-shot CoT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| text-davinci-001-$\star$ 2.6 (-7.2%) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-$\star$ 2.6 (-7.2%) |'
- en: '| text-davinci-002-$\star$ 0.8 (+0.5%) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-$\star$ 0.8 (+0.5%) |'
- en: '| text-davinci-003-$\star$ 0.6 (+4.0%) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-$\star$ 0.6 (+4.0%) |'
- en: '| ChatGPT-$\star$ 1.0 (+3.3%) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-$\star$ 1.0 (+3.3%) |'
- en: '| GPT-4-$\star$  1.0  (+4.5%) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-$\star$  1.0  (+4.5%) |'
- en: '| Cohere-command-52b | 60.2% $\pm$ 0.5 (-0.1%) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 60.2% $\pm$ 0.5 (-0.1%) |'
- en: 'Insight 5: Models often struggle with the same type of examples humans struggle
    with. We manually labeled 217 examples of the 600 examples in the test set according
    to a taxonomy. The remaining 383 examples do not fall as clearly within a category
    and are grouped together as type other. In Table [4](#S4.T4 "Table 4 ‣ 4 Experiments
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") the two types of examples that occur frequently
    in the dataset are exemplified. Generalised implicatures require little or no
    context to be understood. They are the simplest type of example in the test set,
    and generally imply the same thing (“some” almost always implies “not all”). Particularised
    implicatures, by contrast, do require context to be resolved. For example, from
    Table [4](#S4.T4 "Table 4 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs"), we need the
    context that it is undesirable to stay up late drinking when one has to get up
    early (see in Appendix [E](#A5 "Appendix E Background on implicature ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") for more on generalised vs. particularised). In these type of examples,
    the context needed to resolve it is different every time. We label three other
    types of implicatures in the dataset, but since the analysis of these examples
    does not show significant patterns, we present it in Appendix [K.9](#A11.SS9 "K.9
    Detailed results type label analysis ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs"). We show the accuracy broken down per example type for two models from
    the Example IT group, as these patterns hold more broadly for almost all models
    evaluated (see the detailed results broken down per example type in Appendix [K.9](#A11.SS9
    "K.9 Detailed results type label analysis ‣ Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")). Figure [4](#S4.F4 "Figure 4 ‣ 4 Experiments ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") shows that for lower $k$. From the bottom row in Figure [4](#S4.F4 "Figure
    4 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs") we observe that the edge GPT-4 has
    over Cohere-command-52b seems mostly driven by a higher accuracy on generalised
    examples. The accuracy on the particularised examples is comparable between those
    two models.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '见解5：模型通常在处理与人类相同类型的例子时会遇到困难。我们根据分类法对测试集中的600个例子中的217个进行了人工标记。其余的383个例子没有明确地归入某个类别，因此被归为“其他”类型。在表[4](#S4.T4
    "Table 4 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")中，数据集中经常出现的两种类型的例子被举例说明。广义含义几乎不需要上下文即可理解。它们是测试集中最简单的例子，通常意味着相同的事物（“一些”几乎总是意味着“并非所有”）。相比之下，特定含义需要上下文来解决。例如，从表[4](#S4.T4
    "Table 4 ‣ 4 Experiments ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")中，我们需要上下文来理解当一个人需要早起时，熬夜喝酒是不可取的（更多关于广义与特定含义的信息请参见附录[E](#A5
    "Appendix E Background on implicature ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")）。在这些类型的例子中，解决所需的上下文每次都不同。我们在数据集中标记了三种其他类型的含义，但由于对这些例子的分析没有显示出显著模式，我们将其展示在附录[K.9](#A11.SS9
    "K.9 Detailed results type label analysis ‣ Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")中。我们展示了来自Example IT组的两个模型按例子类型划分的准确度，因为这些模式在几乎所有评估的模型中都广泛存在（见附录[K.9](#A11.SS9
    "K.9 Detailed results type label analysis ‣ Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")中的按例子类型划分的详细结果）。图[4](#S4.F4 "Figure 4 ‣ 4 Experiments ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")显示了较低$k$的情况。从图[4](#S4.F4 "Figure 4 ‣ 4 Experiments ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs")的底部行我们观察到，GPT-4相对于Cohere-command-52b的优势似乎主要是由于对广义例子的更高准确度。这两个模型在特定例子上的准确度相当。'
- en: '![Refer to caption](img/ae6e6a9282def6cd95e25f0abf400e95.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae6e6a9282def6cd95e25f0abf400e95.png)'
- en: 'Figure 4: The accuracy v. k for the generalised and particularised examples
    obtained by the Example IT models Cohere-command and GPT-4\. Particularised (context-heavy)
    examples are often significantly more difficult than generalised (context-free)
    examples for both models and humans. For most models, in-context prompting can
    mitigate this, but for others (like GPT-4), a significant gap remains. We see
    that Cohere-command-52b achieves similar performance as GPT-4 on the particularised
    examples, but significantly lower on the generalised examples.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：由Example IT模型Cohere-command和GPT-4获得的泛化和特化示例的准确性对比。特化（依赖上下文）示例通常比泛化（无上下文）示例更难对付，无论是对模型还是对人类。对于大多数模型，基于上下文的提示可以缓解这一问题，但对于其他一些模型（如GPT-4），差距仍然显著。我们看到Cohere-command-52b在特化示例上的表现与GPT-4相似，但在泛化示例上的表现则显著较低。
- en: 'Table 4: An example from the dataset for two types of implicature found in
    the test set. The rightmost column shows the amount of that type we manually found
    in the test set.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：数据集中两个类型的含义推测的示例。最右侧的列显示了我们在测试集中手动找到的这种类型的数量。
- en: '| Type | Example Utterance | Example Response | Impl. | # |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 示例发言 | 示例回应 | 含义推测 | 数量 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Generalised | You know all these people? | Some. | No. | 47 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 | 你认识这些人吗？ | 认识一些。 | 不 | 47 |'
- en: '| Particularised | Want to stay for a nightcap? | I’ve gotta get up early.
    | No. | 94 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 特化 | 想留在这里喝一杯吗？ | 我得早点起床。 | 不 | 94 |'
- en: 5 Discussion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 'In this study we use prompting to evaluate whether different groups of LLMs
    can resolve implicatures. In designing our experimental protocol, we carefully
    considered various alternatives, and here we discuss limitations of the chosen
    approach. Firstly, evaluating LLM competencies is inherently uncertain and sensitive
    to prompt choice. Nonetheless, we are confident our evaluation is comprehensive
    enough to assess implicature understanding: we apply six different prompt templates
    per test example, each used in three different prompting techniques (zero-shot,
    few-shot, chain-of-thought). Additionally, in the appendix we present alternative
    zero-shot prompts and task specifications (Appendix [K.3](#A11.SS3 "K.3 Different
    zero-shot instruction prompts ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") and [K.1](#A11.SS1 "K.1 Contrastive experiment ‣ Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") respectively), but since these did not improve
    performance they were not further considered. Another limitation is the fact that
    a subset of the models we evaluate are behind APIs. This means models are subject
    to change (affecting reproducibility) and certain details about these models are
    unknown. This affects the group instruction-tuned at the example-level, which
    is the group we find outperforms all others and has the most favourable scaling
    properties. How do we know instruction-tuning at the example-level is the main
    driver behind these findings without controlled A/B testing? Unfortunately, due
    to the secrecy surrounding the exact implementation of these models we cannot
    be certain, but we can be relatively confident. We evaluated ten models across
    six model classes and two APIs in the group example-level instruction tuned. Within
    this group, models probably vary significantly in other training and architecture
    details (especially Cohere-command models versus OpenAI models). The most salient
    commonality they share with each other and none of the other models is multi-task
    instruction-tuning at the example level, making it likely that this is the driving
    factor of their performance. A further datapoint in favour of this conclusion
    can be seen in Figure [3](#S4.F3 "Figure 3 ‣ 4 Experiments ‣ The Goldilocks of
    Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") (right); base models at similar scales as Example IT models perform
    significantly worse. We see that Cohere-command 52B significantly outperforms
    Cohere-base 52B, and the only difference between those models is instruction-tuning
    at the example level (Cohere-command is fine-tuned from Cohere-base). In fact,
    Cohere-command 52B outperforms other base models more than 3 times the size by
    a large margin (e.g. GPT-3 175B, BLOOM-176B, OPT-175B). We are therefore confident
    that instruction-tuning at the example-level is important for pragmatic understanding,
    an insight which can guide the development of open-source models capable of pragmatic
    understanding. Investigating the exact effect of this type of instruction-tuning
    on pragmatic understanding in a controlled setting is an interesting future work
    direction (e.g. by isolating the effect of data diversity from instructions).
    Another limitation is that some evaluations are subject to API stochasticity,
    which we address in Appendix [K.5](#A11.SS5 "K.5 Variance over API runs ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs"). After running the zero-shot
    experiment ten times through each API we conclude there is some stochasticity,
    but it is too small to impact our conclusions. We publish exact timestamps at
    which we queried APIs in Appendix [L](#A12 "Appendix L Timestamps API calls ‣
    The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs"). Further, a downside of doing a comprehensive analysis on
    many models is compute costs. In Appendix [M](#A13 "Appendix M Compute and Emissions
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") we publish a list of exact compute used (time
    and hardware), as well as estimated carbon emissions for each of the models that
    are not behind an API. Finally, the likelihood ranking approach we take limits
    our study to implicatures with clear alternative. However, implicatures in natural
    language can entail more complex propositions. For example, imagine Esther now
    asking “Can I use your stapler?” and Juan responding “Here’s the key to my office.”.
    Juan is implicating that (1) Esther can use the stapler, (2) the stapler is located
    in the office, and (3) the office is currently locked. This leaves ample room
    for the design of benchmarks with implicatures entailing multiple non-binary propositions.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们使用提示来评估不同的LLM群体是否能够解决隐含意义。在设计我们的实验协议时，我们仔细考虑了各种替代方案，以下是我们讨论所选方法的局限性。首先，评估LLM的能力本质上是不确定的，并且对提示选择非常敏感。尽管如此，我们对我们的评估足够全面以评估隐含意义理解充满信心：我们对每个测试示例应用了六种不同的提示模板，每种模板用于三种不同的提示技术（零样本、少样本、思维链）。此外，在附录中我们展示了其他零样本提示和任务规范（附录
    [K.3](#A11.SS3 "K.3 不同的零样本指令提示 ‣ 附录 K 额外结果 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响") 和
    [K.1](#A11.SS1 "K.1 对比实验 ‣ 附录 K 额外结果 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响")），但由于这些没有提高性能，因此没有进一步考虑。另一个局限性是我们评估的部分模型位于API后面。这意味着模型可能会发生变化（影响可重复性），并且这些模型的一些细节未知。这影响了在示例级别进行指令调优的组，这是我们发现表现优于其他所有组且具有最有利的扩展特性的组。没有控制的A/B测试，我们如何知道示例级别的指令调优是这些发现的主要驱动因素？不幸的是，由于这些模型确切实现的保密性，我们不能确定，但我们可以相对自信。我们在组示例级别指令调优中评估了十个模型，涵盖六种模型类别和两个API。在这一组中，模型在其他训练和架构细节上可能会有显著差异（尤其是Cohere-command模型与OpenAI模型）。它们与其他模型最显著的共同点是示例级别的多任务指令调优，这使得它们的表现驱动因素很可能是这一点。支持这一结论的另一个数据点可以在图
    [3](#S4.F3 "图 3 ‣ 4 实验 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响")（右侧）中看到；与示例IT模型规模相似的基础模型表现显著较差。我们看到Cohere-command
    52B显著优于Cohere-base 52B，这两个模型之间的唯一区别是示例级别的指令调优（Cohere-command是从Cohere-base微调的）。实际上，Cohere-command
    52B比其他基础模型（例如GPT-3 175B、BLOOM-176B、OPT-175B）大三倍还要好。我们因此相信示例级别的指令调优对语用理解至关重要，这一见解可以指导能够进行语用理解的开源模型的开发。在受控环境中研究这种类型的指令调优对语用理解的确切影响是一个有趣的未来研究方向（例如，通过将数据多样性的影响与指令隔离开来）。另一个局限性是一些评估受API随机性影响，我们在附录
    [K.5](#A11.SS5 "K.5 API运行的方差 ‣ 附录 K 额外结果 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响")中进行了讨论。经过在每个API上运行十次零样本实验后，我们得出结论存在一些随机性，但其影响过小，不会影响我们的结论。我们在附录
    [L](#A12 "附录 L 时间戳 API调用 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响")中发布了查询API的精确时间戳。此外，对许多模型进行全面分析的一个缺点是计算成本。在附录
    [M](#A13 "附录 M 计算和排放 ‣ 语用理解的金发女孩：微调策略对LLMs隐含意义解决的影响")中，我们发布了使用的确切计算量（时间和硬件），以及每个不在API后的模型的估计碳排放。最后，我们采用的可能性排名方法将我们的研究限制在具有明确替代方案的隐含意义。然而，自然语言中的隐含意义可能涉及更复杂的命题。例如，假设Esther现在问“我可以使用你的订书机吗？”而Juan回答“这是我办公室的钥匙。”Juan暗示（1）Esther可以使用订书机，（2）订书机在办公室里，（3）办公室目前锁着。这为设计包含多个非二元命题的隐含意义基准测试提供了充足的空间。
- en: 6 Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'LLMs have made remarkable progress on fluency and coherence in recent years.
    We argue however that a central aspect of language understanding is missing from
    evaluations. To understand language means to understand its pragmatics: its usage
    in a context that incorporates commonsense understanding, goals, objectives, and
    so on. We design a protocol that evaluates LLMs on binary implicature resolution
    and establish a significant gap with human understanding for SotA LLMs in three
    categories; large-scale pre-trained models, models fine-tuned on conversations,
    and models fine-tuned with benchmark-level instructions. By contrast, we find
    that models fine-tuned on example-level instructions perform significantly better.
    This group also exhibits the best correlation between accuracy and model size.
    Scaling analysis shows that for some large-scale pre-trained models accuracy also
    positively correlates with model size, but the best model in this group would
    need at least five times more parameters to reach similar performance. From these
    results, we conclude that instruction-tuning at the example level is important
    for pragmatic understanding. We hypothesise that there is something about the
    multi-task data diversity obtained from example-level instructions (i.e. each
    example a new task) that makes pragmatic understanding appear at smaller scale.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型在流畅性和连贯性方面取得了显著进展。然而，我们认为评估中缺少了一个关键的语言理解方面。理解语言意味着理解其语用学：在包含常识理解、目标、意图等的上下文中的使用。我们设计了一种协议来评估语言模型在二值含义推断上的表现，并在三个类别中发现了与人类理解的显著差距；大规模预训练模型、针对对话进行微调的模型和用基准级指令进行微调的模型。相比之下，我们发现针对示例级指令进行微调的模型表现显著更好。该组模型也表现出准确性与模型规模之间的最佳相关性。规模分析显示，对于一些大规模预训练模型，准确性也与模型规模正相关，但该组中的最佳模型需要至少五倍的参数才能达到类似的性能。根据这些结果，我们得出结论，示例级指令的微调对语用理解至关重要。我们假设，示例级指令获得的多任务数据多样性（即每个示例都是一个新任务）使得语用理解在较小规模上显现。
- en: Acknowledgements
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank members of the UCL DARK lab, members of the UCL NLP group,
    Pontus Stenetorp, Sebastian Borgeaud, Philipp Jettkant, Robert Kirk, and Max Bartolo
    for fruitful discussions and comments on an earlier draft of this paper. We would
    also like to thank the anonymous reviewers for engaging actively in discussion
    and providing feedback that has improved this work. This work was supported by
    the EPSRC Grant EP/S021566/1 and UCL International Scholar Award for Doctoral
    Training Centres.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢UCL DARK实验室的成员、UCL NLP组的成员、Pontus Stenetorp、Sebastian Borgeaud、Philipp
    Jettkant、Robert Kirk和Max Bartolo对本文早期草稿的富有成效的讨论和评论。我们还要感谢匿名审稿人积极参与讨论并提供了改进此工作的反馈。此项工作得到了EPSRC资助EP/S021566/1和UCL国际博士生奖学金的支持。
- en: References
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adolphs et al., (2022) Adolphs, L., Börschinger, B., Buck, C., Huebscher, M. C.,
    Ciaramita, M., Espeholt, L., Hofmann, T., Kilcher, Y., Rothe, S., Sessa, P. G.,
    and Sestorain, L. (2022). Boosting search engines with interactive agents. Transactions
    on Machine Learning Research.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adolphs 等，(2022) Adolphs, L., Börschinger, B., Buck, C., Huebscher, M. C., Ciaramita,
    M., Espeholt, L., Hofmann, T., Kilcher, Y., Rothe, S., Sessa, P. G., 和 Sestorain,
    L. (2022). 利用互动代理增强搜索引擎。机器学习研究学报。
- en: 'American Psychiatric Association, (2013) American Psychiatric Association,
    A. P. A. (2013). Diagnostic and statistical manual of mental disorders : DSM-5.
    American Psychiatric Association Arlington, VA, 5th ed. edition.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国精神病学会，(2013) 美国精神病学会，A. P. A. (2013). 精神疾病诊断与统计手册：DSM-5。美国精神病学会阿灵顿，VA，第5版。
- en: Askell et al., (2021) Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D.,
    Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds,
    Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T. B.,
    Clark, J., McCandlish, S., Olah, C., and Kaplan, J. (2021). A general language
    assistant as a laboratory for alignment. CoRR, abs/2112.00861.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell 等，(2021) Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan,
    T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds,
    Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T.
    B., Clark, J., McCandlish, S., Olah, C., 和 Kaplan, J. (2021). 一般语言助手作为对齐实验室。CoRR,
    abs/2112.00861.
- en: Austin et al., (2021) Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
    H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. (2021). Program synthesis
    with large language models. arXiv preprint arXiv:2108.07732.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al., (2021) Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski,
    H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., 等 (2021). 使用大型语言模型进行程序合成。arXiv
    预印本 arXiv:2108.07732。
- en: Bach, (1999) Bach, K. (1999). The myth of conventional implicature. Linguistics
    and Philosophy, 22(4):327–366.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bach, (1999) Bach, K. (1999). 传统隐含意义的神话。语言学与哲学, 22(4):327–366。
- en: Bai et al., (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma,
    N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022). Training a
    helpful and harmless assistant with reinforcement learning from human feedback.
    arXiv preprint arXiv:2204.05862.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al., (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma,
    N., Drain, D., Fort, S., Ganguli, D., Henighan, T., 等 (2022). 使用来自人类反馈的强化学习训练有用且无害的助手。arXiv
    预印本 arXiv:2204.05862。
- en: 'Bender et al., (2021) Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell,
    S. (2021). On the dangers of stochastic parrots: Can language models be too big?
    In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,
    pages 610–623.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender et al., (2021) Bender, E. M., Gebru, T., McMillan-Major, A., 和 Shmitchell,
    S. (2021). 关于随机鹦鹉的危险：语言模型能否过大？在 2021 年 ACM 公平性、问责制与透明度会议论文集中, 页码 610–623。
- en: Biderman and Raff, (2022) Biderman, S. and Raff, E. (2022). Fooling moss detection
    with pretrained language models. arXiv preprint arXiv:2201.07406.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman 和 Raff, (2022) Biderman, S. 和 Raff, E. (2022). 用预训练语言模型欺骗苔藓检测。arXiv
    预印本 arXiv:2201.07406。
- en: BigScience, (2022) BigScience (2022). Bigscience language open-science open-access
    multilingual (bloom) language model.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigScience, (2022) BigScience (2022). Bigscience 语言开放科学开放访问多语言 (bloom) 语言模型。
- en: 'Black et al., (2022) Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,
    L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth,
    U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S. (2022).
    GPT-NeoX-20B: An open-source autoregressive language model. In Proceedings of
    the ACL Workshop on Challenges & Perspectives in Creating Large Language Models.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Black et al., (2022) Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,
    L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth,
    U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., 和 Weinbach, S. (2022). GPT-NeoX-20B:
    一个开源自回归语言模型。收录于 ACL 会议论文集：创建大型语言模型的挑战与前景工作坊。'
- en: Brown et al., (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information
    Processing Systems, volume 33, pages 1877–1901\. Curran Associates, Inc.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al., (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.
    D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,
    Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I.,
    和 Amodei, D. (2020). 语言模型是少样本学习者。收录于 Larochelle, H., Ranzato, M., Hadsell, R.,
    Balcan, M., 和 Lin, H. 主编, 《神经信息处理系统进展》，第 33 卷，页码 1877–1901。Curran Associates,
    Inc.
- en: Chung et al., (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
    Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S.,
    Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A.,
    Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin,
    J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. (2022). Scaling instruction-finetuned
    language models.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al., (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
    Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S.,
    Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A.,
    Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin,
    J., Roberts, A., Zhou, D., Le, Q. V., 和 Wei, J. (2022). 扩展指令微调语言模型。
- en: 'Cianflone et al., (2018) Cianflone, A., Feng, Y., Kabbara, J., and Cheung,
    J. C. K. (2018). Let’s do it “again”: A first computational approach to detecting
    adverbial presupposition triggers. In Proceedings of the 56th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 2747–2755,
    Melbourne, Australia. Association for Computational Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cianflone et al., (2018) Cianflone, A., Feng, Y., Kabbara, J., 和 Cheung, J.
    C. K. (2018). 让我们“再做一次”：第一次计算方法检测副词预设触发器。收录于第 56 屆計算語言學協會年會论文集（第 1 卷：长篇论文），页码
    2747–2755，澳大利亚墨尔本。计算语言学协会。
- en: Davis, (2019) Davis, W. (2019). Implicature. In Zalta, E. N., editor, The Stanford
    Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, fall
    2019 edition.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis, (2019) Davis, W. (2019). 含意。在 Zalta, E. N., 编辑，《斯坦福哲学百科全书》。形而上学研究实验室，斯坦福大学，2019年秋季版。
- en: 'Davis, (1998) Davis, W. A. (1998). Implicature : intention, convention, and
    principle in the failure of Gricean theory / Wayne A. Davis. Cambridge studies
    in philosophy. Cambridge University Press, Cambridge England ; New York.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davis, (1998) Davis, W. A. (1998). 含意：意图、惯例与格赖斯理论的失败 / Wayne A. Davis。剑桥哲学研究。剑桥大学出版社，剑桥英格兰；纽约。
- en: 'Devlin et al., (2018) Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    CoRR, abs/1810.04805.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al., (2018) Devlin, J., Chang, M., Lee, K., 和 Toutanova, K. (2018).
    BERT：深度双向变换器的预训练用于语言理解。CoRR, abs/1810.04805。
- en: 'Efrat and Levy, (2020) Efrat, A. and Levy, O. (2020). The turking test: Can
    language models understand instructions? CoRR, abs/2010.11982.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Efrat and Levy, (2020) Efrat, A. 和 Levy, O. (2020). Turking测试：语言模型能否理解指令？CoRR,
    abs/2010.11982。
- en: Frank and Goodman, (2012) Frank, M. C. and Goodman, N. D. (2012). Predicting
    pragmatic reasoning in language games. Science, 336:998 – 998.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frank and Goodman, (2012) Frank, M. C. 和 Goodman, N. D. (2012). 预测语言游戏中的语用推理。《科学》，336:998
    – 998。
- en: Gao et al., (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for
    few-shot language model evaluation.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al., (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. (2021). 少量样本语言模型评估框架。
- en: 'Gehman et al., (2020) Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith,
    N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language
    models. In Findings of the Association for Computational Linguistics: EMNLP 2020,
    pages 3356–3369, Online. Association for Computational Linguistics.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehman et al., (2020) Gehman, S., Gururangan, S., Sap, M., Choi, Y., 和 Smith,
    N. A. (2020). RealToxicityPrompts：评估语言模型中的神经毒性退化。在《计算语言学协会发现：EMNLP 2020》中，页码 3356–3369，在线。计算语言学协会。
- en: 'George and Mamidi, (2020) George, E. J. and Mamidi, R. (2020). Conversational
    implicatures in english dialogue: Annotated dataset. Procedia Computer Science,
    171:2316–2323. https://doi.org/10.1016/j.procs.2020.04.251.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: George and Mamidi, (2020) George, E. J. 和 Mamidi, R. (2020). 英语对话中的会话含意：注释数据集。《计算机科学学报》，171:2316–2323。https://doi.org/10.1016/j.procs.2020.04.251。
- en: Glaese et al., (2022) Glaese, A., McAleese, N., Trebacz, M., Aslanides, J.,
    Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham,
    L., Uesato, J., Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S.,
    Greig, R., Chen, C., Fritz, D., Sanchez Elias, J., Green, R., Mokrá, S., Fernando,
    N., Wu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis,
    D., Kavukcuoglu, K., Hendricks, L. A., and Irving, G. (2022). Improving alignment
    of dialogue agents via targeted human judgements.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glaese et al., (2022) Glaese, A., McAleese, N., Trebacz, M., Aslanides, J.,
    Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham,
    L., Uesato, J., Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S.,
    Greig, R., Chen, C., Fritz, D., Sanchez Elias, J., Green, R., Mokrá, S., Fernando,
    N., Wu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis,
    D., Kavukcuoglu, K., Hendricks, L. A., 和 Irving, G. (2022). 通过有针对性的人工判断改进对话代理的对齐。
- en: Goodman and Frank, (2016) Goodman, N. and Frank, M. (2016). Pragmatic language
    interpretation as probabilistic inference. Trends in Cognitive Sciences, 20.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman and Frank, (2016) Goodman, N. 和 Frank, M. (2016). 语用语言解释作为概率推理。《认知科学趋势》，20。
- en: Green, (1996) Green, G. (1996). Pragmatics and Natural Language Understanding.
    Tutorial essays in cognitive science. Erlbaum.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Green, (1996) Green, G. (1996). 《语用学与自然语言理解》。认知科学教程论文集。Erlbaum。
- en: 'Greene and Resnik, (2009) Greene, S. and Resnik, P. (2009). More than words:
    Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies:
    The 2009 Annual Conference of the North American Chapter of the Association for
    Computational Linguistics, pages 503–511, Boulder, Colorado. Association for Computational
    Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greene and Resnik, (2009) Greene, S. 和 Resnik, P. (2009). 超越词汇：句法包装与隐含情感。在《人类语言技术会议论文集：2009年北美计算语言学协会年会》中，页码
    503–511，博尔德，科罗拉多州。计算语言学协会。
- en: 'Grice, (1975) Grice, H. P. (1975). Logic and conversation. In Cole, P. and
    Morgan, J. L., editors, Syntax and Semantics: Vol. 3: Speech Acts, pages 41–58\.
    Academic Press, New York.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grice（1975年）Grice, H. P.（1975年）。逻辑与对话。在 Cole, P. 和 Morgan, J. L., 编辑，《句法与语义学：第3卷：言语行为》，第41–58页。学术出版社，纽约。
- en: 'Hosseini et al., (2023) Hosseini, M. J., Radlinski, F., Pareti, S., and Louis,
    A. (2023). Resolving indirect referring expressions for entity selection. In Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 12313–12335, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini 等人（2023年）Hosseini, M. J., Radlinski, F., Pareti, S., 和 Louis, A.（2023年）。解决间接指称表达以进行实体选择。在第61届计算语言学协会年会（第1卷：长篇论文）论文集中，第12313–12335页，多伦多，加拿大。计算语言学协会。
- en: 'Huang et al., (2022) Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. (2022).
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. arXiv preprint arXiv:2201.07207.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2022年）Huang, W., Abbeel, P., Pathak, D., 和 Mordatch, I.（2022年）。语言模型作为零样本规划器：为具身代理提取可操作的知识。arXiv
    预印本 arXiv:2201.07207。
- en: Huang, (2017) Huang, Y. (2017). The Oxford Handbook of Pragmatics. Oxford handbooks
    in linguistics. Oxford University Press.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang（2017年）Huang, Y.（2017年）。《牛津语用学手册》。牛津语言学手册。牛津大学出版社。
- en: Jeretic et al., (2020) Jeretic, P., Warstadt, A., Bhooshan, S., and Williams,
    A. (2020). Are natural language inference models IMPPRESsive? Learning IMPlicature
    and PRESupposition. In Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics, pages 8690–8705, Online. Association for Computational
    Linguistics.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeretic 等人（2020年）Jeretic, P., Warstadt, A., Bhooshan, S., 和 Williams, A.（2020年）。自然语言推理模型是否令人印象深刻？学习隐含意和预设。在第58届计算语言学协会年会论文集中，第8690–8705页，在线。计算语言学协会。
- en: 'Kasirzadeh and Gabriel, (2022) Kasirzadeh, A. and Gabriel, I. (2022). In conversation
    with artificial intelligence: aligning language models with human values.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasirzadeh 和 Gabriel（2022年）Kasirzadeh, A. 和 Gabriel, I.（2022年）。与人工智能对话：将语言模型与人类价值观对齐。
- en: Katsos et al., (2011) Katsos, N., Roqueta, C. A., Estevan, R. A. C., and Cummins,
    C. (2011). Are children with specific language impairment competent with the pragmatics
    and logic of quantification? Cognition, 119(1):43–57.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katsos 等人（2011年）Katsos, N., Roqueta, C. A., Estevan, R. A. C., 和 Cummins, C.（2011年）。具有特定语言障碍的儿童在量化的语用学和逻辑方面是否具备能力？认知，119(1)：43–57。
- en: Kenton et al., (2021) Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik,
    V., and Irving, G. (2021). Alignment of language agents. CoRR, abs/2103.14659.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kenton 等人（2021年）Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik,
    V., 和 Irving, G.（2021年）。语言代理的对齐。CoRR，abs/2103.14659。
- en: 'Kiela et al., (2021) Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger,
    A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., Ma, Z., Thrush, T.,
    Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C., and Williams,
    A. (2021). Dynabench: Rethinking benchmarking in nlp. In Proceedings of the 2021
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, pages 4110–4124, Online. Association
    for Computational Linguistics.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiela 等人（2021年）Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu,
    Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., Ma, Z., Thrush, T., Riedel,
    S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C., 和 Williams, A.（2021年）。Dynabench：重新思考
    NLP 的基准测试。在第2021届北美计算语言学协会人类语言技术会议论文集中，第4110–4124页，在线。计算语言学协会。
- en: 'Kim et al., (2021) Kim, N., Pavlick, E., Karagol Ayan, B., and Ramachandran,
    D. (2021). Which linguist invented the lightbulb? presupposition verification
    for question-answering. In Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 3932–3945, Online. Association
    for Computational Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2021年）Kim, N., Pavlick, E., Karagol Ayan, B., 和 Ramachandran, D.（2021年）。哪位语言学家发明了灯泡？问题回答的预设验证。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长篇论文）论文集中，第3932–3945页，在线。计算语言学协会。
- en: 'Kim et al., (2022) Kim, S. Y., Park, H., Shin, K., and Kim, K.-M. (2022). Ask
    me what you need: Product retrieval using knowledge from gpt-3. arXiv preprint
    arXiv:2207.02516.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2022年）Kim, S. Y., Park, H., Shin, K., 和 Kim, K.-M.（2022年）。问我你需要什么：利用
    GPT-3 的知识进行产品检索。arXiv 预印本 arXiv:2207.02516。
- en: Kojima et al., (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
    Y. (2022). Large language models are zero-shot reasoners.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人 (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., 和 Iwasawa, Y.
    (2022). 大型语言模型是零-shot 推理者。
- en: 'Lepore and Stone, (2014) Lepore, E. and Stone, M. (2014). Imagination and Convention:
    Distinguishing Grammar and Inference in Language. Oxford University Press.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lepore 和 Stone (2014) Lepore, E. 和 Stone, M. (2014). Imagination and Convention:
    区分语言中的语法和推理。牛津大学出版社。'
- en: Levinson, (1983) Levinson, S. C. (1983). Pragmatics. Cambridge University Press,
    Cambridge, U.K.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levinson (1983) Levinson, S. C. (1983). Pragmatics. 剑桥大学出版社，剑桥，英国。
- en: Lewis et al., (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,
    V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. (2020).
    Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in
    Neural Information Processing Systems, 33:9459–9474.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,
    Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., 等 (2020). 用于知识密集型
    NLP 任务的检索增强生成。神经信息处理系统进展，33:9459–9474。
- en: Li et al., (2021) Li, E., Schuster, S., and Degen, J. (2021). Predicting scalar
    inferences from “or” to “not both” using neural sentence encoders. In Proceedings
    of the Society for Computation in Linguistics 2021, pages 446–450, Online. Association
    for Computational Linguistics.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2021) Li, E., Schuster, S., 和 Degen, J. (2021). 使用神经句子编码器预测从“or”到“not
    both”的标量推理。在2021年计算语言学学会会议论文集，第446–450页，在线。计算语言学协会。
- en: 'Lin et al., (2022) Lin, S., Hilton, J., and Evans, O. (2022). TruthfulQA: Measuring
    how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214–3252,
    Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2022) Lin, S., Hilton, J., 和 Evans, O. (2022). TruthfulQA: 衡量模型如何模仿人类虚假信息。在第60届计算语言学协会年会论文集（第1卷：长篇论文），第3214–3252页，爱尔兰都柏林。计算语言学协会。'
- en: 'Liu et al., (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly
    optimized BERT pretraining approach. CoRR, abs/1907.11692.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
    O., Lewis, M., Zettlemoyer, L., 和 Stoyanov, V. (2019). Roberta: 一种强健优化的 BERT 预训练方法。CoRR,
    abs/1907.11692。'
- en: 'Louis et al., (2020) Louis, A., Roth, D., and Radlinski, F. (2020). “I’d rather
    just go to bed”: Understanding indirect answers. In Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pages 7411–7425,
    Online. Association for Computational Linguistics.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louis 等人 (2020) Louis, A., Roth, D., 和 Radlinski, F. (2020). “我宁愿去睡觉”：理解间接回答。在2020年自然语言处理实证方法会议（EMNLP）论文集，第7411–7425页，在线。计算语言学协会。
- en: 'Lu et al., (2022) Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp,
    P. (2022). Fantastically ordered prompts and where to find them: Overcoming few-shot
    prompt order sensitivity. In ACL.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人 (2022) Lu, Y., Bartolo, M., Moore, A., Riedel, S., 和 Stenetorp, P. (2022).
    Fantastically ordered prompts and where to find them: 克服少量示例提示的顺序敏感性。在 ACL。'
- en: 'Ng et al., (2019) Ng, N., Yee, K., Baevski, A., Ott, M., Auli, M., and Edunov,
    S. (2019). Facebook FAIR’s WMT19 news translation task submission. In Proceedings
    of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,
    Day 1), pages 314–319, Florence, Italy. Association for Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等人 (2019) Ng, N., Yee, K., Baevski, A., Ott, M., Auli, M., 和 Edunov, S. (2019).
    Facebook FAIR 的 WMT19 新闻翻译任务提交。在第四届机器翻译会议论文集（第2卷：共享任务论文，第1天），第314–319页，意大利佛罗伦萨。计算语言学协会。
- en: Ouyang et al., (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., and Lowe, R. J. (2022). Training language models to follow instructions
    with human feedback. ArXiv, abs/2203.02155.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.
    L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,
    J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., 和 Lowe, R. J. (2022). 训练语言模型以遵循人类反馈的指令。ArXiv, abs/2203.02155。
- en: 'Parrish et al., (2021) Parrish, A., Schuster, S., Warstadt, A., Agha, O., Lee,
    S.-H., Zhao, Z., Bowman, S. R., and Linzen, T. (2021). NOPE: A corpus of naturally-occurring
    presuppositions in English. In Proceedings of the 25th Conference on Computational
    Natural Language Learning, pages 349–366, Online. Association for Computational
    Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parrish et al., (2021) Parrish, A., Schuster, S., Warstadt, A., Agha, O., Lee,
    S.-H., Zhao, Z., Bowman, S. R., 和 Linzen, T. (2021). NOPE：英语中自然发生的预设语料库。载于《第25届计算自然语言学习会议论文集》，第349–366页，在线。计算语言学协会。
- en: 'Patel and Pavlick, (2021) Patel, R. and Pavlick, E. (2021). “was it “stated”
    or was it “claimed”?: How linguistic bias affects generative language models.
    In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing, pages 10080–10095, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel 和 Pavlick, (2021) Patel, R. 和 Pavlick, E. (2021). “是‘声明’还是‘声称’？：语言偏见如何影响生成语言模型”。载于《2021年自然语言处理经验方法会议论文集》，第10080–10095页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Potts, (2005) Potts, C. (2005). The Logic of Conventional Implicatures. Oxford
    University Press UK.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Potts, (2005) Potts, C. (2005). 常规含义的逻辑。牛津大学出版社英国。
- en: Potts, (2006) Potts, C. (2006). Conversational implicatures via general pragmatic
    pressures. In Washio, T., Satoh, K., Takeda, H., and Inokuchi, A., editors, New
    Frontiers in Artificial Intelligence, pages 205–218, Berlin, Heidelberg. Springer
    Berlin Heidelberg.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Potts, (2006) Potts, C. (2006). 通过一般语用压力进行会话含义分析。编者 Washio, T., Satoh, K., Takeda,
    H., 和 Inokuchi, A.，《人工智能的新前沿》，第205–218页，柏林，海德堡。Springer Berlin Heidelberg。
- en: Radford et al., (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    and Sutskever, I. (2019). Language models are unsupervised multitask learners.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al., (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    和 Sutskever, I. (2019). 语言模型是无监督的多任务学习者。
- en: 'Recasens et al., (2013) Recasens, M., Danescu-Niculescu-Mizil, C., and Jurafsky,
    D. (2013). Linguistic models for analyzing and detecting biased language. In Proceedings
    of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 1650–1659, Sofia, Bulgaria. Association for Computational
    Linguistics.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Recasens et al., (2013) Recasens, M., Danescu-Niculescu-Mizil, C., 和 Jurafsky,
    D. (2013). 分析和检测偏见语言的语言模型。载于《计算语言学协会第51届年会论文集（第1卷：长篇论文）》，第1650–1659页，保加利亚索非亚。计算语言学协会。
- en: Reed et al., (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G.,
    Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg,
    J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell,
    R., Vinyals, O., Bordbar, M., and de Freitas, N. (2022). A generalist agent.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed et al., (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G.,
    Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg,
    J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell,
    R., Vinyals, O., Bordbar, M., 和 de Freitas, N. (2022). 一般智能体。
- en: '(55) Reynolds, L. and McDonell, K. (2021a). Prompt programming for large language
    models: Beyond the few-shot paradigm.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Reynolds, L. 和 McDonell, K. (2021a). 大型语言模型的提示编程：超越少量示例范式。
- en: '(56) Reynolds, L. and McDonell, K. (2021b). Prompt programming for large language
    models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference
    on Human Factors in Computing Systems, CHI EA ’21, New York, NY, USA. Association
    for Computing Machinery.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Reynolds, L. 和 McDonell, K. (2021b). 大型语言模型的提示编程：超越少量示例范式。载于《2021年计算机系统中的人因会议扩展摘要》，CHI
    EA ’21，美国纽约。计算机协会。
- en: Sanh et al., (2022) Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L.,
    Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C.,
    Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N.,
    Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X.,
    Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli,
    A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf,
    T., and Rush, A. M. (2022). Multitask prompted training enables zero-shot task
    generalization. In International Conference on Learning Representations.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al., (2022) Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L.,
    Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C.,
    Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N.,
    Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z.
    X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli,
    A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf,
    T., 和 Rush, A. M. (2022). 多任务提示训练实现零样本任务泛化。载于国际学习表示会议。
- en: 'Schaeken et al., (2018) Schaeken, W., Van Haeren, M., and Bambini, V. (2018).
    The understanding of scalar implicatures in children with autism spectrum disorder:
    Dichotomized responses to violations of informativeness. Frontiers in Psychology,
    9.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaeken等（2018）Schaeken, W., Van Haeren, M., 和 Bambini, V.（2018）。自闭症谱系障碍儿童对信息量违规的理解：对违反信息性的反应进行二分法分析。《心理学前沿》，9。
- en: Schuster et al., (2020) Schuster, S., Chen, Y., and Degen, J. (2020). Harnessing
    the linguistic signal to predict scalar inferences. In Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, pages 5387–5403,
    Online. Association for Computational Linguistics.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster等（2020）Schuster, S., Chen, Y., 和 Degen, J.（2020）。利用语言信号预测量化推理。在第58届计算语言学协会年会论文集中，第5387–5403页，在线。计算语言学协会。
- en: 'Sperber and Wilson, (1986) Sperber, D. and Wilson, D. (1986). Relevance: Communication
    and Cognition. Language and thought series. Harvard University Press.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sperber 和 Wilson（1986）Sperber, D. 和 Wilson, D.（1986）。《相关性：交流与认知》。语言与思维系列。哈佛大学出版社。
- en: 'Srivastava et al., (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M.,
    Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A.,
    Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek,
    A. W., Safaya, A., Tazarv, A., Xiang, A., Parrish, A., Nie, A., Hussain, A., Askell,
    A., Dsouza, A., Slone, A., Rahane, A., Iyer, A. S., Andreassen, A., Madotto, A.,
    Santilli, A., Stuhlmüller, A., Dai, A., La, A., Lampinen, A., Zou, A., Jiang,
    A., Chen, A., Vuong, A., Gupta, A., Gottardi, A., Norelli, A., Venkatesh, A.,
    Gholamidavoodi, A., Tabassum, A., Menezes, A., Kirubarajan, A., Mullokandov, A.,
    Sabharwal, A., Herrick, A., Efrat, A., Erdem, A., Karakaş, A., Roberts, B. R.,
    Loe, B. S., Zoph, B., Bojanowski, B., Özyurt, B., Hedayatnia, B., Neyshabur, B.,
    Inden, B., Stein, B., Ekmekci, B., Lin, B. Y., Howald, B., Diao, C., Dour, C.,
    Stinson, C., Argueta, C., Ramírez, C. F., Singh, C., Rathkopf, C., Meng, C., Baral,
    C., Wu, C., Callison-Burch, C., Waites, C., Voigt, C., Manning, C. D., Potts,
    C., Ramirez, C., Rivera, C. E., Siro, C., Raffel, C., Ashcraft, C., Garbacea,
    C., Sileo, D., Garrette, D., Hendrycks, D., Kilman, D., Roth, D., Freeman, D.,
    Khashabi, D., Levy, D., González, D. M., Perszyk, D., Hernandez, D., Chen, D.,
    Ippolito, D., Gilboa, D., Dohan, D., Drakard, D., Jurgens, D., Datta, D., Ganguli,
    D., Emelin, D., Kleyko, D., Yuret, D., Chen, D., Tam, D., Hupkes, D., Misra, D.,
    Buzan, D., Mollo, D. C., Yang, D., Lee, D.-H., Shutova, E., Cubuk, E. D., Segal,
    E., Hagerman, E., Barnes, E., Donoway, E., Pavlick, E., Rodola, E., Lam, E., Chu,
    E., Tang, E., Erdem, E., Chang, E., Chi, E. A., Dyer, E., Jerzak, E., Kim, E.,
    Manyasi, E. E., Zheltonozhskii, E., Xia, F., Siar, F., Martínez-Plumed, F., Happé,
    F., Chollet, F., Rong, F., Mishra, G., Winata, G. I., de Melo, G., Kruszewski,
    G., Parascandolo, G., Mariani, G., Wang, G., Jaimovitch-López, G., Betz, G., Gur-Ari,
    G., Galijasevic, H., Kim, H., Rashkin, H., Hajishirzi, H., Mehta, H., Bogar, H.,
    Shevlin, H., Schütze, H., Yakura, H., Zhang, H., Wong, H. M., Ng, I., Noble, I.,
    Jumelet, J., Geissinger, J., Kernion, J., Hilton, J., Lee, J., Fisac, J. F., Simon,
    J. B., Koppel, J., Zheng, J., Zou, J., Kocoń, J., Thompson, J., Kaplan, J., Radom,
    J., Sohl-Dickstein, J., Phang, J., Wei, J., Yosinski, J., Novikova, J., Bosscher,
    J., Marsh, J., Kim, J., Taal, J., Engel, J., Alabi, J., Xu, J., Song, J., Tang,
    J., Waweru, J., Burden, J., Miller, J., Balis, J. U., Berant, J., Frohberg, J.,
    Rozen, J., Hernandez-Orallo, J., Boudeman, J., Jones, J., Tenenbaum, J. B., Rule,
    J. S., Chua, J., Kanclerz, K., Livescu, K., Krauth, K., Gopalakrishnan, K., Ignatyeva,
    K., Markert, K., Dhole, K. D., Gimpel, K., Omondi, K., Mathewson, K., Chiafullo,
    K., Shkaruta, K., Shridhar, K., McDonell, K., Richardson, K., Reynolds, L., Gao,
    L., Zhang, L., Dugan, L., Qin, L., Contreras-Ochando, L., Morency, L.-P., Moschella,
    L., Lam, L., Noble, L., Schmidt, L., He, L., Colón, L. O., Metz, L., Şenel, L. K.,
    Bosma, M., Sap, M., ter Hoeve, M., Farooqi, M., Faruqui, M., Mazeika, M., Baturan,
    M., Marelli, M., Maru, M., Quintana, M. J. R., Tolkiehn, M., Giulianelli, M.,
    Lewis, M., Potthast, M., Leavitt, M. L., Hagen, M., Schubert, M., Baitemirova,
    M. O., Arnaud, M., McElrath, M., Yee, M. A., Cohen, M., Gu, M., Ivanitskiy, M.,
    Starritt, M., Strube, M., Swędrowski, M., Bevilacqua, M., Yasunaga, M., Kale,
    M., Cain, M., Xu, M., Suzgun, M., Tiwari, M., Bansal, M., Aminnaseri, M., Geva,
    M., Gheini, M., T, M. V., Peng, N., Chi, N., Lee, N., Krakover, N. G.-A., Cameron,
    N., Roberts, N., Doiron, N., Nangia, N., Deckers, N., Muennighoff, N., Keskar,
    N. S., Iyer, N. S., Constant, N., Fiedel, N., Wen, N., Zhang, O., Agha, O., Elbaghdadi,
    O., Levy, O., Evans, O., Casares, P. A. M., Doshi, P., Fung, P., Liang, P. P.,
    Vicol, P., Alipoormolabashi, P., Liao, P., Liang, P., Chang, P., Eckersley, P.,
    Htut, P. M., Hwang, P., Miłkowski, P., Patil, P., Pezeshkpour, P., Oli, P., Mei,
    Q., Lyu, Q., Chen, Q., Banjade, R., Rudolph, R. E., Gabriel, R., Habacker, R.,
    Delgado, R. R., Millière, R., Garg, R., Barnes, R., Saurous, R. A., Arakawa, R.,
    Raymaekers, R., Frank, R., Sikand, R., Novak, R., Sitelew, R., LeBras, R., Liu,
    R., Jacobs, R., Zhang, R., Salakhutdinov, R., Chi, R., Lee, R., Stovall, R., Teehan,
    R., Yang, R., Singh, S., Mohammad, S. M., Anand, S., Dillavou, S., Shleifer, S.,
    Wiseman, S., Gruetter, S., Bowman, S. R., Schoenholz, S. S., Han, S., Kwatra,
    S., Rous, S. A., Ghazarian, S., Ghosh, S., Casey, S., Bischoff, S., Gehrmann,
    S., Schuster, S., Sadeghi, S., Hamdan, S., Zhou, S., Srivastava, S., Shi, S.,
    Singh, S., Asaadi, S., Gu, S. S., Pachchigar, S., Toshniwal, S., Upadhyay, S.,
    Shyamolima, D., Shakeri, S., Thormeyer, S., Melzi, S., Reddy, S., Makini, S. P.,
    Lee, S.-H., Torene, S., Hatwar, S., Dehaene, S., Divic, S., Ermon, S., Biderman,
    S., Lin, S., Prasad, S., Piantadosi, S. T., Shieber, S. M., Misherghi, S., Kiritchenko,
    S., Mishra, S., Linzen, T., Schuster, T., Li, T., Yu, T., Ali, T., Hashimoto,
    T., Wu, T.-L., Desbordes, T., Rothschild, T., Phan, T., Wang, T., Nkinyili, T.,
    Schick, T., Kornev, T., Telleen-Lawton, T., Tunduny, T., Gerstenberg, T., Chang,
    T., Neeraj, T., Khot, T., Shultz, T., Shaham, U., Misra, V., Demberg, V., Nyamai,
    V., Raunak, V., Ramasesh, V., Prabhu, V. U., Padmakumar, V., Srikumar, V., Fedus,
    W., Saunders, W., Zhang, W., Vossen, W., Ren, X., Tong, X., Zhao, X., Wu, X.,
    Shen, X., Yaghoobzadeh, Y., Lakretz, Y., Song, Y., Bahri, Y., Choi, Y., Yang,
    Y., Hao, Y., Chen, Y., Belinkov, Y., Hou, Y., Hou, Y., Bai, Y., Seid, Z., Zhao,
    Z., Wang, Z., Wang, Z. J., Wang, Z., and Wu, Z. (2022). Beyond the imitation game:
    Quantifying and extrapolating the capabilities of language models.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava等（2022）Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
    A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska,
    A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A.
    W., Safaya, A., Tazarv, A., Xiang, A., Parrish, A., Nie, A., Hussain, A., Askell,
    A., Dsouza, A., Slone, A., Rahane, A., Iyer, A. S., Andreassen, A., Madotto, A.,
    Santilli, A., Stuhlmüller, A., Dai, A., La, A., Lampinen, A., Zou, A., Jiang,
    A., Chen, A., Vuong, A., Gupta, A., Gottardi, A., Norelli, A., Venkatesh, A.,
    Gholamidavoodi, A., Tabassum, A., Menezes, A., Kirubarajan, A., Mullokandov, A.,
    Sabharwal, A., Herrick, A., Efrat, A., Erdem, A., Karakaş, A., Roberts, B. R.,
    Loe, B. S., Zoph, B., Bojanowski, B., Özyurt, B., Hedayatnia, B., Neyshabur, B.,
    Inden, B., Stein, B., Ekmekci, B., Lin, B. Y., Howald, B., Diao, C., Dour, C.,
    Stinson, C., Argueta, C., Ramírez, C. F., Singh, C., Rathkopf, C., Meng, C., Baral,
    C., Wu, C., Callison-Burch, C., Waites, C., Voigt, C., Manning, C. D., Potts,
    C., Ramirez, C., Rivera, C. E., Siro, C., Raffel, C., Ashcraft, C., Garbacea,
    C., Sileo, D., Garrette, D., Hendrycks, D., Kilman, D., Roth, D., Freeman, D.,
    Khashabi, D., Levy, D., González, D. M., Perszyk, D., Hernandez, D., Chen, D.,
    Ippolito, D., Gilboa, D., Dohan, D., Drakard, D., Jurgens, D., Datta, D., Ganguli,
    D., Emelin, D., Kleyko, D., Yuret, D., Chen, D., Tam, D., Hupkes, D., Misra, D.,
    Buzan, D., Mollo, D. C., Yang, D., Lee, D.-H., Shutova, E., Cubuk, E. D., Segal,
    E., Hagerman, E., Barnes, E., Donoway, E., Pavlick, E., Rodola, E., Lam, E., Chu,
    E., Tang, E., Erdem, E., Chang, E., Chi, E. A., Dyer, E., Jerzak, E., Kim, E.,
    Manyasi, E. E., Zheltonozhskii, E., Xia, F., Siar, F., Martínez-Plumed, F., Happé,
    F., Chollet, F., Rong, F., Mishra, G., Winata, G. I., de Melo, G., Kruszewski,
    G., Parascandolo, G., Mariani, G., Wang, G., Jaimovitch-López, G., Betz, G., Gur-Ari,
    G., Galijasevic, H., Kim, H., Rashkin, H., Hajishirzi, H., Mehta, H., Bogar, H.,
    Shevlin, H., Schütze, H., Yakura, H., Zhang, H., Wong, H. M., Ng, I., Noble, I.,
    Jumelet, J., Geissinger, J., Kernion, J., Hilton, J., Lee, J., Fisac, J. F., Simon,
    J. B., Koppel, J., Zheng, J., Zou, J., Kocoń, J., Thompson, J., Kaplan, J., Radom,
    J., Sohl-Dickstein, J., Phang, J., Wei, J., Yosinski, J., Novikova, J., Bosscher,
    J., Marsh, J., Kim, J., Taal, J., Engel, J., Alabi, J., Xu, J., Song, J., Tang,
    J., Waweru, J., Burden, J., Miller, J., Balis, J. U., Berant, J., Frohberg, J.,
    Rozen, J., Hernandez-Orallo, J., Boudeman, J., Jones, J., Tenenbaum, J. B., Rule,
    J. S., Chua, J., Kanclerz, K., Livescu, K., Krauth, K., Gopalakrishnan, K., Ignatyeva,
    K., Markert, K., Dhole, K. D., Gimpel, K., Omondi, K., Mathewson, K., Chiafullo,
    K., Shkaruta, K., Shridhar, K., McDonell, K., Richardson, K., Reynolds, L., Gao,
    L., Zhang, L., Dugan, L., Qin, L., Contreras-Ochando, L., Morency, L.-P., Moschella,
    L., Lam, L., Noble, L., Schmidt, L., He, L., Colón, L. O., Metz, L., Şenel, L.
    K., Bosma, M., Sap, M., ter Hoeve, M., Farooqi, M., Faruqui, M., Mazeika, M.,
    Baturan, M., Marelli, M., Maru, M., Quintana, M. J. R., Tolkiehn, M., Giulianelli,
    M., Lewis, M., Potthast, M., Leavitt, M. L., Hagen, M., Schubert, M., Baitemirova,
    M. O., Arnaud, M., McElrath, M., Yee, M. A., Cohen, M., Gu, M., Ivanitskiy, M.,
    Starritt, M., Strube, M., Swędrowski, M., Bevilacqua, M., Yasunaga, M., Kale,
    M., Cain, M., Xu, M., Suzgun, M., Tiwari, M., Bansal, M., Aminnaseri, M., Geva,
    M., Gheini, M., T, M. V., Peng, N., Chi, N., Lee, N., Krakover, N. G.-A., Cameron,
    N., Roberts, N., Doiron, N., Nangia, N., Deckers, N., Muennighoff, N., Keskar,
    N. S., Iyer, N. S., Constant, N., Fiedel, N., Wen, N., Zhang, O., Agha, O., Elbaghdadi,
    O., Levy, O., Evans, O., Casares, P. A. M., Doshi, P., Fung, P., Liang, P. P.,
    Vicol, P., Alipoormolabashi, P., Liao, P., Liang, P., Chang, P., Eckersley, P.,
    Htut, P. M., Hwang, P., Miłkowski, P., Patil, P., Pezeshkpour, P., Oli, P., Mei,
    Q., Lyu, Q., Chen, Q., Banjade, R., Rudolph, R. E., Gabriel, R., Habacker, R.,
    Delgado, R. R., Millière, R., Garg, R., Barnes, R., Saurous, R. A., Arakawa, R.,
    Raymaekers, R., Frank, R., Sikand, R., Novak, R., Sitelew, R., LeBras, R., Liu,
    R., Jacobs, R., Zhang, R., Salakhutdinov, R., Chi, R., Lee, R., Stovall, R., Teehan,
    R., Yang, R., Singh, S., Mohammad, S. M., Anand, S., Dillavou, S., Shleifer, S.,
    Wiseman, S., Gruetter, S., Bowman, S. R., Schoenholz, S. S., Han, S., Kwatra,
    S., Rous, S. A., Ghazarian, S., Ghosh, S., Casey, S., Bischoff, S., Gehrmann,
    S., Schuster, S., Sadeghi, S., Hamdan, S., Zhou, S., Srivastava, S., Shi, S.,
    Singh, S., Asaadi, S., Gu, S. S., Pachchigar, S., Toshniwal, S., Upadhyay, S.,
    Shyamolima, D., Shakeri, S., Thormeyer, S., Melzi, S., Reddy, S., Makini, S. P.,
    Lee, S.-H., Torene, S., Hatwar, S., Dehaene, S., Divic, S., Er
- en: 'Tan et al., (2021) Tan, Z., Zhang, X., Wang, S., and Liu, Y. (2021). Msp: Multi-stage
    prompting for making pre-trained language models better translators. arXiv preprint
    arXiv:2110.06609.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等，（2021）Tan，Z.，Zhang，X.，Wang，S.，和 Liu，Y.（2021）。Msp：多阶段提示以提升预训练语言模型的翻译能力。arXiv
    预印本 arXiv:2110.06609。
- en: 'Thoppilan et al., (2022) Thoppilan, R., Freitas, D. D., Hall, J., Shazeer,
    N., Kulshreshtha, A., Cheng, H., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y.,
    Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin,
    D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang,
    C., Krivokon, I., Rusch, W., Pickett, M., Meier-Hellstern, K. S., Morris, M. R.,
    Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran,
    V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J.,
    Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen,
    A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E. H.,
    and Le, Q. (2022). Lamda: Language models for dialog applications. CoRR, abs/2201.08239.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan 等，（2022）Thoppilan，R.，Freitas，D. D.，Hall，J.，Shazeer，N.，Kulshreshtha，A.，Cheng，H.，Jin，A.，Bos，T.，Baker，L.，Du，Y.，Li，Y.，Lee，H.，Zheng，H.
    S.，Ghafouri，A.，Menegali，M.，Huang，Y.，Krikun，M.，Lepikhin，D.，Qin，J.，Chen，D.，Xu，Y.，Chen，Z.，Roberts，A.，Bosma，M.，Zhou，Y.，Chang，C.，Krivokon，I.，Rusch，W.，Pickett，M.，Meier-Hellstern，K.
    S.，Morris，M. R.，Doshi，T.，Santos，R. D.，Duke，T.，Soraker，J.，Zevenbergen，B.，Prabhakaran，V.，Diaz，M.，Hutchinson，B.，Olson，K.，Molina，A.，Hoffman-John，E.，Lee，J.，Aroyo，L.，Rajakumar，R.，Butryna，A.，Lamm，M.，Kuzmina，V.，Fenton，J.，Cohen，A.，Bernstein，R.，Kurzweil，R.，Aguera-Arcas，B.，Cui，C.，Croak，M.，Chi，E.
    H.，和 Le，Q.（2022）。Lamda: 对话应用的语言模型。CoRR，abs/2201.08239。'
- en: Volden, (2017) Volden, J. (2017). Autism Spectrum Disorder, pages 59–83. Springer
    International Publishing, Cham.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Volden，（2017）Volden，J.（2017）。自闭症谱系障碍，页59–83。Springer International Publishing，Cham。
- en: 'Wang and Komatsuzaki, (2021) Wang, B. and Komatsuzaki, A. (2021). GPT-J-6B:
    A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和小松崎，（2021）王，B. 和 小松崎，A.（2021）。GPT-J-6B：一个60亿参数的自回归语言模型。 [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)。
- en: Webson and Pavlick, (2021) Webson, A. and Pavlick, E. (2021). Do prompt-based
    models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webson 和 Pavlick，（2021）Webson，A. 和 Pavlick，E.（2021）。基于提示的模型真的理解它们提示的含义吗？arXiv
    预印本 arXiv:2109.01247。
- en: Wei et al., (2022) Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M., and Le, Q. V. (2022). Finetuned language models are zero-shot
    learners. In International Conference on Learning Representations.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等，（2022）Wei，J.，Bosma，M.，Zhao，V.，Guu，K.，Yu，A. W.，Lester，B.，Du，N.，Dai，A. M.，和
    Le，Q. V.（2022）。微调语言模型是零样本学习者。国际学习表征会议。
- en: 'Wittgenstein, (1921) Wittgenstein, L. (1921). Tractatus logico-philosophicus.
    London: Routledge, 1981.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维特根斯坦，（1921）维特根斯坦，L.（1921）。《逻辑哲学论》。伦敦：Routledge，1981。
- en: Wittgenstein, (1953) Wittgenstein, L. (1953). Philosophical Investigations.
    Basil Blackwell, Oxford.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维特根斯坦，（1953）维特根斯坦，L.（1953）。《哲学研究》。Basil Blackwell，牛津。
- en: 'Wolf et al., (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue,
    C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,
    S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger,
    S., Drame, M., Lhoest, Q., and Rush, A. (2020). Transformers: State-of-the-art
    natural language processing. In Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online.
    Association for Computational Linguistics.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等，（2020）Wolf，T.，Debut，L.，Sanh，V.，Chaumond，J.，Delangue，C.，Moi，A.，Cistac，P.，Rault，T.，Louf，R.，Funtowicz，M.，Davison，J.，Shleifer，S.，von
    Platen，P.，Ma，C.，Jernite，Y.，Plu，J.，Xu，C.，Le Scao，T.，Gugger，S.，Drame，M.，Lhoest，Q.，和
    Rush，A.（2020）。变换器：最先进的自然语言处理技术。发表于2020年自然语言处理经验方法会议：系统演示，页38–45，在线。计算语言学协会。
- en: 'Zhang et al., (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
    L. (2022). Opt: Open pre-trained transformer language models.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等，（2022）张，S.，Roller，S.，Goyal，N.，Artetxe，M.，Chen，M.，Chen，S.，Dewan，C.，Diab，M.，Li，X.，Lin，X.
    V.，Mihaylov，T.，Ott，M.，Shleifer，S.，Shuster，K.，Simig，D.，Koura，P. S.，Sridhar，A.，Wang，T.，和
    Zettlemoyer，L.（2022）。Opt：开放预训练的变换器语言模型。
- en: 'Zheng et al., (2021) Zheng, Z., Qiu, S., Fan, L., Zhu, Y., and Zhu, S.-C. (2021).
    GRICE: A grammar-based dataset for recovering implicature and conversational rEasoning.
    In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,
    pages 2074–2085, Online. Association for Computational Linguistics.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2021）郑、丘、范、朱、朱（2021）。GRICE：一个基于语法的数据集，用于恢复隐含义和对话推理。发表于《计算语言学协会会议论文集：ACL-IJCNLP
    2021》，第2074-2085页，在线。计算语言学协会。
- en: \appendixpage
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: \附录页
- en: Contents
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#S1 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[2 Related Work](#S2 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 相关工作](#S2 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[3 Evaluation Protocol](#S3 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 评估协议](#S3 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[4 Experiments](#S4 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 实验](#S4 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[5 Discussion](#S5 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 讨论](#S5 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[6 Conclusion](#S6 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论](#S6 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[A Contributions](#A1 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 贡献](#A1 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[B Reproducibility Statement](#A2 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 重现性声明](#A2 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[C Ethics Statement](#A3 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 伦理声明](#A3 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[D Opener example with InstructGPT](#A4 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 使用InstructGPT的开场示例](#A4 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[E Background on implicature](#A5 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 隐含义背景](#A5 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[F Detailed prompt templates](#A6 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 详细提示模板](#A6 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[G Model categorization](#A7 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G 模型分类](#A7 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[H Human evaluation](#A8 "In The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H 人工评估](#A8 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[I Comparison with BIG-bench implicatures task](#A9 "In The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I 与BIG-bench隐含义任务的比较](#A9 "在《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[I.1 Discarding ambiguous examples](#A9.SS1 "In Appendix I Comparison with
    BIG-bench implicatures task ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I.1 丢弃模糊的示例](#A9.SS1 "在附录I 与BIG-bench隐含义任务的比较 ‣ 《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[I.2 Overestimation of performance on implicature understanding](#A9.SS2 "In
    Appendix I Comparison with BIG-bench implicatures task ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I.2 对隐含义理解表现的高估](#A9.SS2 "在附录I 与BIG-bench隐含义任务的比较 ‣ 《适度理解的黄金法则：微调策略对LLM的隐含义解影响》的研究中")'
- en: '[I.3 Other limitations](#A9.SS3 "In Appendix I Comparison with BIG-bench implicatures
    task ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I.3 其他限制](#A9.SS3 "在附录I 与 BIG-bench 含义任务比较 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[J Chain-of-thought completions by GPT-4](#A10 "In The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[J GPT-4 的思维链完成](#A10 "在实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K Additional results](#A11 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K 额外结果](#A11 "在实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.1 Contrastive experiment](#A11.SS1 "In Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.1 对比实验](#A11.SS1 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.2 Variance over prompt ordering](#A11.SS2 "In Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.2 提示顺序中的方差](#A11.SS2 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.3 Different zero-shot instruction prompts](#A11.SS3 "In Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.3 不同零样本指令提示](#A11.SS3 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.4 The effect of in-context examples on sensitivity to prompt wording](#A11.SS4
    "In Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.4 上下文示例对提示措辞敏感性的影响](#A11.SS4 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.5 Variance over API runs](#A11.SS5 "In Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.5 API 调用中的方差](#A11.SS5 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.6 Experiment with random in-context labels](#A11.SS6 "In Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.6 使用随机上下文标签的实验](#A11.SS6 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.7 Chain-of-thought on base models](#A11.SS7 "In Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.7 基础模型上的思维链](#A11.SS7 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.8 Testing for spurious correlations](#A11.SS8 "In Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.8 测试虚假相关性](#A11.SS8 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.9 Detailed results type label analysis](#A11.SS9 "In Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs")'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.9 详细结果类型标签分析](#A11.SS9 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[K.10 Detailed results per model](#A11.SS10 "In Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K.10 每个模型的详细结果](#A11.SS10 "在附录K 额外结果 ‣ 实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[L Timestamps API calls](#A12 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[L 时间戳 API 调用](#A12 "在实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: '[M Compute and Emissions](#A13 "In The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[M 计算与排放](#A13 "在实用理解的金发姑娘：微调策略对含义解析的重要性")'
- en: Appendix A Contributions
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 贡献
- en: 'Laura Ruis: project proposal and leadership, dataset development, code writing,
    human experiment, manual error analysis, paper writing and editing.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 劳拉·鲁伊斯：项目提案与领导、数据集开发、代码编写、人类实验、手动错误分析、论文撰写与编辑。
- en: 'Akbir Khan: code writing, model evaluations, human experiment, paper writing
    and editing.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Akbir Khan：代码编写、模型评估、人类实验、论文撰写和编辑。
- en: 'Stella Biderman: model evaluations, compute usage, paper writing and editing,
    advisor.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Stella Biderman：模型评估、计算使用、论文撰写和编辑、顾问。
- en: 'Sara Hooker: compute usage, paper writing and editing, advisor.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Sara Hooker：计算使用、论文撰写和编辑、顾问。
- en: 'Tim Rocktäschel: paper writing and editing, advisor.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Tim Rocktäschel：论文撰写和编辑、顾问。
- en: 'Edward Grefenstette: initial idea, manual error analysis, paper writing and
    editing, advisor.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Edward Grefenstette：初步构思、手动错误分析、论文撰写和编辑、顾问。
- en: Appendix B Reproducibility Statement
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 可重复性声明
- en: 'We share all the data, human annotations, code used for the evaluations, and
    the raw results in the supplementary material. Additionally, in Appendix [K.5](#A11.SS5
    "K.5 Variance over API runs ‣ Appendix K Additional results ‣ The Goldilocks of
    Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") we estimate the variance due to stochasticity in the API’s of OpenAI
    and Cohere. Of course, if either OpenAI or Cohere decides to change the models
    behind the API, the results might look different. We publish the exact date and
    time each API was queried for the results in Appendix [L](#A12 "Appendix L Timestamps
    API calls ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs"). Finally, in Appendix [K.2](#A11.SS2 "K.2
    Variance over prompt ordering ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") we estimate the variance over the prompt order of the in-context examples.
    The compute used for the experiments is detailed in Appendix [M](#A13 "Appendix
    M Compute and Emissions ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs"). The remaining experiments
    were done with API credits received as a research grant from OpenAI and Cohere.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在补充材料中分享了所有数据、人类注释、用于评估的代码以及原始结果。此外，在附录 [K.5](#A11.SS5 "K.5 API 运行的方差 ‣ 附录
    K 额外结果 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性") 中，我们估计了由于 OpenAI 和 Cohere API 的随机性而产生的方差。当然，如果
    OpenAI 或 Cohere 决定更改 API 背后的模型，结果可能会有所不同。我们在附录 [L](#A12 "附录 L 时间戳 API 调用 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")
    中发布了每次 API 查询结果的确切日期和时间。最后，在附录 [K.2](#A11.SS2 "K.2 提示顺序的方差 ‣ 附录 K 额外结果 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")
    中，我们估计了上下文示例的提示顺序的方差。实验中使用的计算资源在附录 [M](#A13 "附录 M 计算和排放 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")
    中详细描述。剩余的实验是使用作为研究资助从 OpenAI 和 Cohere 获得的 API 信用完成的。
- en: Appendix C Ethics Statement
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 伦理声明
- en: 'In this work, we conduct a study with human subjects (see Appendix [H](#A8
    "Appendix H Human evaluation ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") for details). To get matched
    with participants, we used the platform Prolific. Prolific complies with ethical
    standards according to UK law (e.g. complying with the GDPR). We compensated participants
    with a UK living wage at 15 GBP an hour, which is 6 GBP an hour more than Prolific
    recommends at 9 GBP per hour.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们对人类受试者进行了研究（详见附录 [H](#A8 "附录 H 人类评估 ‣ 实用理解的金发女孩：微调策略对隐含意义解析的重要性")）。为了匹配参与者，我们使用了平台
    Prolific。Prolific 遵循英国法律的伦理标准（例如，遵守 GDPR）。我们以每小时 15 英镑的英国生活工资补偿参与者，这比 Prolific
    推荐的每小时 9 英镑高出 6 英镑。
- en: Implicature is an aspect of pragmatics, and pragmatic language impairments are
    universal in Autism Spectrum Disorder (ASD) (American Psychiatric Association,,
    [2013](#bib.bib2)). Difficulties in understanding scalar implicatures are claimed
    to be present in people with ASD (Volden,, [2017](#bib.bib64)), although the nature
    of the relation has proven hard to establish and has recently been debated (Katsos
    et al.,, [2011](#bib.bib32); Schaeken et al.,, [2018](#bib.bib58)). For the purposes
    of this work, whether or not implicature understanding relates to ASD is not important.
    We took the following steps to make sure no sensitive data is collected or published.
    The human annotations we obtain are anonymous, related to a participant only by
    their Prolific ID for the purposes of compensation. In publishing the human annotations,
    we will not publish the Prolific ID of participants or anything else related to
    the participants. Additionally, we did not collect or request any personal or
    demographic characteristics of the participants apart from that they are all native
    English speakers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 含意是语用学的一个方面，且语用语言障碍在自闭症谱系障碍（ASD）中是普遍存在的（美国精神病学协会，[2013](#bib.bib2)）。声称 ASD 患者在理解标度含意方面存在困难（Volden，[2017](#bib.bib64)），尽管这种关系的性质已被证明难以建立，并且最近进行了辩论（Katsos
    等，[2011](#bib.bib32); Schaeken 等，[2018](#bib.bib58)）。对于这项工作而言，含意理解是否与 ASD 相关并不重要。我们采取了以下步骤，以确保不会收集或发布敏感数据。我们获得的人类注释是匿名的，仅通过他们的
    Prolific ID 与参与者相关，以便进行补偿。在发布人类注释时，我们不会发布参与者的 Prolific ID 或与参与者相关的任何其他信息。此外，我们没有收集或要求参与者的任何个人或人口统计特征，除了他们都是母语为英语的说话者。
- en: 'Additionally, in this work we run a lot of compute-intensive experiments. We
    publish the estimated emissions per experiment in Appendix [M](#A13 "Appendix
    M Compute and Emissions ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs"). The total amount of GPU
    hours is estimated at maximally 966\. How this is broken down per experiment can
    be seen in Appendix [M](#A13 "Appendix M Compute and Emissions ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs").'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在这项工作中，我们运行了大量计算密集型实验。我们在附录 [M](#A13 "附录 M 计算与排放 ‣ 实用理解的金发姑娘: 微调策略对 LLMs
    的含意解析至关重要") 中公布了每个实验的估计排放量。GPU 小时的总量估计最多为 966\。每个实验的详细分解可以在附录 [M](#A13 "附录 M 计算与排放
    ‣ 实用理解的金发姑娘: 微调策略对 LLMs 的含意解析至关重要") 中查看。'
- en: Appendix D Opener example with InstructGPT
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 开场示例与 InstructGPT
- en: 'This quote was obtained through the OpenAI playground for text-davinci-002
    on May 15th 2023\. The model text-davinci-001 consistently generates better responses
    for the same prompt. GPT-3 itself (i.e. davinci) mainly gives nonsensical answers.
    In the following, the prompt is italic and the completion bold. The completion
    was generated with a maximum of 10 tokens and a temperature of 0:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个引用是通过 OpenAI playground 对 text-davinci-002 于 2023 年 5 月 15 日获得的。模型 text-davinci-001
    对相同提示生成的回答 consistently 更好。GPT-3 本身（即 davinci）主要给出无意义的回答。在下文中，提示以 *斜体* 表示，完成的内容以
    **粗体** 表示。完成内容是在最多 10 个 token 和 0 温度下生成的：
- en: 'User: “Have you seen my phone?”'
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '用户: “你看到了我的手机吗？”'
- en: 'GPT-3: “Yes, I have seen your phone.”'
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT-3: “是的，我看到了你的手机。”'
- en: The opener example is used to introduce the problem we are looking at, and not
    to judge the model used to generate it. In fact, although text-davinci-002 sometimes
    completes conversations in a way that is unexpected according to pragmatic language
    usage, it is one of the better models when evaluated few-shot.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个开场示例用于引入我们正在研究的问题，而不是评判用于生成它的模型。实际上，虽然 text-davinci-002 有时以一种根据语用语言使用情况意料之外的方式完成对话，但在少量样本评估时，它是表现较好的模型之一。
- en: Appendix E Background on implicature
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 关于含意的背景
- en: 'The first influential consideration of implicature is Grice, ([1975](#bib.bib26)).
    In his work, Grice continues the trend of moving away from purely logical accounts
    of language started by Wittgenstein, ([1921](#bib.bib68)) by hypothesising implicatures
    arise in conversation when some mutually agreed upon maxims seem to be violated.
    For example, if we agree on only making relevant contributions to conversation,
    Juan’s response in the introduction seemingly violates this maxim—after all, he
    starts talking about work when Esther asks him about a party. However, because
    Juan agreed to be relevant he must be implying that having to work means he cannot
    come to the party. Grice contrasts conversational implicatures that arise through
    context with conventional implicatures. These are implicatures where the *conventional*
    meaning of the word determines what is implicated. An example given by Grice is
    the following sentence: “he is an Englishman; he is therefore brave.”. Grice notes
    that this sentence does not literally state that an Englishman being brave is
    a direct consequence of him being English, but it’s implied by the conventional
    meaning of the word ‘therefore’.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 关于隐含意义的首次有影响的考量来自格赖斯 ([1975](#bib.bib26))。在他的研究中，格赖斯延续了维特根斯坦 ([1921](#bib.bib68))
    开始的从纯粹逻辑语言解释的趋势，通过假设在对话中，当某些相互同意的准则似乎被违反时，会产生隐含意义。例如，如果我们同意只对对话做出相关的贡献，胡安在引言中的回答似乎违反了这一准则——毕竟，当埃丝特问他关于聚会的事时，他开始谈论工作。然而，由于胡安同意保持相关性，他一定是在暗示，工作意味着他不能参加聚会。格赖斯将通过语境产生的对话隐含意义与惯常隐含意义进行了对比。这些隐含意义是由词汇的*惯常*意义决定的。格赖斯举的一个例子是以下句子：“他是个英国人；因此他很勇敢。”
    格赖斯指出，这个句子并没有字面上声明一个英国人勇敢是他作为英国人的直接结果，但这是由“因此”一词的惯常意义暗示的。
- en: Since then, issues with the Gricean cooperative principle have been pointed
    out by many (Levinson,, [1983](#bib.bib39); Sperber and Wilson,, [1986](#bib.bib60);
    Davis,, [1998](#bib.bib15); Lepore and Stone,, [2014](#bib.bib38)). The most influential
    alternative theory is relevancy theory by Sperber and Wilson, ([1986](#bib.bib60)).
    They do away with the cooperative principle and instead theorise implicatures
    arise because speakers try to produce utterances that are both as relevant as
    possible and require the least effort to process. Another point of contention
    is the incorporation of conventional implicatures on the pragmatics side. Bach,
    ([1999](#bib.bib5)) argues that there is no such thing as conventional implicatures,
    and they are simply instances of something else. Based on a thorough treatment
    of what Grice calls conventional implicatures, Bach argues all examples of it
    can be filed under other concepts within semantics, like utterance modifiers (called
    “utterance modifiers” instead of “sentence modifiers” because they go against
    the semantic content of the rest of the sentence). Potts, ([2005](#bib.bib50))
    also argues that to explain conventional implicatures we can stay on semantic
    turf. Indeed, even Grice himself says conventional implicatures derive from the
    meaning of the words, not from conversational context. However, [Potts,](#bib.bib50)
    does not claim conventional implicatures do not exist, but instead argues they
    arise by a combination of lexical meaning and novel ways of combining words—the
    latter being the well-known principle of compositionality, an important part of
    semantics, not of pragmatics. [Potts,](#bib.bib50) provides us with an illuminating
    demarcation between conventional and conversational implicatures. Conventional
    implicatures are never negotiable by context, whereas conversational implicatures
    are context-dependent and can always be cancelled without causing incoherent discourse.
    Consider again the sentence “he is an Englishman; he is therefore brave.” and
    the sentence “Eddie has three bicycles” (implicating that Eddie has exactly three
    bicycles and not more). The former sentence can not be cancelled by new context
    without contradiction, whereas for the latter, if we continue saying “In fact,
    Eddie has 10 bicycles, he is a bicycle junkie”, we have cancelled the implicature.
    This demarcation clearly puts conventional implicatures on the semantic side,
    and conversational implicatures on the pragmatic side. [Potts,](#bib.bib50) goes
    on by providing a formal theory for conventional implicatures.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，许多学者指出了格赖斯合作原则的问题（Levinson，，[1983](#bib.bib39)；Sperber和Wilson，，[1986](#bib.bib60)；Davis，，[1998](#bib.bib15)；Lepore和Stone，，[2014](#bib.bib38)）。最具影响力的替代理论是Sperber和Wilson提出的关联理论（[1986](#bib.bib60)）。他们废除了合作原则，转而理论化暗示的产生是因为说话者试图产生尽可能相关且处理起来最少耗费的发言。另一个争议点是语用学方面的传统暗示的纳入。Bach（[1999](#bib.bib5)）认为不存在传统暗示，它们只是其他东西的实例。基于对格赖斯所称的传统暗示的深入探讨，Bach认为所有这些例子可以归入语义学中的其他概念，如发言修饰语（称为“发言修饰语”而非“句子修饰语”，因为它们与句子的其余部分的语义内容相抵触）。Potts（[2005](#bib.bib50)）也认为，为了解释传统暗示，我们可以停留在语义领域。确实，即使是格赖斯自己也说传统暗示源于词汇的意义，而不是会话背景。然而，[Potts](#bib.bib50)并不声称传统暗示不存在，而是认为它们是通过词汇意义和词汇组合的新方式的结合而产生的——后者是著名的组合性原则，是语义学的重要部分，而非语用学。[Potts](#bib.bib50)为传统暗示提供了明确的区分。传统暗示绝不受背景的协商，而会话暗示则依赖于背景，并且可以在不造成不连贯的情况下被取消。再考虑句子“他是一个英国人；因此他是勇敢的。”和句子“Eddie有三辆自行车”（暗示Eddie正好有三辆自行车而不是更多）。前者的句子不能被新背景取消而不发生矛盾，而对于后者，如果我们继续说“实际上，Eddie有10辆自行车，他是个自行车迷”，我们已经取消了暗示。这个区分清楚地将传统暗示归入语义领域，将会话暗示归入语用学领域。[Potts](#bib.bib50)进一步提供了关于传统暗示的正式理论。
- en: In later work, Potts, ([2006](#bib.bib51)) describes how pragmatic pressures
    interacting with context cause conversational implicature to arise. He shows how
    sensitive conversational implicatures are to small changes in the context. Novel
    information about a speaker’s belief state might completely change what is implied.
    There are many more models of implicature that aim to explain how humans understand
    language in context. Most notably, Frank and Goodman, ([2012](#bib.bib18)) formalise
    the view that speakers produce utterances that are helpful and not longer than
    necessary with a Bayesian model called the rational speech act (RSA). Many variants
    on the RSA framework have since been proposed. For example, Goodman and Frank,
    ([2016](#bib.bib23)) extend it to handle nonliteral uses of language, like irony,
    and metaphor. In the context of computational models, prior work uses insights
    from pragmatics to show that the use of certain words can make a language model
    produce biased completions (Patel and Pavlick, ([2021](#bib.bib49)), e.g. saying
    someone “claimed” something rather than “said” something), and inform bias and
    sentiment classifiers (Greene and Resnik,, [2009](#bib.bib25); Recasens et al.,,
    [2013](#bib.bib53)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在后来的工作中，Potts, ([2006](#bib.bib51)) 描述了语用压力如何与上下文交互导致会话含义的产生。他展示了会话含义对上下文中小变化的敏感性。关于说话者信念状态的新信息可能会完全改变所暗示的内容。还有许多其他含义模型旨在解释人类如何在上下文中理解语言。最值得注意的是，Frank
    和 Goodman, ([2012](#bib.bib18)) 用一种叫做理性言语行为 (RSA) 的贝叶斯模型形式化了这种观点，即说话者产生对话有帮助且不超过必要长度的言语。自此以来，已经提出了许多
    RSA 框架的变体。例如，Goodman 和 Frank, ([2016](#bib.bib23)) 扩展了它以处理非字面语言使用，如讽刺和隐喻。在计算模型的背景下，先前的工作利用语用学的见解表明，使用某些词汇可能使语言模型产生有偏见的补全
    (Patel 和 Pavlick, ([2021](#bib.bib49))，例如说某人“声称”某事而不是“说”某事)，并为偏见和情感分类器提供信息 (Greene
    和 Resnik,, [2009](#bib.bib25)；Recasens 等,, [2013](#bib.bib53))。
- en: In this work, we focus on conversational implicatures and not on conventional
    implicatures. All conversational implicatures are negotiable by context, but the
    way they depend on context can be different. Grice, ([1975](#bib.bib26)) identifies
    generalised conversational implicatures and particularised conversational implicatures.
    The former require little or no context to be resolved. For example, “some athletes
    smoke” can imply “not all athletes smoke”, but might also imply “I do not know
    whether all athletes smoke” when it is a response to the question “do you know
    whether all athletes smoke?” (Davis,, [2019](#bib.bib14)). The latter only arise
    in certain contexts. For example, the response “I have an early morning” to the
    question “do you want to stay for a drink?”.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于会话含义，而非传统含义。所有会话含义都可以通过上下文进行协商，但它们对上下文的依赖方式可能不同。Grice, ([1975](#bib.bib26))
    识别了广义会话含义和特定会话含义。前者解决时需要的上下文很少或没有。例如，“一些运动员吸烟”可能暗示“并非所有运动员都吸烟”，但当它是对“你知道所有运动员是否吸烟吗？”这个问题的回答时，可能还暗示“我不知道所有运动员是否吸烟”（Davis,,
    [2019](#bib.bib14)）。后者仅在特定上下文中出现。例如，回应“我有一个早晨”对“你想留下来喝一杯吗？”这个问题。
- en: Appendix F Detailed prompt templates
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 详细提示模板
- en: 'Table [5](#A6.T5 "Table 5 ‣ Appendix F Detailed prompt templates ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") contains the full prompt templates we used for the main evaluation and
    Table [7](#A6.T7 "Table 7 ‣ Appendix F Detailed prompt templates ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") contains the extra prompt templates.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [5](#A6.T5 "Table 5 ‣ 附录 F 详细提示模板 ‣ 语用理解的金发姑娘：微调策略对 LLMs 的含义解析至关重要") 包含了我们用于主要评估的完整提示模板，表格
    [7](#A6.T7 "Table 7 ‣ 附录 F 详细提示模板 ‣ 语用理解的金发姑娘：微调策略对 LLMs 的含义解析至关重要") 包含了额外的提示模板。
- en: 'Table 5: Ranking prompt templates. The six templates we wrap the test examples
    in to present to the models. Template 1, 3, and 4 are of the type structured,
    and 2, 5, and 6 of the type natural. Within the type of prompt template they only
    differ slightly in wording.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：排名提示模板。我们将测试示例包装在六种模板中以呈现给模型。模板 1、3 和 4 属于结构化类型，2、5 和 6 属于自然类型。在提示模板的类型中，它们的措辞仅有细微差别。
- en: '| # | Prompt template |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| # | 提示模板 |'
- en: '| --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 1 |'
- en: '&#124; Does the following response to the question imply yes or no? &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下回应是否暗示是或不是？ &#124;'
- en: '&#124; question:  &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回应:  &#124;'
- en: '&#124; implicature:  &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义:  &#124;'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 2 |'
- en: '&#124; Finish the following text: &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本: &#124;'
- en: '&#124; Esther asked "" and Juan responded "", which means
     &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Esther 问了 ""，Juan 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3 |'
- en: '&#124; Is the implied meaning of the following response yes or no: &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下回答的隐含意义是“是”还是“否”：&#124;'
- en: '&#124; question:  &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答:  &#124;'
- en: '&#124; meaning:  &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义:  &#124;'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 4 |'
- en: '&#124; What is the intent of the following response, yes or no? &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下回答的意图是“是”还是“否”？&#124;'
- en: '&#124; question:  &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答:  &#124;'
- en: '&#124; intent:  &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 意图:  &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 5 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 5 |'
- en: '&#124; Finish the following text: &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本: &#124;'
- en: '&#124; Karen asked "" and William responded "", which
    means  &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Karen 问了 ""，William 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 6 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 6 |'
- en: '&#124; Finish the following text: &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本: &#124;'
- en: '&#124; Bob asked "" and Alice responded "", which means
     &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Bob 问了 ""，Alice 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 6: Completion prompt templates. The six adjusted templates we wrap the
    test examples in to present to the models when we use completion instead of likelihood
    ranking. Template 1, 3, and 4 are of the type structured, and 2, 5, and 6 of the
    type natural. Within the type of prompt template they only differ slightly in
    wording.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：完成提示模板。我们在使用完成而不是可能性排名时，用于向模型展示测试示例的六个调整模板。模板 1、3 和 4 是结构化类型的，2、5 和 6 是自然类型的。在提示模板的类型内，它们的措辞仅有细微差别。
- en: '| # | Prompt template |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| # | 提示模板 |'
- en: '| --- | --- |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 1 |'
- en: '&#124; Does the following response to the question imply yes or no? Only output
    ‘yes’ or ‘no’. &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下对问题的回答是否暗示“是”或“否”？只输出‘yes’或‘no’。&#124;'
- en: '&#124; Even if you’re uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; question:  &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答:  &#124;'
- en: '&#124; implicature:  &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义:  &#124;'
- en: '|'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 2 |'
- en: '&#124; Finish the following text. Only output ‘yes’ or ‘no’. Even if you’re
    uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本。只输出‘yes’或‘no’。即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; Esther asked "" and Juan responded "", which means
     &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Esther 问了 ""，Juan 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 3 |'
- en: '&#124; Is the implied meaning of the following response yes or no. Only output
    ‘yes’ or ‘no’. &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下回答的隐含意义是“是”还是“否”。只输出‘yes’或‘no’。&#124;'
- en: '&#124; Even if you’re uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; question:  &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答:  &#124;'
- en: '&#124; meaning:  &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义:  &#124;'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 4 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 4 |'
- en: '&#124; What is the intent of the following response, yes or no? Only output
    ‘yes’ or ‘no’. &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下回答的意图是“是”还是“否”？只输出‘yes’或‘no’。&#124;'
- en: '&#124; Even if you’re uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; question:  &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题:  &#124;'
- en: '&#124; response:  &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答:  &#124;'
- en: '&#124; intent:  &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 意图:  &#124;'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 5 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 5 |'
- en: '&#124; Finish the following text. Only output ‘yes’ or ‘no’. Even if you’re
    uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本。只输出‘yes’或‘no’。即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; Karen asked "" and William responded "", which
    means  &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Karen 问了 ""，William 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 6 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 6 |'
- en: '&#124; Finish the following text. Only output ‘yes’ or ‘no’. Even if you’re
    uncertain, choose either ‘yes’ or ‘no’. &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成以下文本。只输出‘yes’或‘no’。即使你不确定，也选择‘yes’或‘no’。&#124;'
- en: '&#124; Bob asked "" and Alice responded "", which means
     &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Bob 问了 ""，Alice 回答了 ""，这意味着  &#124;'
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 7: The three additional templates we wrap the test examples in to present
    to the models, adapted from Glaese et al., ([2022](#bib.bib22)).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：我们用于向模型展示测试示例的三个额外模板，改编自 Glaese 等人，（[2022](#bib.bib22)）。
- en: '| # | Prompt template |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| # | 提示模板 |'
- en: '| --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 7 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 7 |'
- en: '&#124; The following text shows an interaction between two humans called Esther
    and Juan. &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下文本展示了两个人类之间的互动，称为艾丝特和胡安。&#124;'
- en: '&#124; In the interaction, Esther will ask Juan a question, and Juan will give
    an answer that contains &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在互动中，艾丝特将向胡安提问，胡安将给出一个包含&#124;'
- en: '&#124; an implicature. &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含意。&#124;'
- en: '&#124; An implicature is an utterance that means something other than the literal
    meaning of the words. &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含意是一种话语，意味着除了文字的字面意思之外的其他东西。&#124;'
- en: '&#124; The implicature of Juan’s response is yes or no. &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 胡安的回应的含意是“是”或“否”。&#124;'
- en: '&#124; You, the AI assistant, are asked to finish the text with yes or no.
    &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你，AI助手，被要求用“是”或“否”完成文本。&#124;'
- en: '&#124; The task begins: &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务开始：&#124;'
- en: '&#124; Esther asked "" and Juan responded "", which means
     &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 艾丝特问了“”，胡安回答了“”，这意味着&#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 8 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 8 |'
- en: '&#124; The following text shows an interaction between two humans called Esther
    and Juan. &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下文本展示了两个人类之间的互动，称为艾丝特和胡安。&#124;'
- en: '&#124; In the interaction, Esther will ask Juan a question, and Juan will give
    an answer that has a meaning &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在互动中，艾丝特将向胡安提问，胡安将给出一个有含义的回答&#124;'
- en: '&#124; besides the literal meaning of the words. &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除了文字的字面意思之外。&#124;'
- en: '&#124; That meaning is either yes or no. &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这个意思是“是”或“否”。&#124;'
- en: '&#124; You, the AI assistant, are asked to finish the text with the correct
    meaning, either yes or no. &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你，AI助手，被要求用正确的意思完成文本，即“是”或“否”。&#124;'
- en: '&#124; The task begins: &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务开始：&#124;'
- en: '&#124; Esther asked "" and Juan responded "", which means
     &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 艾丝特问了“”，胡安回答了“”，这意味着&#124;'
- en: '|'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 9 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 9 |'
- en: '&#124; The following text shows an interaction between two humans called Esther
    and Juan. &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下文本展示了两个人类之间的互动，称为艾丝特和胡安。&#124;'
- en: '&#124; In the interaction, Esther will ask Juan a question, and Juan will give
    an answer that has a meaning &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在互动中，艾丝特将向胡安提问，胡安将给出一个有含义的回答&#124;'
- en: '&#124; besides the literal meaning of the words. &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除了文字的字面意思之外。&#124;'
- en: '&#124; That meaning is either yes or no. &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这个意思是“是”或“否”。&#124;'
- en: '&#124; You, a highly intelligent and knowledgeable AI assistant, are asked
    to finish the text with the &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你，一个高度智能且知识渊博的AI助手，被要求用&#124;完成文本。'
- en: '&#124; correct meaning, either yes or no. &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确的意思，即“是”或“否”。&#124;'
- en: '&#124; The task begins: &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务开始：&#124;'
- en: '&#124; Esther asked "" and Juan responded "", which means
     &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 艾丝特问了“”，胡安回答了“”，这意味着&#124;'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 8: Chain-of-thought (CoT) prompt templates. One of the six chain-of-thought
    prompt templates we use for the CoT experiment. Note that this is a 5-shot prompt.
    Each prompt variation contains five CoT examples. The other five variations are
    separately added to the supplementary materials'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：链式思维（CoT）提示模板。我们用于CoT实验的六个链式思维提示模板之一。请注意，这是一个5-shot提示。每个提示变体包含五个CoT示例。其他五个变体分别添加到补充材料中。
- en: '| # | Prompt template |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| # | 提示模板 |'
- en: '| 1 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 1 |'
- en: '&#124; Bob asks Alice a question, and Alice responds with an implicature. This
    means that Alice’s response &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃问爱丽丝一个问题，爱丽丝以含意回应。这意味着爱丽丝的回答&#124;'
- en: '&#124; does not literally contain the answer to Bob’s question, but implies
    an answer. Assuming that Alice is a &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不直接包含对鲍勃问题的答案，但隐含了一个答案。假设爱丽丝是一个&#124;'
- en: '&#124; cooperative conversational partner, what is the implicated answer to
    the question? For example: &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合作的对话伙伴，问题的隐含答案是什么？例如：&#124;'
- en: '&#124; Bob: You invented fire? &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃：你发明了火？&#124;'
- en: '&#124; Alice: I told you that. &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝：我告诉过你。&#124;'
- en: '&#124; Implicature: Alice says ‘I told you that’. Alice’s response must be
    relevant to Bob’s question because &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含意：爱丽丝说“我告诉过你”。爱丽丝的回应必须与鲍勃的问题相关，因为&#124;'
- en: '&#124; Alice is a cooperative conversational partner. Alice must mean that
    she told Bob that she invented fire. &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝是一个合作的对话伙伴。爱丽丝必须意味着她告诉了鲍勃她发明了火。&#124;'
- en: '&#124; Alice’s response to Bob’s question ’You invented fire?’ is yes. &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝对鲍勃的问题“你发明了火吗？”的回答是“是”。&#124;'
- en: '&#124; Answer: yes &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 答案：是的&#124;'
- en: '&#124; Bob: That cake looks delicious. Aren’t you going to have some with me?
    &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃：那蛋糕看起来很美味。你不打算和我一起吃点吗？&#124;'
- en: '&#124; Alice: But that was a huge meal we just had. &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝：但那顿饭真的很丰盛。&#124;'
- en: '&#124; Implicature: Alice’s response must be relevant to Bob’s question because
    Alice is a cooperative &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义：爱丽丝的回答必须与鲍勃的问题相关，因为爱丽丝是一个合作的&#124;'
- en: '&#124; conversational partner. Alice must mean that the meal they just had
    was so huge she is not hungry &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对话伙伴。爱丽丝必须指的是他们刚吃过的那顿饭太大，她不再饿&#124;'
- en: '&#124; anymore, and this must be relevant to Bob’s question: ‘Aren’t you going
    to have some with me?’ &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不再，并且这必须与鲍勃的问题相关：“你不打算和我一起吃吗？”&#124;'
- en: '&#124; Alice’s response to the question must therefore be no. &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝对问题的回答因此必须是否。&#124;'
- en: '&#124; Answer: no &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答：不&#124;'
- en: '&#124; Bob: Could you perform well? &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃：你能表现好吗？&#124;'
- en: '&#124; Alice: Being bilingual would help put me ahead of the pack. &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝：能说双语会让我在竞争中脱颖而出。&#124;'
- en: '&#124; Implicature: Alice says being bilingual would help put her ahead of
    the pack. Alice’s response must &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义：爱丽丝说，能说双语会让她在竞争中脱颖而出。爱丽丝的回答必须&#124;'
- en: '&#124; be relevant to Bob’s question because Alice is a cooperative conversational
    partner. Alice must be &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 必须与鲍勃的问题相关，因为爱丽丝是一个合作的对话伙伴。爱丽丝必须&#124;'
- en: '&#124; implying that she could perform well because she is bilingual. This
    means the response to Bob’s &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 暗示她可能表现良好，因为她会说双语。这意味着对鲍勃的&#124;'
- en: '&#124; question is yes. &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题是“是”。&#124;'
- en: '&#124; Answer: yes &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答：是&#124;'
- en: '&#124; Bob: Have you any news for me? &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃：你有消息告诉我吗？&#124;'
- en: '&#124; Alice: I’ve made progress &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝：我取得了进展&#124;'
- en: '&#124; Implicature: Alice says she has made progress. Alice’s response must
    be relevant to Bob’s &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义：爱丽丝说她取得了进展。爱丽丝的回答必须与鲍勃的&#124;'
- en: '&#124; question because Alice is a cooperative conversational partner. If Alice
    would not have any &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题，因为爱丽丝是一个合作的对话伙伴。如果爱丽丝没有任何&#124;'
- en: '&#124; news for Bob, Alice would not have said she would have made progress
    because that would &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对鲍勃来说，爱丽丝不会说她取得了进展，因为那样会&#124;'
- en: '&#124; be misleading. The answer to Bob’s question ‘Have you any news for me?’
    must therefore be yes. &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可能会误导。对鲍勃问题“你有消息告诉我吗？”的回答因此必须是“是”。&#124;'
- en: '&#124; Answer: yes &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答：是&#124;'
- en: '&#124; Bob: You looked out for him? &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃：你照顾过他吗？&#124;'
- en: '&#124; Alice: He looked out for me. He taught me. &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝：他照顾过我。他教了我。&#124;'
- en: '&#124; Implicature: Bob asks Alice ‘You looked out for him?’ and Alice’s response
    says that the person &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义：鲍勃问爱丽丝“你照顾过他吗？”而爱丽丝的回答表明那个人&#124;'
- en: '&#124; that is being referred to by ‘him’ here looked out for Alice. If Alice
    meant yes to Bob’s &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 被‘他’指代的人照顾了爱丽丝。如果爱丽丝对鲍勃的&#124;'
- en: '&#124; question, Alice would have said something like ‘he also looked out for
    me’. Stating the &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题，爱丽丝会说类似‘他也照顾过我’的话。说明&#124;'
- en: '&#124; response like this implies that the answer to Bob’s question is no.
    &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这样的回答暗示对鲍勃的问题的答案是否。&#124;'
- en: '&#124; Answer: no &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回答：不&#124;'
- en: '&#124; Only output a ‘yes’ or ‘no’ as a final answer. Write your reasoning
    after ‘Implicature:’ &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅以‘是’或‘不’作为最终答案。将你的推理写在‘含义：’之后&#124;'
- en: '&#124; and then output either ‘Answer: yes’ or ‘Answer: no’. &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 然后输出‘回答：是’或‘回答：不’。&#124;'
- en: '&#124; Bob:  &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 鲍勃： &#124;'
- en: '&#124; Alice:  &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 爱丽丝： &#124;'
- en: '&#124; Implicature: &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 含义：&#124;'
- en: '|'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Appendix G Model categorization
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 模型分类
- en: 'Table [9](#A7.T9 "Table 9 ‣ Appendix G Model categorization ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") contains details on the model classes that are a part of each group
    of models we evaluate, along with their model sizes.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9](#A7.T9 "表9 ‣ 附录G 模型分类 ‣ 语用理解的黄金分割点：微调策略对LLMs的含义解析至关重要")包含了我们评估的每组模型的模型类别及其模型大小的详细信息。
- en: 'Table 9: Model categorization for each of the models. DL stands for dialogue,
    FT for fine-tuning, BL for benchmark-level, EL for example-level, and IT for instruction-tuning.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：每个模型的分类。DL代表对话，FT代表微调，BL代表基准级别，EL代表示例级别，IT代表指令微调。
- en: '| Group | Model class | Model IDs | Model size | Instruct |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 模型类别 | 模型ID | 模型大小 | 指令 |'
- en: '| Base | BERT | base uncased | 110M | No |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Base | BERT | base uncased | 110M | 否 |'
- en: '| RoBERTa | base, large | 125M, 355M | No |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | base, large | 125M, 355M | 否 |'
- en: '| GPT-2 | GPT-2 medium, large, xl | 354M, 774M, 1.6B | No |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | GPT-2 medium, large, xl | 354M, 774M, 1.6B | 否 |'
- en: '| EleutherAI | GPT-J, GPT-NeoX | 125M, 1.3B, 2.7B, 6B, 20B | No |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI | GPT-J, GPT-NeoX | 125M, 1.3B, 2.7B, 6B, 20B | 否 |'
- en: '| BLOOM | - | 560M, 1B1, 3B, 7B1, 176B | No |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | - | 560M, 1B1, 3B, 7B1, 176B | 否 |'
- en: '| OPT | - | 125M, 350M, 1.3B, 13B, 30B, 66B, 175B | No |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| OPT | - | 125M, 350M, 1.3B, 13B, 30B, 66B, 175B | 否 |'
- en: '| Cohere | small, medium, large, XL | 409.3M, 6.067B, 13.12B, 52.4B | No |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Cohere | small, medium, large, XL | 409.3M, 6.067B, 13.12B, 52.4B | 否 |'
- en: '| GPT-3 | ada, babbage, curie, davinci | Est. 350M, 1.3B, 6.7B, 175B | No |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | ada, babbage, curie, davinci | 估计 350M, 1.3B, 6.7B, 175B | 否 |'
- en: '| DL FT | BlenderBot | - | 90M, 2.7B, 9.4B | No |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| DL FT | BlenderBot | - | 90M, 2.7B, 9.4B | 否 |'
- en: '| BL IT | T0 | - | 3B, 11B | Yes |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| BL IT | T0 | - | 3B, 11B | 是 |'
- en: '| Flan-T5 | - | 780M, 3B, 11B | Yes |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5 | - | 780M, 3B, 11B | 是 |'
- en: '| EL IT | Cohere-command | medium, xlarge | 6.067B, 52.4B | Yes |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| EL IT | Cohere-command | medium, xlarge | 6.067B, 52.4B | 是 |'
- en: '| text-davinci-001 | ada, babbage, curie, davinci-1 | Unknown, left-to-right
    increasing in size | Yes |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001 | ada, babbage, curie, davinci-1 | 未知，从左到右大小递增 | 是 |'
- en: '|  | text-davinci-002 | - | Unknown | Yes |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | text-davinci-002 | - | 未知 | 是 |'
- en: '|  | text-davinci-003 | - | Unknown | Yes |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | text-davinci-003 | - | 未知 | 是 |'
- en: '|  | ChatGPT | gpt-3.5.turbo | Unknown | Yes |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | ChatGPT | gpt-3.5.turbo | 未知 | 是 |'
- en: '|  | GPT-4 | gpt-4 | Unknown | Yes |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4 | gpt-4 | 未知 | 是 |'
- en: Appendix H Human evaluation
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 人工评估
- en: 'The participants for the human evaluation in this paper were recruited using
    Prolific ([www.prolific.co](www.prolific.co)). The setup of the experiment is
    as follows. We divide the test set of 600 examples into four non-overlapping subsets
    of 150 examples. Each set of 150 examples was given to five unique annotators.
    This means each example in the test set is labeled five times by different people,
    and we have in total twenty annotators for the whole test set (five different
    ones for each of the four subsets). The only constraint for the annotators is
    that they are native English speakers. In Figure [5](#A8.F5 "Figure 5 ‣ Appendix
    H Human evaluation ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs") the screen shown to potential participants
    on Prolific is shown. Participants are paid 15 pounds an hour, which was the living
    wage at the time of the experiment and more than the 12 dollars an hour Prolific
    recommends. The total amount spent on the human evaluation is 236 pounds. This
    amount came to be from four subsets, each costing about  30 minutes to label per
    annotators, and having 5 annotators per subset: 15 * 4 * 0.5 * 5 = 150\. The extra
    costs were for the annotator that didn’t pass the attention check which we paid
    nonetheless, and for prolific as a platform.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '本文中人工评估的参与者是通过 Prolific 招募的（[www.prolific.co](www.prolific.co)）。实验的设置如下。我们将600个样本的测试集分成四个不重叠的子集，每个子集150个样本。每个150个样本的子集被分配给五位独特的标注员。这意味着测试集中的每个样本被不同的人标注了五次，我们总共有二十位标注员用于整个测试集（每个四个子集中有五位不同的标注员）。对标注员的唯一要求是他们是以英语为母语的人。在图
    [5](#A8.F5 "Figure 5 ‣ Appendix H Human evaluation ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")
    中展示了在 Prolific 上向潜在参与者展示的屏幕。参与者的时薪为15英镑，这在实验时是生活工资，高于 Prolific 推荐的12美元每小时。用于人工评估的总金额为236英镑。这个金额来自四个子集，每个子集的标注时间大约为30分钟，每个子集有5名标注员：15
    * 4 * 0.5 * 5 = 150。额外的费用是支付给未通过注意力检查的标注员的费用，尽管如此我们还是支付了这些费用，还有 Prolific 平台的费用。'
- en: '![Refer to caption](img/e27c15705a50a55bd56497660806a2d0.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e27c15705a50a55bd56497660806a2d0.png)'
- en: 'Figure 5: A screenshot of how the experiment is presented to potential annotators
    on Prolific ([www.prolific.co](www.prolific.co)).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：实验在 Prolific 上展示给潜在标注员的截图（[www.prolific.co](www.prolific.co)）。
- en: '![Refer to caption](img/905aa758ef857d673e21736de48026d3.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/905aa758ef857d673e21736de48026d3.png)'
- en: (a) The start of the Google form participants are asked to fill out for the
    human study.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 参与者在进行人工研究时需要填写的 Google 表单的开始部分。
- en: '![Refer to caption](img/8de8d0823c981d585fd007569c653119.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8de8d0823c981d585fd007569c653119.png)'
- en: (b) Part of the Google form the participants are asked to fill out. The second
    question in this image is part of the attention test. Juan’s response does not
    contain an implicature but simply gives away the correct answer.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 参与者需要填写的 Google 表单的一部分。这张图片中的第二个问题是注意力测试的一部分。胡安的回答没有包含隐含意义，而只是给出了正确答案。
- en: 'Figure 6: Screenshots of the Google form participants fill out as part of the
    implicature study.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：参与者在隐含意义研究中填写的 Google 表单的截图。
- en: 'The 150 test examples are wrapped in prompt template 2 (see Table [5](#A6.T5
    "Table 5 ‣ Appendix F Detailed prompt templates ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs"))
    and presented in a Google form. We opted to wrap all examples in prompt template
    2 to make the full human study directly comparable to the model’s results on template
    2\. If we had done a mix of all templates we either had to spent six times as
    much on the human evalyations (which was not within our budget) or subsample evaluations,
    making it less comparable to part of the model study. Although models have been
    shown to be very sensitive to prompt wording, humans are less likely to perform
    differently for different prompt templates. All templates are coherent natural
    language that any native English speaker will understand. That said, this is speculative,
    and to confirm this hypothesis future work should investigate the effect of different
    wordings on implicature resolution by humans. The participants are asked to choose
    the correct continuation, yes or no (see Figure [6(a)](#A8.F6.sf1 "In Figure 6
    ‣ Appendix H Human evaluation ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")). As recommended by Prolific,
    we subject the participants to an attention test (see Figure [6(b)](#A8.F6.sf2
    "In Figure 6 ‣ Appendix H Human evaluation ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")). At three random
    places in the form, we add a question that does not contain an implicature and
    obviously maps to “yes”. In this way, if the participants fails at least two of
    these questions, we can conclude they were not paying attention and remove their
    answers from the result. In practice, this happened once and we decided to pay
    the participant regardless, but discard their results, which were close to random.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 150 个测试示例被包装在提示模板 2 中（见表 [5](#A6.T5 "表 5 ‣ 附录 F 详细提示模板 ‣ 实用理解的金发女孩：微调策略对隐含义解析的影响")），并在
    Google 表单中呈现。我们选择将所有示例包装在提示模板 2 中，以使整个人工研究与模型在模板 2 上的结果直接可比。如果我们使用混合的所有模板，要么需要在人工评估上花费六倍的费用（这超出了我们的预算），要么需要对评估进行子抽样，使其与部分模型研究不那么可比。尽管模型对提示措辞非常敏感，但人类对不同提示模板的表现差异较小。所有模板都是连贯的自然语言，任何以英语为母语的人都能理解。也就是说，这仍然是推测性的，为了确认这一假设，未来的工作应研究不同措辞对人类隐含义解析的影响。参与者被要求选择正确的继续选项，是或否（见图
    [6(a)](#A8.F6.sf1 "在图 6 ‣ 附录 H 人工评估 ‣ 实用理解的金发女孩：微调策略对隐含义解析的影响")）。根据 Prolific 的建议，我们对参与者进行了注意力测试（见图
    [6(b)](#A8.F6.sf2 "在图 6 ‣ 附录 H 人工评估 ‣ 实用理解的金发女孩：微调策略对隐含义解析的影响")）。在表单中的三个随机位置，我们添加了一个不包含隐含义的问题，并明显映射到“是”。这样，如果参与者至少在两个这些问题上失败，我们可以得出他们未认真回答的结论，并从结果中删除他们的答案。在实践中，这种情况发生过一次，我们决定无论如何支付参与者，但丢弃了他们的结果，这些结果接近随机。
- en: 'Table [10](#A8.T10 "Table 10 ‣ Appendix H Human evaluation ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") shows the performance of each annotator on the subset they annotated.
    The average human performance across subsets and annotators is 86.2% $\pm$ 1.5\.
    The column “IAA” shows the average Cohen’s Kappa coefficient which is the pairwise
    inter-annotator agreement for each annotator per subset. All agreements are substantial
    according to the interpretation guidelines for Cohen’s Kappa (between 0.61–0.80).'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [10](#A8.T10 "表 10 ‣ 附录 H 人工评估 ‣ 实用理解的金发女孩：微调策略对隐含义解析的影响") 显示了每个标注员在他们标注的子集上的表现。跨子集和标注员的平均人类表现为
    86.2% $\pm$ 1.5。列“IAA”显示了每个标注员在每个子集上的平均 Cohen’s Kappa 系数，即配对互标注者一致性。根据 Cohen’s
    Kappa 的解释指南，所有的一致性都很显著（在 0.61–0.80 之间）。
- en: 'Table 10: The performance of the human annotators on the subsets of the test
    set. Subset 1 through 4 are non-overlapping and cover the whole test set. Annotator
    X for subset Y might be a different human than annotator X for subset Z. IAA is
    the average pairwise inter-annotator agreement (Cohen’s kappa coefficient) between
    annotators per subset.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：人类标注员在测试集子集上的表现。子集 1 至 4 互不重叠，覆盖整个测试集。子集 Y 的标注员 X 可能与子集 Z 的标注员 X 是不同的人。IAA
    是每个子集标注员之间的平均配对互标注者一致性（Cohen’s kappa 系数）。
- en: '| Annotator | 1 | 2 | 3 | 4 | 5 | Mean | Best | Worst | IAA |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 标注员 | 1 | 2 | 3 | 4 | 5 | 平均值 | 最佳 | 最差 | IAA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Subset 1 | 86.0% | 92.0% | 90.7% | 90.6% | 86.0% | 89.1% | 92.0% | 86.0%
    | 0.73 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 子集 1 | 86.0% | 92.0% | 90.7% | 90.6% | 86.0% | 89.1% | 92.0% | 86.0% | 0.73
    |'
- en: '| Subset 2 | 84.7% | 83.3% | 87.3% | 86.0% | 86.0% | 85.5% | 87.3% | 83.3%
    | 0.64 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 子集 2 | 84.7% | 83.3% | 87.3% | 86.0% | 86.0% | 85.5% | 87.3% | 83.3% | 0.64
    |'
- en: '| Subset 3 | 84.0% | 85.3% | 88.0% | 86.0% | 82.7% | 85.2% | 88.0% | 82.7%
    | 0.78 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 子集 3 | 84.0% | 85.3% | 88.0% | 86.0% | 82.7% | 85.2% | 88.0% | 82.7% | 0.78
    |'
- en: '| Subset 4 | 85.3% | 82.7% | 84.0% | 82.0% | 92.0% | 85.2% | 92.0% | 82.0%
    | 0.71 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 子集 4 | 85.3% | 82.7% | 84.0% | 82.0% | 92.0% | 85.2% | 92.0% | 82.0% | 0.71
    |'
- en: '| Total | - | - | - | - | - | 86.2% | 89.8% | 83.5% | 0.72 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | - | - | - | - | - | 86.2% | 89.8% | 83.5% | 0.72 |'
- en: '| Std | - | - | - | - | - | 2.3 | 2.2 | 1.5 | 0.1 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | - | - | - | - | - | 2.3 | 2.2 | 1.5 | 0.1 |'
- en: Human source of disagreement with ground-truth. We do an analysis of the source
    of disagreement with the ground-truth. We explicitly do not call this error, as
    in some cases examples might allow multiple interpretations, and both could be
    right. In other cases, the ground-truth might be wrong.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 与真实答案的分歧的来源：我们对与真实答案的分歧来源进行了分析。我们明确不将其称为错误，因为在某些情况下，例子可能允许多重解释，两者都可能是正确的。在其他情况下，真实答案可能是错误的。
- en: 'Annotation errors and multiple interpretations: We analyse the examples for
    which most humans choose a different answer than the ground-truth. For 30 out
    of 600 examples in the test set, only one or zero people choose the same answer
    as the ground-truth. Of these examples, most are annotated wrongly (18 of 30).
    For example: ‘Are you busy?’, ‘I’m drowning in work.’, implicature: ‘no’. Some
    are examples that can have multiple different interpretations (12 of 18), and
    the ground-truth answer likely just chooses one that is unnatural to humans. For
    example: ‘You don’t remember them?’, ‘Leave me alone!’, implicature: ‘yes’. 6
    of the 30 examples are particularised, and 1 is generalised.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 注释错误和多重解释：我们分析了大多数人选择的答案与真实答案不同的例子。在测试集中，600个例子中有30个例子的答案只有一个或零个与真实答案一致。在这些例子中，大多数被错误标注（30个中有18个）。例如：“你忙吗？”，“我忙得不可开交。”，隐含意：‘不’。其中一些是可以有多种不同解释的例子（18个中的12个），而真实答案可能只是选择了一个对人类来说不自然的答案。例如：“你不记得他们了吗？”，“别打扰我！”，隐含意：‘是’。30个例子中有6个是具体化的，1个是泛化的。
- en: 'Examples for which all humans agree with the ground-truth: There are 409 out
    of 600 examples that all humans get correct. This set of examples contains most
    of the generalised implicatures (39 out of 47). These contain 58 out of 94 particularised
    examples.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 所有人都同意真实答案的例子：600个例子中有409个例子所有人都答对了。这些例子包含了大多数泛化隐含意（47中的39个）。这些例子中有94个具体化例子中的58个。
- en: 'Examples most humans agree with the ground-truth: When we look at examples
    that 3 or more humans got correct, that comprises most of the test set (530 of
    600), and all of the generalised examples (47 of 47). This subset has 78 of 94
    particularised examples, so for 16 particularised examples 3 or more humans disagree
    with the ground-truth.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人同意真实答案的例子：当我们查看3人或更多人答对的例子时，这些例子占据了测试集的大部分（600中的530个），以及所有的泛化例子（47中的47个）。这个子集有94个具体化例子中的78个，所以在16个具体化例子中，3人或更多人对真实答案有分歧。
- en: Appendix I Comparison with BIG-bench implicatures task
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 与 BIG-bench 隐含任务的比较
- en: One of the BIG-bench tasks is related to the task in this work⁸⁸8[https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/implicatures](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/implicatures).
    It uses the same underlying dataset we use (George and Mamidi,, [2020](#bib.bib21)).
    With the below we aim to discuss our contribution in light of the BIG-bench result.
    To summarise; the methodology used by the BIG-bench task contributors has limitations,
    which call into question the validity of their claims. Further, some of the BIG-bench
    results are irreproducible due to missing details in the tech report and the use
    of proprietary models. Considering this, our work is an important contribution
    validating the BIG-bench results in a reproducible and methodologically sound
    way, and above that providing insight into what aspects of LLM training are crucial
    for the ability to do pragmatic inferences.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: BIG-bench 任务之一与本研究中的任务相关⁸⁸8[https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/implicatures](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/implicatures)。它使用了与我们相同的基础数据集（George
    和 Mamidi，， [2020](#bib.bib21)）。以下我们旨在根据 BIG-bench 的结果讨论我们的贡献。总结；BIG-bench 任务贡献者所使用的方法论存在局限性，这对他们的主张的有效性提出了质疑。此外，由于技术报告中缺少细节和使用专有模型，一些
    BIG-bench 结果是不可重复的。考虑到这一点，我们的工作是对 BIG-bench 结果进行可重复且方法论上可靠的验证的重要贡献，并且提供了对 LLM
    训练中哪些方面对进行语用推断至关重要的见解。
- en: 'Limitations of the methodological approach of the task contributors in BIG-bench
    implicatures. Our benchmark has 30% more data, which the BIG-bench task contributors
    discard. In this section we motivate the crucial importance of that data for evaluating
    implicature understanding (Section [I.1](#A9.SS1 "I.1 Discarding ambiguous examples
    ‣ Appendix I Comparison with BIG-bench implicatures task ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")),
    and why BIG-bench in turn might be overestimating the performance of LLMs on implicature
    resolution (Section [I.2](#A9.SS2 "I.2 Overestimation of performance on implicature
    understanding ‣ Appendix I Comparison with BIG-bench implicatures task ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs")). Moreover, the human performance on the BIG-bench task indicates low
    quality human annotation, which we will also elaborate upon below, noting that
    this is impossible to verify because the BIG-bench report does not detail how
    the evaluation was done for this task (Section [I.3](#A9.SS3 "I.3 Other limitations
    ‣ Appendix I Comparison with BIG-bench implicatures task ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")).'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 'BIG-bench 含意中的任务贡献者方法论的局限性。我们的基准测试比 BIG-bench 任务贡献者舍弃的数据多出 30%。在这一部分，我们阐述了这些数据在评估含意理解中的关键重要性（第
    [I.1](#A9.SS1 "I.1 Discarding ambiguous examples ‣ Appendix I Comparison with
    BIG-bench implicatures task ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 节），以及为什么 BIG-bench 可能会高估
    LLM 在含意解决上的表现（第 [I.2](#A9.SS2 "I.2 Overestimation of performance on implicature
    understanding ‣ Appendix I Comparison with BIG-bench implicatures task ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") 节）。此外，BIG-bench 任务中的人类表现表明了低质量的人类标注，我们将在下文中详细阐述，指出由于 BIG-bench 报告未详细说明该任务的评估方式，因此这一点无法验证（第
    [I.3](#A9.SS3 "I.3 Other limitations ‣ Appendix I Comparison with BIG-bench implicatures
    task ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") 节）。'
- en: I.1 Discarding ambiguous examples
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.1 丢弃模棱两可的例子
- en: The BIG-bench task preprocesses the 1001 examples that George and Mamidi, ([2020](#bib.bib21))
    curate by keeping only yes/no questions, discarding any examples that are ambiguous
    according to the task contributors, and discarding remaining examples to create
    a 50/50 distribution of yes/no answers. This leaves them with 492 examples. Of
    these examples, 81 appear in our development set and the remaining 411 appear
    in our test set. Our test set has 600 examples, so BIG-bench effectively discarded
    189 ambiguous examples compared to our test set; a bit more than 30% of the benchmark.
    To illustrate the importance of not discarding this data, we cherry picked a few
    examples that the BIG-bench authors discarded from the data.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: BIG-bench任务通过仅保留是/否问题，丢弃任何在任务贡献者看来模糊的例子，并丢弃剩余的例子以创建50/50的是/否回答分布，从而预处理了George和Mamidi（[2020](#bib.bib21)）策划的1001个例子。这使得他们剩下了492个例子。在这些例子中，81个出现在我们的开发集中，其余的411个出现在我们的测试集中。我们的测试集有600个例子，因此BIG-bench相对于我们的测试集有效地丢弃了189个模糊的例子；比基准多出了30%以上。为了说明不丢弃这些数据的重要性，我们挑选了一些BIG-bench作者丢弃的数据中的例子。
- en: •
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Utterance: "Can you lend me hundred dollars?", Response: "Is this supposed
    to be some kind of a joke?", Implicature: "No"'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 话语：“你能借我一百美元吗？”，回应：“这是某种玩笑吗？”，含义：“不”
- en: •
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Utterance: "Do you know, how long is Uncle Arthur staying with us?", Response:
    "Ask your father.", Implicature: "No"'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 话语：“你知道亚瑟叔叔会和我们待多久吗？”，回应：“问你父亲。”，含义：“不”
- en: 'Indeed, these examples are ambiguous. Asking whether the request for a hundred
    dollars is a joke does not literally mean you’re saying no to the request. The
    response “ask your father” does not mean the speaker does not actually know, maybe
    they just do not want to respond. The humans in our study all infer the intended
    ground truth implicature. This shows a general property of implicatures; they
    are ambiguous, but often humans do infer the intended meaning. Ambiguity is not
    a discrete property. Some examples may be so vague that no one gets it. The following
    are examples the BIG-bench task discards that the humans in our study did struggle
    with:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，这些例子是模糊的。询问要求一百美元是否是个玩笑，并不意味着你实际上是在拒绝这个请求。回应“问你父亲”并不意味着说话者实际上不知道，也许他们只是想不回应。我们研究中的人类都推断出了意图中的真实含义。这展示了含义的一个普遍特性；它们是模糊的，但人类通常确实能推断出意图的意思。模糊性不是一个离散的属性。有些例子可能模糊到没有人能理解。以下是BIG-bench任务丢弃的例子，而我们研究中的人类确实遇到了困难：
- en: •
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Utterance: "Got any more of those?", Response: "Nothing I’m at liberty to reveal
    here.", Implicature: "Yes"'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 话语：“还有那些吗？”，回应：“我不能在这里透露什么。”，含义：“有”
- en: •
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Utterance: "Have you finished sight-seeing?", Response: "Sorry. I should’ve
    come to see you first.", Implicature: "Yes"'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 话语：“你完成观光了吗？”，回应：“抱歉。我应该先来见你。”，含义：“是”
- en: In the first of these the implicature is “yes” because the person responding
    is implying that they do have more, they just cannot reveal them. Otherwise they
    would most likely simply say no. In the second example it feels more natural that
    someone says this when they are finished sight-seeing, otherwise they would’ve
    probably said something to the effect of “I’m still out, but I’m sorry..”. In
    any case, humans in our study did not understand these responses like that. This
    illustrates another aspect of implicature; sometimes communication will go wrong
    over it. Removing implicatures that are ambiguous though, defeats the purpose
    of the task, as they are all ambiguous to a certain degree. The purpose of this
    study is to compare how humans resolve this type of non-literal language compared
    to how models do it. The human baseline of 86% accuracy that humans achieve on
    our test set deals more naturally with examples that are too ambiguous for models
    to understand than discarding examples based on the subjective opinion of a few
    people.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，含义是“有”，因为回应者暗示他们确实还有更多，只是不能透露。如果没有更多，他们很可能会直接说“没有”。在第二个例子中，当有人在观光结束后说这句话时感觉更自然，否则他们可能会说类似“我还在外面，但很抱歉”的话。无论如何，我们的研究中的人类并没有像这样理解这些回应。这展示了含义的另一个方面；有时沟通会因为这个问题而出现错误。然而，去除那些含糊不清的含义会破坏任务的目的，因为这些含义都有一定程度的模糊性。本研究的目的是比较人类如何解决这种非字面语言与模型的解决方式。人类在我们测试集上的基准准确率为86%，在处理那些对模型来说过于模糊的例子时，比基于少数人主观意见丢弃例子的处理方式更自然。
- en: I.2 Overestimation of performance on implicature understanding
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.2 对含义理解表现的高估
- en: On the overlapping part of our test set and theirs the humans in our study achieve
    92.8% accuracy. The best model on the BIG-bench task is PaLM, achieving a zero-shot
    performance of 64.4%. Note that this performance is on their full test set (not
    the overlapping part) and hence not directly comparable. Nonetheless, the missing
    examples are randomly sampled for our development set, and we can be pretty confident
    this number indicates a large gap with human performance. Two-shot PaLM comes
    very close to human performance with 91.7% accuracy, but of course this does not
    take into account the 189 more challenging examples that are part of our benchmark.
    Humans achieve 71.9% performance on this subset of ambiguous data, indicating
    that these data are more difficult than average, but nonetheless performance is
    higher than random. Without access to the models used to evaluate the BIG-bench
    task we cannot say anything for certain, but we expect the performance of PaLM
    to be overestimated as it does not account for a large part of the type of implicatures
    found in natural discourse; ambiguous ones.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们测试集和他们的重叠部分中，我们研究中的人类达到了92.8%的准确率。在BIG-bench任务中表现最佳的模型是PaLM，其零-shot表现为64.4%。请注意，这一表现是针对他们的完整测试集（而非重叠部分），因此不能直接比较。不过，遗漏的示例是随机抽样的，我们可以相当有信心地认为这一数字表明了与人类表现之间的巨大差距。两-shot的PaLM接近人类表现，准确率为91.7%，但当然这没有考虑到我们基准测试中189个更具挑战性的示例。人类在这部分含糊数据上取得了71.9%的表现，表明这些数据比平均水平更难，但尽管如此，表现仍高于随机水平。在没有访问用于评估BIG-bench任务的模型的情况下，我们不能确定什么，但我们预计PaLM的表现被高估了，因为它没有涵盖自然话语中大量存在的含义推断类型；即含糊的。
- en: I.3 Other limitations
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.3 其他局限性
- en: Poor quality human evaluation. The average human evaluator on BIG-bench implicatures
    achieves around 82% performance (where ours achieves on average 86% on a more
    challenging dataset), and their human best rater achieves 100% (where our human
    best is 92%). This difference between human average and best hints at poor quality
    average rating. This is impossible to verify because there is no information in
    the BIG-bench tech report on how the human evaluation was done exactly, or even
    which examples where evaluated.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 低质量的人类评估。在BIG-bench implicatures中，平均人类评估员的表现大约为82%（而我们的表现平均为86%在一个更具挑战性的数据集上），而他们的最佳人类评估员的表现达到了100%（而我们的最佳人类表现为92%）。人类平均值与最佳值之间的差异暗示了低质量的平均评分。这一点无法验证，因为BIG-bench技术报告中没有关于人类评估如何进行的详细信息，甚至没有评估的具体示例。
- en: No fine-tuned models. BIG-bench uses only base LLMs and no SotA fine-tuning
    methods. A question that remains is therefore, what aspects of LLMs contribute
    to their performance on implicatures? In our work we find that implicature performance
    emerges at a much smaller scale in models instruction fine-tuned at the example
    level, and that scale and prompting techniques are important.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 没有微调模型。BIG-bench仅使用基础LLM，并未使用SotA微调方法。因此，仍然存在一个问题：LLM的哪些方面有助于它们在含义推断上的表现？在我们的研究中，我们发现含义推断性能在以示例级别进行指令微调的模型中在更小的规模上出现，而且规模和提示技术很重要。
- en: Appendix J Chain-of-thought completions by GPT-4
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J GPT-4的链式思维完成
- en: GPT-4 reaches human-level performance with CoT prompting. In this section we
    look at a few completions to help illuminate how CoT-prompting improves implicature
    resolution.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4通过CoT提示达到了人类级别的表现。在本节中，我们查看一些完成情况，以帮助阐明CoT提示如何改善含义推断。
- en: Specifically, we look at some of the examples for which GPT-4 5-shot gets 0%
    accuracy (i.e. wrong for all templates), and GPT-4 5-shot CoT gets 100% accuracy
    (i.e. right for all templates). This is a set of 10 examples. We only look at
    the first template, but all templates get a similar performance with CoT.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们查看了GPT-4 5-shot获得0%准确率（即所有模板均错误）的示例集，而GPT-4 5-shot CoT获得100%准确率（即所有模板均正确）。这是一个包含10个示例的集合。我们仅查看第一个模板，但所有模板在CoT下的表现相似。
- en: 'Utterance: ‘Do you sell calculators?’'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 话语：‘你们卖计算器吗？’
- en: 'Response: ‘We are all sold out. We wont probably have any more in for a few
    weeks.’'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 回复：‘我们已经售罄了。我们可能几周内不会再有货。’
- en: 'Implicature: Yes. But we dont have any calculators to sell now.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 含义推断：是的。但是我们现在没有计算器出售。
- en: 'This is an example that GPT-4 gets wrong for all templates 5-shot, probably
    because the response has a negative sentiment, and also it means they do not sell
    calculators right now. However, the implicature is yes, because they do sell them
    normally. The CoT completion is:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个GPT-4在所有模板5-shot中都处理错误的示例，可能是因为回复有负面情感，这也意味着他们现在不卖计算器。然而，隐含意思是“是的”，因为他们通常会销售这些计算器。CoT完成情况是：
- en: 'CoT completion: ‘Alice says they are all sold out and won’t have any more in
    for a few weeks. Alice’s response must be relevant to Bob’s question because Alice
    is a cooperative conversational partner. Alice must be implying that they do sell
    calculators, but they currently don’t have any in stock. The answer to Bob’s question
    is yes. Answer: yes’'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: CoT完成情况：‘爱丽丝说他们的计算器已经全部售罄，几周内不会再进货。爱丽丝的回应必须与鲍勃的问题相关，因为爱丽丝是一个合作的对话伙伴。爱丽丝必须意味着他们确实销售计算器，但目前没有库存。因此，对鲍勃的问题的答案是“是”。答案：是’
- en: 'This example is similar to the one given in the main text in Section 4, and
    there are more like these in the dataset: the sentiment is negative, and an interpretation
    can be negative, but strictly the implicature is positive (yes).'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例类似于第4节主文本中给出的示例，数据集中还有更多类似的情况：情感是负面的，解读可以是负面的，但严格来说，隐含意思是积极的（是）。
- en: 'Utterance: ‘Do you know where he is?’'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 发言：‘你知道他在哪里吗？’
- en: 'Response: ‘I’ll find him.’'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 回复：‘我会找到他的。’
- en: 'Implicature: No.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含意思：否。
- en: 'GPT-4 gets this wrong for all templates 5-shot, and right for all of them with
    CoT:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在所有模板5-shot中都处理错误，但在所有CoT模板中都处理正确：
- en: 'CoT completion: ‘Alice says she will find him. Alice’s response must be relevant
    to Bob’s question because Alice is a cooperative conversational partner. If Alice
    knew where he is, she would have provided that information directly. Instead,
    she says she will find him, implying that she does not currently know his location.
    The answer to Bob’s question ’Do you know where he is?’ must therefore be no.
    Answer: no’'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: CoT完成情况：‘爱丽丝说她会找到他。爱丽丝的回应必须与鲍勃的问题相关，因为爱丽丝是一个合作的对话伙伴。如果爱丽丝知道他在哪里，她会直接提供该信息。相反，她说她会找到他，这意味着她目前不知道他的具体位置。因此，对鲍勃的问题‘你知道他在哪里吗？’的答案必须是“否”。答案：否’
- en: It’s unclear why GPT-4 gets this one wrong 5-shot (all humans get this one right),
    but the CoT is very relevant and helpful.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 不清楚为什么GPT-4在5-shot中会出错（所有人类在此问题上均回答正确），但CoT非常相关且有帮助。
- en: 'Utterance: ‘It seems Cathy is not happy at all with her new room mate. Did
    you ask her why?’'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 发言：‘似乎凯西对她的新室友一点也不满意。你问过她为什么吗？’
- en: 'Response: ‘Believe me. I tried to find out. But I simply could not pin it down.’'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 回复：‘相信我。我试图找出答案。但我根本无法确定。’
- en: 'Implicature: Yes. But Cathy did not give specific reasons for her feelings.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含意思：是的。但凯西没有具体说明她的感受原因。
- en: 'Again an example where the sentiment of the response seems to hint at no. Of
    5 human annotators, also only 2 interpret this response as “yes”. With a CoT,
    GPT-4 gets it right six times:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，其中回复的情感似乎暗示为否。在5名人工标注者中，只有2人将此回复解读为“是”。通过CoT，GPT-4正确处理了六次：
- en: 'CoT completion: ‘Alice says she tried to find out but couldn’t pin it down.
    Alice’s response must be relevant to Bob’s question because Alice is a cooperative
    conversational partner. Alice must mean that she attempted to ask Cathy why she
    is unhappy with her new roommate, but was unable to get a clear answer. The answer
    to Bob’s question ’Did you ask her why?’ must therefore be yes. Answer: yes’'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: CoT完成情况：‘爱丽丝说她试图找出答案，但无法确定。爱丽丝的回应必须与鲍勃的问题相关，因为爱丽丝是一个合作的对话伙伴。爱丽丝必须是说她试图询问凯西为什么对她的新室友不满意，但未能得到明确答案。因此，对鲍勃的问题‘你问过她为什么吗？’的答案必须是“是”。答案：是’
- en: 'A helpful reasoning trace. All CoT completions by the models we have run CoT
    on are available in the GitHub: [https://github.com/LauraRuis/do-pigs-fly](https://github.com/LauraRuis/do-pigs-fly).'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有帮助的推理痕迹。我们对模型运行CoT的所有CoT完成情况都可以在GitHub上找到：[https://github.com/LauraRuis/do-pigs-fly](https://github.com/LauraRuis/do-pigs-fly)。
- en: Appendix K Additional results
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 额外结果
- en: K.1 Contrastive experiment
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.1 对比实验
- en: In this section we reframe the implicature resolution task to a contrastive
    one, allowing the model to contrast the coherent to the incoherent sentence in
    a single prompt.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将隐含意思解析任务重新框定为对比任务，让模型在单个提示中对比连贯和不连贯的句子。
- en: Contrastive task. In the ranking task the model is required to assign higher
    likelihood to the coherent utterance than the incoherent one (.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 对比任务。在排名任务中，模型需要将连贯的发言分配更高的可能性，而不是不连贯的发言(。
- en: 'Which of the following sentences is coherent:'
  id: totrans-487
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下哪一句话是连贯的：
- en: 'A: Esther asked “Can you come to my party on Friday?” and Juan responded “I
    have to work”, which means no.'
  id: totrans-488
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'A: Esther问“你能来我周五的派对吗？”Juan回答“我得工作”，这意味着不行。'
- en: 'B: Esther asked “Can you come to my party on Friday?” and Juan responded “I
    have to work”, which means yes.'
  id: totrans-489
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'B: Esther问“你能来我周五的派对吗？”Juan回答“我得工作”，这意味着可以。'
- en: 'Answer:'
  id: totrans-490
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '答案:'
- en: We can now evaluate the models’ ability to understand which is the coherent
    sentence by evaluating whether it assigns $p_{\theta}\left(A\mid\mathbf{p}\right)>
    we can again formulate the task by $来评估模型理解哪个句子是连贯的能力，我们可以再次通过<math
    id=$来重新制定任务。不同之处在于，在连贯和不连贯的提示中，模型可以将连贯和不连贯的发言相互对比。我们随机分配A和B到发言中。
- en: 'We do a small experiment with the contrastive task with one of the best performing
    models overall, OpenAI’s text-davinci-002, for $k=\{0,1,5\}$ would look as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对一个表现最佳的模型之一OpenAI的text-davinci-002进行了一个小实验，对于$k=\{0,1,5\}$，结果如下：
- en: 'Which of the following sentences is coherent:'
  id: totrans-493
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 以下哪一句话是连贯的：
- en: 'A: Esther asked “Can you come to my party on Friday?” and Juan responded “I
    have to work”, which means no.'
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'A: Esther问“你能来我周五的派对吗？”Juan回答“我得工作”，这意味着不行。'
- en: 'B: Esther asked “Can you come to my party on Friday?” and Juan responded “I
    have to work”, which means yes.'
  id: totrans-495
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'B: Esther问“你能来我周五的派对吗？”Juan回答“我得工作”，这意味着可以。'
- en: 'Answer: Esther asked “Can you come to my party on Friday?” and Juan responded
    “I have to work”, which means no.'
  id: totrans-496
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '答案: Esther问“你能来我周五的派对吗？”Juan回答“我得工作”，这意味着不行。'
- en: 'Table 11: Performance on the implicature task framed contrastively by OpenAI’s
    text-davinci-002\. The mean and standard deviation are reported over two different
    prompt templates (template 1 and 2).'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '表11: OpenAI的text-davinci-002在对比性隐含任务上的表现。均值和标准差是基于两个不同的提示模板（模板1和模板2）报告的。'
- en: '| k | Non-contrastive | Rank one, two | Rank A, B | Rank full text |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| k | 非对比 | 排名一、二 | 排名A、B | 排名全文 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 71.3% $\pm$ 0.6 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 71.3% $\pm$ 0.6 |'
- en: '| 1 | 76.1% $\pm$ 0.9 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 76.1% $\pm$ 0.9 |'
- en: '| 5 | 80.5% $\pm$ 2.1 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 80.5% $\pm$ 2.1 |'
- en: 'In Table [11](#A11.T11 "Table 11 ‣ K.1 Contrastive experiment ‣ Appendix K
    Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy
    Matters for Implicature Resolution by LLMs"), perhaps surprisingly, we can see
    that the contrastive task is much more difficult than the original ranking task.
    For $k=0$ the full text ranking does best, but is still significantly worse than
    the original ranking setup. Because of these disappointing results, we did not
    evaluate the other models contrastively. Future work must establish whether the
    contrastive setup is worse across all model classes and sizes.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[11](#A11.T11 "表11 ‣ K.1 对比实验 ‣ 附录K 额外结果 ‣ 语用理解的金发姑娘：微调策略对隐含解析的影响")中，也许出乎意料的是，我们可以看到对比任务比原始排名任务要困难得多。对于$k=0$，全文排名表现最好，但仍然明显逊色于原始排名设置。由于这些令人失望的结果，我们没有对其他模型进行对比评估。未来的工作必须确定对比设置是否在所有模型类别和尺寸中都表现更差。
- en: K.2 Variance over prompt ordering
  id: totrans-504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.2 提示顺序的方差
- en: As mentioned in Section 3 in the main text, models are sensitive to the ordering
    of the $k$ examples in the prompt. Instead of marginalising over this random factor
    by evaluating all possible prompt orderings, we randomly sampled an ordered set
    of examples from the development set for each test example. Throughout experiments,
    we kept this randomly sampled order the same, meaning if you re-run the 5-shot
    evaluation you get exactly the same orderings. The reason for this is that we
    want evaluate each model equally. In this section we ask how the performance chances
    for the best performing model if we select another random order. We do this for
    the 5-shot evaluation, because the results show that adding more in-context examples
    barely helps performance.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如主文本第3节所述，模型对提示中$k$个示例的排序非常敏感。我们没有通过评估所有可能的提示排序来消除这个随机因素，而是从开发集随机抽取了一组有序示例用于每个测试示例。在整个实验过程中，我们保持了这个随机抽取的顺序不变，这意味着如果你重新运行5-shot评估，你会得到完全相同的排序。这样做的原因是我们想要平等地评估每个模型。在本节中，我们询问如果选择另一个随机顺序，表现最好的模型性能会如何变化。我们这样做是因为5-shot评估的结果显示，增加更多的上下文示例几乎不会帮助性能提升。
- en: 'Table 12: Variance over prompt ordering for 5-shot evaluation per prompt template
    (P.T.) for text-davinci-002'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：text-davinci-002的5-shot评估每个提示模板（P.T.）的提示排序方差
- en: '| Seed | P. T. 1 | P. T. 2 | P. T. 3 | P. T. 4 | P. T. 5 | P. T. 6 | Mean |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | P. T. 1 | P. T. 2 | P. T. 3 | P. T. 4 | P. T. 5 | P. T. 6 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 80.17 | 78.17 | 82.83 | 80.50 | 79.17 | 76.50 | 79.56 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 80.17 | 78.17 | 82.83 | 80.50 | 79.17 | 76.50 | 79.56 |'
- en: '| 1 | 80.17 | 76.17 | 81.33 | 81.83 | 76.00 | 76.33 | 78.64 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 80.17 | 76.17 | 81.33 | 81.83 | 76.00 | 76.33 | 78.64 |'
- en: '| 2 | 79.50 | 78.17 | 81.17 | 80.17 | 78.17 | 76.50 | 78.94 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 79.50 | 78.17 | 81.17 | 80.17 | 78.17 | 76.50 | 78.94 |'
- en: '| mean | 79.94 | 77.50 | 81.78 | 80.83 | 77.78 | 76.44 | - |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 79.94 | 77.50 | 81.78 | 80.83 | 77.78 | 76.44 | - |'
- en: '| std | 0.31 | 0.94 | 0.75 | 0.72 | 1.32 | 0.08 | - |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 0.31 | 0.94 | 0.75 | 0.72 | 1.32 | 0.08 | - |'
- en: 'Table [12](#A11.T12 "Table 12 ‣ K.2 Variance over prompt ordering ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") shows the results of this
    experiment. Some prompt templates seem to be more sensitive to prompt example
    ordering than others, but for none of them the variance is high enough to change
    any conclusions.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [12](#A11.T12 "表 12 ‣ K.2 提示排序的方差 ‣ 附录 K 附加结果 ‣ 语言模型的含意解析：微调策略对含意解析的影响") 显示了这次实验的结果。一些提示模板似乎对提示示例排序更为敏感，但没有哪个模板的方差足够高到需要改变结论。
- en: K.3 Different zero-shot instruction prompts
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.3 不同的零-shot指令提示
- en: There is a narrative around large language models that if they fail a task,
    it might be that the prompt was not the right one (through works like [Reynolds
    and McDonell, 2021b](#bib.bib56) ; Kojima et al., ([2022](#bib.bib37))). The idea
    is that they can be prompted to simulate almost anything, if you set them up correctly.
    Because implicature resolution is a ubiquitous result of learning language, we
    hold the view that a model should be able to do this task if a prompt is given
    in coherent natural language. Nonetheless, in an additional effort to find the
    “let’s think step-by-step” (Kojima et al.,, [2022](#bib.bib37)) of zero-shot implicature
    resolution we try three more prompt templates.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大型语言模型有一个说法，如果它们未能完成任务，可能是提示不正确（参见 [Reynolds 和 McDonell, 2021b](#bib.bib56)；Kojima
    等，([2022](#bib.bib37))）。这个观点是，如果设置正确，它们可以被提示模拟几乎任何事情。由于含意解析是学习语言的普遍结果，我们认为模型应该能够完成这个任务，只要提示是连贯的自然语言。然而，为了进一步寻找“逐步思考”（Kojima
    等，[2022](#bib.bib37)）的零-shot含意解析，我们尝试了另外三种提示模板。
- en: 'Table 13: Zero-shot accuracy over three additional prompt templates for a base
    LLM and two instructable models.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：对一个基础LLM和两个可指令模型的三个额外提示模板的零-shot准确率。
- en: '| Model | Templates |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 模板 |'
- en: '| --- | --- |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3-175b | 59.2% $\pm$ 4.5 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 59.2% $\pm$ 4.5 |'
- en: '| text-davinci-001-? | 66.1% $\pm$ 3.2 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-? | 66.1% $\pm$ 3.2 |'
- en: '| text-davinci-002-? | 67.7% $\pm$ 9.6 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-? | 67.7% $\pm$ 9.6 |'
- en: 'We evaluate a base large language model and two instructable models: GPT-3-175B,
    text-davinci-001, and text-davinci-002\. The prompts we use are taken from recent
    work that proposes a dialogue agent trained with human feedback (Glaese et al.,,
    [2022](#bib.bib22)), but adapted to the task of implicature resolution. The full
    prompts are presented in Table [7](#A6.T7 "Table 7 ‣ Appendix F Detailed prompt
    templates ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") and Table [13](#A11.T13 "Table 13 ‣ K.3 Different
    zero-shot instruction prompts ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") shows the results. The new templates do not improve performance for
    any of these models. The variance over the prompt templates for text-davinci-002
    is high, and the best prompt template of these three does achieve a slightly higher
    accuracy than the others: 74.5%. These results do not change the picture sketched
    so far.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了一个基础的大型语言模型和两个可指令模型：GPT-3-175B、text-davinci-001 和 text-davinci-002。我们使用的提示语来自最近的研究，该研究提出了一个通过人类反馈训练的对话代理（Glaese
    et al., [2022](#bib.bib22)），但已调整为隐含义解析任务。完整的提示语见表 [7](#A6.T7 "Table 7 ‣ Appendix
    F Detailed prompt templates ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 和表 [13](#A11.T13 "Table
    13 ‣ K.3 Different zero-shot instruction prompts ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") 显示了结果。这些新模板没有改善任何模型的性能。text-davinci-002 的提示模板方差较高，这三种提示模板中最好的一个确实比其他模板略高，达到了
    74.5%。这些结果没有改变迄今为止描绘的图景。'
- en: K.4 The effect of in-context examples on sensitivity to prompt wording
  id: totrans-524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.4 上下文示例对提示用词敏感性的影响
- en: '![Refer to caption](img/05e6ac337d5ce2ca2e4d6539891ff59e.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05e6ac337d5ce2ca2e4d6539891ff59e.png)'
- en: 'Figure 7: Relative performance increase over 0-shot due to in-context prompting.
    Structured prompt templates are dashed lines (1, 3, 4) and natural prompt templates
    dotted lines (2, 5, 6).'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：由于上下文提示导致的 0-shot 性能相对提升。结构化提示模板用虚线表示（1、3、4），自然提示模板用点线表示（2、5、6）。
- en: 'Figure [7](#A11.F7 "Figure 7 ‣ K.4 The effect of in-context examples on sensitivity
    to prompt wording ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")
    shows the relative performance increase due to in-context prompting broken down
    per prompt template. For text-davinci-001, most templates benefit similarly from
    more in-context examples, except for template 1\. Perhaps surprisingly, we see
    that this template already achieves a performance of 76.5% at the zero-shot evaluation
    and does not improve much with few-shot prompting. For Cohere-52B and OPT-175B
    we see a clear grouping between the structured prompts (dashed lines) and natural
    prompts (dotted lines). Cohere struggles significantly more with the structured
    prompts than with the natural prompts in the zero-shot evaluation, and few-shot
    prompting can mitigate that, lowering the standard deviation over prompt templates
    to 1.89 at $k=30$. OPT benefits from prompting for the natural prompts, but not
    for the structured prompts.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A11.F7 "Figure 7 ‣ K.4 The effect of in-context examples on sensitivity
    to prompt wording ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")
    显示了由于上下文提示导致的相对性能提升，按提示模板分类。对于 text-davinci-001，大多数模板在更多上下文示例下获益相似，除了模板 1。或许令人惊讶的是，我们发现该模板在零-shot
    评估中已达到 76.5% 的性能，并且在少量示例提示下改进不大。对于 Cohere-52B 和 OPT-175B，我们看到结构化提示（虚线）和自然提示（点线）之间有明显的分组。Cohere
    在结构化提示下的表现明显较差，而在自然提示下则有所改善，少量提示可以减轻这一点，将提示模板的标准差降低到 1.89，$k=30$ 时。OPT 在自然提示下受益于提示，但在结构化提示下则没有。'
- en: K.5 Variance over API runs
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.5 API 运行的方差
- en: 'In this section we comment on the reproducibility of research done using APIs.
    OpenAI and Cohere have their models behind an API, meaning we do not have control
    over what happens to the prompt before the model processes it. We run the zero-shot
    evaluation ten more times for two models of OpenAI and Cohere, text-davinci-002
    and Cohere-52B. The results from this experiment are shown in Table [14](#A11.T14
    "Table 14 ‣ K.5 Variance over API runs ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") and [15](#A11.T15 "Table 15 ‣ K.5 Variance over API runs ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs"). From this we can conclude
    that there is some stochasticity in the API that we have no control over, a bit
    more for OpenAI than for Cohere, but again we can be relatively confident that
    the conclusion will not be different because of it. The results from this work
    are therefore reproducible with access to the same models behind the API now.
    Unfortunately, when OpenAI or Cohere changes the models behind the API, these
    results are not exactly reproducible anymore.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论使用 API 进行研究的可重复性。OpenAI 和 Cohere 的模型通过 API 提供，这意味着我们无法控制在模型处理之前对提示所做的操作。我们对
    OpenAI 和 Cohere 的两个模型，text-davinci-002 和 Cohere-52B，进行了十次零样本评估。这项实验的结果见表 [14](#A11.T14
    "表 14 ‣ K.5 API 运行的方差 ‣ 附录 K 其他结果 ‣ 实用理解的金发女孩：微调策略对 LLMs 含义解决的重要性") 和 [15](#A11.T15
    "表 15 ‣ K.5 API 运行的方差 ‣ 附录 K 其他结果 ‣ 实用理解的金发女孩：微调策略对 LLMs 含义解决的重要性")。由此我们可以得出结论，API
    中存在一些我们无法控制的随机性，OpenAI 的随机性比 Cohere 稍多，但我们可以相对自信地认为，结论不会因此而有所不同。因此，只要有对同一 API
    背后的模型的访问，这项工作是可重复的。不幸的是，当 OpenAI 或 Cohere 更改 API 背后的模型时，这些结果将不再完全可重复。
- en: 'For completeness, we add the timestamp that each result was obtained below
    (Appendix [L](#A12 "Appendix L Timestamps API calls ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")).'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们在下面添加了每个结果获得的时间戳（附录 [L](#A12 "附录 L 时间戳 API 调用 ‣ 实用理解的金发女孩：微调策略对 LLMs
    含义解决的重要性")）。
- en: 'Table 14: Results per prompt template (P.T.) for 10 different runs from text-davinci-002
    for 0-shot evaluation.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：text-davinci-002 在 10 次不同运行中的每个提示模板 (P.T.) 的结果，进行零样本评估。
- en: Each evaluation has exactly the same text, so the variance in performance is
    due to API stochasticity.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 每次评估的文本完全相同，因此性能的方差是由于 API 随机性。
- en: '| API-run | P. T. 1 | P. T. 2 | P. T. 3 | P. T. 4 | P. T. 5 | P. T. 6 | Mean
    |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| API-运行 | 提示模板 1 | 提示模板 2 | 提示模板 3 | 提示模板 4 | 提示模板 5 | 提示模板 6 | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 73.50 | 68.83 | 73.00 | 71.17 | 67.17 | 68.83 | 70.42 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 73.50 | 68.83 | 73.00 | 71.17 | 67.17 | 68.83 | 70.42 |'
- en: '| 1 | 73.83 | 69.00 | 72.83 | 71.50 | 67.67 | 68.33 | 70.53 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 73.83 | 69.00 | 72.83 | 71.50 | 67.67 | 68.33 | 70.53 |'
- en: '| 2 | 73.67 | 68.67 | 73.17 | 71.33 | 67.50 | 68.50 | 70.47 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 73.67 | 68.67 | 73.17 | 71.33 | 67.50 | 68.50 | 70.47 |'
- en: '| 3 | 73.83 | 68.17 | 73.17 | 71.00 | 67.67 | 68.17 | 70.33 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 73.83 | 68.17 | 73.17 | 71.00 | 67.67 | 68.17 | 70.33 |'
- en: '| 4 | 73.67 | 68.83 | 73.33 | 71.17 | 67.00 | 68.33 | 70.39 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 73.67 | 68.83 | 73.33 | 71.17 | 67.00 | 68.33 | 70.39 |'
- en: '| 5 | 73.83 | 68.50 | 73.00 | 71.00 | 67.00 | 68.17 | 70.25 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 73.83 | 68.50 | 73.00 | 71.00 | 67.00 | 68.17 | 70.25 |'
- en: '| 6 | 73.67 | 69.00 | 73.00 | 71.17 | 67.33 | 68.50 | 70.44 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 73.67 | 69.00 | 73.00 | 71.17 | 67.33 | 68.50 | 70.44 |'
- en: '| 7 | 73.67 | 68.67 | 72.83 | 71.33 | 67.50 | 68.67 | 70.44 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 73.67 | 68.67 | 72.83 | 71.33 | 67.50 | 68.67 | 70.44 |'
- en: '| 8 | 73.83 | 69.17 | 72.83 | 71.17 | 67.33 | 68.00 | 70.39 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 73.83 | 69.17 | 72.83 | 71.17 | 67.33 | 68.00 | 70.39 |'
- en: '| 9 | 73.50 | 68.50 | 72.83 | 71.00 | 67.50 | 68.67 | 70.33 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 73.50 | 68.50 | 72.83 | 71.00 | 67.50 | 68.67 | 70.33 |'
- en: '| 10 | 73.67 | 69.50 | 73.00 | 71.33 | 67.50 | 68.50 | 70.58 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 73.67 | 69.50 | 73.00 | 71.33 | 67.50 | 68.50 | 70.58 |'
- en: '| mean | 73.70 | 68.80 | 73.00 | 71.20 | 67.38 | 68.42 | - |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 73.70 | 68.80 | 73.00 | 71.20 | 67.38 | 68.42 | - |'
- en: '| std | 0.12 | 0.35 | 0.16 | 0.16 | 0.23 | 0.24 | - |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 0.12 | 0.35 | 0.16 | 0.16 | 0.23 | 0.24 | - |'
- en: 'Table 15: Results per prompt template (P.T.) for 10 different runs from Cohere-52B
    for 0-shot evaluation.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：Cohere-52B 在 10 次不同运行中的每个提示模板 (P.T.) 的结果，进行零样本评估。
- en: Each evaluation has exactly the same text, so the variance in performance is
    due to API stochasticity.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 每次评估的文本完全相同，因此性能的方差是由于 API 随机性。
- en: '| API-run | P. T. 1 | P. T. 2 | P. T. 3 | P. T. 4 | P. T. 5 | P. T. 6 | Mean
    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| API-运行 | 提示模板 1 | 提示模板 2 | 提示模板 3 | 提示模板 4 | 提示模板 5 | 提示模板 6 | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 56.00 | 62.67 | 54.33 | 54.00 | 62.17 | 62.17 | 58.56 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 56.00 | 62.67 | 54.33 | 54.00 | 62.17 | 62.17 | 58.56 |'
- en: '| 1 | 56.00 | 62.83 | 54.33 | 54.00 | 62.33 | 62.33 | 58.64 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 56.00 | 62.83 | 54.33 | 54.00 | 62.33 | 62.33 | 58.64 |'
- en: '| 2 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.33 | 58.61 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.33 | 58.61 |'
- en: '| 3 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.33 | 58.61 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.33 | 58.61 |'
- en: '| 4 | 55.83 | 62.67 | 54.33 | 54.00 | 62.17 | 62.33 | 58.56 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 55.83 | 62.67 | 54.33 | 54.00 | 62.17 | 62.33 | 58.56 |'
- en: '| 5 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.17 | 58.58 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.17 | 58.58 |'
- en: '| 6 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.17 | 58.58 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 56.00 | 62.83 | 54.33 | 54.00 | 62.17 | 62.17 | 58.58 |'
- en: '| 7 | 56.00 | 62.67 | 54.33 | 54.00 | 62.33 | 62.17 | 58.58 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 56.00 | 62.67 | 54.33 | 54.00 | 62.33 | 62.17 | 58.58 |'
- en: '| 8 | 56.00 | 62.83 | 54.33 | 54.00 | 62.00 | 62.33 | 58.58 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 56.00 | 62.83 | 54.33 | 54.00 | 62.00 | 62.33 | 58.58 |'
- en: '| 9 | 56.00 | 62.83 | 54.00 | 53.83 | 62.17 | 62.17 | 58.50 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 56.00 | 62.83 | 54.00 | 53.83 | 62.17 | 62.17 | 58.50 |'
- en: '| mean | 55.98 | 62.78 | 54.30 | 53.98 | 62.18 | 62.25 | - |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 55.98 | 62.78 | 54.30 | 53.98 | 62.18 | 62.25 | - |'
- en: '| std | 0.05 | 0.08 | 0.10 | 0.05 | 0.09 | 0.08 | - |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| std | 0.05 | 0.08 | 0.10 | 0.05 | 0.09 | 0.08 | - |'
- en: K.6 Experiment with random in-context labels
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.6 随机上下文标签实验
- en: 'This paper presents the thesis that instruction-tuning at the example level
    (“Example IT”) is important for pragmatic understanding in LLMs. However, the
    0-shot result that one of the models in the Example IT group achieves is similar
    to that of base models; Cohere-command-52b obtains a zero-shot performance of
    60.2%. From the sharp rise in performance observed for the $k=0$ result (from
    60.2% to 72.8%) we hypothesise that the k-shot in-context examples in this task
    do not necessarily teach the model pragmatics in-context, but prime the model
    for the task format (namely, outputting either “yes” or “no” as detailed in Section
    3 in the main text). If this hypothesis is true, we would observe similar performance
    regardless of whether the labels given in the prompt for the few-shot examples
    are true. We test this empirically for two base models (GPT-3, Cohere-52b) and
    two Example IT models (text-davinci-001, Cohere-command-52b) for 1-shot and 5-shot
    evaluation. The results can be found in Table [16](#A11.T16 "Table 16 ‣ K.6 Experiment
    with random in-context labels ‣ Appendix K Additional results ‣ The Goldilocks
    of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs"). We find that for the Example IT models in-context prompts with random
    labels obtain the same results (i.e. within confidence intervals) as the experiments
    with ground-truth labels in the in-context examples. For base models however we
    do observe a drop in performance; for GPT-3-175b at 5-shot, and Cohere-52b both
    at 1- and 5-shot. Taken together, we can conclude that for base models the content
    of the in-context prompt seems important, whereas for models in the example IT
    group the in-context examples mainly serve as a primer for the task structure.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '本文提出了一个论点，即在示例级别进行指令调优（“示例 IT”）对LLMs中的实用理解非常重要。然而，在示例 IT 组中，0-shot 的结果与基础模型相似；Cohere-command-52b
    的零样本性能为60.2%。从 $k=0$ 结果（从60.2%到72.8%）观察到的性能急剧上升，我们假设此任务中的 k-shot 上下文示例不一定教会模型上下文中的实用性，而是为任务格式做准备（即输出“是”或“否”，如主文第3节所述）。如果这一假设成立，我们将观察到无论提示中给出的少量示例标签是否真实，性能都是相似的。我们对两个基础模型（GPT-3、Cohere-52b）和两个示例
    IT 模型（text-davinci-001、Cohere-command-52b）进行了1-shot和5-shot评估的实证测试。结果可以在表 [16](#A11.T16
    "Table 16 ‣ K.6 Experiment with random in-context labels ‣ Appendix K Additional
    results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs") 中找到。我们发现，对于示例 IT 模型，带有随机标签的上下文提示获得的结果与具有真实标签的上下文示例实验结果相同（即在置信区间内）。然而，对于基础模型，我们确实观察到了性能下降；对于
    GPT-3-175b 的 5-shot 和 Cohere-52b 的 1-shot 和 5-shot。综合来看，我们可以得出结论，对于基础模型，上下文提示的内容似乎很重要，而对于示例
    IT 组的模型，上下文示例主要作为任务结构的引导。'
- en: 'Table 16: The results of the 1- and 5-shot experiment with random labels for
    the few-shot examples as opposed to the the true labels. We find that performance
    does not degrade for the models in the Example IT group, which implies that for
    these models not the content of the examples is important for performance, but
    the structure.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：1-shot 和 5-shot 实验中使用随机标签与真实标签对比的结果。我们发现，对于示例 IT 组的模型，性能没有下降，这意味着对于这些模型，示例的内容对性能并不重要，而是结构。
- en: '| Model | 1-shot | 1-shot rand labels | 5-shot | 5-shot rand labels |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 1-shot | 1-shot 随机标签 | 5-shot | 5-shot 随机标签 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-3-175b | 65.7% $\pm$ 1.9 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 65.7% $\pm$ 1.9 |'
- en: '| Cohere-52b | 63.0% $\pm$ 1.9 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 63.0% $\pm$ 1.9 |'
- en: '| text-davinci-001 | 72.7% $\pm$ 1.2 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001 | 72.7% $\pm$ 1.2 |'
- en: '| Cohere-command-52b | 72.8% $\pm$ 2.7 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 72.8% $\pm$ 2.7 |'
- en: K.7 Chain-of-thought on base models
  id: totrans-573
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.7 基模型上的思维链
- en: 'In the main paper we do a CoT experiment on the models in the Example IT group.
    Base models also benefit from in-context examples, so it makes sense to also try
    CoT prompting on these models. After attempting this for two of the model classes
    in the group, we decided not to apply this prompting technique to the other models,
    because it decreases performance, sometimes significantly. See the results of
    the CoT experiment on the two base model classes in Table [17](#A11.T17 "Table
    17 ‣ K.7 Chain-of-thought on base models ‣ Appendix K Additional results ‣ The
    Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs")'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们对Example IT组中的模型进行了CoT实验。基模型也从上下文示例中受益，因此尝试在这些模型上应用CoT提示是合理的。在对该组中的两个模型类进行尝试后，我们决定不对其他模型应用这种提示技术，因为它会降低性能，有时甚至显著降低。请参见表
    [17](#A11.T17 "表 17 ‣ K.7 基模型上的思维链 ‣ 附录 K 额外结果 ‣ 实用理解的金发姑娘：微调策略对隐含义解决的影响") 中两个基模型类的CoT实验结果。
- en: 'Table 17: Results of the chain-of-thought (CoT) experiment for models in the
    base group. The numbers between brackets show the difference in performance with
    the number on the same row one column to the left. These models do not benefit
    from CoT-prompting. The reason Cohere-6b achieves such a low score for CoT-prompting
    is because it is not able to adhere to the correct output format (yes/no).'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17：基模型的思维链（CoT）实验结果。方括号中的数字显示与同一行左侧一列的数字相比的性能差异。这些模型无法从CoT提示中获益。Cohere-6b
    在CoT提示中得分如此低的原因是它无法遵循正确的输出格式（是/否）。
- en: '| Model | 0-shot | 5-shot | 5-shot CoT |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 0-shot | 5-shot | 5-shot CoT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-3-350m | 51.5% $\pm$ 3.5 (-0.7%) |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 51.5% $\pm$ 3.5 (-0.7%) |'
- en: '| GPT-3-1.3b | 57.7% $\pm$ 5.8 (-8.2%) |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 57.7% $\pm$ 5.8 (-8.2%) |'
- en: '| GPT-3-6.7b | 54.8% $\pm$ 2.3 (+4.0%) |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 54.8% $\pm$ 2.3 (+4.0%) |'
- en: '| GPT-3-175b | 57.2% $\pm$ 4.2 (-8.4%) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 57.2% $\pm$ 4.2 (-8.4%) |'
- en: '| Cohere-6b | 57.3% $\pm$ 14.7 (-31.7%) |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 57.3% $\pm$ 14.7 (-31.7%) |'
- en: '| Cohere-52b | 58.5% $\pm$ 3.2 (-0.4%) |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 58.5% $\pm$ 3.2 (-0.4%) |'
- en: K.8 Testing for spurious correlations
  id: totrans-584
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.8 检测虚假相关性
- en: 'In this section, we do a small scale experiment to test whether the benchmark
    has spurious correlations. Specifically, we run the benchmark with only the utterance
    or only the response as input. Strictly, getting the implicature right from the
    response only does not always indicate spurious correlations, as some examples
    only need the response (e.g. rhetorical questions like ‘do pigs fly?’). Utterance-only
    results do always indicate spurious correlations. We run this experiment for GPT-3.5-turbo
    and GPT-4 0-shot and 5-shot (see Table [18](#A11.T18 "Table 18 ‣ K.8 Testing for
    spurious correlations ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")
    and Table [19](#A11.T19 "Table 19 ‣ K.8 Testing for spurious correlations ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs")).'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了一项小规模实验以测试基准是否存在虚假相关性。具体而言，我们仅使用话语或仅使用响应作为输入来运行基准测试。严格来说，仅从响应中得到隐含义并不总是表示虚假相关性，因为一些示例只需要响应（例如“猪会飞吗？”这样的修辞性问题）。仅话语结果总是表明虚假相关性。我们对GPT-3.5-turbo和GPT-4
    0-shot 和 5-shot 进行了此实验（参见表 [18](#A11.T18 "表 18 ‣ K.8 检测虚假相关性 ‣ 附录 K 额外结果 ‣ 实用理解的金发姑娘：微调策略对隐含义解决的影响")
    和表 [19](#A11.T19 "表 19 ‣ K.8 检测虚假相关性 ‣ 附录 K 额外结果 ‣ 实用理解的金发姑娘：微调策略对隐含义解决的影响")）。
- en: 'Table 18: Results of running the benchmark with only the utterance as input,
    to test for spurious correlations with the label.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：仅使用话语作为输入运行基准测试的结果，以测试与标签的虚假相关性。
- en: '| Utterance-only | 0-shot | 5-shot |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 仅话语 | 0-shot | 5-shot |'
- en: '| --- | --- | --- |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-3.5-Turbo | 54.3% $\pm$ 12.4 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 54.3% $\pm$ 12.4 |'
- en: '| GPT-4 | 48.9% $\pm$ 0.5 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 48.9% $\pm$ 0.5 |'
- en: 'Table 19: Results of running the benchmark with only the response as input,
    to test what part of the examples can be resolved without the utterance.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：仅使用响应作为输入运行基准测试的结果，以测试哪些示例可以在没有话语的情况下解决。
- en: '| Response-only | 0-shot | 5-shot |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| 仅响应 | 0-shot | 5-shot |'
- en: '| --- | --- | --- |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-3.5-Turbo | 59.2% $\pm$ 6.6 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 59.2% $\pm$ 6.6 |'
- en: '| GPT-4 | 62.6% $\pm$ 1.1 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 62.6% $\pm$ 1.1 |'
- en: 'We find that models mostly perform random for utterance-only, so spurious correlations
    do not seem to be an issue. For response-only, GPT-4 5-shot gets 65% accuracy.
    Some examples it gets right are: “do fish swim?” and “let’s hope so”.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现模型在仅发言的情况下大多表现为随机，因此似乎不存在虚假相关的问题。在仅回应的情况下，GPT-4 5-shot 得到 65% 的准确率。它正确的例子包括：“鱼会游泳吗？”和“让我们希望如此”。
- en: K.9 Detailed results type label analysis
  id: totrans-597
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.9 详细结果类型标签分析
- en: 'Table 20: An example from the dataset for each type of implicature found in
    the test set. The rightmost column shows the amount of that type we manually found
    in the test set.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 表 20：测试集中每种隐含类型的一个示例。最右列显示了我们在测试集中手动找到的该类型的数量。
- en: '| Type | Example Utterance | Example Response | Impl. | # |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 示例发言 | 示例回应 | 实现 | # |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Generalised | You know all these people? | Some. | No. | 47 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| 泛化 | 你认识所有这些人吗？ | 一些。 | 不认识。 | 47 |'
- en: '| Particularised | Want to stay for a nightcap? | I’ve gotta get up early.
    | No. | 94 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 具体化 | 想待一晚再走吗？ | 我得早起。 | 不想。 | 94 |'
- en: '| World knowledge | Did you leave fingerprints? | I wore gloves. | No. | 23
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 世界知识 | 你留下了指纹吗？ | 我戴了手套。 | 没有。 | 23 |'
- en: '| Idiom | Would he fire me? | He’s all bark and no bite. | No. | 42 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 成语 | 他会解雇我吗？ | 他只是空口说白话。 | 不会。 | 42 |'
- en: '| Rhetorical question | Can you drive that far? | Can fish swim? | Yes. | 11
    |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| 修辞问题 | 你能开那么远的车吗？ | 鱼会游泳吗？ | 能。 | 11 |'
- en: '| Other | - | - | - | 383 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | - | - | - | 383 |'
- en: 'In the main paper we do an analysis of two types of examples that occur frequently
    in the dataset, namely generalised and particularised implicatures. Here, we detail
    the full taxonomy of types of examples occurring in the dataset and report detailed
    results for each type per model (see [21](#A11.T21 "Table 21 ‣ K.9 Detailed results
    type label analysis ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic
    Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")
    until Table [34](#A11.T34 "Table 34 ‣ K.9 Detailed results type label analysis
    ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") below). In Table [20](#A11.T20
    "Table 20 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") the full taxonomy of the examples is shown, representing
    types of examples that occur frequently in the dataset. We manually labeled 217
    examples of the 600 examples in the test set according to this taxonomy. The remaining
    383 examples do not fall as clearly within a category and are grouped together
    as type other. Generalised implicatures require little or no context to be understood.
    They are the simplest type of example in the test set, and generally imply the
    same thing (“some” almost always implies “not all”). Particularised implicatures,
    by contrast, do require context to be resolved. For example, from Table [20](#A11.T20
    "Table 20 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs"), we need the context that it is undesirable to
    stay up late drinking when one has to get up early (see in Appendix D more on
    generalised vs. particularised). The type world knowledge requires knowledge of
    the physical world to be resolved. From the example in Table [20](#A11.T20 "Table
    20 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs"); we need to know that you cannot leave fingerprints
    when wearing gloves to resolve this implicature. Idiom types contain an idiom
    or a metaphor that one needs to know or understand to resolve the implicature,
    and finally Rhetorical question types contain a question like “Is the Pope Catholic?”,
    often requiring factual knowledge to be resolved.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '在主要论文中，我们对数据集中频繁出现的两种类型的例子进行了分析，即泛化含意和特定含意。在这里，我们详细介绍了数据集中出现的例子的完整分类法，并报告了每种类型在每个模型中的详细结果（见
    [21](#A11.T21 "Table 21 ‣ K.9 Detailed results type label analysis ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 直到表 [34](#A11.T34 "Table
    34 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") 下）。在表 [20](#A11.T20 "Table 20 ‣ K.9 Detailed
    results type label analysis ‣ Appendix K Additional results ‣ The Goldilocks of
    Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution
    by LLMs") 中展示了例子的完整分类法，代表了数据集中频繁出现的例子类型。我们根据该分类法手动标注了600个测试集中的217个例子。剩下的383个例子不明显归入任何类别，因此被归为其他类型。泛化含意几乎不需要上下文即可理解。它们是测试集中最简单的例子类型，通常暗示相同的意思（“一些”几乎总是暗示“并非全部”）。相反，特定含意需要上下文来解决。例如，从表
    [20](#A11.T20 "Table 20 ‣ K.9 Detailed results type label analysis ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 中，我们需要上下文来理解当需要早起时熬夜喝酒是不受欢迎的（见附录D了解更多关于泛化含意与特定含意的信息）。类型世界知识需要对物理世界的知识才能解决。从表
    [20](#A11.T20 "Table 20 ‣ K.9 Detailed results type label analysis ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") 的例子中，我们需要知道戴手套时不能留下指纹才能解决这一含意。习语类型包含一个成语或隐喻，需要了解或理解它才能解决含意，最后，修辞问题类型包含像“教皇是天主教徒吗？”这样的问句，通常需要事实知识才能解决。'
- en: 'The following tables contain the detailed results broken down per example type:
    Table [21](#A11.T21 "Table 21 ‣ K.9 Detailed results type label analysis ‣ Appendix
    K Additional results ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") - Table [34](#A11.T34 "Table
    34 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs"). The most interesting pattern in this data is
    that for almost all models, even the best model (GPT-4 30-shot in Table [32](#A11.T32
    "Table 32 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")), there is a significant gap between human-level
    performance on the particularised examples. This gap is larger than the gap for
    the other labels usually. Few-shot prompting can often mitigate this (e.g. for
    GPT-3-175b, Cohere-52b, and text-davinci-002), but not always (e.g. for GPT-4
    the gap remains large for $k=30$). However, for GPT-4, chain-of-thought can mitigate
    the gap as seen in Table [34](#A11.T34 "Table 34 ‣ K.9 Detailed results type label
    analysis ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs"). Where GPT-4
    30-shot obtains 71.97% accuracy on the particularised examples (and humans 83.18%),
    GPT-4 with 5-shot CoT achieves 81.63%, which is close to human-level. We find
    that the particularised examples mostly benefit from CoT prompting. Namely, for
    the generalised type of examples, GPT-4 30-shot already achieves 86.23% accuracy
    and CoT improves this to 88.66%, which is a much smaller improvement than for
    the particularised examples.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '以下表格包含了按示例类型详细划分的结果：表格 [21](#A11.T21 "Table 21 ‣ K.9 Detailed results type
    label analysis ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs") - 表格 [34](#A11.T34
    "Table 34 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")。在这些数据中，最有趣的模式是，对于几乎所有模型，甚至是最佳模型（表格 [32](#A11.T32
    "Table 32 ‣ K.9 Detailed results type label analysis ‣ Appendix K Additional results
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs")中的 GPT-4 30-shot），在特定示例上的人类水平表现与模型之间存在显著差距。这个差距通常比其他标签的差距要大。少样本提示往往可以缓解这一问题（例如，对于
    GPT-3-175b、Cohere-52b 和 text-davinci-002），但并不总是有效（例如，对于 GPT-4，当 $k=30$ 时，差距仍然很大）。然而，对于
    GPT-4，思路链可以缓解这一差距，如表格 [34](#A11.T34 "Table 34 ‣ K.9 Detailed results type label
    analysis ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs")所示。在特定示例中，GPT-4
    30-shot 获得 71.97% 的准确率（人类为 83.18%），而使用 5-shot CoT 的 GPT-4 达到 81.63%，接近人类水平。我们发现，特定示例大多受益于
    CoT 提示。也就是说，对于广义类型的示例，GPT-4 30-shot 已经达到了 86.23% 的准确率，而 CoT 将其提高到了 88.66%，这一提升远小于特定示例的提升。'
- en: 'Table 21: Accuracy per label for 0-shot evaluation.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 21: 0-shot 评估中的准确率。'
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 50.92 | 50.00 +/- 2.17 | 51.52 +/- 9.96 | 57.58 +/- 10.05 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 50.92 | 50.00 +/- 2.17 | 51.52 +/- 9.96 | 57.58 +/- 10.05 |'
- en: '| OPT-350m | 57.14 | 57.97 +/- 10.25 | 64.77 +/- 3.65 | 65.15 +/- 3.39 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 57.14 | 57.97 +/- 10.25 | 64.77 +/- 3.65 | 65.15 +/- 3.39 |'
- en: '| OPT-1.3b | 60.36 | 60.14 +/- 5.84 | 68.94 +/- 5.52 | 59.09 +/- 4.55 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 60.36 | 60.14 +/- 5.84 | 68.94 +/- 5.52 | 59.09 +/- 4.55 |'
- en: '| OPT-2.7b | 59.56 | 60.87 +/- 6.15 | 67.05 +/- 2.18 | 69.70 +/- 6.78 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.56 | 60.87 +/- 6.15 | 67.05 +/- 2.18 | 69.70 +/- 6.78 |'
- en: '| OPT-6.7b | 60.33 | 59.42 +/- 6.95 | 59.47 +/- 2.04 | 53.03 +/- 19.93 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 60.33 | 59.42 +/- 6.95 | 59.47 +/- 2.04 | 53.03 +/- 19.93 |'
- en: '| OPT-13b | 61.03 | 63.77 +/- 14.78 | 73.86 +/- 7.51 | 66.67 +/- 16.32 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 61.03 | 63.77 +/- 14.78 | 73.86 +/- 7.51 | 66.67 +/- 16.32 |'
- en: '| OPT-30b | 61.47 | 65.94 +/- 10.48 | 62.88 +/- 8.05 | 74.24 +/- 6.25 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 61.47 | 65.94 +/- 10.48 | 62.88 +/- 8.05 | 74.24 +/- 6.25 |'
- en: '| OPT-66b | 61.33 | 69.57 +/- 13.75 | 60.23 +/- 4.30 | 59.09 +/- 18.74 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.33 | 69.57 +/- 13.75 | 60.23 +/- 4.30 | 59.09 +/- 18.74 |'
- en: '| OPT-175b | 55.33 | 55.07 +/- 5.42 | 54.55 +/- 9.19 | 63.64 +/- 21.64 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 55.33 | 55.07 +/- 5.42 | 54.55 +/- 9.19 | 63.64 +/- 21.64 |'
- en: '| BLOOM-560m | 51.58 | 54.35 +/- 5.47 | 54.92 +/- 16.72 | 50.00 +/- 13.64 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 51.58 | 54.35 +/- 5.47 | 54.92 +/- 16.72 | 50.00 +/- 13.64 |'
- en: '| BLOOM-1b1 | 51.17 | 50.00 +/- 2.17 | 50.38 +/- 11.77 | 53.03 +/- 12.22 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 51.17 | 50.00 +/- 2.17 | 50.38 +/- 11.77 | 53.03 +/- 12.22 |'
- en: '| BLOOM-1b7 | 53.61 | 52.17 +/- 6.15 | 53.79 +/- 8.77 | 68.18 +/- 6.94 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 53.61 | 52.17 +/- 6.15 | 53.79 +/- 8.77 | 68.18 +/- 6.94 |'
- en: '| BLOOM-3b | 56.89 | 54.35 +/- 6.02 | 59.85 +/- 4.48 | 63.64 +/- 5.25 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 56.89 | 54.35 +/- 6.02 | 59.85 +/- 4.48 | 63.64 +/- 5.25 |'
- en: '| BLOOM-7b1 | 58.67 | 63.77 +/- 14.57 | 68.94 +/- 5.82 | 68.18 +/- 4.55 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 58.67 | 63.77 +/- 14.57 | 68.94 +/- 5.82 | 68.18 +/- 4.55 |'
- en: '| BLOOM-176b | 54.22 | 55.07 +/- 7.39 | 50.38 +/- 11.01 | 62.12 +/- 9.70 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 54.22 | 55.07 +/- 7.39 | 50.38 +/- 11.01 | 62.12 +/- 9.70 |'
- en: '| EleutherAI-125m | 51.89 | 56.52 +/- 9.72 | 52.65 +/- 8.84 | 63.64 +/- 5.25
    |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 51.89 | 56.52 +/- 9.72 | 52.65 +/- 8.84 | 63.64 +/- 5.25
    |'
- en: '| EleutherAI-1.3b | 53.14 | 51.45 +/- 3.90 | 53.03 +/- 11.19 | 62.12 +/- 3.39
    |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 53.14 | 51.45 +/- 3.90 | 53.03 +/- 11.19 | 62.12 +/- 3.39
    |'
- en: '| EleutherAI-2.7b | 59.17 | 60.14 +/- 13.38 | 65.91 +/- 3.94 | 68.18 +/- 4.55
    |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 59.17 | 60.14 +/- 13.38 | 65.91 +/- 3.94 | 68.18 +/- 4.55
    |'
- en: '| EleutherAI-6b | 56.36 | 57.25 +/- 7.28 | 56.06 +/- 8.87 | 50.00 +/- 17.99
    |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 56.36 | 57.25 +/- 7.28 | 56.06 +/- 8.87 | 50.00 +/- 17.99
    |'
- en: '| EleutherAI-20b | 57.53 | 51.45 +/- 3.90 | 67.80 +/- 5.93 | 72.73 +/- 5.25
    |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 57.53 | 51.45 +/- 3.90 | 67.80 +/- 5.93 | 72.73 +/- 5.25
    |'
- en: '| Cohere-409m | 51.61 | 52.17 +/- 4.35 | 53.41 +/- 11.94 | 54.55 +/- 12.86
    |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 51.61 | 52.17 +/- 4.35 | 53.41 +/- 11.94 | 54.55 +/- 12.86
    |'
- en: '| Cohere-6b | 57.28 | 55.80 +/- 5.28 | 60.23 +/- 5.98 | 72.73 +/- 9.09 |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 57.28 | 55.80 +/- 5.28 | 60.23 +/- 5.98 | 72.73 +/- 9.09 |'
- en: '| Cohere-13b | 57.19 | 59.42 +/- 4.81 | 54.55 +/- 10.82 | 48.48 +/- 10.05 |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 57.19 | 59.42 +/- 4.81 | 54.55 +/- 10.82 | 48.48 +/- 10.05 |'
- en: '| Cohere-52b | 58.50 | 60.14 +/- 13.61 | 65.15 +/- 3.86 | 74.24 +/- 11.03 |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 58.50 | 60.14 +/- 13.61 | 65.15 +/- 3.86 | 74.24 +/- 11.03 |'
- en: '| GPT-3-350m | 51.47 | 51.45 +/- 3.90 | 53.41 +/- 13.56 | 50.00 +/- 13.64 |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 51.47 | 51.45 +/- 3.90 | 53.41 +/- 13.56 | 50.00 +/- 13.64 |'
- en: '| GPT-3-1.3b | 57.72 | 61.59 +/- 11.06 | 64.39 +/- 4.08 | 65.15 +/- 3.39 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 57.72 | 61.59 +/- 11.06 | 64.39 +/- 4.08 | 65.15 +/- 3.39 |'
- en: '| GPT-3-6.7b | 54.83 | 54.35 +/- 6.99 | 53.79 +/- 7.61 | 62.12 +/- 3.39 |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 54.83 | 54.35 +/- 6.99 | 53.79 +/- 7.61 | 62.12 +/- 3.39 |'
- en: '| GPT-3-175b | 57.22 | 55.80 +/- 7.28 | 68.94 +/- 5.19 | 77.27 +/- 8.70 |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 57.22 | 55.80 +/- 7.28 | 68.94 +/- 5.19 | 77.27 +/- 8.70 |'
- en: '| T0-3b | 48.25 | 54.35 +/- 4.86 | 42.42 +/- 4.29 | 36.36 +/- 0.00 |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 48.25 | 54.35 +/- 4.86 | 42.42 +/- 4.29 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 55.61 | 60.14 +/- 6.84 | 54.92 +/- 14.93 | 36.36 +/- 0.00 |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 55.61 | 60.14 +/- 6.84 | 54.92 +/- 14.93 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 46.64 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.64 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.44 | 47.83 +/- 0.00 | 61.36 +/- 1.31 | 63.64 +/- 0.00
    |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.44 | 47.83 +/- 0.00 | 61.36 +/- 1.31 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 53.36 | 52.17 +/- 6.64 | 60.98 +/- 4.81 | 63.64 +/- 0.00
    |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.36 | 52.17 +/- 6.64 | 60.98 +/- 4.81 | 63.64 +/- 0.00
    |'
- en: '| Flan-T5-780m | 63.31 | 72.46 +/- 4.10 | 71.97 +/- 5.82 | 54.55 +/- 13.89
    |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 63.31 | 72.46 +/- 4.10 | 71.97 +/- 5.82 | 54.55 +/- 13.89
    |'
- en: '| Flan-T5-3b | 52.50 | 50.72 +/- 6.95 | 51.89 +/- 4.23 | 42.42 +/- 8.57 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 52.50 | 50.72 +/- 6.95 | 51.89 +/- 4.23 | 42.42 +/- 8.57 |'
- en: '| Flan-T5-11b | 60.78 | 65.94 +/- 5.84 | 72.35 +/- 7.59 | 65.15 +/- 6.25 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.78 | 65.94 +/- 5.84 | 72.35 +/- 7.59 | 65.15 +/- 6.25 |'
- en: '| Cohere-command-6b | 66.31 | 72.46 +/- 7.80 | 78.41 +/- 4.30 | 37.88 +/- 3.39
    |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 66.31 | 72.46 +/- 7.80 | 78.41 +/- 4.30 | 37.88 +/- 3.39
    |'
- en: '| Cohere-command-52b | 60.22 | 66.67 +/- 10.85 | 63.64 +/- 10.33 | 77.27 +/-
    6.94 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 60.22 | 66.67 +/- 10.85 | 63.64 +/- 10.33 | 77.27 +/-
    6.94 |'
- en: '| text-ada-001-unknown | 56.50 | 63.77 +/- 4.10 | 58.71 +/- 16.04 | 51.52 +/-
    10.05 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 56.50 | 63.77 +/- 4.10 | 58.71 +/- 16.04 | 51.52 +/-
    10.05 |'
- en: '| text-babbage-001-unknown | 64.47 | 67.39 +/- 6.02 | 76.52 +/- 1.69 | 60.61
    +/- 10.05 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 64.47 | 67.39 +/- 6.02 | 76.52 +/- 1.69 | 60.61
    +/- 10.05 |'
- en: '| text-curie-001-unknown | 68.94 | 76.81 +/- 3.24 | 76.89 +/- 2.76 | 54.55
    +/- 12.86 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 68.94 | 76.81 +/- 3.24 | 76.89 +/- 2.76 | 54.55
    +/- 12.86 |'
- en: '| text-davinci-001-unknown | 72.31 | 84.78 +/- 7.43 | 78.79 +/- 4.08 | 59.09
    +/- 13.64 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 72.31 | 84.78 +/- 7.43 | 78.79 +/- 4.08 | 59.09
    +/- 13.64 |'
- en: '| text-davinci-002-unknown | 70.58 | 82.61 +/- 9.05 | 75.38 +/- 3.05 | 57.58
    +/- 16.32 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 70.58 | 82.61 +/- 9.05 | 75.38 +/- 3.05 | 57.58
    +/- 16.32 |'
- en: '| text-davinci-003-unknown | 71.25 | 86.96 +/- 13.28 | 72.35 +/- 7.35 | 48.48
    +/- 8.57 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 71.25 | 86.96 +/- 13.28 | 72.35 +/- 7.35 | 48.48
    +/- 8.57 |'
- en: '| ChatGPT-unknown | 72.08 | 82.61 +/- 12.04 | 83.33 +/- 5.97 | 56.06 +/- 16.11
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 72.08 | 82.61 +/- 12.04 | 83.33 +/- 5.97 | 56.06 +/- 16.11
    |'
- en: '| GPT-4-unknown | 81.78 | 92.03 +/- 2.99 | 90.91 +/- 3.21 | 84.85 +/- 8.57
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 81.78 | 92.03 +/- 2.99 | 90.91 +/- 3.21 | 84.85 +/- 8.57
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 22: Accuracy per label for 0-shot evaluation.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 表22：0-shot评估的每标签准确性。
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 特定 | 泛化 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 50.92 | 49.43 +/- 5.52 | 55.07 +/- 21.10 | 50.56 +/- 1.33 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 50.92 | 49.43 +/- 5.52 | 55.07 +/- 21.10 | 50.56 +/- 1.33 |'
- en: '| OPT-350m | 57.14 | 47.92 +/- 4.37 | 69.20 +/- 8.36 | 56.68 +/- 4.99 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 57.14 | 47.92 +/- 4.37 | 69.20 +/- 8.36 | 56.68 +/- 4.99 |'
- en: '| OPT-1.3b | 60.36 | 51.52 +/- 6.81 | 74.64 +/- 2.05 | 59.65 +/- 3.51 |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 60.36 | 51.52 +/- 6.81 | 74.64 +/- 2.05 | 59.65 +/- 3.51 |'
- en: '| OPT-2.7b | 59.56 | 50.19 +/- 5.06 | 69.93 +/- 5.53 | 59.22 +/- 6.14 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.56 | 50.19 +/- 5.06 | 69.93 +/- 5.53 | 59.22 +/- 6.14 |'
- en: '| OPT-6.7b | 60.33 | 52.27 +/- 5.90 | 75.36 +/- 2.71 | 60.77 +/- 6.13 |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 60.33 | 52.27 +/- 5.90 | 75.36 +/- 2.71 | 60.77 +/- 6.13 |'
- en: '| OPT-13b | 61.03 | 55.49 +/- 8.79 | 75.00 +/- 5.72 | 58.79 +/- 5.51 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 61.03 | 55.49 +/- 8.79 | 75.00 +/- 5.72 | 58.79 +/- 5.51 |'
- en: '| OPT-30b | 61.47 | 54.55 +/- 4.15 | 71.38 +/- 5.94 | 61.11 +/- 2.12 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 61.47 | 54.55 +/- 4.15 | 71.38 +/- 5.94 | 61.11 +/- 2.12 |'
- en: '| OPT-66b | 61.33 | 55.11 +/- 7.33 | 69.93 +/- 12.26 | 61.46 +/- 3.68 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.33 | 55.11 +/- 7.33 | 69.93 +/- 12.26 | 61.46 +/- 3.68 |'
- en: '| OPT-175b | 55.33 | 54.17 +/- 7.70 | 58.33 +/- 18.51 | 55.12 +/- 4.21 |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 55.33 | 54.17 +/- 7.70 | 58.33 +/- 18.51 | 55.12 +/- 4.21 |'
- en: '| BLOOM-560m | 51.58 | 50.76 +/- 5.59 | 48.91 +/- 25.22 | 51.59 +/- 3.50 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 51.58 | 50.76 +/- 5.59 | 48.91 +/- 25.22 | 51.59 +/- 3.50 |'
- en: '| BLOOM-1b1 | 51.17 | 50.57 +/- 6.32 | 53.26 +/- 27.40 | 51.16 +/- 2.41 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 51.17 | 50.57 +/- 6.32 | 53.26 +/- 27.40 | 51.16 +/- 2.41 |'
- en: '| BLOOM-1b7 | 53.61 | 50.38 +/- 7.95 | 59.78 +/- 18.82 | 53.23 +/- 1.62 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 53.61 | 50.38 +/- 7.95 | 59.78 +/- 18.82 | 53.23 +/- 1.62 |'
- en: '| BLOOM-3b | 56.89 | 51.70 +/- 8.27 | 67.39 +/- 10.35 | 56.46 +/- 4.39 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 56.89 | 51.70 +/- 8.27 | 67.39 +/- 10.35 | 56.46 +/- 4.39 |'
- en: '| BLOOM-7b1 | 58.67 | 46.59 +/- 2.86 | 79.35 +/- 2.74 | 57.11 +/- 4.03 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 58.67 | 46.59 +/- 2.86 | 79.35 +/- 2.74 | 57.11 +/- 4.03 |'
- en: '| BLOOM-176b | 54.22 | 54.73 +/- 10.61 | 60.14 +/- 16.73 | 53.57 +/- 1.65 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 54.22 | 54.73 +/- 10.61 | 60.14 +/- 16.73 | 53.57 +/- 1.65 |'
- en: '| EleutherAI-125m | 51.89 | 50.38 +/- 5.71 | 57.25 +/- 20.15 | 50.90 +/- 0.99
    |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 51.89 | 50.38 +/- 5.71 | 57.25 +/- 20.15 | 50.90 +/- 0.99
    |'
- en: '| EleutherAI-1.3b | 53.14 | 50.57 +/- 7.03 | 55.43 +/- 23.20 | 53.32 +/- 2.05
    |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 53.14 | 50.57 +/- 7.03 | 55.43 +/- 23.20 | 53.32 +/- 2.05
    |'
- en: '| EleutherAI-2.7b | 59.17 | 50.95 +/- 7.43 | 74.64 +/- 4.97 | 58.10 +/- 2.92
    |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 59.17 | 50.95 +/- 7.43 | 74.64 +/- 4.97 | 58.10 +/- 2.92
    |'
- en: '| EleutherAI-6b | 56.36 | 53.22 +/- 6.86 | 69.20 +/- 7.36 | 55.73 +/- 2.43
    |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 56.36 | 53.22 +/- 6.86 | 69.20 +/- 7.36 | 55.73 +/- 2.43
    |'
- en: '| EleutherAI-20b | 57.53 | 49.43 +/- 6.68 | 72.83 +/- 5.99 | 56.20 +/- 3.01
    |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 57.53 | 49.43 +/- 6.68 | 72.83 +/- 5.99 | 56.20 +/- 3.01
    |'
- en: '| Cohere-409m | 51.61 | 51.33 +/- 4.84 | 52.54 +/- 22.71 | 51.25 +/- 2.93 |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 51.61 | 51.33 +/- 4.84 | 52.54 +/- 22.71 | 51.25 +/- 2.93 |'
- en: '| Cohere-6b | 57.28 | 51.52 +/- 6.68 | 64.49 +/- 16.06 | 57.06 +/- 3.13 |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 57.28 | 51.52 +/- 6.68 | 64.49 +/- 16.06 | 57.06 +/- 3.13 |'
- en: '| Cohere-13b | 57.19 | 52.27 +/- 5.83 | 68.12 +/- 10.99 | 57.45 +/- 3.35 |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 57.19 | 52.27 +/- 5.83 | 68.12 +/- 10.99 | 57.45 +/- 3.35 |'
- en: '| Cohere-52b | 58.50 | 51.52 +/- 7.21 | 73.91 +/- 5.75 | 56.85 +/- 3.81 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 58.50 | 51.52 +/- 7.21 | 73.91 +/- 5.75 | 56.85 +/- 3.81 |'
- en: '| GPT-3-350m | 51.47 | 50.76 +/- 6.96 | 52.90 +/- 24.07 | 51.29 +/- 1.63 |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 51.47 | 50.76 +/- 6.96 | 52.90 +/- 24.07 | 51.29 +/- 1.63 |'
- en: '| GPT-3-1.3b | 57.72 | 50.00 +/- 6.29 | 67.75 +/- 9.84 | 57.06 +/- 3.78 |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 57.72 | 50.00 +/- 6.29 | 67.75 +/- 9.84 | 57.06 +/- 3.78 |'
- en: '| GPT-3-6.7b | 54.83 | 52.65 +/- 8.16 | 63.41 +/- 15.14 | 54.26 +/- 2.12 |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 54.83 | 52.65 +/- 8.16 | 63.41 +/- 15.14 | 54.26 +/- 2.12 |'
- en: '| GPT-3-175b | 57.22 | 53.03 +/- 1.93 | 71.01 +/- 4.81 | 54.61 +/- 5.58 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 57.22 | 53.03 +/- 1.93 | 71.01 +/- 4.81 | 54.61 +/- 5.58 |'
- en: '| T0-3b | 48.25 | 55.68 +/- 0.66 | 27.17 +/- 2.74 | 49.83 +/- 1.90 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 48.25 | 55.68 +/- 0.66 | 27.17 +/- 2.74 | 49.83 +/- 1.90 |'
- en: '| T0-11b | 55.61 | 57.95 +/- 2.18 | 47.10 +/- 17.15 | 56.33 +/- 6.49 |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 55.61 | 57.95 +/- 2.18 | 47.10 +/- 17.15 | 56.33 +/- 6.49 |'
- en: '| BlenderBot-90m | 46.64 | 55.49 +/- 0.42 | 23.91 +/- 0.00 | 48.32 +/- 0.00
    |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.64 | 55.49 +/- 0.42 | 23.91 +/- 0.00 | 48.32 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.44 | 44.51 +/- 0.42 | 76.09 +/- 0.00 | 51.81 +/- 0.20
    |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.44 | 44.51 +/- 0.42 | 76.09 +/- 0.00 | 51.81 +/- 0.20
    |'
- en: '| BlenderBot-9b | 53.36 | 49.24 +/- 4.81 | 71.01 +/- 5.71 | 51.03 +/- 1.63
    |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.36 | 49.24 +/- 4.81 | 71.01 +/- 5.71 | 51.03 +/- 1.63
    |'
- en: '| Flan-T5-780m | 63.31 | 59.28 +/- 3.90 | 68.84 +/- 7.60 | 62.23 +/- 3.13 |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 63.31 | 59.28 +/- 3.90 | 68.84 +/- 7.60 | 62.23 +/- 3.13 |'
- en: '| Flan-T5-3b | 52.50 | 54.55 +/- 1.61 | 48.19 +/- 11.73 | 53.14 +/- 3.15 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 52.50 | 54.55 +/- 1.61 | 48.19 +/- 11.73 | 53.14 +/- 3.15 |'
- en: '| Flan-T5-11b | 60.78 | 51.52 +/- 3.39 | 73.19 +/- 7.28 | 59.60 +/- 2.18 |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.78 | 51.52 +/- 3.39 | 73.19 +/- 7.28 | 59.60 +/- 2.18 |'
- en: '| Cohere-command-6b | 66.31 | 58.90 +/- 3.62 | 73.19 +/- 2.71 | 66.15 +/- 2.41
    |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 66.31 | 58.90 +/- 3.62 | 73.19 +/- 2.71 | 66.15 +/- 2.41
    |'
- en: '| Cohere-command-52b | 60.22 | 55.49 +/- 4.66 | 60.51 +/- 16.67 | 59.99 +/-
    5.09 |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 60.22 | 55.49 +/- 4.66 | 60.51 +/- 16.67 | 59.99 +/-
    5.09 |'
- en: '| text-ada-001-unknown | 56.50 | 52.65 +/- 3.86 | 61.59 +/- 15.96 | 56.24 +/-
    5.74 |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 56.50 | 52.65 +/- 3.86 | 61.59 +/- 15.96 | 56.24 +/-
    5.74 |'
- en: '| text-babbage-001-unknown | 64.47 | 56.25 +/- 2.52 | 72.46 +/- 9.86 | 63.87
    +/- 1.55 |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 64.47 | 56.25 +/- 2.52 | 72.46 +/- 9.86 | 63.87
    +/- 1.55 |'
- en: '| text-curie-001-unknown | 68.94 | 66.48 +/- 2.34 | 68.84 +/- 5.98 | 68.48
    +/- 3.63 |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 68.94 | 66.48 +/- 2.34 | 68.84 +/- 5.98 | 68.48
    +/- 3.63 |'
- en: '| text-davinci-001-unknown | 72.31 | 59.66 +/- 5.07 | 79.35 +/- 9.78 | 73.17
    +/- 2.54 |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 72.31 | 59.66 +/- 5.07 | 79.35 +/- 9.78 | 73.17
    +/- 2.54 |'
- en: '| text-davinci-002-unknown | 70.58 | 64.20 +/- 3.75 | 80.07 +/- 5.67 | 69.94
    +/- 3.69 |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 70.58 | 64.20 +/- 3.75 | 80.07 +/- 5.67 | 69.94
    +/- 3.69 |'
- en: '| text-davinci-003-unknown | 71.25 | 63.64 +/- 1.86 | 82.25 +/- 4.77 | 71.23
    +/- 2.74 |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 71.25 | 63.64 +/- 1.86 | 82.25 +/- 4.77 | 71.23
    +/- 2.74 |'
- en: '| ChatGPT-unknown | 72.08 | 68.75 +/- 2.99 | 69.57 +/- 11.16 | 71.66 +/- 5.79
    |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 72.08 | 68.75 +/- 2.99 | 69.57 +/- 11.16 | 71.66 +/- 5.79
    |'
- en: '| GPT-4-unknown | 81.78 | 71.59 +/- 3.47 | 89.86 +/- 2.05 | 81.35 +/- 1.66
    |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 81.78 | 71.59 +/- 3.47 | 89.86 +/- 2.05 | 81.35 +/- 1.66
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 23: Accuracy per label for 1-shot evaluation.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '表 23: 单次评估的每个标签的准确性。'
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 52.72 | 43.48 +/- 5.02 | 54.92 +/- 7.24 | 59.09 +/- 4.55 |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 52.72 | 43.48 +/- 5.02 | 54.92 +/- 7.24 | 59.09 +/- 4.55 |'
- en: '| OPT-350m | 52.92 | 39.86 +/- 1.62 | 48.11 +/- 2.76 | 59.09 +/- 6.94 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 52.92 | 39.86 +/- 1.62 | 48.11 +/- 2.76 | 59.09 +/- 6.94 |'
- en: '| OPT-1.3b | 56.31 | 54.35 +/- 4.16 | 58.33 +/- 4.48 | 53.03 +/- 12.22 |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 56.31 | 54.35 +/- 4.16 | 58.33 +/- 4.48 | 53.03 +/- 12.22 |'
- en: '| OPT-2.7b | 56.83 | 64.49 +/- 15.35 | 64.39 +/- 2.51 | 66.67 +/- 4.29 |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 56.83 | 64.49 +/- 15.35 | 64.39 +/- 2.51 | 66.67 +/- 4.29 |'
- en: '| OPT-6.7b | 60.08 | 61.59 +/- 13.84 | 68.94 +/- 6.90 | 56.06 +/- 6.25 |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 60.08 | 61.59 +/- 13.84 | 68.94 +/- 6.90 | 56.06 +/- 6.25 |'
- en: '| OPT-13b | 60.56 | 68.84 +/- 7.70 | 69.70 +/- 6.11 | 54.55 +/- 15.75 |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 60.56 | 68.84 +/- 7.70 | 69.70 +/- 6.11 | 54.55 +/- 15.75 |'
- en: '| OPT-30b | 60.33 | 71.74 +/- 5.47 | 63.26 +/- 4.81 | 51.52 +/- 10.05 |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 60.33 | 71.74 +/- 5.47 | 63.26 +/- 4.81 | 51.52 +/- 10.05 |'
- en: '| OPT-66b | 63.19 | 70.29 +/- 11.06 | 62.50 +/- 2.86 | 48.48 +/- 13.55 |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 63.19 | 70.29 +/- 11.06 | 62.50 +/- 2.86 | 48.48 +/- 13.55 |'
- en: '| OPT-175b | 58.36 | 63.77 +/- 4.81 | 66.67 +/- 8.77 | 57.58 +/- 17.14 |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 58.36 | 63.77 +/- 4.81 | 66.67 +/- 8.77 | 57.58 +/- 17.14 |'
- en: '| BLOOM-560m | 54.83 | 50.00 +/- 5.47 | 64.02 +/- 4.62 | 63.64 +/- 0.00 |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 54.83 | 50.00 +/- 5.47 | 64.02 +/- 4.62 | 63.64 +/- 0.00 |'
- en: '| BLOOM-1b1 | 52.56 | 56.52 +/- 9.05 | 59.47 +/- 2.04 | 59.09 +/- 4.55 |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 52.56 | 56.52 +/- 9.05 | 59.47 +/- 2.04 | 59.09 +/- 4.55 |'
- en: '| BLOOM-1b7 | 52.81 | 54.35 +/- 6.52 | 60.98 +/- 3.57 | 63.64 +/- 5.25 |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 52.81 | 54.35 +/- 6.52 | 60.98 +/- 3.57 | 63.64 +/- 5.25 |'
- en: '| BLOOM-3b | 55.94 | 50.72 +/- 4.10 | 64.39 +/- 4.08 | 59.09 +/- 4.55 |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 55.94 | 50.72 +/- 4.10 | 64.39 +/- 4.08 | 59.09 +/- 4.55 |'
- en: '| BLOOM-7b1 | 57.00 | 50.00 +/- 3.32 | 64.77 +/- 2.86 | 62.12 +/- 6.25 |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 57.00 | 50.00 +/- 3.32 | 64.77 +/- 2.86 | 62.12 +/- 6.25 |'
- en: '| BLOOM-176b | 61.11 | 77.54 +/- 3.90 | 66.67 +/- 6.11 | 50.00 +/- 6.94 |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.11 | 77.54 +/- 3.90 | 66.67 +/- 6.11 | 50.00 +/- 6.94 |'
- en: '| EleutherAI-125m | 51.67 | 44.93 +/- 6.95 | 50.76 +/- 4.29 | 57.58 +/- 4.29
    |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 51.67 | 44.93 +/- 6.95 | 50.76 +/- 4.29 | 57.58 +/- 4.29
    |'
- en: '| EleutherAI-1.3b | 55.72 | 47.10 +/- 4.64 | 55.68 +/- 4.50 | 50.00 +/- 11.44
    |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 55.72 | 47.10 +/- 4.64 | 55.68 +/- 4.50 | 50.00 +/- 11.44
    |'
- en: '| EleutherAI-2.7b | 55.50 | 54.35 +/- 5.47 | 67.42 +/- 4.67 | 65.15 +/- 9.70
    |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 55.50 | 54.35 +/- 5.47 | 67.42 +/- 4.67 | 65.15 +/- 9.70
    |'
- en: '| EleutherAI-6b | 54.97 | 57.25 +/- 5.84 | 60.23 +/- 4.30 | 53.03 +/- 8.16
    |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 54.97 | 57.25 +/- 5.84 | 60.23 +/- 4.30 | 53.03 +/- 8.16
    |'
- en: '| EleutherAI-20b | 55.86 | 69.57 +/- 4.35 | 62.88 +/- 4.85 | 53.03 +/- 6.25
    |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 55.86 | 69.57 +/- 4.35 | 62.88 +/- 4.85 | 53.03 +/- 6.25
    |'
- en: '| Cohere-409m | 51.89 | 42.75 +/- 6.84 | 51.89 +/- 4.62 | 54.55 +/- 5.25 |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 51.89 | 42.75 +/- 6.84 | 51.89 +/- 4.62 | 54.55 +/- 5.25 |'
- en: '| Cohere-6b | 57.86 | 58.70 +/- 12.74 | 67.05 +/- 5.83 | 68.18 +/- 11.44 |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 57.86 | 58.70 +/- 12.74 | 67.05 +/- 5.83 | 68.18 +/- 11.44 |'
- en: '| Cohere-13b | 61.78 | 71.74 +/- 11.43 | 67.42 +/- 8.47 | 37.88 +/- 9.70 |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 61.78 | 71.74 +/- 11.43 | 67.42 +/- 8.47 | 37.88 +/- 9.70 |'
- en: '| Cohere-52b | 62.97 | 66.67 +/- 6.48 | 70.08 +/- 4.43 | 62.12 +/- 14.29 |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 62.97 | 66.67 +/- 6.48 | 70.08 +/- 4.43 | 62.12 +/- 14.29 |'
- en: '| GPT-3-350m | 55.97 | 50.72 +/- 4.10 | 61.74 +/- 5.15 | 69.70 +/- 12.49 |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 55.97 | 50.72 +/- 4.10 | 61.74 +/- 5.15 | 69.70 +/- 12.49 |'
- en: '| GPT-3-1.3b | 60.75 | 58.70 +/- 4.16 | 65.53 +/- 4.23 | 54.55 +/- 5.25 |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.75 | 58.70 +/- 4.16 | 65.53 +/- 4.23 | 54.55 +/- 5.25 |'
- en: '| GPT-3-6.7b | 61.17 | 60.87 +/- 11.77 | 69.32 +/- 3.65 | 56.06 +/- 8.16 |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 61.17 | 60.87 +/- 11.77 | 69.32 +/- 3.65 | 56.06 +/- 8.16 |'
- en: '| GPT-3-175b | 65.72 | 76.81 +/- 3.24 | 73.48 +/- 2.51 | 57.58 +/- 16.32 |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 65.72 | 76.81 +/- 3.24 | 73.48 +/- 2.51 | 57.58 +/- 16.32 |'
- en: '| T0-3b | 48.89 | 54.35 +/- 2.17 | 42.80 +/- 2.04 | 36.36 +/- 0.00 |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 48.89 | 54.35 +/- 2.17 | 42.80 +/- 2.04 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 47.78 | 52.17 +/- 0.00 | 40.53 +/- 2.43 | 36.36 +/- 0.00 |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 47.78 | 52.17 +/- 0.00 | 40.53 +/- 2.43 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 49.94 | 55.07 +/- 6.48 | 47.73 +/- 9.99 | 51.52 +/- 13.55
    |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 49.94 | 55.07 +/- 6.48 | 47.73 +/- 9.99 | 51.52 +/- 13.55
    |'
- en: '| BlenderBot-3b | 53.31 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.31 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 52.53 | 50.72 +/- 9.61 | 57.20 +/- 12.95 | 66.67 +/- 6.78
    |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 52.53 | 50.72 +/- 9.61 | 57.20 +/- 12.95 | 66.67 +/- 6.78
    |'
- en: '| Flan-T5-780m | 62.89 | 64.49 +/- 7.70 | 67.42 +/- 13.03 | 46.97 +/- 8.16
    |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 62.89 | 64.49 +/- 7.70 | 67.42 +/- 13.03 | 46.97 +/- 8.16
    |'
- en: '| Flan-T5-3b | 52.75 | 65.22 +/- 15.47 | 55.30 +/- 9.34 | 45.45 +/- 12.86 |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 52.75 | 65.22 +/- 15.47 | 55.30 +/- 9.34 | 45.45 +/- 12.86 |'
- en: '| Flan-T5-11b | 57.44 | 59.42 +/- 3.24 | 61.36 +/- 12.17 | 48.48 +/- 12.49
    |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 57.44 | 59.42 +/- 3.24 | 61.36 +/- 12.17 | 48.48 +/- 12.49
    |'
- en: '| Cohere-command-6b | 65.00 | 71.74 +/- 6.99 | 71.59 +/- 3.15 | 36.36 +/- 0.00
    |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 65.00 | 71.74 +/- 6.99 | 71.59 +/- 3.15 | 36.36 +/- 0.00
    |'
- en: '| Cohere-command-52b | 72.83 | 83.33 +/- 3.90 | 83.33 +/- 2.51 | 71.21 +/-
    6.25 |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 72.83 | 83.33 +/- 3.90 | 83.33 +/- 2.51 | 71.21 +/-
    6.25 |'
- en: '| text-ada-001-unknown | 57.36 | 60.87 +/- 7.10 | 67.80 +/- 3.81 | 66.67 +/-
    6.78 |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.36 | 60.87 +/- 7.10 | 67.80 +/- 3.81 | 66.67 +/-
    6.78 |'
- en: '| text-babbage-001-unknown | 63.89 | 68.84 +/- 3.90 | 76.89 +/- 2.43 | 50.00
    +/- 11.44 |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 63.89 | 68.84 +/- 3.90 | 76.89 +/- 2.43 | 50.00
    +/- 11.44 |'
- en: '| text-curie-001-unknown | 64.39 | 66.67 +/- 5.98 | 68.56 +/- 9.94 | 56.06
    +/- 6.25 |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 64.39 | 66.67 +/- 5.98 | 68.56 +/- 9.94 | 56.06
    +/- 6.25 |'
- en: '| text-davinci-001-unknown | 72.72 | 93.48 +/- 4.16 | 80.68 +/- 2.18 | 57.58
    +/- 12.49 |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 72.72 | 93.48 +/- 4.16 | 80.68 +/- 2.18 | 57.58
    +/- 12.49 |'
- en: '| text-davinci-002-unknown | 75.61 | 91.30 +/- 2.51 | 87.12 +/- 2.51 | 56.06
    +/- 8.16 |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 75.61 | 91.30 +/- 2.51 | 87.12 +/- 2.51 | 56.06
    +/- 8.16 |'
- en: '| text-davinci-003-unknown | 74.31 | 90.58 +/- 5.28 | 82.20 +/- 1.56 | 54.55
    +/- 7.42 |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 74.31 | 90.58 +/- 5.28 | 82.20 +/- 1.56 | 54.55
    +/- 7.42 |'
- en: '| ChatGPT-unknown | 75.11 | 86.23 +/- 2.99 | 85.61 +/- 3.12 | 56.06 +/- 14.29
    |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.11 | 86.23 +/- 2.99 | 85.61 +/- 3.12 | 56.06 +/- 14.29
    |'
- en: '| GPT-4-unknown | 82.31 | 97.10 +/- 3.24 | 88.64 +/- 3.94 | 89.39 +/- 3.39
    |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.31 | 97.10 +/- 3.24 | 88.64 +/- 3.94 | 89.39 +/- 3.39
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 24: Accuracy per label for 1-shot evaluation.'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 表 24：1-shot 评估的每标签准确度。
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 特定 | 泛化 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 52.72 | 48.30 +/- 1.83 | 60.87 +/- 13.04 | 52.89 +/- 1.04 |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 52.72 | 48.30 +/- 1.83 | 60.87 +/- 13.04 | 52.89 +/- 1.04 |'
- en: '| OPT-350m | 52.92 | 47.73 +/- 2.37 | 60.87 +/- 11.71 | 54.35 +/- 3.10 |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 52.92 | 47.73 +/- 2.37 | 60.87 +/- 11.71 | 54.35 +/- 3.10 |'
- en: '| OPT-1.3b | 56.31 | 53.41 +/- 2.71 | 52.17 +/- 9.88 | 57.41 +/- 1.38 |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 56.31 | 53.41 +/- 2.71 | 52.17 +/- 9.88 | 57.41 +/- 1.38 |'
- en: '| OPT-2.7b | 56.83 | 49.81 +/- 5.31 | 69.93 +/- 6.33 | 55.17 +/- 3.94 |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 56.83 | 49.81 +/- 5.31 | 69.93 +/- 6.33 | 55.17 +/- 3.94 |'
- en: '| OPT-6.7b | 60.08 | 52.65 +/- 7.44 | 73.55 +/- 3.18 | 59.09 +/- 5.85 |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 60.08 | 52.65 +/- 7.44 | 73.55 +/- 3.18 | 59.09 +/- 5.85 |'
- en: '| OPT-13b | 60.56 | 53.03 +/- 1.82 | 71.01 +/- 7.60 | 59.56 +/- 2.75 |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 60.56 | 53.03 +/- 1.82 | 71.01 +/- 7.60 | 59.56 +/- 2.75 |'
- en: '| OPT-30b | 60.33 | 55.87 +/- 3.31 | 70.65 +/- 8.01 | 59.26 +/- 4.61 |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 60.33 | 55.87 +/- 3.31 | 70.65 +/- 8.01 | 59.26 +/- 4.61 |'
- en: '| OPT-66b | 63.19 | 60.04 +/- 4.12 | 67.39 +/- 8.70 | 63.39 +/- 4.28 |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 63.19 | 60.04 +/- 4.12 | 67.39 +/- 8.70 | 63.39 +/- 4.28 |'
- en: '| OPT-175b | 58.36 | 56.63 +/- 3.96 | 59.42 +/- 7.06 | 57.28 +/- 7.30 |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 58.36 | 56.63 +/- 3.96 | 59.42 +/- 7.06 | 57.28 +/- 7.30 |'
- en: '| BLOOM-560m | 54.83 | 43.94 +/- 3.12 | 66.30 +/- 8.21 | 54.82 +/- 1.85 |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 54.83 | 43.94 +/- 3.12 | 66.30 +/- 8.21 | 54.82 +/- 1.85 |'
- en: '| BLOOM-1b1 | 52.56 | 47.35 +/- 3.12 | 63.04 +/- 12.36 | 51.16 +/- 1.54 |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 52.56 | 47.35 +/- 3.12 | 63.04 +/- 12.36 | 51.16 +/- 1.54 |'
- en: '| BLOOM-1b7 | 52.81 | 45.64 +/- 3.50 | 67.03 +/- 9.92 | 51.29 +/- 1.44 |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 52.81 | 45.64 +/- 3.50 | 67.03 +/- 9.92 | 51.29 +/- 1.44 |'
- en: '| BLOOM-3b | 55.94 | 45.27 +/- 1.21 | 76.09 +/- 1.26 | 55.12 +/- 1.93 |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 55.94 | 45.27 +/- 1.21 | 76.09 +/- 1.26 | 55.12 +/- 1.93 |'
- en: '| BLOOM-7b1 | 57.00 | 49.62 +/- 4.08 | 77.17 +/- 1.66 | 55.56 +/- 3.57 |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 57.00 | 49.62 +/- 4.08 | 77.17 +/- 1.66 | 55.56 +/- 3.57 |'
- en: '| BLOOM-176b | 61.11 | 58.14 +/- 3.31 | 66.67 +/- 5.98 | 59.73 +/- 3.66 |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.11 | 58.14 +/- 3.31 | 66.67 +/- 5.98 | 59.73 +/- 3.66 |'
- en: '| EleutherAI-125m | 51.67 | 50.19 +/- 2.49 | 53.99 +/- 8.82 | 52.11 +/- 0.89
    |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 51.67 | 50.19 +/- 2.49 | 53.99 +/- 8.82 | 52.11 +/- 0.89
    |'
- en: '| EleutherAI-1.3b | 55.72 | 50.57 +/- 4.67 | 57.97 +/- 13.44 | 57.36 +/- 2.66
    |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 55.72 | 50.57 +/- 4.67 | 57.97 +/- 13.44 | 57.36 +/- 2.66
    |'
- en: '| EleutherAI-2.7b | 55.50 | 48.67 +/- 4.84 | 65.22 +/- 4.86 | 54.22 +/- 2.79
    |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 55.50 | 48.67 +/- 4.84 | 65.22 +/- 4.86 | 54.22 +/- 2.79
    |'
- en: '| EleutherAI-6b | 54.97 | 49.81 +/- 1.21 | 66.30 +/- 4.82 | 54.01 +/- 3.36
    |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 54.97 | 49.81 +/- 1.21 | 66.30 +/- 4.82 | 54.01 +/- 3.36
    |'
- en: '| EleutherAI-20b | 55.86 | 53.03 +/- 3.57 | 64.49 +/- 6.36 | 53.83 +/- 2.41
    |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 55.86 | 53.03 +/- 3.57 | 64.49 +/- 6.36 | 53.83 +/- 2.41
    |'
- en: '| Cohere-409m | 51.89 | 52.84 +/- 3.98 | 48.55 +/- 3.69 | 52.54 +/- 1.96 |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 51.89 | 52.84 +/- 3.98 | 48.55 +/- 3.69 | 52.54 +/- 1.96 |'
- en: '| Cohere-6b | 57.86 | 44.13 +/- 1.66 | 77.54 +/- 2.05 | 57.15 +/- 5.08 |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 57.86 | 44.13 +/- 1.66 | 77.54 +/- 2.05 | 57.15 +/- 5.08 |'
- en: '| Cohere-13b | 61.78 | 53.98 +/- 2.05 | 74.28 +/- 3.64 | 61.41 +/- 1.84 |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 61.78 | 53.98 +/- 2.05 | 74.28 +/- 3.64 | 61.41 +/- 1.84 |'
- en: '| Cohere-52b | 62.97 | 60.42 +/- 8.18 | 69.20 +/- 5.24 | 61.76 +/- 4.21 |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 62.97 | 60.42 +/- 8.18 | 69.20 +/- 5.24 | 61.76 +/- 4.21 |'
- en: '| GPT-3-350m | 55.97 | 50.76 +/- 1.82 | 73.91 +/- 7.94 | 54.31 +/- 1.92 |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 55.97 | 50.76 +/- 1.82 | 73.91 +/- 7.94 | 54.31 +/- 1.92 |'
- en: '| GPT-3-1.3b | 60.75 | 53.79 +/- 2.98 | 68.48 +/- 3.26 | 61.07 +/- 1.82 |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.75 | 53.79 +/- 2.98 | 68.48 +/- 3.26 | 61.07 +/- 1.82 |'
- en: '| GPT-3-6.7b | 61.17 | 55.49 +/- 6.83 | 72.10 +/- 2.64 | 60.29 +/- 4.09 |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 61.17 | 55.49 +/- 6.83 | 72.10 +/- 2.64 | 60.29 +/- 4.09 |'
- en: '| GPT-3-175b | 65.72 | 62.31 +/- 4.17 | 64.86 +/- 7.26 | 65.33 +/- 2.00 |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 65.72 | 62.31 +/- 4.17 | 64.86 +/- 7.26 | 65.33 +/- 2.00 |'
- en: '| T0-3b | 48.89 | 56.25 +/- 1.57 | 34.06 +/- 4.29 | 49.83 +/- 0.55 |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 48.89 | 56.25 +/- 1.57 | 34.06 +/- 4.29 | 49.83 +/- 0.55 |'
- en: '| T0-11b | 47.78 | 56.44 +/- 0.54 | 27.54 +/- 1.02 | 49.22 +/- 0.53 |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 47.78 | 56.44 +/- 0.54 | 27.54 +/- 1.02 | 49.22 +/- 0.53 |'
- en: '| BlenderBot-90m | 49.94 | 52.46 +/- 4.27 | 44.57 +/- 15.66 | 50.00 +/- 1.65
    |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 49.94 | 52.46 +/- 4.27 | 44.57 +/- 15.66 | 50.00 +/- 1.65
    |'
- en: '| BlenderBot-3b | 53.31 | 44.51 +/- 0.42 | 76.09 +/- 0.00 | 51.59 +/- 0.24
    |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.31 | 44.51 +/- 0.42 | 76.09 +/- 0.00 | 51.59 +/- 0.24
    |'
- en: '| BlenderBot-9b | 52.53 | 54.92 +/- 3.45 | 55.80 +/- 12.90 | 50.90 +/- 2.60
    |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 52.53 | 54.92 +/- 3.45 | 55.80 +/- 12.90 | 50.90 +/- 2.60
    |'
- en: '| Flan-T5-780m | 62.89 | 56.44 +/- 3.32 | 68.84 +/- 12.90 | 63.44 +/- 6.28
    |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 62.89 | 56.44 +/- 3.32 | 68.84 +/- 12.90 | 63.44 +/- 6.28
    |'
- en: '| Flan-T5-3b | 52.75 | 55.11 +/- 1.57 | 44.20 +/- 5.98 | 52.41 +/- 3.23 |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 52.75 | 55.11 +/- 1.57 | 44.20 +/- 5.98 | 52.41 +/- 3.23 |'
- en: '| Flan-T5-11b | 57.44 | 53.98 +/- 1.94 | 62.68 +/- 15.85 | 57.28 +/- 4.79 |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 57.44 | 53.98 +/- 1.94 | 62.68 +/- 15.85 | 57.28 +/- 4.79 |'
- en: '| Cohere-command-6b | 65.00 | 60.61 +/- 3.69 | 68.12 +/- 9.53 | 65.25 +/- 1.37
    |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 65.00 | 60.61 +/- 3.69 | 68.12 +/- 9.53 | 65.25 +/- 1.37
    |'
- en: '| Cohere-command-52b | 72.83 | 67.42 +/- 2.83 | 80.07 +/- 2.92 | 71.36 +/-
    1.70 |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 72.83 | 67.42 +/- 2.83 | 80.07 +/- 2.92 | 71.36 +/-
    1.70 |'
- en: '| text-ada-001-unknown | 57.36 | 46.97 +/- 2.76 | 74.64 +/- 2.99 | 55.90 +/-
    3.11 |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.36 | 46.97 +/- 2.76 | 74.64 +/- 2.99 | 55.90 +/-
    3.11 |'
- en: '| text-babbage-001-unknown | 63.89 | 58.52 +/- 1.43 | 63.41 +/- 7.26 | 63.70
    +/- 1.10 |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 63.89 | 58.52 +/- 1.43 | 63.41 +/- 7.26 | 63.70
    +/- 1.10 |'
- en: '| text-curie-001-unknown | 64.39 | 60.98 +/- 2.14 | 69.93 +/- 3.42 | 64.04
    +/- 5.79 |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 64.39 | 60.98 +/- 2.14 | 69.93 +/- 3.42 | 64.04
    +/- 5.79 |'
- en: '| text-davinci-001-unknown | 72.72 | 62.31 +/- 1.66 | 76.81 +/- 2.71 | 72.83
    +/- 1.70 |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 72.72 | 62.31 +/- 1.66 | 76.81 +/- 2.71 | 72.83
    +/- 1.70 |'
- en: '| text-davinci-002-unknown | 75.61 | 68.18 +/- 2.86 | 77.54 +/- 2.05 | 75.32
    +/- 3.14 |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 75.61 | 68.18 +/- 2.86 | 77.54 +/- 2.05 | 75.32
    +/- 3.14 |'
- en: '| text-davinci-003-unknown | 74.31 | 64.20 +/- 1.43 | 80.43 +/- 5.02 | 74.50
    +/- 1.29 |'
  id: totrans-805
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 74.31 | 64.20 +/- 1.43 | 80.43 +/- 5.02 | 74.50
    +/- 1.29 |'
- en: '| ChatGPT-unknown | 75.11 | 70.08 +/- 4.38 | 78.99 +/- 7.50 | 74.46 +/- 1.19
    |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.11 | 70.08 +/- 4.38 | 78.99 +/- 7.50 | 74.46 +/- 1.19
    |'
- en: '| GPT-4-unknown | 82.31 | 74.43 +/- 2.60 | 86.96 +/- 3.32 | 81.70 +/- 1.94
    |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.31 | 74.43 +/- 2.60 | 86.96 +/- 3.32 | 81.70 +/- 1.94
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 25: Accuracy per label for 5-shot evaluation.'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '表 25: 5-shot 评估的每个标签的准确率。'
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-810
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均值 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-811
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 50.22 | 44.93 +/- 3.24 | 57.58 +/- 7.73 | 57.58 +/- 4.29 |'
  id: totrans-812
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 50.22 | 44.93 +/- 3.24 | 57.58 +/- 7.73 | 57.58 +/- 4.29 |'
- en: '| OPT-350m | 51.47 | 53.62 +/- 4.81 | 58.71 +/- 1.56 | 45.45 +/- 0.00 |'
  id: totrans-813
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 51.47 | 53.62 +/- 4.81 | 58.71 +/- 1.56 | 45.45 +/- 0.00 |'
- en: '| OPT-1.3b | 58.03 | 68.84 +/- 8.48 | 63.26 +/- 6.21 | 30.30 +/- 8.57 |'
  id: totrans-814
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 58.03 | 68.84 +/- 8.48 | 63.26 +/- 6.21 | 30.30 +/- 8.57 |'
- en: '| OPT-2.7b | 57.33 | 57.97 +/- 4.81 | 66.67 +/- 4.67 | 71.21 +/- 3.39 |'
  id: totrans-815
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 57.33 | 57.97 +/- 4.81 | 66.67 +/- 4.67 | 71.21 +/- 3.39 |'
- en: '| OPT-6.7b | 63.31 | 66.67 +/- 16.20 | 67.42 +/- 4.08 | 42.42 +/- 16.32 |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.31 | 66.67 +/- 16.20 | 67.42 +/- 4.08 | 42.42 +/- 16.32 |'
- en: '| OPT-13b | 67.39 | 80.43 +/- 4.86 | 68.94 +/- 4.29 | 39.39 +/- 6.78 |'
  id: totrans-817
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 67.39 | 80.43 +/- 4.86 | 68.94 +/- 4.29 | 39.39 +/- 6.78 |'
- en: '| OPT-30b | 65.64 | 84.78 +/- 8.60 | 66.29 +/- 8.13 | 37.88 +/- 6.25 |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 65.64 | 84.78 +/- 8.60 | 66.29 +/- 8.13 | 37.88 +/- 6.25 |'
- en: '| OPT-66b | 61.50 | 75.36 +/- 8.20 | 55.30 +/- 6.90 | 36.36 +/- 7.42 |'
  id: totrans-819
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.50 | 75.36 +/- 8.20 | 55.30 +/- 6.90 | 36.36 +/- 7.42 |'
- en: '| OPT-175b | 63.89 | 78.26 +/- 7.10 | 65.15 +/- 2.83 | 43.94 +/- 3.39 |'
  id: totrans-820
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 63.89 | 78.26 +/- 7.10 | 65.15 +/- 2.83 | 43.94 +/- 3.39 |'
- en: '| BLOOM-560m | 53.75 | 44.20 +/- 2.99 | 65.91 +/- 3.94 | 54.55 +/- 5.25 |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 53.75 | 44.20 +/- 2.99 | 65.91 +/- 3.94 | 54.55 +/- 5.25 |'
- en: '| BLOOM-1b1 | 57.39 | 49.28 +/- 6.95 | 65.15 +/- 4.85 | 66.67 +/- 6.78 |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.39 | 49.28 +/- 6.95 | 65.15 +/- 4.85 | 66.67 +/- 6.78 |'
- en: '| BLOOM-1b7 | 54.44 | 61.59 +/- 5.84 | 56.06 +/- 1.69 | 43.94 +/- 6.25 |'
  id: totrans-823
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 54.44 | 61.59 +/- 5.84 | 56.06 +/- 1.69 | 43.94 +/- 6.25 |'
- en: '| BLOOM-3b | 57.19 | 50.72 +/- 3.24 | 64.77 +/- 4.87 | 63.64 +/- 12.86 |'
  id: totrans-824
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 57.19 | 50.72 +/- 3.24 | 64.77 +/- 4.87 | 63.64 +/- 12.86 |'
- en: '| BLOOM-7b1 | 54.50 | 50.00 +/- 2.17 | 62.88 +/- 1.69 | 69.70 +/- 4.29 |'
  id: totrans-825
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 54.50 | 50.00 +/- 2.17 | 62.88 +/- 1.69 | 69.70 +/- 4.29 |'
- en: '| BLOOM-176b | 65.42 | 76.09 +/- 6.02 | 69.32 +/- 4.87 | 43.94 +/- 3.39 |'
  id: totrans-826
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 65.42 | 76.09 +/- 6.02 | 69.32 +/- 4.87 | 43.94 +/- 3.39 |'
- en: '| EleutherAI-125m | 49.56 | 50.00 +/- 3.32 | 50.38 +/- 4.43 | 34.85 +/- 3.39
    |'
  id: totrans-827
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 49.56 | 50.00 +/- 3.32 | 50.38 +/- 4.43 | 34.85 +/- 3.39
    |'
- en: '| EleutherAI-1.3b | 57.11 | 55.07 +/- 5.98 | 63.64 +/- 4.15 | 37.88 +/- 11.03
    |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.11 | 55.07 +/- 5.98 | 63.64 +/- 4.15 | 37.88 +/- 11.03
    |'
- en: '| EleutherAI-2.7b | 58.03 | 71.74 +/- 4.16 | 59.85 +/- 3.12 | 43.94 +/- 14.29
    |'
  id: totrans-829
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 58.03 | 71.74 +/- 4.16 | 59.85 +/- 3.12 | 43.94 +/- 14.29
    |'
- en: '| EleutherAI-6b | 58.39 | 67.39 +/- 6.99 | 56.82 +/- 6.01 | 42.42 +/- 18.68
    |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.39 | 67.39 +/- 6.99 | 56.82 +/- 6.01 | 42.42 +/- 18.68
    |'
- en: '| EleutherAI-20b | 61.14 | 65.22 +/- 8.70 | 64.77 +/- 11.11 | 30.30 +/- 8.57
    |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 61.14 | 65.22 +/- 8.70 | 64.77 +/- 11.11 | 30.30 +/- 8.57
    |'
- en: '| Cohere-409m | 53.39 | 47.83 +/- 5.61 | 59.47 +/- 5.32 | 31.82 +/- 6.94 |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 53.39 | 47.83 +/- 5.61 | 59.47 +/- 5.32 | 31.82 +/- 6.94 |'
- en: '| Cohere-6b | 60.89 | 65.94 +/- 8.48 | 66.67 +/- 10.05 | 45.45 +/- 9.09 |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.89 | 65.94 +/- 8.48 | 66.67 +/- 10.05 | 45.45 +/- 9.09 |'
- en: '| Cohere-13b | 62.47 | 81.88 +/- 8.10 | 62.88 +/- 10.71 | 34.85 +/- 11.03 |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 62.47 | 81.88 +/- 8.10 | 62.88 +/- 10.71 | 34.85 +/- 11.03 |'
- en: '| Cohere-52b | 65.14 | 73.91 +/- 5.61 | 67.80 +/- 3.05 | 51.52 +/- 6.78 |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 65.14 | 73.91 +/- 5.61 | 67.80 +/- 3.05 | 51.52 +/- 6.78 |'
- en: '| GPT-3-350m | 55.72 | 46.38 +/- 3.24 | 65.53 +/- 1.56 | 51.52 +/- 4.29 |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 55.72 | 46.38 +/- 3.24 | 65.53 +/- 1.56 | 51.52 +/- 4.29 |'
- en: '| GPT-3-1.3b | 62.64 | 72.46 +/- 10.55 | 69.70 +/- 4.48 | 37.88 +/- 12.22 |'
  id: totrans-837
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 62.64 | 72.46 +/- 10.55 | 69.70 +/- 4.48 | 37.88 +/- 12.22 |'
- en: '| GPT-3-6.7b | 62.39 | 76.81 +/- 14.57 | 62.50 +/- 5.53 | 36.36 +/- 7.42 |'
  id: totrans-838
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 62.39 | 76.81 +/- 14.57 | 62.50 +/- 5.53 | 36.36 +/- 7.42 |'
- en: '| GPT-3-175b | 68.72 | 82.61 +/- 4.35 | 71.59 +/- 2.54 | 60.61 +/- 13.55 |'
  id: totrans-839
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.72 | 82.61 +/- 4.35 | 71.59 +/- 2.54 | 60.61 +/- 13.55 |'
- en: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 47.00 | 52.17 +/- 0.00 | 39.02 +/- 0.85 | 36.36 +/- 0.00 |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 47.00 | 52.17 +/- 0.00 | 39.02 +/- 0.85 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 46.58 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.58 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.36 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.36 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 52.81 | 47.83 +/- 4.35 | 60.98 +/- 0.85 | 63.64 +/- 0.00
    |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 52.81 | 47.83 +/- 4.35 | 60.98 +/- 0.85 | 63.64 +/- 0.00
    |'
- en: '| Flan-T5-780m | 61.03 | 61.59 +/- 4.64 | 70.08 +/- 9.59 | 42.42 +/- 4.29 |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.03 | 61.59 +/- 4.64 | 70.08 +/- 9.59 | 42.42 +/- 4.29 |'
- en: '| Flan-T5-3b | 54.89 | 62.32 +/- 7.39 | 60.61 +/- 8.26 | 34.85 +/- 3.39 |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 54.89 | 62.32 +/- 7.39 | 60.61 +/- 8.26 | 34.85 +/- 3.39 |'
- en: '| Flan-T5-11b | 61.64 | 68.84 +/- 6.84 | 67.80 +/- 8.03 | 43.94 +/- 8.16 |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 61.64 | 68.84 +/- 6.84 | 67.80 +/- 8.03 | 43.94 +/- 8.16 |'
- en: '| Cohere-command-6b | 68.56 | 77.54 +/- 9.86 | 78.79 +/- 5.36 | 39.39 +/- 4.29
    |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 68.56 | 77.54 +/- 9.86 | 78.79 +/- 5.36 | 39.39 +/- 4.29
    |'
- en: '| Cohere-command-52b | 75.42 | 87.68 +/- 3.90 | 84.09 +/- 1.31 | 74.24 +/-
    9.70 |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.42 | 87.68 +/- 3.90 | 84.09 +/- 1.31 | 74.24 +/-
    9.70 |'
- en: '| text-ada-001-unknown | 57.61 | 52.17 +/- 3.55 | 64.39 +/- 2.83 | 62.12 +/-
    8.16 |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.61 | 52.17 +/- 3.55 | 64.39 +/- 2.83 | 62.12 +/-
    8.16 |'
- en: '| text-babbage-001-unknown | 66.14 | 71.74 +/- 2.17 | 77.65 +/- 5.15 | 57.58
    +/- 12.49 |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 66.14 | 71.74 +/- 2.17 | 77.65 +/- 5.15 | 57.58
    +/- 12.49 |'
- en: '| text-curie-001-unknown | 71.33 | 76.09 +/- 2.17 | 70.08 +/- 6.07 | 43.94
    +/- 3.39 |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 71.33 | 76.09 +/- 2.17 | 70.08 +/- 6.07 | 43.94
    +/- 3.39 |'
- en: '| text-davinci-001-unknown | 74.53 | 88.41 +/- 3.24 | 78.03 +/- 5.97 | 66.67
    +/- 12.49 |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 74.53 | 88.41 +/- 3.24 | 78.03 +/- 5.97 | 66.67
    +/- 12.49 |'
- en: '| text-davinci-002-unknown | 79.56 | 90.58 +/- 1.62 | 89.02 +/- 2.04 | 69.70
    +/- 6.78 |'
  id: totrans-854
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.56 | 90.58 +/- 1.62 | 89.02 +/- 2.04 | 69.70
    +/- 6.78 |'
- en: '| text-davinci-003-unknown | 79.67 | 89.13 +/- 2.17 | 86.36 +/- 2.27 | 74.24
    +/- 11.03 |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.67 | 89.13 +/- 2.17 | 86.36 +/- 2.27 | 74.24
    +/- 11.03 |'
- en: '| ChatGPT-unknown | 73.89 | 86.96 +/- 6.15 | 87.88 +/- 4.85 | 75.76 +/- 12.49
    |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 73.89 | 86.96 +/- 6.15 | 87.88 +/- 4.85 | 75.76 +/- 12.49
    |'
- en: '| GPT-4-unknown | 82.03 | 95.65 +/- 2.51 | 86.74 +/- 2.04 | 87.88 +/- 6.78
    |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.03 | 95.65 +/- 2.51 | 86.74 +/- 2.04 | 87.88 +/- 6.78
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
  zh: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 26: Accuracy per label for 5-shot evaluation.'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 26: Accuracy per label for 5-shot evaluation.'
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-860
  prefs: []
  type: TYPE_TB
  zh: '| Model | Mean | Particularised | Generalised | Other |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 50.22 | 47.35 +/- 3.12 | 60.87 +/- 13.28 | 48.84 +/- 3.71 |'
  id: totrans-862
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 50.22 | 47.35 +/- 3.12 | 60.87 +/- 13.28 | 48.84 +/- 3.71 |'
- en: '| OPT-350m | 51.47 | 39.58 +/- 1.79 | 67.39 +/- 3.07 | 51.38 +/- 0.95 |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 51.47 | 39.58 +/- 1.79 | 67.39 +/- 3.07 | 51.38 +/- 0.95 |'
- en: '| OPT-1.3b | 58.03 | 56.06 +/- 3.63 | 57.61 +/- 4.12 | 58.01 +/- 2.84 |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 58.03 | 56.06 +/- 3.63 | 57.61 +/- 4.12 | 58.01 +/- 2.84 |'
- en: '| OPT-2.7b | 57.33 | 47.35 +/- 3.05 | 72.46 +/- 2.40 | 56.20 +/- 3.60 |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 57.33 | 47.35 +/- 3.05 | 72.46 +/- 2.40 | 56.20 +/- 3.60 |'
- en: '| OPT-6.7b | 63.31 | 56.63 +/- 6.93 | 71.01 +/- 6.48 | 63.74 +/- 3.18 |'
  id: totrans-866
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.31 | 56.63 +/- 6.93 | 71.01 +/- 6.48 | 63.74 +/- 3.18 |'
- en: '| OPT-13b | 67.39 | 60.23 +/- 2.93 | 64.86 +/- 2.92 | 69.12 +/- 2.83 |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 67.39 | 60.23 +/- 2.93 | 64.86 +/- 2.92 | 69.12 +/- 2.83 |'
- en: '| OPT-30b | 65.64 | 59.85 +/- 1.42 | 59.42 +/- 7.70 | 67.27 +/- 4.36 |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 65.64 | 59.85 +/- 1.42 | 59.42 +/- 7.70 | 67.27 +/- 4.36 |'
- en: '| OPT-66b | 61.50 | 56.44 +/- 3.51 | 58.70 +/- 9.64 | 63.65 +/- 3.93 |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.50 | 56.44 +/- 3.51 | 58.70 +/- 9.64 | 63.65 +/- 3.93 |'
- en: '| OPT-175b | 63.89 | 61.55 +/- 2.22 | 52.54 +/- 5.53 | 65.33 +/- 2.44 |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 63.89 | 61.55 +/- 2.22 | 52.54 +/- 5.53 | 65.33 +/- 2.44 |'
- en: '| BLOOM-560m | 53.75 | 44.89 +/- 2.05 | 73.19 +/- 6.11 | 52.50 +/- 0.78 |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 53.75 | 44.89 +/- 2.05 | 73.19 +/- 6.11 | 52.50 +/- 0.78 |'
- en: '| BLOOM-1b1 | 57.39 | 48.67 +/- 3.44 | 70.65 +/- 4.12 | 57.02 +/- 1.86 |'
  id: totrans-872
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.39 | 48.67 +/- 3.44 | 70.65 +/- 4.12 | 57.02 +/- 1.86 |'
- en: '| BLOOM-1b7 | 54.44 | 48.86 +/- 4.91 | 60.14 +/- 3.24 | 54.74 +/- 0.96 |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 54.44 | 48.86 +/- 4.91 | 60.14 +/- 3.24 | 54.74 +/- 0.96 |'
- en: '| BLOOM-3b | 57.19 | 50.00 +/- 3.35 | 72.46 +/- 2.40 | 56.24 +/- 0.89 |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 57.19 | 50.00 +/- 3.35 | 72.46 +/- 2.40 | 56.24 +/- 0.89 |'
- en: '| BLOOM-7b1 | 54.50 | 46.02 +/- 2.25 | 72.10 +/- 4.24 | 53.10 +/- 1.04 |'
  id: totrans-875
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 54.50 | 46.02 +/- 2.25 | 72.10 +/- 4.24 | 53.10 +/- 1.04 |'
- en: '| BLOOM-176b | 65.42 | 65.53 +/- 4.38 | 49.28 +/- 9.69 | 66.88 +/- 3.42 |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 65.42 | 65.53 +/- 4.38 | 49.28 +/- 9.69 | 66.88 +/- 3.42 |'
- en: '| EleutherAI-125m | 49.56 | 44.32 +/- 5.76 | 56.88 +/- 4.05 | 50.04 +/- 2.49
    |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 49.56 | 44.32 +/- 5.76 | 56.88 +/- 4.05 | 50.04 +/- 2.49
    |'
- en: '| EleutherAI-1.3b | 57.11 | 50.76 +/- 3.26 | 69.93 +/- 4.77 | 56.89 +/- 1.57
    |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.11 | 50.76 +/- 3.26 | 69.93 +/- 4.77 | 56.89 +/- 1.57
    |'
- en: '| EleutherAI-2.7b | 58.03 | 51.52 +/- 2.43 | 61.59 +/- 4.10 | 58.61 +/- 1.39
    |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 58.03 | 51.52 +/- 2.43 | 61.59 +/- 4.10 | 58.61 +/- 1.39
    |'
- en: '| EleutherAI-6b | 58.39 | 49.62 +/- 1.69 | 63.04 +/- 5.02 | 59.91 +/- 5.04
    |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.39 | 49.62 +/- 1.69 | 63.04 +/- 5.02 | 59.91 +/- 5.04
    |'
- en: '| EleutherAI-20b | 61.14 | 51.52 +/- 2.43 | 61.59 +/- 12.78 | 63.48 +/- 5.29
    |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 61.14 | 51.52 +/- 2.43 | 61.59 +/- 12.78 | 63.48 +/- 5.29
    |'
- en: '| Cohere-409m | 53.39 | 50.38 +/- 1.69 | 68.48 +/- 4.82 | 52.45 +/- 0.99 |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 53.39 | 50.38 +/- 1.69 | 68.48 +/- 4.82 | 52.45 +/- 0.99 |'
- en: '| Cohere-6b | 60.89 | 52.65 +/- 2.14 | 64.49 +/- 5.98 | 61.80 +/- 4.77 |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.89 | 52.65 +/- 2.14 | 64.49 +/- 5.98 | 61.80 +/- 4.77 |'
- en: '| Cohere-13b | 62.47 | 59.66 +/- 6.11 | 68.84 +/- 7.28 | 62.02 +/- 3.56 |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 62.47 | 59.66 +/- 6.11 | 68.84 +/- 7.28 | 62.02 +/- 3.56 |'
- en: '| Cohere-52b | 65.14 | 60.04 +/- 4.07 | 68.12 +/- 7.39 | 65.46 +/- 3.30 |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 65.14 | 60.04 +/- 4.07 | 68.12 +/- 7.39 | 65.46 +/- 3.30 |'
- en: '| GPT-3-350m | 55.72 | 44.70 +/- 1.26 | 74.28 +/- 6.07 | 55.47 +/- 1.59 |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 55.72 | 44.70 +/- 1.26 | 74.28 +/- 6.07 | 55.47 +/- 1.59 |'
- en: '| GPT-3-1.3b | 62.64 | 49.24 +/- 2.51 | 67.39 +/- 4.35 | 64.38 +/- 2.51 |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 62.64 | 49.24 +/- 2.51 | 67.39 +/- 4.35 | 64.38 +/- 2.51 |'
- en: '| GPT-3-6.7b | 62.39 | 51.70 +/- 2.25 | 64.86 +/- 7.68 | 64.38 +/- 2.30 |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 62.39 | 51.70 +/- 2.25 | 64.86 +/- 7.68 | 64.38 +/- 2.30 |'
- en: '| GPT-3-175b | 68.72 | 60.98 +/- 5.74 | 66.67 +/- 8.76 | 69.90 +/- 0.68 |'
  id: totrans-889
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.72 | 60.98 +/- 5.74 | 66.67 +/- 8.76 | 69.90 +/- 0.68 |'
- en: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.15 |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.15 |'
- en: '| T0-11b | 47.00 | 55.87 +/- 0.42 | 25.00 +/- 1.66 | 48.62 +/- 0.23 |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 47.00 | 55.87 +/- 0.42 | 25.00 +/- 1.66 | 48.62 +/- 0.23 |'
- en: '| BlenderBot-90m | 46.58 | 55.11 +/- 1.27 | 24.28 +/- 0.81 | 48.28 +/- 0.31
    |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.58 | 55.11 +/- 1.27 | 24.28 +/- 0.81 | 48.28 +/- 0.31
    |'
- en: '| BlenderBot-3b | 53.36 | 44.32 +/- 0.00 | 76.09 +/- 0.00 | 51.72 +/- 0.10
    |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.36 | 44.32 +/- 0.00 | 76.09 +/- 0.00 | 51.72 +/- 0.10
    |'
- en: '| BlenderBot-9b | 52.81 | 44.32 +/- 0.93 | 75.72 +/- 0.81 | 50.95 +/- 1.02
    |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 52.81 | 44.32 +/- 0.93 | 75.72 +/- 0.81 | 50.95 +/- 1.02
    |'
- en: '| Flan-T5-780m | 61.03 | 54.36 +/- 3.50 | 71.01 +/- 12.96 | 60.77 +/- 6.09
    |'
  id: totrans-895
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.03 | 54.36 +/- 3.50 | 71.01 +/- 12.96 | 60.77 +/- 6.09
    |'
- en: '| Flan-T5-3b | 54.89 | 57.01 +/- 1.79 | 41.30 +/- 9.64 | 55.47 +/- 4.00 |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 54.89 | 57.01 +/- 1.79 | 41.30 +/- 9.64 | 55.47 +/- 4.00 |'
- en: '| Flan-T5-11b | 61.64 | 56.25 +/- 3.20 | 64.86 +/- 17.04 | 61.76 +/- 5.21 |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 61.64 | 56.25 +/- 3.20 | 64.86 +/- 17.04 | 61.76 +/- 5.21 |'
- en: '| Cohere-command-6b | 68.56 | 60.23 +/- 5.00 | 74.28 +/- 6.57 | 68.91 +/- 1.47
    |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 68.56 | 60.23 +/- 5.00 | 74.28 +/- 6.57 | 68.91 +/- 1.47
    |'
- en: '| Cohere-command-52b | 75.42 | 70.08 +/- 3.39 | 77.17 +/- 3.26 | 74.68 +/-
    2.80 |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.42 | 70.08 +/- 3.39 | 77.17 +/- 3.26 | 74.68 +/-
    2.80 |'
- en: '| text-ada-001-unknown | 57.61 | 48.86 +/- 2.18 | 72.83 +/- 1.66 | 57.11 +/-
    3.74 |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.61 | 48.86 +/- 2.18 | 72.83 +/- 1.66 | 57.11 +/-
    3.74 |'
- en: '| text-babbage-001-unknown | 66.14 | 57.01 +/- 2.82 | 71.74 +/- 2.81 | 66.06
    +/- 0.85 |'
  id: totrans-901
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 66.14 | 57.01 +/- 2.82 | 71.74 +/- 2.81 | 66.06
    +/- 0.85 |'
- en: '| text-curie-001-unknown | 71.33 | 60.04 +/- 0.78 | 69.93 +/- 5.67 | 74.63
    +/- 0.98 |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 71.33 | 60.04 +/- 0.78 | 69.93 +/- 5.67 | 74.63
    +/- 0.98 |'
- en: '| text-davinci-001-unknown | 74.53 | 60.80 +/- 3.75 | 81.16 +/- 3.69 | 75.80
    +/- 1.32 |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 74.53 | 60.80 +/- 3.75 | 81.16 +/- 3.69 | 75.80
    +/- 1.32 |'
- en: '| text-davinci-002-unknown | 79.56 | 71.02 +/- 2.76 | 87.68 +/- 1.62 | 79.03
    +/- 2.26 |'
  id: totrans-904
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.56 | 71.02 +/- 2.76 | 87.68 +/- 1.62 | 79.03
    +/- 2.26 |'
- en: '| text-davinci-003-unknown | 79.67 | 71.59 +/- 1.86 | 87.68 +/- 1.02 | 79.33
    +/- 1.12 |'
  id: totrans-905
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.67 | 71.59 +/- 1.86 | 87.68 +/- 1.02 | 79.33
    +/- 1.12 |'
- en: '| ChatGPT-unknown | 73.89 | 69.51 +/- 4.80 | 73.91 +/- 11.64 | 72.44 +/- 6.16
    |'
  id: totrans-906
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 73.89 | 69.51 +/- 4.80 | 73.91 +/- 11.64 | 72.44 +/- 6.16
    |'
- en: '| GPT-4-unknown | 82.03 | 71.21 +/- 2.91 | 87.32 +/- 3.64 | 82.30 +/- 2.31
    |'
  id: totrans-907
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.03 | 71.21 +/- 2.91 | 87.32 +/- 3.64 | 82.30 +/- 2.31
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
  zh: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 27: Accuracy per label for 10-shot evaluation.'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 表 27：10-shot 评估每个标签的准确性。
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-910
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-911
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 52.89 | 55.80 +/- 8.48 | 55.68 +/- 8.78 | 66.67 +/- 6.78 |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 52.89 | 55.80 +/- 8.48 | 55.68 +/- 8.78 | 66.67 +/- 6.78 |'
- en: '| OPT-350m | 56.72 | 57.97 +/- 4.10 | 60.61 +/- 3.86 | 65.15 +/- 16.11 |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 56.72 | 57.97 +/- 4.10 | 60.61 +/- 3.86 | 65.15 +/- 16.11 |'
- en: '| OPT-1.3b | 59.92 | 70.29 +/- 4.64 | 54.17 +/- 3.57 | 34.85 +/- 3.39 |'
  id: totrans-914
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 59.92 | 70.29 +/- 4.64 | 54.17 +/- 3.57 | 34.85 +/- 3.39 |'
- en: '| OPT-2.7b | 58.03 | 52.17 +/- 2.51 | 65.53 +/- 2.76 | 63.64 +/- 5.25 |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 58.03 | 52.17 +/- 2.51 | 65.53 +/- 2.76 | 63.64 +/- 5.25 |'
- en: '| OPT-6.7b | 63.28 | 71.01 +/- 5.42 | 65.53 +/- 6.07 | 53.03 +/- 17.73 |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.28 | 71.01 +/- 5.42 | 65.53 +/- 6.07 | 53.03 +/- 17.73 |'
- en: '| OPT-13b | 65.75 | 77.54 +/- 6.84 | 63.26 +/- 4.23 | 50.00 +/- 10.16 |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 65.75 | 77.54 +/- 6.84 | 63.26 +/- 4.23 | 50.00 +/- 10.16 |'
- en: '| OPT-30b | 63.36 | 78.99 +/- 4.64 | 56.06 +/- 8.57 | 33.33 +/- 4.29 |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 63.36 | 78.99 +/- 4.64 | 56.06 +/- 8.57 | 33.33 +/- 4.29 |'
- en: '| OPT-66b | 60.81 | 71.01 +/- 7.80 | 54.55 +/- 9.46 | 36.36 +/- 0.00 |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 60.81 | 71.01 +/- 7.80 | 54.55 +/- 9.46 | 36.36 +/- 0.00 |'
- en: '| OPT-175b | 60.75 | 76.81 +/- 4.10 | 58.33 +/- 6.52 | 43.94 +/- 11.03 |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 60.75 | 76.81 +/- 4.10 | 58.33 +/- 6.52 | 43.94 +/- 11.03 |'
- en: '| BLOOM-560m | 54.56 | 49.28 +/- 3.24 | 64.39 +/- 3.12 | 63.64 +/- 0.00 |'
  id: totrans-921
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 54.56 | 49.28 +/- 3.24 | 64.39 +/- 3.12 | 63.64 +/- 0.00 |'
- en: '| BLOOM-1b1 | 57.31 | 55.07 +/- 5.42 | 60.61 +/- 5.36 | 59.09 +/- 8.70 |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.31 | 55.07 +/- 5.42 | 60.61 +/- 5.36 | 59.09 +/- 8.70 |'
- en: '| BLOOM-1b7 | 53.14 | 68.12 +/- 6.48 | 45.45 +/- 2.62 | 59.09 +/- 11.44 |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 53.14 | 68.12 +/- 6.48 | 45.45 +/- 2.62 | 59.09 +/- 11.44 |'
- en: '| BLOOM-3b | 59.39 | 54.35 +/- 4.86 | 65.53 +/- 5.32 | 66.67 +/- 4.29 |'
  id: totrans-924
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 59.39 | 54.35 +/- 4.86 | 65.53 +/- 5.32 | 66.67 +/- 4.29 |'
- en: '| BLOOM-7b1 | 56.11 | 53.62 +/- 7.39 | 65.53 +/- 3.81 | 69.70 +/- 6.78 |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 56.11 | 53.62 +/- 7.39 | 65.53 +/- 3.81 | 69.70 +/- 6.78 |'
- en: '| BLOOM-176b | 63.47 | 75.36 +/- 5.98 | 68.18 +/- 9.00 | 42.42 +/- 4.29 |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 63.47 | 75.36 +/- 5.98 | 68.18 +/- 9.00 | 42.42 +/- 4.29 |'
- en: '| EleutherAI-125m | 54.39 | 58.70 +/- 6.02 | 52.27 +/- 5.41 | 83.33 +/- 19.93
    |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 54.39 | 58.70 +/- 6.02 | 52.27 +/- 5.41 | 83.33 +/- 19.93
    |'
- en: '| EleutherAI-1.3b | 57.83 | 67.39 +/- 5.47 | 60.61 +/- 5.19 | 62.12 +/- 11.03
    |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.83 | 67.39 +/- 5.47 | 60.61 +/- 5.19 | 62.12 +/- 11.03
    |'
- en: '| EleutherAI-2.7b | 57.03 | 73.19 +/- 2.99 | 55.68 +/- 5.37 | 66.67 +/- 13.55
    |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 57.03 | 73.19 +/- 2.99 | 55.68 +/- 5.37 | 66.67 +/- 13.55
    |'
- en: '| EleutherAI-6b | 57.64 | 64.49 +/- 3.90 | 51.14 +/- 11.04 | 56.06 +/- 16.94
    |'
  id: totrans-930
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 57.64 | 64.49 +/- 3.90 | 51.14 +/- 11.04 | 56.06 +/- 16.94
    |'
- en: '| EleutherAI-20b | 59.33 | 67.39 +/- 5.47 | 62.12 +/- 10.47 | 37.88 +/- 6.25
    |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 59.33 | 67.39 +/- 5.47 | 62.12 +/- 10.47 | 37.88 +/- 6.25
    |'
- en: '| Cohere-409m | 53.92 | 63.04 +/- 5.47 | 46.21 +/- 6.90 | 51.52 +/- 11.34 |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 53.92 | 63.04 +/- 5.47 | 46.21 +/- 6.90 | 51.52 +/- 11.34 |'
- en: '| Cohere-6b | 58.72 | 66.67 +/- 9.61 | 63.26 +/- 14.46 | 50.00 +/- 12.59 |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 58.72 | 66.67 +/- 9.61 | 63.26 +/- 14.46 | 50.00 +/- 12.59 |'
- en: '| Cohere-13b | 60.36 | 76.81 +/- 6.48 | 56.06 +/- 9.52 | 34.85 +/- 3.39 |'
  id: totrans-934
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 60.36 | 76.81 +/- 6.48 | 56.06 +/- 9.52 | 34.85 +/- 3.39 |'
- en: '| Cohere-52b | 63.31 | 72.46 +/- 5.98 | 68.18 +/- 4.55 | 51.52 +/- 10.05 |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 63.31 | 72.46 +/- 5.98 | 68.18 +/- 4.55 | 51.52 +/- 10.05 |'
- en: '| GPT-3-350m | 57.72 | 53.62 +/- 3.24 | 63.64 +/- 6.01 | 65.15 +/- 8.16 |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 57.72 | 53.62 +/- 3.24 | 63.64 +/- 6.01 | 65.15 +/- 8.16 |'
- en: '| GPT-3-1.3b | 60.92 | 73.19 +/- 7.28 | 59.47 +/- 5.78 | 48.48 +/- 8.57 |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.92 | 73.19 +/- 7.28 | 59.47 +/- 5.78 | 48.48 +/- 8.57 |'
- en: '| GPT-3-6.7b | 63.94 | 71.01 +/- 7.80 | 67.80 +/- 1.56 | 40.91 +/- 8.70 |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 63.94 | 71.01 +/- 7.80 | 67.80 +/- 1.56 | 40.91 +/- 8.70 |'
- en: '| GPT-3-175b | 67.28 | 76.81 +/- 9.61 | 68.56 +/- 5.32 | 81.82 +/- 10.50 |'
  id: totrans-939
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 67.28 | 76.81 +/- 9.61 | 68.56 +/- 5.32 | 81.82 +/- 10.50 |'
- en: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 46.72 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.72 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.25 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.25 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 53.36 | 42.03 +/- 4.10 | 62.12 +/- 3.12 | 63.64 +/- 0.00
    |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.36 | 42.03 +/- 4.10 | 62.12 +/- 3.12 | 63.64 +/- 0.00
    |'
- en: '| Flan-T5-780m | 60.19 | 63.04 +/- 2.17 | 68.56 +/- 10.77 | 40.91 +/- 4.55
    |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 60.19 | 63.04 +/- 2.17 | 68.56 +/- 10.77 | 40.91 +/- 4.55
    |'
- en: '| Flan-T5-3b | 55.14 | 61.59 +/- 6.84 | 58.71 +/- 10.37 | 36.36 +/- 0.00 |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 55.14 | 61.59 +/- 6.84 | 58.71 +/- 10.37 | 36.36 +/- 0.00 |'
- en: '| Flan-T5-11b | 60.56 | 67.39 +/- 7.43 | 70.83 +/- 10.45 | 40.91 +/- 4.55 |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.56 | 67.39 +/- 7.43 | 70.83 +/- 10.45 | 40.91 +/- 4.55 |'
- en: '| Cohere-command-6b | 68.22 | 78.99 +/- 5.28 | 74.62 +/- 5.32 | 36.36 +/- 0.00
    |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 68.22 | 78.99 +/- 5.28 | 74.62 +/- 5.32 | 36.36 +/- 0.00
    |'
- en: '| Cohere-command-52b | 75.64 | 88.41 +/- 3.24 | 84.85 +/- 2.51 | 66.67 +/-
    8.57 |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.64 | 88.41 +/- 3.24 | 84.85 +/- 2.51 | 66.67 +/-
    8.57 |'
- en: '| text-ada-001-unknown | 57.36 | 64.49 +/- 7.28 | 56.44 +/- 5.78 | 57.58 +/-
    6.78 |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.36 | 64.49 +/- 7.28 | 56.44 +/- 5.78 | 57.58 +/-
    6.78 |'
- en: '| text-babbage-001-unknown | 63.53 | 67.39 +/- 2.17 | 73.11 +/- 4.62 | 68.18
    +/- 4.55 |'
  id: totrans-951
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 63.53 | 67.39 +/- 2.17 | 73.11 +/- 4.62 | 68.18
    +/- 4.55 |'
- en: '| text-curie-001-unknown | 70.17 | 83.33 +/- 1.62 | 76.14 +/- 2.86 | 45.45
    +/- 5.25 |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 70.17 | 83.33 +/- 1.62 | 76.14 +/- 2.86 | 45.45
    +/- 5.25 |'
- en: '| text-davinci-001-unknown | 74.97 | 89.13 +/- 2.17 | 83.33 +/- 2.83 | 59.09
    +/- 6.94 |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 74.97 | 89.13 +/- 2.17 | 83.33 +/- 2.83 | 59.09
    +/- 6.94 |'
- en: '| text-davinci-002-unknown | 79.56 | 93.48 +/- 2.17 | 88.26 +/- 0.85 | 66.67
    +/- 8.57 |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.56 | 93.48 +/- 2.17 | 88.26 +/- 0.85 | 66.67
    +/- 8.57 |'
- en: '| text-davinci-003-unknown | 79.00 | 94.93 +/- 3.90 | 85.61 +/- 3.39 | 66.67
    +/- 4.29 |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.00 | 94.93 +/- 3.90 | 85.61 +/- 3.39 | 66.67
    +/- 4.29 |'
- en: '| ChatGPT-unknown | 74.28 | 84.06 +/- 6.48 | 86.36 +/- 4.35 | 62.12 +/- 11.03
    |'
  id: totrans-956
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 74.28 | 84.06 +/- 6.48 | 86.36 +/- 4.35 | 62.12 +/- 11.03
    |'
- en: '| GPT-4-unknown | 81.31 | 94.93 +/- 2.99 | 86.74 +/- 4.03 | 89.39 +/- 3.39
    |'
  id: totrans-957
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 81.31 | 94.93 +/- 2.99 | 86.74 +/- 4.03 | 89.39 +/- 3.39
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 28: Accuracy per label for 10-shot evaluation.'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 表 28：每标签的准确性（10-shot评估）。
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 特殊化 | 泛化 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 52.89 | 51.33 +/- 4.52 | 57.97 +/- 15.30 | 51.68 +/- 1.58 |'
  id: totrans-962
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 52.89 | 51.33 +/- 4.52 | 57.97 +/- 15.30 | 51.68 +/- 1.58 |'
- en: '| OPT-350m | 56.72 | 55.11 +/- 1.57 | 72.10 +/- 2.32 | 54.39 +/- 1.50 |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 56.72 | 55.11 +/- 1.57 | 72.10 +/- 2.32 | 54.39 +/- 1.50 |'
- en: '| OPT-1.3b | 59.92 | 60.23 +/- 0.93 | 61.23 +/- 2.32 | 60.34 +/- 2.77 |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 59.92 | 60.23 +/- 0.93 | 61.23 +/- 2.32 | 60.34 +/- 2.77 |'
- en: '| OPT-2.7b | 58.03 | 49.62 +/- 3.32 | 74.64 +/- 2.05 | 57.19 +/- 3.77 |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 58.03 | 49.62 +/- 3.32 | 74.64 +/- 2.05 | 57.19 +/- 3.77 |'
- en: '| OPT-6.7b | 63.28 | 58.33 +/- 3.97 | 73.19 +/- 3.48 | 62.70 +/- 3.22 |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.28 | 58.33 +/- 3.97 | 73.19 +/- 3.48 | 62.70 +/- 3.22 |'
- en: '| OPT-13b | 65.75 | 60.23 +/- 2.45 | 72.10 +/- 3.18 | 66.37 +/- 2.84 |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 65.75 | 60.23 +/- 2.45 | 72.10 +/- 3.18 | 66.37 +/- 2.84 |'
- en: '| OPT-30b | 63.36 | 62.31 +/- 2.40 | 65.22 +/- 6.28 | 64.17 +/- 3.73 |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 63.36 | 62.31 +/- 2.40 | 65.22 +/- 6.28 | 64.17 +/- 3.73 |'
- en: '| OPT-66b | 60.81 | 57.58 +/- 1.26 | 60.51 +/- 7.98 | 62.36 +/- 3.33 |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 60.81 | 57.58 +/- 1.26 | 60.51 +/- 7.98 | 62.36 +/- 3.33 |'
- en: '| OPT-175b | 60.75 | 60.98 +/- 2.24 | 56.16 +/- 2.92 | 60.94 +/- 3.42 |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 60.75 | 60.98 +/- 2.24 | 56.16 +/- 2.92 | 60.94 +/- 3.42 |'
- en: '| BLOOM-560m | 54.56 | 46.21 +/- 1.26 | 73.19 +/- 2.99 | 53.06 +/- 0.77 |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 54.56 | 46.21 +/- 1.26 | 73.19 +/- 2.99 | 53.06 +/- 0.77 |'
- en: '| BLOOM-1b1 | 57.31 | 47.54 +/- 4.97 | 64.13 +/- 7.40 | 58.31 +/- 3.30 |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.31 | 47.54 +/- 4.97 | 64.13 +/- 7.40 | 58.31 +/- 3.30 |'
- en: '| BLOOM-1b7 | 53.14 | 53.03 +/- 3.45 | 53.62 +/- 4.64 | 52.80 +/- 1.74 |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 53.14 | 53.03 +/- 3.45 | 53.62 +/- 4.64 | 52.80 +/- 1.74 |'
- en: '| BLOOM-3b | 59.39 | 55.49 +/- 2.22 | 69.57 +/- 5.89 | 58.35 +/- 0.41 |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 59.39 | 55.49 +/- 2.22 | 69.57 +/- 5.89 | 58.35 +/- 0.41 |'
- en: '| BLOOM-7b1 | 56.11 | 49.62 +/- 5.07 | 71.38 +/- 2.92 | 54.35 +/- 3.34 |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 56.11 | 49.62 +/- 5.07 | 71.38 +/- 2.92 | 54.35 +/- 3.34 |'
- en: '| BLOOM-176b | 63.47 | 68.18 +/- 3.47 | 50.36 +/- 9.00 | 63.35 +/- 4.15 |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 63.47 | 68.18 +/- 3.47 | 50.36 +/- 9.00 | 63.35 +/- 4.15 |'
- en: '| EleutherAI-125m | 54.39 | 55.11 +/- 3.13 | 63.41 +/- 6.20 | 52.20 +/- 2.23
    |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 54.39 | 55.11 +/- 3.13 | 63.41 +/- 6.20 | 52.20 +/- 2.23
    |'
- en: '| EleutherAI-1.3b | 57.83 | 50.00 +/- 2.78 | 69.20 +/- 2.32 | 57.15 +/- 1.52
    |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.83 | 50.00 +/- 2.78 | 69.20 +/- 2.32 | 57.15 +/- 1.52
    |'
- en: '| EleutherAI-2.7b | 57.03 | 57.20 +/- 2.76 | 57.25 +/- 6.23 | 55.77 +/- 0.96
    |'
  id: totrans-979
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 57.03 | 57.20 +/- 2.76 | 57.25 +/- 6.23 | 55.77 +/- 0.96
    |'
- en: '| EleutherAI-6b | 57.64 | 56.25 +/- 2.76 | 59.42 +/- 10.25 | 58.05 +/- 4.71
    |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 57.64 | 56.25 +/- 2.76 | 59.42 +/- 10.25 | 58.05 +/- 4.71
    |'
- en: '| EleutherAI-20b | 59.33 | 57.39 +/- 1.83 | 63.04 +/- 13.63 | 59.04 +/- 3.30
    |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 59.33 | 57.39 +/- 1.83 | 63.04 +/- 13.63 | 59.04 +/- 3.30
    |'
- en: '| Cohere-409m | 53.92 | 57.39 +/- 2.60 | 66.30 +/- 4.98 | 51.94 +/- 1.99 |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 53.92 | 57.39 +/- 2.60 | 66.30 +/- 4.98 | 51.94 +/- 1.99 |'
- en: '| Cohere-6b | 58.72 | 51.70 +/- 2.34 | 64.86 +/- 6.33 | 58.74 +/- 5.75 |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 58.72 | 51.70 +/- 2.34 | 64.86 +/- 6.33 | 58.74 +/- 5.75 |'
- en: '| Cohere-13b | 60.36 | 58.52 +/- 1.27 | 70.29 +/- 5.98 | 59.73 +/- 5.24 |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 60.36 | 58.52 +/- 1.27 | 70.29 +/- 5.98 | 59.73 +/- 5.24 |'
- en: '| Cohere-52b | 63.31 | 53.41 +/- 2.93 | 67.75 +/- 7.88 | 64.17 +/- 2.12 |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 63.31 | 53.41 +/- 2.93 | 67.75 +/- 7.88 | 64.17 +/- 2.12 |'
- en: '| GPT-3-350m | 57.72 | 50.95 +/- 2.89 | 73.91 +/- 1.26 | 56.59 +/- 1.79 |'
  id: totrans-986
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 57.72 | 50.95 +/- 2.89 | 73.91 +/- 1.26 | 56.59 +/- 1.79 |'
- en: '| GPT-3-1.3b | 60.92 | 57.01 +/- 5.01 | 63.77 +/- 2.71 | 61.15 +/- 1.10 |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.92 | 57.01 +/- 5.01 | 63.77 +/- 2.71 | 61.15 +/- 1.10 |'
- en: '| GPT-3-6.7b | 63.94 | 58.52 +/- 4.19 | 66.67 +/- 5.28 | 64.56 +/- 1.31 |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 63.94 | 58.52 +/- 4.19 | 66.67 +/- 5.28 | 64.56 +/- 1.31 |'
- en: '| GPT-3-175b | 67.28 | 63.45 +/- 2.66 | 68.84 +/- 4.64 | 66.75 +/- 2.65 |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 67.28 | 63.45 +/- 2.66 | 68.84 +/- 4.64 | 66.75 +/- 2.65 |'
- en: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 24.28 +/- 0.81 | 48.28 +/- 0.10 |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 24.28 +/- 0.81 | 48.28 +/- 0.10 |'
- en: '| T0-11b | 46.72 | 55.68 +/- 0.00 | 24.28 +/- 0.81 | 48.36 +/- 0.10 |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.72 | 55.68 +/- 0.00 | 24.28 +/- 0.81 | 48.36 +/- 0.10 |'
- en: '| BlenderBot-90m | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00
    |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.25 | 44.70 +/- 0.54 | 76.09 +/- 0.00 | 51.46 +/- 0.23
    |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.25 | 44.70 +/- 0.54 | 76.09 +/- 0.00 | 51.46 +/- 0.23
    |'
- en: '| BlenderBot-9b | 53.36 | 45.27 +/- 1.21 | 76.09 +/- 1.77 | 51.77 +/- 0.69
    |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.36 | 45.27 +/- 1.21 | 76.09 +/- 1.77 | 51.77 +/- 0.69
    |'
- en: '| Flan-T5-780m | 60.19 | 54.17 +/- 2.51 | 71.38 +/- 10.75 | 59.60 +/- 4.49
    |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 60.19 | 54.17 +/- 2.51 | 71.38 +/- 10.75 | 59.60 +/- 4.49
    |'
- en: '| Flan-T5-3b | 55.14 | 54.92 +/- 1.42 | 43.48 +/- 12.10 | 56.29 +/- 3.84 |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 55.14 | 54.92 +/- 1.42 | 43.48 +/- 12.10 | 56.29 +/- 3.84 |'
- en: '| Flan-T5-11b | 60.56 | 59.66 +/- 3.33 | 57.61 +/- 13.09 | 60.03 +/- 5.13 |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.56 | 59.66 +/- 3.33 | 57.61 +/- 13.09 | 60.03 +/- 5.13 |'
- en: '| Cohere-command-6b | 68.22 | 63.07 +/- 4.44 | 77.17 +/- 6.73 | 67.79 +/- 2.45
    |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 68.22 | 63.07 +/- 4.44 | 77.17 +/- 6.73 | 67.79 +/- 2.45
    |'
- en: '| Cohere-command-52b | 75.64 | 70.27 +/- 1.53 | 76.45 +/- 4.24 | 75.15 +/-
    1.17 |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.64 | 70.27 +/- 1.53 | 76.45 +/- 4.24 | 75.15 +/-
    1.17 |'
- en: '| text-ada-001-unknown | 57.36 | 49.24 +/- 3.32 | 61.96 +/- 5.14 | 58.23 +/-
    1.53 |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 57.36 | 49.24 +/- 3.32 | 61.96 +/- 5.14 | 58.23 +/-
    1.53 |'
- en: '| text-babbage-001-unknown | 63.53 | 56.63 +/- 2.22 | 65.22 +/- 5.47 | 63.35
    +/- 1.49 |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 63.53 | 56.63 +/- 2.22 | 65.22 +/- 5.47 | 63.35
    +/- 1.49 |'
- en: '| text-curie-001-unknown | 70.17 | 62.69 +/- 2.12 | 67.75 +/- 7.36 | 71.32
    +/- 1.01 |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 70.17 | 62.69 +/- 2.12 | 67.75 +/- 7.36 | 71.32
    +/- 1.01 |'
- en: '| text-davinci-001-unknown | 74.97 | 63.83 +/- 1.21 | 80.80 +/- 1.95 | 75.41
    +/- 1.79 |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 74.97 | 63.83 +/- 1.21 | 80.80 +/- 1.95 | 75.41
    +/- 1.79 |'
- en: '| text-davinci-002-unknown | 79.56 | 70.08 +/- 1.56 | 84.78 +/- 2.51 | 79.59
    +/- 2.79 |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.56 | 70.08 +/- 1.56 | 84.78 +/- 2.51 | 79.59
    +/- 2.79 |'
- en: '| text-davinci-003-unknown | 79.00 | 68.18 +/- 1.31 | 87.32 +/- 1.49 | 79.07
    +/- 1.38 |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.00 | 68.18 +/- 1.31 | 87.32 +/- 1.49 | 79.07
    +/- 1.38 |'
- en: '| ChatGPT-unknown | 74.28 | 68.37 +/- 4.37 | 75.36 +/- 11.06 | 73.90 +/- 4.82
    |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 74.28 | 68.37 +/- 4.37 | 75.36 +/- 11.06 | 73.90 +/- 4.82
    |'
- en: '| GPT-4-unknown | 81.31 | 70.83 +/- 4.29 | 86.96 +/- 2.81 | 81.31 +/- 3.82
    |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 81.31 | 70.83 +/- 4.29 | 86.96 +/- 2.81 | 81.31 +/- 3.82
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 29: Accuracy per label for 15-shot evaluation.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 表 29：15次测试的标签准确率。
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 51.86 | 44.93 +/- 4.10 | 53.41 +/- 8.48 | 43.94 +/- 19.93 |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 51.86 | 44.93 +/- 4.10 | 53.41 +/- 8.48 | 43.94 +/- 19.93 |'
- en: '| OPT-350m | 55.42 | 48.55 +/- 2.99 | 48.48 +/- 2.51 | 42.42 +/- 6.78 |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 55.42 | 48.55 +/- 2.99 | 48.48 +/- 2.51 | 42.42 +/- 6.78 |'
- en: '| OPT-1.3b | 61.61 | 64.49 +/- 5.28 | 68.94 +/- 4.67 | 42.42 +/- 6.78 |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 61.61 | 64.49 +/- 5.28 | 68.94 +/- 4.67 | 42.42 +/- 6.78 |'
- en: '| OPT-2.7b | 59.53 | 55.80 +/- 5.84 | 62.50 +/- 2.18 | 60.61 +/- 4.29 |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.53 | 55.80 +/- 5.84 | 62.50 +/- 2.18 | 60.61 +/- 4.29 |'
- en: '| OPT-6.7b | 64.72 | 55.80 +/- 7.28 | 68.18 +/- 3.47 | 60.61 +/- 16.32 |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 64.72 | 55.80 +/- 7.28 | 68.18 +/- 3.47 | 60.61 +/- 16.32 |'
- en: '| OPT-13b | 65.17 | 64.49 +/- 6.36 | 66.67 +/- 6.11 | 54.55 +/- 5.25 |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 65.17 | 64.49 +/- 6.36 | 66.67 +/- 6.11 | 54.55 +/- 5.25 |'
- en: '| OPT-30b | 64.06 | 68.84 +/- 4.64 | 60.23 +/- 5.98 | 43.94 +/- 8.16 |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 64.06 | 68.84 +/- 4.64 | 60.23 +/- 5.98 | 43.94 +/- 8.16 |'
- en: '| OPT-66b | 61.83 | 65.94 +/- 11.34 | 55.30 +/- 8.26 | 39.39 +/- 4.29 |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.83 | 65.94 +/- 11.34 | 55.30 +/- 8.26 | 39.39 +/- 4.29 |'
- en: '| OPT-175b | 64.78 | 76.09 +/- 11.16 | 67.05 +/- 9.44 | 50.00 +/- 6.94 |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 64.78 | 76.09 +/- 11.16 | 67.05 +/- 9.44 | 50.00 +/- 6.94 |'
- en: '| BLOOM-560m | 55.00 | 47.83 +/- 2.51 | 59.09 +/- 2.27 | 62.12 +/- 3.39 |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 55.00 | 47.83 +/- 2.51 | 59.09 +/- 2.27 | 62.12 +/- 3.39 |'
- en: '| BLOOM-1b1 | 57.58 | 50.00 +/- 4.86 | 53.03 +/- 2.51 | 57.58 +/- 4.29 |'
  id: totrans-1022
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.58 | 50.00 +/- 4.86 | 53.03 +/- 2.51 | 57.58 +/- 4.29 |'
- en: '| BLOOM-1b7 | 55.14 | 60.14 +/- 12.40 | 50.38 +/- 4.98 | 53.03 +/- 16.94 |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 55.14 | 60.14 +/- 12.40 | 50.38 +/- 4.98 | 53.03 +/- 16.94 |'
- en: '| BLOOM-3b | 58.69 | 44.93 +/- 3.24 | 61.36 +/- 6.94 | 57.58 +/- 6.78 |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 58.69 | 44.93 +/- 3.24 | 61.36 +/- 6.94 | 57.58 +/- 6.78 |'
- en: '| BLOOM-7b1 | 55.67 | 55.07 +/- 7.80 | 61.36 +/- 2.93 | 56.06 +/- 8.16 |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 55.67 | 55.07 +/- 7.80 | 61.36 +/- 2.93 | 56.06 +/- 8.16 |'
- en: '| BLOOM-176b | 61.89 | 77.54 +/- 9.86 | 70.08 +/- 7.59 | 37.88 +/- 3.39 |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.89 | 77.54 +/- 9.86 | 70.08 +/- 7.59 | 37.88 +/- 3.39 |'
- en: '| EleutherAI-125m | 56.03 | 60.14 +/- 7.70 | 42.80 +/- 4.62 | 59.09 +/- 13.64
    |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 56.03 | 60.14 +/- 7.70 | 42.80 +/- 4.62 | 59.09 +/- 13.64
    |'
- en: '| EleutherAI-1.3b | 57.44 | 49.28 +/- 2.05 | 51.52 +/- 6.11 | 39.39 +/- 15.45
    |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.44 | 49.28 +/- 2.05 | 51.52 +/- 6.11 | 39.39 +/- 15.45
    |'
- en: '| EleutherAI-2.7b | 58.08 | 53.62 +/- 4.81 | 57.20 +/- 4.81 | 56.06 +/- 11.03
    |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 58.08 | 53.62 +/- 4.81 | 57.20 +/- 4.81 | 56.06 +/- 11.03
    |'
- en: '| EleutherAI-6b | 58.81 | 58.70 +/- 10.87 | 56.06 +/- 10.47 | 56.06 +/- 6.25
    |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.81 | 58.70 +/- 10.87 | 56.06 +/- 10.47 | 56.06 +/- 6.25
    |'
- en: '| EleutherAI-20b | 59.86 | 55.80 +/- 2.99 | 63.64 +/- 9.19 | 42.42 +/- 4.29
    |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 59.86 | 55.80 +/- 2.99 | 63.64 +/- 9.19 | 42.42 +/- 4.29
    |'
- en: '| Cohere-409m | 55.19 | 50.00 +/- 4.16 | 50.76 +/- 4.85 | 42.42 +/- 6.78 |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 55.19 | 50.00 +/- 4.16 | 50.76 +/- 4.85 | 42.42 +/- 6.78 |'
- en: '| Cohere-6b | 60.44 | 65.94 +/- 9.19 | 67.05 +/- 9.88 | 50.00 +/- 17.99 |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.44 | 65.94 +/- 9.19 | 67.05 +/- 9.88 | 50.00 +/- 17.99 |'
- en: '| Cohere-13b | 62.83 | 67.39 +/- 13.92 | 64.77 +/- 5.53 | 43.94 +/- 9.70 |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 62.83 | 67.39 +/- 13.92 | 64.77 +/- 5.53 | 43.94 +/- 9.70 |'
- en: '| Cohere-52b | 64.72 | 63.04 +/- 6.52 | 69.32 +/- 7.28 | 63.64 +/- 13.89 |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 64.72 | 63.04 +/- 6.52 | 69.32 +/- 7.28 | 63.64 +/- 13.89 |'
- en: '| GPT-3-350m | 58.83 | 50.00 +/- 7.43 | 53.41 +/- 1.74 | 42.42 +/- 13.55 |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 58.83 | 50.00 +/- 7.43 | 53.41 +/- 1.74 | 42.42 +/- 13.55 |'
- en: '| GPT-3-1.3b | 62.86 | 53.62 +/- 7.39 | 65.91 +/- 3.71 | 50.00 +/- 14.61 |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 62.86 | 53.62 +/- 7.39 | 65.91 +/- 3.71 | 50.00 +/- 14.61 |'
- en: '| GPT-3-6.7b | 65.17 | 62.32 +/- 6.95 | 63.64 +/- 2.27 | 51.52 +/- 10.05 |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 65.17 | 62.32 +/- 6.95 | 63.64 +/- 2.27 | 51.52 +/- 10.05 |'
- en: '| GPT-3-175b | 68.31 | 78.26 +/- 5.02 | 66.67 +/- 4.48 | 56.06 +/- 11.03 |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.31 | 78.26 +/- 5.02 | 66.67 +/- 4.48 | 56.06 +/- 11.03 |'
- en: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 46.81 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.81 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 46.56 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 34.85 +/- 3.39
    |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.56 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 34.85 +/- 3.39
    |'
- en: '| BlenderBot-3b | 53.14 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.14 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 53.19 | 45.65 +/- 3.32 | 60.61 +/- 5.03 | 65.15 +/- 3.39
    |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.19 | 45.65 +/- 3.32 | 60.61 +/- 5.03 | 65.15 +/- 3.39
    |'
- en: '| Flan-T5-780m | 61.50 | 65.94 +/- 5.84 | 67.42 +/- 10.71 | 42.42 +/- 4.29
    |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.50 | 65.94 +/- 5.84 | 67.42 +/- 10.71 | 42.42 +/- 4.29
    |'
- en: '| Flan-T5-3b | 55.08 | 66.67 +/- 10.55 | 60.23 +/- 11.64 | 36.36 +/- 0.00 |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 55.08 | 66.67 +/- 10.55 | 60.23 +/- 11.64 | 36.36 +/- 0.00 |'
- en: '| Flan-T5-11b | 60.83 | 65.94 +/- 7.28 | 68.56 +/- 8.65 | 45.45 +/- 7.42 |'
  id: totrans-1047
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.83 | 65.94 +/- 7.28 | 68.56 +/- 8.65 | 45.45 +/- 7.42 |'
- en: '| Cohere-command-6b | 70.03 | 80.43 +/- 3.32 | 78.41 +/- 2.18 | 45.45 +/- 10.50
    |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 70.03 | 80.43 +/- 3.32 | 78.41 +/- 2.18 | 45.45 +/- 10.50
    |'
- en: '| Cohere-command-52b | 75.39 | 89.13 +/- 2.17 | 83.33 +/- 1.69 | 72.73 +/-
    5.25 |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.39 | 89.13 +/- 2.17 | 83.33 +/- 1.69 | 72.73 +/-
    5.25 |'
- en: '| text-ada-001-unknown | 58.28 | 55.07 +/- 5.98 | 56.06 +/- 5.67 | 63.64 +/-
    13.89 |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 58.28 | 55.07 +/- 5.98 | 56.06 +/- 5.67 | 63.64 +/-
    13.89 |'
- en: '| text-babbage-001-unknown | 65.19 | 63.04 +/- 4.86 | 77.27 +/- 3.71 | 68.18
    +/- 6.94 |'
  id: totrans-1051
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 65.19 | 63.04 +/- 4.86 | 77.27 +/- 3.71 | 68.18
    +/- 6.94 |'
- en: '| text-curie-001-unknown | 69.92 | 79.71 +/- 2.05 | 73.11 +/- 1.56 | 45.45
    +/- 10.50 |'
  id: totrans-1052
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 69.92 | 79.71 +/- 2.05 | 73.11 +/- 1.56 | 45.45
    +/- 10.50 |'
- en: '| text-davinci-001-unknown | 75.31 | 88.41 +/- 2.05 | 82.95 +/- 2.86 | 57.58
    +/- 8.57 |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 75.31 | 88.41 +/- 2.05 | 82.95 +/- 2.86 | 57.58
    +/- 8.57 |'
- en: '| text-davinci-002-unknown | 79.06 | 94.93 +/- 1.62 | 85.23 +/- 2.86 | 72.73
    +/- 15.75 |'
  id: totrans-1054
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.06 | 94.93 +/- 1.62 | 85.23 +/- 2.86 | 72.73
    +/- 15.75 |'
- en: '| text-davinci-003-unknown | 79.03 | 91.30 +/- 0.00 | 85.61 +/- 1.69 | 69.70
    +/- 15.45 |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.03 | 91.30 +/- 0.00 | 85.61 +/- 1.69 | 69.70
    +/- 15.45 |'
- en: '| ChatGPT-unknown | 75.56 | 86.23 +/- 4.64 | 86.74 +/- 4.03 | 60.61 +/- 10.05
    |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.56 | 86.23 +/- 4.64 | 86.74 +/- 4.03 | 60.61 +/- 10.05
    |'
- en: '| GPT-4-unknown | 82.08 | 95.65 +/- 2.51 | 81.44 +/- 2.43 | 90.91 +/- 0.00
    |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.08 | 95.65 +/- 2.51 | 81.44 +/- 2.43 | 90.91 +/- 0.00
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 30: Accuracy per label for 15-shot evaluation.'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: '表 30: 15-shot 评估的每个标签的准确性。'
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均值 | 特定值 | 泛化值 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 51.86 | 50.95 +/- 4.32 | 55.80 +/- 20.34 | 51.94 +/- 0.84 |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 51.86 | 50.95 +/- 4.32 | 55.80 +/- 20.34 | 51.94 +/- 0.84 |'
- en: '| OPT-350m | 55.42 | 55.30 +/- 3.39 | 60.51 +/- 4.24 | 56.29 +/- 0.75 |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 55.42 | 55.30 +/- 3.39 | 60.51 +/- 4.24 | 56.29 +/- 0.75 |'
- en: '| OPT-1.3b | 61.61 | 57.39 +/- 5.90 | 63.77 +/- 6.60 | 61.76 +/- 3.50 |'
  id: totrans-1064
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 61.61 | 57.39 +/- 5.90 | 63.77 +/- 6.60 | 61.76 +/- 3.50 |'
- en: '| OPT-2.7b | 59.53 | 49.24 +/- 2.98 | 75.00 +/- 1.66 | 59.78 +/- 5.03 |'
  id: totrans-1065
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.53 | 49.24 +/- 2.98 | 75.00 +/- 1.66 | 59.78 +/- 5.03 |'
- en: '| OPT-6.7b | 64.72 | 58.71 +/- 7.78 | 74.28 +/- 5.24 | 65.12 +/- 2.69 |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 64.72 | 58.71 +/- 7.78 | 74.28 +/- 5.24 | 65.12 +/- 2.69 |'
- en: '| OPT-13b | 65.17 | 64.02 +/- 2.34 | 65.22 +/- 7.32 | 65.50 +/- 1.25 |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 65.17 | 64.02 +/- 2.34 | 65.22 +/- 7.32 | 65.50 +/- 1.25 |'
- en: '| OPT-30b | 64.06 | 62.50 +/- 3.99 | 61.59 +/- 6.84 | 65.42 +/- 3.99 |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 64.06 | 62.50 +/- 3.99 | 61.59 +/- 6.84 | 65.42 +/- 3.99 |'
- en: '| OPT-66b | 61.83 | 58.71 +/- 7.21 | 57.61 +/- 5.14 | 64.25 +/- 3.20 |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 61.83 | 58.71 +/- 7.21 | 57.61 +/- 5.14 | 64.25 +/- 3.20 |'
- en: '| OPT-175b | 64.78 | 64.20 +/- 4.03 | 59.78 +/- 3.92 | 64.90 +/- 3.66 |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 64.78 | 64.20 +/- 4.03 | 59.78 +/- 3.92 | 64.90 +/- 3.66 |'
- en: '| BLOOM-560m | 55.00 | 44.13 +/- 2.58 | 74.64 +/- 4.64 | 54.78 +/- 1.75 |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 55.00 | 44.13 +/- 2.58 | 74.64 +/- 4.64 | 54.78 +/- 1.75 |'
- en: '| BLOOM-1b1 | 57.58 | 45.45 +/- 1.74 | 64.13 +/- 5.58 | 60.42 +/- 2.34 |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 57.58 | 45.45 +/- 1.74 | 64.13 +/- 5.58 | 60.42 +/- 2.34 |'
- en: '| BLOOM-1b7 | 55.14 | 52.27 +/- 5.21 | 57.61 +/- 8.40 | 55.73 +/- 0.84 |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 55.14 | 52.27 +/- 5.21 | 57.61 +/- 8.40 | 55.73 +/- 0.84 |'
- en: '| BLOOM-3b | 58.69 | 49.81 +/- 1.21 | 74.28 +/- 2.92 | 59.30 +/- 0.95 |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 58.69 | 49.81 +/- 1.21 | 74.28 +/- 2.92 | 59.30 +/- 0.95 |'
- en: '| BLOOM-7b1 | 55.67 | 48.67 +/- 3.74 | 70.29 +/- 3.90 | 54.78 +/- 3.23 |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 55.67 | 48.67 +/- 3.74 | 70.29 +/- 3.90 | 54.78 +/- 3.23 |'
- en: '| BLOOM-176b | 61.89 | 64.02 +/- 4.43 | 46.38 +/- 11.20 | 62.02 +/- 4.53 |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.89 | 64.02 +/- 4.43 | 46.38 +/- 11.20 | 62.02 +/- 4.53 |'
- en: '| EleutherAI-125m | 56.03 | 56.82 +/- 3.47 | 54.35 +/- 5.89 | 57.11 +/- 0.65
    |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 56.03 | 56.82 +/- 3.47 | 54.35 +/- 5.89 | 57.11 +/- 0.65
    |'
- en: '| EleutherAI-1.3b | 57.44 | 51.14 +/- 3.21 | 61.59 +/- 9.11 | 59.95 +/- 2.44
    |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 57.44 | 51.14 +/- 3.21 | 61.59 +/- 9.11 | 59.95 +/- 2.44
    |'
- en: '| EleutherAI-2.7b | 58.08 | 57.58 +/- 2.24 | 61.96 +/- 6.49 | 58.27 +/- 1.50
    |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 58.08 | 57.58 +/- 2.24 | 61.96 +/- 6.49 | 58.27 +/- 1.50
    |'
- en: '| EleutherAI-6b | 58.81 | 54.17 +/- 6.07 | 66.67 +/- 8.48 | 59.35 +/- 4.11
    |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.81 | 54.17 +/- 6.07 | 66.67 +/- 8.48 | 59.35 +/- 4.11
    |'
- en: '| EleutherAI-20b | 59.86 | 55.11 +/- 3.98 | 62.68 +/- 11.87 | 60.85 +/- 4.53
    |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 59.86 | 55.11 +/- 3.98 | 62.68 +/- 11.87 | 60.85 +/- 4.53
    |'
- en: '| Cohere-409m | 55.19 | 52.65 +/- 1.69 | 61.23 +/- 8.08 | 56.12 +/- 2.19 |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 55.19 | 52.65 +/- 1.69 | 61.23 +/- 8.08 | 56.12 +/- 2.19 |'
- en: '| Cohere-6b | 60.44 | 49.62 +/- 2.60 | 71.01 +/- 6.48 | 60.85 +/- 4.38 |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.44 | 49.62 +/- 2.60 | 71.01 +/- 6.48 | 60.85 +/- 4.38 |'
- en: '| Cohere-13b | 62.83 | 57.77 +/- 3.17 | 71.01 +/- 5.98 | 63.09 +/- 2.62 |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 62.83 | 57.77 +/- 3.17 | 71.01 +/- 5.98 | 63.09 +/- 2.62 |'
- en: '| Cohere-52b | 64.72 | 57.39 +/- 2.69 | 71.01 +/- 4.81 | 65.16 +/- 1.07 |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 64.72 | 57.39 +/- 2.69 | 71.01 +/- 4.81 | 65.16 +/- 1.07 |'
- en: '| GPT-3-350m | 58.83 | 55.68 +/- 2.54 | 65.22 +/- 8.96 | 60.29 +/- 1.99 |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 58.83 | 55.68 +/- 2.54 | 65.22 +/- 8.96 | 60.29 +/- 1.99 |'
- en: '| GPT-3-1.3b | 62.86 | 56.06 +/- 5.67 | 63.04 +/- 7.63 | 64.86 +/- 1.78 |'
  id: totrans-1087
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 62.86 | 56.06 +/- 5.67 | 63.04 +/- 7.63 | 64.86 +/- 1.78 |'
- en: '| GPT-3-6.7b | 65.17 | 58.33 +/- 4.62 | 73.55 +/- 7.88 | 66.37 +/- 2.47 |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 65.17 | 58.33 +/- 4.62 | 73.55 +/- 7.88 | 66.37 +/- 2.47 |'
- en: '| GPT-3-175b | 68.31 | 64.77 +/- 4.10 | 71.38 +/- 6.33 | 68.60 +/- 3.97 |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.31 | 64.77 +/- 4.10 | 71.38 +/- 6.33 | 68.60 +/- 3.97 |'
- en: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00 |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00 |'
- en: '| T0-11b | 46.81 | 55.68 +/- 0.00 | 25.00 +/- 1.09 | 48.41 +/- 0.12 |'
  id: totrans-1091
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.81 | 55.68 +/- 0.00 | 25.00 +/- 1.09 | 48.41 +/- 0.12 |'
- en: '| BlenderBot-90m | 46.56 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.19 +/- 0.13
    |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.56 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.19 +/- 0.13
    |'
- en: '| BlenderBot-3b | 53.14 | 44.32 +/- 0.00 | 75.36 +/- 1.02 | 51.46 +/- 0.23
    |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.14 | 44.32 +/- 0.00 | 75.36 +/- 1.02 | 51.46 +/- 0.23
    |'
- en: '| BlenderBot-9b | 53.19 | 44.13 +/- 1.79 | 75.72 +/- 1.49 | 51.72 +/- 0.74
    |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.19 | 44.13 +/- 1.79 | 75.72 +/- 1.49 | 51.72 +/- 0.74
    |'
- en: '| Flan-T5-780m | 61.50 | 56.63 +/- 2.22 | 71.74 +/- 11.30 | 60.90 +/- 4.55
    |'
  id: totrans-1095
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.50 | 56.63 +/- 2.22 | 71.74 +/- 11.30 | 60.90 +/- 4.55
    |'
- en: '| Flan-T5-3b | 55.08 | 56.82 +/- 1.31 | 45.65 +/- 12.92 | 55.04 +/- 3.39 |'
  id: totrans-1096
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 55.08 | 56.82 +/- 1.31 | 45.65 +/- 12.92 | 55.04 +/- 3.39 |'
- en: '| Flan-T5-11b | 60.83 | 57.01 +/- 3.37 | 60.14 +/- 15.81 | 61.02 +/- 5.34 |'
  id: totrans-1097
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 60.83 | 57.01 +/- 3.37 | 60.14 +/- 15.81 | 61.02 +/- 5.34 |'
- en: '| Cohere-command-6b | 70.03 | 60.80 +/- 4.72 | 72.83 +/- 8.30 | 70.84 +/- 1.68
    |'
  id: totrans-1098
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 70.03 | 60.80 +/- 4.72 | 72.83 +/- 8.30 | 70.84 +/- 1.68
    |'
- en: '| Cohere-command-52b | 75.39 | 69.89 +/- 2.43 | 76.81 +/- 3.90 | 74.76 +/-
    1.01 |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.39 | 69.89 +/- 2.43 | 76.81 +/- 3.90 | 74.76 +/-
    1.01 |'
- en: '| text-ada-001-unknown | 58.28 | 52.08 +/- 3.74 | 69.20 +/- 3.42 | 58.57 +/-
    2.04 |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 58.28 | 52.08 +/- 3.74 | 69.20 +/- 3.42 | 58.57 +/-
    2.04 |'
- en: '| text-babbage-001-unknown | 65.19 | 58.33 +/- 2.43 | 67.03 +/- 3.42 | 65.12
    +/- 2.40 |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 65.19 | 58.33 +/- 2.43 | 67.03 +/- 3.42 | 65.12
    +/- 2.40 |'
- en: '| text-curie-001-unknown | 69.92 | 62.50 +/- 1.47 | 68.84 +/- 6.72 | 71.40
    +/- 0.93 |'
  id: totrans-1102
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 69.92 | 62.50 +/- 1.47 | 68.84 +/- 6.72 | 71.40
    +/- 0.93 |'
- en: '| text-davinci-001-unknown | 75.31 | 64.58 +/- 2.12 | 83.70 +/- 1.66 | 75.54
    +/- 0.95 |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 75.31 | 64.58 +/- 2.12 | 83.70 +/- 1.66 | 75.54
    +/- 0.95 |'
- en: '| text-davinci-002-unknown | 79.06 | 72.92 +/- 1.02 | 86.96 +/- 3.97 | 77.99
    +/- 2.77 |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 79.06 | 72.92 +/- 1.02 | 86.96 +/- 3.97 | 77.99
    +/- 2.77 |'
- en: '| text-davinci-003-unknown | 79.03 | 69.32 +/- 2.37 | 87.68 +/- 1.62 | 78.94
    +/- 1.12 |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.03 | 69.32 +/- 2.37 | 87.68 +/- 1.62 | 78.94
    +/- 1.12 |'
- en: '| ChatGPT-unknown | 75.56 | 72.16 +/- 4.81 | 77.54 +/- 7.90 | 74.63 +/- 4.41
    |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.56 | 72.16 +/- 4.81 | 77.54 +/- 7.90 | 74.63 +/- 4.41
    |'
- en: '| GPT-4-unknown | 82.08 | 72.92 +/- 1.02 | 86.23 +/- 2.99 | 82.69 +/- 3.87
    |'
  id: totrans-1107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.08 | 72.92 +/- 1.02 | 86.23 +/- 2.99 | 82.69 +/- 3.87
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 31: Accuracy per label for 30-shot evaluation.'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 31：30-shot 评估的每个标签的准确性。
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 51.50 | 55.80 +/- 5.28 | 54.55 +/- 9.28 | 54.55 +/- 9.09 |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 51.50 | 55.80 +/- 5.28 | 54.55 +/- 9.28 | 54.55 +/- 9.09 |'
- en: '| OPT-350m | 54.61 | 49.28 +/- 7.39 | 56.82 +/- 1.86 | 37.88 +/- 9.70 |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 54.61 | 49.28 +/- 7.39 | 56.82 +/- 1.86 | 37.88 +/- 9.70 |'
- en: '| OPT-1.3b | 61.67 | 71.01 +/- 3.24 | 67.80 +/- 5.48 | 28.79 +/- 11.03 |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 61.67 | 71.01 +/- 3.24 | 67.80 +/- 5.48 | 28.79 +/- 11.03 |'
- en: '| OPT-2.7b | 59.86 | 58.70 +/- 10.87 | 71.21 +/- 8.37 | 46.97 +/- 11.03 |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.86 | 58.70 +/- 10.87 | 71.21 +/- 8.37 | 46.97 +/- 11.03 |'
- en: '| OPT-6.7b | 63.61 | 62.32 +/- 11.13 | 67.05 +/- 2.86 | 46.97 +/- 19.93 |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.61 | 62.32 +/- 11.13 | 67.05 +/- 2.86 | 46.97 +/- 19.93 |'
- en: '| OPT-13b | 63.39 | 60.14 +/- 5.28 | 60.98 +/- 5.93 | 46.97 +/- 6.25 |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 63.39 | 60.14 +/- 5.28 | 60.98 +/- 5.93 | 46.97 +/- 6.25 |'
- en: '| OPT-30b | 65.47 | 71.74 +/- 8.23 | 62.88 +/- 7.38 | 37.88 +/- 3.39 |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 65.47 | 71.74 +/- 8.23 | 62.88 +/- 7.38 | 37.88 +/- 3.39 |'
- en: '| OPT-66b | 60.83 | 60.14 +/- 3.90 | 51.52 +/- 11.79 | 43.94 +/- 6.25 |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 60.83 | 60.14 +/- 3.90 | 51.52 +/- 11.79 | 43.94 +/- 6.25 |'
- en: '| OPT-175b | 62.44 | 65.94 +/- 10.77 | 62.50 +/- 13.43 | 60.61 +/- 4.29 |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 62.44 | 65.94 +/- 10.77 | 62.50 +/- 13.43 | 60.61 +/- 4.29 |'
- en: '| BLOOM-560m | 55.00 | 47.10 +/- 1.62 | 60.98 +/- 2.43 | 62.12 +/- 3.39 |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 55.00 | 47.10 +/- 1.62 | 60.98 +/- 2.43 | 62.12 +/- 3.39 |'
- en: '| BLOOM-1b1 | 56.89 | 49.28 +/- 3.24 | 54.92 +/- 8.65 | 46.97 +/- 8.16 |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 56.89 | 49.28 +/- 3.24 | 54.92 +/- 8.65 | 46.97 +/- 8.16 |'
- en: '| BLOOM-1b7 | 52.28 | 52.90 +/- 7.28 | 47.35 +/- 9.32 | 36.36 +/- 16.60 |'
  id: totrans-1123
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 52.28 | 52.90 +/- 7.28 | 47.35 +/- 9.32 | 36.36 +/- 16.60 |'
- en: '| BLOOM-3b | 58.64 | 50.72 +/- 2.05 | 62.50 +/- 5.68 | 59.09 +/- 6.94 |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 58.64 | 50.72 +/- 2.05 | 62.50 +/- 5.68 | 59.09 +/- 6.94 |'
- en: '| BLOOM-7b1 | 57.61 | 50.72 +/- 5.98 | 61.74 +/- 3.81 | 54.55 +/- 9.09 |'
  id: totrans-1125
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 57.61 | 50.72 +/- 5.98 | 61.74 +/- 3.81 | 54.55 +/- 9.09 |'
- en: '| BLOOM-176b | 61.06 | 73.19 +/- 8.85 | 66.29 +/- 9.41 | 48.48 +/- 4.29 |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.06 | 73.19 +/- 8.85 | 66.29 +/- 9.41 | 48.48 +/- 4.29 |'
- en: '| EleutherAI-125m | 53.44 | 47.10 +/- 4.64 | 47.73 +/- 6.43 | 39.39 +/- 15.45
    |'
  id: totrans-1127
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 53.44 | 47.10 +/- 4.64 | 47.73 +/- 6.43 | 39.39 +/- 15.45
    |'
- en: '| EleutherAI-1.3b | 55.97 | 44.93 +/- 4.81 | 51.89 +/- 6.74 | 37.88 +/- 6.25
    |'
  id: totrans-1128
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 55.97 | 44.93 +/- 4.81 | 51.89 +/- 6.74 | 37.88 +/- 6.25
    |'
- en: '| EleutherAI-2.7b | 57.36 | 62.32 +/- 5.98 | 53.41 +/- 2.86 | 37.88 +/- 11.03
    |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 57.36 | 62.32 +/- 5.98 | 53.41 +/- 2.86 | 37.88 +/- 11.03
    |'
- en: '| EleutherAI-6b | 58.75 | 59.42 +/- 12.21 | 52.27 +/- 14.43 | 36.36 +/- 5.25
    |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.75 | 59.42 +/- 12.21 | 52.27 +/- 14.43 | 36.36 +/- 5.25
    |'
- en: '| EleutherAI-20b | 57.36 | 57.97 +/- 5.42 | 60.61 +/- 10.30 | 31.82 +/- 8.70
    |'
  id: totrans-1131
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 57.36 | 57.97 +/- 5.42 | 60.61 +/- 10.30 | 31.82 +/- 8.70
    |'
- en: '| Cohere-409m | 57.17 | 47.83 +/- 3.55 | 53.41 +/- 3.15 | 53.03 +/- 3.39 |'
  id: totrans-1132
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 57.17 | 47.83 +/- 3.55 | 53.41 +/- 3.15 | 53.03 +/- 3.39 |'
- en: '| Cohere-6b | 60.36 | 58.70 +/- 13.92 | 62.50 +/- 10.23 | 54.55 +/- 10.50 |'
  id: totrans-1133
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.36 | 58.70 +/- 13.92 | 62.50 +/- 10.23 | 54.55 +/- 10.50 |'
- en: '| Cohere-13b | 64.81 | 70.29 +/- 21.65 | 65.91 +/- 7.07 | 45.45 +/- 5.25 |'
  id: totrans-1134
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 64.81 | 70.29 +/- 21.65 | 65.91 +/- 7.07 | 45.45 +/- 5.25 |'
- en: '| Cohere-52b | 65.72 | 67.39 +/- 8.60 | 66.29 +/- 1.56 | 53.03 +/- 11.03 |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 65.72 | 67.39 +/- 8.60 | 66.29 +/- 1.56 | 53.03 +/- 11.03 |'
- en: '| GPT-3-350m | 60.25 | 55.07 +/- 2.05 | 57.95 +/- 5.83 | 51.52 +/- 10.05 |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 60.25 | 55.07 +/- 2.05 | 57.95 +/- 5.83 | 51.52 +/- 10.05 |'
- en: '| GPT-3-1.3b | 60.19 | 61.59 +/- 3.90 | 54.92 +/- 6.99 | 43.94 +/- 9.70 |'
  id: totrans-1137
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.19 | 61.59 +/- 3.90 | 54.92 +/- 6.99 | 43.94 +/- 9.70 |'
- en: '| GPT-3-6.7b | 62.86 | 56.52 +/- 4.35 | 65.53 +/- 3.32 | 50.00 +/- 6.94 |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 62.86 | 56.52 +/- 4.35 | 65.53 +/- 3.32 | 50.00 +/- 6.94 |'
- en: '| GPT-3-175b | 68.31 | 67.39 +/- 5.47 | 72.73 +/- 2.62 | 75.76 +/- 4.29 |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.31 | 67.39 +/- 5.47 | 72.73 +/- 2.62 | 75.76 +/- 4.29 |'
- en: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-1140
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| T0-11b | 46.75 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
  id: totrans-1141
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.75 | 52.17 +/- 0.00 | 38.64 +/- 0.00 | 36.36 +/- 0.00 |'
- en: '| BlenderBot-90m | 46.67 | 51.45 +/- 1.62 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
  id: totrans-1142
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.67 | 51.45 +/- 1.62 | 38.64 +/- 0.00 | 36.36 +/- 0.00
    |'
- en: '| BlenderBot-3b | 53.25 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
  id: totrans-1143
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.25 | 47.83 +/- 0.00 | 61.36 +/- 0.00 | 63.64 +/- 0.00
    |'
- en: '| BlenderBot-9b | 53.72 | 46.38 +/- 4.10 | 63.26 +/- 3.32 | 63.64 +/- 0.00
    |'
  id: totrans-1144
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.72 | 46.38 +/- 4.10 | 63.26 +/- 3.32 | 63.64 +/- 0.00
    |'
- en: '| Flan-T5-780m | 61.50 | 67.39 +/- 8.60 | 70.83 +/- 6.35 | 42.42 +/- 4.29 |'
  id: totrans-1145
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.50 | 67.39 +/- 8.60 | 70.83 +/- 6.35 | 42.42 +/- 4.29 |'
- en: '| Flan-T5-3b | 56.11 | 65.22 +/- 7.10 | 62.50 +/- 13.04 | 36.36 +/- 0.00 |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 56.11 | 65.22 +/- 7.10 | 62.50 +/- 13.04 | 36.36 +/- 0.00 |'
- en: '| Flan-T5-11b | 62.11 | 67.39 +/- 10.27 | 72.73 +/- 10.66 | 51.52 +/- 15.45
    |'
  id: totrans-1147
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 62.11 | 67.39 +/- 10.27 | 72.73 +/- 10.66 | 51.52 +/- 15.45
    |'
- en: '| Cohere-command-6b | 70.44 | 81.16 +/- 3.24 | 78.03 +/- 2.83 | 46.97 +/- 8.16
    |'
  id: totrans-1148
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 70.44 | 81.16 +/- 3.24 | 78.03 +/- 2.83 | 46.97 +/- 8.16
    |'
- en: '| Cohere-command-52b | 75.00 | 85.51 +/- 2.05 | 78.41 +/- 1.14 | 78.79 +/-
    6.78 |'
  id: totrans-1149
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.00 | 85.51 +/- 2.05 | 78.41 +/- 1.14 | 78.79 +/-
    6.78 |'
- en: '| text-ada-001-unknown | 55.58 | 50.72 +/- 7.39 | 57.58 +/- 4.29 | 57.58 +/-
    8.57 |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 55.58 | 50.72 +/- 7.39 | 57.58 +/- 4.29 | 57.58 +/-
    8.57 |'
- en: '| text-babbage-001-unknown | 66.00 | 67.39 +/- 5.47 | 71.59 +/- 3.15 | 63.64
    +/- 5.25 |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 66.00 | 67.39 +/- 5.47 | 71.59 +/- 3.15 | 63.64
    +/- 5.25 |'
- en: '| text-curie-001-unknown | 70.33 | 75.36 +/- 3.24 | 76.52 +/- 5.03 | 60.61
    +/- 8.57 |'
  id: totrans-1152
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 70.33 | 75.36 +/- 3.24 | 76.52 +/- 5.03 | 60.61
    +/- 8.57 |'
- en: '| text-davinci-001-unknown | 75.83 | 85.51 +/- 2.05 | 84.09 +/- 1.86 | 65.15
    +/- 8.16 |'
  id: totrans-1153
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 75.83 | 85.51 +/- 2.05 | 84.09 +/- 1.86 | 65.15
    +/- 8.16 |'
- en: '| text-davinci-002-unknown | 80.64 | 97.83 +/- 2.17 | 87.50 +/- 2.18 | 83.33
    +/- 3.39 |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 80.64 | 97.83 +/- 2.17 | 87.50 +/- 2.18 | 83.33
    +/- 3.39 |'
- en: '| text-davinci-003-unknown | 79.53 | 94.93 +/- 1.62 | 84.85 +/- 3.39 | 81.82
    +/- 9.09 |'
  id: totrans-1155
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.53 | 94.93 +/- 1.62 | 84.85 +/- 3.39 | 81.82
    +/- 9.09 |'
- en: '| ChatGPT-unknown | 75.64 | 87.68 +/- 4.64 | 89.02 +/- 4.43 | 83.33 +/- 9.70
    |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.64 | 87.68 +/- 4.64 | 89.02 +/- 4.43 | 83.33 +/- 9.70
    |'
- en: '| GPT-4-unknown | 82.17 | 95.65 +/- 3.55 | 87.12 +/- 3.12 | 90.91 +/- 0.00
    |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.17 | 95.65 +/- 3.55 | 87.12 +/- 3.12 | 90.91 +/- 0.00
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 32: Accuracy per label for 30-shot evaluation.'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 32: Accuracy per label for 30-shot evaluation.'
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均值 | 具体化 | 泛化 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-125m | 51.50 | 50.38 +/- 4.29 | 52.17 +/- 23.95 | 50.99 +/- 2.42 |'
  id: totrans-1162
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 51.50 | 50.38 +/- 4.29 | 52.17 +/- 23.95 | 50.99 +/- 2.42 |'
- en: '| OPT-350m | 54.61 | 55.30 +/- 1.26 | 49.28 +/- 3.48 | 55.51 +/- 1.92 |'
  id: totrans-1163
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 54.61 | 55.30 +/- 1.26 | 49.28 +/- 3.48 | 55.51 +/- 1.92 |'
- en: '| OPT-1.3b | 61.67 | 56.25 +/- 2.76 | 56.16 +/- 4.05 | 63.14 +/- 5.41 |'
  id: totrans-1164
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 61.67 | 56.25 +/- 2.76 | 56.16 +/- 4.05 | 63.14 +/- 5.41 |'
- en: '| OPT-2.7b | 59.86 | 50.19 +/- 2.89 | 69.57 +/- 4.16 | 59.95 +/- 5.23 |'
  id: totrans-1165
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 59.86 | 50.19 +/- 2.89 | 69.57 +/- 4.16 | 59.95 +/- 5.23 |'
- en: '| OPT-6.7b | 63.61 | 58.90 +/- 6.48 | 72.10 +/- 8.08 | 63.74 +/- 4.30 |'
  id: totrans-1166
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b | 63.61 | 58.90 +/- 6.48 | 72.10 +/- 8.08 | 63.74 +/- 4.30 |'
- en: '| OPT-13b | 63.39 | 59.47 +/- 2.14 | 64.86 +/- 6.07 | 65.07 +/- 2.24 |'
  id: totrans-1167
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 63.39 | 59.47 +/- 2.14 | 64.86 +/- 6.07 | 65.07 +/- 2.24 |'
- en: '| OPT-30b | 65.47 | 63.64 +/- 3.01 | 63.04 +/- 7.94 | 66.93 +/- 5.52 |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 65.47 | 63.64 +/- 3.01 | 63.04 +/- 7.94 | 66.93 +/- 5.52 |'
- en: '| OPT-66b | 60.83 | 63.07 +/- 3.64 | 55.43 +/- 9.29 | 62.62 +/- 4.98 |'
  id: totrans-1169
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 60.83 | 63.07 +/- 3.64 | 55.43 +/- 9.29 | 62.62 +/- 4.98 |'
- en: '| OPT-175b | 62.44 | 64.39 +/- 2.34 | 51.09 +/- 5.72 | 63.18 +/- 2.85 |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b | 62.44 | 64.39 +/- 2.34 | 51.09 +/- 5.72 | 63.18 +/- 2.85 |'
- en: '| BLOOM-560m | 55.00 | 47.92 +/- 2.31 | 71.01 +/- 6.23 | 54.18 +/- 1.82 |'
  id: totrans-1171
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m | 55.00 | 47.92 +/- 2.31 | 71.01 +/- 6.23 | 54.18 +/- 1.82 |'
- en: '| BLOOM-1b1 | 56.89 | 49.81 +/- 4.93 | 56.88 +/- 8.46 | 59.35 +/- 3.16 |'
  id: totrans-1172
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1 | 56.89 | 49.81 +/- 4.93 | 56.88 +/- 8.46 | 59.35 +/- 3.16 |'
- en: '| BLOOM-1b7 | 52.28 | 55.49 +/- 1.79 | 40.94 +/- 8.82 | 53.83 +/- 1.25 |'
  id: totrans-1173
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b7 | 52.28 | 55.49 +/- 1.79 | 40.94 +/- 8.82 | 53.83 +/- 1.25 |'
- en: '| BLOOM-3b | 58.64 | 52.46 +/- 1.53 | 68.84 +/- 3.69 | 58.74 +/- 1.44 |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b | 58.64 | 52.46 +/- 1.53 | 68.84 +/- 3.69 | 58.74 +/- 1.44 |'
- en: '| BLOOM-7b1 | 57.61 | 54.36 +/- 8.98 | 70.29 +/- 2.05 | 56.76 +/- 4.88 |'
  id: totrans-1175
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1 | 57.61 | 54.36 +/- 8.98 | 70.29 +/- 2.05 | 56.76 +/- 4.88 |'
- en: '| BLOOM-176b | 61.06 | 67.42 +/- 1.93 | 50.00 +/- 9.05 | 60.08 +/- 4.01 |'
  id: totrans-1176
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 61.06 | 67.42 +/- 1.93 | 50.00 +/- 9.05 | 60.08 +/- 4.01 |'
- en: '| EleutherAI-125m | 53.44 | 59.47 +/- 2.43 | 46.74 +/- 4.65 | 54.18 +/- 1.21
    |'
  id: totrans-1177
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m | 53.44 | 59.47 +/- 2.43 | 46.74 +/- 4.65 | 54.18 +/- 1.21
    |'
- en: '| EleutherAI-1.3b | 55.97 | 51.70 +/- 5.20 | 56.52 +/- 10.80 | 58.40 +/- 1.35
    |'
  id: totrans-1178
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b | 55.97 | 51.70 +/- 5.20 | 56.52 +/- 10.80 | 58.40 +/- 1.35
    |'
- en: '| EleutherAI-2.7b | 57.36 | 57.39 +/- 2.25 | 51.81 +/- 3.18 | 58.79 +/- 0.89
    |'
  id: totrans-1179
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b | 57.36 | 57.39 +/- 2.25 | 51.81 +/- 3.18 | 58.79 +/- 0.89
    |'
- en: '| EleutherAI-6b | 58.75 | 57.77 +/- 4.22 | 58.33 +/- 9.92 | 60.38 +/- 4.52
    |'
  id: totrans-1180
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b | 58.75 | 57.77 +/- 4.22 | 58.33 +/- 9.92 | 60.38 +/- 4.52
    |'
- en: '| EleutherAI-20b | 57.36 | 51.52 +/- 4.08 | 59.42 +/- 14.51 | 58.79 +/- 2.39
    |'
  id: totrans-1181
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b | 57.36 | 51.52 +/- 4.08 | 59.42 +/- 14.51 | 58.79 +/- 2.39
    |'
- en: '| Cohere-409m | 57.17 | 51.14 +/- 3.99 | 64.49 +/- 8.57 | 58.66 +/- 2.15 |'
  id: totrans-1182
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-409m | 57.17 | 51.14 +/- 3.99 | 64.49 +/- 8.57 | 58.66 +/- 2.15 |'
- en: '| Cohere-6b | 60.36 | 51.52 +/- 2.51 | 70.29 +/- 6.11 | 61.20 +/- 5.15 |'
  id: totrans-1183
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-6b | 60.36 | 51.52 +/- 2.51 | 70.29 +/- 6.11 | 61.20 +/- 5.15 |'
- en: '| Cohere-13b | 64.81 | 58.14 +/- 2.96 | 68.12 +/- 8.20 | 65.98 +/- 3.57 |'
  id: totrans-1184
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-13b | 64.81 | 58.14 +/- 2.96 | 68.12 +/- 8.20 | 65.98 +/- 3.57 |'
- en: '| Cohere-52b | 65.72 | 64.96 +/- 4.17 | 69.93 +/- 2.32 | 65.50 +/- 2.01 |'
  id: totrans-1185
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-52b | 65.72 | 64.96 +/- 4.17 | 69.93 +/- 2.32 | 65.50 +/- 2.01 |'
- en: '| GPT-3-350m | 60.25 | 57.58 +/- 2.51 | 65.22 +/- 6.28 | 60.98 +/- 1.63 |'
  id: totrans-1186
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-350m | 60.25 | 57.58 +/- 2.51 | 65.22 +/- 6.28 | 60.98 +/- 1.63 |'
- en: '| GPT-3-1.3b | 60.19 | 57.77 +/- 5.66 | 57.61 +/- 9.12 | 62.06 +/- 4.02 |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-1.3b | 60.19 | 57.77 +/- 5.66 | 57.61 +/- 9.12 | 62.06 +/- 4.02 |'
- en: '| GPT-3-6.7b | 62.86 | 61.17 +/- 6.41 | 63.41 +/- 7.98 | 63.65 +/- 2.52 |'
  id: totrans-1188
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-6.7b | 62.86 | 61.17 +/- 6.41 | 63.41 +/- 7.98 | 63.65 +/- 2.52 |'
- en: '| GPT-3-175b | 68.31 | 64.58 +/- 5.06 | 70.29 +/- 6.23 | 68.17 +/- 2.01 |'
  id: totrans-1189
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-175b | 68.31 | 64.58 +/- 5.06 | 70.29 +/- 6.23 | 68.17 +/- 2.01 |'
- en: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00 |'
  id: totrans-1190
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b | 46.67 | 55.68 +/- 0.00 | 23.91 +/- 0.00 | 48.32 +/- 0.00 |'
- en: '| T0-11b | 46.75 | 55.87 +/- 0.42 | 23.91 +/- 0.00 | 48.41 +/- 0.12 |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b | 46.75 | 55.87 +/- 0.42 | 23.91 +/- 0.00 | 48.41 +/- 0.12 |'
- en: '| BlenderBot-90m | 46.67 | 55.68 +/- 0.00 | 23.55 +/- 0.81 | 48.41 +/- 0.24
    |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m | 46.67 | 55.68 +/- 0.00 | 23.55 +/- 0.81 | 48.41 +/- 0.24
    |'
- en: '| BlenderBot-3b | 53.25 | 44.32 +/- 0.00 | 76.09 +/- 0.00 | 51.55 +/- 0.20
    |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-3b | 53.25 | 44.32 +/- 0.00 | 76.09 +/- 0.00 | 51.55 +/- 0.20
    |'
- en: '| BlenderBot-9b | 53.72 | 45.08 +/- 1.82 | 75.36 +/- 1.02 | 52.11 +/- 0.93
    |'
  id: totrans-1194
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9b | 53.72 | 45.08 +/- 1.82 | 75.36 +/- 1.02 | 52.11 +/- 0.93
    |'
- en: '| Flan-T5-780m | 61.50 | 57.01 +/- 3.85 | 73.19 +/- 8.67 | 60.16 +/- 3.75 |'
  id: totrans-1195
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m | 61.50 | 57.01 +/- 3.85 | 73.19 +/- 8.67 | 60.16 +/- 3.75 |'
- en: '| Flan-T5-3b | 56.11 | 56.44 +/- 1.56 | 47.83 +/- 11.71 | 56.29 +/- 4.18 |'
  id: totrans-1196
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b | 56.11 | 56.44 +/- 1.56 | 47.83 +/- 11.71 | 56.29 +/- 4.18 |'
- en: '| Flan-T5-11b | 62.11 | 61.36 +/- 4.50 | 57.25 +/- 16.68 | 61.58 +/- 6.25 |'
  id: totrans-1197
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b | 62.11 | 61.36 +/- 4.50 | 57.25 +/- 16.68 | 61.58 +/- 6.25 |'
- en: '| Cohere-command-6b | 70.44 | 64.96 +/- 1.21 | 78.62 +/- 5.53 | 69.81 +/- 1.60
    |'
  id: totrans-1198
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 70.44 | 64.96 +/- 1.21 | 78.62 +/- 5.53 | 69.81 +/- 1.60
    |'
- en: '| Cohere-command-52b | 75.00 | 71.78 +/- 1.66 | 75.00 +/- 3.49 | 74.55 +/-
    0.49 |'
  id: totrans-1199
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.00 | 71.78 +/- 1.66 | 75.00 +/- 3.49 | 74.55 +/-
    0.49 |'
- en: '| text-ada-001-unknown | 55.58 | 53.41 +/- 1.97 | 56.52 +/- 4.35 | 55.86 +/-
    3.03 |'
  id: totrans-1200
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 55.58 | 53.41 +/- 1.97 | 56.52 +/- 4.35 | 55.86 +/-
    3.03 |'
- en: '| text-babbage-001-unknown | 66.00 | 60.80 +/- 2.69 | 62.32 +/- 6.11 | 66.88
    +/- 2.36 |'
  id: totrans-1201
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 66.00 | 60.80 +/- 2.69 | 62.32 +/- 6.11 | 66.88
    +/- 2.36 |'
- en: '| text-curie-001-unknown | 70.33 | 60.42 +/- 5.01 | 74.28 +/- 6.20 | 71.32
    +/- 1.42 |'
  id: totrans-1202
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 70.33 | 60.42 +/- 5.01 | 74.28 +/- 6.20 | 71.32
    +/- 1.42 |'
- en: '| text-davinci-001-unknown | 75.83 | 67.05 +/- 1.97 | 83.33 +/- 3.48 | 75.67
    +/- 1.02 |'
  id: totrans-1203
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 75.83 | 67.05 +/- 1.97 | 83.33 +/- 3.48 | 75.67
    +/- 1.02 |'
- en: '| text-davinci-002-unknown | 80.64 | 74.43 +/- 1.83 | 83.70 +/- 2.74 | 79.76
    +/- 1.44 |'
  id: totrans-1204
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 80.64 | 74.43 +/- 1.83 | 83.70 +/- 2.74 | 79.76
    +/- 1.44 |'
- en: '| text-davinci-003-unknown | 79.53 | 72.92 +/- 2.49 | 86.59 +/- 1.95 | 78.55
    +/- 1.20 |'
  id: totrans-1205
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 79.53 | 72.92 +/- 2.49 | 86.59 +/- 1.95 | 78.55
    +/- 1.20 |'
- en: '| ChatGPT-unknown | 75.64 | 67.99 +/- 2.74 | 78.26 +/- 6.15 | 74.55 +/- 3.90
    |'
  id: totrans-1206
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 75.64 | 67.99 +/- 2.74 | 78.26 +/- 6.15 | 74.55 +/- 3.90
    |'
- en: '| GPT-4-unknown | 82.17 | 71.97 +/- 2.83 | 86.23 +/- 3.48 | 82.34 +/- 2.67
    |'
  id: totrans-1207
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 82.17 | 71.97 +/- 2.83 | 86.23 +/- 3.48 | 82.34 +/- 2.67
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-1208
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: 'Table 33: Accuracy per label for model group Example IT for 5-shot chain-of-thought
    evaluation.'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: '表 33: 模型组 Example IT 在 5-shot chain-of-thought 评估中的每个标签的准确率。'
- en: '| Model | Mean | World knowledge | Idiom | Rhetorical question |'
  id: totrans-1210
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 世界知识 | 成语 | 修辞问题 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Cohere-command-6b | 69.14 | 72.46 +/- 5.98 | 78.03 +/- 3.12 | 62.12 +/- 8.16
    |'
  id: totrans-1212
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 69.14 | 72.46 +/- 5.98 | 78.03 +/- 3.12 | 62.12 +/- 8.16
    |'
- en: '| Cohere-command-52b | 75.28 | 78.99 +/- 1.62 | 84.47 +/- 3.57 | 51.52 +/-
    8.57 |'
  id: totrans-1213
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.28 | 78.99 +/- 1.62 | 84.47 +/- 3.57 | 51.52 +/-
    8.57 |'
- en: '| text-ada-001-unknown | 15.33 | 11.59 +/- 8.20 | 17.42 +/- 9.43 | 10.61 +/-
    9.70 |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 15.33 | 11.59 +/- 8.20 | 17.42 +/- 9.43 | 10.61 +/-
    9.70 |'
- en: '| text-babbage-001-unknown | 47.67 | 47.83 +/- 11.50 | 55.30 +/- 16.21 | 42.42
    +/- 8.57 |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 47.67 | 47.83 +/- 11.50 | 55.30 +/- 16.21 | 42.42
    +/- 8.57 |'
- en: '| text-curie-001-unknown | 68.22 | 69.57 +/- 6.64 | 79.17 +/- 0.85 | 69.70
    +/- 10.05 |'
  id: totrans-1216
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 68.22 | 69.57 +/- 6.64 | 79.17 +/- 0.85 | 69.70
    +/- 10.05 |'
- en: '| text-davinci-001-unknown | 67.25 | 69.57 +/- 7.10 | 71.59 +/- 3.65 | 60.61
    +/- 11.34 |'
  id: totrans-1217
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 67.25 | 69.57 +/- 7.10 | 71.59 +/- 3.65 | 60.61
    +/- 11.34 |'
- en: '| text-davinci-002-unknown | 80.06 | 92.03 +/- 1.62 | 88.26 +/- 3.57 | 46.97
    +/- 24.29 |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 80.06 | 92.03 +/- 1.62 | 88.26 +/- 3.57 | 46.97
    +/- 24.29 |'
- en: '| text-davinci-003-unknown | 83.61 | 93.48 +/- 2.17 | 93.18 +/- 0.00 | 69.70
    +/- 10.05 |'
  id: totrans-1219
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 83.61 | 93.48 +/- 2.17 | 93.18 +/- 0.00 | 69.70
    +/- 10.05 |'
- en: '| ChatGPT-unknown | 77.19 | 89.86 +/- 4.10 | 87.88 +/- 3.63 | 65.15 +/- 9.70
    |'
  id: totrans-1220
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 77.19 | 89.86 +/- 4.10 | 87.88 +/- 3.63 | 65.15 +/- 9.70
    |'
- en: '| GPT-4-unknown | 86.47 | 93.48 +/- 3.32 | 93.18 +/- 2.93 | 87.88 +/- 4.29
    |'
  id: totrans-1221
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 86.47 | 93.48 +/- 3.32 | 93.18 +/- 2.93 | 87.88 +/- 4.29
    |'
- en: '| Humans | 86.23 | 93.04 | 92.73 | 92.73 |'
  id: totrans-1222
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 93.04 | 92.73 | 92.73 |'
- en: 'Table 34: Accuracy per label for model group Example IT for 5-shot chain-of-thought
    evaluation.'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 表 34：模型组 Example IT 在 5-shot 思维链评估中的准确率。
- en: '| Model | Mean | Particularised | Generalised | Other |'
  id: totrans-1224
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均值 | 特殊化 | 一般化 | 其他 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1225
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Cohere-command-6b | 69.14 | 58.33 +/- 1.93 | 81.52 +/- 2.08 | 69.04 +/- 2.04
    |'
  id: totrans-1226
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-6b | 69.14 | 58.33 +/- 1.93 | 81.52 +/- 2.08 | 69.04 +/- 2.04
    |'
- en: '| Cohere-command-52b | 75.28 | 68.94 +/- 3.19 | 77.17 +/- 2.08 | 75.88 +/-
    0.53 |'
  id: totrans-1227
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-52b | 75.28 | 68.94 +/- 3.19 | 77.17 +/- 2.08 | 75.88 +/-
    0.53 |'
- en: '| text-ada-001-unknown | 15.33 | 15.53 +/- 7.73 | 14.86 +/- 9.26 | 15.50 +/-
    8.33 |'
  id: totrans-1228
  prefs: []
  type: TYPE_TB
  zh: '| text-ada-001-unknown | 15.33 | 15.53 +/- 7.73 | 14.86 +/- 9.26 | 15.50 +/-
    8.33 |'
- en: '| text-babbage-001-unknown | 47.67 | 45.27 +/- 11.94 | 40.94 +/- 19.34 | 48.19
    +/- 14.11 |'
  id: totrans-1229
  prefs: []
  type: TYPE_TB
  zh: '| text-babbage-001-unknown | 47.67 | 45.27 +/- 11.94 | 40.94 +/- 19.34 | 48.19
    +/- 14.11 |'
- en: '| text-curie-001-unknown | 68.22 | 59.47 +/- 5.15 | 74.28 +/- 7.88 | 68.04
    +/- 1.75 |'
  id: totrans-1230
  prefs: []
  type: TYPE_TB
  zh: '| text-curie-001-unknown | 68.22 | 59.47 +/- 5.15 | 74.28 +/- 7.88 | 68.04
    +/- 1.75 |'
- en: '| text-davinci-001-unknown | 67.25 | 64.58 +/- 3.85 | 64.13 +/- 5.14 | 67.92
    +/- 3.30 |'
  id: totrans-1231
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-001-unknown | 67.25 | 64.58 +/- 3.85 | 64.13 +/- 5.14 | 67.92
    +/- 3.30 |'
- en: '| text-davinci-002-unknown | 80.06 | 75.95 +/- 3.68 | 80.07 +/- 6.69 | 80.23
    +/- 1.07 |'
  id: totrans-1232
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002-unknown | 80.06 | 75.95 +/- 3.68 | 80.07 +/- 6.69 | 80.23
    +/- 1.07 |'
- en: '| text-davinci-003-unknown | 83.61 | 77.46 +/- 1.02 | 87.32 +/- 3.18 | 83.25
    +/- 0.96 |'
  id: totrans-1233
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003-unknown | 83.61 | 77.46 +/- 1.02 | 87.32 +/- 3.18 | 83.25
    +/- 0.96 |'
- en: '| ChatGPT-unknown | 77.19 | 72.35 +/- 1.56 | 80.43 +/- 5.47 | 76.23 +/- 1.11
    |'
  id: totrans-1234
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-unknown | 77.19 | 72.35 +/- 1.56 | 80.43 +/- 5.47 | 76.23 +/- 1.11
    |'
- en: '| GPT-4-unknown | 86.47 | 81.63 +/- 2.58 | 88.77 +/- 4.05 | 86.05 +/- 1.17
    |'
  id: totrans-1235
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-unknown | 86.47 | 81.63 +/- 2.58 | 88.77 +/- 4.05 | 86.05 +/- 1.17
    |'
- en: '| Humans | 86.23 | 83.18 | 92.17 | 84.86 |'
  id: totrans-1236
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 86.23 | 83.18 | 92.17 | 84.86 |'
- en: K.10 Detailed results per model
  id: totrans-1237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.10 按模型详细结果
- en: 'This section contains the results used for the zero-shot and few-shot evaluation
    in the main text in Section 4, broken down per prompt template. See Table [35](#A11.T35
    "Table 35 ‣ K.10 Detailed results per model ‣ Appendix K Additional results ‣
    The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature
    Resolution by LLMs") until Table [84](#A11.T84 "Table 84 ‣ K.10 Detailed results
    per model ‣ Appendix K Additional results ‣ The Goldilocks of Pragmatic Understanding:
    Fine-Tuning Strategy Matters for Implicature Resolution by LLMs").'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含主文第四节中用于零-shot 和少-shot 评估的结果，按提示模板细分。请参见表 [35](#A11.T35 "表 35 ‣ K.10 按模型详细结果
    ‣ 附录 K 其他结果 ‣ 实用理解的金发姑娘：微调策略对隐含意图解决的重要性") 直到表 [84](#A11.T84 "表 84 ‣ K.10 按模型详细结果
    ‣ 附录 K 其他结果 ‣ 实用理解的金发姑娘：微调策略对隐含意图解决的重要性")。
- en: 'Table 35: Accuracy per prompt template for BERT-cased.'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 35：BERT-cased 的每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1240
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 47.3 | 48.8 | 50.5 | 49.8 | 46.7 | 46.7 |'
  id: totrans-1242
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 47.3 | 48.8 | 50.5 | 49.8 | 46.7 | 46.7 |'
- en: '| 2 | 46.8 | 50.3 | 45.5 | 50.2 | 46.7 | 46.5 |'
  id: totrans-1243
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.8 | 50.3 | 45.5 | 50.2 | 46.7 | 46.5 |'
- en: '| 3 | 57.3 | 51.5 | 50.0 | 50.0 | 47.0 | 46.7 |'
  id: totrans-1244
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.3 | 51.5 | 50.0 | 50.0 | 47.0 | 46.7 |'
- en: '| 4 | 48.8 | 51.0 | 49.5 | 48.5 | 46.8 | 46.7 |'
  id: totrans-1245
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 48.8 | 51.0 | 49.5 | 48.5 | 46.8 | 46.7 |'
- en: '| 5 | 46.7 | 50.3 | 44.5 | 47.7 | 46.7 | 46.7 |'
  id: totrans-1246
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 46.7 | 50.3 | 44.5 | 47.7 | 46.7 | 46.7 |'
- en: '| 6 | 46.7 | 50.3 | 45.8 | 47.8 | 46.8 | 46.7 |'
  id: totrans-1247
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.7 | 50.3 | 45.8 | 47.8 | 46.8 | 46.7 |'
- en: '| Mean | 48.9 | 50.4 | 47.6 | 49.0 | 46.8 | 46.7 |'
  id: totrans-1248
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 48.9 | 50.4 | 47.6 | 49.0 | 46.8 | 46.7 |'
- en: '| – std | 3.81 | 0.832 | 2.42 | 1.04 | 0.107 | 0.0745 |'
  id: totrans-1249
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.81 | 0.832 | 2.42 | 1.04 | 0.107 | 0.0745 |'
- en: '| Structured | 51.1 | 50.4 | 50.0 | 49.4 | 46.8 | 46.7 |'
  id: totrans-1250
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 51.1 | 50.4 | 50.0 | 49.4 | 46.8 | 46.7 |'
- en: '| – std | 4.4 | 1.17 | 0.408 | 0.665 | 0.125 | 7.11e-15 |'
  id: totrans-1251
  prefs: []
  type: TYPE_TB
  zh: '| – std | 4.4 | 1.17 | 0.408 | 0.665 | 0.125 | 7.11e-15 |'
- en: '| Natural | 46.7 | 50.3 | 45.3 | 48.6 | 46.7 | 46.6 |'
  id: totrans-1252
  prefs: []
  type: TYPE_TB
  zh: '| Natural | 46.7 | 50.3 | 45.3 | 48.6 | 46.7 | 46.6 |'
- en: '| – std | 0.0471 | 7.11e-15 | 0.556 | 1.16 | 0.0471 | 0.0943 |'
  id: totrans-1253
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.0471 | 7.11e-15 | 0.556 | 1.16 | 0.0471 | 0.0943 |'
- en: 'Table 36: Accuracy per prompt template for BERT-uncased.'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: 表 36：BERT-uncased 的每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1255
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 57.0 | 53.2 | 51.8 | 55.2 | 51.7 | 49.3 |'
  id: totrans-1257
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 57.0 | 53.2 | 51.8 | 55.2 | 51.7 | 49.3 |'
- en: '| 2 | 53.7 | 50.3 | 54.0 | 48.7 | 49.0 | 49.3 |'
  id: totrans-1258
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 53.7 | 50.3 | 54.0 | 48.7 | 49.0 | 49.3 |'
- en: '| 3 | 54.7 | 54.7 | 57.3 | 55.5 | 53.3 | 52.8 |'
  id: totrans-1259
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.7 | 54.7 | 57.3 | 55.5 | 53.3 | 52.8 |'
- en: '| 4 | 56.7 | 51.5 | 52.3 | 54.0 | 50.3 | 49.5 |'
  id: totrans-1260
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 56.7 | 51.5 | 52.3 | 54.0 | 50.3 | 49.5 |'
- en: '| 5 | 53.2 | 50.2 | 50.2 | 48.3 | 48.2 | 47.2 |'
  id: totrans-1261
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 53.2 | 50.2 | 50.2 | 48.3 | 48.2 | 47.2 |'
- en: '| 6 | 53.3 | 50.3 | 54.2 | 49.2 | 53.0 | 53.5 |'
  id: totrans-1262
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 53.3 | 50.3 | 54.2 | 49.2 | 53.0 | 53.5 |'
- en: '| Mean | 54.8 | 51.7 | 53.3 | 51.8 | 50.9 | 50.3 |'
  id: totrans-1263
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 54.8 | 51.7 | 53.3 | 51.8 | 50.9 | 50.3 |'
- en: '| – std | 1.55 | 1.71 | 2.24 | 3.13 | 1.92 | 2.19 |'
  id: totrans-1264
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.55 | 1.71 | 2.24 | 3.13 | 1.92 | 2.19 |'
- en: '| Structured | 56.1 | 53.1 | 53.8 | 54.9 | 51.8 | 50.5 |'
  id: totrans-1265
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.1 | 53.1 | 53.8 | 54.9 | 51.8 | 50.5 |'
- en: '| – std | 1.02 | 1.31 | 2.48 | 0.648 | 1.23 | 1.6 |'
  id: totrans-1266
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.02 | 1.31 | 2.48 | 0.648 | 1.23 | 1.6 |'
- en: '| Natural | 53.4 | 50.3 | 52.8 | 48.7 | 50.1 | 50.0 |'
  id: totrans-1267
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言 | 53.4 | 50.3 | 52.8 | 48.7 | 50.1 | 50.0 |'
- en: '| – std | 0.216 | 0.0471 | 1.84 | 0.368 | 2.1 | 2.62 |'
  id: totrans-1268
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.216 | 0.0471 | 1.84 | 0.368 | 2.1 | 2.62 |'
- en: 'Table 37: Accuracy per prompt template for RoBERTa-base.'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: '表 37: RoBERTa-base 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1270
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.0 | 55.8 | 58.0 | 58.7 | 58.3 | 57.8 |'
  id: totrans-1272
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.0 | 55.8 | 58.0 | 58.7 | 58.3 | 57.8 |'
- en: '| 2 | 56.5 | 50.5 | 52.0 | 55.8 | 56.0 | 54.2 |'
  id: totrans-1273
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 56.5 | 50.5 | 52.0 | 55.8 | 56.0 | 54.2 |'
- en: '| 3 | 53.0 | 56.8 | 56.8 | 61.3 | 59.5 | 58.8 |'
  id: totrans-1274
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.0 | 56.8 | 56.8 | 61.3 | 59.5 | 58.8 |'
- en: '| 4 | 55.2 | 56.0 | 58.7 | 59.8 | 56.8 | 57.2 |'
  id: totrans-1275
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 55.2 | 56.0 | 58.7 | 59.8 | 56.8 | 57.2 |'
- en: '| 5 | 55.7 | 50.3 | 52.3 | 54.8 | 55.5 | 53.0 |'
  id: totrans-1276
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 55.7 | 50.3 | 52.3 | 54.8 | 55.5 | 53.0 |'
- en: '| 6 | 59.2 | 50.3 | 54.2 | 55.8 | 55.7 | 55.3 |'
  id: totrans-1277
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.2 | 50.3 | 54.2 | 55.8 | 55.7 | 55.3 |'
- en: '| Mean | 55.6 | 53.3 | 55.3 | 57.7 | 57.0 | 56.1 |'
  id: totrans-1278
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 55.6 | 53.3 | 55.3 | 57.7 | 57.0 | 56.1 |'
- en: '| – std | 1.97 | 2.93 | 2.65 | 2.38 | 1.47 | 2.05 |'
  id: totrans-1279
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.97 | 2.93 | 2.65 | 2.38 | 1.47 | 2.05 |'
- en: '| Structured | 54.1 | 56.2 | 57.8 | 59.9 | 58.2 | 57.9 |'
  id: totrans-1280
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.1 | 56.2 | 57.8 | 59.9 | 58.2 | 57.9 |'
- en: '| – std | 0.899 | 0.432 | 0.785 | 1.07 | 1.1 | 0.66 |'
  id: totrans-1281
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.899 | 0.432 | 0.785 | 1.07 | 1.1 | 0.66 |'
- en: '| Natural | 57.1 | 50.4 | 52.8 | 55.5 | 55.7 | 54.2 |'
  id: totrans-1282
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言 | 57.1 | 50.4 | 52.8 | 55.5 | 55.7 | 54.2 |'
- en: '| – std | 1.5 | 0.0943 | 0.974 | 0.471 | 0.205 | 0.939 |'
  id: totrans-1283
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.5 | 0.0943 | 0.974 | 0.471 | 0.205 | 0.939 |'
- en: 'Table 38: Accuracy per prompt template for RoBERTa-large.'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: '表 38: RoBERTa-large 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1285
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 57.7 | 50.2 | 62.0 | 64.7 | 64.7 | 60.5 |'
  id: totrans-1287
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 57.7 | 50.2 | 62.0 | 64.7 | 64.7 | 60.5 |'
- en: '| 2 | 46.7 | 53.3 | 58.5 | 64.2 | 61.2 | 55.7 |'
  id: totrans-1288
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.7 | 53.3 | 58.5 | 64.2 | 61.2 | 55.7 |'
- en: '| 3 | 60.8 | 54.8 | 64.5 | 62.8 | 61.8 | 59.5 |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 60.8 | 54.8 | 64.5 | 62.8 | 61.8 | 59.5 |'
- en: '| 4 | 66.2 | 50.3 | 64.0 | 59.0 | 57.0 | 58.2 |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 66.2 | 50.3 | 64.0 | 59.0 | 57.0 | 58.2 |'
- en: '| 5 | 46.7 | 53.3 | 58.8 | 63.5 | 60.5 | 56.5 |'
  id: totrans-1291
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 46.7 | 53.3 | 58.8 | 63.5 | 60.5 | 56.5 |'
- en: '| 6 | 46.7 | 55.5 | 59.3 | 60.0 | 60.8 | 52.3 |'
  id: totrans-1292
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.7 | 55.5 | 59.3 | 60.0 | 60.8 | 52.3 |'
- en: '| Mean | 54.1 | 52.9 | 61.2 | 62.4 | 61.0 | 57.1 |'
  id: totrans-1293
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 54.1 | 52.9 | 61.2 | 62.4 | 61.0 | 57.1 |'
- en: '| – std | 7.84 | 2.03 | 2.45 | 2.13 | 2.26 | 2.7 |'
  id: totrans-1294
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 7.84 | 2.03 | 2.45 | 2.13 | 2.26 | 2.7 |'
- en: '| Structured | 61.6 | 51.8 | 63.5 | 62.2 | 61.2 | 59.4 |'
  id: totrans-1295
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 61.6 | 51.8 | 63.5 | 62.2 | 61.2 | 59.4 |'
- en: '| – std | 3.51 | 2.15 | 1.08 | 2.37 | 3.18 | 0.942 |'
  id: totrans-1296
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.51 | 2.15 | 1.08 | 2.37 | 3.18 | 0.942 |'
- en: '| Natural | 46.7 | 54.0 | 58.9 | 62.6 | 60.8 | 54.8 |'
  id: totrans-1297
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言 | 46.7 | 54.0 | 58.9 | 62.6 | 60.8 | 54.8 |'
- en: '| – std | 7.11e-15 | 1.04 | 0.33 | 1.84 | 0.287 | 1.82 |'
  id: totrans-1298
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 7.11e-15 | 1.04 | 0.33 | 1.84 | 0.287 | 1.82 |'
- en: 'Table 39: Accuracy per prompt template for GPT-2-medium.'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: '表 39: GPT-2-medium 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1300
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.2 | 53.7 | 54.0 | 53.8 | 53.8 | 55.0 |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.2 | 53.7 | 54.0 | 53.8 | 53.8 | 55.0 |'
- en: '| 2 | 52.8 | 53.7 | 55.8 | 57.2 | 60.3 | 57.2 |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 52.8 | 53.7 | 55.8 | 57.2 | 60.3 | 57.2 |'
- en: '| 3 | 53.7 | 54.0 | 52.5 | 56.5 | 55.8 | 55.3 |'
  id: totrans-1304
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.7 | 54.0 | 52.5 | 56.5 | 55.8 | 55.3 |'
- en: '| 4 | 53.5 | 55.7 | 53.3 | 55.8 | 55.5 | 54.3 |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.5 | 55.7 | 53.3 | 55.8 | 55.5 | 54.3 |'
- en: '| 5 | 59.2 | 54.3 | 56.7 | 57.7 | 60.7 | 58.8 |'
  id: totrans-1306
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 59.2 | 54.3 | 56.7 | 57.7 | 60.7 | 58.8 |'
- en: '| 6 | 58.3 | 54.8 | 55.7 | 57.7 | 61.7 | 57.8 |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 58.3 | 54.8 | 55.7 | 57.7 | 61.7 | 57.8 |'
- en: '| Mean | 55.1 | 54.4 | 54.7 | 56.4 | 58.0 | 56.4 |'
  id: totrans-1308
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 55.1 | 54.4 | 54.7 | 56.4 | 58.0 | 56.4 |'
- en: '| – std | 2.6 | 0.706 | 1.5 | 1.36 | 3.03 | 1.63 |'
  id: totrans-1309
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.6 | 0.706 | 1.5 | 1.36 | 3.03 | 1.63 |'
- en: '| Structured | 53.5 | 54.5 | 53.3 | 55.4 | 55.0 | 54.9 |'
  id: totrans-1310
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.5 | 54.5 | 53.3 | 55.4 | 55.0 | 54.9 |'
- en: '| – std | 0.205 | 0.881 | 0.613 | 1.14 | 0.881 | 0.419 |'
  id: totrans-1311
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.205 | 0.881 | 0.613 | 1.14 | 0.881 | 0.419 |'
- en: '| Natural | 56.8 | 54.3 | 56.1 | 57.5 | 60.9 | 57.9 |'
  id: totrans-1312
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言 | 56.8 | 54.3 | 56.1 | 57.5 | 60.9 | 57.9 |'
- en: '| – std | 2.83 | 0.45 | 0.45 | 0.236 | 0.589 | 0.66 |'
  id: totrans-1313
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.83 | 0.45 | 0.45 | 0.236 | 0.589 | 0.66 |'
- en: 'Table 40: Accuracy per prompt template for GPT-2-large.'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: '表 40: GPT-2-large 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1315
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 53.3 | 54.5 | 53.5 | 55.3 | 56.2 |'
  id: totrans-1317
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 53.3 | 54.5 | 53.5 | 55.3 | 56.2 |'
- en: '| 2 | 47.5 | 56.7 | 57.5 | 57.8 | 60.8 | 61.0 |'
  id: totrans-1318
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 47.5 | 56.7 | 57.5 | 57.8 | 60.8 | 61.0 |'
- en: '| 3 | 55.0 | 53.8 | 55.7 | 54.0 | 54.8 | 56.0 |'
  id: totrans-1319
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 55.0 | 53.8 | 55.7 | 54.0 | 54.8 | 56.0 |'
- en: '| 4 | 54.0 | 53.7 | 56.2 | 53.5 | 54.8 | 56.7 |'
  id: totrans-1320
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.0 | 53.7 | 56.2 | 53.5 | 54.8 | 56.7 |'
- en: '| 5 | 47.2 | 54.5 | 56.7 | 58.8 | 61.2 | 60.8 |'
  id: totrans-1321
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 47.2 | 54.5 | 56.7 | 58.8 | 61.2 | 60.8 |'
- en: '| 6 | 47.0 | 53.3 | 57.2 | 59.5 | 60.3 | 60.8 |'
  id: totrans-1322
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 47.0 | 53.3 | 57.2 | 59.5 | 60.3 | 60.8 |'
- en: '| Mean | 50.7 | 54.2 | 56.3 | 56.2 | 57.9 | 58.6 |'
  id: totrans-1323
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 50.7 | 54.2 | 56.3 | 56.2 | 57.9 | 58.6 |'
- en: '| – std | 3.47 | 1.18 | 1.0 | 2.57 | 2.92 | 2.29 |'
  id: totrans-1324
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.47 | 1.18 | 1.0 | 2.57 | 2.92 | 2.29 |'
- en: '| Structured | 54.1 | 53.6 | 55.5 | 53.7 | 55.0 | 56.3 |'
  id: totrans-1325
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.1 | 53.6 | 55.5 | 53.7 | 55.0 | 56.3 |'
- en: '| – std | 0.698 | 0.216 | 0.713 | 0.236 | 0.236 | 0.294 |'
  id: totrans-1326
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.698 | 0.216 | 0.713 | 0.236 | 0.236 | 0.294 |'
- en: '| Natural | 47.2 | 54.8 | 57.1 | 58.7 | 60.8 | 60.9 |'
  id: totrans-1327
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 47.2 | 54.8 | 57.1 | 58.7 | 60.8 | 60.9 |'
- en: '| – std | 0.205 | 1.41 | 0.33 | 0.698 | 0.368 | 0.0943 |'
  id: totrans-1328
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.205 | 1.41 | 0.33 | 0.698 | 0.368 | 0.0943 |'
- en: 'Table 41: Accuracy per prompt template for GPT-2-xl.'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: '表 41: GPT-2-xl 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.2 | 53.3 | 57.0 | 54.5 | 54.7 | 56.2 |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.2 | 53.3 | 57.0 | 54.5 | 54.7 | 56.2 |'
- en: '| 2 | 48.7 | 61.3 | 57.3 | 63.7 | 62.0 | 60.5 |'
  id: totrans-1333
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 48.7 | 61.3 | 57.3 | 63.7 | 62.0 | 60.5 |'
- en: '| 3 | 55.0 | 55.2 | 59.5 | 59.0 | 58.0 | 60.7 |'
  id: totrans-1334
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 55.0 | 55.2 | 59.5 | 59.0 | 58.0 | 60.7 |'
- en: '| 4 | 54.2 | 54.3 | 56.0 | 54.5 | 54.3 | 56.3 |'
  id: totrans-1335
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.2 | 54.3 | 56.0 | 54.5 | 54.3 | 56.3 |'
- en: '| 5 | 48.0 | 59.7 | 58.3 | 60.8 | 62.7 | 61.7 |'
  id: totrans-1336
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 48.0 | 59.7 | 58.3 | 60.8 | 62.7 | 61.7 |'
- en: '| 6 | 48.5 | 60.8 | 58.0 | 61.8 | 61.5 | 61.5 |'
  id: totrans-1337
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 48.5 | 60.8 | 58.0 | 61.8 | 61.5 | 61.5 |'
- en: '| Mean | 51.3 | 57.4 | 57.7 | 59.1 | 58.9 | 59.5 |'
  id: totrans-1338
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 51.3 | 57.4 | 57.7 | 59.1 | 58.9 | 59.5 |'
- en: '| – std | 2.92 | 3.25 | 1.1 | 3.5 | 3.43 | 2.32 |'
  id: totrans-1339
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.92 | 3.25 | 1.1 | 3.5 | 3.43 | 2.32 |'
- en: '| Structured | 54.1 | 54.3 | 57.5 | 56.0 | 55.7 | 57.7 |'
  id: totrans-1340
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.1 | 54.3 | 57.5 | 56.0 | 55.7 | 57.7 |'
- en: '| – std | 0.736 | 0.776 | 1.47 | 2.12 | 1.66 | 2.1 |'
  id: totrans-1341
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.736 | 0.776 | 1.47 | 2.12 | 1.66 | 2.1 |'
- en: '| Natural | 48.4 | 60.6 | 57.9 | 62.1 | 62.1 | 61.2 |'
  id: totrans-1342
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 48.4 | 60.6 | 57.9 | 62.1 | 62.1 | 61.2 |'
- en: '| – std | 0.294 | 0.668 | 0.419 | 1.2 | 0.492 | 0.525 |'
  id: totrans-1343
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.294 | 0.668 | 0.419 | 1.2 | 0.492 | 0.525 |'
- en: 'Table 42: Accuracy per prompt template for EleutherAI-125M.'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: '表 42: EleutherAI-125M 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1345
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1346
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 53.7 | 52.7 | 56.2 | 56.2 | 54.0 |'
  id: totrans-1347
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 53.7 | 52.7 | 56.2 | 56.2 | 54.0 |'
- en: '| 2 | 52.2 | 50.0 | 47.5 | 53.5 | 55.7 | 53.3 |'
  id: totrans-1348
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 52.2 | 50.0 | 47.5 | 53.5 | 55.7 | 53.3 |'
- en: '| 3 | 53.3 | 53.8 | 51.2 | 55.8 | 54.8 | 52.8 |'
  id: totrans-1349
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.3 | 53.8 | 51.2 | 55.8 | 54.8 | 52.8 |'
- en: '| 4 | 53.7 | 52.5 | 51.2 | 53.8 | 55.8 | 53.2 |'
  id: totrans-1350
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.7 | 52.5 | 51.2 | 53.8 | 55.8 | 53.2 |'
- en: '| 5 | 50.7 | 50.2 | 47.3 | 53.8 | 56.2 | 53.8 |'
  id: totrans-1351
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 50.7 | 50.2 | 47.3 | 53.8 | 56.2 | 53.8 |'
- en: '| 6 | 48.2 | 49.8 | 47.5 | 53.2 | 57.5 | 53.5 |'
  id: totrans-1352
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 48.2 | 49.8 | 47.5 | 53.2 | 57.5 | 53.5 |'
- en: '| Mean | 51.9 | 51.7 | 49.6 | 54.4 | 56.0 | 53.4 |'
  id: totrans-1353
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 51.9 | 51.7 | 49.6 | 54.4 | 56.0 | 53.4 |'
- en: '| – std | 1.93 | 1.72 | 2.19 | 1.17 | 0.806 | 0.394 |'
  id: totrans-1354
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.93 | 1.72 | 2.19 | 1.17 | 0.806 | 0.394 |'
- en: '| Structured | 53.4 | 53.3 | 51.7 | 55.3 | 55.6 | 53.3 |'
  id: totrans-1355
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.4 | 53.3 | 51.7 | 55.3 | 55.6 | 53.3 |'
- en: '| – std | 0.189 | 0.591 | 0.707 | 1.05 | 0.589 | 0.499 |'
  id: totrans-1356
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.189 | 0.591 | 0.707 | 1.05 | 0.589 | 0.499 |'
- en: '| Natural | 50.4 | 50.0 | 47.4 | 53.5 | 56.5 | 53.5 |'
  id: totrans-1357
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 50.4 | 50.0 | 47.4 | 53.5 | 56.5 | 53.5 |'
- en: '| – std | 1.65 | 0.163 | 0.0943 | 0.245 | 0.759 | 0.205 |'
  id: totrans-1358
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.65 | 0.163 | 0.0943 | 0.245 | 0.759 | 0.205 |'
- en: 'Table 43: Accuracy per prompt template for EleutherAI-1.3B.'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: '表 43: EleutherAI-1.3B 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1360
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1361
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.3 | 53.7 | 54.8 | 57.5 | 57.2 | 56.2 |'
  id: totrans-1362
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.3 | 53.7 | 54.8 | 57.5 | 57.2 | 56.2 |'
- en: '| 2 | 51.8 | 56.8 | 57.5 | 59.0 | 55.8 | 54.7 |'
  id: totrans-1363
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 51.8 | 56.8 | 57.5 | 59.0 | 55.8 | 54.7 |'
- en: '| 3 | 58.0 | 55.5 | 59.5 | 58.0 | 61.5 | 57.5 |'
  id: totrans-1364
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 58.0 | 55.5 | 59.5 | 58.0 | 61.5 | 57.5 |'
- en: '| 4 | 53.2 | 57.5 | 56.8 | 55.2 | 56.5 | 54.7 |'
  id: totrans-1365
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.2 | 57.5 | 56.8 | 55.2 | 56.5 | 54.7 |'
- en: '| 5 | 49.7 | 55.2 | 57.5 | 58.7 | 57.2 | 56.7 |'
  id: totrans-1366
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 49.7 | 55.2 | 57.5 | 58.7 | 57.2 | 56.7 |'
- en: '| 6 | 51.8 | 55.7 | 56.5 | 58.7 | 56.5 | 56.2 |'
  id: totrans-1367
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 51.8 | 55.7 | 56.5 | 58.7 | 56.5 | 56.2 |'
- en: '| Mean | 53.1 | 55.7 | 57.1 | 57.8 | 57.4 | 56.0 |'
  id: totrans-1368
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 53.1 | 55.7 | 57.1 | 57.8 | 57.4 | 56.0 |'
- en: '| – std | 2.59 | 1.21 | 1.4 | 1.29 | 1.87 | 1.02 |'
  id: totrans-1369
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.59 | 1.21 | 1.4 | 1.29 | 1.87 | 1.02 |'
- en: '| Structured | 55.2 | 55.6 | 57.0 | 56.9 | 58.4 | 56.1 |'
  id: totrans-1370
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.2 | 55.6 | 57.0 | 56.9 | 58.4 | 56.1 |'
- en: '| – std | 2.05 | 1.55 | 1.93 | 1.22 | 2.21 | 1.14 |'
  id: totrans-1371
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.05 | 1.55 | 1.93 | 1.22 | 2.21 | 1.14 |'
- en: '| Natural | 51.1 | 55.9 | 57.2 | 58.8 | 56.5 | 55.9 |'
  id: totrans-1372
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 51.1 | 55.9 | 57.2 | 58.8 | 56.5 | 55.9 |'
- en: '| – std | 0.99 | 0.668 | 0.471 | 0.141 | 0.572 | 0.85 |'
  id: totrans-1373
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.99 | 0.668 | 0.471 | 0.141 | 0.572 | 0.85 |'
- en: 'Table 44: Accuracy per prompt template for EleutherAI-2.7B.'
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: '表 44: EleutherAI-2.7B 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1375
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1376
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.0 | 52.8 | 58.2 | 57.8 | 59.5 | 56.7 |'
  id: totrans-1377
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.0 | 52.8 | 58.2 | 57.8 | 59.5 | 56.7 |'
- en: '| 2 | 62.0 | 56.2 | 57.7 | 55.8 | 57.8 | 57.7 |'
  id: totrans-1378
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 62.0 | 56.2 | 57.7 | 55.8 | 57.8 | 57.7 |'
- en: '| 3 | 58.7 | 60.0 | 58.8 | 59.2 | 57.8 | 57.8 |'
  id: totrans-1379
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 58.7 | 60.0 | 58.8 | 59.2 | 57.8 | 57.8 |'
- en: '| 4 | 56.5 | 54.2 | 57.5 | 56.2 | 57.5 | 55.5 |'
  id: totrans-1380
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 56.5 | 54.2 | 57.5 | 56.2 | 57.5 | 55.5 |'
- en: '| 5 | 62.7 | 54.7 | 58.7 | 55.7 | 57.3 | 57.8 |'
  id: totrans-1381
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.7 | 54.7 | 58.7 | 55.7 | 57.3 | 57.8 |'
- en: '| 6 | 61.2 | 55.2 | 57.3 | 57.5 | 58.5 | 58.7 |'
  id: totrans-1382
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 61.2 | 55.2 | 57.3 | 57.5 | 58.5 | 58.7 |'
- en: '| Mean | 59.2 | 55.5 | 58.0 | 57.0 | 58.1 | 57.4 |'
  id: totrans-1383
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 59.2 | 55.5 | 58.0 | 57.0 | 58.1 | 57.4 |'
- en: '| – std | 3.13 | 2.25 | 0.576 | 1.26 | 0.741 | 1.02 |'
  id: totrans-1384
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.13 | 2.25 | 0.576 | 1.26 | 0.741 | 1.02 |'
- en: '| Structured | 56.4 | 55.7 | 58.2 | 57.7 | 58.3 | 56.7 |'
  id: totrans-1385
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.4 | 55.7 | 58.2 | 57.7 | 58.3 | 56.7 |'
- en: '| – std | 1.92 | 3.12 | 0.531 | 1.23 | 0.881 | 0.939 |'
  id: totrans-1386
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.92 | 3.12 | 0.531 | 1.23 | 0.881 | 0.939 |'
- en: '| Natural | 62.0 | 55.4 | 57.9 | 56.3 | 57.9 | 58.1 |'
  id: totrans-1387
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 62.0 | 55.4 | 57.9 | 56.3 | 57.9 | 58.1 |'
- en: '| – std | 0.613 | 0.624 | 0.589 | 0.826 | 0.492 | 0.45 |'
  id: totrans-1388
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.613 | 0.624 | 0.589 | 0.826 | 0.492 | 0.45 |'
- en: 'Table 45: Accuracy per prompt template for EleutherAI-6B.'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: '表 45: EleutherAI-6B 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1391
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 57.5 | 58.8 | 52.7 | 53.0 | 52.5 | 51.3 |'
  id: totrans-1392
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 57.5 | 58.8 | 52.7 | 53.0 | 52.5 | 51.3 |'
- en: '| 2 | 57.7 | 51.8 | 63.2 | 62.7 | 64.3 | 65.3 |'
  id: totrans-1393
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 57.7 | 51.8 | 63.2 | 62.7 | 64.3 | 65.3 |'
- en: '| 3 | 56.2 | 58.2 | 57.2 | 53.0 | 54.7 | 54.5 |'
  id: totrans-1394
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.2 | 58.2 | 57.2 | 53.0 | 54.7 | 54.5 |'
- en: '| 4 | 52.8 | 55.5 | 53.3 | 52.2 | 54.0 | 53.8 |'
  id: totrans-1395
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 52.8 | 55.5 | 53.3 | 52.2 | 54.0 | 53.8 |'
- en: '| 5 | 56.8 | 52.7 | 62.7 | 63.2 | 65.2 | 64.2 |'
  id: totrans-1396
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 56.8 | 52.7 | 62.7 | 63.2 | 65.2 | 64.2 |'
- en: '| 6 | 57.2 | 52.8 | 61.3 | 61.8 | 62.2 | 63.3 |'
  id: totrans-1397
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 57.2 | 52.8 | 61.3 | 61.8 | 62.2 | 63.3 |'
- en: '| Mean | 56.4 | 55.0 | 58.4 | 57.6 | 58.8 | 58.7 |'
  id: totrans-1398
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 56.4 | 55.0 | 58.4 | 57.6 | 58.8 | 58.7 |'
- en: '| – std | 1.67 | 2.75 | 4.28 | 4.94 | 5.2 | 5.65 |'
  id: totrans-1399
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.67 | 2.75 | 4.28 | 4.94 | 5.2 | 5.65 |'
- en: '| Structured | 55.5 | 57.5 | 54.4 | 52.7 | 53.7 | 53.2 |'
  id: totrans-1400
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.5 | 57.5 | 54.4 | 52.7 | 53.7 | 53.2 |'
- en: '| – std | 1.98 | 1.44 | 1.99 | 0.377 | 0.918 | 1.37 |'
  id: totrans-1401
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.98 | 1.44 | 1.99 | 0.377 | 0.918 | 1.37 |'
- en: '| Natural | 57.2 | 52.4 | 62.4 | 62.6 | 63.9 | 64.3 |'
  id: totrans-1402
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 57.2 | 52.4 | 62.4 | 62.6 | 63.9 | 64.3 |'
- en: '| – std | 0.368 | 0.45 | 0.804 | 0.579 | 1.26 | 0.818 |'
  id: totrans-1403
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.368 | 0.45 | 0.804 | 0.579 | 1.26 | 0.818 |'
- en: 'Table 46: Accuracy per prompt template for EleutherAI-20B.'
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: '表 46: EleutherAI-20B 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1405
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1406
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.0 | 58.0 | 55.3 | 54.3 | 52.8 | 54.3 |'
  id: totrans-1407
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.0 | 58.0 | 55.3 | 54.3 | 52.8 | 54.3 |'
- en: '| 2 | 61.3 | 54.2 | 65.8 | 63.3 | 65.0 | 60.3 |'
  id: totrans-1408
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.3 | 54.2 | 65.8 | 63.3 | 65.0 | 60.3 |'
- en: '| 3 | 54.3 | 58.3 | 58.5 | 56.7 | 55.3 | 52.0 |'
  id: totrans-1409
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.3 | 58.3 | 58.5 | 56.7 | 55.3 | 52.0 |'
- en: '| 4 | 56.2 | 58.2 | 55.3 | 57.2 | 57.0 | 58.7 |'
  id: totrans-1410
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 56.2 | 58.2 | 55.3 | 57.2 | 57.0 | 58.7 |'
- en: '| 5 | 59.0 | 53.0 | 66.7 | 62.8 | 65.0 | 59.2 |'
  id: totrans-1411
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 59.0 | 53.0 | 66.7 | 62.8 | 65.0 | 59.2 |'
- en: '| 6 | 61.3 | 53.5 | 65.2 | 61.7 | 64.0 | 59.7 |'
  id: totrans-1412
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 61.3 | 53.5 | 65.2 | 61.7 | 64.0 | 59.7 |'
- en: '| Mean | 57.5 | 55.9 | 61.1 | 59.3 | 59.9 | 57.4 |'
  id: totrans-1413
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 57.5 | 55.9 | 61.1 | 59.3 | 59.9 | 57.4 |'
- en: '| – std | 3.25 | 2.33 | 4.9 | 3.42 | 4.98 | 3.09 |'
  id: totrans-1414
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.25 | 2.33 | 4.9 | 3.42 | 4.98 | 3.09 |'
- en: '| Structured | 54.5 | 58.2 | 56.4 | 56.1 | 55.0 | 55.0 |'
  id: totrans-1415
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.5 | 58.2 | 56.4 | 56.1 | 55.0 | 55.0 |'
- en: '| – std | 1.31 | 0.125 | 1.51 | 1.27 | 1.72 | 2.78 |'
  id: totrans-1416
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.31 | 0.125 | 1.51 | 1.27 | 1.72 | 2.78 |'
- en: '| Natural | 60.5 | 53.6 | 65.9 | 62.6 | 64.7 | 59.7 |'
  id: totrans-1417
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.5 | 53.6 | 65.9 | 62.6 | 64.7 | 59.7 |'
- en: '| – std | 1.08 | 0.492 | 0.616 | 0.668 | 0.471 | 0.45 |'
  id: totrans-1418
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.08 | 0.492 | 0.616 | 0.668 | 0.471 | 0.45 |'
- en: 'Table 47: Accuracy per prompt template for BLOOM-560M.'
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: '表 47: BLOOM-560M 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1420
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1421
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.3 | 54.2 | 53.5 | 53.8 | 53.8 | 53.5 |'
  id: totrans-1422
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.3 | 54.2 | 53.5 | 53.8 | 53.8 | 53.5 |'
- en: '| 2 | 46.7 | 56.3 | 54.0 | 54.8 | 56.0 | 55.3 |'
  id: totrans-1423
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.7 | 56.3 | 54.0 | 54.8 | 56.0 | 55.3 |'
- en: '| 3 | 58.8 | 53.3 | 53.8 | 53.3 | 54.5 | 54.0 |'
  id: totrans-1424
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 58.8 | 53.3 | 53.8 | 53.3 | 54.5 | 54.0 |'
- en: '| 4 | 56.3 | 54.8 | 53.5 | 54.8 | 52.7 | 56.7 |'
  id: totrans-1425
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 56.3 | 54.8 | 53.5 | 54.8 | 52.7 | 56.7 |'
- en: '| 5 | 46.7 | 54.3 | 53.7 | 55.3 | 56.3 | 55.5 |'
  id: totrans-1426
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 46.7 | 54.3 | 53.7 | 55.3 | 56.3 | 55.5 |'
- en: '| 6 | 46.7 | 56.0 | 54.0 | 55.2 | 56.7 | 55.0 |'
  id: totrans-1427
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.7 | 56.0 | 54.0 | 55.2 | 56.7 | 55.0 |'
- en: '| Mean | 51.6 | 54.8 | 53.8 | 54.5 | 55.0 | 55.0 |'
  id: totrans-1428
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 51.6 | 54.8 | 53.8 | 54.5 | 55.0 | 55.0 |'
- en: '| – std | 5.05 | 1.04 | 0.206 | 0.734 | 1.45 | 1.04 |'
  id: totrans-1429
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 5.05 | 1.04 | 0.206 | 0.734 | 1.45 | 1.04 |'
- en: '| Structured | 56.5 | 54.1 | 53.6 | 54.0 | 53.7 | 54.7 |'
  id: totrans-1430
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.5 | 54.1 | 53.6 | 54.0 | 53.7 | 54.7 |'
- en: '| – std | 1.84 | 0.616 | 0.141 | 0.624 | 0.741 | 1.41 |'
  id: totrans-1431
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.84 | 0.616 | 0.141 | 0.624 | 0.741 | 1.41 |'
- en: '| Natural | 46.7 | 55.5 | 53.9 | 55.1 | 56.3 | 55.3 |'
  id: totrans-1432
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 46.7 | 55.5 | 53.9 | 55.1 | 56.3 | 55.3 |'
- en: '| – std | 7.11e-15 | 0.881 | 0.141 | 0.216 | 0.287 | 0.205 |'
  id: totrans-1433
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 7.11e-15 | 0.881 | 0.141 | 0.216 | 0.287 | 0.205 |'
- en: 'Table 48: Accuracy per prompt template for BLOOM-1B1.'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: '表 48: BLOOM-1B1 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1435
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1436
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 53.5 | 56.2 | 54.2 | 55.2 | 54.5 |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 53.5 | 56.2 | 54.2 | 55.2 | 54.5 |'
- en: '| 2 | 49.0 | 51.5 | 58.2 | 59.8 | 58.8 | 60.8 |'
  id: totrans-1438
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 49.0 | 51.5 | 58.2 | 59.8 | 58.8 | 60.8 |'
- en: '| 3 | 57.2 | 54.2 | 55.8 | 54.0 | 55.5 | 50.8 |'
  id: totrans-1439
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.2 | 54.2 | 55.8 | 54.0 | 55.5 | 50.8 |'
- en: '| 4 | 53.3 | 54.0 | 54.2 | 53.3 | 55.7 | 55.8 |'
  id: totrans-1440
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.3 | 54.0 | 54.2 | 53.3 | 55.7 | 55.8 |'
- en: '| 5 | 47.3 | 51.2 | 59.8 | 61.3 | 60.2 | 60.0 |'
  id: totrans-1441
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 47.3 | 51.2 | 59.8 | 61.3 | 60.2 | 60.0 |'
- en: '| 6 | 46.8 | 51.0 | 60.2 | 61.2 | 60.2 | 59.3 |'
  id: totrans-1442
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.8 | 51.0 | 60.2 | 61.2 | 60.2 | 59.3 |'
- en: '| Mean | 51.2 | 52.6 | 57.4 | 57.3 | 57.6 | 56.9 |'
  id: totrans-1443
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 51.2 | 52.6 | 57.4 | 57.3 | 57.6 | 56.9 |'
- en: '| – std | 3.75 | 1.36 | 2.18 | 3.51 | 2.19 | 3.53 |'
  id: totrans-1444
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.75 | 1.36 | 2.18 | 3.51 | 2.19 | 3.53 |'
- en: '| Structured | 54.6 | 53.9 | 55.4 | 53.8 | 55.5 | 53.7 |'
  id: totrans-1445
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.6 | 53.9 | 55.4 | 53.8 | 55.5 | 53.7 |'
- en: '| – std | 1.84 | 0.294 | 0.864 | 0.386 | 0.205 | 2.12 |'
  id: totrans-1446
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.84 | 0.294 | 0.864 | 0.386 | 0.205 | 2.12 |'
- en: '| Natural | 47.7 | 51.2 | 59.4 | 60.8 | 59.7 | 60.0 |'
  id: totrans-1447
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 47.7 | 51.2 | 59.4 | 60.8 | 59.7 | 60.0 |'
- en: '| – std | 0.942 | 0.205 | 0.864 | 0.685 | 0.66 | 0.613 |'
  id: totrans-1448
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.942 | 0.205 | 0.864 | 0.685 | 0.66 | 0.613 |'
- en: 'Table 49: Accuracy per prompt template for BLOOM-1B7.'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '表 49: BLOOM-1B7 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1450
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.5 | 54.7 | 53.8 | 54.0 | 55.7 | 56.5 |'
  id: totrans-1452
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.5 | 54.7 | 53.8 | 54.0 | 55.7 | 56.5 |'
- en: '| 2 | 57.7 | 52.2 | 56.3 | 55.5 | 55.8 | 52.0 |'
  id: totrans-1453
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 57.7 | 52.2 | 56.3 | 55.5 | 55.8 | 52.0 |'
- en: '| 3 | 54.7 | 53.2 | 53.8 | 51.0 | 54.5 | 54.0 |'
  id: totrans-1454
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.7 | 53.2 | 53.8 | 51.0 | 54.5 | 54.0 |'
- en: '| 4 | 54.5 | 53.8 | 54.5 | 51.2 | 55.5 | 50.3 |'
  id: totrans-1455
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.5 | 53.8 | 54.5 | 51.2 | 55.5 | 50.3 |'
- en: '| 5 | 50.0 | 51.2 | 54.3 | 53.2 | 54.7 | 50.0 |'
  id: totrans-1456
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 50.0 | 51.2 | 54.3 | 53.2 | 54.7 | 50.0 |'
- en: '| 6 | 51.3 | 51.8 | 53.8 | 54.0 | 54.7 | 50.8 |'
  id: totrans-1457
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 51.3 | 51.8 | 53.8 | 54.0 | 54.7 | 50.8 |'
- en: '| Mean | 53.6 | 52.8 | 54.4 | 53.1 | 55.1 | 52.3 |'
  id: totrans-1458
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 53.6 | 52.8 | 54.4 | 53.1 | 55.1 | 52.3 |'
- en: '| – std | 2.49 | 1.2 | 0.886 | 1.6 | 0.528 | 2.31 |'
  id: totrans-1459
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.49 | 1.2 | 0.886 | 1.6 | 0.528 | 2.31 |'
- en: '| Structured | 54.2 | 53.9 | 54.0 | 52.1 | 55.2 | 53.6 |'
  id: totrans-1460
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.2 | 53.9 | 54.0 | 52.1 | 55.2 | 53.6 |'
- en: '| – std | 0.525 | 0.616 | 0.33 | 1.37 | 0.525 | 2.55 |'
  id: totrans-1461
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.525 | 0.616 | 0.33 | 1.37 | 0.525 | 2.55 |'
- en: '| Natural | 53.0 | 51.7 | 54.8 | 54.2 | 55.1 | 50.9 |'
  id: totrans-1462
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 53.0 | 51.7 | 54.8 | 54.2 | 55.1 | 50.9 |'
- en: '| – std | 3.37 | 0.411 | 1.08 | 0.953 | 0.519 | 0.822 |'
  id: totrans-1463
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.37 | 0.411 | 1.08 | 0.953 | 0.519 | 0.822 |'
- en: 'Table 50: Accuracy per prompt template for BLOOM-3B.'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: '表 50: BLOOM-3B 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1465
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1466
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.0 | 54.0 | 56.8 | 59.5 | 60.0 | 58.2 |'
  id: totrans-1467
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.0 | 54.0 | 56.8 | 59.5 | 60.0 | 58.2 |'
- en: '| 2 | 62.5 | 58.0 | 58.2 | 59.7 | 57.5 | 60.0 |'
  id: totrans-1468
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 62.5 | 58.0 | 58.2 | 59.7 | 57.5 | 60.0 |'
- en: '| 3 | 53.5 | 54.0 | 57.2 | 58.7 | 59.2 | 58.2 |'
  id: totrans-1469
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.5 | 54.0 | 57.2 | 58.7 | 59.2 | 58.2 |'
- en: '| 4 | 54.8 | 55.3 | 55.7 | 59.0 | 58.2 | 55.8 |'
  id: totrans-1470
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.8 | 55.3 | 55.7 | 59.0 | 58.2 | 55.8 |'
- en: '| 5 | 58.5 | 57.5 | 58.0 | 59.7 | 58.8 | 60.2 |'
  id: totrans-1471
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 58.5 | 57.5 | 58.0 | 59.7 | 58.8 | 60.2 |'
- en: '| 6 | 59.0 | 56.8 | 57.3 | 59.8 | 58.5 | 59.5 |'
  id: totrans-1472
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.0 | 56.8 | 57.3 | 59.8 | 58.5 | 59.5 |'
- en: '| Mean | 56.9 | 55.9 | 57.2 | 59.4 | 58.7 | 58.6 |'
  id: totrans-1473
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 56.9 | 55.9 | 57.2 | 59.4 | 58.7 | 58.6 |'
- en: '| – std | 3.4 | 1.6 | 0.823 | 0.408 | 0.783 | 1.5 |'
  id: totrans-1474
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.4 | 1.6 | 0.823 | 0.408 | 0.783 | 1.5 |'
- en: '| Structured | 53.8 | 54.4 | 56.6 | 59.1 | 59.1 | 57.4 |'
  id: totrans-1475
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.8 | 54.4 | 56.6 | 59.1 | 59.1 | 57.4 |'
- en: '| – std | 0.759 | 0.613 | 0.634 | 0.33 | 0.736 | 1.13 |'
  id: totrans-1476
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.759 | 0.613 | 0.634 | 0.33 | 0.736 | 1.13 |'
- en: '| Natural | 60.0 | 57.4 | 57.8 | 59.7 | 58.3 | 59.9 |'
  id: totrans-1477
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.0 | 57.4 | 57.8 | 59.7 | 58.3 | 59.9 |'
- en: '| – std | 1.78 | 0.492 | 0.386 | 0.0471 | 0.556 | 0.294 |'
  id: totrans-1478
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.78 | 0.492 | 0.386 | 0.0471 | 0.556 | 0.294 |'
- en: 'Table 51: Accuracy per prompt template for BLOOM-7B1.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: '表 51: BLOOM-7B1 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1480
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1481
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.2 | 55.2 | 55.2 | 52.0 | 53.0 | 52.7 |'
  id: totrans-1482
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.2 | 55.2 | 55.2 | 52.0 | 53.0 | 52.7 |'
- en: '| 2 | 61.2 | 59.0 | 53.7 | 58.3 | 58.8 | 61.7 |'
  id: totrans-1483
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.2 | 59.0 | 53.7 | 58.3 | 58.8 | 61.7 |'
- en: '| 3 | 58.7 | 53.3 | 53.0 | 53.3 | 53.0 | 52.8 |'
  id: totrans-1484
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 58.7 | 53.3 | 53.0 | 53.3 | 53.0 | 52.8 |'
- en: '| 4 | 53.5 | 53.5 | 55.2 | 52.8 | 54.3 | 53.5 |'
  id: totrans-1485
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.5 | 53.5 | 55.2 | 52.8 | 54.3 | 53.5 |'
- en: '| 5 | 62.0 | 61.0 | 55.3 | 60.3 | 58.5 | 62.5 |'
  id: totrans-1486
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.0 | 61.0 | 55.3 | 60.3 | 58.5 | 62.5 |'
- en: '| 6 | 63.5 | 60.0 | 54.7 | 59.8 | 56.3 | 62.5 |'
  id: totrans-1487
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 63.5 | 60.0 | 54.7 | 59.8 | 56.3 | 62.5 |'
- en: '| Mean | 58.7 | 57.0 | 54.5 | 56.1 | 55.7 | 57.6 |'
  id: totrans-1488
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 58.7 | 57.0 | 54.5 | 56.1 | 55.7 | 57.6 |'
- en: '| – std | 4.03 | 3.11 | 0.871 | 3.46 | 2.39 | 4.63 |'
  id: totrans-1489
  prefs: []
  type: TYPE_TB
  zh: '| – std | 4.03 | 3.11 | 0.871 | 3.46 | 2.39 | 4.63 |'
- en: '| Structured | 55.1 | 54.0 | 54.5 | 52.7 | 53.4 | 53.0 |'
  id: totrans-1490
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.1 | 54.0 | 54.5 | 52.7 | 53.4 | 53.0 |'
- en: '| – std | 2.52 | 0.852 | 1.04 | 0.535 | 0.613 | 0.356 |'
  id: totrans-1491
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.52 | 0.852 | 1.04 | 0.535 | 0.613 | 0.356 |'
- en: '| Natural | 62.2 | 60.0 | 54.6 | 59.5 | 57.9 | 62.2 |'
  id: totrans-1492
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 62.2 | 60.0 | 54.6 | 59.5 | 57.9 | 62.2 |'
- en: '| – std | 0.953 | 0.816 | 0.66 | 0.85 | 1.11 | 0.377 |'
  id: totrans-1493
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.953 | 0.816 | 0.66 | 0.85 | 1.11 | 0.377 |'
- en: 'Table 52: Accuracy per prompt template for BLOOM-176B.'
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: '表 52: BLOOM-176B 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1495
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1496
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.8 | 58.8 | 58.5 | 57.7 | 55.7 | 56.7 |'
  id: totrans-1497
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.8 | 58.8 | 58.5 | 57.7 | 55.7 | 56.7 |'
- en: '| 2 | 55.8 | 60.8 | 68.0 | 65.7 | 64.2 | 62.7 |'
  id: totrans-1498
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 55.8 | 60.8 | 68.0 | 65.7 | 64.2 | 62.7 |'
- en: '| 3 | 53.5 | 66.7 | 69.3 | 71.8 | 71.7 | 69.8 |'
  id: totrans-1499
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.5 | 66.7 | 69.3 | 71.8 | 71.7 | 69.8 |'
- en: '| 4 | 54.3 | 59.8 | 64.8 | 62.2 | 60.7 | 61.3 |'
  id: totrans-1500
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.3 | 59.8 | 64.8 | 62.2 | 60.7 | 61.3 |'
- en: '| 5 | 52.3 | 61.3 | 66.2 | 61.8 | 58.8 | 57.5 |'
  id: totrans-1501
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 52.3 | 61.3 | 66.2 | 61.8 | 58.8 | 57.5 |'
- en: '| 6 | 55.5 | 59.2 | 65.7 | 61.7 | 60.3 | 58.3 |'
  id: totrans-1502
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 55.5 | 59.2 | 65.7 | 61.7 | 60.3 | 58.3 |'
- en: '| Mean | 54.2 | 61.1 | 65.4 | 63.5 | 61.9 | 61.1 |'
  id: totrans-1503
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 54.2 | 61.1 | 65.4 | 63.5 | 61.9 | 61.1 |'
- en: '| – std | 1.19 | 2.65 | 3.43 | 4.38 | 5.06 | 4.44 |'
  id: totrans-1504
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.19 | 2.65 | 3.43 | 4.38 | 5.06 | 4.44 |'
- en: '| Structured | 53.9 | 61.8 | 64.2 | 63.9 | 62.7 | 62.6 |'
  id: totrans-1505
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.9 | 61.8 | 64.2 | 63.9 | 62.7 | 62.6 |'
- en: '| – std | 0.33 | 3.51 | 4.43 | 5.88 | 6.68 | 5.43 |'
  id: totrans-1506
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.33 | 3.51 | 4.43 | 5.88 | 6.68 | 5.43 |'
- en: '| Natural | 54.5 | 60.4 | 66.6 | 63.1 | 61.1 | 59.5 |'
  id: totrans-1507
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 54.5 | 60.4 | 66.6 | 63.1 | 61.1 | 59.5 |'
- en: '| – std | 1.58 | 0.896 | 0.988 | 1.86 | 2.28 | 2.29 |'
  id: totrans-1508
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.58 | 0.896 | 0.988 | 1.86 | 2.28 | 2.29 |'
- en: 'Table 53: Accuracy per prompt template for OPT-125M.'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: '表 53: OPT-125M 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1510
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1511
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 55.2 | 54.0 | 55.2 | 54.2 | 55.0 |'
  id: totrans-1512
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 55.2 | 54.0 | 55.2 | 54.2 | 55.0 |'
- en: '| 2 | 49.5 | 50.5 | 47.5 | 52.7 | 50.5 | 48.2 |'
  id: totrans-1513
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 49.5 | 50.5 | 47.5 | 52.7 | 50.5 | 48.2 |'
- en: '| 3 | 53.5 | 55.5 | 53.0 | 55.0 | 53.7 | 56.0 |'
  id: totrans-1514
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.5 | 55.5 | 53.0 | 55.0 | 53.7 | 56.0 |'
- en: '| 4 | 53.3 | 54.5 | 54.2 | 53.8 | 54.3 | 53.8 |'
  id: totrans-1515
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.3 | 54.5 | 54.2 | 53.8 | 54.3 | 53.8 |'
- en: '| 5 | 48.5 | 50.5 | 46.3 | 50.7 | 49.5 | 48.0 |'
  id: totrans-1516
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 48.5 | 50.5 | 46.3 | 50.7 | 49.5 | 48.0 |'
- en: '| 6 | 47.3 | 50.2 | 46.3 | 50.0 | 49.0 | 48.0 |'
  id: totrans-1517
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 47.3 | 50.2 | 46.3 | 50.0 | 49.0 | 48.0 |'
- en: '| Mean | 50.9 | 52.7 | 50.2 | 52.9 | 51.9 | 51.5 |'
  id: totrans-1518
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 50.9 | 52.7 | 50.2 | 52.9 | 51.9 | 51.5 |'
- en: '| – std | 2.55 | 2.35 | 3.56 | 1.99 | 2.25 | 3.49 |'
  id: totrans-1519
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.55 | 2.35 | 3.56 | 1.99 | 2.25 | 3.49 |'
- en: '| Structured | 53.4 | 55.1 | 53.7 | 54.7 | 54.1 | 54.9 |'
  id: totrans-1520
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.4 | 55.1 | 53.7 | 54.7 | 54.1 | 54.9 |'
- en: '| – std | 0.0943 | 0.419 | 0.525 | 0.618 | 0.262 | 0.899 |'
  id: totrans-1521
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.0943 | 0.419 | 0.525 | 0.618 | 0.262 | 0.899 |'
- en: '| Natural | 48.4 | 50.4 | 46.7 | 51.1 | 49.7 | 48.1 |'
  id: totrans-1522
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 48.4 | 50.4 | 46.7 | 51.1 | 49.7 | 48.1 |'
- en: '| – std | 0.899 | 0.141 | 0.566 | 1.14 | 0.624 | 0.0943 |'
  id: totrans-1523
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.899 | 0.141 | 0.566 | 1.14 | 0.624 | 0.0943 |'
- en: 'Table 54: Accuracy per prompt template for OPT-350M.'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: '表 54: OPT-350M 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1525
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1526
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 53.8 | 51.5 | 56.5 | 54.2 | 54.7 |'
  id: totrans-1527
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 53.8 | 51.5 | 56.5 | 54.2 | 54.7 |'
- en: '| 2 | 60.5 | 50.3 | 50.8 | 56.5 | 55.2 | 54.0 |'
  id: totrans-1528
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 60.5 | 50.3 | 50.8 | 56.5 | 55.2 | 54.0 |'
- en: '| 3 | 53.3 | 56.3 | 52.8 | 58.7 | 55.0 | 56.2 |'
  id: totrans-1529
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.3 | 56.3 | 52.8 | 58.7 | 55.0 | 56.2 |'
- en: '| 4 | 53.7 | 56.3 | 52.0 | 55.2 | 55.2 | 56.3 |'
  id: totrans-1530
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.7 | 56.3 | 52.0 | 55.2 | 55.2 | 56.3 |'
- en: '| 5 | 62.3 | 50.3 | 50.8 | 57.0 | 56.5 | 53.5 |'
  id: totrans-1531
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.3 | 50.3 | 50.8 | 57.0 | 56.5 | 53.5 |'
- en: '| 6 | 59.7 | 50.3 | 50.8 | 56.5 | 56.5 | 53.0 |'
  id: totrans-1532
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.7 | 50.3 | 50.8 | 56.5 | 56.5 | 53.0 |'
- en: '| Mean | 57.1 | 52.9 | 51.4 | 56.7 | 55.4 | 54.6 |'
  id: totrans-1533
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 57.1 | 52.9 | 51.4 | 56.7 | 55.4 | 54.6 |'
- en: '| – std | 3.78 | 2.71 | 0.752 | 1.04 | 0.826 | 1.26 |'
  id: totrans-1534
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.78 | 2.71 | 0.752 | 1.04 | 0.826 | 1.26 |'
- en: '| Structured | 53.4 | 55.5 | 52.1 | 56.8 | 54.8 | 55.7 |'
  id: totrans-1535
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 53.4 | 55.5 | 52.1 | 56.8 | 54.8 | 55.7 |'
- en: '| – std | 0.189 | 1.18 | 0.535 | 1.44 | 0.432 | 0.732 |'
  id: totrans-1536
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.189 | 1.18 | 0.535 | 1.44 | 0.432 | 0.732 |'
- en: '| Natural | 60.8 | 50.3 | 50.8 | 56.7 | 56.1 | 53.5 |'
  id: totrans-1537
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.8 | 50.3 | 50.8 | 56.7 | 56.1 | 53.5 |'
- en: '| – std | 1.09 | 7.11e-15 | 7.11e-15 | 0.236 | 0.613 | 0.408 |'
  id: totrans-1538
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.09 | 7.11e-15 | 7.11e-15 | 0.236 | 0.613 | 0.408 |'
- en: 'Table 55: Accuracy per prompt template for OPT-1.3B.'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: '表 55: OPT-1.3B 每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1540
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1541
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 57.8 | 56.2 | 55.5 | 60.2 | 59.8 | 62.7 |'
  id: totrans-1542
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 57.8 | 56.2 | 55.5 | 60.2 | 59.8 | 62.7 |'
- en: '| 2 | 62.2 | 57.0 | 61.2 | 61.8 | 64.8 | 67.2 |'
  id: totrans-1543
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 62.2 | 57.0 | 61.2 | 61.8 | 64.8 | 67.2 |'
- en: '| 3 | 60.8 | 59.5 | 57.2 | 59.7 | 60.3 | 58.2 |'
  id: totrans-1544
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 60.8 | 59.5 | 57.2 | 59.7 | 60.3 | 58.2 |'
- en: '| 4 | 54.8 | 55.8 | 59.2 | 56.5 | 57.0 | 54.7 |'
  id: totrans-1545
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.8 | 55.8 | 59.2 | 56.5 | 57.0 | 54.7 |'
- en: '| 5 | 62.5 | 56.2 | 59.3 | 61.7 | 65.0 | 64.5 |'
  id: totrans-1546
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.5 | 56.2 | 59.3 | 61.7 | 65.0 | 64.5 |'
- en: '| 6 | 64.0 | 53.2 | 55.8 | 59.7 | 62.7 | 62.8 |'
  id: totrans-1547
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 64.0 | 53.2 | 55.8 | 59.7 | 62.7 | 62.8 |'
- en: '| Mean | 60.4 | 56.3 | 58.0 | 59.9 | 61.6 | 61.7 |'
  id: totrans-1548
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 60.4 | 56.3 | 58.0 | 59.9 | 61.6 | 61.7 |'
- en: '| – std | 3.13 | 1.85 | 2.05 | 1.76 | 2.86 | 4.11 |'
  id: totrans-1549
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.13 | 1.85 | 2.05 | 1.76 | 2.86 | 4.11 |'
- en: '| Structured | 57.8 | 57.2 | 57.3 | 58.8 | 59.0 | 58.5 |'
  id: totrans-1550
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 57.8 | 57.2 | 57.3 | 58.8 | 59.0 | 58.5 |'
- en: '| – std | 2.45 | 1.66 | 1.51 | 1.64 | 1.45 | 3.27 |'
  id: totrans-1551
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.45 | 1.66 | 1.51 | 1.64 | 1.45 | 3.27 |'
- en: '| Natural | 62.9 | 55.5 | 58.8 | 61.1 | 64.2 | 64.8 |'
  id: totrans-1552
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 62.9 | 55.5 | 58.8 | 61.1 | 64.2 | 64.8 |'
- en: '| – std | 0.787 | 1.64 | 2.24 | 0.967 | 1.04 | 1.81 |'
  id: totrans-1553
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.787 | 1.64 | 2.24 | 0.967 | 1.04 | 1.81 |'
- en: 'Table 56: Accuracy per prompt template for OPT-2.7B.'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: 表56：OPT-2.7B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1555
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1556
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.7 | 53.0 | 53.2 | 53.8 | 54.3 | 53.7 |'
  id: totrans-1557
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.7 | 53.0 | 53.2 | 53.8 | 54.3 | 53.7 |'
- en: '| 2 | 64.0 | 60.3 | 60.2 | 60.3 | 61.3 | 64.5 |'
  id: totrans-1558
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 64.0 | 60.3 | 60.2 | 60.3 | 61.3 | 64.5 |'
- en: '| 3 | 55.8 | 53.3 | 55.2 | 55.8 | 57.0 | 56.5 |'
  id: totrans-1559
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 55.8 | 53.3 | 55.2 | 55.8 | 57.0 | 56.5 |'
- en: '| 4 | 54.5 | 53.3 | 54.8 | 55.5 | 56.8 | 57.0 |'
  id: totrans-1560
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.5 | 53.3 | 54.8 | 55.5 | 56.8 | 57.0 |'
- en: '| 5 | 64.8 | 60.7 | 60.7 | 62.2 | 64.3 | 64.3 |'
  id: totrans-1561
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 64.8 | 60.7 | 60.7 | 62.2 | 64.3 | 64.3 |'
- en: '| 6 | 63.5 | 60.3 | 60.0 | 60.5 | 63.3 | 63.2 |'
  id: totrans-1562
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 63.5 | 60.3 | 60.0 | 60.5 | 63.3 | 63.2 |'
- en: '| Mean | 59.6 | 56.8 | 57.4 | 58.0 | 59.5 | 59.9 |'
  id: totrans-1563
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 59.6 | 56.8 | 57.4 | 58.0 | 59.5 | 59.9 |'
- en: '| – std | 4.58 | 3.62 | 3.02 | 3.11 | 3.68 | 4.28 |'
  id: totrans-1564
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.58 | 3.62 | 3.02 | 3.11 | 3.68 | 4.28 |'
- en: '| Structured | 55.0 | 53.2 | 54.4 | 55.0 | 56.0 | 55.7 |'
  id: totrans-1565
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.0 | 53.2 | 54.4 | 55.0 | 56.0 | 55.7 |'
- en: '| – std | 0.572 | 0.141 | 0.864 | 0.881 | 1.23 | 1.45 |'
  id: totrans-1566
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.572 | 0.141 | 0.864 | 0.881 | 1.23 | 1.45 |'
- en: '| Natural | 64.1 | 60.4 | 60.3 | 61.0 | 63.0 | 64.0 |'
  id: totrans-1567
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 64.1 | 60.4 | 60.3 | 61.0 | 63.0 | 64.0 |'
- en: '| – std | 0.535 | 0.189 | 0.294 | 0.852 | 1.25 | 0.572 |'
  id: totrans-1568
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.535 | 0.189 | 0.294 | 0.852 | 1.25 | 0.572 |'
- en: 'Table 57: Accuracy per prompt template for OPT-6.7B.'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 表57：OPT-6.7B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1570
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1571
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 55.7 | 54.3 | 60.8 | 61.2 | 61.2 | 58.5 |'
  id: totrans-1572
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 55.7 | 54.3 | 60.8 | 61.2 | 61.2 | 58.5 |'
- en: '| 2 | 64.2 | 68.0 | 66.8 | 65.7 | 66.3 | 66.3 |'
  id: totrans-1573
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 64.2 | 68.0 | 66.8 | 65.7 | 66.3 | 66.3 |'
- en: '| 3 | 54.2 | 53.5 | 59.5 | 61.2 | 63.3 | 60.5 |'
  id: totrans-1574
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.2 | 53.5 | 59.5 | 61.2 | 63.3 | 60.5 |'
- en: '| 4 | 58.8 | 56.3 | 61.8 | 62.2 | 63.5 | 63.2 |'
  id: totrans-1575
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 58.8 | 56.3 | 61.8 | 62.2 | 63.5 | 63.2 |'
- en: '| 5 | 64.2 | 65.2 | 66.0 | 65.2 | 67.7 | 67.5 |'
  id: totrans-1576
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 64.2 | 65.2 | 66.0 | 65.2 | 67.7 | 67.5 |'
- en: '| 6 | 65.0 | 63.2 | 64.8 | 64.3 | 66.3 | 65.7 |'
  id: totrans-1577
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 65.0 | 63.2 | 64.8 | 64.3 | 66.3 | 65.7 |'
- en: '| Mean | 60.4 | 60.1 | 63.3 | 63.3 | 64.7 | 63.6 |'
  id: totrans-1578
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 60.4 | 60.1 | 63.3 | 63.3 | 64.7 | 63.6 |'
- en: '| – std | 4.34 | 5.62 | 2.73 | 1.84 | 2.23 | 3.23 |'
  id: totrans-1579
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.34 | 5.62 | 2.73 | 1.84 | 2.23 | 3.23 |'
- en: '| Structured | 56.2 | 54.7 | 60.7 | 61.5 | 62.7 | 60.7 |'
  id: totrans-1580
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.2 | 54.7 | 60.7 | 61.5 | 62.7 | 60.7 |'
- en: '| – std | 1.92 | 1.18 | 0.942 | 0.471 | 1.04 | 1.93 |'
  id: totrans-1581
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.92 | 1.18 | 0.942 | 0.471 | 1.04 | 1.93 |'
- en: '| Natural | 64.5 | 65.5 | 65.9 | 65.1 | 66.8 | 66.5 |'
  id: totrans-1582
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 64.5 | 65.5 | 65.9 | 65.1 | 66.8 | 66.5 |'
- en: '| – std | 0.377 | 1.97 | 0.822 | 0.579 | 0.66 | 0.748 |'
  id: totrans-1583
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.377 | 1.97 | 0.822 | 0.579 | 0.66 | 0.748 |'
- en: 'Table 58: Accuracy per prompt template for OPT-13B.'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 表58：OPT-13B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1585
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1586
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.7 | 64.0 | 69.8 | 68.2 | 67.8 | 62.2 |'
  id: totrans-1587
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.7 | 64.0 | 69.8 | 68.2 | 67.8 | 62.2 |'
- en: '| 2 | 68.2 | 57.8 | 69.5 | 68.0 | 66.8 | 63.7 |'
  id: totrans-1588
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 68.2 | 57.8 | 69.5 | 68.0 | 66.8 | 63.7 |'
- en: '| 3 | 54.3 | 62.2 | 65.2 | 63.2 | 64.3 | 66.3 |'
  id: totrans-1589
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.3 | 62.2 | 65.2 | 63.2 | 64.3 | 66.3 |'
- en: '| 4 | 58.3 | 63.3 | 64.3 | 63.7 | 63.5 | 64.0 |'
  id: totrans-1590
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 58.3 | 63.3 | 64.3 | 63.7 | 63.5 | 64.0 |'
- en: '| 5 | 66.0 | 58.5 | 67.2 | 65.3 | 63.7 | 62.7 |'
  id: totrans-1591
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 66.0 | 58.5 | 67.2 | 65.3 | 63.7 | 62.7 |'
- en: '| 6 | 64.7 | 57.5 | 68.3 | 66.2 | 64.8 | 61.5 |'
  id: totrans-1592
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 64.7 | 57.5 | 68.3 | 66.2 | 64.8 | 61.5 |'
- en: '| Mean | 61.0 | 60.6 | 67.4 | 65.8 | 65.1 | 63.4 |'
  id: totrans-1593
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 61.0 | 60.6 | 67.4 | 65.8 | 65.1 | 63.4 |'
- en: '| – std | 5.51 | 2.68 | 2.06 | 1.92 | 1.6 | 1.55 |'
  id: totrans-1594
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 5.51 | 2.68 | 2.06 | 1.92 | 1.6 | 1.55 |'
- en: '| Structured | 55.8 | 63.2 | 66.4 | 65.0 | 65.2 | 64.2 |'
  id: totrans-1595
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.8 | 63.2 | 66.4 | 65.0 | 65.2 | 64.2 |'
- en: '| – std | 1.8 | 0.741 | 2.41 | 2.25 | 1.87 | 1.68 |'
  id: totrans-1596
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.8 | 0.741 | 2.41 | 2.25 | 1.87 | 1.68 |'
- en: '| Natural | 66.3 | 57.9 | 68.3 | 66.5 | 65.1 | 62.6 |'
  id: totrans-1597
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 66.3 | 57.9 | 68.3 | 66.5 | 65.1 | 62.6 |'
- en: '| – std | 1.44 | 0.419 | 0.939 | 1.12 | 1.28 | 0.899 |'
  id: totrans-1598
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.44 | 0.419 | 0.939 | 1.12 | 1.28 | 0.899 |'
- en: 'Table 59: Accuracy per prompt template for OPT-30B.'
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: 表59：OPT-30B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1600
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1601
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 62.2 | 62.7 | 66.0 | 65.2 | 65.5 | 65.0 |'
  id: totrans-1602
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 62.2 | 62.7 | 66.0 | 65.2 | 65.5 | 65.0 |'
- en: '| 2 | 62.0 | 58.7 | 69.0 | 65.7 | 66.3 | 69.0 |'
  id: totrans-1603
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 62.0 | 58.7 | 69.0 | 65.7 | 66.3 | 69.0 |'
- en: '| 3 | 60.3 | 63.5 | 62.7 | 60.8 | 60.5 | 61.5 |'
  id: totrans-1604
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 60.3 | 63.5 | 62.7 | 60.8 | 60.5 | 61.5 |'
- en: '| 4 | 65.0 | 66.8 | 57.8 | 57.2 | 57.2 | 56.2 |'
  id: totrans-1605
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 65.0 | 66.8 | 57.8 | 57.2 | 57.2 | 56.2 |'
- en: '| 5 | 60.3 | 55.8 | 70.0 | 66.0 | 67.2 | 71.0 |'
  id: totrans-1606
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 60.3 | 55.8 | 70.0 | 66.0 | 67.2 | 71.0 |'
- en: '| 6 | 59.0 | 54.5 | 68.3 | 65.3 | 67.7 | 70.2 |'
  id: totrans-1607
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.0 | 54.5 | 68.3 | 65.3 | 67.7 | 70.2 |'
- en: '| Mean | 61.5 | 60.3 | 65.6 | 63.4 | 64.1 | 65.5 |'
  id: totrans-1608
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 61.5 | 60.3 | 65.6 | 63.4 | 64.1 | 65.5 |'
- en: '| – std | 1.92 | 4.37 | 4.24 | 3.27 | 3.87 | 5.28 |'
  id: totrans-1609
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.92 | 4.37 | 4.24 | 3.27 | 3.87 | 5.28 |'
- en: '| Structured | 62.5 | 64.3 | 62.2 | 61.1 | 61.1 | 60.9 |'
  id: totrans-1610
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 62.5 | 64.3 | 62.2 | 61.1 | 61.1 | 60.9 |'
- en: '| – std | 1.93 | 1.77 | 3.37 | 3.27 | 3.41 | 3.62 |'
  id: totrans-1611
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.93 | 1.77 | 3.37 | 3.27 | 3.41 | 3.62 |'
- en: '| Natural | 60.4 | 56.3 | 69.1 | 65.7 | 67.1 | 70.1 |'
  id: totrans-1612
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.4 | 56.3 | 69.1 | 65.7 | 67.1 | 70.1 |'
- en: '| – std | 1.23 | 1.76 | 0.698 | 0.287 | 0.579 | 0.822 |'
  id: totrans-1613
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.23 | 1.76 | 0.698 | 0.287 | 0.579 | 0.822 |'
- en: 'Table 60: Accuracy per prompt template for OPT-66B.'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: '表 60: 针对 OPT-66B 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1615
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1616
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 59.3 | 56.2 | 56.7 | 56.5 | 55.7 | 54.3 |'
  id: totrans-1617
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 59.3 | 56.2 | 56.7 | 56.5 | 55.7 | 54.3 |'
- en: '| 2 | 66.5 | 67.3 | 65.3 | 64.2 | 67.2 | 65.2 |'
  id: totrans-1618
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 66.5 | 67.3 | 65.3 | 64.2 | 67.2 | 65.2 |'
- en: '| 3 | 56.5 | 64.3 | 55.5 | 55.0 | 56.2 | 52.2 |'
  id: totrans-1619
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.5 | 64.3 | 55.5 | 55.0 | 56.2 | 52.2 |'
- en: '| 4 | 62.0 | 61.5 | 66.5 | 63.0 | 61.7 | 63.7 |'
  id: totrans-1620
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 62.0 | 61.5 | 66.5 | 63.0 | 61.7 | 63.7 |'
- en: '| 5 | 62.5 | 66.0 | 64.8 | 63.7 | 65.7 | 65.0 |'
  id: totrans-1621
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.5 | 66.0 | 64.8 | 63.7 | 65.7 | 65.0 |'
- en: '| 6 | 61.2 | 63.8 | 60.2 | 62.5 | 64.7 | 64.7 |'
  id: totrans-1622
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 61.2 | 63.8 | 60.2 | 62.5 | 64.7 | 64.7 |'
- en: '| Mean | 61.3 | 63.2 | 61.5 | 60.8 | 61.9 | 60.8 |'
  id: totrans-1623
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 61.3 | 63.2 | 61.5 | 60.8 | 61.9 | 60.8 |'
- en: '| – std | 3.06 | 3.61 | 4.3 | 3.65 | 4.5 | 5.43 |'
  id: totrans-1624
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.06 | 3.61 | 4.3 | 3.65 | 4.5 | 5.43 |'
- en: '| Structured | 59.3 | 60.7 | 59.6 | 58.2 | 57.9 | 56.7 |'
  id: totrans-1625
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 59.3 | 60.7 | 59.6 | 58.2 | 57.9 | 56.7 |'
- en: '| – std | 2.25 | 3.36 | 4.93 | 3.47 | 2.72 | 5.0 |'
  id: totrans-1626
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.25 | 3.36 | 4.93 | 3.47 | 2.72 | 5.0 |'
- en: '| Natural | 63.4 | 65.7 | 63.4 | 63.5 | 65.9 | 65.0 |'
  id: totrans-1627
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 63.4 | 65.7 | 63.4 | 63.5 | 65.9 | 65.0 |'
- en: '| – std | 2.26 | 1.44 | 2.3 | 0.713 | 1.03 | 0.205 |'
  id: totrans-1628
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.26 | 1.44 | 2.3 | 0.713 | 1.03 | 0.205 |'
- en: 'Table 61: Accuracy per prompt template for OPT-175B.'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: '表 61: 针对 OPT-175B 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1630
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1631
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 56.7 | 58.0 | 64.8 | 61.0 | 65.0 | 62.3 |'
  id: totrans-1632
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 56.7 | 58.0 | 64.8 | 61.0 | 65.0 | 62.3 |'
- en: '| 2 | 52.7 | 53.3 | 67.3 | 63.2 | 68.0 | 65.8 |'
  id: totrans-1633
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 52.7 | 53.3 | 67.3 | 63.2 | 68.0 | 65.8 |'
- en: '| 3 | 54.5 | 68.5 | 60.0 | 55.3 | 57.8 | 56.7 |'
  id: totrans-1634
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.5 | 68.5 | 60.0 | 55.3 | 57.8 | 56.7 |'
- en: '| 4 | 64.0 | 66.7 | 61.5 | 58.0 | 62.0 | 58.7 |'
  id: totrans-1635
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 64.0 | 66.7 | 61.5 | 58.0 | 62.0 | 58.7 |'
- en: '| 5 | 52.0 | 52.0 | 65.0 | 63.8 | 67.8 | 65.2 |'
  id: totrans-1636
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 52.0 | 52.0 | 65.0 | 63.8 | 67.8 | 65.2 |'
- en: '| 6 | 52.2 | 51.7 | 64.7 | 63.2 | 68.0 | 66.0 |'
  id: totrans-1637
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 52.2 | 51.7 | 64.7 | 63.2 | 68.0 | 66.0 |'
- en: '| Mean | 55.3 | 58.4 | 63.9 | 60.8 | 64.8 | 62.4 |'
  id: totrans-1638
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 55.3 | 58.4 | 63.9 | 60.8 | 64.8 | 62.4 |'
- en: '| – std | 4.19 | 6.87 | 2.42 | 3.13 | 3.79 | 3.62 |'
  id: totrans-1639
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.19 | 6.87 | 2.42 | 3.13 | 3.79 | 3.62 |'
- en: '| Structured | 58.4 | 64.4 | 62.1 | 58.1 | 61.6 | 59.2 |'
  id: totrans-1640
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 58.4 | 64.4 | 62.1 | 58.1 | 61.6 | 59.2 |'
- en: '| – std | 4.06 | 4.58 | 2.0 | 2.33 | 2.95 | 2.32 |'
  id: totrans-1641
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.06 | 4.58 | 2.0 | 2.33 | 2.95 | 2.32 |'
- en: '| Natural | 52.3 | 52.3 | 65.7 | 63.4 | 67.9 | 65.7 |'
  id: totrans-1642
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 52.3 | 52.3 | 65.7 | 63.4 | 67.9 | 65.7 |'
- en: '| – std | 0.294 | 0.694 | 1.16 | 0.283 | 0.0943 | 0.34 |'
  id: totrans-1643
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.294 | 0.694 | 1.16 | 0.283 | 0.0943 | 0.34 |'
- en: 'Table 62: Accuracy per prompt template for Cohere-409.3M (Cohere-small).'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: '表 62: 针对 Cohere-409.3M (Cohere-small) 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1645
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1646
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.2 | 49.7 | 52.7 | 51.7 | 53.5 | 56.0 |'
  id: totrans-1647
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.2 | 49.7 | 52.7 | 51.7 | 53.5 | 56.0 |'
- en: '| 2 | 47.5 | 50.7 | 52.7 | 53.2 | 55.8 | 57.8 |'
  id: totrans-1648
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 47.5 | 50.7 | 52.7 | 53.2 | 55.8 | 57.8 |'
- en: '| 3 | 57.2 | 55.5 | 55.2 | 55.5 | 55.7 | 57.0 |'
  id: totrans-1649
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.2 | 55.5 | 55.2 | 55.5 | 55.7 | 57.0 |'
- en: '| 4 | 54.8 | 53.8 | 54.5 | 56.8 | 54.8 | 54.5 |'
  id: totrans-1650
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.8 | 53.8 | 54.5 | 56.8 | 54.8 | 54.5 |'
- en: '| 5 | 48.5 | 50.7 | 52.8 | 52.7 | 56.0 | 58.8 |'
  id: totrans-1651
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 48.5 | 50.7 | 52.8 | 52.7 | 56.0 | 58.8 |'
- en: '| 6 | 47.5 | 51.0 | 52.5 | 53.7 | 55.3 | 58.8 |'
  id: totrans-1652
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 47.5 | 51.0 | 52.5 | 53.7 | 55.3 | 58.8 |'
- en: '| Mean | 51.6 | 51.9 | 53.4 | 53.9 | 55.2 | 57.2 |'
  id: totrans-1653
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 51.6 | 51.9 | 53.4 | 53.9 | 55.2 | 57.2 |'
- en: '| – std | 3.91 | 2.05 | 1.05 | 1.72 | 0.847 | 1.54 |'
  id: totrans-1654
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.91 | 2.05 | 1.05 | 1.72 | 0.847 | 1.54 |'
- en: '| Structured | 55.4 | 53.0 | 54.1 | 54.7 | 54.7 | 55.8 |'
  id: totrans-1655
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.4 | 53.0 | 54.1 | 54.7 | 54.7 | 55.8 |'
- en: '| – std | 1.3 | 2.43 | 1.05 | 2.16 | 0.903 | 1.03 |'
  id: totrans-1656
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.3 | 2.43 | 1.05 | 2.16 | 0.903 | 1.03 |'
- en: '| Natural | 47.8 | 50.8 | 52.7 | 53.2 | 55.7 | 58.5 |'
  id: totrans-1657
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 47.8 | 50.8 | 52.7 | 53.2 | 55.7 | 58.5 |'
- en: '| – std | 0.471 | 0.141 | 0.125 | 0.408 | 0.294 | 0.471 |'
  id: totrans-1658
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.471 | 0.141 | 0.125 | 0.408 | 0.294 | 0.471 |'
- en: 'Table 63: Accuracy per prompt template for Cohere-6.067B (Cohere-medium).'
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: '表 63: 针对 Cohere-6.067B (Cohere-medium) 的每个提示模板的准确性。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1660
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1661
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.7 | 54.2 | 55.3 | 51.8 | 56.3 | 55.3 |'
  id: totrans-1662
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.7 | 54.2 | 55.3 | 51.8 | 56.3 | 55.3 |'
- en: '| 2 | 61.8 | 62.8 | 64.3 | 63.8 | 65.2 | 64.7 |'
  id: totrans-1663
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.8 | 62.8 | 64.3 | 63.8 | 65.2 | 64.7 |'
- en: '| 3 | 57.2 | 53.3 | 58.5 | 55.3 | 57.8 | 55.3 |'
  id: totrans-1664
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.2 | 53.3 | 58.5 | 55.3 | 57.8 | 55.3 |'
- en: '| 4 | 56.0 | 53.3 | 57.0 | 53.2 | 55.8 | 56.7 |'
  id: totrans-1665
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 56.0 | 53.3 | 57.0 | 53.2 | 55.8 | 56.7 |'
- en: '| 5 | 57.8 | 60.7 | 64.0 | 64.2 | 64.7 | 64.2 |'
  id: totrans-1666
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 57.8 | 60.7 | 64.0 | 64.2 | 64.7 | 64.2 |'
- en: '| 6 | 56.2 | 62.8 | 66.2 | 64.0 | 62.8 | 66.0 |'
  id: totrans-1667
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 56.2 | 62.8 | 66.2 | 64.0 | 62.8 | 66.0 |'
- en: '| Mean | 57.3 | 57.9 | 60.9 | 58.7 | 60.4 | 60.4 |'
  id: totrans-1668
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 57.3 | 57.9 | 60.9 | 58.7 | 60.4 | 60.4 |'
- en: '| – std | 2.24 | 4.32 | 4.11 | 5.38 | 3.92 | 4.65 |'
  id: totrans-1669
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.24 | 4.32 | 4.11 | 5.38 | 3.92 | 4.65 |'
- en: '| Structured | 56.0 | 53.6 | 56.9 | 53.4 | 56.6 | 55.8 |'
  id: totrans-1670
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.0 | 53.6 | 56.9 | 53.4 | 56.6 | 55.8 |'
- en: '| – std | 1.02 | 0.424 | 1.31 | 1.44 | 0.85 | 0.66 |'
  id: totrans-1671
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.02 | 0.424 | 1.31 | 1.44 | 0.85 | 0.66 |'
- en: '| Natural | 58.6 | 62.1 | 64.8 | 64.0 | 64.2 | 65.0 |'
  id: totrans-1672
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 58.6 | 62.1 | 64.8 | 64.0 | 64.2 | 65.0 |'
- en: '| – std | 2.36 | 0.99 | 0.974 | 0.163 | 1.03 | 0.759 |'
  id: totrans-1673
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.36 | 0.99 | 0.974 | 0.163 | 1.03 | 0.759 |'
- en: 'Table 64: Accuracy per prompt template for Cohere-13.12B (Cohere-large).'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 表格64：Cohere-13.12B（Cohere-large）每个提示模板的准确性。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1675
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1676
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 55.3 | 57.3 | 56.3 | 55.0 | 58.5 | 59.0 |'
  id: totrans-1677
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 55.3 | 57.3 | 56.3 | 55.0 | 58.5 | 59.0 |'
- en: '| 2 | 59.2 | 64.2 | 68.0 | 66.3 | 64.7 | 69.5 |'
  id: totrans-1678
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 59.2 | 64.2 | 68.0 | 66.3 | 64.7 | 69.5 |'
- en: '| 3 | 57.2 | 62.8 | 61.0 | 59.0 | 64.2 | 62.3 |'
  id: totrans-1679
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.2 | 62.8 | 61.0 | 59.0 | 64.2 | 62.3 |'
- en: '| 4 | 55.5 | 61.3 | 56.3 | 54.0 | 59.0 | 59.8 |'
  id: totrans-1680
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 55.5 | 61.3 | 56.3 | 54.0 | 59.0 | 59.8 |'
- en: '| 5 | 56.8 | 64.3 | 66.7 | 64.2 | 65.7 | 69.8 |'
  id: totrans-1681
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 56.8 | 64.3 | 66.7 | 64.2 | 65.7 | 69.8 |'
- en: '| 6 | 59.2 | 60.7 | 66.5 | 63.7 | 65.0 | 68.3 |'
  id: totrans-1682
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.2 | 60.7 | 66.5 | 63.7 | 65.0 | 68.3 |'
- en: '| Mean | 57.2 | 61.8 | 62.5 | 60.4 | 62.9 | 64.8 |'
  id: totrans-1683
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 57.2 | 61.8 | 62.5 | 60.4 | 62.9 | 64.8 |'
- en: '| – std | 1.56 | 2.41 | 4.88 | 4.69 | 2.94 | 4.55 |'
  id: totrans-1684
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.56 | 2.41 | 4.88 | 4.69 | 2.94 | 4.55 |'
- en: '| Structured | 56.0 | 60.5 | 57.9 | 56.0 | 60.6 | 60.4 |'
  id: totrans-1685
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 56.0 | 60.5 | 57.9 | 56.0 | 60.6 | 60.4 |'
- en: '| – std | 0.852 | 2.32 | 2.22 | 2.16 | 2.58 | 1.41 |'
  id: totrans-1686
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.852 | 2.32 | 2.22 | 2.16 | 2.58 | 1.41 |'
- en: '| Natural | 58.4 | 63.1 | 67.1 | 64.7 | 65.1 | 69.2 |'
  id: totrans-1687
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 58.4 | 63.1 | 67.1 | 64.7 | 65.1 | 69.2 |'
- en: '| – std | 1.13 | 1.67 | 0.665 | 1.13 | 0.419 | 0.648 |'
  id: totrans-1688
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.13 | 1.67 | 0.665 | 1.13 | 0.419 | 0.648 |'
- en: 'Table 65: Accuracy per prompt template for Cohere-52B (Cohere-xl).'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: 表格65：Cohere-52B（Cohere-xl）每个提示模板的准确性。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1690
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1691
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 56.0 | 60.7 | 70.3 | 65.3 | 66.3 | 68.7 |'
  id: totrans-1692
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 56.0 | 60.7 | 70.3 | 65.3 | 66.3 | 68.7 |'
- en: '| 2 | 62.8 | 65.0 | 64.3 | 64.2 | 65.0 | 64.3 |'
  id: totrans-1693
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 62.8 | 65.0 | 64.3 | 64.2 | 65.0 | 64.3 |'
- en: '| 3 | 54.0 | 65.3 | 62.8 | 60.2 | 64.0 | 63.5 |'
  id: totrans-1694
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.0 | 65.3 | 62.8 | 60.2 | 64.0 | 63.5 |'
- en: '| 4 | 53.8 | 55.5 | 61.8 | 64.8 | 64.3 | 64.7 |'
  id: totrans-1695
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.8 | 55.5 | 61.8 | 64.8 | 64.3 | 64.7 |'
- en: '| 5 | 62.2 | 65.7 | 67.3 | 63.0 | 63.7 | 65.3 |'
  id: totrans-1696
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 62.2 | 65.7 | 67.3 | 63.0 | 63.7 | 65.3 |'
- en: '| 6 | 62.2 | 65.7 | 64.2 | 62.3 | 65.0 | 67.8 |'
  id: totrans-1697
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 62.2 | 65.7 | 64.2 | 62.3 | 65.0 | 67.8 |'
- en: '| Mean | 58.5 | 63.0 | 65.1 | 63.3 | 64.7 | 65.7 |'
  id: totrans-1698
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 58.5 | 63.0 | 65.1 | 63.3 | 64.7 | 65.7 |'
- en: '| – std | 3.97 | 3.77 | 2.87 | 1.72 | 0.855 | 1.89 |'
  id: totrans-1699
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.97 | 3.77 | 2.87 | 1.72 | 0.855 | 1.89 |'
- en: '| Structured | 54.6 | 60.5 | 65.0 | 63.4 | 64.9 | 65.6 |'
  id: totrans-1700
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.6 | 60.5 | 65.0 | 63.4 | 64.9 | 65.6 |'
- en: '| – std | 0.993 | 4.0 | 3.79 | 2.3 | 1.02 | 2.22 |'
  id: totrans-1701
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.993 | 4.0 | 3.79 | 2.3 | 1.02 | 2.22 |'
- en: '| Natural | 62.4 | 65.5 | 65.3 | 63.2 | 64.6 | 65.8 |'
  id: totrans-1702
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 62.4 | 65.5 | 65.3 | 63.2 | 64.6 | 65.8 |'
- en: '| – std | 0.283 | 0.33 | 1.44 | 0.785 | 0.613 | 1.47 |'
  id: totrans-1703
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.283 | 0.33 | 1.44 | 0.785 | 0.613 | 1.47 |'
- en: 'Table 66: Accuracy per prompt template for GPT-3-350M (ada).'
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: 表格66：GPT-3-350M（ada）每个提示模板的准确性。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1705
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1706
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 55.3 | 57.2 | 58.3 | 57.5 | 58.2 | 60.5 |'
  id: totrans-1707
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 55.3 | 57.2 | 58.3 | 57.5 | 58.2 | 60.5 |'
- en: '| 2 | 46.7 | 56.8 | 56.3 | 59.5 | 59.2 | 61.7 |'
  id: totrans-1708
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.7 | 56.8 | 56.3 | 59.5 | 59.2 | 61.7 |'
- en: '| 3 | 54.0 | 54.5 | 53.3 | 54.0 | 56.5 | 56.7 |'
  id: totrans-1709
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.0 | 54.5 | 53.3 | 54.0 | 56.5 | 56.7 |'
- en: '| 4 | 53.5 | 52.8 | 54.7 | 56.7 | 58.8 | 59.7 |'
  id: totrans-1710
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.5 | 52.8 | 54.7 | 56.7 | 58.8 | 59.7 |'
- en: '| 5 | 49.8 | 57.3 | 55.3 | 58.5 | 58.8 | 61.8 |'
  id: totrans-1711
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 49.8 | 57.3 | 55.3 | 58.5 | 58.8 | 61.8 |'
- en: '| 6 | 49.5 | 57.2 | 56.3 | 60.2 | 61.5 | 61.2 |'
  id: totrans-1712
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 49.5 | 57.2 | 56.3 | 60.2 | 61.5 | 61.2 |'
- en: '| Mean | 51.5 | 56.0 | 55.7 | 57.7 | 58.8 | 60.3 |'
  id: totrans-1713
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 51.5 | 56.0 | 55.7 | 57.7 | 58.8 | 60.3 |'
- en: '| – std | 3.02 | 1.72 | 1.55 | 2.04 | 1.48 | 1.75 |'
  id: totrans-1714
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.02 | 1.72 | 1.55 | 2.04 | 1.48 | 1.75 |'
- en: '| Structured | 54.3 | 54.8 | 55.4 | 56.1 | 57.8 | 59.0 |'
  id: totrans-1715
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.3 | 54.8 | 55.4 | 56.1 | 57.8 | 59.0 |'
- en: '| – std | 0.759 | 1.81 | 2.11 | 1.5 | 0.974 | 1.64 |'
  id: totrans-1716
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.759 | 1.81 | 2.11 | 1.5 | 0.974 | 1.64 |'
- en: '| Natural | 48.7 | 57.1 | 56.0 | 59.4 | 59.8 | 61.6 |'
  id: totrans-1717
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 48.7 | 57.1 | 56.0 | 59.4 | 59.8 | 61.6 |'
- en: '| – std | 1.4 | 0.216 | 0.471 | 0.698 | 1.19 | 0.262 |'
  id: totrans-1718
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.4 | 0.216 | 0.471 | 0.698 | 1.19 | 0.262 |'
- en: 'Table 67: Accuracy per prompt template for GPT-3-1.3B (babbage).'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: 表格67：GPT-3-1.3B（babbage）每个提示模板的准确性。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1720
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1721
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 55.7 | 60.7 | 61.0 | 59.0 | 60.7 | 57.8 |'
  id: totrans-1722
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 55.7 | 60.7 | 61.0 | 59.0 | 60.7 | 57.8 |'
- en: '| 2 | 63.0 | 62.5 | 65.7 | 61.7 | 63.0 | 59.3 |'
  id: totrans-1723
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 63.0 | 62.5 | 65.7 | 61.7 | 63.0 | 59.3 |'
- en: '| 3 | 56.2 | 59.0 | 60.5 | 59.3 | 64.8 | 61.0 |'
  id: totrans-1724
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.2 | 59.0 | 60.5 | 59.3 | 64.8 | 61.0 |'
- en: '| 4 | 53.3 | 59.7 | 60.7 | 62.5 | 65.0 | 66.7 |'
  id: totrans-1725
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.3 | 59.7 | 60.7 | 62.5 | 65.0 | 66.7 |'
- en: '| 5 | 59.2 | 62.5 | 63.7 | 61.8 | 61.5 | 58.7 |'
  id: totrans-1726
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 59.2 | 62.5 | 63.7 | 61.8 | 61.5 | 58.7 |'
- en: '| 6 | 59.0 | 60.2 | 64.3 | 61.2 | 62.2 | 57.7 |'
  id: totrans-1727
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.0 | 60.2 | 64.3 | 61.2 | 62.2 | 57.7 |'
- en: '| Mean | 57.7 | 60.8 | 62.6 | 60.9 | 62.9 | 60.2 |'
  id: totrans-1728
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 57.7 | 60.8 | 62.6 | 60.9 | 62.9 | 60.2 |'
- en: '| – std | 3.1 | 1.33 | 2.01 | 1.31 | 1.6 | 3.11 |'
  id: totrans-1729
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.1 | 1.33 | 2.01 | 1.31 | 1.6 | 3.11 |'
- en: '| Structured | 55.1 | 59.8 | 60.7 | 60.3 | 63.5 | 61.8 |'
  id: totrans-1730
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 55.1 | 59.8 | 60.7 | 60.3 | 63.5 | 61.8 |'
- en: '| – std | 1.27 | 0.698 | 0.205 | 1.58 | 1.98 | 3.68 |'
  id: totrans-1731
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.27 | 0.698 | 0.205 | 1.58 | 1.98 | 3.68 |'
- en: '| Natural | 60.4 | 61.7 | 64.6 | 61.6 | 62.2 | 58.6 |'
  id: totrans-1732
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.4 | 61.7 | 64.6 | 61.6 | 62.2 | 58.6 |'
- en: '| – std | 1.84 | 1.08 | 0.838 | 0.262 | 0.613 | 0.66 |'
  id: totrans-1733
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.84 | 1.08 | 0.838 | 0.262 | 0.613 | 0.66 |'
- en: 'Table 68: Accuracy per prompt template for GPT-3-6.7B (curie).'
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 表 68：GPT-3-6.7B (curie) 每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1735
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1736
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.3 | 58.3 | 63.0 | 64.8 | 67.7 | 64.0 |'
  id: totrans-1737
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.3 | 58.3 | 63.0 | 64.8 | 67.7 | 64.0 |'
- en: '| 2 | 57.5 | 65.2 | 63.2 | 65.3 | 65.8 | 65.2 |'
  id: totrans-1738
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 57.5 | 65.2 | 63.2 | 65.3 | 65.8 | 65.2 |'
- en: '| 3 | 57.0 | 54.2 | 59.2 | 61.2 | 60.8 | 59.3 |'
  id: totrans-1739
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 57.0 | 54.2 | 59.2 | 61.2 | 60.8 | 59.3 |'
- en: '| 4 | 53.3 | 61.7 | 62.8 | 63.8 | 64.7 | 60.7 |'
  id: totrans-1740
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.3 | 61.7 | 62.8 | 63.8 | 64.7 | 60.7 |'
- en: '| 5 | 55.3 | 64.2 | 62.5 | 64.5 | 65.8 | 63.7 |'
  id: totrans-1741
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 55.3 | 64.2 | 62.5 | 64.5 | 65.8 | 63.7 |'
- en: '| 6 | 52.5 | 63.5 | 63.7 | 64.0 | 66.2 | 64.3 |'
  id: totrans-1742
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 52.5 | 63.5 | 63.7 | 64.0 | 66.2 | 64.3 |'
- en: '| Mean | 54.8 | 61.2 | 62.4 | 63.9 | 65.2 | 62.9 |'
  id: totrans-1743
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 54.8 | 61.2 | 62.4 | 63.9 | 65.2 | 62.9 |'
- en: '| – std | 1.92 | 3.83 | 1.48 | 1.32 | 2.14 | 2.12 |'
  id: totrans-1744
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.92 | 3.83 | 1.48 | 1.32 | 2.14 | 2.12 |'
- en: '| Structured | 54.5 | 58.1 | 61.7 | 63.3 | 64.4 | 61.3 |'
  id: totrans-1745
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.5 | 58.1 | 61.7 | 63.3 | 64.4 | 61.3 |'
- en: '| – std | 1.74 | 3.07 | 1.75 | 1.52 | 2.82 | 1.97 |'
  id: totrans-1746
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.74 | 3.07 | 1.75 | 1.52 | 2.82 | 1.97 |'
- en: '| Natural | 55.1 | 64.3 | 63.1 | 64.6 | 65.9 | 64.4 |'
  id: totrans-1747
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 55.1 | 64.3 | 63.1 | 64.6 | 65.9 | 64.4 |'
- en: '| – std | 2.05 | 0.698 | 0.492 | 0.535 | 0.189 | 0.616 |'
  id: totrans-1748
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.05 | 0.698 | 0.492 | 0.535 | 0.189 | 0.616 |'
- en: 'Table 69: Accuracy per prompt template for GPT-3-175B (davinci).'
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
  zh: 表 69：GPT-3-175B (davinci) 每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1750
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1751
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 61.2 | 67.3 | 66.3 | 62.7 | 66.7 | 66.2 |'
  id: totrans-1752
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 61.2 | 67.3 | 66.3 | 62.7 | 66.7 | 66.2 |'
- en: '| 2 | 53.7 | 65.3 | 68.8 | 69.3 | 71.0 | 69.7 |'
  id: totrans-1753
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 53.7 | 65.3 | 68.8 | 69.3 | 71.0 | 69.7 |'
- en: '| 3 | 58.7 | 65.8 | 68.2 | 64.7 | 65.0 | 65.3 |'
  id: totrans-1754
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 58.7 | 65.8 | 68.2 | 64.7 | 65.0 | 65.3 |'
- en: '| 4 | 64.0 | 62.8 | 71.3 | 68.7 | 66.2 | 67.8 |'
  id: totrans-1755
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 64.0 | 62.8 | 71.3 | 68.7 | 66.2 | 67.8 |'
- en: '| 5 | 54.2 | 66.3 | 69.0 | 70.0 | 70.0 | 70.8 |'
  id: totrans-1756
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 54.2 | 66.3 | 69.0 | 70.0 | 70.0 | 70.8 |'
- en: '| 6 | 51.7 | 66.7 | 68.7 | 68.3 | 71.0 | 70.0 |'
  id: totrans-1757
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 51.7 | 66.7 | 68.7 | 68.3 | 71.0 | 70.0 |'
- en: '| Mean | 57.2 | 65.7 | 68.7 | 67.3 | 68.3 | 68.3 |'
  id: totrans-1758
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 57.2 | 65.7 | 68.7 | 67.3 | 68.3 | 68.3 |'
- en: '| – std | 4.4 | 1.44 | 1.46 | 2.65 | 2.43 | 2.03 |'
  id: totrans-1759
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.4 | 1.44 | 1.46 | 2.65 | 2.43 | 2.03 |'
- en: '| Structured | 61.3 | 65.3 | 68.6 | 65.4 | 66.0 | 66.4 |'
  id: totrans-1760
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 61.3 | 65.3 | 68.6 | 65.4 | 66.0 | 66.4 |'
- en: '| – std | 2.16 | 1.87 | 2.06 | 2.49 | 0.713 | 1.03 |'
  id: totrans-1761
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.16 | 1.87 | 2.06 | 2.49 | 0.713 | 1.03 |'
- en: '| Natural | 53.2 | 66.1 | 68.8 | 69.2 | 70.7 | 70.2 |'
  id: totrans-1762
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 53.2 | 66.1 | 68.8 | 69.2 | 70.7 | 70.2 |'
- en: '| – std | 1.08 | 0.589 | 0.125 | 0.698 | 0.471 | 0.464 |'
  id: totrans-1763
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.08 | 0.589 | 0.125 | 0.698 | 0.471 | 0.464 |'
- en: 'Table 70: Accuracy per prompt template for BlenderBot-90M.'
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: 表 70：BlenderBot-90M 每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1765
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1766
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 46.7 | 51.5 | 46.7 | 46.7 | 46.5 | 46.5 |'
  id: totrans-1767
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 46.7 | 51.5 | 46.7 | 46.7 | 46.5 | 46.5 |'
- en: '| 2 | 46.7 | 51.3 | 46.5 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1768
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.7 | 51.3 | 46.5 | 46.7 | 46.7 | 46.7 |'
- en: '| 3 | 46.7 | 46.7 | 46.7 | 46.7 | 46.3 | 46.8 |'
  id: totrans-1769
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 46.7 | 46.7 | 46.7 | 46.7 | 46.3 | 46.8 |'
- en: '| 4 | 46.7 | 46.7 | 46.7 | 46.7 | 46.5 | 46.7 |'
  id: totrans-1770
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 46.7 | 46.7 | 46.7 | 46.7 | 46.5 | 46.7 |'
- en: '| 5 | 46.7 | 50.0 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1771
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 46.7 | 50.0 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| 6 | 46.5 | 53.5 | 46.3 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1772
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.5 | 53.5 | 46.3 | 46.7 | 46.7 | 46.7 |'
- en: '| Mean | 46.7 | 49.9 | 46.6 | 46.7 | 46.6 | 46.7 |'
  id: totrans-1773
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 46.7 | 49.9 | 46.6 | 46.7 | 46.6 | 46.7 |'
- en: '| – std | 0.0745 | 2.52 | 0.153 | 7.11e-15 | 0.149 | 0.0898 |'
  id: totrans-1774
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.0745 | 2.52 | 0.153 | 7.11e-15 | 0.149 | 0.0898 |'
- en: '| Structured | 46.7 | 48.3 | 46.7 | 46.7 | 46.4 | 46.7 |'
  id: totrans-1775
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 46.7 | 48.3 | 46.7 | 46.7 | 46.4 | 46.7 |'
- en: '| – std | 7.11e-15 | 2.26 | 7.11e-15 | 7.11e-15 | 0.0943 | 0.125 |'
  id: totrans-1776
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 7.11e-15 | 2.26 | 7.11e-15 | 7.11e-15 | 0.0943 | 0.125 |'
- en: '| Natural | 46.6 | 51.6 | 46.5 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1777
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 46.6 | 51.6 | 46.5 | 46.7 | 46.7 | 46.7 |'
- en: '| – std | 0.0943 | 1.44 | 0.163 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1778
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.0943 | 1.44 | 0.163 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
- en: 'Table 71: Accuracy per prompt template for BlenderBot-2.7B.'
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: 表 71：BlenderBot-2.7B 每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1780
  prefs: []
  type: TYPE_TB
  zh: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1781
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.0 | 53.2 | 53.3 | 53.0 | 52.8 | 53.3 |'
  id: totrans-1782
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.0 | 53.2 | 53.3 | 53.0 | 52.8 | 53.3 |'
- en: '| 2 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
  id: totrans-1783
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
- en: '| 3 | 53.2 | 53.2 | 53.3 | 53.2 | 53.2 | 53.2 |'
  id: totrans-1784
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.2 | 53.2 | 53.3 | 53.2 | 53.2 | 53.2 |'
- en: '| 4 | 53.5 | 53.5 | 53.5 | 53.3 | 52.8 | 53.0 |'
  id: totrans-1785
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 53.5 | 53.5 | 53.5 | 53.3 | 52.8 | 53.0 |'
- en: '| 5 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
  id: totrans-1786
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
- en: '| 6 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
  id: totrans-1787
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
- en: '| Mean | 53.4 | 53.3 | 53.3 | 53.2 | 53.1 | 53.2 |'
  id: totrans-1788
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 53.4 | 53.3 | 53.3 | 53.2 | 53.1 | 53.2 |'
- en: '| – std | 0.269 | 0.1 | 0.0745 | 0.111 | 0.227 | 0.111 |'
  id: totrans-1789
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.269 | 0.1 | 0.0745 | 0.111 | 0.227 | 0.111 |'
- en: '| Structured | 53.6 | 53.3 | 53.4 | 53.2 | 52.9 | 53.2 |'
  id: totrans-1790
  prefs: []
  type: TYPE_TB
  zh: '| Structured | 53.6 | 53.3 | 53.4 | 53.2 | 52.9 | 53.2 |'
- en: '| – std | 0.33 | 0.141 | 0.0943 | 0.125 | 0.189 | 0.125 |'
  id: totrans-1791
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.33 | 0.141 | 0.0943 | 0.125 | 0.189 | 0.125 |'
- en: '| Natural | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
  id: totrans-1792
  prefs: []
  type: TYPE_TB
  zh: '| Natural | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 | 53.3 |'
- en: '| – std | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1793
  prefs: []
  type: TYPE_TB
  zh: '| – std | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
- en: 'Table 72: Accuracy per prompt template for BlenderBot-9.4B.'
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 72: BlenderBot-9.4B 的每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1795
  prefs: []
  type: TYPE_TB
  zh: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1796
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 53.7 | 51.5 | 53.0 | 53.0 | 53.0 | 54.0 |'
  id: totrans-1797
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 53.7 | 51.5 | 53.0 | 53.0 | 53.0 | 54.0 |'
- en: '| 2 | 53.2 | 53.8 | 54.2 | 52.5 | 52.2 | 52.2 |'
  id: totrans-1798
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 53.2 | 53.8 | 54.2 | 52.5 | 52.2 | 52.2 |'
- en: '| 3 | 53.3 | 49.7 | 52.0 | 54.0 | 54.2 | 55.5 |'
  id: totrans-1799
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 53.3 | 49.7 | 52.0 | 54.0 | 54.2 | 55.5 |'
- en: '| 4 | 54.0 | 55.3 | 52.5 | 54.0 | 53.5 | 53.7 |'
  id: totrans-1800
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 54.0 | 55.3 | 52.5 | 54.0 | 53.5 | 53.7 |'
- en: '| 5 | 53.3 | 52.8 | 53.5 | 53.2 | 53.5 | 53.3 |'
  id: totrans-1801
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 53.3 | 52.8 | 53.5 | 53.2 | 53.5 | 53.3 |'
- en: '| 6 | 52.7 | 52.0 | 51.7 | 53.5 | 52.8 | 53.7 |'
  id: totrans-1802
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 52.7 | 52.0 | 51.7 | 53.5 | 52.8 | 53.7 |'
- en: '| Mean | 53.4 | 52.5 | 52.8 | 53.4 | 53.2 | 53.7 |'
  id: totrans-1803
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 53.4 | 52.5 | 52.8 | 53.4 | 53.2 | 53.7 |'
- en: '| – std | 0.407 | 1.77 | 0.859 | 0.537 | 0.63 | 0.978 |'
  id: totrans-1804
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.407 | 1.77 | 0.859 | 0.537 | 0.63 | 0.978 |'
- en: '| Structured | 53.7 | 52.2 | 52.5 | 53.7 | 53.6 | 54.4 |'
  id: totrans-1805
  prefs: []
  type: TYPE_TB
  zh: '| Structured | 53.7 | 52.2 | 52.5 | 53.7 | 53.6 | 54.4 |'
- en: '| – std | 0.287 | 2.33 | 0.408 | 0.471 | 0.492 | 0.787 |'
  id: totrans-1806
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.287 | 2.33 | 0.408 | 0.471 | 0.492 | 0.787 |'
- en: '| Natural | 53.1 | 52.9 | 53.1 | 53.1 | 52.8 | 53.1 |'
  id: totrans-1807
  prefs: []
  type: TYPE_TB
  zh: '| Natural | 53.1 | 52.9 | 53.1 | 53.1 | 52.8 | 53.1 |'
- en: '| – std | 0.262 | 0.736 | 1.05 | 0.419 | 0.531 | 0.634 |'
  id: totrans-1808
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.262 | 0.736 | 1.05 | 0.419 | 0.531 | 0.634 |'
- en: 'Table 73: Accuracy per prompt template for T0-3B.'
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 73: T0-3B 的每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1810
  prefs: []
  type: TYPE_TB
  zh: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1811
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 48.7 | 49.5 | 46.5 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1812
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 48.7 | 49.5 | 46.5 | 46.7 | 46.7 | 46.7 |'
- en: '| 2 | 46.7 | 47.5 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1813
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 46.7 | 47.5 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| 3 | 49.2 | 48.3 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1814
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 49.2 | 48.3 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| 4 | 51.7 | 49.0 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1815
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 51.7 | 49.0 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| 5 | 46.7 | 49.2 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1816
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 46.7 | 49.2 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| 6 | 46.7 | 49.8 | 46.8 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1817
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 46.7 | 49.8 | 46.8 | 46.7 | 46.7 | 46.7 |'
- en: '| Mean | 48.3 | 48.9 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1818
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 48.3 | 48.9 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| – std | 1.84 | 0.773 | 0.0898 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1819
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.84 | 0.773 | 0.0898 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
- en: '| Structured | 49.9 | 48.9 | 46.6 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1820
  prefs: []
  type: TYPE_TB
  zh: '| Structured | 49.9 | 48.9 | 46.6 | 46.7 | 46.7 | 46.7 |'
- en: '| – std | 1.31 | 0.492 | 0.0943 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1821
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.31 | 0.492 | 0.0943 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
- en: '| Natural | 46.7 | 48.8 | 46.7 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1822
  prefs: []
  type: TYPE_TB
  zh: '| Natural | 46.7 | 48.8 | 46.7 | 46.7 | 46.7 | 46.7 |'
- en: '| – std | 7.11e-15 | 0.974 | 0.0471 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1823
  prefs: []
  type: TYPE_TB
  zh: '| – std | 7.11e-15 | 0.974 | 0.0471 | 7.11e-15 | 7.11e-15 | 7.11e-15 |'
- en: 'Table 74: Accuracy per prompt template for T0-11B.'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 74: T0-11B 的每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1825
  prefs: []
  type: TYPE_TB
  zh: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1826
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 57.5 | 47.7 | 47.3 | 46.8 | 46.7 | 46.7 |'
  id: totrans-1827
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 57.5 | 47.7 | 47.3 | 46.8 | 46.7 | 46.7 |'
- en: '| 2 | 49.3 | 47.5 | 46.7 | 46.7 | 46.8 | 46.7 |'
  id: totrans-1828
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 49.3 | 47.5 | 46.7 | 46.7 | 46.8 | 46.7 |'
- en: '| 3 | 65.3 | 48.8 | 47.3 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1829
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 65.3 | 48.8 | 47.3 | 46.7 | 46.7 | 46.7 |'
- en: '| 4 | 63.8 | 48.0 | 47.0 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1830
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 63.8 | 48.0 | 47.0 | 46.7 | 46.7 | 46.7 |'
- en: '| 5 | 48.0 | 47.2 | 46.7 | 46.7 | 47.0 | 46.8 |'
  id: totrans-1831
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 48.0 | 47.2 | 46.7 | 46.7 | 47.0 | 46.8 |'
- en: '| 6 | 49.7 | 47.5 | 47.0 | 46.8 | 47.0 | 47.0 |'
  id: totrans-1832
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 49.7 | 47.5 | 47.0 | 46.8 | 47.0 | 47.0 |'
- en: '| Mean | 55.6 | 47.8 | 47.0 | 46.7 | 46.8 | 46.8 |'
  id: totrans-1833
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 55.6 | 47.8 | 47.0 | 46.7 | 46.8 | 46.8 |'
- en: '| – std | 7.04 | 0.515 | 0.245 | 0.0471 | 0.134 | 0.111 |'
  id: totrans-1834
  prefs: []
  type: TYPE_TB
  zh: '| – std | 7.04 | 0.515 | 0.245 | 0.0471 | 0.134 | 0.111 |'
- en: '| Structured | 62.2 | 48.2 | 47.2 | 46.7 | 46.7 | 46.7 |'
  id: totrans-1835
  prefs: []
  type: TYPE_TB
  zh: '| Structured | 62.2 | 48.2 | 47.2 | 46.7 | 46.7 | 46.7 |'
- en: '| – std | 3.38 | 0.464 | 0.141 | 0.0471 | 7.11e-15 | 7.11e-15 |'
  id: totrans-1836
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.38 | 0.464 | 0.141 | 0.0471 | 7.11e-15 | 7.11e-15 |'
- en: '| Natural | 49.0 | 47.4 | 46.8 | 46.7 | 46.9 | 46.8 |'
  id: totrans-1837
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 49.0 | 47.4 | 46.8 | 46.7 | 46.9 | 46.8 |'
- en: '| – std | 0.726 | 0.141 | 0.141 | 0.0471 | 0.0943 | 0.125 |'
  id: totrans-1838
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.726 | 0.141 | 0.141 | 0.0471 | 0.0943 | 0.125 |'
- en: 'Table 75: Accuracy per prompt template for Flan-T5-780M.'
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 表75：Flan-T5-780M每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1840
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1841
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 64.5 | 63.3 | 62.2 | 60.7 | 61.5 | 60.2 |'
  id: totrans-1842
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64.5 | 63.3 | 62.2 | 60.7 | 61.5 | 60.2 |'
- en: '| 2 | 66.5 | 65.8 | 65.3 | 62.8 | 65.5 | 65.0 |'
  id: totrans-1843
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 66.5 | 65.8 | 65.3 | 62.8 | 65.5 | 65.0 |'
- en: '| 3 | 61.7 | 60.2 | 58.8 | 60.8 | 59.8 | 59.7 |'
  id: totrans-1844
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 61.7 | 60.2 | 58.8 | 60.8 | 59.8 | 59.7 |'
- en: '| 4 | 58.0 | 50.2 | 50.7 | 51.3 | 52.3 | 54.8 |'
  id: totrans-1845
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 58.0 | 50.2 | 50.7 | 51.3 | 52.3 | 54.8 |'
- en: '| 5 | 63.8 | 69.0 | 64.3 | 63.2 | 65.2 | 65.5 |'
  id: totrans-1846
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 63.8 | 69.0 | 64.3 | 63.2 | 65.2 | 65.5 |'
- en: '| 6 | 65.3 | 68.8 | 64.8 | 62.3 | 64.7 | 63.8 |'
  id: totrans-1847
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 65.3 | 68.8 | 64.8 | 62.3 | 64.7 | 63.8 |'
- en: '| Mean | 63.3 | 62.9 | 61.0 | 60.2 | 61.5 | 61.5 |'
  id: totrans-1848
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 63.3 | 62.9 | 61.0 | 60.2 | 61.5 | 61.5 |'
- en: '| – std | 2.79 | 6.44 | 5.1 | 4.08 | 4.61 | 3.73 |'
  id: totrans-1849
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.79 | 6.44 | 5.1 | 4.08 | 4.61 | 3.73 |'
- en: '| Structured | 61.4 | 57.9 | 57.2 | 57.6 | 57.9 | 58.2 |'
  id: totrans-1850
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 61.4 | 57.9 | 57.2 | 57.6 | 57.9 | 58.2 |'
- en: '| – std | 2.66 | 5.59 | 4.82 | 4.45 | 4.0 | 2.44 |'
  id: totrans-1851
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.66 | 5.59 | 4.82 | 4.45 | 4.0 | 2.44 |'
- en: '| Natural | 65.2 | 67.9 | 64.8 | 62.8 | 65.1 | 64.8 |'
  id: totrans-1852
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 65.2 | 67.9 | 64.8 | 62.8 | 65.1 | 64.8 |'
- en: '| – std | 1.1 | 1.46 | 0.408 | 0.368 | 0.33 | 0.713 |'
  id: totrans-1853
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.1 | 1.46 | 0.408 | 0.368 | 0.33 | 0.713 |'
- en: 'Table 76: Accuracy per prompt template for Flan-T5-3B.'
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: 表76：Flan-T5-3B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1855
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1856
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 54.7 | 58.8 | 56.8 | 56.7 | 57.5 | 60.0 |'
  id: totrans-1857
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 54.7 | 58.8 | 56.8 | 56.7 | 57.5 | 60.0 |'
- en: '| 2 | 51.2 | 50.8 | 59.0 | 59.2 | 59.0 | 59.7 |'
  id: totrans-1858
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 51.2 | 50.8 | 59.0 | 59.2 | 59.0 | 59.7 |'
- en: '| 3 | 54.8 | 51.3 | 49.7 | 49.0 | 48.7 | 48.5 |'
  id: totrans-1859
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 54.8 | 51.3 | 49.7 | 49.0 | 48.7 | 48.5 |'
- en: '| 4 | 55.3 | 50.0 | 48.0 | 49.0 | 49.3 | 50.8 |'
  id: totrans-1860
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 55.3 | 50.0 | 48.0 | 49.0 | 49.3 | 50.8 |'
- en: '| 5 | 51.0 | 54.3 | 57.2 | 58.0 | 58.0 | 57.8 |'
  id: totrans-1861
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 51.0 | 54.3 | 57.2 | 58.0 | 58.0 | 57.8 |'
- en: '| 6 | 48.0 | 51.2 | 58.7 | 59.0 | 58.0 | 59.8 |'
  id: totrans-1862
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 48.0 | 51.2 | 58.7 | 59.0 | 58.0 | 59.8 |'
- en: '| Mean | 52.5 | 52.7 | 54.9 | 55.1 | 55.1 | 56.1 |'
  id: totrans-1863
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 52.5 | 52.7 | 54.9 | 55.1 | 55.1 | 56.1 |'
- en: '| – std | 2.65 | 3.02 | 4.37 | 4.42 | 4.33 | 4.67 |'
  id: totrans-1864
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.65 | 3.02 | 4.37 | 4.42 | 4.33 | 4.67 |'
- en: '| Structured | 54.9 | 53.4 | 51.5 | 51.6 | 51.8 | 53.1 |'
  id: totrans-1865
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 54.9 | 53.4 | 51.5 | 51.6 | 51.8 | 53.1 |'
- en: '| – std | 0.262 | 3.88 | 3.81 | 3.63 | 4.01 | 4.97 |'
  id: totrans-1866
  prefs: []
  type: TYPE_TB
  zh: '| – std | 0.262 | 3.88 | 3.81 | 3.63 | 4.01 | 4.97 |'
- en: '| Natural | 50.1 | 52.1 | 58.3 | 58.7 | 58.3 | 59.1 |'
  id: totrans-1867
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 50.1 | 52.1 | 58.3 | 58.7 | 58.3 | 59.1 |'
- en: '| – std | 1.46 | 1.56 | 0.787 | 0.525 | 0.471 | 0.92 |'
  id: totrans-1868
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.46 | 1.56 | 0.787 | 0.525 | 0.471 | 0.92 |'
- en: 'Table 77: Accuracy per prompt template for Flan-T5-11B.'
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: 表77：Flan-T5-11B每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1870
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1871
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 64.3 | 61.0 | 63.7 | 65.0 | 62.5 | 64.3 |'
  id: totrans-1872
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 64.3 | 61.0 | 63.7 | 65.0 | 62.5 | 64.3 |'
- en: '| 2 | 61.5 | 59.7 | 63.2 | 62.3 | 64.0 | 68.0 |'
  id: totrans-1873
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.5 | 59.7 | 63.2 | 62.3 | 64.0 | 68.0 |'
- en: '| 3 | 56.5 | 63.0 | 60.2 | 57.3 | 56.7 | 56.8 |'
  id: totrans-1874
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.5 | 63.0 | 60.2 | 57.3 | 56.7 | 56.8 |'
- en: '| 4 | 61.7 | 47.7 | 51.7 | 50.3 | 50.3 | 49.5 |'
  id: totrans-1875
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 61.7 | 47.7 | 51.7 | 50.3 | 50.3 | 49.5 |'
- en: '| 5 | 61.5 | 55.8 | 64.8 | 64.7 | 65.5 | 66.3 |'
  id: totrans-1876
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 61.5 | 55.8 | 64.8 | 64.7 | 65.5 | 66.3 |'
- en: '| 6 | 59.2 | 57.5 | 66.3 | 63.7 | 66.0 | 67.7 |'
  id: totrans-1877
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 59.2 | 57.5 | 66.3 | 63.7 | 66.0 | 67.7 |'
- en: '| Mean | 60.8 | 57.4 | 61.7 | 60.5 | 60.8 | 62.1 |'
  id: totrans-1878
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 60.8 | 57.4 | 61.7 | 60.5 | 60.8 | 62.1 |'
- en: '| – std | 2.42 | 4.94 | 4.82 | 5.25 | 5.62 | 6.78 |'
  id: totrans-1879
  prefs: []
  type: TYPE_TB
  zh: '| – std | 2.42 | 4.94 | 4.82 | 5.25 | 5.62 | 6.78 |'
- en: '| Structured | 60.8 | 57.2 | 58.5 | 57.5 | 56.5 | 56.9 |'
  id: totrans-1880
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 60.8 | 57.2 | 58.5 | 57.5 | 56.5 | 56.9 |'
- en: '| – std | 3.24 | 6.79 | 5.04 | 6.0 | 4.98 | 6.04 |'
  id: totrans-1881
  prefs: []
  type: TYPE_TB
  zh: '| – std | 3.24 | 6.79 | 5.04 | 6.0 | 4.98 | 6.04 |'
- en: '| Natural | 60.7 | 57.7 | 64.8 | 63.6 | 65.2 | 67.3 |'
  id: totrans-1882
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 60.7 | 57.7 | 64.8 | 63.6 | 65.2 | 67.3 |'
- en: '| – std | 1.08 | 1.6 | 1.27 | 0.984 | 0.85 | 0.741 |'
  id: totrans-1883
  prefs: []
  type: TYPE_TB
  zh: '| – std | 1.08 | 1.6 | 1.27 | 0.984 | 0.85 | 0.741 |'
- en: 'Table 78: Accuracy per prompt template for Cohere-command-6b.'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: 表78：Cohere-command-6b每个提示模板的准确率。
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1885
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1886
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 65.0 | 63.2 | 71.7 | 70.2 | 71.3 | 70.3 |'
  id: totrans-1887
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 65.0 | 63.2 | 71.7 | 70.2 | 71.3 | 70.3 |'
- en: '| 2 | 64.8 | 64.2 | 66.8 | 67.5 | 69.8 | 71.7 |'
  id: totrans-1888
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 64.8 | 64.2 | 66.8 | 67.5 | 69.8 | 71.7 |'
- en: '| 3 | 68.0 | 65.5 | 69.2 | 65.5 | 66.8 | 68.2 |'
  id: totrans-1889
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 68.0 | 65.5 | 69.2 | 65.5 | 66.8 | 68.2 |'
- en: '| 4 | 70.0 | 68.5 | 69.2 | 71.2 | 71.7 | 73.2 |'
  id: totrans-1890
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 70.0 | 68.5 | 69.2 | 71.2 | 71.7 | 73.2 |'
- en: '| 5 | 66.3 | 65.0 | 66.8 | 67.5 | 70.5 | 69.8 |'
  id: totrans-1891
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 66.3 | 65.0 | 66.8 | 67.5 | 70.5 | 69.8 |'
- en: '| 6 | 63.7 | 63.7 | 67.7 | 67.5 | 70.0 | 69.5 |'
  id: totrans-1892
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 63.7 | 63.7 | 67.7 | 67.5 | 70.0 | 69.5 |'
- en: '| Mean | 66.3 | 65.0 | 68.6 | 68.2 | 70.0 | 70.5 |'
  id: totrans-1893
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 66.3 | 65.0 | 68.6 | 68.2 | 70.0 | 70.5 |'
- en: '| – std | 2.13 | 1.74 | 1.71 | 1.9 | 1.59 | 1.61 |'
  id: totrans-1894
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.13 | 1.74 | 1.71 | 1.9 | 1.59 | 1.61 |'
- en: '| Structured | 67.7 | 65.7 | 70.0 | 69.0 | 69.9 | 70.6 |'
  id: totrans-1895
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 67.7 | 65.7 | 70.0 | 69.0 | 69.9 | 70.6 |'
- en: '| – std | 2.05 | 2.17 | 1.18 | 2.49 | 2.22 | 2.05 |'
  id: totrans-1896
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.05 | 2.17 | 1.18 | 2.49 | 2.22 | 2.05 |'
- en: '| Natural | 64.9 | 64.3 | 67.1 | 67.5 | 70.1 | 70.3 |'
  id: totrans-1897
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 64.9 | 64.3 | 67.1 | 67.5 | 70.1 | 70.3 |'
- en: '| – std | 1.07 | 0.535 | 0.424 | 0.0 | 0.294 | 0.974 |'
  id: totrans-1898
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.07 | 0.535 | 0.424 | 0.0 | 0.294 | 0.974 |'
- en: 'Table 79: Accuracy per prompt template for Cohere-command-52b.'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: '表 79: 每个提示模板的准确性（Cohere-command-52b）。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1900
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1901
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 65.2 | 74.2 | 77.8 | 75.8 | 75.5 | 76.0 |'
  id: totrans-1902
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 65.2 | 74.2 | 77.8 | 75.8 | 75.5 | 76.0 |'
- en: '| 2 | 61.7 | 72.0 | 73.5 | 75.3 | 75.2 | 74.5 |'
  id: totrans-1903
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 61.7 | 72.0 | 73.5 | 75.3 | 75.2 | 74.5 |'
- en: '| 3 | 56.7 | 74.5 | 77.3 | 77.2 | 76.5 | 75.0 |'
  id: totrans-1904
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 56.7 | 74.5 | 77.3 | 77.2 | 76.5 | 75.0 |'
- en: '| 4 | 68.2 | 70.7 | 76.0 | 74.8 | 75.3 | 75.3 |'
  id: totrans-1905
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 68.2 | 70.7 | 76.0 | 74.8 | 75.3 | 75.3 |'
- en: '| 5 | 54.8 | 72.7 | 74.8 | 76.2 | 74.8 | 74.7 |'
  id: totrans-1906
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 54.8 | 72.7 | 74.8 | 76.2 | 74.8 | 74.7 |'
- en: '| 6 | 54.8 | 73.0 | 73.0 | 74.5 | 75.0 | 74.5 |'
  id: totrans-1907
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 54.8 | 73.0 | 73.0 | 74.5 | 75.0 | 74.5 |'
- en: '| Mean | 60.2 | 72.8 | 75.4 | 75.6 | 75.4 | 75.0 |'
  id: totrans-1908
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 60.2 | 72.8 | 75.4 | 75.6 | 75.4 | 75.0 |'
- en: '| – std | 5.19 | 1.29 | 1.8 | 0.903 | 0.546 | 0.529 |'
  id: totrans-1909
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 5.19 | 1.29 | 1.8 | 0.903 | 0.546 | 0.529 |'
- en: '| Structured | 63.4 | 73.1 | 77.0 | 75.9 | 75.8 | 75.4 |'
  id: totrans-1910
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 63.4 | 73.1 | 77.0 | 75.9 | 75.8 | 75.4 |'
- en: '| – std | 4.87 | 1.72 | 0.759 | 0.984 | 0.525 | 0.419 |'
  id: totrans-1911
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 4.87 | 1.72 | 0.759 | 0.984 | 0.525 | 0.419 |'
- en: '| Natural | 57.1 | 72.6 | 73.8 | 75.3 | 75.0 | 74.6 |'
  id: totrans-1912
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 57.1 | 72.6 | 73.8 | 75.3 | 75.0 | 74.6 |'
- en: '| – std | 3.25 | 0.419 | 0.759 | 0.694 | 0.163 | 0.0943 |'
  id: totrans-1913
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.25 | 0.419 | 0.759 | 0.694 | 0.163 | 0.0943 |'
- en: 'Table 80: Accuracy per prompt template for text-ada-001-unknown.'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: '表 80: 每个提示模板的准确性（text-ada-001-unknown）。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1915
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1916
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 60.8 | 62.8 | 60.8 | 59.0 | 58.7 | 58.8 |'
  id: totrans-1917
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 60.8 | 62.8 | 60.8 | 59.0 | 58.7 | 58.8 |'
- en: '| 2 | 50.7 | 56.3 | 54.8 | 56.0 | 57.7 | 52.7 |'
  id: totrans-1918
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 50.7 | 56.3 | 54.8 | 56.0 | 57.7 | 52.7 |'
- en: '| 3 | 63.7 | 58.5 | 60.8 | 59.0 | 56.7 | 57.5 |'
  id: totrans-1919
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 63.7 | 58.5 | 60.8 | 59.0 | 56.7 | 57.5 |'
- en: '| 4 | 61.8 | 56.3 | 59.3 | 58.3 | 61.0 | 56.7 |'
  id: totrans-1920
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 61.8 | 56.3 | 59.3 | 58.3 | 61.0 | 56.7 |'
- en: '| 5 | 53.3 | 55.5 | 55.2 | 55.7 | 58.0 | 54.3 |'
  id: totrans-1921
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 53.3 | 55.5 | 55.2 | 55.7 | 58.0 | 54.3 |'
- en: '| 6 | 48.7 | 54.7 | 54.7 | 56.2 | 57.7 | 53.5 |'
  id: totrans-1922
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 48.7 | 54.7 | 54.7 | 56.2 | 57.7 | 53.5 |'
- en: '| Mean | 56.5 | 57.3 | 57.6 | 57.4 | 58.3 | 55.6 |'
  id: totrans-1923
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 56.5 | 57.3 | 57.6 | 57.4 | 58.3 | 55.6 |'
- en: '| – std | 5.82 | 2.7 | 2.75 | 1.43 | 1.34 | 2.22 |'
  id: totrans-1924
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 5.82 | 2.7 | 2.75 | 1.43 | 1.34 | 2.22 |'
- en: '| Structured | 62.1 | 59.2 | 60.3 | 58.8 | 58.8 | 57.7 |'
  id: totrans-1925
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 62.1 | 59.2 | 60.3 | 58.8 | 58.8 | 57.7 |'
- en: '| – std | 1.2 | 2.7 | 0.707 | 0.33 | 1.76 | 0.865 |'
  id: totrans-1926
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.2 | 2.7 | 0.707 | 0.33 | 1.76 | 0.865 |'
- en: '| Natural | 50.9 | 55.5 | 54.9 | 56.0 | 57.8 | 53.5 |'
  id: totrans-1927
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 50.9 | 55.5 | 54.9 | 56.0 | 57.8 | 53.5 |'
- en: '| – std | 1.88 | 0.653 | 0.216 | 0.205 | 0.141 | 0.653 |'
  id: totrans-1928
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.88 | 0.653 | 0.216 | 0.205 | 0.141 | 0.653 |'
- en: 'Table 81: Accuracy per prompt template for text-babbage-001-unknown.'
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
  zh: '表 81: 每个提示模板的准确性（text-babbage-001-unknown）。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1930
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1931
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 67.5 | 64.0 | 66.3 | 63.0 | 64.0 | 64.7 |'
  id: totrans-1932
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 67.5 | 64.0 | 66.3 | 63.0 | 64.0 | 64.7 |'
- en: '| 2 | 63.0 | 62.5 | 66.2 | 64.2 | 66.5 | 68.2 |'
  id: totrans-1933
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 63.0 | 62.5 | 66.2 | 64.2 | 66.5 | 68.2 |'
- en: '| 3 | 65.3 | 65.2 | 66.0 | 63.2 | 64.7 | 64.5 |'
  id: totrans-1934
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 65.3 | 65.2 | 66.0 | 63.2 | 64.7 | 64.5 |'
- en: '| 4 | 65.2 | 63.5 | 65.7 | 62.7 | 63.0 | 64.8 |'
  id: totrans-1935
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 65.2 | 63.5 | 65.7 | 62.7 | 63.0 | 64.8 |'
- en: '| 5 | 61.8 | 64.3 | 66.5 | 64.0 | 66.3 | 67.8 |'
  id: totrans-1936
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 61.8 | 64.3 | 66.5 | 64.0 | 66.3 | 67.8 |'
- en: '| 6 | 64.0 | 63.8 | 66.2 | 64.2 | 66.7 | 66.0 |'
  id: totrans-1937
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 64.0 | 63.8 | 66.2 | 64.2 | 66.7 | 66.0 |'
- en: '| Mean | 64.5 | 63.9 | 66.1 | 63.6 | 65.2 | 66.0 |'
  id: totrans-1938
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 64.5 | 63.9 | 66.1 | 63.6 | 65.2 | 66.0 |'
- en: '| – std | 1.82 | 0.815 | 0.25 | 0.605 | 1.4 | 1.5 |'
  id: totrans-1939
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.82 | 0.815 | 0.25 | 0.605 | 1.4 | 1.5 |'
- en: '| Structured | 66.0 | 64.2 | 66.0 | 63.0 | 63.9 | 64.7 |'
  id: totrans-1940
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 66.0 | 64.2 | 66.0 | 63.0 | 63.9 | 64.7 |'
- en: '| – std | 1.06 | 0.713 | 0.245 | 0.205 | 0.698 | 0.125 |'
  id: totrans-1941
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.06 | 0.713 | 0.245 | 0.205 | 0.698 | 0.125 |'
- en: '| Natural | 62.9 | 63.5 | 66.3 | 64.1 | 66.5 | 67.3 |'
  id: totrans-1942
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 62.9 | 63.5 | 66.3 | 64.1 | 66.5 | 67.3 |'
- en: '| – std | 0.899 | 0.759 | 0.141 | 0.0943 | 0.163 | 0.957 |'
  id: totrans-1943
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.899 | 0.759 | 0.141 | 0.0943 | 0.163 | 0.957 |'
- en: 'Table 82: Accuracy per prompt template for text-curie-001-unknown.'
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: '表 82: 每个提示模板的准确性（text-curie-001-unknown）。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1945
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1946
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 70.7 | 70.2 | 72.5 | 70.8 | 70.8 | 70.7 |'
  id: totrans-1947
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 70.7 | 70.2 | 72.5 | 70.8 | 70.8 | 70.7 |'
- en: '| 2 | 66.5 | 59.3 | 70.3 | 69.7 | 68.3 | 71.2 |'
  id: totrans-1948
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 66.5 | 59.3 | 70.3 | 69.7 | 68.3 | 71.2 |'
- en: '| 3 | 73.2 | 70.2 | 73.5 | 69.7 | 71.8 | 69.7 |'
  id: totrans-1949
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 73.2 | 70.2 | 73.5 | 69.7 | 71.8 | 69.7 |'
- en: '| 4 | 71.3 | 68.0 | 71.0 | 69.8 | 71.0 | 69.0 |'
  id: totrans-1950
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 71.3 | 68.0 | 71.0 | 69.8 | 71.0 | 69.0 |'
- en: '| 5 | 65.5 | 58.8 | 70.0 | 70.2 | 68.5 | 70.7 |'
  id: totrans-1951
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 65.5 | 58.8 | 70.0 | 70.2 | 68.5 | 70.7 |'
- en: '| 6 | 66.5 | 59.8 | 70.7 | 70.8 | 69.0 | 70.8 |'
  id: totrans-1952
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 66.5 | 59.8 | 70.7 | 70.8 | 69.0 | 70.8 |'
- en: '| Mean | 69.0 | 64.4 | 71.3 | 70.2 | 69.9 | 70.4 |'
  id: totrans-1953
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 69.0 | 64.4 | 71.3 | 70.2 | 69.9 | 70.4 |'
- en: '| – std | 2.9 | 5.14 | 1.25 | 0.478 | 1.35 | 0.754 |'
  id: totrans-1954
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.9 | 5.14 | 1.25 | 0.478 | 1.35 | 0.754 |'
- en: '| Structured | 71.7 | 69.5 | 72.3 | 70.1 | 71.2 | 69.8 |'
  id: totrans-1955
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 71.7 | 69.5 | 72.3 | 70.1 | 71.2 | 69.8 |'
- en: '| – std | 1.07 | 1.04 | 1.03 | 0.497 | 0.432 | 0.698 |'
  id: totrans-1956
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.07 | 1.04 | 1.03 | 0.497 | 0.432 | 0.698 |'
- en: '| Natural | 66.2 | 59.3 | 70.3 | 70.2 | 68.6 | 70.9 |'
  id: totrans-1957
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 66.2 | 59.3 | 70.3 | 70.2 | 68.6 | 70.9 |'
- en: '| – std | 0.471 | 0.408 | 0.287 | 0.45 | 0.294 | 0.216 |'
  id: totrans-1958
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.471 | 0.408 | 0.287 | 0.45 | 0.294 | 0.216 |'
- en: 'Table 83: Accuracy per prompt template for text-davinci-001-unknown.'
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: '表 83: text-davinci-001-unknown 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1960
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1961
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 76.5 | 73.7 | 75.7 | 75.7 | 76.3 | 76.8 |'
  id: totrans-1962
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 76.5 | 73.7 | 75.7 | 75.7 | 76.3 | 76.8 |'
- en: '| 2 | 72.0 | 72.5 | 74.3 | 75.2 | 76.0 | 75.3 |'
  id: totrans-1963
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 72.0 | 72.5 | 74.3 | 75.2 | 76.0 | 75.3 |'
- en: '| 3 | 74.8 | 74.2 | 75.7 | 77.2 | 75.8 | 76.8 |'
  id: totrans-1964
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 74.8 | 74.2 | 75.7 | 77.2 | 75.8 | 76.8 |'
- en: '| 4 | 68.0 | 70.2 | 72.8 | 72.8 | 73.3 | 75.0 |'
  id: totrans-1965
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 68.0 | 70.2 | 72.8 | 72.8 | 73.3 | 75.0 |'
- en: '| 5 | 72.5 | 73.2 | 74.3 | 74.3 | 75.3 | 75.7 |'
  id: totrans-1966
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 72.5 | 73.2 | 74.3 | 74.3 | 75.3 | 75.7 |'
- en: '| 6 | 70.0 | 72.7 | 74.3 | 74.7 | 75.0 | 75.3 |'
  id: totrans-1967
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 70.0 | 72.7 | 74.3 | 74.7 | 75.0 | 75.3 |'
- en: '| Mean | 72.3 | 72.7 | 74.5 | 75.0 | 75.3 | 75.8 |'
  id: totrans-1968
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 72.3 | 72.7 | 74.5 | 75.0 | 75.3 | 75.8 |'
- en: '| – std | 2.82 | 1.28 | 0.991 | 1.34 | 0.986 | 0.724 |'
  id: totrans-1969
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.82 | 1.28 | 0.991 | 1.34 | 0.986 | 0.724 |'
- en: '| Structured | 73.1 | 72.7 | 74.7 | 75.2 | 75.1 | 76.2 |'
  id: totrans-1970
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 73.1 | 72.7 | 74.7 | 75.2 | 75.1 | 76.2 |'
- en: '| – std | 3.67 | 1.78 | 1.37 | 1.83 | 1.31 | 0.849 |'
  id: totrans-1971
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.67 | 1.78 | 1.37 | 1.83 | 1.31 | 0.849 |'
- en: '| Natural | 71.5 | 72.8 | 74.3 | 74.7 | 75.4 | 75.4 |'
  id: totrans-1972
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 71.5 | 72.8 | 74.3 | 74.7 | 75.4 | 75.4 |'
- en: '| – std | 1.08 | 0.294 | 0.0 | 0.368 | 0.419 | 0.189 |'
  id: totrans-1973
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.08 | 0.294 | 0.0 | 0.368 | 0.419 | 0.189 |'
- en: 'Table 84: Accuracy per prompt template for text-davinci-002-unknown.'
  id: totrans-1974
  prefs: []
  type: TYPE_NORMAL
  zh: '表 84: text-davinci-002-unknown 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1975
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1976
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 73.7 | 76.2 | 80.2 | 79.5 | 79.8 | 80.7 |'
  id: totrans-1977
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 73.7 | 76.2 | 80.2 | 79.5 | 79.8 | 80.7 |'
- en: '| 2 | 69.5 | 73.5 | 78.2 | 78.5 | 76.7 | 79.8 |'
  id: totrans-1978
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 69.5 | 73.5 | 78.2 | 78.5 | 76.7 | 79.8 |'
- en: '| 3 | 73.0 | 78.7 | 82.8 | 82.8 | 82.7 | 82.8 |'
  id: totrans-1979
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 73.0 | 78.7 | 82.8 | 82.8 | 82.7 | 82.8 |'
- en: '| 4 | 71.3 | 79.7 | 80.5 | 80.8 | 82.0 | 81.5 |'
  id: totrans-1980
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 71.3 | 79.7 | 80.5 | 80.8 | 82.0 | 81.5 |'
- en: '| 5 | 67.5 | 72.5 | 79.2 | 79.2 | 77.0 | 79.8 |'
  id: totrans-1981
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 67.5 | 72.5 | 79.2 | 79.2 | 77.0 | 79.8 |'
- en: '| 6 | 68.5 | 73.2 | 76.5 | 76.5 | 76.2 | 79.2 |'
  id: totrans-1982
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 68.5 | 73.2 | 76.5 | 76.5 | 76.2 | 79.2 |'
- en: '| Mean | 70.6 | 75.6 | 79.6 | 79.5 | 79.1 | 80.6 |'
  id: totrans-1983
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 70.6 | 75.6 | 79.6 | 79.5 | 79.1 | 80.6 |'
- en: '| – std | 2.28 | 2.79 | 1.96 | 1.94 | 2.6 | 1.22 |'
  id: totrans-1984
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.28 | 2.79 | 1.96 | 1.94 | 2.6 | 1.22 |'
- en: '| Structured | 72.7 | 78.2 | 81.2 | 81.0 | 81.5 | 81.7 |'
  id: totrans-1985
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 72.7 | 78.2 | 81.2 | 81.0 | 81.5 | 81.7 |'
- en: '| – std | 1.01 | 1.47 | 1.16 | 1.36 | 1.24 | 0.865 |'
  id: totrans-1986
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.01 | 1.47 | 1.16 | 1.36 | 1.24 | 0.865 |'
- en: '| Natural | 68.5 | 73.1 | 78.0 | 78.1 | 76.6 | 79.6 |'
  id: totrans-1987
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 68.5 | 73.1 | 78.0 | 78.1 | 76.6 | 79.6 |'
- en: '| – std | 0.816 | 0.419 | 1.11 | 1.14 | 0.33 | 0.283 |'
  id: totrans-1988
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.816 | 0.419 | 1.11 | 1.14 | 0.33 | 0.283 |'
- en: 'Table 85: Accuracy per prompt template for text-davinci-003-unknown.'
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: '表 85: text-davinci-003-unknown 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-1990
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1991
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 74.3 | 71.7 | 79.8 | 80.2 | 80.7 | 80.3 |'
  id: totrans-1992
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 74.3 | 71.7 | 79.8 | 80.2 | 80.7 | 80.3 |'
- en: '| 2 | 71.8 | 75.0 | 80.2 | 78.8 | 78.2 | 78.3 |'
  id: totrans-1993
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 71.8 | 75.0 | 80.2 | 78.8 | 78.2 | 78.3 |'
- en: '| 3 | 71.8 | 73.7 | 79.7 | 79.5 | 79.2 | 81.2 |'
  id: totrans-1994
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 71.8 | 73.7 | 79.7 | 79.5 | 79.2 | 81.2 |'
- en: '| 4 | 65.2 | 74.2 | 78.5 | 78.2 | 79.7 | 79.5 |'
  id: totrans-1995
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 65.2 | 74.2 | 78.5 | 78.2 | 79.7 | 79.5 |'
- en: '| 5 | 72.2 | 75.3 | 80.2 | 78.5 | 78.2 | 78.8 |'
  id: totrans-1996
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 72.2 | 75.3 | 80.2 | 78.5 | 78.2 | 78.8 |'
- en: '| 6 | 72.2 | 76.0 | 79.7 | 78.8 | 78.3 | 79.0 |'
  id: totrans-1997
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 72.2 | 76.0 | 79.7 | 78.8 | 78.3 | 79.0 |'
- en: '| Mean | 71.2 | 74.3 | 79.7 | 79.0 | 79.0 | 79.5 |'
  id: totrans-1998
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 71.2 | 74.3 | 79.7 | 79.0 | 79.0 | 79.5 |'
- en: '| – std | 2.84 | 1.38 | 0.57 | 0.666 | 0.929 | 0.975 |'
  id: totrans-1999
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 2.84 | 1.38 | 0.57 | 0.666 | 0.929 | 0.975 |'
- en: '| Structured | 70.4 | 73.2 | 79.3 | 79.3 | 79.9 | 80.3 |'
  id: totrans-2000
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 70.4 | 73.2 | 79.3 | 79.3 | 79.9 | 80.3 |'
- en: '| – std | 3.84 | 1.08 | 0.591 | 0.829 | 0.624 | 0.694 |'
  id: totrans-2001
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 3.84 | 1.08 | 0.591 | 0.829 | 0.624 | 0.694 |'
- en: '| Natural | 72.1 | 75.4 | 80.0 | 78.7 | 78.2 | 78.7 |'
  id: totrans-2002
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 72.1 | 75.4 | 80.0 | 78.7 | 78.2 | 78.7 |'
- en: '| – std | 0.189 | 0.419 | 0.236 | 0.141 | 0.0471 | 0.294 |'
  id: totrans-2003
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.189 | 0.419 | 0.236 | 0.141 | 0.0471 | 0.294 |'
- en: 'Table 86: Accuracy per prompt template for ChatGPT-unknown.'
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: '表 86: ChatGPT-unknown 每个提示模板的准确率。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-2005
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2006
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 77.8 | 73.3 | 72.7 | 72.7 | 74.2 | 74.5 |'
  id: totrans-2007
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 77.8 | 73.3 | 72.7 | 72.7 | 74.2 | 74.5 |'
- en: '| 2 | 73.2 | 76.2 | 78.7 | 78.2 | 79.7 | 79.2 |'
  id: totrans-2008
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 73.2 | 76.2 | 78.7 | 78.2 | 79.7 | 79.2 |'
- en: '| 3 | 72.7 | 74.0 | 74.3 | 74.7 | 75.0 | 74.8 |'
  id: totrans-2009
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 72.7 | 74.0 | 74.3 | 74.7 | 75.0 | 74.8 |'
- en: '| 4 | 59.3 | 73.7 | 60.8 | 63.5 | 66.0 | 68.0 |'
  id: totrans-2010
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 59.3 | 73.7 | 60.8 | 63.5 | 66.0 | 68.0 |'
- en: '| 5 | 74.7 | 76.8 | 77.8 | 77.7 | 79.3 | 78.8 |'
  id: totrans-2011
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 74.7 | 76.8 | 77.8 | 77.7 | 79.3 | 78.8 |'
- en: '| 6 | 74.8 | 76.7 | 79.0 | 79.0 | 79.2 | 78.5 |'
  id: totrans-2012
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 74.8 | 76.7 | 79.0 | 79.0 | 79.2 | 78.5 |'
- en: '| Mean | 72.1 | 75.1 | 73.9 | 74.3 | 75.6 | 75.6 |'
  id: totrans-2013
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 72.1 | 75.1 | 73.9 | 74.3 | 75.6 | 75.6 |'
- en: '| – std | 5.94 | 1.48 | 6.29 | 5.29 | 4.79 | 3.9 |'
  id: totrans-2014
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 5.94 | 1.48 | 6.29 | 5.29 | 4.79 | 3.9 |'
- en: '| Structured | 69.9 | 73.7 | 69.3 | 70.3 | 71.7 | 72.4 |'
  id: totrans-2015
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 69.9 | 73.7 | 69.3 | 70.3 | 71.7 | 72.4 |'
- en: '| – std | 7.8 | 0.287 | 6.02 | 4.88 | 4.07 | 3.14 |'
  id: totrans-2016
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 7.8 | 0.287 | 6.02 | 4.88 | 4.07 | 3.14 |'
- en: '| Natural | 74.2 | 76.6 | 78.5 | 78.3 | 79.4 | 78.8 |'
  id: totrans-2017
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 74.2 | 76.6 | 78.5 | 78.3 | 79.4 | 78.8 |'
- en: '| – std | 0.732 | 0.262 | 0.51 | 0.535 | 0.216 | 0.287 |'
  id: totrans-2018
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.732 | 0.262 | 0.51 | 0.535 | 0.216 | 0.287 |'
- en: 'Table 87: Accuracy per prompt template for GPT-4-unknown.'
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: '表 87: 每个提示模板的准确率 (GPT-4-unknown)。'
- en: '| Template | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
  id: totrans-2020
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | k = 0 | k = 1 | k = 5 | k = 10 | k = 15 | k = 30 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-2021
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 83.3 | 82.7 | 84.0 | 84.2 | 85.5 | 84.5 |'
  id: totrans-2022
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 83.3 | 82.7 | 84.0 | 84.2 | 85.5 | 84.5 |'
- en: '| 2 | 81.8 | 80.8 | 80.8 | 78.0 | 79.3 | 79.7 |'
  id: totrans-2023
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 81.8 | 80.8 | 80.8 | 78.0 | 79.3 | 79.7 |'
- en: '| 3 | 84.7 | 83.7 | 84.2 | 85.3 | 85.5 | 85.3 |'
  id: totrans-2024
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 84.7 | 83.7 | 84.2 | 85.3 | 85.5 | 85.3 |'
- en: '| 4 | 80.5 | 84.3 | 82.5 | 84.3 | 83.3 | 83.7 |'
  id: totrans-2025
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 80.5 | 84.3 | 82.5 | 84.3 | 83.3 | 83.7 |'
- en: '| 5 | 79.5 | 81.0 | 80.8 | 77.0 | 79.0 | 79.0 |'
  id: totrans-2026
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 79.5 | 81.0 | 80.8 | 77.0 | 79.0 | 79.0 |'
- en: '| 6 | 80.8 | 81.3 | 79.8 | 79.0 | 79.8 | 80.8 |'
  id: totrans-2027
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 80.8 | 81.3 | 79.8 | 79.0 | 79.8 | 80.8 |'
- en: '| Mean | 81.8 | 82.3 | 82.0 | 81.3 | 82.1 | 82.2 |'
  id: totrans-2028
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 81.8 | 82.3 | 82.0 | 81.3 | 82.1 | 82.2 |'
- en: '| – std | 1.76 | 1.36 | 1.67 | 3.37 | 2.81 | 2.44 |'
  id: totrans-2029
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.76 | 1.36 | 1.67 | 3.37 | 2.81 | 2.44 |'
- en: '| Structured | 82.8 | 83.6 | 83.6 | 84.6 | 84.8 | 84.5 |'
  id: totrans-2030
  prefs: []
  type: TYPE_TB
  zh: '| 结构化 | 82.8 | 83.6 | 83.6 | 84.6 | 84.8 | 84.5 |'
- en: '| – std | 1.75 | 0.66 | 0.759 | 0.497 | 1.04 | 0.653 |'
  id: totrans-2031
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 1.75 | 0.66 | 0.759 | 0.497 | 1.04 | 0.653 |'
- en: '| Natural | 80.7 | 81.0 | 80.5 | 78.0 | 79.4 | 79.8 |'
  id: totrans-2032
  prefs: []
  type: TYPE_TB
  zh: '| 自然 | 80.7 | 81.0 | 80.5 | 78.0 | 79.4 | 79.8 |'
- en: '| – std | 0.942 | 0.205 | 0.471 | 0.816 | 0.33 | 0.741 |'
  id: totrans-2033
  prefs: []
  type: TYPE_TB
  zh: '| – 标准差 | 0.942 | 0.205 | 0.471 | 0.816 | 0.33 | 0.741 |'
- en: Appendix L Timestamps API calls
  id: totrans-2034
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 L 时间戳 API 调用
- en: 'For reproducibility purposes, Table [88](#A12.T88 "Table 88 ‣ Appendix L Timestamps
    API calls ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs"), [89](#A12.T89 "Table 89 ‣ Appendix L Timestamps
    API calls ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters
    for Implicature Resolution by LLMs"), and [90](#A12.T90 "Table 90 ‣ Appendix L
    Timestamps API calls ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") contain the dates and times
    the APIs from OpenAI and Cohere were queried for the results.'
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: '为了可重复性，表格 [88](#A12.T88 "表 88 ‣ 附录 L 时间戳 API 调用 ‣ 实用理解的黄金比例: 微调策略对隐含意义解析的影响")、[89](#A12.T89
    "表 89 ‣ 附录 L 时间戳 API 调用 ‣ 实用理解的黄金比例: 微调策略对隐含意义解析的影响") 和 [90](#A12.T90 "表 90 ‣
    附录 L 时间戳 API 调用 ‣ 实用理解的黄金比例: 微调策略对隐含意义解析的影响") 包含了 OpenAI 和 Cohere 查询结果的日期和时间。'
- en: 'Table 88: Timestamp each was evaluated through OpenAI’s API (1/2).'
  id: totrans-2036
  prefs: []
  type: TYPE_NORMAL
  zh: '表 88: 每个时间戳通过 OpenAI 的 API 进行评估 (1/2)。'
- en: '| model | timestamp |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 时间戳 |'
- en: '| --- | --- |'
  id: totrans-2038
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GPT-3-ada/0-shot | 2022-09-22 13:13:29 |'
  id: totrans-2039
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/0-shot | 2022-09-22 13:13:29 |'
- en: '| GPT-3-ada/1-shot | 2022-09-22 15:11:13 |'
  id: totrans-2040
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/1-shot | 2022-09-22 15:11:13 |'
- en: '| GPT-3-ada/5-shot | 2022-09-22 15:40:12 |'
  id: totrans-2041
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/5-shot | 2022-09-22 15:40:12 |'
- en: '| GPT-3-ada/10-shot | 2022-09-22 18:14:18 |'
  id: totrans-2042
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/10-shot | 2022-09-22 18:14:18 |'
- en: '| GPT-3-ada/15-shot | 2022-09-22 19:15:29 |'
  id: totrans-2043
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/15-shot | 2022-09-22 19:15:29 |'
- en: '| GPT-3-ada/30-shot | 2022-09-22 22:47:58 |'
  id: totrans-2044
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-ada/30-shot | 2022-09-22 22:47:58 |'
- en: '| GPT-3-babbage/0-shot | 2022-09-22 23:19:05 |'
  id: totrans-2045
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/0-shot | 2022-09-22 23:19:05 |'
- en: '| GPT-3-babbage/1-shot | 2022-09-22 23:39:53 |'
  id: totrans-2046
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/1-shot | 2022-09-22 23:39:53 |'
- en: '| GPT-3-babbage/5-shot | 2022-09-23 00:01:32 |'
  id: totrans-2047
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/5-shot | 2022-09-23 00:01:32 |'
- en: '| GPT-3-babbage/10-shot | 2022-09-23 00:24:27 |'
  id: totrans-2048
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/10-shot | 2022-09-23 00:24:27 |'
- en: '| GPT-3-babbage/15-shot | 2022-09-23 00:49:13 |'
  id: totrans-2049
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/15-shot | 2022-09-23 00:49:13 |'
- en: '| GPT-3-babbage/30-shot | 2022-09-23 01:15:44 |'
  id: totrans-2050
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-babbage/30-shot | 2022-09-23 01:15:44 |'
- en: '| GPT-3-curie/0-shot | 2022-09-22 14:04:32 |'
  id: totrans-2051
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/0-shot | 2022-09-22 14:04:32 |'
- en: '| GPT-3-curie/1-shot | 2022-09-23 02:09:14 |'
  id: totrans-2052
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/1-shot | 2022-09-23 02:09:14 |'
- en: '| GPT-3-curie/5-shot | 2022-09-23 02:32:20 |'
  id: totrans-2053
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/5-shot | 2022-09-23 02:32:20 |'
- en: '| GPT-3-curie/10-shot | 2022-09-23 02:56:43 |'
  id: totrans-2054
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/10-shot | 2022-09-23 02:56:43 |'
- en: '| GPT-3-curie/15-shot | 2022-09-23 03:23:19 |'
  id: totrans-2055
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/15-shot | 2022-09-23 03:23:19 |'
- en: '| GPT-3-curie/30-shot | 2022-09-23 03:52:30 |'
  id: totrans-2056
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-curie/30-shot | 2022-09-23 03:52:30 |'
- en: '| GPT-3-davinci/0-shot | 2022-09-22 12:21:48 |'
  id: totrans-2057
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/0-shot | 2022-09-22 12:21:48 |'
- en: '| GPT-3-davinci/1-shot | 2022-09-23 14:27:15 |'
  id: totrans-2058
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/1-shot | 2022-09-23 14:27:15 |'
- en: '| GPT-3-davinci/5-shot | 2022-09-23 15:10:40 |'
  id: totrans-2059
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/5-shot | 2022-09-23 15:10:40 |'
- en: '| GPT-3-davinci/10-shot | 2022-09-23 16:04:53 |'
  id: totrans-2060
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/10-shot | 2022-09-23 16:04:53 |'
- en: '| GPT-3-davinci/15-shot | 2022-09-23 17:17:04 |'
  id: totrans-2061
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/15-shot | 2022-09-23 17:17:04 |'
- en: '| GPT-3-davinci/30-shot | 2022-09-23 18:36:38 |'
  id: totrans-2062
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3-davinci/30-shot | 2022-09-23 18:36:38 |'
- en: '| OpenAI-text-ada-001/0-shot | 2022-08-17 16:59:45 |'
  id: totrans-2063
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/0-shot | 2022-08-17 16:59:45 |'
- en: '| OpenAI-text-ada-001/1-shot | 2022-08-17 18:23:12 |'
  id: totrans-2064
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/1-shot | 2022-08-17 18:23:12 |'
- en: '| OpenAI-text-ada-001/5-shot | 2022-08-17 19:16:48 |'
  id: totrans-2065
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/5-shot | 2022-08-17 19:16:48 |'
- en: '| OpenAI-text-ada-001/10-shot | 2022-08-17 20:24:16 |'
  id: totrans-2066
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/10-shot | 2022-08-17 20:24:16 |'
- en: '| OpenAI-text-ada-001/15-shot | 2022-08-17 21:21:46 |'
  id: totrans-2067
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/15-shot | 2022-08-17 21:21:46 |'
- en: '| OpenAI-text-ada-001/30-shot | 2022-08-17 22:44:47 |'
  id: totrans-2068
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-ada-001/30-shot | 2022-08-17 22:44:47 |'
- en: '| OpenAI-text-babbage-001/0-shot | 2022-08-17 11:50:44 |'
  id: totrans-2069
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/0-shot | 2022-08-17 11:50:44 |'
- en: '| OpenAI-text-babbage-001/1-shot | 2022-08-17 12:22:08 |'
  id: totrans-2070
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/1-shot | 2022-08-17 12:22:08 |'
- en: '| OpenAI-text-babbage-001/5-shot | 2022-08-17 12:50:59 |'
  id: totrans-2071
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/5-shot | 2022-08-17 12:50:59 |'
- en: '| OpenAI-text-babbage-001/10-shot | 2022-08-17 13:27:52 |'
  id: totrans-2072
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/10-shot | 2022-08-17 13:27:52 |'
- en: '| OpenAI-text-babbage-001/15-shot | 2022-08-17 14:57:43 |'
  id: totrans-2073
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/15-shot | 2022-08-17 14:57:43 |'
- en: '| OpenAI-text-babbage-001/30-shot | 2022-08-17 15:45:16 |'
  id: totrans-2074
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-babbage-001/30-shot | 2022-08-17 15:45:16 |'
- en: '| OpenAI-text-curie-001/0-shot | 2022-08-18 04:39:55 |'
  id: totrans-2075
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/0-shot | 2022-08-18 04:39:55 |'
- en: '| OpenAI-text-curie-001/1-shot | 2022-08-18 05:10:17 |'
  id: totrans-2076
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/1-shot | 2022-08-18 05:10:17 |'
- en: '| OpenAI-text-curie-001/5-shot | 2022-08-18 05:40:56 |'
  id: totrans-2077
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/5-shot | 2022-08-18 05:40:56 |'
- en: '| OpenAI-text-curie-001/10-shot | 2022-08-18 06:15:28 |'
  id: totrans-2078
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/10-shot | 2022-08-18 06:15:28 |'
- en: '| OpenAI-text-curie-001/15-shot | 2022-08-18 06:53:09 |'
  id: totrans-2079
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/15-shot | 2022-08-18 06:53:09 |'
- en: '| OpenAI-text-curie-001/30-shot | 2022-08-18 07:35:40 |'
  id: totrans-2080
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-curie-001/30-shot | 2022-08-18 07:35:40 |'
- en: '| OpenAI-text-davinci-001/0-shot | 2022-08-26 20:26:21 |'
  id: totrans-2081
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/0-shot | 2022-08-26 20:26:21 |'
- en: '| OpenAI-text-davinci-001/1-shot | 2022-08-26 21:02:31 |'
  id: totrans-2082
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/1-shot | 2022-08-26 21:02:31 |'
- en: '| OpenAI-text-davinci-001/5-shot | 2022-08-26 21:35:19 |'
  id: totrans-2083
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/5-shot | 2022-08-26 21:35:19 |'
- en: '| OpenAI-text-davinci-001/10-shot | 2022-08-27 07:14:02 |'
  id: totrans-2084
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/10-shot | 2022-08-27 07:14:02 |'
- en: '| OpenAI-text-davinci-001/15-shot | 2022-08-27 07:58:25 |'
  id: totrans-2085
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/15-shot | 2022-08-27 07:58:25 |'
- en: '| OpenAI-text-davinci-001/30-shot | 2022-08-27 08:44:42 |'
  id: totrans-2086
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-001/30-shot | 2022-08-27 08:44:42 |'
- en: 'Table 89: Timestamp each was evaluated through OpenAI’s API - continued (2/2).'
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: 表89：每个模型通过OpenAI的API进行评估的时间戳 - 续（2/2）。
- en: '| model | timestamp |'
  id: totrans-2088
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 时间戳 |'
- en: '| --- | --- |'
  id: totrans-2089
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| OpenAI-text-davinci-002/0-shot | 2022-08-10 21:41:50 |'
  id: totrans-2090
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/0-shot | 2022-08-10 21:41:50 |'
- en: '| OpenAI-text-davinci-002/1-shot | 2022-08-11 10:04:17 |'
  id: totrans-2091
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/1-shot | 2022-08-11 10:04:17 |'
- en: '| OpenAI-text-davinci-002/5-shot | 2022-08-12 15:41:45 |'
  id: totrans-2092
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/5-shot | 2022-08-12 15:41:45 |'
- en: '| OpenAI-text-davinci-002/10-shot | 2022-08-12 16:41:14 |'
  id: totrans-2093
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/10-shot | 2022-08-12 16:41:14 |'
- en: '| OpenAI-text-davinci-002/15-shot | 2022-08-16 12:11:43 |'
  id: totrans-2094
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/15-shot | 2022-08-16 12:11:43 |'
- en: '| OpenAI-text-davinci-002/30-shot | 2022-08-16 14:35:38 |'
  id: totrans-2095
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-002/30-shot | 2022-08-16 14:35:38 |'
- en: '| OpenAI-text-davinci-003/0-shot | 2023-03-15 11:35:23 |'
  id: totrans-2096
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/0-shot | 2023-03-15 11:35:23 |'
- en: '| OpenAI-text-davinci-003/1-shot | 2023-04-04 13:12:05 |'
  id: totrans-2097
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/1-shot | 2023-04-04 13:12:05 |'
- en: '| OpenAI-text-davinci-003/5-shot | 2023-03-15 12:30:39 |'
  id: totrans-2098
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/5-shot | 2023-03-15 12:30:39 |'
- en: '| OpenAI-text-davinci-003/10-shot | 2023-04-04 14:01:03 |'
  id: totrans-2099
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/10-shot | 2023-04-04 14:01:03 |'
- en: '| OpenAI-text-davinci-003/15-shot | 2023-04-04 15:23:29 |'
  id: totrans-2100
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/15-shot | 2023-04-04 15:23:29 |'
- en: '| OpenAI-text-davinci-003/30-shot | 2023-04-06 15:08:38 |'
  id: totrans-2101
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-text-davinci-003/30-shot | 2023-04-06 15:08:38 |'
- en: '| OpenAI-gpt-3.5.turbo/0-shot | 2023-04-05 13:33:09 |'
  id: totrans-2102
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/0-shot | 2023-04-05 13:33:09 |'
- en: '| OpenAI-gpt-3.5.turbo/1-shot | 2023-04-05 16:36:45 |'
  id: totrans-2103
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/1-shot | 2023-04-05 16:36:45 |'
- en: '| OpenAI-gpt-3.5.turbo/5-shot | 2023-04-06 08:46:09 |'
  id: totrans-2104
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/5-shot | 2023-04-06 08:46:09 |'
- en: '| OpenAI-gpt-3.5.turbo/10-shot | 2023-04-06 09:54:07 |'
  id: totrans-2105
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/10-shot | 2023-04-06 09:54:07 |'
- en: '| OpenAI-gpt-3.5.turbo/15-shot | 2023-04-06 10:57:18 |'
  id: totrans-2106
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/15-shot | 2023-04-06 10:57:18 |'
- en: '| OpenAI-gpt-3.5.turbo/30-shot | 2023-04-06 12:03:59 |'
  id: totrans-2107
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-3.5.turbo/30-shot | 2023-04-06 12:03:59 |'
- en: '| OpenAI-gpt-4/0-shot | 2023-04-06 17:38:16 |'
  id: totrans-2108
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/0-shot | 2023-04-06 17:38:16 |'
- en: '| OpenAI-gpt-4/1-shot | 2023-04-06 19:41:59 |'
  id: totrans-2109
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/1-shot | 2023-04-06 19:41:59 |'
- en: '| OpenAI-gpt-4/5-shot | 2023-04-06 22:56:31 |'
  id: totrans-2110
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/5-shot | 2023-04-06 22:56:31 |'
- en: '| OpenAI-gpt-4/10-shot | 2023-04-08 12:06:03 |'
  id: totrans-2111
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/10-shot | 2023-04-08 12:06:03 |'
- en: '| OpenAI-gpt-4/15-shot | 2023-04-08 17:32:04 |'
  id: totrans-2112
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/15-shot | 2023-04-08 17:32:04 |'
- en: '| OpenAI-gpt-4/30-shot | 2023-04-08 19:56:26 |'
  id: totrans-2113
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI-gpt-4/30-shot | 2023-04-08 19:56:26 |'
- en: 'Table 90: Timestamp each model was evaluated through Cohere’s API.'
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 90：每个模型通过 Cohere 的 API 进行评估的时间戳。
- en: '| model | timestamp |'
  id: totrans-2115
  prefs: []
  type: TYPE_TB
  zh: '| model | timestamp |'
- en: '| --- | --- |'
  id: totrans-2116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Cohere-small/0-shot | 2022-08-16 22:22:17 |'
  id: totrans-2117
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/0-shot | 2022-08-16 22:22:17 |'
- en: '| Cohere-small/1-shot | 2022-08-17 08:22:43 |'
  id: totrans-2118
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/1-shot | 2022-08-17 08:22:43 |'
- en: '| Cohere-small/5-shot | 2022-08-17 09:19:57 |'
  id: totrans-2119
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/5-shot | 2022-08-17 09:19:57 |'
- en: '| Cohere-small/10-shot | 2022-08-17 10:43:53 |'
  id: totrans-2120
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/10-shot | 2022-08-17 10:43:53 |'
- en: '| Cohere-small/15-shot | 2022-08-17 12:53:02 |'
  id: totrans-2121
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/15-shot | 2022-08-17 12:53:02 |'
- en: '| Cohere-small/30-shot | 2022-08-17 13:46:08 |'
  id: totrans-2122
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-small/30-shot | 2022-08-17 13:46:08 |'
- en: '| Cohere-medium/0-shot | 2022-08-17 15:14:02 |'
  id: totrans-2123
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/0-shot | 2022-08-17 15:14:02 |'
- en: '| Cohere-medium/1-shot | 2022-08-17 16:00:21 |'
  id: totrans-2124
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/1-shot | 2022-08-17 16:00:21 |'
- en: '| Cohere-medium/5-shot | 2022-08-17 18:23:38 |'
  id: totrans-2125
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/5-shot | 2022-08-17 18:23:38 |'
- en: '| Cohere-medium/10-shot | 2022-08-17 19:16:00 |'
  id: totrans-2126
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/10-shot | 2022-08-17 19:16:00 |'
- en: '| Cohere-medium/15-shot | 2022-08-17 20:24:12 |'
  id: totrans-2127
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/15-shot | 2022-08-17 20:24:12 |'
- en: '| Cohere-medium/30-shot | 2022-08-17 21:20:28 |'
  id: totrans-2128
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-medium/30-shot | 2022-08-17 21:20:28 |'
- en: '| Cohere-large/0-shot | 2022-08-17 22:47:49 |'
  id: totrans-2129
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/0-shot | 2022-08-17 22:47:49 |'
- en: '| Cohere-large/1-shot | 2022-08-17 23:27:00 |'
  id: totrans-2130
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/1-shot | 2022-08-17 23:27:00 |'
- en: '| Cohere-large/5-shot | 2022-08-18 00:10:08 |'
  id: totrans-2131
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/5-shot | 2022-08-18 00:10:08 |'
- en: '| Cohere-large/10-shot | 2022-08-18 00:56:55 |'
  id: totrans-2132
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/10-shot | 2022-08-18 00:56:55 |'
- en: '| Cohere-large/15-shot | 2022-08-18 01:48:30 |'
  id: totrans-2133
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/15-shot | 2022-08-18 01:48:30 |'
- en: '| Cohere-large/30-shot | 2022-08-18 02:47:14 |'
  id: totrans-2134
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-large/30-shot | 2022-08-18 02:47:14 |'
- en: '| Cohere-xl/0-shot | 2022-07-29 |'
  id: totrans-2135
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/0-shot | 2022-07-29 |'
- en: '| Cohere-xl/1-shot | 2022-07-31 |'
  id: totrans-2136
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/1-shot | 2022-07-31 |'
- en: '| Cohere-xl/5-shot | 2022-08-02 |'
  id: totrans-2137
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/5-shot | 2022-08-02 |'
- en: '| Cohere-xl/10-shot | 2022-08-02 15:16:45 |'
  id: totrans-2138
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/10-shot | 2022-08-02 15:16:45 |'
- en: '| Cohere-xl/15-shot | 2022-08-07 13:55:44 |'
  id: totrans-2139
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/15-shot | 2022-08-07 13:55:44 |'
- en: '| Cohere-xl/30-shot | 2022-08-16 19:51:08 |'
  id: totrans-2140
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-xl/30-shot | 2022-08-16 19:51:08 |'
- en: '| Cohere-command-medium/0-shot | 2023-04-04 09:54:27 |'
  id: totrans-2141
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/0-shot | 2023-04-04 09:54:27 |'
- en: '| Cohere-command-medium/1-shot | 2023-04-04 11:51:07 |'
  id: totrans-2142
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/1-shot | 2023-04-04 11:51:07 |'
- en: '| Cohere-command-medium/5-shot | 2023-04-04 13:03:07 |'
  id: totrans-2143
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/5-shot | 2023-04-04 13:03:07 |'
- en: '| Cohere-command-medium/10-shot | 2023-04-04 13:31:47 |'
  id: totrans-2144
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/10-shot | 2023-04-04 13:31:47 |'
- en: '| Cohere-command-medium/15-shot | 2023-04-04 14:06:10 |'
  id: totrans-2145
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/15-shot | 2023-04-04 14:06:10 |'
- en: '| Cohere-command-medium/30-shot | 2023-04-04 14:42:13 |'
  id: totrans-2146
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-medium/30-shot | 2023-04-04 14:42:13 |'
- en: '| Cohere-command-xl/0-shot | 2023-04-04 10:25:30 |'
  id: totrans-2147
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/0-shot | 2023-04-04 10:25:30 |'
- en: '| Cohere-command-xl/1-shot | 2023-04-04 15:27:01 |'
  id: totrans-2148
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/1-shot | 2023-04-04 15:27:01 |'
- en: '| Cohere-command-xl/5-shot | 2023-04-04 15:59:47 |'
  id: totrans-2149
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/5-shot | 2023-04-04 15:59:47 |'
- en: '| Cohere-command-xl/10-shot | 2023-04-04 16:36:22 |'
  id: totrans-2150
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/10-shot | 2023-04-04 16:36:22 |'
- en: '| Cohere-command-xl/15-shot | 2023-04-04 17:22:58 |'
  id: totrans-2151
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/15-shot | 2023-04-04 17:22:58 |'
- en: '| Cohere-command-xl/30-shot | 2023-04-04 18:16:54 |'
  id: totrans-2152
  prefs: []
  type: TYPE_TB
  zh: '| Cohere-command-xl/30-shot | 2023-04-04 18:16:54 |'
- en: Appendix M Compute and Emissions
  id: totrans-2153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 M 计算和排放
- en: 'Find below in Table [91](#A13.T91 "Table 91 ‣ Appendix M Compute and Emissions
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") until Table [94](#A13.T94 "Table 94 ‣ Appendix
    M Compute and Emissions ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") the timestamps, durations,
    and emissions per experiment (calculated with the CodeCarbon library in Python).
    Find below in Table [95](#A13.T95 "Table 95 ‣ Appendix M Compute and Emissions
    ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for
    Implicature Resolution by LLMs") until Table [98](#A13.T98 "Table 98 ‣ Appendix
    M Compute and Emissions ‣ The Goldilocks of Pragmatic Understanding: Fine-Tuning
    Strategy Matters for Implicature Resolution by LLMs") the cpu-type and count and
    gpu-type and count per experiment. In terms of compute the following GPU hours
    can be estimated if we assume each run is entirely done on the GPU (which is not
    true in reality, but worst case):'
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: 见下表 [91](#A13.T91 "表 91 ‣ 附录 M 计算与排放 ‣ 实用理解的金发姑娘：微调策略对隐含意义解析的重要性") 至表 [94](#A13.T94
    "表 94 ‣ 附录 M 计算与排放 ‣ 实用理解的金发姑娘：微调策略对隐含意义解析的重要性") 的时间戳、时长和每个实验的排放（使用 Python 中的
    CodeCarbon 库计算）。见下表 [95](#A13.T95 "表 95 ‣ 附录 M 计算与排放 ‣ 实用理解的金发姑娘：微调策略对隐含意义解析的重要性")
    至表 [98](#A13.T98 "表 98 ‣ 附录 M 计算与排放 ‣ 实用理解的金发姑娘：微调策略对隐含意义解析的重要性") 的每个实验的 CPU 类型和数量以及
    GPU 类型和数量。就计算而言，如果我们假设每次运行完全在 GPU 上完成（实际上并非如此，但这是最坏情况），可以估算以下 GPU 小时数：
- en: NVIDIA A100-SXM4-40GB used for 926.4291392151515 hours.
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA A100-SXM4-40GB 使用了 926.4291392151515 小时。
- en: Tesla V100-PCIE-32GB used for 29.282544113265143 hours.
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: Tesla V100-PCIE-32GB 使用了 29.282544113265143 小时。
- en: Tesla V100-PCIE-16GB used for 11.462701331244574 hours.
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: Tesla V100-PCIE-16GB 使用了 11.462701331244574 小时。
- en: 'Table 91: Timestamp, duration, and emissions per experiment with non-API models.
    (1/4)'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 91：每个实验的时间戳、时长和排放（非 API 模型）。 (1/4)
- en: '| model | timestamp | duration |'
  id: totrans-2159
  prefs: []
  type: TYPE_TB
  zh: '| model | timestamp | duration |'
- en: '| --- | --- | --- |'
  id: totrans-2160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| EleutherAI-125m-0-shot | 2022-09-01T21:33:14 | 8549.649220 |'
  id: totrans-2161
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-0-shot | 2022-09-01T21:33:14 | 8549.649220 |'
- en: '| EleutherAI-125m-1-shot | 2022-09-02T00:10:03 | 640.861120 |'
  id: totrans-2162
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-1-shot | 2022-09-02T00:10:03 | 640.861120 |'
- en: '| EleutherAI-125m-5-shot | 2022-09-02T00:26:27 | 982.369876 |'
  id: totrans-2163
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-5-shot | 2022-09-02T00:26:27 | 982.369876 |'
- en: '| EleutherAI-125m-10-shot | 2022-09-02T00:51:24 | 1495.525381 |'
  id: totrans-2164
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-10-shot | 2022-09-02T00:51:24 | 1495.525381 |'
- en: '| EleutherAI-125m-15-shot | 2022-09-02T01:29:03 | 2257.290708 |'
  id: totrans-2165
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-15-shot | 2022-09-02T01:29:03 | 2257.290708 |'
- en: '| EleutherAI-125m-30-shot | 2022-09-02T09:04:03 | 27298.375266 |'
  id: totrans-2166
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-30-shot | 2022-09-02T09:04:03 | 27298.375266 |'
- en: '| EleutherAI-2.7b-0-shot | 2022-09-03T00:36:14 | 3752.897449 |'
  id: totrans-2167
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-0-shot | 2022-09-03T00:36:14 | 3752.897449 |'
- en: '| EleutherAI-2.7b-1-shot | 2022-09-03T02:04:16 | 5279.884696 |'
  id: totrans-2168
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-1-shot | 2022-09-03T02:04:16 | 5279.884696 |'
- en: '| EleutherAI-2.7b-5-shot | 2022-09-03T04:28:19 | 8641.654516 |'
  id: totrans-2169
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-5-shot | 2022-09-03T04:28:19 | 8641.654516 |'
- en: '| EleutherAI-2.7b-10-shot | 2022-09-03T08:18:13 | 13792.592126 |'
  id: totrans-2170
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-10-shot | 2022-09-03T08:18:13 | 13792.592126 |'
- en: '| EleutherAI-2.7b-15-shot | 2022-09-03T13:33:25 | 18909.551123 |'
  id: totrans-2171
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-15-shot | 2022-09-03T13:33:25 | 18909.551123 |'
- en: '| EleutherAI-2.7b-30-shot | 2022-09-03T22:47:06 | 33219.682098 |'
  id: totrans-2172
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-30-shot | 2022-09-03T22:47:06 | 33219.682098 |'
- en: '| EleutherAI-20b-0-shot | 2022-08-25T07:40:55 | 1378.197924 |'
  id: totrans-2173
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-0-shot | 2022-08-25T07:40:55 | 1378.197924 |'
- en: '| EleutherAI-20b-1-shot | 2022-08-25T08:15:23 | 807.702344 |'
  id: totrans-2174
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-1-shot | 2022-08-25T08:15:23 | 807.702344 |'
- en: '| EleutherAI-20b-5-shot | 2022-08-25T15:39:51 | 859.585535 |'
  id: totrans-2175
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-5-shot | 2022-08-25T15:39:51 | 859.585535 |'
- en: '| EleutherAI-20b-10-shot | 2022-08-25T16:18:50 | 1175.128651 |'
  id: totrans-2176
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-10-shot | 2022-08-25T16:18:50 | 1175.128651 |'
- en: '| EleutherAI-20b-15-shot | 2022-08-25T16:47:30 | 1713.266182 |'
  id: totrans-2177
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-15-shot | 2022-08-25T16:47:30 | 1713.266182 |'
- en: '| EleutherAI-20b-30-shot | 2022-08-25T17:45:28 | 3469.811664 |'
  id: totrans-2178
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-30-shot | 2022-08-25T17:45:28 | 3469.811664 |'
- en: '| EleutherAI-6b-0-shot | 2022-08-24T22:29:30 | 1287.627453 |'
  id: totrans-2179
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-0-shot | 2022-08-24T22:29:30 | 1287.627453 |'
- en: '| EleutherAI-6b-1-shot | 2022-08-24T23:22:30 | 1831.554774 |'
  id: totrans-2180
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-1-shot | 2022-08-24T23:22:30 | 1831.554774 |'
- en: '| EleutherAI-6b-5-shot | 2022-08-25T00:16:57 | 3255.128955 |'
  id: totrans-2181
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-5-shot | 2022-08-25T00:16:57 | 3255.128955 |'
- en: '| EleutherAI-6b-10-shot | 2022-08-25T01:23:21 | 3971.650578 |'
  id: totrans-2182
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-10-shot | 2022-08-25T01:23:21 | 3971.650578 |'
- en: '| EleutherAI-6b-15-shot | 2022-08-25T02:26:23 | 3772.113814 |'
  id: totrans-2183
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-15-shot | 2022-08-25T02:26:23 | 3772.113814 |'
- en: '| EleutherAI-6b-30-shot | 2022-08-25T04:18:30 | 6719.419030 |'
  id: totrans-2184
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-30-shot | 2022-08-25T04:18:30 | 6719.419030 |'
- en: '| EleutherAI-1.3b-0-shot | 2022-09-02T09:54:06 | 3000.666020 |'
  id: totrans-2185
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-0-shot | 2022-09-02T09:54:06 | 3000.666020 |'
- en: '| EleutherAI-1.3b-1-shot | 2022-09-02T10:46:30 | 3142.207699 |'
  id: totrans-2186
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-1-shot | 2022-09-02T10:46:30 | 3142.207699 |'
- en: '| EleutherAI-1.3b-5-shot | 2022-09-02T12:25:25 | 5933.046596 |'
  id: totrans-2187
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-5-shot | 2022-09-02T12:25:25 | 5933.046596 |'
- en: '| EleutherAI-1.3b-10-shot | 2022-09-02T12:39:00 | 8509.257493 |'
  id: totrans-2188
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-10-shot | 2022-09-02T12:39:00 | 8509.257493 |'
- en: '| EleutherAI-1.3b-15-shot | 2022-09-02T18:00:39 | 11615.289366 |'
  id: totrans-2189
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-15-shot | 2022-09-02T18:00:39 | 11615.289366 |'
- en: '| EleutherAI-1.3b-30-shot | 2022-09-02T23:33:39 | 19978.306457 |'
  id: totrans-2190
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-30-shot | 2022-09-02T23:33:39 | 19978.306457 |'
- en: 'Table 92: Timestamp, duration, and emissions per experiment with non-API models.
    (2/4)'
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 92: 时间戳、时长以及非 API 模型每次实验的排放量。 (2/4)'
- en: '| model | timestamp | duration |'
  id: totrans-2192
  prefs: []
  type: TYPE_TB
  zh: '| model | timestamp | duration |'
- en: '| --- | --- | --- |'
  id: totrans-2193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BLOOM-3b-0-shot | 2022-08-31T12:54:37 | 5178.369790 |'
  id: totrans-2194
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-0-shot | 2022-08-31T12:54:37 | 5178.369790 |'
- en: '| BLOOM-3b-1-shot | 2022-08-31T14:39:32 | 6292.560350 |'
  id: totrans-2195
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-1-shot | 2022-08-31T14:39:32 | 6292.560350 |'
- en: '| BLOOM-3b-5-shot | 2022-08-31T17:37:29 | 10675.230701 |'
  id: totrans-2196
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-5-shot | 2022-08-31T17:37:29 | 10675.230701 |'
- en: '| BLOOM-3b-10-shot | 2022-08-31T21:59:27 | 15715.744792 |'
  id: totrans-2197
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-10-shot | 2022-08-31T21:59:27 | 15715.744792 |'
- en: '| BLOOM-3b-15-shot | 2022-09-01T03:41:02 | 20492.823278 |'
  id: totrans-2198
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-15-shot | 2022-09-01T03:41:02 | 20492.823278 |'
- en: '| BLOOM-3b-30-shot | 2022-09-01T15:47:21 | 43577.882397 |'
  id: totrans-2199
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-30-shot | 2022-09-01T15:47:21 | 43577.882397 |'
- en: '| BLOOM-7b1-0-shot | 2022-08-25T04:56:35 | 625.931470 |'
  id: totrans-2200
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-0-shot | 2022-08-25T04:56:35 | 625.931470 |'
- en: '| BLOOM-7b1-1-shot | 2022-08-25T05:07:13 | 630.628939 |'
  id: totrans-2201
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-1-shot | 2022-08-25T05:07:13 | 630.628939 |'
- en: '| BLOOM-7b1-5-shot | 2022-08-25T05:24:22 | 1022.138932 |'
  id: totrans-2202
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-5-shot | 2022-08-25T05:24:22 | 1022.138932 |'
- en: '| BLOOM-7b1-10-shot | 2022-08-25T05:49:00 | 1471.008220 |'
  id: totrans-2203
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-10-shot | 2022-08-25T05:49:00 | 1471.008220 |'
- en: '| BLOOM-7b1-15-shot | 2022-08-25T06:23:26 | 2058.455127 |'
  id: totrans-2204
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-15-shot | 2022-08-25T06:23:26 | 2058.455127 |'
- en: '| BLOOM-7b1-30-shot | 2022-08-25T07:29:46 | 3972.772039 |'
  id: totrans-2205
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-30-shot | 2022-08-25T07:29:46 | 3972.772039 |'
- en: '| BLOOM-560m-0-shot | 2022-08-29T15:35:52 | 2541.248956 |'
  id: totrans-2206
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-0-shot | 2022-08-29T15:35:52 | 2541.248956 |'
- en: '| BLOOM-560m-1-shot | 2022-08-29T18:52:16 | 2532.794568 |'
  id: totrans-2207
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-1-shot | 2022-08-29T18:52:16 | 2532.794568 |'
- en: '| BLOOM-560m-5-shot | 2022-08-29T20:16:16 | 5038.547060 |'
  id: totrans-2208
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-5-shot | 2022-08-29T20:16:16 | 5038.547060 |'
- en: '| BLOOM-560m-10-shot | 2022-08-29T22:17:43 | 7285.239875 |'
  id: totrans-2209
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-10-shot | 2022-08-29T22:17:43 | 7285.239875 |'
- en: '| BLOOM-560m-15-shot | 2022-08-30T00:38:23 | 8438.096533 |'
  id: totrans-2210
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-15-shot | 2022-08-30T00:38:23 | 8438.096533 |'
- en: '| BLOOM-560m-30-shot | 2022-08-30T04:38:44 | 14419.447170 |'
  id: totrans-2211
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-30-shot | 2022-08-30T04:38:44 | 14419.447170 |'
- en: '| BLOOM-1b1-0-shot | 2022-08-30T05:18:44 | 2398.828856 |'
  id: totrans-2212
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-0-shot | 2022-08-30T05:18:44 | 2398.828856 |'
- en: '| BLOOM-1b1-1-shot | 2022-08-30T06:06:45 | 2879.435828 |'
  id: totrans-2213
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-1-shot | 2022-08-30T06:06:45 | 2879.435828 |'
- en: '| BLOOM-1b1-5-shot | 2022-08-30T07:35:59 | 5352.607075 |'
  id: totrans-2214
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-5-shot | 2022-08-30T07:35:59 | 5352.607075 |'
- en: '| BLOOM-1b1-10-shot | 2022-08-30T10:15:02 | 9541.535419 |'
  id: totrans-2215
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-10-shot | 2022-08-30T10:15:02 | 9541.535419 |'
- en: '| BLOOM-1b1-15-shot | 2022-08-30T13:22:42 | 11257.077128 |'
  id: totrans-2216
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-15-shot | 2022-08-30T13:22:42 | 11257.077128 |'
- en: '| BLOOM-1b1-30-shot | 2022-08-30T18:08:15 | 17131.797610 |'
  id: totrans-2217
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-30-shot | 2022-08-30T18:08:15 | 17131.797610 |'
- en: '| BLOOM-176b-0-shot | 2022-10-14T12:51:11 | 3015.240235 |'
  id: totrans-2218
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-0-shot | 2022-10-14T12:51:11 | 3015.240235 |'
- en: '| BLOOM-176b-1-shot | 2022-10-14T13:57:53 | 3906.461752 |'
  id: totrans-2219
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-1-shot | 2022-10-14T13:57:53 | 3906.461752 |'
- en: '| BLOOM-176b-5-shot | 2022-10-14T20:41:10 | 7411.725385 |'
  id: totrans-2220
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-5-shot | 2022-10-14T20:41:10 | 7411.725385 |'
- en: '| BLOOM-176b-10-shot | 2022-10-23T21:43:21 | 14462.201855 |'
  id: totrans-2221
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-10-shot | 2022-10-23T21:43:21 | 14462.201855 |'
- en: '| BLOOM-176b-15-shot | 2022-10-24T01:14:10 | 12609.026736 |'
  id: totrans-2222
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-15-shot | 2022-10-24T01:14:10 | 12609.026736 |'
- en: '| BLOOM-176b-30-shot | 2022-10-14T20:47:02 | 33159.499966 |'
  id: totrans-2223
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-30-shot | 2022-10-14T20:47:02 | 33159.499966 |'
- en: 'Table 93: Timestamp, duration, and emissions per experiment with non-API models.
    (3/4)'
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
  zh: '表 93: 时间戳、时长以及非 API 模型每次实验的排放量。 (3/4)'
- en: '| model | timestamp | duration |'
  id: totrans-2225
  prefs: []
  type: TYPE_TB
  zh: '| model | timestamp | duration |'
- en: '| --- | --- | --- |'
  id: totrans-2226
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| OPT-13b-0-shot | 2022-08-25T07:07:08 | 878.202579 |'
  id: totrans-2227
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-0-shot | 2022-08-25T07:07:08 | 878.202579 |'
- en: '| OPT-13b-1-shot | 2022-08-25T07:31:30 | 458.133617 |'
  id: totrans-2228
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-1-shot | 2022-08-25T07:31:30 | 458.133617 |'
- en: '| OPT-13b-5-shot | 2022-08-25T07:37:39 | 578.308507 |'
  id: totrans-2229
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-5-shot | 2022-08-25T07:37:39 | 578.308507 |'
- en: '| OPT-13b-10-shot | 2022-08-25T08:01:50 | 821.158826 |'
  id: totrans-2230
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-10-shot | 2022-08-25T08:01:50 | 821.158826 |'
- en: '| OPT-13b-15-shot | 2022-08-25T08:20:49 | 1131.479665 |'
  id: totrans-2231
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-15-shot | 2022-08-25T08:20:49 | 1131.479665 |'
- en: '| OPT-13b-30-shot | 2022-08-25T16:05:27 | 2235.869414 |'
  id: totrans-2232
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-30-shot | 2022-08-25T16:05:27 | 2235.869414 |'
- en: '| OPT-350m-0-shot | 2022-09-16T17:26:28 | 389.173905 |'
  id: totrans-2233
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-0-shot | 2022-09-16T17:26:28 | 389.173905 |'
- en: '| OPT-350m-1-shot | 2022-09-16T17:33:42 | 424.832551 |'
  id: totrans-2234
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-1-shot | 2022-09-16T17:33:42 | 424.832551 |'
- en: '| OPT-350m-5-shot | 2022-09-16T18:00:14 | 1583.824094 |'
  id: totrans-2235
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-5-shot | 2022-09-16T18:00:14 | 1583.824094 |'
- en: '| OPT-350m-10-shot | 2022-09-16T18:32:12 | 1908.822462 |'
  id: totrans-2236
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-10-shot | 2022-09-16T18:32:12 | 1908.822462 |'
- en: '| OPT-350m-15-shot | 2022-09-16T19:03:23 | 1863.625027 |'
  id: totrans-2237
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-15-shot | 2022-09-16T19:03:23 | 1863.625027 |'
- en: '| OPT-350m-30-shot | 2022-09-16T19:47:29 | 2637.811867 |'
  id: totrans-2238
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-30-shot | 2022-09-16T19:47:29 | 2637.811867 |'
- en: '| OPT-125m-0-shot | 2022-09-16T15:15:56 | 273.178967 |'
  id: totrans-2239
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-0-shot | 2022-09-16T15:15:56 | 273.178967 |'
- en: '| OPT-125m-1-shot | 2022-09-16T15:20:28 | 259.680856 |'
  id: totrans-2240
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-1-shot | 2022-09-16T15:20:28 | 259.680856 |'
- en: '| OPT-125m-5-shot | 2022-09-16T15:41:37 | 1259.801105 |'
  id: totrans-2241
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-5-shot | 2022-09-16T15:41:37 | 1259.801105 |'
- en: '| OPT-125m-10-shot | 2022-09-16T16:09:59 | 1693.598805 |'
  id: totrans-2242
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-10-shot | 2022-09-16T16:09:59 | 1693.598805 |'
- en: '| OPT-125m-15-shot | 2022-09-16T16:41:46 | 1899.415318 |'
  id: totrans-2243
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-15-shot | 2022-09-16T16:41:46 | 1899.415318 |'
- en: '| OPT-125m-30-shot | 2022-09-16T17:19:51 | 2276.441314 |'
  id: totrans-2244
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-30-shot | 2022-09-16T17:19:51 | 2276.441314 |'
- en: '| OPT-6.7b-0-shot | 2022-08-24T23:03:07 | 1140.485014 |'
  id: totrans-2245
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-0-shot | 2022-08-24T23:03:07 | 1140.485014 |'
- en: '| OPT-6.7b-1-shot | 2022-08-24T23:17:51 | 872.225225 |'
  id: totrans-2246
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-1-shot | 2022-08-24T23:17:51 | 872.225225 |'
- en: '| OPT-6.7b-5-shot | 2022-08-24T23:34:40 | 995.894396 |'
  id: totrans-2247
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-5-shot | 2022-08-24T23:34:40 | 995.894396 |'
- en: '| OPT-6.7b-10-shot | 2022-08-24T23:55:44 | 1252.956499 |'
  id: totrans-2248
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-10-shot | 2022-08-24T23:55:44 | 1252.956499 |'
- en: '| OPT-6.7b-15-shot | 2022-08-25T00:23:04 | 1627.749039 |'
  id: totrans-2249
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-15-shot | 2022-08-25T00:23:04 | 1627.749039 |'
- en: '| OPT-6.7b-30-shot | 2022-08-25T01:05:49 | 2553.054289 |'
  id: totrans-2250
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-30-shot | 2022-08-25T01:05:49 | 2553.054289 |'
- en: '| OPT-2.7b-0-shot | 2022-09-18T16:35:05 | 686.197892 |'
  id: totrans-2251
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-0-shot | 2022-09-18T16:35:05 | 686.197892 |'
- en: '| OPT-2.7b-1-shot | 2022-09-18T16:45:11 | 593.508211 |'
  id: totrans-2252
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-1-shot | 2022-09-18T16:45:11 | 593.508211 |'
- en: '| OPT-2.7b-5-shot | 2022-09-18T17:12:11 | 1613.313387 |'
  id: totrans-2253
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-5-shot | 2022-09-18T17:12:11 | 1613.313387 |'
- en: '| OPT-2.7b-10-shot | 2022-09-18T17:44:48 | 1949.808232 |'
  id: totrans-2254
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-10-shot | 2022-09-18T17:44:48 | 1949.808232 |'
- en: '| OPT-2.7b-15-shot | 2022-09-18T18:22:02 | 2225.927837 |'
  id: totrans-2255
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-15-shot | 2022-09-18T18:22:02 | 2225.927837 |'
- en: '| OPT-2.7b-30-shot | 2022-09-18T19:09:05 | 2815.327871 |'
  id: totrans-2256
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-30-shot | 2022-09-18T19:09:05 | 2815.327871 |'
- en: '| OPT-30b-0-shot | 2022-08-25T19:03:37 | 591.665447 |'
  id: totrans-2257
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-0-shot | 2022-08-25T19:03:37 | 591.665447 |'
- en: '| OPT-30b-1-shot | 2022-08-25T19:14:32 | 645.923823 |'
  id: totrans-2258
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-1-shot | 2022-08-25T19:14:32 | 645.923823 |'
- en: '| OPT-30b-5-shot | 2022-08-25T16:44:22 | 1825.821606 |'
  id: totrans-2259
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-5-shot | 2022-08-25T16:44:22 | 1825.821606 |'
- en: '| OPT-30b-10-shot | 2022-08-25T17:07:22 | 1372.752916 |'
  id: totrans-2260
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-10-shot | 2022-08-25T17:07:22 | 1372.752916 |'
- en: '| OPT-30b-15-shot | 2022-08-25T17:41:05 | 2015.006104 |'
  id: totrans-2261
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-15-shot | 2022-08-25T17:41:05 | 2015.006104 |'
- en: '| OPT-30b-30-shot | 2022-08-25T18:10:39 | 3859.078056 |'
  id: totrans-2262
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-30-shot | 2022-08-25T18:10:39 | 3859.078056 |'
- en: '| OPT-1.3b-0-shot | 2022-09-17T17:53:50 | 595.193443 |'
  id: totrans-2263
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-0-shot | 2022-09-17T17:53:50 | 595.193443 |'
- en: '| OPT-1.3b-1-shot | 2022-09-17T18:03:45 | 579.367790 |'
  id: totrans-2264
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-1-shot | 2022-09-17T18:03:45 | 579.367790 |'
- en: '| OPT-1.3b-5-shot | 2022-09-17T18:33:18 | 1759.103432 |'
  id: totrans-2265
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-5-shot | 2022-09-17T18:33:18 | 1759.103432 |'
- en: '| OPT-1.3b-10-shot | 2022-09-17T19:12:19 | 2327.300123 |'
  id: totrans-2266
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-10-shot | 2022-09-17T19:12:19 | 2327.300123 |'
- en: '| OPT-1.3b-15-shot | 2022-09-17T19:48:32 | 2161.637401 |'
  id: totrans-2267
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-15-shot | 2022-09-17T19:48:32 | 2161.637401 |'
- en: '| OPT-1.3b-30-shot | 2022-09-17T20:37:00 | 2893.829010 |'
  id: totrans-2268
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-30-shot | 2022-09-17T20:37:00 | 2893.829010 |'
- en: '| OPT-175b-0-shot | 2022-10-19T15:02:56 | 2387.104187 |'
  id: totrans-2269
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-0-shot | 2022-10-19T15:02:56 | 2387.104187 |'
- en: '| OPT-175b-1-shot | 2022-10-19T16:34:06 | 1589.972279 |'
  id: totrans-2270
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-1-shot | 2022-10-19T16:34:06 | 1589.972279 |'
- en: '| OPT-175b-5-shot | 2022-10-19T17:25:58 | 3072.591171 |'
  id: totrans-2271
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-5-shot | 2022-10-19T17:25:58 | 3072.591171 |'
- en: '| OPT-175b-10-shot | 2022-10-19T17:33:15 | 6211.692086 |'
  id: totrans-2272
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-10-shot | 2022-10-19T17:33:15 | 6211.692086 |'
- en: '| OPT-175b-15-shot | 2022-10-19T21:29:16 | 8019.585246 |'
  id: totrans-2273
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-15-shot | 2022-10-19T21:29:16 | 8019.585246 |'
- en: '| OPT-175b-30-shot | 2022-10-19T21:36:53 | 19901.470347 |'
  id: totrans-2274
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-30-shot | 2022-10-19T21:36:53 | 19901.470347 |'
- en: '| OPT-66b-0-shot | 2022-08-25T18:58:11 | 2834.901372 |'
  id: totrans-2275
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-0-shot | 2022-08-25T18:58:11 | 2834.901372 |'
- en: '| OPT-66b-1-shot | 2022-08-25T19:22:09 | 1427.806986 |'
  id: totrans-2276
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-1-shot | 2022-08-25T19:22:09 | 1427.806986 |'
- en: '| OPT-66b-5-shot | 2022-08-25T19:47:39 | 1521.168440 |'
  id: totrans-2277
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-5-shot | 2022-08-25T19:47:39 | 1521.168440 |'
- en: '| OPT-66b-10-shot | 2022-08-25T20:24:56 | 2228.407874 |'
  id: totrans-2278
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-10-shot | 2022-08-25T20:24:56 | 2228.407874 |'
- en: '| OPT-66b-15-shot | 2022-08-25T21:41:21 | 3370.689256 |'
  id: totrans-2279
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-15-shot | 2022-08-25T21:41:21 | 3370.689256 |'
- en: '| OPT-66b-30-shot | 2022-08-26T00:31:36 | 6816.312183 |'
  id: totrans-2280
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-30-shot | 2022-08-26T00:31:36 | 6816.312183 |'
- en: 'Table 94: Timestamp, duration, and emissions per experiment with non-API models.
    (4/4)'
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
  zh: 表 94：非 API 模型的每次实验时间戳、持续时间和排放量。（4/4）
- en: '| model | timestamp | duration |'
  id: totrans-2282
  prefs: []
  type: TYPE_TB
  zh: '| model | timestamp | duration |'
- en: '| --- | --- | --- |'
  id: totrans-2283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BlenderBot-2.7b-0-shot | 2022-09-04T08:09:56 | 3656.381540 |'
  id: totrans-2284
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-0-shot | 2022-09-04T08:09:56 | 3656.381540 |'
- en: '| BlenderBot-2.7b-1-shot | 2022-09-12T15:58:01 | 4051.858183 |'
  id: totrans-2285
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-1-shot | 2022-09-12T15:58:01 | 4051.858183 |'
- en: '| BlenderBot-2.7b-5-shot | 2022-09-12T17:16:20 | 4696.628979 |'
  id: totrans-2286
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-5-shot | 2022-09-12T17:16:20 | 4696.628979 |'
- en: '| BlenderBot-2.7b-10-shot | 2022-09-12T18:35:53 | 4772.083818 |'
  id: totrans-2287
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-10-shot | 2022-09-12T18:35:53 | 4772.083818 |'
- en: '| BlenderBot-2.7b-15-shot | 2022-09-12T19:54:13 | 4698.638356 |'
  id: totrans-2288
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-15-shot | 2022-09-12T19:54:13 | 4698.638356 |'
- en: '| BlenderBot-2.7b-30-shot | 2022-09-12T21:10:34 | 4579.460884 |'
  id: totrans-2289
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-30-shot | 2022-09-12T21:10:34 | 4579.460884 |'
- en: '| BlenderBot-9.4b-0-shot | 2022-10-22T04:04:24 | 614.201131 |'
  id: totrans-2290
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-0-shot | 2022-10-22T04:04:24 | 614.201131 |'
- en: '| BlenderBot-9.4b-1-shot | 2022-10-22T17:17:21 | 659.975971 |'
  id: totrans-2291
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-1-shot | 2022-10-22T17:17:21 | 659.975971 |'
- en: '| BlenderBot-9.4b-5-shot | 2022-10-22T17:31:48 | 839.336277 |'
  id: totrans-2292
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-5-shot | 2022-10-22T17:31:48 | 839.336277 |'
- en: '| BlenderBot-9.4b-10-shot | 2022-10-22T17:46:18 | 843.852691 |'
  id: totrans-2293
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-10-shot | 2022-10-22T17:46:18 | 843.852691 |'
- en: '| BlenderBot-9.4b-15-shot | 2022-10-22T17:53:41 | 1262.038660 |'
  id: totrans-2294
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-15-shot | 2022-10-22T17:53:41 | 1262.038660 |'
- en: '| BlenderBot-9.4b-30-shot | 2022-10-22T18:23:25 | 853.334728 |'
  id: totrans-2295
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-30-shot | 2022-10-22T18:23:25 | 853.334728 |'
- en: '| BlenderBot-90m-0-shot | 2022-09-14T15:11:44 | 273.134700 |'
  id: totrans-2296
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-0-shot | 2022-09-14T15:11:44 | 273.134700 |'
- en: '| BlenderBot-90m-1-shot | 2022-09-14T15:17:38 | 351.542638 |'
  id: totrans-2297
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-1-shot | 2022-09-14T15:17:38 | 351.542638 |'
- en: '| BlenderBot-90m-5-shot | 2022-09-14T15:29:50 | 730.774348 |'
  id: totrans-2298
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-5-shot | 2022-09-14T15:29:50 | 730.774348 |'
- en: '| BlenderBot-90m-10-shot | 2022-09-14T15:47:22 | 1050.647882 |'
  id: totrans-2299
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-10-shot | 2022-09-14T15:47:22 | 1050.647882 |'
- en: '| BlenderBot-90m-15-shot | 2022-09-14T16:07:27 | 1204.079804 |'
  id: totrans-2300
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-15-shot | 2022-09-14T16:07:27 | 1204.079804 |'
- en: '| BlenderBot-90m-30-shot | 2022-09-14T16:28:55 | 1285.913686 |'
  id: totrans-2301
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-30-shot | 2022-09-14T16:28:55 | 1285.913686 |'
- en: '| T0-3b-0-shot | 2022-10-21T17:33:36 | 348.245298 |'
  id: totrans-2302
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-0-shot | 2022-10-21T17:33:36 | 348.245298 |'
- en: '| T0-3b-1-shot | 2022-10-24T23:20:57 | 350.730799 |'
  id: totrans-2303
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-1-shot | 2022-10-24T23:20:57 | 350.730799 |'
- en: '| T0-3b-5-shot | 2022-10-24T23:29:21 | 474.378557 |'
  id: totrans-2304
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-5-shot | 2022-10-24T23:29:21 | 474.378557 |'
- en: '| T0-3b-10-shot | 2022-10-25T15:56:54 | 676.111759 |'
  id: totrans-2305
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-10-shot | 2022-10-25T15:56:54 | 676.111759 |'
- en: '| T0-3b-15-shot | 2022-10-25T16:12:55 | 928.215524 |'
  id: totrans-2306
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-15-shot | 2022-10-25T16:12:55 | 928.215524 |'
- en: '| T0-3b-30-shot | 2022-10-24T23:30:17 | 1961.897054 |'
  id: totrans-2307
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-30-shot | 2022-10-24T23:30:17 | 1961.897054 |'
- en: '| T0-11b-0-shot | 2022-10-21T15:38:13 | 2289.815276 |'
  id: totrans-2308
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-0-shot | 2022-10-21T15:38:13 | 2289.815276 |'
- en: '| T0-11b-1-shot | 2022-10-22T19:18:25 | 814.872760 |'
  id: totrans-2309
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-1-shot | 2022-10-22T19:18:25 | 814.872760 |'
- en: '| T0-11b-5-shot | 2022-10-22T19:41:45 | 1368.644314 |'
  id: totrans-2310
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-5-shot | 2022-10-22T19:41:45 | 1368.644314 |'
- en: '| T0-11b-10-shot | 2022-10-22T20:17:30 | 2112.628515 |'
  id: totrans-2311
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-10-shot | 2022-10-22T20:17:30 | 2112.628515 |'
- en: '| T0-11b-15-shot | 2022-10-22T21:06:30 | 2904.655213 |'
  id: totrans-2312
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-15-shot | 2022-10-22T21:06:30 | 2904.655213 |'
- en: '| T0-11b-30-shot | 2022-10-22T22:41:16 | 5648.105648 |'
  id: totrans-2313
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-30-shot | 2022-10-22T22:41:16 | 5648.105648 |'
- en: '| Flan-T5-3b-0-shot | 2022-10-24T11:20:36 | 617.820384 |'
  id: totrans-2314
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-0-shot | 2022-10-24T11:20:36 | 617.820384 |'
- en: '| Flan-T5-3b-1-shot | 2022-10-25T12:29:59 | 348.405589 |'
  id: totrans-2315
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-1-shot | 2022-10-25T12:29:59 | 348.405589 |'
- en: '| Flan-T5-3b-5-shot | 2022-10-25T12:38:24 | 474.872964 |'
  id: totrans-2316
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-5-shot | 2022-10-25T12:38:24 | 474.872964 |'
- en: '| Flan-T5-3b-10-shot | 2022-10-25T12:50:00 | 665.592482 |'
  id: totrans-2317
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-10-shot | 2022-10-25T12:50:00 | 665.592482 |'
- en: '| Flan-T5-3b-15-shot | 2022-10-25T13:05:34 | 902.197151 |'
  id: totrans-2318
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-15-shot | 2022-10-25T13:05:34 | 902.197151 |'
- en: '| Flan-T5-3b-30-shot | 2022-10-25T13:37:14 | 1864.885266 |'
  id: totrans-2319
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-30-shot | 2022-10-25T13:37:14 | 1864.885266 |'
- en: '| Flan-T5-780m-0-shot | 2022-10-24T11:54:09 | 160.503411 |'
  id: totrans-2320
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-0-shot | 2022-10-24T11:54:09 | 160.503411 |'
- en: '| Flan-T5-780m-1-shot | 2022-10-25T14:41:28 | 3816.321305 |'
  id: totrans-2321
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-1-shot | 2022-10-25T14:41:28 | 3816.321305 |'
- en: '| Flan-T5-780m-5-shot | 2022-10-25T14:46:09 | 251.699700 |'
  id: totrans-2322
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-5-shot | 2022-10-25T14:46:09 | 251.699700 |'
- en: '| Flan-T5-780m-10-shot | 2022-10-25T14:52:09 | 331.340966 |'
  id: totrans-2323
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-10-shot | 2022-10-25T14:52:09 | 331.340966 |'
- en: '| Flan-T5-780m-15-shot | 2022-10-25T14:59:00 | 381.107934 |'
  id: totrans-2324
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-15-shot | 2022-10-25T14:59:00 | 381.107934 |'
- en: '| Flan-T5-780m-30-shot | 2022-10-25T15:11:18 | 705.711192 |'
  id: totrans-2325
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-30-shot | 2022-10-25T15:11:18 | 705.711192 |'
- en: '| Flan-T5-11b-0-shot | 2022-10-24T10:25:09 | 1111.283857 |'
  id: totrans-2326
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-0-shot | 2022-10-24T10:25:09 | 1111.283857 |'
- en: '| Flan-T5-11b-1-shot | 2022-10-24T10:56:52 | 654.411412 |'
  id: totrans-2327
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-1-shot | 2022-10-24T10:56:52 | 654.411412 |'
- en: '| Flan-T5-11b-5-shot | 2022-10-25T17:26:50 | 1403.159768 |'
  id: totrans-2328
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-5-shot | 2022-10-25T17:26:50 | 1403.159768 |'
- en: '| Flan-T5-11b-10-shot | 2022-10-25T18:29:59 | 3756.529085 |'
  id: totrans-2329
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-10-shot | 2022-10-25T18:29:59 | 3756.529085 |'
- en: '| Flan-T5-11b-15-shot | 2022-10-25T19:21:15 | 3042.271478 |'
  id: totrans-2330
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-15-shot | 2022-10-25T19:21:15 | 3042.271478 |'
- en: '| Flan-T5-11b-30-shot | 2022-10-25T20:57:13 | 5722.244579 |'
  id: totrans-2331
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-30-shot | 2022-10-25T20:57:13 | 5722.244579 |'
- en: 'Table 95: Compute used per experiment with non-API models. (1/4)'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 95：每个实验使用的计算资源（非 API 模型）。(1/4)
- en: '| model | cpus | cpu model | gpu model |'
  id: totrans-2333
  prefs: []
  type: TYPE_TB
  zh: '| model | cpus | cpu model | gpu model |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| EleutherAI-125m-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2335
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-0-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-125m-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2336
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-1-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-125m-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2337
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-5-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-125m-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2338
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-10-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-125m-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2339
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-15-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-125m-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2340
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-125m-30-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2341
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-0-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2342
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-1-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2343
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-5-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2344
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-10-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2345
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-15-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-2.7b-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2346
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-2.7b-30-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-20b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2347
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-20b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2348
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-20b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2349
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-20b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2350
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-20b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2351
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-20b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2352
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-20b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2353
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2354
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2355
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2356
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2357
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-6b-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
  id: totrans-2358
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-6b-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
    | 8 x NVIDIA A100-40GB |'
- en: '| EleutherAI-1.3b-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2359
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-0-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-1.3b-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2360
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-1-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-1.3b-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2361
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-5-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-1.3b-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2362
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-10-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-1.3b-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2363
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-15-shot | 10 | Apple M1 Max |  |'
- en: '| EleutherAI-1.3b-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2364
  prefs: []
  type: TYPE_TB
  zh: '| EleutherAI-1.3b-30-shot | 10 | Apple M1 Max |  |'
- en: 'Table 96: Compute used per experiment with non-API models. (2/4)'
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 96: Compute used per experiment with non-API models. (2/4)'
- en: '| model | cpus | cpu model | gpu model |'
  id: totrans-2366
  prefs: []
  type: TYPE_TB
  zh: '| model | cpus | cpu model | gpu model |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2367
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BLOOM-3b-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2368
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-0-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-3b-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2369
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-1-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-3b-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2370
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-5-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-3b-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2371
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-10-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-3b-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2372
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-15-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-3b-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2373
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-3b-30-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-7b1-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2374
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| BLOOM-7b1-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2375
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| BLOOM-7b1-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2376
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| BLOOM-7b1-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
  id: totrans-2377
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
- en: '| BLOOM-7b1-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
  id: totrans-2378
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
- en: '| BLOOM-7b1-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
  id: totrans-2379
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-7b1-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz |
    8 x NVIDIA A100-40GB |'
- en: '| BLOOM-560m-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2380
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-0-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-560m-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2381
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-1-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-560m-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2382
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-5-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-560m-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2383
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-10-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-560m-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2384
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-15-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-560m-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2385
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-560m-30-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2386
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-0-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2387
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-1-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2388
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-5-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2389
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-10-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2390
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-15-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-1b1-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2391
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-1b1-30-shot | 10 | Apple M1 Max |  |'
- en: '| BLOOM-176b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2392
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| BLOOM-176b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2393
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| BLOOM-176b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2394
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| BLOOM-176b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2395
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| BLOOM-176b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2396
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| BLOOM-176b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2397
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: 'Table 97: Compute used per experiment with non-API models. (3/4)'
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 97: 计算每个实验使用的非 API 模型的资源。 (3/4)'
- en: '| model | cpus | cpu model | gpu model |'
  id: totrans-2399
  prefs: []
  type: TYPE_TB
  zh: '| model | cpus | cpu model | gpu model |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2400
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| OPT-13b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2401
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-13b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2402
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-13b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8 x
    NVIDIA A100-40GB |'
  id: totrans-2403
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8 x
    NVIDIA A100-40GB |'
- en: '| OPT-13b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2404
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-13b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2405
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-13b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2406
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-350m-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2407
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-350m-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2408
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-350m-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2409
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-350m-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2410
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-350m-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2411
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-350m-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2412
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2413
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2414
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2415
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2416
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2417
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-125m-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2418
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-6.7b-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2419
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-0-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-6.7b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2420
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-1-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-6.7b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2421
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-5-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-6.7b-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2422
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-10-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-6.7b-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2423
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-15-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-6.7b-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2424
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7b-30-shot | 1 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-2.7b-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2425
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-0-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-2.7b-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2426
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-1-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-2.7b-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2427
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-5-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-2.7b-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2428
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-10-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-2.7b-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2429
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-15-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-2.7b-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
  id: totrans-2430
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b-30-shot | 24 | Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz | 4 x Tesla
    V100-PCIE-32GB |'
- en: '| OPT-30b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2431
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-30b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2432
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-30b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2433
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-30b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2434
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-30b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2435
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-30b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2436
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-1.3b-0-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
  id: totrans-2437
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-0-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
- en: '| OPT-1.3b-1-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
  id: totrans-2438
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-1-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
- en: '| OPT-1.3b-5-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
  id: totrans-2439
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-5-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x Tesla
    V100-PCIE-16GB |'
- en: '| OPT-1.3b-10-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
  id: totrans-2440
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-10-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
- en: '| OPT-1.3b-15-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
  id: totrans-2441
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-15-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
- en: '| OPT-1.3b-30-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
  id: totrans-2442
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b-30-shot | 40 | Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz | 4 x
    Tesla V100-PCIE-16GB |'
- en: '| OPT-175b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2443
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-175b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2444
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-175b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2445
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-175b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2446
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-175b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2447
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-175b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2448
  prefs: []
  type: TYPE_TB
  zh: '| OPT-175b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| OPT-66b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2449
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-0-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-66b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2450
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-1-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-66b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2451
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-5-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-66b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2452
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-10-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-66b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2453
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-15-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: '| OPT-66b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
  id: totrans-2454
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b-30-shot | 48 | Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz | 8
    x NVIDIA A100-40GB |'
- en: 'Table 98: Compute used per experiment with non-API models. (4/4)'
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 98: 每个实验使用的计算资源（非API模型）。 (4/4)'
- en: '| model | cpus | cpu model | gpu model |'
  id: totrans-2456
  prefs: []
  type: TYPE_TB
  zh: '| model | cpus | cpu model | gpu model |'
- en: '| --- | --- | --- | --- |'
  id: totrans-2457
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BlenderBot-2.7b-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2458
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-0-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-2.7b-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2459
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-1-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-2.7b-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2460
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-5-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-2.7b-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2461
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-10-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-2.7b-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2462
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-15-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-2.7b-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2463
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-2.7b-30-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-9.4b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2464
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-9.4b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2465
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-9.4b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2466
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-9.4b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2467
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-9.4b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2468
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-9.4b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2469
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-9.4b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| BlenderBot-90m-0-shot | 10 | Apple M1 Max |  |'
  id: totrans-2470
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-0-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-90m-1-shot | 10 | Apple M1 Max |  |'
  id: totrans-2471
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-1-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-90m-5-shot | 10 | Apple M1 Max |  |'
  id: totrans-2472
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-5-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-90m-10-shot | 10 | Apple M1 Max |  |'
  id: totrans-2473
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-10-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-90m-15-shot | 10 | Apple M1 Max |  |'
  id: totrans-2474
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-15-shot | 10 | Apple M1 Max |  |'
- en: '| BlenderBot-90m-30-shot | 10 | Apple M1 Max |  |'
  id: totrans-2475
  prefs: []
  type: TYPE_TB
  zh: '| BlenderBot-90m-30-shot | 10 | Apple M1 Max |  |'
- en: '| T0-3b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2476
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-3b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2477
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-3b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2478
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-3b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2479
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-3b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2480
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-3b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2481
  prefs: []
  type: TYPE_TB
  zh: '| T0-3b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2482
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2483
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2484
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2485
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2486
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| T0-11b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2487
  prefs: []
  type: TYPE_TB
  zh: '| T0-11b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2488
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2489
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2490
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2491
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2492
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-3b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2493
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-3b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-780m-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2494
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-780m-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2495
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-780m-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2496
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-780m-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2497
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| Flan-T5-780m-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2498
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| Flan-T5-780m-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
  id: totrans-2499
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-780m-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA
    A100-40GB |'
- en: '| Flan-T5-11b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2500
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-0-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-11b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2501
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-1-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-11b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2502
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-5-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-11b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2503
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-10-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-11b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2504
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-15-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
- en: '| Flan-T5-11b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
  id: totrans-2505
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-11b-30-shot | 96 | Intel(R) Xeon(R) CPU @ 2.20GHz | 16 x NVIDIA A100-40GB
    |'
