- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同声掩码，而非提示优化：LLMs同声传译微调中的范式转变
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10443](https://ar5iv.labs.arxiv.org/html/2405.10443)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10443](https://ar5iv.labs.arxiv.org/html/2405.10443)
- en: Matthew Raffel     Victor Agostinelli     Lizhong Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Matthew Raffel     Victor Agostinelli     Lizhong Chen
- en: Oregon State University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 俄勒冈州立大学
- en: '{raffelm, agostinv, chenliz}@oregonstate.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{raffelm, agostinv, chenliz}@oregonstate.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have achieved state-of-the-art performance in various
    language processing tasks, motivating their adoption in simultaneous translation.
    Current fine-tuning methods to adapt LLMs for simultaneous translation focus on
    prompting optimization strategies using either data augmentation or prompt structure
    modifications. However, these methods suffer from several issues, such as an unnecessarily
    expanded training set, computational inefficiency from dumping the KV cache, increased
    prompt sizes, or restriction to a single decision policy. To eliminate these issues,
    we propose a new paradigm in fine-tuning LLMs for simultaneous translation, called
    SimulMask. It utilizes a novel attention mask technique that models simultaneous
    translation during fine-tuning by masking attention connections under a desired
    decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT
    2017 dataset, we have observed a significant translation quality improvement compared
    to state-of-the-art prompting optimization strategies on three language pairs
    when averaged across four different latency regimes while reducing the computational
    cost.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种语言处理任务中已实现了最先进的性能，激励了它们在同声传译中的应用。当前的微调方法通过数据增强或提示结构修改来优化提示策略，以适应LLMs进行同声传译。然而，这些方法存在若干问题，如不必要的训练集扩展、由于丢弃KV缓存导致的计算低效、提示大小增加或限制于单一决策策略。为了解决这些问题，我们提出了一种新的LLMs微调范式，称为SimulMask。它利用了一种新颖的注意力掩码技术，通过在期望的决策策略下掩盖注意力连接来模拟同声传译。将所提出的SimulMask应用于Falcon
    LLM，并在IWSLT 2017数据集上进行测试，我们观察到与最先进的提示优化策略相比，在三对语言上的翻译质量显著提高，并在四种不同延迟模式下平均，同时减少了计算成本。
- en: 'Simultaneous Masking, Not Prompting Optimization:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同声掩码，而非提示优化：
- en: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同声传译微调中的范式转变
- en: Matthew Raffel     Victor Agostinelli     Lizhong Chen Oregon State University
    {raffelm, agostinv, chenliz}@oregonstate.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Matthew Raffel     Victor Agostinelli     Lizhong Chen 俄勒冈州立大学 {raffelm, agostinv,
    chenliz}@oregonstate.edu
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Simultaneous translation refers to the process of producing a target output
    translation concurrently with an oncoming source input. In our increasingly interconnected
    world, where communication across languages in real-time is desired, simultaneous
    translation is becoming a requirement. Unfortunately, the task of simultaneous
    translation is extremely straining for human interpreters, and among the limited
    number of simultaneous human interpreters, their maximum translation length is
    around thirty minutes Moser-Mercer et al. ([1998](#bib.bib14)). As such, there
    is a need for machine learning models to alleviate the burden placed on simultaneous
    human interpreters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 同声传译是指在接收到源输入的同时生成目标输出翻译的过程。在我们日益互联的世界中，实时跨语言沟通的需求日益增长，同声传译正成为一种必要。然而，同声传译对于人类口译员来说极具挑战性，而在有限的同声人类口译员中，他们的最大翻译时间约为三十分钟
    Moser-Mercer 等人 ([1998](#bib.bib14))。因此，需要机器学习模型来减轻对同声人类口译员施加的负担。
- en: Current literature has primarily focused on adapting end-to-end Transformer
    models Vaswani et al. ([2017](#bib.bib22)) to overcome the difficulties of simultaneous
    translation due to their reduced parameter counts and greater inference speedMa
    et al. ([2020b](#bib.bib13)). However, the recent successes of large language
    models (LLMs) Touvron et al. ([2023](#bib.bib21)); Jiang et al. ([2023](#bib.bib8));
    Almazrouei et al. ([2023](#bib.bib2)) has prompted preliminary research applying
    them to simultaneous translation through fine-tuning and inference techniques
    Agostinelli et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25)); Koshkin
    et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6)).
    Unfortunately, most modern works have neglected the computational increases created
    by dumping the target sequence’s key and value (KV) cache Wang et al. ([2024](#bib.bib24)).
    Furthermore, there has yet to be a universal approach to fine-tuning LLMs for
    simultaneous translation that is not unnecessarily computationally expensive by
    either expanding the dataset through data augmentation, a process referred to
    as prefix fine-tuning Agostinelli et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25));
    Koshkin et al. ([2024](#bib.bib9)) or increasing the prompt length through prompt
    restructuring Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的文献主要集中在将端到端 Transformer 模型（Vaswani et al. ([2017](#bib.bib22))）适应于克服同步翻译的困难，这些模型因其参数较少和推理速度更快（Ma
    et al. ([2020b](#bib.bib13))）。然而，近期大语言模型（LLMs）的成功（Touvron et al. ([2023](#bib.bib21));
    Jiang et al. ([2023](#bib.bib8)); Almazrouei et al. ([2023](#bib.bib2))）促使初步研究将其应用于同步翻译，通过微调和推理技术（Agostinelli
    et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25)); Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6))）。不幸的是，大多数现代工作忽视了通过丢弃目标序列的关键和值（KV）缓存（Wang
    et al. ([2024](#bib.bib24))）所带来的计算增加。此外，还没有一种通用的 LLM 微调策略，用于同步翻译，既不通过数据增强扩展数据集，这一过程称为前缀微调（Agostinelli
    et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25)); Koshkin et al. ([2024](#bib.bib9))），也不通过提示重组增加提示长度（Koshkin
    et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24))）而不至于过度消耗计算资源。
- en: The lack of an efficient fine-tuning strategy of LLMs for simultaneous translation
    has led us to propose a new paradigm, referred to as SimulMask. SimulMask is a
    novel attention mask to model simultaneous translation during fine-tuning by redistributing
    the attention connections under a decision policy. By design, SimulMask is broadly
    applicable to both flexible and fixed decision policies, creating a path forward
    for future work to build upon it. Furthermore, if we eliminate injecting positional
    information into the keys and values through a modified ALiBi Press et al. ([2021](#bib.bib20)),
    SimulMask allows for key and value caching during simultaneous translation without
    accuracy degradation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在同步翻译中的高效微调策略的缺乏促使我们提出了一种新的范式，称为 SimulMask。SimulMask 是一种新型的注意力掩码，通过在决策策略下重新分配注意力连接来建模同步翻译。在设计上，SimulMask
    广泛适用于灵活和固定的决策策略，为未来的工作提供了前进的道路。此外，如果我们通过修改的 ALiBi（Press et al. ([2021](#bib.bib20))）消除将位置信息注入到关键和值中，SimulMask
    允许在同步翻译中进行关键和值缓存，而不会导致准确性下降。
- en: To validate the efficacy of SimulMask, we fine-tuned and evaluated 1.3 billion
    parameter Falcon models pre-trained on the RefinedWeb dataset Almazrouei et al.
    ([2023](#bib.bib2)); Penedo et al. ([2023](#bib.bib18)) using the IWSLT 2017 dataset
    Cettolo et al. ([2017](#bib.bib3)). All models were fine-tuned using a wait-k
    decision policy. From the averaged evaluation results across all wait-k values,
    we found an LLM fine-tuned with SimulMask and evaluated with KV caching outperforms
    an LLM fine-tuned with prefix fine-tuning and evaluated without KV caching by
    7.34, 3.00, and 2.98 BLEU on the English-French, English-Dutch, and English-Italian
    language pairs, respectively. Furthermore, on average, with the wait-3 policy,
    recomputing the KV cache contributes toward 26.8 % of the computation time and
    87.9 % of the FLOPs to generate a target translation on an A40 GPU, substantiating
    the need for SimulMask, which enables inference time KV caching to avoid those
    computations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证 SimulMask 的有效性，我们对在 RefinedWeb 数据集上预训练的 13 亿参数 Falcon 模型进行了微调和评估（Almazrouei
    et al. ([2023](#bib.bib2))；Penedo et al. ([2023](#bib.bib18))），使用了 IWSLT 2017
    数据集（Cettolo et al. ([2017](#bib.bib3))）。所有模型都使用 wait-k 决策策略进行微调。从所有 wait-k 值的平均评估结果来看，我们发现用
    SimulMask 微调并用 KV 缓存评估的 LLM 在英语-法语、英语-荷兰语和英语-意大利语语言对上比用前缀微调并在没有 KV 缓存下评估的 LLM
    分别高出 7.34、3.00 和 2.98 BLEU。此外，平均而言，在 wait-3 策略下，重新计算 KV 缓存占 A40 GPU 上生成目标翻译计算时间的
    26.8% 和 FLOPs 的 87.9%，证明了 SimulMask 的必要性，它能够通过启用推理时间 KV 缓存来避免这些计算。
- en: 'In summary, the main contributions of the paper include:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文的主要贡献包括：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Providing insights on the shortcomings of current literature in adapting LLMs
    to simultaneous translation.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了关于当前文献在将 LLMs 适应于同时翻译方面不足之处的见解。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Proposing a novel attention masking approach to fine-tune LLMs for simultaneous
    translation that allows for efficient training and inference.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了一种新颖的注意力掩码方法，以微调 LLMs 进行同时翻译，从而实现高效的训练和推理。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Demonstrating the efficacy of our proposed approach in terms of translation
    quality and computational costs by evaluating them on multiple language pairs
    across many wait-k values.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在多个语言对和多种 wait-k 值上进行评估，展示了我们提出的方法在翻译质量和计算成本方面的有效性。
- en: 2 Background and Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 2.1 Masked Transformer Self-Attention
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 掩码变换器自注意力
- en: 'We briefly review self-attention functionality in transformers (Vaswani et al.,
    [2017](#bib.bib22)) focusing on masking behavior in Equation [1](#S2.E1 "In 2.1
    Masked Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation"), which describes per-head attention calculations for
    some attention head $h$ representing model dimensionality, the query matrix ${\bm{Q}}^{(h)}={\bm{X}}{\bm{W}}^{(h)}_{q}$
    are defined as ${\bm{W}}^{(h)}_{q}\in\mathbb{R}^{d_{model}\times d_{head}}$ representing
    per-attention head dimensionality. We define each individual query, key, and value
    in their respective matrices as ${\bm{q}}_{i}^{(h)}\in{\bm{Q}}^{(h)}$ is defined
    as an optional attention mask.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要回顾了变换器中自注意力功能（Vaswani et al., [2017](#bib.bib22)），重点关注方程 [1](#S2.E1 "在 2.1
    掩码变换器自注意力 ‣ 2 背景与相关工作 ‣ 同时掩码，而非提示优化：对 LLMs 同时翻译微调的范式转变") 中的掩码行为，该方程描述了某个注意力头 $h$
    的每头注意力计算，表示模型维度的查询矩阵 ${\bm{Q}}^{(h)}={\bm{X}}{\bm{W}}^{(h)}_{q}$ 被定义为 ${\bm{W}}^{(h)}_{q}\in\mathbb{R}^{d_{model}\times
    d_{head}}$，表示每个注意力头的维度。我们将各个查询、键和值在其各自矩阵中定义为 ${\bm{q}}_{i}^{(h)}\in{\bm{Q}}^{(h)}$
    被定义为一个可选的注意力掩码。
- en: '|  | ${\bm{A}}^{(h)}=\texttt{softmax}\left(\frac{{\bm{Q}}^{(h)}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}}{\sqrt{d_{head}}}\right){\bm{V}}^{(h)}$
    |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{A}}^{(h)}=\texttt{softmax}\left(\frac{{\bm{Q}}^{(h)}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}}{\sqrt{d_{head}}}\right){\bm{V}}^{(h)}$
    |  | (1) |'
- en: 'Critically, ${\bm{M}}$ is represented by Equation [2](#S2.E2 "In 2.1 Masked
    Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，${\bm{M}}$ 由方程 [2](#S2.E2 "在 2.1 掩码变换器自注意力 ‣ 2 背景与相关工作 ‣ 同时掩码，而非提示优化：对
    LLMs 同时翻译微调的范式转变") 表示。
- en: '|  | ${\bm{M}}_{ij}=\begin{cases}0,&amp;\text{if }j\leq i\\ -\infty,&amp;\text{otherwise}\end{cases}$
    |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{M}}_{ij}=\begin{cases}0,&amp;\text{如果 }j\leq i\\ -\infty,&amp;\text{否则}\end{cases}$
    |  | (2) |'
- en: 2.2 Simultaneous Translation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 同时翻译
- en: Simultaneous translation is dictated by read-write decision policies, whereby
    a model will wait a specific amount of time before alternating between reading
    and writing in fixed or flexible intervals. One fixed decision policy for simultaneous
    translation that is broadly adopted as a common baseline to build on due to its
    effectiveness and simplicity is the wait-k policy Ma et al. ([2019](#bib.bib11)).
    As the name suggests, the wait-k policy will wait for k words before alternating
    between writing and reading a word. Although effective in simultaneous translation,
    alternative adaptive policies have gained traction, which base reading and writing
    on an auxiliary model or a predefined set of rules Cho and Esipova ([2016](#bib.bib4));
    Gu et al. ([2017](#bib.bib5)); Zheng et al. ([2019](#bib.bib28)). While capable
    of impressive results, such adaptive policies often incur additional computational
    costs and delays.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 同步翻译由读写决策策略决定，其中模型将在固定或灵活的时间间隔内在阅读和写作之间切换。一种固定的同步翻译决策策略是广泛采用的 wait-k 策略，因其有效性和简洁性而作为一个常见基线进行构建
    Ma 等人 ([2019](#bib.bib11))。顾名思义，wait-k 策略将在 k 个单词之后在写作和阅读之间切换。虽然在同步翻译中有效，但基于辅助模型或预定义规则集的替代自适应策略也逐渐受到关注
    Cho 和 Esipova ([2016](#bib.bib4))；Gu 等人 ([2017](#bib.bib5))；Zheng 等人 ([2019](#bib.bib28))。尽管能够取得令人印象深刻的结果，但这些自适应策略通常会带来额外的计算成本和延迟。
- en: 'A Transformer is trained for a simultaneous translation decision policy by
    masking attention scores in the encoder self-attention and the decoder cross-attention.
    In the case of the encoder self-attention, each source token is prevented from
    attending to future source tokens following a decision policy Ma et al. ([2019](#bib.bib11)).
    For example, if the source sequence length was 5 tokens and the first read step
    reads 2 tokens, the second read step reads 1 token, and the third read step reads
    2 tokens, then the respective attention mask is provided in Figure [1](#S2.F1
    "Figure 1 ‣ 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在编码器自注意力和解码器交叉注意力中掩盖注意力分数来训练变换器进行同步翻译决策策略。在编码器自注意力的情况下，按照 Ma 等人 ([2019](#bib.bib11))
    的决策策略，防止每个源令牌关注未来的源令牌。例如，如果源序列长度为 5 个令牌，第一次读取步骤读取 2 个令牌，第二次读取步骤读取 1 个令牌，第三次读取步骤读取
    2 个令牌，则相应的注意力掩码如图 [1](#S2.F1 "图 1 ‣ 2.2 同步翻译 ‣ 2 背景与相关工作 ‣ 同步掩码，而非提示优化：微调大语言模型进行同步翻译的范式转变")
    所示。
- en: '![Refer to caption](img/1e6a4f34201ea2a2f363dbc74abb79ff.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1e6a4f34201ea2a2f363dbc74abb79ff.png)'
- en: 'Figure 1: An attention mask to model simultaneous translation for a transformer
    encoder during training.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：训练期间变换器编码器用于建模同步翻译的注意力掩码。
- en: 'Alternatively, in decoder cross-attention, each target token is prevented from
    attending to future source hidden states following the decision policy Papi et al.
    ([2022a](#bib.bib16)). Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣
    2 Background and Related Work ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") expresses
    each entry of the decoder cross-attention mask, ${\bm{M}}_{tj}$.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在解码器交叉注意力中，每个目标令牌按照 Papi 等人 ([2022a](#bib.bib16)) 的决策策略防止关注未来的源隐藏状态。方程 [3](#S2.E3
    "在 2.2 同步翻译 ‣ 2 背景与相关工作 ‣ 同步掩码，而非提示优化：微调大语言模型进行同步翻译的范式转变") 表示解码器交叉注意力掩码的每个条目，${\bm{M}}_{tj}$。
- en: '|  | ${\bm{M}}_{tj}=\begin{cases}0,&amp;\text{if }j\leq f(t)\\ -\infty,&amp;\text{otherwise}\end{cases}$
    |  | (3) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{M}}_{tj}=\begin{cases}0,&\text{如果 }j\leq f(t)\\ -\infty,&\text{否则}\end{cases}$
    |  | (3) |'
- en: 'In Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and
    Related Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), $f(t)$. Each source hidden
    state aligns with one source input token.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [3](#S2.E3 "在 2.2 同步翻译 ‣ 2 背景与相关工作 ‣ 同步掩码，而非提示优化：微调大语言模型进行同步翻译的范式转变") 中，$f(t)$。每个源隐藏状态与一个源输入令牌对齐。
- en: 2.3 Applying Large Language Models for Simultaneous Translation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 将大语言模型应用于同步翻译
- en: LLMs have demonstrated remarkable performance on the task of neural machine
    translation Moslem et al. ([2023](#bib.bib15)); Vilar et al. ([2023](#bib.bib23));
    Xu et al. ([2023](#bib.bib26)); Zhang et al. ([2023](#bib.bib27)); Iyer et al.
    ([2023](#bib.bib7)). Such successes have prompted recent works to extend the reach
    of LLMs into the realm of simultaneous translation Agostinelli et al. ([2023](#bib.bib1));
    Wang et al. ([2023](#bib.bib25)); Koshkin et al. ([2024](#bib.bib9)); Wang et al.
    ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6)). LLMs are especially promising
    for the field of simultaneous translation due to their strong understanding of
    language semantics and meaning. Intuitively, SimulMT LLMs inject holistic linguistic
    knowledge that could allow for correct translation decisions when facing difficult
    contextual obstacles (e.g., translating a verb in a target language without access
    to that verb in the source language).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在神经机器翻译任务中表现出了显著的性能 Moslem et al. ([2023](#bib.bib15)); Vilar et al. ([2023](#bib.bib23));
    Xu et al. ([2023](#bib.bib26)); Zhang et al. ([2023](#bib.bib27)); Iyer et al.
    ([2023](#bib.bib7))。这些成功促使近期的研究将LLMs的应用扩展到实时翻译领域 Agostinelli et al. ([2023](#bib.bib1));
    Wang et al. ([2023](#bib.bib25)); Koshkin et al. ([2024](#bib.bib9)); Wang et
    al. ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6))。由于LLMs对语言语义和意义的强大理解，它们在实时翻译领域尤其具有前景。直观地说，SimulMT
    LLMs注入了整体语言知识，使其在面对困难的语境障碍时能够做出正确的翻译决策（例如，在没有源语言中该动词的情况下翻译目标语言中的动词）。
- en: 'Unfortunately, Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background
    and Related Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation") is no longer effective
    in modeling simultaneous translation for decoder-only LLMs as with the decoder
    of a Transformer. The reason is Equation [3](#S2.E3 "In 2.2 Simultaneous Translation
    ‣ 2 Background and Related Work ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") is constructed
    specifically for the cross-attention calculation between keys exclusively from
    the source and queries exclusively from the target, as in Transformers. Since
    LLMs perform a self-attention, including the prompt, the source, and the target,
    Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related
    Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation") cannot properly mask the source from the target
    with the additional prompt and target sequences included in the keys, and the
    additional prompt and source sequences included in the queries. Furthermore, Equation
    [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣
    Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation") does not enforce the autoregressive language
    modeling behavior of LLMs. As such, alternative means to model simultaneous translation
    have been proposed, leveraging prompting optimization.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，方程 [3](#S2.E3 "在 2.2 实时翻译 ‣ 2 背景和相关工作 ‣ 实时掩蔽，而非提示优化：一种针对实时翻译的LLMs微调的范式转变")
    对于仅解码器LLMs的实时翻译建模不再有效，就像Transformer的解码器一样。原因是方程 [3](#S2.E3 "在 2.2 实时翻译 ‣ 2 背景和相关工作
    ‣ 实时掩蔽，而非提示优化：一种针对实时翻译的LLMs微调的范式转变") 专门为从源语言中提取的键和仅来自目标语言的查询之间的交叉注意力计算构建，而这正是Transformers的特点。由于LLMs进行自注意力，包括提示、源语言和目标语言，方程
    [3](#S2.E3 "在 2.2 实时翻译 ‣ 2 背景和相关工作 ‣ 实时掩蔽，而非提示优化：一种针对实时翻译的LLMs微调的范式转变") 无法在键中正确掩蔽源语言与目标语言之间的关系，以及在查询中包括的额外提示和源语言序列。此外，方程
    [3](#S2.E3 "在 2.2 实时翻译 ‣ 2 背景和相关工作 ‣ 实时掩蔽，而非提示优化：一种针对实时翻译的LLMs微调的范式转变") 也没有强制执行LLMs的自回归语言建模行为。因此，已经提出了利用提示优化来建模实时翻译的替代方法。
- en: 3 Prompting Optimization Methods
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提示优化方法
- en: Current methods of fine-tuning LLMs for simultaneous translation fall into the
    general category of prompting optimization. We define prompting optimization as
    either employing data augmentation to help with prompting Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2023](#bib.bib25)); Agostinelli et al. ([2023](#bib.bib1)) or redefining
    the prompt structure Wang et al. ([2024](#bib.bib24)); Koshkin et al. ([2024](#bib.bib9))
    to somewhat simulate simultaneous translation. In this section, we will cover
    these prompting optimization strategies in depth.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的 LLM 同步翻译微调方法大致属于提示优化的范畴。我们将提示优化定义为：要么采用数据增强来帮助提示 Koshkin 等人 ([2024](#bib.bib9));
    Wang 等人 ([2023](#bib.bib25)); Agostinelli 等人 ([2023](#bib.bib1))，要么重新定义提示结构 Wang
    等人 ([2024](#bib.bib24)); Koshkin 等人 ([2024](#bib.bib9))，以某种程度上模拟同步翻译。在本节中，我们将深入探讨这些提示优化策略。
- en: 3.1 Data Augmentation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据增强
- en: Prompting optimization focusing on data augmentation resorts to subdividing
    each sentence in a dataset into multiple partial sentence pairs. These partial
    sentence pairs mimic simultaneous translation, as simultaneous translation produces
    outputs with a partial input. We label such a method as prefix fine-tuning, and
    although the high-level procedure is identical amongst current works, the algorithms
    employed to obtain these partial sentence pairs are unique. In the case of Agostinelli
    et al. ([2023](#bib.bib1)), each source-target sentence pair is subdivided according
    to the wait-k policy such that if we order the new samples from smallest to largest,
    each subsequent sentence pair will have one additional target word and source
    word so long as the end of the target or source is not reached. Upon completion
    there will be $\texttt{max}(|S|-(k-1),|T|)$ are the original source and target
    sequence lengths. The approach requires the model to predict only the final target
    word in the sequence during fine-tuning.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 关注数据增强的提示优化方法则是将数据集中的每个句子划分为多个部分句子对。这些部分句子对模拟了同步翻译，因为同步翻译会产生部分输入的输出。我们将这种方法称为前缀微调，尽管当前方法的高级过程是相同的，但获取这些部分句子对的算法是独特的。在
    Agostinelli 等人 ([2023](#bib.bib1)) 的情况下，每个源-目标句子对根据 wait-k 策略进行细分，因此如果我们将新样本从小到大排序，每个后续的句子对将多一个目标词和源词，只要目标或源的末尾没有到达。完成后，$\texttt{max}(|S|-(k-1),|T|)$
    是原始源和目标序列的长度。该方法要求模型在微调过程中仅预测序列中的最终目标词。
- en: Alternatively, Wang et al. ([2023](#bib.bib25)) randomly sampled a subset of
    sentence pairs from the dataset and truncated the source sentence to be 20% to
    80% of the full length according to a uniform distribution. They obtained the
    respective target translations by prompting ChatGPT. The new truncated source-target
    sentence pairs were then added to the complete dataset to expand the overall fine-tuning
    dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Wang 等人 ([2023](#bib.bib25)) 从数据集中随机抽取了一部分句子对，并根据均匀分布将源句子截断为完整长度的 20% 到 80%。他们通过提示
    ChatGPT 获得了相应的目标翻译。然后，将这些新的截断源-目标句子对添加到完整的数据集中，以扩展整体微调数据集。
- en: 3.2 Prompt Restructuring
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提示重构
- en: Prompting optimization that modifies the prompting structure adjusts the prompt
    to include the decision policy. In the case of Wang et al. ([2024](#bib.bib24)),
    a conversational prompting structure is adopted for the LLM, alternating between
    source and target subsequences of the original complete sequences using delimiting
    tokens to separate regions. For instance, if we have the source sequence $S=[s_{1},s_{2},...,s_{n}]$,
    then one potential conversational prompt could be $$\texttt{},\texttt{[U]},s_{1},s_{2},\texttt{[A]},t_{1},t_{2},\texttt{},...,\texttt{},\texttt{[U]},\\
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 修改提示结构的提示优化调整提示以包含决策政策。在 Wang 等人 ([2024](#bib.bib24)) 的情况下，为 LLM 采用了对话提示结构，在原始完整序列的源和目标子序列之间交替使用分隔符来分隔区域。例如，如果我们有源序列
    $S=[s_{1},s_{2},...,s_{n}]$，那么一个潜在的对话提示可能是 $$\texttt{},\texttt{[U]},s_{1},s_{2},\texttt{[A]},t_{1},t_{2},\texttt{},...,\texttt{},\texttt{[U]},\\
- en: s_{n},\texttt{[A]},t_{m},\texttt{}$$, where , , [A], [U] are delimiting
    tokens. During fine-tuning, the choice of alternating subsequences is arrived
    at by attempting to maximize the relevant source context before each target sequence
    in the form of an oracle decision policy. For instance, the prompt will ensure
    an arbitrary target verb prediction only after the respective source verb is read.
    Some minor perturbations are added to the oracle decision policy to improve generalizability.
    Then, at inference, a prompt constructor provides the source sequence in fixed-size
    chunks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: s_{n},\texttt{[A]},t_{m},\texttt{}$$，其中 , , [A], [U] 是分隔标记。在微调过程中，通过尝试最大化每个目标序列之前的相关源上下文，来选择交替子序列，这种方式类似于神谕决策策略。例如，提示将确保仅在读取相应的源动词后才进行任意目标动词预测。为了提高泛化能力，对神谕决策策略进行了轻微的扰动。然后，在推理时，提示构造器以固定大小的块提供源序列。
- en: Similarly, Koshkin et al. ([2024](#bib.bib9)) leverages prompt restructuring;
    however, it also employs prefix finetuning. Like the conversational prompting
    structure, it constructs a fine-tuning prompt by aligning words between the source
    and target sequence to mimic an oracle decision policy. However, it deviates from
    conversational prompting by ensuring the alignment using padding tokens in the
    target sequence. Then, the causally aligned sentence prompt is subdivided using
    a prefix fine-tuning strategy to expand the dataset with partially filled source-target
    sentence pairs. At inference, the LLM contains the decision policy outputting
    padding tokens whenever it requires more source context tokens.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Koshkin 等人（[2024](#bib.bib9)）利用了提示重构；然而，它还采用了前缀微调。类似于对话式提示结构，它通过对齐源序列和目标序列中的单词来构建一个微调提示，以模拟一个神谕决策策略。然而，它通过在目标序列中使用填充标记来确保对齐，从而偏离了对话式提示。然后，因果对齐的句子提示通过前缀微调策略进行细分，以扩展部分填充的源-目标句子对的数据集。在推理时，LLM
    包含决策策略，每当需要更多源上下文标记时，就会输出填充标记。
- en: 4 Analysis and Shortcomings of Prompting Optimization Methods
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提示优化方法的分析和不足之处
- en: Prompting optimization, while works to a certain degree, is inherently deficient,
    possessing a host of fine-tuning and inference issues. These issues include a
    persistent fine-tuning-inference mismatch, consistent positional confusion in
    the target sequence, and high computational costs. We will first present the fine-tuning-inference
    mismatch, followed by the positional confusion problem, and finish with the computational
    burdens facing the current paradigm.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提示优化在一定程度上有效，但本质上存在缺陷，具有许多微调和推理问题。这些问题包括持续的微调-推理不匹配、目标序列中的位置混淆以及高计算成本。我们将首先介绍微调-推理不匹配，然后是位置混淆问题，最后讨论当前范式面临的计算负担。
- en: 4.1 Fine-tuning/Inference Mismatch
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 微调/推理不匹配
- en: A fine-tuning-inference mismatch is a mismatch between an LLM’s fine-tuning
    and inference environments. For instance, fine-tuning an LLM for neural machine
    translation where the entire sentence is available and deploying it for simultaneous
    translation where little of the sentence is available when beginning generation
    will create a massive inference time fine-tuning-inference mismatch. Furthermore,
    the LLM must be fine-tuned to accommodate KV caching, the process of caching the
    keys and values at inference to prevent recomputation. Overall, fine-tuning for
    simultaneous translation aims to minimize the fine-tuning-inference mismatch.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 微调-推理不匹配是指LLM的微调和推理环境之间的不匹配。例如，微调用于神经机器翻译的LLM时，整个句子都可用，而在进行实时翻译时，生成开始时几乎没有句子可用，这将导致巨大的推理时间微调-推理不匹配。此外，LLM
    必须进行微调以适应 KV 缓存，即在推理时缓存键和值的过程，以防止重新计算。总体而言，针对实时翻译的微调旨在最小化微调-推理不匹配。
- en: 'Unfortunately, prefix fine-tuning precludes high-quality simultaneous translation
    with KV caching Agostinelli et al. ([2023](#bib.bib1)); Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2023](#bib.bib25)) as with the continuously increasing prompt size,
    each subsequent entry in the KV cache continuously becomes outdated, a scenario
    the LLM was not fine-tuned to handle. For example, suppose we have an LLM in the
    middle of simultaneous translation using KV caching adhering to a wait-1 policy
    with the following prompting structure: “Translate the following sentence from
    English to German: $s_{1},s_{2},...,s_{i}$. At the given write step, the KV cache
    entries for $t_{1},t_{2},...,t_{i}$, whereas the keys and values of $t_{i}$ are
    each generated with the same subset of the source sequence $s_{1},s_{2},...,s_{i}$.
    Such fine-tuning-inference mismatch is unsolved through conventional prompting
    structures and is necessitated to increase translation quality.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，前缀微调排除了使用 KV 缓存的高质量同时翻译 Agostinelli 等人 ([2023](#bib.bib1)); Koshkin 等人
    ([2024](#bib.bib9)); Wang 等人 ([2023](#bib.bib25))，因为随着提示大小的不断增加，KV 缓存中的每个后续条目不断过时，这是
    LLM 在微调时没有处理的情况。例如，假设我们有一个正在进行同时翻译的 LLM，使用 KV 缓存，遵循一个 wait-1 策略，具有以下提示结构：“将以下句子从英语翻译成德语：$s_{1},s_{2},...,s_{i}$。在给定的写入步骤中，KV
    缓存条目为 $t_{1},t_{2},...,t_{i}$，而 $t_{i}$ 的键和值是用相同的源序列子集 $s_{1},s_{2},...,s_{i}$
    生成的。这种微调-推理不匹配通过传统提示结构无法解决，必须提高翻译质量。
- en: Prompting restructuring also creates additional fine-tuning-inference mismatches.
    In Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)), they
    all fine-tune for an oracle decision policy. However, at inference, such an oracle
    decision policy is not truly achievable, creating a mismatch. Furthermore, since
    the LLMs that leverage prompt restructuring encapsulate a specific oracle decision
    policy into their fine-tuning curriculum, extending them to alternative decision
    policies at inference is infeasible without incurring a mismatch. As such, there
    is a need for a flexible method adaptable to a range of decision policies that
    also eliminates the fine-tuning-inference mismatch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 提示重构也会造成额外的微调-推理不匹配。在 Koshkin 等人 ([2024](#bib.bib9)); Wang 等人 ([2024](#bib.bib24))
    的研究中，他们都对一个“oracle”决策策略进行了微调。然而，在推理过程中，这种“oracle”决策策略并不真正可实现，从而产生了不匹配。此外，由于利用提示重构的
    LLM 将特定的“oracle”决策策略封装到其微调课程中，将其扩展到推理时的其他决策策略是不可行的，否则会造成不匹配。因此，需要一种灵活的方法，能够适应各种决策策略，并消除微调-推理不匹配。
- en: 4.2 Positional Confusion
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 位置信息混淆
- en: Positional confusion describes the process whereby the relative and/or global
    positional information during simultaneous translation progressively becomes incorrect.
    Unfortunately, most simultaneous translation LLMs using KV caching suffer from
    this positional confusion Agostinelli et al. ([2023](#bib.bib1)); Koshkin et al.
    ([2024](#bib.bib9)); Wang et al. ([2023](#bib.bib25)). The reason is that as the
    source sequence grows with simultaneous translation, the target sequence shifts,
    necessitating the target sequence’s positional information to follow suit. However,
    since KV caching is employed, updating the keys and values is infeasible, causing
    them to hold incorrect positional information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 位置信息混淆描述了在同时翻译过程中，相对和/或全局位置信息逐渐变得不准确的过程。不幸的是，大多数使用 KV 缓存的同时翻译 LLM 都受到这种位置信息混淆的影响
    Agostinelli 等人 ([2023](#bib.bib1)); Koshkin 等人 ([2024](#bib.bib9)); Wang 等人 ([2023](#bib.bib25))。原因是随着源序列在同时翻译过程中增长，目标序列会发生移动，迫使目标序列的位置信息也要跟随。然而，由于使用了
    KV 缓存，更新键和值变得不可行，导致它们持有不正确的位置信息。
- en: 'Aligning with our previous example, the sequence portion that would experience
    this positional confusion would be “[a]: $t_{1},t_{2},...,t_{i}$ the positional
    distance between $s_{1}$ would change to 2 and 3, respectively. However, while
    using KV caching, this positional distance would remain 1 and 2 in the keys and/or
    values for subsequent predictions, causing positional confusion. Continuing translation
    would see the gap between the true positional distance and the positional distance
    in the KV cache grow. Identifying an effective method to deal with positional
    confusion is a requirement to prevent LLM hallucinations.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '与我们之前的例子一致，会经历这种位置混淆的序列部分是 “[a]: $t_{1},t_{2},...,t_{i}$”，$s_{1}$ 之间的位置距离将分别变为
    2 和 3。然而，在使用 KV 缓存时，这个位置距离在键和值中的位置将保持为 1 和 2，从而导致位置混淆。继续翻译将看到真实位置距离与 KV 缓存中的位置距离之间的差距扩大。识别一种有效处理位置混淆的方法是防止
    LLM 幻觉的必要要求。'
- en: 4.3 Computational Inefficiency
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 计算低效
- en: Avoiding KV caching and recomputing all the keys and values at each prediction
    step is the default solution for resolving the aforementioned fine-tuning-inference
    mismatch and positional confusion problems while employing prefix fine-tuning.
    Although effective from a translation quality standpoint, doing so incurs a large
    computational cost, an undesirable result for streaming tasks like simultaneous
    translation where latency is equally important.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 避免 KV 缓存并在每个预测步骤重新计算所有的键和值是解决前缀微调中上述微调-推理不匹配和位置混淆问题的默认解决方案。虽然从翻译质量的角度来看有效，但这样做会产生巨大的计算成本，这对于像实时翻译这样延迟同样重要的流任务来说是不理想的结果。
- en: Outside of KV caching, the computational costs necessary for prefix fine-tuning
    methods are excessive. By subdividing each sample into multiple, the dataset drastically
    expands, contributing toward an increased cost to complete each epoch Agostinelli
    et al. ([2023](#bib.bib1)); Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2023](#bib.bib25)).
    Such an increase causes the duration of each epoch to rise by upwards of a factor
    of 5\. However, unlike normal methods of expanding a dataset through data augmentation,
    prefix fine-tuning does not add additional information. It is from this added
    computational burden that Agostinelli et al. ([2023](#bib.bib1)); Wang et al.
    ([2023](#bib.bib25)) are forced to fine-finetune with a subset of their entire
    prefix datasets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 KV 缓存外，前缀微调方法所需的计算成本过高。通过将每个样本细分为多个，数据集急剧扩大，从而增加了完成每个训练周期的成本。Agostinelli
    等人 ([2023](#bib.bib1))；Koshkin 等人 ([2024](#bib.bib9))；Wang 等人 ([2023](#bib.bib25))。这种增加使得每个训练周期的持续时间上升了超过
    5 倍。然而，与通过数据增强扩展数据集的正常方法不同，前缀微调并没有添加额外的信息。正因为这种额外的计算负担，Agostinelli 等人 ([2023](#bib.bib1))；Wang
    等人 ([2023](#bib.bib25)) 被迫仅用其整个前缀数据集的一个子集进行微调。
- en: Alternatively, methods of restructuring the prompt as in Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2024](#bib.bib24)) have computational burdens of their own. For
    instance, Wang et al. ([2024](#bib.bib24)) requires adding delimiting tokens in
    the prompt sequence, expanding the sequence length. Similarly, the requirement
    of padding tokens to induce a causal alignment between the source and target sequences,
    as in Koshkin et al. ([2024](#bib.bib9)), also expands the sequence length. Since
    the computational cost of the self-attention cost in the LLM scales quadratically
    with the sequence length, such a method is undesirable for both inference and
    fine-tuning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如 Koshkin 等人 ([2024](#bib.bib9)) 和 Wang 等人 ([2024](#bib.bib24)) 所述的重新构建提示的方法有其自身的计算负担。例如，Wang
    等人 ([2024](#bib.bib24)) 需要在提示序列中添加分隔符令牌，从而扩展序列长度。同样，Koshkin 等人 ([2024](#bib.bib9))
    为了在源序列和目标序列之间引入因果对齐的填充令牌要求也会扩展序列长度。由于 LLM 中自注意力的计算成本随着序列长度的平方增长，因此这种方法在推理和微调中都是不受欢迎的。
- en: Currently, no computationally efficient fine-tuning approach exists that enables
    computationally efficient inference. Identifying such a method is necessitated
    by the desire for low latency and high-quality simultaneous translations and reducing
    the already high computational costs of fine-tuning LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有一种计算上高效的微调方法可以实现计算上高效的推理。识别这样的一个方法是由于需要低延迟和高质量的实时翻译，并降低已经很高的 LLM 微调计算成本。
- en: '![Refer to caption](img/e9cec84601e1d8021e51298534928f75.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9cec84601e1d8021e51298534928f75.png)'
- en: (a) The attention for the first prediction step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一次预测步骤的注意力。
- en: '![Refer to caption](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
- en: (b) The inference mirrored attention for the first prediction step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （b）第一次预测步骤的推理镜像注意力。
- en: 'Figure 2: The matching attention connections during inference and finetuning
    for simultaneous translation.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：同步翻译中推理与微调过程中的注意力连接匹配情况。
- en: '5 SimulMask: A Paradigm Shift'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '5 SimulMask: 一种范式转变'
- en: In this work, we propose SimulMask, a paradigm shift in fine-tuning LLMs for
    simultaneous translation that eschews current methods of prompting optimization.
    It is through SimulMask, a method for restricting attention connections during
    fine-tuning, that we efficiently solve the fine-tuning-inference mismatch and
    positional confusion problem. Although we specifically apply SimulMask for the
    wait-k decision policy, it is broadly applicable to a range of decision policies.
    In this section, we will (1) explain the process of restricting attention connections
    to model simultaneous translation during fine-tuning, (2) describe SimulMask,
    a masking method for inducing these restricted attention connections, and (3)
    present a solution for positional confusion that enabled by SimulMask.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了SimulMask，这是一种在微调LLMs进行同步翻译时的范式转变，摒弃了当前的提示优化方法。通过SimulMask，这种限制注意力连接的方法，在微调过程中我们高效解决了微调与推理不匹配和位置混淆的问题。虽然我们特别应用SimulMask于wait-k决策策略，但它广泛适用于各种决策策略。在本节中，我们将（1）解释在微调过程中如何限制注意力连接以建模同步翻译，（2）描述SimulMask，这是一种用于引导这些限制性注意力连接的掩蔽方法，以及（3）介绍通过SimulMask实现的位置混淆解决方案。
- en: 5.1 Inference Mirrored Attention
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 推理镜像注意力
- en: Under simultaneous translation, the latest translation token at each prediction
    step is conditioned only on the running source tokens. Specialized attention masks
    on the Transformer could achieve such conditioning; however, directly mapping
    these to LLMs is impossible since they fail to enforce autoregressive language
    modeling and cannot mask properly when the prompt, source, and target sequences
    are collectively included in the queries and keys. As such, prior works attempted
    to achieve such conditioning during fine-tuning using prompting optimization strategies
    littered with shortcomings. We aim to return to a modeling simultaneous translation
    with attention masks by creating inference mirrored attention. Through inference
    mirrored attention, we mirror the attention connections during inference at fine-tuning
    according to the chosen decision policy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在同步翻译中，每个预测步骤的最新翻译令牌仅依赖于运行中的源令牌。Transformer上的专用注意力掩蔽可以实现这种条件；然而，由于它们未能强制执行自回归语言建模，并且在提示、源和目标序列在查询和键中共同出现时无法正确掩蔽，直接将这些掩蔽映射到LLMs是不可能的。因此，先前的工作试图通过充满缺陷的提示优化策略在微调过程中实现这种条件。我们的目标是通过创建推理镜像注意力重新回到使用注意力掩蔽的同步翻译建模中。通过推理镜像注意力，我们在微调过程中根据所选决策策略镜像推理时的注意力连接。
- en: 'As an example, suppose we model the attention connections for a wait-1 decision
    policy where the complete oracle input sequence is “$p_{1},s_{1},s_{2},s_{3},s_{4},p_{2},t_{1},t_{2},t_{3},t_{4}$
    and $p_{2}$. As such, as shown in Figure [2(a)](#S4.F2.sf1 "In Figure 2 ‣ 4.3
    Computational Inefficiency ‣ 4 Analysis and Shortcomings of Prompting Optimization
    Methods ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in
    Fine-tuning LLMs for Simultaneous Translation") the query of $p_{2}$ as shown
    in Figure [2(b)](#S4.F2.sf2 "In Figure 2 ‣ 4.3 Computational Inefficiency ‣ 4
    Analysis and Shortcomings of Prompting Optimization Methods ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation") rather than the entire source sequence. For each successive prediction
    step, the previously predicted target word, $t_{i}$ attends to identical keys
    as its inference step. We provide the attention connections for the remainder
    of the queries in our example in Appendix [A](#A1 "Appendix A Inference Mirrored
    Attention Example ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个示例，假设我们为一个 wait-1 决策策略建模注意力连接，其中完整的 oracle 输入序列为“$p_{1},s_{1},s_{2},s_{3},s_{4},p_{2},t_{1},t_{2},t_{3},t_{4}$
    和 $p_{2}$。如图 [2(a)](#S4.F2.sf1 "在图 2 ‣ 4.3 计算低效 ‣ 4 提示优化方法的分析与不足 ‣ 同时掩蔽，而非提示优化：对同时翻译的
    LLM 微调的范式转变") 所示，$p_{2}$ 的查询如图 [2(b)](#S4.F2.sf2 "在图 2 ‣ 4.3 计算低效 ‣ 4 提示优化方法的分析与不足
    ‣ 同时掩蔽，而非提示优化：对同时翻译的 LLM 微调的范式转变") 所示，而不是整个源序列。对于每一个后续预测步骤，之前预测的目标词 $t_{i}$ 会关注于与其推理步骤相同的键。我们在附录
    [A](#A1 "附录 A 推理镜像注意力示例 ‣ 同时掩蔽，而非提示优化：对同时翻译的 LLM 微调的范式转变") 中提供了我们示例中其余查询的注意力连接。
- en: 5.2 SimulMask
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 SimulMask
- en: To achieve the above inference mirrored attention, we opt for an attention mask
    to restrict attention during fine-tuning to mimic an arbitrary decision policy
    during simultaneous translation. An attention mask is preferable to prompting
    optimization as it is flexible and directly extends the LLM causal attention mask.
    We call such an attention mask SimulMask.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现上述推理镜像注意力，我们选择使用注意力掩蔽来限制在微调期间的注意力，以模拟在同时翻译过程中任意决策策略的行为。注意力掩蔽比提示优化更为优选，因为它灵活且直接扩展了
    LLM 的因果注意力掩蔽。我们称这种注意力掩蔽为 SimulMask。
- en: 'As a demonstration, let us create a SimulMask for the wait-1 policy that extends
    our example from Section [5.1](#S5.SS1 "5.1 Inference Mirrored Attention ‣ 5 SimulMask:
    A Paradigm Shift ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"). Since the LLM is autoregressive,
    SimulMask begins with a causal attention mask from which attention connections
    are removed to be identical to attention connections during simultaneous translation.
    If we recall from our example, the prompt, $p_{2}$ and $s_{2},s_{3},s_{4}$ and
    $p_{1},s_{1},s_{2},s_{3},p_{2},t_{1},t_{2}$ and $s_{4}$. The SimulMask for the
    aforementioned wait-1 policy is provided in Figure [3](#S5.F3 "Figure 3 ‣ 5.2
    SimulMask ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '作为演示，让我们为 wait-1 策略创建一个 SimulMask，该策略扩展了我们在 [5.1](#S5.SS1 "5.1 推理镜像注意力 ‣ 5
    SimulMask: 一种范式转变 ‣ 同时掩蔽，而非提示优化：对同时翻译的 LLM 微调的范式转变") 节中的示例。由于 LLM 是自回归的，SimulMask
    从一个因果注意力掩蔽开始，从中移除注意力连接，以使其与同时翻译期间的注意力连接相同。如果我们回顾一下示例，提示为 $p_{2}$ 和 $s_{2},s_{3},s_{4}$
    以及 $p_{1},s_{1},s_{2},s_{3},p_{2},t_{1},t_{2}$ 和 $s_{4}$。图 [3](#S5.F3 "图 3 ‣ 5.2
    SimulMask ‣ 5 SimulMask: 一种范式转变 ‣ 同时掩蔽，而非提示优化：对同时翻译的 LLM 微调的范式转变") 中提供了上述 wait-1
    策略的 SimulMask。'
- en: '![Refer to caption](img/5d19a3c4f060282c5a28dff95ddb9252.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d19a3c4f060282c5a28dff95ddb9252.png)'
- en: 'Figure 3: The SimulMask for modeling simultaneous translation according to
    a wait-1 decision policy during fine-tuning.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于建模同时翻译的 SimulMask，根据 wait-1 决策策略在微调期间应用。
- en: 'Since each decision policy performs read/write decisions differently and each
    limits attention differently, this requires a unique attention mask for every
    sentence. However, this can be done straightforwardly. The general procedure to
    construct a SimulMask for a given policy and sentence consists of the following
    steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每种决策策略在读取/写入决策上表现不同且限制注意力的方式也不同，这需要为每个句子创建独特的注意力掩码。然而，这可以直接完成。构建特定策略和句子的 SimulMask
    的一般步骤包括以下几个步骤：
- en: '1.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Construct a causal attention mask using Equation [2](#S2.E2 "In 2.1 Masked
    Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation") as a starting point for SimulMask.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用方程式 [2](#S2.E2 "在 2.1 掩码 Transformer 自注意力 ‣ 2 背景与相关工作 ‣ 同步掩码，而非提示优化：微调 LLM
    的同步翻译的范式转变") 作为 SimulMask 的起始点来构建因果注意力掩码。
- en: '2.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Starting from the intersection between the query that predicts the first target
    token and the first source key, apply the sub-attention mask expressed in Equation
    [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣
    Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation"). The sub-attention mask prevents the target
    queries from attending to source keys following the arbitrary decision policy.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从预测第一个目标词的查询与第一个源键的交集开始，应用方程式 [3](#S2.E3 "在 2.2 同步翻译 ‣ 2 背景与相关工作 ‣ 同步掩码，而非提示优化：微调
    LLM 的同步翻译的范式转变") 中表达的子注意力掩码。子注意力掩码防止目标查询关注按照任意决策策略后的源键。
- en: '3.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Mask any non-source queries before the query predicting the first target token
    from attending to the source keys not included in the first read decision. Such
    a step is necessary to prevent the hidden states associated with these queries
    from holding information of the entire source sequence at later layers in the
    LLM.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在预测第一个目标词的查询之前，掩盖任何非源查询，使其不能关注不包含在第一次读取决策中的源键。这样做是为了防止这些查询相关的隐藏状态在 LLM 的后续层中保存整个源序列的信息。
- en: The computation for constructing an arbitrary SimulMask is negligible compared
    with the computation of the LLM’s forward and backward passes during fine-tuning.
    Since SimulMask is not applied during inference, it does not impact computational
    cost at deployment. Therefore, SimulMask is a necessary option for mimicking simultaneous
    translation during fine-tuning and providing low-latency translations at inference.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 构建任意 SimulMask 的计算与 LLM 在微调过程中前向和反向传播的计算相比，可以忽略不计。由于在推理过程中未应用 SimulMask，它不会影响部署时的计算成本。因此，SimulMask
    是在微调过程中模拟同步翻译和提供低延迟翻译的必要选项。
- en: 5.3 Positional Reordering
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 位置重排序
- en: 'Since positional confusion during inference is a byproduct of retaining outdated
    positional information in either the keys or values, bypassing it requires providing
    a form of positional information without injecting it directly into the sequence
    or KV cache. One positioning method that satisfies such a constraint is the popular
    ALiBi, which supplies positional information through biases in attention Press
    et al. ([2021](#bib.bib20)). The bias is applied to each query-key dot product
    row in the attention calculation as shown in Equation [4](#S5.E4 "In 5.3 Positional
    Reordering ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    where $m$ is a head-specific constant.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '由于推理过程中的位置混淆是保留过时的位置信息（无论是在键还是值中）的副产品，绕过这一点需要提供一种位置性信息，而不是直接注入到序列或 KV 缓存中。一种满足这种限制的位置方法是流行的
    ALiBi，它通过注意力中的偏置来提供位置信息（Press et al. [2021](#bib.bib20)）。该偏置应用于注意力计算中的每个查询-键点积行，如方程式
    [4](#S5.E4 "在 5.3 位置重排序 ‣ 5 SimulMask: 一个范式转变 ‣ 同步掩码，而非提示优化：微调 LLM 的同步翻译的范式转变")
    所示，其中 $m$ 是特定头的常量。'
- en: '|  | ${\bm{q}}^{(h)}_{i}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}_{i}+m\cdot[-(i-1),...,-1,0]$
    |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{q}}^{(h)}_{i}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}_{i}+m\cdot[-(i-1),...,-1,0]$
    |  | (4) |'
- en: Though simple, ALiBi has demonstrated an ability to extrapolate to much larger
    sequence lengths than other state-of-the-art positional encodings, making it desirable
    for LLMs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然简单，但 ALiBi 已展示出能外推到比其他先进的位置信息编码更长的序列长度，使其对 LLMs 来说非常有价值。
- en: 'Unfortunately, ALiBi, by default, does not mesh with SimulMask due to SimulMask
    removing attention connections between the target queries and source keys. These
    removed attention connections create a gap in ALiBi biases during fine-tuning
    that are not present at inference. An example of such a gap is provided in Figure
    [4(a)](#S5.F4.sf1 "In Figure 4 ‣ 5.3 Positional Reordering ‣ 5 SimulMask: A Paradigm
    Shift ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in
    Fine-tuning LLMs for Simultaneous Translation"). In Figure [4(a)](#S5.F4.sf1 "In
    Figure 4 ‣ 5.3 Positional Reordering ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation"), the gap is present for $q_{4}$.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于 SimulMask 删除了目标查询和源键之间的注意力连接，ALiBi 默认情况下无法与 SimulMask 兼容。这些被删除的注意力连接在微调期间会在
    ALiBi 偏置中产生一个缺口，而在推理时则不存在这种缺口。图 [4(a)](#S5.F4.sf1 "在图 4 ‣ 5.3 位置重新排序 ‣ 5 SimulMask：范式转变
    ‣ 同时掩蔽，而非提示优化：微调 LLMs 以进行同时翻译的范式转变") 提供了这种缺口的示例。在图 [4(a)](#S5.F4.sf1 "在图 4 ‣ 5.3
    位置重新排序 ‣ 5 SimulMask：范式转变 ‣ 同时掩蔽，而非提示优化：微调 LLMs 以进行同时翻译的范式转变") 中，该缺口出现在 $q_{4}$
    上。
- en: '![Refer to caption](img/6f65d6e186d2654f9ca548e493be30b7.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f65d6e186d2654f9ca548e493be30b7.png)'
- en: (a) Original ALiBi.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始 ALiBi。
- en: '![Refer to caption](img/ae67c9af116f24d391c053857f087657.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae67c9af116f24d391c053857f087657.png)'
- en: (b) Modified ALiBi.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 修改后的 ALiBi。
- en: 'Figure 4: The ALiBi biases for a 5x5 attention matrix with SimulMask.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：具有 SimulMask 的 5x5 注意力矩阵的 ALiBi 偏置。
- en: 'To eliminate the bias gap, we modify ALiBi by reducing the bias values of all
    query rows influenced by SimulMask. For each query row, the reduction in bias
    values is equivalent to the number of attention connections removed along the
    row using SimulMask. Figure [4(b)](#S5.F4.sf2 "In Figure 4 ‣ 5.3 Positional Reordering
    ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") provides an
    example of such a modification. In the case of $q_{4}$; therefore, the bias on
    the right of the gap is reduced by 2\. In modifying SimulMask, we eliminate positional
    confusion from the LLM during simultaneous translation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除偏置缺口，我们通过减少所有受 SimulMask 影响的查询行的偏置值来修改 ALiBi。对于每一行查询，偏置值的减少量等于使用 SimulMask
    沿行删除的注意力连接的数量。图 [4(b)](#S5.F4.sf2 "在图 4 ‣ 5.3 位置重新排序 ‣ 5 SimulMask：范式转变 ‣ 同时掩蔽，而非提示优化：微调
    LLMs 以进行同时翻译的范式转变") 提供了这种修改的示例。在 $q_{4}$ 的情况下，因此，缺口右侧的偏置减少了 2。在修改 SimulMask 时，我们消除了
    LLM 在同时翻译中的位置混淆。
- en: 6 Experimental Setup
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验设置
- en: 6.1 Fine-tuning
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 微调
- en: Our fine-tuning experiments used the Simul-LLM framework Agostinelli et al.
    ([2023](#bib.bib1)). Each experiment used a 1.3 billion parameter Falcon model
    pre-trained on the RefinedWeb dataset Penedo et al. ([2023](#bib.bib18)). The
    model architecture consisted of 24 layers. Each self-attention calculation had
    32 attention heads, and the hidden size was 2048\. We refer to models fine-tuned
    with prefix fine-tuning that recompute and do not recompute the KV cache as falcon-prefix-rec
    and falcon-prefix-norec, respectively. The models fine-tuned with SimulMask with
    and without modifying ALiBi are named as falcon-simul-norec-mod falcon-simul-norec,
    respectively. Finally, the model fine-tuned with a causal attention mask and evaluated
    recomputing the KV cache is named as falcon-causal-rec. For comparison purposes,
    we fine-tuned a Falcon model with a causal attention mask and evaluated it for
    neural machine translation. We refer to it as falcon-offline.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的微调实验使用了 Simul-LLM 框架 Agostinelli 等人（[2023](#bib.bib1)）。每个实验都使用了一个在 RefinedWeb
    数据集 Penedo 等人（[2023](#bib.bib18)）上预训练的 13 亿参数 Falcon 模型。模型架构由 24 层组成。每个自注意力计算有
    32 个注意力头，隐藏层大小为 2048。我们将使用前缀微调、重新计算和不重新计算 KV 缓存的模型分别称为 falcon-prefix-rec 和 falcon-prefix-norec。使用
    SimulMask 进行微调的模型，修改和未修改 ALiBi 分别称为 falcon-simul-norec-mod 和 falcon-simul-norec。最后，使用因果注意力掩码并评估重新计算
    KV 缓存的模型称为 falcon-causal-rec。为了比较目的，我们用因果注意力掩码微调了一个 Falcon 模型，并对其进行了神经机器翻译评估，称其为
    falcon-offline。
- en: All fine-tuning was conducted on a single H100 GPU with bfloat16 precision.
    The non-prefix fine-tuning duration consisted of 2 epochs using a batch size of
    64, whereas the prefix fine-tuning duration consisted of 1 epoch using a batch
    size of 1024\. We applied a learning rate of $2e^{-4}$, a weight decay of 0.1,
    and an inverse square root scheduler. Our optimizer was AdamW Loshchilov and Hutter
    ([2017](#bib.bib10)). The fine-tuned falcon models all used ALiBi. All fine-tuning
    used the English-French (en-fr), English-Italian (en-it), and English-Dutch (en-nl)
    language pairs of the IWSLT 2017 training set Cettolo et al. ([2017](#bib.bib3)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所有微调均在单个 H100 GPU 上进行，使用 bfloat16 精度。非前缀微调持续了 2 个周期，批量大小为 64，而前缀微调持续了 1 个周期，批量大小为
    1024。我们应用了 $2e^{-4}$ 的学习率，0.1 的权重衰减，以及一个逆平方根调度器。我们的优化器是 AdamW Loshchilov 和 Hutter
    ([2017](#bib.bib10))。所有微调后的 falcon 模型都使用了 ALiBi。所有微调都使用了 IWSLT 2017 训练集中的英语-法语
    (en-fr)、英语-意大利语 (en-it) 和英语-荷兰语 (en-nl) 语言对 Cettolo 等 ([2017](#bib.bib3))。
- en: 6.2 Evaluation
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 评估
- en: We evaluated translation quality and latency for simultaneous translation using
    Simul-LLM inference agents Agostinelli et al. ([2023](#bib.bib1)) interfacing
    with the SimulEval toolkit Ma et al. ([2020a](#bib.bib12)). The translation quality
    was determined using detokenized BLEU with SacreBLEU Post ([2018](#bib.bib19)).
    Alternatively, the latency was determined using Length-Adaptive Average Lagging
    (LAAL) Papi et al. ([2022b](#bib.bib17)). The computational cost of simultaneous
    translation was recorded with FLOPs, and time in seconds to complete translation.
    All metrics were obtained on a single A40 GPU with bfloat16 precision. Each model
    was evaluated at a wait-k four lower, for which it was fine-tuned. For instance,
    a model fine-tuned for wait-5 was evaluated at wait-1.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Simul-LLM 推理代理 Agostinelli 等 ([2023](#bib.bib1)) 通过 SimulEval 工具包 Ma 等
    ([2020a](#bib.bib12)) 评估了同步翻译的质量和延迟。翻译质量通过去标记的 BLEU 和 SacreBLEU Post ([2018](#bib.bib19))
    确定。或者，延迟通过 Length-Adaptive Average Lagging (LAAL) Papi 等 ([2022b](#bib.bib17))
    确定。同步翻译的计算成本通过 FLOPs 和完成翻译所需的秒数记录。所有指标都在单个 A40 GPU 上使用 bfloat16 精度获得。每个模型在其微调的
    wait-k 四个较低值上进行评估。例如，微调为 wait-5 的模型在 wait-1 上进行评估。
- en: 7 Results
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结果
- en: '| Target Length | Frequency | Pred Time (s) | Rec Time (s) | Pred GFLOPs |
    Rec GFLOPs |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 目标长度 | 频率 | 预测时间 (s) | 回收时间 (s) | 预测 GFLOPs | 回收 GFLOPs |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
- en: '| 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
- en: '| 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
- en: '| 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
- en: '| 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
- en: '| 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
- en: '| 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
- en: '| 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
- en: '| 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
- en: '| 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
- en: 'Table 1: The computation measurements using the wait-3 policy for falcon-causal-rec.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用 wait-3 策略对 falcon-causal-rec 进行的计算测量。
- en: 7.1 Translation Quality and Latency Results
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 翻译质量和延迟结果
- en: 'In this section, we demonstrate the efficacy of fine-tuning with SimulMask
    compared to fine-tuning with a causal attention mask or with prefix fine-tuning
    using BLEU scores and LAAL. All evaluations are performed across wait-1, wait-3,
    wait-5, and wait-7 policies. The translation quality and latency results on the
    English-French, English-Dutch, and English-French language pairs are provided
    in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6 ‣ 7.1
    Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not
    Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了与使用因果注意力掩码或使用BLEU评分和LAAL进行前缀微调相比，使用SimulMask微调的有效性。所有评估均在wait-1、wait-3、wait-5和wait-7策略下进行。图[5](#S7.F5
    "图5 ‣ 7.1 翻译质量和延迟结果 ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")、[6](#S7.F6 "图6 ‣
    7.1 翻译质量和延迟结果 ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")和[7](#S7.F7 "图7 ‣ 7.1 翻译质量和延迟结果
    ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")中提供了英-法、英-荷和英-意语言对的翻译质量和延迟结果。
- en: '![Refer to caption](img/a3ba0180be8da62fee2448d82f90a835.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3ba0180be8da62fee2448d82f90a835.png)'
- en: 'Figure 5: The translation quality plotted against latency for LLMs on the English-French
    language pair.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在英-法语言对中，LLMs的翻译质量与延迟的关系图。
- en: '![Refer to caption](img/2ea06bc996b001a3a08eb23ea10831bd.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ea06bc996b001a3a08eb23ea10831bd.png)'
- en: 'Figure 6: The translation quality plotted against latency for LLMs on the English-Dutch
    language pair.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在英-荷语言对中，LLMs的翻译质量与延迟的关系图。
- en: '![Refer to caption](img/e4e21d4e86c37baa873880e45c217770.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4e21d4e86c37baa873880e45c217770.png)'
- en: 'Figure 7: The translation quality plotted against latency for LLMs on the English-Italian
    language pair.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在英-意语言对中，LLMs的翻译质量与延迟的关系图。
- en: 'Notably, in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6
    ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"), falcon-simul-norec-mod
    outperforms falcon-causal-rec across all wait-k values. On average, the increase
    across these wait-k values in terms of translation quality is 5.44, 8.38, and
    4.78 BLEU on the English-French, English-Dutch, and English-Italian language pairs.
    Even more impressive, falcon-simul-norec-mod outperforms falcon-prefix-rec at
    low wait-k values and matches it for higher wait-k values where, on average, across
    all wait-k values, it increases BLEU score by 7.34, 3.00, and 2.98 BLEU on the
    English-French, English-Dutch, and English-Italian language pairs. Such increases
    in translation quality reflect the efficacy of SimulMask in adapting an LLM for
    simultaneous translation during fine-tuning.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在图[5](#S7.F5 "图5 ‣ 7.1 翻译质量和延迟结果 ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")、[6](#S7.F6
    "图6 ‣ 7.1 翻译质量和延迟结果 ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")和[7](#S7.F7 "图7 ‣
    7.1 翻译质量和延迟结果 ‣ 7 结果 ‣ 同步掩码，而非提示优化：微调LLMs以进行同步翻译的范式转变")中，falcon-simul-norec-mod在所有wait-k值下均优于falcon-causal-rec。平均而言，这些wait-k值的翻译质量分别提高了5.44、8.38和4.78
    BLEU（英-法、英-荷和英-意语言对）。更令人印象深刻的是，falcon-simul-norec-mod在低wait-k值下优于falcon-prefix-rec，在高wait-k值时与其相当，在所有wait-k值中，平均提高了7.34、3.00和2.98
    BLEU（英-法、英-荷和英-意语言对）。这种翻译质量的提升反映了SimulMask在微调过程中使LLM适应同步翻译的有效性。
- en: 'Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6 ‣ 7.1
    Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not
    Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation") also provide an ablation
    demonstrating the importance of modifying ALiBi with SimulMask for high-quality
    translations by comparing falcon-simul-norec-mod with falcon-simul-norec. For
    each wait-k value and language pair, falcon-simul-norec-mod outperforms falcon-simul-norec.
    Quantified when averaged over all wait-k values, falcon-simul-norec-mod provides
    an increase of 1.20, 4.56, and 3.03 BLEU on the English-French, English-Dutch,
    and English-Italian language pairs over falcon-simul-norec. Unsurprisingly, at
    higher wait-k values where the setting approaches neural machine translation,
    the difference in BLEU scores becomes less pronounced between the models.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results
    ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation")、[6](#S7.F6 "Figure 6 ‣ 7.1 Translation Quality
    and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")和[7](#S7.F7
    "Figure 7 ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation")还展示了修改ALiBi与**SimulMask**结合的重要性，通过比较**falcon-simul-norec-mod**和**falcon-simul-norec**。对于每个wait-k值和语言对，**falcon-simul-norec-mod**的表现优于**falcon-simul-norec**。在对所有wait-k值取平均时，**falcon-simul-norec-mod**提供了在英语-法语、英语-荷兰语和英语-意大利语语言对上的1.20、4.56和3.03
    BLEU分数的提升。毫不奇怪，在较高wait-k值下，当设置接近神经机器翻译时，模型之间的BLEU分数差异变得不那么明显。'
- en: 'A secondary ablation is provided in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation
    Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    [6](#S7.F6 "Figure 6 ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results
    ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation
    Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")
    by comparing falcon-prefix-rec and falcon-prefix-norec. Doing so demonstrates
    translation quality increasing by recomputing the KV cache across all wait-k values.
    When averaged across these wait-k values, the increase is 3.58, 7.48, and 1.72
    BLEU on the English-French, English-Dutch, and English-Italian language pairs.
    Furthermore, as with the previous ablation, the difference in the BLEU score becomes
    less pronounced for the higher wait-k values.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '次级消融结果见图[5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation")、[6](#S7.F6 "Figure 6 ‣ 7.1
    Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not
    Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation")和[7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation")，通过比较**falcon-prefix-rec**和**falcon-prefix-norec**。这样做展示了通过重新计算KV缓存来提高翻译质量，涵盖了所有wait-k值。对这些wait-k值取平均时，增加值分别为英语-法语、英语-荷兰语和英语-意大利语语言对的3.58、7.48和1.72
    BLEU分数。此外，与之前的消融一样，在较高wait-k值下，BLEU分数的差异变得不那么明显。'
- en: One observation that initially may seem strange is that models evaluated at
    lower wait-k values have their LAAL deviate from their respective k to a greater
    degree than those evaluated at higher wait-k. Such an increase is a byproduct
    of the lower wait-k models generating longer predictions than their corresponding
    references. The increased generation length is a byproduct of the model hallucinating
    on sequences provided insufficient context, causing them to repeat a single token
    multiple times. The most pronounced occurrence of this hallucination is provided
    by falcon-prefix-norec and falcon-prefix-rec on the English-French language pair.
    The output predictions from both these models showcase repeating single tokens
    or phrases causing not only increased LAAL but decreased BLEU score.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一项最初可能显得奇怪的观察是，评估在较低 wait-k 值的模型，其 LAAL 从其相应的 k 偏离的程度要比评估在较高 wait-k 的模型更大。这种增加是由于较低
    wait-k 的模型生成的预测比其对应的参考更长。生成长度的增加是模型在提供不足上下文的序列上出现幻觉的副作用，导致它们多次重复单个标记。这种幻觉的最明显表现是
    falcon-prefix-norec 和 falcon-prefix-rec 在英法语言对中提供的。这两个模型的输出预测展示了重复的单个标记或短语，导致
    LAAL 增加并且 BLEU 分数降低。
- en: 7.2 Compuational Saving Results
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 计算节省结果
- en: While not using KV caching, the computation for each write step of simultaneous
    translation comprises of (1) recomputing the keys and values to reflect the source
    sequence update and (2) generating the new target tokens. In this section, we
    report the average time and GFLOPs required to recompute keys and values of the
    predicted target sequence (Rec Time and Rec GFLOPs) and generate the new target
    tokens (Pred Time and Pred GFLOPs) for each translation. In doing so, we demonstrate
    the necessity of enabling KV caching for simultaneous translation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在未使用 KV 缓存的情况下，同时翻译的每个写入步骤的计算包括（1）重新计算键和值以反映源序列的更新，以及（2）生成新的目标标记。在这一部分，我们报告了重新计算预测目标序列的键和值所需的平均时间和
    GFLOPs（Rec Time 和 Rec GFLOPs），以及生成新的目标标记所需的时间和 GFLOPs（Pred Time 和 Pred GFLOPs）。通过这样做，我们展示了启用
    KV 缓存在同时翻译中的必要性。
- en: 'All computational evaluations are performed with falcon-causal-rec using the
    wait-3 policy on a random sampling of 1000 samples from the English-French language
    pair of the IWSLT 2017 test set and are reported in Table [1](#S7.T1 "Table 1
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"). The computation measurements
    in Table [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")
    are an average of samples grouped by the number of tokens in their output translations
    (Target Length). The frequency that a predicted translation from falcon-causal-rec
    is of a specified length range is also reported (Frequency). The results for wait-1,
    wait-5, and wait-7 policies are provided in Appendix [B](#A2 "Appendix B Extended
    Computational Costs ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '所有计算评估都使用 falcon-causal-rec 进行，采用 wait-3 策略，在 IWSLT 2017 测试集的英法语言对中随机抽取了 1000
    个样本，结果报告在表格 [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")
    中。表格 [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") 中的计算测量值是根据输出翻译的标记数量（目标长度）对样本进行分组的平均值。还报告了
    falcon-causal-rec 预测翻译属于指定长度范围的频率（频率）。wait-1、wait-5 和 wait-7 策略的结果见附录 [B](#A2
    "Appendix B Extended Computational Costs ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")。'
- en: 'From Table [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    in terms of GFLOPs, the recomputation of the keys and values contributes toward
    a greater portion of the total computation than the new target token generation.
    However, inversely, more time is spent generating new target tokens than recomputing
    the keys and values. The reason for the discrepancy is the recomputation of the
    keys and values is performed in parallel on the A40 GPU whereas the generation
    of the new target tokens is a sequential operation. On average, across all target
    lengths, taking into account their frequencies, the recomputation of the KV cache
    contributed to 26.8% of the computation time and 87.9% of the FLOPs required to
    generate a target translation at wait-3.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '从表[1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")来看，在 GFLOPs
    方面，键和值的重新计算占总计算量的比重大于新目标令牌生成。然而，相反地，生成新目标令牌所花费的时间要比重新计算键和值的时间多。差异的原因在于，键和值的重新计算在
    A40 GPU 上并行进行，而新目标令牌的生成是一个顺序操作。平均而言，在所有目标长度中，考虑到它们的频率，KV 缓存的重新计算占据了计算时间的 26.8%
    和生成目标翻译所需 FLOPs 的 87.9%，在 wait-3 状态下。'
- en: 8 Conclusion
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this work, we examine current LLM fine-tuning approaches for simultaneous
    translation and identify their shortcomings. We then propose a new paradigm for
    fine-tuning LLM for simultaneous translation using a novel SimulMask that avoids
    the shortcomings of previous methods. Using SimulMask, the target sequence is
    prevented from attending to a portion of the source sequence according to an arbitrary
    decision policy modeling simultaneous translation. Through the application of
    SimulMask, we can efficiently fine-tune an LLM for simultaneous translation and
    reduce the computational costs of inference by eliminating the recomputation of
    the KV cache for the target sequence, unlike prior works. Furthermore, we can
    exceed or match the translation quality of prior works at all wait-k values across
    multiple language pairs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们检查了当前 LLM 微调方法在同步翻译中的应用，并识别出它们的不足之处。然后，我们提出了一种用于同步翻译的新微调范式，使用一种新颖的
    SimulMask，避免了先前方法的不足。使用 SimulMask，可以根据任意决策策略建模同步翻译，防止目标序列关注源序列的一部分。通过应用 SimulMask，我们可以高效地微调
    LLM 以进行同步翻译，并通过消除对目标序列 KV 缓存的重新计算来降低推理的计算成本，这与之前的工作不同。此外，我们可以在多个语言对的所有 wait-k
    值下超越或匹配先前工作的翻译质量。
- en: References
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agostinelli et al. (2023) Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif
    Fuad, and Lizhong Chen. 2023. Simul-llm: A framework for exploring high-quality
    simultaneous translation with large language models. *arXiv preprint arXiv:2312.04691*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agostinelli 等（2023）Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif Fuad,
    和 Lizhong Chen。2023年。Simul-llm：一个用于探索高质量同步翻译的大型语言模型的框架。*arXiv 预印本 arXiv:2312.04691*。
- en: Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel
    Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open
    language models. *arXiv preprint arXiv:2311.16867*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei 等（2023）Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro
    Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
    Julien Launay, Quentin Malartic 等。2023年。猎鹰系列开放语言模型。*arXiv 预印本 arXiv:2311.16867*。
- en: Cettolo et al. (2017) Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan
    Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann.
    2017. [Overview of the IWSLT 2017 evaluation campaign](https://aclanthology.org/2017.iwslt-1.1).
    In *Proceedings of the 14th International Conference on Spoken Language Translation*,
    pages 2–14, Tokyo, Japan. International Workshop on Spoken Language Translation.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cettolo 等（2017）Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues,
    Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, 和 Christian Federmann。2017年。
    [IWSLT 2017 评估活动概述](https://aclanthology.org/2017.iwslt-1.1)。在 *第十四届国际口语语言翻译大会论文集*
    中，第 2–14 页，日本东京。国际口语语言翻译研讨会。
- en: Cho and Esipova (2016) Kyunghyun Cho and Masha Esipova. 2016. Can neural machine
    translation do simultaneous translation? *arXiv preprint arXiv:1606.02012*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 和 Esipova（2016）Kyunghyun Cho 和 Masha Esipova。2016年。神经机器翻译能否进行同步翻译？*arXiv
    预印本 arXiv:1606.02012*。
- en: 'Gu et al. (2017) Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li.
    2017. [Learning to translate in real-time with neural machine translation](https://aclanthology.org/E17-1099).
    In *Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 1, Long Papers*, pages 1053–1062, Valencia,
    Spain. Association for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2017）谷家涛、格雷厄姆·纽比、赵庆勋和维克多·O.K.·李。2017。[实时神经机器翻译学习](https://aclanthology.org/E17-1099)。在
    *第15届欧洲计算语言学协会年会：第一卷，长篇论文*，页码 1053–1062，西班牙瓦伦西亚。计算语言学协会。
- en: 'Guo et al. (2024) Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, and Yang
    Feng. 2024. Sillm: Large language models for simultaneous machine translation.
    *arXiv preprint arXiv:2402.13036*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人（2024）郭守涛、张少磊、马政睿、张敏和冯阳。2024。Sillm: 大型语言模型用于同步机器翻译。*arXiv 预印本 arXiv:2402.13036*。'
- en: Iyer et al. (2023) Vivek Iyer, Pinzhen Chen, and Alexandra Birch. 2023. [Towards
    effective disambiguation for machine translation with large language models](https://doi.org/10.18653/v1/2023.wmt-1.44).
    In *Proceedings of the Eighth Conference on Machine Translation*, pages 482–495,
    Singapore. Association for Computational Linguistics.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer 等人（2023）维维克·艾耶、陈品臻和亚历山德拉·伯奇。2023。[面向大语言模型的机器翻译有效消歧](https://doi.org/10.18653/v1/2023.wmt-1.44)。在
    *第八届机器翻译会议*，页码 482–495，新加坡。计算语言学协会。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2023）阿尔伯特·Q·江、亚历山大·萨布拉约尔、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦吉尔、纪尧姆·兰普尔、露西尔·索尔尼耶等人。2023。Mistral
    7b。*arXiv 预印本 arXiv:2310.06825*。
- en: 'Koshkin et al. (2024) Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura.
    2024. Transllama: Llm-based simultaneous translation system. *arXiv preprint arXiv:2402.04636*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koshkin 等人（2024）罗曼·科什金、须藤胜人和中村聪。2024。Transllama: 基于 Llm 的同步翻译系统。*arXiv 预印本
    arXiv:2402.04636*。'
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter（2017）伊利亚·洛什基洛夫和弗兰克·胡特。2017。解耦权重衰减正则化。*arXiv 预印本 arXiv:1711.05101*。
- en: 'Ma et al. (2019) Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu,
    Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
    Haifeng Wang. 2019. [STACL: Simultaneous translation with implicit anticipation
    and controllable latency using prefix-to-prefix framework](https://doi.org/10.18653/v1/P19-1289).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3025–3036, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人（2019）马名博、梁辉、肖浩、任杰·郑、刘凯博、郑拜功、张传强、何中俊、刘海容、李星、吴华和王海峰。2019。[STACL: 使用前缀到前缀框架进行隐式预期和可控延迟的同步翻译](https://doi.org/10.18653/v1/P19-1289)。在
    *第57届计算语言学协会年会*，页码 3025–3036，意大利佛罗伦萨。计算语言学协会。'
- en: 'Ma et al. (2020a) Xutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu,
    and Juan Pino. 2020a. [SIMULEVAL: An evaluation toolkit for simultaneous translation](https://doi.org/10.18653/v1/2020.emnlp-demos.19).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 144–150, Online. Association for Computational
    Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人（2020a）马旭台、穆罕默德·贾瓦德·杜斯蒂、常汉旺、谷家涛和胡安·皮诺。2020a。[SIMULEVAL: 同步翻译的评估工具包](https://doi.org/10.18653/v1/2020.emnlp-demos.19)。在
    *2020年自然语言处理领域实证方法会议：系统演示*，页码 144–150，在线。计算语言学协会。'
- en: 'Ma et al. (2020b) Xutai Ma, Juan Pino, and Philipp Koehn. 2020b. Simulmt to
    simulst: Adapting simultaneous text translation to end-to-end simultaneous speech
    translation. *arXiv preprint arXiv:2011.02048*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人（2020b）马旭台、胡安·皮诺和菲利普·科恩。2020b。Simulmt 到 simulst: 将同步文本翻译适应到端到端同步语音翻译。*arXiv
    预印本 arXiv:2011.02048*。'
- en: 'Moser-Mercer et al. (1998) Barbara Moser-Mercer, Alexander Künzli, and Marina
    Korac. 1998. Prolonged turns in interpreting: Effects on quality, physiological
    and psychological stress (pilot study). *Interpreting*, 3(1):47–64.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moser-Mercer 等人（1998）芭芭拉·摩泽-梅瑟、亚历山大·昆茨利和玛丽娜·科拉奇。1998。口译中的长时间轮次：对质量、生理和心理压力的影响（初步研究）。*口译*，3(1):47–64。
- en: Moslem et al. (2023) Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy
    Way. 2023. [Adaptive machine translation with large language models](https://aclanthology.org/2023.eamt-1.22).
    In *Proceedings of the 24th Annual Conference of the European Association for
    Machine Translation*, pages 227–237, Tampere, Finland. European Association for
    Machine Translation.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moslem 等（2023）Yasmin Moslem、Rejwanul Haque、John D. Kelleher 和 Andy Way。2023。[使用大型语言模型进行自适应机器翻译](https://aclanthology.org/2023.eamt-1.22)。在*第
    24 届欧洲机器翻译协会年会论文集*，第 227–237 页，坦佩雷，芬兰。欧洲机器翻译协会。
- en: 'Papi et al. (2022a) Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi.
    2022a. [Does simultaneous speech translation need simultaneous models?](https://doi.org/10.18653/v1/2022.findings-emnlp.11)
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    141–153, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papi 等（2022a）Sara Papi、Marco Gaido、Matteo Negri 和 Marco Turchi。2022a。[同步语音翻译是否需要同步模型？](https://doi.org/10.18653/v1/2022.findings-emnlp.11)。在*计算语言学协会：EMNLP
    2022 会议论文集*，第 141–153 页，阿布扎比，阿联酋。计算语言学协会。
- en: 'Papi et al. (2022b) Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi.
    2022b. [Over-generation cannot be rewarded: Length-adaptive average lagging for
    simultaneous speech translation](https://doi.org/10.18653/v1/2022.autosimtrans-1.2).
    In *Proceedings of the Third Workshop on Automatic Simultaneous Translation*,
    pages 12–17, Online. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papi 等（2022b）Sara Papi、Marco Gaido、Matteo Negri 和 Marco Turchi。2022b。[过度生成不能被奖励：长度自适应平均滞后用于同步语音翻译](https://doi.org/10.18653/v1/2022.autosimtrans-1.2)。在*第三届自动同步翻译研讨会论文集*，第
    12–17 页，在线。计算语言学协会。
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. 2023. [The RefinedWeb dataset for Falcon LLM: outperforming
    curated corpora with web data, and web data only](https://arxiv.org/abs/2306.01116).
    *arXiv preprint arXiv:2306.01116*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo 等（2023）Guilherme Penedo、Quentin Malartic、Daniel Hesslow、Ruxandra Cojocaru、Alessandro
    Cappelli、Hamza Alobeidli、Baptiste Pannier、Ebtesam Almazrouei 和 Julien Launay。2023。[Falcon
    LLM 的 RefinedWeb 数据集：用网页数据超越精心策划的语料库，仅使用网页数据](https://arxiv.org/abs/2306.01116)。*arXiv
    预印本 arXiv:2306.01116*。
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting bleu scores. *arXiv
    preprint arXiv:1804.08771*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post（2018）Matt Post。2018。呼吁在报告 BLEU 分数时保持清晰。*arXiv 预印本 arXiv:1804.08771*。
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等（2021）Ofir Press、Noah A Smith 和 Mike Lewis。2021。训练短，测试长：带线性偏差的注意力机制实现输入长度外推。*arXiv
    预印本 arXiv:2108.12409*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。2023。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N Gomez、Łukasz Kaiser 和 Illia Polosukhin。2017。[注意力机制是你所需要的一切](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。在*神经信息处理系统进展*，第
    30 卷。Curran Associates, Inc.
- en: 'Vilar et al. (2023) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
    Viresh Ratnakar, and George Foster. 2023. [Prompting PaLM for translation: Assessing
    strategies and performance](https://doi.org/10.18653/v1/2023.acl-long.859). In
    *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pages 15406–15427, Toronto, Canada. Association for
    Computational Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vilar 等（2023）David Vilar、Markus Freitag、Colin Cherry、Jiaming Luo、Viresh Ratnakar
    和 George Foster。2023。[为翻译提示 PaLM：评估策略和性能](https://doi.org/10.18653/v1/2023.acl-long.859)。在*第
    61 届计算语言学协会年会（第一卷：长篇论文）*，第 15406–15427 页，多伦多，加拿大。计算语言学协会。
- en: 'Wang et al. (2024) Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, and Gholamreza
    Haffari. 2024. Conversational simulmt: Efficient simultaneous translation with
    large language models. *arXiv preprint arXiv:2402.10552*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2024) Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, 和 Gholamreza Haffari.
    2024. 会话式 SimulMT: 使用大型语言模型的高效实时翻译。 *arXiv 预印本 arXiv:2402.10552*。'
- en: Wang et al. (2023) Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fatemeh Shiri,
    Ehsan Shareghi, and Gholamreza Haffari. 2023. Simultaneous machine translation
    with large language models. *arXiv preprint arXiv:2309.06706*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023) Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fatemeh Shiri, Ehsan
    Shareghi, 和 Gholamreza Haffari. 2023. 使用大型语言模型的实时机器翻译。 *arXiv 预印本 arXiv:2309.06706*。
- en: 'Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.
    2023. A paradigm shift in machine translation: Boosting translation performance
    of large language models. *arXiv preprint arXiv:2309.11674*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等 (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, 和 Hany Hassan Awadalla. 2023.
    机器翻译中的范式转变: 提升大型语言模型的翻译性能。 *arXiv 预印本 arXiv:2309.11674*。'
- en: 'Zhang et al. (2023) Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn.
    2023. [Machine translation with large language models: Prompting, few-shot learning,
    and fine-tuning with QLoRA](https://doi.org/10.18653/v1/2023.wmt-1.43). In *Proceedings
    of the Eighth Conference on Machine Translation*, pages 468–481, Singapore. Association
    for Computational Linguistics.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2023) Xuan Zhang, Navid Rajabi, Kevin Duh, 和 Philipp Koehn. 2023.
    [使用大型语言模型的机器翻译: 提示、少量学习和 QLoRA 微调](https://doi.org/10.18653/v1/2023.wmt-1.43)。见于
    *第八届机器翻译会议论文集*，第 468–481 页，新加坡。计算语言学协会。'
- en: Zheng et al. (2019) Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang Huang.
    2019. Simpler and faster learning of adaptive policies for simultaneous translation.
    *arXiv preprint arXiv:1909.01559*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2019) Baigong Zheng, Renjie Zheng, Mingbo Ma, 和 Liang Huang. 2019.
    更简单、更快速地学习适应策略以实现实时翻译。 *arXiv 预印本 arXiv:1909.01559*。
- en: Appendix A Inference Mirrored Attention Example
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 推理镜像注意力示例
- en: '![Refer to caption](img/e9cec84601e1d8021e51298534928f75.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9cec84601e1d8021e51298534928f75.png)'
- en: (a) The attention for the first prediction step.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一预测步骤的注意力。
- en: '![Refer to caption](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
- en: (b) The inference mirrored attention for the first prediction step.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第一预测步骤的推理镜像注意力。
- en: '![Refer to caption](img/7e17fd2888d91b96847da8129e15079d.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e17fd2888d91b96847da8129e15079d.png)'
- en: (c) The attention for the second prediction step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第二预测步骤的注意力。
- en: '![Refer to caption](img/755d92e3ebc8a61acb90053579b3d751.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/755d92e3ebc8a61acb90053579b3d751.png)'
- en: (d) The inference mirrored attention for the second prediction step.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第二预测步骤的推理镜像注意力。
- en: '![Refer to caption](img/04602375295f9b9d7e6f8b4466d7d5f2.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04602375295f9b9d7e6f8b4466d7d5f2.png)'
- en: (e) The attention for the third prediction step.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 第三预测步骤的注意力。
- en: '![Refer to caption](img/f5abd215725db8657c0f20a753aed436.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5abd215725db8657c0f20a753aed436.png)'
- en: (f) The inference mirrored attention for the third prediction step.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 第三预测步骤的推理镜像注意力。
- en: 'Figure 8: The attention during inference and finetuning for simultaneous translation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 实时翻译中推理和微调期间的注意力。'
- en: '![Refer to caption](img/65e36fadd6b661a3a74cb64e22f1439c.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65e36fadd6b661a3a74cb64e22f1439c.png)'
- en: 'Figure 9: A color-coded SimulMask for the wait-1 policy.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 为 wait-1 策略生成的彩色编码 SimulMask。'
- en: '![Refer to caption](img/94a5337d7116679066869f502da43772.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94a5337d7116679066869f502da43772.png)'
- en: 'Figure 10: Color-coded inference mirrored attention produced by SimulMask for
    the wait-1 policy.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 由 SimulMask 为 wait-1 策略生成的彩色编码推理镜像注意力。'
- en: Appendix B Extended Computational Costs
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 扩展计算成本
- en: '| Wait-k | Length | Frequency | Pred Time (s) | Rec Time (s) | Pred GFLOPs
    | Rec GFLOPs |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 等待-k | 长度 | 频率 | 预测时间 (秒) | 接收时间 (秒) | 预测 GFLOPs | 接收 GFLOPs |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 0-10 | 27.9 % | 0.30 | 0.09 | 22.32 | 27.21 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0-10 | 27.9 % | 0.30 | 0.09 | 22.32 | 27.21 |'
- en: '| 1 | 10-20 | 33.0 % | 0.73 | 0.29 | 43.46 | 125.00 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10-20 | 33.0 % | 0.73 | 0.29 | 43.46 | 125.00 |'
- en: '| 1 | 20-30 | 17.2 % | 1.28 | 0.52 | 70.76 | 315.79 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 20-30 | 17.2 % | 1.28 | 0.52 | 70.76 | 315.79 |'
- en: '| 1 | 30-40 | 8.5 % | 1.82 | 0.76 | 96.91 | 592.41 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 30-40 | 8.5 % | 1.82 | 0.76 | 96.91 | 592.41 |'
- en: '| 1 | 40-50 | 4.7 % | 2.35 | 0.98 | 123.13 | 920.19 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 40-50 | 4.7 % | 2.35 | 0.98 | 123.13 | 920.19 |'
- en: '| 1 | 50-60 | 3.2 % | 2.90 | 1.19 | 149.76 | 1314.32 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 50-60 | 3.2 % | 2.90 | 1.19 | 149.76 | 1314.32 |'
- en: '| 1 | 60-70 | 1.6 % | 3.46 | 1.43 | 176.84 | 1868.62 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 60-70 | 1.6 % | 3.46 | 1.43 | 176.84 | 1868.62 |'
- en: '| 1 | 70-80 | 1.1 % | 3.99 | 1.55 | 202.72 | 2197.84 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 70-80 | 1.1 % | 3.99 | 1.55 | 202.72 | 2197.84 |'
- en: '| 1 | 80-90 | 0.9 % | 4.51 | 1.72 | 229.44 | 2725.79 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 80-90 | 0.9 % | 4.51 | 1.72 | 229.44 | 2725.79 |'
- en: '| 1 | 90-100 | 0.6 % | 5.04 | 1.70 | 254.21 | 2682.38 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 90-100 | 0.6 % | 5.04 | 1.70 | 254.21 | 2682.38 |'
- en: '| 3 | 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
- en: '| 3 | 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
- en: '| 3 | 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
- en: '| 3 | 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
- en: '| 3 | 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
- en: '| 3 | 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
- en: '| 3 | 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
- en: '| 3 | 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
- en: '| 3 | 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
- en: '| 3 | 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
- en: '| 5 | 0-10 | 4.5 % | 0.35 | 0.01 | 24.98 | 3.21 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0-10 | 4.5 % | 0.35 | 0.01 | 24.98 | 3.21 |'
- en: '| 5 | 10-20 | 20.0 % | 0.80 | 0.11 | 47.13 | 40.15 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 10-20 | 20.0 % | 0.80 | 0.11 | 47.13 | 40.15 |'
- en: '| 5 | 20-30 | 24.0 % | 1.31 | 0.35 | 71.70 | 178.58 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 20-30 | 24.0 % | 1.31 | 0.35 | 71.70 | 178.58 |'
- en: '| 5 | 30-40 | 17.6 % | 1.82 | 0.58 | 96.96 | 393.19 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 30-40 | 17.6 % | 1.82 | 0.58 | 96.96 | 393.19 |'
- en: '| 5 | 40-50 | 11.6 % | 2.36 | 0.84 | 123.17 | 720.18 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 40-50 | 11.6 % | 2.36 | 0.84 | 123.17 | 720.18 |'
- en: '| 5 | 50-60 | 6.9 % | 2.92 | 1.09 | 150.63 | 1137.89 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 50-60 | 6.9 % | 2.92 | 1.09 | 150.63 | 1137.89 |'
- en: '| 5 | 60-70 | 4.8 % | 3.40 | 1.26 | 174.20 | 1527.99 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 60-70 | 4.8 % | 3.40 | 1.26 | 174.20 | 1527.99 |'
- en: '| 5 | 70-80 | 3.3 % | 4.02 | 1.36 | 204.38 | 1744.19 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 70-80 | 3.3 % | 4.02 | 1.36 | 204.38 | 1744.19 |'
- en: '| 5 | 80-90 | 2.3 % | 4.60 | 1.80 | 232.77 | 2856.33 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 80-90 | 2.3 % | 4.60 | 1.80 | 232.77 | 2856.33 |'
- en: '| 5 | 90-100 | 1.6 % | 5.07 | 1.88 | 255.55 | 3146.94 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 90-100 | 1.6 % | 5.07 | 1.88 | 255.55 | 3146.94 |'
- en: '| 7 | 0-10 | 3.7 % | 0.38 | 0 | 26.05 | 0 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0-10 | 3.7 % | 0.38 | 0 | 26.05 | 0 |'
- en: '| 7 | 10-20 | 19.8 % | 0.80 | 0.04 | 60.47 | 12.11 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 10-20 | 19.8 % | 0.80 | 0.04 | 60.47 | 12.11 |'
- en: '| 7 | 20-30 | 22.1 % | 1.31 | 0.22 | 71.96 | 95.39 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 20-30 | 22.1 % | 1.31 | 0.22 | 71.96 | 95.39 |'
- en: '| 7 | 30-40 | 18.5 % | 1.83 | 0.49 | 97.44 | 303.22 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 30-40 | 18.5 % | 1.83 | 0.49 | 97.44 | 303.22 |'
- en: '| 7 | 40-50 | 12.0 % | 2.35 | 0.71 | 122.54 | 560.15 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 40-50 | 12.0 % | 2.35 | 0.71 | 122.54 | 560.15 |'
- en: '| 7 | 50-60 | 7.8 % | 2.94 | 0.98 | 151.09 | 972.22 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 50-60 | 7.8 % | 2.94 | 0.98 | 151.09 | 972.22 |'
- en: '| 7 | 60-70 | 5.1 % | 3.44 | 1.16 | 175.45 | 1311.03 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 60-70 | 5.1 % | 3.44 | 1.16 | 175.45 | 1311.03 |'
- en: '| 7 | 70-80 | 2.9 % | 4.03 | 1.25 | 204.60 | 1491.26 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 70-80 | 2.9 % | 4.03 | 1.25 | 204.60 | 1491.26 |'
- en: '| 7 | 80-90 | 2.3 % | 4.58 | 1.55 | 230.83 | 2190.85 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 80-90 | 2.3 % | 4.58 | 1.55 | 230.83 | 2190.85 |'
- en: '| 7 | 90-100 | 1.3 % | 5.07 | 1.80 | 255.25 | 2262.61 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 90-100 | 1.3 % | 5.07 | 1.80 | 255.25 | 2262.61 |'
- en: 'Table 2: The relationship between the target translation length and computational
    costs.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：目标翻译长度与计算成本之间的关系。
