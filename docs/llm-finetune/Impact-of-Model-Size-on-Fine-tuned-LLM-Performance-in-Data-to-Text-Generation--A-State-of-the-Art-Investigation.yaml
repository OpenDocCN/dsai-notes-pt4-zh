- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:35:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型规模对数据到文本生成中微调大语言模型（LLM）性能的影响：一项前沿调查
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14088](https://ar5iv.labs.arxiv.org/html/2407.14088)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14088](https://ar5iv.labs.arxiv.org/html/2407.14088)
- en: Joy Mahapatra joymahapatra90@gmail.com Indian Statistical Institute Kolkata
    Utpal Garain utpal@isical.ac.in Indian Statistical Institute Kolkata
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Joy Mahapatra joymahapatra90@gmail.com 印度统计研究所 加尔各答 Utpal Garain utpal@isical.ac.in
    印度统计研究所 加尔各答
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Data-to-text (D2T) generation aims to generate human-readable text from semi-structured
    data, such as tables and graphs. The recent success of D2T is largely attributed
    to advancements in LLMs. Despite the success of LLMs, no research has been conducted
    to illustrate the impact of model size on the performance of fine-tuned LLMs for
    D2T tasks. D2T model performance is typically assessed based on three key qualities:
    readability (indicates fluency and coherence), informativeness (measures content
    similarity), and faithfulness (assesses consistency of factual information). It
    is currently uncertain whether increasing the size of LLMs effectively improves
    performance in D2T tasks across these three qualities. The objective of this study
    is to investigate the performance of fine-tuned LLMs in D2T tasks in terms of
    model size. Through extensive comparative analysis, we aim to elucidate both the
    advantages and limitations of scaling model sizes across five widely used D2T
    datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and twelve state-of-the-art
    LLMs with varying sizes from five different LLM families (T5, BART, OPT, BLOOM,
    and Llama 2). To comprehensively cover all the three essential qualities of D2T
    models, we incorporate six widely recognized automatic metrics—BLEU, METEOR, BERTScore,
    MoverScore, Parent, and BARTScore. We also provide an in-depth analysis of LLM
    performance concerning model size in the presence of source-reference divergence,
    a critical aspect of D2T tasks. Our investigation reveals that increasing LLM
    size enhances readability and informativeness in D2T tasks, but larger (in terms
    of size) LLMs may sacrifice faithfulness. Moreover, small-sized LLMs show more
    resilience than larger ones when source-reference divergence is present.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据到文本（D2T）生成旨在从半结构化数据（如表格和图表）中生成可读文本。D2T的近期成功在很大程度上归功于大语言模型（LLMs）的进步。尽管LLMs取得了成功，但尚未进行研究来说明模型规模对微调LLMs在D2T任务中性能的影响。D2T模型性能通常基于三个关键特性进行评估：可读性（指示流畅性和连贯性）、信息量（衡量内容相似性）和忠实性（评估事实信息的一致性）。目前尚不确定增加LLMs的规模是否能有效提升D2T任务在这三项特性上的性能。本研究的目标是调查微调LLMs在D2T任务中就模型规模而言的表现。通过广泛的比较分析，我们旨在阐明在五个广泛使用的D2T数据集（E2E、ViGGo、WikiTableText、DART和WebNLG）以及来自五个不同LLM家族（T5、BART、OPT、BLOOM和Llama
    2）的十二个前沿LLMs中，扩大模型规模的优点和局限性。为了全面覆盖D2T模型的三个基本特性，我们结合了六种广泛认可的自动评估指标——BLEU、METEOR、BERTScore、MoverScore、Parent和BARTScore。我们还提供了关于LLM在模型规模方面的表现的深入分析，特别是在源参考分歧存在的情况下，这是D2T任务的一个关键方面。我们的调查显示，增加LLM规模可以提升D2T任务中的可读性和信息量，但较大的（在规模上）LLMs可能会牺牲忠实性。此外，较小规模的LLMs在存在源参考分歧时表现出比较大规模的LLMs更强的抗干扰能力。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Since their inception, large language models (LLMs) have emerged as dominant
    models in text generations, surpassing their predecessor models in terms of generality
    and performance (Raffel et al., [2020](#bib.bib88); Brown et al., [2020](#bib.bib11);
    Chowdhery et al., [2023](#bib.bib15); Touvron et al., [2023](#bib.bib103)). Current
    LLMs have demonstrated exceptional performance in a multitude of text generation
    tasks, including but not limited to machine translation, automatic summarization,
    and text simplification (Lewis et al., [2020](#bib.bib51); Zhang et al., [2022](#bib.bib121);
    Taori et al., [2023](#bib.bib100)). The remarkable effectiveness of LLMs in these
    tasks has led to a clear trend of increasing their sizes (numbers of model parameters)
    in existing LLMs to further enhance performance (Petroni et al., [2019](#bib.bib83);
    Roberts et al., [2020](#bib.bib91)). Consequently, there has been an emergence
    of numerous new LLM families (Touvron et al., [2023](#bib.bib103); Chowdhery et al.,
    [2023](#bib.bib15); Scao et al., [2022](#bib.bib92)) characterized by substantially
    larger sizes, which emphasizes the ongoing progress within the field of text generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自其诞生以来，大型语言模型（LLMs）已经成为文本生成领域的主导模型，在通用性和性能上超越了其前身模型（Raffel et al., [2020](#bib.bib88);
    Brown et al., [2020](#bib.bib11); Chowdhery et al., [2023](#bib.bib15); Touvron
    et al., [2023](#bib.bib103)）。当前的LLMs在多种文本生成任务中表现出色，包括但不限于机器翻译、自动摘要和文本简化（Lewis
    et al., [2020](#bib.bib51); Zhang et al., [2022](#bib.bib121); Taori et al., [2023](#bib.bib100)）。LLMs在这些任务中的显著效果导致了现有LLMs模型参数数量的持续增长，以进一步提升性能（Petroni
    et al., [2019](#bib.bib83); Roberts et al., [2020](#bib.bib91)）。因此，出现了许多新的LLM家族（Touvron
    et al., [2023](#bib.bib103); Chowdhery et al., [2023](#bib.bib15); Scao et al.,
    [2022](#bib.bib92)），这些家族的规模大大增加，突显了文本生成领域的持续进展。
- en: 'Data-to-text (D2T) generation (Lin et al., [2024](#bib.bib56)), an essential
    facet of text generation, where goal is to produce human-readable text from semi-structured
    data sources, including slot-value paired meaning representations (MR) (Novikova
    et al., [2017](#bib.bib75)), tables (Bao et al., [2018](#bib.bib5)), or graphs (Nan
    et al., [2021](#bib.bib70)). Based on the source data types, three major types
    of D2T tasks are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"). D2T has diverse applications across various domains, spanning
    dialogue generation (Wen et al., [2015](#bib.bib110)), sports reporting (Wiseman
    et al., [2017](#bib.bib111)), weather forecasting (Belz, [2008](#bib.bib6)), business
    intelligence (Gatt and Krahmer, [2018](#bib.bib27)), and many more. In critical
    fields like healthcare, D2T plays a vital role in automatic diagnostic reporting (Hommes
    et al., [2019](#bib.bib36); Yermakov et al., [2021](#bib.bib117)), underscoring
    its relevance in critical decision-making. The recent success of D2T tasks can
    be largely attributed to the advancement of LLMs, which are leveraged through
    several effective inference and training methods such as parameter-efficient fine-tuning (Li
    and Liang, [2021](#bib.bib55); Lester et al., [2021](#bib.bib50); Dettmers et al.,
    [2023](#bib.bib16)). The synergy between LLMs and D2T generation marks a significant
    advancement (Li et al., [2024b](#bib.bib53); Jing et al., [2024](#bib.bib41);
    Kasner and Dusek, [2024](#bib.bib45)) across various domains and critical safety
    applications.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '数据到文本（D2T）生成（Lin et al., [2024](#bib.bib56)），是文本生成的一个重要方面，其目标是从半结构化数据源中生成可读文本，包括槽值配对的语义表示（MR）（Novikova
    et al., [2017](#bib.bib75)）、表格（Bao et al., [2018](#bib.bib5)）或图形（Nan et al., [2021](#bib.bib70)）。根据源数据类型，D2T任务的三种主要类型如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")所示。D2T在对话生成（Wen
    et al., [2015](#bib.bib110)）、体育报道（Wiseman et al., [2017](#bib.bib111)）、天气预报（Belz,
    [2008](#bib.bib6)）、商业智能（Gatt and Krahmer, [2018](#bib.bib27)）等各种领域有着广泛应用。在医疗保健等关键领域，D2T在自动诊断报告中扮演着重要角色（Hommes
    et al., [2019](#bib.bib36); Yermakov et al., [2021](#bib.bib117)），突显了其在关键决策中的重要性。D2T任务的近期成功在很大程度上归因于LLMs的发展，这些模型通过多种有效的推理和训练方法，如参数高效的微调（Li
    and Liang, [2021](#bib.bib55); Lester et al., [2021](#bib.bib50); Dettmers et
    al., [2023](#bib.bib16)）得以实现。LLMs与D2T生成的协同标志着在各个领域和关键安全应用中的重大进展（Li et al., [2024b](#bib.bib53);
    Jing et al., [2024](#bib.bib41); Kasner and Dusek, [2024](#bib.bib45)）。'
- en: '![Refer to caption](img/da8ffed2591c5bfa953ff83c68b7e038.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/da8ffed2591c5bfa953ff83c68b7e038.png)'
- en: 'Figure 1: Overview of data-to-text (D2T) generation with three major types:
    graph-to-text (left), table-to-text (middle), MR (meaning representation)-to-text
    (right).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：数据到文本（D2T）生成的概述，包括三种主要类型：图表到文本（左）、表格到文本（中）、MR（意义表示）到文本（右）。
- en: 'Despite advancements in LLMs for D2T tasks, there exists a siggnificant gap
    in analyzing the impact of model size on the performance of fine-tuned LLMs as
    D2T models. The assessment of D2T models commonly revolves around three fundamental
    performance qualities: readability, informativeness, and faithfulness (see Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")). Readability (Chen
    et al., [2020](#bib.bib14); Li et al., [2022](#bib.bib54)) mainly concerns the
    fluency and coherence of generated text from the model, whereas informativeness (Shen
    et al., [2019](#bib.bib96); Li et al., [2022](#bib.bib54)) determines whether
    the D2T model is effective in terms of transforming essential information from
    given data to generated text. Faithfulness (Tian et al., [2019](#bib.bib102);
    Wang et al., [2020](#bib.bib109)) is an important performance indicator for D2T,
    assessing whether the generated text presents any incorrect or irrelevant facts
    w.r.t. the given source data. Considering the huge potential of LLMs in various
    critical D2T domains (Hommes et al., [2019](#bib.bib36); Pauws et al., [2019](#bib.bib79)),
    it is crucial to understand the impact of model size over fine-tuned LLMs in terms
    of D2T performance. However, almost no existing literature has investigated the
    impact of model size on the performance of fine-tuned LLMs across all three performance
    qualities of D2T.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 D2T 任务中，LLMs 已有了显著进展，但在分析模型大小对微调后的 LLMs 作为 D2T 模型性能的影响方面仍存在显著差距。对 D2T 模型的评估通常围绕三个基本性能质量展开：可读性、信息性和忠实度（参见图 [2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 模型大小对微调后的 LLM 在数据到文本生成中的性能影响：最新研究")）。可读性（Chen et al., [2020](#bib.bib14);
    Li et al., [2022](#bib.bib54)）主要关注模型生成文本的流畅性和连贯性，而信息性（Shen et al., [2019](#bib.bib96);
    Li et al., [2022](#bib.bib54)）决定了 D2T 模型在将给定数据中的关键信息转化为生成文本方面的有效性。忠实度（Tian et
    al., [2019](#bib.bib102); Wang et al., [2020](#bib.bib109)）是 D2T 的一个重要性能指标，评估生成的文本是否呈现了与给定源数据不一致或无关的事实。考虑到
    LLMs 在各种关键 D2T 领域中的巨大潜力（Hommes et al., [2019](#bib.bib36); Pauws et al., [2019](#bib.bib79)），了解模型大小对微调后的
    LLM 在 D2T 性能方面的影响至关重要。然而，几乎没有现有文献研究过模型大小对微调后的 LLM 在 D2T 三个性能质量方面的影响。
- en: '![Refer to caption](img/b4fe95f2853d0693e9c4e736312e943c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b4fe95f2853d0693e9c4e736312e943c.png)'
- en: 'Figure 2: Three key qualities to assess the performance of a D2T model are:
    readability (focusing on fluency and coherence), informativeness (evaluating the
    ability to generate essential content), and faithfulness (indicating the consistency
    of the generated text by measuring the presence of irrelevant facts).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：评估 D2T 模型性能的三个关键质量是：可读性（关注流畅性和连贯性）、信息性（评估生成基本内容的能力）和忠实度（通过测量无关事实的存在来指示生成文本的一致性）。
- en: 'This paper aims to address the research gap by providing a comparative analysis
    to demonstrate the impact of model size (number of parameters) on the performance
    of fine-tuned LLMs for D2T tasks. We conduct the comprehensive performance comparison
    of twelve LLMs with varying sizes, drawn from five widely used LLM families: BART (Lewis
    et al., [2020](#bib.bib51)), T5 (Raffel et al., [2020](#bib.bib88)), OPT (Zhang
    et al., [2022](#bib.bib121)), BLOOM (Scao et al., [2022](#bib.bib92)), and Llama
    2 (Touvron et al., [2023](#bib.bib103)). We measure the performances of LLMs on
    D2T tasks across three key qualities of D2T model—readability, informativeness,
    and faithfulness. To cover a broad spectrum, we include three primary types of
    D2T tasks—table-to-text, graph-to-text, and MR-to-text. All of these D2T are depicted
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation").
    We incorporate five state-of-the-art datasets in our experiments: E2E (Novikova
    et al., [2017](#bib.bib75)) (MR-to-text), ViGGo (Juraska et al., [2019](#bib.bib43))
    (MR-to-text), WikiTableText (Bao et al., [2018](#bib.bib5)) (table-to-text), DART (Nan
    et al., [2021](#bib.bib70)) (graph-to-text), and WebNLG (Gardent et al., [2017](#bib.bib26))
    (graph-to-text). Through comparative analyses, we aim to investigate both the
    advantages and limitations of scaling up the parameters in LLMs (or size of LLMs)
    within the domain of D2T. Additionally, we provide insights into the performance
    of LLMs in terms of their size with considering the factual/information divergence
    between the source and reference data—known as source-reference divergence. Through
    considering source-reference divergence, we aim to assess LLMs’ generalization
    capability in D2T tasks and evaluate if increasing LLM size contributes to quality
    generation when references diverge from the source data in D2T.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '本文旨在通过提供比较分析来填补研究空白，以展示模型规模（参数数量）对微调LLMs在D2T任务中的性能的影响。我们对来自五个广泛使用的LLM家族的十二个不同规模的LLMs进行了全面的性能比较：BART （Lewis
    et al., [2020](#bib.bib51)），T5 （Raffel et al., [2020](#bib.bib88)），OPT （Zhang
    et al., [2022](#bib.bib121)），BLOOM （Scao et al., [2022](#bib.bib92)）和Llama 2 （Touvron
    et al., [2023](#bib.bib103)）。我们测量了LLMs在D2T任务中在三个关键质量——可读性、信息量和忠实度上的表现。为了覆盖广泛的范围，我们包括了三种主要的D2T任务——表格到文本、图表到文本和MR到文本。这些D2T任务在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")中有所展示。我们在实验中结合了五个最先进的数据集：E2E （Novikova
    et al., [2017](#bib.bib75)）（MR到文本），ViGGo （Juraska et al., [2019](#bib.bib43)）（MR到文本），WikiTableText （Bao
    et al., [2018](#bib.bib5)）（表格到文本），DART （Nan et al., [2021](#bib.bib70)）（图表到文本）和WebNLG （Gardent
    et al., [2017](#bib.bib26)）（图表到文本）。通过比较分析，我们旨在调查在D2T领域中扩大LLMs参数（或LLMs规模）的优势和局限性。此外，我们提供了关于LLMs在其规模方面的性能见解，考虑了源数据和参考数据之间的事实/信息差异——即源-参考差异。通过考虑源-参考差异，我们旨在评估LLMs在D2T任务中的泛化能力，并评估当参考数据与源数据在D2T中有所偏离时，增加LLM规模是否有助于生成质量。'
- en: 'The organization of the remainder of the paper is as follows. Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation") outlines
    the research questions and motivations behind this paper. Prevalent related work
    concerning D2T and application of LLMs is presented in Section [3](#S3 "3 Related
    Work ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation"). Section [4](#S4 "4 Preliminaries ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") offers a concise overview of conditional text generation, (large)
    language models, D2T, and source-reference divergence. All details about the models,
    datasets, and experimental settings are provided in Section [5](#S5 "5 Models,
    Datasets and Experimental Settings ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"). In Section [6](#S6
    "6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), we
    present a comparative performance evaluation of LLMs with different model sizes
    for D2T tasks. Section [7](#S7 "7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") analyzes the impact of model size on LLM performance
    in the presence of source-reference divergence in D2T tasks. Section [8](#S8 "8
    Case Studies ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") includes case studies to illustrate
    the outcomes of LLMs more clearly. Finally, we conclude our findings in Section [9](#S9
    "9 Conclusion ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '本文其余部分的组织结构如下。第[2节](#S2 "2 Research Questions and Motivations ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")概述了本文的研究问题和动机。关于D2T和LLMs应用的相关工作在第[3节](#S3 "3 Related Work ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")中进行了介绍。第[4节](#S4 "4 Preliminaries ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")提供了条件文本生成、（大规模）语言模型、D2T以及源引用偏差的简要概述。有关模型、数据集和实验设置的所有细节都在第[5节](#S5
    "5 Models, Datasets and Experimental Settings ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")中提供。在第[6节](#S6
    "6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")中，我们展示了不同模型大小的LLMs在D2T任务中的比较性能评估。第[7节](#S7
    "7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")分析了模型大小在D2T任务中存在源引用偏差时对LLM性能的影响。第[8节](#S8
    "8 Case Studies ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")包括案例研究，以更清晰地说明LLMs的结果。最后，我们在第[9节](#S9
    "9 Conclusion ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")中总结了我们的发现。'
- en: 2 Research Questions and Motivations
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 研究问题和动机
- en: This paper aims to empirically address the following research questions regarding
    the performance of fine-tuned LLMs for D2T, focusing impact of model sizes (i.e.,
    parameter counts) across the three key qualities of readability, informativeness,
    and faithfulness.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在实证性地解决以下关于微调LLMs在D2T任务中表现的研究问题，重点关注模型大小（即参数数量）对可读性、信息性和忠实性这三项关键特质的影响。
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: What are the impacts of model size within a family of fine-tuned LLMs on the
    performance of data-to-text (D2T) tasks, in terms of the readability, informativeness,
    and faithfulness?
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型大小在同一系列的微调LLMs中对数据到文本（D2T）任务的可读性、信息性和忠实性的影响是什么？
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Do larger LLM families (such as OPT, BLOOM, Llama 2, etc.) convincingly outperform
    smaller LLM families (such as BART, T5, etc.) in terms of D2T task performance?
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较大的LLM家族（如OPT、BLOOM、Llama 2等）在D2T任务的表现上是否明显优于较小的LLM家族（如BART、T5等）？
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Does the presence of source-reference divergence influence the performance of
    LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 源引用偏差的存在是否会影响LLMs在D2T任务中的表现？如果会，增加LLM的模型大小是否有助于减轻源引用偏差的影响？
- en: The motivation behind our objectives stems from several key considerations.
    Firstly, the recent trend of increasing number of parameters in LLMs comes with
    significant computational costs and longer inference times (Zheng et al., [2023](#bib.bib125);
    Li et al., [2024a](#bib.bib52); Jiang et al., [2024](#bib.bib40)). If this escalation
    in model size does not yield substantial improvements (Aghajanyan et al., [2021](#bib.bib1);
    Tulchinskii et al., [2023](#bib.bib104)) in D2T, it raises questions about the
    necessity of bearing such computational burdens (Luccioni et al., [2023](#bib.bib64);
    Faiz et al., [2023](#bib.bib22)). Secondly, despite the widespread use of LLMs,
    their performance in D2T tasks has not been comprehensively explored. This research
    gap hinders the effective application of LLMs in D2T scenarios. Lastly, LLMs are
    known for their generalization capabilities (Ge et al., [2023](#bib.bib28); Zhao
    et al., [2024](#bib.bib123); Yang et al., [2024](#bib.bib115)), as evidenced by
    their success in various generative text tasks. Therefore, it is pertinent to
    investigate whether this generalization extends to enhancing LLM performance for
    D2T tasks in the presence of source-reference divergence.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目标背后的动机源于几个关键考虑因素。首先，最近LLMs参数数量的增加带来了显著的计算成本和更长的推理时间（Zheng et al., [2023](#bib.bib125);
    Li et al., [2024a](#bib.bib52); Jiang et al., [2024](#bib.bib40)）。如果模型规模的扩大没有在D2T上带来实质性的改进（Aghajanyan
    et al., [2021](#bib.bib1); Tulchinskii et al., [2023](#bib.bib104)），这就引发了关于承受这种计算负担的必要性的疑问（Luccioni
    et al., [2023](#bib.bib64); Faiz et al., [2023](#bib.bib22)）。其次，尽管LLMs被广泛使用，但它们在D2T任务中的表现尚未得到全面探索。这一研究空白阻碍了LLMs在D2T场景中的有效应用。最后，LLMs以其泛化能力著称（Ge
    et al., [2023](#bib.bib28); Zhao et al., [2024](#bib.bib123); Yang et al., [2024](#bib.bib115)），这在各种生成文本任务中的成功证明了这一点。因此，有必要探讨这种泛化是否延伸到提高LLM在D2T任务中表现的能力，特别是在源参考分歧的情况下。
- en: 3 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: 'Traditionally, D2T models have relied on rule-based methodologies (Reiter and
    Dale, [1997](#bib.bib90)), meticulously crafted by domain experts. However, these
    rule-based approaches have been hindered by two primary challenges: scalability
    and the scarcity of domain experts (Gatt and Krahmer, [2018](#bib.bib27); Angeli
    et al., [2010](#bib.bib2)). As the demand for D2T models continues to rise across
    various applications (Lu and Ng, [2011](#bib.bib63); Gatt and Krahmer, [2018](#bib.bib27)),
    there has been a notable shift towards the development of automatic D2T models.
    Statistical n-gram based language models (Brown et al., [1992](#bib.bib10); Bickel
    et al., [2005](#bib.bib9); Pauls and Klein, [2011](#bib.bib78)) laid the foundation
    for probabilistic D2T models (Chen and Mooney, [2008](#bib.bib13); Angeli et al.,
    [2010](#bib.bib2); Lu and Ng, [2011](#bib.bib63)). While these initial probabilistic
    models have gained traction due to their scalability, they still struggle to emulate
    human-like text generation for D2T tasks (Belz and Reiter, [2006](#bib.bib7);
    Reiter, [2018](#bib.bib89)). The emergence of recurrent neural network-based (RNN)
    language models (Mikolov et al., [2010](#bib.bib69)) marked a significant milestone
    in the realm of D2T tasks (Lebret et al., [2016](#bib.bib49)), enabling D2T models
    to exhibit extreme fluency. With the introduction of encoder-decoder framework-based
    sequence-to-sequence neural networks (Sutskever et al., [2014](#bib.bib99)), D2T
    generation experienced a surge in popularity compared to existing methods (Graves
    et al., [2013](#bib.bib31)). These models (Wen et al., [2015](#bib.bib110); Shang
    et al., [2015](#bib.bib95)) excel in readability, representing a significant advancement
    not only in D2T but also in several text generation tasks. Nevertheless, these
    RNN-based encoder-decoder D2T models still struggle to ensure the faithfulness
    and informativeness of generated text (Wiseman et al., [2017](#bib.bib111)). With
    the integration of attention mechanisms (Bahdanau et al., [2015](#bib.bib3); Luong
    et al., [2015](#bib.bib66); Xu et al., [2015](#bib.bib114)) into deep encoder-decoder
    networks and the introduction of pre-training/fine-tuning mechanisms (Howard and
    Ruder, [2018](#bib.bib37)), numerous D2T models (Mei et al., [2016](#bib.bib68);
    Liu et al., [2018](#bib.bib57); Sha et al., [2018](#bib.bib94); Nema et al., [2018](#bib.bib71);
    Budzianowski et al., [2018](#bib.bib12); Juraska et al., [2018](#bib.bib42); Nie
    et al., [2019a](#bib.bib72); Puduppully et al., [2019](#bib.bib86); Gong et al.,
    [2019](#bib.bib30)) have emerged. The advent of the transformer models (Vaswani
    et al., [2017](#bib.bib107)) has led to their widespread adoption across various
    NLP tasks, including D2T generation. Almost all recent language models, built
    upon the transformer architecture, play pivotal roles in text generation tasks,
    including D2T (Erdem et al., [2022](#bib.bib21); Ji et al., [2023](#bib.bib39);
    Lin et al., [2024](#bib.bib56)). These transformer-based pre-trained language
    models serve as foundational models for D2T tasks, benefiting from pre-training
    on large text datasets using self-supervised learning (Taylor, [1953](#bib.bib101);
    Bickel et al., [2005](#bib.bib9)). Through fine-tuning strategies, these language
    models have gained immense popularity in D2T tasks (Devlin et al., [2019](#bib.bib17);
    Lewis et al., [2020](#bib.bib51); Radford et al., [2019](#bib.bib87); Brown et al.,
    [2020](#bib.bib11)). D2T models built upon these language models often achieve
    higher rankings compared to earlier D2T models (Ge et al., [2023](#bib.bib28);
    Zhang et al., [2024](#bib.bib119)). Utilizing language models in D2T tasks also
    streamlines the training process, as they require only a small amount of task-specific
    data for fine-tuning (Erdem et al., [2022](#bib.bib21)). While language model-led
    D2T systems show improvements in informativeness, they still struggle to generate
    faithful content.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，D2T 模型依赖于由领域专家精心制定的基于规则的方法论（Reiter 和 Dale，[1997](#bib.bib90)）。然而，这些基于规则的方法面临着两个主要挑战：可扩展性和领域专家的稀缺（Gatt
    和 Krahmer，[2018](#bib.bib27)；Angeli 等，[2010](#bib.bib2)）。随着对 D2T 模型需求的不断上升（Lu
    和 Ng，[2011](#bib.bib63)；Gatt 和 Krahmer，[2018](#bib.bib27)），自动化 D2T 模型的开发已经成为一种显著趋势。基于统计的
    n-gram 语言模型（Brown 等，[1992](#bib.bib10)；Bickel 等，[2005](#bib.bib9)；Pauls 和 Klein，[2011](#bib.bib78)）为概率性
    D2T 模型（Chen 和 Mooney，[2008](#bib.bib13)；Angeli 等，[2010](#bib.bib2)；Lu 和 Ng，[2011](#bib.bib63)）奠定了基础。虽然这些初期的概率模型由于其可扩展性而获得了一定的认可，但它们仍然难以模拟人类的文本生成能力（Belz
    和 Reiter，[2006](#bib.bib7)；Reiter，[2018](#bib.bib89)）。基于递归神经网络（RNN）的语言模型（Mikolov
    等，[2010](#bib.bib69)）在 D2T 任务领域标志着一个重要的里程碑（Lebret 等，[2016](#bib.bib49)），使得 D2T
    模型展现出极高的流畅度。随着编码器-解码器框架的序列到序列神经网络（Sutskever 等，[2014](#bib.bib99)）的引入，与现有方法相比，D2T
    生成的流行度急剧上升（Graves 等，[2013](#bib.bib31)）。这些模型（Wen 等，[2015](#bib.bib110)；Shang 等，[2015](#bib.bib95)）在可读性方面表现出色，代表了
    D2T 以及多个文本生成任务的显著进步。然而，这些基于 RNN 的编码器-解码器 D2T 模型在确保生成文本的忠实性和信息性方面仍面临挑战（Wiseman
    等，[2017](#bib.bib111)）。随着注意力机制（Bahdanau 等，[2015](#bib.bib3)；Luong 等，[2015](#bib.bib66)；Xu
    等，[2015](#bib.bib114)）的整合到深度编码器-解码器网络中，以及预训练/微调机制（Howard 和 Ruder，[2018](#bib.bib37)）的引入，众多
    D2T 模型（Mei 等，[2016](#bib.bib68)；Liu 等，[2018](#bib.bib57)；Sha 等，[2018](#bib.bib94)；Nema
    等，[2018](#bib.bib71)；Budzianowski 等，[2018](#bib.bib12)；Juraska 等，[2018](#bib.bib42)；Nie
    等，[2019a](#bib.bib72)；Puduppully 等，[2019](#bib.bib86)；Gong 等，[2019](#bib.bib30)）应运而生。变换器模型的出现（Vaswani
    等，[2017](#bib.bib107)）导致它们在各种 NLP 任务中，包括 D2T 生成，得到了广泛应用。几乎所有近期的语言模型，基于变换器架构，都在文本生成任务中发挥了关键作用，包括
    D2T（Erdem 等，[2022](#bib.bib21)；Ji 等，[2023](#bib.bib39)；Lin 等，[2024](#bib.bib56)）。这些基于变换器的预训练语言模型作为
    D2T 任务的基础模型，得益于在大型文本数据集上使用自监督学习的预训练（Taylor，[1953](#bib.bib101)；Bickel 等，[2005](#bib.bib9)）。通过微调策略，这些语言模型在
    D2T 任务中获得了极大的普及（Devlin 等，[2019](#bib.bib17)；Lewis 等，[2020](#bib.bib51)；Radford
    等，[2019](#bib.bib87)；Brown 等，[2020](#bib.bib11)）。基于这些语言模型的 D2T 模型往往在排名上超过了早期的
    D2T 模型（Ge 等，[2023](#bib.bib28)；Zhang 等，[2024](#bib.bib119)）。在 D2T 任务中利用语言模型也简化了训练过程，因为它们仅需要少量的任务特定数据进行微调（Erdem
    等，[2022](#bib.bib21)）。尽管基于语言模型的 D2T 系统在信息性方面有所改善，但它们仍然在生成忠实内容方面面临困难。
- en: Several recent studies (Petroni et al., [2019](#bib.bib83); Heinzerling and
    Inui, [2021](#bib.bib32); Roberts et al., [2020](#bib.bib91)) on language models,
    suggest that the parameters within these models are responsible for encoding knowledge.
    To enhance the knowledge capacity of these models, recent efforts have significantly
    increased the number of parameters (Raffel et al., [2020](#bib.bib88); Scao et al.,
    [2022](#bib.bib92); Touvron et al., [2023](#bib.bib103)). These large, parameter-rich
    language models (i.e., LLMs) demonstrate remarkable performance across various
    generative tasks, such as text generation, and discriminative tasks, like text
    classification (Devlin et al., [2019](#bib.bib17); Lewis et al., [2020](#bib.bib51);
    Radford et al., [2019](#bib.bib87)). Even in few-shot and zero-shot training setups (Brown
    et al., [2020](#bib.bib11)), these LLMs consistently excel in numerous NLP tasks.
    The trend of augmenting parameters within LLMs has significantly shaped the landscape
    of text generation, with LLMs significantly outperforming earlier deep learning-based
    models. This phenomenon has also impacted D2T tasks, as the majority of recent
    D2T models utilize LLMs (Kasner and Dusek, [2024](#bib.bib45); Li et al., [2024b](#bib.bib53);
    Jing et al., [2024](#bib.bib41); Lorandi and Belz, [2024](#bib.bib60)). Recent
    advancements in parameter-efficient fine-tuning approaches for LLMs, such as prefix-tuning (Li
    and Liang, [2021](#bib.bib55)), P-tuning (Liu et al., [2022](#bib.bib58)), and
    prompt tuning (Lester et al., [2021](#bib.bib50)), along with popular adapter-based
    fine-tuning methods like LoRA (Hu et al., [2022](#bib.bib38)), have made LLMs
    more suitable for D2T tasks. Given that LLMs are pre-trained on vast text corpora (Raffel
    et al., [2020](#bib.bib88); Touvron et al., [2023](#bib.bib103)), they excel in
    capturing rich contextual information, making them well-suited for generating
    text in D2T scenarios.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几项研究（Petroni 等，[2019](#bib.bib83)；Heinzerling 和 Inui，[2021](#bib.bib32)；Roberts
    等，[2020](#bib.bib91)）表明，语言模型中的参数负责编码知识。为了增强这些模型的知识容量，最近的努力显著增加了参数数量（Raffel 等，[2020](#bib.bib88)；Scao
    等，[2022](#bib.bib92)；Touvron 等，[2023](#bib.bib103)）。这些大型、参数丰富的语言模型（即 LLMs）在各种生成任务（如文本生成）和判别任务（如文本分类）（Devlin
    等，[2019](#bib.bib17)；Lewis 等，[2020](#bib.bib51)；Radford 等，[2019](#bib.bib87)）中表现出色。即使在少量样本和零样本训练设置下（Brown
    等，[2020](#bib.bib11)），这些 LLMs 在众多 NLP 任务中始终表现优异。增加 LLM 参数的趋势显著塑造了文本生成的格局，LLMs
    的表现显著优于早期的深度学习模型。这一现象也影响了 D2T 任务，因为大多数近期 D2T 模型利用 LLMs（Kasner 和 Dusek，[2024](#bib.bib45)；Li
    等，[2024b](#bib.bib53)；Jing 等，[2024](#bib.bib41)；Lorandi 和 Belz，[2024](#bib.bib60)）。LLMs
    的参数高效微调方法的最新进展，如前缀调优（Li 和 Liang，[2021](#bib.bib55)）、P-tuning（Liu 等，[2022](#bib.bib58)）和提示调优（Lester
    等，[2021](#bib.bib50)），以及流行的基于适配器的微调方法如 LoRA（Hu 等，[2022](#bib.bib38)），使得 LLMs 更加适合
    D2T 任务。鉴于 LLMs 在广泛的文本语料库上进行预训练（Raffel 等，[2020](#bib.bib88)；Touvron 等，[2023](#bib.bib103)），它们在捕捉丰富的上下文信息方面表现出色，使其非常适合在
    D2T 场景中生成文本。
- en: Despite the widespread application of LLMs in D2T, several important questions
    persist regarding their performance, particularly considering their sizes (numbers
    of parameters) (Luccioni et al., [2023](#bib.bib64); Jiang et al., [2024](#bib.bib40)).
    Achieving human-like readability in generated text remains a challenge for LLMs
    in D2T (Jing et al., [2024](#bib.bib41)). Regarding informativeness, a crucial
    quality of D2T models, LLMs may overlook essential information when generating
    from non-textual data due to underlying biases stemming from their large number
    of parameters (Wu and Aji, [2023](#bib.bib113)). Recent studies highlight that
    even with an increase in parameters, the effective or extrinsic parameters of
    LLMs remain low (Aghajanyan et al., [2021](#bib.bib1); Tulchinskii et al., [2023](#bib.bib104)).
    Concerns also persist regarding the faithfulness of LLMs in D2T (Ji et al., [2023](#bib.bib39)).
    Recent investigations have sought to highlight the limitations of LLMs as D2T
    models. However, these studies have primarily consisted of surveys discussing
    the progress of D2T tasks (Lin et al., [2024](#bib.bib56)) or outlining D2T models
    focusing on single key attributes (Gatt and Krahmer, [2018](#bib.bib27); Erdem
    et al., [2022](#bib.bib21); Ji et al., [2023](#bib.bib39)). None of these studies
    offers a comprehensive analysis of D2T models considering all three essential
    qualities (readability, informativeness, and faithfulness), particularly in relation
    to LLMs. Our study aims to address this research gap.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在数据到文本（D2T）的广泛应用中表现出色，但仍存在一些重要问题，特别是考虑到它们的规模（参数数量）（Luccioni 等，[2023](#bib.bib64);
    Jiang 等，[2024](#bib.bib40)）。在生成文本中实现类似人类的可读性仍然是 D2T 中 LLMs 面临的挑战（Jing 等，[2024](#bib.bib41)）。关于信息量，这是
    D2T 模型的一个关键特性，LLMs 在从非文本数据生成时可能会忽略关键信息，因为它们大量的参数所带来的潜在偏差（Wu 和 Aji，[2023](#bib.bib113)）。最近的研究表明，即使参数增加，LLMs
    的有效或外部参数仍然较低（Aghajanyan 等，[2021](#bib.bib1); Tulchinskii 等，[2023](#bib.bib104)）。关于
    LLMs 在 D2T 中的忠实度仍然存在担忧（Ji 等，[2023](#bib.bib39)）。近期的调查旨在揭示 LLMs 作为 D2T 模型的局限性。然而，这些研究主要包括讨论
    D2T 任务进展的综述（Lin 等，[2024](#bib.bib56)）或概述 D2T 模型，集中于单一关键属性（Gatt 和 Krahmer，[2018](#bib.bib27);
    Erdem 等，[2022](#bib.bib21); Ji 等，[2023](#bib.bib39)）。这些研究中没有提供一个综合分析 D2T 模型的所有三项基本特性（可读性、信息量和忠实度），尤其是与
    LLMs 相关的综合分析。我们的研究旨在填补这一研究空白。
- en: 4 Preliminaries
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 前言
- en: 4.1 Conditional Text Generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 条件文本生成
- en: 'Text generation or natural language generation (Lu et al., [2018](#bib.bib62);
    Erdem et al., [2022](#bib.bib21)), both fundamental and crucial task in NLP, encompasses
    various applications such as machine translation (Gatt and Krahmer, [2018](#bib.bib27)),
    D2T (Ye et al., [2020](#bib.bib116)), dialogue generation (Wen et al., [2015](#bib.bib110)),
    image captioning (Karpathy and Fei-Fei, [2017](#bib.bib44)), summarization (See
    et al., [2017](#bib.bib93); ul Islam et al., [2023](#bib.bib105)), story generation (Fan
    et al., [2018](#bib.bib23)), report writing (Lu and Ng, [2011](#bib.bib63); Lin
    et al., [2024](#bib.bib56)), etc. These text generation tasks can be broadly categorized
    into two main types: open-ended text generation and conditional (or controlled)
    text generation (Holtzman et al., [2018](#bib.bib34); Li et al., [2022](#bib.bib54)).
    In open-ended text generation, models produce text without any constraint from
    prior source information. For illustration, generating story narratives after
    a given context is an open-ended generation (Fan et al., [2018](#bib.bib23)).
    Completing text from a text fragment is also an example of open-ended generation.
    On the other hand, in conditional text generation, the model generates text based
    on a constraint given through prior source information and instructions. Examples
    of conditional text generation tasks include automatic summarization, machine
    translation, and task-oriented dialogue generation, all of which rely on specific
    input (source) conditions for text generation. D2T falls within conditional text
    generation (Puduppully et al., [2019](#bib.bib86); Upadhyay and Massie, [2023](#bib.bib106)),
    as it involves generating text from structured data inputs, further emphasizing
    its significance in this category. Conditional text generation tasks are always
    represented through conditional probability distributions (Graves et al., [2013](#bib.bib31);
    Gatt and Krahmer, [2018](#bib.bib27); Erdem et al., [2022](#bib.bib21)). To generate
    a text, $w=w_{1},w_{2},w_{3},\dots,w_{n}$ means empty word) from a source, $s$,
    we can be explicitly written as follow:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成或自然语言生成 (Lu et al., [2018](#bib.bib62); Erdem et al., [2022](#bib.bib21))，这是
    NLP 中的一项基础且关键任务，涵盖了各种应用，如机器翻译 (Gatt and Krahmer, [2018](#bib.bib27))、D2T (Ye et
    al., [2020](#bib.bib116))、对话生成 (Wen et al., [2015](#bib.bib110))、图像描述 (Karpathy
    and Fei-Fei, [2017](#bib.bib44))、摘要生成 (See et al., [2017](#bib.bib93); ul Islam
    et al., [2023](#bib.bib105))、故事生成 (Fan et al., [2018](#bib.bib23))、报告写作 (Lu and
    Ng, [2011](#bib.bib63); Lin et al., [2024](#bib.bib56)) 等。这些文本生成任务可以大致分为两种主要类型：开放式文本生成和条件（或受控）文本生成 (Holtzman
    et al., [2018](#bib.bib34); Li et al., [2022](#bib.bib54))。在开放式文本生成中，模型生成的文本没有任何来自先前源信息的约束。例如，在给定上下文后生成故事叙述是开放式生成的一个例子 (Fan
    et al., [2018](#bib.bib23))。从文本片段完成文本也是开放式生成的一个例子。另一方面，在条件文本生成中，模型根据先前源信息和指令所提供的约束生成文本。条件文本生成任务的例子包括自动摘要、机器翻译和任务导向对话生成，这些都依赖于特定的输入（源）条件来生成文本。D2T
    属于条件文本生成 (Puduppully et al., [2019](#bib.bib86); Upadhyay and Massie, [2023](#bib.bib106))，因为它涉及从结构化数据输入生成文本，进一步强调了它在这一类别中的重要性。条件文本生成任务总是通过条件概率分布来表示 (Graves
    et al., [2013](#bib.bib31); Gatt and Krahmer, [2018](#bib.bib27); Erdem et al.,
    [2022](#bib.bib21))。要从源文本 $s$ 生成文本 $w=w_{1},w_{2},w_{3},\dots,w_{n}$（即空词），可以明确写作如下：
- en: '|  | $\displaystyle w_{i}\sim\left.Pr\left(\cdot\Big{&#124;}{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{s}_{\text{Given
    source}}},{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{w_{1},w_{2},w_{3},\dots,w_{i-1}}_{\text{Previous
    generated context}}}\right)\right&#124;_{i=1}^{n-1}$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{i}\sim\left.Pr\left(\cdot\Big{&#124;}{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{s}_{\text{给定源}}},{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\underbrace{w_{1},w_{2},w_{3},\dots,w_{i-1}}_{\text{先前生成的上下文}}}\right)\right&#124;_{i=1}^{n-1}$
    |  |'
- en: '![Refer to caption](img/7e5733d139a0a18ca687dd8a4a0b3219.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7e5733d139a0a18ca687dd8a4a0b3219.png)'
- en: 'Figure 3: Two prevalent types of language models based on transformer (Vaswani
    et al., [2017](#bib.bib107)) architecture are depicted here: bidirectional and
    unidirectional language models. The red lines represent attention mechanisms.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：这里描绘了基于 transformer (Vaswani et al., [2017](#bib.bib107)) 架构的两种常见语言模型：双向和单向语言模型。红色线条代表注意力机制。
- en: 4.2 (Large) Language Model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2（大型）语言模型
- en: 'Most of the recent advancement of generative text generation tasks are direct
    outcomes of predictive language modelling (Pauls and Klein, [2011](#bib.bib78);
    Mikolov et al., [2010](#bib.bib69); Devlin et al., [2019](#bib.bib17); Radford
    et al., [2019](#bib.bib87)). With the advent of transformers (Vaswani et al.,
    [2017](#bib.bib107)) and pre-training paradigm (Howard and Ruder, [2018](#bib.bib37)),
    language models have demonstrated unparalleled dominance across various NLG tasks.
    Primarily there are mainly two type of language modelling (Figure [3](#S4.F3 "Figure
    3 ‣ 4.1 Conditional Text Generation ‣ 4 Preliminaries ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"))—
    bidirectional and uni-directional language modelling (Lu et al., [2018](#bib.bib62)).
    In bidirectional language model, both left and right contexts are considered in
    predicting a context. Masked language modelling (Taylor, [1953](#bib.bib101)),
    where the objective is to predict mask word in a text, paved the path for bidirectional
    language model. With the appearance of BERT (Devlin et al., [2019](#bib.bib17)),
    bi-directional language model show performance enhancement both in terms of semantic
    and contextual informativeness in several NLP task. Objective of language model
    is to predict masked words ($w_{i}$), as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的生成文本生成任务的进展大多直接来源于预测语言建模（Pauls 和 Klein, [2011](#bib.bib78)；Mikolov 等, [2010](#bib.bib69)；Devlin
    等, [2019](#bib.bib17)；Radford 等, [2019](#bib.bib87)）。随着 transformers（Vaswani 等,
    [2017](#bib.bib107)）和预训练范式（Howard 和 Ruder, [2018](#bib.bib37)）的出现，语言模型在各种 NLG
    任务中展示了无与伦比的优势。主要有两种语言建模（图 [3](#S4.F3 "Figure 3 ‣ 4.1 Conditional Text Generation
    ‣ 4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation)）— 双向和单向语言建模（Lu 等, [2018](#bib.bib62)）。在双向语言模型中，预测上下文时同时考虑左右两侧的上下文。掩码语言建模（Taylor,
    [1953](#bib.bib101)），其目标是预测文本中的掩码词，为双向语言模型铺平了道路。随着 BERT（Devlin 等, [2019](#bib.bib17)）的出现，双向语言模型在多个
    NLP 任务中显示了语义和上下文信息性的性能提升。语言模型的目标是预测掩码词（$w_{i}$），如下所示：'
- en: '|  | $\displaystyle w_{i}\leftarrow\underset{w}{\text{argmax}}\ p(w&#124;w_{1}\dots
    w_{i-1}w_{\text{mask}}w_{i+1}\dots w_{n})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{i}\leftarrow\underset{w}{\text{argmax}}\ p(w\mid w_{1}\dots
    w_{i-1}w_{\text{mask}}w_{i+1}\dots w_{n})$ |  |'
- en: On the other hand, unidirectional language models predicts target words from
    one direction, mostly left-to-right directional are followed (Radford et al.,
    [2019](#bib.bib87); Brown et al., [2020](#bib.bib11); Raffel et al., [2020](#bib.bib88)).
    Earlier, unidirectional language model seems to have less powerful compare to
    the bi-directional language model. However several recent studies (Li and Liang,
    [2021](#bib.bib55); Liu et al., [2022](#bib.bib58)) have shown that unidirectional
    language models can also be powerful as bi-direction language model in terms of
    language understanding. Currently most of the popular language models are uni-directional
    language models. Objective of unidirectional language model is to predict next
    words ($w_{n}$),
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，单向语言模型从一个方向预测目标词，通常遵循从左到右的方向（Radford 等, [2019](#bib.bib87)；Brown 等, [2020](#bib.bib11)；Raffel
    等, [2020](#bib.bib88)）。早期，单向语言模型似乎相比双向语言模型力量较弱。然而，最近的几项研究（Li 和 Liang, [2021](#bib.bib55)；Liu
    等, [2022](#bib.bib58)）表明，单向语言模型在语言理解方面也可以与双向语言模型一样强大。目前，大多数流行的语言模型都是单向语言模型。单向语言模型的目标是预测下一个词（$w_{n}$），
- en: '|  | $\displaystyle w_{n}\leftarrow\underset{w}{\text{argmax}}\ p(w&#124;w_{1}w_{2}\dots
    w_{n-1})$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{n}\leftarrow\underset{w}{\text{argmax}}\ p(w\mid w_{1}w_{2}\dots
    w_{n-1})$ |  |'
- en: '![Refer to caption](img/97c062a360ab22e56480f1bc38907800.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/97c062a360ab22e56480f1bc38907800.png)'
- en: 'Figure 4: Two of the most popular architectures used for implementing language
    models: the encoder-decoder architecture (left) and the decoder-only architecture
    (right). The sequence $e_{1}e_{2}\dots e_{m}$.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：用于实现语言模型的两种最流行架构：编码器-解码器架构（左）和仅解码器架构（右）。序列 $e_{1}e_{2}\dots e_{m}$。
- en: 'Two primary architectures are commonly used for implementing language models:
    encoder-decoder architecture (Sutskever et al., [2014](#bib.bib99); Vinyals et al.,
    [2016](#bib.bib108)) and decoder-only architecture (Radford et al., [2019](#bib.bib87);
    Brown et al., [2020](#bib.bib11)) (Figure [4](#S4.F4 "Figure 4 ‣ 4.2 (Large) Language
    Model ‣ 4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation")). In the encoder-decoder
    architecture, the encoder part encodes an input sequence, and the decoder generates
    a new sequence from the encoded representation. On the other hand, in the decoder-only
    architecture, the decoder auto-regressively completes a sequence from a given
    context.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '实现语言模型时，通常使用两种主要架构：编码器-解码器架构（Sutskever et al., [2014](#bib.bib99); Vinyals
    et al., [2016](#bib.bib108)）和仅解码器架构（Radford et al., [2019](#bib.bib87); Brown
    et al., [2020](#bib.bib11)）（图 [4](#S4.F4 "Figure 4 ‣ 4.2 (Large) Language Model
    ‣ 4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")）。在编码器-解码器架构中，编码器部分对输入序列进行编码，解码器则从编码的表示生成新的序列。另一方面，在仅解码器架构中，解码器根据给定的上下文自回归地完成序列。'
- en: 'In contemporary times, large language models (LLMs) have become omnipresent
    in nearly all NLP tasks, owing to their remarkable performance and versatility (Devlin
    et al., [2019](#bib.bib17); Brown et al., [2020](#bib.bib11); Touvron et al.,
    [2023](#bib.bib103)). LLMs stand out from previous language models primarily due
    to two factors: their enormous size (Chowdhery et al., [2023](#bib.bib15)), characterized
    by a vast number of underlying parameters, and their extensive self-supervised
    pre-training on massive text corpora (Raffel et al., [2020](#bib.bib88); Penedo
    et al., [2023](#bib.bib80)). Compared to earlier language models (Graves et al.,
    [2013](#bib.bib31); Mikolov et al., [2010](#bib.bib69); Peters et al., [2018](#bib.bib82)),
    contemporary LLMs boast billions, and even trillions, of parameters, enabling
    them to achieve exceptional language understanding (Roberts et al., [2020](#bib.bib91);
    Aghajanyan et al., [2021](#bib.bib1)). Additionally, LLMs demonstrate impressive
    results through supervised fine-tuning, also known as instruction tuning (Si et al.,
    [2023](#bib.bib97); Zhang et al., [2023](#bib.bib120)), and training aided by
    reinforcement learning techniques (Fernandes et al., [2023](#bib.bib24); Ouyang
    et al., [2022](#bib.bib76)), incorporating human-like feedback through preference
    models (Touvron et al., [2023](#bib.bib103); Chowdhery et al., [2023](#bib.bib15)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在当代，大型语言模型（LLMs）因其卓越的性能和多功能性，已在几乎所有自然语言处理任务中无处不在（Devlin et al., [2019](#bib.bib17);
    Brown et al., [2020](#bib.bib11); Touvron et al., [2023](#bib.bib103)）。LLMs 与以往的语言模型的主要区别在于两个方面：其巨大的规模（Chowdhery
    et al., [2023](#bib.bib15)），特点是拥有大量的基础参数，以及在大规模文本语料上的广泛自监督预训练（Raffel et al., [2020](#bib.bib88);
    Penedo et al., [2023](#bib.bib80)）。与早期的语言模型（Graves et al., [2013](#bib.bib31);
    Mikolov et al., [2010](#bib.bib69); Peters et al., [2018](#bib.bib82)）相比，当代的LLMs拥有数十亿甚至数万亿的参数，使其能够实现卓越的语言理解（Roberts
    et al., [2020](#bib.bib91); Aghajanyan et al., [2021](#bib.bib1)）。此外，LLMs 通过监督微调，也称为指令调优（Si
    et al., [2023](#bib.bib97); Zhang et al., [2023](#bib.bib120)），以及通过强化学习技术（Fernandes
    et al., [2023](#bib.bib24); Ouyang et al., [2022](#bib.bib76)）进行训练，展示了令人印象深刻的结果，融入了通过偏好模型获得的人类反馈（Touvron
    et al., [2023](#bib.bib103); Chowdhery et al., [2023](#bib.bib15)）。
- en: 4.3 Data-to-Text (D2T) Generation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 数据到文本（D2T）生成
- en: 'The aim of Data-to-Text generation (D2T) (Lin et al., [2024](#bib.bib56)) is
    to convert non-textual, semi-structured source data, such as tables, graphs, or
    slot-value pairs (meaning representation, MR), into human-readable text output.
    D2T encompasses various types based on source representation, including graph-to-text,
    table-to-text, and MR-to-text (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")). In graph-to-text (Gardent et al., [2017](#bib.bib26);
    Nan et al., [2021](#bib.bib70)), structured graph data or knowledge triple sets
    are transformed into coherent narratives, while table-to-text (Liu et al., [2018](#bib.bib57);
    Bao et al., [2018](#bib.bib5); Gong et al., [2019](#bib.bib30)) involves converting
    tabular data into fluent textual descriptions. MR-to-text focuses on generating
    textual reports from slot-value pairs, also known as Meaning Representation (MR) (Novikova
    et al., [2017](#bib.bib75); Juraska et al., [2019](#bib.bib43); Sha et al., [2018](#bib.bib94);
    Kedzie and McKeown, [2020](#bib.bib47)). While D2T shares similarities with other
    text generation tasks like machine translation and summarization, its primary
    distinction lies in the use of semi-structured non-textual data as input (Ji et al.,
    [2023](#bib.bib39)). As the demand for automated text generation from semi-structured
    data grows, D2T remains a crucial research area with promising implications for
    enhancing data accessibility and decision-making processes (Gatt and Krahmer,
    [2018](#bib.bib27)). Numerous predictive models have been developed for D2T generation,
    with recent advancements largely attributed to the use of predictive language
    models (Pauls and Klein, [2011](#bib.bib78); Mikolov et al., [2010](#bib.bib69);
    Devlin et al., [2019](#bib.bib17); Radford et al., [2019](#bib.bib87)). These
    models, utilizing recurrent neural networks (RNNs) Hochreiter and Schmidhuber,
    [1997](#bib.bib33); Graves et al., [2013](#bib.bib31); Mikolov et al., [2010](#bib.bib69)
    and transformer architecture (Vaswani et al., [2017](#bib.bib107)), exhibit significant
    potential across various D2T tasks. Transformers have attracted significant attention
    and demonstrated remarkable success in D2T applications (Su et al., [2021](#bib.bib98);
    Wang et al., [2020](#bib.bib109)).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '数据到文本生成（D2T）(Lin et al., [2024](#bib.bib56)) 的目标是将非文本的半结构化源数据，如表格、图表或槽值对（意义表示，MR），转换为人类可读的文本输出。D2T
    根据源表示类型包括图到文本、表到文本和 MR 到文本（图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")）。在图到文本 (Gardent et al., [2017](#bib.bib26); Nan et al., [2021](#bib.bib70))
    中，结构化图数据或知识三元组被转化为连贯的叙述，而表到文本 (Liu et al., [2018](#bib.bib57); Bao et al., [2018](#bib.bib5);
    Gong et al., [2019](#bib.bib30)) 涉及将表格数据转换为流畅的文本描述。MR 到文本专注于从槽值对生成文本报告，也称为意义表示（MR）（Novikova
    et al., [2017](#bib.bib75); Juraska et al., [2019](#bib.bib43); Sha et al., [2018](#bib.bib94);
    Kedzie and McKeown, [2020](#bib.bib47)）。尽管 D2T 与机器翻译和摘要等其他文本生成任务有相似之处，其主要区别在于使用半结构化非文本数据作为输入（Ji
    et al., [2023](#bib.bib39)）。随着对从半结构化数据自动生成文本的需求增长，D2T 仍然是一个关键的研究领域，对提高数据可及性和决策过程有着有前景的影响（Gatt
    and Krahmer, [2018](#bib.bib27)）。许多预测模型已经被开发用于 D2T 生成，最近的进展主要归功于预测语言模型的使用（Pauls
    and Klein, [2011](#bib.bib78); Mikolov et al., [2010](#bib.bib69); Devlin et al.,
    [2019](#bib.bib17); Radford et al., [2019](#bib.bib87)）。这些模型利用递归神经网络（RNNs）（Hochreiter
    and Schmidhuber, [1997](#bib.bib33); Graves et al., [2013](#bib.bib31); Mikolov
    et al., [2010](#bib.bib69)）和变压器架构（Vaswani et al., [2017](#bib.bib107)），在各种 D2T
    任务中表现出显著潜力。变压器引起了广泛关注，并在 D2T 应用中展示了显著成功（Su et al., [2021](#bib.bib98); Wang et
    al., [2020](#bib.bib109)）。'
- en: 'D2T is typically learned from a dataset consisting of a set of source-reference
    pairs, $\mathcal{D}=\left.{\{s,r\}}\right|_{\in\mathcal{D}}$, i.e., $r_{i}\in\mathcal{V}$.
    These predictive D2T models (Sha et al., [2018](#bib.bib94); Liu et al., [2018](#bib.bib57);
    Dusek et al., [2018](#bib.bib20); Gong et al., [2019](#bib.bib30)) often follow
    either encoder-decoder or decoder-only paradigms, with transformer-based architectures
    being commonly used for both encoder and decoder components. In training or learning
    phase, the predictive D2T model $\mathcal{M}_{\theta}$, typically using maximum
    likelihood estimation (Gkatzia, [2016](#bib.bib29); Holtzman et al., [2018](#bib.bib34);
    Puduppully and Lapata, [2021](#bib.bib85); Luo et al., [2023](#bib.bib65)) strategies
    (equation [1](#S4.E1 "In 4.3 Data-to-Text (D2T) Generation ‣ 4 Preliminaries ‣
    Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: D2T 通常是从一个由源-参考对组成的数据集学习的，$\mathcal{D}=\left.{\{s,r\}}\right|_{\in\mathcal{D}}$，即
    $r_{i}\in\mathcal{V}$。这些预测性 D2T 模型（Sha 等，[2018](#bib.bib94)；Liu 等，[2018](#bib.bib57)；Dusek
    等，[2018](#bib.bib20)；Gong 等，[2019](#bib.bib30)）通常遵循编码器-解码器或仅解码器的范式，基于 transformer
    的架构在编码器和解码器组件中都被广泛使用。在训练或学习阶段，预测性 D2T 模型 $\mathcal{M}_{\theta}$ 通常使用最大似然估计（Gkatzia，[2016](#bib.bib29)；Holtzman
    等，[2018](#bib.bib34)；Puduppully 和 Lapata，[2021](#bib.bib85)；Luo 等，[2023](#bib.bib65)）策略（方程
    [1](#S4.E1 "在 4.3 数据到文本 (D2T) 生成 ‣ 4 基础知识 ‣ 模型规模对数据到文本生成中的微调 LLM 性能的影响：最新调查")）。
- en: '|  | $\displaystyle\theta^{*}\leftarrow\underset{\theta}{\text{argmax}}\sum\limits_{(s,r)\in\mathcal{D}}\mathcal{L}(\mathcal{M}_{\theta}(s),r)$
    |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}\leftarrow\underset{\theta}{\text{argmax}}\sum\limits_{(s,r)\in\mathcal{D}}\mathcal{L}(\mathcal{M}_{\theta}(s),r)$
    |  | (1) |'
- en: Here, $\mathcal{L}$) from a given source data, $s$ (Fan et al., [2018](#bib.bib23)),
    etc.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathcal{L}$) 从给定的源数据 $s$（Fan 等，[2018](#bib.bib23)）等。
- en: '|  | $\displaystyle g\leftarrow\underset{\mathcal{V}^{*}}{\text{decoding}}\
    \mathcal{M}_{\theta^{*}}(s)$ |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g\leftarrow\underset{\mathcal{V}^{*}}{\text{解码}}\ \mathcal{M}_{\theta^{*}}(s)$
    |  | (2) |'
- en: 4.4 Source-Reference Divergence
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 源参考差异
- en: 'Previous studies (Dhingra et al., [2019](#bib.bib18); Tian et al., [2019](#bib.bib102);
    ul Islam et al., [2023](#bib.bib105)) have noted that across various D2T tasks (Lebret
    et al., [2016](#bib.bib49)), there often exists a discrepancy between the information
    or facts present in the source data ($s$). This disparity is known as source-reference
    divergence (Dhingra et al., [2019](#bib.bib18); Li et al., [2022](#bib.bib54)).
    An illustrative example of such divergence is depicted in Figure [5](#S4.F5 "Figure
    5 ‣ 4.4 Source-Reference Divergence ‣ 4 Preliminaries ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究（Dhingra 等，[2019](#bib.bib18)；Tian 等，[2019](#bib.bib102)；ul Islam 等，[2023](#bib.bib105)）已注意到，在各种
    D2T 任务（Lebret 等，[2016](#bib.bib49)）中，源数据（$s$）中的信息或事实之间通常存在差异。这种差异被称为源参考差异（Dhingra
    等，[2019](#bib.bib18)；Li 等，[2022](#bib.bib54)）。这种差异的一个示例如图 [5](#S4.F5 "图 5 ‣ 4.4
    源参考差异 ‣ 4 基础知识 ‣ 模型规模对数据到文本生成中的微调 LLM 性能的影响：最新调查") 所示。
- en: '![Refer to caption](img/a6867828ff8fa0e467e0999736c3820b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6867828ff8fa0e467e0999736c3820b.png)'
- en: 'Figure 5: An example of source-reference divergence, taken from the WikiTableText (Bao
    et al., [2018](#bib.bib5)) dataset. The reference text includes two additional
    facts (bolded and underlined) - ‘scrolling performer’ and ‘fun game’ - that are
    absent in the corresponding source data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：源参考差异的一个示例，取自 WikiTableText（Bao 等，[2018](#bib.bib5)）数据集。参考文本包含两个额外的事实（加粗和下划线）
    - ‘滚动表演者’和‘有趣的游戏’ - 这些在对应的源数据中不存在。
- en: The primary causes of such divergence stem from the processes of data collection
    and the inherent nature of the tasks (Lebret et al., [2016](#bib.bib49); Gardent
    et al., [2017](#bib.bib26); Dusek et al., [2018](#bib.bib20)). In many cases,
    source and reference texts are drawn from disparate origins, resulting in a weak
    factual correlation between them. Additionally, D2T datasets are often curated
    by multiple human annotators who interpret the source data before generating corresponding
    textual references (Nie et al., [2019b](#bib.bib73)). Each annotator brings their
    unique background knowledge and comprehension abilities to the task, leading to
    generated references that may include additional or misaligned facts. Another
    significant factor contributing to source-reference divergence in D2T tasks is
    the inherent nature of D2T tasks (Li et al., [2022](#bib.bib54)). Source data
    of D2T task are typically more compact and specific, whereas references tend to
    be more generalized. Consequently, source-reference divergence is common and challenging
    to eliminate entirely, particularly in D2T contexts (Dhingra et al., [2019](#bib.bib18);
    Nie et al., [2019b](#bib.bib73)). Therefore, evaluating D2T model performance
    in the presence of such divergence is of utmost importance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异的主要原因源于数据收集过程和任务的固有性质（Lebret et al., [2016](#bib.bib49)；Gardent et al.,
    [2017](#bib.bib26)；Dusek et al., [2018](#bib.bib20)）。在许多情况下，源文本和参考文本来源不同，导致它们之间的事实关联较弱。此外，D2T数据集通常由多个人工标注者整理，他们在生成相应的文本参考之前会解读源数据（Nie
    et al., [2019b](#bib.bib73)）。每个标注者将其独特的背景知识和理解能力带入任务中，导致生成的参考中可能包含额外或对不齐的事实。另一个显著因素是D2T任务的固有性质（Li
    et al., [2022](#bib.bib54)）。D2T任务的源数据通常更紧凑和具体，而参考数据往往更为通用。因此，源文本和参考文本之间的差异是常见且难以完全消除的，尤其是在D2T背景下（Dhingra
    et al., [2019](#bib.bib18)；Nie et al., [2019b](#bib.bib73)）。因此，在存在这种差异的情况下评估D2T模型性能至关重要。
- en: 5 Models, Datasets and Experimental Settings
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 模型、数据集和实验设置
- en: In this section, we describe the large language models (LLMs) and data-to-text
    (D2T) datasets employed in our subsequent experiments. Additionally, we include
    a subsection outlining the experimental settings, where we provide detailed information
    regarding the experimental setup.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了在随后的实验中使用的大型语言模型（LLMs）和数据到文本（D2T）数据集。此外，我们还包括了一个小节，概述实验设置，在其中提供了有关实验设置的详细信息。
- en: 5.1 Models
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 模型
- en: 'We include 12 popular LLMs from five widely recognized LLM families (BART,
    T5, BLOOM, OPT, and Llama 2), all of which are widely utilized in the text generation
    field. We select LLMs of varying model sizes from each of the five model families.
    Considering the model sizes, among these five LLM families, we categorize BART
    and T5 as families of smaller LLMs, while the remaining three are families of
    larger LLMs. Figure [6](#S5.F6 "Figure 6 ‣ 5.1 Models ‣ 5 Models, Datasets and
    Experimental Settings ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation") depicts all 12 incorporated
    LLMs along with their sizes, i.e., the number of parameters. All these LLMs are
    sourced from Hugging Face’s Model Hub (Wolf et al., [2020](#bib.bib112)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们包含了来自五个广泛认可的LLM家族（BART、T5、BLOOM、OPT和Llama 2）的12个流行LLM，这些LLM在文本生成领域被广泛使用。我们从每个家族中选择了不同模型尺寸的LLM。考虑到模型尺寸，在这五个LLM家族中，我们将BART和T5归类为较小的LLM家族，而其余三个则为较大的LLM家族。图[6](#S5.F6
    "Figure 6 ‣ 5.1 Models ‣ 5 Models, Datasets and Experimental Settings ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")展示了所有12个包含的LLM及其尺寸，即参数数量。这些LLM均来源于Hugging Face的模型库（Wolf et al.,
    [2020](#bib.bib112)）。'
- en: '![Refer to caption](img/435fa74b32b00f102c6101723183d505.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/435fa74b32b00f102c6101723183d505.png)'
- en: 'Figure 6: Sizes (number of parameters) of the 12 LLMs from five LLM families
    included in our experiments. BART, T5, BLOOM, OPT, and Llama 2 families are indicated
    with blue, orange, green, red, and purple respectively.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：我们实验中包含的五个LLM家族的12个LLM的尺寸（参数数量）。BART、T5、BLOOM、OPT和Llama 2家族分别用蓝色、橙色、绿色、红色和紫色表示。
- en: BART.
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BART。
- en: 'BART (Lewis et al., [2020](#bib.bib51)) is an encoder-decoder architecture-based
    language model family. The encoder encodes the given context through bidirectional
    language modeling, while the decoder performs auto-regressive generation. BART
    is pre-trained using a self-supervised approach involving text denoising, which
    includes methods like token masking, token deletion, text infilling, etc. In our
    experiments, we use two models of the BART family, which differ in terms of their
    sizes: BART-base (139 million parameters) and BART-large (406 million parameters).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: BART（Lewis等，[2020](#bib.bib51)）是一个基于编码器-解码器架构的语言模型系列。编码器通过双向语言建模对给定上下文进行编码，而解码器则执行自回归生成。BART使用自监督的方法进行预训练，包括文本去噪，这些方法包括token掩码、token删除、文本填充等。在我们的实验中，我们使用了BART系列的两个模型，尺寸上有所不同：BART-base（1.39亿参数）和BART-large（4.06亿参数）。
- en: T5.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: T5。
- en: 'Similar to BART, T5 (Raffel et al., [2020](#bib.bib88)) follows the encoder-decoder
    paradigm. However, the main motivation behind T5 is to unify several NLP tasks
    and train them together as a single task. T5 is pre-trained using BERT-like span-corruption
    techniques (Devlin et al., [2019](#bib.bib17)) on a large crawled dataset called
    C4 (Colossal Clean Crawled Corpus) (Raffel et al., [2020](#bib.bib88)). T5 has
    demonstrated state-of-the-art results on several natural language understanding
    tasks. In our experiments, we utilize two models of T5 family with different model
    sizes: T5-base (223 million parameters) and T5-large (738 million parameters).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于BART，T5（Raffel等，[2020](#bib.bib88)）遵循编码器-解码器范式。然而，T5的主要动机是统一多个NLP任务，并将它们作为一个单一任务进行训练。T5使用类似BERT的跨度损坏技术（Devlin等，[2019](#bib.bib17)）在一个大型爬取数据集C4（Colossal
    Clean Crawled Corpus）（Raffel等，[2020](#bib.bib88)）上进行预训练。T5在多个自然语言理解任务中展示了最先进的结果。在我们的实验中，我们使用了T5系列的两个不同尺寸的模型：T5-base（2.23亿参数）和T5-large（7.38亿参数）。
- en: BLOOM.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BLOOM。
- en: 'BLOOM (Scao et al., [2022](#bib.bib92)) is a publicly available decoder-only
    language model family trained on a large dataset—ROOT dataset, which includes
    46 natural languages and 13 programming languages. Similar to BART and T5, BLOOM
    is based on a transformer architecture. In our study, we consider three different
    sizes for BLOOM LLMs: BLOOM-1B (1 billion parameters), BLOOM-2.7B (2.7 billion
    parameters), and BLOOM-6.7B (6.7 billion parameters).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: BLOOM（Scao等，[2022](#bib.bib92)）是一个公开可用的仅解码器语言模型系列，训练在一个大型数据集——ROOT数据集上，该数据集包括46种自然语言和13种编程语言。类似于BART和T5，BLOOM基于transformer架构。在我们的研究中，我们考虑了BLOOM
    LLM的三种不同尺寸：BLOOM-1B（10亿参数）、BLOOM-2.7B（27亿参数）和BLOOM-6.7B（67亿参数）。
- en: OPT.
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OPT。
- en: 'Like BLOOM, OPT (Zhang et al., [2022](#bib.bib121)) is an auto-regressive decoder-based
    language model family that utilizes a transformer decoder architecture. OPT is
    pre-trained on three datasets: the RoBERTa dataset (Liu et al., [2019](#bib.bib59)),
    the Pile, and PushShift.io Reddit (Zhang et al., [2022](#bib.bib121)). These corpora
    were previously collected or filtered. OPT has demonstrated performance comparable
    to popular GPT models on several state-of-the-art benchmarks. Unlike GPT (Brown
    et al., [2020](#bib.bib11)), OPT is fully publicly available. In this paper, we
    use three OPT models with different model sizes: OPT-2.7B (2.7 billion parameters),
    OPT-6.7B (6.7 billion parameters), and OPT-13B (13 billion parameters).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 像BLOOM一样，OPT（Zhang等，[2022](#bib.bib121)）是基于自回归解码器的语言模型系列，采用了transformer解码器架构。OPT在三个数据集上进行了预训练：RoBERTa数据集（Liu等，[2019](#bib.bib59)）、Pile和PushShift.io
    Reddit（Zhang等，[2022](#bib.bib121)）。这些语料库之前已经被收集或筛选过。OPT在多个最先进的基准测试中表现出与流行的GPT模型相当的性能。与GPT（Brown等，[2020](#bib.bib11)）不同，OPT是完全公开的。在本文中，我们使用了三个不同模型尺寸的OPT模型：OPT-2.7B（27亿参数）、OPT-6.7B（67亿参数）和OPT-13B（130亿参数）。
- en: Llama 2.
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Llama 2。
- en: 'Llama 2 family (Touvron et al., [2023](#bib.bib103)) is the successor of Llama
    from [Meta AI](https://ai.meta.com/). It is a decoder-based LLM trained through
    three stages: traditional pre-training, supervised fine-tuning, and human feedback
    endorsed by reinforcement learning. Llama 2 is pre-trained with almost 2 trillion
    tokens and is ranked higher in terms of human safety and usability compared to
    other LLMs. It shows comparable results with several proprietary LLMs like ChatGPT.
    In our experiments, we utilize two Llama 2 models with different sizes: Llama2-6B
    (6 billion parameters) and Llama2-13B (13 billion parameters).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 系列 (Touvron et al., [2023](#bib.bib103)) 是 [Meta AI](https://ai.meta.com/)
    的 Llama 的继任者。它是一个基于解码器的 LLM，通过三个阶段进行训练：传统的预训练、监督微调和通过强化学习获得的人类反馈。Llama 2 经过了近
    2 万亿个标记的预训练，并在人员安全性和可用性方面的排名高于其他 LLM。它的结果与多个专有 LLM 如 ChatGPT 相媲美。在我们的实验中，我们使用了两种不同规模的
    Llama 2 模型：Llama2-6B（60 亿个参数）和 Llama2-13B（130 亿个参数）。
- en: 5.2 Datasets
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 数据集
- en: 'We included five state-of-the-art D2T datasets covering all three major types
    of D2T tasks: E2E and ViGGO for MR-to-text, WikiTableText for table-to-text, and
    DART and WebNLG for graph-to-text. Table [1](#S5.T1 "Table 1 ‣ WikiTableText dataset.
    ‣ 5.2 Datasets ‣ 5 Models, Datasets and Experimental Settings ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") presents key statistics for all five D2T datasets. All these dataset
    are downloaded from Kasner et al. ([2023](#bib.bib46)) and Wolf et al. ([2020](#bib.bib112)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们包括了五个最先进的 D2T 数据集，涵盖了所有三种主要类型的 D2T 任务：E2E 和 ViGGO 用于 MR-to-text，WikiTableText
    用于 table-to-text，DART 和 WebNLG 用于 graph-to-text。表 [1](#S5.T1 "Table 1 ‣ WikiTableText
    dataset. ‣ 5.2 Datasets ‣ 5 Models, Datasets and Experimental Settings ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") 展示了所有五个 D2T 数据集的关键统计数据。这些数据集均从 Kasner et al. ([2023](#bib.bib46))
    和 Wolf et al. ([2020](#bib.bib112)) 下载。'
- en: E2E dataset.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E2E 数据集。
- en: This dataset contains information from the restaurant domain and is frequently
    utilized in D2T tasks (Novikova et al., [2017](#bib.bib75); Dusek et al., [2018](#bib.bib20)).
    It consists of input data in the form of slot-value pairs, where each slot represents
    an attribute and its corresponding value indicates the attribute’s realization.
    Known for its lexical richness and diverse syntactical structures, this dataset
    is highly esteemed in D2T research. Comprising over 2 million tokens (including
    both references and source) and almost 37K data-text pairs, with approximately
    4.5K unique words, it provides abundant data for both training and evaluation
    purposes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含来自餐馆领域的信息，并且在 D2T 任务中被广泛使用 (Novikova et al., [2017](#bib.bib75); Dusek
    et al., [2018](#bib.bib20))。它由插槽-值对的输入数据组成，其中每个插槽代表一个属性，而其对应的值表示该属性的实现。该数据集以其词汇丰富性和多样的句法结构而著称，在
    D2T 研究中备受推崇。数据集包含超过 200 万个标记（包括参考和来源），以及近 37K 对数据-文本对，具有约 4.5K 个独特词汇，为训练和评估提供了丰富的数据。
- en: ViGGo dataset.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ViGGo 数据集。
- en: The ViGGO dataset (Juraska et al., [2018](#bib.bib42)), focusing on the video
    game domain, is ideal for training open-domain D2T models. With a diverse range
    of dialogue acts and forms drawn from nearly a hundred video games, ViGGO offers
    extensive coverage and high text diversity. Containing around 7K instances, ViGGO
    exhibits greater lexical richness compared to other datasets like E2E. Each meaning
    representation in ViGGO corresponds to one of nine distinct conversational dialogue
    acts, providing nuanced dialogue interactions. Overall, ViGGO excels in prioritizing
    lexical richness and text diversity, making it a valuable resource for D2T tasks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ViGGO 数据集 (Juraska et al., [2018](#bib.bib42)) 专注于视频游戏领域，非常适合训练开放领域的 D2T 模型。ViGGO
    从近百款视频游戏中提取了各种对话行为和形式，提供了广泛的覆盖面和高文本多样性。该数据集包含约 7K 个实例，相比其他数据集如 E2E，ViGGO 展示了更丰富的词汇。ViGGO
    中的每个意义表示都对应于九种不同的对话行为之一，提供了细致的对话互动。总体而言，ViGGO 在优先考虑词汇丰富性和文本多样性方面表现突出，使其成为 D2T
    任务的宝贵资源。
- en: WikiTableText dataset.
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WikiTableText 数据集。
- en: WikiTableText (Bao et al., [2018](#bib.bib5)) is designed for the table-to-text
    form of Data-to-Text (D2T) task. Unlike the E2E and ViGGO datasets, it belongs
    to the open-domain category. It consists of numerous domain Wikipedia tables,
    randomly selected and manually annotated to generate corresponding text. With
    a total of $\sim 13$K data-text pairs, each pair contains an average of 13.9 tokens.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: WikiTableText（Bao 等人，[2018](#bib.bib5)）设计用于表到文本形式的数据到文本（D2T）任务。与 E2E 和 ViGGO
    数据集不同，它属于开放领域类别。它由大量领域维基百科表组成，这些表是随机选择并手动注释以生成相应的文本。总共有约 13K 数据-文本对，每对包含平均 13.9
    个词元。
- en: '| dataset | D2T types | domain | dataset size | source (linearized) | reference
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | D2T 类型 | 领域 | 数据集大小 | 源（线性化） | 参考文献 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| (# instances) | average length | unique tokens | total tokens | average length
    | unique tokens | total tokens |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| （实例数） | 平均长度 | 唯一词元 | 总词元 | 平均长度 | 唯一词元 | 总词元 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| E2E | mr-to-text | closed | 36,856 | 27.3 | 125 | 1M | 20.8 | 4.5K | 885K
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| E2E | mr-to-text | closed | 36,856 | 27.3 | 125 | 1M | 20.8 | 4.5K | 885K
    |'
- en: '| ViGGo | mr-to-text | closed | 6,900 | 29.9 | 618 | 206K | 21.5 | 4.4K | 148K
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ViGGo | mr-to-text | closed | 6,900 | 29.9 | 618 | 206K | 21.5 | 4.4K | 148K
    |'
- en: '| WikiTableText | table-to-text | open | 13,318 | 35.2 | 29K | 469K | 13.9
    | 24K | 185K |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WikiTableText | table-to-text | open | 13,318 | 35.2 | 29K | 469K | 13.9
    | 24K | 185K |'
- en: '| DART | graph-to-text | open | 70,524 | 34.8 | 44K | 2.5M | 19.3 | 45K | 1.5M
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| DART | graph-to-text | open | 70,524 | 34.8 | 44K | 2.5M | 19.3 | 45K | 1.5M
    |'
- en: '| WebNLG | graph-to-text | open | 38,872 | 30.4 | 7K | 1.2M | 20.1 | 19K |
    905K |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| WebNLG | graph-to-text | open | 38,872 | 30.4 | 7K | 1.2M | 20.1 | 19K |
    905K |'
- en: 'Table 1: Summary of key statistics, D2T types, and domains for the five incorporated
    datasets. The table presents average length (number of tokens in text), unique
    tokens, and total tokens for both sources (linearized to text) and references.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：五个纳入数据集的关键统计数据、D2T 类型和领域的汇总。该表展示了源文本（线性化为文本）和参考文献的平均长度（文本中的词元数量）、唯一词元和总词元。
- en: DART dataset.
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DART 数据集。
- en: The DART dataset (Nan et al., [2021](#bib.bib70)) is integral for open-domain
    structured D2T generation, similar to the WikiTableText dataset. It pertains specifically
    to the graph-to-text D2T task, aiming to generate text from knowledge graph triplets.
    Each instance in the dataset covers a variety of domains, with inputs consisting
    of semantic triplet sets accompanied by detailed sentence descriptions. These
    descriptions are curated from a multitude of datasets. On average, the reference
    text sets contain approximately 19.3 tokens each.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: DART 数据集（Nan 等人，[2021](#bib.bib70)）对于开放领域的结构化 D2T 生成至关重要，类似于 WikiTableText 数据集。它专门涉及图到文本
    D2T 任务，旨在从知识图谱三元组生成文本。数据集中的每个实例涵盖各种领域，输入由语义三元组集和详细的句子描述组成。这些描述从众多数据集中精选而来。平均而言，参考文本集每个包含约
    19.3 个词元。
- en: WebNLG dataset.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: WebNLG 数据集。
- en: The underlying task of this dataset involves generating text from a series of
    knowledge graph entries, also known as RDF (Resource Description Format)-to-text
    generation, a type of graph-to-text generation (Gardent et al., [2017](#bib.bib26)).
    The dataset focuses on generating text from knowledge graph triplets sourced mainly
    from DBPedia across six domains (Perez-Beltrachini and Gardent, [2017](#bib.bib81);
    Gardent et al., [2017](#bib.bib26)). It’s gathered via crowdsourcing, ensuring
    diverse and validated outputs. Notably, it exhibits greater lexical and semantic
    diversity compared to similar datasets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的基础任务涉及从一系列知识图谱条目生成文本，也称为 RDF（资源描述格式）到文本生成，这是一种图到文本生成方法（Gardent 等人，[2017](#bib.bib26)）。数据集专注于从主要来自
    DBPedia 的知识图谱三元组中生成文本，覆盖六个领域（Perez-Beltrachini 和 Gardent，[2017](#bib.bib81)；Gardent
    等人，[2017](#bib.bib26)）。它通过众包收集，确保了多样化和经过验证的输出。值得注意的是，它展示了比类似数据集更大的词汇和语义多样性。
- en: 5.3 Experimental Settings
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 实验设置
- en: In all our experiments in this study, we fined-tune LLMs for D2T task. We fine-tune
    each LLM on all five datasets for 3 epochs. Because of smaller sizes, T5 and BART
    models underwent full fine-tuning with a learning rate of 1e-05. Both T5 and BART
    were loaded with torch.bfloat16 precision. However, due to the large number of
    parameters in BLOOM, OPT, and Llama 2, we employed the QLoRA (Quantized version
    of Low Rank Adapter) (Dettmers et al., [2023](#bib.bib16)) technique as a parameter-efficient
    fine-tuning approach, utilizing a learning rate of 1e-04. All these three larger
    LLM families’ models were loaded with 4-bit precision. The peft library are used
    for implementing QLoRA in our experiments (Mangrulkar et al., [2022](#bib.bib67)).
    As previously mentioned, all 12 LLMs and the five D2T datasets were sourced from
    HuggingFace (Wolf et al., [2020](#bib.bib112)). We maintained a consistent text
    length of 256 for all generations, encompassing both source data and generated
    text. For optimization purposes, we utilized the popular AdamW optimizer (Loshchilov
    and Hutter, [2019](#bib.bib61)), which is primarily based on the Adam optimizer (Kingma
    and Ba, [2015](#bib.bib48)) with $L^{2}$-norm over the weight space. In QLoRA
    set-ups, we continue AdamW optimizer’s operation at torch.bfloat16 precision.
    Decoding strategies play a crucial role in text generation. We compared all generated
    text using the beam-search decoding strategy with a beam size of 5. All experiments
    were conducted on a local system equipped with an NVIDIA RTX A6000 (48 GB) graphics
    card to ensure adequate GPU access.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究的所有实验中，我们对 LLM 进行 D2T 任务的微调。我们对所有五个数据集上的每个 LLM 进行了 3 个周期的微调。由于规模较小，T5 和
    BART 模型进行了完整的微调，学习率为 1e-05。T5 和 BART 都以 torch.bfloat16 精度加载。然而，由于 BLOOM、OPT 和
    Llama 2 的参数数量较多，我们采用了 QLoRA（低秩适配器的量化版本）（Dettmers 等，[2023](#bib.bib16)）技术作为参数高效的微调方法，使用了
    1e-04 的学习率。这三种较大的 LLM 家族模型都以 4 位精度加载。在我们的实验中使用了 peft 库来实现 QLoRA（Mangrulkar 等，[2022](#bib.bib67)）。如前所述，所有
    12 个 LLM 和五个 D2T 数据集均来源于 HuggingFace（Wolf 等，[2020](#bib.bib112)）。我们保持了所有生成文本的长度为
    256，包括源数据和生成文本。为了优化，我们使用了流行的 AdamW 优化器（Loshchilov 和 Hutter，[2019](#bib.bib61)），它主要基于
    Adam 优化器（Kingma 和 Ba，[2015](#bib.bib48)），在权重空间上使用 $L^{2}$-范数。在 QLoRA 设置中，我们继续以
    torch.bfloat16 精度运行 AdamW 优化器。解码策略在文本生成中发挥着至关重要的作用。我们使用 beam-search 解码策略（束宽为 5）比较所有生成的文本。所有实验均在配备
    NVIDIA RTX A6000（48 GB）显卡的本地系统上进行，以确保足够的 GPU 访问。
- en: Statistical Significant Testing.
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计显著性测试。
- en: We aim to determine the statistical significance of the performance evaluations
    in all our comparative analyses across the three key qualities of D2T (readability,
    informativeness, and faithfulness). To achieve this, we conduct Welch’s t-test (Dror
    et al., [2018](#bib.bib19)) comparing the results of each LLM to the best-performing
    LLM within the same family. We apply a significance level of $p |  | (3) |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  |  | (3) |'
- en: Where $|r|$) text, respectively. In most cases, $n$ is set to 4, which is also
    known as Bleu-4. In our evaluation also we employed Bleu-4. We used SacreBLEU (Post,
    [2018](#bib.bib84)) implementation to obtain our blue results.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $|r|$）文本，分别。在大多数情况下，$n$ 设置为 4，这也称为 Bleu-4。在我们的评估中也使用了 Bleu-4。我们使用了 SacreBLEU（Post，[2018](#bib.bib84)）的实现来获得我们的
    Bleu 结果。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Meteor. Meteor (Banerjee and Lavie, [2005](#bib.bib4)), similar to Bleu, relies
    on n-gram string matching and finds predominant use in machine translation tasks.
    While Bleu focuses solely on n-gram precision, it overlooks the recall component.
    Meteor addresses this by leveraging unigram-based string overlaps to compute both
    recall and precision. Ultimately, Meteor distinguishes itself as a weighted variant
    of the F1-measure, as expressed by the following equation:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meteor。Meteor（Banerjee 和 Lavie，[2005](#bib.bib4)），类似于 Bleu，依赖于 n-gram 字符串匹配，并在机器翻译任务中得到广泛应用。尽管
    Bleu 专注于 n-gram 精确度，但忽略了召回组件。Meteor 通过利用 unigram 字符串重叠来计算召回率和精确度，从而解决了这个问题。最终，Meteor
    作为 F1 衡量的加权变体，与以下方程相关：
- en: '|  | $\displaystyle\textsc{Meteo}_{\text{F1-measure}}=\frac{10\cdot\text{Precison}\cdot\text{Recall}}{\text{Recall}+9\text{Precison}}$
    |  |'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\textsc{Meteo}_{\text{F1-measure}}=\frac{10\cdot\text{Precison}\cdot\text{Recall}}{\text{Recall}+9\text{Precison}}$
    |  |'
- en: 'Where, Precision and Recall denote precision and recall based on unigram-based
    matching. To account for higher-order $n$-grams, Meteor applies a penalty factor
    to the F1-measure. The penalty factor is calculated as the ratio of the number
    of chunks concatenated through matched unigrams to the number of matched unigrams
    (equation [4](#S6.E4 "In 2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")).'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，精确度（Precision）和召回率（Recall）表示基于 unigram 匹配的精确度和召回率。为了考虑更高阶的 $n$-grams，Meteor
    对 F1 衡量应用了一个惩罚因子。惩罚因子计算为通过匹配 unigram 连接的块的数量与匹配 unigram 的数量的比率（方程 [4](#S6.E4 "第2项
    ‣ 6.1 可读性 ‣ 6 性能评估和结果 ‣ 模型规模对数据到文本生成中微调 LLM 性能的影响：最先进的研究")）。
- en: '|  | $\displaystyle\text{penalty}=0.5\times\frac{\text{number of chunks}}{\text{number
    of matched unigrams}}$ |  | (4) |'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{penalty}=0.5\times\frac{\text{number of chunks}}{\text{number
    of matched unigrams}}$ |  | (4) |'
- en: Finally, Meteor score is calculated through,
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终，Meteor 分数的计算如下：
- en: '|  | $\displaystyle\textsc{Meteor}=(1-\text{penalty})\cdot\textsc{Meteo}_{\text{F1-measure}}$
    |  |'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\textsc{Meteor}=(1-\text{penalty})\cdot\textsc{Meteo}_{\text{F1-measure}}$
    |  |'
- en: In addition to the above formulation, unlike Bleu, Meteor integrates synonym
    matching (via WordNet synsets) and word stemming (via Porter’s stemmer) features
    to capture textual similarity between candidates and references.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了上述公式外，与 Bleu 不同，Meteor 结合了同义词匹配（通过 WordNet 同义词集）和词干提取（通过 Porter 的词干提取器）功能，以捕捉候选文本与参考文本之间的文本相似性。
- en: '| family | model |'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| family | model |'
- en: '&#124; size (number of &#124;'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 大小（参数数量）&#124;'
- en: '&#124; parameters in billion) &#124;'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 以十亿为单位的参数数量&#124;'
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
- en: '| BART | BART-base | 0.1 | 0.368 | 0.315 | 0.367 | 0.365 | 0.377 |'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART | BART-base | 0.1 | 0.368 | 0.315 | 0.367 | 0.365 | 0.377 |'
- en: '| BART-large | 0.4 | 0.371 | 0.321 | 0.367 | 0.381 | 0.408 |'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART-large | 0.4 | 0.371 | 0.321 | 0.367 | 0.381 | 0.408 |'
- en: '| T5 | T5-base | 0.2 | 0.369 | 0.312 | 0.361 | 0.390 | 0.410 |'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5 | T5-base | 0.2 | 0.369 | 0.312 | 0.361 | 0.390 | 0.410 |'
- en: '| T5-large | 0.7 | 0.377 | 0.324 | 0.370 | 0.400 | 0.420 |'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5-large | 0.7 | 0.377 | 0.324 | 0.370 | 0.400 | 0.420 |'
- en: '| OPT | OPT-2.7B | 2.7 | 0.355 | 0.307 | 0.366 | 0.382 | 0.403 |'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT | OPT-2.7B | 2.7 | 0.355 | 0.307 | 0.366 | 0.382 | 0.403 |'
- en: '| OPT-6.7B | 6.7 | 0.366 | 0.305 | 0.371 | 0.387 | 0.406 |'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-6.7B | 6.7 | 0.366 | 0.305 | 0.371 | 0.387 | 0.406 |'
- en: '| OPT-13B | 13.0 | 0.355 | 0.308 | 0.361 | 0.391 | 0.411 |'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-13B | 13.0 | 0.355 | 0.308 | 0.361 | 0.391 | 0.411 |'
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.369 | 0.304 | 0.367 | 0.379 | 0.386 |'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM | BLOOM-1.1B | 1.1 | 0.369 | 0.304 | 0.367 | 0.379 | 0.386 |'
- en: '| BLOOM-3B | 3.0 | 0.371 | 0.305 | 0.356 | 0.386 | 0.403 |'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-3B | 3.0 | 0.371 | 0.305 | 0.356 | 0.386 | 0.403 |'
- en: '| BLOOM-7B | 7.0 | 0.369 | 0.314 | 0.375 | 0.385 | 0.412 |'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-7B | 7.0 | 0.369 | 0.314 | 0.375 | 0.385 | 0.412 |'
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.376 | 0.307 | 0.375 | 0.403 | 0.412 |'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama 2 | Llama2-7B | 7.0 | 0.376 | 0.307 | 0.375 | 0.403 | 0.412 |'
- en: '| Llama2-13B | 13.0 | 0.37 | 0.318 | 0.38 | 0.407 | 0.419 |'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-13B | 13.0 | 0.37 | 0.318 | 0.38 | 0.407 | 0.419 |'
- en: 'Table 3: Analysis of the readability of 12 LLMs from five LLM families (BERT,
    T5, BLOOM, OPT, and Llama 2) across all five D2T datasets, evaluated using the
    Meteor metric. The best-performing results in terms of model size for each family
    are highlighted in bold. Similar to the Bleu results, the Meteor score tends to
    increase with larger model sizes, with a few exceptions. All results are statistically
    significant at the $p<0.05$ level.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表3：对来自五个 LLM 家族（BERT、T5、BLOOM、OPT 和 Llama 2）的12个 LLM 在所有五个 D2T 数据集上的可读性进行分析，使用
    Meteor 指标评估。每个家族中按模型规模排序的最佳结果以粗体显示。与 Bleu 结果类似，Meteor 分数趋向于随着模型规模的增加而提高，尽管有一些例外。所有结果在
    $p<0.05$ 水平上具有统计显著性。
- en: Takeaways.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结论。
- en: 'From our empirical results on readability based on Table [2](#S6.T2 "Table
    2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") and Table [3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"), we can derive
    insights to address the readability part of first question posed in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation") .'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '从我们基于表格[2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and
    Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")和表格[3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")的可读性经验结果，我们可以获得见解来解决第[2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")节中提出的第一个问题的可读性部分。'
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability
    , informativeness, and faithfulness?'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：在同一系列的微调LLMs中，模型大小对数据到文本（D2T）任务的可读性、信息量和忠实度的影响是什么？
- en: 'Answer: Undoubtedly, the parameter count of LLMs significantly influences the
    readability of D2T tasks. Across all three categories of D2T tasks, it becomes
    apparent from Table [2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") and Table [3](#S6.T3 "Table 3 ‣
    2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") that augmenting the model parameters substantially enhances the
    readability of D2T tasks, with only minor discrepancies observed. Nearly all LLMs
    demonstrate superior performance compared to their lower-sized counterparts within
    the same LLM family. Consequently, the Bleu and Meteor scores consistently indicate
    improved readability across all LLMs within their respective families. If we consider
    the sum of all results across the five datasets, it is evident that in a family
    of large language models (LLMs), the readability scores consistently improve with
    an increase in the number of parameters, i.e., model size. As both Table [2](#S6.T2
    "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") and Table [3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation") show statistically
    significant results at the $p<0.05$ level, thereby reinforcing our findings.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '答案：毫无疑问，LLMs的参数数量显著影响D2T任务的可读性。从表格[2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")和表格[3](#S6.T3 "Table
    3 ‣ 2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")中可以明显看出，增加模型参数会显著提升D2T任务的可读性，且观察到的差异仅为微小的。几乎所有LLMs在同一LLM系列中均表现优于其较小尺寸的对应模型。因此，Bleu和Meteor得分在所有LLMs中一致地指示出可读性的提高。如果我们考虑五个数据集中的所有结果之和，可以明显看出，在一个大型语言模型（LLMs）系列中，随着参数数量的增加，即模型大小的增加，可读性得分持续提高。由于表格[2](#S6.T2
    "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")和表格[3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation")的结果在$p<0.05$水平上具有统计显著性，从而进一步支持我们的发现。'
- en: 6.2 Informativeness
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 信息量
- en: Informativeness of D2T models concerns the content similarity between reference
    text and the generated text from the model (Shen et al., [2019](#bib.bib96); Erdem
    et al., [2022](#bib.bib21); Belz et al., [2020](#bib.bib8)). A highly informative
    D2T model indicates its ability to generate genuinely helpful information. Semantic
    similarity between reference and generated text contributes to content similarity.
    To measure informativeness based on content similarity, we incorporate two contextual
    similarity-based metrics—BertScore and MoverScore. It’s important to note that,
    similar to reference text, source text also contains valuable information, and
    in the presence of source-reference divergence, it becomes crucial to assess the
    similarity between source and generated text, similar to reference text. For measuring
    informativeness in the presence of source-reference divergence, we use the Parent
    metric, which considers both reference and source text to compute content similarity
    with respect to the generated text.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: D2T 模型的信息量涉及参考文本与模型生成文本之间的内容相似性（Shen et al., [2019](#bib.bib96)；Erdem et al.,
    [2022](#bib.bib21)；Belz et al., [2020](#bib.bib8)）。一个高度信息化的 D2T 模型表明其能够生成真正有用的信息。参考文本与生成文本之间的语义相似性有助于内容相似性。为了基于内容相似性来衡量信息量，我们引入了两个基于上下文相似性的度量标准——BertScore
    和 MoverScore。需要注意的是，类似于参考文本，源文本也包含有价值的信息，在存在源参考分歧的情况下，评估源文本和生成文本之间的相似性变得至关重要，类似于参考文本。为了在存在源参考分歧的情况下衡量信息量，我们使用
    Parent 度量标准，该标准考虑了参考文本和源文本，以计算相对于生成文本的内容相似性。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BertScore. BertScore (Zhang et al., [2020](#bib.bib122)) leverages contextual
    representations to assess text similarity. Unlike models based on n-gram matching,
    which may struggle to capture contextual nuances, BertScore utilizes contextual
    representations derived from the BERT model. BERT is trained using masked language
    modeling. Contextual representations of words within a text can be extracted from
    either the hidden layers or output layers of BERT. BertScore utilizes these contextual
    representations and employs the cosine similarity function to estimate the similarity
    between two contextual representations. It calculates an F1-score based on contextual
    matching between the generated text ($g$ words—$g_{1}g_{2}\dots g_{m}$, Contextual
    representation of BERT of $g$ is,
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BertScore。BertScore（Zhang et al., [2020](#bib.bib122)）利用上下文表示来评估文本相似性。与基于 n-gram
    匹配的模型不同，这些模型可能难以捕捉上下文细微差别，BertScore 利用从 BERT 模型中得到的上下文表示。BERT 是通过掩蔽语言模型进行训练的。文本中单词的上下文表示可以从
    BERT 的隐藏层或输出层中提取。BertScore 利用这些上下文表示，并使用余弦相似度函数来估算两个上下文表示之间的相似性。它基于生成文本（$g$ 词——$g_{1}g_{2}\dots
    g_{m}$）之间的上下文匹配计算 F1 评分，BERT 对 $g$ 的上下文表示是，
- en: '|  | $\displaystyle\text{BERT}(g)=h[g_{1}]h[g_{2}]\dots h[g_{m}]$ |  |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{BERT}(g)=h[g_{1}]h[g_{2}]\dots h[g_{m}]$ |  |'
- en: 'Likewise we can obtained $\text{BERT}(r)$. To compute the recall of BertScore,
    we perform the following steps:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，我们也可以得到 $\text{BERT}(r)$。为了计算 BertScore 的召回率，我们执行以下步骤：
- en: '|  | $\displaystyle\text{Recall}_{\textsc{BertScore}}=\frac{1}{&#124;r&#124;}\sum_{r_{i}\in
    r}\underset{g_{j}\in g}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Recall}_{\textsc{BertScore}}=\frac{1}{&#124;r&#124;}\sum_{r_{i}\in
    r}\underset{g_{j}\in g}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
- en: 'Similarly, the precision of BertScore is measured as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，BertScore 的精确度测量如下：
- en: '|  | $\displaystyle\text{Precision}_{\textsc{BertScore}}=\frac{1}{&#124;g&#124;}\sum_{g_{j}\in
    g}\underset{r_{i}\in r}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Precision}_{\textsc{BertScore}}=\frac{1}{&#124;g&#124;}\sum_{g_{j}\in
    g}\underset{r_{i}\in r}{\text{max}}(h[r_{i}]^{\mathsf{T}}h[g_{j}])$ |  |'
- en: Finally, BertScore is obtained by calculating the F1-measure of $\text{Precision}_{\textsc{BertScore}}$.
    Widely regarded as a superior metric for semantic similarity, BERTScore outperforms
    several other metrics.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，BertScore 是通过计算 $\text{Precision}_{\textsc{BertScore}}$ 的 F1 评分获得的。BERTScore
    被广泛认为是一个优越的语义相似性度量标准，其性能优于其他几种度量标准。
- en: '| family | model |'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 家族 | 模型 |'
- en: '&#124; size (number of &#124;'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 大小（数量 &#124;'
- en: '&#124; parameters in billion) &#124;'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 参数（以十亿为单位） &#124;'
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
- en: '| BART | BART-base | 0.1 | 0.918 | 0.889 | 0.899 | 0.908 | 0.919 |'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART | BART-base | 0.1 | 0.918 | 0.889 | 0.899 | 0.908 | 0.919 |'
- en: '| BART-large | 0.4 | 0.918 | 0.890 | 0.896 | 0.914 | 0.927 |'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART-large | 0.4 | 0.918 | 0.890 | 0.896 | 0.914 | 0.927 |'
- en: '| T5 | T5-base | 0.2 | 0.917 | 0.884 | 0.895 | 0.925 | 0.932 |'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5 | T5-base | 0.2 | 0.917 | 0.884 | 0.895 | 0.925 | 0.932 |'
- en: '| T5-large | 0.7 | 0.919 | 0.895 | 0.899 | 0.927 | 0.936 |'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5-large | 0.7 | 0.919 | 0.895 | 0.899 | 0.927 | 0.936 |'
- en: '| OPT | OPT-2.7B | 2.7 | 0.908 | 0.879 | 0.896 | 0.919 | 0.927 |'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT | OPT-2.7B | 2.7 | 0.908 | 0.879 | 0.896 | 0.919 | 0.927 |'
- en: '| OPT-6.7B | 6.7 | 0.910 | 0.874 | 0.898 | 0.919 | 0.927 |'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-6.7B | 6.7 | 0.910 | 0.874 | 0.898 | 0.919 | 0.927 |'
- en: '| OPT-13B | 13.0 | 0.908 | 0.879 | 0.897 | 0.920 | 0.928 |'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-13B | 13.0 | 0.908 | 0.879 | 0.897 | 0.920 | 0.928 |'
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.911 | 0.884 | 0.895 | 0.915 | 0.917 |'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM | BLOOM-1.1B | 1.1 | 0.911 | 0.884 | 0.895 | 0.915 | 0.917 |'
- en: '| BLOOM-3B | 3.0 | 0.912 | 0.883 | 0.887 | 0.916 | 0.925 |'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-3B | 3.0 | 0.912 | 0.883 | 0.887 | 0.916 | 0.925 |'
- en: '| BLOOM-7B | 7.0 | 0.911 | 0.886 | 0.896 | 0.917 | 0.928 |'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-7B | 7.0 | 0.911 | 0.886 | 0.896 | 0.917 | 0.928 |'
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.918 | 0.882 | 0.897 | 0.931 | 0.934 |'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama 2 | Llama2-7B | 7.0 | 0.918 | 0.882 | 0.897 | 0.931 | 0.934 |'
- en: '| Llama2-13B | 13.0 | 0.918 | 0.889 | 0.900 | 0.931 | 0.938 |'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-13B | 13.0 | 0.918 | 0.889 | 0.900 | 0.931 | 0.938 |'
- en: 'Table 4: Comparative evaluation of Informativeness for 12 LLMs from five families
    (BERT, T5, BLOOM, OPT, and Llama 2) using the BertScore metric on five D2T datasets.
    Bold highlights indicate the best performing model sizes within each family. Generally,
    BertScore increases with larger model sizes, with minor exceptions. All results
    are statistically significant at $p<0.05$.'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 4：使用 BertScore 指标对来自五个系列（BERT、T5、BLOOM、OPT 和 Llama 2）的 12 个 LLM 的信息量进行的比较评估。粗体高亮显示了每个系列中表现最好的模型大小。一般来说，BertScore
    随着模型大小的增加而提高，个别情况除外。所有结果在 $p<0.05$ 时具有统计显著性。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MoverScore. Similar to BertScore, MoverScore (Zhao et al., [2019](#bib.bib124))
    prioritizes contextual similarity between texts. It quantifies text embeddings
    derived from the contextual representation of all underlying words using power
    law. To compute the semantic distance between two texts, it utilizes Word Mover’s
    Distance (WMD) with the underlying $n$-grams of the texts (equation [5](#S6.E5
    "In 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")).'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MoverScore。类似于 BertScore，MoverScore （Zhao 等，[2019](#bib.bib124)）优先考虑文本之间的上下文相似性。它量化了从所有基础词的上下文表示中得到的文本嵌入，使用幂律来计算。为了计算两个文本之间的语义距离，它利用了
    Word Mover’s Distance (WMD) 和文本的基础 $n$-grams（方程 [5](#S6.E5 "在第二项 ‣ 6.2 信息量 ‣ 6
    性能评估与结果 ‣ 模型大小对微调 LLM 在数据到文本生成中的影响：最先进的调查")）。
- en: '|  | $\displaystyle\text{WMD}(g,r)=\underset{F\in\mathbb{R}^{&#124;g&#124;\times&#124;r&#124;}}{\text{min}}\sum
    C\odot F$ |  | (5) |'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{WMD}(g,r)=\underset{F\in\mathbb{R}^{&#124;g&#124;\times&#124;r&#124;}}{\text{min}}\sum
    C\odot F$ |  | (5) |'
- en: Where $C$ denote cost matrix and flow matrix. In WMD, each entry of the cost
    matrix is determined by the contextual representation of the n-grams obtained
    after applying the power law. The flow metrics are obtained by determining the
    weight of each n-gram through inverse document frequency.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $C$ 表示成本矩阵和流量矩阵。在 WMD 中，成本矩阵的每一项由应用幂律后的 n-gram 的上下文表示决定。流量指标通过确定每个 n-gram
    的逆文档频率来获得。
- en: '| family | model |'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 系列 | 模型 |'
- en: '&#124; size (number of &#124;'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 大小（数量） &#124;'
- en: '&#124; parameters in billion) &#124;'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 参数（十亿） &#124;'
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
- en: '| BART | BART-base | 0.1 | 0.667 | 0.658 | 0.688 | 0.666 | 0.673 |'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART | BART-base | 0.1 | 0.667 | 0.658 | 0.688 | 0.666 | 0.673 |'
- en: '| BART-large | 0.4 | 0.667 | 0.664 | 0.683 | 0.671 | 0.684 |'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART-large | 0.4 | 0.667 | 0.664 | 0.683 | 0.671 | 0.684 |'
- en: '| T5 | T5-base | 0. | 0.666 | 0.650 | 0.679 | 0.688 | 0.685 |'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5 | T5-base | 0. | 0.666 | 0.650 | 0.679 | 0.688 | 0.685 |'
- en: '| T5-large | 0.7 | 0.672 | 0.669 | 0.683 | 0.692 | 0.691 |'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5-large | 0.7 | 0.672 | 0.669 | 0.683 | 0.692 | 0.691 |'
- en: '| OPT | OPT-2.7B | 2.7 | 0.655 | 0.645 | 0.682 | 0.673 | 0..676 |'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT | OPT-2.7B | 2.7 | 0.655 | 0.645 | 0.682 | 0.673 | 0..676 |'
- en: '| OPT-6.7B | 6.7 | 0.659 | 0.639 | 0.687 | 0.673 | 0.678 |'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-6.7B | 6.7 | 0.659 | 0.639 | 0.687 | 0.673 | 0.678 |'
- en: '| OPT-13B | 13.0 | 0.654 | 0.646 | 0.684 | 0.676 | 0.679 |'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-13B | 13.0 | 0.654 | 0.646 | 0.684 | 0.676 | 0.679 |'
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.658 | 0.649 | 0.676 | 0.669 | 0.663 |'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM | BLOOM-1.1B | 1.1 | 0.658 | 0.649 | 0.676 | 0.669 | 0.663 |'
- en: '| BLOOM-3B | 3.0 | 0.658 | 0.651 | 0.670 | 0.670 | 0.673 |'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-3B | 3.0 | 0.658 | 0.651 | 0.670 | 0.670 | 0.673 |'
- en: '| BLOOM-7B | 7.0 | 0.658 | 0.655 | 0.678 | 0.671 | 0.679 |'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-7B | 7.0 | 0.658 | 0.655 | 0.678 | 0.671 | 0.679 |'
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.671 | 0.651 | 0.686 | 0.691 | 0.689 |'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama 2 | Llama2-7B | 7.0 | 0.671 | 0.651 | 0.686 | 0.691 | 0.689 |'
- en: '| Llama2-13B | 13.0 | 0.668 | 0.66 | 0.69 | 0.692 | 0.691 |'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-13B | 13.0 | 0.668 | 0.66 | 0.69 | 0.692 | 0.691 |'
- en: 'Table 5: Assessment of Informativeness across 12 LLMs from five families (BERT,
    T5, BLOOM, OPT, and Llama 2) using the MoverScore metric on five D2T datasets.
    The highest-performing model sizes within each family are highlighted in bold.
    Consistent with the BertScore metric, MoverScore generally increases with model
    size, reflecting improved informativeness. All results are statistically significant
    at $p<0.05$.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 5：使用 MoverScore 指标对来自五个家族（BERT、T5、BLOOM、OPT 和 Llama 2）的 12 个 LLM 进行的信息量评估。在每个家族中，表现最佳的模型大小用粗体字突出显示。与
    BertScore 指标一致，MoverScore 通常随着模型大小增加而提高，反映了信息量的改善。所有结果在 $p<0.05$ 时具有统计显著性。
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parent. Compared to BertScore and MoverScore, Parent (Dhingra et al., [2019](#bib.bib18))
    assesses the generated text using both reference and source texts. It is particularly
    valuable in scenarios with source-reference divergence and to verify if the generated
    text aligns with the source data. Parent correlates strongly with human judgments,
    as it considers both reference and source data. It calculates an F1-measure based
    on recall and precision, computed using lexical entailment techniques such as
    word overlap and co-occurrence. Let’s assume that the lexical entailment of $g_{i}$,
    i.e., $g_{i}\in\text{n-gram}(g)$ and ${E}_{n}(g_{i},r)$) is defined as follows:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Parent。与 BertScore 和 MoverScore 相比，Parent (Dhingra et al., [2019](#bib.bib18))
    通过参考文本和源文本来评估生成的文本。在源文本和参考文本存在偏差的情况下特别有价值，并且可以验证生成的文本是否与源数据一致。Parent 与人类评判高度相关，因为它同时考虑了参考数据和源数据。它基于回忆和精度计算
    F1 测量，采用词汇蕴含技术，如词重叠和共现。假设 $g_{i}$ 的词汇蕴含，即 $g_{i}\in\text{n-gram}(g)$ 和 ${E}_{n}(g_{i},r)$)
    定义如下：
- en: '|  | $\displaystyle\text{Precision}_{{E}_{n}}(g,s,r)\propto\ \frac{\sum\limits_{g_{i}\in\text{n-gram}(g)}{E}_{n}(g_{i},r)+(1-{E}_{n}(g_{i},r)){E}_{n}(g_{i},s)}{&#124;g_{i}\in\text{n-gram}(g)&#124;}$
    |  |'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Precision}_{{E}_{n}}(g,s,r)\propto\ \frac{\sum\limits_{g_{i}\in\text{n-gram}(g)}{E}_{n}(g_{i},r)+(1-{E}_{n}(g_{i},r)){E}_{n}(g_{i},s)}{&#124;g_{i}\in\text{n-gram}(g)&#124;}$
    |  |'
- en: 'Likewise, the entailed recall of $g$ is calculated as follows:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，$g$ 的蕴含召回率计算如下：
- en: '|  | $\displaystyle\text{Recall}_{{E}_{n}}(g,s,r)=\text{Recall}_{{E}_{n}}(g,s)^{\lambda}\cdot\text{Recall}_{{E}_{n}}(g,r)^{1-\lambda}$
    |  |'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Recall}_{{E}_{n}}(g,s,r)=\text{Recall}_{{E}_{n}}(g,s)^{\lambda}\cdot\text{Recall}_{{E}_{n}}(g,r)^{1-\lambda}$
    |  |'
- en: where $\lambda$) often is in semi-structured data. Finally, Parent is measure
    through F1-measure of $\text{Precision}_{{E}_{n}}$.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 通常存在于半结构化数据中。最后，通过 $\text{Precision}_{{E}_{n}}$ 的 F1 测量来评估 Parent。
- en: '| family | model |'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 家族 | 模型 |'
- en: '&#124; size (number of &#124;'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 大小（数量的 &#124;'
- en: '&#124; parameters in billion) &#124;'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 参数（十亿个）&#124;'
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
- en: '| BART | BART-base | 0.1 | 0.613 | 0.420 | 0.526 | 0.538 | 0.567 |'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART | BART-base | 0.1 | 0.613 | 0.420 | 0.526 | 0.538 | 0.567 |'
- en: '| BART-large | 0.4 | 0.600 | 0.413 | 0.514 | 0.563 | 0.586 |'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART-large | 0.4 | 0.600 | 0.413 | 0.514 | 0.563 | 0.586 |'
- en: '| T5 | T5-base | 0.2 | 0.599 | 0.430 | 0.533 | 0.614 | 0.618 |'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5 | T5-base | 0.2 | 0.599 | 0.430 | 0.533 | 0.614 | 0.618 |'
- en: '| T5-large | 0.738 | 0.603 | 0.436 | 0.547 | 0.622 | 0.644 |'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5-large | 0.738 | 0.603 | 0.436 | 0.547 | 0.622 | 0.644 |'
- en: '| OPT | OPT-2.7B | 2.7 | 0.528 | 0.395 | 0.510 | 0.557 | 0.589 |'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT | OPT-2.7B | 2.7 | 0.528 | 0.395 | 0.510 | 0.557 | 0.589 |'
- en: '| OPT-6.7B | 6.7 | 0.549 | 0.401 | 0.507 | 0.559 | 0.600 |'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-6.7B | 6.7 | 0.549 | 0.401 | 0.507 | 0.559 | 0.600 |'
- en: '| OPT-13B | 13.0 | 0.529 | 0.399 | 0.499 | 0.565 | 0.612 |'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-13B | 13.0 | 0.529 | 0.399 | 0.499 | 0.565 | 0.612 |'
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.565 | 0.405 | 0.502 | 0.561 | 0.566 |'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM | BLOOM-1.1B | 1.1 | 0.565 | 0.405 | 0.502 | 0.561 | 0.566 |'
- en: '| BLOOM-3B | 3.0 | 0.569 | 0.400 | 0.476 | 0.556 | 0.585 |'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-3B | 3.0 | 0.569 | 0.400 | 0.476 | 0.556 | 0.585 |'
- en: '| BLOOM-7B | 7.0 | 0.565 | 0.404 | 0.504 | 0.557 | 0.594 |'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-7B | 7.0 | 0.565 | 0.404 | 0.504 | 0.557 | 0.594 |'
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.602 | 0.390 | 0.531 | 0.621 | 0.621 |'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama 2 | Llama2-7B | 7.0 | 0.602 | 0.390 | 0.531 | 0.621 | 0.621 |'
- en: '| Llama2-13B | 13.0 | 0.599 | 0.418 | 0.531 | 0.622 | 0.640 |'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-13B | 13.0 | 0.599 | 0.418 | 0.531 | 0.622 | 0.640 |'
- en: 'Table 6: Assessment of informativeness across 12 LLMs from five LLM families
    on all five D2T datasets, using the Parent metric. The best-performing model sizes
    within each family are highlighted in bold. Unlike BertScore and MoverScore, Parent
    shows a mixed response, with inconsistent behavior observed in some LLM families
    such as BART, OPT, and BLOOM. However, in most cases, Parent exhibits a positive
    correlation with increasing model size. All results are statistically significant
    at $p<0.05$.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 6：使用 Parent 指标对来自五个 LLM 家族的 12 个 LLM 在所有五个 D2T 数据集上的信息量进行评估。每个家族中表现最佳的模型尺寸以**粗体**标出。与
    BertScore 和 MoverScore 不同，Parent 的表现呈现出混合反应，某些 LLM 家族如 BART、OPT 和 BLOOM 的行为不一致。然而，在大多数情况下，Parent
    显示出与模型尺寸增加的正相关关系。所有结果在统计上具有显著性，$p<0.05$。
- en: Takeaways.
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结论。
- en: 'Except some negligible exceptions, the empirical findings from Tables [4](#S6.T4
    "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation"), [5](#S6.T5 "Table 5 ‣ 2nd item ‣ 6.2 Informativeness
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), and
    [6](#S6.T6 "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") consistently validate the relationship
    between model parameters and the informativeness of D2T models. As the model parameters
    increase, there is a clear and noticeable improvement in the LLM’s informativeness.
    With this comprehensive understanding, we are now ready to address informativeness
    part the first question posed in Section [2](#S2 "2 Research Questions and Motivations
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation").'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一些微不足道的例外，从表 [4](#S6.T4 "表 4 ‣ 第 1 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型尺寸对数据到文本生成中的微调
    LLM 性能的影响：前沿调查")、[5](#S6.T5 "表 5 ‣ 第 2 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型尺寸对数据到文本生成中的微调
    LLM 性能的影响：前沿调查") 和 [6](#S6.T6 "表 6 ‣ 第 3 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型尺寸对数据到文本生成中的微调
    LLM 性能的影响：前沿调查") 的实证结果一致验证了模型参数与 D2T 模型信息量之间的关系。随着模型参数的增加，LLM 的信息量有了明显的改进。通过这一全面的理解，我们现在可以着手回答第
    [2](#S2 "2 研究问题与动机 ‣ 模型尺寸对数据到文本生成中的微调 LLM 性能的影响：前沿调查") 节中提出的第一个问题。
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability,
    informativeness, and faithfulness?'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：在一个微调的 LLM 家族中，模型尺寸对数据到文本（D2T）任务的性能有哪些影响，包括可读性、信息量和忠实度？
- en: 'Answer: Both Table [4](#S6.T4 "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6
    Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation") and Table [5](#S6.T5
    "Table 5 ‣ 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") present compelling evidence supporting the
    notion that increasing model size enhances the informativeness of LLMs in the
    same family, particularly concerning the alignment between reference and generated
    text. Across all three primary D2T tasks, a consistent improvement in informativeness
    is observed with the size of LLMs. While isolated cases of decreased informativeness
    with increasing LLM parameters are noted, such as with BART family (in the case
    of the WikiTableText dataset using both BertScore and MoverScore), and Llama 2
    family (in the case of the E2E dataset using MoverScore), these instances exhibit
    minimal degradation. Consequently, they do not significantly challenge our conclusion
    that increasing number of parameters enhances the informativeness of LLMs inside
    of a LLM family for D2T tasks. Furthermore, Table [6](#S6.T6 "Table 6 ‣ 3rd item
    ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") also indicates that increasing size often leads to enhanced informativeness
    between source and generated text. However, compared to BertScore and MoverScore,
    discrepancies are more pronounced. Some LLMs from BART, OPT, and BLOOM families
    exhibit instances where increasing parameters may degrade Parent scores (see Table [6](#S6.T6
    "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")). Therefore, it is apparent that enlarging
    model size may occasionally diminish the informativeness inside of a LLM family
    when informativeness evaluated between source and generated text.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：表格 [4](#S6.T4 "表格 4 ‣ 第 1 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型规模对微调 LLM 在数据到文本生成中的表现的影响：最先进的调查")
    和表格 [5](#S6.T5 "表格 5 ‣ 第 2 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型规模对微调 LLM 在数据到文本生成中的表现的影响：最先进的调查")
    提供了有力证据，支持增加模型规模可以增强同一家族 LLM 的信息量，尤其是参考文本和生成文本之间的一致性。在所有三个主要的数据到文本任务中，随着 LLM 规模的增加，信息量表现出一致的提升。虽然也有个别情况下随着
    LLM 参数的增加信息量有所下降，例如 BART 家族（在使用 BertScore 和 MoverScore 的 WikiTableText 数据集中）和
    Llama 2 家族（在使用 MoverScore 的 E2E 数据集中），但这些情况表现出的降级程度较小。因此，这些情况并没有显著挑战我们关于增加参数数量可以提升同一家族
    LLM 在数据到文本任务中的信息量的结论。此外，表格 [6](#S6.T6 "表格 6 ‣ 第 3 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型规模对微调
    LLM 在数据到文本生成中的表现的影响：最先进的调查") 也表明，增加规模通常会导致源文本和生成文本之间的信息量增强。然而，与 BertScore 和 MoverScore
    相比，差异更为明显。一些来自 BART、OPT 和 BLOOM 家族的 LLM 展现了增加参数可能会降低 Parent 分数的情况（见表格 [6](#S6.T6
    "表格 6 ‣ 第 3 项 ‣ 6.2 信息量 ‣ 6 性能评估与结果 ‣ 模型规模对微调 LLM 在数据到文本生成中的表现的影响：最先进的调查")）。因此，显然增大模型规模可能偶尔会减少
    LLM 家族内部在源文本和生成文本之间的可信息量。
- en: 6.3 Faithfulness
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 可信度
- en: 'Faithfulness is a crucial aspect of D2T model (Wang et al., [2020](#bib.bib109);
    Ji et al., [2023](#bib.bib39)), primarily assessing factual consistency or accuracy
    of the generated text compared to the provided source data. It can be characterized
    by two cases: i) the model generating incorrect facts compared to the given source
    data, and ii) the model generating irrelevant facts that do not appear in the
    source data. Figure [7](#S6.F7 "Figure 7 ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations
    and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation") illustrates an instance of unfaithful
    generation on the E2E dataset. To quantify faithfulness of a D2T model, we employ
    the BartScore metric, which utilizes the likelihood of a BART model for estimating
    faithfulness.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度是D2T模型（Wang等人，[2020](#bib.bib109); Ji等人，[2023](#bib.bib39)）的一个关键方面，主要评估生成文本相对于提供的源数据的事实一致性或准确性。它可以通过两种情况来描述：i)
    模型生成的事实与给定源数据不符，ii) 模型生成的事实与源数据无关。图[7](#S6.F7 "图7 ‣ 6.3 可信度 ‣ 6 性能评估和结果 ‣ 模型规模对数据到文本生成中的微调LLM性能的影响：前沿调查")展示了E2E数据集上的一个不忠实生成实例。为了量化D2T模型的可信度，我们采用了BartScore指标，该指标利用BART模型的可能性来估计可信度。
- en: '![Refer to caption](img/e28e3921a6932cdd6081f5592baca82e.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e28e3921a6932cdd6081f5592baca82e.png)'
- en: 'Figure 7: An illustration of unfaithful generation on the E2E dataset. The
    generated text contains the phrase “Highly price”, which is irrelevant with respect
    to the information provided in the source data.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：E2E数据集上不忠实生成的示例。生成的文本包含短语“Highly price”，这与源数据提供的信息无关。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BartScore. BartScore (Yuan et al., [2021](#bib.bib118)) evaluates the faithfulness
    between two texts by estimating the likelihood of text generation based on a sequence-to-sequence
    model like BART. Its premise is that if two pieces of text are factually equivalent,
    the likelihood of generating one from the other will be higher. Faithfulness in
    BartScore is determined by the probability (equation [6](#S6.E6 "In 1st item ‣
    6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"))
    of transforming from the source text ($s$).'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BartScore。BartScore（Yuan等人，[2021](#bib.bib118)）通过估计基于序列到序列模型（如BART）的文本生成可能性来评估两个文本之间的可信度。其前提是，如果两段文本在事实上一致，那么从其中生成另一段文本的可能性将更高。在BartScore中，可信度由从源文本（$s$）转化的概率（公式[6](#S6.E6
    "在第1项 ‣ 6.3 可信度 ‣ 6 性能评估和结果 ‣ 模型规模对数据到文本生成中的微调LLM性能的影响：前沿调查")）决定。
- en: '|  | $\displaystyle\text{Faithfulness}(s,g)$ |  |'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Faithfulness}(s,g)$ |  |'
- en: '|  |  | $1$2 |  | (6) |'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (6) |'
- en: BartScore has exhibited superior performance compared to several earlier evaluation
    metrics. To calculate BartScore, we employ a pre-trained BARTScore model trained
    on the PARABANK dataset (Yuan et al., [2021](#bib.bib118)). It’s essential to
    note that when dealing with source ($s$ through text.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BartScore相比于早期的几个评估指标展示了更卓越的性能。为了计算BartScore，我们使用了在PARABANK数据集上训练的预训练BARTScore模型（Yuan等人，[2021](#bib.bib118)）。需要注意的是，在处理源文本时（$s$）。
- en: '| family | model |'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| family | model |'
- en: '&#124; size (number of &#124;'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 大小（参数数量）&#124;'
- en: '&#124; parameters in billion) &#124;'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 参数（十亿）&#124;'
- en: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| E2E | ViGGo | WikiTableText | DART | WebNLG |'
- en: '| BART | BART-base | 0.1 | 0.088 | 0.047 | 0.066 | 0.061 | 0.097 |'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART | BART-base | 0.1 | 0.088 | 0.047 | 0.066 | 0.061 | 0.097 |'
- en: '| BART-large | 0.4 | 0.092 | 0.046 | 0.066 | 0.065 | 0.094 |'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BART-large | 0.4 | 0.092 | 0.046 | 0.066 | 0.065 | 0.094 |'
- en: '| T5 | T5-base | 0.2 | 0.091 | 0.052 | 0.072 | 0.067 | 0.093 |'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5 | T5-base | 0.2 | 0.091 | 0.052 | 0.072 | 0.067 | 0.093 |'
- en: '| T5-large | 0.7 | 0.093 | 0.048 | 0.072 | 0.068 | 0.095 |'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| T5-large | 0.7 | 0.093 | 0.048 | 0.072 | 0.068 | 0.095 |'
- en: '| OPT | OPT-2.7B | 2.7 | 0.057 | 0.045 | 0.068 | 0.063 | 0.089 |'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT | OPT-2.7B | 2.7 | 0.057 | 0.045 | 0.068 | 0.063 | 0.089 |'
- en: '| OPT-6.7B | 6.7 | 0.053 | 0.046 | 0.066 | 0.062 | 0.087 |'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-6.7B | 6.7 | 0.053 | 0.046 | 0.066 | 0.062 | 0.087 |'
- en: '| OPT-13B | 13.0 | 0.050 | 0.043 | 0.065 | 0.063 | 0.087 |'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| OPT-13B | 13.0 | 0.050 | 0.043 | 0.065 | 0.063 | 0.087 |'
- en: '| BLOOM | BLOOM-1.1B | 1.1 | 0.089 | 0.042 | 0.057 | 0.064 | 0.090 |'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM | BLOOM-1.1B | 1.1 | 0.089 | 0.042 | 0.057 | 0.064 | 0.090 |'
- en: '| BLOOM-3B | 3.0 | 0.083 | 0.041 | 0.054 | 0.064 | 0.089 |'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-3B | 3.0 | 0.083 | 0.041 | 0.054 | 0.064 | 0.089 |'
- en: '| BLOOM-7B | 7.0 | 0.086 | 0.040 | 0.056 | 0.063 | 0.088 |'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| BLOOM-7B | 7.0 | 0.086 | 0.040 | 0.056 | 0.063 | 0.088 |'
- en: '| Llama 2 | Llama2-7B | 7.0 | 0.094 | 0.05 | 0.062 | 0.073 | 0.106 |'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama 2 | Llama2-7B | 7.0 | 0.094 | 0.05 | 0.062 | 0.073 | 0.106 |'
- en: '| Llama2-13B | 13.0 | 0.089 | 0.048 | 0.061 | 0.074 | 0.105 |'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-13B | 13.0 | 0.089 | 0.048 | 0.061 | 0.074 | 0.105 |'
- en: 'Table 7: Assessment of faithfulness across 12 LLMs from five families (BERT,
    T5, BLOOM, OPT, and Llama 2) on the five D2T datasets, using probabilities from
    the BartScore metric (noting that some studies report BartScore using logits).
    The best-performing model sizes within each family are highlighted in bold. Unlike
    readability and informativeness, BartScore shows an inverse trend for faithfulness.
    In most LLM families, increasing model size correlates with a decrease in BartScore,
    indicating reduced faithfulness in larger models. All results are statistically
    significant at $p<0.05$.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表7：评估来自五个系列（BERT、T5、BLOOM、OPT 和 Llama 2）的12个LLM在五个D2T数据集上的真实性，使用的是BartScore指标的概率（注意，某些研究报告使用logits计算BartScore）。每个系列中表现最佳的模型大小以粗体字突出显示。与可读性和信息性不同，BartScore在真实性上表现出相反的趋势。在大多数LLM系列中，增加模型大小与BartScore下降相关，表明更大的模型在真实性方面有所减少。所有结果在统计上都具有显著性，$p<0.05$。
- en: Takeaways.
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结论。
- en: 'Upon scrutinizing the empirical findings presented in Table [7](#S6.T7 "Table
    7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), we are now equipped to address faithfulness part of the first
    question posed in Section [2](#S2 "2 Research Questions and Motivations ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation") concerning the faithfulness of D2T models.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细分析表[7](#S6.T7 "表 7 ‣ 1 项 ‣ 6.3 真实性 ‣ 6 性能评估与结果 ‣ 模型大小对微调LLM在数据到文本生成中的表现影响：前沿研究")中呈现的实证结果，我们现在可以回答第[2](#S2
    "2 研究问题与动机 ‣ 模型大小对微调LLM在数据到文本生成中的表现影响：前沿研究")节中提出的第一个问题，关于D2T模型的真实性。
- en: 'Question: What are the impacts of model size within a family of fine-tuned
    LLMs on the performance of data-to-text (D2T) tasks, in terms of the readability,
    informativeness, and faithfulness?'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：在微调的语言模型（LLM）系列中，模型大小对数据到文本（D2T）任务的表现有何影响，具体体现在可读性、信息性和真实性方面？
- en: 'Answer: No, an increase in the number of parameters (i.e., model size) will
    not necessarily improve the faithfulness of LLMs inside of a LLM family for D2T
    tasks. In fact, increasing parameters can degrade the performance of LLMs in terms
    of faithfulness. Table [7](#S6.T7 "Table 7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation") clearly suggests that
    for LLMs from BLOOM, OPT, and Llama 2 families, increasing their parameters adversely
    affects their faithfulness aspect for all D2T datasets. Although we have seen
    a very small number of discrepancies (five cases with a very low margin) in T5
    and BART families (for the E2E and DART datasets) and Llama 2 family (for the
    DART dataset), they still indicate that faithfulness often degrades with increasing
    parameters on other datasets. Hence, we can certainly claim that increasing the
    number of parameters will not be helpful for enhancing the faithfulness of LLMs
    toward D2T.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：不，参数数量的增加（即模型大小）不一定会提高LLM在D2T任务中的真实性。事实上，增加参数可能会降低LLM在真实性方面的表现。表[7](#S6.T7
    "表 7 ‣ 1 项 ‣ 6.3 真实性 ‣ 6 性能评估与结果 ‣ 模型大小对微调LLM在数据到文本生成中的表现影响：前沿研究")明确表明，对于BLOOM、OPT和Llama
    2系列的LLM，增加参数会对所有D2T数据集的真实性产生不利影响。虽然我们在T5和BART系列（对于E2E和DART数据集）以及Llama 2系列（对于DART数据集）中发现了少量差异（五个案例，差异非常小），但它们仍然表明在其他数据集中，真实性往往随着参数的增加而下降。因此，我们可以明确声称，增加参数数量对提高LLM在D2T任务中的真实性没有帮助。
- en: 6.4 Comparative Performance Analysis across LLM Families
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 各LLM系列的比较性能分析
- en: 'Thus far, our focus has been primarily on analyzing the performance of LLMs
    within individual LLM families. Now, our attention shifts towards the comparative
    performance analysis of LLMs across different LLM families. Considering all six
    metrics’ tables, Table  [2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"),  [3](#S6.T3 "Table
    3 ‣ 2nd item ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"),  [4](#S6.T4 "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"),  [5](#S6.T5 "Table
    5 ‣ 2nd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"),  [6](#S6.T6 "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance
    Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM Performance in
    Data-to-Text Generation: A State-of-the-Art Investigation"), and  [7](#S6.T7 "Table
    7 ‣ 1st item ‣ 6.3 Faithfulness ‣ 6 Performance Evaluations and Results ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), we can effectively address the second question raised in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止，我们的重点主要集中在分析单个 LLM 家族的性能。现在，我们的注意力转向了对不同 LLM 家族之间的比较性能分析。考虑到所有六个指标的表格，即表格
    [2](#S6.T2 "Table 2 ‣ 6.1 Readability ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")、[3](#S6.T3 "Table 3 ‣ 2nd item ‣ 6.1 Readability
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")、[4](#S6.T4
    "Table 4 ‣ 1st item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")、[5](#S6.T5 "Table 5 ‣ 2nd item ‣ 6.2 Informativeness
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")、[6](#S6.T6
    "Table 6 ‣ 3rd item ‣ 6.2 Informativeness ‣ 6 Performance Evaluations and Results
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") 和 [7](#S6.T7 "Table 7 ‣ 1st item ‣ 6.3 Faithfulness
    ‣ 6 Performance Evaluations and Results ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation")，我们可以有效地解答第
    [2](#S2 "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")
    节提出的第二个问题。'
- en: 'Question: Do larger LLM families (such as OPT, BLOOM, Llama 2, etc.) convincingly
    outperform smaller LLM families (such as BART, T5, etc.) in terms of D2T task
    performance?'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：较大的 LLM 家族（例如 OPT、BLOOM、Llama 2 等）在 D2T 任务性能方面是否显著优于较小的 LLM 家族（如 BART、T5
    等）？
- en: 'Answer: From the results of all six automatic metrics, there is no significant
    evidence to conclusively state that larger LLM families (from BLOOM, OPT, and
    Llama 2 families) outperform smaller LLM families (from BART and T5). While Llama
    2 family shows significant improvement over T5 and BART families in terms of readability
    and informativeness, it falls short compared to T5-large model of T5 family in
    terms of faithfulness. On the other hand, both OPTs and BLOOMs families perform
    lower than the T5 family across all three quality aspects. Consequently, it can
    be inferred that LLM from smaller model sized family, such as T5-large model,
    can surprisingly perform quite well in D2T tasks. Moreover, their main advantage
    lies in their lower computational cost compared to larger LLM families like Llama
    2. Especially for faithful D2T tasks (Wang et al., [2020](#bib.bib109); Ji et al.,
    [2023](#bib.bib39)), it is preferable to utilize such smaller LLM families.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案：根据所有六个自动评估指标的结果，没有显著证据可以明确表示较大的 LLM 家族（来自 BLOOM、OPT 和 Llama 2 家族）在性能上优于较小的
    LLM 家族（来自 BART 和 T5）。虽然 Llama 2 家族在可读性和信息性方面相比 T5 和 BART 家族表现出显著提升，但在忠实度方面却不如
    T5 家族的 T5-large 模型。另一方面，OPT 和 BLOOM 家族在所有三个质量方面的表现都低于 T5 家族。因此，可以推断出较小模型家族的 LLM，如
    T5-large 模型，在 D2T 任务中表现出意外的优越性。此外，相比于像 Llama 2 这样的大型 LLM 家族，它们的主要优势在于较低的计算成本。尤其是在忠实度要求高的
    D2T 任务中（Wang 等，[2020](#bib.bib109); Ji 等，[2023](#bib.bib39)），使用这种较小的 LLM 家族是更为理想的。
- en: 6.5 Discussion
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 讨论
- en: From the empirical results of all six automatic metrics on three important qualities
    (readability, informativeness, and faithfulness), we have seen that increasing
    the model parameters in LLMs inside of an LLM family can boost readability and
    informativeness, but it tends to reduce the faithfulness of the D2T task. Therefore,
    in scenarios where faithfulness is crucial, such as in safety-critical applications
    like medical report generation (Pauws et al., [2019](#bib.bib79); Hommes et al.,
    [2019](#bib.bib36); Nishino et al., [2020](#bib.bib74)), it is advisable to use
    a LLM from smaller model-sized LLM family in D2T applications. However, in cases
    where readability (fluency and coherence) and informativeness are paramount, increasing
    parameters is a viable approach.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从对三种重要特性（可读性、信息量和忠实度）的六种自动度量的经验结果来看，我们发现增加LLM系列中的模型参数可以提高可读性和信息量，但往往会降低D2T任务的忠实度。因此，在忠实度至关重要的场景中，如医学报告生成（Pauws等，[2019](#bib.bib79)；Hommes等，[2019](#bib.bib36)；Nishino等，[2020](#bib.bib74)）等安全关键应用中，建议在D2T应用中使用较小模型规模的LLM。然而，在可读性（流畅性和连贯性）和信息量至关重要的情况下，增加参数是一种可行的方法。
- en: 7 Analyzing the Effect of Source-Reference Divergence
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 分析源-参考偏差的影响
- en: 'As discussed in Subsection [4.4](#S4.SS4 "4.4 Source-Reference Divergence ‣
    4 Preliminaries ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"), source-reference divergence is
    a common phenomenon in D2T tasks and is unavoidable in D2T datasets (Dhingra et al.,
    [2019](#bib.bib18); Wiseman et al., [2017](#bib.bib111); ul Islam et al., [2023](#bib.bib105)).
    In this analysis, we examine two aspects: how LLMs perform in D2T tasks when source-reference
    divergence is present, and whether the number of parameters in LLMs affects their
    performance in the context of such source-reference divergence. For this analysis,
    we consider three LLM families—T5, BLOOM, and Llama 2—along with six LLMs: T5-base,
    T5-large, BLOOM-3B, BLOOM-7B, Llama2-7B, and Llama2-13B. Additionally, we incorporate
    all three types of D2T tasks, using one dataset for each type: E2E for MR-to-Text,
    WikiTableText for Table-to-Text, and DART for Graph-to-Text. We evaluate the performance
    of these models based on the three important qualities of D2T: readability (measured
    by Bleu), informativeness (measured by MoverScore and Parent), and faithfulness
    (measured by BartScore). To provide a clearer illustration of the impact of source-reference
    divergence, we categorize the test partitions of each dataset into three groups
    based on their average source-reference divergence: low, medium, and high. The
    low group consists of instances with low source-reference divergences, while the
    high group comprises instances with high source-reference divergences. Source-reference
    divergence ($\text{div}(s,r)$) at the unigram level, as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '如[4.4](#S4.SS4 "4.4 Source-Reference Divergence ‣ 4 Preliminaries ‣ Impact
    of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation")小节所讨论，源-参考偏差是D2T任务中的常见现象，在D2T数据集中是不可避免的（Dhingra等，[2019](#bib.bib18)；Wiseman等，[2017](#bib.bib111)；ul
    Islam等，[2023](#bib.bib105)）。在本分析中，我们研究了两个方面：LLMs在源-参考偏差存在的情况下在D2T任务中的表现，以及LLMs的参数数量是否影响其在这种源-参考偏差背景下的表现。我们考虑了三个LLM系列——T5、BLOOM和Llama
    2——以及六个LLM：T5-base、T5-large、BLOOM-3B、BLOOM-7B、Llama2-7B和Llama2-13B。此外，我们结合了所有三种D2T任务，每种任务使用一个数据集：E2E用于MR到文本，WikiTableText用于表格到文本，DART用于图表到文本。我们根据D2T的三个重要特性评估这些模型的性能：可读性（通过Bleu测量）、信息量（通过MoverScore和Parent测量）和忠实度（通过BartScore测量）。为了更清楚地展示源-参考偏差的影响，我们将每个数据集的测试分区根据其平均源-参考偏差分为低、中、高三组。低组包含源-参考偏差低的实例，高组则包含源-参考偏差高的实例。源-参考偏差（$\text{div}(s,r)$）在unigram层级的计算如下：'
- en: '|  | $\displaystyle\text{div}(s,r)=1-\frac{\big{&#124;}\text{unigram}(s)\cap\text{unigram}(r)\big{&#124;}}{\big{&#124;}\text{unigram}(s)\cup\text{unigram}(r)\big{&#124;}}$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{div}(s,r)=1-\frac{\big{&#124;}\text{unigram}(s)\cap\text{unigram}(r)\big{&#124;}}{\big{&#124;}\text{unigram}(s)\cup\text{unigram}(r)\big{&#124;}}$
    |  |'
- en: Where $|x|$ often consists of semi-structured non-textual data, where higher-order
    grams may overlook significant portions of information.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在$|x|$通常由半结构化的非文本数据组成的情况下，更高阶的n-gram可能会忽略大量信息。
- en: '![Refer to caption](img/923c614c190eb0aefdaf32f768e72020.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/923c614c190eb0aefdaf32f768e72020.png)'
- en: 'Figure 8: Analysis of performance of T5 in context of source reference divergence
    based on three datasets (E2E, WikiTableText and DART) in terms of Bleu, Parent,
    BartScore and MoverScore. First row represents performance of T5-base, and second
    row represents performance of T5-large.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：基于三个数据集（E2E、WikiTableText 和 DART）分析T5在源参考偏差背景下的表现，包括Bleu、Parent、BartScore
    和 MoverScore。第一行表示T5-base的表现，第二行表示T5-large的表现。
- en: 7.1 Influence of Source-Reference Divergence on LLM Performance for D2T
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 源参考偏差对LLM在D2T任务中表现的影响
- en: 'In this part of analysis, we investigated the performance of LLMs under source-reference
    divergence. Figures [8](#S7.F8 "Figure 8 ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"),  [8](#S7.F8 "Figure 8 ‣ 7 Analyzing
    the Effect of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
    and [10](#S7.F10 "Figure 10 ‣ 7.1 Influence of Source-Reference Divergence on
    LLM Performance for D2T ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") illustrate the performance of T5, BLOOM, and
    Llama 2 across three D2T datasets in the context of source-reference divergence.
    These figures provide insights to address the first part of the third question
    raised in Section [2](#S2 "2 Research Questions and Motivations ‣ Impact of Model
    Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation").'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分分析中，我们探讨了LLM在源参考偏差下的表现。图[8](#S7.F8 "图8 ‣ 7 分析源参考偏差的影响 ‣ 模型规模对数据到文本生成中微调LLM表现的影响：前沿调查")、图[8](#S7.F8
    "图8 ‣ 7 分析源参考偏差的影响 ‣ 模型规模对数据到文本生成中微调LLM表现的影响：前沿调查")和图[10](#S7.F10 "图10 ‣ 7.1 源参考偏差对LLM在D2T任务中表现的影响
    ‣ 7 分析源参考偏差的影响 ‣ 模型规模对数据到文本生成中微调LLM表现的影响：前沿调查")展示了T5、BLOOM和Llama 2在源参考偏差背景下在三个D2T数据集中的表现。这些图提供了针对第[2](#S2
    "2 研究问题和动机 ‣ 模型规模对数据到文本生成中微调LLM表现的影响：前沿调查")节中提出的第三个问题的第一部分的见解。
- en: '![Refer to caption](img/eebb319f82b94ba54a9fcf63ad1eb3f5.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eebb319f82b94ba54a9fcf63ad1eb3f5.png)'
- en: 'Figure 9: Analysis of performance of BLOOM in context of source reference divergence
    based on three datasets (E2E, WikiTableText and DART) in terms of Bleu, Parent,
    BartScore and MoverScore. First row represents performance of BLOOM-3B, and second
    row represents performance of BLOOM-7B.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：基于三个数据集（E2E、WikiTableText 和 DART）分析BLOOM在源参考偏差背景下的表现，包括Bleu、Parent、BartScore
    和 MoverScore。第一行表示BLOOM-3B的表现，第二行表示BLOOM-7B的表现。
- en: 'Question: Does the presence of source-reference divergence influence the performance
    of LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：源参考偏差的存在是否影响了LLM在D2T任务中的表现？如果是，增加LLM的模型规模是否有助于缓解源参考偏差的影响？
- en: 'Answer: The analysis of Figures [8](#S7.F8 "Figure 8 ‣ 7 Analyzing the Effect
    of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation"),  [9](#S7.F9 "Figure
    9 ‣ 7.1 Influence of Source-Reference Divergence on LLM Performance for D2T ‣
    7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size on
    Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
    and  [10](#S7.F10 "Figure 10 ‣ 7.1 Influence of Source-Reference Divergence on
    LLM Performance for D2T ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") reveals a significant influence of source-reference
    divergence on the performance of LLMs across all three LLM families. Across the
    low, medium, and high divergence groups, distinct performance trends of LLMs emerge.
    In the low divergence group, characterized by minimal source-reference discrepancies,
    all LLM models consistently exhibit higher scores across the four evaluated metrics
    (Bleu, Parent, MoverScore, and BartScore). This suggests that when the source-reference
    divergence is low, LLMs tend to perform better in terms of readability, informativeness,
    and faithfulness. Conversely, in the high divergence group where source-reference
    discrepancies are more pronounced, all LLMs demonstrate notably lower performance
    across the metrics compared to the low and medium groups. This observation underscores
    the significant impact of source-reference divergence on LLM performance, with
    higher levels of divergence leading to decreased performance across the evaluated
    qualities. In summary, these findings confirm that LLM performance is indeed influenced
    by source-reference divergence, with lower levels of divergence consistently associated
    with superior performance across the evaluated metrics. So, despite the claims
    of LLMs to possess greater generalization and knowledge, their performance in
    D2T tasks remains inadequate when source-reference divergence is present.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答：图 [8](#S7.F8 "图 8 ‣ 7 源引用差异的影响 ‣ 模型规模对数据到文本生成中的微调 LLM 性能的影响：最先进的调查")、[9](#S7.F9
    "图 9 ‣ 7.1 源引用差异对 D2T 的 LLM 性能的影响 ‣ 7 源引用差异的分析 ‣ 模型规模对数据到文本生成中的微调 LLM 性能的影响：最先进的调查")
    和 [10](#S7.F10 "图 10 ‣ 7.1 源引用差异对 D2T 的 LLM 性能的影响 ‣ 7 源引用差异的分析 ‣ 模型规模对数据到文本生成中的微调
    LLM 性能的影响：最先进的调查") 显示源引用差异对所有三种 LLM 系列的性能有显著影响。在低、中、高差异组中，LLM 的表现趋势各不相同。在低差异组中，源引用差异最小，所有
    LLM 模型在四个评估指标（Bleu、Parent、MoverScore 和 BartScore）上均表现出较高的得分。这表明，当源引用差异较小时，LLM
    的可读性、信息量和忠实性表现较好。相反，在源引用差异更明显的高差异组中，所有 LLM 的指标得分显著低于低差异组和中等差异组。这一观察结果突显了源引用差异对
    LLM 性能的重大影响，高水平的差异导致评估指标的性能下降。总之，这些发现确认了 LLM 性能确实受到源引用差异的影响，较低的差异水平与各评估指标的优越性能一致。因此，尽管
    LLM 宣称具有更强的泛化能力和知识，但在存在源引用差异的 D2T 任务中的表现仍然不足。
- en: '![Refer to caption](img/b76824bec3ebeb3f679eb2630e63124c.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b76824bec3ebeb3f679eb2630e63124c.png)'
- en: 'Figure 10: Analysis of performance of Llama 2 in context of source reference
    divergence based on three datasets (E2E, WikiTableText and DART) in terms of Bleu,
    Parent, BartScore and MoverScore. First row represents performance of Llama2-7B,
    and second row represents performance of Llama2-13B.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于三种数据集（E2E、WikiTableText 和 DART），分析 Llama 2 在源引用差异背景下的表现，包括 Bleu、Parent、BartScore
    和 MoverScore。第一行表示 Llama2-7B 的表现，第二行表示 Llama2-13B 的表现。
- en: 7.2 Impact of model Size on LLM Performance for D2T in the Presence of Source-Reference
    Divergence
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 模型规模对 D2T 任务中 LLM 性能的影响，考虑到源引用差异
- en: 'In this section, we conduct an analysis to determine the influence of underlying
    parameter numbers (or simply size) on the performance of LLMs in the context of
    source-reference divergence. To facilitate this analysis, we follow a similar
    methodology as in our previous analysis, segmenting the entire test set into three
    groups (low, medium, and high) based on source-reference divergence. For comparison
    purposes, we juxtapose two LLMs in a single plot, specifically focusing on three
    comparisons: Llama2-7B vs T5-base, Llama2-13B vs T5-base, and BLOOM-7B vs T5-base.
    These comparisons enable us to juxtapose the performance of three larger LLMs
    (BLOOM-7B, Llama2-7B, and Llama2-13B) against that of a relatively smaller LLM,
    T5-base. We analyze these comparisons across four metrics (Bleu, Parent, MoverScore,
    and BartScore), encompassing all three key qualities of D2T models over the three
    D2T datasets—E2E, WikiTableText, and DART. Through these comparative analyses,
    we aim to address the final part of the third question raised in Section [2](#S2
    "2 Research Questions and Motivations ‣ Impact of Model Size on Fine-tuned LLM
    Performance in Data-to-Text Generation: A State-of-the-Art Investigation"), shedding
    light on the relationship between LLM parameter numbers and performance in the
    presence of source-reference divergence.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行分析以确定底层参数数量（或简称规模）在源参考分歧背景下对LLMs性能的影响。为了便于分析，我们采用与之前分析类似的方法，将整个测试集基于源参考分歧分成三个组（低、中、高）。为了比较，我们将两个LLMs并排展示，具体关注三组比较：Llama2-7B与T5-base，Llama2-13B与T5-base，以及BLOOM-7B与T5-base。这些比较使我们能够将三个较大的LLMs（BLOOM-7B、Llama2-7B和Llama2-13B）的性能与相对较小的LLM，T5-base的性能进行对比。我们分析这些比较的四个指标（Bleu、Parent、MoverScore和BartScore），涵盖了所有三个D2T模型在三个D2T数据集—E2E、WikiTableText和DART中的关键特质。通过这些比较分析，我们旨在解答第[2](#S2
    "2 研究问题与动机 ‣ 模型规模对数据到文本生成中微调LLM性能的影响：最前沿调查")节提出的第三个问题的最后部分，揭示LLM参数数量与在源参考分歧情况下的性能之间的关系。
- en: '![Refer to caption](img/40ce5e6dfbece3644f6b4d8fb5189327.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/40ce5e6dfbece3644f6b4d8fb5189327.png)'
- en: 'Figure 11: Comparative performance analysis between T5-base and BLOOM-7B in
    context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在源参考分歧的背景下，T5-base与BLOOM-7B在三个数据集（E2E、WikiTableText和DART）上的比较性能分析，包括Bleu、Parent、BartScore和MoverScore。
- en: 'Question: Does the presence of source-reference divergence influence the performance
    of LLMs for D2T tasks? If so, does increasing the model size of LLM aid in mitigating
    the effects of source-reference divergence?'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题：源参考分歧是否影响LLMs在D2T任务中的表现？如果是，增加LLM的模型规模是否有助于缓解源参考分歧的影响？
- en: '![Refer to caption](img/dd50c9e963133f82ddc8000e3c129c22.png)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见标题](img/dd50c9e963133f82ddc8000e3c129c22.png)'
- en: 'Figure 12: Comparative performance analysis between T5-base and Llama2-7B in
    context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12：在源参考分歧背景下，T5-base与Llama2-7B在三个数据集（E2E、WikiTableText和DART）上的比较性能分析，包括Bleu、Parent、BartScore和MoverScore。
- en: '![Refer to caption](img/a6b15937300b7b160a110a27bb13e5f0.png)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见标题](img/a6b15937300b7b160a110a27bb13e5f0.png)'
- en: 'Figure 13: Comparative performance analysis between T5-base and Llama2-13B
    in context of source reference divergence based on three datasets (E2E, WikiTableText
    and DART) in terms of Bleu, Parent, BartScore and MoverScore.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13：在源参考分歧背景下，T5-base与Llama2-13B在三个数据集（E2E、WikiTableText和DART）上的比较性能分析，包括Bleu、Parent、BartScore和MoverScore。
- en: 'Answer: From the comparison plots [11](#S7.F11 "Figure 11 ‣ 7.2 Impact of model
    Size on LLM Performance for D2T in the Presence of Source-Reference Divergence
    ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"),
     [12](#S7.F12 "Figure 12 ‣ 7.2 Impact of model Size on LLM Performance for D2T
    in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation"), and  [13](#S7.F13 "Figure 13 ‣
    7.2 Impact of model Size on LLM Performance for D2T in the Presence of Source-Reference
    Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"), it becomes apparent that LLMs with larger parameters (Llama2-13B,
    Llama2-7B, and BLOOM-7B) tend to outperform low-parameter models like T5-base
    in D2T tasks when source-reference divergence is low. However, there are some
    exceptions, particularly in comparisons with the BLOOM-7B and T5-base models.
    As the source-reference divergence rate rises (i.e., in the medium group of source-reference
    divergence), T5-base, being a low-parameter model, begins to perform well and
    competes with the larger LLMs. Finally, when the source-reference divergence becomes
    high, T5-base often either outperforms the larger LLMs (Llama2-13B, Llama2-7B,
    and BLOOM-7B) in almost all metrics or significantly narrows the performance gap
    with them. Although there are some exceptional cases where T5-base cannot outperform
    LLMs, this is primarily observed with the WikiTableText dataset. Nevertheless,
    in the medium group of source-reference divergence in the WikiTableText dataset,
    the T5-base model often performs better than all three larger LLMs. These observations
    indicate that when source-reference divergence is high, it is advisable to use
    smaller LLMs like T5-base for better performance.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答：从比较图 [11](#S7.F11 "图 11 ‣ 7.2 模型大小对 D2T 性能的影响 ‣ 7 分析源-参考差异的影响 ‣ 模型大小对微调 LLM
    在数据到文本生成中的性能的影响：最先进的调查")、[12](#S7.F12 "图 12 ‣ 7.2 模型大小对 D2T 性能的影响 ‣ 7 分析源-参考差异的影响
    ‣ 模型大小对微调 LLM 在数据到文本生成中的性能的影响：最先进的调查") 和 [13](#S7.F13 "图 13 ‣ 7.2 模型大小对 D2T 性能的影响
    ‣ 7 分析源-参考差异的影响 ‣ 模型大小对微调 LLM 在数据到文本生成中的性能的影响：最先进的调查") 可以明显看出，参数较大的 LLM（Llama2-13B、Llama2-7B
    和 BLOOM-7B）在源-参考差异较低时往往优于像 T5-base 这样的低参数模型。然而，特别是在与 BLOOM-7B 和 T5-base 模型的比较中存在一些例外。当源-参考差异率上升（即，在源-参考差异的中等组中），作为低参数模型的
    T5-base 开始表现良好并与较大的 LLM 竞争。最后，当源-参考差异变高时，T5-base 往往在几乎所有指标上超越较大的 LLM（Llama2-13B、Llama2-7B
    和 BLOOM-7B）或显著缩小了与它们的性能差距。虽然有些特殊情况 T5-base 无法超越 LLM，但这主要发生在 WikiTableText 数据集上。然而，在
    WikiTableText 数据集中源-参考差异的中等组中，T5-base 模型往往比所有三个较大的 LLM 表现更好。这些观察结果表明，当源-参考差异较高时，建议使用像
    T5-base 这样的较小 LLM 来获得更好的性能。
- en: '| source | reference | LLM | generated text | remarks |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| source | reference | LLM | generated text | remarks |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| name=The Vaults; eatType = pub; food=French; priceRange=less than £20; area=city
    centre; familyFriendly=yes | Near Raja Cuisine, in the city centre is a pub offering
    French food. The Vaults is cheap and family-friendly. | T5-base |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| name=The Vaults; eatType = pub; food=French; priceRange=less than £20; area=city
    centre; familyFriendly=yes | 在市中心的 Raja Cuisine 附近，有一家提供法国菜的酒吧。The Vaults 便宜且适合家庭。
    | T5-base |'
- en: '&#124; The Vaults is a family-friendly &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Vaults 是一个适合家庭的 &#124;'
- en: '&#124; French pub in the city centre &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 市中心的法国酒吧 &#124;'
- en: '&#124; with a price range of less than £20. &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 价格范围低于 £20。 &#124;'
- en: '| All LLMs are successfully generated similar texts, however T5-base transformed
    with ‘food=French’ into ‘French pub’, mistakenly. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 所有 LLM 都成功生成了类似的文本，但 T5-base 错误地将‘food=French’ 转换为‘French pub’。 |'
- en: '| T5-large |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| T5-large |'
- en: '&#124; The Vaults is a family-friendly pub &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Vaults 是一个适合家庭的酒吧 &#124;'
- en: '&#124; in the city centre that &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在市中心的 &#124;'
- en: '&#124; serves French food for less than £20. &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供低于 £20 的法国菜。 &#124;'
- en: '|'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-7B |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B |'
- en: '&#124; The Vaults is a family-friendly &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Vaults 是一个适合家庭的 &#124;'
- en: '&#124; pub that serves French food. &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供法国菜的酒吧。 &#124;'
- en: '&#124; It is located in the city centre &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它位于市中心 &#124;'
- en: '&#124; and has a price range of less than £20. &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并且价格范围少于20英镑。 &#124;'
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-13B |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B |'
- en: '&#124; The Vaults is a family-friendly pub that &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Vaults 是一个适合家庭的酒吧，它 &#124;'
- en: '&#124; serves French food for less than £20. &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供少于20英镑的法式菜肴。 &#124;'
- en: '&#124; It is located in the city centre. &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它位于市中心。 &#124;'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| name=The Waterman; eatType=restaurant; priceRange=high; area=riverside; familyFriendly=yes;
    near=Raja Indian Cuisine | The Waterman is a restaurant near to Raja Indian Cuisine
    that is child-friendly. It has a high price range but is located in riverside.
    | T5-base |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| name=The Waterman; eatType=restaurant; priceRange=high; area=riverside; familyFriendly=yes;
    near=Raja Indian Cuisine | The Waterman 是一家靠近Raja Indian Cuisine的适合儿童的餐馆。它价格较高，但位于河滨区。
    | T5-base |'
- en: '&#124; The Waterman is a high priced, &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Waterman 是一个高价的， &#124;'
- en: '&#124; child friendly restaurant in &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适合儿童的餐馆在 &#124;'
- en: '&#124; the riverside area near Raja &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 靠近Raja的河滨区 &#124;'
- en: '&#124; Indian Cuisine. &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 印度菜。 &#124;'
- en: '| In this example, there is a discrepancy between the source data and the reference.
    The source data mentions familyFriendly=yes, while the reference uses the term
    ‘child-friendly’. Interestingly, all three LLMs, except Llama2-13B, successfully
    identify this discrepancy. This case highlights the possibility that larger LLMs
    may overlook certain divergences in the data. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 在这个例子中，源数据与参考资料之间存在差异。源数据提到familyFriendly=yes，而参考资料使用了‘child-friendly’这一术语。有趣的是，除了Llama2-13B之外，所有三种LLM都成功识别了这一差异。这个案例突显了较大的LLM可能忽视数据中的某些差异。
    |'
- en: '| T5-large |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| T5-large |'
- en: '&#124; The Waterman is a child friendly &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Waterman 是一个适合儿童的餐馆 &#124;'
- en: '&#124; restaurant in the riverside area &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 河滨区域的餐馆 &#124;'
- en: '&#124; near Raja Indian Cuisine. &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 靠近Raja Indian Cuisine。 &#124;'
- en: '&#124; It has a high price range. &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其价格范围较高。 &#124;'
- en: '|'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-7B |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B |'
- en: '&#124; The Waterman is a high priced &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Waterman 是一个高价的 &#124;'
- en: '&#124; restaurant in the riverside area &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 河滨区域的餐馆 &#124;'
- en: '&#124; near Raja Indian Cuisine. &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 靠近Raja Indian Cuisine。 &#124;'
- en: '&#124; It is child friendly. &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它适合儿童。 &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-13B |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B |'
- en: '&#124; The Waterman is a high priced family &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; The Waterman 是一个高价的家庭友好型 &#124;'
- en: '&#124; friendly restaurant in the riverside area &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 河滨区的适合家庭的餐馆 &#124;'
- en: '&#124; near Raja Indian Cuisine. &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 靠近Raja Indian Cuisine。 &#124;'
- en: '|'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 8: Case study on the E2E dataset featuring four LLMs (T5-base, T5-large,
    Llama2-7B, and Llama2-13B), illustrating errors made by smaller LLMs such as T5-base
    and instances where larger LLMs like Llama2-13B fail to identify source-reference
    divergence.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 8: 案例研究展示了E2E数据集中的四种LLM（T5-base, T5-large, Llama2-7B和Llama2-13B），说明了较小LLM如T5-base所犯的错误以及较大LLM如Llama2-13B在识别源参考差异方面的失败。'
- en: '| source | reference | LLM | generated text | remarks |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| source | reference | LLM | generated text | remarks |'
- en: '| [[’subjtitle’, ’history of the jews in malaysia’], [’subjsubtitle’, ’history’],
    [’year’, ’1881’], [’males’, ’14’], [’females’, ’9’], [’boys’, ’5’], [’girls’,
    ’4’], [’total’, ’32’]] | in 1988 , there were 32 the jews in malaysia in which
    there were 14 males , 9 females , 5 boys and 4 girls . | T5-base | in 1881, there
    were 32 jews in malaysia. | All LLMs are experiencing issues with informativeness
    as they struggle to generate all the source content accurately. In this example,
    larger LLMs show improved informativeness, with Llama2-13B achieving comparatively
    better results. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| [[’subjtitle’, ’犹太人在马来西亚的历史’], [’subjsubtitle’, ’历史’], [’year’, ’1881’],
    [’males’, ’14’], [’females’, ’9’], [’boys’, ’5’], [’girls’, ’4’], [’total’, ’32’]]
    | 1988年，马来西亚的犹太人有32人，其中14名男性，9名女性，5名男孩和4名女孩。 | T5-base | 1881年，马来西亚有32名犹太人。 |
    所有LLM在信息完整性上都有问题，因为它们难以准确生成所有源内容。在这个例子中，较大的LLM显示出改进的信息性，Llama2-13B取得了相对较好的结果。
    |'
- en: '| T5-large |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| T5-large |'
- en: '&#124; in 1881, there were 14 males and &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1881年，有14名男性和 &#124;'
- en: '&#124; 9 females in malaysia. &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9名女性在马来西亚。 &#124;'
- en: '|'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-7B |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B |'
- en: '&#124; there were 14 males and 9 females &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有14名男性和9名女性 &#124;'
- en: '&#124; in the jews in malaysia in 1881 . &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1881年马来西亚的犹太人中。 &#124;'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Llama2-13B |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B |'
- en: '&#124; there were 14 males , 9 females , &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有14名男性，9名女性， &#124;'
- en: '&#124; 5 boys and 4 girls in the jews &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 犹太人中的5个男孩和4个女孩 &#124;'
- en: '&#124; in malaysia in 1881 . &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1881年马来西亚。 &#124;'
- en: '|'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| [[’subjtitle’, ’bath rugby’], [’subjsubtitle’, ’current squad’], [’player’,
    ’jack wilson’], [’position’, ’wing’], [’union’, ’new zealand’]] | in bath rugby
    , wing jack wilson was a member of new zealand union . | T5-base | jack wilson
    was a wing in new zealand. | While T5-base and T5-large provide faithful information,
    Llama2-7B and Llama2-13B outperform them in terms of readability. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| [[’subjtitle’, ’bath rugby’], [’subjsubtitle’, ’current squad’], [’player’,
    ’jack wilson’], [’position’, ’wing’], [’union’, ’new zealand’]] | 在巴斯橄榄球俱乐部，边锋
    Jack Wilson 是新西兰联盟的一员。 | T5-base | Jack Wilson 是新西兰的一名边锋。 | 虽然 T5-base 和 T5-large
    提供了忠实的信息，但在阅读性方面，Llama2-7B 和 Llama2-13B 表现更佳。'
- en: '| T5-large | jack wilson was from union new zealand. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| T5-large | Jack Wilson 来自新西兰联盟。 |'
- en: '| Llama2-7B | jack wilson was from new zealand . |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | Jack Wilson 来自新西兰。 |'
- en: '| Llama2-13B | jack wilson was from new zealand . |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | Jack Wilson 来自新西兰。 |'
- en: 'Table 9: Case study on the WikiTableText dataset featuring four LLMs (T5-base,
    T5-large, Llama2-7B, and Llama2-13B), demonstrating how increasing LLM size enhances
    informativeness, while smaller LLMs like T5-base and T5-large generate faithful
    but less readable content compared to larger LLMs.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在 WikiTableText 数据集中对四种大语言模型（T5-base、T5-large、Llama2-7B 和 Llama2-13B）的案例研究，展示了增加大语言模型的规模如何增强信息量，同时像
    T5-base 和 T5-large 这样较小的大语言模型生成的内容虽然忠实，但与较大的大语言模型相比，阅读性较差。
- en: 8 Case Studies
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 个案例研究
- en: 'We present two case studies (Table [8](#S7.T8 "Table 8 ‣ 7.2 Impact of model
    Size on LLM Performance for D2T in the Presence of Source-Reference Divergence
    ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of Model Size
    on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")
    and [9](#S7.T9 "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T
    in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference
    Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text
    Generation: A State-of-the-Art Investigation")) conducted on the E2E and WikiTableText
    datasets, each showcasing two samples and their corresponding generations from
    four LLMs: T5-base, T5-large, Llama2-7B, and Llama2-13B. In the first case (Table [8](#S7.T8
    "Table 8 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")) study using the E2E dataset, one example illustrates
    how smaller LLMs like T5-base may fail to accurately convey information. Another
    example highlights a source-reference divergence phenomenon, where larger LLMs
    like Llama2-13B fail to capture the divergence, while other LLMs successfully
    handle it. Therefore, Table [8](#S7.T8 "Table 8 ‣ 7.2 Impact of model Size on
    LLM Performance for D2T in the Presence of Source-Reference Divergence ‣ 7 Analyzing
    the Effect of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned
    LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation")
    demonstrates that smaller-sized LLMs may sacrifice informativeness, while larger
    LLMs frequently struggle to maintain source-reference divergence. The second case
    study (Table [9](#S7.T9 "Table 9 ‣ 7.2 Impact of model Size on LLM Performance
    for D2T in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect
    of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation")) on the WikiTableText
    dataset demonstrates how increasing LLM size enhances informativeness as a D2T
    model. Additionally, it reveals that smaller LLMs such as T5-base and T5-large
    tend to prioritize faithfulness, while larger LLMs prioritize readability. Table [9](#S7.T9
    "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") reveals that smaller-sized LLMs prioritize
    faithfulness, rendering them more suitable for applications requiring safety-critical
    considerations.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了两个案例研究（表[8](#S7.T8 "Table 8 ‣ 7.2 Impact of model Size on LLM Performance
    for D2T in the Presence of Source-Reference Divergence ‣ 7 Analyzing the Effect
    of Source-Reference Divergence ‣ Impact of Model Size on Fine-tuned LLM Performance
    in Data-to-Text Generation: A State-of-the-Art Investigation") 和 [9](#S7.T9 "Table
    9 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence of Source-Reference
    Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence ‣ Impact of
    Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art
    Investigation"))，这些研究在E2E和WikiTableText数据集上进行，每个研究展示了两个样本及其来自四种LLM的生成结果：T5-base、T5-large、Llama2-7B和Llama2-13B。在第一个案例（表[8](#S7.T8
    "Table 8 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")）使用E2E数据集时，一个例子展示了较小的LLM，如T5-base，可能无法准确传达信息。另一个例子突出了源引用偏差现象，在这种现象中，较大的LLM，如Llama2-13B，未能捕捉到偏差，而其他LLM成功处理了这个问题。因此，表[8](#S7.T8
    "Table 8 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") 证明了较小的LLM可能会牺牲信息量，而较大的LLM则经常难以维持源引用偏差。第二个案例研究（表[9](#S7.T9
    "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation")）在WikiTableText数据集上展示了如何通过增加LLM的规模来提升作为D2T模型的信息量。此外，它还揭示了较小的LLM，如T5-base和T5-large，倾向于优先考虑忠实性，而较大的LLM则优先考虑可读性。表[9](#S7.T9
    "Table 9 ‣ 7.2 Impact of model Size on LLM Performance for D2T in the Presence
    of Source-Reference Divergence ‣ 7 Analyzing the Effect of Source-Reference Divergence
    ‣ Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation:
    A State-of-the-Art Investigation") 揭示了较小的LLM优先考虑忠实性，使它们更适合需要安全关键考虑的应用。'
- en: 9 Conclusion
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: 'This paper investigates the impact of model size (number of model parameters)
    on the performance of fine-tuned LLMs in D2T generation tasks, focusing on three
    key qualities: readability, informativeness, and faithfulness. Despite its significant
    importance, this investigation has been entirely overlooked in the existing literature
    on LLMs for D2T applications. We conducted a comprehensive assessment involving
    twelve LLMs drawn from five popular LLM families: T5, BART, OPT, BLOOM, and Llama
    2. Our analysis encompasses three primary D2T task categories: graph-to-text,
    table-to-text, and MR-to-text. Six widely recognized evaluation metrics to ensure
    a thorough examination of D2T models: BLEU, METEOR, BERTScore, MoverScore, Parent,
    and BARTScore. Given the inherent risks associated with human evaluation (Freitag
    et al., [2021](#bib.bib25)), such as inter-annotator knowledge disparities and
    cognitive biases, we have opted not to include human evaluation results in our
    study to ensure unbiased comparisons. Nonetheless, two case studies are presented
    to enhance clarity in our analyses. Moreover, we examine a crucial aspect of D2T
    tasks: source-reference divergence, which presents challenges in generating a
    reference from the source data. Our primary findings can be summarized into three
    main points. Firstly, we find that increasing the parameters of large language
    models (LLMs) within a given LLM family generally improves readability and informativeness
    in data-to-text (D2T) tasks. However, this often comes at the expense of faithfulness
    in such tasks, suggesting a preference for smaller LLMs in safety-critical D2T
    domains. Secondly, we demonstrate that larger-sized LLM families do not consistently
    outperform smaller ones in D2T task performance, challenging the notion that bigger
    models always yield better results. Lastly, in the context of source-reference
    divergence, we meticulously observe that the performance of all LLMs is negatively
    affected. However, smaller LLMs exhibit greater resilience compared to their larger
    counterparts in dealing with source-reference divergence. We firmly believe that
    our research offers a profound understanding of the nuanced interplay between
    model size and LLM performance in D2T tasks, furnishing valuable insights to optimize
    the utilization of LLMs across diverse D2T applications.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了模型规模（模型参数数量）对微调大语言模型（LLMs）在数据到文本（D2T）生成任务中的性能影响，重点关注三个关键特性：可读性、信息量和忠实度。尽管其重要性重大，但现有文献中对LLMs在D2T应用中的这一调查完全被忽视。我们进行了全面评估，涉及十二个来自五个流行LLM系列的模型：T5、BART、OPT、BLOOM和Llama
    2。我们的分析涵盖了三个主要的D2T任务类别：图到文本、表到文本和MR到文本。六个广泛认可的评估指标用于确保对D2T模型的全面检查：BLEU、METEOR、BERTScore、MoverScore、Parent和BARTScore。鉴于与人类评估相关的固有风险（Freitag
    et al., [2021](#bib.bib25)），例如标注者知识差异和认知偏差，我们选择不在研究中包含人类评估结果，以确保公正的比较。然而，提供了两个案例研究以增强我们分析的清晰度。此外，我们考察了D2T任务的一个关键方面：源-参考偏差，这在从源数据生成参考时带来了挑战。我们的主要发现可以总结为三点。首先，我们发现，在给定LLM系列中，增加大语言模型（LLMs）的参数通常能改善数据到文本（D2T）任务中的可读性和信息量。然而，这通常以忠实度的下降为代价，建议在安全关键的D2T领域偏好使用较小的LLMs。其次，我们证明了较大规模的LLM系列在D2T任务性能上不一定优于较小的系列，挑战了大模型总能产生更好结果的观念。最后，在源-参考偏差的背景下，我们细致观察到所有LLMs的性能受到负面影响。然而，较小的LLMs在应对源-参考偏差时表现出比较大模型更强的韧性。我们坚信我们的研究提供了对模型规模与LLM性能在D2T任务中微妙相互关系的深刻理解，为优化LLMs在各种D2T应用中的利用提供了宝贵的见解。
- en: Acknowledgment
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is partially supported by the Indo-French Centre for the Promotion
    of Advanced Research (IFCPAR/CEFIPRA) through CSRP Project No. 6702-2.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由印度-法国高级研究促进中心（IFCPAR/CEFIPRA）通过CSRP项目编号6702-2资助。
- en: References
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aghajanyan et al. (2021) Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
    Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
    In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, *Proceedings
    of the ACL/IJCNLP*, pages 7319–7328\. Association for Computational Linguistics,
    2021. URL [https://doi.org/10.18653/v1/2021.acl-long.568](https://doi.org/10.18653/v1/2021.acl-long.568).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aghajanyan等（2021）Armen Aghajanyan、Sonal Gupta和Luke Zettlemoyer。内在维度解释了语言模型微调的有效性。在Chengqing
    Zong、Fei Xia、Wenjie Li和Roberto Navigli编辑的*ACL/IJCNLP会议论文集*中，第7319–7328页。计算语言学协会，2021年。网址[https://doi.org/10.18653/v1/2021.acl-long.568](https://doi.org/10.18653/v1/2021.acl-long.568)。
- en: Angeli et al. (2010) Gabor Angeli, Percy Liang, and Dan Klein. A simple domain-independent
    probabilistic approach to generation. In *Proceedings of the EMNLP*, pages 502–512\.
    Association for Computational Linguistics, 2010. URL [https://aclanthology.org/D10-1049/](https://aclanthology.org/D10-1049/).
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angeli 等 (2010) Gabor Angeli、Percy Liang 和 Dan Klein. 一种简单的领域无关的概率生成方法。在 *EMNLP
    会议论文集* 中，第 502–512 页。计算语言学协会，2010。URL [https://aclanthology.org/D10-1049/](https://aclanthology.org/D10-1049/)。
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural
    machine translation by jointly learning to align and translate. In Yoshua Bengio
    and Yann LeCun, editors, *Proceedings of the ICLR*, 2015. URL [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 (2015) Dzmitry Bahdanau、Kyunghyun Cho 和 Yoshua Bengio. 通过联合学习对齐和翻译进行神经机器翻译。在
    Yoshua Bengio 和 Yann LeCun 编辑的 *ICLR 会议论文集* 中，2015。URL [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)。
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. METEOR: an automatic
    metric for MT evaluation with improved correlation with human judgments. In Jade
    Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss, editors, *Proceedings
    of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
    and/or Summarization*, pages 65–72\. Association for Computational Linguistics,
    2005. URL [https://aclanthology.org/W05-0909/](https://aclanthology.org/W05-0909/).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie (2005) Satanjeev Banerjee 和 Alon Lavie. METEOR：一种用于机器翻译评估的自动度量，改进了与人工评估的一致性。在
    Jade Goldstein、Alon Lavie、Chin-Yew Lin 和 Clare R. Voss 编辑的 *机器翻译及/或摘要内在与外在评估措施研讨会论文集*
    中，第 65–72 页。计算语言学协会，2005。URL [https://aclanthology.org/W05-0909/](https://aclanthology.org/W05-0909/)。
- en: 'Bao et al. (2018) Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming
    Zhou, and Tiejun Zhao. Table-to-text: Describing table region with natural language.
    In Sheila A. McIlraith and Kilian Q. Weinberger, editors, *Proceedings of the
    AAAI*, pages 5020–5027\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11944](https://doi.org/10.1609/aaai.v32i1.11944).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等 (2018) Junwei Bao、Duyu Tang、Nan Duan、Zhao Yan、Yuanhua Lv、Ming Zhou 和 Tiejun
    Zhao. 从表格到文本：用自然语言描述表格区域。在 Sheila A. McIlraith 和 Kilian Q. Weinberger 编辑的 *AAAI
    会议论文集* 中，第 5020–5027 页。AAAI 出版社，2018。URL [https://doi.org/10.1609/aaai.v32i1.11944](https://doi.org/10.1609/aaai.v32i1.11944)。
- en: Belz (2008) Anja Belz. Automatic generation of weather forecast texts using
    comprehensive probabilistic generation-space models. *Natural Language Engineering*,
    14(4):431–455, 2008. URL [https://doi.org/10.1017/S1351324907004664](https://doi.org/10.1017/S1351324907004664).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belz (2008) Anja Belz. 使用全面的概率生成空间模型自动生成天气预报文本。*自然语言工程*，14(4)：431–455，2008。URL
    [https://doi.org/10.1017/S1351324907004664](https://doi.org/10.1017/S1351324907004664)。
- en: Belz and Reiter (2006) Anja Belz and Ehud Reiter. Comparing automatic and human
    evaluation of NLG systems. In Diana McCarthy and Shuly Wintner, editors, *Proceedings
    of the EACL*. The Association for Computer Linguistics, 2006. URL [https://aclanthology.org/E06-1040/](https://aclanthology.org/E06-1040/).
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belz 和 Reiter (2006) Anja Belz 和 Ehud Reiter. 比较自动和人工对 NLG 系统的评估。在 Diana McCarthy
    和 Shuly Wintner 编辑的 *EACL 会议论文集* 中。计算机语言学协会，2006。URL [https://aclanthology.org/E06-1040/](https://aclanthology.org/E06-1040/)。
- en: 'Belz et al. (2020) Anya Belz, Simon Mille, and David M. Howcroft. Disentangling
    the properties of human evaluation methods: A classification system to support
    comparability, meta-evaluation and reproducibility testing. In Brian Davis, Yvette
    Graham, John D. Kelleher, and Yaji Sripada, editors, *Proceedings of the INLG*,
    pages 183–194\. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.inlg-1.24](https://doi.org/10.18653/v1/2020.inlg-1.24).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belz 等 (2020) Anya Belz、Simon Mille 和 David M. Howcroft. 揭示人工评估方法的属性：一个分类系统以支持可比性、元评估和可重复性测试。在
    Brian Davis、Yvette Graham、John D. Kelleher 和 Yaji Sripada 编辑的 *INLG 会议论文集* 中，第
    183–194 页。计算语言学协会，2020。URL [https://doi.org/10.18653/v1/2020.inlg-1.24](https://doi.org/10.18653/v1/2020.inlg-1.24)。
- en: Bickel et al. (2005) Steffen Bickel, Peter Haider, and Tobias Scheffer. Predicting
    sentences using n-gram language models. In *Proceedings of the EMNLP-HLT*, pages
    193–200\. The Association for Computational Linguistics, 2005. URL [https://aclanthology.org/H05-1025/](https://aclanthology.org/H05-1025/).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bickel 等 (2005) Steffen Bickel、Peter Haider 和 Tobias Scheffer. 使用 n-gram 语言模型预测句子。在
    *EMNLP-HLT 会议论文集* 中，第 193–200 页。计算语言学协会，2005。URL [https://aclanthology.org/H05-1025/](https://aclanthology.org/H05-1025/)。
- en: Brown et al. (1992) Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
    Jennifer C. Lai, and Robert L. Mercer. Class-based n-gram models of natural language.
    *Computational Linguistics*, 18(4):467–479, 1992. URL [https://aclanthology.org/J92-4003.pdf](https://aclanthology.org/J92-4003.pdf).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（1992）Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jennifer
    C. Lai, 和 Robert L. Mercer。基于类别的自然语言n-gram模型。*计算语言学*，18(4):467–479，1992年。网址 [https://aclanthology.org/J92-4003.pdf](https://aclanthology.org/J92-4003.pdf)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio
    Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, *Proceedings
    of the NeurIPS*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少样本学习者。在 Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, 和 Hsuan-Tien Lin 编者，*NeurIPS 论文集*，2020年。网址 [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。
- en: Budzianowski et al. (2018) Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. Multiwoz - A large-scale
    multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Ellen
    Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, *Proceedings
    of the EMNLP*, pages 5016–5026. Association for Computational Linguistics, 2018.
    URL [https://aclanthology.org/D18-1547/](https://aclanthology.org/D18-1547/).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Budzianowski 等人（2018）Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo
    Casanueva, Stefan Ultes, Osman Ramadan, 和 Milica Gasic。Multiwoz - 大规模多领域的Wizard-of-Oz数据集，用于任务导向对话建模。在
    Ellen Riloff, David Chiang, Julia Hockenmaier, 和 Jun’ichi Tsujii 编者，*EMNLP 论文集*，页码
    5016–5026。计算语言学协会，2018年。网址 [https://aclanthology.org/D18-1547/](https://aclanthology.org/D18-1547/)。
- en: 'Chen and Mooney (2008) David L. Chen and Raymond J. Mooney. Learning to sportscast:
    A test of grounded language acquisition. In William W. Cohen, Andrew McCallum,
    and Sam T. Roweis, editors, *Proceedings of the ICML*, volume 307, pages 128–135\.
    Association for Computing Machinery, 2008. URL [https://doi.org/10.1145/1390156.1390173](https://doi.org/10.1145/1390156.1390173).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Mooney（2008）David L. Chen 和 Raymond J. Mooney。学习运动广播：语言获得的实证测试。在 William
    W. Cohen, Andrew McCallum, 和 Sam T. Roweis 编者，*ICML 论文集*，第 307 卷，页码 128–135。计算机协会，2008年。网址
    [https://doi.org/10.1145/1390156.1390173](https://doi.org/10.1145/1390156.1390173)。
- en: 'Chen et al. (2020) Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang,
    Sairam Sundaresan, and William Yang Wang. Logic2text: High-fidelity natural language
    generation from logical forms. In Trevor Cohn, Yulan He, and Yang Liu, editors,
    *Proceedings of the EMNLP Findings*, volume EMNLP 2020, pages 2096–2111\. Association
    for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.190](https://doi.org/10.18653/v1/2020.findings-emnlp.190).'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2020）Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam
    Sundaresan, 和 William Yang Wang。Logic2text：从逻辑形式生成高保真自然语言。在 Trevor Cohn, Yulan
    He, 和 Yang Liu 编者，*EMNLP 发现论文集*，EMNLP 2020，第 2096–2111 页。计算语言学协会，2020年。网址 [https://doi.org/10.18653/v1/2020.findings-emnlp.190](https://doi.org/10.18653/v1/2020.findings-emnlp.190)。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways. *Journal of Machine Learning Research*, 24:240:1–240:113, 2023.
    URL [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew
    M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, 和 Noah Fiedel. Palm: 通过路径扩展语言建模。*机器学习研究期刊*，24:240:1–240:113,
    2023。网址 [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html)。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,
    *Proceedings of the NeurIPS*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora: 高效微调量化的 LLMs。在 Alice Oh, Tristan Naumann, Amir Globerson,
    Kate Saenko, Moritz Hardt, 和 Sergey Levine 主编的 *NeurIPS 会议论文集* 中，2023。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html)。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, *Proceedings
    of the NAACL-HLT*, pages 4171–4186\. Association for Computational Linguistics,
    2019. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    BERT: 深度双向变换器的预训练用于语言理解。在 Jill Burstein, Christy Doran, 和 Thamar Solorio 主编的 *NAACL-HLT
    会议论文集* 中，第 4171–4186 页。计算语言学协会，2019。网址 [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)。'
- en: Dhingra et al. (2019) Bhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh, Ming-Wei
    Chang, Dipanjan Das, and William W. Cohen. Handling divergent reference texts
    when evaluating table-to-text generation. In Anna Korhonen, David R. Traum, and
    Lluís Màrquez, editors, *Proceedings of the ACL*, pages 4884–4895\. Association
    for Computational Linguistics, 2019. URL [https://doi.org/10.18653/v1/p19-1483](https://doi.org/10.18653/v1/p19-1483).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhingra et al. (2019) Bhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh, Ming-Wei
    Chang, Dipanjan Das, 和 William W. Cohen. 处理评估表格到文本生成中的不同参考文本。在 Anna Korhonen,
    David R. Traum, 和 Lluís Màrquez 主编的 *ACL 会议论文集* 中，第 4884–4895 页。计算语言学协会，2019。网址
    [https://doi.org/10.18653/v1/p19-1483](https://doi.org/10.18653/v1/p19-1483)。
- en: Dror et al. (2018) Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart.
    The hitchhiker’s guide to testing statistical significance in natural language
    processing. In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings of the ACL*,
    pages 1383–1392\. Association for Computational Linguistics, 2018. URL [https://aclanthology.org/P18-1128/](https://aclanthology.org/P18-1128/).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dror et al. (2018) Rotem Dror, Gili Baumer, Segev Shlomov, 和 Roi Reichart. 自然语言处理中的统计显著性测试指南。在
    Iryna Gurevych 和 Yusuke Miyao 主编的 *ACL 会议论文集* 中，第 1383–1392 页。计算语言学协会，2018。网址
    [https://aclanthology.org/P18-1128/](https://aclanthology.org/P18-1128/)。
- en: Dusek et al. (2018) Ondrej Dusek, Jekaterina Novikova, and Verena Rieser. Findings
    of the E2E NLG challenge. In Emiel Krahmer, Albert Gatt, and Martijn Goudbeek,
    editors, *Proceedings of the INLG*, pages 322–328\. Association for Computational
    Linguistics, 2018. URL [https://doi.org/10.18653/v1/w18-6539](https://doi.org/10.18653/v1/w18-6539).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dusek 等（2018）**Ondrej Dusek**、**Jekaterina Novikova** 和 **Verena Rieser**。E2E
    NLG 挑战的发现。在 **Emiel Krahmer**、**Albert Gatt** 和 **Martijn Goudbeek**（编辑），*《INLG
    会议录》*，页码 322–328。计算语言学协会，2018年。URL [https://doi.org/10.18653/v1/w18-6539](https://doi.org/10.18653/v1/w18-6539)。
- en: 'Erdem et al. (2022) Erkut Erdem, Menekse Kuyu, Semih Yagcioglu, Anette Frank,
    Letitia Parcalabescu, Barbara Plank, Andrii Babii, Oleksii Turuta, Aykut Erdem,
    Iacer Calixto, Elena Lloret, Elena Simona Apostol, Ciprian-Octavian Truica, Branislava
    Sandrih, Sanda Martincic-Ipsic, Gábor Berend, Albert Gatt, and Grazina Korvel.
    Neural natural language generation: A survey on multilinguality, multimodality,
    controllability and learning. *Journal of Artificial Intelligence Research*, 73:1131–1207,
    2022. URL [https://doi.org/10.1613/jair.1.12918](https://doi.org/10.1613/jair.1.12918).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdem 等（2022）**Erkut Erdem**、**Menekse Kuyu**、**Semih Yagcioglu**、**Anette Frank**、**Letitia
    Parcalabescu**、**Barbara Plank**、**Andrii Babii**、**Oleksii Turuta**、**Aykut Erdem**、**Iacer
    Calixto**、**Elena Lloret**、**Elena Simona Apostol**、**Ciprian-Octavian Truica**、**Branislava
    Sandrih**、**Sanda Martincic-Ipsic**、**Gábor Berend**、**Albert Gatt** 和 **Grazina
    Korvel**。神经自然语言生成：关于多语言性、多模态性、可控性和学习的调查。*《人工智能研究期刊》*，73:1131–1207，2022年。URL [https://doi.org/10.1613/jair.1.12918](https://doi.org/10.1613/jair.1.12918)。
- en: 'Faiz et al. (2023) Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek
    Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint
    of large language models. *CoRR*, abs/2309.14393, 2023. URL [https://doi.org/10.48550/arXiv.2309.14393](https://doi.org/10.48550/arXiv.2309.14393).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faiz 等（2023）**Ahmad Faiz**、**Sotaro Kaneda**、**Ruhan Wang**、**Rita Osi**、**Parteek
    Sharma**、**Fan Chen** 和 **Lei Jiang**。Llmcarbon：大规模语言模型的端到端碳足迹建模。*《计算研究报告》*，abs/2309.14393，2023年。URL
    [https://doi.org/10.48550/arXiv.2309.14393](https://doi.org/10.48550/arXiv.2309.14393)。
- en: Fan et al. (2018) Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical
    neural story generation. In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings
    of the ACL*, pages 889–898\. Association for Computational Linguistics, 2018.
    URL [https://aclanthology.org/P18-1082/](https://aclanthology.org/P18-1082/).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2018）**Angela Fan**、**Mike Lewis** 和 **Yann N. Dauphin**。分层神经故事生成。在 **Iryna
    Gurevych** 和 **Yusuke Miyao**（编辑），*《ACL 会议录》*，页码 889–898。计算语言学协会，2018年。URL [https://aclanthology.org/P18-1082/](https://aclanthology.org/P18-1082/)。
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Wu, Graham Neubig, and André F. T. Martins. Bridging the Gap: A Survey on Integrating
    (Human) Feedback for Natural Language Generation. *Transactions of the Association
    for Computational Linguistics*, 11:1643–1668, 2023. ISSN 2307-387X. URL [https://doi.org/10.1162/tacl_a_00626](https://doi.org/10.1162/tacl_a_00626).'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes 等（2023）**Patrick Fernandes**、**Aman Madaan**、**Emmy Liu**、**António
    Farinhas**、**Pedro Henrique Martins**、**Amanda Bertsch**、**José G. C. de Souza**、**Shuyan
    Zhou**、**Tongshuang Wu**、**Graham Neubig** 和 **André F. T. Martins**。弥合差距：关于将（人类）反馈集成到自然语言生成中的调查。*《计算语言学协会会刊》*，11:1643–1668，2023年。ISSN
    2307-387X。URL [https://doi.org/10.1162/tacl_a_00626](https://doi.org/10.1162/tacl_a_00626)。
- en: 'Freitag et al. (2021) Markus Freitag, George F. Foster, David Grangier, Viresh
    Ratnakar, Qijun Tan, and Wolfgang Macherey. Experts, errors, and context: A large-scale
    study of human evaluation for machine translation. *Transactions of the Association
    for Computational Linguistics*, 9:1460–1474, 2021. URL [https://doi.org/10.1162/tacl_a_00437](https://doi.org/10.1162/tacl_a_00437).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag 等（2021）**Markus Freitag**、**George F. Foster**、**David Grangier**、**Viresh
    Ratnakar**、**Qijun Tan** 和 **Wolfgang Macherey**。专家、错误与上下文：对机器翻译人类评估的大规模研究。*《计算语言学协会会刊》*，9:1460–1474，2021年。URL
    [https://doi.org/10.1162/tacl_a_00437](https://doi.org/10.1162/tacl_a_00437)。
- en: 'Gardent et al. (2017) Claire Gardent, Anastasia Shimorina, Shashi Narayan,
    and Laura Perez-Beltrachini. The webnlg challenge: Generating text from RDF data.
    In José Maria Alonso, Alberto Bugarín, and Ehud Reiter, editors, *Proceedings
    of the INLG*, pages 124–133\. Association for Computational Linguistics, 2017.
    URL [https://doi.org/10.18653/v1/w17-3518](https://doi.org/10.18653/v1/w17-3518).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gardent 等（2017）**Claire Gardent**、**Anastasia Shimorina**、**Shashi Narayan**
    和 **Laura Perez-Beltrachini**。WebNLG 挑战：从 RDF 数据生成文本。在 **José Maria Alonso**、**Alberto
    Bugarín** 和 **Ehud Reiter**（编辑），*《INLG 会议录》*，页码 124–133。计算语言学协会，2017年。URL [https://doi.org/10.18653/v1/w17-3518](https://doi.org/10.18653/v1/w17-3518)。
- en: 'Gatt and Krahmer (2018) Albert Gatt and Emiel Krahmer. Survey of the state
    of the art in natural language generation: Core tasks, applications and evaluation.
    *Journal of Artificial Intelligence Research*, 61:65–170, 2018. URL [https://doi.org/10.1613/jair.5477](https://doi.org/10.1613/jair.5477).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gatt 和 Krahmer (2018) Albert Gatt 和 Emiel Krahmer. 自然语言生成的前沿综述：核心任务、应用和评估。*人工智能研究期刊*，61:65–170，2018年。网址
    [https://doi.org/10.1613/jair.5477](https://doi.org/10.1613/jair.5477)。
- en: 'Ge et al. (2023) Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. Openagi: When LLM meets domain experts.
    In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey
    Levine, editors, *Proceedings of the NeurIPS*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ge 等人 (2023) Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan
    Xu, Zelong Li, 和 Yongfeng Zhang. Openagi: 当 LLM 遇到领域专家。见于 Alice Oh, Tristan Naumann,
    Amir Globerson, Kate Saenko, Moritz Hardt, 和 Sergey Levine 编辑的*NeurIPS 会议论文集*，2023年。网址
    [http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html)。'
- en: 'Gkatzia (2016) Dimitra Gkatzia. Content selection in data-to-text systems:
    A survey. *CoRR*, abs/1610.08375, 2016. URL [http://arxiv.org/abs/1610.08375](http://arxiv.org/abs/1610.08375).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gkatzia (2016) Dimitra Gkatzia. 数据到文本系统中的内容选择：综述。*CoRR*，abs/1610.08375，2016年。网址
    [http://arxiv.org/abs/1610.08375](http://arxiv.org/abs/1610.08375)。
- en: Gong et al. (2019) Heng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu. Table-to-text
    generation with effective hierarchical encoder on three dimensions (row, column
    and time). In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
    *Proceedings of the EMNLP*, pages 3141–3150\. Association for Computational Linguistics,
    2019. URL [https://doi.org/10.18653/v1/D19-1310](https://doi.org/10.18653/v1/D19-1310).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等人 (2019) Heng Gong, Xiaocheng Feng, Bing Qin, 和 Ting Liu. 使用有效的分层编码器进行表格到文本生成（行、列和时间三个维度）。见于
    Kentaro Inui, Jing Jiang, Vincent Ng, 和 Xiaojun Wan 编辑的*EMNLP 会议论文集*，第3141–3150页。计算语言学协会，2019年。网址
    [https://doi.org/10.18653/v1/D19-1310](https://doi.org/10.18653/v1/D19-1310)。
- en: Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton.
    Speech recognition with deep recurrent neural networks. In *Proceedings of the
    ICASSP*, pages 6645–6649\. IEEE, 2013. URL [https://doi.org/10.1109/ICASSP.2013.6638947](https://doi.org/10.1109/ICASSP.2013.6638947).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 等人 (2013) Alex Graves, Abdel-rahman Mohamed, 和 Geoffrey E. Hinton. 使用深度递归神经网络的语音识别。见于*ICASSP
    会议论文集*，第6645–6649页。IEEE，2013年。网址 [https://doi.org/10.1109/ICASSP.2013.6638947](https://doi.org/10.1109/ICASSP.2013.6638947)。
- en: 'Heinzerling and Inui (2021) Benjamin Heinzerling and Kentaro Inui. Language
    models as knowledge bases: On entity representations, storage capacity, and paraphrased
    queries. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, *Proceedings
    of the EACL*, pages 1772–1791\. Association for Computational Linguistics, 2021.
    URL [https://doi.org/10.18653/v1/2021.eacl-main.153](https://doi.org/10.18653/v1/2021.eacl-main.153).'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heinzerling 和 Inui (2021) Benjamin Heinzerling 和 Kentaro Inui. 语言模型作为知识库：关于实体表示、存储能力和改写查询。见于
    Paola Merlo, Jörg Tiedemann, 和 Reut Tsarfaty 编辑的*EACL 会议论文集*，第1772–1791页。计算语言学协会，2021年。网址
    [https://doi.org/10.18653/v1/2021.eacl-main.153](https://doi.org/10.18653/v1/2021.eacl-main.153)。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural Computation*, 9(8):1735–1780, 1997. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆。*神经计算*，9(8):1735–1780，1997年。网址
    [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)。
- en: Holtzman et al. (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, and Yejin Choi. Learning to write with cooperative discriminators.
    In Iryna Gurevych and Yusuke Miyao, editors, *Proceedings of the ACL*, pages 1638–1649\.
    Association for Computational Linguistics, 2018. URL [https://aclanthology.org/P18-1152/](https://aclanthology.org/P18-1152/).
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等人 (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, 和 Yejin Choi. 使用合作判别器进行写作学习。见于 Iryna Gurevych 和 Yusuke Miyao 编辑的*ACL
    会议论文集*，第1638–1649页。计算语言学协会，2018年。网址 [https://aclanthology.org/P18-1152/](https://aclanthology.org/P18-1152/)。
- en: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. The curious case of neural text degeneration. In *Proceedings of the ICLR*.
    OpenReview.net, 2020. URL [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH).
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等（2020）Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, 和 Yejin Choi.
    神经文本退化的奇怪案例。发表于*ICLR 会议录*。OpenReview.net, 2020. URL [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH)。
- en: Hommes et al. (2019) Saar Hommes, Chris van der Lee, Felix J. Clouth, Jeroen K.
    Vermunt, Xander Verbeek, and Emiel Krahmer. A personalized data-to-text support
    tool for cancer patients. In Kees van Deemter, Chenghua Lin, and Hiroya Takamura,
    editors, *Proceedings of the INLG*, pages 443–452\. Association for Computational
    Linguistics, 2019. URL [https://aclanthology.org/W19-8656/](https://aclanthology.org/W19-8656/).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hommes 等（2019）Saar Hommes, Chris van der Lee, Felix J. Clouth, Jeroen K. Vermunt,
    Xander Verbeek, 和 Emiel Krahmer. 为癌症患者提供个性化的数据到文本支持工具。收录于 Kees van Deemter, Chenghua
    Lin, 和 Hiroya Takamura 主编的*INLG 会议录*，页 443–452。计算语言学协会，2019. URL [https://aclanthology.org/W19-8656/](https://aclanthology.org/W19-8656/)。
- en: Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. Universal language
    model fine-tuning for text classification. In Iryna Gurevych and Yusuke Miyao,
    editors, *Proceedings of the ACL*, pages 328–339\. Association for Computational
    Linguistics, 2018. URL [https://aclanthology.org/P18-1031/](https://aclanthology.org/P18-1031/).
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 和 Ruder（2018）Jeremy Howard 和 Sebastian Ruder. 用于文本分类的通用语言模型微调。收录于 Iryna
    Gurevych 和 Yusuke Miyao 主编的*ACL 会议录*，页 328–339。计算语言学协会，2018. URL [https://aclanthology.org/P18-1031/](https://aclanthology.org/P18-1031/)。
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. In *Proceedings of the ICLR*. OpenReview.net, 2022. URL
    [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2022）Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适配。发表于*ICLR 会议录*。OpenReview.net,
    2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9)。
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination
    in natural language generation. *ACM Computing Surveys*, 55(12):248:1–248:38,
    2023. URL [https://doi.org/10.1145/3571730](https://doi.org/10.1145/3571730).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2023）Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko
    Ishii, Yejin Bang, Andrea Madotto, 和 Pascale Fung. 自然语言生成中的幻觉调查。*ACM 计算调查*，55(12):248:1–248:38,
    2023. URL [https://doi.org/10.1145/3571730](https://doi.org/10.1145/3571730)。
- en: Jiang et al. (2024) Peng Jiang, Christian Sonne, Wangliang Li, Fengqi You, and
    Siming You. Preventing the immense increase in the life-cycle energy and carbon
    footprints of llm-powered intelligent chatbots. *Engineering*, 2024. ISSN 2095-8099.
    URL [https://www.sciencedirect.com/science/article/pii/S2095809924002315](https://www.sciencedirect.com/science/article/pii/S2095809924002315).
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2024）Peng Jiang, Christian Sonne, Wangliang Li, Fengqi You, 和 Siming
    You. 防止 LLM 驱动的智能聊天机器人在生命周期能量和碳足迹上的巨大增加。*工程*，2024. ISSN 2095-8099. URL [https://www.sciencedirect.com/science/article/pii/S2095809924002315](https://www.sciencedirect.com/science/article/pii/S2095809924002315)。
- en: 'Jing et al. (2024) Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao,
    Wei Zhou, and Liqiang Nie. Stylized data-to-text generation: A case study in the
    e-commerce domain. *ACM Transactions on Information Systems*, 42(1):25:1–25:24,
    2024. URL [https://doi.org/10.1145/3603374](https://doi.org/10.1145/3603374).'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing 等（2024）Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou,
    和 Liqiang Nie. 风格化的数据到文本生成：以电子商务领域为例。*ACM 信息系统学报*，42(1):25:1–25:24, 2024. URL
    [https://doi.org/10.1145/3603374](https://doi.org/10.1145/3603374)。
- en: Juraska et al. (2018) Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, and
    Marilyn A. Walker. A deep ensemble model with slot alignment for sequence-to-sequence
    natural language generation. In Marilyn A. Walker, Heng Ji, and Amanda Stent,
    editors, *Proceedings of the NAACL-HLT*, pages 152–162\. Association for Computational
    Linguistics, 2018. URL [https://doi.org/10.18653/v1/n18-1014](https://doi.org/10.18653/v1/n18-1014).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juraska 等（2018）Juraj Juraska, Panagiotis Karagiannis, Kevin Bowden, 和 Marilyn
    A. Walker. 带槽对齐的深度集成模型用于序列到序列的自然语言生成。收录于 Marilyn A. Walker, Heng Ji, 和 Amanda
    Stent 主编的*NAACL-HLT 会议录*，页 152–162。计算语言学协会，2018. URL [https://doi.org/10.18653/v1/n18-1014](https://doi.org/10.18653/v1/n18-1014)。
- en: 'Juraska et al. (2019) Juraj Juraska, Kevin Bowden, and Marilyn A. Walker. Viggo:
    A video game corpus for data-to-text generation in open-domain conversation. In
    Kees van Deemter, Chenghua Lin, and Hiroya Takamura, editors, *Proceedings of
    the INLG*, pages 164–172\. Association for Computational Linguistics, 2019. URL
    [https://aclanthology.org/W19-8623/](https://aclanthology.org/W19-8623/).'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juraska 等 (2019) Juraj Juraska, Kevin Bowden 和 Marilyn A. Walker. Viggo：一个用于开放域对话中数据到文本生成的视频游戏语料库。编辑：Kees
    van Deemter, Chenghua Lin 和 Hiroya Takamura，*INLG 会议论文集*，第 164–172 页。计算语言学协会，2019。网址
    [https://aclanthology.org/W19-8623/](https://aclanthology.org/W19-8623/)。
- en: Karpathy and Fei-Fei (2017) Andrej Karpathy and Li Fei-Fei. Deep visual-semantic
    alignments for generating image descriptions. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 39(4):664–676, 2017. URL [https://doi.org/10.1109/TPAMI.2016.2598339](https://doi.org/10.1109/TPAMI.2016.2598339).
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 和 Fei-Fei (2017) Andrej Karpathy 和 Li Fei-Fei. 生成图像描述的深度视觉-语义对齐。*IEEE
    计算机学会模式分析与机器智能汇刊*，39(4):664–676，2017。网址 [https://doi.org/10.1109/TPAMI.2016.2598339](https://doi.org/10.1109/TPAMI.2016.2598339)。
- en: 'Kasner and Dusek (2024) Zdenek Kasner and Ondrej Dusek. Beyond reference-based
    metrics: Analyzing behaviors of open llms on data-to-text generation. *CoRR*,
    abs/2401.10186, 2024. URL [https://doi.org/10.48550/arXiv.2401.10186](https://doi.org/10.48550/arXiv.2401.10186).'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasner 和 Dusek (2024) Zdenek Kasner 和 Ondrej Dusek. 超越基于参考的度量：分析开放式 llms 在数据到文本生成中的行为。*CoRR*，abs/2401.10186，2024。网址
    [https://doi.org/10.48550/arXiv.2401.10186](https://doi.org/10.48550/arXiv.2401.10186)。
- en: 'Kasner et al. (2023) Zdenek Kasner, Ekaterina Garanina, Ondrej Plátek, and
    Ondrej Dusek. Tabgenie: A toolkit for table-to-text generation. In Danushka Bollegala,
    Ruihong Huang, and Alan Ritter, editors, *Proceedings of the ACL*, pages 444–455\.
    Association for Computational Linguistics, 2023. URL [https://doi.org/10.18653/v1/2023.acl-demo.42](https://doi.org/10.18653/v1/2023.acl-demo.42).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasner 等 (2023) Zdenek Kasner, Ekaterina Garanina, Ondrej Plátek 和 Ondrej Dusek.
    Tabgenie：一个表格到文本生成的工具包。编辑：Danushka Bollegala, Ruihong Huang 和 Alan Ritter，*ACL
    会议论文集*，第 444–455 页。计算语言学协会，2023。网址 [https://doi.org/10.18653/v1/2023.acl-demo.42](https://doi.org/10.18653/v1/2023.acl-demo.42)。
- en: 'Kedzie and McKeown (2020) Chris Kedzie and Kathleen R. McKeown. Controllable
    meaning representation to text generation: Linearization and data augmentation
    strategies. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings
    of the EMNLP*, pages 5160–5185\. Association for Computational Linguistics, 2020.
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.419](https://doi.org/10.18653/v1/2020.emnlp-main.419).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kedzie 和 McKeown (2020) Chris Kedzie 和 Kathleen R. McKeown. 可控的意义表示到文本生成：线性化和数据增强策略。编辑：Bonnie
    Webber, Trevor Cohn, Yulan He 和 Yang Liu，*EMNLP 会议论文集*，第 5160–5185 页。计算语言学协会，2020。网址
    [https://doi.org/10.18653/v1/2020.emnlp-main.419](https://doi.org/10.18653/v1/2020.emnlp-main.419)。
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. In Yoshua Bengio and Yann LeCun, editors, *Proceedings of the ICLR*,
    2015. URL [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba (2015) Diederik P. Kingma 和 Jimmy Ba. Adam：一种用于随机优化的方法。编辑：Yoshua
    Bengio 和 Yann LeCun，*ICLR 会议论文集*，2015。网址 [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)。
- en: Lebret et al. (2016) Rémi Lebret, David Grangier, and Michael Auli. Neural text
    generation from structured data with application to the biography domain. In Jian
    Su, Xavier Carreras, and Kevin Duh, editors, *Proceedings of the EMNLP*, pages
    1203–1213\. Association for Computational Linguistics, 2016. URL [https://doi.org/10.18653/v1/d16-1128](https://doi.org/10.18653/v1/d16-1128).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lebret 等 (2016) Rémi Lebret, David Grangier 和 Michael Auli. 从结构化数据中生成神经文本，并应用于传记领域。编辑：Jian
    Su, Xavier Carreras 和 Kevin Duh，*EMNLP 会议论文集*，第 1203–1213 页。计算语言学协会，2016。网址 [https://doi.org/10.18653/v1/d16-1128](https://doi.org/10.18653/v1/d16-1128)。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing
    Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of the EMNLP*,
    pages 3045–3059\. Association for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.emnlp-main.243](https://doi.org/10.18653/v1/2021.emnlp-main.243).
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等 (2021) Brian Lester, Rami Al-Rfou 和 Noah Constant. 参数高效的提示调优的规模力量。编辑：Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia 和 Scott Wen-tau Yih，*EMNLP 会议论文集*，第 3045–3059
    页。计算语言学协会，2021。网址 [https://doi.org/10.18653/v1/2021.emnlp-main.243](https://doi.org/10.18653/v1/2021.emnlp-main.243)。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.
    Tetreault, editors, *Proceedings of the ACL*, pages 7871–7880\. Association for
    Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, 和 Luke Zettlemoyer. BART: 去噪序列到序列的预训练用于自然语言生成、翻译和理解。在
    Dan Jurafsky, Joyce Chai, Natalie Schluter, 和 Joel R. Tetreault 编辑的 *ACL 会议录*
    中，第 7871–7880 页。计算语言学协会，2020 年。网址 [https://doi.org/10.18653/v1/2020.acl-main.703](https://doi.org/10.18653/v1/2020.acl-main.703)。'
- en: Li et al. (2024a) Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari.
    Toward sustainable genai using generation directives for carbon-friendly large
    language model inference. *CoRR*, abs/2403.12900, 2024a. URL [https://doi.org/10.48550/arXiv.2403.12900](https://doi.org/10.48550/arXiv.2403.12900).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024a) Baolin Li, Yankai Jiang, Vijay Gadepally, 和 Devesh Tiwari.
    通过生成指令实现可持续的生成 AI，以便进行碳友好的大型语言模型推理。*CoRR*，abs/2403.12900，2024a。网址 [https://doi.org/10.48550/arXiv.2403.12900](https://doi.org/10.48550/arXiv.2403.12900)。
- en: Li et al. (2024b) Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu
    Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, et al. Unifying structured data
    as graph for data-to-text pre-training. *Transactions of the Association for Computational
    Linguistics*, 12:210–228, 2024b. ISSN 2307-387X. URL [https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf).
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024b) Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu
    Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, 等等. 将结构化数据统一为图用于数据到文本预训练。*计算语言学协会汇刊*，12:210–228，2024b。ISSN
    2307-387X。网址 [https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00641/2346090/tacl_a_00641.pdf)。
- en: 'Li et al. (2022) Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and
    Hua Wu. Faithfulness in natural language generation: A systematic survey of analysis,
    evaluation and optimization methods. *CoRR*, abs/2203.05227, 2022. URL [https://doi.org/10.48550/arXiv.2203.05227](https://doi.org/10.48550/arXiv.2203.05227).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022) Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, 和
    Hua Wu. 自然语言生成中的忠实度: 一项关于分析、评估和优化方法的系统综述。*CoRR*，abs/2203.05227，2022 年。网址 [https://doi.org/10.48550/arXiv.2203.05227](https://doi.org/10.48550/arXiv.2203.05227)。'
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli, editors, *Proceedings of the ACL/IJCNLP*, pages 4582–4597\. Association
    for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.acl-long.353](https://doi.org/10.18653/v1/2021.acl-long.353).'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li and Liang (2021) Xiang Lisa Li 和 Percy Liang. 前缀调优: 优化连续提示用于生成。在 Chengqing
    Zong, Fei Xia, Wenjie Li, 和 Roberto Navigli 编辑的 *ACL/IJCNLP 会议录* 中，第 4582–4597
    页。计算语言学协会，2021 年。网址 [https://doi.org/10.18653/v1/2021.acl-long.353](https://doi.org/10.18653/v1/2021.acl-long.353)。'
- en: Lin et al. (2024) Yupian Lin, Tong Ruan, Jingping Liu, and Haofen Wang. A survey
    on neural data-to-text generation. *IEEE Transactions on Knowledge and Data Engineering*,
    36(4):1431–1449, 2024. URL [https://doi.org/10.1109/TKDE.2023.3304385](https://doi.org/10.1109/TKDE.2023.3304385).
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2024) Yupian Lin, Tong Ruan, Jingping Liu, 和 Haofen Wang. 神经数据到文本生成的综述。*IEEE
    知识与数据工程汇刊*，36(4):1431–1449，2024 年。网址 [https://doi.org/10.1109/TKDE.2023.3304385](https://doi.org/10.1109/TKDE.2023.3304385)。
- en: Liu et al. (2018) Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang
    Sui. Table-to-text generation by structure-aware seq2seq learning. In Sheila A.
    McIlraith and Kilian Q. Weinberger, editors, *Proceedings of the AAAI*, pages
    4881–4888\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11925](https://doi.org/10.1609/aaai.v32i1.11925).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, 和 Zhifang
    Sui. 结构感知 seq2seq 学习的表格到文本生成。在 Sheila A. McIlraith 和 Kilian Q. Weinberger 编辑的
    *AAAI 会议录* 中，第 4881–4888 页。AAAI Press，2018 年。网址 [https://doi.org/10.1609/aaai.v32i1.11925](https://doi.org/10.1609/aaai.v32i1.11925)。
- en: 'Liu et al. (2022) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning
    across scales and tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,
    editors, *Proceedings of the ACL*, pages 61–68\. Association for Computational
    Linguistics, 2022. URL [https://doi.org/10.18653/v1/2022.acl-short.8](https://doi.org/10.18653/v1/2022.acl-short.8).'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) 刘晓、季凯旋、傅一程、谭翁、杜正霄、杨智麟和唐杰。P-tuning：提示调整在规模和任务上可以媲美微调。在斯玛兰达·穆雷桑、普雷斯拉夫·纳科夫和艾琳·维拉维辛西奥编辑的*ACL会议论文集*中，页面61–68。计算语言学协会，2022年。URL
    [https://doi.org/10.18653/v1/2022.acl-short.8](https://doi.org/10.18653/v1/2022.acl-short.8)。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized BERT pretraining approach. *CoRR*, abs/1907.11692, 2019.
    URL [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) 刘寅寒、迈尔·奥特、纳曼·戈亚尔、杜晶飞、曼达尔·乔希、陈丹奇、奥默·莱维、迈克·刘易斯、卢克·泽特尔莫耶和维塞林·斯托亚诺夫。Roberta：一种稳健优化的BERT预训练方法。*CoRR*，abs/1907.11692，2019年。URL
    [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)。
- en: Lorandi and Belz (2024) Michela Lorandi and Anya Belz. High-quality data-to-text
    generation for severely under-resourced languages with out-of-the-box large language
    models. In Yvette Graham and Matthew Purver, editors, *Proceedings of the EACL
    Findings*, pages 1451–1461\. Association for Computational Linguistics, 2024.
    URL [https://aclanthology.org/2024.findings-eacl.98](https://aclanthology.org/2024.findings-eacl.98).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorandi and Belz (2024) 米凯拉·洛兰迪和安雅·贝尔兹。针对严重资源匮乏语言的高质量数据到文本生成，使用现成的大型语言模型。在伊薇特·格雷厄姆和马修·帕弗编辑的*EACL发现论文集*中，页面1451–1461。计算语言学协会，2024年。URL
    [https://aclanthology.org/2024.findings-eacl.98](https://aclanthology.org/2024.findings-eacl.98)。
- en: Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *Proceedings of the ICLR*. OpenReview.net, 2019. URL
    [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov and Hutter (2019) 伊利亚·洛什奇洛夫和弗兰克·胡特。解耦权重衰减正则化。在*ICLR会议论文集*中。OpenReview.net，2019年。URL
    [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Lu et al. (2018) Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and Yong Yu.
    Neural text generation: Past, present and beyond. *CoRR*, abs/1803.07133, 2018.
    URL [http://arxiv.org/abs/1803.07133](http://arxiv.org/abs/1803.07133).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2018) 施迪·卢、朱耀铭、张伟南、王俊和余勇。神经文本生成：过去、现在及未来。*CoRR*，abs/1803.07133，2018年。URL
    [http://arxiv.org/abs/1803.07133](http://arxiv.org/abs/1803.07133)。
- en: Lu and Ng (2011) Wei Lu and Hwee Tou Ng. A probabilistic forest-to-string model
    for language generation from typed lambda calculus expressions. In *Proceedings
    of the EMNLP*, pages 1611–1622\. Association for Computational Linguistics, 2011.
    URL [https://aclanthology.org/D11-1149/](https://aclanthology.org/D11-1149/).
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu and Ng (2011) 卢伟和黄辉涛。从类型化lambda演算表达式生成语言的概率森林到字符串模型。在*EMNLP会议论文集*中，页面1611–1622。计算语言学协会，2011年。URL
    [https://aclanthology.org/D11-1149/](https://aclanthology.org/D11-1149/)。
- en: Luccioni et al. (2023) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure
    Ligozat. Estimating the carbon footprint of bloom, a 176b parameter language model.
    *Journal of Machine Learning Research*, 24:253:1–253:15, 2023. URL [http://jmlr.org/papers/v24/23-0069.html](http://jmlr.org/papers/v24/23-0069.html).
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luccioni et al. (2023) 亚历山德拉·萨莎·卢乔尼、希尔万·维吉耶和安妮-劳尔·利戈扎。估算Bloom（一个1760亿参数的语言模型）的碳足迹。*机器学习研究期刊*，24:253:1–253:15，2023年。URL
    [http://jmlr.org/papers/v24/23-0069.html](http://jmlr.org/papers/v24/23-0069.html)。
- en: Luo et al. (2023) Wei Er Luo, Xi Yue, and Chao Tie Zhong. Data-to-text generation
    with data control and multi-loss fusion. In *Proceedings of the FAIML*, pages
    244–247\. Association for Computing Machinery, 2023. URL [https://doi.org/10.1145/3616901.3616995](https://doi.org/10.1145/3616901.3616995).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023) 罗伟尔、徐悦和钟超铁。带有数据控制和多损失融合的数据到文本生成。在*FAIML会议论文集*中，页面244–247。计算机协会，2023年。URL
    [https://doi.org/10.1145/3616901.3616995](https://doi.org/10.1145/3616901.3616995)。
- en: Luong et al. (2015) Thang Luong, Hieu Pham, and Christopher D. Manning. Effective
    approaches to attention-based neural machine translation. In Lluís Màrquez, Chris
    Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, *Proceedings
    of the EMNLP*, pages 1412–1421\. Association for Computational Linguistics, 2015.
    URL [https://doi.org/10.18653/v1/d15-1166](https://doi.org/10.18653/v1/d15-1166).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong et al. (2015) 刘璋、阮浩和克里斯托弗·D·曼宁。基于注意力的神经机器翻译的有效方法。在刘易斯·马尔克斯、克里斯·卡利森-布赫、苏剑、达尼埃尔·皮金和尤瓦尔·马尔顿编辑的*EMNLP会议论文集*中，页面1412–1421。计算语言学协会，2015年。URL
    [https://doi.org/10.18653/v1/d15-1166](https://doi.org/10.18653/v1/d15-1166)。
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient
    fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mangrulkar 等 (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes
    Belkada, Sayak Paul 和 Benjamin Bossan. Peft: 最先进的参数高效微调方法。 [https://github.com/huggingface/peft](https://github.com/huggingface/peft)，2022。'
- en: Mei et al. (2016) Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. What to
    talk about and how? selective generation using lstms with coarse-to-fine alignment.
    In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, *Proceedings of the NAACL-HLT*,
    pages 720–730\. Association for Computational Linguistics, 2016. URL [https://doi.org/10.18653/v1/n16-1086](https://doi.org/10.18653/v1/n16-1086).
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei 等 (2016) Hongyuan Mei, Mohit Bansal 和 Matthew R. Walter. 讨论什么以及如何讨论？使用 LSTM
    进行粗到细对齐的选择性生成。收录于 Kevin Knight、Ani Nenkova 和 Owen Rambow 编辑的 *NAACL-HLT 会议录*，第
    720–730 页。计算语言学协会，2016。网址 [https://doi.org/10.18653/v1/n16-1086](https://doi.org/10.18653/v1/n16-1086)。
- en: Mikolov et al. (2010) Tomás Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký,
    and Sanjeev Khudanpur. Recurrent neural network based language model. In Takao
    Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, *Proceedings of the
    Interspeech*, pages 1045–1048\. ISCA, 2010. URL [https://doi.org/10.21437/Interspeech.2010-343](https://doi.org/10.21437/Interspeech.2010-343).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2010) Tomás Mikolov, Martin Karafiát, Lukás Burget, Jan Cernocký
    和 Sanjeev Khudanpur. 基于递归神经网络的语言模型。收录于 Takao Kobayashi、Keikichi Hirose 和 Satoshi
    Nakamura 编辑的 *Interspeech 会议录*，第 1045–1048 页。ISCA，2010。网址 [https://doi.org/10.21437/Interspeech.2010-343](https://doi.org/10.21437/Interspeech.2010-343)。
- en: 'Nan et al. (2021) Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand
    Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,
    Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia
    Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming
    Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: open-domain structured
    data record to text generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,
    Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
    and Yichao Zhou, editors, *Proceedings of the NAACL-HLT*, pages 432–447\. Association
    for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.naacl-main.37](https://doi.org/10.18653/v1/2021.naacl-main.37).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nan 等 (2021) Linyong Nan, Dragomir R. Radev, Rui Zhang, Amrit Rau, Abhinand
    Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,
    Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia
    Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming
    Xiong, Richard Socher 和 Nazneen Fatema Rajani. DART: 开放领域结构化数据记录到文本生成。收录于 Kristina
    Toutanova、Anna Rumshisky、Luke Zettlemoyer、Dilek Hakkani-Tür、Iz Beltagy、Steven
    Bethard、Ryan Cotterell、Tanmoy Chakraborty 和 Yichao Zhou 编辑的 *NAACL-HLT 会议录*，第
    432–447 页。计算语言学协会，2021。网址 [https://doi.org/10.18653/v1/2021.naacl-main.37](https://doi.org/10.18653/v1/2021.naacl-main.37)。'
- en: Nema et al. (2018) Preksha Nema, Shreyas Shetty, Parag Jain, Anirban Laha, Karthik
    Sankaranarayanan, and Mitesh M. Khapra. Generating descriptions from structured
    data using a bifocal attention mechanism and gated orthogonalization. In Marilyn A.
    Walker, Heng Ji, and Amanda Stent, editors, *Proceedings of the NAACL-HLT*, pages
    1539–1550\. Association for Computational Linguistics, 2018. URL [https://doi.org/10.18653/v1/n18-1139](https://doi.org/10.18653/v1/n18-1139).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nema 等 (2018) Preksha Nema, Shreyas Shetty, Parag Jain, Anirban Laha, Karthik
    Sankaranarayanan 和 Mitesh M. Khapra. 使用双焦点注意机制和门控正交化从结构化数据生成描述。收录于 Marilyn A.
    Walker、Heng Ji 和 Amanda Stent 编辑的 *NAACL-HLT 会议录*，第 1539–1550 页。计算语言学协会，2018。网址
    [https://doi.org/10.18653/v1/n18-1139](https://doi.org/10.18653/v1/n18-1139)。
- en: Nie et al. (2019a) Feng Nie, Jinpeng Wang, Rong Pan, and Chin-Yew Lin. An encoder
    with non-sequential dependency for neural data-to-text generation. In Kees van
    Deemter, Chenghua Lin, and Hiroya Takamura, editors, *Proceedings of the INLG*,
    pages 141–146\. Association for Computational Linguistics, 2019a. URL [https://aclanthology.org/W19-8619/](https://aclanthology.org/W19-8619/).
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等 (2019a) Feng Nie, Jinpeng Wang, Rong Pan 和 Chin-Yew Lin. 一种具有非序列依赖性的编码器用于神经数据到文本生成。收录于
    Kees van Deemter、Chenghua Lin 和 Hiroya Takamura 编辑的 *INLG 会议录*，第 141–146 页。计算语言学协会，2019a。网址
    [https://aclanthology.org/W19-8619/](https://aclanthology.org/W19-8619/)。
- en: Nie et al. (2019b) Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and Chin-Yew
    Lin. A simple recipe towards reducing hallucination in neural surface realisation.
    In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, *Proceedings of
    the ACL*, pages 2673–2679\. Association for Computational Linguistics, 2019b.
    URL [https://doi.org/10.18653/v1/p19-1256](https://doi.org/10.18653/v1/p19-1256).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等（2019b）Feng Nie、Jin-Ge Yao、Jinpeng Wang、Rong Pan 和 Chin-Yew Lin。《减少神经表面实现中的虚假信息的简单方法》。在
    Anna Korhonen、David R. Traum 和 Lluís Màrquez 编辑的*ACL 会议录*中，第 2673–2679 页。计算语言学协会，2019b。网址
    [https://doi.org/10.18653/v1/p19-1256](https://doi.org/10.18653/v1/p19-1256)。
- en: Nishino et al. (2020) Toru Nishino, Ryota Ozaki, Yohei Momoki, Tomoki Taniguchi,
    Ryuji Kano, Norihisa Nakano, Yuki Tagawa, Motoki Taniguchi, Tomoko Ohkuma, and
    Keigo Nakamura. Reinforcement learning with imbalanced dataset for data-to-text
    medical report generation. In Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings
    of the EMNLP Findings*, volume EMNLP 2020, pages 2223–2236\. Association for Computational
    Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.202](https://doi.org/10.18653/v1/2020.findings-emnlp.202).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nishino 等（2020）Toru Nishino、Ryota Ozaki、Yohei Momoki、Tomoki Taniguchi、Ryuji
    Kano、Norihisa Nakano、Yuki Tagawa、Motoki Taniguchi、Tomoko Ohkuma 和 Keigo Nakamura。《用于数据到文本医学报告生成的失衡数据集的强化学习》。在
    Trevor Cohn、Yulan He 和 Yang Liu 编辑的*EMNLP 发现录*中，第 2223–2236 页。计算语言学协会，2020。网址
    [https://doi.org/10.18653/v1/2020.findings-emnlp.202](https://doi.org/10.18653/v1/2020.findings-emnlp.202)。
- en: 'Novikova et al. (2017) Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
    The E2E dataset: New challenges for end-to-end generation. In Kristiina Jokinen,
    Manfred Stede, David DeVault, and Annie Louis, editors, *Proceedings of the SIGDIAL*,
    pages 201–206\. Association for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/w17-5525](https://doi.org/10.18653/v1/w17-5525).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novikova 等（2017）Jekaterina Novikova、Ondrej Dusek 和 Verena Rieser。《E2E 数据集：端到端生成的新挑战》。在
    Kristiina Jokinen、Manfred Stede、David DeVault 和 Annie Louis 编辑的*SIGDIAL 会议录*中，第
    201–206 页。计算语言学协会，2017。网址 [https://doi.org/10.18653/v1/w17-5525](https://doi.org/10.18653/v1/w17-5525)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed,
    A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, *Proceedings of the
    NeurIPS*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    F. Christiano、Jan Leike 和 Ryan Lowe。《训练语言模型以跟随人类反馈的指令》。在 Sanmi Koyejo、S. Mohamed、A.
    Agarwal、Danielle Belgrave、K. Cho 和 A. Oh 编辑的*NeurIPS 会议录*中，2022。网址 [http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: A method for automatic evaluation of machine translation. In *Proceedings
    of the ACL*, pages 311–318\. Association for Computational Linguistics, 2002.
    URL [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等（2002）Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu。《Bleu：一种自动评估机器翻译的方法》。在*ACL
    会议录*中，第 311–318 页。计算语言学协会，2002。网址 [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)。
- en: Pauls and Klein (2011) Adam Pauls and Dan Klein. Faster and smaller n-gram language
    models. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea, editors, *Proceedings
    of the ACL-HLT*, pages 258–267\. Association for Computer Linguistics, 2011. URL
    [https://aclanthology.org/P11-1027/](https://aclanthology.org/P11-1027/).
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pauls 和 Klein（2011）Adam Pauls 和 Dan Klein。《更快更小的 n-gram 语言模型》。在 Dekang Lin、Yuji
    Matsumoto 和 Rada Mihalcea 编辑的*ACL-HLT 会议录*中，第 258–267 页。计算语言学协会，2011。网址 [https://aclanthology.org/P11-1027/](https://aclanthology.org/P11-1027/)。
- en: Pauws et al. (2019) Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter.
    Making effective use of healthcare data using data-to-text technology. In Sergio
    Consoli, Diego Reforgiato Recupero, and Milan Petkovic, editors, *Data Science
    for Healthcare - Methodologies and Applications*, pages 119–145\. Springer, 2019.
    URL [https://doi.org/10.1007/978-3-030-05249-2_4](https://doi.org/10.1007/978-3-030-05249-2_4).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pauws 等 (2019) Steffen Pauws, Albert Gatt, Emiel Krahmer 和 Ehud Reiter. 利用数据到文本技术有效利用医疗保健数据。在
    Sergio Consoli, Diego Reforgiato Recupero 和 Milan Petkovic 编辑的 *Data Science for
    Healthcare - Methodologies and Applications* 中，第 119–145 页。Springer，2019。网址 [https://doi.org/10.1007/978-3-030-05249-2_4](https://doi.org/10.1007/978-3-030-05249-2_4)。
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated
    corpora with web data only. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
    Saenko, Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*,
    2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html).'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo 等 (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei
    和 Julien Launay. 用于 Falcon LLM 的 refinedweb 数据集：仅使用网络数据超越精心挑选的语料库。在 Alice Oh,
    Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt 和 Sergey Levine 编辑的
    *Proceedings of the NeurIPS* 中，2023。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html)。
- en: Perez-Beltrachini and Gardent (2017) Laura Perez-Beltrachini and Claire Gardent.
    Analysing data-to-text generation benchmarks. In José Maria Alonso, Alberto Bugarín,
    and Ehud Reiter, editors, *Proceedings of the INLG*, pages 238–242\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/w17-3537](https://doi.org/10.18653/v1/w17-3537).
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez-Beltrachini 和 Gardent (2017) Laura Perez-Beltrachini 和 Claire Gardent.
    分析数据到文本生成基准。在 José Maria Alonso, Alberto Bugarín 和 Ehud Reiter 编辑的 *Proceedings
    of the INLG* 中，第 238–242 页。计算语言学协会，2017。网址 [https://doi.org/10.18653/v1/w17-3537](https://doi.org/10.18653/v1/w17-3537)。
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
    representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, *Proceedings
    of the NAACL-HLT*, pages 2227–2237\. Association for Computational Linguistics,
    2018. URL [https://doi.org/10.18653/v1/n18-1202](https://doi.org/10.18653/v1/n18-1202).
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等 (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee 和 Luke Zettlemoyer. 深度上下文化词表示。在 Marilyn A. Walker,
    Heng Ji 和 Amanda Stent 编辑的 *Proceedings of the NAACL-HLT* 中，第 2227–2237 页。计算语言学协会，2018。网址
    [https://doi.org/10.18653/v1/n18-1202](https://doi.org/10.18653/v1/n18-1202)。
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. Language models
    as knowledge bases? In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan,
    editors, *Proceedings of the EMNLP*, pages 2463–2473\. Association for Computational
    Linguistics, 2019. URL [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250).
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等 (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S.
    H. Lewis, Anton Bakhtin, Yuxiang Wu 和 Alexander H. Miller. 语言模型作为知识库？在 Kentaro
    Inui, Jing Jiang, Vincent Ng 和 Xiaojun Wan 编辑的 *Proceedings of the EMNLP* 中，第
    2463–2473 页。计算语言学协会，2019。网址 [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250)。
- en: Post (2018) Matt Post. A call for clarity in reporting BLEU scores. In Ondrej
    Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry
    Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, Christof Monz, Matteo
    Negri, Aurélie Névéol, Mariana L. Neves, Matt Post, Lucia Specia, Marco Turchi,
    and Karin Verspoor, editors, *Proceedings of the WMT*, pages 186–191\. Association
    for Computational Linguistics, 2018. URL [https://doi.org/10.18653/v1/w18-6319](https://doi.org/10.18653/v1/w18-6319).
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post (2018) Matt Post. 呼吁在报告 BLEU 分数时明确。在 Ondrej Bojar, Rajen Chatterjee, Christian
    Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes,
    Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana L. Neves,
    Matt Post, Lucia Specia, Marco Turchi 和 Karin Verspoor 编辑的 *Proceedings of the
    WMT* 中，第 186–191 页。计算语言学协会，2018。网址 [https://doi.org/10.18653/v1/w18-6319](https://doi.org/10.18653/v1/w18-6319)。
- en: Puduppully and Lapata (2021) Ratish Puduppully and Mirella Lapata. Data-to-text
    generation with macro planning. *Transactions of the Association for Computational
    Linguistics*, 9:510–527, 2021. URL [https://doi.org/10.1162/tacl_a_00381](https://doi.org/10.1162/tacl_a_00381).
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puduppully 和 Lapata (2021) Ratish Puduppully 和 Mirella Lapata. 基于宏观规划的数据到文本生成。*计算语言学会期刊*，9:510–527，2021。网址
    [https://doi.org/10.1162/tacl_a_00381](https://doi.org/10.1162/tacl_a_00381)。
- en: Puduppully et al. (2019) Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text
    generation with content selection and planning. In *Proceedings of the AAAI*,
    pages 6908–6915\. AAAI Press, 2019. URL [https://doi.org/10.1609/aaai.v33i01.33016908](https://doi.org/10.1609/aaai.v33i01.33016908).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puduppully 等 (2019) Ratish Puduppully, Li Dong 和 Mirella Lapata. 带有内容选择和规划的数据到文本生成。在
    *AAAI 会议论文集* 中，第 6908–6915 页。AAAI Press，2019。网址 [https://doi.org/10.1609/aaai.v33i01.33016908](https://doi.org/10.1609/aaai.v33i01.33016908)。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019. URL [https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf).
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等. 语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019。网址 [https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:140:1–140:67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu. 使用统一的文本到文本转换器探索迁移学习的极限。*机器学习研究期刊*，21:140:1–140:67，2020。网址
    [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)。
- en: Reiter (2018) Ehud Reiter. A structured review of the validity of BLEU. *Computational
    Linguistics*, 44(3), 2018. URL [https://doi.org/10.1162/coli_a_00322](https://doi.org/10.1162/coli_a_00322).
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter (2018) Ehud Reiter. 对 BLEU 有效性的结构化评审。*计算语言学*，44(3)，2018。网址 [https://doi.org/10.1162/coli_a_00322](https://doi.org/10.1162/coli_a_00322)。
- en: Reiter and Dale (1997) Ehud Reiter and Robert Dale. Building applied natural
    language generation systems. *Natural Language Engineering*, 3(1):57–87, 1997.
    URL [https://doi.org/10.1017/S1351324997001502](https://doi.org/10.1017/S1351324997001502).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter 和 Dale (1997) Ehud Reiter 和 Robert Dale. 构建应用自然语言生成系统。*自然语言工程*，3(1):57–87，1997。网址
    [https://doi.org/10.1017/S1351324997001502](https://doi.org/10.1017/S1351324997001502)。
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. How much
    knowledge can you pack into the parameters of a language model? In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings of the EMNLP*, pages
    5418–5426\. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts 等 (2020) Adam Roberts, Colin Raffel 和 Noam Shazeer. 你能在语言模型的参数中装入多少知识？在
    Bonnie Webber、Trevor Cohn、Yulan He 和 Yang Liu 编辑的 *EMNLP 会议论文集* 中，第 5418–5426
    页。计算语言学协会，2020。网址 [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437)。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert
    Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff,
    Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina
    McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz
    Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell,
    Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy,
    Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher
    Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM:
    A 176b-parameter open-access multilingual language model. *CoRR*, abs/2211.05100,
    2022. URL [https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scao等（2022）Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana Ilic、Daniel
    Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias Gallé、Jonathan
    Tow、Alexander M. Rush、Stella Biderman、Albert Webson、Pawan Sasanka Ammanamanchi、Thomas
    Wang、Benoît Sagot、Niklas Muennighoff、Albert Villanova del Moral、Olatunji Ruwase、Rachel
    Bawden、Stas Bekman、Angelina McMillan-Major、Iz Beltagy、Huu Nguyen、Lucile Saulnier、Samson
    Tan、Pedro Ortiz Suarez、Victor Sanh、Hugo Laurençon、Yacine Jernite、Julien Launay、Margaret
    Mitchell、Colin Raffel、Aaron Gokaslan、Adi Simhi、Aitor Soroa、Alham Fikri Aji、Amit
    Alfassy、Anna Rogers、Ariel Kreisberg Nitzav、Canwen Xu、Chenghao Mou、Chris Emezue、Christopher
    Klamm、Colin Leong、Daniel van Strien、David Ifeoluwa Adelani等。BLOOM：一个176b参数的开放访问多语言模型。*CoRR*，abs/2211.05100，2022年。网址
    [https://doi.org/10.48550/arXiv.2211.05100](https://doi.org/10.48550/arXiv.2211.05100)。
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. In Regina Barzilay
    and Min-Yen Kan, editors, *Proceedings of the ACL*, pages 1073–1083\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/P17-1099](https://doi.org/10.18653/v1/P17-1099).'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See等（2017）Abigail See、Peter J. Liu和Christopher D. Manning。直击要点：使用指针-生成网络进行摘要。在Regina
    Barzilay和Min-Yen Kan编辑的*ACL会议录*中，第1073–1083页。计算语言学协会，2017年。网址 [https://doi.org/10.18653/v1/P17-1099](https://doi.org/10.18653/v1/P17-1099)。
- en: Sha et al. (2018) Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li,
    Baobao Chang, and Zhifang Sui. Order-planning neural text generation from structured
    data. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, *Proceedings of
    the AAAI*, pages 5414–5421\. AAAI Press, 2018. URL [https://doi.org/10.1609/aaai.v32i1.11947](https://doi.org/10.1609/aaai.v32i1.11947).
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sha等（2018）Lei Sha、Lili Mou、Tianyu Liu、Pascal Poupart、Sujian Li、Baobao Chang和Zhifang
    Sui。基于结构化数据的顺序规划神经文本生成。在Sheila A. McIlraith和Kilian Q. Weinberger编辑的*AAAI会议录*中，第5414–5421页。AAAI
    Press，2018年。网址 [https://doi.org/10.1609/aaai.v32i1.11947](https://doi.org/10.1609/aaai.v32i1.11947)。
- en: Shang et al. (2015) Lifeng Shang, Zhengdong Lu, and Hang Li. Neural responding
    machine for short-text conversation. In *Proceedings of the ACL/IJCNLP*, pages
    1577–1586. Association for Computational Linguistics, 2015. URL [https://doi.org/10.3115/v1/p15-1152](https://doi.org/10.3115/v1/p15-1152).
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang等（2015）Lifeng Shang、Zhengdong Lu和Hang Li。短文本对话的神经响应模型。在*ACL/IJCNLP会议录*中，第1577–1586页。计算语言学协会，2015年。网址
    [https://doi.org/10.3115/v1/p15-1152](https://doi.org/10.3115/v1/p15-1152)。
- en: Shen et al. (2019) Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. Pragmatically
    informative text generation. In Jill Burstein, Christy Doran, and Thamar Solorio,
    editors, *Proceedings of the NAACL-HLT*, pages 4060–4067\. Association for Computational
    Linguistics, 2019. URL [https://doi.org/10.18653/v1/n19-1410](https://doi.org/10.18653/v1/n19-1410).
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等（2019）Sheng Shen、Daniel Fried、Jacob Andreas和Dan Klein。实用性信息文本生成。在Jill Burstein、Christy
    Doran和Thamar Solorio编辑的*NAACL-HLT会议录*中，第4060–4067页。计算语言学协会，2019年。网址 [https://doi.org/10.18653/v1/n19-1410](https://doi.org/10.18653/v1/n19-1410)。
- en: Si et al. (2023) Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping
    Wang. An empirical study of instruction-tuning large language models in chinese.
    In Houda Bouamor, Juan Pino, and Kalika Bali, editors, *Proceedings of the EMNLP
    Findings*, pages 4086–4107\. Association for Computational Linguistics, 2023.
    URL [https://doi.org/10.18653/v1/2023.findings-emnlp.269](https://doi.org/10.18653/v1/2023.findings-emnlp.269).
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si等（2023）Qingyi Si、Tong Wang、Zheng Lin、Xu Zhang、Yanan Cao和Weiping Wang。中文指令调整大型语言模型的实证研究。在Houda
    Bouamor、Juan Pino和Kalika Bali编辑的*EMNLP Findings会议录*中，第4086–4107页。计算语言学协会，2023年。网址
    [https://doi.org/10.18653/v1/2023.findings-emnlp.269](https://doi.org/10.18653/v1/2023.findings-emnlp.269)。
- en: 'Su et al. (2021) Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel
    Collier. Plan-then-generate: Controlled data-to-text generation via planning.
    In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih,
    editors, *Proceedings of the EMNLP Findings*, pages 895–909\. Association for
    Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.findings-emnlp.76](https://doi.org/10.18653/v1/2021.findings-emnlp.76).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2021) Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, 和 Nigel Collier.
    计划后生成：通过规划进行控制的数据到文本生成。在 Marie-Francine Moens, Xuanjing Huang, Lucia Specia, 和
    Scott Wen-tau Yih 主编的 *Proceedings of the EMNLP Findings* 中，第 895–909 页。计算语言学协会，2021.
    网址 [https://doi.org/10.18653/v1/2021.findings-emnlp.76](https://doi.org/10.18653/v1/2021.findings-emnlp.76)。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence
    to sequence learning with neural networks. In Zoubin Ghahramani, Max Welling,
    Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, *Proceedings
    of the NeurIPS*, pages 3104–3112, 2014. URL [https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html).
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, 和 Quoc V. Le. 使用神经网络的序列到序列学习。在
    Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, 和 Kilian Q.
    Weinberger 主编的 *Proceedings of the NeurIPS* 中，第 3104–3112 页，2014. 网址 [https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html)。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. Stanford alpaca：一个跟随指令的
    llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023。
- en: 'Taylor (1953) Wilson L. Taylor. ‘cloze procedure’: A new tool for measuring
    readability. *Journalism Quarterly*, 30(4):415–433, 1953. ISSN 0022-5533. URL
    [https://doi.org/10.1177/107769905303000401](https://doi.org/10.1177/107769905303000401).'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor (1953) Wilson L. Taylor. ‘cloze procedure’：测量可读性的新工具。*Journalism Quarterly*,
    30(4):415–433, 1953. ISSN 0022-5533. 网址 [https://doi.org/10.1177/107769905303000401](https://doi.org/10.1177/107769905303000401)。
- en: 'Tian et al. (2019) Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P.
    Parikh. Sticking to the facts: Confident decoding for faithful data-to-text generation.
    *CoRR*, abs/1910.08684, 2019. URL [http://arxiv.org/abs/1910.08684](http://arxiv.org/abs/1910.08684).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2019) Ran Tian, Shashi Narayan, Thibault Sellam, 和 Ankur P. Parikh.
    依赖事实：为忠实的数据到文本生成进行自信解码。*CoRR*, abs/1910.08684, 2019. 网址 [http://arxiv.org/abs/1910.08684](http://arxiv.org/abs/1910.08684)。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023. URL
    [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. Llama 2：开放基础和微调的聊天模型。*CoRR*,
    abs/2307.09288, 2023. 网址 [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)。
- en: Tulchinskii et al. (2023) Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva,
    Daniil Cherniavskii, Sergey I. Nikolenko, Evgeny Burnaev, Serguei Barannikov,
    and Irina Piontkovskaya. Intrinsic dimension estimation for robust detection of
    ai-generated texts. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*, 2023.
    URL [http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html).
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulchinskii等人（2023）Eduard Tulchinskii、Kristian Kuznetsov、Laida Kushnareva、Daniil
    Cherniavskii、Sergey I. Nikolenko、Evgeny Burnaev、Serguei Barannikov和Irina Piontkovskaya。用于鲁棒检测AI生成文本的内在维度估计。在Alice
    Oh、Tristan Naumann、Amir Globerson、Kate Saenko、Moritz Hardt和Sergey Levine编辑的*NeurIPS会议录*中，2023年。网址
    [http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html)。
- en: ul Islam et al. (2023) Saad Obaid ul Islam, Iza Skrjanec, Ondrej Dusek, and
    Vera Demberg. Tackling hallucinations in neural chart summarization. In C. Maria
    Keet, Hung-Yi Lee, and Sina Zarrieß, editors, *Proceedings of the INLG*, pages
    414–423\. Association for Computational Linguistics, 2023. URL [https://doi.org/10.18653/v1/2023.inlg-main.30](https://doi.org/10.18653/v1/2023.inlg-main.30).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ul Islam等人（2023）Saad Obaid ul Islam、Iza Skrjanec、Ondrej Dusek和Vera Demberg。应对神经图表总结中的虚假信息。在C.
    Maria Keet、Hung-Yi Lee和Sina Zarrieß编辑的*INLG会议录*中，第414–423页。计算语言学协会，2023年。网址 [https://doi.org/10.18653/v1/2023.inlg-main.30](https://doi.org/10.18653/v1/2023.inlg-main.30)。
- en: Upadhyay and Massie (2023) Ashish Upadhyay and Stewart Massie. CBR assisted
    context-aware surface realisation for data-to-text generation. In Stewart Massie
    and Sutanu Chakraborti, editors, *Proceedings of the ICCBR*, volume 14141, pages
    34–49\. Springer, 2023. URL [https://doi.org/10.1007/978-3-031-40177-0_3](https://doi.org/10.1007/978-3-031-40177-0_3).
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Upadhyay和Massie（2023）Ashish Upadhyay和Stewart Massie。CBR辅助的上下文感知表面实现，用于数据到文本生成。在Stewart
    Massie和Sutanu Chakraborti编辑的*ICCBR会议录*中，第14141卷，第34–49页。Springer，2023年。网址 [https://doi.org/10.1007/978-3-031-40177-0_3](https://doi.org/10.1007/978-3-031-40177-0_3)。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, *Proceedings of
    the NeurIPS*, pages 5998–6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人（2017）Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。注意力机制才是关键。在Isabelle Guyon、Ulrike
    von Luxburg、Samy Bengio、Hanna M. Wallach、Rob Fergus、S. V. N. Vishwanathan和Roman
    Garnett编辑的*NeurIPS会议录*中，第5998–6008页，2017年。网址 [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)。
- en: 'Vinyals et al. (2016) Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order
    matters: Sequence to sequence for sets. In Yoshua Bengio and Yann LeCun, editors,
    *Proceedings of the ICLR*, 2016. URL [http://arxiv.org/abs/1511.06391](http://arxiv.org/abs/1511.06391).'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals等人（2016）Oriol Vinyals、Samy Bengio和Manjunath Kudlur。顺序的重要性：集合的序列到序列。在Yoshua
    Bengio和Yann LeCun编辑的*ICLR会议录*中，2016年。网址 [http://arxiv.org/abs/1511.06391](http://arxiv.org/abs/1511.06391)。
- en: Wang et al. (2020) Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou
    Chen. Towards faithful neural table-to-text generation with content-matching constraints.
    In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,
    *Proceedings of the ACL*, pages 1072–1086\. Association for Computational Linguistics,
    2020. URL [https://doi.org/10.18653/v1/2020.acl-main.101](https://doi.org/10.18653/v1/2020.acl-main.101).
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2020）Zhenyi Wang、Xiaoyang Wang、Bang An、Dong Yu和Changyou Chen。朝着真实的神经表格到文本生成，结合内容匹配约束。在Dan
    Jurafsky、Joyce Chai、Natalie Schluter和Joel R. Tetreault编辑的*ACL会议录*中，第1072–1086页。计算语言学协会，2020年。网址
    [https://doi.org/10.18653/v1/2020.acl-main.101](https://doi.org/10.18653/v1/2020.acl-main.101)。
- en: Wen et al. (2015) Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-hao Su,
    David Vandyke, and Steve J. Young. Semantically conditioned lstm-based natural
    language generation for spoken dialogue systems. In Lluís Màrquez, Chris Callison-Burch,
    Jian Su, Daniele Pighin, and Yuval Marton, editors, *Proceedings of the EMNLP*,
    pages 1711–1721\. Association for Computational Linguistics, 2015. URL [https://doi.org/10.18653/v1/d15-1199](https://doi.org/10.18653/v1/d15-1199).
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温等人 (2015) Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-hao Su, David Vandyke,
    和 Steve J. Young. 基于语义条件的 LSTM 自然语言生成用于对话系统。编者 Lluís Màrquez, Chris Callison-Burch,
    Jian Su, Daniele Pighin, 和 Yuval Marton, *EMNLP 会议论文集*, 页码 1711–1721。计算语言学协会,
    2015. URL [https://doi.org/10.18653/v1/d15-1199](https://doi.org/10.18653/v1/d15-1199).
- en: Wiseman et al. (2017) Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush.
    Challenges in data-to-document generation. In Martha Palmer, Rebecca Hwa, and
    Sebastian Riedel, editors, *Proceedings of the EMNLP*, pages 2253–2263\. Association
    for Computational Linguistics, 2017. URL [https://doi.org/10.18653/v1/d17-1239](https://doi.org/10.18653/v1/d17-1239).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiseman 等人 (2017) Sam Wiseman, Stuart M. Shieber, 和 Alexander M. Rush. 数据到文档生成中的挑战。编者
    Martha Palmer, Rebecca Hwa, 和 Sebastian Riedel, *EMNLP 会议论文集*, 页码 2253–2263。计算语言学协会,
    2017. URL [https://doi.org/10.18653/v1/d17-1239](https://doi.org/10.18653/v1/d17-1239).
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. Transformers: State-of-the-art natural language processing.
    In Qun Liu and David Schlangen, editors, *Proceedings of the EMNLP*, pages 38–45\.
    Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653/v1/2020.emnlp-demos.6).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等人 (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander M. Rush. Transformers: 先进的自然语言处理技术。编者 Qun Liu 和 David Schlangen, *EMNLP
    会议论文集*, 页码 38–45。计算语言学协会, 2020. URL [https://doi.org/10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653/v1/2020.emnlp-demos.6).'
- en: 'Wu and Aji (2023) Minghao Wu and Alham Fikri Aji. Style over substance: Evaluation
    biases for large language models. *CoRR*, abs/2307.03025, 2023. URL [https://doi.org/10.48550/arXiv.2307.03025](https://doi.org/10.48550/arXiv.2307.03025).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴和阿吉 (2023) Minghao Wu 和 Alham Fikri Aji. 风格重于实质: 大语言模型的评估偏差。*CoRR*, abs/2307.03025,
    2023. URL [https://doi.org/10.48550/arXiv.2307.03025](https://doi.org/10.48550/arXiv.2307.03025).'
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell:
    Neural image caption generation with visual attention. In Francis R. Bach and
    David M. Blei, editors, *Proceedings of the ICML*, volume 37, pages 2048–2057\.
    Proceedings of Machine Learning Research, 2015. URL [http://proceedings.mlr.press/v37/xuc15.html](http://proceedings.mlr.press/v37/xuc15.html).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '徐等人 (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, 和 Yoshua Bengio. 展示、关注和讲述: 具有视觉注意力的神经图像描述生成。编者
    Francis R. Bach 和 David M. Blei, *ICML 会议论文集*, 卷 37, 页码 2048–2057。机器学习研究会议, 2015.
    URL [http://proceedings.mlr.press/v37/xuc15.html](http://proceedings.mlr.press/v37/xuc15.html).'
- en: Yang et al. (2024) Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann
    Heng, and Wai Lam. Unveiling the generalization power of fine-tuned large language
    models. *CoRR*, abs/2403.09162, 2024. URL [https://doi.org/10.48550/arXiv.2403.09162](https://doi.org/10.48550/arXiv.2403.09162).
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人 (2024) Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann Heng,
    和 Wai Lam. 揭示微调大语言模型的泛化能力。*CoRR*, abs/2403.09162, 2024. URL [https://doi.org/10.48550/arXiv.2403.09162](https://doi.org/10.48550/arXiv.2403.09162).
- en: Ye et al. (2020) Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational
    template machine for data-to-text generation. In *Proceedings of the ICLR*. OpenReview.net,
    2020. URL [https://openreview.net/forum?id=HkejNgBtPB](https://openreview.net/forum?id=HkejNgBtPB).
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶等人 (2020) Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, 和 Lei Li. 用于数据到文本生成的变分模板机器。*ICLR
    会议论文集*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=HkejNgBtPB](https://openreview.net/forum?id=HkejNgBtPB).
- en: Yermakov et al. (2021) Ruslan Yermakov, Nicholas Drago, and Angelo Ziletti.
    Biomedical data-to-text generation via fine-tuning transformers. In Anya Belz,
    Angela Fan, Ehud Reiter, and Yaji Sripada, editors, *Proceedings of the INLG*,
    pages 364–370\. Association for Computational Linguistics, 2021. URL [https://doi.org/10.18653/v1/2021.inlg-1.40](https://doi.org/10.18653/v1/2021.inlg-1.40).
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yermakov 等（2021）Ruslan Yermakov, Nicholas Drago 和 Angelo Ziletti。通过微调变换器进行生物医学数据到文本生成。载于
    Anya Belz, Angela Fan, Ehud Reiter 和 Yaji Sripada 主编的 *Proceedings of the INLG*，页码
    364–370。计算语言学协会，2021。网址 [https://doi.org/10.18653/v1/2021.inlg-1.40](https://doi.org/10.18653/v1/2021.inlg-1.40)。
- en: 'Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore:
    Evaluating generated text as text generation. In Marc’Aurelio Ranzato, Alina Beygelzimer,
    Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, *Proceedings
    of the NeurIPS*, pages 27263–27277, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等（2021）Weizhe Yuan, Graham Neubig 和 Pengfei Liu。Bartscore: 作为文本生成的文本评估。载于
    Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang 和 Jennifer
    Wortman Vaughan 主编的 *Proceedings of the NeurIPS*，页码 27263–27277，2021。网址 [https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html)。'
- en: Zhang et al. (2024) Min Zhang, Jianfeng He, Shuo Lei, Murong Yue, Linhan Wang,
    and Chang-Tien Lu. Can llm find the green circle? investigation and human-guided
    tool manipulation for compositional generalization. In *Proceedings of the ICASSP*,
    pages 11996–12000, 2024. URL [https://www.doi.org/10.1109/ICASSP48485.2024.10446355](https://www.doi.org/10.1109/ICASSP48485.2024.10446355).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024）Min Zhang, Jianfeng He, Shuo Lei, Murong Yue, Linhan Wang 和 Chang-Tien
    Lu。LLM 能找到绿色圆圈吗？调查和人为指导的工具操作以实现组合泛化。载于 *Proceedings of the ICASSP*，页码 11996–12000，2024。网址
    [https://www.doi.org/10.1109/ICASSP48485.2024.10446355](https://www.doi.org/10.1109/ICASSP48485.2024.10446355)。
- en: 'Zhang et al. (2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei
    Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction
    tuning for large language models: A survey. *CoRR*, abs/2308.10792, 2023. URL
    [https://doi.org/10.48550/arXiv.2308.10792](https://doi.org/10.48550/arXiv.2308.10792).'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun,
    Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu 和 Guoyin Wang。大规模语言模型的指令调优：综述。*CoRR*，abs/2308.10792，2023。网址
    [https://doi.org/10.48550/arXiv.2308.10792](https://doi.org/10.48550/arXiv.2308.10792)。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria
    Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained
    transformer language models. *CoRR*, abs/2205.01068, 2022. URL [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang 和 Luke Zettlemoyer。OPT: 开源预训练变换器语言模型。*CoRR*，abs/2205.01068，2022。网址
    [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068)。'
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In *Proceedings
    of the ICLR*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SkeHuCVFDr](https://openreview.net/forum?id=SkeHuCVFDr).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2020）Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger 和
    Yoav Artzi。Bertscore: 使用 BERT 评估文本生成。载于 *Proceedings of the ICLR*。OpenReview.net，2020。网址
    [https://openreview.net/forum?id=SkeHuCVFDr](https://openreview.net/forum?id=SkeHuCVFDr)。'
- en: 'Zhao et al. (2024) Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. Expel: LLM agents are experiential learners. In Michael J.
    Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, *Proceedings of the
    AAAI*, pages 19632–19642\. AAAI Press, 2024. URL [https://doi.org/10.1609/aaai.v38i17.29936](https://doi.org/10.1609/aaai.v38i17.29936).'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2024）Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu
    和 Gao Huang。Expel: LLM 代理是体验型学习者。载于 Michael J. Wooldridge, Jennifer G. Dy 和 Sriraam
    Natarajan 主编的 *Proceedings of the AAAI*，页码 19632–19642。AAAI Press，2024。网址 [https://doi.org/10.1609/aaai.v38i17.29936](https://doi.org/10.1609/aaai.v38i17.29936)。'
- en: 'Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M.
    Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized
    embeddings and earth mover distance. In Kentaro Inui, Jing Jiang, Vincent Ng,
    and Xiaojun Wan, editors, *Proceedings of the EMNLP*, pages 563–578\. Association
    for Computational Linguistics, 2019. URL [https://doi.org/10.18653/v1/D19-1053](https://doi.org/10.18653/v1/D19-1053).'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2019) Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M.
    Meyer, 和 Steffen Eger. Moverscore: 使用上下文嵌入和地球移动者距离评估文本生成。见 Kentaro Inui, Jing
    Jiang, Vincent Ng, 和 Xiaojun Wan 编辑的 *EMNLP 会议录*，第 563–578 页。计算语言学协会，2019年。网址
    [https://doi.org/10.18653/v1/D19-1053](https://doi.org/10.18653/v1/D19-1053)。'
- en: 'Zheng et al. (2023) Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang,
    and Yang You. Response length perception and sequence scheduling: An llm-empowered
    LLM inference pipeline. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,
    Moritz Hardt, and Sergey Levine, editors, *Proceedings of the NeurIPS*, 2023.
    URL [http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang,
    和 Yang You. 响应长度感知与序列调度：一个由 llm 驱动的 LLM 推理管道。见 Alice Oh, Tristan Naumann, Amir
    Globerson, Kate Saenko, Moritz Hardt, 和 Sergey Levine 编辑的 *NeurIPS 会议录*，2023年。网址
    [http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html)。
