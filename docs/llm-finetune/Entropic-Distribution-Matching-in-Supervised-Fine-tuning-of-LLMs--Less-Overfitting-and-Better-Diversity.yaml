- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:34:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:34:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**有监督微调中的熵分布匹配：减少过拟合和提高多样性**'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16673](https://ar5iv.labs.arxiv.org/html/2408.16673)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16673](https://ar5iv.labs.arxiv.org/html/2408.16673)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable
- en: Ziniu Li The Chinese University of Hong Kong, Shenzhen Shenzhen Research Institute
    of Big Data Congliang Chen The Chinese University of Hong Kong, Shenzhen Shenzhen
    Research Institute of Big Data Tian Xu Nanjing University Zeyu Qin Hong Kong University
    of Science and Technology Jiancong Xiao University of Pennsylvania Ruoyu Sun Corresponding
    author. The Chinese University of Hong Kong, Shenzhen Shenzhen Research Institute
    of Big Data Zhi-Quan Luo The Chinese University of Hong Kong, Shenzhen Shenzhen
    Research Institute of Big Data
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 李自如 香港中文大学（深圳） 深圳大数据研究院 陈聪亮 香港中文大学（深圳） 深圳大数据研究院 许天 南京大学 秦泽宇 香港科技大学 肖建聪 宾夕法尼亚大学
    孙若宇 通讯作者。 香港中文大学（深圳） 深圳大数据研究院 罗智全 香港中文大学（深圳） 深圳大数据研究院
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models rely on Supervised Fine-Tuning (SFT) to specialize in
    downstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it
    often leads to overfitting and limited output diversity due to its aggressive
    updates to the data distribution. This paper aim to address these issues by introducing
    the maximum entropy principle, which favors models with flatter distributions
    that still effectively capture the data. Specifically, we develop a new distribution
    matching method called GEM, which solves reverse Kullback-Leibler divergence minimization
    with an entropy regularizer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型依赖于有监督微调（SFT）来专注于下游任务。交叉熵（CE）损失是SFT中的实际选择，但由于其对数据分布的激进更新，常常导致过拟合和有限的输出多样性。本文旨在通过引入最大熵原理来解决这些问题，该原理偏爱具有较平坦分布的模型，同时仍然有效地捕捉数据。具体而言，我们开发了一种新的分布匹配方法，称为GEM，该方法通过熵正则化器解决了反Kullback-Leibler散度最小化问题。
- en: For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects. First,
    when applied to the UltraFeedback dataset to develop general instruction-following
    abilities, GEM exhibits reduced overfitting, evidenced by lower perplexity and
    better performance on the IFEval benchmark. Furthermore, GEM enhances output diversity,
    leading to performance gains of up to 7 points on math reasoning and code generation
    tasks using best-of-n sampling, even without domain-specific data. Second, when
    fine-tuning with domain-specific datasets for math reasoning and code generation,
    GEM also shows less overfitting and improvements of up to 10 points compared with
    CE.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama-3-8B模型的SFT，GEM在多个方面优于CE。首先，在应用于UltraFeedback数据集以开发通用指令跟随能力时，GEM表现出减少的过拟合，这从较低的困惑度和在IFEval基准上的更好表现可以看出。此外，GEM提高了输出多样性，即使在没有特定领域数据的情况下，通过最优采样法在数学推理和代码生成任务中也获得了高达7分的性能提升。其次，当使用特定领域的数据集进行数学推理和代码生成的微调时，GEM相比CE也显示出较少的过拟合，并且性能提高了高达10分。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[38](#bib.bib38), [53](#bib.bib53), [52](#bib.bib52)]
    are powerful generative models excelling in specialized tasks across various fields.
    Typically, LLMs first go through a pre-training stage [[42](#bib.bib42), [7](#bib.bib7)],
    where they learn to predict the next token from a large corpus of texts, such
    as books, scientific papers, and code. Despite this extensive pre-training, LLMs
    often struggle to follow instructions and answer users’ queries effectively, because
    such scenarios are not commonly encountered during pre-training. To improve their
    performance in these tasks, instruction tuning [[44](#bib.bib44), [60](#bib.bib60),
    [10](#bib.bib10)], also known as Supervised Fine-Tuning (SFT) [[39](#bib.bib39),
    [3](#bib.bib3)], is employed. This process involves using high-quality labeled
    data (i.e., prompt-response pairs) and typically utilizes supervised learning
    with Cross Entropy (CE) loss to maximize the likelihood of the labeled data. Later
    on, these models may be further aligned with human preferences to ensure their
    outputs align with human values [[39](#bib.bib39), [3](#bib.bib3)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[38](#bib.bib38), [53](#bib.bib53), [52](#bib.bib52)] 是强大的生成模型，擅长于各种领域的专业任务。通常，LLMs
    首先经过预训练阶段 [[42](#bib.bib42), [7](#bib.bib7)]，在这一阶段，它们通过大量的文本语料（如书籍、科学论文和代码）学习预测下一个标记。尽管有如此广泛的预训练，LLMs
    往往在遵循指令和有效回答用户查询时存在困难，因为这些场景在预训练期间并不常见。为了提高这些任务中的表现，采用了指令调优 [[44](#bib.bib44),
    [60](#bib.bib60), [10](#bib.bib10)]，也称为监督微调（SFT）[[39](#bib.bib39), [3](#bib.bib3)]。这一过程涉及使用高质量的标注数据（即提示-回应对），通常利用带有交叉熵（CE）损失的监督学习来最大化标注数据的可能性。随后，这些模型可能会进一步与人类偏好对齐，以确保它们的输出符合人类价值观
    [[39](#bib.bib39), [3](#bib.bib3)]。
- en: SFT elicits the knowledge acquired from pre-training to answer various downstream
    questions and further paves the way for future developments, making it crucial
    part of the post-training pipeline [[68](#bib.bib68), [54](#bib.bib54), [34](#bib.bib34),
    [65](#bib.bib65)]. We expect models to generalize well by providing accurate answers
    and hope these answers are diverse as well. While the importance of generalization
    is clear, we also stress the significance of generation diversity. In creative
    writing, diversity sparks new ideas [[12](#bib.bib12)], and in chit-chat dialogues,
    users often appreciate having multiple output options to suit their preferences
    [[30](#bib.bib30)]. Many modern AI interfaces, such as ChatGPT and Claude AI,
    recognize this need by incorporating features like regeneration buttons. Additionally,
    generation diversity is vital when advanced generation algorithms are applied
    in LLMs to tackle complex tasks [[50](#bib.bib50)]. For example, the best-of-n
    sampling, commonly used in math reasoning [[50](#bib.bib50)] and code generation
    [[8](#bib.bib8)], benefits from selecting the optimal response from a diverse
    set of generated options.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 从预训练中提取知识以回答各种下游问题，并进一步为未来的发展铺平道路，使其成为后训练流程中的关键部分 [[68](#bib.bib68), [54](#bib.bib54),
    [34](#bib.bib34), [65](#bib.bib65)]。我们期望模型能够通过提供准确的答案来实现良好的泛化，并希望这些答案也能具有多样性。虽然泛化的重要性显而易见，但我们也强调生成多样性的意义。在创意写作中，多样性激发新的想法
    [[12](#bib.bib12)]，而在闲聊对话中，用户通常会欣赏能够满足他们偏好的多种输出选项 [[30](#bib.bib30)]。许多现代 AI 界面，如
    ChatGPT 和 Claude AI，通过引入如重生按钮等功能来识别这一需求。此外，当在 LLMs 中应用先进的生成算法以处理复杂任务时，生成多样性至关重要
    [[50](#bib.bib50)]。例如，最佳 n 采样，常用于数学推理 [[50](#bib.bib50)] 和代码生成 [[8](#bib.bib8)]，通过从多样化的生成选项中选择最佳响应来获得好处。
- en: '![Refer to caption](img/2a6ee4572a69c65650039af1ec30c655.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a6ee4572a69c65650039af1ec30c655.png)'
- en: 'Figure 1: Illustration of the difference between the standard CE and the proposed
    method GEM for SFT of LLMs.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：标准 CE 方法与提议的 GEM 方法在 LLMs SFT 中的差异示意图。
- en: 'However, models trained using CE loss in SFT often struggle with overfitting
    (see, e.g., [[16](#bib.bib16), [17](#bib.bib17)]) and poor generation diversity
    (see, e.g., [[40](#bib.bib40), [37](#bib.bib37)]). In theory, optimizing CE loss
    corresponds to minimizing the *forward* Kullback–Leibler (KL) divergence between
    the data distribution and the generative distribution of the LLM.¹¹1It is called
    “forward” KL because the loss is defined across the data distribution. We will
    later discuss *reverse* KL approaches, where the loss is defined across the generative
    model’s distribution. This distribution matching process aggressively increases
    the likelihood of observed data, which often exhibit narrow coverage of real distributions
    that have diverse outcomes that we want the LLM to learn. However, the CE loss
    is unaware of this, and the optimized model biases toward low-entropy distributions,
    resulting in reduced output diversity. One particular example is shown in [Figure 1](#S1.F1
    "In 1 Introduction ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") (details are provided in [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")). Prior research
    [[41](#bib.bib41), [14](#bib.bib14)] links low-entropy predictive distributions
    with poor generalization, indicating that these issues are interconnected. While
    practitioners often use weight decay regularization with CE loss [[39](#bib.bib39),
    [3](#bib.bib3)], it does not fully address these problems, necessitating more
    principled approaches.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用CE损失进行SFT训练的模型常常面临过拟合（例如，见[[16](#bib.bib16)，[17](#bib.bib17)]）和生成多样性差（例如，见[[40](#bib.bib40)，[37](#bib.bib37)]）的问题。理论上，优化CE损失对应于最小化数据分布与LLM生成分布之间的*前向*Kullback–Leibler
    (KL)散度。¹¹1它被称为“前向”KL，因为损失是定义在数据分布上的。我们稍后将讨论*反向*KL方法，其中损失是定义在生成模型的分布上的。这种分布匹配过程会积极增加观测数据的可能性，而这些数据通常表现出狭窄的实际分布范围，这些实际分布具有我们希望LLM学习的多样化结果。然而，CE损失对此并不敏感，优化后的模型倾向于低熵分布，从而导致输出多样性减少。一个具体的例子见[图1](#S1.F1
    "在1介绍 ‣ LLM的监督微调中的熵分布匹配：减少过拟合和改善多样性")（详细信息见[附录E](#A5 "附录E 额外结果 ‣ LLM的监督微调中的熵分布匹配：减少过拟合和改善多样性")）。先前的研究[[41](#bib.bib41)，[14](#bib.bib14)]将低熵预测分布与差的泛化能力联系起来，表明这些问题是相互关联的。尽管从业者通常使用权重衰减正则化与CE损失[[39](#bib.bib39)，[3](#bib.bib3)]，但这并没有完全解决这些问题，仍需更有原则的方法。
- en: 'Our contributions. We formulate fine-tuning of LLMs as a distribution matching
    problem and propose two principles. The first principle advocates *generative*
    distribution matching methods to encourage the model learns from both ground truth
    supervision and its own generated mistakes. This contrasts with the passive imitation
    of supervision used in the CE loss. The second principle is that the model should
    assign higher probabilities to the observed data while preventing over-memorization²²2By
    over-memorization, we mean that the probabilities of the supervised data become
    overwhelmingly dominant in the distributions after learning., especially when
    dealing with the limited data. To implement these principles, we study the formulation
    of reverse KL divergence minimization with entropy regularization. However, this
    formulation is technically challenging and may require adversarial training techniques
    akin to those used in GANs [[18](#bib.bib18)]. Our main technical contribution
    is the development of a new training algorithm, referred to as GEM (Generative
    Entropy-regularized Matching of distributions), that addresses the above challenge
    and is tractable as the CE loss. By adhering to the proposed principles, GEM favors
    flatter distributions that effectively capture the data; see [Figure 1](#S1.F1
    "In 1 Introduction ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") for an illustration.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献。我们将大规模语言模型（LLM）的微调形式化为分布匹配问题，并提出了两个原则。第一个原则提倡使用*生成*分布匹配方法，以鼓励模型从实际的监督和自身生成的错误中学习。这与在CE损失中被动模仿监督的做法形成了对比。第二个原则是模型应为观察到的数据分配更高的概率，同时防止过度记忆²²2过度记忆是指在学习后，监督数据的概率在分布中变得占据压倒性主导地位。，尤其是在处理有限数据时。为了实现这些原则，我们研究了带有熵正则化的反向KL散度最小化的形式。然而，这种形式化在技术上具有挑战性，可能需要类似于GAN中使用的对抗训练技术[[18](#bib.bib18)]。我们主要的技术贡献是开发了一种新的训练算法，称为GEM（生成熵正则化分布匹配），它解决了上述挑战，并且与CE损失一样可处理。通过遵循这些提议的原则，GEM倾向于产生更平坦的分布，从而有效地捕捉数据；见[图1](#S1.F1
    "在1 引言 ‣ 在LLMs的监督微调中的熵分布匹配：更少的过拟合和更好的多样性")以供说明。
- en: We demonstrate the effectiveness of our model by fine-tuning the Llama-3-8B
    pre-trained model³³3[https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
    with two types of datasets. First, we fine-tune the model using the UltraFeedback
    dataset [[13](#bib.bib13)] to develop general instruction-following abilities.
    Our results show that GEM achieves lower evaluation perplexity than CE and better
    performance on the IFEval benchmark [[69](#bib.bib69)], indicating reduced overfitting.
    We also assess output diversity by evaluating the model’s ability to generate
    varied content in creative tasks, such as poem and story writing [[36](#bib.bib36)].
    Our findings show that GEM significantly enhances output diversity. This improved
    diversity translates into performance gains in math reasoning (GSM8K [[11](#bib.bib11)])
    and code generation tasks (HumanEval [[8](#bib.bib8)] and MBPP [[2](#bib.bib2)])
    when utilizing advanced generation strategies like Majority Voting (MV) and Best-Of-N
    (BON). In these tasks, GEM achieves performance improvements of up to 7 points.
    In a second experiment, we fine-tune the model on domain-specific datasets for
    specialized abilities in math reasoning (MetaMathQA dataset [[66](#bib.bib66)])
    and code generation (MagicCoder-OSS-Instruct dataset [[62](#bib.bib62)]), seperately.
    In this setting, GEM can outperform CE by up to 10 points when using MV and BON,
    further demonstrating its effectiveness. Overall, these results suggest that GEM
    can mitigate overfitting and improve output diversity compared with CE.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用两种类型的数据集对预训练的Llama-3-8B模型³³3[https://huggingface.co/meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)进行微调，来展示我们模型的有效性。首先，我们使用UltraFeedback数据集[[13](#bib.bib13)]对模型进行微调，以培养通用的指令跟随能力。我们的结果表明，GEM在评估困惑度方面优于CE，并且在IFEval基准测试[[69](#bib.bib69)]中表现更好，表明过拟合有所减少。我们还通过评估模型在创造性任务中的内容生成多样性，例如诗歌和故事写作[[36](#bib.bib36)]，来评估输出的多样性。我们的发现表明，GEM显著增强了输出的多样性。这种改进的多样性在数学推理（GSM8K
    [[11](#bib.bib11)])和代码生成任务（HumanEval [[8](#bib.bib8)]和MBPP [[2](#bib.bib2)])中转化为性能提升，当使用高级生成策略如多数投票（MV）和最佳-N（BON）时。在这些任务中，GEM的性能提升达到7个百分点。在第二个实验中，我们分别在领域特定的数据集上对模型进行微调，以增强数学推理（MetaMathQA数据集[[66](#bib.bib66)])和代码生成（MagicCoder-OSS-Instruct数据集[[62](#bib.bib62)]）的能力。在这种设置下，GEM在使用MV和BON时可比CE高出最多10个百分点，进一步证明了其有效性。总体而言，这些结果表明，与CE相比，GEM可以减少过拟合并提高输出多样性。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Supervised Fine-tuning. SFT is the first stage of the post-training pipeline
    and plays an important role in subsequent developments. As mentioned in the introduction,
    using CE loss during the SFT stage often leads to overfitting and reduced output
    diversity. To address this, there is a line of research in scaling up SFT data
    (see, e.g., [[66](#bib.bib66), [62](#bib.bib62), [67](#bib.bib67)]), which, while
    effective, increases computational burden. Our work aims to develop training methods
    that more effectively leverage supervised data to mitigate overfitting and to
    enhance output diversity. While recent studies such as [[9](#bib.bib9), [29](#bib.bib29)]
    attempt to improve CE-trained models through techniques like self-play, we aim
    to address the limitations of CE loss and design methods that can improve the
    pre-trained models directly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调。SFT是后训练管道的第一阶段，在后续发展中发挥重要作用。如引言中所提到的，SFT阶段使用CE损失往往会导致过拟合和输出多样性降低。为了解决这个问题，有一系列研究致力于扩大SFT数据（例如，[[66](#bib.bib66),
    [62](#bib.bib62), [67](#bib.bib67)]），虽然有效，但会增加计算负担。我们的工作旨在开发更有效利用监督数据的训练方法，以减轻过拟合并增强输出多样性。虽然最近的研究如[[9](#bib.bib9),
    [29](#bib.bib29)]尝试通过自我对弈等技术来改进CE训练的模型，但我们旨在解决CE损失的局限性，并设计可以直接改进预训练模型的方法。
- en: We also emphasize the importance of output diversity based on previous research.
    SFT-trained models are often further refined through Reinforcement Learning from
    Human Feedback (RLHF) to better align with human values [[39](#bib.bib39), [3](#bib.bib3)].
    Xiao et al. [[64](#bib.bib64)] studied the impact of SFT models on preference
    learning in RLHF, showing that if an SFT model collapses (i.e., becomes biased
    toward certain outputs with near-certain probability), it may further lead to
    preference collapse in the alignment. Their findings highlight the need to address
    collapse during the SFT stage. Additionally, SFT-trained models are often used
    as synthetic data generators for self-improvement (see, e.g., [[1](#bib.bib1),
    [15](#bib.bib15)]). In this context, maintaining output diversity is crucial.
    It helps find better solutions and alleviate the mode collapse issue [[19](#bib.bib19),
    [49](#bib.bib49)]. For specialized applications, Wang et al. [[59](#bib.bib59)]
    highlighted the benefits of output diversity through majority voting in math reasoning.
    Recent research [[6](#bib.bib6), [50](#bib.bib50)] have explored enhancing search
    over multiple generated responses with a verifier, showing that scaling up test-time
    compute is effective. Unlike these studies focused on inference-time techniques,
    our paper improves SFT training methods to promote diversity.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还强调了基于先前研究的输出多样性的重要性。SFT 训练的模型通常会通过来自人类反馈的强化学习 (RLHF) 进一步优化，以更好地与人类价值观对齐 [[39](#bib.bib39),
    [3](#bib.bib3)]。Xiao 等人 [[64](#bib.bib64)] 研究了 SFT 模型对 RLHF 中偏好学习的影响，表明如果一个 SFT
    模型发生崩溃（即，偏向某些输出的概率接近确定），它可能进一步导致对齐中的偏好崩溃。他们的发现强调了在 SFT 阶段解决崩溃问题的必要性。此外，SFT 训练的模型通常被用作自我改进的合成数据生成器（参见，例如，[[1](#bib.bib1),
    [15](#bib.bib15)]）。在这种情况下，保持输出多样性至关重要。它有助于找到更好的解决方案，并缓解模式崩溃问题 [[19](#bib.bib19),
    [49](#bib.bib49)]。对于专业应用，Wang 等人 [[59](#bib.bib59)] 通过多数投票突出了输出多样性的好处。最近的研究 [[6](#bib.bib6),
    [50](#bib.bib50)] 探讨了使用验证器增强对多个生成响应的搜索，显示扩展测试时计算是有效的。与这些专注于推理时间技术的研究不同，我们的论文改进了
    SFT 训练方法以促进多样性。
- en: Entropy Regularization. Dubey et al. [[14](#bib.bib14)] investigated the application
    of maximum entropy in visual fine-grained classification tasks, where the challenge
    lies in distinguishing between very similar categories of objects. Although their
    objective differs from ours, their insights remain valuable. They proposed that
    achieving zero CE loss is not essential for high accuracy. Instead, they suggested
    that a conditional probability distribution where the argmax corresponds to the
    correct class is sufficient for many applications. This concept motivates our
    use of entropy regularization, which allows for assigning probabilities to alternative
    options beyond the observed data. Prior to our work, Pereyra et al. [[41](#bib.bib41)]
    also explored entropy regularization in the context of neural network training.
    Their method closely resembles the CE with entropy regularization that we investigate
    in this paper, and they found that penalizing confident outputs improves generalization.
    It is important to note that Pereyra et al. [[41](#bib.bib41)] focused on image
    classification tasks, while our focus is on text generation where data is sequential
    in nature and is more challenging. In the context of LLMs, Hu et al. [[23](#bib.bib23)]
    also explored the maximum entropy regularization by using GFlowNet [[4](#bib.bib4)],
    but their methods require a reward function rather than supervised data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 熵正则化。Dubey 等人 [[14](#bib.bib14)] 研究了最大熵在视觉细粒度分类任务中的应用，其中的挑战在于区分非常相似的物体类别。虽然他们的目标与我们的不同，但他们的见解仍然很有价值。他们提出，实现零
    CE 损失并非高准确率的必要条件。相反，他们建议，在许多应用中，具有正确类别的 argmax 的条件概率分布是足够的。这个概念激励我们使用熵正则化，它允许对观察数据之外的替代选项分配概率。在我们的研究之前，Pereyra
    等人 [[41](#bib.bib41)] 也在神经网络训练的背景下探索了熵正则化。他们的方法与我们在本文中研究的带熵正则化的 CE 非常相似，他们发现惩罚自信的输出有助于改进泛化。需要注意的是，Pereyra
    等人 [[41](#bib.bib41)] 关注的是图像分类任务，而我们的重点是文本生成，数据具有序列性质且更具挑战性。在 LLM 的背景下，Hu 等人 [[23](#bib.bib23)]
    也通过使用 GFlowNet [[4](#bib.bib4)] 探索了最大熵正则化，但他们的方法需要奖励函数而非监督数据。
- en: 3 Preliminary
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步
- en: Large Language Models (LLMs). LLMs have a large vocabulary, denoted as $[K]=\{1,2,\ldots,K\}$,
    where each token $x_{i}\in[K]$ represents the sequence length. Let $f$ specifies
    the categorical distribution over $[K]$. Typically, $f$. For the $i$, its prediction
    probability is given by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）。LLMs拥有一个大词汇表，表示为$[K]=\{1,2,\ldots,K\}$，其中每个标记$x_{i}\in[K]$代表序列长度。设$f$指定了$[K]$上的类别分布。通常，$f$。对于第$i$个，其预测概率由下式给出
- en: '|  | $1$2 |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $z_{t}\in\mathbb{R}^{K}$, and $z_{t}[i]$-th element of $z_{t}$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$z_{t}\in\mathbb{R}^{K}$，$z_{t}[i]$是$z_{t}$的第$i$个元素。
- en: LLMs are pre-trained to predict the next token in a sequence, thereby learning
    complex conditional probability distributions from vast amounts of data. In practical
    applications, LLMs are tasked with generating a response $y$. However, these question-answer
    scenarios often differ from the textbook-like pre-training data, causing pre-trained
    LLMs to struggle in generating responses that follow human instructions effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过预测序列中的下一个标记来进行预训练，从而从大量数据中学习复杂的条件概率分布。在实际应用中，LLMs被要求生成响应$y$。然而，这些问答场景通常与教科书式的预训练数据不同，导致预训练的LLMs在生成符合人类指令的响应时表现不佳。
- en: 'Supervised Fine-Tuning. To address the above issue, Supervised Fine-Tuning
    (SFT) is introduced. This process involves using a supervised dataset with high-quality
    prompt-response pairs $\{(x^{i},y^{i})\}_{i=1}^{N}$. In theory, this corresponds
    to minimizing the *forward* KL divergence between the data distribution $p$:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督的微调。为了解决上述问题，引入了有监督的微调（SFT）。这个过程涉及使用具有高质量提示-响应对的监督数据集$\{(x^{i},y^{i})\}_{i=1}^{N}$。理论上，这对应于最小化数据分布$p$与模型之间的*前向*KL散度：
- en: '|  | $\displaystyle\min_{\theta}D_{\mathrm{KL}}\left(p,f_{\theta}\right)\Longleftrightarrow\max_{\theta}\mathbb{E}_{x\sim\rho(\cdot)}{\mathbb{E}_{y\sim{p}(\cdot&#124;x)}[\log
    f_{\theta}(y&#124;x)]},$ |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta}D_{\mathrm{KL}}\left(p,f_{\theta}\right)\Longleftrightarrow\max_{\theta}\mathbb{E}_{x\sim\rho(\cdot)}{\mathbb{E}_{y\sim{p}(\cdot\mid
    x)}[\log f_{\theta}(y\mid x)]},$ |  |'
- en: where $\rho$ can be treated as a constant and we omit it when the context is
    clear. In practice, many questions can correspond to multiple valid answers, and
    it is nearly impossible to collect a comprehensive high-quality dataset that encompasses
    all possibilities. As a result, the empirical data tends to be limited in size
    and often exhibits a narrower distribution than desired. In such scenarios, the
    CE loss function aggressively maximizes the likelihood of the available empirical
    data, adjusting the generative distribution $f_{\theta}$ to closely align with
    it. However, this approach can lead to poor generation diversity and overfitting,
    as previously noted.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\rho$可以被视为常量，当上下文明确时我们省略它。在实际中，许多问题可以对应多个有效答案，几乎不可能收集到涵盖所有可能性的全面高质量数据集。因此，经验数据往往规模有限，并且常常表现出比期望更窄的分布。在这种情况下，CE损失函数会激进地最大化可用经验数据的可能性，调整生成分布$f_{\theta}$以紧密匹配它。然而，这种方法可能导致生成多样性差和过拟合，正如之前所指出的。
- en: 4 Entropic Distribution Matching
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 熵分布匹配
- en: 'In this paper, we explore principled approaches to address limitations of existing
    SFT that uses the CE loss, especially when dealing with limited data. We present
    two core principles: the first focuses on the methodology of distribution matching,
    while the second offers guidance on learning from limited data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了解决现有SFT（使用CE损失）局限性的原则性方法，特别是在处理有限数据时。我们提出了两个核心原则：第一个关注于分布匹配的方法论，而第二个则提供了有限数据学习的指导。
- en: 'Our first principle advocates for a *generative* approach to distribution matching.
    This approach encourages the model to learn from its own generated data and mistakes,
    rather than merely imitating supervised demonstrations. Unlike the traditional
    CE loss, which leads the model to imitate training data labels passively, a generative
    approach involves active learning through self-generated feedback. This principle
    is grounded in cognitive science [[47](#bib.bib47), [20](#bib.bib20)], which demonstrates
    that children learn more effectively through exploration and experimentation,
    adjusting their understanding based on discrepancies between expectations and
    reality. Similarly, research on Generative Adversarial Networks (GANs) [[18](#bib.bib18),
    [24](#bib.bib24)] supports this notion by showing how models can learn to produce
    realistic data through iterative refinement. To summarize, we propose:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个原则提倡一种*生成式*的分布匹配方法。这种方法鼓励模型从自身生成的数据和错误中学习，而不仅仅是模仿监督示例。与传统的CE损失不同，后者使模型被动地模仿训练数据标签，生成式方法涉及通过自生成的反馈进行主动学习。这个原则基于认知科学[[47](#bib.bib47),
    [20](#bib.bib20)]，该科学表明儿童通过探索和实验更有效地学习，根据期望与现实之间的差异调整理解。同样，对生成对抗网络（GANs）的研究[[18](#bib.bib18),
    [24](#bib.bib24)]也支持这一观点，显示了模型如何通过迭代改进学习生成现实的数据。总之，我们建议：
- en: 'Principle 1: The distribution matching approach should be “generative”, meaning
    the model learns from both ground truth supervision and its own generated mistakes.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 1：分布匹配方法应为“生成式”，意味着模型从真实监督和自身生成的错误中学习。
- en: 'Our second principle addresses the challenge of overfitting. We draw inspiration
    from neuroscience, specifically the concept of avoiding over-memorization and
    achieving balanced learning. In neuroscience, synaptic plasticity, particularly
    homeostatic plasticity, underscores the importance of maintaining balance in learning
    processes [[56](#bib.bib56), [55](#bib.bib55)]. Overly strengthening certain neural
    connections can lead to rigid, maladaptive behaviors, analogous to how assigning
    excessively high probabilities to observed tokens can result in over-memorization
    in models, thereby limiting their ability to adapt and generalize. Based on these
    insights, especially for limited data, we propose:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个原则解决了过拟合的问题。我们受到神经科学的启发，特别是避免过度记忆和实现平衡学习的概念。在神经科学中，突触可塑性，特别是稳态可塑性，强调在学习过程中保持平衡的重要性[[56](#bib.bib56),
    [55](#bib.bib55)]。过度加强某些神经连接可能导致僵化的、不适应的行为，这类似于将过高的概率分配给观察到的标记会导致模型的过度记忆，从而限制其适应和泛化能力。基于这些见解，特别是在数据有限的情况下，我们建议：
- en: 'Principle 2: The model should assign higher probabilities to the observed data
    while preventing over-memorization.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：模型应对观察到的数据分配更高的概率，同时防止过度记忆。
- en: '4.1 Proposed Formulation: Reserve KL with Entropy Regularization'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 提议的公式：保留带有熵正则化的KL散度
- en: 'To implement the two principles outlined above, we propose studying the formulation
    of *reverse* KL divergence minimization with maximum entropy regularization. The
    objective is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为实现上述两个原则，我们建议研究*逆*KL散度最小化的公式，并结合最大熵正则化。目标定义如下：
- en: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}\left[\log p(y&#124;x)\right]-\mathbb{E}_{y\sim f(\cdot&#124;x)}[\log
    f(y&#124;x)]}_{=-D_{\mathrm{KL}}(f,p)}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[\log f(y&#124;x)]}_{{\mathcal{H}}(f)}\big{\}}.$ |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}\left[\log p(y&#124;x)\right]-\mathbb{E}_{y\sim f(\cdot&#124;x)}[\log
    f(y&#124;x)]}_{=-D_{\mathrm{KL}}(f,p)}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[\log f(y&#124;x)]}_{{\mathcal{H}}(f)}\big{\}}.$ |  | (1) |'
- en: 'The first term in this formulation corresponds to the *reverse* KL divergence
    between the target distribution $p$. This term supports Principle 1 by encouraging
    the model to learn from its generated data samples. The second term, entropy regularization,
    aligns with Principle 2 by preventing over-memorization, as it ensures that the
    probabilities for labeled data do not become excessively high. Furthermore, it
    brings another benefit: the output diversity can be improved. In this case, greedy
    sampling can reliably output the knowledge in the training data. We also note
    that adding entropy regularization to the CE loss supports Principle 2 but not
    Principle 1; its limitations are discussed in [Appendix C](#A3 "Appendix C Discussion
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"), and we will show that it is inferior to the proposed approach
    in [Section 5](#S5 "5 Experiments ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式中的第一个项对应于目标分布$p$的*反向* KL散度。该项通过鼓励模型从生成的数据样本中学习来支持原则1。第二项，熵正则化，通过防止过度记忆来支持原则2，因为它确保了标记数据的概率不会过高。此外，它还有另一个好处：可以提高输出的多样性。在这种情况下，贪婪采样可以可靠地输出训练数据中的知识。我们还注意到，将熵正则化添加到CE损失中支持原则2，但不支持原则1；其局限性在[附录
    C](#A3 "附录 C 讨论 ‣ 在LLMs的监督微调中的熵分布匹配：较少过拟合和更好的多样性")中进行了讨论，我们将展示它在[第5节](#S5 "5 实验
    ‣ 在LLMs的监督微调中的熵分布匹配：较少过拟合和更好的多样性")中不如所提议的方法。
- en: 'While the objective defined in [Equation 1](#S4.E1 "In 4.1 Proposed Formulation:
    Reserve KL with Entropy Regularization ‣ 4 Entropic Distribution Matching ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity") appears promising, it presents significant challenges in practice.
    The main challenge is that we only have access to empirical data from the distribution
    $p$, not its full probability density function, making the reverse KL term impossible
    to compute directly. Additionally, calculating the expectation of the reverse
    KL across the model’s generative distribution is not easy. This paper contributes
    a new algorithm to address these challenges.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[方程1](#S4.E1 "在 4.1 提议的公式：带熵正则化的反向KL ‣ 4 熵分布匹配 ‣ 在LLMs的监督微调中的熵分布匹配：较少过拟合和更好的多样性")中定义的目标看起来很有前景，但在实践中存在重大挑战。主要挑战在于我们仅能获得来自分布$p$的经验数据，而无法获得其完整的概率密度函数，这使得反向KL项无法直接计算。此外，计算反向KL在模型生成分布上的期望也不容易。本文贡献了一种新算法来解决这些挑战。
- en: '4.2 Proposed Algorithm: GEM'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 提议的算法：GEM
- en: In this section, we present a practical algorithm for solving the optimization
    problem of reverse KL with entropy regularization. As discussed earlier, the key
    is to obtain an estimate for the log probability density function $\log p$ is
    not sequential. We begin by outlining a conceptually simple but technically complicated
    solution. Building on this proposal, we then introduce a more tractable solution
    by new techniques.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提出了一种解决带有熵正则化的反向KL优化问题的实用算法。如前所述，关键在于获得对对数概率密度函数$\log p$的估计，这不是顺序的。我们首先概述了一个概念上简单但技术上复杂的解决方案。基于这一提案，我们随后通过新技术引入了一个更具可操作性的解决方案。
- en: An Initial Proposal.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初步提案。
- en: 'Drawing inspiration from GANs [[18](#bib.bib18), [25](#bib.bib25)], one may
    propose the following formulation for estimating the distribution $p$:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从GANs[[18](#bib.bib18), [25](#bib.bib25)]中汲取灵感，提出了以下用于估计分布$p$的公式：
- en: '|  | $\displaystyle\min_{q}\max_{r}\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[h\left(r(x,y^{\texttt{real}})-r(x,y^{\texttt{gene}})\right)\right].$
    |  | (2) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{q}\max_{r}\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[h\left(r(x,y^{\texttt{real}})-r(x,y^{\texttt{gene}})\right)\right].$
    |  | (2) |'
- en: Here we use the subscript real to denote the supervised data and gene to denote
    the model-generated data for clarity. In addition, $h$ acts as a discriminator,
    designed to maximize the gap between samples drawn from the data distribution
    $p$. The goal of the opponent $q$ is a linear function, it simplifies to the reward
    maximization problem $\max_{q}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot|x)}[r(x,y^{\texttt{gene}})]$,
    a fact that shall be used later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用下标 real 来表示监督数据，使用 gene 来表示模型生成的数据以便于清晰。此外，$h$ 作为判别器，旨在最大化从数据分布$p$中抽取的样本之间的差距。对手
    $q$ 的目标是一个线性函数，它简化为奖励最大化问题 $\max_{q}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot|x)}[r(x,y^{\texttt{gene}})]$，这一事实将在后面使用。
- en: 'On its theoretical foundation, Jolicoeur-Martineau [[25](#bib.bib25)] proved
    that the inner maximization problem is to find a divergence function between the
    distributions $p$, and thus the outer minimization problem is to reduce this divergence.
    Therefore, the solved $q$. If we can solve problem ([2](#S4.E2 "Equation 2 ‣ An
    Initial Proposal. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity")), we can substitute $\log p=\log q$, $q$, which are hard
    to solve:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在其理论基础上，Jolicoeur-Martineau [[25](#bib.bib25)] 证明了内层最大化问题是找出分布$p$之间的散度函数，因此外层最小化问题是减少这种散度。因此，解决了$q$。如果我们能解决问题（[2](#S4.E2
    "公式 2 ‣ 初步提案 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在LLM的监督微调中：更少的过拟合和更好的多样性")），我们可以代入 $\log
    p=\log q$，$q$，这些问题是难以解决的：
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The variables $q$ are introduced to be adversarially trained in [Equation 2](#S4.E2
    "In An Initial Proposal. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity");'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变量$q$被引入以进行对抗性训练，在[公式 2](#S4.E2 "在初步提案中 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在LLM的监督微调中：更少的过拟合和更好的多样性")中。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The distribution $f$ has been solved.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布$f$已被解决。
- en: It is important to note that optimizing the reward function $r$ can be particularly
    challenging when dealing with sequential data, often requiring Reinforcement Learning
    (RL) algorithms (see, e.g., [[22](#bib.bib22)]). Our initial attempts to implement
    the above proposal were not successful. We introduce a new tractable solution
    below.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在处理序列数据时，优化奖励函数$r$可能特别具有挑战性，通常需要强化学习（RL）算法（见，例如，[[22](#bib.bib22)]）。我们最初尝试实施上述提案并不成功。我们在下面引入了一种新的可处理的解决方案。
- en: Algorithm 1 GEM
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 GEM
- en: Dataset ${\mathcal{D}}=\{(x_{i},y_{i}^{\texttt{real}})\}$1:Set $q_{k}=\texttt{softmax}(1/\beta*\log
    f_{\theta_{k}})$2:Define the loss function
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 ${\mathcal{D}}=\{(x_{i},y_{i}^{\texttt{real}})\}$1:设置 $q_{k}=\texttt{softmax}(1/\beta*\log
    f_{\theta_{k}})$2:定义损失函数
- en: '|  | $1$2 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 3:Update $\theta_{k+1}=\theta_{k}+\eta\cdot\nabla_{\theta}{{\mathcal{L}}}_{q}(f_{\theta})\mid_{\theta=\theta_{k}}$\Require\For
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 3:更新 $\theta_{k+1}=\theta_{k}+\eta\cdot\nabla_{\theta}{{\mathcal{L}}}_{q}(f_{\theta})\mid_{\theta=\theta_{k}}$\Require\For
- en: Proposed Solution.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提议的解决方案。
- en: 'At a high level, our approach simplifies the process by solving a single-stage
    optimization problem, eliminating the need to first estimate the distribution
    $p$. Our approach involves two key techniques:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们的方法通过解决一个单阶段优化问题来简化过程，从而消除了首先估计分布$p$的需要。我们的方法涉及两个关键技术：
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reparameterization: We reparameterize the discriminator using the generative
    distribution $f$ as a real-valued function and parameterize it as $\log f(y|x)$,
    ensuring it remains real-valued.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重参数化：我们使用生成分布$f$作为实值函数来重新参数化判别器，并将其参数化为 $\log f(y|x)$，确保其保持实值。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direct entropy regularization: We introduce entropy regularization for the
    distribution $q$. This technique offers two advantages. First, it establishes
    a connection between $q$, eliminating the need of solving problem ([1](#S4.E1
    "Equation 1 ‣ 4.1 Proposed Formulation: Reserve KL with Entropy Regularization
    ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")) separately. Second,
    since $q$ has a closed-form solution, it does not require explicit training.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接熵正则化：我们为分布$q$引入了熵正则化。这种技术有两个优点。首先，它建立了$q$之间的联系，消除了单独解决问题（[1](#S4.E1 "公式 1
    ‣ 4.1 提议的公式：保留KL与熵正则化 ‣ 4 熵分布匹配 ‣ 在LLM的监督微调中：更少的过拟合和更好的多样性")）的需要。其次，由于$q$有一个封闭形式的解，因此不需要显式训练。
- en: 'Specifically, our formulation is that:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们的公式是：
- en: '|  | $\displaystyle\max_{f}\quad$ |  | (3) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{f}\quad$ |  | (3) |'
- en: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y\sim\pi(\cdot&#124;x)}\left[\log
    f(y&#124;x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x))=\texttt{softmax}(1/\beta*\log
    f).$ |  | (4) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y\sim\pi(\cdot\vert
    x)}\left[\log f(y\vert x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot\vert x))=\texttt{softmax}(1/\beta*\log
    f).$ |  | (4) |'
- en: 'In this formulation, we optimize $f$. Simultaneously, we optimize $q$, incorporating
    entropy regularization. Fortunately, this yields a closed-form solution, so we
    do not need to maintain or explicitly train $q$ in [Equation 3](#S4.E3 "In Proposed
    Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity"). Note that although $q$ generally, we do not calculate the
    gradient through $q$. This is similar to the target network used in RL [[35](#bib.bib35),
    [32](#bib.bib32)]. We have the following theoretical justification for this formulation.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，我们优化$f$。同时，我们优化$q$，并结合熵正则化。幸运的是，这产生了一个封闭形式的解，因此我们不需要在[方程3](#S4.E3 "在提议的解决方案。
    ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性")中维护或显式训练$q$。注意，虽然通常情况下我们不通过$q$计算梯度。这类似于在强化学习中使用的目标网络[[35](#bib.bib35),
    [32](#bib.bib32)]。我们有以下理论依据来支持这个公式。
- en: Proposition 1.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 1。
- en: 'Assume that $h$, ${\mathcal{L}}_{q}(f)$) corresponds to the optimal solution
    of Problem ([1](#S4.E1 "Equation 1 ‣ 4.1 Proposed Formulation: Reserve KL with
    Entropy Regularization ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设$h$，${\mathcal{L}}_{q}(f)$) 对应于问题([1](#S4.E1 "方程 1 ‣ 4.1 提议的公式：保留KL与熵正则化 ‣
    4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性"))的最优解。
- en: '[Proposition 1](#Thmprop1 "Proposition 1\. ‣ Proposed Solution. ‣ 4.2 Proposed
    Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity") implies
    that solving the proposed problem in [Equations 3](#S4.E3 "In Proposed Solution.
    ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")
    and [4](#S4.E4 "Equation 4 ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM
    ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") provides the optimal
    solution of reverse KL with entropy regularization in [Equation 1](#S4.E1 "In
    4.1 Proposed Formulation: Reserve KL with Entropy Regularization ‣ 4 Entropic
    Distribution Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). In practice, we can parameterize
    $f$ is calculated because we assume $y^{\texttt{gene}}$, meaning that the proposed
    formulation cannot solve the pure reverse KL minimization problem.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[命题 1](#Thmprop1 "命题 1\. ‣ 提议的解决方案。 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性")意味着解决[方程3](#S4.E3
    "在提议的解决方案。 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性")和[4](#S4.E4
    "方程 4 ‣ 提议的解决方案。 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性")中提出的问题提供了[方程1](#S4.E1
    "在 4.1 提议的公式：保留KL与熵正则化 ‣ 4 熵分布匹配 ‣ 在监督微调LLMs中的熵分布匹配：减少过拟合和更好的多样性")的逆KL与熵正则化的最优解。在实践中，我们可以对$f$进行参数化计算，因为我们假设$y^{\texttt{gene}}$，这意味着提议的公式无法解决纯逆KL最小化问题。'
- en: Intuition and Example.
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直观和示例。
- en: 'We provide an intuitive understanding of GEM by explaining its training mechanism
    on a simple model: for a fixed $x\in{\mathcal{X}}$ with $\theta_{x}\in\mathbb{R}^{K}$
    as the linear function described in [Proposition 1](#Thmprop1 "Proposition 1\.
    ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). For a paired sample $(y^{\texttt{real}},y^{\texttt{gene}})=(i,j)$,
    we have the gradient for this sample:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在一个简单模型上解释其训练机制，提供了对GEM的直观理解：对于固定的$x\in{\mathcal{X}}$，其中$\theta_{x}\in\mathbb{R}^{K}$是[命题1](#Thmprop1
    "命题 1. ‣ 提议的解决方案。 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配 ‣ 在LLMs的监督微调中的熵分布匹配：减少过拟合和更好的多样性")中描述的线性函数。对于一个配对样本$(y^{\texttt{real}},y^{\texttt{gene}})=(i,j)$，我们得到该样本的梯度：
- en: '|  | $\displaystyle\nabla_{\theta}{\mathcal{L}}_{q}(f_{\theta})[i,j]=\left\{\begin{array}[]{cc}w_{ij}e_{ij}&amp;\text{
    if }\quad i\neq j\\ \mathbf{0}&amp;\text{ otherwise}\end{array}\right.$ |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}{\mathcal{L}}_{q}(f_{\theta})[i,j]=\left\{\begin{array}[]{cc}w_{ij}e_{ij}&amp;\text{
    if }\quad i\neq j\\ \mathbf{0}&amp;\text{ otherwise}\end{array}\right.$ |  |'
- en: Here $w_{ij}=p(y^{\texttt{real}}|x)q(y^{\texttt{gene}}|x)$, and $e_{ij}$-th
    element being $1$-th element being $-1$ otherwise. Thus, the gradient of this
    paired data gives a direction for moving the logit $\theta_{x}$-th position to
    $i$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$w_{ij}=p(y^{\texttt{real}}|x)q(y^{\texttt{gene}}|x)$，并且$e_{ij}$-th元素为$1$-th元素为$-1$否则。因此，这个配对数据的梯度给出了将logit
    $\theta_{x}$-th位置移动到$i$的方向。
- en: Consider a numerical example where $\theta_{x}=[2,1]$, resulting in $f=[0.73,0.27]$,
    we have $q=[0.81,0.19]$. Given the data distribution $p=[0.9,0.1]$, leading to
    a relative logit change of $0.2$, resulting in a relative logit change of $0.28$
    due to the induced entropy regularization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个数值示例，其中$\theta_{x}=[2,1]$，得到$f=[0.73,0.27]$，我们有$q=[0.81,0.19]$。给定数据分布$p=[0.9,0.1]$，导致相对logit变化为$0.2$，由于引入的熵正则化，导致相对logit变化为$0.28$。
- en: In the above analysis, we see that the distribution $q$, a *narrowed* distribution
    $q$, prioritizes the high-probability regions in $f$ contributes less. This contrasts
    with CE, which would push probabilities of non-labeled tokens towards the labeled
    ones, potentially causing overfitting. We also observe that $h$ for a general
    function $h$ is always equal to $1$ is the log-sigmoid function $h(u)=\log\texttt{sigmoid}(u)=u-\log(1+\exp(u))$,
    it results in a large weight when $y^{\texttt{real}}$ has already become dominant.
    Later on, we will study this function in experiments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述分析中，我们看到分布$q$，一个*收窄*的分布$q$，优先考虑了在$f$中高概率区域的影响较小。这与CE相反，CE会将非标记标记的概率推向标记的标记，可能导致过拟合。我们还观察到，一般函数$h$的$h$始终等于$1$，即对数-
    sigmoid函数$h(u)=\log\texttt{sigmoid}(u)=u-\log(1+\exp(u))$，当$y^{\texttt{real}}$已经占据主导地位时，结果会导致较大的权重。稍后，我们将通过实验研究这个函数。
- en: Algorithm 2 GEM for Sequential Data
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 顺序数据的GEM
- en: Dataset ${\mathcal{D}}=\{(x_{i},y_{1},\ldots,y_{T})\}$ \Forsample index $i$
    “Reset” data distribution \Fortimestep index $t=1,\ldots,T$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集${\mathcal{D}}=\{(x_{i},y_{1},\ldots,y_{T})\}$ \Forsample index $i$ “重置”数据分布
    \Fortimestep index $t=1,\ldots,T$
- en: '|  | $\displaystyle\widetilde{x}$ |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widetilde{x}$ |  |'
- en: '|  | $\displaystyle\widetilde{{\mathcal{D}}}$ |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widetilde{{\mathcal{D}}}$ |  |'
- en: \EndFor\EndFor2:$f_{\theta}\leftarrow$ \EnsureGenerative model $f_{\theta}$\Require
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \EndFor\EndFor2:$f_{\theta}\leftarrow$ \EnsureGenerative model $f_{\theta}$\Require
- en: Extensions to Sequential Data.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对顺序数据的扩展。
- en: 'In the above part, we have derived the algorithm for the case $y$. We can extend
    the formulation in [Equations 3](#S4.E3 "In Proposed Solution. ‣ 4.2 Proposed
    Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity") and [4](#S4.E4
    "Equation 4 ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") to the following:⁴⁴4Generally speaking, the
    prompt $x$ would also be sequential, but this does not affect our discussion and
    formulation as it serves the input to the conditional distribution.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述部分中，我们推导了$y$的情况的算法。我们可以将[方程3](#S4.E3 "在提议的解决方案中。 ‣ 4.2 提议的算法：GEM ‣ 4 熵分布匹配
    ‣ 在LLMs的监督微调中的熵分布匹配：减少过拟合和更好的多样性")和[4](#S4.E4 "方程 4 ‣ 提议的解决方案。 ‣ 4.2 提议的算法：GEM
    ‣ 4 熵分布匹配 ‣ 在LLMs的监督微调中的熵分布匹配：减少过拟合和更好的多样性")扩展到以下内容：⁴⁴4一般来说，提示$x$也将是顺序的，但这不影响我们的讨论和公式，因为它作为条件分布的输入。
- en: '|  | $\displaystyle\max_{f}\quad$ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{f}\quad$ |  |'
- en: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y_{1:T}\sim\pi(\cdot&#124;x)}\left[\log
    f(y_{1:T}&#124;x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x))$ |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\mathbb{E}_{y_{1:T}\sim\pi(\cdot&#124;x)}\left[\log
    f(y_{1:T}&#124;x)\right]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x))$ |  |'
- en: 'Here, we encounter a challenge: the joint distribution of $y_{1:T}$ cannot
    be easily calculated as before. While Monte Carlo estimation—drawing samples to
    approximate the gradient—might seem like a viable solution, we found it does not
    work in experiments. We believe the main reason is that the sample space is huge,⁵⁵5For
    the Llama-3-8B model, its vocabulary size is 128k. For a typical case with a sequence
    length of 2048, the sample size is $128000^{2048}$ is quite different from the
    data distribution $p$ that we aim to learn.⁶⁶6Specifically, pre-trained models
    cannot generate the EOS (end-of-sentence) token properly, resulting in repetitive
    and less informative sequences, even with infinite length. But the supervised
    data has an EOS token and finite length. To bypass this challenge, methods proposed
    in [[9](#bib.bib9), [29](#bib.bib29)] rely on models that has been SFT-trained
    with CE. As mentioned, their methods cannot operate with pre-trained models directly.
    As a result, when we use stochastic sampling to estimate the gradient, it does
    not provide effective feedback.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遇到了一个挑战：$y_{1:T}$的联合分布无法像之前那样轻易计算。虽然蒙特卡罗估计——通过抽样来近似梯度——可能看起来是一个可行的解决方案，但我们发现它在实验中并不起作用。我们认为主要原因是样本空间非常庞大。⁵⁵对于Llama-3-8B模型，其词汇表大小为128k。在序列长度为2048的典型情况下，样本大小为$128000^{2048}$，这与我们旨在学习的数据分布$p$大相径庭。⁶⁶具体而言，预训练模型无法正确生成EOS（句子结束）标记，导致生成重复且信息量较少的序列，即使序列长度无限也是如此。但监督数据具有EOS标记和有限长度。为了绕过这个挑战，[[9](#bib.bib9)、[29](#bib.bib29)]提出的方法依赖于已经经过CE训练的SFT模型。如前所述，他们的方法不能直接与预训练模型一起使用。因此，当我们使用随机采样来估计梯度时，它无法提供有效的反馈。
- en: 'To deal with the above challenge, we propose decomposing the multi-stage sequential
    optimization problem into multiple single-stage optimization problems and solve
    each efficiently. Concretely, we restrict the distribution matching to the case
    that the prefix samples up to time step $t$ and solves the optimization problem
    at the $t$-th time step as before. Its mathematical formulation is given below:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，我们提出将多阶段顺序优化问题分解为多个单阶段优化问题，并有效地解决每个问题。具体而言，我们将分布匹配限制为前缀样本直到时间步$t$，并像之前一样在第$t$步解决优化问题。其数学表述如下：
- en: '|  | $\displaystyle\max_{f}{\mathcal{L}}_{q}^{\texttt{seq}}(f)$ |  | (5) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{f}{\mathcal{L}}_{q}^{\texttt{seq}}(f)$ |  | (5) |'
- en: '|  | $\displaystyle\text{where}\quad\Delta$ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{where}\quad\Delta$ |  |'
- en: 'The main advantage of this formulation is that for each sub-problem, we still
    have access to the exact conditional distribution, so the gradient estimation
    is accurate. The same idea applies to the training of distribution $q$:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势在于，对于每个子问题，我们仍然可以获取精确的条件分布，因此梯度估计是准确的。相同的思想也适用于分布$q$的训练：
- en: '|  | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\left\{\sum_{t=1}^{T}\mathbb{E}_{y^{\texttt{real}}_{1:t-1}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y_{t}\sim\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}})}[\log
    f(y_{t}&#124;x,y_{1:t-1}^{\texttt{real}})]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}}))\right\}.$
    |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q=\mathop{\rm argmax}_{\pi}\mathbb{E}_{x}\left\{\sum_{t=1}^{T}\mathbb{E}_{y^{\texttt{real}}_{1:t-1}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y_{t}\sim\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}})}[\log
    f(y_{t}&#124;x,y_{1:t-1}^{\texttt{real}})]+1/\beta\cdot{\mathcal{H}}(\pi(\cdot&#124;x,y_{1:t-1}^{\texttt{real}}))\right\}.$
    |  |'
- en: 'That is, we still have the closed-form solution that $q(\cdot|x,y_{1:t-1}^{\texttt{real}})=\texttt{softmax}(1/\beta\cdot\log
    f(\cdot|x,y_{1:t-1}^{\texttt{real}}))$ when used in [Equation 5](#S4.E5 "In Extensions
    to Sequential Data. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). We outline the proposed procedure in [Algorithm 2](#alg2
    "In Intuition and Example. ‣ 4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution
    Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") and its PyTorch code is provided in [Appendix A](#A1
    "Appendix A Implementation of GEM ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") for reference. We
    acknowledge that our technique draws inspiration from the data distribution “reset”
    trick introduced by [[45](#bib.bib45)] in imitation learning, in which the teacher
    first shows few demonstration actions and then the student is asked to finish
    the other actions in a full trajectory.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们仍然拥有封闭形式的解 $q(\cdot|x,y_{1:t-1}^{\texttt{real}})=\texttt{softmax}(1/\beta\cdot\log
    f(\cdot|x,y_{1:t-1}^{\texttt{real}}))$，在[方程 5](#S4.E5 "在序列数据的扩展中。 ‣ 4.2 提议的算法：GEM
    ‣ 4 熵分布匹配 ‣ 监督式LLM精调中的熵分布匹配：减少过拟合与更好多样性")中使用。我们在[算法 2](#alg2 "在直觉和示例中。 ‣ 4.2 提议的算法：GEM
    ‣ 4 熵分布匹配 ‣ 监督式LLM精调中的熵分布匹配：减少过拟合与更好多样性")中概述了提出的程序，其PyTorch代码可在[附录 A](#A1 "附录
    A GEM的实现 ‣ 监督式LLM精调中的熵分布匹配：减少过拟合与更好多样性")中找到供参考。我们承认，我们的技术灵感来源于[[45](#bib.bib45)]在模仿学习中引入的数据分布“重置”技巧，其中教师首先展示少量示范动作，然后学生被要求完成全轨迹中的其他动作。
- en: 5 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we present our experiment results for fine-tuning the Llama-3-8B
    model (specifically, its pre-trained version). A brief overview of our experiment
    setting is provided below, with further details available in [Appendix D](#A4
    "Appendix D Experiment Details ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了对Llama-3-8B模型（特别是其预训练版本）的精调实验结果。下面提供了实验设置的简要概述，详细信息见[附录 D](#A4 "附录
    D 实验细节 ‣ 监督式LLM精调中的熵分布匹配：减少过拟合与更好多样性")。
- en: 5.1 General-Purpose Fine-tuning
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 通用精调
- en: Set-up. In this section, we develop an LLM that is capable of following instructions
    for various prompts. To this end, we utilize the UltraFeedback dataset [[13](#bib.bib13)],
    specifically the version filtered by the HuggingfaceH4 team⁷⁷7[https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).
    This dataset contains prompts from instruction datasets like Evol-Instruct and
    UltraChat, and responses generated by models such as GPT-4 and Llama-2-7B/13B/70B-Chat.
    For more information, see [[13](#bib.bib13)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。在本节中，我们开发了一个能够遵循各种提示指令的LLM。为此，我们使用了UltraFeedback数据集[[13](#bib.bib13)]，特别是由HuggingfaceH4团队筛选的版本[https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)。该数据集包含来自Evol-Instruct和UltraChat等指令数据集的提示，以及由GPT-4和Llama-2-7B/13B/70B-Chat等模型生成的响应。有关更多信息，请参见[[13](#bib.bib13)]。
- en: 'Each data point comprises two responses: one selected as the preferred option
    and the other as the rejected option, with the selection made by GPT-4\. In our
    study, we use the preferred response for SFT, a practice commonly adopted in previous
    research [[39](#bib.bib39), [3](#bib.bib3)]. Following [[66](#bib.bib66), [34](#bib.bib34),
    [13](#bib.bib13)] we set the learning rate to $2\times 10^{-5}$, employing a cosine
    learning rate decay schedule, and use a macro batch size of 128\. The maximum
    sequence length, encompassing both the prompt and response, is set to 2,048 tokens.
    Models are trained for three epochs.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点包括两个响应：一个被选为首选选项，另一个被选为拒绝选项，选择由GPT-4完成。在我们的研究中，我们使用首选响应进行SFT，这在以往研究中是常见的做法[[39](#bib.bib39),
    [3](#bib.bib3)]。遵循[[66](#bib.bib66), [34](#bib.bib34), [13](#bib.bib13)]，我们将学习率设置为$2\times
    10^{-5}$，采用余弦学习率衰减计划，并使用宏批量大小128。最大序列长度（包括提示和响应）设置为2,048个标记。模型训练三轮。
- en: 'We implement the proposed GEM method with $\beta=0.7$. Our primary baseline
    is the standard CE loss. Additionally, we explore a variant incorporating a weight
    decay of 0.1, which has been commonly used in previous studies [[39](#bib.bib39),
    [3](#bib.bib3)]. We refer to this approach as CE + WD. We also implement a method
    called CE + Entropy, which adds an entropy regularization term of 0.1 to the CE
    loss. This method aligns with the proposed Principle 2 but not Principle 1 (see
    [Appendix C](#A3 "Appendix C Discussion ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity") for more discussion).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用$\beta=0.7$实现了所提出的GEM方法。我们的主要基准是标准CE损失。此外，我们还探讨了一种变体，加入了0.1的权重衰减，这在之前的研究中被广泛使用[[39](#bib.bib39),
    [3](#bib.bib3)]。我们称这种方法为CE + WD。我们还实现了一种称为CE + Entropy的方法，它在CE损失中添加了0.1的熵正则化项。该方法与所提出的原则2一致，但不符合原则1（详见[附录
    C](#A3 "Appendix C Discussion ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity")进行进一步讨论）。'
- en: Instruction-Following.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令遵循。
- en: 'We first examine the model’s learned ability in terms of instruction-following.
    We follow the IFEval benchmark in [[69](#bib.bib69)], which includes 500 prompts
    from 25 types of verifiable instructions, such as writing more than 400 words.
    Depending on whether we use strict or loose judgment and whether we measure accuracy
    in the prompt space or instruction space, there are four evaluation criteria:
    prompt-level strict accuracy, instruction-level strict accuracy, prompt-level
    loose accuracy, and instruction-level loose accuracy. For all metrics, a higher
    value indicates better performance.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考察模型在指令遵循方面的学习能力。我们遵循[[69](#bib.bib69)]中的IFEval基准，该基准包括来自25种可验证指令的500个提示，例如写作超过400字。根据我们是否使用严格或松散的判断以及我们是否在提示空间或指令空间中测量准确度，共有四个评估标准：提示级别严格准确度、指令级别严格准确度、提示级别松散准确度和指令级别松散准确度。所有指标中，值越高表示性能越好。
- en: 'Table 1: Performance of instruction-following on the benchmark IFEval [[69](#bib.bib69)].
    For all metrics, a higher value means a better instruction following ability.
    The best results are shown in bold, with the second-best underlined.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在基准IFEval[[69](#bib.bib69)]上的指令遵循性能。所有指标中，值越高表示指令遵循能力越好。最佳结果用粗体显示，次佳结果用下划线标记。
- en: '| Method | Instruction-Following |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 指令遵循 |'
- en: '| Strict Accuracy | Strict Accuracy | Loose Accuracy | Loose Accuracy |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 严格准确度 | 严格准确度 | 松散准确度 | 松散准确度 |'
- en: '| (Prompt Level) | (Instruction Level) | (Prompt Level) | (Instruction Level)
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (提示级别) | (指令级别) | (提示级别) | (指令级别) |'
- en: '| CE | 36.23 | 46.76 | 40.85 | 50.96 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CE | 36.23 | 46.76 | 40.85 | 50.96 |'
- en: '| CE+WD | 37.89 | 47.48 | 42.88 | 52.52 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 37.89 | 47.48 | 42.88 | 52.52 |'
- en: '| CE+Entropy | 36.78 | 47.60 | 40.66 | 51.08 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 36.78 | 47.60 | 40.66 | 51.08 |'
- en: '| GEM-Linear | 37.34 | 48.20 | 41.96 | 52.64 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 37.34 | 48.20 | 41.96 | 52.64 |'
- en: '| GEM-LS | 37.52 | 47.60 | 42.14 | 52.04 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 37.52 | 47.60 | 42.14 | 52.04 |'
- en: 'We evaluate the trained models using *greedy decoding* and present the results
    in [Table 1](#S5.T1 "In Instruction-Following. ‣ 5.1 General-Purpose Fine-tuning
    ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning of
    LLMs: Less Overfitting and Better Diversity"). We observe that CE underperforms
    compared with regularization-based methods, such as weight decay and entropy regularization,
    suggesting that CE suffers from overfitting. On average across the four criteria,
    GEM-LS improves by 1.1 points (2.5% relative) and 1.4 points (3.2% relative) compared
    to CE. We also observe this overfitting in the evaluation perplexity: GEM-LS and
    GEM-Linear achieve lower perplexity (around 3.16) than CE (3.48); see [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity"). It is important
    to note that this overfitting is not due to over-optimization, as performance
    continues to improve over three training epochs for CE (36.15 in epoch 1, 41.45
    in epoch 2, and 43.70 in epoch 3).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*贪婪解码*评估经过训练的模型，并在[表格1](#S5.T1 "在指令跟随中。 ‣ 5.1 通用微调 ‣ 5 实验 ‣ 监督微调中的熵分布匹配：过拟合较少且多样性更好")中展示结果。我们观察到，与基于正则化的方法（如权重衰减和熵正则化）相比，CE的表现较差，这表明CE存在过拟合。根据四个标准的平均值，GEM-LS比CE提高了1.1分（相对2.5%）和1.4分（相对3.2%）。我们还在评估困惑度中观察到这种过拟合：GEM-LS和GEM-Linear的困惑度（约3.16）低于CE（3.48）；见[附录E](#A5
    "附录E 额外结果 ‣ 监督微调中的熵分布匹配：过拟合较少且多样性更好")。需要注意的是，这种过拟合不是由于过度优化，因为CE的表现随着训练轮次的增加而持续改善（第1轮36.15，第2轮41.45，第3轮43.70）。
- en: Creative Writing.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创意写作。
- en: 'We continue to assess models’ output diversity in two creative writing tasks:
    poem writing and story writing. For poems, we use prompts from the poetry⁸⁸8[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)
    dataset on the Huggingface website, which includes 573 poems on themes such as
    love, nature, and mythology. For stories, we design 500 prompts based on the ROC
    story dataset [[36](#bib.bib36)]. In both cases, we prompt the models to write
    a poem or story titled “[X]” with no more than 200 words, where [X] is a title
    from the respective dataset. Following [[26](#bib.bib26)], we use three criteria
    to evaluate diversity: 1) N-gram diversity: the proportion of distinct n-grams
    in a single response (intra-diversity); 2) Self-BLEU diversity: the Self-BLEU
    score, treating one response as a reference among multiple generated responses
    (inter-diversity); 3) Sentence-BERT diversity: the cosine dissimilarity between
    pairs of responses in the embedding space. All criteria range from 0 to 100 (with
    Sentence-BERT diversity scaled by multiplying by 100), where higher values indicate
    greater diversity.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续评估模型在两个创意写作任务中的输出多样性：诗歌创作和故事创作。对于诗歌，我们使用来自Huggingface网站上的poetry⁸⁸8[https://huggingface.co/datasets/merve/poetry](https://huggingface.co/datasets/merve/poetry)
    数据集的提示，该数据集包括573首关于爱情、自然和神话等主题的诗歌。对于故事，我们设计了基于ROC故事数据集[[36](#bib.bib36)]的500个提示。在这两种情况下，我们提示模型写一篇标题为“[X]”且不超过200字的诗歌或故事，其中[X]是来自相应数据集的标题。按照[[26](#bib.bib26)]，我们使用三个标准来评估多样性：1)
    N-gram多样性：单个响应中不同n-gram的比例（内部多样性）；2) 自我BLEU多样性：Self-BLEU分数，将一个响应视为多个生成响应中的参考（外部多样性）；3)
    Sentence-BERT多样性：嵌入空间中响应对之间的余弦不相似度。所有标准的范围从0到100（Sentence-BERT多样性通过乘以100进行缩放），其中更高的值表示更大的多样性。
- en: 'To calculate these metrics, we ask the trained models to generate 16 samples
    using the decoding configuration temperature=1, top_k=50, and top_p=0.9. The evaluation
    results are presented in [Table 2](#S5.T2 "In Creative Writing. ‣ 5.1 General-Purpose
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). In this task, we note that weight
    decay does not improve generation diversity, although it has shown effectiveness
    in mitigating overfitting in previous examples. On the other hand, entropy regularization,
    implemented to support Principle 2, brings the benefit of output diversity.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这些指标，我们要求经过训练的模型使用解码配置 temperature=1、top_k=50 和 top_p=0.9 生成16个样本。评估结果展示在[表格2](#S5.T2
    "在创意写作中。 ‣ 5.1 通用微调 ‣ 5 实验 ‣ 监督微调中的熵分布匹配：过拟合较少且多样性更好")中。在此任务中，我们注意到权重衰减并未提高生成多样性，尽管它在之前的示例中显示了缓解过拟合的效果。另一方面，实现了支持原则2的熵正则化带来了输出多样性的好处。
- en: 'Table 2: Evaluation of generation diversity in creative tasks of poem writing
    and story writing. For all criterion, a higher value indicates greater diversity.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：诗歌创作和故事创作中的生成多样性评估。所有标准中，数值越高表示多样性越大。
- en: '| Method | Poem Writing |  | Story Writing |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 诗歌创作 |  | 故事创作 |'
- en: '| N-gram | Self-BLEU | Sentence-BERT |  | N-gram | Self-BLEU | Sentence-BERT
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| N-gram | Self-BLEU | Sentence-BERT |  | N-gram | Self-BLEU | Sentence-BERT
    |'
- en: '| CE | 48.50 | 72.50 | 21.79 |  | 48.74 | 72.77 | 21.94 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| CE | 48.50 | 72.50 | 21.79 |  | 48.74 | 72.77 | 21.94 |'
- en: '| CE+WD | 48.58 | 71.29 | 21.80 |  | 48.85 | 71.73 | 21.79 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 48.58 | 71.29 | 21.80 |  | 48.85 | 71.73 | 21.79 |'
- en: '| CE+Entropy | 53.74 | 75.82 | 23.80 |  | 53.86 | 76.11 | 23.94 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 53.74 | 75.82 | 23.80 |  | 53.86 | 76.11 | 23.94 |'
- en: '| GEM-Linear | 56.50 | 76.73 | 24.73 |  | 56.69 | 76.83 | 24.82 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 56.50 | 76.73 | 24.73 |  | 56.69 | 76.83 | 24.82 |'
- en: '| GEM-LS | 56.55 | 76.31 | 24.63 |  | 56.82 | 76.61 | 24.68 | ![Refer to caption](img/ddc834ad55d41109cd6ca19e2a667790.png)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '| GEM-LS | 56.55 | 76.31 | 24.63 |  | 56.82 | 76.61 | 24.68 | ![参考说明](img/ddc834ad55d41109cd6ca19e2a667790.png)'
- en: 'Figure 2: Performance of using advanced generation strategies such as best-of-n
    and majority voting in chatting (left), math reasoning (middle) and code generation
    (right) tasks.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在聊天（左）、数学推理（中）和代码生成（右）任务中使用先进生成策略，如最佳 n（best-of-n）和多数投票（majority voting）的表现。
- en: Chatting, Math Reasoning, and Code Generation.
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天、数学推理和代码生成。
- en: 'In this part, we show that improved generation diversity offers benefits beyond
    creative writing tasks. Specifically, diverse generation, when quality is ensured,
    is advantageous when using advanced generation methods such as Best-Of-N (BON)
    or Majority-Voting (MV) [[59](#bib.bib59)] to find better solutions. This is inline
    with recent advances in scaling up test-time compute [[6](#bib.bib6), [50](#bib.bib50)]
    and self-distillation [[48](#bib.bib48)]. Specifically, we conduct three experiments
    in chatting, math reasoning and code generation below to validate the superiority
    of GEM through its improved generation diversity. In this part, we use the configuration
    temperature=0.6, top_k=50, and top_p=0.9. A lower temperature is chosen here to
    enhance response quality ($0.6$ is the default value in Llama models). The overall
    performance is displayed in [Figure 2](#S5.F2 "In Creative Writing. ‣ 5.1 General-Purpose
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity") with detailed results in [Appendix E](#A5
    "Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity"). Specific settings
    and analyses are provided below.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了改进的生成多样性在创意写作任务之外的好处。具体来说，当保证质量时，多样化生成在使用先进的生成方法，如最佳 n（BON）或多数投票（MV）[[59](#bib.bib59)]来寻找更好解决方案时具有优势。这与最近在扩展测试时计算[[6](#bib.bib6),
    [50](#bib.bib50)]和自我蒸馏[[48](#bib.bib48)]方面的进展一致。具体来说，我们在聊天、数学推理和代码生成中进行三项实验，以验证
    GEM 通过其改进的生成多样性所表现出的优越性。在这一部分，我们使用配置 temperature=0.6, top_k=50 和 top_p=0.9。这里选择较低的温度以提高响应质量（$0.6$
    是 Llama 模型中的默认值）。总体表现显示在 [图 2](#S5.F2 "在创意写作中。 ‣ 5.1 通用精调 ‣ 5 实验 ‣ LLM 的监督精调中的熵分布匹配：较少的过拟合和更好的多样性")，详细结果见
    [附录 E](#A5 "附录 E 额外结果 ‣ LLM 的监督精调中的熵分布匹配：较少的过拟合和更好的多样性")。具体设置和分析如下。
- en: 'For chatting, we assess the model’s ability to generate human-preferred responses.
    We prompt the trained models to answer 805 questions from the AlpacaEval dataset
    [[31](#bib.bib31)]. For each question, the model generates 32 responses and a
    reward model is then used to select the best responses. We employ the reward model
    FsfairX-LLaMA3-RM-v0.1⁹⁹9[https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1](https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1),
    which has demonstrated top performance on RewardBench [[27](#bib.bib27)], making
    it a reliable choice for this task. Since the reward value itself does not mean
    anything, we choose the win rate as a metric. In particular, we estimate the win
    rate over GPT-4’s generated response by the Bradley–Terry model. From [Figure 2](#S5.F2
    "In Creative Writing. ‣ 5.1 General-Purpose Fine-tuning ‣ 5 Experiments ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity"), we observe that GEM-LS can achieve about 3 points improvement
    in the win rate compared with CE.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天任务，我们评估模型生成符合人类偏好的回答的能力。我们让训练后的模型回答来自 AlpacaEval 数据集的 805 个问题 [[31](#bib.bib31)]。对于每个问题，模型生成
    32 个回答，然后使用奖励模型来选择最佳回答。我们使用奖励模型 FsfairX-LLaMA3-RM-v0.1⁹⁹9[https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1](https://huggingface.co/sfairXC/FsfairX-LLaMA3-RM-v0.1)，该模型在
    RewardBench [[27](#bib.bib27)] 上表现优异，是这个任务的可靠选择。由于奖励值本身没有实际意义，我们选择胜率作为指标。特别地，我们通过
    Bradley–Terry 模型估计 GPT-4 生成的回答的胜率。从[图 2](#S5.F2 "在创意写作中。 ‣ 5.1 通用微调 ‣ 5 个实验 ‣
    LLMs 的有监督微调中的熵分布匹配：减少过拟合和提高多样性")中，我们观察到 GEM-LS 在胜率上比 CE 提升了大约 3 个点。
- en: For math reasoning, we evaluate performance on the GSM8K [[11](#bib.bib11)]
    benchmark, which contains 1,319 test questions. We use chain-of-thought prompts
    [[61](#bib.bib61)] to guide LLMs to generate 32 responses for each question. We
    assess answer accuracy using both Majority-Voting (MV) [[59](#bib.bib59)] and
    Best-Of-N (BON) methods. Compared with CE, GEM-LS shows improvements of up to
    4.8 points (7.7% relative) with MV and 2.5 points (2.8% relative) with BON. The
    strong performance of BON@32 indicates that while the model might know how to
    solve these questions, it is uncertain about these solutions in generation. This
    aligns with previous research [[28](#bib.bib28)], which found that even 7B language
    models demonstrate strong math reasoning abilities through sampling multiple responses.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数学推理，我们在 GSM8K [[11](#bib.bib11)] 基准上评估性能，该基准包含 1,319 个测试问题。我们使用 chain-of-thought
    提示 [[61](#bib.bib61)] 引导 LLMs 为每个问题生成 32 个回答。我们使用 Majority-Voting (MV) [[59](#bib.bib59)]
    和 Best-Of-N (BON) 方法评估回答的准确性。与 CE 相比，GEM-LS 在 MV 上表现出高达 4.8 个点（7.7% 相对）的改进，在 BON
    上表现出 2.5 个点（2.8% 相对）的改进。BON@32 的强劲表现表明，虽然模型可能知道如何解决这些问题，但在生成过程中对这些解决方案仍不确定。这与先前的研究
    [[28](#bib.bib28)] 一致，研究发现即使是 7B 语言模型通过采样多个回答也能表现出强大的数学推理能力。
- en: 'For code generation, we consider two benchmarks: HumanEval [[8](#bib.bib8)]
    and MBPP [[2](#bib.bib2)]. In these scenarios, the trained models are asked to
    generate Python code, and the executor judges their correctness. The common evaluation
    metric is the pass rate over multiple samples. We ask the trained models to generate
    200 samples to estimate the pass@100\. The generation configuration is the same
    as for the chatting task. We find that weight decay does not show significant
    improvement over CE, while GEM-LS can achieve up to a 7.6-point (10.7% relative)
    improvement over CE on HumanEval and a 6.4-point (9.0% relative) improvement on
    MBPP for pass@100.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码生成，我们考虑了两个基准：HumanEval [[8](#bib.bib8)] 和 MBPP [[2](#bib.bib2)]。在这些场景中，训练后的模型需要生成
    Python 代码，由执行者判断其正确性。常用的评估指标是多个样本的通过率。我们要求训练后的模型生成 200 个样本以估计 pass@100。生成配置与聊天任务相同。我们发现，权重衰减对
    CE 没有显著改善，而 GEM-LS 在 HumanEval 上相对于 CE 可以实现高达 7.6 个点（10.7% 相对）的改进，在 MBPP 上实现 6.4
    个点（9.0% 相对）的改进。
- en: 5.2 Domain-specific Fine-tuning
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 特定领域微调
- en: In this section, we conduct experiments with domain-specific datasets. For math
    reasoning, we use the dataset MetaMathQA [[66](#bib.bib66)]. For code generation,
    we use the dataset Magicoder-OSS-Instruct [[62](#bib.bib62)]. The experimental
    setup, including training details and hyperparameters, is the same as before,
    and the specifics are provided in the Appendix.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了特定领域的数据集进行实验。对于数学推理，我们使用数据集 MetaMathQA [[66](#bib.bib66)]。对于代码生成，我们使用数据集
    Magicoder-OSS-Instruct [[62](#bib.bib62)]。实验设置，包括训练细节和超参数，与之前相同，具体细节在附录中提供。
- en: '![Refer to caption](img/4196260f5b1a49daea8bad5c9fcc0b26.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4196260f5b1a49daea8bad5c9fcc0b26.png)'
- en: 'Figure 3: Performance on GSM8K (left) and MATH (right) when fine-tuning Llama-3-8B
    with the MetaMathQA dataset.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用 MetaMathQA 数据集微调 Llama-3-8B 时，GSM8K（左）和 MATH（右）的性能。
- en: Math Reasoning.
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学推理。
- en: 'We present the final answer accuracy for two benchmarks: GSM8K, which has been
    studied previously, and MATH [[21](#bib.bib21)], which is more challenging^(10)^(10)10MATH
    is not studied in models fine-tuned with the UltraFeedback dataset because models
    have poor performance in this task., as shown in [Figure 3](#S5.F3 "In 5.2 Domain-specific
    Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). Following the previous set-up,
    we evaluate performance using Majority Voting over 32 samples (MV@32), and Best-Of-N
    over 32 samples (BON@32). The greedy decoding performance is also reported. Our
    results show significant improvements in both majority voting and greedy decoding
    across both GSM8K and MATH. We observe that the weight decay regularization performs
    well on GSM8K but shows no clear improvement on MATH. In contrast, GEM-LS outperforms
    CE on GSM8K by 1.2 points (1.7% relative), 2.9 points (3.8% relative), and 2.6
    points (2.9% relative) for greedy decoding, MV@32, and BON@32, respectively. On
    the MATH benchmark, GEM-LS shows improvements of 1.9 points (8.0% relative), 1.7
    points (5.8% relative), and 1.6 points (2.7% relative) for the same methods. These
    improvements in greedy decoding indicate that entropy regularization methods effectively
    mitigate overfitting, while the enhancements in MV and BON suggest increased generation
    diversity.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了两个基准测试的最终答案准确率：GSM8K，这是之前研究过的，和 MATH [[21](#bib.bib21)]，它更具挑战性^(10)^(10)10MATH
    在使用 UltraFeedback 数据集进行模型微调时没有被研究，因为模型在这项任务上的表现较差，如[图 3](#S5.F3 "在 5.2 领域特定微调
    ‣ 5 实验 ‣ LLMs 的监督微调中的熵分布匹配：较少的过拟合和更好的多样性")所示。按照之前的设置，我们使用对 32 个样本进行的多数投票（MV@32）和对
    32 个样本进行的最佳 N（BON@32）来评估性能。我们还报告了贪心解码的性能。我们的结果显示，在 GSM8K 和 MATH 的多数投票和贪心解码中都有显著改进。我们观察到，权重衰减正则化在
    GSM8K 上表现良好，但在 MATH 上没有明显改进。相比之下，GEM-LS 在 GSM8K 上比 CE 提高了 1.2 分（相对 1.7%）、2.9 分（相对
    3.8%）和 2.6 分（相对 2.9%），分别对应贪心解码、MV@32 和 BON@32。在 MATH 基准上，GEM-LS 对相同方法显示了 1.9 分（相对
    8.0%）、1.7 分（相对 5.8%）和 1.6 分（相对 2.7%）的改进。这些贪心解码的改进表明，熵正则化方法有效缓解了过拟合，而 MV 和 BON
    的改进则表明生成多样性增加。
- en: Code Generation.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成。
- en: 'Following the previous set-up, we report the pass rate over {1, 10, 100} on
    two key benchmarks, HumanEval and MBPP, in [Figure 4](#S5.F4 "In Code Generation.
    ‣ 5.2 Domain-specific Fine-tuning ‣ 5 Experiments ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity"). We
    find that the Pass@100 performance on HumanEval is reduced compared with the previous
    one while the Pass@100 performance on MBPP improved for all methods. We observe
    that weight decay (WD) does not achive consistent improvement while entropy regularization
    does. Notably, GEM-LS significantly enhances performance over CE: on HumanEval,
    it improves by 4.6 points (11.7% relative) for Pass@1, 6.5 points (11.1% relative)
    for Pass@10, and 9.7 points (14.7% relative) for Pass@100\. On MBPP, GEM-LS achieves
    gains of 3.4 points (6.3% relative) for Pass@1, 6.8 points (10.2% relative) for
    Pass@10, and 8.0 points (11.1% relative) for Pass@100\. These results suggest
    similar conclusions regarding overfitting and generation diversity as before.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的设置，我们报告了在两个关键基准测试 HumanEval 和 MBPP 上的 {1, 10, 100} 通过率，如[图 4](#S5.F4 "在代码生成。
    ‣ 5.2 领域特定微调 ‣ 5 实验 ‣ LLMs 的监督微调中的熵分布匹配：较少的过拟合和更好的多样性")所示。我们发现 HumanEval 上的 Pass@100
    性能较之前有所下降，而 MBPP 上的 Pass@100 性能在所有方法中均有所提升。我们观察到，权重衰减（WD）没有实现一致的改进，而熵正则化则有显著改进。值得注意的是，GEM-LS
    显著提升了 CE 的性能：在 HumanEval 上，Pass@1 提高了 4.6 分（相对 11.7%），Pass@10 提高了 6.5 分（相对 11.1%），Pass@100
    提高了 9.7 分（相对 14.7%）。在 MBPP 上，GEM-LS 分别提高了 3.4 分（相对 6.3%）、6.8 分（相对 10.2%）和 8.0
    分（相对 11.1%），对应 Pass@1、Pass@10 和 Pass@100。这些结果表明与之前类似的结论，涉及过拟合和生成多样性。
- en: '![Refer to caption](img/145a0c3451f7789e4cf8380a4e4da763.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/145a0c3451f7789e4cf8380a4e4da763.png)'
- en: 'Figure 4: Performance on HumanEval (left) and MBPP (right) when fine-tuning
    Llama-3-8B with the MagiCoder-OSS-Instruct dataset.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用 MagiCoder-OSS-Instruct 数据集微调 Llama-3-8B 时，HumanEval（左）和 MBPP（右）的性能。
- en: 6 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we propose an alternative method for the SFT of LLMs to tackle
    the challenges of overfitting and limited generation diversity, which are often
    caused by the aggressive updates of the CE loss and limited data. We demonstrate
    the effectiveness of combining generative distribution matching with entropy regularization.
    We note that the improved diversity also boosts performance in downstream tasks
    when advanced generation methods, such as the best-of-n sampling, are used. Overall,
    our results indicate that the proposed method is well-suited for generative models.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种替代方法来进行LLM的SFT，以应对过拟合和生成多样性有限的挑战，这些问题通常由CE损失的激进更新和有限的数据引起。我们展示了将生成分布匹配与熵正则化相结合的有效性。我们注意到，改进的多样性在使用先进的生成方法，如最佳的n次采样时，也能提升下游任务的表现。总体而言，我们的结果表明，所提出的方法非常适合生成模型。
- en: 'We focus on the initial stage of post-training pipeline in this paper and recognize
    that the models trained with our proposed methods can be further refined in subsequent
    stages. Notably, the enhanced diversity achieved by our approach can be advantageous
    in several contexts: it supports scaling up test-time computation [[6](#bib.bib6),
    [50](#bib.bib50)], improves exploration in RL methods [[46](#bib.bib46), [33](#bib.bib33)],
    addresses the preference collapse issue [[64](#bib.bib64)], facilitates self-improvement
    through distillation with best-of-n techniques [[48](#bib.bib48)], and helps mitigate
    mode collapse in synthetic data generation [[49](#bib.bib49), [5](#bib.bib5),
    [63](#bib.bib63)]. We see significant potential for our method in these areas
    and plan to explore these topics in future work.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中集中关注了后训练流程的初始阶段，并认识到使用我们提出的方法训练的模型可以在后续阶段进一步优化。特别是，我们的方法所获得的增强多样性在多个背景下具有优势：它支持扩大测试时间计算[[6](#bib.bib6),
    [50](#bib.bib50)]，改善RL方法中的探索[[46](#bib.bib46), [33](#bib.bib33)]，解决偏好崩溃问题[[64](#bib.bib64)]，通过最佳的n技术进行蒸馏以促进自我改进[[48](#bib.bib48)]，并有助于缓解合成数据生成中的模式崩溃问题[[49](#bib.bib49),
    [5](#bib.bib5), [63](#bib.bib63)]。我们看到我们的方法在这些领域具有显著潜力，并计划在未来的工作中深入探讨这些主题。
- en: Acknowledgement
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Yushun Zhang for reading the manuscript and providing useful feedback.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Yushun Zhang阅读手稿并提供有用的反馈。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Adler et al. [2024] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab
    Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan
    Cohen, et al. Nemotron-4 340b technical report. *arXiv preprint arXiv:2406.11704*,
    2024.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adler等 [2024] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H Anh, Pallab Bhattacharya,
    Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, 等.
    Nemotron-4 340b技术报告。*arXiv预印本 arXiv:2406.11704*，2024年。
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin等 [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
    Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, 等.
    使用大型语言模型进行程序合成。*arXiv预印本 arXiv:2108.07732*，2021年。
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等 [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等. 通过人类反馈的强化学习训练有用且无害的助手。*arXiv预印本
    arXiv:2204.05862*，2022年。
- en: Bengio et al. [2021] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup,
    and Yoshua Bengio. Flow network based generative models for non-iterative diverse
    candidate generation. *Advances in Neural Information Processing Systems*, 34:27381–27394,
    2021.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio等 [2021] Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup,
    和 Yoshua Bengio. 基于流网络的生成模型用于非迭代多样化候选生成。*神经信息处理系统进展*，34:27381–27394，2021年。
- en: Bertrand et al. [2023] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis,
    Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining
    of generative models on their own data. *arXiv preprint arXiv:2310.00429*, 2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertrand等 [2023] Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco
    Jiralerspong, 和 Gauthier Gidel. 关于生成模型在其自身数据上迭代再训练的稳定性。*arXiv预印本 arXiv:2310.00429*，2023年。
- en: 'Brown et al. [2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark,
    Quoc V Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling
    inference compute with repeated sampling. *arXiv preprint arXiv:2407.21787*, 2024.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人 [2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc
    V Le, Christopher Ré, 和 Azalia Mirhoseini。大型语言猴子：通过重复采样来扩展推理计算。*arXiv预印本 arXiv:2407.21787*，2024。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in Neural Information
    Processing Systems 33*, pages 1877–1901, 2020.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人。语言模型是少样本学习者。*神经信息处理系统进展 33*，第1877–1901页，2020。
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人 [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等人。评估训练于代码的大型语言模型。*arXiv预印本 arXiv:2107.03374*，2021。
- en: Chen et al. [2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan
    Gu. Self-play fine-tuning converts weak language models to strong language models.
    *arXiv preprint arXiv:2401.01335*, 2024.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人 [2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, 和 Quanquan
    Gu。自我对弈微调将弱语言模型转变为强语言模型。*arXiv预印本 arXiv:2401.01335*，2024。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *Journal of Machine Learning Research*,
    25(70):1–53, 2024.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等人 [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma 等人。扩展指令微调的语言模型。*机器学习研究杂志*，25(70):1–53，2024。
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人 [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano
    等人。训练验证器以解决数学文字问题。*arXiv预印本 arXiv:2110.14168*，2021。
- en: 'Colton and Wiggins [2012] Simon Colton and Geraint A Wiggins. Computational
    creativity: The final frontier? In *ECAI 2012*, pages 21–26\. IOS Press, 2012.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colton和Wiggins [2012] Simon Colton 和 Geraint A Wiggins。计算创意：最终的边界？在 *ECAI 2012*
    中，第21–26页。IOS出版社，2012。
- en: 'Cui et al. [2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang
    He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback:
    Boosting language models with scaled ai feedback. In *Forty-first International
    Conference on Machine Learning*, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui等人 [2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He,
    Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin 等人。Ultrafeedback: 通过扩大AI反馈来提升语言模型。在
    *第41届国际机器学习大会*，2024。'
- en: Dubey et al. [2018] Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, and Nikhil
    Naik. Maximum-entropy fine grained classification. *Advances in neural information
    processing systems*, 31, 2018.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey等人 [2018] Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, 和 Nikhil Naik。最大熵细粒度分类。*神经信息处理系统进展*，31，2018。
- en: Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
    Angela Fan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*,
    2024.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey等人 [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
    Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan
    等人。Llama 3 模型群。*arXiv预印本 arXiv:2407.21783*，2024。
- en: 'Fu et al. [2024] Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, and Rui Yan.
    Disperse-then-merge: Pushing the limits of instruction tuning via alignment tax
    reduction. *arXiv preprint arXiv:2405.13432*, 2024.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu等人 [2024] Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, 和 Rui Yan。Disperse-then-merge:
    通过减少对齐税来推动指令调整的极限。*arXiv预印本 arXiv:2405.13432*，2024。'
- en: Gekhman et al. [2024] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir
    Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge
    encourage hallucinations? *arXiv preprint arXiv:2405.05904*, 2024.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gekhman等人 [2024] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder,
    Roi Reichart, 和 Jonathan Herzig。对新知识进行微调是否会促使幻觉的产生？*arXiv预印本 arXiv:2405.05904*，2024。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络。*神经信息处理系统进展*，27，2014年。
- en: 'Guo et al. [2023] Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, and Chloé
    Clavel. The curious decline of linguistic diversity: Training language models
    on synthetic text. *arXiv preprint arXiv:2311.09807*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 [2023] Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, 和 Chloé Clavel.
    语言多样性的好奇下降：在合成文本上训练语言模型。*arXiv 预印本 arXiv:2311.09807*，2023年。
- en: 'Gweon et al. [2014] Hyowon Gweon, Hannah Pelton, Jaclyn A Konopka, and Laura E
    Schulz. Sins of omission: Children selectively explore when teachers are under-informative.
    *Cognition*, 132(3):335–341, 2014.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gweon 等 [2014] Hyowon Gweon, Hannah Pelton, Jaclyn A Konopka, 和 Laura E Schulz.
    遗漏的罪过：当教师信息不足时，儿童会有选择性地探索。*认知*，132(3):335–341，2014年。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 用数学数据集测量数学问题解决能力。*arXiv
    预印本 arXiv:2103.03874*，2021年。
- en: Ho and Ermon [2016] Jonathan Ho and Stefano Ermon. Generative adversarial imitation
    learning. In *Advances in Neural Information Processing Systems 29*, pages 4565–4573,
    2016.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 和 Ermon [2016] Jonathan Ho 和 Stefano Ermon. 生成对抗模仿学习。见 *神经信息处理系统进展 29*，第4565–4573页，2016年。
- en: Hu et al. [2023] Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume
    Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in
    large language models. *arXiv preprint arXiv:2310.04363*, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 [2023] Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume
    Lajoie, Yoshua Bengio, 和 Nikolay Malkin. 在大型语言模型中摊销难以处理的推理。*arXiv 预印本 arXiv:2310.04363*，2023年。
- en: 'Jolicoeur-Martineau [2018] Alexia Jolicoeur-Martineau. The relativistic discriminator:
    a key element missing from standard gan. *arXiv preprint arXiv:1807.00734*, 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jolicoeur-Martineau [2018] Alexia Jolicoeur-Martineau. 相对论判别器：标准GAN中缺失的关键元素。*arXiv
    预印本 arXiv:1807.00734*，2018年。
- en: Jolicoeur-Martineau [2020] Alexia Jolicoeur-Martineau. On relativistic f-divergences.
    In *International Conference on Machine Learning*, pages 4931–4939\. PMLR, 2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jolicoeur-Martineau [2020] Alexia Jolicoeur-Martineau. 关于相对论 f-散度。见 *国际机器学习会议*，第4931–4939页。PMLR，2020年。
- en: Kirk et al. [2023] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding
    the effects of rlhf on llm generalisation and diversity. *arXiv preprint arXiv:2310.06452*,
    2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk 等 [2023] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, 和 Roberta Raileanu. 理解RLHF对LLM泛化和多样性的影响。*arXiv
    预印本 arXiv:2310.06452*，2023年。
- en: 'Lambert et al. [2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda,
    Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
    et al. Rewardbench: Evaluating reward models for language modeling. *arXiv preprint
    arXiv:2403.13787*, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambert 等 [2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda,
    Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi
    等. Rewardbench：评估语言建模的奖励模型。*arXiv 预印本 arXiv:2403.13787*，2024年。
- en: Li et al. [2024a] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng,
    Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess
    strong math capabilities. *arXiv preprint arXiv:2403.04706*, 2024a.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2024a] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han
    Hu, Zheng Zhang, 和 Houwen Peng. 常见的7b语言模型已具备强大的数学能力。*arXiv 预印本 arXiv:2403.04706*，2024a年。
- en: 'Li et al. [2024b] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo
    Garcia, and Mingyi Hong. Getting more juice out of the sft data: Reward learning
    from human demonstration improves sft for llm alignment. *arXiv preprint arXiv:2405.17888*,
    2024b.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2024b] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia,
    和 Mingyi Hong. 从sft数据中榨取更多收益：从人类示范中学习奖励，提高llm对齐的sft。*arXiv 预印本 arXiv:2405.17888*，2024b年。
- en: Li et al. [2015] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and
    Bill Dolan. A diversity-promoting objective function for neural conversation models.
    *arXiv preprint arXiv:1510.03055*, 2015.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2015] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, 和 Bill Dolan.
    用于神经对话模型的多样性促进目标函数。*arXiv 预印本 arXiv:1510.03055*，2015年。
- en: 'Li et al. [2023a] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023a.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2023a] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani,
    Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto。Alpacaeval：指令跟随模型的自动评估器。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)，2023a年。
- en: Li et al. [2022] Ziniu Li, Tian Xu, and Yang Yu. A note on target q-learning
    for solving finite mdps with a generative oracle. *arXiv preprint arXiv:2203.11489*,
    2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2022] Ziniu Li, Tian Xu 和 Yang Yu。关于使用生成型神谕解决有限 Markov 决策过程的目标 Q 学习的说明。*arXiv
    预印本 arXiv:2203.11489*，2022年。
- en: 'Li et al. [2023b] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, and
    Zhi-Quan Luo. Remax: A simple, effective, and efficient method for aligning large
    language models. *arXiv preprint arXiv:2310.10505*, 2023b.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2023b] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun 和 Zhi-Quan
    Luo。Remax：对齐大语言模型的简单、有效且高效的方法。*arXiv 预印本 arXiv:2310.10505*，2023b年。
- en: Liu et al. [2023] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
    What makes good data for alignment? a comprehensive study of automatic data selection
    in instruction tuning. *arXiv preprint arXiv:2312.15685*, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang 和 Junxian He。什么数据适合对齐？对指令调优中自动数据选择的全面研究。*arXiv
    预印本 arXiv:2312.15685*，2023年。
- en: Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
    learning. *Nature*, 518(7540):529–533, 2015.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
    Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
    Georg Ostrovski 等。通过深度强化学习实现人类水平的控制。*自然*，518(7540):529–533，2015年。
- en: 'Mostafazadeh et al. [2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
    He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen.
    A corpus and cloze evaluation for deeper understanding of commonsense stories.
    In *Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 839–849, 2016.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh 等 [2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He,
    Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, 和 James Allen。用于更深入理解常识故事的语料库和填空评估。在
    *2016年北美计算语言学协会年会：人类语言技术会议论文集*，第839–849页，2016年。
- en: O’Mahony et al. [2024] Laura O’Mahony, Leo Grinsztajn, Hailey Schoelkopf, and
    Stella Biderman. Attributing mode collapse in the fine-tuning of large language
    models. In *ICLR 2024 Workshop on Mathematical and Empirical Understanding of
    Foundation Models*, 2024.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Mahony 等 [2024] Laura O’Mahony, Leo Grinsztajn, Hailey Schoelkopf 和 Stella
    Biderman。在大语言模型的微调中归因模式崩溃。在 *ICLR 2024 数学与经验理解基础模型研讨会*，2024年。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023年。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems 35*, pages 27730–27744, 2022.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展
    35*，第27730–27744页，2022年。
- en: Padmakumar and He [2023] Vishakh Padmakumar and He He. Does writing with language
    models reduce content diversity? *arXiv preprint arXiv:2309.05196*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padmakumar 和 He [2023] Vishakh Padmakumar 和 He He。使用语言模型是否会减少内容多样性？*arXiv 预印本
    arXiv:2309.05196*，2023年。
- en: Pereyra et al. [2017] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz
    Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident
    output distributions. *arXiv preprint arXiv:1701.06548*, 2017.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pereyra 等 [2017] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser
    和 Geoffrey Hinton。通过惩罚自信的输出分布来正则化神经网络。*arXiv 预印本 arXiv:1701.06548*，2017年。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 2019.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等。语言模型是无监督的多任务学习者。*OpenAI 博客*，2019年。
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型。*arXiv 预印本
    arXiv:2305.18290*，2023年。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 探索统一文本到文本的变换器在迁移学习中的极限。*机器学习研究期刊*，21(140):1–67，2020年。
- en: Ross et al. [2011] Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction
    of imitation learning and structured prediction to no-regret online learning.
    In *Proceedings of the 14th International Conference on Artificial Intelligence
    and Statistics*, pages 627–635, 2011.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross et al. [2011] Stéphane Ross, Geoffrey J. Gordon, 和 Drew Bagnell. 将模仿学习和结构化预测简化为无遗憾的在线学习。在*第十四届人工智能与统计学国际会议论文集*，第627–635页，2011年。
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv*, 1707.06347,
    2017.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, 和 Oleg Klimov. 近端策略优化算法。*arXiv*，1707.06347，2017年。
- en: 'Schulz and Bonawitz [2007] Laura E Schulz and Elizabeth Baraff Bonawitz. Serious
    fun: preschoolers engage in more exploratory play when evidence is confounded.
    *Developmental psychology*, 43(4):1045, 2007.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulz and Bonawitz [2007] Laura E Schulz 和 Elizabeth Baraff Bonawitz. 严肃的乐趣：当证据混淆时，学前儿童更倾向于进行更多探索性游戏。*发展心理学*，43(4):1045，2007年。
- en: 'Sessa et al. [2024] Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot,
    Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe
    Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation.
    *arXiv preprint arXiv:2407.14622*, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sessa et al. [2024] Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot,
    Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe
    Friesen, Geoffrey Cideron, 等等。Bond: 用最优选择蒸馏对齐大型语言模型。*arXiv 预印本 arXiv:2407.14622*，2024年。'
- en: 'Shumailov et al. [2023] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin
    Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on
    generated data makes models forget. *arXiv preprint arXiv:2305.17493*, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shumailov et al. [2023] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin
    Gal, Nicolas Papernot, 和 Ross Anderson. 递归的诅咒：在生成数据上训练使模型遗忘。*arXiv 预印本 arXiv:2305.17493*，2023年。
- en: Snell et al. [2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
    Scaling llm test-time compute optimally can be more effective than scaling model
    parameters. *arXiv preprint arXiv:2408.03314*, 2024.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell et al. [2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, 和 Aviral Kumar. 最优扩展大型语言模型的测试时间计算可能比扩展模型参数更有效。*arXiv
    预印本 arXiv:2408.03314*，2024年。
- en: Sun et al. [2020] Ruoyu Sun, Tiantian Fang, and Alexander Schwing. Towards a
    better global loss landscape of gans. *Advances in Neural Information Processing
    Systems*, 33:10186–10198, 2020.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2020] Ruoyu Sun, Tiantian Fang, 和 Alexander Schwing. 朝着更好的生成对抗网络的全局损失景观前进。*神经信息处理系统进展*，33:10186–10198，2020年。
- en: 'Team et al. [2024] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. [2024] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, 等等。Gemma: 基于双子研究和技术的开放模型。*arXiv 预印本 arXiv:2403.08295*，2024年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等等。Llama 2: 开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: 'Tunstall et al. [2023] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. *arXiv
    preprint arXiv:2310.16944*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunstall et al. [2023] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, 等等。Zephyr: 直接蒸馏语言模型对齐。*arXiv 预印本 arXiv:2310.16944*，2023年。'
- en: 'Turrigiano [2012] Gina Turrigiano. Homeostatic synaptic plasticity: local and
    global mechanisms for stabilizing neuronal function. *Cold Spring Harbor perspectives
    in biology*, 4(1):a005736, 2012.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turrigiano [2012] Gina Turrigiano. 稳态突触可塑性：稳定神经功能的局部和全局机制。*Cold Spring Harbor
    生物学观点*，4(1):a005736，2012年。
- en: 'Turrigiano [2008] Gina G Turrigiano. The self-tuning neuron: synaptic scaling
    of excitatory synapses. *Cell*, 135(3):422–435, 2008.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turrigiano [2008] Gina G Turrigiano. 自调节神经元：兴奋性突触的突触缩放。*Cell*, 135(3):422–435,
    2008。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Advances in Neural Information Processing Systems 30*, pages
    5998–6008, 2017.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin. 注意力机制是你所需要的一切。在*神经信息处理系统进展
    30*，第5998–6008页，2017年。
- en: 'Vieillard et al. [2020] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier
    Pietquin, Rémi Munos, and Matthieu Geist. Leverage the average: an analysis of
    kl regularization in reinforcement learning. In *Advances in Neural Information
    Processing Systems 33*, pages 12163–12174, 2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieillard et al. [2020] Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier
    Pietquin, Rémi Munos, 和 Matthieu Geist. 利用平均值：强化学习中 KL 正则化的分析。在*神经信息处理系统进展 33*，第12163–12174页，2020年。
- en: Wang et al. [2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *Proceedings of the 11st International
    Conference on Learning Representations*, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 自洽性提高了语言模型中的思维链推理。在*第11届国际学习表征会议论文集*中，2023年。
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le. 微调语言模型是零样本学习者。*arXiv预印本
    arXiv:2109.01652*，2021年。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837, 2022.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 思维链提示激发了大语言模型中的推理。*神经信息处理系统进展*，35:24824–24837，2022年。
- en: 'Wei et al. [2024] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. Magicoder: Empowering code generation with oss-instruct. In *Forty-first
    International Conference on Machine Learning*, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. [2024] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, 和 Lingming
    Zhang. Magicoder: 通过 oss-instruct 提升代码生成。在*第41届国际机器学习会议*中，2024年。'
- en: Wu et al. [2024] Ting Wu, Xuefeng Li, and Pengfei Liu. Progress or regress?
    self-improvement reversal in post-training. *arXiv preprint arXiv:2407.05013*,
    2024.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2024] Ting Wu, Xuefeng Li, 和 Pengfei Liu. 进步还是倒退？后训练中的自我改进逆转。*arXiv预印本
    arXiv:2407.05013*，2024年。
- en: 'Xiao et al. [2024] Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong
    Fang, Qi Long, and Weijie J Su. On the algorithmic bias of aligning large language
    models with rlhf: Preference collapse and matching regularization. *arXiv preprint
    arXiv:2405.16455*, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. [2024] Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang,
    Qi Long, 和 Weijie J Su. 对齐大语言模型的算法偏见：偏好崩溃和匹配正则化。*arXiv预印本 arXiv:2405.16455*，2024年。
- en: 'Young et al. [2024] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,
    Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open
    foundation models by 01\. ai. *arXiv preprint arXiv:2403.04652*, 2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young et al. [2024] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,
    Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, 等. Yi: 由 01\.
    ai 开放的基础模型。*arXiv预印本 arXiv:2403.04652*，2024年。'
- en: 'Yu et al. [2023] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath:
    Bootstrap your own mathematical questions for large language models. *arXiv preprint
    arXiv:2309.12284*, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. [2023] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, 和 Weiyang Liu. Metamath:
    为大语言模型自生成数学问题。*arXiv预印本 arXiv:2309.12284*，2023年。'
- en: 'Zhang et al. [2024] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat.
    When scaling meets llm finetuning: The effect of data, model and finetuning method.
    *arXiv preprint arXiv:2402.17193*, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] Biao Zhang, Zhongtao Liu, Colin Cherry 和 Orhan Firat. 当扩展遇上
    LLM 微调：数据、模型和微调方法的影响。*arXiv 预印本 arXiv:2402.17193*，2024。
- en: 'Zhou et al. [2023a] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: less is more for alignment.
    *arXiv preprint arXiv:2305.11206*, 2023a.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [2023a] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer 和 Omer Levy. LIMA: 更少即更多的对齐方法。*arXiv 预印本 arXiv:2305.11206*，2023a。'
- en: Zhou et al. [2023b] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma,
    Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation
    for large language models. *arXiv preprint arXiv:2311.07911*, 2023b.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2023b] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma,
    Sujoy Basu, Yi Luan, Denny Zhou 和 Le Hou. 大型语言模型的指令跟随评估。*arXiv 预印本 arXiv:2311.07911*，2023b。
- en: Appendix A Implementation of GEM
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A GEM 实现
- en: 1def  gem_loss(logits,  labels,  beta=0.7,  ignore_index=-100,  h="linear"):23  shift_logits  =  logits[...,  :-1,  :].contiguous()4  shift_labels  =  labels[...,  1:].contiguous()56  mask  =  shift_labels  !=  ignore_index7  shift_logits  =  shift_logits[mask]8  shift_labels  =  shift_labels[mask]910  with  torch.no_grad():11  logits_on_labels  =  torch.gather(12  shift_logits,  dim=-1,  index=shift_labels.unsqueeze(-1)13  ).squeeze(-1)1415  logits_diff  =  shift_logits  -  logits_on_labels.unsqueeze(-1)16  if  h  ==  "linear":17  weights  =  torch.ones_like(logits_diff)18  elif  h  ==  "log_sigmoid":19  weights  =  F.sigmoid(0.01  *  logits_diff)20  else:21  raise  ValueError(h)2223  gene_log_probs  =  F.log_softmax(shift_logits,  dim=-1)24  q_probs  =  torch.exp(25  F.log_softmax(shift_logits  /  beta,  dim=-1)26  ).detach()2728  real_log_probs  =  torch.gather(29  gene_log_probs,  dim=-1,  index=shift_labels.unsqueeze(-1)30  ).squeeze(-1)3132  loss  =  -torch.sum(33  q_probs  *  weights  *  (real_log_probs.unsqueeze(-1)  -  gene_log_probs),  dim=-134  ).mean()3536  return  loss
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 1def gem_loss(logits, labels, beta=0.7, ignore_index=-100, h="linear"):23 shift_logits
    = logits[..., :-1, :].contiguous()4 shift_labels = labels[..., 1:].contiguous()56
    mask = shift_labels != ignore_index7 shift_logits = shift_logits[mask]8 shift_labels
    = shift_labels[mask]910 with torch.no_grad():11 logits_on_labels = torch.gather(12
    shift_logits, dim=-1, index=shift_labels.unsqueeze(-1)13 ).squeeze(-1)1415 logits_diff
    = shift_logits - logits_on_labels.unsqueeze(-1)16 if h == "linear":17 weights
    = torch.ones_like(logits_diff)18 elif h == "log_sigmoid":19 weights = F.sigmoid(0.01
    * logits_diff)20 else:21 raise ValueError(h)2223 gene_log_probs = F.log_softmax(shift_logits,
    dim=-1)24 q_probs = torch.exp(25 F.log_softmax(shift_logits / beta, dim=-1)26
    ).detach()2728 real_log_probs = torch.gather(29 gene_log_probs, dim=-1, index=shift_labels.unsqueeze(-1)30
    ).squeeze(-1)3132 loss = -torch.sum(33 q_probs * weights * (real_log_probs.unsqueeze(-1)
    - gene_log_probs), dim=-134 ).mean()3536 return loss
- en: 'Listing 1: Pytorch Code of GEM'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 1: GEM 的 Pytorch 代码'
- en: We have two remarks regarding the implementation above. First, we use a coefficient
    of $0.01$ to scale the input in the log-sigmoid function. This ensures that the
    function behaves nearly linearly. Second, this implementation requires almost
    the same GPU memory and computation time as the CE loss.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述实现，我们有两点说明。首先，我们使用 $0.01$ 的系数来缩放 log-sigmoid 函数中的输入，这确保了该函数几乎线性。其次，这种实现所需的
    GPU 内存和计算时间几乎与 CE 损失相同。
- en: Appendix B Proof
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 证明
- en: 'Proof of [Proposition 1](#Thmprop1 "Proposition 1\. ‣ Proposed Solution. ‣
    4.2 Proposed Algorithm: GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity").'
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[命题 1 的证明](#Thmprop1 "Proposition 1\. ‣ Proposed Solution. ‣ 4.2 Proposed Algorithm:
    GEM ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")。'
- en: When $h$ is a linear function, we have that
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $h$ 是线性函数时，我们有：
- en: '|  | $\displaystyle{\mathcal{L}}_{q}(f)$ |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}}_{q}(f)$ |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$
    |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim
    p(\cdot&#124;x)}\mathbb{E}_{y^{\texttt{gene}}\sim q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\left[\log
    f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{real}}\sim p(\cdot&#124;x)}\left[\log
    f(y^{\texttt{real}}&#124;x)\right]-\mathbb{E}_{x}\mathbb{E}_{y^{\texttt{gene}}\sim
    q(\cdot&#124;x)}\left[\log f(y^{\texttt{gene}}&#124;x)\right]$ |  |'
- en: For any $x\in{\mathcal{X}}$, we have that
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $x\in{\mathcal{X}}$，我们有
- en: '|  | $\displaystyle\frac{\partial{\mathcal{L}}}{\partial f}=\frac{p-q}{f}$
    |  | (6) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial{\mathcal{L}}}{\partial f}=\frac{p-q}{f}$
    |  | (6) |'
- en: 'To calculate the stationary point of ${\mathcal{L}}$. Since $q=\texttt{softmax}(1/\beta\cdot\log
    f)$. As analyzed in [Proposition 2](#Thmprop2 "Proposition 2\. ‣ Appendix B Proof
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"), for $\beta=1/(\gamma+1)$, this corresponds to the the
    optimal solution of minimizing reverse KL with entropy regularization.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '为了计算 ${\mathcal{L}}$ 的驻点。由于 $q=\texttt{softmax}(1/\beta\cdot\log f)$。如在[命题
    2](#Thmprop2 "Proposition 2\. ‣ Appendix B Proof ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")中分析的，对于
    $\beta=1/(\gamma+1)$，这对应于最小化反向KL并进行熵正则化的最优解。'
- en: ∎
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Proposition 2.
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 2。
- en: 'For the entropy-regularized KL minimization problem in [Equation 1](#S4.E1
    "In 4.1 Proposed Formulation: Reserve KL with Entropy Regularization ‣ 4 Entropic
    Distribution Matching ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"), in the function space, we have
    the optimal solution:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '对于[方程 1](#S4.E1 "In 4.1 Proposed Formulation: Reserve KL with Entropy Regularization
    ‣ 4 Entropic Distribution Matching ‣ Entropic Distribution Matching in Supervised
    Fine-tuning of LLMs: Less Overfitting and Better Diversity")中的熵正则化KL最小化问题，在函数空间中，我们得到最优解：'
- en: '|  | $\displaystyle f^{\star}(y&#124;x)=\frac{1}{Z_{x}}p(y&#124;x)^{1/(\gamma+1)}$
    |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f^{\star}(y&#124;x)=\frac{1}{Z_{x}}p(y&#124;x)^{1/(\gamma+1)}$
    |  |'
- en: where $Z_{x}$.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z_{x}$。
- en: The proof is based on the optimality condition of constrained optimization.
    Its proof can be found in the previous literature (see, e.g., [[58](#bib.bib58),
    Appendix A]). We note that the above closed-form solution cannot be applied in
    practice because we do not have access to the density function of the data distribution
    $p$.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 证明基于约束优化的最优性条件。其证明可以在以前的文献中找到（参见，例如，[[58](#bib.bib58)，附录 A]）。我们注意到，由于我们无法访问数据分布
    $p$ 的密度函数，上述封闭形式的解在实践中无法应用。
- en: Appendix C Discussion
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 讨论
- en: '![Refer to caption](img/5ca919f06839283b044ea4dfb5540f84.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5ca919f06839283b044ea4dfb5540f84.png)'
- en: 'Figure 5: Distributions of next-token probabilities for trained models with
    the UltraFeedback dataset, presented from top to bottom: CE, CE+Entropy, GEM-LS.
    The prompt is “Give me a single-digit number”. The top 300 probabilities are shown
    with a subsampling rate of 20 for clear visualization. A red dotted line indicates
    the probability threshold of $10^{-4}$. The figure demonstrates that the CE+Entropy
    model has a longer tail with higher probabilities assigned to some nonsensical
    tokens, marked with crosses.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 UltraFeedback 数据集训练模型的下一个词概率分布，从上到下呈现：CE，CE+熵，GEM-LS。提示是“给我一个单一数字”。显示了前
    300 个概率，并采用 20 的子采样率以便于清晰可视化。红色虚线表示 $10^{-4}$ 的概率阈值。图示表明，CE+熵模型的尾部更长，并且分配给一些无意义标记的概率更高，这些标记用交叉符号标记。
- en: 'We discuss the formulation of forward KL with entropy regularization in this
    section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节讨论了具有熵正则化的前向KL的表述：
- en: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    p(\cdot&#124;x)}[\log f(y&#124;x)]}_{=-D_{\mathrm{KL}}(p,f)+\text{constant}}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[-\log f(y&#124;x)]}_{={\mathcal{H}}(f)}\big{\}}$ |  | (7) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{f}\mathbb{E}_{x}\big{\{}\underbrace{\mathbb{E}_{y\sim
    p(\cdot&#124;x)}[\log f(y&#124;x)]}_{=-D_{\mathrm{KL}}(p,f)+\text{constant}}+\gamma\cdot\underbrace{\mathbb{E}_{y\sim
    f(\cdot&#124;x)}[-\log f(y&#124;x)]}_{={\mathcal{H}}(f)}\big{\}}$ |  | (7) |'
- en: 'This formulation supports the proposed Principle 2 but not Principle 1\. We
    find that this formulation leads to an improper increase in tail probabilities
    when maximizing the entropy, as illustrated in [Figure 5](#A3.F5 "In Appendix
    C Discussion ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs:
    Less Overfitting and Better Diversity"). In the context of LLMs, this increase
    often translates into nonsensical tokens in the vocabulary, leading to undesirable
    generation outputs (if additional strategies like top-k and top-p sampling are
    not used). A concrete example is provided in [Table 3](#A3.T3 "In Appendix C Discussion
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). The core issue arises because the gradient of the entropy
    regularizer can dominate for tokens with low probabilities. Specifically, the
    gradient of the forward KL is computed as $-p/f$. Consequently, for tokens with
    low probabilities in both $f$, the gradient given by the forward KL is much smaller
    than that given by the entropy regularizer, thus disproportionately increasing
    the tail probabilities. In contrast, the proposed reverse KL formulation with
    entropy regularization does not have this issue. This is because the optimization
    is defined over the generative distribution $f$ in our formulation, ensuring balanced
    gradients even for tokens with low probabilities (refer to [Equation 6](#A2.E6
    "In Proof of Proposition 1\. ‣ Appendix B Proof ‣ Entropic Distribution Matching
    in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity")).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式支持提议的原则2，但不支持原则1。我们发现这个公式在最大化熵时会导致尾部概率的不当增加，如[图 5](#A3.F5 "附录 C 讨论 ‣ 熵分布匹配在
    LLM 监督微调中的应用：减少过拟合和提高多样性")所示。在 LLMs 的背景下，这种增加通常会转化为词汇中的无意义标记，导致不理想的生成输出（如果不使用额外的策略如
    top-k 和 top-p 采样）。[表 3](#A3.T3 "附录 C 讨论 ‣ 熵分布匹配在 LLM 监督微调中的应用：减少过拟合和提高多样性")中提供了一个具体的例子。核心问题是由于熵正则化器的梯度对低概率标记可能占主导地位。具体而言，前向
    KL 的梯度计算为 $-p/f$。因此，对于在 $f$ 中概率低的标记，前向 KL 给出的梯度远小于熵正则化器给出的梯度，从而不成比例地增加了尾部概率。相比之下，提议的带熵正则化的反向
    KL 公式没有这个问题。这是因为在我们的公式中，优化是在生成分布 $f$ 上定义的，确保即使对于低概率标记也有平衡的梯度（参见[方程 6](#A2.E6 "在命题
    1 的证明中 ‣ 附录 B 证明 ‣ 熵分布匹配在 LLM 监督微调中的应用：减少过拟合和提高多样性")）。
- en: 'Table 3: Examples of generated sentences for models trained with the UltraFeedback
    dataset. The symbol “[…]” indicates good sentences that have been omitted to save
    space. The vanilla generation configuration temperature=1, top_k=None, and top_p=1.0
    is used. This example shows that CE with entropy regularization can generate nonsensical
    tokens (shown in red) due to the increase in tail probabilities.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用 UltraFeedback 数据集训练的模型生成的句子示例。符号“[…]”表示为了节省空间而省略的优良句子。使用了原始生成配置 temperature=1、top_k=None
    和 top_p=1.0。此示例表明，带有熵正则化的 CE 可能会由于尾部概率的增加而生成无意义的标记（以红色显示）。
- en: '| Prompt | Whats the largest star in our galaxy? |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Prompt | 我们银河系中最大的恒星是什么？ |'
- en: '| CE | Hello! Atlantis is a legendary city that was said to have existed in
    ancient Greece. According to the story, it was a highly advanced and prosperous
    city that was located on an island in the ocean. […] |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| CE | 你好！亚特兰蒂斯是一个传说中的城市，据说存在于古希腊。根据故事，它是一个高度发达和繁荣的城市，位于海洋中的一个岛屿上。[…] |'
- en: '| CE+Entropy | Hello! Atlantis Documentary is a 2019 American documentaryéhoflict
    film directed by Já oblík and produced by Werner Herzog. The film explores the
    history and legacy of Atlantis, $\Box{}$ an ancient Greek city-state that was
    said to have_calendar knowledge and advanced technology, through interviews with
    scholars and historians.ython |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 你好！《亚特兰蒂斯纪录片》是一部2019年美国纪录片，由Já oblík导演，Werner Herzog制作。该片通过对学者和历史学家的访谈，探讨了亚特兰蒂斯的历史和遗产，$\Box{}$
    这是一个古希腊城邦，据说拥有日历知识和先进技术。python |'
- en: '| GEM-LS | Hello there! As a helpful, respectful, and honest assistant, I’d
    be happy to help you explore the fascinating topic of Atlantis! Atlantis is an
    ancient Greek myth that tells the story of a legendary realm said to have existed
    in the Atlantic Ocean, west of the Pillars of Hercules. […] |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 你好！作为一个乐于助人、尊重他人和诚实的助手，我很高兴帮助你深入探讨迷人的亚特兰蒂斯话题！亚特兰蒂斯是一个古希腊神话，讲述了一个传说中的领域，据说存在于赫拉克勒斯之柱以西的大西洋中。[…]
    |'
- en: Appendix D Experiment Details
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实验细节
- en: All experiments are conducted using A800-80GB GPUs with the DeepSpeed distributed
    training framework, utilizing ZeRO-2 and gradient checkpointing without offloading.
    We use flash-attention-2 with deterministic backward for reproducibility. The
    experiments are based on the pretrained Llama-3-8B model, using Adam as the optimizer
    with a global batch size of 128\. Following [[66](#bib.bib66), [34](#bib.bib34),
    [13](#bib.bib13)], the learning rate is set to 2e-5, with a warm-up ratio of 0.03
    and cosine learning rate decay. Training is performed over 3 epochs. All supervised
    datasets are formatted into the chat format using the Llama-3-8B-Instruct’s tokenizer.
    When generation of responses is required for evaluation, we use the vLLM to accelerate
    inference.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均使用 A800-80GB GPU 和 DeepSpeed 分布式训练框架进行，利用 ZeRO-2 和梯度检查点，无需卸载。我们使用 flash-attention-2
    和确定性反向传播以确保可重复性。实验基于预训练的 Llama-3-8B 模型，使用 Adam 作为优化器，全球批量大小为 128。根据 [[66](#bib.bib66),
    [34](#bib.bib34), [13](#bib.bib13)] 的方法，学习率设置为 2e-5，暖启动比例为 0.03，并使用余弦学习率衰减。训练进行
    3 个周期。所有监督数据集使用 Llama-3-8B-Instruct 的标记器格式化为聊天格式。当需要生成响应进行评估时，我们使用 vLLM 来加速推理。
- en: D.1 UltraFeedback
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 UltraFeedback
- en: We use the dataset filtered by HuggingfaceH4 team, which is available at [https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).
    The dataset contains 61,135 training samples and 1,000 test samples. For training,
    we set the maximum sequence length to 2,048, dropping longer sequences and padding
    shorter ones. To achieve a global batch size of 128, we use a per-device batch
    size of 4, a gradient accumulation step of 4, and 4 GPUs. The training times takes
    about 24 GPU hours. For the CE method, we have tuned hyperparameters for weight
    decay and entropy regularization, selecting values from $\{0.1,0.01,0.001\}$ provided
    the best overall results.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了由 HuggingfaceH4 团队筛选的数据集，该数据集可在 [https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)
    上获取。该数据集包含 61,135 个训练样本和 1,000 个测试样本。训练时，我们将最大序列长度设置为 2,048，丢弃较长的序列并填充较短的序列。为了实现全球批量大小为
    128，我们使用每设备批量大小为 4，梯度累积步长为 4，以及 4 个 GPU。训练时间大约需要 24 GPU 小时。对于 CE 方法，我们已调整了权重衰减和熵正则化的超参数，选择了
    $\{0.1,0.01,0.001\}$ 中的值，这些值提供了最佳的整体结果。
- en: Evaluation metrics, including perplexity, and entropy, are based on these 1,000
    test samples. For entropy calculation, we compute the conditional entropy, whose
    expectation can be calculated exactly, and average over the sequence. For the
    instruction-following evaluation, we use the IFEval benchmark from [[69](#bib.bib69)].
    We apply greedy decoding with a maximum generation length of 1,024 tokens.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标包括困惑度和熵，基于这 1,000 个测试样本。对于熵计算，我们计算条件熵，其期望值可以精确计算，并对序列进行平均。对于指令遵循评估，我们使用来自
    [[69](#bib.bib69)] 的 IFEval 基准。我们应用贪婪解码，最大生成长度为 1,024 个标记。
- en: For the diversity evaluation in poem writing, we use prompts derived from the
    poetry dataset on the Huggingface website, which includes 573 poems on themes
    like love, nature, and mythology by poets such as William Shakespeare. We prompt
    the trained models with questions like, “Write a poem titled ‘[X]’ with no more
    than 200 words,” where [X] is a title from the dataset. For story writing, we
    create 500 prompts based on the ROC Story dataset (2017 winter) [[36](#bib.bib36)],
    asking models to “Write a story titled ‘[X]’ with no more than 200 words,” where
    [X] is a title from the dataset. The maximum number of generation tokens is set
    to 512\. The evaluation script follows the methodology from previous work by [[26](#bib.bib26)],
    using the script available at [https://github.com/facebookresearch/rlfh-gen-div](https://github.com/facebookresearch/rlfh-gen-div).
    For each question, 16 samples with the generation configuration temperature=1.0,
    top_k=50, top_p=0.9 is used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于诗歌创作的多样性评估，我们使用来自 Huggingface 网站上的诗歌数据集的提示，该数据集包括 573 首主题为爱情、大自然和神话的诗歌，由威廉·莎士比亚等诗人创作。我们用诸如“写一首题为‘[X]’的诗，字数不超过
    200”之类的问题提示训练好的模型，其中 [X] 是数据集中的标题。对于故事创作，我们根据 ROC Story 数据集（2017 冬季）[[36](#bib.bib36)]
    创建了 500 个提示，要求模型“写一个题为‘[X]’的故事，字数不超过 200”，其中 [X] 是数据集中的标题。生成标记的最大数量设置为 512。评估脚本遵循
    [[26](#bib.bib26)] 的方法，使用可在 [https://github.com/facebookresearch/rlfh-gen-div](https://github.com/facebookresearch/rlfh-gen-div)
    上获取的脚本。对于每个问题，使用生成配置 temperature=1.0，top_k=50，top_p=0.9 的 16 个样本。
- en: 'For the chat evaluation, we use the 805 test questions from the AlpacaEval
    dataset and employ the reward model FsfairX-LLaMA3-RM-v0.1. The maximum generation
    sequence length is set to 2048\. For each question, 32 samples are generated with
    the configuration temperature=0.6, top_k=50, top_p=0.9. To calculate the win rate,
    we use the Bradley-Terry model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天评估，我们使用AlpacaEval数据集中的805个测试问题，并采用奖励模型FsfairX-LLaMA3-RM-v0.1。最大生成序列长度设为2048。对于每个问题，生成了32个样本，配置为temperature=0.6,
    top_k=50, top_p=0.9。为了计算胜率，我们使用Bradley-Terry模型：
- en: '|  | $\displaystyle\mathbb{P}(y\succ y^{\prime}\mid x)=\frac{\exp(r(x,y))}{\exp(r(x,y))+\exp(r(x,y^{\prime}))}.$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{P}(y\succ y^{\prime}\mid x)=\frac{\exp(r(x,y))}{\exp(r(x,y))+\exp(r(x,y^{\prime}))}.$
    |  |'
- en: We use GPT-4 generated responses as a baseline for calculating the win rate,
    specifically the gpt4_1106_preview^(11)^(11)11[https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json](https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json)
    version.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GPT-4生成的响应作为计算胜率的基准，特别是gpt4_1106_preview^(11)^(11)11[https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json](https://github.com/tatsu-lab/alpaca_eval/blob/main/results/gpt4_1106_preview/model_outputs.json)版本。
- en: 'For the math reasoning task on GSM8K, we use the following prompt:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GSM8K的数学推理任务，我们使用以下提示：
- en: 'Your task is to answer the
    question below. Give step-by-step reasoning before you answer, and when you’re
    ready to answer, please use the format ”The answer is: …”. Question: {question}'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'Your task is to answer the
    question below. Give step-by-step reasoning before you answer, and when you’re
    ready to answer, please use the format ”The answer is: …”. Question: {question}'
- en: Answer extraction from the generated responses follows the approach from previous
    work [[66](#bib.bib66)], using the script available at [https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py](https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py).
    For each question, 32 responses are generated with the configuration temperature=0.6,
    top_k=50, top_p=0.9. The reported accuracy is based on 1,319 test questions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的响应中提取答案的方法遵循以前的工作[[66](#bib.bib66)]，使用可在[https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py](https://github.com/meta-math/MetaMath/blob/main/eval_gsm8k.py)找到的脚本。对于每个问题，生成了32个响应，配置为temperature=0.6,
    top_k=50, top_p=0.9。报告的准确率基于1,319个测试问题。
- en: 'For the code generation tasks on HumanEval and MBPP, there are 164 test questions
    for HumanEval and 378 test questions for MBPP. We use the prompt from [[62](#bib.bib62)]:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于HumanEval和MBPP的代码生成任务，HumanEval有164个测试问题，MBPP有378个测试问题。我们使用来自[[62](#bib.bib62)]的提示：
- en: You are an exceptionally intelligent
    coding assistant that consistently delivers accurate and reliable responses to
    user instructions. @@ Instruction {instruction}
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: You are an exceptionally intelligent
    coding assistant that consistently delivers accurate and reliable responses to
    user instructions. @@ Instruction {instruction}
- en: For each question, 200 responses are generated with the configuration temperature=0.6,
    top_k=50, top_p=0.9 to estimate the pass rate. The evaluation scripts are from
    [https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py](https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，生成了200个响应，配置为temperature=0.6, top_k=50, top_p=0.9，以估算通过率。评估脚本来自[https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py](https://github.com/ise-uiuc/magicoder/blob/main/experiments/text2code.py)。
- en: D.2 MagiCoder
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 MagiCoder
- en: We use the MagiCoder-OSS-Instruct dataset [[62](#bib.bib62)], which contains
    74,197 training samples and 1,000 test samples (randomly selected from the original
    training set). The maximum sequence length for training is 1,024\. To achieve
    a global batch size of 128, we use a per-device batch size of 8, gradient accumulation
    steps of 2, and 8 GPUs. The training takes approximately 24 GPU hours. The evaluation
    method is the same as previously described.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用MagiCoder-OSS-Instruct数据集[[62](#bib.bib62)]，该数据集包含74,197个训练样本和1,000个测试样本（从原始训练集随机选择）。训练的最大序列长度为1,024。为了实现全球批量大小128，我们使用每设备批量大小8，梯度累积步数为2，8个GPU。训练大约需要24个GPU小时。评估方法与之前描述的相同。
- en: D.3 MetaMathQA
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 MetaMathQA
- en: We use the MetaMathQA dataset [[66](#bib.bib66)]. To make the code generation
    task manageable, we select a subset of 79,000 samples for training and 1,000 samples
    for evaluation. The maximum sequence length for training is set to 1,024\. To
    achieve a global batch size of 128, we use a per-device batch size of 8, gradient
    accumulation steps of 2, and 8 GPUs. Training takes approximately 24 GPU hours.
    The evaluation method is as previously described. For the MATH task, the prompt
    is the same as for the GSM8K task.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用MetaMathQA数据集[[66](#bib.bib66)]。为了使代码生成任务可管理，我们选择了79,000个样本用于训练，1,000个样本用于评估。训练的最大序列长度设为1,024。为了实现全球批量大小128，我们使用每设备批量大小8，梯度累积步数为2，8个GPU。训练大约需要24个GPU小时。评估方法与之前描述的相同。对于MATH任务，提示与GSM8K任务相同。
- en: Appendix E Additional Results
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 额外结果
- en: E.1 General Purpose Fine-tuning
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 通用目的微调
- en: Next-Token Prediction Distributions.
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下一标记预测分布。
- en: 'We demonstrate the distribution collapse issue associated with the CE method
    using three simple prompts for the trained LLMs: 1) “Complete this sequence with
    a single letter: A, B, C, ___”; 2) “Give me a single-digit number”; and 3) “Tell
    me a type of fruit”. All prompts are designed to have answers with 1 token for
    visualization.^(12)^(12)12For the first prompt, while “D” is the most likely answer,
    “A” could also be a valid response due to the pattern A, B, C, A, B, C, $\ldots$.
    The distributions are visualized in [Figure 6](#A5.F6 "In Next-Token Prediction
    Distributions. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional Results
    ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting
    and Better Diversity"). We see GEM-trained models produce flatter distributions,
    indicating support for multiple possible answers.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用三个简单的提示展示了与 CE 方法相关的分布崩溃问题，针对训练过的 LLMs：1) “用一个字母完成这个序列：A, B, C, ___”；2)
    “给我一个个位数”；3) “告诉我一种水果”。所有提示都设计为用 1 个 token 给出答案以便于可视化。^(12)^(12)12对于第一个提示，尽管“D”是最可能的答案，但由于模式是
    A, B, C, A, B, C, $\ldots$，“A”也可能是有效的回答。分布在[图 6](#A5.F6 "在下一个词预测分布中。 ‣ E.1 一般用途微调
    ‣ 附录 E 其他结果 ‣ 在 LLM 的监督微调中的熵分布匹配：较少的过拟合和更好的多样性")中可视化。我们看到 GEM 训练的模型产生了较平坦的分布，表示支持多种可能的答案。
- en: '![Refer to caption](img/0fc8b18e3a29d0aa15dfbc3081672c00.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0fc8b18e3a29d0aa15dfbc3081672c00.png)'
- en: '(a) Prompt: Complete this sequence with a single letter: A, B, C, ___'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 提示：用一个字母完成这个序列：A, B, C, ___
- en: '![Refer to caption](img/1edc0cbd6fd4a807d443e6a974b76673.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1edc0cbd6fd4a807d443e6a974b76673.png)'
- en: '(b) Prompt: Give me a single-digit number.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 提示：给我一个个位数。
- en: '![Refer to caption](img/9875282de62b40b90f2c81c7ac081c2c.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9875282de62b40b90f2c81c7ac081c2c.png)'
- en: '(c) Prompt: Tell me a type of fruit.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 提示：告诉我一种水果。
- en: 'Figure 6: Distributions of next-token probabilities for trained models with
    the UltraFeedback dataset, presented from left to right: CE, CE+WD, CE+Entropy,
    and GEM-Linear, and GEM-LS. Only top-10 probabilities are visualized for clarity.
    These examples highlight the issue of limited generation diversity in CE.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用 UltraFeedback 数据集训练的模型的下一个词概率分布，从左到右呈现：CE、CE+WD、CE+Entropy、GEM-Linear
    和 GEM-LS。为清晰起见，仅可视化了前 10 个概率。这些示例突显了 CE 方法生成多样性有限的问题。
- en: Perplexity and Entropy.
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 困惑度和熵。
- en: 'For trained models, we also examine two statistics: perplexity, and entropy
    of the output distribution on 1,000 evaluation samples from the Ultrafeedback
    dataset. Results are reported in [Figure 7](#A5.F7 "In Perplexity and Entropy.
    ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional Results ‣ Entropic Distribution
    Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity").
    Using CE as a baseline, we make several observations. First, weight decay does
    not significantly change the statistics. Second, directly incorporating entropy
    regularization increases both perplexity and entropy considerably. Notably, this
    increase is mainly due to relatively large tail probabilities. Third, GEM generally
    reduces perplexity while increasing entropy. As a side note, the reduced evaluation
    perplexity does not directly translate to better performance in the area of LLMs
    (see e.g., [[68](#bib.bib68)]), but it does imply that GEM-trained models tend
    to favor grounded answers with high probability, thus enhancing diversity.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练模型，我们还检查了两个统计数据：困惑度和 Ultrafeedback 数据集上 1,000 个评估样本的输出分布的熵。结果报告在[图 7](#A5.F7
    "在困惑度和熵中。 ‣ E.1 一般用途微调 ‣ 附录 E 其他结果 ‣ 在 LLM 的监督微调中的熵分布匹配：较少的过拟合和更好的多样性")中。以 CE
    作为基线，我们做出了一些观察。首先，权重衰减不会显著改变统计数据。其次，直接引入熵正则化会显著增加困惑度和熵。值得注意的是，这种增加主要是由于相对较大的尾部概率。第三，GEM
    通常降低困惑度同时增加熵。附带说明一下，降低的评估困惑度并不直接转化为 LLM 领域的更好表现（见例如 [[68](#bib.bib68)]），但这确实意味着
    GEM 训练的模型倾向于支持高概率的有根据的答案，从而增强了多样性。
- en: '![Refer to caption](img/52853a04b05ddea849afbb1f938cfb87.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/52853a04b05ddea849afbb1f938cfb87.png)'
- en: 'Figure 7: Evaluation perplexity and entropy. Models are trained with the UltraFeedback
    dataset.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：评估困惑度和熵。模型使用 UltraFeedback 数据集进行训练。
- en: Chatting, Math Reasoning, and Code Generation.
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天、数学推理和代码生成。
- en: 'We provide the detailed results in [Tables 4](#A5.T4 "In Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity"), [5](#A5.T5 "Table 5 ‣ Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity") and [6](#A5.T6 "Table 6 ‣ Chatting, Math Reasoning,
    and Code Generation. ‣ E.1 General Purpose Fine-tuning ‣ Appendix E Additional
    Results ‣ Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less
    Overfitting and Better Diversity"). We observe that even with less generation
    samples, GEM also shows better performance.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 4](#A5.T4 "在聊天、数学推理和代码生成中。 ‣ E.1 一般用途的微调 ‣ 附录 E 额外结果 ‣ 监督微调中的熵分布匹配：减少过拟合和更好的多样性")、[表
    5](#A5.T5 "表 5 ‣ 聊天、数学推理和代码生成。 ‣ E.1 一般用途的微调 ‣ 附录 E 额外结果 ‣ 监督微调中的熵分布匹配：减少过拟合和更好的多样性")
    和[表 6](#A5.T6 "表 6 ‣ 聊天、数学推理和代码生成。 ‣ E.1 一般用途的微调 ‣ 附录 E 额外结果 ‣ 监督微调中的熵分布匹配：减少过拟合和更好的多样性")中提供了详细结果。我们观察到，即使生成样本较少，GEM
    的表现也更佳。
- en: 'Table 4: Evaluation of reward and win rate on AlpacaEval dataset. Models are
    trained with the UltraFeedback dataset.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: AlpacaEval 数据集上的奖励和胜率评估。模型使用 UltraFeedback 数据集进行训练。'
- en: '| Method | Reward |  | Win Rate |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 奖励 |  | 胜率 |'
- en: '| BON@4 | BON@8 | BON@16 | BON@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| BON@4 | BON@8 | BON@16 | BON@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
- en: '| CE | 1.06 | 1.43 | 1.86 | 2.39 |  | 26.59 | 31.35 | 37.43 | 46.61 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| CE | 1.06 | 1.43 | 1.86 | 2.39 |  | 26.59 | 31.35 | 37.43 | 46.61 |'
- en: '| CE+WD | 1.09 | 1.47 | 1.85 | 2.41 |  | 27.17 | 32.00 | 37.59 | 46.98 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 1.09 | 1.47 | 1.85 | 2.41 |  | 27.17 | 32.00 | 37.59 | 46.98 |'
- en: '| CE+Entropy | 1.11 | 1.48 | 1.89 | 2.46 |  | 26.86 | 31.83 | 37.84 | 47.69
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 1.11 | 1.48 | 1.89 | 2.46 |  | 26.86 | 31.83 | 37.84 | 47.69
    |'
- en: '| GEM-Linear | 1.12 | 1.52 | 1.94 | 2.51 |  | 27.27 | 32.36 | 38.76 | 48.50
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 1.12 | 1.52 | 1.94 | 2.51 |  | 27.27 | 32.36 | 38.76 | 48.50
    |'
- en: '| GEM-LS | 1.11 | 1.52 | 1.96 | 2.56 |  | 26.98 | 32.53 | 39.18 | 49.46 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 1.11 | 1.52 | 1.96 | 2.56 |  | 26.98 | 32.53 | 39.18 | 49.46 |'
- en: 'Table 5: Evaluation of accuracy on the math reasoning task GSM8K. Models are
    trained with the UltraFeedback dataset.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 数学推理任务 GSM8K 的准确性评估。模型使用 UltraFeedback 数据集进行训练。'
- en: '| Method | GSM8K |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GSM8K |  |'
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
- en: '| CE | 51.63 | 55.57 | 58.61 | 62.17 |  | 65.28 | 74.68 | 82.11 | 90.22 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| CE | 51.63 | 55.57 | 58.61 | 62.17 |  | 65.28 | 74.68 | 82.11 | 90.22 |'
- en: '| CE+WD | 54.51 | 58.76 | 62.47 | 65.66 |  | 69.90 | 77.48 | 84.46 | 90.45
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 54.51 | 58.76 | 62.47 | 65.66 |  | 69.90 | 77.48 | 84.46 | 90.45
    |'
- en: '| CE+Entropy | 53.75 | 56.63 | 60.58 | 64.44 |  | 67.32 | 76.57 | 83.93 | 91.21
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 53.75 | 56.63 | 60.58 | 64.44 |  | 67.32 | 76.57 | 83.93 | 91.21
    |'
- en: '| GEM-Linear | 53.68 | 58.07 | 62.77 | 65.58 |  | 69.83 | 79.30 | 86.50 | 91.96
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 53.68 | 58.07 | 62.77 | 65.58 |  | 69.83 | 79.30 | 86.50 | 91.96
    |'
- en: '| GEM-LS | 55.95 | 60.42 | 64.82 | 67.02 |  | 70.05 | 79.68 | 86.96 | 92.72
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 55.95 | 60.42 | 64.82 | 67.02 |  | 70.05 | 79.68 | 86.96 | 92.72
    |'
- en: 'Table 6: Performance of pass rate on the code generation tasks HumanEval and
    MBPP. Models are trained with the UltraFeedback dataset.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 代码生成任务 HumanEval 和 MBPP 的通过率表现。模型使用 UltraFeedback 数据集进行训练。'
- en: '| Method | HumanEval |  | MBPP |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | HumanEval |  | MBPP |'
- en: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
- en: '| CE | 58.06 | 62.51 | 67.50 | 70.88 |  | 62.71 | 65.73 | 69.13 | 71.18 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| CE | 58.06 | 62.51 | 67.50 | 70.88 |  | 62.71 | 65.73 | 69.13 | 71.18 |'
- en: '| CE+WD | 56.18 | 61.53 | 67.85 | 71.91 |  | 63.13 | 66.35 | 69.40 | 71.35
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 56.18 | 61.53 | 67.85 | 71.91 |  | 63.13 | 66.35 | 69.40 | 71.35
    |'
- en: '| CE+Entropy | 58.85 | 64.02 | 70.29 | 74.44 |  | 65.50 | 68.75 | 71.77 | 73.48
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 58.85 | 64.02 | 70.29 | 74.44 |  | 65.50 | 68.75 | 71.77 | 73.48
    |'
- en: '| GEM-Linear | 60.34 | 66.12 | 73.12 | 77.97 |  | 64.54 | 68.57 | 72.30 | 74.33
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 60.34 | 66.12 | 73.12 | 77.97 |  | 64.54 | 68.57 | 72.30 | 74.33
    |'
- en: '| GEM-LS | 60.94 | 66.95 | 73.83 | 78.47 |  | 67.28 | 71.50 | 75.50 | 77.64
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 60.94 | 66.95 | 73.83 | 78.47 |  | 67.28 | 71.50 | 75.50 | 77.64
    |'
- en: E.2 Domain-specific Fine-tuning
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 领域特定的微调
- en: 'We provide the detailed results in [Tables 7](#A5.T7 "In E.2 Domain-specific
    Fine-tuning ‣ Appendix E Additional Results ‣ Entropic Distribution Matching in
    Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity"), [8](#A5.T8
    "Table 8 ‣ E.2 Domain-specific Fine-tuning ‣ Appendix E Additional Results ‣ Entropic
    Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and
    Better Diversity") and [9](#A5.T9 "Table 9 ‣ E.2 Domain-specific Fine-tuning ‣
    Appendix E Additional Results ‣ Entropic Distribution Matching in Supervised Fine-tuning
    of LLMs: Less Overfitting and Better Diversity"). The results indicate that GEM
    outperforms CE even with fewer generated samples.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 7](#A5.T7 "在 E.2 领域特定微调 ‣ 附录 E 附加结果 ‣ 监督微调中的熵分布匹配：较少过拟合和更好的多样性")、[表 8](#A5.T8
    "表 8 ‣ E.2 领域特定微调 ‣ 附录 E 附加结果 ‣ 监督微调中的熵分布匹配：较少过拟合和更好的多样性") 和 [表 9](#A5.T9 "表 9
    ‣ E.2 领域特定微调 ‣ 附录 E 附加结果 ‣ 监督微调中的熵分布匹配：较少过拟合和更好的多样性") 中提供了详细结果。这些结果表明，即使生成的样本较少，GEM
    也优于 CE。
- en: 'Table 7: Evaluation of accuracy on the math reasoning task GSM8K. Models are
    trained with the MetaMathQA dataset.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 数学推理任务 GSM8K 的准确性评估。模型使用 MetaMathQA 数据集进行训练。'
- en: '| Method | GSM8K |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GSM8K |  |'
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
- en: '| CE | 73.46 | 73.77 | 75.13 | 76.57 |  | 76.50 | 80.74 | 85.14 | 90.67 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| CE | 73.46 | 73.77 | 75.13 | 76.57 |  | 76.50 | 80.74 | 85.14 | 90.67 |'
- en: '| CE+WD | 73.84 | 75.06 | 76.50 | 78.24 |  | 77.94 | 81.05 | 86.05 | 90.67
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 73.84 | 75.06 | 76.50 | 78.24 |  | 77.94 | 81.05 | 86.05 | 90.67
    |'
- en: '| CE+Entropy | 75.06 | 76.04 | 77.71 | 79.68 |  | 79.61 | 83.70 | 88.70 | 92.95
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 75.06 | 76.04 | 77.71 | 79.68 |  | 79.61 | 83.70 | 88.70 | 92.95
    |'
- en: '| GEM-Linear | 74.83 | 75.82 | 78.09 | 78.77 |  | 81.43 | 85.60 | 89.69 | 93.56
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 74.83 | 75.82 | 78.09 | 78.77 |  | 81.43 | 85.60 | 89.69 | 93.56
    |'
- en: '| GEM-LS | 75.21 | 76.35 | 77.33 | 79.53 |  | 80.82 | 85.06 | 89.31 | 93.33
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 75.21 | 76.35 | 77.33 | 79.53 |  | 80.82 | 85.06 | 89.31 | 93.33
    |'
- en: 'Table 8: Evaluation of accuracy on the math reasoning task MATH. Models are
    trained with the MetaMathQA dataset.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 数学推理任务 MATH 的准确性评估。模型使用 MetaMathQA 数据集进行训练。'
- en: '| Method | MATH |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MATH |  |'
- en: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| MV@4 | MV@8 | MV@16 | MV@32 |  | BON@4 | BON@8 | BON@16 | BON@32 |'
- en: '| CE | 26.40 | 27.04 | 28.30 | 29.34 |  | 33.20 | 39.98 | 48.20 | 58.46 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| CE | 26.40 | 27.04 | 28.30 | 29.34 |  | 33.20 | 39.98 | 48.20 | 58.46 |'
- en: '| CE+WD | 26.20 | 27.02 | 28.38 | 29.56 |  | 33.22 | 39.32 | 47.54 | 57.10
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 26.20 | 27.02 | 28.38 | 29.56 |  | 33.22 | 39.32 | 47.54 | 57.10
    |'
- en: '| CE+Entropy | 28.06 | 29.26 | 30.34 | 31.20 |  | 35.58 | 41.84 | 50.66 | 59.64
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 28.06 | 29.26 | 30.34 | 31.20 |  | 35.58 | 41.84 | 50.66 | 59.64
    |'
- en: '| GEM-Linear | 27.62 | 29.30 | 30.64 | 31.48 |  | 36.82 | 43.74 | 52.04 | 60.30
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 27.62 | 29.30 | 30.64 | 31.48 |  | 36.82 | 43.74 | 52.04 | 60.30
    |'
- en: '| GEM-LS | 27.46 | 28.88 | 29.92 | 31.00 |  | 36.00 | 42.98 | 50.96 | 60.12
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 27.46 | 28.88 | 29.92 | 31.00 |  | 36.00 | 42.98 | 50.96 | 60.12
    |'
- en: 'Table 9: Performance of pass rate on the code generation tasks HumanEval and
    MBPP. Models are trained with the MagiCoder-OSS-Instruct dataset.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 在代码生成任务 HumanEval 和 MBPP 上的通过率表现。模型使用 MagiCoder-OSS-Instruct 数据集进行训练。'
- en: '| Method | HumanEval |  | MBPP |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | HumanEval |  | MBPP |'
- en: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Pass@10 | Pass@20 | Pass@50 | Pass@100 |  | Pass@10 | Pass@20 | Pass@50 |
    Pass@100 |'
- en: '| CE | 58.71 | 61.50 | 64.18 | 65.86 |  | 66.54 | 68.68 | 70.76 | 71.95 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| CE | 58.71 | 61.50 | 64.18 | 65.86 |  | 66.54 | 68.68 | 70.76 | 71.95 |'
- en: '| CE+WD | 58.33 | 61.06 | 63.77 | 65.89 |  | 65.96 | 68.38 | 70.67 | 71.89
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| CE+WD | 58.33 | 61.06 | 63.77 | 65.89 |  | 65.96 | 68.38 | 70.67 | 71.89
    |'
- en: '| CE+Entropy | 58.66 | 62.66 | 66.79 | 69.17 |  | 69.47 | 71.76 | 73.79 | 75.02
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| CE+Entropy | 58.66 | 62.66 | 66.79 | 69.17 |  | 69.47 | 71.76 | 73.79 | 75.02
    |'
- en: '| GEM-Linear | 58.69 | 62.39 | 67.16 | 70.64 |  | 72.00 | 74.54 | 76.74 | 78.08
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| GEM-Linear | 58.69 | 62.39 | 67.16 | 70.64 |  | 72.00 | 74.54 | 76.74 | 78.08
    |'
- en: '| GEM-LS | 65.15 | 68.73 | 72.64 | 75.58 |  | 73.30 | 75.90 | 78.42 | 79.97
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| GEM-LS | 65.15 | 68.73 | 72.64 | 75.58 |  | 73.30 | 75.90 | 78.42 | 79.97
    |'
