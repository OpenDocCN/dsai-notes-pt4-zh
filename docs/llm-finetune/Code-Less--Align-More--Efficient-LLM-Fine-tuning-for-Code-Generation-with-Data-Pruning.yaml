- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:35:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**代码更少，对齐更多**：通过数据修剪提高代码生成的高效LLM微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05040](https://ar5iv.labs.arxiv.org/html/2407.05040)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05040](https://ar5iv.labs.arxiv.org/html/2407.05040)
- en: Yun-Da Tsai
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yun-Da Tsai
- en: NVIDIA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: yundat@nvidia.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: yundat@nvidia.com
- en: '&Mingjie Liu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Mingjie Liu'
- en: NVIDIA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: mingjiel@nvidia.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: mingjiel@nvidia.com
- en: '&Haoxing Ren'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Haoxing Ren'
- en: NVIDIA
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: haoxingr@nvidia.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: haoxingr@nvidia.com
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent work targeting large language models (LLMs) for code generation demonstrated
    that increasing the amount of training data through synthetic code generation
    often leads to exceptional performance. In this paper we explore data pruning
    methods aimed at enhancing the efficiency of model training specifically for code
    LLMs. We present techniques that integrate various clustering and pruning metrics
    to selectively reduce training data without compromising the accuracy and functionality
    of the generated code. We observe significant redundancies in synthetic training
    data generation, where our experiments demonstrate that benchmark performance
    can be largely preserved by training on only 10% of the data. Moreover, we observe
    consistent improvements in benchmark results through moderate pruning of the training
    data. Our experiments show that these pruning strategies not only reduce the computational
    resources needed but also enhance the overall quality code generation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 针对代码生成的大型语言模型（LLMs）的近期研究表明，通过合成代码生成增加训练数据量通常会导致卓越的性能。在本文中，我们探讨了旨在提高模型训练效率的数据修剪方法，特别是针对代码
    LLM。我们提出了整合各种聚类和修剪指标的技术，以选择性地减少训练数据，同时不影响生成代码的准确性和功能。我们观察到合成训练数据生成中的显著冗余，我们的实验表明，仅使用
    10% 的数据进行训练可以在很大程度上保持基准性能。此外，我们观察到通过适度修剪训练数据，基准结果得到了一致的改善。我们的实验表明，这些修剪策略不仅减少了所需的计算资源，还提高了代码生成的整体质量。
- en: \SetTblrInner
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \SetTblrInner
- en: '[booktabs]abovesep=0pt, belowsep=0pt, rowsep=0.5pt \SetTblrInner[booktabs]cells
    = cmd= \NewTableCommand\seprule \NewTableCommand\uniquerule'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[booktabs]abovesep=0pt, belowsep=0pt, rowsep=0.5pt \SetTblrInner[booktabs]cells
    = cmd= \NewTableCommand\seprule \NewTableCommand\uniquerule'
- en: 'Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码更少，对齐更多**：通过数据修剪提高代码生成的高效LLM微调'
- en: Yun-Da Tsai NVIDIA yundat@nvidia.com                        Mingjie Liu NVIDIA
    mingjiel@nvidia.com                        Haoxing Ren NVIDIA haoxingr@nvidia.com
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Yun-Da Tsai NVIDIA yundat@nvidia.com                        Mingjie Liu NVIDIA
    mingjiel@nvidia.com                        Haoxing Ren NVIDIA haoxingr@nvidia.com
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The performance of large language models (LLMs) is heavily dependent on the
    size and quality of their training datasets, as highlighted by recent studies
    on scaling laws Achiam et al. ([2023](#bib.bib1)); Zhang et al. ([2024](#bib.bib2)).
    State-of-the-art code LLMs, such as CodeAlpaca Chaudhary ([2023](#bib.bib3)),
    WizardCoder Luo et al. ([2024](#bib.bib4)), and MagicCoder Wei et al. ([2023](#bib.bib5)),
    have achieved remarkable performance by significantly expanding their supervised
    fine-tuning datasets through synthetic code generation. Various synthetic code
    generation approaches have been developed, including the Self-Instruct technique Wang
    et al. ([2022](#bib.bib6)), Evol-Instruct Xu et al. ([2023a](#bib.bib7)), and
    OSS-Instruct Wei et al. ([2023](#bib.bib5)). However, such scaling approaches
    not only increase the training cost but also demands substantial computational
    resources, making it expensive and less accessible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的性能在很大程度上依赖于其训练数据集的规模和质量，这一点在最近关于扩展法则的研究中得到了强调，如 Achiam 等人 ([2023](#bib.bib1))
    和 Zhang 等人 ([2024](#bib.bib2))。最先进的代码 LLM，如 CodeAlpaca Chaudhary ([2023](#bib.bib3))、WizardCoder
    Luo 等人 ([2024](#bib.bib4)) 和 MagicCoder Wei 等人 ([2023](#bib.bib5))，通过显著扩展其监督微调数据集，通过合成代码生成实现了显著的性能提升。已经开发了多种合成代码生成方法，包括
    Self-Instruct 技术 Wang 等人 ([2022](#bib.bib6))、Evol-Instruct Xu 等人 ([2023a](#bib.bib7))
    和 OSS-Instruct Wei 等人 ([2023](#bib.bib5))。然而，这些扩展方法不仅增加了训练成本，还需要大量计算资源，导致其变得昂贵且不易获得。
- en: Achieving optimal performance in fine-tuned models for downstream tasks often
    relies on large, high-quality datasets. Recently, there has been a growing interest
    in more efficient fine-tuning methods for large language models (LLMs). One recent
    work introduces the Superficial Alignment Hypothesis Zhou et al. ([2023](#bib.bib9)),
    which suggests that most knowledge in LLMs is acquired during pretraining, and
    only minimal instruction tuning data is required to align models with human preferences.
    Promising strategies to reduce computational demands include parameter-efficient
    fine-tuning (PEFT) methods, which reduce the number of parameters needed for training Fu
    et al. ([2023](#bib.bib10)); Hu et al. ([2021](#bib.bib11)). Another research
    direction uses active learning to iteratively select data samples during training,
    thereby enhancing model learning Su et al. ([2022](#bib.bib12)); Diao et al. ([2023](#bib.bib13)).
    These methods primarily aim to improve model accuracy through iterative processes,
    requiring multiple rounds of training and data selection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下游任务中实现最佳性能通常依赖于大规模、高质量的数据集。最近，针对大语言模型（LLMs）的更高效的微调方法受到了越来越多的关注。一项最新研究提出了**表面对齐假设** Zhou
    et al. ([2023](#bib.bib9))，该假设认为LLMs中的大部分知识是在预训练期间获得的，只需最少的指令调优数据即可将模型与人类偏好对齐。减少计算需求的有前途的策略包括**参数高效微调（PEFT）**方法，它减少了训练所需的参数数量 Fu
    et al. ([2023](#bib.bib10)); Hu et al. ([2021](#bib.bib11))。另一个研究方向使用主动学习在训练过程中迭代地选择数据样本，从而增强模型学习 Su
    et al. ([2022](#bib.bib12)); Diao et al. ([2023](#bib.bib13))。这些方法主要通过迭代过程提高模型准确性，要求多轮训练和数据选择。
- en: Data selection and pruning methods have also been well-explored in literature,
    with evidence suggesting that careful pruning can sometimes even surpass the performance
    of using the full dataset Penedo et al. ([2024](#bib.bib15)); Wang et al. ([2023](#bib.bib16)).
    Moreover, many of these methods are computationally intensive such as supervised
    metrics that involves multiple times of model training to keep track of loss and
    gradients Xia et al. ([2024](#bib.bib18)); Pruthi et al. ([2020](#bib.bib19))
    or heavy sampling method with Monte Carlo Schoch et al. ([2023](#bib.bib20)),
    limiting their scalability. Practical pruning methods that aims for large-scale
    data have been investigated in the contexts of LLM pretraining Das and Khetan
    ([2023](#bib.bib21)); Penedo et al. ([2024](#bib.bib15)) and fine-tuning Chen
    et al. ([2024](#bib.bib22)); Schoch et al. ([2023](#bib.bib20)) datasets, image
    datasets Moser et al. ([2024](#bib.bib23)); Meding et al. ([2021](#bib.bib24)),
    and vision-text training datasets Wang et al. ([2023](#bib.bib16)), and demonstrate
    success by applying clustering and by choosing proper indicator functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据选择和修剪方法在文献中已经得到了很好的探索，有证据表明，仔细的修剪有时甚至可以超越使用完整数据集的表现 Penedo et al. ([2024](#bib.bib15));
    Wang et al. ([2023](#bib.bib16))。此外，许多这些方法计算密集，例如涉及多次模型训练以跟踪损失和梯度的监督指标 Xia et
    al. ([2024](#bib.bib18)); Pruthi et al. ([2020](#bib.bib19))，或带有蒙特卡洛的重采样方法 Schoch
    et al. ([2023](#bib.bib20))，这限制了它们的可扩展性。针对大规模数据的实用修剪方法已经在LLM预训练 Das and Khetan
    ([2023](#bib.bib21)); Penedo et al. ([2024](#bib.bib15)) 和微调 Chen et al. ([2024](#bib.bib22));
    Schoch et al. ([2023](#bib.bib20)) 数据集、图像数据集 Moser et al. ([2024](#bib.bib23));
    Meding et al. ([2021](#bib.bib24)) 和视觉-文本训练数据集 Wang et al. ([2023](#bib.bib16))
    中得到了研究，并通过应用聚类和选择适当的指标函数展示了成功。
- en: Despite these advances, there remains a gap in efficient pruning strategies
    specifically tailored for coding datasets. Most large-scale code datasets are
    synthetically generated, resulting in many data samples with similar lexical appearances
    due to consistent formatting and style. Large-scale synthetic datasets commonly
    used for training code LLMs often suffer from significant redundancy and noise Wang
    et al. ([2023](#bib.bib16)). This redundancy arises from the impracticality of
    verifying the functional correctness of each program, leading to a substantial
    portion of instruction-code pairs being noisy. Therefore, enhancing data efficiency
    through careful selection and pruning of data samples is crucial for improving
    model performance without relying on excessively large datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，但针对编码数据集的高效修剪策略仍存在差距。大多数大规模代码数据集是合成生成的，由于格式和风格的一致性，许多数据样本具有相似的词法外观。用于训练代码LLMs的大规模合成数据集通常会遭受显著的冗余和噪声 Wang
    et al. ([2023](#bib.bib16))。这种冗余源于验证每个程序功能正确性的实际困难，导致大量指令-代码对是噪声。因此，通过仔细选择和修剪数据样本来提高数据效率对于在不依赖过大数据集的情况下改善模型性能至关重要。
- en: 'In this work, we present a scalable and effective data pruning method to enhance
    code generation in large language models. Our approach clusters data samples based
    on problem instructions and their code solutions, applying dimensionality reduction
    to reduce computational load. We then select a representative subset from each
    cluster using various pruning metrics. Experiments on large-scale datasets and
    evaluations on downstream coding tasks show that our method maintains or even
    improves model performance while significantly reducing training data. Our contributions
    and key findings are summarized as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种可扩展且有效的数据剪枝方法，以增强大型语言模型中的代码生成。我们的方法基于问题指令及其代码解决方案对数据样本进行聚类，应用降维技术以减少计算负荷。然后，我们使用各种剪枝指标从每个聚类中选择一个代表性子集。在大规模数据集上的实验和下游编码任务的评估表明，我们的方法在显著减少训练数据的同时，保持或甚至提高了模型性能。我们的贡献和主要发现总结如下：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We are the first to study data pruning for large-scale synthetic code fine-tuning.
    We create an efficient and scalable pruning strategy based on unsupervised learning
    methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们是首个研究大规模合成代码微调的数据剪枝方法的团队。我们基于无监督学习方法创建了一种高效且可扩展的剪枝策略。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We find large redundancies in synthetic generated code datasets, as training
    on just 10% retains most benchmark performance, with slight degradation of 3.9%
    on HumanEval and 1.5% on MBPP compared with using all data.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现合成生成的代码数据集中存在大量冗余，因为只用 10% 的数据训练即可保留大部分基准性能，相比使用所有数据，HumanEval 的性能略微下降 3.9%，MBPP
    的性能下降 1.5%。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We observe consistent improvement by moderately pruning the dataset, leading
    to improvements of up to 2.7% on HumanEval and 3.5% on MBPP compared with using
    all data.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们观察到通过适度剪枝数据集可以持续改善，使用所有数据相比，HumanEval 的性能提高了最高 2.7%，MBPP 的性能提高了 3.5%。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform detailed ablation studies, where results demonstrate the clustering
    algorithm to be critical, while pruning metrics to be less important.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了详细的消融研究，结果表明，聚类算法至关重要，而剪枝指标则相对不那么重要。
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'In this section, we review the advancements of large language models (LLMs)
    for code generation in Section [2.1](#S2.SS1 "2.1 Large Language Models for Code
    Generation ‣ 2 Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning") and review prior work on instructional
    finetuning in Section [2.2](#S2.SS2 "2.2 Instructional Fine-tuning ‣ 2 Related
    Work ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with
    Data Pruning"). Finally, we discuss earlier research on data selection and pruning
    methods in Section [2.3](#S2.SS3 "2.3 Data Pruning for Efficient Training ‣ 2
    Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们回顾了大语言模型（LLMs）在代码生成方面的进展，见第 [2.1](#S2.SS1 "2.1 Large Language Models
    for Code Generation ‣ 2 Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning") 节，并回顾了第 [2.2](#S2.SS2 "2.2 Instructional
    Fine-tuning ‣ 2 Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning") 节中的指令微调相关工作。最后，我们在第 [2.3](#S2.SS3 "2.3
    Data Pruning for Efficient Training ‣ 2 Related Work ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning") 节讨论了数据选择和剪枝方法的早期研究。'
- en: 2.1 Large Language Models for Code Generation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型用于代码生成
- en: Great advancements have been achieved in improving Large Language Models (LLMs)
    for code generation. Codealpaca Chaudhary ([2023](#bib.bib3)) extends the capabilities
    of the LLaMA model Touvron et al. ([2023a](#bib.bib26)) by incorporating 20,000
    instruction-following data points generated through the Self-Instruct technique Wang
    et al. ([2022](#bib.bib6)), which aligns language models with self-generated instructions.
    CodeLlama Roziere et al. ([2023](#bib.bib27)) further enhances this methodology
    by fine-tuning from LLaMA2 Touvron et al. ([2023b](#bib.bib28)), utilizing 14,000
    instruction-following data points also generated via the Self-Instruct technique.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在提高大型语言模型（LLMs）用于代码生成方面取得了重大进展。Codealpaca Chaudhary ([2023](#bib.bib3)) 通过结合
    Self-Instruct 技术生成的 20,000 个指令跟随数据点扩展了 LLaMA 模型 Touvron 等人 ([2023a](#bib.bib26))
    的能力，该技术将语言模型与自生成指令对齐。CodeLlama Roziere 等人 ([2023](#bib.bib27)) 通过从 LLaMA2 Touvron
    等人 ([2023b](#bib.bib28)) 微调，利用 Self-Instruct 技术生成的 14,000 个指令跟随数据点，进一步增强了这种方法。
- en: Wizardcoder Luo et al. ([2024](#bib.bib4)) utilizes the Evol-Instruct method Xu
    et al. ([2023a](#bib.bib7)) to evolve the Codealpaca dataset further. This technique
    iteratively evolves instruction-following data in both depth and breadth dimensions.
    On the other hand, Magicoder Wei et al. ([2023](#bib.bib5)) employs the OSS-Instruct
    technique to create instruction-following data from unlabeled open-source code
    snippets, constructing a dataset of 75,000 samples based on the StarCoder dataset Lozhkov
    et al. ([2024](#bib.bib30)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Wizardcoder 罗等人 ([2024](#bib.bib4)) 利用 Evol-Instruct 方法，徐等人 ([2023a](#bib.bib7))
    对 Codealpaca 数据集进行了进一步演进。这种技术在深度和广度维度上迭代演进指令跟随数据。另一方面，Magicoder 魏等人 ([2023](#bib.bib5))
    采用 OSS-Instruct 技术从未标记的开源代码片段中创建指令跟随数据，基于 StarCoder 数据集 [洛日科夫等人 ([2024](#bib.bib30))]
    构建了一个包含 75,000 个样本的数据集。
- en: 2.2 Instructional Fine-tuning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 指令微调
- en: Fine-tuning language models with instructional datasets has emerged as a powerful
    technique, offering notable improvements in model performance and alignment with
    human preferences and safety. By exploring a diverse array of instructional tasks,
    Wei et al. ([2021](#bib.bib31)) demonstrated a significant enhancement in zero-shot
    performance on unseen tasks through fine-tuning. Building on this, Chung et al.
    ([2024](#bib.bib32)) showed that scaling both the number of tasks and the model
    size can lead to substantial performance gains across different model architectures.
    Peng et al. ([2023](#bib.bib33)) further advanced this field by leveraging large
    language models (LLMs) to generate high-quality instruction-following data, resulting
    in improved zero-shot performance on new tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指令数据集对语言模型进行微调已成为一种强有力的技术，显著提高了模型的性能和与人类偏好及安全性的对齐。通过探索各种指令任务，魏等人 ([2021](#bib.bib31))
    通过微调展示了在未见任务上的零样本性能的显著提升。在此基础上，钟等人 ([2024](#bib.bib32)) 证明了同时扩大任务数量和模型规模可以在不同模型架构中带来显著的性能提升。彭等人
    ([2023](#bib.bib33)) 进一步推动了这一领域，通过利用大型语言模型（LLMs）生成高质量的指令跟随数据，从而在新任务上提高了零样本性能。
- en: A recent study Zhou et al. ([2023](#bib.bib9)) introduces the Superficial Alignment
    Hypothesis, which posits that the bulk of knowledge in LLMs is acquired during
    pretraining. It further suggests that minimal fine-tuning data is sufficient to
    align these models with human preferences. The study demonstrates a noteworthy
    enhancement in LLM performance with just 1,000 high-quality instruction data points.
    Subsequently, a plethora of research endeavors have concentrated on refining dataset
    quality through diverse filtering methodologies for general instruction following Xu
    et al. ([2023b](#bib.bib34)); Chen et al. ([2024](#bib.bib22)); Liu et al. ([2023a](#bib.bib35)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项研究 [周等人 ([2023](#bib.bib9))] 提出了表面对齐假说，该假说认为 LLMs 中的大部分知识是在预训练期间获得的。它进一步建议，最少的微调数据就足以使这些模型与人类偏好对齐。该研究展示了仅使用
    1,000 个高质量指令数据点就能显著提升 LLM 性能。随后，大量的研究工作集中在通过多样化的过滤方法改进数据集质量，以进行通用指令跟随 [徐等人 ([2023b](#bib.bib34))]；[陈等人
    ([2024](#bib.bib22))]；[刘等人 ([2023a](#bib.bib35))]。
- en: 2.3 Data Pruning for Efficient Training
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 高效训练的数据剪枝
- en: Various pruning methods have been explored for selecting more informative samples
    for model training, each tailored to different scenarios. Data clustering has
    been widely used as a highly effective technique for data pruning. TLDR Wang et al.
    ([2023](#bib.bib16)) utilized KMeans clustering to group similar data points and
    uniformly sampled from each cluster. They employ Image-Text Matching (ITM) scores
    to identify suitable vision-text pairs, offering another perspective on sample
    selection. DEFT Das and Khetan ([2023](#bib.bib21)) utilizes unsupervised core-set
    selection for clustering-based data-efficient fine-tuning of LLMs. This approach
    significantly enhances data efficiency in fine-tuning for text-editing applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 各种剪枝方法已被探索用于选择更具信息量的样本进行模型训练，每种方法都针对不同场景。数据聚类已被广泛使用作为一种高效的数据剪枝技术。TLDR 王等人 ([2023](#bib.bib16))
    利用 KMeans 聚类将相似的数据点分组，并从每个聚类中均匀抽样。他们采用图像-文本匹配（ITM）评分来识别合适的视觉-文本对，提供了样本选择的另一种视角。DEFT
    Das 和 Khetan ([2023](#bib.bib21)) 利用无监督的核心集选择进行基于聚类的数据高效微调 LLMs。这种方法显著提升了文本编辑应用中的数据效率。
- en: Metrics like Hardness Sorscher et al. ([2022](#bib.bib37)), Instruction Following
    Difficulty (IFD) Li et al. ([2023](#bib.bib38)) (Li et al., 2023), and SuperFiltering Li
    et al. ([2024](#bib.bib39)) focus on identifying "hard" samples that are either
    difficult to learn or easy to forget, tracking each data sample throughout training.
    In addition to these, sample influence metrics such as LESS Xia et al. ([2024](#bib.bib18))
    and TracIn Pruthi et al. ([2020](#bib.bib19)) monitor model gradients and the
    impact of individual samples, albeit with significant computational overhead for
    large models and datasets. Quality metrics from external oracles Chen et al. ([2024](#bib.bib22));
    Liu et al. ([2023a](#bib.bib35)), leverage strong language models like ChatGPT
    for data selection. However, utilizing external oracles may not always be feasible
    due to cost constraints.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 指标如 Hardness Sorscher et al. ([2022](#bib.bib37))、Instruction Following Difficulty
    (IFD) Li et al. ([2023](#bib.bib38))（Li et al.，2023）和 SuperFiltering Li et al.
    ([2024](#bib.bib39)) 关注于识别那些“困难”的样本，这些样本要么难以学习，要么容易遗忘，跟踪每个数据样本在训练过程中的表现。除此之外，像
    LESS Xia et al. ([2024](#bib.bib18)) 和 TracIn Pruthi et al. ([2020](#bib.bib19))
    等样本影响指标监控模型梯度和单个样本的影响，但对于大型模型和数据集来说，这些方法的计算开销很大。来自外部预言者的质量指标 Chen et al. ([2024](#bib.bib22));
    Liu et al. ([2023a](#bib.bib35))，利用像 ChatGPT 这样的强大语言模型进行数据选择。然而，由于成本限制，利用外部预言者可能并不总是可行的。
- en: '![Refer to caption](img/7b381bd511aae9d7aeefaca461d5527b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7b381bd511aae9d7aeefaca461d5527b.png)'
- en: 'Figure 1: The overview of efficient data pruning for fine-tuning LLMs with
    large scale datasets. First, We reduce the encode instruction-following data into
    embedding and reduce the dimension of feature representation. Second, we apply
    clustering to identify and group up similar data samples. Finally, we applied
    pruning metrics to further reduce data size.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：大规模数据集下，针对 LLMs 进行高效数据修剪的概述。首先，我们将编码指令跟随数据转换为嵌入，并减少特征表示的维度。其次，我们应用聚类来识别和分组相似的数据样本。最后，我们应用修剪指标进一步减少数据规模。
- en: 3 Methodology
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Our goal is to select high-quality, representative data samples so that training
    on these subsets yields performance that is comparable to or better than training
    on the entire dataset. The overview of efficient data pruning for fine-tuning
    LLMs with large scale datasets is illustrate in Figure [1](#S2.F1 "Figure 1 ‣
    2.3 Data Pruning for Efficient Training ‣ 2 Related Work ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning"). First, we use
    an embedding model to project the instruction-code pairs into a vector representation.
    We further reduce the dimension of feature representation to reduce computation
    complexity of the following steps. We then apply clustering to identify and group
    up similar data samples. Finally, we applied pruning metrics to further reduce
    data size. The detail pseudo code is in Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的目标是选择高质量、具有代表性的数据样本，以便在这些子集上的训练能获得与在整个数据集上训练相当或更好的性能。大规模数据集下针对 LLMs 进行高效数据修剪的概述如图
    [1](#S2.F1 "Figure 1 ‣ 2.3 Data Pruning for Efficient Training ‣ 2 Related Work
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning") 所示。首先，我们使用嵌入模型将指令-代码对投影到向量表示中。我们进一步减少特征表示的维度，以减少后续步骤的计算复杂性。然后，我们应用聚类来识别和分组相似的数据样本。最后，我们应用修剪指标进一步减少数据规模。详细的伪代码见算法
    [1](#alg1 "Algorithm 1 ‣ 3 Methodology ‣ Code Less, Align More: Efficient LLM
    Fine-tuning for Code Generation with Data Pruning")。'
- en: 'When dealing with coding datasets, two primary selection directions can be
    considered: syntactical and semantic. Selecting programs that are syntactically
    different but semantically equivalent, or vice versa, can be inefficient. Our
    design will focus on identifying syntactical differences. Detecting semantic differences
    between programs typically requires fuzzing techniques Chen et al. ([2018](#bib.bib40)),
    which involve creating larger test samples and executing programs to group them
    based on behavior. This approach contradicts our objective of reducing computational
    costs. Therefore, our method emphasizes syntactical analysis to achieve efficient
    and effective data selection.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理编码数据集时，可以考虑两种主要的选择方向：语法上的和语义上的。选择在语法上不同但语义等效的程序，或者反之，可能效率低下。我们的设计将集中于识别语法上的差异。检测程序之间的语义差异通常需要模糊测试技术
    Chen et al. ([2018](#bib.bib40))，这些技术涉及创建更大的测试样本并执行程序，以根据行为对程序进行分组。这种方法与我们减少计算成本的目标相矛盾。因此，我们的方法强调语法分析，以实现高效和有效的数据选择。
- en: Algorithm 1 Data Pruning Algorithm
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 数据修剪算法
- en: '1:Initialize $Embbedding$, Compression $Ratio$2:Initialize $selected\leftarrow[]$3:$X\leftarrow$
    PCA($Embedding$)4:$Cluster\leftarrow$ ClusterAlgo($X$)5:for each $idx,items$ in
    $Cluster$ do6:     $score\leftarrow$ PruningMetrics($item$)7:     $remain\leftarrow$
    Random($items$, prob=$score$)8:     Update $Cluster[ids]\leftarrow remain$9:     Append
    $selected\leftarrow remain$10:end for11:Output: $selected$'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '1:初始化 $Embbedding$，压缩 $Ratio$2:初始化 $selected\leftarrow[]$3:$X\leftarrow$ PCA($Embedding$)4:$Cluster\leftarrow$
    ClusterAlgo($X$)5:对每个 $idx,items$ 在 $Cluster$ 中进行6:     $score\leftarrow$ PruningMetrics($item$)7:     $remain\leftarrow$
    Random($items$, prob=$score$)8:     更新 $Cluster[ids]\leftarrow remain$9:     附加
    $selected\leftarrow remain$10:结束 for11:输出: $selected$'
- en: 3.1 Dimension Reduction
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 维度减少
- en: We convert each instruction-code pair into vector representation using a embedding
    model from raw text to enhance the efficiency of clustering and computation of
    pruning metrics Naik ([2024](#bib.bib41)). Recent research indicates that distances
    based on LLM embeddings effectively capture syntactic differences. To address
    the computational complexity, we employ Principle Component Analysis (PCA) Maćkiewicz
    and Ratajczak ([1993](#bib.bib42)) to reduce the dimensionality of the vector
    representations, as representations extracted from LLMs often exceed a thousand
    dimensions. Moreover, this approach prevents the subsequent utilization of several
    pruning metrics, which involve kernel methods, from being hindered in high-dimensional
    spaces by the curse of dimensionality.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从原始文本到嵌入模型的向量表示来转换每个指令-代码对，以提高聚类效率和修剪度量的计算效率 Naik ([2024](#bib.bib41))。最近的研究表明，基于LLM嵌入的距离能够有效地捕捉句法差异。为了应对计算复杂性，我们采用主成分分析（PCA）
    Maćkiewicz 和 Ratajczak ([1993](#bib.bib42)) 来降低向量表示的维度，因为从LLM中提取的表示通常超过一千维。此外，这种方法可以防止多个修剪度量（涉及核方法）在高维空间中因维度灾难而受到阻碍。
- en: 3.2 Clustering
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 聚类
- en: Clustering is a critical step in our methodology to group similar instruction-code
    pairs, which facilitates the selection of diverse and representative samples.
    Before clustering, we normalize the vector representations to ensure that each
    feature contributes equally to the distance calculations. From each cluster, we
    then sample instruction-code pairs to create a subset that is representative of
    the entire dataset. The sampling strategy is further decided by different pruning
    metrics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是我们方法中的关键步骤，用于将相似的指令-代码对分组，这有助于选择多样且具有代表性的样本。在聚类之前，我们对向量表示进行标准化，以确保每个特征在距离计算中均等贡献。然后，从每个簇中抽取指令-代码对，以创建一个代表整个数据集的子集。采样策略由不同的修剪度量进一步决定。
- en: 3.2.1 KMeans
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 KMeans
- en: The KMeans algorithm Kanungo et al. ([2002](#bib.bib44)) partitions data into
    $k$ clusters. By minimizing the within-cluster sum-of-squares, KMeans ensures
    that each cluster is as compact as possible. The main advantage of KMeans is its
    scalability and efficiency in handling large datasets.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans 算法 Kanungo et al. ([2002](#bib.bib44)) 将数据划分为 $k$ 个簇。通过最小化簇内平方和，KMeans
    确保每个簇尽可能紧凑。KMeans 的主要优点是其可扩展性和处理大型数据集的效率。
- en: 3.2.2 Agglomerative Clustering
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 聚合聚类
- en: Agglomerative Clustering Müllner ([2011](#bib.bib45)) builds nested clusters
    with linkage criteria. This method is advantageous since it does not require the
    number of clusters to be specified a priori. This flexibility allows for a more
    nuanced selection of representative samples, which is beneficial for maintaining
    the quality of the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类 Müllner ([2011](#bib.bib45)) 使用连通性标准构建嵌套簇。这种方法的优势在于它不需要事先指定簇的数量。这种灵活性允许对代表性样本进行更细致的选择，这有助于保持数据集的质量。
- en: 3.2.3 HDBSCAN
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 HDBSCAN
- en: Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) Rahman
    et al. ([2016](#bib.bib46)) performs clustering based on the concept of core samples,
    which are samples located in high-density areas measured by a distance metric.
    This approach aligns well with our design hypothesis to find the most syntactically
    representative data samples. Notably, HDBSCAN removes noisy samples not clustered
    into core samples as outliers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于核心样本的层次密度空间聚类（HDBSCAN） Rahman et al. ([2016](#bib.bib46)) 根据核心样本的概念进行聚类，这些样本位于通过距离度量测量的高密度区域内。这种方法与我们设计假设相一致，可以找到最具句法代表性的数据信样本。值得注意的是，HDBSCAN
    将未聚类到核心样本中的噪声样本作为离群点删除。
- en: '| Model | Training | Benchmark | Improvement Over Base |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练 | 基准测试 | 相对于基础的改进 |'
- en: '|  | Tokens | HumanEval (+) | MBPP (+) | HumanEval (+) | MBPP (+) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | Tokens | HumanEval (+) | MBPP (+) | HumanEval (+) | MBPP (+) |'
- en: '| GPT-3.5 Turbo | - | 72.6  (65.9) | 81.7  (69.4) | - | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | - | 72.6  (65.9) | 81.7  (69.4) | - | - |'
- en: '| GPT-4 Turbo | - | 85.4  (81.7) | 83.0  (70.7) | - | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Turbo | - | 85.4  (81.7) | 83.0  (70.7) | - | - |'
- en: '| DeepSeek-Coder-Base | - | 47.6  (39.6) | 70.2  (56.6) | - | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-Base | - | 47.6  (39.6) | 70.2  (56.6) | - | - |'
- en: '| DeepSeek-Coder-Instruct | 2B | 73.8  (70.1) | 72.7  (63.4) | 26.2  (30.5)
    | 2.5  (6.8) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-Instruct | 2B | 73.8  (70.1) | 72.7  (63.4) | 26.2  (30.5)
    | 2.5  (6.8) |'
- en: '| Magicoder-DS | 90M | 66.5  (60.4) | 75.4  (61.9) | 18.9  (20.8) | 5.2  (5.3)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Magicoder-DS | 90M | 66.5  (60.4) | 75.4  (61.9) | 18.9  (20.8) | 5.2  (5.3)
    |'
- en: '| Magicoder$\mathcal{S}$-DS | 240M | 76.8  (70.7) | 75.7  (64.4) | 29.2  (31.1)
    | 5.5  (7.8) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Magicoder$\mathcal{S}$-DS | 240M | 76.8  (70.7) | 75.7  (64.4) | 29.2  (31.1)
    | 5.5  (7.8) |'
- en: '| Ours (full data) | 234M | 74.3  (70.8) | 74.5  (62.3) | 26.7  (31.2) | 4.3  (5.7)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (完整数据) | 234M | 74.3  (70.8) | 74.5  (62.3) | 26.7  (31.2) | 4.3  (5.7)
    |'
- en: '| Ours (90%) | 192M | 77.0  (71.6) | 76.9  (64.0) | 29.4  (32.0) | 6.7  (7.4)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (90%) | 192M | 77.0  (71.6) | 76.9  (64.0) | 29.4  (32.0) | 6.7  (7.4)
    |'
- en: '| Ours (50%) | 106M | 71.0  (64.0) | 78.0  (64.0) | 23.4  (24.4) | 7.8  (7.4)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (50%) | 106M | 71.0  (64.0) | 78.0  (64.0) | 23.4  (24.4) | 7.8  (7.4)
    |'
- en: '| Ours (10%) | 21M | 70.4  (65.0) | 73.0  (60.2) | 22.8  (25.4) | 2.8  (3.6)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (10%) | 21M | 70.4  (65.0) | 73.0  (60.2) | 22.8  (25.4) | 2.8  (3.6)
    |'
- en: '| Ours (1%) | 2M | 64.6  (58.0) | 74.3  (61.9) | 17.0  (18.4) | 4.1  (5.3)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (1%) | 2M | 64.6  (58.0) | 74.3  (61.9) | 17.0  (18.4) | 4.1  (5.3) |'
- en: 'Table 1: $pass@1$ (%) results of different LLMs on HumanEval (+) and MBPP (+)
    with greedy decoding. We directly use results from prior work Guo et al. ([2024](#bib.bib47));
    Wei et al. ([2023](#bib.bib5)). All our results are reported using the HDBSCAN
    clustering algorithm with the diversity pruning metric (HDBSCAN-diversity). To
    account for the randomness of clustering and training, we report the averaged
    results from three runs evaluated with EvalPlus Liu et al. ([2023b](#bib.bib48)).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同 LLM 在 HumanEval (+) 和 MBPP (+) 上的 $pass@1$ (%) 结果，使用贪婪解码。我们直接使用来自之前工作的结果 Guo
    et al. ([2024](#bib.bib47)); Wei et al. ([2023](#bib.bib5))。我们所有的结果都使用 HDBSCAN
    聚类算法与多样性剪枝指标（HDBSCAN-diversity）报告。为了考虑聚类和训练的随机性，我们报告了三次运行的平均结果，使用 EvalPlus Liu
    et al. ([2023b](#bib.bib48)) 进行评估。
- en: 3.3 Pruning Metrics
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 剪枝指标
- en: The criteria of choosing pruning metrics continually aligns with the idea of
    detecting syntactic difference and find most representative samples. We explain
    the pruning metrics explored in our experiments in the following sections.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 选择剪枝指标的标准始终与检测句法差异和寻找最具代表性的样本的思想一致。我们在以下部分解释了我们实验中探索的剪枝指标。
- en: 3.3.1 Diversity Metric
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 多样性指标
- en: We use a distance-based metric that simply evaluates the diversity score of
    a single instance shown as follow,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一种基于距离的指标来简单评估单个实例的多样性分数，如下所示，
- en: '|  | $d_{i}=\min_{\mathbf{x}\in\mathcal{K}\setminus\{\mathbf{x}_{i}\}}\text{dist}(\mathbf{x}_{i},\mathbf{x}),$
    |  | (1) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{i}=\min_{\mathbf{x}\in\mathcal{K}\setminus\{\mathbf{x}_{i}\}}\text{dist}(\mathbf{x}_{i},\mathbf{x}),$
    |  | (1) |'
- en: where $x_{i}$ is the vector representation, dist is a distance function, $K$
    represents selected query set within the dataset cluster, and $d_{i}$ is the diversity
    score of a sample $x_{i}$. We use the dot product of the embeddings as the distance
    function as our embeddings are normalized prior to pruning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}$ 是向量表示，dist 是距离函数，$K$ 代表数据集簇内选择的查询集，而 $d_{i}$ 是样本 $x_{i}$ 的多样性分数。我们使用嵌入的点积作为距离函数，因为我们的嵌入在剪枝之前已被归一化。
- en: 3.3.2 Density Metric
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 密度指标
- en: We applied kernel density estimation (KDE) to measure the density of samples
    in the feature space. KDE estimates the probability density function of a random
    variable. The density score for a sample $\mathbf{x}_{i}$ is given by,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了核密度估计（KDE）来测量特征空间中样本的密度。KDE 估计随机变量的概率密度函数。样本 $\mathbf{x}_{i}$ 的密度分数由下式给出，
- en: '|  | $\rho(\mathbf{x}_{i})=\frac{1}{nh^{d}}\sum_{j=1}^{n}K\left(\frac{\mathbf{x}_{i}-\mathbf{x}_{j}}{h}\right),$
    |  | (2) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho(\mathbf{x}_{i})=\frac{1}{nh^{d}}\sum_{j=1}^{n}K\left(\frac{\mathbf{x}_{i}-\mathbf{x}_{j}}{h}\right),$
    |  | (2) |'
- en: where $K$ is the kernel function, $h$ is the bandwidth parameter, $d$ is the
    dimension of the feature space, and $n$ is the total number of samples. The kernel
    function $K$ (typically a Gaussian) measures the influence of nearby points on
    the density estimate. A high density score indicates that a sample is located
    in a region with many similar instances, suggesting it is less critical for maintaining
    diversity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $K$ 是核函数，$h$ 是带宽参数，$d$ 是特征空间的维度，$n$ 是样本总数。核函数 $K$（通常是高斯核）衡量附近点对密度估计的影响。高密度分数表示样本位于具有许多相似实例的区域，表明它在保持多样性方面不那么重要。
- en: 3.3.3 Random
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 随机
- en: The simplest baseline is random selection, where we randomly sample data from
    the selected cluster or entire training dataset (without clustering) for instruction
    tuning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的基线是随机选择，我们从选定的簇或整个训练数据集（不进行聚类）中随机抽取数据进行指令调整。
- en: '![Refer to caption](img/7bdcde9ac1423d5ca7b185ea9e47f808.png)![Refer to caption](img/dd4cce74a103c8446e374b19563bbce1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7bdcde9ac1423d5ca7b185ea9e47f808.png)![参见说明](img/dd4cce74a103c8446e374b19563bbce1.png)'
- en: 'Figure 2: Performance comparison of HDBSCAN-diversity and nocluster-random
    methods across different benchmarks. Our strategy outperform the baseline across
    different datasets with a large margin. We also maintain better or equivalent
    performance compare to full dataset even at the size of 10% on MBPP. The $pass@1$
    metric is plotted against varying compression ratios, demonstrating the robustness
    and effectiveness. HumanEval presents larger variance across experiments possibly
    due to less problems entries.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：HDBSCAN-diversity 和 nocluster-random 方法在不同基准测试中的性能比较。我们的策略在不同数据集上显著优于基准。即使在MBPP的10%大小下，我们也保持了与完整数据集相当或更好的性能。$pass@1$
    指标与不同压缩比绘制图示，展示了鲁棒性和有效性。由于问题条目较少，HumanEval的实验结果表现出更大的方差。
- en: 4 Experiments
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we first present the experimental setup in Section [4.1](#S4.SS1
    "4.1 Setup ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning"), followed by our primary findings in Section [4.5](#S4.SS5
    "4.5 Main Results ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning"). Here, we highlight the performance improvements
    of our pruning methods compared to full dataset training across four datasets:
    MBPP(+), and HumanEval(+). We also compare the $pass@1$ scores with baseline methods
    at various compression ratios.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们首先在第[4.1节](#S4.SS1 "4.1 Setup ‣ 4 Experiments ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning")介绍实验设置，接着在第[4.5节](#S4.SS5
    "4.5 Main Results ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning")展示我们的主要发现。在这里，我们重点展示了我们剪枝方法在四个数据集（MBPP(+)
    和 HumanEval(+)）上的性能提升，并与完整数据集训练进行了比较。我们还比较了不同压缩比下的 $pass@1$ 分数与基准方法。'
- en: 4.1 Setup
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: We employed DeepSeek-Coder-Base 6.7B Guo et al. ([2024](#bib.bib47)) as the
    base model due to its superior performance among open-source models. We used PCA Maćkiewicz
    and Ratajczak ([1993](#bib.bib42)) algorithm in all experiments and reduce the
    dimension to 10\. To account for randomness in clustering algorithm and training,
    we repeat each experiment 3 times and report the average and standard deviation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了 DeepSeek-Coder-Base 6.7B Guo 等 ([2024](#bib.bib47)) 作为基础模型，因为它在开源模型中表现优异。我们在所有实验中使用了
    PCA Maćkiewicz 和 Ratajczak ([1993](#bib.bib42)) 算法，将维度降到 10。为了考虑到聚类算法和训练中的随机性，我们重复每个实验3次，并报告平均值和标准差。
- en: 4.2 Training
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 训练
- en: 'Datasets In our experiment, we adopt two synthetic code dataset as training
    data: Magicoder-OSS-Instruct-75K ¹¹1[https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K)
    (MIT License) and Magicoder-Evol-Instruct-110K ²²2[https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K)
    (Apache-2.0 License). Together we have a combined 185k entries in total as our
    target large scale dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 在我们的实验中，我们采用了两个合成代码数据集作为训练数据：Magicoder-OSS-Instruct-75K ¹¹1[https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K)（MIT许可证）和
    Magicoder-Evol-Instruct-110K ²²2[https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K)（Apache-2.0许可证）。我们共有185k条记录作为目标大规模数据集。
- en: We fine-tune the base model by combining and shuffling the two training dataset.
    This is different as in the original Magicoder Wei et al. ([2023](#bib.bib5))
    implementation, where they first fine-tune the base models for 2 epochs on OSS-Instruct
    data and continue training for 2 more epochs on Evol-Instruct data. We note that
    despite such difference in our implementation details, our full dataset performance
    closely matches the Magicoder$\mathcal{S}$-DS results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过结合和打乱两个训练数据集来微调基础模型。这不同于原始的 Magicoder Wei 等人（[2023](#bib.bib5)）实现，其中他们首先在
    OSS-Instruct 数据上微调基础模型 2 个周期，然后在 Evol-Instruct 数据上继续训练 2 个周期。我们注意到，尽管在我们的实现细节上有所不同，我们的完整数据集性能仍然与
    Magicoder$\mathcal{S}$-DS 结果非常接近。
- en: 'Training Training is conducted with 16 NVIDIA A100-80GB GPUs through the Distributed
    Data Parallel (DDP) module from PyTorch. We set the learning rate at 5e-5 with
    15 warmup steps and a linear learning rate scheduler. We use Adam Kingma and Ba
    ([2014](#bib.bib49)) as our optimizer with full parameter updates and truncate
    sequence length longer than 4096 tokens. We use a batch size of 512 samples Wei
    et al. ([2023](#bib.bib5)) when the dataset size exceeds $\geq 10\%$ of the original
    size, and a batch size of 32 Zhou et al. ([2023](#bib.bib9)) for heavily pruned
    small-scaled data experiments in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation
    ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"). We fine-tune for 2 epochs regardless of the dataset size.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '训练 训练通过 PyTorch 的分布式数据并行（DDP）模块在 16 台 NVIDIA A100-80GB GPU 上进行。我们将学习率设置为 5e-5，进行
    15 次预热步骤，并使用线性学习率调度器。我们使用 Adam Kingma 和 Ba（[2014](#bib.bib49)）作为优化器，进行全参数更新，并将序列长度截断为
    4096 个标记以上。数据集大小超过 $\geq 10\%$ 原始大小时，我们使用 512 的批量大小 Wei 等人（[2023](#bib.bib5)），而对于严重剪枝的小规模数据实验中，我们在图
    [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation ‣ 4 Experiments ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning") 中使用 32 的批量大小
    Zhou 等人（[2023](#bib.bib9)）。无论数据集大小如何，我们都进行 2 个周期的微调。'
- en: 4.3 Evaluation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估
- en: '![Refer to caption](img/6efef175f63129a0cc2e7ddffad7acff.png)![Refer to caption](img/747677a9a30c3978af74cd4f07296d40.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6efef175f63129a0cc2e7ddffad7acff.png)![参考说明](img/747677a9a30c3978af74cd4f07296d40.png)'
- en: 'Figure 3: Comparison of performance under extreme data pruning conditions on
    the MBPP and HumanEval benchmarks. The $pass@1$ score on MBPP shows that even
    with just 1% of the data, our method achieves nearly equivalent performance to
    the full dataset, with a 4.1% improvement over the base model. On the HumanEval
    benchmark, while the performance with 1% of the data degrades compared to the
    full dataset training, it still achieves an 17.0% improvement over the base model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 MBPP 和 HumanEval 基准下，在极端数据剪枝条件下的性能比较。MBPP 上的 $pass@1$ 分数表明，即使只有 1% 的数据，我们的方法也能实现几乎与完整数据集相当的性能，比基础模型提高了
    4.1%。在 HumanEval 基准上，尽管使用 1% 的数据时性能相比完整数据集训练有所下降，但相比基础模型仍提高了 17.0%。
- en: Datasets HumanEval Chen et al. ([2021](#bib.bib50)) and MBPP Austin et al. ([2021](#bib.bib51))
    are two of the most widely used benchmarks for code generation. The two datasets
    contains 164 and 1401 problems respectively. Each task in these benchmarks includes
    a task description (e.g., docstring) as the prompt, where LLMs generate corresponding
    code whose correctness is checked by a handful of test cases. Because tests in
    these benchmarks can be insufficient, for more rigorous evaluation, we use HumanEval+
    and MBPP+, both powered by EvalPlus Liu et al. ([2023b](#bib.bib48)) to obtain
    80× and 35× more tests, respectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 HumanEval Chen 等人（[2021](#bib.bib50)）和 MBPP Austin 等人（[2021](#bib.bib51)）是代码生成中使用最广泛的基准数据集之一。这两个数据集分别包含
    164 个和 1401 个问题。这些基准中的每个任务都包括一个任务描述（例如，文档字符串）作为提示，LLMs 生成相应的代码，其正确性由少量的测试用例进行检查。由于这些基准中的测试可能不够充分，为了进行更严格的评估，我们使用
    HumanEval+ 和 MBPP+，这两者都由 EvalPlus Liu 等人（[2023b](#bib.bib48)）提供支持，分别获得了 80 倍和
    35 倍的测试用例。
- en: 'Metric Following prior work Chen et al. ([2021](#bib.bib50)); Liu et al. ([2023b](#bib.bib48)),
    for each experiment we use the unbiased pass@k estimator shown as follow and mainly
    focus on comparing $pass@1$ metric:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标 参考之前的工作 Chen 等人（[2021](#bib.bib50)）；Liu 等人（[2023b](#bib.bib48)），在每个实验中我们使用如下所示的无偏的
    pass@k 估计器，并主要关注比较 $pass@1$ 指标：
- en: '|  | $pass@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right].$
    |  | (3) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $pass@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right].$
    |  | (3) |'
- en: Inference We employ the EvalPlus Liu et al. ([2023b](#bib.bib48)) inference
    script with sanitation postprocessing. We adopted the vLLM Kwon et al. ([2023](#bib.bib52))
    framework and use greedy decoding for every code generation. The inference engine
    is setup with bf16 dtype, tensor parallel size of 2 and a maximum length of 4096.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 推断：我们使用了 EvalPlus Liu 等人（[2023b](#bib.bib48)）的推断脚本，并进行了清理后处理。我们采用了 vLLM Kwon
    等人（[2023](#bib.bib52)）的框架，并对每次代码生成使用贪婪解码。推断引擎设置为 bf16 数据类型，张量并行大小为 2，最大长度为 4096。
- en: 4.4 Implementation Details
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实现细节
- en: In our experiment, the PCA reduction is fitted on the benchmark dataset and
    then apply the projection to the instruction data. We used the OpenAI text-embedding-ada-002
    embedding model to encode data. All the clustering and kernel density estimation
    parameters are as default in sklearn Pedregosa et al. ([2011](#bib.bib53)). For
    algorithms that requires choosing an optimal number of clusters (such as KMeans)
    is crucial, we utilize the Elbow method Roy ([1953](#bib.bib54)) to find the point
    where adding more clusters does not significantly improve the variance explained.
    For pruning metrics, we applied the Scott’s Rule Scott ([2010](#bib.bib55)), a
    normal-reference rule for deciding the Gaussian kernel bandwidth, for kernel density
    estimation and random select 10% of the dataset as query set ($K$) for diversity
    metric.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，PCA 降维是对基准数据集进行拟合后，再将投影应用到指令数据上。我们使用了 OpenAI text-embedding-ada-002
    嵌入模型来编码数据。所有的聚类和核密度估计参数都采用 sklearn Pedregosa 等人（[2011](#bib.bib53)）中的默认设置。对于需要选择最佳聚类数的算法（如
    KMeans），我们使用 Elbow 方法 Roy（[1953](#bib.bib54)）来找到添加更多聚类不会显著改善解释方差的点。对于修剪度量，我们应用了
    Scott 的规则 Scott（[2010](#bib.bib55)），这是用于确定高斯核带宽的标准参考规则，用于核密度估计，并随机选择 10% 的数据集作为查询集（$K$）用于多样性度量。
- en: 4.5 Main Results
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 主要结果
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.2.3 HDBSCAN ‣ 3.2 Clustering ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning") presents the $pass@1$ results of different leading code LLMs on the
    HumanEval and MBPP benchmarks, computed with greedy decoding. All our results
    are reported using the HDBSCAN clustering algorithm with the diversity pruning
    metric (HDBSCAN-diversity). To account for the randomness of clustering and training,
    we report the averaged results from three runs. Notably, slight pruning of the
    training data could yield a performance improvement of up to 2.7% on HumanEval
    and 3.5% on MBPP compared to training with the full dataset. We further show that
    benchmark accuracy can be largely retained with 10% of the dataset, with slight
    degradation of 3.9% on HumanEval and 1.5% on MBPP compared with using the full
    training data. Even with just 1% of the data ($\sim$ 700 samples), our method
    maintains competitive performance and achieves large improvements over the base
    model, underscoring the efficiency of our pruning strategy.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S3.T1 "Table 1 ‣ 3.2.3 HDBSCAN ‣ 3.2 Clustering ‣ 3 Methodology ‣ Code
    Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning")
    显示了不同领先代码 LLM 在 HumanEval 和 MBPP 基准测试上的 $pass@1$ 结果，这些结果是通过贪婪解码计算的。我们所有的结果都是使用
    HDBSCAN 聚类算法和多样性修剪度量（HDBSCAN-diversity）报告的。为了考虑到聚类和训练的随机性，我们报告了三次运行的平均结果。值得注意的是，与使用完整数据集进行训练相比，略微修剪训练数据可以在
    HumanEval 上提高多达 2.7% 的性能，在 MBPP 上提高 3.5%。我们进一步展示了使用 10% 数据集可以大致保持基准准确性，相较于使用完整训练数据在
    HumanEval 上轻微下降 3.9%，在 MBPP 上下降 1.5%。即使仅使用 1% 的数据（$\sim$ 700 个样本），我们的方法也能保持竞争性能，并在基础模型上实现显著改进，突显了我们修剪策略的效率。'
- en: 'Figure [2](#S3.F2 "Figure 2 ‣ 3.3.3 Random ‣ 3.3 Pruning Metrics ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning") illustrates the detail of our pruning methods across four datasets:
    MBPP, MBPP+, HumanEval, and HumanEval+. Each subplot compares the $pass@1$ scores
    of the HDBSCAN-diversity method with the nocluster-random baseline at various
    compression ratios. HDBSCAN-diversity method consistently outperforms the nocluster-random
    baseline. The performance typically improves with slight compression, peaking
    around 10-20%, and then gradually declines. This trend highlights the robustness
    of the HDBSCAN-diversity method, maintaining higher $pass@1$ scores than full
    dataset even at 90% compression.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S3.F2 "图 2 ‣ 3.3.3 随机 ‣ 3.3 修剪指标 ‣ 3 方法论 ‣ 少编码，多对齐：高效的 LLM 微调用于代码生成的数据修剪")
    展示了我们在四个数据集上修剪方法的详细情况：MBPP、MBPP+、HumanEval 和 HumanEval+。每个子图比较了 HDBSCAN-diversity
    方法与 nocluster-random 基线在不同压缩比下的 $pass@1$ 分数。HDBSCAN-diversity 方法始终优于 nocluster-random
    基线。性能通常在轻微压缩时有所提高，约在 10-20% 达到峰值，然后逐渐下降。这一趋势突显了 HDBSCAN-diversity 方法的稳健性，即使在 90%
    压缩下也保持了高于完整数据集的 $pass@1$ 分数。
- en: 'We further examine how our data pruning method performs when pushed to the
    extreme, aiming to achieve the smallest possible dataset size on the MBPP benchmark.
    The results are presented in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation ‣ 4
    Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"). Remarkably, we found that even with just 1% of the data,
    our method achieves a 4.1% improvement over the base model, which is nearly equivalent
    to training on the full dataset. This demonstrates the robustness of our pruning
    method, highlighting its ability to maintain high performance with minimal data,
    thus significantly reducing the computational resources required.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步检验了当数据修剪方法被推向极限时的表现，旨在实现 MBPP 基准测试中尽可能小的数据集大小。结果见图 [3](#S4.F3 "图 3 ‣ 4.3
    评估 ‣ 4 实验 ‣ 少编码，多对齐：高效的 LLM 微调用于代码生成的数据修剪")。值得注意的是，即使仅使用 1% 的数据，我们的方法也比基础模型提高了
    4.1%，这几乎等同于在完整数据集上训练。这证明了我们修剪方法的稳健性，突显了它在数据量极少的情况下仍能保持高性能的能力，从而显著减少所需的计算资源。
- en: Overall, these results demonstrate the effectiveness of data pruning strategy
    in preserving critical data features and maintaining model performance under significant
    data reduction, making it a superior choice for coding dataset pruning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些结果表明数据修剪策略在保留关键数据特征和在显著数据减少下保持模型性能方面的有效性，使其成为编码数据集修剪的优选方法。
- en: 5 Ablation Studies
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 消融研究
- en: Our research includes four ablation studies designed to evaluate the impact
    of (1) clustering algorithms (2) pruning metrics (3) dimension reduction (4) input
    for vector representation on the effectiveness of data pruning. In the studies,
    we will mainly focus on the MBPP benchmark since it provides more stable and consistent
    results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究包括四项消融研究，旨在评估 (1) 聚类算法 (2) 修剪指标 (3) 维度减少 (4) 向量表示输入 对数据修剪效果的影响。在研究中，我们将主要关注
    MBPP 基准测试，因为它提供了更稳定和一致的结果。
- en: 5.1 Compare Clustering Algorithm
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 比较聚类算法
- en: 'In Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Compare Clustering Algorithm ‣ 5 Ablation
    Studies ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"), we present the results of applying different clustering algorithms
    without additional pruning metrics. The algorithms evaluated include Agglomerative
    Clustering, HDBSCAN, KMeans, and a baseline with no clustering (nocluster).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](#S5.F4 "图 4 ‣ 5.1 比较聚类算法 ‣ 5 消融研究 ‣ 少编码，多对齐：高效的 LLM 微调用于代码生成的数据修剪") 中，我们展示了应用不同聚类算法而不使用额外修剪指标的结果。评估的算法包括
    Agglomerative Clustering、HDBSCAN、KMeans 和无聚类基线 (nocluster)。
- en: The results demonstrate that clustering algorithms generally improve performance
    compared to the nocluster baseline, particularly at higher compression ratios.
    HDBSCAN consistently maintains higher $pass@1$ scores, showcasing its robustness
    in preserving critical data features. KMeans and Agglomerative Clustering also
    perform well, though with higher variability. These findings highlight the importance
    of clustering algorithms in enhancing data efficiency for coding datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，聚类算法通常在性能上优于无聚类基线，尤其是在较高的压缩比下。HDBSCAN 一贯保持更高的 $pass@1$ 分数，展示了其在保留关键数据特征方面的稳健性。KMeans
    和 Agglomerative Clustering 也表现良好，但变异性较大。这些发现强调了聚类算法在提高编码数据集的数据效率方面的重要性。
- en: '![Refer to caption](img/b922ca8b1900ce2b93155196f148b2b6.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b922ca8b1900ce2b93155196f148b2b6.png)'
- en: 'Figure 4: $pass@1$ on the MBPP benchmark comparing across different clustering
    algorithms and varied compression ratios of the training dataset. HDBSCAN demonstrate
    strong robustness in maintaining higher $pass@1$ scores compared to full dataset
    at the compression ratio of 90%.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MBPP 基准上的 $pass@1$ 分数，比较不同聚类算法和训练数据集的不同压缩比。HDBSCAN 展现出在 90% 压缩比下维持更高 $pass@1$
    分数的强鲁棒性。
- en: '![Refer to caption](img/9dca266baf411d67f58b1b3d8394cb42.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9dca266baf411d67f58b1b3d8394cb42.png)'
- en: 'Figure 5: Comparison of different pruning metrics using HDBSCAN clustering
    algorithms. Diversity metric has marginal advantage but its benefit may be limited
    and dependent on the clustering algorithm.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 HDBSCAN 聚类算法比较不同剪枝度量。多样性度量有轻微优势，但其好处可能有限且依赖于聚类算法。
- en: 5.2 Compare Pruning Metrics
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 比较剪枝度量
- en: 'We examine the impact of different pruning metrics on model performance. Using
    HDBSCAN clustering algorithm, we assess how these metrics influence performance
    as the data size decreases, as illustrated in Figure [5](#S5.F5 "Figure 5 ‣ 5.1
    Compare Clustering Algorithm ‣ 5 Ablation Studies ‣ Code Less, Align More: Efficient
    LLM Fine-tuning for Code Generation with Data Pruning"). The results indicate
    that the effectiveness of pruning metrics varies across different compression
    ratio. While Diversity metrics show slight improvements over other metrics, the
    margin of improvement is not substantial and only works between 10-40% compression
    ratio. This suggests that while more sophisticated pruning metrics can offer some
    benefits, their impact may be limited and also dependent on the clustering algorithm
    used.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考察了不同剪枝度量对模型性能的影响。使用 HDBSCAN 聚类算法，我们评估了这些度量如何随着数据大小的减少而影响性能，如图 [5](#S5.F5
    "图 5 ‣ 5.1 比较聚类算法 ‣ 5 消融研究 ‣ 代码更少，更多对齐：高效的 LLM 微调用于代码生成与数据剪枝") 所示。结果表明，剪枝度量的有效性在不同的压缩比下有所变化。虽然多样性度量相较于其他度量有轻微改善，但改进幅度不大，仅在
    10-40% 压缩比之间有效。这表明，虽然更复杂的剪枝度量可以提供一些好处，但其影响可能有限，并且依赖于使用的聚类算法。
- en: 5.3 Effect of PCA
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 PCA 的影响
- en: 'In Table [2](#S5.T2 "Table 2 ‣ 5.3 Effect of PCA ‣ 5 Ablation Studies ‣ Code
    Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning"),
    we evaluate the impact of applying Principal Component Analysis (PCA) on the performance
    of the KMeans clustering algorithm and Density metric at the compression ratio
    of 50%. The findings indicate that applying PCA generally degrades performance
    in terms of $pass@1$ scores for less than 0.6% on MBPP, and moderate negative
    impact of 4.3% on HumanEval. We hypothesize that the observed impact might be
    due to the imbalance between the MBPP and HumanEval datasets used for PCA training.
    Since the HumanEval dataset is significantly smaller than the MBPP dataset, it
    results in suboptimal extraction of principal components for HumanEval-like data.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [2](#S5.T2 "表格 2 ‣ 5.3 PCA 的影响 ‣ 5 消融研究 ‣ 代码更少，更多对齐：高效的 LLM 微调用于代码生成与数据剪枝")
    中，我们评估了在 50% 压缩比下应用主成分分析（PCA）对 KMeans 聚类算法和密度度量性能的影响。结果表明，应用 PCA 通常会导致 MBPP 上
    $pass@1$ 分数下降不到 0.6%，对 HumanEval 的负面影响中等，为 4.3%。我们推测观察到的影响可能是由于用于 PCA 训练的 MBPP
    和 HumanEval 数据集之间的不平衡。由于 HumanEval 数据集明显小于 MBPP 数据集，导致对 HumanEval 类数据的主成分提取不理想。
- en: Nonetheless, reducing the dimension from 1536 to 10 leads to $\sim$12x speed
    up for KMeans. HDBSCAN clustering without PCA does not complete within 4 hours,
    thus we do not report its numbers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将维度从 1536 降到 10 导致 KMeans 的速度提高了 $\sim$12 倍。HDBSCAN 聚类在没有 PCA 的情况下无法在 4
    小时内完成，因此我们没有报告其数据。
- en: '|  | No PCA | PCA |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 无 PCA | PCA |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dimension | 1536 | 10 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 1536 | 10 |'
- en: '| --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Runtime | 1307 sec | 183 sec |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | 1307 秒 | 183 秒 |'
- en: '| MBPP (+) | 74.4 (63.3) | 73.8 (62.4) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MBPP (+) | 74.4 (63.3) | 73.8 (62.4) |'
- en: '| HumanEval (+) | 71.8 (65.0) | 67.5 (62.5) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval (+) | 71.8 (65.0) | 67.5 (62.5) |'
- en: 'Table 2: Comparison of $pass@1$ scores, dimension, and data pruning runtime
    (excludes embedding and training) at 50% compression ratio for KMeans clustering
    with and without PCA (averaged over 3 runs).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：比较在 50% 压缩比下 KMeans 聚类的 $pass@1$ 分数、维度和数据剪枝运行时间（不包括嵌入和训练）（平均 3 次运行）。
- en: 5.4 Embeddings for Instruction or Code
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 指令或代码的嵌入
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.4 Embeddings for Instruction or Code ‣ 5 Ablation
    Studies ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"), we investigate the influence of various inputs on the embedding
    model. Specifically, we examine the effects of using only the instruction, only
    the code solution, or both as inputs for generating embeddings. Our findings indicate
    that combining both instructions and code as embedding inputs yields better performance
    compared to using either one alone. There are no significant differences in the
    results when using only instructions or only code. This suggests that even though
    instructions and code samples often correspond closely, it is crucial to maintain
    diversity and select informative samples from both during data pruning.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [3](#S5.T3 "表 3 ‣ 5.4 指令或代码的嵌入 ‣ 5 消融研究 ‣ 代码更少，对齐更多：数据修剪的高效 LLM 微调")中，我们研究了各种输入对嵌入模型的影响。具体来说，我们检查了仅使用指令、仅使用代码解决方案或两者作为生成嵌入的输入的效果。我们的研究结果表明，将指令和代码作为嵌入输入进行组合的性能优于仅使用其中之一。当仅使用指令或仅使用代码时，结果没有显著差异。这表明，即使指令和代码样本通常紧密对应，保持多样性并在数据修剪过程中从两者中选择有信息量的样本仍然至关重要。
- en: '| Feature Type | MBPP (+) | HumanEval (+) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 特征类型 | MBPP (+) | HumanEval (+) |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Both | 76.3 (62.5) | 73.1 (69.6) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 两者 | 76.3 (62.5) | 73.1 (69.6) |'
- en: '| Instruction | 74.0 (63.7) | 69.1 (63.6) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 指令 | 74.0 (63.7) | 69.1 (63.6) |'
- en: '| Code | 74.1 (62.7) | 69.2 (63.3) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | 74.1 (62.7) | 69.2 (63.3) |'
- en: 'Table 3: $pass@1$ scores for different embedding inputs with 50% compression
    ratio using KMeans clustering. Using both instruction and code brings slight benefits.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 使用 KMeans 聚类的 50% 压缩比下，不同嵌入输入的 $pass@1$ 分数。使用指令和代码都带来了一些微小的好处。'
- en: 6 Conclusion
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This study presents an efficient data pruning strategy designed to improve the
    efficiency of fine-tuning large language models on coding datasets. Our results
    demonstrate that advanced clustering and pruning techniques can significantly
    improve data efficiency in LLMs, reducing computational costs while maintaining
    performance. Future work could focus on enhancing data quality by generating more
    informative data from clusters with low pruning metrics. We hope our findings
    provide valuable insights for developing more effective and scalable strategies
    in training code-focused LLMs, further enhancing synthetic data generation and
    the efficiency of human annotations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种高效的数据修剪策略，旨在提高对编码数据集进行大规模语言模型微调的效率。我们的结果表明，先进的聚类和修剪技术可以显著提高 LLM 的数据效率，减少计算成本，同时保持性能。未来的工作可以集中在通过从具有低修剪指标的簇中生成更有信息的数据来提升数据质量。我们希望我们的发现能为开发更有效和可扩展的训练代码集中
    LLM 的策略提供有价值的见解，进一步提升合成数据生成和人工标注的效率。
- en: Limitations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: One of the key limitations of our study is the inherent randomness from the
    clustering algorithms and training framework. Due to computational constraints,
    we only performed three runs and averaged the results for each of our experiments.
    While this approach provides a general indication of performance, it may not fully
    capture the variability and could lead to less accurate conclusions. More extensive
    experimentation with a larger number of runs would be necessary to achieve a higher
    degree of confidence in the results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的一个关键限制是来自聚类算法和训练框架的固有随机性。由于计算限制，我们只进行了三次运行，并对每个实验的结果进行了平均。虽然这种方法提供了性能的一般指示，但可能无法完全捕捉变异性，可能导致结论不够准确。需要进行更多的实验，增加运行次数，以获得更高程度的结果置信度。
- en: Throughout our experiments, we closely follow the hyperparameters described
    in Wei et al. ([2023](#bib.bib5)), using a batch size of 512 samples and training
    for 2 epochs. However, such a high batch size results in only a few gradient updates
    when training on smaller datasets. Therefore, we switch to a lower batch size
    of 32, as recommended in Zhou et al. ([2023](#bib.bib9)), when our pruned dataset
    is less than 10% of the original size. We acknowledge that different hyperparameter
    settings could affect training outcomes and defer the determination of optimal
    hyperparameter settings for various training data sizes as future work.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们严格遵循了 Wei 等人（[2023](#bib.bib5)）描述的超参数设置，使用批量大小为 512 样本，训练 2 个周期。然而，这样高的批量大小在较小的数据集上训练时仅产生少量梯度更新。因此，当我们的修剪数据集少于原始大小的
    10% 时，我们会切换到更低的批量大小 32，正如 Zhou 等人（[2023](#bib.bib9)）所建议的。我们承认，不同的超参数设置可能会影响训练结果，因此将确定各种训练数据规模的最佳超参数设置作为未来工作。
- en: Potential Risks
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在风险
- en: This study focus exclusively on English prompts for Python code generation,
    thus prompts in other languages might not produce accurate or functional code.
    Additionally, the lack of safety alignment means there is a risk of generating
    malicious code or harmful language, which could lead to security vulnerabilities
    or unintended consequences. Code generation using LLMs carries inherent risks,
    such as producing incorrect or suboptimal code, failing to adhere to best practices,
    or introducing security flaws. Furthermore, LLMs may inadvertently propagate biases
    present in their training data, leading to biased outcomes in the generated code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究专注于 Python 代码生成的英文提示，因此其他语言的提示可能无法生成准确或有效的代码。此外，缺乏安全对齐意味着有生成恶意代码或有害语言的风险，这可能导致安全漏洞或意外后果。使用
    LLM 进行代码生成具有固有风险，例如生成不正确或次优的代码、未能遵循最佳实践或引入安全缺陷。此外，LLM 可能会不自觉地传播其训练数据中的偏见，从而导致生成的代码出现偏见结果。
- en: Use of AI Assistants
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AI 助手
- en: ChatGPT was utilized to refine paper writing and generate code templates for
    drawing figures. The authors took careful attention to ensure that AI-generated
    contents are accurate and align with the authors intentions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 被用来完善论文写作和生成绘图模板。作者仔细确保 AI 生成的内容准确并符合作者的意图。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] 乔什·阿奇亚姆、史蒂文·阿德勒、桑迪尼·阿加瓦尔、拉玛·艾哈迈德、伊尔盖·阿卡亚、弗洛伦西亚·莱奥尼·阿莱曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、萨姆·奥特曼、沙耶马尔·安纳德卡特
    等。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'Zhang et al. [2024] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat.
    When scaling meets llm finetuning: The effect of data, model and finetuning method.
    *arXiv preprint arXiv:2402.17193*, 2024.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] 张彪、刘忠涛、科林·切瑞、奥尔汉·菲拉特。当扩展遇到 LLM 微调：数据、模型和微调方法的影响。*arXiv 预印本 arXiv:2402.17193*，2024。
- en: 'Chaudhary [2023] Sahil Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhary [2023] Sahil Chaudhary。Code alpaca：一种用于代码生成的指令跟随 llama 模型。 [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)，2023。
- en: 'Luo et al. [2024] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering
    code large language models with evol-instruct. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=UnUwSIgK5W](https://openreview.net/forum?id=UnUwSIgK5W).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罗等人 [2024] 罗子阳、徐灿、赵普、孙青峰、耿秀博、胡文祥、陶崇阳、马晶、林庆伟、姜达鑫。Wizardcoder：通过 evol-instruct
    赋能大规模语言模型。发表于 *第十二届国际学习表征会议*，2024。网址 [https://openreview.net/forum?id=UnUwSIgK5W](https://openreview.net/forum?id=UnUwSIgK5W)。
- en: 'Wei et al. [2023] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2023] 韦玉翔、王哲、刘佳伟、丁一峰、张令明。Magicoder：源代码就是你所需要的一切。*arXiv 预印本 arXiv:2312.02120*，2023。
- en: 'Wang et al. [2022] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning
    language models with self-generated instructions. *arXiv preprint arXiv:2212.10560*,
    2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] 王一中、叶戈内赫·科尔迪、斯瓦罗普·米什拉、艾莉萨·刘、诺亚·A·史密斯、丹尼尔·卡沙比、哈娜赫·哈吉什尔齐。Self-instruct：通过自生成指令对齐语言模型。*arXiv
    预印本 arXiv:2212.10560*，2022。
- en: 'Xu et al. [2023a] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models
    to follow complex instructions. *arXiv preprint arXiv:2304.12244*, 2023a.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. [2023a] 可以 Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, 和 Daxin Jiang. Wizardlm: 使大型语言模型能够执行复杂指令。*arXiv 预印本 arXiv:2304.12244*，2023a。'
- en: Tsai et al. [2023a] Yun-Da Tsai, Tzu-Hsien Tsai, and Shou-De Lin. Differential
    good arm identification. *arXiv preprint arXiv:2303.07154*, 2023a.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai et al. [2023a] Yun-Da Tsai, Tzu-Hsien Tsai, 和 Shou-De Lin. 差分好臂识别。*arXiv
    预印本 arXiv:2303.07154*，2023a。
- en: 'Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment,
    2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, 和 Omer Levy. Lima: 少即是多的对齐，2023。'
- en: Fu et al. [2023] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong
    Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37,
    pages 12799–12807, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. [2023] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong
    Bing, 和 Nigel Collier. 关于参数高效微调的有效性。收录于 *AAAI 人工智能会议论文集*，第 37 卷，第 12799–12807
    页，2023。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021。'
- en: Su et al. [2022] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al.
    Selective annotation makes language models better few-shot learners. *arXiv preprint
    arXiv:2209.01975*, 2022.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. [2022] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, 等等。选择性标注使语言模型成为更好的少样本学习者。*arXiv
    预印本 arXiv:2209.01975*，2022。
- en: Diao et al. [2023] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active
    prompting with chain-of-thought for large language models. *arXiv preprint arXiv:2302.12246*,
    2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao et al. [2023] Shizhe Diao, Pengcheng Wang, Yong Lin, 和 Tong Zhang. 使用链式思维的主动提示针对大型语言模型。*arXiv
    预印本 arXiv:2302.12246*，2023。
- en: Tsai et al. [2024a] Yun-Da Tsai, Cayon Liow, Yin Sheng Siang, and Shou-De Lin.
    Toward more generalized malicious url detection models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 21628–21636, 2024a.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai et al. [2024a] Yun-Da Tsai, Cayon Liow, Yin Sheng Siang, 和 Shou-De Lin.
    朝着更通用的恶意 URL 检测模型前进。收录于 *AAAI 人工智能会议论文集*，第 38 卷，第 21628–21636 页，2024a。
- en: Penedo et al. [2024] Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, and
    Thomas Wolf. Fineweb, April 2024. URL [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo et al. [2024] Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, 和
    Thomas Wolf. Fineweb, 2024年4月。网址 [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)。
- en: Wang et al. [2023] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang,
    Stan Weixian Lei, and Mike Zheng Shou. Too large; data reduction for vision-language
    pre-training. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pages 3147–3157, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang,
    Stan Weixian Lei, 和 Mike Zheng Shou. 过大；用于视觉-语言预训练的数据减少。收录于 *IEEE/CVF 国际计算机视觉大会论文集*，第
    3147–3157 页，2023。
- en: Tsai and Lin [2024] Yun-Da Tsai and Shou-De Lin. Handling concept drift in non-stationary
    bandit through predicting future rewards. In *Pacific-Asia Conference on Knowledge
    Discovery and Data Mining*, pages 161–173\. Springer, 2024.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai and Lin [2024] Yun-Da Tsai 和 Shou-De Lin. 通过预测未来奖励处理非平稳赌博问题中的概念漂移。收录于 *太平洋-亚洲知识发现与数据挖掘会议*，第
    161–173 页。Springer，2024。
- en: 'Xia et al. [2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev
    Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction
    tuning. *arXiv preprint arXiv:2402.04333*, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. [2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev
    Arora, 和 Danqi Chen. Less: 为定向指令调整选择有影响的数据。*arXiv 预印本 arXiv:2402.04333*，2024。'
- en: Pruthi et al. [2020] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan.
    Estimating training data influence by tracing gradient descent. *Advances in Neural
    Information Processing Systems*, 33:19920–19930, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pruthi et al. [2020] Garima Pruthi, Frederick Liu, Satyen Kale, 和 Mukund Sundararajan.
    通过追踪梯度下降估计训练数据的影响。*神经信息处理系统进展*，33:19920–19930，2020。
- en: Schoch et al. [2023] Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data
    selection for fine-tuning large language models using transferred shapley values.
    *arXiv preprint arXiv:2306.10165*, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schoch et al. [2023] Stephanie Schoch, Ritwick Mishra, 和 Yangfeng Ji。使用转移的 Shapley
    值进行大语言模型的微调数据选择。*arXiv 预印本 arXiv:2306.10165*，2023年。
- en: 'Das and Khetan [2023] Devleena Das and Vivek Khetan. Deft: Data efficient fine-tuning
    for large language models via unsupervised core-set selection. *arXiv preprint
    arXiv:2310.16776*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Das and Khetan [2023] Devleena Das 和 Vivek Khetan。Deft: 通过无监督核心集选择进行的大语言模型数据高效微调。*arXiv
    预印本 arXiv:2310.16776*，2023年。'
- en: 'Chen et al. [2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia
    Jin. Alpagasus: Training a better alpaca with fewer data, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, 和 Hongxia
    Jin。Alpagasus: 用更少的数据训练更好的 alpaca，2024年。'
- en: Moser et al. [2024] Brian B Moser, Federico Raue, and Andreas Dengel. A study
    in dataset pruning for image super-resolution. *arXiv preprint arXiv:2403.17083*,
    2024.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moser et al. [2024] Brian B Moser, Federico Raue, 和 Andreas Dengel。图像超分辨率的数据集剪枝研究。*arXiv
    预印本 arXiv:2403.17083*，2024年。
- en: Meding et al. [2021] Kristof Meding, Luca M Schulze Buschoff, Robert Geirhos,
    and Felix A Wichmann. Trivial or impossible–dichotomous data difficulty masks
    model differences (on imagenet and beyond). *arXiv preprint arXiv:2110.05922*,
    2021.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meding et al. [2021] Kristof Meding, Luca M Schulze Buschoff, Robert Geirhos,
    和 Felix A Wichmann。琐碎还是不可能—二分数据难度掩盖模型差异（在 imagenet 及其他领域）。*arXiv 预印本 arXiv:2110.05922*，2021年。
- en: Da Tsai and De Lin [2022] Yun Da Tsai and Shou De Lin. Fast online inference
    for nonlinear contextual bandit based on generative adversarial network. *arXiv
    preprint arXiv:2202.08867*, 2022.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Da Tsai and De Lin [2022] Yun Da Tsai 和 Shou De Lin。基于生成对抗网络的非线性上下文赌博机的快速在线推断。*arXiv
    预印本 arXiv:2202.08867*，2022年。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等人。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等人。Code llama: 用于代码的开放基础模型。*arXiv 预印本 arXiv:2308.12950*，2023年。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人。Llama 2: 开放的基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。'
- en: Tsai et al. [2021] Yun-Da Tsai, ChengKuan Chen, and Shou-De Lin. Toward an effective
    black-box adversarial attack on functional javascript malware against commercial
    anti-virus. In *Proceedings of the 30th ACM International Conference on Information
    & Knowledge Management*, pages 4165–4172, 2021.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai et al. [2021] Yun-Da Tsai, ChengKuan Chen, 和 Shou-De Lin。针对商业杀毒软件的功能性 JavaScript
    恶意软件的有效黑箱对抗攻击。发表于*第30届ACM国际信息与知识管理会议论文集*，第4165–4172页，2021年。
- en: 'Lozhkov et al. [2024] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. *arXiv
    preprint arXiv:2402.19173*, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lozhkov et al. [2024] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, 等人。Starcoder 2 和 Stack v2: 下一代。*arXiv 预印本 arXiv:2402.19173*，2024年。'
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le。微调的语言模型是零-shot 学习者。*arXiv
    预印本 arXiv:2109.01652*，2021年。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *Journal of Machine Learning Research*,
    25(70):1–53, 2024.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人 [2024] 郑云轲、侯乐、肖恩·朗普雷、巴雷特·佐普、易泰、威廉·费德斯、李云轩、王雪智、穆斯塔法·德赫赫尼、悉达多·布拉赫马等。扩展指令微调语言模型。*机器学习研究期刊*，25(70):1–53，2024。
- en: Peng et al. [2023] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*,
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等人 [2023] 包林·彭、李春元、何鹏程、米歇尔·加利和高剑锋。与 gpt-4 的指令调优。*arXiv 预印本 arXiv:2304.03277*，2023。
- en: 'Xu et al. [2023b] Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan
    Wang, Bin Gu, and Neel Sundaresan. Rethinking the instruction quality: Lift is
    what you need, 2023b.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等人 [2023b] 许洋、姚永强、黄宇凡、齐梦南、王茂泉、顾斌和尼尔·孙达雷桑。重新审视指令质量：提升是你所需要的，2023b。
- en: Liu et al. [2023a] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian
    He. What makes good data for alignment? a comprehensive study of automatic data
    selection in instruction tuning. *arXiv preprint arXiv:2312.15685*, 2023a.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023a] 刘伟、曾伟豪、何克卿、姜勇和何俊贤。什么样的数据对对齐有帮助？指令调优中自动数据选择的全面研究。*arXiv 预印本 arXiv:2312.15685*，2023a。
- en: Tsai et al. [2024b] Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, and Shou-De
    Lin. Text-centric alignment for multi-modality learning. *arXiv preprint arXiv:2402.08086*,
    2024b.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等人 [2024b] 云达·蔡、丁宇·闫、裴福·郭、哲彦·李和守德·林。面向文本的多模态学习对齐。*arXiv 预印本 arXiv:2402.08086*，2024b。
- en: 'Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling
    via data pruning. *Advances in Neural Information Processing Systems*, 35:19523–19536,
    2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索尔舍等人 [2022] 本·索尔舍、罗伯特·盖尔霍斯、沙尚克·谢赫、苏里亚·甘古利和阿里·莫尔科斯。超越神经缩放定律：通过数据剪枝击败幂律缩放。*神经信息处理系统进展*，35:19523–19536，2022。
- en: 'Li et al. [2023] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen,
    Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality:
    Boosting llm performance with self-guided data selection for instruction tuning.
    *arXiv preprint arXiv:2308.12032*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023] 李明、张永、李志涛、陈久海、陈立昌、程宁、王剑宗、周天逸和肖静。由量到质：通过自我引导数据选择提升 llm 性能以进行指令调优。*arXiv
    预印本 arXiv:2308.12032*，2023。
- en: 'Li et al. [2024] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong
    Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering
    for fast instruction-tuning. *arXiv preprint arXiv:2402.00530*, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2024] 李明、张永、施怀·何、李志涛、赵洪宇、王剑宗、程宁和周天逸。超级过滤：从弱到强的数据过滤以加快指令调优。*arXiv 预印本 arXiv:2402.00530*，2024。
- en: Chen et al. [2018] Chen Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo,
    and Wenqian Liu. A systematic review of fuzzing techniques. *Computers & Security*,
    75:118–137, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2018] 陈陈、蔡宝江、马锦鑫、吴润普、郭建超和刘文倩。模糊测试技术的系统性综述。*计算机与安全*，75:118–137，2018。
- en: Naik [2024] Atharva Naik. On the limitations of embedding based methods for
    measuring functional correctness for code generation. *arXiv preprint arXiv:2405.01580*,
    2024.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奈克 [2024] 阿特瓦尔·奈克。关于基于嵌入的方法在测量代码生成功能正确性方面的局限性。*arXiv 预印本 arXiv:2405.01580*，2024。
- en: Maćkiewicz and Ratajczak [1993] Andrzej Maćkiewicz and Waldemar Ratajczak. Principal
    components analysis (pca). *Computers & Geosciences*, 19(3):303–342, 1993.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马切维奇和拉塔伊查克 [1993] 安德烈·马切维奇和瓦尔德马尔·拉塔伊查克。主成分分析 (pca)。*计算机与地球科学*，19(3):303–342，1993。
- en: 'Tsai et al. [2023b] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically
    fixing rtl syntax errors with large language models. *arXiv preprint arXiv:2311.16543*,
    2023b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等人 [2023b] 云达·蔡、刘明杰和任浩星。Rtlfixer：利用大型语言模型自动修复 rtl 语法错误。*arXiv 预印本 arXiv:2311.16543*，2023b。
- en: 'Kanungo et al. [2002] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D
    Piatko, Ruth Silverman, and Angela Y Wu. An efficient k-means clustering algorithm:
    Analysis and implementation. *IEEE transactions on pattern analysis and machine
    intelligence*, 24(7):881–892, 2002.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡农戈等人 [2002] 塔帕斯·卡农戈、大卫·M·蒙特、内森·S·内塔尼亚胡、克里斯汀·D·皮亚特科、鲁思·西尔弗曼和安吉拉·Y·吴。高效的 k-means
    聚类算法：分析与实现。*IEEE 模式分析与机器智能交易*，24(7):881–892，2002。
- en: Müllner [2011] Daniel Müllner. Modern hierarchical, agglomerative clustering
    algorithms. *arXiv preprint arXiv:1109.2378*, 2011.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穆尔纳 [2011] 丹尼尔·穆尔纳。现代层次聚类算法。*arXiv 预印本 arXiv:1109.2378*，2011。
- en: 'Rahman et al. [2016] Md Farhadur Rahman, Weimo Liu, Saad Bin Suhaim, Saravanan
    Thirumuruganathan, Nan Zhang, and Gautam Das. Hdbscan: Density based clustering
    over location based services. *arXiv preprint arXiv:1602.03730*, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman 等人 [2016] Md Farhadur Rahman, Weimo Liu, Saad Bin Suhaim, Saravanan Thirumuruganathan,
    Nan Zhang 和 Gautam Das。Hdbscan：基于密度的定位服务聚类。*arXiv 预印本 arXiv:1602.03730*，2016 年。
- en: 'Guo et al. [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. Deepseek-coder: When the large language model meets programming – the rise
    of code intelligence, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong 和 Wenfeng
    Liang。Deepseek-coder：当大型语言模型遇上编程——代码智能的崛起，2024 年。
- en: Liu et al. [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of
    large language models for code generation. In *Thirty-seventh Conference on Neural
    Information Processing Systems*, 2023b. URL [https://openreview.net/forum?id=1qvx610Cu7](https://openreview.net/forum?id=1qvx610Cu7).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang 和 Lingming Zhang。你的代码是由
    chatGPT 生成的是真的吗？对大型语言模型在代码生成中的严格评估。见于 *第 37 届神经信息处理系统大会*，2023b。网址 [https://openreview.net/forum?id=1qvx610Cu7](https://openreview.net/forum?id=1qvx610Cu7)。
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2014] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014
    年。
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等人。评估基于代码训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021 年。
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等人 [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
    Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le 等人。使用大型语言模型的程序合成。*arXiv
    预印本 arXiv:2108.07732*，2021 年。
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang 和 Ion Stoica。用于大型语言模型服务的高效内存管理与分页注意力。见于
    *ACM SIGOPS 第 29 届操作系统原理研讨会论文集*，2023 年。
- en: 'Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
    B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
    A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
    Machine learning in Python. *Journal of Machine Learning Research*, 12:2825–2830,
    2011.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedregosa 等人 [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
    O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A.
    Passos, D. Cournapeau, M. Brucher, M. Perrot 和 E. Duchesnay。Scikit-learn：Python
    中的机器学习。*机器学习研究期刊*，12:2825–2830，2011 年。
- en: Roy [1953] Samarendra Nath Roy. On a heuristic method of test construction and
    its use in multivariate analysis. *The Annals of Mathematical Statistics*, 24(2):220–238,
    1953.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy [1953] Samarendra Nath Roy。关于测试构造的启发式方法及其在多变量分析中的应用。*数学统计年鉴*，24(2):220–238，1953
    年。
- en: 'Scott [2010] David W Scott. Scott’s rule. *Wiley Interdisciplinary Reviews:
    Computational Statistics*, 2(4):497–502, 2010.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scott [2010] David W Scott。Scott’s rule。*Wiley 交叉学科评论：计算统计*，2(4):497–502，2010
    年。
- en: Appendix A Code Samples from Data Pruning
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据修剪中的代码示例
- en: We show examples from our data pruning. The selected data samples adheres closely
    to the downstream coding tasks, from English problem description to code generation.
    We also provide an example of removed data from our pruning strategy.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了来自数据修剪的示例。所选数据样本与下游编码任务紧密相关，从英文问题描述到代码生成。我们还提供了修剪策略中删除的数据示例。
- en: 'Input You
    are given a Python function that takes in a list of integers and a target integer.
    The function is intended to find two distinct elements in the list that sum up
    to the target integer. However, the function is incomplete and needs to be filled
    in with the appropriate code to achieve this functionality. Complete the Python
    function ‘find_two_sum‘ to find and return the indices of two distinct elements
    in the input list that sum up to the target integer. If no such pair exists, return
    an empty list. The function signature is: def  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]:
    For example, given the input ‘nums = [2, 7, 11, 15]’ and ‘target = 9’, the function
    should return ‘[0, 1]‘ since ‘nums[0] + nums[1] = 2 + 7 = 9’. Output from  typing  import  List
    \pardef  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]: num_indices  =  {}
    for  i,  num  in  enumerate(nums): complement  =  target  -  num if  complement  in  num_indices:
    return  [num_indices[complement],  i] num_indices[num]  =  i return  []'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input You
    are given a Python function that takes in a list of integers and a target integer.
    The function is intended to find two distinct elements in the list that sum up
    to the target integer. However, the function is incomplete and needs to be filled
    in with the appropriate code to achieve this functionality. Complete the Python
    function ‘find_two_sum‘ to find and return the indices of two distinct elements
    in the input list that sum up to the target integer. If no such pair exists, return
    an empty list. The function signature is: def  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]:
    For example, given the input ‘nums = [2, 7, 11, 15]’ and ‘target = 9’, the function
    should return ‘[0, 1]‘ since ‘nums[0] + nums[1] = 2 + 7 = 9’. Output from  typing  import  List
    \pardef  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]: num_indices  =  {}
    for  i,  num  in  enumerate(nums): complement  =  target  -  num if  complement  in  num_indices:
    return  [num_indices[complement],  i] num_indices[num]  =  i return  []'
- en: 'Figure 6: Example of a selected data sample.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：选定数据样本的示例。
- en: 'Input You
    are given a list of integers representing the daily stock prices of a company.
    You need to write a function to calculate the maximum profit that can be obtained
    by buying and selling the stock at most once. If it is not possible to make any
    profit, return 0. Write a function ‘maxProfit’ that takes in a list of integers
    ‘prices‘ as input and returns the maximum profit that can be obtained. Function
    signature: def  maxProfit(prices:  List[int])  ->  int: Example: Input:  prices  =  [7,  1,  5,  3,  6,  4]
    Output:  5 Explanation:  Buy  on  day  2  (price  =  1)  and  sell  on  day  5  (price  =  6),  profit  =  6  -  1  =  5.
    \parInput:  prices  =  [7,  6,  4,  3,  1] Output:  0 Explanation:  In  this  case,  no  transaction  is  done,  i.e.,  max  profit  =  0.
    Assumptions: - The input list ‘prices’ will have at least one element. - The stock
    prices are given in chronological order. Output from  typing  import  List \pardef  maxProfit(prices:  List[int])  ->  int:
    if  not  prices: return  0 \parmin_price  =  prices[0] max_profit  =  0 \parfor  price  in  prices:
    if  price  <  min_price: min_price  =  price else: max_profit  =  max(max_profit,  price  -  min_price)
    \parreturn  max_profit The ‘maxProfit’ function iterates through the list of prices,
    keeping track of the minimum price encountered so far and the maximum profit that
    can be obtained. It updates these values as it iterates through the list and returns
    the maximum profit at the end.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input You
    are given a list of integers representing the daily stock prices of a company.
    You need to write a function to calculate the maximum profit that can be obtained
    by buying and selling the stock at most once. If it is not possible to make any
    profit, return 0. Write a function ‘maxProfit’ that takes in a list of integers
    ‘prices‘ as input and returns the maximum profit that can be obtained. Function
    signature: def  maxProfit(prices:  List[int])  ->  int: Example: Input:  prices  =  [7,  1,  5,  3,  6,  4]
    Output:  5 Explanation:  Buy  on  day  2  (price  =  1)  and  sell  on  day  5  (price  =  6),  profit  =  6  -  1  =  5.
    \parInput:  prices  =  [7,  6,  4,  3,  1] Output:  0 Explanation:  In  this  case,  no  transaction  is  done,  i.e.,  max  profit  =  0.
    Assumptions: - The input list ‘prices’ will have at least one element. - The stock
    prices are given in chronological order. Output from  typing  import  List \pardef  maxProfit(prices:  List[int])  ->  int:
    if  not  prices: return  0 \parmin_price  =  prices[0] max_profit  =  0 \parfor  price  in  prices:
    if  price  <  min_price: min_price  =  price else: max_profit  =  max(max_profit,  price  -  min_price)
    \parreturn  max_profit The ‘maxProfit’ function iterates through the list of prices,
    keeping track of the minimum price encountered so far and the maximum profit that
    can be obtained. It updates these values as it iterates through the list and returns
    the maximum profit at the end.'
- en: 'Figure 7: Another example of a selected data sample.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：另一个选定数据样本的示例。
- en: 'Input Background:
    You  are  a  Product  Manager,  named  Alice,  your  goal  is  Efficiently  create  a  successful  product,  and  the  constraint  is  .
    Question: \par#  Context ##  Original  Requirements [BOSS:  Write  a  podcast  search  engine  which  search  for  all  index  podcast.  Each  podcast  belongs  to  a  channel  and  each  podcast  contain  title,  description,  published,  audio  link,  transcript  text]
    \par##  Search  Information ###  Search  Results ###  Search  Summary \par##  mermaid  quadrantChart  code  syntax  example.  DONT  USE  QUOTO  IN  CODE  DUE  TO  INVALID  SYNTAX.  Replace  the    with  REAL  COMPETITOR  NAME
    \parquadrantChart title  Reach  and  engagement  of  campaigns x-axis  Low  Reach  –>  High  Reach
    y-axis  Low  Engagement  –>  High  Engagement … Output ##  Original  Requirements
    We  are  tasked  with  creating  a  podcast  search  engine  that  indexes  all  podcasts.  Each  podcast  is  associated  with  a  channel  and  contains  a  title,  description,  publication  date,  audio  link,  and  transcript  text.
    \par##  Product  Goals “‘python [ "Create  a  user-friendly  podcast  search  engine  that  indexes  all  podcasts",
    "Ensure  each  podcast  contains  necessary  information:  title,  description,  publication  date,  audio  link,  and  transcript  text",
    "Develop  a  system  that  efficiently  updates  and  maintains  the  podcast  index"
    ] “‘ \par##  User  Stories “‘python [ "As  a  user,  I  want  to  search  for  podcasts  by  title  or  description  so  that  I  can  find  the  content  I’m  interested  in",
    "As  a  user,  I  want  to  see  all  the  necessary  information  about  a  podcast  (title,  description,  publication  date,  audio  link,  transcript  text)  so  that  I  can  decide  whether  to  listen  to  it",
    "As  a  user,  I  want  the  search  engine  to  suggest  podcasts  based  on  my  search  history  so  that  I  can  discover  new  content",
    "As  a  user,  I  want  the  podcast  search  engine  to  be  updated  regularly  so  that  I  can  access  the  latest  podcasts",
    "As  a  user,  I  want  to  filter  search  results  by  publication  date,  channel,  or  transcript  text  so  that  I  can  narrow  down  my  search"
    ] “‘ \par##  Competitive  Analysis …'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input Background:
    You  are  a  Product  Manager,  named  Alice,  your  goal  is  Efficiently  create  a  successful  product,  and  the  constraint  is  .
    Question: \par#  Context ##  Original  Requirements [BOSS:  Write  a  podcast  search  engine  which  search  for  all  index  podcast.  Each  podcast  belongs  to  a  channel  and  each  podcast  contain  title,  description,  published,  audio  link,  transcript  text]
    \par##  Search  Information ###  Search  Results ###  Search  Summary \par##  mermaid  quadrantChart  code  syntax  example.  DONT  USE  QUOTO  IN  CODE  DUE  TO  INVALID  SYNTAX.  Replace  the    with  REAL  COMPETITOR  NAME
    \parquadrantChart title  Reach  and  engagement  of  campaigns x-axis  Low  Reach  –>  High  Reach
    y-axis  Low  Engagement  –>  High  Engagement … Output ##  Original  Requirements
    We  are  tasked  with  creating  a  podcast  search  engine  that  indexes  all  podcasts.  Each  podcast  is  associated  with  a  channel  and  contains  a  title,  description,  publication  date,  audio  link,  and  transcript  text.
    \par##  Product  Goals “‘python [ "Create  a  user-friendly  podcast  search  engine  that  indexes  all  podcasts",
    "Ensure  each  podcast  contains  necessary  information:  title,  description,  publication  date,  audio  link,  and  transcript  text",
    "Develop  a  system  that  efficiently  updates  and  maintains  the  podcast  index"
    ] “‘ \par##  User  Stories “‘python [ "As  a  user,  I  want  to  search  for  podcasts  by  title  or  description  so  that  I  can  find  the  content  I’m  interested  in",
    "As  a  user,  I  want  to  see  all  the  necessary  information  about  a  podcast  (title,  description,  publication  date,  audio  link,  transcript  text)  so  that  I  can  decide  whether  to  listen  to  it",
    "As  a  user,  I  want  the  search  engine  to  suggest  podcasts  based  on  my  search  history  so  that  I  can  discover  new  content",
    "As  a  user,  I  want  the  podcast  search  engine  to  be  updated  regularly  so  that  I  can  access  the  latest  podcasts",
    "As  a  user,  I  want  to  filter  search  results  by  publication  date,  channel,  or  transcript  text  so  that  I  can  narrow  down  my  search"
    ] “‘ \par##  Competitive  Analysis …'
- en: 'Figure 8: Example of a removed data sample (outlier).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：删除的数据样本示例（异常值）。
