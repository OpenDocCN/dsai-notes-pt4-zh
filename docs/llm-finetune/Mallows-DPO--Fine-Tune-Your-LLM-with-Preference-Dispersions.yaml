- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:36:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mallows-DPO：通过偏好离散化微调您的LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14953](https://ar5iv.labs.arxiv.org/html/2405.14953)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14953](https://ar5iv.labs.arxiv.org/html/2405.14953)
- en: Haoxian Chen^∗ ,  Hanyang Zhao^∗ ,  Henry Lam ,  David D. Yao  and  Wenpin Tang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Haoxian Chen^∗ ，Hanyang Zhao^∗ ，Henry Lam ，David D. Yao 和 Wenpin Tang
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Direct Preference Optimization (DPO) has recently emerged as a popular approach
    to improve reinforcement learning with human feedback (RLHF), leading to better
    techniques to fine-tune large language models (LLM). A weakness of DPO, however,
    lies in its lack of capability to characterize the diversity of human preferences.
    Inspired by Mallows’ theory of preference ranking, we develop in this paper a
    new approach, the Mallows-DPO. A distinct feature of this approach is a dispersion
    index, which reflects the dispersion of human preference to prompts. We show that
    existing DPO models can be reduced to special cases of this dispersion index,
    thus unified with Mallows-DPO. More importantly, we demonstrate (empirically)
    how to use this dispersion index to enhance the performance of DPO in a broad
    array of benchmark tasks, from synthetic bandit selection to controllable generations
    and dialogues, while maintaining great generalization capabilities.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化（DPO）最近作为一种流行的方法出现，用于改进具有人工反馈的强化学习（RLHF），从而改进大型语言模型（LLM）的微调技术。然而，DPO的一个弱点在于其缺乏描述人类偏好多样性的能力。受Mallows偏好排名理论的启发，我们在本文中提出了一种新的方法，即Mallows-DPO。该方法的一个显著特征是离散指数，它反映了对提示的人类偏好的离散程度。我们展示了现有的DPO模型可以简化为这种离散指数的特殊情况，从而与Mallows-DPO统一。更重要的是，我们通过（经验）演示了如何利用这种离散指数来增强DPO在从合成赌博选择到可控生成和对话等广泛基准任务中的表现，同时保持出色的泛化能力。
- en: '^∗Equal Contribution. Emails: {hc3136,hz2684,khl2114,ddy1,wt2319}@columbia.edu,
    Department of Industrial Engineering and Operations Research, Columbia University.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗平等贡献。电子邮件：{hc3136,hz2684,khl2114,ddy1,wt2319}@columbia.edu，哥伦比亚大学工业工程与运筹学系。
- en: 'Key words: Dispersion, direct preference optimization, fine-tuning, large language
    models (LLMs), Mallows models, reinforcement learning with human feedback (RLHF).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：离散度、直接偏好优化、微调、大型语言模型（LLMs）、Mallows模型、具有人工反馈的强化学习（RLHF）。
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Reinforcement Learning with Human Feeback (RLHF, [[28](#bib.bib28), [33](#bib.bib33),
    [44](#bib.bib44)]) has made significant contributions to the success of modern
    Large Language Models (LLMs) such as ChatGPT and GPT4 [[1](#bib.bib1)]. More recently,
    Direct Preference Optimization (DPO) [[30](#bib.bib30)] solves essentially the
    same problem as RLHF, but bypasses the training of the reward model, and thus
    leading to faster speed and better resource efficiency. More importantly, DPO
    also achieves comparable or superior performance against RLHF in downstream tasks
    such as fine-tuning LLMs in Llama3, Zephyr [[37](#bib.bib37)], Neural Chat, BTLM-DPO
    [[18](#bib.bib18)], etc. DPO’s success has attracted much research attention,
    leading to variants beyond pairwise ranking [[32](#bib.bib32)], and unified perspectives
    on loss parameterization [[2](#bib.bib2), [15](#bib.bib15), [36](#bib.bib36),
    [40](#bib.bib40)], etc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工反馈强化学习（RLHF，[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]）对现代大型语言模型（LLMs）如ChatGPT和GPT4的成功做出了重要贡献[[1](#bib.bib1)]。最近，直接偏好优化（DPO）[[30](#bib.bib30)]
    实质上解决了与RLHF相同的问题，但绕过了奖励模型的训练，因此速度更快，资源效率更高。更重要的是，DPO在下游任务中也实现了与RLHF相当或更优的表现，如在Llama3、Zephyr
    [[37](#bib.bib37)]、Neural Chat、BTLM-DPO [[18](#bib.bib18)]等中对LLMs进行微调。DPO的成功吸引了大量研究关注，导致了超越成对排名的变体[[32](#bib.bib32)]，以及对损失参数化的统一观点[[2](#bib.bib2),
    [15](#bib.bib15), [36](#bib.bib36), [40](#bib.bib40)]等。
- en: 'Notwithstanding the successes achieved by RLHF and DPO, both are limited by
    the restrictive assumption that the underlying preference follows the Bradley-Terry
    (BT) model [[5](#bib.bib5)]. In particular, the degree of possible agreement or
    disagreement in response to different prompts is not accounted for in the objective
    function. For instance, people are more likely to agree on “$1+1=$.” as opposed
    to “What is the best city to live in the U.S.? // New York.” In the context of
    language models, this concerns the issue of dispersion of the next-token prediction,
    which was also observed in [[9](#bib.bib9), [16](#bib.bib16)]. See Section [5.1](#S5.SS1
    "5.1\. Evidence of preference dispersion ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions") for dispersion analysis of IMDB and Anthropic
    HH datasets.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管RLHF和DPO都取得了一定的成功，但两者都受到限制性假设的制约，即假设基础偏好遵循Bradley-Terry (BT)模型[[5](#bib.bib5)]。特别是，目标函数中没有考虑对不同提示的可能一致性或不一致性的程度。例如，人们更容易在“$1+1=$”这一问题上达成一致，而不是在“美国最好的城市是哪里？
    // 纽约。”这个问题上。在语言模型的背景下，这涉及到下一个标记预测的分散问题，这也在[[9](#bib.bib9), [16](#bib.bib16)]中被观察到。有关IMDB和Anthropic
    HH数据集的分散分析，请参见[5.1](#S5.SS1 "5.1\. Evidence of preference dispersion ‣ 5\. Experiments
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")。'
- en: 'The purpose of this paper is to formalize the idea of prompt dispersion in
    the design of DPO. We adapt Mallows’ preference ranking theory [[12](#bib.bib12),
    [24](#bib.bib24)], a family of ranking models that provide a natural carrier for
    prompt dispersion, and propose the following decomposition/factorization of the
    (latent) reward function:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是在DPO设计中形式化提示分散的思想。我们采用了Mallows的偏好排序理论[[12](#bib.bib12), [24](#bib.bib24)]，这是一个自然适用于提示分散的排序模型家族，并提出了(潜在)奖励函数的以下分解/因式分解：
- en: '|  | $\mbox{reward}(\mbox{prompt, completion})=\mbox{dispersion}(\mbox{prompt})\times\mbox{scaled
    reward}(\mbox{completion}\mid\mbox{prompt}),$ |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mbox{reward}(\mbox{prompt, completion})=\mbox{dispersion}(\mbox{prompt})\times\mbox{scaled
    reward}(\mbox{completion}\mid\mbox{prompt}),$ |  |'
- en: where “prompt” and “completion” correspond, respectively, to question and answer.
    This decomposition allows to specify the diverse level of prompt dispersions hidden
    in the DPO, which is translated into a prompt-dependent factor – the dispersion
    index in the preference likelihood. The scaled reward is given by the relative
    rank of the (possible) completions, which further enhances the model interpretability.
    We then leverage the change of variables technique to propose two models, Mallows-$\theta$-DPO.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“prompt”和“completion”分别对应问题和答案。这种分解允许指定DPO中隐藏的提示分散的多样化水平，这转化为一个提示相关的因子——偏好似然中的分散指数。缩放奖励由（可能的）完成的相对排名给出，这进一步增强了模型的可解释性。然后，我们利用变量变化技术提出了两个模型，即Mallows-$\theta$-DPO。
- en: The main contributions of this paper are as follows. First, we formalize the
    idea of prompt dispersion in DPO, and develop the Mallows-DPO approach to implement
    this idea, so as to improve and generalize DPO. Second, we propose approximations
    to the dispersion index so as to facilitate computation. We also provide various
    analytical results for Mallows-DPO, which lead to various new insights on existing
    DPO models, including a generalized $\Psi$PO model that unifies all DPO models
    (including Mallows-DPO). Finally, our experiments on bandit, IMDB and Anthropic
    HH dataset all show clear advantages of Mallows-DPO over (BT-)DPO.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献如下。首先，我们在DPO中形式化了提示分散的思想，并开发了Mallows-DPO方法来实现这一思想，从而改进和推广DPO。其次，我们提出了对分散指数的近似，以便于计算。我们还提供了Mallows-DPO的各种分析结果，这些结果为现有的DPO模型带来了新的见解，包括一个统一所有DPO模型（包括Mallows-DPO）的广义$\Psi$PO模型。最后，我们在bandit、IMDB和Anthropic
    HH数据集上的实验均显示出Mallows-DPO相对于(BT-)DPO的明显优势。
- en: Other Related Works. Existing literature studying personalization in dialogue
    generation such as [[16](#bib.bib16), [21](#bib.bib21)] have also paid attention
    to the diversity of human preferences (“there are a thousand Hamlets in a thousand
    people’s eyes” [[16](#bib.bib16)]); and [[27](#bib.bib27)] proposes a Nash game
    model to incorporate this diversity. There are also recent works that propose
    learning the online preferences [[8](#bib.bib8), [34](#bib.bib34)] or learning
    from AI feedbacks [[4](#bib.bib4), [10](#bib.bib10), [20](#bib.bib20)]. Studies
    to improve the design and capabilities of RLHF include [[14](#bib.bib14), [19](#bib.bib19),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其他相关工作。现有文献研究了对话生成中的个性化，如 [[16](#bib.bib16), [21](#bib.bib21)] 也关注人类偏好的多样性（“千人眼中有千个哈姆雷特”
    [[16](#bib.bib16)]）；而 [[27](#bib.bib27)] 提出了一个纳什博弈模型来融合这种多样性。也有最近的研究提出了学习在线偏好
    [[8](#bib.bib8), [34](#bib.bib34)] 或者从 AI 反馈中学习 [[4](#bib.bib4), [10](#bib.bib10),
    [20](#bib.bib20)]。提升 RLHF 设计和能力的研究包括 [[14](#bib.bib14), [19](#bib.bib19), [38](#bib.bib38),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]。
- en: 'The remainder of the paper is organized as follows. Background materials on
    RLHF and DPO are highlighted in Section [2](#S2 "2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"). Section [3](#S3 "3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    focuses on the development of Mallows-DPO, followed by more analytical results
    and various perspectives in Section [4](#S4 "4\. Perspectives on Mallows-DPO ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"). Experimental results
    are detailed in Section [5](#S5 "5\. Experiments ‣ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions"), and concluding remarks in Section [6](#S6 "6\.
    Conclusion ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下。第[2](#S2 "2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM with
    Preference Dispersions")节重点介绍了 RLHF 和 DPO 的背景材料。[3](#S3 "3\. DPO based on Mallows
    Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")节关注于
    Mallows-DPO 的发展，接下来的[4](#S4 "4\. Perspectives on Mallows-DPO ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")节提供了更多的分析结果和各种视角。实验结果详见[5](#S5 "5\. Experiments
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")节，结论则在[6](#S6 "6\.
    Conclusion ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")节中给出。'
- en: 2\. Preliminaries
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基础知识
- en: 2.1\. Supervised fine-tuning (SFT)
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 监督微调（SFT）
- en: Both RLHF and DPO (reviewed below) start with fine-tuning a pre-trained large
    language model by supervised learning on high-quality data for some downstream
    tasks of interest (e.g. dialogue, summarization, etc.), to acquire a model $\pi^{\mathrm{SFT}}$.
    This step is referred to as the SFT phase. For instance, for training InstructGPT
    [[28](#bib.bib28)], GPT-3 [[6](#bib.bib6)] is first fine-tuned on the given input
    prompt distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 和 DPO（下文将回顾）都从通过监督学习在高质量数据上微调预训练的大型语言模型开始，以处理一些感兴趣的下游任务（如对话、总结等），以获得一个模型
    $\pi^{\mathrm{SFT}}$。这个步骤称为 SFT 阶段。例如，在训练 InstructGPT [[28](#bib.bib28)] 时，首先对
    GPT-3 [[6](#bib.bib6)] 进行针对给定输入提示分布的微调。
- en: 2.2\. RLHF
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. RLHF
- en: '[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]. On top of $\pi^{\mathrm{SFT}}$
    to produce pairs of answers (or, “completions”), $\left\{y_{1},y_{2}\right\}\sim\pi(y\mid
    x)$. The preferences are assumed to be generated by some latent reward model $r^{*}(x,y)$
    in which the prompt $x$ is the action.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[[28](#bib.bib28), [33](#bib.bib33), [44](#bib.bib44)]。在 $\pi^{\mathrm{SFT}}$
    的基础上生成答案对（或“完成”），$\left\{y_{1},y_{2}\right\}\sim\pi(y\mid x)$。假设这些偏好由某个潜在的奖励模型
    $r^{*}(x,y)$ 生成，其中提示 $x$ 是行动。'
- en: '(a) Reward model. To capture the underlying human preferences, RLHF assumes
    the Bradley-Terry model [[5](#bib.bib5)] that stipulates the pairwise preference
    distribution:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 奖励模型。为了捕捉潜在的人类偏好，RLHF 假设使用 Bradley-Terry 模型 [[5](#bib.bib5)]，该模型规定了成对偏好的分布：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\sigma(s):=\frac{1}{1+e^{-s}}$ is the sigmoid function. Given access
    to a static dataset of comparisons
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma(s):=\frac{1}{1+e^{-s}}$ 是 sigmoid 函数。给定对静态比较数据集的访问
- en: '|  | $\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1,\ldots,N},$
    |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1,\ldots,N},$
    |  |'
- en: 'RLHF seeks to approximate the latent reward $r^{*}(x,y)$, and estimate the
    parameters by minimizing the (negative) log-likelihood loss:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 旨在近似潜在奖励 $r^{*}(x,y)$，并通过最小化（负的）对数似然损失来估计参数：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Denote by $r_{\psi_{*}}(x,y)$ the solution to the problem in ([2](#S2.E2 "In
    2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 记 $r_{\psi_{*}}(x,y)$ 为 ([2](#S2.E2 "在 2.2\. RLHF ‣ 2\. 初步概念 ‣ Mallows-DPO：通过偏好分散来微调你的
    LLM")) 中问题的解。
- en: '(b) RL. The learned reward function $r_{\psi_{*}}(x,y)$ is then used to provide
    feedback to the language model. More precisely, the following KL-regularized RL
    problem is considered:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RL。学习到的奖励函数 $r_{\psi_{*}}(x,y)$ 随后用于向语言模型提供反馈。更准确地说，考虑以下 KL 正则化 RL 问题：
- en: '|  |  | $\displaystyle\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y\mid
    x)}\left[r_{\psi_{*}}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right]$ |  | (3) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(y\mid
    x)}\left[r_{\psi_{*}}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right]$ |  | (3) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $$\beta>. The regularization is important as it prevents deviating
    too far from the SFT model that is trained to conform to the true preference,
    while maintaining the generation diversity to avoid mode-collapsing to a single
    high-reward answer. In terms of optimization, RLHF leverages RL algorithms due
    to the discrete nature of the language and the associated non-differentiable property
    of the objective.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $$\beta>。正则化很重要，因为它防止偏离训练以符合真实偏好的 SFT 模型，同时保持生成的多样性，以避免模式崩溃为单一的高奖励答案。在优化方面，RLHF
    利用 RL 算法，因为语言的离散性质和目标函数的不可微性。
- en: 'In view of ([3](#S2.E3 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")), RLHF uses the reward function $r(x,y)=r_{\psi}(x,y)-\beta\left(\log\pi(y\mid
    x)-\log\pi_{\text{ref }}(y\mid x)\right)$, and solves the RL problem by proximal
    policy optimization [[31](#bib.bib31)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 ([3](#S2.E3 "在 2.2\. RLHF ‣ 2\. 初步概念 ‣ Mallows-DPO：通过偏好分散来微调你的 LLM"))，RLHF
    使用奖励函数 $r(x,y)=r_{\psi}(x,y)-\beta\left(\log\pi(y\mid x)-\log\pi_{\text{ref }}(y\mid
    x)\right)$，并通过邻近策略优化解决 RL 问题 [[31](#bib.bib31)]。
- en: 2.3\. DPO
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. DPO
- en: '[[30](#bib.bib30)]. One disadvantage of RLHF is that the RL step often requires
    substantial computational effort (e.g., to carry out the proximal policy optimization).
    The idea of DPO is to combine the two steps (a)–(b) in RLHF into a single one,
    bypassing the computation in the RL step.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[[30](#bib.bib30)]。RLHF 的一个缺点是 RL 步骤通常需要大量的计算工作（例如，进行邻近策略优化）。DPO 的思想是将 RLHF
    中的两个步骤 (a)–(b) 合并为一个，绕过 RL 步骤中的计算。'
- en: 'The key idea is that given a reward function $r(x,y)$, the problem in ([3](#S2.E3
    "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) has a closed-form solution:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是，给定奖励函数 $r(x,y)$，在 ([3](#S2.E3 "在 2.2\. RLHF ‣ 2\. 初步概念 ‣ Mallows-DPO：通过偏好分散来微调你的
    LLM")) 中的问题有一个闭式解：
- en: '|  | $\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right),$
    |  | (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right),$
    |  | (4) |'
- en: 'where $Z(x)=\sum_{y}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)$
    is a normalizing constant. Rewrite the above as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z(x)=\sum_{y}\pi_{\text{ref }}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)$
    是一个归一化常数。将上述内容重写为：
- en: '|  | $r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid x)}+\beta\log
    Z(x).$ |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid x)}+\beta\log
    Z(x).$ |  | (5) |'
- en: 'Through this change of variables, the latent reward $r^{*}(x,y)$. Substituting
    this $r^{*}$ expression into ([1](#S2.E1 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) yields:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种变量变换，潜在奖励 $r^{*}(x,y)$。将这个 $r^{*}$ 表达式代入 ([1](#S2.E1 "在 2.2\. RLHF ‣ 2\.
    初步概念 ‣ Mallows-DPO：通过偏好分散来微调你的 LLM")) 得到：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $Z^{*}(x)$.)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z^{*}(x)$。)
- en: 'The expression in ([6](#S2.E6 "In 2.3\. DPO ‣ 2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) motivates the DPO objective:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式中的 ([6](#S2.E6 "在 2.3\. DPO ‣ 2\. 初步概念 ‣ Mallows-DPO：通过偏好分散来微调你的 LLM")) 激发了
    DPO 目标：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: which is a supervised learning problem, requiring much less computation than
    the RLHF.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个监督学习问题，相比 RLHF 需要的计算要少得多。
- en: 3\. DPO based on Mallows Ranking Models
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于 Mallows 排序模型的 DPO
- en: 3.1\. Mallows ranking models
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. Mallows 排序模型
- en: 'For $n\geq 1$. Consider the following preference probability:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $n\geq 1$。考虑以下偏好概率：
- en: '|  | $\mathbb{P}_{\phi,\mu_{0},d}(\mu):=\frac{1}{Z(\phi,d)}\phi^{d\left(\mu,\mu_{0}\right)}\quad\text{
    for }\mu\in\mathfrak{S}_{n},$ |  | (8) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}_{\phi,\mu_{0},d}(\mu):=\frac{1}{Z(\phi,d)}\phi^{d\left(\mu,\mu_{0}\right)}\quad\text{
    对于 }\mu\in\mathfrak{S}_{n},$ |  | (8) |'
- en: 'where $\phi\in(0,1]$ is a discrepancy function that is right invariant: $d(\mu_{1},\mu_{2})=d\left(\mu_{1}\circ\mu_{2}^{-1},id\right)$,
    the distribution ([8](#S3.E8 "In 3.1\. Mallows ranking models ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    is concentrated on $\mu_{0}$ items) with pairwise preferences, Mallows [[24](#bib.bib24)]
    considered two specific cases of the discrepancy function in ([8](#S3.E8 "In 3.1\.
    Mallows ranking models ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi\in(0,1]$ 是一个右不变的差异函数：$d(\mu_{1},\mu_{2})=d\left(\mu_{1}\circ\mu_{2}^{-1},id\right)$，分布
    ([8](#S3.E8 "在 3.1\. Mallows 排名模型 ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：用偏好分散微调你的
    LLM")) 集中在 $\mu_{0}$ 项目上，并具有成对偏好，Mallows [[24](#bib.bib24)] 考虑了 ([8](#S3.E8 "在
    3.1\. Mallows 排名模型 ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：用偏好分散微调你的 LLM")) 中差异函数的两个特定情况：
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mallows-$\theta$ is the Spearman’s rho,
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mallows-$\theta$ 是 Spearman 的 rho，
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mallows-$\phi$ is the Kendall’s tau,
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mallows-$\phi$ 是 Kendall 的 tau，
- en: where $\operatorname{inv}(\mu):=\#\left\{(i,j)\in[n]^{2}:i<j\right.$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\operatorname{inv}(\mu):=\#\left\{(i,j)\in[n]^{2}:i<j\right.$。
- en: 'The general form in ([8](#S3.E8 "In 3.1\. Mallows ranking models ‣ 3\. DPO
    based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions")) was suggested by [[12](#bib.bib12)] along with other discrepancy
    functions (e.g. Cayley, Hamming, Ulam distances, etc.) See [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)] for the related group representation approach
    to ranked, or partially ranked data. Note that the Mallows-$\theta$ (see [[26](#bib.bib26),
    [29](#bib.bib29), [35](#bib.bib35)].) In the context of language models, this
    conforms to a possibly infinite number of completions given a prompt, and allows
    interpreting unseen completions conceptually.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一般形式在 ([8](#S3.E8 "在 3.1\. Mallows 排名模型 ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：用偏好分散微调你的
    LLM")) 中由 [[12](#bib.bib12)] 提出，并且还有其他差异函数（例如 Cayley、Hamming、Ulam 距离等）。有关排名数据或部分排名数据的相关群体表示方法，请参阅
    [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。请注意 Mallows-$\theta$（参见
    [[26](#bib.bib26), [29](#bib.bib29), [35](#bib.bib35)]）。在语言模型的背景下，这符合给定提示的可能无限个补全，并允许从概念上解释未见过的补全。
- en: 3.2\. Mallows-DPO
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. Mallows-DPO
- en: 'Now we adapt Mallows ranking models highlighted above to the setting of language
    models. First, denote by $\mu(\cdot\mid x)$, such that the preference distribution
    is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将上述 Mallows 排名模型调整为语言模型的设置。首先，记为 $\mu(\cdot\mid x)$，使得偏好分布为：
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=\mathbb{P}\left(\mu(y_{1}\mid
    x)<\mu\left(y_{2}\mid x\right)\right).$ |  | (9) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=\mathbb{P}\left(\mu(y_{1}\mid
    x)<\mu\left(y_{2}\mid x\right)\right).$ |  | (9) |'
- en: 'Next, for the preference probability in ([8](#S3.E8 "In 3.1\. Mallows ranking
    models ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your
    LLM with Preference Dispersions")), given an input prompt $x$ may be computationally
    hard. Similar to RLHF in ([3](#S2.E3 "In 2.2\. RLHF ‣ 2\. Preliminaries ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), our goal here is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，给定输入提示 $x$，在 ([8](#S3.E8 "在 3.1\. Mallows 排名模型 ‣ 3\. 基于 Mallows 排名模型的 DPO
    ‣ Mallows-DPO：用偏好分散微调你的 LLM")) 中，偏好概率可能计算起来很困难。类似于 ([3](#S2.E3 "在 2.2\. RLHF ‣
    2\. 初步 ‣ Mallows-DPO：用偏好分散微调你的 LLM")) 中的 RLHF，我们在这里的目标是：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[-\mu_{0}(y\mid x)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[-\mu_{0}(y\mid x)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  |'
- en: 'where $r^{*}(x,y)$—note that a smaller rank is preferred as per ([9](#S3.E9
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions"))—and hence providing a natural
    candidate for the scaled reward that enhances model interpretation. This yields
    the change of variables similar to ([5](#S2.E5 "In 2.3\. DPO ‣ 2\. Preliminaries
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r^{*}(x,y)$—注意，较小的排名是首选（参见 ([9](#S3.E9 "在 3.2\. Mallows-DPO ‣ 3\. 基于 Mallows
    排名模型的 DPO ‣ Mallows-DPO：用偏好分散微调你的 LLM"))）—因此提供了一个自然的候选缩放奖励，增强模型解释。这产生了类似于 ([5](#S2.E5
    "在 2.3\. DPO ‣ 2\. 初步 ‣ Mallows-DPO：用偏好分散微调你的 LLM")) 的变量变换：
- en: '|  | $1$2 |  | (10) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (10) |'
- en: which “cleverly" avoids estimating $\mu_{0}(\cdot\mid x)$. We then derive the
    two versions of Mallows-DPO.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这“巧妙地”避免了估计 $\mu_{0}(\cdot\mid x)$。然后我们推导了 Mallows-DPO 的两个版本。
- en: Mallows-$\theta$, we have
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Mallows-$\theta$，我们有
- en: '|  | $1$2 |  | (11) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: 'where $\log\phi(x)\in(-\infty,0)$. The change of variables in ([10](#S3.E10
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")) leads to the objective:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\log\phi(x)\in(-\infty,0)$。 ([10](#S3.E10 "在 3.2\. Mallows-DPO ‣ 3\. 基于
    Mallows 排序模型的 DPO ‣ Mallows-DPO：用偏好离散度微调你的 LLM")) 中的变量变换导致目标：
- en: '|  |  | $\displaystyle\mathcal{L}_{\mathrm{MDPO}}\left(\pi;\pi_{\mathrm{ref}}\right):=$
    |  | (12) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{\mathrm{MDPO}}\left(\pi;\pi_{\mathrm{ref}}\right):=$
    |  | (12) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'In comparison with the objective of the BT-DPO (Bradley-Terry based DPO) in
    ([7](#S2.E7 "In 2.3\. DPO ‣ 2\. Preliminaries ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions")), the objective of Mallows-$\theta$-DPO can be viewed
    as DPO with prompt dispersion. (In contrast, prompt dispersion is not present
    at all in the BT-DPO.)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ([7](#S2.E7 "在 2.3\. DPO ‣ 2\. 基础知识 ‣ Mallows-DPO：用偏好离散度微调你的 LLM")) 中的 BT-DPO（基于
    Bradley-Terry 的 DPO）目标相比，Mallows-$\theta$-DPO 的目标可以看作是具有提示离散度的 DPO。（相反，BT-DPO
    中完全没有提示离散度。）
- en: 'Mallows-$\phi$ model, it was shown in [[24](#bib.bib24)] (see also [[7](#bib.bib7),
    [25](#bib.bib25)]):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Mallows-$\phi$ 模型，已在 [[24](#bib.bib24)] 中展示（参见 [[7](#bib.bib7), [25](#bib.bib25)]）：
- en: '|  | $1$2 |  | (13) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (13) |'
- en: where
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $1$2 |  | (14) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: or compactly,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 或简洁地，
- en: '|  | $1$2 |  | (15) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: 'is the link function. Similar to Mallows-$\theta$-DPO:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 是链接函数。类似于 Mallows-$\theta$-DPO：
- en: '|  | $1$2 |  | (16) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (16) |'
- en: 'In comparison with the BT-DPO in ([7](#S2.E7 "In 2.3\. DPO ‣ 2\. Preliminaries
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), Mallows-$\phi$.
    Refer to Table [1](#S3.T1 "Table 1 ‣ 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows
    Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for a summary of the key features of the different models.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ([7](#S2.E7 "在 2.3\. DPO ‣ 2\. 基础知识 ‣ Mallows-DPO：用偏好离散度微调你的 LLM")) 中的 BT-DPO
    相比，Mallows-$\phi$。有关不同模型关键特性的总结，请参见表 [1](#S3.T1 "表 1 ‣ 3.2\. Mallows-DPO ‣ 3\.
    基于 Mallows 排序模型的 DPO ‣ Mallows-DPO：用偏好离散度微调你的 LLM")。
- en: Table 1. Key Features of DPO Models
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. DPO 模型的关键特性
- en: '| Model Class | Model Name | Dispersion | Link function |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型类别 | 模型名称 | 离散度 | 链接函数 |'
- en: '| BT-DPO | DPO [[30](#bib.bib30)] | ✗ | sigmoid $\sigma(\cdot)$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | DPO [[30](#bib.bib30)] | ✗ | sigmoid $\sigma(\cdot)$ |'
- en: '| Mallows-DPO | Mallows-$\theta$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-DPO | Mallows-$\theta$ |'
- en: '| Mallows-$\phi$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO ‣ 3\. DPO based on
    Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$ 在 ([15](#S3.E15 "在 3.2\. Mallows-DPO ‣ 3\. 基于 Mallows 排序模型的
    DPO ‣ Mallows-DPO：用偏好离散度微调你的 LLM")) |'
- en: 3.3\. Approximate the dispersion index
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 近似离散度指数
- en: As the dispersion index $\phi(x)$ to the empirical output distribution of the
    pre-trained model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预训练模型的经验输出分布的离散度指数 $\phi(x)$。
- en: 'Suppose the preference follows Mallows-$\phi$ model. There are two extreme
    cases:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设偏好遵循 Mallows-$\phi$ 模型。有两种极端情况：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'When $-\log(\phi(x))\rightarrow\infty$, we have:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当 $-\log(\phi(x))\rightarrow\infty$ 时，我们有：
- en: '|  | $1$2 |  |'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Thus, the probability distribution of the next token will concentrate on a point
    mass.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，下一个标记的概率分布将集中在一个点质量上。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When $-\log(\phi(x))\rightarrow 0$ so the next token will be uniformly distributed.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当 $-\log(\phi(x))\rightarrow 0$ 时，下一个标记将被均匀分布。
- en: The above observation motivates us to use Shannon’s entropy, for a discrete
    random variable $X\in\{x_{1},\ldots,x_{n}\}$ when $X$ is uniformly distributed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 上述观察促使我们使用 Shannon 熵，对于离散随机变量 $X\in\{x_{1},\ldots,x_{n}\}$ 当 $X$ 均匀分布时。
- en: 'For a given constant <math id=$$, we propose:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的常量 <math id=$$，我们提出：
- en: '|  | $-\phi^{*}\log\left(\frac{H(\pi(\cdot\mid x))}{\log(n)}\right),$ |  |
    (17) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $-\phi^{*}\log\left(\frac{H(\pi(\cdot\mid x))}{\log(n)}\right),$ |  |
    (17) |'
- en: as a proxy to $-\log\phi(x)$.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 $-\log\phi(x)$ 的代理。
- en: 'Here the hyperparameter $\phi^{*}$:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的超参数 $\phi^{*}$：
- en: '|  | $H(\pi(\cdot\mid x))\approx\frac{1}{2}\sum_{i=1}^{N-1}\left[H(X_{i+1}\mid
    X_{i}=x^{w}_{i})+H(X_{i+1}\mid X_{i}=x^{l}_{i})\right],$ |  | (18) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(\pi(\cdot\mid x))\approx\frac{1}{2}\sum_{i=1}^{N-1}\left[H(X_{i+1}\mid
    X_{i}=x^{w}_{i})+H(X_{i+1}\mid X_{i}=x^{l}_{i})\right],$ |  | (18) |'
- en: which can be directly computed by the logits of the model given the output (preference)
    data. This is also closely related to the predictive entropy [[17](#bib.bib17),
    [23](#bib.bib23)] of the next-token predictions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以直接通过模型给出的 logits 和输出（偏好）数据计算。这也与下一 token 预测的预测熵 [[17](#bib.bib17), [23](#bib.bib23)]
    密切相关。
- en: 3.4\. Unify Mallows-$\theta$ for computation
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 统一计算中的 Mallows-$\theta$
- en: Note that the link function $g_{x}$, with
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意链接函数 $g_{x}$，其中
- en: '|  | $1$2 |  | (19) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: For computational purposes, we propose two smooth approximations to $g_{x}$.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 出于计算目的，我们提出了两种平滑的 $g_{x}$ 近似。
- en: (i)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Sigmoid approximation: Since $g_{x}(1)=\frac{1}{1+\phi(x)}$. See Figure [3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration of this approximation. With this approximation, Mallows-$\phi$).
    Thus, Mallows-$\theta$-DPO with sigmoid approximation.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sigmoid 近似：由于 $g_{x}(1)=\frac{1}{1+\phi(x)}$。请参见图 [3](#S3.F3 "图 3 ‣ 3.4\. 统一计算中的
    Mallows-𝜃 和 Mallows-ϕ ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：使用偏好分散微调您的 LLM")
    以获取该近似的说明。使用该近似，Mallows-$\phi$）。因此，Mallows-$\theta$-DPO 与 sigmoid 近似。
- en: (ii)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'Polynomial fitting: We use a polynomial of form $P(x)=a_{3}x^{3}+a_{1}x+a_{0}$
    being a hyperparameter. We choose $\epsilon$ for $\phi(x)=0.5$). See Figures [3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[3](#S3.F3
    "Figure 3 ‣ 3.4\. Unify Mallows-𝜃 and Mallows-ϕ for computation ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    for an illustration.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多项式拟合：我们使用形式为 $P(x)=a_{3}x^{3}+a_{1}x+a_{0}$ 的多项式作为超参数。我们选择 $\epsilon$ 使得 $\phi(x)=0.5$）。参见图
    [3](#S3.F3 "图 3 ‣ 3.4\. 统一计算中的 Mallows-𝜃 和 Mallows-ϕ ‣ 3\. 基于 Mallows 排名模型的 DPO
    ‣ Mallows-DPO：使用偏好分散微调您的 LLM")–[3](#S3.F3 "图 3 ‣ 3.4\. 统一计算中的 Mallows-𝜃 和 Mallows-ϕ
    ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：使用偏好分散微调您的 LLM") 以获取说明。
- en: '![Refer to caption](img/4089e75f667b4937a6f747d63ff862b1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4089e75f667b4937a6f747d63ff862b1.png)'
- en: Figure 1. Sigmoid approximation
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. Sigmoid 近似
- en: '![Refer to caption](img/3c2da4454b7107de2ad5ea0dacb552f3.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3c2da4454b7107de2ad5ea0dacb552f3.png)'
- en: Figure 2. Poly-fitting on $\pm\epsilon$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 在 $\pm\epsilon$ 上的多项式拟合
- en: '![Refer to caption](img/4ef9060eb0d2d9f488ca4422be0a6a4d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ef9060eb0d2d9f488ca4422be0a6a4d.png)'
- en: Figure 3. Poly-fitting on $\pm 2\log\phi$
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 在 $\pm 2\log\phi$ 上的多项式拟合
- en: 4\. Perspectives on Mallows-DPO
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. Mallows-DPO的视角
- en: 'In this section, we provide several alternative perspectives on Mallows-DPO
    in ([12](#S3.E12 "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")) and ([16](#S3.E16
    "In 3.2\. Mallows-DPO ‣ 3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")), with the proofs given in Appendix
    A. We say a DPO is directed by $g(\cdot)$ if the preference distribution can be
    expressed as'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了关于 Mallows-DPO 的几种替代视角，详见 ([12](#S3.E12 "在 3.2\. Mallows-DPO ‣ 3\.
    基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：使用偏好分散微调您的 LLM")) 和 ([16](#S3.E16 "在 3.2\.
    Mallows-DPO ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：使用偏好分散微调您的 LLM"))，证明见附录 A。我们称一个
    DPO 由 $g(\cdot)$ 导向，如果偏好分布可以表示为
- en: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=g\left(r^{*}(x,y_{1})-r^{*}\left(x,y_{2}\right)\right),$
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}\left(y_{1}\succ y_{2}\mid x\right)=g\left(r^{*}(x,y_{1})-r^{*}\left(x,y_{2}\right)\right),$
    |  |'
- en: for some reward function $r^{*}$.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些奖励函数 $r^{*}$。
- en: 4.1\. Dispersion weighted objectives
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 离散加权目标
- en: The following result shows that Mallows-$\theta$-DPO can be viewed as a DPO
    with either the reward or the KL-regularizer weighted by the dispersion index.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果显示，Mallows-$\theta$-DPO 可以被视为一个 DPO，其中奖励或 KL-正则化项由离散指数加权。
- en: Theorem 1  (Mallows-$\theta$-DPO as dispersion weighted DPO).
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（Mallows-$\theta$-DPO 作为离散加权 DPO）。
- en: 'Let $c(x)=-2\log\phi(x)$:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $c(x)=-2\log\phi(x)$：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (20) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (20) |'
- en: or
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (21) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (21) |'
- en: A similar result holds for Mallows-$\phi$-DPO.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Mallows-$\phi$-DPO，类似的结果成立。
- en: Theorem 2  (Mallows-$\phi$-DPO as dispersion weighted DPO).
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理2（Mallows-$\phi$-DPO作为加权DPO的离散度）。
- en: 'Setting $\phi(x)=e$ in ([15](#S3.E15 "In 3.2\. Mallows-DPO ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"))
    yields'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在([15](#S3.E15 "在3.2\. Mallows-DPO ‣ 3\. 基于Mallows排序模型的DPO ‣ Mallows-DPO：用偏好离散度微调你的LLM"))中设定$\phi(x)=e$，得到
- en: '|  | $g(s):=\frac{1-\operatorname{sgn}(s)}{2}+\operatorname{sgn}(s)\left(\frac{&#124;s&#124;+1}{1-e^{&#124;s&#124;+1}}-\frac{&#124;s&#124;}{1-e^{&#124;s&#124;}}\right).$
    |  | (22) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(s):=\frac{1-\operatorname{sgn}(s)}{2}+\operatorname{sgn}(s)\left(\frac{|s|+1}{1-e^{|s|+1}}-\frac{|s|}{1-e^{|s|}}\right).$
    |  | (22) |'
- en: 'Let $c(x)=-2\log\phi(x)$ or the KL-regularizer weighted by $\beta c(x)$:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 设$c(x)=-2\log\phi(x)$或由$\beta c(x)$加权的KL-正则化项：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (23) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[c(x)^{-1}r^{*}(x,y)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (23) |'
- en: or
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (24) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(y\mid
    x)}\left[r^{*}(x,y)\right]-\beta c(x)\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (24) |'
- en: 4.2\. Connection to $\Psi$PO
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 与$\Psi$PO的联系
- en: 'The objective of $\Psi$PO [[2](#bib.bib2)] is:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: $\Psi$PO的目标[[2](#bib.bib2)]是：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (25) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right],$ |  | (25) |'
- en: where $\Psi:[0,1]\to\mathbb{R}$PO to the Bradley-Terry based DPO.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Psi:[0,1]\to\mathbb{R}$PO应用于基于Bradley-Terry的DPO。
- en: 'Roughly speaking, the function $\Psi$. Assume such a function exists, which
    we denote as $\Psi^{M}(\cdot)$ model in ([13](#S3.E13 "In 3.2\. Mallows-DPO ‣
    3\. DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with
    Preference Dispersions")), we have'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略来说，函数$\Psi$。假设存在这样一个函数，我们用$\Psi^{M}(\cdot)$在([13](#S3.E13 "在3.2\. Mallows-DPO
    ‣ 3\. 基于Mallows排序模型的DPO ‣ Mallows-DPO：用偏好离散度微调你的LLM"))中表示，那么我们有
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid x)}\left[\Psi^{M}\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (26) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid x)}\left[\Psi^{M}\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (26) |'
- en: '|  |  | $\displaystyle\neq r(x,y_{1})-\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid
    x)}\left[r\left(x,y_{2}\right)\right],$ |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\neq r(x,y_{1})-\mathbb{E}_{y_{2}\sim\bar{\pi}(\cdot\mid
    x)}\left[r\left(x,y_{2}\right)\right],$ |  |'
- en: i.e., for any $\Psi^{M}(\cdot)$PO to take into account prompt dispersion.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即，对于任何$\Psi^{M}(\cdot)$PO需要考虑提示离散度。
- en: 'Generalized $\Psi$. The generalized $\Psi$PO takes the form:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 广义的$\Psi$。广义的$\Psi$PO形式为：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\tilde{\Psi}\left(x,p^{*}(y\succ
    y^{\prime}\mid x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (27) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[\tilde{\Psi}\left(x,p^{*}(y\succ
    y^{\prime}\mid x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\mid\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (27) |'
- en: 'A special instance is when $\tilde{\Psi}(x,p)=f(x)\Psi(p)$ is separable:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特殊的实例是当$\tilde{\Psi}(x,p)=f(x)\Psi(p)$是可分离的：
- en: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[f(x)\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (28) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_{\theta}(\cdot\mid
    x),\,y^{\prime}\sim\tilde{\pi}(\cdot\mid x)}\left[f(x)\Psi\left(p^{*}(y\succ y^{\prime}\mid
    x)\right)\right]-\beta\mathrm{KL}\left(\pi(\cdot\mid x)\&#124;\pi_{\mathrm{ref}}(\cdot\mid
    x)\right)\right].$ |  | (28) |'
- en: Theorem 3  (Mallows-DPO as generalized $\Psi$PO).
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3  (Mallows-DPO 作为广义 $\Psi$PO)。
- en: (i) Mallows-$\theta$ and $f(x)=-\frac{1}{2\log\phi(x)}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Mallows-$\theta$ 和 $f(x)=-\frac{1}{2\log\phi(x)}$。
- en: (ii) Mallows-$\phi$ and $f(x)=-\frac{1}{\log\phi(x)}$.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) Mallows-$\phi$ 和 $f(x)=-\frac{1}{\log\phi(x)}$。
- en: 5\. Experiments
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 实验
- en: In this section, we evaluate the capability of our proposed Mallows-DPO to learn
    the preferences in comparison with DPO. First, we use the preferences dataset
    of IMDB [[22](#bib.bib22)] datasets and Anthropic Helpful and Harmless dialogue
    [[3](#bib.bib3)] dataset to provide evidence that human preferences may be diversed.
    Next, we consider a synthetic bandit problem to demonstrate the effectiveness
    of our proposed Mallows-$\phi$-DPO, even without prompt dispersions. We further
    conduct experiments on tasks such as conditional generation (IMDB) and dialogue
    (Anthropic HH). Our findings show that Mallows-DPO outperforms DPO with an evident
    margin, both for in-distribution performance and out-of-distribution generalization
    capability.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们提出的 Mallows-DPO 学习偏好的能力，并与 DPO 进行了比较。首先，我们使用 IMDB [[22](#bib.bib22)]
    数据集和 Anthropic Helpful 和 Harmless 对话 [[3](#bib.bib3)] 数据集，提供人类偏好可能多样化的证据。接下来，我们考虑一个合成赌博问题，以展示我们提出的
    Mallows-$\phi$-DPO 的有效性，即使没有提示分散。我们进一步在条件生成（IMDB）和对话（Anthropic HH）等任务上进行了实验。我们的发现表明，Mallows-DPO
    在分布内表现和分布外泛化能力方面明显优于 DPO。
- en: 5.1\. Evidence of preference dispersion
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 偏好分散的证据
- en: 'A first natural question is: are human preferences dispersed? To verify this
    key motivation for our work, we plot the distribution of the dispersion estimators
    given the SFT model and pairwise preferences. Recall from Section [3](#S3 "3\.
    DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") that the dispersion estimator is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个自然的问题是：人类偏好是否分散？为了验证这一关键动机，我们绘制了在 SFT 模型和成对偏好下的分散估计量的分布。回顾第 [3](#S3 "3\.
    DPO based on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions) 节，分散估计量是：'
- en: '|  | $1$2 |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and we take the hyperparameter  such that the empirical mean is equal to 1, so we do
    not need to tune this scaling constant.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用超参数，使得经验均值等于
    1，因此我们不需要调整这个缩放常数。
- en: '![Refer to caption](img/892ecba31376f2f4fe90627f7b4cd48d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/892ecba31376f2f4fe90627f7b4cd48d.png)'
- en: (a) IMDB preference dispersion
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (a) IMDB 偏好分散
- en: '![Refer to caption](img/397713bda291ca043c63c48d526dda52.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/397713bda291ca043c63c48d526dda52.png)'
- en: (b) Anthropic-HH preference dispersion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Anthropic-HH 偏好分散。
- en: Figure 4. LEFT. Distribution of our dispersion estimator on IMDB. RIGHT. Anthropic-HH
    perference dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 左侧. 我们的分散估计量在 IMDB 上的分布。右侧. Anthropic-HH 偏好数据集。
- en: 'We find that for the task of conditional generation such as IMDB, the human
    preferences are not quite diverse: the dispersion estimators are located near
    $1$. However, for tasks such as single dialogue, our plot shows that human preferences
    may be dispersed: the distribution is both skewed and of high variance.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，对于条件生成任务如 IMDB，人类偏好并不十分多样：分散估计量接近 $1$。然而，对于单一对话等任务，我们的图表显示人类偏好可能是分散的：分布既偏斜又具有高方差。
- en: 5.2\. Mallows-$\phi$-DPO mitigates reward collapse
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. Mallows-$\phi$-DPO 缓解奖励崩溃
- en: We study Mallows-DPO in a synthetic bandit experiment where there is no contextual
    information $x$. Second, the limited data availability tests the ability of the
    approaches to produce diversified policies and avoid reward collapse.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个没有上下文信息 $x$ 的合成赌博实验中研究了 Mallows-DPO。其次，有限的数据可用性测试了这些方法产生多样化策略和避免奖励崩溃的能力。
- en: Concretely, we consider five arms, each associated with a random reward drawn
    from a probability distribution. Preference between any two picked arms is determined
    by the random reward realizations, with larger reward being preferred. In the
    experiment, we collect $16$, and (2) across different epochs. The details are
    provided in Appendix B.1.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们考虑五个臂，每个臂都与从概率分布中抽取的随机奖励相关。任何两个被挑选的臂之间的偏好由随机奖励实现决定，较大的奖励更受欢迎。在实验中，我们收集了
    $16$，以及（2）跨不同的纪元。详细信息见附录 B.1。
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.2\. Mallows-ϕ-DPO mitigates reward collapse
    ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")
    displays the efficient frontiers for Mallows-$\phi$ is small. (2) Over all possible
    $\beta$ gets smaller. That is, Mallows-$\phi$-DPO leads to the policies that have
    both high rewards and small KL divergence.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S5.F5 "图 5 ‣ 5.2\. Mallows-ϕ-DPO 缓解奖励崩溃 ‣ 5\. 实验 ‣ Mallows-DPO: 用偏好分散微调你的
    LLM") 显示了 Mallows-$\phi$ 的有效前沿很小。（2）在所有可能的 $\beta$ 上变得更小。也就是说，Mallows-$\phi$-DPO
    导致的策略既具有高奖励又具有小的 KL 散度。'
- en: '![Refer to caption](img/ebd4e666fdc6751cb771f3e560f5d24e.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebd4e666fdc6751cb771f3e560f5d24e.png)'
- en: (a)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: （a）
- en: '![Refer to caption](img/e2e0370ae681d81084341acf5b1eec0c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2e0370ae681d81084341acf5b1eec0c.png)'
- en: (b)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: （b）
- en: 'Figure 5. Efficient frontiers: reward vs KL. LEFT. generated by measuring KL
    and reward for the policy trained with different $\beta$.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 有效前沿：奖励与 KL。左侧。通过测量不同 $\beta$ 训练的策略的 KL 和奖励生成。
- en: '![Refer to caption](img/68770a2f80f7991cc30bb188271cc9b6.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68770a2f80f7991cc30bb188271cc9b6.png)'
- en: Figure 6. Training curves of Mallows-$\phi$.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. Mallows-$\phi$ 的训练曲线。
- en: 5.3\. Mallows-DPO yields better tradeoff between accuracy and regularization
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. Mallows-DPO 在准确性和正则化之间提供更好的权衡
- en: We conduct the conditional generation for IMDB dataset. In this task, $x$ with
    positive sentiment. Following the setting in [[30](#bib.bib30)], we first fine-tune
    GPT-2-large on the training split of IMDB datasets until convergence to get the
    SFT model. Next, we use the pairwise preference data from [[39](#bib.bib39)] to
    fine-tune the SFT model by DPO and Mallows-DPO.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 IMDB 数据集进行条件生成。在此任务中，$x$ 具有正面情感。根据 [[30](#bib.bib30)] 的设置，我们首先对 IMDB 数据集的训练分割进行
    GPT-2-large 微调，直至收敛以获得 SFT 模型。接下来，我们使用来自 [[39](#bib.bib39)] 的成对偏好数据通过 DPO 和 Mallows-DPO
    对 SFT 模型进行微调。
- en: '![Refer to caption](img/7e109a6c199521d77942962c0c857b67.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e109a6c199521d77942962c0c857b67.png)'
- en: 'Figure 7. Efficient frontiers: accuracy vs KL achieved by Mallows-DPO and BT-DPO.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 有效前沿：Mallows-DPO 和 BT-DPO 实现的准确性与 KL。
- en: 'Figure [7](#S5.F7 "Figure 7 ‣ 5.3\. Mallows-DPO yields better tradeoff between
    accuracy and regularization ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") displays the efficient frontiers (during the training
    process) for BT-DPO, Mallows-$\theta$-DPO outperforms both, achieving the same
    accuracy (evaluated by the reward model) at a smaller KL divergence to the SFT
    model/policy.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#S5.F7 "图 7 ‣ 5.3\. Mallows-DPO 在准确性和正则化之间提供更好的权衡 ‣ 5\. 实验 ‣ Mallows-DPO:
    用偏好分散微调你的 LLM") 显示了 BT-DPO 的有效前沿（在训练过程中），Mallows-$\theta$-DPO 优于两者，在较小的 KL 散度下实现相同的准确性（通过奖励模型评估）。'
- en: '5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution and out-of-distribution
    performances'
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 分散性的重要性：Mallows-DPO 提升了分布内和分布外的表现
- en: 'We compare the performances of Mallows-DPO and BT-DPO in terms of the win rate
    evaluated by GPT4, and generalization capability on the out-of-distribution datasets.
    In the experiment, we choose $\beta$ value leads to a drop both in performance
    and per-input diversity of RLHF and DPO. Results are shown in Figure [8](#S5.F8
    "Figure 8 ‣ 5.4\. Dispersion matters: Mallows-DPO enhancing both in-distribution
    and out-of-distribution performances ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions").'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了 Mallows-DPO 和 BT-DPO 在 GPT4 评估的胜率以及在分布外数据集上的泛化能力。在实验中，我们选择了 $\beta$ 值，这导致
    RLHF 和 DPO 的表现以及每输入的多样性下降。结果见图[8](#S5.F8 "图 8 ‣ 5.4\. 分散性的重要性：Mallows-DPO 提升了分布内和分布外的表现
    ‣ 5\. 实验 ‣ Mallows-DPO: 用偏好分散微调你的 LLM")。'
- en: 'In-distribution test. We first fine-tune a pretrained Pythia-2.8B model on
    the training set of Anthropic HH dataset using Mallows-DPO and BT-DPO, and then
    evaluate the responses on a subset of its test split, generated by these fine-tuned
    models. GPT-4 serves as the evaluator, and compares pairs of responses: one from
    the model fine-tuned with Mallows-$\theta$-DPO have an edge over BT-DPO. In particular,
    Mallows-$\phi$ for $\beta=0.1$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 分布内测试。我们首先在Anthropic HH数据集的训练集上对预训练的Pythia-2.8B模型进行微调，使用Mallows-DPO和BT-DPO，然后在这些微调模型生成的测试集子集上评估响应。GPT-4作为评估者，比较响应对：一个来自Mallows-$\theta$-DPO微调模型的响应相较于BT-DPO有优势。特别是，Mallows-$\phi$对于$\beta=0.1$。
- en: '![Refer to caption](img/4dc0d4b95f28003f2f295ed466962f4c.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4dc0d4b95f28003f2f295ed466962f4c.png)'
- en: Figure 8. Win rates computed by GPT-4 evaluations for responses on both the
    in-distribution dataset (Anthropic HH) and out-of-distribution datasets (H4 Stack
    Exchange and Stanford Human Preferences), generated by the models fine-tuned on
    the Anthropic HH training set.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. GPT-4评估计算的胜率，针对由在Anthropic HH训练集上微调的模型生成的分布内数据集（Anthropic HH）和分布外数据集（H4
    Stack Exchange和斯坦福人类偏好）的响应。
- en: Out-of-distribution test. We apply the models, fine-tuned on the train split
    of the Antropic HH dataset, to other datasets with different input distributions.
    The H4 Stack Exchange Preferences Dataset, collected from the Stack Overflow,
    and Stanford Human Preferences (SHP), containing questions or instructions in
    various subject areas, are used for evaluation. In these out-of-distribution tasks,
    the advantage of dispersion on generalization becomes apparent, as Mallows-$\theta$,
    the performance of Mallows-DPO is consistently above 55%, with Mallows-$\phi$-DPO
    achieving a notable improvement on the H4 Stack Exchange dataset, reaching a win
    rate with more than 60%.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 分布外测试。我们将针对Antropic HH数据集训练集上微调的模型应用于具有不同输入分布的其他数据集。使用H4 Stack Exchange偏好数据集（来自Stack
    Overflow）和斯坦福人类偏好（SHP，包含各种学科领域的问题或指令）进行评估。在这些分布外任务中，散布对泛化的优势变得明显，Mallows-$\theta$，Mallows-DPO的表现始终在55%以上，其中Mallows-$\phi$-DPO在H4
    Stack Exchange数据集上的表现显著提升，赢率超过60%。
- en: 6\. Conclusion
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: We have developed in this paper a novel approach, the Mallows-DPO, to fine-tune
    LLM. A distinct feature of this approach is a dispersion index, which naturally
    captures the dispersion of human preference to prompts, and can be systematically
    incorporated into the reward function as a weight factor, thus ushering in a new
    class of dispersion-weighted DPO models. We demonstrate empirically how Mallows-DPO
    achieves improved performance in a broad array of benchmark tasks, including synthetic
    bandit selection, controllable generation, and dialogues.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中开发了一种新方法，即Mallows-DPO，用于微调LLM。这种方法的一个独特特点是散布指数，它自然地捕捉人类对提示的偏好散布，并可以系统地纳入奖励函数作为权重因子，从而开创了一类新的散布加权DPO模型。我们通过实证演示了Mallows-DPO在合成赌博选择、可控生成和对话等各种基准任务中如何实现性能提升。
- en: There are a few issues that we have yet to address in this study, for instance,
    to explore why Mallows-DPO outperforms BT-DPO, how the dispersion index contributes
    to performance improvement, what guidelines to follow to set the $\beta$-divergence
    [[39](#bib.bib39)]). These will be pursued in our future works.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中还有一些问题待解决，例如，探索为何Mallows-DPO优于BT-DPO，散布指数如何促进性能提升，如何设置$\beta$-散度的指南[[39](#bib.bib39)]。这些将在我们未来的工作中深入探讨。
- en: 'Acknowledgement: Hanyang Zhao and Wenpin Tang are supported by NSF grants DMS-2113779
    and DMS-2206038, and by a start-up grant at Columbia University. The works of
    Haoxian Chen, Hanyang Zhao, Henry Lam and David Yao are part of a Columbia-CityU/HK
    collaborative project that is supported by InnotHK Initiative, The Government
    of the HKSAR and the AIFT Lab. Additionally, Haoxian Chen is supported by the
    Amazon CAIT fellowship. Henry Lam and Wenpin Tang receive support from the Columbia
    Innovation Hub grant.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢：Hanyang Zhao和Wenpin Tang获得了NSF资助（DMS-2113779和DMS-2206038），以及哥伦比亚大学的启动资金。Haoxian
    Chen、Hanyang Zhao、Henry Lam和David Yao的研究属于哥伦比亚大学与城大/香港的合作项目，该项目得到了InnotHK Initiative、香港特别行政区政府和AIFT实验室的支持。此外，Haoxian
    Chen还获得了Amazon CAIT奖学金。Henry Lam和Wenpin Tang则获得了哥伦比亚创新中心的资助。
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and Shyamal Anadkat. GPT-4
    technical report. 2023. arXiv:2303.08774.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, 和 Shyamal Anadkat.
    GPT-4技术报告。2023年。arXiv:2303.08774。'
- en: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm
    to understand learning from human preferences. In AISTATS, pages 4447–4455, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, 和 Daniele Calandriello. 理解从人类偏好中学习的通用理论范式。发表于AISTATS，页面4447–4455，2024年。'
- en: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. 2022.
    arXiv:2204.05862.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等等。通过从人类反馈中学习的强化学习训练有用且无害的助手。2022年。arXiv:2204.05862。'
- en: '[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
    Constitutional AI: Harmlessness from AI feedback. 2022. arXiv:2212.08073.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, 等等。宪法AI：来自AI反馈的无害性。2022年。arXiv:2212.08073。'
- en: '[5] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block
    designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ralph Allan Bradley 和 Milton E Terry. 不完全区组设计的排名分析：I. 成对比较的方法。Biometrika,
    39(3/4):324–345, 1952年。'
- en: '[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda
    Askell. Language models are few-shot learners. In Neurips, volume 33, pages 1877–1901,
    2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, 和 Amanda Askell.
    语言模型是少样本学习者。发表于Neurips，第33卷，页面1877–1901，2020年。'
- en: '[7] Róbert Busa-Fekete, Eyke Hüllermeier, and Balázs Szörényi. Preference-based
    rank elicitation using statistical models: The case of Mallows. In ICML, pages
    1071–1079, 2014.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Róbert Busa-Fekete, Eyke Hüllermeier, 和 Balázs Szörényi. 使用统计模型的基于偏好的排名引出：Mallows的案例。发表于ICML，页面1071–1079，2014年。'
- en: '[8] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang,
    Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko,
    Tianqi Liu, et al. Human alignment of large language models through online preference
    optimisation. arXiv preprint arXiv:2403.08635, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang,
    Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko,
    Tianqi Liu, 等等。通过在线偏好优化实现大型语言模型的人类对齐。arXiv预印本 arXiv:2403.08635，2024年。'
- en: '[9] David M Chan, Yiming Ni, David A Ross, Sudheendra Vijayanarasimhan, Austin
    Myers, and John Canny. Distribution aware metrics for conditional natural language
    generation. 2022. arXiv:2209.07518.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] David M Chan, Yiming Ni, David A Ross, Sudheendra Vijayanarasimhan, Austin
    Myers, 和 John Canny. 关注分布的条件自然语言生成度量。2022年。arXiv:2209.07518。'
- en: '[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play
    fine-tuning converts weak language models to strong language models. 2024. arXiv:2401.01335.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, 和 Quanquan Gu. 自我训练微调将弱语言模型转换为强语言模型。2024年。arXiv:2401.01335。'
- en: '[11] Douglas Critchlow. Metric methods for analyzing partially ranked data,
    volume 34. Lecture notes in Statistics, Springer, 1985.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Douglas Critchlow. 分析部分排序数据的度量方法，第34卷。统计学讲义，Springer出版社，1985年。'
- en: '[12] Persi Diaconis. Group representations in probability and statistics, volume 11.
    Lecture Notes-Monograph Series, 1988.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Persi Diaconis. 概率与统计中的群体表示，第11卷。讲义笔记-专著系列，1988年。'
- en: '[13] Persi Diaconis. A generalization of spectral analysis with application
    to ranked data. Ann. Stat., pages 949–979, 1989.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Persi Diaconis. 具有排名数据应用的谱分析推广。Ann. Stat., 页面949–979，1989年。'
- en: '[14] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm:
    A simulation framework for methods that learn from human feedback. In Neurips,
    volume 36, 2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, Percy S Liang, 和 Tatsunori B Hashimoto. Alpacafarm：一个学习人类反馈的方法模拟框架。发表于Neurips，第36卷，2024年。'
- en: '[15] Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. Human-aware
    loss functions (halos). Technical report, Contextual AI, 2023. https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, 和 Douwe Kiela. 关注人类的损失函数（halos）。技术报告，Contextual
    AI，2023年。https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf。'
- en: '[16] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There
    are a thousand hamlets in a thousand people’s eyes: Enhancing knowledge-grounded
    dialogue with personal memory. 2022. arXiv:2204.02624.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, 和 Rui Yan. 一千个人眼中有一千个哈姆雷特：通过个人记忆增强知识基础对话。2022年。arXiv:2204.02624。'
- en: '[17] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani.
    Predictive entropy search for efficient global optimization of black-box functions.
    In NIPS, volume 27, pages 918––926, 2014.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] José Miguel Hernández-Lobato, Matthew W Hoffman, 和 Zoubin Ghahramani.
    用于高效全局优化黑箱函数的预测熵搜索。在 NIPS，第 27 卷，页码 918–926，2014。'
- en: '[18] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, and Iz Beltagy.
    Camels in a changing climate: Enhancing LM adaptation with Tulu 2. 2023. arXiv:2311.10702.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, 和 Iz Beltagy. 在变化的气候中骆驼：通过
    Tulu 2 增强 LM 适应性。2023年。arXiv:2311.10702。'
- en: '[19] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina,
    Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects
    of RLHF on LLM generalisation and diversity. arXiv preprint arXiv:2310.06452,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina,
    Eric Hambro, Edward Grefenstette, 和 Roberta Raileanu. 理解 RLHF 对 LLM 泛化和多样性的影响。arXiv
    预印本 arXiv:2310.06452, 2023。'
- en: '[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement
    learning from human feedback with AI feedback. 2023. arXiv:2309.00267.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard,
    Colton Bishop, Victor Carbune, 和 Abhinav Rastogi. Rlaif：通过 AI 反馈扩展来自人类反馈的强化学习。2023年。arXiv:2309.00267。'
- en: '[21] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan
    Jurafsky. Deep reinforcement learning for dialogue generation. 2016. arXiv:1606.01541.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, 和 Dan
    Jurafsky. 用于对话生成的深度强化学习。2016年。arXiv:1606.01541。'
- en: '[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and
    Christopher Potts. Learning word vectors for sentiment analysis. In ACL, pages
    142–150, 2011.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, 和 Christopher
    Potts. 为情感分析学习词向量。在 ACL，页码 142–150，2011。'
- en: '[23] David JC MacKay. Information-based objective functions for active data
    selection. Neural computation, 4(4):590–604, 1992.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] David JC MacKay. 基于信息的主动数据选择目标函数。Neural computation, 4(4):590–604, 1992。'
- en: '[24] Colin L Mallows. Non-null ranking models. I. Biometrika, 44(1/2):114–130,
    1957.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Colin L Mallows. 非零排名模型。我。Biometrika, 44(1/2):114–130, 1957。'
- en: '[25] Cheng Mao and Yihong Wu. Learning mixtures of permutations: groups of
    pairwise comparisons and combinatorial method of moments. Ann. Statist., 50(4):2231–2255,
    2022.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Cheng Mao 和 Yihong Wu. 学习排列的混合：成对比较的组和组合矩方法。Ann. Statist., 50(4):2231–2255,
    2022。'
- en: '[26] Marina Meila and Le Bao. An exponential model for infinite rankings. J.
    Mach. Learn. Res., 11:3481–3518, 2010.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Marina Meila 和 Le Bao. 无限排名的指数模型。J. Mach. Learn. Res., 11:3481–3518, 2010。'
- en: '[27] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar,
    Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    and Andrea Michi. Nash learning from human feedback. 2023. arXiv:2312.00886.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar,
    Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    和 Andrea Michi. 从人类反馈中学习 Nash。2023年。arXiv:2312.00886。'
- en: '[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. Training
    language models to follow instructions with human feedback. In Neurips, volume 35,
    pages 27730–27744, 2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, 和 Alex Ray. 训练语言模型以遵循带有人类反馈的指令。在
    Neurips，第 35 卷，页码 27730–27744，2022。'
- en: '[29] Jim Pitman and Wenpin Tang. Regenerative random permutations of integers.
    Ann. Probab., 47(3):1378–1416, 2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Jim Pitman 和 Wenpin Tang. 整数的再生随机排列。Ann. Probab., 47(3):1378–1416, 2019。'
- en: '[30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. In Neurips, volume 36, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D Manning, 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型。在Neurips，第36卷，2023。'
- en: '[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. 2017. arXiv:1707.06347.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov.
    近端策略优化算法。2017。arXiv:1707.06347。'
- en: '[32] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li,
    and Houfeng Wang. Preference ranking optimization for human alignment. In AAAI,
    volume 38, pages 18990–18998, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li,
    和 Houfeng Wang. 用于人类对齐的偏好排序优化。在AAAI，第38卷，第18990–18998页，2024。'
- en: '[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize
    with human feedback. In Neurips, volume 33, pages 3008–3021, 2020.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, 和 Paul F Christiano. 学习总结与人类反馈。在Neurips，第33卷，第3008–3021页，2020。'
- en: '[34] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider,
    Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning
    of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367,
    2024.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider,
    Tengyang Xie, Stefano Ermon, Chelsea Finn, 和 Aviral Kumar. LLM的偏好微调应利用次优的、在政策内的数据。arXiv预印本
    arXiv:2404.14367, 2024。'
- en: '[35] Wenpin Tang. Mallows ranking models: maximum likelihood estimate and regeneration.
    In ICML, pages 6125–6134, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Wenpin Tang. Mallows排序模型：最大似然估计和再生。在ICML，第6125–6134页，2019。'
- en: '[36] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi
    Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires,
    and Bilal Piot. Generalized preference optimization: A unified approach to offline
    alignment. 2024. arXiv:2402.05749.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi
    Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires,
    和 Bilal Piot. 泛化偏好优化：离线对齐的统一方法。2024。arXiv:2402.05749。'
- en: '[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif
    Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier,
    and Nathan Habib. Zephyr: Direct distillation of LM alignment. 2023. arXiv:2310.16944.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif
    Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier,
    和 Nathan Habib. Zephyr：语言模型对齐的直接蒸馏。2023。arXiv:2310.16944。'
- en: '[38] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. Secrets of rlhf in large language
    models part II: Reward modeling. 2024. arXiv:2401.06080.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang,
    Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi 等。大型语言模型中RLHF的秘密第二部分：奖励建模。2024。arXiv:2401.06080。'
- en: '[39] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond
    reverse KL: Generalizing direct preference optimization with diverse divergence
    constraints. 2023. arXiv:2309.16240.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, 和 Yuxin Chen. 超越反向KL：通过多样的散度约束推广直接偏好优化。2023。arXiv:2309.16240。'
- en: '[40] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin
    Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization:
    Pushing the boundaries of LLM performance in machine translation. 2024. arXiv:2401.08417.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin
    Van Durme, Kenton Murray, 和 Young Jin Kim. 对比偏好优化：推动LLM在机器翻译中的性能边界。2024。arXiv:2401.08417。'
- en: '[41] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee,
    and Yi Ma. Investigating the catastrophic forgetting in multimodal large language
    models. 2023. arXiv:2309.10313.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee,
    和 Yi Ma. 研究多模态大语言模型中的灾难性遗忘。2023。arXiv:2309.10313。'
- en: '[42] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and
    Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. 2023.
    arXiv:2305.10425.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, 和
    Peter J Liu. Slic-hf：基于人类反馈的序列似然校准。2023。arXiv:2305.10425。'
- en: '[43] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of RLHF in large language
    models part I: PPO. 2023. arXiv:2307.04964.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang,
    Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou 等。大型语言模型中RLHF的秘密第一部分：PPO。2023。arXiv:2307.04964。'
- en: '[44] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford,
    Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models
    from human preferences. 2019. arXiv:1909.08593.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford,
    Dario Amodei, Paul Christiano, 和 Geoffrey Irving. 从人类偏好中微调语言模型。2019. arXiv:1909.08593.'
- en: Appendix / Supplemental Material
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 / 补充材料
- en: Appendix A Proofs
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 证明
- en: 'Proof of Theorem 1. For ([20](#S4.E20 "In Theorem 1 (Mallows-𝜃-DPO as dispersion
    weighted DPO). ‣ 4.1\. Dispersion weighted objectives ‣ 4\. Perspectives on Mallows-DPO
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")), the proof follows
    from the derivation of the equivalence between RLHF and DPO, as now the optimal
    policy satisfies'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 1 的证明。对于 ([20](#S4.E20 "在定理 1 (Mallows-𝜃-DPO 作为加权 DPO) 中。 ‣ 4.1\. 加权目标 ‣
    4\. Mallows-DPO 的观点 ‣ Mallows-DPO：通过偏好分散微调您的 LLM"))，证明遵循 RLHF 和 DPO 之间等价性的推导，因为现在最优策略满足
- en: '|  | $c(x)^{-1}r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid
    x)}+\beta\log Z(x),$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $c(x)^{-1}r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref }}(y\mid
    x)}+\beta\log Z(x),$ |  |'
- en: 'leading to the objective in ([12](#S3.E12 "In 3.2\. Mallows-DPO ‣ 3\. DPO based
    on Mallows Ranking Models ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")).
    The same argument establishes ([21](#S4.E21 "In Theorem 1 (Mallows-𝜃-DPO as dispersion
    weighted DPO). ‣ 4.1\. Dispersion weighted objectives ‣ 4\. Perspectives on Mallows-DPO
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")). $\blacksquare$'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 导致目标在 ([12](#S3.E12 "在 3.2\. Mallows-DPO ‣ 3\. 基于 Mallows 排名模型的 DPO ‣ Mallows-DPO：通过偏好分散微调您的
    LLM")) 中。相同的论点建立了 ([21](#S4.E21 "在定理 1 (Mallows-𝜃-DPO 作为加权 DPO) 中。 ‣ 4.1\. 加权目标
    ‣ 4\. Mallows-DPO 的观点 ‣ Mallows-DPO：通过偏好分散微调您的 LLM"))。$\blacksquare$
- en: Proof of Theorem 3. (i) With the Bradley-Terry connection as mentioned above,
    we have
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 3 的证明。 (i) 通过上述提到的 Bradley-Terry 连接，我们有
- en: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\Psi\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (29) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\Psi\left(p^{*}\left(y_{1}\succ
    y_{2}\mid x\right)\right)\right]$ |  | (29) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\left(r(x,y_{1})-r\left(x,y_{2}\right)\right)\right]$
    |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{y_{2}\sim\tilde{\pi}}\left[f(x)\left(r(x,y_{1})-r\left(x,y_{2}\right)\right)\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: which is a weighted reward of DPO, up to an additive constant. It follows that
    the optimal policy of the generalized $\Psi$
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个加权奖励的 DPO，加上一个附加常数。因此，广义的 $\Psi$ 的最优策略
- en: Appendix B Experimental Details
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验细节
- en: B.1\. Bandit Experiment
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. 赌博机实验
- en: 'In the bandit experiment detailed in Section [5.2](#S5.SS2 "5.2\. Mallows-ϕ-DPO
    mitigates reward collapse ‣ 5\. Experiments ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions"), we conduct two sub-experiments to compute the efficient
    frontiers using Mallow-$\phi$ values required to compute the full efficient frontier,
    and for each $\beta$ among the four policies for every $100$ training steps. Given
    that we know the real reward distribution, all these quantities can be computed
    analytically.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5.2](#S5.SS2 "5.2\. Mallows-ϕ-DPO 缓解奖励崩溃 ‣ 5\. 实验 ‣ Mallows-DPO：通过偏好分散微调您的
    LLM") 节中详细描述的赌博机实验中，我们进行两个子实验以计算使用 Mallow-$\phi$ 值所需的有效前沿，并且对每个 $\beta$ 在每 $100$
    训练步骤中进行计算。鉴于我们知道真实的奖励分布，所有这些量都可以通过分析计算得出。
- en: In terms of the training details, we use all 16 data in a single batch and adopts
    SGD as the optimizer, with learning rate of 5e-3. To ensure convergence, we run
    the optimization for a large number of epochs, set to 500,000. For Mallows-$\phi$
    to be 0.05.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 就训练细节而言，我们在一个批次中使用所有 16 个数据，并采用 SGD 作为优化器，学习率为 5e-3。为了确保收敛，我们将优化运行大量的 epoch，设置为
    500,000。对于 Mallows-$\phi$ 设置为 0.05。
- en: Table 2. Reward distributions of the five arms.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. 五个臂的奖励分布。
- en: '| Arm 1 | Arm 2 | Arm 3 | Arm 4 | Arm 5 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 臂 1 | 臂 2 | 臂 3 | 臂 4 | 臂 5 |'
- en: '| Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward | Prob. | Reward
    | Prob. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | 概率 | 奖励 | 概率 | 奖励 | 概率 | 奖励 | 概率 | 奖励 | 概率 |'
- en: '| 20 | 0.5 | 30 | 0.5 | 18 | 0.5 | 15 | 0.99 | 1 | 0.99 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.5 | 30 | 0.5 | 18 | 0.5 | 15 | 0.99 | 1 | 0.99 |'
- en: '| 11 | 0.5 | 3 | 0.5 | 15 | 0.5 | 10 | 0.01 | 4 | 0.01 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.5 | 3 | 0.5 | 15 | 0.5 | 10 | 0.01 | 4 | 0.01 |'
- en: Table 3. 16 pairs of sampled preference data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 16 对采样的偏好数据。
- en: '| Win | 3 | 2 | 2 | 1 | 3 | 1 | 1 | 1 | 4 | 2 | 2 | 2 | 1 | 3 | 3 | 4 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 赢 | 3 | 2 | 2 | 1 | 3 | 1 | 1 | 1 | 4 | 2 | 2 | 2 | 1 | 3 | 3 | 4 |'
- en: '| Lose | 5 | 5 | 5 | 2 | 5 | 5 | 4 | 5 | 5 | 4 | 1 | 5 | 3 | 5 | 4 | 2 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 输 | 5 | 5 | 5 | 2 | 5 | 5 | 4 | 5 | 5 | 4 | 1 | 5 | 3 | 5 | 4 | 2 |'
- en: B.2\. Controllable Generation Experiment Details
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 可控生成实验细节
- en: 'We follow the training setup in [[30](#bib.bib30)], and first fine-tune GPT-2-large
    on the training split of IMDB datasets until convergence to get the SFT model.
    The next step is different from [[30](#bib.bib30)] in that we directly utilize
    the (offline) preference dataset from [[39](#bib.bib39)] instead of generating
    pairwise preferences from the trained SFT model, as in DPO. The rest is the same:
    we use the pairwise preference data to fine-tune the SFT model by either DPO or
    Mallows-DPO. The evaluation metric: accuracy is obtained from a prior sentiment
    classifier as the ground truth reward. By default, we use RMSprop optimizer with
    a learning rate of 1e-6, with a linear learning rate warmup from 0 to 1e-6 over
    the first 150 steps. The training batch size is 64.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了[[30](#bib.bib30)]中的训练设置，首先在IMDB数据集的训练分割上对GPT-2-large进行微调，直至收敛，以获得SFT模型。下一步与[[30](#bib.bib30)]不同的是，我们直接利用[[39](#bib.bib39)]的（离线）偏好数据集，而不是像DPO中那样从训练好的SFT模型中生成成对偏好数据。其余部分相同：我们使用成对偏好数据通过DPO或Mallows-DPO微调SFT模型。评价指标：准确率来自先前的情感分类器作为真实奖励。默认情况下，我们使用学习率为1e-6的RMSprop优化器，在前150步内进行从0到1e-6的线性学习率预热。训练批量大小为64。
- en: B.3\. Language Modeling Experiment Details
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3\. 语言建模实验细节
- en: We follow the training setup in [[30](#bib.bib30)]. By default, we use RMSprop
    optimizer with a learning rate of 1e-6, with a linear learning rate warmup from
    0 to 1e-6 over the first 150 steps. The training batch size is 32.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了[[30](#bib.bib30)]中的训练设置。默认情况下，我们使用学习率为1e-6的RMSprop优化器，在前150步内进行从0到1e-6的线性学习率预热。训练批量大小为32。
- en: B.3.1\. GPT-4 Judgement Prompt
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1\. GPT-4判断提示
- en: 'Response quality evaluation is completed by GPT-4\. The prompt for instructing
    GPT-4 to evaluate which response is better is particularly important. Thus, we
    use the fastchat package for GPT-4 evaluation, and we used their well-written
    pair-v2 judge prompt. The prompt is shown as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 响应质量评估由GPT-4完成。指示GPT-4评估哪个响应更好的提示非常重要。因此，我们使用了用于GPT-4评估的fastchat包，并使用了他们编写良好的pair-v2
    judge提示。提示如下所示：
- en: 'Please act as an impartial judge and evaluate the quality of the responses
    provided by two AI assistants to the user question displayed below. You should
    choose the assistant that follows the user’s instructions and answers the user’s
    question better. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their responses.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any position biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor certain names of the assistants. Be
    as objective as possible. After providing your explanation, output your final
    verdict by strictly following this format: \‘‘ [[A]]\’’ if assistant A is better,
    \‘‘[[B]]\’’ if assistant B is better, and \‘‘[[C]]\’’ for a tie."'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 请作为公正的裁判，评估两个AI助手对下列用户问题提供的响应质量。你应该选择遵循用户指令并更好地回答用户问题的助手。你的评估应考虑他们响应的有用性、相关性、准确性、深度、创造力和细节水平。开始评估时请比较两个响应，并提供简短的解释。避免任何位置偏见，确保响应的顺序不会影响你的决定。不要让响应的长度影响你的评估。不要偏袒某些助手的名字。尽可能客观。在提供解释后，严格按照以下格式输出最终裁决：\‘‘
    [[A]]\’’ 如果助手A更好，\‘‘[[B]]\’’ 如果助手B更好，\‘‘[[C]]\’’ 如果平局。
- en: 'To ensure fairness and unbiasedness, for each pairwise input $(x,y_{1},y_{2})$
    wins if and only if it wins both comparisons, or wins one comparison while the
    other is tied. We compute win rate as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保公平性和无偏性，对于每对输入$(x,y_{1},y_{2})$，仅当它在两个比较中都获胜，或者在一个比较中获胜而另一个比较平局时，才算胜出。我们按如下方式计算胜率：
- en: '|  | $\text{Win rate (Model A)}=\frac{\text{Number of samples where Model A
    wins}}{\text{Total number of test samples}}+0.5\times\frac{\text{Number of tied
    samples}}{\text{Total number of test samples}}$ |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{胜率（模型A）}=\frac{\text{模型A获胜的样本数量}}{\text{测试样本总数}}+0.5\times\frac{\text{平局样本数量}}{\text{测试样本总数}}$
    |  |'
- en: Appendix C Qualitative Examples
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 定性示例
- en: 'In this section, we present a series of comparisons between Mallows-DPO variants
    and BT-DPO, as shown in Tables [4](#A3.T4 "Table 4 ‣ Appendix C Qualitative Examples
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[15](#A3.T15 "Table
    15 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions"). These tables demonstrate the qualitative examples of responses
    to in-distribution inputs from the Anthropic-HH test set, to out-of-distribution
    inputs from the SHP test set, and to out-of-distribution inputs from the SE dataset
    respectively.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们呈现了一系列对比，包括Mallows-DPO变体与BT-DPO的比较，如表[4](#A3.T4 "表4 ‣ 附录C 定性示例 ‣ Mallows-DPO:
    使用偏好分散微调您的LLM")–[15](#A3.T15 "表15 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")所示。这些表格展示了对来自Anthropic-HH测试集的分布内输入、来自SHP测试集的分布外输入和来自SE数据集的分布外输入的响应的定性示例。'
- en: 'To interpret, Tables [4](#A3.T4 "Table 4 ‣ Appendix C Qualitative Examples
    ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[5](#A3.T5 "Table
    5 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO and Mallows-$\phi$-DPO,
    their responses do not deviate too much from the user prompt and remain meaningful.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '解释来说，表[4](#A3.T4 "表4 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")–[5](#A3.T5 "表5
    ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")显示，Mallows-$\theta$-DPO和Mallows-$\phi$-DPO微调的模型，其响应与用户提示没有太大偏离，并且保持了有意义性。'
- en: 'On the contrary, the examples in Tables [8](#A3.T8 "Table 8 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[11](#A3.T11
    "Table 11 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") indicate that models of Mallows-$\theta$-DPO in
    Tables [8](#A3.T8 "Table 8 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune
    Your LLM with Preference Dispersions")–[9](#A3.T9 "Table 9 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions") include
    peer feedback and additional suggestions, like double majoring, regarding a question
    of major transferring. Tables [10](#A3.T10 "Table 10 ‣ Appendix C Qualitative
    Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[11](#A3.T11
    "Table 11 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM
    with Preference Dispersions") show that models fine-tuned by Mallows-$\theta$-DPO
    demonstrate strong knowledge in history and philosophy, by providing more related
    details and supportive arguments, compared to BT-DPO.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '相反，表[8](#A3.T8 "表8 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")–[11](#A3.T11 "表11
    ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")中的示例表明，Mallows-$\theta$-DPO模型在表[8](#A3.T8
    "表8 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")–[9](#A3.T9 "表9 ‣ 附录C 定性示例 ‣ Mallows-DPO:
    使用偏好分散微调您的LLM")中包括了同行反馈和额外建议，比如双学位，关于专业转移的问题。表[10](#A3.T10 "表10 ‣ 附录C 定性示例 ‣ Mallows-DPO:
    使用偏好分散微调您的LLM")–[11](#A3.T11 "表11 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")显示，Mallows-$\theta$-DPO微调的模型展示了在历史和哲学方面的强大知识，通过提供更多相关细节和支持性论据，与BT-DPO相比表现更佳。'
- en: 'Finally, we also present examples for answering questions regarding coding
    in Tables [12](#A3.T12 "Table 12 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO:
    Fine-Tune Your LLM with Preference Dispersions")–[15](#A3.T15 "Table 15 ‣ Appendix
    C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions"),
    among which Tables [12](#A3.T12 "Table 12 ‣ Appendix C Qualitative Examples ‣
    Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions")–[13](#A3.T13 "Table
    13 ‣ Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") show that Mallows-$\theta$-DPO in Table [15](#A3.T15 "Table 15 ‣
    Appendix C Qualitative Examples ‣ Mallows-DPO: Fine-Tune Your LLM with Preference
    Dispersions") provides additional code to support the user, complementing the
    suggestions by BT-DPO.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们还展示了回答编码相关问题的示例，这些示例见表[12](#A3.T12 "表12 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")–[15](#A3.T15
    "表15 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")，其中表[12](#A3.T12 "表12 ‣ 附录C 定性示例
    ‣ Mallows-DPO: 使用偏好分散微调您的LLM")–[13](#A3.T13 "表13 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")显示，Mallows-$\theta$-DPO在表[15](#A3.T15
    "表15 ‣ 附录C 定性示例 ‣ Mallows-DPO: 使用偏好分散微调您的LLM")中提供了额外的代码来支持用户，补充了BT-DPO的建议。'
- en: Table 4. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\theta$.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. 来自 Anthropic-HH 测试集的提示示例响应。GPT-4 选择了 Mallows-$\theta$。
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 她喜欢宾果游戏、编织、购物、她的孙子孙女、咖啡。 |'
- en: '| Mallows-$\theta$-DPO | Grandma loves receiving special gifts from family
    members, so an elegant, quality, personalized hand knit sweater is a great idea.
    She loves warm, cozy clothing, so a cashmere sweater is a nice gesture, or a cozy
    pullover knit sweater, for a cozy fall or winter wardrobe. Some other ideas are
    a personalized coffee mug, a knitting patterns, and a hand knit tea cozy. Also
    a small ornament from a special hand knit sweater is a nice gift. If you know
    your grandma’s interests, you can even surprise her by shopping together, so you
    can pick up special gifts for her such as a cookbook for baking, or a knitting
    pattern book, or a coffee mug, or a tea cozy. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 奶奶喜欢收到来自家人的特别礼物，所以一件优雅、优质、个性化的手工编织毛衣是一个不错的主意。她喜欢温暖、舒适的衣物，所以一件羊绒毛衣是一个体贴的选择，或者是一件舒适的套头毛衣，适合秋冬季节的衣橱。其他的一些想法包括个性化的咖啡杯、编织图案和手工编织的茶杯套。此外，一件特别手工编织毛衣的小饰品也是很好的礼物。如果你知道奶奶的兴趣爱好，你甚至可以通过一起购物来给她惊喜，这样你可以为她挑选特别的礼物，比如一本烘焙食谱、一本编织图案书、一只咖啡杯或一个茶杯套。
    |'
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentine’s Day. You could knit
    her a Valentine’s gift like, a Valentine’s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandma’s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Let’s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Let’s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Here’s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so let’s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, let’s see, could be the one with a
    Bible verse on the front or the one with Glover. So it’s a hand knit sweater with
    a Bible verse on the front. Here’s a list of specific Bible verses that people
    would like Grandma to learn: “I shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 奶奶喜欢收到家人的特别礼物，所以一件优雅、优质、个性化的手工编织毛衣是一个很好的主意。她可能会喜欢一杯好咖啡和美味的餐点，或者一本书和一些花。如果你有特别的技能，你可以在圣诞节为她编织一件毛衣，甚至可以在她生日时送她一件毛衣。你也可以选择其他特别的场合，比如情人节。你可以为她编织一个情人节礼物，比如情人节戒指或手链。或者你可以制作一件节日毛衣，为圣诞节编织，这样她可以在年底放松身心。如果她有很多朋友，你可以为她编织一件特别的手工毛衣，供她的朋友们一起享用。如果你认识她，可能是在教堂或体育活动中，你可以为她编织一件以体育为主题的毛衣。或者你可以在前面加上圣经经文的毛衣，就像Myron为他的奶奶做的毛衣那样。制作这件毛衣时，你可以自由发挥，所以你甚至可以制作一件带有圣经经文的毛衣，就像Myron为他的奶奶做的那样。要制作这件毛衣，你可以编织一件带有圣经经文的毛衣，或者编织一件像奶奶最喜欢的毛衣一样的毛衣，或者编织一件前面有圣经经文的毛衣。所以Myron给他的奶奶做了一件带有圣经经文的毛衣。你也可以给奶奶做一件带有圣经经文的毛衣。奶奶喜欢给家人和朋友送特别的礼物，所以我推荐一件前面有圣经经文的毛衣。或者你可以做一件你自己特别的礼物的毛衣。你想让我帮忙吗？我们来看一下，你想要一件手工编织的毛衣，由编织机编织，你只想要一件毛衣，并且你想为奶奶编织，怎么样？假设你想编织一件毛衣然后亲手交给她。这样可以吗？让我列一个毛衣的可能性清单，我们可以选择一个或几个特别的选项。以下是最匹配的项目清单：圣经经文
    Glover Shawnee 奶奶想更多地了解圣经经文，所以我们来具体看看。奶奶会更喜欢唯一的一件毛衣，你可以选择一件，比如前面有圣经经文的毛衣或有Glover的毛衣。所以这是一件前面有圣经经文的手工编织毛衣。以下是人们希望奶奶学习的具体圣经经文列表：“我将把我所有的繁荣告诉我的主王。我将说，我将对你说平安。我将向穷人宣告好消息。我将宣扬恩典的福音。以下是人们希望奶奶学习的具体圣经经文：要有光！要有光！要有光！-
    - - 要有光！让 [最大生成长度已达到]'
- en: '| Judgment | Assistant A’s response is more relevant and helpful to the user’s
    statement. The assistant suggests personalized gifts that align with the grandmother’s
    interests, such as a hand knit sweater, a personalized coffee mug, a knitting
    pattern book, and a tea cozy. The assistant also suggests shopping together as
    a way to spend quality time and pick out special gifts. On the other hand, Assistant
    B’s response is repetitive and confusing. It focuses heavily on knitting a sweater
    with a Bible verse, which was not mentioned in the user’s statement. The assistant
    also suggests knitting a sweater for the grandmother’s friends and for people
    from church or sports, which is not relevant to the user’s statement. The assistant’s
    suggestions for Bible verses are also not relevant to the user’s statement. Therefore,
    Assistant A’s response is more helpful, relevant, and accurate. It provides a
    variety of gift ideas that align with the grandmother’s interests and suggests
    spending quality time together. Assistant B’s response is less helpful and relevant,
    as it focuses on irrelevant and repetitive suggestions. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助理A的回应更相关且对用户的陈述更有帮助。助理建议了与奶奶兴趣相符的个性化礼物，如手工编织的毛衣、个性化咖啡杯、编织图案书和茶壶套。助理还建议一起购物，以便花时间挑选特别的礼物。另一方面，助理B的回应重复且令人困惑。它过于关注编织一件带有圣经经文的毛衣，而这在用户的陈述中没有提及。助理还建议为奶奶的朋友以及教会或运动中的人编织毛衣，这与用户的陈述无关。助理提出的圣经经文建议也与用户的陈述不相关。因此，助理A的回应更为有帮助、相关且准确。它提供了与奶奶兴趣相符的多种礼物创意，并建议一起度过美好时光。助理B的回应则较少有帮助和相关，因为它集中在无关且重复的建议上。
    |'
- en: Table 5. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses Mallows-$\phi$.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表5. 来自Anthropic-HH测试集的提示样本回应。GPT-4选择了Mallows-$\phi$。
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 她喜欢宾果游戏、编织、购物、她的孙子孙女、咖啡。 |'
- en: '| Mallows-$\phi$-DPO | Grandma loves receiving special gifts from family members,
    so an elegant, quality bingo or knitting set is a great idea. She also loves coffee,
    so a nice coffee table book with coffee-themed photos and a small mug is a nice
    gift. Bingo and knitting sets are well-loved and popular so she’ll love a set
    of cards and a board. A photo album is a great idea too, so she can look at pictures
    of her family. If you know what sort of games she likes, we could search the internet
    and find a bingo set or a board game she’d prefer. It also doesn’t hurt to give
    her coffee while you make your gift, so you could leave a cup of hot coffee. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 奶奶喜欢收到家人送的特别礼物，因此一个优雅高质量的宾果游戏或编织套件是一个很好的主意。她也喜欢咖啡，所以一本咖啡主题的精美咖啡桌书和一个小杯子是不错的礼物。宾果游戏和编织套件都非常受欢迎，她一定会喜欢一副卡片和一个游戏板。相册也是一个很好的主意，她可以查看家庭的照片。如果你知道她喜欢什么游戏，我们可以在网上寻找她可能喜欢的宾果游戏或棋盘游戏。制作礼物时赠送她咖啡也不错，你可以留下一杯热咖啡。
    |'
- en: '| BT-DPO | Grandma loves receiving special gifts from family members, so an
    elegant, quality, personalized hand knit sweater is a great idea. She might enjoy
    a good coffee and a tasty meal, or a book and flowers. If you have a special skill,
    you could knit her a sweater for Christmas or even a sweater for her birthday.
    You can choose a different special occasion, like Valentine’s Day. You could knit
    her a Valentine’s gift like, a Valentine’s ring or bracelet. Or you could make
    a holiday sweater and knit it for Christmas, so she can relax and unwind at the
    end of the year. If she has a lot of friends, you could make her a special hand
    knit sweater and knit it for all her friends to enjoy. If you know her from church
    or play sports, you could knit her a sports themed sweater. Or you could make
    a sweater with a Bible verse on the front, like the sweater Myron did for his
    grandmother. When you make the sweater, you make it all up, so you could even
    make a sweater with a Bible verse on the front, like the sweater that Myron did
    for his grandmother. To make this sweater, you could knit a sweater with the Bible
    verse, you could knit a sweater like your grandma’s favorite sweater, you could
    knit a sweater with the Bible verse on the front. So Myron made his grandmother
    a sweater with the Bible verse. You could make your grandma a sweater with the
    Bible verse. Grandma loves giving special gifts to family and friends, so I recommend
    a sweater with a Bible verse on the front. Or you could make her a sweater with
    your own special gift. Would you like me to help you with this? Let’s see, you
    want a hand knit sweater, knit by a knitting machine, you want only one sweater
    and you want to knit it for your grandma, how about that? Let’s say you want to
    knit up a sweater and hand it to her. Is that ok? Let me make a list of possibilities
    for your sweater, we can pick one or a few that might be special. Here’s a list
    of items that are the best match: Bible verse Glover Shawnee Grandma would like
    to know more about the Bible verse, so let’s get specific. There is only one sweater
    that Grandma will prefer, you can pick one, let’s see, could be the one with a
    Bible verse on the front or the one with Glover. So it’s a hand knit sweater with
    a Bible verse on the front. Here’s a list of specific Bible verses that people
    would like Grandma to learn: “I shall tell my lord the king all the prosperity
    that I have. I shall say, I shall say to you of peace. I shall declare the good
    news to the poor. I shall declare the gospel of grace. Here is a list of specific
    Bible verses that people would like Grandma to learn: Let there be light! Let
    there be light! Let there be light! - - - Let there be light! Let [maximum generation
    length reached] |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 奶奶喜欢收到家人送的特别礼物，因此一件优雅、高品质、个性化的手工编织毛衣是个好主意。她可能会喜欢一杯好咖啡和一顿美餐，或者一本书和一些花。如果你有特别的技能，你可以在圣诞节为她编织一件毛衣，甚至可以为她的生日编织一件毛衣。你也可以选择其他特别的节日，比如情人节。你可以编织一份情人节礼物，比如情人节戒指或手链。或者你可以制作一件节日毛衣，编织成圣诞节的毛衣，这样她可以在年底放松一下。如果她有很多朋友，你可以为她编织一件特别的手工毛衣，编织成她所有朋友都喜欢的样子。如果你认识她来自教堂或者参加体育活动，你可以为她编织一件运动主题的毛衣。或者你可以制作一件前面印有圣经经文的毛衣，就像迈伦为他奶奶做的那件毛衣一样。当你制作毛衣时，你可以完全自创设计，因此你甚至可以制作一件前面印有圣经经文的毛衣，就像迈伦为他奶奶做的那件毛衣一样。为了制作这件毛衣，你可以编织一件带有圣经经文的毛衣，或者编织一件像你奶奶最喜欢的毛衣那样的毛衣，或者编织一件前面带有圣经经文的毛衣。因此，迈伦为他的奶奶做了一件带有圣经经文的毛衣。你也可以为你的奶奶编织一件带有圣经经文的毛衣。奶奶喜欢给家人和朋友送特别的礼物，因此我推荐一件前面带有圣经经文的毛衣。或者你可以为她制作一件你自己的特别礼物毛衣。你希望我帮忙吗？让我们看看，你想要一件手工编织的毛衣，通过编织机编织，你只要一件毛衣，并且你想要为你的奶奶编织，这样怎么样？假设你想要编织一件毛衣并亲手递给她。这样可以吗？让我列一个毛衣的可能性清单，我们可以挑选一个或几个可能特别的选择。以下是最佳匹配的项目列表：圣经经文格洛弗肖尼奶奶想要了解更多的圣经经文，所以让我们具体一下。只有一件毛衣是奶奶会更喜欢的，你可以挑选一个，让我们看看，可能是前面有圣经经文的那件，或者是有格洛弗的那件。所以是一件前面有圣经经文的手工编织毛衣。以下是人们希望奶奶学习的具体圣经经文列表：“我将告诉我主王所有我拥有的繁荣。我将说，我将对你说平安。我将向穷人传递好消息。我将宣讲恩典的福音。”以下是人们希望奶奶学习的具体圣经经文列表：要有光！要有光！要有光！
    - - - 要有光！[最大生成长度已达到]'
- en: '| Judgment | Assistant A’s response is more relevant and helpful to the user’s
    statement. The user mentioned that the person in question likes bingo, knitting,
    shopping, her grandkids, and coffee. Assistant A took all these interests into
    account and suggested appropriate gifts such as a bingo or knitting set, a coffee
    table book, a mug, and a photo album. On the other hand, Assistant B focused solely
    on knitting and suggested making a hand-knit sweater with a Bible verse on it.
    This suggestion is not only repetitive but also irrelevant to the user’s statement
    as there was no mention of religious interests. Furthermore, Assistant B’s response
    is confusing and lacks coherence, making it less helpful to the user. Therefore,
    Assistant A provided a better response. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 的回应对用户的陈述更相关且更有帮助。用户提到她喜欢宾果游戏、编织、购物、孙子孙女和咖啡。助手 A 考虑了所有这些兴趣，并建议了合适的礼物，例如宾果游戏或编织套件、咖啡桌书、马克杯和相册。另一方面，助手
    B 仅关注编织，并建议制作一件上面印有圣经经文的手工编织毛衣。这个建议不仅重复，而且与用户的陈述无关，因为没有提到宗教兴趣。此外，助手 B 的回应令人困惑且缺乏连贯性，因此对用户的帮助较少。因此，助手
    A 提供了更好的回应。 |'
- en: Table 6. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\theta$.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. 来自 Anthropic-HH 测试集的示例回应。GPT-4 选择了 BT-DPO 而非 Mallows-$\theta$。
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 她喜欢宾果游戏、编织、购物、孙子孙女、咖啡。 |'
- en: '| Mallows-$\theta$-DPO | This is a difficult question! Let me start by saying
    it is true that an elderly person can often enjoy some or all of these things,
    and they might enjoy doing them for many reasons. However, it can sometimes be
    difficult for them to enjoy them for very long, since being able to do them is
    dependent on so many things that can be difficult to take for granted. For example,
    a person can’t get used to the fact that their body is breaking down, or that
    they might not be able to do something at high level for the next few years, so
    they might get bored with some of the things they used to like. However, it is
    still important to have a range of things that they can do and enjoy. If you could
    suggest a Christmas gift that would suit her interests, I could look at some ideas.
    The best gift would have some sort of physical thing that she could use to do,
    like knitting or bingo. Maybe an app would be more like a book that she could
    read? You can help her find games and things like that to play, so it is not just
    about reading? |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 这是一个难题！让我首先说，的确老年人通常会喜欢这些东西中的一些或全部，他们可能因多种原因而喜欢做这些事。然而，有时候他们可能无法长时间享受这些事，因为能够做这些事依赖于许多难以理所当然的因素。例如，人们可能无法适应身体逐渐衰退的事实，或者他们可能在未来几年里无法以高水平完成某些事情，因此他们可能会对曾经喜欢的事情感到厌倦。然而，仍然重要的是提供一系列他们可以做和享受的活动。如果你能建议一个适合她兴趣的圣诞礼物，我可以考虑一些想法。最佳礼物应该包含一些她可以使用的实际物品，比如编织或宾果游戏。也许一个应用程序更像是一本她可以阅读的书？你可以帮助她找到游戏和类似的东西来玩，这样就不仅仅是阅读了？'
- en: '| BT-DPO | Well, I’d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 好吧，我会说一个好的礼物可能是订阅一本杂志或在线杂志，或者在线宾果游戏或在线编织社区。或者你可以送她一个实体宾果卡的订阅，或一些纱线以及一个编织项目的图案。或者你可以买些咖啡给她，并在她的宾果卡或编织项目上写上她的名字。或者你可以送她一本关于宾果的书，或者宾果游戏，或者一个编织项目。或者你可以给她做一个礼物，比如一个陶艺碗。
    |'
- en: '| Judgment | Assistant B’s response is more relevant and helpful to the user’s
    question. Although the user’s question is not explicitly stated, it can be inferred
    that the user is looking for gift suggestions for a person who enjoys bingo, knitting,
    shopping, her grandkids, and coffee. Assistant B provides a variety of gift suggestions
    that align with the person’s interests, such as a subscription to an online magazine,
    a physical bingo card, yarn for knitting, coffee, and a book about bingo. On the
    other hand, Assistant A’s response is less focused and does not provide as many
    concrete gift suggestions. Therefore, Assistant B’s response is more helpful and
    relevant. On the other hand, Assistant A’s response is less focused and does not
    provide as many concrete gift suggestions. Therefore, Assistant B’s response is
    more helpful and relevant. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助理 B 的回应更贴切并且对用户的问题更有帮助。虽然用户的问题没有明确陈述，但可以推断出用户正在寻找适合喜欢宾果游戏、编织、购物、孙子孙女和咖啡的人群的礼物建议。助理
    B 提供了多种符合该人士兴趣的礼物建议，如在线杂志订阅、实物宾果卡、编织用毛线、咖啡和有关宾果的书籍。另一方面，助理 A 的回应较为分散，并没有提供那么多具体的礼物建议。因此，助理
    B 的回应更为有用和相关。另一方面，助理 A 的回应较为分散，并没有提供那么多具体的礼物建议。因此，助理 B 的回应更为有用和相关。 |'
- en: Table 7. Sample responses to a prompt from the Anthropic-HH test set. GPT-4
    chooses BT-DPO over Mallows-$\phi$.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 来自 Anthropic-HH 测试集的提示示例响应。GPT-4 选择了 BT-DPO 而非 Mallows-$\phi$。
- en: '| Prompt | She likes bingo, knitting, shopping, her grandkids, coffee. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 她喜欢宾果游戏、编织、购物、孙子孙女、咖啡。 |'
- en: '| Mallows-$\phi$-DPO | This is a good list of categories to check when buying
    for an elderly person, but there are many more options. You could check out whether
    she enjoys gardening and flowers, or if she likes baking and cooking. If you have
    a special skill you can try to learn more about her hobby or skills, and make
    her a special item that she can use to improve her skills. For example, are your
    grandma into sports? Does she like to play a musical instrument? If so, you can
    make a special instrument for her, and the instrument can be a tool to help her
    develop her passion for music. Similarly, if she enjoys baking, you could make
    something for her to make, to use in her kitchen. There are all kinds of ways
    you could find out more about her interests, and make them something she’ll enjoy.
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 这是一个在为年长者购买礼物时检查的好类别列表，但还有许多其他选项。你可以看看她是否喜欢园艺和花卉，或者她是否喜欢烘焙和烹饪。如果你有特别的技能，你可以尝试了解她的爱好或技能，并制作一个特别的物品，帮助她提高技能。例如，你的奶奶喜欢运动吗？她喜欢演奏乐器吗？如果是这样，你可以为她制作一个特别的乐器，这个乐器可以帮助她发展对音乐的热情。类似地，如果她喜欢烘焙，你可以为她制作一些厨房用具。你可以通过各种方式了解她的兴趣，并制作一些她会喜欢的东西。
    |'
- en: '| BT-DPO | Well, I’d say a good gift might be a subscription to a magazine
    or an online magazine, or an online bingo game or an online knitting community.
    Or you could give her a subscription to a physical bingo card, or some yarn, and
    a pattern for a knitting project. Or you could buy her coffee, and put her name
    on her bingo card, or a knitting project. Or you could give her a book about bingo,
    or bingo, or a knitting project. Or you could just make her a present, like a
    pottery bowl. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 嗯，我觉得一个好的礼物可能是杂志或在线杂志的订阅，或者在线宾果游戏或在线编织社区。或者你可以给她一个实物宾果卡的订阅，或者一些毛线和编织项目的图案。或者你可以买咖啡，给她的宾果卡上写上她的名字，或者编织项目。或者你可以给她一本关于宾果的书，或者宾果游戏，或者一个编织项目。或者你可以给她做一个礼物，比如一个陶瓷碗。
    |'
- en: '| Judgment | Assistant B’s response is more relevant and helpful to the user’s
    question. The user didn’t ask a question, but it seems like they’re looking for
    gift ideas for a woman who likes bingo, knitting, shopping, her grandkids, and
    coffee. Assistant B provides specific gift ideas that align with the woman’s interests,
    such as a subscription to an online bingo game or knitting community, a physical
    bingo card, yarn and a pattern for a knitting project, coffee, or a book about
    bingo or knitting. On the other hand, Assistant A suggests finding out more about
    the woman’s interests, which doesn’t directly address the user’s implied question.
    Therefore, Assistant B’s response is more helpful and relevant. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 B 的回应对用户的问题更为相关和有帮助。用户没有明确提问，但似乎在寻找适合喜欢宾果游戏、编织、购物、孙子孙女和咖啡的女性的礼物创意。助手
    B 提供了与女性兴趣相符的具体礼物创意，如在线宾果游戏或编织社区的订阅、纸质宾果卡、编织项目所需的纱线和图案、咖啡或有关宾果游戏或编织的书籍。另一方面，助手
    A 建议进一步了解女性的兴趣，这并没有直接解决用户的隐含问题。因此，助手 B 的回应更为有用和相关。 |'
- en: Table 8. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8. 来自斯坦福人类偏好数据集测试集的示例回应。GPT-4 在 Anthropic-HH 数据集中选择了 Mallows-$\theta$。
- en: '| Prompt | I don’t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i don’t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while I’m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 提问 | 我不想成为工程师，我喜欢物理。你好，我现在在高中，家里有一些经济问题，我的父母对我学习物理（即获得学位并追求更高的学业）不满意，因为我也在考虑做工程学，因为这已经成为一种需要。但我不想离开物理，我非常热爱原子物理和宇宙学。我想在这里问一下，在我学习工程（IT）的同时，我是否可以继续追求对物理的热情，会有时间投入到我的热情中吗？或者还有没有其他途径让我追求我的热情并改善家里的经济状况？
    |'
- en: '| Mallows-$\theta$-DPO | There is a way you can pursue your interest in physics
    while continuing to do engineering! Let’s take a step back and understand more
    about what motivates you to study physics, what are you learning that you enjoy,
    and what career would you like to go into? It’s quite possible that you could
    pursue your interest in physics even while working with technologies like computers
    or electronics and electronics, but these fields don’t always allow for the kind
    of hands-on work or the level of interactivity that you are looking for in physics
    and astrophysics. Here’s some feedback from my peers who have made the transition:
    "I initially studied physics in school to be a medical physicist. When I realized
    I only wanted to pursue a career in physics, I worked full-time in the physics
    field and went to graduate school in another field, to focus on physics." "I studied
    Physics A in university, then switched to engineering to do physics B. Now I want
    to come back to physics but take a break to have more time with my kids. I love
    Physics!" "For me, physics is primarily about learning concepts and techniques
    from a particular area. After taking my first physics exam in university, I fell
    in love with the process and started doing physics outside of school, with friends
    and family and working." You can do these things. So maybe don’t let your family
    hold you back from continuing your passion. You could take courses in basic quantum
    mechanics and relativity and cosmology to learn about the universe and why it’s
    the way it is. Just think about what you want to do and how you could combine
    physics with your interests! |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 有一种方法可以让你在继续从事工程工作时追求你对物理的兴趣！让我们退一步，更多地了解是什么激励你学习物理，你喜欢学习哪些内容，以及你希望从事什么职业？即使在使用计算机或电子技术的领域，你也有可能继续追求对物理的兴趣，但这些领域并不总是提供你在物理和天体物理学中寻找的那种动手操作或互动水平。以下是一些已经转行的同行的反馈：
    “我最初在学校学习物理是为了成为一名医学物理学家。当我意识到我只想从事物理领域的职业时，我在物理领域全职工作，并在另一个领域上研究生，以专注于物理学。” “我在大学学习了物理A，然后转到工程学去学习物理B。现在我想回到物理领域，但需要休息一下，以便有更多时间陪伴我的孩子。我爱物理！”
    “对我来说，物理主要是学习特定领域的概念和技术。在大学参加了第一次物理考试后，我爱上了这个过程，并开始在学校外与朋友、家人一起做物理，并工作。” 你可以做这些事情。所以，也许不要让你的家庭阻碍你继续追求你的热情。你可以学习基本的量子力学、相对论和宇宙学课程，了解宇宙及其存在的原因。只需考虑一下你想做什么，以及你如何将物理与自己的兴趣结合起来！'
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day it’s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. It’s
    a great thing, if you’re interested in physics, to be able to pursue it while
    doing something else. I’m always happy to be a resource and answer any specific
    questions you might have if you don’t yet know how to work it out. Here’s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - it’s
    like a food for the mind - so if you can’t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 是的，你绝对可以同时做物理和工程学。很多人都这样做。最终这取决于你喜欢用什么来度过生活，大多数人发现他们的热情和工作是他们想做的事。如果你对物理感兴趣，能在做其他事情的同时追求物理学是一件很棒的事。如果你不确定如何解决这个问题，我很乐意作为资源回答你可能有的任何具体问题。以下是一些需要考虑的事项：-
    如果你喜欢物理学，你应该从物理学的入门课程开始，这对于物理学的学习至关重要。你可以学一两年的物理，然后尝试在以后的某些年里同时学习物理和工程。但同样重要的是要不断回到物理学上——这就像是思想的食粮——所以即使你不能完成高级物理课程，最终你也应该致力于物理学或工程学领域的感兴趣的事情。-
    需要考虑的第二点是，这确实需要花费大量时间和金钱——但我们应该预期这些。我希望能给你一些免费的建议，但我不是全职教授，还有很多人有相同的问题。你可以试着在学校里询问，也许有人可以帮助你找到一个能给你更多建议的导师。只要记住，你投入在某件事情上的时间——对很多年轻人来说——大多是因为别人对他们的期望。所以如果感觉到你并不完全喜欢这件事...'
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the user’s question. They both suggest that it is possible to pursue
    a passion for physics while studying engineering. However, Assistant A’s response
    is more detailed and provides more practical advice. It includes feedback from
    people who have made the transition from physics to engineering, which could be
    very helpful for the user. Assistant B’s response, while supportive, is less specific
    and ends abruptly, leaving the user without a complete answer. Therefore, Assistant
    A’s response is more helpful and complete. |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 和助手 B 都对用户的问题提供了有帮助和鼓励的回应。他们都建议在学习工程的同时追求物理学的热情是可能的。然而，助手 A 的回应更详细，并提供了更多实际的建议。它包括从物理学转向工程学的人的反馈，这对用户可能非常有帮助。助手
    B 的回应虽然支持，但不够具体，并且结束得很突然，让用户没有得到完整的答案。因此，助手 A 的回应更为有帮助和完整。 |'
- en: Table 9. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9. 斯坦福人类偏好数据集中测试集提示的样本响应。GPT-4 在 Anthropic-HH 数据集中选择了 Mallows-$\phi$。
- en: '| Prompt | I don’t want to be an engineer, I like physics. Hi there, im currently
    in high school and have some financial problems in the family, my parents are
    not satisfied with me doing physics (i mean get a degree and pursue higher studies
    in) as im also considering doing engineering as it has become a need. But i don’t
    want to leave physics, i love atomic physics and cosmology so much. I want to
    ask here, can i continue my passion for physics while I’m doing engineering (IT)
    , will there be time for my passion. Or else is there any scope to follow my passion
    and take out my family of this? |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 我不想成为工程师，我喜欢物理学。你好，我目前在高中，家庭有些经济问题，我的父母对我学习物理（我指的是获得学位并追求更高的研究）不太满意，因为我也在考虑做工程学，这已经变成一种需要。但我不想放弃物理，我非常热爱原子物理学和宇宙学。我想问一下，我在学习工程（IT）期间是否可以继续我的物理学热情，会有时间去追求我的热情吗？或者是否有可能在这个过程中实现我的热情并帮助我的家人解决经济问题？
    |'
- en: '| Mallows-$\phi$-DPO | Yes, you can definitely do both physics and engineering.
    Many people do both. At the end of the day, physics is about the mind-brain interaction
    of the fundamental particles in the universe, whereas engineering is about the
    world-building of technologies and products. So it’s possible to pursue both!
    If you enjoy physics, you may be able to learn about and participate in research
    in engineering, and vice versa. Or, you can teach yourself engineering by reading
    technical papers and following online tutorials and then pick a project to work
    on. Most engineering schools have classes or tutorials for self-study! So that’s
    the way it’s usually done. It’s a choice of whether you like physics, or of whether
    you go into engineering! Hope that helps! You could pursue a "dual degree", where
    you have some physics courses, and also some engineering course work, as an option.
    The physics courses could be in an engineering lab, or physics courses before
    your engineering classes. The more common thing is to just take a class in physics
    before taking an engineering class, either in another department at your university,
    or via a separate institution. And that might be a good place to take a class.
    They might also have online physics courses, or they might make it a requirement
    to take a physics course for an engineering degree. Hope that helps. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 是的，你绝对可以同时做物理学和工程学。很多人都做这两者。归根结底，物理学涉及宇宙中基本粒子的心脑互动，而工程学则涉及技术和产品的世界建设。因此，追求这两者是可能的！如果你喜欢物理学，你可能能够了解并参与工程学的研究，反之亦然。或者，你可以通过阅读技术论文和跟随在线教程自学工程学，然后选择一个项目进行实践。大多数工程学院都有自学的课程或教程！这就是通常的做法。你可以选择是否喜欢物理学，或者是否进入工程学！希望这对你有帮助！你可以选择“双学位”，其中包括一些物理学课程和一些工程学课程。物理学课程可以在工程实验室中进行，或者在工程课程之前完成。更常见的做法是，在上工程课之前先上物理课，无论是在你大学的其他部门，还是通过另一个机构。那可能是一个不错的上课地点。他们也可能有在线物理课程，或者可能要求你为工程学学位修一门物理课。希望这对你有帮助。'
- en: '| BT-DPO | Yes, you can definitely do both physics and engineering. Many people
    do both. At the end of the day it’s about what you like to do with your life,
    and most people find that their passion and work is what they want to do. It’s
    a great thing, if you’re interested in physics, to be able to pursue it while
    doing something else. I’m always happy to be a resource and answer any specific
    questions you might have if you don’t yet know how to work it out. Here’s some
    things to consider: -If you love physics you should start out with an introductory
    course in physics, this is essential for the study of physics. You might go on
    to physics for a year or two, then try physics and engineering for a later year
    or two. But it is also really important to keep coming back to physics - it’s
    like a food for the mind - so if you can’t make it all the way to advanced physics,
    eventually you should aim to do things in the field of physics or engineering
    that you are interested in. -The second thing to consider is that this does take
    up a lot of time - and a lot of money - but we should expect that. I wish I could
    give you some free advice here but I am not a full-time professor and there are
    lots of people with the same questions. You might try asking around at your school
    and perhaps someone can help you find a mentor who can give you a little more
    advice. Just keep in mind that the time you devote to a certain thing - for many
    young people - is mostly because of the pressure of other people and their expectations
    around them. So if there is the feeling that you aren [maximum generation length
    reached] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 是的，你绝对可以同时做物理学和工程学。很多人都做这两者。归根结底，关键在于你喜欢用自己的生活做什么，大多数人发现他们的激情和工作是他们想要做的事。如果你对物理学感兴趣，能够在做其他事情的同时追求它是件很棒的事。如果你还不知道如何解决问题，我很乐意作为资源回答你可能有的任何具体问题。这里有一些需要考虑的事项：
    - 如果你喜欢物理学，你应该从物理学入门课程开始，这对物理学的学习至关重要。你可以继续学习物理学一两年，然后尝试在后来的年份里学习物理学和工程学。但同样重要的是要不断回到物理学上——这就像是心灵的食粮——所以即使你不能一直进行高级物理学研究，最终你也应该瞄准在你感兴趣的物理学或工程学领域从事一些工作。
    - 第二件需要考虑的事是，这确实会花费很多时间和很多钱——但我们应该预料到这一点。我希望我能给你一些免费的建议，但我不是全职教授，还有很多人有同样的问题。你可以尝试在学校里询问，也许有人能帮助你找到一个可以给你更多建议的导师。只需记住，你花在某一事物上的时间——对于许多年轻人来说——主要是由于他人及其期望的压力。所以如果有这种感觉，你不必感到
    |'
- en: '| Judgment | Both Assistant A and Assistant B provide helpful and encouraging
    responses to the user’s question. They both affirm that it’s possible to pursue
    both physics and engineering, and they both suggest taking introductory physics
    courses. However, Assistant A goes a step further by suggesting the possibility
    of a “dual degree” and mentioning that some engineering schools offer self-study
    classes or tutorials. Assistant B, on the other hand, provides more emotional
    support and acknowledges the time and financial constraints the user might face.
    Assistant B also suggests seeking a mentor for further advice. Both responses
    are valuable, but Assistant A’s response is more detailed and practical, providing
    more concrete steps the user can take to pursue both interests. Therefore, Assistant
    A’s response is better. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 和 助手 B 都对用户的问题提供了有帮助和鼓励的回答。他们都确认了同时追求物理学和工程学的可能性，并且都建议参加入门物理课程。然而，助手
    A 进一步提出了“双学位”的可能性，并提到一些工程学校提供自学课程或辅导班。另一方面，助手 B 提供了更多的情感支持，并认识到用户可能面临的时间和经济限制。助手
    B 还建议寻求导师进一步的建议。两个回答都很有价值，但助手 A 的回答更详细和实用，提供了更多具体的步骤来追求这两个兴趣。因此，助手 A 的回答更好。 |'
- en: Table 10. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10. 来自斯坦福人类偏好数据集测试集的示例回答。GPT-4 在 Anthropic-HH 数据集上选择了 Mallows-$\theta$。
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I haven’t read it myself, but I’ve heard that Bertrand Russell’s
    characterisation of Hegel’s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 是否有著名哲学家严重误解/误解其他哲学家的例子？我自己没有读过，但我听说伯特兰·罗素在他的*西方哲学史*中对黑格尔哲学的描述相当糟糕。我想知道是否还有其他哲学家对其他哲学家的误解。
    |'
- en: '| Mallows-$\theta$-DPO | According to the book *What Is Philosophy?: A Historical
    Survey* by George Murdock, Bertrand Russell actually read and criticized Hegel’s
    philosophy in his own writings in the early 20th century, but he didn’t have a
    clear enough idea about the history of philosophy to understand Hegel. At the
    time of writing the book, Russell believed that Hegel’s philosophy was a philosophy
    of religion, and that he denied freedom and happiness because of this. However,
    Hegel’s own ideas were quite incompatible with certain parts of Christianity.
    Russell also had problems understanding Kant, and he thought Kant gave a simplified
    view of the history of philosophy. He took Kant at face value, when in reality
    Kant himself was very critical of Hegel, describing him as an atheist. The book
    does also have an extensive list of references which Russell could have checked
    to be sure that he wasn’t making a mistake himself, and I think the biggest mistake
    he made was to take Hegel’s ideas at face value when he thought they were religious.
    As far as other philosophers having been misunderstood, Plato and Kant are the
    most famous examples of this in the Western tradition, because they were so influential.
    The problem with both of these was that they weren’t just philosophers but also
    scholars, who studied other philosophers’ ideas and came up with their own versions
    of them. But philosophers weren’t able to interpret ideas of other philosophers
    until the 20th century, because people had not thought about philosophy like they
    do now, but rather as a practical skill where the only real goals were to understand
    the world more and to make people behave better. There are many examples of famous
    philosophers, like Socrates, Hobbes, and Descartes, saying things that other philosophers
    said. These statements are usually based on the historical context of the times,
    so they’re not false statements. Even people who are aware of this situation,
    and have not given up their prejudices, will still form incorrect assumptions
    about philosophers. But that’s not what philosophers do. They don’t need to prove
    all their points in order to prove them, which is what people who study history
    do, so they are usually much more accurate in their thinking. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 根据乔治·穆尔多克的书籍《什么是哲学？：历史概述》，伯特兰·罗素在20世纪初实际上阅读并批评了黑格尔的哲学，但他对哲学历史的了解并不充分，无法真正理解黑格尔。在撰写这本书的时候，罗素认为黑格尔的哲学是一种宗教哲学，因此他认为黑格尔否定了自由和幸福。然而，黑格尔的思想与基督教的某些部分实际上是相互矛盾的。罗素在理解康德时也遇到了问题，他认为康德简化了哲学史的观点。他把康德的观点当作绝对正确，而实际上康德本人对黑格尔非常批评，称其为无神论者。这本书确实有一个详细的参考文献列表，罗素本可以检查这些文献以确保他没有犯错，我认为他最大的错误是把黑格尔的思想当作宗教观点来理解。至于其他哲学家的误解，柏拉图和康德是西方传统中最著名的例子，因为他们的影响力很大。这两个哲学家的问题在于，他们不仅是哲学家，还是学者，研究其他哲学家的思想并提出了自己的版本。但哲学家们在20世纪之前无法解读其他哲学家的思想，因为当时的人们没有像现在这样思考哲学，而是将其视为一种实用技能，唯一的实际目标是更好地理解世界和改善人的行为。许多著名的哲学家，如苏格拉底、霍布斯和笛卡尔，曾经说过其他哲学家的话。这些陈述通常基于当时的历史背景，因此并不是错误的陈述。即使是那些意识到这种情况的人，并且没有放弃他们的偏见，仍然会对哲学家形成不正确的假设。但这并不是哲学家们的做法。他们不需要像研究历史的人那样证明他们的观点，因此他们的思维通常更为准确。'
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of “misrepresenting the views of Hegel and Marx”. It seems like Bertrand Russell’s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if you’re interested in the
    idea of “bad philosophy” in which a philosopher is unfairly caricatured by others?
    But Russell’s book is widely regarded as having historical importance, and for
    the most part I don’t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 根据维基百科，截至2012年，伯特兰·罗素被指控“歪曲了黑格尔和马克思的观点”。看起来伯特兰·罗素对黑格尔的理解并不完全准确，而更像是一种讽刺。由于这本书是一本哲学史，我想知道你是否对“坏哲学”的观点感兴趣，即哲学家被其他人不公正地刻画？但罗素的书被广泛认为具有历史重要性，而且我认为他的黑格尔解释大多不被认为具有争议。他确实有一些批评者。此外，他的西方哲学史在当时具有巨大影响力，而自那时以来许多事情发生了变化。但我认为他的书还是有价值的，我认为罗素和黑格尔并未相互不公正地刻画对方。
    |'
- en: '| Judgment | Assistant A provides a more detailed response, explaining how
    Bertrand Russell misunderstood Hegel’s philosophy and also mentioning that Russell
    had problems understanding Kant. Assistant A also provides examples of other philosophers
    who have been misunderstood, such as Plato and Kant, and explains why this misunderstanding
    often occurs. On the other hand, Assistant B’s response is less detailed and does
    not provide any examples of other philosophers who have been misunderstood. Assistant
    B also seems to contradict itself by stating that Russell’s interpretation of
    Hegel is not considered controversial, but then mentioning that Russell does have
    critics. Therefore, Assistant A’s response is more helpful, relevant, accurate,
    and detailed. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 提供了更详细的回答，解释了伯特兰·罗素如何误解黑格尔的哲学，并且提到罗素在理解康德方面也存在问题。助手 A 还提供了其他被误解的哲学家示例，如柏拉图和康德，并解释了这种误解为何常常发生。另一方面，助手
    B 的回答则较为简略，没有提供其他被误解哲学家的示例。助手 B 还似乎自相矛盾，一方面称罗素对黑格尔的解释并不被认为有争议，另一方面又提到罗素确实有一些批评者。因此，助手
    A 的回答更为有用、相关、准确且详细。 |'
- en: Table 11. Sample responses to a prompt from the test set of the Stanford Human
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11. 对斯坦福人类偏好数据集测试集中的一个提示的示例回答。GPT-4 在 Anthropic-HH 数据集上选择了 Mallows-$\phi$。
- en: '| Prompt | Are there any examples of famous philosophers grossly misinterpreting/misunderstanding
    other philosophers? I haven’t read it myself, but I’ve heard that Bertrand Russell’s
    characterisation of Hegel’s philosophy in his *History of Western Philosophy*
    is quite bad. I was wondering if there are any other examples of philosophers
    misunderstanding other philosophers in a big way. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 有没有一些著名哲学家严重误解/误读其他哲学家的例子？我自己没有读过，但我听说伯特兰·罗素在他的*西方哲学史*中对黑格尔哲学的刻画相当糟糕。我在想是否还有其他哲学家严重误解其他哲学家的例子。
    |'
- en: '| Mallows-$\phi$-DPO | According to the book *Bertrand Russell on Philosophy:
    The Philosopher’s Role in Life*, Russell had a major problem with the German philosopher
    Hegel’s understanding of social and political order, because at the time Russell
    was growing up Hegel’s writings appealed to him for insights about human nature,
    society, and politics. Hegel believed that human consciousness developed toward
    a certain ultimate goal, and that this ideal was a “goal” and a “principle” that
    humans could strive to reach. Russell thought Hegel was wrong because he believed
    that humanity’s final goal would be one based on some kind of scientific understanding.
    In addition, Russell thinks that Hegel’s philosophy isn’t compatible with human
    experience, and that it’s kind of a “dream world” kind of philosophy, and that
    it’s too abstract for people to be able to understand it. He thought that Hegel’s
    philosophy wasn’t a very useful perspective in the world, even though he was very
    big in Europe at the time. His philosophy is called “theistic idealism” in philosophy.
    Here are some points about the Hegel’s system of philosophy from Russell: - Hegel
    believed in the “dialectical unity of nature and history,” meaning that the universe
    consists of two parts: One which is called nature, and the other which is called
    spirit. These terms describe each other. - Hegel believed that human reality is
    a kind of “moral drama” in which human consciousness has a goal, and where the
    only real “moral actors” are human beings, and people are real actors who can
    make choices and make a difference. People can choose how to respond to social
    interactions, and people can choose how to respond to their own lives. But since
    humans have free will, their values and their moral choices are not necessarily
    predictable. And humans are not a deterministic system, since humans have free
    will, and humans can make moral choices and make a difference in the world. -
    The Hegelian version of the dialectical process is this: In “nature” there are
    two distinct concepts: the concept of “consciousness” and the concept of “matter,”
    that are both independent and distinct. But then “consciousness” and “matter”
    are in a “state of development” called “spirit,” and when the development in “spirit”
    is complete, then the dialectical relationship is complete. Human reality is basically
    this kind of two system-like dialectical relationship. - Hegel believed in a kind
    of “synthesis of opposites”: the concept of “matter” is really not “matter” by
    itself, since the “world” is really a thing, and “nature” is not “primitive nature”
    in itself, but a “self-developing whole” that is “spirit” in a special sense.
    When “spirit” develops fully, then the “whole” becomes “pure spirit”. - Russell
    thought this “synthesis of opposites” Hegelian system is not a “scientific” system,
    since humanity cannot have a “scientific” understanding of how the “world” is,
    and the idea that they could reach an ultimate goal is not scientifically possible.
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 根据书籍*《伯特兰·罗素论哲学：哲学家在生活中的角色》*，罗素对德国哲学家黑格尔对社会和政治秩序的理解有一个重大问题，因为在罗素成长的时期，黑格尔的著作吸引了他对人性、社会和政治的见解。黑格尔认为，人类意识是朝着某个终极目标发展的，这个理想是一个“目标”和一个“原则”，人类可以努力达到。罗素认为黑格尔是错的，因为他认为人类的最终目标应建立在某种科学理解的基础上。此外，罗素认为黑格尔的哲学与人类经验不相容，它是一种“梦幻世界”般的哲学，过于抽象，人们难以理解。他认为黑格尔的哲学在当时欧洲虽然非常重要，但在世界上并不是一个很有用的视角。他的哲学在哲学中被称为“有神论唯心主义”。以下是罗素对黑格尔哲学体系的一些观点：
    - 黑格尔相信“自然与历史的辩证统一”，即宇宙由两个部分组成：一个称为自然，另一个称为精神。这些术语相互描述。 - 黑格尔认为，人类现实是一种“道德戏剧”，在人类意识中有一个目标，其中唯一真正的“道德演员”是人类，人们是真正的演员，可以做出选择并产生影响。人们可以选择如何回应社会互动，如何回应自己的生活。但由于人类拥有自由意志，他们的价值观和道德选择不一定是可预测的。人类不是一个决定论系统，因为人类有自由意志，可以做出道德选择，并在世界上产生影响。
    - 黑格尔的辩证过程版本是这样的：在“自然”中有两个不同的概念：即“意识”的概念和“物质”的概念，它们都是独立而不同的。但随后“意识”和“物质”处于一种称为“精神”的“发展状态”中，当“精神”的发展完成时，辩证关系也就完成了。人类现实基本上是这种类似于双系统的辩证关系。
    - 黑格尔相信一种“对立统一”：概念“物质”实际上并不是“物质”本身，因为“世界”本身是一种事物，而“自然”并不是“原始自然”本身，而是以特殊意义上的“精神”存在的“自我发展整体”。当“精神”完全发展时，“整体”就成为“纯精神”。
    - 罗素认为这种“对立统一”的黑格尔体系不是一个“科学”体系，因为人类无法“科学”地理解“世界”是什么，并且他们能够达到终极目标的想法在科学上是不可能的。'
- en: '| BT-DPO | According to Wikipedia, as of 2012 Bertrand Russell was accused
    of “misrepresenting the views of Hegel and Marx”. It seems like Bertrand Russell’s
    understanding of Hegel was not entirely accurate, but rather a caricature. Since
    the book is a history of philosophy, I was wondering if you’re interested in the
    idea of “bad philosophy” in which a philosopher is unfairly caricatured by others?
    But Russell’s book is widely regarded as having historical importance, and for
    the most part I don’t think his interpretation of Hegel is considered to be controversial.
    He does have some critics. In addition, his history of Western philosophy was
    enormously influential at the time, and much has changed since then. But I do
    think there is value to his book, and I think Russell and Hegel are not unfairly
    caricatured by each other. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 根据维基百科，截至 2012 年，伯特兰·罗素被指控“歪曲了黑格尔和马克思的观点”。看起来伯特兰·罗素对黑格尔的理解并不完全准确，而是一个讽刺。由于这本书是哲学史，我想知道你是否对“糟糕哲学”的概念感兴趣，其中一个哲学家被其他人不公正地讽刺？不过，罗素的书被广泛认为具有历史重要性，在大多数情况下，我认为他的黑格尔解释并不被认为是有争议的。他确实有一些批评者。此外，他的西方哲学史在当时具有极大的影响力，而之后发生了许多变化。但我认为他的书还是有价值的，我认为罗素和黑格尔并没有不公正地互相讽刺。'
- en: '| Judgment | Assistant A provides a detailed explanation of Bertrand Russell’s
    interpretation of Hegel’s philosophy, highlighting the points of disagreement
    and misunderstanding. This response is not only relevant to the user’s question
    but also provides a comprehensive understanding of the topic. On the other hand,
    Assistant B’s response is less detailed and does not provide any other examples
    of philosophers misunderstanding other philosophers, which was the user’s main
    question. Therefore, Assistant A’s response is more helpful and informative. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 提供了关于伯特兰·罗素对黑格尔哲学解释的详细说明，突出了分歧和误解的要点。这个回应不仅与用户的问题相关，而且提供了对主题的全面理解。另一方面，助手
    B 的回应则不够详细，并且没有提供其他哲学家误解其他哲学家的例子，这正是用户的主要问题。因此，助手 A 的回应更具帮助性和信息性。 |'
- en: Table 12. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12. H4 Stack Exchange 偏好数据集测试集提示的示例回应。GPT-4 在 Anthropic-HH 数据集上选择了 Mallows-$\theta$。
- en: '| Prompt | I’m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: $(’a’).click(function()
    dataLayer.push(’event’: ’linkclickedtest’); );  This just isn’t working,
    at all. If I watch Chrome’s console/network tabs, no data seems to get sent to
    Google. I’ve also installed for Chrome and it doesn’t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 我正在尝试让 Google Tag Manager 和 Analytics 的一个（在我看来）非常简单的场景正常工作，却遇到问题。现在我们希望通过自定义
    JavaScript 跟踪事件。例如：$(’a’).click(function() dataLayer.push(’event’: ’linkclickedtest’);
    );  这根本不起作用。如果我查看 Chrome 的控制台/网络标签，没有数据似乎被发送到 Google。我还为 Chrome 安装了插件，但它也没有显示事件触发。我一定遗漏了一些简单的东西。任何指导都会很感激。谢谢！
    |'
- en: '| Mallows-$\theta$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. - The code you are running also won’t run in that scenario because the event
    has not yet been added, so the code you have won’t get executed. - You have to
    wait for the JavaScript to run before the code can be evaluated, so the first
    part of the code above is going to run after the code and the link you’ve just
    added to the DOM, but before the event has been loaded. If you change the code
    in your question to be more along these lines, it’ll work:  // Wait for
    the JavaScript to load, before executing the code below. setTimeout(function()
    document.querySelector(’a’).onclick = function() dataLayer.push(’event’: ’linkclickedtest’);
    , 0); |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 这里有几个可能性：- 你在链接还没有被添加到 DOM 之前就定义了事件，因此你的代码将永远不会运行。-
    你运行的代码也不会在这种情况下运行，因为事件尚未添加，所以你的代码不会被执行。- 你必须等待 JavaScript 运行完毕，才能评估代码，因此上面代码的第一部分将在你刚刚添加到
    DOM 的代码和链接之后运行，但在事件加载之前。如果你将问题中的代码更改为以下形式，它将起作用： // 等待 JavaScript 加载完毕，然后再执行下面的代码。
    setTimeout(function() document.querySelector(’a’).onclick = function() dataLayer.push(’event’:
    ’linkclickedtest’); , 0); |'
- en: '| BT-DPO | There are a few possibilities here: You’re using the latest version
    of Google Tag Manager, and you haven’t configured it correctly. You’re using custom
    JavaScript in a page other than the one being tracked. You’re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using  tags at the start of , those would
    interfere with Google Tag Manager script. To resolve issue 1 here, you’ll need
    to update the version of Google Tag Manager you’re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. You’ll also need to make sure it’s configured correctly
    in your environment, and also set it up correctly in the header of the page you’re
    tracking. If you’re using a different version of Google Tag Manager than the one
    you downloaded, you’ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 这里有几种可能性：你正在使用最新版本的 Google Tag Manager，但配置不正确。你在跟踪页面之外的页面中使用了自定义
    JavaScript。你在代码的其他部分使用了分析标签，这些标签干扰了你的跟踪代码。例如，如果你在 `` 开始时使用了 `` 标签，这些标签会干扰
    Google Tag Manager 脚本。要解决这里的问题1，你需要更新你使用的 Google Tag Manager 版本。在 Google Tag Manager
    设置页面中，点击“下载与安装”以下载并安装最新版本的 Google Tag Manager。你还需要确保它在你的环境中配置正确，并在你跟踪的页面的头部正确设置。如果你使用的是与下载版本不同的
    Google Tag Manager 版本，你需要安装 Google Tag Manager 版本 1.5 或更高版本。例如 |'
- en: '| Judgment | Both assistants provide potential reasons why the user’s code
    might not be working and offer solutions. However, Assistant A’s response is more
    relevant and directly addresses the user’s question. Assistant A suggests that
    the user’s event might be defined before the link has been added to the DOM, and
    provides a code snippet to resolve this issue. On the other hand, Assistant B’s
    response is less specific and doesn’t provide a direct solution to the user’s
    problem. Therefore, Assistant A’s response is more helpful and accurate. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Judgment | 两个助手都提供了用户代码可能无法工作的潜在原因并提出了解决方案。然而，助手 A 的回应更相关，直接解决了用户的问题。助手 A
    建议用户的事件可能在链接被添加到 DOM 之前就已定义，并提供了一个代码片段来解决此问题。另一方面，助手 B 的回应不够具体，没有提供直接解决用户问题的方案。因此，助手
    A 的回应更有帮助且更准确。 |'
- en: Table 13. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\theta$ on Anthropic-HH dataset.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表13. H4 Stack Exchange Preferences Dataset 测试集提示的示例回应。GPT-4 在 Anthropic-HH 数据集上选择了
    Mallows-$\theta$。
- en: '| Prompt | I’m attempting to get a (in my opinion) very simple scenario working
    with Google Tag Manager and Analytics, and running into problems. Now we would
    like to track events using custom JavaScript, via the . For example: $(’a’).click(function()
    dataLayer.push(’event’: ’linkclickedtest’); );  This just isn’t working,
    at all. If I watch Chrome’s console/network tabs, no data seems to get sent to
    Google. I’ve also installed for Chrome and it doesn’t show the event firing either.
    I must be missing something simple. Any guidance would be appreciated. Thanks!
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Prompt | 我正在尝试让一个（在我看来）非常简单的场景与 Google Tag Manager 和 Analytics 一起工作，但遇到了问题。现在我们希望通过自定义
    JavaScript 跟踪事件，例如：$(’a’).click(function() dataLayer.push(’event’: ’linkclickedtest’);
    );  这根本不起作用。如果我查看 Chrome 的控制台/网络选项卡，似乎没有数据发送到 Google。我也为 Chrome 安装了扩展，但它也没有显示事件触发。我一定遗漏了一些简单的东西。任何指导将不胜感激。谢谢！'
- en: '| Mallows-$\phi$-DPO | There are a few possibilities here: - You are defining
    the event before the link has even been added to the DOM, so your code will never
    run. Also, check whether your code is running at all or not, that is, whether
    you see any result in the Chrome console. - Your selector is incorrect in the
    code you shared here. - The code is not firing at all on the click event of the
    link because analytics script is not being run at all in the browser. If this
    is the case, you could verify that a click event is actually created from the
    element before trying to push the dataLayer. Have you checked these things? If
    you find out why the code runs on your computer but not on the computer of the
    person you are testing on, can you test it on their computer and tell us what
    happens? If you are just trying to make sure your code works from the example
    you gave above, I’m afraid I’m not sure what other details to go over. I’m |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 这里有几种可能性： - 你在链接添加到 DOM 之前就定义了事件，所以你的代码永远不会运行。此外，检查一下你的代码是否真的在运行，即是否在
    Chrome 控制台中看到任何结果。 - 你在这里共享的代码中的选择器不正确。 - 代码在链接点击事件上根本没有触发，因为分析脚本根本没有在浏览器中运行。如果是这种情况，你可以验证在尝试推送
    dataLayer 之前，元素是否确实创建了点击事件。你检查过这些问题吗？如果你发现代码在你的电脑上运行但在你测试的人的电脑上没有运行，可以在他们的电脑上测试一下并告诉我们发生了什么吗？如果你只是想确保你的代码从你给出的示例中工作，那么我不确定还需要讨论哪些细节。
    |'
- en: '| BT-DPO | There are a few possibilities here: You’re using the latest version
    of Google Tag Manager, and you haven’t configured it correctly. You’re using custom
    JavaScript in a page other than the one being tracked. You’re using analytics
    tags in other parts of your code, which are interfering with your tracking code.
    For instance, if you were using  tags at the start of , those would
    interfere with Google Tag Manager script. To resolve issue 1 here, you’ll need
    to update the version of Google Tag Manager you’re using. In the Google Tag Manager
    Settings page, click "Download & Install" to download and install the latest version
    of the Google Tag Manager. You’ll also need to make sure it’s configured correctly
    in your environment, and also set it up correctly in the header of the page you’re
    tracking. If you’re using a different version of Google Tag Manager than the one
    you downloaded, you’ll need to install Google Tag Manager version 1.5 or higher.
    For example |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 这里有几种可能性：你使用的是最新版本的 Google Tag Manager，但没有正确配置。你在跟踪页面之外的页面上使用了自定义
    JavaScript。你在代码的其他部分使用了分析标签，这些标签干扰了你的跟踪代码。例如，如果你在  开头使用了  标签，这些标签会干扰
    Google Tag Manager 脚本。要解决第一个问题，你需要更新你使用的 Google Tag Manager 版本。在 Google Tag Manager
    设置页面，点击“下载与安装”以下载和安装最新版本的 Google Tag Manager。你还需要确保它在你的环境中配置正确，并且在你跟踪的页面的头部中正确设置。如果你使用的是与下载版本不同的
    Google Tag Manager 版本，你需要安装 Google Tag Manager 1.5 或更高版本。例如 |'
- en: '| Judgment | Both assistants provide useful suggestions for troubleshooting
    the user’s issue with Google Tag Manager and Analytics. However, Assistant A’s
    response is more directly related to the user’s specific code example and the
    issues that might be causing it not to work, such as the timing of the event definition,
    the accuracy of the selector, and the firing of the analytics script. On the other
    hand, Assistant B’s response is more general and focuses on potential issues with
    the Google Tag Manager version and configuration, which may not be directly related
    to the user’s problem. Therefore, Assistant A’s response is more helpful and relevant
    to the user’s question. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 判定 | 两位助手都提供了有用的建议来解决用户在 Google Tag Manager 和 Analytics 中的问题。然而，助手 A 的回应与用户的具体代码示例及可能导致其无法工作的原因（如事件定义的时机、选择器的准确性、以及分析脚本的触发）直接相关。另一方面，助手
    B 的回应更为一般，关注于 Google Tag Manager 版本和配置的潜在问题，这可能与用户的问题不直接相关。因此，助手 A 的回应对用户的问题更有帮助和相关。
    |'
- en: Table 14. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses BT-DPO over Mallows-$\theta$ on Anthropic-HH
    dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14. 从 H4 Stack Exchange Preferences 数据集的测试集中的提示的示例回应。GPT-4 在 Anthropic-HH
    数据集上选择了 BT-DPO 而非 Mallows-$\theta$。
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? $("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); });  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 我有一个 Ajax 函数，由于文件较大，它需要一些时间才能响应。如果我想中断 Ajax 请求，应该如何中断响应函数？ $("#startAjaxButton").click(function(){
    $.ajax({ url:"ajaxRequest.txt", success:function(result){ $("#div1").html(result);
    } }); });  |'
- en: '| Mallows-$\theta$-DPO | That code should pause the Ajax request until you
    click the “Start” button. Another approach is to use “setTimeout” to have JavaScript
    keep the Ajax request active until it’s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to continue on executing the Ajax request after you’ve stopped
    the request, you’ll need to “un-set-timeout” the request. “setTimeout(function(),
    0)” will un-set the timeout and start the Ajax request again. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\theta$-DPO | 这段代码应该会暂停 Ajax 请求，直到你点击“开始”按钮。另一种方法是使用“setTimeout”让
    JavaScript 保持 Ajax 请求直到它完成。 setTimeout($.ajax(..), 5000); 注意：如果你在停止请求后还想继续执行 Ajax
    请求，你需要“取消设置超时”请求。“setTimeout(function(), 0)”将取消设置超时并重新启动 Ajax 请求。 |'
- en: '| BT-DPO | That code should pause the Ajax request until you click the “Start”
    button. Another approach is to use “setTimeout” to have JavaScript keep the Ajax
    request active until it’s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the “setTimeout” function to run the Ajax before the long request process
    has finished. To interrupt the request, use the “clearTimeout” function. As an
    alternative, you can also just use JavaScript’s default wait loop (“setTimeout”
    and “clearTimeout”) without having to wait for a specific event to trigger the
    function. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 这段代码应该会暂停 Ajax 请求，直到你点击“开始”按钮。另一种方法是使用“setTimeout”让 JavaScript 保持
    Ajax 请求直到它完成。 setTimeout($.ajax(..), 5000); 注意：如果你想中断一个你没有等待的长请求，建议使用“setTimeout”函数在长请求处理完成之前运行
    Ajax 请求。要中断请求，使用“clearTimeout”函数。作为替代方案，你也可以只使用 JavaScript 的默认等待循环（“setTimeout”和“clearTimeout”），无需等待特定事件触发函数。
    |'
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses initially,
    suggesting the use of "setTimeout" to keep the Ajax request active until it’s
    finished. However, Assistant B went a step further by explaining how to interrupt
    a long request using the "clearTimeout" function. This directly addresses the
    user’s question about how to interrupt the response function if they want to stop
    the Ajax request. Therefore, Assistant B’s response is more helpful and relevant
    to the user’s question. |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | 助手 A 和助手 B 最初都提供了类似的响应，建议使用“setTimeout”保持 Ajax 请求直到它完成。然而，助手 B 进一步解释了如何使用“clearTimeout”函数中断长请求。这直接解决了用户关于如何中断响应函数的问题。因此，助手
    B 的回答对用户的问题更有帮助和相关。 |'
- en: Table 15. Sample responses to a prompt from the test set of the H4 Stack Exchange
    Preferences Dataset. GPT-4 chooses Mallows-$\phi$ on Anthropic-HH dataset.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15. 来自 H4 Stack Exchange 偏好数据集测试集的提示样本响应。GPT-4 在 Anthropic-HH 数据集上选择了 Mallows-$\phi$。
- en: '| Prompt | I have an Ajax function which takes some time to respond, due to
    large file. How do I interrupt the response function if I want to stop the Ajax
    request? $("#startAjaxButton").click(function(){ $.ajax({ url:"ajaxRequest.txt",
    success:function(result){ $("#div1").html(result); } }); });  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 我有一个 Ajax 函数，由于文件较大，它需要一些时间才能响应。如果我想中断 Ajax 请求，应该如何中断响应函数？ $("#startAjaxButton").click(function(){
    $.ajax({ url:"ajaxRequest.txt", success:function(result){ $("#div1").html(result);
    } }); });  |'
- en: '| Mallows-$\phi$-DPO | That code should pause the Ajax request until you click
    the “Start” button. Another approach is to use “setTimeout” to have JavaScript
    keep the Ajax request active until it’s finished. setTimeout($.ajax(..), 5000);
    Note: If you want to interrupt a long request for which you have not waited a
    long time, you may need to perform a "clearTimeout" on your variable holding the
    time to wait, because the code in the function will not execute until after that
    time has passed. You can do this using: clearTimeout($("#id").data("timeOut"));
    //remove the timeout reference from your element $("#id").data("timeOut", null);
    //and clear all timeout references from your element |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Mallows-$\phi$-DPO | 这段代码应该会暂停 Ajax 请求，直到你点击“开始”按钮。另一种方法是使用“setTimeout”让
    JavaScript 保持 Ajax 请求活动，直到它完成。`setTimeout($.ajax(..), 5000);` 注意：如果你想中断一个你还没有等待太久的长请求，你可能需要对保存等待时间的变量执行“clearTimeout”，因为函数中的代码不会在时间过去之前执行。你可以这样做：`clearTimeout($("#id").data("timeOut"));`
    //从你的元素中移除超时引用 `$("#id").data("timeOut", null);` //并清除你元素中的所有超时引用 |'
- en: '| BT-DPO | That code should pause the Ajax request until you click the “Start”
    button. Another approach is to use “setTimeout” to have JavaScript keep the Ajax
    request active until it’s finished. setTimeout($.ajax(..), 5000); Note: If you
    want to interrupt a long request for which you have not waited, I would recommend
    using the “setTimeout” function to run the Ajax before the long request process
    has finished. To interrupt the request, use the “clearTimeout” function. As an
    alternative, you can also just use JavaScript’s default wait loop (“setTimeout”
    and “clearTimeout”) without having to wait for a specific event to trigger the
    function. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| BT-DPO | 这段代码应该会暂停 Ajax 请求，直到你点击“开始”按钮。另一种方法是使用“setTimeout”让 JavaScript 保持
    Ajax 请求活动，直到它完成。`setTimeout($.ajax(..), 5000);` 注意：如果你想中断一个长时间的请求，而你还没有等待太久，我建议使用“setTimeout”函数在长请求处理完成之前运行
    Ajax。要中断请求，使用“clearTimeout”函数。作为替代，你还可以仅仅使用 JavaScript 的默认等待循环（“setTimeout”和“clearTimeout”），而不必等待特定事件触发函数。
    |'
- en: '| Judgment | Both Assistant A and Assistant B provided similar responses, suggesting
    the use of “setTimeout” to keep the Ajax request active until it’s finished. However,
    Assistant A’s response is more accurate and detailed. Assistant A correctly explains
    that to interrupt a long request, you need to perform a “clearTimeout” on your
    variable holding the time to wait. Assistant A also provides the code to do this.
    On the other hand, Assistant B’s explanation of using “setTimeout” to run the
    Ajax before the long request process has finished is not clear and could be misleading.
    Therefore, Assistant A’s response is more helpful and accurate. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Judgment | 助手 A 和助手 B 都提供了类似的回答，建议使用“setTimeout”来保持 Ajax 请求活动，直到它完成。然而，助手
    A 的回答更准确和详细。助手 A 正确解释了中断长请求时需要对保存等待时间的变量执行“clearTimeout”。助手 A 还提供了执行此操作的代码。另一方面，助手
    B 对于使用“setTimeout”在长请求处理完成之前运行 Ajax 的解释不够清晰，可能会误导。因此，助手 A 的回答更有帮助和准确。 |'
