- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:35:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PocketLLM: 实现个性化LLM的设备端微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01031](https://ar5iv.labs.arxiv.org/html/2407.01031)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01031](https://ar5iv.labs.arxiv.org/html/2407.01031)
- en: Dan Peng
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dan Peng
- en: OPPO Research Institute
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OPPO 研究院
- en: Shenzhen, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深圳，中国
- en: lepangdan@outlook.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: lepangdan@outlook.com
- en: \AndZhihui Fu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AndZhihui Fu
- en: OPPO Research Institute
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: OPPO 研究院
- en: Shenzhen, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深圳，中国
- en: hzzhzzf@gmail.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: hzzhzzf@gmail.com
- en: \AndJun Wang
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \And王俊
- en: OPPO Research Institute
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: OPPO 研究院
- en: Shenzhen, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深圳，中国
- en: junwang.lu@gmail.com
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: junwang.lu@gmail.com
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in large language models (LLMs) have indeed showcased their
    impressive capabilities. On mobile devices, the wealth of valuable, non-public
    data generated daily holds great promise for locally fine-tuning personalized
    LLMs, while maintaining privacy through on-device processing. However, the constraints
    of mobile device resources pose challenges to direct on-device LLM fine-tuning,
    mainly due to the memory-intensive nature of derivative-based optimization required
    for saving gradients and optimizer states. To tackle this, we propose employing
    derivative-free optimization techniques to enable on-device fine-tuning of LLM,
    even on memory-limited mobile devices. Empirical results demonstrate that the
    RoBERTa-large model and OPT-1.3B can be fine-tuned locally on the OPPO Reno 6
    smartphone using around 4GB and 6.5GB of memory respectively, using derivative-free
    optimization techniques. This highlights the feasibility of on-device LLM fine-tuning
    on mobile devices, paving the way for personalized LLMs on resource-constrained
    devices while safeguarding data privacy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近期大规模语言模型（LLMs）的进展确实展示了其令人印象深刻的能力。在移动设备上，每天生成的大量有价值的非公开数据为本地微调个性化LLM提供了巨大潜力，同时通过设备端处理来维护隐私。然而，移动设备资源的限制给直接在设备上微调LLM带来了挑战，主要是由于保存梯度和优化器状态所需的基于导数的优化过程的高内存消耗。为解决这个问题，我们建议采用无导数优化技术，以实现即使在内存有限的移动设备上也能进行设备端LLM微调。实验证明，使用无导数优化技术，RoBERTa-large
    模型和 OPT-1.3B 分别可以在 OPPO Reno 6 智能手机上本地微调，所需内存分别为约 4GB 和 6.5GB。这突显了在移动设备上进行LLM微调的可行性，为资源受限的设备上的个性化LLM铺平了道路，同时保障数据隐私。
- en: 'PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 'PocketLLM: 实现个性化LLM的设备端微调'
- en: Dan Peng OPPO Research Institute Shenzhen, China lepangdan@outlook.com                       
    Zhihui Fu OPPO Research Institute Shenzhen, China hzzhzzf@gmail.com                       
    Jun Wang OPPO Research Institute Shenzhen, China junwang.lu@gmail.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Dan Peng OPPO 研究院 深圳，中国 lepangdan@outlook.com                        Zhihui
    Fu OPPO 研究院 深圳，中国 hzzhzzf@gmail.com                        Jun Wang OPPO 研究院 深圳，中国
    junwang.lu@gmail.com
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapidly evolving field of Large Language Models (LLMs), exemplified by advanced
    models such as OpenAI’s ChatGPT, marks a substantial breakthrough in artificial
    intelligence Cao et al. ([2023](#bib.bib2)). The implications and benefits of
    the advancements of LLMs for mobile devices are profound and pervasive. As reported
    in Almeida et al. ([2021](#bib.bib1)) Xu et al. ([2019](#bib.bib17)), the number
    of deep models incorporated within individual devices is growing rapidly, making
    mobile devices are the primary vehicle for AI.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由先进模型如 OpenAI 的 ChatGPT 体现的快速发展的大规模语言模型（LLMs）领域，标志着人工智能的重大突破 Cao et al. ([2023](#bib.bib2))。LLMs
    在移动设备上的进展带来的影响和好处深远而广泛。如 Almeida et al. ([2021](#bib.bib1)) 和 Xu et al. ([2019](#bib.bib17))
    所报道，单个设备中包含的深度模型数量正在迅速增长，使得移动设备成为人工智能的主要载体。
- en: The continuous generation of private, inaccessible personal data on mobile devices,
    often diverging from publicly pre-trained LLM distributions, necessitates on-device
    post-deployment fine-tuning to develop tailored, personalized models while safeguarding
    data privacy Li et al. ([2024](#bib.bib8)). On-device fine-tuning of personal
    data locally is an effective solution for model fine-tuning using personal data
    while ensuring user data privacy, as all data storage and computation occur exclusively
    on the device without any data leaving it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动设备上，个人数据的持续生成，通常与公开预训练的LLM分布不同，这要求在设备上进行后续部署的微调，以开发量身定制的个性化模型，同时保障数据隐私 Li
    et al. ([2024](#bib.bib8))。在设备上对个人数据进行本地微调是利用个人数据进行模型微调的有效解决方案，同时确保用户数据隐私，因为所有数据存储和计算都仅在设备上进行，数据不会离开设备。
- en: Fine-tuning current LLMs on mobile devices with limited resources is challenging
    due to LLMs’ large size, which demands high computational and memory resources.
    Despite some work claims of achieving on-device fine-tuning using various computation-efficient
    and memory-saving techniques, these implementations are often demonstrated on
    edge devices like Raspberry Pi Zhu et al. ([2023](#bib.bib21)) rather than on
    mobile devices such as smartphones and tablets. Mobile devices, especially smartphones,
    more so than other edge devices, generate a substantial amount of highly private
    and valuable personal data daily due to their extensive usage, holding great potential
    for enhancing applications by leveraging this data. However, to the best of our
    knowledge, there have been no successful on-device fine-tuning implementations
    on mobile devices to date.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动设备上对当前大型语言模型（LLM）进行微调具有挑战性，因为LLM的庞大体积需要大量的计算和内存资源。尽管有些工作声称通过各种计算高效和节省内存的技术实现了设备上的微调，这些实现通常展示在边缘设备如树莓派上（Zhu等，[2023](#bib.bib21)），而不是在智能手机和平板电脑等移动设备上。移动设备，特别是智能手机，由于其广泛使用，每天生成大量高度私密和有价值的个人数据，这些数据具有很大的潜力，可以通过利用这些数据来增强应用程序。然而，据我们所知，迄今为止尚未在移动设备上成功实现设备上的微调。
- en: To bridge this gap, our work aims to enable and optimize the fine-tuning of
    LLMs on resource-constrained mobile devices, particularly smartphones. Memory
    is crucial for determining the feasibility of fine-tuning LLMs on resource-constrained
    mobile devices locally, while computational capacity and communication bandwidth
    primarily impact efficiency, particularly latency. Therefore, in this work, our
    emphasis lies in reducing the memory footprint to make practical fine-tuning on
    mobile devices feasible, regardless of efficiency concerns. Future efforts are
    expected to further enhance efficiency.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，我们的工作旨在使资源受限的移动设备，特别是智能手机上的LLM微调成为可能并进行优化。内存对于确定在资源受限的移动设备上进行本地微调的可行性至关重要，而计算能力和通信带宽主要影响效率，特别是延迟。因此，在这项工作中，我们的重点是减少内存占用，使在移动设备上进行实际微调成为可能，而不考虑效率问题。未来的工作预计将进一步提高效率。
- en: The substantial memory overhead of LLM fine-tuning arises from the computational
    and storage demands associated with gradients and optimization states inherent
    in traditional derivative-based methods. To tackle this challenge on mobile devices,
    we propose leveraging derivative-free fine-tuning optimization. This approach
    aims to reduce the memory footprint during fine-tuning by circumventing the memory-intensive
    nature of traditional derivative-based methods. Our experimental results show
    that we can fine-tune RoBERTa-large and OPT-1.3B on a current off-the-shelf smartphone,
    OPPO Reno 6, with a memory consumption of around 4GB and 6.5GB, respectively.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM微调的巨大内存开销源于传统基于导数的方法中梯度和优化状态所需的计算和存储需求。为了解决移动设备上的这一挑战，我们建议利用无导数微调优化。这种方法旨在通过规避传统基于导数方法的内存密集特性来减少微调过程中的内存占用。我们的实验结果表明，我们可以在当前的智能手机
    OPPO Reno 6 上对 RoBERTa-large 和 OPT-1.3B 进行微调，内存消耗分别约为 4GB 和 6.5GB。
- en: 'We organize our article with the following structure: first, we present related
    works in  Section [2](#S2 "2 Related Works ‣ PocketLLM: Enabling On-Device Fine-Tuning
    for Personalized LLMs"), followed by an introduction to our approach (See Section [3](#S3
    "3 Proposed Approach ‣ PocketLLM: Enabling On-Device Fine-Tuning for Personalized
    LLMs")) and experimental results (See Section [4](#S4 "4 Experiments ‣ PocketLLM:
    Enabling On-Device Fine-Tuning for Personalized LLMs")). Finally, we conclude
    with our findings in Section [5](#S5 "5 Conclusions ‣ PocketLLM: Enabling On-Device
    Fine-Tuning for Personalized LLMs"). Moreover, limitations are discussed in Section [6](#S6
    "6 Limitations ‣ PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的文章结构如下：首先，我们在第[2](#S2 "2 相关工作 ‣ PocketLLM: 使设备上微调个性化LLM成为可能")节中介绍相关工作，然后介绍我们的方法（见第[3](#S3
    "3 提议的方法 ‣ PocketLLM: 使设备上微调个性化LLM成为可能")节）以及实验结果（见第[4](#S4 "4 实验 ‣ PocketLLM:
    使设备上微调个性化LLM成为可能")节）。最后，我们在第[5](#S5 "5 结论 ‣ PocketLLM: 使设备上微调个性化LLM成为可能")节总结我们的发现。此外，第[6](#S6
    "6 限制 ‣ PocketLLM: 使设备上微调个性化LLM成为可能")节讨论了限制。'
- en: 2 Related Works
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Numerous studies focus on resource-efficient fine-tuning, which can benefit
    on-device fine-tuning, categorized into lightweight foundation model design, fine-tuning
    process optimization, and external resource utilization. Moreover, Wang et al.
    ([2024](#bib.bib16)) provides a comprehensive survey on integrating LLMs with
    IoT devices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究专注于资源高效的微调，这对设备端微调非常有益，可以分为轻量级基础模型设计、微调过程优化和外部资源利用。此外，Wang等人（[2024](#bib.bib16)）提供了关于将大语言模型（LLMs）与物联网设备整合的全面综述。
- en: 2.1 Design lightweight foundation models
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 设计轻量级基础模型
- en: Employing lightweight foundation models for fine-tuning can reduce computational
    and memory demands. Techniques such as model pruning Ma et al. ([2023](#bib.bib11))
    and quantization Dettmers et al. ([2022](#bib.bib3)) are often used to lighten
    foundation models. However, these compression techniques often degrade the performance
    of the foundation model, which can further compromise the effectiveness of fine-tuning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 采用轻量级基础模型进行微调可以减少计算和内存需求。诸如模型剪枝Ma等人（[2023](#bib.bib11)）和量化Dettmers等人（[2022](#bib.bib3)）等技术通常用于减轻基础模型。然而，这些压缩技术往往会降低基础模型的性能，从而进一步影响微调的效果。
- en: 2.2 Optimize fine-tuning processes
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 优化微调过程
- en: A strand of research is dedicated to optimizing the fine-tuning process to enhance
    its efficiency in resource consumption. Ding et al. ([2023](#bib.bib4)) minimizes
    the computational cost of fine-tuning by selectively adjusting a small subset
    of key model parameters, while Hu et al. ([2021](#bib.bib6)) achieves this by
    reformulating updated matrices as products of low-rank ones. Despite these approaches
    reducing computational demands, these approaches still impose a considerable runtime
    memory burden, making it impractical for memory-constrained mobile devices Zhang
    et al. ([2023](#bib.bib19)). On the other hand, many works aim to reduce runtime
    memory usage during fine-tuning by lowering activation memory Liao et al. ([2023](#bib.bib9)) Zhang
    et al. ([2023](#bib.bib19)), using zeroth-order gradient estimator Malladi et al.
    ([2024](#bib.bib12)), or integrating gradient calculation with parameter updates Lv
    et al. ([2023](#bib.bib10)). Although memory-efficient, these approaches often
    suffer from longer running times and may exhibit reduced performance. Our work
    aligns closely with this line of research. Notably, none of these methods have
    been implemented on mobile devices, a gap our research addresses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一项研究致力于优化微调过程，以提高资源消耗的效率。Ding等人（[2023](#bib.bib4)）通过选择性地调整少量关键模型参数来最小化微调的计算成本，而Hu等人（[2021](#bib.bib6)）则通过将更新的矩阵重新表述为低秩矩阵的乘积来实现这一目标。尽管这些方法减少了计算需求，但它们仍然带来了相当大的运行时内存负担，这使得在内存受限的移动设备上实现这些方法变得不切实际Zhang等人（[2023](#bib.bib19)）。另一方面，许多工作旨在通过降低激活内存来减少微调过程中的运行时内存使用Liao等人（[2023](#bib.bib9)）Zhang等人（[2023](#bib.bib19)），使用零阶梯度估计器Malladi等人（[2024](#bib.bib12)），或将梯度计算与参数更新结合起来Lv等人（[2023](#bib.bib10)）。尽管这些方法在内存使用上更为高效，但它们通常会遭遇更长的运行时间，并可能表现出性能下降。我们的工作与这一研究方向紧密相关。值得注意的是，这些方法中的任何一种都没有在移动设备上实施，这是我们研究所弥补的一个空白。
- en: 2.3 Leverage external resource support
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 利用外部资源支持
- en: Another line of work involves offloading some or all of the model’s execution
    to nearby resource-rich edge devices or the cloud Zhou et al. ([2019](#bib.bib20)).
    These approaches leverage external resources to address limitations in resource-constrained
    scenarios. However, offloading often entails substantial communication volume,
    while mobile devices are constrained by limited bandwidth. Moreover, transferring
    even intermittent data to external devices not owned by the user may pose privacy
    risks He et al. ([2020](#bib.bib5)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作涉及将模型的部分或全部执行卸载到附近资源丰富的边缘设备或云端Zhou等人（[2019](#bib.bib20)）。这些方法利用外部资源来解决资源受限场景中的限制。然而，卸载通常涉及大量通信，而移动设备则受限于带宽有限。此外，将数据传输到用户不拥有的外部设备可能会带来隐私风险He等人（[2020](#bib.bib5)）。
- en: 3 Proposed Approach
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提议的方法
- en: 3.1 On-device fine-tuning to ensure privacy
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 设备端微调以确保隐私
- en: In this paper, we employ on-device fine-tuning to enable personalized LLM fine-tuning
    while safeguarding user data privacy. Traditionally, fine-tuning LLMs involves
    using public data on powerful GPUs hosted by service providers. However, privacy
    regulations prohibit transferring user personal data to these service providers’
    servers for the fine-tuning of personalized LLMs Voigt and Von dem Bussche ([2017](#bib.bib15)).
    Even with an Edge-Cloud collaboration paradigm  Yao et al. ([2022](#bib.bib18)),
    processing raw data on the user’s device to enhance privacy also carries risks,
    as intermediate data transferred to untrusted clouds could reveal raw data He
    et al. ([2020](#bib.bib5)). Our method provides a privacy-preserving solution
    through on-device fine-tuning, ensuring all computation and storage for fine-tuning
    remain strictly on the user’s device.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们采用设备上微调的方法来实现个性化LLM微调，同时保护用户数据隐私。传统上，LLM的微调涉及使用服务提供商托管的强大GPU上的公共数据。然而，隐私法规禁止将用户个人数据传输到这些服务提供商的服务器上，以用于个性化LLM的微调 Voigt
    和 Von dem Bussche ([2017](#bib.bib15))。即使在边缘云协作范式下 Yao 等 ([2022](#bib.bib18))，为了增强隐私而在用户设备上处理原始数据也存在风险，因为转移到不可信云的中间数据可能会泄露原始数据 He
    等 ([2020](#bib.bib5))。我们的方法通过设备上微调提供了一种保护隐私的解决方案，确保所有微调计算和存储都严格保留在用户的设备上。
- en: 3.2 Critical resource limitations
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 关键资源限制
- en: 'Generally, the key resource constraints for fine-tuning on mobile devices fall
    into three categories: computational power, memory capacity, and communication
    bandwidth. The computational power affects processing efficiency, with weaker
    computational power extending fine-tuning time but not necessarily hindering feasibility
    on mobile devices. The communication bandwidth does not present a resource constraint
    in our on-device LLM fine-tuning, without the need for communication, despite
    serving as a critical bottleneck in offloading settings. However, memory capacity
    is critical for the functional feasibility of on-device LLM fine-tuning, as insufficient
    memory can result in program crashes or out-of-memory errors. Therefore, as an
    initial step towards on-device LLM fine-tuning, our goal is to minimize the memory
    footprint to enable LLM fine-tuning on mobile devices.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在移动设备上进行微调的关键资源限制分为三类：计算能力、内存容量和通信带宽。计算能力影响处理效率，计算能力较弱会延长微调时间，但不一定会阻碍在移动设备上的可行性。通信带宽在我们进行设备上LLM微调时并不构成资源限制，因为不需要通信，尽管在卸载设置中通信带宽是一个关键瓶颈。然而，内存容量对设备上LLM微调的功能可行性至关重要，因为内存不足可能会导致程序崩溃或内存溢出错误。因此，作为设备上LLM微调的初步步骤，我们的目标是最小化内存占用，以使LLM微调能够在移动设备上进行。
- en: 3.3 Derivative-free fine-tuning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 无导数微调
- en: In this paper, we propose using derivative-free optimization to locally fine-tune
    LLMs on mobile devices, mitigating the memory-intensive nature of traditional
    derivative-based optimization. In derivative-based LLM fine-tuning, such as with
    SGD and Adam Kingma and Ba ([2014](#bib.bib7)), the model’s states—including parameters,
    gradients, and optimizer states—constitute the primary part of memory consumption Ren
    et al. ([2021](#bib.bib13)). However, computing gradients and optimizer states
    is not essential for fine-tuning. The primary objective is to minimize the loss
    function by identifying optimal parameters. In derivative-free techniques, such
    as evolutionary algorithms and zeroth-order gradient estimators Spall ([1992](#bib.bib14)),
    the parameter space is explored by iteratively evaluating the objective function
    at different points. This approach bypasses the need to compute and store gradients
    and optimizer states, as required in derivative-based methods, thereby reducing
    memory usage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们建议使用无导数优化方法在移动设备上进行本地LLM微调，以减轻传统基于导数优化的内存密集型特性。在基于导数的LLM微调中，例如使用SGD和Adam Kingma
    和 Ba ([2014](#bib.bib7))，模型的状态——包括参数、梯度和优化器状态——构成了主要的内存消耗部分 Ren 等 ([2021](#bib.bib13))。然而，计算梯度和优化器状态并不是微调的必要条件。主要目标是通过识别最优参数来最小化损失函数。在无导数技术中，如进化算法和零阶梯度估计器 Spall
    ([1992](#bib.bib14))，通过在不同点迭代评估目标函数来探索参数空间。这种方法绕过了计算和存储梯度及优化器状态的需求，从而减少了内存使用。
- en: To achieve this, we employ memory-efficient zeroth-order optimization, known
    as MeZo Malladi et al. ([2024](#bib.bib12)), as our chosen method for derivative-free
    optimization in our work. While MeZo’s efficiency is evident on NVIDIA GPUs, its
    performance on mobile devices remains unexplored, despite its memory-efficient
    nature. Furthermore, while we utilize MeZo as our implementation, other derivative-free
    optimization methods are also aligned with our approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们采用了内存高效的零阶优化方法，即MeZo Malladi等人（[2024](#bib.bib12)），作为我们工作中选择的无导数优化方法。虽然MeZo在NVIDIA
    GPU上的效率很明显，但其在移动设备上的表现尚未探索，尽管其具有内存高效的特性。此外，虽然我们使用MeZo作为实现，但其他无导数优化方法也与我们的方法一致。
- en: 4 Experiments
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We conducted experiments using MeZo on the OPPO Reno6 smartphone, which has
    12GB of memory. Results show MeZo can fine-tune RoBERTa-large and OPT-1.3B using
    approximately 4GB and 6.5GB of memory, respectively. In contrast, attempting fine-tuning
    with Adam resulted in an out-of-memory crash. This highlights the memory efficiency
    of the derivative-free approach, making it viable for fine-tuning LLMs on resource-constrained
    devices like smartphones.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在具有12GB内存的OPPO Reno6智能手机上使用MeZo进行了实验。结果显示，MeZo能够分别使用约4GB和6.5GB的内存来微调RoBERTa-large和OPT-1.3B。相比之下，使用Adam微调则导致了内存不足崩溃。这突显了无导数方法在内存方面的高效性，使其在像智能手机这样的资源受限设备上微调LLMs变得可行。
- en: 4.1 Experimental setting
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'We fine-tuned RoBERTa-large on the SST-2 dataset and OPT-1.3B on SuperGLUE
    tasks, following the MeZo repository ¹¹footnotemark: 1. We conducted all experiments
    using a commercial off-the-shelf OPPO Reno6 smartphone, employing both the MeZo
    and Adam fine-tuning methods. Each method runs for 10 steps, ensuring a fair comparison.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在SST-2数据集上微调了RoBERTa-large，在SuperGLUE任务上微调了OPT-1.3B，参考了MeZo仓库¹¹脚注标记：1。我们使用商业现货的OPPO
    Reno6智能手机进行了所有实验，采用了MeZo和Adam微调方法。每种方法运行10步，确保了公平比较。
- en: To run MeZo and Adam fine-tuning on Android-based smartphones, we used Termux ²²2https://github.com/termux ,
    a Linux simulation environment for Android. This made it feasible to implement
    these fine-tuning methods on smartphones, which typically operate on Linux systems
    with GPUs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在基于Android的智能手机上运行MeZo和Adam微调，我们使用了Termux ²²2https://github.com/termux，这是一个用于Android的Linux模拟环境。这使得在通常运行Linux系统并配有GPU的智能手机上实现这些微调方法成为可能。
- en: 4.2 Performance analysis
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 性能分析
- en: 'We present the training loss during fine-tuning RoBERTa-large using MeZo and
    Adam fine-tuning on the OPPO Reno 6, as shown in Figure [1](#S4.F1 "Figure 1 ‣
    4.2 Performance analysis ‣ 4 Experiments ‣ PocketLLM: Enabling On-Device Fine-Tuning
    for Personalized LLMs"). We observe that the loss decreases slightly but steadily
    with MeZo, albeit not as rapidly as with Adam fine-tuning. This discrepancy may
    stem from the estimated gradient’s approximation in Mezo, which may not accurately
    reflect the true gradient and, therefore, the steepest descent direction. This
    demonstrates the effectiveness of derivative-free fine-tuning, like MeZo, on mobile
    devices in terms of performance improvement (with decreasing loss), despite its
    requirement of more steps to converge compared to derivative-based methods.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了在OPPO Reno 6上使用MeZo和Adam进行微调RoBERTa-large时的训练损失，如图[1](#S4.F1 "图 1 ‣ 4.2
    性能分析 ‣ 4 实验 ‣ PocketLLM: 实现个性化LLMs的设备端微调")所示。我们观察到，尽管MeZo的损失下降幅度略微缓慢，但总体上是稳定的，这与Adam微调的速度相比并不那么快。这种差异可能源于MeZo中估计梯度的近似，可能无法准确反映真实梯度，因此无法找到最陡下降方向。这证明了像MeZo这样的无导数微调在移动设备上提升性能（损失减少）方面的有效性，尽管其收敛所需的步骤较多，相较于基于导数的方法。'
- en: '![Refer to caption](img/1aab2b258665b22dddca7f77c1594fea.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1aab2b258665b22dddca7f77c1594fea.png)'
- en: 'Figure 1: Training loss for fine-tuning RoBERTa-large using MeZo and Adam fine-tuning.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用MeZo和Adam微调RoBERTa-large的训练损失。
- en: 4.3 Memory usage analysis
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 内存使用分析
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4.3 Memory usage analysis ‣ 4 Experiments ‣
    PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs"), we compare
    the memory consumption in fine-tuning RoBERTa-large using MeZo and Adam fine-tuning
    on the OPPO Reno 6\. When using a small batch size of 8, both MeZo and Adam fine-tuning
    can be conducted on the OPPO Reno 6, with Adam fine-tuning consuming more memory.
    However, when increasing the batch size to 64, MeZo does not require additional
    memory, whereas Adam fine-tuning does, exceeding the available memory on the smartphone
    and resulting in out-of-memory crashes. Further, we fine-tune the larger model
    OPT-1.3B using MeZo with a memory consumption of about 6.5GB. These all indicate
    the effectiveness of MeZo for fine-tuning on mobile devices, with regards to memory
    usage.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S4.T1 "表 1 ‣ 4.3 内存使用分析 ‣ 4 实验 ‣ PocketLLM：启用设备上的个性化LLM微调")中，我们比较了在OPPO
    Reno 6上使用MeZo和Adam微调RoBERTa-large的内存消耗。当使用小批量大小8时，MeZo和Adam微调都可以在OPPO Reno 6上进行，但Adam微调消耗的内存更多。然而，当批量大小增加到64时，MeZo不需要额外内存，而Adam微调需要，超出了智能手机的可用内存，导致内存溢出崩溃。此外，我们使用MeZo微调了更大的模型OPT-1.3B，内存消耗约为6.5GB。这些都表明MeZo在移动设备上进行微调的有效性，尤其在内存使用方面。
- en: We observe that MeZo’s memory usage does not significantly increase with batch
    size, whereas Adam fine-tuning shows a dramatic increase. This is because in derivative-based
    methods like Adam, activation needs to be saved for gradient computation, and
    activation linearly increases with batch size. In contrast, derivative-free methods
    do not require gradient computation or activation saving during optimization,
    which is an inherent advantage of derivative-free approaches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，MeZo的内存使用不会随着批量大小的增加而显著增加，而Adam微调则表现出显著增加。这是因为在基于导数的方法如Adam中，激活需要保存以计算梯度，而激活会随着批量大小线性增加。相比之下，无导数的方法在优化过程中不需要计算梯度或保存激活，这是无导数方法的固有优势。
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Memory Usage &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存使用 &#124;'
- en: '&#124; (GB) &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (GB) &#124;'
- en: '| MeZo |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| MeZo |'
- en: '&#124; Adam &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adam &#124;'
- en: '&#124; fine-tuning &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| batch size = 8 | 4.8 | 6.5 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 = 8 | 4.8 | 6.5 |'
- en: '| 4.6 | 6.7 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 4.6 | 6.7 |'
- en: '| batch size = 64 | 4.0 | OOM |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 = 64 | 4.0 | 内存溢出 |'
- en: '| 4.5 | OOM |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 4.5 | 内存溢出 |'
- en: 'Table 1: Memory usage comparison for fine-tuning RoBERTa-large using MeZo and
    Adam fine-tuning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用MeZo和Adam微调RoBERTa-large的内存使用比较。
- en: 4.4 Wall-clock time analysis
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实时时间分析
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ 4.4 Wall-clock time analysis ‣ 4 Experiments
    ‣ PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs"), there is
    no significant difference in per-step training time for RoBERTa-large using MeZo
    and Adam on the OPPO Reno 6, contradicting the MeZo paper’s claim that MeZo can
    reduce GPU-hour usage by up to 2× compared to traditional fine-tuning Malladi
    et al. ([2024](#bib.bib12)). The variance is due to MeZo’s potential to parallelize
    gradient estimation, unlike backpropagation, which relies on sequential derivative
    calculations. However, the Reno 6’s limited parallel processing capabilities prevent
    MeZo from fully utilizing its parallelization potential, resulting in similar
    per-step training times for both MeZo and Adam, as shown in Table [2](#S4.T2 "Table
    2 ‣ 4.4 Wall-clock time analysis ‣ 4 Experiments ‣ PocketLLM: Enabling On-Device
    Fine-Tuning for Personalized LLMs"). We also note that parallelization is an inherent
    vantage of the derivative-free family, extending beyond just MeZo. Furthermore,
    we observe that the per-step training time in MeZo increases with larger batch
    sizes. This is reasonable because as the batch size increases, the forward pass
    in MeZo requires more computation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S4.T2 "表 2 ‣ 4.4 实时时间分析 ‣ 4 实验 ‣ PocketLLM：启用设备上的个性化LLM微调")所示，使用MeZo和Adam在OPPO
    Reno 6上微调RoBERTa-large的每步训练时间没有显著差异，这与MeZo论文中声称的MeZo相比传统微调Malladi等人（[2024](#bib.bib12)）可以减少高达2倍的GPU小时使用时间的说法相矛盾。差异的原因在于MeZo可能并行估计梯度，而反向传播则依赖于顺序导数计算。然而，Reno
    6有限的并行处理能力阻止了MeZo充分利用其并行化潜力，导致MeZo和Adam的每步训练时间相似，如表[2](#S4.T2 "表 2 ‣ 4.4 实时时间分析
    ‣ 4 实验 ‣ PocketLLM：启用设备上的个性化LLM微调")所示。我们还注意到，并行化是无导数家族的固有优势，超出了仅MeZo的范围。此外，我们观察到MeZo的每步训练时间随着批量大小增加而增加。这是合理的，因为随着批量大小的增加，MeZo的前向传递需要更多的计算。
- en: Moreover, we conduct fine-tuning of the large model OPT-1.3B on the OPPO Reno
    6, with a per-step training time of approximately 1800 seconds, which is over
    10 times longer than fine-tuning RoBERTa-large. This longer duration is anticipated,
    given that the parameter size of OPT-1.3B is over 5 times larger than that of
    RoBERTa-large. Additionally, our experiments show that fine-tuning OPT-1.3B on
    a single NVIDIA GeForce RTX 3090 GPU takes about 1.99 seconds per step, nearly
    1000× faster than on the OPPO Reno 6\. This underscores the substantial gap in
    computational power between mobile devices and GPUs, which are typically used
    for large model fine-tuning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在 OPPO Reno 6 上对大模型 OPT-1.3B 进行了微调，每步训练时间约为 1800 秒，比微调 RoBERTa-large 的时间长了超过
    10 倍。考虑到 OPT-1.3B 的参数规模是 RoBERTa-large 的 5 倍以上，这种较长的时间是可以预期的。此外，我们的实验显示，在单个 NVIDIA
    GeForce RTX 3090 GPU 上微调 OPT-1.3B 每步需要约 1.99 秒，比在 OPPO Reno 6 上快了近 1000 倍。这凸显了移动设备与
    GPU 之间在计算能力上的巨大差距，而 GPU 通常用于大型模型的微调。
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training time (s) &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练时间（秒） &#124;'
- en: '&#124; / per step &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; / 每步 &#124;'
- en: '| MeZo |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MeZo |'
- en: '&#124; Adam &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adam &#124;'
- en: '&#124; fine-tuning &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| batch size = 8 | 97 | 74 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 = 8 | 97 | 74 |'
- en: '| 83 | 85 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 83 | 85 |'
- en: '| batch size = 64 | 123 | OOM |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 = 64 | 123 | 内存溢出 |'
- en: '| 121 | OOM |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 121 | 内存溢出 |'
- en: 'Table 2: Wall-clock time comparison for fine-tuning RoBERTa-large using MeZo
    and Adam fine-tuning.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 使用 MeZo 和 Adam 微调 RoBERTa-large 的实际时间比较。'
- en: 5 Conclusions
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We demonstrate that derivative-free optimization allows on-device fine-tuning
    of LLMs on mobile devices, mitigating the memory constraints of traditional derivative-based
    methods. Experiments show RoBERTa-large and OPT-1.3B can be fine-tuned on the
    OPPO Reno 6 using  4GB and  6.5GB of memory, respectively. This highlights the
    advantages of derivative-free optimization for fine-tuning LLMs on resource-constrained
    mobile devices. Further experiments reveal the efficiency gap between smartphones
    and GPUs, suggesting a need to better utilize hardware capabilities. Despite these
    challenges, our successful implementation of fine-tuning LLMs on mobile devices
    is a significant stride towards personalized models while upholding user data
    privacy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了无导数优化方法允许在移动设备上进行 LLM 的设备端微调，从而缓解了传统基于导数方法的内存限制。实验表明，RoBERTa-large 和 OPT-1.3B
    分别可以在 OPPO Reno 6 上使用 4GB 和 6.5GB 的内存进行微调。这突显了无导数优化方法在资源受限的移动设备上微调 LLM 的优势。进一步实验揭示了智能手机与
    GPU 之间的效率差距，建议需要更好地利用硬件能力。尽管面临这些挑战，我们成功在移动设备上实现 LLM 微调是朝着个性化模型的重大进步，同时维护用户数据隐私。
- en: 6 Limitations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: 6.1 Memory footprint
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 内存占用
- en: While RoBERTa-large and OPT-1.3B have achieved successful fine-tuning with approximately
    4GB and 6.5GB of memory respectively, these memory requirements remain too high
    for typical mobile applications, which often operate within a 1GB memory consumption
    constraint. It remains crucial to continue minimizing the memory footprint for
    future implementations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RoBERTa-large 和 OPT-1.3B 已经在约 4GB 和 6.5GB 的内存下成功微调，但这些内存要求对于典型的移动应用仍然过高，移动应用通常在
    1GB 内存消耗限制范围内运行。继续减少未来实现的内存占用仍然至关重要。
- en: 6.2 Efficiency of derivative-free family
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 无导数方法的效率
- en: Derivative-free optimization methods are often less efficient in determining
    the optimization direction, which is a strength of derivative-based methods. Therefore,
    more effective derivative-free methods are needed in future work to reduce the
    number of steps required for convergence in fine-tuning compared to existing derivative-based
    methods, thus shortening training times.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 无导数优化方法在确定优化方向方面通常效率较低，这是基于导数方法的一个优点。因此，未来工作需要更有效的无导数方法，以减少与现有基于导数方法相比微调所需的步骤数量，从而缩短训练时间。
- en: 6.3 Adaptation to hardware capabilities
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 硬件能力适应
- en: Despite many flagship mobile devices being equipped with GPUs and even NPUs,
    which offer powerful computation and parallelization capabilities, the current
    fine-tuning processes, including our on-device implementation of MeZo, do not
    fully exploit these hardware capabilities. Derivative-free methods inherently
    possess parallelization potential, which is currently underutilized. It is crucial
    to adapt derivative-free methods to fully leverage the powerful computational
    and parallelization capabilities of current mobile devices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多旗舰移动设备配备了GPU甚至NPU，这些硬件提供了强大的计算和并行化能力，但当前的微调过程，包括我们在设备上的MeZo实现，并没有充分利用这些硬件能力。无导数方法本质上具有并行化潜力，而这一潜力目前被低估了。适应无导数方法以充分利用当前移动设备强大的计算和并行化能力至关重要。
- en: 6.4 Execution environment
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 执行环境
- en: Our current implementation involves simulating a Linux system using Termux instead
    of running directly on a mobile device. While beneficial for initial testing,
    this method serves as a temporary solution and does not accurately reflect performance
    in a real mobile environment. Specifically, executing programs in Termux may not
    fully utilize the mobile device’s hardware capabilities, potentially leading to
    suboptimal performance. Additionally, some libraries may be incompatible with
    Termux, causing issues with the execution of certain algorithms. Moreover, it’s
    important to note that this method does not align with the typical usage scenarios
    of real users, who interact directly with applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的实现涉及使用 Termux 模拟 Linux 系统，而不是直接在移动设备上运行。虽然这对于初步测试是有益的，但此方法作为一种临时解决方案，并不能准确反映真实移动环境中的性能。具体来说，在
    Termux 中执行程序可能无法充分利用移动设备的硬件能力，可能导致性能不佳。此外，一些库可能与 Termux 不兼容，导致某些算法执行问题。此外，需要注意的是，此方法与实际用户直接互动的典型使用场景不符。
- en: A practical approach is to develop native applications that leverage mobile
    AI frameworks like TensorFlow Lite ³³3https://www.tensorflow.org/lite , empowering
    developers to integrate LLMs directly into their mobile applications. Future work
    should strive to deploy on-device fine-tuning algorithms within Android applications.
    This will facilitate accurate measurement of the algorithm’s performance, including
    efficiency and accuracy, on real-world mobile devices.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一种实际的方法是开发利用移动AI框架如 TensorFlow Lite 的原生应用程序³³3https://www.tensorflow.org/lite，赋予开发者将LLM直接集成到移动应用程序中的能力。未来的工作应致力于在
    Android 应用程序中部署设备上的微调算法。这将有助于准确测量算法的性能，包括效率和准确性，在真实的移动设备上。
- en: References
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Almeida et al. (2021) Mario Almeida, Stefanos Laskaridis, Abhinav Mehrotra,
    Lukasz Dudziak, Ilias Leontiadis, and Nicholas D Lane. 2021. Smart at what cost?
    characterising mobile deep neural networks in the wild. In *Proceedings of the
    21st ACM Internet Measurement Conference*, pages 658–672.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almeida 等人（2021）马里奥·阿尔梅达、斯特法诺斯·拉斯卡里迪斯、阿比纳夫·梅赫罗特拉、卢卡什·杜季亚克、伊利亚斯·莱昂提亚迪斯、尼古拉斯·D·莱恩。2021。**《成本效益如何？在实际环境中对移动深度神经网络进行表征》**。在
    *第21届ACM互联网测量会议论文集*，第658–672页。
- en: 'Cao et al. (2023) Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S
    Yu, and Lichao Sun. 2023. A comprehensive survey of ai-generated content (aigc):
    A history of generative ai from gan to chatgpt. *arXiv preprint arXiv:2303.04226*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人（2023）姚涵·曹、思瑜·李、艺欣·刘、志灵·颜、玉彤·戴、菲利普·S·余、丽超·孙。2023。**《AI生成内容（AIGC）的全面调查：从GAN到ChatGPT的生成AI历史》**。*arXiv预印本
    arXiv:2303.04226*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人（2022）蒂姆·德特梅斯、迈克·刘易斯、尤尼斯·贝尔卡达、卢克·泽特尔摩耶。2022。**《GPT-3 int8 (): 大规模转换器的8位矩阵乘法》**。*神经信息处理系统进展*，35：30318–30332。'
- en: Ding et al. (2023) Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang,
    Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3):220–235.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2023）宁·丁、玉佳·秦、广·杨、富超·魏、宗汉·杨、玉生·苏、盛丁·胡、雨林·陈、志敏·陈、魏泽·陈 等。2023。**《大规模预训练语言模型的参数高效微调》**。*自然机器智能*，5(3)：220–235。
- en: He et al. (2020) Zecheng He, Tianwei Zhang, and Ruby B Lee. 2020. Attacking
    and protecting data privacy in edge–cloud collaborative inference systems. *IEEE
    Internet of Things Journal*, 8(12):9706–9716.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2020）泽成·何、天伟·张、鲁比·B·李。2020。**《在边缘-云协作推理系统中攻击与保护数据隐私》**。*IEEE物联网期刊*，8(12)：9706–9716。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2021. Lora：大规模语言模型的低秩适应。*arXiv
    预印本 arXiv:2106.09685*。
- en: 'Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for
    stochastic optimization. *arXiv preprint arXiv:1412.6980*.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Ba (2014) Diederik P Kingma 和 Jimmy Ba. 2014. Adam：一种用于随机优化的方法。*arXiv
    预印本 arXiv:1412.6980*。
- en: 'Li et al. (2024) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang,
    Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang
    Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. 2024.
    [Personal llm agents: Insights and survey about the capability, efficiency and
    security](https://arxiv.org/abs/2401.05459). *Preprint*, arXiv:2401.05459.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang,
    Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang
    Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, 和 Yunxin Liu. 2024.
    [个人 llm 代理：能力、效率和安全性的洞察与调查](https://arxiv.org/abs/2401.05459)。*预印本*，arXiv:2401.05459。
- en: 'Liao et al. (2023) Baohao Liao, Shaomu Tan, and Christof Monz. 2023. Make your
    pre-trained model reversible: From parameter to memory efficient fine-tuning.
    *arXiv preprint arXiv:2306.00477*.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao et al. (2023) Baohao Liao, Shaomu Tan, 和 Christof Monz. 2023. 让你的预训练模型可逆：从参数到内存高效的微调。*arXiv
    预印本 arXiv:2306.00477*。
- en: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    and Xipeng Qiu. 2023. Full parameter fine-tuning for large language models with
    limited resources. *arXiv preprint arXiv:2306.09782*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv et al. (2023) Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo,
    和 Xipeng Qiu. 2023. 有限资源下的大规模语言模型全参数微调。*arXiv 预印本 arXiv:2306.09782*。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner:
    On the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023. Llm-pruner：关于大规模语言模型的结构剪枝。*神经信息处理系统进展*，36:21702–21720。
- en: Malladi et al. (2024) Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D Lee, Danqi Chen, and Sanjeev Arora. 2024. Fine-tuning language models
    with just forward passes. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi et al. (2024) Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian,
    Jason D Lee, Danqi Chen, 和 Sanjeev Arora. 2024. 仅通过前向传递微调语言模型。*神经信息处理系统进展*，36。
- en: Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. $\{$ model
    training. In *2021 USENIX Annual Technical Conference (USENIX ATC 21)*, pages
    551–564.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, 和 Yuxiong He. 2021. $\{$ 模型训练。在
    *2021 USENIX 年度技术会议 (USENIX ATC 21)*，页面 551–564。
- en: Spall (1992) James C Spall. 1992. Multivariate stochastic approximation using
    a simultaneous perturbation gradient approximation. *IEEE transactions on automatic
    control*, 37(3):332–341.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spall (1992) James C Spall. 1992. 使用同时扰动梯度近似的多变量随机逼近。*IEEE 自动控制汇刊*，37(3):332–341。
- en: 'Voigt and Von dem Bussche (2017) Paul Voigt and Axel Von dem Bussche. 2017.
    The eu general data protection regulation (gdpr). *A Practical Guide, 1st Ed.,
    Cham: Springer International Publishing*, 10(3152676):10–5555.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voigt and Von dem Bussche (2017) Paul Voigt 和 Axel Von dem Bussche. 2017. 欧盟通用数据保护条例（GDPR）。*实用指南，第
    1 版，Cham：Springer International Publishing*，10(3152676):10–5555。
- en: 'Wang et al. (2024) Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul
    Alam, Mi Zhang, and Bhaskar Krishnamachari. 2024. Iot in the era of generative
    ai: Vision and challenges. *arXiv preprint arXiv:2401.01923*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024) Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul
    Alam, Mi Zhang, 和 Bhaskar Krishnamachari. 2024. 生成式 AI 时代的 IoT：愿景与挑战。*arXiv 预印本
    arXiv:2401.01923*。
- en: Xu et al. (2019) Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin
    Liu, and Xuanzhe Liu. 2019. A first look at deep learning apps on smartphones.
    In *The World Wide Web Conference*, pages 2125–2136.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019) Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin
    Liu, 和 Xuanzhe Liu. 2019. 智能手机上深度学习应用的初步观察。在 *全球互联网会议*，页面 2125–2136。
- en: 'Yao et al. (2022) Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin
    Ma, Jianwei Zhang, Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al. 2022. Edge-cloud
    polarization and collaboration: A comprehensive survey for ai. *IEEE Transactions
    on Knowledge and Data Engineering*, 35(7):6866–6886.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2022）Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei
    Zhang, Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, 等人. 2022. 边缘-云极化与协作：人工智能的全面调查。*IEEE
    知识与数据工程汇刊*，35(7):6866–6886。
- en: 'Zhang et al. (2023) Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and
    Bo Li. 2023. Lora-fa: Memory-efficient low-rank adaptation for large language
    models fine-tuning. *arXiv preprint arXiv:2308.03303*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023）Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, 和 Bo Li.
    2023. Lora-fa: 高效的低秩适应用于大型语言模型微调。*arXiv 预印本 arXiv:2308.03303*。'
- en: 'Zhou et al. (2019) Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, and Junshan
    Zhang. 2019. [Edge intelligence: Paving the last mile of artificial intelligence
    with edge computing](https://doi.org/10.1109/JPROC.2019.2918951). *Proceedings
    of the IEEE*, 107(8):1738–1762.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2019）Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, 和 Junshan Zhang.
    2019. [边缘智能：用边缘计算铺平人工智能的最后一公里](https://doi.org/10.1109/JPROC.2019.2918951)。*IEEE
    期刊论文集*，107(8):1738–1762。
- en: 'Zhu et al. (2023) Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen
    Wang, Chuang Gan, and Song Han. 2023. Pockengine: Sparse and efficient fine-tuning
    in a pocket. In *Proceedings of the 56th Annual IEEE/ACM International Symposium
    on Microarchitecture*, pages 1381–1394.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2023）Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang,
    Chuang Gan, 和 Song Han. 2023. Pockengine: 口袋里的稀疏且高效微调。发表于 *第56届年度IEEE/ACM国际微架构研讨会论文集*，第1381–1394页。'
