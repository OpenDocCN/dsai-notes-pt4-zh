- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:39:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:39:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language
    Models in Federated Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FederatedScope-LLM: 一个用于联邦学习中大规模语言模型微调的综合包'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.00363](https://ar5iv.labs.arxiv.org/html/2309.00363)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.00363](https://ar5iv.labs.arxiv.org/html/2309.00363)
- en: ​Weirui Kuang  , Bingchen Qian^∗, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen
    Pan, Yuexiang Xie,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ​匡伟瑞, 钱冰辰^∗, 李子涛, 陈道元, 高大伟, 潘旭晨, 谢月香,
- en: \AND Yaliang Li  , Bolin Ding, Jingren Zhou
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \AND 李亚良, 丁博霖, 周竞仁
- en: 'Alibaba Group Co-first authors.Corresponding author, email: yaliang.li@alibaba-inc.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '阿里巴巴集团共同第一作者。通讯作者，电子邮件: yaliang.li@alibaba-inc.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have demonstrated great capabilities in various
    natural language understanding and generation tasks. Platforms such as Hugging
    Face facilitate access and utilization of the pre-trained LLMs for different entities,
    ranging from computer science researchers to users with little machine learning
    background. Different entities can further improve the performance of those LLMs
    on their specific downstream tasks by fine-tuning LLMs. When several entities
    have similar interested tasks, but their local data cannot be shared directly
    because of privacy concerns regulations, federated learning (FL) is a mainstream
    solution to leverage the data of different entities. Besides avoiding direct data
    sharing, FL can also achieve rigorous data privacy protection, model intelligent
    property protection, and model customization via composition with different techniques.
    However, fine-tuning LLMs in federated learning settings still lacks adequate
    support from the existing FL frameworks because it has to deal with optimizing
    the consumption of significant communication and computational resources, various
    data preparation for different tasks, and distinct information protection demands.
    This paper first discusses these challenges of federated fine-tuning LLMs in detail,
    and introduces our implemented package FederatedScope-LLM (FS-LLM) as a main contribution,
    which consists of the following components: (1) we build a complete end-to-end
    benchmarking pipeline, automizing the processes of dataset preprocessing, federated
    fine-tuning execution or simulation, and performance evaluation on federated LLM
    fine-tuning with different capability demonstration purposes; (2) we provide comprehensive
    and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations
    and versatile programming interfaces for future extension to enhance the capabilities
    of LLMs in FL scenarios with low communication and computation costs, even without
    accessing the full model (e.g., closed-source LLMs); (3) we adopt several accelerating
    operators and resource-efficient operators for fine-tuning LLMs with limited resources
    and the flexible pluggable sub-routines for interdisciplinary study (e.g., LLMs
    in personalized FL). We conduct extensive and reproducible experiments to validate
    the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art
    parameter-efficient fine-tuning algorithms in a federated setting, which also
    yields many valuable insights into federated fine-tuning LLMs for the research
    community. To facilitate further research and adoption, we release FS-LLM at [https://github.com/alibaba/FederatedScope/tree/llm](https://github.com/alibaba/FederatedScope/tree/llm).
    ¹¹1We will continuously update the codebase and arXiv version.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言理解和生成任务中展示了出色的能力。像 Hugging Face 这样的平台使得不同实体可以方便地访问和利用预训练的
    LLMs，从计算机科学研究人员到具有较少机器学习背景的用户皆可使用。不同实体可以通过微调 LLMs 来进一步提高它们在特定下游任务上的性能。当多个实体有类似的任务兴趣时，但由于隐私保护法规无法直接共享本地数据，联邦学习（FL）是利用不同实体数据的主流解决方案。除了避免直接数据共享外，FL
    还可以通过与不同技术的组合实现严格的数据隐私保护、模型智能产权保护以及模型定制。然而，联邦学习设置下的 LLMs 微调在现有 FL 框架中仍然缺乏足够的支持，因为它必须处理优化大量通信和计算资源的消耗、不同任务的数据准备以及不同的信息保护需求。本文首先详细讨论了这些联邦微调
    LLMs 的挑战，并介绍了我们实现的包 FederatedScope-LLM (FS-LLM) 作为主要贡献，其中包含以下组件：（1）我们构建了一个完整的端到端基准测试管道，实现了数据集预处理、联邦微调执行或模拟以及在联邦
    LLM 微调中的性能评估的自动化处理，旨在展示不同能力的微调效果；（2）我们提供了全面的现成联邦参数高效微调（PEFT）算法实现和多功能编程接口，以便未来扩展，提升
    LLMs 在 FL 场景中的能力，具有低通信和计算成本，即使在无法访问完整模型（如封闭源 LLMs）的情况下；（3）我们采用了多个加速运算符和资源高效运算符来微调有限资源下的
    LLMs，并提供了灵活的可插拔子程序用于跨学科研究（例如，个性化 FL 中的 LLMs）。我们进行了广泛且可重复的实验来验证 FS-LLM 的有效性，并在联邦设置下对先进的
    LLMs 进行基准测试，使用了最先进的参数高效微调算法，这也为研究社区提供了许多有价值的见解。为了促进进一步的研究和采用，我们在 [https://github.com/alibaba/FederatedScope/tree/llm](https://github.com/alibaba/FederatedScope/tree/llm)
    发布了 FS-LLM。¹¹1 我们将持续更新代码库和 arXiv 版本。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advances in large language models (LLMs) (Touvron et al., [2023](#bib.bib80);
    Brown et al., [2020](#bib.bib8); OpenAI, [2023](#bib.bib60); Zhang et al., [2022](#bib.bib92);
    Scao et al., [2022](#bib.bib74); Zeng et al., [2023](#bib.bib91); Chowdhery et al.,
    [2022](#bib.bib16)) have enabled a wide range of real-world applications across
    various domains, such as chatbots (i.e., ChatGPT²²2https://openai.com/blog/chatgpt),
    writing assistants (Lee et al., [2022](#bib.bib44); Ippolito et al., [2022](#bib.bib36)),
    search engines (i.e., New Bing³³3https://www.bing.com/new), tool/API retriever (Qin
    et al., [2023](#bib.bib67); Patil et al., [2023](#bib.bib62)) and multimodal systems (Driess
    et al., [2023](#bib.bib26); Huang et al., [2023](#bib.bib34); Wu et al., [2023](#bib.bib85)).
    Compared to previous pre-trained language models (Devlin et al., [2019](#bib.bib24);
    Liu et al., [2019](#bib.bib52)), LLMs exhibit remarkable emergent abilities that
    have not been observed before (Zhao et al., [2023](#bib.bib94)). These emergent
    abilities of LLMs are the cores of the unprecedented proficiency and efficiency
    of AI systems built on top of them. Consequently, both academic and industrial
    people have demonstrated a keen interest in investigating the potentialities of
    LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）（Touvron et al., [2023](#bib.bib80); Brown et al., [2020](#bib.bib8);
    OpenAI, [2023](#bib.bib60); Zhang et al., [2022](#bib.bib92); Scao et al., [2022](#bib.bib74);
    Zeng et al., [2023](#bib.bib91); Chowdhery et al., [2022](#bib.bib16)）的进展使得在各种领域的现实世界应用成为可能，如聊天机器人（即，[ChatGPT](https://openai.com/blog/chatgpt)）、写作助手（Lee
    et al., [2022](#bib.bib44); Ippolito et al., [2022](#bib.bib36)）、搜索引擎（即，[New Bing](https://www.bing.com/new)）、工具/API
    检索器（Qin et al., [2023](#bib.bib67); Patil et al., [2023](#bib.bib62)）以及多模态系统（Driess
    et al., [2023](#bib.bib26); Huang et al., [2023](#bib.bib34); Wu et al., [2023](#bib.bib85)）。与之前的预训练语言模型（Devlin
    et al., [2019](#bib.bib24); Liu et al., [2019](#bib.bib52)）相比，LLMs 展现出前所未有的**突现能力**（Zhao
    et al., [2023](#bib.bib94)），这些能力是构建在其基础上的 AI 系统前所未有的**熟练度**和**效率**的核心。因此，学术界和工业界都对研究
    LLMs 的潜力表现出了浓厚的兴趣。
- en: When applying LLMs in practical applications, such as education, law, and medicine,
    fine-tuning LLMs with domain-specific data can be essential. Fine-tuning can enrich
    LLMs with domain knowledge, enhance their specific ability, improve the fairness
    and reliability of the outputs, and prevent certain damage caused by hallucination (Ji
    et al., [2023](#bib.bib38)). However, fine-tuning LLMs entails a high demand for
    computational resources and a substantial amount of domain data that may not be
    sharable due to privacy concerns. The former challenge can be addressed by recent
    works (Hu et al., [2022](#bib.bib33); Li & Liang, [2021](#bib.bib47); Liu et al.,
    [2021a](#bib.bib50); [b](#bib.bib51); Lester et al., [2021](#bib.bib45); Houlsby
    et al., [2019](#bib.bib32); Karimi Mahabadi et al., [2021](#bib.bib39); Pfeiffer
    et al., [2020a](#bib.bib64); [c](#bib.bib66)), which adapt pre-trained LLMs to
    specific domains by tuning modules with limited trainable parameters (denoted
    as *adapters*). For the latter issue, one of the feasible solutions is federated
    learning (FL) (Konečný et al., [2016](#bib.bib41); McMahan et al., [2017](#bib.bib55);
    Yang et al., [2019](#bib.bib89)), a distributed learning paradigm that allows
    multiple entities to optimize a model collaboratively without directly sharing
    their data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，如教育、法律和医学，使用领域特定数据对 LLMs 进行微调可能是必要的。微调可以使 LLMs 充实领域知识，增强其特定能力，提升输出的公平性和可靠性，并防止由幻觉引起的某些损害（Ji
    et al., [2023](#bib.bib38)）。然而，微调 LLMs 需要高计算资源和大量可能由于隐私问题而无法共享的领域数据。前者的挑战可以通过最近的工作来解决（Hu
    et al., [2022](#bib.bib33); Li & Liang, [2021](#bib.bib47); Liu et al., [2021a](#bib.bib50);
    [b](#bib.bib51); Lester et al., [2021](#bib.bib45); Houlsby et al., [2019](#bib.bib32);
    Karimi Mahabadi et al., [2021](#bib.bib39); Pfeiffer et al., [2020a](#bib.bib64);
    [c](#bib.bib66)），这些工作通过调整具有有限可训练参数的模块（称为 *adapters*）将预训练的 LLMs 适配到特定领域。对于后者问题，一种可行的解决方案是联邦学习（FL）（Konečný
    et al., [2016](#bib.bib41); McMahan et al., [2017](#bib.bib55); Yang et al., [2019](#bib.bib89)），这是一种分布式学习范式，允许多个实体协作优化模型而无需直接共享数据。
- en: Although the existing FL frameworks (Bonawitz et al., [2019](#bib.bib6); Ryffel
    et al., [2018](#bib.bib73)) can usually support various machine learning models,
    the development of federated fine-tuning on LLM is still in a premature stage
    because of the following gaps in existing work. (i) No existing FL package contains
    comprehensive and efficient implementations of LLM fine-tuning algorithms and
    a standardized benchmark for comparing the model performance, communication cost,
    and computation overhead when federated fine-tuning LLMs. (ii) Fine-tuning LLMs
    in FL is still computationally expensive on the client side, even with the parameter-efficient
    fine-tuning (PEFT) algorithms. (iii) Because pre-trained LLMs are of great intelligent
    property value and may not belong to clients, it might be necessary to let clients
    conduct federated fine-tuning without accessing the full model (e.g., closed-source
    LLMs). (iv) It is unclear whether the existing algorithms for solving advanced
    FL problems, such as personalized FL (pFL) (Tan et al., [2022](#bib.bib78); Chen
    et al., [2022](#bib.bib11)) and federated hyperparameter optimization (FedHPO) (Wang
    et al., [2023](#bib.bib82)), are still effective with different federated fine-tuning
    algorithms for LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的 FL 框架（Bonawitz 等，[2019](#bib.bib6); Ryffel 等，[2018](#bib.bib73)）通常支持各种机器学习模型，但由于现有工作中的以下差距，联邦微调
    LLM 的发展仍处于初期阶段。(i) 目前没有现有的 FL 包含全面且高效的 LLM 微调算法实现和标准化基准，用于比较模型性能、通信成本和计算开销。(ii)
    即使使用参数高效微调 (PEFT) 算法，客户端在 FL 中微调 LLM 仍然计算开销高。(iii) 由于预训练的 LLM 具有很高的智能财产价值，并且可能不属于客户端，因此可能需要让客户端在不访问完整模型的情况下进行联邦微调（例如，封闭源
    LLM）。(iv) 不清楚现有解决高级 FL 问题的算法，如个性化 FL (pFL)（Tan 等，[2022](#bib.bib78); Chen 等，[2022](#bib.bib11)）和联邦超参数优化
    (FedHPO)（Wang 等，[2023](#bib.bib82)），在不同的联邦微调 LLM 算法下是否仍然有效。
- en: 'We aim to bridge the aforementioned gaps and further promote the study of fine-tuning
    LLMs in the context of federated learning. Thus, we build up a novel open-source
    package for fine-tuning LLMs via federated learning, called FederatedScope-LLM
    (FS-LLM), on top of FederatedScope (FS) (Xie et al., [2023](#bib.bib88)). Our
    contributions can be summarized as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是弥合上述差距，并进一步推动在联邦学习背景下对微调 LLM 的研究。因此，我们建立了一个新的开源包，通过联邦学习微调 LLM，称为 FederatedScope-LLM
    (FS-LLM)，基于 FederatedScope (FS)（Xie 等，[2023](#bib.bib88)）。我们的贡献可以总结如下：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FS-LLM packages a collection of diverse federated fine-tuning datasets from
    various domains with tunable levels of heterogeneity and a suite of corresponding
    evaluation tasks to form a complete pipeline to benchmark federated fine-tuning
    LLMs algorithms in FL scenarios.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FS-LLM 汇集了来自不同领域的各种联邦微调数据集，具有可调节的异质性水平，并配备了一套对应的评估任务，形成了一个完整的管道，用于基准测试 FL 场景中的联邦微调
    LLM 算法。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FS-LLM provides comprehensive federated fine-tuning algorithms for LLMs with
    low communication and computation costs and versatile programming interfaces,
    which support both scenarios where clients can or cannot access the full model.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FS-LLM 提供了全面的联邦微调 LLM 算法，具有低通信和计算成本，以及多样化的编程接口，支持客户端是否能访问完整模型的场景。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FS-LLM is equipped with an optimized federated fine-tuning training paradigm
    for LLMs towards customizable efficiency-boosting (e.g., memory consumption reduction
    and multi-GPU parallelism) and interdisciplinary research potentials (e.g., pFL
    and FedHPO).
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FS-LLM 配备了针对 LLM 的优化联邦微调训练范式，旨在实现可定制的效率提升（例如，减少内存消耗和多 GPU 并行处理）和跨学科研究潜力（例如，pFL
    和 FedHPO）。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform extensive experiments based on FS-LLM and investigate the empirical
    performances of federated fine-tuned LLMs. Based on our observations, we point
    out the challenges for federated fine-tuning LLMs and offer insights for future
    research in this emerging field.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们基于 FS-LLM 进行大量实验，并调查了联邦微调 LLM 的实证性能。根据我们的观察，我们指出了联邦微调 LLM 面临的挑战，并为这一新兴领域的未来研究提供了见解。
- en: '![Refer to caption](img/3c6368e062399255a17532ae311d9996.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3c6368e062399255a17532ae311d9996.png)'
- en: 'Figure 1: Overview of the architecture of FS-LLM, which consists of three main
    modules: LLM-Benchmarks, LLM-AlgZoo, and LLM-Trainer. As an example in the figure,
    we use the PEFT algorithms to fine-tune LLaMA (Touvron et al., [2023](#bib.bib80))
    in FL, with half-precision (Micikevicius et al., [2018](#bib.bib56)) training
    and offloading (Ren et al., [2021](#bib.bib71)) strategy and pFedMe (T Dinh et al.,
    [2020](#bib.bib77)) algorithm. Under this learning paradigm, the clients can efficiently
    train on their local data with limited hardware resources, while the communication
    between the clients and the server only requires transmitting the adapter (which
    typically has very few parameters). This achieves high efficiency in both communication
    and computation. In the figure, Acc. stands for accelerating operator, Perf. stands
    for performance, Comm. stands for communication, Comp. stands for computation,
    and Fair. stands for fairness.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：FS-LLM 架构概述，包括三个主要模块：LLM-Benchmarks、LLM-AlgZoo 和 LLM-Trainer。图中示例中，我们使用
    PEFT 算法对 LLaMA (Touvron et al., [2023](#bib.bib80)) 进行 FL 微调，采用半精度 (Micikevicius
    et al., [2018](#bib.bib56)) 训练和卸载 (Ren et al., [2021](#bib.bib71)) 策略以及 pFedMe
    (T Dinh et al., [2020](#bib.bib77)) 算法。在这种学习模式下，客户端可以在有限的硬件资源上高效训练本地数据，而客户端与服务器之间的通信只需传输适配器（通常具有很少的参数）。这在通信和计算方面都实现了高效性。图中，Acc.
    代表加速操作符，Perf. 代表性能，Comm. 代表通信，Comp. 代表计算，Fair. 代表公平性。
- en: 2 Overview
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 概述
- en: 'In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), we illustrate
    the overall architecture of FS-LLM using a concrete example. As mentioned above,
    FS-LLM is built upon FS (Xie et al., [2023](#bib.bib88)), an easy-to-use platform
    that provides essential modules for constructing FL courses (e.g., communication
    modules, aggregation strategies, and training functionality interfaces), rich
    extended functionality, and plenty of instantiated FL algorithms. On top of FS,
    we develop three enrichments to address the gaps mentioned in Section [1](#S1
    "1 Introduction ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning") and facilitate fine-tuning LLMs
    in the federated setting: LLM-Benchmarks, LLM-AlgZoo, and LLM-Trainer. With these
    new modules, FS-LLM supports end-to-end federated fine-tuning for LLMs, providing
    (1) data preparation for fine-tuning and evaluation, (2) several out-of-the-box
    popular fine-tuning algorithms and unified and flexible programming interfaces,
    (3) various accelerating and resource-efficient operators and flexible pluggable
    sub-routines for interdisciplinary study. We present an overview in this section
    and give detailed descriptions of these modules in Section [3](#S3 "3 LLM-Benchmarks:
    A Complete Pipeline for Benchmarking ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning"), [4](#S4 "4 LLM-AlgZoo:
    A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning"), and [5](#S5 "5
    LLM-Trainer: Training Operators and Paradigm ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), respectively.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ FederatedScope-LLM：一个用于在联邦学习中微调大型语言模型的综合包") 中，我们通过具体示例展示了
    FS-LLM 的整体架构。如上所述，FS-LLM 建立在 FS (Xie et al., [2023](#bib.bib88)) 上，这是一个易于使用的平台，提供了构建
    FL 课程的基本模块（例如，通信模块、聚合策略和训练功能接口）、丰富的扩展功能和大量的 FL 算法实例。在 FS 之上，我们开发了三种增强功能，以解决第 [1](#S1
    "1 引言 ‣ FederatedScope-LLM：一个用于在联邦学习中微调大型语言模型的综合包") 节中提到的差距，并促进 LLM 在联邦环境中的微调：LLM-Benchmarks、LLM-AlgZoo
    和 LLM-Trainer。借助这些新模块，FS-LLM 支持 LLM 的端到端联邦微调，提供 (1) 微调和评估的数据准备，(2) 几种开箱即用的流行微调算法及统一灵活的编程接口，(3)
    各种加速和资源高效的操作符以及灵活的可插拔子例程用于跨学科研究。我们在本节中提供了概述，并在第 [3](#S3 "3 LLM-Benchmarks：一个用于基准测试的完整管道
    ‣ FederatedScope-LLM：一个用于在联邦学习中微调大型语言模型的综合包")、[4](#S4 "4 LLM-AlgZoo：一个微调算法集合 ‣
    FederatedScope-LLM：一个用于在联邦学习中微调大型语言模型的综合包") 和 [5](#S5 "5 LLM-Trainer：训练操作符和范式
    ‣ FederatedScope-LLM：一个用于在联邦学习中微调大型语言模型的综合包") 节中对这些模块进行了详细描述。
- en: To consolidate the implementations and facilitate the FL research about LLMs,
    one of the critical steps is to construct a module with an end-to-end benchmarking
    pipeline, i.e., LLM-Benchmarks in FS-LLM. We assemble various corpus datasets
    from different domains for fine-tuning and pair each of them with one specific
    relevant evaluation task to assess the performance of the fine-tuned LLMs on different
    domains. For the datasets prepared for fine-tuning, we partition them according
    to their meta-information, formulating them into federated versions. Meanwhile,
    FS-LLM offers Splitter to split centralized datasets, enhancing the extensibility
    of the fine-tuning dataset for federated fine-tuning LLMs. Furthermore, we provide
    a rich set of docker images where the runtime environment of the evaluation tasks
    has been prepared. LLM-Benchmarks enables users to conveniently compare the effectiveness
    of different fine-tuning algorithms in various FL scenarios.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固实现并促进关于大型语言模型（LLMs）的联邦学习（FL）研究，关键的一步是构建一个端到端的基准测试管道，即 FS-LLM 中的 LLM-Benchmarks。我们汇集了来自不同领域的各种语料库数据集用于微调，并将每个数据集配对一个特定相关的评估任务，以评估微调后的
    LLM 在不同领域的表现。对于用于微调的数据集，我们根据其元信息对其进行分区，形成联邦版本。同时，FS-LLM 提供了 Splitter 来分割集中式数据集，增强了微调数据集在联邦微调
    LLM 中的可扩展性。此外，我们提供了一整套丰富的 Docker 镜像，其中评估任务的运行环境已经准备好。LLM-Benchmarks 使用户能够方便地比较不同微调算法在各种
    FL 场景中的效果。
- en: Another important and helpful component provided in FS-LLM, LLM-AlgZoo, includes
    a collection of fine-tuning algorithms tailored for FL. As communication and computation
    resources of clients are usually limited in FL, we leverage and integrate several
    PEFT algorithms into the FL setting, such as LoRA (Hu et al., [2022](#bib.bib33)),
    prefix-tuning (Li & Liang, [2021](#bib.bib47)), P-tuning (Liu et al., [2021b](#bib.bib51)),
    and prompt tuning (Lester et al., [2021](#bib.bib45)). Compared to full-parameter
    fine-tuning, these PEFT algorithms significantly reduce memory consumption, training
    time, and communication cost for fine-tuning LLMs. Besides, motivated by the practical
    concerns on intelligent property protection of LLMs, we also integrate a privacy-preserving
    fine-tuning algorithm, offsite-tuning (Xiao et al., [2023](#bib.bib87)), for the
    scenario where clients only tune small adapters based on a distilled model from
    a full LLM.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: FS-LLM 中另一个重要且有用的组件 LLM-AlgZoo 包含了一系列针对 FL 的微调算法。由于 FL 中客户端的通信和计算资源通常有限，我们在
    FL 设置中利用并整合了几种 PEFT 算法，如 LoRA （Hu 等， [2022](#bib.bib33)）、prefix-tuning （Li & Liang，
    [2021](#bib.bib47)）、P-tuning （Liu 等， [2021b](#bib.bib51)）和 prompt tuning （Lester
    等， [2021](#bib.bib45)）。与全参数微调相比，这些 PEFT 算法显著减少了内存消耗、训练时间和微调 LLM 的通信成本。此外，受到对 LLM
    智能产权保护的实际关注的启发，我们还集成了一种隐私保护微调算法 offsite-tuning （Xiao 等， [2023](#bib.bib87)），适用于客户端仅基于从完整
    LLM 中提取的精简模型进行小型适配器微调的场景。
- en: Based on the fine-tuning algorithms mentioned above (i.e., the PEFT algorithms
    and offsite-tuning), LLM-Trainer powers the FL fine-tuning process by adopting
    a hook-like training scheme. Though these algorithms fine-tune a small number
    of parameters of LLMs, they can still be computationally expensive for some clients.
    Therefore, FS-LLM incorporates various accelerating operators and resource-efficient
    operators as hook-like functions, such as DeepSpeed’s ZeRO (Rasley et al., [2020](#bib.bib70)),
    Pytorch’s data parallelism (Paszke et al., [2019](#bib.bib61)), and model quantization (Zhang
    et al., [2019](#bib.bib93)), to accelerate the local fine-tuning process and enable
    FS-LLM to work efficiently on consumer-level GPUs. Besides, LLM-Trainer, with
    the help of its customizable hook-like programming interfaces, can be quickly
    and seamlessly integrated with existing plug-ins in FS for interdisciplinary studies,
    such as pFL (Tan et al., [2022](#bib.bib78)) and FedHPO (Wang et al., [2023](#bib.bib82)),
    which could be a promising direction for further research. For meeting different
    hardware settings or research goals, LLM-Trainer enables FS-LLM to fine-tune LLMs
    in simulated mode (single-machine FL for simulation, all clients in one machine),
    distributed mode (multi-machine FL, one client per machine), and clustered mode
    (multi-machine FL, one client per cluster).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述的微调算法（即PEFT算法和离线微调），LLM-Trainer通过采用类似钩子的训练方案来驱动FL微调过程。尽管这些算法只微调了LLMs的一小部分参数，但对某些客户端来说，它们仍然可能计算开销较大。因此，FS-LLM结合了各种加速操作符和资源高效操作符作为类似钩子的功能，如DeepSpeed的ZeRO（Rasley等，[2020](#bib.bib70)）、Pytorch的数据并行（Paszke等，[2019](#bib.bib61)）和模型量化（Zhang等，[2019](#bib.bib93)），以加速本地微调过程，使FS-LLM能够在消费级GPU上高效工作。此外，LLM-Trainer借助其可定制的类似钩子的编程接口，可以迅速无缝地与FS中的现有插件集成，用于跨学科研究，如pFL（Tan等，[2022](#bib.bib78)）和FedHPO（Wang等，[2023](#bib.bib82)），这可能是进一步研究的有前途方向。为了满足不同的硬件设置或研究目标，LLM-Trainer使FS-LLM能够在模拟模式（单机FL进行模拟，所有客户端在一台机器上）、分布式模式（多机FL，每台机器一个客户端）和集群模式（多机FL，每个集群一个客户端）下微调LLMs。
- en: '3 LLM-Benchmarks: A Complete Pipeline for Benchmarking'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM-Benchmarks：基准测试的完整流程
- en: 'As gap (i) mentioned in Section [1](#S1 "1 Introduction ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"),
    there has yet to be a consensus in the academic community or industry on how to
    fairly evaluate the LLM fine-tuning algorithms in FL and what baselines are to
    compare. Therefore, we introduce LLM-Benchmarks, the first convenient and fair
    module to evaluate federated LLM fine-tuning. LLM-Benchmarks covers a complete
    benchmarking pipeline, consisting of stages from the construction of fine-tuning
    datasets to the evaluation with a collection of tasks. To facilitate replication
    and validation, we offer a series of look-up tables containing benchmark results
    for the fine-tuning datasets and their corresponding evaluation tasks. Additionally,
    we containerize the runtime environment of the evaluation for conveniently benchmarking
    the performance of the federated fine-tuned LLMs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[1](#S1 "1 Introduction ‣ FederatedScope-LLM: A Comprehensive Package for
    Fine-tuning Large Language Models in Federated Learning")节提到的gap (i)，学术界或行业尚未达成共识，关于如何公平评估FL中的LLM微调算法以及要比较哪些基准。因此，我们引入了LLM-Benchmarks，这是第一个方便且公平的模块，用于评估联邦LLM微调。LLM-Benchmarks覆盖了一个完整的基准测试流程，从微调数据集的构建到任务集合的评估。为了方便复制和验证，我们提供了一系列包含微调数据集基准结果及其对应评估任务的查找表。此外，我们将评估的运行环境容器化，以便于基准测试联邦微调LLMs的性能。'
- en: 3.1 Federated fine-tuning dataset construction
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 联邦微调数据集构建
- en: Unlike pre-training from scratch, fine-tuning LLMs is typically for adapting
    pre-trained LLMs to one specific domain, which can be very diverse, such as code
    generation and mathematical reasoning. Additionally, as considered in existing
    FL literature, the local datasets held by individual clients can exhibit varying
    degrees of heterogeneity, even within the same domain. For instance, although
    a set of clients share a common interest in fine-tuning an LLM for code generation,
    the code base of some clients may mainly consist of Jave, while the code bases
    of other clients may contain a substantial portion of Python.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与从头开始预训练不同，微调LLMs通常是为了将预训练的LLMs适应于一个特定的领域，这些领域可以非常多样化，例如代码生成和数学推理。此外，正如现有FL文献所考虑的那样，个别客户端持有的本地数据集可以表现出不同程度的异质性，即使在同一领域内也是如此。例如，尽管一组客户端共同关注于将LLM微调用于代码生成，但某些客户端的代码库可能主要包含Java，而其他客户端的代码库则可能包含大量Python。
- en: To echo the diversity of target domains and the heterogeneity of data in real-world
    FL scenarios, we curate three fine-tuning datasets in LLM-Benchmarks. They cover
    a wide range of domains and exhibit realistic data distributions across different
    clients. (1) Fed-CodeAlpaca is built from CodeAlpaca (Chaudhary, [2023](#bib.bib10))
    to enhance LLMs’ code generation capability. It simulates a *nine*-client FL scenario.
    Each client’s dataset consists of coding exercises with answers limited to *one*
    specific programming language, such as Java or Python. (2) Fed-Dolly is for enhancing
    the capability of LLMs for generic language. We partition Databricks-dolly-15k (Conover
    et al., [2023](#bib.bib20)) into *eight* clients’ local datasets. Each client’s
    dataset consists of a series of high-quality human-generated question-response
    pairs but is limited to *one* specific NLP task, such as information extraction
    or QA. (3) Fed-GSM8K-3 is curated into *three* subsets from the train split in
    GSM8K (Cobbe et al., [2021](#bib.bib18)), aiming to enhance the capability of
    LLMs for the chain of thought (CoT). Each client’s dataset consists of grade school
    math questions randomly partitioned from the original dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了呼应目标领域的多样性以及现实世界FL场景中的数据异质性，我们在LLM-Benchmarks中策划了三个微调数据集。它们覆盖了广泛的领域，并展示了不同客户端之间现实的数据分布。(1)
    Fed-CodeAlpaca是基于CodeAlpaca (Chaudhary, [2023](#bib.bib10))构建的，旨在增强LLMs的代码生成能力。它模拟了一个*九*客户端FL场景。每个客户端的数据集由限于*一种*特定编程语言（如Java或Python）的编码练习和答案组成。(2)
    Fed-Dolly旨在增强LLMs在通用语言方面的能力。我们将Databricks-dolly-15k (Conover et al., [2023](#bib.bib20))划分为*八*个客户端的本地数据集。每个客户端的数据集由一系列高质量的人工生成的问答对组成，但限于*一种*特定的NLP任务，如信息提取或QA。(3)
    Fed-GSM8K-3从GSM8K (Cobbe et al., [2021](#bib.bib18))的训练集划分为*三个*子集，旨在增强LLMs的链式思维（CoT）能力。每个客户端的数据集由从原始数据集中随机分配的学级数学问题组成。
- en: 'It is worth noting that, in addition to the built-in datasets mentioned above,
    we provide splitters for partitioning the centralized dataset into a federated
    version based on different meta-information or with different heterogeneity degrees
    among clients. For example, users can apply different splitters to a centralized
    dataset, such as the uniform splitter, Dirichlet splitter, or meta-information
    splitter, to construct fine-tuning datasets that mirror the heterogeneity inherent
    in different FL scenarios. We provide several built-in datasets for these splitters,
    such as Alpaca (Taori et al., [2023](#bib.bib79)), cleanedAlpaca (Ruebsamen, [2023](#bib.bib72)),
    etc. For more details about the provided fine-tuning datasets, please refer to
    Appendix [A.1](#A1.SS1 "A.1 Fine-tuning Dataset description ‣ Appendix A Appendix
    ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，除了上述内置数据集外，我们还提供了用于将集中数据集划分为基于不同元信息或不同异质性程度的联邦版本的分割器。例如，用户可以对集中数据集应用不同的分割器，如均匀分割器、Dirichlet分割器或元信息分割器，以构建反映不同FL场景中固有异质性的微调数据集。我们为这些分割器提供了几个内置数据集，如Alpaca (Taori
    et al., [2023](#bib.bib79))、cleanedAlpaca (Ruebsamen, [2023](#bib.bib72))等。有关提供的微调数据集的更多详细信息，请参阅附录 [A.1](#A1.SS1
    "A.1 Fine-tuning Dataset description ‣ Appendix A Appendix ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")。'
- en: 'Table 1: Federated fine-tuning dataset and its corresponding evaluation task.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：联邦微调数据集及其对应的评估任务。
- en: '|  | Fine-tuning Dataset | Goal of Evaluation Task |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | 微调数据集 | 评估任务目标 |'
- en: '| Fed-CodeAlpaca & HumanEval | Coding exercises with different programming
    languages | How much can a federated fine-tuning algorithm improve the performance
    of an LLM with heterogeneous data on an in-domain task (coding)? |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Fed-CodeAlpaca & HumanEval | 不同编程语言的编码练习 | 联邦微调算法在领域内任务（编码）中对于异质数据的LLM性能提升程度有多大？
    |'
- en: '| Fed-Dolly & HELM | Human-generated question-response pairs with different
    types | How much can a federated fine-tuning algorithm improve the performance
    of an LLM with heterogeneous data on generic language capabilities? |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Fed-Dolly & HELM | 不同类型的人工生成问答对 | 联邦微调算法在通用语言能力方面对于异质数据的LLM性能提升程度有多大？ |'
- en: '| Fed-GSM8K-3 & GSM8K-test | Math questions with independent and homogeneous
    distribution | How much can a federated fine-tuning algorithm improve the performance
    of an LLM with i.i.d. data on an in-domain task (CoT)? |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Fed-GSM8K-3 & GSM8K-test | 独立且同质分布的数学问题 | 联邦微调算法在领域内任务（CoT）中对于i.i.d.数据的LLM性能提升程度有多大？
    |'
- en: 3.2 Federated LLM fine-tuning evaluation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 联邦LLM微调评估
- en: LLMs are known to be very powerful, but it is challenging to evaluate their
    capabilities by a single metric. To the best of our knowledge, there are no ready-to-use
    evaluation tools for assessing federated LLM fine-tuning (let alone with the personalized
    FL algorithms) in terms of accuracy and efficiency. To fulfill such a gap, in
    LLM-Benchmarks, we provide a complete benchmarking pipeline to assess LLM fine-tuning
    in various FL scenarios.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LLM被认为是非常强大的，但通过单一指标评估其能力具有挑战性。根据我们所知，目前没有现成的评估工具来评估联邦LLM微调（更不用说个性化FL算法）的准确性和效率。为弥补这一空白，在LLM-Benchmarks中，我们提供了一个完整的基准测试流程，以评估在各种FL场景下的LLM微调。
- en: 'We argue that fine-tuning should aim to enhance one of the two aspects of LLMs:
    either to improve their generic language capabilities or to improve their domain-specific
    capabilities for one particular downstream task. Therefore, we curate three evaluation
    datasets from different subject areas, including HumanEval (Chen et al., [2021](#bib.bib13))
    for the code generation, HELM (Liang et al., [2022](#bib.bib48)) for the generic
    language capabilities, and GSM8K-test (Cobbe et al., [2021](#bib.bib18)) (the
    test split in GSM8K) for CoT. Given that different datasets employ different default
    evaluation metrics, for simplicity, we introduce the term *evaluation score* as
    a unifying descriptor for the evaluated results obtained on these datasets with
    their metrics. Specifically, the evaluation score represents Pass@1 score when
    using HumanEval, a mixture of metric scores on 16 subtasks when evaluating on
    HELM⁴⁴4Because evaluation on 16 subtasks is time-consuming, we also curate a smaller
    HELM-MINI with 4 subtasks but consistent performance with original HELM for a
    brief evaluation. , and accuracy when utilizing GSM8K-test. And more details can
    be found in Appendix [A.2](#A1.SS2 "A.2 Evaluation task description ‣ Appendix
    A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '我们认为，微调应该旨在提升LLM的两个方面之一：要么提高其通用语言能力，要么提高其在特定下游任务上的领域特定能力。因此，我们从不同学科领域中策划了三个评估数据集，包括用于代码生成的HumanEval（Chen等，[2021](#bib.bib13)），用于通用语言能力的HELM（Liang等，[2022](#bib.bib48)），以及用于CoT的GSM8K-test（Cobbe等，[2021](#bib.bib18)）（GSM8K中的测试集）。由于不同数据集采用不同的默认评估指标，为简化起见，我们引入了术语*评估分数*作为在这些数据集上用其指标获得的评估结果的统一描述符。具体来说，评估分数在使用HumanEval时表示Pass@1分数，在HELM评估时表示16个子任务的指标分数的混合，且由于评估16个子任务非常耗时，我们还策划了一个较小的HELM-MINI，包含4个子任务，但与原HELM性能一致，以便进行简短评估，此外，在使用GSM8K-test时表示准确度。更多细节可见附录[A.2](#A1.SS2
    "A.2 Evaluation task description ‣ Appendix A Appendix ‣ FederatedScope-LLM: A
    Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")。'
- en: 'Then, we define evaluating an LLM on a specific dataset and generating the
    corresponding evaluation score as an *evaluation task*. We combine each fine-tuning
    dataset mentioned in Section [3.1](#S3.SS1 "3.1 Federated fine-tuning dataset
    construction ‣ 3 LLM-Benchmarks: A Complete Pipeline for Benchmarking ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")
    with one specific evaluation task, which allows users to fairly assess the improvement
    of fine-tuned LLMs in FL. The fine-tuning dataset, the corresponding evaluation
    task, and the goal of the evaluation task are listed in Table [1](#S3.T1 "Table
    1 ‣ 3.1 Federated fine-tuning dataset construction ‣ 3 LLM-Benchmarks: A Complete
    Pipeline for Benchmarking ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning"). Note that there is generally a
    distribution shift between the fine-tuning and evaluation datasets, making it
    much more challenging than those in other domains of FL (Caldas et al., [2018](#bib.bib9);
    Dong et al., [2022](#bib.bib25); Wang et al., [2022](#bib.bib81)). To ensure the
    consistency of the evaluation results, we containerize the runtime environment
    of these evaluation tasks to docker images for conveniently assessing the performance
    of the federated LLM fine-tuning.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，我们定义在特定数据集上评估LLM并生成相应的评估分数为*评估任务*。我们将第[3.1](#S3.SS1 "3.1 Federated fine-tuning
    dataset construction ‣ 3 LLM-Benchmarks: A Complete Pipeline for Benchmarking
    ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning")节中提到的每个微调数据集与一个特定的评估任务结合，这允许用户公平地评估在FL中微调的LLM的改进。微调数据集、相应的评估任务以及评估任务的目标列在表[1](#S3.T1
    "Table 1 ‣ 3.1 Federated fine-tuning dataset construction ‣ 3 LLM-Benchmarks:
    A Complete Pipeline for Benchmarking ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning")中。请注意，微调和评估数据集之间通常存在分布偏移，这使得与FL其他领域中的挑战相比更加困难（Caldas等，[2018](#bib.bib9)；Dong等，[2022](#bib.bib25)；Wang等，[2022](#bib.bib81)）。为了确保评估结果的一致性，我们将这些评估任务的运行环境容器化为docker镜像，以便于评估联邦LLM微调的性能。'
- en: Last but not least, we also introduce a rich set of cost-related metrics to
    measure the efficiency of a federated fine-tuning process, including both computation
    costs (e.g., GPU usage, computation time, flops count) and communication costs
    (e.g., message size). Along with evaluation scores, these metrics could give a
    comprehensive assessment of the federated fine-tuning process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们还引入了一套丰富的成本相关度量标准来衡量联邦微调过程的效率，包括计算成本（例如，GPU使用情况、计算时间、flops计数）和通信成本（例如，消息大小）。这些度量标准连同评估分数可以提供对联邦微调过程的全面评估。
- en: '4 LLM-AlgZoo: A Collection of Fine-tuning Algorithms'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LLM-AlgZoo：微调算法集
- en: In addition to the benchmarking module, LLM-Benchmarks, we implement a set of
    popular fine-tuning algorithms in FS-LLM and introduce them as LLM-AlgZoo in this
    section. Aiming to fulfill the gaps (i) and (ii), LLM-AlgZoo first includes a
    collection of PEFT algorithms to satisfy the constraints on the communication
    and computation costs in federated fine-tuning when all clients have access to
    the full model. However, we also realize that there are cases where the LLM owner
    may not be willing to share the full model in the federated fine-tuning stage.
    Thus, to fulfill gap (iii), we further adopt a fine-tuning algorithm that does
    not require full model access in the FL setting. Notice that all these fine-tuning
    algorithms are implemented on a set of unified but flexible interfaces, which
    hides the underlying standard functionalities from algorithm implementors, such
    as coordinating communication in FL. The same interfaces can support more diverse
    fine-tuning algorithms in future development if users follow the same programming
    paradigm as our implemented algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基准测试模块LLM-Benchmarks外，我们在FS-LLM中实现了一套流行的微调算法，并在本节中将其介绍为LLM-AlgZoo。为了弥补(i)和(ii)的空白，LLM-AlgZoo首先包括了一系列PEFT算法，以满足联邦微调中的通信和计算成本约束，当所有客户端都可以访问完整模型时。然而，我们也意识到在联邦微调阶段，LLM所有者可能不愿意共享完整模型。因此，为了填补(iii)的空白，我们进一步采用了一种在FL设置中不需要完整模型访问的微调算法。请注意，这些微调算法都在一组统一但灵活的接口上实现，这些接口将底层标准功能隐藏起来，例如在FL中的协调通信。如果用户遵循与我们实现的算法相同的编程范式，相同的接口可以支持未来开发中的更多多样化微调算法。
- en: 4.1 Reducing communication and computation costs with PEFT algorithms
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 使用PEFT算法减少通信和计算成本
- en: Achieving communication and computation efficiencies are two major challenges
    for fine-tuning LLMs in FL. The communication bottleneck arises from the limited
    bandwidth of the internet connection between the server and the clients. This
    challenge becomes exacerbated when full-parameter fine-tuning LLMs in FL, as they
    require transmitting more parameters than previous pre-trained language models.
    For example, full-parameter fine-tuning LLaMA-7B in FL requires 28GB of message
    transfer for one communication round from the client to the server. Assuming the
    network bandwidth is 100MBps, only model uploading and downloading will take about
    75 minutes, which is intolerant, especially for clients with limited network bandwidth.
    The computation efficiency is the other critical issue for federated fine-tuning
    LLMs. For example, full-parameter fine-tuning LLaMA-7B requires about 28GB of
    GPU memory for the model. In addition, the SGD optimizer and the gradients need
    another 92GB of GPU memory, leading to at least 112GB of GPU memory in total -
    this is unaffordable for most resource-limited entities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实现通信和计算效率是联邦学习中微调大型语言模型（LLMs）的两个主要挑战。通信瓶颈源于服务器和客户端之间的互联网连接带宽限制。当在联邦学习中进行完整参数微调时，这一挑战更加严重，因为它们需要传输比以前的预训练语言模型更多的参数。例如，在联邦学习中对LLaMA-7B进行完整参数微调需要28GB的消息传输，以完成一次从客户端到服务器的通信回合。假设网络带宽为100MBps，仅模型的上传和下载就会花费约75分钟，这在带宽有限的客户端中尤其不可接受。计算效率是联邦微调LLMs的另一个关键问题。例如，完整参数微调LLaMA-7B需要约28GB的GPU内存。此外，SGD优化器和梯度还需要额外的92GB
    GPU内存，总共需要至少112GB的GPU内存——这对大多数资源有限的实体来说是难以承担的。
- en: We provide a set of implemented PEFT algorithms in FS-LLM as solutions for our
    users to encounter these challenges, including LoRA (Hu et al., [2022](#bib.bib33)),
    prefix-tuning (Li & Liang, [2021](#bib.bib47)), P-tuning (Liu et al., [2021b](#bib.bib51)),
    and prompt tuning (Lester et al., [2021](#bib.bib45)). These algorithms perform
    fine-tuning by only training (additional) modules with limited parameters, known
    as *adapters*, but keep other parameters frozen. Compared to full-parameter fine-tuning
    in FL, clients only need to transmit the adapters in each communication round,
    which reduces the transmission time to tens of seconds or even seconds. Meanwhile,
    PEFT algorithms reduce the computation cost and make local training more viable
    for resource-limited clients. For example, if we only fine-tune the adapter of
    LLaMA-7B, the total GPU memory consumption will be a little more than 28GB, and
    the computation time will be less than that of full-parameter fine-tuning as well.
    Thus, based on considerations of communication and computation costs, LLM-AlgZoo
    adopts PEFT algorithms and makes them considerably more viable for resource-limited
    entities when federated fine-tuning LLMs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在FS-LLM中提供了一套实现的PEFT算法，以帮助用户应对这些挑战，包括LoRA（Hu等，[2022](#bib.bib33)）、prefix-tuning（Li
    & Liang，[2021](#bib.bib47)）、P-tuning（Liu等，[2021b](#bib.bib51)）和prompt tuning（Lester等，[2021](#bib.bib45)）。这些算法通过仅训练（额外的）有限参数模块，即*适配器*，而保持其他参数不变来进行微调。与在联邦学习中进行的完整参数微调相比，客户端只需要在每次通信回合中传输适配器，这将传输时间减少到几十秒甚至几秒。同时，PEFT算法降低了计算成本，使得资源有限的客户端的本地训练变得更加可行。例如，如果我们只对LLaMA-7B的适配器进行微调，总的GPU内存消耗将略高于28GB，计算时间也会少于完整参数微调。因此，基于通信和计算成本的考虑，LLM-AlgZoo采用了PEFT算法，并使其在联邦微调LLMs时对于资源有限的实体来说更具可行性。
- en: 4.2 Federated fine-tuning without accessing full model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 联邦微调无需访问完整模型
- en: Many existing LLMs (OpenAI, [2022](#bib.bib59); [2023](#bib.bib60); Anthropic,
    [2023](#bib.bib2)) are closed-source for some reasons such as high training costs,
    preventing training data leakage, and maintaining commercial secrets. However,
    in some commercial scenarios, downstream customers may not be satisfied with simply
    using APIs to perform inference on these all-around LLMs but also want to customize
    them more to their domain-specific data. These domain-specific data are often
    private, limited, and incomplete, which leads to insufficient adaptation and generalization
    of LLMs to the customers’ needs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有的LLMs（OpenAI，[2022](#bib.bib59)；[2023](#bib.bib60)；Anthropic，[2023](#bib.bib2)）由于高昂的训练成本、防止训练数据泄露和维护商业秘密等原因是封闭源代码的。然而，在一些商业场景中，下游客户可能不满足于仅使用API对这些全面的LLMs进行推理，还希望根据其特定领域的数据进行更多的定制。这些特定领域的数据通常是私有的、有限的且不完整的，导致LLMs在满足客户需求方面的适应性和泛化能力不足。
- en: 'To satisfy such practical demand, we adapt a privacy-preserving fine-tuning
    algorithm, offsite-tuning (Xiao et al., [2023](#bib.bib87)), to a federated version,
    and name it FedOT for short. It sends a lossy compressed model with untrainable
    parameters to the clients as an emulator of the complete LLM at the beginning
    of FL. During the FL, the clients fine-tune adapters with the frozen emulator
    and their domain-specific data. FedOT safeguards both the intelligent property
    of the model providers and the data privacy of the clients, while leveraging the
    distributed data for adapting LLMs to specific domains. This algorithm can be
    further integrated with the PEFT algorithms mentioned in Section [4.1](#S4.SS1
    "4.1 Reducing communication and computation costs with PEFT algorithms ‣ 4 LLM-AlgZoo:
    A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '为满足这种实际需求，我们将隐私保护的微调算法 offsite-tuning（Xiao 等人，[2023](#bib.bib87)）调整为联邦版本，并将其简称为
    FedOT。它在 FL 开始时将一个具有不可训练参数的有损压缩模型发送给客户端，作为完整 LLM 的仿真器。在 FL 过程中，客户端使用冻结的仿真器和其领域特定的数据对适配器进行微调。FedOT
    既保护了模型提供者的智能财产，也保护了客户端的数据隐私，同时利用分布式数据将 LLM 适配到特定领域。该算法还可以与第 [4.1](#S4.SS1 "4.1
    Reducing communication and computation costs with PEFT algorithms ‣ 4 LLM-AlgZoo:
    A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning") 节中提到的 PEFT 算法进一步集成。'
- en: 4.3 Extendability by unified interfaces for federated fine-tuning
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 通过统一接口实现联邦微调的可扩展性
- en: 'Notice that the aforementioned federated fine-tuning algorithms, regardless
    of whether the clients have access to the full model, are all implemented via
    a set of unified interfaces in FS-LLM. The interfaces compose a skeleton structure
    that can be customized to various FL applications. The interfaces can be invoked
    with customized functions in different stages, handling the underlying communication
    and synchronization between the server and clients in FL. Figure [2](#S4.F2 "Figure
    2 ‣ 4.3 Extendability by unified interfaces for federated fine-tuning ‣ 4 LLM-AlgZoo:
    A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning") illustrates some
    unified interfaces used by fine-tuning algorithms in Section [4.1](#S4.SS1 "4.1
    Reducing communication and computation costs with PEFT algorithms ‣ 4 LLM-AlgZoo:
    A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning") and [4.2](#S4.SS2
    "4.2 Federated fine-tuning without accessing full model ‣ 4 LLM-AlgZoo: A Collection
    of Fine-tuning Algorithms ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning"). The unified interfaces include
    but are not limited to the model pre-processing interface (arrow ①), initial model
    broadcast interface (arrow ②), shared parameter aggregation interface (arrow ③),
    and parameter re-distribution interface (arrow ④). For closed-source LLMs, which
    are inaccessible to clients, the model providers can compress the LLM into an
    emulator by implementing a distillation function with the model pre-processing
    interface; for open-source LLMs, which are accessible, the pre-processing interface
    just returns the original LLM.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，上述联邦微调算法，无论客户端是否可以访问完整模型，都通过 FS-LLM 中的一组统一接口实现。这些接口组成了一个可以根据各种 FL 应用自定义的骨架结构。接口可以在不同阶段通过自定义函数调用，处理
    FL 中服务器与客户端之间的底层通信和同步。图 [2](#S4.F2 "Figure 2 ‣ 4.3 Extendability by unified interfaces
    for federated fine-tuning ‣ 4 LLM-AlgZoo: A Collection of Fine-tuning Algorithms
    ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning") 说明了第 [4.1](#S4.SS1 "4.1 Reducing communication and computation
    costs with PEFT algorithms ‣ 4 LLM-AlgZoo: A Collection of Fine-tuning Algorithms
    ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning") 和 [4.2](#S4.SS2 "4.2 Federated fine-tuning without accessing
    full model ‣ 4 LLM-AlgZoo: A Collection of Fine-tuning Algorithms ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")
    节中微调算法使用的一些统一接口。这些统一接口包括但不限于模型预处理接口（箭头 ①）、初始模型广播接口（箭头 ②）、共享参数聚合接口（箭头 ③）和参数再分配接口（箭头
    ④）。对于无法访问的闭源 LLM，模型提供者可以通过实现具有模型预处理接口的蒸馏功能，将 LLM 压缩成仿真器；对于可以访问的开源 LLM，预处理接口只需返回原始
    LLM。'
- en: These unified but flexible interfaces are based on the event-driven architecture
    in FL (Xie et al., [2023](#bib.bib88)). In the context of federated LLM fine-tuning,
    events are the exchanged information (e.g., local updated weights). Each event
    has a corresponding handler, which is the actions triggered by it. The event-driven
    architecture allows the entities of FL to program their behaviors and react to
    the events. Thus, by designing message-handler pairs, users can extend and customize
    the federated fine-tuning algorithms easily because of the extensibility of FS-LLM.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统一但灵活的接口基于FL中的事件驱动架构 (Xie等，[2023](#bib.bib88))。在联邦LLM微调的背景下，事件是交换的信息（例如，本地更新的权重）。每个事件都有一个相应的处理程序，即由事件触发的操作。事件驱动架构允许FL的实体编程其行为并对事件做出反应。因此，通过设计消息-处理程序对，用户可以轻松地扩展和自定义联邦微调算法，因为FS-LLM的可扩展性。
- en: '![Refer to caption](img/d1ed48623f0a12d9417572e68dbc641a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d1ed48623f0a12d9417572e68dbc641a.png)'
- en: 'Figure 2: The unified interfaces for federated fine-tuning LLMs with or without
    accessing the full model. When the LLM is not accessible to clients, different
    algorithms can be used to generate an emulator, including distillation, pruning,
    and quantization via ① LLM model pre-processing interface; if the LLM is accessible,
    ① just output the input by default. The other three interfaces in the figure are
    ② initial model broadcast, ③ shared parameter aggregation, and ④ parameter re-distribution.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：有或没有访问完整模型的情况下，联邦微调LLM的统一接口。当LLM无法被客户端访问时，可以使用不同的算法生成模拟器，包括通过① LLM模型预处理接口进行蒸馏、剪枝和量化；如果LLM可访问，则①默认输出输入。图中的其他三个接口是②初始模型广播、③共享参数聚合和④参数重新分配。
- en: '5 LLM-Trainer: Training Operators and Paradigm'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 LLM-Trainer：训练操作符和范式
- en: Although PEFT algorithms can significantly reduce the computation cost, they
    may still be computationally expensive for some clients with limited resources.
    To alleviate such concerns, we provide LLM-Trainer, which is designed to further
    accelerate computation and save resources during the local training and message
    transmission stages in FL. We implement a set of accelerating operators and resource-efficient
    operators to fulfill gap (ii). Moreover, to meet different hardware settings or
    research goals, FS-LLM supports simulated mode, distributed mode, and clustered
    mode. For simulated mode, all clients run on a single machine to simulate the
    federated fine-tuning process. For the distributed mode and clustered mode, each
    client runs on one or more machines and communicates with the server separately.
    These modes share a consistent programming paradigm and behavior. Meanwhile, we
    note that there are also advanced FL problems in federated fine-tuning LLMs, such
    as pFL (Tan et al., [2022](#bib.bib78)) and FedHPO (Wang et al., [2023](#bib.bib82)).
    Thus, to further fulfill gap (iv), we implement LLM-Trainer as a collection of
    hook-like functions to support rich extensions, following the design principles
    of FS Trainer (Xie et al., [2023](#bib.bib88)). These hook-like functions will
    be executed by some arranged pattern to fine-tune the adapter within the local
    client. By adding, removing, or replacing hook-like functions, entities can conveniently
    customize local fine-tuning procedures and extend applicabilities. LLM-Trainer
    can be effortlessly compatible with implemented advanced FL algorithms for interdisciplinary
    research.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PEFT算法可以显著降低计算成本，但对于一些资源有限的客户端，它们仍可能计算开销较大。为了缓解这些问题，我们提供了LLM-Trainer，旨在进一步加速计算并节省FL中的本地训练和消息传输阶段的资源。我们实现了一组加速操作符和资源高效操作符以满足差距(ii)。此外，为了适应不同的硬件设置或研究目标，FS-LLM支持模拟模式、分布式模式和集群模式。对于模拟模式，所有客户端在单台机器上运行以模拟联邦微调过程。对于分布式模式和集群模式，每个客户端在一台或多台机器上运行，并与服务器单独通信。这些模式共享一致的编程范式和行为。同时，我们注意到在联邦微调LLM中还有高级FL问题，例如pFL
    (Tan等，[2022](#bib.bib78))和FedHPO (Wang等，[2023](#bib.bib82))。因此，为了进一步满足差距(iv)，我们将LLM-Trainer实现为一组类似钩子的函数，以支持丰富的扩展，遵循FS
    Trainer (Xie等，[2023](#bib.bib88))的设计原则。这些类似钩子的函数将通过某种安排的模式来执行，以在本地客户端中微调适配器。通过添加、删除或替换类似钩子的函数，实体可以方便地自定义本地微调过程并扩展应用范围。LLM-Trainer可以轻松兼容实现的高级FL算法，以进行跨学科研究。
- en: 5.1 Training operators for acceleration and efficiency
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 加速和效率的训练操作符
- en: We introduce various accelerating operators and resource-efficient operators
    for fine-tuning LLMs in FS-LLM. These provided operators aim to optimize the federated
    fine-tuning process in terms of CPU/GPU memory consumption, multi-GPU parallel,
    and communication cost. We describe the operators and show how they can be combined
    to achieve better compatibility and efficiency.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在FS-LLM中引入了各种加速操作符和资源高效操作符，用于微调大语言模型（LLMs）。这些提供的操作符旨在优化联邦微调过程中的CPU/GPU内存消耗、多GPU并行和通信成本。我们描述了这些操作符，并展示了它们如何组合以实现更好的兼容性和效率。
- en: Mode-generic operators. We provide operators generalized to different modes
    in the local fine-tuning stage of FS-LLM. We implement mixed-precision training
    and gradient accumulation operators in several hook-like functions to save GPU
    resources. Furthermore, to accelerate the local fine-tuning process and enable
    multi-GPU parallelism, FS-LLM integrates Pytorch’s data parallel mechanism.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模式通用操作符。我们提供了在FS-LLM的本地微调阶段对不同模式进行泛化的操作符。我们在多个钩子函数中实现了混合精度训练和梯度累积操作符，以节省GPU资源。此外，为了加速本地微调过程并启用多GPU并行，FS-LLM集成了Pytorch的数据并行机制。
- en: Mode-specific operators. Besides the aforementioned generalized operators, we
    develop specialized operators that are tailored for each mode and aim to address
    the bottlenecks in each mode. To be more specific, in the simulated mode, instantiating
    multiple clients on a single machine with several independent instantiated models
    causes a lot of memory consumption. Therefore, we use a round-robin switching
    operator, which allows the clients to take turns using the frozen full model for
    fine-tuning the adapter and then aggregating the updated adapters. Under this
    operator, when the number of clients grows, the memory consumption will only increase
    by an additional amount of the adapter. This improvement makes it possible to
    conduct simulated FL experiments with a number of clients on one single machine.
    For the distributed mode and clustered mode, we accelerate the communication between
    the server and the clients by one to two orders of magnitude by reducing the size
    of communication messages. We apply different communication optimizations for
    different messages and introduce communication-efficient operators, including
    quantization operator, streaming operator, and compression operator. Specifically,
    the quantization operator reduces the bit-width of the model parameters in the
    message to 16 or 8 bits; the streaming operator serializes the model parameters
    to eliminate the overhead of type conversion; the compression operator applies
    DEFLATE (Deutsch, [1996a](#bib.bib22)) or Gzip (Deutsch, [1996b](#bib.bib23))
    algorithms to compress the messages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模式特定的操作符。除了上述的通用操作符，我们还开发了针对每种模式的专用操作符，旨在解决每种模式中的瓶颈。具体来说，在模拟模式下，在单台机器上实例化多个客户端，并且每个客户端有多个独立实例化的模型会导致大量内存消耗。因此，我们使用了循环切换操作符，允许客户端轮流使用冻结的完整模型来微调适配器，然后聚合更新后的适配器。在这种操作符下，当客户端数量增加时，内存消耗只会增加适配器额外的量。这一改进使得可以在一台机器上进行多个客户端的模拟联邦学习实验。对于分布式模式和集群模式，我们通过减少通信消息的大小，将服务器和客户端之间的通信加速了一个到两个数量级。我们对不同的消息应用不同的通信优化，并引入了高效通信操作符，包括量化操作符、流式操作符和压缩操作符。具体来说，量化操作符将消息中的模型参数的位宽减少到16位或8位；流式操作符将模型参数序列化，以消除类型转换的开销；压缩操作符应用DEFLATE（Deutsch，[1996a](#bib.bib22)）或Gzip（Deutsch，[1996b](#bib.bib23)）算法对消息进行压缩。
- en: 'Parallelization operators. Meanwhile, we also migrate the functionality of
    DeepSpeed as shown in Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Training operators for
    acceleration and efficiency ‣ 5 LLM-Trainer: Training Operators and Paradigm ‣
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning"), providing data parallelism with multi-GPU memory optimization
    and CPU offloading capabilities, further enhancing resource utilization. Specifically,
    after launching the fine-tuning with DeepSpeed, we disable some modules (e.g.,
    the logging module, the WandB module, and the file writing module) for the subprocess
    other than the Rank $0$ process to avoid conflicts. In addition, in distributed
    and clustered modes, each subprocess communicates independently with each other
    across the server and the clients, and only synchronizes with other subprocesses
    during the local fine-tuning process, ensuring consistent behavior across different
    modes.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化操作符。与此同时，我们还迁移了 DeepSpeed 的功能，如图 [3](#S5.F3 "图 3 ‣ 5.1 加速和效率的训练操作符 ‣ 5 LLM-Trainer：训练操作符和范式
    ‣ FederatedScope-LLM：用于联邦学习的大型语言模型微调的综合包") 所示，提供了多 GPU 内存优化和 CPU 卸载功能的数据并行性，进一步提升资源利用率。具体而言，在启动
    DeepSpeed 微调后，我们禁用了某些模块（例如，日志模块、WandB 模块和文件写入模块）以避免与 Rank $0$ 进程之外的子进程发生冲突。此外，在分布式和集群模式下，每个子进程在服务器和客户端之间独立通信，仅在本地微调过程中与其他子进程同步，确保在不同模式下行为的一致性。
- en: '![Refer to caption](img/8e9b04a2f73abe3655da644ae76694bb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e9b04a2f73abe3655da644ae76694bb.png)'
- en: 'Figure 3: FS-LLM integrates DeepSpeed for federated fine-tuning in different
    hardware conditions. Rank $0$ indicates the main process for multi-GPU training,
    and some modules of other subprocesses are disabled (e.g., logging and saving
    checkpoints). Msg stands for messages transmitted between the server and the clients,
    which trigger the events to happen.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：FS-LLM 在不同硬件条件下集成了 DeepSpeed 以进行联邦微调。Rank $0$ 表示多 GPU 训练的主进程，一些其他子进程的模块被禁用（例如，日志记录和保存检查点）。Msg
    代表服务器与客户端之间传输的消息，触发事件的发生。
- en: 5.2 Training paradigm for extendability towards advanced FL tasks
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 针对高级联邦学习任务的扩展性训练范式
- en: 'Another design philosophy of LLM-Trainer is to support various extensions easily
    and integrate different hook-like functions with existing training paradigms seamlessly
    for FL interdisciplinary study. A vanilla fine-tuning process involves three steps:
    (1) preparing datasets and extracting them by batches, (2) iterating over the
    training datasets to update the model parameters, and (3) evaluating the performance
    of the fine-tuned model on validation datasets. We implement fine-grained learning
    behaviors for advanced FL tasks at different points in the above steps with hook-like
    functions. For instance, pFL (Tan et al., [2022](#bib.bib78)) and FedHPO (Wang
    et al., [2023](#bib.bib82)) are two advanced FL tasks that can significantly improve
    model performance in FL.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-Trainer 的另一个设计理念是支持各种扩展，轻松集成不同的类似钩子功能与现有训练范式，实现联邦学习跨学科研究。一个标准的微调过程包括三个步骤：（1）准备数据集并按批次提取，（2）迭代训练数据集以更新模型参数，以及（3）在验证数据集上评估微调模型的性能。我们在上述步骤的不同点实现了针对高级联邦学习任务的细粒度学习行为，并使用类似钩子的功能。例如，pFL（Tan
    等，[2022](#bib.bib78)）和 FedHPO（Wang 等，[2023](#bib.bib82)）是两个可以显著提高联邦学习模型性能的高级任务。
- en: Adaptation of pFL with LLMs. FS (Xie et al., [2023](#bib.bib88)) provides many
    implemented pFL plugins (Chen et al., [2022](#bib.bib11)), which can be integrated
    for federated fine-finetuning LLMs. However, due to limited resources, it is unrealistic
    for the clients to maintain both global and local models at the same time for
    some pFL algorithms (T Dinh et al., [2020](#bib.bib77); Li et al., [2021](#bib.bib46)),
    as it would consume a lot of memory. Therefore, we optimize the implementation
    of these pFL algorithms by only using global and local adapters. With such an
    implementation, by adding a pFL hook-like function, we can achieve the cooperation
    of any PEFT algorithms with the implemented pFL algorithms, which provides strong
    extensibility for federated fine-tuning LLMs in personalized settings.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: pFL 在 LLMs 上的适配。FS（Xie et al.，[2023](#bib.bib88)）提供了许多已实现的 pFL 插件（Chen et al.，[2022](#bib.bib11)），可以集成用于联邦微调
    LLMs。然而，由于资源有限，客户端在某些 pFL 算法（T Dinh et al.，[2020](#bib.bib77)；Li et al.，[2021](#bib.bib46)）的情况下同时维护全局模型和本地模型是不现实的，因为这会消耗大量内存。因此，我们通过仅使用全局和本地适配器来优化这些
    pFL 算法的实现。通过这样的实现，添加类似 pFL 钩子的功能，我们可以实现任何 PEFT 算法与已实现的 pFL 算法的合作，这为个性化设置中的联邦微调
    LLMs 提供了强大的扩展性。
- en: Adaptation of FedHPO with LLMs. Similarly, for FedHPO, we offer extensions for
    FS-LLM with model-free HPO methods (e.g., random search (Bergstra & Bengio, [2012](#bib.bib4))
    and grid search), model-based HPO methods (e.g., Bayesian Optimization (Shahriari
    et al., [2015](#bib.bib76))), multi-fidelity HPO method (e.g., Successive Halving
    Algorithm (Jamieson & Talwalkar, [2016](#bib.bib37))), and FedHPO methods (e.g.,
    FTS (Dai et al., [2020](#bib.bib21)) and FLoRA (Zhou et al., [2021](#bib.bib95))).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: FedHPO 在 LLMs 上的适配。类似地，对于 FedHPO，我们为 FS-LLM 提供了与无模型 HPO 方法（例如，随机搜索（Bergstra
    & Bengio，[2012](#bib.bib4)）和网格搜索）、基于模型的 HPO 方法（例如，贝叶斯优化（Shahriari et al.，[2015](#bib.bib76)））、多保真度
    HPO 方法（例如，连续减半算法（Jamieson & Talwalkar，[2016](#bib.bib37)））以及 FedHPO 方法（例如，FTS（Dai
    et al.，[2020](#bib.bib21)）和 FLoRA（Zhou et al.，[2021](#bib.bib95)））的扩展。
- en: 'As our vision, the extensibility of FS-LLM to support the interdisciplinary
    study of LLMs and other FL scenarios dramatically expands the application scenarios
    of fine-tuning LLMs in FL and raises many new research opportunities, which we
    will discuss more details later in Section [7](#S7 "7 Discussions and Future Directions
    ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的愿景来看，FS-LLM 的扩展性以支持 LLMs 和其他 FL 场景的跨学科研究，极大地扩展了在 FL 中对 LLMs 进行微调的应用场景，并提出了许多新的研究机会，我们将在第[7](#S7
    "7 讨论与未来方向 ‣ FederatedScope-LLM：联邦学习中大规模语言模型微调的综合包")节中进一步讨论。
- en: 6 Experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 'In this section, we demonstrate the effectiveness of FS-LLM by a set of comprehensive
    experiments with different algorithms and tasks. We want to answer the following
    research questions: (1) How effective and efficient is it to federated fine-tuning
    LLMs with PEFT algorithms (Section [6.1](#S6.SS1 "6.1 Effectiveness of PEFT algorithms
    in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning") and [6.2](#S6.SS2 "6.2 Efficiency
    of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"))? (2) How
    effective is it to federated fine-tune LLMs without accessing the full model (Section [6.3](#S6.SS3
    "6.3 Fine-tuning LLMs without accessing the full model in FL ‣ 6 Experiments ‣
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning"))? (3) What insights can we obtain from the interdisciplinary
    capabilities of FS-LLM in resolving pFL and FedHPO problems when federated fine-tuning
    LLMs (Section [6.4](#S6.SS4 "6.4 Personalized federated learning for LLMs ‣ 6
    Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning") and [6.5](#S6.SS5 "6.5 Study of the FedHPO
    for LLMs ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning"))? We go through our experimental
    results and provide answers to the above questions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们通过一系列综合实验展示了FS-LLM的有效性，这些实验涵盖了不同的算法和任务。我们希望回答以下研究问题：（1）使用PEFT算法进行联邦微调LLM的效果和效率如何（第[6.1](#S6.SS1
    "6.1 PEFT算法在FS-LLM中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")和[6.2](#S6.SS2
    "6.2 PEFT算法在FS-LLM中的效率 ‣ 6 实验 ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")节）？（2）在没有访问完整模型的情况下进行联邦微调LLM的效果如何（第[6.3](#S6.SS3
    "6.3 在FL中未访问完整模型的LLM微调 ‣ 6 实验 ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")节）？（3）在进行联邦微调LLM时，我们能从FS-LLM解决pFL和FedHPO问题的跨学科能力中获得什么见解（第[6.4](#S6.SS4
    "6.4 针对LLM的个性化联邦学习 ‣ 6 实验 ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")和[6.5](#S6.SS5
    "6.5 对LLM的FedHPO研究 ‣ 6 实验 ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")节）？我们将详细探讨我们的实验结果，并回答上述问题。'
- en: 6.1 Effectiveness of PEFT algorithms in FS-LLM
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 PEFT算法在FS-LLM中的有效性
- en: 'Firstly, we benchmark the performance of different PEFT algorithms in different
    application domains and scenarios. As described in Section [3](#S3 "3 LLM-Benchmarks:
    A Complete Pipeline for Benchmarking ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning"), we use three federated
    fine-tuning datasets to fine-tune LLMs and evaluate them with corresponding tasks:
    (i) federated fine-tuning with Fed-CodeAlpaca for code generation and evaluating
    with HumanEval, (ii) federated fine-tuning with Fed-Dolly for generic language
    capability and evaluating with HELM, and (iii) federated fine-tuning with Fed-GSM8K-3
    for mathematical reasoning and evaluating with GSM8K-test. We conduct experiments
    in three scenarios: global (centralized fine-tuning), fed (federated fine-tuning),
    and local (separated fine-tuning). To be more specific, the global scenario can
    be regarded as fine-tuning LLMs with one client who holds the whole fine-tuning
    dataset. Fed scenario means that clients federated fine-tune LLMs where each client
    holds a different fine-tuning dataset. Local scenario means that each client independently
    fine-tunes LLMs with its own fine-tuning dataset.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们在不同的应用领域和场景中对不同的PEFT算法的性能进行了基准测试。如第[3](#S3 "3 LLM-Benchmarks: 用于基准测试的完整管道
    ‣ FederatedScope-LLM: 用于联邦学习中大型语言模型微调的综合包")节所述，我们使用三个联邦微调数据集对LLM进行微调，并用相应的任务进行评估：（i）使用Fed-CodeAlpaca进行代码生成的联邦微调，并使用HumanEval进行评估，（ii）使用Fed-Dolly进行通用语言能力的联邦微调，并使用HELM进行评估，（iii）使用Fed-GSM8K-3进行数学推理的联邦微调，并使用GSM8K-test进行评估。我们在三种场景下进行实验：全球（集中式微调）、联邦（联邦微调）和本地（分离式微调）。具体来说，全球场景可以视为用一个持有整个微调数据集的客户端进行LLM微调。联邦场景意味着客户端在每个客户端持有不同的微调数据集的情况下进行联邦微调。地方场景意味着每个客户端独立地用自己的微调数据集对LLM进行微调。'
- en: 'All the experiments are conducted on the machines with the same hardware configuration:
    Nvidia A100 GPU (80GB) with Intel Xeon Platinum 8369B CPU and 512GB of RAM. For
    all scenarios, we repeat the experiments three times with different random seeds.
    We report the average evaluation score with its standard deviation.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在配置相同硬件的机器上进行：Nvidia A100 GPU（80GB）配备 Intel Xeon Platinum 8369B CPU 和 512GB
    RAM。对于所有场景，我们使用不同的随机种子重复实验三次。我们报告了平均评估分数及其标准差。
- en: 'Benchmark federated fine-tuned LLaMA-7B. We use a widely adopted LLM, LLaMA-7B,
    with three PEFT algorithms ⁵⁵5 We exclude prefix-tuning from our experiments,
    because its implementation in  Mangrulkar et al. ([2022](#bib.bib54)) contains
    unresolved issues when we were preparing this package. , including LoRA (Hu et al.,
    [2022](#bib.bib33)), P-tuning (Liu et al., [2021b](#bib.bib51)), and prompt tuning (Lester
    et al., [2021](#bib.bib45)). We employ FedAvg (McMahan et al., [2017](#bib.bib55))
    as the federated aggregation strategy. To conduct the experiments uniformly and
    fairly, we fix the FL-specific hyperparameters and the hyperparameters that have
    a large impact on the computation cost for all experiments. For example, we set
    the communication round to $500$. Please refer to Appendix [A.3](#A1.SS3 "A.3
    Detailed hyperparameters and results ‣ Appendix A Appendix ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")
    for more algorithm-specific hyperparameters corresponding to each PEFT algorithm.
    Moreover, to further reduce the GPU memory consumption for efficiency, we employ
    the half-precision operator during fine-tuning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '基准联邦微调 LLaMA-7B。我们使用一种广泛采用的 LLM，LLaMA-7B，结合三种 PEFT 算法⁵⁵5。我们在实验中排除了前缀微调，因为其在
    Mangrulkar 等人（[2022](#bib.bib54)）中的实现存在未解决的问题，而我们在准备此软件包时遇到了这些问题，包括 LoRA（Hu 等人，[2022](#bib.bib33)）、P-tuning（Liu
    等人，[2021b](#bib.bib51)）和 prompt tuning（Lester 等人，[2021](#bib.bib45)）。我们采用 FedAvg（McMahan
    等人，[2017](#bib.bib55)）作为联邦聚合策略。为了统一和公平地进行实验，我们固定了 FL 特定的超参数以及对计算成本影响较大的超参数。例如，我们将通信轮次设置为
    $500$。有关每种 PEFT 算法的更多算法特定超参数，请参见附录 [A.3](#A1.SS3 "A.3 Detailed hyperparameters
    and results ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning")。此外，为了进一步减少 GPU 内存消耗以提高效率，我们在微调过程中使用了半精度操作符。'
- en: 'Table 2: Performance comparisons among different PEFT algorithms when fine-tuning
    LLaMA-7B in FL: Evaluation Scores(%) $\pm$ standard deviation(%).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同 PEFT 算法在联邦学习中微调 LLaMA-7B 的性能比较：评估分数（%）$\pm$ 标准差（%）。
- en: '| Algorithm | Scenario | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Algorithm | Scenario | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
- en: '| LoRA | Global | 13.54$\pm$1.04 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | Global | 13.54$\pm$1.04 |'
- en: '| Fed | 13.29$\pm$1.37 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Fed | 13.29$\pm$1.37 |'
- en: '| Local | 10.99$\pm$1.35 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Local | 10.99$\pm$1.35 |'
- en: '| P-tuning | Global | 10.24$\pm$0.41 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| P-tuning | Global | 10.24$\pm$0.41 |'
- en: '| Fed | 9.71$\pm$0.39 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Fed | 9.71$\pm$0.39 |'
- en: '| Local | 7.78$\pm$0.96 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Local | 7.78$\pm$0.96 |'
- en: '| Prompt tuning | Global | 9.80$\pm$1.49 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Prompt tuning | Global | 9.80$\pm$1.49 |'
- en: '| Fed | 9.63$\pm$0.59 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Fed | 9.63$\pm$0.59 |'
- en: '| Local | 7.18$\pm$0.77 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Local | 7.18$\pm$0.77 |'
- en: 'Results and Analysis. Table [2](#S6.T2 "Table 2 ‣ 6.1 Effectiveness of PEFT
    algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning") shows the comparisons
    among different PEFT algorithms for federated fine-tuned LLaMA-7B under different
    scenarios. In summary, we can draw the following conclusions. (1) All algorithms
    with federated fine-tuning can significantly outperform those under the local
    scenario, and they all show very competitive results against those under the global
    scenario. This suggests that it is feasible and effective to federated fine-tuning
    LLMs with PEFT algorithms, which allows multiple entities to benefit from the
    collaborative training without directly sharing their private data. (2) Among
    these PEFT algorithms, LoRA shows the most promising performance and beats the
    other two algorithms by a large margin in all three scenarios. P-tuning and prompt
    tuning are two parameterized prompt algorithms that insert some learnable tokens
    into the model input to improve the performance of downstream tasks. However,
    they are outperformed by LoRA, a low-rank adaptation algorithm that augments each
    layer of LLMs with two low-rank matrices to capture new domain-specific knowledge.
    We conjecture that these parameterized prompt algorithms are limited by the knowledge
    encoded in the original LLM during the pre-training stage. This indicates that
    LoRA is a suitable PEFT algorithm for federate fine-tuning LLMs in future research
    or realistic scenarios.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与分析。表格 [2](#S6.T2 "表格 2 ‣ 6.1 PEFT 算法在 FS-LLM 中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM：一个用于联邦学习中大语言模型微调的综合包")
    展示了在不同场景下，不同 PEFT 算法对联邦微调 LLaMA-7B 的比较。总的来说，我们可以得出以下结论。(1) 所有使用联邦微调的算法都显著优于本地场景下的算法，并且它们在全球场景下的结果也非常具有竞争力。这表明，使用
    PEFT 算法进行联邦微调 LLM 是可行且有效的，这使得多个实体可以从协作训练中受益而无需直接分享其私有数据。(2) 在这些 PEFT 算法中，LoRA
    显示出了最有前景的性能，并且在所有三个场景下都大幅度超越了其他两个算法。P-tuning 和 prompt tuning 是两种参数化提示算法，通过将一些可学习的标记插入模型输入来提高下游任务的性能。然而，它们不及
    LoRA，后者是一种低秩适配算法，通过两个低秩矩阵增强每一层 LLM 以捕获新的领域特定知识。我们推测，这些参数化提示算法受到原始 LLM 在预训练阶段编码知识的限制。这表明，LoRA
    是未来研究或实际场景中进行联邦微调 LLM 的合适 PEFT 算法。
- en: 'How about federated fine-tuning previous-generation language models? As a comparison,
    we select the superior PEFT algorithm in Table [2](#S6.T2 "Table 2 ‣ 6.1 Effectiveness
    of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), LoRA (Hu
    et al., [2022](#bib.bib33)), to federated fine-tune a previous-generation language
    model, OPT-2.7B (Zhang et al., [2022](#bib.bib92)), which has fewer parameters
    than LLaMA-7B. Table [3](#S6.T3 "Table 3 ‣ 6.1 Effectiveness of PEFT algorithms
    in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning") shows the evaluation results of
    OPT-2.7B on the HumanEval, HELM evaluations, and GSM8K-test.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对上一代语言模型进行联邦微调情况如何？作为比较，我们选择了表格 [2](#S6.T2 "表格 2 ‣ 6.1 PEFT 算法在 FS-LLM 中的有效性
    ‣ 6 实验 ‣ FederatedScope-LLM：一个用于联邦学习中大语言模型微调的综合包") 中表现优越的 PEFT 算法 LoRA（Hu 等，[2022](#bib.bib33)），对上一代语言模型
    OPT-2.7B（Zhang 等，[2022](#bib.bib92)）进行联邦微调，后者的参数数量少于 LLaMA-7B。表格 [3](#S6.T3 "表格
    3 ‣ 6.1 PEFT 算法在 FS-LLM 中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM：一个用于联邦学习中大语言模型微调的综合包")
    展示了 OPT-2.7B 在 HumanEval、HELM 评估和 GSM8K-test 上的评估结果。
- en: 'Though the model performance of OPT-2.7B is improved with federated fine-tuning
    compared to those under the local scenario, it is not significant. Moreover, OPT-2.7B
    fails in the evaluation of some subtasks in HELM due to the exceeded input length,
    which limits the scope of the application of the previous-generation language
    models with fewer parameters. Comparing the performance of federated fine-tuned
    LLaMA-7B and OPT-2.7B in Figure [4](#S6.F4 "Figure 4 ‣ 6.1 Effectiveness of PEFT
    algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package
    for Fine-tuning Large Language Models in Federated Learning"), we observe that
    the current-generation LLM with a larger scale has an obvious advantage over the
    previous-generation language model with a smaller scale on various evaluation
    tasks. Thus, if the communication and computation costs are affordable, one should
    consider the larger scale of current-generation LLMs for more significant FL improvement
    and better absolute performance on downstream tasks.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与本地场景下的模型性能相比，OPT-2.7B 通过联邦微调有所提升，但提升并不显著。此外，由于输入长度超限，OPT-2.7B 在 HELM 的一些子任务评估中失败，这限制了参数较少的前一代语言模型的应用范围。通过比较图
    [4](#S6.F4 "图 4 ‣ 6.1 PEFT 算法在 FS-LLM 中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM：用于联邦学习中大规模语言模型微调的全面包")
    中联邦微调的 LLaMA-7B 和 OPT-2.7B 的性能，我们观察到当前一代大规模 LLM 在各种评估任务中明显优于前一代小规模语言模型。因此，如果通信和计算成本可承受，应考虑使用更大规模的当前一代
    LLM，以实现更显著的 FL 改进和更好的下游任务绝对性能。
- en: 'Table 3: Performance of federated fine-tuned previous-generation language model,
    OPT-2.7B, with LoRA: Evaluation Scores(%) $\pm$ standard deviation(%). (^∗OPT-2.7B
    fails on some subtasks in HELM due to the exceeded input length, and failed subtasks
    are excluded when calculating the final results.)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：联邦微调的前一代语言模型 OPT-2.7B 在 LoRA 下的性能：评估分数（%）$\pm$ 标准差（%）。 (^∗由于输入长度超限，OPT-2.7B
    在 HELM 的某些子任务中失败，失败的子任务在计算最终结果时被排除。)
- en: '| Algorithm | Scenario | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 场景 | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
- en: '| LoRA | Global | 0.61$\pm$0.11 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 全局 | 0.61$\pm$0.11 |'
- en: '| Fed | 0.43$\pm$0.17 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Fed | 0.43$\pm$0.17 |'
- en: '| Local | 0.25$\pm$0.22 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Local | 0.25$\pm$0.22 |'
- en: '![Refer to caption](img/4f2f4c9de962d4a9243393abb2bbefde.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4f2f4c9de962d4a9243393abb2bbefde.png)'
- en: 'Figure 4: Visualization of the performance comparison of fine-tuned LLaMA-7B
    and OPT-2.7B under federated and local scenarios. (The axes are scaled to highlight
    the differences.)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：联邦和本地场景下微调的 LLaMA-7B 和 OPT-2.7B 性能比较的可视化。（坐标轴经过缩放以突出差异。）
- en: 6.2 Efficiency of PEFT algorithms in FS-LLM
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 PEFT 算法在 FS-LLM 中的效率
- en: 'In this section, we evaluate the efficiency of various PEFT algorithms in FL.
    Among all the metrics, we focus on their GPU memory consumption, message size,
    and computation time in the federated fine-tuning process. We note that the GPU
    memory consumption refers to the model only, excluding the input tokens and the
    optimizer state, because the input length varies greatly across data and affects
    the GPU memory consumption during fine-tuning LLMs. For message size, we report
    the number of bytes of the serialized adapter parameters during one communication
    between the server and one client. The computation time is defined as the time
    duration of one training step with a batch size of $1$, from the start of forward
    propagation to the end of backward propagation. We report the computation time
    on two types of hardware, both with 512GB of RAM: Nvidia A100 GPU (80GB) with
    Intel Xeon Platinum 8369B CPU and Nvidia V100 GPU (32GB) with Intel Xeon Platinum
    8163 CPU. For a fair comparison, we follow the same experimental settings as Section [6.1](#S6.SS1
    "6.1 Effectiveness of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning")
    for all PEFT algorithms.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们评估了各种 PEFT 算法在 FL 中的效率。在所有指标中，我们重点关注它们在联邦微调过程中的 GPU 内存消耗、消息大小和计算时间。我们注意到，GPU
    内存消耗仅指模型本身，不包括输入令牌和优化器状态，因为输入长度在数据之间差异很大，并影响在微调 LLMs 时的 GPU 内存消耗。关于消息大小，我们报告在服务器与一个客户端之间进行一次通信时，序列化适配器参数的字节数。计算时间定义为一个训练步骤的时间长度，批量大小为
    $1$，从前向传播开始到后向传播结束。我们在两种硬件上报告计算时间，两者均有 512GB 的 RAM：Nvidia A100 GPU (80GB) 配有 Intel
    Xeon Platinum 8369B CPU 和 Nvidia V100 GPU (32GB) 配有 Intel Xeon Platinum 8163 CPU。为了公平比较，我们对所有
    PEFT 算法遵循与第 [6.1](#S6.SS1 "6.1 Effectiveness of PEFT algorithms in FS-LLM ‣ 6
    Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning") 节相同的实验设置。'
- en: 'Results and Analysis. As shown in Table [4](#S6.T4 "Table 4 ‣ 6.2 Efficiency
    of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), we can
    first notice that (1) fine-tuning with different PEFT algorithms has negligible
    impact on the GPU memory consumption. (2) However, there are large differences
    in the message sizes, which in turn lead to large variations in the transmission
    time. For example, for a client with 100Mbps bandwidth, the transmission time
    (upload and download) per round ranges from about 0.01 seconds (with prompt tuning)
    to 40 seconds (with P-tuning) when using different PEFT algorithms. (3) Moreover,
    from the table, we can observe that the computation time varies due to different
    GPUs, with almost a twofold difference. Therefore, a more critical issue deserves
    attention: federated fine-tuning LLMs may suffer from more idle time due to the
    heterogeneity of computing resources among different clients. Because this computation
    efficiency difference can be significant, the benefit of mitigating communication
    latency by sending messages asynchronously in multiple computational stages may
    diminish.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '结果与分析。如表 [4](#S6.T4 "Table 4 ‣ 6.2 Efficiency of PEFT algorithms in FS-LLM
    ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning
    Large Language Models in Federated Learning") 所示，我们首先注意到 (1) 使用不同 PEFT 算法进行微调对
    GPU 内存消耗的影响可以忽略不计。 (2) 然而，消息大小存在较大差异，这反过来导致了传输时间的较大变化。例如，对于带宽为 100Mbps 的客户端，使用不同
    PEFT 算法时每轮的传输时间（上传和下载）从大约 0.01 秒（使用提示微调）到 40 秒（使用 P-tuning）不等。 (3) 此外，从表中我们可以观察到，由于不同的
    GPU，计算时间存在差异，几乎差了两倍。因此，一个更关键的问题需要关注：由于不同客户端之间计算资源的异质性，联邦微调 LLMs 可能会遭遇更多的空闲时间。由于这种计算效率差异可能很大，通过在多个计算阶段异步发送消息来减轻通信延迟的好处可能会减少。'
- en: 'Therefore, there are two research directions for federated fine-tuning LLMs
    in efficiency: (1) how to leverage the idle time of computation-resource-rich
    clients while they wait for computation-resource-limited clients to complete local
    updates, and (2) how to optimize the utilization of the available bandwidth resources
    in computation-resource-limited clients during computation.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，联邦微调 LLMs 在效率方面有两个研究方向：(1) 如何利用计算资源丰富的客户端的空闲时间，等待计算资源有限的客户端完成本地更新，(2) 如何在计算资源有限的客户端的计算过程中优化可用带宽资源的利用。
- en: 'Table 4: Efficiency comparisons among different PEFT algorithms when fine-tuning
    LLaMA-7B in FL. (^∗The GPU usage is subject to minor variations due to the differences
    in Cuda versions.)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 FL 中微调 LLaMA-7B 时不同 PEFT 算法的效率比较。（^∗GPU 使用情况由于 Cuda 版本的不同可能会有所变化。）
- en: '|  | LoRA | P-tuning | Prompt tuning |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA | P-tuning | Prompt tuning |'
- en: '| GPU Usage^∗ (MB) | 13,450 | 13,538 | 13,442 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPU 使用^∗（MB） | 13,450 | 13,538 | 13,442 |'
- en: '| Message Size (MB) | 21.40 | 256.48 | 0.17 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 消息大小（MB） | 21.40 | 256.48 | 0.17 |'
- en: '| Comp. Time on A100 (Sec.) | 0.16$\pm$0.04 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 在 A100 上的计算时间（秒） | 0.16$\pm$0.04 |'
- en: '| Comp. Time on V100 (Sec.) | 0.33$\pm$0.10 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 在 V100 上的计算时间（秒） | 0.33$\pm$0.10 |'
- en: 6.3 Fine-tuning LLMs without accessing the full model in FL
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 在 FL 中不访问完整模型的 LLM 微调
- en: 'In this section, we investigate the performance of federated fine-tuning LLMs
    without accessing the full model. As mentioned in Section [4.2](#S4.SS2 "4.2 Federated
    fine-tuning without accessing full model ‣ 4 LLM-AlgZoo: A Collection of Fine-tuning
    Algorithms ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning"), we adapt a privacy-preserving fine-tuning
    algorithm, offsite-tuning (Xiao et al., [2023](#bib.bib87)), to federated scenarios.
    Specifically, we use the first and last two layers of LLaMA-7B as the adapter
    and compress the model as the emulator by dropping $20\%$ of the remaining layers
    uniformly. Then the server broadcasts both the adapter and emulator to all clients,
    and the clients only fine-tune the adapter with FedAvg. We compare the performance
    of LLMs with fine-tuned adapters via federated offsite-tuning (denoted as FedOT)
    and corresponding local offsite-tuning (denoted as LocalOT). Following Section [6.1](#S6.SS1
    "6.1 Effectiveness of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM:
    A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"),
    we benchmark FedOT on LLM-Benchmarks.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了在不访问完整模型的情况下联邦微调 LLM 的性能。如第[4.2节](#S4.SS2 "4.2 在不访问完整模型的情况下的联邦微调
    ‣ 4 LLM-AlgZoo：微调算法集 ‣ FederatedScope-LLM：在联邦学习中微调大型语言模型的综合包")所述，我们将一种隐私保护的微调算法
    offsite-tuning（Xiao 等， [2023](#bib.bib87)）适应到联邦场景中。具体来说，我们使用 LLaMA-7B 的第一层和最后两层作为适配器，并通过均匀丢弃剩余层的
    $20\%$ 将模型压缩为模拟器。然后服务器将适配器和模拟器广播到所有客户端，客户端仅使用 FedAvg 对适配器进行微调。我们通过联邦 offsite-tuning（简称
    FedOT）和相应的本地 offsite-tuning（简称 LocalOT）比较了微调适配器的 LLM 的性能。按照第[6.1节](#S6.SS1 "6.1
    PEFT 算法在 FS-LLM 中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM：在联邦学习中微调大型语言模型的综合包")，我们在 LLM-Benchmarks
    上基准测试了 FedOT。
- en: 'Results and Analysis. We present the evaluation scores of FedOT and LocalOT
    in Table [5](#S6.T5 "Table 5 ‣ 6.3 Fine-tuning LLMs without accessing the full
    model in FL ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive Package for
    Fine-tuning Large Language Models in Federated Learning"). We can have the following
    observations: (1) Comparing FedOT and LocalOT, FL offers significant benefits
    for this privacy-preserving fine-tuning algorithm for federated fine-tuning LLMs
    without accessing the full model. This demonstrates that FedOT can still enable
    multiple entities to benefit from collaborative training without sharing their
    private data directly when they cannot access the full model. (2) When the dropping
    rate is $20\%$, the model loses a large amount of knowledge from the pre-training
    stage, almost fails to retain the capacity of the CoT and code generation, and
    hardly acquire new knowledge from the fine-tuning. There is a trade-off between
    the compression rate and the performance of LLMs: increasing the compression rate
    enhances the privacy of LLMs but degrades their performance. This indicates that
    how to compress LLMs while maintaining their generalization ability and model
    privacy is a promising research direction to explore.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与分析。我们在表[5](#S6.T5 "表 5 ‣ 6.3 在 FL 中不访问完整模型的 LLM 微调 ‣ 6 实验 ‣ FederatedScope-LLM：在联邦学习中微调大型语言模型的综合包")中展示了
    FedOT 和 LocalOT 的评估分数。我们可以得出以下观察结果：(1) 比较 FedOT 和 LocalOT，FL 对于这种隐私保护的微调算法在不访问完整模型的情况下，显著提升了联邦微调
    LLM 的效果。这表明，FedOT 仍能使多个实体在无法访问完整模型时，通过协作训练受益，而不直接共享其私有数据。(2) 当丢失率为 $20\%$ 时，模型在预训练阶段丧失了大量知识，几乎无法保留
    CoT 和代码生成的能力，也难以从微调中获取新知识。压缩率与 LLM 性能之间存在权衡：增加压缩率可以增强 LLM 的隐私性，但会降低其性能。这表明，如何在保持
    LLM 的泛化能力和模型隐私的同时进行压缩是一个值得探索的有前景的研究方向。
- en: 'Table 5: Performance comparisons between different compression rates (dropping
    layers uniformly) when fine-tuning LLaMA-7B without accessing the full model under
    federated and local scenarios: Evaluation Scores(%) $\pm$ standard deviation(%).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在联邦和本地场景下，不访问完整模型时，调整LLaMA-7B的不同压缩率（均匀丢弃层）的性能比较：评估得分（%）$\pm$ 标准差（%）。
- en: '| Dropping Rate | Scenario | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 丢弃率 | 场景 | Fed-CodeAlpaca | Fed-Dolly | Fed-GSM8K-3 |'
- en: '| $20\%$0.71 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $20\%$ | 0.71 |'
- en: '| Local | 0.18$\pm$2.91 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.18$\pm$2.91 |'
- en: '| $50\%$0.98 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| $50\%$ | 0.98 |'
- en: '| Local | 0.00$\pm$1.29 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 0.00$\pm$1.29 |'
- en: 6.4 Personalized federated learning for LLMs
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 个性化联邦学习用于LLM
- en: To explore the potential of personalized federated learning algorithms (Tan
    et al., [2022](#bib.bib78)) for fine-tuning LLMs in FL, we compare pFedMe (T Dinh
    et al., [2020](#bib.bib77)) and FedAvg (McMahan et al., [2017](#bib.bib55)) with
    LoRA in this subsection under different data heterogeneity. To simulate different
    levels of data heterogeneity, we create variants of Fed-CodeAlpaca.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索个性化联邦学习算法（Tan等，[2022](#bib.bib78)）在FL中微调LLM的潜力，我们在不同数据异质性下将pFedMe（T Dinh等，[2020](#bib.bib77)）和FedAvg（McMahan等，[2017](#bib.bib55)）与LoRA进行比较。为了模拟不同级别的数据异质性，我们创建了Fed-CodeAlpaca的变体。
- en: 'We use the programming language of the code samples as the label and split
    the fine-tuning dataset into nine clients by Latent Dirichlet Allocation (i.e.,
    Dirichlet splitter). We use four different values for $\alpha$. Along with the
    original Fed-CodeAlpaca dataset, we obtain five federated fine-tuning datasets
    with different heterogeneity. Following Section [6.1](#S6.SS1 "6.1 Effectiveness
    of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), we repeat
    each experiment three times with different random seeds. The averaged evaluation
    scores (Pass@1 scores) with their standard deviation are reported. We note that
    evaluation scores with pFedMe are obtained by benchmarking each personalized client
    individually and then computing their average scores.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用代码示例中的编程语言作为标签，并通过潜在狄利克雷分配（即狄利克雷分割器）将微调数据集分为九个客户端。我们使用四个不同的$\alpha$值。结合原始的Fed-CodeAlpaca数据集，我们获得了五个具有不同异质性的联邦微调数据集。按照第[6.1节](#S6.SS1
    "6.1 PEFT算法在FS-LLM中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM：一个用于在联邦学习中微调大语言模型的综合包")，我们将每个实验重复三次，使用不同的随机种子。报告了平均评估得分（Pass@1得分）及其标准差。我们注意到，使用pFedMe的评估得分是通过单独基准测试每个个性化客户端，然后计算它们的平均得分得到的。
- en: 'Results and Analysis. We present the performance of fine-tuning with LoRA using
    FedAvg and pFedMe in different data heterogeneities in Figure [5(a)](#S6.F5.sf1
    "In Figure 5 ‣ 6.4 Personalized federated learning for LLMs ‣ 6 Experiments ‣
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning"), which shows the performance of fine-tuning with FedAvg
    gradually approaches that under the global scenario as data heterogeneity decreases.
    However, pFedMe surprisingly does not outperform FedAvg under any data distribution,
    which shows different results with previous language models in pFL-bench (Chen
    et al., [2022](#bib.bib11)). We analyze this phenomenon and find that: (1) To
    improve efficiency, we use the half-precision operator to fine-tune LLMs in the
    experiment. However, this leads to a more pronounced precision loss for pFedMe
    than FedAvg, since pFedMe multiplies the update of the local model with respect
    to the global model by a small learning rate. This adversely impacts the performance
    of pFedMe. (2) The use of acceleration operators for efficient LLM fine-tuning
    restricts the range of hyperparameter space in these pFL algorithms, affecting
    the upper bound of algorithm performance in the valid parameter space. For example,
    assuming the LLM performs better when the learning rate is $0.00001$, but at this
    point, using half-precision or mixed-precision training for efficiency, the model
    can barely be updated due to the precision loss.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与分析。我们展示了在不同数据异质性下使用FedAvg和pFedMe进行LoRA微调的表现，如图[5(a)](#S6.F5.sf1 "在图5 ‣ 6.4
    个性化联邦学习 ‣ 6 实验 ‣ FederatedScope-LLM：一个全面的包用于联邦学习中的大规模语言模型微调")所示，这表明随着数据异质性的减少，FedAvg微调的性能逐渐接近于全局场景下的性能。然而，令人惊讶的是，pFedMe在任何数据分布下都没有超越FedAvg，这与以前在pFL-bench中的语言模型结果不同(Chen
    et al., [2022](#bib.bib11))。我们分析了这一现象，并发现：(1) 为了提高效率，我们在实验中使用了半精度运算符来微调LLMs。然而，这导致pFedMe的精度损失比FedAvg更为明显，因为pFedMe以较小的学习率对局部模型相对于全局模型的更新进行加权。这不利于pFedMe的性能。(2)
    使用加速运算符进行高效LLM微调限制了这些pFL算法的超参数空间范围，影响了有效参数空间中的算法性能上限。例如，假设LLM在学习率为$0.00001$时表现更好，但此时由于精度损失，使用半精度或混合精度训练以提高效率时，模型几乎无法更新。
- en: Based on these observations, we believe how to ensure the compatibility of various
    efficient training operators and different pFL algorithms is still unclear and
    deserves more attention from the community. Besides, sharing a common base LLM
    and only maintaining multiple versions of adapters may not be compatible with
    some existing pFL algorithms because pFL algorithms may introduce access conflicts.
    For instance, when a penalty term is used to constrain the large updates of the
    local model, the computation to compute updates requires using both the global
    adapter with the base LLM and the local adapter with the base LLM in the same
    step. Significantly, more memory cost is required to avoid the conflict by maintaining
    multiple copies of LLMs and their adapters. Resolving this challenge may require
    new algorithm development or a new pFL computation pattern for future work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些观察，我们认为如何确保各种高效训练运算符和不同pFL算法的兼容性仍不明确，需要社区更多关注。此外，共享一个公共基础LLM并仅维护多个版本的适配器可能与一些现有的pFL算法不兼容，因为pFL算法可能会引入访问冲突。例如，当使用惩罚项约束局部模型的大更新时，计算更新的过程需要在同一步骤中使用基础LLM的全局适配器和基础LLM的局部适配器。显著的是，避免冲突需要维护多个LLMs及其适配器，这会消耗更多的内存成本。解决这一挑战可能需要新的算法开发或未来工作的新的pFL计算模式。
- en: '![Refer to caption](img/d2119d4856cc7077dc3211f6a2ade904.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2119d4856cc7077dc3211f6a2ade904.png)'
- en: '(a) The comparison of performance with pFedMe and FedAvg over different data
    heterogeneity: the higher $\alpha$, the lower the data heterogeneity among clients.
    Global stands for fine-tuning LLMs under the global scenario, and Base stands
    for the original model, which is not fine-tuned.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同数据异质性下pFedMe和FedAvg性能的比较：$\alpha$越高，客户端之间的数据异质性越低。Global表示在全局场景下对LLMs进行微调，Base表示原始模型，未进行微调。
- en: '![Refer to caption](img/df737abb8f6f25808a96e664234f4fad.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df737abb8f6f25808a96e664234f4fad.png)'
- en: (b) The landscape of the rank of the validation loss and the Pass@1 scores over
    all the hyperparameter combinations. The grey surface shows the distribution of
    the rank based on the Pass@1 scores, and the blue-red surface indicates the distribution
    of the rank based on the validation loss.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: （b）所有超参数组合下验证损失和 Pass@1 分数的排名分布图。灰色表面显示了基于 Pass@1 分数的排名分布，而蓝红色表面则表示基于验证损失的排名分布。
- en: 'Figure 5: Fine-tuning LLMs in pFL (Left) and FedHPO (Right).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 pFL（左）和 FedHPO（右）中微调 LLMs。
- en: 6.5 Study of the FedHPO for LLMs
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 研究 LLMs 的 FedHPO
- en: 'Since fine-tuning LLMs in FL is very costly, it is usually infeasible to perform
    full-fidelity hyperparameter optimization. However, we observe that the performance
    of fine-tuned LLMs in FL is highly dependent on the choice of hyperparameters
    (see Appendix [A.3](#A1.SS3 "A.3 Detailed hyperparameters and results ‣ Appendix
    A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning")). Therefore, we investigate whether we
    can use low-fidelity FedHPO (Wang et al., [2023](#bib.bib82)) methods in this
    scenario. We follow the experiment settings in Section [6.1](#S6.SS1 "6.1 Effectiveness
    of PEFT algorithms in FS-LLM ‣ 6 Experiments ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning") and use
    LoRA to fine-tune LLaMA-7B on Fed-CodeAlpaca yet with lower fidelity (i.e., fewer
    communication rounds). We rank all the hyperparameter combinations searched by
    their validation loss in ascending order and evaluation scores in descending order
    separately. We plot the two landscapes of the rank to explore the feasibility
    of using low-fidelity FedHPO methods when federated fine-tuning LLMs.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '由于在 FL 中微调 LLMs 的成本非常高，因此通常无法进行完全保真的超参数优化。然而，我们观察到在 FL 中微调 LLMs 的性能高度依赖于超参数的选择（见附录
    [A.3](#A1.SS3 "A.3 详细的超参数及结果 ‣ 附录 A 附录 ‣ FederatedScope-LLM: 一个全面的联邦学习大型语言模型微调包")）。因此，我们调查了在这种情况下是否可以使用低保真的
    FedHPO（Wang 等人，[2023](#bib.bib82)）方法。我们遵循了第 [6.1](#S6.SS1 "6.1 PEFT 算法在 FS-LLM
    中的有效性 ‣ 6 实验 ‣ FederatedScope-LLM: 一个全面的联邦学习大型语言模型微调包") 节中的实验设置，使用 LoRA 对 Fed-CodeAlpaca
    上的 LLaMA-7B 进行微调，但保真度较低（即通信轮次较少）。我们将所有超参数组合按验证损失升序和评价分数降序分别排序。我们绘制了排名的两个分布图，以探讨在联邦微调
    LLMs 时使用低保真度 FedHPO 方法的可行性。'
- en: 'Results and Analysis. Based on the results shown in Figure [5(b)](#S6.F5.sf2
    "In Figure 5 ‣ 6.4 Personalized federated learning for LLMs ‣ 6 Experiments ‣
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning"), we distill the following observations. (1) We observe
    that the rank of the evaluation scores of the fine-tuned LLMs varies drastically
    and non-smoothly with respect to the hyperparameter changes. This poses a great
    challenge for finding the optimal hyperparameters, as it requires a fine-grained
    and exhaustive search over the hyperparameter space. (2) Moreover, we reveal a
    significant discrepancy between the ranks of validation loss and the ranks of
    final generalization performance in evaluation tasks during fine-tuning. This
    implies that the validation loss may not reflect the generalization ability of
    the fine-tuned LLMs.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '结果与分析。根据图 [5(b)](#S6.F5.sf2 "图 5 ‣ 6.4 针对 LLMs 的个性化联邦学习 ‣ 6 实验 ‣ FederatedScope-LLM:
    一个全面的联邦学习大型语言模型微调包") 中的结果，我们提炼出以下观察结果：（1）我们观察到，微调 LLMs 的评价分数排名随着超参数的变化而剧烈且不平滑地变化。这对找到最优超参数提出了巨大挑战，因为这需要在超参数空间中进行细致且全面的搜索。（2）此外，我们发现验证损失排名与微调过程中最终泛化性能的排名之间存在显著差异。这意味着验证损失可能无法反映微调
    LLMs 的泛化能力。'
- en: 'In summary, we uncover two major challenges for fine-tuning LLMs in FedHPO:
    (1) the evaluation scores are highly sensitive and non-smooth to the hyperparameter
    changes, and (2) the validation loss may not be a reliable indicator of the generalization
    performance. These challenges identify two promising yet unexplored research directions
    for future work on fine-tuning LLMs in FedHPO. The first direction is to develop
    fine-grained but efficient FedHPO methods for finding the optimal hyperparameters
    on federated fine-tuning LLMs, which can avoid exhaustive searches over the hyperparameter
    space. The second direction is to exploit concurrent exploration in FedHPO to
    evaluate the generalization ability of the client-side hyperparameters with low
    fidelity for each client during the FL process.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们揭示了在FedHPO中微调LLMs的两个主要挑战：（1）评估分数对超参数的变化非常敏感且不平滑，（2）验证损失可能不是泛化性能的可靠指标。这些挑战确定了未来在FedHPO中微调LLMs的两个有前景但尚未探索的研究方向。第一个方向是开发细粒度但高效的FedHPO方法，以在联邦微调LLMs中找到最佳超参数，从而避免对超参数空间的穷尽搜索。第二个方向是利用FedHPO中的并行探索，以评估在FL过程中每个客户端的客户端超参数的低保真度泛化能力。
- en: 7 Discussions and Future Directions
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论与未来方向
- en: This paper introduces a comprehensive and practical package for federated fine-tuning
    LLMs. Our experimental results illustrate how our FS-LLM bridges the gaps between
    the universal FL framework and the need for fine-tuning LLMs under various FL
    settings. More importantly, our benchmark results also provide positive guidance
    and insights for the research community regarding how to optimize the federated
    fine-tuning and what sub-problems in the field deserve more focus.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个全面且实用的联邦微调LLMs的软件包。我们的实验结果说明了FS-LLM如何弥合通用FL框架与在各种FL设置下微调LLMs的需求之间的差距。更重要的是，我们的基准结果还为研究社区提供了积极的指导和见解，关于如何优化联邦微调以及该领域哪些子问题值得更多关注。
- en: However, the results of this paper are limited by several factors. (1) Due to
    the resource limit, all experiments use a batch size of $1$, but federated fine-tuning
    LLMs with larger batch sizes might perform better. (2) We also find that designing
    different prompts (either in fine-tuning or evaluation) will impact the evaluation
    results. To ensure a fair evaluation and comparison, we use a fixed prompt, but
    more explorations are possible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本文的结果受到几个因素的限制。（1）由于资源限制，所有实验使用的批次大小为$1$，但具有较大批次大小的联邦微调LLMs可能表现更好。（2）我们还发现，设计不同的提示（无论是在微调还是评估中）会影响评估结果。为了确保公平评估和比较，我们使用了固定的提示，但仍有更多的探索空间。
- en: Based on our observations and experiments, we outline some promising directions
    for future research in federated fine-tuning LLMs as follows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的观察和实验，我们概述了一些联邦微调LLMs未来研究的有前景的方向。
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Designing computation-efficient fine-tuning algorithms for federated fine-tuning
    LLMs. Even with PEFT algorithms, the computation cost is still too high for most
    resource-limited clients. Reducing the computation cost can lower the barrier
    for more data holders and allow more entities to benefit from the federated fine-tuning
    LLMs.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为联邦微调LLMs设计计算效率高的微调算法。即使使用PEFT算法，计算成本对大多数资源有限的客户端而言仍然过高。降低计算成本可以降低更多数据持有者的门槛，让更多实体从联邦微调LLMs中受益。
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Exploring more privacy-preserving fine-tuning algorithms without accessing the
    full model in FL. FedOT suffers from a trade-off between model compression rate
    and model performance. Addressing this issue would protect the sensitive information
    of LLMs in FL from exposing pre-training data and the valuable full model, which
    could be exploited by malicious entities for adversarial attacks or commercial
    gains while maintaining model performance in FL.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索更多隐私保护的微调算法，而不访问FL中的完整模型。FedOT面临模型压缩率与模型性能之间的权衡。解决这个问题将有助于保护FL中LLMs的敏感信息，防止预训练数据和有价值的完整模型被恶意实体用于对抗攻击或商业利益，同时保持模型在FL中的性能。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimizing pFL algorithms to enable robust combination with various accelerating
    and resource-efficient operators. If performance degradation due to low-precision
    training can be overcome, it would improve the personalized model performance
    when data are heterogeneous and computation resources are limited among clients.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化pFL算法，以实现与各种加速和资源高效操作符的稳健组合。如果可以克服由于低精度训练导致的性能下降，那么它将改善当数据异质且客户端计算资源有限时的个性化模型性能。
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Investigating low-fidelity FedHPO methods for fine-tuning LLMs in FL. Based
    on our experimental results, we find the inconsistency between validation loss
    and the generalization performance of LLMs. Overcoming this inconsistency would
    help find optimal hyperparameters for federated fine-tuning LLMs with low cost,
    resulting in better generalization performance.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调查低保真度FedHPO方法在FL中微调LLM。根据我们的实验结果，我们发现验证损失与LLM的泛化性能之间的不一致。克服这种不一致将有助于以低成本找到联邦微调LLM的最佳超参数，从而获得更好的泛化性能。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extending the federated LLM fine-tuning to cross-device setting. As we have
    already observed the demand for federated fine-tuning LLMs in the cross-silo scenario,
    we also notice a similar need in the cross-device scenario (Lai et al., [2022](#bib.bib43);
    Chen et al., [2023](#bib.bib12); Gao et al., [2023](#bib.bib28)). In the cross-device
    scenario, the clients are more numerous and heterogeneous, the computational resources
    are more limited, and the network conditions are more diverse. How to federated
    fine-tune LLMs under the cross-device scenario is an urgent problem that needs
    to be solved.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将联邦LLM微调扩展到跨设备设置。正如我们已经观察到在跨孤岛场景中对联邦微调LLM的需求一样，我们还注意到在跨设备场景中也有类似的需求（Lai 等， [2022](#bib.bib43)；Chen
    等，[2023](#bib.bib12)；Gao 等，[2023](#bib.bib28)）。在跨设备场景中，客户端数量更多且异质性更强，计算资源更有限，网络条件更加多样。如何在跨设备场景中进行联邦微调LLM是一个亟待解决的问题。
- en: 8 Conclusions
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we first identify gaps that need to be addressed between fine-tuning
    LLMs in federated settings and the existing universal FL frameworks. To bridge
    these gaps, we introduce our open-source package, FS-LLM, with rich functionalities
    and extensibilities, which supports federated fine-tuning LLMs under various FL
    scenarios. We conduct extensive experiments to demonstrate the utility of our
    package and gain insights into how to fine-tune LLMs in FL settings. Based on
    the findings from these experimental results, we outline some promising directions
    for future research in federated LLM fine-tuning to advance the FL and LLM community.
    We have released FS-LLM at [https://github.com/alibaba/FederatedScope/tree/llm](https://github.com/alibaba/FederatedScope/tree/llm)
    for promoting further research.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先识别出需要解决的联邦环境下LLM微调与现有通用FL框架之间的差距。为了弥补这些差距，我们推出了开源包FS-LLM，具有丰富的功能和扩展性，支持在各种FL场景下进行联邦微调LLM。我们进行了广泛的实验，以展示我们包的实用性，并获得有关如何在FL环境中微调LLM的见解。基于这些实验结果，我们概述了一些有前景的未来研究方向，以推动FL和LLM社区的发展。我们已经在
    [https://github.com/alibaba/FederatedScope/tree/llm](https://github.com/alibaba/FederatedScope/tree/llm)
    发布了FS-LLM，以促进进一步的研究。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alex et al. (2021) Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur,
    Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis
    Carlier, et al. RAFT: A Real-World Few-Shot Text Classification Benchmark. *arXiv
    preprint arXiv:2109.14076*, 2021.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alex et al. (2021) 尼尔·亚历克斯，伊莱·利夫兰，路易斯·坦斯特尔，阿比谢克·塔库尔，佩佳·马哈姆，C·杰斯·里德尔，埃米·海因，卡罗琳·阿什赫斯特，保罗·塞迪尔，亚历克西斯·卡尔耶，等。RAFT：一个真实世界的小样本文本分类基准。*arXiv预印本
    arXiv:2109.14076*，2021。
- en: Anthropic (2023) Anthropic. Introducing Claude, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2023) Anthropic。介绍Claude，2023。
- en: 'Bajaj et al. (2016) Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng
    Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
    et al. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. *arXiv
    preprint arXiv:1611.09268*, 2016.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bajaj et al. (2016) 帕亚尔·巴贾杰，丹尼尔·坎波斯，尼克·克拉斯韦尔，李登，简锋·高，肖冬·刘，兰根·马久姆德，安德鲁·麦克纳马拉，巴斯卡·米特拉，崔·阮，等。MS
    MARCO：一个人类生成的机器阅读理解数据集。*arXiv预印本 arXiv:1611.09268*，2016。
- en: Bergstra & Bengio (2012) James Bergstra and Yoshua Bengio. Random Search for
    Hyper-Parameter Optimization. *Journal of machine learning research*, 13(2), 2012.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra & Bengio (2012) 詹姆斯·伯格斯特拉和约书亚·本吉奥。随机搜索用于超参数优化。*机器学习研究期刊*，13(2)，2012。
- en: Bi et al. (2023) Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu,
    and Qi Tian. Accurate medium-range global weather forecasting with 3D neural networks.
    *Nature*, 619:533 – 538, 2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等 (2023) Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, 和
    Qi Tian. 使用3D神经网络进行准确的中范围全球天气预测. *Nature*, 619:533 – 538, 2023年。
- en: 'Bonawitz et al. (2019) Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,
    Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
    Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System
    design. In *Proc. of machine learning and systems (MLSys’19)*, volume 1, pp. 
    374–388, 2019.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonawitz 等 (2019) Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry
    Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano Mazzocchi,
    Brendan McMahan, 等. 朝向大规模联邦学习：系统设计. 收录于 *机器学习与系统 (MLSys’19) 会议论文集*，第1卷，第374–388页，2019年。
- en: Borkan et al. (2019) Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain,
    and Lucy Vasserman. Nuanced Metrics for Measuring Unintended Bias with Real Data
    for Text Classification. In *Companion Proceedings of the World Wide Web Conference
    (WWW’19)*, pp.  491–500, 2019.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borkan 等 (2019) Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain,
    和 Lucy Vasserman. 用真实数据测量意外偏见的细致指标. 收录于 *万维网会议 (WWW’19) 附录会议论文集*，第491–500页，2019年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language Models are Few-Shot Learners. In *Proc. of the Advances
    in Neural Information Processing Systems (NeurIPS’20)*, volume 33, pp.  1877–1901,
    2020.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 语言模型是少样本学习者. 收录于 *神经信息处理系统进展 (NeurIPS’20)*，第33卷，第1877–1901页，2020年。
- en: 'Caldas et al. (2018) Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian
    Li, Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf:
    A benchmark for federated settings. *arXiv preprint arXiv:1812.01097*, 2018.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caldas 等 (2018) Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li,
    Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, 和 Ameet Talwalkar. Leaf: 一个联邦设置的基准.
    *arXiv 预印本 arXiv:1812.01097*, 2018年。'
- en: 'Chaudhary (2023) Sahil Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaudhary (2023) Sahil Chaudhary. Code alpaca: 一种用于代码生成的跟随指令的 llama 模型. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)，2023年。'
- en: 'Chen et al. (2022) Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin
    Ding. pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning.
    In *Proc. of the Advances in Neural Information Processing Systems (NeurIPS’22)*,
    pp.  9344–9360, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2022) Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, 和 Bolin Ding.
    pFL-Bench: 个性化联邦学习的综合基准. 收录于 *神经信息处理系统进展 (NeurIPS’22)*，第9344–9360页，2022年。'
- en: 'Chen et al. (2023) Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao
    Li, Yaliang Li, Bolin Ding, and Jingren Zhou. FS-Real: Towards Real-World Cross-Device
    Federated Learning. In *Proc. of the ACM SIGKDD International Conference on Knowledge
    Discovery and Data Mining (KDD’23)*, pp.  3829–3841, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023) Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li,
    Yaliang Li, Bolin Ding, 和 Jingren Zhou. FS-Real: 向现实世界跨设备联邦学习迈进. 收录于 *ACM SIGKDD
    国际知识发现与数据挖掘会议论文集 (KDD’23)*，第3829–3841页，2023年。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, et al. Evaluating Large
    Language Models Trained on Code. *arXiv perprint arXiv:2107.03374*, 2021.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2021) Mark Chen, Jerry Tworek, Heewoo Jun, 等. 评估基于代码训练的大型语言模型. *arXiv
    预印本 arXiv:2107.03374*, 2021年。
- en: 'Cheng et al. (2021) Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen,
    Dimitrios Papadopoulos, and Qiang Yang. Secureboost: A lossless federated learning
    framework. *IEEE Intelligent Systems*, 36:87–98, 2021.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 (2021) Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, Dimitrios
    Papadopoulos, 和 Qiang Yang. Secureboost: 无损的联邦学习框架. *IEEE Intelligent Systems*,
    36:87–98, 2021年。'
- en: 'Choi et al. (2018) Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih,
    Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC: Question Answering in Context.
    In *Proc. of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP’18)*, pp.  2174–2184, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi 等 (2018) Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin
    Choi, Percy Liang, 和 Luke Zettlemoyer. QuAC: 上下文中的问题回答. 收录于 *自然语言处理经验方法会议 (EMNLP’18)*，第2174–2184页，2018年。'
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al.
    PaLM: Scaling Language Modeling with Pathways. *arXiv preprint arXiv:2204.02311*,
    2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等 (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, 等. PaLM:
    使用路径扩展语言建模. *arXiv 预印本 arXiv:2204.02311*, 2022年。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty
    of Natural Yes/No Questions. In *Proc. of the Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies
    (NAACL’19)*, pp.  2924–2936, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2019) 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚科夫斯基、迈克尔·柯林斯和克里斯蒂娜·托塔诺娃。BoolQ:
    探索自然是/否问题的意外困难。在*北美计算语言学协会：人类语言技术会议（NAACL’19）*，第 2924–2936 页，2019。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math
    Word Problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) 卡尔·科比、维尼特·科萨拉朱、穆罕默德·巴瓦里安、马克·陈、崔伍俊、卢卡斯·凯泽、马蒂亚斯·普拉普特、杰瑞·特沃雷克、雅各布·希尔顿、内田怜一郎、克里斯托弗·赫斯和约翰·舒尔曼。训练验证者解决数学文字题。*arXiv
    预印本 arXiv:2110.14168*，2021。
- en: 'Computer (2023) Together Computer. RedPajama: An Open Source Recipe to Reproduce
    LLaMA training dataset. [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data),
    2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Computer (2023) Together Computer。RedPajama: 一个开源配方用于重现 LLaMA 训练数据集。 [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)，2023。'
- en: 'Conover et al. (2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conover et al. (2023) 迈克·科诺弗、马特·海斯、安基特·马图尔、谢剑伟、俊万、萨姆·沙赫、阿里·戈德西、帕特里克·温德尔、马泰伊·扎哈里亚和雷诺德·辛。Free
    Dolly: 介绍世界上第一个真正开放的指令调优大语言模型，2023。'
- en: Dai et al. (2020) Zhongxiang Dai, Bryan Kian Hsiang Low, and Patrick Jaillet.
    Federated Bayesian Optimization via Thompson Sampling. In *Proc. of the Advances
    in Neural Information Processing Systems (NeurIPS’20)*, volume 33, pp.  9687–9699,
    2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2020) 戴中祥、布莱恩·基安·香·洛和帕特里克·贾利特。通过汤普森抽样进行联邦贝叶斯优化。在*神经信息处理系统会议（NeurIPS’20）*，第
    33 卷，第 9687–9699 页，2020。
- en: Deutsch (1996a) L. Peter Deutsch. DEFLATE Compressed Data Format Specification
    version 1.3. RFC 1951, Network Working Group, 1996a.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deutsch (1996a) L. Peter Deutsch. DEFLATE 压缩数据格式规范 版本 1.3。RFC 1951，网络工作组，1996a。
- en: Deutsch (1996b) L. Peter Deutsch. GZIP file format specification version 4.3.
    RFC 1952, Network Working Group, 1996b.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deutsch (1996b) L. Peter Deutsch. GZIP 文件格式规范 版本 4.3。RFC 1952，网络工作组，1996b。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proc. of NAACL-HLT (NAACL-HLT’19)*, pp.  4171–4186, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) 雅各布·德夫林、明伟·张、肯顿·李和克里斯蒂娜·托塔诺娃。BERT: 用于语言理解的深度双向变换器预训练。在*NAACL-HLT
    会议（NAACL-HLT’19）*，第 4171–4186 页，2019。'
- en: Dong et al. (2022) Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, and Yaliang
    Li. Collaborating Heterogeneous Natural Language Processing Tasks via Federated
    Learning. *arXiv preprint arXiv:2212.05789*, 2022.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2022) 陈赫东、谢跃翔、丁博霖、沈颖、李亚亮。通过联邦学习协作异构自然语言处理任务。*arXiv 预印本 arXiv:2212.05789*，2022。
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. PaLM-E: An Embodied Multimodal Language Model. *arXiv preprint
    arXiv:2303.03378*, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess et al. (2023) 丹尼·德里斯、肖飞、梅赫迪·SM·萨贾迪、科里·林奇、阿坎莎·乔杜里、布赖恩·伊切特、艾扎恩·瓦希德、乔纳森·汤普森、关望、田赫宇等。PaLM-E:
    一种具身的多模态语言模型。*arXiv 预印本 arXiv:2303.03378*，2023。'
- en: Fang et al. (2021) Wenjing Fang, Derun Zhao, Jin Tan, Chaochao Chen, Chaofan
    Yu, Li Wang, Lei Wang, Jun Zhou, and Benyu Zhang. Large-scale secure xgb for vertical
    federated learning. In *Proc. of the Conference on Information & Knowledge Management
    (CIKM’21)*, pp.  443–452, 2021.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2021) 方文静、赵德润、谭锦、陈超超、余超凡、王丽、王磊、周俊和张本宇。大规模安全xgb用于垂直联邦学习。在*信息与知识管理会议（CIKM’21）*，第
    443–452 页，2021。
- en: 'Gao et al. (2023) Dawei Gao, Daoyuan Chen, Zitao Li, Yuexiang Xie, Xuchen Pan,
    Yaliang Li, Bolin Ding, and Jingren Zhou. FS-Real: A Real-World Cross-Device Federated
    Learning Platform. *PVLDB*, 16(12):4046–4049, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2023) 高大伟、陈道源、李自涛、谢跃翔、潘旭晨、李亚亮、丁博霖和周静任。FS-Real: 一个真实世界的跨设备联邦学习平台。*PVLDB*，16(12):4046–4049，2023。'
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andrew
    Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI With Shared Human
    Values. In *Proc. of the International Conference on Learning Representations
    (ICLR’21)*, 2021a.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andrew
    Critch, Jerry Li, Dawn Song, 和 Jacob Steinhardt。将 AI 与共同的人类价值观对齐。载于*国际学习表征大会 (ICLR’21)
    论文集*，2021a 年。
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language
    Understanding. In *Proc. of the International Conference on Learning Representations
    (ICLR’21)*, 2021b.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt。测量大规模多任务语言理解。载于*国际学习表征大会 (ICLR’21)
    论文集*，2021b 年。
- en: Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching Machines
    to Read and Comprehend. In *Proc. of the Advances in Neural Information Processing
    Systems (NeurIPS’15)*, volume 28, pp.  1693–1701, 2015.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, 和 Phil Blunsom。教机器阅读和理解。载于*神经信息处理系统进展
    (NeurIPS’15) 论文集*，第 28 卷，第 1693–1701 页，2015 年。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-Efficient Transfer Learning for NLP. In *Proc. of the International
    Conference on Machine Learning (ICML’21)*, pp.  2790–2799, 2019.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain Gelly。参数高效的迁移学习在
    NLP 中的应用。载于*国际机器学习大会 (ICML’21) 论文集*，第 2790–2799 页，2019 年。
- en: 'Hu et al. (2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language
    Models. In *Proc. of the International Conference on Learning Representations
    (ICLR’22)*, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen 等。LoRA：大语言模型的低秩适配。载于*国际学习表征大会 (ICLR’22) 论文集*，2022
    年。
- en: 'Huang et al. (2023) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham
    Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.
    Language Is Not All You Need: Aligning Perception with Language Models. *arXiv
    preprint arXiv:2302.14045*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal,
    Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu 等。语言不是你所需的一切：将感知与语言模型对齐。*arXiv
    预印本 arXiv:2302.14045*，2023 年。
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. CodeSearchNet Challenge: Evaluating the State of Semantic
    Code Search. *arXiv preprint arXiv:1909.09436*, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    和 Marc Brockschmidt。CodeSearchNet 挑战：评估语义代码搜索的现状。*arXiv 预印本 arXiv:1909.09436*，2019
    年。
- en: 'Ippolito et al. (2022) Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam.
    Creative Writing with an AI-Powered Writing Assistant: Perspectives from Professional
    Writers. *arXiv preprint arXiv:2211.05030*, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ippolito et al. (2022) Daphne Ippolito, Ann Yuan, Andy Coenen, 和 Sehmon Burnam。使用
    AI 驱动的写作助手进行创意写作：专业作家的观点。*arXiv 预印本 arXiv:2211.05030*，2022 年。
- en: Jamieson & Talwalkar (2016) Kevin Jamieson and Ameet Talwalkar. Non-stochastic
    Best Arm Identification and Hyperparameter Optimization. In *Proc. of the Artificial
    intelligence and statistics (AISTATS’16)*, pp.  240–248, 2016.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jamieson & Talwalkar (2016) Kevin Jamieson 和 Ameet Talwalkar。非随机最佳臂识别和超参数优化。载于*人工智能与统计
    (AISTATS’16) 论文集*，第 240–248 页，2016 年。
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination
    in Natural Language Generation. *ACM Computing Surveys*, 55:1–38, 2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, 和 Pascale Fung。自然语言生成中的幻觉调查。*ACM
    计算机调查*，55:1–38，2023 年。
- en: 'Karimi Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and
    Sebastian Ruder. Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. In
    *Proc. of the Advances in Neural Information Processing Systems (NeurIPS’21)*,
    volume 34, pp.  1022–1035, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimi Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, 和 Sebastian
    Ruder。Compacter：高效的低秩超复合适配器层。载于*神经信息处理系统进展 (NeurIPS’21) 论文集*，第 34 卷，第 1022–1035
    页，2021 年。
- en: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA Reading
    Comprehension Challenge. *Transactions of the Association for Computational Linguistics*,
    6:317–328, 2018.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, 和 Edward Grefenstette. NarrativeQA 阅读理解挑战。*计算语言学协会会刊*，6:317–328，2018年。
- en: 'Konečný et al. (2016) Jakub Konečný, H. Brendan McMahan, Daniel Ramage, and
    Peter Richtárik. Federated Optimization: Distributed Machine Learning for On-Device
    Intelligence. *arXiv preprint arXiv:1610.02527*, 2016.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Konečný et al. (2016) Jakub Konečný, H. Brendan McMahan, Daniel Ramage, 和 Peter
    Richtárik. 联邦优化: 设备智能的分布式机器学习。*arXiv 预印本 arXiv:1610.02527*，2016年。'
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. Natural Questions: A Benchmark for Question Answering
    Research. *Transactions of the Association for Computational Linguistics*, 7:453–466,
    2019.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee 等. Natural Questions: 问答研究基准。*计算语言学协会会刊*，7:453–466，2019年。'
- en: 'Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng
    Zhu, Harsha Madhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking Model and
    System Performance of Federated Learning at Scale. In *Proc. of the International
    Conference on Machine Learning (ICML’22)*, volume 162, pp.  11814–11827, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng
    Zhu, Harsha Madhyastha, 和 Mosharaf Chowdhury. FedScale: 大规模联邦学习模型和系统性能基准测试。在 *国际机器学习大会
    (ICML’22) 论文集*，第162卷，第11814–11827页，2022年。'
- en: 'Lee et al. (2022) Mina Lee, Percy Liang, and Qian Yang. CoAuthor: Designing
    a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities.
    In *Proc. of the CHI Conference on Human Factors in Computing Systems (CHI’22)*,
    2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2022) Mina Lee, Percy Liang, 和 Qian Yang. CoAuthor: 设计一个人类与人工智能协作写作的数据集，以探索语言模型的能力。在
    *CHI 人机交互会议 (CHI’22) 论文集*，2022年。'
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The Power
    of Scale for Parameter-Efficient Prompt Tuning. In *Proc. of the Conference on
    Empirical Methods in Natural Language Processing (EMNLP’21)*, pp.  3045–3059,
    2021.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, 和 Noah Constant. 参数高效的提示调优的规模效应。
    在 *自然语言处理实证方法大会 (EMNLP’21) 论文集*，第3045–3059页，2021年。
- en: 'Li et al. (2021) Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.
    Ditto: Fair and Robust Federated Learning Through Personalization. In *Proc. of
    the International Conference on Machine Learning (ICML’21)*, volume 139, pp. 
    6357–6368, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2021) Tian Li, Shengyuan Hu, Ahmad Beirami, 和 Virginia Smith. Ditto:
    通过个性化实现公平且稳健的联邦学习。在 *国际机器学习大会 (ICML’21) 论文集*，第139卷，第6357–6368页，2021年。'
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing
    Continuous Prompts for Generation. In *Proc. of the Annual Meeting of the Association
    for Computational Linguistics and the International Joint Conference on Natural
    Language Processing (ACL-IJCNLP’21)*, pp.  4582–4597, 2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li & Liang (2021) Xiang Lisa Li 和 Percy Liang. Prefix-Tuning: 优化连续提示以生成内容。
    在 *计算语言学协会年会及国际自然语言处理联合会议 (ACL-IJCNLP’21) 论文集*，第4582–4597页，2021年。'
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. Holistic Evaluation of Language Models. *arXiv preprint arXiv:2211.09110*,
    2022.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar 等. 语言模型的整体评估。*arXiv 预印本 arXiv:2211.09110*，2022年。
- en: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA:
    Measuring How Models Mimic Human Falsehoods. *arXiv preprint arXiv:2109.07958*,
    2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2021) Stephanie Lin, Jacob Hilton, 和 Owain Evans. TruthfulQA: 测量模型如何模仿人类虚假信息。*arXiv
    预印本 arXiv:2109.07958*，2021年。'
- en: 'Liu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    and Jie Tang. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally
    Across Scales and Tasks. *arXiv preprint arXiv:2110.07602*, 2021a.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    和 Jie Tang. P-Tuning v2: 提示调优在不同规模和任务中的普遍性与微调相当。*arXiv 预印本 arXiv:2110.07602*，2021年。'
- en: Liu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. GPT Understands, Too. *arXiv preprint arXiv:2103.10385*,
    2021b.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, 和 Jie Tang. GPT 也能理解. *arXiv 预印本 arXiv:2103.10385*，2021b 年。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
    A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. RoBERTa：一种强健的优化
    BERT 预训练方法. *arXiv 预印本 arXiv:1907.11692*，2019 年。
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. Learning Word Vectors for Sentiment Analysis.
    In *Proc. of the Annual Meeting of the Association for Computational Linguistics:
    Human Language Technologies (AMACL’11)*, pp. 142–150, 2011.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, 和 Christopher Potts. 用于情感分析的词向量学习. 见 *计算语言学协会年会：人类语言技术（AMACL’11）*，第
    142–150 页，2011 年。
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, and Sayak Paul. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning
    methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, 和 Sayak Paul. PEFT：最先进的参数高效微调方法. [https://github.com/huggingface/peft](https://github.com/huggingface/peft)，2022
    年。
- en: McMahan et al. (2017) H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Agüera y Arcas. Communication-Efficient Learning of Deep Networks from
    Decentralized Data. In *Proc. of the Artificial intelligence and statistics (AISTATS’17)*,
    pp.  1273–1282, 2017.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan et al. (2017) H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    和 Blaise Agüera y Arcas. 从去中心化数据中高效学习深度网络. 见 *人工智能与统计会议（AISTATS’17）*，第 1273–1282
    页，2017 年。
- en: Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh, et al. Mixed Precision Training. In *Proc. of the
    International Conference on Learning Representations (ICLR’18)*, 2018.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh 等. 混合精度训练. 见 *学习表示国际会议（ICLR’18）*，2018 年。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book
    Question Answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    盔甲能导电吗？一个新的开放书籍问答数据集. *arXiv 预印本 arXiv:1809.02789*，2018 年。
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t
    Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks
    for Extreme Summarization. In *Proc. of the Conference on Empirical Methods in
    Natural Language Processing (EMNLP’18)*, pp.  1797–1807, 2018.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, 和 Mirella Lapata. 别告诉我细节，只要总结！针对极端摘要的主题感知卷积神经网络.
    见 *自然语言处理经验方法会议（EMNLP’18）*，第 1797–1807 页，2018 年。
- en: OpenAI (2022) OpenAI. Introducing ChatGPT, 2022.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 介绍 ChatGPT，2022 年。
- en: OpenAI (2023) OpenAI. GPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. GPT-4 技术报告. *arXiv 预印本 arXiv:2303.08774*，2023 年。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library.
    In *Proc. of the Advances in Neural Information Processing Systems (NeurIPS’19)*,
    pp.  8024–8035, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga 等. PyTorch：一种命令式风格的高性能深度学习库. 见 *神经信息处理系统进展（NeurIPS’19）*，第 8024–8035 页，2019
    年。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. Gorilla: Large Language Model Connected with Massive APIs. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, 和 Joseph E. Gonzalez.
    Gorilla：与大规模 API 连接的大型语言模型. *arXiv 预印本 arXiv:2305.15334*，2023 年。
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep Contextualized Word
    Representations. In *Proc. of North American Chapter of the Association for Computational
    Linguistics (NAACL-HLT’18)*, pp.  2227–2237, 2018.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等（2018）Matthew E. Peters、Mark Neumann、Mohit Iyyer、Matt Gardner、Christopher
    Clark、Kenton Lee 和 Luke Zettlemoyer。深度上下文化词表示。在 *北美计算语言学协会章节会议（NAACL-HLT’18）*，第2227–2237页，2018年。
- en: 'Pfeiffer et al. (2020a) Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun
    Cho, and Iryna Gurevych. AdapterFusion: Non-Destructive Task Composition for Transfer
    Learning. *arXiv preprint arXiv:2005.00247*, 2020a.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer 等（2020a）Jonas Pfeiffer、Aishwarya Kamath、Andreas Rücklé、Kyunghyun Cho
    和 Iryna Gurevych。AdapterFusion: 非破坏性任务组合用于迁移学习。*arXiv 预印本 arXiv:2005.00247*，2020年。'
- en: 'Pfeiffer et al. (2020b) Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya
    Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. AdapterHub:
    A Framework for Adapting Transformers. In *Proceedings of the Conference on Empirical
    Methods in Natural Language Processing: Systems Demonstrations (EMNLP’20)*, pp. 46–54,
    2020b.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer 等（2020b）Jonas Pfeiffer、Andreas Rücklé、Clifton Poth、Aishwarya Kamath、Ivan
    Vulić、Sebastian Ruder、Kyunghyun Cho 和 Iryna Gurevych。AdapterHub: 一个用于适应变换器的框架。在
    *自然语言处理经验方法会议论文集: 系统演示（EMNLP’20）*，第46–54页，2020b。'
- en: 'Pfeiffer et al. (2020c) Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian
    Ruder. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer.
    In *Proc. of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP’20)*, pp.  7654–7673, 2020c.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer 等（2020c）Jonas Pfeiffer、Ivan Vulić、Iryna Gurevych 和 Sebastian Ruder。MAD-X:
    一个基于适配器的多任务跨语言迁移框架。在 *自然语言处理经验方法会议论文集（EMNLP’20）*，第7654–7673页，2020c。'
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. ToolLLM: Facilitating
    Large Language Models to Master 16000+ Real-world APIs. *arXiv preprint arXiv:2307.16789*,
    2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等（2023）Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai
    Lin、Xin Cong、Xiangru Tang、Bill Qian 等。ToolLLM: 促进大型语言模型掌握16000+实际应用程序接口。*arXiv
    预印本 arXiv:2307.16789*，2023。'
- en: Radford & Narasimhan (2018) Alec Radford and Karthik Narasimhan. Improving Language
    Understanding by Generative Pre-Training. 2018.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford & Narasimhan（2018）Alec Radford 和 Karthik Narasimhan。通过生成性预训练改善语言理解。2018年。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners.
    2019.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Alec Radford、Jeff Wu、Rewon Child、David Luan、Dario Amodei 和 Ilya
    Sutskever。语言模型是无监督多任务学习者。2019年。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. DeepSpeed: System Optimizations Enable Training Deep Learning Models
    with Over 100 Billion Parameters. In *Proc. of the ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining (KDD’20)*, pp.  3505–3506, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley 等（2020）Jeff Rasley、Samyam Rajbhandari、Olatunji Ruwase 和 Yuxiong He。DeepSpeed:
    系统优化使训练超过1000亿参数的深度学习模型成为可能。在 *ACM SIGKDD 知识发现与数据挖掘国际会议论文集（KDD’20）*，第3505–3506页，2020年。'
- en: 'Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyang Yang, Minjia Zhang, Dong Li, and Yuxiong He. ZeRO-Offload:
    Democratizing Billion-Scale Model Training. In *Proc. of the USENIX Annual Technical
    Conference (USENIX ATC’21)*, pp.  551–564, 2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等（2021）Jie Ren、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyang
    Yang、Minjia Zhang、Dong Li 和 Yuxiong He。ZeRO-Offload: 实现亿级规模模型训练的民主化。在 *USENIX
    年度技术会议论文集（USENIX ATC’21）*，第551–564页，2021年。'
- en: Ruebsamen (2023) Gene Ruebsamen. Cleaned Alpaca Dataset. [https://github.com/gururise/AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned),
    2023.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruebsamen（2023）Gene Ruebsamen。清理过的Alpaca数据集。 [https://github.com/gururise/AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned)，2023年。
- en: Ryffel et al. (2018) Théo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason V.
    Mancuso, Daniel Rueckert, and Jonathan Passerat-Palmbach. A generic framework
    for privacy preserving deep learning. *arXiv preprint arXiv:1811.04017*, 2018.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ryffel 等（2018）Théo Ryffel、Andrew Trask、Morten Dahl、Bobby Wagner、Jason V. Mancuso、Daniel
    Rueckert 和 Jonathan Passerat-Palmbach。一个用于隐私保护深度学习的通用框架。*arXiv 预印本 arXiv:1811.04017*，2018年。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, et al. BLOOM:
    A 176B-Parameter Open-Access Multilingual Language Model. *arXiv preprint arXiv:2211.05100*,
    2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao 等（2022）Teven Le Scao、Angela Fan、Christopher Akiki 等。BLOOM: 一个176B参数的开放访问多语言模型。*arXiv
    预印本 arXiv:2211.05100*，2022年。'
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    To The Point: Summarization with Pointer-Generator Networks. In *Proc. of the
    Annual Meeting of the Association for Computational Linguistics (AMACL’17)*, pp. 
    1073–1083, 2017.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'See et al. (2017) Abigail See, Peter J. Liu, 和 Christopher D. Manning. Get
    To The Point: 使用 Pointer-Generator Networks 的摘要方法。见 *计算语言学协会年会论文集 (AMACL’17)*，第
    1073–1083 页，2017年。'
- en: 'Shahriari et al. (2015) Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams,
    and Nando De Freitas. Taking the Human Out of the Loop: A Review of Bayesian Optimization.
    *Proceedings of the IEEE*, 104(1):148–175, 2015.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahriari et al. (2015) Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams,
    和 Nando De Freitas. 将人为因素排除在外：贝叶斯优化综述。*IEEE 会议录*，104(1):148–175，2015年。
- en: T Dinh et al. (2020) Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized
    Federated Learning with Moreau Envelopes. In *Proc. of the Advances in Neural
    Information Processing Systems (NeurIPS’20)*, volume 33, pp.  21394–21405, 2020.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T Dinh et al. (2020) Canh T Dinh, Nguyen Tran, 和 Josh Nguyen. 具有 Moreau 包络的个性化联邦学习。见
    *神经信息处理系统进展 (NeurIPS’20)*，第 33 卷，第 21394–21405 页，2020年。
- en: Tan et al. (2022) Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards
    Personalized Federated Learning. *IEEE Transactions on Neural Networks and Learning
    Systems*, pp.  1–17, 2022.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2022) Alysa Ziying Tan, Han Yu, Lizhen Cui, 和 Qiang Yang. 迈向个性化联邦学习。*IEEE
    神经网络与学习系统期刊*，第 1–17 页，2022年。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    Alpaca: An Instruction-following LLaMA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. Stanford Alpaca:
    一种遵循指令的 LLaMA 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language
    Models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. LLaMA: 开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Wang et al. (2022) Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang
    Li, Bolin Ding, and Jingren Zhou. FederatedScope-GNN: Towards a Unified, Comprehensive
    and Efficient Package for Federated Graph Learning. In *Proc. of the ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining (KDD’22)*, 2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang
    Li, Bolin Ding, 和 Jingren Zhou. FederatedScope-GNN: 迈向统一、全面且高效的联邦图学习工具包。见 *ACM
    SIGKDD 知识发现与数据挖掘国际会议论文集 (KDD’22)*，2022年。'
- en: 'Wang et al. (2023) Zhen Wang, Weirui Kuang, Ce Zhang, Bolin Ding, and Yaliang
    Li. FedHPO-Bench: A Benchmark Suite for Federated Hyperparameter Optimization.
    In *Proc. of the International Conference on Machine Learning (ICML’23)*, pp. 
    35908–35948, 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Zhen Wang, Weirui Kuang, Ce Zhang, Bolin Ding, 和 Yaliang
    Li. FedHPO-Bench: 联邦超参数优化基准套件。见 *国际机器学习大会论文集 (ICML’23)*，第 35908–35948 页，2023年。'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-Thought Prompting Elicits
    Reasoning in Large Language Models. In *Proc. of the Advances in Neural Information
    Processing Systems (NeurIPS’22)*, volume 35, pp.  24824–24837, 2022.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. Chain-of-Thought 提示引发大型语言模型的推理。见 *神经信息处理系统进展
    (NeurIPS’22)*，第 35 卷，第 24824–24837 页，2022年。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing.
    In *Proc. of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP’20)*, pp.  38–45, 2020.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander M. Rush. Transformers: 最先进的自然语言处理。见 *自然语言处理经验方法会议论文集 (EMNLP’20)*，第
    38–45 页，2020年。'
- en: 'Wu et al. (2023) Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
    Models. *arXiv preprint arXiv:2303.04671*, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) 陈飞·吴、盛凯·尹、维震·齐、晓东·王、泽成·唐和南端。《Visual ChatGPT：与视觉基础模型进行对话、绘图和编辑》。*arXiv
    预印本 arXiv:2303.04671*，2023年。
- en: Wu et al. (2020) Yuncheng Wu, Shaofeng Cai, Xiaokui Xiao, Gang Chen, and Beng Chin
    Ooi. Privacy preserving vertical federated learning for tree-based models. *PVLDB*,
    13:2090–2103, 2020.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) 云程·吴、绍峰·蔡、晓奎·肖、刚·陈和本·秦·吴。《面向树模型的隐私保护垂直联邦学习》。*PVLDB*，13：2090–2103，2020年。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, and Song Han. Offsite-Tuning: Transfer
    Learning without Full Model. *arXiv preprint arXiv:2302.04870*, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) 光轩·肖、吉·林和宋·韩。《离线微调：无需完整模型的迁移学习》。*arXiv 预印本 arXiv:2302.04870*，2023年。
- en: 'Xie et al. (2023) Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. FederatedScope: A Flexible
    Federated Learning Platform for Heterogeneity. *PVLDB*, 16(5):1059–1072, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) 叶翔·谢、臻·王、道远·陈、大伟·高、刘怡·姚、伟瑞·邝、雅亮·李、博林·丁和静仁·周。《FederatedScope：一个灵活的联邦学习平台以应对异质性》。*PVLDB*，16(5)：1059–1072，2023年。
- en: Yang et al. (2019) Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen,
    and Han Yu. Federated Learning. *Synthesis Lectures on Artificial Intelligence
    and Machine Learning*, 13:1–207, 2019.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) 强·杨、杨·刘、永·程、彦·康、天健·陈和韩·于。《联邦学习》。*人工智能与机器学习合成讲座*，13：1–207，2019年。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. HellaSwag: Can a Machine Really Finish Your Sentence? In *Proc. of
    the Annual Meeting of the Association for Computational Linguistics (AMACL’19)*,
    2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) 罗温·泽勒斯、阿里·霍尔茨曼、乔纳坦·比斯克、阿里·法尔哈迪和叶金·崔。《HellaSwag：机器真的能完成你的句子吗？》在*计算语言学协会年会
    (AMACL’19) 会议论文集*中，2019年。
- en: 'Zeng et al. (2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An Open
    Bilingual Pre-trained Model. In *Proc. of the International Conference on Learning
    Representations (ICLR’23)*, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. (2023) 昂汉·曾、肖·刘、郑晓·杜、子涵·王、寒宇·赖、明·丁、卓宜·杨、亦凡·徐、文迪·郑、肖·夏等人。《GLM-130B：一个开放的双语预训练模型》。在*国际学习表示会议
    (ICLR’23) 会议论文集*中，2023年。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. OPT: Open Pre-trained Transformer Language Models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) 苏珊·张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿特特克斯、莫亚·陈、朔辉·陈、克里斯托弗·德万、莫娜·迪亚布、谢安·李、维多利亚·林等人。《OPT：开放预训练变换器语言模型》。*arXiv
    预印本 arXiv:2205.01068*，2022年。
- en: 'Zhang et al. (2019) Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang,
    Shiyi Zhou, Jiaming Guo, Yu Kang, Qi Guo, Zidong Du, and Yunji Chen. Adaptive
    Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point
    Numbers. *arXiv preprint arXiv:1911.00361*, 2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019) 西山·张、绍丽·刘、瑞·张、常·刘、迪·黄、诗怡·周、嘉铭·郭、玉·康、启·郭、子东·杜和云集·陈。《自适应精度训练：用定点数量化神经网络中的反向传播》。*arXiv
    预印本 arXiv:1911.00361*，2019年。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A Survey of Large Language Models. *arXiv preprint arXiv:2303.18223*, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) 韦恩·辛·赵、昆·周、俊义·李、天意·唐、晓磊·王、玉鹏·侯、颖倩·敏、北辰·张、俊杰·张、子灿·董等人。《大型语言模型综述》。*arXiv
    预印本 arXiv:2303.18223*，2023年。
- en: 'Zhou et al. (2021) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo
    Angel, Horst Samulowitz, and Heiko Ludwig. FLoRA: Single-shot Hyper-parameter
    Optimization for Federated Learning. In *Proc. of the Advances in Neural Information
    Processing Systems (NeurIPS’21)*, 2021.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2021) 易·周、帕里克希特·拉姆、西奥多罗斯·萨隆迪斯、娜塔莉·巴拉卡尔多·安赫尔、霍斯特·萨穆洛维茨和海科·路德维希。《FLoRA：联邦学习的单次超参数优化》。在*神经信息处理系统进展
    (NeurIPS’21) 会议论文集*中，2021年。
- en: Appendix A Appendix
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Fine-tuning Dataset description
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 微调数据集描述
- en: 'In this section, we describe the fine-tuning datasets curated in FS-LLM, and
    summarize the statistics and information in Table [6](#A1.T6 "Table 6 ‣ A.1 Fine-tuning
    Dataset description ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"). These datasets
    are derived from existing and widely used fine-tuning datasets that cover diverse
    domains, such as code, natural language, dialogues, and math problems. The curated
    fine-tuning datasets exhibit different degrees of heterogeneity across clients,
    which pose various challenges and opportunities for fine-tuning LLMs in FL. We
    describe the construction and characteristics of each dataset in detail below
    and illustrate their scenarios. We will constantly adopt new datasets for fine-tuning
    LLMs in FS-LLM.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了 FS-LLM 中策划的微调数据集，并在表 [6](#A1.T6 "表 6 ‣ A.1 微调数据集描述 ‣ 附录 A 附录 ‣ FederatedScope-LLM：用于联邦学习中大规模语言模型微调的综合包")
    中总结了统计信息和信息。这些数据集来源于现有的广泛使用的微调数据集，涵盖了代码、自然语言、对话和数学问题等多个领域。策划的微调数据集在客户端之间表现出不同程度的异质性，这为在
    FL 中微调 LLM 带来了各种挑战和机会。我们将在下文中详细描述每个数据集的构建和特征，并阐明它们的场景。我们将不断采用新的数据集来微调 FS-LLM 中的
    LLM。
- en: 'Table 6: Statistics and information of the fine-tuning datasets.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：微调数据集的统计信息和信息。
- en: '| Name | #Client | #Sample | Split | Domain | Evaluation Dataset |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | #客户端 | #样本 | 分割 | 领域 | 评估数据集 |'
- en: '| Fed-CodeAlpaca | 9 | $\sim$8.0k | Meta | Code Generation | HumanEval |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Fed-CodeAlpaca | 9 | $\sim$8.0k | Meta | 代码生成 | HumanEval |'
- en: '| Fed-Dolly | 8 | $\sim$15.0k | Meta | Generic Language | HELM |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Fed-Dolly | 8 | $\sim$15.0k | Meta | 通用语言 | HELM |'
- en: '| Fed-GSM8K-3 | 3 | $\sim$7.5k | IID | CoT | GSM8K-test |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Fed-GSM8K-3 | 3 | $\sim$7.5k | IID | CoT | GSM8K-test |'
- en: '| Fed-CodeSearchNet | 6 | $\sim$1880.8k | Meta | Code Generation | HumanEval
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Fed-CodeSearchNet | 6 | $\sim$1880.8k | Meta | 代码生成 | HumanEval |'
- en: '| Alpaca | - | $\sim$52.0k | - | Generic Language | HELM |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Alpaca | - | $\sim$52.0k | - | 通用语言 | HELM |'
- en: '| CleanedAlpaca | - | $\sim$51.8k | - | Generic Language | HELM |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| CleanedAlpaca | - | $\sim$51.8k | - | 通用语言 | HELM |'
- en: A.1.1 Federated fine-tuning dataset
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 联邦微调数据集
- en: The federated fine-tuning datasets are a collection of curated datasets that
    we adopt based on the meta-information or some distribution of the original corpora.
    Users can directly use them for federated fine-tuning LLMs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦微调数据集是我们基于原始语料的元信息或某些分布所策划的 curated 数据集集合。用户可以直接使用这些数据集来进行联邦微调 LLM。
- en: Fed-CodeAlpaca is a federated version of CodeAlpaca (Chaudhary, [2023](#bib.bib10)),
    a code dataset that contains ten programming languages, including C, C#, C++,
    Go, Java, PHP, Pascal, Python, Scale, and X86-64 Assemble. We exclude the X86-64
    Assembly samples, as they are very scarce in the original corpora. Then, we split
    the remaining samples into nine subsets based on the language category and assign
    each subset to one client.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Fed-CodeAlpaca 是 CodeAlpaca（Chaudhary，[2023](#bib.bib10)）的联邦版本，该代码数据集包含十种编程语言，包括
    C、C#、C++、Go、Java、PHP、Pascal、Python、Scale 和 X86-64 汇编。由于 X86-64 汇编样本在原始语料中非常稀少，我们将其排除。然后，我们将剩余的样本根据语言类别分成九个子集，并将每个子集分配给一个客户端。
- en: 'Fed-Dolly is a federated corpus dataset derived from Databricks-dolly-15k (Conover
    et al., [2023](#bib.bib20)), which comprises eight categories of NLP tasks: brainstorming,
    classification, closed QA, creative writing, general QA, information extraction,
    open QA, and summarization. The corpora within each client only belong to one
    category.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Fed-Dolly 是一个来自 Databricks-dolly-15k（Conover 等，[2023](#bib.bib20)）的联邦语料库数据集，包含八类
    NLP 任务：头脑风暴、分类、封闭式 QA、创意写作、通用 QA、信息提取、开放式 QA 和总结。每个客户端内的语料仅属于一个类别。
- en: Fed-GSM8K-3 is built from GSM8K (Cobbe et al., [2021](#bib.bib18)), which is
    a mathematical fine-tuning dataset consisting of 7.5K training problems and 1K
    test problems. We split the training problems into three subsets by the uniform
    splitter, and assign each subset to one client.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Fed-GSM8K-3 来源于 GSM8K（Cobbe 等，[2021](#bib.bib18)），这是一个数学微调数据集，包括 7.5K 训练问题和
    1K 测试问题。我们通过均匀分割器将训练问题分成三个子集，并将每个子集分配给一个客户端。
- en: In addition to the three datasets introduced in the main text, we also curate
    another code dataset, Fed-CodeSearchNet, as an alternative to Fed-CodeAlpaca.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正文中介绍的三个数据集外，我们还策划了另一个代码数据集，Fed-CodeSearchNet，作为 Fed-CodeAlpaca 的替代品。
- en: 'Fed-CodeSearchNet is built from CodeSearchNet (Husain et al., [2019](#bib.bib35)),
    which is a large-scale code dataset of functions and their associated documentation
    for six programming languages: Go, Java, JavaScript, PHP, Python, and Ruby. The
    data are extracted from open-source projects on GitHub. Similar to Fed-CodeAlpaca,
    we split the samples into six subsets according to the language category and allocate
    each subset to one client.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Fed-CodeSearchNet 是由 CodeSearchNet (Husain et al., [2019](#bib.bib35)) 构建的，后者是一个大规模的代码数据集，包含六种编程语言的函数及其相关文档：Go、Java、JavaScript、PHP、Python
    和 Ruby。数据来自 GitHub 上的开源项目。与 Fed-CodeAlpaca 类似，我们根据语言类别将样本分成六个子集，并将每个子集分配给一个客户端。
- en: A.1.2 Centralized fine-tuning dataset
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 集中化微调数据集
- en: The centralized fine-tuning datasets are a collection of corpora that we have
    collected from the Internet without any prior partitioning. Users can use our
    provided splitters to customize the data partition according to different criteria,
    such as heterogeneity, number balance, root verb, etc.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 集中化微调数据集是我们从互联网上收集的语料库的集合，没有任何事先的分区。用户可以使用我们提供的分割器根据不同的标准（如异质性、数量平衡、根动词等）自定义数据分区。
- en: Alpaca (Taori et al., [2023](#bib.bib79)) is a fine-tuning dataset containing
    natural language questions and responses for various NLP tasks such as text generation,
    translation, and open QA. The dataset covers a wide range of domains, such as
    math, text processing, code generation, etc.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Alpaca (Taori et al., [2023](#bib.bib79)) 是一个包含自然语言问题和回答的微调数据集，用于各种 NLP 任务，如文本生成、翻译和开放式
    QA。该数据集涵盖了广泛的领域，如数学、文本处理、代码生成等。
- en: CleanedAlpaca (Ruebsamen, [2023](#bib.bib72)) is a fine-tuning dataset that
    improves the quality and usefulness of the original Alpaca dataset. It is expected
    to be more reliable and consistent for fine-tuning LLMs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: CleanedAlpaca (Ruebsamen, [2023](#bib.bib72)) 是一个微调数据集，旨在提高原始 Alpaca 数据集的质量和实用性。预计它对微调
    LLMs 会更可靠和一致。
- en: A.2 Evaluation task description
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估任务描述
- en: We believe that fine-tuning LLMs should either improve their generic language
    capabilities or improve their domain-specific capabilities for one particular
    downstream task. Thus, we use three different evaluation tasks for benchmarking
    fine-tuned LLMs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，微调 LLMs 应该要么提高它们的通用语言能力，要么提高它们在特定下游任务上的领域特定能力。因此，我们使用三种不同的评估任务来基准测试微调后的
    LLMs。
- en: Evaluation task for code generation capability. HumanEval (Chen et al., [2021](#bib.bib13))
    is to measure whether the code generated by LLMs is correct or not. It contains
    an evaluation dataset and an accordingly metric Pass@k score for assessing the
    performance of LLMs on code generation capability. Specifically, the model generates
    $m$ as the number of correct samples generated by LLM. Then
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成能力的评估任务。HumanEval (Chen et al., [2021](#bib.bib13)) 用于衡量 LLMs 生成的代码是否正确。它包含一个评估数据集以及一个相应的度量指标
    Pass@k 分数，用于评估 LLMs 在代码生成能力上的表现。具体来说，模型生成 $m$ 作为 LLM 生成的正确样本数量。然后
- en: '|  | $$\text{Pass@}\mathrm{k}:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\left(\begin{array}[]{c}m-c\\
    k\end{array}\right)}{\left(\begin{array}[]{c}m\\'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\text{Pass@}\mathrm{k}:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\left(\begin{array}[]{c}m-c\\
    k\end{array}\right)}{\left(\begin{array}[]{c}m\\'
- en: k\end{array}\right)}\right]\text{.}$$ |  |
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: k\end{array}\right)}\right]\text{.}$$ |  |
- en: In practice, we set $m=5$ and use the Pass@1 score as the evaluation score,
    i.e.,
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们设置 $m=5$ 并使用 Pass@1 分数作为评估分数，即，
- en: '|  | $\text{Pass@1}:=\mathbb{E}_{\text{Problems}}\left[\frac{c}{5}\right]\text{.}$
    |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Pass@1}:=\mathbb{E}_{\text{Problems}}\left[\frac{c}{5}\right]\text{.}$
    |  |'
- en: Evaluation task for generic language capability. We adapt HELM (Liang et al.,
    [2022](#bib.bib48)), including 16 subtasks, in FS-LLM to evaluate the generic
    language capability of fine-tuned LLMs. To be more precise, the subtasks we use
    are MMLU (Hendrycks et al., [2021b](#bib.bib30); [a](#bib.bib29)), BoolQ (Clark
    et al., [2019](#bib.bib17)), NarrativeQA (Kočiskỳ et al., [2018](#bib.bib40)),
    NaturalQuestions (closed-book) (Kwiatkowski et al., [2019](#bib.bib42)) , NaturalQuestions
    (open-book) (Kwiatkowski et al., [2019](#bib.bib42)) , QuAC (Choi et al., [2018](#bib.bib15)),
    HellaSwag (Zellers et al., [2019](#bib.bib90)), OpenbookQA (Mihaylov et al., [2018](#bib.bib57)),
    TruthfulQA (Lin et al., [2021](#bib.bib49)), MS MARCO (regular) (Bajaj et al.,
    [2016](#bib.bib3)), MS MARCO (TREC) (Bajaj et al., [2016](#bib.bib3)), CNN/DailyMail (See
    et al., [2017](#bib.bib75); Hermann et al., [2015](#bib.bib31)), XSUM (Narayan
    et al., [2018](#bib.bib58)), IMDB (Maas et al., [2011](#bib.bib53)), CivilComments (Borkan
    et al., [2019](#bib.bib7)), and RAFT (Alex et al., [2021](#bib.bib1)). For each
    task, we randomly use 100 samples for evaluation. The evaluation score for HELM
    is a mixture of metric scores on these 16 subtasks. Specifically, in HELM, MMLU,
    BoolQ, HellaSwag, OpenbookQA, TruthfulQA, IMDB, CivilComments, and RAFT use accuracy;
    NarrativeQA, NaturalQuestions (closed-book), NaturalQuestions (closed-book), and
    QuAC use F1 score; CNN/DailyMail and XSUM use ROUGE-2 score; MS MARCO (regular)
    and MS MARCO use RR@10 score and NDCG@10 score, respectively. For more details,
    please refer to Liang et al. ([2022](#bib.bib48)). In FS-LLM, the evaluation score
    for HELM is the average value of these 16 subtasks’ scores.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 通用语言能力的评估任务。我们在FS-LLM中适配了包括16个子任务的HELM（Liang et al., [2022](#bib.bib48)），以评估微调LLMs的通用语言能力。更具体地说，我们使用的子任务包括MMLU（Hendrycks
    et al., [2021b](#bib.bib30); [a](#bib.bib29)）、BoolQ（Clark et al., [2019](#bib.bib17)）、NarrativeQA（Kočiskỳ
    et al., [2018](#bib.bib40)）、NaturalQuestions（闭卷）（Kwiatkowski et al., [2019](#bib.bib42)）、NaturalQuestions（开卷）（Kwiatkowski
    et al., [2019](#bib.bib42)）、QuAC（Choi et al., [2018](#bib.bib15)）、HellaSwag（Zellers
    et al., [2019](#bib.bib90)）、OpenbookQA（Mihaylov et al., [2018](#bib.bib57)）、TruthfulQA（Lin
    et al., [2021](#bib.bib49)）、MS MARCO（常规）（Bajaj et al., [2016](#bib.bib3)）、MS MARCO（TREC）（Bajaj
    et al., [2016](#bib.bib3)）、CNN/DailyMail（See et al., [2017](#bib.bib75); Hermann
    et al., [2015](#bib.bib31)）、XSUM（Narayan et al., [2018](#bib.bib58)）、IMDB（Maas
    et al., [2011](#bib.bib53)）、CivilComments（Borkan et al., [2019](#bib.bib7)）和RAFT（Alex
    et al., [2021](#bib.bib1)）。对于每个任务，我们随机使用100个样本进行评估。HELM的评估分数是这16个子任务的度量分数的混合。具体来说，在HELM中，MMLU、BoolQ、HellaSwag、OpenbookQA、TruthfulQA、IMDB、CivilComments和RAFT使用准确率；NarrativeQA、NaturalQuestions（闭卷）、NaturalQuestions（开卷）和QuAC使用F1分数；CNN/DailyMail和XSUM使用ROUGE-2分数；MS
    MARCO（常规）和MS MARCO分别使用RR@10分数和NDCG@10分数。更多细节，请参见Liang et al.（[2022](#bib.bib48)）。在FS-LLM中，HELM的评估分数是这16个子任务分数的平均值。
- en: However, evaluation of all these 16 subtasks in HELM is very time-consuming.
    We notice that there is a trade-off between the evaluation’s comprehensiveness
    and efficiency. The more subtasks we use, the more accurate the evaluation scores
    will be, but also more time-consuming. Thus, we build HELM-MINI with fewer subtasks
    to assess the LLMs’ generic language capabilities. We first randomly and uniformly
    sample serval samples of different combinations of configurations, including different
    PEFT algorithms and different hyperparameters. After that, we pick 6 subtasks
    such that the evaluation score of these 6 subtasks is closest to the evaluation
    score of all 16 subtasks in the $L^{2}$ and $x_{\min}$ are the maximum and minimum
    evaluation scores among all the samples of HELM and HELM-MINI, respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对HELM中的这16个子任务进行评估非常耗时。我们注意到评估的全面性与效率之间存在权衡。使用的子任务越多，评估分数越准确，但也越耗时。因此，我们构建了子任务较少的HELM-MINI，以评估LLMs的通用语言能力。我们首先随机均匀地抽取几组不同配置组合的样本，包括不同的PEFT算法和不同的超参数。之后，我们选择6个子任务，使这6个子任务的评估分数与所有16个子任务的评估分数最接近，其中$L^{2}$和$x_{\min}$分别是HELM和HELM-MINI中所有样本的最大和最小评估分数。
- en: '![Refer to caption](img/64280ca590e63882ba6857092b4cec71.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64280ca590e63882ba6857092b4cec71.png)'
- en: (a) The average scores of the 16 subtasks in HELM and the 4 subtasks in HELM-MINI
    for the samples with different configurations, respectively.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同配置样本中，HELM的16个子任务和HELM-MINI的4个子任务的平均分数。
- en: '![Refer to caption](img/7c196a222e336bac065382ba7ff40964.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c196a222e336bac065382ba7ff40964.png)'
- en: (b) The density distribution of the normalized performance of HELM and HELM-MINI
    among the samples, respectively.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 样本中HELM和HELM-MINI的归一化性能的密度分布。
- en: 'Figure 6: The performance comparison of HELM and HELM-MINI.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：HELM和HELM-MINI的性能比较。
- en: Evaluation task for CoT capability. We use GSM8K-test (Cobbe et al., [2021](#bib.bib18)),
    which consists of 1k math problems from the test split in GSM8K to evaluate the
    performance of LLMs on mathematic problem-solving capability. We also adopt 8-shot-CoT
    prompting (Wei et al., [2022](#bib.bib83)) in evaluation. The evaluation score
    is the accuracy of these 1k math problems.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 能力的评估任务。我们使用 GSM8K-test（Cobbe 等，[2021](#bib.bib18)），它包含来自 GSM8K 测试集的 1k
    数学问题，用于评估 LLMs 在数学问题解决能力上的表现。我们还在评估中采用了 8-shot-CoT 提示（Wei 等，[2022](#bib.bib83)）。评估得分是这
    1k 数学问题的准确率。
- en: A.3 Detailed hyperparameters and results
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 详细的超参数和结果
- en: We perform a grid search over the hyperparameter space to ensure that each algorithm
    achieves its optimal performance and to enable a fair comparison among different
    algorithms. The search space of the learning rate for all adapters is $\{$1\text{\times}{10}^{-4}$,$3\text{\times}{10}^{-4}$,$5\text{\times}{10}^{-4}$,$1\text{\times}{10}^{-3}$,$3\text{\times}{10}^{-3}$,$5\text{\times}{10}^{-3}$\}$.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对超参数空间进行网格搜索，以确保每种算法都能达到其最佳性能，并使不同算法之间的比较公平。所有适配器的学习率搜索空间为 $\{$1\text{\times}{10}^{-4}$,$3\text{\times}{10}^{-4}$,$5\text{\times}{10}^{-4}$,$1\text{\times}{10}^{-3}$,$3\text{\times}{10}^{-3}$,$5\text{\times}{10}^{-3}$\}$。
- en: 'LoRA has three hyperparameters: the rank of the adaption matrices, the scaling
    coefficient, and the dropout probability for LoRA layers. The rank has a significant
    impact on resource consumption, so we fix it to 8, which is the default value
    in the algorithm. We grid search the other two parameters. The search spaces of
    the scaling coefficient and dropout probability for LoRA layers are $\{16,32,64,128\}$,
    respectively.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 有三个超参数：适配矩阵的秩、缩放系数和 LoRA 层的 dropout 概率。秩对资源消耗有显著影响，因此我们将其固定为 8，这是算法中的默认值。我们对其他两个参数进行网格搜索。缩放系数和
    LoRA 层的 dropout 概率的搜索空间分别为 $\{16,32,64,128\}$。
- en: For P-tuning, because of the source limitation, we fix the type of reparameterization
    to be “MLP” and only search the number of virtual tokens in $\{10,20,30\}$.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 P-tuning，由于源数据的限制，我们将重新参数化的类型固定为“MLP”，并且只在 $\{10,20,30\}$ 中搜索虚拟标记的数量。
- en: For prompt tuning, we find that initializing the prompt randomly will result
    in poor performances on these datasets. Therefore, we set it to “TEXT” and initialize
    different prompts for different fine-tuning datasets. Specifically, for Fed-CodeAlpaca,
    we use “Program a function by following description.” together with the number
    of virtual tokens equals 8; For Fed-Dolly, we use “Assume you can understand and
    answer questions.” together with the number of virtual tokens equals 9; For Fed-GSM8K-3,
    we use “Think step by step.” together with the number of virtual tokens equals
    6. The reason why we use the prompts above is that we think these prompts appropriately
    describe the fine-tuning tasks, respectively. The number of virtual tokens we
    use is exactly the length of the corresponding prompt processed by the tokenizer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提示调整，我们发现随机初始化提示会导致这些数据集上的表现较差。因此，我们将其设置为“TEXT”，并为不同的微调数据集初始化不同的提示。具体而言，对于
    Fed-CodeAlpaca，我们使用“按照描述编写函数。”并且虚拟标记的数量等于 8；对于 Fed-Dolly，我们使用“假设你能理解并回答问题。”并且虚拟标记的数量等于
    9；对于 Fed-GSM8K-3，我们使用“逐步思考。”并且虚拟标记的数量等于 6。我们使用上述提示的原因是我们认为这些提示恰当地描述了微调任务。我们使用的虚拟标记数量正好是分词器处理的相应提示的长度。
- en: 'In Table [7](#A1.T7 "Table 7 ‣ A.3 Detailed hyperparameters and results ‣ Appendix
    A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning")- [15](#A1.T15 "Table 15 ‣ A.3 Detailed
    hyperparameters and results ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning"), we present
    experimental results when we grid search the hyperparameters for different PEFT
    algorithms. In Table [16](#A1.T16 "Table 16 ‣ A.3 Detailed hyperparameters and
    results ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for
    Fine-tuning Large Language Models in Federated Learning"), we show examples of
    creative writing generated by the fine-tuned LLaMA-7B with different PEFT algorithms.
    The fine-tuning dataset is Fed-Dolly. It can be seen that the LLM, without fine-tuning
    in FL, lacks imagination and only describes the situation of a crocodile on the
    moon. The response of the model fine-tuned with LoRA is more accurate, which prefers
    the ocean to outer space and does write a haiku in the voice of a pirate. The
    model fine-tuned with P-tuning combines crocodiles and the moon, but the logic
    of the generated answers is not very coherent. The response generated by the model
    fine-tuned with prompt tuning is the shortest one, showing the savagery of the
    crocodile as a pirate. In summary, after federated fine-tuning, the accuracy and
    fluency of the LLM in answering questions are improved.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [7](#A1.T7 "Table 7 ‣ A.3 Detailed hyperparameters and results ‣ Appendix
    A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large
    Language Models in Federated Learning")- [15](#A1.T15 "Table 15 ‣ A.3 Detailed
    hyperparameters and results ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive
    Package for Fine-tuning Large Language Models in Federated Learning")中，我们展示了在对不同
    PEFT 算法进行网格搜索时的实验结果。在表 [16](#A1.T16 "Table 16 ‣ A.3 Detailed hyperparameters and
    results ‣ Appendix A Appendix ‣ FederatedScope-LLM: A Comprehensive Package for
    Fine-tuning Large Language Models in Federated Learning")中，我们展示了使用不同 PEFT 算法微调的
    LLaMA-7B 生成的创意写作示例。微调数据集为 Fed-Dolly。可以看出，未经联邦微调的 LLM 缺乏想象力，只描述了月球上的鳄鱼的情况。使用 LoRA
    微调的模型反应更准确，更喜欢海洋而非外太空，并且确实用海盗的语气写了一首俳句。使用 P-tuning 微调的模型结合了鳄鱼和月球，但生成的回答逻辑不太连贯。使用
    prompt tuning 微调的模型生成的响应最短，表现了鳄鱼作为海盗的野性。总之，经过联邦微调后，LLM 在回答问题的准确性和流畅性都有所提高。'
- en: 'Table 7: Evaluation scores (%): federated fine-tuning LLaMA-7B with LoRA on
    Fed-CodeAlpaca. In this table, the dropout probability is 0.0.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 评估得分（%）：在 Fed-CodeAlpaca 上使用 LoRA 对 LLaMA-7B 进行联邦微调。在此表中，丢弃概率为 0.0。'
- en: '| Scaling coef. | Seed | LR |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: 缩放系数 | 种子 | 学习率 |
- en: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 16 | 0 | 13.29 | 11.34 | 10.37 | 9.39 | 11.34 | 10.73 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 0 | 13.29 | 11.34 | 10.37 | 9.39 | 11.34 | 10.73 |'
- en: '| 1 | 13.17 | 11.71 | 10.49 | 10.73 | 11.71 | 11.46 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 13.17 | 11.71 | 10.49 | 10.73 | 11.71 | 11.46 |'
- en: '| 2 | 13.41 | 11.22 | 11.10 | 10.85 | 12.68 | 11.83 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 13.41 | 11.22 | 11.10 | 10.85 | 12.68 | 11.83 |'
- en: '| 32 | 0 | 12.56 | 10.24 | 9.27 | 10.61 | 10.24 | 2.07 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 0 | 12.56 | 10.24 | 9.27 | 10.61 | 10.24 | 2.07 |'
- en: '| 1 | 12.07 | 11.22 | 10.98 | 10.49 | 8.54 | 9.88 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 12.07 | 11.22 | 10.98 | 10.49 | 8.54 | 9.88 |'
- en: '| 2 | 12.44 | 11.34 | 11.22 | 11.34 | 10.73 | 10.00 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 12.44 | 11.34 | 11.22 | 11.34 | 10.73 | 10.00 |'
- en: '| 64 | 0 | 10.00 | 9.63 | 11.10 | 11.10 | 10.24 | 11.22 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 0 | 10.00 | 9.63 | 11.10 | 11.10 | 10.24 | 11.22 |'
- en: '| 1 | 10.73 | 10.37 | 11.22 | 11.10 | 10.98 | 11.71 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10.73 | 10.37 | 11.22 | 11.10 | 10.98 | 11.71 |'
- en: '| 2 | 11.10 | 12.56 | 12.68 | 12.44 | 9.76 | 12.68 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 11.10 | 12.56 | 12.68 | 12.44 | 9.76 | 12.68 |'
- en: '| 128 | 0 | 9.88 | 10.98 | 11.83 | 11.83 | 10.85 | 10.73 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0 | 9.88 | 10.98 | 11.83 | 11.83 | 10.85 | 10.73 |'
- en: '| 1 | 10.24 | 12.07 | 11.83 | 11.95 | 0.00 | 10.00 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10.24 | 12.07 | 11.83 | 11.95 | 0.00 | 10.00 |'
- en: '| 2 | 10.98 | 12.68 | 11.95 | 9.15 | 6.83 | 5.85 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 10.98 | 12.68 | 11.95 | 9.15 | 6.83 | 5.85 |'
- en: 'Table 8: Evaluation scores (%): federated fine-tuning LLaMA-7B with P-tuning
    on Fed-CodeAlpaca.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 评估得分（%）：在 Fed-CodeAlpaca 上使用 P-tuning 对 LLaMA-7B 进行联邦微调。'
- en: '| #Virtual tokens | Seed | LR |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| #虚拟代币 | 种子 | 学习率 |'
- en: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 10 | 0 | 8.90 | 6.83 | 8.78 | 10.24 | 10.61 | 6.83 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 8.90 | 6.83 | 8.78 | 10.24 | 10.61 | 6.83 |'
- en: '| 1 | 11.10 | 9.15 | 10.00 | 9.51 | 1.46 | 6.95 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 11.10 | 9.15 | 10.00 | 9.51 | 1.46 | 6.95 |'
- en: '| 2 | 5.37 | 9.39 | 5.85 | 4.14 | 5.37 | 1.83 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5.37 | 9.39 | 5.85 | 4.14 | 5.37 | 1.83 |'
- en: '| 20 | 0 | 5.73 | 11.22 | 9.39 | 13.05 | 9.51 | 4.51 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0 | 5.73 | 11.22 | 9.39 | 13.05 | 9.51 | 4.51 |'
- en: '| 1 | 8.78 | 10.85 | 10.00 | 9.39 | 9.02 | 3.05 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8.78 | 10.85 | 10.00 | 9.39 | 9.02 | 3.05 |'
- en: '| 2 | 7.93 | 3.17 | 8.78 | 2.93 | 10.61 | 3.66 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 7.93 | 3.17 | 8.78 | 2.93 | 10.61 | 3.66 |'
- en: '| 30 | 0 | 9.02 | 10.24 | 10.00 | 10.98 | 9.14 | 1.95 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 0 | 9.02 | 10.24 | 10.00 | 10.98 | 9.14 | 1.95 |'
- en: '| 1 | 12.32 | 5.49 | 5.37 | 10.24 | 2.68 | 2.80 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 12.32 | 5.49 | 5.37 | 10.24 | 2.68 | 2.80 |'
- en: '| 2 | 7.07 | 9.02 | 9.63 | 5.24 | 10.61 | 0.24 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 7.07 | 9.02 | 9.63 | 5.24 | 10.61 | 0.24 |'
- en: 'Table 9: Evaluation scores (%): federated fine-tuning LLaMA-7B with prompt
    tuning on Fed-CodeAlpaca. In this table, the initialized prompt is “Program a
    function by following description.”. The number of virtual tokens is 8.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：评估得分（%）：在 Fed-CodeAlpaca 上使用提示调优进行的 LLaMA-7B 联邦微调。在此表中，初始化提示为“根据描述编写函数。”虚拟令牌的数量为
    8。
- en: '| Seed | LR |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | LR |'
- en: '|  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 0 | 10.16 | 11.07 | 10.69 | 9.63 | 12.28 | 14.03 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 10.16 | 11.07 | 10.69 | 9.63 | 12.28 | 14.03 |'
- en: '| 1 | 7.96 | 9.40 | 10.61 | 11.90 | 11.60 | 9.63 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7.96 | 9.40 | 10.61 | 11.90 | 11.60 | 9.63 |'
- en: '| 2 | 8.64 | 9.40 | 10.24 | 10.39 | 11.37 | 11.22 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8.64 | 9.40 | 10.24 | 10.39 | 11.37 | 11.22 |'
- en: 'Table 10: Evaluation scores (%): federated fine-tuning LLaMA-7B with LoRA on
    Fed-Dolly.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：评估得分（%）：在 Fed-Dolly 上使用 LoRA 进行的 LLaMA-7B 联邦微调。
- en: '| Scaling coef. | 32 | 64 | 128 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 扩展系数 | 32 | 64 | 128 |'
- en: '| Dropout prob. | 0.0 | 0.1 | 0.0 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 丢弃概率 | 0.0 | 0.1 | 0.0 |'
- en: '| LR | 0.005 | 0.003 | 0.0003 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| LR | 0.005 | 0.003 | 0.0003 |'
- en: '| Seed | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
- en: '| MMLU | 36.30 | 35.50 | 36.80 | 36.50 | 37.20 | 37.40 | 33.20 | 35.20 | 33.80
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 36.30 | 35.50 | 36.80 | 36.50 | 37.20 | 37.40 | 33.20 | 35.20 | 33.80
    |'
- en: '| BoolQ | 80.00 | 77.00 | 78.00 | 82.00 | 81.00 | 74.00 | 78.00 | 79.00 | 80.00
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 80.00 | 77.00 | 78.00 | 82.00 | 81.00 | 74.00 | 78.00 | 79.00 | 80.00
    |'
- en: '| NarrativeQA | 55.60 | 49.80 | 55.00 | 53.80 | 51.60 | 53.20 | 51.00 | 51.60
    | 50.90 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 55.60 | 49.80 | 55.00 | 53.80 | 51.60 | 53.20 | 51.00 | 51.60
    | 50.90 |'
- en: '| NaturalQuestions(closed) | 21.20 | 22.00 | 20.40 | 21.40 | 28.00 | 20.60
    | 23.70 | 22.20 | 23.80 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions（封闭） | 21.20 | 22.00 | 20.40 | 21.40 | 28.00 | 20.60 | 23.70
    | 22.20 | 23.80 |'
- en: '| NaturalQuestions(open) | 71.30 | 66.90 | 61.60 | 70.90 | 67.10 | 69.40 |
    64.00 | 60.90 | 63.20 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions（开放） | 71.30 | 66.90 | 61.60 | 70.90 | 67.10 | 69.40 | 64.00
    | 60.90 | 63.20 |'
- en: '| QuAC | 32.30 | 31.20 | 31.20 | 32.40 | 28.40 | 28.50 | 32.10 | 34.30 | 32.60
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| QuAC | 32.30 | 31.20 | 31.20 | 32.40 | 28.40 | 28.50 | 32.10 | 34.30 | 32.60
    |'
- en: '| HellaSwag | 81.00 | 82.00 | 80.00 | 82.00 | 79.00 | 79.00 | 81.00 | 81.00
    | 81.00 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 81.00 | 82.00 | 80.00 | 82.00 | 79.00 | 79.00 | 81.00 | 81.00
    | 81.00 |'
- en: '| OpenbookQA | 50.00 | 53.00 | 46.00 | 52.00 | 55.00 | 51.00 | 52.00 | 49.00
    | 52.00 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| OpenbookQA | 50.00 | 53.00 | 46.00 | 52.00 | 55.00 | 51.00 | 52.00 | 49.00
    | 52.00 |'
- en: '| TruthfulQA | 18.00 | 30.00 | 30.00 | 24.00 | 24.00 | 23.00 | 23.00 | 30.00
    | 25.00 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA | 18.00 | 30.00 | 30.00 | 24.00 | 24.00 | 23.00 | 23.00 | 30.00
    | 25.00 |'
- en: '| MS MARCO (regular) | 18.60 | 14.20 | 17.40 | 18.20 | 15.50 | 22.90 | 17.20
    | 15.90 | 15.00 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO（常规） | 18.60 | 14.20 | 17.40 | 18.20 | 15.50 | 22.90 | 17.20 | 15.90
    | 15.00 |'
- en: '| MS MARCO (TREC) | 40.60 | 42.70 | 42.50 | 38.50 | 41.20 | 42.70 | 41.40 |
    41.90 | 39.70 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO（TREC） | 40.60 | 42.70 | 42.50 | 38.50 | 41.20 | 42.70 | 41.40 |
    41.90 | 39.70 |'
- en: '| CNN/DailyMail | 15.10 | 12.30 | 14.00 | 13.60 | 14.40 | 13.00 | 12.00 | 12.10
    | 11.80 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| CNN/DailyMail | 15.10 | 12.30 | 14.00 | 13.60 | 14.40 | 13.00 | 12.00 | 12.10
    | 11.80 |'
- en: '| XSUM | 9.50 | 10.10 | 10.30 | 13.70 | 10.90 | 10.80 | 8.80 | 9.50 | 11.80
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| XSUM | 9.50 | 10.10 | 10.30 | 13.70 | 10.90 | 10.80 | 8.80 | 9.50 | 11.80
    |'
- en: '| IMDB | 94.00 | 98.00 | 97.00 | 95.00 | 94.00 | 97.00 | 98.00 | 96.00 | 98.00
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | 94.00 | 98.00 | 97.00 | 95.00 | 94.00 | 97.00 | 98.00 | 96.00 | 98.00
    |'
- en: '| CivilComments | 59.50 | 66.00 | 56.90 | 57.00 | 59.70 | 58.30 | 62.20 | 57.30
    | 61.70 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| CivilComments | 59.50 | 66.00 | 56.90 | 57.00 | 59.70 | 58.30 | 62.20 | 57.30
    | 61.70 |'
- en: '| RAFT | 59.70 | 59.80 | 58.80 | 55.90 | 61.60 | 58.90 | 64.80 | 60.90 | 59.80
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| RAFT | 59.70 | 59.80 | 58.80 | 55.90 | 61.60 | 58.90 | 64.80 | 60.90 | 59.80
    |'
- en: '| Average | 46.42 | 46.91 | 45.99 | 46.68 | 46.79 | 46.23 | 46.40 | 46.05 |
    46.26 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 46.42 | 46.91 | 45.99 | 46.68 | 46.79 | 46.23 | 46.40 | 46.05 | 46.26
    |'
- en: 'Table 11: Evaluation scores (%): federated fine-tuning LLaMA-7B with P-tuning
    on Fed-Dolly.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：评估得分（%）：在 Fed-Dolly 上使用 P-tuning 进行的 LLaMA-7B 联邦微调。
- en: '| #Virtual tokens | 20 | 20 | 30 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| #虚拟令牌 | 20 | 20 | 30 |'
- en: '| LR | 0.0003 | 0.0005 | 0.0005 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| LR | 0.0003 | 0.0005 | 0.0005 |'
- en: '| Seed | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
- en: '| MMLU | 35.20 | 34.60 | 35.20 | 35.30 | 35.10 | 35.10 | 34.30 | 35.40 | 35.10
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 35.20 | 34.60 | 35.20 | 35.30 | 35.10 | 35.10 | 34.30 | 35.40 | 35.10
    |'
- en: '| BoolQ | 79.00 | 78.00 | 78.00 | 77.00 | 80.00 | 76.00 | 82.00 | 78.00 | 79.00
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 79.00 | 78.00 | 78.00 | 77.00 | 80.00 | 76.00 | 82.00 | 78.00 | 79.00
    |'
- en: '| NarrativeQA | 50.40 | 54.60 | 52.60 | 52.30 | 52.10 | 49.70 | 52.20 | 52.30
    | 52.50 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 50.40 | 54.60 | 52.60 | 52.30 | 52.10 | 49.70 | 52.20 | 52.30
    | 52.50 |'
- en: '| NaturalQuestions(closed) | 26.20 | 25.50 | 25.00 | 25.40 | 27.60 | 25.50
    | 25.60 | 24.90 | 27.10 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions（封闭） | 26.20 | 25.50 | 25.00 | 25.40 | 27.60 | 25.50 | 25.60
    | 24.90 | 27.10 |'
- en: '| NaturalQuestions(open) | 66.80 | 66.10 | 66.60 | 67.10 | 66.20 | 64.00 |
    66.80 | 65.90 | 65.60 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions（开放） | 66.80 | 66.10 | 66.60 | 67.10 | 66.20 | 64.00 | 66.80
    | 65.90 | 65.60 |'
- en: '| QuAC | 28.80 | 30.00 | 29.50 | 30.10 | 27.70 | 29.80 | 31.10 | 30.50 | 30.20
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| QuAC | 28.80 | 30.00 | 29.50 | 30.10 | 27.70 | 29.80 | 31.10 | 30.50 | 30.20
    |'
- en: '| HellaSwag | 22.00 | 22.00 | 20.00 | 25.00 | 23.00 | 21.00 | 17.00 | 20.00
    | 19.00 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 22.00 | 22.00 | 20.00 | 25.00 | 23.00 | 21.00 | 17.00 | 20.00
    | 19.00 |'
- en: '| OpenbookQA | 23.30 | 23.30 | 27.30 | 30.30 | 23.30 | 18.30 | 28.30 | 22.30
    | 25.30 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| OpenbookQA | 23.30 | 23.30 | 27.30 | 30.30 | 23.30 | 18.30 | 28.30 | 22.30
    | 25.30 |'
- en: '| TruthfulQA | 30.00 | 32.00 | 33.00 | 29.00 | 28.00 | 35.00 | 29.00 | 26.00
    | 28.00 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA | 30.00 | 32.00 | 33.00 | 29.00 | 28.00 | 35.00 | 29.00 | 26.00
    | 28.00 |'
- en: '| MS MARCO (regular) | 18.10 | 17.60 | 16.00 | 15.70 | 15.50 | 13.90 | 20.50
    | 12.80 | 16.40 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO (regular) | 18.10 | 17.60 | 16.00 | 15.70 | 15.50 | 13.90 | 20.50
    | 12.80 | 16.40 |'
- en: '| MS MARCO (TREC) | 45.00 | 39.50 | 42.60 | 43.50 | 42.40 | 36.40 | 41.00 |
    30.00 | 44.00 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO (TREC) | 45.00 | 39.50 | 42.60 | 43.50 | 42.40 | 36.40 | 41.00 |
    30.00 | 44.00 |'
- en: '| CNN/DailyMail | 12.90 | 11.70 | 13.70 | 12.40 | 13.40 | 12.40 | 13.50 | 13.50
    | 12.30 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| CNN/DailyMail | 12.90 | 11.70 | 13.70 | 12.40 | 13.40 | 12.40 | 13.50 | 13.50
    | 12.30 |'
- en: '| XSUM | 11.30 | 10.00 | 10.50 | 11.30 | 11.60 | 11.10 | 10.60 | 9.50 | 10.70
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| XSUM | 11.30 | 10.00 | 10.50 | 11.30 | 11.60 | 11.10 | 10.60 | 9.50 | 10.70
    |'
- en: '| IMDB | 95.00 | 92.00 | 95.00 | 93.00 | 94.00 | 94.00 | 95.00 | 95.00 | 94.00
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | 95.00 | 92.00 | 95.00 | 93.00 | 94.00 | 94.00 | 95.00 | 95.00 | 94.00
    |'
- en: '| CivilComments | 63.70 | 59.40 | 59.60 | 61.90 | 62.20 | 57.40 | 56.60 | 59.70
    | 63.10 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| CivilComments | 63.70 | 59.40 | 59.60 | 61.90 | 62.20 | 57.40 | 56.60 | 59.70
    | 63.10 |'
- en: '| RAFT | 61.60 | 60.70 | 61.10 | 63.20 | 60.90 | 61.60 | 61.40 | 61.40 | 62.30
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| RAFT | 61.60 | 60.70 | 61.10 | 63.20 | 60.90 | 61.60 | 61.40 | 61.40 | 62.30
    |'
- en: '| Average | 41.83 | 41.06 | 41.61 | 42.03 | 41.44 | 40.08 | 41.56 | 39.83 |
    41.54 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 41.83 | 41.06 | 41.61 | 42.03 | 41.44 | 40.08 | 41.56 | 39.83 | 41.54
    |'
- en: 'Table 12: Evaluation scores (%): federated fine-tuning LLaMA-7B with prompt
    tuning on Fed-Dolly. In this table, the initialized prompt is “Assume you can
    understand and answer questions.”. The number of virtual tokens is 9.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 评估分数（%）：在 Fed-Dolly 上使用提示微调 LLaMA-7B。在此表中，初始化提示为“假设你能理解并回答问题。”虚拟标记的数量为
    9。'
- en: '| LR | 0.0003 | 0.0005 | 0.001 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| LR | 0.0003 | 0.0005 | 0.001 |'
- en: '| Seed | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Seed | 0 | 1 | 2 | 0 | 1 | 2 | 0 | 1 | 2 |'
- en: '| MMLU | 34.10 | 34.50 | 34.00 | 35.10 | 35.50 | 34.60 | 36.00 | 34.40 | 34.20
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 34.10 | 34.50 | 34.00 | 35.10 | 35.50 | 34.60 | 36.00 | 34.40 | 34.20
    |'
- en: '| BoolQ | 79.00 | 77.00 | 76.00 | 77.00 | 78.00 | 74.00 | 77.00 | 77.00 | 76.00
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 79.00 | 77.00 | 76.00 | 77.00 | 78.00 | 74.00 | 77.00 | 77.00 | 76.00
    |'
- en: '| NarrativeQA | 50.10 | 49.20 | 50.10 | 50.50 | 51.50 | 52.10 | 50.10 | 52.70
    | 50.40 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 50.10 | 49.20 | 50.10 | 50.50 | 51.50 | 52.10 | 50.10 | 52.70
    | 50.40 |'
- en: '| NaturalQuestions(closed) | 24.70 | 24.40 | 24.90 | 23.50 | 25.80 | 24.30
    | 23.80 | 26.30 | 25.80 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions(closed) | 24.70 | 24.40 | 24.90 | 23.50 | 25.80 | 24.30
    | 23.80 | 26.30 | 25.80 |'
- en: '| NaturalQuestions(open) | 66.60 | 64.60 | 66.00 | 64.90 | 62.80 | 65.80 |
    61.90 | 63.00 | 64.60 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| NaturalQuestions(open) | 66.60 | 64.60 | 66.00 | 64.90 | 62.80 | 65.80 |
    61.90 | 63.00 | 64.60 |'
- en: '| QuAC | 32.00 | 28.50 | 28.40 | 29.50 | 29.00 | 29.60 | 30.20 | 30.10 | 28.60
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| QuAC | 32.00 | 28.50 | 28.40 | 29.50 | 29.00 | 29.60 | 30.20 | 30.10 | 28.60
    |'
- en: '| HellaSwag | 19.00 | 19.00 | 20.00 | 21.00 | 19.00 | 20.00 | 21.00 | 20.00
    | 19.00 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 19.00 | 19.00 | 20.00 | 21.00 | 19.00 | 20.00 | 21.00 | 20.00
    | 19.00 |'
- en: '| OpenbookQA | 27.30 | 20.00 | 21.00 | 32.30 | 27.30 | 20.30 | 27.30 | 19.30
    | 19.00 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| OpenbookQA | 27.30 | 20.00 | 21.00 | 32.30 | 27.30 | 20.30 | 27.30 | 19.30
    | 19.00 |'
- en: '| TruthfulQA | 32.00 | 32.00 | 34.00 | 32.00 | 31.00 | 33.00 | 29.00 | 36.00
    | 33.00 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| TruthfulQA | 32.00 | 32.00 | 34.00 | 32.00 | 31.00 | 33.00 | 29.00 | 36.00
    | 33.00 |'
- en: '| MS MARCO (regular) | 13.60 | 14.50 | 13.70 | 14.10 | 14.20 | 14.10 | 12.70
    | 17.70 | 21.80 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO (regular) | 13.60 | 14.50 | 13.70 | 14.10 | 14.20 | 14.10 | 12.70
    | 17.70 | 21.80 |'
- en: '| MS MARCO (TREC) | 35.90 | 41.20 | 39.00 | 39.70 | 39.60 | 35.30 | 39.80 |
    43.20 | 43.70 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| MS MARCO (TREC) | 35.90 | 41.20 | 39.00 | 39.70 | 39.60 | 35.30 | 39.80 |
    43.20 | 43.70 |'
- en: '| CNN/DailyMail | 12.30 | 13.00 | 13.70 | 13.60 | 14.20 | 13.90 | 13.30 | 14.10
    | 14.40 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| CNN/DailyMail | 12.30 | 13.00 | 13.70 | 13.60 | 14.20 | 13.90 | 13.30 | 14.10
    | 14.40 |'
- en: '| XSUM | 10.30 | 12.10 | 10.40 | 11.70 | 11.20 | 10.70 | 10.70 | 11.20 | 11.10
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| XSUM | 10.30 | 12.10 | 10.40 | 11.70 | 11.20 | 10.70 | 10.70 | 11.20 | 11.10
    |'
- en: '| IMDB | 94.00 | 97.00 | 98.00 | 94.00 | 95.00 | 95.00 | 91.00 | 94.00 | 95.00
    |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | 94.00 | 97.00 | 98.00 | 94.00 | 95.00 | 95.00 | 91.00 | 94.00 | 95.00
    |'
- en: '| CivilComments | 61.30 | 59.10 | 62.30 | 62.60 | 54.70 | 56.60 | 58.50 | 59.90
    | 60.00 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| CivilComments | 61.30 | 59.10 | 62.30 | 62.60 | 54.70 | 56.60 | 58.50 | 59.90
    | 60.00 |'
- en: '| RAFT | 60.50 | 60.50 | 60.20 | 58.60 | 56.80 | 59.30 | 54.80 | 60.20 | 61.80
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| RAFT | 60.50 | 60.50 | 60.20 | 58.60 | 56.80 | 59.30 | 54.80 | 60.20 | 61.80
    |'
- en: '| Average | 40.79 | 40.41 | 40.73 | 41.26 | 40.35 | 39.91 | 39.82 | 41.19 |
    41.15 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 40.79 | 40.41 | 40.73 | 41.26 | 40.35 | 39.91 | 39.82 | 41.19 | 41.15
    |'
- en: 'Table 13: Evaluation scores (%): federated fine-tuning LLaMA-7B with LoRA on
    Fed-GSM8K-3.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 评估分数（%）：在 Fed-GSM8K-3 上使用 LoRA 进行联邦微调 LLaMA-7B。'
- en: '| Scaling coef. | Seed | LR |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Scaling coef. | Seed | LR |'
- en: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 16 | 0 | 8.79 | 6.97 | 8.87 | 8.26 | 12.13 | 11.22 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 0 | 8.79 | 6.97 | 8.87 | 8.26 | 12.13 | 11.22 |'
- en: '| 1 | 8.34 | 8.19 | 8.95 | 8.95 | 9.55 | 13.04 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8.34 | 8.19 | 8.95 | 8.95 | 9.55 | 13.04 |'
- en: '| 2 | 9.17 | 6.37 | 9.63 | 9.33 | 11.83 | 13.42 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9.17 | 6.37 | 9.63 | 9.33 | 11.83 | 13.42 |'
- en: '| 32 | 0 | 6.82 | 7.73 | 9.25 | 12.21 | 10.99 | 4.09 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 0 | 6.82 | 7.73 | 9.25 | 12.21 | 10.99 | 4.09 |'
- en: '| 1 | 7.81 | 8.04 | 8.79 | 10.16 | 13.19 | 0.00 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7.81 | 8.04 | 8.79 | 10.16 | 13.19 | 0.00 |'
- en: '| 2 | 9.10 | 8.79 | 8.95 | 11.9 | 12.89 | 13.87 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9.10 | 8.79 | 8.95 | 11.9 | 12.89 | 13.87 |'
- en: '| 64 | 0 | 8.95 | 10.61 | 11.30 | 12.13 | 13.42 | 0.00 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 0 | 8.95 | 10.61 | 11.30 | 12.13 | 13.42 | 0.00 |'
- en: '| 1 | 8.26 | 9.40 | 11.30 | 10.99 | 0.00 | 0.30 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8.26 | 9.40 | 11.30 | 10.99 | 0.00 | 0.30 |'
- en: '| 2 | 9.93 | 10.46 | 12.28 | 12.43 | 12.28 | 13.42 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9.93 | 10.46 | 12.28 | 12.43 | 12.28 | 13.42 |'
- en: '| 128 | 0 | 9.93 | 11.14 | 12.96 | 3.49 | 15.31 | 15.47 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0 | 9.93 | 11.14 | 12.96 | 3.49 | 15.31 | 15.47 |'
- en: '| 1 | 10.54 | 9.40 | 13.12 | 10.99 | 15.09 | 10.92 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 10.54 | 9.40 | 13.12 | 10.99 | 15.09 | 10.92 |'
- en: '| 2 | 9.25 | 11.83 | 11.60 | 12.96 | 12.28 | 11.60 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9.25 | 11.83 | 11.60 | 12.96 | 12.28 | 11.60 |'
- en: 'Table 14: Evaluation scores (%): federated fine-tuning LLaMA-7B with P-tuning
    on Fed-GSM8K-3.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '表14: 评估分数（%）：在 Fed-GSM8K-3 上使用 P-tuning 联邦微调 LLaMA-7B。'
- en: '| #Virtual tokens | Seed | LR |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| #虚拟标记 | 种子 | 学习率 |'
- en: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 10 | 0 | 10.16 | 11.07 | 10.69 | 9.63 | 12.28 | 14.03 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 10.16 | 11.07 | 10.69 | 9.63 | 12.28 | 14.03 |'
- en: '| 1 | 7.96 | 9.40 | 10.61 | 11.90 | 11.60 | 9.63 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 7.96 | 9.40 | 10.61 | 11.90 | 11.60 | 9.63 |'
- en: '| 2 | 8.64 | 9.40 | 10.24 | 10.39 | 11.37 | 11.22 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8.64 | 9.40 | 10.24 | 10.39 | 11.37 | 11.22 |'
- en: '| 20 | 0 | 9.40 | 11.22 | 12.74 | 11.75 | 9.40 | 10.24 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0 | 9.40 | 11.22 | 12.74 | 11.75 | 9.40 | 10.24 |'
- en: '| 1 | 8.26 | 9.17 | 10.08 | 11.60 | 12.36 | 8.72 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8.26 | 9.17 | 10.08 | 11.60 | 12.36 | 8.72 |'
- en: '| 2 | 10.39 | 10.54 | 10.77 | 10.01 | 10.77 | 11.68 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 10.39 | 10.54 | 10.77 | 10.01 | 10.77 | 11.68 |'
- en: '| 30 | 0 | 10.08 | 11.14 | 12.05 | 11.07 | 10.69 | 8.19 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 0 | 10.08 | 11.14 | 12.05 | 11.07 | 10.69 | 8.19 |'
- en: '| 1 | 6.60 | 9.63 | 9.33 | 9.25 | 10.16 | 10.84 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6.60 | 9.63 | 9.33 | 9.25 | 10.16 | 10.84 |'
- en: '| 2 | 8.87 | 9.93 | 10.77 | 10.46 | 10.39 | 11.22 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8.87 | 9.93 | 10.77 | 10.46 | 10.39 | 11.22 |'
- en: 'Table 15: Evaluation scores (%): federated fine-tuning LLaMA-7B with prompt
    tuning on Fed-GSM8K-3. In this table, the initialized prompt is “Think step by
    step.”. The number of virtual tokens is 6.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '表15: 评估分数（%）：在 Fed-GSM8K-3 上使用提示微调 LLaMA-7B。本表中，初始化的提示是“逐步思考。”虚拟标记的数量是 6。'
- en: '| Seed | LR |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | 学习率 |'
- en: '|  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | 0.0001 | 0.0003 | 0.0005 | 0.001 | 0.003 | 0.005 |'
- en: '| 0 | 8.64 | 10.01 | 9.33 | 10.08 | 9.70 | 9.93 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 8.64 | 10.01 | 9.33 | 10.08 | 9.70 | 9.93 |'
- en: '| 1 | 8.19 | 9.33 | 9.78 | 9.10 | 7.28 | 9.10 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8.19 | 9.33 | 9.78 | 9.10 | 7.28 | 9.10 |'
- en: '| 2 | 8.79 | 8.87 | 8.72 | 9.40 | 9.02 | 10.54 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8.79 | 8.87 | 8.72 | 9.40 | 9.02 | 10.54 |'
- en: '| User: | Write a haiku about crocodiles in outer space in the voice of a pirate.
    |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 用户: | 用海盗的口吻写一首关于外太空鳄鱼的俳句。 |'
- en: '| System0: | I’m a croc in space, And I don’t know what to do. I’ve never been
    here before, And |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 系统0: | 我是太空中的鳄鱼， 我不知道该怎么做。我从未到过这里， |'
- en: '| it’s all so new to me. But I know I have to find a way To get back home to
    you. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 一切对我来说都是如此陌生。但我知道我必须找到一种方法 回到你身边。 |'
- en: '| System1: | Crocodiles, in space? I’d rather be on the sea. Pirate’s life
    for me! -Arrrrrrr! Hoist |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 系统1: | 鳄鱼们，太空里？我宁愿在海上。海盗的生活就是我！-Arrrrrrr！升起 |'
- en: '| the Jolly Roger, matey. We’re off to plunder the seven seas! Ahoy, me hearties!
    |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 升起骷髅旗，伙伴们。我们出发去掠夺七大洋！啊oy，我的心肝们！ |'
- en: '| Let the pillaging begin! -Aye, aye Cap’n! - Soon we’ll be rich beyond our
    wildest |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 让掠夺开始吧！- 是的，船长！- 很快我们将富有到难以想象的地步 |'
- en: '| dreams, and we won’t have to work a day in our lives. |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 梦想，我们将不再需要工作。 |'
- en: '| System2: | Crocodiles on the moon, Ahoy me hearties! We’ll be pirates no
    more! The crocs |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 系统2: | 月球上的鳄鱼， 啊oy，我的心肝们！我们将不再做海盗！鳄鱼们 |'
- en: '| have won! Mwahahaha. Aye, matey! Hoist the Jolly Roger! |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 已经赢了！呜哈哈哈。是的，伙伴们！升起骷髅旗！ |'
- en: '| System3: | Crocodiles on the moon, I’m going to eat you all! Ahoy, matey!
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 系统3: | 月球上的鳄鱼，我要吃掉你们所有人！啊oy，伙伴们！ |'
- en: 'Table 16: Comparison of creative writing capability among different fine-tuning
    algorithms for LLaMA-7B in FL. System0 is the baseline model without fine-tuning,
    System1 is fine-tuned with LoRA, System2 is fine-tuned with P-tuning, and System3
    is fine-tuned with prompt tuning.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '表16: 在 FL 中不同微调算法对 LLaMA-7B 创意写作能力的比较。系统0 是未微调的基线模型，系统1 使用 LoRA 微调，系统2 使用 P-tuning
    微调，系统3 使用提示微调。'
