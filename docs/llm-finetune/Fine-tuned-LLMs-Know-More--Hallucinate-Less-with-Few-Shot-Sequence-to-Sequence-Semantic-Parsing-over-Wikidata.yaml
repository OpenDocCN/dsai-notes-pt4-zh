- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:40:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:40:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调的 LLMs 知道更多，幻觉更少，通过在 Wikidata 上进行少样本序列到序列语义解析
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14202](https://ar5iv.labs.arxiv.org/html/2305.14202)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.14202](https://ar5iv.labs.arxiv.org/html/2305.14202)
- en: Silei Xu^∗ Shicheng Liu^∗ Theo Culhane  Elizaveta Pertseva
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 徐思蕾^∗ 刘世成^∗ 西奥·卡尔哈内 埃利扎维塔·佩尔采娃
- en: Meng-Hsi Wu¹  Sina J. Semnani  Monica S. Lam Computer Science Department, Stanford
    University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 吴孟熙¹  赛娜·J·塞姆纳尼  莫妮卡·S·拉姆 计算机科学系，斯坦福大学
- en: Stanford, CA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福，加利福尼亚
- en: '{silei, shicheng, tculhane, pertseva, sinaj, lam}@cs.stanford.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{silei, shicheng, tculhane, pertseva, sinaj, lam}@cs.stanford.edu'
- en: ¹Ailly.ai
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Ailly.ai
- en: jwu@ailly.ai
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: jwu@ailly.ai
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While large language models (LLMs) can answer many questions correctly, they
    can also hallucinate and give wrong answers. Wikidata, with its over 12 billion
    facts, can be used to ground LLMs to improve their factuality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型 (LLMs) 可以正确回答许多问题，但它们也可能出现幻觉并给出错误的答案。Wikidata 拥有超过 120 亿个事实，可以用于对大型语言模型进行基础性验证，以提高其准确性。
- en: This paper presents WikiWebQuestions, a high-quality question answering benchmark
    for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world
    data with SPARQL annotation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 WikiWebQuestions，一个用于 Wikidata 的高质量问答基准。它从 Freebase 的 WebQuestions 移植而来，由带有
    SPARQL 注释的真实世界数据组成。
- en: This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata.
    We modify SPARQL to use the unique domain and property names instead of their
    IDs. We train the parser to use either the results from an entity linker or mentions
    in the query. We fine-tune LLaMA by adding the few-shot training data to that
    used to fine-tune Alpaca.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个用于 Wikidata 的少样本序列到序列语义解析器。我们修改 SPARQL 使用唯一的领域和属性名称，而不是它们的 ID。我们训练解析器使用来自实体链接器的结果或查询中的提及。我们通过将少样本训练数据添加到用于微调
    Alpaca 的数据中来微调 LLaMA。
- en: Our experimental results demonstrate the effectiveness of this methodology,
    establishing a strong baseline of 76% and 65% answer accuracy in the dev and test
    sets of WikiWebQuestions, respectively. By pairing our semantic parser with GPT-3,
    we combine verifiable results with qualified GPT-3 guesses to provide useful answers
    to 96% of the questions in dev. We also show that our method outperforms the state-of-the-art
    for the QALD-7 Wikidata dataset by 3.6% in F1 score.¹¹1Code, data, and model are
    available at [https://github.com/stanford-oval/wikidata-emnlp23](https://github.com/stanford-oval/wikidata-emnlp23)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果证明了这种方法的有效性，在 WikiWebQuestions 的开发集和测试集上分别建立了 76% 和 65% 的答案准确率强基线。通过将我们的语义解析器与
    GPT-3 配对，我们将可验证的结果与合格的 GPT-3 猜测结合起来，为开发集中的 96% 问题提供了有用的答案。我们还展示了我们的方法在 QALD-7
    Wikidata 数据集上的 F1 分数比现有最先进技术高出 3.6%。¹¹1代码、数据和模型可在 [https://github.com/stanford-oval/wikidata-emnlp23](https://github.com/stanford-oval/wikidata-emnlp23)
    获得
- en: '^*^*footnotetext: Equal contribution'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注：贡献相等
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/663ea14e2396116934ae4581d0fc5835.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/663ea14e2396116934ae4581d0fc5835.png)'
- en: 'Figure 1: An Overview of WikiSP. An entity linker is used to link entities
    in the user query to their unique ID in Wikidata; e.g. “A Bronx Tale” is linked
    to entity ID “Q1130705”. The query and entity linker outputs are fed to the WikiSP
    semantic parser to produce a modified version of SPARQL, where property IDs (e.g.
    “P915”) are replaced by their unique string identifiers (e.g. “filming_location”).
    If applying the query to Wikidata fails to return a result, we default to GPT-3,
    labeling the result as a GPT-3 guess. Returned answers are presented in the context
    of the query, so the user can tell if the answer is acceptable; if not, we also
    show the guess from GPT-3\. Here WikiSP mistakenly uses “filming_location” instead
    of “narrative_location”; the user detects the mistake, thumbs down the answer,
    and the GPT-3 answer is provided.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: WikiSP 概述。一个实体链接器用于将用户查询中的实体链接到它们在 Wikidata 中的唯一 ID；例如，“A Bronx Tale”链接到实体
    ID “Q1130705”。查询和实体链接器的输出被送入 WikiSP 语义解析器，生成一个修改版的 SPARQL，其中属性 ID（例如“P915”）被替换为其唯一字符串标识符（例如“filming_location”）。如果将查询应用于
    Wikidata 未能返回结果，我们默认使用 GPT-3，将结果标记为 GPT-3 猜测。返回的答案在查询的上下文中呈现，以便用户可以判断答案是否可接受；如果不接受，我们还会显示
    GPT-3 的猜测。在这里，WikiSP 错误地使用了“filming_location”而不是“narrative_location”；用户发现了错误，给答案点了不赞成，GPT-3
    的答案被提供。'
- en: Large language models (LLMs) such as GPT-3 can answer open-domain questions
    without access to external knowledge or any task-specific training examples. However,
    LLMs are prone to hallucinate (Bang et al., [2023](#bib.bib4)), while using a
    convincing and confident tone. This may cause significant harm as people increasingly
    accept LLMs as a knowledge source (Goddard, [2023](#bib.bib14); Weiser, [2023](#bib.bib46)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3这样的语言模型（LLMs）可以回答开放领域的问题，而无需访问外部知识或任何特定任务的训练示例。然而，LLMs容易出现幻觉（Bang等，[2023](#bib.bib4)），同时使用令人信服且自信的语气。这可能会造成重大伤害，因为人们越来越多地将LLMs视为知识来源（Goddard，[2023](#bib.bib14)；Weiser，[2023](#bib.bib46)）。
- en: In contrast, traditional knowledge base question answering (KBQA) is grounded
    with a given knowledge base. Semantic parsing (SP) has been widely used to tackle
    this challenging task, where the questions are first parsed into a logical form
    and then executed to retrieve answers from the knowledge base. It has better interpretability
    than GPT-3 and other information-retrieval-based approaches (Dong et al., [2015](#bib.bib13);
    Miller et al., [2016](#bib.bib24); Sun et al., [2018](#bib.bib38), [2019](#bib.bib37))
    where answers are predicted directly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，传统的知识库问答（KBQA）则基于给定的知识库。语义解析（SP）已被广泛用于解决这一挑战性任务，其中问题首先被解析成逻辑形式，然后执行以从知识库中检索答案。它比GPT-3和其他基于信息检索的方法（Dong等，[2015](#bib.bib13)；Miller等，[2016](#bib.bib24)；Sun等，[2018](#bib.bib38)，[2019](#bib.bib37)）具有更好的可解释性，其中答案是直接预测的。
- en: '![Refer to caption](img/a0a5c992fd14712255fd32fd337a0d98.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a0a5c992fd14712255fd32fd337a0d98.png)'
- en: 'Figure 2: Distribution of correct, incomplete, and incorrect answers for the
    WikiWebQuestions dev set, when GPT-3 is used alone and when combined with WikiSP.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在WikiWebQuestions开发集上，当单独使用GPT-3与结合WikiSP时，正确、不完整和错误答案的分布。
- en: To handle large knowledge bases, previous SP-based approaches tend to use a
    multi-stage pipeline of sub-tasks, starting with extracting the relevant subgraph
    based on entities detected in the questions (Yih et al., [2015](#bib.bib50); Luo
    et al., [2018](#bib.bib22)). Such an approach struggles with questions that have
    a large search space and fails to understand questions that refer to information
    missing in the knowledge graph. Having to retrieve the relevant subgraphs to create
    the logical form conflates query resolution with semantic parsing, rendering classical
    query optimization inapplicable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理大型知识库，之前的基于SP的方法通常使用多阶段的子任务流水线，从提取基于问题中检测到的实体的相关子图开始（Yih等，[2015](#bib.bib50)；Luo等，[2018](#bib.bib22)）。这种方法在面对具有大搜索空间的问题时表现不佳，且无法理解那些涉及知识图谱中缺失信息的问题。必须检索相关子图以创建逻辑形式，将查询解析与语义解析混为一谈，从而使得传统的查询优化方法不适用。
- en: End-to-end seq2seq translation, on the other hand, has mainly been used on schemas
    of relatively small relational databases (Yu et al., [2018](#bib.bib53); Xu et al.,
    [2020a](#bib.bib47), [b](#bib.bib48)) and web APIs (Campagna et al., [2017](#bib.bib7);
    Su et al., [2017](#bib.bib36)). To handle large knowledge graphs, recent work
    proposed retrieving (1) information on linked entities, (2) exemplary logical
    forms relevant to the query Gu et al. ([2021](#bib.bib15)); Ye et al. ([2022](#bib.bib49)),
    and (3) schemas as context to semantic parsing Shu et al. ([2022](#bib.bib34)).
    Others use induction or iterative methods to generate complex logical forms (Cao
    et al., [2022b](#bib.bib10); Gu and Su, [2022](#bib.bib16)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，端到端的seq2seq翻译主要应用于相对较小的关系数据库模式（Yu等，[2018](#bib.bib53)；Xu等，[2020a](#bib.bib47)，[b](#bib.bib48)）和网页API（Campagna等，[2017](#bib.bib7)；Su等，[2017](#bib.bib36)）。为了处理大型知识图谱，最近的研究提出了检索（1）关于链接实体的信息，（2）与查询相关的示例逻辑形式Gu等（[2021](#bib.bib15)）；Ye等（[2022](#bib.bib49)），以及（3）作为语义解析上下文的模式Shu等（[2022](#bib.bib34)）。其他研究使用归纳或迭代方法生成复杂的逻辑形式（Cao等，[2022b](#bib.bib10)；Gu和Su，[2022](#bib.bib16)）。
- en: 1.1 Few-Shot Seq2Seq Semantic Parsing
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 少样本Seq2Seq语义解析
- en: This paper investigates how we can leverage large language models (LLMs) to
    create seq2seq neural semantic parsers for large knowledge bases such as Wikidata.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了如何利用大型语言模型（LLMs）为像Wikidata这样的庞大知识库创建seq2seq神经语义解析器。
- en: Pretrained with the internet corpora, LLMs are already familiar with the syntax
    of formal query languages such as SQL (Hu et al., [2022](#bib.bib17); Poesia et al.,
    [2022](#bib.bib28); Li et al., [2023](#bib.bib21); An et al., [2023](#bib.bib1);
    Nan et al., [2023](#bib.bib26); Arora et al., [2023](#bib.bib2)). When given simple
    SQL schemas, they can perform zero-shot semantic parsing of simple natural language
    queries into formal queries. Unlike Freebase, the KB used in most of the KBQA
    semantic parsing research, Wikidata does not have a pre-defined schema, making
    it a much harder problem. It has 150K domains, 3K applicable properties, and 107M
    entities, each of the properties and entities are uniquely identified with PIDs
    and QIDs, respectively. While zero-shot LLMs can generate SPARQL queries for the
    easiest and most common questions, they do not know all the PIDs and QIDs, and
    nor is it possible to include them in a prompt.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 经过互联网语料库的预训练，LLM 已经熟悉 SQL（Hu 等，[2022](#bib.bib17)；Poesia 等，[2022](#bib.bib28)；Li
    等，[2023](#bib.bib21)；An 等，[2023](#bib.bib1)；Nan 等，[2023](#bib.bib26)；Arora 等，[2023](#bib.bib2)）等正式查询语言的语法。当给定简单的
    SQL 架构时，它们可以将简单的自然语言查询进行零-shot 语义解析成正式查询。与 Freebase 不同，大多数 KBQA 语义解析研究中使用的 KB，Wikidata
    没有预定义的架构，使得问题更为复杂。它有 150K 领域、3K 适用属性和 107M 实体，每个属性和实体分别用 PID 和 QID 唯一标识。虽然零-shot
    LLM 能够为最简单和最常见的问题生成 SPARQL 查询，但它们并不知晓所有 PID 和 QID，也无法在提示中包含这些信息。
- en: This paper presents WikiSP, a few-shot sequence-to-sequence semantic parser
    for Wikidata that translates a user query, along with results from an entity linker,
    directly into SPARQL queries. To handle the 100M+ entities in Wikidata, we train
    the parser to use either the entity linker results or a mention in the query;
    to handle the 150K domains and 3K applicable properties, we modify SPARQL to use
    domain and property names instead of their unique QIDs and PIDs, respectively.
    We fine-tune a LLaMA (Touvron et al., [2023](#bib.bib41)) with a few-shot training
    set along with the instructions used to fine-tune Alpaca (Taori et al., [2023](#bib.bib40)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 WikiSP，一个针对 Wikidata 的少量样本序列到序列语义解析器，它将用户查询以及实体链接器的结果直接转换为 SPARQL 查询。为了处理
    Wikidata 中的 100M+ 实体，我们训练解析器使用实体链接器的结果或查询中的提及；为了处理 150K 领域和 3K 适用属性，我们修改 SPARQL
    以使用领域和属性名称，而不是它们唯一的 QID 和 PID。我们使用少量样本训练集以及用于微调 Alpaca（Taori 等，[2023](#bib.bib40)）的指令来微调
    LLaMA（Touvron 等，[2023](#bib.bib41)）。
- en: '1.2 A New Dataset: WikiWebQuestions'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 新的数据集：WikiWebQuestions
- en: Most of the widely-used high-quality benchmarks for KBQA are based on Freebase (Bollacker
    et al., [2008](#bib.bib6)) which has been shut down since 2015\. With outdated
    knowledge, it is hard to compare the results with modern LLMs such as GPT-3, since
    answers have changed over time for most of the questions. Wikidata, despite being
    the largest and most popular knowledge base nowadays, has very few datasets annotated
    with SPARQL queries; they are either extremely small (Usbeck et al., [2017](#bib.bib42))
    or synthetic (Saha et al., [2018](#bib.bib31)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数广泛使用的高质量 KBQA 基准基于 Freebase（Bollacker 等，[2008](#bib.bib6)），该平台自 2015 年起已关闭。由于知识过时，难以将结果与现代
    LLM（如 GPT-3）进行比较，因为大多数问题的答案随着时间的推移而发生变化。尽管 Wikidata 现在是最大的和最受欢迎的知识库，但注释有 SPARQL
    查询的数据集非常少；它们要么极小（Usbeck 等，[2017](#bib.bib42)），要么是合成的（Saha 等，[2018](#bib.bib31)）。
- en: We migrated the popular WebQuestionsSP (Yih et al., [2016](#bib.bib51)) benchmark
    from Freebase to Wikidata, with updated SPARQL and up-to-date answers from the
    much larger Wikidata.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将流行的 WebQuestionsSP（Yih 等，[2016](#bib.bib51)）基准从 Freebase 迁移到 Wikidata，并更新了
    SPARQL 和来自更大 Wikidata 的最新答案。
- en: 1.3 Complementing Large Language Models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 补充大型语言模型
- en: Trained on Wikipedia and all of the internet, LLMs can answer many questions
    directly. Unfortunately, the user cannot tell if the answers are correct, thus
    requiring them to fact-check every answer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在维基百科和整个互联网中训练的 LLM 能够直接回答许多问题。不幸的是，用户无法判断答案是否正确，因此需要对每个答案进行事实核查。
- en: Unlike humans, GPT-3 always sounds definitive even when they are wrong by providing
    specific and plausible facts. For example, on the question “what is the biggest
    country in Europe by population?”, GPT-3 answers “Germany”, when the answer is
    “Russia”. Or, on the question, “where does the name Melbourne come from?” GPT-3
    answers “Melbourne comes from the Latin word ‘melburnum’ meaning ‘blackburn’ or
    ‘blackbird’.”, but in reality, Melbourne is named after William Lamb, 2nd Viscount
    Melbourne. It is not possible to tell when GPT-3’s answers are wrong, and every
    answer needs to be fact-checked.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类不同的是，GPT-3 即使在错误时也总是显得很确定，通过提供具体和合理的事实。例如，对于问题“欧洲人口最多的国家是哪个？”GPT-3 回答“德国”，但答案应为“俄罗斯”。或者，对于问题“墨尔本的名字来源于哪里？”GPT-3
    回答“墨尔本来源于拉丁词‘melburnum’，意思是‘黑色的燃烧’或‘黑鸟’。”但实际上，墨尔本是以威廉·兰姆，第二代墨尔本子爵命名的。无法判断 GPT-3
    的回答是否错误，每个答案都需要进行事实核查。
- en: Semantic parsers can be used to complement LLMs as they are interpretable; their
    results are grounded in Wikidata, which we assume to be correct. It is possible
    for semantic parsers to misunderstand a query, but by providing the answer in
    the context of the query, the user can spot the error.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析器可以用来补充 LLM，因为它们是可解释的；它们的结果基于我们认为正确的 Wikidata。语义解析器可能会误解查询，但通过在查询的上下文中提供答案，用户可以发现错误。
- en: 'We propose getting the best of both worlds by answering the question with WikiSP
    if possible. Otherwise, we report GPT-3’s guesses by prefacing it with: “GPT-3
    guesses that” (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Fine-tuned LLMs
    Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata")). In this way, the user can have full confidence with the answers
    from the former, while also benefiting from the latter. It is easier for users
    to fact-check an answer than trying to find the answer.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提议在可能的情况下使用 WikiSP 来回答问题，从而兼得两全其美。否则，我们通过前缀“GPT-3 猜测是”来报告 GPT-3 的猜测（图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 微调的 LLM 更了解更多，虚假信息更少，通过 Wikidata 的少量示例序列到序列语义解析")）。这样，用户可以完全相信前者的答案，同时也能从后者中受益。对用户来说，核查一个答案比尝试找出答案要容易。
- en: 1.4 Contributions
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 贡献
- en: WikiWebQuestions, a high-quality semantic parsing dataset for Wikidata, migrated
    from the popular WebQuestions dataset for Freebase.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: WikiWebQuestions 是一个高质量的 Wikidata 语义解析数据集，迁移自 Freebase 的热门 WebQuestions 数据集。
- en: WikiSP, a few-shot Seq2Seq semantic parser by fine-tuning LLaMA with a few shot
    training set. We improve the learnability of SPARQL queries by replacing the IDs
    of properties and domains with their unique names; we tolerate errors in entity
    linking by accepting mentions in the queries as entities. We establish a first,
    strong baseline of 76% and 65% answer accuracy for the dev set and test set of
    our new WikiWebQuestions benchmark, respectively. We also demonstrate that our
    method surpasses the state of the art for QALD-7 wikidata set by 3.6% in F1 score.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: WikiSP 是一个通过用少量训练集微调 LLaMA 得到的少量示例 Seq2Seq 语义解析器。我们通过用属性和领域的唯一名称替换 ID 来提高 SPARQL
    查询的可学习性；我们通过接受查询中的提及作为实体来容忍实体链接中的错误。我们建立了新 WikiWebQuestions 基准测试的开发集和测试集的第一个强基线，分别为
    76% 和 65% 的回答准确率。我们还展示了我们的方法在 QALD-7 Wikidata 集合中 F1 分数超越了现有技术 3.6%。
- en: We improve GPT-3’s trustworthiness by first returning interpretable results
    from semantic parser and backing it up with GPT-3 guesses. WikiSP can provide
    verifiable results for WikiWebQuestions 76% of the time and improves the guesses
    by GPT-3, resulting in errors only 4% of the time (Figure [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata")).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过首先从语义解析器返回可解释的结果并用 GPT-3 的猜测进行支持来提高 GPT-3 的可信度。WikiSP 可以在 76% 的情况下为 WikiWebQuestions
    提供可验证的结果，并改善 GPT-3 的猜测，使错误率仅为 4%（图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 微调的 LLM 更了解更多，虚假信息更少，通过
    Wikidata 的少量示例序列到序列语义解析")）。
- en: 2 Related Work
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 KBQA
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 KBQA
- en: The KBQA task aims to make large knowledge bases accessible by natural language.
    One common approach is semantic parsing where a natural language query is translated
    into a formal logical form, which is then executed to retrieve an answer from
    the knowledge base. To handle large KBs, one method is to formulate SP as a multi-staged
    search problem by retrieving entities and expanding the graphs according to the
    relationships between their properties and the query Yih et al. ([2015](#bib.bib50),
    [2016](#bib.bib51)); Luo et al. ([2018](#bib.bib22)). Lan and Jiang ([2020](#bib.bib18))
    add constraints to the staged query graph generation method. Another popular method
    is to use seq2seq models obtained by fine-tuning pretrained language models. Das
    et al. ([2021](#bib.bib11)) first find other queries that contain semantically
    similar subparts, and construct a new logical form by combining the similar subparts
    of the found queries. Ye et al. ([2022](#bib.bib49)) search over the KB based
    on predefined rules to derive a set of candidate logical forms, rank them, and
    generate the final logical form. Cao et al. ([2022b](#bib.bib10)) first generate
    a “sketch” program and then fill in its arguments. Gu and Su ([2022](#bib.bib16))
    use dynamic program induction to generate query structures. Based on a user query,
    Shu et al. ([2022](#bib.bib34)) retrieve entities, example logical forms, and
    related schema. Unlike FreeBase, Wikidata does not have a fixed schema.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: KBQA任务旨在通过自然语言使大型知识库变得可访问。一种常见的方法是语义解析，其中自然语言查询被翻译成正式的逻辑形式，然后执行该逻辑形式以从知识库中检索答案。为了处理大型知识库，一种方法是将语义解析（SP）形式化为一个多阶段搜索问题，通过检索实体并根据属性和查询之间的关系扩展图谱来实现，Yih等人（[2015](#bib.bib50)，[2016](#bib.bib51)）；Luo等人（[2018](#bib.bib22)）。Lan和Jiang（[2020](#bib.bib18)）对阶段性查询图生成方法添加了约束。另一种流行的方法是使用通过微调预训练语言模型获得的seq2seq模型。Das等人（[2021](#bib.bib11)）首先找到包含语义上相似子部分的其他查询，并通过组合找到查询的相似子部分来构造新的逻辑形式。Ye等人（[2022](#bib.bib49)）基于预定义规则在知识库上进行搜索，以导出一组候选逻辑形式，对其进行排名，并生成最终逻辑形式。Cao等人（[2022b](#bib.bib10)）首先生成一个“草图”程序，然后填充其参数。Gu和Su（[2022](#bib.bib16)）使用动态程序生成查询结构。根据用户查询，Shu等人（[2022](#bib.bib34)）检索实体、示例逻辑形式和相关模式。与FreeBase不同，Wikidata没有固定的模式。
- en: Another approach to KBQA is based on graph retrieval (Dong et al., [2015](#bib.bib13);
    Miller et al., [2016](#bib.bib24); Sun et al., [2018](#bib.bib38), [2019](#bib.bib37);
    Mavromatis and Karypis, [2022](#bib.bib23); Sen et al., [2021](#bib.bib32); Vivona
    and Hassani, [2019](#bib.bib44); Verga et al., [2021](#bib.bib43)). It predicts
    the answers directly within the subgraph extracted based on the topic entity in
    the question. Yu et al. ([2023](#bib.bib52)) combine semantic parsing with retrieval
    and achieve the state-of-the-art on the WebQuestionsSP dataset (Yih et al., [2016](#bib.bib51)).
    However, retrieval-based methods cannot handle entire categories of questions,
    such as questions with no available answer and questions like “the tallest mountain”
    where no entities are mentioned by name. They have poor interpretability and do
    not support query optimization.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种KBQA方法基于图检索（Dong等人，[2015](#bib.bib13)；Miller等人，[2016](#bib.bib24)；Sun等人，[2018](#bib.bib38)，[2019](#bib.bib37)；Mavromatis和Karypis，[2022](#bib.bib23)；Sen等人，[2021](#bib.bib32)；Vivona和Hassani，[2019](#bib.bib44)；Verga等人，[2021](#bib.bib43)）。它在基于问题中的主题实体提取的子图中直接预测答案。Yu等人（[2023](#bib.bib52)）将语义解析与检索结合起来，在WebQuestionsSP数据集上实现了最先进的成果（Yih等人，[2016](#bib.bib51)）。然而，基于检索的方法无法处理整类问题，例如没有可用答案的问题以及像“最高的山”这样没有提到名称的实体的问题。它们的可解释性差，且不支持查询优化。
- en: 2.2 KBQA Benchmarks
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 KBQA基准测试
- en: Most of the early KBQA benchmarks are based on Freebase  (Berant et al., [2013](#bib.bib5);
    Yih et al., [2016](#bib.bib51); Talmor and Berant, [2018](#bib.bib39)). Recently,
    new benchmarks have been created for Wikidata  (Cao et al., [2022a](#bib.bib9);
    Saha et al., [2019](#bib.bib30)). However, these benchmarks are created using
    rule-based synthesis or paraphrases, which are easier for semantic parsers. CSQA
    collects human-written questions for single triples and constructs complex questions
    using fixed rules with very limited natural language variety  (Saha et al., [2019](#bib.bib30)).
    KQA Pro first synthesizes queries with canonical natural language and then crowdsources
    human paraphrases  (Cao et al., [2022a](#bib.bib9)). Campagna et al. ([2019](#bib.bib8))
    show that a model can achieve significantly higher accuracy over paraphrased data
    compared to real-world data even for untrained queries. Thus, we base our WikiWebQuestions
    dataset on WebQuestionsSP  (Yih et al., [2016](#bib.bib51)), where data are collected
    from real-world users using the Google Suggest API.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的KBQA基准大多基于Freebase（Berant等人，[2013](#bib.bib5)；Yih等人，[2016](#bib.bib51)；Talmor和Berant，[2018](#bib.bib39)）。最近，为Wikidata创建了新的基准（Cao等人，[2022a](#bib.bib9)；Saha等人，[2019](#bib.bib30)）。然而，这些基准是使用基于规则的合成或同义句创建的，这对语义解析器更容易。CSQA为单个三元组收集人工编写的问题，并使用固定规则构建复杂的问题，具有非常有限的自然语言多样性（Saha等人，[2019](#bib.bib30)）。KQA
    Pro首先使用规范自然语言合成查询，然后众包人工同义句（Cao等人，[2022a](#bib.bib9)）。Campagna等人 ([2019](#bib.bib8))
    表明，即使对于未经训练的查询，与真实世界数据相比，模型在同义句数据上的准确性也显著提高。因此，我们基于WebQuestionsSP（Yih等人，[2016](#bib.bib51)）构建了WikiWebQuestions数据集，其中数据来自使用Google
    Suggest API的真实用户。
- en: 2.3 LLMs for Semantic Parsing
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLMs用于语义解析
- en: Shin et al. ([2021](#bib.bib33)) show the promise of few-shot prompting LLMs
    for semantic parsing. They use constrained decoding to enforce the syntax of the
    formal language, and achieve comparable results with a smaller fine-tuned BART
    model (Lewis et al., [2020](#bib.bib19)) on datasets with small database schemas.
    Rubin et al. ([2022](#bib.bib29)) fine-tune a small retriever to obtain the most
    relevant few-shot examples to use for each input. Niu et al. ([2023](#bib.bib27))
    use a few-shot prompted Codex model to break down the natural language input to
    make the task easier for a smaller semantic parser. LLMs have also been applied
    to semantic parsing on relational databases (Hu et al., [2022](#bib.bib17); Poesia
    et al., [2022](#bib.bib28); Li et al., [2023](#bib.bib21); An et al., [2023](#bib.bib1);
    Nan et al., [2023](#bib.bib26); Arora et al., [2023](#bib.bib2)). The schemas
    used in these projects are very small when compared to Wikidata.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Shin等人 ([2021](#bib.bib33)) 展示了少量示例提示LLMs在语义解析中的潜力。他们使用约束解码来强制执行形式语言的语法，并在具有小型数据库模式的数据集上实现了与较小的微调BART模型（Lewis等人，[2020](#bib.bib19)）相当的结果。Rubin等人
    ([2022](#bib.bib29)) 微调了一个小型检索器，以获取每个输入最相关的少量示例。Niu等人 ([2023](#bib.bib27)) 使用少量示例提示的Codex模型来拆解自然语言输入，从而简化较小的语义解析器的任务。LLMs还被应用于关系数据库的语义解析（Hu等人，[2022](#bib.bib17)；Poesia等人，[2022](#bib.bib28)；Li等人，[2023](#bib.bib21)；An等人，[2023](#bib.bib1)；Nan等人，[2023](#bib.bib26)；Arora等人，[2023](#bib.bib2)）。与Wikidata相比，这些项目中使用的模式非常小。
- en: 2.4 Entity Linking
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 实体链接
- en: Entity linking involves finding the named entities in a query, and linking them
    to the corresponding entities in the knowledge graph so that the query can be
    executed using the proper entities as reference points. The current state-of-the-art
    entity linking model on the WebQuestionsSP dataset is ReFinED (Ayoola et al.,
    [2022](#bib.bib3)). They use a bidirectional transformer on the query to predict
    the most likely mentions of named entities within a query, and then combine that
    information with embeddings computed over every entity in the knowledge base to
    predict which entity the mention is most likely to be referring to. Prior to ReFinED,
    the state-of-the-art was ELQ (Li et al., [2020](#bib.bib20)). They similarly generate
    embeddings for each entity in the knowledge base, and then use the predicted mentions
    of entities combined with these predicted embeddings to generate likely entities.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接涉及在查询中找到命名实体，并将它们链接到知识图谱中的相应实体，以便使用正确的实体作为参考点来执行查询。当前在WebQuestionsSP数据集上的最先进实体链接模型是ReFinED（Ayoola等人，[2022](#bib.bib3)）。他们在查询上使用双向变换器来预测查询中最可能提到的命名实体，然后将这些信息与知识库中每个实体计算的嵌入结合起来，预测该提及最可能指代哪个实体。在ReFinED之前，最先进的模型是ELQ（Li等人，[2020](#bib.bib20)）。他们类似地为知识库中的每个实体生成嵌入，然后使用这些预测的嵌入和预测的实体提及生成可能的实体。
- en: 3 Semantic Parsing for Wikidata
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 对Wikidata的语义解析
- en: Wikidata is the largest public knowledge base with over 12 billion facts represented
    by subject-predicate-object triples using 100+ million entities and 10,000 properties.
    3,000 of the properties are useful for answering natural language questions, whereas
    the rest are used to link data in Wikidata with external library catalogs and
    database IDs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Wikidata是最大的公共知识库，拥有超过120亿个事实，这些事实通过主题-谓词-宾语三元组表示，使用超过1亿个实体和10,000个属性。其中3,000个属性对回答自然语言问题很有用，而其余的用于将Wikidata中的数据与外部图书馆目录和数据库ID进行链接。
- en: Entities and properties are given unique identifiers, QIDs and PIDs, respectively.
    For example, the fact that Joe Biden is the president of the US can be represented
    as a triple (Q6279, P39, Q11696), where P39 is the PID for property position held,
    Q6279 and Q11696 are QIDs for Joe Biden and the president of the United States,
    respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实体和属性分别被赋予唯一标识符QIDs和PIDs。例如，乔·拜登是美国总统这一事实可以表示为三元组（Q6279, P39, Q11696），其中P39是属性“职位”的PID，Q6279和Q11696分别是乔·拜登和美国总统的QID。
- en: 3.1 Query Format
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 查询格式
- en: Unlike relational databases and Freebase, Wikidata has no predefined domains
    or types. Any entity can have an arbitrary set of properties. However, even though
    Wikidata is property-based, all named entities have one or more instance of properties
    to some domain entity; domain entities are organized into a hierarchy with the
    subclass of property.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与关系数据库和Freebase不同，Wikidata没有预定义的领域或类型。任何实体都可以拥有任意数量的属性。然而，尽管Wikidata是基于属性的，但所有命名实体都具有一个或多个属性实例，并与某个领域实体相关联；领域实体被组织成一个层次结构，并且具有属性的子类。
- en: Note that the names of domain entities and properties are unique. Non-domain
    entities, on the other hand, can be ambiguous. For example, “Lincoln” can refer
    to the president, a car brand, a sparrow, an aircraft, and many different cities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，领域实体和属性的名称是唯一的。另一方面，非领域实体可能会产生歧义。例如，“Lincoln”可以指总统、汽车品牌、麻雀、飞机以及许多不同的城市。
- en: We posit that it is impossible for LLMs to memorize the QIDs and PIDs for domains
    and properties. We modify the format of SPARQL queries to use the more mnemonic
    property name, instead of its PID. Similarly, we use entity names for domains.
    For example, the original SPARQL for the query “What car models does GM make?”
    is
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为LLMs不可能记住领域和属性的QIDs和PIDs。我们修改了SPARQL查询的格式，以使用更易记的属性名称，而不是其PID。同样，我们对领域使用实体名称。例如，查询“What
    car models does GM make?”的原始SPARQL是
- en: SELECT DISTINCT ?x WHERE {
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SELECT DISTINCT ?x WHERE {
- en: ?x wdt:P31/wdt:P279* wd:Q3231690\.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?x wdt:P31/wdt:P279* wd:Q3231690\.
- en: ?x wdt:P176 wd:Q81965\. }
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?x wdt:P176 wd:Q81965\. }
- en: This says that we are seeking $x$ has General Motors (wd:Q81965) as the manufacturer
    (wdt:P176). Note wdt is the prefix for Wikidata property, and wd is for Wikidata
    entity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们正在寻找制造商为通用汽车（wd:Q81965）的$x$（wdt:P176）。注意，wdt是Wikidata属性的前缀，wd是Wikidata实体的前缀。
- en: 'With our modification, the query becomes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的修改，查询变为：
- en: SELECT DISTINCT ?x WHERE {
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SELECT DISTINCT ?x WHERE {
- en: ?x wdt:instance_of/wdt:subclass_of*
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?x wdt:instance_of/wdt:subclass_of*
- en: wd:automobile_model.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wd:automobile_model.
- en: ?x wdt:manufacturer wd:Q81965\. }
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?x wdt:manufacturer wd:Q81965\. }
- en: For non-domain entity QIDs, we also accept a string in lieu of a QID in case
    of entity linking errors. At inference time, we use simple heuristics to resolve
    the string to a QID before applying the query. For example, “wd:Q81965” in the
    query may be replaced with “wd:GM”. See Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Recovering
    from Entity Linker Errors ‣ 3.2 Entity Linking ‣ 3 Semantic Parsing for Wikidata
    ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata") for more details.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非领域实体QIDs，我们也接受字符串代替QID，以防实体链接错误。在推理时，我们使用简单的启发式方法将字符串解析为QID，然后再应用查询。例如，查询中的“wd:Q81965”可能会被替换为“wd:GM”。有关更多细节，请参见第[3.2.2](#S3.SS2.SSS2
    "3.2.2 Recovering from Entity Linker Errors ‣ 3.2 Entity Linking ‣ 3 Semantic
    Parsing for Wikidata ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot
    Sequence-to-Sequence Semantic Parsing over Wikidata")节。
- en: Normally, we refrain from changing standard query notations since LLMs have
    been pretrained on them. However, we posit that learning this new syntax is much
    easier than learning the PIDs and QIDs. Our experimentation with few-shot prompting
    suggests that LLMs can easily adjust to this format.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们避免更改标准查询符号，因为大型语言模型（LLM）已经在这些符号上进行了预训练。然而，我们认为学习这种新语法比学习PIDs和QIDs要容易得多。我们在几次示例提示实验中的发现表明，LLMs可以轻松适应这种格式。
- en: 3.2 Entity Linking
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实体链接
- en: Linking entities for WikiWebQuestions is particularly difficult. First, since
    the dataset is collected from real-world questions without prompting the users
    for more information, users tend to refer to their entities of interest without
    using their full names. Second, the questions are generally short with very limited
    context, making it harder to disambiguate among entities with similar names. Lastly,
    many QIDs in Wikidata are used to represent terms not generally known as “named
    entities”. For example, domain entities are often ignored by entity linker models,
    as in “What is the biggest country in Europe by population?”, both “country” (Q6256)
    and “Europe” (Q46) are required to construct the correct SPARQL, but entity linkers
    only provide “Europe” and ignore “country”.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为 WikiWebQuestions 进行实体链接尤其困难。首先，由于数据集是从现实世界的问题中收集的，没有提示用户提供更多信息，用户往往在不使用完整名称的情况下引用他们感兴趣的实体。其次，问题通常很简短，上下文非常有限，使得在具有相似名称的实体之间进行歧义消解变得更加困难。最后，Wikidata
    中的许多 QID 用于表示通常不被认为是“命名实体”的术语。例如，领域实体通常被实体链接器模型忽略，如在“欧洲哪个国家人口最多？”中，“国家” (Q6256)
    和 “欧洲” (Q46) 都需要构造正确的 SPARQL，但实体链接器只提供“欧洲”并忽略“国家”。
- en: 3.2.1 Semantic Parsing with Entity Linking
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 使用实体链接的语义解析
- en: To handle ambiguous entities, we use an entity linker to first find the domain
    names and QIDs of the entities mentioned in the text. We train a semantic parser
    that accepts users’ input along with the results produced by the entity linker.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理模糊的实体，我们使用实体链接器首先找到文本中提到的实体的领域名称和 QID。我们训练一个语义解析器，接受用户输入以及实体链接器生成的结果。
- en: Formally, given a user input $T$ in our modified SPARQL format.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，给定用户输入 $T$，以我们修改后的 SPARQL 格式。
- en: For the example above, the SOTA ReFinED entity linker (Ayoola et al., [2022](#bib.bib3))
    returns $\{\langle$. Unfortunately, it misses the entity automobile model (Q3231690),
    a term not usually considered to be an entity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述示例，SOTA ReFinED 实体链接器 (Ayoola 等，[2022](#bib.bib3)) 返回 $\{\langle$。不幸的是，它遗漏了实体汽车型号
    (Q3231690)，这个术语通常不被认为是一个实体。
- en: 3.2.2 Recovering from Entity Linker Errors
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 从实体链接器错误中恢复
- en: We want our semantic parser to be able to recover from mistakes by an entity
    linker. That is, the semantic parser should use entity linking when it is helpful,
    but it should still try to predict the right logical form when the linker fails.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的语义解析器能够从实体链接器的错误中恢复。也就是说，语义解析器应在有帮助时使用实体链接，但在链接器失败时仍应尝试预测正确的逻辑形式。
- en: The semantic parser is trained to accept, along with the user query, an optional
    set of potentially useful QIDs from the entity linker. We include samples where
    some of the supplied linked entities are not used in the gold answer, as well
    as samples where there are missing linked entities. For the latter, we use mentions
    in the original query in lieu of the QIDs. At inference time, we use the mentions
    to look up the QIDs in Wikidata. If multiple matches exist, the most popular entity
    is returned. An example is shown in Appendix [A](#A1 "Appendix A Examples of Recovering
    from Entity Linking Errors ‣ Fine-tuned LLMs Know More, Hallucinate Less with
    Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata").
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析器被训练接受来自实体链接器的、可能有用的 QID 的可选集合。我们包括一些示例，其中一些提供的链接实体没有用于黄金答案，以及一些示例，其中缺少链接实体。对于后者，我们使用原始查询中的提及代替
    QID。在推理时，我们使用这些提及来查找 Wikidata 中的 QID。如果存在多个匹配项，则返回最受欢迎的实体。示例见附录 [A](#A1 "附录 A
    从实体链接错误中恢复的示例 ‣ 微调的 LLM 知道更多，幻觉更少，使用少量样本的 Wikidata 语义解析")。
- en: With the above example where the entity linker misses “automobile model”, the
    semantic parser is likely to predict “car model” by copying from the user query.
    We search “automobile model” among aliases in domains to find the correct QID.
    This design allows the model to potentially recover from entity-linking failures.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，实体链接器遗漏了“汽车型号”，语义解析器很可能会通过从用户查询中复制来预测“车型号”。我们在领域的别名中搜索“汽车型号”以找到正确的QID。这一设计使得模型有可能从实体链接失败中恢复。
- en: 4 WikiWebQuestions (WWQ) Dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 WikiWebQuestions (WWQ) 数据集
- en: Despite being the most popular large knowledge base for a long time, existing
    benchmarks on Wikidata with labeled SPARQL queries are unfortunately either small
    or of low quality. On the other hand, benchmarks over the deprecated Freebase
    still dominate the KBQA research with better-quality data. For example, the WebQuestions (Yih
    et al., [2015](#bib.bib50)) dataset was collected by using Google Search API instead
    of human paraphrasing or synthesis. As a result, it is much more natural and truly
    reflects the real-world questions users may ask. This dataset is later annotated
    with SPARQL over Freebase, named WebQuestionsSP (Yih et al., [2016](#bib.bib51)).
    Examples with no legitimate SPARQL to retrieve answers from Freebase are dropped.
    In total, WebQuestionsSP consists of 3098 examples in the training set and 1639
    in the test set.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管长期以来Wikidata是最受欢迎的大型知识库，但现有的标注SPARQL查询的Wikidata基准不幸地要么规模较小，要么质量较低。另一方面，基于已弃用的Freebase的基准仍然以更高质量的数据主导KBQA研究。例如，WebQuestions（Yih等，[2015](#bib.bib50)）数据集是使用Google
    Search API收集的，而不是通过人工释义或合成。因此，它更加自然，真正反映了用户可能提出的实际问题。该数据集后来用Freebase上的SPARQL进行了标注，命名为WebQuestionsSP（Yih等，[2016](#bib.bib51)）。从Freebase中没有合法SPARQL来检索答案的示例被删除。总的来说，WebQuestionsSP包含3098个训练集示例和1639个测试集示例。
- en: We migrated WebQuestionsSP, the best collection of natural language questions
    over a general knowledge graph, from Freebase to Wikidata, with the help of an
    automatic tool we developed, based on Google’s entity mapping²²2[https://developers.google.com/freebase](https://developers.google.com/freebase)
    and Wikidata’s relation mapping³³3[https://www.wikidata.org/wiki/Wikidata:WikiProject_Freebase/Mapping](https://www.wikidata.org/wiki/Wikidata:WikiProject_Freebase/Mapping).
    About 60% of the dataset was automatically converted. One of the authors of this
    paper, who did not participate in model tuning, manually converted those instances
    that failed to convert automatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将WebQuestionsSP，从Freebase迁移到Wikidata，这是一套最佳的自然语言问题集合，覆盖了一个通用知识图谱，借助我们开发的自动工具，基于Google的实体映射²²2[https://developers.google.com/freebase](https://developers.google.com/freebase)和Wikidata的关系映射³³3[https://www.wikidata.org/wiki/Wikidata:WikiProject_Freebase/Mapping](https://www.wikidata.org/wiki/Wikidata:WikiProject_Freebase/Mapping)。大约60%的数据集被自动转换。本文的一位作者未参与模型调整，手动转换了那些未能自动转换的实例。
- en: 4.1 Migrating WebQuestionsSP to Wikidata
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 将WebQuestionsSP迁移到Wikidata
- en: Here are the major decisions we made in migrating WebQuestionsSP dataset to
    Wikidata. While much bigger, Wikidata does not necessarily contain all the information
    available in Freebase. For example, it lacks countries’ trade partners, hence
    we drop all such questions from the WebQuestionsSP dataset.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们在将WebQuestionsSP数据集迁移到Wikidata时做出的主要决定。尽管Wikidata规模更大，但并不一定包含Freebase中的所有信息。例如，它缺乏国家的贸易伙伴，因此我们从WebQuestionsSP数据集中删除了所有这类问题。
- en: If multiple paths can lead to the correct answer, we choose the path that provides
    the most complete answers and has the best availability among entities in the
    same domain. For example, when asking for books written by an author X, we can
    either search for books whose author is X or find notable works of X that are
    books. While the latter is more efficient, the property notable works is not always
    available for all authors and it often does not provide a complete list. Thus,
    we annotate such examples using the former representation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多条路径可以得到正确答案，我们选择提供最完整答案且在同一领域实体中可用性最佳的路径。例如，当询问作者X写的书籍时，我们可以选择搜索作者为X的书籍，或找到X的著名著作中属于书籍的部分。虽然后者更高效，但著名著作属性并不是所有作者都有，并且通常不能提供完整的列表。因此，我们使用前一种表示法对这些示例进行标注。
- en: We also cleaned up the original dataset. The dataset contained questions like
    “who does Ronaldinho play for now in 2011?”. We drop the appended year as it conflicts
    with “now” in the utterance, and it would refer to the live information in Wikidata.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还清理了原始数据集。数据集中包含了像“罗纳尔迪尼奥现在（2011年）效力于哪个俱乐部？”这样的问题。我们去掉了附加的年份，因为它与语句中的“现在”冲突，并且它会引用Wikidata中的实时信息。
- en: In total, we dropped 9% of the examples from WebQuestionsSP and created a training,
    dev, and test set of 2431, 454, and 1431 samples, respectively. Given that Wikidata
    has 100 million entities and 3,000 useful properties for answering questions,
    the training data set is woefully inadequate and can be considered as a “fewshot”
    training set at best.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们从WebQuestionsSP中删除了9%的示例，并创建了分别为2431、454和1431个样本的训练集、开发集和测试集。鉴于Wikidata有1亿个实体和3000个用于回答问题的有用属性，训练数据集远远不够，至多可以算作是一个“少量样本”训练集。
- en: 5 Implementation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实施
- en: This section discusses the implementation details of the entity linker and the
    WikiSP semantic parser.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了实体链接器和WikiSP语义解析器的实施细节。
- en: 5.1 Entity Linking
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实体链接
- en: We use ReFinED  (Ayoola et al., [2022](#bib.bib3)) for entity linking, which
    is the current state of the art for WebQuestionsSP. As discussed before, Wikidata
    treats many common terms such as “country” as named entities and assigns them
    QIDs. To fine-tune ReFinED to learn such terms, we add the question and entity
    pairs from the training set of WikiWebQuestions to the data used to train ReFinED’s
    questions model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用ReFinED (Ayoola et al., [2022](#bib.bib3))进行实体链接，这是WebQuestionsSP的当前最先进技术。如前所述，Wikidata将许多常见术语（如“国家”）视为命名实体，并分配QID。为了微调ReFinED以学习这些术语，我们将WikiWebQuestions训练集中的问题和实体对添加到用于训练ReFinED问题模型的数据中。
- en: We run 10 epochs of finetuning using the default hyperparameters suggested by
    Ayoola et al. ([2022](#bib.bib3)). For each identified entity, we provide the
    mention in the original utterance, the QID, as well as its domain in plain text.
    The information is appended to the utterance before being fed into the neural
    semantic parsing model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Ayoola et al. ([2022](#bib.bib3))建议的默认超参数进行了10个周期的微调。对于每个识别出的实体，我们提供原始发言中的提及、QID以及其领域的纯文本信息。这些信息会在送入神经语义解析模型之前附加到发言中。
- en: 5.2 The WikiSP Semantic Parser
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 WikiSP语义解析器
- en: We prepare the training data with entities provided by fine-tuned ReFinED. Comparing
    with the gold entities, ReFinED provides extra entities in 215 cases, while missing
    at least one entity in 137 cases. When ReFinED failed to produce the correct entities,
    we replace the missing QIDs in the logical form with the corresponding mention
    of the entity in the question. During evaluation, if a mention of an entity is
    predicted by the model, we look up the QID using the Wikidata “wbsearchentities”
    API ⁴⁴4[https://www.wikidata.org/w/api.php?action=wbsearchentities](https://www.wikidata.org/w/api.php?action=wbsearchentities).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用由微调的ReFinED提供的实体来准备训练数据。与黄金实体相比，ReFinED在215个案例中提供了额外的实体，但在137个案例中至少遗漏了一个实体。当ReFinED未能生成正确的实体时，我们会在逻辑形式中用问题中的相应实体提及来替换缺失的QID。在评估过程中，如果模型预测了实体的提及，我们会使用Wikidata
    “wbsearchentities” API ⁴⁴4[https://www.wikidata.org/w/api.php?action=wbsearchentities](https://www.wikidata.org/w/api.php?action=wbsearchentities)来查找QID。
- en: 'We fine-tune LLaMA with 7B parameters because it has been shown to perform
    well despite its relatively small size Touvron et al. ([2023](#bib.bib41)). We
    include the Alpaca (Taori et al., [2023](#bib.bib40)) instruction following data,
    which was derived using the self-instruct (Wang et al., [2023](#bib.bib45)) method,
    in our training data. The training data samples in WikiWebQuestion start with
    the following instruction: “Given a Wikidata query with resolved entities, generate
    the corresponding SPARQL. Use property names instead of PIDs.”. We concatenate
    the resolved entities and the user utterance together as input. We up-sample the
    WikiWebQuestion fewshot set 5 times and train for 3 epochs using 2e-5 learning
    rate and 0.03 warmup ratio.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用7B参数的LLaMA进行微调，因为尽管其规模相对较小，但已经显示出表现良好 Touvron et al. ([2023](#bib.bib41))。我们将Alpaca
    (Taori et al., [2023](#bib.bib40))指令跟随数据（该数据是使用self-instruct (Wang et al., [2023](#bib.bib45))方法得出的）包括在我们的训练数据中。WikiWebQuestion中的训练数据样本以以下指令开始：“给定一个已解析实体的Wikidata查询，生成相应的SPARQL。使用属性名称代替PID。”我们将解析后的实体和用户发言一起作为输入。我们将WikiWebQuestion的少样本集上采样5倍，并使用2e-5学习率和0.03的预热比例训练3个周期。
- en: 5.3 Executing Queries on Wikidata
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 在Wikidata上执行查询
- en: SPARQL queries are used to retrieve answers from the Wikidata SPARQL endpoint⁵⁵5[https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service).
    Since Wikidata is actively being updated, the gold SPARQL can be easily re-executed
    to acquire up-to-date answers, allowing the benchmark to compare with forthcoming
    iterations of large language models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SPARQL查询用于从Wikidata SPARQL端点⁵⁵5[https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service)中检索答案。由于Wikidata正在积极更新，黄金SPARQL可以很容易地重新执行以获取最新答案，从而使基准可以与即将发布的大型语言模型的迭代版本进行比较。
- en: '|  | EM | F1 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | EM | F1 |'
- en: '| WikiSP (ours) | 65.5 | 71.9 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| WikiSP（我们的方法） | 65.5 | 71.9 |'
- en: 'Table 1: Results of WikiSP on the WWQ test set.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：WikiSP在WWQ测试集上的结果。
- en: 6 Experiments
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: In this section, we evaluate WikiSP on WikiWebQuestions and demonstrate how
    it can be used to complement large language models such as GPT-3.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估WikiSP在WikiWebQuestions上的表现，并演示如何将其用于补充大型语言模型，如GPT-3。
- en: 6.1 Semantic Parser Results
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 语义解析器结果
- en: 'We evaluate our model with two different answer accuracy metrics: (1) exact
    match (EM): the percentage of examples where the answers of the predicted SPARQL
    exactly match the gold answers, and (2) Macro F1 score (F1): the average F1 score
    for answers of each example. The evaluation results are shown in Table [1](#S5.T1
    "Table 1 ‣ 5.3 Executing Queries on Wikidata ‣ 5 Implementation ‣ Fine-tuned LLMs
    Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata"). Our approach achieves a 65.5% exact match accuracy and a 71.9%
    F1 score on the WWQ dataset.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个不同的答案准确度指标来评估我们的模型：（1）精确匹配（EM）：预测的 SPARQL 答案与金标准答案完全匹配的示例百分比；（2）宏观 F1
    分数（F1）：每个示例答案的平均 F1 分数。评估结果如表 [1](#S5.T1 "表 1 ‣ 5.3 在 Wikidata 上执行查询 ‣ 5 实现 ‣
    微调的 LLM 知道更多，错觉更少，通过少量示例的序列到序列语义解析") 所示。我们的方法在 WWQ 数据集上实现了 65.5% 的精确匹配准确度和 71.9%
    的 F1 分数。
- en: As a reference, the current state-of-the-art result on the original WebQuestionsSP
    dataset for Freebase is 78.8% F1 (Yu et al., [2023](#bib.bib52)). The result was
    obtained with a combination of semantic parsing and retrieval. The WikiWebQuestions
    dataset is slightly different, as discussed above. More significantly, unlike
    Freebase, Wikidata does not have a fixed schema and ours is an end-to-end, seq2seq
    semantic parser.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，Freebase 在原始 WebQuestionsSP 数据集上的当前最先进结果为 78.8% F1（Yu 等，[2023](#bib.bib52)）。这一结果是通过语义解析和检索的组合获得的。WikiWebQuestions
    数据集稍有不同，如上所述。更重要的是，与 Freebase 不同，Wikidata 没有固定的模式，而我们的模型是端到端的 seq2seq 语义解析器。
- en: 6.2 Ablation Experiments
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 消融实验
- en: 6.2.1 Entity Linking
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 实体链接
- en: Our first ablation study evaluates the need for entity linking with ReFinED,
    by replacing it with simply using the LLM to detect entities as mentions. In this
    experiment, all entity IDs in the training data are replaced by their mentions;
    during inference, we map the predicted entities to their actual QIDs according
    to Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Recovering from Entity Linker Errors ‣
    3.2 Entity Linking ‣ 3 Semantic Parsing for Wikidata ‣ Fine-tuned LLMs Know More,
    Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata").
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个消融研究评估了使用 ReFinED 进行实体链接的必要性，通过将其替换为仅使用 LLM 检测提及。在这个实验中，训练数据中的所有实体 ID
    都被其提及替换；在推理过程中，我们根据第 [3.2.2](#S3.SS2.SSS2 "3.2.2 从实体链接器错误中恢复 ‣ 3.2 实体链接 ‣ 3 语义解析
    ‣ 微调的 LLM 知道更多，错觉更少，通过少量示例的序列到序列语义解析") 节映射预测的实体到其实际的 QID。
- en: The results show that replacing the neural entity linker with just using mentions
    reduces the exact match by 9.1% and the F1 score by 9.3%. This suggests that entity
    linking is important.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，用提及替换神经实体链接器会使精确匹配减少 9.1%，F1 分数减少 9.3%。这表明实体链接是重要的。
- en: 6.2.2 Allowing Mentions as Entities
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 允许提及作为实体
- en: 'Our logical form is designed to recover from entity linking errors by allowing
    entities be specified by a mention, as an alternative to a QID. Our ablation study
    on this feature tested two training strategies:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逻辑形式设计用于通过允许用提及代替 QID 来恢复实体链接错误。我们对这一特性的消融研究测试了两种训练策略：
- en: '|  | EM | F1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | EM | F1 |'
- en: '| WikiSP (ours) | 75.6 | 76.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| WikiSP（我们的） | 75.6 | 76.9 |'
- en: '| No Entity Linking | 66.5 | 67.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 无实体链接 | 66.5 | 67.6 |'
- en: '| No mentions, trained with ReFinED | 73.3 | 75.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 无提及，使用 ReFinED 训练 | 73.3 | 75.0 |'
- en: '| No mentions, trained with Oracle entities | 72.2 | 73.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 无提及，使用 Oracle 实体训练 | 72.2 | 73.4 |'
- en: '| PIDs and QIDs for properties & domains | 73.6 | 74.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 属性和领域的 PIDs 和 QIDs | 73.6 | 74.7 |'
- en: 'Table 2: Ablation results of WikiSP on the WWQ dev set.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：WikiSP 在 WWQ 开发集上的消融实验结果。
- en: ReFinED. The entity linker tuples are produced by fine-tuned ReFinED, which
    may be missing entities in the gold target. The data show that generating unseen
    QIDs is needed for missing entities.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ReFinED。实体链接器元组是由微调的 ReFinED 生成的，可能缺少金标准中的实体。数据表明，生成未见的 QID 对于缺失的实体是必要的。
- en: Oracle. The entity linker tuples are exactly all the entities used in the gold.
    The model would only encounter missing QIDs at test time when ReFinED fails to
    generate all the necessary QIDs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle。实体链接器生成的元组正是金标准中使用的所有实体。当 ReFinED 无法生成所有必要的 QID 时，模型只会在测试时遇到缺失的 QID。
- en: The answer accuracy of the model using entity linked tuples from ReFinED (“No
    mentions, trained with ReFinED” in Table [2](#S6.T2 "Table 2 ‣ 6.2.2 Allowing
    Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6 Experiments ‣ Fine-tuned LLMs
    Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata")) lags by 2.3% when compared against our best model. The model
    using Oracle (“No mentions, trained with Oracle entities” in Table [2](#S6.T2
    "Table 2 ‣ 6.2.2 Allowing Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6
    Experiments ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata")) lags by 3.4%. These results indicate that allowing
    mentions is useful for recovering from entity linking errors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自 ReFinED 的实体链接元组的模型（“无提及，使用 ReFinED 训练”在表 [2](#S6.T2 "Table 2 ‣ 6.2.2 Allowing
    Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6 Experiments ‣ Fine-tuned LLMs
    Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata")）与我们最佳模型相比，准确率低了 2.3%。使用 Oracle 的模型（“无提及，使用 Oracle 实体训练”在表 [2](#S6.T2
    "Table 2 ‣ 6.2.2 Allowing Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6
    Experiments ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata")）低了 3.4%。这些结果表明，允许提及对于从实体链接错误中恢复是有用的。
- en: 6.2.3 Names vs. IDs for Properties & Domains
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 属性与领域的名称与 ID
- en: Our logical form replaces PIDs with property names, and domain-entity QIDs with
    the domain names. Here we evaluate the effectiveness of this query format. We
    compare our approach with the original SPARQL where all properties and entities
    are represented with PIDs and QIDs. Our ablation study shows that our representation
    with property names and domain names improves the answer accuracy by 2.0% (Table [2](#S6.T2
    "Table 2 ‣ 6.2.2 Allowing Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6
    Experiments ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata")). This shows that LLMs can adapt to changes in
    query notation with fine-tuning, and it is easier to learn names than remembering
    random IDs. If we did not allow mentions in the predicted logical form, the replacement
    of QIDs with their names is likely to be more significant.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逻辑形式用属性名称替换 PIDs，用领域名称替换领域实体 QIDs。在这里，我们评估这种查询格式的有效性。我们将我们的方法与原始 SPARQL 进行比较，在原始
    SPARQL 中所有属性和实体都用 PIDs 和 QIDs 表示。我们的消融研究表明，使用属性名称和领域名称的表示使答案准确率提高了 2.0%（表 [2](#S6.T2
    "Table 2 ‣ 6.2.2 Allowing Mentions as Entities ‣ 6.2 Ablation Experiments ‣ 6
    Experiments ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata")）。这表明，LLMs 可以通过微调适应查询符号的变化，并且学习名称比记住随机 ID 更容易。如果我们没有在预测的逻辑形式中允许提及，QIDs
    替换为名称的效果可能会更显著。
- en: 6.3 Complementing GPT-3
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 补充 GPT-3
- en: LLMs like GPT-3 can answer many questions on general knowledge correctly; however,
    they may also hallucinate. WWQ is representative of popular questions, so we expect
    GPT-3 to perform well. We use text-davinci-002 with the temperature set to 0 to
    evaluate GPT-3’s performance on WWQ.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3 这样的语言模型可以正确回答许多关于一般知识的问题；然而，它们也可能产生幻觉。WWQ 是流行问题的代表，因此我们期望 GPT-3 能表现良好。我们使用
    `text-davinci-002`，将温度设置为 0 来评估 GPT-3 在 WWQ 上的表现。
- en: On the dev set of WWQ, GPT-3 answers 66.4% of the questions correctly and provides
    incomplete answers to 26.5% of the questions. For example, when asked “What does
    Obama have a degree in?”, GPT-3 correctly identifies President Obama’s political
    science degree, but fails to mention his law degree. In total, GPT-3 gives wrong
    answers to 7.1% of the questions.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 WWQ 的开发集上，GPT-3 正确回答了 66.4% 的问题，并对 26.5% 的问题提供了不完整的答案。例如，当被问到“奥巴马拥有什么学位？”时，GPT-3
    正确识别了奥巴马总统的政治学学位，但未提及他的法学学位。总的来说，GPT-3 对 7.1% 的问题给出了错误的答案。
- en: For this dev set, we can give definitive answers to 75.6% of the questions with
    WikiSP (Table [2](#S6.T2 "Table 2 ‣ 6.2.2 Allowing Mentions as Entities ‣ 6.2
    Ablation Experiments ‣ 6 Experiments ‣ Fine-tuned LLMs Know More, Hallucinate
    Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata")). For
    the rest of the questions (24.4%), accounting for the overlap between the GPT-3
    and our semantic parser’s results, the percentages of guessing correctly, incompletely,
    and incorrectly are at 15.2%, 5.5%, and 3.7%, respectively (Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Fine-tuned LLMs Know More, Hallucinate Less with
    Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata")).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个开发集，我们可以使用WikiSP对75.6%的问题给出明确的答案（表[2](#S6.T2 "表 2 ‣ 6.2.2 允许提及作为实体 ‣ 6.2
    消融实验 ‣ 6 实验 ‣ 微调LLMs了解更多，幻觉更少，通过少量示例的序列到序列语义解析在Wikidata上")）。对于其余的问题（24.4%），考虑到GPT-3和我们的语义解析器结果之间的重叠，正确、部分正确和错误的百分比分别为15.2%、5.5%和3.7%（图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 微调LLMs了解更多，幻觉更少，通过少量示例的序列到序列语义解析在Wikidata上")）。
- en: In summary, the combination of GPT-3 and WikiSP makes it possible to give a
    definitive, correct, and complete answer three quarters of the time for the dev
    set. Users can also benefit from GPT-3’s guesses the rest of the time at a 3.7%
    error rate, which is about half of the original error rate.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，GPT-3和WikiSP的结合使得在开发集中，有三分之二的时间可以提供明确、正确和完整的答案。用户还可以在其余时间从GPT-3的猜测中受益，错误率为3.7%，约为原始错误率的一半。
- en: 6.4 Error Analysis
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 错误分析
- en: We analyzed the 111 examples in the WWQ dev set where the model failed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了WWQ开发集中的111个模型失败的例子。
- en: 6.4.1 Acceptable Alternative Results (18.0%)
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 可接受的替代结果（18.0%）
- en: Our analysis shows that 18.0% of the “errors” can actually be deemed to be correct.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析显示，18.0%的“错误”实际上可以被认为是正确的。
- en: Reasonable alternate answers (11.7%). In 11.7% of the cases, the model predicts
    an alternative interpretation to the question and returns a reasonable answer
    that is different from the gold. For example, the gold for question “what did
    Boudicca do?” uses the position held property, while the model predicts occupation
    property. Both are considered valid answers to the question.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的替代答案（11.7%）。在11.7%的情况下，模型预测了问题的另一种解释，并返回了与黄金标准不同的合理答案。例如，问题“Boudicca做了什么？”的黄金标准使用了职位属性，而模型预测了职业属性。这两种答案都被认为是有效的。
- en: Reasonable alternative SPARQL but no answer was retrieved (6.3%). In another
    6.3% of cases, the model predicts a reasonable alternative SPARQL, but the SPARQL
    returns no answer. Sometimes, since the information for the “correct” property
    is missing, the question is represented with a similar property. For example,
    since residence property is missing for Patrick Henry, the gold SPARQL for “where
    did Patrick Henry live?” uses place of birth instead, while our model predicts
    residence.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 合理的替代SPARQL但未检索到答案（6.3%）。在另一个6.3%的情况下，模型预测了合理的替代SPARQL，但SPARQL没有返回答案。有时，由于“正确”属性的信息缺失，问题被表示为类似的属性。例如，由于Patrick
    Henry的居住地属性缺失，黄金SPARQL“Patrick Henry住在哪里？”使用了出生地属性，而我们的模型预测了居住地。
- en: 6.4.2 Errors in Entity Linking (35.1%)
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 实体链接中的错误（35.1%）
- en: The biggest source of errors is entity linking. Entity linker failed to provide
    the correct entities in 35.1% of the failed examples. While WikiSP can potentially
    recover from missing entities, it cannot recover from incorrect entities. This
    is especially common for character roles, as some character roles have different
    entities for books and movies or even different series of movies. Sometimes WikiSP
    located the correct mention from the question, but the lookup failed. For example,
    the model located the mention of the event “allied invasion of France” in question
    “where did the allied invasion of France take place?”, but failed to find the
    corresponding entity from Wikidata by the name.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 错误的最大来源是实体链接。实体链接器在35.1%的失败例子中未能提供正确的实体。虽然WikiSP可以从缺失的实体中恢复，但它不能从不正确的实体中恢复。这在角色扮演中尤其常见，因为一些角色扮演在书籍和电影中或甚至不同的电影系列中有不同的实体。有时WikiSP从问题中找到了正确的提及，但查找失败。例如，模型在问题“盟军入侵法国发生在哪里？”中找到了“盟军入侵法国”的提及，但未能通过名称在Wikidata中找到相应的实体。
- en: 6.4.3 Errors Beyond Entity Linking
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3 超越实体链接的错误
- en: 'Semantic parsing in Wikidata is challenging as there are no predefined schemas,
    and there are 150K domains and 3K applicable properties. Some representative mistakes
    include the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Wikidata中的语义解析具有挑战性，因为没有预定义的模式，并且有150K个领域和3K个适用属性。一些代表性的错误包括：
- en: Wrong property (17.1%). 17.1% of the errors are caused by predicting the wrong
    property. Some of the examples require background knowledge to parse. For example
    the answer of the question “what did martin luther king jr do in his life?” should
    return the value of movement, while the model predicts occupation. Properties
    are a challenge in Wikidata because as illustrated here which property to predict
    depends on the entity itself.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 错误属性（17.1%）。17.1%的错误是由于预测了错误的属性。一些示例需要背景知识来解析。例如，问题“马丁·路德·金在一生中做了什么？”的答案应该返回运动的值，而模型预测为职业。属性在Wikidata中是一个挑战，因为如这里所示，预测哪个属性取决于实体本身。
- en: Missing domain constraint (5.4%). Another common problem is missing the domain
    constraint. For example, the model correctly identifies that property shares border
    with should be used for question “what countries are around Egypt?”. However,
    it does not limit the answer to countries only, thus extra entities are returned.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失领域约束（5.4%）。另一个常见的问题是缺失领域约束。例如，模型正确识别了属性“与某地接壤”应该用于问题“埃及周围的国家有哪些？”。然而，它并没有将答案限制为国家，因此返回了额外的实体。
- en: 7 Experiment with QALD-7
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 QALD-7实验
- en: For another evaluation of WikiSP, we apply our model on Task 4 from QALD-7 (Usbeck
    et al., [2017](#bib.bib42)) dataset. QALD-7 is part of the QALD (Question Answering
    over Linked Data) which is a series of challenges started in 2011 known for their
    complex, manually created questions. It mainly focuses on DBpedia, but QALD-7’s
    Task 4 is engineered for Wikidata. The task includes 100 train examples, which
    we use to fine-tune our model and 50 test examples. There is no dev set.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对WikiSP的另一次评估中，我们将模型应用于QALD-7（Usbeck et al., [2017](#bib.bib42)）数据集的任务4。QALD-7是QALD（基于链接数据的问答）的一部分，这是自2011年开始的一系列挑战，以其复杂的人工创建问题而闻名。它主要集中在DBpedia，但QALD-7的任务4是为Wikidata设计的。该任务包括100个训练示例，我们用来微调模型，以及50个测试示例。没有开发集。
- en: '|  | EM | F1 |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | EM | F1 |  |'
- en: '| STAGG Yih et al. ([2016](#bib.bib51)) | - | 19.0 |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| STAGG Yih et al. ([2016](#bib.bib51)) | - | 19.0 |  |'
- en: '| GGNN Sorokin and Gurevych ([2018](#bib.bib35)) | - | 21.3 |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| GGNN Sorokin and Gurevych ([2018](#bib.bib35)) | - | 21.3 |  |'
- en: '| WDAqua Diefenbach et al. ([2017](#bib.bib12)) | - | 40.0 |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| WDAqua Diefenbach et al. ([2017](#bib.bib12)) | - | 40.0 |  |'
- en: '| WikiSP (Ours) | 38.0 | 43.6 |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| WikiSP（我们的） | 38.0 | 43.6 |  |'
- en: 'Table 3: Evaluation results of WikiSP on QALD-7 Task 4 and comparison with
    prior work.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：WikiSP在QALD-7任务4上的评估结果及与先前工作的比较。
- en: We choose QALD-7 as it is a manually crafted dataset with complex questions.
    We avoid datasets built on synthetic or human-paraphrased data, such as CSQA (Saha
    et al., [2018](#bib.bib31)) and KQA-Pro (Cao et al., [2022a](#bib.bib9)). As they
    have limited natural language variety between the training and evaluation data,
    models can get artificially high accuracy. For example, a simple BART based model
    can achieve over 90% accuracy on KQA-Pro even without an entity linking module (Cao
    et al., [2022a](#bib.bib9)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择QALD-7，因为它是一个手工制作的数据集，包含复杂的问题。我们避免使用基于合成或人类改写数据的数据集，如CSQA（Saha et al., [2018](#bib.bib31)）和KQA-Pro（Cao
    et al., [2022a](#bib.bib9)）。因为它们在训练和评估数据之间的自然语言变异性有限，模型可能会获得人为的高准确率。例如，简单的基于BART的模型即使没有实体链接模块，也能在KQA-Pro上达到90%以上的准确率（Cao
    et al., [2022a](#bib.bib9)）。
- en: The QALD-7 test set provides both the SPARQL queries as well as the answers.
    To double-check the correctness of the QALD-7 dataset, we applied the 50 gold
    queries of the test set to Wikidata and found that 4 did not return an answer.
    We hypothesize that the discrepancy is caused by the change in Wikidata structure/quantity
    of information. We evaluate WikiSP by comparing the answers where possible, and
    by comparing the generated SPARQL syntactically otherwise.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: QALD-7测试集提供了SPARQL查询和答案。为了再次确认QALD-7数据集的正确性，我们将测试集的50个黄金查询应用于Wikidata，发现其中4个没有返回答案。我们推测这种差异是由于Wikidata结构/信息量的变化造成的。我们通过比较答案来评估WikiSP，在可能的情况下，如果不行则通过比较生成的SPARQL语法。
- en: For this experiment, we use the same hyperparameters and data format as described
    in Section [5.3](#S5.SS3 "5.3 Executing Queries on Wikidata ‣ 5 Implementation
    ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata"). In addition to the training data for WikiSP,
    we also include the QALD-7 train samples, upsampled 20 times.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实验，我们使用了与第 [5.3](#S5.SS3 "5.3 Executing Queries on Wikidata ‣ 5 Implementation
    ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence
    Semantic Parsing over Wikidata") 节中描述的相同超参数和数据格式。除了 WikiSP 的训练数据外，我们还包括了 QALD-7
    的训练样本，放大了 20 倍。
- en: 7.1 QALD-7 Results
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 QALD-7 结果
- en: Our model achieves 38% accuracy on the QALD-7 dataset and outperforms the F1
    score of the state-of-the-art WDAqua (Diefenbach et al., [2017](#bib.bib12)) by
    3.6%, as shown in Table [3](#S7.T3 "Table 3 ‣ 7 Experiment with QALD-7 ‣ Fine-tuned
    LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata"). Note that WDAqua is based on retrieval, whereas WikiSP is based
    on sequence-to-sequence semantic parsing. QALD-7 (Usbeck et al., [2017](#bib.bib42))
    reports WDAqua as the winner of the leaderboard with 55.2 F1, however the authors
    of WDAqua reported 40.0 F1 in their papers (Diefenbach et al., [2017](#bib.bib12)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在 QALD-7 数据集上的准确率达到 38%，比最先进的 WDAqua (Diefenbach 等人，[2017](#bib.bib12))
    的 F1 分数高出 3.6%，如表 [3](#S7.T3 "Table 3 ‣ 7 Experiment with QALD-7 ‣ Fine-tuned
    LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing
    over Wikidata") 所示。请注意，WDAqua 基于检索，而 WikiSP 基于序列到序列的语义解析。QALD-7 (Usbeck 等人，[2017](#bib.bib42))
    报告 WDAqua 在排行榜上获胜，F1 为 55.2，但 WDAqua 的作者在他们的论文 (Diefenbach 等人，[2017](#bib.bib12))
    中报告了 40.0 F1。
- en: 7.2 Complementing GPT-3 on QALD-7
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 补充 GPT-3 在 QALD-7 上的表现
- en: Similar to WWQ, we also assess the combination of GPT with WikiSP on QALD-7
    as shown in Figure [3](#S7.F3 "Figure 3 ‣ 7.2 Complementing GPT-3 on QALD-7 ‣
    7 Experiment with QALD-7 ‣ Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot
    Sequence-to-Sequence Semantic Parsing over Wikidata"). The GPT model used was
    "text-davinci-002". Since there is no validation set and the test set is already
    very small, one of the authors who was not involved in training or finetuning
    the model evaluated GPT-3 on the test set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 WWQ，我们还在 QALD-7 上评估了 GPT 与 WikiSP 的结合，如图 [3](#S7.F3 "Figure 3 ‣ 7.2 Complementing
    GPT-3 on QALD-7 ‣ 7 Experiment with QALD-7 ‣ Fine-tuned LLMs Know More, Hallucinate
    Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata") 所示。使用的
    GPT 模型为 "text-davinci-002"。由于没有验证集且测试集已经非常小，因此没有参与模型训练或微调的作者对 GPT-3 在测试集上的表现进行了评估。
- en: GPT-3 is fully accurate on 62% of the questions, 20% incomplete, and 18% wrong.
    With our approach, we can provide 38% verifiably good answers from WikiSP; the
    guesses of GPT-3 get an additional 34% correct, 16% incomplete, and only 12% wrong.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 在 62% 的问题上完全准确，20% 的问题不完整，18% 的问题错误。使用我们的方法，我们可以从 WikiSP 提供 38% 经过验证的良好答案；GPT-3
    的猜测额外获得了 34% 正确答案、16% 不完整答案和仅 12% 错误答案。
- en: '![Refer to caption](img/7714f896358bcdd5d25671ed3eb2721f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7714f896358bcdd5d25671ed3eb2721f.png)'
- en: 'Figure 3: Distribution of correct, incomplete, and incorrect answers for the
    QALD-7 test set, when GPT-3 is used alone and when combined with WikiSP.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：当 GPT-3 单独使用和与 WikiSP 结合时，QALD-7 测试集中正确、不完整和错误答案的分布。
- en: 7.3 Discussion
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 讨论
- en: We did not conduct error analysis on the performance of QALD-7 as it has no
    dev set. The author evaluating GPT-3 noted that the test set of QALD-7 is much
    more complicated than the training data (of just 100 samples), with most of the
    queries containing multiple properties. This explains the lower accuracy of WikiSP
    on QALD-7 when compared to WikiWebQuestions, which has a few-shot training data
    set with a similar distribution as the test set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有对 QALD-7 的性能进行错误分析，因为它没有开发集。评估 GPT-3 的作者指出，QALD-7 的测试集比训练数据（仅 100 个样本）复杂得多，大多数查询包含多个属性。这解释了
    WikiSP 在 QALD-7 上的准确率低于 WikiWebQuestions 的原因，后者有一个与测试集分布类似的少样本训练数据集。
- en: This result suggests that the performance of WikiSP depends heavily on a good
    few-shot training data for fine-tuning the LLMs. We hypothesize that we can increase
    the performance of WikiSP in handling less popular questions with a better, possibly
    synthesized, training dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果表明，WikiSP 的性能在很大程度上依赖于用于微调 LLM 的优质少样本训练数据。我们假设，通过更好的、可能合成的训练数据集，我们可以提高 WikiSP
    处理不太流行问题的性能。
- en: 8 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We have created a new high-quality benchmark, WikiWebQuestions, for large knowledge-base
    question answering. The dataset is based on the popular WebQuestionsSP dataset
    with natural questions, annotated with SPARQL for Wikidata.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个新的高质量基准数据集WikiWebQuestions，用于大型知识库问答。该数据集基于流行的WebQuestionsSP数据集，包含自然语言问题，并用SPARQL对Wikidata进行标注。
- en: We establish a first, strong baseline of 65% answer accuracy and 72% F1 score
    for WikiWebQuestions. This is achieved by fine-tuning LLaMA with a few-shot training
    data set using a SPARQL query format modified for semantic parsing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为WikiWebQuestions建立了一个**首个强基线**，其答案准确率为65%，F1分数为72%。这是通过使用为语义解析修改的SPARQL查询格式对LLaMA进行少量训练数据集微调实现的。
- en: We show that we can reduce the hallucination of large language models like GPT-3
    by grounding it with a semantic parser. For the dev set of WikiWebQuestions, this
    combination approach provides useful information for 96% of the questions in the
    dev set of the benchmark. More importantly, it generates verifiable answers for
    76% of the questions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了通过用语义解析器对大型语言模型如GPT-3进行**基础验证**，可以减少**幻觉**。对于WikiWebQuestions的开发集，这种组合方法为基准开发集中的96%问题提供了有用的信息。更重要的是，它为76%的问题生成了可验证的答案。
- en: Limitations
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: While applications of large language models seem to expand every day, this paper
    mainly focuses on factoid question answering. Long-form text generation, for example,
    is outside the scope of the experiments of this paper, but the methodology described
    here may be extended to this setting in the future. Even though knowledge bases
    are an important source of facts, a large portion of the knowledge available in
    digital form (e.g. Wikipedia, news articles, etc.), is not organized into knowledge
    bases. As such, the results of this paper can be considered complementary to the
    larger body of fact-checking research based on free text.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型的应用似乎每天都在扩展，但这篇论文主要集中在事实问答上。长期文本生成，例如，超出了本论文实验的范围，但这里描述的方法可能在未来扩展到这种设置中。尽管知识库是事实的重要来源，但以数字形式存在的大部分知识（例如，维基百科、新闻文章等）并未组织成知识库。因此，这篇论文的结果可以被视为对基于自由文本的事实检查研究的补充。
- en: Our semantic parser can be used to verify answers from LLMs. However, this additional
    round of running the semantic parser and querying Wikidata increase the response
    latency, which may be noticeable by end-users of such systems.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语义解析器可以用来验证LLMs的答案。然而，运行语义解析器并查询Wikidata的额外步骤会增加响应延迟，这可能会被这些系统的最终用户察觉。
- en: All of our datasets and experiments are conducted for English. Expanding to
    other languages, while possible (Moradshahi et al., [2020](#bib.bib25)) are outside
    the scope of this work.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的数据集和实验都是针对英语进行的。扩展到其他语言，虽然可能（Moradshahi等，[2020](#bib.bib25)），但超出了这项工作的范围。
- en: Our experiments were performed using GPT-3 (davinci-002) as that was what we
    had access to when we started the project. Undoubtedly, the later LLMs will produce
    better results. Nonetheless, the need to have verifiable results based on live
    database accesses will remain.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验使用了GPT-3（davinci-002），因为这是我们启动项目时可以访问的模型。毫无疑问，后来的LLMs将会产生更好的结果。然而，基于实时数据库访问的可验证结果的需求将会持续存在。
- en: Ethical Considerations
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: LLMs are used by millions of people everyday. We hope that this line of work
    will help make them more reliable for everyone, mitigating some of their potential
    downsides, and giving users access to more accurate information. Our use of Wikidata
    will enable future researchers and developers to connect their systems with a
    large, diverse and live knowledge graph that is updated every day. We do not anticipate
    any harm resulting from the methods introduced in this work.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs每天被数百万人使用。我们希望这项工作能帮助使它们对每个人更加可靠，减轻其潜在的负面影响，并使用户获得更准确的信息。我们对Wikidata的使用将使未来的研究人员和开发者能够将他们的系统连接到一个大型、多样化且每日更新的实时知识图谱。我们不预期这项工作中引入的方法会产生任何危害。
- en: We did not crowdsource any datasets for this paper, as the questions are converted
    from a previous dataset and all the re-annotation and analysis is done by the
    authors.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有为这篇论文众包任何数据集，因为这些问题是从先前的数据集中转换过来的，所有的重新标注和分析都是由作者完成的。
- en: To conduct experiments in this paper, we used an estimated total of 60 NC96ads-A100
    GPU hours on Microsoft Azure. Each finetuning experiment takes roughly 3 hours,
    and we conducted roughly 20 experiments to arrive at the results in this paper.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行本文中的实验，我们在Microsoft Azure上使用了大约60小时的NC96ads-A100 GPU时间。每次微调实验大约需要3小时，我们进行了约20次实验以得到本文中的结果。
- en: Acknowledgements
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported in part by the National Science Foundation, the Alfred
    P. Sloan Foundation, the Verdant Foundation, Microsoft Azure AI credit, KDDI,
    JPMorgan Chase, and the Stanford Human-Centered Artificial Intelligence (HAI)
    Institute. We also thank the reviewers for their valuable comments and suggestions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作部分得到了国家科学基金会、阿尔弗雷德·P·斯隆基金会、Verdant基金会、Microsoft Azure AI信用、KDDI、摩根大通以及斯坦福人本人工智能（HAI）研究所的支持。我们还感谢评审员的宝贵意见和建议。
- en: References
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: An et al. (2023) Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning
    Zheng, Weizhu Chen, and Jian-Guang Lou. 2023. [Skill-based few-shot selection
    for in-context learning](http://arxiv.org/abs/2305.14210).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等 (2023) Shengnan An、Bo Zhou、Zeqi Lin、Qiang Fu、Bei Chen、Nanning Zheng、Weizhu
    Chen 和 Jian-Guang Lou。2023年。[基于技能的少-shot选择用于上下文学习](http://arxiv.org/abs/2305.14210)。
- en: 'Arora et al. (2023) Aseem Arora, Shabbirhussain Bhaisaheb, Harshit Nigam, Manasi
    Patwardhan, Lovekesh Vig, and Gautam Shroff. 2023. [Adapt and decompose: Efficient
    generalization of text-to-sql via domain adapted least-to-most prompting](http://arxiv.org/abs/2308.02582).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arora 等 (2023) Aseem Arora、Shabbirhussain Bhaisaheb、Harshit Nigam、Manasi Patwardhan、Lovekesh
    Vig 和 Gautam Shroff。2023年。[Adapt and decompose: 通过领域适应的最小到最多提示来高效泛化文本到SQL](http://arxiv.org/abs/2308.02582)。'
- en: 'Ayoola et al. (2022) Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos,
    and Andrea Pierleoni. 2022. [ReFinED: An efficient zero-shot-capable approach
    to end-to-end entity linking](https://doi.org/10.18653/v1/2022.naacl-industry.24).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies: Industry Track*, pages
    209–220, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ayoola 等 (2022) Tom Ayoola、Shubhi Tyagi、Joseph Fisher、Christos Christodoulopoulos
    和 Andrea Pierleoni。2022年。[ReFinED: 一种高效的零-shot能力的端到端实体链接方法](https://doi.org/10.18653/v1/2022.naacl-industry.24)。在*2022年北美计算语言学协会：人类语言技术：行业轨道会议论文集*，第209–220页，Hybrid:
    华盛顿州西雅图 + 在线。计算语言学协会。'
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale Fung. 2023. [A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity](http://arxiv.org/abs/2302.04023).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang 等 (2023) Yejin Bang、Samuel Cahyawijaya、Nayeon Lee、Wenliang Dai、Dan Su、Bryan
    Wilie、Holy Lovenia、Ziwei Ji、Tiezheng Yu、Willy Chung、Quyet V. Do、Yan Xu 和 Pascale
    Fung。2023年。[对ChatGPT在推理、幻觉和互动方面的多任务、多语言、多模态评估](http://arxiv.org/abs/2302.04023)。
- en: Berant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
    2013. [Semantic parsing on Freebase from question-answer pairs](https://aclanthology.org/D13-1160).
    In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing*, pages 1533–1544, Seattle, Washington, USA. Association for Computational
    Linguistics.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berant 等 (2013) Jonathan Berant、Andrew Chou、Roy Frostig 和 Percy Liang。2013年。[基于问题-答案对的Freebase语义解析](https://aclanthology.org/D13-1160)。在*2013年自然语言处理实证方法会议论文集*，第1533–1544页，美国华盛顿州西雅图。计算语言学协会。
- en: 'Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
    Sturge, and Jamie Taylor. 2008. [Freebase: A collaboratively created graph database
    for structuring human knowledge](https://doi.org/10.1145/1376616.1376746). In
    *Proceedings of the 2008 ACM SIGMOD International Conference on Management of
    Data*, SIGMOD ’08, page 1247–1250, New York, NY, USA. Association for Computing
    Machinery.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bollacker 等 (2008) Kurt Bollacker、Colin Evans、Praveen Paritosh、Tim Sturge 和
    Jamie Taylor。2008年。[Freebase: 一个协作创建的图形数据库，用于结构化人类知识](https://doi.org/10.1145/1376616.1376746)。在*2008年ACM
    SIGMOD国际数据管理会议论文集*，SIGMOD ’08，第1247–1250页，美国纽约。计算机协会。'
- en: 'Campagna et al. (2017) Giovanni Campagna, Rakesh Ramesh, Silei Xu, Michael
    Fischer, and Monica S. Lam. 2017. [Almond: The architecture of an open, crowdsourced,
    privacy-preserving, programmable virtual assistant](https://doi.org/10.1145/3038912.3052562).
    In *Proceedings of the 26th International Conference on World Wide Web - WWW ’17*,
    pages 341–350, New York, New York, USA. ACM Press.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Campagna et al. (2017) Giovanni Campagna、Rakesh Ramesh、Silei Xu、Michael Fischer
    和 Monica S. Lam。2017年。[Almond: 一个开放的、众包的、隐私保护的、可编程虚拟助手架构](https://doi.org/10.1145/3038912.3052562)。收录于*第26届国际万维网会议论文集
    - WWW ’17*，第341–350页，美国纽约。ACM出版社。'
- en: 'Campagna et al. (2019) Giovanni Campagna, Silei Xu, Mehrad Moradshahi, Richard
    Socher, and Monica S. Lam. 2019. [Genie: A generator of natural language semantic
    parsers for virtual assistant commands](https://doi.org/10.1145/3314221.3314594).
    In *Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design
    and Implementation*, PLDI 2019, page 394–410, New York, NY, USA. Association for
    Computing Machinery.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Campagna et al. (2019) Giovanni Campagna、Silei Xu、Mehrad Moradshahi、Richard
    Socher 和 Monica S. Lam。2019年。[Genie: 一种用于虚拟助手命令的自然语言语义解析器生成器](https://doi.org/10.1145/3314221.3314594)。收录于*第40届ACM
    SIGPLAN编程语言设计与实现会议论文集*，PLDI 2019，第394–410页，美国纽约。计算机协会。'
- en: 'Cao et al. (2022a) Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong
    Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang. 2022a. [KQA pro: A dataset
    with explicit compositional programs for complex question answering over knowledge
    base](https://doi.org/10.18653/v1/2022.acl-long.422). In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 6101–6119, Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao et al. (2022a) Shulin Cao、Jiaxin Shi、Liangming Pan、Lunyiu Nie、Yutong Xiang、Lei
    Hou、Juanzi Li、Bin He 和 Hanwang Zhang。2022年。[KQA pro: 一个具有显式组合程序的复杂问题回答数据集](https://doi.org/10.18653/v1/2022.acl-long.422)。收录于*第60届计算语言学协会年会（第1卷：长论文）*，第6101–6119页，爱尔兰都柏林。计算语言学协会。'
- en: 'Cao et al. (2022b) Shulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu, Lei
    Hou, Juanzi Li, Zhiyuan Liu, and Jinghui Xiao. 2022b. [Program transfer for answering
    complex questions over knowledge bases](https://doi.org/10.18653/v1/2022.acl-long.559).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8128–8140, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2022b) Shulin Cao、Jiaxin Shi、Zijun Yao、Xin Lv、Jifan Yu、Lei Hou、Juanzi
    Li、Zhiyuan Liu 和 Jinghui Xiao。2022年。[知识库复杂问题回答的程序迁移](https://doi.org/10.18653/v1/2022.acl-long.559)。收录于*第60届计算语言学协会年会（第1卷：长论文）*，第8128–8140页，爱尔兰都柏林。计算语言学协会。
- en: Das et al. (2021) Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan
    Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021.
    [Case-based reasoning for natural language queries over knowledge bases](https://doi.org/10.18653/v1/2021.emnlp-main.755).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 9594–9611, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das et al. (2021) Rajarshi Das、Manzil Zaheer、Dung Thai、Ameya Godbole、Ethan Perez、Jay
    Yoon Lee、Lizhen Tan、Lazaros Polymenakos 和 Andrew McCallum。2021年。[基于案例推理的知识库自然语言查询](https://doi.org/10.18653/v1/2021.emnlp-main.755)。收录于*2021年自然语言处理经验方法会议论文集*，第9594–9611页，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Diefenbach et al. (2017) Dennis Diefenbach, Kamal Singh, and Pierre Maret.
    2017. Wdaqua-core0: A question answering component for the research community.
    In *Semantic Web Evaluation Challenge*, pages 84–89\. Springer.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Diefenbach et al. (2017) Dennis Diefenbach、Kamal Singh 和 Pierre Maret。2017年。Wdaqua-core0:
    一个用于研究社区的问题回答组件。收录于*语义网评估挑战*，第84–89页。施普林格出版社。'
- en: 'Dong et al. (2015) Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2015. [Question
    answering over Freebase with multi-column convolutional neural networks](https://doi.org/10.3115/v1/P15-1026).
    In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 260–269, Beijing, China. Association for Computational
    Linguistics.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2015) Li Dong、Furu Wei、Ming Zhou 和 Ke Xu。2015年。[通过多列卷积神经网络在Freebase上进行问题回答](https://doi.org/10.3115/v1/P15-1026)。收录于*第53届计算语言学协会年会及第7届国际自然语言处理联合会议论文集（第1卷：长论文）*，第260–269页，中国北京。计算语言学协会。
- en: 'Goddard (2023) Jerome Goddard. 2023. Hallucinations in chatgpt: A cautionary
    tale for biomedical researchers. *The American Journal of Medicine*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goddard（2023）Jerome Goddard。2023。ChatGPT中的幻觉：对生物医学研究者的警示故事。*美国医学杂志*。
- en: 'Gu et al. (2021) Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang,
    Xifeng Yan, and Yu Su. 2021. [Beyond i.i.d.: Three levels of generalization for
    question answering on knowledge bases](https://doi.org/10.1145/3442381.3449992).
    In *Proceedings of the Web Conference 2021*. ACM.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2021）Yu Gu、Sue Kase、Michelle Vanni、Brian Sadler、Percy Liang、Xifeng Yan
    和 Yu Su。2021。[超越独立同分布：知识库问答的三层泛化](https://doi.org/10.1145/3442381.3449992)。在*2021年网络会议论文集*。ACM。
- en: 'Gu and Su (2022) Yu Gu and Yu Su. 2022. [ArcaneQA: Dynamic program induction
    and contextualized encoding for knowledge base question answering](https://aclanthology.org/2022.coling-1.148).
    In *Proceedings of the 29th International Conference on Computational Linguistics*,
    pages 1718–1731, Gyeongju, Republic of Korea. International Committee on Computational
    Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 和 Su（2022）Yu Gu 和 Yu Su。2022。[ArcaneQA：用于知识库问答的动态程序归纳和上下文化编码](https://aclanthology.org/2022.coling-1.148)。在*第29届国际计算语言学会议论文集*，第1718–1731页，庆州，韩国。国际计算语言学委员会。
- en: 'Hu et al. (2022) Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith,
    and Mari Ostendorf. 2022. [In-context learning for few-shot dialogue state tracking](https://doi.org/10.18653/v1/2022.findings-emnlp.193).
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    2627–2643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2022）Yushi Hu、Chia-Hsuan Lee、Tianbao Xie、Tao Yu、Noah A. Smith 和 Mari Ostendorf。2022。[少样本对话状态追踪的上下文学习](https://doi.org/10.18653/v1/2022.findings-emnlp.193)。在*计算语言学协会发现：EMNLP
    2022*，第2627–2643页，阿布扎比，阿联酋。计算语言学协会。
- en: Lan and Jiang (2020) Yunshi Lan and Jing Jiang. 2020. [Query graph generation
    for answering multi-hop complex questions from knowledge bases](https://doi.org/10.18653/v1/2020.acl-main.91).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 969–974, Online. Association for Computational Linguistics.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 和 Jiang（2020）Yunshi Lan 和 Jing Jiang。2020。[用于回答来自知识库的多跳复杂问题的查询图生成](https://doi.org/10.18653/v1/2020.acl-main.91)。在*第58届计算语言学协会年会论文集*，第969–974页，在线。计算语言学协会。
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.
    [BART: Denoising sequence-to-sequence pre-training for natural language generation,
    translation, and comprehension](https://doi.org/10.18653/v1/2020.acl-main.703).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7871–7880, Online. Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis 等人（2020）Mike Lewis、Yinhan Liu、Naman Goyal、Marjan Ghazvininejad、Abdelrahman
    Mohamed、Omer Levy、Veselin Stoyanov 和 Luke Zettlemoyer。2020。[BART: 用于自然语言生成、翻译和理解的去噪序列到序列预训练](https://doi.org/10.18653/v1/2020.acl-main.703)。在*第58届计算语言学协会年会论文集*，第7871–7880页，在线。计算语言学协会。'
- en: Li et al. (2020) Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and
    Wen-tau Yih. 2020. [Efficient one-pass end-to-end entity linking for questions](https://doi.org/10.18653/v1/2020.emnlp-main.522).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 6433–6441, Online. Association for Computational Linguistics.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020）Belinda Z. Li、Sewon Min、Srinivasan Iyer、Yashar Mehdad 和 Wen-tau Yih。2020。[高效的一次性端到端实体链接](https://doi.org/10.18653/v1/2020.emnlp-main.522)。在*2020年自然语言处理经验方法会议（EMNLP）论文集*，第6433–6441页，在线。计算语言学协会。
- en: Li et al. (2023) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao
    Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.
    2023. [Can llm already serve as a database interface? a big bench for large-scale
    database grounded text-to-sqls](http://arxiv.org/abs/2305.03111).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Jinyang Li、Binyuan Hui、Ge Qu、Binhua Li、Jiaxi Yang、Bowen Li、Bailin
    Wang、Bowen Qin、Rongyu Cao、Ruiying Geng、Nan Huo、Xuanhe Zhou、Chenhao Ma、Guoliang
    Li、Kevin C. C. Chang、Fei Huang、Reynold Cheng 和 Yongbin Li。2023。[大型数据库为基础的文本到 SQL
    的大基准：LLM 是否已经能够作为数据库接口？](http://arxiv.org/abs/2305.03111)。
- en: Luo et al. (2018) Kangqi Luo, Fengli Lin, Xusheng Luo, and Kenny Zhu. 2018.
    [Knowledge base question answering via encoding of complex query graphs](https://doi.org/10.18653/v1/D18-1242).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 2185–2194, Brussels, Belgium. Association for Computational
    Linguistics.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2018）Kangqi Luo, Fengli Lin, Xusheng Luo, 和 Kenny Zhu。2018年。[通过复杂查询图编码的知识库问答](https://doi.org/10.18653/v1/D18-1242)。在
    *2018年自然语言处理实证方法会议论文集*，第 2185–2194 页，布鲁塞尔，比利时。计算语言学协会。
- en: 'Mavromatis and Karypis (2022) Costas Mavromatis and George Karypis. 2022. [ReaRev:
    Adaptive reasoning for question answering over knowledge graphs](https://aclanthology.org/2022.findings-emnlp.181).
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    2447–2458, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mavromatis 和 Karypis（2022）Costas Mavromatis 和 George Karypis。2022年。[ReaRev：针对知识图谱问答的自适应推理](https://aclanthology.org/2022.findings-emnlp.181)。在
    *计算语言学协会：EMNLP 2022 发现*，第 2447–2458 页，阿布扎比，阿联酋。计算语言学协会。
- en: Miller et al. (2016) Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein
    Karimi, Antoine Bordes, and Jason Weston. 2016. [Key-value memory networks for
    directly reading documents](https://doi.org/10.18653/v1/D16-1147). In *Proceedings
    of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages
    1400–1409, Austin, Texas. Association for Computational Linguistics.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller 等人（2016）Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi,
    Antoine Bordes, 和 Jason Weston。2016年。[用于直接读取文档的键值记忆网络](https://doi.org/10.18653/v1/D16-1147)。在
    *2016年自然语言处理实证方法会议论文集*，第 1400–1409 页，奥斯汀，德克萨斯州。计算语言学协会。
- en: Moradshahi et al. (2020) Mehrad Moradshahi, Giovanni Campagna, Sina Semnani,
    Silei Xu, and Monica Lam. 2020. [Localizing open-ontology QA semantic parsers
    in a day using machine translation](https://doi.org/10.18653/v1/2020.emnlp-main.481).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 5970–5983, Online. Association for Computational Linguistics.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moradshahi 等人（2020）Mehrad Moradshahi, Giovanni Campagna, Sina Semnani, Silei
    Xu, 和 Monica Lam。2020年。[通过机器翻译在一天内本地化开放本体 QA 语义解析器](https://doi.org/10.18653/v1/2020.emnlp-main.481)。在
    *2020年自然语言处理实证方法会议（EMNLP）论文集*，第 5970–5983 页，在线。计算语言学协会。
- en: 'Nan et al. (2023) Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung
    Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. 2023. [Enhancing few-shot text-to-sql
    capabilities of large language models: A study on prompt design strategies](http://arxiv.org/abs/2305.12586).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nan 等人（2023）Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae,
    Ellen Zhang, Arman Cohan, 和 Dragomir Radev。2023年。[增强大型语言模型的少量样本文本到 SQL 能力：关于提示设计策略的研究](http://arxiv.org/abs/2305.12586)。
- en: Niu et al. (2023) Yilin Niu, Fei Huang, Wei Liu, Jianwei Cui, Bin Wang, and
    Minlie Huang. 2023. [Bridging the Gap between Synthetic and Natural Questions
    via Sentence Decomposition for Semantic Parsing](https://doi.org/10.1162/tacl_a_00552).
    *Transactions of the Association for Computational Linguistics*, 11:367–383.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等人（2023）Yilin Niu, Fei Huang, Wei Liu, Jianwei Cui, Bin Wang, 和 Minlie Huang。2023年。[通过句子分解弥合合成问题与自然问题之间的差距以进行语义解析](https://doi.org/10.1162/tacl_a_00552)。*计算语言学协会会刊*，11:367–383。
- en: 'Poesia et al. (2022) Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo
    Soares, Christopher Meek, and Sumit Gulwani. 2022. [Synchromesh: Reliable code
    generation from pre-trained language models](https://openreview.net/forum?id=KmtVD97J43e).
    In *The Tenth International Conference on Learning Representations, ICLR 2022,
    Virtual Event, April 25-29, 2022*. OpenReview.net.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poesia 等人（2022）Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares,
    Christopher Meek, 和 Sumit Gulwani。2022年。[Synchromesh：来自预训练语言模型的可靠代码生成](https://openreview.net/forum?id=KmtVD97J43e)。在
    *第十届国际学习表示会议，ICLR 2022，虚拟会议，2022年4月25-29日*。OpenReview.net。
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    [Learning to retrieve prompts for in-context learning](https://doi.org/10.18653/v1/2022.naacl-main.191).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2655–2671,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 等人（2022）Ohad Rubin, Jonathan Herzig, 和 Jonathan Berant。2022年。[学习检索上下文学习的提示](https://doi.org/10.18653/v1/2022.naacl-main.191)。在
    *2022年计算语言学协会北美章节会议：人类语言技术论文集*，第 2655–2671 页，西雅图，美国。计算语言学协会。
- en: Saha et al. (2019) Amrita Saha, Ghulam Ahmed Ansari, Abhishek Laddha, Karthik
    Sankaranarayanan, and Soumen Chakrabarti. 2019. [Complex program induction for
    querying knowledge bases in the absence of gold programs](https://doi.org/10.1162/tacl_a_00262).
    *Transactions of the Association for Computational Linguistics*, 7:185–200.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha 等（2019）阿姆丽塔·萨哈、古拉姆·艾哈迈德·安萨里、阿比谢克·拉达、卡尔基克·桑卡拉纳亚南和苏门·查克拉巴尔蒂。2019年。 [在缺乏黄金程序的情况下，复杂程序归纳以查询知识库](https://doi.org/10.1162/tacl_a_00262)。
    *计算语言学协会会刊*，7:185–200。
- en: 'Saha et al. (2018) Amrita Saha, Vardaan Pahuja, Mitesh Khapra, Karthik Sankaranarayanan,
    and Sarath Chandar. 2018. Complex sequential question answering: Towards learning
    to converse over linked question answer pairs with a knowledge graph. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 32.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha 等（2018）阿姆丽塔·萨哈、瓦尔丹·帕胡贾、米特什·卡普拉、卡尔基克·桑卡拉纳亚南和萨拉斯·钱达尔。2018年。复杂的序列问答：学习通过知识图谱对链接问答对进行对话。发表于
    *美国人工智能协会会议论文集*，第32卷。
- en: Sen et al. (2021) Priyanka Sen, Armin Oliya, and Amir Saffari. 2021. [Expanding
    end-to-end question answering on differentiable knowledge graphs with intersection](https://doi.org/10.18653/v1/2021.emnlp-main.694).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 8805–8812, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sen 等（2021）普里扬卡·森、阿尔敏·奥利亚和阿米尔·萨法里。2021年。 [通过交集扩展可微知识图谱上的端到端问答](https://doi.org/10.18653/v1/2021.emnlp-main.694)。发表于
    *2021年自然语言处理实证方法会议论文集*，页8805–8812，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Shin et al. (2021) Richard Shin, Christopher Lin, Sam Thomson, Charles Chen,
    Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner,
    and Benjamin Van Durme. 2021. [Constrained language models yield few-shot semantic
    parsers](https://doi.org/10.18653/v1/2021.emnlp-main.608). In *Proceedings of
    the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7699–7715, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等（2021）理查德·申、克里斯托弗·林、萨姆·汤普森、查尔斯·陈、苏布罗·罗伊、埃曼努伊尔·安东尼奥斯·普拉塔尼奥斯、亚当·保尔斯、丹·克莱因、杰森·艾斯纳和本杰明·范·杜尔梅。2021年。
    [受限语言模型产生少样本语义解析器](https://doi.org/10.18653/v1/2021.emnlp-main.608)。发表于 *2021年自然语言处理实证方法会议论文集*，页7699–7715，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Shu et al. (2022) Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje Karlsson, Tingting
    Ma, Yuzhong Qu, and Chin-Yew Lin. 2022. [TIARA: Multi-grained retrieval for robust
    question answering over large knowledge base](https://aclanthology.org/2022.emnlp-main.555).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 8108–8121, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等（2022）耶恒·舒、志伟·余、玉涵·李、伯耶·卡尔松、婷婷·马、宇忠·曲和陈伟·林。2022年。 [TIARA：大规模知识库上的多粒度检索以实现鲁棒问答](https://aclanthology.org/2022.emnlp-main.555)。发表于
    *2022年自然语言处理实证方法会议论文集*，页8108–8121，阿布扎比，阿拉伯联合酋长国。计算语言学协会。
- en: Sorokin and Gurevych (2018) Daniil Sorokin and Iryna Gurevych. 2018. [Modeling
    semantics with gated graph neural networks for knowledge base question answering](https://aclanthology.org/C18-1280).
    In *Proceedings of the 27th International Conference on Computational Linguistics*,
    pages 3306–3317, Santa Fe, New Mexico, USA. Association for Computational Linguistics.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorokin 和 Gurevych（2018）达尼尔·索罗金和伊琳娜·古列维奇。2018年。 [用门控图神经网络建模语义以进行知识库问答](https://aclanthology.org/C18-1280)。发表于
    *第27届国际计算语言学会议论文集*，页3306–3317，新墨西哥州圣菲，美国。计算语言学协会。
- en: Su et al. (2017) Yu Su, Ahmed Hassan Awadallah, Madian Khabsa, Patrick Pantel,
    Michael Gamon, and Mark Encarnacion. 2017. Building natural language interfaces
    to web apis. In *Proceedings of the 2017 ACM on Conference on Information and
    Knowledge Management*, pages 177–186.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2017）余·苏、艾哈迈德·哈桑·阿瓦达拉、马迪安·哈布萨、帕特里克·潘特尔、迈克尔·加蒙和马克·恩卡纳西翁。2017年。构建自然语言接口到网络
    API。发表于 *2017年ACM信息与知识管理会议论文集*，页177–186。
- en: 'Sun et al. (2019) Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019.
    [PullNet: Open domain question answering with iterative retrieval on knowledge
    bases and text](https://doi.org/10.18653/v1/D19-1242). In *Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 2380–2390, Hong Kong, China. Association for Computational Linguistics.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等（2019）Haitian Sun, Tania Bedrax-Weiss, 和 William Cohen. 2019. [PullNet:
    通过知识库和文本的迭代检索进行开放领域问答](https://doi.org/10.18653/v1/D19-1242)。在 *2019年自然语言处理实证方法会议及第9届国际自然语言处理联合会议（EMNLP-IJCNLP）*，第2380–2390页，中国香港。计算语言学协会。'
- en: Sun et al. (2018) Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis,
    Ruslan Salakhutdinov, and William Cohen. 2018. [Open domain question answering
    using early fusion of knowledge bases and text](https://doi.org/10.18653/v1/D18-1455).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 4231–4242, Brussels, Belgium. Association for Computational
    Linguistics.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2018）Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan
    Salakhutdinov, 和 William Cohen. 2018. [使用知识库和文本的早期融合进行开放领域问答](https://doi.org/10.18653/v1/D18-1455)。在
    *2018年自然语言处理实证方法会议论文集*，第4231–4242页，比利时布鲁塞尔。计算语言学协会。
- en: 'Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. [The web as
    a knowledge-base for answering complex questions](https://doi.org/10.18653/v1/N18-1059).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*,
    pages 641–651, New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor和Berant（2018）Alon Talmor 和 Jonathan Berant. 2018. [将网络作为回答复杂问题的知识库](https://doi.org/10.18653/v1/N18-1059)。在
    *2018年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇论文）*，第641–651页，美国路易斯安那州新奥尔良。计算语言学协会。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori等（2023）Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. Stanford alpaca:
    一种遵循指令的llama模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等. 2023. Llama: 开放且高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*。'
- en: Usbeck et al. (2017) Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian Haarmann,
    Anastasia Krithara, Michael Röder, and Giulio Napolitano. 2017. 7th open challenge
    on question answering over linked data (qald-7). In *Semantic web evaluation challenge*,
    pages 59–69\. Springer.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Usbeck等（2017）Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian Haarmann, Anastasia
    Krithara, Michael Röder, 和 Giulio Napolitano. 2017. 第七届基于关联数据的问答开放挑战赛（qald-7）。在
    *语义网评估挑战*，第59–69页。施普林格。
- en: 'Verga et al. (2021) Pat Verga, Haitian Sun, Livio Baldini Soares, and William
    Cohen. 2021. [Adaptable and interpretable neural MemoryOver symbolic knowledge](https://doi.org/10.18653/v1/2021.naacl-main.288).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3678–3691,
    Online. Association for Computational Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verga等（2021）Pat Verga, Haitian Sun, Livio Baldini Soares, 和 William Cohen. 2021.
    [可适应且可解释的神经 MemoryOver 符号知识](https://doi.org/10.18653/v1/2021.naacl-main.288)。在
    *2021年北美计算语言学协会年会：人类语言技术会议论文集*，第3678–3691页，在线。计算语言学协会。
- en: Vivona and Hassani (2019) Salvatore Vivona and Kaveh Hassani. 2019. [Relational
    graph representation learning for open-domain question answering](http://arxiv.org/abs/1910.08249).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vivona和Hassani（2019）Salvatore Vivona 和 Kaveh Hassani. 2019. [用于开放领域问答的关系图表示学习](http://arxiv.org/abs/1910.08249)。
- en: 'Wang et al. (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. [Self-instruct:
    Aligning language models with self-generated instructions](https://doi.org/10.18653/v1/2023.acl-long.754).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 13484–13508, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah
    A. Smith, Daniel Khashabi 和 Hannaneh Hajishirzi. 2023. [Self-instruct：将语言模型与自生成指令对齐](https://doi.org/10.18653/v1/2023.acl-long.754)。在
    *第61届计算语言学协会年会 (第1卷：长篇论文)* 中，页码 13484–13508，多伦多，加拿大。计算语言学协会。
- en: Weiser (2023) Benjamin Weiser. 2023. [Here’s what happens when your lawyer uses
    chatgpt](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html).
    *The New York Times*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiser (2023) Benjamin Weiser. 2023. [当你的律师使用 chatgpt 时会发生什么](https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html)。*纽约时报*。
- en: 'Xu et al. (2020a) Silei Xu, Giovanni Campagna, Jian Li, and Monica S. Lam.
    2020a. [Schema2qa: High-quality and low-cost q&a agents for the structured web](https://doi.org/10.1145/3340531.3411974).
    In *Proceedings of the 29th ACM International Conference on Information & Knowledge
    Management*, CIKM ’20, page 1685–1694, New York, NY, USA. Association for Computing
    Machinery.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2020a) Silei Xu, Giovanni Campagna, Jian Li 和 Monica S. Lam. 2020a. [Schema2qa：高质量和低成本的结构化网络问答代理](https://doi.org/10.1145/3340531.3411974)。在
    *第29届ACM国际信息与知识管理大会* 中，CIKM ’20，页码 1685–1694，纽约，NY，美国。计算机协会。
- en: 'Xu et al. (2020b) Silei Xu, Sina Semnani, Giovanni Campagna, and Monica Lam.
    2020b. [AutoQA: From databases to QA semantic parsers with only synthetic training
    data](https://doi.org/10.18653/v1/2020.emnlp-main.31). In *Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pages 422–434, Online. Association for Computational Linguistics.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2020b) Silei Xu, Sina Semnani, Giovanni Campagna 和 Monica Lam. 2020b.
    [AutoQA：从数据库到仅用合成训练数据的QA语义解析器](https://doi.org/10.18653/v1/2020.emnlp-main.31)。在
    *2020年自然语言处理实证方法会议 (EMNLP)* 中，页码 422–434，在线。计算语言学协会。
- en: 'Ye et al. (2022) Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming
    Xiong. 2022. [RNG-KBQA: Generation augmented iterative ranking for knowledge base
    question answering](https://doi.org/10.18653/v1/2022.acl-long.417). In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 6032–6043, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人 (2022) Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou 和 Caiming Xiong.
    2022. [RNG-KBQA：增强生成的迭代排序用于知识库问答](https://doi.org/10.18653/v1/2022.acl-long.417)。在
    *第60届计算语言学协会年会 (第1卷：长篇论文)* 中，页码 6032–6043，都柏林，爱尔兰。计算语言学协会。
- en: 'Yih et al. (2015) Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao.
    2015. [Semantic parsing via staged query graph generation: Question answering
    with knowledge base](https://doi.org/10.3115/v1/P15-1128). In *Proceedings of
    the 53rd Annual Meeting of the Association for Computational Linguistics and the
    7th International Joint Conference on Natural Language Processing (Volume 1: Long
    Papers)*, pages 1321–1331, Beijing, China. Association for Computational Linguistics.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yih 等人 (2015) Wen-tau Yih, Ming-Wei Chang, Xiaodong He 和 Jianfeng Gao. 2015.
    [通过分阶段查询图生成的语义解析：基于知识库的问答](https://doi.org/10.3115/v1/P15-1128)。在 *第53届计算语言学协会年会暨第7届国际自然语言处理联合会议
    (第1卷：长篇论文)* 中，页码 1321–1331，北京，中国。计算语言学协会。
- en: 'Yih et al. (2016) Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang,
    and Jina Suh. 2016. [The value of semantic parse labeling for knowledge base question
    answering](https://doi.org/10.18653/v1/P16-2033). In *Proceedings of the 54th
    Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
    Papers)*, pages 201–206, Berlin, Germany. Association for Computational Linguistics.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yih 等人 (2016) Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang 和
    Jina Suh. 2016. [语义解析标记对知识库问答的价值](https://doi.org/10.18653/v1/P16-2033)。在 *第54届计算语言学协会年会
    (第2卷：短篇论文)* 中，页码 201–206，柏林，德国。计算语言学协会。
- en: 'Yu et al. (2023) Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo
    Li, Jun Wang, Yiqun Hu, William Wang, Zhiguo Wang, and Bing Xiang. 2023. [Decaf:
    Joint decoding of answers and logical forms for question answering over knowledge
    bases](https://www.amazon.science/publications/decaf-joint-decoding-of-answers-and-logical-forms-for-question-answering-over-knowledge-bases).
    In *ICLR 2023*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2023) 董汉 Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li,
    Jun Wang, Yiqun Hu, William Wang, Zhiguo Wang 和 Bing Xiang. 2023. [Decaf: 联合解码知识库问答的答案和逻辑形式](https://www.amazon.science/publications/decaf-joint-decoding-of-answers-and-logical-forms-for-question-answering-over-knowledge-bases).
    在 *ICLR 2023*。'
- en: 'Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
    Radev. 2018. [Spider: A large-scale human-labeled dataset for complex and cross-domain
    semantic parsing and text-to-SQL task](https://doi.org/10.18653/v1/D18-1425).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 3911–3921, Brussels, Belgium. Association for Computational
    Linguistics.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan
    Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang 和 Dragomir Radev.
    2018. [Spider: 大规模人工标注的数据集，用于复杂和跨领域的语义解析和文本到 SQL 任务](https://doi.org/10.18653/v1/D18-1425).
    在 *2018年自然语言处理实证方法会议论文集*，第 3911-3921 页，比利时布鲁塞尔。计算语言学协会。'
- en: Appendix A Examples of Recovering from Entity Linking Errors
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实体链接错误恢复示例
- en: 'Here, we illustrate our proposal of using entity mentions to recover from entity
    linking errors. In the training set, we have the following example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们说明了使用实体提及来从实体链接错误中恢复的提议。在训练集中，我们有如下示例：
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Query: What year did giants win the world series?'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '查询: 巨人队在哪一年赢得了世界大赛?'
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Original Gold SPARQL:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '原始金标 SPARQL:'
- en: SELECT DISTINCT ?x WHERE {
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SELECT DISTINCT ?x WHERE {
- en: ?y wdt:sports_season_of_league_or_competition
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?y wdt:体育赛季_联赛_或_比赛
- en: wd:Q265538;
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wd:Q265538;
- en: wdt:winner wd:Q308966;
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wdt:赢家 wd:Q308966;
- en: wdt:point_in_time ?x. }
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wdt:时间点 ?x. }
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gold Entity linker result:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '金标实体链接器结果:'
- en: World Series (QID Q265538),
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 世界大赛 (QID Q265538),
- en: San Francisco Giants (QID Q308966);
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 旧金山巨人队 (QID Q308966);
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ReFinED result:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ReFinED 结果:'
- en: San Francisco Giants (QID Q308966);
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 旧金山巨人队 (QID Q308966);
- en: 'Here, the ReFinED entity linker model fails to identify the “World Series”
    entity. Our proposal of mentions gives the semantic parser a chance to recover
    from entity linker failures. To train the parser to generate mentions, our training
    includes samples like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ReFinED 实体链接模型未能识别“世界大赛”实体。我们提出的提及方法为语义解析器提供了从实体链接器失败中恢复的机会。为了训练解析器生成提及，我们的训练包括如下样本：
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Query: what year did giants win the world series?'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '查询: 巨人队在哪一年赢得了世界大赛?'
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ReFinED result:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ReFinED 结果:'
- en: San Francisco Giants (QID Q308966);
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 旧金山巨人队 (QID Q308966);
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gold target:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '金标:'
- en: SELECT DISTINCT ?x WHERE {
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SELECT DISTINCT ?x WHERE {
- en: ?y wdt:sports_season_of_league_or_competition;
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ?y wdt:体育赛季_联赛_或_比赛;
- en: wd:world_series;
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wd:世界大赛;
- en: wdt:winner wd:Q308966;
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wdt:赢家 wd:Q308966;
- en: wdt:point_in_time ?x. }
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: wdt:时间点 ?x. }
- en: The gold query mentions “world_series”. At inference time, our heuristics use
    the predicted mention to look up the actual Wikidata entity. For example, if wd:world_series
    is predicted at inference time, our heuristics maps it back to wd:Q265538.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 金标查询提及了“world_series”。在推断时，我们的启发式方法使用预测的提及来查找实际的 Wikidata 实体。例如，如果在推断时预测了 wd:world_series，我们的启发式方法会将其映射回
    wd:Q265538。
