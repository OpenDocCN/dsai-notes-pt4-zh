- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:40:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:40:05'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GrammarGPT: 探索开源 LLM 在本土中文语法错误纠正中的应用与监督调优'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.13923](https://ar5iv.labs.arxiv.org/html/2307.13923)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.13923](https://ar5iv.labs.arxiv.org/html/2307.13923)
- en: '¹¹institutetext: School of Data Science, The Chinese University of Hong Kong,
    Shenzhen, China ²²institutetext: School of Computer Science and Technology, Soochow
    University, China ³³institutetext: Shenzhen Research Institute of Big Data, Shenzhen,
    Guangdong, China ⁴⁴institutetext: School of Information Science and Technology,
    University of Science and Technology of China, China'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹机构文本: 香港中文大学数据科学学院, 深圳, 中国 ²²机构文本: 苏州大学计算机科学与技术学院, 中国 ³³机构文本: 深圳大数据研究院, 深圳,
    广东, 中国 ⁴⁴机构文本: 中国科学技术大学信息科学与技术学院, 中国'
- en: '⁴⁴email: yxfansuda@stu.suda.edu.cn'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '⁴⁴邮箱: yxfansuda@stu.suda.edu.cn'
- en: pfli@suda.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: pfli@suda.edu.cn
- en: '{jeffreyjiang,haizhouli}@cuhk.edu.cnYaxin Fan 112233    Feng Jiang Corresponding
    Author113344    Peifeng Li 22    Haizhou Li 1133'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{jeffreyjiang, haizhouli}@cuhk.edu.cn Yaxin Fan 112233    Feng Jiang 通讯作者 113344
       Peifeng Li 22    Haizhou Li 1133'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Grammatical error correction aims to correct ungrammatical sentences automatically.
    Recently, some work has demonstrated the excellent capabilities of closed-source
    Large Language Models (LLMs, e.g., ChatGPT) in grammatical error correction. However,
    the potential of open-source LLMs remains unexplored. In this paper, we introduced
    GrammarGPT, an open-source LLM, to preliminary explore its potential for native
    Chinese grammatical error correction. The core recipe of GrammarGPT is to leverage
    the hybrid dataset of ChatGPT-generated and human-annotated. For grammatical errors
    with clues, we proposed a heuristic method to guide ChatGPT to generate ungrammatical
    sentences by providing those clues. For grammatical errors without clues, we collected
    ungrammatical sentences from publicly available websites and manually corrected
    them. In addition, we employed an error-invariant augmentation method to enhance
    the ability of the model to correct native Chinese grammatical errors. We ultimately
    constructed about 1k parallel data and utilized these data to fine-tune open-source
    LLMs (e.g., Phoenix, released by The Chinese University of Hong Kong, Shenzhen)
    with instruction tuning. The experimental results show that GrammarGPT outperforms
    the existing SOTA system significantly. Although model parameters are 20x larger
    than the SOTA baseline, the required amount of data for instruction tuning is
    1200x smaller, illustrating the potential of open-source LLMs on native CGEC.
    Our GrammarGPT ranks $3^{rd}$ on NLPCC2023 SharedTask1, demonstrating our approach’s
    effectiveness. The code and data are available at [https://github.com/FreedomIntelligence/GrammarGPT](https://github.com/FreedomIntelligence/GrammarGPT).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 语法错误纠正旨在自动纠正不符合语法的句子。近期的一些研究展示了封闭源大型语言模型（LLMs，例如 ChatGPT）在语法错误纠正方面的卓越能力。然而，开源
    LLM 的潜力仍未被探索。在本文中，我们引入了 GrammarGPT，一个开源 LLM，初步探索其在本土中文语法错误纠正中的潜力。GrammarGPT 的核心方法是利用
    ChatGPT 生成的数据和人工注释的混合数据集。对于有线索的语法错误，我们提出了一种启发式方法，通过提供这些线索来指导 ChatGPT 生成不符合语法的句子。对于没有线索的语法错误，我们从公开可用的网站收集了不符合语法的句子并手动纠正。此外，我们采用了一种错误不变的增强方法，以提高模型纠正本土中文语法错误的能力。最终，我们构建了约
    1k 的平行数据，并利用这些数据对开源 LLM（例如深圳中文大学发布的 Phoenix）进行指令调优。实验结果表明，GrammarGPT 显著优于现有的 SOTA
    系统。尽管模型参数比 SOTA 基线大 20 倍，但进行指令调优所需的数据量却小 1200 倍，这显示了开源 LLM 在本土中文语法错误纠正中的潜力。我们的
    GrammarGPT 在 NLPCC2023 SharedTask1 中排名第 $3^{rd}$，证明了我们方法的有效性。代码和数据可在 [https://github.com/FreedomIntelligence/GrammarGPT](https://github.com/FreedomIntelligence/GrammarGPT)
    获取。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Native Chinese grammatical error correction Large language models ChatGPT Instruction
    tuning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本土中文语法错误纠正 大型语言模型 ChatGPT 指令调优。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Grammatical Error Correction (GEC) aims to automatically correct ungrammatical
    sentences without changing their meaning [[26](#bib.bib26), [10](#bib.bib10),
    [27](#bib.bib27)]. Previous works [[28](#bib.bib28), [13](#bib.bib13), [14](#bib.bib14),
    [26](#bib.bib26)] in Chinese Grammatical Error Correction (CGEC) mainly study
    the errors from foreign Chinese learners, which are very obvious and naive. Therefore,
    recent works  [[27](#bib.bib27), [10](#bib.bib10)] shift to the grammatical errors
    made by native speakers, which are more subtle and challenging. Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ GrammarGPT: Exploring Open-Source LLMs for Native
    Chinese Grammatical Error Correction with Supervised Fine-Tuning") shows the six
    main types of grammatical errors made by native speakers, which can be divided
    into two types, e.g., with (w/) and without (w/o) clues. We can find that the
    incorrect sentences are fluent and in line with the habits of native Chinese.
    However, they do not conform to Chinese grammar, which is more difficult to correct.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '语法错误纠正（GEC）的目标是自动纠正不符合语法的句子而不改变其含义 [[26](#bib.bib26), [10](#bib.bib10), [27](#bib.bib27)]。以往的研究
    [[28](#bib.bib28), [13](#bib.bib13), [14](#bib.bib14), [26](#bib.bib26)] 在中文语法错误纠正（CGEC）方面主要研究了来自外国中文学习者的错误，这些错误非常明显且幼稚。因此，近期的研究
    [[27](#bib.bib27), [10](#bib.bib10)] 转向了由母语者犯的语法错误，这些错误更为微妙和具有挑战性。表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ GrammarGPT: Exploring Open-Source LLMs for Native
    Chinese Grammatical Error Correction with Supervised Fine-Tuning") 展示了母语者常犯的六种主要语法错误，这些错误可以分为两类，例如，有（w/）和没有（w/o）线索。我们可以发现，不正确的句子流畅且符合母语者的习惯。然而，这些句子不符合中文语法，因此更难纠正。'
- en: Previous studies in GEC mainly adopted both Seq2edit [[5](#bib.bib5), [26](#bib.bib26),
    [9](#bib.bib9), [10](#bib.bib10)] and Seq2seq [[7](#bib.bib7), [29](#bib.bib29),
    [15](#bib.bib15)] paradigms and have achieved impressive performance on various
    GEC benchmarks. With the emergence of LLMs, Fang et al. [[4](#bib.bib4)] evaluated
    the performance of closed-source LLMs (e.g., ChatGPT ¹¹1https://chat.openai.com/)
    on GEC and revealed its excellent capabilities for error detection and correction.
    However, the potential of open-source LLMs remains unexplored.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以往在GEC领域的研究主要采用了 Seq2edit [[5](#bib.bib5), [26](#bib.bib26), [9](#bib.bib9),
    [10](#bib.bib10)] 和 Seq2seq [[7](#bib.bib7), [29](#bib.bib29), [15](#bib.bib15)]
    模式，并在各种GEC基准测试中取得了显著的成绩。随着大语言模型（LLMs）的出现，Fang 等 [[4](#bib.bib4)] 评估了闭源LLMs（例如，ChatGPT
    ¹¹1https://chat.openai.com/）在GEC中的表现，并揭示了其在错误检测和纠正方面的优异能力。然而，开源LLMs的潜力仍未得到探索。
- en: In this paper, we introduce GrammarGPT, a novel model for studying the potential
    of open-source LLMs architectures in addressing Native Chinese Grammatical Error
    Correction (CGEC) through supervised fine-tuning. The key challenge in fine-tuning
    LLMs for CGEC is obtaining high-quality parallel data comprising grammatical errors
    made by native speakers. However, manually annotating such data is not only time-consuming
    but also expensive, necessitating the exploration of automatic data annotation
    methods. Recent works [[25](#bib.bib25), [22](#bib.bib22)] have successfully leveraged
    distilled data from ChatGPT and real-world datasets to fine-tune LLMs for specific
    domains, effectively reducing costs while achieving superior performance. Inspired
    by this line of research, we propose a hybrid dataset that incorporates different
    types of native Chinese grammatical errors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 GrammarGPT，这是一种新颖的模型，用于研究开源LLMs架构在通过监督微调解决母语中文语法错误纠正（CGEC）方面的潜力。在对LLMs进行CGEC微调时的主要挑战是获得高质量的平行数据，这些数据包括母语者所犯的语法错误。然而，手动标注这些数据不仅耗时而且昂贵，因此需要探索自动数据标注方法。最近的研究
    [[25](#bib.bib25), [22](#bib.bib22)] 成功地利用了 ChatGPT 的精炼数据和现实世界数据集来微调LLMs，显著降低了成本，同时取得了优异的性能。受到这类研究的启发，我们提出了一种混合数据集，结合了不同类型的母语中文语法错误。
- en: 'Specifically, we first proposed a heuristic method for the grammatical errors
    with clues as shown in Fig. [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning") that guides ChatGPT to generate ungrammatical sentences
    by providing those clues. Then, for those errors without clues, we collected the
    ungrammatical sentences from the public website and corrected them manually. In
    addition, we proposed an error-invariant data augmentation method to enhance the
    diversity of the data by substituting the named entities in parallel data with
    similar ones, which can improve the ability of the model to correct native Chinese
    grammatical errors. We ultimately constructed 1k parallel data and utilized these
    data to fine-tune LLMs with instruction tuning. The experimental results show
    that GrammarGPT can significantly outperform state-of-the-art (SOTA) systems.
    Although the size of model parameters is 20x larger than the SOTA baseline, the
    data for fine-tuning is 1200x smaller, which demonstrated the potential of open-source
    LLMs on Chinese grammatical error correction.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先提出了一种启发式方法来处理带有线索的语法错误，如图 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning") 所示，通过提供这些线索指导 ChatGPT 生成语法错误的句子。然后，对于没有线索的错误，我们从公共网站收集了语法错误的句子并手动纠正。此外，我们提出了一种错误不变的数据增强方法，通过用类似的实体替换平行数据中的命名实体，以增强数据的多样性，从而提高模型纠正中文语法错误的能力。我们最终构建了
    1k 平行数据，并利用这些数据对 LLM 进行指令调优。实验结果表明，GrammarGPT 可以显著优于现有的最先进 (SOTA) 系统。尽管模型参数的规模比
    SOTA 基线大 20 倍，但用于微调的数据却小了 1200 倍，这展示了开源 LLM 在中文语法错误纠正中的潜力。'
- en: 'Table 1: Examples of sentences with various types of grammatical errors. For
    those errors with clues, we can easily detect and correct them. For example, the
    co-occurrence of *超过*(*more than*) and *左右* (*about*) lead to redundant component
    error and we can remove one of them to make the sentence conform to Chinese grammar.
    However, for those errors without clues, a deeper understanding of Chinese grammar
    is required to detect and correct.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：各种类型语法错误的句子示例。对于那些带有线索的错误，我们可以轻松检测和纠正。例如，*超过*（*more than*）和*左右*（*about*）的共现导致了冗余成分错误，我们可以删除其中一个，使句子符合中文语法。然而，对于那些没有线索的错误，需要更深入地理解中文语法才能检测和纠正。
- en: '| w/ Clues |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 有线索 |'
- en: '&#124; Redundant &#124;'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 冗余 &#124;'
- en: '&#124; Component &#124;'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 组成 &#124;'
- en: '&#124; (RC) &#124;'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (RC) &#124;'
- en: '|'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:这座卫星城的人口估计 超过一百万左右。 &#124;'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:这座卫星城的人口估计 超过一百万左右。 &#124;'
- en: '&#124; The population of this satellite city is estimated to be &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这座卫星城的人口估计为 &#124;'
- en: '&#124; more than  about one million. &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超过大约一百万。 &#124;'
- en: '&#124; Correct:这座卫星城的人口估计超过一百万。 &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确:这座卫星城的人口估计超过一百万。 &#124;'
- en: '&#124; The population of this satellite city is estimated to be &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这座卫星城的人口估计为 &#124;'
- en: '&#124; over one million. &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超过一百万。 &#124;'
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Structural &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结构 &#124;'
- en: '&#124; Confusion &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 混淆 &#124;'
- en: '&#124; (SC) &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (SC) &#124;'
- en: '|'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:这次网络故障的原因是由服务器故障引起的。 &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:这次网络故障的原因是由服务器故障引起的。 &#124;'
- en: '&#124; The cause of this network failure is caused by the server failure. &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 造成此网络故障的原因是服务器故障。 &#124;'
- en: '&#124; Correct:这次网络故障的原因是服务器故障。 &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确:这次网络故障的原因是服务器故障。 &#124;'
- en: '&#124; The cause of the network failure is the server failure. &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络故障的原因是服务器故障。 &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improper &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不当 &#124;'
- en: '&#124; Collocation &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 搭配 &#124;'
- en: '&#124; (IC) &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (IC) &#124;'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:西湖区正全面 提升区域产城融合发展的 步伐。 &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:西湖区正全面 提升区域产城融合发展的 步伐。 &#124;'
- en: '&#124; Xihu District is promoting the pace of integration of regional &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 西湖区正在加快区域产业与城市融合的步伐 &#124;'
- en: '&#124; industry and city development. &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行业和城市发展。 &#124;'
- en: '&#124; Correct: 西湖区正全面加快区域产城融合发展的步伐。 &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确: 西湖区正全面加快区域产城融合发展的步伐。 &#124;'
- en: '&#124; Xihu District is accelerating the pace of integration of regional &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 西湖区正在加快区域产业与城市融合的步伐 &#124;'
- en: '&#124; industry and city development. &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行业和城市发展。 &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| w/o Clues |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 无线索 |'
- en: '&#124; Improper &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不当 &#124;'
- en: '&#124; Word Order &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 词序 &#124;'
- en: '&#124; (IWO) &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (IWO) &#124;'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:: 学校三个月内要求每名学生完成20个小时的义工服务。 &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:: 学校三个月内要求每名学生完成20个小时的义工服务。 &#124;'
- en: '&#124; The school in three months requires each student to complete &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学校在三个月内要求每位学生完成 &#124;'
- en: '&#124; 20 hours of volunteer service. &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 20小时的志愿服务。 &#124;'
- en: '&#124; Correct:学校要求每名学生三个月内完成20个小时的义工服务。 &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确:学校要求每名学生三个月内完成20小时的义工服务。 &#124;'
- en: '&#124; The school requires each student to complete 20 hours of &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学校要求每位学生完成20小时的 &#124;'
- en: '&#124; volunteer service in three months. &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三个月内的义工服务。 &#124;'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Improper &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不恰当 &#124;'
- en: '&#124; Logicality &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合理性 &#124;'
- en: '&#124; (IL) &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (IL) &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:集团向社会各界人士、沿途村庄百姓表示歉意。 &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:集团向社会各界人士、沿途村庄百姓表示歉意。 &#124;'
- en: '&#124; The group apologizes to people from all walks of life and &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集团向各界人士表示歉意，并 &#124;'
- en: '&#124; villagers along the way. &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 沿途的村民。 &#124;'
- en: '&#124; Correct:集团向社会各界人士表示歉意。 &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确:集团向社会各界人士表示歉意。 &#124;'
- en: '&#124; The group apologizes to people from all walks of life. &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集团向各界人士表示歉意。 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Missing &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遗漏 &#124;'
- en: '&#124; Component &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 组件 &#124;'
- en: '&#124; (MC) &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (MC) &#124;'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Incorrect:这篇报告控诉了人类破坏大自然(…)。 &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误:这篇报告控诉了人类破坏大自然(…)。 &#124;'
- en: '&#124; The report accused man of destroying nature. &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这篇报告控诉了人类破坏自然的罪行。 &#124;'
- en: '&#124; Correct:这篇报告控诉了人类破坏大自然的罪行。 &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确:这篇报告控诉了人类破坏大自然的罪行。 &#124;'
- en: '&#124; The report accused man the crime of destroying nature. &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这篇报告控诉了人类破坏自然的罪行。 &#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Our contributions are as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献如下：
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, we are the first to explore the potential of open-source
    LLMs with instruction tuning for native Chinese grammatical error correction.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们是首个探索开源LLMs在指令调优中用于中文母语语法错误纠正的研究。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have constructed a hybrid dataset generated by ChatGPT and manual annotation,
    which can effectively cover native Chinese grammatical errors for taming the LLMs
    into an excellent grammar detector.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们构建了一个由ChatGPT和人工标注生成的混合数据集，这可以有效涵盖中文母语语法错误，用于将LLMs驯化为卓越的语法检测器。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We designed an error-invariant data augmentation method to substitute the named
    entities in parallel data with similar ones, making the model more accurate in
    correcting grammatical errors.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种错误不变的数据增强方法，通过用类似的实体替换平行数据中的命名实体，使模型在纠正语法错误时更加准确。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental results show that GrammarGPT can outperform the SOTA system
    significantly, and the data size for instruction tuning is only 1/1200 of the
    SOTA system.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，GrammarGPT可以显著超越现有最佳系统，并且指令调优的数据量仅为现有最佳系统的1/1200。
- en: 2 Related Work
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Grammatical Error Correction
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 语法错误纠正
- en: 'The works in grammatical error correction can be divided into two paradigms:
    the Seq2edit paradigm and the Seq2seq paradigm.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 语法错误纠正的工作可以分为两种范式：Seq2edit范式和Seq2seq范式。
- en: Seq2edit paradigm
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Seq2edit范式
- en: Seq2edit paradigm aims to predict the modification label, including insertion,
    deletion, and substitution, for each position of the sentence iteratively. Hinson
    et al. [[5](#bib.bib5)] proposed a heterogeneous approach to CGEC, composed of
    a NMT-based model, a sequence editing model, and a spell checker. Liang et al.
    [[9](#bib.bib9)] introduced and transferred the BERT-fused NMT model and sequence
    tagging model into the CGEC field. Zhang et al. [[26](#bib.bib26)] proposed a
    multi-reference multi-source evaluation dataset for CGEC and adopted the seq2edit
    method that enhanced with large pre-trained language models. Ma et al. [[10](#bib.bib10)]
    propose a linguistic rules-based approach to construct large-scale CGEC training
    corpora with automatically generated grammatical errors and adopt the seq2edit
    method for evaluation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2edit范式旨在迭代地预测句子每个位置的修改标签，包括插入、删除和替换。Hinson等人[[5](#bib.bib5)]提出了一种异质方法用于CGEC，由基于NMT的模型、序列编辑模型和拼写检查器组成。Liang等人[[9](#bib.bib9)]将BERT融合的NMT模型和序列标记模型引入CGEC领域。Zhang等人[[26](#bib.bib26)]提出了一种多参考多源评估数据集用于CGEC，并采用了通过大型预训练语言模型增强的seq2edit方法。Ma等人[[10](#bib.bib10)]提出了一种基于语言规则的方法，通过自动生成语法错误来构建大规模CGEC训练语料库，并采用seq2edit方法进行评估。
- en: Seq2seq paradigm
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Seq2seq范式
- en: This paradigm treats CGEC as a monolingual translation task. Katsumata and Komachi
    [[7](#bib.bib7)] explored the utility of bidirectional and auto-regressive transformers
    (BART) as a generic pre-trained encoder-decoder model for GEC. Zhao and Wang [[29](#bib.bib29)]
    proposed a simple yet effective method to improve the NMT-based GEC models by
    dynamic masking, which can generate more diverse instances to enhance model generalization.
    Rothe et al. [[15](#bib.bib15)] proposed a language-agnostic method to generate
    a large number of synthetic examples, and then fine-tune large-scale multilingual
    language models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个范式将 CGEC 视为单语翻译任务。Katsumata 和 Komachi [[7](#bib.bib7)] 探索了双向和自回归变换器（BART）作为通用预训练编码器-解码器模型在
    GEC 中的实用性。Zhao 和 Wang [[29](#bib.bib29)] 提出了通过动态掩蔽来改进基于 NMT 的 GEC 模型的简单而有效的方法，该方法能够生成更多样化的实例以增强模型的泛化能力。Rothe
    等人 [[15](#bib.bib15)] 提出了一个语言无关的方法来生成大量合成示例，然后微调大规模多语言模型。
- en: In addition, several works [[9](#bib.bib9), [5](#bib.bib5), [8](#bib.bib8),
    [26](#bib.bib26)] observe the complementary power of the above two paradigms,
    thus promoting the performance through the model ensemble. In this paper, we adopt
    the Se2seq paradigm to fine-tune LLMs with instruction tuning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究 [[9](#bib.bib9), [5](#bib.bib5), [8](#bib.bib8), [26](#bib.bib26)] 观察到上述两种范式的互补性，从而通过模型集成提升性能。在本文中，我们采用
    Se2seq 范式来通过指令微调 LLMs。
- en: 2.2 Instruction Tuning for LLMs
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs 的指令调优
- en: 'Instruction tuning [[21](#bib.bib21), [16](#bib.bib16)] can improve the ability
    of model generalization by learning from a large number of tasks guided by instruction,
    which has been successfully applied to fine-tune LLMs on some specific tasks.
    The work on task-specific instruction tuning can be categorized into three types
    by data sources: ChatGPT-generated, human-annotated, and hybrid dataset of ChatGPT
    and human.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调优 [[21](#bib.bib21), [16](#bib.bib16)] 可以通过从大量任务中学习以指令指导的方式来提高模型的泛化能力，这已成功应用于在一些特定任务上微调
    LLMs。任务特定的指令调优工作可以根据数据来源分为三类：ChatGPT 生成的数据、人类标注的数据和 ChatGPT 与人类的混合数据集。
- en: ChatGPT-generated data
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ChatGPT 生成的数据
- en: Several works adopted the data generated by ChatGPT to fine-tune LLMs in the
    form of instructions. Ho et al. [[6](#bib.bib6)] proposed Fine-tune-CoT, a method
    that generates reasoning samples from LLMS to fine-tune smaller models, which
    enables substantial reasoning capability of small models. Wang et al. [[19](#bib.bib19)]
    proposed SCOTT, a faithful knowledge distillation method to learn a small, self-consistent
    CoT model from a teacher model that is orders of magnitude. Chen et al. [[1](#bib.bib1)]
    explored distilling the reasoning ability of LLMs into a more compact student
    model for multimodal named entity and multimodal relation extraction. Chen et
    al. [[1](#bib.bib1)] proposed a data synthesis framework built upon the data generation
    functions parameterized by LLMs and prompts and used synthesized data to fine-tune
    LLaMA.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究采用了 ChatGPT 生成的数据来以指令的形式微调 LLMs。Ho 等人 [[6](#bib.bib6)] 提出了 Fine-tune-CoT，这是一种生成推理样本以微调较小模型的方法，从而增强小模型的推理能力。Wang
    等人 [[19](#bib.bib19)] 提出了 SCOTT，这是一种忠实的知识蒸馏方法，用于从数量级差异的教师模型中学习一个小而自洽的 CoT 模型。Chen
    等人 [[1](#bib.bib1)] 探索了将 LLMs 的推理能力蒸馏到一个更紧凑的学生模型中，以实现多模态命名实体和多模态关系提取。Chen 等人 [[1](#bib.bib1)]
    提出了一个基于 LLMs 和提示参数化的数据生成函数构建的数据合成框架，并使用合成数据来微调 LLaMA。
- en: Human-annotated data
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工标注的数据
- en: Some works directly convert the supervised data into the format of instructions
    to fine-tune LLMs. Zhang et al. [[24](#bib.bib24)] proposed to fine-tune LLaMA
    [[18](#bib.bib18)] on financial sentiment analysis with a small portion of supervised
    financial sentiment analysis data. Wang et al. [[20](#bib.bib20)] proposed a unified
    information extraction framework based on instruction tuning to model various
    information extraction tasks and capture the inter-task dependency.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究直接将监督数据转换为指令格式来微调 LLMs。Zhang 等人 [[24](#bib.bib24)] 提出了在金融情感分析中用少量监督金融情感分析数据微调
    LLaMA [[18](#bib.bib18)]。Wang 等人 [[20](#bib.bib20)] 提出了一个基于指令调优的统一信息提取框架，用于建模各种信息提取任务并捕捉任务间的依赖关系。
- en: Hybrid dataset of ChatGPT and human
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ChatGPT 和人类的混合数据集
- en: Recently, some works utilized the hybrid data of humans and ChatGPT/GPT-4 to
    fine-tune LLMs. Zhang et al. [[25](#bib.bib25)] proposed to leverage both distilled
    data from ChatGPT and real-world data from doctors to fine-tune Bloom [[17](#bib.bib17)].
    Yu et al. [[22](#bib.bib22)] adopted a hybrid data of Chinese education and general-domain
    instructions [[12](#bib.bib12)] generated by GPT-4 to fine-tune LLaMA [[18](#bib.bib18)].
    In this paper, we follow this line and fine-tune LLMs on native CGEC with the
    hybrid dataset of ChatGPT-generated and human-annotated with instruction tuning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究利用了人类和 ChatGPT/GPT-4 的混合数据来对 LLMs 进行微调。Zhang 等人 [[25](#bib.bib25)] 提出了利用来自
    ChatGPT 的提炼数据和来自医生的真实世界数据来对 Bloom [[17](#bib.bib17)] 进行微调。Yu 等人 [[22](#bib.bib22)]
    采用了由 GPT-4 生成的中文教育和通用领域指令 [[12](#bib.bib12)] 的混合数据来对 LLaMA [[18](#bib.bib18)]
    进行微调。本文遵循这一思路，在本地 CGEC 上使用 ChatGPT 生成和人工标注的混合数据集进行 LLMs 的微调。
- en: '![Refer to caption](img/511c4823d6895e17ef1f1d1f57551d5c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/511c4823d6895e17ef1f1d1f57551d5c.png)'
- en: 'Figure 1: The framework of our method.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们方法的框架。
- en: 3 Methods
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Fig. [1](#S2.F1 "Figure 1 ‣ Hybrid dataset of ChatGPT and human ‣ 2.2 Instruction
    Tuning for LLMs ‣ 2 Related Work ‣ GrammarGPT: Exploring Open-Source LLMs for
    Native Chinese Grammatical Error Correction with Supervised Fine-Tuning") illustrates
    the framework of our method, which involves the construction of parallel data
    comprising six types of native Chinese grammatical errors to facilitate the fine-tuning
    of open-source Language Model (LLMs). While human-annotated data offer high-quality
    samples, the associated high cost remains a significant concern. To address this,
    we adopt a compromise approach. We first guide ChatGPT to generate ungrammatical
    sentences with clues by providing those clues collected from the Internet. Then,
    we annotate the ungrammatical sentences without clues collected from the Internet.
    Additionally, we propose an error-invariant augmentation technique to substitute
    named entities in the parallel data with similar ones, further enhancing the model’s
    capability to correct native Chinese grammatical errors. Finally, we convert the
    parallel data into instructions, which are then utilized for fine-tuning LLMs.
    Detailed explanations of these steps are provided in the following subsections.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S2.F1 "图 1 ‣ ChatGPT 和人工混合数据集 ‣ 2.2 LLM 指令调整 ‣ 2 相关工作 ‣ GrammarGPT：探索用于本地中文语法错误修正的开源
    LLM 通过监督微调") 说明了我们方法的框架，该框架涉及构建包含六种类型的本地中文语法错误的平行数据，以便于对开源语言模型（LLMs）进行微调。虽然人工标注的数据提供了高质量的样本，但相关的高成本仍然是一个显著的关注点。为了解决这一问题，我们采取了一种折中方法。我们首先引导
    ChatGPT 通过提供从互联网收集的提示生成不规范句子。然后，我们对那些没有从互联网收集的提示的不规范句子进行标注。此外，我们提出了一种错误不变增强技术，用于用相似的实体替换平行数据中的命名实体，从而进一步增强模型纠正本地中文语法错误的能力。最后，我们将平行数据转换为指令，并用于对
    LLMs 进行微调。以下子章节提供了这些步骤的详细解释。
- en: 3.1 Hybrid Dataset Construction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 混合数据集构建
- en: 3.1.1 ChatGPT-generated Data
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 ChatGPT 生成的数据
- en: 'As shown in the first three lines of Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning"), the grammatical errors with clues are
    easy to detect and correct by recognizing the specific clues. For example, *”more
    than”* and *”about”* are used together leading to redundant component, *”The cause”*
    and *”caused by”* are used together leading to structural confusion, and *”prompting”*
    and *”pace”* are used together leading to improper collocation. Conversely, we
    can construct the ungrammatical sentences by inserting these cues into grammatical
    sentences. Thanks to the strong capabilities of ChatGPT, we can instruct ChatGPT
    to generate the ungrammatical sentences that meet our requirements by providing
    these clues collected from public websites ²²2https://wenku.baidu.com. An example
    is as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1.2 Human-annotated Data ‣ 3.1 Hybrid
    Dataset Construction ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source LLMs for
    Native Chinese Grammatical Error Correction with Supervised Fine-Tuning").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning")前三行所示，有线索的语法错误容易通过识别特定线索来检测和纠正。例如，*“more
    than”* 和 *“about”* 组合导致冗余成分，*“The cause”* 和 *“caused by”* 组合导致结构混乱，*“prompting”*
    和 *“pace”* 组合导致搭配不当。相反，我们可以通过将这些线索插入语法正确的句子中来构造不符合语法的句子。得益于ChatGPT强大的能力，我们可以通过提供从公开网站²²2https://wenku.baidu.com收集的这些线索来指导ChatGPT生成符合我们要求的不符合语法的句子。示例如图[2](#S3.F2
    "Figure 2 ‣ 3.1.2 Human-annotated Data ‣ 3.1 Hybrid Dataset Construction ‣ 3 Methods
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning")所示。'
- en: 3.1.2 Human-annotated Data
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 人工标注数据
- en: 'Some types of native ungrammatical errors are hard to recognize, as shown in
    the last three lines of Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning"). We can find that those ungrammatical sentences are fluent
    and with no obvious clues of grammatical errors can help us to recognize them.
    For these types of grammatical errors, we mainly collected ungrammatical sentences
    from publicly available websites³³3https://tiku.baidu.com/ and then manually annotated
    them.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '某些类型的母语语法错误难以识别，如表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ GrammarGPT: Exploring
    Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised
    Fine-Tuning")最后三行所示。我们发现这些语法错误的句子流畅且没有明显的语法错误线索，因此难以识别。对于这些类型的语法错误，我们主要从公开网站³³3https://tiku.baidu.com/收集了不符合语法的句子，并进行人工标注。'
- en: '![Refer to caption](img/906f19d8db823dba2f721b356e21dd08.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/906f19d8db823dba2f721b356e21dd08.png)'
- en: 'Figure 2: Process of ungrammatical sentences generated by ChatGPT.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ChatGPT生成的不符合语法的句子的过程。'
- en: '![Refer to caption](img/ed4a5a7c9bfc9ae718d85140563510f3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ed4a5a7c9bfc9ae718d85140563510f3.png)'
- en: 'Figure 3: An example of error-invariant augmentation.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 错误不变增强的示例。'
- en: 3.2 Error-invariant Data Augmentation
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 错误不变数据增强
- en: 'To prioritize the model’s focus on native grammar errors and improve its robustness,
    we have devised an error-invariant augmentation method, as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ 3.1.2 Human-annotated Data ‣ 3.1 Hybrid Dataset Construction ‣ 3 Methods
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning"). Native Chinese grammatical errors are
    often subtle and infrequently found in the position of named entities. To address
    this, we adopt a strategy of substituting the named entities in the parallel data
    with similar ones⁴⁴4https://github.com/chatopera/Synonyms. By employing this augmentation
    method, the model can concentrate on identifying unchanged errors rather than
    specific nouns, thereby improving its performance in correcting subtle and imperceptible
    grammar errors.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '为了优先关注模型对母语语法错误的识别并提高其鲁棒性，我们设计了一种错误不变增强方法，如图[3](#S3.F3 "Figure 3 ‣ 3.1.2 Human-annotated
    Data ‣ 3.1 Hybrid Dataset Construction ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning")所示。母语汉语语法错误往往微妙且不常出现在命名实体的位置。为了解决这一问题，我们采用了用相似命名实体替换平行数据中的命名实体的策略⁴⁴4https://github.com/chatopera/Synonyms。通过这种增强方法，模型可以集中识别不变的错误，而不是特定名词，从而提高纠正微妙和难以察觉的语法错误的表现。'
- en: 3.3 Instruction Tuning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 指令调优
- en: 'Instruction tuning[[21](#bib.bib21), [16](#bib.bib16)] has emerged as the mainstream
    approach for fine-tuning LLMs by providing explicit instructions to enhance model
    comprehension. In this paper, we followed this mainstream trend and fine-tuned
    LLMs with instruction tuning. Instruction details are as shown in Table [2](#S3.T2
    "Table 2 ‣ 3.3 Instruction Tuning ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning"),
    which mainly consists of four components.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '指令微调[[21](#bib.bib21), [16](#bib.bib16)]已经成为微调LLM的主流方法，通过提供明确的指令来提升模型理解能力。本文遵循了这一主流趋势，使用指令微调对LLM进行了微调。指令细节如表[2](#S3.T2
    "表2 ‣ 3.3 指令微调 ‣ 3 方法 ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese
    Grammatical Error Correction with Supervised Fine-Tuning")所示，主要包括四个组件。'
- en: '1\. Task prefix: This component guides LLMs to assume the role of an AI assistant.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 任务前缀：该组件指导LLM假设AI助手的角色。
- en: '2\. Task description: Here, the specific task that LLMs are required to accomplish
    is outlined.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 任务描述：在这里，详细说明了LLM需要完成的具体任务。
- en: '3\. Input: This corresponds to ungrammatical sentences that are used as input
    during the fine-tuning process.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 输入：这对应于在微调过程中用作输入的语法不正确的句子。
- en: '4\. Output: This represents grammatical sentences, which serve as the expected
    output during fine-tuning.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 输出：这表示语法正确的句子，作为微调过程中期望的输出。
- en: 'Table 2: Components of an instruction.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：指令的组成部分。
- en: '| Instruction |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 指令 |'
- en: '&#124; {Task Prefix} &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {任务前缀} &#124;'
- en: '&#124; Human:{Task Description} {Input} Assistant :{Output} &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类:{任务描述} {输入} 助手 :{输出} &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Task Prefix |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 任务前缀 |'
- en: '&#124; A chat between a curious human and an artificial &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类与人工智能之间的对话 &#124;'
- en: '&#124; intelligence assistant. The assistant gives helpful, &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 智能助手。助手提供有用的、详细且礼貌的回答。 &#124;'
- en: '&#124; detailed, and polite answers to the human’s questions. &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 详细且礼貌地回答人类的问题。 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Task Description | Evaluate this sentence for grammar mistake |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 任务描述 | 评估这个句子的语法错误 |'
- en: '| Input | *Ungrammatical sentence* |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | *语法不正确的句子* |'
- en: '| Output | *Grammatical sentence* |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | *语法正确的句子* |'
- en: 'Table 3: Statistic of the dataset.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：数据集统计。
- en: '| Dataset | Number | Percentage of Different Grammatical Errors (%) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数量 | 不同语法错误的百分比 (%) |'
- en: '| ChatGPT-generated | Human-annotated |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT生成 | 人工标注 |'
- en: '|  RC |  SC |  IC | IWO |  IL | MC |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  RC |  SC |  IC | IWO |  IL | MC |'
- en: '| training set | 1061 |  23.54 |  28.25 |  13.70 | 6.50 |  13.18 | 15.07 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | 1061 |  23.54 |  28.25 |  13.70 | 6.50 |  13.18 | 15.07 |'
- en: '| validating set | 500 |  - |  - |  - |  - |  - |  - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 验证集 | 500 |  - |  - |  - |  - |  - |  - |'
- en: 4 Experiments
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: 'We constructed a total of 1061 parallel data samples for training, and the
    data statistics are provided in Table [3](#S3.T3 "Table 3 ‣ 3.3 Instruction Tuning
    ‣ 3 Methods ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical
    Error Correction with Supervised Fine-Tuning"). Roughly 35% of the data were manually
    annotated, while the remaining 65% were generated using ChatGPT. To evaluate the
    performance of our model, we utilized the validating set available on the NLPCC2023
    SharedTask1 website⁵⁵5https://github.com/masr2000/NaCGEC, which consists of 500
    parallel data samples. We report the model’s performance on this validating set
    for all the experiments conducted.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '我们共构建了1061个用于训练的平行数据样本，数据统计见表[3](#S3.T3 "表3 ‣ 3.3 指令微调 ‣ 3 方法 ‣ GrammarGPT:
    Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with
    Supervised Fine-Tuning")。大约35%的数据是手工标注的，其余65%是使用ChatGPT生成的。为了评估模型的表现，我们利用了NLPCC2023
    SharedTask1网站⁵⁵5https://github.com/masr2000/NaCGEC上提供的验证集，该验证集包含500个平行数据样本。我们报告了模型在这个验证集上的表现，涵盖了所有进行的实验。'
- en: 4.2 Metrics
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: The evaluation of a grammatical error correction system relies on the extent
    to which its proposed corrections or edits align with the gold-standard edits
    [[11](#bib.bib11)]. In line with previous research [[10](#bib.bib10), [26](#bib.bib26)],
    we adopt the word-level and char-level MaxMatch (M2) Scorer [[3](#bib.bib3)] for
    evaluation⁶⁶6https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT.
    This scorer computes Precision, Recall, and F[0.5] scores, comparing the gold
    edit set with the system edit set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对语法错误纠正系统的评估依赖于其提出的修正或编辑与金标准编辑的对齐程度[[11](#bib.bib11)]。根据之前的研究[[10](#bib.bib10),
    [26](#bib.bib26)]，我们采用了单词级别和字符级别的MaxMatch (M2) 评分器[[3](#bib.bib3)]进行评估⁶⁶6https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT。该评分器计算了精确度、召回率和F[0.5]分数，通过比较金标准编辑集和系统编辑集。
- en: 4.3 Hyper-parameters
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 超参数
- en: 'The models are implemented in PyTorch using the Huggingface Transformers⁷⁷7https://huggingface.co/.
    We used phoenix-inst-chat-7b ⁸⁸8https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b
    [[2](#bib.bib2)] as the backbone. We set the max sequence length to 256\. The
    model is trained with the AdamW optimizer, where the batch size and epoch are
    set to 64 and 3, respectively. We set the learning rate and the schedule type
    of learning rate to 2e-5 and ’linear’, respectively. The warmup step is set to
    5\. The hyper-parameters are shown in Table [4](#S4.T4 "Table 4 ‣ 4.3 Hyper-parameters
    ‣ 4 Experiments ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical
    Error Correction with Supervised Fine-Tuning").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在PyTorch中使用Huggingface Transformers⁷⁷7https://huggingface.co/实现。我们使用了phoenix-inst-chat-7b
    ⁸⁸8https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b [[2](#bib.bib2)]
    作为主干。我们将最大序列长度设置为256。模型使用AdamW优化器进行训练，批量大小和轮次分别设置为64和3。学习率和学习率调度类型分别设置为2e-5和’线性’。预热步骤设置为5。超参数见表[4](#S4.T4
    "表 4 ‣ 4.3 超参数 ‣ 4 实验 ‣ GrammarGPT：探索开源LLM在本地中文语法错误纠正中的监督微调")。
- en: 'Table 4: Details of hyper-parameters.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：超参数详细信息。
- en: '| Backbone | phoenix-inst-chat-7b |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 主干 | phoenix-inst-chat-7b |'
- en: '| --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Max length | 256 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 最大长度 | 256 |'
- en: '| Optimizer | AdamW |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW |'
- en: '| Batch size | 64 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 64 |'
- en: '| Epoch | 1 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 1 |'
- en: '| Learning rate | 2e-5 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 2e-5 |'
- en: '| Lr schedule type | Linear |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度类型 | 线性 |'
- en: '| Warmup steps | 5 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 预热步骤 | 5 |'
- en: 4.4 Experimental Results
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实验结果
- en: 'To validate the effectiveness of our method, we conducted a comparison between
    our GrammarGPT and the state-of-the-art (SOTA) baseline, S2S_BART [[26](#bib.bib26)].
    S2S_BART utilizes Chinese BART as the pre-trained model and fine-tunes it on the
    Lang8 [[28](#bib.bib28)] and HSK [[23](#bib.bib23)] datasets, which consist of
    approximately 1.2 million parallel data samples. We also fine-tuned S2S_BART on
    the hybrid dataset that we constructed, and the results are presented in Table [5](#S4.T5
    "Table 5 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ GrammarGPT: Exploring Open-Source
    LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning").'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证我们方法的有效性，我们将GrammarGPT与最先进的（SOTA）基准S2S_BART [[26](#bib.bib26)]进行了比较。S2S_BART使用中文BART作为预训练模型，并在Lang8
    [[28](#bib.bib28)] 和HSK [[23](#bib.bib23)] 数据集上进行微调，这些数据集包含大约120万对平行数据样本。我们还在我们构建的混合数据集上对S2S_BART进行了微调，结果见表[5](#S4.T5
    "表 5 ‣ 4.4 实验结果 ‣ 4 实验 ‣ GrammarGPT：探索开源LLM在本地中文语法错误纠正中的监督微调")。
- en: Remarkably, we observed that S2S_BART trained on our 1k hybrid dataset achieved
    17.57 and 18.16 $F_{0.5}$ on Word-level and Char-level separately, which is comparable
    to that baseline model using the 1.2M data from foreign language speakers. We
    attribute this to the significant discrepancy between the grammatical errors made
    by foreign language speakers and native Chinese speakers, making it challenging
    to effectively improve the performance of native CGEC by relying solely on data
    from foreign language speakers. These results further highlight the effectiveness
    of our method in constructing a hybrid dataset that contains native Chinese grammatical
    errors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们观察到在我们的1k混合数据集上训练的S2S_BART在词级和字符级上分别达到了17.57和18.16 $F_{0.5}$，这与使用来自外语说话者的1.2M数据的基准模型相当。我们将这一结果归因于外语说话者与本地中文说话者之间的语法错误差异，这使得仅依靠外语数据很难有效提高本地CGEC的性能。这些结果进一步突显了我们在构建包含本地中文语法错误的混合数据集方法的有效性。
- en: Furthermore, our GrammarGPT exhibited substantial improvement with only about
    1k data samples for fine-tuning, achieving 32.56 and 35.84 $F_{0.5}$ ⁹⁹9https://github.com/masr2000/NaCGEC.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的GrammarGPT在仅用约1k数据样本进行微调时表现出显著的改进，达到了32.56和35.84 $F_{0.5}$ ⁹⁹9https://github.com/masr2000/NaCGEC。
- en: 'Table 5: Performance comparison between GrammarGPT and the SOTA baseline.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：GrammarGPT与SOTA基准的性能比较。
- en: '| Model | #Param. | Data | Data size | Word-level | Char-level |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数数量 | 数据 | 数据大小 | 词级 | 字符级 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Prec | Rec | F[0.5] | Prec | Rec | F[0.5] |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 召回率 | F[0.5] | 精确度 | 召回率 | F[0.5] |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| S2S_BART | 375M |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| S2S_BART | 375M |'
- en: '&#124; Lang8 &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Lang8 &#124;'
- en: '&#124; HSK &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HSK &#124;'
- en: '| 1.2M | 22.31 | 10.14 | 17.99 | 22.13 | 9.66 | 17.59 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 1.2M | 22.31 | 10.14 | 17.99 | 22.13 | 9.66 | 17.59 |'
- en: '| S2S_BART | 375M | Ours | 1061 | 21.08 | 10.54 | 17.57 | 22.09 | 10.62 | 18.16
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| S2S_BART | 375M | 我们的 | 1061 | 21.08 | 10.54 | 17.57 | 22.09 | 10.62 | 18.16
    |'
- en: '| GrammarGPT | 7B | Ours | 1061 | 42.42 | 16.87 | 32.56 | 46.67 | 18.58 | 35.84
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| GrammarGPT | 7B | 我们的 | 1061 | 42.42 | 16.87 | 32.56 | 46.67 | 18.58 | 35.84
    |'
- en: 'Table 6: Ablation study of our method.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：我们方法的消融研究。
- en: '| Data | Word-level | Char-level |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 词级别 | 字符级别 |'
- en: '| Prec | Rec | F[0.5] | Prec | Rec | F[0.5] |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 召回率 | F[0.5] | 精确度 | 召回率 | F[0.5] |'
- en: '| w/o Augmentation | Human-annotated | 12.20 | 1.51 | 5.04 | 13.89 | 1.48 |
    5.19 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 无增强 | 人工标注 | 12.20 | 1.51 | 5.04 | 13.89 | 1.48 | 5.19 |'
- en: '| ChatGPT-generated | 30.38 | 7.21 | 18.49 | 30.86 | 7.35 | 18.83 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT生成 | 30.38 | 7.21 | 18.49 | 30.86 | 7.35 | 18.83 |'
- en: '| Hybrid dataset | 41.76 | 11.45 | 27.30 | 44.32 | 11.50 | 28.22 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 混合数据集 | 41.76 | 11.45 | 27.30 | 44.32 | 11.50 | 28.22 |'
- en: '| w/ Augmentation | Human-annotated | 15.46 | 4.52 | 10.42 | 16.48 | 4.44 |
    10.68 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | 人工标注 | 15.46 | 4.52 | 10.42 | 16.48 | 4.44 | 10.68 |'
- en: '| ChatGPT-generated | 43.75 | 6.33 | 20.04 | 44.90 | 6.49 | 20.56 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT生成 | 43.75 | 6.33 | 20.04 | 44.90 | 6.49 | 20.56 |'
- en: '| Hybrid dataset | 42.42 | 16.87 | 32.56 | 46.87 | 18.58 | 35.84 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 混合数据集 | 42.42 | 16.87 | 32.56 | 46.87 | 18.58 | 35.84 |'
- en: 4.5 Ablation Study
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   4.5 消融研究'
- en: 'In our analysis of the impact of our contributions, namely the construction
    of a hybrid dataset and the error-invariant augmentation method, we present the
    results in Table [6](#S4.T6 "Table 6 ‣ 4.4 Experimental Results ‣ 4 Experiments
    ‣ GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error
    Correction with Supervised Fine-Tuning").'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在我们对贡献的影响分析中，即混合数据集的构建和错误不变增强方法，我们在表[6](#S4.T6 "表 6 ‣ 4.4 实验结果 ‣ 4 实验 ‣
    GrammarGPT：探索开源 LLM 用于本地中文语法错误纠正的监督微调")中展示了结果。'
- en: Notably, the model trained on ChatGPT-generated data consistently outperforms
    that trained the human-annotated data, irrespective of whether data augmentation
    is applied. We attribute this observation to two primary reasons. First, the quantity
    of human-annotated data is smaller than the data generated by ChatGPT due to the
    high cost of human annotation. Second, grammatical errors without clues are more
    challenging to correct.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '-   值得注意的是，训练在ChatGPT生成数据上的模型始终优于训练在人工标注数据上的模型，无论是否应用数据增强。我们将这一观察归因于两个主要原因。首先，由于人工标注的高成本，人工标注数据的数量小于ChatGPT生成的数据。其次，没有提示的语法错误更难纠正。'
- en: Additionally, our hybrid dataset demonstrates the potential for enhancing the
    performance of native CGEC. This finding substantiates the effectiveness of our
    approach in constructing the hybrid dataset consisting of native Chinese grammatical
    errors.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '-   此外，我们的混合数据集展示了提升本地中文语法错误纠正性能的潜力。这一发现证实了我们在构建包含本地中文语法错误的混合数据集方面的方法的有效性。'
- en: Moreover, by employing the error-invariant augmentation method, we observe our
    model trained on hybrid dataset has significant improvements in Recall and F[0.5]
    metrics but only minor improvements in Precision. It indicates that our augmentation
    technique enhances the model’s ability to detect grammatical errors by forcing
    the model to pay more attention to grammar errors in the augmentation data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '-   此外，通过采用错误不变增强方法，我们观察到在混合数据集上训练的模型在召回率和F[0.5]指标上有显著提升，但在精确度上的改善仅为轻微。这表明，我们的增强技术通过迫使模型更加关注增强数据中的语法错误，提高了模型检测语法错误的能力。'
- en: 5 Conclusion
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   5 结论'
- en: In this paper, we introduce GrammarGPT, an open-source Large Language Model
    (LLM) specifically designed for native Chinese grammatical error correction. We
    first construct a hybrid dataset containing approximately 1k parallel data samples.
    It comprises both ChatGPT-generated data and human-annotated data for dealing
    with grammatical errors with and without clues. Additionally, we introduced an
    error-invariant augmentation method to improve the model’s capabilities in native
    Chinese grammatical error correction by forcing the model to pay more attention
    to grammar errors in the augmentation data. We further fine-tune the open-source
    large-scale language model on the constructed dataset. Experimental results and
    in-depth analysis demonstrate the effectiveness of our GrammarGPT in native Chinese
    grammatical error correction.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在本文中，我们介绍了GrammarGPT，一种专为本地中文语法错误纠正设计的开源大型语言模型（LLM）。我们首先构建了一个包含约1k对齐数据样本的混合数据集。该数据集包括ChatGPT生成的数据和人工标注的数据，以处理有提示和无提示的语法错误。此外，我们引入了一种错误不变增强方法，通过迫使模型更加关注增强数据中的语法错误，来提高模型在本地中文语法错误纠正中的能力。我们进一步在构建的数据集上对开源大规模语言模型进行了微调。实验结果和深入分析证明了我们GrammarGPT在本地中文语法错误纠正中的有效性。'
- en: Acknowledge
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   致谢'
- en: This work is supported by the National Natural Science Foundation of China (Grant
    No. 62271432) and the Guangdong Provincial Key Laboratory of Big Data Computing,
    The Chinese University of Hong Kong, Shenzhen (Grant No. B10120210117).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了中国国家自然科学基金（资助编号：62271432）和香港中文大学（深圳）大数据计算广东省重点实验室（资助编号：B10120210117）的支持。
- en: References
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chen, F., Feng, Y.: Chain-of-Thought Prompt Distillation for Multimodal
    Named Entity and Multimodal Relation Extraction. ArXiv preprint arXiv:2306.14122
    (2023)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Chen, F., Feng, Y.：链式思维提示蒸馏用于多模态命名实体和多模态关系提取。arXiv预印本 arXiv:2306.14122（2023）'
- en: '[2] Chen, Z., Jiang, F., Chen, J., Wang, T., Yu, F., Chen, G., Zhang, H., Liang,
    J., Zhang, C., Zhang, Z., et al.: Phoenix: Democratizing ChatGPT across languages.
    arXiv preprint arXiv:2304.10453 (2023)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Chen, Z., Jiang, F., Chen, J., Wang, T., Yu, F., Chen, G., Zhang, H., Liang,
    J., Zhang, C., Zhang, Z., 等：Phoenix：跨语言民主化ChatGPT。arXiv预印本 arXiv:2304.10453（2023）'
- en: '[3] Dahlmeier, D., Ng, H.T.: Better Evaluation for Grammatical Error Correction.
    In: Proceedings of the 2012 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies. pp. 568–572\. Association
    for Computational Linguistics, Montréal, Canada (Jun 2012)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Dahlmeier, D., Ng, H.T.：更好的语法错误纠正评估。收录于：2012年北美计算语言学协会会议：人类语言技术。第568–572页。计算语言学协会，蒙特利尔，加拿大（2012年6月）'
- en: '[4] Fang, T., Yang, S., Lan, K., Wong, D.F., Hu, J., Chao, L.S., Zhang, Y.:
    Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive
    Evaluation. arXiv preprint arXiv:2304.01746 (2023)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Fang, T., Yang, S., Lan, K., Wong, D.F., Hu, J., Chao, L.S., Zhang, Y.：ChatGPT是一个高流畅度的语法错误纠正系统吗？全面评估。arXiv预印本
    arXiv:2304.01746（2023）'
- en: '[5] Hinson, C., Huang, H.H., Chen, H.H.: Heterogeneous Recycle Generation for
    Chinese Grammatical Error Correction. In: Proceedings of the 28th International
    Conference on Computational Linguistics. pp. 2191–2201 (2020)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Hinson, C., Huang, H.H., Chen, H.H.：异质回收生成用于中文语法错误纠正。收录于：第28届国际计算语言学会议论文集。第2191–2201页（2020）'
- en: '[6] Ho, N., Schmid, L., Yun, S.Y.: Large Language Models Are Reasoning Teachers.
    arXiv preprint arXiv:2212.10071 (2022)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Ho, N., Schmid, L., Yun, S.Y.：大型语言模型是推理老师。arXiv预印本 arXiv:2212.10071（2022）'
- en: '[7] Katsumata, S., Komachi, M.: Stronger Baselines for Grammatical Error Correction
    Using a Pretrained Encoder-Decoder Model. In: Proceedings of the 1st Conference
    of the Asia-Pacific Chapter of the Association for Computational Linguistics and
    the 10th International Joint Conference on Natural Language Processing. pp. 827–832
    (2020)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Katsumata, S., Komachi, M.：使用预训练的编码器-解码器模型的更强基准用于语法错误纠正。收录于：第1届亚太计算语言学协会分会会议和第10届国际联合自然语言处理会议论文集。第827–832页（2020）'
- en: '[8] Li, J., Guo, J., Zhu, Y., Sheng, X., Jiang, D., Ren, B., Xu, L.: Sequence-to-Action:
    Grammatical Error Correction with Action Guided Sequence Generation. Proceedings
    of the AAAI Conference on Artificial Intelligence 36(10), 10974–10982 (2022)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Li, J., Guo, J., Zhu, Y., Sheng, X., Jiang, D., Ren, B., Xu, L.：序列到动作：通过动作引导序列生成进行语法错误纠正。AAAI人工智能会议论文集
    36(10)，第10974–10982页（2022）'
- en: '[9] Liang, D., Zheng, C., Guo, L., Cui, X., Xiong, X., Rong, H., Dong, J.:
    BERT Enhanced Neural Machine Translation and Sequence Tagging Model for Chinese
    Grammatical Error Diagnosis. In: Proceedings of the 6th Workshop on Natural Language
    Processing Techniques for Educational Applications. pp. 57–66. Association for
    Computational Linguistics (2020)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Liang, D., Zheng, C., Guo, L., Cui, X., Xiong, X., Rong, H., Dong, J.：BERT增强的神经机器翻译和序列标注模型用于中文语法错误诊断。收录于：第6届自然语言处理技术教育应用研讨会论文集。第57–66页。计算语言学协会（2020）'
- en: '[10] Ma, S., Li, Y., Sun, R., Zhou, Q., Huang, S., Zhang, D., Yangning, L.,
    Liu, R., Li, Z., Cao, Y., Zheng, H., Shen, Y.: Linguistic Rules-Based Corpus Generation
    for Native Chinese Grammatical Error Correction. In: Findings of the Association
    for Computational Linguistics: EMNLP 2022\. pp. 576–589 (2022)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ma, S., Li, Y., Sun, R., Zhou, Q., Huang, S., Zhang, D., Yangning, L.,
    Liu, R., Li, Z., Cao, Y., Zheng, H., Shen, Y.：基于语言规则的原生中文语法错误纠正语料库生成。收录于：计算语言学协会的发现：EMNLP
    2022。第576–589页（2022）'
- en: '[11] Ng, H.T., Wu, S.M., Briscoe, T., Hadiwinoto, C., Susanto, R.H., Bryant,
    C.: The CoNLL-2014 Shared Task on Grammatical Error Correction. In: Proceedings
    of the Eighteenth Conference on Computational Natural Language Learning: Shared
    Task. pp. 1–14 (2014)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Ng, H.T., Wu, S.M., Briscoe, T., Hadiwinoto, C., Susanto, R.H., Bryant,
    C.：CoNLL-2014语法错误纠正共享任务。收录于：第十八届计算自然语言学习会议：共享任务。第1–14页（2014）'
- en: '[12] Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction Tuning with
    GPT-4. arXiv preprint arXiv:2304.03277 (2023)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Peng, B., Li, C., He, P., Galley, M., Gao, J.: 使用 GPT-4 的指令调优。arXiv 预印本
    arXiv:2304.03277 (2023)'
- en: '[13] Rao, G., Gong, Q., Zhang, B., Xun, E.: Overview of NLPTEA-2018 Share Task
    Chinese Grammatical Error Diagnosis. In: Proceedings of the 5th Workshop on Natural
    Language Processing Techniques for Educational Applications. pp. 42–51 (2018)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Rao, G., Gong, Q., Zhang, B., Xun, E.: NLPTEA-2018 共享任务中文语法错误诊断概述。收录于第五届教育应用自然语言处理技术研讨会论文集。第
    42–51 页 (2018)'
- en: '[14] Rao, G., Yang, E., Zhang, B.: Overview of NLPTEA-2020 Shared Task for
    Chinese grammatical error diagnosis. In: Proceedings of the 6th Workshop on Natural
    Language Processing Techniques for Educational Applications. pp. 25–35 (2020)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Rao, G., Yang, E., Zhang, B.: NLPTEA-2020 共享任务中文语法错误诊断概述。收录于第六届教育应用自然语言处理技术研讨会论文集。第
    25–35 页 (2020)'
- en: '[15] Rothe, S., Mallinson, J., Malmi, E., Krause, S., Severyn, A.: A Simple
    Recipe for Multilingual Grammatical Error Correction. In: Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 2: Short Papers). pp.
    702–707 (2021)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Rothe, S., Mallinson, J., Malmi, E., Krause, S., Severyn, A.: 多语言语法错误纠正的简单方法。收录于第
    59 届计算语言学协会年会和第 11 届国际联合自然语言处理会议（第 2 卷：短论文）论文集。第 702–707 页 (2021)'
- en: '[16] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A., et al.: Multitask Prompted
    Training Enables Zero-shot Task Generalization. arXiv preprint arXiv:2110.08207
    (2021)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A., 等: 多任务提示训练使零-shot 任务泛化成为可能。arXiv
    预印本 arXiv:2110.08207 (2021)'
- en: '[17] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné,
    R., Luccioni, A.S., Yvon, F., Gallé, M., et al.: Bloom: A 176B-parameter Open-access
    Multilingual Language Model. arXiv preprint arXiv:2211.05100 (2022)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Scao, T.L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné,
    R., Luccioni, A.S., Yvon, F., Gallé, M., 等: Bloom: 一个 176B 参数的开放获取多语言模型。arXiv
    预印本 arXiv:2211.05100 (2022)'
- en: '[18] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (2023)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., Lample, G.: LLaMA: 开放且高效的基础语言模型 (2023)'
- en: '[19] Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., Ren, X.: SCOTT: Self-Consistent
    Chain-of-Thought Distillation. arXiv preprint arXiv:2305.01879 (2023)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., Ren, X.: SCOTT: 自一致链式思维蒸馏。arXiv
    预印本 arXiv:2305.01879 (2023)'
- en: '[20] Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., Zheng, R., Ye,
    J., Zhang, Q., Gui, T., et al.: InstructUIE: Multi-task Instruction Tuning for
    Unified Information Extraction. arXiv preprint arXiv:2304.08085 (2023)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., Zheng, R., Ye,
    J., Zhang, Q., Gui, T., 等: InstructUIE: 统一信息提取的多任务指令调优。arXiv 预印本 arXiv:2304.08085
    (2023)'
- en: '[21] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N.,
    Dai, A.M., Le, Q.V.: Finetuned Language Models Are Zero-shot Learners. arXiv preprint
    arXiv:2109.01652 (2021)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N.,
    Dai, A.M., Le, Q.V.: 微调语言模型是零-shot 学习者。arXiv 预印本 arXiv:2109.01652 (2021)'
- en: '[22] Yu, J., Zhu, J., Wang, Y., Liu, Y., Chang, H., Nie, J., Kong, C., Cong,
    R., XinLiu, An, J., Lu, L., Fang, M., Zhu, L.: Taoli LLaMA. [https://github.com/blcuicall/taoli](https://github.com/blcuicall/taoli)
    (2023)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Yu, J., Zhu, J., Wang, Y., Liu, Y., Chang, H., Nie, J., Kong, C., Cong,
    R., XinLiu, An, J., Lu, L., Fang, M., Zhu, L.: Taoli LLaMA. [https://github.com/blcuicall/taoli](https://github.com/blcuicall/taoli)
    (2023)'
- en: '[23] Zhang, B.: Features and Functions of the HSK Dynamic Composition Corpus.
    International Chinese Language Education 4, 71–79 (2009)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Zhang, B.: HSK 动态组成语料库的特征与功能。国际中文教育 4, 71–79 (2009)'
- en: '[24] Zhang, B., Yang, H., Liu, X.Y.: Instruct-FinGPT: Financial Sentiment Analysis
    by Instruction Tuning of General-Purpose Large Language Models. arXiv preprint
    arXiv:2306.12659 (2023)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Zhang, B., Yang, H., Liu, X.Y.: Instruct-FinGPT: 通过指令调优的通用大语言模型进行金融情感分析。arXiv
    预印本 arXiv:2306.12659 (2023)'
- en: '[25] Zhang, H., Chen, J., Jiang, F., Yu, F., Chen, Z., Li, J., Chen, G., Wu,
    X., Zhang, Z., Xiao, Q., et al.: HuatuoGPT, towards Taming Language Model to Be
    a Doctor. arXiv preprint arXiv:2305.15075 (2023)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Zhang, H., Chen, J., Jiang, F., Yu, F., Chen, Z., Li, J., Chen, G., Wu,
    X., Zhang, Z., Xiao, Q., 等: 华佗GPT，致力于将语言模型驯化为医生。arXiv 预印本 arXiv:2305.15075 (2023)'
- en: '[26] Zhang, Y., Li, Z., Bao, Z., Li, J., Zhang, B., Li, C., Huang, F., Zhang,
    M.: MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical
    Error Correction. In: Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies.
    pp. 3118–3130 (2022)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Zhang, Y., Li, Z., Bao, Z., Li, J., Zhang, B., Li, C., Huang, F., Zhang,
    M.：MuCGEC：用于中文语法错误纠正的多参考多来源评估数据集。发表于2022年北美计算语言学协会：人类语言技术会议论文集。pp. 3118–3130 (2022)'
- en: '[27] Zhang, Y., Zhang, B., Jiang, H., Li, Z., Li, C., Huang, F., Zhang, M.:
    NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native
    Speaker Texts. arXiv preprint arXiv:2305.16023 (2023)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zhang, Y., Zhang, B., Jiang, H., Li, Z., Li, C., Huang, F., Zhang, M.：NaSGEC：来自母语者文本的多领域中文语法错误纠正数据集。arXiv预印本
    arXiv:2305.16023 (2023)'
- en: '[28] Zhao, Y., Jiang, N., Sun, W., Wan, X.: Overview of the NLPCC 2018 Shared
    Task: Grammatical Error Correction. In: Natural Language Processing and Chinese
    Computing: 7th CCF International Conference, NLPCC 2018, Hohhot, China, August
    26–30, 2018, Proceedings, Part II 7\. pp. 439–445\. Springer (2018)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Zhao, Y., Jiang, N., Sun, W., Wan, X.：NLPCC 2018共享任务概述：语法错误纠正。在：自然语言处理与中文计算：第七届CCF国际会议，NLPCC
    2018，内蒙古呼和浩特，中国，2018年8月26–30日，会议论文集，第二部分 7\. pp. 439–445\. Springer (2018)'
- en: '[29] Zhao, Z., Wang, H.: MaskGEC: Improving Neural Grammatical Error Correction
    via Dynamic Masking. Proceedings of the AAAI Conference on Artificial Intelligence
    34(01), 1226–1233 (2020)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zhao, Z., Wang, H.：MaskGEC：通过动态掩蔽改进神经语法错误纠正。人工智能会议论文集 34(01)，1226–1233
    (2020)'
