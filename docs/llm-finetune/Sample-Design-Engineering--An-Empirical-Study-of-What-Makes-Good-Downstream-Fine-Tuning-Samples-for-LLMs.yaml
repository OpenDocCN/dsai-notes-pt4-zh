- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:37:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样本设计工程：什么使得下游微调样本对LLMs更优秀的实证研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13033](https://ar5iv.labs.arxiv.org/html/2404.13033)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13033](https://ar5iv.labs.arxiv.org/html/2404.13033)
- en: Biyang Guo^(1†), He Wang^(1†), Wenyilin Xiao^(1†)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郭比扬^(1†)，王赫^(1†)，肖文伊林^(1†)
- en: Hong Chen^(2†), Zhuxin Lee³, Songqiao Han^(1∗), Hailiang Huang^(1,4∗)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 洪晨^(2†)，李竹欣³，韩松桥^(1∗)，黄海亮^(1,4∗)
- en: ¹AI Lab, SIME, Shanghai University of Finance and Economics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹人工智能实验室，上海财经大学SIME
- en: ²Ant Group, ³Guangdong Yunxi Technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²蚂蚁集团，³广东云溪科技
- en: ⁴Key Laboratory of Interdisciplinary Research of Computation and Economics,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴计算与经济学交叉研究重点实验室，
- en: Ministry of Education, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 中国教育部
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In the burgeoning field of Large Language Models (LLMs) like ChatGPT and LLaMA,
    Prompt Engineering (PE) is renowned for boosting zero-shot or in-context learning
    (ICL) through prompt modifications. Yet, the realm of the sample design for downstream
    fine-tuning, crucial for task-specific LLM adaptation, is largely unexplored.
    This paper introduces Sample Design Engineering (SDE), a methodical approach to
    enhancing LLMs’ post-tuning performance by refining input, output, and reasoning
    designs. We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments
    to assess the impact of various design options on LLMs’ downstream performance,
    revealing several intriguing patterns that hold consistently across different
    LLMs. Based on these insights, we propose an integrated SDE strategy, combining
    the most effective options, and validate its consistent superiority over heuristic
    sample designs in complex downstream tasks like multi-aspect sentiment analysis,
    event extraction, and nested entity recognition. Additionally, analyses of LLMs’
    inherent prompt/output perplexity, zero-shot, and ICL abilities illustrate that
    good PE strategies may not always translate to good SDE strategies. Code available
    at [https://github.com/beyondguo/LLM-Tuning](https://github.com/beyondguo/LLM-Tuning).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的大型语言模型（LLMs）领域，如ChatGPT和LLaMA，提示工程（PE）因通过提示修改提升零-shot或上下文学习（ICL）而闻名。然而，对于下游微调样本设计这一对任务特定LLM适配至关重要的领域，研究仍较为缺乏。本文介绍了样本设计工程（SDE），这是一种通过优化输入、输出和推理设计来提升LLMs后期调优表现的方法论。我们进行了一系列领域内（ID）和领域外（OOD）实验，以评估各种设计选项对LLMs下游表现的影响，揭示了在不同LLMs中一致的若干有趣模式。基于这些见解，我们提出了一种综合SDE策略，结合了最有效的选项，并验证了其在复杂下游任务如多方面情感分析、事件抽取和嵌套实体识别中的持续优越性。此外，对LLMs固有的提示/输出困惑度、零-shot和ICL能力的分析表明，好的PE策略可能并不总能转化为好的SDE策略。代码可在
    [https://github.com/beyondguo/LLM-Tuning](https://github.com/beyondguo/LLM-Tuning)
    获取。
- en: 'Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 样本设计工程：什么使得下游微调样本对LLMs更优秀的实证研究
- en: Biyang Guo^(1†), He Wang^(1†), Wenyilin Xiao^(1†) Hong Chen^(2†), Zhuxin Lee³,
    Songqiao Han^(1∗), Hailiang Huang^(1,4∗) ¹AI Lab, SIME, Shanghai University of
    Finance and Economics ²Ant Group, ³Guangdong Yunxi Technology ⁴Key Laboratory
    of Interdisciplinary Research of Computation and Economics, Ministry of Education,
    China
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 郭比扬^(1†)，王赫^(1†)，肖文伊林^(1†) 洪晨^(2†)，李竹欣³，韩松桥^(1∗)，黄海亮^(1,4∗) ¹人工智能实验室，上海财经大学SIME
    ²蚂蚁集团，³广东云溪科技 ⁴计算与经济学交叉研究重点实验室，中国教育部
- en: '^(${\dagger}$)^(${\dagger}$)footnotetext: Equal Contribution^($*$)^($*$)footnotetext:
    Corresponding authors, emails:^†^†footnotetext: han.songqiao@shufe.edu.cn, hlhuang@shufe.edu.cn'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^(${\dagger}$)^(${\dagger}$)脚注：同等贡献^($*$)^($*$)脚注：通讯作者，电子邮件：^†^†脚注：han.songqiao@shufe.edu.cn,
    hlhuang@shufe.edu.cn
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The emergence of Large Language Models (LLMs) such as GPT-3 Brown et al. ([2020](#bib.bib4)),
    PaLM Chowdhery et al. ([2023](#bib.bib6)), LLaMA Touvron et al. ([2023a](#bib.bib32))
    and GPT-4 Achiam et al. ([2023](#bib.bib1)) revolutionized natural language processing
    (NLP), enabling complex tasks to be tackled with a single model. This shift has
    profoundly broadened the range of tasks manageable by NLP models, while simultaneously
    consolidating the methodologies for various tasks under the unified framework
    of text generation. In this background, Prompt Engineering (PE) has emerged as
    a key area in leveraging cutting-edge LLMs, leading to advances in applying LLMs
    to new tasks Brown et al. ([2020](#bib.bib4)), enhancing logical reasoning Wei
    et al. ([2022](#bib.bib39)), and increasing task-specific accuracy Wang et al.
    ([2023a](#bib.bib34)); Wei et al. ([2023](#bib.bib40)), without updating model
    weights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现，如 GPT-3 Brown et al. ([2020](#bib.bib4))、PaLM Chowdhery et al.
    ([2023](#bib.bib6))、LLaMA Touvron et al. ([2023a](#bib.bib32)) 和 GPT-4 Achiam
    et al. ([2023](#bib.bib1))，彻底革新了自然语言处理（NLP），使得复杂任务可以通过单一模型来处理。这一变化深刻拓宽了 NLP 模型能够处理的任务范围，同时也将各种任务的方法论整合到了统一的文本生成框架下。在这种背景下，Prompt
    Engineering (PE) 成为了利用前沿 LLM 的关键领域，推动了将 LLM 应用于新任务 Brown et al. ([2020](#bib.bib4))、增强逻辑推理
    Wei et al. ([2022](#bib.bib39)) 和提高任务特定准确性 Wang et al. ([2023a](#bib.bib34))；Wei
    et al. ([2023](#bib.bib40))，而无需更新模型权重。
- en: '![Refer to caption](img/7533e8b9579d7af98aebef16e378ce86.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/7533e8b9579d7af98aebef16e378ce86.png)'
- en: 'Figure 1: A simplified comparison between PE and our proposed SDE.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：PE 与我们提出的 SDE 之间的简化比较。
- en: 'While numerous PE techniques have been developed for LLMs’ zero-shot and in-context
    learning (ICL), the challenge of designing effective training samples for fine-tuning
    LLMs—termed Sample Design Engineering (SDE) in this paper—remains underexplored.
    SDE is crucial for tailoring smaller open-source LLMs to specific requirements,
    especially given the complexity of training samples for downstream tasks. Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs") is a simplified
    demonstration of PE and SDE.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经开发了大量的 PE 技术用于 LLM 的零样本和上下文学习（ICL），但为微调 LLM 设计有效的训练样本——本文中称之为样本设计工程（SDE）——的挑战仍未得到充分探索。SDE
    对于将较小的开源 LLM 调整到特定需求是至关重要的，尤其考虑到下游任务训练样本的复杂性。图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 样本设计工程：对
    LLM 进行有效下游微调样本的实证研究") 是 PE 和 SDE 的简化演示。
- en: To address this gap, this paper undertakes a detailed and comprehensive exploration
    of SDE for LLMs’ downstream fine-tuning. Our study is based on the hypothesis
    that the structure or elements of training samples may have a big impact on the
    fine-tuned LLMs. Different sample designs may make it easier or harder for the
    LLMs to learn, especially in scenarios where data is scarce.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一空白，本文对 LLM 下游微调的 SDE 进行了详细和全面的探讨。我们的研究基于这样一个假设：训练样本的结构或元素可能对微调后的 LLM 产生重大影响。不同的样本设计可能会让
    LLM 学习变得更容易或更困难，特别是在数据稀缺的情况下。
- en: 'We begin by identifying a range of typical SDE options and categorizing them
    into three groups: input, output , and reasoning design options (shown in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.2 Fine-tuning LLMs ‣ 2 Background and Related Work ‣
    Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs")). To reveal the impact of each SDE option, we conduct experiments
    on a typical downstream scenario – multi-aspect sentiment analysis (MASA), with
    2 in-domain (ID) tasks and 2 out-of-domain (OOD) tasks. Different from instruction-tuning
    datasets like FLAN Longpre et al. ([2023](#bib.bib25)), the MASA task involves
    more complicated input and output elements, making it suitable for in-depth investigation
    of different sample designs. Comprehensive experiments on these 4 tasks with 6
    popular open-source LLMs are undertaken to reveal how different SDE options affect
    downstream performances. Some interesting and thought-provoking conclusions are
    revealed through our experiments. For example, simply switching the position of
    the task instruction can make a difference; adding placeholders to unmentioned
    targets brings a notable performance gain, etc.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先识别了一系列典型的SDE选项，并将其分为三组：输入、输出和推理设计选项（见图 [2](#S2.F2 "图 2 ‣ 2.2 微调LLMs ‣ 2
    背景与相关工作 ‣ 样本设计工程：什么样的下游微调样本对LLMs有效的经验研究")）。为了揭示每个SDE选项的影响，我们在一个典型的下游场景——多方面情感分析（MASA）上进行实验，包括2个领域内（ID）任务和2个领域外（OOD）任务。与FLAN
    Longpre等的指令调整数据集（[2023](#bib.bib25)）不同，MASA任务涉及更复杂的输入和输出元素，使其适合深入研究不同的样本设计。我们对这些4个任务与6个流行的开源LLM进行的全面实验揭示了不同SDE选项如何影响下游表现。我们的实验揭示了一些有趣且发人深省的结论。例如，仅仅改变任务指令的位置就可以产生不同的效果；在未提及的目标中添加占位符会显著提高性能等。
- en: Leveraging these findings, we combine the empirically well-performing SDE options
    and propose an integrated SDE strategy ES-SDE. Extensive experiments on 3 complex
    downstream tasks (Nested-NER, Event Detection, and MASA) on 2 additional LLMs
    demonstrate that ES-SDE notably surpasses weaker SDE combination, as well as heuristic
    design from other studies. ES-SDE’s robustness on different training sizes, decoding
    randomness or instruction variation further underscores its stable effectiveness.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些发现，我们结合了经验上表现良好的SDE选项，并提出了一种集成的SDE策略ES-SDE。对2个额外LLM上的3个复杂下游任务（Nested-NER、事件检测和MASA）进行的大量实验表明，ES-SDE显著超越了较弱的SDE组合以及其他研究中的启发式设计。ES-SDE在不同训练规模、解码随机性或指令变化下的稳健性进一步强调了其稳定的有效性。
- en: In an exploratory analysis, we investigate the link between effective prompt
    and sample designs, via perplexity, zero-shot, and ICL analysis. Our findings
    suggest that a well-crafted PE strategy may not necessarily translate to a successful
    SDE strategy. This observation encourages further research into SDE’s mechanisms,
    promising for enhancing LLMs’ downstream applications.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索性分析中，我们通过困惑度、零样本和ICL分析调查了有效提示与样本设计之间的联系。我们的发现表明，一个精心设计的PE策略可能并不一定能转化为成功的SDE策略。这一观察结果鼓励进一步研究SDE的机制，这对提升LLM的下游应用具有前景。
- en: 2 Background and Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 2.1 Prompt Engineering (PE)
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 提示工程（PE）
- en: The effectiveness of PE methods is largely built upon the strong inherent capabilities
    of LLMs, with most research focusing on very large models such as GPT-3, GPT-4,
    PaLM, etc. (refer to Sahoo et al. ([2024](#bib.bib28))). These models are pre-trained
    on extremely vast corpora, acquiring a wealth of knowledge and patterns, which
    enables them to directly perform complex tasks through careful prompt design.
    For instance, Brown et al. ([2020](#bib.bib4)) use carefully crafted prompts and
    in-context-learning (ICL) techniques to guide GPT-3 on novel tasks without training;
    Wei et al. ([2022](#bib.bib39)) propose the Chain-of-Thought (CoT) technique that
    can boost the logic reasoning performance; RAG Lewis et al. ([2020](#bib.bib21))
    and CoVe Dhuliawala et al. ([2023](#bib.bib10)) methods are used to reduce hallucination
    during generation; Li et al. ([2023](#bib.bib22)) introduce EmotionPrompt to improve
    LLMs’ emotional intelligence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PE 方法的有效性主要基于LLMs的强大固有能力，大多数研究集中在如 GPT-3、GPT-4、PaLM 等非常大的模型上（参见 Sahoo 等人 ([2024](#bib.bib28))）。这些模型在极其庞大的语料库上进行预训练，获取了大量的知识和模式，这使得它们能够通过精心设计的提示直接执行复杂任务。例如，Brown
    等人 ([2020](#bib.bib4)) 使用精心设计的提示和上下文学习（ICL）技术来指导 GPT-3 完成新任务，无需训练；Wei 等人 ([2022](#bib.bib39))
    提出了 Chain-of-Thought (CoT) 技术，以提升逻辑推理性能；RAG Lewis 等人 ([2020](#bib.bib21)) 和 CoVe
    Dhuliawala 等人 ([2023](#bib.bib10)) 方法用于减少生成过程中的幻觉；Li 等人 ([2023](#bib.bib22)) 介绍了
    EmotionPrompt，以提升 LLMs 的情感智能。
- en: However, these most advanced and effective LLMs are either black-box models
    that are only accessible via APIs, or extremely large models that are unaffordable
    for most companies to serve in production. Consequently, many practitioners turn
    to smaller but open-source LLMs, especially 10B around models. In this situation,
    solely relying on PE for zero-shot or ICL inference is unable to handle many real-world
    complex NLP tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些最先进且有效的 LLMs 要么是只能通过 API 访问的黑箱模型，要么是对于大多数公司而言无法承受的极其庞大的模型。因此，许多从业者转向更小但开源的
    LLMs，特别是约 10B 的模型。在这种情况下，仅仅依靠 PE 进行零样本或 ICL 推理无法处理许多现实世界复杂的 NLP 任务。
- en: 2.2 Fine-tuning LLMs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 微调 LLMs
- en: 'According to the different purposes, we can divide LLMs’ fine-tuning into two
    types: instruction-tuning (IT) and downstream-tuning (DT)¹¹1It is also known as
    task tuning (TT) in some literature, like Weber et al. ([2023](#bib.bib37))..'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的目的，我们可以将 LLMs 的微调分为两种类型：指令微调 (IT) 和下游微调 (DT)¹¹1在一些文献中也称为任务微调 (TT)，如 Weber
    等人 ([2023](#bib.bib37))。
- en: IT trains LLMs to comprehend and execute instructions across a range of NLP
    tasks, enabling predictions for new tasks Wei et al. ([2021](#bib.bib38)); Mishra
    et al. ([2022](#bib.bib26)) with datasets like FLAN Longpre et al. ([2023](#bib.bib25)),
    Self-instruct Wang et al. ([2023b](#bib.bib36)), Alpaca Taori et al. ([2023](#bib.bib30))
    and HC3 Guo et al. ([2023](#bib.bib15)), covering tasks like such as classification,
    QA and translation. This is mainly applied to base models to enable them to follow
    general human instructions. DT focuses on customizing LLMs for specific, often
    complex, tasks in industrial applications, demanding high output stability for
    easier parsing and application in downstream products. An example is multi-aspect
    sentiment analysis, which requires detailed task instructions and outputs. Our
    study centers on SDE in DT scenarios, highlighting sample design challenges, but
    the insights may also benefit IT sample design, a topic for future exploration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: IT 训练 LLMs 理解并执行一系列 NLP 任务中的指令，使其能够对新任务进行预测 Wei 等人 ([2021](#bib.bib38))；Mishra
    等人 ([2022](#bib.bib26))，使用数据集如 FLAN Longpre 等人 ([2023](#bib.bib25))，Self-instruct
    Wang 等人 ([2023b](#bib.bib36))，Alpaca Taori 等人 ([2023](#bib.bib30)) 和 HC3 Guo 等人
    ([2023](#bib.bib15))，涵盖分类、QA 和翻译等任务。这主要应用于基础模型，使其能够遵循一般的人类指令。DT 侧重于为工业应用中的特定、通常复杂的任务定制
    LLMs，要求高输出稳定性，以便更容易解析和应用于下游产品。一个例子是多方面情感分析，它需要详细的任务指令和输出。我们的研究集中在 DT 情境中的 SDE，突出了样本设计挑战，但这些见解也可能对
    IT 样本设计有所裨益，这是未来探索的主题。
- en: '![Refer to caption](img/f279587ad781adc45802d3aa911bd979.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f279587ad781adc45802d3aa911bd979.png)'
- en: 'Figure 2: Typical SDE options to be considered when designing downstream-tuning
    samples, taking the MASA task as an example. $Ai$ means its sentiment label, [P]
    refers to placeholder tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 在设计下游微调样本时需要考虑的典型 SDE 选项，以 MASA 任务为例。 $Ai$ 代表其情感标签，[P] 代表占位符标记。'
- en: '![Refer to caption](img/c0128683df24b4bd639fbd8ec626ca19.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c0128683df24b4bd639fbd8ec626ca19.png)'
- en: 'Figure 3: An example for the MASA task.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: MASA 任务的一个示例。'
- en: 2.3 Parameter-efficient fine-tuning
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 参数高效微调
- en: The expansion of language models has made traditional full-parameter fine-tuning
    (FFT) less viable due to its high computational and storage demands. Parameter-efficient
    fine-tuning (PEFT) methods, such as prefix-tuningLi and Liang ([2021](#bib.bib23)),
    prompt-tuningLester et al. ([2021](#bib.bib19)), p-tuningLiu et al. ([2023](#bib.bib24)),
    and LoRAHu et al. ([2021](#bib.bib17)) provide cost-effective alternatives that
    retain FFT’s effectiveness, gaining popularity in industrial applications. These
    techniques are adaptable to both IT and DT scenarios. In this research, we use
    the widely-used LoRA as the default fine-tuning technique. However, we believe
    results from our study are also applicable to other PEFT methods.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的扩展使得传统的全参数微调（FFT）因其高计算和存储需求而变得不太可行。参数高效微调（PEFT）方法，如prefix-tuningLi 和 Liang
    ([2021](#bib.bib23))、prompt-tuningLester et al. ([2021](#bib.bib19))、p-tuningLiu
    et al. ([2023](#bib.bib24))和LoRAHu et al. ([2021](#bib.bib17))，提供了成本效益高的替代方案，保留了FFT的有效性，并在工业应用中日益受到欢迎。这些技术适用于IT和DT场景。在本研究中，我们使用广泛使用的LoRA作为默认的微调技术。然而，我们认为我们的研究结果也适用于其他PEFT方法。
- en: 3 Sample Design Engineering
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 样本设计工程
- en: 3.1 Typical SDE Options
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 典型SDE选项
- en: 'We categorize sample design options into three aspects: input, output, and
    reasoning. We take the Multi-Aspect Sentiment Analysis (MASA), a typical downstream
    task, as an example to clarify each design option for fine-tuning samples. As
    illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Fine-tuning LLMs ‣ 2 Background
    and Related Work ‣ Sample Design Engineering: An Empirical Study of What Makes
    Good Downstream Fine-Tuning Samples for LLMs"), MASA requires analyzing review
    texts to assign sentiments to predefined aspects, while some aspects may be unmentioned.
    Figure [2](#S2.F2 "Figure 2 ‣ 2.2 Fine-tuning LLMs ‣ 2 Background and Related
    Work ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") is an overview of different SDE options, which
    should be considered to design proper DT samples.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将样本设计选项分为三个方面：输入、输出和推理。我们以多方面情感分析（MASA）作为例子，来澄清每种微调样本的设计选项。正如图 [3](#S2.F3
    "Figure 3 ‣ 2.2 Fine-tuning LLMs ‣ 2 Background and Related Work ‣ Sample Design
    Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples
    for LLMs") 所示，MASA需要分析评论文本以将情感分配给预定义的方面，而某些方面可能未被提及。图 [2](#S2.F2 "Figure 2 ‣ 2.2
    Fine-tuning LLMs ‣ 2 Background and Related Work ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")
    是不同SDE选项的概述，这些选项应考虑以设计适当的DT样本。'
- en: 3.1.1 Input Design Options
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 输入设计选项
- en: '$a.$ Instruction Placement: We explore the effect of instruction positioning
    relative to task text (for MASA, the review text), examining Inst-first (before
    the task text), Inst-last (after the task text). We also compare with the No-inst
    (no instruction) option to evaluate the effectiveness of explicit instructions,
    as used in many previous conditional text generation tasks Lewis et al. ([2019](#bib.bib20));
    Guo et al. ([2022](#bib.bib14)); Zhang et al. ([2023](#bib.bib43)).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: $a.$ 指令位置：我们探索了指令相对于任务文本（对于MASA，即评论文本）的位置效果，检查Inst-first（任务文本之前）、Inst-last（任务文本之后）。我们还与No-inst（无指令）选项进行比较，以评估明确指令的有效性，如许多之前的条件文本生成任务所用Lewis
    et al. ([2019](#bib.bib20))；Guo et al. ([2022](#bib.bib14))；Zhang et al. ([2023](#bib.bib43))。
- en: '$b.$ Input Modeling: Considering the distinction between unified sequence modeling
    in LLM pre-training and the explicit input/output segmentation in fine-tuning,
    we compare No-MI that excluding input from loss calculation, akin to LLaMA2’s
    SFT process Touvron et al. ([2023b](#bib.bib33))) against MI (modeling input in
    backpropagation).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $b.$ 输入建模：考虑到LLM预训练中的统一序列建模和微调中的显式输入/输出分割之间的区别，我们比较了排除输入的No-MI，类似于LLaMA2的SFT过程Touvron
    et al. ([2023b](#bib.bib33)))，与MI（在反向传播中建模输入）。
- en: 3.1.2 Output Design Options
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 输出设计选项
- en: '$a.$ Multiple Predictions Formatting: For tasks necessitating several predictions,
    we evaluate output formatting from less to more structured: Natural (free-form
    text), Lines (each aspect on a new line), and JSON (JSON-lines for precision and
    explicitness).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $a.$ 多重预测格式：对于需要多个预测的任务，我们评估输出格式的结构化程度：自然（自由文本）、行（每个方面一行）和JSON（用于精确性和明确性的JSON-lines）。
- en: '$b.$ Handling Unmentioned Targets: We consider whether to omit the unmentioned
    (OU) targets in the output, or place placeholders (PU) for those targets. The
    placeholder tokens can be strings like "Unmentioned", "None", or "[]" according
    to tasks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $b.$ 处理未提及的目标：我们考虑是否在输出中省略未提及（OU）目标，或为这些目标放置占位符（PU）。占位符令牌可以是字符串，如“未提及”、“无”或“[]”，具体取决于任务。
- en: '$c.$ Textual or numerical labels: By default, we use the TxtLabel option for
    textual output labels. However, in some cases, using numbers to represent outcomes
    (NumLabel) may enhance prediction robustness.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $c.$ 文本或数字标签：默认情况下，我们使用 TxtLabel 选项来获取文本输出标签。然而，在某些情况下，使用数字表示结果（NumLabel）可能会增强预测的鲁棒性。
- en: 3.1.3 Reasoning Design Options
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 推理设计选项
- en: Many tasks require reasoning, where the Chain-of-Thought (CoT) Wei et al. ([2022](#bib.bib39))
    has shown promise in improving LLM’s reasoning in zero-shot and ICL, as well as
    IT scenarios Kim et al. ([2023](#bib.bib18)). Yet, its impact on DT remains less
    studied.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务需要推理，其中 Chain-of-Thought (CoT) Wei 等人 ([2022](#bib.bib39)) 已显示在零样本和 ICL
    以及 IT 场景中提高 LLM 的推理能力的潜力 Kim 等人 ([2023](#bib.bib18))。然而，它对 DT 的影响仍然较少研究。
- en: We introduce the CoT option for training models to "think before they predict".
    We use JSON as the default output format to make the representation clearer and
    add a new description field before the sentiment field. Conversely, the R-CoT
    (Reverse-CoT) reverses these fields, enabling a "predict then explain" approach
    to explore CoT’s mechanics further. Note that Implementing CoT-like samples incurs
    additional annotation costs due to the description fields, making the reasoning
    design options task-dependent.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入 CoT 选项来训练模型“在预测之前思考”。我们使用 JSON 作为默认输出格式，以使表示更清晰，并在情感字段之前添加一个新的描述字段。相反，R-CoT（Reverse-CoT）则反转这些字段，实现“预测后解释”的方法，以进一步探讨
    CoT 的机制。请注意，实施类似 CoT 的样本会因描述字段而产生额外的标注成本，使得推理设计选项依赖于任务。
- en: 3.2 Integrated SDE Strategy
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 综合 SDE 策略
- en: A final sample design is a combination of the above design options, which we
    call an integrated SDE strategy. This paper initially explores the impact of each
    individual option through extensive experimentation, leading to the proposal of
    an evidence-based integrated SDE strategy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终样本设计是上述设计选项的组合，我们称之为综合 SDE 策略。本文最初通过广泛的实验探讨了每个单独选项的影响，从而提出了一种基于证据的综合 SDE 策略。
- en: '4 Experiments I: Evaluating The Impact of Each SDE Option'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验 I：评估每个 SDE 选项的影响
- en: 4.1 Settings
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'Tasks and Datasets. We experiment with in-domain (ID) evaluations and out-of-domain
    (OOD) evaluations, for the Chinese online review MASA scenario. The data is provided
    and annotated by our collaborating company, which encounters a real-world business
    need for the analysis of extensive customer online reviews. The data annotations
    come from two domains of aspects: D1 about food, beverage, price, hygiene, staff
    attitude, and parking convenience and D2 about traffic convenience, queuing, serving
    speed, decoration, and noise. The model needs to give a sentiment label from {positive,
    neutral, negative} for each aspect, while some aspects may not occur in the review.
    Based on the two domains, we construct the following 4 tasks:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 任务和数据集。我们在中文在线评论 MASA 场景中进行领域内（ID）评估和领域外（OOD）评估。数据由我们合作的公司提供和标注，该公司遇到对大量客户在线评论分析的实际业务需求。数据标注来自两个方面的领域：D1
    涉及食品、饮料、价格、卫生、员工态度和停车便利性，D2 涉及交通便利性、排队、服务速度、装饰和噪音。模型需要为每个方面提供一个情感标签，取值范围为 {positive,
    neutral, negative}，而某些方面可能在评论中不存在。根据这两个领域，我们构建了以下 4 个任务：
- en: $\bullet$D2 are two ID evaluation tasks, where train and test sets come from
    the same domains;
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ D2 是两个 ID 评估任务，其中训练和测试集来自相同领域；
- en: $\bullet$D1 are two OOD generalization tasks, where the model trains on one
    domain but tests on an unseen domain.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ D1 是两个 OOD 泛化任务，其中模型在一个领域进行训练，但在未见过的领域进行测试。
- en: 'Considering the high cost of annotation in industries and the fact that fine-tuning
    LLMs requires less annotated data Zhou et al. ([2024](#bib.bib44)), we train the
    model with $500$ samples to make results more stable and convincing. Dataset details
    see Appendix [A.2](#A1.SS2 "A.2 Datasets and Training Settings ‣ Appendix A Appendix
    ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到行业中标注的高成本以及微调LLM所需的标注数据较少Zhou等人（[2024](#bib.bib44)），我们用$500$个样本训练模型，以使结果更稳定和有说服力。数据集细节见附录
    [A.2](#A1.SS2 "A.2 Datasets and Training Settings ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs")。'
- en: '![Refer to caption](img/69f35fbe0100d223b71a97493f04feb3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/69f35fbe0100d223b71a97493f04feb3.png)'
- en: 'Figure 4: Sentiment analysis performances ($\kappa$) of different SDE options.
    Results of ID are the average of D1->D1 and D2->D2, same for OOD. The bars depict
    each method’s relative improvement or degradation compared to the baseline, with
    each method differing from the baseline in only one option (colored in red). Detailed
    results for each task see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations
    of Each SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")-[8](#A1.T8
    "Table 8 ‣ A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A Appendix ‣
    Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：不同SDE选项的情感分析性能（$\kappa$）。ID的结果是D1->D1和D2->D2的平均值，OOD也一样。条形图描绘了每种方法相对于基线的相对改善或退化，每种方法仅在一个选项上（以红色标记）与基线不同。每个任务的详细结果见表
    [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A
    Appendix ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs")-[8](#A1.T8 "Table 8 ‣ A.4 Detailed Evaluations
    of Each SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")。'
- en: 'Models. We utilize the following widely used open-source LLMs of 7B size of
    both the base and chat versions: 1) chinese-llama-2-7b (note as c-llama2-base)
    and the instruction-tuned version chinese-alpaca-2-7b (c-llama2-chat) from the
    Chinese-LLaMA2 series Cui et al. ([2023](#bib.bib9)), which is the vocabulary-expanded
    version of LLaMA2 Touvron et al. ([2023b](#bib.bib33)) with secondary pre-training
    and fine-tuning on Chinese corpus; 2) internlm-7b-base (intern-base) and internlm-7b-chat
    (intern-chat) from the InternLM series Team ([2023](#bib.bib31)), which are pretrained
    on trillions of high-quality tokens, performs well in Chinese and English tasks;
    3) baichuan2-7b-base (bc2-base) and baichuan2-7b-chat (bc2-chat) from the Baichuan2
    series Yang et al. ([2023](#bib.bib41)), one of the SOTA LLMs at the time of release.
    We use LoRA as the default efficient fine-tuning technique. Hyperparameters and
    other training details can be found in Appendix [A.2](#A1.SS2 "A.2 Datasets and
    Training Settings ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '模型。我们使用以下广泛使用的7B规模开源LLM，包括基础版和聊天版：1）chinese-llama-2-7b（记作c-llama2-base）和指令调优版chinese-alpaca-2-7b（c-llama2-chat），来自Chinese-LLaMA2系列Cui等人（[2023](#bib.bib9)），这是LLaMA2
    Touvron等人（[2023b](#bib.bib33)）的词汇扩展版，并在中文语料上进行了二次预训练和微调；2）internlm-7b-base（intern-base）和internlm-7b-chat（intern-chat），来自InternLM系列Team（[2023](#bib.bib31)），在数万亿高质量标记上进行预训练，在中文和英文任务中表现良好；3）baichuan2-7b-base（bc2-base）和baichuan2-7b-chat（bc2-chat），来自Baichuan2系列Yang等人（[2023](#bib.bib41)），是发布时的SOTA
    LLM之一。我们使用LoRA作为默认的高效微调技术。超参数和其他训练细节见附录 [A.2](#A1.SS2 "A.2 Datasets and Training
    Settings ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study
    of What Makes Good Downstream Fine-Tuning Samples for LLMs")。'
- en: 'Evaluation Metrics. We evaluate the MASA’s performance from two perspectives:
    1) Sentiment analysis performance. We use the weighted Kappa score $\kappa$, Kappa
    weight matrix, and format-parsing rules can be seen in Appendix [A.1](#A1.SS1
    "A.1 Metrics for MASA ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '评估指标。我们从两个角度评估MASA的性能：1）情感分析性能。我们使用加权Kappa分数$\kappa$、Kappa权重矩阵，格式解析规则见附录 [A.1](#A1.SS1
    "A.1 Metrics for MASA ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")。'
- en: 4.2 Experimental Results on Each Option
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 各选项的实验结果
- en: We report and analyze the results from two perspectives—sentiment analysis performances,
    and format adherence abilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从两个角度报告和分析结果——情感分析性能和格式符合能力。
- en: 4.2.1 Sentiment Analysis Performance
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 情感分析性能
- en: 'We first assess the sentiment analysis performances of LLMs using different
    sample design options. The comparative results of ID and OOD tasks on 3 Chat-LLMs
    and 3 Base-LLMs are plotted in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Settings ‣ 4
    Experiments I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")
    (full results see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each
    SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study
    of What Makes Good Downstream Fine-Tuning Samples for LLMs") to Table [8](#A1.T8
    "Table 8 ‣ A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A Appendix ‣
    Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs") in Appendix [A.4](#A1.SS4 "A.4 Detailed Evaluations of Each
    SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study
    of What Makes Good Downstream Fine-Tuning Samples for LLMs")). Some shared and
    intriguing patterns are revealed from the results.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先评估了使用不同样本设计选项的LLMs的情感分析性能。图 [4](#S4.F4 "Figure 4 ‣ 4.1 Settings ‣ 4 Experiments
    I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs") 中绘制了3个Chat-LLMs和3个Base-LLMs在ID和OOD任务中的比较结果（完整结果见附录
    [A.4](#A1.SS4 "A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A Appendix
    ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") 的表 [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations
    of Each SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs") 到表 [8](#A1.T8
    "Table 8 ‣ A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A Appendix ‣
    Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs")）。结果揭示了一些共同且引人注目的模式。'
- en: 'Conclusions for Input Options: 1) Instructions enhance DT performances: The
    No-Inst option leads to poorer performance in ID tasks and a lack of OOD generalization
    ability compared to Inst-first or Inst-last methods that incorporate instructions.
    This underlines the critical role of including instructions for improving both
    understanding and generalizability of LLMs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入选项的结论：1) 指令增强了DT性能：No-Inst选项在ID任务中表现较差，并且缺乏与Inst-first或Inst-last方法相比的OOD泛化能力，这些方法包含了指令。这突显了包括指令对于提高LLMs的理解和泛化能力的重要作用。
- en: '2) Better to place instruction first: The Inst-first method outperforms Inst-last
    across both ID and OOD tasks for different LLMs. This demonstrates the significance
    of instruction placement for LLMs’ tuning process. We hypothesize that this may
    partly be explained by the attention mechanism, see Appendix [A.6](#A1.SS6 "A.6
    Additional Analysis on Inst-last and Inst-first ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '2) 最好将指令放在前面：Inst-first方法在不同LLMs的ID和OOD任务中优于Inst-last。这表明指令放置对于LLMs的调优过程具有重要意义。我们假设这部分可以通过注意机制来解释，参见附录
    [A.6](#A1.SS6 "A.6 Additional Analysis on Inst-last and Inst-first ‣ Appendix
    A Appendix ‣ Sample Design Engineering: An Empirical Study of What Makes Good
    Downstream Fine-Tuning Samples for LLMs")。'
- en: '3) Modeling input detracts from performance: Employing the MI approach results
    in worse outcomes compared to the No-MI baselines across various models and tasks.
    This indicates that modeling the input part during fine-tuning may hinder the
    LLM’s effectiveness, suggesting a cautious approach to what aspects of the task
    are modeled.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 建模输入会削弱性能：采用MI方法的结果较No-MI基准差，适用于各种模型和任务。这表明，在微调过程中建模输入部分可能会影响LLM的效果，因此建议对任务中的建模方面采取谨慎的态度。
- en: '![Refer to caption](img/4fc540bdc0b6078e7f7e7be5113a2956.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4fc540bdc0b6078e7f7e7be5113a2956.png)'
- en: 'Figure 5: Format adherence performance, measured by parsing error rates (%).
    ’*’ means same option as above.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：格式遵循性能，通过解析错误率（%）来测量。’*’表示与上面的选项相同。
- en: 'Conclusions for Output Options: 1) Lines is a reliable output format for multiple
    predictions: The Lines format, positioned between the Natural and JSON formats,
    demonstrates stable and high performance in sentiment analysis across various
    models and tasks. Its effectiveness lies in offering structured information while
    retaining natural language readability, making it versatile for different LLMs.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出选项的结论：1) Lines是一种可靠的多重预测输出格式：Lines格式位于Natural和JSON格式之间，在各种模型和任务中的情感分析中表现稳定且高效。其有效性在于提供结构化信息，同时保持自然语言的可读性，使其适用于不同的LLMs。
- en: '2) Base-LLMs exhibit similar patterns while Chat-LLMs diverse: Base models
    respond similarly to output formats, indicating consistency in their responses.
    In contrast, Chat models, such as bc2-chat and cllama2-chat, exhibit varied performances,
    suggesting differences in their SFT or RLHF data’s structure. For instance, bc2-chat
    and cllama2-chat perform well with JSON format, unlike intern-chat, implying a
    variance in the amount of structured data used in training.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基础LLMs展示类似模式，而聊天LLMs则多样：基础模型对输出格式响应相似，表明其回答的一致性。相比之下，聊天模型如bc2-chat和cllama2-chat展现了不同的表现，暗示其SFT或RLHF数据结构存在差异。例如，bc2-chat和cllama2-chat在JSON格式下表现良好，而intern-chat则不然，这暗示了训练中使用的结构化数据量的差异。
- en: '3) Base-LLMs favor more natural formats while Chat-LLMs can fit or bear more
    sophisticated formats: Base models prefer Natural and Lines over JSON. Conversely,
    Chat models lean towards structured formats, with Lines and JSON. This divergence
    hints at the different training backgrounds, with Chat models being more accommodating
    to sophisticated data formats. One more piece of evidence is that the NumLabel
    option brings much more damage to the Base models than to the Chat models, which
    is less natural than TxtLabel.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 基础LLMs偏好更自然的格式，而聊天LLMs能够适应或承受更复杂的格式：基础模型更喜欢Natural和Lines而非JSON。相反，聊天模型倾向于结构化格式，如Lines和JSON。这种分歧暗示了不同的训练背景，聊天模型对复杂数据格式的适应性更强。另一个证据是，NumLabel选项对基础模型的损害远大于对聊天模型的损害，而后者比TxtLabel更不自然。
- en: '4) Textual over numeric labels: Switching from textual to numeric labels worsens
    performance, likely because numeric labels lack the descriptive depth and context
    clues that textual labels provide, crucial for LLMs trained on natural language
    text.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 文本标签优于数字标签：从文本标签切换到数字标签会恶化表现，可能因为数字标签缺乏文本标签提供的描述深度和上下文线索，这对训练在自然语言文本上的LLMs至关重要。
- en: '5) Omitting the unmentioned targets may not be a good choice: While the OU
    option, which excludes unmentioned aspects, might seem to simplify outputs, it
    also introduces format inconsistency. This lack of uniformity forces the model
    to adapt to varied aspect mentions per sample, increasing task complexity with
    dynamic adjustment of the output format. Instead, the PU option keeps a consistent
    output format by adding placeholders, perhaps making LLMs easier to learn. Additional
    analysis shows that the aspects with a higher degree of unmentioning suffer greater
    underperformance with OU compared to PU, see Appendix [A.7](#A1.SS7 "A.7 Additional
    Analysis on OU and PU ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '5) 忽略未提及的目标可能不是一个好选择：尽管OU选项排除了未提及的方面，可能会简化输出，但也会引入格式不一致。这种不统一迫使模型适应每个样本的不同方面提及，从而增加了任务复杂性，并动态调整输出格式。相反，PU选项通过添加占位符保持一致的输出格式，这也许让LLMs更容易学习。额外分析表明，相比于PU，OU在未提及方面程度较高的情况下表现更差，见附录
    [A.7](#A1.SS7 "A.7 Additional Analysis on OU and PU ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs")。'
- en: 'Conclusions for Reasoning Options: 1) Subtle impact of CoT on ID, while significant
    on OOD tasks: CoT design marginally affects ID tasks but markedly improves OOD
    performance. This contrast highlights CoT’s role in enhancing model reasoning
    and adaptability in unfamiliar contexts, underpinning its value for generalization.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 推理选项的结论：1) CoT对ID任务的微妙影响，而对OOD任务的显著影响：CoT设计对ID任务影响微弱，但显著提升了OOD表现。这种对比突显了CoT在提升模型推理能力和适应陌生环境中的作用，支撑其对泛化的重要价值。
- en: '2) "Think before predict" beats "predict then explain": When the reasoning
    step is placed after predicting, like the R-CoT method, the performance does not
    match that of the standard CoT approach. However, R-CoT can still outperform No-CoT
    in many cases, suggesting that a single reasoning component is also beneficial.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2) “思考再预测”优于“预测后解释”：当推理步骤放在预测之后时，如R-CoT方法，其表现未能匹配标准CoT方法。然而，R-CoT在许多情况下仍能超越No-CoT，表明单一的推理组件也有益。
- en: 4.2.2 Format Adherence Performance
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 格式遵守性能
- en: '![Refer to caption](img/5035105ff2c5e9052e56f1a954a04b4c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5035105ff2c5e9052e56f1a954a04b4c.png)'
- en: 'Figure 6: Comparison of different sample design strategies.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同样本设计策略的比较。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Sentiment Analysis Performance ‣ 4.2 Experimental
    Results on Each Option ‣ 4 Experiments I: Evaluating The Impact of Each SDE Option
    ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") presents the results of the format adherence performances
    for Chat-LLMs, from which we find that: 1) While the Inst-first method improves
    sentiment analysis, it shows less stability in format adherence, especially in
    OOD scenarios, indicating that leading with instructions might increase format
    errors with unfamiliar content; 2) Structured design options lead to better format
    adherence abilities: A noticeable trend is that structured outputs, especially
    in the order JSON > Lines > Natural, have lower format error rates. JSON format,
    in particular, demonstrates strong adherence to the correct structure, highlighting
    a balance between output complexity and precision; 3) MI, NumLabel and CoT options
    can be quite unstable for certain LLMs, while other options are generally consistent
    across different models. In applications where stability is vital, these unstable
    options should be taken seriously; 4) Though improving the understanding or reasoning
    performances, CoT design puts LLMs at a higher risk of parsing failure for customized
    downstream tasks, underlining a trade-off for this option.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S4.F5 "图 5 ‣ 4.2.1 情感分析性能 ‣ 4.2 各选项的实验结果 ‣ 4 实验 I：评估每个 SDE 选项的影响 ‣ 样本设计工程：对使良好的下游微调样本的经验研究")
    展示了 Chat-LLMs 的格式遵循性能结果，我们发现：1）尽管 Inst-first 方法改善了情感分析，但在格式遵循方面稳定性较差，尤其是在 OOD
    场景中，表明以指令为主可能会在不熟悉的内容中增加格式错误；2）结构化设计选项具有更好的格式遵循能力：一个显著的趋势是结构化输出，特别是 JSON > Lines
    > Natural 顺序的格式错误率较低。JSON 格式特别展示了对正确结构的强遵循，突出输出复杂性与精确性之间的平衡；3）MI、NumLabel 和 CoT
    选项对某些 LLMs 可能非常不稳定，而其他选项在不同模型间通常较为一致。在稳定性至关重要的应用中，这些不稳定的选项应引起重视；4）虽然 CoT 设计能提高理解或推理表现，但对于定制化下游任务，LLMs
    遇到解析失败的风险更高，突显了这一选项的权衡。
- en: Considering LLMs’ format adherence alongside the understanding abilities is
    crucial for specialized downstream applications, suggesting a need for a balanced
    approach in industrial scenarios.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 LLMs 的格式遵循能力以及理解能力对于专业下游应用至关重要，这表明在工业场景中需要一种平衡的方法。
- en: '5 Experiments II: An Robust Integrated SDE Strategy'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验 II：一种稳健的集成 SDE 策略
- en: 'Based on the experimental evidence from the previous section, we propose an
    empirically strong SDE strategy (termed as ES-SDE) using the well-performing options:
    a combination of Inst-first, No-MI input designs and Lines, PU, TxtLabel output
    designs. We don’t use the CoT design because of its high annotation cost and relatively
    unstable output. In this section, we conduct comprehensive experiments to validate
    its effectiveness across different downstream tasks, as well as the robustness
    against perturbations in instructions or generation.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前一部分的实验证据，我们提出了一种经验上有效的 SDE 策略（称为 ES-SDE），使用表现良好的选项：结合 Inst-first、No-MI 输入设计和
    Lines、PU、TxtLabel 输出设计。我们不使用 CoT 设计，因为其标注成本高且输出相对不稳定。在本节中，我们进行全面实验以验证其在不同下游任务中的有效性，以及对指令或生成干扰的鲁棒性。
- en: 5.1 Settings
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: 'Tasks and datasets. To evaluate the effectiveness of ES-SDE, we conduct experiments
    on three challenging downstream tasks:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 任务和数据集。为了评估 ES-SDE 的有效性，我们在三个具有挑战性的下游任务上进行了实验：
- en: $\bullet$ GENIA Ohta et al. ([2002](#bib.bib27)). A nested named entity recognition
    (Nested-NER) dataset in the molecular biology domain, where ChatGPT (GPT-3.5)
    only achieves an F1 score of 50.89%, using 5-shot CoT reasoning Han et al. ([2023](#bib.bib16)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ GENIA Ohta 等人 ([2002](#bib.bib27))。这是一个分子生物学领域的嵌套命名实体识别（Nested-NER）数据集，其中
    ChatGPT（GPT-3.5）的 F1 分数仅为 50.89%，使用 5-shot CoT 推理 Han 等人 ([2023](#bib.bib16))。
- en: $\bullet$ MAVEN Wang et al. ([2020](#bib.bib35)). A general domain event detection
    (ED) dataset. Han et al. ([2023](#bib.bib16)) demonstrate that the performance
    of ChatGPT in ED tasks falls below expectations. We use the top-10 event types
    in our experiments.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MAVEN Wang 等人 ([2020](#bib.bib35))。这是一个通用领域事件检测（ED）数据集。Han 等人 ([2023](#bib.bib16))
    证明了 ChatGPT 在 ED 任务中的表现低于预期。我们在实验中使用了前 10 个事件类型。
- en: '$\bullet$ Review11. This is our self-collected Chinese MASA dataset that involves
    11 aspects, more complicated than the MASA tasks in Section [4](#S4 "4 Experiments
    I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ Review11。这是我们自收集的中文MASA数据集，涉及11个方面，比第 [4](#S4 "4 实验 I：评估每个 SDE 选项的影响
    ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")节中的MASA任务更复杂。
- en: 'Baselines. As a comparison to ES-SDE, we also propose an empirically weak SDE
    strategy (EW-SDE), combining Inst-last, Natural, and OU, while keeping other options
    the same. We naturally hypothesize that EW-SDE should be weaker than ES-SDE. Note
    that ES-SDE and EW-SDE are both evidence-based strategies according to the previous
    empirical results, therefore, we also set up a heuristic-based baseline, referring
    to the prompt designs from the study of Han et al. ([2023](#bib.bib16)), which
    are similar to a combination of Inst-first and OU options, with a "lines-of-list"
    output format. Examples of these strategies see Appendix [11](#A1.F11 "Figure
    11 ‣ A.8.2 Perplexity Analysis ‣ A.8 Can PE Guide SDE? Detailed Results ‣ Appendix
    A Appendix ‣ Sample Design Engineering: An Empirical Study of What Makes Good
    Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。作为对ES-SDE的对比，我们还提出了一种经验上较弱的SDE策略（EW-SDE），结合了Inst-last、Natural和OU，同时保持其他选项不变。我们自然假设EW-SDE应当比ES-SDE更弱。请注意，ES-SDE和EW-SDE都是根据先前经验结果得出的基于证据的策略，因此我们还建立了一个基于启发式的基准线，参考了Han等人研究中的提示设计（[2023](#bib.bib16)），这类似于Inst-first和OU选项的组合，输出格式为“lines-of-list”。这些策略的示例见附录
    [11](#A1.F11 "图 11 ‣ A.8.2 困惑度分析 ‣ A.8 PE 能否指导 SDE？详细结果 ‣ 附录 A 附录 ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")。
- en: 'Models. For a more generalized evaluation, we utilize two new LLMs, instead
    of those used in Section [4](#S4 "4 Experiments I: Evaluating The Impact of Each
    SDE Option ‣ Sample Design Engineering: An Empirical Study of What Makes Good
    Downstream Fine-Tuning Samples for LLMs"). Considering the task language, the
    llama2-7b-chat Touvron et al. ([2023b](#bib.bib33)) is used for GENIA and MAVEN
    and qwen1.5-4b-chat Bai et al. ([2023](#bib.bib2)), a very latest LLM, is used
    for Review11\. The training details are the same as Section [4](#S4 "4 Experiments
    I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。为了进行更为广泛的评估，我们使用了两个新的LLMs，而不是在第 [4](#S4 "4 实验 I：评估每个 SDE 选项的影响 ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")节中使用的模型。考虑到任务语言，使用了llama2-7b-chat
    Touvron等人（[2023b](#bib.bib33)）用于GENIA和MAVEN，而最新的LLM qwen1.5-4b-chat Bai等人（[2023](#bib.bib2)）则用于Review11。训练细节与第
    [4](#S4 "4 实验 I：评估每个 SDE 选项的影响 ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")节相同。
- en: 5.2 Results
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Format Adherence Performance ‣ 4.2 Experimental
    Results on Each Option ‣ 4 Experiments I: Evaluating The Impact of Each SDE Option
    ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") reports the comparison between different sample
    design strategies, from different perspectives. Soft-match F1 scores Han et al.
    ([2023](#bib.bib16)) are reported for GENIA and MAVEN, and $\kappa$ reported for
    Review 11\. More detailed results see Appendix [A.5](#A1.SS5 "A.5 Detailed Results
    on GENIA, MAVEN and Review11 ‣ Appendix A Appendix ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").
    Several key conclusions can be observed:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#S4.F6 "图 6 ‣ 4.2.2 格式遵循性能 ‣ 4.2 各选项的实验结果 ‣ 4 实验 I：评估每个 SDE 选项的影响 ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")
    报告了从不同角度比较不同样本设计策略的情况。报告了GENIA和MAVEN的软匹配F1分数，Han等人（[2023](#bib.bib16)），以及Review
    11的$\kappa$值。更详细的结果见附录 [A.5](#A1.SS5 "A.5 GENIA、MAVEN和Review11的详细结果 ‣ 附录 A 附录
    ‣ 样本设计工程：对LLMs有效下游微调样本的实证研究")。可以观察到几个关键结论：
- en: '1) ES-SDE maintains advantages across tasks and training sizes. Figure [6](#S4.F6
    "Figure 6 ‣ 4.2.2 Format Adherence Performance ‣ 4.2 Experimental Results on Each
    Option ‣ 4 Experiments I: Evaluating The Impact of Each SDE Option ‣ Sample Design
    Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples
    for LLMs")-(a) demonstrates a consistent trend that ES-SDE keeps its advantage
    as the training size increases from $500$ EW-SDE and heuristic samples in GENIA
    and Review11 tasks, indicating the high quality of ES-SDE samples. 2) Stable on
    decoding randomness. By default, the model employs a greedy decoding strategy
    (no sampling). Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Format Adherence Performance
    ‣ 4.2 Experimental Results on Each Option ‣ 4 Experiments I: Evaluating The Impact
    of Each SDE Option ‣ Sample Design Engineering: An Empirical Study of What Makes
    Good Downstream Fine-Tuning Samples for LLMs")-(b) shows the results when activating
    decoding sampling with varying random seeds. ES-SDE maintains exceptional stability
    across different seeds on three tasks. The adoption of decoding sampling tends
    to diminish the performances of both SW-SDE and heuristic strategies for GENIA
    and MAVEN, while ES-SDE gives stable performances. 3) Robust to instruction variation.
    For instructions about a specific task, we have various ways of expressing the
    same idea. Therefore, we validate the sensitivity of different strategies to different
    formulations of the instruction, by changing the common content to other formulations
    (examples in Appendix [12](#A1.F12 "Figure 12 ‣ A.8.2 Perplexity Analysis ‣ A.8
    Can PE Guide SDE? Detailed Results ‣ Appendix A Appendix ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")).
    As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Format Adherence Performance ‣
    4.2 Experimental Results on Each Option ‣ 4 Experiments I: Evaluating The Impact
    of Each SDE Option ‣ Sample Design Engineering: An Empirical Study of What Makes
    Good Downstream Fine-Tuning Samples for LLMs")-(c), ES-SDE keeps its edge in different
    variations, showing its robustness to instruction content.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1) ES-SDE在任务和训练规模上保持优势。图[6](#S4.F6 "图6 ‣ 4.2.2 格式遵守性能 ‣ 4.2 各选项的实验结果 ‣ 4 实验I：评估每个SDE选项的影响
    ‣ 样本设计工程：对LLMs的良好下游微调样本的实证研究")-(a)展示了ES-SDE在训练规模从$500$ EW-SDE和启发式样本增加的GENIA和Review11任务中保持其优势的持续趋势，表明ES-SDE样本的高质量。2)
    解码随机性的稳定性。默认情况下，模型采用贪婪解码策略（无采样）。图[6](#S4.F6 "图6 ‣ 4.2.2 格式遵守性能 ‣ 4.2 各选项的实验结果
    ‣ 4 实验I：评估每个SDE选项的影响 ‣ 样本设计工程：对LLMs的良好下游微调样本的实证研究")-(b)显示了激活解码采样并使用不同随机种子的结果。ES-SDE在三个任务中保持了不同种子下的卓越稳定性。解码采样的采用往往会降低SW-SDE和启发式策略在GENIA和MAVEN上的性能，而ES-SDE保持了稳定的表现。3)
    对指令变化的鲁棒性。对于特定任务的指令，我们有多种方式表达相同的观点。因此，我们通过将常见内容更改为其他表达方式来验证不同策略对不同指令表述的敏感性（见附录[12](#A1.F12
    "图12 ‣ A.8.2 困惑度分析 ‣ A.8 PE能指导SDE吗？详细结果 ‣ 附录A 附录 ‣ 样本设计工程：对LLMs的良好下游微调样本的实证研究")）。如图[6](#S4.F6
    "图6 ‣ 4.2.2 格式遵守性能 ‣ 4.2 各选项的实验结果 ‣ 4 实验I：评估每个SDE选项的影响 ‣ 样本设计工程：对LLMs的良好下游微调样本的实证研究")-(c)所示，ES-SDE在不同变化中保持其优势，显示出对指令内容的鲁棒性。
- en: Overall, ES-SDE represents a reliable and potent approach for the DT of LLMs,
    illustrating that—through a careful SDE process, LLMs can achieve much higher
    performances in downstream tasks. Note that ES-SDE may not be the best strategy
    for all tasks. A detailed investigation into SDE across a broader spectrum of
    tasks and models could yield even more effective strategies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，ES-SDE代表了一种可靠且强大的LLMs（大规模语言模型）DT（数据转化）方法，表明通过精心的SDE（样本设计工程）过程，LLMs可以在下游任务中实现更高的性能。需要注意的是，ES-SDE可能并不是所有任务的最佳策略。对更广泛任务和模型范围内的SDE进行详细调查可能会得出更有效的策略。
- en: 6 Can PE guide SDE? An Additional Analysis
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 PE能指导SDE吗？附加分析
- en: 'Prompts are the key to understand models’ innate qualities and capabilities.
    A good PE method often indicates some patterns that a LLM is more familiar with
    or excels in. A natural question is: can PE guide SDE? To answer this question,
    we craft zero-shot and ICL prompts according to different SDE options to evaluate
    their PE performances. Figure [7](#S6.F7 "Figure 7 ‣ 6 Can PE guide SDE? An Additional
    Analysis ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") reports the average rankings of SDE options and
    their corresponding prompts in the MASA ID tasks. Detailed results for each task
    see Appendix [A.8](#A1.SS8 "A.8 Can PE Guide SDE? Detailed Results ‣ Appendix
    A Appendix ‣ Sample Design Engineering: An Empirical Study of What Makes Good
    Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是理解模型固有特性和能力的关键。一个好的 PE 方法通常会指示出 LLM 更熟悉或擅长的某些模式。一个自然的问题是：PE 能否指导 SDE？为了回答这个问题，我们根据不同的
    SDE 选项制作了零-shot 和 ICL 提示，以评估它们的 PE 表现。图 [7](#S6.F7 "图 7 ‣ 6 PE 能否指导 SDE？附加分析 ‣
    样本设计工程：LLM 的良好下游微调样本的经验研究") 报告了 SDE 选项及其对应提示在 MASA ID 任务中的平均排名。每个任务的详细结果见附录 [A.8](#A1.SS8
    "A.8 PE 能否指导 SDE？详细结果 ‣ 附录 A 附录 ‣ 样本设计工程：LLM 的良好下游微调样本的经验研究")。
- en: 'Our analysis revealed some consistent patterns: Inst-first is an effective
    choice for both PE and SDE; CoT improves performances for both PE and SDE evaluations.
    However, there are also many counter-intuitive findings. For example, the OU option
    consistently harms DT performances according to our previous experiments, however,
    its corresponding prompts results in notably better zero-shot or ICL results for
    certain LLMs; Similarly, while the Natural option outperforms the Lines approach
    for base models in SDE, the reverse is true in zero-shot or ICL evaluations for
    models like c-llama2-base and intern-base. Gonen et al. ([2023](#bib.bib12)) showed
    through a wide range of tasks that the lower that lower perplexity (PPL) generally
    leads to better prompt designs. Inspired by this, we also conduct PPL analysis
    on the ICL prompts/predictions corresponding to each SDE options. Interestingly,
    OU-like prompt gives the highest averaged PPL scores across all options, which
    seems to be contradictory that OU brings better zero-shot or ICL results. The
    JSON format surprisingly achieves rather low PPL scores, however its SDE performances
    are worse than Lines.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析揭示了一些一致的模式：Inst-first 对于 PE 和 SDE 都是有效的选择；CoT 改善了 PE 和 SDE 评估的表现。然而，也有许多违背直觉的发现。例如，根据我们之前的实验，OU
    选项一贯损害 DT 表现，然而其对应的提示在某些 LLM 中却带来了显著更好的零-shot 或 ICL 结果；类似地，虽然 Natural 选项在 SDE
    中优于 Lines 方法，但对于像 c-llama2-base 和 intern-base 这样的模型，在零-shot 或 ICL 评估中则正好相反。Gonen
    等人 ([2023](#bib.bib12)) 通过广泛的任务展示了较低的困惑度（PPL）通常导致更好的提示设计。受此启发，我们也对每个 SDE 选项对应的
    ICL 提示/预测进行了 PPL 分析。有趣的是，OU 类提示在所有选项中给出了最高的平均 PPL 分数，这似乎与 OU 带来更好的零-shot 或 ICL
    结果相矛盾。JSON 格式意外地取得了相当低的 PPL 分数，但其 SDE 表现却比 Lines 更差。
- en: These findings highlight a complex landscape where prompt design patterns do
    not always align with SDE effectiveness, underscoring the nuanced relationship
    between PE and SDE.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现突显了一个复杂的格局，即提示设计模式并不总是与 SDE 效果对齐，强调了 PE 和 SDE 之间的微妙关系。
- en: '![Refer to caption](img/b2fa0cb1b2f734a38501b9da7ce84828.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2fa0cb1b2f734a38501b9da7ce84828.png)'
- en: 'Figure 7: Average rankings of the DT performances of SDE options and zero-shot/ICL/PPL
    rankings of their corresponding prompts. Results based on the MASA ID tasks across
    6 LLMs.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：SDE 选项的 DT 表现的平均排名及其对应提示的零-shot/ICL/PPL 排名。基于 6 个 LLM 的 MASA ID 任务的结果。
- en: 7 Conclusion & Future Work
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: In this study, we introduce SDE as an effective method to enhance the downstream-tuning
    performances of LLMs. Through comprehensive ID and OOD experiments involving six
    LLMs, we demonstrate the effects of various sample design strategies, uncovering
    some interesting patterns that are consistent across different LLMs. Building
    on these findings, we develop the ES-SDE approach, which integrates the most effective
    options. Our experiments on three new tasks with two additional LLMs consistently
    show ES-SDE’s superiority over baseline methods. Further analysis of the relationship
    between PE and SDE suggests that effective prompt designs do not necessarily translate
    to successful sample designs. This observation opens up avenues for more detailed
    investigations into the mechanisms of SDE in future research.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究引入了SDE作为提高LLMs下游调优性能的有效方法。通过对六个LLMs进行综合的ID和OOD实验，我们展示了各种样本设计策略的效果，揭示了一些在不同LLMs中一致的有趣模式。在这些发现的基础上，我们开发了ES-SDE方法，结合了最有效的选项。我们在三个新任务上与两个额外LLMs的实验一致地显示了ES-SDE相较于基线方法的优越性。对PE和SDE关系的进一步分析表明，有效的提示设计不一定转化为成功的样本设计。这一观察为未来研究深入探讨SDE机制提供了新方向。
- en: 8 Limitations
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: 'This research follows a two-step experimental approach. In the first step,
    we investigate the impact of each SDE option, the results are then used as evidence
    for the second step—proposing an empirically strong SDE combination strategy.
    As an empirical study, this research is subject to certain limitations:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究采用了两步实验方法。在第一步中，我们调查了每个SDE选项的影响，结果随后作为第二步——提出一个经验上强的SDE组合策略的证据。作为一项实证研究，本研究存在一些限制：
- en: '1.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: While we demonstrate that the experimental findings from the first phase are
    extendable to different downstream tasks, the applicability to other untested
    scenarios remains uncertain. For instance, although the Lines output design outperforms
    the JSON format in our current experiments, it is unclear if this advantage persists
    in more complex tasks with intricate structures. Future research will address
    these more challenging contexts;
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们展示了第一阶段的实验发现可以扩展到不同的下游任务，但其在其他未测试情境中的适用性仍不确定。例如，尽管在我们当前的实验中，Lines输出设计优于JSON格式，但尚不清楚这种优势是否在具有复杂结构的更复杂任务中持续存在。未来研究将会处理这些更具挑战性的背景；
- en: '2.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: With the rapid pace of advancements in LLMs, new and more sophisticated models
    are being introduced frequently. The models we used in our study were among the
    best open-source options available at the start of our research but have since
    been surpassed by newer releases. Although we assessed a total of 8 LLMs, including
    both base and chat variants, there remains a possibility that our findings may
    not be universally applicable to other models;
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着LLMs的快速发展，新型和更复杂的模型频繁推出。我们在研究中使用的模型是我们研究开始时最好的开源选项之一，但已被更新的版本超越。虽然我们评估了包括基础和聊天变体在内的总共8个LLMs，但我们的发现可能无法普遍适用于其他模型；
- en: 3\.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3\.
- en: 'Combining different SDE options poses significant challenges, particularly
    without prior validation experiments such as those described in Section [4](#S4
    "4 Experiments I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").
    The challenges are twofold. Firstly, unlike typical hyperparameters like learning
    rate or network layers, choosing different SDE options alters the training data
    itself, rendering traditional hyperparameter-tuning techniques such as Bayesian
    Optimization Snoek et al. ([2012](#bib.bib29)) less practical. Secondly, evaluating
    LLMs on downstream tasks is both resource-intensive and costly, due to the need
    for customized task metrics, parsing rules, and high model inference costs. Therefore,
    developing a more efficient framework for SDE studies is a critical objective
    for future research.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '结合不同SDE选项面临重大挑战，特别是在没有先前验证实验的情况下，例如第[4](#S4 "4 Experiments I: Evaluating The
    Impact of Each SDE Option ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs")节中描述的实验。这些挑战有两个方面。首先，与学习率或网络层等典型超参数不同，选择不同的SDE选项会改变训练数据本身，从而使传统的超参数调优技术如贝叶斯优化Snoek等（[2012](#bib.bib29)）变得不够实用。其次，对LLMs在下游任务中的评估既资源密集又成本高昂，需要定制的任务指标、解析规则和高模型推断成本。因此，开发更高效的SDE研究框架是未来研究的关键目标。'
- en: References
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等。2023年。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2023）Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu。2023年。Qwen 技术报告。*arXiv 预印本 arXiv:2309.16609*。
- en: Ben-David (2008) Arie Ben-David. 2008. Comparison of classification accuracy
    using cohen’s weighted kappa. *Expert Systems with Applications*, 34(2):825–832.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-David（2008）Arie Ben-David。2008年。使用 Cohen 的加权 kappa 比较分类准确性。*应用专家系统*，34(2):825–832。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Chen et al. (1998) Stanley F Chen, Douglas Beeferman, and Roni Rosenfeld. 1998.
    Evaluation metrics for language models.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（1998）Stanley F Chen, Douglas Beeferman, 和 Roni Rosenfeld。1998年。语言模型的评估指标。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等（2023）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
    Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann 等。2023年。Palm：通过路径扩展语言建模。*机器学习研究期刊*，24(240):1–113。
- en: 'Cohen (1968) J Cohen. 1968. Weighted kappa: nominal scale agreement with provision
    for scaled disagreement or partial credit. *Psychological bulletin*, 70(4):213–220.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen（1968）J Cohen。1968年。加权kappa：名义尺度的一致性，并考虑了缩放不一致或部分信用。*心理学公报*，70(4):213–220。
- en: Cohen (1960) Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
    *Educational and psychological measurement*, 20(1):37–46.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen（1960）Jacob Cohen。1960年。名义尺度的协议系数。*教育与心理测量*，20(1):37–46。
- en: Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and
    effective text encoding for chinese llama and alpaca. *arXiv preprint arXiv:2304.08177*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等（2023）Yiming Cui, Ziqing Yang, 和 Xin Yao。2023年。针对中文 llama 和 alpaca 的高效文本编码。*arXiv
    预印本 arXiv:2304.08177*。
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification
    reduces hallucination in large language models. *arXiv preprint arXiv:2309.11495*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala 等（2023）Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu,
    Xian Li, Asli Celikyilmaz, 和 Jason Weston。2023年。Chain-of-verification 减少了大型语言模型中的幻觉。*arXiv
    预印本 arXiv:2309.11495*。
- en: 'Galar et al. (2011) Mikel Galar, Alberto Fernández, Edurne Barrenechea, Humberto
    Bustince, and Francisco Herrera. 2011. An overview of ensemble methods for binary
    classifiers in multi-class problems: Experimental study on one-vs-one and one-vs-all
    schemes. *Pattern Recognition*, 44(8):1761–1776.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galar 等（2011）Mikel Galar, Alberto Fernández, Edurne Barrenechea, Humberto Bustince,
    和 Francisco Herrera。2011年。多类问题中二分类器的集成方法概述：一对一和一对多方案的实验研究。*模式识别*，44(8):1761–1776。
- en: 'Gonen et al. (2023) Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and
    Luke Zettlemoyer. 2023. Demystifying prompts in language models via perplexity
    estimation. In *Findings of the Association for Computational Linguistics: EMNLP
    2023*, pages 10136–10148.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonen等人（2023）Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, 和 Luke Zettlemoyer.
    2023. 通过困惑度估计揭示语言模型中的提示。发表于*计算语言学协会发现：EMNLP 2023*，页码10136–10148。
- en: 'Grandini et al. (2020) Margherita Grandini, Enrico Bagli, and Giorgio Visani.
    2020. Metrics for multi-class classification: an overview. *arXiv preprint arXiv:2008.05756*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grandini等人（2020）Margherita Grandini, Enrico Bagli, 和 Giorgio Visani. 2020. 多类别分类的度量：概述。*arXiv预印本
    arXiv:2008.05756*。
- en: 'Guo et al. (2022) Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang
    Huang, Nan Duan, and Weizhu Chen. 2022. Genius: Sketch-based language model pre-training
    via extreme and selective masking for text generation and augmentation. *arXiv
    preprint arXiv:2211.10330*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2022）Biyang Guo, Yeyun Gong, Yelong Shen, Songqiao Han, Hailiang Huang,
    Nan Duan, 和 Weizhu Chen. 2022. Genius：通过极端和选择性掩蔽进行文本生成和增强的基于草图的语言模型预训练。*arXiv预印本
    arXiv:2211.10330*。
- en: Guo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie,
    Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts?
    comparison corpus, evaluation, and detection. *arXiv preprint arXiv:2301.07597*.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2023）Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan
    Ding, Jianwei Yue, 和 Yupeng Wu. 2023. ChatGPT与人类专家有多接近？比较语料库、评估和检测。*arXiv预印本 arXiv:2301.07597*。
- en: Han et al. (2023) Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, and
    Xiang Wan. 2023. Is information extraction solved by chatgpt? an analysis of performance,
    evaluation criteria, robustness and errors. *arXiv preprint arXiv:2305.14450*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han等人（2023）Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, 和 Xiang
    Wan. 2023. 信息提取是否被ChatGPT解决了？对性能、评估标准、鲁棒性和错误的分析。*arXiv预印本 arXiv:2305.14450*。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人（2021）Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
    Lu Wang, Weizhu Chen, 等. 2021. Lora：大型语言模型的低秩适配。发表于*国际学习表征会议*。
- en: 'Kim et al. (2023) Seungone Kim, Se Joo, Doyoung Kim, Joel Jang, Seonghyeon
    Ye, Jamin Shin, and Minjoon Seo. 2023. The cot collection: Improving zero-shot
    and few-shot learning of language models via chain-of-thought fine-tuning. In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 12685–12708.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等人（2023）Seungone Kim, Se Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin
    Shin, 和 Minjoon Seo. 2023. COT集合：通过链式思维微调改进语言模型的零样本和少样本学习。发表于*2023年自然语言处理经验方法会议论文集*，页码12685–12708。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021
    Conference on Empirical Methods in Natural Language Processing*, pages 3045–3059.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester等人（2021）Brian Lester, Rami Al-Rfou, 和 Noah Constant. 2021. 参数高效提示调优的规模效应。发表于*2021年自然语言处理经验方法会议论文集*，页码3045–3059。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis等人（2019）Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, 和 Luke Zettlemoyer. 2019. BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练。*arXiv预印本
    arXiv:1910.13461*。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis等人（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    等. 2020. 用于知识密集型NLP任务的检索增强生成。*神经信息处理系统进展*，33:9459–9474。
- en: Li et al. (2023) Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou,
    Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023. Large language models
    understand and can be enhanced by emotional stimuli. *arXiv preprint arXiv:2307.11760*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023）Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun
    Lian, Fang Luo, Qiang Yang, 和 Xing Xie. 2023. 大型语言模型理解并能通过情感刺激得到增强。*arXiv预印本 arXiv:2307.11760*。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 4582–4597.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang（2021）Xiang Lisa Li 和 Percy Liang。2021。前缀调优：优化生成的连续提示。在 *第 59 届计算语言学协会年会暨第
    11 届国际自然语言处理联合会议（第 1 卷：长篇论文）*，第 4582–4597 页。
- en: Liu et al. (2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. 2023. Gpt understands, too. *AI Open*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Xiao Liu、Yanan Zheng、Zhengxiao Du、Ming Ding、Yujie Qian、Zhilin Yang
    和 Jie Tang。2023。GPT 也懂。*AI Open*。
- en: 'Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won
    Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The
    flan collection: Designing data and methods for effective instruction tuning.
    In *International Conference on Machine Learning*, pages 22631–22648\. PMLR.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longpre 等（2023）Shayne Longpre、Le Hou、Tu Vu、Albert Webson、Hyung Won Chung、Yi
    Tay、Denny Zhou、Quoc V Le、Barret Zoph、Jason Wei 等。2023。Flan 集合：为有效指令调优设计的数据和方法。在
    *国际机器学习会议*，第 22631–22648 页。PMLR。
- en: 'Mishra et al. (2022) Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh
    Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing
    instructions. In *Proceedings of the 60th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pages 3470–3487.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等（2022）Swaroop Mishra、Daniel Khashabi、Chitta Baral 和 Hannaneh Hajishirzi。2022。通过自然语言众包指令实现跨任务泛化。在
    *第 60 届计算语言学协会年会（第 1 卷：长篇论文）*，第 3470–3487 页。
- en: 'Ohta et al. (2002) Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and
    Junichi Tsujii. 2002. The genia corpus: An annotated research abstract corpus
    in molecular biology domain. In *Proceedings of the human language technology
    conference*, pages 73–77\. Citeseer.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohta 等（2002）Tomoko Ohta、Yuka Tateisi、Jin-Dong Kim、Hideki Mima 和 Junichi Tsujii。2002。Genia
    语料库：一个在分子生物学领域的标注研究摘要语料库。在 *人类语言技术会议论文集*，第 73–77 页。Citeseer。
- en: 'Sahoo et al. (2024) Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija
    Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering
    in large language models: Techniques and applications. *arXiv preprint arXiv:2402.07927*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahoo 等（2024）Pranab Sahoo、Ayush Kumar Singh、Sriparna Saha、Vinija Jain、Samrat
    Mondal 和 Aman Chadha。2024。大型语言模型中的提示工程系统性调查：技术与应用。*arXiv 预印本 arXiv:2402.07927*。
- en: Snoek et al. (2012) Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical
    bayesian optimization of machine learning algorithms. *Advances in neural information
    processing systems*, 25.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snoek 等（2012）Jasper Snoek、Hugo Larochelle 和 Ryan P Adams。2012。机器学习算法的实用贝叶斯优化。*神经信息处理系统进展*，25。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等（2023）Rohan Taori、Ishaan Gulrajani、Tianyi Zhang、Yann Dubois、Xuechen Li、Carlos
    Guestrin、Percy Liang 和 Tatsunori B Hashimoto。2023。斯坦福 alpaca：一个遵循指令的 llama 模型。
- en: 'Team (2023) InternLM Team. 2023. Internlm: A multilingual language model with
    progressively enhanced capabilities.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team（2023）InternLM 团队。2023。Internlm：一个具有逐步增强能力的多语言模型。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。2023a。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。2023b。Llama
    2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Wang et al. (2023a) Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing
    Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, Chenxi Yue, Haiyang Zhang,
    Yi-Hsueh Liu, Xiang Li, Bao Ge, Dajiang Zhu, Yixuan Yuan, Dinggang Shen, Tianming
    Liu, and Shu Zhang. 2023a. Prompt engineering for healthcare: Methodologies and
    applications. *ArXiv*, abs/2304.14670.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing
    Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, Chenxi Yue, Haiyang Zhang,
    Yi-Hsueh Liu, Xiang Li, Bao Ge, Dajiang Zhu, Yixuan Yuan, Dinggang Shen, Tianming
    Liu, 和 Shu Zhang. 2023a. 医疗领域的提示工程：方法论与应用。*ArXiv*, abs/2304.14670。
- en: 'Wang et al. (2020) Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han,
    Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. 2020. Maven: A massive
    general domain event detection dataset. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1652–1671.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020) Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han,
    Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, 和 Jie Zhou. 2020. Maven: 大规模通用领域事件检测数据集。发表于
    *2020年自然语言处理经验方法会议 (EMNLP)*，页码 1652–1671。'
- en: 'Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct:
    Aligning language models with self-generated instructions. In *The 61st Annual
    Meeting Of The Association For Computational Linguistics*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. 2023b. 自我指导：通过自生成指令对齐语言模型。发表于
    *第61届计算语言学协会年会*。
- en: 'Weber et al. (2023) Lucas Weber, Elsa M. Bruni Bruni, and Dieuwke Hupkes. 2023.
    Mind the instructions: a holistic evaluation of consistency and interactions in
    prompt-based learning. In *Proceedings of the 27th Conference on Computational
    Natural Language Learning (CoNLL)*, pages 294–313.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weber et al. (2023) Lucas Weber, Elsa M. Bruni Bruni, 和 Dieuwke Hupkes. 2023.
    注意指令：对基于提示学习中一致性和交互的全面评估。发表于 *第27届计算自然语言学习会议 (CoNLL)*，页码 294–313。
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language
    models are zero-shot learners. In *International Conference on Learning Representations*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le. 2021. 微调语言模型是零样本学习者。发表于
    *国际学习表征会议*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. 连锁思维提示在大型语言模型中引发推理。*神经信息处理系统进展*,
    35:24824–24837。
- en: Wei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,
    Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2023. Zero-shot
    information extraction via chatting with chatgpt. *arXiv preprint arXiv:2302.10205*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,
    Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, 等. 2023. 通过与 chatgpt
    聊天进行零样本信息提取。*arXiv preprint arXiv:2302.10205*。
- en: 'Yang et al. (2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian,
    Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. Baichuan 2: Open
    large-scale language models. *arXiv preprint arXiv:2309.10305*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian,
    Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, 等. 2023. Baichuan 2: 开放的大规模语言模型。*arXiv
    preprint arXiv:2309.10305*。'
- en: Yilmaz and Demirhan (2023) Ayfer Ezgi Yilmaz and Haydar Demirhan. 2023. Weighted
    kappa measures for ordinal multi-class classification performance. *Applied Soft
    Computing*, 134:110020.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yilmaz and Demirhan (2023) Ayfer Ezgi Yilmaz 和 Haydar Demirhan. 2023. 有序多类分类性能的加权
    kappa 衡量。*Applied Soft Computing*, 134:110020。
- en: Zhang et al. (2023) Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei
    Song. 2023. A survey of controllable text generation using transformer-based pre-trained
    language models. *ACM Computing Surveys*, 56(3):1–37.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, 和 Dawei
    Song. 2023. 使用基于变换器的预训练语言模型进行可控文本生成的综述。*ACM Computing Surveys*, 56(3):1–37。
- en: 'Zhou et al. (2024) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less
    is more for alignment. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2024) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao
    Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, 等. 2024. Lima: 少即是多用于对齐。*神经信息处理系统进展*,
    36。'
- en: Appendix A Appendix
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Metrics for MASA
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 MASA 的度量标准
- en: 'Weighted Kappa. Considering the imbalance of different aspects and the ordinal
    nature of labels, weighted agreement measures are proved to be more effective
    than traditional metrics Ben-David ([2008](#bib.bib3)); Galar et al. ([2011](#bib.bib11));
    Grandini et al. ([2020](#bib.bib13)). Thus we adopt Weighted Kappa (Cohen, [1968](#bib.bib7);
    Yilmaz and Demirhan, [2023](#bib.bib42)) as the measure of classification effect,
    which is an extension of Cohen’s Kappa (Cohen, [1960](#bib.bib8)). Weighted Kappa
    $\kappa$. The probabilities $p_{ij},p_{i.},p_{.j}$, enables a nuanced assessment
    of different error degrees. For example, classifying "positive" as "negative"
    is more detrimental than classifying "positive" as "neutral," hence a higher penalty
    should be imposed on the former. Based on the feedback from enterprises in practical
    applications, we define the weight matrix without loss of generality as Table
    [1](#A1.T1 "Table 1 ‣ A.1 Metrics for MASA ‣ Appendix A Appendix ‣ Sample Design
    Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples
    for LLMs").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 加权 Kappa。考虑到不同方面的不平衡和标签的序数性质，加权一致性度量已被证明比传统指标更有效（Ben-David [2008](#bib.bib3)；Galar
    等 [2011](#bib.bib11)；Grandini 等 [2020](#bib.bib13)）。因此，我们采用加权 Kappa（Cohen, [1968](#bib.bib7)；Yilmaz
    和 Demirhan, [2023](#bib.bib42)）作为分类效果的度量，它是 Cohen’s Kappa（Cohen, [1960](#bib.bib8)）的扩展。加权
    Kappa $\kappa$。概率 $p_{ij},p_{i.},p_{.j}$，使得对不同错误程度进行细致评估成为可能。例如，将“正面”分类为“负面”比将“正面”分类为“中性”更有害，因此对前者应施加更高的惩罚。根据企业在实际应用中的反馈，我们定义了权重矩阵，具体见表
    [1](#A1.T1 "表 1 ‣ A.1 MASA 的度量 ‣ 附录 A 附录 ‣ 样本设计工程：对什么构成好的下游微调样本的实证研究")。
- en: '|  | Pre-Pos | Pre-Neu | Pre-Neg | Pre-Unm |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 预-正 | 预-中性 | 预-负 | 预-未标记 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Label-Pos | 1 | 1/2 | 0 | 1/2 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 标签-正 | 1 | 1/2 | 0 | 1/2 |'
- en: '| Label-Neu | 2/3 | 1 | 2/3 | 2/3 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 标签-中性 | 2/3 | 1 | 2/3 | 2/3 |'
- en: '| Label-Neg | 0 | 1/2 | 1 | 1/2 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 标签-负 | 0 | 1/2 | 1 | 1/2 |'
- en: '| Label-Unm | 1/2 | 2/3 | 1/2 | 1 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 标签-未标记 | 1/2 | 2/3 | 1/2 | 1 |'
- en: 'Table 1: Weight matrix for calculating weighted Kappa.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于计算加权 Kappa 的权重矩阵。
- en: '|  |  | TrainSet (size=500) | TrainSet (size=1000) | TestSet |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 训练集 (大小=500) | 训练集 (大小=1000) | 测试集 |'
- en: '|  |  | Pos | Neu | Neg | Unm | Pos | Neu | Neg | Unm | Pos | Neu | Neg | Unm
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 正 | 中性 | 负 | 未标记 | 正 | 中性 | 负 | 未标记 | 正 | 中性 | 负 | 未标记 |'
- en: '| D1 | F | 65.20 | 15.00 | 18.80 | 1.00 | 66.60 | 13.70 | 18.30 | 1.40 | 66.01
    | 12.23 | 20.12 | 1.64 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| D1 | F | 65.20 | 15.00 | 18.80 | 1.00 | 66.60 | 13.70 | 18.30 | 1.40 | 66.01
    | 12.23 | 20.12 | 1.64 |'
- en: '| B | 22.20 | 4.20 | 8.20 | 65.40 | 23.50 | 3.60 | 7.20 | 65.70 | 21.50 | 3.15
    | 6.29 | 69.07 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| B | 22.20 | 4.20 | 8.20 | 65.40 | 23.50 | 3.60 | 7.20 | 65.70 | 21.50 | 3.15
    | 6.29 | 69.07 |'
- en: '| P | 33.40 | 13.00 | 15.60 | 38.00 | 35.60 | 10.70 | 15.80 | 37.90 | 36.64
    | 10.24 | 13.97 | 39.15 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| P | 33.40 | 13.00 | 15.60 | 38.00 | 35.60 | 10.70 | 15.80 | 37.90 | 36.64
    | 10.24 | 13.97 | 39.15 |'
- en: '| H | 14.80 | 1.20 | 6.00 | 78.00 | 17.10 | 1.00 | 5.50 | 76.40 | 16.12 | 0.82
    | 5.58 | 77.48 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| H | 14.80 | 1.20 | 6.00 | 78.00 | 17.10 | 1.00 | 5.50 | 76.40 | 16.12 | 0.82
    | 5.58 | 77.48 |'
- en: '| SA | 48.80 | 3.60 | 14.00 | 33.60 | 47.90 | 4.10 | 13.60 | 34.40 | 42.73
    | 3.46 | 13.87 | 39.94 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SA | 48.80 | 3.60 | 14.00 | 33.60 | 47.90 | 4.10 | 13.60 | 34.40 | 42.73
    | 3.46 | 13.87 | 39.94 |'
- en: '| PC | 4.40 | 0.60 | 1.40 | 93.60 | 4.80 | 0.30 | 1.90 | 93.00 | 3.93 | 0.34
    | 1.56 | 94.18 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| PC | 4.40 | 0.60 | 1.40 | 93.60 | 4.80 | 0.30 | 1.90 | 93.00 | 3.93 | 0.34
    | 1.56 | 94.18 |'
- en: '| D2 | TC | 52.40 | 13.20 | 7.60 | 26.80 | 53.10 | 13.20 | 8.10 | 25.60 | 48.56
    | 12.84 | 7.03 | 31.57 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| D2 | TC | 52.40 | 13.20 | 7.60 | 26.80 | 53.10 | 13.20 | 8.10 | 25.60 | 48.56
    | 12.84 | 7.03 | 31.57 |'
- en: '| Q | 18.80 | 8.20 | 11.20 | 61.80 | 17.90 | 10.10 | 11.00 | 61.00 | 14.67
    | 10.00 | 10.44 | 64.89 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Q | 18.80 | 8.20 | 11.20 | 61.80 | 17.90 | 10.10 | 11.00 | 61.00 | 14.67
    | 10.00 | 10.44 | 64.89 |'
- en: '| SS | 16.80 | 3.60 | 8.20 | 71.40 | 15.70 | 3.80 | 8.90 | 71.60 | 14.86 |
    3.15 | 8.58 | 73.41 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SS | 16.80 | 3.60 | 8.20 | 71.40 | 15.70 | 3.80 | 8.90 | 71.60 | 14.86 |
    3.15 | 8.58 | 73.41 |'
- en: '| D | 46.00 | 8.20 | 4.20 | 41.60 | 48.50 | 8.10 | 4.30 | 39.10 | 43.10 | 7.68
    | 5.28 | 43.93 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| D | 46.00 | 8.20 | 4.20 | 41.60 | 48.50 | 8.10 | 4.30 | 39.10 | 43.10 | 7.68
    | 5.28 | 43.93 |'
- en: '| N | 1.00 | 1.40 | 2.80 | 94.80 | 1.40 | 1.30 | 3.40 | 93.90 | 2.10 | 1.08
    | 3.36 | 93.46 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| N | 1.00 | 1.40 | 2.80 | 94.80 | 1.40 | 1.30 | 3.40 | 93.90 | 2.10 | 1.08
    | 3.36 | 93.46 |'
- en: 'Table 2: Label distribution(%) in various aspects of train set and test set.
    D1 contains annotations for 6 aspects—food (F), beverage (B), price (P), hygiene
    (H), staff attitude (SA), and parking convenience (PC); D2 contains annotations
    for 5 different aspects—traffic convenience (TC), queuing (Q), serving speed (SS),
    decoration (D), and noise (N). We use ’Pos’, ‘Neu’, ’Neg’, ‘Unm’ to represent
    Positive, Neutral, Negative and Unmentioned labels, respectively.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：训练集和测试集中各方面的标签分布（%）。D1 包含 6 个方面的注释——食物 (F)、饮料 (B)、价格 (P)、卫生 (H)、员工态度 (SA)
    和停车便利性 (PC)；D2 包含 5 个不同方面的注释——交通便利性 (TC)、排队 (Q)、服务速度 (SS)、装饰 (D) 和噪音 (N)。我们使用’Pos’、‘Neu’、’Neg’、‘Unm’来分别表示积极、中性、消极和未提及的标签。
- en: 'Format adherence. Format adherence not only ensures that outputs from the model
    can be reliably parsed and utilized in practical applications, but also reflects
    the model’s ability to understand the context and the nuances of different instructions.
    We set up parsers according to the prescribed formats of different designs, then
    we calculate the ratio of predictions that cannot be successfully parsed with
    our output parser. Considering the inherently uncertainty nature of generative
    language models, we relaxed the format such as the expression of aspects and sentiments.
    Meanwhile, in order to compare the content correctness between designs more fairly,
    for some cases such as common punctuation errors, we will correct it into the
    required format when calculating the Kappa. Figure [10](#A1.F10 "Figure 10 ‣ A.8.2
    Perplexity Analysis ‣ A.8 Can PE Guide SDE? Detailed Results ‣ Appendix A Appendix
    ‣ Sample Design Engineering: An Empirical Study of What Makes Good Downstream
    Fine-Tuning Samples for LLMs") shows a variety of representative format error
    types and how they are processed by the parsers we design.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '格式遵循。格式遵循不仅确保模型输出可以可靠地解析并在实际应用中使用，还反映了模型理解上下文和不同指令细微差别的能力。我们根据不同设计的规定格式设置解析器，然后计算不能被我们的输出解析器成功解析的预测比例。考虑到生成语言模型固有的不确定性，我们放宽了格式要求，例如方面和情感的表达。同时，为了更公平地比较不同设计之间的内容正确性，对于一些常见的标点错误等情况，在计算
    Kappa 时我们将其纠正为所需格式。图 [10](#A1.F10 "Figure 10 ‣ A.8.2 Perplexity Analysis ‣ A.8
    Can PE Guide SDE? Detailed Results ‣ Appendix A Appendix ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")
    显示了各种代表性的格式错误类型以及它们如何被我们设计的解析器处理。'
- en: A.2 Datasets and Training Settings
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 数据集和训练设置
- en: 'Table [2](#A1.T2 "Table 2 ‣ A.1 Metrics for MASA ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs") shows the label distribution of each aspect for two domains
    D1 and D2, where we can see the distributions are highly unbalanced.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#A1.T2 "Table 2 ‣ A.1 Metrics for MASA ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs") 显示了两个领域 D1 和 D2 中每个方面的标签分布，我们可以看到分布非常不平衡。'
- en: 'The training setup was as follows: learning rate set to 1e-4, batch size of
    4, LoRA rank of 8 LoRA alpha of 32, LoRA dropout of 0.1.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 训练设置如下：学习率设置为 1e-4，批量大小为 4，LoRA 等级为 8，LoRA alpha 为 32，LoRA dropout 为 0.1。
- en: A.3 Sample Design Examples
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 示例设计
- en: 'Figure [9](#A1.F9 "Figure 9 ‣ A.6 Additional Analysis on Inst-last and Inst-first
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs") shows a detailed example
    of our sample designs on MASA tasks.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](#A1.F9 "Figure 9 ‣ A.6 Additional Analysis on Inst-last and Inst-first
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs") 显示了我们在 MASA 任务上的示例设计的详细例子。'
- en: A.4 Detailed Evaluations of Each SDE Option
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 每个 SDE 选项的详细评估
- en: 'The detailed results of in-domain (ID) and out-of-domain (OOD) evaluations
    on the MASA task of different SDE options across six LLMs are shown in Table [3](#A1.T3
    "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option ‣ Appendix A Appendix ‣
    Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs") to Table [8](#A1.T8 "Table 8 ‣ A.4 Detailed Evaluations of
    Each SDE Option ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs"), including
    both the sentiment analysis performances ($\kappa$) and the format adherence performances
    (format error rate). An averaged results of training size 500 and 1000 of ID and
    OOD scenarios are visualized in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Settings ‣ 4
    Experiments I: Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同SDE选项在六种LLM上进行的领域内（ID）和领域外（OOD）评估的详细结果，请参见表[3](#A1.T3 "表3 ‣ A.4 各SDE选项的详细评估
    ‣ 附录A ‣ 样本设计工程：什么使LLM的下游微调样本变得优秀的实证研究")到表[8](#A1.T8 "表8 ‣ A.4 各SDE选项的详细评估 ‣ 附录A
    ‣ 样本设计工程：什么使LLM的下游微调样本变得优秀的实证研究")，包括情感分析性能（$\kappa$）和格式遵守性能（格式错误率）。训练规模为500和1000的ID和OOD场景的平均结果在图[4](#S4.F4
    "图4 ‣ 4.1 设置 ‣ 4 实验I：评估每个SDE选项的影响 ‣ 样本设计工程：什么使LLM的下游微调样本变得优秀的实证研究")中可视化展示。
- en: '| model: c-llama2-chat | Weighted Kappa $\kappa$ | # Wrong format (7969 test
    samples in total) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| model: c-llama2-chat | 加权 Kappa $\kappa$ | # 错误格式（共7969个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | .8091 | .6882 | .5243 | .7217 | 0 | 0 | 2 | 2
    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | .8091 | .6882 | .5243 | .7217 | 0 | 0 | 2 | 2 |'
- en: '| Inst-first, _ | .8136 | .7079 | .5124 | .7223 | 0 | 0 | 9 | 15 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | .8136 | .7079 | .5124 | .7223 | 0 | 0 | 9 | 15 |'
- en: '| No-inst, _ | .7757 | .6626 | \ | \ | 20 | 1 | \ | \ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| No-inst, _ | .7757 | .6626 | \ | \ | 20 | 1 | \ | \ |'
- en: '| _, MI | .6187 | .6187 | .4806 | .2756 | 1 | 0 | 0 | 1079 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | .6187 | .6187 | .4806 | .2756 | 1 | 0 | 0 | 1079 |'
- en: '| Output | Natural, TxtLabel, PU | .8091 | .6882 | .5243 | .7217 | 0 | 0 |
    2 | 2 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | Natural, TxtLabel, PU | .8091 | .6882 | .5243 | .7217 | 0 | 0 | 2 |
    2 |'
- en: '| Lines, _, _ | .8083 | .6969 | .5068 | .7447 | 0 | 0 | 0 | 0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Lines, _, _ | .8083 | .6969 | .5068 | .7447 | 0 | 0 | 0 | 0 |'
- en: '| JSON, _, _ | .8086 | .6952 | .4905 | .7354 | 0 | 0 | 0 | 0 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | .8086 | .6952 | .4905 | .7354 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | .7697 | .6373 | .4221 | .6723 | 3 | 1 | 0 | 1260 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | .7697 | .6373 | .4221 | .6723 | 3 | 1 | 0 | 1260 |'
- en: '|  | _, _, OU | .7934 | .6005 | .5282 | .6203 | 0 | 0 | 87 | 0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | _, _, OU | .7934 | .6005 | .5282 | .6203 | 0 | 0 | 87 | 0 |'
- en: '| Reasoning | No-CoT | .7934 | .6005 | .5282 | .6203 | 0 | 0 | 87 | 0 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning | No-CoT | .7934 | .6005 | .5282 | .6203 | 0 | 0 | 87 | 0 |'
- en: '| CoT | .7928 | .6873 | .5249 | .7085 | 56 | 65 | 36 | 282 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| CoT | .7928 | .6873 | .5249 | .7085 | 56 | 65 | 36 | 282 |'
- en: '| R-CoT | .8074 | .6752 | .4726 | .7297 | 93 | 65 | 141 | 263 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | .8074 | .6752 | .4726 | .7297 | 93 | 65 | 141 | 263 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.8256 | 0.7110 | 0.5518 | 0.7312 | 0 | 0 | 0
    | 3 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | 0.8256 | 0.7110 | 0.5518 | 0.7312 | 0 | 0 | 0 | 3
    |'
- en: '| Inst-first, _ | 0.8236 | 0.7090 | 0.5483 | 0.7264 | 0 | 0 | 5 | 1 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | 0.8236 | 0.7090 | 0.5483 | 0.7264 | 0 | 0 | 5 | 1 |'
- en: '| No-inst, _ | 0.8003 | 0.6920 | \ | \ | 6 | 4 | \ | \ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| No-inst, _ | 0.8003 | 0.6920 | \ | \ | 6 | 4 | \ | \ |'
- en: '| _, MI | 0.8113 | 0.6700 | 0.5095 | 0.5182 | 0 | 0 | 0 | 728 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.8113 | 0.6700 | 0.5095 | 0.5182 | 0 | 0 | 0 | 728 |'
- en: '| Output | Natural, TxtLabel, PU | 0.7916 | 0.7253 | 0.5303 | 0.7356 | 0 |
    0 | 0 | 3 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | Natural, TxtLabel, PU | 0.7916 | 0.7253 | 0.5303 | 0.7356 | 0 | 0 |
    0 | 3 |'
- en: '| Lines, _, _ | 0.8259 | 0.7118 | 0.5560 | 0.7452 | 0 | 0 | 0 | 0 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Lines, _, _ | 0.8259 | 0.7118 | 0.5560 | 0.7452 | 0 | 0 | 0 | 0 |'
- en: '| JSON, _, _ | 0.8249 | 0.7094 | 0.5488 | 0.7432 | 0 | 0 | 0 | 0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8249 | 0.7094 | 0.5488 | 0.7432 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.7624 | 0.6604 | 0.4210 | 0.6840 | 2 | 2 | 0 | 765 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7624 | 0.6604 | 0.4210 | 0.6840 | 2 | 2 | 0 | 765 |'
- en: '| _, _, OU | 0.8172 | 0.7125 | 0.5511 | 0.6746 | 0 | 0 | 493 | 1 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.8172 | 0.7125 | 0.5511 | 0.6746 | 0 | 0 | 493 | 1 |'
- en: '| Reasoning | No-CoT | 0.8018 | 0.7175 | 0.5332 | 0.7323 | 0 | 0 | 493 | 1
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning | No-CoT | 0.8018 | 0.7175 | 0.5332 | 0.7323 | 0 | 0 | 493 | 1
    |'
- en: '| CoT | 0.8111 | 0.7111 | 0.5354 | 0.7311 | 59 | 24 | 30 | 253 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.8111 | 0.7111 | 0.5354 | 0.7311 | 59 | 24 | 30 | 253 |'
- en: '| R-CoT | 0.8214 | 0.7137 | 0.5085 | 0.7532 | 51 | 25 | 75 | 115 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8214 | 0.7137 | 0.5085 | 0.7532 | 51 | 25 | 75 | 115 |'
- en: 'Table 3: MASA evaluations of each SDE option for model c-llama2-chat. The first
    method in each group is the group baseline. "_" means keeping the same option
    with the group baseline.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 每种SDE选项的MASA评估结果，模型为c-llama2-chat。每组中的第一个方法是组基线。 "_"表示与组基线保持相同的选项。'
- en: '| model: c-llama2-base | Weighted Kappa $\kappa$ | # Wrong format (7969 test
    samples in total) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 模型: c-llama2-base | 加权Kappa $\kappa$ | # 错误格式（总共7969个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.8067 | 0.6801 | 0.5246 | 0.7000 | 0 | 0 | 6
    | 98 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.8067 | 0.6801 | 0.5246 | 0.7000 | 0 | 0 | 6 | 98 |'
- en: '| Inst-first, _ | 0.8092 | 0.6921 | 0.5575 | 0.6794 | 0 | 0 | 34 | 3 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8092 | 0.6921 | 0.5575 | 0.6794 | 0 | 0 | 34 | 3 |'
- en: '| No-inst, _ | 0.7762 | 0.6511 | \ | \ | 0 | 1 | \ | \ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.7762 | 0.6511 | \ | \ | 0 | 1 | \ | \ |'
- en: '| _, MI | 0.7778 | 0.5024 | 0.4946 | 0.4184 | 2 | 0 | 118 | 0 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7778 | 0.5024 | 0.4946 | 0.4184 | 2 | 0 | 118 | 0 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8067 | 0.6801 | 0.5246 | 0.7000 | 0 |
    0 | 6 | 98 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.8067 | 0.6801 | 0.5246 | 0.7000 | 0 | 0 | 6 | 98
    |'
- en: '| Lines, _, _ | 0.8066 | 0.6410 | 0.5128 | 0.6622 | 0 | 0 | 19 | 0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.8066 | 0.6410 | 0.5128 | 0.6622 | 0 | 0 | 19 | 0 |'
- en: '| JSON, _, _ | 0.8010 | 0.6242 | 0.5170 | 0.6287 | 0 | 0 | 0 | 0 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8010 | 0.6242 | 0.5170 | 0.6287 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.7728 | 0.5949 | 0.5155 | 0.6296 | 14 | 1 | 26 | 356 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7728 | 0.5949 | 0.5155 | 0.6296 | 14 | 1 | 26 | 356 |'
- en: '| _, _, OU | 0.7746 | 0.5012 | 0.4199 | 0.5711 | 0 | 3 | 300 | 7 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7746 | 0.5012 | 0.4199 | 0.5711 | 0 | 3 | 300 | 7 |'
- en: '| Reasoning | No-CoT | 0.8010 | 0.6242 | 0.5170 | 0.6287 | 0 | 0 | 0 | 0 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.8010 | 0.6242 | 0.5170 | 0.6287 | 0 | 0 | 0 | 0 |'
- en: '| CoT | 0.7789 | 0.6652 | 0.4649 | 0.6974 | 83 | 82 | 33 | 226 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7789 | 0.6652 | 0.4649 | 0.6974 | 83 | 82 | 33 | 226 |'
- en: '| R-CoT | 0.8019 | 0.6428 | 0.4657 | 0.4199 | 88 | 11 | 87 | 1823 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8019 | 0.6428 | 0.4657 | 0.4199 | 88 | 11 | 87 | 1823 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.8237 | 0.7011 | 0.6010 | 0.7197 | 0 | 0 | 3
    | 177 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.8237 | 0.7011 | 0.6010 | 0.7197 | 0 | 0 | 3 | 177 |'
- en: '| Inst-first, _ | 0.8231 | 0.7068 | 0.6069 | 0.6956 | 0 | 2 | 16 | 28 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8231 | 0.7068 | 0.6069 | 0.6956 | 0 | 2 | 16 | 28 |'
- en: '| No-inst, _ | 0.7957 | 0.6882 | \ | \ | 2 | 2 | \ | \ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.7957 | 0.6882 | \ | \ | 2 | 2 | \ | \ |'
- en: '| _, MI | 0.8048 | 0.6174 | 0.5306 | 0.6390 | 0 | 3 | 139 | 6 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.8048 | 0.6174 | 0.5306 | 0.6390 | 0 | 3 | 139 | 6 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8237 | 0.7011 | 0.6010 | 0.7197 | 0 |
    0 | 3 | 177 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.8237 | 0.7011 | 0.6010 | 0.7197 | 0 | 0 | 3 | 177
    |'
- en: '| Lines, _, _ | 0.8205 | 0.6947 | 0.5900 | 0.6963 | 0 | 0 | 10 | 0 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.8205 | 0.6947 | 0.5900 | 0.6963 | 0 | 0 | 10 | 0 |'
- en: '| JSON, _, _ | 0.8212 | 0.6857 | 0.5649 | 0.6875 | 0 | 0 | 0 | 0 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8212 | 0.6857 | 0.5649 | 0.6875 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.7619 | 0.6536 | 0.4804 | 0.6709 | 1 | 2 | 0 | 584 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7619 | 0.6536 | 0.4804 | 0.6709 | 1 | 2 | 0 | 584 |'
- en: '| _, _, OU | 0.8179 | 0.6774 | 0.5034 | 0.6277 | 0 | 5 | 64 | 29 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.8179 | 0.6774 | 0.5034 | 0.6277 | 0 | 5 | 64 | 29 |'
- en: '| Reasoning | No-CoT | 0.8212 | 0.6857 | 0.5649 | 0.6875 | 0 | 0 | 0 | 0 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.8212 | 0.6857 | 0.5649 | 0.6875 | 0 | 0 | 0 | 0 |'
- en: '| CoT | 0.8026 | 0.6979 | 0.5519 | 0.7159 | 70 | 31 | 16 | 125 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.8026 | 0.6979 | 0.5519 | 0.7159 | 70 | 31 | 16 | 125 |'
- en: '| R-CoT | 0.8195 | 0.7034 | 0.5368 | 0.6454 | 46 | 14 | 24 | 666 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8195 | 0.7034 | 0.5368 | 0.6454 | 46 | 14 | 24 | 666 |'
- en: 'Table 4: MASA evaluations of each SDE option for model c-llama2-base. Definition
    of "_" see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 每种SDE选项的MASA评估结果，模型为c-llama2-base。 "_"的定义见表 [3](#A1.T3 "表 3 ‣ A.4 每种SDE选项的详细评估
    ‣ 附录 A 附录 ‣ 样本设计工程: 关于LLM的良好下游微调样本的实证研究")。'
- en: '| model: intern-chat | Weighted Kappa $\kappa$ | # Wrong format (7969 test
    samples in total) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 模型: intern-chat | 加权Kappa $\kappa$ | # 错误格式（总共7969个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.7774 | 0.6278 | 0.3947 | 0.6707 | 0 | 0 | 0
    | 11 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.7774 | 0.6278 | 0.3947 | 0.6707 | 0 | 0 | 0 | 11 |'
- en: '| Inst-first, _ | 0.8035 | 0.6609 | 0.3949 | 0.7090 | 4 | 2 | 13 | 304 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8035 | 0.6609 | 0.3949 | 0.7090 | 4 | 2 | 13 | 304 |'
- en: '| T2L | 0.7862 | 0.5963 | \ | \ | 10 | 7 | \ | \ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| T2L | 0.7862 | 0.5963 | \ | \ | 10 | 7 | \ | \ |'
- en: '| _, MI | 0.7463 | 0.5178 | 0.3153 | 0.5363 | 0 | 0 | 0 | 395 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7463 | 0.5178 | 0.3153 | 0.5363 | 0 | 0 | 0 | 395 |'
- en: '| Output | Natural, TxtLabel, PU | 0.7774 | 0.6278 | 0.3947 | 0.6707 | 0 |
    0 | 0 | 11 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.7774 | 0.6278 | 0.3947 | 0.6707 | 0 | 0 | 0 | 11
    |'
- en: '| Lines, _, _ | 0.7827 | 0.6261 | 0.4032 | 0.6799 | 0 | 1 | 1 | 1 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.7827 | 0.6261 | 0.4032 | 0.6799 | 0 | 1 | 1 | 1 |'
- en: '| JSON, _, _ | 0.7713 | 0.5966 | 0.3965 | 0.6129 | 0 | 0 | 0 | 2 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.7713 | 0.5966 | 0.3965 | 0.6129 | 0 | 0 | 0 | 2 |'
- en: '| _, NumLabel, _ | 0.7765 | 0.6261 | 0.4165 | 0.6926 | 0 | 0 | 3 | 23 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7765 | 0.6261 | 0.4165 | 0.6926 | 0 | 0 | 3 | 23 |'
- en: '| _, _, OU | 0.7520 | 0.4888 | 0.4029 | 0.6221 | 0 | 1 | 16 | 7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7520 | 0.4888 | 0.4029 | 0.6221 | 0 | 1 | 16 | 7 |'
- en: '| Reasoning | No-CoT | 0.7713 | 0.5966 | 0.3965 | 0.6129 | 0 | 0 | 0 | 2 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.7713 | 0.5966 | 0.3965 | 0.6129 | 0 | 0 | 0 | 2 |'
- en: '| CoT | 0.7666 | 0.6401 | 0.4843 | 0.6797 | 43 | 19 | 30 | 121 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7666 | 0.6401 | 0.4843 | 0.6797 | 43 | 19 | 30 | 121 |'
- en: '| R-CoT | 0.7764 | 0.6124 | 0.3892 | 0.6648 | 44 | 23 | 23 | 72 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.7764 | 0.6124 | 0.3892 | 0.6648 | 44 | 23 | 23 | 72 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 训练规模=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 | D2→D1
    |'
- en: '| Input | Inst-last, No-MI | 0.8049 | 0.6793 | 0.4330 | 0.6982 | 0 | 0 | 0
    | 0 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.8049 | 0.6793 | 0.4330 | 0.6982 | 0 | 0 | 0 | 0 |'
- en: '| Inst-first, _ | 0.8173 | 0.7125 | 0.4640 | 0.7343 | 0 | 1 | 6 | 259 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8173 | 0.7125 | 0.4640 | 0.7343 | 0 | 1 | 6 | 259 |'
- en: '| No-inst, _ | 0.8139 | 0.6811 | \ | \ | 8 | 5 | \ | \ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.8139 | 0.6811 | \ | \ | 8 | 5 | \ | \ |'
- en: '| _, MI | 0.7819 | 0.6256 | 0.3332 | 0.6520 | 1 | 0 | 8 | 29 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7819 | 0.6256 | 0.3332 | 0.6520 | 1 | 0 | 8 | 29 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8049 | 0.6793 | 0.4330 | 0.6982 | 0 |
    0 | 0 | 0 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.8049 | 0.6793 | 0.4330 | 0.6982 | 0 | 0 | 0 | 0
    |'
- en: '| Lines, _, _ | 0.8060 | 0.6797 | 0.4498 | 0.7038 | 0 | 1 | 0 | 1 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.8060 | 0.6797 | 0.4498 | 0.7038 | 0 | 1 | 0 | 1 |'
- en: '| JSON, _, _ | 0.8021 | 0.6649 | 0.4661 | 0.6647 | 0 | 0 | 0 | 0 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8021 | 0.6649 | 0.4661 | 0.6647 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.8081 | 0.6764 | 0.4393 | 0.7286 | 0 | 0 | 3 | 3 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.8081 | 0.6764 | 0.4393 | 0.7286 | 0 | 0 | 3 | 3 |'
- en: '| _, _, OU | 0.8008 | 0.6369 | 0.4374 | 0.6694 | 0 | 0 | 33 | 1 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.8008 | 0.6369 | 0.4374 | 0.6694 | 0 | 0 | 33 | 1 |'
- en: '| Reasoning | No-CoT | 0.8021 | 0.6649 | 0.4661 | 0.6647 | 0 | 0 | 0 | 0 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.8021 | 0.6649 | 0.4661 | 0.6647 | 0 | 0 | 0 | 0 |'
- en: '| CoT | 0.7981 | 0.6966 | 0.5190 | 0.7098 | 36 | 7 | 10 | 132 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7981 | 0.6966 | 0.5190 | 0.7098 | 36 | 7 | 10 | 132 |'
- en: '| R-CoT | 0.8043 | 0.6709 | 0.3994 | 0.7195 | 50 | 4 | 19 | 42 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8043 | 0.6709 | 0.3994 | 0.7195 | 50 | 4 | 19 | 42 |'
- en: 'Table 5: MASA evaluations of each SDE option for model intern-chat. Definition
    of "_" see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 每个SDE选项在模型intern-chat中的MASA评估。有关“_”的定义，请参见表[3](#A1.T3 "表 3 ‣ A.4 每个SDE选项的详细评估
    ‣ 附录 A 附录 ‣ 样本设计工程：LLMs下游微调样本质量的实证研究")。'
- en: '| model: intern-base | Weighted Kappa $\kappa$ | # Wrong format (7969 test
    samples in total) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 模型: intern-base | 加权Kappa $\kappa$ | 错误格式（总共7969个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 训练规模=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 | D2→D1
    |'
- en: '| Input | Inst-last, No-MI | 0.7849 | 0.6465 | 0.4898 | 0.6129 | 0 | 1 | 1
    | 0 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.7849 | 0.6465 | 0.4898 | 0.6129 | 0 | 1 | 1 | 0 |'
- en: '| Inst-first, _ | 0.7955 | 0.6472 | 0.4947 | 0.7006 | 3 | 8 | 18 | 221 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.7955 | 0.6472 | 0.4947 | 0.7006 | 3 | 8 | 18 | 221 |'
- en: '| No-inst, _ | 0.7936 | 0.6119 | \ | \ | 11 | 6 | \ | \ |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.7936 | 0.6119 | \ | \ | 11 | 6 | \ | \ |'
- en: '| _, MI | 0.7562 | 0.5029 | 0.3305 | 0.4672 | 0 | 1 | 232 | 447 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7562 | 0.5029 | 0.3305 | 0.4672 | 0 | 1 | 232 | 447 |'
- en: '| Output | Natural, TxtLabel, PU | 0.7849 | 0.6465 | 0.4898 | 0.6129 | 0 |
    1 | 1 | 0 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.7849 | 0.6465 | 0.4898 | 0.6129 | 0 | 1 | 1 | 0
    |'
- en: '| Lines, _, _ | 0.7873 | 0.6455 | 0.4939 | 0.6365 | 0 | 2 | 4 | 0 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.7873 | 0.6455 | 0.4939 | 0.6365 | 0 | 2 | 4 | 0 |'
- en: '| JSON, _, _ | 0.7859 | 0.6250 | 0.4727 | 0.6127 | 0 | 0 | 3 | 82 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.7859 | 0.6250 | 0.4727 | 0.6127 | 0 | 0 | 3 | 82 |'
- en: '| _, NumLabel, _ | 0.7605 | 0.6003 | 0.3861 | 0.6412 | 14 | 3 | 10 | 102 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7605 | 0.6003 | 0.3861 | 0.6412 | 14 | 3 | 10 | 102 |'
- en: '| _, _, OU | 0.7275 | 0.5185 | 0.3943 | 0.4935 | 0 | 4 | 48 | 6 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7275 | 0.5185 | 0.3943 | 0.4935 | 0 | 4 | 48 | 6 |'
- en: '| Reasoning | No-CoT | 0.7859 | 0.6250 | 0.4727 | 0.6127 | 0 | 0 | 3 | 82 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.7859 | 0.6250 | 0.4727 | 0.6127 | 0 | 0 | 3 | 82 |'
- en: '| CoT | 0.7621 | 0.6489 | 0.4581 | 0.6388 | 77 | 12 | 2347 | 50 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7621 | 0.6489 | 0.4581 | 0.6388 | 77 | 12 | 2347 | 50 |'
- en: '| R-CoT | 0.7734 | 0.6342 | 0.3752 | 0.6816 | 141 | 49 | 1496 | 206 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.7734 | 0.6342 | 0.3752 | 0.6816 | 141 | 49 | 1496 | 206 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 训练规模=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 | D2→D1
    |'
- en: '| Input | Inst-last, No-MI | 0.8112 | 0.6874 | 0.5216 | 0.7065 | 1 | 0 | 0
    | 0 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.8112 | 0.6874 | 0.5216 | 0.7065 | 1 | 0 | 0 | 0 |'
- en: '| Inst-first, _ | 0.8167 | 0.6965 | 0.5195 | 0.7544 | 0 | 0 | 5 | 46 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8167 | 0.6965 | 0.5195 | 0.7544 | 0 | 0 | 5 | 46 |'
- en: '| No-inst, _ | 0.8191 | 0.6963 | \ | \ | 5 | 8 | \ | \ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.8191 | 0.6963 | \ | \ | 5 | 8 | \ | \ |'
- en: '| _, MI | 0.7937 | 0.6238 | 0.2780 | 0.6492 | 0 | 2 | 383 | 45 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7937 | 0.6238 | 0.2780 | 0.6492 | 0 | 2 | 383 | 45 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8112 | 0.6874 | 0.5216 | 0.7065 | 1 |
    0 | 0 | 0 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.8112 | 0.6874 | 0.5216 | 0.7065 | 1 | 0 | 0 | 0
    |'
- en: '| Lines, _, _ | 0.8113 | 0.6919 | 0.5060 | 0.7126 | 0 | 0 | 3 | 0 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.8113 | 0.6919 | 0.5060 | 0.7126 | 0 | 0 | 3 | 0 |'
- en: '| JSON, _, _ | 0.8076 | 0.6781 | 0.5195 | 0.6817 | 0 | 0 | 3 | 1 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8076 | 0.6781 | 0.5195 | 0.6817 | 0 | 0 | 3 | 1 |'
- en: '| _, NumLabel, _ | 0.8084 | 0.6776 | 0.4426 | 0.7139 | 3 | 1 | 31 | 20 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.8084 | 0.6776 | 0.4426 | 0.7139 | 3 | 1 | 31 | 20 |'
- en: '| _, _, OU | 0.8006 | 0.6330 | 0.4587 | 0.6098 | 0 | 1 | 30 | 3 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.8006 | 0.6330 | 0.4587 | 0.6098 | 0 | 1 | 30 | 3 |'
- en: '| Reasoning | No-CoT | 0.8076 | 0.6781 | 0.5195 | 0.6817 | 0 | 0 | 3 | 1 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.8076 | 0.6781 | 0.5195 | 0.6817 | 0 | 0 | 3 | 1 |'
- en: '| CoT | 0.7956 | 0.6874 | 0.5196 | 0.6903 | 34 | 12 | 405 | 56 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7956 | 0.6874 | 0.5196 | 0.6903 | 34 | 12 | 405 | 56 |'
- en: '| R-CoT | 0.8069 | 0.6725 | 0.4890 | 0.7185 | 46 | 11 | 220 | 125 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8069 | 0.6725 | 0.4890 | 0.7185 | 46 | 11 | 220 | 125 |'
- en: 'Table 6: MASA evaluations of each SDE option for model intern-base. Definition
    of "_" see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '表6: 每种SDE选项对模型内部基准的MASA评估。定义“_”见表[3](#A1.T3 "表 3 ‣ A.4 每种SDE选项的详细评估 ‣ 附录A ‣
    样本设计工程：LLMs的有效下游微调样本的实证研究")。'
- en: '| model: bc2-chat | Weighted Kappa $\kappa$ | # Wrong format (7969 test samples
    in total) |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 模型: bc2-chat | 加权Kappa $\kappa$ | # 错误格式（总共7969个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 训练集大小=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 | D2→D1
    |'
- en: '| Input | Inst-last, No-MI | 0.7904 | 0.6544 | 0.4067 | 0.6170 | 8 | 0 | 21
    | 10 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.7904 | 0.6544 | 0.4067 | 0.6170 | 8 | 0 | 21 | 10 |'
- en: '| Inst-first, _ | 0.7958 | 0.6660 | 0.3858 | 0.6739 | 19 | 36 | 12 | 385 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.7958 | 0.6660 | 0.3858 | 0.6739 | 19 | 36 | 12 | 385 |'
- en: '| No-inst, _ | 0.7176 | 0.4776 | \ | \ | 23 | 13 | \ | \ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.7176 | 0.4776 | \ | \ | 23 | 13 | \ | \ |'
- en: '| _, MI | 0.7645 | 0.5636 | 0.3713 | 0.5490 | 0 | 0 | 5 | 16 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7645 | 0.5636 | 0.3713 | 0.5490 | 0 | 0 | 5 | 16 |'
- en: '| Output | Natural, TxtLabel, PU | 0.7904 | 0.6544 | 0.4067 | 0.6170 | 8 |
    0 | 21 | 10 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.7904 | 0.6544 | 0.4067 | 0.6170 | 8 | 0 | 21 |
    10 |'
- en: '| Lines, _, _ | 0.7869 | 0.6653 | 0.4091 | 0.6344 | 0 | 0 | 9 | 1 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.7869 | 0.6653 | 0.4091 | 0.6344 | 0 | 0 | 9 | 1 |'
- en: '| JSON, _, _ | 0.7927 | 0.6489 | 0.4714 | 0.6196 | 0 | 0 | 1 | 0 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.7927 | 0.6489 | 0.4714 | 0.6196 | 0 | 0 | 1 | 0 |'
- en: '| _, NumLabel, _ | 0.7839 | 0.6401 | 0.3671 | 0.6506 | 5 | 4 | 12 | 17 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7839 | 0.6401 | 0.3671 | 0.6506 | 5 | 4 | 12 | 17 |'
- en: '| _, _, OU | 0.7016 | 0.5670 | 0.3599 | 0.3285 | 2 | 81 | 50 | 19 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7016 | 0.5670 | 0.3599 | 0.3285 | 2 | 81 | 50 | 19 |'
- en: '| Reasoning | No-CoT | 0.7927 | 0.6489 | 0.4714 | 0.6196 | 0 | 0 | 1 | 0 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.7927 | 0.6489 | 0.4714 | 0.6196 | 0 | 0 | 1 | 0 |'
- en: '| CoT | 0.7722 | 0.6400 | 0.5006 | 0.6776 | 3641 | 757 | 739 | 3323 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7722 | 0.6400 | 0.5006 | 0.6776 | 3641 | 757 | 739 | 3323 |'
- en: '| R-CoT | 0.7922 | 0.6535 | 0.4534 | 0.6579 | 107 | 126 | 280 | 563 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.7922 | 0.6535 | 0.4534 | 0.6579 | 107 | 126 | 280 | 563 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 训练集大小=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 | D2→D1
    |'
- en: '| Input | Inst-last, No-MI | 0.8113 | 0.7060 | 0.4709 | 0.6365 | 0 | 4 | 13
    | 18 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 实例最后, 无MI | 0.8113 | 0.7060 | 0.4709 | 0.6365 | 0 | 4 | 13 | 18 |'
- en: '| Inst-first, _ | 0.8142 | 0.7095 | 0.4733 | 0.6787 | 31 | 12 | 21 | 136 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 实例优先, _ | 0.8142 | 0.7095 | 0.4733 | 0.6787 | 31 | 12 | 21 | 136 |'
- en: '| No-inst, _ | 0.7466 | 0.6172 | \ | \ | 6 | 6 | \ | \ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 无实例, _ | 0.7466 | 0.6172 | \ | \ | 6 | 6 | \ | \ |'
- en: '| _, MI | 0.7935 | 0.6514 | 0.3951 | 0.5885 | 0 | 0 | 7 | 3 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7935 | 0.6514 | 0.3951 | 0.5885 | 0 | 0 | 7 | 3 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8113 | 0.7060 | 0.4709 | 0.6365 | 0 |
    4 | 13 | 18 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 0.8113 | 0.7060 | 0.4709 | 0.6365 | 0 | 4 | 13 |
    18 |'
- en: '| Lines, _, _ | 0.8103 | 0.7057 | 0.4691 | 0.6387 | 0 | 0 | 3 | 0 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 行, _, _ | 0.8103 | 0.7057 | 0.4691 | 0.6387 | 0 | 0 | 3 | 0 |'
- en: '| JSON, _, _ | 0.8118 | 0.7064 | 0.5237 | 0.6323 | 0 | 0 | 1 | 0 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8118 | 0.7064 | 0.5237 | 0.6323 | 0 | 0 | 1 | 0 |'
- en: '| _, NumLabel, _ | 0.8121 | 0.6962 | 0.4042 | 0.6697 | 10 | 17 | 4 | 15 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.8121 | 0.6962 | 0.4042 | 0.6697 | 10 | 17 | 4 | 15 |'
- en: '| _, _, OU | 0.8061 | 0.6467 | 0.4843 | 0.5155 | 1 | 25 | 44 | 4 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.8061 | 0.6467 | 0.4843 | 0.5155 | 1 | 25 | 44 | 4 |'
- en: '| Reasoning | No-CoT | 0.8118 | 0.7064 | 0.5237 | 0.6323 | 0 | 0 | 1 | 0 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 无CoT | 0.8118 | 0.7064 | 0.5237 | 0.6323 | 0 | 0 | 1 | 0 |'
- en: '| CoT | 0.7995 | 0.7026 | 0.4992 | 0.6975 | 2273 | 193 | 560 | 2043 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7995 | 0.7026 | 0.4992 | 0.6975 | 2273 | 193 | 560 | 2043 |'
- en: '| R-CoT | 0.8087 | 0.6961 | 0.5022 | 0.6772 | 57 | 48 | 85 | 167 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8087 | 0.6961 | 0.5022 | 0.6772 | 57 | 48 | 85 | 167 |'
- en: 'Table 7: MASA evaluations of each SDE option for model bc2-chat. Definition
    of "_" see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 对模型 bc2-chat 的每个 SDE 选项的 MASA 评估。关于“_”的定义请见表 [3](#A1.T3 "表 3 ‣ A.4 每个
    SDE 选项的详细评估 ‣ 附录 A ‣ 样本设计工程：什么样的下游微调样本对 LLM 有效的实证研究")。'
- en: '| model: bc2-base | Weighted Kappa $\kappa$ | # Wrong format (7969 test samples
    in total) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| model: bc2-base | 加权 Kappa $\kappa$ | # 错误格式（总共 7969 个测试样本） |'
- en: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| train_size=500 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.8017 | 0.6412 | 0.4441 | 0.6146 | 0 | 0 | 75
    | 0 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | 0.8017 | 0.6412 | 0.4441 | 0.6146 | 0 | 0 | 75 |
    0 |'
- en: '| Inst-first, _ | 0.8016 | 0.6649 | 0.4488 | 0.6657 | 0 | 6 | 27 | 4 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | 0.8016 | 0.6649 | 0.4488 | 0.6657 | 0 | 6 | 27 | 4 |'
- en: '| No-inst, _ | 0.7533 | 0.6020 | \ | \ | 2 | 3 | \ | \ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| No-inst, _ | 0.7533 | 0.6020 | \ | \ | 2 | 3 | \ | \ |'
- en: '| _, MI | 0.7660 | 0.4999 | 0.3220 | 0.1978 | 0 | 0 | 1 | 164 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.7660 | 0.4999 | 0.3220 | 0.1978 | 0 | 0 | 1 | 164 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8017 | 0.6412 | 0.4441 | 0.6146 | 0 |
    0 | 75 | 0 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | Natural, TxtLabel, PU | 0.8017 | 0.6412 | 0.4441 | 0.6146 | 0 | 0 |
    75 | 0 |'
- en: '| Lines, _, _ | 0.7996 | 0.6317 | 0.4583 | 0.6191 | 0 | 0 | 2 | 0 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Lines, _, _ | 0.7996 | 0.6317 | 0.4583 | 0.6191 | 0 | 0 | 2 | 0 |'
- en: '| JSON, _, _ | 0.8008 | 0.6476 | 0.4316 | 0.6104 | 0 | 0 | 0 | 0 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8008 | 0.6476 | 0.4316 | 0.6104 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.7969 | 0.5794 | 0.4312 | 0.5206 | 7 | 45 | 469 | 47 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.7969 | 0.5794 | 0.4312 | 0.5206 | 7 | 45 | 469 | 47 |'
- en: '| _, _, OU | 0.7595 | 0.5202 | 0.4240 | 0.4944 | 0 | 0 | 116 | 2 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7595 | 0.5202 | 0.4240 | 0.4944 | 0 | 0 | 116 | 2 |'
- en: '| Reasoning | No-CoT | 0.7595 | 0.5202 | 0.4240 | 0.4944 | 0 | 0 | 116 | 2
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 0.7595 | 0.5202 | 0.4240 | 0.4944 | 0 | 0 | 116 | 2 |'
- en: '| CoT | 0.7865 | 0.6814 | 0.3854 | 0.6745 | 63 | 17 | 43 | 483 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.7865 | 0.6814 | 0.3854 | 0.6745 | 63 | 17 | 43 | 483 |'
- en: '| R-CoT | 0.7980 | 0.6548 | 0.4240 | 0.6349 | 32 | 44 | 39 | 32 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.7980 | 0.6548 | 0.4240 | 0.6349 | 32 | 44 | 39 | 32 |'
- en: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| train_size=1000 | D1→D1 | D2→D2 | D1→D2 | D2→D1 | D1→D1 | D2→D2 | D1→D2 |
    D2→D1 |'
- en: '| Input | Inst-last, No-MI | 0.8143 | 0.6981 | 0.4747 | 0.6767 | 0 | 0 | 26
    | 4 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | 0.8143 | 0.6981 | 0.4747 | 0.6767 | 0 | 0 | 26 |
    4 |'
- en: '| Inst-first, _ | 0.8155 | 0.7157 | 0.5061 | 0.6974 | 0 | 3 | 26 | 4 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | 0.8155 | 0.7157 | 0.5061 | 0.6974 | 0 | 3 | 26 | 4 |'
- en: '| No-inst, _ | 0.7543 | 0.6391 | \ | \ | 0 | 3 | \ | \ |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| No-inst, _ | 0.7543 | 0.6391 | \ | \ | 0 | 3 | \ | \ |'
- en: '| _, MI | 0.8010 | 0.6489 | 0.4164 | 0.5250 | 0 | 0 | 1 | 431 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| _, MI | 0.8010 | 0.6489 | 0.4164 | 0.5250 | 0 | 0 | 1 | 431 |'
- en: '| Output | Natural, TxtLabel, PU | 0.8143 | 0.6981 | 0.4747 | 0.6767 | 0 |
    0 | 26 | 4 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | Natural, TxtLabel, PU | 0.8143 | 0.6981 | 0.4747 | 0.6767 | 0 | 0 |
    26 | 4 |'
- en: '| Lines, _, _ | 0.8103 | 0.7003 | 0.4732 | 0.6713 | 0 | 0 | 6 | 1 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Lines, _, _ | 0.8103 | 0.7003 | 0.4732 | 0.6713 | 0 | 0 | 6 | 1 |'
- en: '| JSON, _, _ | 0.8120 | 0.7039 | 0.4785 | 0.6819 | 0 | 0 | 0 | 0 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.8120 | 0.7039 | 0.4785 | 0.6819 | 0 | 0 | 0 | 0 |'
- en: '| _, NumLabel, _ | 0.8119 | 0.6812 | 0.4575 | 0.6467 | 1 | 5 | 292 | 8 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.8119 | 0.6812 | 0.4575 | 0.6467 | 1 | 5 | 292 | 8 |'
- en: '| _, _, OU | 0.7894 | 0.6484 | 0.4031 | 0.6235 | 0 | 1 | 31 | 0 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.7894 | 0.6484 | 0.4031 | 0.6235 | 0 | 1 | 31 | 0 |'
- en: '| Reasoning | No-CoT | 0.7894 | 0.6484 | 0.4031 | 0.6235 | 0 | 1 | 31 | 0 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 0.7894 | 0.6484 | 0.4031 | 0.6235 | 0 | 1 | 31 | 0 |'
- en: '| CoT | 0.8045 | 0.7063 | 0.5319 | 0.6965 | 21 | 12 | 25 | 494 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.8045 | 0.7063 | 0.5319 | 0.6965 | 21 | 12 | 25 | 494 |'
- en: '| R-CoT | 0.8160 | 0.7021 | 0.4604 | 0.6949 | 15 | 14 | 24 | 115 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.8160 | 0.7021 | 0.4604 | 0.6949 | 15 | 14 | 24 | 115 |'
- en: 'Table 8: MASA evaluations of each SDE option for model bc2-base. Definition
    of "_" see Table [3](#A1.T3 "Table 3 ‣ A.4 Detailed Evaluations of Each SDE Option
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs").'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 对模型 bc2-base 的每个 SDE 选项的 MASA 评估。关于“_”的定义请见表 [3](#A1.T3 "表 3 ‣ A.4 每个
    SDE 选项的详细评估 ‣ 附录 A ‣ 样本设计工程：什么样的下游微调样本对 LLM 有效的实证研究")。'
- en: A.5 Detailed Results on GENIA, MAVEN and Review11
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 GENIA、MAVEN 和 Review11 的详细结果
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.5 Detailed Results on GENIA, MAVEN and Review11
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs") shows the comparison of different
    sample design strategies on three downstream tasks—GENIA (Nested NER), MAVEN (Event
    Detection), and Review11 (MASA). Hard and soft-matching F1 scores are reported
    for GENIA and MAVEN, while kappa $\kappa$ and accuracy are reported for Review11.
    From the results, we can see that ES-SDE maintains its advantage over other methods,
    across different tasks and training sizes.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [9](#A1.T9 "表 9 ‣ A.5 GENIA、MAVEN 和 Review11 的详细结果 ‣ 附录 A 附录 ‣ 示例设计工程：对 LLMs
    的优质下游微调样本的实证研究") 展示了不同样本设计策略在三个下游任务——GENIA（嵌套 NER）、MAVEN（事件检测）和 Review11（MASA）上的比较。对
    GENIA 和 MAVEN 报告了硬匹配和软匹配的 F1 分数，对 Review11 报告了 kappa $\kappa$ 和准确率。结果显示，ES-SDE
    在不同任务和训练规模下保持了相对于其他方法的优势。
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.5 Detailed Results on GENIA, MAVEN and Review11
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs") illustrates the performances
    of different sample design strategies on three downstream tasks across different
    instruction variations.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [10](#A1.T10 "表 10 ‣ A.5 GENIA、MAVEN 和 Review11 的详细结果 ‣ 附录 A 附录 ‣ 示例设计工程：对
    LLMs 的优质下游微调样本的实证研究") 展示了不同样本设计策略在不同指令变体下对三个下游任务的表现。
- en: '|  |  | GENIA (Nested-NER) | MAVEN (ED) | Review11 (MASA) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GENIA（嵌套 NER） | MAVEN（事件检测） | Review11（MASA） |'
- en: '| training size | Strategies | F1-hard | F1-soft | F1-hard | F1-soft | $\kappa$
    | Acc |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 训练规模 | 策略 | F1-hard | F1-soft | F1-hard | F1-soft | $\kappa$ | 准确率 |'
- en: '| $500$ | heuristic | 0.51232 | 0.57465 | 0.5197 | 0.5356 | 0.588 | 0.7586
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| $500$ | 启发式 | 0.51232 | 0.57465 | 0.5197 | 0.5356 | 0.588 | 0.7586 |'
- en: '| EW-SDE | 0.48328 | 0.54318 | 0.4922 | 0.5364 | 0.7235 | 0.8327 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.48328 | 0.54318 | 0.4922 | 0.5364 | 0.7235 | 0.8327 |'
- en: '| ES-SDE | 0.54068 | 0.61412 | 0.5846 | 0.6331 | 0.7691 | 0.8626 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.54068 | 0.61412 | 0.5846 | 0.6331 | 0.7691 | 0.8626 |'
- en: '| $1,000$ | heuristic | 0.56537 | 0.62275 | 0.6237 | 0.6354 | 0.7058 | 0.8262
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| $1,000$ | 启发式 | 0.56537 | 0.62275 | 0.6237 | 0.6354 | 0.7058 | 0.8262 |'
- en: '| EW-SDE | 0.48785 | 0.55166 | 0.6109 | 0.6275 | 0.7565 | 0.8502 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.48785 | 0.55166 | 0.6109 | 0.6275 | 0.7565 | 0.8502 |'
- en: '| ES-SDE | 0.61593 | 0.68951 | 0.6432 | 0.6726 | 0.7892 | 0.8716 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.61593 | 0.68951 | 0.6432 | 0.6726 | 0.7892 | 0.8716 |'
- en: '| $2,000$ | heuristic | 0.64759 | 0.69905 | 0.6722 | 0.6813 | 0.7479 | 0.8483
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| $2,000$ | 启发式 | 0.64759 | 0.69905 | 0.6722 | 0.6813 | 0.7479 | 0.8483 |'
- en: '| EW-SDE | 0.54351 | 0.6025 | 0.6966 | 0.7106 | 0.7805 | 0.8649 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.54351 | 0.6025 | 0.6966 | 0.7106 | 0.7805 | 0.8649 |'
- en: '| ES-SDE | 0.68069 | 0.7393 | 0.7033 | 0.7172 | 0.8023 | 0.8785 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.68069 | 0.7393 | 0.7033 | 0.7172 | 0.8023 | 0.8785 |'
- en: '| $4,000$ | heuristic | 0.68726 | 0.73825 | 0.7118 | 0.7176 | 0.7751 | 0.8644
    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| $4,000$ | 启发式 | 0.68726 | 0.73825 | 0.7118 | 0.7176 | 0.7751 | 0.8644 |'
- en: '| EW-SDE | 0.71109 | 0.77093 | 0.7265 | 0.7338 | 0.7917 | 0.8715 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.71109 | 0.77093 | 0.7265 | 0.7338 | 0.7917 | 0.8715 |'
- en: '| ES-SDE | 0.72726 | 0.78487 | 0.7295 | 0.7466 | 0.805 | 0.8814 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.72726 | 0.78487 | 0.7295 | 0.7466 | 0.805 | 0.8814 |'
- en: 'Table 9: Comparison of different sample design strategies on three downstream
    tasks. ES-SDE maintains its advantage over other methods, across different tasks
    and training sizes.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同样本设计策略在三个下游任务上的比较。ES-SDE 在不同任务和训练规模下保持了相对于其他方法的优势。
- en: '|  |  | GENIA (Nested-NER) | MAVEN (ED) | Review11 (MASA) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GENIA（嵌套 NER） | MAVEN（事件检测） | Review11（MASA） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Instruction Variation | Strategies | F1-hard | F1-soft | F1-hard | F1-soft
    | $\kappa$ | Acc |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 指令变体 | 策略 | F1-hard | F1-soft | F1-hard | F1-soft | $\kappa$ | 准确率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| inst-1 | heuristic | 0.5123 | 0.5747 | 0.5197 | 0.5356 | 0.588 | 0.7586 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| inst-1 | 启发式 | 0.5123 | 0.5747 | 0.5197 | 0.5356 | 0.588 | 0.7586 |'
- en: '| EW-SDE | 0.4833 | 0.5432 | 0.4922 | 0.5364 | 0.7235 | 0.8327 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.4833 | 0.5432 | 0.4922 | 0.5364 | 0.7235 | 0.8327 |'
- en: '| ES-SDE | 0.5407 | 0.6141 | 0.5846 | 0.6331 | 0.7691 | 0.8626 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.5407 | 0.6141 | 0.5846 | 0.6331 | 0.7691 | 0.8626 |'
- en: '| inst-2 | heuristic | 0.49813 | 0.56095 | 0.5134 | 0.5334 | 0.6009 | 0.7685
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| inst-2 | 启发式 | 0.49813 | 0.56095 | 0.5134 | 0.5334 | 0.6009 | 0.7685 |'
- en: '| EW-SDE | 0.48593 | 0.54999 | 0.4956 | 0.5339 | 0.7208 | 0.8344 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.48593 | 0.54999 | 0.4956 | 0.5339 | 0.7208 | 0.8344 |'
- en: '| ES-SDE | 0.53479 | 0.60767 | 0.5636 | 0.6167 | 0.7659 | 0.8615 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.53479 | 0.60767 | 0.5636 | 0.6167 | 0.7659 | 0.8615 |'
- en: '| inst-3 | heuristic | 0.48733 | 0.55491 | 0.4940 | 0.5060 | 0.5793 | 0.7533
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| inst-3 | 启发式 | 0.48733 | 0.55491 | 0.4940 | 0.5060 | 0.5793 | 0.7533 |'
- en: '| EW-SDE | 0.47638 | 0.53685 | 0.4925 | 0.5399 | 0.721 | 0.8365 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| EW-SDE | 0.47638 | 0.53685 | 0.4925 | 0.5399 | 0.721 | 0.8365 |'
- en: '| ES-SDE | 0.53525 | 0.60902 | 0.5530 | 0.6087 | 0.7624 | 0.8601 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| ES-SDE | 0.53525 | 0.60902 | 0.5530 | 0.6087 | 0.7624 | 0.8601 |'
- en: 'Table 10: Performances of different sample design strategies on three downstream
    tasks across different instruction variations.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：不同样本设计策略在不同指令变体下的三个下游任务的表现。
- en: A.6 Additional Analysis on Inst-last and Inst-first
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 对 Inst-last 和 Inst-first 的额外分析
- en: 'The experimental results showing that Inst-first consistently outperforms Inst-last
    across various tasks and models are thought-provoking, leading us to conduct a
    more in-depth analysis. We extract the attention weights related to some task-related
    fields in the instruction, and sum up these task-related attention weights for
    each token. Figure [8](#A1.F8 "Figure 8 ‣ A.6 Additional Analysis on Inst-last
    and Inst-first ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs") shows the comparison
    of the attention weights for a certain customer review. As we can see, tokens
    that are closer to the instruction usually get higher task-related attention weights.
    Intuitively, when people write reviews, they generally present their core opinions
    at the beginning. This leads to the possibility that if the instructions are placed
    at the front, those core parts may receive greater task-related attention weights.
    This may partly explain why Inst-first usually leads to a higher sentiment analysis
    performance.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果显示 Inst-first 在各种任务和模型中始终优于 Inst-last，这一发现引人深思，使我们进行更深入的分析。我们提取了与指令中一些任务相关领域相关的注意力权重，并汇总了每个标记的这些任务相关注意力权重。图
    [8](#A1.F8 "图 8 ‣ A.6 对 Inst-last 和 Inst-first 的额外分析 ‣ 附录 A 附录 ‣ 样本设计工程：对 LLMs
    的良好下游微调样本的实证研究") 显示了某客户评论的注意力权重比较。正如我们所看到的，离指令较近的标记通常获得更高的任务相关注意力权重。直观地，当人们撰写评论时，他们通常会在开头呈现他们的核心观点。这导致了一个可能性，即如果指令放在前面，这些核心部分可能会获得更大的任务相关注意力权重。这可能部分解释了为什么
    Inst-first 通常导致更高的情感分析表现。
- en: '![Refer to caption](img/d2b6106d39a97a922fd38183d9da802d.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2b6106d39a97a922fd38183d9da802d.png)'
- en: 'Figure 8: Comparison of task-related attention scores using Inst-last and Inst-first.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 Inst-last 和 Inst-first 进行任务相关注意力分数的比较。
- en: '![Refer to caption](img/b4852be1e85384d6aa871b4667cdb817.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b4852be1e85384d6aa871b4667cdb817.png)'
- en: 'Figure 9: Examples of different sample designs on the MASA task.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：MASA 任务中不同样本设计的示例。
- en: A.7 Additional Analysis on OU and PU
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 对 OU 和 PU 的额外分析
- en: In previous experiments, we found that OU performs much worse than PU. This
    intriguing result motivates us to a further analysis. Specifically, we calculate
    and compare the kappa scores of OU and PU for each aspect, to analyze the relationship
    between label distributions and the effect of OU.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实验中，我们发现 OU 的表现远逊于 PU。这个引人注目的结果促使我们进行进一步的分析。具体来说，我们计算并比较了 OU 和 PU 每个方面的
    kappa 分数，以分析标签分布与 OU 效果之间的关系。
- en: 'From the result in Table [11](#A1.T11 "Table 11 ‣ A.7 Additional Analysis on
    OU and PU ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study
    of What Makes Good Downstream Fine-Tuning Samples for LLMs"), we can observe that
    when training the model with 500 samples, for aspects with a higher number of
    unmentioned, the OU method showed a significant gap compared to the PU format.
    When the training set increased to 1000 samples, this gap noticeably narrowed.
    This suggests that for the OU method, aspects with more unmentioned, implying
    less frequent occurrence in answers, are harder for the model to learn, so requiring
    more data. From another perspective, it also indicates that even if a certain
    aspect is not covered in the text, mentioning this aspect in the answers can enhance
    the model’s understanding of it.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 [11](#A1.T11 "表 11 ‣ A.7 对 OU 和 PU 的额外分析 ‣ 附录 A 附录 ‣ 样本设计工程：对 LLMs 的良好下游微调样本的实证研究")
    的结果中，我们可以观察到，当用 500 个样本训练模型时，对于未提及数量较多的方面，OU 方法与 PU 格式相比显示出显著差距。当训练集增加到 1000 个样本时，这个差距明显缩小。这表明，对于
    OU 方法而言，未提及较多的方面（即在回答中出现频率较低）对模型的学习更加困难，因此需要更多的数据。从另一个角度看，这也表明，即使某个方面在文本中没有覆盖，在回答中提及这一方面可以增强模型对其的理解。
- en: '| Aspect | Trainsize=500 | Trainsize=1000 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | Trainsize=500 | Trainsize=1000 |'
- en: '| (%)Num_ | $\Delta$ |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| (%)Num_ | $\Delta$ |'
- en: '|  |  | Unmen | Avg_Chat | Avg_Base | Unmen | Avg_Chat | Avg_Base |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 未提及 | 平均聊天 | 平均基础 | 未提及 | 平均聊天 | 平均基础 |'
- en: '| D1 | F | 1.00 | -.0004 | .0007 | 1.40 | -.0026 | -.0011 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| D1 | F | 1.00 | -.0004 | .0007 | 1.40 | -.0026 | -.0011 |'
- en: '|  | SA | 33.60 | -.0687 | -.0555 | 34.40 | -.0062 | -.0212 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | SA | 33.60 | -.0687 | -.0555 | 34.40 | -.0062 | -.0212 |'
- en: '|  | P | 38.00 | -.0469 | -.0495 | 37.90 | -.0068 | -.0255 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | P | 38.00 | -.0469 | -.0495 | 37.90 | -.0068 | -.0255 |'
- en: '|  | B | 65.40 | -.0410 | -.0291 | 65.70 | -.0117 | -.0079 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | B | 65.40 | -.0410 | -.0291 | 65.70 | -.0117 | -.0079 |'
- en: '|  | H | 78.00 | -.0920 | -.1367 | 76.40 | -.0033 | -.0207 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | H | 78.00 | -.0920 | -.1367 | 76.40 | -.0033 | -.0207 |'
- en: '|  | PC | 93.60 | -.2338 | -.2590 | 93.00 | -.0181 | -.0305 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | PC | 93.60 | -.2338 | -.2590 | 93.00 | -.0181 | -.0305 |'
- en: '| D2 | TC | 26.80 | -.0891 | -.1341 | 25.60 | -.0497 | -.0492 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| D2 | TC | 26.80 | -.0891 | -.1341 | 25.60 | -.0497 | -.0492 |'
- en: '|  | D | 41.60 | -.1106 | -.2475 | 39.10 | -.0280 | -.0500 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | D | 41.60 | -.1106 | -.2475 | 39.10 | -.0280 | -.0500 |'
- en: '|  | Q | 61.80 | -.0329 | -.0588 | 61.00 | -.0361 | -.0149 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | Q | 61.80 | -.0329 | -.0588 | 61.00 | -.0361 | -.0149 |'
- en: '|  | SS | 71.40 | -.2537 | -.2575 | 71.60 | -.0574 | -.0896 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  | SS | 71.40 | -.2537 | -.2575 | 71.60 | -.0574 | -.0896 |'
- en: '|  | N | 94.80 | -.3347 | -.3954 | 93.90 | -.0494 | -.1405 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | N | 94.80 | -.3347 | -.3954 | 93.90 | -.0494 | -.1405 |'
- en: 'Table 11: Number of ‘Unmentioned’ labels and average $\Delta$) for different
    aspects.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：不同方面的“未提及”标签数量和平均 $\Delta$。
- en: A.8 Can PE Guide SDE? Detailed Results
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.8 PE 是否可以指导 SDE？详细结果
- en: Evaluating the performances of sample designs involves fine-tuning models on
    downstream tasks, which can be time-consuming. Therefore, we also pondered whether
    it might be possible to design better samples without training models first. We
    tried to understand the inherent capabilities and potential of the model by experimenting
    with different prompt designs in both the zero-shot and in-context learning scenarios.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 评估样本设计的性能涉及对下游任务的模型微调，这可能是时间消耗大的。因此，我们还考虑是否可以在不首先训练模型的情况下设计更好的样本。我们尝试通过在零-shot
    和 in-context 学习场景中实验不同的提示设计来理解模型的内在能力和潜力。
- en: A.8.1 Zero-shot and In-context Learning Analysis
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.8.1 零-shot 和 In-context 学习分析
- en: Zero-shot and In-context learning ability can directly reveal LLMs’ familiarity
    with the given task. In the zero-shot approach, we use the input (which contains
    the instruction on output format) from each SDE option as the prompt for the original
    frozen LLMs prediction. For the ICL approach, we add two fixed examples from the
    training set before each test instance. Considering the inference time cost caused
    by the increase in sample length, we limit our prediction and analysis to 500
    samples. All other experimental setups remain aligned with those described in
    Experiments I.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 和 In-context 学习能力可以直接揭示 LLM 对给定任务的熟悉程度。在零-shot 方法中，我们使用每个 SDE 选项的输入（包含输出格式的指令）作为原始冻结
    LLM 预测的提示。对于 ICL 方法，我们在每个测试实例之前添加了两个来自训练集的固定示例。考虑到样本长度增加带来的推理时间成本，我们将预测和分析限制在
    500 个样本。所有其他实验设置与实验 I 中描述的保持一致。
- en: 'Zero-shot Study. All six 7B LLMs used in Section [4](#S4 "4 Experiments I:
    Evaluating The Impact of Each SDE Option ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs") exhibit poor
    zero-shot MASA ability, failing to follow the instructions to generate proper
    output in most cases, as shown in Table [13](#A1.T13 "Table 13 ‣ A.8.2 Perplexity
    Analysis ‣ A.8 Can PE Guide SDE? Detailed Results ‣ Appendix A Appendix ‣ Sample
    Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning
    Samples for LLMs"), making it hard to analysis its relationship with SDE results.
    Variations in format preferences across different models are observed, which we
    conjecture is strongly related to the datasets employed for instruction fine-tuning
    in each model. Some patterns are also contradictory between zero-shot and SDE.
    For example, the OU SDE option consistently harms DT performances, however, its
    prompts result in notably fewer format errors in zero-shot inference, for certain
    LLMs. Therefore, zero-shot performances can hardly tell good or bad SDE options.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 研究。第 [4](#S4 "4 实验 I：评估每个 SDE 选项的影响 ‣ 样本设计工程：对什么是有效下游微调样本的实证研究") 节中使用的所有六个
    7B LLM 展示了较差的零-shot MASA 能力，大多数情况下无法遵循指令生成适当的输出，如表 [13](#A1.T13 "表 13 ‣ A.8.2
    困惑度分析 ‣ A.8 PE 是否可以指导 SDE？详细结果 ‣ 附录 A 附录 ‣ 样本设计工程：对什么是有效下游微调样本的实证研究") 所示，这使得分析其与
    SDE 结果的关系变得困难。观察到不同模型之间的格式偏好差异，我们推测这与每个模型用于指令微调的数据集密切相关。一些模式在零-shot 和 SDE 之间也存在矛盾。例如，OU
    SDE 选项始终对 DT 性能产生负面影响，但其提示在零-shot 推理中对某些 LLM 产生了显著较少的格式错误。因此，零-shot 性能很难判断 SDE
    选项的优劣。
- en: 'In-context Learning Study. ICL can effectively improve LLMs’ instruction-following
    abilities resulting in far fewer formatting errors than zero-shot. Therefore we
    report the average sentiment analysis performances of each model on two domains
    in Table [14](#A1.T14 "Table 14 ‣ A.8.2 Perplexity Analysis ‣ A.8 Can PE Guide
    SDE? Detailed Results ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical
    Study of What Makes Good Downstream Fine-Tuning Samples for LLMs"). The results
    suggest that Inst-first and CoT enhance the performance of most models, which
    provides valuable insights for format selection during the fine-tuning process.
    For output designs, JSON and OU options outperform the other approaches for some
    models, differing from the SDE results.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文学习研究。ICL 可以有效提高 LLM 的指令跟随能力，从而比零-shot 产生更少的格式错误。因此，我们在表 [14](#A1.T14 "Table
    14 ‣ A.8.2 Perplexity Analysis ‣ A.8 Can PE Guide SDE? Detailed Results ‣ Appendix
    A Appendix ‣ Sample Design Engineering: An Empirical Study of What Makes Good
    Downstream Fine-Tuning Samples for LLMs") 中报告了每个模型在两个领域的平均情感分析表现。结果表明，Inst-first
    和 CoT 增强了大多数模型的性能，这为微调过程中的格式选择提供了宝贵的见解。在输出设计方面，对于某些模型，JSON 和 OU 选项表现优于其他方法，与 SDE
    结果有所不同。'
- en: A.8.2 Perplexity Analysis
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.8.2 困惑度分析
- en: Perplexity measures the uncertainty of the model in generating a given text
    sequence Chen et al. ([1998](#bib.bib5)), with lower perplexity values indicating
    more confident predictions by the model. In calculations, we estimate perplexity
    using the common practice of taking the logarithm of the model’s loss.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度衡量模型生成给定文本序列的 uncertainty Chen 等人 ([1998](#bib.bib5))，较低的困惑度值表示模型的预测更为自信。在计算中，我们使用对模型损失取对数的常见方法来估计困惑度。
- en: In our task, we compare the PPL scores of the ICL prompts corresponding to each
    different SDE option, as well as the conditional PPL of the models’ ICL predictions.
    For predictions, we concatenate the prompt and the prediction together as a sequence,
    then consider the prompt as its context.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的任务中，我们比较了对应于每个不同 SDE 选项的 ICL 提示的 PPL 分数，以及模型 ICL 预测的条件 PPL。对于预测，我们将提示和预测连接在一起作为一个序列，然后将提示视为其上下文。
- en: 'The perplexity results for different designs are shown in Table [12](#A1.T12
    "Table 12 ‣ A.8.2 Perplexity Analysis ‣ A.8 Can PE Guide SDE? Detailed Results
    ‣ Appendix A Appendix ‣ Sample Design Engineering: An Empirical Study of What
    Makes Good Downstream Fine-Tuning Samples for LLMs"). For input designs, the PPL
    score of Inst-first option is lower than that of Inst-last in general, which is
    consistent with the conclusion that Inst-first performs better in ICL and SDE
    experiments. For output designs, the OU option gets the highest score, which is
    inconsistent with its performance on the ICL, but is consistent with its being
    the worst option in the SDE experiment. Surprisingly, the JSON format achieved
    the significantly lowest ppl score, but it was on par with the Lines format in
    ICL and even worse than Lines in SDE. The most interesting result appears in the
    reasoning designs. The CoT and R-CoT options have low PPL scores on prompts but
    have high scores on predictions conversely. Such contradictions make it difficult
    to analyze the results of ICL or SDE through PPL scores.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '不同设计的困惑度结果见表 [12](#A1.T12 "Table 12 ‣ A.8.2 Perplexity Analysis ‣ A.8 Can PE
    Guide SDE? Detailed Results ‣ Appendix A Appendix ‣ Sample Design Engineering:
    An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs")。对于输入设计，Inst-first
    选项的 PPL 分数通常低于 Inst-last，这与 Inst-first 在 ICL 和 SDE 实验中表现更好的结论一致。对于输出设计，OU 选项获得了最高分，这与其在
    ICL 中的表现不一致，但与其在 SDE 实验中最差的选项一致。令人惊讶的是，JSON 格式取得了显著最低的 ppl 分数，但在 ICL 中与 Lines
    格式不相上下，在 SDE 中甚至表现更差。最有趣的结果出现在推理设计中。CoT 和 R-CoT 选项在提示上具有较低的 PPL 分数，但在预测上却有较高的分数。这样的矛盾使得通过
    PPL 分数分析 ICL 或 SDE 的结果变得困难。'
- en: The analysis above also highlights the indispensability of our SDE experiments,
    cause we cannot predetermine the final effectiveness of different designs through
    preliminary analysis alone.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分析还突显了我们 SDE 实验的不可或缺性，因为我们不能仅通过初步分析预判不同设计的最终效果。
- en: '![Refer to caption](img/6fd7699e9b06656876b04d7a41aea359.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6fd7699e9b06656876b04d7a41aea359.png)'
- en: 'Figure 10: Examples of format error types and how they are processed.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：格式错误类型的示例及其处理方式。
- en: '![Refer to caption](img/516f27f97616a4c46a17d7804c5ae455.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/516f27f97616a4c46a17d7804c5ae455.png)'
- en: 'Figure 11: Examples of different sample designs on GENIA, MAVEN and Review11.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：GENIA、MAVEN 和 Review11 上不同样本设计的示例。
- en: '![Refer to caption](img/7c73bea5fd489c8d4c76faa362f2ca61.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7c73bea5fd489c8d4c76faa362f2ca61.png)'
- en: 'Figure 12: Variations of Instructions on different strategies.(taking MAVEN
    as an example)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 不同策略下指令的变化（以 MAVEN 为例）'
- en: '| Perplexity:Prompts | c-llama2-chat | c-llama2-base | intern-chat | intern-base
    | bc2-chat | bc2-base |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度:提示 | c-llama2-chat | c-llama2-base | intern-chat | intern-base | bc2-chat
    | bc2-base |'
- en: '| Input | Inst-last, No-MI | 47.662 | 111.063 | 18.422 | 19.036 | 59.046 |
    42.030 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | 47.662 | 111.063 | 18.422 | 19.036 | 59.046 | 42.030
    |'
- en: '| Inst-first, _ | 46.357 | 110.065 | 19.561 | 18.632 | 54.795 | 39.003 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | 46.357 | 110.065 | 19.561 | 18.632 | 54.795 | 39.003 |'
- en: '| Output | Natural, TxtLabel, PU | 47.662 | 111.063 | 18.422 | 19.036 | 59.046
    | 42.030 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 47.662 | 111.063 | 18.422 | 19.036 | 59.046 | 42.030
    |'
- en: '| Lines, _, _ | 47.918 | 191.274 | 18.561 | 19.219 | 60.498 | 42.638 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 行数, _, _ | 47.918 | 191.274 | 18.561 | 19.219 | 60.498 | 42.638 |'
- en: '| JSON, _, _ | 29.008 | 78.848 | 14.675 | 13.260 | 38.547 | 25.405 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 29.008 | 78.848 | 14.675 | 13.260 | 38.547 | 25.405 |'
- en: '| _, NumLabel, _ | 41.690 | 92.717 | 17.664 | 16.348 | 51.963 | 35.185 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 41.690 | 92.717 | 17.664 | 16.348 | 51.963 | 35.185 |'
- en: '| _, _, OU | 55.345 | 129.055 | 20.862 | 21.450 | 69.022 | 49.426 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 55.345 | 129.055 | 20.862 | 21.450 | 69.022 | 49.426 |'
- en: '| Reasoning | No-CoT | 29.008 | 78.848 | 14.675 | 13.260 | 38.547 | 25.405
    |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 29.008 | 78.848 | 14.675 | 13.260 | 38.547 | 25.405 |'
- en: '| CoT | 18.263 | 41.312 | 10.812 | 9.379 | 23.406 | 15.267 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 18.263 | 41.312 | 10.812 | 9.379 | 23.406 | 15.267 |'
- en: '| R-CoT | 18.210 | 42.648 | 10.789 | 9.354 | 22.671 | 15.333 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 18.210 | 42.648 | 10.789 | 9.354 | 22.671 | 15.333 |'
- en: '| Perplexity:Predictions | c-llama2-chat | c-llama2-base | intern-chat | intern-base
    | bc2-chat | bc2-base |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度:预测 | c-llama2-chat | c-llama2-base | intern-chat | intern-base | bc2-chat
    | bc2-base |'
- en: '| Input | Inst-last, No-MI | 1.052 | 1.109 | 1.051 | 1.394 | 1.061 | 1.127
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last, No-MI | 1.052 | 1.109 | 1.051 | 1.394 | 1.061 | 1.127 |'
- en: '| Inst-first, _ | 1.088 | 1.284 | 1.046 | 1.360 | 1.066 | 1.113 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first, _ | 1.088 | 1.284 | 1.046 | 1.360 | 1.066 | 1.113 |'
- en: '| Output | Natural, TxtLabel, PU | 1.052 | 1.109 | 1.051 | 1.394 | 1.061 |
    1.127 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 1.052 | 1.109 | 1.051 | 1.394 | 1.061 | 1.127 |'
- en: '| Lines, _, _ | 1.052 | 1.137 | 1.058 | 1.386 | 1.222 | 1.136 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 行数, _, _ | 1.052 | 1.137 | 1.058 | 1.386 | 1.222 | 1.136 |'
- en: '| JSON, _, _ | 1.038 | 1.074 | 1.045 | 1.407 | 1.019 | 1.042 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 1.038 | 1.074 | 1.045 | 1.407 | 1.019 | 1.042 |'
- en: '| _, NumLabel, _ | 1.096 | 1.142 | 1.078 | 1.403 | 1.088 | 1.102 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 1.096 | 1.142 | 1.078 | 1.403 | 1.088 | 1.102 |'
- en: '| _, _, OU | 1.183 | 1.368 | 1.089 | 1.279 | 1.353 | 1.823 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 1.183 | 1.368 | 1.089 | 1.279 | 1.353 | 1.823 |'
- en: '| Reasoning | No-CoT | 1.038 | 1.074 | 1.045 | 1.407 | 1.019 | 1.042 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 1.038 | 1.074 | 1.045 | 1.407 | 1.019 | 1.042 |'
- en: '| CoT | 1.234 | 1.475 | 1.084 | 1.186 | 1.090 | 1.129 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 1.234 | 1.475 | 1.084 | 1.186 | 1.090 | 1.129 |'
- en: '| R-CoT | 1.239 | 1.293 | 1.069 | 1.185 | 1.063 | 1.090 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 1.239 | 1.293 | 1.069 | 1.185 | 1.063 | 1.090 |'
- en: 'Table 12: The PPL scores on the ICL prompts and predictions corresponding to
    each SDE options on the MASA ID tasks.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 对应每个 SDE 选项的 ICL 提示和预测的 PPL 分数。'
- en: '|  |  | c-llama2-chat | Intern-chat | bc2-chat | c-llama2-base | Intern-base
    | bc2-base |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  |  | c-llama2-chat | Intern-chat | bc2-chat | c-llama2-base | Intern-base
    | bc2-base |'
- en: '|  |  | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  |  | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 | D1 | D2 |'
- en: '| Input | Ins-last | 74.24 | 31.67 | 85.82 | 11.75 | 40.67 | 22.12 | 88.92
    | 36.60 | 94.89 | 81.60 | 100 | 98.18 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last | 74.24 | 31.67 | 85.82 | 11.75 | 40.67 | 22.12 | 88.92 |
    36.60 | 94.89 | 81.60 | 100 | 98.18 |'
- en: '| Ins-first | 70.05 | 44.82 | 98.76 | 99.61 | 59.56 | 24.18 | 88.62 | 27.49
    | 89.79 | 75.59 | 99.66 | 96.26 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first | 70.05 | 44.82 | 98.76 | 99.61 | 59.56 | 24.18 | 88.62 | 27.49
    | 89.79 | 75.59 | 99.66 | 96.26 |'
- en: '| Output | Natural, TxtLabel, PU | 74.24 | 31.67 | 85.82 | 11.75 | 40.67 |
    22.12 | 88.92 | 36.60 | 94.89 | 81.60 | 100 | 98.18 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 自然, TxtLabel, PU | 74.24 | 31.67 | 85.82 | 11.75 | 40.67 | 22.12 | 88.92
    | 36.60 | 94.89 | 81.60 | 100 | 98.18 |'
- en: '| Lines, _, _ | 1.18 | 1.31 | 99.94 | 97.06 | 4.17 | 1.57 | 72.51 | 12.10 |
    99.57 | 99.79 | 99.99 | 99.94 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 行数, _, _ | 1.18 | 1.31 | 99.94 | 97.06 | 4.17 | 1.57 | 72.51 | 12.10 | 99.57
    | 99.79 | 99.99 | 99.94 |'
- en: '| JSON, _, _ | 5.94 | 16.49 | 100 | 100 | 96.15 | 73.53 | 99.94 | 100 | 100
    | 100 | 100 | 100 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 5.94 | 16.49 | 100 | 100 | 96.15 | 73.53 | 99.94 | 100 | 100
    | 100 | 100 | 100 |'
- en: '| _, Numerical, _ | 99.87 | 92.21 | 99.99 | 100 | 100 | 100 | 100 | 100 | 100
    | 100 | 100 | 100 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| _, 数值, _ | 99.87 | 92.21 | 99.99 | 100 | 100 | 100 | 100 | 100 | 100 | 100
    | 100 | 100 |'
- en: '| _, _, OU | 45.75 | 18.31 | 70.21 | 31.38 | 44.15 | 50.93 | 72.79 | 87.99
    | 76.80 | 56.87 | 99.74 | 95.33 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 45.75 | 18.31 | 70.21 | 31.38 | 44.15 | 50.93 | 72.79 | 87.99
    | 76.80 | 56.87 | 99.74 | 95.33 |'
- en: '| Reasoning | No-CoT | 5.94 | 16.49 | 100 | 100 | 96.15 | 73.53 | 99.94 | 100
    | 100 | 100 | 100 | 100 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 5.94 | 16.49 | 100 | 100 | 96.15 | 73.53 | 99.94 | 100 | 100
    | 100 | 100 | 100 |'
- en: '| CoT | 35.25 | 34.25 | 100 | 100 | 58.66 | 53.29 | 100 | 100 | 100 | 100 |
    99.99 | 99.99 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 35.25 | 34.25 | 100 | 100 | 58.66 | 53.29 | 100 | 100 | 100 | 100 |
    99.99 | 99.99 |'
- en: '|  | R-CoT | 33.84 | 75.87 | 100 | 100 | 80.71 | 77.12 | 98.24 | 90.58 | 100
    | 100 | 100 | 100 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | R-CoT | 33.84 | 75.87 | 100 | 100 | 80.71 | 77.12 | 98.24 | 90.58 | 100
    | 100 | 100 | 100 |'
- en: 'Table 13: Format error rate(%) in zero-shot scenario'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：零样本场景中的格式错误率（%）
- en: '| test_size=500 | c-llama2-chat | c-llama2-base | intern-chat | intern-base
    | bc2-chat | bc2-base |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| test_size=500 | c-llama2-chat | c-llama2-base | intern-chat | intern-base
    | bc2-chat | bc2-base |'
- en: '| Input | Inst-last | 0.3834 | 0.2835 | 0.1856 | 0.1212 | 0.4402 | 0.4187 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | Inst-last | 0.3834 | 0.2835 | 0.1856 | 0.1212 | 0.4402 | 0.4187 |'
- en: '| Inst-first | 0.4832 | 0.2959 | 0.2038 | 0.2044 | 0.5091 | 0.4345 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| Inst-first | 0.4832 | 0.2959 | 0.2038 | 0.2044 | 0.5091 | 0.4345 |'
- en: '| Output | Natural, TxtLabel, PU | 0.3834 | 0.2835 | 0.1856 | 0.1212 | 0.4402
    | 0.4187 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | Natural, TxtLabel, PU | 0.3834 | 0.2835 | 0.1856 | 0.1212 | 0.4402 |
    0.4187 |'
- en: '| Lines, _, _ | 0.4220 | 0.2921 | 0.2436 | 0.1846 | 0.3971 | 0.4077 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| Lines, _, _ | 0.4220 | 0.2921 | 0.2436 | 0.1846 | 0.3971 | 0.4077 |'
- en: '| JSON, _, _ | 0.3773 | 0.2132 | 0.3390 | 0.2954 | 0.4614 | 0.3683 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| JSON, _, _ | 0.3773 | 0.2132 | 0.3390 | 0.2954 | 0.4614 | 0.3683 |'
- en: '| _, NumLabel, _ | 0.1522 | 0.1666 | 0.2470 | 0.2603 | 0.2406 | 0.1960 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| _, NumLabel, _ | 0.1522 | 0.1666 | 0.2470 | 0.2603 | 0.2406 | 0.1960 |'
- en: '| _, _, OU | 0.3612 | 0.3168 | 0.2461 | 0.1443 | 0.1948 | 0.1924 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| _, _, OU | 0.3612 | 0.3168 | 0.2461 | 0.1443 | 0.1948 | 0.1924 |'
- en: '| Reasoning | No-CoT | 0.3773 | 0.2132 | 0.3390 | 0.2954 | 0.4614 | 0.3683
    |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | No-CoT | 0.3773 | 0.2132 | 0.3390 | 0.2954 | 0.4614 | 0.3683 |'
- en: '| CoT | 0.3383 | 0.2174 | 0.3636 | 0.3167 | 0.4810 | 0.4466 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.3383 | 0.2174 | 0.3636 | 0.3167 | 0.4810 | 0.4466 |'
- en: '| R-CoT | 0.3638 | 0.2445 | 0.3522 | 0.2633 | 0.4668 | 0.4075 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| R-CoT | 0.3638 | 0.2445 | 0.3522 | 0.2633 | 0.4668 | 0.4075 |'
- en: 'Table 14: The average weighted Kappa $\kappa$ on the MASA ID tasks in in-context
    learning scenario'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：在上下文学习场景中 MASA ID 任务的平均加权 Kappa $\kappa$
