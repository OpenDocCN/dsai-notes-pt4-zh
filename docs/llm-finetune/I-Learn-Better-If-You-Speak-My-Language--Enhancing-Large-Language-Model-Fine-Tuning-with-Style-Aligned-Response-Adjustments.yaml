- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning
    with Style-Aligned Response Adjustments'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我在你说我的语言时学得更好：通过风格一致的响应调整来增强大型语言模型的微调
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11192](https://ar5iv.labs.arxiv.org/html/2402.11192)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11192](https://ar5iv.labs.arxiv.org/html/2402.11192)
- en: Xuan Ren
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xuan Ren
- en: xuan.ren@adelaide.edu.au
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: xuan.ren@adelaide.edu.au
- en: \AndBio Wu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \AndBio Wu
- en: biaowu165534@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: biaowu165534@gmail.com
- en: \AndLingqiao Liu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AndLingqiao Liu
- en: lingqiao.liu@adelaide.edu.au Corresponding author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: lingqiao.liu@adelaide.edu.au 通讯作者。
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning large language models (LLMs) with a small data set for particular
    tasks is a widely encountered yet complex challenge. The potential for overfitting
    on a limited number of examples can negatively impact the model’s ability to generalize
    and retain its original skills. Our research explores the impact of the style
    of ground-truth responses during the fine-tuning process. We found that matching
    the ground-truth response style with the LLM’s inherent style results in better
    learning outcomes. Building on this insight, we developed a method that minimally
    alters the LLM’s pre-existing responses to correct errors, using these adjusted
    responses as training targets. This technique enables precise corrections in line
    with the model’s native response style, safeguarding the model’s core capabilities
    and thus avoid overfiting. Our findings show that this approach not only improves
    the LLM’s task-specific accuracy but also crucially maintains its original competencies
    and effectiveness.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小数据集对大型语言模型（LLMs）进行特定任务的微调是一项广泛遇到但复杂的挑战。过拟合有限样本的潜力可能会对模型的泛化能力和保留其原始技能产生负面影响。我们的研究探索了在微调过程中真实响应风格的影响。我们发现将真实响应风格与LLM的固有风格相匹配可以获得更好的学习效果。在此基础上，我们开发了一种方法，通过最小程度地改变LLM的预先存在的响应来纠正错误，并使用这些调整后的响应作为训练目标。这种技术使得纠正与模型的原生响应风格一致，从而保护模型的核心能力，避免过拟合。我们的研究结果表明，这种方法不仅提高了LLM的任务特定准确性，而且重要的是保持了其原始能力和有效性。
- en: 'I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning
    with Style-Aligned Response Adjustments'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我在你说我的语言时学得更好：通过风格一致的响应调整来增强大型语言模型的微调
- en: 'Xuan Ren xuan.ren@adelaide.edu.au                        Bio Wu biaowu165534@gmail.com
                           Lingqiao Liu^†^†thanks: Corresponding author. lingqiao.liu@adelaide.edu.au'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Xuan Ren xuan.ren@adelaide.edu.au                        Bio Wu biaowu165534@gmail.com
                           Lingqiao Liu^†^†感谢：通讯作者。 lingqiao.liu@adelaide.edu.au
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/39e48e76d4a7d37ef0f0fc27c01626c3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/39e48e76d4a7d37ef0f0fc27c01626c3.png)'
- en: 'Figure 1: Minimum Change Data Example'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：最小变化数据示例
- en: '![Refer to caption](img/90e7a7a4bfb9bec8b43bbe6fe8a59e43.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/90e7a7a4bfb9bec8b43bbe6fe8a59e43.png)'
- en: 'Figure 2: This figure displays the model’s performance on 100 training samples
    across 4 datasets: GSM8K, MATH Algebra, MATH Counting and Probability, and HumanEval(coding
    dataset). It compares outcomes from various training data construction methods:
    Minimum Change, GPT-4, Ground Truth, Sample 10, and Paraphrase, highlighting the
    diverse impacts of each method.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：该图展示了模型在 100 个训练样本上在 4 个数据集（GSM8K、MATH 代数、MATH 计数和概率、HumanEval（编程数据集））上的表现。它比较了各种训练数据构建方法的结果：最小变化、GPT-4、真实数据、样本
    10 和同义改写，突出显示了每种方法的不同影响。
- en: '|  | Rank 8 | Rank 2 |  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | 排名 8 | 排名 2 |  |'
- en: '| Method | GSM8K | Math Algebra | Math Counting | HumanEval | GSM8K | Math
    Algebra | Math Counting | HumanEval | Perplexity |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GSM8K | 数学代数 | 数学计数 | 人工评估 | GSM8K | 数学代数 | 数学计数 | 人工评估 | 困惑度 |'
- en: '| Zero-shot | 0.32 | 0.22 | 0.108 | - | 0.32 | 0.22 | 0.108 | - | - |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot | 0.32 | 0.22 | 0.108 | - | 0.32 | 0.22 | 0.108 | - | - |'
- en: '| Groundtruth | 0.255 | 0.186 | 0.081 | - | 0.249 | 0.182 | 0.108 | - | 3.98
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 真实数据 | 0.255 | 0.186 | 0.081 | - | 0.249 | 0.182 | 0.108 | - | 3.98 |'
- en: '| GPT4 | 0.362 | 0.204 | 0.144 | - | 0.373 | 0.223 | 0.117 | - | 2.53 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.362 | 0.204 | 0.144 | - | 0.373 | 0.223 | 0.117 | - | 2.53 |'
- en: '| Sample 10 | 0.336 | 0.201 | 0.126 | - | 0.356 | 0.201 | 0.126 | - | 1.80
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 样本 10 | 0.336 | 0.201 | 0.126 | - | 0.356 | 0.201 | 0.126 | - | 1.80 |'
- en: '| Paraphrased | 0.311 | 0.212 | 0.10 | - | 0.324 | 0.223 | 0.126 | - | 4.02
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 同义改写 | 0.311 | 0.212 | 0.10 | - | 0.324 | 0.223 | 0.126 | - | 4.02 |'
- en: '| Minimum Change | 0.390 | 0.230 | 0.108 | - | 0.385 | 0.238 | 0.126 | - |
    1.88 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.390 | 0.230 | 0.108 | - | 0.385 | 0.238 | 0.126 | - | 1.88 |'
- en: '| Groundtruth | 0.215 | 0.175 | 0.135 | - | 0.215 | 0.160 | 0.126 | - | 8.33
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 真实数据 | 0.215 | 0.175 | 0.135 | - | 0.215 | 0.160 | 0.126 | - | 8.33 |'
- en: '| GPT4 | 0.294 | 0.223 | 0.090 | - | 0.280 | 0.257 | 0.117 | - | 3.21 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.294 | 0.223 | 0.090 | - | 0.280 | 0.257 | 0.117 | - | 3.21 |'
- en: '| Sample 10 | 0.325 | 0.204 | 0.171 | - | 0.356 | 0.216 | 0.126 | - | 4.13
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 样本10 | 0.325 | 0.204 | 0.171 | - | 0.356 | 0.216 | 0.126 | - | 4.13 |'
- en: '| Paraphrased | 0.321 | 0.201 | 0.090 | - | 0.339 | 0.216 | 0.144 | - | 3.97
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 释义 | 0.321 | 0.201 | 0.090 | - | 0.339 | 0.216 | 0.144 | - | 3.97 |'
- en: '| Minimum Change | 0.390 | 0.279 | 0.135 | - | 0.395 | 0.271 | 0.153 | - |
    2.59 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.390 | 0.279 | 0.135 | - | 0.395 | 0.271 | 0.153 | - | 2.59 |'
- en: '| Groundtruth | 0.180 | 0.144 | 0.126 | - | 0.230 | 0.134 | 0.162 | - | 9.34
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 0.180 | 0.144 | 0.126 | - | 0.230 | 0.134 | 0.162 | - | 9.34 |'
- en: '| GPT4 | 0.315 | 0.162 | 0.186 | - | 0.328 | 0.197 | 0.153 | - | 3.39 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.315 | 0.162 | 0.186 | - | 0.328 | 0.197 | 0.153 | - | 3.39 |'
- en: '| Sample 10 | 0.325 | 0.171 | 0.204 | - | 0.318 | 0.193 | 0.171 | - | 3.39
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 样本10 | 0.325 | 0.171 | 0.204 | - | 0.318 | 0.193 | 0.171 | - | 3.39 |'
- en: '| Paraphrased | 0.342 | 0.178 | 0.162 | - | 0.352 | 0.219 | 0.180 | - | 4.60
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 释义 | 0.342 | 0.178 | 0.162 | - | 0.352 | 0.219 | 0.180 | - | 4.60 |'
- en: '| Minimum Change | 0.365 | 0.198 | 0.201 | - | 0.361 | 0.201 | 0.162 | - |
    2.91 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.365 | 0.198 | 0.201 | - | 0.361 | 0.201 | 0.162 | - | 2.91 |'
- en: '| Groundtruth | 0.028 | 0.037 | 0.057 | 0.148 | 0.101 | 0.104 | 0.072 | 0.205
    | 16.2 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 0.028 | 0.037 | 0.057 | 0.148 | 0.101 | 0.104 | 0.072 | 0.205 | 16.2
    |'
- en: '| GPT4 | 0.301 | 0.192 | 0.114 | 0.137 | 0.323 | 0.200 | 0.102 | 0.126 | 3.68
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.301 | 0.192 | 0.114 | 0.137 | 0.323 | 0.200 | 0.102 | 0.126 | 3.68
    |'
- en: '| Paraphrased | 0.293 | 0.190 | 0.138 | 0.185 | 0.343 | 0.213 | 0.129 | 0.162
    | 4.43 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 释义 | 0.293 | 0.190 | 0.138 | 0.185 | 0.343 | 0.213 | 0.129 | 0.162 | 4.43
    |'
- en: '| Minimum Change | 0.327 | 0.188 | 0.129 | 0.190 | 0.341 | 0.190 | 0.126 |
    0.189 | 2.28 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.327 | 0.188 | 0.129 | 0.190 | 0.341 | 0.190 | 0.126 | 0.189 | 2.28
    |'
- en: 'Table 1: Comparison of model performance across Rank 8 and Rank 2 training
    conditions for GSM8K, Math Algebra, Math Counting and Probability, and HumanEval
    (coding task) with train =100, alongside the Perplexity values for each dataset.
    In-domain performance is highlighted in grey. For example, in the first block,
    the column under GSM8K is highlighted in grey, indicating that the training dataset
    is GSM8K, and the evaluations for the other datasets are cross-task. The Perplexity
    value displayed on the right for the first block represents the perplexity of
    the datasets used in the data construction methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GSM8K、数学代数、数学计数与概率和HumanEval（编程任务）的模型性能在Rank 8和Rank 2训练条件下的比较，以及每个数据集的困惑度值。领域内的表现用灰色突出显示。例如，在第一个块中，GSM8K下的列用灰色突出显示，表示训练数据集是GSM8K，其他数据集的评估是跨任务的。第一个块右侧显示的困惑度值表示数据构建方法中使用的数据集的困惑度。
- en: Despite the remarkable achievements of Large Language Models (LLMs) across a
    myriad of tasks, their performance is not universally excellent. Particularly,
    LLMs, especially those with parameter sizes ranging from 3 to 20 billion, often
    require fine-tuning to excel at specific tasks. This process of fine-tuning LLMs
    with a small set of training data, sometimes just hundreds of samples, presents
    a desirable yet formidable challenge. The utility of such a setting is significant,
    as it enables the adaptation of LLMs to niche tasks with limited available data,
    fostering broader applicability and facilitating rapid deployment in dynamic environments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLM）在众多任务上取得了显著成绩，但它们的表现并非普遍优秀。特别是那些参数规模从30亿到200亿的LLM，通常需要通过微调才能在特定任务中表现出色。用少量训练数据（有时只有数百个样本）对LLM进行微调，虽然具有挑战性，但也是非常渴望的。这种设置的实用性很大，因为它使LLM能够适应具有有限数据的专业任务，从而促进了更广泛的适用性并加速了在动态环境中的部署。
- en: 'The challenge, however, lies in the nuanced nature of LLM learning. Our investigation
    reveals that the style of response — how instructions are interpreted and responses
    are generated by LLMs — plays a critical role in training efficacy. LLMs can produce
    multiple, equivalent responses varying in wording, format, and presentation order.
    This variance raises the question: do these stylistic differences affect training
    outcomes, and if so, which version of a response is most conducive to learning?'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，挑战在于LLM学习的细微特性。我们的研究揭示了回应的风格——即指令如何被解释以及LLM如何生成回应——在训练效果中起着关键作用。LLM可以生成多种在措辞、格式和呈现顺序上不同但等效的回应。这种差异引发了一个问题：这些风格上的差异是否影响训练结果？如果是的话，哪种回应版本最有利于学习？
- en: To address these questions, we conducted a series of experiments comparing different
    methods of response generation, including human-provided ground truth, responses
    generated by GPT-4 (a teacher LLM), paraphrased data, minimum change data, and
    correct responses collected directfly from the model after multiple attempts.
    Our findings suggest that the style of responses significantly impacts learning
    outcomes. Specifically, we observed a correlation between the perplexity of the
    response, as measured by the LLM, and performance; lower perplexity is helpful
    for performance. The model’s quicker learning from low perplexity knowledge can
    be attributed to the minimal need for extensive parameter modifications to align
    with the target domain’s distribution.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们进行了一系列实验，比较了不同的响应生成方法，包括人工提供的真实数据、由GPT-4生成的响应（作为教师LLM）、改写的数据、最小变化数据，以及在多次尝试后直接从模型收集的正确响应。我们的发现表明，响应的风格对学习效果有显著影响。具体来说，我们观察到响应的困惑度（由LLM测量）与性能之间存在相关性；较低的困惑度对性能有帮助。模型从低困惑度知识中更快地学习，可以归因于对目标领域分布的对齐所需的参数调整较少。
- en: Inspired by these insights, we propose a novel training approach termed "minimum
    change." This method involves the model making an initial prediction, which is
    then minimally corrected by GPT-4 to address inaccuracies. By pairing the minimally
    altered target with the original input, we create a new training dataset that
    preserves much of the original text style, reducing the need for the model to
    adapt to a new domain. This approach not only addresses the challenge of language
    style discrepancies but also enhances cross task generalization and accelerates
    the learning process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些见解，我们提出了一种新颖的训练方法，称为“最小变化”。这种方法涉及模型进行初步预测，然后通过GPT-4进行最小修正以解决不准确之处。通过将最小改变后的目标与原始输入配对，我们创建了一个新的训练数据集，这样可以保留大部分原始文本风格，减少模型适应新领域的需求。这种方法不仅解决了语言风格差异的问题，还增强了跨任务的泛化能力，并加速了学习过程。
- en: 'In summary, our contributions are threefold: 1\. We highlight the impact of
    language style discrepancies between training data and the model’s internal preferences
    on learning behavior, demonstrating that minimizing these discrepancies can improve
    learning efficiency and cross task generalization. 2\. We introduce a versatile
    "minimum change" training data construction method that consistently generates
    high-quality training data with low language style discrepancies, thereby enhancing
    learning effectiveness.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献有三方面：1. 我们强调了训练数据与模型内部偏好之间的语言风格差异对学习行为的影响，证明了最小化这些差异可以提高学习效率和跨任务泛化能力。2.
    我们介绍了一种多用途的“最小变化”训练数据构建方法，该方法持续生成高质量的训练数据，具有低语言风格差异，从而提升学习效果。
- en: This paper studies the nuanced relationship between response style and training
    effectiveness, offering a novel methodology to optimize LLM performance across
    diverse tasks and domains.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了响应风格与训练效果之间的微妙关系，提出了一种优化LLM在多样任务和领域中表现的新方法。
- en: 2 Related Works
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Our work intersects several key areas in natural language processing and machine
    learning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作涉及自然语言处理和机器学习的多个关键领域。
- en: 'Alignment Methods: Several alignment methods like Proximal Policy Optimization
    (PPO) Schulman et al. ([2017](#bib.bib16)), Reward Learning from Human Feedback
    (RLHF) Christiano et al. ([2017](#bib.bib2)), and Direct Preference Optimization
    (DPO) Rafailov et al. ([2023](#bib.bib14)) aim to retain the model’s core knowledge
    while aligning its values with human preferences. Unlike data-heavy fine-tuning,
    which risks catastrophic forgetting, alignment adapts model outputs to preferred
    human outcomes with minimal retraining. This efficient approach requires less
    data, suits limited dataset scenarios, and preserves the model’s general knowledge
    without significant weight adjustments.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐方法：几种对齐方法如近端策略优化（PPO）Schulman等人 ([2017](#bib.bib16))、基于人类反馈的奖励学习（RLHF）Christiano等人
    ([2017](#bib.bib2)) 和直接偏好优化（DPO）Rafailov等人 ([2023](#bib.bib14)) 旨在保留模型的核心知识，同时使其价值观与人类偏好对齐。与数据密集型的微调不同，对齐在最小重新训练的情况下将模型输出调整为人类所偏好的结果。这种高效的方法需要较少的数据，适用于数据集有限的情况，并且在不显著调整权重的情况下保留模型的一般知识。
- en: 'Self-Training: Several works utilize a model’s own predictions for self-training.
    For instance, STAR Zelikman et al. ([2022](#bib.bib19)) and REST Gulcehre et al.
    ([2023](#bib.bib5)) generates a dataset through sample production from the LLMs,
    subsequently utilizing these samples to enhance the LLMs via training. RESTem
    Singh et al. ([2023](#bib.bib17)) enhances model performance by using initial
    predictions, filtering for accuracy, and retraining the model with correct predictions.
    This iterative process improves the model’s accuracy over multiple cycles.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自我训练：一些研究利用模型自身的预测进行自我训练。例如，STAR Zelikman 等人 ([2022](#bib.bib19)) 和 REST Gulcehre
    等人 ([2023](#bib.bib5)) 通过从大语言模型中生成样本来创建数据集，随后利用这些样本通过训练提升大语言模型的性能。RESTem Singh
    等人 ([2023](#bib.bib17)) 通过使用初始预测、筛选准确性，并用正确预测重新训练模型，从而提高模型性能。这一迭代过程在多个周期中改善了模型的准确性。
- en: 'Knowledge Distillation: (Hinton et al., [2015](#bib.bib7)) introduced the concept
    of knowledge distillation, where a smaller model (student) learns to mimic the
    behavior of a larger, pre-trained model (teacher). Several works in NLP distilling
    the knowledge from the large language models for smaller modelsKim and Rush ([2016](#bib.bib11));
    Sanh et al. ([2019](#bib.bib15)); He et al. ([2021](#bib.bib6)); Latif et al.
    ([2023](#bib.bib12)); Gu et al. ([2023](#bib.bib4)); Hsieh et al. ([2023](#bib.bib8)).
    Using initial model predictions and GPT-4 for error correction, An et al. ([2023](#bib.bib1))
    introduces a novel method. This error correction data helps the model correct
    its errors, enhancing performance when combined with ground truth data. Unlike
    the minimum change method, this approach inputs questions and original answers,
    outputting both correction rationale and corrected data. However, our tests show
    this method doesn’t preserve original text styles in GPT-4’s corrections, as detailed
    with an example in the Appendix.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏：（Hinton 等人，[2015](#bib.bib7)）引入了知识蒸馏的概念，其中一个较小的模型（学生）学习模仿一个较大、经过预训练的模型（教师）的行为。在自然语言处理领域，已经有多个研究工作致力于将大语言模型中的知识蒸馏到较小的模型中，Kim
    和 Rush ([2016](#bib.bib11))；Sanh 等人 ([2019](#bib.bib15))；He 等人 ([2021](#bib.bib6))；Latif
    等人 ([2023](#bib.bib12))；Gu 等人 ([2023](#bib.bib4))；Hsieh 等人 ([2023](#bib.bib8))。An
    等人 ([2023](#bib.bib1)) 介绍了一种新方法，利用初始模型预测和 GPT-4 进行错误修正。这些错误修正数据帮助模型纠正错误，与真实数据结合使用时，能够提升性能。与最小变更方法不同，这种方法输入问题和原始答案，输出修正理由和修正后的数据。然而，我们的测试显示，这种方法在
    GPT-4 的修正中没有保留原文的风格，详细情况在附录中提供了示例。
- en: 'Counterfactual: (Kaushik et al., [2019](#bib.bib10)) propose a Study on Counterfactuals.
    They investigated counterfactual reasoning in language models, examining how altering
    input conditions can impact model outputs. Their findings are critical for understanding
    causality in NLP models and improving their decision-making processes. counter
    factual is proven to be effectiveness for align language model’s value for fairness
    Garg et al. ([2019](#bib.bib3)) and debiasing Qian et al. ([2021](#bib.bib13));
    Xu et al. ([2023](#bib.bib18)); Huang et al. ([2019](#bib.bib9))'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实：（Kaushik 等人，[2019](#bib.bib10)）提出了关于反事实的研究。他们研究了语言模型中的反事实推理，考察了如何改变输入条件可以影响模型输出。他们的发现对理解自然语言处理模型中的因果关系以及改善其决策过程至关重要。反事实被证明在对齐语言模型的公平性（Garg
    等人，[2019](#bib.bib3)）和去偏见（Qian 等人，[2021](#bib.bib13)；Xu 等人，[2023](#bib.bib18)；Huang
    等人，[2019](#bib.bib9)）方面有效。
- en: 3 The Role of the Response Style in Fine-tuning a LLM
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 响应风格在微调大语言模型中的作用
- en: We developed datasets with diverse language styles using various data construction
    methods(refer to the Compared Data Construction Methods section), noting significant
    performance variations across them during training. Figure 2 illustrates that
    datasets built with ground truth data underachieve in math tasks with 100 samples,
    yet perform better in coding tasks. In contrast, GPT-4 generated datasets excel
    in GSM8K and Math Algebra tasks but lag in the more challenging Math Counting
    and Probability and coding tasks. Training on a GPT-4 generated dataset with a
    model perplexity below 3 (for GSM8K) often results in cross-task performance equaling
    or exceeding zero-shot performance. However, with perplexity above 3, performance
    in one or two cross-domain tasks significantly drops, falling below zero-shot
    performance. The Minimum Change method consistently delivers strong performance,
    both in-domain and cross-domain (always surpassing zero-shot performance), across
    all tasks. This performance correlation is linked to perplexity levels, indicating
    that GPT-4 thrives with lower-perplexity datasets and struggles with higher-perplexity
    ones. Datasets constructed via Minimum Change invariably show low perplexity,
    leading to robust performance even on tasks where models trained with other methods
    falter. Ground truth datasets rank lowest in performance, also showing the highest
    perplexity, especially evident in their poor domain generalization on the HumanEval
    dataset, indicated by a perplexity of 16.2, while models trained with other construction
    methods maintain cross-task generalization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用各种数据构建方法（参见“数据构建方法比较”部分）开发了具有不同语言风格的数据集，并在训练过程中注意到它们之间存在显著的性能差异。图2 表明，使用真实数据构建的数据集在100个样本的数学任务中表现不佳，但在编码任务中表现更好。相比之下，GPT-4
    生成的数据集在GSM8K和数学代数任务中表现出色，但在更具挑战性的数学计数和概率以及编码任务中表现较差。在使用困惑度低于3（针对GSM8K）的GPT-4 生成的数据集进行训练时，模型的跨任务性能往往达到或超过零-shot性能。然而，当困惑度超过3时，一些跨领域任务的性能显著下降，低于零-shot性能。最小变化方法在所有任务中始终表现强劲，无论是在领域内还是跨领域（总是超过零-shot性能）。这种性能关联与困惑度水平有关，表明GPT-4在低困惑度数据集上表现优越，而在高困惑度数据集上表现不佳。通过最小变化构建的数据集通常表现出低困惑度，即使在其他方法训练的模型表现不佳的任务上，也能展现出强劲的性能。真实数据集的性能排名最低，同时也表现出最高的困惑度，特别是在HumanEval数据集上的领域泛化能力差，困惑度为16.2，而其他构建方法训练的模型保持了跨任务泛化能力。
- en: This pattern prompts inquiries into the connections between perplexity, model
    learning, and generalization. Training on smaller datasets becomes easier for
    models familiar with the styles of the target labels, suggesting cross-task style
    adaptation might induce forgetting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式引发了关于困惑度、模型学习和泛化之间关系的询问。对于熟悉目标标签风格的模型来说，在较小的数据集上训练变得更容易，这表明跨任务风格适应可能会导致遗忘。
- en: It’s important to note that perplexity serves as a measure of how well a model
    is acquainted with the training data’s styles. A high perplexity doesn’t automatically
    signify dataset difficulty but can influence learning effectiveness.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，困惑度（perplexity）作为衡量模型对训练数据风格的熟悉程度的指标。高困惑度并不自动意味着数据集难度大，但可能影响学习效果。
- en: 3.1 Compared Data Construction Methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据构建方法比较
- en: 'In our research, we employ five distinct methods to construct training sets,
    each tailored to explore different aspects of model training and evaluation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们采用了五种不同的方法来构建训练集，每种方法都旨在探索模型训练和评估的不同方面：
- en: 1\. The Ground Truth Method employs original training sets for specified tasks
    as a baseline, marked by high perplexity stemming from the varied language styles
    of human annotators, which differ significantly from those of language models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 真实数据方法：使用原始训练集作为指定任务的基线，由于人类注释者的语言风格与语言模型的风格存在显著差异，导致困惑度较高。
- en: '2\. Minimum Change Method: Involves generating initial model predictions and
    then subtly refining these through minimal adjustments. This method aligns the
    training data closely with the model’s inherent logic and text style preferences,
    resulting in lower perplexity due to the minor yet targeted modifications.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 最小变化方法：涉及生成初始模型预测，然后通过最小的调整进行微妙的改进。此方法使训练数据与模型固有的逻辑和文本风格偏好紧密对齐，从而由于这些小而有针对性的修改，导致较低的困惑度。
- en: '3\. GPT-4 Generation Method: Leverages GPT-4 to interpret questions and autonomously
    generate answers. This approach produces training data that often shares similarities
    with the model’s training corpus, yielding answers with lower perplexity compared
    to the ground truth.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. GPT-4生成方法：利用GPT-4解读问题并自主生成答案。这种方法生成的训练数据通常与模型的训练语料库相似，从而产生的答案相较于真实数据具有更低的困惑度。
- en: '4\. Mix Sampling Method: Randomly sampling 10 answers to the same question,
    then selects the most accurate responses via a correctness verifier. This method
    blends low-perplexity model-generated data with high-perplexity ground truth,
    leading to a mixed perplexity profile.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 混合采样方法：随机抽取10个相同问题的答案，然后通过正确性验证器选择最准确的回答。这种方法将低困惑度的模型生成数据与高困惑度的真实数据混合，形成混合困惑度的特征。
- en: '5\. Paraphrasing Method: Applies to the Minimum Change data, instructing GPT-4
    to paraphrase answers without altering their logical or structural essence. This
    process introduces textual style variations, increasing perplexity while preserving
    the logical framework of the Minimum Change Method.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 改述方法：应用于最小变化数据，指示GPT-4对答案进行改述而不改变其逻辑或结构本质。此过程引入了文本风格的变化，提高了困惑度，同时保留了最小变化方法的逻辑框架。
- en: By adopting these methods, our study aims to investigate the impact of training
    data construction techniques on model performance, specifically focusing on in-domain
    accuracy, cross-task generalizability, and the relationship between language style
    and learning efficiency.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这些方法，我们的研究旨在探讨训练数据构建技术对模型性能的影响，特别关注领域内准确性、跨任务的泛化能力以及语言风格与学习效率之间的关系。
- en: 4 Minimum Change Method
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 最小变化方法
- en: In the previous section, we observed that there seems to be a connection between
    the model’s learning capability, the phenomenon of forgetting, and language style.
    To validate our hypothesis, we constructed datasets using the Minimum Change method
    across different tasks, which are very close to the model’s internal distribution
    and have correct answers. We assume that the training dataset constructed using
    the "Minimum Change" approach essentially aligns with the model’s language preferences.
    The language preference not only involves the text style the model is using, but
    also include the logic it is using the perform inference. We conducted in-domain
    and cross-task evaluations mainly on small datasets. In addition, we constructed
    datasets in various formats for mathematics and coding tasks. We measured alignment
    between dataset text styles and the model’s preferences using perplexity. Lower
    perplexity indicates greater similarity between the training data and the model’s
    text style preference.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们观察到模型的学习能力、遗忘现象和语言风格之间似乎存在关联。为了验证我们的假设，我们使用最小变化方法在不同任务中构建了数据集，这些数据集非常接近模型的内部分布并具有正确答案。我们假设使用“最小变化”方法构建的训练数据集基本上与模型的语言偏好一致。语言偏好不仅涉及模型使用的文本风格，还包括模型在执行推理时使用的逻辑。我们主要在小数据集上进行了领域内和跨任务评估。此外，我们还构建了各种格式的数学和编程任务数据集。我们使用困惑度来衡量数据集文本风格与模型偏好的对齐程度。较低的困惑度表示训练数据与模型文本风格偏好的相似性更高。
- en: Training with the Minimum Change data is divided into three steps. First, we
    let the model generate an initial prediction. Second, we have GPT-4 make as few
    changes as possible to the initial prediction to correct it. A modification example
    is shown in Figure 1\. Third, we use the minimally changed predictions, modified
    by GPT-4, as target labels to train the model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最小变化数据的训练分为三个步骤。首先，我们让模型生成初步预测。其次，我们让GPT-4对初步预测进行尽可能少的修改以纠正它。修改示例见图1。第三，我们使用GPT-4修改后的最小变化预测作为目标标签来训练模型。
- en: The most crucial step here is to have GPT-4 make minimal modifications to the
    model’s initial predictions. Only by ensuring that changes to its initial predictions
    are kept as minimal as possible can we maximally preserve the model’s original
    language style. Specifically, to guiding GPT-4 for generating minimum changed
    training data, we prompted it using 3 or 4 minimum change examples. In each example,
    we add a explanation of why it is changed in this way. We list our specific requirements
    as bullet points. We show the prompt we used to guide GPT-4 for MATH Algebra dataset
    in the Appendix.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最关键的步骤是让GPT-4对模型初始预测做最小的修改。只有确保对其初始预测的更改保持尽可能最小，我们才能最大限度地保留模型的原始语言风格。具体来说，为了指导GPT-4生成最小变更的训练数据，我们使用了3或4个最小变更示例进行提示。在每个示例中，我们添加了说明为什么会这样更改。我们将具体要求列为要点。我们在附录中展示了我们用来指导GPT-4进行MATH
    Algebra数据集的提示。
- en: 5 Experiments
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: When training the model on small datasets, we found that training across multiple
    epochs can result in better performance compared to selecting the peak point on
    the validation curve. On small dataset, to achieve better performance, we examined
    the model’s behavior over various epochs and illustrated this with a learning
    curve on epochs. The training data size varies from 100 to 380\. We also experimenting
    the model’s performance on the 7473 GSM8K training dataset. we created a validation
    plot to more clearly demonstrate the model’s validation curve. In all experiment,
    we are plotting the learning curve on epochs and validation curves using only
    the testing data. We did not construct a validation set because some datasets,
    such as HumanEval and the Math counting and probability dataset we collected,
    are small. Constructing a validation dataset would further reduce their size.
    Our aim is to demonstrate training language models with the texts they are familiar
    with can results in better learning outcomes, rather than surpassing SOTA benchmarks.
    This goal can be achieved by exclusively plotting with the testing data, thereby
    providing a more accurate reflection of performance on the test distribution.
    We selected the model with the highest accuracy on learning curve for cross-task
    evaluation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当在小数据集上训练模型时，我们发现训练多个周期的结果比在验证曲线上选择峰值点能带来更好的性能。在小数据集上，为了获得更好的性能，我们检查了模型在不同周期的表现，并用周期学习曲线进行了说明。训练数据大小从100到380不等。我们还实验了模型在7473
    GSM8K训练数据集上的表现。我们创建了验证图，以更清晰地展示模型的验证曲线。在所有实验中，我们仅使用测试数据绘制学习曲线和验证曲线。我们没有构建验证集，因为一些数据集，如我们收集的HumanEval和数学计数与概率数据集，都很小。构建验证数据集会进一步减少其大小。我们的目标是展示使用熟悉文本训练语言模型可以带来更好的学习成果，而不是超越SOTA基准。这个目标可以通过仅使用测试数据绘图来实现，从而更准确地反映在测试分布上的性能。我们选择了在学习曲线上具有最高准确度的模型进行跨任务评估。
- en: 5.1 Implementation Details
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实施细节
- en: All experiments were conducted using the LLaMA2-13B-chat model. Both training
    and inference were performed with 16-bit precision. We trained the model using
    LoRA with a rank of 8 or 2, and all experiments were run on a single A100 GPU.
    Each experiment was conducted once, with the seed number set to 0\. The learning
    rate was set at 5*e-4, and the training epochs were configured to 3, 4, 5, 6,
    7, and 8\. When training the model on the full GSM8K training set, we set the
    learning rate to 5*e-5\. The number of training steps is related to the size of
    the training set, which can be seen in the validation plot. All experiments are
    trained on datasets used a batch size of 10.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都使用了LLaMA2-13B-chat模型。训练和推理都采用了16位精度。我们使用LoRA进行模型训练，秩为8或2，所有实验都在一台A100 GPU上运行。每个实验只进行了一次，种子号设置为0。学习率设置为5*e-4，训练周期配置为3、4、5、6、7和8。当在完整的GSM8K训练集上训练模型时，学习率设置为5*e-5。训练步骤的数量与训练集的大小相关，这可以在验证图中看到。所有实验在数据集上训练时使用的批量大小为10。
- en: 5.2 Datasets
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 数据集
- en: 'GSM8K (Grade School Math 8K): This dataset consists of math word problems typically
    found in grade school curricula, comprising 7,473 training data points and 1,319
    testing data points.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: GSM8K（小学数学8K）：该数据集包含通常在小学课程中找到的数学文字题，包含7473个训练数据点和1319个测试数据点。
- en: The MATH Dataset comprises a wide range of math problems across topics like
    algebra, counting and probability, geometry, and more, with difficulty levels
    from 1 to 5\. GPT-4’s accuracy on this comprehensive dataset is about 40%. For
    our study, we focus on algebra and counting and probability questions at difficulty
    levels 1 and 2, due to their straightforward answer formats suitable for our correctness
    verifier. Complex answers that our verifier can’t accurately assess are excluded
    from our dataset. This selection process results in 380 training and 269 testing
    data for algebra, and 132 training and 111 testing data for counting and probability.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MATH 数据集包含了各种数学问题，涵盖了代数、计数与概率、几何等主题，难度等级从1到5不等。GPT-4在这个综合数据集上的准确率大约为40%。在我们的研究中，我们专注于代数和计数与概率问题，难度等级为1和2，因为这些问题的答案格式简单，适合我们的正确性验证器。复杂的答案无法被我们的验证器准确评估，因此被排除在数据集之外。这一选择过程导致代数数据集有380条训练数据和269条测试数据，而计数与概率数据集有132条训练数据和111条测试数据。
- en: The HumanEval dataset is a benchmark designed to assess code generation models,
    testing their comprehension of problem statements, algorithmic solution generation,
    and the creation of syntactically correct code. With 164 examples, it’s considered
    small for extensive training, prompting us to utilize 3-fold cross-validation
    to maintain robust evaluation. Initially, we train with the first 100 examples,
    testing on the remaining 64\. Subsequently, we shift training to examples 100-164,
    testing on the initial 64\. Lastly, we combine the first 36 and last 64 examples
    for training, testing on examples 64-100\. This approach ensures a consistent
    training size of 100 and a total testing size of 164 across folds.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval 数据集是一个用于评估代码生成模型的基准，测试模型对问题陈述的理解、算法解决方案的生成以及语法正确代码的创建。该数据集有164个例子，对于大规模训练来说较小，因此我们采用3折交叉验证来保持评估的稳健性。最初，我们使用前100个例子进行训练，对剩余64个例子进行测试。随后，我们将训练转移到100-164个例子，对前64个例子进行测试。最后，我们将前36个和最后64个例子合并进行训练，对64-100个例子进行测试。这种方法确保了每折的训练大小为100，总测试大小为164。
- en: '|  | Rank 8 | Rank 2 |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 排名 8 | 排名 2 |  |'
- en: '| Method | GSM8K | Math Algebra | Math counting | GSM8K | Math Algebra | Math
    Counting | Perplexity |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GSM8K | 数学代数 | 数学计数 | GSM8K | 数学代数 | 数学计数 | 困惑度 |'
- en: '| Zero-shot | 0.32 | 0.22 | 0.108 | 0.32 | 0.22 | 0.108 | - |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | 0.32 | 0.22 | 0.108 | 0.32 | 0.22 | 0.108 | - |'
- en: '| Training data size = 200 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据量 = 200 |'
- en: '| Groundtruth | 0.262 | 0.171 | 0.072 | 0.262 | 0.171 | 0.072 | 3.98 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Groundtruth | 0.262 | 0.171 | 0.072 | 0.262 | 0.171 | 0.072 | 3.98 |'
- en: '| GPT4 | 0.438 | 0.201 | 0.099 | 0.397 | 0.245 | 0.099 | 2.53 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.438 | 0.201 | 0.099 | 0.397 | 0.245 | 0.099 | 2.53 |'
- en: '| Sample 10 | 0.246 | 0.160 | 0.126 | 0.246 | 0.160 | 0.126 | 1.80 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 样本 10 | 0.246 | 0.160 | 0.126 | 0.246 | 0.160 | 0.126 | 1.80 |'
- en: '| Paraphrased | 0.328 | 0.197 | 0.117 | 0.328 | 0.197 | 0.117 | 4.02 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 改述 | 0.328 | 0.197 | 0.117 | 0.328 | 0.197 | 0.117 | 4.02 |'
- en: '| Minimum Change | 0.394 | 0.197 | 0.153 | 0.390 | 0.212 | 0.117 | 1.88 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.394 | 0.197 | 0.153 | 0.390 | 0.212 | 0.117 | 1.88 |'
- en: '| Training data size = 300 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据量 = 300 |'
- en: '| Groundtruth | 0.309 | 0.134 | 0.072 | 0.297 | 0.197 | 0.072 | 3.98 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Groundtruth | 0.309 | 0.134 | 0.072 | 0.297 | 0.197 | 0.072 | 3.98 |'
- en: '| GPT4 | 0.428 | 0.156 | 0.117 | 0.397 | 0.208 | 0.117 | 2.53 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.428 | 0.156 | 0.117 | 0.397 | 0.208 | 0.117 | 2.53 |'
- en: '| Sample 10 | 0.270 | 0.108 | 0.090 | 0.246 | 0.160 | 0.126 | 1.80 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 样本 10 | 0.270 | 0.108 | 0.090 | 0.246 | 0.160 | 0.126 | 1.80 |'
- en: '| Paraphrased | 0.340 | 0.178 | 0.045 | 0.340 | 0.178 | 0.045 | 4.02 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 改述 | 0.340 | 0.178 | 0.045 | 0.340 | 0.178 | 0.045 | 4.02 |'
- en: '| Minimum Change | 0.406 | 0.182 | 0.153 | 0.382 | 0.219 | 0.117 | 1.88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.406 | 0.182 | 0.153 | 0.382 | 0.219 | 0.117 | 1.88 |'
- en: '| Training data size = 200 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据量 = 200 |'
- en: '| Groundtruth | 0.109 | 0.152 | 0.153 | 0.131 | 0.160 | 0.090 | 8.33 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Groundtruth | 0.109 | 0.152 | 0.153 | 0.131 | 0.160 | 0.090 | 8.33 |'
- en: '| GPT4 | 0.272 | 0.249 | 0.108 | 0.303 | 0.279 | 0.135 | 3.21 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.272 | 0.249 | 0.108 | 0.303 | 0.279 | 0.135 | 3.21 |'
- en: '| Sample 10 | 0.347 | 0.208 | 0.099 | 0.345 | 0.264 | 0.144 | 4.13 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 样本 10 | 0.347 | 0.208 | 0.099 | 0.345 | 0.264 | 0.144 | 4.13 |'
- en: '| Paraphrased | 0.339 | 0.208 | 0.099 | 0.337 | 0.216 | 0.081 | 3.97 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 改述 | 0.339 | 0.208 | 0.099 | 0.337 | 0.216 | 0.081 | 3.97 |'
- en: '| Minimum Change | 0.384 | 0.279 | 0.162 | 0.389 | 0.283 | 0.117 | 2.59 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.384 | 0.279 | 0.162 | 0.389 | 0.283 | 0.117 | 2.59 |'
- en: '| Training data size = 380 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据量 = 380 |'
- en: '| Groundtruth | 0.113 | 0.167 | 0.054 | 0.126 | 0.152 | 0.072 | 8.33 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Groundtruth | 0.113 | 0.167 | 0.054 | 0.126 | 0.152 | 0.072 | 8.33 |'
- en: '| GPT4 | 0.305 | 0.268 | 0.072 | 0.292 | 0.290 | 0.099 | 3.21 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 0.305 | 0.268 | 0.072 | 0.292 | 0.290 | 0.099 | 3.21 |'
- en: '| Sample 10 | 0.317 | 0.238 | 0.171 | 0.352 | 0.249 | 0.135 | 4.13 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 样本 10 | 0.317 | 0.238 | 0.171 | 0.352 | 0.249 | 0.135 | 4.13 |'
- en: '| Paraphrased | 0.298 | 0.178 | 0.135 | 0.334 | 0.216 | 0.108 | 3.97 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 改述 | 0.298 | 0.178 | 0.135 | 0.334 | 0.216 | 0.108 | 3.97 |'
- en: '| Minimum Change | 0.378 | 0.294 | 0.171 | 0.393 | 0.290 | 0.126 | 2.59 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.378 | 0.294 | 0.171 | 0.393 | 0.290 | 0.126 | 2.59 |'
- en: 'Table 2: We compare model performance across Rank 8 and Rank 2 training conditions
    for GSM8K, Math Algebra, and Math Counting and Probability, with training sizes
    of 200, 300, or 380, and include Perplexity values for each dataset. In-domain
    performance is marked in grey; for instance, the grey-highlighted GSM8K column
    signifies its use as the training dataset, with other datasets assessed for cross-task
    performance. The rightmost Perplexity value indicates the complexity of datasets
    involved in constructing the training data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们比较了 GSM8K、Math Algebra 和 Math Counting and Probability 的 Rank 8 和 Rank
    2 训练条件下的模型性能，训练规模为 200、300 或 380，并包括了每个数据集的 Perplexity 值。领域内表现以灰色标记；例如，灰色突出显示的
    GSM8K 列表示其作为训练数据集的使用情况，其他数据集则用于跨任务性能评估。最右侧的 Perplexity 值指示了用于构建训练数据的数据集的复杂性。
- en: 5.3 Evaluation
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 实验结果分析
- en: We assessed model performance primarily using accuracy metrics. For Math and
    coding tasks, we employed a correctness verification script and the original HumanEval
    evaluation script, respectively. To facilitate straightforward evaluation, we
    standardized the presentation of final answers across all datasets, including
    GSM8K, MATH, and HumanEval, by appending them with a "Final Answer:" keyword when
    necessary. This standardization ensures compatibility with our verification script,
    enhancing the reliability of our correctness assessment process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用准确性指标来评估模型性能。对于数学和编码任务，我们分别使用了正确性验证脚本和原始的 HumanEval 评估脚本。为了简化评估，我们在所有数据集中，包括
    GSM8K、MATH 和 HumanEval 中，通过在必要时附加“Final Answer:”关键字来标准化最终答案的呈现。这种标准化确保了与我们的验证脚本的兼容性，从而提高了正确性评估过程的可靠性。
- en: For evaluating zero-shot learning, we implemented a strategy where prompts explicitly
    format the model’s responses to end with the "Answer:" keyword, directly preceding
    the final answer. This structured approach not only standardized the response
    format across the MATH and GSM8K datasets but also significantly enhanced the
    model’s ability to provide direct answers. We manually verified the accuracy of
    this method by checking the first 100 zero-shot predictions in both datasets,
    confirming its effectiveness without any errors. Using this prompt does not degrade
    model’s zeroshot performance, which was confirmed in our detailed analysis of
    the first 300 GSM8K outputs. Models utilizing this prompt consistently generated
    direct final responses more often than those not using the prompt, which occasionally
    sought clarification or doubted the problem’s validity. This led to an improvement
    in zero-shot accuracy from 25% to 31% for the initially evaluated GSM8K data,
    as assessed manually by our team.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估零-shot 学习，我们实施了一种策略，在该策略中，提示明确要求模型的回答以“Answer:”关键字结尾，直接位于最终答案之前。这种结构化的方法不仅在
    MATH 和 GSM8K 数据集之间标准化了响应格式，而且显著提升了模型提供直接答案的能力。我们通过检查两个数据集中前 100 个零-shot 预测的准确性来手动验证该方法，确认了其有效性且没有任何错误。使用此提示不会降低模型的零-shot
    性能，这在我们对前 300 个 GSM8K 输出的详细分析中得到了确认。使用此提示的模型比未使用提示的模型更频繁地生成直接的最终回答，后者有时会寻求澄清或对问题的有效性产生怀疑。这导致了最初评估的
    GSM8K 数据的零-shot 准确性从 25% 提升到 31%，经过我们团队的手动评估。
- en: 6 Experimental Result Analyzing
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验结果分析
- en: We use perplexity metrics to briefly reflect the discrepancy of model’s generative
    preference and the training text styles. We train the model on ground-truth datasets,
    minimum change datasets, gpt-4 generated datasets, sampling 10 datasets and paraphrased
    datasets. after initial training, we then evaluate the checkpoint with the highest
    performance on cross-task datasets. We making the learning curve on epochs by
    plot the accuracy on testing dataset vs the number of training epochs. We plot
    the validation curve on testing dataset using accuracy vs training steps. We summarize
    the in-domain learning performance based on a training data size of 100 in Figure
    2\. We summarize the in-domain and cross-task performance for training datasets
    with 100 or more training data points in Table 1 and Table 2, respectively.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用困惑度指标来简要反映模型生成偏好与训练文本风格之间的差异。我们在Ground Truth数据集、最小变化数据集、GPT-4生成的数据集、Sample
    10数据集和改写数据集上训练了模型。初步训练后，我们评估在跨任务数据集上性能最高的检查点。我们通过绘制测试数据集上的准确率与训练周期数来展示学习曲线。我们绘制了测试数据集上的验证曲线，使用准确率与训练步骤进行比较。我们基于100的训练数据大小总结了图2中的领域内学习性能。我们在表1和表2中分别总结了具有100个或更多训练数据点的训练数据集的领域内和跨任务性能。
- en: 6.1 Performance Comparison when training dataset = 100
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 当训练数据集=100时的性能比较
- en: We trained the model on GSM8K, MATH Algebra, MATH Counting and Probability,
    and HumanEval (a coding task), each with 100 training data points. The experimental
    results, displayed in Figure 2 and Table 1, reveal that models trained on Minimum
    Change datasets converge faster and perform best among the datasets. In contrast,
    Ground Truth datasets consistently underperform. Models trained on datasets created
    by GPT-4, Sample 10, or Paraphrased methods show mixed results, excelling in some
    tasks while falling short in others. Table 1 shows that the Ground Truth dataset
    exhibits the highest perplexity, whereas the Minimum Change dataset has the lowest,
    mirroring their performance levels. The reasons behind this perplexity distribution
    are discussed in the Compared Data Construction Methods section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在GSM8K、MATH代数、MATH计数与概率以及HumanEval（一个编码任务）上训练了模型，每个数据集有100个训练数据点。实验结果（见图2和表1）显示，使用最小变化数据集训练的模型收敛速度更快，并且在所有数据集中表现最佳。相比之下，Ground
    Truth数据集的表现始终较差。使用GPT-4生成的数据集、Sample 10或改写方法创建的数据集的模型结果则表现不一，在某些任务上表现优异，而在其他任务上表现不佳。表1显示，Ground
    Truth数据集的困惑度最高，而最小变化数据集的困惑度最低，这与它们的表现水平相吻合。关于这种困惑度分布的原因，在比较数据构建方法部分有详细讨论。
- en: When comparing the Paraphrased to the Minimum Change datasets in an in-domain
    context, it’s clear that models trained on Minimum Change datasets surpass those
    trained on Paraphrased datasets in all tasks. Paraphrased datasets, though stylistically
    different, maintain the same logical structure as Minimum Change datasets, resulting
    in higher perplexity. We deduce that aligning training datasets’ text styles with
    the model’s internal preferences is advantageous for in-domain training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域内背景下，将改写数据集与最小变化数据集进行比较时，很明显，使用最小变化数据集训练的模型在所有任务上都超越了使用改写数据集训练的模型。虽然改写数据集在风格上不同，但保持了与最小变化数据集相同的逻辑结构，因此困惑度更高。我们推测，将训练数据集的文本风格与模型的内部偏好对齐对领域内训练是有利的。
- en: Models trained on Paraphrased datasets show comparable cross-task performance
    to those trained on Minimum Change datasets in certain scenarios when the rank
    is 2, such as in HumanEval, MATH Counting, and GSM8K. However, at LORA rank 8,
    models trained on Paraphrased datasets see a decline in cross-task performance,
    suggesting that unfamiliar text styles may impair cross-task capabilities with
    more trainable parameters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景下（如HumanEval、MATH计数和GSM8K），使用改写数据集训练的模型在跨任务性能上与使用最小变化数据集训练的模型相当，当排名为2时。然而，在LORA排名为8时，使用改写数据集训练的模型的跨任务性能下降，表明不熟悉的文本风格可能会影响更多可训练参数的跨任务能力。
- en: Despite similar perplexity to GPT-4 datasets, models trained on Paraphrased
    datasets exceed those trained on GPT-4 datasets in HumanEval, both in-domain and
    cross-task. This success can be attributed to the Paraphrased dataset’s preservation
    of the model’s familiar logical structures, highlighting the importance of familiar
    logic for learning and cross task generalization.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管困惑度与GPT-4数据集相似，但在HumanEval中，使用改写数据集训练的模型超越了使用GPT-4数据集训练的模型，包括领域内和跨任务的表现。这一成功可以归因于改写数据集保留了模型熟悉的逻辑结构，突显了熟悉逻辑对学习和跨任务泛化的重要性。
- en: Sample 10 performs well in some in-domain scenarios but often at the cost of
    cross-task performance, likely due to its mixed nature of ground truth and sampled
    datasets. Ground truth datasets, as shown in Table 1 and Figure 2, generally underperform
    across most datasets. Our "Comparing to Other Methods" section explores the effects
    of training models only on self-generated correct data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 样本10在某些领域内场景中表现良好，但通常会以牺牲跨任务性能为代价，这可能是由于其混合了真实数据和采样数据的特性。真实数据集，如表1和图2所示，通常在大多数数据集上表现不佳。我们的“与其他方法比较”部分探讨了仅在自生成正确数据上训练模型的效果。
- en: Training on Minimum Change datasets markedly improves in-domain and cross-task
    performance. Other methods, while boosting in-domain performance for specific
    datasets, typically sacrifice cross-task performance. This emphasizes the value
    of creating training datasets that resonate with the model’s familiarities.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在最小变化数据集上训练显著改善了领域内和跨任务的性能。其他方法虽然提升了特定数据集的领域内性能，但通常会牺牲跨任务性能。这强调了创建与模型熟悉度相契合的训练数据集的重要性。
- en: Notably, models trained on GPT-4 constructed HumanEval datasets show lower in-domain
    performance on the HumanEval dataset, despite not having significantly higher
    perplexity compared to math datasets. Conversely, models trained on Paraphrased
    datasets significantly outperform those trained on GPT-4\. The higher perplexity
    of the Paraphrased dataset, coupled with the retention of familiar problem-solving
    logic, underscores the critical role of aligning training datasets with the model’s
    known logic for optimal HumanEval performance, suggesting that deep familiarity
    with problem logic and structure is crucial for enhancing effectiveness in complex
    coding tasks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管GPT-4构建的HumanEval数据集的困惑度与数学数据集相比没有显著提高，但在HumanEval数据集上的领域内表现较低。相反，训练于改述数据集的模型显著优于训练于GPT-4的数据集。改述数据集较高的困惑度，加上保留了熟悉的问题解决逻辑，突显了将训练数据集与模型已知逻辑对齐的关键作用，以实现最佳的HumanEval表现，这表明对问题逻辑和结构的深度熟悉对提升复杂编码任务的效果至关重要。
- en: 6.2 Performance Comparison with Larger Training Datasets
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 使用较大训练数据集的性能比较
- en: To further analyze how discrepancies between data styles and the model’s internal
    preferences impact learning and cross task generalization, we increased the training
    dataset sizes. Specifically, we expanded the datasets to 200 and 300 for GSM8K,
    and to 200 and 380 for MATH Algebra. This expansion allows us to examine the effect
    of training volume on model performance across various domains. As indicated in
    Table 2, enlarging the training dataset size reveals that the in-domain performance
    of models trained on GPT-4 constructed datasets begins to outperform those trained
    on Minimum Change constructed datasets for the GSM8K task. For GSM8K datasets,
    the cross-task performance on MATH Algebra is sometimes improved, albeit the performance
    on MATH Counting remains comparatively low. Conversely, models trained on GPT-4
    constructed MATH Algebra datasets exhibit improved in-domain performance at the
    expense of a noticeable reduction in cross-task performance on GSM8K and, occasionally,
    MATH Counting. A plausible explanation is that the GPT-4 constructed GSM8K datasets
    align closely with the model’s internal preferences, as indicated by their low
    perplexity scores. Models trained on familiar GSM8K datasets, crafted by the expert
    "teacher" GPT-4, acquire generalize knowledge without significantly forgetting
    how to adapt to new domains. Despite GPT-4 created datasets having higher perplexity,
    their data quality appears superior to that of Minimum Change datasets, thus yielding
    better in-domain performance. Unlike the GSM8K datasets, which comprise elementary
    school MATH questions, MATH Algebra includes more challenging algebra questions
    that may involve solving equations, calculating fractions, and addressing complex
    word problems. This complexity is reflected by higher perplexity scores for datasets
    constructed by Ground Truth, GPT-4, and Minimum Change, among others. Models trained
    on GPT-4 constructed datasets learn more slowly compared to those trained on Minimum
    Change datasets and sacrifice more cross-task performance for in-domain gains.
    Sample 10, Paraphrased, and Ground Truth datasets exhibit a similar pattern to
    those observed in Table 1, consistently under performing compared to models trained
    on Minimum Change.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析数据风格与模型内部偏好之间的差异如何影响学习和跨任务泛化，我们增加了训练数据集的规模。具体来说，我们将GSM8K的数据集扩展到200和300，将MATH
    Algebra的数据集扩展到200和380。这种扩展使我们能够检查训练量对模型在不同领域性能的影响。如表2所示，扩大训练数据集规模显示，使用GPT-4构建的数据集训练的模型在GSM8K任务上开始超过使用最小变化构建的数据集训练的模型。对于GSM8K数据集，跨任务在MATH
    Algebra上的性能有时有所提高，尽管在MATH Counting上的表现仍然相对较低。相反，使用GPT-4构建的MATH Algebra数据集训练的模型在本领域的性能有所提高，但在GSM8K和偶尔在MATH
    Counting上的跨任务性能明显下降。一个合理的解释是，GPT-4构建的GSM8K数据集与模型的内部偏好高度一致，如其低困惑度分数所示。由专家“老师”GPT-4构建的熟悉GSM8K数据集，使得模型在不显著遗忘如何适应新领域的情况下，能够获得更广泛的知识。尽管GPT-4创建的数据集困惑度较高，但其数据质量优于最小变化数据集，从而导致更好的本领域表现。与GSM8K数据集（包含小学数学问题）不同，MATH
    Algebra包含更多具有挑战性的代数问题，可能涉及方程求解、分数计算以及复杂的文字问题。这种复杂性在真实值、GPT-4和最小变化等构建的数据集中表现为更高的困惑度分数。与使用最小变化数据集训练的模型相比，使用GPT-4构建的数据集训练的模型学习速度较慢，并且在本领域获得的收益换来了更多的跨任务性能损失。样本10、改述和真实值数据集展现了与表1中观察到的类似模式，始终低于使用最小变化训练的模型的表现。
- en: '![Refer to caption](img/9cf4ced8fa21c60ca6f37a6cc3a9fa19.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9cf4ced8fa21c60ca6f37a6cc3a9fa19.png)'
- en: 'Figure 3: Minimum Change Vs. Groundtruth on GSM8K train data size = 7473'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：最小变化与真实值在GSM8K训练数据规模 = 7473上的比较
- en: '|  | Gsm8k | Math al | Math cp | Perplexity |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | Gsm8k | 数学代数 | 数学计数 | 困惑度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Zero-shot | 0.32 | 0.22 | 0.108 | 1.19 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot | 0.32 | 0.22 | 0.108 | 1.19 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Groundtruth | 0.444 | 0.164 | 0.063 | 3.98 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 真实值 | 0.444 | 0.164 | 0.063 | 3.98 |'
- en: '| Minimum Change | 0.449 | 0.197 | 0.126 | 1.88 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.449 | 0.197 | 0.126 | 1.88 |'
- en: 'Table 3: Trained on GSM8K n_train = 7473\. Math al = Math algebra; Math cp
    = Math counting and probability;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：训练于GSM8K n_train = 7473\. 数学代数 = 数学代数；数学计数 = 数学计数与概率；
- en: 6.3 Performance Comparison on the full training dataset
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 在完整训练数据集上的性能比较
- en: We present a performance comparison between models trained on Minimum Change
    datasets and those trained on Ground Truth datasets in Figure 3, showcasing a
    validation curve across approximately 74,730 training instances. Initially, the
    model trained on the Minimum Change dataset demonstrates rapid convergence. However,
    its performance improvement rate gradually decreases over time, eventually stabilizing
    at a certain level. This phenomenon is attributed to the relatively low data quality
    of the Minimum Change datasets. The target labels in these datasets are derived
    from the model’s initial outputs, and while they are correct, they may be of inferior
    quality due to the model’s limitations. In contrast, Ground Truth data, crafted
    by experts, are of higher quality. As the model progressively adjusts to the target
    domain, it begins to close the gap with the model trained on the Minimum Change
    dataset. Nonetheless, as indicated in Table 3, this adaptation to the target text
    and logic style comes at the expense of cross-task performance. Consequently,
    models trained on Minimum Change datasets maintain superior cross-task performance,
    suggesting that while adapting to high-quality target domain data can enhance
    in-domain accuracy, it may also limit the model’s generalizability across different
    domains.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在图3中比较在最小变化数据集上训练的模型与在真实数据集上训练的模型的性能，这展示了大约74,730个训练实例的验证曲线。最初，在最小变化数据集上训练的模型表现出快速收敛。然而，其性能提升率随着时间的推移逐渐减少，最终稳定在某一水平。这一现象归因于最小变化数据集的数据质量相对较低。这些数据集中的目标标签源于模型的初步输出，虽然它们是正确的，但由于模型的局限性，可能质量较差。相比之下，由专家制作的真实数据质量更高。随着模型逐渐调整到目标领域，它开始缩小与在最小变化数据集上训练的模型之间的差距。然而，如表3所示，这种对目标文本和逻辑风格的适应以跨任务性能为代价。因此，在最小变化数据集上训练的模型保持了优越的跨任务性能，这表明，虽然适应高质量目标领域数据可以提高领域内准确性，但也可能限制模型在不同领域的泛化能力。
- en: '|  | gsm8k | math al | math cp |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | gsm8k | math al | math cp |'
- en: '| Zero-shot | 0.32 | 0.22 | 0.108 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot | 0.32 | 0.22 | 0.108 |'
- en: '| REST em R1 | 0.350 | 0.227 | 0.108 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| REST em R1 | 0.350 | 0.227 | 0.108 |'
- en: '| REST em R2 | 0.373 | 0.189 | 0.198 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| REST em R2 | 0.373 | 0.189 | 0.198 |'
- en: '| Learn from M | 0.359 | 0.216 | 0.153 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 从错误中学习 | 0.359 | 0.216 | 0.153 |'
- en: '| Minimum Change | 0.390 | 0.230 | 0.108 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.390 | 0.230 | 0.108 |'
- en: '| REST em R1 | 0.195 | 0.138 | 0.144 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| REST em R1 | 0.195 | 0.138 | 0.144 |'
- en: '| Minimum Change | 0.390 | 0.279 | 0.135 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.390 | 0.279 | 0.135 |'
- en: '| REST em R1 | 0.124 | 0.138 | 0.162 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| REST em R1 | 0.124 | 0.138 | 0.162 |'
- en: '| Minimum Change | 0.365 | 0.198 | 0.201 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 最小变化 | 0.365 | 0.198 | 0.201 |'
- en: 'Table 4: Comparing Minimum Change to REST emSingh et al. ([2023](#bib.bib17))
    and Learn from mistakesAn et al. ([2023](#bib.bib1)). We conduct experiments for
    Rest em for 2 self-training iterations, including iteration 1(R1) and iteration
    2(R2), respectively.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：将最小变化与REST em Singh等（[2023](#bib.bib17)）以及从错误中学习 An 等（[2023](#bib.bib1)）进行比较。我们对Rest
    em进行了2次自我训练迭代，包括迭代1（R1）和迭代2（R2）。
- en: 6.4 Comparing to Other Methods
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 与其他方法的比较
- en: We contrast our method against "Learning From Mistakes Makes LLM Better Reasoner"
    by An et al. ([2023](#bib.bib1)) and "REST em" by Singh et al. ([2023](#bib.bib17)),
    focusing on math datasets. REST em shows in-domain performance of 0.35 and 0.373
    across two iterations on GSM8K, while "Learn from Mistakes" achieves 0.359 with
    combined ground truth and error correction data. Our method surpasses the both
    methods on the in-domain math tasks. REST em struggles on cross-task performance
    when trained on math dataset possibly because it doesn’t introduce knowledge beyond
    the model’s capability, reflected in its generated data’s low perplexity (below
    1.2), reinforcing model biases and hindering cross-task performance. In contrast,
    our method, with perplexity over 1.8, prevents bias reinforcement and incorporates
    additional knowledge, enhancing performance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与 An 等（[2023](#bib.bib1)）的“从错误中学习使LLM更好的推理器”和 Singh 等（[2023](#bib.bib17)）的“REST
    em”进行了对比，重点关注数学数据集。REST em在GSM8K上经过两次迭代的领域内性能分别为0.35和0.373，而“从错误中学习”在结合真实数据和错误纠正数据的情况下达到了0.359。我们的方法在领域内数学任务上超越了这两种方法。REST
    em在数学数据集上训练时的跨任务性能表现不佳，这可能是因为它没有引入超出模型能力的知识，这在其生成数据的低困惑度（低于1.2）中有所体现，从而加剧了模型偏见并阻碍了跨任务性能。相比之下，我们的方法具有超过1.8的困惑度，防止了偏见的加剧，并融入了额外的知识，从而提升了性能。
- en: 7 Conclusion and Discussion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与讨论
- en: Our experiments have found that each method of constructing training data has
    its specific advantages and disadvantages for certain tasks. Through our research,
    we discovered that familiarity with the style of the target label is a significant
    factor influencing the model’s learning effectiveness. By mitigating this factor,
    we can enhance the model’s learning speed, reduce catastrophic forgetting, and
    even acquire knowledge that improves cross-task capabilities. Based on these principles,
    this work proposes a training data construction method that is applicable to most
    tasks when the training data are limited. We hope this work will inspire future
    researchers in data construction.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验发现，每种构建训练数据的方法在某些任务上都有其特定的优点和缺点。通过我们的研究，我们发现对目标标签风格的熟悉程度是影响模型学习效果的重要因素。通过减轻这一因素，我们可以提高模型的学习速度，减少灾难性遗忘，甚至获得提升跨任务能力的知识。基于这些原则，本工作提出了一种适用于大多数任务的训练数据构建方法，特别是在训练数据有限的情况下。我们希望这项工作能激发未来研究人员在数据构建方面的灵感。
- en: Our current implementation of Minimum Change only utilizes the most basic data
    construction method—directly having GPT-4 modify the model’s initial prediction.
    Indeed, this approach has considerable room for improvement. For example, could
    sampling and filtering enhance initial prediction quality for GPT-4 modifications?
    How might we develop datasets with both model-aligned styles and superior logical
    coherence? Furthermore, exploring Minimum Change’s applicability in refining the
    model’s tone, style, and internal knowledge without inducing catastrophic forgetting,
    and its role in alignment, warrants deeper investigation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前对“最小变化”的实现仅使用了最基本的数据构建方法——直接让GPT-4修改模型的初始预测。确实，这种方法还有很大的改进空间。例如，采样和过滤是否能提高GPT-4修改的初始预测质量？我们如何开发具有模型对齐风格和更优逻辑连贯性的
    数据集？此外，探索“最小变化”在改进模型的语调、风格和内部知识而不引发灾难性遗忘，以及其在对齐中的作用，值得深入研究。
- en: Limitations
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The new data construction approach, Minimum Change, presents the following limitations.
    First, to implement minimum change effectively, GPT-4 needs to have sufficient
    reasoning ability to solve problems. If the difficulty of a problem exceeds GPT-4’s
    capabilities, then accurate minimum changes to the predictions cannot be made
    directly through GPT-4\. Second, minimum change is most effective for tasks that
    require a textual segment as part of the final answer. If a task does not require
    a textual answer, the in-domain performance of minimum change might not be as
    good as training directly with ground truth. For instance, in simple sentiment
    classification tasks where the model can directly output the correct answer, training
    with gold labels might be more suitable. Adding a reasoning process to derive
    the final answer could be superfluous, as fitting the reasoning chain itself also
    requires gradients. In such cases, the final effectiveness might not be as good
    as focusing all gradients on the gold label. We have only provided a basic minimum
    change pipeline and experimental report. We hope that the issues mentioned above
    will be studied and addressed in the future.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 新的数据构建方法“最小变化”存在以下限制。首先，为了有效实施最小变化，GPT-4需要具备足够的推理能力来解决问题。如果问题的难度超出了GPT-4的能力范围，那么通过GPT-4直接做出准确的最小变化是不可能的。其次，最小变化在需要文本片段作为最终答案的任务中效果最佳。如果任务不需要文本答案，最小变化的领域内表现可能不如直接使用真实标签进行训练。例如，在简单的情感分类任务中，如果模型可以直接输出正确答案，那么使用真实标签进行训练可能更合适。将推理过程加入以得出最终答案可能是多余的，因为调整推理链本身也需要梯度。在这种情况下，最终效果可能不如将所有梯度集中在真实标签上。我们只提供了一个基本的最小变化管道和实验报告。希望上述问题能在未来得到研究和解决。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: An et al. (2023) Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang
    Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner.
    *arXiv preprint arXiv:2310.20689*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An et al. (2023) Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang
    Lou, and Weizhu Chen. 2023. 从错误中学习使llm成为更好的推理者。*arXiv预印本arXiv:2310.20689*。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. 从人类偏好中进行深度强化学习。*神经信息处理系统的进展*，30。
- en: Garg et al. (2019) Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H
    Chi, and Alex Beutel. 2019. Counterfactual fairness in text classification through
    robustness. In *Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and
    Society*, pages 219–226.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg 等（2019）萨哈杰·戈格、文森特·佩罗、妮可·林提亚科、安库尔·塔利、埃德·H·奇和亚历克斯·比图尔。2019。通过鲁棒性实现文本分类中的反事实公平。在*2019
    AAAI/ACM 人工智能、伦理与社会会议*的会议记录中，第219-226页。
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    distillation of large language models. *arXiv preprint arXiv:2306.08543*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2023）谷雨闲、董力、魏富如和黄敏烈。2023。大型语言模型的知识蒸馏。*arXiv 预印本 arXiv:2306.08543*。
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. 2023. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulcehre 等（2023）卡格拉尔·古尔切赫雷、汤姆·勒·佩恩、斯里瓦茨·斯里尼瓦桑、克谢尼娅·科纽什科娃、洛特·维尔茨、阿比谢克·夏尔马、阿迪提亚·西丹特、亚历克斯·阿赫恩、苗森·王、陈杰·顾等。2023。语言建模的强化自训练（rest）。*arXiv
    预印本 arXiv:2308.08998*。
- en: 'He et al. (2021) Xuanli He, Islam Nassar, Jamie Ryan Kiros, Gholamreza Haffari,
    and Mohammad Norouzi. 2021. Generate, annotate, and learn: Generative models advance
    self-training and knowledge distillation.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2021）何旋力、伊斯兰·纳萨尔、杰米·瑞安·基罗斯、戈拉姆雷扎·哈法里和穆罕默德·诺鲁兹。2021。生成、注释和学习：生成模型推动自训练和知识蒸馏。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015） Geoffrey Hinton、Oriol Vinyals 和 Jeff Dean。2015。蒸馏神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等（2023）程昱、李春亮、叶智宽、霍坦·纳科斯特、藤井靖久、亚历山大·拉特纳、兰杰·克里希纳、李晨雨和托马斯·菲斯特。2023。逐步蒸馏！用更少的训练数据和更小的模型超越更大的语言模型。*arXiv
    预印本 arXiv:2305.02301*。
- en: Huang et al. (2019) Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes
    Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. 2019. Reducing
    sentiment bias in language models via counterfactual evaluation. *arXiv preprint
    arXiv:1911.03064*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2019）黄泊森、张欢、姜睿、罗伯特·斯坦福斯、约翰内斯·维尔布尔、杰克·雷、维沙尔·迈尼、丹尼·瑜伽塔马和普什米特·科赫利。2019。通过反事实评估减少语言模型中的情感偏差。*arXiv
    预印本 arXiv:1911.03064*。
- en: Kaushik et al. (2019) Divyansh Kaushik, Eduard Hovy, and Zachary C Lipton. 2019.
    Learning the difference that makes a difference with counterfactually-augmented
    data. *arXiv preprint arXiv:1909.12434*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaushik 等（2019）迪维扬什·考什克、爱德华·霍维和扎卡里·C·利普顿。2019。利用反事实增强数据学习改变的差异。*arXiv 预印本 arXiv:1909.12434*。
- en: Kim and Rush (2016) Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge
    distillation. *arXiv preprint arXiv:1606.07947*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Rush（2016）金允和亚历山大·M·拉什。2016。序列级知识蒸馏。*arXiv 预印本 arXiv:1606.07947*。
- en: Latif et al. (2023) Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai. 2023.
    Knowledge distillation of llm for education. *arXiv preprint arXiv:2312.15842*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 等（2023）伊赫桑·拉提夫、吕阳·方、马平和夏明。2023。用于教育的 LLM 知识蒸馏。*arXiv 预印本 arXiv:2312.15842*。
- en: 'Qian et al. (2021) Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and Pengjun
    Xie. 2021. Counterfactual inference for text classification debiasing. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*, pages 5434–5445.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2021）钱晨、冯福力、温立杰、马春平和谢鹏俊。2021。用于文本分类去偏的反事实推理。在*第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长论文）*会议记录中，第5434-5445页。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2023）拉斐尔·拉斐洛夫、阿基特·夏尔马、埃里克·米切尔、斯特凡诺·厄蒙、克里斯托弗·D·曼宁和切尔西·芬。2023。直接偏好优化：你的语言模型实际上是一个奖励模型。*arXiv
    预印本 arXiv:2305.18290*。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter. *arXiv preprint arXiv:1910.01108*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2019）维克托·桑、莉桑德·德布、朱利安·肖蒙和托马斯·沃尔夫。2019。Distilbert，一个 BERT 的蒸馏版本：更小、更快、更便宜、更轻便。*arXiv
    预印本 arXiv:1910.01108*。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, 和 Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    预印本 arXiv:1707.06347*。
- en: 'Singh et al. (2023) Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand,
    Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi,
    et al. 2023. Beyond human data: Scaling self-training for problem-solving with
    language models. *arXiv preprint arXiv:2312.06585*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh et al. (2023) Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand,
    Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi,
    等人. 2023. Beyond human data: Scaling self-training for problem-solving with language
    models. *arXiv 预印本 arXiv:2312.06585*。'
- en: 'Xu et al. (2023) Weizhi Xu, Qiang Liu, Shu Wu, and Liang Wang. 2023. Counterfactual
    debiasing for fact verification. In *Proceedings of the 61st Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    6777–6789.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) Weizhi Xu, Qiang Liu, Shu Wu, 和 Liang Wang. 2023. Counterfactual
    debiasing for fact verification. 收录于 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，页码 6777–6789。
- en: 'Zelikman et al. (2022) Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony
    Wu. 2022. Star: Self-taught reasoner bootstrapping reasoning with reasoning.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zelikman et al. (2022) Eric Zelikman, Jesse Mu, Noah D Goodman, 和 Yuhuai Tony
    Wu. 2022. Star: Self-taught reasoner bootstrapping reasoning with reasoning.'
- en: Appendix A Evaluation
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 评估
- en: 'We evaluated the performance of the models using accuracy. For the Math problems,
    we developed a correctness verification script designed to determine whether the
    final answer provided by the model corresponds with the final answer in the gold
    labels. For the coding task, we utilized the original evaluation script provided
    by HumanEval. For GSM8K, MATH and HumanEval datasets, in cases where the gold
    labels are not readily amenable to evaluation by the correctness verification
    script, we modify the gold labels to ensure they can be easily assessed. Specifically,
    if the original target label does not present the answer in a format that the
    script can straightforwardly evaluate, we adapt the label by appending the final
    answer at the end, preceded by the keyword "Final Answer:". For instance, if the
    original target label states, "2 people have 4 eyes. Thus, there are 4 eyes in
    the 2 people group," we instruct GPT-4 to modify it to "2 people have 4 eyes.
    Thus, there are 4 eyes in the 2 people group. Final Answer: 4 eyes." This approach
    allows the correctness verification script to identify the keyword ’Final Answer:’
    and extract the numerical answer that follows for verifying the correctness. By
    training the model with data that consistently places the final answer after the
    ’Final Answer:’ keyword, we ensure the model learns to format its responses in
    a way that aligns with the verification script’s requirements, thereby enhancing
    the reliability of the correctness verification process.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用准确率评估模型的性能。对于数学问题，我们开发了一个正确性验证脚本，用于确定模型提供的最终答案是否与金标准标签中的最终答案一致。对于编码任务，我们使用了
    HumanEval 提供的原始评估脚本。对于 GSM8K、MATH 和 HumanEval 数据集，在金标准标签不易通过正确性验证脚本评估的情况下，我们会修改金标准标签以确保它们可以被轻松评估。具体而言，如果原始目标标签未以脚本可以直接评估的格式呈现答案，我们通过在末尾附加最终答案，并在其前面添加关键词“Final
    Answer:”，来调整标签。例如，如果原始目标标签说明“2 people have 4 eyes. Thus, there are 4 eyes in the
    2 people group”，我们指示 GPT-4 将其修改为“2 people have 4 eyes. Thus, there are 4 eyes
    in the 2 people group. Final Answer: 4 eyes。”这种方法允许正确性验证脚本识别关键词“Final Answer:”并提取其后的数字答案进行验证。通过训练模型在“Final
    Answer:”关键词后始终放置最终答案，我们确保模型学会以符合验证脚本要求的格式进行响应，从而提高正确性验证过程的可靠性。'
- en: To assess zero-shot learning, we designed prompts to ensure that the llama2-13b-chat
    model always positions the final answer at the end, following the keyword ’Answer:’.
    We manually checked the accuracy of this script against the first 100 zero-shot
    predictions across MATH Algebra, MATH Counting and Probability, and GSM8K datasets.
    The scripts were error-free.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估零样本学习，我们设计了提示，以确保 llama2-13b-chat 模型始终将最终答案放在末尾，跟在关键词“Answer:”之后。我们手动检查了该脚本在
    MATH Algebra、MATH Counting and Probability 和 GSM8K 数据集上的前 100 个零样本预测的准确性。脚本没有错误。
- en: Zero-shot performance for the coding task on the HumanEval dataset was 0\. This
    is because the official testing evaluation script is designed to place the code
    prediction beneath the function name and execute it. If the model’s output includes
    the function definition, then the script fails. For example, for a task requiring
    the model to calculate the sum of 2 numbers with the entry point ’def summation(a,
    b):’, the model should output ’ return a+b’ rather than ’def summation(a, b):$\backslash$n
    return a+b’. This requirement proved challenging for the model to follow despite
    numerous attempts with various prompts and detailed examples. Regardless of our
    efforts, the model consistently failed zero-shot testing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HumanEval 数据集上的编码任务的零-shot 性能为 0。原因是官方测试评估脚本设计为将代码预测放在函数名称下方并执行它。如果模型的输出包括函数定义，则脚本会失败。例如，对于一个要求模型计算
    2 个数字之和的任务，入口点为 ’def summation(a, b):’，模型应该输出 ’ return a+b’，而不是 ’def summation(a,
    b):$\backslash$n return a+b’。这一要求对模型来说证明是具有挑战性的，尽管进行了多次尝试和详细示例。尽管我们做出了努力，模型在零-shot
    测试中始终失败。
- en: Appendix B Error Correction Data Example
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 错误纠正数据示例
- en: This is the work from from Learning From Mistakes Makes LLM Better Reasoner
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自《从错误中学习使 LLM 成为更好的推理者》的工作。
- en: '![Refer to caption](img/a9120dab155349913a96a0986cc23e0c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a9120dab155349913a96a0986cc23e0c.png)'
- en: 'Figure 4: Error Correction Data Training Data Example An et al. ([2023](#bib.bib1))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：错误纠正数据训练数据示例 An 等人（[2023](#bib.bib1)）
- en: '![Refer to caption](img/d1955261b1234173891f2d2476a23a44.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d1955261b1234173891f2d2476a23a44.png)'
- en: 'Figure 5: A Minimum Change prompt guides GPT-4 to minimally adjust target labels.
    Each example includes a question, prior prediction, and correct answer, alongside
    explanations for each change. GPT-4 is then given the previous prediction and
    instructed to modify it, aligning with the provided ground truth and question.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：最小变更提示引导 GPT-4 最小化地调整目标标签。每个示例包括一个问题、先前的预测和正确答案，以及每个变更的解释。然后，GPT-4 会得到先前的预测，并被指示进行修改，以符合提供的真实情况和问题。
- en: '![Refer to caption](img/f095800e5408b2382a9bdc14b65c1dcc.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f095800e5408b2382a9bdc14b65c1dcc.png)'
- en: 'Figure 6: We analyzed the first three training data examples from the datasets
    used in the Learn from Mistakes project An et al. ([2023](#bib.bib1)), generated
    through the author-provided prompt. A line-by-line review reveals that corrected
    answers 1 and 3 deviate in text style from the original predictions. Only the
    second corrected answer shows some stylistic similarities with the original answer,
    yet it still includes numerous words from GPT-4 that the original model may not
    align with the internal text style preference of the original model.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：我们分析了 Learn from Mistakes 项目 An 等人（[2023](#bib.bib1)）使用的数据集中前面三个训练数据示例，这些数据是通过作者提供的提示生成的。逐行审查表明，纠正后的答案
    1 和 3 在文本风格上与原始预测存在偏差。只有第二个纠正答案与原始答案有一些风格上的相似性，但它仍包含 GPT-4 的多个单词，这些单词可能与原始模型的内部文本风格偏好不一致。
- en: Appendix C AI Tools
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C AI 工具
- en: All of the content is edited by ChatGPT.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所有内容均由 ChatGPT 编辑。
