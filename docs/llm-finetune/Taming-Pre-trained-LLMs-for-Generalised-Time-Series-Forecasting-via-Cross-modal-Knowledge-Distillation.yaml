- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用跨模态知识蒸馏对预训练LLMs进行广泛时间序列预测
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.07300](https://ar5iv.labs.arxiv.org/html/2403.07300)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.07300](https://ar5iv.labs.arxiv.org/html/2403.07300)
- en: 'Peiyuan Liu^(1,) Equal Contribution    Hang Guo^(1,)¹¹footnotemark: 1    Tao
    Dai^(2,) Correspondence to: Tao Dai and Naiqi Li    Naiqi Li^(1,)²²footnotemark:
    2    Jigang Bao¹    Xudong Ren¹    Yong Jiang¹&Shu-Tao Xia¹ ¹Tsinghua Shenzhen
    International Graduate School, Tsinghua University'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Peiyuan Liu^(1,) 贡献相等    Hang Guo^(1,)¹¹脚注标记：1    Tao Dai^(2,) 通讯作者：Tao Dai
    和 Naiqi Li    Naiqi Li^(1,)²²脚注标记：2    Jigang Bao¹    Xudong Ren¹    Yong Jiang¹&Shu-Tao
    Xia¹ ¹清华大学深圳国际研究生院
- en: ²College of Computer Science and Software Engineering, Shenzhen University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²深圳大学计算机科学与软件工程学院
- en: '{peiyuanliu.edu, cshguo, daitao.edu, linaiqi.thu}@google.com, {baojg19, rxd21}@mails.tsinghua.edu.cn,
    {jiangy, xiast}@sz.tsinghua.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{peiyuanliu.edu, cshguo, daitao.edu, linaiqi.thu}@google.com, {baojg19, rxd21}@mails.tsinghua.edu.cn,
    {jiangy, xiast}@sz.tsinghua.edu.cn'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Multivariate time series forecasting has recently gained great success with
    the rapid growth of deep learning models. However, existing approaches usually
    train models from scratch using limited temporal data, preventing their generalization.
    Recently, with the surge of the Large Language Models (LLMs), several works have
    attempted to introduce LLMs into time series forecasting. Despite promising results,
    these methods directly take time series as the input to LLMs, ignoring the inherent
    modality gap between temporal and text data. In this work, we propose a novel
    Large Language Models and Time series Alignment framework, dubbed LLaTA, to fully
    unleash the potentials of LLMs in the time series forecasting challenge. Based
    on cross-modal knowledge distillation, the proposed method exploits both input-agnostic
    static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In
    this way, it empowers the forecasting model with favorable performance as well
    as strong generalization abilities. Extensive experiments demonstrate the proposed
    method establishes a new state of the art for both long- and short-term forecasting.
    Code is available at [https://github.com/Hank0626/LLaTA](https://github.com/Hank0626/LLaTA).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量时间序列预测最近随着深度学习模型的快速发展取得了巨大成功。然而，现有方法通常从头开始训练模型，使用的时间数据有限，阻碍了模型的泛化能力。最近，随着大型语言模型（LLMs）的兴起，一些研究尝试将LLMs引入时间序列预测中。尽管取得了有希望的结果，但这些方法直接将时间序列作为LLMs的输入，忽视了时间数据和文本数据之间固有的模态差距。在这项工作中，我们提出了一个新颖的大型语言模型与时间序列对齐框架，称为LLaTA，以充分释放LLMs在时间序列预测挑战中的潜力。基于跨模态知识蒸馏，提出的方法利用了预训练LLMs中的输入无关静态知识和输入相关动态知识。这样，它赋予预测模型优异的性能以及强大的泛化能力。大量实验表明，提出的方法在长期和短期预测中都建立了新的最先进水平。代码可以在[https://github.com/Hank0626/LLaTA](https://github.com/Hank0626/LLaTA)找到。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Multivariate time series forecasting is an important task in time series analysis
    and plays an essential role in various real-world applications, such as forecasting
    on weather (Angryk et al., [2020](#bib.bib2)), energy (Demirel et al., [2012](#bib.bib12)),
    finance (Patton, [2013](#bib.bib27)), etc. To this end, numerous effective deep
    learning methods have been proposed in recent years (Wu et al., [2023](#bib.bib34);
    Zhang and Yan, [2023](#bib.bib37); Nie et al., [2023](#bib.bib25); Zeng et al.,
    [2023](#bib.bib36); Das et al., [2023](#bib.bib11); Wang et al., [2022](#bib.bib31);
    Dai et al., [2024](#bib.bib10)). Although existing methods have achieved promising
    results, they usually learn from scratch with scarce time series data, which in
    turn hampers their broader applicability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量时间序列预测是时间序列分析中的一项重要任务，在各种现实应用中扮演着重要角色，例如天气预测（Angryk等，[2020](#bib.bib2)），能源（Demirel等，[2012](#bib.bib12)），金融（Patton，[2013](#bib.bib27)）等。为此，近年来提出了许多有效的深度学习方法（Wu等，[2023](#bib.bib34)；Zhang和Yan，[2023](#bib.bib37)；Nie等，[2023](#bib.bib25)；Zeng等，[2023](#bib.bib36)；Das等，[2023](#bib.bib11)；Wang等，[2022](#bib.bib31)；Dai等，[2024](#bib.bib10)）。尽管现有方法已取得有希望的结果，但它们通常从头开始学习，数据稀少，这限制了它们的更广泛适用性。
- en: Recently, pre-trained Large Language Models (LLMs) have been introduced to time
    series analysis. Due to the large-scale pre-training, LLMs offer time series forecasting
    models with strong context modeling ability and also alleviate the data insufficiency
    challenge. For example, (Zhou et al., [2023](#bib.bib39)) proposed a unified time
    series analysis framework by adapting and fine-tuning LLMs. Building upon this,
    other works introduced additional enhancements to further refine and expand the
    capabilities of LLMs in time series forecasting, including refined fine-tuning
    methods (Chang et al., [2023](#bib.bib7)), sequence decomposition (Cao et al.,
    [2023](#bib.bib5)), and the incorporation of textual prompts (Jin et al., [2023a](#bib.bib18)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，预训练的大型语言模型（LLMs）被引入到时间序列分析中。由于大规模的预训练，LLMs为时间序列预测模型提供了强大的上下文建模能力，同时也缓解了数据不足的挑战。例如，（Zhou
    et al., [2023](#bib.bib39)）通过调整和微调LLMs提出了一个统一的时间序列分析框架。在此基础上，其他工作引入了额外的增强方法，以进一步细化和扩展LLMs在时间序列预测中的能力，包括改进的微调方法（Chang
    et al., [2023](#bib.bib7)），序列分解（Cao et al., [2023](#bib.bib5)）以及文本提示的融合（Jin et
    al., [2023a](#bib.bib18)）。
- en: 'Despite the promising results, most existing LLM-powered methods typically
    treat the pre-trained LLM as a well-initialized forecasting model, and directly
    use the time series data as the input. This straightforward way can lead to modality
    misalignment between pre-training and downstream tasks. As shown in [Fig. 1](#S1.F1
    "In 1 Introduction ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting
    via Cross-modal Knowledge Distillation"), the temporal tokens modeled by the current
    method can not align well with the word vectors of the pre-trained LLMs, which
    constrains the ability of LLMs for time series forecasting. Based on this observation,
    we ask a novel question: Is there a reasonable manner to narrow the modality gap
    between time series and language to further enhance the capability of LLMs-based
    time series forecasting models?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果颇具前景，大多数现有的基于LLM的方法通常将预训练的LLM视为一个初始化良好的预测模型，并直接将时间序列数据作为输入。这种简单的方法可能导致预训练与下游任务之间的模态不匹配。如[图
    1](#S1.F1 "在 1 引言 ‣ 通过跨模态知识蒸馏驯服预训练LLM进行通用时间序列预测")所示，当前方法建模的时间标记与预训练LLM的词向量无法很好对齐，这限制了LLM在时间序列预测中的能力。基于这一观察，我们提出了一个新问题：是否存在合理的方法来缩小时间序列和语言之间的模态差距，从而进一步增强基于LLM的时间序列预测模型的能力？
- en: '![Refer to caption](img/6ca987ed30a0fd06baefdfe5b4f7d30f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6ca987ed30a0fd06baefdfe5b4f7d30f.png)'
- en: 'Figure 1: The t-SNE visualization of LLM input tokens, featuring pre-trained
    word tokens with temporal tokens of GPT4TS (Left) (Zhou et al., [2023](#bib.bib39))
    and our method (Right). The integration of temporal tokens with pre-trained word
    tokens is notably more cohesive in our method, indicating effective modality alignment.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM输入标记的t-SNE可视化，展示了预训练的词标记与GPT4TS（左）（Zhou et al., [2023](#bib.bib39)）和我们的方法（右）的时间标记的对比。我们的方法中时间标记与预训练词标记的整合显著更为协调，表明了有效的模态对齐。
- en: The key to answering the above question is to find an effective way to mitigate
    modality differences. Recently, cross-modal knowledge distillation has been proposed
    to achieve cross-model knowledge transfer. Some works have been proposed and achieved
    favorable performance in the domains of semantic segmentation (Vobecky et al.,
    [2022](#bib.bib30)), action detection (Dai et al., [2021](#bib.bib9)), speaker
    recognition (Jin et al., [2023b](#bib.bib19)), etc. However, existing advancements
    mainly focus on computer vision or natural language processing, while cross-modal
    knowledge distillation for temporal modality is still under-explored.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解答上述问题的关键在于找到有效的方法来减轻模态差异。最近，跨模态知识蒸馏已被提出以实现跨模型知识转移。一些工作已经提出并在语义分割（Vobecky et
    al., [2022](#bib.bib30)），动作检测（Dai et al., [2021](#bib.bib9)），说话人识别（Jin et al.,
    [2023b](#bib.bib19)）等领域取得了良好的效果。然而，现有的进展主要集中在计算机视觉或自然语言处理领域，而针对时间模态的跨模态知识蒸馏仍然研究不足。
- en: 'In this work, we propose LLaTA, a novel cross-modal knowledge distillation
    architecture, to overcome modality misalignment by distilling knowledge from pre-trained
    LLMs. Specifically, temporal modal tokens are projected into the same latent space
    of the textual modal tokens, and then knowledge distillation is used to align
    the two modalities. Therefore, the proposed LLaTA consists of two branches: (a)
    the temporal modal branch and (b) the textual modal branch. The former is used
    for processing time series information, and the latter is used to extract and
    adapt knowledge from pre-trained LLMs using aligned textual modal tokens. To ensure
    high-quality transfer, we consider two types of knowledge: input-agnostic static
    knowledge contained in frozen word embedding layers, and input-dependent dynamic
    knowledge encapsulated in forward parameters. The static knowledge is adequately
    exploited with the proposed principle word embedding extraction as well as the
    cross-attention mechanism, while the dynamic knowledge captures the modal consistency
    with the help of feature regularization loss and modal consistency loss.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 LLaTA，这是一种新颖的跨模态知识蒸馏架构，通过从预训练的 LLMs 中蒸馏知识以克服模态不对齐问题。具体而言，时间模态令牌被投影到与文本模态令牌相同的潜在空间中，然后使用知识蒸馏来对齐这两种模态。因此，所提出的
    LLaTA 包括两个分支：(a) 时间模态分支和 (b) 文本模态分支。前者用于处理时间序列信息，后者用于从预训练的 LLMs 中提取和调整知识，使用对齐的文本模态令牌。为了确保高质量的转移，我们考虑了两种类型的知识：包含在冻结的词嵌入层中的输入无关静态知识，以及封装在前向参数中的输入依赖动态知识。静态知识通过所提出的原理词嵌入提取和交叉注意力机制得到充分利用，而动态知识通过特征正则化损失和模态一致性损失捕捉模态一致性。
- en: 'In a nutshell, the contributions of this paper are threefold:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文的贡献有三方面：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce LLaTA, a novel cross-modal knowledge distillation framework to
    exploit the power of LLMs in time series forecasting. To the best of our knowledge,
    it is the first work attempting to address the modal-misalignment problem with
    knowledge distillation.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 LLaTA，这是一种新颖的跨模态知识蒸馏框架，用于发挥大型语言模型在时间序列预测中的作用。据我们所知，这是第一个尝试通过知识蒸馏解决模态不对齐问题的工作。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We exploit both static and dynamic knowledge in the pre-trained LLMs and introduce
    well-designed modules/losses to adequately facilitate knowledge transfer between
    different modalities.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们利用了预训练 LLMs 中的静态和动态知识，并引入了精心设计的模块/损失，以充分促进不同模态之间的知识转移。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments on eight real-world datasets demonstrate that LLaTA achieves
    state-of-the-art performance on both long- and short-term time series forecasting
    tasks, with favorable generalization ability.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在八个真实数据集上的大量实验表明，LLaTA 在长短期时间序列预测任务中均实现了最先进的性能，并具有良好的泛化能力。
- en: '![Refer to caption](img/dbe198218955ecf0d356e51e188a8e8b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/dbe198218955ecf0d356e51e188a8e8b.png)'
- en: 'Figure 2: An overview of the proposed cross-modal distillation framework. The
    above is the textual modal branch, and the below is the temporal modal branch.
    Both static and dynamic knowledge are utilized to adapt LLMs for time series analysis.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：所提出的跨模态蒸馏框架概述。上方为文本模态分支，下方为时间模态分支。静态和动态知识均用于将 LLMs 调整为时间序列分析。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Time Series Forecasting
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 时间序列预测
- en: In recent years, deep learning has significantly revolutionized the field of
    time series forecasting, with a plethora of methods emerging to enhance predictive
    accuracy (Zeng et al., [2023](#bib.bib36); Das et al., [2023](#bib.bib11); Wu
    et al., [2023](#bib.bib34); Wang et al., [2022](#bib.bib31)). Among these, Transformer-based
    models have emerged as the frontrunners, offering unparalleled performance due
    to their exceptional ability to model complex dependencies in data  (Nie et al.,
    [2023](#bib.bib25); Zhang and Yan, [2023](#bib.bib37); Woo et al., [2022](#bib.bib32);
    Wu et al., [2021](#bib.bib33); Zhou et al., [2022](#bib.bib38)). However, they
    often have limitations due to the scarcity of training data and the necessity
    for intricate architectural designs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习在时间序列预测领域取得了重大突破，出现了大量方法以提高预测准确性 (Zeng et al., [2023](#bib.bib36); Das
    et al., [2023](#bib.bib11); Wu et al., [2023](#bib.bib34); Wang et al., [2022](#bib.bib31))。其中，基于
    Transformer 的模型因其卓越的数据复杂依赖建模能力而成为领先者 (Nie et al., [2023](#bib.bib25); Zhang and
    Yan, [2023](#bib.bib37); Woo et al., [2022](#bib.bib32); Wu et al., [2021](#bib.bib33);
    Zhou et al., [2022](#bib.bib38))。然而，由于训练数据稀缺和复杂的架构设计需求，它们往往存在局限性。
- en: In response to these challenges, the integration of LLMs into time series forecasting
    has emerged as a novel and promising direction. This approach leverages the extensive
    pre-training of LLMs to enhance the context-modeling capacity in time series analysis.
    A groundbreaking framework proposed by (Zhou et al., [2023](#bib.bib39)) first
    demonstrated the potential of adapting LLMs for time series analysis. Following
    this paradigm, subsequent research has introduced further refinements and innovations.
    For example, (Chang et al., [2023](#bib.bib7)) introduced a novel two-stage fine-tuning
    method and integrated time-series patching with additional temporal encoding into
    pre-trained LLMs. (Cao et al., [2023](#bib.bib5)) incorporated decomposition of
    time series and selection-based prompts for adapting to non-stationary data. (Jin
    et al., [2023a](#bib.bib18)) reprograms time series input with text prototypes
    and enriches it using context as a prefix for LLM alignment. However, these works
    directly input time series data into LLMs, overlooking the misalignment between
    time series and textual modalities.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，将大语言模型（LLMs）整合到时间序列预测中，已经成为一种新颖且有前景的方向。这种方法利用LLMs的广泛预训练来增强时间序列分析中的上下文建模能力。由（Zhou
    et al., [2023](#bib.bib39)）提出的一个开创性框架首次展示了将LLMs应用于时间序列分析的潜力。遵循这一范式，后续研究引入了进一步的改进和创新。例如，（Chang
    et al., [2023](#bib.bib7)）提出了一种新颖的两阶段微调方法，并将时间序列补丁与额外的时间编码集成到预训练的LLMs中。（Cao et
    al., [2023](#bib.bib5)）结合了时间序列分解和基于选择的提示，以适应非平稳数据。（Jin et al., [2023a](#bib.bib18)）用文本原型重新编程时间序列输入，并利用上下文作为LLM对齐的前缀来丰富数据。然而，这些工作直接将时间序列数据输入到LLMs中，忽视了时间序列与文本模态之间的错配。
- en: 2.2 Knowledge Distillation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识蒸馏
- en: Knowledge distillation was first introduced by  (Hinton et al., [2015](#bib.bib15))
    to achieve model compression. Since then, this technique has been used to improve
    performance and efficiency across various tasks (Gou et al., [2021](#bib.bib13)),
    including transfer learning (Xu et al., [2020](#bib.bib35)), question answering (Hu
    et al., [2018](#bib.bib16)), text recognition (Guo et al., [2023](#bib.bib14))
    and so on. Recently, a few works (Dai et al., [2021](#bib.bib9); Jin et al., [2023b](#bib.bib19))
    have been proposed to address the cross-modal misalignment problem with knowledge
    distillation. However, these works do not focus on time series analysis tasks.
    Moreover, to the best of our knowledge, very little work has been done to investigate
    the solution to pre-trained LLMs cross-modal misalignment using knowledge distillation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏最早由（Hinton et al., [2015](#bib.bib15)）引入，以实现模型压缩。从那时起，这一技术被用于提高各种任务的性能和效率（Gou
    et al., [2021](#bib.bib13)），包括迁移学习（Xu et al., [2020](#bib.bib35)），问答系统（Hu et al.,
    [2018](#bib.bib16)），文本识别（Guo et al., [2023](#bib.bib14)）等。最近，一些研究（Dai et al.,
    [2021](#bib.bib9)；Jin et al., [2023b](#bib.bib19)）提出了用知识蒸馏解决跨模态错配问题的方法。然而，这些工作并不专注于时间序列分析任务。此外，据我们了解，目前几乎没有研究探讨使用知识蒸馏解决预训练LLMs的跨模态错配问题。
- en: 3 Methodology
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'As shown in [Fig. 2](#S1.F2 "In 1 Introduction ‣ Taming Pre-trained LLMs for
    Generalised Time Series Forecasting via Cross-modal Knowledge Distillation"),
    we propose a novel knowledge distillation framework to transfer the knowledge
    from pre-trained LLMs for time series forecasting. The proposed framework consists
    of two branches: the textual modal branch and the temporal modal branch. In concrete,
    the textual modal branch takes the aligned text tokens $X_{text}$. A task-specific
    head is used to generate the output $Y_{text}$. The output of this branch is denoted
    as $Y_{time}$. During training, both static and dynamic knowledge from the textual
    modal branch is exploited to help the temporal branch achieve favorable performance
    on time series forecasting tasks.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 2](#S1.F2 "在 1 引言 ‣ 通过跨模态知识蒸馏驯化预训练 LLMs 以进行通用时间序列预测")所示，我们提出了一种新颖的知识蒸馏框架，用于将预训练LLMs的知识转移到时间序列预测中。该框架由两个分支组成：文本模态分支和时间模态分支。具体而言，文本模态分支接受对齐的文本标记$X_{text}$。使用任务特定的头部生成输出$Y_{text}$。该分支的输出表示为$Y_{time}$。在训练过程中，利用文本模态分支中的静态和动态知识来帮助时间模态分支在时间序列预测任务中获得良好的表现。
- en: 3.1 Static Knowledge Learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 静态知识学习
- en: As demonstrated in previous work (Mikolov et al., [2013](#bib.bib24)), the matrices
    of word embedding layers in pre-trained LLMs constitute a well-structured context
    representation space, e.g., semantic distances between different words can be
    quantized by vector similarity. We refer to this type of information as static
    knowledge, since this knowledge is input-agnostic. Despite this promising property,
    this static knowledge is usually neglected by previous LLMs-based time series
    methods (Zhou et al., [2023](#bib.bib39); Cao et al., [2023](#bib.bib5); Chang
    et al., [2023](#bib.bib7)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如先前的研究所示 (Mikolov et al., [2013](#bib.bib24))，预训练LLMs中的词嵌入层矩阵构成了一个结构良好的上下文表示空间，例如，不同词之间的语义距离可以通过向量相似性进行量化。我们将这种类型的信息称为静态知识，因为这种知识与输入无关。尽管有这一有前景的特性，之前的LLMs基础时间序列方法通常忽略了这种静态知识
    (Zhou et al., [2023](#bib.bib39); Cao et al., [2023](#bib.bib5); Chang et al.,
    [2023](#bib.bib7))。
- en: 'In this work, we attempt to exploit the static knowledge in pre-trained LLMs.
    The main challenge lies in the significant modal discrepancy between text and
    temporal series, which will lead to suboptimal results if we directly employ original
    word embedding as the input for the textual modal branch. For this reason, we
    propose a static knowledge distillation strategy to deal with this problem. Specifically,
    given a multivariate time series $I\in\mathbb{R}^{T\times C}$:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们尝试利用预训练LLMs中的静态知识。主要挑战在于文本与时间序列之间的显著模态差异，这将导致如果直接将原始词嵌入作为文本模态分支的输入，结果将会次优。因此，我们提出了一种静态知识蒸馏策略来解决这个问题。具体而言，给定一个多变量时间序列
    $I\in\mathbb{R}^{T\times C}$：
- en: '|  | $X_{time}={\rm MHSA(Embedding}(I))\in\mathbb{R}^{C\times M},$ |  | (1)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $X_{time}={\rm MHSA(Embedding}(I))\in\mathbb{R}^{C\times M},$ |  | (1)
    |'
- en: where $M$ is the feature dimension of pre-trained LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M$ 是预训练LLMs的特征维度。
- en: After that, we consider using cross attention to align $X_{time}$ is usually
    huge, e.g., 50257 in GPT2 (Radford et al., [2019](#bib.bib28)), directly using
    cross attention will incur a significant cost. Observing that semantic-similar
    words will form “synonym clusters”, we thus propose a principal word embedding
    extraction strategy, which uses the cluster center to represent its surrounding
    words, to reduce the number of word entries. Specifically, we use Principal Component
    Analysis (PCA) to perform dimension reduction on $\mathcal{D}$,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们考虑使用交叉注意力对齐 $X_{time}$ 通常是巨大的，例如，在GPT2中为50257 (Radford et al., [2019](#bib.bib28))，直接使用交叉注意力会产生显著的成本。观察到语义相似的词会形成“同义词簇”，因此我们提出了一种主词嵌入提取策略，该策略使用簇中心表示其周围的词，以减少词条数量。具体而言，我们使用主成分分析（PCA）对
    $\mathcal{D}$ 进行降维，
- en: '|  | $\hat{\mathcal{D}}={\rm PCA}(\mathcal{D}),$ |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{D}}={\rm PCA}(\mathcal{D}),$ |  | (2) |'
- en: where $d$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$。
- en: It is worth noting that this process needs to be done only once before model
    training and does not incur much training overhead. We then use Multi-head Cross
    Attention with $\hat{\mathcal{D}}$,
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这一过程只需在模型训练前执行一次，并不会带来太多的训练开销。然后我们使用多头交叉注意力与 $\hat{\mathcal{D}}$，
- en: '|  |  | $\displaystyle X_{text}={\rm Softmax}(\frac{QK^{T}}{\sqrt{C}})V,$ |  |
    (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle X_{text}={\rm Softmax}(\frac{QK^{T}}{\sqrt{C}})V,$ |  |
    (3) |'
- en: '|  |  | $\displaystyle Q=X_{time}W_{q},K=\hat{\mathcal{D}}W_{k},V=\hat{\mathcal{D}}W_{v},$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle Q=X_{time}W_{q},K=\hat{\mathcal{D}}W_{k},V=\hat{\mathcal{D}}W_{v},$
    |  |'
- en: where $W_{q}$), key ($K$), respectively.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{q}$）、键 ($K$)。
- en: 3.2 Dynamic Knowledge Learning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 动态知识学习
- en: 'Pre-trained LLMs have powerful capability of modeling dynamic contexts with
    different inputs, and we refer to this input-specific feature from the textual
    modal branch as dynamic knowledge. We propose two distillation losses to migrate
    the pre-trained dynamic knowledge:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练LLMs具有强大的动态上下文建模能力，我们将来自文本模态分支的这一特定输入特征称为动态知识。我们提出了两个蒸馏损失来迁移预训练的动态知识：
- en: Feature Regularization Loss.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征正则化损失。
- en: 'We first transfer the dynamic knowledge from the hidden text feature $F^{l}_{text}$-th
    Transformer block in the text-modal and time-modal branches, respectively, the
    feature regularization loss is defined as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从文本模态和时间模态分支的隐藏文本特征 $F^{l}_{text}$-th Transformer块中传递动态知识，特征正则化损失定义为：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $\gamma$ and $\phi^{text}_{l}(\cdot)$ to transform the features from temporal
    and textual modalities to the shared representation space.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 和 $\phi^{text}_{l}(\cdot)$ 将来自时间和文本模态的特征转换到共享表示空间。
- en: Modal Consistency Loss.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模态一致性损失。
- en: 'We further add constraints to the output layer to supervise the learning of
    the temporal modal branch. Specifically, given the outputs $Y_{time}$ from the
    temporal modal branch and textual modal branch respectively, we use the following
    loss to ensure that the output is consistent across modalities:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步对输出层添加约束以监督时间模态分支的学习。具体来说，给定来自时间模态分支和文本模态分支的输出 $Y_{time}$，我们使用以下损失函数以确保跨模态输出的一致性：
- en: '|  | $\mathcal{L}_{{output}}={\rm sim}({Y}_{{text}},Y_{time}),$ |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{{output}}={\rm sim}({Y}_{{text}},Y_{time}),$ |  | (5) |'
- en: where ${\rm sim}(\cdot,\cdot)$ is a chosen similarity function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '| 其中 ${\rm sim}(\cdot,\cdot)$ 是选定的相似度函数。 |'
- en: '| Models | LLaTA | GPT4TS | PatchTST | Crossformer | FEDformer | TimesNet |
    MICN | DLinear | TiDE |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaTA | GPT4TS | PatchTST | Crossformer | FEDformer | TimesNet | MICN
    | DLinear | TiDE |'
- en: '|  (Ours)  |  ([2023](#bib.bib39))  |  ([2023](#bib.bib25))  |  ([2023](#bib.bib37))  |  ([2022](#bib.bib38))  |  ([2023](#bib.bib34))  |  ([2022](#bib.bib31))  |  ([2023](#bib.bib36))  |  ([2023](#bib.bib11))  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  (我们的)  |  ([2023](#bib.bib39))  |  ([2023](#bib.bib25))  |  ([2023](#bib.bib37))  |  ([2022](#bib.bib38))  |  ([2023](#bib.bib34))  |  ([2022](#bib.bib31))  |  ([2023](#bib.bib36))  |  ([2023](#bib.bib11))  |'
- en: '| Metric | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE
    | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 评价指标 | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE |
    MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
- en: '| ETTm1 | 96 | 0.323 | 0.349 | 0.329 | 0.364 | 0.321 | 0.360 | 0.360 | 0.401
    | 0.379 | 0.419 | 0.338 | 0.375 | 0.316 | 0.362 | 0.345 | 0.372 | 0.352 | 0.373
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ETTm1 | 96 | 0.323 | 0.349 | 0.329 | 0.364 | 0.321 | 0.360 | 0.360 | 0.401
    | 0.379 | 0.419 | 0.338 | 0.375 | 0.316 | 0.362 | 0.345 | 0.372 | 0.352 | 0.373
    |'
- en: '| 192 | 0.374 | 0.375 | 0.368 | 0.382 | 0.362 | 0.384 | 0.402 | 0.440 | 0.426
    | 0.441 | 0.374 | 0.387 | 0.363 | 0.390 | 0.380 | 0.389 | 0.389 | 0.391 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.374 | 0.375 | 0.368 | 0.382 | 0.362 | 0.384 | 0.402 | 0.440 | 0.426
    | 0.441 | 0.374 | 0.387 | 0.363 | 0.390 | 0.380 | 0.389 | 0.389 | 0.391 |'
- en: '| 336 | 0.409 | 0.399 | 0.400 | 0.403 | 0.392 | 0.402 | 0.543 | 0.528 | 0.445
    | 0.459 | 0.410 | 0.411 | 0.408 | 0.426 | 0.413 | 0.413 | 0.423 | 0.413 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.409 | 0.399 | 0.400 | 0.403 | 0.392 | 0.402 | 0.543 | 0.528 | 0.445
    | 0.459 | 0.410 | 0.411 | 0.408 | 0.426 | 0.413 | 0.413 | 0.423 | 0.413 |'
- en: '| 720 | 0.477 | 0.438 | 0.460 | 0.439 | 0.450 | 0.435 | 0.704 | 0.642 | 0.543
    | 0.490 | 0.478 | 0.450 | 0.481 | 0.476 | 0.474 | 0.453 | 0.485 | 0.448 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.477 | 0.438 | 0.460 | 0.439 | 0.450 | 0.435 | 0.704 | 0.642 | 0.543
    | 0.490 | 0.478 | 0.450 | 0.481 | 0.476 | 0.474 | 0.453 | 0.485 | 0.448 |'
- en: '| *Avg.* | 0.395 | 0.390 | 0.389 | 0.397 | 0.381 | 0.395 | 0.502 | 0.502 |
    0.448 | 0.452 | 0.400 | 0.406 | 0.392 | 0.413 | 0.403 | 0.407 | 0.412 | 0.406
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.395 | 0.390 | 0.389 | 0.397 | 0.381 | 0.395 | 0.502 | 0.502 | 0.448
    | 0.452 | 0.400 | 0.406 | 0.392 | 0.413 | 0.403 | 0.407 | 0.412 | 0.406 |'
- en: '| ETTm2 | 96 | 0.178 | 0.256 | 0.178 | 0.263 | 0.178 | 0.260 | 0.273 | 0.356
    | 0.203 | 0.287 | 0.187 | 0.267 | 0.179 | 0.275 | 0.193 | 0.292 | 0.181 | 0.264
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ETTm2 | 96 | 0.178 | 0.256 | 0.178 | 0.263 | 0.178 | 0.260 | 0.273 | 0.356
    | 0.203 | 0.287 | 0.187 | 0.267 | 0.179 | 0.275 | 0.193 | 0.292 | 0.181 | 0.264
    |'
- en: '| 192 | 0.242 | 0.297 | 0.245 | 0.306 | 0.249 | 0.307 | 0.426 | 0.487 | 0.269
    | 0.328 | 0.249 | 0.309 | 0.307 | 0.376 | 0.284 | 0.362 | 0.246 | 0.304 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.242 | 0.297 | 0.245 | 0.306 | 0.249 | 0.307 | 0.426 | 0.487 | 0.269
    | 0.328 | 0.249 | 0.309 | 0.307 | 0.376 | 0.284 | 0.362 | 0.246 | 0.304 |'
- en: '| 336 | 0.307 | 0.339 | 0.309 | 0.347 | 0.313 | 0.346 | 1.013 | 0.714 | 0.325
    | 0.366 | 0.321 | 0.351 | 0.325 | 0.388 | 0.369 | 0.427 | 0.307 | 0.341 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.307 | 0.339 | 0.309 | 0.347 | 0.313 | 0.346 | 1.013 | 0.714 | 0.325
    | 0.366 | 0.321 | 0.351 | 0.325 | 0.388 | 0.369 | 0.427 | 0.307 | 0.341 |'
- en: '| 720 | 0.397 | 0.393 | 0.409 | 0.408 | 0.400 | 0.398 | 3.154 | 1.274 | 0.421
    | 0.415 | 0.408 | 0.403 | 0.502 | 0.490 | 0.554 | 0.522 | 0.407 | 0.397 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.397 | 0.393 | 0.409 | 0.408 | 0.400 | 0.398 | 3.154 | 1.274 | 0.421
    | 0.415 | 0.408 | 0.403 | 0.502 | 0.490 | 0.554 | 0.522 | 0.407 | 0.397 |'
- en: '| *Avg.* | 0.281 | 0.321 | 0.285 | 0.331 | 0.285 | 0.327 | 1.216 | 0.707 |
    0.305 | 0.349 | 0.291 | 0.333 | 0.328 | 0.382 | 0.350 | 0.401 | 0.289 | 0.326
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.281 | 0.321 | 0.285 | 0.331 | 0.285 | 0.327 | 1.216 | 0.707 | 0.305
    | 0.349 | 0.291 | 0.333 | 0.328 | 0.382 | 0.350 | 0.401 | 0.289 | 0.326 |'
- en: '| ETTh1 | 96 | 0.369 | 0.389 | 0.376 | 0.397 | 0.393 | 0.408 | 0.420 | 0.439
    | 0.376 | 0.419 | 0.384 | 0.402 | 0.421 | 0.431 | 0.386 | 0.400 | 0.384 | 0.393
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ETTh1 | 96 | 0.369 | 0.389 | 0.376 | 0.397 | 0.393 | 0.408 | 0.420 | 0.439
    | 0.376 | 0.419 | 0.384 | 0.402 | 0.421 | 0.431 | 0.386 | 0.400 | 0.384 | 0.393
    |'
- en: '| 192 | 0.427 | 0.423 | 0.438 | 0.426 | 0.445 | 0.434 | 0.540 | 0.519 | 0.420
    | 0.448 | 0.436 | 0.429 | 0.474 | 0.487 | 0.437 | 0.432 | 0.436 | 0.422 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.427 | 0.423 | 0.438 | 0.426 | 0.445 | 0.434 | 0.540 | 0.519 | 0.420
    | 0.448 | 0.436 | 0.429 | 0.474 | 0.487 | 0.437 | 0.432 | 0.436 | 0.422 |'
- en: '| 336 | 0.456 | 0.436 | 0.479 | 0.446 | 0.484 | 0.451 | 0.722 | 0.648 | 0.459
    | 0.465 | 0.491 | 0.469 | 0.569 | 0.551 | 0.481 | 0.459 | 0.480 | 0.445 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.456 | 0.436 | 0.479 | 0.446 | 0.484 | 0.451 | 0.722 | 0.648 | 0.459
    | 0.465 | 0.491 | 0.469 | 0.569 | 0.551 | 0.481 | 0.459 | 0.480 | 0.445 |'
- en: '| 720 | 0.479 | 0.467 | 0.495 | 0.476 | 0.480 | 0.471 | 0.799 | 0.685 | 0.506
    | 0.507 | 0.521 | 0.500 | 0.770 | 0.672 | 0.519 | 0.516 | 0.481 | 0.469 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.479 | 0.467 | 0.495 | 0.476 | 0.480 | 0.471 | 0.799 | 0.685 | 0.506
    | 0.507 | 0.521 | 0.500 | 0.770 | 0.672 | 0.519 | 0.516 | 0.481 | 0.469 |'
- en: '| *Avg.* | 0.432 | 0.428 | 0.447 | 0.436 | 0.450 | 0.441 | 0.620 | 0.572 |
    0.440 | 0.460 | 0.458 | 0.450 | 0.558 | 0.535 | 0.456 | 0.452 | 0.445 | 0.432
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.432 | 0.428 | 0.447 | 0.436 | 0.450 | 0.441 | 0.620 | 0.572 | 0.440
    | 0.460 | 0.458 | 0.450 | 0.558 | 0.535 | 0.456 | 0.452 | 0.445 | 0.432 |'
- en: '| ETTh2 | 96 | 0.279 | 0.331 | 0.295 | 0.348 | 0.294 | 0.343 | 0.745 | 0.584
    | 0.358 | 0.397 | 0.340 | 0.374 | 0.299 | 0.364 | 0.333 | 0.387 | 0.400 | 0.440
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ETTh2 | 96 | 0.279 | 0.331 | 0.295 | 0.348 | 0.294 | 0.343 | 0.745 | 0.584
    | 0.358 | 0.397 | 0.340 | 0.374 | 0.299 | 0.364 | 0.333 | 0.387 | 0.400 | 0.440
    |'
- en: '| 192 | 0.353 | 0.380 | 0.386 | 0.404 | 0.377 | 0.393 | 0.877 | 0.656 | 0.429
    | 0.439 | 0.402 | 0.414 | 0.441 | 0.454 | 0.477 | 0.476 | 0.528 | 0.509 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.353 | 0.380 | 0.386 | 0.404 | 0.377 | 0.393 | 0.877 | 0.656 | 0.429
    | 0.439 | 0.402 | 0.414 | 0.441 | 0.454 | 0.477 | 0.476 | 0.528 | 0.509 |'
- en: '| 336 | 0.362 | 0.394 | 0.421 | 0.435 | 0.381 | 0.409 | 1.043 | 0.731 | 0.496
    | 0.487 | 0.452 | 0.452 | 0.654 | 0.567 | 0.594 | 0.541 | 0.643 | 0.571 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.362 | 0.394 | 0.421 | 0.435 | 0.381 | 0.409 | 1.043 | 0.731 | 0.496
    | 0.487 | 0.452 | 0.452 | 0.654 | 0.567 | 0.594 | 0.541 | 0.643 | 0.571 |'
- en: '| 720 | 0.404 | 0.426 | 0.422 | 0.445 | 0.412 | 0.433 | 1.104 | 0.763 | 0.463
    | 0.474 | 0.462 | 0.468 | 0.956 | 0.716 | 0.831 | 0.657 | 0.874 | 0.679 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.404 | 0.426 | 0.422 | 0.445 | 0.412 | 0.433 | 1.104 | 0.763 | 0.463
    | 0.474 | 0.462 | 0.468 | 0.956 | 0.716 | 0.831 | 0.657 | 0.874 | 0.679 |'
- en: '| *Avg.* | 0.349 | 0.382 | 0.381 | 0.408 | 0.366 | 0.394 | 0.942 | 0.684 |
    0.437 | 0.449 | 0.414 | 0.427 | 0.587 | 0.525 | 0.559 | 0.515 | 0.611 | 0.550
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.349 | 0.382 | 0.381 | 0.408 | 0.366 | 0.394 | 0.942 | 0.684 | 0.437
    | 0.449 | 0.414 | 0.427 | 0.587 | 0.525 | 0.559 | 0.515 | 0.611 | 0.550 |'
- en: '| Weather | 96 | 0.164 | 0.204 | 0.182 | 0.223 | 0.177 | 0.218 | 0.158 | 0.230
    | 0.217 | 0.296 | 0.172 | 0.220 | 0.161 | 0.229 | 0.196 | 0.255 | 0.202 | 0.261
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 天气 | 96 | 0.164 | 0.204 | 0.182 | 0.223 | 0.177 | 0.218 | 0.158 | 0.230 |
    0.217 | 0.296 | 0.172 | 0.220 | 0.161 | 0.229 | 0.196 | 0.255 | 0.202 | 0.261
    |'
- en: '| 192 | 0.214 | 0.250 | 0.231 | 0.263 | 0.225 | 0.259 | 0.206 | 0.277 | 0.276
    | 0.336 | 0.219 | 0.261 | 0.220 | 0.281 | 0.237 | 0.296 | 0.242 | 0.298 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.214 | 0.250 | 0.231 | 0.263 | 0.225 | 0.259 | 0.206 | 0.277 | 0.276
    | 0.336 | 0.219 | 0.261 | 0.220 | 0.281 | 0.237 | 0.296 | 0.242 | 0.298 |'
- en: '| 336 | 0.269 | 0.291 | 0.283 | 0.300 | 0.278 | 0.297 | 0.272 | 0.335 | 0.339
    | 0.380 | 0.280 | 0.306 | 0.278 | 0.331 | 0.283 | 0.335 | 0.287 | 0.335 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.269 | 0.291 | 0.283 | 0.300 | 0.278 | 0.297 | 0.272 | 0.335 | 0.339
    | 0.380 | 0.280 | 0.306 | 0.278 | 0.331 | 0.283 | 0.335 | 0.287 | 0.335 |'
- en: '| 720 | 0.355 | 0.352 | 0.360 | 0.350 | 0.354 | 0.348 | 0.398 | 0.418 | 0.403
    | 0.428 | 0.365 | 0.359 | 0.311 | 0.356 | 0.345 | 0.381 | 0.351 | 0.386 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.355 | 0.352 | 0.360 | 0.350 | 0.354 | 0.348 | 0.398 | 0.418 | 0.403
    | 0.428 | 0.365 | 0.359 | 0.311 | 0.356 | 0.345 | 0.381 | 0.351 | 0.386 |'
- en: '| *Avg.* | 0.250 | 0.274 | 0.264 | 0.284 | 0.258 | 0.280 | 0.259 | 0.315 |
    0.309 | 0.360 | 0.259 | 0.287 | 0.242 | 0.299 | 0.265 | 0.317 | 0.271 | 0.320
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.250 | 0.274 | 0.264 | 0.284 | 0.258 | 0.280 | 0.259 | 0.315 | 0.309
    | 0.360 | 0.259 | 0.287 | 0.242 | 0.299 | 0.265 | 0.317 | 0.271 | 0.320 |'
- en: '| ECL | 96 | 0.145 | 0.238 | 0.185 | 0.272 | 0.195 | 0.285 | 0.219 | 0.314
    | 0.193 | 0.308 | 0.168 | 0.272 | 0.164 | 0.269 | 0.197 | 0.282 | 0.237 | 0.329
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ECL | 96 | 0.145 | 0.238 | 0.185 | 0.272 | 0.195 | 0.285 | 0.219 | 0.314
    | 0.193 | 0.308 | 0.168 | 0.272 | 0.164 | 0.269 | 0.197 | 0.282 | 0.237 | 0.329
    |'
- en: '| 192 | 0.161 | 0.252 | 0.189 | 0.276 | 0.199 | 0.289 | 0.231 | 0.322 | 0.201
    | 0.315 | 0.184 | 0.289 | 0.177 | 0.285 | 0.196 | 0.285 | 0.236 | 0.330 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.161 | 0.252 | 0.189 | 0.276 | 0.199 | 0.289 | 0.231 | 0.322 | 0.201
    | 0.315 | 0.184 | 0.289 | 0.177 | 0.285 | 0.196 | 0.285 | 0.236 | 0.330 |'
- en: '| 336 | 0.175 | 0.267 | 0.204 | 0.291 | 0.215 | 0.305 | 0.246 | 0.337 | 0.214
    | 0.329 | 0.198 | 0.300 | 0.193 | 0.304 | 0.209 | 0.301 | 0.249 | 0.344 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.175 | 0.267 | 0.204 | 0.291 | 0.215 | 0.305 | 0.246 | 0.337 | 0.214
    | 0.329 | 0.198 | 0.300 | 0.193 | 0.304 | 0.209 | 0.301 | 0.249 | 0.344 |'
- en: '| 720 | 0.222 | 0.303 | 0.245 | 0.324 | 0.256 | 0.337 | 0.280 | 0.363 | 0.246
    | 0.355 | 0.220 | 0.320 | 0.212 | 0.321 | 0.245 | 0.333 | 0.284 | 0.373 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.222 | 0.303 | 0.245 | 0.324 | 0.256 | 0.337 | 0.280 | 0.363 | 0.246
    | 0.355 | 0.220 | 0.320 | 0.212 | 0.321 | 0.245 | 0.333 | 0.284 | 0.373 |'
- en: '| *Avg.* | 0.175 | 0.265 | 0.205 | 0.290 | 0.216 | 0.304 | 0.244 | 0.334 |
    0.214 | 0.327 | 0.192 | 0.295 | 0.186 | 0.294 | 0.212 | 0.300 | 0.251 | 0.344
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.175 | 0.265 | 0.205 | 0.290 | 0.216 | 0.304 | 0.244 | 0.334 | 0.214
    | 0.327 | 0.192 | 0.295 | 0.186 | 0.294 | 0.212 | 0.300 | 0.251 | 0.344 |'
- en: '| Traffic | 96 | 0.407 | 0.268 | 0.468 | 0.307 | 0.544 | 0.359 | 0.522 | 0.290
    | 0.587 | 0.366 | 0.593 | 0.321 | 0.519 | 0.309 | 0.650 | 0.396 | 0.805 | 0.493
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 交通 | 96 | 0.407 | 0.268 | 0.468 | 0.307 | 0.544 | 0.359 | 0.522 | 0.290 |
    0.587 | 0.366 | 0.593 | 0.321 | 0.519 | 0.309 | 0.650 | 0.396 | 0.805 | 0.493
    |'
- en: '| 192 | 0.430 | 0.278 | 0.476 | 0.311 | 0.540 | 0.354 | 0.530 | 0.293 | 0.604
    | 0.373 | 0.617 | 0.336 | 0.537 | 0.315 | 0.598 | 0.370 | 0.756 | 0.474 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.430 | 0.278 | 0.476 | 0.311 | 0.540 | 0.354 | 0.530 | 0.293 | 0.604
    | 0.373 | 0.617 | 0.336 | 0.537 | 0.315 | 0.598 | 0.370 | 0.756 | 0.474 |'
- en: '| 336 | 0.444 | 0.281 | 0.488 | 0.317 | 0.551 | 0.358 | 0.558 | 0.305 | 0.621
    | 0.383 | 0.629 | 0.336 | 0.534 | 0.313 | 0.605 | 0.373 | 0.762 | 0.477 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 336 | 0.444 | 0.281 | 0.488 | 0.317 | 0.551 | 0.358 | 0.558 | 0.305 | 0.621
    | 0.383 | 0.629 | 0.336 | 0.534 | 0.313 | 0.605 | 0.373 | 0.762 | 0.477 |'
- en: '| 720 | 0.477 | 0.300 | 0.521 | 0.333 | 0.586 | 0.375 | 0.589 | 0.328 | 0.626
    | 0.382 | 0.640 | 0.350 | 0.577 | 0.325 | 0.645 | 0.394 | 0.719 | 0.449 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 720 | 0.477 | 0.300 | 0.521 | 0.333 | 0.586 | 0.375 | 0.589 | 0.328 | 0.626
    | 0.382 | 0.640 | 0.350 | 0.577 | 0.325 | 0.645 | 0.394 | 0.719 | 0.449 |'
- en: '| *Avg.* | 0.439 | 0.281 | 0.488 | 0.317 | 0.555 | 0.361 | 0.550 | 0.304 |
    0.610 | 0.376 | 0.620 | 0.336 | 0.541 | 0.315 | 0.625 | 0.383 | 0.760 | 0.473
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| *平均值* | 0.439 | 0.281 | 0.488 | 0.317 | 0.555 | 0.361 | 0.550 | 0.304 | 0.610
    | 0.376 | 0.620 | 0.336 | 0.541 | 0.315 | 0.625 | 0.383 | 0.760 | 0.473 |'
- en: '| $1^{\text{st}}$  *Count* | 56 | 1 | 7 | 2 | 1 | 0 | 4 | 0 | 2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $1^{\text{st}}$  *计数* | 56 | 1 | 7 | 2 | 1 | 0 | 4 | 0 | 2 |'
- en: 'Table 1: Multivariate long-term forecasting results with different prediction
    lengths $H\in\{96,192,336,720\}$ *Count* indicates the number of times each method
    achieves the best results.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同预测长度 $H\in\{96,192,336,720\}$ 的多变量长期预测结果 *计数* 表示每种方法取得最佳结果的次数。
- en: 3.3 Parameter Efficient Training
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 参数高效训练
- en: 'To avoid catastrophic forgetting when tuning for downstream tasks, and to improve
    training efficiency, we employ the parameter-efficient training technique to fine-tune
    the pre-trained LLMs. Specifically, for the temporal modal branch, we introduce
    Low-rank Adaptation (LoRA) (Hu et al., [2021](#bib.bib17)) and fine-tune the positional
    encoding weights. The total loss during training is the weighted summation of
    the supervised loss $\mathcal{L}_{sup}$:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在下游任务调优时发生灾难性遗忘，并提高训练效率，我们采用了参数高效的训练技术来微调预训练的LLMs。具体来说，对于时间模态分支，我们引入了低秩自适应（LoRA）
    (Hu et al., [2021](#bib.bib17)) 并微调位置编码权重。训练过程中的总损失是监督损失 $\mathcal{L}_{sup}$ 的加权总和：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $\lambda_{1}$ are hyper-parameters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$ 是超参数。
- en: 4 Experiments
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: To demonstrate the effectiveness of the proposed LLaTA, we conduct extensive
    experiments on various time series forecasting tasks, including short/long-term
    forecasting and few/zero-shot learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示所提出的LLaTA的有效性，我们在各种时间序列预测任务上进行了广泛的实验，包括短期/长期预测和少量/零样本学习。
- en: Baselines.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线。
- en: 'We carefully select representative baselines from the recent time series forecasting
    landscape, including the following categories: (1) LLMs-based models: GPT4TS (Zhou
    et al., [2023](#bib.bib39)); (2) Transformer-based models: PatchTST (Nie et al.,
    [2023](#bib.bib25)), Crossformer (Zhang and Yan, [2023](#bib.bib37)), ETSformer (Woo
    et al., [2022](#bib.bib32)), FEDformer (Zhou et al., [2022](#bib.bib38)) and Autoformer (Wu
    et al., [2021](#bib.bib33)); (3) CNN-based models: TCN (Bai et al., [2018](#bib.bib3)),
    MICN (Wang et al., [2022](#bib.bib31)) and TimesNet (Wu et al., [2023](#bib.bib34));
    (4) MLP-based models: DLinear (Zeng et al., [2023](#bib.bib36)) and TiDE (Das
    et al., [2023](#bib.bib11)). Besides, N-HiTS (Challu et al., [2022](#bib.bib6))
    and N-BEATS (Oreshkin et al., [2019](#bib.bib26)) are included for short-term
    forecasting.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从近期的时间序列预测领域中精心选择了代表性基线，包括以下几类：（1）基于LLMs的模型：GPT4TS (Zhou et al., [2023](#bib.bib39))；（2）基于Transformer的模型：PatchTST
    (Nie et al., [2023](#bib.bib25))、Crossformer (Zhang and Yan, [2023](#bib.bib37))、ETSformer
    (Woo et al., [2022](#bib.bib32))、FEDformer (Zhou et al., [2022](#bib.bib38)) 和
    Autoformer (Wu et al., [2021](#bib.bib33))；（3）基于CNN的模型：TCN (Bai et al., [2018](#bib.bib3))、MICN
    (Wang et al., [2022](#bib.bib31)) 和 TimesNet (Wu et al., [2023](#bib.bib34))；（4）基于MLP的模型：DLinear
    (Zeng et al., [2023](#bib.bib36)) 和 TiDE (Das et al., [2023](#bib.bib11))。此外，N-HiTS
    (Challu et al., [2022](#bib.bib6)) 和 N-BEATS (Oreshkin et al., [2019](#bib.bib26))
    被包括在短期预测中。
- en: Implementation Details.
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: Following (Zhou et al., [2023](#bib.bib39)), we use pre-trained GPT2 based model
    (Radford et al., [2019](#bib.bib28)) with the first 6 Transformer layers as our
    backbone. Optimization is conducted using the Adam optimizer (Kingma and Ba, [2014](#bib.bib20)),
    with a learning rate of $0.0005$. In terms of loss functions for long-term forecasting,
    we apply L1 loss across all three loss types for ETT datasets, while for the other
    three datasets, smooth L1 loss is utilized. For short-term forecasting, we compute
    supervised loss with SMAPE, modal consistency loss with MASE, and feature regularization
    loss with smooth L1 loss, respectively. More details are provided in the supplementary
    material.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 根据（Zhou et al., [2023](#bib.bib39)），我们使用基于预训练GPT2的模型（Radford et al., [2019](#bib.bib28)），其前6层Transformer作为我们的骨干。优化使用Adam优化器（Kingma和Ba，[2014](#bib.bib20)），学习率为$0.0005$。在长期预测的损失函数方面，我们对ETT数据集应用L1损失，而对其他三个数据集则使用平滑L1损失。对于短期预测，我们分别计算带有SMAPE的监督损失、带有MASE的模态一致性损失以及带有平滑L1损失的特征正则化损失。更多细节请参见补充材料。
- en: '| Models | LLaTA | GPT4TS | PatchTST | ETSformer | FEDformer | Autoformer |
    TimesNet | TCN | N-HiTS | N-BEATS | DLinear |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaTA | GPT4TS | PatchTST | ETSformer | FEDformer | Autoformer | TimesNet
    | TCN | N-HiTS | N-BEATS | DLinear |'
- en: '|  (Ours)  |  ([2023](#bib.bib39))  |  ([2023](#bib.bib25))  |  ([2022](#bib.bib32))  |  ([2022](#bib.bib38))  |  ([2021](#bib.bib33))  |  ([2023](#bib.bib34))  |  ([2018](#bib.bib3))  |  ([2022](#bib.bib6))  |  ([2019](#bib.bib26))  |  ([2023](#bib.bib36))  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  （我们的）  |  （[2023](#bib.bib39)）  |  （[2023](#bib.bib25)）  |  （[2022](#bib.bib32)）  |  （[2022](#bib.bib38)）  |  （[2021](#bib.bib33)）  |  （[2023](#bib.bib34)）  |  （[2018](#bib.bib3)）  |  （[2022](#bib.bib6)）  |  （[2019](#bib.bib26)）  |  （[2023](#bib.bib36)）  |'
- en: '| Yearly |  SMAPE  | 13.351 | 13.531 | 13.477 | 18.009 | 13.728 | 13.974 |
    13.387 | 14.920 | 13.418 | 13.436 | 16.965 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 年度 |  SMAPE  | 13.351 | 13.531 | 13.477 | 18.009 | 13.728 | 13.974 | 13.387
    | 14.920 | 13.418 | 13.436 | 16.965 |'
- en: '|  MASE  | 3.003 | 3.015 | 3.019 | 4.487 | 3.048 | 3.134 | 2.996 | 3.364 |
    3.045 | 3.043 | 4.283 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  MASE  | 3.003 | 3.015 | 3.019 | 4.487 | 3.048 | 3.134 | 2.996 | 3.364 |
    3.045 | 3.043 | 4.283 |'
- en: '|  OWA  | 0.786 | 0.793 | 0.792 | 1.115 | 0.803 | 0.822 | 0.786 | 0.880 | 0.793
    | 0.794 | 1.058 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  OWA  | 0.786 | 0.793 | 0.792 | 1.115 | 0.803 | 0.822 | 0.786 | 0.880 | 0.793
    | 0.794 | 1.058 |'
- en: '| Quarterly |  SMAPE  | 9.990 | 10.177 | 10.380 | 13.376 | 10.792 | 11.338
    | 10.100 | 11.122 | 10.202 | 10.124 | 12.145 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 按季度 |  SMAPE  | 9.990 | 10.177 | 10.380 | 13.376 | 10.792 | 11.338 | 10.100
    | 11.122 | 10.202 | 10.124 | 12.145 |'
- en: '|  MASE  | 1.164 | 1.194 | 1.233 | 1.906 | 1.283 | 1.365 | 1.182 | 1.360 |
    1.194 | 1.169 | 1.520 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  MASE  | 1.164 | 1.194 | 1.233 | 1.906 | 1.283 | 1.365 | 1.182 | 1.360 |
    1.194 | 1.169 | 1.520 |'
- en: '|  OWA  | 0.878 | 0.898 | 0.921 | 1.302 | 0.958 | 1.012 | 0.890 | 1.001 | 0.899
    | 0.886 | 1.106 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  OWA  | 0.878 | 0.898 | 0.921 | 1.302 | 0.958 | 1.012 | 0.890 | 1.001 | 0.899
    | 0.886 | 1.106 |'
- en: '| Monthly |  SMAPE  | 12.643 | 12.894 | 12.959 | 14.588 | 14.260 | 13.958 |
    12.679 | 15.626 | 12.791 | 12.677 | 13.514 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 按月 |  SMAPE  | 12.643 | 12.894 | 12.959 | 14.588 | 14.260 | 13.958 | 12.679
    | 15.626 | 12.791 | 12.677 | 13.514 |'
- en: '|  MASE  | 0.922 | 0.956 | 0.970 | 1.368 | 1.102 | 1.103 | 0.933 | 1.274 |
    0.969 | 0.937 | 1.037 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  MASE  | 0.922 | 0.956 | 0.970 | 1.368 | 1.102 | 1.103 | 0.933 | 1.274 |
    0.969 | 0.937 | 1.037 |'
- en: '|  OWA  | 0.872 | 0.897 | 0.905 | 1.149 | 1.012 | 1.002 | 0.878 | 1.141 | 0.899
    | 0.880 | 0.956 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  OWA  | 0.872 | 0.897 | 0.905 | 1.149 | 1.012 | 1.002 | 0.878 | 1.141 | 0.899
    | 0.880 | 0.956 |'
- en: '| Others |  SMAPE  | 4.552 | 4.940 | 4.952 | 7.267 | 4.954 | 5.485 | 4.891
    | 7.186 | 5.061 | 4.925 | 6.709 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 其他 |  SMAPE  | 4.552 | 4.940 | 4.952 | 7.267 | 4.954 | 5.485 | 4.891 | 7.186
    | 5.061 | 4.925 | 6.709 |'
- en: '|  MASE  | 3.092 | 3.228 | 3.347 | 5.240 | 3.264 | 3.865 | 3.302 | 4.677 |
    3.216 | 3.391 | 4.953 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  MASE  | 3.092 | 3.228 | 3.347 | 5.240 | 3.264 | 3.865 | 3.302 | 4.677 |
    3.216 | 3.391 | 4.953 |'
- en: '|  OWA  | 0.967 | 1.029 | 1.049 | 1.591 | 1.036 | 1.187 | 1.035 | 1.494 | 1.040
    | 1.053 | 1.487 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  OWA  | 0.967 | 1.029 | 1.049 | 1.591 | 1.036 | 1.187 | 1.035 | 1.494 | 1.040
    | 1.053 | 1.487 |'
- en: '| Average |  SMAPE  | 11.765 | 11.991 | 12.059 | 14.718 | 12.840 | 12.909 |
    11.829 | 13.961 | 11.927 | 11.851 | 13.639 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 平均 |  SMAPE  | 11.765 | 11.991 | 12.059 | 14.718 | 12.840 | 12.909 | 11.829
    | 13.961 | 11.927 | 11.851 | 13.639 |'
- en: '|  MASE  | 1.567 | 1.600 | 1.623 | 2.408 | 1.701 | 1.771 | 1.585 | 1.945 |
    1.613 | 1.599 | 2.095 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  MASE  | 1.567 | 1.600 | 1.623 | 2.408 | 1.701 | 1.771 | 1.585 | 1.945 |
    1.613 | 1.599 | 2.095 |'
- en: '|  OWA  | 0.844 | 0.861 | 0.869 | 1.172 | 0.918 | 0.939 | 0.851 | 1.023 | 0.861
    | 0.855 | 1.051 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  OWA  | 0.844 | 0.861 | 0.869 | 1.172 | 0.918 | 0.939 | 0.851 | 1.023 | 0.861
    | 0.855 | 1.051 |'
- en: '| $1^{\text{st}}$  *Count* | 14 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $1^{\text{st}}$  *计数* | 14 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 |'
- en: 'Table 2: Short-term forecasting results on M4 dataset. The input length and
    prediction length are set to $[12,96]$ *Count* indicates the number of times each
    method achieves the best results.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：M4数据集上的短期预测结果。输入长度和预测长度设置为$[12,96]$ *计数*表示每种方法达到最佳结果的次数。
- en: '| Models | ETTm1 | ETTm2 | ETTh1 | ETTh2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ETTm1 | ETTm2 | ETTh1 | ETTh2 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  TiDE ([2023](#bib.bib11))  | 0.515 | 0.469 | 0.303 | 0.337 | 0.779 | 0.604
    | 0.421 | 0.428 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  TiDE ([2023](#bib.bib11))  | 0.515 | 0.469 | 0.303 | 0.337 | 0.779 | 0.604
    | 0.421 | 0.428 |'
- en: '|  DLinear ([2022](#bib.bib31))  | 0.567 | 0.499 | 0.329 | 0.382 | 0.647 |
    0.552 | 0.441 | 0.458 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  DLinear ([2022](#bib.bib31))  | 0.567 | 0.499 | 0.329 | 0.382 | 0.647 |
    0.552 | 0.441 | 0.458 |'
- en: '|  MICN ([2022](#bib.bib31))  | 0.970 | 0.674 | 1.073 | 0.716 | 1.405 | 0.814
    | 2.533 | 1.158 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  MICN ([2022](#bib.bib31))  | 0.970 | 0.674 | 1.073 | 0.716 | 1.405 | 0.814
    | 2.533 | 1.158 |'
- en: '|  TimesNet ([2023](#bib.bib34))  | 0.673 | 0.534 | 0.321 | 0.354 | 0.865 |
    0.625 | 0.476 | 0.463 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  TimesNet ([2023](#bib.bib34))  | 0.673 | 0.534 | 0.321 | 0.354 | 0.865 |
    0.625 | 0.476 | 0.463 |'
- en: '|  FEDformer ([2022](#bib.bib38))  | 0.696 | 0.572 | 0.356 | 0.392 | 0.750
    | 0.607 | 0.553 | 0.525 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  FEDformer ([2022](#bib.bib38))  | 0.696 | 0.572 | 0.356 | 0.392 | 0.750
    | 0.607 | 0.553 | 0.525 |'
- en: '|  Crossformer ([2023](#bib.bib37))  | 1.340 | 0.848 | 1.985 | 1.048 | 1.744
    | 0.914 | 3.139 | 1.378 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  Crossformer ([2023](#bib.bib37))  | 1.340 | 0.848 | 1.985 | 1.048 | 1.744
    | 0.914 | 3.139 | 1.378 |'
- en: '|  PatchTST ([2023](#bib.bib25))  | 0.557 | 0.483 | 0.295 | 0.334 | 0.683 |
    0.546 | 0.550 | 0.487 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  PatchTST ([2023](#bib.bib25))  | 0.557 | 0.483 | 0.295 | 0.334 | 0.683 |
    0.546 | 0.550 | 0.487 |'
- en: '|  GPT4TS ([2023](#bib.bib39))  | 0.608 | 0.500 | 0.303 | 0.336 | 0.689 | 0.555
    | 0.579 | 0.497 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  GPT4TS ([2023](#bib.bib39))  | 0.608 | 0.500 | 0.303 | 0.336 | 0.689 | 0.555
    | 0.579 | 0.497 |'
- en: '|  LLaTA (Ours)  | 0.504 | 0.462 | 0.302 | 0.330 | 0.644 | 0.541 | 0.419 |
    0.427 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  LLaTA (我们的方法)  | 0.504 | 0.462 | 0.302 | 0.330 | 0.644 | 0.541 | 0.419 |
    0.427 |'
- en: 'Table 3: Few-shot learning results on 10% training data of ETT datasets. All
    the results are averaged from 4 different prediction lengths $H\in\{96,192,336,720\}$.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 ETT 数据集的 10% 训练数据上的少-shot 学习结果。所有结果均为 4 种不同预测长度 $H\in\{96,192,336,720\}$
    的平均值。
- en: '| Models |  h1 $\rightarrow$ m2  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |  h1 $\rightarrow$ m2  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| MSE | MAE | MSE | MAE | MSE | MAE | MSE | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  TiDE ([2023](#bib.bib11))  | 0.774 | 0.574 | 0.314 | 0.355 | 0.841 | 0.590
    | 0.321 | 0.364 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  TiDE ([2023](#bib.bib11))  | 0.774 | 0.574 | 0.314 | 0.355 | 0.841 | 0.590
    | 0.321 | 0.364 |'
- en: '|  DLinear ([2023](#bib.bib11))  | 0.760 | 0.577 | 0.399 | 0.439 | 0.778 |
    0.594 | 0.496 | 0.496 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  DLinear ([2023](#bib.bib11))  | 0.760 | 0.577 | 0.399 | 0.439 | 0.778 |
    0.594 | 0.496 | 0.496 |'
- en: '|  MICN ([2022](#bib.bib31))  | 1.439 | 0.780 | 2.428 | 1.236 | 0.764 | 0.601
    | 0.527 | 0.519 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  MICN ([2022](#bib.bib31))  | 1.439 | 0.780 | 2.428 | 1.236 | 0.764 | 0.601
    | 0.527 | 0.519 |'
- en: '|  TimesNet ([2023](#bib.bib34))  | 0.794 | 0.575 | 0.339 | 0.370 | 1.286 |
    0.705 | 0.361 | 0.390 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  TimesNet ([2023](#bib.bib34))  | 0.794 | 0.575 | 0.339 | 0.370 | 1.286 |
    0.705 | 0.361 | 0.390 |'
- en: '|  FEDformer ([2022](#bib.bib38))  | 0.765 | 0.588 | 0.357 | 0.403 | 0.741
    | 0.588 | 0.365 | 0.405 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  FEDformer ([2022](#bib.bib38))  | 0.765 | 0.588 | 0.357 | 0.403 | 0.741
    | 0.588 | 0.365 | 0.405 |'
- en: '|  Crossformer ([2023](#bib.bib37))  | 0.999 | 0.736 | 1.120 | 0.789 | 1.195
    | 0.711 | 2.043 | 1.124 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  Crossformer ([2023](#bib.bib37))  | 0.999 | 0.736 | 1.120 | 0.789 | 1.195
    | 0.711 | 2.043 | 1.124 |'
- en: '|  PatchTST ([2023](#bib.bib25))  | 0.894 | 0.610 | 0.318 | 0.362 | 0.871 |
    0.596 | 0.420 | 0.433 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  PatchTST ([2023](#bib.bib25))  | 0.894 | 0.610 | 0.318 | 0.362 | 0.871 |
    0.596 | 0.420 | 0.433 |'
- en: '|  GPT4TS ([2023](#bib.bib39))  | 0.798 | 0.574 | 0.317 | 0.359 | 0.920 | 0.610
    | 0.331 | 0.371 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  GPT4TS ([2023](#bib.bib39))  | 0.798 | 0.574 | 0.317 | 0.359 | 0.920 | 0.610
    | 0.331 | 0.371 |'
- en: '|  LLaTA (Ours)  | 0.755 | 0.574 | 0.316 | 0.355 | 0.836 | 0.586 | 0.319 |
    0.360 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  LLaTA (我们的方法)  | 0.755 | 0.574 | 0.316 | 0.355 | 0.836 | 0.586 | 0.319 |
    0.360 |'
- en: 'Table 4: Zero-shot learning results on ETT datasets, where ‘h1’, ‘h2’, ‘m1’,
    and ‘m2’ denote ETTh1, ETTh2, ETTm1, and ETTm2 respectively. “${{\color[rgb]{1,0.65,0.3}\blacklozenge}}\to{{\color[rgb]{0.75,0.5,0.75}\bigstar}}$.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：ETT 数据集上的零-shot 学习结果，其中 ‘h1’，‘h2’，‘m1’，和 ‘m2’ 分别表示 ETTh1、ETTh2、ETTm1 和 ETTm2。
    “${{\color[rgb]{1,0.65,0.3}\blacklozenge}}\to{{\color[rgb]{0.75,0.5,0.75}\bigstar}}$。
- en: 4.1 Long-term Forecasting
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 长期预测
- en: Setups.
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置。
- en: We conduct experiments on seven widely-used real-world datasets, including the
    Electricity Transformer Temperature (ETT) dataset with its four subsets (ETTh1,
    ETTh2, ETTm1, ETTm2), Weather, ECL, and Traffic (Wu et al., [2021](#bib.bib33)).
    Detailed descriptions of the implementation and datasets are provided in supplementary
    material. The input time series length $T$. Consistent with prior works, the Mean
    Square Error (MSE) and Mean Absolute Error (MAE) are chosen as evaluation metrics.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在七个广泛使用的真实世界数据集上进行了实验，包括电力变压器温度 (ETT) 数据集及其四个子集（ETTh1、ETTh2、ETTm1、ETTm2）、天气、ECL
    和交通（Wu 等人，[2021](#bib.bib33)）。实施和数据集的详细描述见补充材料。输入时间序列长度 $T$。与先前的工作一致，我们选择均方误差
    (MSE) 和平均绝对误差 (MAE) 作为评估指标。
- en: Results.
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果。
- en: Comprehensive long-term forecasting results are presented in Table [1](#S3.T1
    "Table 1 ‣ Modal Consistency Loss. ‣ 3.2 Dynamic Knowledge Learning ‣ 3 Methodology
    ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"). Our method consistently delivers state-of-the-art performance,
    achieving the top results in 56 evaluations, in contrast to the nearest competing
    baseline which achieves top results only 7 times. Notably, our approach reduces
    MSE/MAE by 7.05%/6.53% compared to the state-of-the-art Transformer-based model
    PatchTST. In comparison with the LLM-powered method GPT4TS, we observe a reduction
    of 5.94%/5.14% in MSE/MAE. Moreover, our improvements are substantial against
    other baseline methods, exceeding 10% in most cases.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 综合长期预测结果见表[1](#S3.T1 "Table 1 ‣ Modal Consistency Loss. ‣ 3.2 Dynamic Knowledge
    Learning ‣ 3 Methodology ‣ Taming Pre-trained LLMs for Generalised Time Series
    Forecasting via Cross-modal Knowledge Distillation")。我们的方法在56次评估中始终表现出色，而最接近的竞争基线方法仅有7次达到最佳结果。值得注意的是，我们的方法相比于最先进的基于Transformer的模型PatchTST减少了7.05%/6.53%的MSE/MAE。与LLM驱动的方法GPT4TS相比，我们观察到MSE/MAE分别减少了5.94%/5.14%。此外，我们的方法在其他基线方法中也表现出显著改进，大多数情况下超过了10%。
- en: 4.2 Short-term Forecasting
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 短期预测
- en: Setups.
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置。
- en: We adopt the M4 datasets (Spyros Makridakis, [2018](#bib.bib29)), which comprise
    univariate marketing data collected yearly, quarterly, and monthly. Comprehensive
    details are available in the supplementary material. In this case, the prediction
    horizons are comparatively short, ranging in $[6,48]$. Correspondingly, the input
    lengths are set to be twice the size of the prediction horizons. The evaluation
    metrics are symmetric mean absolute percentage error (SMAPE), mean absolute scaled
    error (MSAE), and overall weighted average (OWA).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了M4数据集（Spyros Makridakis，[2018](#bib.bib29)），该数据集包含按年、季度和月度收集的单变量市场数据。详细信息可见补充材料。在这种情况下，预测视野相对较短，范围在$[6,48]$。相应地，输入长度设置为预测视野的两倍。评估指标为对称平均绝对百分比误差（SMAPE）、平均绝对缩放误差（MSAE）和总体加权平均（OWA）。
- en: Results.
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果。
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ Implementation Details. ‣ 4 Experiments
    ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"), our method demonstrates superior performance in short-term
    forecasting across various evaluation metrics. Notably, it achieves the best results
    in 14 out of 15 categories, markedly outperforming all baselines. In comparison
    with TimesNet, currently the leading method in short-term forecasting, our model
    achieves a 1% overall improvement in performance.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S4.T2 "Table 2 ‣ Implementation Details. ‣ 4 Experiments ‣ Taming Pre-trained
    LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation")所示，我们的方法在各种评估指标中展示了卓越的短期预测性能。特别地，在15个类别中有14个类别取得了最佳结果，明显优于所有基线方法。与目前在短期预测中领先的TimesNet相比，我们的模型在整体性能上提高了1%。
- en: 4.3 Few/zero-shot Learning
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 小样本/零样本学习
- en: LLMs have demonstrated remarkable performance in both few-shot and zero-shot
    tasks. The capabilities of few-shot and zero-shot learning are critically important
    for general time series forecasting models (Brown et al., [2020](#bib.bib4); Achiam
    et al., [2023](#bib.bib1); Liu et al., [2023a](#bib.bib22); Kojima et al., [2022](#bib.bib21)).
    To thoroughly assess the generalized ability of our method in time series forecasting,
    we conduct experiments under few-shot and zero-shot learning settings. In few-shot
    learning, only a small ratio of the training data is utilized. For zero-shot learning,
    the model trained on one dataset is directly employed for testing on another dataset
    without any additional training.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在小样本和零样本任务中表现出色。小样本和零样本学习的能力对于通用时间序列预测模型至关重要（Brown et al., [2020](#bib.bib4)；Achiam
    et al., [2023](#bib.bib1)；Liu et al., [2023a](#bib.bib22)；Kojima et al., [2022](#bib.bib21)）。为了全面评估我们的方法在时间序列预测中的泛化能力，我们在小样本和零样本学习设置下进行了实验。在小样本学习中，仅使用了一小部分训练数据。对于零样本学习，训练于一个数据集的模型被直接用于测试另一个数据集，而无需额外的训练。
- en: Few-shot Learning.
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小样本学习。
- en: We conduct few-shot experiments on four ETT datasets. Specifically, for each
    dataset, we utilize only the first 10% of the training data. This constrained
    data scenario presents a considerable challenge, testing the ability of the model
    to learn effectively with limited information. Table [3](#S4.T3 "Table 3 ‣ Implementation
    Details. ‣ 4 Experiments ‣ Taming Pre-trained LLMs for Generalised Time Series
    Forecasting via Cross-modal Knowledge Distillation") demonstrates that our method
    outperforms other baselines, highlighting its robustness in the few-shot setting.
    Compared with GPT4TS and PatchTST, our method achieves an average reduction of
    8% and 9%, respectively.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个 ETT 数据集上进行了少-shot 实验。具体而言，对于每个数据集，我们仅使用了前10%的训练数据。这种受限的数据场景提出了相当大的挑战，测试了模型在有限信息下有效学习的能力。表格
    [3](#S4.T3 "Table 3 ‣ Implementation Details. ‣ 4 Experiments ‣ Taming Pre-trained
    LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation")
    展示了我们的方法优于其他基准，突出了其在少-shot 环境下的鲁棒性。与 GPT4TS 和 PatchTST 相比，我们的方法分别实现了 8% 和 9% 的平均减少。
- en: Zero-shot Learning.
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零-shot 学习。
- en: Going beyond few-shot scenarios, we further delve into zero-shot learning, where
    LLMs demonstrate their prowess as adept and intuitive reasoners. In this setting,
    models trained on one dataset $\blacklozenge$, without any further training. As
    shown in Table [4](#S4.T4 "Table 4 ‣ Implementation Details. ‣ 4 Experiments ‣
    Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal
    Knowledge Distillation"), our method stands out for its exceptional performance,
    surpassing GPT4TS and PatchTST by 4% and 9% respectively. This indicates that
    our approach significantly enhances the model’s capability for effective learning
    transfer across different domains.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 超越少-shot 场景，我们进一步深入探讨了零-shot 学习，其中 LLM 展现了作为熟练和直觉推理者的能力。在这种设置中，模型在一个数据集上进行训练
    $\blacklozenge$，无需进一步训练。如表格 [4](#S4.T4 "Table 4 ‣ Implementation Details. ‣ 4
    Experiments ‣ Taming Pre-trained LLMs for Generalised Time Series Forecasting
    via Cross-modal Knowledge Distillation") 所示，我们的方法因其卓越的表现脱颖而出，分别超越了 GPT4TS 和 PatchTST
    4% 和 9%。这表明我们的方法显著提高了模型在不同领域的有效学习转移能力。
- en: Linear Model Insights.
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性模型洞察。
- en: It is also noteworthy that the performances of two MLP-based models, DLinear
    and TiDE, show great performance under few-shot and zero-shot settings. This can
    be attributed to their linear simplicity and good generalization capabilities.
    Furthermore, the success of these models underscores the potential of linear approaches
    in extracting underlying patterns even in complex datasets (Zeng et al., [2023](#bib.bib36)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，两种基于 MLP 的模型，DLinear 和 TiDE，在少-shot 和零-shot 设置下表现出色。这可以归因于它们的线性简单性和良好的泛化能力。此外，这些模型的成功强调了线性方法在提取复杂数据集中潜在模式的潜力（Zeng
    等，[2023](#bib.bib36)）。
- en: '![Refer to caption](img/b086af3981f2eff9c92066062718b16e.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b086af3981f2eff9c92066062718b16e.png)'
- en: 'Figure 3: Distribution of cross attention weights between different datasets
    and the principal word embeddings. For presentation clarity, we give the distribution
    of attention weights corresponding to the first 100 principal word embeddings
    since it includes over 99% weights. In addition, we also quantitatively give the
    index of the principal word embeddings corresponding to the Top10 weights of each
    dataset.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 不同数据集和主要词嵌入之间的交叉注意力权重分布。为了清晰展示，我们给出了对应前 100 个主要词嵌入的注意力权重分布，因为它包括了超过 99%
    的权重。此外，我们还定量给出了每个数据集中与前 10 个权重对应的主要词嵌入的索引。'
- en: 5 Abaltion Study
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 消融研究
- en: Ablation on Different Loss Functions.
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对不同损失函数的消融研究。
- en: The feature regularization loss transfers dynamic text modality knowledge, while
    the modal consistency loss ensures output coherence across modalities. The supervised
    loss directly guides learning with ground truth data. We analyze the specific
    effects of each proposed loss function as detailed in Table [5](#S5.T5 "Table
    5 ‣ Ablation on the Number of Principal Components. ‣ 5 Abaltion Study ‣ Taming
    Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge
    Distillation"). Employing only the supervised loss resulted in MSE/MAE of 0.436/0.428
    on ETTh1 and 0.253/0.276 on Weather, respectively. The addition of feature regularization
    loss or model consistency loss led to incremental improvements, with the best
    performance observed when all three losses were combined, achieving the lowest
    MSE and MAE on both datasets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 特征正则化损失传递了动态文本模态知识，而模态一致性损失确保了跨模态的输出一致性。监督损失则直接通过真实数据指导学习。我们分析了每个提出的损失函数的具体效果，如表[5](#S5.T5
    "表 5 ‣ 主成分数量的消融实验 ‣ 5 消融研究 ‣ 通过跨模态知识蒸馏来驯化预训练的语言模型以进行广义时间序列预测")所述。仅使用监督损失在ETTh1上的MSE/MAE为0.436/0.428，在天气数据集上的MSE/MAE为0.253/0.276。增加特征正则化损失或模态一致性损失带来了渐进的改进，当三种损失函数结合使用时，性能最佳，在两个数据集上都实现了最低的MSE和MAE。
- en: Ablation on the Number of Principal Components.
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主成分数量的消融实验。
- en: We employ PCA to conduct dimensional reduction on the original word embeddings
    for efficient training. Despite the reduced cost, however, PCA may inevitably
    lead to information loss. In this section, we ablate the number of principal components
    $d$, which can attain an explainable variance ratio of 88% while achieving satisfactory
    performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PCA对原始词嵌入进行降维以提高训练效率。然而，尽管降低了成本，PCA不可避免地可能导致信息丢失。在这一部分，我们消融了主成分数量$d$，它可以获得88%的可解释方差比，同时达到令人满意的性能。
- en: '| feature | consist | supervised | ETTh1 | Weather |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 包含 | 监督 | ETTh1 | 天气 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  MSE  |  MAE  |  MSE  |  MAE  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  均方误差 (MSE)  |  平均绝对误差 (MAE)  |  均方误差 (MSE)  |  平均绝对误差 (MAE)  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $-$ | ✓ | 0.436 | 0.428 | 0.253 | 0.276 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| $-$ | ✓ | 0.436 | 0.428 | 0.253 | 0.276 |'
- en: '| ✓ | $-$ | ✓ | 0.434 | 0.431 | 0.254 | 0.276 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | $-$ | ✓ | 0.434 | 0.431 | 0.254 | 0.276 |'
- en: '| $-$ | ✓ | ✓ | 0.438 | 0.426 | 0.258 | 0.283 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| $-$ | ✓ | ✓ | 0.438 | 0.426 | 0.258 | 0.283 |'
- en: '| ✓ | ✓ | ✓ | 0.432 | 0.428 | 0.250 | 0.274 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | 0.432 | 0.428 | 0.250 | 0.274 |'
- en: 'Table 5: Ablation on different loss functions. ‘feature’ denotes feature regularization
    loss. ‘consist’ denotes modal consistency loss. ‘supervised’ denotes supervised
    loss. All the results are averaged from 4 different prediction lengths $H\in\{96,192,336,720\}$.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同损失函数的消融实验。‘feature’表示特征正则化损失。‘consist’表示模态一致性损失。‘supervised’表示监督损失。所有结果都取自4种不同预测长度$H\in\{96,192,336,720\}$的平均值。
- en: 6 Discussion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Why Implicit Textual Tokens.
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么使用隐式文本标记。
- en: Here, we would like to discuss why implicit word embedding vectors are adopted
    instead of the more intuitive natural language description. As demonstrated in (Jin
    et al., [2023a](#bib.bib18)), it is hard to describe a time series losslessly
    through limited natural language, and it is also cumbersome to find accurate descriptions.
    Therefore, we use an implicit approach to learn static knowledge from frozen word
    embedding.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们想讨论为什么采用隐式词嵌入向量而不是更直观的自然语言描述。正如在(Jin et al., [2023a](#bib.bib18))中所展示的，通过有限的自然语言描述时间序列是困难的，而且找到准确的描述也是繁琐的。因此，我们采用隐式方法从冻结的词嵌入中学习静态知识。
- en: Difference form Other Work.
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与其他工作的差异。
- en: In addition, one concurrent work (Jin et al., [2023a](#bib.bib18)) also considers
    cross attention to extracting knowledge from the word embedding layer, and we
    would like to clarify the difference to emphasize our contribution. First, the
    existing method uses cross attention to generate embeddings and combines them
    with prompt prefixes as input to frozen LLMs, while our LLaTA aims to generate
    aligned textual tokens as the input of the textual modal branch for subsequent
    cross-modal distillation. Second, previous work introduces linear weight $W\in\mathbb{R}^{|\mathcal{A}|\times
    d}$, this solution can lead to significant costs, while our approach uses an offline
    manner to generate synonym clusters, which guarantees efficiency.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，另一项相关工作 (Jin 等，[2023a](#bib.bib18)) 也考虑了从词嵌入层提取知识的交叉注意力，我们希望澄清差异以强调我们的贡献。首先，现有方法使用交叉注意力生成嵌入，并将其与提示前缀结合作为冻结LLM的输入，而我们的LLaTA旨在生成对齐的文本标记作为文本模态分支的输入，以进行后续的跨模态蒸馏。其次，以前的工作引入了线性权重
    $W\in\mathbb{R}^{|\mathcal{A}|\times d}$，该解决方案可能导致显著成本，而我们的方法采用离线方式生成同义词集群，从而保证效率。
- en: Interpretability on Implicit Alignment.
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 隐式对齐的可解释性。
- en: To narrow the temporal-textual modality gap, we perform cross attention on word
    embedding weights to generate aligned text tokens instead of intuitive natural
    language. To understand this implicit alignment, we visualize the distribution
    of cross attention weights on different datasets in  [Fig. 3](#S4.F3 "In Linear
    Model Insights. ‣ 4.3 Few/zero-shot Learning ‣ 4 Experiments ‣ Taming Pre-trained
    LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation")
    and derive some interesting findings. Firstly, the distribution of attention weights
    is diverse across datasets, suggesting that different types of time series attend
    to different principal word embeddings. In addition, we also found an interesting
    phenomenon that similar datasets exhibit similar behavior, e.g., the ETT datasets
    ETTh1, ETTh2, and ETTm1 have similar distribution shapes, while the weather-related
    time series exhibit significantly different distributions, suggesting that similar
    domain may share some common implicit principal word embeddings.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩小时间-文本模态间的差距，我们对词嵌入权重执行交叉注意力以生成对齐的文本标记，而不是直观的自然语言。为了理解这种隐式对齐，我们在 [图 3](#S4.F3
    "在线性模型见解。 ‣ 4.3 少样本/零样本学习 ‣ 4 实验 ‣ 通过跨模态知识蒸馏驯化预训练LLMs进行广义时间序列预测") 中可视化了不同数据集上的交叉注意力权重分布，并得出了一些有趣的发现。首先，注意力权重的分布在不同数据集上各异，这表明不同类型的时间序列关注不同的主要词嵌入。此外，我们还发现一个有趣的现象，即相似的数据集表现出类似的行为，例如，ETT
    数据集 ETTh1、ETTh2 和 ETTm1 具有类似的分布形状，而与天气相关的时间序列则表现出显著不同的分布，表明相似领域可能共享一些共同的隐式主要词嵌入。
- en: '![Refer to caption](img/8a6367df18b58834ae6ab92f8ff84fd9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a6367df18b58834ae6ab92f8ff84fd9.png)'
- en: 'Figure 4: Ablation on different low dimension $d$ of PCA on (a) ETTh1 and (b)
    ETTh2 datasets.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同低维 $d$ 的 PCA 在 (a) ETTh1 和 (b) ETTh2 数据集上的消融实验。
- en: 7 Conclusion
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we propose LLaTA, a novel cross-modal knowledge distillation framework
    that leverages the robust capabilities of Large Language Models (LLMs) for time
    series forecasting. Our approach effectively bridges the inherent modality gap
    between temporal data and the textual nature of LLMs. By distilling both input-agnostic
    static and input-dependent dynamic knowledge from LLMs, LLaTA not only improves
    time series forecasting performance but also substantially enhances generalization
    capabilities. Extensive experiments across several real-world datasets validate
    that LLaTA sets a new benchmark in both long- and short-term forecasting. This
    work paves the way for future research in integrating diverse modalities with
    LLMs, promising more versatile and powerful applications in time series analysis
    and beyond. Future efforts could focus on enriching LLMs with explicit time series
    knowledge and building multi-modal models with joint reasoning across modalities
    via advanced pre-training techniques.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 LLaTA，一种新颖的跨模态知识蒸馏框架，利用了大型语言模型（LLMs）在时间序列预测中的强大能力。我们的方法有效地弥合了时间数据与
    LLMs 文本特性之间的固有模态差距。通过从 LLMs 中提取输入无关的静态知识和输入依赖的动态知识，LLaTA 不仅提高了时间序列预测的性能，还显著增强了泛化能力。对多个真实世界数据集的广泛实验验证了
    LLaTA 在长期和短期预测中都设立了新的基准。这项工作为未来将多样化模态与 LLMs 整合的研究铺平了道路，预示着在时间序列分析及其他领域中可能出现更为多样化和强大的应用。未来的工作可以集中于丰富
    LLMs 的明确时间序列知识，并通过先进的预训练技术构建具有跨模态联合推理的多模态模型。
- en: References
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774,
    2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人. GPT-4 技术报告. arXiv 预印本 arXiv:2303.08774, 2023.
- en: Angryk et al. [2020] Rafal A Angryk, Petrus C Martens, Berkay Aydin, Dustin
    Kempton, Sushant S Mahajan, Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina
    Filali Boubrahimi, Shah Muhammad Hamdi, et al. Multivariate time series dataset
    for space weather data analytics. Scientific data, 7(1):227, 2020.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angryk 等人 [2020] Rafal A Angryk, Petrus C Martens, Berkay Aydin, Dustin Kempton,
    Sushant S Mahajan, Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina Filali
    Boubrahimi, Shah Muhammad Hamdi, 等人. 用于空间天气数据分析的多变量时间序列数据集. Scientific data, 7(1):227,
    2020.
- en: Bai et al. [2018] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical
    evaluation of generic convolutional and recurrent networks for sequence modeling.
    arXiv preprint arXiv:1803.01271, 2018.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2018] Shaojie Bai, J Zico Kolter, 和 Vladlen Koltun. 对序列建模的通用卷积和递归网络的实证评估.
    arXiv 预印本 arXiv:1803.01271, 2018.
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. Advances in Neural Information
    Processing Systems, 33:1877–1901, 2020.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等人. 语言模型是少样本学习者. Advances in Neural Information Processing Systems,
    33:1877–1901, 2020.
- en: 'Cao et al. [2023] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang
    Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer
    for time series forecasting. arXiv preprint arXiv:2310.04948, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao 等人 [2023] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng,
    Wen Ye, 和 Yan Liu. TEMPO: 基于提示的生成预训练变换器用于时间序列预测. arXiv 预印本 arXiv:2310.04948, 2023.'
- en: 'Challu et al. [2022] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico
    Garza, Max Mergenthaler, and Artur Dubrawski. N-HiTs: Neural hierarchical interpolation
    for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Challu 等人 [2022] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico
    Garza, Max Mergenthaler, 和 Artur Dubrawski. N-HiTs: 用于时间序列预测的神经层次插值. arXiv 预印本
    arXiv:2201.12886, 2022.'
- en: 'Chang et al. [2023] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. LLM4TS: Two-stage
    fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint
    arXiv:2308.08469, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人 [2023] Ching Chang, Wen-Chih Peng, 和 Tien-Fu Chen. LLM4TS: 基于预训练语言模型的时间序列预测的两阶段微调.
    arXiv 预印本 arXiv:2308.08469, 2023.'
- en: Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. A simple framework for contrastive learning of visual representations.
    In International Conference on Machine Learning, pages 1597–1607\. PMLR, 2020.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, 和 Geoffrey Hinton.
    用于视觉表示对比学习的简单框架. 发表在国际机器学习会议, 页码 1597–1607. PMLR, 2020.
- en: Dai et al. [2021] Rui Dai, Srijan Das, and François Bremond. Learning an augmented
    rgb representation with cross-modal knowledge distillation for action detection.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    13053–13064, 2021.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2021] Rui Dai、Srijan Das 和 François Bremond。通过跨模态知识蒸馏学习增强的 rgb 表示用于动作检测。发表于
    IEEE/CVF 国际计算机视觉大会论文集，第 13053–13064 页，2021。
- en: Dai et al. [2024] Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong
    Jiang, and Shu-Tao Xia. Periodicity decoupling framework for long-term series
    forecasting. In International Conference on Learning Representations, 2024.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2024] Tao Dai、Beiliang Wu、Peiyuan Liu、Naiqi Li、Jigang Bao、Yong Jiang
    和 Shu-Tao Xia。用于长期系列预测的周期性解耦框架。发表于国际学习表征会议，2024。
- en: 'Das et al. [2023] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and
    Rose Yu. Long-term forecasting with TiDE: Time-series dense encoder. arXiv preprint
    arXiv:2304.08424, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等人 [2023] Abhimanyu Das、Weihao Kong、Andrew Leach、Rajat Sen 和 Rose Yu。使用
    TiDE 进行长期预测：时间序列密集编码器。arXiv 预印本 arXiv:2304.08424，2023。
- en: Demirel et al. [2012] Ömer Fahrettin Demirel, Selim Zaim, Ahmet Çalişkan, and
    Pinar Özuyar. Forecasting natural gas consumption in istanbul using neural networks
    and multivariate time series methods. Turkish Journal of Electrical Engineering
    and Computer Sciences, 20(5):695–711, 2012.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demirel 等人 [2012] Ömer Fahrettin Demirel、Selim Zaim、Ahmet Çalişkan 和 Pinar Özuyar。使用神经网络和多变量时间序列方法预测伊斯坦布尔的天然气消费。《土耳其电气工程与计算机科学期刊》，20(5)：695–711，2012。
- en: 'Gou et al. [2021] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng
    Tao. Knowledge distillation: A survey. International Journal of Computer Vision,
    129:1789–1819, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等人 [2021] Jianping Gou、Baosheng Yu、Stephen J Maybank 和 Dacheng Tao。知识蒸馏：一项综述。《计算机视觉国际期刊》，129：1789–1819，2021。
- en: Guo et al. [2023] Hang Guo, Tao Dai, Mingyan Zhu, Guanghao Meng, Bin Chen, Zhi
    Wang, and Shu-Tao Xia. One-stage low-resolution text recognition with high-resolution
    knowledge transfer. In Proceedings of the 31st ACM International Conference on
    Multimedia, pages 2189–2198, 2023.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2023] Hang Guo、Tao Dai、Mingyan Zhu、Guanghao Meng、Bin Chen、Zhi Wang 和
    Shu-Tao Xia。单阶段低分辨率文本识别与高分辨率知识转移。发表于第 31 届 ACM 国际多媒体大会论文集，第 2189–2198 页，2023。
- en: Hinton et al. [2015] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人 [2015] Geoffrey Hinton、Oriol Vinyals 和 Jeff Dean。蒸馏神经网络中的知识。arXiv
    预印本 arXiv:1503.02531，2015。
- en: Hu et al. [2018] Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li,
    Nan Yang, and Ming Zhou. Attention-guided answer distillation for machine reading
    comprehension. arXiv preprint arXiv:1808.07644, 2018.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2018] Minghao Hu、Yuxing Peng、Furu Wei、Zhen Huang、Dongsheng Li、Nan Yang
    和 Ming Zhou。基于注意力的答案蒸馏用于机器阅读理解。arXiv 预印本 arXiv:1808.07644，2018。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。LoRA：大规模语言模型的低秩适应。arXiv 预印本 arXiv:2106.09685，2021。
- en: 'Jin et al. [2023a] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-LLM:
    Time series forecasting by reprogramming large language models. arXiv preprint
    arXiv:2310.01728, 2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 [2023a] Ming Jin、Shiyu Wang、Lintao Ma、Zhixuan Chu、James Y Zhang、Xiaoming
    Shi、Pin-Yu Chen、Yuxuan Liang、Yuan-Fang Li、Shirui Pan 等。Time-LLM：通过重新编程大型语言模型进行时间序列预测。arXiv
    预印本 arXiv:2310.01728，2023。
- en: Jin et al. [2023b] Yufeng Jin, Guosheng Hu, Haonan Chen, Duoqian Miao, Liang
    Hu, and Cairong Zhao. Cross-modal distillation for speaker recognition. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 37, pages 12977–12985,
    2023.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 [2023b] Yufeng Jin、Guosheng Hu、Haonan Chen、Duoqian Miao、Liang Hu 和 Cairong
    Zhao。用于说话人识别的跨模态蒸馏。发表于 AAAI 人工智能会议论文集，第 37 卷，第 12977–12985 页，2023。
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2014] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。arXiv 预印本 arXiv:1412.6980，2014。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
    in Neural Information Processing Systems, 35:22199–22213, 2022.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人 [2022] Takeshi Kojima、Shixiang Shane Gu、Machel Reid、Yutaka Matsuo
    和 Yusuke Iwasawa。大型语言模型是零样本推理器。《神经信息处理系统进展》，35：22199–22213，2022。
- en: Liu et al. [2023a] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy,
    Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and
    Shwetak Patel. Large language models are few-shot health learners. arXiv preprint
    arXiv:2305.15525, 2023.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy,
    Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, 和 Shwetak
    Patel。大型语言模型是少量样本健康学习者。arXiv预印本 arXiv:2305.15525, 2023年。
- en: 'Liu et al. [2023b] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang,
    Lintao Ma, and Mingsheng Long. iTransformer: Inverted transformers are effective
    for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023b] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang,
    Lintao Ma, 和 Mingsheng Long。iTransformer: 反向变换器在时间序列预测中的有效性。arXiv预印本 arXiv:2310.06625,
    2023年。'
- en: Mikolov et al. [2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781,
    2013.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. [2013] Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean。高效估计向量空间中的词表示。arXiv预印本
    arXiv:1301.3781, 2013年。
- en: 'Nie et al. [2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
    A time series is worth 64 words: Long-term forecasting with transformers. In International
    Conference on Learning Representations, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie et al. [2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, 和 Jayant Kalagnanam。一个时间序列值得64个词：使用变换器的长期预测。发表于国际学习表征会议，2023年。
- en: 'Oreshkin et al. [2019] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and
    Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time
    series forecasting. International Conference on Learning Representations, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oreshkin et al. [2019] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, 和
    Yoshua Bengio。N-BEATS: 具有可解释性的时间序列预测的神经基础扩展分析。发表于国际学习表征会议，2019年。'
- en: Patton [2013] Andrew Patton. Copula methods for forecasting multivariate time
    series. Handbook of economic forecasting, 2:899–960, 2013.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patton [2013] Andrew Patton。用于预测多变量时间序列的copula方法。《经济预测手册》，2:899–960，2013年。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    2019.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等。语言模型是无监督多任务学习者。2019年。
- en: Spyros Makridakis [2018] Spyros Makridakis. M4 dataset, 2018.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spyros Makridakis [2018] Spyros Makridakis。M4数据集，2018年。
- en: 'Vobecky et al. [2022] Antonin Vobecky, David Hurych, Oriane Siméoni, Spyros
    Gidaris, Andrei Bursuc, Patrick Pérez, and Josef Sivic. Drive&segment: Unsupervised
    semantic segmentation of urban scenes via cross-modal distillation. In European
    Conference on Computer Vision, pages 478–495\. Springer, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vobecky et al. [2022] Antonin Vobecky, David Hurych, Oriane Siméoni, Spyros
    Gidaris, Andrei Bursuc, Patrick Pérez, 和 Josef Sivic。Drive&segment: 通过跨模态蒸馏进行城市场景的无监督语义分割。发表于欧洲计算机视觉会议，页478–495。Springer，2022年。'
- en: 'Wang et al. [2022] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui
    Chen, and Yifei Xiao. MICN: Multi-scale local and global context modeling for
    long-term series forecasting. In International Conference on Learning Representations,
    2022.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2022] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui
    Chen, 和 Yifei Xiao。MICN: 多尺度局部和全局上下文建模用于长期系列预测。发表于国际学习表征会议，2022年。'
- en: 'Woo et al. [2022] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and
    Steven C. H. Hoi. ETSformer: Exponential smoothing transformers for time-series
    forecasting. arXiv preprint arXiv:2202.01381, 2022.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Woo et al. [2022] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, 和 Steven
    C. H. Hoi。ETSformer: 用于时间序列预测的指数平滑变换器。arXiv预印本 arXiv:2202.01381, 2022年。'
- en: 'Wu et al. [2021] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer:
    Decomposition transformers with Auto-Correlation for long-term series forecasting.
    In Advances in Neural Information Processing Systems, 2021.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2021] Haixu Wu, Jiehui Xu, Jianmin Wang, 和 Mingsheng Long。Autoformer:
    具有自动相关性的分解变换器用于长期系列预测。发表于神经信息处理系统进展，2021年。'
- en: 'Wu et al. [2023] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and
    Mingsheng Long. TimesNet: Temporal 2d-variation modeling for general time series
    analysis. In International Conference on Learning Representations, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2023] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, 和
    Mingsheng Long。TimesNet: 一般时间序列分析的时间2d-变异建模。发表于国际学习表征会议，2023年。'
- en: Xu et al. [2020] Guodong Xu, Ziwei Liu, Xiaoxiao Li, and Chen Change Loy. Knowledge
    distillation meets self-supervision. In European Conference on Computer Vision,
    pages 588–604\. Springer, 2020.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2020] Guodong Xu, Ziwei Liu, Xiaoxiao Li, 和 Chen Change Loy。知识蒸馏遇上自我监督。发表于欧洲计算机视觉会议，页588–604。Springer，2020年。
- en: Zeng et al. [2023] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers
    effective for time series forecasting? In Proceedings of the AAAI conference on
    artificial intelligence, volume 37, pages 11121–11128, 2023.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. [2023] 曾爱玲、陈慕熙、张磊和徐强。变压器在时间序列预测中是否有效？见于《AAAI人工智能会议论文集》，第37卷，第11121–11128页，2023年。
- en: 'Zhang and Yan [2023] Yunhao Zhang and Junchi Yan. Crossformer: Transformer
    utilizing cross-dimension dependency for multivariate time series forecasting.
    In International Conference on Learning Representations, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang and Yan [2023] 张云浩和闫军驰。Crossformer：利用跨维度依赖的变压器进行多变量时间序列预测。见于《学习表示国际会议》，2023年。
- en: 'Zhou et al. [2022] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun,
    and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term
    series forecasting. In International Conference on Machine Learning, pages 27268–27286\.
    PMLR, 2022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] 周天、马紫清、温青松、王雪、孙亮和金戎。FEDformer：频率增强分解变压器用于长期序列预测。见于《国际机器学习会议》，第27268–27286页。PMLR，2022年。
- en: 'Zhou et al. [2023] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin.
    One Fits All: Power general time series analysis by pretrained lm. Advances in
    Neural Information Processing Systems, 36, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2023] 周天、牛佩松、王雪、孙亮和金戎。One Fits All：通过预训练语言模型进行强大的通用时间序列分析。见于《神经信息处理系统进展》，第36卷，2023年。
