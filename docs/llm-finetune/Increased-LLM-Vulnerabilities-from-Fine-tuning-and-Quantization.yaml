- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:37:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Increased LLM Vulnerabilities from Fine-tuning and Quantization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调和量化导致 LLM 脆弱性增加
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04392](https://ar5iv.labs.arxiv.org/html/2404.04392)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04392](https://ar5iv.labs.arxiv.org/html/2404.04392)
- en: Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal & Prashanth Harshangi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Divyanshu Kumar、Anurakt Kumar、Sahil Agarwal 和 Prashanth Harshangi
- en: Enkrypt AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Enkrypt AI
- en: '{divyanshu, anurakt, sahil, prashanth}@enkryptai.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{divyanshu, anurakt, sahil, prashanth}@enkryptai.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have become very popular and have found use cases
    in many domains, such as chatbots, auto-task completion agents, and much more.
    However, LLMs are vulnerable to different types of attacks, such as jailbreaking,
    prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo
    adversarial and alignment training to learn not to generate malicious and toxic
    content. For specialized use cases, these foundational LLMs are subjected to fine-tuning
    or quantization for better performance and efficiency. We examine the impact of
    downstream tasks such as fine-tuning and quantization on LLM vulnerability. We
    test foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions.
    Our research shows that fine-tuning and quantization reduces jailbreak resistance
    significantly, leading to increased LLM vulnerabilities. Finally, we demonstrate
    the utility of external guardrails in reducing LLM vulnerabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经变得非常流行，并在许多领域找到了应用场景，如聊天机器人、自动任务完成代理等。然而，LLMs 易受到不同类型的攻击，如越狱攻击、提示注入攻击和隐私泄露攻击。基础
    LLMs 经过对抗性和对齐训练，以学习不生成恶意和有害内容。对于专业化的应用场景，这些基础 LLMs 会进行微调或量化，以提高性能和效率。我们研究了下游任务如微调和量化对
    LLM 脆弱性的影响。我们测试了基础模型，如 Mistral、Llama、MosaicML 及其微调版本。我们的研究表明，微调和量化显著降低了越狱抵抗能力，从而增加了
    LLM 的脆弱性。最后，我们展示了外部保护措施在减少 LLM 脆弱性方面的实用性。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative models are becoming more and more important as they are becoming
    capable of automating a lot of tasks, taking autonomous actions and decisions,
    and at the same time, becoming better at content generation and summarization
    tasks. As LLMs become more powerful, these capabilities are at risk of being misused
    by an adversary, which can lead to fake content generation, toxic, malicious,
    or hateful content generation, privacy leakages, copyrighted content generation,
    and much more Chao et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14))
    Zou et al. ([2023](#bib.bib23)) Greshake et al. ([2023](#bib.bib6)) Liu et al.
    ([2023](#bib.bib13)) Zhu et al. ([2023](#bib.bib22)) He et al. ([2021](#bib.bib7))
    Le et al. ([2020](#bib.bib12)). To prevent LLMs from generating content that contradicts
    human values and to prevent their malicious misuse, they undergo a supervised
    fine-tuning phase after their pre-training, and they are further evaluated by
    humans and trained using reinforcement learning from human feedback (RLHF) Ouyang
    et al. ([2022](#bib.bib15)), to make them more aligned with human values. Further,
    special filters called guardrails are put in place as filters to prevent LLMs
    from getting toxic prompts as inputs and outputting toxic or copyrighted responses
    Rebedea et al. ([2023](#bib.bib17)) Kumar et al. ([2023](#bib.bib11)) Wei et al.
    ([2023](#bib.bib18)) Zhou et al. ([2024](#bib.bib21)). The complexity of human
    language makes it difficult for LLMs to completely understand what instructions
    are right and which are wrong in terms of human values. After going through the
    alignment training and after the implementation of guardrails, it becomes unlikely
    that the LLM will generate a toxic response. But these safety measures can easily
    be circumvented using adversarial attacks, and the LLM can be jailbroken to generate
    any content that the adversary wants, as shown in recent works Chao et al. ([2023](#bib.bib2))
    Mehrotra et al. ([2023](#bib.bib14)) Zhu et al. ([2023](#bib.bib22)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型变得越来越重要，因为它们能够自动化许多任务、采取自主行动和决策，同时在内容生成和总结任务上表现越来越好。随着大型语言模型（LLMs）变得越来越强大，这些能力可能会被对手滥用，从而导致生成虚假内容、毒性、有害或仇恨内容、隐私泄露、版权内容生成等问题
    Chao et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14)) Zou et al.
    ([2023](#bib.bib23)) Greshake et al. ([2023](#bib.bib6)) Liu et al. ([2023](#bib.bib13))
    Zhu et al. ([2023](#bib.bib22)) He et al. ([2021](#bib.bib7)) Le et al. ([2020](#bib.bib12))。为了防止LLMs生成与人类价值观相悖的内容，并防止其恶意滥用，在预训练之后，它们会经过监督的微调阶段，并由人工进一步评估和通过人类反馈的强化学习（RLHF）进行训练
    Ouyang et al. ([2022](#bib.bib15))，以使其更加符合人类价值观。此外，还会设置称为保护措施的特殊过滤器，以防止LLMs接收有毒提示作为输入并输出有毒或侵犯版权的回应
    Rebedea et al. ([2023](#bib.bib17)) Kumar et al. ([2023](#bib.bib11)) Wei et al.
    ([2023](#bib.bib18)) Zhou et al. ([2024](#bib.bib21))。人类语言的复杂性使得LLMs很难完全理解哪些指令在伦理上是正确的，哪些是错误的。经过对齐训练和保护措施的实施之后，LLM生成有毒回应的可能性变得很小。但这些安全措施可以通过对抗性攻击轻易绕过，并且LLM可以被破解以生成对手想要的任何内容，如近期的研究所示
    Chao et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14)) Zhu et al.
    ([2023](#bib.bib22))。
- en: 'Recent works such as the Prompt Automatic Iterative Refinement (PAIR) attacks
    Chao et al. ([2023](#bib.bib2)) and Tree-of-attacks pruning (TAP) Mehrotra et al.
    ([2023](#bib.bib14)) have shown the vulnerability of LLMs and how easy it is to
    jailbreak them into generating content for harmful tasks specified by the user.
    Similarly, a class of methods called privacy leakage attacks Debenedetti et al.
    ([2023](#bib.bib3)) are used to attack LLMs to extract their training data or
    personally identifiable information Kim et al. ([2023](#bib.bib10)), and prompt
    injection attacks can be used to make an LLM application perform tasks that are
    not requested by the user but are hidden in the third-party instruction which
    the LLM automatically executes. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Increased LLM Vulnerabilities from Fine-tuning and Quantization") shows how an
    instruction can be hidden inside a summarization text and how the LLM will ignore
    the previous instruction to execute the malicious instruction. Qi et al. ([2023](#bib.bib16))
    showed that it only takes a few examples to fine-tune an LLM into generating toxic
    responses by forgetting its safety training. Our work in this paper extends that
    notion and shows that both fine-tuning the LLM on any task (not necessarily toxic
    content generation) and quantization can affect its safety training. In this study,
    we use a subset of adversarial harmful prompts called AdvBench SubsetAndy Zou
    ([2023](#bib.bib1)). It contains 50 prompts asking for harmful information across
    32 categories. It is a subset of prompts from the harmful behaviours dataset in
    the AdvBench benchmark selected to cover a diverse range of harmful prompts. The
    attacking algorithm used is tree-of-attacks pruning Mehrotra et al. ([2023](#bib.bib14))
    as it has shown to have the best performance in jailbreaking and, more importantly,
    this algorithm fulfils three important goals (1) Black-box: the algorithm only
    needs black-box access to the model (2) Automatic: it does not need human intervention
    once started, and (3) Interpretable: the algorithm generates semantically meaningful
    prompts. The TAP algorithm is used with the tasks from the AdvBench subset to
    attack the target LLMs in different settings, and their response is used to evaluate
    whether or not they have been jailbroken.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究，如 Prompt Automatic Iterative Refinement (PAIR) Chao 等人 ([2023](#bib.bib2))
    和 Tree-of-attacks pruning (TAP) Mehrotra 等人 ([2023](#bib.bib14))，展示了 LLM 的脆弱性以及如何轻松地将其破解生成用户指定的有害任务内容。类似地，一类称为隐私泄露攻击的方法
    Debenedetti 等人 ([2023](#bib.bib3)) 被用于攻击 LLM，以提取其训练数据或个人身份信息 Kim 等人 ([2023](#bib.bib10))，而提示注入攻击可以使
    LLM 应用执行用户未请求的任务，而这些任务隐藏在第三方指令中，LLM 会自动执行。图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization") 展示了如何将指令隐藏在总结文本中，以及
    LLM 如何忽略之前的指令而执行恶意指令。Qi 等人 ([2023](#bib.bib16)) 表明，只需几个示例即可微调 LLM 生成有毒响应，从而忘记其安全培训。我们在本文中的工作扩展了这一概念，并展示了无论是对
    LLM 进行任何任务（不一定是有毒内容生成）的微调还是量化，都可能影响其安全培训。在这项研究中，我们使用了一部分称为 AdvBench SubsetAndy
    Zou ([2023](#bib.bib1)) 的对抗性有害提示。它包含 50 个请求有害信息的提示，涵盖 32 个类别。它是 AdvBench 基准测试中有害行为数据集的一个子集，旨在涵盖各种有害提示。使用的攻击算法是
    Tree-of-attacks pruning Mehrotra 等人 ([2023](#bib.bib14))，因为它在破解方面表现最佳，更重要的是，该算法满足三个重要目标：（1）黑箱：该算法仅需对模型进行黑箱访问；（2）自动化：一旦开始，不需要人工干预；（3）可解释：该算法生成语义上有意义的提示。使用
    TAP 算法和 AdvBench 子集中的任务来攻击目标 LLM，在不同设置下测试其响应，以评估是否已被破解。
- en: '![Refer to caption](img/d9934cea2f2064860b4868467c9271aa.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9934cea2f2064860b4868467c9271aa.png)'
- en: 'Figure 1: An example of an adversarial attack on LLM. Here, GPT-3.5 ignores
    the original instruction of summarizing the text and executes the last instruction
    in angle brackets hidden in the text'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个对大型语言模型（LLM）进行对抗攻击的示例。在这里，GPT-3.5 忽略了总结文本的原始指令，而执行了文本中隐藏在尖括号中的最后一条指令。
- en: The rest of the paper is organized in the following manner. Section [2](#S2
    "2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization") talks about the experimental setup for jailbreaking in which
    the models are tested. It specifically describes the different modes of downstream
    process that an LLM has undergone e.g fine-tuning, quantization and tested for
    these modes. Section [3](#S3 "3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization") describes the experiment set-up used and defines
    guardrails Rebedea et al. ([2023](#bib.bib17)), fine-tuning and quantization Kashiwamura
    et al. ([2024](#bib.bib9)) Gorsline et al. ([2021](#bib.bib5)) Xiao et al. ([2023](#bib.bib20))
    Hu et al. ([2021](#bib.bib8)) Dettmers et al. ([2023](#bib.bib4)) settings used
    in the experimental context. We demonstrate the results in detail and show how
    model vulnerability is affected by downstream tasks for LLMs. Finally, section
    [4](#S4 "4 Conclusion ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")
    concludes the study and talks about methods to reduce model vulnerability and
    ensure safe and reliable LLM development.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。第[2](#S2 "2 Problem Formulation and Experiments ‣ Increased LLM
    Vulnerabilities from Fine-tuning and Quantization")节讨论了破解实验的设置，其中测试了模型。它具体描述了LLM经历的不同下游处理模式，例如细化、量化，并测试这些模式。第[3](#S3
    "3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization")节描述了使用的实验设置，并定义了防护措施 Rebedea et al. ([2023](#bib.bib17))，细化和量化
    Kashiwamura et al. ([2024](#bib.bib9)) Gorsline et al. ([2021](#bib.bib5)) Xiao
    et al. ([2023](#bib.bib20)) Hu et al. ([2021](#bib.bib8)) Dettmers et al. ([2023](#bib.bib4))
    在实验背景下使用的设置。我们详细展示了结果，并说明了下游任务如何影响LLM的模型脆弱性。最后，第[4](#S4 "4 Conclusion ‣ Increased
    LLM Vulnerabilities from Fine-tuning and Quantization")节总结了研究，并讨论了减少模型脆弱性的方法，确保LLM的安全和可靠发展。
- en: 2 Problem Formulation and Experiments
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题表述与实验
- en: We want to understand the role played by fine-tuning, quantization, and guardrails
    on LLM’s vulnerability towards jailbreaking attacks. We create a pipeline to test
    for jailbreaking of LLMs which undergo these further processes before deployment.
    As mentioned in the introduction, we attack the LLM via the TAP algorithm using
    the AdvBench subset. We use a subset of AdvBenchAndy Zou ([2023](#bib.bib1)).
    It contains 50 prompts asking for harmful information across 32 categories. The
    evaluation results, along with the complete system information, are then logged.
    The overall flow is shown in the figure [2](#S2.F2 "Figure 2 ‣ 2 Problem Formulation
    and Experiments ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization").
    This process continues for multiple iterations, taking into account the stochastic
    nature associated with LLMs. The complete experiment pipeline is shown in Figure
    [2](#S2.F2 "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization").
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望了解细化、量化和防护措施对大型语言模型（LLM）在破解攻击方面的脆弱性所起的作用。我们创建了一个测试管道，用于检测在部署前经历这些进一步处理的LLM的破解情况。如引言中提到的，我们通过使用AdvBench子集的TAP算法对LLM进行攻击。我们使用了AdvBenchAndy
    Zou ([2023](#bib.bib1))的一个子集，包含50个请求有害信息的提示，覆盖32个类别。评估结果和完整的系统信息会被记录下来。整体流程如图[2](#S2.F2
    "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization")所示。这个过程会进行多次迭代，考虑到与LLM相关的随机性。完整的实验管道如图[2](#S2.F2
    "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization")所示。
- en: '![Refer to caption](img/5289f526979f8e92057d57f6556b25a6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5289f526979f8e92057d57f6556b25a6.png)'
- en: 'Figure 2: Jailbreak process followed to generate the reports'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于生成报告的破解过程
- en: TAP Mehrotra et al. ([2023](#bib.bib14)) is used as the jailbreaking method,
    as it is currently the state-of-the-art, black-box, and automatic method which
    generates prompts with semantic meaning to jailbreak LLMs. TAP algorithm uses
    an attacker LLM A, which sends a prompt P to the target LLM T. The response of
    the target LLM R along with the prompt P are fed into the evaluator LLM JUDGE,
    which determines if the prompt is on-topic or off-topic. If the prompt is off-topic,
    it is removed, thereby eliminating the tree of bad attack prompts it was going
    to generate. Otherwise, if the prompt is on-topic, it receives a score between
    0-10 by the JUDGE LLM. This prompt is used to generate a new attack prompt using
    breadth-first search. This process continues till the LLM is jailbroken or a specified
    number of iterations are exhausted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TAP Mehrotra等（[2023](#bib.bib14)）被用作越狱方法，因为它目前是最先进的、黑盒的和自动的方法，它生成具有语义意义的提示以越狱LLMs。TAP算法使用一个攻击者LLM
    A，它向目标LLM T发送一个提示P。目标LLM R的响应以及提示P被送入评估LLM JUDGE，JUDGE判断提示是否相关。如果提示不相关，它会被移除，从而消除其生成的坏攻击提示树。否则，如果提示相关，JUDGE
    LLM将其评分在0到10之间。这个提示用于通过广度优先搜索生成新的攻击提示。这个过程持续进行，直到LLM被越狱或达到指定的迭代次数。
- en: Now describing our guardrail against jailbreaking prompts, we use our in-house
    Deberta-V3 model, which has been trained to detect jailbreaking prompts. It acts
    as an input filter to ensure that only sanitized prompts are received by the LLM.
    If the input prompt is filtered out by the guardrail or fails to jailbreak the
    LLM, then the TAP algorithm generates a new prompt considering the initial prompt
    and response. The new attacking prompt is then again passed through the guardrail.
    This process is repeated till a jailbreaking prompt is found or a pre-specified
    number of iterations are exhausted.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在描述我们针对越狱提示的保护措施，我们使用我们内部的Deberta-V3模型，该模型已经过训练以检测越狱提示。它作为输入过滤器，以确保只有经过清理的提示被LLM接收。如果输入提示被保护措施过滤掉或未能越狱LLM，那么TAP算法将生成一个新提示，考虑初始提示和响应。新的攻击提示再次通过保护措施。这一过程重复进行，直到找到越狱提示或达到预设的迭代次数。
- en: 3 Experiment Set-up & Results
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置与结果
- en: 'The following section outlines the testing environments utilized for various
    LLMs. The LLMs are tested under three different downstream tasks: (1) fine-tuning,
    (2) quantization, and (3) guardrails. They are chosen to cover most of the LLM
    practical use cases and applications in the industry and academia. For TAP configuration,
    as mentioned before, we use GPT-3.5-turbo as the attack model, and GPT-4-turbo
    as the judge model. We employ Anyscale endpoints, the OpenAI API, and HuggingFace
    for our target model. Further information regarding the model and its sources
    is available in appendix [A.1](#A1.SS1 "A.1 Experiment Utils ‣ Appendix A Appendix
    ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization"). The details
    about different settings are mentioned below :'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分概述了用于各种LLMs的测试环境。这些LLMs在三种不同的下游任务下进行测试：（1）微调，（2）量化，以及（3）保护措施。选择这些任务是为了涵盖LLM在行业和学术界的大多数实际应用场景。对于TAP配置，如前所述，我们使用GPT-3.5-turbo作为攻击模型，GPT-4-turbo作为判断模型。我们使用Anyscale端点、OpenAI
    API和HuggingFace作为我们的目标模型。有关模型及其来源的更多信息，请参见附录 [A.1](#A1.SS1 "A.1 Experiment Utils
    ‣ Appendix A Appendix ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")。不同设置的详细信息如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning: Fine-tuning LLMs on different tasks increases their effectiveness
    in completing the tasks as it incorporates the specialized domain knowledge needed;
    these can include SQL code generation, chat, and more. We compare the jailbreaking
    vulnerability of foundational models compared to their fine tune versions. This
    helps us understand the role of fine-tuning in increasing or decreasing the vulnerability
    of LLMs and the strategies to mitigate this risk Weyssow et al. ([2023](#bib.bib19)).
    We use foundation models such as Llama2, Mistral, and MPT-7B and their fine-tuned
    versions such as CodeLlama, SQLCoder, Dolphin, and Intel Neural Chat. The details
    of the models and versions are specified in the appendix. From the table  [1](#S3.T1
    "Table 1 ‣ 1st item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization"), we empirically conclude that fine-tuned models
    lose their safety alignment and are jailbroken quite easily compared to the foundational
    models.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：在不同任务上对 LLM 进行微调可以提高其完成任务的有效性，因为它融合了所需的专业领域知识；这些任务可以包括 SQL 代码生成、聊天等。我们比较了基础模型与其微调版本的破解脆弱性。这有助于我们了解微调在增加或减少
    LLM 脆弱性中的作用以及缓解这种风险的策略 Weyssow 等 ([2023](#bib.bib19))。我们使用了基础模型，如 Llama2、Mistral
    和 MPT-7B，以及其微调版本，如 CodeLlama、SQLCoder、Dolphin 和 Intel Neural Chat。模型和版本的详细信息在附录中指定。从表
    [1](#S3.T1 "表 1 ‣ 第 1 项 ‣ 3 实验设置与结果 ‣ 通过微调和量化增加的 LLM 脆弱性") 中，我们经验性地得出结论，微调的模型相较于基础模型更容易失去安全对齐，并且更容易被破解。
- en: 'Table 1: Effect of fine-tuning on model vulnerability'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 1：微调对模型脆弱性的影响
- en: '| Model | Derived From | Finetune | Jailbreak(%) |'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型 | 来源 | 微调 | Jailbreak(%) |'
- en: '| Llama2-7B | – | – | 6 |'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | – | – | 6 |'
- en: '| CodeLlama-7B | Llama2-7B | Yes | 32 |'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | Llama2-7B | 是 | 32 |'
- en: '| SQLCoder-2 | CodeLlama-7B | Yes | 82 |'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| SQLCoder-2 | CodeLlama-7B | 是 | 82 |'
- en: '| Mistral-7B-v0.1 | – | – | 85.3 |'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | – | – | 85.3 |'
- en: '| dolphin-2.2.1-Mistral-7B-v0.1 | Mistral-7B-v0.1 | Yes | 99 |'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| dolphin-2.2.1-Mistral-7B-v0.1 | Mistral-7B-v0.1 | 是 | 99 |'
- en: '| MPT-7B | – | – | 93 |'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| MPT-7B | – | – | 93 |'
- en: '| IntelNeuralChat-7B | MPT-7B | Yes | 94 |'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| IntelNeuralChat-7B | MPT-7B | 是 | 94 |'
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization: Many models require huge computational resources during training,
    fine-tuning, and even during inference Hu et al. ([2021](#bib.bib8)). Quantization
    is one of the most popular ways to reduce the computational burden, but it comes
    at the cost of the numerical precision of model parameters. The quantized models
    we evaluate below use GPT-Generated Unified Format (GGUF) for quantization, which
    involves scaling down model weights (stored in 16-bit floating point numbers)
    to save computational resources at the cost of numerical precision of the model
    parameters. Kashiwamura et al. ([2024](#bib.bib9)) Gorsline et al. ([2021](#bib.bib5))
    Xiao et al. ([2023](#bib.bib20)) Hu et al. ([2021](#bib.bib8)) Dettmers et al.
    ([2023](#bib.bib4)). The table  [2](#S3.T2 "Table 2 ‣ 2nd item ‣ 3 Experiment
    Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")
    demonstrates that quantization of the model renders it susceptible to vulnerabilities.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化：许多模型在训练、微调，甚至推理过程中需要大量计算资源 Hu 等 ([2021](#bib.bib8))。量化是减少计算负担的最流行方法之一，但代价是模型参数的数值精度。我们下面评估的量化模型使用了
    GPT 生成的统一格式（GGUF），这涉及将模型权重（以 16 位浮点数存储）缩小，以节省计算资源，但模型参数的数值精度也因此受到影响。Kashiwamura
    等 ([2024](#bib.bib9)) Gorsline 等 ([2021](#bib.bib5)) Xiao 等 ([2023](#bib.bib20))
    Hu 等 ([2021](#bib.bib8)) Dettmers 等 ([2023](#bib.bib4))。表 [2](#S3.T2 "表 2 ‣ 第
    2 项 ‣ 3 实验设置与结果 ‣ 通过微调和量化增加的 LLM 脆弱性") 表明，模型的量化使其易受到脆弱性的影响。
- en: 'Table 2: Effect of quantization on model vulnerability'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 2：量化对模型脆弱性的影响
- en: '| Model Name | Source Model | Quantization | Jailbreak(%) |'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型名称 | 源模型 | 量化 | Jailbreak(%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7B | – | – | 6 |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | – | – | 6 |'
- en: '| Llama-2-7B-Chat-GGUF-8bit | Llama2-7B | Yes | 9 |'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat-GGUF-8bit | Llama2-7B | 是 | 9 |'
- en: '| CodeLlama-7B | – | – | 32 |'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | – | – | 32 |'
- en: '| CodeLlama-7B-GGUF-8bit | CodeLlama-7B | Yes | 72 |'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF-8bit | CodeLlama-7B | 是 | 72 |'
- en: '| Mistral-7B-v0.1 | – | – | 85.3 |'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | – | – | 85.3 |'
- en: '| Mistral-7B-v0.1-GGUF-8bit | Mistral-7B-v0.1 | Yes | 96 |'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1-GGUF-8bit | Mistral-7B-v0.1 | 是 | 96 |'
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Guardrails: Guardrails act as a line of defence against LLM attacks. Their
    primary function is to meticulously filter out prompts that could potentially
    lead to harmful or malicious outcomes, preventing such prompts from reaching the
    LLM as an instruction Rebedea et al. ([2023](#bib.bib17)). This proactive approach
    not only promotes the safe utilization of LLMs but also facilitates their optimal
    performance, thereby maximizing their potential benefits in various domains Kumar
    et al. ([2023](#bib.bib11)) Wei et al. ([2023](#bib.bib18)) Zhou et al. ([2024](#bib.bib21)).
    We use our proprietary jailbreak attack detector derived from Deberta-V3 models,
    trained on harmful prompts generated to jailbreak LLMs. You can reach out to the
    authors to get more details on this model. From table  [3](#S3.T3 "Table 3 ‣ 3rd
    item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization"), we observe that the introduction of guardrails as a pre-step
    has a significant effect and can mitigate jailbreaking attempts by a considerable
    margin.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保护措施：保护措施作为对抗 LLM 攻击的防线。它们的主要功能是精确过滤出可能导致有害或恶意结果的提示，防止这些提示作为指令到达 LLM Rebedea
    等人 ([2023](#bib.bib17))。这种主动的方法不仅促进了 LLM 的安全使用，还提高了它们的最佳性能，从而最大化其在各个领域的潜在好处 Kumar
    等人 ([2023](#bib.bib11)) Wei 等人 ([2023](#bib.bib18)) Zhou 等人 ([2024](#bib.bib21))。我们使用从
    Deberta-V3 模型派生的专有破解攻击检测器，经过训练以识别用于破解 LLM 的有害提示。您可以联系作者以获取有关该模型的更多细节。从表[3](#S3.T3
    "表 3 ‣ 第 3 项 ‣ 实验设置与结果 ‣ 微调和量化导致的 LLM 脆弱性增加")中，我们观察到引入保护措施作为前置步骤有显著效果，并能显著减轻破解尝试。
- en: 'Table 3: Effect of guardrails on model vulnerability'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 3：保护措施对模型脆弱性的影响
- en: '| Model Name | Jailbreak (%) | Jailbreak w/ guardrails(%) | Factor Improvement
    |'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型名称 | 破解 (%) | 含保护措施的破解 (%) | 改进因子 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7B | 6 | 0.67 | 9x |'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | 6 | 0.67 | 9x |'
- en: '| Mistral-7B-v0.1 | 85.3 | 31.3 | 2.7x |'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | 85.3 | 31.3 | 2.7x |'
- en: '| Mixtral7x8B | 52 | 7 | 7.4x |'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mixtral7x8B | 52 | 7 | 7.4x |'
- en: '| CodeLLama-34B | 18 | 1 | 18x |'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLLama-34B | 18 | 1 | 18x |'
- en: '| CodeLlama-7B | 32 | 2 | 16x |'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | 32 | 2 | 16x |'
- en: '| SQLCoder | 82 | 12.7 | 6.5x |'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| SQLCoder | 82 | 12.7 | 6.5x |'
- en: '| Llama-2-7B-Chat-GGUF | 9 | 1 | 9x |'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat-GGUF | 9 | 1 | 9x |'
- en: '| CodeLlama-7B-GGUF | 72 | 15 | 4.8x |'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF | 72 | 15 | 4.8x |'
- en: '| Phi2 | 97 | 21 | 4.6x |'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Phi2 | 97 | 21 | 4.6x |'
- en: The results from tables  [1](#S3.T1 "Table 1 ‣ 1st item ‣ 3 Experiment Set-up
    & Results ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization"),
     [2](#S3.T2 "Table 2 ‣ 2nd item ‣ 3 Experiment Set-up & Results ‣ Increased LLM
    Vulnerabilities from Fine-tuning and Quantization"), and  [3](#S3.T3 "Table 3
    ‣ 3rd item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from
    Fine-tuning and Quantization") conclusively show the vulnerability of LLMs post
    fine-tuning and quantization, and effectiveness of external guardrails in mitigating
    this vulnerability. The detailed experimental results can be found in appendix
    [A.2](#A1.SS2 "A.2 Experiment Results in details ‣ Appendix A Appendix ‣ Increased
    LLM Vulnerabilities from Fine-tuning and Quantization"). Additionally, this section
    provides a comprehensive analysis of the experiments conducted, offering insights
    into the various aspects of the research findings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格[1](#S3.T1 "表 1 ‣ 第 1 项 ‣ 实验设置与结果 ‣ 微调和量化导致的 LLM 脆弱性增加")、[2](#S3.T2 "表 2
    ‣ 第 2 项 ‣ 实验设置与结果 ‣ 微调和量化导致的 LLM 脆弱性增加")和[3](#S3.T3 "表 3 ‣ 第 3 项 ‣ 实验设置与结果 ‣ 微调和量化导致的
    LLM 脆弱性增加") 的结果明确显示了 LLM 在微调和量化后的脆弱性以及外部保护措施在减轻这种脆弱性方面的有效性。详细的实验结果可以在附录[A.2](#A1.SS2
    "A.2 实验结果详细说明 ‣ 附录 A ‣ 微调和量化导致的 LLM 脆弱性增加")中找到。此外，本节提供了对进行的实验的全面分析，提供了对研究发现各个方面的见解。
- en: 4 Conclusion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: Our work investigates the LLM’s safety against Jailbreak attempts. We have demonstrated
    how fine-tuned and quantized models are vulnerable to jailbreak attempts and stress
    the importance of using external guardrails to reduce this risk. Fine-tuning or
    quantizing model weights alters the risk profile of LLMs, potentially undermining
    the safety alignment established through RLHF. This could result from catastrophic
    forgetting, where LLMs lose memory of safety protocols, or the fine-tuning process
    shifting the model’s focus to new topics at the expense of existing safety measures.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作调查了LLM对越狱尝试的安全性。我们已经展示了精细调优和量化模型如何易受越狱尝试的影响，并强调了使用外部保护措施以降低这一风险的重要性。精细调优或量化模型权重改变了LLM的风险特征，可能会削弱通过RLHF建立的安全对齐。这可能由于灾难性遗忘，LLM失去了对安全协议的记忆，或调优过程将模型的关注点转向新的主题，从而牺牲了现有的安全措施。
- en: The lack of safety measures in these fine-tuned and quantized models is concerning,
    highlighting the need to incorporate safety protocols during the fine-tuning process.
    We propose using these tests as part of a CI/CD stress test before deploying the
    model. The effectiveness of guardrails in preventing jailbreaking highlights the
    importance of integrating them with safety practices in AI development. This approach
    not only enhances AI models but also establishes a new standard for responsible
    AI development. By ensuring that AI advancements prioritize innovation and safety,
    we promote ethical AI deployment, safeguarding against potential misuse and fostering
    a secure digital future.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些精细调优和量化模型中缺乏安全措施令人担忧，这突显了在调优过程中纳入安全协议的必要性。我们建议将这些测试作为模型部署前CI/CD压力测试的一部分。防止越狱的保护措施的有效性强调了将其与AI开发中的安全实践相结合的重要性。这种方法不仅提升了AI模型的性能，还为负责任的AI开发树立了新标准。通过确保AI进步在创新和安全之间保持优先级，我们推动了伦理AI部署，防止潜在的滥用，并促进了安全的数字未来。
- en: References
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Andy Zou (2023) Zifan Wang Andy Zou. AdvBench Dataset, July 2023. URL [https://github.com/llm-attacks/llm-attacks/tree/main/data](https://github.com/llm-attacks/llm-attacks/tree/main/data).
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andy Zou (2023) Zifan Wang Andy Zou. AdvBench 数据集, 2023年7月. URL [https://github.com/llm-attacks/llm-attacks/tree/main/data](https://github.com/llm-attacks/llm-attacks/tree/main/data).
- en: 'Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. Jailbreaking Black Box Large Language Models
    in Twenty Queries. *arXiv*, October 2023. doi: 10.48550/arXiv.2310.08419.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chao等 (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, 和 Eric Wong. 在二十个查询中破解黑箱大型语言模型. *arXiv*, 2023年10月. doi: 10.48550/arXiv.2310.08419.'
- en: 'Debenedetti et al. (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, and
    Florian Tramèr. Privacy Side Channels in Machine Learning Systems. *arXiv*, September
    2023. doi: 10.48550/arXiv.2309.05610.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Debenedetti等 (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, 和
    Florian Tramèr. 机器学习系统中的隐私侧信道. *arXiv*, 2023年9月. doi: 10.48550/arXiv.2309.05610.'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. *arXiv*, May 2023.
    doi: 10.48550/arXiv.2305.14314.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等 (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    QLoRA: 高效精细调优量化LLMs. *arXiv*, 2023年5月. doi: 10.48550/arXiv.2305.14314.'
- en: 'Gorsline et al. (2021) Micah Gorsline, James Smith, and Cory Merkel. On the
    Adversarial Robustness of Quantized Neural Networks. *arXiv*, May 2021. doi: 10.1145/3453688.3461755.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gorsline等 (2021) Micah Gorsline, James Smith, 和 Cory Merkel. 量化神经网络的对抗鲁棒性.
    *arXiv*, 2021年5月. doi: 10.1145/3453688.3461755.'
- en: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. Not what you’ve signed up for: Compromising
    Real-World LLM-Integrated Applications with Indirect Prompt Injection. *arXiv*,
    February 2023. doi: 10.48550/arXiv.2302.12173.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Greshake等 (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, 和 Mario Fritz. 不是你签署的内容：通过间接提示注入来危害现实世界的LLM集成应用. *arXiv*,
    2023年2月. doi: 10.48550/arXiv.2302.12173.'
- en: 'He et al. (2021) Bing He, Mustaque Ahamad, and Srijan Kumar. PETGEN: Personalized
    Text Generation Attack on Deep Sequence Embedding-based Classification Models.
    In *KDD ’21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*, pp.  575–584\. Association for Computing Machinery, New York,
    NY, USA, August 2021. ISBN 978-1-45038332-5. doi: 10.1145/3447548.3467390.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2021) Bing He, Mustaque Ahamad, 和 Srijan Kumar. PETGEN: 基于深度序列嵌入的分类模型的个性化文本生成攻击。在
    *KDD ’21: 第27届ACM SIGKDD知识发现与数据挖掘会议论文集* 中，第575–584页。计算机协会，纽约，NY，美国，2021年8月。ISBN
    978-1-45038332-5。doi: 10.1145/3447548.3467390。'
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of
    Large Language Models. *arXiv*, June 2021. doi: 10.48550/arXiv.2106.09685.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA: 大型语言模型的低秩适配。*arXiv*，2021年6月。doi:
    10.48550/arXiv.2106.09685。'
- en: 'Kashiwamura et al. (2024) Shuhei Kashiwamura, Ayaka Sakata, and Masaaki Imaizumi.
    Effect of Weight Quantization on Learning Models by Typical Case Analysis. *arXiv*,
    January 2024. doi: 10.48550/arXiv.2401.17269.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kashiwamura et al. (2024) Shuhei Kashiwamura, Ayaka Sakata, 和 Masaaki Imaizumi.
    通过典型案例分析权重量化对学习模型的影响。*arXiv*，2024年1月。doi: 10.48550/arXiv.2401.17269。'
- en: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, and Seong Joon Oh. ProPILE: Probing Privacy Leakage in Large Language Models.
    *arXiv*, July 2023. doi: 10.48550/arXiv.2307.01881.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, 和 Seong Joon Oh. ProPILE: 探测大型语言模型中的隐私泄露。*arXiv*，2023年7月。doi: 10.48550/arXiv.2307.01881。'
- en: 'Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun
    Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying LLM Safety against Adversarial
    Prompting. *arXiv*, September 2023. doi: 10.48550/arXiv.2309.02705.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun
    Li, Soheil Feizi, 和 Himabindu Lakkaraju. 认证LLM安全性以防对抗性提示。*arXiv*，2023年9月。doi:
    10.48550/arXiv.2309.02705。'
- en: 'Le et al. (2020) Thai Le, Suhang Wang, and Dongwon Lee. *MALCOM: Generating
    Malicious Comments to Attack Neural Fake News Detection Models*. IEEE Computer
    Society, November 2020. ISBN 978-1-7281-8316-9. doi: 10.1109/ICDM50108.2020.00037.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le et al. (2020) Thai Le, Suhang Wang, 和 Dongwon Lee. *MALCOM: 生成恶意评论以攻击神经假新闻检测模型*。IEEE计算机学会，2020年11月。ISBN
    978-1-7281-8316-9。doi: 10.1109/ICDM50108.2020.00037。'
- en: 'Liu et al. (2023) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via Prompt
    Engineering: An Empirical Study. *arXiv*, May 2023. doi: 10.48550/arXiv.2305.13860.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, 和 Yang Liu. 通过提示工程破解ChatGPT：一项实证研究。*arXiv*，2023年5月。doi:
    10.48550/arXiv.2305.13860。'
- en: 'Mehrotra et al. (2023) Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine
    Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of Attacks: Jailbreaking
    Black-Box LLMs Automatically. *arXiv*, December 2023. doi: 10.48550/arXiv.2312.02119.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehrotra et al. (2023) Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine
    Nelson, Hyrum Anderson, Yaron Singer, 和 Amin Karbasi. 攻击树：自动破解黑箱LLMs。*arXiv*，2023年12月。doi:
    10.48550/arXiv.2312.02119。'
- en: 'Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback. *arXiv*, March 2022. doi: 10.48550/arXiv.2203.02155.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll
    L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,
    Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe. 训练语言模型以通过人工反馈遵循指令。*arXiv*，2022年3月。doi:
    10.48550/arXiv.2203.02155。'
- en: 'Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia,
    Prateek Mittal, and Peter Henderson. Fine-tuning Aligned Language Models Compromises
    Safety, Even When Users Do Not Intend To! *arXiv*, October 2023. doi: 10.48550/arXiv.2310.03693.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia,
    Prateek Mittal, 和 Peter Henderson. 微调对齐的语言模型会妥协安全性，即使用户不打算这样做！*arXiv*，2023年10月。doi:
    10.48550/arXiv.2310.03693。'
- en: 'Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar,
    Christopher Parisien, and Jonathan Cohen. NeMo Guardrails: A Toolkit for Controllable
    and Safe LLM Applications with Programmable Rails. *ACL Anthology*, pp.  431–445,
    December 2023. doi: 10.18653/v1/2023.emnlp-demo.40.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rebedea et al. (2023) 特拉伊安·雷贝德，拉兹万·迪努，马凯什·纳尔希曼·斯里德哈尔，克里斯托弗·帕里森，和乔纳森·科恩。NeMo
    Guardrails：用于可控和安全的LLM应用的工具包，具有可编程轨道。*ACL Anthology*，第431-445页，2023年12月。doi: 10.18653/v1/2023.emnlp-demo.40。'
- en: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How Does LLM Safety Training Fail? *arXiv*, July 2023. doi: 10.48550/arXiv.2307.02483.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2023) 亚历山大·魏，妮卡·哈赫塔拉布，和雅各布·斯坦哈特。破解：LLM安全培训如何失败？*arXiv*，2023年7月。doi:
    10.48550/arXiv.2307.02483。'
- en: 'Weyssow et al. (2023) Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari
    Sahraoui. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation
    with Large Language Models. *arXiv*, August 2023. doi: 10.48550/arXiv.2308.10462.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weyssow et al. (2023) 马丁·韦索，周欣，金奇洙，戴维·洛，和侯阿里·萨赫拉维。探索用于代码生成的大型语言模型的参数高效微调技术。*arXiv*，2023年8月。doi:
    10.48550/arXiv.2308.10462。'
- en: 'Xiao et al. (2023) Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang
    Guo, and Xianglong Liu. RobustMQ: Benchmarking Robustness of Quantized Models.
    *arXiv*, August 2023. doi: 10.48550/arXiv.2308.02350.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) 肖一松，刘爱珊，张天远，秦浩通，郭金阳，和刘向龙。RobustMQ：量化模型的鲁棒性基准。*arXiv*，2023年8月。doi:
    10.48550/arXiv.2308.02350。'
- en: 'Zhou et al. (2024) Andy Zhou, Bo Li, and Haohan Wang. Robust Prompt Optimization
    for Defending Language Models Against Jailbreaking Attacks. *arXiv*, January 2024.
    doi: 10.48550/arXiv.2401.17263.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2024) 安迪·周，李波，和王浩涵。用于防御语言模型的破解攻击的鲁棒提示优化。*arXiv*，2024年1月。doi: 10.48550/arXiv.2401.17263。'
- en: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. AutoDAN: Interpretable Gradient-Based
    Adversarial Attacks on Large Language Models. *arXiv*, October 2023. doi: 10.48550/arXiv.2310.15140.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) 司成竹，张瑞一，安邦，吴刚，乔·巴罗，王紫超，黄芙蓉，阿尼·嫩科瓦，和孙童。AutoDAN：对大型语言模型的可解释的基于梯度的对抗攻击。*arXiv*，2023年10月。doi:
    10.48550/arXiv.2310.15140。'
- en: 'Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico
    Kolter, and Matt Fredrikson. Universal and Transferable Adversarial Attacks on
    Aligned Language Models. *arXiv*, July 2023. doi: 10.48550/arXiv.2307.15043.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou et al. (2023) 安迪·邹，王子凡，尼古拉斯·卡林尼，米拉德·纳斯尔，J·齐科·科尔特，和马特·弗雷德里克森。对齐语言模型的通用和可转移对抗攻击。*arXiv*，2023年7月。doi:
    10.48550/arXiv.2307.15043。'
- en: Appendix A Appendix
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 Experiment Utils
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实验工具
- en: We utilize various platforms for our target model, including Anyscale’s endpoint,
    OpenAI’s API, and our local system, Azure’s NC12sv3, equipped with a 32GB V100
    GPU, along with Hugging Face, to conduct inference tasks effectively. We import
    models from Hugging Face to operate on our local system.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用多个平台来进行目标模型的推理任务，包括Anyscale的端点，OpenAI的API，以及我们本地系统Azure的NC12sv3（配备32GB V100
    GPU），还包括Hugging Face。我们从Hugging Face导入模型以在本地系统上运行。
- en: 'Table 4: Model Details'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：模型详情
- en: '| Name | Model | Source |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Name | Model | Source |'
- en: '| --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CodeLlama34B | codellama/CodeLlama-34b-Instruct-hf | Anyscale |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama34B | codellama/CodeLlama-34b-Instruct-hf | Anyscale |'
- en: '| Mixtral8x7B | mistralai/Mixtral-8x7B-Instruct-v0.1 | Anyscale |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral8x7B | mistralai/Mixtral-8x7B-Instruct-v0.1 | Anyscale |'
- en: '| SQLCoder | defog/sqlcoder-7b-2 | HuggingFace |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SQLCoder | defog/sqlcoder-7b-2 | HuggingFace |'
- en: '| Llama2 | meta-llama/Llama-2-7b-chat-hf | HuggingFace |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 | meta-llama/Llama-2-7b-chat-hf | HuggingFace |'
- en: '| NeuralChat | Intel/neural-chat-7b-v3-3 | HuggingFace |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| NeuralChat | Intel/neural-chat-7b-v3-3 | HuggingFace |'
- en: '| Mistral7B | mistralai/Mistral-7B-v0.1-v0.1 | HuggingFace |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Mistral7B | mistralai/Mistral-7B-v0.1-v0.1 | HuggingFace |'
- en: '| CodeLlama7B | codellama/CodeLlama-7b-hf | HuggingFace |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama7B | codellama/CodeLlama-7b-hf | HuggingFace |'
- en: '| CodeLlama-7B-GGUF | TheBloke/CodeLlama-7B-GGUF | HuggingFace |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF | TheBloke/CodeLlama-7B-GGUF | HuggingFace |'
- en: '| Llama2-7B-GGUF | TheBloke/Llama-2-7B-Chat-GGUF | HuggingFace |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-GGUF | TheBloke/Llama-2-7B-Chat-GGUF | HuggingFace |'
- en: '| Dolphin-Mistral | cognitivecomputations/dolphin-2.2.1-Mistral-7B-v0.1 | HuggingFace
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Dolphin-Mistral | cognitivecomputations/dolphin-2.2.1-Mistral-7B-v0.1 | HuggingFace
    |'
- en: '| MPT7B | mosaicml/mpt-7b | HuggingFace |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MPT7B | mosaicml/mpt-7b | HuggingFace |'
- en: '| Phi2 | microsoft/phi-2 | HuggingFace |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Phi2 | microsoft/phi-2 | HuggingFace |'
- en: '| GPT-3.5-turbo | GPT-3.5-turbo-0125 | OpenAI |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | GPT-3.5-turbo-0125 | OpenAI |'
- en: '| GPT-4-turbo | GPT-4-turbo-0125 | OpenAI |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-turbo | GPT-4-turbo-0125 | OpenAI |'
- en: A.2 Experiment Results in details
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验结果详细信息
- en: In our experimentation, we explore various foundational models, including the
    latest iterations from OpenAI’s GPT series, as well as models derived from previous
    fine-tuned versions. We conduct tests on these models both with and without the
    integration of guardrails. Additionally, we examine models that have been quantized,
    further expanding the scope of our investigation. This comprehensive approach
    allows us to assess the performance and effectiveness of guardrails across a range
    of model architectures and configurations. By analyzing these diverse scenarios,
    we aim to gain insights into the impact of guardrails on model stability and security,
    contributing to the advancement of responsible AI deployment practices. Figure
    [3](#A1.F3 "Figure 3 ‣ A.2 Experiment Results in details ‣ Appendix A Appendix
    ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization") showcases
    the impact of Guardrails.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们探讨了各种基础模型，包括来自 OpenAI GPT 系列的最新版本，以及基于之前微调版本的模型。我们对这些模型进行了有无防护措施的测试。此外，我们还研究了经过量化的模型，进一步扩展了我们的调查范围。这种全面的方法使我们能够评估防护措施在各种模型架构和配置中的性能和有效性。通过分析这些不同的情境，我们旨在深入了解防护措施对模型稳定性和安全性的影响，从而推动负责任的人工智能部署实践的发展。图
    [3](#A1.F3 "图 3 ‣ A.2 实验结果详细说明 ‣ 附录 A 附录 ‣ 微调和量化带来的 LLM 脆弱性增加") 展示了防护措施的影响。
- en: '![Refer to caption](img/021a93a79c99ad5cd4606a989900ba4c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/021a93a79c99ad5cd4606a989900ba4c.png)'
- en: 'Figure 3: Jailbreak'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：破解
- en: We monitor the number of queries needed to jailbreak the model. Figure [4](#A1.F4
    "Figure 4 ‣ A.2 Experiment Results in details ‣ Appendix A Appendix ‣ Increased
    LLM Vulnerabilities from Fine-tuning and Quantization") examines the sustainability
    of Guardrails in resisting jailbreak attempts (the data includes only instances
    when the models were jailbroken). It’s quite evident that having guardrails does
    offer additional resistance to jailbreak attempts, even if the model has been
    compromised.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们监测了破解模型所需的查询次数。图 [4](#A1.F4 "图 4 ‣ A.2 实验结果详细说明 ‣ 附录 A 附录 ‣ 微调和量化带来的 LLM 脆弱性增加")
    考察了防护措施在抵御破解尝试中的持续性（数据仅包括模型被破解的情况）。显而易见，尽管模型已经被攻破，但有防护措施确实提供了额外的破解阻力。
- en: '![Refer to caption](img/3d2b15e0f8bd1ec4f55137008a312961.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3d2b15e0f8bd1ec4f55137008a312961.png)'
- en: 'Figure 4: Queries to Jailbreak'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：破解查询
