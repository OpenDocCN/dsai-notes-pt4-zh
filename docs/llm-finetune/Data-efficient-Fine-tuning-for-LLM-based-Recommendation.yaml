- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Data-efficient Fine-tuning for LLM-based Recommendation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据高效微调用于基于LLMs的推荐
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17197](https://ar5iv.labs.arxiv.org/html/2401.17197)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17197](https://ar5iv.labs.arxiv.org/html/2401.17197)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: Xinyu Lin¹, Wenjie Wang¹, Yongqi Li², Shuo Yang³, Fuli Feng⁴, Yinwei Wei⁵, and
    Tat-Seng Chua¹ ¹National University of Singapore, ²The Hong Kong Polytechnic University,
    ³University of Technology Sydney, ⁴University of Science and Technology of China,
    ⁵Monash University  [xylin1028, wenjiewang96, liyongqi0@gmail.com, shuo.yang@student.uts.edu.au](mailto:xylin1028,%20wenjiewang96,%20liyongqi0@gmail.com,%20shuo.yang@student.uts.edu.au)
    [fulifeng93@gmail.com, weiyinwei@hotmail.com, dcscts@nus.edu.sg](mailto:fulifeng93@gmail.com,%20weiyinwei@hotmail.com,%20dcscts@nus.edu.sg)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Xinyu Lin¹, Wenjie Wang¹, Yongqi Li², Shuo Yang³, Fuli Feng⁴, Yinwei Wei⁵, 和
    Tat-Seng Chua¹ ¹新加坡国立大学, ²香港理工大学, ³悉尼科技大学, ⁴中国科学技术大学, ⁵蒙纳士大学  [xylin1028, wenjiewang96,
    liyongqi0@gmail.com, shuo.yang@student.uts.edu.au](mailto:xylin1028,%20wenjiewang96,%20liyongqi0@gmail.com,%20shuo.yang@student.uts.edu.au)
    [fulifeng93@gmail.com, weiyinwei@hotmail.com, dcscts@nus.edu.sg](mailto:fulifeng93@gmail.com,%20weiyinwei@hotmail.com,%20dcscts@nus.edu.sg)
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Leveraging Large Language Models (LLMs) for recommendation has recently garnered
    considerable attention, where fine-tuning plays a key role in LLMs’ adaptation.
    However, the cost of fine-tuning LLMs on rapidly expanding recommendation data
    limits their practical application. To address this challenge, few-shot fine-tuning
    offers a promising approach to quickly adapt LLMs to new recommendation data.
    We propose the task of data pruning for efficient LLM-based recommendation, aimed
    at identifying representative samples tailored for LLMs’ few-shot fine-tuning.
    While coreset selection is closely related to the proposed task, existing coreset
    selection methods often rely on suboptimal heuristic metrics or entail costly
    optimization on large-scale recommendation data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大型语言模型（LLMs）进行推荐最近引起了相当大的关注，其中微调在LLMs的适应中起着关键作用。然而，微调LLMs以应对快速扩展的推荐数据的成本限制了它们的实际应用。为了解决这个挑战，少样本微调提供了一种有前景的方法，可以快速将LLMs适应新的推荐数据。我们提出了数据修剪任务以实现高效的基于LLMs的推荐，旨在识别出适合LLMs少样本微调的代表性样本。虽然核心集选择与提议的任务密切相关，但现有的核心集选择方法通常依赖于次优的启发式度量或需要在大规模推荐数据上进行昂贵的优化。
- en: 'To tackle these issues, we introduce two primary objectives for the data pruning
    task in the context of LLM-based recommendation: 1) high accuracy aims to identify
    the influential samples that can lead to high overall performance; and 2) high
    efficiency underlines the low costs of the data pruning process. To pursue the
    two objectives, we propose a novel data pruning method incorporating two scores,
    namely influence score and effort score, to efficiently identify the influential
    samples. Particularly, the influence score is introduced to accurately estimate
    the influence of removing each sample on the overall performance. To achieve low
    costs of the data pruning process, we employ a small-sized surrogate model to
    replace LLMs to obtain the influence score. Considering the potential gap between
    the surrogate model and LLMs, we further propose an effort score to prioritize
    some hard samples specifically for LLMs. We instantiate the proposed method on
    two competitive LLM-based recommender models, and empirical results on three real-world
    datasets validate the effectiveness of our proposed method. In particular, the
    proposed method uses only 2% samples to surpass the full data fine-tuning, reducing
    time costs by 97%.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们在LLM-based推荐的背景下引入了数据修剪任务的两个主要目标：1）高准确度旨在识别出能够导致高整体表现的影响样本；2）高效率强调数据修剪过程的低成本。为了追求这两个目标，我们提出了一种新颖的数据修剪方法，结合了两个评分，即影响评分和努力评分，以高效地识别影响样本。特别是，引入影响评分以准确估计每个样本移除对整体表现的影响。为了实现低成本的数据修剪过程，我们采用了一个小型的替代模型来代替LLMs以获取影响评分。考虑到替代模型与LLMs之间可能存在的差距，我们进一步提出了努力评分，以优先考虑一些特别适用于LLMs的困难样本。我们在两个竞争的基于LLMs的推荐模型上实例化了提议的方法，三个真实世界数据集上的实证结果验证了我们方法的有效性。特别是，该方法仅使用了2%的样本即可超越全数据微调，将时间成本减少了97%。
- en: 'Data Pruning, LLM-based Recommendation, Efficient Fine-tuning^†^†ccs: Information
    systems Recommender systems'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '数据修剪、基于LLMs的推荐、有效微调^†^†ccs: 信息系统 推荐系统'
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Leveraging Large Language Models (LLMs) for recommendation has demonstrated
    promising efficacy across various tasks, including Click-Through Rate (CTR) prediction (Bao
    et al., [2023b](#bib.bib5)), sequential recommendation (Rajput et al., [2023](#bib.bib41)),
    and explainable recommendation (Gao et al., [2023](#bib.bib15)). To build LLM-based
    recommender models, it is crucial to fine-tune LLMs on recommendation data for
    two primary reasons: 1) there exists a significant gap between previous LLMs’
    tuning tasks and the recommendation tasks (Bao et al., [2023b](#bib.bib5)), and
    2) the rapid and continuous update of recommendation data necessitates frequent
    fine-tuning of LLMs (Sachdeva et al., [2022](#bib.bib42)). For example, there
    are approximately 160 million new videos and 942 billion interactions emerging
    on TikTok per day¹¹1https://www.tiktok.com/transparency/.. Thus, frequent fine-tuning
    is imperative to incorporate up-to-date item information and enhance user behavior
    comprehension. However, fine-tuning LLMs on large-scale recommendation data demands
    substantial computational resources and time costs (Li et al., [2023b](#bib.bib31)),
    thereby diminishing the practicality of LLM-based recommender models in real-world
    applications. As such, it is essential to enhance the fine-tuning efficiency of
    LLM-based recommender models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大型语言模型（LLMs）进行推荐在各种任务中显示出令人期待的效果，包括点击率（CTR）预测（Bao et al., [2023b](#bib.bib5)）、序列推荐（Rajput
    et al., [2023](#bib.bib41)）和可解释推荐（Gao et al., [2023](#bib.bib15)）。为了构建基于LLM的推荐模型，关键是对推荐数据进行微调，原因有两个：1）先前LLMs的调优任务与推荐任务之间存在显著差距（Bao
    et al., [2023b](#bib.bib5)），2）推荐数据的快速和持续更新要求频繁微调LLMs（Sachdeva et al., [2022](#bib.bib42)）。例如，TikTok每天出现约1.6亿个新视频和9420亿次互动¹¹1https://www.tiktok.com/transparency/。因此，频繁微调对于纳入最新的项目信息和增强用户行为理解至关重要。然而，在大规模推荐数据上微调LLMs需要大量的计算资源和时间成本（Li
    et al., [2023b](#bib.bib31)），从而降低了LLM基于推荐模型在现实应用中的实用性。因此，提高LLM基于推荐模型的微调效率至关重要。
- en: 'Fortunately, the rich world knowledge encoded in LLMs offers a promising solution
    for efficient fine-tuning: few-shot fine-tuning. Previous studies have uncovered
    that LLMs have the potential to quickly adapt to recommendation tasks by fine-tuning
    on randomly sampled few-shot data (Bao et al., [2023b](#bib.bib5), [a](#bib.bib4);
    Lin et al., [2023](#bib.bib33)) (Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(a)), significantly
    reducing training time and computational costs. Despite its efficiency, randomly
    sampled data may lack sufficient representativeness to enable LLMs to effectively
    comprehend new items and user behaviors. To combat this issue, we introduce the
    task of data pruning for efficient LLM-based recommendation, which aims to identify
    representative samples tailored for LLMs’ few-shot fine-tuning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，LLMs中编码的丰富世界知识为高效微调提供了有希望的解决方案：少量样本微调。以往研究发现，LLMs通过在随机抽样的少量样本数据上进行微调，能够迅速适应推荐任务（Bao
    et al., [2023b](#bib.bib5), [a](#bib.bib4); Lin et al., [2023](#bib.bib33)）(图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")(a))，显著减少了训练时间和计算成本。尽管其效率高，随机抽样的数据可能缺乏足够的代表性，无法使LLMs有效理解新项和用户行为。为了解决这个问题，我们引入了数据修剪任务，以实现高效的LLM基于推荐，该任务旨在识别针对LLMs少量样本微调的代表性样本。
- en: 'A closely related literature to this data pruning task is coreset selection (Guo
    et al., [2022](#bib.bib17)). It tries to select a small but representative subset
    from the full data, aiming to achieve comparable performance. Existing coreset
    selection methods generally fall into two categories²²2More detailed related work
    is discussed and compared in Section [4](#S4 "4\. Experiment ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation") and [5](#S5 "5\. Related Work ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation").: 1) Heuristic methods select hard
    or diverse samples based on pre-defined metrics (Paul et al., [2021](#bib.bib40);
    Wu et al., [2023b](#bib.bib54); Luo et al., [2023](#bib.bib37)). Such heuristic
    methods do not estimate the impact of selected samples on empirical risk, possibly
    leading to suboptimal coreset selection. 2) Optimization-based methods mainly
    optimize the selection of subsets to minimize the empirical risk (Borsos et al.,
    [2020](#bib.bib6); Yang et al., [2023b](#bib.bib56)). However, these methods are
    inapplicable to large-scale recommendation datasets due to the complex and costly
    bi-level or discrete optimization problem (He et al., [2023](#bib.bib21)). Worse
    still, both heuristic and optimization-based methods rely on the model well-trained
    by the full data to select the coreset, *e.g.,* calculating pre-defined scores
    or optimizing the data subset based on the well-trained model (*cf.* Section [2](#S2
    "2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    As such, it is infeasible to directly apply these methods for LLM-based recommendation
    because of the high training costs of LLMs on the large-scale full recommendation
    data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据修剪任务密切相关的文献是核心集选择 (Guo et al., [2022](#bib.bib17))。它试图从完整数据中选择一个小但具有代表性的子集，以实现可比的性能。现有的核心集选择方法通常分为两类²²2更详细的相关工作在第
    [4](#S4 "4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")
    和第 [5](#S5 "5\. Related Work ‣ Data-efficient Fine-tuning for LLM-based Recommendation")
    节中讨论和比较：1）启发式方法基于预定义的指标选择困难或多样化的样本 (Paul et al., [2021](#bib.bib40); Wu et al.,
    [2023b](#bib.bib54); Luo et al., [2023](#bib.bib37))。这些启发式方法未能评估所选样本对经验风险的影响，可能导致次优的核心集选择。2）基于优化的方法主要优化子集的选择以最小化经验风险
    (Borsos et al., [2020](#bib.bib6); Yang et al., [2023b](#bib.bib56))。然而，由于复杂且昂贵的双层或离散优化问题
    (He et al., [2023](#bib.bib21))，这些方法不适用于大规模推荐数据集。更糟糕的是，启发式和基于优化的方法都依赖于由完整数据充分训练的模型来选择核心集，例如，计算预定义的评分或基于充分训练的模型优化数据子集
    (*参见* 第 [2](#S2 "2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation") 节)。因此，由于对大规模完整推荐数据进行LLM训练的高成本，直接应用这些方法于基于LLM的推荐是不切实际的。
- en: '![Refer to caption](img/7af1145458995dae8de4a72fa335b713.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/7af1145458995dae8de4a72fa335b713.png)'
- en: Figure 1\. (a) reveals that BIGRec achieves remarkable performance with only
    hundreds of samples. (b) shows the low costs of surrogate models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. (a) 显示了BIGRec在仅用几百个样本的情况下取得了显著的性能。 (b) 显示了代理模型的低成本。
- en: 'To overcome the above issues, we summarize two principal objectives for data
    pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses
    on selecting the samples that can lead to low empirical risk; and 2) high efficiency,
    which emphasizes the low costs of the data pruning process, *i.e.,* eliminating
    the dependency of well-trained LLMs on the full data. Nevertheless, pursuing the
    two objectives faces two challenges:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服上述问题，我们总结了在基于LLM的推荐系统中数据修剪的两个主要目标：1）高准确性，关注选择能够导致低经验风险的样本；2）高效率，强调数据修剪过程的低成本，即消除对完全数据的高度依赖。尽管如此，追求这两个目标面临着两个挑战：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To achieve high accuracy, it is essential to measure the influence of removing
    each training sample on the empirical risk. However, assessing the influence of
    all samples is costly, as it requires the leaving-one-out retraining for each
    sample (Tan et al., [2023](#bib.bib47)).
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了实现高准确性，测量每个训练样本移除对经验风险的影响至关重要。然而，评估所有样本的影响成本高昂，因为这需要对每个样本进行留一重训练 (Tan et al.,
    [2023](#bib.bib47))。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To achieve high efficiency, one possible solution is to train a surrogate model
    for sample selection, *e.g.,* using a small-sized traditional recommender model,
    which can drastically reduce the GPU memory usage and the training time compared
    to LLMs (see Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")(b)). However, there exists a gap between LLMs and
    surrogate models, attributable to their divergent capabilities in learning user
    behaviors (refer to Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence
    Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    As such, influential samples selected by surrogate models might deviate from the
    ones on LLMs, potentially hurting the adaptation of LLMs.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了实现高效性，一种可能的解决方案是训练代理模型进行样本选择，例如，使用一个小型的传统推荐模型，这可以显著减少与LLMs相比的GPU内存使用和训练时间（见图 [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(b)）。然而，LLMs和代理模型之间存在差距，这归因于它们在学习用户行为方面的不同能力（参见图 [5](#footnote5
    "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")）。因此，代理模型选择的重要样本可能与LLMs上的样本有所偏差，这可能会影响LLMs的适应性。
- en: To address the challenges, we propose a novel Data pruning method, to Efficiently
    identify the influentiAl samples for LLM-based Recommender fine-tuning (shorted
    as DEALRec). DEALRec leverages two scores, namely influence score and effort score,
    to identify the influential samples. The influence score is formulated to estimate
    the influence of removing each training sample on the empirical risk. It is calculated
    by extending the influence function (Hampel, [1974](#bib.bib19)) via chain rules
    and second-order optimization techniques (Koh and Liang, [2017](#bib.bib29)).
    To efficiently calculate the influence score for all samples, DEALRec employs
    a simple yet effective symmetric property to accelerate the calculation, requiring
    only the estimation once for all samples (*cf.* Section [3.1](#S3.SS1 "3.1\. Influence
    Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    Thereafter, DEALRec uses a traditional recommender model as a surrogate model
    to obtain the influence score and introduces the effort score to mitigate the
    gap between the surrogate model and LLMs. The effort score is obtained by calculating
    the gradient norm of a sample loss *w.r.t.* the parameters of LLMs, intuitively
    measuring the effort of LLMs to fit a specific sample. By regularizing the influence
    score with the effort score, DEALRec identifies the influential samples that encompass
    both the representativeness of the full data and the significance to LLMs. We
    instantiate DEALRec on two LLM-based recommender models and conduct extensive
    experiments on three real-world datasets, validating the superiority of DEALRec
    in terms of both efficiency and accuracy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们提出了一种新颖的数据剪枝方法，以高效识别对LLM-based推荐模型微调（简称DEALRec）的影响样本。DEALRec利用影响评分和努力评分两个指标来识别重要样本。影响评分用于估算删除每个训练样本对经验风险的影响。它通过链式规则和二阶优化技术扩展影响函数（Hampel,
    [1974](#bib.bib19)）来计算。为了高效计算所有样本的影响评分，DEALRec利用一种简单而有效的对称属性来加速计算，只需对所有样本进行一次估计（*cf.*
    Section [3.1](#S3.SS1 "3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")）。随后，DEALRec使用传统推荐模型作为代理模型来获得影响评分，并引入努力评分以减小代理模型与LLM之间的差距。努力评分通过计算样本损失对LLM参数的梯度范数获得，直观地衡量了LLM拟合特定样本的努力。通过用努力评分来正则化影响评分，DEALRec识别出既涵盖了全部数据的代表性，又对LLMs重要的样本。我们在两个基于LLM的推荐模型上实例化了DEALRec，并在三个真实世界的数据集上进行了广泛的实验，验证了DEALRec在效率和准确性方面的优越性。
- en: 'In summary, this work offers three major contributions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这项工作提供了三大主要贡献：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a data pruning task to identify the influential samples tailored
    for efficient LLM-based recommender fine-tuning, unlocking the remarkable potential
    of applying LLM-based recommender models to real-world platforms.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一个数据剪枝任务，以识别针对高效LLM-based推荐模型微调的关键样本，释放了将基于LLM的推荐模型应用于实际平台的显著潜力。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel data pruning method to discover the influential samples for
    LLM-based recommendation, which effectively and efficiently assesses the influence
    of removing a sample on empirical risk.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的数据剪枝方法，用于发现对基于LLM的推荐系统有影响的样本，该方法有效且高效地评估了删除样本对经验风险的影响。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments on three real-world datasets, demonstrating
    the effectiveness of DEALRec in achieving both high efficiency and accuracy.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在三个实际数据集上进行了广泛的实验，展示了 DEALRec 在实现高效率和高准确性方面的有效性。
- en: 2\. Task Formulation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 任务制定
- en: In this section, we first introduce LLM-based recommender models and uncover
    the challenge of real-world applicability. Thereafter, we formulate the task of
    data pruning for LLM-based recommendation and compare the related work on coreset
    selection.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍基于 LLM 的推荐模型，并揭示实际应用中的挑战。随后，我们制定了基于 LLM 的推荐的数据修剪任务，并比较了相关的核心集选择工作。
- en: '$\bullet\quad$ and $\mathcal{I}$, where $x=[i_{1},i_{2},\dots,i_{|x|}]$ is
    the next interacted item of the user³³3Our main focus lies in sequential recommendation,
    which holds notable practical significance by intricately considering the temporal
    aspect in real-world scenarios., where $\{i_{1},\dots,i_{|x|},y\}\subset\mathcal{I}$,
    the target is to fine-tune an LLM for recommendation tasks. The learnable parameters
    ($\phi\in\Phi$ conditioned on input $x$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 和 $\mathcal{I}$，其中 $x=[i_{1},i_{2},\dots,i_{|x|}]$ 是用户接下来互动的项目³³3我们主要关注的是序列推荐，通过细致考虑实际场景中的时间因素，具有显著的实际意义。，其中
    $\{i_{1},\dots,i_{|x|},y\}\subset\mathcal{I}$，目标是对推荐任务进行 LLM 的微调。可学习的参数 ($\phi\in\Phi$)
    以输入 $x$ 为条件：
- en: '| (1) |  | $1$2 |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $1$2 |  |'
- en: where $y_{t}$-th token of $y$ represents the token sequence preceding $y_{t}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{t}$-th token 的 $y$ 代表了 $y_{t}$ 之前的 token 序列。
- en: While fine-tuning LLMs has demonstrated effectiveness in recommendation tasks (Liu
    et al., [2024](#bib.bib36)), its practical application is hindered by the high
    resource costs required by LLMs and the continuous influx of new recommendation
    data (Sachdeva et al., [2022](#bib.bib42)). Hence, it is essential to enhance
    the efficiency of LLM-based recommender fine-tuning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对 LLM 进行微调在推荐任务中已证明有效（Liu 等，[2024](#bib.bib36)），但由于 LLM 需要的高资源成本以及不断涌入的新推荐数据（Sachdeva
    等，[2022](#bib.bib42)），其实际应用受到限制。因此，提升基于 LLM 的推荐器微调效率是至关重要的。
- en: $\bullet\quad$, the target of data pruning is to select a subset $\mathcal{S}\subset\mathcal{D}$
    can yield good performance on the testing set. The size of $\mathcal{S}$, *i.e.,*
    $|\mathcal{S}|=r|\mathcal{D}|$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$，数据修剪的目标是选择一个子集 $\mathcal{S}\subset\mathcal{D}$，使其在测试集上能表现良好。$\mathcal{S}$
    的大小，*即，* $|\mathcal{S}|=r|\mathcal{D}|$。
- en: '$\bullet\quad$Retrospect of coreset selection. As the closely related work
    to this data pruning task, coreset selection methods generally fall into two groups:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 核心集选择的回顾。作为与数据修剪任务密切相关的工作，核心集选择方法通常分为两类：
- en: 1)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1)
- en: 'Heuristic methods (Coleman et al., [2020](#bib.bib9); Toneva et al., [2018](#bib.bib48);
    Feldman and Zhang, [2020](#bib.bib13)) typically design some heuristic strategies
    to select samples based on an empirical minimizer:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启发式方法（Coleman 等，[2020](#bib.bib9)；Toneva 等，[2018](#bib.bib48)；Feldman 和 Zhang，[2020](#bib.bib13)）通常设计一些启发式策略，以基于经验最小化器选择样本：
- en: '| (2) |  | $\displaystyle\mathcal{S}=H(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{D}),$
    |  |'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\mathcal{S}=H(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{D}),$
    |  |'
- en: where $\mathcal{L}(\cdot)$ denotes the heuristic strategy such as selecting
    samples with larger prediction entropy (Coleman et al., [2020](#bib.bib9)), or
    clustering the samples based on the sample representations (Chai et al., [2023](#bib.bib7)).
    However, this group of methods designs the strategy $H(\cdot)$ intuitively and
    fails to explicitly consider the influence of a sample on the empirical risk.
    This might lead to suboptimal selection, thereby declining the performance of
    the model trained by the selected subset.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}(\cdot)$ 表示启发式策略，例如选择具有更大预测熵的样本（Coleman 等，[2020](#bib.bib9)），或根据样本表示对样本进行聚类（Chai
    等，[2023](#bib.bib7)）。然而，这些方法的策略 $H(\cdot)$ 设计较为直观，未明确考虑样本对经验风险的影响。这可能导致次优选择，从而降低了由所选子集训练的模型的性能。
- en: 2)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2)
- en: 'Optimization-based methods (Borsos et al., [2020](#bib.bib6); Killamsetty et al.,
    [2021c](#bib.bib28), [b](#bib.bib27); Wu et al., [2023a](#bib.bib53)) mainly utilize
    bi-level optimization techniques to learn the best subset chosen for training:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于优化的方法（Borsos 等，[2020](#bib.bib6)；Killamsetty 等，[2021c](#bib.bib28)，[b](#bib.bib27)；Wu
    等，[2023a](#bib.bib53)）主要利用双层优化技术来学习为训练选择的最佳子集：
- en: '| (3) |  | $\small\mathcal{S^{*}}=\mathop{\arg\min}_{\mathcal{S}\subset\mathcal{D}}\mathcal{L}(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{S}).$
    |  |'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (3) |  | $\small\mathcal{S^{*}}=\mathop{\arg\min}_{\mathcal{S}\subset\mathcal{D}}\mathcal{L}(\hat{\theta},\mathcal{D}),\quad\text{s.t.}\quad\hat{\theta}=\mathop{\arg\min}_{\theta\in\Theta}\mathcal{L}(\theta,\mathcal{S}).$
    |  |'
- en: Besides, there is also some work that employs discrete optimization problems
    based on the empirical minimizer $\hat{\theta}$ in Eq. ([2](#S2.E2 "In item 1)
    ‣ 2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    Nevertheless, they struggle to be applied to large-scale datasets *e.g.,* recommendation
    data, due to the complex solving of the optimization problem (He et al., [2023](#bib.bib21)).
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，还有一些工作利用基于公式 ([2](#S2.E2 "在第1项) ‣ 2\. 任务公式 ‣ 基于LLM的推荐的高效微调")) 的经验最小化器 $\hat{\theta}$
    的离散优化问题。然而，由于优化问题的复杂求解，它们在大型数据集（*例如*，推荐数据）上的应用面临困难（He 等，[2023](#bib.bib21)）。
- en: Furthermore, as shown in Eq. ([2](#S2.E2 "In item 1) ‣ 2\. Task Formulation
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")-[3](#S2.E3 "In item
    2) ‣ 2\. Task Formulation ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    previous coreset selection methods usually require the model to be trained over
    original training samples $\mathcal{D}$, which however is infeasible for LLM-based
    recommender models due to the continuous influx of data and the high resource
    costs of LLMs (*cf.* Section [1](#S1 "1\. Introduction ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如公式 ([2](#S2.E2 "在第1项) ‣ 2\. 任务公式 ‣ 基于LLM的推荐的高效微调")-[3](#S2.E3 "在第2项) ‣ 2\.
    任务公式 ‣ 基于LLM的推荐的高效微调")) 所示，之前的核心集合选择方法通常要求模型在原始训练样本 $\mathcal{D}$ 上进行训练，然而，由于数据的持续流入和LLM的高资源成本，这对于基于LLM的推荐模型来说是不可行的（*参见*
    第[1](#S1 "1\. 引言 ‣ 基于LLM的推荐的高效微调")节）。
- en: '$\bullet\quad$ Drawing upon the above insights, we consider two objectives
    for data pruning: 1) high accuracy emphasizes the low empirical risk of the model
    trained on the selected samples, and 2) high efficiency focuses on the low costs
    of the data pruning process, breaking free from the heavy fine-tuning of LLMs
    for data pruning.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 根据以上见解，我们考虑数据修剪的两个目标：1) 高准确性强调在选择样本上训练的模型的低经验风险，2) 高效率关注数据修剪过程的低成本，从而摆脱了对LLM进行数据修剪的繁重微调。
- en: 3\. DEALRec
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. DEALRec
- en: To pursue efficient LLM-based recommendation, we propose a novel data pruning
    method DEALRec, which involves two key components, *i.e.,* the influence score
    to estimate the influence on empirical risk, and the effort score as a regularization
    to mitigate the gap between surrogate model and LLMs. The overview of our method
    is presented in Figure [2](#S3.F2 "Figure 2 ‣ 3.1\. Influence Score ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高效的基于LLM的推荐，我们提出了一种新颖的数据修剪方法 DEALRec，该方法包含两个关键组件，即估计经验风险影响的影响分数，以及作为正则化来缓解代理模型与LLMs之间差距的努力分数。我们的方法概述如图[2](#S3.F2
    "图2 ‣ 3.1\. 影响分数 ‣ 3\. DEALRec ‣ 基于LLM的推荐的高效微调")所示。
- en: 3.1\. Influence Score
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 影响分数
- en: To achieve good overall performance with the model trained on the pruned dataset
    $\mathcal{S}$ times. To overcome this challenge, we propose an efficient approximation
    of the influence for all samples by extending influence on parameter change (*i.e.,*
    a classic result from influence function (Koh and Liang, [2017](#bib.bib29)))
    via chain rule and second-order optimization techniques. We further utilize the
    symmetric property to speed up the calculation of the influence score.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在修剪后的数据集 $\mathcal{S}$ 上训练的模型中实现良好的整体性能。为了克服这一挑战，我们通过链式法则和二阶优化技术扩展对参数变化的影响（*即*，影响函数的经典结果（Koh
    和 Liang，[2017](#bib.bib29)））来提出高效的所有样本影响的近似。我们进一步利用对称性来加速影响分数的计算。
- en: '![Refer to caption](img/583797eaaaa183f8b35f905374e791a9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/583797eaaaa183f8b35f905374e791a9.png)'
- en: Figure 2\. Overview of DEALRec. DEALRec first trains a surrogate model on the
    full training samples. Subsequently, it calculates the influence score, which
    is then regularized by the effort score, to identify influential samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. DEALRec 概述。DEALRec 首先在完整训练样本上训练一个代理模型。随后，它计算影响分数，并通过努力分数进行正则化，以识别有影响力的样本。
- en: '$\bullet\quad$ for training. Considering a training sample $s$, the empirical
    minimizer can be rewritten as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 用于训练。考虑一个训练样本 $s$，经验最小化器可以被重写为：
- en: '| (4) |  | $\hat{\theta}_{\epsilon,s}=\mathop{\arg\min}_{\theta\in\Theta}\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\mathcal{L}(s_{i},\theta)+\epsilon\mathcal{L}(s,\theta).$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\hat{\theta}_{\epsilon,s}=\mathop{\arg\min}_{\theta\in\Theta}\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\mathcal{L}(s_{i},\theta)+\epsilon\mathcal{L}(s,\theta).$
    |  |'
- en: 'According to (Ling, [1984](#bib.bib34)), the influence of upweighting a sample
    $s$ on the parameter change is then given as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 (Ling, [1984](#bib.bib34))，上调样本 $s$ 对参数变化的影响可以表示为：
- en: '| (5) |  | $\mathcal{I}_{\text{param}}(s)=\frac{\mathrm{d}\hat{\theta}_{\epsilon,s}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\mathcal{I}_{\text{param}}(s)=\frac{\mathrm{d}\hat{\theta}_{\epsilon,s}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}=-H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
- en: 'where $H_{\hat{\theta}}=\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\nabla_{\theta}^{2}\mathcal{L}(s_{i},\hat{\theta})$,
    and $m$ to $\epsilon$ from training. As such, the parameter change of removing
    a training sample $s$ can be linearly approximated as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{\hat{\theta}}=\frac{1}{n}\sum_{s_{i}\in\mathcal{D}}\nabla_{\theta}^{2}\mathcal{L}(s_{i},\hat{\theta})$，$m$
    表示从训练中得到的 $\epsilon$。因此，移除训练样本 $s$ 的参数变化可以线性近似为：
- en: '| (6) |  | $\hat{\theta}_{-s}-\hat{\theta}\approx-\frac{1}{n}\mathcal{I}_{\text{param}}(s)=\frac{1}{n}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\hat{\theta}_{-s}-\hat{\theta}\approx-\frac{1}{n}\mathcal{I}_{\text{param}}(s)=\frac{1}{n}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}),$
    |  |'
- en: where $\hat{\theta}_{-s}=\mathop{\arg\min}_{\theta\in\Theta}\sum\nolimits_{s_{i}\in\mathcal{D},s_{i}\neq
    s}\mathcal{L}(s_{i},\theta)$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\theta}_{-s}=\mathop{\arg\min}_{\theta\in\Theta}\sum\nolimits_{s_{i}\in\mathcal{D},s_{i}\neq
    s}\mathcal{L}(s_{i},\theta)$。
- en: Based on Eq. ([6](#S3.E6 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")), an intuitive approach to assess the
    sample influence for model training is to utilize the L2 norm of a sample’s influence
    on parameter change or an additional discrete optimization problem as proposed
    in (Yang et al., [2023b](#bib.bib56)). Nevertheless, large parameter changes do
    not necessarily lead to performance improvements. Besides, calculating Eq. ([6](#S3.E6
    "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")) for all training samples can be computationally costly (He et al.,
    [2023](#bib.bib21)) and is infeasible for recommendation data. To alleviate the
    issues, we propose an efficient approximation for the influence of removing a
    sample on the empirical risk.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于公式 ([6](#S3.E6 "在 3.1\. 影响评分 ‣ 3\. DEALRec ‣ 基于 LLM 的推荐的高效微调"))，评估样本对模型训练的影响的直观方法是利用样本对参数变化的
    L2 范数，或是采用 (Yang et al., [2023b](#bib.bib56)) 提出的额外离散优化问题。然而，大的参数变化不一定会导致性能的提升。此外，对于所有训练样本计算公式
    ([6](#S3.E6 "在 3.1\. 影响评分 ‣ 3\. DEALRec ‣ 基于 LLM 的推荐的高效微调")) 可能计算代价高昂 (He et al.,
    [2023](#bib.bib21))，且在推荐数据中不可行。为了解决这些问题，我们提出了一种高效的近似方法来评估移除样本对经验风险的影响。
- en: '$\bullet\quad$ by a small $\epsilon$:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 通过一个小的 $\epsilon$：
- en: '| (7) |  | $\displaystyle\mathcal{I}_{\text{upweight,loss}}(s,s^{\prime})$
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle\mathcal{I}_{\text{upweight,loss}}(s,s^{\prime})$
    |  |'
- en: '|  |  | $\displaystyle=\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}\frac{\mathrm{d}{\hat{\theta}_{\epsilon,s}}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}\quad\quad\text{(chain
    rule)}$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}\frac{\mathrm{d}{\hat{\theta}_{\epsilon,s}}}{\mathrm{d}\epsilon}\bigg{&#124;}_{\epsilon=0}\quad\quad\text{(链式法则)}$
    |  |'
- en: '|  |  | $\displaystyle=-\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}).$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\nabla_{\theta}\mathcal{L}(s^{\prime},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta}).$
    |  |'
- en: 'Similarly, the influence of removing a training sample $s$ can be linearly
    approximated as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，移除训练样本 $s$ 的影响可以线性近似为：
- en: '| (8) |  | $1$2 |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $1$2 |  |'
- en: We can then obtain the influence of removing a sample $s$ on the empirical risk
    (*i.e.,* influence score) by
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过以下公式获得移除样本 $s$ 对经验风险的影响（*即* 影响评分）：
- en: '| (9) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: However, it is non-trivial to directly obtain $H_{\hat{\theta}}^{-1}$ requires
    $\mathcal{O}(nm^{2}+m^{3})$ training samples and $\theta\in\mathbb{R}^{m}$. This
    results in cumbersome calculation of influence scores for all training samples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接获得 $H_{\hat{\theta}}^{-1}$ 是不简单的，这需要 $\mathcal{O}(nm^{2}+m^{3})$ 训练样本和
    $\theta\in\mathbb{R}^{m}$。这导致了对所有训练样本计算影响评分的繁琐计算。
- en: Algorithm 1 Procedure of HVP Estimation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 HVP 估计过程
- en: 1:Original training dataset $\mathcal{D}$, iteration number $T$ for $\forall
    i\in\{1,\dots,n\}$.4:for all $t\in\{1,\dots,T\}$;6:Calculate $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$;7:    $\tilde{H}_{t}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\leftarrow\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})+$;
    $\triangleright$.10:Unbiased estimation $\tilde{H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 原始训练数据集 $\mathcal{D}$，对 $\forall i\in\{1,\dots,n\}$ 的迭代次数 $T$。4: 对所有 $t\in\{1,\dots,T\}$;
    6: 计算 $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$; 7: $\tilde{H}_{t}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\leftarrow\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})+$;
    $\triangleright$。10: 无偏估计 $\tilde{H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$。'
- en: '$\bullet\quad$. The idea of stochastic-based HVP estimation is to iteratively
    obtain an unbiased estimator of $H_{\hat{\theta}}$. Specifically, we omit the
    $\hat{\theta}$ terms in Taylor expansion of $H^{-1}$, which can be further rewritten
    recursively as $H_{j}^{-1}=I+(I-H)H_{j-1}^{-1}$ as $j\rightarrow\infty$ as $v$
    at step $t$ can be written as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 随机基 HVP 估计的思想是迭代地获得 $H_{\hat{\theta}}$ 的无偏估计量。具体来说，我们在 $H^{-1}$ 的泰勒展开中省略 $\hat{\theta}$
    项，这可以进一步递归重写为 $H_{j}^{-1}=I+(I-H)H_{j-1}^{-1}$，当 $j\rightarrow\infty$ 时，步长 $t$
    的 $v$ 可以写作：
- en: '| (10) |  | $\small\tilde{H}_{t}^{-1}v=v+\left(I-\nabla_{\theta}^{2}\mathcal{L}(s_{t})\right)\tilde{H}^{-1}_{t-1}v,$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\small\tilde{H}_{t}^{-1}v=v+\left(I-\nabla_{\theta}^{2}\mathcal{L}(s_{t})\right)\tilde{H}^{-1}_{t-1}v,$
    |  |'
- en: where $s_{t}$, and $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$ at step $t$ estimations
    of $H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta})$ (refer to
    Eq. ([9](#S3.E9 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation"))).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在步长 $t$ 处，$s_{t}$ 和 $\nabla_{\theta}^{2}\mathcal{L}(s_{t})$ 是 $H_{\hat{\theta}}^{-1}\nabla_{\theta}\mathcal{L}(s,\hat{\theta})$
    的估计量（见 Eq. ([9](#S3.E9 "在 3.1\. Influence Score ‣ 3\. DEALRec ‣ 基于 LLM 的数据高效微调推荐"))）。
- en: 'To further enhance the efficiency of acquiring influence scores for all samples,
    we use symmetric property to rewrite Eq. ([9](#S3.E9 "In 3.1\. Influence Score
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")) into:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步提高获取所有样本影响评分的效率，我们使用对称性质将 Eq. ([9](#S3.E9 "在 3.1\. Influence Score ‣ 3\.
    DEALRec ‣ 基于 LLM 的数据高效微调推荐")) 重新写作：
- en: '| (11) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle\mathcal{I}_{\text{remove,loss}}(s,{\mathcal{D}})$
    |  |'
- en: The reformulation is based on the assumption that $\mathcal{L}(\cdot)$ is symmetric.
    Since $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\in\mathbb{R}^{m}$,
    we can efficiently obtain influence scores for all samples by only applying HVP
    estimation once for $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$.
    The detailed HVP estimation process is illustrated in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation").
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 重新公式化基于 $\mathcal{L}(\cdot)$ 是对称的假设。由于 $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]\in\mathbb{R}^{m}$，我们可以通过仅对
    $H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$
    应用一次 HVP 估计来高效地获得所有样本的影响评分。详细的 HVP 估计过程在算法 [1](#alg1 "算法 1 ‣ 3.1\. Influence Score
    ‣ 3\. DEALRec ‣ 基于 LLM 的数据高效微调推荐") 中说明。
- en: '![Refer to caption](img/49b29e1cb16246901ab9c2a500267c12.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/49b29e1cb16246901ab9c2a500267c12.png)'
- en: Figure 3\. (a) depicts the different learning ability due to the prior knowledge
    in LLMs. (b) presents the distributions of effort scores of LLM and surrogate
    model on Games dataset⁵⁵5We obtain the effort scores for surrogate model by calculating
    the gradient norm of the parameters of the surrogate model (Eq. ([12](#S3.E12
    "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")))..
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. (a) 描述了由于先前知识在 LLMs 中的不同学习能力。(b) 展示了 Games 数据集上 LLM 和替代模型的努力评分分布⁵⁵5我们通过计算替代模型参数的梯度范数（见
    Eq. ([12](#S3.E12 "在 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ 基于 LLM 的数据高效微调推荐"))）获得替代模型的努力评分。
- en: 3.2\. Gap Regularization
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. Gap Regularization
- en: As shown in Eq. ([11](#S3.E11 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")), assessing the influence score of
    a sample requires the optimized parameters $\hat{\theta}$. Nevertheless, this
    poses challenges for LLM-based recommender models due to the continuous influx
    of large-scale new data in real-world scenarios. In this light, we propose to
    utilize a surrogate model to replace the LLMs and introduce an effort score as
    a gap regularization to complement the learning ability gap between LLMs and the
    surrogate models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程式 ([11](#S3.E11 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")) 所示，评估样本的影响分数需要优化参数 $\hat{\theta}$。然而，由于在实际场景中大规模新数据的持续涌入，这对基于
    LLM 的推荐模型构成了挑战。在这种情况下，我们建议利用代理模型替代 LLMs，并引入努力分数作为间隙正则化，以补充 LLMs 和代理模型之间的学习能力差距。
- en: $\bullet\quad$Surrogate model. To reduce the costs, we propose utilizing a surrogate
    model, *e.g.,* a small-sized traditional recommender model, to compute the influence
    scores. Nevertheless, since LLMs acquire rich world knowledge during the pre-training
    stage, they intricately possess different learning abilities compared to the surrogate
    model (Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence Score ‣
    3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(a)). Therefore,
    the influential samples on LLMs might deviate from the ones for LLMs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 代理模型。为了降低成本，我们建议使用代理模型，*例如* 小型传统推荐模型，来计算影响分数。然而，由于 LLMs 在预训练阶段获取了丰富的世界知识，它们的学习能力与代理模型（图
    [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence Score ‣ 3\. DEALRec ‣
    Data-efficient Fine-tuning for LLM-based Recommendation")(a)）相比复杂地不同。因此，LLMs 上的重要样本可能与
    LLMs 的样本有所偏离。
- en: '$\bullet\quad$ as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 如下：
- en: '| (12) |  | $\delta_{s}=\lVert\nabla_{\phi}\mathcal{L}^{LLM}(s)\rVert_{2},$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\delta_{s}=\lVert\nabla_{\phi}\mathcal{L}^{LLM}(s)\rVert_{2},$
    |  |'
- en: where $\phi$ is the learnable parameters of LLMs⁶⁶6The learnable parameters
    can be either the whole parameters of LLMs or the learnable parameters from parameter-efficient
    training, *e.g.,* LoRA (Hu et al., [2021](#bib.bib24)).. Intuitively, it measures
    the learning effort of LLMs to fit a specific user sequence, and a larger score
    indicates a harder sample for LLMs to learn. To elaborate, Eq. ([12](#S3.E12 "In
    3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")) measures the change in the model parameters, which can be interpreted
    as the discrepancy from the current knowledge encoded in LLMs’ parameters to the
    latest item knowledge or user behavior. As such, the effort score can emphasize
    significant samples particularly for LLMs, supplementing the different learning
    ability of the surrogate model (Figure [5](#footnote5 "footnote 5 ‣ Figure 3 ‣
    3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")(b)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 是 LLMs 的可学习参数⁶⁶6 可学习参数可以是 LLMs 的所有参数，也可以是通过参数高效训练得到的可学习参数，*例如* LoRA
    (Hu et al., [2021](#bib.bib24))。直观地说，它衡量 LLMs 适应特定用户序列的学习努力程度，分数越大，表示 LLMs 学习的样本越困难。具体来说，方程式
    ([12](#S3.E12 "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")) 衡量了模型参数的变化，这可以解释为 LLMs 参数中当前知识与最新项知识或用户行为之间的差异。因此，努力分数可以特别强调
    LLMs 的重要样本，补充代理模型（图 [5](#footnote5 "footnote 5 ‣ Figure 3 ‣ 3.1\. Influence Score
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")(b)）的不同学习能力。
- en: Algorithm 2 Procedure of DEALRec
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 DEALRec 的过程
- en: 1:Original training dataset $\mathcal{D}$, pre-trained parameters of LLM $\phi$.3:Obtain
    estimated ${H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$ do5:    $I_{s_{i}}=\frac{1}{n^{2}}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{j}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{j},\hat{\theta})\right]+$;
    $\triangleright$ Split training samples $\mathcal{D}$ groups according to the
    final score $I_{s}$, $B\leftarrow\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$ do10:    $k^{*}=\mathop{\arg\min}_{k}|G_{k}|$
    randomly select $\mathop{\min}\{B,|G_{k^{*}}|\}$;12:    $\mathcal{S}\leftarrow\mathcal{S}\cup\mathcal{S}_{k^{*}}$;13:    $B\leftarrow\lfloor\frac{r|\mathcal{D}|-|\mathcal{S}|}{|\mathcal{G}|}\rfloor$
    Update sampling budget14:Selected samples $\mathcal{S}$ for few-shot fine-tuning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 原始训练数据集$\mathcal{D}$，LLM的预训练参数$\phi$.3: 获取估计的${H}^{-1}\left[\sum\nolimits_{i}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})\right]$ do5:    $I_{s_{i}}=\frac{1}{n^{2}}\nabla_{\theta}\mathcal{L}(s_{i},\hat{\theta})^{\mathrm{T}}H_{\hat{\theta}}^{-1}\left[\sum\nolimits_{j}\frac{1}{n}\nabla_{\theta}\mathcal{L}(s_{j},\hat{\theta})\right]+$;
    $\triangleright$ 根据最终评分$I_{s}$将训练样本$\mathcal{D}$分组，$B\leftarrow\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$
    do10:    $k^{*}=\mathop{\arg\min}_{k}|G_{k}|$ 随机选择 $\mathop{\min}\{B,|G_{k^{*}}|\}$;12:    $\mathcal{S}\leftarrow\mathcal{S}\cup\mathcal{S}_{k^{*}}$;13:    $B\leftarrow\lfloor\frac{r|\mathcal{D}|-|\mathcal{S}|}{|\mathcal{G}|}\rfloor$
    更新采样预算14: 选择的样本$\mathcal{S}$用于少样本微调。'
- en: '$\bullet\quad$Overall score. By injecting the signals of LLMs’ learning ability
    into the calculation of influence score, we can obtain the final score of each
    user sequence for LLM-based recommender fine-tuning:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$总体评分。通过将LLMs学习能力的信号注入影响力评分的计算中，我们可以获得每个用户序列在基于LLM的推荐系统中的最终评分：
- en: '| (13) |  | $1$2 |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $1$2 |  |'
- en: where $\lambda$ is a hyper-parameter to balance the strength of the gap regularization.
    Notably, the gap regularization would suppress the easy samples with smaller effort
    scores while emphasizing the samples that are more difficult to learn, *i.e.,*
    larger effort scores.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda$是一个超参数，用于平衡间隙正则化的强度。值得注意的是，间隙正则化会抑制那些努力分数较低的简单样本，同时强调那些学习起来更困难的样本，*即*，具有更高的努力分数。
- en: 'Intuitively, DEALRec identifies the influential samples with two key considerations:
    1) the influence score focuses on selecting the representative samples from the
    full dataset, capturing collaborative filtering information for low empirical
    risk; and 2) the effort score highlights the non-trivial samples that are significant
    to the learning of LLMs. The effectiveness of the two scores is empirically validated
    in Section [4.3.1](#S4.SS3.SSS1 "4.3.1\. Ablation Study (RQ2). ‣ 4.3\. In-depth
    Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，DEALRec通过两个关键考虑来识别有影响力的样本：1) 影响力评分集中于从完整数据集中选择代表性样本，捕获低经验风险的协同过滤信息；2) 努力评分突出对LLMs学习重要的非平凡样本。这两种评分的有效性在第[4.3.1](#S4.SS3.SSS1
    "4.3.1\. 消融研究 (RQ2). ‣ 4.3\. 深入分析 ‣ 4\. 实验 ‣ 基于LLM的推荐系统的数据高效微调")节中通过实证验证。
- en: 3.3\. Few-shot Fine-tuning
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 少样本微调
- en: Based on the final influential score obtained via Eq. ([13](#S3.E13 "In 3.2\.
    Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    we can select a subset of data $\mathcal{S}$.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于通过公式([13](#S3.E13 "在3.2\. 间隙正则化 ‣ 3\. DEALRec ‣ 基于LLM的推荐系统的数据高效微调"))获得的最终影响力评分，我们可以选择一个数据子集$\mathcal{S}$。
- en: '$\bullet\quad$ percentage of the training data. However, greedily selecting
    the samples with higher scores might result in very similar samples with low data
    coverage, which leads to: 1) Inadequacy of samples from other areas, thus hurting
    the bounded empirical risk (Zheng et al., [2022](#bib.bib61)) and lowering the
    overall performance (*cf.* Section [4.2](#S4.SS2 "4.2\. Overall Performance (RQ1)
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).
    2) Poor utilization of training samples because of the redundant samples with
    similar patterns, thereby causing suboptimal selection for few-shot fine-tuning.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 训练数据的百分比。然而，贪婪地选择高分样本可能会导致样本间非常相似，数据覆盖度低，从而导致：1) 来自其他领域的样本不足，进而影响有界经验风险（Zheng等，[2022](#bib.bib61)）并降低整体性能（*参见*第[4.2](#S4.SS2
    "4.2\. 整体表现 (RQ1) ‣ 4\. 实验 ‣ 基于LLM的推荐系统的数据高效微调")节）。2) 由于具有相似模式的冗余样本导致训练样本利用不足，从而造成少样本微调的次优选择。
- en: $\bullet\quad$ groups according to their overall scores. We then iteratively
    sample $n_{s}$ is the average sampling budget for all groups and is initialized
    with $\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$. If the group size is smaller than
    the average sampling budget, we select all users from this group and update the
    average sampling budget for the remaining groups (see Algorithm [2](#alg2 "Algorithm
    2 ‣ 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 根据总体分数对组进行分组。然后我们迭代抽样 $n_{s}$ 是所有组的平均抽样预算，并初始化为 $\lfloor\frac{r|\mathcal{D}|}{K}\rfloor$。如果组的大小小于平均抽样预算，我们从该组中选择所有用户，并更新剩余组的平均抽样预算（见算法
    [2](#alg2 "Algorithm 2 ‣ 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")）。
- en: 'Based on the selected few-shot samples $\mathcal{S}$) of LLMs:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基于选择的少量样本 $\mathcal{S}$) 的 LLM：
- en: '| (14) |  | $\hat{\phi}=\mathop{\arg\min}_{\phi\in\Phi}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{s_{i}\in\mathcal{S}}\mathcal{L}_{\phi}^{\text{LLM}}(s_{i}).$
    |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\hat{\phi}=\mathop{\arg\min}_{\phi\in\Phi}\frac{1}{&#124;\mathcal{S}&#124;}\sum_{s_{i}\in\mathcal{S}}\mathcal{L}_{\phi}^{\text{LLM}}(s_{i}).$
    |  |'
- en: $\bullet\quad$ and calculate the influence score for all samples via Eq. ([11](#S3.E11
    "In 3.1\. Influence Score ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")). We then obtain the effort score for LLMs via Eq. ([12](#S3.E12
    "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")), where $\phi$ can be the learnable parameters from any backend
    LLM-based recommender models. Eventually, we apply the stratified sampling to
    select the samples for LLMs’ few-shot fine-tuning. The detailed data pruning process
    of DEALRec is demonstrated in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2\. Gap Regularization
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 通过公式 ([11](#S3.E11 "In 3.1\. Influence Score ‣ 3\. DEALRec ‣
    Data-efficient Fine-tuning for LLM-based Recommendation")) 计算所有样本的影响分数。然后通过公式
    ([12](#S3.E12 "In 3.2\. Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")) 获得 LLM 的努力分数，其中 $\phi$ 可以是任何后端 LLM 基于推荐模型的可学习参数。最终，我们应用分层抽样来选择
    LLM 的少量样本微调。DEALRec 的详细数据剪枝过程在算法 [2](#alg2 "Algorithm 2 ‣ 3.2\. Gap Regularization
    ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation") 中演示。
- en: 4\. Experiment
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 'We conduct extensive experiments on three real-world datasets to answer the
    following research questions:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个真实世界的数据集上进行了广泛的实验，以回答以下研究问题：
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: How does our proposed DEALRec perform compared to the coreset selection
    baselines for LLM-based recommendation and the models trained with full data?'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1：我们提出的 DEALRec 在 LLM 基于推荐的 coreset 选择基准和使用全数据训练的模型中的表现如何？
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: How do the different components of DEALRec (*i.e.,* influence score, gap
    regularization, and stratified sampling) affect the performance, and is DEALRec
    generalizable to different surrogate models?'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2：DEALRec 的不同组件（*即*，影响分数、间隙正则化和分层抽样）如何影响性能，并且 DEALRec 是否可以推广到不同的代理模型？
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: How does DEALRec perform under different selection ratios and how does
    DEALRec improve the overall performance?'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3：DEALRec 在不同选择比例下表现如何，DEALRec 如何提升整体性能？
- en: 4.1\. Experimental Settings
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: 4.1.1\. Datasets.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 数据集。
- en: Table 1\. Statistics of the three datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 三个数据集的统计信息。
- en: '| Datasets | # Users | # Items | # Interactions | Density |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 用户数 | 项目数 | 交互数 | 密度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Games | 49,156 | 17,332 | 342,329 | 0.04% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 游戏 | 49,156 | 17,332 | 342,329 | 0.04% |'
- en: '| MicroLens-50K | 49,887 | 19,217 | 359,048 | 0.04% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MicroLens-50K | 49,887 | 19,217 | 359,048 | 0.04% |'
- en: '| Book | 88,263 | 86,272 | 5,303,707 | 0.07% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 书籍 | 88,263 | 86,272 | 5,303,707 | 0.07% |'
- en: 'We conduct experiments on three real-world recommendation datasets across different
    domains: 1) Games is from the Amazon review datasets⁷⁷7[https://jmcauley.ucsd.edu/data/amazon/.](https://jmcauley.ucsd.edu/data/amazon/.),
    which covers interactions between users and video games with rich textual features
    such as title and categories. 2) MicroLens-50K⁸⁸8[https://github.com/westlake-repl/MicroLens/.](https://github.com/westlake-repl/MicroLens/.)
    is a newly released micro-video recommendation dataset (Ni et al., [2023](#bib.bib39)).
    It contains 50$k$. For the three datasets, we sort all user-item interactions
    according to the global timestamps, and then split the interactions into training,
    validation, and testing sets with the ratio of 8:1:1. Besides, we consider two
    different fine-tuning settings as follows: 1) Few-shot fine-tuning fine-tunes
    LLM-based recommender models with limited samples at a fixed size, *e.g.,* 1024-shot,
    obtained via different data pruning methods. 2) Full fine-tuning utilizes all
    samples to fine-tune LLM-based recommender models without data pruning.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个不同领域的真实推荐数据集上进行了实验：1）Games 数据集来自 Amazon 评价数据集⁷⁷7[https://jmcauley.ucsd.edu/data/amazon/.](https://jmcauley.ucsd.edu/data/amazon/.)，涵盖了用户与视频游戏之间的交互，并且具有丰富的文本特征，如标题和类别。2）MicroLens-50K⁸⁸8[https://github.com/westlake-repl/MicroLens/.](https://github.com/westlake-repl/MicroLens/.)
    是一个新发布的微视频推荐数据集（Ni 等， [2023](#bib.bib39)）。它包含 50$k$。对于这三个数据集，我们根据全局时间戳对所有用户-项目交互进行排序，然后将交互拆分为训练集、验证集和测试集，比例为
    8:1:1。此外，我们考虑了两种不同的微调设置：1）少样本微调使用有限样本（例如，1024-shot）对基于 LLM 的推荐模型进行微调，这些样本通过不同的数据剪枝方法获得。2）全量微调利用所有样本对基于
    LLM 的推荐模型进行微调，不进行数据剪枝。
- en: 4.1.2\. Baselines.
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 基准。
- en: 'We compare DEALRec with the random sampling and several competitive coreset
    selection methods, including difficulty-based methods and diversity-based methods:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 DEALRec 与随机抽样和几种具有竞争力的核心集合选择方法进行了比较，包括基于难度的方法和基于多样性的方法：
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Random obtains the data subset via random sampling, which is a popular and strong
    baseline in data-efficient training (Guo et al., [2022](#bib.bib17)).
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机方法通过随机抽样获得数据子集，这是数据高效训练中的一种流行且强大的基准方法（Guo 等， [2022](#bib.bib17)）。
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GraNd (Paul et al., [2021](#bib.bib40)) is a representative coreset selection
    method that assumes hard samples are important. It uses the average gradient norm
    of each sample during training and selects the samples with larger gradient norms.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GraNd （Paul 等， [2021](#bib.bib40)）是一种代表性的核心集合选择方法，假设困难样本是重要的。它使用每个样本在训练过程中的平均梯度范数，并选择梯度范数较大的样本。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: EL2N (Paul et al., [2021](#bib.bib40)) proposes to select the samples with larger
    errors between the labels and the prediction from the model trained by the original
    dataset. This method is also difficulty-based.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EL2N （Paul 等， [2021](#bib.bib40)）提出了从由原始数据集训练的模型中选择标签和预测之间误差较大的样本的方法。这种方法也是基于难度的。
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CCS (Zheng et al., [2022](#bib.bib61)) is a competitive method that selects
    the samples considering both high data coverage and sample importance. We use
    EL2N as the importance metric for CCS.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CCS （Zheng 等， [2022](#bib.bib61)）是一种竞争性方法，考虑到高数据覆盖率和样本重要性来选择样本。我们使用 EL2N 作为
    CCS 的重要性度量。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TF-DCon (Wu et al., [2023b](#bib.bib54)) is a recently proposed data pruning
    method for content-based recommendation, which clusters the user sequences based
    on the user representations obtained from both well-trained recommender models
    and LLMs for selection.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TF-DCon （Wu 等， [2023b](#bib.bib54)）是一种最近提出的基于内容的数据剪枝方法，它基于从经过充分训练的推荐模型和大型语言模型中获得的用户表示对用户序列进行聚类，以便进行选择。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RecRanker (Luo et al., [2023](#bib.bib37)) proposes a sampling strategy to select
    high-quality user sequences. It selects the users with more interactions for better
    user modeling and utilizes a cluster-based sampling strategy to enhance user diversity.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RecRanker （Luo 等， [2023](#bib.bib37)）提出了一种采样策略来选择高质量的用户序列。它选择互动更多的用户以改善用户建模，并利用基于聚类的采样策略来增强用户多样性。
- en: 'We do not perform optimization-based methods for comparison because of the
    inapplicability of complex bi-level or discrete optimization for LLMs on large-scale
    recommendation data (*cf.* Section [2](#S2 "2\. Task Formulation ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation")). We instantiate our proposed DEALRec
    and all baselines on two competitive backend LLM-based recommender models: 1)
    BIGRec (Bao et al., [2023a](#bib.bib4)) utilizes the item title to present the
    user sequence for recommendation generation; 2) TIGER (Rajput et al., [2023](#bib.bib41))
    learns extra tokens from item features to present items, and then converts the
    user sequence into the sequence of the new item token for next-item generation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有执行基于优化的方法进行比较，因为复杂的双层或离散优化在大规模推荐数据上对 LLM 不适用 (*参见* 第 [2](#S2 "2\. 任务定义 ‣
    面向 LLM 推荐的高效微调") 节)。我们在两个竞争性的后端 LLM 推荐模型上实例化我们提出的 DEALRec 和所有基线模型：1）BIGRec (Bao
    et al., [2023a](#bib.bib4)) 利用项目标题来表示用户序列以生成推荐；2）TIGER (Rajput et al., [2023](#bib.bib41))
    从项目特征中学习额外的标记来表示项目，然后将用户序列转换为新项目标记的序列以生成下一个项目。
- en: $\bullet\quad$ and NDCG@$K$ set to $10$ for Games, and $K=20$ for MicroLens-50K
    and Book.⁹⁹9We report metrics@$20$ because of the challenging modeling of user
    behavior on book and micro-video recommendations, where the temporal shifts of
    user interests and the item feature is stronger and thus more difficult to capture (Wang
    et al., [2023a](#bib.bib50), [b](#bib.bib51)). For the two backend LLM-based recommender
    models, we adopt full ranking protocal (He and Chua, [2017](#bib.bib22)) for BIGRec.
    Since TIGER does not support full ranking, we select the top-$K$ items from the
    generated items according to the probability scores for evaluation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$ 和 NDCG@$K$ 设置为 $10$ 用于 Games，$K=20$ 用于 MicroLens-50K 和 Book。⁹⁹9
    我们报告指标@$20$ 是因为书籍和微视频推荐中用户行为建模具有挑战性，其中用户兴趣的时间变化和项目特征更强，因此更难捕捉 (Wang et al., [2023a](#bib.bib50),
    [b](#bib.bib51))。对于两个后端 LLM 推荐模型，我们采用完整排序协议 (He 和 Chua, [2017](#bib.bib22)) 用于
    BIGRec。由于 TIGER 不支持完整排序，我们根据概率分数从生成的项目中选择前 $K$ 个项目进行评估。
- en: Table 2\. Overall performance comparison between the baselines and DEALRec instantiated
    on two competitive LLM-based recommender models on three datasets. For each backend
    model, the bold results highlight the best results while the second-best ones
    are underlined. $*$-value ¡ 0.01) under one-sample t-tests. We run all experiments
    for 3 times with different random seeds and report the averaged results.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 基线模型与 DEALRec 在三个数据集上实例化的两个竞争性 LLM 推荐模型之间的整体性能比较。对于每个后端模型，粗体结果突出显示最佳结果，而次佳结果则有下划线。$*$-值
    ¡ 0.01) 是通过单样本 t 检验得出的。我们进行所有实验 3 次，使用不同的随机种子并报告平均结果。
- en: '|  |  | Games | MicroLens-50K | Book |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Games | MicroLens-50K | Book |'
- en: '|  |  | 1024-shot ($\bm{r}$=2%) | 1024-shot ($\bm{r}$=1%) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1024-shot ($\bm{r}$=2%) | 1024-shot ($\bm{r}$=1%) |'
- en: '|  | Methods | R@10 | R@20 | N@10 | N@20 | R@20 | R@50 | N@20 | N@50 | R@20
    | R@50 | N@20 | N@50 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | R@10 | R@20 | N@10 | N@20 | R@20 | R@50 | N@20 | N@50 | R@20 | R@50
    | N@20 | N@50 |'
- en: '|  | TF-DCon | 0.0102 | 0.0157 | 0.0062 | 0.0078 | 0.0066 | 0.0099 | 0.0027
    | 0.0034 | 0.0104 | 0.0144 | 0.0083 | 0.0092 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | TF-DCon | 0.0102 | 0.0157 | 0.0062 | 0.0078 | 0.0066 | 0.0099 | 0.0027
    | 0.0034 | 0.0104 | 0.0144 | 0.0083 | 0.0092 |'
- en: '|  | RecRanker | 0.0112 | 0.0166 | 0.0074 | 0.0090 | 0.0024 | 0.0042 | 0.0011
    | 0.0014 | 0.0108 | 0.0145 | \ul0.0090 | \ul0.0097 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | RecRanker | 0.0112 | 0.0166 | 0.0074 | 0.0090 | 0.0024 | 0.0042 | 0.0011
    | 0.0014 | 0.0108 | 0.0145 | \ul0.0090 | \ul0.0097 |'
- en: '|  | CCS | \ul0.0164 | 0.0246 | 0.0097 | 0.0122 | 0.0096 | 0.0131 | 0.0041
    | 0.0049 | \ul0.0110 | 0.0145 | 0.0088 | 0.0096 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | CCS | \ul0.0164 | 0.0246 | 0.0097 | 0.0122 | 0.0096 | 0.0131 | 0.0041
    | 0.0049 | \ul0.0110 | 0.0145 | 0.0088 | 0.0096 |'
- en: '|  | GraNd | 0.0158 | 0.0250 | 0.0098 | 0.0125 | 0.0014 | 0.0032 | 0.0006 |
    0.0010 | 0.0102 | 0.0136 | 0.0080 | 0.0087 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | GraNd | 0.0158 | 0.0250 | 0.0098 | 0.0125 | 0.0014 | 0.0032 | 0.0006 |
    0.0010 | 0.0102 | 0.0136 | 0.0080 | 0.0087 |'
- en: '|  | EL2N | 0.0154 | \ul0.0256 | 0.0098 | \ul0.0128 | 0.0096 | 0.0045 | 0.0041
    | 0.0016 | 0.0107 | \ul0.0149 | 0.0085 | 0.0094 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | EL2N | 0.0154 | \ul0.0256 | 0.0098 | \ul0.0128 | 0.0096 | 0.0045 | 0.0041
    | 0.0016 | 0.0107 | \ul0.0149 | 0.0085 | 0.0094 |'
- en: '|  | Random | 0.0163 | 0.0241 | \ul0.0100 | 0.0122 | \ul0.0108 | \ul0.0151
    | \ul0.0044 | \ul0.0054 | 0.0099 | 0.0134 | 0.0083 | 0.0090 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机 | 0.0163 | 0.0241 | \ul0.0100 | 0.0122 | \ul0.0108 | \ul0.0151 | \ul0.0044
    | \ul0.0054 | 0.0099 | 0.0134 | 0.0083 | 0.0090 |'
- en: '| BIGRec | DEALRec | 0.0181* | 0.0276* | 0.0115* | 0.0142* | 0.0124* | 0.0160*
    | 0.0055* | 0.0064* | 0.0117* | 0.0155* | 0.0096* | 0.0104* |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| BIGRec | DEALRec | 0.0181* | 0.0276* | 0.0115* | 0.0142* | 0.0124* | 0.0160*
    | 0.0055* | 0.0064* | 0.0117* | 0.0155* | 0.0096* | 0.0104* |'
- en: '|  | TF-DCon | 0.0051 | 0.0074 | 0.0033 | 0.0040 | 0.0006 | 0.0057 | 0.0002
    | 0.0013 | 0.0028 | 0.0051 | 0.0020 | 0.0027 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | TF-DCon | 0.0051 | 0.0074 | 0.0033 | 0.0040 | 0.0006 | 0.0057 | 0.0002
    | 0.0013 | 0.0028 | 0.0051 | 0.0020 | 0.0027 |'
- en: '|  | RecRanker | 0.0028 | 0.0045 | 0.0019 | 0.0024 | \ul0.0043 | \ul0.0064
    | \ul0.0011 | \ul0.0014 | 0.0027 | 0.0052 | 0.0018 | 0.0025 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | RecRanker | 0.0028 | 0.0045 | 0.0019 | 0.0024 | \ul0.0043 | \ul0.0064
    | \ul0.0011 | \ul0.0014 | 0.0027 | 0.0052 | 0.0018 | 0.0025 |'
- en: '|  | CCS | 0.0050 | 0.0084 | 0.0031 | 0.0041 | 0.0026 | 0.0061 | 0.0010 | 0.0013
    | 0.0026 | 0.0048 | 0.0018 | 0.0024 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | CCS | 0.0050 | 0.0084 | 0.0031 | 0.0041 | 0.0026 | 0.0061 | 0.0010 | 0.0013
    | 0.0026 | 0.0048 | 0.0018 | 0.0024 |'
- en: '|  | GraNd | 0.0042 | 0.0053 | 0.0027 | 0.0030 | 0.0006 | 0.0014 | 0.0003 |
    0.0005 | 0.0008 | 0.0020 | 0.0006 | 0.0010 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | GraNd | 0.0042 | 0.0053 | 0.0027 | 0.0030 | 0.0006 | 0.0014 | 0.0003 |
    0.0005 | 0.0008 | 0.0020 | 0.0006 | 0.0010 |'
- en: '|  | EL2N | 0.0034 | 0.0048 | 0.0024 | 0.0029 | 0.0011 | 0.0016 | 0.0004 |
    0.0004 | 0.0005 | 0.0015 | 0.0004 | 0.0007 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | EL2N | 0.0034 | 0.0048 | 0.0024 | 0.0029 | 0.0011 | 0.0016 | 0.0004 |
    0.0004 | 0.0005 | 0.0015 | 0.0004 | 0.0007 |'
- en: '|  | Random | \ul0.0062 | \ul0.0102 | \ul0.0039 | \ul0.0051 | 0.0037 | 0.0059
    | 0.0011 | 0.0014 | \ul0.0033 | \ul0.0066 | \ul0.0022 | \ul0.0031 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机 | \ul0.0062 | \ul0.0102 | \ul0.0039 | \ul0.0051 | 0.0037 | 0.0059 |
    0.0011 | 0.0014 | \ul0.0033 | \ul0.0066 | \ul0.0022 | \ul0.0031 |'
- en: '| TIGER | DEALRec | 0.0074* | 0.0114* | 0.0062* | 0.0074* | 0.0058* | 0.0076*
    | 0.0020* | 0.0020* | 0.0039* | 0.0076* | 0.0026* | 0.0037* |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| TIGER | DEALRec | 0.0074* | 0.0114* | 0.0062* | 0.0074* | 0.0058* | 0.0076*
    | 0.0020* | 0.0020* | 0.0039* | 0.0076* | 0.0026* | 0.0037* |'
- en: Table 3\. Performance comparison between DEALRec under 1024-shot fine-tuning
    and the full fine-tuning of the BIGRec in terms of both accuracy and time costs.
    “%Com.” denotes the performance achieved by DEALRec compared to the full fine-tuning.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 在准确性和时间成本方面，1024-shot 微调下 DEALRec 与 BIGRec 全部微调的性能比较。“%Com.” 表示 DEALRec
    相对于全面微调所取得的性能。
- en: '|  | Games | MicroLens-50K | Book |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏 | MicroLens-50K | 图书 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | R@10$\uparrow$ | N@10$\uparrow$ | Time$\downarrow$ | R@50$\uparrow$ |
    N@50$\uparrow$ | R@20$\uparrow$ | N@20$\uparrow$ | Time$\downarrow$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | R@10$\uparrow$ | N@10$\uparrow$ | Time$\downarrow$ | R@50$\uparrow$ |
    N@50$\uparrow$ | R@20$\uparrow$ | N@20$\uparrow$ | Time$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '| Full | 0.0169 | 0.0233 | 0.0102 | 0.0120 | 36.87h | 0.0081 | 0.0136 | 0.0038
    | 0.0053 | 66.64h | 0.0076 | 0.0108 | 0.0060 | 0.0068 | 84.77h |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Full | 0.0169 | 0.0233 | 0.0102 | 0.0120 | 36.87h | 0.0081 | 0.0136 | 0.0038
    | 0.0053 | 66.64h | 0.0076 | 0.0108 | 0.0060 | 0.0068 | 84.77h |'
- en: '| DEALRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 1.67h | 0.0124 | 0.0160 | 0.0055
    | 0.0064 | 1.23h | 0.0117 | 0.0155 | 0.0096 | 0.0104 | 1.93h |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| DEALRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 1.67h | 0.0124 | 0.0160 | 0.0055
    | 0.0064 | 1.23h | 0.0117 | 0.0155 | 0.0096 | 0.0104 | 1.93h |'
- en: '| % Com. | 107.10% | 118.45% | 112.75% | 118.33% | 4.53% | 153.09% | 117.65%
    | 144.74% | 120.75% | 1.85% | 153.95% | 143.52% | 160.00% | 152.94% | 2.28% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| % Com. | 107.10% | 118.45% | 112.75% | 118.33% | 4.53% | 153.09% | 117.65%
    | 144.74% | 120.75% | 1.85% | 153.95% | 143.52% | 160.00% | 152.94% | 2.28% |'
- en: 4.1.3\. Implementation.
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3. 实现。
- en: As for the two backend LLM-based recommender models, we follow the original
    settings in their paper for implementation. We employ LLaMA-7B for BIGRec and
    transformer-based encoder-decoder architecture for TIGER as introduced in their
    paper (Rajput et al., [2023](#bib.bib41)). All fine-tuning experiments are conducted
    on four NVIDIA RTX A5000 GPUs. Besides, we adopt the parameter-efficient fine-tuning
    technique LoRA (Hu et al., [2021](#bib.bib24)) to fine-tune BIGRec and fully fine-tune
    the parameters of TIGER. We utilize SASRec (Kang and McAuley, [2018](#bib.bib25)),
    a representative sequential recommender model, as the surrogate model in DEALRec.
    We set the iteration number $T$ in $\{0.1,0.3,0.5,1.0,2.0\}$ is explored in $\{25,50,75\}$.
    As for the coreset selection methods that require the training of LLMs, we consider
    a feasible implementation (Coleman et al., [2020](#bib.bib9)) by executing them
    on the same surrogate model as DEALRec.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 至于两个基于 LLM 的推荐模型，我们遵循它们论文中的原始设置进行实现。我们为 BIGRec 使用 LLaMA-7B，并为 TIGER 使用其论文中介绍的基于
    transformer 的编码器-解码器架构 (Rajput et al., [2023](#bib.bib41))。所有的微调实验都在四台 NVIDIA
    RTX A5000 GPU 上进行。此外，我们采用了参数高效微调技术 LoRA (Hu et al., [2021](#bib.bib24)) 来微调 BIGRec，并对
    TIGER 的参数进行完全微调。我们利用 SASRec (Kang and McAuley, [2018](#bib.bib25))，一个具有代表性的序列推荐模型，作为
    DEALRec 的替代模型。我们设置迭代次数 $T$ 在 $\{0.1,0.3,0.5,1.0,2.0\}$ 中探索 $\{25,50,75\}$。对于需要训练
    LLM 的 coreset 选择方法，我们考虑通过在与 DEALRec 相同的替代模型上执行来实现 (Coleman et al., [2020](#bib.bib9))。
- en: 4.2\. Overall Performance (RQ1)
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 总体性能 (RQ1)
- en: 'The results of the baselines and DEALRec with two competitive backend LLM-based
    recommender models on three datasets under few-shot fine-tuning (1024 samples)
    are presented in Table [2](#S4.T2 "Table 2 ‣ 4.1.2\. Baselines. ‣ 4.1\. Experimental
    Settings ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation"),
    from which we have the following observations:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S4.T2 "Table 2 ‣ 4.1.2\. Baselines. ‣ 4.1\. Experimental Settings ‣ 4\.
    Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")展示了在三个数据集下，使用两种竞争性后端LLM推荐模型的基准和DEALRec的结果（1024个样本），从中我们可以得到以下观察：
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'All methods with BIGRec typically yield better performance than those with
    TIGER, which is attributed to two reasons: 1) BIGRec employs a larger LLM (*i.e.,*
    LLaMA-7B) compared to TIGER, thereby benefiting from the stronger generalization
    ability of large-sized LLMs (Lin et al., [2023](#bib.bib33)); and 2) BIGRec leverages
    item titles to present the user sequence, leading to better utilization of world
    knowledge in LLMs. In contrast, TIGER learns extra item tokens for LLMs. This
    might result in cold-start item issues since only limited item tokens are learned
    while others are maintained randomly initialized under the few-shot fine-tuning
    setting.'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用BIGRec的所有方法通常比使用TIGER的方法表现更好，这归因于两个原因：1) 与TIGER相比，BIGRec使用了更大的LLM（*即* LLaMA-7B），从而受益于大型LLM的更强泛化能力（Lin等，[2023](#bib.bib33)）；2)
    BIGRec利用项目标题来呈现用户序列，从而更好地利用了LLM中的世界知识。相比之下，TIGER为LLM学习额外的项目标记。这可能导致冷启动项目问题，因为只有有限的项目标记被学习，而其他标记在少量样本微调设置下保持随机初始化。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Among all coreset selection baselines, difficulty-based (GraNd, EL2N) methods
    generally perform better than diversity-based methods (TF-DCon, RecRanker). This
    is reasonable since diversity-based methods merely heuristically encourage selecting
    users with divergent preference, which lacks the assessments of their contributions
    to the model training. In contrast, GraNd and EL2N use pre-defined metrics to
    measure the sample difficulty and select the samples with larger scores, which
    encourages selecting the samples that are more informative for models’ optimization.
    Besides, CCS improves EL2N in most cases, as it maintains easy samples for selection,
    thus compensating the knowledge of recommendation data from high-density areas.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在所有核心集选择基准中，基于难度的方法（GraNd, EL2N）通常比基于多样性的方法（TF-DCon, RecRanker）表现更好。这是合理的，因为基于多样性的方法仅通过启发式地鼓励选择具有不同偏好的用户，但缺乏对其对模型训练贡献的评估。相比之下，GraNd和EL2N使用预定义的指标来测量样本难度，并选择分数较高的样本，这鼓励选择对模型优化更有信息量的样本。此外，CCS在大多数情况下改善了EL2N，因为它保持了易于选择的样本，从而弥补了从高密度区域获得的推荐数据的知识。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Another interesting observation is that random sampling yields competitive
    performance or even outperforms other coreset selection methods in some cases,
    which might attributed to two possible reasons: 1) Uniformly selected user sequences
    preserve high coverage of the original training distribution compared to other
    baselines, which ensures a high probability of guaranteed bound for low empirical
    risk (Zheng et al., [2022](#bib.bib61)). This observation is also consistent with
    the findings in (Guo et al., [2022](#bib.bib17)). 2) The inferior performance
    of some coreset selection methods also might be caused by the implementation settings
    (Section [4.1.3](#S4.SS1.SSS3 "4.1.3\. Implementation. ‣ 4.1\. Experimental Settings
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")),
    where they may suffer from the learning ability gap between the surrogate model
    and LLMs. (*cf.* Section [3.2](#S3.SS2 "3.2\. Gap Regularization ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")).'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个有趣的观察是，随机抽样在某些情况下表现出竞争力的性能，甚至超越了其他核心集选择方法，这可能归因于两个可能的原因：1) 相比其他基准，均匀选择的用户序列保持了原始训练分布的高覆盖率，这确保了低经验风险的保证界限的高概率（Zheng等，[2022](#bib.bib61)）。这一观察结果也与（Guo等，[2022](#bib.bib17)）的发现一致。2)
    一些核心集选择方法的劣质性能也可能是由于实施设置（第[4.1.3](#S4.SS1.SSS3 "4.1.3\. Implementation. ‣ 4.1\.
    Experimental Settings ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")节），在这些设置中，它们可能遭受了代理模型与LLMs之间的学习能力差距。（*参见* 第[3.2](#S3.SS2 "3.2\.
    Gap Regularization ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based Recommendation")节）。
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DEALRec significantly outperforms all coreset selection methods across the
    three datasets. The consistent performance improvements on both backend models
    validate the superiority of DEALRec in identifying influential samples for LLMs’
    adaptation to the recommendation data. The superior performance is attributed
    to: 1) the accurate and efficient estimation of the influence on empirical risk,
    *i.e.,* overall performance by removing a sample in training; and 2) the gap regularization
    based on the effort score to penalize the easy samples for LLMs. By emphasizing
    the non-trivial samples specifically for LLMs, gap regularization alleviates the
    learning ability gap between the surrogate model and the LLMs.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DEALRec 在三个数据集上显著优于所有核心集选择方法。在两个后端模型上的一致性性能提升验证了 DEALRec 在识别对 LLM 适应推荐数据的有影响样本方面的优越性。这种卓越性能归因于：1)
    对经验风险，即通过在训练中去除一个样本对总体性能的准确和高效的估计；以及 2) 基于努力评分的间隙正则化，旨在惩罚 LLM 的简单样本。通过专门强调 LLM
    的非平凡样本，间隙正则化减轻了代理模型与 LLM 之间的学习能力差距。
- en: '$\bullet\quad$Comparison with full fine-tuning. We further compare DEALRec
    with BIGRec under full training *w.r.t.* accuracy and efficiency, as presented
    in Table [3](#S4.T3 "Table 3 ‣ 4.1.2\. Baselines. ‣ 4.1\. Experimental Settings
    ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
    We can find that: 1) DEALRec achieves higher performance compared to the model
    trained by the full data, indicating the effectiveness of DEALRec for high accuracy.
    The inferior performance of BIGRec under full training also implies that not all
    user sequences are informative for model training, or even harmful to the training,
    *e.g.,* false negative interactions. This has also been observed in CTR prediction (Wu
    et al., [2023a](#bib.bib53)) and has been discussed in (Agarwal et al., [2020](#bib.bib3))
    from the view of data redundancy. 2) DEALRec significantly reduces the time costs
    for LLMs’ fine-tuning (97.11% reduction of fine-tuning costs on average). With
    the remarkably declined training costs, DEALRec has the potential to facilitate
    real-world applications of LLM-based recommender models.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$与全量微调的比较。我们进一步将 DEALRec 与 BIGRec 在全量训练下的准确性和效率进行比较，如表[3](#S4.T3
    "表 3 ‣ 4.1.2\. 基准 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ 基于 LLM 的数据高效微调")所示。我们可以发现：1) 与使用全量数据训练的模型相比，DEALRec
    实现了更高的性能，表明 DEALRec 在高准确性方面的有效性。BIGRec 在全量训练下的较差表现也意味着并非所有用户序列对模型训练都有信息价值，甚至可能对训练有害，例如虚假负交互。这在
    CTR 预测（Wu 等，[2023a](#bib.bib53)）中也有所观察，并在（Agarwal 等，[2020](#bib.bib3)）中从数据冗余的角度进行了讨论。2)
    DEALRec 显著减少了 LLM 微调的时间成本（平均减少了 97.11% 的微调成本）。由于训练成本显著下降，DEALRec 有潜力促进基于 LLM 的推荐模型在实际应用中的应用。
- en: 4.3\. In-depth Analysis
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 深入分析
- en: We carry out further experiments^(10)^(10)10We conduct in-depth experiments
    using BIGRec as the backend model because of its better performance compared to
    TIGER. to analyze the effectiveness of each component of DEALRec and the robustness
    of surrogate model selection. Besides, we investigate how DEALRec performs under
    different selection ratios and explore how DEALRec performs over different user
    groups.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步进行实验^(10)^(10)10我们使用 BIGRec 作为后端模型进行深入实验，因为其性能优于 TIGER。以分析 DEALRec 每个组件的有效性和代理模型选择的稳健性。此外，我们还研究了
    DEALRec 在不同选择比例下的表现，并探索 DEALRec 在不同用户群体中的表现。
- en: 4.3.1\. Ablation Study (RQ2).
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 消融研究 (RQ2)。
- en: '![Refer to caption](img/537dfd4059a78d475e54fc2600d2bf87.png)![Refer to caption](img/8bda131f113acd2ba54bc1d60dd2bba2.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/537dfd4059a78d475e54fc2600d2bf87.png)![参考说明](img/8bda131f113acd2ba54bc1d60dd2bba2.png)'
- en: Figure 4\. Ablation study of the influence score, effort score, and coverage-enhanced
    sample selection strategy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 影响评分、努力评分和覆盖度增强样本选择策略的消融研究。
- en: 'To study the effectiveness of each component of DEALRec, *i.e.,* influence
    score, effort score, and coverage-enhanced sample selection strategy, we separately
    remove the Influence Score (IS) and effort score $\delta_{s}$”, respectively.
    Besides, we replace the coverage-enhanced sample selection strategy by greedily
    selecting the samples with higher scores, denoted as “Greedy”. From the results
    presented in Figure [4](#S4.F4 "Figure 4 ‣ 4.3.1\. Ablation Study (RQ2). ‣ 4.3\.
    In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation"), we can observe that: removing either the influence score or
    effort score will cause performance drops. This validates the effectiveness of
    1) the assessment of overall performance change caused by removing samples from
    training; 2) additional signals of learning ability captured from LLMs as regularization,
    alleviating the gap between the surrogate model and the LLMs. Moreover, simply
    selecting the samples with higher overall scores might weaken the learning of
    distinct user behaviors and item knowledge (inferior performance of “Greedy”),
    as discussed in Section [3.3](#S3.SS3 "3.3\. Few-shot Fine-tuning ‣ 3\. DEALRec
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究DEALRec中每个组件的有效性，即，影响评分、努力评分和覆盖增强样本选择策略，我们分别去除了影响评分（IS）和努力评分$\delta_{s}$”。此外，我们用贪心选择具有更高评分的样本来替代覆盖增强样本选择策略，记为“贪心”。从图[4](#S4.F4
    "Figure 4 ‣ 4.3.1\. Ablation Study (RQ2). ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment
    ‣ Data-efficient Fine-tuning for LLM-based Recommendation")中呈现的结果可以观察到：去除影响评分或努力评分都会导致性能下降。这验证了1）通过从训练中去除样本评估整体性能变化的有效性；2）从LLMs中捕捉到的额外学习能力信号作为正则化，缓解了替代模型和LLMs之间的差距。此外，简单地选择总体评分更高的样本可能会削弱对不同用户行为和项目知识的学习（“贪心”的性能较差），如第[3.3](#S3.SS3
    "3.3\. Few-shot Fine-tuning ‣ 3\. DEALRec ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation")节讨论的那样。
- en: 4.3.2\. Robustness on different surrogate model (RQ2).
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 在不同替代模型上的鲁棒性（RQ2）。
- en: Table 4\. Performance comparison between DEALRec with different surrogate models
    and the BIGRec under full training. “Time” presents the time costs for training
    the surrogate model on a single NVIDIA RTX A5000.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. DEALRec与不同替代模型以及在完全训练下的BIGRec的性能比较。“时间”表示在单个NVIDIA RTX A5000上训练替代模型的时间成本。
- en: '|  | R@10$\uparrow$ | N@10$\uparrow$ | Time$\downarrow$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | R@10$\uparrow$ | N@10$\uparrow$ | 时间$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Full | 0.0169 | 0.0233 | 0.0102 | 0.0120 | / |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | 0.0169 | 0.0233 | 0.0102 | 0.0120 | / |'
- en: '| BERT4Rec | 0.0175 | 0.0258 | 0.0103 | 0.0128 | 0.76h |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| BERT4Rec | 0.0175 | 0.0258 | 0.0103 | 0.0128 | 0.76小时 |'
- en: '| SASRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 0.45h |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| SASRec | 0.0181 | 0.0276 | 0.0115 | 0.0142 | 0.45小时 |'
- en: '| DCRec | 0.0211 | 0.0283 | 0.0117 | 0.0137 | 0.61h |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DCRec | 0.0211 | 0.0283 | 0.0117 | 0.0137 | 0.61小时 |'
- en: To further assess the generalization ability of DEALRec on different surrogate
    models, we employ three representative sequential recommender models, *i.e.,*
    BERT4Rec (Sun et al., [2019](#bib.bib45)), SASRec (Kang and McAuley, [2018](#bib.bib25)),
    and a recently proposed DCRec (Yang et al., [2023a](#bib.bib57)) as the surrogate
    models for data pruning, respectively. The results on Games are presented in Table [4](#S4.T4
    "Table 4 ‣ 4.3.2\. Robustness on different surrogate model (RQ2). ‣ 4.3\. In-depth
    Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation"),
    and we omit the results with similar observations on the other two datasets to
    save space.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步评估DEALRec在不同替代模型上的泛化能力，我们使用了三个具有代表性的序列推荐模型，即，BERT4Rec（Sun等，[2019](#bib.bib45)）、SASRec（Kang和McAuley，[2018](#bib.bib25)）以及最近提出的DCRec（Yang等，[2023a](#bib.bib57)）作为数据修剪的替代模型。表[4](#S4.T4
    "Table 4 ‣ 4.3.2\. Robustness on different surrogate model (RQ2). ‣ 4.3\. In-depth
    Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation")中呈现了在Games上的结果，我们省略了其他两个数据集上类似观察的结果以节省空间。
- en: 'From the table, we can find that: 1) DEALRec with the three surrogate models
    consistently outperforms BIGRec under full fine-tuning. This demonstrates the
    strong robustness of DEALRec on different surrogate models. 2) Nonetheless, different
    surrogate models cause some fluctuations in the accuracy of the LLM-based recommender
    model. This is reasonable because different model architectures have different
    expressiveness of user behavior and item knowledge. As such, the selected samples
    possibly vary across different surrogate models, thus affecting the effectiveness
    of LLMs’ few-shot fine-tuning. 2) It is noted that DCRec surpasses SASRec and
    BERTRec by a large margin. The possible reason might be that the SOTA DCRec employs
    contrastive learning to enhance the learning of user representations, thus leading
    to better user modeling and yielding lower empirical risk. 3) SASRec exhibits
    the least time costs for training and achieves competitive performance among the
    three surrogate models. Therefore, based on the empirical results, SASRec could
    be a good choice of surrogate model for DEALRec to facilitate efficient LLM-based
    recommender fine-tuning in real-world deployments.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格中可以发现：1) DEALRec 在三个替代模型下的表现始终优于 BIGRec，显示了 DEALRec 在不同替代模型上的强鲁棒性。2) 然而，不同的替代模型会导致
    LLM 基推荐模型的准确性有一些波动。这是合理的，因为不同的模型架构对用户行为和项目信息的表达能力不同。因此，选定的样本在不同替代模型中可能有所不同，从而影响
    LLM 的少量样本微调的有效性。2) 注意到 DCRec 比 SASRec 和 BERTRec 超过了较大的差距。可能的原因是 SOTA DCRec 采用了对比学习来增强用户表示的学习，从而导致更好的用户建模和较低的实证风险。3)
    SASRec 在训练时间成本上最低，并且在三个替代模型中表现具有竞争力。因此，根据实证结果，SASRec 可能是 DEALRec 在实际部署中进行高效 LLM
    基推荐微调的良好选择。
- en: 4.3.3\. Effect of selection ratio $\bm{r}$ (RQ3).
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 选择比例 $\bm{r}$ 的效果 (RQ3)。
- en: '![Refer to caption](img/9dea565f63a4c12729187c4494df60fb.png)![Refer to caption](img/c111e07ab5f650ae8c81ef38a4a2da2d.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9dea565f63a4c12729187c4494df60fb.png)![参见标题](img/c111e07ab5f650ae8c81ef38a4a2da2d.png)'
- en: Figure 5\. Performance of DEALRec with different selection ratio $r$ *w.r.t.*
    accuracy and efficiency on Games.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. DEALRec 在不同选择比例 $r$ 下的准确性和效率表现（Games）。
- en: 'To investigate the effect of selection ratio $r$ from $0.2\%$ (4096-shot) and
    present the results in Figure [5](#S4.F5 "Figure 5 ‣ 4.3.3\. Effect of selection
    ratio 𝑟 (RQ3). ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation"). From the figures, it is observed that: 1) The
    recommendation accuracy rapidly improves as the number of selected samples increases
    from $0.2\%$, surpassing the full training when $r=1\%$ to $4\%$. 3) Empirically,
    setting $r=1\%$ is recommended to achieve comparable performance to full fine-tuning
    as well as achieving low costs for real-world deployments.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究选择比例 $r$ 从 $0.2\%$ (4096-shot) 的效果，并在图 [5](#S4.F5 "Figure 5 ‣ 4.3.3\. Effect
    of selection ratio 𝑟 (RQ3). ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient
    Fine-tuning for LLM-based Recommendation") 中展示结果。从这些图中可以观察到：1) 当选择的样本数量从 $0.2\%$
    增加时，推荐准确性迅速提高，当 $r=1\%$ 到 $4\%$ 时超过了完全训练的效果。3) 实证上，建议设置 $r=1\%$ 以实现与完全微调相当的性能，同时在实际部署中成本较低。
- en: 4.3.4\. User group evaluation (RQ3).
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 用户组评估 (RQ3)。
- en: '![Refer to caption](img/ccf8917abad159f99ec9bfb8d878efcb.png)![Refer to caption](img/12cb4422c299f7171ef43274715a3ddb.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ccf8917abad159f99ec9bfb8d878efcb.png)![参见标题](img/12cb4422c299f7171ef43274715a3ddb.png)'
- en: Figure 6\. Performance of DEALRec over easy to difficult samples (Group 1 to
    Group 3).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. DEALRec 在从简单到困难样本（组 1 到组 3）上的表现。
- en: To study how DEALRec achieves superior overall performance, we test the DEALRec
    over user sequences of different difficulties. Specifically, we calculate the
    loss of each user sequence via the model trained by randomly selected few-shot
    samples; we then divide the users into three groups according to their loss values,
    from the easier samples with smaller loss (Group 1) to the harder samples with
    larger loss (Group 3). The results of each group of DEALRec and Random on Games
    are presented in Figure [6](#S4.F6 "Figure 6 ‣ 4.3.4\. User group evaluation (RQ3).
    ‣ 4.3\. In-depth Analysis ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based
    Recommendation"). We can find that 1) the performance of both DEALRec and Random
    gradually declines from Group 1 to Group 3, because users with larger loss are
    more difficult to predict. Nevertheless, 2) DEALRec consistently outperforms Random
    in each group, which validates the effectiveness of DEALRec in considering the
    influence on overall performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究 DEALRec 如何实现卓越的整体性能，我们测试了 DEALRec 在不同难度的用户序列上的表现。具体来说，我们通过随机选择的少量样本训练的模型来计算每个用户序列的损失；然后，我们根据损失值将用户分为三组，从损失较小的较易样本（组
    1）到损失较大的较难样本（组 3）。图 [6](#S4.F6 "图 6 ‣ 4.3.4\. 用户组评估 (RQ3). ‣ 4.3\. 深度分析 ‣ 4\.
    实验 ‣ 数据高效微调") 展示了 DEALRec 和随机方法在 Games 上的各组结果。我们可以发现 1) DEALRec 和随机方法的性能都从组 1
    到组 3 逐渐下降，因为损失较大的用户更难以预测。尽管如此，2) DEALRec 在每组中始终优于随机方法，这验证了 DEALRec 在考虑整体性能影响方面的有效性。
- en: 4.3.5\. Effect of regularization strength $\bm{\lambda}$.
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5\. 正则化强度 $\bm{\lambda}$ 的影响。
- en: '![Refer to caption](img/8a95e1a4ca1f22e264e897957fbbece5.png)![Refer to caption](img/87589dc72705f29b2211882544c95675.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8a95e1a4ca1f22e264e897957fbbece5.png)![参见标题](img/87589dc72705f29b2211882544c95675.png)'
- en: Figure 7\. Performance of DEALRec with different $\lambda$.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. DEALRec 在不同 $\lambda$ 下的性能。
- en: To examine the impact of gap regularization strength, we vary $\lambda$ to $2$
    to balance between the performance-driven influential samples from the surrogate
    model and the difficult samples for the LLMs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查间隙正则化强度的影响，我们将 $\lambda$ 调整为 $2$，以平衡来自代理模型的性能驱动影响样本和 LLMs 的困难样本。
- en: 5\. Related Work
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 相关工作
- en: 5.1\. LLM-based Recommendation
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 基于 LLM 的推荐
- en: Leveraging LLMs for recommendation has gained remarkable attention recently (Li
    et al., [2023a](#bib.bib32); Wu et al., [2023c](#bib.bib55); Feng et al., [2023](#bib.bib14);
    Wang et al., [2023c](#bib.bib49)), showcasing their potential across various recommendation
    tasks, including CTR predicton (Bao et al., [2023b](#bib.bib5)), sequential recommendation (Lin
    et al., [2023](#bib.bib33)), and cross-domain recommendation (Gong et al., [2023](#bib.bib16)).
    Some early studies explore the recommendation ability of powerful LLMs, *e.g.,*
    ChatGPT, through the in-context-learning ability (Liu et al., [2023](#bib.bib35);
    Dai et al., [2023](#bib.bib11); Sun et al., [2023](#bib.bib46)). Nevertheless,
    the performance of LLMs is limited without extra fine-tuning over the domain-specific
    recommendation data (Liu et al., [2023](#bib.bib35); Bao et al., [2023b](#bib.bib5)).
    To fully leverage the potential of LLMs for recommendation, a series of work studies
    various fine-tuning strategies tailored for recommendation tasks (Li et al., [2023b](#bib.bib31);
    Zhang et al., [2023](#bib.bib58); Gong et al., [2023](#bib.bib16)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用 LLMs 进行推荐获得了显著关注（Li et al., [2023a](#bib.bib32); Wu et al., [2023c](#bib.bib55);
    Feng et al., [2023](#bib.bib14); Wang et al., [2023c](#bib.bib49)），展示了其在各种推荐任务中的潜力，包括
    CTR 预测（Bao et al., [2023b](#bib.bib5)）、序列推荐（Lin et al., [2023](#bib.bib33)）和跨领域推荐（Gong
    et al., [2023](#bib.bib16)）。一些早期研究探讨了强大 LLMs 的推荐能力，例如 ChatGPT，通过其上下文学习能力（Liu et
    al., [2023](#bib.bib35); Dai et al., [2023](#bib.bib11); Sun et al., [2023](#bib.bib46)）。然而，LLMs
    的性能在没有额外领域特定推荐数据的微调的情况下是有限的（Liu et al., [2023](#bib.bib35); Bao et al., [2023b](#bib.bib5)）。为了充分利用
    LLMs 在推荐中的潜力，一系列工作研究了针对推荐任务的各种微调策略（Li et al., [2023b](#bib.bib31); Zhang et al.,
    [2023](#bib.bib58); Gong et al., [2023](#bib.bib16)）。
- en: However, fine-tuning LLMs requires extensive computational resources and time
    costs, thus hindering the real-world applications of LLM-based recommender models.
    Therefore, it is crucial to enhance the fine-tuning efficiency of LLM-based recommender
    models. In this work, we propose the task of data pruning for efficient LLM-based
    recommendation, which aims to identify representative samples tailored for LLMs’
    few-shot fine-tuning.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微调大语言模型（LLMs）需要大量的计算资源和时间成本，从而阻碍了基于LLM的推荐模型在实际中的应用。因此，提高基于LLM的推荐模型的微调效率至关重要。在这项工作中，我们提出了针对高效LLM推荐的数据剪枝任务，旨在识别适合LLM少样本微调的代表性样本。
- en: 5.2\. Coreset Selection
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. Coreset 选择
- en: 'Coreset selection has been widely studied in both traditional machine learning (Wei
    et al., [2015](#bib.bib52); Chen et al., [2012](#bib.bib8); Feldman et al., [2011](#bib.bib12))
    and deep learning (Yang et al., [2023b](#bib.bib56)), benefiting many downstream
    tasks such as data-efficient learning (Toneva et al., [2018](#bib.bib48)), continual
    learning (Borsos et al., [2020](#bib.bib6)), neural architecture search (Shim
    et al., [2021](#bib.bib44)), and active learning (Sener and Savarese, [2018](#bib.bib43)).
    It aims to select a small but representative subset from the full data that can
    lead to comparable model performance. Previous work mainly falls into two groups:
    1) Heuristic methods (Coleman et al., [2020](#bib.bib9); Toneva et al., [2018](#bib.bib48);
    Feldman and Zhang, [2020](#bib.bib13)) typically assume difficult or diverse samples
    are informative for model training and use pre-defined metrics to compute a score
    for selection. 2) Optimization-based methods (Yang et al., [2023b](#bib.bib56);
    Mirzasoleiman et al., [2020](#bib.bib38); Killamsetty et al., [2021a](#bib.bib26);
    Kothawade et al., [2022](#bib.bib30)) leverages the bi-level or discrete optimization
    techniques to optimize the data subset that can minimize the empirical risk. However,
    heuristic methods do not estimate the impact of selected samples on empirical
    risk, thus might lead to suboptimal coreset selection. And optimization-based
    methods fail to be applied to LLM-based recommendation due to the cumbersome calculation
    for complex optimization. Furthermore, previous methods usually rely on the training
    of the model on full data for selection, which is infeasible for LLM-based recommendation
    (*cf.* Section [2](#S2 "2\. Task Formulation ‣ Data-efficient Fine-tuning for
    LLM-based Recommendation")).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Coreset 选择在传统机器学习（Wei 等人，[2015](#bib.bib52)；Chen 等人，[2012](#bib.bib8)；Feldman
    等人，[2011](#bib.bib12)）和深度学习（Yang 等人，[2023b](#bib.bib56)）中得到了广泛研究，对许多下游任务如数据高效学习（Toneva
    等人，[2018](#bib.bib48)）、持续学习（Borsos 等人，[2020](#bib.bib6)）、神经架构搜索（Shim 等人，[2021](#bib.bib44)）和主动学习（Sener
    和 Savarese，[2018](#bib.bib43)）都有益。它的目标是从完整数据中选择一个小而具有代表性的子集，以达到与原模型相当的性能。以前的工作主要分为两类：1）启发式方法（Coleman
    等人，[2020](#bib.bib9)；Toneva 等人，[2018](#bib.bib48)；Feldman 和 Zhang，[2020](#bib.bib13)）通常假设困难或多样的样本对模型训练有信息量，并使用预定义的指标来计算选择分数。2）基于优化的方法（Yang
    等人，[2023b](#bib.bib56)；Mirzasoleiman 等人，[2020](#bib.bib38)；Killamsetty 等人，[2021a](#bib.bib26)；Kothawade
    等人，[2022](#bib.bib30)）利用双层或离散优化技术来优化数据子集，以最小化经验风险。然而，启发式方法不能估计所选样本对经验风险的影响，因此可能导致次优的
    coresets 选择。而基于优化的方法由于复杂优化的繁琐计算无法应用于基于LLM的推荐。此外，以往的方法通常依赖于对完整数据的模型训练来进行选择，这在基于LLM的推荐中是不切实际的（*参见*
    第[2](#S2 "2\. 任务形式化 ‣ 基于LLM的推荐的数据高效微调")节）。
- en: $\bullet\quad$Data Condensation (Zhao et al., [2020](#bib.bib60)) is another
    potential solution to achieve data-efficient training. However, it is intrinsically
    different from our proposed task of data pruning. While it aims to synthesize
    a small but informative dataset (Zhao and Bilen, [2023](#bib.bib59)), our proposed
    task targets to identify representative samples from the existing samples for
    LLM’s few-shot fine-tuning. Besides, previous work mainly designed for continuous
    data, which is not applicable to LLM-based recommendation (Wu et al., [2023a](#bib.bib53)).
    TF-DCon (Wu et al., [2023b](#bib.bib54)) is recently proposed for content-based
    recommendation and we compare it in Section [4.2](#S4.SS2 "4.2\. Overall Performance
    (RQ1) ‣ 4\. Experiment ‣ Data-efficient Fine-tuning for LLM-based Recommendation").
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet\quad$数据压缩 （赵等，[2020](#bib.bib60)）是实现数据高效训练的另一种潜在解决方案。然而，它本质上与我们提出的数据剪枝任务不同。虽然它旨在合成一个小而信息丰富的数据集（赵和比伦，[2023](#bib.bib59)），我们提出的任务则旨在从现有样本中识别代表性样本，以用于LLM的少量样本微调。此外，之前的工作主要设计用于连续数据，这不适用于基于LLM的推荐（吴等，[2023a](#bib.bib53)）。TF-DCon（吴等，[2023b](#bib.bib54)）最近提出用于基于内容的推荐，我们在第[4.2](#S4.SS2
    "4.2\. Overall Performance (RQ1) ‣ 4\. Experiment ‣ Data-efficient Fine-tuning
    for LLM-based Recommendation")节中进行了比较。
- en: 6\. Conclusion
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: 'In this work, we proposed the task of data pruning for efficient LLM-based
    recommendation, which aims to identify representative samples tailored for LLMs’
    few-shot fine-tuning. Furthermore, we posited two objectives for this data pruning
    task: 1) high accuracy targets to select the samples that can lead to low empirical
    risk; and 2) high efficiency strives to consume low costs for the data pruning
    process. To this end, we proposed a novel data pruning method, namely DEALRec,
    to efficiently identify the influential samples with two scores. 1) The influence
    score is formulated to estimate the influence of sample removal on empirical risk,
    where the calculation is extended from the influence function and is accelerated
    through the symmetric property. 2) We introduced a small-sized surrogate model
    to calculate the influence score efficiently and proposed the effort score to
    bridge the gap between the surrogate model and LLMs. Empirical results validate
    the effectiveness of DEALRec in achieving both high efficiency and high accuracy.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们提出了一个用于高效LLM推荐的数据剪枝任务，旨在识别针对LLM少量样本微调的代表性样本。此外，我们为这个数据剪枝任务设定了两个目标：1)
    高准确度旨在选择能够导致低经验风险的样本；2) 高效率致力于减少数据剪枝过程的成本。为此，我们提出了一种新颖的数据剪枝方法，即DEALRec，利用两个评分高效识别有影响力的样本。1)
    影响评分被制定用来估计样本移除对经验风险的影响，其中计算从影响函数扩展，并通过对称性加速。2) 我们引入了一个小型的替代模型来高效计算影响评分，并提出了努力评分来弥合替代模型与LLMs之间的差距。实证结果验证了DEALRec在实现高效率和高准确度方面的有效性。
- en: This work proposes a data pruning task for LLM fine-tuning, opening up a new
    research direction for efficient LLM-based recommendation and leaving many promising
    directions for future work. 1) It is worthwhile to apply DEALRec to more LLM-based
    recommender models on more cross-domain datasets, improving fine-tuning performance
    with limited resources. 2) Due to the limited context window length of LLMs, it
    is promising to select the informative interacted items in users’ interaction
    sequences for LLMs’ fine-tuning. 3) Enhancing the inference efficiency of LLM-based
    recommender models is also a crucial problem for their real-world deployments.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作提出了一个用于LLM微调的数据剪枝任务，为高效的LLM推荐系统开辟了新的研究方向，并为未来的工作留下了许多有前景的方向。1) 将DEALRec应用于更多的LLM推荐模型和更多的跨域数据集，以在有限资源下提升微调性能是值得的。2)
    由于LLMs的上下文窗口长度有限，选择用户交互序列中的信息丰富的交互项目以进行LLMs微调是有前景的。3) 提升LLM推荐模型的推理效率也是其实际部署中的一个关键问题。
- en: References
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Agarwal et al. (2016) Naman Agarwal, Brian Bullins, and Elad Hazan. 2016. Second-order
    stochastic optimization in linear time. *stat* 1050 (2016), 15.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal et al. (2016) Naman Agarwal, Brian Bullins, 和 Elad Hazan. 2016. 线性时间的二阶随机优化.
    *stat* 1050 (2016), 15.
- en: Agarwal et al. (2020) Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan
    Arora. 2020. Contextual diversity for active learning. In *ECCV*. Springer, 137–153.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal et al. (2020) Sharat Agarwal, Himanshu Arora, Saket Anand, 和 Chetan
    Arora. 2020. 主动学习的上下文多样性. 发表在 *ECCV* 上. Springer, 137–153.
- en: Bao et al. (2023a) Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi
    Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023a. A bi-step grounding
    paradigm for large language models in recommendation systems. *arXiv:2308.08434*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等（2023a）柯钦·包、纪智张、文杰王、杨张、正义杨、彦成罗、傅立峰、向南何、齐田。2023a年。《大型语言模型在推荐系统中的双步落地范式》。*arXiv:2308.08434*。
- en: 'Bao et al. (2023b) Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng,
    and Xiangnan He. 2023b. Tallrec: An effective and efficient tuning framework to
    align large language model with recommendation. In *RecSys*. ACM.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等（2023b）柯钦·包、纪智张、杨张、文杰王、傅立峰、向南何。2023b年。《Tallrec：一个有效且高效的调整框架，用于使大型语言模型与推荐系统对齐》。在
    *RecSys*。ACM。
- en: Borsos et al. (2020) Zalán Borsos, Mojmir Mutny, and Andreas Krause. 2020. Coresets
    via bilevel optimization for continual learning and streaming. *NeurIPS* 33 (2020),
    14879–14890.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borsos 等（2020）扎兰·博尔索斯、莫伊米尔·穆特尼、安德烈亚斯·考劳斯。2020年。《通过双层优化进行核心集合的持续学习和流式处理》。*NeurIPS*
    33（2020），14879–14890。
- en: Chai et al. (2023) Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu,
    Yuhao Deng, and Guoren Wang. 2023. Efficient coreset selection with cluster-based
    methods. In *KDD*. ACM, 167–178.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai 等（2023）程亮柴、佳怡王、南唐、叶袁、佳宾刘、宇豪邓、国仁王。2023年。《基于聚类的方法进行高效的核心集合选择》。在 *KDD*。ACM，167–178。
- en: Chen et al. (2012) Yutian Chen, Max Welling, and Alex Smola. 2012. Super-samples
    from kernel herding. *arXiv:1203.3472* (2012).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2012）于天陈、马克斯·威林、亚历克斯·斯莫拉。2012年。《从核采样中提取超样本》。*arXiv:1203.3472*（2012）。
- en: 'Coleman et al. (2020) C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis,
    P Liang, J Leskovec, and M Zaharia. 2020. Selection via Proxy: Efficient Data
    Selection for Deep Learning. In *ICLR*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coleman 等（2020）C·科尔曼、C·叶、S·穆斯曼、B·米尔扎索雷曼、P·贝利斯、P·梁、J·列斯科维奇、M·扎哈里亚。2020年。《通过代理选择：深度学习的数据选择效率》。在
    *ICLR*。
- en: Cook (1977) R Dennis Cook. 1977. Detection of influential observation in linear
    regression. *Technometrics* 19, 1 (1977), 15–18.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cook（1977）R·丹尼斯·库克。1977年。《线性回归中的影响观察检测》。*Technometrics* 19，1（1977），15–18。
- en: Dai et al. (2023) Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si,
    Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT’s Capabilities
    in Recommender Systems. *arXiv:2305.02182*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2023）孙浩戴、宁陆邵、海源赵、伟杰余、自华四、陈旭、钟祥孙、小张、俊徐。2023年。《揭示 ChatGPT 在推荐系统中的能力》。*arXiv:2305.02182*。
- en: Feldman et al. (2011) Dan Feldman, Matthew Faulkner, and Andreas Krause. 2011.
    Scalable training of mixture models via coresets. *NeurIPS* 24 (2011).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feldman 等（2011）丹·费尔德曼、马修·福克纳、安德烈亚斯·考劳斯。2011年。《通过核心集合可扩展的混合模型训练》。*NeurIPS* 24（2011）。
- en: 'Feldman and Zhang (2020) Vitaly Feldman and Chiyuan Zhang. 2020. What neural
    networks memorize and why: Discovering the long tail via influence estimation.
    *NeurIPS* 33 (2020), 2881–2891.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feldman 和 Zhang（2020）维塔利·费尔德曼和池远·张。2020年。《神经网络记住了什么以及为什么：通过影响估计发现长尾》。*NeurIPS*
    33（2020），2881–2891。
- en: Feng et al. (2023) Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao
    Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational
    Recommender System. *arXiv:2308.06212* (2023).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2023）岳峰、舒畅刘、郑海薛、青鹏蔡、兰涛胡、彭江、坤盖、费孙。2023年。《一种增强对话推荐系统的大型语言模型》。*arXiv:2308.06212*（2023）。
- en: 'Gao et al. (2023) Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang,
    and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented
    recommender system. *arXiv:2303.14524*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2023）云帆高、陶盛、友林向、云雄、浩芬王、佳伟张。2023年。《Chat-rec：面向互动和可解释的 LLM 增强推荐系统》。*arXiv:2303.14524*。
- en: Gong et al. (2023) Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu,
    and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model
    for Cold-Start Scenario. In *CIKM*. 4595–4601.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等（2023）于奇·龚、希辰·丁、叶辉·苏、凯明·申、钟毅·刘、观南·张。2023年。《针对冷启动场景的统一搜索和推荐基础模型》。在 *CIKM*。4595–4601。
- en: 'Guo et al. (2022) Chengcheng Guo, Bo Zhao, and Yanbing Bai. 2022. Deepcore:
    A comprehensive library for coreset selection in deep learning. In *DEXA*. Springer,
    181–195.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2022）程程郭、博赵、彦冰白。2022年。《Deepcore：深度学习中核心集合选择的综合库》。在 *DEXA*。施普林格，181–195。
- en: 'Guo et al. (2017) Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang
    He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction.
    In *IJCAI*. 1725–1731.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2017）惠丰郭、瑞明唐、云铭叶、郑国李、秀强何。2017年。《DeepFM：基于因子分解机的神经网络用于CTR预测》。在 *IJCAI*。1725–1731。
- en: Hampel (1974) Frank R Hampel. 1974. The influence curve and its role in robust
    estimation. *Journal of the american statistical association* 69, 346 (1974),
    383–393.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hampel（1974）弗兰克·R·汉佩尔。1974年。《影响曲线及其在稳健估计中的作用》。*美国统计学会期刊* 69，346（1974），383–393。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *CVPR*. IEEE, 770–778.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) 何恺明、张翔宇、任少卿和孙剑。2016。深度残差学习用于图像识别。在 *CVPR*。IEEE，770–778。
- en: He et al. (2023) Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. 2023. Large-scale
    Dataset Pruning with Dynamic Uncertainty. *arXiv:2306.05175*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023) 何睦扬、杨硕、黄铁军和赵博。2023。大规模数据集修剪与动态不确定性。*arXiv:2306.05175*。
- en: He and Chua (2017) Xiangnan He and Tat-Seng Chua. 2017. Neural Factorization
    Machines for Sparse Predictive Analytics. In *SIGIR*. ACM, 355–364.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He and Chua (2017) 何香南和蔡达生。2017。稀疏预测分析的神经因子分解机。在 *SIGIR*。ACM，355–364。
- en: 'He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang,
    and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network
    for recommendation. In *SIGIR*. 639–648.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2020) 何香南、邓宽、王翔、李艳、张永东和王萌。2020。Lightgcn：简化和增强图卷积网络用于推荐。在 *SIGIR*。639–648。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv:2106.09685*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) 爱德华·J·胡、沈业龙、菲利普·沃利斯、泽远·艾伦-朱、刘元志、王先和卢旺和陈伟柱。2021。Lora：大型语言模型的低秩适配。*arXiv:2106.09685*。
- en: Kang and McAuley (2018) Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive
    sequential recommendation. In *ICDM*. IEEE, 197–206.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang and McAuley (2018) 王成康和朱利安·麦考利。2018。自注意力序列推荐。在 *ICDM*。IEEE，197–206。
- en: 'Killamsetty et al. (2021a) Krishnateja Killamsetty, Sivasubramanian Durga,
    Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021a. Grad-match: Gradient matching
    based data subset selection for efficient deep model training. In *ICML*. PMLR,
    5464–5474.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Killamsetty et al. (2021a) 克里什纳特贾·基拉姆塞蒂、西瓦斯布拉曼·杜尔加、加内什·拉马克里希南、阿比尔·德和里沙布·艾耶尔。2021a。Grad-match：基于梯度匹配的数据子集选择以实现高效的深度模型训练。在
    *ICML*。PMLR，5464–5474。
- en: 'Killamsetty et al. (2021b) Krishnateja Killamsetty, Durga Sivasubramanian,
    Ganesh Ramakrishnan, and Rishabh Iyer. 2021b. Glister: Generalization based data
    subset selection for efficient and robust learning. In *AAAI*, Vol. 35\. 8110–8118.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Killamsetty et al. (2021b) 克里什纳特贾·基拉姆塞蒂、杜尔加·西瓦斯布拉曼、加内什·拉马克里希南和里沙布·艾耶尔。2021b。Glister：基于泛化的数据子集选择以实现高效且稳健的学习。在
    *AAAI*，第 35 卷。8110–8118。
- en: 'Killamsetty et al. (2021c) Krishnateja Killamsetty, Xujiang Zhao, Feng Chen,
    and Rishabh Iyer. 2021c. Retrieve: Coreset selection for efficient and robust
    semi-supervised learning. *NeurIPS* 34 (2021), 14488–14501.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Killamsetty et al. (2021c) 克里什纳特贾·基拉姆塞蒂、赵绪江、陈峰和里沙布·艾耶尔。2021c。Retrieve：用于高效且稳健的半监督学习的核心集选择。*NeurIPS*
    34 (2021)，14488–14501。
- en: Koh and Liang (2017) Pang Wei Koh and Percy Liang. 2017. Understanding black-box
    predictions via influence functions. In *ICML*. PMLR, 1885–1894.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh and Liang (2017) 庞伟·科和帕西·梁。2017。通过影响函数理解黑箱预测。在 *ICML*。PMLR，1885–1894。
- en: 'Kothawade et al. (2022) Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan,
    Jeff Bilmes, and Rishabh Iyer. 2022. PRISM: A Unified Framework of Parameterized
    Submodular Information Measures for Targeted Data Subset Selection and Summarization.
    In *AAAI*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kothawade et al. (2022) 苏拉吉·科塔瓦德、维沙尔·考沙尔、加内什·拉马克里希南、杰夫·比尔梅斯和里沙布·艾耶尔。2022。PRISM：用于目标数据子集选择和摘要的参数化子模量信息度量的统一框架。在
    *AAAI*。
- en: Li et al. (2023b) Lei Li, Yongfeng Zhang, and Li Chen. 2023b. Prompt distillation
    for efficient llm-based recommendation. In *CIKM*. 1348–1357.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) 李磊、张永峰和陈力。2023b。用于高效 LLM 基于推荐的提示蒸馏。在 *CIKM*。1348–1357。
- en: 'Li et al. (2023a) Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao
    Xing. 2023a. E4SRec: An Elegant Effective Efficient Extensible Solution of Large
    Language Models for Sequential Recommendation. *arXiv:2312.02443*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) 李心航、陈冲、赵向宇、张永和邢春晓。2023a。E4SRec：大型语言模型用于序列推荐的优雅高效扩展解决方案。*arXiv:2312.02443*。
- en: Lin et al. (2023) Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng,
    and Tat-Seng Chua. 2023. A multi-facet paradigm to bridge large language model
    and recommendation. *arXiv:2310.06491*.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) 林欣瑜、王文杰、李永琪、冯富立、黄锡琼和蔡达生。2023。一个多面向范式以桥接大型语言模型和推荐系统。*arXiv:2310.06491*。
- en: Ling (1984) Robert F Ling. 1984. Residuals and influence in regression.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling (1984) 罗伯特·F·林。1984。回归中的残差与影响。
- en: Liu et al. (2023) Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang.
    2023. Is chatgpt a good recommender? a preliminary study. *arXiv:2304.10149*.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) 刘军凌、刘超、吕仁杰、周康和张岩。2023。ChatGPT 是否是一个优秀的推荐系统？初步研究。*arXiv:2304.10149*。
- en: 'Liu et al. (2024) Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024.
    ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source
    Large Language Models. In *WSDM*. ACM.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人（2024）Qijiong Liu, Nuo Chen, Tetsuya Sakai, 和 Xiao-Ming Wu. 2024. ONCE:
    利用开放源和闭源大语言模型提升基于内容的推荐。在*WSDM*中。ACM。'
- en: 'Luo et al. (2023) Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou,
    Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker: Instruction
    Tuning Large Language Model as Ranker for Top-k Recommendation. *arXiv:2312.16018*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo等人（2023）Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng
    Li, Yuanzhang Xiao, Mingjie Zhan, 和 Linqi Song. 2023. RecRanker: 将大型语言模型作为排名器进行Top-k推荐的指令微调。*arXiv:2312.16018*。'
- en: Mirzasoleiman et al. (2020) Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
    2020. Coresets for data-efficient training of machine learning models. In *ICML*.
    PMLR, 6950–6960.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirzasoleiman等人（2020）Baharan Mirzasoleiman, Jeff Bilmes, 和 Jure Leskovec. 2020.
    用于数据高效训练机器学习模型的核心集。在*ICML*中。PMLR, 6950–6960。
- en: Ni et al. (2023) Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li,
    Xiangnan He, Yongfeng Zhang, and Fajie Yuan. 2023. A Content-Driven Micro-Video
    Recommendation Dataset at Scale. *arXiv:2309.15379* (2023).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni等人（2023）Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan
    He, Yongfeng Zhang, 和 Fajie Yuan. 2023. 大规模内容驱动的微视频推荐数据集。*arXiv:2309.15379* (2023)。
- en: 'Paul et al. (2021) Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite.
    2021. Deep learning on a data diet: Finding important examples early in training.
    *NeurIPS* 34, 20596–20607.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paul等人（2021）Mansheej Paul, Surya Ganguli, 和 Gintare Karolina Dziugaite. 2021.
    数据节食中的深度学习：在训练早期发现重要示例。*NeurIPS* 34, 20596–20607。
- en: Rajput et al. (2023) Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H
    Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost,
    et al. 2023. Recommender Systems with Generative Retrieval. In *NeurIPS*. Curran
    Associates, Inc.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajput等人（2023）Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan,
    Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, 等人。2023.
    使用生成检索的推荐系统。在*NeurIPS*中。Curran Associates, Inc.
- en: 'Sachdeva et al. (2022) Noveen Sachdeva, Mehak Dhaliwal, Carole-Jean Wu, and
    Julian McAuley. 2022. Infinite recommendation networks: a data-centric approach.
    *NeurIPS* 35, 31292–31305.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sachdeva等人（2022）Noveen Sachdeva, Mehak Dhaliwal, Carole-Jean Wu, 和 Julian McAuley.
    2022. 无限推荐网络：以数据为中心的方法。*NeurIPS* 35, 31292–31305。
- en: 'Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. Active learning
    for convolutional neural networks: A core-set approach. (2018).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sener和Savarese（2018）Ozan Sener 和 Silvio Savarese. 2018. 卷积神经网络的主动学习：核心集方法。（2018）。
- en: Shim et al. (2021) Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. 2021. Core-set
    sampling for efficient neural architecture search. *arXiv:2107.06869*.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shim等人（2021）Jae-hun Shim, Kyeongbo Kong, 和 Suk-Ju Kang. 2021. 高效神经架构搜索的核心集采样。*arXiv:2107.06869*。
- en: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional
    encoder representations from transformer. In *CIKM*. 1441–1450.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人（2019）Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, 和 Peng
    Jiang. 2019. BERT4Rec：基于双向编码器表示的序列推荐。在*CIKM*中。1441–1450。
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language
    Models as Re-Ranking Agent. In *EMNLP*. ACL, 14918–14937.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人（2023）Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, 和 Zhaochun
    Ren. 2023. ChatGPT在搜索中的表现如何？调查大型语言模型作为重新排序代理。在*EMNLP*中。ACL, 14918–14937。
- en: Tan et al. (2023) Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan
    Wang, and Xiaojuan Qi. 2023. Data Pruning via Moving-one-Sample-out. *arXiv:2310.14664*.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等人（2023）Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang,
    和 Xiaojuan Qi. 2023. 通过移除一个样本进行数据修剪。*arXiv:2310.14664*。
- en: Toneva et al. (2018) Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes,
    Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. 2018. An empirical study
    of example forgetting during deep neural network learning. *arXiv:1812.05159*.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toneva等人（2018）Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam
    Trischler, Yoshua Bengio, 和 Geoffrey J Gordon. 2018. 深度神经网络学习过程中示例遗忘的实证研究。*arXiv:1812.05159*。
- en: 'Wang et al. (2023c) Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, and Yong
    Wang. 2023c. LLM4Vis: Explainable visualization recommendation using ChatGPT.
    *arXiv:2310.07652* (2023).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2023c）Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, 和 Yong Wang.
    2023c. LLM4Vis: 使用ChatGPT的可解释可视化推荐。*arXiv:2310.07652* (2023)。'
- en: Wang et al. (2023a) Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yunshan
    Ma, and Tat-Seng Chua. 2023a. Causal Disentangled Recommendation Against User
    Preference Shifts. *TOIS* (2023).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023a）Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yunshan Ma, 和 Tat-Seng
    Chua. 2023a. 对抗用户偏好变化的因果解耦推荐。*TOIS* (2023)。
- en: Wang et al. (2023b) Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yinwei Wei,
    and Tat-Seng Chua. 2023b. Equivariant Learning for Out-of-Distribution Cold-start
    Recommendation. In *MM*. 903–914.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023b）王文杰、林心宇、王刘辉、冯富力、魏银伟和蔡达生。2023b。用于分布外冷启动推荐的等变学习。发表于*MM*。903–914。
- en: Wei et al. (2015) Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submodularity
    in data subset selection and active learning. In *ICML*. PMLR, 1954–1963.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韦等（2015）韦凯、艾瑞什·伊耶尔和杰夫·比尔梅斯。2015。数据子集选择和主动学习中的子模性。发表于*ICML*。PMLR，1954–1963。
- en: Wu et al. (2023a) Jiahao Wu, Wenqi Fan, Shengcai Liu, Qijiong Liu, Rui He, Qing
    Li, and Ke Tang. 2023a. Dataset condensation for recommendation. *arXiv:2310.01038*.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023a）吴佳浩、范文琪、刘胜彩、刘启炯、何睿、李青和唐可。2023a。用于推荐的数据集压缩。*arXiv:2310.01038*。
- en: Wu et al. (2023b) Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai
    Liu, Qing Li, Xiao-Ming Wu, and Ke Tang. 2023b. Leveraging Large Language Models
    (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation.
    *arXiv:2310.09874*.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023b）吴佳浩、刘启炯、胡恒昌、范文琪、刘胜彩、李青和吴晓明、唐可。2023b。利用大型语言模型（LLMs）来增强无训练数据集压缩的内容推荐。*arXiv:2310.09874*。
- en: Wu et al. (2023c) Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong
    Chen. 2023c. Exploring large language model for graph data understanding in online
    job recommendations. *arXiv:2307.05722*.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2023c）吴立康、邱兆鹏、郑智、朱恒书和陈恩洪。2023c。探索用于在线工作推荐的图数据理解的大型语言模型。*arXiv:2307.05722*。
- en: 'Yang et al. (2023b) Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun,
    and Ping Li. 2023b. Dataset pruning: reducing training data by examining generalization
    influence. (2023).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2023b）杨硕、谢泽克、彭汉宇、徐敏、孙明明和李平。2023b。数据集修剪：通过检查泛化影响来减少训练数据。（2023）。
- en: Yang et al. (2023a) Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da
    Luo, and Kangyi Lin. 2023a. Debiased Contrastive Learning for Sequential Recommendation.
    In *WWW*. 1063–1073.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2023a）杨宇昊、黄超、夏梁浩、黄春真、罗达和林康益。2023a。用于序列推荐的去偏对比学习。发表于*WWW*。1063–1073。
- en: 'Zhang et al. (2023) Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao,
    Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large
    language model empowered recommendation approach. *arXiv:2305.07001*.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2023）张俊杰、谢若冰、侯宇鹏、赵馨威、林乐瑜和文纪荣。2023。作为指令跟随的推荐：一种大型语言模型赋能的推荐方法。*arXiv:2305.07001*。
- en: Zhao and Bilen (2023) Bo Zhao and Hakan Bilen. 2023. Dataset condensation with
    distribution matching. In *WACV*. IEEE, 6514–6523.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵和比伦（2023）赵博和哈坎·比伦。2023。通过分布匹配进行数据集压缩。发表于*WACV*。IEEE，6514–6523。
- en: Zhao et al. (2020) Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset
    Condensation with Gradient Matching. In *ICLR*.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2020）赵博、孔达·雷迪·莫普里和哈坎·比伦。2020。通过梯度匹配进行数据集压缩。发表于*ICLR*。
- en: Zheng et al. (2022) Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. 2022.
    Coverage-centric Coreset Selection for High Pruning Rates. In *ICLR*.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2022）郑海中、刘睿、赖凡和阿图尔·普拉卡什。2022。高修剪率的覆盖中心Coreset选择。发表于*ICLR*。
