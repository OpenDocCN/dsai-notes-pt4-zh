- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:37:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:37:39'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的偏好微调应利用次优的、在线策略数据
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14367](https://ar5iv.labs.arxiv.org/html/2404.14367)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14367](https://ar5iv.labs.arxiv.org/html/2404.14367)
- en: \reportnumber\correspondingauthor
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \reportnumber\correspondingauthor
- en: 'anikait@stanford.edu, ftajwar@cs.cmu.edu. Project Website: [https://understanding-rlhf.github.io/](https://understanding-rlhf.github.io/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'anikait@stanford.edu, ftajwar@cs.cmu.edu。项目网站: [https://understanding-rlhf.github.io/](https://understanding-rlhf.github.io/)'
- en: Fahim Tajwar Anikait Singh Archit Sharma Stanford University Rafael Rafailov
    Stanford University Jeff Schneider Carnegie Mellon University Tengyang Xie UW-Madison
    Stefano Ermon Stanford University Chelsea Finn Stanford University Aviral Kumar
    Google DeepMind
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Fahim Tajwar Anikait Singh Archit Sharma 斯坦福大学 Rafael Rafailov 斯坦福大学 Jeff Schneider
    卡内基梅隆大学 Tengyang Xie 威斯康星大学麦迪逊分校 Stefano Ermon 斯坦福大学 Chelsea Finn 斯坦福大学 Aviral
    Kumar 谷歌 DeepMind
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Learning from preference labels plays a crucial role in fine-tuning large language
    models. There are several distinct approaches for preference fine-tuning, including
    supervised learning, on-policy reinforcement learning (RL), and contrastive learning.
    Different methods come with different implementation tradeoffs and performance
    differences, and existing empirical findings present different conclusions, for
    instance, some results show that online RL is quite important to attain good fine-tuning
    results, while others find (offline) contrastive or even purely supervised methods
    sufficient. This raises a natural question: *what kind of approaches are important
    for fine-tuning with preference data and why?* In this paper, we answer this question
    by performing a rigorous analysis of a number of fine-tuning techniques on didactic
    and full-scale LLM problems. Our main finding is that, in general, approaches
    that use on-policy sampling or attempt to push down the likelihood on certain
    responses (i.e., employ a “negative gradient”) outperform offline and maximum
    likelihood objectives. We conceptualize our insights and unify methods that use
    on-policy sampling or negative gradient under a notion of mode-seeking objectives
    for categorical distributions. Mode-seeking objectives are able to alter probability
    mass on specific bins of a categorical distribution at a fast rate compared to
    maximum likelihood, allowing them to relocate masses across bins more effectively.
    Our analysis prescribes actionable insights for preference fine-tuning of LLMs
    and informs how data should be collected for maximal improvement.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从偏好标签中学习在微调大型语言模型中起着至关重要的作用。偏好微调有几种不同的方法，包括监督学习、在线策略强化学习（RL）和对比学习。不同的方法具有不同的实现权衡和性能差异，现有的实证研究也得出了不同的结论，例如，一些结果表明在线强化学习对于获得良好的微调效果非常重要，而另一些结果发现（离线）对比学习甚至纯监督方法就足够了。这引出了一个自然的问题：*哪些方法对于利用偏好数据进行微调是重要的，为什么？*
    在本文中，我们通过对多种微调技术在教学和全尺度 LLM 问题上的严格分析来回答这个问题。我们的主要发现是，一般来说，使用在线策略采样或尝试降低某些响应的可能性（即，使用“负梯度”）的方法优于离线和最大似然目标。我们将我们的见解进行概念化，并将使用在线策略采样或负梯度的方法统一在分类分布的模式寻求目标的概念下。模式寻求目标能够以比最大似然更快的速度改变分类分布中特定区间的概率质量，使其能够更有效地重新分配区间中的质量。我们的分析为
    LLM 的偏好微调提供了可操作的见解，并告知如何收集数据以实现最大化改进。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Pre-training endows a large language model (LLM) with knowledge about the world.
    Yet, it does not provide a lever to control responses from these models, especially
    when we want these solutions to optimize some task-dependent success criteria
    (e.g., align with human preferences, optimize correctness or compactness). To
    align LLMs with downstream success criteria, they are then fine-tuned with downstream
    objectives after pre-training. In this paper, we focus on fine-tuning problems
    that aim to optimize for binary preferences (from humans or other AI models).
    A plethora of methods have been proposed for this sort of fine-tuning, including
    supervised learning on filtered responses (Gulcehre et al., [2023](#bib.bib19)),
    contrastive training (Rafailov et al., [2023](#bib.bib38)), and on-policy reinforcement
    learning (RL) (Ouyang et al., [2022](#bib.bib35)) on a reward function extracted
    from human preferences.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练赋予了大型语言模型（LLM）有关世界的知识。然而，它并未提供控制这些模型响应的杠杆，尤其是当我们希望这些解决方案优化某些任务依赖的成功标准时（例如，与人类偏好对齐、优化正确性或紧凑性）。为了使LLM与下游成功标准对齐，它们在预训练后会进行下游目标的微调。在本文中，我们关注于旨在优化二元偏好的微调问题（来自人类或其他AI模型）。针对这种微调，提出了大量的方法，包括对过滤响应进行监督学习（Gulcehre
    et al., [2023](#bib.bib19)）、对比训练（Rafailov et al., [2023](#bib.bib38)），以及在从人类偏好中提取的奖励函数上的策略强化学习（RL）（Ouyang
    et al., [2022](#bib.bib35)）。
- en: 'In theory, while all of these methods aim to discover identical optimal policies,
    achieving this in practice would require full data coverage and infinite computation.
    These requirements are not met in practice, and hence, the choice of the loss
    function and the optimization procedure affects performance. However, a lack of
    a clear understanding of different approaches, coupled with different tradeoffs
    in implementation, has resulted in substantial confusion: practitioners are unsure
    as to: (1) whether RL (Ouyang et al., [2022](#bib.bib35)) is required at all,
    or contrastive approaches (Rafailov et al., [2023](#bib.bib38); Gheshlaghi Azar
    et al., [2023](#bib.bib18)), supervised fine-tuning are good enough; and (2) whether
    preference data should be collected with models in the loop (i.e., in an “on-policy”
    fashion) or not.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，虽然所有这些方法旨在发现相同的最优策略，但实际中实现这一目标需要完整的数据覆盖和无限的计算。这些要求在实际中并不满足，因此，损失函数和优化程序的选择会影响性能。然而，对不同方法缺乏清晰的理解，加上实施中的不同权衡，导致了相当大的混淆：实践者不确定：（1）是否完全需要RL（Ouyang
    et al., [2022](#bib.bib35)），还是对比方法（Rafailov et al., [2023](#bib.bib38)；Gheshlaghi
    Azar et al., [2023](#bib.bib18)）、监督微调足够好；以及（2）是否应在模型循环中收集偏好数据（即“策略内”方式）。
- en: Our goal is to provide clarity on these questions by performing a rigorous study
    to understand the behavior of existing methods when optimizing for preferences.
    Our study operates under assumptions typical in preference fine-tuning, including
    the existence of an underlying ground truth reward function that explains the
    preference data. We study methods that train an LLM policy to optimize a surrogate
    loss given by the expected reward under a model of the reward function (learned
    from preference data) penalized by the KL-divergence between the policy and a
    reference policy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是通过进行严格的研究来澄清这些问题，以理解现有方法在优化偏好时的行为。我们的研究在偏好微调中典型的假设下进行，包括存在一个解释偏好数据的基础真实奖励函数。我们研究的方法训练LLM策略以优化由奖励函数模型（从偏好数据中学习）下的期望奖励给出的替代损失，这个损失通过策略与参考策略之间的KL散度来惩罚。
- en: '![Refer to caption](img/77d72652c94fa18502fb8c08ed35c861.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/77d72652c94fa18502fb8c08ed35c861.png)'
- en: 'Figure 1: Left: an illustration of various fine-tuning techniques. On-policy
    sampling gradually shifts policy mass from $\pi_{\mathrm{ref}}$ compared to policies
    that only maximize some sort of likelihood, $\pi_{\text{sup}}$ already lies in
    the high likelihood regions of $\pi_{\mathrm{ref}}$, offline supervised methods
    can work well. No on-policy sampling or negative gradients may be needed.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：左侧：各种微调技术的示意图。与仅最大化某种可能性的策略相比，策略的实际采样逐渐将策略质量从$\pi_{\mathrm{ref}}$转移，而$\pi_{\text{sup}}$已经位于$\pi_{\mathrm{ref}}$的高可能性区域，离线监督方法可能效果很好。可能不需要任何策略采样或负梯度。
- en: To answer the above questions, we develop an analysis framework consisting of
    didactic bandit problems, synthetic LLM problems, and full-scale LLM problems,
    constructed out of AlpacaFarm (Dubois et al., [2024](#bib.bib14)) and UltraFeedback (Cui
    et al., [2023](#bib.bib11)). We then study behaviors of different methods given
    coverage conditions and geometric relationships in the problem. *Our main observation*
    is that algorithms that use on-policy RL in a reward model or attempt to push-down
    likelihood on certain responses, i.e., utilize a negative gradient term as in
    contrastive objectives tend to outperform other offline supervised objectives
    with no on-policy sampling or negative gradient. This is surprising because both
    on-policy and offline methods still utilize the same data for learning. We also
    find that using on-policy sampling and negative gradients are especially important
    when high-reward responses appear in less-likely regions of the reference policy
    distribution, and provide benefits complementary to each other. In particular,
    we find that supervised objectives such as Pref-FT and Binary Feed-ME (Dubois
    et al., [2024](#bib.bib14)) are not able to effectively move probability mass
    from low reward responses to high-reward responses. Sampling on-policy responses
    for training, contrastive learning, or employing both on-policy sampling and contrastive
    training can accomplish this.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上述问题，我们开发了一个分析框架，包括教学性赌博问题、合成LLM问题和全规模LLM问题，这些问题由AlpacaFarm（Dubois et al.,
    [2024](#bib.bib14)）和UltraFeedback（Cui et al., [2023](#bib.bib11)）构建。然后，我们研究了在覆盖条件和问题的几何关系下，不同方法的行为。*我们的主要观察*
    是，使用在政策RL的奖励模型或试图降低某些响应的可能性的算法，即在对比目标中利用负梯度项，往往比不使用在政策采样或负梯度的其他离线监督目标表现更好。这一点令人惊讶，因为在政策方法和离线方法仍然使用相同的数据进行学习。我们还发现，当高奖励响应出现在参考策略分布的低可能性区域时，使用在政策采样和负梯度尤其重要，并且它们的互补效益尤为明显。特别地，我们发现像Pref-FT和Binary
    Feed-ME（Dubois et al., [2024](#bib.bib14)）这样的监督目标无法有效地将概率质量从低奖励响应转移到高奖励响应。通过在政策响应采样进行训练、对比学习或同时使用在政策采样和对比训练可以实现这一点。
- en: We theoretically show that approaches that use on-policy RL or certain variants
    of contrastive training exhibit “mode-seeking” behavior, resulting in faster accumulation
    of probability mass on a subset of high-reward responses during learning. This
    behavior is in contrast to “mode-covering” supervised objectives that attempt
    to increase likelihood on all high-reward responses, and as a result, are unable
    to efficiently increase probability mass enough on one subset of high-reward responses.
    We then compare the behavior of a representative mode-seeking objective, the reverse
    KL-divergence, with the mode-covering forward KL-divergence to formalize this
    behavior for categorical distributions. Conceptually, this ability to commit to
    a certain subset of high-reward responses enables algorithms with on-policy sampling
    (and optionally, a negative gradient) to perform better than likelihood.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理论上展示了，使用在政策强化学习（on-policy RL）或某些变体的对比训练方法表现出“模式寻求”行为，这导致在学习过程中在高奖励响应的一个子集上更快地积累概率质量。这种行为与“模式覆盖”监督目标形成对比，后者试图增加所有高奖励响应的可能性，因此无法有效地在一个高奖励响应的子集上增加足够的概率质量。随后，我们将代表性的模式寻求目标——逆KL散度（reverse
    KL-divergence）与模式覆盖的前向KL散度（forward KL-divergence）进行比较，以形式化这种行为在分类分布中的表现。从概念上讲，这种对特定高奖励响应子集的承诺能力使得使用在政策采样（并可选择地使用负梯度）的算法能够比最大似然方法表现得更好。
- en: Our work presents several actionable takeaways for practitioners. First, we
    tie the performance of various methods to geometric conditions on the problem,
    which can inform practitioners which approach to use. Second, we observe a tradeoff
    between drawing more on-policy samples and performing more gradient steps with
    a different policy training objective. Understanding this tradeoff is useful for
    practitioners since on-policy sampling and training present different computational
    tradeoffs. Finally, since the performance of fine-tuning is tied to the data composition,
    we study the effect of conditions on the coverage of the preference data, which
    could inform data collection.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作为从业者提供了几个可操作的见解。首先，我们将各种方法的性能与问题的几何条件联系起来，这可以为从业者提供使用哪种方法的指导。其次，我们观察到在使用不同的政策训练目标时，获取更多的在政策样本和执行更多梯度步骤之间存在权衡。理解这种权衡对从业者很有用，因为在政策采样和训练存在不同的计算权衡。最后，由于微调的性能与数据组成相关，我们研究了条件对偏好数据覆盖的影响，这可能为数据收集提供参考。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: A dominant recipe for fine-tuning LLMs is to run supervised next token prediction
    (“supervised fine-tuning”) on a dataset of high-quality responses to obtain a
    good policy initialization. This is followed by fine-tuning on a dataset of human
    preferences (Casper et al., [2023](#bib.bib6); Ouyang et al., [2022](#bib.bib35)).
    This fine-tuning can use on-policy RL methods such as REINFORCE (Sutton et al.,
    [1999](#bib.bib45)) or PPO (Schulman et al., [2017](#bib.bib41)) to maximize the
    predictions of a reward model obtained from the preference data, regularized with
    a KL constraint. Another approach (Dubois et al., [2024](#bib.bib14)) performs
    supervised fine-tuning on the filtered set of preferred completions in the preference
    dataset. A different family of methods runs supervised learning on preferred responses
    iteratively such as ReST (Gulcehre et al., [2023](#bib.bib19)), RWR (Hu et al.,
    [2023](#bib.bib21)), and SuperHF (Mukobi et al., [2023](#bib.bib32)). Alternatively,
    methods such as DPO (Rafailov et al., [2023](#bib.bib38)), IPO (Gheshlaghi Azar
    et al., [2023](#bib.bib18)), SLiC-HF (Zhao et al., [2023](#bib.bib61)), and KTO (ContextualAI,
    [2024](#bib.bib9)) learn directly from human preferences, with no explicit reward
    model. Concurrent work also runs DPO iteratively (Yuan et al., [2024](#bib.bib60);
    Chen et al., [2024](#bib.bib8)). These methods come with different tradeoffs necessitating
    a study to understand their behaviors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对大规模语言模型（LLMs）进行微调的一个主要方法是对高质量响应数据集进行监督式下一个词预测（“监督式微调”），以获得良好的策略初始化。接着在包含人类偏好的数据集上进行微调（Casper
    等，[2023](#bib.bib6); Ouyang 等，[2022](#bib.bib35)）。这种微调可以使用诸如 REINFORCE（Sutton
    等，[1999](#bib.bib45)）或 PPO（Schulman 等，[2017](#bib.bib41)）的在政策强化学习方法，以最大化从偏好数据中获得的奖励模型的预测，并用
    KL 约束进行正则化。另一种方法（Dubois 等，[2024](#bib.bib14)）对偏好数据集中筛选出的优选完成集进行监督式微调。另一类方法迭代地在优选响应上进行监督学习，例如
    ReST（Gulcehre 等，[2023](#bib.bib19)），RWR（Hu 等，[2023](#bib.bib21)），和 SuperHF（Mukobi
    等，[2023](#bib.bib32)）。另外，像 DPO（Rafailov 等，[2023](#bib.bib38)），IPO（Gheshlaghi Azar
    等，[2023](#bib.bib18)），SLiC-HF（Zhao 等，[2023](#bib.bib61)），和 KTO（ContextualAI，[2024](#bib.bib9)）的方法直接从人类偏好中学习，没有显式的奖励模型。相关的工作还迭代地运行
    DPO（Yuan 等，[2024](#bib.bib60); Chen 等，[2024](#bib.bib8)）。这些方法具有不同的权衡，需要研究以理解其行为。
- en: 'Prior analysis work. To understand the effect of preference fine-tuning, prior
    work attempts to uncover its effect on network parameters for a certain set of
    tasks (Jain et al., [2023](#bib.bib22); Lee et al., [2024](#bib.bib29)). Our analysis
    is complementary in that it studies conditions when different algorithms perform
    well, and is applicable to any downstream task. Kirk et al. ([2023](#bib.bib27))
    study the contribution of RL fine-tuning on generalization to out-of-distribution
    prompts but this is complementary to our approach. Gao et al. ([2022](#bib.bib17));
    Coste et al. ([2023](#bib.bib10)); Eisenstein et al. ([2023](#bib.bib15)) study
    reward over-optimization to better build reward models, which is complementary
    to the behavior of the policy optimization approach. Agarwal et al. ([2023](#bib.bib2))
    develop a recipe that uses the mode-seeking KL divergence for knowledge distillation:
    this prior work is largely centered in the problem setting of distillation and
    does not study the optimization behavior of RL, contrastive, or supervised objectives.
    Perhaps closely related to our work is Singhal et al. ([2023](#bib.bib44)), which
    investigates the interplay between PPO and the composition of preference data,
    but this analysis is largely concentrated on studying the length bias of RL fine-tuning
    rather than developing insights into the behavior of fine-tuning algorithms. We
    do design didactic examples that use rewards dependent on length, but this is
    solely for analysis.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的分析工作。为了理解偏好微调的效果，先前的工作尝试揭示其对特定任务的网络参数的影响（Jain 等，[2023](#bib.bib22); Lee 等，[2024](#bib.bib29)）。我们的分析是补充性的，研究了不同算法表现良好的条件，并适用于任何下游任务。Kirk
    等人（[2023](#bib.bib27)）研究了 RL 微调对分布外提示的泛化贡献，但这与我们的方法是互补的。Gao 等人（[2022](#bib.bib17)）；Coste
    等人（[2023](#bib.bib10)）；Eisenstein 等人（[2023](#bib.bib15)）研究了奖励过度优化以更好地构建奖励模型，这与政策优化方法的行为是互补的。Agarwal
    等人（[2023](#bib.bib2)）开发了一种使用模式寻求 KL 散度的知识蒸馏方法：这项先前的工作主要集中在蒸馏问题的设置上，并未研究 RL、对比或监督目标的优化行为。与我们的工作密切相关的是
    Singhal 等人（[2023](#bib.bib44)），其研究了 PPO 和偏好数据组合之间的相互作用，但这一分析主要集中在研究 RL 微调的长度偏差，而非对微调算法行为的洞察。我们确实设计了依赖于长度的奖励的示范性例子，但这仅用于分析。
- en: Concurrently, Ahmadian et al. ([2024](#bib.bib3)) show that REINFORCE may simply
    be enough for preference fine-tuning of LLMs and complex policy optimization methods
    such as PPO may not be needed. Our conclusions are mostly complementary, though
    we do observe that PPO is more robust to sample reuse than REINFORCE. Concurrently,
    Sharma et al. ([2024](#bib.bib43)) compares contrastive and supervised fine-tuning
    on LLM-generated data, but this work does not study the role of coverage or geometric
    conditions. Nevertheless their conclusions that various approaches perform similarly
    when the peak in the reward function (i.e., oracle AI preferences) aligns with
    the likely regions in the data (i.e., responses generated from the same AI model),
    thus providing evidence to support our findings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，Ahmadian 等人 ([2024](#bib.bib3)) 表明，REINFORCE 可能足以进行 LLM 的偏好微调，复杂的策略优化方法如
    PPO 可能并不必要。我们的结论大多是互补的，尽管我们确实观察到 PPO 对样本重用比 REINFORCE 更加稳健。与此同时，Sharma 等人 ([2024](#bib.bib43))
    比较了对比和监督微调在 LLM 生成的数据上的效果，但该工作没有研究覆盖率或几何条件的作用。然而，他们的结论表明，当奖励函数（即，oracle AI 偏好）的峰值与数据中的可能区域（即，从同一
    AI 模型生成的响应）对齐时，各种方法的表现相似，从而为支持我们的发现提供了证据。
- en: 3 Characterizing And Unifying Preference Fine-Tuning Methods
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 特征化和统一偏好微调方法
- en: Typical preference fine-tuning methods use a variety of objectives including
    RL, maximum likelihood, and contrastive learning. While the huge number of fine-tuning
    methods inhibits us from empirically analyzing each of them, in this section we
    characterize several existing methods into different families and subsequently
    study a representative member from each family.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的偏好微调方法使用各种目标，包括强化学习（RL）、最大似然和对比学习。虽然大量的微调方法阻碍了我们对每种方法进行实证分析，但在本节中，我们将现有的几种方法归类到不同的家族中，并随后研究每个家族中的一个代表性成员。
- en: 3.1 Preliminaries and Notation
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 前提和符号
- en: 'Typically, before training on preference data, a pre-trained model is fine-tuned
    on high-quality data from the task of interest via supervised fine-tuning (SFT),
    to obtain a “reference” model $\pi_{\mathrm{ref}}$ denotes a prompt and $\mathbf{y}_{w}^{(i)},\mathbf{y}_{l}^{(i)}$.
    One popular framework for this is the Bradley-Terry (BT) model (Bradley and Terry,
    [1952](#bib.bib4)), assuming that human preferences can be written as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在对偏好数据进行训练之前，会对来自感兴趣任务的高质量数据进行监督微调（SFT），以获得一个“参考”模型 $\pi_{\mathrm{ref}}$，表示为提示和
    $\mathbf{y}_{w}^{(i)},\mathbf{y}_{l}^{(i)}$。其中一个流行的框架是 Bradley-Terry (BT) 模型（Bradley
    和 Terry，[1952](#bib.bib4)），假设人类偏好可以表示为：
- en: '|  | $1$2 |  | (3.1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3.1) |'
- en: 'Given this reward function $r^{*}$. While the ultimate goal of preference fine-tuning
    is to find the *unconstrained* optimum of the reward function, in practice, we
    often replace the reward function with a reward model. Since the reward model
    is erroneous, we apply KL-constraint to prevent exploitation in the reward model.
    To align our results with typical preference fine-tuning procedures, we will consider
    such a KL-constrained reward optimization as our fine-tuning goal:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一奖励函数 $r^{*}$。虽然偏好微调的**终极目标**是找到奖励函数的 *无约束* 最优解，但在实际操作中，我们通常用奖励模型替代奖励函数。由于奖励模型存在误差，我们应用
    KL 约束以防止在奖励模型中进行利用。为了使我们的结果与典型的偏好微调过程保持一致，我们将考虑这样的 KL 约束奖励优化作为我们的微调目标：
- en: '|  | $\displaystyle\!\!\!\!\max_{\pi_{\theta}}\leavevmode\nobreak\ $ |  | (3.2)
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!\!\max_{\pi_{\theta}}\leavevmode\nobreak\ $ |  | (3.2)
    |'
- en: The regularizer, weighted by $\beta$ under the reverse KL divergence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器，由 $\beta$ 权重调整，在反向 KL 散度下。
- en: 'Reward model training. In order to fine-tune an LLM policy $\pi_{\theta}(\mathbf{y}|\mathbf{x})$).
    Explicit reward models are trained using the following classification objective:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型训练。为了微调一个 LLM 策略 $\pi_{\theta}(\mathbf{y}|\mathbf{x})$）。使用以下分类目标来训练显式奖励模型：
- en: '|  | $1$2 |  | (3.3) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3.3) |'
- en: 'where $\sigma$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$：
- en: '|  | $\displaystyle r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\right].$
    |  | (3.4) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\right].$
    |  | (3.4) |'
- en: 3.2 Characterizing Fine-Tuning Methods
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 微调方法的特征
- en: 'With a reward model $r_{\phi}(\mathbf{x},\mathbf{y})$. Since we cannot empirically
    investigate all of these methods, we group them into different categories (summary
    shown in [Table 1](#S3.T1 "In 3.2 Characterizing Fine-Tuning Methods ‣ 3 Characterizing
    And Unifying Preference Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")). In particular, we are interested in whether
    these methods employ:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用奖励模型 $r_{\phi}(\mathbf{x},\mathbf{y})$。由于我们无法实证研究所有这些方法，我们将它们分为不同的类别（总结见[表
    1](#S3.T1 "In 3.2 Characterizing Fine-Tuning Methods ‣ 3 Characterizing And Unifying
    Preference Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data")）。特别地，我们感兴趣的是这些方法是否采用：
- en: '1.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*on-policy sampling*: an explicit sampling of new responses from the policy
    (e.g., PPO, REINFORCE) or purely learning from offline data (e.g., RWR, DPO, IPO)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*在线策略采样*：从策略中显式采样新响应（例如，PPO，REINFORCE）或仅从离线数据中学习（例如，RWR，DPO，IPO）'
- en: '2.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*on-policy sample reuse*: for only those approaches that perform on-policy
    sampling, whether the approach makes more than one gradient update on a given
    prompt-response $(\mathbf{x},\mathbf{y})$ for PPO, online RWR)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*在线策略样本重用*：对于仅进行在线策略采样的方法，方法是否对给定的提示-响应 $(\mathbf{x},\mathbf{y})$ 进行多次梯度更新（例如
    PPO，在线 RWR）'
- en: '3.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '*negative gradient*: whether the approach explicitly minimizes a loss that
    attempts to “push-down” likelihood on certain responses by multiplying the gradient
    of their likelihood with a negative coefficient (e.g., contrastive methods such
    as DPO; RL methods REINFORCE, PPO)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*负梯度*：该方法是否明确最小化一个损失，通过将其梯度与负系数相乘来“压低”某些响应的似然（例如，DPO 等对比方法；REINFORCE，PPO 等
    RL 方法）'
- en: 'On-policy RL approaches such as PPO (Schulman et al., [2017](#bib.bib42)) and
    REINFORCE (Williams, [1992](#bib.bib51)) explicitly sample new responses from
    the current snapshot of the learned policy, $\mathbf{y}_{i}\sim\pi_{\theta}(\cdot|\mathbf{x}_{i})$,
    for example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在线策略强化学习方法如 PPO（Schulman et al., [2017](#bib.bib42)）和 REINFORCE（Williams, [1992](#bib.bib51)）明确从当前学习到的策略快照中采样新响应，例如
    $\mathbf{y}_{i}\sim\pi_{\theta}(\cdot|\mathbf{x}_{i})$：
- en: '|  | $\displaystyle\theta^{\prime}\leftarrow\theta-\eta\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}},\mathbf{y}\sim\pi_{\theta}(\cdot&#124;\mathbf{x})}\left[\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})\cdot\bar{r}_{\phi}(\mathbf{x},\mathbf{y})\right]\leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \text{(REINFORCE)},$ |  | (3.5) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{\prime}\leftarrow\theta-\eta\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}},\mathbf{y}\sim\pi_{\theta}(\cdot&#124;\mathbf{x})}\left[\nabla_{\theta}\log\pi_{\theta}(\mathbf{y}&#124;\mathbf{x})\cdot\bar{r}_{\phi}(\mathbf{x},\mathbf{y})\right]\leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \text{(REINFORCE)},$ |  | (3.5) |'
- en: is the gradient update employed by REINFORCE, where $\bar{r}_{\phi}(\mathbf{x},\mathbf{y})$
    corresponds to a normalized estimate of the reward model’s predictions over a
    batch of samples drawn from the policy. As we discuss in more detail in [Section D.1](#A4.SS1
    "D.1 Score/Reward Standardization ‣ Appendix D Additional Algorithmic Details
    ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data")), using a normalized reward estimate instead of directly the raw reward
    value helps reduce the variance of the policy gradient estimate. High variance
    gradients slow down convergence and even sometimes lead to sub-optimal solutions
    in deep RL (Mei et al., [2022](#bib.bib31)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 REINFORCE 使用的梯度更新，其中 $\bar{r}_{\phi}(\mathbf{x},\mathbf{y})$ 对应于从策略中抽取的一批样本的奖励模型预测的标准化估计。正如我们在[第
    D.1 节](#A4.SS1 "D.1 Score/Reward Standardization ‣ Appendix D Additional Algorithmic
    Details ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data")中更详细讨论的那样，使用标准化奖励估计而不是直接使用原始奖励值有助于减少策略梯度估计的方差。高方差梯度会减慢收敛速度，有时甚至导致深度强化学习中的次优解（Mei
    et al., [2022](#bib.bib31)）。
- en: 'Due to the use of normalized reward estimates, policy gradient approaches behave
    distinctly from maximum likelihood supervised learning: a policy gradient update
    also updates the parameters $\theta$. This means that on-policy RL also has a
    form of the “negative gradient”.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了标准化奖励估计，策略梯度方法的行为与最大似然监督学习明显不同：策略梯度更新也会更新参数 $\theta$。这意味着在线策略强化学习也具有一种“负梯度”形式。
- en: 'PPO differs from REINFORCE because it employs *sample reuse* in addition to
    on-policy sampling: unlike REINFORCE which only performs a single gradient update
    on a response sampled from the current policy, PPO can utilize a response for
    several policy updates. To prevent making updates on overly off-policy responses,
    there is a mechanism in place to filter responses by the magnitude of the importance
    ratio between the current policy $\pi_{\theta}(\mathbf{y}|\mathbf{x})$ and the
    data collection policy.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 与 REINFORCE 的不同在于它除了基于策略的采样外，还采用了*样本重用*：与 REINFORCE 仅对从当前策略中采样的响应执行一次梯度更新不同，PPO
    可以对多个策略更新利用同一响应。为了防止对过于偏离策略的响应进行更新，系统有机制通过当前策略 $\pi_{\theta}(\mathbf{y}|\mathbf{x})$
    和数据收集策略之间的权重比的大小来筛选响应。
- en: Finally, we also remark that while on-policy methods do generate new rollouts
    from the policy, these responses are still scored by a reward model (and not the
    ground truth reward function, i.e., humans). Since reward labels come from a reward
    model, on-policy preference fine-tuning approaches are instances of offline model-based
    RL (Yu et al., [2021](#bib.bib58), [2020](#bib.bib57); Kidambi et al., [2020](#bib.bib25))
    methods that run on-policy rollouts against a learned dynamics and reward model
    (due to the single step nature of preference fine-tuning, there is no dynamics
    model).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还指出，虽然基于策略的方法确实会从策略中生成新滚动，但这些响应仍由奖励模型（而非真实奖励函数，即人类）进行评分。由于奖励标签来自奖励模型，基于策略的偏好微调方法是离线基于模型的强化学习（Yu
    et al., [2021](#bib.bib58), [2020](#bib.bib57); Kidambi et al., [2020](#bib.bib25)）方法的实例，这些方法在一个学习的动态和奖励模型上运行基于策略的滚动（由于偏好微调的单步特性，没有动态模型）。
- en: '| Fine-Tuning Approach | On-Policy Sampling | Sample Reuse | Negative Gradient
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 微调方法 | 基于策略采样 | 样本重用 | 负梯度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PPO | $\checkmark$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| PPO | $\checkmark$ |'
- en: '| REINFORCE | $\checkmark$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| REINFORCE | $\checkmark$ |'
- en: '| DPO, IPO, and variants | $\times$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| DPO、IPO 及变种 | $\times$ |'
- en: '| Pref-FT, Binary FeedMe | $\times$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Pref-FT，Binary FeedMe | $\times$ |'
- en: '| offline RWR, offline Best-of-N | $\times$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 离线 RWR，离线 Best-of-N | $\times$ |'
- en: '| ReST, RWR, online Best-of-N | $\checkmark$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ReST, RWR, online Best-of-N | $\checkmark$ |'
- en: 'Table 1: Grouping various fine-tuning methods along the axes on-policy sampling,
    sample reuse, and negative gradient. Since offline methods do not collect on-policy
    data, the question of discarding or reusing on-policy samples is not applicable.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：根据基于策略采样、样本重用和负梯度的不同，将各种微调方法进行分组。由于离线方法不收集基于策略的数据，因此丢弃或重用基于策略样本的问题不适用。
- en: 'On-policy supervised approaches such as RAFT (Dong et al., [2023](#bib.bib13)),
    ReST (Gulcehre et al., [2023](#bib.bib19)), and SuperHF (Mukobi et al., [2023](#bib.bib32))
    iteratively minimize a weighted maximum likelihood loss inspired by Peters and
    Schaal ([2007](#bib.bib36)); Korbak et al. ([2022](#bib.bib28)). For a given prompt
    $\mathbf{x}_{i}$ as in the case of reward-weighted regression (RWR) or obtain
    the subset of $K$ to a scalar value conditioned on other responses $\mathbf{y}_{i}^{k}$,
    these methods maximize:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的监督方法，如 RAFT（Dong et al., [2023](#bib.bib13)）、ReST（Gulcehre et al., [2023](#bib.bib19)）和
    SuperHF（Mukobi et al., [2023](#bib.bib32)）通过最小化一个加权的最大似然损失，灵感来自 Peters 和 Schaal
    ([2007](#bib.bib36)); Korbak et al. ([2022](#bib.bib28))。对于给定的提示 $\mathbf{x}_{i}$，如在奖励加权回归（RWR）的情况下，或获得条件于其他响应
    $\mathbf{y}_{i}^{k}$ 的 $K$ 的子集，这些方法最大化：
- en: '|  | $1$2 |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'These algorithms employ sample reuse because they operate in a “batched” online
    fashion: instead of performing *exactly one* gradient step on a given model sample;
    RWR, ReST, and SuperHF run more gradient updates, after which new samples are
    drawn. However, since these methods only maximize likelihood (i.e., only positive
    multipliers), there is no negative gradient effect.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法采用样本重用，因为它们以“批量”在线方式运行：RWR、ReST 和 SuperHF 在对给定模型样本进行*精确一次*梯度步骤后，执行更多的梯度更新，然后再抽取新样本。然而，由于这些方法仅最大化似然（即，仅正乘子），因此不存在负梯度效应。
- en: Fully offline methods like DPO and IPO run contrastive training on the preference
    dataset $\mathcal{D}_{\text{pref}}$. Despite no on-policy sampling, contrastive
    loss between winning and losing responses explicitly attempts to reduce log-likelihood
    ratio $\log\left(\frac{\pi_{\theta}(\mathbf{y}|\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})}\right)$.
    Another offline method is Pref-FT (Dubois et al., [2024](#bib.bib14)) which runs
    supervised fine-tuning on preferred responses. These methods in general are akin
    to offline model-free methods, in that no reward model is utilized by these methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 像 DPO 和 IPO 这样的完全离线方法在偏好数据集 $\mathcal{D}_{\text{pref}}$ 上进行对比训练。尽管没有策略采样，但获胜和失败响应之间的对比损失明确地尝试减少对数似然比
    $\log\left(\frac{\pi_{\theta}(\mathbf{y}|\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})}\right)$。另一种离线方法是
    Pref-FT (Dubois 等，[2024](#bib.bib14))，它在偏好响应上进行有监督的微调。这些方法通常类似于离线无模型方法，即这些方法没有利用奖励模型。
- en: 4 Research Questions and Analysis Setup
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 研究问题和分析设置
- en: 'Our goal is to understand the behaviors of various procedures for fine-tuning
    language models. As discussed above, typically these methods differ along the
    use of on-policy sampling (with additional differences pertaining to sample reuse)
    and the presence of a negative gradient. We build a setup to understand these
    differences empirically by answering the following questions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是理解各种调整语言模型程序的行为。如上所述，这些方法通常在使用策略采样（以及与样本重用相关的额外差异）和负梯度的存在上有所不同。我们构建了一个设置，通过回答以下问题来实证理解这些差异：
- en: 'Question 1: When does on-policy sampling improve over offline fine-tuning,
    even though on-policy samples are annotated by a reward model, which itself is
    learned from offline data? Is sample reuse useful or harmful for on-policy methods?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 1：即使策略采样的样本由从离线数据学习得来的奖励模型注释，策略采样何时比离线微调更有效？样本重用对于策略方法有用还是有害？
- en: 'Question 2: When does an explicit negative gradient help the discovery of effective
    policies compared to maximum likelihood approaches such as distilling the Best-of-N
    policy?'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 2：与最大似然方法（如提取最佳策略）相比，明确的负梯度何时有助于发现有效的策略？
- en: 'Question 3: Does on-policy sampling offer complementary benefits to negative
    gradient, resulting in better performance with effective contrastive approaches
    (e.g., DPO)?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 3：策略采样是否提供了对负梯度的互补益处，从而通过有效的对比方法（例如，DPO）实现更好的性能？
- en: To gain practically useful and actionable insights, we must answer these questions
    in the context of coverage and geometric relations between the training data,
    reference policy, and the reward function. These relations affect the shape of
    the optimally fine-tuned policy and dictate the dynamics of various objectives
    under consideration. We consider specific conditions and relations that we discuss
    next.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得实用且可操作的见解，我们必须在覆盖范围和训练数据、参考策略与奖励函数之间的几何关系的背景下回答这些问题。这些关系影响优化调整策略的形状，并决定各种目标的动态。我们将讨论具体的条件和关系。
- en: 4.1 Coverage Conditions and Geometric Relationships
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 覆盖条件和几何关系
- en: 'The dynamics of the KL-constrained surrogate optimization problem ([Equation 3.2](#S3.E2
    "In 3.1 Preliminaries and Notation ‣ 3 Characterizing And Unifying Preference
    Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data")) depends on the geometric alignment between the ground-truth
    reward function $r^{*}$ also dictates the correctness of reward estimates and
    hence controls the efficacy of the surrogate fine-tuning optimization. Likewise,
    the performance of purely offline methods (e.g., offline best-of-N or contrastive
    methods such as offline DPO) that do not use a reward model also depends on the
    relative geometric alignment between $r^{*}$) and also on the relative coverage
    of preference data (i.e., the lower the coverage, the harder it is to discover
    high-reward responses). To understand the efficacy of various methods, we consider
    multiple scenarios that differ along these two factors:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: KL约束的替代优化问题的动态（[方程3.2](#S3.E2 "在3.1 前言和符号 ‣ 3 描述和统一偏好微调方法 ‣ LLM的偏好微调应利用次优的、在政策内的数据")）依赖于真实奖励函数$r^{*}$之间的几何对齐情况，这也决定了奖励估计的正确性，从而控制了替代微调优化的效果。同样，不使用奖励模型的纯离线方法（例如，离线best-of-N或对比方法，如离线DPO）的性能也依赖于$r^{*}$之间的几何对齐情况以及偏好数据的相对覆盖范围（即，覆盖范围越低，发现高奖励响应的难度越大）。为了理解各种方法的效果，我们考虑了在这两个因素上有所不同的多个场景：
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[C1]: the geometric alignment between the ground-truth reward function $r^{*}$.
    This concept is analogous to that of a “concentrability coefficient” (Munos and
    Szepesvári, [2008](#bib.bib33)).'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[C1]：真实奖励函数$r^{*}$之间的几何对齐。这一概念类似于“集中度系数”（Munos和Szepesvári，[2008](#bib.bib33)）。'
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[C2]: the coverage of the preference data used to train the surrogate reward
    model $r_{\phi}$.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[C2]：用于训练替代奖励模型$r_{\phi}$的偏好数据的覆盖范围。'
- en: Understanding the behavior of various approaches as a function of these factors
    will allow us to better understand the performance of various approaches on downstream
    fine-tuning in terms of problem geometry [C1] and statistical learning considerations [C2].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 理解各种方法在这些因素的影响下的表现将帮助我们更好地了解不同方法在下游微调中的性能，涉及问题几何[C1]和统计学习考虑因素[C2]。
- en: 4.2 Tasks and Datasets
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 任务与数据集
- en: We construct a variety of didactic and LLM tasks that allow us to gain intuition
    for performance of different methods under various scenarios grouped along relationships
    [C1] and [C2].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了各种教学和LLM任务，使我们能够在不同场景下对不同方法的性能有直观了解，这些场景按照关系[C1]和[C2]进行分组。
- en: '![Refer to caption](img/8d4ecd0db8df19edc67464f81e2fcede.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d4ecd0db8df19edc67464f81e2fcede.png)'
- en: 'Figure 2: The didactic bandit problem which we use for our analysis in this
    paper. Reference policy initialization and reward slice for each token (the total
    reward is a mean of token-level rewards). The optima of reward functions $\mathbf{R}_{1}$
    occur in low-density and high-density regions respectively.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们在本文分析中使用的教学赌博问题。每个token的参考策略初始化和奖励切片（总奖励是token级奖励的平均值）。奖励函数$\mathbf{R}_{1}$的最优值分别出现在低密度和高密度区域。
- en: Didactic $N$ of size $100$ is a sequence of $N=10$ is roughly aligned with the
    mode of the reference policy. We hypothesize that on-policy sampling will be crucial
    to optimize reward function $\mathbf{R}_{1}$.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 教学$N$大小为$100$的序列$N=10$大致与参考策略的模式对齐。我们假设在政策内采样对于优化奖励函数$\mathbf{R}_{1}$至关重要。
- en: '![Refer to caption](img/9b1929eefa9b138fd53761aea7680f59.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9b1929eefa9b138fd53761aea7680f59.png)'
- en: 'Figure 3: Word length distribution. Above, we show the word length distribution
    for the preferred and dispreferred completions of the Left: min and Right: skew
    synthetic LLM datasets.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：词长分布。上图显示了左侧：最小值和右侧：偏斜合成LLM数据集的首选和非首选补全的词长分布。
- en: 'Synthetic LLM fine-tuning problems. Next, we will generalize our intuitions
    from bandit problems to the LLM setting. Instead of directly experimenting with
    human preferences, we first study two synthetic problems that utilize hand-crafted
    reward functions, which can be approximated via reward models. Access to functional
    forms of these hand-crafted reward functions will enable us to track the ground-truth
    objective throughout training to see if our insights about various approaches
    under condition [C1] will hold even when learning against a reward model. Subsequently,
    we run this experiment with an altered skewed preference data distribution (see
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Tasks and Datasets ‣ 4 Research Questions and
    Analysis Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data")) to understand the effect of coverage conditions [C2]. We consider two
    reward functions: (1) one that minimizes the response length (“Min Length”), analogous
    to $\mathbf{R}_{1}$. The Skew Length scenario skews the preference data in the
    Min Length problem scenario.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 合成LLM微调问题。接下来，我们将把从赌博机问题获得的直觉推广到LLM设置中。我们首先研究两个利用手工设计奖励函数的合成问题，这些函数可以通过奖励模型进行近似。访问这些手工设计的奖励函数的功能形式将使我们能够在训练过程中跟踪真实目标，以查看在对抗奖励模型学习的情况下我们对不同方法的见解是否成立。随后，我们用一个改变的偏斜偏好数据分布运行这个实验（见图[3](#S4.F3
    "Figure 3 ‣ 4.2 Tasks and Datasets ‣ 4 Research Questions and Analysis Setup ‣
    Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")），以了解覆盖条件[C2]的效果。我们考虑两个奖励函数：（1）一个最小化响应长度（“Min
    Length”），类似于$\mathbf{R}_{1}$。Skew Length场景在Min Length问题场景中偏斜偏好数据。
- en: Full-scale LLM fine-tuning. Finally, we scale up our study to full-scale LLMs,
    with real preference data. Recent work (Singhal et al., [2023](#bib.bib44)) shows
    that preference labels are usually biased towards much longer responses, indicating
    that preference fine-tuning usually admits a geometric relationship where the
    mode of the reward function is distinct from the mode of human data (and hence,
    any reference policy). For the majority of our experiments, we use preference
    datasets from the AlpacaFarm benchmark (Dubois et al., [2024](#bib.bib14)). We
    also scale up our experiments to UltraChat (Ding et al., [2023](#bib.bib12)),
    a $\sim 10$ times larger dataset with responses from many strong LLMs such as
    GPT 4 and GPT-3.5.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 全尺度LLM微调。最后，我们将研究范围扩大到全尺度LLM，并使用真实的偏好数据。最近的工作（Singhal 等人，[2023](#bib.bib44)）显示，偏好标签通常偏向于更长的回答，这表明偏好微调通常存在几何关系，其中奖励函数的模式与人类数据的模式（以及任何参考策略）不同。在我们的大多数实验中，我们使用来自AlpacaFarm基准的偏好数据集（Dubois
    等人，[2024](#bib.bib14)）。我们还将实验扩大到UltraChat（Ding 等人，[2023](#bib.bib12)），这是一个$\sim
    10$倍于的更大数据集，包含来自许多强大LLM（如GPT 4和GPT-3.5）的回答。
- en: 4.3 A Generic Fine-Tuning Algorithm Encapsulating All Axes
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3 一个通用的微调算法，涵盖所有维度
- en: To systematically analyze the behavior of fine-tuning methods that differ along
    the axes discussed in [Section 3.2](#S3.SS2 "3.2 Characterizing Fine-Tuning Methods
    ‣ 3 Characterizing And Unifying Preference Fine-Tuning Methods ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data"), in this section, we introduce
    a generic algorithm with different hyperparameters associated with each axes.
    With a generic algorithm of this sort, we will be able to answer our research
    questions by varying each hyperparameter. Our unified practical algorithm is shown
    [Algorithm 1](#alg1 "In 4.3 A Generic Fine-Tuning Algorithm Encapsulating All
    Axes ‣ 4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data"). While on-policy algorithms perform
    steps 1 and 2 of on-policy data collection with a reward model, purely offline
    methods (e.g., DPO and RWR) utilize preference data directly.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统地分析沿着[第3.2节](#S3.SS2 "3.2 Characterizing Fine-Tuning Methods ‣ 3 Characterizing
    And Unifying Preference Fine-Tuning Methods ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")讨论的维度上不同的微调方法的行为，本节介绍了一个通用算法，其中每个维度关联不同的超参数。通过这种通用算法，我们将能够通过改变每个超参数来回答我们的研究问题。我们的统一实用算法见[算法1](#alg1
    "In 4.3 A Generic Fine-Tuning Algorithm Encapsulating All Axes ‣ 4 Research Questions
    and Analysis Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data")。虽然在政策算法使用奖励模型执行步骤1和2的数据收集，但纯离线方法（例如DPO和RWR）直接利用偏好数据。
- en: Algorithm 1 A Unified Fine-Tuning Algorithm
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 统一微调算法
- en: for training iterations do     (1) Sample $B/C$ responses for $\frac{B}{C}$,
    with rewards drawn           from the learned reward model $\widehat{r}_{\phi}(\mathbf{y}|\mathbf{x})$,
    each with $M$ prescribed by the fine-tuning method.         end for     end forend for
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 训练迭代 做     （1）对 $\frac{B}{C}$ 进行 $B/C$ 响应采样，从学习到的奖励模型 $\widehat{r}_{\phi}(\mathbf{y}|\mathbf{x})$
    中抽取奖励，每个由微调方法规定的 $M$。         结束 对于     结束 对于结束 对于
- en: 'To study the impact of on-policy sampling, we vary the extent to which updates
    are made on data from the current policy. We can control this by two means in
    [Algorithm 1](#alg1 "In 4.3 A Generic Fine-Tuning Algorithm Encapsulating All
    Axes ‣ 4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data"): (1) by varying the total number
    of samples $|\mathcal{D}|=\frac{B}{C}\times C=B$ of on-policy samples (i.e., a
    larger $T$ is larger. While both approaches enable us to control how on-policy
    an algorithm is, approach (1) does not reuse samples (since $\mathcal{D}$ in the
    dataset to understand the role of the negative gradient.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究策略采样的影响，我们调整了对当前策略数据进行更新的程度。我们可以通过 [算法 1](#alg1 "在 4.3 通用微调算法封装所有轴 ‣ 4 研究问题和分析设置
    ‣ LLM 的偏好微调应利用次优的、策略上的数据") 通过两种方式来控制这一点：（1）通过调整策略样本的总数 $|\mathcal{D}|=\frac{B}{C}\times
    C=B$（即，较大的 $T$ 更大。虽然这两种方法都可以控制算法的策略性，但方法（1）不重复使用样本（因为 $\mathcal{D}$ 在数据集中以了解负梯度的作用）。
- en: 5 Empirical Analysis Results
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 实证分析结果
- en: In this section, we will present the results of our empirical study to answer
    our research questions. To answer each question, we will begin by studying the
    didactic bandit problem with the ground-truth reward function, followed by synthetic
    and then full-scale LLM fine-tuning problems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示我们的实证研究结果以回答我们的研究问题。为了回答每个问题，我们将首先研究具有真实奖励函数的教学赌博机问题，然后是合成问题，最后是大规模
    LLM 微调问题。
- en: '5.1 Question 1: The Role of On-Policy Sampling'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1 问题 1：策略采样的作用
- en: To understand the role of on-policy sampling, we will investigate if on-policy
    sampling can improve performance for several approaches followed by making conclusions
    regarding sample reuse.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解策略采样的作用，我们将调查策略采样是否能提高几种方法的性能，然后得出有关样本重用的结论。
- en: '5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance'
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.1 结论 1：奖励模型中的策略采样提高了性能
- en: We first study on-policy sampling as a function of the geometric relationship
    [C1] in our bandit setting (see [Figure 2](#S4.F2 "In 4.2 Tasks and Datasets ‣
    4 Research Questions and Analysis Setup ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")), with no sampling error. Then, we will
    extend our conclusions to the LLM setting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在无采样误差的情况下，研究了我们在赌博机设置中的几何关系 [C1] 作为**策略采样**的函数（见 [图 2](#S4.F2 "在 4.2 任务和数据集
    ‣ 4 研究问题和分析设置 ‣ LLM 的偏好微调应利用次优的、策略上的数据")）。然后，我们将把我们的结论扩展到 LLM 设置中。
- en: '![Refer to caption](img/bc1e440ec2de6b93bb8e0c9fd83493bd.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bc1e440ec2de6b93bb8e0c9fd83493bd.png)'
- en: 'Figure 4: On-policy sampling on bandit problems. Performance of on-policy best-of-N
    as a function of the data sampled in each iteration. Larger batch sizes result
    in more off-policy updates. Left: (i) reward vs update step for $\mathbf{R}_{1}$,
    but less severe degradation for $\mathbf{R}_{2}$, where peaks in the reference
    policy and reward function are more aligned.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：赌博机问题中的策略采样。每次迭代中数据采样的策略最佳-N 的性能。较大的批量大小会导致更多的策略外更新。左：对 $\mathbf{R}_{1}$
    的奖励与更新步数，但对 $\mathbf{R}_{2}$ 的降解较轻，其中参考策略和奖励函数的峰值更为对齐。
- en: 'Didactic bandit problems.  [Figure 4](#S5.F4 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") shows that given a fixed amount
    of total data budget, *sampling data more frequently from more recent policies*,
    but in smaller batches, results in better performance with both $\mathbf{R}_{1}$,
    $\mathbb{D}_{\text{KL}}(\pi_{\theta}||\pi_{\text{gen}})$ results in higher peak
    values of this divergence during training indicating further deviation from the
    data at intermediate times during training. This means that being more on-policy
    corresponds to better performance and faster convergence for best-of-N.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 教学型强盗问题。[图 4](#S5.F4 "在 5.1.1 摘要 1：奖励模型中的策略采样提升了性能 ‣ 5.1 问题 1：策略采样的作用 ‣ 5 实证分析结果
    ‣ LLM 的偏好微调应利用次优的、在策略数据")显示，在给定固定总数据预算的情况下，*从更新的策略中更频繁地采样数据*，尽管批量更小，能在$\mathbf{R}_{1}$和$\mathbb{D}_{\text{KL}}(\pi_{\theta}||\pi_{\text{gen}})$中获得更好的性能，结果显示训练期间该差异的峰值更高，表示在训练中期对数据的进一步偏离。这意味着更接近策略的表现对应于最佳
    N 的更好性能和更快的收敛。
- en: 'That said, we also note in [Figure 4](#S5.F4 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") that the performance degradation
    with more off-policy updates is substantially milder for $\mathbf{R}_{2}$, indicating
    that when the peak in the reward function lies in the high likely regions of the
    reference policy, a higher degree of off-policy updates is tolerable.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们还注意到在[图 4](#S5.F4 "在 5.1.1 摘要 1：奖励模型中的策略采样提升了性能 ‣ 5.1 问题 1：策略采样的作用 ‣
    5 实证分析结果 ‣ LLM 的偏好微调应利用次优的、在策略数据")中，当奖励函数的峰值位于参考策略的高概率区域时，$\mathbf{R}_{2}$ 的性能下降要显著温和，这表明更高程度的离策略更新是可以接受的。
- en: '| [C1] $\downarrow$ and $\pi_{\text{ref}}$ overlap |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [C1] $\downarrow$ 和 $\pi_{\text{ref}}$ 重叠 |'
- en: '| --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| peaks of $r^{*}$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| $r^{*}$ 的峰值 |'
- en: '| peaks of $r^{*}$ Skew Length |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| $r^{*}$ 偏斜长度 |'
- en: 'Table 2: Coverage conditions and geometric relations that we study with synthetic
    LLM fine-tuning data. The three settings we study differ in terms of overlap between
    $\pi_{\text{ref}}$.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们使用合成 LLM 微调数据研究的覆盖条件和几何关系。我们研究的三种设置在$\pi_{\text{ref}}$的重叠程度上有所不同。
- en: 'Synthetic LLM problems. In this problem setting, we optimize the policy against
    a reward model, which is learned from preference data. Per [Section 4.2](#S4.SS2
    "4.2 Tasks and Datasets ‣ 4 Research Questions and Analysis Setup ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"), we construct
    three scenarios that differ along geometric ([C1]) and coverage ([C2]) conditions
    as depicted in [Table 2](#S5.T2 "In 5.1.1 Takeaway 1: On-Policy Sampling in the
    Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). The peak of the reward in the Min Length scenario
    appears in the less likely regions of $\pi_{\text{ref}}$.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 合成 LLM 问题。在这个问题设置中，我们优化一个基于从偏好数据中学习到的奖励模型的策略。根据[第 4.2 节](#S4.SS2 "4.2 任务和数据集
    ‣ 4 研究问题和分析设置 ‣ LLM 的偏好微调应利用次优的、在策略数据")，我们构建了三个场景，这些场景在几何 ([C1]) 和覆盖 ([C2]) 条件上有所不同，如[表
    2](#S5.T2 "在 5.1.1 摘要 1：奖励模型中的策略采样提升了性能 ‣ 5.1 问题 1：策略采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的、在策略数据")所示。在最小长度场景中，奖励的峰值出现在$\pi_{\text{ref}}$的可能性较低的区域。
- en: '![Refer to caption](img/9952493d5b740fd5fc70ded1604622d0.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9952493d5b740fd5fc70ded1604622d0.png)'
- en: 'Figure 5: On-policy sampling for PPO in the Min Length scenario. This plot
    keeps the minibatch size $M$). Left: average completion length (lower the better),
    and Right: proxy reward vs gradient steps. Being more on-policy results in better
    performance. The mini-batch size $M$ used for gradient updates is kept fixed to
    avoid confounders arising from the use of stochastic optimization procedures.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在最小长度场景中 PPO 的在策略采样。该图保持了小批量大小 $M$)。左：平均完成长度（越低越好），右：代理奖励与梯度步骤。更接近策略的表现会更好。用于梯度更新的
    mini-batch 大小 $M$ 被固定，以避免使用随机优化程序带来的混淆因素。
- en: 'We present our results for one algorithm in detail (in this case, PPO) ([Figures 5](#S5.F5
    "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance
    ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"),
    [6](#S5.F6 "Figure 6 ‣ 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model
    Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data") and [7](#S5.F7 "Figure 7 ‣ 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")) and then present a summary bar chart showing
    that our conclusions also transfer to other algorithms (such as REINFORCE and
    RWR) ([Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward
    Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣
    5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data")). Extending insights from the bandit problem, in
    the Min Length scenario, we find that being more on-policy (i.e., a smaller $B$
    and $B=256$. This indicates that with a significant overlap between the preference
    data and the reference policy, on-policy sampling still leads to better performance
    with fewer updates. We also find similar trends across on-policy variants of RWR
    and REINFORCE, where modulo training instabilities, being more on-policy results
    in better performance ([Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"); Min Length).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细展示了一个算法的结果（在这种情况下为 PPO）（[图 5](#S5.F5 "在 5.1.1 结论 1：奖励模型中的在政策采样提高了性能 ‣ 5.1
    问题 1：在政策采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在政策数据")，[6](#S5.F6 "图 6 ‣ 5.1.1 结论 1：奖励模型中的在政策采样提高了性能
    ‣ 5.1 问题 1：在政策采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在政策数据") 和 [7](#S5.F7 "图 7 ‣ 5.1.1
    结论 1：奖励模型中的在政策采样提高了性能 ‣ 5.1 问题 1：在政策采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在政策数据")），然后展示一个总结条形图，表明我们的结论也适用于其他算法（如
    REINFORCE 和 RWR）（[图 8](#S5.F8 "在 5.1.1 结论 1：奖励模型中的在政策采样提高了性能 ‣ 5.1 问题 1：在政策采样的作用
    ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在政策数据")）。扩展来自赌博问题的见解，在最小长度场景中，我们发现更高的在政策性（即，更小的 $B$
    和 $B=256$）。这表明，在偏好数据与参考策略之间存在显著重叠的情况下，在政策采样仍能在更少的更新中实现更好的性能。我们还发现 RWR 和 REINFORCE
    的在政策变体中存在类似的趋势，其中在政策性更高的情况下，性能更佳（[图 8](#S5.F8 "在 5.1.1 结论 1：奖励模型中的在政策采样提高了性能 ‣
    5.1 问题 1：在政策采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在政策数据")；最小长度）。
- en: '![Refer to caption](img/d1cae1c97dd8996680f44c1ed1685fa6.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d1cae1c97dd8996680f44c1ed1685fa6.png)'
- en: 'Figure 6: On-policy sampling for PPO in the Mode Length scenario. In this case,
    since the peak in the reward function and the highly likely regions of the reference
    policy are close, we find that the degree of on-policyness does not significantly
    affect performance. Left: distance to mode i.e., |completion length - average
    length in the dataset| (lower the better), Right: proxy reward vs gradient steps.
    As optimal policy $\pi^{*}$ used for the gradient update is kept fixed.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在模式长度场景中的 PPO 策略采样。在这种情况下，由于奖励函数的峰值和参考策略的高概率区域很接近，我们发现策略的“在政策性”程度对性能的影响不显著。左侧：距离模式，即
    |完成长度 - 数据集中的平均长度|（越低越好），右侧：代理奖励与梯度步数。由于用于梯度更新的最优策略 $\pi^{*}$ 保持不变。
- en: 'In the Mode Length scenario, where the preferred response for each preference
    pair are those that are closest to the average length in the dataset (203), varying
    the degree of on-policy sampling by adjusting the sampling frequency largely does
    not affect either the proxy or gold reward for PPO ([Figure 6](#S5.F6 "In 5.1.1
    Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance ‣ 5.1
    Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")). We make similar
    observations for other algorithms: [Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy
    Sampling in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of
    On-Policy Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"); Mode Length: different degrees
    of on-policyness perform similarly, except the more on-policy runs sometimes exhibit
    instability. This is in agreement with the results from the bandit setting above:
    when the peak in the reward function lies in highly likely regions under the reference
    policy, on-policy sampling has minor effect and more off-policy configurations
    of the algorithm can perform similarly too.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mode Length 场景下，其中每对偏好对的首选响应是数据集中最接近平均长度（203）的响应，通过调整采样频率来改变策略采样的程度，在 PPO
    中对代理奖励或真实奖励几乎没有影响（[图 6](#S5.F6 "在 5.1.1 要点 1：奖励模型中的策略采样改善性能 ‣ 5.1 问题 1：策略采样的作用
    ‣ 5 实证分析结果 ‣ 偏好微调 LLM 应利用次优的策略数据")）。我们对其他算法也有类似观察：[图 8](#S5.F8 "在 5.1.1 要点 1：奖励模型中的策略采样改善性能
    ‣ 5.1 问题 1：策略采样的作用 ‣ 5 实证分析结果 ‣ 偏好微调 LLM 应利用次优的策略数据")；Mode Length：不同的策略采样程度表现相似，但更接近策略的运行有时会表现出不稳定。这与上述老虎机设置中的结果一致：当奖励函数的峰值位于参考策略下的高概率区域时，策略采样的影响很小，而算法的更多离策略配置也能获得类似的表现。
- en: '![Refer to caption](img/e45086ce03467fcb1d29dc627b95f314.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e45086ce03467fcb1d29dc627b95f314.png)'
- en: 'Figure 7: On-policy sampling for PPO on the Skew Length scenario. Being more
    on-policy results in faster convergence and better performance. Left: average
    completion length (lower the better), and Right: proxy reward vs gradient steps.
    Being more on-policy results in better performance.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在 Skew Length 场景下 PPO 的策略采样。策略越接近，收敛速度越快，性能越好。左侧：平均完成长度（越低越好），右侧：代理奖励与梯度步数。策略越接近，性能越好。
- en: Finally, to evaluate the robustness of these findings under more challenging
    coverage conditions, we deliberately skew the length distribution in the preference
    dataset to make it distinct from the reference policy (called Skew Length). Concretely,
    with a 95% probability, we truncate the length of the response by sampling a length
    from an exponential distribution, which naturally leads to a shorter completion
    length. The remaining 5% of samples are drawn from the standard SFT policy to
    simulate the broader coverage for the preference data. Overall, the resulting
    data admits a significantly skewed distribution over response lengths, as visualized
    in [Figure 3](#S4.F3 "In 4.2 Tasks and Datasets ‣ 4 Research Questions and Analysis
    Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    Not only does the peak in the reward function now appear in less likely regions
    of the reference policy, but to succeed, an optimization algorithm must now do
    the required heavy lifting to shift the probability mass to the low-density regions
    of the response space that maximize reward.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了评估这些发现对更具挑战性的覆盖条件下的稳健性，我们故意在偏好数据集中扭曲长度分布，使其与参考策略不同（称为 Skew Length）。具体来说，我们以
    95% 的概率从指数分布中抽取长度来截断响应长度，这自然会导致较短的完成长度。剩余 5% 的样本则来自标准 SFT 策略，以模拟偏好数据的更广泛覆盖。总体而言，结果数据在响应长度上显著偏斜，如[图
    3](#S4.F3 "在 4.2 任务和数据集 ‣ 4 研究问题和分析设置 ‣ 偏好微调 LLM 应利用次优的策略数据")所示。现在奖励函数的峰值不仅出现在参考策略的低概率区域，为了成功，优化算法现在必须进行必要的重载，将概率质量转移到最大化奖励的低密度区域。
- en: '![Refer to caption](img/160778de12b6c18b293005f46921c218.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/160778de12b6c18b293005f46921c218.png)'
- en: 'Figure 8: Summary: effect of on-policy sampling on synthetic LLM problems.
    Average gold reward over the course of training for RWR, and REINFORCE with different
    $B$-axis is small), with performance differences largely due to instability.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：总结：策略采样对合成 LLM 问题的影响。训练过程中 RWR 和 REINFORCE 的平均黄金奖励在不同 $B$ 轴上变化较小，性能差异主要由于不稳定性。
- en: 'Our detailed results of running PPO in this setting are shown in [Figure 7](#S5.F7
    "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model Improves Performance
    ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    In this setting, we still find that more on-policy updates lead to a higher gold
    reward with PPO. In addition, we also observe much larger gaps in proxy reward
    values attained at any given gradient step compared to the Min Length scenario,
    in favor of on-policy sampling. For other algorithms, we also observe strong and
    clear trends supporting that on-policy sampling with a smaller but frequently
    sampled batch results in better performance as shown in the summary plot (see
    [Figure 8](#S5.F8 "In 5.1.1 Takeaway 1: On-Policy Sampling in the Reward Model
    Improves Performance ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"); Skew Length).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此设置下运行 PPO 的详细结果显示在 [图 7](#S5.F7 "在 5.1.1 主要结论 1：奖励模型中的策略采样改善了性能 ‣ 5.1 问题
    1：策略采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的策略数据")。在此设置下，我们仍然发现更多的策略更新导致 PPO 的黄金奖励更高。此外，与最小长度情境相比，我们还观察到在任何给定梯度步骤下，代理奖励值的差距明显更大，支持策略采样。对于其他算法，我们也观察到强烈且明确的趋势，支持小但频繁采样的批次的策略采样会带来更好的性能，正如总结图（见
    [图 8](#S5.F8 "在 5.1.1 主要结论 1：奖励模型中的策略采样改善了性能 ‣ 5.1 问题 1：策略采样的作用 ‣ 5 实证分析结果 ‣ LLM
    的偏好微调应利用次优的策略数据")；偏斜长度）所示。
- en: '![Refer to caption](img/0ec6c49be2b18da1badaeee35a03e1e2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0ec6c49be2b18da1badaeee35a03e1e2.png)'
- en: 'Figure 9: Effect of on-policy sampling on AlpacaFarm with a fixed mini-batch,
    but varying batch size $B$ makes updates more off-policy and this results in lower
    performance.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 AlpacaFarm 上策略采样的效果，固定的小批量，但批量大小 $B$ 的变化使得更新更偏离策略，这导致性能下降。
- en: 'Full-scale LLM problems. Finally, we evaluate if our insights transfer to the
    full-scale AlpacaFarm setup. We use a Pythia-1.4B model as our reference policy
    and generate two responses per prompt. We label the preferred and dispreferred
    responses with a gold reward model of human preferences from AlpacaFarm to construct
    a preference dataset. [Figure 9](#S5.F9 "In 5.1.1 Takeaway 1: On-Policy Sampling
    in the Reward Model Improves Performance ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data") shows that our intuitions from the simple
    bandit and synthetic LLM experiments transfer to this real preference learning
    task, as making updates on only on-policy samples leads to higher gold reward
    for both on-policy RWR and REINFORCE.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 全尺度 LLM 问题。最后，我们评估了我们的见解是否可以迁移到全尺度的 AlpacaFarm 设置中。我们使用 Pythia-1.4B 模型作为参考策略，并为每个提示生成两个响应。我们使用来自
    AlpacaFarm 的人类偏好黄金奖励模型对优选和非优选响应进行标记，以构建一个偏好数据集。[图 9](#S5.F9 "在 5.1.1 主要结论 1：奖励模型中的策略采样改善了性能
    ‣ 5.1 问题 1：策略采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的策略数据") 显示我们从简单的赌博机和合成 LLM 实验中的直觉可以迁移到这一真实的偏好学习任务，因为仅对策略样本进行更新会导致策略
    RWR 和 REINFORCE 的黄金奖励更高。
- en: '5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy Data'
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.1.2 主要结论 2：在策略样本重用可以实现利用离策略数据
- en: In the previous section, exactly one gradient step was taken on a given sample
    and we found that making updates on stale data was not helpful due to off-policy
    updates. Is there any scenario under which we can still attain good policy performance
    despite employing off-policy updates? In this section, we will answer this question,
    and show that it might be possible to learn with off-policy updates for some algorithms
    if we are allowed to make more than one update on a given sample. Of course, a
    substantial amount of sample reuse is detrimental since it would lead to more
    off-policy updates, thus leading to statistical or even propensity overfiting (Swaminathan
    and Joachims, [2015](#bib.bib46)) for some methods, but it is reasonable to surmise
    that some amount of sample reuse can help. To study sample reuse, we compare methods
    when $$T> gradient steps can be made on a given sample.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，对给定样本进行了一次梯度更新，我们发现由于 off-policy 更新，对陈旧数据进行更新并没有帮助。是否存在某种情况，即使采用 off-policy
    更新，我们仍能获得良好的策略性能？在本节中，我们将回答这个问题，并展示在允许对给定样本进行多次更新的情况下，某些算法可能仍能通过 off-policy 更新进行学习。当然，大量的样本重用是有害的，因为这会导致更多的
    off-policy 更新，从而导致一些方法的统计学或甚至倾向性过拟合（Swaminathan 和 Joachims，[2015](#bib.bib46)），但合理的推测是适量的样本重用可能有帮助。为了研究样本重用，我们比较了在给定样本上可以进行
    $$T>$$ 梯度步骤的方法。
- en: '![Refer to caption](img/8f53e4ea644f2f6902b20cef07b88e02.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f53e4ea644f2f6902b20cef07b88e02.png)'
- en: 'Figure 10: Effect of on-policy sample reuse on bandit problems. Reward vs gradient
    steps for a different number of inner iteration steps, $T$.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：On-Policy 样本重用对 bandit 问题的影响。不同内循环步骤数 $T$ 的奖励与梯度步骤。
- en: 'We study sample reuse for on-policy RWR in the bandit setting in [Figure 10](#S5.F10
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
    While increasing $T$; $T=10$).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [图 10](#S5.F10 "在 5.1.2 关键点 2：On-Policy 样本重用可以利用 off-policy 数据 ‣ 5.1 问题
    1：On-Policy 采样的角色 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的 On-Policy 数据") 中研究了在 bandit 设置下的
    on-policy RWR 的样本重用。随着 $T$ 的增加；$T=10$）。
- en: 'Synthetic LLM problems. We also evaluate the effect of sample reuse on synthetic
    LLM problems. In this case, we study two algorithms PPO and on-policy best-of-N
    to be able to understand the effect of sample reuse on multiple algorithms. In
    contrast to the performance degradation with off-policy updates induced due to
    stale samples in PPO, we find that off-policy updates induced due to sample reuse
    do not hurt performance ([Figure 11](#S5.F11 "In 5.1.2 Takeaway 2: On-Policy Sample
    Reuse Can Enable Leveraging Off-Policy Data ‣ 5.1 Question 1: The Role of On-Policy
    Sampling ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"); PPO), with even $T=8$ to $2$, i.e., performing
    two gradient updates on each sample improves the golden reward for best-of-N ([Figure 11](#S5.F11
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data");
    Best-of-N) within a given data sampling budget.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 合成 LLM 问题。我们还评估了样本重用对合成 LLM 问题的影响。在这种情况下，我们研究了两个算法 PPO 和 on-policy best-of-N，以便理解样本重用对多种算法的影响。与
    PPO 中由于陈旧样本导致的 off-policy 更新性能退化相比，我们发现由于样本重用引起的 off-policy 更新并不会损害性能（[图 11](#S5.F11
    "在 5.1.2 关键点 2：On-Policy 样本重用可以利用 off-policy 数据 ‣ 5.1 问题 1：On-Policy 采样的角色 ‣ 5
    实证分析结果 ‣ LLM 的偏好微调应利用次优的 On-Policy 数据")；PPO），甚至 $T=8$ 到 $2$，即对每个样本进行两次梯度更新会提升
    best-of-N 的黄金奖励（[图 11](#S5.F11 "在 5.1.2 关键点 2：On-Policy 样本重用可以利用 off-policy 数据
    ‣ 5.1 问题 1：On-Policy 采样的角色 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的 On-Policy 数据")；Best-of-N），在给定的数据采样预算内。
- en: '![Refer to caption](img/31f9c4fb0f84f4f16a20af4c6770d37d.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31f9c4fb0f84f4f16a20af4c6770d37d.png)'
- en: 'Figure 11: Effect of on-policy sample reuse in the Min Length scenario. Average
    completion length (i.e., the lower the better) vs gradient steps for a different
    number of inner iteration steps, $T$ implies that the algorithm is more off-policy.
    Observe that some sample reuse can improve sample efficiency (T = 2 outperforms
    T = 1), but excessive sample reuse can hurt performance. Also note that algorithms
    with mechanisms to control off-policy updates such as PPO are suited to perform
    better in the off-policy sample reuse setting.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：在最小长度场景中的政策样本重用效果。不同内迭代步骤数的平均完成长度（即越低越好）与梯度步骤的关系，$T$ 表示算法更加偏离政策。观察到一些样本重用可以提高样本效率（T
    = 2 优于 T = 1），但过度的样本重用会损害性能。还注意到，具有控制离政策更新机制的算法，如PPO，更适合在离政策样本重用设置中表现更好。
- en: Why do PPO and best-of-N respond differently to sample reuse? We believe that
    this is because PPO employs an off-policy correction, and hence, significantly
    off-policy samples do not contribute to the gradient, addressing the well-known
    challenge of propensity overfitting (Swaminathan and Joachims, [2015](#bib.bib46)).
    This is not the case with on-policy best-of-N, where excessive sample reuse can
    hurt exploration, because training on old samples with a log-likelihood loss push
    the current policy to be close to the stale data-generating policy. That said,
    more than one gradient step can still be useful when presented with a fixed data
    budget, unless it bottlenecks exploration of high reward regions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么PPO和最佳-N对样本重用的响应不同？我们认为这是因为PPO使用了离政策修正，因此，显著的离政策样本不会对梯度产生贡献，解决了著名的倾向过拟合挑战（Swaminathan和Joachims，[2015](#bib.bib46)）。这对于在政策最佳-N的情况则不然，过度的样本重用会损害探索，因为使用带有对数似然损失的旧样本进行训练会使当前政策靠近过时的数据生成政策。也就是说，当面对固定的数据预算时，多个梯度步骤仍然可能有用，除非它瓶颈了对高奖励区域的探索。
- en: Takeaways
    for on-policy sampling On-policy sampling generally
    improves performance and efficiency, especially in cases when the peak of reward
    appears farther from the reference policy, even when the reward model is learned
    from the same preference dataset that methods without on-policy learning also
    use. In some cases, sample reuse can reduce the dependency on on-policy sampling
    of data, but it presents a tradeoff by reducing the exploration of the response
    space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Takeaways
    for on-policy sampling On-policy sampling generally
    improves performance and efficiency, especially in cases when the peak of reward
    appears farther from the reference policy, even when the reward model is learned
    from the same preference dataset that methods without on-policy learning also
    use. In some cases, sample reuse can reduce the dependency on on-policy sampling
    of data, but it presents a tradeoff by reducing the exploration of the response
    space.
- en: '5.2 Question 2: The Role of Negative Gradient'
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2 问题 2：负梯度的作用
- en: To understand the role of negative gradient, we will compare contrastive algorithms
    such as DPO and IPO with maximum likelihood methods such as RWR (or Pref-FT, which
    attempts to increase the likelihood of the preferred response only) and best-of-N
    in a fully offline setting, where no new on-policy samples are used. We will also
    aim to understand the mechanisms behind these methods.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解负梯度的作用，我们将比较对比性算法，如DPO和IPO，与最大似然方法，如RWR（或Pref-FT，仅尝试增加首选响应的可能性）以及最佳-N，在完全离线的环境中，其中不使用新的在政策样本。我们还将旨在理解这些方法背后的机制。
- en: '5.2.1 Takeaway 1: Negative Gradient Enables Faster Convergence Amongst Offline
    Methods'
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.1 收获 1：负梯度使离线方法之间的收敛更快
- en: '![Refer to caption](img/9e149bc4cf2a558e8b93cab24a4622b4.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9e149bc4cf2a558e8b93cab24a4622b4.png)'
- en: 'Figure 12: Negative gradients on the didactic bandit problems. Average reward
    during training and the KL-reward trade-off for four algorithms in the fully offline
    setting: best-of-N (no negative gradient), RWR (no negative gradient), best-of-N
    + an explicit negative gradient on dispreferred actions, and IPO (with negative
    gradient). Negative gradient helps find a better policy by aggressively pushing
    down the likelihood of bad actions, and this leads to larger KL values.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：在教学性赌博问题上的负梯度。四种算法在完全离线设置中的平均奖励和KL-奖励权衡：最佳-N（没有负梯度）、RWR（没有负梯度）、最佳-N + 对不喜欢动作的显式负梯度，以及IPO（有负梯度）。负梯度通过积极降低不良动作的可能性来帮助找到更好的策略，这导致了更大的KL值。
- en: 'We begin by comparing a representative set of offline algorithms on the didactic
    bandit problem. These methods include those that do not use a contrastive update
    on the didactic bandit problem, namely offline supervised approaches, Best-of-N
    and offline RWR, and offline IPO (Gheshlaghi Azar et al., [2023](#bib.bib18)),
    a representative offline fine-tuning method which uses a contrastive negative
    gradient term. We also consider a variant of best-of-N where we explicitly add
    a term to the loss function that attempts to minimize the likelihood of the dispreferred
    response akin to unlikelihood (Welleck et al., [2020](#bib.bib50)) (see [Section G.2](#A7.SS2
    "G.2 Algorithmic Details ‣ Appendix G More on Didactic Bandit Problems ‣ Appendices
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    for more details). In [Figure 12](#S5.F12 "In 5.2.1 Takeaway 1: Negative Gradient
    Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role
    of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"), we find that IPO and best-of-N
    + negative gradient learn a better policy from an offline dataset collected from
    sub-optimal $\pi_{\mathrm{ref}}$ are far away from each other). While best-of-N
    attains a higher reward when the reward function is given by $\mathbf{R}_{2}$,
    it still underperforms IPO. We suspect that this is because maximizing likelihood
    on some responses alone is not enough to steer the learned policy away meaningfully
    away from $\pi_{\mathrm{ref}}$ and $r^{*}$. This is possibly due to the much smaller
    space of possible tokens and responses, where maximum likelihood methods perform
    well enough.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在教学型强盗问题上比较了一组代表性的离线算法。这些方法包括那些在教学型强盗问题上不使用对比更新的方法，即离线监督方法、Best-of-N和离线RWR，以及离线IPO（Gheshlaghi
    Azar等，[2023](#bib.bib18)），这是一种使用对比负梯度项的代表性离线微调方法。我们还考虑了一种Best-of-N的变体，在这种变体中，我们显式地在损失函数中添加一个项，试图最小化不受欢迎的响应的可能性，类似于不可能性（Welleck等，[2020](#bib.bib50)）（更多细节请参见[第G.2节](#A7.SS2
    "G.2 算法细节 ‣ 附录G 更多关于教学型强盗问题 ‣ 附录 ‣ LLM的偏好微调应利用次优的、按政策的数据")）。在[图12](#S5.F12 "在5.2.1
    小结1：负梯度使离线方法更快收敛 ‣ 5.2 问题2：负梯度的作用 ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的、按政策的数据")中，我们发现IPO和Best-of-N
    + 负梯度从离线数据集中学习到的策略彼此相距较远（从次优$\pi_{\mathrm{ref}}$收集）。尽管Best-of-N在奖励函数由$\mathbf{R}_{2}$给出时获得了更高的奖励，但它仍然不如IPO表现好。我们怀疑这是因为仅仅最大化某些响应的可能性不足以有效地将学习到的策略远离$\pi_{\mathrm{ref}}$和$r^{*}$。这可能是由于可能的标记和响应的空间较小，其中最大似然方法表现得足够好。
- en: '![Refer to caption](img/6fd1d65766684761d02c524c2368478f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6fd1d65766684761d02c524c2368478f.png)'
- en: 'Figure 13: Negative gradients in synthetic LLM problems. Completion length
    (inverse of the true reward) for three offline algorithms. DPO outperforms Pref-FT
    and offline RWR in Min Length and the Skew Length settings, where the peak in
    $r^{*}$ are misaligned. For the Mode Length setting, all of the algorithms perform
    similarly.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：合成LLM问题中的负梯度。三种离线算法的完成长度（真实奖励的倒数）。在最小长度和偏斜长度设置中，DPO在$r^{*}$的峰值未对齐的情况下优于Pref-FT和离线RWR。在模式长度设置中，所有算法表现相似。
- en: 'Synthetic LLM problems. Our experiments in the synthetic LLM setting corroborate
    this finding. Here we compare Pref-FT with DPO (with negative gradients). In the
    Min Length setting, we find in [Figure 13](#S5.F13 "In 5.2.1 Takeaway 1: Negative
    Gradient Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2:
    The Role of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") that DPO significantly outperforms
    Pref-FT. On the other hand, when the peak in the ground-truth reward appears in
    high-likely regions of the reference policy and the preference data $\mathcal{D}_{\text{pref}}$
    is covered by the preference dataset $\mathcal{D}_{\text{pref}}$, we also find
    that DPO is much more effective in driving the policy further from the reference
    initialization and outperforms Pref-FT.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 合成LLM问题。我们在合成LLM设置中的实验证实了这一发现。在这里，我们将Pref-FT与DPO（具有负梯度）进行比较。在最小长度设置中，我们在[图13](#S5.F13
    "在5.2.1 小结1：负梯度使离线方法更快收敛 ‣ 5.2 问题2：负梯度的作用 ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的、按政策的数据")中发现，DPO显著优于Pref-FT。另一方面，当真实奖励的峰值出现在参考策略的高可能区域，并且偏好数据$\mathcal{D}_{\text{pref}}$被偏好数据集$\mathcal{D}_{\text{pref}}$覆盖时，我们也发现DPO在将策略从参考初始化位置进一步驱动方面更为有效，并且优于Pref-FT。
- en: '![Refer to caption](img/602d77636f569e9dc05989ccff47aab1.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/602d77636f569e9dc05989ccff47aab1.png)'
- en: 'Figure 14: Negative gradients in AlpacaFarm (left) and UltraFeedback (right)
    for offline methods. We plot the increase in average gold reward compared to the
    reference model for different offline approaches. Algorithms with a negative gradient
    such as DPO outperform approaches such as Pref-FT not utilizing any negative gradient
    term.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：离线方法中AlpacaFarm（左）和UltraFeedback（右）的负梯度。我们绘制了与参考模型相比，不同离线方法在平均金奖奖励上的增加。具有负梯度的算法（如DPO）在性能上优于不利用任何负梯度项的方法（如Pref-FT）。
- en: Full-scale LLM fine-tuning. Finally, we compare supervised Pref-FT and contrastive
    DPO when fine-tuning on actual preference data. In addition to AlpacaFarm, we
    also run experiments using the Ultra-Feedback (Ding et al., [2023](#bib.bib12))
    dataset. For the Ultra-Feedback dataset, we use different models (GPT-3.5, GPT-4)
    to generate responses to various prompts. The resulting dataset has a broader
    preference dataset distribution than $\pi_{\mathrm{ref}}$compared to methods that
    do not utilize a negative gradient (e.g., Pref-FT).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 全规模LLM微调。最后，我们比较了在实际偏好数据上微调的监督式Pref-FT和对比式DPO。除了AlpacaFarm，我们还使用了Ultra-Feedback （Ding等，[2023](#bib.bib12)）数据集进行实验。对于Ultra-Feedback数据集，我们使用不同的模型（GPT-3.5，GPT-4）生成各种提示的响应。生成的数据集比不利用负梯度的方法（例如Pref-FT）的$\pi_{\mathrm{ref}}$具有更广泛的偏好数据分布。
- en: '5.2.2 Takeaway 2: Mechanisms Explaining the Behavior of the Negative Gradient'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 5.2.2 要点2：解释负梯度行为的机制
- en: Having seen that using a negative gradient leads to much better performance,
    we next attempt to understand the mechanism behind this better performance. To
    do so, we visualize the evolution of the log-likelihoods of the preferred response
    and the dispreferred response in a held-out dataset as multiple gradient steps
    are taken on an offline preference optimization loss.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 既然使用负梯度可以带来更好的性能，我们接下来尝试理解这种更好性能背后的机制。为此，我们可视化了在离线偏好优化损失上进行多次梯度步骤时，偏好响应和非偏好响应的对数可能性的演变。
- en: 'Contrastive training increases the gap between the likelihoods of preferred
    and dispreferred responses. Perhaps as expected, we find that DPO-style contrastive
    training is more effective at increasing the gap between the likelihoods of preferred
    and dispreferred responses compared to offline Pref-FT in several LLM settings:
    the synthetic LLM settings with Min Length and Skew Length, and full-scale AlpacaFarm
    and UltraFeedback settings ([Figure 15](#S5.F15 "In 5.2.2 Takeaway 2: Mechanisms
    Explaining the Behavior of the Negative Gradient ‣ 5.2 Question 2: The Role of
    Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data")). More concretely, note that the
    margin for Pref-FT largely converges to 0, whereas offline DPO can enable a larger
    margin.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对比训练增加了偏好和非偏好响应之间的可能性差距。或许正如预期，我们发现DPO风格的对比训练在多个LLM设置中比离线Pref-FT更有效地增加了偏好和非偏好响应之间的差距：包括具有最小长度和偏斜长度的合成LLM设置，以及全规模的AlpacaFarm和UltraFeedback设置（[图15](#S5.F15
    "在5.2.2要点2：解释负梯度行为的机制 ‣ 5.2 问题2：负梯度的作用 ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的在线数据")）。更具体地说，注意到Pref-FT的边际大致收敛到0，而离线DPO则可以实现更大的边际。
- en: '![Refer to caption](img/317735ad905e3047a71e6ab0a88b358c.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/317735ad905e3047a71e6ab0a88b358c.png)'
- en: 'Figure 15: Difference in likelihoods of preferred and dispreferred responses.
    DPO increases the log probability margin $\log\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})-\log\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})$
    more compared to non-contrastive methods such as Pref-FT.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：偏好和非偏好响应之间的可能性差异。与Pref-FT等非对比方法相比，DPO在$\log\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})-\log\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})$的对数概率边际上有更大的提升。
- en: '![Refer to caption](img/e63af5bd1ce6c3cd9a08cb3c150872b9.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e63af5bd1ce6c3cd9a08cb3c150872b9.png)'
- en: 'Figure 16: DPO implicit reward during training. We observe that with fewer
    prompts, contrastive methods can increase the implicit reward, $r_{\theta}(\mathbf{x},\mathbf{y})=\log\left(\pi_{\theta}(\mathbf{y}|\mathbf{x})\right)-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})$,
    of the preferred response while reducing this quantity for the dispreferred response,
    however as the number of data points grows, this may not be possible and the likelihood
    of both positives and negatives might reduce.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：训练过程中DPO的隐式奖励。我们观察到，使用较少的提示时，对比方法可以增加所偏好的响应的隐式奖励$r_{\theta}(\mathbf{x},\mathbf{y})=\log\left(\pi_{\theta}(\mathbf{y}|\mathbf{x})\right)-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})$，同时减少不偏好的响应的这一数量，但随着数据点数量的增加，这可能不可行，正负样本的可能性可能减少。
- en: Changes in log likelihoods depend on model capacity, reference initialization,
    data size, and composition. The natural next question is if DPO-like objectives
    use the probability mass recovered by increasing the reward margin between $\mathbf{y}_{w}$
    in the dataset, then induced rewards will always decrease. This does not contradict
    our findings because this condition is not satisfied in typical fine-tuning pipelines
    where *both* $\mathbf{y}_{w}$ is obtained by first running supervised Pref-FT
    only on $\mathbf{y}_{w}$ and $\mathbf{y}_{l}$ on the bandit problem while varying
    the size of the preference dataset. Following standard protocols, both $\mathbf{y}_{l}$
    while reducing the likelihood of $\mathbf{y}_{l}$, dataset size, and composition,
    contrastive objectives such as DPO extrapolate, and this extrapolation might produce
    good or bad responses.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然的变化依赖于模型容量、参考初始化、数据规模和组成。自然的下一个问题是，如果DPO类似的目标使用通过增加数据集中$\mathbf{y}_{w}$之间的奖励边际恢复的概率质量，那么诱发的奖励将始终减少。这并不与我们的发现相矛盾，因为在典型的微调管道中这个条件没有得到满足，其中*两者*的$\mathbf{y}_{w}$是通过首先在$\mathbf{y}_{w}$上运行监督性Pref-FT和在带子问题上对$\mathbf{y}_{l}$进行的，同时变化偏好数据集的大小。根据标准协议，在减少$\mathbf{y}_{l}$的可能性、数据集大小和组成的同时，对比目标如DPO进行外推，这种外推可能产生好的或坏的响应。
- en: 'We also observe a similar trend in full-scale LLM experiments in [Figure 17](#S5.F17
    "In 5.2.2 Takeaway 2: Mechanisms Explaining the Behavior of the Negative Gradient
    ‣ 5.2 Question 2: The Role of Negative Gradient ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"):
    we observe a decrease in the log-likelihoods of both the preferred and dispreferred
    responses throughout training on AlpacaFarm with small 1.4B Pythia policies. However,
    using a Mistral7B model to train a policy on the UltraFeedback dataset results
    in an increasing value of log-likelihood of $\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})$.
    In contrast, perhaps as expected, running Pref-FT increases the likelihoods of
    both $\mathbf{y}_{w}$ ([Figure 17](#S5.F17 "In 5.2.2 Takeaway 2: Mechanisms Explaining
    the Behavior of the Negative Gradient ‣ 5.2 Question 2: The Role of Negative Gradient
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data")).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在全规模LLM实验中也观察到了类似的趋势，在[图17](#S5.F17 "在5.2.2要点2：解释负梯度行为的机制 ‣ 5.2问题2：负梯度的作用
    ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的在政策数据")中：我们观察到在AlpacaFarm上使用小型1.4B Pythia策略的训练过程中，所偏好的和不偏好的响应的对数似然都在减少。然而，使用Mistral7B模型在UltraFeedback数据集上训练策略时，$\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})$的对数似然值增加。相比之下，可能如预期的那样，运行Pref-FT增加了两者的可能性（[图17](#S5.F17
    "在5.2.2要点2：解释负梯度行为的机制 ‣ 5.2问题2：负梯度的作用 ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的在政策数据")）。
- en: '![Refer to caption](img/f36dca2af44e9882a9cad842c4cc23ee.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/f36dca2af44e9882a9cad842c4cc23ee.png)'
- en: 'Figure 17: DPO reward estimates for Pref-FT and DPO on AlpacaFarm and UltraFeedback.
    For a Pythia-1.4B model trained on AlpacaFarm, DPO decreases the implicit reward,
    $r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}|\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\right]$
    and decrease the reward for $\mathbf{y}_{l}$, whereas Pref-FT increases both.
    In both cases, DPO leads to a higher margin than Pref-FT.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：在AlpacaFarm和UltraFeedback上对Pref-FT和DPO的奖励估计。对于在AlpacaFarm上训练的Pythia-1.4B模型，DPO降低了隐式奖励$r_{\theta}(\mathbf{x},\mathbf{y})=\beta\left[\log\pi_{\theta}(\mathbf{y}|\mathbf{x})-\log\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\right]$和$\mathbf{y}_{l}$的奖励，而Pref-FT则增加了这两者。在这两种情况下，DPO的边际比Pref-FT高。
- en: Takeaways
    for negative gradients A negative gradient improves
    over offline supervised methods when the peak in the reward appears in less likely
    regions of $\pi_{\mathrm{ref}}$, model capacity is large, and $\pi_{\mathrm{ref}}$.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Takeaways
    for negative gradients A negative gradient improves
    over offline supervised methods when the peak in the reward appears in less likely
    regions of $\pi_{\mathrm{ref}}$, model capacity is large, and $\pi_{\mathrm{ref}}$.
- en: '5.3 Question 3: On-Policy Sampling and Negative Gradients are Complementary'
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3 问题3：在政策采样和负梯度是互补的
- en: 'Based on our findings that both on-policy sampling and negative gradients are
    independently effective, we now study if combining them would provide any additional
    benefits. To understand this, we empirically study a straightforward on-policy
    variant of DPO/IPO: instead of utilizing the PPO or Best-of-N objective on on-policy
    samples, for each prompt $\mathbf{x}$, and construct preference pairs by taking
    the higher reward completion as the preferred one and lower reward completion
    as the dispreferred one. This recipe is similar to concurrent works such as Rosset
    et al. ([2024](#bib.bib40)). Then we calculate the DPO/IPO loss on this preference
    dataset and update our model accordingly.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们发现在线采样和负梯度都独立有效，我们现在研究它们的结合是否会提供额外的益处。为了理解这一点，我们实证研究了一种直接的 DPO/IPO 在线变体：对每个提示
    $\mathbf{x}$，而不是在在线样本上使用 PPO 或 Best-of-N 目标，我们通过将更高奖励的完成结果作为偏好的完成结果，将较低奖励的完成结果作为不偏好的完成结果来构建偏好对。这种方法类似于
    Rosset 等人（[2024](#bib.bib40)）的并行工作。然后我们计算这个偏好数据集上的 DPO/IPO 损失，并相应地更新我们的模型。
- en: '![Refer to caption](img/6a50055e0669386e2cfc016f8e0996da.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a50055e0669386e2cfc016f8e0996da.png)'
- en: 'Figure 18: On-policy sampling + negative gradients in bandit setup. Complimentary
    benefit of on-policy sampling and negative gradients. Online IPO (using both on-policy
    sampling and negative gradients) performs better than offline IPO (negative gradients
    but no on-policy sampling) and RWR (on-policy sampling but no negative gradients).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：在老虎机设置中的在线采样 + 负梯度。在线采样和负梯度的互补效益。在线 IPO（使用在线采样和负梯度）比离线 IPO（负梯度但没有在线采样）和
    RWR（在线采样但没有负梯度）表现更好。
- en: 'Performance on bandit and synthetic LLM problems. [Figure 18](#S5.F18 "In 5.3
    Question 3: On-Policy Sampling and Negative Gradients are Complementary ‣ 5 Empirical
    Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data") shows that the on-policy version of IPO achieves both faster
    convergence and better performance compared to the offline version, for both $\mathbf{R}_{1}$
    in the didactic bandit problem. We also ran on-policy DPO in synthetic LLM problems
    we studied and found it to converge significantly faster and to a better solution
    than offline DPO, on-policy RL, and on-policy variants of supervised learning
    approaches as shown in [Figure 19](#S5.F19 "In 5.3 Question 3: On-Policy Sampling
    and Negative Gradients are Complementary ‣ 5 Empirical Analysis Results ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"). We also find
    that on-policy versions of contrastive approaches exhibit favorable computational
    vs wall-clock time tradeoffs compared to purely on-policy RL methods and even
    offline contrastive methods that may not find as good solutions as their on-policy
    counterparts (see [Appendix B](#A2 "Appendix B Computational vs Wall-Clock Time
    Tradeoff for Various Methods ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data")).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在老虎机和合成 LLM 问题上的表现。[图 18](#S5.F18 "在 5.3 问题 3：在线采样和负梯度是互补的 ‣ 5 实证分析结果 ‣ LLM
    的偏好微调应利用次优的在线数据") 显示，在教学老虎机问题中，IPO 的在线版本在 $\mathbf{R}_{1}$ 上实现了比离线版本更快的收敛速度和更好的性能。我们还在我们研究的合成
    LLM 问题中运行了在线 DPO，并发现它比离线 DPO、在线 RL 和在线变体的监督学习方法收敛速度显著更快且效果更好，如 [图 19](#S5.F19
    "在 5.3 问题 3：在线采样和负梯度是互补的 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的在线数据") 所示。我们还发现，与纯粹的在线 RL
    方法相比，在线对比方法在计算与实时时间的权衡方面表现出有利的特性，甚至比可能无法找到与在线方法相匹配的解决方案的离线对比方法更好（见 [附录 B](#A2
    "附录 B 各种方法的计算与实时时间权衡 ‣ 附录 ‣ LLM 的偏好微调应利用次优的在线数据")）。
- en: '![Refer to caption](img/0b6f119930c63ad1d9f2a91af4972db1.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0b6f119930c63ad1d9f2a91af4972db1.png)'
- en: 'Figure 19: On-policy sampling + negative gradients in LLM length experiments.
    Complimentary benefit of on-policy sampling and negative gradients on the synthetic
    LLM length experiments. On-policy DPO performs the best where optimal policy and
    reference policy lies far from each other (min length and skew length), and all
    algorithms perform similarly when these two policies are close (mode length).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：在 LLM 长度实验中的在线采样 + 负梯度。在线采样和负梯度在合成 LLM 长度实验中的互补效益。在线 DPO 在最优策略和参考策略相距较远（最小长度和偏斜长度）时表现最佳，而当这两种策略接近（模式长度）时，所有算法的表现相似。
- en: 'Why can on-policy versions of contrastive methods perform better than on-policy
    RL? We saw in [Section 5.2.1](#S5.SS2.SSS1 "5.2.1 Takeaway 1: Negative Gradient
    Enables Faster Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role
    of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data") that offline contrastive training
    with a negative gradient was effective at quickly reorganizing probability mass
    to high-reward responses covered by the preference data. When combined with on-policy
    sampling, this behavior results in faster convergence: for any given batch of
    on-policy data, contrastive training with a negative gradient can quickly reconfigure
    the policy distribution within the support of the on-policy data obtained thus
    far (i.e., it provides a stronger, low-variance learning signal). Similarly to
    how best-of-N + negative gradient outperforms vanilla best-of-N but underperforms
    DPO in [Figure 12](#S5.F12 "In 5.2.1 Takeaway 1: Negative Gradient Enables Faster
    Convergence Amongst Offline Methods ‣ 5.2 Question 2: The Role of Negative Gradient
    ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"), PPO also improves over RWR without a negative gradient
    term (in the bandit setting this corresponds to a better reward-KL tradeoff in
    [Figure 18](#S5.F18 "In 5.3 Question 3: On-Policy Sampling and Negative Gradients
    are Complementary ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs
    Should Leverage Suboptimal, On-Policy Data") and in the synthetic LLM setting
    this appears in final performance), but it is still unable to match on-policy
    DPO in [Figure 19](#S5.F19 "In 5.3 Question 3: On-Policy Sampling and Negative
    Gradients are Complementary ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data"). Note that this does not
    mean that on-policy DPO would always outperform PPO, but that it might be a good
    choice for users to experiment with on-policy versions of contrastive methods.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么对比方法的策略内版本比策略内强化学习表现更好？我们在[第5.2.1节](#S5.SS2.SSS1 "5.2.1 关键点 1: 负梯度在离线方法中实现了更快的收敛
    ‣ 5.2 问题 2: 负梯度的作用 ‣ 5 实证分析结果 ‣ 偏好微调 LLMs 应利用次优的策略内数据")中看到，带有负梯度的离线对比训练有效地将概率质量迅速重新组织到由偏好数据覆盖的高奖励响应中。与策略内采样结合时，这种行为会导致更快的收敛：对于任何给定的策略内数据批次，带有负梯度的对比训练可以快速重新配置政策分布在迄今获得的策略内数据的支持范围内（即，它提供了一个更强的、低方差的学习信号）。类似于[图12](#S5.F12
    "在5.2.1节关键点1: 负梯度在离线方法中实现了更快的收敛 ‣ 5.2 问题2: 负梯度的作用 ‣ 5 实证分析结果 ‣ 偏好微调LLMs应利用次优的策略内数据")中最佳的N
    + 负梯度优于普通的最佳N，但表现逊色于DPO，PPO在没有负梯度项的RWR上也有所改善（在赌博设置中，这对应于[图18](#S5.F18 "在5.3问题3:
    策略内采样和负梯度是互补的 ‣ 5 实证分析结果 ‣ 偏好微调LLMs应利用次优的策略内数据")中的更好的奖励-KL权衡，而在合成LLM设置中这体现在最终表现上），但仍无法匹敌[图19](#S5.F19
    "在5.3问题3: 策略内采样和负梯度是互补的 ‣ 5 实证分析结果 ‣ 偏好微调LLMs应利用次优的策略内数据")中的策略内DPO。需要注意的是，这并不意味着策略内DPO总是优于PPO，而是它可能是用户尝试策略内对比方法的一个不错选择。'
- en: Takeaways
    for on-policy sampling + negative gradient On-policy
    sampling and offline negative gradients present complementary benefits, in that
    the best offline loss function with negative gradients can be used to train on
    on-policy data, improving over on-policy RL or supervised learning. Conceptually,
    while sampling responses on policy provides coverage of the response space, an
    effective negative gradient loss provides a stronger learning signal given a set
    of samples. It can also result in computational benefits.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Takeaways
    for on-policy sampling + negative gradient On-policy
    sampling and offline negative gradients present complementary benefits, in that
    the best offline loss function with negative gradients can be used to train on
    on-policy data, improving over on-policy RL or supervised learning. Conceptually,
    while sampling responses on policy provides coverage of the response space, an
    effective negative gradient loss provides a stronger learning signal given a set
    of samples. It can also result in computational benefits.
- en: 6 Conceptual Unification and Theoretical Analysis
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 概念统一与理论分析
- en: With empirical results showing the benefits of on-policy sampling and negative
    gradient for preference fine-tuning of LLMs, in this section, we attempt to conceptually
    understand the benefits by building a mental model. In this section, we will first
    unify these seemingly distinct notions of on-policy sampling and negative gradient
    into a unified notion of mode-seeking objectives, and contrast them against mode-covering
    maximum likelihood objectives. Then, we will contrast the learning dynamics of
    the reverse KL-divergence, a representative mode-seeking objective against the
    mode-seeking forward KL-divergence (i.e., the supervised learning loss) to intuitively
    explain some of our findings.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 随着实证结果显示策略内采样和负梯度在LLMs的偏好微调中的好处，在本节中，我们试图通过建立一个心理模型来概念性地理解这些好处。在本节中，我们将首先将这些看似不同的策略内采样和负梯度概念统一为一个统一的模式寻求目标，并与模式覆盖最大似然目标进行对比。然后，我们将对比反向KL散度作为一种代表性的模式寻求目标与前向KL散度（即监督学习损失）的学习动态，以直观地解释我们的部分发现。
- en: 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1 寻找模式统一了策略内采样和负梯度
- en: In this section, we will show that the notion of mode-seeking divergences unifies
    on-policy sampling and negative gradients for the various objectives we investigated
    in the paper. Specifically, we show below that several on-policy RL methods that
    we studied optimize the reverse KL-divergence, and are hence mode-seeking, offline
    contrastive methods that employ a negative gradient are also mode-seeking, and
    finally, supervised weighted maximum likelihood approaches (e.g., offline Best-of-N,
    Pref-FT, Binary FeedMe) are mode-covering. First, we show that on-policy sampling
    leads to mode-seeking behavior. To do this, we prove that RL and supervised objectives
    combined on-policy sampling optimize the reverse KL divergence, which is known
    to be mode-seeking.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将展示模式寻求分歧的概念如何统一了我们在论文中研究的各种目标的政策采样和负梯度。具体来说，我们将展示几个我们研究的政策 RL 方法如何优化反向KL散度，因此是模式寻求的，使用负梯度的离线对比方法也同样是模式寻求的，最后，监督加权最大似然方法（例如，离线
    Best-of-N、Pref-FT、Binary FeedMe）是模式覆盖的。首先，我们展示了政策采样导致了模式寻求行为。为此，我们证明了 RL 和监督目标结合的政策采样优化了反向
    KL 散度，而反向 KL 散度已知是模式寻求的。
- en: Lemma 6.1.
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 6.1。
- en: On-policy RL and on-policy weighted-likelihood methods optimize a regularized
    version of a reverse KL-divergence with respect to the optimal policy and are
    hence mode seeking.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 政策 RL 和政策加权似然方法优化了相对于最优政策的反向 KL 散度的正则化版本，因此是模式寻求的。
- en: A proof for Lemma [6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is shown in [Section C.1.1](#A3.SS1.SSS1 "C.1.1 On-policy Methods Are Mode-Seeking
    ‣ C.1 Unifying On-Policy Sampling and Negative Gradients via Mode-Seeking Divergences
    ‣ Appendix C More Details on Conceptual Unification and Theoretical Analysis ‣
    Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). Next, we show that offline contrastive methods that employ a negative
    gradient are also mode-seeking. While these approaches do not optimize the reverse
    KL-divergence, we can still show that the probability mass obtained by minimizing
    density on negative responses $\mathbf{y}_{l}$) compared to other categories.
    When the offline dataset consists of multiple high-reward categories, this preference
    to put more probability mass on the mode of the current policy results in mode-seeking
    behavior, compared to increasing probability mass on all high-reward categories.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于引理 [6.1](#S6.Thmtheorem1 "引理 6.1\. ‣ 6.1 寻找模式统一了政策采样和负梯度 ‣ 6 概念统一和理论分析 ‣ LLM的偏好微调应利用次优的政策数据")
    的证明见于 [第 C.1.1 节](#A3.SS1.SSS1 "C.1.1 政策方法是模式寻求的 ‣ C.1 通过模式寻求分歧统一政策采样和负梯度 ‣ 附录
    C 概念统一和理论分析的更多细节 ‣ 附录 ‣ LLM 的偏好微调应利用次优的政策数据")。接下来，我们展示了使用负梯度的离线对比方法也同样是模式寻求的。尽管这些方法没有优化反向KL散度，我们仍然可以展示通过最小化负响应
    $\mathbf{y}_{l}$ 上的密度获得的概率质量。与其他类别相比，当离线数据集包含多个高奖励类别时，相比于增加所有高奖励类别的概率质量，这种将更多概率质量放在当前政策模式上的偏好导致了模式寻求行为。
- en: Lemma 6.2.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 6.2。
- en: 'Let $\theta_{t}$. Consider contrastive approaches that induce a negative gradient
    under a functional form shown below:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\theta_{t}$。考虑下述功能形式下诱导负梯度的对比方法：
- en: '|  | $1$2 |  | (6.1) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6.1) |'
- en: 'where $c_{1}$. In contrast, weighted maximum likelihood without the negative
    gradient sets $c_{2}=0$, there always exists an appropriate dataset of positive
    and negative samples $\mathcal{D}$, such that:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $c_{1}$ 的情况下。相比之下，不包含负梯度的加权最大似然方法将 $c_{2}=0$，总是存在一个适当的正负样本数据集 $\mathcal{D}$，使得：
- en: '|  | $1$2 |  | (6.2) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6.2) |'
- en: 'In addition, if the model class $\pi_{\theta}$ that satisfies this condition):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果模型类 $\pi_{\theta}$ 满足这一条件：
- en: '|  | $1$2 |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'then, we find that the likelihood of positives is larger (and similarly likelihood
    of negatives is smaller) when 
    |  | (6.3) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\mathbb{E}_{\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l}\sim\mathcal{D}}\left[\log\pi_{\theta}(\mathbf{y}_{w}&#124;\mathbf{x})\right]\Big{&#124;}_{c_{2}>
    |  | (6.3) |'
- en: '|  | .
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反向KL比正向KL更积极地修改概率质量。如果$\mathbf{x}_{1}$，$$\delta_{2}>\mathit{math}$。
- en: '2.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Reverse KL increases probability mass only on a subset of categories that equal
    target likelihoods. If $\mathbf{x}_{1}$, where $\mathbf{c}_{0}$.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反向KL仅在等于目标可能性的类别子集上增加概率质量。如果$\mathbf{x}_{1}$，其中$\mathbf{c}_{0}$。
- en: '3.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Reverse KL aggressively reduces probability mass on less-likely categories in
    the target distribution. If $\mathbf{x}_{1}$, where $\mathbf{c}_{1}$.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反向KL在目标分布中减少概率质量的可能性较低的类别。如果$\mathbf{x}_{1}$，其中$\mathbf{c}_{1}$。
- en: 'A proof of Theorem [6.5](#S6.Thmtheorem5 "Theorem 6.5\. ‣ 6.2 Case Study: Mode-Seeking
    Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is shown in [Section C.3](#A3.SS3 "C.3 Quantifying the Differences Between
    Forward and Reverse KL ‣ Appendix C More Details on Conceptual Unification and
    Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). Essentially, this theorem enlists several cases
    where the forward KL modifies probability mass in different amounts across various
    categories, but the reverse KL acts disproportionately. In particular, case 1
    says that the reverse KL exhibits more disproportionate probability mass changes
    on categories with equal likelihood $p_{t}(\mathbf{x})$ under certain conditions.
    Finally, case 3 shows that when the likelihood of a category is significantly
    larger than the target $q(\mathbf{x})$. In this case, while the forward KL will
    increase log probability ratios for both $\mathbf{x}_{1}$ value. These results
    highlight some scenarios under which the reverse KL can more efficiently re-organize
    probability mass across categories.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 定理[6.5](#S6.Thmtheorem5 "定理 6.5\. ‣ 6.2 案例研究：模式寻求的反向KL与模式覆盖的前向KL ‣ 6 概念统一和理论分析
    ‣ LLM的偏好微调应利用次优的、策略数据")的证明见[第C.3节](#A3.SS3 "C.3 量化前向和反向KL之间的差异 ‣ 附录C 关于概念统一和理论分析的更多细节
    ‣ 附录 ‣ LLM的偏好微调应利用次优的、策略数据")。本质上，这个定理列出了几种情况，其中前向KL在不同类别中以不同的量修改概率质量，但反向KL的作用不成比例。特别是，情况1说明在某些条件下，反向KL在具有相等可能性$p_{t}(\mathbf{x})$的类别上表现出更不成比例的概率质量变化。最后，情况3表明，当某个类别的可能性显著大于目标$q(\mathbf{x})$时，虽然前向KL将增加$\mathbf{x}_{1}$值的对数概率比。这些结果突出了在某些情况下反向KL可以更有效地重新组织跨类别的概率质量。
- en: Mode-seeking
    vs. mode-covering objectives for categorical distributions
    Typically the benefits of mode-seeking behavior are more apparent
    when the model $p(\mathbf{x})$, reverse KL can quickly re-distribute probability
    mass to only a subset of the required categories likely in target distribution,
    within a few gradient steps.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Mode-seeking
    vs. mode-covering objectives for categorical distributions
    Typically the benefits of mode-seeking behavior are more apparent
    when the model $p(\mathbf{x})$, reverse KL can quickly re-distribute probability
    mass to only a subset of the required categories likely in target distribution,
    within a few gradient steps.
- en: 7 Discussion, Conclusion, and Limitations
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 讨论、结论和局限性
- en: We attempted to understand which components are particularly important for fine-tuning
    language models with preference data. Through extensive experiments on different
    fine-tuning problems in both didactic and LLM settings, we established that on-policy
    sampling is crucial for good performance especially when the peak in the ground-truth
    reward lies in less-likely regions of the reference policy initialization. That
    said, in practice, doing so requires preference datasets with broader coverage
    than the reference policy. We also showed that negative gradients can enable faster
    convergence and that objectives that induce a negative gradient are complementary
    to using on-policy sampling. Finally, we show that the notion of mode-seeking
    divergences unifies the notion of on-policy sampling and negative gradient. Our
    case study comparing forward and reverse KL divergences demonstrates the superiority
    of the reverse KL divergence in re-distributing probability mass efficiently,
    supporting our empirical findings pertaining to on-policy sampling and negative
    gradients.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了解哪些组件对于使用偏好数据微调语言模型特别重要。通过在教学和LLM设置中对不同微调问题的广泛实验，我们确定了在策略采样对良好性能至关重要，特别是当真实奖励的峰值位于参考策略初始化的可能性较低的区域时。也就是说，在实际应用中，这需要比参考策略覆盖范围更广的偏好数据集。我们还展示了负梯度可以实现更快的收敛，并且诱导负梯度的目标与使用策略采样是互补的。最后，我们展示了模式寻求发散的概念统一了策略采样和负梯度的概念。我们的案例研究比较了前向和反向KL散度，表明反向KL散度在有效地重新分配概率质量方面优于前向KL散度，这支持了我们关于策略采样和负梯度的经验发现。
- en: While we conceptualize our observations, a limitation is that we don’t derive
    rigorous statistical guarantees in this work. As an example, we note that while
    the notion of concentrability coefficients (and associated guarantees) can potentially
    provide guarantees on on-policy sampling, to the best of our knowledge the notion
    of the negative gradient is not fully studied in the literature. We conjecture
    that negative gradient can perhaps be formalized statistically from the lens of
    providing a lower variance learning signal; it would be interesting for future
    work to formalize this. It would also be interesting to study more recent approaches
    based on minimax formulations (e.g., Munos et al. ([2023](#bib.bib34)); Yuan et al.
    ([2024](#bib.bib60)); Swamy et al. ([2024](#bib.bib47)); Chen et al. ([2024](#bib.bib8)))
    in our empirical and conceptual framework. Next, while we consider the coverage
    of preference data relative to that of the reference policy in our study, this
    is a simplification that does not account for the coverage of the pre-training
    distribution which future work can incorporate. Finally, we remark that our study
    does not explore the effect of reward model quality, which tends to also play
    a central role in LLM fine-tuning. It would be interesting to extend our analysis
    to incorporate the role of reward model quality and parameterization.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们对观察进行了概念化，但一个限制是我们没有在这项工作中得出严格的统计保证。例如，我们注意到，尽管集中系数的概念（及其相关保证）可能对在策略采样上提供保证，但据我们所知，负梯度的概念在文献中尚未完全研究。我们推测，负梯度或许可以从提供较低方差学习信号的角度进行统计形式化；未来的工作将对此进行形式化将是有趣的。基于最小最大公式的更近期方法（例如，Munos
    等（[2023](#bib.bib34)）；Yuan 等（[2024](#bib.bib60)）；Swamy 等（[2024](#bib.bib47)）；Chen
    等（[2024](#bib.bib8)））在我们的实证和概念框架中进行研究也将是有趣的。接下来，尽管我们在研究中考虑了相对于参考策略的偏好数据覆盖情况，但这是一种简化，不考虑预训练分布的覆盖情况，未来的工作可以纳入这一点。最后，我们指出，我们的研究没有探索奖励模型质量的影响，而奖励模型质量通常在LLM微调中扮演重要角色。将我们的分析扩展到包含奖励模型质量和参数化的作用将是有趣的。
- en: Acknowledgements
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Yi Su, Rishabh Agarwal, Zhang-Wei Hong, Young Geng, Abitha
    Thankaraj, Yuxiao Qu, So Yeon Min, Yutong He, Kevin Li, Sukjun Hwang, Khurram
    Yamin, Charlie Snell, Amrith Setlur, Kaylee Burns, Eric Mitchell, and others in
    CMU Russ Lab, CMU Auton Lab, Stanford IRIS Lab, and Stanford Ermon Group for discussions
    and feedback. AK thanks Aleksandra Faust, George Tucker, and Sergey Levine for
    informative discussions. This research is supported by computational resources
    from Google TPU Research Cloud (TRC) and the National Science Foundation. FT thanks
    Ruslan Salakhutdinov for insightful suggestions during this project. AS gratefully
    acknowledges the support of the NSF Graduate Research Fellowship Program.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Yi Su, Rishabh Agarwal, Zhang-Wei Hong, Young Geng, Abitha Thankaraj, Yuxiao
    Qu, So Yeon Min, Yutong He, Kevin Li, Sukjun Hwang, Khurram Yamin, Charlie Snell,
    Amrith Setlur, Kaylee Burns, Eric Mitchell 及其他 CMU Russ Lab, CMU Auton Lab, Stanford
    IRIS Lab 和 Stanford Ermon Group 的成员提供的讨论和反馈。AK 感谢 Aleksandra Faust, George Tucker
    和 Sergey Levine 提供的有益讨论。这项研究得到了 Google TPU Research Cloud (TRC) 和国家科学基金会的计算资源支持。FT
    感谢 Ruslan Salakhutdinov 在此项目中的深刻建议。AS 感谢 NSF 研究生奖学金项目的支持。
- en: References
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adolphs et al. (2022) Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar
    Sukhbaatar, and Jason Weston. The CRINGE Loss: Learning what language not to model.
    *arXiv e-prints*, art. arXiv:2211.05826, November 2022. [10.48550/arXiv.2211.05826](https:/doi.org/10.48550/arXiv.2211.05826).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adolphs 等（2022）Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar
    Sukhbaatar, 和 Jason Weston. CRINGE 损失: 学习不该建模的语言。*arXiv 电子预印本*，文章 arXiv:2211.05826，2022年11月。[10.48550/arXiv.2211.05826](https:/doi.org/10.48550/arXiv.2211.05826)。'
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal 等（2023）Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos,
    Matthieu Geist, 和 Olivier Bachem. Gkd: 泛化知识蒸馏用于自回归序列模型。*arXiv 预印本 arXiv:2306.13649*，2023年。'
- en: 'Ahmadian et al. (2024) Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh
    Fadaee, Julia Kreutzer, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting
    reinforce style optimization for learning from human feedback in llms. *arXiv
    preprint arXiv:2402.14740*, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahmadian 等（2024）Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee,
    Julia Kreutzer, Ahmet Üstün, 和 Sara Hooker. 回归基础: 重新审视基于强化风格的优化以从人类反馈中学习。*arXiv
    预印本 arXiv:2402.14740*，2024年。'
- en: 'Bradley and Terry (1952) Ralph Allan Bradley and Milton E. Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952. ISSN 00063444. URL [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley and Terry (1952) Ralph Allan Bradley 和 Milton E. Terry。不完全区组设计的秩分析：I.
    配对比较法。*Biometrika*，39(3/4):324–345，1952年。ISSN 00063444。网址 [http://www.jstor.org/stable/2334029](http://www.jstor.org/stable/2334029)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少量学习者，2020年。
- en: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie,
    Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum,
    Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii
    Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan,
    David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental
    limitations of reinforcement learning from human feedback. *Transactions on Machine
    Learning Research*, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=bx24KpJ4Eb](https://openreview.net/forum?id=bx24KpJ4Eb).
    Survey Certification.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casper et al. (2023) Stephen Casper、Xander Davies、Claudia Shi、Thomas Krendl
    Gilbert、Jérémy Scheurer、Javier Rando、Rachel Freedman、Tomasz Korbak、David Lindner、Pedro
    Freire、Tony Tong Wang、Samuel Marks、Charbel-Raphael Segerie、Micah Carroll、Andi
    Peng、Phillip Christoffersen、Mehul Damani、Stewart Slocum、Usman Anwar、Anand Siththaranjan、Max
    Nadeau、Eric J Michaud、Jacob Pfau、Dmitrii Krasheninnikov、Xin Chen、Lauro Langosco、Peter
    Hase、Erdem Biyik、Anca Dragan、David Krueger、Dorsa Sadigh 和 Dylan Hadfield-Menell。人类反馈强化学习中的开放问题和基本局限。*Transactions
    on Machine Learning Research*，2023年。ISSN 2835-8856。网址 [https://openreview.net/forum?id=bx24KpJ4Eb](https://openreview.net/forum?id=bx24KpJ4Eb)。调查认证。
- en: Chang et al. (2024) Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kianté Brantley,
    Dipendra Misra, Jason D. Lee, and Wen Sun. Dataset Reset Policy Optimization for
    RLHF. *arXiv e-prints*, art. arXiv:2404.08495, April 2024. [10.48550/arXiv.2404.08495](https:/doi.org/10.48550/arXiv.2404.08495).
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2024) Jonathan D. Chang、Wenhao Zhan、Owen Oertell、Kianté Brantley、Dipendra
    Misra、Jason D. Lee 和 Wen Sun。针对 RLHF 的数据集重置策略优化。*arXiv e-prints*，艺术编号 arXiv:2404.08495，2024年4月。[10.48550/arXiv.2404.08495](https://doi.org/10.48550/arXiv.2404.08495)。
- en: Chen et al. (2024) Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan
    Gu. Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.
    *arXiv e-prints*, art. arXiv:2401.01335, January 2024. [10.48550/arXiv.2401.01335](https:/doi.org/10.48550/arXiv.2401.01335).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024) Zixiang Chen、Yihe Deng、Huizhuo Yuan、Kaixuan Ji 和 Quanquan
    Gu。自我对弈微调将弱语言模型转换为强语言模型。*arXiv e-prints*，艺术编号 arXiv:2401.01335，2024年1月。[10.48550/arXiv.2401.01335](https://doi.org/10.48550/arXiv.2401.01335)。
- en: ContextualAI (2024) ContextualAI. Human-centered loss functions (halos), 2024.
    URL [https://github.com/ContextualAI/HALOs](https://github.com/ContextualAI/HALOs).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ContextualAI (2024) ContextualAI。以人为本的损失函数（halos），2024年。网址 [https://github.com/ContextualAI/HALOs](https://github.com/ContextualAI/HALOs)。
- en: Coste et al. (2023) Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
    Reward Model Ensembles Help Mitigate Overoptimization. *arXiv e-prints*, art.
    arXiv:2310.02743, October 2023. [10.48550/arXiv.2310.02743](https:/doi.org/10.48550/arXiv.2310.02743).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coste et al. (2023) Thomas Coste、Usman Anwar、Robert Kirk 和 David Krueger。奖励模型集成有助于减轻过度优化。*arXiv
    e-prints*，艺术编号 arXiv:2310.02743，2023年10月。[10.48550/arXiv.2310.02743](https://doi.org/10.48550/arXiv.2310.02743)。
- en: 'Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu,
    Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting Language
    Models with High-quality Feedback. *arXiv e-prints*, art. arXiv:2310.01377, October
    2023. [10.48550/arXiv.2310.01377](https:/doi.org/10.48550/arXiv.2310.01377).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui et al. (2023) Ganqu Cui、Lifan Yuan、Ning Ding、Guanming Yao、Wei Zhu、Yuan Ni、Guotong
    Xie、Zhiyuan Liu 和 Maosong Sun。UltraFeedback：通过高质量反馈提升语言模型。*arXiv e-prints*，艺术编号
    arXiv:2310.01377，2023年10月。[10.48550/arXiv.2310.01377](https://doi.org/10.48550/arXiv.2310.01377)。
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by
    scaling high-quality instructional conversations. *arXiv preprint arXiv:2305.14233*,
    2023.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等 (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, 和 Bowen Zhou. 通过扩展高质量指导性对话来增强聊天语言模型。*arXiv preprint
    arXiv:2305.14233*，2023年。
- en: 'Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie
    Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward
    ranked finetuning for generative foundation model alignment, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow,
    Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, 和 Tong Zhang. Raft：用于生成基础模型对齐的奖励排名微调，2023年。
- en: 'Dubois et al. (2024) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
    Alpacafarm: A simulation framework for methods that learn from human feedback,
    2024.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois 等 (2024) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani,
    Jimmy Ba, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. Alpacafarm：一个从人类反馈中学习的方法模拟框架，2024年。
- en: Eisenstein et al. (2023) Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad
    Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl,
    Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or Herding? Reward
    Model Ensembles Mitigate but do not Eliminate Reward Hacking. *arXiv e-prints*,
    art. arXiv:2312.09244, December 2023. [10.48550/arXiv.2312.09244](https:/doi.org/10.48550/arXiv.2312.09244).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenstein 等 (2023) Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami,
    Alex D’Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak
    Ramachandran, Peter Shaw, 和 Jonathan Berant. 帮助还是驱赶？奖励模型集成缓解但未消除奖励黑客行为。*arXiv
    e-prints*，文章编号 arXiv:2312.09244，2023年12月。 [10.48550/arXiv.2312.09244](https:/doi.org/10.48550/arXiv.2312.09244)。
- en: Ethayarajh et al. (2023) Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe
    Kiela. Human-aware loss functions (halos). Technical report, Contextual AI, 2023.
    https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ethayarajh 等 (2023) Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, 和 Douwe Kiela.
    人类感知的损失函数 (halos)。技术报告，Contextual AI，2023年。 https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf。
- en: Gao et al. (2022) Leo Gao, John Schulman, and Jacob Hilton. Scaling Laws for
    Reward Model Overoptimization. *arXiv e-prints*, art. arXiv:2210.10760, October
    2022. [10.48550/arXiv.2210.10760](https:/doi.org/10.48550/arXiv.2210.10760).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2022) Leo Gao, John Schulman, 和 Jacob Hilton. 奖励模型过度优化的缩放规律。*arXiv e-prints*，文章编号
    arXiv:2210.10760，2022年10月。 [10.48550/arXiv.2210.10760](https:/doi.org/10.48550/arXiv.2210.10760)。
- en: Gheshlaghi Azar et al. (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal
    Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A General
    Theoretical Paradigm to Understand Learning from Human Preferences. *arXiv e-prints*,
    art. arXiv:2310.12036, October 2023. [10.48550/arXiv.2310.12036](https:/doi.org/10.48550/arXiv.2310.12036).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gheshlaghi Azar 等 (2023) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot,
    Daniel Guo, Daniele Calandriello, Michal Valko, 和 Rémi Munos. 一个理解人类偏好学习的通用理论范式。*arXiv
    e-prints*，文章编号 arXiv:2310.12036，2023年10月。 [10.48550/arXiv.2310.12036](https:/doi.org/10.48550/arXiv.2310.12036)。
- en: Gulcehre et al. (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language
    modeling. *arXiv preprint arXiv:2308.08998*, 2023.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulcehre 等 (2023) Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia
    Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen
    Wang, Chenjie Gu 等. 语言建模的强化自训练 (rest)。*arXiv preprint arXiv:2308.08998*，2023年。
- en: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret,
    and Mathieu Blondel. Direct Language Model Alignment from Online AI Feedback.
    *arXiv e-prints*, art. arXiv:2402.04792, February 2024. [10.48550/arXiv.2402.04792](https:/doi.org/10.48550/arXiv.2402.04792).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret,
    和 Mathieu Blondel. 来自在线 AI 反馈的直接语言模型对齐。*arXiv e-prints*，文章编号 arXiv:2402.04792，2024年2月。
    [10.48550/arXiv.2402.04792](https:/doi.org/10.48550/arXiv.2402.04792)。
- en: Hu et al. (2023) Jian Hu, Li Tao, June Yang, and Chandler Zhou. Aligning Language
    Models with Offline Learning from Human Feedback. *arXiv e-prints*, art. arXiv:2308.12050,
    August 2023. [10.48550/arXiv.2308.12050](https:/doi.org/10.48550/arXiv.2308.12050).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2023) Jian Hu, Li Tao, June Yang, 和 Chandler Zhou. 将语言模型与来自人类反馈的离线学习对齐。*arXiv
    e-prints*，文章编号 arXiv:2308.12050，2023年8月。 [10.48550/arXiv.2308.12050](https:/doi.org/10.48550/arXiv.2308.12050)。
- en: Jain et al. (2023) Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P.
    Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktäschel, and David Scott Krueger.
    Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks.
    *arXiv e-prints*, art. arXiv:2311.12786, November 2023. [10.48550/arXiv.2311.12786](https:/doi.org/10.48550/arXiv.2311.12786).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人（2023）萨米亚克·贾因、罗伯特·柯克、埃克迪普·辛格·卢巴纳、罗伯特·P·迪克、田中英典、爱德华·格雷芬斯泰特、蒂姆·罗克特谢尔和大卫·斯科特·克鲁格。对微调在程序化定义任务上的效果的机制分析。*arXiv
    e-prints*，文章编号 arXiv:2311.12786，2023年11月。[10.48550/arXiv.2311.12786](https:/doi.org/10.48550/arXiv.2311.12786)。
- en: (23) Andrej Karpathy. minGPT. URL [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Andrej Karpathy。minGPT。网址 [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)。
- en: 'Khaki et al. (2024) Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, and Prathap Ramachandra.
    RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method
    for Alignment of Large Language Models. *arXiv e-prints*, art. arXiv:2402.10038,
    February 2024. [10.48550/arXiv.2402.10038](https:/doi.org/10.48550/arXiv.2402.10038).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khaki 等人（2024）赛义德·哈基、金金·李、兰·马、刘洋和普拉萨普·拉马昌德拉。RS-DPO：用于大语言模型对齐的混合拒绝采样和直接偏好优化方法。*arXiv
    e-prints*，文章编号 arXiv:2402.10038，2024年2月。[10.48550/arXiv.2402.10038](https:/doi.org/10.48550/arXiv.2402.10038)。
- en: 'Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli,
    and Thorsten Joachims. Morel: Model-based offline reinforcement learning. *arXiv
    preprint arXiv:2005.05951*, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kidambi 等人（2020）拉胡尔·基丹比、阿拉文德·拉杰斯瓦兰、普拉尼思·内特拉帕利和托尔斯滕·乔希姆斯。Morel：基于模型的离线强化学习。*arXiv
    预印本 arXiv:2005.05951*，2020年。
- en: 'Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization, 2017.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba（2017）迪德里克·P·金马和吉米·巴。Adam：一种随机优化方法，2017年。
- en: Kirk et al. (2023) Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena
    Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding
    the Effects of RLHF on LLM Generalisation and Diversity. *arXiv e-prints*, art.
    arXiv:2310.06452, October 2023. [10.48550/arXiv.2310.06452](https:/doi.org/10.48550/arXiv.2310.06452).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirk 等人（2023）罗伯特·柯克、伊什塔·梅迪拉塔、克里斯托弗罗斯·纳尔帕蒂斯、耶莲娜·卢克蒂娜、埃里克·汉布罗、爱德华·格雷芬斯泰特和罗伯塔·赖莱努。理解
    RLHF 对 LLM 泛化和多样性的影响。*arXiv e-prints*，文章编号 arXiv:2310.06452，2023年10月。[10.48550/arXiv.2310.06452](https:/doi.org/10.48550/arXiv.2310.06452)。
- en: Korbak et al. (2022) Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc
    Dymetman. On reinforcement learning and distribution matching for fine-tuning
    language models with no catastrophic forgetting. *Advances in Neural Information
    Processing Systems*, 35:16203–16220, 2022.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korbak 等人（2022）托马斯·科尔巴克、哈迪·埃尔沙赫、赫尔曼·克鲁泽夫斯基和马克·迪门特曼。关于强化学习和分布匹配，用于微调语言模型以避免灾难性遗忘。*神经信息处理系统进展*，35:16203–16220，2022年。
- en: 'Lee et al. (2024) Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg,
    Jonathan K. Kummerfeld, and Rada Mihalcea. A Mechanistic Understanding of Alignment
    Algorithms: A Case Study on DPO and Toxicity. *arXiv e-prints*, art. arXiv:2401.01967,
    January 2024. [10.48550/arXiv.2401.01967](https:/doi.org/10.48550/arXiv.2401.01967).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2024）安德鲁·李、肖燕·白、伊塔玛·普雷斯、马丁·瓦滕贝格、乔纳森·K·库默费尔德和拉达·米哈尔恰。对对齐算法的机械理解：DPO 和毒性案例研究。*arXiv
    e-prints*，文章编号 arXiv:2401.01967，2024年1月。[10.48550/arXiv.2401.01967](https:/doi.org/10.48550/arXiv.2401.01967)。
- en: 'Lu et al. (2022) Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui
    Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text
    generation with reinforced unlearning. *Advances in neural information processing
    systems*, 35:27591–27609, 2022.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人（2022）吕西明、肖恩·韦莱克、杰克·赫塞尔、李伟·姜、连辉·秦、彼得·韦斯特、普里特维拉杰·阿曼纳布鲁和叶锦春。Quark：通过强化“忘记”进行可控文本生成。*神经信息处理系统进展*，35:27591–27609，2022年。
- en: Mei et al. (2022) Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba
    Szepesvari, and Dale Schuurmans. The role of baselines in policy gradient optimization.
    *Advances in Neural Information Processing Systems*, 35:17818–17830, 2022.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mei 等人（2022）梅金成、韦斯利·钟、瓦伦丁·托马斯、博·戴、查巴·谢佩斯瓦里和戴尔·舒尔曼斯。基线在策略梯度优化中的作用。*神经信息处理系统进展*，35:17818–17830，2022年。
- en: 'Mukobi et al. (2023) Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim,
    Gitta Kutyniok, Kush Bhatia, and Silas Alberti. SuperHF: Supervised Iterative
    Learning from Human Feedback. *arXiv e-prints*, art. arXiv:2310.16763, October
    2023. [10.48550/arXiv.2310.16763](https:/doi.org/10.48550/arXiv.2310.16763).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukobi 等人（2023）加布里埃尔·穆科比、彼得·查坦、苏·方、罗伯特·温德斯海姆、吉塔·库提纽克、库什·巴蒂亚和西拉斯·阿尔贝尔提。SuperHF：从人类反馈中进行监督迭代学习。*arXiv
    e-prints*，文章编号 arXiv:2310.16763，2023年10月。[10.48550/arXiv.2310.16763](https:/doi.org/10.48550/arXiv.2310.16763)。
- en: Munos and Szepesvári (2008) Rémi Munos and Csaba Szepesvári. Finite-time bounds
    for fitted value iteration. *Journal of Machine Learning Research*, 9(5), 2008.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos 和 Szepesvári（2008）Rémi Munos 和 Csaba Szepesvári. 适应值迭代的有限时间界限。*机器学习研究期刊*，9(5)，2008
    年。
- en: Munos et al. (2023) Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad
    Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist,
    Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier
    Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash Learning from
    Human Feedback. *arXiv e-prints*, art. arXiv:2312.00886, December 2023. [10.48550/arXiv.2312.00886](https:/doi.org/10.48550/arXiv.2312.00886).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munos 等（2023）Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi
    Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
    Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel
    J. Mankowitz, Doina Precup 和 Bilal Piot. 纳什学习来自人类反馈。*arXiv 电子版*，文章 arXiv:2312.00886，2023
    年 12 月。 [10.48550/arXiv.2312.00886](https:/doi.org/10.48550/arXiv.2312.00886)。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback, 2022.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike 和 Ryan Lowe. 训练语言模型按照指令进行操作，采用人类反馈，2022
    年。
- en: Peters and Schaal (2007) Jan Peters and Stefan Schaal. Reinforcement learning
    by reward-weighted regression for operational space control. In *Proceedings of
    the 24th International Conference on Machine Learning*, pages 745–750\. ACM, 2007.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 和 Schaal（2007）Jan Peters 和 Stefan Schaal. 通过奖励加权回归进行操作空间控制的强化学习。在 *第24届国际机器学习会议论文集*
    中，页面 745–750。ACM，2007 年。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. Improving language understanding by generative pre-training. 2018.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）Alec Radford, Karthik Narasimhan, Tim Salimans 和 Ilya Sutskever.
    通过生成预训练提高语言理解。2018 年。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*,
    2023.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2023）Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,
    Christopher D Manning 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型。*arXiv 预印本 arXiv:2305.18290*，2023
    年。
- en: 'Rafailov et al. (2024) Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea
    Finn. From r to $q^{*}$: Your language model is secretly a q-function. *arXiv
    preprint arXiv:2404.12358*, 2024.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2024）Rafael Rafailov, Joey Hejna, Ryan Park 和 Chelsea Finn. 从 r 到
    $q^{*}$：你的语言模型实际上是一个 q 函数。*arXiv 预印本 arXiv:2404.12358*，2024 年。
- en: 'Rosset et al. (2024) Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce,
    Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language
    models to self-improve with general preferences. *arXiv preprint arXiv:2404.03715*,
    2024.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosset 等（2024）Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce,
    Ahmed Awadallah 和 Tengyang Xie. 直接纳什优化：教语言模型通过通用偏好自我改进。*arXiv 预印本 arXiv:2404.03715*，2024
    年。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. *arXiv e-prints*,
    art. arXiv:1707.06347, July 2017. [10.48550/arXiv.1707.06347](https:/doi.org/10.48550/arXiv.1707.06347).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov. 近端策略优化算法。*arXiv 电子版*，文章 arXiv:1707.06347，2017 年 7 月。 [10.48550/arXiv.1707.06347](https:/doi.org/10.48550/arXiv.1707.06347)。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*，2017 年。
- en: Sharma et al. (2024) Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn,
    Kushal Arora, and Thomas Kollar. A critical evaluation of ai feedback for aligning
    large language models. *arXiv preprint arXiv:2402.12366*, 2024.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等（2024）Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal
    Arora 和 Thomas Kollar. 对 AI 反馈在对齐大型语言模型中的关键评估。*arXiv 预印本 arXiv:2402.12366*，2024
    年。
- en: 'Singhal et al. (2023) Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett.
    A long way to go: Investigating length correlations in rlhf. *arXiv preprint arXiv:2310.03716*,
    2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等（2023）Prasann Singhal, Tanya Goyal, Jiacheng Xu 和 Greg Durrett. 还有很长的路要走：研究
    rlhf 中的长度相关性。*arXiv 预印本 arXiv:2310.03716*，2023 年。
- en: Sutton et al. (1999) Richard S Sutton, David McAllester, Satinder Singh, and
    Yishay Mansour. Policy gradient methods for reinforcement learning with function
    approximation. In S. Solla, T. Leen, and K. Müller, editors, *Advances in Neural
    Information Processing Systems*, volume 12\. MIT Press, 1999. URL [https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf).
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等（1999）**理查德·S·萨顿**、**大卫·麦卡勒斯特**、**萨廷德·辛格** 和 **伊沙伊·曼苏尔**。用于函数逼近的强化学习策略梯度方法。在
    S. Solla、T. Leen 和 K. Müller 主编的 *神经信息处理系统进展*，第12卷。MIT出版社，1999年。网址 [https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)。
- en: Swaminathan and Joachims (2015) Adith Swaminathan and Thorsten Joachims. The
    self-normalized estimator for counterfactual learning. In *advances in neural
    information processing systems*, pages 3231–3239, 2015.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swaminathan 和 Joachims（2015）**阿迪斯·斯瓦敏纳坦** 和 **托尔斯滕·乔阿希姆斯**。用于反事实学习的自归一化估计量。在
    *神经信息处理系统进展*，第3231–3239页，2015年。
- en: Swamy et al. (2024) Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven
    Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from
    human feedback. *arXiv preprint arXiv:2401.04056*, 2024.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swamy 等（2024）**戈库尔·斯瓦米**、**克里斯托夫·丹**、**拉胡尔·基丹比**、**智伟·史蒂文·吴** 和 **阿雷赫·阿加瓦尔**。一种最小最大主义的方法来通过人类反馈进行强化学习。
    *arXiv 预印本 arXiv:2401.04056*，2024年。
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tunstall 等（2023）**路易斯·坦斯特尔**、**爱德华·比钦**、**内森·兰伯特**、**纳兹敏·拉贾尼**、**卡希夫·拉苏尔**、**优尼斯·贝尔卡达**、**盛逸·黄**、**利安德罗·冯·维拉**、**克莱门丁·福里耶**、**内森·哈比布**、**内森·萨拉津**、**奥马尔·桑塞维罗**、**亚历山大·M·拉什**
    和 **托马斯·沃尔夫**。Zephyr: 直接蒸馏语言模型对齐，2023。'
- en: 'von Werra et al. (2020) Leandro von Werra, Younes Belkada, Lewis Tunstall,
    Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer
    reinforcement learning. [https://github.com/huggingface/trl](https://github.com/huggingface/trl),
    2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '冯·维拉 等（2020）**利安德罗·冯·维拉**、**优尼斯·贝尔卡达**、**路易斯·坦斯特尔**、**爱德华·比钦**、**特里斯坦·斯鲁什**、**内森·兰伯特**
    和 **盛逸·黄**。Trl: Transformer 强化学习。 [https://github.com/huggingface/trl](https://github.com/huggingface/trl)，2020。'
- en: Welleck et al. (2020) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan,
    Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training.
    In *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SJeYe0NtvH](https://openreview.net/forum?id=SJeYe0NtvH).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等（2020）**肖恩·韦莱克**、**伊利亚·库利科夫**、**斯蒂芬·罗勒**、**艾米莉·迪南**、**全炫勋** 和 **杰森·韦斯顿**。带有不太可能性训练的神经文本生成。在
    *国际学习表征会议*，2020年。网址 [https://openreview.net/forum?id=SJeYe0NtvH](https://openreview.net/forum?id=SJeYe0NtvH)。
- en: Williams (1992) R. J. Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Machine Learning*, 8(3-4):229–256,
    May 1992.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（1992）**R.J. 威廉姆斯**。用于连接主义强化学习的简单统计梯度跟随算法。 *机器学习*，8(3-4)：229–256，1992年5月。
- en: 'Xie et al. (2022) Annie Xie, Fahim Tajwar, Archit Sharma, and Chelsea Finn.
    When to ask for help: Proactive interventions in autonomous reinforcement learning.
    In *Advances in Neural Information Processing Systems*, volume 35, 2022.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2022）**安妮·谢**、**法希姆·塔吉瓦尔**、**阿基特·香玛** 和 **切尔西·芬**。何时寻求帮助：自主强化学习中的主动干预。在
    *神经信息处理系统进展*，第35卷，2022年。
- en: 'Xiong et al. (2023) Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong,
    Heng Ji, Nan Jiang, and Tong Zhang. Iterative Preference Learning from Human Feedback:
    Bridging Theory and Practice for RLHF under KL-Constraint. *arXiv e-prints*, art.
    arXiv:2312.11456, December 2023. [10.48550/arXiv.2312.11456](https:/doi.org/10.48550/arXiv.2312.11456).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等（2023）**魏·熊**、**韩泽·董**、**陈璐·叶**、**子琪·王**、**汉·钟**、**恒·吉**、**南·姜** 和 **童·张**。来自人类反馈的迭代偏好学习：在
    KL 约束下桥接理论与实践。 *arXiv 电子预印本*，文章 arXiv:2312.11456，2023年12月。 [10.48550/arXiv.2312.11456](https:/doi.org/10.48550/arXiv.2312.11456)。
- en: Xu et al. (2024) Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu
    Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO Superior to PPO for LLM Alignment?
    A Comprehensive Study. *arXiv e-prints*, art. arXiv:2404.10719, April 2024. [10.48550/arXiv.2404.10719](https:/doi.org/10.48550/arXiv.2404.10719).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2024）**徐述生**、**魏·傅**、**贾轩·高**、**温杰·叶**、**韦林·刘**、**志瑜·梅**、**光举·王**、**超·于**
    和 **伊·吴**。DPO 是否优于 PPO 进行大语言模型对齐？一项全面的研究。 *arXiv 电子预印本*，文章 arXiv:2404.10719，2024年4月。
    [10.48550/arXiv.2404.10719](https:/doi.org/10.48550/arXiv.2404.10719)。
- en: 'Yarats et al. (2021a) Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel
    Pinto. Mastering visual continuous control: Improved data-augmented reinforcement
    learning. *arXiv preprint arXiv:2107.09645*, 2021a.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarats 等 (2021a) Denis Yarats, Rob Fergus, Alessandro Lazaric 和 Lerrel Pinto.
    掌握视觉连续控制：改进的数据增强强化学习。 *arXiv 预印本 arXiv:2107.09645*，2021a。
- en: 'Yarats et al. (2021b) Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation
    is all you need: Regularizing deep reinforcement learning from pixels. In *International
    Conference on Learning Representations*, 2021b. URL [https://openreview.net/forum?id=GY6-6sTvGaf](https://openreview.net/forum?id=GY6-6sTvGaf).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarats 等 (2021b) Denis Yarats, Ilya Kostrikov 和 Rob Fergus. 图像增强就是你所需的一切：从像素正则化深度强化学习。
    在 *国际学习表征会议*，2021b。 URL [https://openreview.net/forum?id=GY6-6sTvGaf](https://openreview.net/forum?id=GY6-6sTvGaf)。
- en: 'Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James
    Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy
    optimization. *arXiv preprint arXiv:2005.13239*, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou,
    Sergey Levine, Chelsea Finn 和 Tengyu Ma. Mopo: 基于模型的离线策略优化。 *arXiv 预印本 arXiv:2005.13239*，2020。'
- en: 'Yu et al. (2021) Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran,
    Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy
    optimization. *Advances in neural information processing systems*, 34:28954–28967,
    2021.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2021) Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey
    Levine 和 Chelsea Finn. Combo: 保守的离线基于模型的策略优化。 *神经信息处理系统进展*，34:28954–28967，2021。'
- en: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar
    Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. *arXiv
    preprint arXiv:2401.10020*, 2024.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu 和 Jason Weston. 自奖励语言模型。 *arXiv 预印本 arXiv:2401.10020*，2024。
- en: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li,
    Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-Rewarding Language Models.
    *arXiv e-prints*, art. arXiv:2401.10020, January 2024. [10.48550/arXiv.2401.10020](https:/doi.org/10.48550/arXiv.2401.10020).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar
    Sukhbaatar, Jing Xu 和 Jason Weston. 自奖励语言模型。 *arXiv 电子预印本*，art. arXiv:2401.10020，2024年1月。
    [10.48550/arXiv.2401.10020](https:/doi.org/10.48550/arXiv.2401.10020)。
- en: 'Zhao et al. (2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad
    Saleh, and Peter J. Liu. SLiC-HF: Sequence Likelihood Calibration with Human Feedback.
    *arXiv e-prints*, art. arXiv:2305.10425, May 2023. [10.48550/arXiv.2305.10425](https:/doi.org/10.48550/arXiv.2305.10425).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等 (2023) Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad
    Saleh 和 Peter J. Liu. SLiC-HF: 基于人类反馈的序列似然校准。 *arXiv 电子预印本*，art. arXiv:2305.10425，2023年5月。
    [10.48550/arXiv.2305.10425](https:/doi.org/10.48550/arXiv.2305.10425)。'
- en: Ziegler et al. (2020) Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
    Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences, 2020.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等 (2020) Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano 和 Geoffrey Irving. 从人类偏好中微调语言模型，2020。
- en: Appendices
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Connections to Existing Fine-Tuning Results
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 与现有微调结果的联系
- en: 'Our proposed framework also allows us to explain experiments and evaluations
    in several existing LLM fine-tuning results, and as a result, implies several
    practical guidelines for LLM practitioners. On the AlpacaFarm benchmark (Dubois
    et al., [2024](#bib.bib14)), our results corroborate the gap between conditional
    supervised fine-tuning objectives such as binary FeedME and reward conditioning,
    and RL or contrastive training methods such as PPO and DPO: these results are
    perhaps even more extreme in that these conditional and weighted supervised fine-tuning
    objectives are not even able to outperform regular SFT. Methods that utilize on-policy
    sampling such as ReST (Gulcehre et al., [2023](#bib.bib19)) and Quark (Lu et al.,
    [2022](#bib.bib30)) do outperform SFT but still underperform on-policy RL or on-policy
    contrastive training. The top-performing methods on the benchmark are offline
    DPO, which uses a negative gradient, and PPO, which leverages on-policy sampling.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的框架还使我们能够解释现有LLM微调结果中的实验和评估，因此，暗示了针对LLM从业者的几个实际指导原则。在AlpacaFarm基准（Dubois
    et al., [2024](#bib.bib14)）上，我们的结果证实了条件监督微调目标（如二元FeedME）与奖励调节以及RL或对比训练方法（如PPO和DPO）之间的差距：这些结果甚至更加极端，因为这些条件和加权监督微调目标甚至无法超越常规SFT。利用策略采样的方法，如ReST（Gulcehre
    et al., [2023](#bib.bib19)）和Quark（Lu et al., [2022](#bib.bib30)）确实优于SFT，但仍然不如策略RL或策略对比训练。在基准测试中表现最佳的方法是离线DPO（使用负梯度）和PPO（利用策略采样）。
- en: Additionally, methods such as self-rewarding language models (Yuan et al., [2024](#bib.bib59)),
    OAIF (Guo et al., [2024](#bib.bib20)), DR-PO (Chang et al., [2024](#bib.bib7)),
    Hybrid-DPO (Xiong et al., [2023](#bib.bib53)), and RS-DPO (Khaki et al., [2024](#bib.bib24))
    couple on-policy sampling or rejection sampling with contrastive training objectives.
    These works corroborate our observation regarding the efficacy of on-policy sampling
    and negative gradients and how they are complementary. Approaches such as CRINGE (Adolphs
    et al., [2022](#bib.bib1)) combine maximum likelihood with a token level contrastive
    loss term and show gains over solely utilizing supervised likelihood, corroborating
    our insights about negative gradients.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，诸如自奖励语言模型（Yuan et al., [2024](#bib.bib59)）、OAIF（Guo et al., [2024](#bib.bib20)）、DR-PO（Chang
    et al., [2024](#bib.bib7)）、Hybrid-DPO（Xiong et al., [2023](#bib.bib53)）以及RS-DPO（Khaki
    et al., [2024](#bib.bib24)）等方法将策略采样或拒绝采样与对比训练目标结合起来。这些工作证实了我们关于策略采样和负梯度的有效性的观察，以及它们是如何互补的。诸如CRINGE（Adolphs
    et al., [2022](#bib.bib1)）等方法将最大似然与令牌级对比损失项相结合，显示出比单纯利用监督似然更好的效果，证实了我们关于负梯度的见解。
- en: 'Concurrently to us, Xu et al. ([2024](#bib.bib54)) show that on many practical
    LLM fine-tuning problems offline DPO underperforms on-policy PPO. While we do
    not study the same LLM fine-tuning problems, the insights from this work corroborate
    our findings, which in turn extend insights from this work. For instance, this
    work observes that DPO can learn to find out-of-distribution responses, which
    is consistent with our analysis in Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Takeaway
    2: Mechanisms Explaining the Behavior of the Negative Gradient ‣ 5.2 Question
    2: The Role of Negative Gradient ‣ 5 Empirical Analysis Results ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") that offline DPO training
    might increase probability mass on the highly likely regions of $\pi_{\theta}$.
    To avoid this issue, this work prescribes an iterated DPO recipe where the reference
    policy (i.e., the SFT policy in their setting) is used to iteratively collect
    new samples for DPO training. Section [5.3](#S5.SS3 "5.3 Question 3: On-Policy
    Sampling and Negative Gradients are Complementary ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    arrives at a similar conclusion that using on-policy samples for policy optimization,
    though we recommend collecting samples from the current policy and not the reference
    policy, which might fail to cover important regions of the space when the peak
    in the reward function appears farther away from the high-likely regions of the
    reference policy.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '与我们同时，Xu 等人 ([2024](#bib.bib54)) 表明，在许多实际的 LLM 微调问题中，离线 DPO 不如策略性 PPO。虽然我们没有研究相同的
    LLM 微调问题，但这项工作的见解证实了我们的发现，这反过来扩展了这项工作的见解。例如，这项工作观察到 DPO 可以学习发现分布外的响应，这与我们在第 [5.2.2](#S5.SS2.SSS2
    "5.2.2 Takeaway 2: Mechanisms Explaining the Behavior of the Negative Gradient
    ‣ 5.2 Question 2: The Role of Negative Gradient ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    节的分析一致，即离线 DPO 训练可能会增加在 $\pi_{\theta}$ 的高可能区域上的概率质量。为了避免这个问题，这项工作建议使用迭代的 DPO 方法，其中参考策略（即他们设置中的
    SFT 策略）用于迭代收集新的样本进行 DPO 训练。第 [5.3](#S5.SS3 "5.3 Question 3: On-Policy Sampling
    and Negative Gradients are Complementary ‣ 5 Empirical Analysis Results ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data") 节得出类似的结论，即使用策略性样本进行策略优化，尽管我们建议从当前策略中收集样本，而不是参考策略，因为当奖励函数的峰值距离参考策略的高可能区域较远时，这可能无法覆盖空间中的重要区域。'
- en: Appendix B Computational vs Wall-Clock Time Tradeoff for Various Methods
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 各种方法的计算时间与实时钟时间权衡
- en: '|  | Bandit (R1) | Min Length | Skew Length |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | Bandit (R1) | 最短长度 | 偏差长度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Reward ($\uparrow$) | Time |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | 奖励 ($\uparrow$) | 时间 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Offline DPO / IPO | 0.82 (0.04) | 1.7 hours | 1.0 (0.0) | 1.3 hours | 11.8
    (14.0) | 0.12 hours |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 离线 DPO / IPO | 0.82 (0.04) | 1.7 小时 | 1.0 (0.0) | 1.3 小时 | 11.8 (14.0) |
    0.12 小时 |'
- en: '| On-policy PPO | 0.92 (0.01) | 0.93 hours | 20.5 (25.4) | 4.84 hours | 15.8
    (11.1) | 7.26 hours |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 策略性 PPO | 0.92 (0.01) | 0.93 小时 | 20.5 (25.4) | 4.84 小时 | 15.8 (11.1) | 7.26
    小时 |'
- en: '| On-policy RWR | 0.88 (0.01) | 0.12 hours | 65.5 (36.7) | 15.5 hours | 15.8
    (9.3) | 15.5 hours |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 策略性 RWR | 0.88 (0.01) | 0.12 小时 | 65.5 (36.7) | 15.5 小时 | 15.8 (9.3) | 15.5
    小时 |'
- en: '| On-policy DPO / IPO | 0.92 (0.01) | 0.12 hours | 1.0 (0.0) | 0.4 hours |
    0.0 (0.0) | 0.4 hours |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 策略性 DPO / IPO | 0.92 (0.01) | 0.12 小时 | 1.0 (0.0) | 0.4 小时 | 0.0 (0.0) |
    0.4 小时 |'
- en: 'Table 3: Wall-clock time comparisons. Comparison between on-policy and offline
    variants of contrastive objectives (DPO/IPO) in terms of reward and wall-clock
    time required till convergence of the run. Generally, on-policy contrastive approaches
    achieve both superior reward and wall-clock time as opposed to offline contrastive
    approaches (offline DPO/IPO) and on-policy RL (PPO, RWR). Synthetic LLM experiments
    use a single A40 GPU. Bandit experiments use a Intel(R) Xeon(R) CPU E5-2698 v4
    @ 2.20GHz CPU, with 4 threads.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 实时钟时间比较。对比策略性和离线对比目标（DPO/IPO）的变体在奖励和收敛所需的实时钟时间方面。通常，相较于离线对比方法（离线 DPO/IPO）和策略性
    RL（PPO, RWR），策略性对比方法在奖励和实时钟时间上都表现更优。合成 LLM 实验使用单个 A40 GPU。Bandit 实验使用 Intel(R)
    Xeon(R) CPU E5-2698 v4 @ 2.20GHz CPU，配备 4 个线程。'
- en: 'A natural takeaway extending the empirical results from Section [5.3](#S5.SS3
    "5.3 Question 3: On-Policy Sampling and Negative Gradients are Complementary ‣
    5 Empirical Analysis Results ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data") is that on-policy variants of contrastive approaches
    might provide for an better tradeoff between computation and wall-clock time.
    We perform a comparison of wall-clock time needed to run our experiments in [Table 3](#A2.T3
    "In Appendix B Computational vs Wall-Clock Time Tradeoff for Various Methods ‣
    Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). in particular, we found that on-policy DPO only requires 0.4 hours to
    converge, while offline DPO requires a wall-clock time of 1.3 hours to converge
    to the same solution in the Min Length scenario. In the Skew Length scenario,
    where the learned policy must deviate from the initial reference policy substantially,
    we find that while offline DPO can converge a bit quickly (0.12 hours), it flatlines
    at a sub-optimal solution (completion length of 11.8) as compared to on-policy
    DPO which takes merely 0.4 hours to reach a more optimal solution. This is far
    more time-efficient compared to other on-policy methods such as PPO and RWR that
    present a sampling bottleneck.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从第[5.3](#S5.SS3 "5.3 问题 3：政策内采样和负梯度是互补的 ‣ 5 实证分析结果 ‣ LLM的偏好微调应利用次优的政策内数据")节扩展实证结果的一个自然结论是，政策内对比方法可能在计算和实际时间之间提供更好的权衡。我们在[表
    3](#A2.T3 "附录 B 各种方法的计算与实际时间权衡 ‣ 附录 ‣ LLM的偏好微调应利用次优的政策内数据")中比较了运行实验所需的实际时间。特别地，我们发现，政策内
    DPO 只需 0.4 小时即可收敛，而离线 DPO 在 Min Length 场景下需要 1.3 小时的实际时间才能收敛到相同的解决方案。在 Skew Length
    场景中，学习的策略必须大幅度偏离初始参考策略，我们发现，虽然离线 DPO 可以稍微快一点（0.12 小时），但其最终停留在一个次优解（完成长度为 11.8），相比之下，政策内
    DPO 仅需 0.4 小时即可达到更优的解决方案。这比其他政策内方法，如 PPO 和 RWR，具有更高的时间效率，这些方法存在采样瓶颈。
- en: Appendix C More Details on Conceptual Unification and Theoretical Analysis
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 关于概念统一和理论分析的更多细节
- en: C.1 Unifying On-Policy Sampling and Negative Gradients via Mode-Seeking Divergences
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1 通过模式寻求的散度统一政策内采样和负梯度
- en: Here we provide proofs for the claims in [6.1](#S6.SS1 "6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). We will show that on-policy methods and offline constrastive methods,
    both are mode-seeking as opposed to supervised maximum likelihood approaches,
    which are mode-covering. This conceptually explains the differences in their behaviors
    that we observe in our experiments.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了[6.1](#S6.SS1 "6.1 寻找模式统一了政策内采样和负梯度 ‣ 6 概念统一和理论分析 ‣ LLM的偏好微调应利用次优的政策内数据")中声明的证明。我们将展示，政策内方法和离线对比方法都是模式寻求的，而监督最大似然方法则是模式覆盖的。这在概念上解释了我们在实验中观察到的它们行为的差异。
- en: C.1.1 On-policy Methods Are Mode-Seeking
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: C.1.1 政策内方法是模式寻求的
- en: First, we prove [Lemma 6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6.1 Seeking Modes
    Unifies On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and
    Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"), i.e., we want to show that on-policy RL methods and on-policy
    versions of weighted supervised learning methods optimize regularized version
    of a reverse KL-divergence.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们证明了[引理 6.1](#S6.Thmtheorem1 "引理 6.1\. ‣ 6.1 寻找模式统一了政策内采样和负梯度 ‣ 6 概念统一和理论分析
    ‣ LLM的偏好微调应利用次优的政策内数据")，即我们想展示政策内强化学习方法和政策内加权监督学习方法优化的是反向KL散度的正则化版本。
- en: Proof.
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Both on-policy RL algorithms and on-policy versions of weighted supervised
    learning, optimize the following loss function:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 政策内强化学习算法和政策内加权监督学习版本都优化以下损失函数：
- en: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})=-\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}}}[\mathbb{E}_{\mathbf{y}\sim\pi_{\theta}(.&#124;\mathbf{x})}[r(\mathbf{x},\mathbf{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_{\theta}(.&#124;\mathbf{x})&#124;&#124;\pi_{\mathrm{ref}}(.&#124;\mathbf{x})]]$
    |  | (C.1) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})=-\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{pref}}}[\mathbb{E}_{\mathbf{y}\sim\pi_{\theta}(.&#124;\mathbf{x})}[r(\mathbf{x},\mathbf{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_{\theta}(.&#124;\mathbf{x})&#124;&#124;\pi_{\mathrm{ref}}(.&#124;\mathbf{x})]]$
    |  | (C.1) |'
- en: 'Following Appendix A.1 of Rafailov et al. ([2023](#bib.bib38)), there exists
    some policy $\pi^{*}$ as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Rafailov 等人附录 A.1 ([2023](#bib.bib38))，存在一些策略 $\pi^{*}$ 如下：
- en: '|  | $\displaystyle r(\mathbf{x},\mathbf{y})=\beta\log Z(\mathbf{x})+\beta\log\left(\frac{\pi^{*}(\mathbf{y}&#124;\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})}\right)$
    |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r(\mathbf{x},\mathbf{y})=\beta\log Z(\mathbf{x})+\beta\log\left(\frac{\pi^{*}(\mathbf{y}&#124;\mathbf{x})}{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})}\right)$
    |  |'
- en: 'where $Z(\mathbf{x})=\sum_{\mathbf{y}}\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\exp\left(\frac{r(\mathbf{x},\mathbf{y})}{\beta}\right)$
    is the partition function. Combining these two, we get:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z(\mathbf{x})=\sum_{\mathbf{y}}\pi_{\mathrm{ref}}(\mathbf{y}|\mathbf{x})\exp\left(\frac{r(\mathbf{x},\mathbf{y})}{\beta}\right)$
    是分区函数。结合这两者，我们得到：
- en: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})={}$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{RL}}(\mathcal{D}_{\text{pref}},\pi_{\theta})={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: Note that $Z(\mathbf{x})$ is equivalent to optimizing the reverse KL-divergence.
    Since optimizing the reverse KL-divergence is mode-seeking, we see that on-policy
    RL algorithms have mode-seeking behavior. ∎
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 $Z(\mathbf{x})$ 等同于优化反向KL散度。由于优化反向KL散度是模式寻求的，我们看到在策略RL算法中具有模式寻求行为。∎
- en: C.1.2 Contrastive Approaches (e.g., DPO/IPO) are Mode-Seeking
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: C.1.2 对比方法（例如，DPO/IPO）是模式寻求的
- en: Next, we show that this is also the case for contrastive approaches as we prove
    [Lemma 6.2](#S6.Thmtheorem2 "Lemma 6.2\. ‣ 6.1 Seeking Modes Unifies On-Policy
    Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical Analysis
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data").
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示这也适用于对比方法，因为我们证明了 [引理 6.2](#S6.Thmtheorem2 "Lemma 6.2\. ‣ 6.1 Seeking
    Modes Unifies On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification
    and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data")。
- en: Proof.
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'First consider an input $\mathbf{x}$. Consider the gradient update (with a
    small enough learning rate):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑一个输入 $\mathbf{x}$。考虑梯度更新（学习率足够小的情况下）：
- en: '|  | $1$2 |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'We shall prove that for all possible models $\theta$, such that after taking
    the gradient update, we have:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将证明对于所有可能的模型 $\theta$，在进行梯度更新后，我们有：
- en: '|  |  and then study under what conditions is
    it possible that for any pairing of positives and negatives, 
    然后研究在什么条件下，对于任何正负样本的配对，. This means that if 。这意味着如果  for the negative response. This proves the second
    part of this statement. ∎
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一结果的第二部分，我们注意到当梯度点积为负时 $$c_{2}> 对于负响应。这证明了这一陈述的第二部分。∎
- en: 'Gradients for both DPO and IPO exhibit the form in [Lemma 6.2](#S6.Thmtheorem2
    "Lemma 6.2\. ‣ 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients
    ‣ 6 Conceptual Unification and Theoretical Analysis ‣ Preference Fine-Tuning of
    LLMs Should Leverage Suboptimal, On-Policy Data"). We now show that the gradient
    of both DPO and IPO takes the form shown in [Equation 6.1](#S6.E1 "In Lemma 6.2\.
    ‣ 6.1 Seeking Modes Unifies On-Policy Sampling and Negative Gradients ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"). From Rafailov et al. ([2023](#bib.bib38)), the gradient
    of the DPO loss is:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: DPO和IPO的梯度都表现出 [引理 6.2](#S6.Thmtheorem2 "Lemma 6.2\. ‣ 6.1 寻找模式统一了策略采样和负梯度 ‣
    6 概念统一和理论分析 ‣ LLM的偏好微调应该利用次优的策略数据") 中所示的形式。我们现在展示DPO和IPO的梯度都采取了 [公式 6.1](#S6.E1
    "In Lemma 6.2\. ‣ 6.1 寻找模式统一了策略采样和负梯度 ‣ 6 概念统一和理论分析 ‣ LLM的偏好微调应该利用次优的策略数据") 中所示的形式。根据
    Rafailov 等人 ([2023](#bib.bib38))，DPO损失的梯度是：
- en: '|  | $1$2 |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $c^{\text{DPO}}(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})=\sigma\left(\beta\log\frac{\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{l}|\mathbf{x})}-\beta\log\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{w}|\mathbf{x})}\right)$.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c^{\text{DPO}}(\mathbf{x},\mathbf{y}_{w},\mathbf{y}_{l})=\sigma\left(\beta\log\frac{\pi_{\theta}(\mathbf{y}_{l}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{l}|\mathbf{x})}-\beta\log\frac{\pi_{\theta}(\mathbf{y}_{w}|\mathbf{x})}{\pi_{\mathrm{ref}}\mathbf{y}_{w}|\mathbf{x})}\right)$。
- en: Now we derive the gradient of the IPO loss. Define
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们推导IPO损失的梯度。定义
- en: '|  | $1$2 |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'The gradient of the IPO loss is:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: IPO损失的梯度是：
- en: '|  | $\displaystyle\nabla_{\theta}\mathcal{L}_{\text{IPO}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathcal{L}_{\text{IPO}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: C.1.3 Supervised Offline Algorithms are Mode-Covering
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: C.1.3 监督离线算法是模式覆盖的
- en: Now we prove [Lemma 6.3](#S6.Thmtheorem3 "Lemma 6.3\. ‣ 6.1 Seeking Modes Unifies
    On-Policy Sampling and Negative Gradients ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"), which shows that supervised offline methods that optimize a maximum likelihood
    loss exhibit mode-covering behavior.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们证明 [引理 6.3](#S6.Thmtheorem3 "Lemma 6.3\. ‣ 6.1 寻找模式统一了策略采样和负梯度 ‣ 6 概念统一和理论分析
    ‣ LLM的偏好微调应该利用次优的策略数据")，这表明优化最大似然损失的监督离线方法表现出模式覆盖行为。
- en: Proof.
  id: totrans-339
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Offline supervised methods optimize the following loss function:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 离线监督方法优化以下损失函数：
- en: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
- en: Define a new distribution
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个新的分布
- en: '|  | $\tilde{\pi}(\mathbf{y}&#124;\mathbf{x})=\frac{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{y})}{Z(\mathbf{x})}$ |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\pi}(\mathbf{y}&#124;\mathbf{x})=\frac{\pi_{\mathrm{ref}}(\mathbf{y}&#124;\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{y})}{Z(\mathbf{x})}$ |  |'
- en: 'Here $Z(\mathbf{x})=\sum_{\mathbf{z}}\pi_{\mathrm{ref}}(\mathbf{z}|\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{z})$ is the normalization constant. It is easy to check that
    this a valid conditional distribution. This gives us:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $Z(\mathbf{x})=\sum_{\mathbf{z}}\pi_{\mathrm{ref}}(\mathbf{z}|\mathbf{x})\cdot
    F(\mathbf{x},\mathbf{z})$ 是归一化常数。很容易检查这是一个有效的条件分布。这给了我们：
- en: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{off-sup}}(\pi_{\theta};\pi_{\mathrm{ref}})={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: Hence offline supervised methods minimize the re-weighted forward KL-divergence.
    ∎
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，离线监督方法最小化重新加权的前向KL散度。∎
- en: C.2 Characterization of Gradients of Forward and Reverse KL
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2 前向和反向KL梯度的表征
- en: 'For simplicity, let $\mathcal{X}$ be our network that outputs $V$ over discrete
    tokens $1,\ldots,V$ using the softmax function, namely:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让 $\mathcal{X}$ 是我们的网络，使用softmax函数对离散标记 $1,\ldots,V$ 输出 $V$，即：
- en: '|  | $\displaystyle p_{i}(x)=\frac{\exp(f_{i}(x))}{\sum_{k=1}^{V}\exp(f_{k}(x))}$
    |  | (C.2) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}(x)=\frac{\exp(f_{i}(x))}{\sum_{k=1}^{V}\exp(f_{k}(x))}$
    |  | (C.2) |'
- en: for any $x\in\mathcal{X}$.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $x\in\mathcal{X}$。
- en: 'Now assume that for some given input $x$ via SGD:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设对于某个给定输入 $x$ 通过SGD：
- en: '|  | $p_{0}(x)\leftarrow p_{\text{ref}}(x)$ |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{0}(x)\leftarrow p_{\text{ref}}(x)$ |  |'
- en: '|  | $p_{t+1}(x)\leftarrow p_{t}(x)-\eta\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))\big{&#124;}_{p=p_{t}}$
    |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{t+1}(x)\leftarrow p_{t}(x)-\eta\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))\big{&#124;}_{p=p_{t}}$
    |  |'
- en: where $p_{\text{ref}}$ is the true distribution of interest. In contrast, $\mathbb{D}_{\text{KL}}(p||q)$
    in a similar fashion.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{\text{ref}}$ 是感兴趣的真实分布。相反，$\mathbb{D}_{\text{KL}}(p||q)$ 也是类似的。
- en: 'We shall now prove [Lemma 6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2 Case Study:
    Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification
    and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal,
    On-Policy Data"). We break this lemma in two parts. First, let us investigate
    how the gradients $\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)||p(x))\big{|}_{p=p_{t}}$
    look like:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将证明[引理 6.4](#S6.Thmtheorem4 "引理 6.4。 ‣ 6.2 案例研究：模式寻求反向 KL 与模式覆盖前向 KL ‣ 6
    概念统一和理论分析 ‣ LLM 的偏好微调应利用次优的、在政策中的数据")。我们将这个引理分为两部分。首先，让我们研究梯度 $\nabla_{f(x)}\mathbb{D}_{\text{KL}}(q(x)||p(x))\big{|}_{p=p_{t}}$
    的样子：
- en: Lemma C.1.
  id: totrans-358
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 C.1。
- en: 'The gradients of forward and reverse KL are given by:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 前向和反向 KL 的梯度如下：
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))=p_{j}(x)-q_{j}(x)$
    |  | (C.3) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))=p_{j}(x)-q_{j}(x)$
    |  | (C.3) |'
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))=p_{j}(x)\left[\log\frac{p_{j}(x)}{q_{j}(x)}-\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))\right]$
    |  | (C.4) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))=p_{j}(x)\left[\log\frac{p_{j}(x)}{q_{j}(x)}-\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))\right]$
    |  | (C.4) |'
- en: Proof.
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We start with the definition of KL-divergence:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 KL 散度的定义开始：
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$ |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: 'Therefore, we have:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到：
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$
    |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\mathbb{D}_{\text{KL}}(q(x)&#124;&#124;p(x))={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: 'This proves [Equation C.3](#A3.E3 "In Lemma C.1\. ‣ C.2 Characterization of
    Gradients of Forward and Reverse KL ‣ Appendix C More Details on Conceptual Unification
    and Theoretical Analysis ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should
    Leverage Suboptimal, On-Policy Data"). Similarly, we can write:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了[方程 C.3](#A3.E3 "在引理 C.1 中。 ‣ C.2 前向和反向 KL 梯度的特征 ‣ 附录 C 概念统一和理论分析的更多细节 ‣
    附录 ‣ LLM 的偏好微调应利用次优的、在政策中的数据")。类似地，我们可以写成：
- en: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$ |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: 'Now we calculate the partial derivative with respect to $f_{j}$:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算关于 $f_{j}$ 的偏导数：
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}f_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}f_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\frac{\partial}{\partial f_{j}}\log\left(\sum_{k}e^{f_{k}(x)}\right)=\frac{e^{f_{j}(x)}}{\sum_{k}e^{f_{k}(x)}}=p_{j}(x)$
    |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial}{\partial f_{j}}\log\left(\sum_{k}e^{f_{k}(x)}\right)=\frac{e^{f_{j}(x)}}{\sum_{k}e^{f_{k}(x)}}=p_{j}(x)$
    |  |'
- en: And for the third term,
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三项，
- en: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}\log q_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial f_{j}}\frac{\sum_{i}\log q_{i}(x)e^{f_{i}(x)}}{\sum_{k}e^{f_{k}(x)}}={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: 'Putting it all together, we obtain:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，我们得到：
- en: '|  | $\displaystyle\nabla_{f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$
    |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{f_{j}}\mathbb{D}_{\text{KL}}(p(x)&#124;&#124;q(x))={}$
    |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: completing our proof. ∎
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 完成我们的证明。∎
- en: 'Proof for Lemma [6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2 Case Study: Mode-Seeking
    Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual Unification and Theoretical
    Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data"). Now, if the logits $f_{t}$ is given by:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[引理 6.4](#S6.Thmtheorem4 "引理 6.4。 ‣ 6.2 案例研究：模式寻求反向 KL 与模式覆盖前向 KL ‣ 6 概念统一和理论分析
    ‣ LLM 的偏好微调应利用次优的、在政策中的数据") 的证明。现在，如果 logits $f_{t}$ 给定为：'
- en: '|  | $\displaystyle p^{t+1}_{j}(x)={}$ |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{t+1}_{j}(x)={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: '|  | $\displaystyle={}$ |  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle={}$ |  |'
- en: 'Let’s consider what the characterization of $p^{t+1}$ for the forward kl:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑正向 KL 的 $p^{t+1}$ 的表征：
- en: '|  | $\displaystyle p^{t+1}_{j}(x)$ |  |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{t+1}_{j}(x)$ |  |'
- en: 'Noticing that the denominator is just a normalization constant, we can write
    this as:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到分母只是一个归一化常数，我们可以将其写作：
- en: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}\propto\exp\left(-\eta\left(p_{j}^{t}(x)-q_{j}(x)\right)\right)$
    |  | (C.5) |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}\propto\exp\left(-\eta\left(p_{j}^{t}(x)-q_{j}(x)\right)\right)$
    |  | (C.5) |'
- en: 'Similarly the characterization of $p^{t+1}$ for the reverse KL looks like:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，反向 KL 的 $p^{t+1}$ 的表征如下：
- en: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}$ |  | (C.6) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{p^{t+1}_{j}(x)}{p^{t}_{j}(x)}$ |  | (C.6) |'
- en: 'This completes the proof of [Lemma 6.4](#S6.Thmtheorem4 "Lemma 6.4\. ‣ 6.2
    Case Study: Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data").'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了[Lemma 6.4](#S6.Thmtheorem4 "引理 6.4. ‣ 6.2 案例研究：模式寻求的反向 KL 与模式覆盖的正向 KL ‣
    6 概念统一与理论分析 ‣ LLM 的偏好微调应利用次优的、按策略的数据") 的证明。
- en: C.3 Quantifying the Differences Between Forward and Reverse KL
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.3 量化正向和反向 KL 之间的差异
- en: 'In this section we will prove [Theorem 6.5](#S6.Thmtheorem5 "Theorem 6.5\.
    ‣ 6.2 Case Study: Mode-Seeking Reverse KL vs. Mode-Covering Forward KL ‣ 6 Conceptual
    Unification and Theoretical Analysis ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data"): specifically, we will study certain special cases
    to explain the differences between approaches that optimize the forward and reverse
    KL divergences. We drop the subscript $t$ from all terms to prevent notational
    clutter.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将证明[定理 6.5](#S6.Thmtheorem5 "定理 6.5. ‣ 6.2 案例研究：模式寻求的反向 KL 与模式覆盖的正向 KL
    ‣ 6 概念统一与理论分析 ‣ LLM 的偏好微调应利用次优的、按策略的数据")：具体来说，我们将研究一些特殊情况，以解释优化正向和反向 KL 散度的不同方法之间的差异。我们去掉所有项中的下标
    $t$ 以避免符号混乱。
- en: Proof.
  id: totrans-404
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We prove these statements case by case. First we prove the result for Case
    1\. In this scenario, we have the following:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐个证明这些陈述。首先我们证明案例 1 的结果。在这种情况下，我们有如下情况：
- en: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
- en: 'The gap between $\Delta^{f}$ is now given by:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: $\Delta^{f}$ 之间的差距现在由以下给出：
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})-\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$
    |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})-\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})$
    |  |'
- en: Now, we note by mean-value theorem, that there exists a $c_{0}\in[q(\mathbf{x}_{2}),q(\mathbf{x}_{1})]$
    such that,
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们注意到根据均值定理，存在 $c_{0}\in[q(\mathbf{x}_{2}),q(\mathbf{x}_{1})]$ 使得，
- en: '|  | $1$2 |  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Since . This shows the result
    for Case 1.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 当$$p(\mathbf{x}_{1})>时，这个量是正的。这展示了案例 1 的结果。
- en: 'Next we prove Case 2. In this setting we are given $q(\mathbf{x}_{1})=q(\mathbf{x}_{2})\geq
    p(\mathbf{x}_{1})\geq p(\mathbf{x}_{2})+\beta$ are given by:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们证明案例 2。在这种情况下，我们有 $q(\mathbf{x}_{1})=q(\mathbf{x}_{2})\geq p(\mathbf{x}_{1})\geq
    p(\mathbf{x}_{2})+\beta$。
- en: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\leq-\eta\beta.$
    |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\leq-\eta\beta.$
    |  |'
- en: 'On the other hand, the expression for $\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$
    is given by:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，$\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ 的表达式为：
- en: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta^{r}(\mathbf{x}_{1},\mathbf{x}_{2})$ |  |'
- en: '|  |  | $\displaystyle\leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ +\eta\mathrm{D}_{\text{KL}}(p,q)\underbrace{\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)}_{\geq
    0}.$ |  | (C.7) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\
    \leavevmode\nobreak\ \leavevmode\nobreak\ \leavevmode\nobreak\ +\eta\mathrm{D}_{\text{KL}}(p,q)\underbrace{\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)}_{\geq
    0}.$ |  | (C.7) |'
- en: 'Now we analyze each sub-term independently. First, we note the following expression
    for term (b):'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们独立分析每一个子项。首先，我们注意到项 (b) 的以下表达式：
- en: '|  | $\displaystyle(b)\coloneqq$ |  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(b)\coloneqq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'Combining $(a)$, we get:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 结合 $(a)$，我们得到：
- en: '|  | $\displaystyle(a)+(b)$ |  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(a)+(b)$ |  |'
- en: '|  |  | $1$2 |  | (C.8) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (C.8) |'
- en: where $c^{\prime}$. Hence, if $p(\mathbf{x}_{2})$, although $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})<0$.
    This concludes the proof.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c^{\prime}$。因此，如果 $p(\mathbf{x}_{2})$，尽管 $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})<0$。这就完成了证明。
- en: 'Next, we prove Case 3. Similar to the previous case, here $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta(p(\mathbf{x}_{1})-p(\mathbf{x}_{2}))\leq-\eta\beta<0$,
    we need to prove that:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们证明案例 3。与之前的案例类似，这里 $\Delta^{f}(\mathbf{x}_{1},\mathbf{x}_{2})=-\eta(p(\mathbf{x}_{1})-p(\mathbf{x}_{2}))\leq-\eta\beta<0$，我们需要证明：
- en: '|  | $\displaystyle\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\cdot\log
    q(\mathbf{x}_{1})\leavevmode\nobreak\ $ |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right)\cdot\log
    q(\mathbf{x}_{1})\leavevmode\nobreak\ $ |  |'
- en: 'where $\alpha_{0}$. By applying mean value theorem, on the RHS of this equation,
    we note that:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{0}$。通过应用均值定理，我们注意到这个方程的右侧：
- en: '|  | $1$2 |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Then, to attain the desired inequality, we need:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了得到所需的不等式，我们需要：
- en: '|  | $\displaystyle\left[p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right]\cdot\left[\log
    q(\mathbf{x}_{1})-1-\log c^{\prime\prime}\right]\leq\alpha_{0}.$ |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left[p(\mathbf{x}_{1})-p(\mathbf{x}_{2})\right]\cdot\left[\log
    q(\mathbf{x}_{1})-1-\log c^{\prime\prime}\right]\leq\alpha_{0}.$ |  |'
- en: 'Note that since $c^{\prime\prime}\geq p(\mathbf{x}_{2})$, such that:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到由于 $c^{\prime\prime}\geq p(\mathbf{x}_{2})$，使得：
- en: '|  | $\displaystyle q(\mathbf{x}_{1})$ |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(\mathbf{x}_{1})$ |  |'
- en: '|  |  | $\displaystyle\implies\log q(\mathbf{x}_{1})\leq\log\mathbf{c}_{1}+\log
    c^{\prime\prime},$ |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\implies\log q(\mathbf{x}_{1})\leq\log\mathbf{c}_{1}+\log
    c^{\prime\prime},$ |  |'
- en: the LHS of this equation will be smaller than the RHS $\alpha_{0}$. This proves
    the result for this case. ∎
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程的左侧将小于右侧的 $\alpha_{0}$。这证明了此情况下的结果。∎
- en: Appendix D Additional Algorithmic Details
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 额外算法细节
- en: D.1 Score/Reward Standardization
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.1 评分/奖励标准化
- en: 'Online methods such as PPO or RWR that uses a learned reward model can suffer
    from gradient variance issues due to the differences in the reward score. In particular,
    adding or subtracting a baseline $b$ sampled from policy $\pi_{\theta}$ as:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 像 PPO 或 RWR 这样的在线方法，使用学习到的奖励模型可能会因为奖励分数的差异而遭遇梯度方差问题。特别是，添加或减去一个从策略 $\pi_{\theta}$
    中抽样的基线 $b$ 如下：
- en: '|  | $\bar{r}_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})=\frac{r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})-\hat{\mu}}{\hat{\sigma}}$
    |  | (D.1) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{r}_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})=\frac{r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})-\hat{\mu}}{\hat{\sigma}}$
    |  | (D.1) |'
- en: where $\hat{\mu}=\frac{1}{\mathcal{B}}\sum_{i=1}^{\mathcal{B}}r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})$.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mu}=\frac{1}{\mathcal{B}}\sum_{i=1}^{\mathcal{B}}r_{\phi}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})$。
- en: D.2 IPO
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.2 IPO
- en: 'IPO (Gheshlaghi Azar et al., [2023](#bib.bib18)) is a contrastive algorithm
    similar to DPO. The key difference between them is their loss function: DPO optimizes
    the negative log-sigmoid loss whereas IPO optimizes an MSE-type objective. Formally,
    the IPO objective is:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: IPO (Gheshlaghi Azar 等， [2023](#bib.bib18)) 是一个类似于 DPO 的对比算法。它们之间的主要区别在于损失函数：DPO
    优化负对数 sigmoid 损失，而 IPO 优化的是 MSE 类型的目标。形式上，IPO 的目标是：
- en: '|  | $1$2 |  | (D.2) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (D.2) |'
- en: where $\tau$.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$。
- en: Appendix E Method Hyperparameters
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 E 方法超参数
- en: We did an extensive sweep over hyperparameters for individual offline and online
    algorithms for the language model experiments. We built our algorithm implementations
    off of the Huggingface TRL implementation (von Werra et al., [2020](#bib.bib49)).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对语言模型实验中的离线和在线算法进行了广泛的超参数搜索。我们基于 Huggingface TRL 实现（von Werra 等，[2020](#bib.bib49)）构建了我们的算法实现。
- en: E.1 Standardized Parameters (Consistent for all Methods)
  id: totrans-449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1 标准化参数（所有方法一致）
- en: 'Table 4: Algorithm Agnostic Hyperparamters'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 算法无关超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $B$ | 64 | Batch Size |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| $B$ | 64 | 批量大小 |'
- en: '| $B_{mini}$ | 8 | Mini-Batch Size |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| $B_{mini}$ | 8 | 小批量大小 |'
- en: '| $G$ | 8 | Gradient Accumulation Steps |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| $G$ | 8 | 梯度累积步数 |'
- en: '| $\hat{\pi}_{\theta}$ | Pythia1.4B, Mistral-7b | Policy Architecture |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{\pi}_{\theta}$ | Pythia1.4B, Mistral-7b | 策略架构 |'
- en: '| $\hat{R}_{\theta}$ | Pythia410M, Mistral-7B | Reward Model Architecture |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{R}_{\theta}$ | Pythia410M, Mistral-7B | 奖励模型架构 |'
- en: '| optimizer | Adam | Gradient Optimizer |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| optimizer | Adam | 梯度优化器 |'
- en: 'Table 5: Sampling Hyperparamters'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 采样超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| top_k | 0.0 | Disables top-k sampling |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| top_k | 0.0 | 禁用 top-k 采样 |'
- en: '| top_p | 1.0 | Disables nucleus sampling |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| top_p | 1.0 | 禁用核心采样 |'
- en: '| do_sample | True | Enables sampling |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| do_sample | True | 启用采样 |'
- en: '| max_new_tokens | 256 | Maximum number of new tokens to generate |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| max_new_tokens | 256 | 最大生成新令牌数 |'
- en: '| temperature | 1.0 | Sets sampling temperature (1.0 for default) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| temperature | 1.0 | 设置采样温度（默认为 1.0） |'
- en: '| use_cache | True | Uses past key/values attentions if supported by the model
    |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| use_cache | True | 如果模型支持，使用过去的关键/值注意力 |'
- en: E.2 DPO (Rafailov et al., [2023](#bib.bib38))
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2 DPO（Rafailov 等，[2023](#bib.bib38)）
- en: 'Table 6: DPO Hyperparameters'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: DPO 超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| lr | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| lr | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | 学习率 |'
- en: '| $\beta$, $0.5$ | KL weight |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$, $0.5$ | KL 权重 |'
- en: E.3 Pref-FT (Dubois et al., [2024](#bib.bib14))
  id: totrans-474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.3 Pref-FT（Dubois 等，[2024](#bib.bib14)）
- en: 'Table 7: Pref-FT/Binary FeedMe Hyperparameters'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: Pref-FT/二进制 FeedMe 超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6 | learning rate |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6 | 学习率 |'
- en: E.4 PPO (Schulman et al., [2017](#bib.bib41))
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.4 PPO（Schulman 等，[2017](#bib.bib41)）
- en: 'Table 8: PPO Hyperparameters'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: PPO 超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | Learning rate. |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | 学习率。 |'
- en: '| vf_coef | 0.1 | Coefficient for the value function loss. |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| vf_coef | 0.1 | 价值函数损失的系数。 |'
- en: '| adap_kl_ctrl | True | Enables adaptive KL penalty control. |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| adap_kl_ctrl | True | 启用自适应 KL 惩罚控制。 |'
- en: '| init_kl_coef | 0.2 | Initial coefficient for KL penalty. |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| init_kl_coef | 0.2 | KL 惩罚的初始系数。 |'
- en: '| target_kl | 0.1 | Target KL divergence for policy updates. |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| target_kl | 0.1 | 策略更新的目标 KL 散度。 |'
- en: '| $N$ | 1 | actions per prompt |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| $N$ | 1 | 每个提示的动作 |'
- en: E.5 RWR
  id: totrans-488
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.5 RWR
- en: 'Table 9: RWR Hyperparameters'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: RWR 超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | 学习率 |'
- en: '| $\beta$, $20$ | temperature |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$, $20$ | 温度 |'
- en: '| $N$ | 1 | actions per prompt |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| $N$ | 1 | 每个提示的动作 |'
- en: E.6 Iterated Best-of-N (Mukobi et al., [2023](#bib.bib32))
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.6 迭代最佳的 N（Mukobi 等，[2023](#bib.bib32)）
- en: 'Table 10: Iterated BofN Hyperparameters'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 迭代 BofN 超参数'
- en: '| Hyperparameters | Values | Description |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | learning rate |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| $\eta$ | 1e-7, 5e-7, 1e-6, 5e-6, 1e-5 | 学习率 |'
- en: '| $N$ | actions per prompt |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| $N$ | 每个提示的动作'
- en: Appendix F Code For Running Experiments
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F 运行实验的代码
- en: 'We have made the code for this project public in this [repository](https://github.com/Asap7772/understanding-rlhf).
    The additional datasets used in our experiments are listed below:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已在这个 [仓库](https://github.com/Asap7772/understanding-rlhf) 公开了本项目的代码。我们实验中使用的附加数据集列在下面：
- en: •
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[Min Length](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_minlength)'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[最小长度](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_minlength)'
- en: •
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[Mode Length](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_modelength)'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[模式长度](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data_modelength)'
- en: •
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[Skew Length](https://huggingface.co/datasets/Asap7772/alpaca_skewexp_minlength_merged)'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[Skew Length](https://huggingface.co/datasets/Asap7772/alpaca_skewexp_minlength_merged)'
- en: •
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[Relabelled AlpacaFarm](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data)'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[重标记的AlpacaFarm](https://huggingface.co/datasets/Asap7772/relabeled_alpacafarm_pythiasft_20K_preference_data)'
- en: 'We gratefully acknowledge the following codebases: [TRL](https://github.com/huggingface/trl) (von
    Werra et al., [2020](#bib.bib49)), [HALOs](https://github.com/ContextualAI/HALOs) (Ethayarajh
    et al., [2023](#bib.bib16)), [minGPT](https://github.com/karpathy/minGPT) ([Karpathy,](#bib.bib23)
    ), [DrQ-v2](https://github.com/facebookresearch/drqv2) (Yarats et al., [2021a](#bib.bib55),
    [b](#bib.bib56)) and [PAINT](https://github.com/tajwarfahim/proactive_interventions) Xie
    et al. ([2022](#bib.bib52)).'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衷心感谢以下代码库：[TRL](https://github.com/huggingface/trl) (von Werra等人，[2020](#bib.bib49))，[HALOs](https://github.com/ContextualAI/HALOs)
    (Ethayarajh等人，[2023](#bib.bib16))，[minGPT](https://github.com/karpathy/minGPT)
    ([Karpathy,](#bib.bib23))，[DrQ-v2](https://github.com/facebookresearch/drqv2)
    (Yarats等人，[2021a](#bib.bib55)，[b](#bib.bib56)) 和 [PAINT](https://github.com/tajwarfahim/proactive_interventions)
    Xie等人 ([2022](#bib.bib52))。
- en: Appendix G More on Didactic Bandit Problems
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录G 关于教学bandit问题的更多内容
- en: G.1 Problem Setup
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.1 问题设置
- en: Here we present details of our didactic bandit problem. The reference policy
    shown in [Figure 2](#S4.F2 "In 4.2 Tasks and Datasets ‣ 4 Research Questions and
    Analysis Setup ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy
    Data") is obtained by collecting 10000 samples from a Cauchy distribution with
    location $x_{0}=-0.7$, we label these bins $0,\ldots,99$ sequentially, and calculate
    the frequency of samples that fell into each bin. Finally, we define,
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们呈现了我们的教学bandit问题的细节。参考策略如[图2](#S4.F2 "在4.2任务和数据集 ‣ 4 研究问题和分析设置 ‣ LLM的偏好微调应该利用次优的、在策略中的数据")所示，是通过从位置为$x_{0}=-0.7$的Cauchy分布中收集10000个样本获得的，我们按顺序标记这些箱为$0,\ldots,99$，并计算每个箱中样本的频率。最后，我们定义，
- en: '|  | $\pi_{\mathrm{ref}}(a_{i})=\frac{\text{Freq}(bin_{i})}{10000}$ |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{\mathrm{ref}}(a_{i})=\frac{\text{Freq}(bin_{i})}{10000}$ |  |'
- en: 'The reward functions $\mathbf{R}_{1}$ are defined as:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数$\mathbf{R}_{1}$定义为：
- en: '|  | $\mathbf{R}_{1}(a)=\exp\left(-\left(\frac{a-70}{10}\right)^{2}\right)$
    |  |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{R}_{1}(a)=\exp\left(-\left(\frac{a-70}{10}\right)^{2}\right)$
    |  |'
- en: and
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\mathbf{R}_{2}(a)=\exp\left(-\left(\frac{a-20}{10}\right)^{2}\right)$
    |  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{R}_{2}(a)=\exp\left(-\left(\frac{a-20}{10}\right)^{2}\right)$
    |  |'
- en: G.2 Algorithmic Details
  id: totrans-520
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.2 算法细节
- en: 'In the bandit setting, we consider five algorithms: (1) Best-of-N, (2) IPO,
    (3) REINFORCE, (4) PPO and (5) RWR.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在bandit设置中，我们考虑了五种算法：（1）Best-of-N，（2）IPO，（3）REINFORCE，（4）PPO 和（5）RWR。
- en: G.2.1 Best-of-N
  id: totrans-522
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: G.2.1 Best-of-N
- en: Best-of-N is similar to SuperHF (Mukobi et al., [2023](#bib.bib32))/ReST (Gulcehre
    et al., [2023](#bib.bib19)) and in some way their simplification for the bandit
    setting. Best-of-N collects $N$, and based on these rewards, choose the best action
    $\mathbf{y}_{\text{best}}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})$.
    Finally, the loss function is the negative log-likelihood of this best action.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: Best-of-N类似于SuperHF (Mukobi等人，[2023](#bib.bib32))/ReST (Gulcehre等人，[2023](#bib.bib19))，在某种程度上，它们简化了bandit设置。Best-of-N收集$N$个样本，并基于这些奖励选择最佳动作$\mathbf{y}_{\text{best}}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})$。最终，损失函数是该最佳动作的负对数似然。
- en: '|  | $\mathcal{L}_{\text{bofn}}(\pi_{\theta};\mathbf{x},\mathbf{y}_{1},\ldots,\mathbf{y}_{N})=-\log\pi_{\theta}(\mathbf{y}_{\text{best}}&#124;\mathbf{x})$
    |  |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{bofn}}(\pi_{\theta};\mathbf{x},\mathbf{y}_{1},\ldots,\mathbf{y}_{N})=-\log\pi_{\theta}(\mathbf{y}_{\text{best}}&#124;\mathbf{x})$
    |  |'
- en: 'In both the online and offline setting, we have a fixed set of prompts $\mathcal{D}_{\text{prompts}}$,
    we can form a training set as:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线和离线设置中，我们有一组固定的提示$\mathcal{D}_{\text{prompts}}$，我们可以形成一个训练集，如下所示：
- en: '|  | $\mathcal{D}_{\text{train}}(\mathcal{D}_{\text{prompts}},\pi)=\{(\mathbf{x},\mathbf{y}):\mathbf{x}\in\mathcal{D}_{\text{prompts}},\mathbf{y}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})\text{
    where }\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi_{\mathrm{ref}}(.&#124;\mathbf{x})\}$
    |  |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{\text{train}}(\mathcal{D}_{\text{prompts}},\pi)=\{(\mathbf{x},\mathbf{y}):\mathbf{x}\in\mathcal{D}_{\text{prompts}},\mathbf{y}=\operatorname*{arg\,max}_{\mathbf{y}_{i}}\mathbf{R}(\mathbf{x},\mathbf{y}_{i})\text{
    where }\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi_{\mathrm{ref}}(.&#124;\mathbf{x})\}$
    |  |'
- en: In the offline setting, we collect a fixed training dataset where actions are
    sampled from $\pi_{\mathrm{ref}}$, after every $T$ gradient steps, and discard
    the previous dataset.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线设置中，我们收集了一个固定的训练数据集，其中的动作从$\pi_{\mathrm{ref}}$中采样，每进行$T$次梯度步骤后，丢弃之前的数据集。
- en: 'To show the efficacy of negative gradient, we can also directly add a term
    to this loss function minimizing log probability on dispreferred actions. Explicitly,
    we consider the following loss function:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示负梯度的有效性，我们还可以直接向这个损失函数中添加一个项，最小化对不喜欢的动作的对数概率。明确地，我们考虑以下损失函数：
- en: '|  | $1$2 |  |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\beta$, in practice we only minimize the probability of dispreferred
    actions if it is above a certain threshold.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$，实际上我们仅在不喜欢的动作的概率超过某个阈值时才会最小化。
- en: G.2.2 IPO
  id: totrans-531
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: G.2.2 IPO
- en: In contrast, IPO uses the loss function defined in [Equation D.2](#A4.E2 "In
    D.2 IPO ‣ Appendix D Additional Algorithmic Details ‣ Appendices ‣ Preference
    Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data"). While regular
    IPO is an offline algorithm that uses a fixed preference dataset $\mathcal{D}_{\text{pref}}$,
    we can generate completions $\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi(.|\mathbf{x})$
    and $\mathbf{y}_{j}$ tuples.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，IPO 使用了在[方程 D.2](#A4.E2 "在 D.2 IPO ‣ 附录 D 附加算法细节 ‣ 附录 ‣ LLM 的偏好微调应利用次优的在线数据")中定义的损失函数。虽然常规
    IPO 是一种离线算法，使用固定的偏好数据集 $\mathcal{D}_{\text{pref}}$，我们可以生成完成 $\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{N}\sim\pi(.|\mathbf{x})$
    和 $\mathbf{y}_{j}$ 元组。
- en: In the offline setting, the preference dataset is collected by generating samples
    from the reference policy $\pi_{\mathrm{ref}}$ gradient steps, and discard the
    previous dataset.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线设置中，偏好数据集是通过从参考策略 $\pi_{\mathrm{ref}}$ 的梯度步骤中生成样本收集的，并丢弃之前的数据集。
- en: G.2.3 REINFORCE
  id: totrans-534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: G.2.3 REINFORCE
- en: 'For REINFORCE, we sample $\mathbf{y}\sim\pi_{\theta}(.|\mathbf{x})$, and use
    the following loss:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 REINFORCE，我们从 $\mathbf{y}\sim\pi_{\theta}(.|\mathbf{x})$ 中采样，并使用以下损失：
- en: '|  | $1$2 |  |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: G.2.4 PPO
  id: totrans-537
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: G.2.4 PPO
- en: 'For PPO, let $\pi_{\text{gen}}$. Then we use the following loss function:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PPO，设 $\pi_{\text{gen}}$。然后我们使用以下损失函数：
- en: '|  | $1$2 |  |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where <math id=$$ is a hyperparameter that controls how much we clip off-policy
    updates.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math id=$$ 是一个超参数，用于控制我们裁剪离策略更新的程度。
- en: G.2.5 RWR
  id: totrans-541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: G.2.5 RWR
- en: 'For RWR, we use the following loss function:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RWR，我们使用以下损失函数：
- en: '|  | $1$2 |  |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\beta$ in our experiments unless otherwise noted.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\beta$ 在我们的实验中，除非另有说明。
- en: G.3 Experiment Details
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.3 实验细节
- en: For all experiments, we use $N=10$ randomly sampled prompts from tokens $\{0,\ldots,99\}$
    times for all experiments. We set $\tau=0.05$ and $\pi_{\mathrm{ref}}$.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们使用 $N=10$ 从令牌 $\{0,\ldots,99\}$ 中随机采样的提示进行所有实验。我们设定 $\tau=0.05$ 和 $\pi_{\mathrm{ref}}$。
- en: 'For all experiments, we use a small GPT (Radford et al., [2018](#bib.bib37);
    Brown et al., [2020](#bib.bib5))-like transformer architecture (named ‘GPT-Nano’)
    with 0.9M parameters. We took the implementation from this public repository:
    [minGPT](https://github.com/karpathy/minGPT) ([Karpathy,](#bib.bib23) ).'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们使用了一个小型 GPT (Radford et al., [2018](#bib.bib37); Brown et al., [2020](#bib.bib5))
    类似的变换器架构（命名为‘GPT-Nano’），具有 0.9M 参数。我们从这个公共仓库中获取了实现：[minGPT](https://github.com/karpathy/minGPT)
    ([Karpathy,](#bib.bib23))。
- en: Appendix H Additional Experiments on Synthetic LLM Setup
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 H 关于合成 LLM 设置的附加实验
- en: H.1 Performance of Various Algorithms on the Mode Length Setting
  id: totrans-549
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.1 各种算法在模式长度设置下的表现
- en: '[Figure 20](#A8.F20 "In H.1 Performance of Various Algorithms on the Mode Length
    Setting ‣ Appendix H Additional Experiments on Synthetic LLM Setup ‣ Appendices
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")
    shows the performance of various algorithms in the mode length setup. We see that
    all algorithms perform similarly here.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 20](#A8.F20 "在 H.1 各种算法在模式长度设置下的表现 ‣ 附录 H 关于合成 LLM 设置的附加实验 ‣ 附录 ‣ LLM 的偏好微调应利用次优的在线数据")
    显示了各种算法在模式长度设置下的表现。我们看到所有算法在这里的表现相似。'
- en: '![Refer to caption](img/330c456c96329e99bf580c55817e9cb8.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/330c456c96329e99bf580c55817e9cb8.png)'
- en: 'Figure 20: Performance of various algorithms on mode length setup. Distance
    to mode of the completion lengths from $\pi_{\text{ref}}$, 203, for different
    algorithms. All algorithms perform similarly, and varying degrees of on-policyness
    does not generally degrade performance.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：各种算法在模式长度设置下的表现。完成长度距离模式 $\pi_{\text{ref}}$ 为 203，不同算法的表现。所有算法的表现类似，并且在线性程度的不同一般不会降低性能。
- en: H.2 Effect of On-policy Samples vs Samples from an Older Policy in Synthetic
    Length Settings
  id: totrans-553
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.2 在线样本与来自旧策略的样本在合成长度设置中的影响
- en: '[Figures 21](#A8.F21 "In H.2 Effect of On-policy Samples vs Samples from an
    Older Policy in Synthetic Length Settings ‣ Appendix H Additional Experiments
    on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning of LLMs Should Leverage
    Suboptimal, On-Policy Data") and [22](#A8.F22 "Figure 22 ‣ H.2 Effect of On-policy
    Samples vs Samples from an Older Policy in Synthetic Length Settings ‣ Appendix
    H Additional Experiments on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") shows the effect of using
    on-policy samples vs samples from an older policy for RWR in the synthetic length
    experiments.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 21](#A8.F21 "在 H.2 合成长度设置中政策内样本与旧政策样本的效果 ‣ 附录 H 合成 LLM 设置的额外实验 ‣ 附录 ‣ LLM
    的偏好微调应利用次优的、在政策内的数据") 和 [图 22](#A8.F22 "图 22 ‣ H.2 合成长度设置中政策内样本与旧政策样本的效果 ‣ 附录
    H 合成 LLM 设置的额外实验 ‣ 附录 ‣ LLM 的偏好微调应利用次优的、在政策内的数据") 显示了在合成长度实验中使用政策内样本与旧政策样本用于 RWR
    的效果。'
- en: '![Refer to caption](img/86661445b67b80b2399d6077e4783dea.png)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/86661445b67b80b2399d6077e4783dea.png)'
- en: 'Figure 21: On-policy sampling on Min Length (RWR). Effect of using on-policy
    samples vs samples from an older policy for RWR and the min length setup. In all
    experiments, the mini-batch size to calculate the gradient is fixed at 64, and
    we sample batch size $B$ thus makes the algorithm make updates on samples from
    an older policy. Left: average completion length (lower the better), and Right:
    proxy reward vs gradient steps. Being more on-policy results in better performance.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：在 Min Length（RWR）上进行政策内采样。使用政策内样本与旧政策样本进行 RWR 和最小长度设置的效果。在所有实验中，用于计算梯度的迷你批次大小固定为
    64，并且我们采样批次大小 $B$，从而使算法在旧政策样本上进行更新。左：平均完成长度（越低越好），右：代理奖励与梯度步骤。更多的政策内采样导致更好的性能。
- en: '![Refer to caption](img/acf2581cf29ec73e828c44f55c287501.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/acf2581cf29ec73e828c44f55c287501.png)'
- en: 'Figure 22: On-policy sampling on Skew Length (RWR). Effect of using on-policy
    samples vs samples from an older policy for RWR and the skew length setup. Left:
    average completion length (lower the better), and Right: proxy reward vs gradient
    steps. Being more on-policy results in better performance.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：在 Skew Length（RWR）上进行政策内采样。使用政策内样本与旧政策样本进行 RWR 和偏斜长度设置的效果。左：平均完成长度（越低越好），右：代理奖励与梯度步骤。更多的政策内采样导致更好的性能。
- en: H.3 Sample Reuse in Synthetic LLM Settings
  id: totrans-559
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.3 合成 LLM 设置中的样本重用
- en: '[Figure 23](#A8.F23 "In H.3 Sample Reuse in Synthetic LLM Settings ‣ Appendix
    H Additional Experiments on Synthetic LLM Setup ‣ Appendices ‣ Preference Fine-Tuning
    of LLMs Should Leverage Suboptimal, On-Policy Data") shows the effect of sample
    reuse in the Skew Length setting: similar to Min Length ( [Figure 11](#S5.F11
    "In 5.1.2 Takeaway 2: On-Policy Sample Reuse Can Enable Leveraging Off-Policy
    Data ‣ 5.1 Question 1: The Role of On-Policy Sampling ‣ 5 Empirical Analysis Results
    ‣ Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data")),
    some sample reuse can improve sample efficiency. but excessive sample reuse can
    also hurt performance. Also, we see PPO with importance clipping is much better
    at sample reuse than Best-of-N.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 23](#A8.F23 "在 H.3 合成 LLM 设置中的样本重用 ‣ 附录 H 合成 LLM 设置的额外实验 ‣ 附录 ‣ LLM 的偏好微调应利用次优的、在政策内的数据")
    显示了在 Skew Length 设置中样本重用的效果：类似于 Min Length（[图 11](#S5.F11 "在 5.1.2 主要结论 2: 在政策内样本重用可以利用政策外数据
    ‣ 5.1 问题 1: 在政策内采样的作用 ‣ 5 实证分析结果 ‣ LLM 的偏好微调应利用次优的、在政策内的数据")），一些样本重用可以提高样本效率，但过度的样本重用也可能损害性能。同时，我们发现，带有重要性裁剪的
    PPO 在样本重用方面要比 Best-of-N 好得多。'
- en: '![Refer to caption](img/5619f5f03a7cd6a0a720bfdf7599d0c6.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5619f5f03a7cd6a0a720bfdf7599d0c6.png)'
- en: 'Figure 23: Effect of on-policy sample reuse in the Skew Length scenario. Average
    completion length (i.e., the lower the better) vs gradient steps for different
    numbers of inner iteration steps, $T$ implies that the algorithm is more off-policy.
    Observe that some sample reuse can improve sample efficiency (T = 2 and T = 4
    outperform T = 1), but excessive sample reuse can hurt performance (T = 8 becomes
    unstable for PPO). Also note that algorithms with mechanisms to control off-policy
    updates such as PPO with importance-weight clipping are suited to perform better
    in the off-policy sample reuse setting.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：在偏斜长度场景中，策略样本重用的效果。不同内循环步骤数的平均完成长度（即越低越好）与梯度步骤的关系，$T$ 表示算法偏离策略的程度。观察到，适度的样本重用可以提高样本效率（T
    = 2 和 T = 4 优于 T = 1），但过度样本重用可能会影响性能（T = 8 对 PPO 不稳定）。还要注意，具有控制偏离策略更新机制的算法，如具有重要性加权裁剪的
    PPO，更适合在偏离策略样本重用设置中表现良好。
