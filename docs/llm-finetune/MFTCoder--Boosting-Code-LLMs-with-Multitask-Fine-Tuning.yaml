- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MFTCoder：通过多任务微调提升代码大语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02303](https://ar5iv.labs.arxiv.org/html/2311.02303)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02303](https://ar5iv.labs.arxiv.org/html/2311.02303)
- en: Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming
    Liang,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘冰畅、陈超宇、龚梓、廖聪、王焕、雷智超、梁铭，
- en: 'Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu, Jianguo Li¹¹footnotemark:
    1'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈大军、沈敏、周海莲、姜伟、余杭、李建国¹¹脚注标记：1
- en: 'Ant Group, China Corresponding Author: {hyu.hugo, lijg.zero}@antgroup.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 蚂蚁集团，中国 联系作者：{hyu.hugo, lijg.zero}@antgroup.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Code LLMs have emerged as a specialized research field, with remarkable studies
    dedicated to enhancing model’s coding capabilities through fine-tuning on pre-trained
    models. Previous fine-tuning approaches were typically tailored to specific downstream
    tasks or scenarios, which meant separate fine-tuning for each task, requiring
    extensive training resources and posing challenges in terms of deployment and
    maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness
    among different code-related tasks. To overcome these limitations, we present
    a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel
    fine-tuning on multiple tasks. By incorporating various loss functions, we effectively
    address common challenges in multi-task learning, such as data imbalance, varying
    difficulty levels, and inconsistent convergence speeds. Extensive experiments
    have conclusively demonstrated that our multi-task fine-tuning approach outperforms
    both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble
    of tasks. Moreover, MFTCoder offers efficient training capabilities, including
    efficient data tokenization modes and PEFT fine-tuning, resulting in significantly
    improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly
    integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen.
    Leveraging the CodeLLama foundation, our MFTCoder fine-tuned model, CodeFuse-CodeLLama-34B,
    achieves an impressive pass@1 score of 74.4% on the HumaneEval benchmark, surpassing
    GPT-4 performance (67%, zero-shot). MFTCoder is open-sourced at [https://github.com/codefuse-ai/MFTCOder](https://github.com/codefuse-ai/MFTCOder)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 代码大语言模型（Code LLMs）作为一个专业研究领域已逐渐兴起，并且有显著研究致力于通过在预训练模型上进行微调来提升模型的编码能力。以往的微调方法通常针对特定的下游任务或场景，这意味着每个任务都需要单独微调，耗费大量训练资源，并且在部署和维护方面面临挑战。此外，这些方法未能利用不同代码相关任务之间的内在关联性。为了克服这些局限性，我们提出了一种多任务微调框架，MFTCoder，它支持对多个任务进行同时和并行的微调。通过结合各种损失函数，我们有效地解决了多任务学习中的常见挑战，如数据不平衡、难度差异和收敛速度不一致。大量实验证明，我们的多任务微调方法在性能上优于单任务微调和混合任务微调。此外，MFTCoder
    提供了高效的训练能力，包括高效的数据标记模式和 PEFT 微调，与传统微调方法相比，速度显著提升。MFTCoder 能与多个主流开源 LLMs 无缝集成，如
    CodeLLama 和 Qwen。基于 CodeLLama 的 MFTCoder 微调模型 CodeFuse-CodeLLama-34B 在 HumaneEval
    基准测试中取得了 74.4% 的令人印象深刻的 pass@1 分数，超越了 GPT-4 的表现（67%，零-shot）。MFTCoder 已开源于 [https://github.com/codefuse-ai/MFTCOder](https://github.com/codefuse-ai/MFTCOder)
- en: '*Keywords* Large Language Model  $\cdot$ Multi-task Learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 大语言模型  $\cdot$ 多任务学习'
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The paradigm-shifting emergence of ChatGPT¹¹1https://openai.com/blog/chatgpt,
    powered by both GPT-3.5 and GPT-4 OpenAI ([2023](#bib.bib43)), has set ablaze
    the landscape of research and development in the realm of large language models
    (LLMs). This breakthrough has further sparked the interest in leveraging LLMs
    for code understanding and generation, commonly referred to as Code LLMs. By pretraining
    on extensive code data sources such as the Github public data, these Code LLMs
    can acquire comprehensive contextual representations that can be applied to various
    code-related tasks .
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT¹¹1https://openai.com/blog/chatgpt的出现，引发了对大语言模型（LLMs）研究和发展的范式变革，该技术由
    GPT-3.5 和 GPT-4 支持（OpenAI [2023](#bib.bib43)）。这一突破进一步激发了对利用 LLMs 进行代码理解和生成的兴趣，通常称为代码大语言模型。通过在大量代码数据源如
    Github 公共数据上进行预训练，这些代码大语言模型能够获得全面的上下文表示，并可应用于各种代码相关任务。
- en: While the pretraining stage of (Code) LLMs seek to ensure their generalizability
    to different downstream tasks, the subsequent finetuning stage typically only
    adapt the (Code) LLMs to a specific task or a scenario. However, this approach
    overlooks two critical challenges. Firstly, it involves resource-intensive individual
    finetuning of large LLMs for each task, which hinders efficient deployment in
    production. Secondly, the interrelated nature of code domain tasks suggests that
    joint finetuning can enhance performance compared to separate finetuning. It is
    therefore imperative to conduct multitask finetuning, enabling simultaneous handling
    of all tasks while leveraging the strengths of related tasks to enhance performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管（代码）大语言模型（LLMs）的预训练阶段旨在确保其对不同下游任务的泛化能力，但随后微调阶段通常仅将（代码）LLMs适应于特定任务或场景。然而，这种方法忽视了两个关键挑战。首先，它涉及对每个任务进行资源密集型的大型LLMs单独微调，这妨碍了在生产中的高效部署。其次，代码领域任务的相互关联性表明，与单独微调相比，联合微调可以提高性能。因此，进行多任务微调是必要的，它能够同时处理所有任务，同时利用相关任务的优势来提升性能。
- en: 'As an illuminating example, suppose we have two related tasks: code completion
    and code summarization. Code completion involves predicting the next line of code
    based on a partial code snippet, while code summarization aims to generate a concise
    human-readable summary of a given code snippet. Traditionally, separate models
    would be fine-tuned for each task, resulting in resource-intensive duplication.
    However, code completion and code summarization have inherent connections. Completion
    of a code snippet relies on understanding the overall functionality and purpose,
    while generating an accurate summary requires comprehending the structure, dependencies,
    and intended functionality. By employing multitask learning, a single model can
    be trained to jointly learn both tasks, leveraging shared knowledge and patterns,
    leading to improved performance on both tasks. The model understands the contextual
    dependencies between code elements, aiding in predicting the next snippet and
    generating informative summaries. Furthermore, multitask learning offers additional
    benefits beyond individual task performance: the shared representation between
    tasks helps mitigate overfitting, promote better generalization, and enhance the
    model’s ability to handle data scarcity for specific tasks. If code completion
    has a larger training dataset than code summarization, the model can leverage
    the abundance of completion data to enhance performance in summarization, effectively
    addressing data scarcity challenges. Multitask learning even enables the model
    to handle unseen but related tasks without specific training data. Overall, multitask
    learning allows models to jointly learn multiple related tasks, benefiting from
    shared knowledge, improving performance, enhancing generalization, and handling
    data scarcity.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个生动的例子，假设我们有两个相关的任务：代码补全和代码总结。代码补全涉及根据部分代码片段预测下一行代码，而代码总结则旨在生成给定代码片段的简洁人类可读摘要。传统上，每个任务会为其单独微调模型，导致资源密集型的重复工作。然而，代码补全和代码总结具有固有的联系。代码片段的补全依赖于理解整体功能和目的，而生成准确的摘要需要理解结构、依赖关系和预期功能。通过采用多任务学习，单个模型可以同时学习这两个任务，利用共享知识和模式，从而提高两个任务的性能。该模型理解代码元素之间的上下文依赖关系，有助于预测下一片段并生成信息丰富的摘要。此外，多任务学习还提供了超越个别任务性能的额外好处：任务之间的共享表示有助于减轻过拟合，促进更好的泛化，并增强模型在特定任务上的数据稀缺处理能力。如果代码补全的训练数据集大于代码总结，模型可以利用补全数据的丰富性来提高总结的性能，有效应对数据稀缺挑战。多任务学习甚至使模型能够处理未见过的但相关的任务，而无需特定的训练数据。总体而言，多任务学习允许模型共同学习多个相关任务，利用共享知识，提高性能，增强泛化能力，并处理数据稀缺问题。
- en: Despite the importance of multitask learning for finetuning, only a handful
    of existing studies have explored this approach in the domain of NLP Raffel et al.
    ([2023](#bib.bib46)); Aghajanyan et al. ([2021](#bib.bib2)); Aribandi et al. ([2022](#bib.bib6)).
    These studies incorporate multi-task data and merge it for large-scale model learning,
    without explicitly separating the tasks. Unfortunately, these studies tend to
    prioritize tasks with larger sample sizes, disregarding tasks with smaller sample
    sizes. Furthermore, they fail to ensure equal convergence speed among tasks, leading
    to over-optimization of some tasks and under-optimization of others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多任务学习在微调中的重要性不言而喻，但只有少数现有研究在NLP领域探索了这种方法 Raffel et al. ([2023](#bib.bib46));
    Aghajanyan et al. ([2021](#bib.bib2)); Aribandi et al. ([2022](#bib.bib6))。这些研究将多任务数据整合到大规模模型学习中，而没有明确分离这些任务。不幸的是，这些研究倾向于优先考虑样本量较大的任务，而忽视了样本量较小的任务。此外，它们未能确保任务之间的收敛速度相等，导致某些任务过度优化而其他任务优化不足。
- en: In this paper, we focus on multitask fine-tuing (MFT) of (Code) LLMs, in order
    to guarantee equitable attention to tasks with varying sample sizes and approximately
    similar optimization progress. In particular, our attention is on Code LLMs, as
    code domain tasks often exhibit correlations, and so we name our approach MFTCoder.
    We emphasize that MFTcoder can be extended to an arbitrary set of related-NLP
    tasks in a straighforward manner. To enhance the efficiency of MFTCoder, we incorporate
    parameter-efficient fine-tuning techniques, including LoRA Hu et al. ([2021](#bib.bib24))
    and QLoRA Dettmers et al. ([2023](#bib.bib18)). Experimental results demonstrate
    that multi-task models trained using the MFT approach outperform those fine-tuned
    individually for each task or by merging data from multiple tasks. We further
    validate the effectiveness of MFTCoder on various baseline pretrained LLMs, such
    as Qwen Bai et al. ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)),
    Llama Touvron et al. ([2023a](#bib.bib51)), Llama 2 Touvron et al. ([2023b](#bib.bib52)),
    StarCoder Li et al. ([2023a](#bib.bib32)), CodeLLama Rozière et al. ([2023](#bib.bib47)),
    and CodeGeex2 Zheng et al. ([2023](#bib.bib59)). Remarkably, when applying MFTCoder
    to the CodeLlama-34B-Python Rozière et al. ([2023](#bib.bib47)) base model, it
    achieves a pass@1 score of 74.4% on the humanEval evaluation dataset, even surpassing
    the performance of GPT-4 (67%, zero-shot) OpenAI ([2023](#bib.bib43)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于（代码）LLM的多任务微调（MFT），以保证对样本量不同的任务给予公平关注，并取得大致相似的优化进展。特别是，我们关注于代码LLM，因为代码领域的任务通常存在相关性，因此我们将我们的方法命名为MFTCoder。我们强调MFTCoder可以以直接的方式扩展到任意相关的NLP任务集合。为了提高MFTCoder的效率，我们结合了参数高效的微调技术，包括LoRA
    Hu et al. ([2021](#bib.bib24)) 和 QLoRA Dettmers et al. ([2023](#bib.bib18))。实验结果表明，使用MFT方法训练的多任务模型优于单独针对每个任务微调或通过合并多个任务数据进行微调的模型。我们进一步验证了MFTCoder在各种基准预训练LLM上的有效性，例如Qwen
    Bai et al. ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)), Llama Touvron
    et al. ([2023a](#bib.bib51)), Llama 2 Touvron et al. ([2023b](#bib.bib52)), StarCoder
    Li et al. ([2023a](#bib.bib32)), CodeLLama Rozière et al. ([2023](#bib.bib47)),
    和 CodeGeex2 Zheng et al. ([2023](#bib.bib59))。值得注意的是，当将MFTCoder应用于CodeLlama-34B-Python
    Rozière et al. ([2023](#bib.bib47)) 基础模型时，它在humanEval评估数据集上取得了74.4%的pass@1分数，甚至超过了GPT-4（67%，零-shot） OpenAI
    ([2023](#bib.bib43))的表现。
- en: 'The main contributions of this paper can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献可以总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose MFTCoder, a novel multitask finetuning approach for concurrently
    adapting LLMs to multiple code-related tasks. Our focus is on addressing the issues
    of data balance and convergence speed that commonly arise in previous multitask
    finetuning methods.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了MFTCoder，一种新颖的多任务微调方法，用于同时调整LLM以适应多个代码相关任务。我们重点解决了以往多任务微调方法中常见的数据平衡和收敛速度问题。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We validate MFTCoder on various baseline pretrained models, including Qwen Bai
    et al. ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)), Llama Touvron
    et al. ([2023a](#bib.bib51)), Llama 2 Touvron et al. ([2023b](#bib.bib52)), StarCoder Li
    et al. ([2023a](#bib.bib32)), CodeLLama Rozière et al. ([2023](#bib.bib47)), CodeFuse Di
    et al. ([2023](#bib.bib19)), and CodeGeex2 Zheng et al. ([2023](#bib.bib59)),
    demonstrating its compatibility with different baseline models.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在各种基准预训练模型上验证了MFTCoder，包括Qwen Bai et al. ([2023](#bib.bib8)), Baichuan Baichuan
    ([2023](#bib.bib9)), Llama Touvron et al. ([2023a](#bib.bib51)), Llama 2 Touvron
    et al. ([2023b](#bib.bib52)), StarCoder Li et al. ([2023a](#bib.bib32)), CodeLLama
    Rozière et al. ([2023](#bib.bib47)), CodeFuse Di et al. ([2023](#bib.bib19)),
    和 CodeGeex2 Zheng et al. ([2023](#bib.bib59))，展示了其与不同基准模型的兼容性。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments show that the MFT approach outperforms individual fine-tuning
    for each task or data merging from multiple tasks. Notably, when implementing
    MFTCoder with the CodeLlama-34B-Python Rozière et al. ([2023](#bib.bib47)) base
    model, it achieves an impressive pass@1 score of 74.4% on the humanEval evaluation
    dataset, surpassing the performance of GPT-4 (67%, zero-shot) OpenAI ([2023](#bib.bib43)).
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模实验表明，MFT 方法优于针对每个任务单独微调或从多个任务中合并数据的方式。特别是，当将 MFTCoder 与 CodeLlama-34B-Python Rozière
    et al. ([2023](#bib.bib47)) 基础模型结合使用时，它在 humanEval 评估数据集上获得了 74.4% 的 impressive
    pass@1 分数，超过了 GPT-4 (67%，零样本) OpenAI ([2023](#bib.bib43)) 的表现。
- en: 2 Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Code LLMs
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代码 LLMs
- en: Coding capability serves as a critical criterion for evaluating general large
    language models (LLMs) in code-related tasks. Notable performance on the widely-used
    HumanEval dataset Chen et al. ([2021](#bib.bib12)), a benchmark for code generation,
    has been observed across various models, including LaMDA Thoppilan et al. ([2022](#bib.bib50)),
    PaLM Chowdhery et al. ([2022](#bib.bib15)), PaLM 2 Anil et al. ([2023](#bib.bib4)),
    ChatGPT, and GPT-4 OpenAI ([2023](#bib.bib43)). In particular, GPT-4 has set a
    remarkable record of 67.0% pass@1 score. However, their closed-source nature limits
    their availability and hinders further collaborative advancements. In contrast,
    recent open-source LLMs, including LLaMA Touvron et al. ([2023a](#bib.bib51)),
    LLaMA 2 Touvron et al. ([2023b](#bib.bib52)), Qwen Bai et al. ([2023](#bib.bib8)),
    and Phi-1.5 Li et al. ([2023b](#bib.bib33)), have demonstrated notable progress
    in code-related tasks, with commentable scores of 23.7%, 29.9%, 32.3%, and 41.4%
    respectively. Despite this progress, their performance still lags behind the state-of-the-art
    closed-source models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 编码能力作为评估通用大型语言模型（LLMs）在与代码相关任务中的关键标准，尤其是在广泛使用的 HumanEval 数据集上表现突出。包括 LaMDA Thoppilan
    et al. ([2022](#bib.bib50))、PaLM Chowdhery et al. ([2022](#bib.bib15))、PaLM 2 Anil
    et al. ([2023](#bib.bib4))、ChatGPT 和 GPT-4 OpenAI ([2023](#bib.bib43)) 在内的多种模型在这个代码生成基准测试上均表现优异。特别是
    GPT-4 创下了 67.0% pass@1 的惊人记录。然而，其封闭源代码的特性限制了其可用性，并阻碍了进一步的协作进展。相比之下，最近的开源 LLMs，包括
    LLaMA Touvron et al. ([2023a](#bib.bib51))、LLaMA 2 Touvron et al. ([2023b](#bib.bib52))、Qwen Bai
    et al. ([2023](#bib.bib8)) 和 Phi-1.5 Li et al. ([2023b](#bib.bib33))，在与代码相关的任务中取得了显著进展，评分分别为
    23.7%、29.9%、32.3% 和 41.4%。尽管如此，它们的表现仍落后于最先进的封闭源模型。
- en: On the other hand, LLMs specifically designed for code-related tasks, often
    referred to as code LLMs, have also undergone significant developments. Alongside
    closed-source Code LLMs such as Codex Chen et al. ([2021](#bib.bib12)), Code-Davinci Chen
    et al. ([2021](#bib.bib12)), AlphaCode Li et al. ([2022](#bib.bib34)), PaLM-Coder Chowdhery
    et al. ([2022](#bib.bib15)), and PanGu-Coder Christopoulou et al. ([2022](#bib.bib16)),
    open-source alternatives like including SantaCoder Allal et al. ([2023](#bib.bib3)),
    Phi-1.0 Gunasekar et al. ([2023](#bib.bib22)), CodeGeeX-2 Zheng et al. ([2023](#bib.bib59)),
    StarCoder Li et al. ([2023a](#bib.bib32)), Code LLaMA Rozière et al. ([2023](#bib.bib47))
    have showcased competitive performance with their closed-source counterparts.
    Notably, CodeLLama-34B-Python Rozière et al. ([2023](#bib.bib47)) obtains a score
    of 53.7% on HumanEval. Apart from pretraining, another intriguing approach to
    further enhancing Code LLMs is instruction fine-tuning, as showcased by CodeT5+ Wang
    et al. ([2023](#bib.bib54)), Phi-1.0 Gunasekar et al. ([2023](#bib.bib22)), OctoPack Muennighoff
    et al. ([2023](#bib.bib42)), and WizardCoder Luo et al. ([2023](#bib.bib40)).
    By leveraging carefully curated high-quality instruction datasets, these methods
    exhibit the potential of fine-tuning to enhance code generation capabilities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，专门设计用于代码相关任务的 LLMs，通常称为代码 LLMs，也经历了显著的发展。除了封闭源代码 LLMs，如 Codex Chen et al.
    ([2021](#bib.bib12))、Code-Davinci Chen et al. ([2021](#bib.bib12))、AlphaCode Li
    et al. ([2022](#bib.bib34))、PaLM-Coder Chowdhery et al. ([2022](#bib.bib15)) 和
    PanGu-Coder Christopoulou et al. ([2022](#bib.bib16)) 外，开源替代品如 SantaCoder Allal
    et al. ([2023](#bib.bib3))、Phi-1.0 Gunasekar et al. ([2023](#bib.bib22))、CodeGeeX-2 Zheng
    et al. ([2023](#bib.bib59))、StarCoder Li et al. ([2023a](#bib.bib32)) 和 Code LLaMA Rozière
    et al. ([2023](#bib.bib47)) 也展示了与其封闭源同行竞争的表现。值得注意的是，CodeLLama-34B-Python Rozière
    et al. ([2023](#bib.bib47)) 在 HumanEval 数据集上取得了 53.7% 的分数。除了预训练，进一步提升代码 LLMs 的另一种有趣方法是指令微调，如
    CodeT5+ Wang et al. ([2023](#bib.bib54))、Phi-1.0 Gunasekar et al. ([2023](#bib.bib22))、OctoPack Muennighoff
    et al. ([2023](#bib.bib42)) 和 WizardCoder Luo et al. ([2023](#bib.bib40)) 展示的那样。通过利用精心策划的高质量指令数据集，这些方法展示了微调提升代码生成能力的潜力。
- en: 2.2 Multitask Learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多任务学习
- en: 'Multitask learning (MTL) Caruana ([1997](#bib.bib11)); Crawshaw ([2020](#bib.bib17))
    is a potent approach in machine learning that holds significant promise for enhancing
    model performance and addressing diverse challenges Crawshaw ([2020](#bib.bib17)).
    By training a single model on multiple related tasks, MTL enables the model to
    leverage shared knowledge and patterns, leading to enhanced generalization and
    improved accuracy. MTL methods can be categorized into two groups: hard parameter
    sharing Zhao et al. ([2018](#bib.bib58)); Liu et al. ([2019b](#bib.bib36), [a](#bib.bib38));
    Kendall et al. ([2018](#bib.bib26)); Liu et al. ([2019c](#bib.bib37)); Chen et al.
    ([2018](#bib.bib13)); Jean et al. ([2019](#bib.bib25)) and soft parameter sharing Duong
    et al. ([2015](#bib.bib21)); Yang and Hospedales ([2017](#bib.bib55)); Long et al.
    ([2017](#bib.bib39)); Lee et al. ([2018](#bib.bib29)); Sun et al. ([2020](#bib.bib49));
    Pascal et al. ([2021](#bib.bib44)). Hard parameter sharing involves sharing model
    weights between tasks, while soft parameter sharing incorporates task-specific
    models with separate weights. In the context of large language models (LLMs),
    hard parameter sharing is particularly relevant, since the large number of parameters
    in LLMs facilitates their ability to handle multiple related tasks with a common
    set of parameters. As a result, optimizing an LLM to effectively tackle multiple
    tasks lies at the heart of MTL for LLMs. In recent years, notable advancements
    have been made in MTL techniques. Google introduced T5 Raffel et al. ([2023](#bib.bib46))
    in 2020, where they explored the application of MTL techniques. Meta further introduced
    Mupper Aghajanyan et al. ([2021](#bib.bib2)) in 2021, which applies multi-task
    learning between pretraining and fine-tuning, termed as pre-fine-tuning (PFT).
    They discovered that incorporating this step enhances the performance of the pretrained
    model across various downstream tasks and significantly improves the speed of
    fine-tuning. However, if the number of tasks in PFT is too small, it can have
    a negative impact on the model’s performance. Therefore, it is recommended to
    have a minimum of 15 tasks for optimal results. Building upon T5, Google introduced
    ExT5 Aribandi et al. ([2022](#bib.bib6)), which increased the number of tasks
    to 107\. They found that as long as the number of tasks in pretraining is sufficiently
    large, even if there may be mutual interference among tasks, the ultimate results
    are still remarkably good. Ultimately, ExT5 outperformed T5 across multiple metrics.
    It is worth noting that these studies mainly focused on incorporating multi-task
    data and merging it for the large model to learn, without explicitly segregating
    the tasks. While these approaches have shown promising results, they tend to overlook
    data imbalance and convergence speed issues that often arise in MTL. In this paper,
    we address these challenges and propose MFTCoder, a multitask finetuning approach
    for LLMs that tackles these problems effectively.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）Caruana ([1997](#bib.bib11)); Crawshaw ([2020](#bib.bib17)) 是一种在机器学习中具有强大潜力的方法，能够显著提升模型性能并解决各种挑战
    Crawshaw ([2020](#bib.bib17))。通过在多个相关任务上训练单一模型，MTL 使模型能够利用共享的知识和模式，从而提高泛化能力和准确性。MTL
    方法可以分为两类：硬参数共享 Zhao et al. ([2018](#bib.bib58)); Liu et al. ([2019b](#bib.bib36),
    [a](#bib.bib38)); Kendall et al. ([2018](#bib.bib26)); Liu et al. ([2019c](#bib.bib37));
    Chen et al. ([2018](#bib.bib13)); Jean et al. ([2019](#bib.bib25)) 和软参数共享 Duong
    et al. ([2015](#bib.bib21)); Yang and Hospedales ([2017](#bib.bib55)); Long et
    al. ([2017](#bib.bib39)); Lee et al. ([2018](#bib.bib29)); Sun et al. ([2020](#bib.bib49));
    Pascal et al. ([2021](#bib.bib44))。硬参数共享涉及在任务之间共享模型权重，而软参数共享则采用具有独立权重的任务特定模型。在大语言模型（LLMs）的背景下，硬参数共享尤其相关，因为
    LLM 中的参数数量庞大，这有助于它们使用一组通用参数处理多个相关任务。因此，优化 LLM 以有效处理多个任务是 MTL 在 LLM 中的核心。在近年来，MTL
    技术取得了显著进展。谷歌于 2020 年推出了 T5 Raffel et al. ([2023](#bib.bib46))，探索了 MTL 技术的应用。Meta
    在 2021 年进一步推出了 Mupper Aghajanyan et al. ([2021](#bib.bib2))，该方法在预训练和微调之间应用了多任务学习，称为预微调（PFT）。他们发现，加入此步骤能提升预训练模型在各种下游任务中的表现，并显著提高微调速度。然而，如果
    PFT 中的任务数量过少，可能会对模型性能产生负面影响。因此，建议至少设置 15 个任务以获得最佳结果。在 T5 的基础上，谷歌推出了 ExT5 Aribandi
    et al. ([2022](#bib.bib6))，将任务数量增加至 107。研究发现，只要预训练中的任务数量足够大，即使任务之间可能存在相互干扰，最终结果仍然非常出色。最终，ExT5
    在多个指标上优于 T5。值得注意的是，这些研究主要集中在将多任务数据合并以供大型模型学习，而没有明确区分任务。尽管这些方法显示出了良好的结果，但它们往往忽略了在
    MTL 中常见的数据不平衡和收敛速度问题。本文解决了这些挑战，并提出了 MFTCoder，一种针对 LLM 的多任务微调方法，有效应对这些问题。
- en: 3 Approach
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/637e7d5d79940ee0d8493042d8901bdd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/637e7d5d79940ee0d8493042d8901bdd.png)'
- en: 'Figure 1: Overview of MFTCoder framework.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MFTCoder 框架概述。
- en: In this section, we will introduce our multi-task fine-tuning framework, MFTCoder ²²2https://github.com/codefuse-ai/MFTCoder,
    along with the design of its key components.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍我们的多任务微调框架 MFTCoder ²²2https://github.com/codefuse-ai/MFTCoder，以及其关键组件的设计。
- en: 3.1 MFT Framework
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 MFT 框架
- en: MFTCoder aims to seamlessly adapt LLMs to diverse new scenarios while maximizing
    their performance within a specific context. When applying MFTCoder to a new scenario,
    the initial step involves decomposing the scenario into smaller tasks that correspond
    to targeted abilities. For instance, in the domain of code LLMs, the overarching
    objective of enhancing models’ code capabilities can be further divided into specific
    tasks like code completion, text-to-code generation, unit test case generation,
    code repair, code debugging, and even cross-language translation. Our extensive
    practical experience has demonstrated that MFTCoder effectively handles multi-task
    scales ranging from single to dozens or even hundreds of tasks. Each task necessitates
    the collection and organization of fine-tuning datasets. However, data collection
    for certain tasks can pose challenges. To overcome this, MFTCoder leverages Self-Instruct Wang
    et al. ([2022](#bib.bib53)) techniques and Agents to generate instruction datasets.
    With the capability to concurrently fine-tune multiple downstream tasks, MFTCoder
    effectively handles substantial volumes of fine-tuning data, ensuring efficient
    training. It incorporates two efficient data tokenization modes and implements
    PEFT (Parameter-Efficient Fine-Tuning) techniques to enhance training efficiency.
    In the realm of multi-task learning, MFTCoder confronts the issue of task imbalances,
    encompassing imbalanced data distribution, varying task difficulties, and divergent
    convergence rates. To mitigate these challenges, MFTCoder introduces or adapts
    different loss functions to achieve task balance. Recognizing that different large-scale
    models possess distinct strengths and capabilities, MFTCoder facilitates the selection
    of suitable model architectures based on specific scenarios to achieve optimal
    performance. It has been adapted to popular LLMs, including LLama Touvron et al.
    ([2023a](#bib.bib51)), LLama 2 Touvron et al. ([2023b](#bib.bib52)), CodeLLama Rozière
    et al. ([2023](#bib.bib47)), Qwen Bai et al. ([2023](#bib.bib8)), Baichuan 1/2 Baichuan
    ([2023](#bib.bib9)), ChatGLM 2 Du et al. ([2022](#bib.bib20)), CodeGeeX 2 Zheng
    et al. ([2023](#bib.bib59)), GPT-NEOX Black et al. ([2022](#bib.bib10)), CodeFuse Di
    et al. ([2023](#bib.bib19)), StarCoder Li et al. ([2023a](#bib.bib32)), AntLLM,
    and more. We continuously update and expand the compatibility with additional
    models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MFTCoder 旨在无缝地将大型语言模型（LLMs）适应于各种新场景，同时最大化其在特定上下文中的表现。在将 MFTCoder 应用于新场景时，初始步骤是将场景分解为对应于目标能力的小任务。例如，在代码
    LLM 的领域中，提升模型的代码能力的总体目标可以进一步细分为具体任务，如代码完成、文本到代码生成、单元测试用例生成、代码修复、代码调试，甚至跨语言翻译。我们广泛的实践经验表明，MFTCoder
    能有效处理从单一任务到数十甚至数百任务的多任务规模。每个任务都需要收集和组织微调数据集。然而，某些任务的数据收集可能会面临挑战。为了解决这个问题，MFTCoder
    利用 Self-Instruct Wang et al. ([2022](#bib.bib53)) 技术和 Agents 来生成指令数据集。凭借同时微调多个下游任务的能力，MFTCoder
    能有效处理大量微调数据，确保高效训练。它采用两种高效的数据标记模式，并实现了 PEFT（参数高效微调）技术，以提高训练效率。在多任务学习领域，MFTCoder
    面临任务不平衡的问题，包括数据分布不平衡、任务难度差异和收敛速度不同。为应对这些挑战，MFTCoder 引入或调整不同的损失函数以实现任务平衡。认识到不同的大规模模型具有不同的优势和能力，MFTCoder
    促进了基于特定场景选择合适的模型架构，以实现最佳性能。它已经适配了包括 LLama Touvron et al. ([2023a](#bib.bib51))、LLama
    2 Touvron et al. ([2023b](#bib.bib52))、CodeLLama Rozière et al. ([2023](#bib.bib47))、Qwen Bai
    et al. ([2023](#bib.bib8))、Baichuan 1/2 Baichuan ([2023](#bib.bib9))、ChatGLM 2 Du
    et al. ([2022](#bib.bib20))、CodeGeeX 2 Zheng et al. ([2023](#bib.bib59))、GPT-NEOX Black
    et al. ([2022](#bib.bib10))、CodeFuse Di et al. ([2023](#bib.bib19))、StarCoder Li
    et al. ([2023a](#bib.bib32))、AntLLM 等在内的流行 LLM。我们不断更新和扩展与其他模型的兼容性。
- en: 'The overall framework of MFTCoder is illustrated in Figure [1](#S3.F1 "Figure
    1 ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"). In
    the subsequent sections, we will provide a more detailed exploration of these
    components, including instruction datasets construction, efficient tokenization
    modes, PEFT fine-tuning and balanced loss functions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'MFTCoder 的整体框架如图 [1](#S3.F1 "Figure 1 ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning") 所示。在随后的章节中，我们将更详细地探讨这些组件，包括指令数据集的构建、高效的标记化模式、PEFT
    微调和均衡的损失函数。'
- en: 3.2 Instruction Dataset Construction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 指令数据集构建
- en: '![Refer to caption](img/5a073458f1289833d4b15ce4da3eaa46.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a073458f1289833d4b15ce4da3eaa46.png)'
- en: 'Figure 2: Data Generation Approach for Code Exercises Datasets using Single-turn
    Conversation Scheme.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用单轮对话方案生成代码练习数据集的数据生成方法。
- en: For tasks with challenging data collection, We employ the Self-Instruct Wang
    et al. ([2022](#bib.bib53)) technique to generate fine-tuning data for downstream
    code-related tasks in MFTCoder. This involves providing customized prompts to
    GPT-3.5 or GPT-4 that clearly describe our instruction generation requirements,
    thereby generating instructional data. Furthermore, we drew inspiration from the
    Textbook approach employed in the PHI work Gunasekar et al. ([2023](#bib.bib22)),
    incorporating the self-instruct technique to generate Code Exercises datasets
    for downstream code-related tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据收集具有挑战性的任务，我们采用了 Self-Instruct Wang 等人（[2022](#bib.bib53)）的方法来生成 MFTCoder
    下游代码相关任务的微调数据。这包括向 GPT-3.5 或 GPT-4 提供自定义提示，明确描述我们的指令生成需求，从而生成指令数据。此外，我们还借鉴了 PHI
    工作 Gunasekar 等人（[2023](#bib.bib22)）中使用的教材方法，将自我指令技术用于生成下游代码相关任务的代码练习数据集。
- en: 'In terms of specific implementation, we have two options. One is the Agents
    multi-turn conversation approach achieved through Camel Li et al. ([2023c](#bib.bib30)),
    and the other is the single-turn conversation method by directly invoking the
    ChatGPT API. In our multi-turn approach, we employ Camel to launch two agents,
    each assigned specific roles and task themes, facilitating a dialogue between
    them to generate instructional data aligned with the given theme. For instance,
    when generating Python exercise data, we designate the roles of ’teacher’ (simulating
    the user role of ChatGPT) and ’student’ (simulating the assistant role of ChatGPT)
    for the agents. The teacher’s responsibility is to provide exercise instructions
    to the student, while the student’s task is to offer corresponding solutions to
    those instructions. This iterative process continues, generating multiple exercise
    questions, until the task requirements are met or the maximum input length of
    ChatGPT is reached. To accommodate ChatGPT’s input length limitation, we cannot
    directly utilize a large question as the task theme. For instance, when creating
    Python exercise questions to assess students’ proficiency, we break down the main
    theme into smaller Python knowledge points (e.g. binary search tree) and initiate
    separate Camel sessions for each knowledge point. For a concrete example, please
    refer to Appendix [A](#A1 "Appendix A Code Exercises Generation with Camel ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在具体实施方面，我们有两个选项。一个是通过 Camel Li 等人（[2023c](#bib.bib30)）实现的 Agents 多轮对话方法，另一个是通过直接调用
    ChatGPT API 实现的单轮对话方法。在我们的多轮对话方法中，我们使用 Camel 启动两个代理，每个代理被分配特定的角色和任务主题，促进它们之间的对话，以生成与给定主题对齐的指令数据。例如，在生成
    Python 练习数据时，我们将代理的角色指定为“教师”（模拟 ChatGPT 的用户角色）和“学生”（模拟 ChatGPT 的助手角色）。教师的责任是向学生提供练习指令，而学生的任务是提供相应的解决方案。这个迭代过程不断进行，生成多个练习问题，直到满足任务要求或达到
    ChatGPT 的最大输入长度。为了适应 ChatGPT 的输入长度限制，我们不能直接利用一个大型问题作为任务主题。例如，在创建评估学生能力的 Python
    练习问题时，我们将主要主题拆分为较小的 Python 知识点（例如，二叉搜索树），并为每个知识点启动单独的 Camel 会话。具体示例请参见附录 [A](#A1
    "Appendix A Code Exercises Generation with Camel ‣ MFTCoder: Boosting Code LLMs
    with Multitask Fine-Tuning")。'
- en: 'The multi-turn approach provides high automation but can be costly due to the
    need for maintaining two agents, each making multi-turn calls to the ChatGPT API.
    To mitigate this, we propose a more cost-effective single-turn conversation generation
    approach, and the overall process is illustrated in Figure [2](#S3.F2 "Figure
    2 ‣ 3.2 Instruction Dataset Construction ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning"). We begin by creating an initial set of seeds,
    such as hundreds of Python knowledge points. These seeds are then combined with
    prepared fixed prompt templates to generate a set of patterned task prompts. To
    address the issue of reduced diversity caused by fixed templates and to ensure
    accurate prompt descriptions, we utilize Camel’s task prompt refinement feature
    to obtain precise and diverse task prompts. Each task prompt is used to generate
    a set of instructions related to the corresponding seed (e.g. exercise problems
    related to binary search trees). Using ChatGPT, we generate the corresponding
    solutions for the generated instructions. Finally, we assemble and deduplicate
    the instructions with their respective solutions to obtain an exercise dataset.
    We have open-sourced a Python Code Exercises dataset ³³3https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k
    constructed using this approach.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '多轮对话方法提供了高自动化，但由于需要维护两个代理，每个代理都进行多轮调用ChatGPT API，因此可能成本较高。为此，我们提出了一种更具成本效益的单轮对话生成方法，整体过程如图 [2](#S3.F2
    "图 2 ‣ 3.2 指令数据集构建 ‣ 3 方法 ‣ MFTCoder: 通过多任务微调提升代码LLMs")所示。我们首先创建一组初始种子，例如数百个Python知识点。然后将这些种子与准备好的固定提示模板相结合，生成一组有模式的任务提示。为了应对固定模板导致的多样性减少问题并确保提示描述的准确性，我们利用Camel的任务提示优化功能获得精确且多样的任务提示。每个任务提示用于生成一组与对应种子相关的指令（例如与二叉搜索树相关的练习题）。使用ChatGPT，我们生成生成指令的相应解决方案。最后，我们组装并去重这些指令及其相应的解决方案，以获得一个练习数据集。我们已经开源了一个使用这种方法构建的Python代码练习数据集 ³³3https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k。'
- en: 3.3 Efficient Tokenization Modes
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 高效标记化模式
- en: 'Tokenization is an essential step in the pre-training and fine-tuning of LLM
    models, where input and output texts are split into smaller units to be processed.
    It, along with the loss function, effectively defines how the data is utilized
    during the training process, thus playing a crucial role in both the model’s effectiveness
    and training efficiency. In the typical SFT tokenization scheme, samples within
    the same batch are aligned to the maximum input length (seq-length) of the model
    with extra padding tokens, shown as Figure [3(a)](#S3.F3.sf1 "In Figure 3 ‣ 3.3
    Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning"). However, in practice, we have found that this approach
    results in a high proportion of padding tokens. For example, when using the CodeFuse-13B Di
    et al. ([2023](#bib.bib19)) tokenizer to process 35 downstream tasks, the average
    proportion of padding tokens is 92.22% (with seq-length set to 4096). This means
    a significant number of tokens are used solely for alignment purposes, providing
    no value to the training process. This results in lower training efficiency and
    wastage of offline tokenization storage space. To address this issue, we have
    adopted and optimized two tokenization modes, namely dynamic padding and pack
    modes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '标记化是大模型（LLM）预训练和微调中的一个重要步骤，其中输入和输出文本被拆分成较小的单元进行处理。它与损失函数一起，实际上定义了数据在训练过程中的利用方式，从而在模型的有效性和训练效率中发挥了至关重要的作用。在典型的SFT标记化方案中，同一批次中的样本被对齐到模型的最大输入长度（seq-length），并添加额外的填充标记，如图 [3(a)](#S3.F3.sf1
    "图 3 ‣ 3.3 高效标记化模式 ‣ 3 方法 ‣ MFTCoder: 通过多任务微调提升代码LLMs")所示。然而，实际上，我们发现这种方法导致了大量的填充标记。例如，当使用CodeFuse-13B
    Di 等人（[2023](#bib.bib19)）的标记化器处理35个下游任务时，填充标记的平均比例为92.22%（seq-length设置为4096）。这意味着大量的标记仅用于对齐目的，对训练过程没有实际价值。这导致了较低的训练效率和离线标记化存储空间的浪费。为了解决这个问题，我们采用并优化了两种标记化模式，即动态填充和打包模式。'
- en: 'In dynamic padding mode, the micro batch window size of each GPU is determined
    by the maximum sample length in the micro batch. Shorter samples are padded with
    additional padding tokens to match this size, as shown in Figure [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Although padding tokens do not affect
    the model’s training effectiveness, they add computational overhead during training,
    impacting the training speed. Dynamic padding mode effectively reduces the proportion
    of padding tokens used, leading to faster training. In our experience, this approach
    can yield approximately a twofold speed improvement compared to the traditional
    SFT tokenization mode (actual improvement depends on the dataset). It’s important
    to note that this mode is suitable for online tokenization scenarios only.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '在动态填充模式下，每个 GPU 的微批次窗口大小由微批次中的最大样本长度决定。较短的样本会通过额外的填充标记来匹配这个大小，如图 [3(b)](#S3.F3.sf2
    "在图 3 ‣ 3.3 高效标记化模式 ‣ 3 方法 ‣ MFTCoder: 利用多任务微调提升代码 LLM")所示。尽管填充标记不会影响模型的训练效果，但它们在训练期间会增加计算开销，影响训练速度。动态填充模式有效地减少了填充标记的比例，从而加快训练速度。我们的经验表明，与传统的
    SFT 标记化模式相比，这种方法可以提高大约两倍的速度（实际改进取决于数据集）。需要注意的是，这种模式仅适用于在线标记化场景。'
- en: 'While the dynamic padding mode reduces the micro batch window size, the pack
    mode, similar to Llama 2’s SFT tokenization mode Touvron et al. ([2023b](#bib.bib52)),
    maximizes the utilization of the model’s maximum input window length (seq-length).
    In the pack mode, multiple fine-tuning samples are sequentially packed into a
    window of seq-length, separated by eos tokens, as shown in Figure [3(c)](#S3.F3.sf3
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). In the figure, samples 1-4 of Figure [3(a)](#S3.F3.sf1
    "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") are combined and placed in one window after
    one another. If a sample cannot fit in the current window, it is placed in the
    next window with padding tokens filling the remaining space. For instance, in
    Figure [3(c)](#S3.F3.sf3 "In Figure 3 ‣ 3.3 Efficient Tokenization Modes ‣ 3 Approach
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), sample 5 is placed
    in the second window with padding tokens, while sample 6 is accommodated in the
    third window. The pack mode, in comparison to the dynamic padding mode, offers
    even lower padding token ratio, resulting in improved training speed. Our practical
    experience demonstrates that this approach reduces the average proportion of padding
    tokens to less than 10% among the 35 tasks mentioned earlier, leading to a substantial
    boost in training speed while maintaining training effectiveness. It is important
    to highlight that MFTCoder supports both online and offline pack tokenization
    scenarios, serving not only the SFT phase but also the pre-training stages.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然动态填充模式减少了微批次窗口大小，但打包模式，类似于 Llama 2 的 SFT 标记化模式 Touvron 等人 ([2023b](#bib.bib52))，最大化了模型最大输入窗口长度（seq-length）的利用。在打包模式下，多个微调样本被依次打包到
    seq-length 的窗口中，用 eos 标记分隔，如图 [3(c)](#S3.F3.sf3 "在图 3 ‣ 3.3 高效标记化模式 ‣ 3 方法 ‣ MFTCoder:
    利用多任务微调提升代码 LLM")所示。图中，图 [3(a)](#S3.F3.sf1 "在图 3 ‣ 3.3 高效标记化模式 ‣ 3 方法 ‣ MFTCoder:
    利用多任务微调提升代码 LLM")中的样本 1-4 被组合在一个窗口中。如果样本不能适应当前窗口，它将被放置在下一个窗口中，填充标记填补剩余空间。例如，在图 [3(c)](#S3.F3.sf3
    "在图 3 ‣ 3.3 高效标记化模式 ‣ 3 方法 ‣ MFTCoder: 利用多任务微调提升代码 LLM")中，样本 5 被放置在第二个窗口中并使用填充标记，而样本
    6 被容纳在第三个窗口中。与动态填充模式相比，打包模式提供了更低的填充标记比例，从而提高了训练速度。我们的实际经验表明，这种方法将填充标记的平均比例减少到
    35 个任务中不到 10%，显著提升了训练速度，同时保持训练效果。值得强调的是，MFTCoder 支持在线和离线打包标记化场景，不仅适用于 SFT 阶段，还适用于预训练阶段。'
- en: '![Refer to caption](img/19af03e73b12240f5aa403a2282710f2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/19af03e73b12240f5aa403a2282710f2.png)'
- en: (a) Normal SFT Mode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 普通 SFT 模式
- en: '![Refer to caption](img/5ef6612e6088bc2850692a6a4559a89e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5ef6612e6088bc2850692a6a4559a89e.png)'
- en: (b) Dynamic Padding Mode
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 动态填充模式
- en: '![Refer to caption](img/39ade741508de9f2aadeb8b1aad92369.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/39ade741508de9f2aadeb8b1aad92369.png)'
- en: (c) Pack SFT Mode
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 打包 SFT 模式
- en: 'Figure 3: Illustration of the differences in sample organization within a batch
    between normal SFT, dynmaic padding and Pack SFT tokenization modes. The light-colored
    squares in the figure represent the Prompt section of the samples, while the dark-colored
    squares represent the Label section (participating in loss calculation). The blank
    squares represent padding section.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：说明了正常SFT、动态填充和Pack SFT标记化模式中一个批次内样本组织的差异。图中的浅色方块代表样本的Prompt部分，而深色方块代表Label部分（参与损失计算）。空白方块代表填充部分。
- en: 3.4 PEFT Efficient Fine-tuning
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 PEFT高效微调
- en: 'The prevalent large-scale models typically contain billions of parameters,
    while multi-task learning scenarios often involve numerous tasks, resulting in
    a substantial total number of fine-tuning samples. If we were to opt for full-fledged
    fine-tuning of these large models using a vast amount of data, two challenges
    would arise: firstly, the need for extensive storage and computational resources;
    secondly, the potential risk of catastrophic forgetting during training. To address
    these issues, MFTCoder incorporates the PEFT (Parameter-efficient fine-tuning)
    technique Houlsby et al. ([2019](#bib.bib23)), enabling efficient fine-tuning
    to be accomplished within a short timeframe and with minimal resource requirements.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的大规模模型通常包含数十亿个参数，而多任务学习场景中通常涉及多个任务，导致微调样本的总数量非常庞大。如果我们选择使用大量数据对这些大模型进行全面微调，将会面临两个挑战：首先，需要大量存储和计算资源；其次，在训练过程中可能存在灾难性遗忘的风险。为了解决这些问题，MFTCoder采用了PEFT（参数高效微调）技术Houlsby等人（[2019](#bib.bib23)），使得高效微调可以在短时间内完成，并且资源需求最小。
- en: 'Specifically, MFTCoder supports two PEFT methods: Lora (Large-scale Language
    Model Low-Rank Adaptation) Hu et al. ([2021](#bib.bib24)) and QLora (Quantized
    Large-scale Language Model Low-Rank Adaptation) Dettmers et al. ([2023](#bib.bib18)).
    The fundamental concept of Lora is quite simple, as depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.4 PEFT Efficient Fine-tuning ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning"). It involves adding an auxiliary branch to the
    original pretrained language model. During training, the parameters $W\in\mathbb{R}^{d\times
    d}$ is then added to the original model $W$, resulting in the newly trained model.
    Due to the significantly smaller magnitude of r compared to d, the number of trainable
    parameters can be dramatically reduced. Building upon LoRA, QLoRA incorporates
    a novel high-precision quantization technique called NF4 and dual quantization
    to quantize the pretrained model to 4 bits. It also introduces a small set of
    learnable low-rank adapter weights. These weights are fine-tuned by optimizing
    the gradients through back-propagation of the quantized weights. As a result,
    QLoRA enables the fine-tuning of larger models using fewer GPU resources. As an
    example, MFTCoder can fine-tune a 70B model on a single A100 with 80GB of VRAM.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '具体而言，MFTCoder支持两种PEFT方法：Lora（大规模语言模型低秩适应）Hu等人（[2021](#bib.bib24)）和QLora（量化大规模语言模型低秩适应）Dettmers等人（[2023](#bib.bib18)）。Lora的基本概念非常简单，如图[4](#S3.F4
    "Figure 4 ‣ 3.4 PEFT Efficient Fine-tuning ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning")所示。它涉及在原始预训练语言模型中添加一个辅助分支。在训练过程中，将参数$W\in\mathbb{R}^{d\times
    d}$添加到原始模型$W$中，从而形成新的训练模型。由于r的量级比d小得多，可训练参数的数量可以大幅减少。在LoRA的基础上，QLoRA采用了一种新的高精度量化技术——NF4和双重量化，将预训练模型量化为4位。它还引入了一小部分可学习的低秩适配器权重。这些权重通过优化量化权重的梯度进行微调。因此，QLoRA能够使用更少的GPU资源微调更大的模型。例如，MFTCoder可以在单个80GB
    VRAM的A100上微调一个70B模型。'
- en: '![Refer to caption](img/0e985fabe14ab6a8435c82d63d1f7984.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0e985fabe14ab6a8435c82d63d1f7984.png)'
- en: 'Figure 4: Visualizing the Essence of Lora’s Basic Idea.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：可视化Lora基本思想的本质。
- en: 3.5 Multitask Fine-Tuning with Balanced Losses
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 平衡损失的多任务微调
- en: 'As a multi-task learning framework, MFTCoder, as described in Section [2](#S2
    "2 Related Works ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"),
    faces a significant challenge of data imbalance, task heterogeneity, and varying
    convergence speeds. To address these challenges, MFTCoder incorporates a set of
    loss functions specifically designed to alleviate these imbalances.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '作为一个多任务学习框架，MFTCoder（如第[2](#S2 "2 Related Works ‣ MFTCoder: Boosting Code LLMs
    with Multitask Fine-Tuning")节所述）面临数据不平衡、任务异质性和收敛速度变化等重大挑战。为应对这些挑战，MFTCoder结合了一组特定设计的损失函数，以缓解这些不平衡。'
- en: 'To address the issue of data imbalance, we first ensure that all samples from
    all tasks are utilized exactly once within a single epoch. To avoid the model
    favoring tasks with larger amounts of data, we introduce a weight assignment strategy
    during loss computation. Specifically, we support two weight calculation schemes:
    one based on the number of task samples and the other based on the number of valid
    tokens involved in the loss calculation. The former is more straightforward, but
    it may perform poorly when dealing with tasks that have extreme differences in
    the number of valid tokens, such as binary classification tasks like "yes" or
    "no" answering or single-choice exam tasks. On the other hand, the latter weight
    assignment scheme based on the actual number of valid tokens involved in the loss
    calculation can mitigate these issues. The specific formulation for weighted loss
    calculation is shown in Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with
    Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").
    In Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3
    Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), $N$ refers
    to the k-th valid token of the j-th sample for the i-th task.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决数据不平衡的问题，我们首先确保在单个周期内所有任务的样本都被准确利用一次。为了避免模型偏向于数据量较大的任务，我们在损失计算过程中引入了权重分配策略。具体而言，我们支持两种权重计算方案：一种基于任务样本数量，另一种基于参与损失计算的有效标记数量。前者较为直接，但在处理具有极端有效标记数量差异的任务时表现可能较差，例如“是”或“否”回答的二分类任务或单项选择考试任务。另一方面，基于实际有效标记数量的权重分配方案可以缓解这些问题。加权损失计算的具体公式如**方程
    [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning")** 所示。在**方程 [1](#S3.E1 "In 3.5
    Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning")**中，$N$ 表示第$i$个任务的第$j$个样本的第$k$个有效标记。'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'To address the issue of task heterogeneity, we drew inspiration from the focal
    loss approach and incorporated it into MFTCoder. We implemented two different
    levels of focal loss functions to cater to different granularities. One operates
    at the sample level, as shown in Equation [2](#S3.E2 "In 3.5 Multitask Fine-Tuning
    with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask
    Fine-Tuning"), while the other operates at the task level, as shown in Equation [3](#S3.E3
    "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决任务异质性的问题，我们从**焦点损失方法**中获得了灵感，并将其融入了**MFTCoder**中。我们实施了两个不同级别的焦点损失函数，以适应不同的粒度。一个在样本级别上操作，如**方程
    [2](#S3.E2 "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning")** 所示，而另一个在任务级别上操作，如**方程 [3](#S3.E3
    "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning")** 所示。'
- en: '|  | $$\mathcal{L}_{2}(\theta)=\min\limits_{\theta}\frac{\sum^{N}_{i=1}\sum^{M_{i}}_{j=1}-\alpha_{i}*(1-P_{ij})^{\gamma}*Q_{ij}}{\sum^{N}_{i=1}M_{i}},\\
    P_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\mathcal{L}_{2}(\theta)=\min\limits_{\theta}\frac{\sum^{N}_{i=1}\sum^{M_{i}}_{j=1}-\alpha_{i}*(1-P_{ij})^{\gamma}*Q_{ij}}{\sum^{N}_{i=1}M_{i}},\\
    P_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
- en: Q_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$ |  | (2) |
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $$Q_{ij}=\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$ |  | (2) |
- en: '|  | $$\mathcal{L}_{3}(\theta)=\min\limits_{\theta}\frac{1}{N}\sum^{N}_{i=1}-\alpha_{i}*(1-P_{i})^{\gamma}*Q_{i},\\
    P_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\mathcal{L}_{3}(\theta)=\min\limits_{\theta}\frac{1}{N}\sum^{N}_{i=1}-\alpha_{i}*(1-P_{i})^{\gamma}*Q_{i},\\
    P_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}P_{ijk},\\'
- en: Q_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$
    |  | (3) |
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: $$Q_{i}=\frac{1}{M_{i}}\sum^{M_{i}}_{j=1}\frac{1}{T_{ij}}\sum^{T_{ij}}_{k=1}\log(P_{ijk})$$
    |  | (3) |
- en: 'To address the issue of inconsistent convergence speeds, we drew inspiration
    from the FAMO Liu et al. ([2023](#bib.bib35)) approach and innovatively applied
    it to calculate the validation loss. Firstly, we assumed that each task, indexed
    by $i$ for the task with the slowest convergence speed, shown as Equation [4](#S3.E4
    "In 3.5 Multitask Fine-Tuning with Balanced Losses ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Here, $g_{t}$ denotes the parameters of
    the network in the $t$ is a small constant to prevent division by zero. Furthermore,
    we would like to provide further explanation on how we achieve balanced convergence.
    To ensure that tasks converge at a similar pace, we introduce a dynamic balancing
    mechanism. At each iteration, we update the task-specific weights based on the
    gradients of their validation losses. This approach aims to give more importance
    to tasks with slower convergence speeds, allowing them to have a larger influence
    on the overall optimization process. By dynamically adjusting the task weights,
    we create a balanced convergence scenario, where all tasks progress towards their
    optimal solutions at a similar rate. This mechanism effectively addresses the
    issue of disparate convergence speeds and enhances the overall stability and performance
    of the MFTCoder framework.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决不一致的收敛速度问题，我们借鉴了 FAMO Liu 等人（[2023](#bib.bib35)）的方法，并创新性地将其应用于计算验证损失。首先，我们假设每个任务（以
    $i$ 为下标）中收敛速度最慢的任务如方程[4](#S3.E4 "在 3.5 多任务微调与平衡损失 ‣ 3 方法 ‣ MFTCoder：通过多任务微调提升代码
    LLM") 所示。这里，$g_{t}$ 表示网络在 $t$ 时刻的参数，$t$ 是一个小常数，用于防止除以零。此外，我们还希望进一步解释如何实现平衡收敛。为了确保任务以类似的速度收敛，我们引入了动态平衡机制。在每次迭代中，我们根据验证损失的梯度更新任务特定的权重。该方法旨在给予收敛速度较慢的任务更多的重视，使它们对整体优化过程产生更大的影响。通过动态调整任务权重，我们创造了一个平衡的收敛场景，使所有任务以类似的速度朝着其最优解前进。该机制有效地解决了收敛速度不一致的问题，并提高了
    MFTCoder 框架的整体稳定性和性能。
- en: '|  | $$\mathcal{L}_{4}(\theta)=\max_{g_{t}}\min_{i}\frac{1}{\alpha}c^{i}(\alpha,g_{t})-\frac{1}{2}\&#124;g_{t}\&#124;^{2},\\
    g_{t}=\sum_{i}w_{t}^{i}\nabla\mathcal{L}^{i}(\theta_{t}),\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\mathcal{L}_{4}(\theta)=\max_{g_{t}}\min_{i}\frac{1}{\alpha}c^{i}(\alpha,g_{t})-\frac{1}{2}\&#124;g_{t}\&#124;^{2},\\
    g_{t}=\sum_{i}w_{t}^{i}\nabla\mathcal{L}^{i}(\theta_{t}),\\'
- en: c^{i}(\alpha,g_{t})=\frac{\mathcal{L}^{i}(\theta_{t})-\mathcal{L}^{i}(\theta_{t}-\alpha
    d_{t})}{\mathcal{L}^{i}(\theta_{t})+\epsilon}$$ |  | (4) |
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: c^{i}(\alpha,g_{t})=\frac{\mathcal{L}^{i}(\theta_{t})-\mathcal{L}^{i}(\theta_{t}-\alpha
    d_{t})}{\mathcal{L}^{i}(\theta_{t})+\epsilon}$$ |  | (4) |
- en: By incorporating these different loss functions, MFTCoder effectively addresses
    the diverse requirements of various multitask scenarios and alleviates the challenges
    of data imbalance, task heterogeneity, and inconsistent convergence speeds typically
    encountered in existing large-scale MTL research. MFTCoder’s flexible framework
    provides a robust solution to these issues, empowering the development of more
    efficient and accurate multitask models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些不同的损失函数，MFTCoder 有效地应对了各种多任务场景的多样化需求，并缓解了数据不平衡、任务异质性以及现有大规模 MTL 研究中常见的收敛速度不一致的问题。MFTCoder
    的灵活框架为这些问题提供了一个强有力的解决方案，促进了更高效和准确的多任务模型的发展。
- en: 4 Evaluation
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: 'In this section, we will conduct multiple sets of experiments using MFTCoder
    to validate the effectiveness and superiority of the MFT method. Specifically,
    we aim to address the following three research questions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行多组实验，以验证 MFTCoder 的有效性和优越性。具体而言，我们旨在回答以下三个研究问题：
- en: 'RQ1:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'RQ1:'
- en: Does the MFT model, obtained by fine-tuning multiple tasks using MFT methodology,
    outperform the SFT-S(ingle) models, where each task is individually fine-tuned?
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用 MFT 方法对多个任务进行微调获得的 MFT 模型是否优于 SFT-S(ingle) 模型，即每个任务单独微调的模型？
- en: 'RQ2:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'RQ2:'
- en: Does the MFT model outperform the SFT-Mixed model, where multiple tasks are
    combined and fine-tuned as one?
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MFT 模型是否优于 SFT-Mixed 模型，即将多个任务组合在一起并作为一个整体进行微调的模型？
- en: 'RQ3:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'RQ3:'
- en: In terms of generalization to unseen tasks, does the MFT model outperform the
    SFT-Mixed model?
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在对未见任务的泛化能力方面，MFT 模型是否优于 SFT-Mixed 模型？
- en: Next, we will commence by presenting the experimental setup. Subsequently, we
    will showcase and delve into the experimental results. Finally, we will culminate
    by summarizing and addressing the research questions raised in this section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始介绍实验设置。随后，我们将展示并深入探讨实验结果。最后，我们将总结并回答本节提出的研究问题。
- en: 4.1 Evaluation Setup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估设置
- en: 'To address these three research questions, we selected 5 code-related downstream
    tasks and prepared the corresponding fine-tuning data, as shown in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Evaluation Setup ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs
    with Multitask Fine-Tuning"). Table [1](#S4.T1 "Table 1 ‣ 4.1 Evaluation Setup
    ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") presents
    the desired enhancements (Column III) and the number of samples (Column IV) for
    each task. For instance, the codecompletion-task aims to improve the model’s code
    completion ability and includes 192,547 fine-tuning samples. The codetrans-task
    aims to enhance the model’s code translation capability and consists of 307,585
    fine-tuning samples. Thus, we trained 7 models (Column I), including individual
    SFT-S-* models trained for each downstream task, a combined SFT-Mixed model for
    the 5 task data, and an MFT-5Tasks model trained using the MFT method.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这三个研究问题，我们选择了 5 个与代码相关的下游任务，并准备了相应的微调数据，如表[1](#S4.T1 "表 1 ‣ 4.1 评估设置 ‣ 4
    评估 ‣ MFTCoder：通过多任务微调提升代码 LLM")所示。表[1](#S4.T1 "表 1 ‣ 4.1 评估设置 ‣ 4 评估 ‣ MFTCoder：通过多任务微调提升代码
    LLM")展示了每个任务所需的增强能力（第 III 列）和样本数量（第 IV 列）。例如，代码补全任务旨在提高模型的代码补全能力，包括 192,547 个微调样本。代码翻译任务旨在提升模型的代码翻译能力，包括
    307,585 个微调样本。因此，我们训练了 7 个模型（第 I 列），包括为每个下游任务训练的单独 SFT-S-* 模型、用于 5 个任务数据的组合 SFT-Mixed
    模型以及使用 MFT 方法训练的 MFT-5Tasks 模型。
- en: 'Table 1: Various experimental models and their corresponding training data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：各种实验模型及其对应的训练数据。
- en: '| Experimental Model | Task | Desired Ability | #Samples |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 实验模型 | 任务 | 期望能力 | #样本 |'
- en: '&#124; #Samples &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #样本 &#124;'
- en: '&#124; after packing &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打包后 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SFT-S-CodeCompletion | code-completion | Code Completion | 192,547 | 18,811
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeCompletion | code-completion | 代码补全 | 192,547 | 18,811 |'
- en: '| SFT-S-Text2Code | text2code | Text-to-code Generation | 94,086 | 14,399 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-Text2Code | text2code | 文本转代码生成 | 94,086 | 14,399 |'
- en: '| SFT-S-CodeComment | code-comment | Code Comments Generation | 645,711 | 134,775
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeComment | code-comment | 代码评论生成 | 645,711 | 134,775 |'
- en: '| SFT-S-CodeTrans | code-trans | Code Translation | 307,585 | 71,573 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeTrans | code-trans | 代码翻译 | 307,585 | 71,573 |'
- en: '| SFT-S-UnitTest | unit-test | Unit test-case generation | 390,393 | 77,681
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-UnitTest | unit-test | 单元测试用例生成 | 390,393 | 77,681 |'
- en: '| SFT-Mixed | Mix of the above 5 tasks | All of the above | 1,630,322 | 317,239
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 上述 5 个任务的混合 | 所有上述任务 | 1,630,322 | 317,239 |'
- en: '| MFT-5Tasks | The above 5 tasks | All of the above | 1,630,322 | 317,239 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 上述 5 个任务 | 所有上述任务 | 1,630,322 | 317,239 |'
- en: 'In the experiment, all models were configured identically except for the training
    data. The base model for all models was CodeLlama-13B-Python Rozière et al. ([2023](#bib.bib47)).
    Each model was trained using 16 A100 GPUs (with 80GB VRAM), a micro batch size
    of 8, and a global batch size of 128\. The Adam optimizer Kingma and Ba ([2017](#bib.bib27))
    was used with an initial learning rate of 2e-4, and a minimum learning rate of
    1e-5\. We employed the QLora-INT4 mode of MFTCoder for fine-tuning, with a consistent
    fine-tuning parameter proportion of 2.52%. The positions and initial values of
    the trainable parameters were also the same. All models incorporate the Data-Balance
    Loss (i.e., Equation [1](#S3.E1 "In 3.5 Multitask Fine-Tuning with Balanced Losses
    ‣ 3 Approach ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")) and
    employ pack tokenization mode. Notably, when there is only one task, this loss
    function aligns with the conventional loss employed in standard GPT model pre-training.
    To determine the convergence point for each model, we trained them until the validation
    loss surpassed the loss from the current epoch for the next two consecutive epochs.
    This mechanism, known as early-stopping strategy, was employed to ensure optimal
    convergence for each model.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，除训练数据外，所有模型的配置均相同。所有模型的基础模型为 CodeLlama-13B-Python Rozière 等人 ([2023](#bib.bib47))。每个模型使用
    16 台 A100 GPU（每台 80GB VRAM）、微批量大小为 8、全局批量大小为 128 进行训练。Adam 优化器 Kingma 和 Ba ([2017](#bib.bib27))
    被用来进行初始学习率为 2e-4 的优化，最小学习率为 1e-5。我们采用了 MFTCoder 的 QLora-INT4 模式进行微调，微调参数比例保持在
    2.52%。训练参数的位置和初始值也保持一致。所有模型都集成了 Data-Balance Loss（即方程 [1](#S3.E1 "在 3.5 多任务微调中具有平衡损失
    ‣ 3 方法 ‣ MFTCoder：通过多任务微调提升代码 LLM")）并采用了 pack 分词模式。值得注意的是，当只有一个任务时，这种损失函数与标准 GPT
    模型预训练中采用的传统损失函数一致。为了确定每个模型的收敛点，我们训练了模型，直到验证损失超过当前时期的损失，并持续两个连续时期。这种机制称为早停策略，旨在确保每个模型的最佳收敛。
- en: 4.2 Evaluation Datasets
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估数据集
- en: 'In this paper, we utilized publicly available and representative code assessment
    benchmarks for comparative evaluation. These benchmarks include:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们利用了公开可用且具有代表性的代码评估基准进行比较评估。这些基准包括：
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HumanEval Chen et al. ([2021](#bib.bib12)) is a widely used Python code completion
    evaluation dataset, meticulously curated by researchers at OpenAI.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HumanEval Chen 等人 ([2021](#bib.bib12)) 是一个广泛使用的 Python 代码补全评估数据集，由 OpenAI 的研究人员精心策划。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HumanEval-X Zheng et al. ([2023](#bib.bib59)) is an extension of HumanEval,
    translated into multiple programming languages, enabling multi-language code completion
    evaluation.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HumanEval-X Zheng 等人 ([2023](#bib.bib59)) 是 HumanEval 的扩展，翻译成多种编程语言，实现了多语言代码补全评估。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DS-1000 Lai et al. ([2022](#bib.bib28)) focuses on assessing a model’s ability
    to perform data science analysis using Python code, covering essential libraries
    such as Numpy, Pandas, TensorFlow, Pytorch, Scipy, Sklearn, and Matplotlib.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DS-1000 Lai 等人 ([2022](#bib.bib28)) 侧重于评估模型使用 Python 代码进行数据科学分析的能力，涵盖了 Numpy、Pandas、TensorFlow、Pytorch、Scipy、Sklearn
    和 Matplotlib 等重要库。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MBPP Austin et al. ([2021](#bib.bib7)) comprises 1000 Python programming problems,
    constructed through crowdsourcing, primarily targeting a model’s proficiency in
    basic Python. In this study, we selected 500 problems with ID 11-510 from MBPP
    to evaluate the text-to-code generation capability, specifically generating code
    based on problem descriptions.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MBPP Austin 等人 ([2021](#bib.bib7)) 包含 1000 个 Python 编程问题，通过众包方式构建，主要针对模型在基本
    Python 编程上的能力。在本研究中，我们从 MBPP 中选择了 500 个 ID 为 11-510 的问题来评估文本到代码生成的能力，特别是基于问题描述生成代码。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeFuseEval Di et al. ([2023](#bib.bib19)), building upon HumanEval and HumanEval-X,
    further extends the evaluation to include Chinese code completion (with Chinese
    docstrings), code translation, and unit test case generation capabilities, referred
    to as CodeFuseEval-CN, CodeFuseEval-CodeTrans, and CodeFuseEval-UnitTest, respectively.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeFuseEval Di 等人 ([2023](#bib.bib19)) 在 HumanEval 和 HumanEval-X 的基础上，进一步扩展了评估内容，包括中文代码补全（带有中文文档字符串）、代码翻译和单元测试用例生成能力，分别称为
    CodeFuseEval-CN、CodeFuseEval-CodeTrans 和 CodeFuseEval-UnitTest。
- en: Throughout these evaluation datasets, we employed "pass@1" as the evaluation
    metric in this paper.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些评估数据集中，我们采用了“pass@1”作为本文的评估指标。
- en: 4.3 Evaluation Results
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估结果
- en: In this section, we will showcase the evaluation results of seven trained models.
    For the SFT-S-* models, which were trained individually for each task, we will
    focus on testing their specific target capabilities. For instance, we will exclusively
    evaluate the performance of the SFT-S-CodeCompletion model in the code completion
    task. On the other hand, for the SFT-Mixed and MFT-5Tasks models, we will assess
    their performance on each task and compare it with the corresponding SFT-S-* models.
    Specifically, we will conduct tests to evaluate the capabilities of code completion,
    text-to-code generation, code comment generation, code translation, and unit test
    case generation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示七个训练模型的评估结果。对于每个任务单独训练的SFT-S-*模型，我们将重点测试它们的具体目标能力。例如，我们将专门评估SFT-S-CodeCompletion模型在代码补全任务中的表现。另一方面，对于SFT-Mixed和MFT-5Tasks模型，我们将评估它们在每个任务上的表现，并与相应的SFT-S-*模型进行比较。具体来说，我们将进行测试，以评估代码补全、文本到代码生成、代码注释生成、代码翻译和单元测试用例生成的能力。
- en: 4.3.1 Code Completion
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 代码补全
- en: For code completion, we employed the HumanEval Chen et al. ([2021](#bib.bib12))
    and HumanEval-X  Zheng et al. ([2023](#bib.bib59)) evaluation datasets to assess
    the model’s performance. HumanEval is a widely-used benchmark dataset released
    by OpenAI specifically designed to evaluate the Python code completion ability
    of large language models. HumanEval-X, on the other hand, is an expansion of HumanEval
    that enables the evaluation of large models’ code completion performance across
    various programming languages. Consistent with other studies, we employed the
    pass@1 metric as the evaluation measure.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码补全任务，我们使用了HumanEval Chen et al. ([2021](#bib.bib12))和HumanEval-X Zheng et
    al. ([2023](#bib.bib59))评估数据集来评估模型的性能。HumanEval是OpenAI发布的广泛使用的基准数据集，专门设计用于评估大型语言模型的Python代码补全能力。而HumanEval-X是HumanEval的扩展，支持评估大型模型在各种编程语言中的代码补全表现。与其他研究一致，我们使用了pass@1指标作为评估标准。
- en: 'We evaluated three models: SFT-S-CodeCompletion, SFT-Mixed, and MFT-5Tasks.
    The performance of these models on the HumanEval dataset is summarized in Table [2](#S4.T2
    "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning") (Column III). Results indicate
    that the MFT-5Tasks model, trained using the MFT approach, outperforms the other
    two models. It achieves a 2.44% higher performance compared to the SFT-Mixed model,
    which was fine-tuned with mixed task data. It is worth noting that the SFT-Mixed
    model does not perform as well as the SFT-S-CodeCompletion model, which was trained
    individually for the code-completion task.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了三种模型：SFT-S-CodeCompletion、SFT-Mixed和MFT-5Tasks。这些模型在HumanEval数据集上的表现总结见表[2](#S4.T2
    "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning")（第III列）。结果表明，采用MFT方法训练的MFT-5Tasks模型优于其他两个模型。与使用混合任务数据微调的SFT-Mixed模型相比，MFT-5Tasks模型的性能提高了2.44%。值得注意的是，SFT-Mixed模型的表现不如单独为代码补全任务训练的SFT-S-CodeCompletion模型。'
- en: 'Furthermore, we conducted a multilingual evaluation on the HumanEval-X dataset
    for the three models, as presented in Table [3](#S4.T3 "Table 3 ‣ 4.3.1 Code Completion
    ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask
    Fine-Tuning"). The MFT-5Tasks model demonstrates superior performance in Java
    and Golang, while the SFT-Mixed model excels in C++ and JavaScript. Overall, the
    evaluation affirms that the MFT-5Tasks model outperforms the others, with an average
    improvement of 1.22% over the SFT-Mixed model.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们还对这三个模型进行了多语言评估，如表[3](#S4.T3 "Table 3 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation
    Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")所示。MFT-5Tasks模型在Java和Golang中表现优异，而SFT-Mixed模型在C++和JavaScript中表现突出。总体而言，评估结果确认MFT-5Tasks模型的表现优于其他模型，平均提升了1.22%，超过了SFT-Mixed模型。'
- en: Overall, in terms of code completion tasks, models trained using the MFT method
    outperform both individually fine-tuned models and models fine-tuned after combining
    multiple tasks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，就代码补全任务而言，采用MFT方法训练的模型在性能上优于单独微调的模型和多任务组合后微调的模型。
- en: 'Table 2: Pass@1 performance on HumanEval (Code Completion) and MBPP (Text-to-Code
    Generation). We utilized the greedy decoding strategy with zero-shot. The values
    of CodeLlama-Python-base are taken from Rozière et al. ([2023](#bib.bib47)).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：HumanEval（代码补全）和MBPP（文本到代码生成）的pass@1性能。我们采用了零-shot的贪心解码策略。CodeLlama-Python-base的数值来自Rozière
    et al. ([2023](#bib.bib47))。
- en: '| Model | Size |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 |'
- en: '&#124; Humaneval &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Humaneval &#124;'
- en: '&#124; pass@1 &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; pass@1 &#124;'
- en: '|'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MBPP &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MBPP &#124;'
- en: '&#124; pass@1 &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; pass@1 &#124;'
- en: '| Average |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 平均'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 13B | 43.3% |
    49.0% | 46.15% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Python-base Rozière 等人 ([2023](#bib.bib47)) | 13B | 43.3% | 49.0%
    | 46.15% |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SFT-S-CodeCompletion | 13B | 59.76% | NA | NA |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeCompletion | 13B | 59.76% | NA | NA |'
- en: '| SFT-S-Text2Code | 13B | NA | 54.2% | NA |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-Text2Code | 13B | NA | 54.2% | NA |'
- en: '| SFT-Mixed | 13B | 57.93% | 53.6% | 55.765% |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 13B | 57.93% | 53.6% | 55.765% |'
- en: '| MFT-5Tasks | 13B | 60.37% | 56.0% | 58.185% |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 13B | 60.37% | 56.0% | 58.185% |'
- en: 'Table 3: Comparison of pass@1 Metric Performance on the multilingual HumanEval-X
    (zero-shot, greedy-decoding)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：多语言 HumanEval-X (零-shot，贪婪解码) 上 pass@1 指标性能比较
- en: '| Trained Model | Java | C++ | JavaScript | Golang | Average |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型 | Java | C++ | JavaScript | Golang | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CodeLlama-13B-Py-base | 43.3% | 41.46% | 34.76% | 38.41% | 29.27% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-Py-base | 43.3% | 41.46% | 34.76% | 38.41% | 29.27% |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SFT-S-CodeCompletion | 50.0% | 39.02% | 47.56% | 40.23% | 44.20% |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeCompletion | 50.0% | 39.02% | 47.56% | 40.23% | 44.20% |'
- en: '| SFT-Mixed | 56.1% | 48.17% | 56.10% | 37.80% | 49.54% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 56.1% | 48.17% | 56.10% | 37.80% | 49.54% |'
- en: '| MFT-5Tasks | 57.32% | 46.34% | 54.27% | 45.12% | 50.76% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 57.32% | 46.34% | 54.27% | 45.12% | 50.76% |'
- en: 4.3.2 Text-to-Code Generation
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 文本到代码生成
- en: To evaluate the models’ ability to generate code based on descriptions, we selected
    the MBPP Austin et al. ([2021](#bib.bib7)) evaluation dataset and used the pass@1
    metric. MBPP is specifically designed to assess models’ capacity to synthesize
    concise Python programs from natural language descriptions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型基于描述生成代码的能力，我们选择了 MBPP Austin 等人 ([2021](#bib.bib7)) 的评估数据集，并使用了 pass@1
    指标。MBPP 专门设计用于评估模型从自然语言描述中合成简洁 Python 程序的能力。
- en: 'We tested three models, namely SFT-S-Text2Code, SFT-Mixed, and MFT-5Tasks,
    on the MBPP dataset, measuring their pass@1 performance as shown in Table [2](#S4.T2
    "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning") (Column IV). Among these models,
    MFT-5Tasks exhibited the highest performance, surpassing the SFT-Mixed model by
    2.4%. Similarly, in terms of the text-to-code generation task, models fine-tuned
    after combining multiple tasks showed inferior performance compared to models
    fine-tuned specifically for this individual task.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 MBPP 数据集上测试了三个模型，即 SFT-S-Text2Code、SFT-Mixed 和 MFT-5Tasks，测量了它们的 pass@1
    性能，如表 [2](#S4.T2 "Table 2 ‣ 4.3.1 Code Completion ‣ 4.3 Evaluation Results ‣ 4
    Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") (列 IV)
    所示。在这些模型中，MFT-5Tasks 展现了最高的性能，比 SFT-Mixed 模型高出 2.4%。同样，在文本到代码生成任务中，经过多任务组合微调后的模型表现较差，相比之下，专门为该单一任务微调的模型效果更佳。'
- en: Overall, in terms of text-to-code generation tasks, models trained using the
    MFT method outperform both individually fine-tuned models and models fine-tuned
    after combining multiple tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，在文本到代码生成任务中，采用 MFT 方法训练的模型优于单独微调的模型和经过多任务组合微调的模型。
- en: 4.3.3 Code Comment Generation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 代码注释生成
- en: The objective of the code comment generation task is to have models add necessary
    comments to the code without modifying the input code itself. This includes both
    line comments and interface comments, making the code more readable and user-friendly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 代码注释生成任务的目标是让模型为代码添加必要的注释，而不修改输入代码本身。这包括行注释和接口注释，使代码更具可读性和用户友好性。
- en: 'To assess this capability, we constructed an evaluation set based on 500 MBPP
    test questions (id xx-xx). For each question in the evaluation set, we had the
    SFT-S-CodeComment, SFT-Mixed and MFT-5Tasks models generate comments for it. Subsequently,
    we employed GPT-4 as the referee, which has been instructed with criteria for
    good comments, to determine which model performed the best. If it was not possible
    to determine, the output was labeled as UNKNOWN. Finally, we counted the number
    of questions where each model was determined to perform the best and calculated
    the corresponding proportions, shown in Table [4](#S4.T4 "Table 4 ‣ 4.3.3 Code
    Comment Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning").'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估这一能力，我们基于 500 MBPP 测试问题（id xx-xx）构建了一个评估集。对于评估集中的每个问题，我们让 SFT-S-CodeComment、SFT-Mixed
    和 MFT-5Tasks 模型生成注释。随后，我们使用了 GPT-4 作为裁判，该模型已经被指导了良好注释的标准，以确定哪个模型表现最佳。如果无法确定，则将结果标记为
    UNKNOWN。最后，我们统计了每个模型被确定为最佳表现的问卷数量，并计算了相应的比例，如表[4](#S4.T4 "Table 4 ‣ 4.3.3 Code
    Comment Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning")所示。'
- en: It can be observed that 38.8% of the questions were determined to be best performed
    by the MFT-5Tasks model, surpassing the second-ranked SFT-Mixed by 7.4% and the
    third-ranked SFT-S-CodeComment by 10.8%. Additionally, 1.8% of the questions were
    marked as indeterminable by GPT-4. In summary, for this task, the models trained
    using the MFT method exhibit the best performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，38.8% 的问题被确定为由 MFT-5Tasks 模型表现最佳，超越了排名第二的 SFT-Mixed 7.4% 和排名第三的 SFT-S-CodeComment
    10.8%。此外，1.8% 的问题被标记为 GPT-4 无法确定。在总结中，对于此任务，使用 MFT 方法训练的模型表现最佳。
- en: 'Table 4: Performance Comparison of Three Models on Code Commenting Task. GPT-4
    Determines the Best Performing Model for Each Question. This Table Presents the
    Proportion of Questions Where Each Model Performs the Best. In particular, 1.8%
    of the evaluation cases were indeterminate for GPT-4 to determine the best-performing
    model.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：三种模型在代码注释任务上的性能比较。GPT-4 确定了每个问题的最佳表现模型。此表展示了每个模型在最佳表现方面的比例。特别地，1.8% 的评估案例对于
    GPT-4 来说是无法确定最佳表现模型的。
- en: '| Training Model | Best identified by GPT-4 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型 | GPT-4 确定的最佳模型 |'
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SFT-S-CodeComment | 28% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeComment | 28% |'
- en: '| SFT-Mixed | 31.4% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 31.4% |'
- en: '| MFT-5Tasks | 38.8% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 38.8% |'
- en: 4.3.4 Code Translation
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 代码翻译
- en: The objective of the code translation task is to accurately and precisely translate
    a given code snippet implemented in the source language into an equivalent code
    snippet implemented in the target language while ensuring that both implementations
    possess identical functionality. Here, we utilize the codefuseEval ⁴⁴4https://github.com/codefuse-ai/codefuse-evaluation Di
    et al. ([2023](#bib.bib19)) evaluation datasets’ code translation subset to support
    bidirectional translation between Java, Python, and C++. In order to evaluate
    the accuracy and functional equivalence of the translation results, we employ
    test cases that are semantically equivalent to ones of the source program for
    each task. These test cases are used to verify whether the result code can run
    and pass successfully, as indicated by the pass@1 criterion.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代码翻译任务的目标是准确无误地将源语言中实现的代码片段翻译成目标语言中等效的代码片段，同时确保两种实现具有相同的功能。在这里，我们利用 codefuseEval
    ⁴⁴4https://github.com/codefuse-ai/codefuse-evaluation Di 等人 ([2023](#bib.bib19))
    评估数据集的代码翻译子集，以支持 Java、Python 和 C++ 之间的双向翻译。为了评估翻译结果的准确性和功能等效性，我们使用了与源程序在语义上等价的测试用例。这些测试用例用于验证结果代码是否可以运行并成功通过，如
    pass@1 标准所示。
- en: 'The test results of the three models are presented in Table [5](#S4.T5 "Table
    5 ‣ 4.3.4 Code Translation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning"). The MFT-5Tasks model performs
    the best in Python-to-Java, Python-to-C++, and C++-to-Java translations. The SFT-Mixed
    model excels in C++-to-Python translation, while the SFT-S-CodeTrans model performs
    the best in Java-to-Python and Java-to-C++ translations. Overall, the MFT-5Tasks
    model demonstrates superior performance, with an average improvement of 0.93%
    over SFT-Mixed and 10.9% over SFT-S-CodeTrans. This task also highlights the phenomenon
    that models trained using the MFT approach outperform the other two training methods.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '三种模型的测试结果见表 [5](#S4.T5 "Table 5 ‣ 4.3.4 Code Translation ‣ 4.3 Evaluation Results
    ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")。MFT-5Tasks
    模型在 Python 到 Java、Python 到 C++ 和 C++ 到 Java 的翻译中表现最好。SFT-Mixed 模型在 C++ 到 Python
    的翻译中表现出色，而 SFT-S-CodeTrans 模型在 Java 到 Python 和 Java 到 C++ 的翻译中表现最好。总体而言，MFT-5Tasks
    模型表现优越，相比 SFT-Mixed 平均提高 0.93%，相比 SFT-S-CodeTrans 提高 10.9%。这一任务也突显了使用 MFT 方法训练的模型优于其他两种训练方法的现象。'
- en: 'Table 5: Comparison of pass@1 Metric Performance on the codefuseEVAL-CodeTranslation
     Di et al. ([2023](#bib.bib19)) (zero-shot, greedy-decoding)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在 codefuseEVAL-CodeTranslation Di 等 ([2023](#bib.bib19)) 上 pass@1 指标性能的比较（零样本，贪婪解码）
- en: '| Training Model | Py2Java | Py2C++ | Java2Py | C++2Py | Java2C++ | C++2Java
    | Avg. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型 | Py2Java | Py2C++ | Java2Py | C++2Py | Java2C++ | C++2Java | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SFT-S-CodeTrans | 59.52% | 57.40% | 70.73% | 62.20% | 67.07% | 62.80% | 63.29%
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-CodeTrans | 59.52% | 57.40% | 70.73% | 62.20% | 67.07% | 62.80% | 63.29%
    |'
- en: '| SFT-Mixed | 80.16% | 71.20% | 67.68% | 72.56% | 65.85% | 82.31% | 73.29%
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 80.16% | 71.20% | 67.68% | 72.56% | 65.85% | 82.31% | 73.29%
    |'
- en: '| MFT-5Tasks | 82.16% | 77.20% | 65.85% | 70.73% | 64.64% | 84.76% | 74.22%
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 82.16% | 77.20% | 65.85% | 70.73% | 64.64% | 84.76% | 74.22%
    |'
- en: 4.3.5 Unit Test Case Generation
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 单元测试用例生成
- en: The task at hand is to generate unit test cases by training a model to produce
    a set of test cases for a given code snippet, such as a method or class, and verify
    if the provided code implementation is correct. We have opted to utilize the unittest
    subset from the codefuseEVAL Di et al. ([2023](#bib.bib19)) evaluation datasets
    as our test suite. We evaluate the test cases using the pass@1 metric, which means
    that if a model generates test cases for a sample program and the sample program
    passes all the test cases, the count of correctly generated samples increases
    by 1\. Similar to CodeLLama Rozière et al. ([2023](#bib.bib47)), we employ a greedy
    decoding strategy during the evaluation process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是通过训练模型生成一组测试用例，用于验证给定代码片段（如方法或类）的正确性。我们选择了 codefuseEVAL Di 等 ([2023](#bib.bib19))
    评估数据集中的 unittest 子集作为测试套件。我们使用 pass@1 指标来评估测试用例，这意味着如果模型为样本程序生成的测试用例全部通过，该样本程序的正确生成计数增加
    1。类似于 CodeLLama Rozière 等 ([2023](#bib.bib47))，我们在评估过程中采用贪婪解码策略。
- en: 'We compared three models across Python, Java, and JavaScript for their test
    generation capabilities. The results in Table [6](#S4.T6 "Table 6 ‣ 4.3.5 Unit
    Test Case Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") indicate that the MFT-5Tasks model outperforms
    others in test generation for Python, with a 5.73% lead over the second-ranked
    SFT-Mixed model, and a significant 10.19% lead over the third-ranked SFT-S-UnitTest
    model. In JavaScript, the MFT-5Tasks model also excels, with a 7.93% advantage
    over the other models. However, in Java, the MFT-5Tasks model performs 5.37% better
    than SFT-S-UnitTest, but slightly lags behind SFT-Mixed by 5.44%. Overall, the
    MFT-5Tasks model consistently demonstrates the highest performance, with an average
    improvement of 2.74% over SFT-Mixed, and a remarkable 7.83% improvement over SFT-S-UnitTest.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了三种模型在 Python、Java 和 JavaScript 的测试生成能力。表 [6](#S4.T6 "Table 6 ‣ 4.3.5 Unit
    Test Case Generation ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") 的结果显示，MFT-5Tasks 模型在 Python 的测试生成上优于其他模型，比第二名的
    SFT-Mixed 模型高出 5.73%，比第三名的 SFT-S-UnitTest 模型高出 10.19%。在 JavaScript 中，MFT-5Tasks
    模型也表现出色，比其他模型有 7.93% 的优势。然而，在 Java 中，MFT-5Tasks 模型比 SFT-S-UnitTest 高 5.37%，但比
    SFT-Mixed 低 5.44%。总体而言，MFT-5Tasks 模型始终展现出最高的性能，相比 SFT-Mixed 平均提高 2.74%，相比 SFT-S-UnitTest
    提高了 7.83%。'
- en: In summary, training models using the MFT approach yielded better performance
    compared to models fine-tuned with the same data mixed together. Furthermore,
    the performance of the MFT-trained models surpassed that of the model individually
    fine-tuned for the UNIT-TEST task.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用 MFT 方法训练的模型表现优于使用相同数据混合训练的模型。此外，MFT 训练的模型在 UNIT-TEST 任务中表现超越了单独为该任务微调的模型。
- en: 'Table 6: Comparison of pass@1 Metric Performance on the codefuseEVAL-TestcaseGeneration Di
    et al. ([2023](#bib.bib19)) (zero-shot, greedy-decoding)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在 codefuseEVAL-TestcaseGeneration Di 等 ([2023](#bib.bib19)) 上 pass@1 绩效的比较（零样本，贪婪解码）
- en: '| Trained Model | Python | Java | JavaScript | Average |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型 | Python | Java | JavaScript | 平均 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SFT-S-UnitTest | 33.76% | 32.43% | 41.46% | 35.88% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| SFT-S-UnitTest | 33.76% | 32.43% | 41.46% | 35.88% |'
- en: '| SFT-Mixed | 38.22% | 43.24% | 41.46% | 40.97% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 38.22% | 43.24% | 41.46% | 40.97% |'
- en: '| MFT-5Tasks | 43.95% | 37.8% | 49.39% | 43.71% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 43.95% | 37.8% | 49.39% | 43.71% |'
- en: 4.3.6 Generalization on an Unseen Task
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.6 对未见任务的泛化
- en: In addition to evaluating the performance of our models on tasks with training
    data to address RQ1 and RQ2, we also have RQ3 to answer, which is whether the
    models trained using the MFT approach exhibit better generalization on unseen
    tasks compared to models trained by combining multiple datasets into a single
    SFT approach. To investigate this, we selected the Text-to-SQL generation task
    as our testing target. The data for this task was not included in the training
    of the seven existing models. Furthermore, this task is code-related but distinctly
    different from the previous five downstream tasks.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评估我们模型在有训练数据任务上的表现以应对 RQ1 和 RQ2，我们还需要回答 RQ3，即使用 MFT 方法训练的模型是否比通过将多个数据集结合成单一
    SFT 方法训练的模型在未见任务上的泛化能力更强。为此，我们选择了文本到 SQL 生成任务作为测试目标。该任务的数据未包含在七个现有模型的训练中。此外，这个任务与之前五个下游任务相关，但有所不同。
- en: We have selected two evaluation metrics, BLEU score and logical accuracy of
    SQL statements. The BLEU score assesses the textual similarity between the generated
    outputs and the reference answers. The logical accuracy metric, on the other hand,
    allows us to address the variations in SQL syntax that may occur. Specifically,
    Logical Accuracy measures the proportion of test samples in the dataset where
    the generated SQL statements are both syntactically correct and semantically equivalent
    to the reference answers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了两个评估指标：BLEU 分数和 SQL 语句的逻辑准确性。BLEU 分数评估生成输出与参考答案之间的文本相似性。另一方面，逻辑准确性指标使我们能够处理可能出现的
    SQL 语法变异。具体来说，逻辑准确性衡量数据集中测试样本的比例，其中生成的 SQL 语句在语法上正确且语义上等同于参考答案。
- en: 'Table 7: Sample Text-to-SQL Test Cases from the Spider Yu et al. ([2019b](#bib.bib57))
    dataset and Example Predictions by the SFT-Mixed Model and MFT-5Tasks Model.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：来自 Spider Yu 等 ([2019b](#bib.bib57)) 数据集的文本到 SQL 测试用例示例以及 SFT-Mixed 模型和
    MFT-5Tasks 模型的示例预测。
- en: '| PROMPT: |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 提示： |'
- en: '&#124; <&#124;role_start&#124;>system<&#124;role_end&#124;>You are a professional
    SQL engineer and you are writing SQL queries for data query tasks. &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <&#124;role_start&#124;>system<&#124;role_end&#124;>你是专业的 SQL 工程师，你正在为数据查询任务编写
    SQL 查询。&#124;'
- en: '&#124; <&#124;role_start&#124;>human<&#124;role_end&#124;>What are the names
    of the heads who are born outside the California state? &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <&#124;role_start&#124;>human<&#124;role_end&#124;>哪些负责人出生在加州以外？ &#124;'
- en: '&#124; schema:head(head_ID&#124;name&#124;born_state&#124;age &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; schema:head(head_ID&#124;name&#124;born_state&#124;age &#124;'
- en: '&#124; <&#124;role_start&#124;>bot<&#124;role_end&#124;> &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <&#124;role_start&#124;>bot<&#124;role_end&#124;> &#124;'
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LABEL: |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 标签： |'
- en: '&#124; SELECT name FROM head WHERE born_state != ’California’ &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SELECT name FROM head WHERE born_state != ’California’ &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SFT-Mixed: |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed： |'
- en: '&#124; You can use the SQL SELECT statement to retrieve the names of the heads
    who are born outside the California state. Here is a sample query: &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你可以使用 SQL SELECT 语句来检索出生在加州以外的负责人姓名。以下是一个示例查询：&#124;'
- en: '&#124; “‘sql &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “‘sql &#124;'
- en: '&#124; SELECT name &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SELECT name &#124;'
- en: '&#124; FROM head &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FROM head &#124;'
- en: '&#124; WHERE born_state <> ’California’; &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WHERE born_state <> ’California’; &#124;'
- en: '&#124; “‘ &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “‘ &#124;'
- en: '&#124; This query selects the name column from the head table where the born_state
    column is not equal to ’California’. &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 该查询从负责人表中选择姓名列，其中 born_state 列的值不等于 ’California’。&#124;'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MFT-5Tasks: |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks： |'
- en: '&#124; SELECT name &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SELECT name &#124;'
- en: '&#124; FROM head &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FROM head &#124;'
- en: '&#124; WHERE born_state <> ’California’; &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WHERE born_state <> ’California’; &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'We selected five representative text-to-SQL datasets, including WikiSQL Zhong
    et al. ([2017](#bib.bib60)), Spider Yu et al. ([2019b](#bib.bib57)), CSpider Min
    et al. ([2019](#bib.bib41)), CoSQL Yu et al. ([2019a](#bib.bib56)), and BirdSQL Li
    et al. ([2023d](#bib.bib31)), and randomly sampled 200 examples from each dataset
    for evaluation. The test case examples are shown in Table [7](#S4.T7 "Table 7
    ‣ 4.3.6 Generalization on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning"), where the first row
    demonstrates the fine-tuned data format similar to OpenAI ChatML format ⁵⁵5https://github.com/openai/openai-python/blob/main/chatml.md.
    Using each sampled dataset, we tested the logical accuracy and BLEU score of the
    SFT-Mixed and MFT-5Tasks models, as shown in Table [8](#S4.T8 "Table 8 ‣ 4.3.6
    Generalization on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了五个具有代表性的文本到 SQL 数据集，包括 WikiSQL Zhong et al. ([2017](#bib.bib60))，Spider
    Yu et al. ([2019b](#bib.bib57))，CSpider Min et al. ([2019](#bib.bib41))，CoSQL
    Yu et al. ([2019a](#bib.bib56)) 和 BirdSQL Li et al. ([2023d](#bib.bib31))，并从每个数据集中随机抽取了
    200 个示例进行评估。测试案例示例如表格 [7](#S4.T7 "Table 7 ‣ 4.3.6 Generalization on an Unseen
    Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning") 所示，第一行展示了类似于 OpenAI ChatML 格式的微调数据格式 ⁵⁵5https://github.com/openai/openai-python/blob/main/chatml.md。使用每个抽样数据集，我们测试了
    SFT-Mixed 和 MFT-5Tasks 模型的逻辑准确性和 BLEU 分数，如表格 [8](#S4.T8 "Table 8 ‣ 4.3.6 Generalization
    on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning") 所示。'
- en: 'According to Table [8](#S4.T8 "Table 8 ‣ 4.3.6 Generalization on an Unseen
    Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning"), MFT-5Tasks outperforms SFT-Mixed in terms of BLEU scores
    on each dataset, averaging 2.78 times higher. This indicates that the generated
    results of MFT-5Tasks exhibit higher similarity to the reference answer texts.
    This similarity can also be observed in Table [7](#S4.T7 "Table 7 ‣ 4.3.6 Generalization
    on an Unseen Task ‣ 4.3 Evaluation Results ‣ 4 Evaluation ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"), where MFT-5Tasks produces cleaner results,
    while SFT-Mixed provides more explanations, which may be preferred in certain
    scenarios. Moreover, MFT-5Tasks demonstrates better performance in terms of logical
    accuracy, achieving an overall accuracy that is 2.18 times higher than SFT-Mixed
    model, and up to 4.67 times higher on the WikiSQL dataset. Numerically, MFT-5Tasks
    exhibits superior performance compared to SFT-Mixed, indicating stronger generalization
    of MFT-trained models on the Text-to-SQL task, which is an unseen task during
    training.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表格 [8](#S4.T8 "Table 8 ‣ 4.3.6 Generalization on an Unseen Task ‣ 4.3 Evaluation
    Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")，MFT-5Tasks
    在每个数据集上的 BLEU 分数优于 SFT-Mixed，平均高出 2.78 倍。这表明 MFT-5Tasks 生成的结果与参考答案文本的相似度更高。这种相似性也可以在表格
    [7](#S4.T7 "Table 7 ‣ 4.3.6 Generalization on an Unseen Task ‣ 4.3 Evaluation
    Results ‣ 4 Evaluation ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")
    中观察到，其中 MFT-5Tasks 生成的结果更干净，而 SFT-Mixed 提供更多解释，这在某些情况下可能更受欢迎。此外，MFT-5Tasks 在逻辑准确性方面表现更好，总体准确率是
    SFT-Mixed 模型的 2.18 倍，在 WikiSQL 数据集上的准确率高达 4.67 倍。从数字上看，MFT-5Tasks 的表现优于 SFT-Mixed，表明
    MFT 训练的模型在文本到 SQL 任务上的泛化能力更强，而该任务在训练期间是未见过的。'
- en: 'Table 8: Comparison of generalization capabilities between MFT-5Tasks and SFT-Mixed
    on the Text-to-SQL task. The evaluation metrics include SQL logical accuracy and
    BLEU score.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：MFT-5Tasks 和 SFT-Mixed 在文本到 SQL 任务上的泛化能力比较。评估指标包括 SQL 逻辑准确性和 BLEU 分数。
- en: '| Trained Model | WIKISQL | SPIDER | CSPIDER | COSQL | BiRDSQL | Average |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 训练模型 | WIKISQL | SPIDER | CSPIDER | COSQL | BiRDSQL | 平均 |'
- en: '| Logical Accuracy |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑准确性 |'
- en: '| SFT-Mixed | 1.5% | 2.0% | 7.0% | 6.5% | 5.5% | 4.5% |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 1.5% | 2.0% | 7.0% | 6.5% | 5.5% | 4.5% |'
- en: '| MFT-5Tasks | 7.0% (4.67x) | 4.5% (2.25x) | 16.5% (2.36x) | 10.5%(1.62x) |
    10.5% (1.91x) | 9.8% (2.18x) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 7.0% (4.67x) | 4.5% (2.25x) | 16.5% (2.36x) | 10.5%(1.62x) |
    10.5% (1.91x) | 9.8% (2.18x) |'
- en: '| BLEU |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| BLEU |'
- en: '| SFT-Mixed | 0.032 | 0.047 | 0.025 | 0.081 | 0.026 | 0.042 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SFT-Mixed | 0.032 | 0.047 | 0.025 | 0.081 | 0.026 | 0.042 |'
- en: '| MFT-5Tasks | 0.138 | 0.138 | 0.116 | 0.119 | 0.074 | 0.117 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| MFT-5Tasks | 0.138 | 0.138 | 0.116 | 0.119 | 0.074 | 0.117 |'
- en: 4.4 Evaluation Summary
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 评估总结
- en: 'We selected five downstream tasks related to code and trained a total of seven
    models, including SFT-S-* models fine-tuned individually for each task, the SFT-Mixed
    model fine-tuned with a mixture of all task data, and the MFT-5Tasks model trained
    using the MFT method. We compared and tested the performance of each model in
    terms of their target capabilities. Additionally, we evaluated the generalization
    performance of the MFT method and the mixed SFT method on unseen tasks. The results
    can be summarized as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了五个与代码相关的下游任务，训练了总共七个模型，包括单独为每个任务微调的 SFT-S-* 模型，混合所有任务数据进行微调的 SFT-Mixed
    模型，以及使用 MFT 方法训练的 MFT-5Tasks 模型。我们比较和测试了每个模型在目标能力方面的性能。此外，我们评估了 MFT 方法和混合 SFT
    方法在未见过任务上的泛化性能。结果可以总结如下：
- en: i
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: i
- en: Models trained with the MFT method outperformed those fine-tuned individually
    for each task, indicating a positive answer to RQ1.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 MFT 方法训练的模型在性能上优于单独为每个任务微调的模型，这表明对 RQ1 的回答是积极的。
- en: ii
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ii
- en: Models trained with the MFT method outperformed those fine-tuned with a mixture
    of multiple tasks, providing a positive answer to RQ2.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 MFT 方法训练的模型在性能上优于通过多任务混合进行微调的模型，这为 RQ2 提供了积极的回答。
- en: iii
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: iii
- en: Models trained with the MFT method exhibit stronger generalization capabilities
    on new, unseen tasks compared to the SFT models fine-tuned with a mixture of multiple
    task data.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 MFT 方法训练的模型在处理新出现的任务时，较 SFT 模型表现出更强的泛化能力，而 SFT 模型是通过混合多任务数据进行微调的。
- en: 5 Application
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 应用
- en: 'Considering the outstanding performance of the MFT training method, we have
    leveraged our MFTCoder ^([2](#footnote2 "footnote 2 ‣ 3 Approach ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning")), developed based on this approach, to
    fine-tune the existing mainstream open-source LLM models. e.g. QWen Bai et al.
    ([2023](#bib.bib8)), Baichuan Baichuan ([2023](#bib.bib9)), CodeGeex2 Zheng et al.
    ([2023](#bib.bib59)), Llama Touvron et al. ([2023a](#bib.bib51)), LLama2 Touvron
    et al. ([2023b](#bib.bib52)), CodeLLama Rozière et al. ([2023](#bib.bib47)), StarCoder Li
    et al. ([2023a](#bib.bib32)).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 MFT 训练方法的卓越表现，我们利用基于这种方法开发的 MFTCoder ^([2](#footnote2 "脚注 2 ‣ 3 方法 ‣ MFTCoder：通过多任务微调提升代码
    LLM"))，对现有主流开源 LLM 模型进行了微调。例如：QWen Bai 等人 ([2023](#bib.bib8))，Baichuan Baichuan
    ([2023](#bib.bib9))，CodeGeex2 Zheng 等人 ([2023](#bib.bib59))，Llama Touvron 等人 ([2023a](#bib.bib51))，LLama2
    Touvron 等人 ([2023b](#bib.bib52))，CodeLLama Rozière 等人 ([2023](#bib.bib47))，StarCoder
    Li 等人 ([2023a](#bib.bib32))。
- en: MFTCoder supports Lora and QLora, which significantly reduces the number of
    model training parameters. Coupled with dual quantization for model size compression,
    this ultimately leads to a substantial reduction in GPU memory requirements. As
    a result, it becomes possible to fine-tune a 70B model on a single A100 GPU with
    ease. When fine-tuning these models using MFTCoder, we set the trainable parameters
    to be within the range of 0.1% to 5% of the total parameters. Through empirical
    evidence, we have found that as the proportion of trainable parameters increases,
    performance improvement tends to plateau. In fact, we have observed that a trainable
    parameter proportion of less than 5% is often sufficient to achieve performance
    levels close to that of full-scale fine-tuning.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: MFTCoder 支持 Lora 和 QLora，这显著减少了模型训练参数的数量。结合双重量化进行模型尺寸压缩，这最终导致 GPU 内存需求的大幅减少。因此，可以轻松地在单个
    A100 GPU 上微调 70B 模型。在使用 MFTCoder 微调这些模型时，我们将可训练的参数设置为总参数的 0.1% 到 5% 范围内。通过实证研究，我们发现随着可训练参数比例的增加，性能提升趋于平稳。实际上，我们观察到可训练参数比例低于
    5% 通常足以实现接近全面微调的性能水平。
- en: 'When fine-tuning these models, we configure them for multitasking with a range
    of 3-7 tasks. Depending on the model size, we typically use Lora mode for models
    below 20B, and QLora mode for models larger than 20B. After fine-tuning, we evaluate
    their performance in code completion and text-to-code generation tasks, measuring
    their performance on HumanEval Chen et al. ([2021](#bib.bib12)) and MBPP Austin
    et al. ([2021](#bib.bib7)), as shown in Table [9](#S5.T9 "Table 9 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") Column III and IV.
    We have calculated the average improvement of MFT fine-tuning compared to the
    base models in terms of HumanEval and MBPP. As shown in column 5, the improvement
    ranges from 6.26% to 12.75%, with the improvements on HumanEval consistently surpassing
    those on MBPP. Additionally, we have also evaluated the code completion performance
    of the MFTCoder fine-tuned models on the multilingual benchmark, HumanEval-X Zheng
    et al. ([2023](#bib.bib59)). The results of this evaluation are presented in Table [10](#S5.T10
    "Table 10 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").
    Notably, the fine-tuned CodeFuse-CodeLLama-Python-MFT (34B) achieved an average
    pass@1 of 56.88% across four languages: Java, C++, JavaScript, and Golang.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '在对这些模型进行微调时，我们将它们配置为支持3到7个任务的多任务处理。根据模型的大小，我们通常对20B以下的模型使用Lora模式，对20B以上的模型使用QLora模式。微调后，我们在代码完成和文本到代码生成任务中评估它们的表现，测量它们在HumanEval Chen
    et al. ([2021](#bib.bib12)) 和MBPP Austin et al. ([2021](#bib.bib7))上的表现，如表[9](#S5.T9
    "Table 9 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")的第III和IV列所示。我们计算了MFT微调相比于基础模型在HumanEval和MBPP上的平均改进。如第5列所示，改进范围从6.26%到12.75%，其中HumanEval的改进
    consistently 超过了MBPP。此外，我们还在多语言基准HumanEval-X Zheng et al. ([2023](#bib.bib59))上评估了MFTCoder微调模型的代码完成性能。此评估的结果呈现在表[10](#S5.T10
    "Table 10 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")中。值得注意的是，经过微调的CodeFuse-CodeLLama-Python-MFT
    (34B)在四种语言（Java、C++、JavaScript和Golang）中的平均pass@1为56.88%。'
- en: 'Table [9](#S5.T9 "Table 9 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with
    Multitask Fine-Tuning") also presents the performance of fine-tuned open-source
    models (e.g. OctoPack Muennighoff et al. ([2023](#bib.bib42)) and WizardCoder-Python Luo
    et al. ([2023](#bib.bib40))) and representative closed-source models (e.g., Claude2 Anthropic
    ([2023](#bib.bib5)), GPT-4 OpenAI ([2023](#bib.bib43))) on HumanEval and MBPP.
    It is worth noting that our fine-tuned model, CodeFuse-CodeLLama-34B ⁶⁶6https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B,
    based on CodeLlama-34B-Python achieves a remarkable performance of 74.4% on HumanEval,
    surpassing all the listed models in the table, including GPT-4 (67.00%, zero-shot) OpenAI
    ([2023](#bib.bib43)). We also evaluated the performance of the model on other
    benchmarks, including multilingual HumanEval-X Zheng et al. ([2023](#bib.bib59)),
    MBPP Austin et al. ([2021](#bib.bib7)), DS-1000 Lai et al. ([2022](#bib.bib28))and
    codefuseEval Di et al. ([2023](#bib.bib19)), and compared it against GPT-3.5 and
    GPT-4, as shown in Figure [5](#S5.F5 "Figure 5 ‣ 5 Application ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). CodeFuse-CodeLLama-34B outperforms GPT-4
    on CodeFuseEval-UnitTest and Humaneval, matches its performance in code translation
    ability, but falls behind in Chinese code completion (CodeFuseEval-CN), multi-language
    completion, data-science analysis (DS-1000), and text-to-code generation (MBPP)
    capabilities compared to GPT-4\. However, it surpasses or equals GPT-3.5 on all
    evaluation datasets. The input-output examples on each evaluation dataset can
    be found in Appendix [C](#A3 "Appendix C Examples of codefuse-codellama-34b across
    various tasks ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning").'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#S5.T9 "Table 9 ‣ 5 Application ‣ MFTCoder: Boosting Code LLMs with Multitask
    Fine-Tuning") 还展示了微调的开源模型（如 OctoPack Muennighoff 等 ([2023](#bib.bib42)) 和 WizardCoder-Python
    Luo 等 ([2023](#bib.bib40))) 和代表性闭源模型（如 Claude2 Anthropic ([2023](#bib.bib5))、GPT-4
    OpenAI ([2023](#bib.bib43))) 在 HumanEval 和 MBPP 上的表现。值得注意的是，我们微调后的模型 CodeFuse-CodeLLama-34B
    ⁶⁶6https://huggingface.co/codefuse-ai/CodeFuse-CodeLlama-34B 基于 CodeLlama-34B-Python
    在 HumanEval 上取得了 74.4% 的显著成绩，超越了表中所有列出的模型，包括 GPT-4 (67.00%，零-shot) OpenAI ([2023](#bib.bib43))。我们还评估了该模型在其他基准上的表现，包括多语言
    HumanEval-X Zheng 等 ([2023](#bib.bib59))、MBPP Austin 等 ([2021](#bib.bib7))、DS-1000
    Lai 等 ([2022](#bib.bib28)) 和 codefuseEval Di 等 ([2023](#bib.bib19))，并与 GPT-3.5
    和 GPT-4 进行了比较，如图 [5](#S5.F5 "Figure 5 ‣ 5 Application ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning") 所示。CodeFuse-CodeLLama-34B 在 CodeFuseEval-UnitTest
    和 Humaneval 上优于 GPT-4，在代码翻译能力上与其表现相当，但在中文代码补全 (CodeFuseEval-CN)、多语言补全、数据科学分析 (DS-1000)
    和文本到代码生成 (MBPP) 能力上落后于 GPT-4。然而，在所有评估数据集上，它超越或与 GPT-3.5 相当。各评估数据集上的输入输出示例可以在附录
    [C](#A3 "Appendix C Examples of codefuse-codellama-34b across various tasks ‣
    MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") 中找到。'
- en: 'Furthermore, we conducted an evaluation to assess the impact of fine-tuning
    the models with MFTCoder and code-related data on their performance in NLP tasks,
    as illustrated in Figure [6](#S5.F6 "Figure 6 ‣ 5 Application ‣ MFTCoder: Boosting
    Code LLMs with Multitask Fine-Tuning"). Taking CodeFuse-QWen-14B as a case study,
    we compared it against the base model QWen-14B and the official model QWen-14B-chat
    fine-tuned by Alibaba Cloud on top of it. It is evident that CodeFuse-QWen-14B
    maintains its proficiency in NLP. In fact, it exhibits a slight enhancement in
    language, reasoning, and understanding abilities compared to the other two models.
    However, there is a minor decline in its examination ability when compared to
    the base model QWen-14B and similar findings are observed for the fine-tuned QWen-14B-chat
    model.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们进行了评估，以评估使用 MFTCoder 和与代码相关的数据对模型性能的影响，如图 [6](#S5.F6 "Figure 6 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning") 所示。以 CodeFuse-QWen-14B
    为案例，我们将其与基础模型 QWen-14B 和 Alibaba Cloud 在其基础上微调的官方模型 QWen-14B-chat 进行了比较。显而易见，CodeFuse-QWen-14B
    保持了其在 NLP 方面的高效性。实际上，与其他两个模型相比，它在语言、推理和理解能力上略有提升。然而，与基础模型 QWen-14B 相比，其考试能力略有下降，微调后的
    QWen-14B-chat 模型也有类似的发现。'
- en: 'Table 9: pass@1 performance on HumanEval Chen et al. ([2021](#bib.bib12)) (Code
    Completion) and MBPP Austin et al. ([2021](#bib.bib7)) (Text-to-Code Generation)
    after fine-tuning with MFTCoder across multiple mainstream open-source models.
    The CodeFuse-*-MFT models are evaluated using a combination of greedy decoding
    and zero-shot testing strategy, while the metric values for the other models are
    taken from their respective papers, technical reports, or open-source project
    homepages.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在经过 MFTCoder 微调后，HumanEval Chen et al. ([2021](#bib.bib12))（代码补全）和 MBPP Austin
    et al. ([2021](#bib.bib7))（文本到代码生成）上的 pass@1 性能比较。这些 CodeFuse-*-MFT 模型使用贪婪解码和零-shot
    测试策略进行评估，而其他模型的指标值取自其各自的论文、技术报告或开源项目主页。
- en: '| Model | Size |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 |'
- en: '&#124; Humaneval &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Humaneval &#124;'
- en: '&#124; pass@1 &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; pass@1 &#124;'
- en: '|'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MBPP &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MBPP &#124;'
- en: '&#124; pass@1 &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; pass@1 &#124;'
- en: '| Average |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 |'
- en: '| Open-source base models |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 开源基础模型 |  |'
- en: '| QWen-base Bai et al. ([2023](#bib.bib8)) | 14B | 32.3% | 40.8% | 36.55% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| QWen-base Bai et al. ([2023](#bib.bib8)) | 14B | 32.3% | 40.8% | 36.55% |'
- en: '| Llama-base Touvron et al. ([2023a](#bib.bib51)) | 65B | 23.7% | 37.7% | 30.7%
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Llama-base Touvron et al. ([2023a](#bib.bib51)) | 65B | 23.7% | 37.7% | 30.7%'
- en: '| Llama2-base Touvron et al. ([2023b](#bib.bib52)) | 70B | 29.9% | 45.0% |
    37.45% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-base Touvron et al. ([2023b](#bib.bib52)) | 70B | 29.9% | 45.0% |
    37.45% |'
- en: '| StarCoder-base Li et al. ([2023a](#bib.bib32)) | 15B | 33.6% | 52.7% | 43.15%
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-base Li et al. ([2023a](#bib.bib32)) | 15B | 33.6% | 52.7% | 43.15%
    |'
- en: '| CodeGeex2-base Zheng et al. ([2023](#bib.bib59)) | 6B | 35.9% | 42.4% | 39.15%
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| CodeGeex2-base Zheng et al. ([2023](#bib.bib59)) | 6B | 35.9% | 42.4% | 39.15%
    |'
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 13B | 43.3% |
    49.0% | 46.15% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 13B | 43.3% |
    49.0% | 46.15% |'
- en: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 34B | 53.7% |
    56.2% | 54.95% |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Python-base Rozière et al. ([2023](#bib.bib47)) | 34B | 53.7% |
    56.2% | 54.95% |'
- en: '| MFT fine-tuned models |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| MFT 微调模型 |  |'
- en: '| CodeFuse-QWen-MFT ⁷⁷7https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B
    | 14B | 48.78% | 43.8% | 46.29% (+9.74%) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-QWen-MFT ⁷⁷7https://huggingface.co/codefuse-ai/CodeFuse-QWen-14B
    | 14B | 48.78% | 43.8% | 46.29% (+9.74%) |'
- en: '| CodeFuse-Llama-MFT | 65B | 34.76% | 41.8% | 38.28% (+7.58) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-Llama-MFT | 65B | 34.76% | 41.8% | 38.28% (+7.58) |'
- en: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 40.8% | 40.83% (+3.38%) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 40.8% | 40.83% (+3.38%) |'
- en: '| CodeFuse-StarCoder-MFT ⁸⁸8https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B
    | 15B | 54.90% | 49.60% | 52.25% (+9.10%) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-StarCoder-MFT ⁸⁸8https://huggingface.co/codefuse-ai/CodeFuse-StarCoder-15B
    | 15B | 54.90% | 49.60% | 52.25% (+9.10%) |'
- en: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 46.2% | 45.66% (+6.51%) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 46.2% | 45.66% (+6.51%) |'
- en: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 56.0% | 58.19% (+12.04%) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 56.0% | 58.19% (+12.04%) |'
- en: '| CodeFuse-CodeLLama-Python-MFT ^([6](#footnote6 "footnote 6 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")) | 34B | 74.4% | 61.0%
    | 67.70% (+12.75%) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeLLama-Python-MFT ^([6](#footnote6 "footnote 6 ‣ 5 Application
    ‣ MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning")) | 34B | 74.4% | 61.0%
    | 67.70% (+12.75%) |'
- en: '| Open-source fine-tuned models |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 开源微调模型 |  |'
- en: '| QWen-chat Bai et al. ([2023](#bib.bib8)) | 14B | 43.9% | 46.4% | 45.15% |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| QWen-chat Bai et al. ([2023](#bib.bib8)) | 14B | 43.9% | 46.4% | 45.15% |'
- en: '| PHI-1 Gunasekar et al. ([2023](#bib.bib22)) | 1.3B | 50.6% | 55.5% | 53.05%
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| PHI-1 Gunasekar et al. ([2023](#bib.bib22)) | 1.3B | 50.6% | 55.5% | 53.05%
    |'
- en: '| OctoCoder Muennighoff et al. ([2023](#bib.bib42)) | 15B | 46.2% | NA | NA
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| OctoCoder Muennighoff et al. ([2023](#bib.bib42)) | 15B | 46.2% | NA | NA
    |'
- en: '| WizardCoder Luo et al. ([2023](#bib.bib40)) | 15B | 57.3% | 51.8% | 54.55%
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder Luo et al. ([2023](#bib.bib40)) | 15B | 57.3% | 51.8% | 54.55%
    |'
- en: '| Phind-CodeLlama-v2 Phind ([2023](#bib.bib45)) | 34B | 71.95% | NA | NA |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Phind-CodeLlama-v2 Phind ([2023](#bib.bib45)) | 34B | 71.95% | NA | NA |'
- en: '| WizardCoder-Python Luo et al. ([2023](#bib.bib40)) | 34B | 73.2% | 61.2%
    | 67.2% |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder-Python Luo et al. ([2023](#bib.bib40)) | 34B | 73.2% | 61.2%
    | 67.2% |'
- en: '| Closed-source models |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 封闭源模型 |  |'
- en: '| PanGu-Coder2 Shen et al. ([2023](#bib.bib48)) | 15B | 61.2% | NA | NA |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| PanGu-Coder2 Shen et al. ([2023](#bib.bib48)) | 15B | 61.2% | NA | NA |'
- en: '| Unnatural CodeLlama Rozière et al. ([2023](#bib.bib47)) | 34B | 62.2% | 61.2%
    | 61.7% |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Unnatural CodeLlama Rozière et al. ([2023](#bib.bib47)) | 34B | 62.2% | 61.2%
    | 61.7% |'
- en: '| Claude2 Anthropic ([2023](#bib.bib5)) | NA | 71.2% | NA | NA |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Claude2 Anthropic ([2023](#bib.bib5)) | NA | 71.2% | NA | NA |'
- en: '| GPT-3.5 OpenAI ([2023](#bib.bib43)) | 175B | 48.1% | 52.2% | 50.15% |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 OpenAI ([2023](#bib.bib43)) | 175B | 48.1% | 52.2% | 50.15% |'
- en: '| GPT-4 (zero-shot) OpenAI ([2023](#bib.bib43)) | NA | 67.00% | NA | NA |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (零-shot) OpenAI ([2023](#bib.bib43)) | NA | 67.00% | NA | NA |'
- en: 'Table 10: pass@1 performance on Multi-lingual HumanEval-X (Zheng et al., [2023](#bib.bib59))
    after fine-tuning with MFTCoder across multiple mainstream open-source models.
    The metric values marked with an asterisk (*) were obtained from the models’ corresponding
    papers, technical reports, or open-source project homepages, while the remaining
    metric values were evaluated using a combination of greedy decoding and zero-shot
    testing strategy.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：在对多个主流开源模型进行MFTCoder微调后的Multi-lingual HumanEval-X（郑等，[2023](#bib.bib59)）上的pass@1性能。带有星号（*）的度量值来自于模型的相关论文、技术报告或开源项目主页，而其余度量值则通过贪婪解码和零-shot测试策略评估得到。
- en: '| Model | Size | Python | Java | C++ | JavaScript | Golang | Avgerage |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | Python | Java | C++ | JavaScript | Golang | 平均值 |'
- en: '| QWen-base | 14B | $32.3\%^{*}$ | 35.37% | 30.49% | 32.93% | 21.34% | 30.49%
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| QWen-base | 14B | $32.3\%^{*}$ | 35.37% | 30.49% | 32.93% | 21.34% | 30.49%
    |'
- en: '| CodeFuse-QWen-MFT | 14B | 48.78% | 41.46% | 38.41% | 46.34% | 26.83% | 40.36%
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-QWen-MFT | 14B | 48.78% | 41.46% | 38.41% | 46.34% | 26.83% | 40.36%
    |'
- en: '| Llama-base | 65B | $23.7\%^{*}$ | 29.26% | 20.73% | 23.78% | 18.9% | 23.27%
    |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Llama-base | 65B | $23.7\%^{*}$ | 29.26% | 20.73% | 23.78% | 18.9% | 23.27%
    |'
- en: '| CodeFuse-Llama-MFT | 65B | 34.76% | 37.2% | 29.88% | 32.93% | 23.78% | 31.71%
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-Llama-MFT | 65B | 34.76% | 37.2% | 29.88% | 32.93% | 23.78% | 31.71%
    |'
- en: '| Llama2-base | 70B | $29.9\%^{*}$ | 39.02% | 31.10% | 35.98% | 23.78% | 31.96%
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-base | 70B | $29.9\%^{*}$ | 39.02% | 31.10% | 35.98% | 23.78% | 31.96%
    |'
- en: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 35.98% | 32.32% | 38.41% | 27.44% |
    35.00% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-Llama2-MFT | 70B | 40.85% | 35.98% | 32.32% | 38.41% | 27.44% |
    35.00% |'
- en: '| StarCoder-base | 15B | $33.6\%^{*}$ | 34.15% | 25.61% | 22.56% | 22.56% |
    29.48% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| StarCoder-base | 15B | $33.6\%^{*}$ | 34.15% | 25.61% | 22.56% | 22.56% |
    29.48% |'
- en: '| CodeFuse-StarCoder-MFT | 15B | 54.9% | 47.56 | 46.34% | 48.17% | 37.20% |
    46.83% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-StarCoder-MFT | 15B | 54.9% | 47.56 | 46.34% | 48.17% | 37.20% |
    46.83% |'
- en: '| CodeGeex2-base | 6B | $35.9\%^{*}$ | $22.5\%^{*}$ | 30.14% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| CodeGeex2-base | 6B | $35.9\%^{*}$ | $22.5\%^{*}$ | 30.14% |'
- en: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 45.73% | 37.2% | 37.2% | 28.05% |
    38.66% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeGeex2-MFT | 6B | 45.12% | 45.73% | 37.2% | 37.2% | 28.05% |
    38.66% |'
- en: '| CodeLlama-Python-base | 13B | $43.3\%^{*}$ | 41.46% | 34.76% | 38.41% | 29.27%
    | 37.44% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Python-base | 13B | $43.3\%^{*}$ | 41.46% | 34.76% | 38.41% | 29.27%
    | 37.44% |'
- en: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 57.32% | 46.34% | 54.27% |
    45.12% | 52.68% |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeLlama-Python-MFT | 13B | 60.37% | 57.32% | 46.34% | 54.27% |
    45.12% | 52.68% |'
- en: '| CodeLlama-34B-Python-base | 34B | $53.7\%^{*}$ | 45.73% | 42.68% | 45.73%
    | 31.71% | 43.91% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-34B-Python-base | 34B | $53.7\%^{*}$ | 45.73% | 42.68% | 45.73%
    | 31.71% | 43.91% |'
- en: '| CodeFuse-CodeLLama-Python-MFT | 34B | 74.4% | 61.6% | 54.3% | 61.0% | 50.6%
    | 60.38% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| CodeFuse-CodeLLama-Python-MFT | 34B | 74.4% | 61.6% | 54.3% | 61.0% | 50.6%
    | 60.38% |'
- en: '![Refer to caption](img/0681650bc66eb8a200ef95534b4da45c.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0681650bc66eb8a200ef95534b4da45c.png)'
- en: 'Figure 5: Radar Chart of CodeFuse-CodeLlama-34B Model on HumanEval, HumanEval-X,
    MBPP, DS-1000, and codefuseEval benchmarks compared to GPT-3.5 and GPT-4.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：CodeFuse-CodeLlama-34B模型在HumanEval、HumanEval-X、MBPP、DS-1000和codefuseEval基准测试上的雷达图，与GPT-3.5和GPT-4进行比较。
- en: '![Refer to caption](img/634af086fd97c1ba7de354f05fd1e1b4.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/634af086fd97c1ba7de354f05fd1e1b4.png)'
- en: 'Figure 6: Performance comparison of CodeFuse-QWen-14B fine-tuned with MFTCoder
    and code-related data, QWen-14B base model, and officially fine-tuned model QWen-14B-chat
    on NLP evaluation datasets. Detailed data can be found in Appendix [D](#A4 "Appendix
    D Evaluation results of CodeFuse-QWen-14B on NLP benchmark datasets ‣ MFTCoder:
    Boosting Code LLMs with Multitask Fine-Tuning").'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '图6：CodeFuse-QWen-14B在MFTCoder和与代码相关的数据微调后的性能与QWen-14B基础模型和官方微调模型QWen-14B-chat在NLP评估数据集上的比较。详细数据见附录[D](#A4
    "附录 D CodeFuse-QWen-14B在NLP基准数据集上的评估结果 ‣ MFTCoder: 通过多任务微调提升代码LLMs")。'
- en: 6 Discussion
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Despite the superior performance of the MFT training method compared to the
    task data mixing-based SFT training method in the aforementioned experiments,
    it should be noted that the effectiveness of the MFT approach is highly dependent
    on the task-splitting strategy. Not all scenarios are suitable for being split
    into multiple tasks. For instance, in our practical experience, we found that
    splitting a task based on difficulty levels and training it using the MFT method
    did not yield better results compared to the task-mixed SFT training method. Moreover,
    training code completion tasks as multiple tasks based on programming languages
    also did not outperform the mixed SFT method. Based on our practical experience,
    we have concluded that tasks with distinct main desired abilities are more suitable
    for task splitting and MFT training, whereas tasks with similar main training
    objectives are not well-suited for MFT training. We plan to further investigate
    and establish more precise criteria for task delineation in future research.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在上述实验中，MFT训练方法的表现优于基于任务数据混合的SFT训练方法，但应注意，MFT方法的有效性在很大程度上依赖于任务拆分策略。并非所有场景都适合拆分为多个任务。例如，在我们的实际经验中，我们发现基于难度级别拆分任务并使用MFT方法进行训练，并未比任务混合的SFT训练方法取得更好的结果。此外，基于编程语言将代码完成任务拆分为多个任务也未能超越混合SFT方法。根据我们的实际经验，我们得出结论，具有明显主要期望能力的任务更适合任务拆分和MFT训练，而主要训练目标相似的任务则不适合MFT训练。我们计划在未来的研究中进一步探讨并建立更精确的任务划分标准。
- en: In our task generalization experiments, we observed that models trained using
    the MFT method produced inference results that were more similar to the reference
    answers and had more concise content. Conversely, inference results generated
    by the task-mixed SFT training method contained more Chain-of-Thought (CoT) information.
    In certain scenarios, the former approach was more preferred, such as in IDE plugins,
    while the latter approach was favored in other scenarios, such as web assistants.
    As a result, we cannot simply generalize that one method is better than the other.
    We are currently researching the reasons behind these performance differences.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的任务泛化实验中，我们观察到使用MFT方法训练的模型生成的推理结果更接近参考答案且内容更为简洁。相反，由任务混合的SFT训练方法生成的推理结果包含更多的思维链（CoT）信息。在某些场景下，前者方法更受青睐，例如在IDE插件中，而后者方法则在其他场景中更受欢迎，如网页助手。因此，我们不能简单地概括某一种方法优于另一种。我们目前正在研究这些性能差异的原因。
- en: 'As a multi-task learning method, MFT also faces a major challenge during the
    training process: inconsistent convergence speeds among different tasks. For example,
    in the aforementioned experiments, the code completion task converged much faster
    than the unit test-case generation task (details can be found in Appendix [B](#A2
    "Appendix B Convergence Speed of MFT Training Process ‣ MFTCoder: Boosting Code
    LLMs with Multitask Fine-Tuning")). This makes it difficult to find an optimal
    point that performs well on all tasks. The selected checkpoint either converges
    insufficiently on some tasks or overfits on others. To address this issue, we
    experimented with existing multi-task learning balancing optimization solutions
    such as FAMO Liu et al. ([2023](#bib.bib35)). However, FAMO requires dual back-propagation
    in each iteration, resulting in training time being approximately doubled. Furthermore,
    the required number of epochs for convergence also increases significantly, and
    the adjustability of the convergence speed is limited. Unfortunately, this exponentially
    increased cost does not yield equivalent benefits. In response, we are currently
    developing a more optimal and adaptive multi-task optimization balancing approach.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '作为一种多任务学习方法，MFT在训练过程中也面临一个主要挑战：不同任务间收敛速度不一致。例如，在上述实验中，代码完成任务的收敛速度远快于单元测试用例生成任务（详细信息见附录 [B](#A2
    "附录 B MFT训练过程的收敛速度 ‣ MFTCoder: 通过多任务微调提升代码LLMs")）。这使得很难找到一个在所有任务上表现良好的最佳点。所选择的检查点要么在某些任务上收敛不足，要么在其他任务上过拟合。为了解决这个问题，我们尝试了现有的多任务学习平衡优化解决方案，如FAMO Liu等人（[2023](#bib.bib35)）。然而，FAMO在每次迭代中需要双重反向传播，导致训练时间大约翻倍。此外，收敛所需的轮次也显著增加，并且收敛速度的可调性有限。不幸的是，这种指数级增加的成本并没有带来相应的好处。对此，我们目前正在开发一种更优且适应性更强的多任务优化平衡方法。'
- en: Furthermore, even after balancing the convergence speeds of multiple tasks,
    where the same set of parameters is updated, it is still challenging to fundamentally
    eliminate the inherent conflicts in weight updates across different tasks. To
    address this issue, we are currently exploring the utilization of MoE (Mixture
    of Experts) Chen et al. ([2022](#bib.bib14)) to achieve MFT.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使在平衡多个任务的收敛速度之后，更新相同参数集时，仍然很难从根本上消除不同任务之间权重更新的固有冲突。为了解决这个问题，我们目前正在探索利用MoE（Mixture
    of Experts）Chen等人（[2022](#bib.bib14)）来实现MFT。
- en: 7 Conclusion
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This paper introduces MFTCoder, a framework that supports multi-task fine-tuning,
    effectively addressing the challenges of data imbalance, varying difficulty levels,
    and inconsistent convergence speeds through the design of various loss functions.
    Experimental results demonstrate that this approach outperforms individual fine-tuning
    on each task or fine-tuning on a mixed ensemble of tasks. Additionally, MFTCoder
    facilitates efficient training, including efficient data utilization and PEFT
    training. It also provides a high-quality instruction dataset construction solution.
    Leveraging MFTCoder for fine-tuning on the CodeLLama base, the CodeFuse-CodeLLama-34B
    model achieves an impressive pass@1 score of 74.4% on the HumanEval dataset, surpassing
    the performance of GPT-4 (67%, zero-shot).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了MFTCoder，一个支持多任务微调的框架，通过设计各种损失函数，有效解决了数据不平衡、难度水平不同和收敛速度不一致的挑战。实验结果表明，这种方法优于对每个任务单独微调或在任务混合集上进行微调。此外，MFTCoder促进了高效训练，包括高效的数据利用和PEFT训练。它还提供了高质量的指令数据集构建解决方案。利用MFTCoder在CodeLLama基础上进行微调，CodeFuse-CodeLLama-34B模型在HumanEval数据集上的pass@1得分为74.4%，超越了GPT-4（67%，零-shot）的表现。
- en: References
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Aghajanyan et al. (2021) Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
    Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive Multi-task
    Representations with Pre-Finetuning. arXiv:cs.CL/2101.11038'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aghajanyan等人（2021年）Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun
    Chen, Luke Zettlemoyer 和 Sonal Gupta. 2021年。Muppet: 大规模多任务表示与预微调。arXiv:cs.CL/2101.11038'
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi,
    Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel
    Romero, Michael Lappert, Francesco De Toni, Bernardo GarcÃa del RÃo, Qian Liu,
    Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco
    Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa,
    Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha,
    Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don’t reach for the stars!
    arXiv:cs.SE/2301.03988'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal等人（2023年）Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher
    Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan
    Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier,
    Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert,
    Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya,
    Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David
    Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine
    Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries 和 Leandro von Werra.
    2023年。SantaCoder: 不要过于追求星辰！arXiv:cs.SE/2301.03988'
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, ClÃ©ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark DÃaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2
    Technical Report. arXiv:cs.CL/2305.10403
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2023）Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, 和 Yonghui Wu。2023。《PaLM 2 技术报告》。arXiv:cs.CL/2305.10403
- en: Anthropic (2023) Anthropic. 2023. *Model Card and Evaluations for Claude Models*.
    [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2023）Anthropic。2023年。*Claude 模型的模型卡和评估*。[https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf)
- en: 'Aribandi et al. (2022) Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven
    Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo
    Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. ExT5: Towards
    Extreme Multi-Task Scaling for Transfer Learning. arXiv:cs.CL/2111.10952'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aribandi 等人（2022）Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu
    Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri,
    Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, 和 Donald Metzler。2022。《ExT5：面向极限多任务扩展的迁移学习》。arXiv:cs.CL/2111.10952
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program Synthesis with Large Language Models. *arXiv preprint
    arXiv:2108.07732* (2021).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等人（2021）Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
    Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le 等人。2021。《大型语言模型的程序合成》。*arXiv
    预印本 arXiv:2108.07732*（2021年）。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. *arXiv preprint arXiv:2309.16609*
    (2023).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等（2023）白进泽、白帅、朱云飞、崔泽宇、邓凯、邓晓东、范杨、葛文斌、韩宇、黄飞、惠斌源、罗吉、李梅、林俊扬、林润基、刘大义恒、刘高、卢成强、卢克铭、马建鑫、门睿、任兴章、任轩诚、谭传奇、谭思南、涂建宏、王鹏、王世杰、王伟、吴胜光、徐本锋、徐锦、杨安、杨浩、杨健、杨树生、姚杨、余博文、袁红毅、袁征、张建伟、张兴轩、张一昌、张珍如、周常、周京人、周小欢、朱天航。2023年。《Qwen技术报告》。*arXiv预印本
    arXiv:2309.16609*（2023年）。
- en: 'Baichuan (2023) Baichuan. 2023. Baichuan 2: Open Large-scale Language Models.
    *arXiv preprint arXiv:2309.10305* (2023). [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 百川（2023）百川。2023年。《百川2：开放的大规模语言模型》。*arXiv预印本 arXiv:2309.10305*（2023年）。[https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305)
- en: 'Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony,
    Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang,
    Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
    Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive
    Language Model. arXiv:cs.CL/2204.06745'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布莱克等（2022）西德·布莱克、斯特拉·比德曼、埃里克·哈拉汉、昆廷·安东尼、李奥·高、劳伦斯·戈尔丁、霍勒斯·赫、康纳·利希、凯尔·麦克唐奈、贾森·芳、迈克尔·皮勒、USVSN·赛·普拉尚、希瓦恩修·普罗希特、拉里亚·雷诺兹、乔纳森·陶、本·王、塞缪尔·温巴赫。2022年。《GPT-NeoX-20B：一个开源自回归语言模型》。arXiv:cs.CL/2204.06745
- en: Caruana (1997) Rich Caruana. 1997. Multitask learning. *Machine learning* 28
    (1997), 41–75.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡鲁安娜（1997）瑞奇·卡鲁安娜。1997年。《多任务学习》。*机器学习* 28（1997），41–75。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. (2021).
    arXiv:cs.LG/2107.03374
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）马克·陈、杰瑞·特沃雷克、朱赫武、袁启明、亨里克·庞德·德·奥利维拉·平托、贾雷德·卡普兰、哈里·爱德华兹、尤里·布尔达、尼古拉斯·约瑟夫、格雷格·布罗克曼、亚历克斯·雷、劳尔·普里、格雷琴·克鲁格、迈克尔·彼得罗夫、海蒂·克拉夫、吉里什·萨斯特里、帕梅拉·米什金、布鲁克·陈、斯科特·格雷、尼克·赖德、米哈伊尔·帕夫洛夫、艾莉西亚·鲍尔、卢卡斯·凯泽、穆罕默德·巴瓦里安、克莱门斯·温特、菲利普·蒂莱、费利佩·彼得罗斯基·苏奇、戴夫·卡明斯、马蒂亚斯·普拉普特、福提奥斯·香芝斯、伊丽莎白·巴恩斯、阿里尔·赫伯特-沃斯、威廉·赫布根·古斯、亚历克斯·尼科尔、亚历克斯·派诺、尼科拉斯·特扎克、姜唐、伊戈尔·巴布什金、苏奇尔·巴拉吉、尚塔努·贾因、威廉·桑德斯、克里斯托弗·赫斯、安德鲁·N·卡尔、简·莱克、乔什·阿基亚姆、维丹特·米斯拉、埃文·莫里卡瓦、亚历克·拉德福德、马修·奈特、迈尔斯·布伦代奇、米拉·穆拉提、凯蒂·梅耶、彼得·维林德、鲍勃·麦克格鲁、达里奥·阿莫代、萨姆·麦坎德利什、伊利亚·苏茨克弗、沃伊切赫·扎伦巴。2021年。《评估基于代码训练的大型语言模型》。arXiv:cs.LG/2107.03374
- en: 'Chen et al. (2018) Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew
    Rabinovich. 2018. Gradnorm: Gradient normalization for adaptive loss balancing
    in deep multitask networks. In *International conference on machine learning*.
    PMLR, 794–803.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018）陈赵、维贾伊·巴德里纳拉扬、李陈瑜、安德鲁·拉比诺维奇。2018年。《Gradnorm：用于深度多任务网络的梯度归一化》。在*国际机器学习会议*。PMLR，794–803。
- en: Chen et al. (2022) Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi
    Li. 2022. Towards Understanding Mixture of Experts in Deep Learning. arXiv:cs.LG/2208.02813
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022）陈子祥、邓义赫、吴跃、顾全权、李元智。2022年。《深度学习中的专家混合理解》。arXiv:cs.LG/2208.02813
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language
    Modeling with Pathways. arXiv:cs.CL/2204.02311'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2022）Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann、Parker
    Schuh、Kensen Shi、Sasha Tsvyashchenko、Joshua Maynez、Abhishek Rao、Parker Barnes、Yi
    Tay、Noam Shazeer、Vinodkumar Prabhakaran、Emily Reif、Nan Du、Ben Hutchinson、Reiner
    Pope、James Bradbury、Jacob Austin、Michael Isard、Guy Gur-Ari、Pengcheng Yin、Toju
    Duke、Anselm Levskaya、Sanjay Ghemawat、Sunipa Dev、Henryk Michalewski、Xavier Garcia、Vedant
    Misra、Kevin Robinson、Liam Fedus、Denny Zhou、Daphne Ippolito、David Luan、Hyeontaek
    Lim、Barret Zoph、Alexander Spiridonov、Ryan Sepassi、David Dohan、Shivani Agrawal、Mark
    Omernick、Andrew M. Dai、Thanumalayan Sankaranarayana Pillai、Marie Pellat、Aitor
    Lewkowycz、Erica Moreira、Rewon Child、Oleksandr Polozov、Katherine Lee、Zongwei Zhou、Xuezhi
    Wang、Brennan Saeta、Mark Diaz、Orhan Firat、Michele Catasta、Jason Wei、Kathy Meier-Hellstern、Douglas
    Eck、Jeff Dean、Slav Petrov 和 Noah Fiedel。2022年。PaLM: 通过路径扩展语言建模。arXiv:cs.CL/2204.02311'
- en: 'Christopoulou et al. (2022) Fenia Christopoulou, Gerasimos Lampouras, Milan
    Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin
    Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma, Ignacio Iacobacci, Yasheng
    Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022.
    PanGu-Coder: Program Synthesis with Function-Level Language Modeling. arXiv:cs.LG/2207.11280'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Christopoulou 等（2022）Fenia Christopoulou、Gerasimos Lampouras、Milan Gritta、Guchun
    Zhang、Yinpeng Guo、Zhongqi Li、Qi Zhang、Meng Xiao、Bo Shen、Lin Li、Hao Yu、Li Yan、Pingyi
    Zhou、Xin Wang、Yuchi Ma、Ignacio Iacobacci、Yasheng Wang、Guangtai Liang、Jiansheng
    Wei、Xin Jiang、Qianxiang Wang 和 Qun Liu。2022年。PanGu-Coder: 基于函数级语言建模的程序合成。arXiv:cs.LG/2207.11280'
- en: 'Crawshaw (2020) Michael Crawshaw. 2020. Multi-task learning with deep neural
    networks: A survey. *arXiv preprint arXiv:2009.09796* (2020).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crawshaw（2020）Michael Crawshaw。2020年。深度神经网络的多任务学习：综述。*arXiv 预印本 arXiv:2009.09796*（2020）。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:cs.LG/2305.14314'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。2023年。QLoRA:
    高效微调量化LLM。arXiv:cs.LG/2305.14314'
- en: 'Di et al. (2023) Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang
    Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi
    Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao,
    Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan
    Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin
    Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. 2023. CodeFuse-13B:
    A Pretrained Multi-lingual Code Large Language Model. arXiv:cs.SE/2310.06266'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Di 等（2023）Peng Di、Jianguo Li、Hang Yu、Wei Jiang、Wenting Cai、Yang Cao、Chaoyu
    Chen、Dajun Chen、Hongwei Chen、Liang Chen、Gang Fan、Jie Gong、Zi Gong、Wen Hu、Tingting
    Guo、Zhichao Lei、Ting Li、Zheng Li、Ming Liang、Cong Liao、Bingchang Liu、Jiachen Liu、Zhiwei
    Liu、Shaojun Lu、Min Shen、Guangpei Wang、Huan Wang、Zhi Wang、Zhaogui Xu、Jiawei Yang、Qing
    Ye、Gehao Zhang、Yu Zhang、Zelin Zhao、Xunjin Zheng、Hailian Zhou、Lifu Zhu 和 Xianying
    Zhu。2023年。CodeFuse-13B: 一种预训练的多语言代码大型语言模型。arXiv:cs.SE/2310.06266'
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with
    Autoregressive Blank Infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*. 320–335.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等（2022）Zhengxiao Du、Yujie Qian、Xiao Liu、Ming Ding、Jiezhong Qiu、Zhilin Yang
    和 Jie Tang。2022年。GLM: 通过自回归空白填充的通用语言模型预训练。发表于 *第60届计算语言学协会年会论文集（第1卷：长文）*。320–335。'
- en: 'Duong et al. (2015) Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015.
    Low resource dependency parsing: Cross-lingual parameter sharing in a neural network
    parser. In *Proceedings of the 53rd annual meeting of the Association for Computational
    Linguistics and the 7th international joint conference on natural language processing
    (volume 2: short papers)*. 845–850.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duong et al. (2015) Long Duong, Trevor Cohn, Steven Bird, 和 Paul Cook. 2015.
    低资源依赖分析：神经网络解析器中的跨语言参数共享。在 *第53届计算语言学协会年会和第7届国际自然语言处理联合会议（第2卷：短论文集）*。845–850。
- en: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. *arXiv preprint
    arXiv:2306.11644* (2023).
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César
    Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann,
    Gustavo de Rosa, Olli Saarikivi, 等等。2023. 教科书就是你所需的全部。*arXiv 预印本 arXiv:2306.11644*
    (2023)。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. arXiv:cs.LG/1902.00751
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain Gelly.
    2019. 参数高效的 NLP 迁移学习。arXiv:cs.LG/1902.00751
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation
    of Large Language Models. arXiv:cs.CL/2106.09685'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2021. LoRA: 大型语言模型的低秩适应。arXiv:cs.CL/2106.09685'
- en: Jean et al. (2019) Sébastien Jean, Orhan Firat, and Melvin Johnson. 2019. Adaptive
    scheduling for multi-task learning. *arXiv preprint arXiv:1909.06434* (2019).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jean et al. (2019) Sébastien Jean, Orhan Firat, 和 Melvin Johnson. 2019. 多任务学习的自适应调度。*arXiv
    预印本 arXiv:1909.06434* (2019)。
- en: Kendall et al. (2018) Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018. Multi-task
    learning using uncertainty to weigh losses for scene geometry and semantics. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    7482–7491.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall et al. (2018) Alex Kendall, Yarin Gal, 和 Roberto Cipolla. 2018. 使用不确定性来权衡场景几何和语义损失的多任务学习。在
    *IEEE 计算机视觉与模式识别会议论文集*。7482–7491。
- en: 'Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method
    for Stochastic Optimization. arXiv:cs.LG/1412.6980'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma and Ba (2017) Diederik P. Kingma 和 Jimmy Ba. 2017. Adam: 一种随机优化方法。arXiv:cs.LG/1412.6980'
- en: 'Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
    2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
    *ArXiv* abs/2211.11501 (2022).'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, 和 Tao Yu.
    2022. DS-1000: 一个自然且可靠的数据科学代码生成基准。*ArXiv* abs/2211.11501 (2022)。'
- en: Lee et al. (2018) Hae Beom Lee, Eunho Yang, and Sung Ju Hwang. 2018. Deep asymmetric
    multi-task feature learning. In *International Conference on Machine Learning*.
    PMLR, 2956–2964.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2018) Hae Beom Lee, Eunho Yang, 和 Sung Ju Hwang. 2018. 深度非对称多任务特征学习。在
    *国际机器学习大会*。PMLR，2956–2964。
- en: 'Li et al. (2023c) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023c. CAMEL: Communicative Agents for "Mind"
    Exploration of Large Scale Language Model Society. arXiv:cs.AI/2303.17760'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023c) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, 和 Bernard Ghanem. 2023c. CAMEL: 用于“大规模语言模型社会”探索的交流代理。arXiv:cs.AI/2303.17760'
- en: Li et al. (2023d) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao
    Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.
    2023d. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale
    Database Grounded Text-to-SQLs. arXiv:cs.CL/2305.03111
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023d) Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen
    Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao
    Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, 和 Yongbin Li. 2023d.
    大型语言模型是否已经能够充当数据库接口？一个用于大规模数据库基础文本到 SQL 的大基准。arXiv:cs.CL/2305.03111
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    et al. 2023a. StarCoder: may the source be with you! *arXiv preprint arXiv:2305.06161*
    (2023).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    等等。2023a. StarCoder: 愿源代码与你同在！*arXiv 预印本 arXiv:2305.06161* (2023)。'
- en: 'Li et al. (2023b) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks Are All You Need II: phi-1.5
    technical report. *arXiv preprint arXiv:2309.05463* (2023).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023b) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, 和 Yin Tat Lee. 2023b. Textbooks Are All You Need II: phi-1.5
    technical report. *arXiv 预印本 arXiv:2309.05463* (2023)。'
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal
    Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de
    Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation
    with AlphaCode. *Science* 378, 6624 (dec 2022), 1092–1097. [https://doi.org/10.1126/science.abq1158](https://doi.org/10.1126/science.abq1158)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de
    Freitas, Koray Kavukcuoglu, 和 Oriol Vinyals. 2022. Competition-level code generation
    with AlphaCode. *Science* 378, 6624 (2022年12月)，1092–1097。 [https://doi.org/10.1126/science.abq1158](https://doi.org/10.1126/science.abq1158)
- en: 'Liu et al. (2023) Bo Liu, Yihao Feng, Peter Stone, and Qiang Liu. 2023. FAMO:
    Fast Adaptive Multitask Optimization. arXiv:cs.LG/2306.03792'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Bo Liu, Yihao Feng, Peter Stone, 和 Qiang Liu. 2023. FAMO:
    Fast Adaptive Multitask Optimization. arXiv:cs.LG/2306.03792'
- en: Liu et al. (2019b) Shikun Liu, Edward Johns, and Andrew J Davison. 2019b. End-to-end
    multi-task learning with attention. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 1871–1880.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019b) Shikun Liu, Edward Johns, 和 Andrew J Davison. 2019b. End-to-end
    multi-task learning with attention. 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，1871–1880。
- en: Liu et al. (2019c) Shengchao Liu, Yingyu Liang, and Anthony Gitter. 2019c. Loss-balanced
    task weighting to reduce negative transfer in multi-task learning. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 33\. 9977–9978.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019c) Shengchao Liu, Yingyu Liang, 和 Anthony Gitter. 2019c. Loss-balanced
    task weighting to reduce negative transfer in multi-task learning. 在 *AAAI 人工智能会议论文集*，第
    33 卷，9977–9978。
- en: Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019a. Multi-Task Deep Neural Networks for Natural Language Understanding. In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.
    4487–4496.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, 和 Jianfeng Gao.
    2019a. Multi-Task Deep Neural Networks for Natural Language Understanding. 在 *第
    57 届计算语言学协会年会论文集*，4487–4496。
- en: Long et al. (2017) Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S
    Yu. 2017. Learning multiple tasks with multilinear relationship networks. *Advances
    in neural information processing systems* 30 (2017).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2017) Mingsheng Long, Zhangjie Cao, Jianmin Wang, 和 Philip S Yu.
    2017. Learning multiple tasks with multilinear relationship networks. *神经信息处理系统进展*
    30 (2017)。
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering
    Code Large Language Models with Evol-Instruct. *arXiv preprint arXiv:2306.08568*
    (2023).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. 2023. WizardCoder: Empowering
    Code Large Language Models with Evol-Instruct. *arXiv 预印本 arXiv:2306.08568* (2023)。'
- en: Min et al. (2019) Qingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A Pilot Study
    for Chinese SQL Semantic Parsing. arXiv:cs.CL/1909.13293
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2019) Qingkai Min, Yuefeng Shi, 和 Yue Zhang. 2019. A Pilot Study
    for Chinese SQL Semantic Parsing. arXiv:cs.CL/1909.13293
- en: 'Muennighoff et al. (2023) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models.
    arXiv:cs.CL/2308.07124'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Muennighoff et al. (2023) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    和 Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models.
    arXiv:cs.CL/2308.07124'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:cs.CL/2303.08774
- en: Pascal et al. (2021) Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet,
    and Maria A Zuluaga. 2021. Maximum roaming multi-task learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 35\. 9331–9341.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascal et al. (2021) Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet,
    和 Maria A Zuluaga. 2021. Maximum roaming multi-task learning. 在 *AAAI 人工智能会议论文集*，第
    35 卷，9331–9341。
- en: Phind (2023) Phind. 2023. *Phind-CodeLlama-34B-v2*. [https://huggingface.co/Phind/Phind-CodeLlama-34B-v2](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phind (2023) Phind。2023。*Phind-CodeLlama-34B-v2*。[https://huggingface.co/Phind/Phind-CodeLlama-34B-v2](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)
- en: Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:cs.LG/1910.10683
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2023) 科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、颜琪·周、韦·李和彼得·J·刘。2023。Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer。arXiv:cs.LG/1910.10683
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière et al. (2023) 巴普蒂斯特·罗齐埃、乔纳斯·盖林、法比安·格洛克、斯滕·索特拉、伊泰·加特、肖青·艾伦·谭、约西·阿迪、景宇·刘、塔尔·雷梅兹、杰雷米·拉潘等。2023。Code
    llama: Open foundation models for code。*arXiv 预印本 arXiv:2308.12950*（2023）。'
- en: 'Shen et al. (2023) Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing
    Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, et al. 2023. Pangu-coder2:
    Boosting large language models for code with ranking feedback. *arXiv preprint
    arXiv:2307.14936* (2023).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2023) 沈博、张佳欣、陈泰鸿、詹道广、耿冰、傅安、曾沐涵、余爱伦、纪家川、赵景阳等。2023。Pangu-coder2:
    Boosting large language models for code with ranking feedback。*arXiv 预印本 arXiv:2307.14936*（2023）。'
- en: 'Sun et al. (2020) Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko.
    2020. Adashare: Learning what to share for efficient deep multi-task learning.
    *Advances in Neural Information Processing Systems* 33 (2020), 8728–8740.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2020) 孙希萌、拉梅斯瓦尔·潘达、罗杰里奥·费里斯和凯特·桑科。2020。Adashare: Learning what
    to share for efficient deep multi-task learning。*神经信息处理系统进展* 33（2020），8728–8740。'
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239* (2022).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan et al. (2022) 罗马尔·托皮兰、丹尼尔·德·弗雷塔斯、杰米·霍尔、诺姆·沙泽尔、阿普尔夫·库尔什雷斯塔、亨-泽·程、艾莉西亚·金、泰勒·博斯、莱斯利·贝克、余度等。2022。Lamda:
    Language models for dialog applications。*arXiv 预印本 arXiv:2201.08239*（2022）。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) 休戈·图夫龙、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、克萨维尔·马尔蒂内、玛丽-安娜·拉肖、蒂莫泰·拉克鲁瓦、巴普蒂斯特·罗齐埃、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿兹哈尔等。2023a。Llama:
    Open and efficient foundation language models。*arXiv 预印本 arXiv:2302.13971*（2023）。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) 休戈·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马海里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏姆亚·巴特拉、普拉吉瓦尔·巴尔加瓦、舒尔提·博萨尔等。2023b。Llama
    2: Open foundation and fine-tuned chat models。*arXiv 预印本 arXiv:2307.09288*（2023）。'
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning
    language model with self generated instructions. *arXiv preprint arXiv:2212.10560*
    (2022).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) 王亦中、耶根·科尔迪、斯瓦鲁普·米什拉、阿丽莎·刘、诺亚·A·史密斯、丹尼尔·哈沙比和哈娜赫·哈吉希尔齐。2022。Self-instruct:
    Aligning language model with self generated instructions。*arXiv 预印本 arXiv:2212.10560*（2022）。'
- en: 'Wang et al. (2023) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui,
    Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code Large Language Models
    for Code Understanding and Generation. arXiv:cs.CL/2305.07922'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) 王岳、洪乐、阿基莱什·迪帕克·戈特马尔、阮 D. Q. 布伊、李军南和史蒂文·C·H·霍伊。2023。CodeT5+:
    Open Code Large Language Models for Code Understanding and Generation。arXiv:cs.CL/2305.07922'
- en: Yang and Hospedales (2017) Yongxin Yang and Timothy Hospedales. 2017. Trace
    Norm Regularised Deep Multi-Task Learning. In *5th International Conference on
    Learning Representations*.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang and Hospedales (2017) 杨永欣和提莫西·霍斯佩达莱斯。2017。Trace Norm Regularised Deep Multi-Task
    Learning。载于*第五届国际学习表征会议*。
- en: 'Yu et al. (2019a) Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro
    Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen
    Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki,
    and Dragomir Radev. 2019a. CoSQL: A Conversational Text-to-SQL Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases. arXiv:cs.CL/1909.05378'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2019a) Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro
    Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen
    Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S Lasecki,
    and Dragomir Radev. 2019a. CoSQL: A Conversational Text-to-SQL Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases. arXiv:cs.CL/1909.05378'
- en: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
    Radev. 2019b. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain
    Semantic Parsing and Text-to-SQL Task. arXiv:cs.CL/1809.08887'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir
    Radev. 2019b. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain
    Semantic Parsing and Text-to-SQL Task. arXiv:cs.CL/1809.08887'
- en: Zhao et al. (2018) Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang,
    and Ying Wu. 2018. A modulation module for multi-task learning with applications
    in image retrieval. In *Proceedings of the European Conference on Computer Vision
    (ECCV)*. 401–416.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2018) Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang,
    and Ying Wu. 2018. A modulation module for multi-task learning with applications
    in image retrieval. In *欧洲计算机视觉会议论文集 (ECCV)*. 401–416.
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and
    Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
    Evaluations on HumanEval-X. In *KDD*.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and
    Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
    Evaluations on HumanEval-X. In *KDD*.'
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement
    Learning. *CoRR* abs/1709.00103 (2017).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement
    Learning. *CoRR* abs/1709.00103 (2017).'
- en: Appendix A Code Exercises Generation with Camel
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A Camel代码练习生成
- en: 'Table 11: Examples of Prompts for Code Exercises Generation using Camel'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：使用Camel生成代码练习的提示示例
- en: '|'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Original task prompt: &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始任务提示：&#124;'
- en: '&#124; Create 50 coding exercises to test and improve students’ Python programming
    skills. The exercises must must must focus on Binary search. The difficulty of
    exercises must be from easy to medium to hard. &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 创建50个编码练习，以测试和提高学生的Python编程技能。这些练习必须专注于二分搜索。练习的难度必须从简单到中等再到困难。 &#124;'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Specified task prompt: &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指定任务提示：&#124;'
- en: '&#124; A student will collaborate with a programming teacher to develop 50
    Python coding exercises centered specifically on Binary search algorithms. The
    exercises should progressively increase in difficulty, from easy to medium to
    hard, targeting various aspects of implementation and optimization techniques.
    &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学生将与编程教师合作开发50个以二分搜索算法为中心的Python编码练习。这些练习应逐步增加难度，从简单到中等再到困难，涵盖实现和优化技术的各个方面。
    &#124;'
- en: '|'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Final task prompt: &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最终任务提示：&#124;'
- en: '&#124; A student will collaborate with a programming teacher to develop 50
    Python coding exercises centered specifically on Binary search algorithms. The
    exercises should progressively increase in difficulty, from easy to medium to
    hard, targeting various aspects of implementation and optimization techniques.
    &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学生将与编程教师合作开发50个以二分搜索算法为中心的Python编码练习。这些练习应逐步增加难度，从简单到中等再到困难，涵盖实现和优化技术的各个方面。
    &#124;'
- en: '|'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AI Assistant sys message: &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AI助手系统消息：&#124;'
- en: '&#124; SystemMessage(role_name=’A student’, role_type = , meta_dict={’task’: ’A student will assist a programming teacher
    in designing a series of 50 Python coding exercises that emphasize the application
    and optimization of Bubble Sort algorithm. These exercises will span varying levels
    of difficulty, from easy to medium to hard, allowing students to strengthen their
    sorting skills progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A
    programming teacher’}, role=’system’, content=’Never forget you are a A student
    and I am a A programming teacher. Never flip roles! Never instruct me! &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SystemMessage(role_name=’A student’, role_type = , meta_dict={’task’: ’一名学生将协助编程教师设计一系列50个Python编码练习，这些练习强调了冒泡排序算法的应用和优化。这些练习将涵盖从简单到中等到困难的不同难度级别，让学生逐步强化他们的排序技能。’,
    ’assistant_role’: ’A student’, ’user_role’: ’A programming teacher’}, role=’system’,
    content=’永远不要忘记你是一个学生，而我是一名编程教师。不要交换角色！不要指导我！ &#124;'
- en: '&#124; We share a common interest in collaborating to successfully complete
    a task. &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们在共同的兴趣下合作以成功完成任务。 &#124;'
- en: '&#124; You must help me to complete the task. &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你必须帮助我完成任务。 &#124;'
- en: '&#124; Here is the task: A student will assist a programming teacher in designing
    a series of 50 Python coding exercises that emphasize the application and optimization
    of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
    from easy to medium to hard, allowing students to strengthen their sorting skills
    progressively.. Never forget our task! &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务如下：一名学生将协助编程教师设计一系列50个Python编码练习，这些练习强调了冒泡排序算法的应用和优化。这些练习将涵盖从简单到中等到困难的不同难度级别，让学生逐步强化他们的排序技能。绝不要忘记我们的任务！
    &#124;'
- en: '&#124; I must instruct you based on your expertise and my needs to complete
    the task. &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我必须根据你的专业知识和我的需求来指导你完成任务。 &#124;'
- en: '&#124; I must give you one instruction at a time. &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我必须一次给你一个指令。 &#124;'
- en: '&#124; You must write a specific solution that appropriately solves the requested
    instruction and explain your solutions. &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你必须写出一个具体的解决方案，适当地解决请求的指令，并解释你的解决方案。 &#124;'
- en: '&#124; You must decline my instruction honestly if you cannot perform the instruction
    due to physical, moral, legal reasons or your capability and explain the reasons.
    &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果你由于身体、道德、法律原因或你的能力无法执行指令，你必须诚实地拒绝我的指令，并解释原因。 &#124;'
- en: '&#124; Unless I say the task is completed, you should always start with: &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除非我说任务已完成，否则你应始终以： &#124;'
- en: '&#124; Solution:  &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解决方案： &#124;'
- en: '&#124;  should be very specific, include detailed explanations
    and provide preferable detailed implementations and examples and lists for task-solving.
    &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  应该非常具体，包括详细的解释，并提供详细的实现和示例以及任务解决的列表。 &#124;'
- en: '&#124; Always end  with: Next request.’) &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 始终以：下一步请求。’来结束 &#124;'
- en: '|'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AI User sys message: &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AI 用户系统消息： &#124;'
- en: '&#124; SystemMessage(role_name=’A programming teacher’, role_type = , meta_dict = {’task’: ’A student will assist a programming teacher in
    designing a series of 50 Python coding exercises that emphasize the application
    and optimization of Bubble Sort algorithm. These exercises will span varying levels
    of difficulty, from easy to medium to hard, allowing students to strengthen their
    sorting skills progressively.’, ’assistant_role’: ’A student’, ’user_role’: ’A
    programming teacher’}, role=’system’, content=’Never forget you are a A programming
    teacher and I am a A student. Never flip roles! You will always instruct me. &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SystemMessage(role_name=’A programming teacher’, role_type = , meta_dict = {’task’: ’一名学生将协助编程教师设计一系列50个Python编码练习，这些练习强调了冒泡排序算法的应用和优化。这些练习将涵盖从简单到中等到困难的不同难度级别，让学生逐步强化他们的排序技能。’,
    ’assistant_role’: ’A student’, ’user_role’: ’A programming teacher’}, role=’system’,
    content=’永远不要忘记你是一个编程教师，而我是一名学生。不要交换角色！你将始终指导我。 &#124;'
- en: '&#124; We share a common interest in collaborating to successfully complete
    a task. &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们在共同的兴趣下合作以成功完成任务。 &#124;'
- en: '&#124; I must help you to complete the task. &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我必须帮助你完成任务。 &#124;'
- en: '&#124; Here is the task: A student will assist a programming teacher in designing
    a series of 50 Python coding exercises that emphasize the application and optimization
    of Bubble Sort algorithm. These exercises will span varying levels of difficulty,
    from easy to medium to hard, allowing students to strengthen their sorting skills
    progressively.. Never forget our task! &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务如下：学生将协助编程老师设计一系列 50 个 Python 编码练习，重点是应用和优化冒泡排序算法。这些练习将涵盖从简单到中等再到困难的不同难度级别，使学生能够逐步提升排序技能。永远不要忘记我们的任务！
    &#124;'
- en: '&#124; You must instruct me based on my expertise and your needs to solve the
    task ONLY in the following two ways: &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你必须根据我的专业知识和你的需求，以以下两种方式来指示我仅解决任务： &#124;'
- en: '&#124; 1\. Instruct with a necessary input: &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. 用必要的输入进行指示: &#124;'
- en: '&#124; Instruction:  &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指示:  &#124;'
- en: '&#124; Input:  &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入:  &#124;'
- en: '&#124; 2\. Instruct without any input: &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. 不带任何输入进行指示: &#124;'
- en: '&#124; Instruction:  &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指示:  &#124;'
- en: '&#124; Input: None &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入: 无 &#124;'
- en: '&#124; The "Instruction" describes a task or question. The paired "Input" provides
    further context or information for the requested "Instruction". &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; “指示”描述一个任务或问题。配对的“输入”提供了更多上下文或信息。 &#124;'
- en: '&#124; You must give me one instruction at a time. &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你必须一次给我一个指示。 &#124;'
- en: '&#124; I must write a response that appropriately solves the requested instruction.
    &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我必须写出一个适当解决请求指示的回应。 &#124;'
- en: '&#124; I must decline your instruction honestly if I cannot perform the instruction
    due to physical, moral, legal reasons or my capability and explain the reasons.
    &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果由于身体、道德、法律原因或我的能力问题，我无法执行指示，我必须诚实地拒绝你的指示并解释原因。 &#124;'
- en: '&#124; You should instruct me not ask me questions. &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你应该给我指示而不是问我问题。 &#124;'
- en: '&#124; Now you must start to instruct me using the two ways described above.
    &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现在你必须开始使用上述两种方式来指示我。 &#124;'
- en: '&#124; Do not add anything else other than your instruction and the optional
    corresponding input! &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除了你的指示和可选的相应输入外，请不要添加其他内容！ &#124;'
- en: '&#124; Keep giving me instructions and necessary inputs until you think the
    task is completed. &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 持续给我指示和必要的输入，直到你认为任务完成。 &#124;'
- en: '&#124; When the task is completed, you must only reply with a single word .
    &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 当任务完成时，你必须只回复一个词 。 &#124;'
- en: '&#124; Never say  unless my responses have solved your task.’)
    &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除非我的回应已解决你的任务，否则永远不要说 。&#124;'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Appendix B Convergence Speed of MFT Training Process
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B MFT 训练过程的收敛速度
- en: '![Refer to caption](img/c7886b1c3507c2b0fb5aa11df31c35bb.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c7886b1c3507c2b0fb5aa11df31c35bb.png)'
- en: (a) Code Comment Task
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 代码注释任务
- en: '![Refer to caption](img/81b6cf2416231f62d039b89106b6edb6.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/81b6cf2416231f62d039b89106b6edb6.png)'
- en: (b) Code Completion Task
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 代码补全任务
- en: '![Refer to caption](img/2dcbb01369c8f0b252e2660ae617f418.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2dcbb01369c8f0b252e2660ae617f418.png)'
- en: (c) Code Translation Task
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 代码翻译任务
- en: '![Refer to caption](img/8eed76d3e6fb88cce0ad431e74fea032.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8eed76d3e6fb88cce0ad431e74fea032.png)'
- en: (d) Text-to-Code Task
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 文本到代码任务
- en: '![Refer to caption](img/15dc9c6bfc3ad38da9fa1f2de84f913c.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/15dc9c6bfc3ad38da9fa1f2de84f913c.png)'
- en: (e) Unit Test-case Generation Task
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 单元测试用例生成任务
- en: '![Refer to caption](img/828b915520980a1e0b676e65e52f57c7.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/828b915520980a1e0b676e65e52f57c7.png)'
- en: (f) Overall of 5 tasks
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 5 个任务的总体情况
- en: 'Figure 7: Validation Loss Convergence Speeds: A Comparative Analysis of 5 Code-related
    Downstream Tasks and Overall Training Progress using the data-balanced loss function.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 验证损失收敛速度：5个与代码相关的下游任务和使用数据平衡损失函数的整体训练进度的比较分析。'
- en: Appendix C Examples of codefuse-codellama-34b across various tasks
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 各种任务中的 codefuse-codellama-34b 示例
- en: C.1 Code Completion on HumanEval
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 HumanEval 上的代码补全
- en: 'Prompt:
    <|role_start|>human<|role_end|># language: Python from  typing  import  List
    def  below_zero(operations:  List[int])  ->  bool: """␣You’re␣given␣a␣list␣of␣deposit␣and␣withdrawal␣operations␣on␣a␣bank␣account␣that␣starts␣with
    ␣␣␣␣zero␣balance.␣Your␣task␣is␣to␣detect␣if␣at␣any␣point␣the␣balance␣of␣account␣fallls␣below␣zero,␣and
    ␣␣␣␣at␣that␣point␣function␣should␣return␣True.␣Otherwise␣it␣should␣return␣False.
    ␣␣␣␣>>>␣below_zero([1,␣2,␣3]) ␣␣␣␣False ␣␣␣␣>>>␣below_zero([1,␣2,␣-4,␣5]) ␣␣␣␣True
    ␣␣␣␣""" <|role_start|>bot<|role_end|>![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   balance  =  0    for  op  in  operations:    balance  +=  op    if  balance  <  0:    return  True    return  False'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：
    <|role_start|>human<|role_end|># language: Python from  typing  import  List
    def  below_zero(operations:  List[int])  ->  bool: """␣给定一个包含存款和取款操作的列表，该账户的起始余额为␣␣␣␣零。␣你的任务是检测账户的余额是否在任何时刻低于零，␣␣␣␣若低于零，函数应返回␣True。否则，应返回␣False。␣␣␣␣>>>␣below_zero([1,␣2,␣3])
    ␣␣␣␣False ␣␣␣␣>>>␣below_zero([1,␣2,␣-4,␣5]) ␣␣␣␣True ␣␣␣␣""" <|role_start|>bot<|role_end|>![[无标题图像]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   balance  =  0    for  op  in  operations:    balance  +=  op    if  balance  <  0:    return  True    return  False'
- en: C.2 Multilingual Code Completion on HumanEval-X
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 人类评估上的多语言代码补全
- en: 'Prompt
    (Java): <|role_start|>human<|role_end|>//
    language: Java import  java.util.*; import  java.lang.*; class  Solution  { /**
    Given  a  string  s,  count  the  number  of  uppercase  vowels  in  even  indices.
    For  example: countUpper("aBCdEf")  returns  1 countUpper("abcdefg")  returns  0
    countUpper("dBBE")  returns  0 */ public  int  countUpper(String  s)  { <|role_start|>bot<|role_end|>![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   int  count  =  0;    for  (int  i  =  0;  i  <  s.length();  i  +=  2)  {    char  c  =  s.charAt(i);    if  (Character.isUpperCase(c)  &&  (c  ==  ’A’  ||  c  ==  ’E’  ||  c  ==  ’I’  ||  c  ==  ’O’  ||  c  ==  ’U’))  {    count++;    }    }    return  count;    }  }
    Prompt
    (Golang): <|role_start|>human<|role_end|>//
    language: Go import  ( "strings" ) //  Create  a  function  Encrypt  that  takes  a  string  as  an  argument  and
    //  returns  a  string  Encrypted  with  the  alphabet  being  rotated. //  The  alphabet  should  be  rotated  in  a  manner  such  that  the  letters
    //  shift  down  by  two  multiplied  to  two  places. //  For  example: //  Encrypt(’hi’)  returns  ’lm’
    //  Encrypt(’asdfghjkl’)  returns  ’ewhjklnop’ //  Encrypt(’gf’)  returns  ’kj’
    //  Encrypt(’et’)  returns  ’ix’ func  Encrypt(s  string)  string  { <|role_start|>bot<|role_end|>![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   encrypted  :=  make([]byte,  len(s))    for  i,  c  :=  range  s  {    if  c  >=  ’a’  &&  c  <=  ’z’  {    encrypted[i]  =  byte((c-’a’+2)%26  +  ’a’)  //  shift  down  two  places    }  else  if  c  >=  ’A’  &&  c  <=  ’Z’  {    encrypted[i]  =  byte((c-’A’+2)%26  +  ’A’)    }  else  {    encrypted[i]  =  byte(c)  //  non-alphabetic  characters  remain  the  same    }    }    return  string(encrypted)  }'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 提示（Java）：
    <|role_start|>human<|role_end|>// 语言：Java 导入  java.util.*; 导入  java.lang.*;
    类  Solution  { /** 给定一个字符串 s，计算偶数索引处的大写元音字母的数量。例如： countUpper("aBCdEf") 返回 1 countUpper("abcdefg")
    返回 0 countUpper("dBBE") 返回 0 */ 公共  int  countUpper(String  s)  { <|role_start|>bot<|role_end|>![[未标注的图像]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   int  count  =  0;    对于  (int  i  =  0;  i  <  s.length();  i  +=  2)  {    char  c  =  s.charAt(i);    如果  (Character.isUpperCase(c)  &&  (c  ==  ’A’  ||  c  ==  ’E’  ||  c  ==  ’I’  ||  c  ==  ’O’  ||  c  ==  ’U’))  {    count++;    }    }    返回  count;    }  }
    提示（Golang）：
    <|role_start|>human<|role_end|>// 语言：Go 导入  ( "strings" ) // 创建一个名为
    Encrypt 的函数，它接受一个字符串作为参数，并 // 返回一个字符串，字母表经过旋转。 // 字母表应按两位乘二的方式旋转。 // 例如： // Encrypt(’hi’)
    返回 ’lm’ // Encrypt(’asdfghjkl’) 返回 ’ewhjklnop’ // Encrypt(’gf’) 返回 ’kj’ // Encrypt(’et’)
    返回 ’ix’ 函数  Encrypt(s  string)  string  { <|role_start|>bot<|role_end|>![[未标注的图像]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   encrypted  :=  make([]byte,  len(s))    对于  i,  c  :=  范围  s  {    如果  c  >=  ’a’  &&  c  <=  ’z’  {    encrypted[i]  =  byte((c-’a’+2)%26  +  ’a’)  //  向下移动两位    }  否则  如果  c  >=  ’A’  &&  c  <=  ’Z’  {    encrypted[i]  =  byte((c-’A’+2)%26  +  ’A’)    }  否则  {    encrypted[i]  =  byte(c)  //  非字母字符保持不变    }    }    返回  string(encrypted)  }
- en: C.3 Code Completion with Chinese prompts on CodeFuseEval-CN
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 使用中文提示进行代码补全（CodeFuseEval-CN）
- en: 'Prompt:
    <|role_start|>human<|role_end|># language: Python![[Uncaptioned
    image]](img/86db69e57c0426a2d0beaae9ba122e98.png) <|role_start|>bot<|role_end|>
    ![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   encoded_message  =  ’’    vowels  =  {’a’:  ’y’,  ’A’:  ’Y’,  ’e’:  ’c’,  ’E’:  ’C’,  ’i’:  ’g’,  ’I’:  ’G’,  ’o’:  ’m’,  ’O’:  ’M’,  ’u’:  ’s’,  ’U’:  ’S’}      for  letter  in  message:    if  letter.isalpha():    if  letter  in  vowels:    encoded_message  +=  vowels[letter]    elif  letter.islower():    encoded_message  +=  letter.upper()    elif  letter.isupper():    encoded_message  +=  letter.lower()    else:    encoded_message  +=  letter      return  encoded_message  }'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：
    <|role_start|>human<|role_end|># 语言: Python![[未标注的图片]](img/86db69e57c0426a2d0beaae9ba122e98.png)
    <|role_start|>bot<|role_end|> ![[未标注的图片]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:   encoded_message  =  ’’    vowels  =  {’a’:  ’y’,  ’A’:  ’Y’,  ’e’:  ’c’,  ’E’:  ’C’,  ’i’:  ’g’,  ’I’:  ’G’,  ’o’:  ’m’,  ’O’:  ’M’,  ’u’:  ’s’,  ’U’:  ’S’}      for  letter  in  message:    if  letter.isalpha():    if  letter  in  vowels:    encoded_message  +=  vowels[letter]    elif  letter.islower():    encoded_message  +=  letter.upper()    elif  letter.isupper():    encoded_message  +=  letter.lower()    else:    encoded_message  +=  letter      return  encoded_message  }'
- en: C.4 Text-to-Code on MBPP
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 MBPP上的文本到代码
- en: 'Prompt:
    <|role_start|>human<|role_end|> Write a function to find the number
    of subsequences having product smaller than k for the given non negative array.
    Your code should satisfy these tests: assert  no_of_subsequences([1,2,3,4],  10)  ==  11
    assert  no_of_subsequences([4,8,7,2],  50)  ==  9 assert  no_of_subsequences([5,6,7,8],  15)  ==  4
    <|role_start|>bot<|role_end|>![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    def  no_of_subsequences(arr,  k):    n  =  len(arr)    dp  =  [0]  *  n    dp[0]  =  1    for  i  in  range(1,  n):    dp[i]  =  dp[i-1]    if  arr[i]  <  k:    dp[i]  +=  1    for  j  in  range(i):    if  arr[j]  <  k//arr[i]:    dp[i]  +=  dp[j]    return  dp[n-1]      import  unittest    class  TestNoOfSubsequences(unittest.TestCase):    def  test_no_of_subsequences(self):    self.assertEqual(no_of_subsequences([1,2,3,4],  10),  111)    self.assertEqual(no_of_subsequences([4,8,7,2],  50),  9)    self.assertEqual(no_of_subsequences([5,6,7,8],  15),  4)      if  __name__  ==  "__main__":    unittest.main()'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：
    <|role_start|>人类<|role_end|> 编写一个函数，找出给定非负数组中乘积小于 `k` 的子序列数量。你的代码应满足以下测试：
    assert  no_of_subsequences([1,2,3,4],  10)  ==  11 assert  no_of_subsequences([4,8,7,2],  50)  ==  9
    assert  no_of_subsequences([5,6,7,8],  15)  ==  4 <|role_start|>机器人<|role_end|>![[无标题图片]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    def  no_of_subsequences(arr,  k):    n  =  len(arr)    dp  =  [0]  *  n    dp[0]  =  1    for  i  in  range(1,  n):    dp[i]  =  dp[i-1]    if  arr[i]  <  k:    dp[i]  +=  1    for  j  in  range(i):    if  arr[j]  <  k//arr[i]:    dp[i]  +=  dp[j]    return  dp[n-1]      import  unittest    class  TestNoOfSubsequences(unittest.TestCase):    def  test_no_of_subsequences(self):    self.assertEqual(no_of_subsequences([1,2,3,4],  10),  11)    self.assertEqual(no_of_subsequences([4,8,7,2],  50),  9)    self.assertEqual(no_of_subsequences([5,6,7,8],  15),  4)      if  __name__  ==  "__main__":    unittest.main()'
- en: C.5 Data Science Analysis on DS-1000
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 数据科学分析 DS-1000
- en: 'Prompt:
    I’m using tensorflow 2.10.0. I have a tensor of lengths in tensorflow,
    let’s say it looks like this:  [4,  3,  5,  2] I wish to create a mask of 1s and
    0s whose number of 1s correspond to the entries to this tensor, padded in front
    by 0s to a total length of 8\. I.e. I want to create this tensor: [[0.  0.  0.  0.  1.  1.  1.  1.]
    [0.  0.  0.  0.  0.  1.  1.  1.] [0.  0.  0.  1.  1.  1.  1.  1.] [0.  0.  0.  0.  0.  0.  1.  1.]]
    How might I do this? A:  import tensorflow as tf lengths = [4, 3, 5, 2]
     result = … # put solution in this variable BEGIN SOLUTION ![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    max_length  =  tf.reduce_max(lengths)    #  Create  a  tensor  of  1s  and  0s  mask  =  tf.sequence_mask(lengths,  maxlen=max_length,  dtype=tf.float32)    #  Pad  the  mask  with  0s  to  a  total  length  of  8  padded_mask  =  tf.pad(mask,  [[0,  0],  [0,  8  -  max_length]],  constant_values=0)    result  =  padded_mask'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '我正在使用 tensorflow 2.10.0。我有一个 tensorflow 中的长度张量，假设它看起来像这样：[4, 3, 5, 2]。我希望创建一个
    1 和 0 的掩码，其中 1 的数量对应于此张量的条目，并在前面用 0 填充到总长度为 8。也就是说，我想创建这样的张量：[[0. 0. 0. 0. 1.
    1. 1. 1.] [0. 0. 0. 0. 0. 1. 1. 1.] [0. 0. 0. 1. 1. 1. 1. 1.] [0. 0. 0. 0. 0.
    0. 1. 1.]] 我该怎么做？A: ` import tensorflow as tf lengths = [4, 3, 5, 2] `
    result = … # 将解决方案放入此变量 BEGIN SOLUTION `![[未标注图像]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    max_length  =  tf.reduce_max(lengths)    #  创建  1 和 0 的掩码  mask  =  tf.sequence_mask(lengths,  maxlen=max_length,  dtype=tf.float32)    #  将掩码填充到总长度为
    8  padded_mask  =  tf.pad(mask,  [[0,  0],  [0,  8  -  max_length]],  constant_values=0)    result  =  padded_mask`'
- en: C.6 Code Translation on CodeFuseEval-CodeTrans
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.6 代码翻译在 CodeFuseEval-CodeTrans
- en: 'Prompt:
    <|role_start|>human<|role_end|> #  Translate  the  following  Java  program  to  Python
    #  Java import  java.util.*; import  java.lang.*; class  Solution  { public  List>  getRow(List>  lst,  int  x)  {
    List>  coords  =  new  ArrayList<>(); for  (int  i  =  0;  i  <  lst.size();  i++)  {
    List>  row  =  new  ArrayList<>(); for  (int  j  =  lst.get(i).size()  -  1;  j  >=  0;  j--)  {
    if  (lst.get(i).get(j)  ==  x)  { row.add(Arrays.asList(i,  j)); } } coords.addAll(row);
    } return  coords; } } #  Python <|role_start|>bot<|role_end|>![[Uncaptioned image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    def  get_row(lst,  x):    coords  =  []    for  i  in  range(len(lst)):    for  j  in  range(len(lst[i])-1,  -1,  -1):    if  lst[i][j]  ==  x:    coords.append([i,  j])    return  coords'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：
    <|role_start|>人类<|role_end|> #  将下面的 Java 程序翻译成 Python #  Java
    import  java.util.*; import  java.lang.*; class  Solution  { public  List>  getRow(List>  lst,  int  x)  {
    List>  coords  =  new  ArrayList<>(); for  (int  i  =  0;  i  <  lst.size();  i++)  {
    List>  row  =  new  ArrayList<>(); for  (int  j  =  lst.get(i).size()  -  1;  j  >=  0;  j--)  {
    if  (lst.get(i).get(j)  ==  x)  { row.add(Arrays.asList(i,  j)); } } coords.addAll(row);
    } return  coords; } } #  Python <|role_start|>机器人<|role_end|>![[无标题图像]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    def  get_row(lst,  x):    coords  =  []    for  i  in  range(len(lst)):    for  j  in  range(len(lst[i])-1,  -1,  -1):    if  lst[i][j]  ==  x:    coords.append([i,  j])    return  coords'
- en: C.7 Unit test case Generation on CodeFuseEval-UnitTest
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.7 单元测试用例生成于 CodeFuseEval-UnitTest
- en: 'Prompt:
    <|role_start|>human<|role_end|> Write  unit  tests  for  the  following  code,  without  providing  explanations:
    ‘‘‘ public  class  ExceptionUtils  { private  static  final  String[]  CAUSE_METHOD_NAMES  =  {
    "getCause", "getNextException", "getTargetException", "getException", "getSourceException",
    "getRootCause", "getCausedByException", "getNested", "getLinkedException", "getNestedException",
    "getLinkedCause", "getThrowable", }; public  static  Throwable  getCause(final  Throwable  throwable,  String[]  methodNames)  {
    if  (throwable  ==  null)  { return  null; } if  (methodNames  ==  null)  { final  Throwable  cause  =  throwable.getCause();
    if  (cause  !=  null)  { return  cause; } methodNames  =  CAUSE_METHOD_NAMES;
    } return  Stream.of(methodNames).map(m  ->  getCauseUsingMethodName(throwable,  m)).filter(Objects::nonNull).findFirst().orElse(null);
    } } ‘‘‘ <|role_start|>bot<|role_end|>![[Uncaptioned
    image]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B:
    ‘‘‘  import  org.junit.Assert;  import  org.junit.Test;    import  java.util.Objects;  import  java.util.stream.Stream;    public  class  ExceptionUtilsTest  {    @Test    public  void  testGetCause_NullThrowable()  {    Throwable  throwable  =  null;    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_NullMethodNames()  {    Throwable  throwable  =  new  RuntimeException("test");    String[]  methodNames  =  null;    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_FoundCause()  {    Throwable  throwable  =  new  RuntimeException("test",  new  NullPointerException());    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(NullPointerException.class,  output.getClass());    }    @Test    public  void  testGetCause_NotFoundCause()  {    Throwable  throwable  =  new  RuntimeException("test");    String[]  methodNames  =  {"getCause"};    Throwable  output  =  ExceptionUtils.getCause(throwable,  methodNames);    Assert.assertEquals(null,  output);    }    @Test    public  void  testGetCause_DefaultMethodNames_FoundCause()  {    Throwable  throwable  =  new  RuntimeException("test",  new  NullPointerException());    Throwable  output  =  ExceptionUtils.getCause(throwable);    Assert.assertEquals(NullPointerException.class,  output.getClass());    }    @Test    public  void  testGetCause_DefaultMethodNames_NotFoundCause()  {    Throwable  throwable  =  new  RuntimeException("test");    Throwable  output  =  ExceptionUtils.getCause(throwable);    Assert.assertEquals(null,  output);    }    private  Throwable  getCauseUsingMethodName(Throwable  throwable,  String  methodName)  {    try  {    return  (Throwable)  throwable.getClass().getMethod(methodName).invoke(throwable);    }  catch  (Exception  e)  {    return  null;    }    }  }  ‘‘‘'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
    <|role_start|>人类<|role_end|> 编写以下代码的单元测试，不提供解释：‘‘‘ public class
    ExceptionUtils { private static final String[] CAUSE_METHOD_NAMES = { "getCause",
    "getNextException", "getTargetException", "getException", "getSourceException",
    "getRootCause", "getCausedByException", "getNested", "getLinkedException", "getNestedException",
    "getLinkedCause", "getThrowable", }; public static Throwable getCause(final Throwable
    throwable, String[] methodNames) { if (throwable == null) { return null; } if
    (methodNames == null) { final Throwable cause = throwable.getCause(); if (cause
    != null) { return cause; } methodNames = CAUSE_METHOD_NAMES; } return Stream.of(methodNames).map(m
    -> getCauseUsingMethodName(throwable, m)).filter(Objects::nonNull).findFirst().orElse(null);
    } } ‘‘‘ <|role_start|>bot<|role_end|>![[未标注的图片]](img/ad790c907c104b4df876d249d118ffe3.png)
    CodeFuse-CodeLLama-34B：
    ‘‘‘ import org.junit.Assert; import org.junit.Test; import java.util.Objects;
    import java.util.stream.Stream; public class ExceptionUtilsTest { @Test public
    void testGetCause_NullThrowable() { Throwable throwable = null; String[] methodNames
    = {"getCause"}; Throwable output = ExceptionUtils.getCause(throwable, methodNames);
    Assert.assertEquals(null, output); } @Test public void testGetCause_NullMethodNames()
    { Throwable throwable = new RuntimeException("test"); String[] methodNames = null;
    Throwable output = ExceptionUtils.getCause(throwable, methodNames); Assert.assertEquals(null,
    output); } @Test public void testGetCause_FoundCause() { Throwable throwable =
    new RuntimeException("test", new NullPointerException()); String[] methodNames
    = {"getCause"}; Throwable output = ExceptionUtils.getCause(throwable, methodNames);
    Assert.assertEquals(NullPointerException.class, output.getClass()); } @Test public
    void testGetCause_NotFoundCause() { Throwable throwable = new RuntimeException("test");
    String[] methodNames = {"getCause"}; Throwable output = ExceptionUtils.getCause(throwable,
    methodNames); Assert.assertEquals(null, output); } @Test public void testGetCause_DefaultMethodNames_FoundCause()
    { Throwable throwable = new RuntimeException("test", new NullPointerException());
    Throwable output = ExceptionUtils.getCause(throwable); Assert.assertEquals(NullPointerException.class,
    output.getClass()); } @Test public void testGetCause_DefaultMethodNames_NotFoundCause()
    { Throwable throwable = new RuntimeException("test"); Throwable output = ExceptionUtils.getCause(throwable);
    Assert.assertEquals(null, output); } private Throwable getCauseUsingMethodName(Throwable
    throwable, String methodName) { try { return (Throwable) throwable.getClass().getMethod(methodName).invoke(throwable);
    } catch (Exception e) { return null; } } } ‘‘‘
- en: Appendix D Evaluation results of CodeFuse-QWen-14B on NLP benchmark datasets
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D CodeFuse-QWen-14B 在 NLP 基准数据集上的评估结果
- en: 'Table 12: Comparisons of the performances of CodeFuse-QWen-14B, QWen-14B, and
    QWen-14b-chat on several NLP evaluation datasets. QWen-14B is a base model trained
    by Alibaba Cloud, QWen-14B-chat is a model fine-tuned by themselves on top of
    the QWen-14B base model, and CodeFuse-QWen-14B is a model fine-tuned by us using
    MFTCoder and code data.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：CodeFuse-QWen-14B、QWen-14B 和 QWen-14b-chat 在多个 NLP 评估数据集上的性能比较。QWen-14B
    是由阿里巴巴云训练的基础模型，QWen-14B-chat 是在 QWen-14B 基础模型上进行自我微调的模型，而 CodeFuse-QWen-14B 是我们使用
    MFTCoder 和代码数据微调的模型。
- en: '|  |  | QWen-14B | QWen-14B-chat | CodeFuse-QWen-14B |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  |  | QWen-14B | QWen-14B-chat | CodeFuse-QWen-14B |'
- en: '| Language | AFQMC | 69.00% | 72.6% | 71.99% |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 语言 | AFQMC | 69.00% | 72.6% | 71.99% |'
- en: '| CHID | 84.7% | 72.3% | 84.42% |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| CHID | 84.7% | 72.3% | 84.42% |'
- en: '| Wic | 50.9% | 50.5% | 55.02% |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Wic | 50.9% | 50.5% | 55.02% |'
- en: '| WSC | 66.3% | 66.3% | 68.27% |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| WSC | 66.3% | 66.3% | 68.27% |'
- en: '|  | Average | 67.73% | 65.43% | 69.93% |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 67.73% | 65.43% | 69.93% |'
- en: '| Reasoning | COPA | 93% | 89% | 95.00% |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | COPA | 93% | 89% | 95.00% |'
- en: '| CMNLI | 62.1% | 60.3% | 55.33% |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| CMNLI | 62.1% | 60.3% | 55.33% |'
- en: '| OCNLI | 58.2% | 53.8% | 49.22% |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| OCNLI | 58.2% | 53.8% | 49.22% |'
- en: '| AX-b | 49.5% | 58.5% | 67.84% |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| AX-b | 49.5% | 58.5% | 67.84% |'
- en: '| AX-g | 80.9% | 52.5% | 82.87% |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| AX-g | 80.9% | 52.5% | 82.87% |'
- en: '| RTE | 71.5% | 51.6% | 76.53% |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 71.5% | 51.6% | 76.53% |'
- en: '|  | Average | 69.20% | 60.95% | 71.13% |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 69.20% | 60.95% | 71.13% |'
- en: '| Understanding | CSL | 54.4% | 55.6% | 68.50% |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 理解 | CSL | 54.4% | 55.6% | 68.50% |'
- en: '| C3 | 90.8% | 91.7% | 91.01% |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 90.8% | 91.7% | 91.01% |'
- en: '| EPRSTMT | 86.9% | 91.2% | 84.92% |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| EPRSTMT | 86.9% | 91.2% | 84.92% |'
- en: '|  | Average | 77.37% | 79.50% | 81.48% |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 77.37% | 79.50% | 81.48% |'
- en: '| Examination | AX-b | 67.9% | 66.4% | 64.27% |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 考试 | AX-b | 67.9% | 66.4% | 64.27% |'
- en: '| AX-g | 71.7% | 71.7% | 68.98% |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| AX-g | 71.7% | 71.7% | 68.98% |'
- en: '| RTE | 84.4% | 80.3% | 84.75% |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 84.4% | 80.3% | 84.75% |'
- en: '|  | Average | 74.67% | 72.80% | 72.67% |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均值 | 74.67% | 72.80% | 72.67% |'
