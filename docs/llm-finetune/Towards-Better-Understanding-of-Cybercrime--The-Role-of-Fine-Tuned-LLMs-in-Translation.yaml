- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '--> '
- en: 'Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in
    Translation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更好地理解网络犯罪：微调LLM在翻译中的作用
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01940](https://ar5iv.labs.arxiv.org/html/2404.01940)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01940](https://ar5iv.labs.arxiv.org/html/2404.01940)
- en: Veronica Valeros Department of Computer Science
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Veronica Valeros 计算机科学系
- en: Czech Technical University Prague, Czech Republic
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 捷克技术大学，捷克共和国
- en: valerver@fel.cvut.cz    Anna Širokova Rapid7
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: valerver@fel.cvut.cz    Anna Širokova Rapid7
- en: Prague, Czech Republic
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 布拉格，捷克共和国
- en: anna_sirokova@rapid7.com    Carlos Catania School of Engineering
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: anna_sirokova@rapid7.com    Carlos Catania 工程学院
- en: National University of Cuyo (UNCuyo) Prague, Czech Republic
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 库约国家大学（UNCuyo），捷克共和国
- en: harpo@ingenieria.uncuyo.edu.ar    Sebastian Garcia Department of Computer Science
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: harpo@ingenieria.uncuyo.edu.ar    Sebastian Garcia 计算机科学系
- en: Czech Technical University Prague, Czech Republic
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 捷克技术大学，捷克共和国
- en: sebastian.garcia@agents.fel.cvut.cz
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: sebastian.garcia@agents.fel.cvut.cz
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Understanding cybercrime communications is paramount for cybersecurity defence.
    This often involves translating communications into English for processing, interpreting,
    and generating timely intelligence. The problem is that translation is hard. Human
    translation is slow, expensive, and scarce. Machine translation is inaccurate
    and biased. We propose using fine-tuned Large Language Models (LLM) to generate
    translations that can accurately capture the nuances of cybercrime language. We
    apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist
    group. Our results show that our fine-tuned LLM model is better, faster, more
    accurate, and able to capture nuances of the language. Our method shows it is
    possible to achieve high-fidelity translations and significantly reduce costs
    by a factor ranging from 430 to 23,000 compared to a human translator.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 了解网络犯罪通信对网络安全防御至关重要。这通常涉及将通信翻译成英语进行处理、解释和生成及时情报。问题在于翻译是困难的。人工翻译慢、昂贵且稀缺。机器翻译不准确且有偏见。我们建议使用微调的大型语言模型（LLM）来生成能够准确捕捉网络犯罪语言细微差别的翻译。我们将这一技术应用于NoName057(16)俄语黑客组织的公共聊天中。我们的结果表明，我们的微调LLM模型更好、更快、更准确，能够捕捉语言的细微差别。我们的方法表明，可以实现高保真的翻译，并将成本显著降低，从430到23,000倍，相比人工翻译。
- en: 'Index Terms:'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: cybercrime, hacktivism, LLM, machine translation, model fine-tuning
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络犯罪，黑客主义，LLM，机器翻译，模型微调
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The escalation of the Russia-Ukraine war in 2022 has brought with it a large
    number of cyber-attacks [[6](#bib.bibx6), [8](#bib.bibx8)]. The war fuelled and
    instigated cyber-hacktivist groups to join in, which in turn influenced and sustained
    more cyber operations. Many cyber-hacktivist groups quickly pledged allegiance
    to one side or the other [[11](#bib.bibx11)], generating a massive increase in
    cyber attacks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年俄乌战争的升级带来了大量网络攻击[[6](#bib.bibx6), [8](#bib.bibx8)]。战争激励和煽动了网络黑客组织的参与，这反过来影响和维持了更多的网络操作。许多网络黑客组织迅速宣誓效忠于某一方，导致网络攻击大幅增加[[11](#bib.bibx11)]。
- en: Given the participation of cyber-hacktivists in the war, it has become paramount
    to interpret their online campaigns in a time-efficient manner in order to better
    understand their tactics, motivations, and alliances. A better understanding of
    this evolving landscape contributes to the implementation of effective countermeasures [[15](#bib.bibx15),
    [11](#bib.bibx11)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于网络黑客活动者参与战争，及时解读他们的在线活动变得至关重要，以便更好地理解他们的战术、动机和联盟。对这一不断演变的格局有更好的理解，有助于实施有效的对策[[15](#bib.bibx15),
    [11](#bib.bibx11)]。
- en: The main problem that this research addresses is that manually translating and
    analysing online chats in Russian-language groups is hard, costly [[34](#bib.bibx34)],
    slow, not scalable, biased, inaccurate, and exposes human analysts to toxic and
    disturbing content [[3](#bib.bibx3)]. Additionally, analysts who can accurately
    produce translations are scarce, and analysts do not know all languages with sufficient
    proficiency [[29](#bib.bibx29)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究解决的主要问题是，手动翻译和分析俄语群组中的在线聊天是困难的、昂贵的[[34](#bib.bibx34)]、缓慢的、不可扩展的、有偏见的、不准确的，并且暴露给分析师有毒和令人不安的内容[[3](#bib.bibx3)]。此外，能够准确生成翻译的分析师稀缺，且分析师并不精通所有语言[[29](#bib.bibx29)]。
- en: Translating is difficult because of the complexity given by cultural differences,
    jargon, Internet slang, and inner terminology. It is costly because human translators
    are scarce, their time is very valuable, and often, many are needed even to understand
    one individual group, thus limiting the possible number of translations. Translating
    is slow, averaging 2,000 words per day per translator [[23](#bib.bibx23)], making
    it not scalable for the hundreds of thousands of chats online, given that a single
    group can produce more than 1,000 words in one day. Also, there is the issue of
    uncontrolled human bias, given translators will vary in their experiences, expertise,
    tiredness and availability. Translating has inaccuracies, as human translators
    also need to learn and understand the language nuances and the context of the
    translations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译之所以困难，是因为文化差异、术语、网络俚语和内部术语带来的复杂性。成本高是因为人工翻译人员稀缺，他们的时间非常宝贵，并且往往需要许多人才能理解一个特定的群体，从而限制了可能的翻译数量。翻译速度慢，平均每位翻译人员每天翻译2,000个词[[23](#bib.bibx23)]，这使得面对数十万在线聊天的情况时，难以扩展，因为一个单独的群体一天可以产生超过1,000个词。此外，还有不受控的人为偏见问题，因为翻译人员的经验、专业水平、疲劳程度和可用性会有所不同。翻译中也存在不准确性，因为人工翻译人员需要学习和理解语言的细微差别以及翻译的背景。
- en: Human translators also face an additional challenge regarding the type of content.
    They are susceptible to hate speech and inflammatory material, which is one of
    the reasons that it is unhealthy to have any one translator exposed to this type
    of content for an extended period, thus increasing the time and cost [[3](#bib.bibx3)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人工翻译人员还面临着额外的挑战，涉及内容的类型。他们容易受到仇恨言论和煽动性材料的影响，这也是为什么让任何一个翻译人员长时间接触这种内容是不健康的原因之一，从而增加了时间和成本[[3](#bib.bibx3)]。
- en: Another important challenge lies in the ability to process and study thousands
    of these messages in a time-efficient manner. At the time of writing, state-of-the-art
    shows that real-time processing of these messages using machine translation is
    not possible [[17](#bib.bibx17)]. Solutions like Google Translate [[10](#bib.bibx10)]
    or DeepL [[7](#bib.bibx7)] introduce important mistakes, such as translating a
    URL, which forces the need to have human analysts validate and correct the translations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的挑战在于以时间高效的方式处理和研究成千上万条这些信息。根据当前的最先进技术，使用机器翻译进行这些信息的实时处理是不可能的[[17](#bib.bibx17)]。像Google
    Translate[[10](#bib.bibx10)]或DeepL[[7](#bib.bibx7)]这样的解决方案会引入重要的错误，例如翻译网址，这就迫使需要人类分析师来验证和修正翻译。
- en: Several methods have been proposed to address different parts of the previously
    mentioned problems. Human translation is possibly the most common method, with
    the translators trying to obtain some knowledge of the subject, albeit with the
    previously discussed limitations [[28](#bib.bibx28)]. Machine translation was
    used for cybercrime Twitter translation [[29](#bib.bibx29)]. However, it was a
    hybrid approach, with humans verifying the translation and categories by hand.
    Machine translation has its own limitations, such as biases and inaccuracies [[20](#bib.bibx20)].
    In addition, machine translation alone may be insufficient, given the translation
    does not only include content typically present in conversations. Common problems
    include mistranslating punctuation, URLs, slang, emojis, humour, and a lack of
    consistency in naming conventions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决之前提到的各种问题，已经提出了几种方法。人工翻译可能是最常见的方法，翻译人员会尽量了解主题，尽管存在之前讨论的限制[[28](#bib.bibx28)]。机器翻译曾用于网络犯罪推特翻译[[29](#bib.bibx29)]，但这是一种混合方法，人类手动验证翻译和类别。机器翻译有其自身的局限性，如偏见和不准确[[20](#bib.bibx20)]。此外，单靠机器翻译可能不足够，因为翻译不仅仅包括对话中常见的内容。常见的问题包括标点符号、网址、俚语、表情符号、幽默的误译，以及命名惯例缺乏一致性。
- en: To overcome most of these limitations, we propose to study and eventually fine-tune
    a cloud-based Large Language Model (LLM) with curated translations of cyber-hacktivism
    chat messages. Cloud-based LLMs were trained with a large amount of data, documents,
    and expertise in many languages. By default, vanilla LLMs may not be sufficient
    to translate the messages accurately [[31](#bib.bibx31)]. Therefore, we propose
    to fine-tune models to learn the specifics of the hacktivist groups’ messages
    to generate better, faster, cheaper, and more accurate translations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，我们建议研究并最终微调基于云的**大型语言模型**（LLM），使用精心编排的网络黑客主义聊天信息翻译。基于云的LLM已通过大量数据、文档和多语言专业知识进行训练。默认情况下，原始LLM可能不足以准确翻译这些信息[[31](#bib.bibx31)]。因此，我们建议微调模型以学习黑客主义组织消息的具体细节，从而生成更好、更快、更便宜、更准确的翻译。
- en: Our methodology consists of downloading a large number of Russian-language chats
    from the public online Telegram channel of the cyber-hacktivist group NoName057(16) [[22](#bib.bibx22)],
    and a combination of several vanilla cloud-based LLM models, local-based LLM models,
    and human translators to fine-tune a cloud-based LLM. The dataset was split into
    training and testing sets to evaluate the performance of the system in unseen
    messages. The models were evaluated and compared by a test group of native Russian
    speakers with cybersecurity knowledge who did not participate in the original
    translation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括从网络黑客主义组织NoName057(16)的公共在线Telegram频道下载大量俄语聊天记录，并结合多个原始的基于云的LLM模型、本地LLM模型和人工翻译员来微调基于云的LLM。数据集被分为训练集和测试集，以评估系统在未见消息中的性能。模型由一组具有网络安全知识的俄语母语者进行评估和比较，他们未参与原始翻译。
- en: Comparing the fine-tuned LLM model with the not fine-tuned model and measuring
    differences with human translators shows that our fine-tuned model based on GPT-3.5-turbo
    obtained the best performance. These models performance was also measured with
    BLUE (0.347), METEOR (0.711), and TER (47.792) metrics. In a blind test, human
    translators choose the fine-tuned model as the best translation in 64.08% of the
    cases.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 比较微调后的LLM模型与未微调模型，并与人工翻译员测量差异显示，我们基于GPT-3.5-turbo的微调模型获得了最佳表现。这些模型的表现还通过BLUE（0.347）、METEOR（0.711）和TER（47.792）指标进行衡量。在盲测中，人工翻译员在64.08%的情况下选择了微调模型作为最佳翻译。
- en: Such an accurate model for Russian-based cybercrime chats allows the research
    community to have a cheaper, faster, and more accurate cybercrime-oriented translation
    of Russian text to English. We hope this will allow for more timely and accurate
    translations and to better understanding of cyber-hacktivism activities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种针对俄语网络犯罪聊天的准确模型使研究界能够实现更便宜、更快、更准确的网络犯罪导向翻译，从俄语到英语。我们希望这能带来更及时和准确的翻译，并更好地理解网络黑客主义活动。
- en: 'The contributions of this paper are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献包括：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Publication of a manually translated and curated dataset of a selection of NoName057(16)
    chat messages.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发布手动翻译和整理的NoName057(16)聊天信息数据集。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A thorough comparison of various LLM-based translation methods with human translators.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种基于LLM的翻译方法与人工翻译的全面比较。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Methodology on how to generate a fine-tuned model from cybercrime chats.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于如何从网络犯罪聊天中生成微调模型的方法。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Two public tools for collection and translation of text from Russian to English.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个用于从俄语到英语文本的收集和翻译的公共工具。
- en: 2 Related Work
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: The reviewed literature focuses on three main areas. First, we review studies
    on translations from cyber-hacktivism chats or alike, specifically Russian-English
    translations that include jargon. Second, we review previous efforts on machine
    translation in the context of cybersecurity and its limitations. Third, we review
    existing work on the use of Large Language Models (LLMs) for translation in the
    context of cybersecurity. While most approaches to produce advances in this area
    have been focused on underground forums, there are no specific evaluations of
    hacktivist public broadcasting channels that are so prevalent today and are the
    focus of this work.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 综述文献集中在三个主要领域。首先，我们回顾了网络黑客主义聊天或类似内容的翻译研究，特别是包括行话的俄英翻译。其次，我们回顾了网络安全背景下机器翻译的先前工作及其局限性。第三，我们回顾了在网络安全背景下使用大型语言模型（LLMs）进行翻译的现有工作。尽管在这一领域取得进展的大多数方法集中在地下论坛上，但目前没有专门评估像今天这样普遍存在并且是本文重点的黑客主义公共广播频道的工作。
- en: The task of translating content from the cybercrime world is challenging, as
    it is filled with jargon, internet slang, emojis, and other content that current
    machine translation (MT) methods fail to capture and translate correctly [[18](#bib.bibx18),
    [30](#bib.bibx30)]. Manatova et al. [[18](#bib.bibx18)] argue that current machine
    translation is not capable of capturing dark humour, jokes, and other aspects
    of how underground actors communicate. Specifically, the authors mention that
    correctly understanding these nuances of the language can help better understand
    attackers’ motivations, strategies, and other social dynamics that are key in
    cyber threat research. Seyler et al. [[30](#bib.bibx30)] further emphasise the
    challenges of understanding the dark jargon in underground forums where mistranslated
    or misinterpreted words can lead to the wrong classification of content.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络犯罪领域翻译内容的任务具有挑战性，因为它充满了行话、网络俚语、表情符号以及其他当前机器翻译（MT）方法无法准确捕捉和翻译的内容[[18](#bib.bibx18),
    [30](#bib.bibx30)]。Manatova 等人[[18](#bib.bibx18)]认为，目前的机器翻译无法捕捉到黑暗幽默、笑话以及地下参与者交流的其他方面。具体来说，作者提到，正确理解这些语言的细微差别有助于更好地理解攻击者的动机、策略以及其他在网络威胁研究中至关重要的社会动态。Seyler
    等人[[30](#bib.bibx30)]进一步强调了理解地下论坛中黑暗行话的挑战，因为翻译错误或误解可能导致内容的错误分类。
- en: The limitations of machine translation in the context of cybersecurity have
    already been discussed thoroughly in [[18](#bib.bibx18)], where authors highlight
    how state-of-the-art MT in content from underground forums and other jargon-loaded
    content leads to semantic loss and directly affects the efficacy of cyber threat
    identification. Ebrahimi et al.  [[9](#bib.bibx9)] remark that the three key drawbacks
    of MT are that they omit language-specific semantics, miss hacker-specific jargon,
    and rely on separate monolingual models for each language. This results in mistranslations
    or incomplete ones. Moreover, Michel and Neubig [[20](#bib.bibx20)] observed that
    a large portion of research in the MT field relies on synthetically generated
    datasets for model evaluation. They noted that these datasets lack noise, such
    as emojis, internet slang, profanities, and other linguistic complexities, which
    cybercrime forums and chats are known to have. This further emphasises the inadequacy
    of MT and its tendency to produce inaccurate translations in such environments.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译在网络安全领域的局限性已经在[[18](#bib.bibx18)]中得到了充分讨论，作者强调，最先进的 MT 在处理来自地下论坛和其他充满行话的内容时会导致语义损失，并直接影响网络威胁识别的效果。Ebrahimi
    等人[[9](#bib.bibx9)]指出，MT 的三个关键缺点是遗漏语言特有的语义、遗漏黑客专用行话，以及依赖于每种语言的单语模型。这导致了翻译错误或不完整。此外，Michel
    和 Neubig[[20](#bib.bibx20)]观察到，MT 领域的大部分研究依赖于合成生成的数据集进行模型评估。他们指出，这些数据集缺乏噪声，如表情符号、网络俚语、脏话和其他语言复杂性，而这些在网络犯罪论坛和聊天中是常见的。这进一步强调了
    MT 的不足以及在此类环境中产生不准确翻译的倾向。
- en: The use of LLMs for translation is an emerging topic that promises to resolve
    some machine translation shortcomings. Nikolich and Puchkova [[21](#bib.bibx21)]
    fine-tuned a GPT-3 model using a Russian dataset to produce an English translation,
    producing promising results. Nevertheless, the approach still presented limitations,
    especially when respecting essential elements of the text, such as names, surnames,
    places, and dates.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）进行翻译是一个新兴话题，承诺解决一些机器翻译的不足之处。Nikolich 和 Puchkova[[21](#bib.bibx21)]使用俄语数据集对
    GPT-3 模型进行了微调，以生成英文翻译，取得了有希望的结果。然而，这种方法仍然存在局限性，特别是在尊重文本的基本元素（如姓名、姓氏、地点和日期）时。
- en: Both Zhu et al. [[35](#bib.bibx35)] and Jiao et al. [[14](#bib.bibx14)] evaluated
    ChatGPT with GPT-3.5 and GPT-4 in translation tasks and compared it to other translation
    methods. Their work shows the promises of GPT-4, noting challenges in languages
    that may be more distant from English, such as Chinese. In both cases, the ChatGPT
    platform is used and not the OpenAI-specific models, which makes it harder to
    know precisely which version of the models was used. Manakhimova et al. [[17](#bib.bibx17)]
    conducted a systematic translation comparison primarily from English to other
    two languages with 37 translation systems. While GPT-4 performance was good, it
    was outperformed by other techniques and approaches. Among the main challenges
    listed by the authors are the linguistic nuances, which are significant in our
    focus area.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu 等人[[35](#bib.bibx35)] 和 Jiao 等人[[14](#bib.bibx14)] 评估了 ChatGPT 在翻译任务中的表现，使用了
    GPT-3.5 和 GPT-4，并与其他翻译方法进行了比较。他们的工作展示了 GPT-4 的前景，但也指出了在与英语较远的语言（如中文）中存在的挑战。在这两种情况下，使用的是
    ChatGPT 平台而非 OpenAI 专用模型，这使得很难准确知道使用了哪个版本的模型。Manakhimova 等人[[17](#bib.bibx17)]
    主要通过 37 个翻译系统进行了系统的翻译比较，翻译方向从英语到其他两种语言。虽然 GPT-4 的表现良好，但它被其他技术和方法超越。作者列出的主要挑战之一是语言学的细微差别，这在我们关注的领域中尤为重要。
- en: In [[28](#bib.bibx28)], authors show how fine-tuning and prompt engineering
    can be used in LLMs for augmented MT. While the evaluation shows that the human
    translation is overall better, the combination of fine-tuning and prompt engineering
    can produce better results and help reduce the human input to the most needed
    tasks, like post-editing. Similarly, Siu [[31](#bib.bibx31)] showed how LLM models
    can help in various translation tasks such as error detection and grammar checking,
    further helping use the costly and expensive human input in more critical tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[28](#bib.bibx28)]中，作者展示了如何在 LLMs 中使用微调和提示工程来增强机器翻译。虽然评估显示人类翻译总体上更好，但微调和提示工程的结合可以产生更好的结果，并帮助将人类输入减少到最需要的任务，如后期编辑。同样，Siu[[31](#bib.bibx31)]
    展示了 LLM 模型如何在各种翻译任务中提供帮助，如错误检测和语法检查，从而将昂贵的人力投入到更关键的任务中。
- en: Peng et al. [[27](#bib.bibx27)] proposed to improve MT using Chat-GPT and Domain-Specific-Prompts
    (DSP) through the prompts. They concluded that this method indeed improves the
    translation. However, they also pointed out that their prompts were not designed
    to test ChatGPT abilities fully.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Peng 等人[[27](#bib.bibx27)] 提出了通过提示改善机器翻译的 Chat-GPT 和领域特定提示 (DSP) 的方法。他们得出结论，这种方法确实改善了翻译。然而，他们也指出，他们的提示并没有完全测试
    ChatGPT 的能力。
- en: 3 Methodology
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'The raw data used for this study contains 5,455 text messages extracted from
    the Telegram public channel of the hacktivist group NoName057(16) [[22](#bib.bibx22)],
    spanning from the creation of the channel on March 11, 2022, to December 26, 2023\.
    The Telegram channel, shown in Figure [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ Towards
    Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation"),
    is an open public channel that anyone can read anonymously. The data was collected
    by developing a custom Python tool.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究使用的原始数据包含 5,455 条从黑客组织 NoName057(16) 的 Telegram 公开频道提取的文本消息，涵盖了频道创建日期 2022
    年 3 月 11 日到 2023 年 12 月 26 日。Telegram 频道如图 [1](#S3.F1 "Figure 1 ‣ 3 Methodology
    ‣ Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation")
    所示，是一个开放的公共频道，任何人都可以匿名阅读。数据是通过开发一个自定义的 Python 工具收集的。'
- en: '![Refer to caption](img/016302efad34d2f64eb135e097522ac9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/016302efad34d2f64eb135e097522ac9.png)'
- en: 'Figure 1: Public Telegram channel of the NoName057(16) hacktivist group, in
    Russian, where they showcase and publicise their activity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: NoName057(16) 黑客组织的公共 Telegram 频道，用俄语展示和宣传他们的活动。'
- en: 'The methodology is composed of (i) creating a dataset from the Telegram chats;
    (ii) using eight different LLM models to translate the messages (both cloud and
    local); (iii) a translator selecting the best translation (train translator);
    (iv) training a fine-tuned model; (v) comparing all models and evaluating with
    a new group of translators (test translators); (vi) evaluating the results with
    analytical metrics. Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodology ‣ Towards Better
    Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation") shows
    the detailed steps.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法包括（i）从Telegram聊天创建数据集；（ii）使用八种不同的LLM模型翻译消息（包括云端和本地）；（iii）翻译人员选择最佳翻译（训练翻译人员）；（iv）训练一个微调的模型；（v）比较所有模型并与新的翻译人员组进行评估（测试翻译人员）；（vi）使用分析指标评估结果。图[2](#S3.F2
    "图2 ‣ 3 方法论 ‣ 更好地理解网络犯罪：微调LLM在翻译中的作用")展示了详细的步骤。
- en: '![Refer to caption](img/e39e74c2516b78ade9c2ec25c29ccb0f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e39e74c2516b78ade9c2ec25c29ccb0f.png)'
- en: 'Figure 2: The hacktivist group messages were processed first to compare existing
    translation methods and select the best one, using the output and the expert input
    to produce ground truth and fine-tune the selected LLM model, and finally, generating
    the data needed for the human and automatic evaluation.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：首先处理了黑客组织的消息，以比较现有的翻译方法并选择最佳方案，使用输出和专家输入来生成真实翻译并微调所选择的LLM模型，最后生成所需的数据用于人工和自动评估。
- en: 3.1 Creation of the dataset
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集的创建
- en: The chat messages of the Telegram channel of the group NoName057(16) between
    March 11, 2022, and December 26, 2023, were downloaded with our open-source tool
    called Spylegram [[2](#bib.bibx2)], that accesses the Telegram API using the Telethon
    library [[16](#bib.bibx16)] and stores them in a SQLite database.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从2022年3月11日到2023年12月26日，NoName057(16)小组的Telegram频道聊天消息通过我们名为Spylegram的开源工具[[2](#bib.bibx2)]下载，该工具使用Telethon库[[16](#bib.bibx16)]访问Telegram
    API，并将其存储在SQLite数据库中。
- en: The first 130 messages were selected (chronologically) for the training, evaluation,
    and testing. A hundred messages were used for training and validation, while 30
    were reserved only for testing and never used in the creation of the models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的130条消息被按时间顺序选择用于训练、评估和测试。100条消息用于训练和验证，而30条消息则仅用于测试，并且从未在模型创建中使用。
- en: The native Russian expert was asked to manually translate all 130 messages from
    Russian to English and generate the ground truth translation. The complete dataset
    had 130 rows with the original message in Russian and the ground truth translation
    in English.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请俄罗斯母语专家手动将所有130条消息从俄语翻译成英语，并生成真实翻译。完整的数据集包含130行，包含原始的俄语消息和真实的英语翻译。
- en: This dataset of 130 messages was split into 100 messages for training/validation
    and 30 for testing ¹¹1The link to the dataset has been omitted for anonymization
    purposes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个130条消息的数据集被分为100条用于训练/验证，30条用于测试¹¹1由于匿名化目的，数据集的链接已被省略。
- en: 'Later, during fine-tuning, the 100 messages of training/validation were augmented
    with 25 messages of vocabulary to complete the 125 messages of the dataset for
    fine-tuning, as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodology ‣ Towards
    Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，在微调过程中，100条训练/验证消息被25条词汇消息扩充，以完成用于微调的数据集中的125条消息，如图[2](#S3.F2 "图2 ‣ 3 方法论
    ‣ 更好地理解网络犯罪：微调LLM在翻译中的作用")所示。
- en: 3.2 Using current LLMs as a Translation Method
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用当前LLM作为翻译方法
- en: 'The first 100 messages in the dataset were translated with the following eight
    models:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中前100条消息使用了以下八种模型进行翻译：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Google Translate (Deep Neural Nets, cloud) [[10](#bib.bibx10)]
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Google Translate（深度神经网络，云端）[[10](#bib.bibx10)]
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepL (Deep Neural Nets, cloud) [[7](#bib.bibx7)]
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepL（深度神经网络，云端）[[7](#bib.bibx7)]
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-3.5-turbo-0125 (LLM, cloud) [[25](#bib.bibx25)]
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-3.5-turbo-0125（LLM，云端）[[25](#bib.bibx25)]
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mistral (LLM, local) [[13](#bib.bibx13)]
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mistral（LLM，本地）[[13](#bib.bibx13)]
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Neural chat (LLM, local) [[12](#bib.bibx12)]
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Neural chat（LLM，本地）[[12](#bib.bibx12)]
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zephyr (LLM, local) [[33](#bib.bibx33)]
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zephyr（LLM，本地）[[33](#bib.bibx33)]
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 prompt 1 (LLM, cloud) [[24](#bib.bibx24)]
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 prompt 1（LLM，云端）[[24](#bib.bibx24)]
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 prompt 2 (LLM, cloud) [[24](#bib.bibx24)]
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 prompt 2（LLM，云端）[[24](#bib.bibx24)]
- en: In total, 800 translations were produced. The same native Russian expert who
    generated the ground truth evaluated the translations for each of the 100 messages.
    For each message, the expert selected which method generated the best translation.
    The top selected method was then chosen for the next stage.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总共生成了 800 个翻译。生成原始基准的同一位俄语专家对每条消息的翻译进行了评估。对于每条消息，专家选择了哪种方法生成了最佳翻译。然后选择了排名最高的方法用于下一阶段。
- en: The LLM translations were orchestrated with our own tool, called HermeneisGPT [[1](#bib.bibx1)].
    The tool is able to translate messages using a given prompt using OpenAI API and
    storing the translation along with the translation parameters on an SQLite DB.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 翻译是通过我们自己的工具 HermeneisGPT [[1](#bib.bibx1)] 进行协调的。该工具能够使用给定提示通过 OpenAI API
    翻译消息，并将翻译及翻译参数存储在 SQLite 数据库中。
- en: 3.3 LLM Fine-tuning
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 微调
- en: The Russian expert selected an LLM model as the best model in the training dataset.
    We decided to fine-tune it to adapt it to the nuances of the Russian language
    cybercriminal world. Fine-tuning consists of improving the LLM model by giving
    as input a new dataset with the correct expected outputs. In our case, the correct
    expected outputs were the corrections of the native Russian expert over the translated
    messages of the best model on training data. The native Russian expert also provided
    new vocabulary and its correct translation based on the errors made by the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 俄语专家在训练数据集中选择了一个 LLM 模型作为最佳模型。我们决定对其进行微调，以适应俄语网络犯罪世界的细微差别。微调包括通过提供新的数据集和正确的期望输出来改进
    LLM 模型。在我们的案例中，正确的期望输出是对最佳模型在训练数据上的翻译消息的俄语专家的修正。俄语专家还根据模型的错误提供了新的词汇及其正确翻译。
- en: 'The dataset used for fine-tuning has three parts: (i) a prompt with instructions,
    (ii) the original message in Russian, and (iii) the correct translation. In some
    cases, the correct translation is the improvement of the Russian expert upon the
    output of the best model during training; in others, it is the correct translation
    of a word. In total, there are 130 Russian texts along with their correct English
    translation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调的数据集分为三部分：（i）带有指令的提示，（ii）原始俄语消息，以及（iii）正确的翻译。在某些情况下，正确的翻译是俄语专家在训练期间对最佳模型输出的改进；在其他情况下，它是单词的正确翻译。总共有
    130 条俄语文本及其正确的英文翻译。
- en: The prompt for fine-tuning used sentences with clear directives and incorporated
    feedback from the language expert. It emphasised respecting URLs, names, links,
    dates, and other important information. The full prompt is shown in Appendix I.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的提示使用了明确指令的句子，并结合了语言专家的反馈。它强调了尊重 URLs、名称、链接、日期和其他重要信息。完整的提示见附录 I。
- en: The original ground-truth 100 messages for training/validation were augmented
    with 25 vocabulary corrections, to sum up to 125 messages in the fine-tuning dataset.
    This fine-tuning dataset was split using an 80/20 separation into fine-tuning
    training (100 messages) and fine-tuning validation (25 messages). The split was
    done so that the specialised vocabulary generated by the Russian expert was present
    only in the training, with the rest of the messages randomised. The validation
    data was composed only of hacktivist messages.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 100 条训练/验证消息通过 25 条词汇纠正进行了扩充，总计 125 条消息在微调数据集中。这个微调数据集按照 80/20 的比例被拆分为微调训练（100
    条消息）和微调验证（25 条消息）。拆分的目的是确保俄语专家生成的专业词汇仅存在于训练数据中，其余消息则随机化。验证数据仅由黑客主义消息组成。
- en: The fine-tuning was done through the OpenAI web platform, using the base model
    gpt-3.5-turbo-0125 with training data up to September 2021, which was the best
    model chosen by the language expert.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是在 OpenAI 网络平台上完成的，使用了基础模型 gpt-3.5-turbo-0125，训练数据截至到 2021 年 9 月，这是语言专家选择的最佳模型。
- en: 'The dataset used for fine-tuning is in JSONL format, the standard from OpenAI.
    Each JSONL in the dataset contains a message with three key roles: the system
    role contains the prompt, the user role contains the message in Russian, and the
    assistant role contains the English translation’s ground truth. The template for
    each entry in the dataset used for fine-tuning is shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.3 LLM Fine-tuning ‣ 3 Methodology ‣ Towards Better Understanding of Cybercrime:
    The Role of Fine-Tuned LLMs in Translation").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调的数据集采用 JSONL 格式，这是 OpenAI 的标准。数据集中的每个 JSONL 包含一个具有三个关键角色的消息：系统角色包含提示，用户角色包含俄语消息，助手角色包含英语翻译的真实值。用于微调的数据集中每个条目的模板如图
    [3](#S3.F3 "图 3 ‣ 3.3 LLM 微调 ‣ 3 方法论 ‣ 朝向更好地理解网络犯罪：微调 LLM 在翻译中的作用") 所示。
- en: '![Refer to caption](img/19a49927d04f1f37abbf88a9d9f1537c.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/19a49927d04f1f37abbf88a9d9f1537c.png)'
- en: 'Figure 3: The dataset used for fine-tuning has a JSONL format, where each line
    contains a message with three keys. Each key represents a role: system, user,
    and assistant.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于微调的数据集采用 JSONL 格式，每一行包含一个具有三个键的消息。每个键代表一个角色：系统、用户和助手。
- en: 3.4 Human Evaluation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 人工评估
- en: Our first evaluation of whether our fine-tuned LLM model is better than the
    non-fine-tuned LLM model was done using a new test group of human translators.
    Human evaluation is arguably the most important and precise type of comparison,
    given humans are much better than any tool at evaluating the quality of translations [[5](#bib.bibx5)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首次评估了微调后的 LLM 模型是否优于未微调的 LLM 模型，使用了一个新的人工翻译测试组。人工评估可以说是最重要和最精确的比较类型，因为人类在评估翻译质量方面远胜于任何工具[[5](#bib.bibx5)]。
- en: The evaluation methodology consisted of asking native Russian speakers with
    technical background if, given the original Russian text, they preferred the translation
    of the original LLM model or the fine-tuned LLM model. The interaction was done
    using an online form where the experiment was explained in full, the privacy disclosure
    was done, and the voting was performed for 30 questions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法包括询问具有技术背景的俄语母语者，如果给定原始俄语文本，他们更喜欢原始 LLM 模型的翻译还是微调后的 LLM 模型的翻译。互动通过在线表单进行，其中详细解释了实验，进行了隐私披露，并对
    30 个问题进行了投票。
- en: For each question, respondents were presented with an original hacktivist message
    in Russian, the translation by the base LLM model, and the translation by the
    fine-tuned LLM model. The survey was configured in a way that the respondents
    did not know which translation belonged to which model. The position of the model
    translations was randomised from question to question, making it harder for respondents
    to guess which model was which.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个问题，受访者会看到一条原始的黑客行动信息，LLM 基础模型的翻译，以及微调 LLM 模型的翻译。调查设置为受访者不知道哪个翻译属于哪个模型。模型翻译的位置在问题之间随机化，使得受访者更难猜测哪个模型是哪个。
- en: 'The survey included two questions regarding the respondents’ knowledge. The
    first question asked them to self-assess their English proficiency at European
    standard language levels. The three options were A1/A2, B1/B2, and C1/C2\. The
    second question asked them to self-assess their cybersecurity knowledge. The four
    options were: 1\. Beginner, I have a basic understanding of cybersecurity concepts;
    2\. Intermediate, I have moderate experience, and I’m familiar with routine cybersecurity
    concepts; 3\. Advanced, I have extensive experience, and I am knowledgeable in
    complex cybersecurity concepts; 4\. Expert, I am highly knowledgeable and recognised
    as an authority in the field of cybersecurity.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 调查包含了两个关于受访者知识的问题。第一个问题要求他们自我评估其英语水平，按照欧洲语言标准。三个选项为 A1/A2、B1/B2 和 C1/C2。第二个问题要求他们自我评估其网络安全知识。四个选项为：1.
    初学者，我对网络安全概念有基本了解；2. 中级，我有一定经验，并熟悉常规网络安全概念；3. 高级，我有丰富经验，并对复杂的网络安全概念有深入了解；4. 专家，我在网络安全领域有高度知识并被公认为权威。
- en: 3.5 Quantitative Evaluation with Metrics
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 量化评估与指标
- en: The methodology for the automatic evaluation consisted of using three well-known
    automatic machine translation methods BLEU, METEOR, TER. These methods take as
    input and compare the ground truth translation and the candidate translation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自动评估的方法包括使用三种著名的自动机器翻译方法 BLEU、METEOR、TER。这些方法将真实翻译与候选翻译进行比较。
- en: BLEU, BiLingual Evaluation Understudy [[26](#bib.bibx26)], assigns a score ranging
    from zero to one, indicating how closely the machine translation aligns with the
    reference translation (1 is better). METEOR, Evaluation of Translation with Explicit
    Ordering [[4](#bib.bibx4)], calculates a score using unigram precision, unigram
    recall, and their combined harmonic F1 score. It puts particular emphasis on accuracy,
    fluency, and word order and scores the similarity ranging from 0 to 1 (1 is better).
    TER, Translation Edit Rate [[32](#bib.bibx32)], estimates the level of editing
    required by a human to align a system output with a reference translation (0 is
    better).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU，双语评估仿真器 [[26](#bib.bibx26)]，给出从零到一的评分，指示机器翻译与参考翻译的对齐程度（1分为更好）。METEOR，显式排序翻译评估 [[4](#bib.bibx4)]，使用单词精度、单词召回率以及它们的综合调和F1分数来计算评分。它特别强调准确性、流畅性和词序，评分范围从0到1（1分为更好）。TER，翻译编辑率 [[32](#bib.bibx32)]，估计对齐系统输出与参考翻译所需的人为编辑水平（0分为更好）。
- en: 3.6 Ethical Considerations
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 伦理考虑
- en: The human evaluation part of this study involved human participants. As such,
    we received approval from our university’s institutional review board (IRB) to
    ensure compliance with ethical guidelines and regulations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的人工评估部分涉及人类参与者。因此，我们获得了我们大学机构审查委员会（IRB）的批准，以确保符合伦理指南和规定。
- en: The survey was conducted using the SurveyMonkey platform. No personally identifiable
    information was collected from the participants. The participants were warned
    beforehand that the messages could contain hate speech, propaganda and other inflammatory
    material and that the participation was entirely voluntary. Participants were
    offered the option to exit the survey at any time.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 调查通过SurveyMonkey平台进行。未收集受访者的个人身份信息。受访者在调查前被警告消息可能包含仇恨言论、宣传和其他煽动性材料，并且参与完全自愿。受访者可以随时选择退出调查。
- en: 4 Experiments and Results
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与结果
- en: The human evaluation included 7 respondents, which produced a total of 103 answers.
    When asked about their level of English proficiency, 100% of the respondents reported
    an advanced level of English (C1/C2). When asked about their level of cybersecurity
    knowledge, 14% reported a basic level, 57% reported an intermediate level, 0%
    reported an advanced level, and 29% reported an expert level.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估包括7名受访者，共产生了103个答案。问及他们的英语水平时，100%的受访者报告了高级英语水平（C1/C2）。问及他们的网络安全知识水平时，14%报告了基础水平，57%报告了中级水平，0%报告了高级水平，29%报告了专家水平。
- en: The results from the survey indicate that participant translators prefer the
    fine-tuned LLM model in 64.08% of the cases. In contrast, the base LLM model without
    fine-tuning was chosen in 35.92% of cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 调查结果显示，参与者翻译者在64.08%的情况下更倾向于使用经过微调的LLM模型。相比之下，未经过微调的基础LLM模型在35.92%的情况下被选择。
- en: 4.1 Statistical Analysis of Survey Results
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 调查结果的统计分析
- en: We employed a Generalized Linear Mixed Model (GLMM) [[19](#bib.bibx19)] to examine
    the effect of different modeling approaches on participants’ preferences. We considered
    LLM model as a fixed effect and both question and participant as random effects.
    We use a logit link function to model the odds of preference as a function of
    the fixed effect (LLM model) and random intercepts for question and participant.
    This setup allows us to account for within-participant and within-paragraph variations
    in preferences, acknowledging that responses might be clustered by these factors.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了广义线性混合模型（GLMM） [[19](#bib.bibx19)]来检验不同建模方法对参与者偏好的影响。我们将LLM模型视为固定效应，同时将问题和参与者视为随机效应。我们使用对数几率链接函数将偏好的几率建模为固定效应（LLM模型）的函数，并对问题和参与者进行随机截距建模。这种设置允许我们考虑参与者内和段落内偏好的变异，承认响应可能会根据这些因素进行聚类。
- en: 'Finally, we applied an ANOVA test using Type III Wald Chi-square Tests to analyse
    the significant differences in the results. The final results are presented in
    Table [I](#S4.T1 "Table I ‣ 4.1 Statistical Analysis of Survey Results ‣ 4 Experiments
    and Results ‣ Towards Better Understanding of Cybercrime: The Role of Fine-Tuned
    LLMs in Translation")'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用了类型III Wald卡方检验进行ANOVA测试，以分析结果中的显著差异。最终结果见表[I](#S4.T1 "表 I ‣ 4.1 调查结果的统计分析
    ‣ 4 实验与结果 ‣ 更好地理解网络犯罪：微调LLM在翻译中的作用")
- en: 'Table I: Analysis of Deviance Table (Type III Wald Chi-square Tests)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：偏差分析表（类型III Wald卡方检验）
- en: '| Term | $\chi^{2}$) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 项 | $\chi^{2}$ |'
- en: '| (Intercept) | 7.9409 | 0.004833 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| (Intercept) | 7.9409 | 0.004833 |'
- en: '| model | 15.8819 | 6.742e-05 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| model | 15.8819 | 6.742e-05 |'
- en: The intercept, representing the log odds of preference being 1 when all predictors
    variables are at their reference levels, is significantly different from zero
    (p-value = 0.004833), suggesting a baseline preference that is distinct from a
    neutral standpoint. Furthermore, the analysis reveals that the effect of the model
    when comparing the reference base LLM to the fine-tuned LLM model —is highly significant
    (p-value = 6.742e-05), demonstrating a strong influence of the translator type
    on the preference outcome. This level of significance underscores the substantial
    impact that different models have on shaping preferences, affirming the importance
    of the model variable in the analysis.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 截距表示在所有预测变量处于其参考水平时，偏好为1的对数几率，与零显著不同（p值 = 0.004833），这表明基线偏好与中立观点不同。此外，分析显示，当将参考基准LLM与微调后的LLM模型进行比较时——效果显著（p值
    = 6.742e-05），显示出翻译器类型对偏好结果有强烈影响。这一显著性水平突显了不同模型在塑造偏好方面的实质性影响，确认了模型变量在分析中的重要性。
- en: 4.2 Quantitative Analysis of Results
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果的定量分析
- en: 'Following the methodology presented in Section [3](#S3 "3 Methodology ‣ Towards
    Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation"),
    we also used algorithms to measure the similarity of the LLM-based translations
    with the ground truth. Table [II](#S4.T2 "Table II ‣ 4.2 Quantitative Analysis
    of Results ‣ 4 Experiments and Results ‣ Towards Better Understanding of Cybercrime:
    The Role of Fine-Tuned LLMs in Translation") shows the scores achieved by the
    gpt-3.5-turbo-0125 (base model) and ft:gpt-3.5-turbo-0125 (fine-tuned) translations.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[3](#S3 "3 Methodology ‣ Towards Better Understanding of Cybercrime: The
    Role of Fine-Tuned LLMs in Translation")节中提出的方法，我们还使用算法来测量LLM翻译与真实值的相似度。表[II](#S4.T2
    "Table II ‣ 4.2 Quantitative Analysis of Results ‣ 4 Experiments and Results ‣
    Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation")显示了gpt-3.5-turbo-0125（基础模型）和ft:gpt-3.5-turbo-0125（微调）翻译所获得的分数。'
- en: 'Table II: Main Metrics Comparison between the base LLM model gpt-3.5-turbo-0125
    and fine-tuned LLM model ft:gpt-3.5-turbo-0125'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：基础LLM模型gpt-3.5-turbo-0125与微调LLM模型ft:gpt-3.5-turbo-0125之间的主要指标比较
- en: '| Metric | Base LLM model | Fine-tuned LLM model |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Metric | 基础LLM模型 | 微调LLM模型 |'
- en: '| gpt-3.5-turbo-0125 | ft:gpt-3.5-turbo-0125 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0125 | ft:gpt-3.5-turbo-0125 |'
- en: '| BLEU | 0.3523 ± 0.0912 | 0.3477 ± 0.0968 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | 0.3523 ± 0.0912 | 0.3477 ± 0.0968 |'
- en: '| METEOR | 0.6914 ± 0.0583 | 0.7119 ± 0.0833 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| METEOR | 0.6914 ± 0.0583 | 0.7119 ± 0.0833 |'
- en: '| TER | 46.6983 ± 9.5051 | 47.7292 ± 10.0451 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| TER | 46.6983 ± 9.5051 | 47.7292 ± 10.0451 |'
- en: 5 Analysis of Both Evaluations
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 两种评估的分析
- en: Regarding the analysis of results from the humans and algorithms, results suggest
    that the fine-tuned model may produce, in general, better translations than the
    base model. However, these evaluations do not explain the translations’ difficulties,
    subtleties, and intricacies.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人类和算法结果的分析，结果表明，微调模型通常可能产生比基础模型更好的翻译。然而，这些评估并未解释翻译的困难、细微差别和复杂性。
- en: When analysing the human evaluation, we found that it was not an easy task for
    them. For example, feedback from respondents was that the two translation options
    were very similar, and it was hard to spot the differences. Respondents also mentioned
    they were “irritated” and “triggered” by the messages they were asked to review,
    that they had to “take breaks”, and that they “would not imagine spending more
    time doing this work”. This is corroborated by the fact that only 42% of the respondents
    were able to complete the survey in its totality. At the same time, the other
    58% spent an average of 30 minutes before exiting the survey. This further highlights
    the importance of having automated tools that are not affected by such emotions,
    take no sides, and do not get tired.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析人类评估时，我们发现这对他们来说并非易事。例如，受访者反馈两个翻译选项非常相似，很难找出差异。受访者还提到他们对被要求审阅的消息感到“烦恼”和“刺激”，他们必须“休息”，并且“无法想象花更多时间做这项工作”。这一点得到了验证，因为只有42%的受访者能够完成整个调查。同时，其他58%的受访者在退出调查前平均花费了30分钟。这进一步突显了自动化工具的重要性，这些工具不会受到情感影响，不偏不倚，也不会感到疲倦。
- en: 'The quantitative metrics used to measure the distance between the translations
    produced by the fine-tuned model and the base model, when compared to the ground
    truth translation, slightly favour the base model. METEOR was the only metric
    found to prefer the fine-tuned model. A case-by-case analysis of these results
    shows that these metrics are not representative of the quality of the translations
    and can be misleading. In Figure [4](#S5.F4 "Figure 4 ‣ 5 Analysis of Both Evaluations
    ‣ Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation"),
    we present a case where the three metrics chose the base model translation over
    the fine-tuned model translation. The message ground truth, composed of two paragraphs,
    is shown alongside their translations generated by both the base model and the
    fine-tuned model. A human can clearly see how the base model translation is incorrect
    by using the word attached, and the fine-tuned model is able to understand better
    the context and understand the message is referring to something being attacked.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '用于衡量微调模型和基础模型生成的翻译与真实翻译之间距离的定量指标，在对比真实翻译时，稍微倾向于基础模型。METEOR 是唯一一个偏向微调模型的指标。对这些结果逐一分析表明，这些指标不能代表翻译质量，可能会产生误导。在图
    [4](#S5.F4 "Figure 4 ‣ 5 Analysis of Both Evaluations ‣ Towards Better Understanding
    of Cybercrime: The Role of Fine-Tuned LLMs in Translation") 中，我们展示了一个例子，其中三个指标选择了基础模型的翻译而非微调模型的翻译。真实消息由两个段落组成，展示了基础模型和微调模型生成的翻译。通过使用单词“attached”，人类可以清楚地看到基础模型翻译的错误，而微调模型能够更好地理解上下文，明白消息指的是某些东西被攻击。'
- en: '![Refer to caption](img/a0989820e29c9a144804cbf69dad1011.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a0989820e29c9a144804cbf69dad1011.png)'
- en: 'Figure 4: A hacktivist message ground truth (black, top), alongside the translations
    by the base LLM model (green, middle) and fine-tuned model (blue, bottom). The
    three metrics chose the base-model translation when, as can be seen, the best
    translation is generated by the fine-tuned model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个黑色的黑客活动消息的真实情况（顶部），以及基础 LLM 模型（绿色，中间）和微调模型（蓝色，底部）的翻译。正如所见，三个指标在翻译时选择了基础模型，而最佳翻译由微调模型生成。
- en: Traditional MT, such as DeepL and Google Translate, have a tendency to translate
    the given text literally, word for word. In contrast, LLMs are able to interpret
    the context and extrapolate missing words to generate a better translation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器翻译（MT），如 DeepL 和 Google Translate，倾向于逐字逐句地翻译给定的文本。相比之下，LLMs 能够解读上下文并推测缺失的词汇，从而生成更好的翻译。
- en: Ranade et al. [[29](#bib.bibx29)] used LSTM-based neural machine translation
    to translate text extracted from social media matching specific cybersecurity
    terminology from Russian to English. Their method achieved a BLEU score of 28.4\.
    The authors compare the system to Google Translate, but BLEU score metrics are
    not provided. Our method shows a higher score.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Ranade 等人 [[29](#bib.bibx29)] 使用基于 LSTM 的神经机器翻译将从社交媒体中提取的符合特定网络安全术语的文本从俄语翻译成英语。他们的方法获得了
    28.4 的 BLEU 分数。作者将该系统与 Google Translate 进行了比较，但未提供 BLEU 分数指标。我们的方法显示了更高的得分。
- en: Our cost analysis shows that the proposed method is 430 to 23000 times cheaper
    than a human translator, depending on the cost of the translation service. Translations
    made by a native Russian cybersecurity analyst are estimated to cost 0.21$ per
    message, in contrast to specialised services, which may cost up to 0.21$ per word
    but produce very high fidelity and accurate translations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的成本分析显示，所提方法比人工翻译便宜 430 到 23000 倍，具体取决于翻译服务的成本。由俄语网络安全分析师翻译的消息估计每条花费 0.21
    美元，而专业服务可能每个词花费高达 0.21 美元，但能产生非常高的保真度和准确的翻译。
- en: 5.1 Qualitative Analysis of Translation Errors
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 翻译错误的定性分析
- en: The evaluation of messages is a very difficult task. Often, translations may
    contain tiny differences that can have a significant impact on the overall meaning.
    A wrong character in a URL can render it unusable and hamper intelligence collection.
    The use of LLMs in translation can help correct small mistakes, although at a
    high cost. In this subsection, we analyse some common errors where the use of
    LLMs shows improved performance over traditional MT methods.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 消息的评估是一个非常困难的任务。翻译中常常包含微小的差异，这些差异可能对整体意义产生重大影响。URL 中的一个错误字符可能使其无法使用，并阻碍情报收集。使用
    LLMs 进行翻译可以帮助纠正小错误，尽管代价较高。在本小节中，我们分析了一些常见错误，其中使用 LLMs 相较于传统的 MT 方法表现出更好的性能。
- en: Wrong Handling of URLs. A critical error when using MT is the modification of
    URLs, where the edition of a single character can render the URL useless. This
    error is common when using Google Translate, which often changes the URLs. For
    example, We-are-not-alone.ru translated to We-Ra-not-alone.ru, strana.today to
    Strana.tude, and espreso.tv to Espresso.TV. The base LLM model and the fine-tuned
    model were both able to respect the instructions and did not modify the URLs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 错误处理网址。使用机器翻译时的一个关键错误是修改网址，其中一个字符的编辑会使网址失效。这种错误在使用谷歌翻译时很常见，谷歌翻译常常更改网址。例如，We-are-not-alone.ru
    翻译为 We-Ra-not-alone.ru，strana.today 翻译为 Strana.tude，espreso.tv 翻译为 Espresso.TV。基础LLM模型和微调模型都能够尊重指示，没有修改网址。
- en: Wrong Handling of Emoji. Another error when using MT is the difficulty in handling
    special characters, in particular emojis. The use of emoji is prevalent in hacktivist
    messages. Google Translate tends to remove the emoji completely from the message.
    Open-source LLM models often replace the emoji with text interpretations, with
    different emoji, or with a combination of emojis. DeepL performed well, sometimes
    duplicating emojis. The base LLM model and the fine-tuned model were both able
    to respect the instructions and did not modify them.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 错误处理表情符号。使用机器翻译时的另一个错误是处理特殊字符，特别是表情符号的困难。表情符号在黑客活动消息中很常见。谷歌翻译往往会完全移除消息中的表情符号。开源的LLM模型通常会用文本解释、不同的表情符号或表情符号的组合来替代表情符号。DeepL表现良好，有时会重复表情符号。基础LLM模型和微调模型都能够尊重指示，没有对表情符号进行修改。
- en: 'Missed puns, humour, and play of words. Another limitation of MT is their inability
    to understand humour, puns, jokes and plays on words. One salient example is a
    pun the hacktivist group did on an attack on the news site “Segodnya”. In Russian,
    “Segodnya” means “today”. The original pun used was “Каламбур: сегодня у «Сегодня»
    не задалось.”, which correctly interpreted means “A pun: Today did not go well
    for “Segodnya” [today]”. Google Translate fails to capture the meaning with its
    translation “Kalamble: Today, ”today” did not set.”. Open-source LLM models and
    DeepL failed to capture the pun as well. The base LLM model and the fine-tuned
    model were both able to capture the pun correctly.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '漏掉双关语、幽默和文字游戏。机器翻译的另一个限制是无法理解幽默、双关语、笑话和文字游戏。一个突出的例子是黑客组织对新闻网站“Segodnya”进行的攻击中的双关语。在俄语中，“Segodnya”意为“今天”。原始的双关语是“Каламбур:
    сегодня у «Сегодня» не задалось.”，正确的解释是“一个双关语：今天对‘Segodnya’（今天）不太顺利”。谷歌翻译未能捕捉到其意义，翻译为“Kalamble:
    Today, ‘today’ did not set.”。开源的LLM模型和DeepL也未能捕捉到这个双关语。基础LLM模型和微调模型都能够正确捕捉到双关语。'
- en: Poor translation of jargon. The correct translation of jargon is a known issue
    of MT methods. Google Translate performs poorly in this area. For example, “Толстосумы”
    is translated as “heaps of dough”, but it should be “Moneybags”; “DDOS-атаки”
    is translated to “DDOS adjectives”, but it should be “DDoS-attacks”; and “айтишник”
    is translated to “ITISHNIK” but it should be “person who works in IT”. Open-source
    LLM models, DeepL, and the base model all fail to capture these to some degree.
    The fine-tuned model performs better when taught, which is one of the clear advantages
    in this context.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 行话翻译差。行话的正确翻译是机器翻译方法中的已知问题。谷歌翻译在这方面表现不佳。例如，“Толстосумы”被翻译为“heaps of dough”，但应为“Moneybags”；“DDOS-атаки”被翻译为“DDOS
    adjectives”，但应为“DDoS-attacks”；而“айтишник”被翻译为“ITISHNIK”，但应为“person who works in
    IT”。开源LLM模型、DeepL和基础模型在某种程度上都未能捕捉到这些。微调模型在教学后表现更好，这是在这种情况下的一个明显优势。
- en: Wrong translations of words. Other errors of mistranslation of words or expressions
    are also common. Google Translate fails to translate some common expressions,
    often translating word-for-word and losing context. For example, “Недо-хакеры”
    is translated as “non-chairs”, but it should be “wannabe hackers”; “А нас за шо?”
    is translated to “And we are for sho?”, but it should be “Why us?”, and “Ахи-вздохи”
    is translated as “Ahi-sizdokhs”, but it should be “signs and moans”. Open-source
    models perform better depending on the context. The fine-tuned model performs
    slightly better as it seems to contextualise the words better.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 单词翻译错误。其他翻译单词或表达的错误也很常见。谷歌翻译未能翻译一些常见表达，往往逐字翻译，导致丢失上下文。例如，“Недо-хакеры”被翻译为“non-chairs”，但应为“wannabe
    hackers”；“А нас за шо?”被翻译为“And we are for sho?”，但应为“Why us?”，而“Ахи-вздохи”被翻译为“Ahi-sizdokhs”，但应为“signs
    and moans”。开源模型的表现取决于上下文。微调模型的表现略好，因为它似乎能够更好地上下文化单词。
- en: 6 Conclusion
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Our research shows that with small ground truth data, it is possible to fine-tune
    an LLM model that will produce better translations of cyber-hacktivist groups
    than those of traditional machine translation methods. Translations are faster,
    more accurate, and are not susceptible to toxic or inflammable content. With proper
    training, LLM models can be taught the nuances of the language and jargon much
    more accurately than other MT methods. Furthermore, we showed how fine-tuned LLM
    models can help produce translations significantly cheaper than human translators,
    making translation scalability primarily dependent on money. Biases are still
    present, mainly when working with close source models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，使用少量真实数据，可以微调一个 LLM 模型，使其比传统的机器翻译方法产生更好的网络黑客组织翻译。翻译更快、更准确，且不易受有毒或易燃内容的影响。通过适当的训练，LLM
    模型可以比其他机器翻译方法更准确地学习语言和行话的细微差别。此外，我们展示了如何使微调的 LLM 模型帮助产生比人工翻译便宜得多的翻译，使翻译的可扩展性主要取决于资金。偏见仍然存在，尤其是在处理封闭源模型时。
- en: The use of a fine-tuned LLM model shows clear benefits, including respect for
    the punctuation, the emojis, the URLs, using appropriate formality levels in the
    language that is closer to the original, and understanding word plays and humour
    from the original text. This creates the opportunity for a more rich and in-depth
    analysis of chats and messages.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微调的 LLM 模型显示出明显的好处，包括尊重标点符号、表情符号、网址，使用更接近原始内容的适当正式度，以及理解原文中的文字游戏和幽默。这为对聊天和消息进行更丰富、更深入的分析创造了机会。
- en: Using closed platforms such as OpenAI has its challenges. On more than one occasion,
    the translation of messages or fine-tuning tasks were automatically cancelled
    due to violations of the terms and conditions. We discovered the messages were
    flagged by OpenAI as containing hate speech. The inability to do research should
    not be constrained by restrictions from third-party private companies. This is
    why one of the main areas we aim to move forward is reproducing these results
    using open models. Furthermore, fine-tuned models through OpenAI cannot be publicly
    shared, restricting researchers from sharing models and furthering the discussion
    and collaboration.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 OpenAI 这样的封闭平台有其挑战。在多次情况下，由于违反条款和条件，消息翻译或微调任务被自动取消。我们发现这些消息被 OpenAI 标记为包含仇恨言论。研究的能力不应受到第三方私人公司的限制。这就是为什么我们主要的目标之一是使用开放模型重现这些结果。此外，通过
    OpenAI 微调的模型无法公开分享，限制了研究人员分享模型并进一步讨论与合作。
- en: Our future work includes expanding this work to fine-tune open models to match
    current performance levels, cut costs, and share fine-tuned models openly with
    the community. Future work also includes the use of this output in a pipeline
    of intelligence analysis. If messages can be accurately translated, they can be
    further studied in real time to produce timely intelligence and analysis that
    can be used for defence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们未来的工作包括扩展此工作以微调开放模型，以匹配当前的性能水平，降低成本，并与社区公开分享微调模型。未来的工作还包括在情报分析流程中使用这些输出。如果消息能够被准确翻译，它们可以实时进一步研究，以产生及时的情报和分析，可用于防御。
- en: Acknowledgement
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Acknowledgements are anonymized for blind review.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢被匿名化以进行盲审。
- en: References
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Anonymized Repository “hermeneisGPT” URL: [https://anonymous.4open.science/r/hermeneisGPT-8EDA/README.md](https://anonymous.4open.science/r/hermeneisGPT-8EDA/README.md)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 匿名仓库 “hermeneisGPT” URL: [https://anonymous.4open.science/r/hermeneisGPT-8EDA/README.md](https://anonymous.4open.science/r/hermeneisGPT-8EDA/README.md)'
- en: '[2] Anonymized Repository “Spylegram” URL: [https://anonymous.4open.science/r/Spylegram-47B4/README.md](https://anonymous.4open.science/r/Spylegram-47B4/README.md)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 匿名仓库 “Spylegram” URL: [https://anonymous.4open.science/r/Spylegram-47B4/README.md](https://anonymous.4open.science/r/Spylegram-47B4/README.md)'
- en: '[3] Arnav Arora et al. “Detecting Harmful Content on Online Platforms: What
    Platforms Need vs. Where Research Efforts Go” In *ACM Computing Surveys* 56.3,
    2024, pp. 1–17 DOI: [10.1145/3603399](https://dx.doi.org/10.1145/3603399)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Arnav Arora 等人。“检测在线平台上的有害内容：平台需要什么与研究努力的去向”《ACM Computing Surveys》 56.3,
    2024, 第 1–17 页 DOI: [10.1145/3603399](https://dx.doi.org/10.1145/3603399)'
- en: '[4] Satanjeev Banerjee and Alon Lavie “METEOR: An Automatic Metric for MT Evaluation
    with Improved Correlation with Human Judgments” In *Proceedings of the ACL Workshop
    on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or
    Summarization* Ann Arbor, Michigan: Association for Computational Linguistics,
    2005, pp. 65–72 URL: [https://aclanthology.org/W05-0909](https://aclanthology.org/W05-0909)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Satanjeev Banerjee 和 Alon Lavie “METEOR：一种与人工判断相关性改进的自动化机器翻译评估指标” 见 *Proceedings
    of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine
    Translation and/or Summarization* 密歇根州安娜堡：计算语言学协会, 2005, 第65–72页 URL: [https://aclanthology.org/W05-0909](https://aclanthology.org/W05-0909)'
- en: '[5] Shihua Brazill “ANALYSIS OF HUMAN VERSUS MACHINE TRANSLATION ACCURACY”
    Number: 1-4 December In *Intermountain Journal of Sciences* 21.1-4 December, 2015,
    pp. 106–107 URL: [https://arc.lib.montana.edu/ojs/index.php/IJS/article/view/993](https://arc.lib.montana.edu/ojs/index.php/IJS/article/view/993)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Shihua Brazill “人类与机器翻译准确性的分析” 期号：1-4 12月 见 *Intermountain Journal of Sciences*
    21.1-4 12月, 2015, 第106–107页 URL: [https://arc.lib.montana.edu/ojs/index.php/IJS/article/view/993](https://arc.lib.montana.edu/ojs/index.php/IJS/article/view/993)'
- en: '[6] CyberPeace Institute “Timeline of Cyberattacks and Operations — CyberPeace
    Institute” In *Timeline of Cyberattacks and Operations*, 2024 URL: [https://cyberconflicts.cyberpeaceinstitute.org/threats/timeline](https://cyberconflicts.cyberpeaceinstitute.org/threats/timeline)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] CyberPeace Institute “网络攻击和行动时间线 — CyberPeace Institute” 见 *网络攻击和行动时间线*,
    2024 URL: [https://cyberconflicts.cyberpeaceinstitute.org/threats/timeline](https://cyberconflicts.cyberpeaceinstitute.org/threats/timeline)'
- en: '[7] DeepL SE “DeepL Translate: The world’s most accurate translator” DeepL
    SE, 2024 URL: [https://www.deepl.com/translator](https://www.deepl.com/translator)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] DeepL SE “DeepL 翻译：全球最准确的翻译器” DeepL SE, 2024 URL: [https://www.deepl.com/translator](https://www.deepl.com/translator)'
- en: '[8] Stéphane Duguin and Pavlina Pavlova “The role of cyber in the Russian war
    against Ukraine: Its impact and the consequences for the future of armed conflict”'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Stéphane Duguin 和 Pavlina Pavlova “网络在俄罗斯对乌克兰战争中的作用：其影响及未来武装冲突的后果”'
- en: '[9] Mohammadreza Ebrahimi, Sagar Samtani, Yidong Chai and Hsinchun Chen “Detecting
    Cyber Threats in Non-English Hacker Forums: An Adversarial Cross-Lingual Knowledge
    Transfer Approach” In *2020 IEEE Security and Privacy Workshops (SPW)*, 2020,
    pp. 20–26 DOI: [10.1109/SPW50608.2020.00021](https://dx.doi.org/10.1109/SPW50608.2020.00021)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mohammadreza Ebrahimi, Sagar Samtani, Yidong Chai 和 Hsinchun Chen “在非英语黑客论坛中检测网络威胁：一种对抗性跨语言知识转移方法”
    见 *2020 IEEE Security and Privacy Workshops (SPW)*, 2020, 第20–26页 DOI: [10.1109/SPW50608.2020.00021](https://dx.doi.org/10.1109/SPW50608.2020.00021)'
- en: '[10] Google LLC “Google Translate” Google LLC, 2024 URL: [https://translate.google.com/](https://translate.google.com/)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Google LLC “Google 翻译” Google LLC, 2024 URL: [https://translate.google.com/](https://translate.google.com/)'
- en: '[11] Insikt Group® “Dark Covenant 2.0: Cybercrime, the Russian State, and War
    in Ukraine — Recored Future” In *Dark Covenant 2.0: Cybercrime, the Russian State,
    and the War in Ukraine*, 2023 URL: [https://www.recordedfuture.com/dark-covenant-2-cybercrime-russian-state-war-ukraine](https://www.recordedfuture.com/dark-covenant-2-cybercrime-russian-state-war-ukraine)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Insikt Group® “黑暗契约 2.0：网络犯罪、俄罗斯国家与乌克兰战争——Recorded Future” 见 *黑暗契约 2.0：网络犯罪、俄罗斯国家与乌克兰战争*,
    2023 URL: [https://www.recordedfuture.com/dark-covenant-2-cybercrime-russian-state-war-ukraine](https://www.recordedfuture.com/dark-covenant-2-cybercrime-russian-state-war-ukraine)'
- en: '[12] Intel(R) Neural Compressor “Supervised Fine-Tuning and Direct Preference
    Optimization on Intel Gaudi2” In *Intel Analytics Software*, 2023 URL: [https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Intel(R) Neural Compressor “在 *Intel Analytics Software* 上的“有监督微调和直接偏好优化””
    2023 URL: [https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)'
- en: '[13] Albert Q. Jiang et al. “Mistral 7B” arXiv:2310.06825 [cs] arXiv, 2023
    DOI: [10.48550/arXiv.2310.06825](https://dx.doi.org/10.48550/arXiv.2310.06825)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Albert Q. Jiang 等 “Mistral 7B” arXiv:2310.06825 [cs] arXiv, 2023 DOI:
    [10.48550/arXiv.2310.06825](https://dx.doi.org/10.48550/arXiv.2310.06825)'
- en: '[14] Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang and Xing Wang “Is ChatGPT A
    Good Translator? A Preliminary Study”, 2023 URL: [https://www.researchgate.net/publication/367359399_Is_ChatGPT_A_Good_Translator_A_Preliminary_Study](https://www.researchgate.net/publication/367359399_Is_ChatGPT_A_Good_Translator_A_Preliminary_Study)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang 和 Xing Wang “ChatGPT 是一个好的翻译器吗？初步研究”，2023
    网址: [https://www.researchgate.net/publication/367359399_Is_ChatGPT_A_Good_Translator_A_Preliminary_Study](https://www.researchgate.net/publication/367359399_Is_ChatGPT_A_Good_Translator_A_Preliminary_Study)'
- en: '[15] KELA Cybercrime Intelligence Center “Beyond Donations: How Hacktivist
    Groups Fund Their Operations”, pp. 37 URL: [https://www.kelacyber.com/wp-content/uploads/2023/08/Research-by-KELA_How-Hacktivist-Groups-Fund-Their-Operations.pdf](https://www.kelacyber.com/wp-content/uploads/2023/08/Research-by-KELA_How-Hacktivist-Groups-Fund-Their-Operations.pdf)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] KELA 网络犯罪情报中心 “超越捐款：黑客组织如何资助其活动”，第 37 页 网址: [https://www.kelacyber.com/wp-content/uploads/2023/08/Research-by-KELA_How-Hacktivist-Groups-Fund-Their-Operations.pdf](https://www.kelacyber.com/wp-content/uploads/2023/08/Research-by-KELA_How-Hacktivist-Groups-Fund-Their-Operations.pdf)'
- en: '[16] LonamiWebs “Telethon” In *GitHub*, 2023 URL: [https://github.com/LonamiWebs/Telethon](https://github.com/LonamiWebs/Telethon)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] LonamiWebs “Telethon” 载于 *GitHub*, 2023 网址: [https://github.com/LonamiWebs/Telethon](https://github.com/LonamiWebs/Telethon)'
- en: '[17] Shushen Manakhimova et al. “Linguistically Motivated Evaluation of the
    2023 State-of-the-art Machine Translation: Can ChatGPT Outperform NMT?” In *Proceedings
    of the Eighth Conference on Machine Translation* Singapore: Association for Computational
    Linguistics, 2023, pp. 224–245 DOI: [10.18653/v1/2023.wmt-1.23](https://dx.doi.org/10.18653/v1/2023.wmt-1.23)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Shushen Manakhimova 等 “对 2023 年最新机器翻译的语言学驱动评估：ChatGPT 能否超越 NMT？” 载于 *第八届机器翻译会议论文集*
    新加坡: 计算语言学协会, 2023, 第 224–245 页 DOI: [10.18653/v1/2023.wmt-1.23](https://dx.doi.org/10.18653/v1/2023.wmt-1.23)'
- en: '[18] Dalyapraz Manatova et al. “An Argument for Linguistic Expertise in Cyberthreat
    Analysis: LOLSec in Russian Language eCrime Landscape” In *2023 IEEE European
    Symposium on Security and Privacy Workshops (EuroS&PW)* Delft, Netherlands: IEEE,
    2023, pp. 170–176 DOI: [10.1109/EuroSPW59978.2023.00024](https://dx.doi.org/10.1109/EuroSPW59978.2023.00024)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Dalyapraz Manatova 等 “对网络威胁分析中的语言学专业知识的论证：LOLSec 在俄语语言电子犯罪领域” 载于 *2023
    IEEE 欧洲安全与隐私研讨会 (EuroS&PW)* 荷兰代尔夫特: IEEE, 2023, 第 170–176 页 DOI: [10.1109/EuroSPW59978.2023.00024](https://dx.doi.org/10.1109/EuroSPW59978.2023.00024)'
- en: '[19] Charles E. Mcculloch and John M. Neuhaus “Generalized Linear Mixed Models”
    In *Wiley StatsRef: Statistics Reference Online* John Wiley & Sons, Ltd, 2014
    URL: [https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07540](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07540)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Charles E. Mcculloch 和 John M. Neuhaus “广义线性混合模型” 载于 *Wiley StatsRef:
    Statistics Reference Online* John Wiley & Sons, Ltd, 2014 网址: [https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07540](https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07540)'
- en: '[20] Paul Michel and Graham Neubig “MTNT: A Testbed for Machine Translation
    of Noisy Text” arXiv:1809.00388 [cs] arXiv, 2018 URL: [http://arxiv.org/abs/1809.00388](http://arxiv.org/abs/1809.00388)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Paul Michel 和 Graham Neubig “MTNT: 嘈杂文本机器翻译测试平台” arXiv:1809.00388 [cs]
    arXiv, 2018 网址: [http://arxiv.org/abs/1809.00388](http://arxiv.org/abs/1809.00388)'
- en: '[21] Alexandr Nikolich and Arina Puchkova “Fine-tuning GPT-3 for Russian Text
    Summarization” In *ArXiv*, 2021 URL: [https://www.semanticscholar.org/paper/eb18be41441260c40cdee36f17fc7ad48f426c5f](https://www.semanticscholar.org/paper/eb18be41441260c40cdee36f17fc7ad48f426c5f)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Alexandr Nikolich 和 Arina Puchkova “对 GPT-3 进行俄语文本摘要的微调” 载于 *ArXiv*, 2021
    网址: [https://www.semanticscholar.org/paper/eb18be41441260c40cdee36f17fc7ad48f426c5f](https://www.semanticscholar.org/paper/eb18be41441260c40cdee36f17fc7ad48f426c5f)'
- en: '[22] NoName057(16) “NoName057(16) Telegram Channel” In *Telegram*, 2024 URL:
    [https://t.me/s/noname05716](https://t.me/s/noname05716)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] NoName057(16) “NoName057(16) Telegram 频道” 载于 *Telegram*, 2024 网址: [https://t.me/s/noname05716](https://t.me/s/noname05716)'
- en: '[23] Ofer Tirosh “What is the average translation speed?” In *Tomedes*, 2016
    URL: [https://www.tomedes.com/translator-hub/what-average-translation-speed.php](https://www.tomedes.com/translator-hub/what-average-translation-speed.php)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Ofer Tirosh “什么是平均翻译速度？” 载于 *Tomedes*, 2016 网址: [https://www.tomedes.com/translator-hub/what-average-translation-speed.php](https://www.tomedes.com/translator-hub/what-average-translation-speed.php)'
- en: '[24] Open AI “ChatGPT [GPT-4]” URL: [https://chat.openai.com](https://chat.openai.com)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Open AI “ChatGPT [GPT-4]” 网址: [https://chat.openai.com](https://chat.openai.com)'
- en: '[25] “OpenAI GPT-3 API [gpt-3.5-turbo-0125]”, 2024 URL: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] “OpenAI GPT-3 API [gpt-3.5-turbo-0125]”，2024 网址: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
- en: '[26] Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu “Bleu: a Method
    for Automatic Evaluation of Machine Translation” In *Proceedings of the 40th Annual
    Meeting of the Association for Computational Linguistics* Philadelphia, Pennsylvania,
    USA: Association for Computational Linguistics, 2002, pp. 311–318 DOI: [10.3115/1073083.1073135](https://dx.doi.org/10.3115/1073083.1073135)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing Zhu “Bleu：一种自动评估机器翻译的方法”
    收录于 *第40届计算语言学协会年会论文集* 美国宾夕法尼亚州费城：计算语言学协会, 2002，第311–318页 DOI: [10.3115/1073083.1073135](https://dx.doi.org/10.3115/1073083.1073135)'
- en: '[27] Keqin Peng et al. “Towards Making the Most of ChatGPT for Machine Translation”
    arXiv:2303.13780 [cs] arXiv, 2023 DOI: [10.48550/arXiv.2303.13780](https://dx.doi.org/10.48550/arXiv.2303.13780)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Keqin Peng 等 “充分利用 ChatGPT 进行机器翻译” arXiv:2303.13780 [cs] arXiv, 2023 DOI:
    [10.48550/arXiv.2303.13780](https://dx.doi.org/10.48550/arXiv.2303.13780)'
- en: '[28] Ming Qian “Performance Evaluation on Human-Machine Teaming Augmented Machine
    Translation Enabled by GPT-4” In *Proceedings of the First Workshop on NLP Tools
    and Resources for Translation and Interpreting Applications* Varna, Bulgaria:
    INCOMA Ltd., Shoumen, Bulgaria, 2023, pp. 20–31 URL: [https://aclanthology.org/2023.nlp4tia-1.4](https://aclanthology.org/2023.nlp4tia-1.4)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Ming Qian “基于 GPT-4 的人机团队增强机器翻译性能评估” 收录于 *第一届自然语言处理工具与资源翻译与口译应用研讨会论文集*
    保加利亚瓦尔纳：INCOMA Ltd., Shoumen, Bulgaria, 2023，第20–31页 URL: [https://aclanthology.org/2023.nlp4tia-1.4](https://aclanthology.org/2023.nlp4tia-1.4)'
- en: '[29] Priyanka Ranade, Sudip Mittal, Anupam Joshi and Karuna Joshi “Using Deep
    Neural Networks to Translate Multi-lingual Threat Intelligence” In *2018 IEEE
    International Conference on Intelligence and Security Informatics (ISI)*, 2018,
    pp. 238–243 DOI: [10.1109/ISI.2018.8587374](https://dx.doi.org/10.1109/ISI.2018.8587374)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Priyanka Ranade, Sudip Mittal, Anupam Joshi 和 Karuna Joshi “使用深度神经网络翻译多语言威胁情报”
    收录于 *2018 IEEE 国际情报与安全信息学会议 (ISI)*, 2018，第238–243页 DOI: [10.1109/ISI.2018.8587374](https://dx.doi.org/10.1109/ISI.2018.8587374)'
- en: '[30] Dominic Seyler, Wei Liu, XiaoFeng Wang and ChengXiang Zhai “Towards Dark
    Jargon Interpretation in Underground Forums” arXiv:2011.03011 [cs] arXiv, 2021
    URL: [http://arxiv.org/abs/2011.03011](http://arxiv.org/abs/2011.03011)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Dominic Seyler, Wei Liu, XiaoFeng Wang 和 ChengXiang Zhai “朝向地下论坛中的黑暗术语解释”
    arXiv:2011.03011 [cs] arXiv, 2021 URL: [http://arxiv.org/abs/2011.03011](http://arxiv.org/abs/2011.03011)'
- en: '[31] Sai Cheong Siu “ChatGPT and GPT-4 for Professional Translators: Exploring
    the Potential of Large Language Models in Translation”, 2023 DOI: [10.2139/ssrn.4448091](https://dx.doi.org/10.2139/ssrn.4448091)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sai Cheong Siu “ChatGPT 和 GPT-4 对于专业翻译人员的应用：探索大型语言模型在翻译中的潜力”，2023 DOI:
    [10.2139/ssrn.4448091](https://dx.doi.org/10.2139/ssrn.4448091)'
- en: '[32] Matthew Snover et al. “A Study of Translation Edit Rate with Targeted
    Human Annotation” In *Proceedings of the 7th Conference of the Association for
    Machine Translation in the Americas: Technical Papers* Cambridge, Massachusetts,
    USA: Association for Machine Translation in the Americas, 2006, pp. 223–231 URL:
    [https://aclanthology.org/2006.amta-papers.25](https://aclanthology.org/2006.amta-papers.25)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Matthew Snover 等 “带目标的人工注释翻译编辑率研究” 收录于 *第七届美洲机器翻译协会会议：技术论文集* 美国麻省剑桥：美洲机器翻译协会,
    2006，第223–231页 URL: [https://aclanthology.org/2006.amta-papers.25](https://aclanthology.org/2006.amta-papers.25)'
- en: '[33] Lewis Tunstall et al. “Zephyr: Direct Distillation of LM Alignment” arXiv:2310.16944
    [cs] arXiv, 2023 DOI: [10.48550/arXiv.2310.16944](https://dx.doi.org/10.48550/arXiv.2310.16944)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Lewis Tunstall 等 “Zephyr：语言模型对齐的直接蒸馏” arXiv:2310.16944 [cs] arXiv, 2023
    DOI: [10.48550/arXiv.2310.16944](https://dx.doi.org/10.48550/arXiv.2310.16944)'
- en: '[34] Galiya Ybytayeva et al. “Creating a Thesaurus ”Crime-Related Web Content”
    Based on a Multilingual Corpus” CEUR-WS, 2023, pp. 77–87 URL: [https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-209572](https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-209572)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Galiya Ybytayeva 等 “基于多语言语料库创建‘犯罪相关网络内容’词汇表” CEUR-WS, 2023，第77–87页 URL:
    [https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-209572](https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-209572)'
- en: '[35] Wenhao Zhu et al. “Multilingual Machine Translation with Large Language
    Models: Empirical Results and Analysis” arXiv:2304.04675 [cs] arXiv, 2023 DOI:
    [10.48550/arXiv.2304.04675](https://dx.doi.org/10.48550/arXiv.2304.04675)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Wenhao Zhu 等 “使用大型语言模型进行多语言机器翻译：实证结果与分析” arXiv:2304.04675 [cs] arXiv,
    2023 DOI: [10.48550/arXiv.2304.04675](https://dx.doi.org/10.48550/arXiv.2304.04675)'
- en: Appendix I Fine-Tuning Prompt
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 微调提示
- en: [PRE0]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: [PRE0]
