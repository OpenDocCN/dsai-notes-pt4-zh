- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EBFT: 高效的块级微调用于稀疏LLMs'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12419](https://ar5iv.labs.arxiv.org/html/2402.12419)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12419](https://ar5iv.labs.arxiv.org/html/2402.12419)
- en: 'Song Guo¹²²footnotemark: 2, Fan Wu¹²²footnotemark: 2, Lei Zhang¹, Xiawu Zheng¹,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 宋果¹²²脚注标记：2，范武¹²²脚注标记：2，张磊¹，郑夏吴¹，
- en: Shengchuan Zhang¹, Fei Chao¹, Yiyu Shi², Rongrong Ji¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张盛川¹，赵飞¹，史奕宇²，季蓉蓉¹
- en: ¹MAC Lab, Xiamen University. ²University of Notre Dame.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹厦门大学MAC实验室。²圣母大学。
- en: '{guosong, wfanstory, leizhang}@stu.xmu.edu.cn, {zhengxiawu, fchao, zsc_2016,
    rrji}@xmu.edu.cn'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{guosong, wfanstory, leizhang}@stu.xmu.edu.cn，{zhengxiawu, fchao, zsc_2016,
    rrji}@xmu.edu.cn'
- en: yshi4@nd.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: yshi4@nd.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive
    requirements and high retraining costs. Additionally, many fine-tuning methods
    often rely on approximations or heuristic optimization strategies, which may lead
    to suboptimal solutions. To address these issues, we propose an efficient and
    fast framework for fine-tuning sparse LLMs based on minimizing reconstruction
    error. Our approach involves sampling a small dataset for calibration and utilizing
    backpropagation to iteratively optimize block-wise reconstruction error, on a
    block-by-block basis, aiming for optimal solutions. Extensive experiments on various
    benchmarks consistently demonstrate the superiority of our method over other baselines.
    For instance, on the Wikitext2 dataset with LlamaV1-7B at $70\%$, EBFT achieves
    a perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the
    fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,
    and the entire framework can be executed on a single 16GB GPU. The source code
    is available at [https://github.com/sunggo/EBFT](https://github.com/sunggo/EBFT).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的稀疏LLMs微调方法通常面临资源密集型要求和高额的重新训练成本。此外，许多微调方法往往依赖于近似或启发式优化策略，这可能导致次优解。为了解决这些问题，我们提出了一种基于最小化重建误差的高效快速微调框架。我们的方法涉及对小型数据集进行采样以进行校准，并利用反向传播迭代优化块级重建误差，逐块优化，旨在获得最优解。大量实验在各种基准上始终展示了我们方法相对于其他基准的优越性。例如，在LlamaV1-7B的Wikitext2数据集上，EBFT实现了16.27的困惑度，优于LoRA（困惑度16.44）。此外，EBFT对LlamaV1-7B的微调过程仅需约30分钟，整个框架可以在单个16GB
    GPU上执行。源代码可在[https://github.com/sunggo/EBFT](https://github.com/sunggo/EBFT)找到。
- en: 'EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'EBFT: 高效的块级微调用于稀疏LLMs'
- en: 'Song Guo¹²²footnotemark: 2, Fan Wu¹²²footnotemark: 2, Lei Zhang¹, Xiawu Zheng¹,
    Shengchuan Zhang¹, Fei Chao¹, Yiyu Shi², Rongrong Ji¹ ¹MAC Lab, Xiamen University.
    ²University of Notre Dame. {guosong, wfanstory, leizhang}@stu.xmu.edu.cn, {zhengxiawu,
    fchao, zsc_2016, rrji}@xmu.edu.cn yshi4@nd.edu'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 宋果¹²²脚注标记：2，范武¹²²脚注标记：2，张磊¹，郑夏吴¹，张盛川¹，赵飞¹，史奕宇²，季蓉蓉¹ ¹厦门大学MAC实验室。²圣母大学。 {guosong,
    wfanstory, leizhang}@stu.xmu.edu.cn，{zhengxiawu, fchao, zsc_2016, rrji}@xmu.edu.cn
    yshi4@nd.edu
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs have demonstrated remarkable potential in various NLP tasks. However, the
    large sizes of these models pose challenges in terms of resource requirements
    for deployment. For instance, the inference of GPT-3 Brown et al. ([2020](#bib.bib5))
    in half-precision floating-point format demands at least 5 80G A100 GPUs. To address
    this issue, several model compression methods, such as network quantization Lin
    et al. ([2023](#bib.bib24)); Frantar et al. ([2022](#bib.bib13)), network pruning
    Frantar and Alistarh ([2023](#bib.bib12)), and knowledge distillation Hsieh et al.
    ([2023](#bib.bib18)), have been proposed to compress and accelerate these Large
    Language Models. Among these methods, network pruning has gained increasing attention.
    However, pruning often leads to a decline in the performance of sparse models.
    To address this issue, recent works Zhang et al. ([2023d](#bib.bib49)); Frantar
    and Alistarh ([2023](#bib.bib12)); Zhang et al. ([2023a](#bib.bib45)) have emerged
    that can fine-tune the pruned models to recover their performance through regression
    reconstruction, costly retraining, or other heuristic methods. In this paper,
    we introduce EBFT, a framework designed to effectively fine-tune sparse LLMs,
    significantly enhancing the performance and generality of pruned models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在各种NLP任务中展示了显著的潜力。然而，这些模型的大规模带来了资源需求方面的挑战。例如，GPT-3 Brown et al. ([2020](#bib.bib5))
    在半精度浮点格式下的推理需要至少5块80G A100 GPU。为了解决这一问题，提出了多种模型压缩方法，如网络量化 Lin et al. ([2023](#bib.bib24));
    Frantar et al. ([2022](#bib.bib13))，网络剪枝 Frantar and Alistarh ([2023](#bib.bib12))，以及知识蒸馏
    Hsieh et al. ([2023](#bib.bib18))，这些方法旨在压缩和加速这些大型语言模型。在这些方法中，网络剪枝受到越来越多的关注。然而，剪枝通常会导致稀疏模型性能的下降。为了解决这一问题，近期的研究
    Zhang et al. ([2023d](#bib.bib49)); Frantar and Alistarh ([2023](#bib.bib12));
    Zhang et al. ([2023a](#bib.bib45)) 提出了通过回归重建、成本高昂的再训练或其他启发式方法来微调剪枝模型，以恢复其性能。本文介绍了EBFT，一个旨在有效微调稀疏LLMs的框架，显著提升了剪枝模型的性能和通用性。
- en: Dataset used for fine-tuning. Some existing pruning then fine-tuning approaches
    require significant retraining resources, partly due to the large size of the
    retraining dataset. For example, LLM-Pruner Ma et al. ([2023](#bib.bib26)) employs
    Alpaca-cleaned Taori et al. ([2023](#bib.bib34)) as its fine-tuning dataset to
    restore the performance of sparse LLMs. Alpaca-cleaned consists of 51.8K rows
    of data, resulting in substantial time costs for fine-tuning LLMs. Similarly,
    Sheared Llama Xia et al. ([2023](#bib.bib42)) employs RedPajama Computer ([2023](#bib.bib8)),
    containing 2.17M rows of data, for LLM pruning and fine-tuning, which incurs huge
    resource costs. In this paper, we sample a small calibration dataset comprising
    only 256 1024-token segments extracted from C4 Raffel et al. ([2020](#bib.bib29)).
    By fine-tuning sparse LLMs using these samples, we effectively reduce the resource
    requirements and time costs associated with the process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 微调所用的数据集。一些现有的剪枝后再微调方法需要大量的再训练资源，部分原因是再训练数据集的规模较大。例如，LLM-Pruner Ma et al. ([2023](#bib.bib26))
    使用Alpaca-cleaned Taori et al. ([2023](#bib.bib34)) 作为其微调数据集，以恢复稀疏LLMs的性能。Alpaca-cleaned包含51.8K行数据，导致微调LLMs的时间成本显著。同样，Sheared
    Llama Xia et al. ([2023](#bib.bib42)) 使用RedPajama Computer ([2023](#bib.bib8))，包含2.17M行数据，进行LLM剪枝和微调，带来了巨大的资源成本。本文中，我们从C4
    Raffel et al. ([2020](#bib.bib29))中提取了仅256个1024-token段的小型校准数据集。通过使用这些样本微调稀疏LLMs，我们有效地减少了与该过程相关的资源需求和时间成本。
- en: Optimization algorithm. Current LLM pruning methods, like SparseGPT Frantar
    and Alistarh ([2023](#bib.bib12)), construct reconstruction errors based on feature
    maps before and after pruning. They approximate the reconstruction error using
    the second-order term of Taylor’s Formula and optimize it by regression reconstruction.
    Wanda Sun et al. ([2023](#bib.bib32)) can be viewed as an approximation of the
    pruning criteria used in SparseGPT. DSnoT Zhang et al. ([2023d](#bib.bib49)) utilizes
    masks from Wanda or SparseGPT as initialization and designs a heuristic criterion
    to reselect masks that can reduce the reconstruction error further. These algorithms
    only optimize an approximation and often rely on heuristic experiences, leading
    to sub-optimal solutions. In contrast, our method defines the block-wise reconstruction
    error and directly optimizes it through backpropagation Werbos ([1990](#bib.bib41)),
    ensuring the attainment of an optimal and convergent solution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法。目前的 LLM 剪枝方法，如 SparseGPT Frantar 和 Alistarh（[2023](#bib.bib12)），基于剪枝前后的特征图构建重建误差。他们使用泰勒公式的二阶项来近似重建误差，并通过回归重建进行优化。Wanda
    Sun 等人（[2023](#bib.bib32)）可以被视为对 SparseGPT 使用的剪枝标准的近似。DSnoT Zhang 等人（[2023d](#bib.bib49)）利用
    Wanda 或 SparseGPT 的掩码作为初始化，并设计了一种启发式标准来重新选择可以进一步减少重建误差的掩码。这些算法仅优化近似值，通常依赖启发式经验，导致次优解。相比之下，我们的方法定义了块级重建误差，并通过反向传播
    Werbos（[1990](#bib.bib41)）直接优化它，确保获得最佳且收敛的解决方案。
- en: Fine-tuning costs. EBFT can be integrated with any pruning method and optimizes
    the block-wise reconstruction error through a backpropagation algorithm. Our framework
    can avoid the simultaneous loading of all LLM blocks onto the GPU and require
    only a few samples, significantly reducing costs. Experimental results indicate
    that the time required of EBFT for fine-tuning each block in Llama-7B Touvron
    et al. ([2023a](#bib.bib37)) ranges between 50 and 60 seconds, resulting in a
    total time cost of approximately 30 minutes. EBFT enables fine-tuning Llama-7B
    with a single 16GB GPU, making LLM fine-tuning feasible even under resource-constrained
    conditions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 微调成本。EBFT 可以与任何剪枝方法集成，并通过反向传播算法优化块级重建误差。我们的框架可以避免同时将所有 LLM 块加载到 GPU 上，只需少量样本，显著降低成本。实验结果表明，EBFT
    对 Llama-7B Touvron 等人（[2023a](#bib.bib37)）进行微调每个块所需的时间在 50 到 60 秒之间，总时间成本约为 30
    分钟。EBFT 使得使用单个 16GB GPU 对 Llama-7B 进行微调成为可能，即使在资源有限的条件下，LLM 微调也变得可行。
- en: 'In summary, our contributions can be summarized as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献可以总结为以下几点：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce EBFT, a block-by-block fine-tuning framework for sparse LLMs, which
    requires only a few samples, significantly reducing resource dependencies.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 EBFT，一个针对稀疏 LLM 的逐块微调框架，该框架只需少量样本，显著减少了对资源的依赖。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: EBFT updates the network based on the minimization of block-wise reconstruction
    error through backpropagation, resulting in an optimal and convergent solution.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EBFT 基于通过反向传播最小化块级重建误差来更新网络，从而得出最佳且收敛的解决方案。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: EBFT consistently surpass other state-of-the-art algorithms on various benchmarks
    and models, demonstrating the strong efficiency of our method.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EBFT 在各种基准测试和模型上 consistently 超越了其他最先进的算法，展示了我们方法的强大效率。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Network pruning. According to different levels of granularity, pruning can be
    categorized into unstructured pruning, structured pruning, and semi-structured
    pruning. (1) Unstructured pruning. Unstructured pruning methods involve removing
    individual weights in the weight matrix. Han et al. Han et al. ([2015](#bib.bib16))
    proposed an algorithm based on $l_{1}$ regulation, suggesting that smaller-norm
    weights are less important. LTH Frankle and Carbin ([2018](#bib.bib11)) increases
    the sparsity ratio during training and utilizes magnitude for pruning.(2) Structured
    pruning. Structured pruning involves removing entire rows or columns of the weight
    matrix. Li et al. Li et al. ([2016](#bib.bib22)) use the $l_{l}$-norm as the importance
    scores for channels. A pruning method Sanh et al. ([2020](#bib.bib31)) called
    movement pruning was proposed, which used the product of weight value and its
    gradient as the criterion for importance, surpassing magnitude pruning on BERT
    Devlin et al. ([2018](#bib.bib10)). Cofipruning Xia et al. ([2022](#bib.bib43))
    generates masks for BERT pruning via l0 regularization Louizos et al. ([2017](#bib.bib25));
    Wang et al. ([2019](#bib.bib40)). Guo et al. Guo et al. ([2023b](#bib.bib15))
    analyze existing pruning criteria and propose a method based on the information
    bottleneck principle Tishby et al. ([2000](#bib.bib35)); Tishby and Zaslavsky
    ([2015](#bib.bib36)).(3) Semi-structured pruning. Semi-structured pruning, also
    known as N:M sparsity Zhou et al. ([2021](#bib.bib50)); Zhang et al. ([2022](#bib.bib46)),
    ensures that for every continuous M weights in the weight matrix, only N weights
    are non-zero. N:M sparsity can accelerate the sparse model on specific devices.
    Zhang et al. Zhang et al. ([2023c](#bib.bib48)) proposed transposable Hubara et al.
    ([2021](#bib.bib20)) bi-directional masks to accelerate sparse models in both
    the forward and backward processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 网络剪枝。根据不同的粒度水平，剪枝可以分为非结构化剪枝、结构化剪枝和半结构化剪枝。(1) 非结构化剪枝。非结构化剪枝方法涉及移除权重矩阵中的个别权重。Han
    等人（[2015](#bib.bib16)）提出了一种基于 $l_{1}$ 正则化的算法，建议较小的范数权重不太重要。LTH Frankle 和 Carbin（[2018](#bib.bib11)）在训练过程中增加了稀疏比率，并利用幅度进行剪枝。(2)
    结构化剪枝。结构化剪枝涉及移除权重矩阵的整行或整列。Li 等人（[2016](#bib.bib22)）使用 $l_{l}$-范数作为通道的重要性评分。Sanh
    等人（[2020](#bib.bib31)）提出了一种叫做移动剪枝的方法，使用权重值及其梯度的乘积作为重要性标准，超越了在 BERT 上的幅度剪枝（Devlin
    等人，[2018](#bib.bib10)）。Cofipruning Xia 等人（[2022](#bib.bib43)）通过 l0 正则化（Louizos
    等人，[2017](#bib.bib25)；Wang 等人，[2019](#bib.bib40)）为 BERT 剪枝生成掩码。Guo 等人（[2023b](#bib.bib15)）分析了现有的剪枝标准，并提出了一种基于信息瓶颈原理的方法（Tishby
    等人，[2000](#bib.bib35)；Tishby 和 Zaslavsky，[2015](#bib.bib36)）。(3) 半结构化剪枝。半结构化剪枝，也称为
    N:M 稀疏性（Zhou 等人，[2021](#bib.bib50)；Zhang 等人，[2022](#bib.bib46)），确保在权重矩阵中的每连续 M
    个权重中，只有 N 个权重为非零。N:M 稀疏性可以加速特定设备上的稀疏模型。Zhang 等人（[2023c](#bib.bib48)）提出了可转置的 Hubara
    等人（[2021](#bib.bib20)）双向掩码，以加速前向和反向过程中的稀疏模型。
- en: '![Refer to caption](img/6efef3a2fbe7356ebff2c8a360962cfe.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6efef3a2fbe7356ebff2c8a360962cfe.png)'
- en: 'Figure 1: EBFT can be integrated with any other pruning methods, requiring
    only a small number of samples from C4\. When the initial mask $M_{0}^{l}$ are
    provided, EBFT updates the weight $W_{t}^{l}$ represents the weight vector of
    the l-th block of the LLM in the t-th iteration.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：EBFT 可以与任何其他剪枝方法集成，只需从 C4 中获取少量样本。当提供初始掩码 $M_{0}^{l}$ 时，EBFT 更新权重 $W_{t}^{l}$，表示
    t 次迭代中 LLM 第 l 块的权重向量。
- en: Fine-tuning for pruned LLMs. For LLMs, specific pruning methods Ashkboos et al.
    ([2024](#bib.bib2)); An et al. ([2023](#bib.bib1)); Syed et al. ([2023](#bib.bib33));
    Li et al. ([2023](#bib.bib23)) have been proposed. LoraPruner Zhang et al. ([2023a](#bib.bib45)),
    LLM-pruner Ma et al. ([2023](#bib.bib26)), and Compresso Guo et al. ([2023a](#bib.bib14))
    aim to remove entire attention heads or FFN units in the transformers Vaswani
    et al. ([2017](#bib.bib39)), followed by fine-tuning on a large dataset using
    PEFT Hu et al. ([2021](#bib.bib19)). However, these methods suffer from performance
    degradation and high retraining costs. SparseGPT Frantar and Alistarh ([2023](#bib.bib12))
    employs OBS Hassibi et al. ([1993](#bib.bib17)) to prune the weights of LLMs and
    recovers their performance through regression reconstruction. Wanda Sun et al.
    ([2023](#bib.bib32)) proposes a new importance criterion, which approximates the
    criterion used in SparseGPT. DSnoT Zhang et al. ([2023d](#bib.bib49)) aims to
    fine-tune sparse LLMs and designs a criterion to further reduce reconstruction
    error by reselecting masks. These methods require costly retraining or rely on
    approximation and heuristic optimization strategies, resulting in significant
    resource consumption or sub-optimal solutions.To address these challenges, we
    propose a fine-tuning framework called EBFT, which helps us obtain an optimal
    and convergent sparse model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝 LLM 的微调。针对 LLM 的特定剪枝方法已被提出，例如 Ashkboos 等人 ([2024](#bib.bib2))；An 等人 ([2023](#bib.bib1))；Syed
    等人 ([2023](#bib.bib33))；Li 等人 ([2023](#bib.bib23))。LoraPruner 张等人 ([2023a](#bib.bib45))，LLM-pruner
    马等人 ([2023](#bib.bib26))，以及 Compresso 郭等人 ([2023a](#bib.bib14)) 旨在去除变换器中的整个注意力头或
    FFN 单元 Vaswani 等人 ([2017](#bib.bib39))，随后在大数据集上使用 PEFT 胡等人 ([2021](#bib.bib19))
    进行微调。然而，这些方法存在性能下降和高重训练成本的问题。SparseGPT Frantar 和 Alistarh ([2023](#bib.bib12))
    使用 OBS 哈西比等人 ([1993](#bib.bib17)) 剪枝 LLM 的权重，并通过回归重建恢复其性能。Wanda 孙等人 ([2023](#bib.bib32))
    提出了一个新的重要性标准，近似于 SparseGPT 中使用的标准。DSnoT 张等人 ([2023d](#bib.bib49)) 旨在微调稀疏 LLM，并设计了一个标准，通过重新选择掩码进一步减少重建误差。这些方法需要昂贵的重训练或依赖于近似和启发式优化策略，导致显著的资源消耗或次优解决方案。为了应对这些挑战，我们提出了一个称为
    EBFT 的微调框架，帮助我们获得一个最佳且收敛的稀疏模型。
- en: 3 Methodology
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基础知识
- en: 'Large Language Model. The structure of a large language model is based on the
    transformer, which consists of multiple stacked blocks. Each block consists of
    two modules: multi-head self-attention (MHA) and multi-layer perceptron (MLP).
    MHA typically comprises four linear layers, while MLP consists of two or three
    linear layers. For the l-th block in the large language model, it can be formulated
    as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。大型语言模型的结构基于变换器（transformer），由多个堆叠的块组成。每个块包括两个模块：多头自注意力（MHA）和多层感知器（MLP）。MHA
    通常包括四个线性层，而 MLP 由两个或三个线性层组成。对于大型语言模型中的第 l 个块，可以表示如下：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: '|  | $1$2 |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $W_{mha}^{l}$ represents the weight vector of the multi-layer perceptron
    module in the i-th block. LN represents the layer normalization function. $z_{ffn}^{l-1}$
    is first passed through the MHA module and then through the MLP module.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{mha}^{l}$ 代表第 i 个块中多层感知器模块的权重向量。LN 代表层归一化函数。$z_{ffn}^{l-1}$ 首先通过 MHA
    模块，然后通过 MLP 模块。
- en: 'Pruning for LLMs. Existing pruning methods for LLMs Frantar and Alistarh ([2023](#bib.bib12));
    Zhang et al. ([2023d](#bib.bib49)); Boža ([2024](#bib.bib4)); Das et al. ([2023](#bib.bib9))
    typically employ the reconstruction error of the layer-wise feature maps before
    and after pruning as the optimization objective. This objective can be defined
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的剪枝。现有的 LLM 剪枝方法 Frantar 和 Alistarh ([2023](#bib.bib12))；张等人 ([2023d](#bib.bib49))；Boža
    ([2024](#bib.bib4))；Das 等人 ([2023](#bib.bib9)) 通常使用剪枝前后层级特征图的重建误差作为优化目标。该目标可以定义如下：
- en: '|  | $\min_{M,\bar{W}}&#124;&#124;WX-(M\odot\bar{W})X&#124;&#124;_{2},\;s.t.\;1-\frac{&#124;&#124;M&#124;&#124;_{0}}{N}=S,$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{M,\bar{W}}&#124;&#124;WX-(M\odot\bar{W})X&#124;&#124;_{2},\;s.t.\;1-\frac{&#124;&#124;M&#124;&#124;_{0}}{N}=S,$
    |  | (2) |'
- en: where X represents the input activation. W and $\bar{W}$ is the mask for this
    layer, indicating whether the corresponding weights should be preserved (1) or
    discarded (0). S is the pre-designed target sparsity, and N denotes the total
    number of weights in the layer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 X 代表输入激活。W 和 $\bar{W}$ 是该层的掩码，指示相应的权重是否应保留（1）或丢弃（0）。S 是预设的目标稀疏度，N 表示该层中权重的总数。
- en: 'These methods often employ the second-order term of the Taylor formula to approximate
    the layer-wise reconstruction error in Eq. [2](#S3.E2 "In 3.1 Preliminaries ‣
    3 Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs") or
    design heuristic criteria to optimize Eq. [2](#S3.E2 "In 3.1 Preliminaries ‣ 3
    Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs"). However,
    these approaches may result in suboptimal solutions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '这些方法通常使用泰勒公式的二阶项来近似公式 [2](#S3.E2 "在3.1 基础知识 ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")中的层级重建误差，或设计启发式标准来优化公式 [2](#S3.E2
    "在3.1 基础知识 ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")。然而，这些方法可能会导致次优解。'
- en: 3.2 EBFT
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 EBFT
- en: 'Overview. We propose a framework called EBFT for the fine-tuning of sparse
    LLMs, aiming to achieve optimal solutions. Unlike other costly methods that involve
    pruning and then fine-tuning on a large dataset Xia et al. ([2023](#bib.bib42));
    Ma et al. ([2023](#bib.bib26)); Zhang et al. ([2023a](#bib.bib45)), EBFT only
    requires a small calibration dataset consisting of a few samples. Specifically,
    we extract 256 1024-token samples from C4 and use them as the calibration dataset
    denoted as $D_{c}$. The principle of EBFT is based on minimizing the block-wise
    reconstruction error. An overview of our algorithm is depicted in Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse
    LLMs").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '概述。我们提出了一个名为EBFT的框架，用于稀疏LLMs的微调，旨在实现最优解。与其他涉及剪枝然后在大数据集上微调的昂贵方法不同，如 Xia et al.
    ([2023](#bib.bib42)); Ma et al. ([2023](#bib.bib26)); Zhang et al. ([2023a](#bib.bib45))，EBFT只需一个包含少量样本的小型校准数据集。具体来说，我们从C4中提取了256个1024-token的样本，并将其作为校准数据集，记作$D_{c}$。EBFT的原则是最小化块级重建误差。我们的算法概述如图 [1](#S2.F1
    "图1 ‣ 2 相关工作 ‣ EBFT: 稀疏LLMs的有效块级微调")所示。'
- en: 'Optimization objective. For the l-th block in the sparse LLM, it can be formulated
    as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 优化目标。对于稀疏LLM中的第l块，可以表示为：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: '|  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\bar{W}_{mha}^{l}=M_{mha}^{l}*W_{mha}^{l}$ represent the remain weight
    vector of the multi-head self-attention module and multi-layer perceptron module,
    respectively, in the l-th block. $M_{mha}^{l}$ represent their corresponding masks.
    $\bar{z}_{ffn}^{l}$ denotes the output of the l-th block after pruning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{W}_{mha}^{l}=M_{mha}^{l}*W_{mha}^{l}$ 代表第l块中多头自注意力模块和多层感知模块的剩余权重向量，$M_{mha}^{l}$
    代表它们对应的掩码。$\bar{z}_{ffn}^{l}$ 表示剪枝后的第l块输出。
- en: 'we define our block-wise optimization objective as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将块级优化目标定义为：
- en: '|  | $\min_{\bar{W}_{mha}^{l},\bar{W}_{mlp}^{l}}&#124;&#124;z_{ffn}^{l}-\bar{z}_{ffn}^{l}&#124;&#124;_{2}$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bar{W}_{mha}^{l},\bar{W}_{mlp}^{l}}&#124;&#124;z_{ffn}^{l}-\bar{z}_{ffn}^{l}&#124;&#124;_{2}$
    |  | (4) |'
- en: 'In Eq. [4](#S3.E4 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs"), we preserve the masks obtained from other pruning
    methods unchanged and focus on optimizing the remaining weights within the current
    block.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在公式 [4](#S3.E4 "在3.2 EBFT ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")中，我们保持其他剪枝方法获得的掩码不变，并专注于优化当前块内的剩余权重。'
- en: 'Compared to the layer-wise reconstruction error in Eq.[2](#S3.E2 "In 3.1 Preliminaries
    ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs"),
    the block-wise optimization process in Eq.[4](#S3.E4 "In 3.2 EBFT ‣ 3 Methodology
    ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs") allows for interaction
    and information exchange among different layers within the block. This enables
    the model to avoid potential issues associated with local optima in layer-wise
    optimization and explore the solution space more effectively, leading to the discovery
    of a globally optimal solution. Our EBFT is to directly optimize Eq.[4](#S3.E4
    "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for
    Sparse LLMs") without relying on any approximations or heuristic methods.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '与公式 [2](#S3.E2 "在3.1 基础知识 ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")中的层级重建误差相比，公式 [4](#S3.E4
    "在3.2 EBFT ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")中的块级优化过程允许块内不同层之间的交互和信息交换。这使得模型能够避免层级优化中的局部最优问题，更有效地探索解空间，从而发现全局最优解。我们的EBFT旨在直接优化公式 [4](#S3.E4
    "在3.2 EBFT ‣ 3 方法论 ‣ EBFT: 稀疏LLMs的有效块级微调")，而不依赖于任何近似或启发式方法。'
- en: 'Optimization algorithm. Unlike some methods Kwon et al. ([2022](#bib.bib21));
    Frantar and Alistarh ([2023](#bib.bib12)); Zhang et al. ([2023a](#bib.bib45))
    that update the weights of LLM based on regression reconstruction or costly retraining,
    we employ the backpropagation algorithm to minimize Eq. [4](#S3.E4 "In 3.2 EBFT
    ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs")
    by updating the value of the variable $\bar{W}_{mha}^{l}$ block by block on the
    $D_{c}$, without utilizing any heuristic methods.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '优化算法。与一些方法不同，如 Kwon 等（[2022](#bib.bib21)）；Frantar 和 Alistarh（[2023](#bib.bib12)）；Zhang
    等（[2023a](#bib.bib45)），这些方法基于回归重建或昂贵的重新训练来更新 LLM 的权重，我们采用反向传播算法通过在 $D_{c}$ 上逐块更新变量
    $\bar{W}_{mha}^{l}$ 的值，来最小化 Eq. [4](#S3.E4 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT:
    Effective and Block-Wise Fine-Tuning for Sparse LLMs")，而无需利用任何启发式方法。'
- en: 'The workflow of our EBFT framework is illustrated in Alg. [1](#algorithm1 "In
    3.2 EBFT ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse
    LLMs"). Prior to the fine-tuning process, we establish a maximum iteration T to
    control the overall fine-tuning cost. Specifically, we set T to 10 epochs. During
    the fine-tuning phase, if the loss remains unchanged or changes within a small
    range, we consider the loss to have converged. At this point, the fine-tuning
    algorithm for the current block will terminate early, allowing us to proceed to
    the subsequent block for a new round of fine-tuning.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的 EBFT 框架的工作流程在 Alg. [1](#algorithm1 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT:
    Effective and Block-Wise Fine-Tuning for Sparse LLMs") 中进行了说明。在微调过程之前，我们设定一个最大迭代次数
    T 来控制整体微调成本。具体来说，我们将 T 设定为 10 个 epoch。在微调阶段，如果损失保持不变或变化范围较小，我们认为损失已经收敛。这时，当前块的微调算法将提前终止，从而允许我们进入下一块进行新一轮的微调。'
- en: 'In Alg. [1](#algorithm1 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT: Effective and
    Block-Wise Fine-Tuning for Sparse LLMs"), $m_{0}$ represents the learning rate
    which determines the size of updating step for the variable $\bar{W}_{l}^{t}$.
    Specifically, we set the learning rate to 2e-4.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Alg. [1](#algorithm1 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs") 中，$m_{0}$ 代表学习率，它决定了变量 $\bar{W}_{l}^{t}$ 更新步骤的大小。具体来说，我们将学习率设定为
    2e-4。'
- en: input : sparse LLM F with L blocks; Initial Mask $m_{0}$; Max fine-tuning iterations
    T; Learning rate $\alpha$for *block $l=1$* do       for *iteration $t=0$* do            
    $E\leftarrow$ Calculating the gradient of $\bar{W_{t}^{l}}$ $\bar{W_{t}^{l}}$$\nabla\bar{W_{t}^{l}}$;
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：稀疏 LLM F，具有 L 个块；初始掩码 $m_{0}$；最大微调迭代次数 T；学习率 $\alpha$ 对于 *块 $l=1$* 执行       
    对于 *迭代 $t=0$* 执行             $E\leftarrow$ 计算 $\bar{W_{t}^{l}}$ 的梯度 $\bar{W_{t}^{l}}$$\nabla\bar{W_{t}^{l}}$；
- en: Algorithm 1 Pseudocode of EBFT
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 EBFT 的伪代码
- en: 4 Experiments
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Models and Baselines. We apply magnitude pruning, SparseGPT, and Wanda techniques
    to the widely adopted LLMs, LlamaV1 Touvron et al. ([2023a](#bib.bib37)) and LlamaV2
    Touvron et al. ([2023b](#bib.bib38)). Subsequently, we compare the evaluation
    results of the state-of-the-art method DsnoT Zhang et al. ([2023d](#bib.bib49))
    with our approach on the pruned LlamaV1 and LlamaV2, considering both unstructured
    sparsity and N:M sparsity. To further assess the effectiveness of our method,
    we also compare EBFT with LoRA Hu et al. ([2021](#bib.bib19)) under structured
    sparsity using FLAP An et al. ([2023](#bib.bib1)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与基准。我们将幅度修剪、SparseGPT 和 Wanda 技术应用于广泛采用的 LLM，即 LlamaV1 Touvron 等（[2023a](#bib.bib37)）和
    LlamaV2 Touvron 等（[2023b](#bib.bib38)）。随后，我们将最先进方法 DsnoT Zhang 等（[2023d](#bib.bib49)）的评估结果与我们的方法在修剪后的
    LlamaV1 和 LlamaV2 上进行比较，考虑了无结构稀疏性和 N:M 稀疏性。为了进一步评估我们方法的有效性，我们还将 EBFT 与 LoRA Hu
    等（[2021](#bib.bib19)）在使用 FLAP An 等（[2023](#bib.bib1)）的结构化稀疏性下进行比较。
- en: Evaluation. To evaluate the performance of our method and other baselines, we
    conduct comparisons on the widely-used dataset Wikitext2 Merity et al. ([2016](#bib.bib27))
    to calculate perplexity scores. Additionally, we perform a series of zero-shot
    tasks, including PIQA Bisk et al. ([2020](#bib.bib3)), StoryCloze Mostafazadeh
    et al. ([2017](#bib.bib28)), ARC-Easy and ARC-Challenge Clark et al. ([2018](#bib.bib7)),
    HellaSwag Zellers et al. ([2019](#bib.bib44)), Winogrande Sakaguchi et al. ([2021](#bib.bib30)),
    and Boolq Clark et al. ([2019](#bib.bib6)). These tasks aim to assess the generality
    of the pruned model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。为了评估我们方法和其他基准的性能，我们在广泛使用的数据集 Wikitext2 Merity 等（[2016](#bib.bib27)）上进行比较，以计算困惑度得分。此外，我们还执行了一系列零样本任务，包括
    PIQA Bisk 等（[2020](#bib.bib3)）、StoryCloze Mostafazadeh 等（[2017](#bib.bib28)）、ARC-Easy
    和 ARC-Challenge Clark 等（[2018](#bib.bib7)）、HellaSwag Zellers 等（[2019](#bib.bib44)）、Winogrande
    Sakaguchi 等（[2021](#bib.bib30)）和 Boolq Clark 等（[2019](#bib.bib6)）。这些任务旨在评估修剪模型的通用性。
- en: '|  | LlamaV1-7B | LlamaV2-7B |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | LlamaV1-7B | LlamaV2-7B |'
- en: '| --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method
    Sparsity
    | $\boldsymbol{50\%}$ | $\boldsymbol{70\%}$ | $\boldsymbol{90\%}$ | $\boldsymbol{60\%}$
    | $\boldsymbol{80\%}$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法
    稀疏度
    | $\boldsymbol{50\%}$ | $\boldsymbol{70\%}$ | $\boldsymbol{90\%}$ | $\boldsymbol{60\%}$
    | $\boldsymbol{80\%}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Magnitude | 17.29 | 559.99 | 48415 | 132176 | 317879 | 16.03 | 1924.81 |
    49906 | nan | nan |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 幅度 | 17.29 | 559.99 | 48415 | 132176 | 317879 | 16.03 | 1924.81 | 49906 |
    nan | nan |'
- en: '| w. DsnoT | 13.80 | 127.67 | 9614795 | 37474 | 202562 | 13.90 | 3749.55 |
    14271e4 | 21760e2 | 34462e2 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 13.80 | 127.67 | 9614795 | 37474 | 202562 | 13.90 | 3749.55 |
    14271e4 | 21760e2 | 34462e2 |'
- en: '| w. Ours | 7.11 | 9.53 | 26.30 | 659.12 | 9718.99 | 6.59 | 9.29 | 33.50 |
    462.32 | 2930.51 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们 | 7.11 | 9.53 | 26.30 | 659.12 | 9718.99 | 6.59 | 9.29 | 33.50 | 462.32
    | 2930.51 |'
- en: '| Wanda | 7.26 | 10.69 | 88.84 | 5671.52 | 12748 | 6.94 | 10.96 | 78.26 | 3136.23
    | 6995.88 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 7.26 | 10.69 | 88.84 | 5671.52 | 12748 | 6.94 | 10.96 | 78.26 | 3136.23
    | 6995.88 |'
- en: '| w. DsnoT | 7.14 | 10.40 | 75.14 | 3635.94 | 9043.63 | 6.85 | 10.85 | 75.55
    | 4197.74 | 7311.58 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 7.14 | 10.40 | 75.14 | 3635.94 | 9043.63 | 6.85 | 10.85 | 75.55
    | 4197.74 | 7311.58 |'
- en: '| w. Ours | 6.81 | 8.59 | 16.88 | 118.38 | 2993.32 | 6.18 | 7.90 | 16.94 |
    72.80 | 903.45 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们 | 6.81 | 8.59 | 16.88 | 118.38 | 2993.32 | 6.18 | 7.90 | 16.94 | 72.80
    | 903.45 |'
- en: '| SparseGPT | 7.20 | 10.40 | 27.00 | 167.55 | 3912.78 | 7.09 | 10.54 | 29.37
    | 131.17 | 1542.22 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 7.20 | 10.40 | 27.00 | 167.55 | 3912.78 | 7.09 | 10.54 | 29.37
    | 131.17 | 1542.22 |'
- en: '| w. DsnoT | 9.25 | 9.68 | 46.99 | 8038.14 | 198898 | 6.97 | 10.23 | 59.62
    | 2510.54 | 49639 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 9.25 | 9.68 | 46.99 | 8038.14 | 198898 | 6.97 | 10.23 | 59.62
    | 2510.54 | 49639 |'
- en: '| w. Ours | 6.73 | 8.33 | 16.07 | 141.15 | 3366.39 | 6.20 | 7.88 | 18.13 |
    130.89 | 1233.80 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们 | 6.73 | 8.33 | 16.07 | 141.15 | 3366.39 | 6.20 | 7.88 | 18.13 | 130.89
    | 1233.80 |'
- en: 'Table 1: Comparison of perplexity for pruning and fine-tuning LlamaV1-7B and
    LlamaV2-7B on Wikitext2 dataset at unstructured sparsity levels ranging from $50\%$.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 Wikitext2 数据集上，比较不同稀疏度下的 LlamaV1-7B 和 LlamaV2-7B 的剪枝和微调后的困惑度。
- en: 4.1 Language Modeling
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 语言建模
- en: 'Unstructured Pruning. We perform comprehensive comparative experiments on the
    Wikitext2 dataset, and the results are presented in Table.[1](#S4.T1 "Table 1
    ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs")
    We compare the perplexity of pruned LlamaV1 and LlamaV2 models using our method,
    DsnoT, magnitude pruning, Wanda, and SparseGPT across a range of sparsity levels,
    from $50\%$. The experimental results show the strong effectiveness of our EBFT.
    We can observe that regardless of the magnitude pruning method used, be it SparseGPT
    or Wanda, our method enhances the performance of the sparse model. For instance,
    with magnitude pruning, our method achieves a perplexity of 7.11, surpassing the
    perplexity of 17.29 before fine-tuning, and even outperforming Wanda (7.26) and
    SparseGPT (7.20).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化剪枝。我们在 Wikitext2 数据集上进行全面的比较实验，结果如表格[1](#S4.T1 "表 1 ‣ 4 实验 ‣ EBFT：针对稀疏 LLM
    的有效且块状微调")所示。我们比较了使用我们的方法、DsnoT、幅度剪枝、Wanda 和 SparseGPT 的剪枝 LlamaV1 和 LlamaV2 模型的困惑度，稀疏度范围从
    $50\%$ 开始。实验结果显示，我们的 EBFT 的强大效果。我们可以观察到，无论使用哪种幅度剪枝方法，无论是 SparseGPT 还是 Wanda，我们的方法都能提升稀疏模型的性能。例如，使用幅度剪枝时，我们的方法实现了
    7.11 的困惑度，超越了微调前的 17.29 的困惑度，甚至超越了 Wanda (7.26) 和 SparseGPT (7.20)。
- en: 'We also find that as the sparsity increases, two observations emerge: (1) The
    state-of-the-art DsnoT loses its effectiveness as a fine-tuning method. For example,
    when using SparseGPT, DsnoT degrades the performance of the sparse model at sparsity
    levels of $70\%$, and $90\%$. This demonstrates the limitations of heuristic optimization
    strategies, which lack theoretical support. (2) The advantage of our method becomes
    more pronounced, indicating that our method enhances the ability of pruned models
    even at extremely high sparsity levels.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现，随着稀疏度的增加，有两个观察结果： (1) 尖端技术DsnoT作为微调方法的效果降低。例如，使用SparseGPT时，DsnoT在稀疏度为$70\%$和$90\%$时会降低稀疏模型的性能。这展示了启发式优化策略的局限性，因为它们缺乏理论支持。
    (2) 我们方法的优势更加明显，表明即使在极高稀疏度下，我们的方法也能提升修剪模型的能力。
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs"), we further observe that SparseGPT, which updates
    the values of the remaining weights, outperforms Wanda, which leaves the remaining
    weights unchanged. As sparsity increases, the advantage of SparseGPT over Wanda
    becomes more evident, particularly at high sparsity levels. Additionally, the
    DsnoT approach, which reselects the masks after pruning and keep weights unchanged,
    also faces challenges. For example, when the sparsity exceeds $70\%$, regardless
    of LlamaV1 or LlamaV2, DsnoT significantly decreases the performance of the sparse
    model pruned by SparseGPT. In contrast, our method effectively and efficiently
    fine-tunes the weights of the LLM block by block, surpassing other baselines overall.
    In the later section, we will conduct comprehensive and detailed experiments to
    further compare mask-tuning and weight-tuning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S4.T1 "表 1 ‣ 4 实验 ‣ EBFT：有效和块级微调用于稀疏LLMs")中，我们进一步观察到，更新剩余权重值的SparseGPT优于保持剩余权重不变的Wanda。随着稀疏度的增加，SparseGPT相对于Wanda的优势变得更加明显，特别是在高稀疏度下。此外，DsnoT方法在修剪后重新选择掩码并保持权重不变，也面临挑战。例如，当稀疏度超过$70\%$时，无论是LlamaV1还是LlamaV2，DsnoT都会显著降低SparseGPT修剪的稀疏模型的性能。相比之下，我们的方法有效且高效地逐块微调LLM的权重，总体上超越了其他基线。在后续部分，我们将进行全面而详细的实验，以进一步比较掩码调整和权重调整。
- en: '|  | LlamaV1-7B | LlamaV2-7B |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | LlamaV1-7B | LlamaV2-7B |'
- en: '| --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method
    Sparsity
    | $\boldsymbol{2:4}$ | $\boldsymbol{2:4}$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 方法
    稀疏度
    | $\boldsymbol{2:4}$ | $\boldsymbol{2:4}$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Magnitude | 42.54 | 16.83 | 54.39 | 16.53 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 42.54 | 16.83 | 54.39 | 16.53 |'
- en: '| w. DsnoT | 38.32 | 17.01 | 40.81 | 18.34 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 38.32 | 17.01 | 40.81 | 18.34 |'
- en: '| w. Ours | 9.62 | 8.10 | 9.14 | 7.56 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们的方法 | 9.62 | 8.10 | 9.14 | 7.56 |'
- en: '| Wanda | 11.50 | 8.57 | 12.11 | 8.66 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 11.50 | 8.57 | 12.11 | 8.66 |'
- en: '| w. DsnoT | 10.95 | 8.46 | 11.98 | 8.57 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 10.95 | 8.46 | 11.98 | 8.57 |'
- en: '| w. Ours | 8.89 | 7.66 | 8.30 | 7.11 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们的方法 | 8.89 | 7.66 | 8.30 | 7.11 |'
- en: '| SparseGPT | 11.05 | 8.55 | 10.44 | 8.01 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 11.05 | 8.55 | 10.44 | 8.01 |'
- en: '| w. DsnoT | 10.00 | 8.26 | 10.06 | 8.06 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| w. DsnoT | 10.00 | 8.26 | 10.06 | 8.06 |'
- en: '| w. Ours | 8.82 | 7.59 | 8.25 | 7.06 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| w. 我们的方法 | 8.82 | 7.59 | 8.25 | 7.06 |'
- en: 'Table 2: Comparison of perplexity for pruning and fine-tuning LlamaV1-7B and
    LlamaV2-7B on the Wikitext2 dataset at N:M sparsity levels, including two patterns,
    2:4 and 4:8.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在Wikitext2数据集上对LlamaV1-7B和LlamaV2-7B进行修剪和微调的困惑度比较，包括两种模式，2:4和4:8。
- en: 'Semi-structured Pruning. Semi-structured pruning, also known as N:M sparsity,
    is considered superior to unstructured pruning when it comes to accelerating models
    on devices. We conducted extensive comparison experiments on the Wikitext2 dataset,
    and the results are presented in Table [2](#S4.T2 "Table 2 ‣ 4.1 Language Modeling
    ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs").
    Irrespective of the 2:4 or 4:8 pattern, our method consistently outperforms DsnoT,
    significantly enhancing the performance of the pruned models. For example, when
    using the 2:4 pattern and Wanda mask initialization, our method achieves a perplexity
    of 8.30 for the sparse LlamaV2 model, which even surpasses the performance of
    DsnoT using the 4:8 pattern. The sparse LLMs pruned by magnitude pruning and fine-tuned
    with our method demonstrate a remarkable improvement. Our fine-tuning approach
    can effectively narrow the performance gap between magnitude pruning and the state-of-the-art
    baselines, Wanda and SparseGPT.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 半结构化剪枝。半结构化剪枝，也称为N:M稀疏度，在加速设备上的模型时被认为优于无结构剪枝。我们在Wikitext2数据集上进行了广泛的对比实验，结果如表[2](#S4.T2
    "表 2 ‣ 4.1 语言建模 ‣ 4 实验 ‣ EBFT：稀疏LLMs的有效和块级微调")所示。无论是2:4还是4:8模式，我们的方法始终优于DsnoT，显著提升了剪枝模型的性能。例如，使用2:4模式和Wanda掩码初始化时，我们的方法在稀疏LlamaV2模型上达到了8.30的困惑度，甚至超过了使用4:8模式的DsnoT的表现。通过幅度剪枝并使用我们的方法进行微调的稀疏LLMs展示了显著的改进。我们的微调方法可以有效缩小幅度剪枝与最先进的基准（Wanda和SparseGPT）之间的性能差距。
- en: '| Model | Method | PIQA | ARC-E | ARC-C | WinoGrande | HellaSwag | Boolq |
    StoryCloze | Mean |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | PIQA | ARC-E | ARC-C | WinoGrande | HellaSwag | Boolq | StoryCloze
    | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Lla.1(60%) | Mag. | 60.55 | 42.30 | 23.21 | 50.04 | 31.86 | 38.29 | 57.40
    | 43.38 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Lla.1(60%) | Mag. | 60.55 | 42.30 | 23.21 | 50.04 | 31.86 | 38.29 | 57.40
    | 43.38 |'
- en: '| w.DSnoT | 66.65 | 51.01 | 26.02 | 52.96 | 38.31 | 46.82 | 65.37 | 49.59 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 66.65 | 51.01 | 26.02 | 52.96 | 38.31 | 46.82 | 65.37 | 49.59 |'
- en: '| w.Ours | 72.69 | 63.26 | 32.17 | 63.85 | 46.61 | 65.72 | 73.33 | 59.66 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 72.69 | 63.26 | 32.17 | 63.85 | 46.61 | 65.72 | 73.33 | 59.66 |'
- en: '| Wanda | 72.74 | 62.67 | 30.03 | 62.67 | 43.71 | 68.90 | 71.25 | 58.85 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 72.74 | 62.67 | 30.03 | 62.67 | 43.71 | 68.90 | 71.25 | 58.85 |'
- en: '| w.DSnoT | 73.07 | 63.38 | 30.80 | 61.56 | 43.51 | 68.20 | 71.46 | 58.85 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 73.07 | 63.38 | 30.80 | 61.56 | 43.51 | 68.20 | 71.46 | 58.85 |'
- en: '| w.Ours | 73.67 | 65.57 | 32.17 | 65.11 | 47.80 | 69.79 | 73.86 | 61.14 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 73.67 | 65.57 | 32.17 | 65.11 | 47.80 | 69.79 | 73.86 | 61.14 |'
- en: '| SparseGPT | 72.36 | 62.58 | 31.14 | 64.40 | 45.38 | 69.79 | 73.65 | 59.90
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 72.36 | 62.58 | 31.14 | 64.40 | 45.38 | 69.79 | 73.65 | 59.90
    |'
- en: '| w.DSnoT | 73.70 | 63.17 | 31.83 | 63.06 | 47.41 | 67.52 | 73.22 | 59.99 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 73.70 | 63.17 | 31.83 | 63.06 | 47.41 | 67.52 | 73.22 | 59.99 |'
- en: '|  | w.Ours | 73.77 | 64.02 | 32.51 | 64.40 | 47.84 | 69.27 | 74.02 | 60.83
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | w.Ours | 73.77 | 64.02 | 32.51 | 64.40 | 47.84 | 69.27 | 74.02 | 60.83
    |'
- en: '| Lla.2(60%) | Mag. | 62.73 | 44.78 | 25.00 | 53.12 | 34.99 | 47.86 | 62.21
    | 47.24 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Lla.2(60%) | Mag. | 62.73 | 44.78 | 25.00 | 53.12 | 34.99 | 47.86 | 62.21
    | 47.24 |'
- en: '| w.DSnoT | 69.42 | 63.13 | 30.89 | 61.56 | 40.48 | 54.77 | 67.93 | 55.46 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 69.42 | 63.13 | 30.89 | 61.56 | 40.48 | 54.77 | 67.93 | 55.46 |'
- en: '| w.Ours | 72.63 | 64.94 | 32.25 | 65.11 | 46.40 | 71.01 | 73.06 | 60.77 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 72.63 | 64.94 | 32.25 | 65.11 | 46.40 | 71.01 | 73.06 | 60.77 |'
- en: '| Wanda | 71.71 | 64.98 | 30.55 | 64.56 | 43.82 | 65.57 | 71.99 | 59.02 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 71.71 | 64.98 | 30.55 | 64.56 | 43.82 | 65.57 | 71.99 | 59.02 |'
- en: '| w.DSnoT | 71.33 | 64.44 | 29.95 | 64.17 | 42.53 | 64.83 | 70.55 | 58.25 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 71.33 | 64.44 | 29.95 | 64.17 | 42.53 | 64.83 | 70.55 | 58.25 |'
- en: '| w.Ours | 73.56 | 68.73 | 33.19 | 64.40 | 47.26 | 67.22 | 73.49 | 61.12 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 73.56 | 68.73 | 33.19 | 64.40 | 47.26 | 67.22 | 73.49 | 61.12 |'
- en: '| SparseGPT | 71.44 | 63.72 | 31.48 | 66.69 | 45.25 | 72.54 | 74.29 | 60.77
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 71.44 | 63.72 | 31.48 | 66.69 | 45.25 | 72.54 | 74.29 | 60.77
    |'
- en: '| w.DSnoT | 72.85 | 66.58 | 33.19 | 62.83 | 46.71 | 65.72 | 73.54 | 60.20 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 72.85 | 66.58 | 33.19 | 62.83 | 46.71 | 65.72 | 73.54 | 60.20 |'
- en: '|  | w.Ours | 73.29 | 67.42 | 32.59 | 66.98 | 47.10 | 72.60 | 73.70 | 61.96
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | w.Ours | 73.29 | 67.42 | 32.59 | 66.98 | 47.10 | 72.60 | 73.70 | 61.96
    |'
- en: '| Lla.1(2: 4) | Mag. | 68.01 | 53.32 | 27.22 | 59.91 | 42.30 | 53.09 | 70.02
    | 53.41 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Lla.1(2: 4) | Mag. | 68.01 | 53.32 | 27.22 | 59.91 | 42.30 | 53.09 | 70.02
    | 53.41 |'
- en: '| w.DSnoT | 68.18 | 54.38 | 26.54 | 58.96 | 41.24 | 48.32 | 68.68 | 52.33 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 68.18 | 54.38 | 26.54 | 58.96 | 41.24 | 48.32 | 68.68 | 52.33 |'
- en: '| w.Ours | 72.80 | 64.18 | 30.89 | 64.25 | 45.80 | 68.29 | 72.37 | 59.80 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 72.80 | 64.18 | 30.89 | 64.25 | 45.80 | 68.29 | 72.37 | 59.80 |'
- en: '| Wanda | 70.40 | 60.82 | 27.79 | 63.22 | 42.08 | 69.08 | 70.71 | 57.76 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 70.40 | 60.82 | 27.79 | 63.22 | 42.08 | 69.08 | 70.71 | 57.76 |'
- en: '| w.DSnoT | 70.62 | 61.78 | 28.07 | 61.56 | 42.35 | 48.32 | 70.71 | 54.91 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 70.62 | 61.78 | 28.07 | 61.56 | 42.35 | 48.32 | 70.71 | 54.91 |'
- en: '| w.Ours | 72.42 | 64.81 | 30.97 | 65.19 | 46.05 | 67.25 | 72.42 | 59.87 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 72.42 | 64.81 | 30.97 | 65.19 | 46.05 | 67.25 | 72.42 | 59.87 |'
- en: '| SparseGPT | 71.22 | 60.73 | 30.46 | 63.38 | 42.95 | 69.85 | 70.23 | 58.40
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 71.22 | 60.73 | 30.46 | 63.38 | 42.95 | 69.85 | 70.23 | 58.40
    |'
- en: '| w.DSnoT | 72.63 | 63.13 | 30.72 | 62.67 | 45.91 | 67.77 | 71.73 | 59.22 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 72.63 | 63.13 | 30.72 | 62.67 | 45.91 | 67.77 | 71.73 | 59.22 |'
- en: '|  | w.Ours | 73.45 | 64.77 | 30.80 | 66.30 | 46.39 | 68.44 | 72.15 | 60.33
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | w.Ours | 73.45 | 64.77 | 30.80 | 66.30 | 46.39 | 68.44 | 72.15 | 60.33
    |'
- en: '| Lla.2(2: 4) | Mag. | 70.08 | 61.91 | 30.12 | 60.93 | 45.43 | 59.85 | 72.31
    | 57.23 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Lla.2(2: 4) | Mag. | 70.08 | 61.91 | 30.12 | 60.93 | 45.43 | 59.85 | 72.31
    | 57.23 |'
- en: '| w.DSnoT | 69.10 | 61.45 | 29.01 | 59.12 | 43.75 | 65.37 | 70.82 | 55.76 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 69.10 | 61.45 | 29.01 | 59.12 | 43.75 | 65.37 | 70.82 | 55.76 |'
- en: '| w.Ours | 73.07 | 67.17 | 30.72 | 64.64 | 45.27 | 66.73 | 72.63 | 60.03 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 73.07 | 67.17 | 30.72 | 64.64 | 45.27 | 66.73 | 72.63 | 60.03 |'
- en: '| Wanda | 70.89 | 61.91 | 30.72 | 62.51 | 41.27 | 68.53 | 70.23 | 58.01 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 70.89 | 61.91 | 30.72 | 62.51 | 41.27 | 68.53 | 70.23 | 58.01 |'
- en: '| w.DSnoT | 70.18 | 61.74 | 29.78 | 62.75 | 40.90 | 67.86 | 69.91 | 57.59 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 70.18 | 61.74 | 29.78 | 62.75 | 40.90 | 67.86 | 69.91 | 57.59 |'
- en: '| w.Ours | 72.91 | 65.91 | 31.91 | 63.77 | 45.49 | 69.33 | 73.06 | 60.34 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| w.Ours | 72.91 | 65.91 | 31.91 | 63.77 | 45.49 | 69.33 | 73.06 | 60.34 |'
- en: '| SparseGPT | 70.40 | 63.80 | 31.23 | 65.75 | 43.83 | 68.04 | 73.06 | 59.44
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 70.40 | 63.80 | 31.23 | 65.75 | 43.83 | 68.04 | 73.06 | 59.44
    |'
- en: '| w.DSnoT | 73.34 | 65.24 | 32.17 | 63.14 | 45.41 | 67.55 | 73.76 | 60.09 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| w.DSnoT | 73.34 | 65.24 | 32.17 | 63.14 | 45.41 | 67.55 | 73.76 | 60.09 |'
- en: '|  | w.Ours | 73.34 | 66.33 | 30.80 | 65.88 | 45.80 | 69.79 | 73.76 | 60.82
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | w.Ours | 73.34 | 66.33 | 30.80 | 65.88 | 45.80 | 69.79 | 73.76 | 60.82
    |'
- en: 'Table 3: Accuracy results of pruning and fine-tuning LlamaV1-7B and LlamaV2-7B
    on a series of zero-shot tasks at 60% sparsity and 2:4 pattern sparsity.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在 60% 稀疏性和 2:4 模式稀疏下，对 LlamaV1-7B 和 LlamaV2-7B 进行剪枝和微调的准确性结果。'
- en: 4.2 Zero-Shot Tasks
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 零样本任务
- en: 'We conducted extensive experiments to evaluate the performance of the sparse
    model on 7 zero-shot tasks. The metric we used is accuracy. The experimental results
    of different methods at the unstructured sparsity level are shown in Table [3](#S4.T3
    "Table 3 ‣ 4.1 Language Modeling ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs"). It can be observed that EBFT significantly enhances
    the generality of the pruned model. For instance, with magnitude pruning, EBFT
    improves the accuracy by 16.28 on LlamaV1-7B and by 13.53 on LlamaV2-7B. With
    Wanda, our methods achieve a mean accuracy of 61.14 on LlamaV1-7B and 61.12 on
    LlamaV2-7B. However, DSnoT hardly enhances the performance of the pruned model.
    It achieves a mean accuracy of 58.85 on LlamaV1-7B and 58.25 on LlamaV2-7B, respectively.
    For LlamaV2, DSnoT even degrades the performance after fine-tuning. The mean accuracy
    before fine-tuning for Wanda and SparseGPT is 59.02 and 60.77, respectively. After
    fine-tuning, the mean accuracy drops to 58.25 for Wanda and 60.20 for SparseGPT,
    highlighting the limitations of DSnoT. In contrast, after fine-tuning with EBFT,
    the sparse LlamaV2 model shows a significant improvement in overall accuracy,
    with a mean accuracy of 61.12 for Wanda and 61.96 for SparseGPT.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了广泛的实验，以评估稀疏模型在 7 个零样本任务上的性能。我们使用的度量是准确性。不同方法在无结构稀疏水平上的实验结果见表 [3](#S4.T3
    "Table 3 ‣ 4.1 Language Modeling ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs")。可以观察到，EBFT 显著提高了剪枝模型的通用性。例如，通过幅度剪枝，EBFT 提高了 LlamaV1-7B
    的准确率 16.28，LlamaV2-7B 的准确率 13.53。使用 Wanda，我们的方法在 LlamaV1-7B 上达到了 61.14 的平均准确率，在
    LlamaV2-7B 上达到了 61.12。然而，DSnoT 几乎没有提高剪枝模型的性能。它在 LlamaV1-7B 和 LlamaV2-7B 上分别达到了
    58.85 和 58.25 的平均准确率。对于 LlamaV2，DSnoT 甚至在微调后会降低性能。Wanda 和 SparseGPT 在微调前的平均准确率分别为
    59.02 和 60.77。微调后，Wanda 的平均准确率降至 58.25，SparseGPT 降至 60.20，这突显了 DSnoT 的局限性。相反，通过
    EBFT 微调后，稀疏的 LlamaV2 模型在整体准确性上有了显著提高，Wanda 的平均准确率为 61.12，SparseGPT 为 61.96。'
- en: 'N:M sparsity. We also investigated the generality of our EBFT approach at N:M
    sparsity levels. Similar to unstructured sparsity, EBFT demonstrates significant
    advantages compared to other baselines. The experimental results for the 2:4 pattern
    are presented in Tab.[3](#S4.T3 "Table 3 ‣ 4.1 Language Modeling ‣ 4 Experiments
    ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs"). In the case of
    magnitude pruning, EBFT improves the mean accuracy of sparse LlamaV1-7B by 6.39
    and sparse LlamaV2-7B by 2.8\. Conversely, DSnoT fails to restore the performance
    of magnitude-pruned sparse models. When using Wanda initialization, EBFT enhances
    the mean accuracy of sparse LlamaV1-7B by 2.11 and sparse LlamaV2-7B by 2.33\.
    Under SparseGPT initialization, EBFT improves the mean accuracy of sparse LlamaV1-7B
    by 1.93 and sparse LlamaV2-7B by 1.38\. In contrast, DSnoT loses its effectiveness
    with the current pattern as a fine-tuning method. Excluding the SparseGPT initialization
    on LlamaV2-7B, DSnoT significantly degrades the accuracy of the sparse model.
    For instance, with Wanda initialization, it results in a drop of 2.85 in accuracy
    for LlamaV1-7B and 0.42 for LlamaV2-7B.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'N:M 稀疏度。我们还研究了 EBFT 方法在 N:M 稀疏度水平上的普适性。与非结构化稀疏度类似，EBFT 相对于其他基准方法表现出显著优势。2:4
    模式的实验结果见表。[3](#S4.T3 "表 3 ‣ 4.1 语言建模 ‣ 4 实验 ‣ EBFT: 有效且块状的稀疏 LLM 微调")。在幅度剪枝的情况下，EBFT
    将稀疏 LlamaV1-7B 的平均准确率提高了 6.39，将稀疏 LlamaV2-7B 的平均准确率提高了 2.8。相反，DSnoT 无法恢复幅度剪枝稀疏模型的性能。在使用
    Wanda 初始化时，EBFT 将稀疏 LlamaV1-7B 的平均准确率提高了 2.11，将稀疏 LlamaV2-7B 的平均准确率提高了 2.33。在
    SparseGPT 初始化下，EBFT 将稀疏 LlamaV1-7B 的平均准确率提高了 1.93，将稀疏 LlamaV2-7B 的平均准确率提高了 1.38。相比之下，DSnoT
    作为微调方法在当前模式下失去了效果。除了 SparseGPT 初始化在 LlamaV2-7B 上，DSnoT 显著降低了稀疏模型的准确率。例如，使用 Wanda
    初始化时，LlamaV1-7B 的准确率下降了 2.85，而 LlamaV2-7B 的准确率下降了 0.42。'
- en: '![Refer to caption](img/c75b24b8ffb8d1d59feb5268c85d33b6.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c75b24b8ffb8d1d59feb5268c85d33b6.png)'
- en: 'Figure 2: The perplexity of the fine-tuned LlamaV1-7B on Wikitext2, with a
    sparsity level of 50%, varies with the number of samples in the calibration dataset.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 Wikitext2 上微调的 LlamaV1-7B 的困惑度，稀疏度为 50%，随着校准数据集中样本数量的变化而变化。
- en: 4.3 Calibration Samples
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 校准样本
- en: 'We vary the number of samples in the calibration dataset and generate a plot
    illustrating the perplexity and number of samples for the fine-tuned sparse LlamaV1-7B
    under Wanda initialization. The results are presented in Fig.[2](#S4.F2 "Figure
    2 ‣ 4.2 Zero-Shot Tasks ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise Fine-Tuning
    for Sparse LLMs"). The experimental findings demonstrate the robustness of our
    proposed method. Generally, as the number of samples increases, the performance
    of the sparse model improves. However, once the number of samples reaches 512,
    the perplexity does not decrease further. Notably, even with just 8 samples, the
    fine-tuned sparse LlamaV1 model exhibits an improvement compared to the model
    before fine-tuning. In addition, as the number of samples decreases, the fine-tuning
    speed can be further accelerated.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调整校准数据集中样本的数量，并生成一张图，展示 Wanda 初始化下微调的稀疏 LlamaV1-7B 的困惑度与样本数量的关系。结果见图。[2](#S4.F2
    "图 2 ‣ 4.2 零样本任务 ‣ 4 实验 ‣ EBFT: 有效且块状的稀疏 LLM 微调")。实验结果展示了我们提出方法的鲁棒性。一般而言，随着样本数量的增加，稀疏模型的性能有所提升。然而，一旦样本数量达到
    512，困惑度不再进一步下降。值得注意的是，即使只有 8 个样本，微调后的稀疏 LlamaV1 模型相比于微调前的模型也有所改进。此外，随着样本数量的减少，微调速度可以进一步加快。'
- en: 4.4 EBFT vs. LoRA
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 EBFT 与 LoRA
- en: Low-Rank Adaptation (LoRA) has gained popularity as a technique for retraining
    large language models. Recent works such as Ma et al. ([2023](#bib.bib26)); Guo
    et al. ([2023a](#bib.bib14)); Li et al. ([2023](#bib.bib23)) have extensively
    used LoRA for retraining pruned models. This involves fine-tuning the low-rank
    parameters A and B of an additional adapter on a large dataset to restore its
    performance. In this paper, we provide a detailed comparison of the fine-tuning
    cost and performance between LoRA and our EBFT.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配（LoRA）作为重新训练大型语言模型的技术已获得广泛关注。最近的研究如 Ma 等人 ([2023](#bib.bib26)); Guo 等人 ([2023a](#bib.bib14));
    Li 等人 ([2023](#bib.bib23)) 已广泛使用 LoRA 进行剪枝模型的重新训练。这涉及在大规模数据集上微调附加适配器的低秩参数 A 和
    B 以恢复其性能。在本文中，我们提供了 LoRA 与我们 EBFT 在微调成本和性能方面的详细比较。
- en: In our study, we applied Low-Rank Adaptation (LoRA) and EBFT to FLAP An et al.
    ([2023](#bib.bib1)) with structured sparsity levels. FLAP is a state-of-the-art
    method that outperforms LLM-Pruner in various tasks. It introduces a novel metric
    for channels in large language models and utilizes this metric score to search
    for the global structure of the model. We utilized the masks generated by FLAP
    as initialization for the fine-tuning process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们应用了低秩自适应（LoRA）和EBFT方法于FLAP An等（[2023](#bib.bib1)）的结构化稀疏度水平。FLAP是一种最先进的方法，在各种任务中优于LLM-Pruner。它引入了一种针对大型语言模型的通道的新指标，并利用该指标分数来搜索模型的全局结构。我们利用FLAP生成的掩码作为微调过程的初始化。
- en: When fine-tuning the model pruned by FLAP using LoRA, we selected the Alpaca-GPT4
    dataset as the retraining dataset. The Alpaca-GPT4 dataset consists of 50k rows
    of data and was fine-tuned using GPT4\. We performed fine-tuning with LoRA for
    2 epochs on the Alpaca-GPT4 dataset, using a learning rate of 1e-4 and a batch
    size of 64, which is the same as LLM-Pruner.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用LoRA微调FLAP修剪过的模型时，我们选择了Alpaca-GPT4数据集作为再训练数据集。Alpaca-GPT4数据集包含50k行数据，并使用GPT4进行了微调。我们在Alpaca-GPT4数据集上用LoRA进行了2轮微调，学习率为1e-4，批量大小为64，与LLM-Pruner相同。
- en: 'The fine-tuning methods employed in recent state-of-the-art works, as mentioned
    above, can incur a significant retraining cost. We compared their retraining methods
    with ours on a 40G A100 GPU. The time costs and perplexity on Wikitext2 of LoRA
    and EBFT are listed in Table 4\. It is observed that compared to LoRA, our EBFT
    achieves a 10$\times$ speedup, resulting in a significant reduction in fine-tuning
    costs. Additionally, EBFT demonstrates better performance compared to LoRA. As
    shown in Table [4](#S4.T4 "Table 4 ‣ 4.4 EBFT vs. LoRA ‣ 4 Experiments ‣ EBFT:
    Effective and Block-Wise Fine-Tuning for Sparse LLMs"), when reducing 20% of the
    parameters of LlamaV2-7B, EBFT achieves a perplexity of 15.71 on Wikitext2, which
    is superior to the perplexity obtained by LoRA (16.08).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '最近最先进的工作中使用的微调方法，如上所述，可能会产生显著的再训练成本。我们将它们的再训练方法与我们的进行比较，使用40G A100 GPU。LoRA和EBFT在Wikitext2上的时间成本和困惑度列于表4。观察到与LoRA相比，我们的EBFT实现了10$\times$的加速，从而显著减少了微调成本。此外，EBFT表现出比LoRA更好的性能。如表[4](#S4.T4
    "Table 4 ‣ 4.4 EBFT vs. LoRA ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs")所示，当减少LlamaV2-7B的20%参数时，EBFT在Wikitext2上实现了15.71的困惑度，优于LoRA（16.08）获得的困惑度。'
- en: '| Method | sparsity | time | perplexity |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 稀疏度 | 时间 | 困惑度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LoRA | 20% | 5h | 16.08 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 20% | 5h | 16.08 |'
- en: '| Ours | 20% | 0.5h | 15.71 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 20% | 0.5h | 15.71 |'
- en: 'Table 4: The time cost and perplexity of LoRA and EBFT on the LlamaV2-7B at
    sparsity of 20%.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在20%稀疏度下，LoRA和EBFT在LlamaV2-7B上的时间成本和困惑度。
- en: '| Model | Param. | Method | ARC-E | ARC-C | PIQA | WinoGrande | StoryCloze
    | Boolq | Mean | wiki.ppl |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 方法 | ARC-E | ARC-C | PIQA | WinoGrande | StoryCloze | Boolq | 平均
    | wiki.ppl |'
- en: '| Lla.1 | 5.5B | LoRA | 64.31 | 37.46 | 76.66 | 64.64 | 77.28 | 71.47 | 65.30
    | 15.46 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Lla.1 | 5.5B | LoRA | 64.31 | 37.46 | 76.66 | 64.64 | 77.28 | 71.47 | 65.30
    | 15.46 |'
- en: '| 5.5B | Ours | 72.52 | 38.65 | 75.46 | 66.46 | 75.63 | 71.19 | 66.65 | 14.81
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 5.5B | 我们的 | 72.52 | 38.65 | 75.46 | 66.46 | 75.63 | 71.19 | 66.65 | 14.81
    |'
- en: '| 5.0B | LoRA | 60.48 | 33.87 | 75.08 | 61.80 | 76.16 | 63.82 | 61.87 | 16.67
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 5.0B | LoRA | 60.48 | 33.87 | 75.08 | 61.80 | 76.16 | 63.82 | 61.87 | 16.67
    |'
- en: '| 5.0B | Ours | 68.31 | 33.96 | 72.85 | 63.85 | 73.45 | 68.90 | 63.55 | 16.27
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 5.0B | 我们的 | 68.31 | 33.96 | 72.85 | 63.85 | 73.45 | 68.90 | 63.55 | 16.27
    |'
- en: '| Lla.2 | 5.5B | LoRA | 64.35 | 34.90 | 75.84 | 62.51 | 75.47 | 50.06 | 60.52
    | 16.08 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Lla.2 | 5.5B | LoRA | 64.35 | 34.90 | 75.84 | 62.51 | 75.47 | 50.06 | 60.52
    | 16.08 |'
- en: '| 5.5B | Ours | 68.81 | 35.24 | 74.81 | 63.93 | 72.31 | 60.73 | 62.64 | 15.71
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 5.5B | 我们的 | 68.81 | 35.24 | 74.81 | 63.93 | 72.31 | 60.73 | 62.64 | 15.71
    |'
- en: '| 5.0B | LoRA | 61.32 | 32.08 | 73.78 | 61.96 | 74.13 | 55.05 | 59.72 | 17.63
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 5.0B | LoRA | 61.32 | 32.08 | 73.78 | 61.96 | 74.13 | 55.05 | 59.72 | 17.63
    |'
- en: '| 5.0B | Ours | 65.74 | 32.76 | 71.87 | 64.40 | 71.04 | 59.39 | 60.87 | 17.63
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5.0B | 我们的 | 65.74 | 32.76 | 71.87 | 64.40 | 71.04 | 59.39 | 60.87 | 17.63
    |'
- en: 'Table 5: The accuracy and perplexity of the fine-tuned LlamaV1-7B and LlamaV2-7B
    models on Wikitext2, as well as their performance on a series of zero-shot tasks.
    The pruned models used in our experiments have parameters set at 5.5B and 5B,
    respectively.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在Wikitext2上的LlamaV1-7B和LlamaV2-7B微调模型的准确率和困惑度，以及它们在一系列零-shot任务上的表现。我们实验中使用的修剪模型参数分别设定为5.5B和5B。
- en: 'We further conducted detailed experiments to compare our method with LoRA.
    We varied the parameters of the pruned models, including LlamaV1-7B and LlamaV2-7B,
    and evaluated the perplexity and accuracy of the fine-tuned models on Wikitext2
    as well as a series of zero-shot tasks. The experimental results are summarized
    in Tab.[5](#S4.T5 "Table 5 ‣ 4.4 EBFT vs. LoRA ‣ 4 Experiments ‣ EBFT: Effective
    and Block-Wise Fine-Tuning for Sparse LLMs"). Indeed, the comparison between EBFT
    and LoRA continues to demonstrate the advantages of EBFT. For example, after fine-tuning
    LlamaV1-5.5B, EBFT achieves a perplexity of 14.81, surpassing LoRA, which achieves
    a perplexity of 15.46 on Wikitext2\. Similarly, for LlamaV2-5.5B, EBFT achieves
    a perplexity of 15.71, outperforming LoRA with a perplexity of 16.08\. This trend
    carries over to the zero-shot tasks as well, where the fine-tuned models using
    EBFT exhibit better performance compared to LoRA. The mean accuracy of our approach
    is higher than that of LoRA, regardless of whether it is applied to LlamaV1 or
    LlamaV2\. While it is true that LoRA may achieve better scores on certain tasks
    such as PIQA and StoryCloze, the overall results consistently support the conclusion
    that the pruned models fine-tuned using EBFT outperform those fine-tuned using
    LoRA. When comparing EBFT to LoRA, EBFT demonstrates faster speed, lower cost,
    and superior performance.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步进行了详细实验，将我们的方法与LoRA进行比较。我们对剪枝模型的参数进行了变化，包括LlamaV1-7B和LlamaV2-7B，并评估了在Wikitext2上的细调模型的困惑度和准确率，以及一系列零样本任务。实验结果总结在表格[5](#S4.T5
    "Table 5 ‣ 4.4 EBFT vs. LoRA ‣ 4 Experiments ‣ EBFT: Effective and Block-Wise
    Fine-Tuning for Sparse LLMs")中。实际上，EBFT与LoRA的比较继续展示了EBFT的优势。例如，在对LlamaV1-5.5B进行细调后，EBFT达到了14.81的困惑度，超过了LoRA在Wikitext2上达到的15.46的困惑度。类似地，对于LlamaV2-5.5B，EBFT的困惑度为15.71，优于LoRA的16.08。这种趋势也延续到了零样本任务中，使用EBFT细调的模型表现出比LoRA更好的性能。无论是应用于LlamaV1还是LlamaV2，我们的方法的平均准确率均高于LoRA。虽然LoRA在PIQA和StoryCloze等特定任务上可能获得更好的分数，但整体结果一致支持这样一个结论：使用EBFT细调的剪枝模型优于使用LoRA细调的模型。在EBFT与LoRA的比较中，EBFT展现了更快的速度、更低的成本和更优的性能。'
- en: 4.5 Weight Tuning vs. Mask Tuning
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 权重调整与掩码调整
- en: 'Some optimization methods for sparse models, such as Zhang et al. ([2023b](#bib.bib47),
    [d](#bib.bib49)), solely update the positions of masks without adjusting weights.
    To explore the effectiveness of this strategy, we conducted experiments to compare
    two fine-tuning strategies: weight tuning and mask tuning.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一些针对稀疏模型的优化方法，例如张等人（[2023b](#bib.bib47), [d](#bib.bib49)），仅更新掩码的位置而不调整权重。为了探讨这种策略的有效性，我们进行了实验，比对了两种细调策略：权重调整和掩码调整。
- en: 'For mask tuning, we employed Eq.[4](#S3.E4 "In 3.2 EBFT ‣ 3 Methodology ‣ EBFT:
    Effective and Block-Wise Fine-Tuning for Sparse LLMs") as the optimization objective,
    aiming to minimize the block-wise reconstruction error. The fine-tuning process
    of mask tuning is the same as that of EBFT, except that mask tuning only updates
    the positions of masks while keeping the weights unchanged. We recorded the experimental
    results in Tab.[6](#S4.T6 "Table 6 ‣ 4.5 Weight Tuning vs. Mask Tuning ‣ 4 Experiments
    ‣ EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs"). Specifically,
    we conducted variations in the sparsity levels of LlamaV1-7B and LlamaV2-7B, and
    evaluated the perplexity of the fine-tuned sparse models on Wikitext2\. The results
    consistently highlight the clear advantage of weight tuning over mask tuning,
    even though the mask tuning method used in this study outperforms the SOTA mask-tuning
    method DSnoT in Tab.[1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ EBFT: Effective and
    Block-Wise Fine-Tuning for Sparse LLMs"). However, mask tuning still falls short
    when compared to EBFT. Regardless of the sparsity level, weight tuning consistently
    outperforms mask tuning. These findings clearly indicate the limitations of mask-tuning
    methods.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于mask tuning，我们采用了Eq.[4](#S3.E4 "在3.2 EBFT ‣ 3 方法 ‣ EBFT：针对稀疏LLMs的有效和块级微调")作为优化目标，旨在最小化块级重建误差。mask
    tuning的微调过程与EBFT相同，只不过mask tuning仅更新mask的位置，而保持权重不变。我们在Tab.[6](#S4.T6 "表6 ‣ 4.5
    权重微调与掩码微调 ‣ 4 实验 ‣ EBFT：针对稀疏LLMs的有效和块级微调")中记录了实验结果。具体来说，我们对LlamaV1-7B和LlamaV2-7B的稀疏级别进行了变化，并评估了在Wikitext2上微调后的稀疏模型的困惑度。结果一致地突出了权重微调相对于掩码微调的明显优势，尽管本研究中使用的掩码微调方法在Tab.[1](#S4.T1
    "表1 ‣ 4 实验 ‣ EBFT：针对稀疏LLMs的有效和块级微调")中优于SOTA掩码微调方法DSnoT。然而，与EBFT相比，掩码微调仍然有所不足。无论稀疏级别如何，权重微调始终优于掩码微调。这些发现清楚地表明了掩码微调方法的局限性。
- en: '| LlamaV1-7B |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| LlamaV1-7B |'
- en: '| Method | 50% | 60% | 70% | 80% | 90% |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 50% | 60% | 70% | 80% | 90% |'
- en: '| w.Mask | 7.05 | 9.15 | 25.90 | 456.0 | 5378 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| w.Mask | 7.05 | 9.15 | 25.90 | 456.0 | 5378 |'
- en: '| w.Weight | 6.81 | 8.59 | 16.88 | 118.4 | 2993 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| w.Weight | 6.81 | 8.59 | 16.88 | 118.4 | 2993 |'
- en: '| LlamaV2-7B |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| LlamaV2-7B |'
- en: '| Method | 50% | 60% | 70% | 80% | 90% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 50% | 60% | 70% | 80% | 90% |'
- en: '| w.Mask | 6.29 | 8.40 | 26.99 | 755.8 | 3793 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| w.Mask | 6.29 | 8.40 | 26.99 | 755.8 | 3793 |'
- en: '| w.Weight | 6.18 | 7.90 | 16.94 | 72.80 | 903.4 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| w.Weight | 6.18 | 7.90 | 16.94 | 72.80 | 903.4 |'
- en: 'Table 6: The Wikitext2 perplexity of mask-tuning and weight-tuning were evaluated
    on LlamaV1-7B and LlamaV2-7B at various sparsity levels with Wanda initialization.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在不同稀疏级别下，使用Wanda初始化评估了LlamaV1-7B和LlamaV2-7B的mask-tuning和weight-tuning的Wikitext2困惑度。
- en: 5 Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We propose EBFT, a unified fine-tuning framework for sparse Language Models
    that can be integrated with any pruning method. In EBFT, we define the block-wise
    reconstruction error and optimize it on a block-by-block basis through backpropagation
    algorithm, aiming to achieve a convergent and optimal solution. This approach
    proves to be effective and efficient, requiring only a small number of samples
    for calibration. Extensive experiments demonstrate that EBFT achieves state-of-the-art
    performance on various benchmark datasets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了EBFT，一个统一的稀疏语言模型微调框架，可以与任何剪枝方法集成。在EBFT中，我们定义了块级重建误差，并通过反向传播算法逐块优化，旨在实现收敛和最优解。这种方法被证明是有效和高效的，仅需少量样本进行校准。大量实验表明，EBFT在各种基准数据集上达到了最先进的性能。
- en: 6 Limitation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: Although the use of a small calibration dataset significantly reduces costs,
    the fine-tuning process of EBFT still incurs computation costs due to gradient
    calculations. In future work, we will continue to focus on fine-tuning with a
    limited number of samples and explore gradient-free methods to further mitigate
    these costs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用小的校准数据集显著降低了成本，但EBFT的微调过程仍然由于梯度计算而产生计算成本。在未来的工作中，我们将继续关注在有限样本下的微调，并探索无梯度方法以进一步降低这些成本。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: An et al. (2023) Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2023.
    Fluctuation-based adaptive structured pruning for large language models. *arXiv
    preprint arXiv:2312.11983*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An等（2023）杨琪·安、徐赵、陶宇、明唐和金桥·王。2023年。基于波动的自适应结构化剪枝用于大型语言模型。*arXiv预印本 arXiv:2312.11983*。
- en: 'Ashkboos et al. (2024) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do
    Nascimento, Torsten Hoefler, and James Hensman. 2024. Slicegpt: Compress large
    language models by deleting rows and columns. *arXiv preprint arXiv:2401.15024*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等（2024）Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento,
    Torsten Hoefler 和 James Hensman。2024。Slicegpt：通过删除行和列来压缩大型语言模型。*arXiv 预印本 arXiv:2401.15024*。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等。2020。Piqa：关于自然语言中物理常识的推理。在*人工智能
    AAAI 会议论文集*，第34卷，第7432–7439页。
- en: Boža (2024) Vladimír Boža. 2024. Fast and optimal weight update for pruned large
    language models. *arXiv preprint arXiv:2401.02938*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boža（2024）Vladimír Boža。2024。修剪的大型语言模型的快速和优化的权重更新。*arXiv 预印本 arXiv:2401.02938*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等。2020。语言模型是少样本学习者。*神经信息处理系统进展*，33：1877–1901。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2019）Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins 和 Kristina Toutanova。2019。Boolq：探索自然是/否问题的惊人困难。*arXiv 预印本 arXiv:1905.10044*。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick 和 Oyvind Tafjord。2018。认为你已经解决了问答问题？尝试 arc，ai2 推理挑战。*arXiv 预印本
    arXiv:1803.05457*。
- en: 'Computer (2023) Together Computer. 2023. [Redpajama: an open dataset for training
    large language models](https://github.com/togethercomputer/RedPajama-Data).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Computer（2023）Together Computer。2023。[Redpajama: 用于训练大型语言模型的开放数据集](https://github.com/togethercomputer/RedPajama-Data)。'
- en: 'Das et al. (2023) Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond
    size: How gradients shape pruning decisions in large language models. *arXiv preprint
    arXiv:2311.04902*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等（2023）Rocktim Jyoti Das, Liqun Ma 和 Zhiqiang Shen。2023。超越规模：梯度如何塑造大型语言模型中的修剪决策。*arXiv
    预印本 arXiv:2311.04902*。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova。2018。Bert：用于语言理解的深度双向变换器的预训练。*arXiv
    预印本 arXiv:1810.04805*。
- en: 'Frankle and Carbin (2018) Jonathan Frankle and Michael Carbin. 2018. The lottery
    ticket hypothesis: Finding sparse, trainable neural networks. *arXiv preprint
    arXiv:1803.03635*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle 和 Carbin（2018）Jonathan Frankle 和 Michael Carbin。2018。彩票票据假设：寻找稀疏的、可训练的神经网络。*arXiv
    预印本 arXiv:1803.03635*。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh（2023）Elias Frantar 和 Dan Alistarh。2023。Sparsegpt：大型语言模型可以在一次性修剪中准确地被修剪。在*国际机器学习大会*，第10323–10337页。PMLR。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。2022。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*。
- en: 'Guo et al. (2023a) Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023a.
    Compresso: Structured pruning with collaborative prompting learns compact large
    language models. *arXiv preprint arXiv:2310.05015*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2023a）Song Guo, Jiahang Xu, Li Lyna Zhang 和 Mao Yang。2023a。Compresso：通过协作提示的结构化修剪学习紧凑的大型语言模型。*arXiv
    预印本 arXiv:2310.05015*。
- en: Guo et al. (2023b) Song Guo, Lei Zhang, Xiawu Zheng, Yan Wang, Yuchao Li, Fei
    Chao, Chenglin Wu, Shengchuan Zhang, and Rongrong Ji. 2023b. Automatic network
    pruning via hilbert-schmidt independence criterion lasso under information bottleneck
    principle. In *Proceedings of the IEEE/CVF international conference on computer
    vision*, pages 17458–17469.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2023b) Song Guo, Lei Zhang, Xiawu Zheng, Yan Wang, Yuchao Li, Fei
    Chao, Chenglin Wu, Shengchuan Zhang, 和 Rongrong Ji. 2023b. 基于信息瓶颈原理的 Hilbert-Schmidt
    独立性准则 Lasso 自动网络剪枝。在*IEEE/CVF 国际计算机视觉会议论文集*，第17458–17469页。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning
    both weights and connections for efficient neural network. *Advances in neural
    information processing systems*, 28.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Song Han, Jeff Pool, John Tran, 和 William Dally. 2015. 学习权重和连接以提高神经网络的效率。*神经信息处理系统进展*,
    28。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. 1993.
    Optimal brain surgeon and general network pruning. In *IEEE international conference
    on neural networks*, pages 293–299\. IEEE.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) Babak Hassibi, David G Stork, 和 Gregory J Wolff. 1993.
    最优脑外科医生和通用网络剪枝。在*IEEE国际神经网络会议*，第293–299页。IEEE。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    2023. 逐步蒸馏！用更少的训练数据和更小的模型尺寸超越更大的语言模型。*arXiv 预印本 arXiv:2305.02301*。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2021. Lora: 大语言模型的低秩适应。*arXiv
    预印本 arXiv:2106.09685*。'
- en: 'Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph
    Naor, and Daniel Soudry. 2021. Accelerated sparse neural training: A provable
    and efficient method to find n: m transposable masks. *Advances in neural information
    processing systems*, 34:21099–21111.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph
    Naor, 和 Daniel Soudry. 2021. 加速稀疏神经网络训练：一种可证明且高效的方法来找到 n: m 可转置掩码。*神经信息处理系统进展*,
    34:21099–21111。'
- en: Kwon et al. (2022) Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. 2022. A fast post-training pruning framework for
    transformers. *Advances in Neural Information Processing Systems*, 35:24101–24116.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. (2022) Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, 和 Amir Gholami. 2022. 一种快速的变换器后训练剪枝框架。*神经信息处理系统进展*, 35:24101–24116。
- en: Li et al. (2016) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. 2016. Pruning filters for efficient convnets. *arXiv preprint arXiv:1608.08710*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2016) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, 和 Hans Peter
    Graf. 2016. 为高效卷积神经网络剪枝过滤器。*arXiv 预印本 arXiv:1608.08710*。
- en: 'Li et al. (2023) Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He,
    Weizhu Chen, and Tuo Zhao. 2023. Losparse: Structured compression of large language
    models based on low-rank and sparse approximation. *arXiv preprint arXiv:2306.11222*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He,
    Weizhu Chen, 和 Tuo Zhao. 2023. Losparse: 基于低秩和稀疏近似的大语言模型结构化压缩。*arXiv 预印本 arXiv:2306.11222*。'
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression
    and acceleration. *arXiv preprint arXiv:2306.00978*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. 2023. Awq: 基于激活的权重量化用于大语言模型的压缩与加速。*arXiv 预印本 arXiv:2306.00978*。'
- en: Louizos et al. (2017) Christos Louizos, Max Welling, and Diederik P Kingma.
    2017. Learning sparse neural networks through $l\_0$ regularization. *arXiv preprint
    arXiv:1712.01312*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louizos et al. (2017) Christos Louizos, Max Welling, 和 Diederik P Kingma. 2017.
    通过 $l\_0$ 正则化学习稀疏神经网络。*arXiv 预印本 arXiv:1712.01312*。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner:
    On the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023. Llm-pruner:
    大语言模型的结构化剪枝。*arXiv 预印本 arXiv:2305.11627*。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 2016. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*。
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis,
    Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story
    cloze test. In *Proceedings of the 2nd Workshop on Linking Models of Lexical,
    Sentential and Discourse-level Semantics*, pages 46–51.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh 等人（2017）纳斯林·穆斯塔法扎德、迈克尔·罗斯、安妮·路易斯、纳撒尼尔·钱伯斯和詹姆斯·艾伦。2017。LSDSEM 2017
    共享任务：故事完形填空测试。在*第二届词汇、句法和话语层次语义模型链接研讨会论文集*，第46–51页。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020）科林·拉费尔、诺阿姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、颜其周、魏力和彼得·J·刘。2020。通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*，21(1)：5485–5551。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等人（2021）坂口圭介、罗南·勒布拉斯、昌德拉·巴伽瓦图拉和叶津·崔。2021。Winogrande：大规模的对抗性 Winograd
    语料库挑战。*ACM 通讯*，64(9)：99–106。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander Rush. 2020. Movement
    pruning: Adaptive sparsity by fine-tuning. *Advances in Neural Information Processing
    Systems*, 33:20378–20389.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人（2020）维克托·桑、托马斯·沃尔夫和亚历山大·拉什。2020。运动剪枝：通过微调实现自适应稀疏性。*神经信息处理系统进展*，33：20378–20389。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. *arXiv preprint
    arXiv:2306.11695*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023）孙明杰、刘庄、安娜·贝尔和 J·齐科·科尔特。2023。大型语言模型的简单而有效的剪枝方法。*arXiv 预印本 arXiv:2306.11695*。
- en: 'Syed et al. (2023) Aaquib Syed, Phillip Huang Guo, and Vijaykaarti Sundarapandiyan.
    2023. Prune and tune: Improving efficient pruning techniques for massive language
    models.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Syed 等人（2023）阿奎布·赛义德、菲利普·黄果和维贾卡提·桑达拉潘迪扬。2023。剪枝与调优：改进大规模语言模型的高效剪枝技术。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford
    alpaca: an instruction-following llama model (2023). *URL https://github. com/tatsu-lab/stanford_alpaca*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人（2023）罗汉·陶瑞、伊沙恩·古尔拉贾尼、张天毅、扬·迪布瓦、李雪晨、卡洛斯·古斯特林、珀西·梁和田中博。2023。斯坦福 alpaca：一个指令跟随的
    llama 模型（2023）。*URL https://github.com/tatsu-lab/stanford_alpaca*。
- en: Tishby et al. (2000) Naftali Tishby, Fernando C Pereira, and William Bialek.
    2000. The information bottleneck method. *arXiv preprint physics/0004057*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby 等人（2000）纳夫塔利·提什比、费尔南多·C·佩雷拉和威廉·比亚莱克。2000。信息瓶颈方法。*arXiv 预印本 physics/0004057*。
- en: Tishby and Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. 2015. Deep learning
    and the information bottleneck principle. In *2015 ieee information theory workshop
    (itw)*, pages 1–5\. IEEE.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby 和 Zaslavsky（2015）纳夫塔利·提什比和诺加·扎斯拉夫斯基。2015。深度学习与信息瓶颈原理。在*2015 IEEE 信息理论研讨会（ITW）*，第1–5页。IEEE。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023a）雨果·图夫朗、提博·拉夫里尔、戈蒂埃·伊扎卡尔、泽维尔·马尔蒂内、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴普蒂斯特·罗齐埃、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿兹哈尔等。2023a。Llama：开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023b）雨果·图夫朗、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马哈伊里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏米亚·巴特拉、普拉杰瓦尔·巴尔加瓦、舒鲁提·博萨尔等。2023b。Llama
    2：开放的基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017）阿希什·瓦斯瓦尼、诺阿姆·沙泽尔、尼基·帕尔马、雅各布·乌斯科雷特、利昂·琼斯、艾登·N·戈麦斯、卢卡斯·凯瑟和伊利亚·波洛苏欣。2017。注意力机制就是你所需要的一切。*神经信息处理系统进展*，30。
- en: Wang et al. (2019) Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019. Structured
    pruning of large language models. *arXiv preprint arXiv:1910.04732*.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019）王梓恒、杰里米·沃尔文德和雷涛。2019。大型语言模型的结构化剪枝。*arXiv 预印本 arXiv:1910.04732*。
- en: 'Werbos (1990) Paul J Werbos. 1990. Backpropagation through time: what it does
    and how to do it. *Proceedings of the IEEE*, 78(10):1550–1560.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Werbos (1990) 保罗 J 韦伯斯. 1990. 通过时间的反向传播: 它的作用及如何实现. *IEEE 会议录*, 78(10):1550–1560.'
- en: 'Xia et al. (2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023.
    Sheared llama: Accelerating language model pre-training via structured pruning.
    *arXiv preprint arXiv:2310.06694*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. (2023) 孟舟 夏, 天宇 高, 智远 曾, 和 丹琪 陈. 2023. Sheared llama: 通过结构化剪枝加速语言模型预训练.
    *arXiv 预印本 arXiv:2310.06694*.'
- en: Xia et al. (2022) Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured
    pruning learns compact and accurate models. *arXiv preprint arXiv:2204.00408*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2022) 孟舟 夏, 泽轩 钟, 和 丹琪 陈. 2022. 结构化剪枝学习紧凑且准确的模型. *arXiv 预印本 arXiv:2204.00408*.
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) 罗温 兹勒斯, 阿里 霍尔茨曼, 乔纳坦 比斯克, 阿里 法赫迪, 和 叶锦 崔. 2019. Hellaswag:
    机器真的能完成你的句子吗? *arXiv 预印本 arXiv:1905.07830*.'
- en: Zhang et al. (2023a) Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi
    Yu, Bohan Zhuang, et al. 2023a. Pruning meets low-rank parameter-efficient fine-tuning.
    *arXiv preprint arXiv:2305.18403*.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023a) 明阳 张, 春华 沈, 臻 杨, 林琳 欧, 心怡 于, 博涵 庄, 等. 2023a. 剪枝与低秩参数高效微调的结合.
    *arXiv 预印本 arXiv:2305.18403*.
- en: 'Zhang et al. (2022) Yuxin Zhang, Mingbao Lin, Zhihang Lin, Yiting Luo, Ke Li,
    Fei Chao, Yongjian Wu, and Rongrong Ji. 2022. Learning best combination for efficient
    n: M sparsity. *Advances in Neural Information Processing Systems*, 35:941–953.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) 余欣 张, 明宝 林, 智航 林, 怡婷 罗, 克 李, 菲 超, 永健 吴, 和 荣荣 纪. 2022. 高效
    n: M 稀疏性的最佳组合学习. *神经信息处理系统进展*, 35:941–953.'
- en: Zhang et al. (2023b) Yuxin Zhang, Mingbao Lin, Yunshan Zhong, Fei Chao, and
    Rongrong Ji. 2023b. Lottery jackpots exist in pre-trained models. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) 余欣 张, 明宝 林, 云杉 钟, 菲 超, 和 荣荣 纪. 2023b. 彩票大奖存在于预训练模型中. *IEEE
    计算机视觉与模式识别学报*.
- en: 'Zhang et al. (2023c) Yuxin Zhang, Yiting Luo, Mingbao Lin, Yunshan Zhong, Jingjing
    Xie, Fei Chao, and Rongrong Ji. 2023c. Bi-directional masks for efficient n: M
    sparse training. *arXiv preprint arXiv:2302.06058*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023c) 余欣 张, 怡婷 罗, 明宝 林, 云杉 钟, 靖靖 谢, 菲 超, 和 荣荣 纪. 2023c. 高效 n:
    M 稀疏训练的双向掩码. *arXiv 预印本 arXiv:2302.06058*.'
- en: 'Zhang et al. (2023d) Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu
    Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. 2023d. Dynamic sparse
    no training: Training-free fine-tuning for sparse llms. *arXiv preprint arXiv:2310.08915*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023d) 余欣 张, 理睿 赵, 明宝 林, 云云 孙, 义武 姚, 兴佳 韩, 贾里德 坦纳, 世伟 刘, 和 荣荣
    纪. 2023d. 动态稀疏无训练: 针对稀疏 llms 的无训练微调. *arXiv 预印本 arXiv:2310.08915*.'
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. 2021. Learning n: m fine-grained structured
    sparse neural networks from scratch. *arXiv preprint arXiv:2102.04010*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2021) 傲君 周, 玉坤 马, 军南 朱, 健博 刘, 智杰 张, 昆 袁, 文秀 孙, 和 红生 李. 2021. 从头学习
    n: m 细粒度结构稀疏神经网络. *arXiv 预印本 arXiv:2102.04010*.'
