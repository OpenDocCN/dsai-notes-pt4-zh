- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:36:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:36:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mechanism Design for LLM Fine-tuning with Multiple Reward Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多奖励模型的LLM微调机制设计
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16276](https://ar5iv.labs.arxiv.org/html/2405.16276)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16276](https://ar5iv.labs.arxiv.org/html/2405.16276)
- en: Haoran Sun
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 孙浩然
- en: Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: sunhaoran0301@stu.pku.edu.cn    Yurong Chen
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: sunhaoran0301@stu.pku.edu.cn    陈宇荣
- en: Peking University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: chenyurong@pku.edu.cn    Siwei Wang
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: chenyurong@pku.edu.cn    王思伟
- en: Microsoft Research Asia
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微软亚洲研究院
- en: siweiwang@microsoft.com    Wei Chen
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: siweiwang@microsoft.com    陈伟
- en: Microsoft Research Asia
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 微软亚洲研究院
- en: weic@microsoft.com    Xiaotie Deng
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: weic@microsoft.com    邓小铁
- en: Peking University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: xiaotie@pku.edu.cn
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: xiaotie@pku.edu.cn
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent research on fine-tuning large language models (LLMs) through the aggregation
    of multiple preferences has attracted considerable attention. However, the existing
    literature predominantly focuses on the empirical performance of aggregation algorithms,
    while neglecting the underlying motivation for agents to misreport their preferences.
    In this paper, we formalize this as a multi-parameter mechanism design problem,
    where an LLM provider designs both training and payment rules to achieve specific
    objectives and promote the truthful reporting of preferences. Firstly, we claim
    the necessity of a payment scheme by demonstrating that without payments, truth-telling
    is a strictly dominated strategy under a wide range of training rules. Then, we
    introduce the affine maximizer payment scheme for the social welfare maximizing
    training rules that are widely used in practice, which ensures both dominant-strategy
    incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we
    prove that under mild conditions, any other payment rule that also implements
    these training rules in DSIC can be converted to the affine maximizer payment
    by adding a factor irrelevant to the agents’ own reports. We also show that this
    mechanism satisfies approximate DSIC when the input of the mechanism is a biased
    version of the reported preferences, showcasing its robustness in real-world applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于通过聚合多个偏好来微调大语言模型（LLMs）的研究引起了广泛关注。然而，现有文献主要集中在聚合算法的实际性能上，而忽略了代理人虚报其偏好的根本动机。在本文中，我们将其形式化为一个多参数机制设计问题，其中LLM提供者设计训练和支付规则，以实现特定目标并促进真实偏好的报告。首先，我们通过展示在广泛的训练规则下，没有支付的情况下真实报告是严格被支配的策略，来说明支付方案的必要性。然后，我们介绍了用于社会福利最大化训练规则的仿射最大化支付方案，这些规则在实践中广泛使用，确保了主导策略激励相容性（DSIC）和个人理性（IR）。此外，我们证明在温和条件下，任何其他也在DSIC中实现这些训练规则的支付规则，都可以通过添加一个与代理人自身报告无关的因子转化为仿射最大化支付方案。我们还展示了当机制的输入是报告偏好的有偏版本时，该机制满足近似DSIC，展示了其在现实世界应用中的鲁棒性。
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The pre-training and fine-tuning paradigm is fundamental in developing language
    models (Devlin et al. ([2018](#bib.bib16)); Radford et al. ([2018](#bib.bib49));
    Liu et al. ([2019](#bib.bib38)); Touvron et al. ([2023](#bib.bib60))). During
    pre-training, the model is fed with vast amounts of data to acquire a general
    capability to understand and generate language through self-supervised learning.
    The subsequent fine-tuning phase customizes these pre-trained models for specific
    downstream tasks using smaller, task-oriented datasets, ensuring that the model
    outputs are more closely aligned with particular requirements. As LLMs gain increasing
    popularity, there is a growing demand for fine-tuning basic LLMs, as basic models
    often fail to meet users’ demands, especially in catering to individual preferences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练和微调范式是开发语言模型的基础（Devlin et al. ([2018](#bib.bib16)); Radford et al. ([2018](#bib.bib49));
    Liu et al. ([2019](#bib.bib38)); Touvron et al. ([2023](#bib.bib60)））。在预训练阶段，模型通过自监督学习接收大量数据，从而获得理解和生成语言的通用能力。随后，微调阶段使用较小的任务导向数据集来定制这些预训练模型，以确保模型的输出更符合特定要求。随着大语言模型（LLMs）的日益流行，对基础LLMs的微调需求也在增长，因为基础模型往往无法满足用户的需求，特别是在迎合个人偏好方面。
- en: The process of fine-tuning an LLM to align with certain human preferences is
    challenging to achieve through supervision (Ji et al. ([2023](#bib.bib34)); Köpf
    et al. ([2024](#bib.bib35)); Wang et al. ([2023b](#bib.bib64)); Shen et al. ([2023](#bib.bib57))),
    primarily due to the difficulty in constructing datasets with a substantial number
    of valid question-answer pairs for supervised training. Reinforcement learning
    from human feedback (RLHF) (Ouyang et al. ([2022](#bib.bib46)); Christiano et al.
    ([2017](#bib.bib10))) offers a promising solution to this problem. In RLHF, a
    reward model is first trained to be used as a proxy for human judgment. This model
    then provides reward signals for the standard reinforcement learning process.
    This technique of fine-tuning with a reward model has proven effective in encoding
    human preferences into models and has become a fundamental component of the training
    process for most advanced LLMs. With the advancement of RLHF, numerous studies
    have investigated efficient methods for aggregating multiple preferences into
    a single fine-tuned model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过监督来微调LLM以符合某些人类偏好是一个具有挑战性的任务（Ji et al. ([2023](#bib.bib34)); Köpf et al. ([2024](#bib.bib35));
    Wang et al. ([2023b](#bib.bib64)); Shen et al. ([2023](#bib.bib57)))，主要由于构建具有大量有效问答对的数据集以进行监督训练的难度。来自人类反馈的强化学习（RLHF）（Ouyang
    et al. ([2022](#bib.bib46)); Christiano et al. ([2017](#bib.bib10)))提供了一个有前途的解决方案。在RLHF中，首先训练一个奖励模型作为人类判断的代理。该模型随后为标准的强化学习过程提供奖励信号。这种使用奖励模型进行微调的技术已被证明在将人类偏好编码到模型中是有效的，并已成为大多数先进LLMs训练过程中的一个基本组成部分。随着RLHF的进展，许多研究已经探讨了将多种偏好聚合到一个微调模型中的高效方法。
- en: However, most of these studies focus primarily on improving empirical performance
    across various metrics (Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Coste et al. ([2023](#bib.bib14)); Zhang et al.
    ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62)); Eisenstein et al. ([2023](#bib.bib21))).
    They often implicitly assume that we are accessible to real preferences, neglecting
    the possibility of agents’ misreporting their preferences. This problem becomes
    more crucial when we consider a real-world scenario, where different agents provide
    their preferences for the aggregation. In such cases, agents may engage in strategic
    misreporting to increase their utility. An intuitive example is that if an agent
    knows beforehand that the fine-tuning process aims to neutralize all preferences,
    it might pretend to have a more polarized preference as a beneficial strategy.
    These strategic behaviors can distort the final training results, even if the
    trained algorithm is highly effective. Nevertheless, this issue has not attracted
    sufficient attention in the existing literature, particularly concerning the fine-tuning
    process of LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数这些研究主要集中在提高各种指标的经验性能上（Ramé et al. ([2024](#bib.bib51)); Wu et al. ([2024](#bib.bib65));
    Jang et al. ([2023](#bib.bib32)); Coste et al. ([2023](#bib.bib14)); Zhang et
    al. ([2024](#bib.bib68)); Wang et al. ([2024](#bib.bib62)); Eisenstein et al.
    ([2023](#bib.bib21)))。它们通常隐含地假设我们可以获得真实的偏好，忽视了代理人可能会误报其偏好的可能性。当我们考虑到现实世界场景时，这个问题变得更加重要，因为不同的代理人提供其偏好进行汇总。在这种情况下，代理人可能会进行战略性误报以增加其效用。一个直观的例子是，如果代理人事先知道微调过程旨在中和所有偏好，它可能会伪装成具有更极端的偏好作为一种有利的策略。这些战略行为可能会扭曲最终的训练结果，即使训练出的算法非常有效。然而，这个问题在现有文献中尚未受到足够的关注，特别是在LLMs的微调过程中。
- en: Our Contribution.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的贡献。
- en: In this paper, we mainly study the incentive design in such scenarios. First,
    we formalize this as a multi-parameter mechanism design problem between a fine-tuning
    service provider and groups of agents seeking fine-tuning services. The provider
    proposes a mechanism that includes a *training rule* for integrating different
    groups’ preferences into a fine-tuned model and a *payment rule* to charge the
    groups. After observing the mechanism, each group strategically reports its preference
    to maximize its utility. We consider that the subsequent fine-tuning process is
    implemented using RLHF, a standard method for aligning a model with human preference.
    Therefore, we abstract the preference of each group to be reward models, and term
    the whole scenario the *RLHF Game*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要研究这类场景中的激励设计。首先，我们将其形式化为一个多参数机制设计问题，该问题涉及一个微调服务提供商和寻求微调服务的多个代理群体。提供商提出了一种机制，其中包括一个*训练规则*，用于将不同群体的偏好整合到微调后的模型中，以及一个*支付规则*，用于向这些群体收费。在观察到该机制后，每个群体会策略性地报告其偏好以最大化自身效用。我们考虑到后续的微调过程是使用RLHF这一标准方法来使模型与人类偏好对齐。因此，我们将每个群体的偏好抽象为奖励模型，并将整个场景称为*RLHF游戏*。
- en: Secondly, we demonstrate the profitability of misreporting a polarized preference
    under a wide range of mechanisms that include only a training rule ([Theorem 3.3](#S3.Thmtheorem3
    "Theorem 3.3\. ‣ 3.1 Necessity of Payment Rule ‣ 3 Incentives for General Training
    Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). This
    underscores the necessity of a payment rule to address incentive issues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们展示了在仅包含训练规则的各种机制下，虚报极化偏好的获利情况 ([定理 3.3](#S3.Thmtheorem3 "定理 3.3. ‣ 3.1
    支付规则的必要性 ‣ 3 一般训练规则的激励 ‣ 具有多个奖励模型的LLM微调的机制设计"))。这突显了支付规则在解决激励问题中的必要性。
- en: Thirdly, we focus on a representative set of training rules, termed the SW-Maximizing
    training rules, in which the provider aims to maximize social welfare while incorporating
    different regularization measures. For SW-Maximizing training rules, we propose
    the affine maximizer payment scheme, a weighted version of the Vickrey-Clarke-Groves
    (VCG) payment (Vickrey ([1961](#bib.bib61)); Clarke ([1971](#bib.bib11)); Groves
    ([1973](#bib.bib26))). We prove that agents truthfully reporting their preferences
    constitutes a dominant strategy in such mechanisms ([Theorem 4.2](#S4.Thmtheorem2
    "Theorem 4.2\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). Utilizing
    the notion of payment equivalence, we prove that under a mild condition, any other
    payment rule that also implements these training rules in dominant-strategy incentive
    compatibility (DSIC) can be converted to the affine maximizer payment by adding
    a factor irrelevant to agents’ own reports ([Theorem 4.5](#S4.Thmtheorem5 "Theorem
    4.5\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing Mechanism ‣
    Mechanism Design for LLM Fine-tuning with Multiple Reward Models")). We validate
    this condition for many commonly used regularization terms like KL-divergence ([Proposition 4.4](#S4.Thmtheorem4
    "Proposition 4.4\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Consequently, we derive the revenue-maximizing payment rule that implements SW-Maximizing
    training rules in both DSIC and individual rationality (IR) ([Corollary 4.6](#S4.Thmtheorem6
    "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣ 4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
    Furthermore, we show that this mechanism remains approximately DSIC when the input
    of the mechanism is a biased version of the reported preferences, which is an
    abstraction modeling for the inevitable errors that occur in practice. This showcases
    the robustness of the proposed mechanisms in real-world applications ([Theorem 4.9](#S4.Thmtheorem9
    "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social Welfare Maximizing Mechanism
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们关注一组具有代表性的训练规则，称为SW-最大化训练规则，其中提供者旨在最大化社会福利，同时结合不同的正则化措施。对于SW-最大化训练规则，我们提出了仿射最大化支付方案，这是Vickrey-Clarke-Groves
    (VCG)支付的加权版本（Vickrey ([1961](#bib.bib61))；Clarke ([1971](#bib.bib11))；Groves ([1973](#bib.bib26))）。我们证明了在这种机制中，代理人真实报告其偏好是一种主导策略（[定理
    4.2](#S4.Thmtheorem2 "定理 4.2. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。利用支付等价的概念，我们证明了在一个温和的条件下，任何其他也实现这些训练规则的支付规则在主导策略激励兼容性（DSIC）下都可以通过添加一个与代理人自身报告无关的因素转化为仿射最大化支付（[定理
    4.5](#S4.Thmtheorem5 "定理 4.5. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。我们验证了这一条件适用于许多常用的正则化项，如KL散度（[命题
    4.4](#S4.Thmtheorem4 "命题 4.4. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。因此，我们推导出了在DSIC和个人理性（IR）下实施SW-最大化训练规则的收益最大化支付规则（[推论
    4.6](#S4.Thmtheorem6 "推论 4.6. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。此外，我们还展示了当机制的输入是报告偏好的偏置版本时，这一机制在近似DSIC下仍然有效，这是一种对实际中不可避免的错误的抽象建模。这展示了所提出机制在现实世界应用中的鲁棒性（[定理
    4.9](#S4.Thmtheorem9 "定理 4.9. ‣ 4.2 近似估值 ‣ 4 社会福利最大化机制 ‣ 具有多个奖励模型的LLM微调的机制设计")）。
- en: Primary Related Work.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要相关工作。
- en: Several studies have investigated similar scenarios. Among them, Duetting et al.
    ([2023](#bib.bib20)) and Soumalias et al. ([2024](#bib.bib58)) are most related
    to ours. Duetting et al. ([2023](#bib.bib20)) examines the problem of designing
    a mechanism to aggregate multiple agents’ preferences based on each agent’s bids
    and determine their payments. However, they exclude the case where preferences
    can be misreported, which is the primary concern in our study. The concurrent
    work by Soumalias et al. ([2024](#bib.bib58)) also considers the mechanism design
    for aggregating multiple preferences. Their focus is mainly on the practical implementation
    of SW-Maximizing training rule with KL-divergence and the payment scheme that
    obtains both DSIC and interpretability. However, in this scenario, we are more
    concerned with the theoretical properties of more general mechanisms, including
    the implementability and the property of payment equivalence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究调查了类似的场景。其中，Duetting 等人 ([2023](#bib.bib20)) 和 Soumalias 等人 ([2024](#bib.bib58))
    与我们的研究最相关。Duetting 等人 ([2023](#bib.bib20)) 研究了设计一个机制以汇总多个代理的偏好，基于每个代理的竞标并确定他们的支付。然而，他们排除了偏好可能被错误报告的情况，这正是我们研究中的主要关注点。Soumalias
    等人 ([2024](#bib.bib58)) 的同期工作也考虑了汇总多个偏好的机制设计。他们的重点主要是实现 SW-Maximizing 训练规则与 KL-divergence
    的实际应用以及获取 DSIC 和解释性的支付方案。然而，在这种情况下，我们更关注的是更一般机制的理论性质，包括可实现性和支付等效性。
- en: Additionally, there are works studying other scenarios related to LLMs from
    the perspective of algorithmic game theory. Laufer et al. ([2023](#bib.bib36))
    abstracts the fine-tuning process as a bargaining game and characterizes the perfect
    sub-game equilibria. Dubey et al. ([2024](#bib.bib19)) proposes an auction where
    bidders compete to place their content within a summary generated by an LLM. Conitzer
    et al. ([2024](#bib.bib13)) considers incorporating social choice theory in LLM
    alignment. Feizi et al. ([2023](#bib.bib23)) explores the potential for leveraging
    LLMs in online advertising systems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些研究从算法博弈论的角度研究与LLMs相关的其他场景。Laufer 等人 ([2023](#bib.bib36)) 将微调过程抽象为一种讨价还价游戏，并描述了完美子博弈均衡。Dubey
    等人 ([2024](#bib.bib19)) 提出了一个拍卖，其中竞标者竞争将他们的内容放入由LLM生成的摘要中。Conitzer 等人 ([2024](#bib.bib13))
    考虑在LLM对齐中引入社会选择理论。Feizi 等人 ([2023](#bib.bib23)) 探讨了在在线广告系统中利用LLMs的潜力。
- en: Paper Organization.
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 论文组织。
- en: In [Section 2](#S2 "2 Preliminaries and Model ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), we provide the preliminaries and the formal description
    of the RLHF Game. In [Section 3](#S3 "3 Incentives for General Training Rules
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models"), we study
    the incentive design for general training rules in the RLHF Game. We demonstrate
    the properties of mechanisms that consist of SW-Maximizing training rules and
    payment rules in [Section 4](#S4 "4 Social Welfare Maximizing Mechanism ‣ Mechanism
    Design for LLM Fine-tuning with Multiple Reward Models"). Further related work
    is provided in [Section 5](#S5 "5 Further Related Work ‣ Mechanism Design for
    LLM Fine-tuning with Multiple Reward Models"), and we conclude in [Section 6](#S6
    "6 Discussion and Conclusion ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第2节](#S2 "2 Preliminaries and Model ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models")，我们提供了预备知识和RLHF游戏的正式描述。在 [第3节](#S3 "3 Incentives
    for General Training Rules ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")，我们研究了RLHF游戏中通用训练规则的激励设计。我们在 [第4节](#S4 "4 Social Welfare Maximizing
    Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
    中展示了由 SW-Maximizing 训练规则和支付规则组成的机制的性质。进一步相关工作在 [第5节](#S5 "5 Further Related Work
    ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models") 中提供，我们在 [第6节](#S6
    "6 Discussion and Conclusion ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models") 中总结。
- en: 2 Preliminaries and Model
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识与模型
- en: 2.1 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 预备知识
- en: Large Language Models.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: Large language models (LLMs) function as mappings from a sequence of tokens
    to a probability distribution over the next token. The input sequence is usually
    constrained by a maximum length $K$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）作为从一系列标记到下一个标记的概率分布的映射进行工作。输入序列通常受到最大长度 $K$ 的限制。
- en: 'An LLM parameterized by $\theta\in\Theta$. For practical purposes, the output
    sequence is also required to be of finite length. We assume the maximum output
    length is also $K$ generated by $g_{\theta}$ is given by:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由 $\theta\in\Theta$ 参数化的LLM。为了实际目的，输出序列也要求是有限长度的。我们假设由 $g_{\theta}$ 生成的最大输出长度也是
    $K$，公式如下：
- en: '|  | $\text{LLM}_{\theta}({\bm{x}})=\prod_{t=1}^{&#124;\bm{x}&#124;}g_{\theta}(x_{t}\mid{\bm{x}}_{, , we have
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $$\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})>，我们有
- en: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})}\leq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{1})}.$ |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial D}{\partial\text{LLM}_{\theta^{\prime}}({\bm{x}}_{1})}\leq\frac{\partial
    D}{\partial\text{LLM}_{\theta}({\bm{x}}_{1})}.$ |  |'
- en: With $\text{rm}^{\prime}_{i}({\bm{x}}_{1})>. However, since for all $。然而，由于对于所有 , we have
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $$\delta>，我们有
- en: '|  | $1$2 |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '(2) For $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}(p({\bm{x}})-q({\bm{x}}))^{2}$
    as an optimization problem as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于 $D(p||q)=\sum_{{\bm{x}}\in T^{\ast}}(p({\bm{x}})-q({\bm{x}}))^{2}$ 作为以下优化问题：
- en: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{x\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}(x)$ |  |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=\arg\max_{\theta\in\Theta}\sum_{x\in
    T^{\ast}}\Bigg{(}\text{LLM}_{\theta}(x)$ |  |'
- en: '|  | $\displaystyle\text{s.t.}\quad\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)$
    |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{s.t.}\quad\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)$
    |  |'
- en: '|  | $\displaystyle\text{LLM}_{\theta}(x)$ |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{LLM}_{\theta}(x)$ |  |'
- en: Since we have assumed that the optimal point is unique, and the optimal model
    $\text{LLM}_{\theta}$ is that there exists $\mu\in\mathbb{R}$, such that
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设最优点是唯一的，并且最优模型 $\text{LLM}_{\theta}$ 存在 $\mu\in\mathbb{R}$，使得
- en: '|  | $1$2 |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Similarly, for the input $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$
    satisfies
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于输入 $(\overrightarrow{\text{rm}}^{\prime},\vec{w}^{\prime})$ 满足
- en: '|  | $1$2 |  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For convenience, we define $\Delta(x)=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)-\sum_{i=1}^{n}w_{i}\text{rm}_{i}(x)$
    is given by
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们定义 $\Delta(x)=\sum_{i=1}^{n}w^{\prime}_{i}\text{rm}^{\prime}_{i}(x)-\sum_{i=1}^{n}w_{i}\text{rm}_{i}(x)$
    由
- en: '|  | $\text{LLM}_{\theta^{\prime}}(x)=\text{LLM}_{\theta}(x)+\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime}).$
    |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{LLM}_{\theta^{\prime}}(x)=\text{LLM}_{\theta}(x)+\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime}).$
    |  |'
- en: Note that we also have the condition
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还有条件
- en: '|  | $1$2 |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Since $\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)=1$, we can infer that
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\sum_{x\in T^{\ast}}\text{LLM}_{\theta}(x)=1$，我们可以推断出
- en: '|  | $\displaystyle\sum_{x\in T^{\ast}}\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime})=0.$
    |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{x\in T^{\ast}}\frac{1}{2\lambda}(\Delta(x)+\mu-\mu^{\prime})=0.$
    |  |'
- en: This is equivalent to
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于
- en: '|  | $\displaystyle\mu^{\prime}-\mu=\frac{1}{&#124;T^{\ast}&#124;}\sum_{x\in
    T^{\ast}}\Delta(x).$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mu^{\prime}-\mu=\frac{1}{&#124;T^{\ast}&#124;}\sum_{x\in
    T^{\ast}}\Delta(x).$ |  |'
- en: Thus, the difference for $\text{LLM}_{\theta}(x)$ can be bounded by
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\text{LLM}_{\theta}(x)$ 的差异可以被界定为
- en: '|  | $1$2 |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: For any  |  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle> |  |'
- en: 'Note that $\text{rm}^{*}(x)=\frac{1}{|T^{\ast}|}$. Thus, the above equation
    can rewritten as:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到$\text{rm}^{*}(x)=\frac{1}{|T^{\ast}|}$。因此，上述方程可以重新写成：
- en: '|  |  | $1$2 |  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | .
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 根据的任意性。
- en: Therefore, it is proven that
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已证明
- en: '|  | $V(t_{i},t^{\prime}_{i})+V(t_{i},t^{\prime}_{i})=0.$ |  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(t_{i},t^{\prime}_{i})+V(t_{i},t^{\prime}_{i})=0.$ |  |'
- en: which means that $V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i})$. ∎
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着$V(t_{i},t^{\prime}_{i})=-V(t^{\prime}_{i},t_{i})$。∎
- en: See [4.6](#S4.Thmtheorem6 "Corollary 4.6\. ‣ 4.1 Affine Maximizer Payment ‣
    4 Social Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with
    Multiple Reward Models")
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 见[4.6](#S4.Thmtheorem6 "推论 4.6\. ‣ 4.1 仿射最大化支付 ‣ 4 社会福利最大化机制 ‣ 多奖励模型的LLM微调机制设计")
- en: Proof.
  id: totrans-476
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Given the payment equivalence of ${\psi}$ here.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这里的${\psi}$的支付等价性。
- en: '|  | $\displaystyle\max_{h_{i}}\quad$ |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{h_{i}}\quad$ |  |'
- en: '|  | s.t. | $1$2 |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $1$2 |  |'
- en: The solution of this programming can be trivially given by,
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 这个编程的解可以很简单地给出，
- en: '|  | $\displaystyle h_{i}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=$
    |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{i}(\overrightarrow{\text{rm}}_{-i},\vec{w}_{-i},\theta_{\text{init}})=$
    |  |'
- en: '|  | $\displaystyle=:$ |  |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=:$ |  |'
- en: Therefore, the revenue-maximizing payment is
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，收入最大化支付是
- en: '|  | $\displaystyle p_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})=$
    |  |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}((\text{rm}_{i},\overrightarrow{\text{rm}}_{-i}),(w_{i},\vec{w}_{-i}),\theta_{\text{init}})=$
    |  |'
- en: '|  | $\displaystyle+$ |  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ |  |'
- en: ∎
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Lemma B.2.
  id: totrans-487
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 B.2。
- en: For any $\text{rm},\text{rm}^{\prime}$, we have
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $\text{rm},\text{rm}^{\prime}$，我们有
- en: '|  | $&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;\leq\epsilon$
    |  |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left|v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})\right|\leq\epsilon$
    |  |'
- en: Proof.
  id: totrans-490
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We can derive that
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以推导出
- en: '|  | $\displaystyle&#124;v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})&#124;$
    |  |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left|v(\theta;\text{rm})-v(\theta;\text{rm}^{\prime})\right|$
    |  |'
- en: '|  |  | $\displaystyle\leq\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\epsilon=\epsilon.$
    |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\sum_{{\bm{x}}\in T^{\ast}}\text{LLM}_{\theta}({\bm{x}})\epsilon=\epsilon.$
    |  |'
- en: ∎
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.8](#S4.Thmtheorem8 "Lemma 4.8\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.8](#S4.Thmtheorem8 "Lemma 4.8. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
- en: Proof.
  id: totrans-496
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $\hat{\theta}={\psi}(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta_{\text{init}})$.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\hat{\theta}={\psi}(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta_{\text{init}})$。
- en: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\hat{\theta};\theta_{\text{init}})$
    |  |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle ASW(\overrightarrow{\text{rm}},\vec{w},\hat{\theta};\theta_{\text{init}})$
    |  |'
- en: '|  |  | $\displaystyle\overset{(1)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\hat{\theta};\widehat{\text{rm}}_{i})-\epsilon\right)-\lambda
    D(\hat{\theta}&#124;&#124;\theta_{\text{init}})$ |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(1)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\hat{\theta};\widehat{\text{rm}}_{i})-\epsilon\right)-\lambda
    D(\hat{\theta}||\theta_{\text{init}})$ |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\hat{\theta};\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\hat{\theta};\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
- en: '|  |  | $\displaystyle\overset{(2)}{\geq}ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(2)}{\geq}ASW(\overrightarrow{\widehat{\text{rm}}},\vec{w},\theta;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$
    |  |'
- en: '|  |  | $\displaystyle=\sum_{i=1}^{n}w_{i}v_{i}(\theta;\widehat{\text{rm}}_{i})-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\sum_{i=1}^{n}w_{i}v_{i}(\theta;\widehat{\text{rm}}_{i})-\lambda
    D(\theta||\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
- en: '|  |  | $\displaystyle\overset{(3)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\theta;\text{rm}_{i})-\epsilon\right)-\lambda
    D(\theta&#124;&#124;\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\overset{(3)}{\geq}\sum_{i=1}^{n}w_{i}\left(v_{i}(\theta;\text{rm}_{i})-\epsilon\right)-\lambda
    D(\theta||\theta_{\text{init}})-\sum_{i=1}^{n}w_{i}\epsilon$ |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-2\sum_{i=1}^{n}w_{i}\epsilon.$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-2\sum_{i=1}^{n}w_{i}\epsilon.$
    |  |'
- en: (1) and (3) can be directly induced by [Lemma B.2](#A2.Thmtheorem2 "Lemma B.2\.
    ‣ Appendix B Omitted proof in Section 4 ‣ Mechanism Design for LLM Fine-tuning
    with Multiple Reward Models"), and (2) holds by the definition of $\hat{\theta}$.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 和 (3) 可以直接由 [引理 B.2](#A2.Thmtheorem2 "Lemma B.2. ‣ Appendix B Omitted proof
    in Section 4 ‣ Mechanism Design for LLM Fine-tuning with Multiple Reward Models")
    推导，(2) 由 $\hat{\theta}$ 的定义得出。
- en: '|  | $1$2 |  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: See [4.9](#S4.Thmtheorem9 "Theorem 4.9\. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4.9](#S4.Thmtheorem9 "Theorem 4.9. ‣ 4.2 Approximate Valuation ‣ 4 Social
    Welfare Maximizing Mechanism ‣ Mechanism Design for LLM Fine-tuning with Multiple
    Reward Models")
- en: Proof.
  id: totrans-509
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Recall that the calculation of payment in $p^{AFF}$ is
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下 $p^{AFF}$ 中支付的计算
- en: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}^{AFF}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}})=ASW_{-i}$
    |  |'
- en: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},{\psi}(\overrightarrow{\text{rm}},\vec{w},\theta_{\text{init}});\theta_{\text{init}}).$
    |  |'
- en: 'Let $\vec{w}=(w_{i},\vec{w}_{-i})$, the utility function can be written as:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\vec{w}=(w_{i},\vec{w}_{-i})$，效用函数可以写成：
- en: '|  | $\displaystyle u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w};{\psi},p,\text{rm}_{i},w_{i})$
    |  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u_{i}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w};{\psi},p,\text{rm}_{i},w_{i})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta_{-i};\theta_{\text{init}}),$
    |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=ASW(\overrightarrow{\text{rm}},\vec{w},\theta;\theta_{\text{init}})-ASW_{-i}(\overrightarrow{\text{rm}},\vec{w},\theta_{-i};\theta_{\text{init}}),$
    |  |'
- en: where we define $\theta={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w},\theta_{\text{init}})$
    or $w_{i}$.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 $\theta={\psi}((\text{rm}^{\prime}_{i},\overrightarrow{\text{rm}}_{-i}),\vec{w},\theta_{\text{init}})$
    或 $w_{i}$。
- en: 'Therefore, we can derive that:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以推导出：
- en: '|  |  | $1$2 |  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\overset{(1)}{\geq}$ |  |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(1)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(2)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(3)}{\geq}$ |  |'
- en: '|  | $\displaystyle\overset{(4)}{=}$ |  |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(4)}{=}$ |  |'
- en: '|  | $\displaystyle\overset{(5)}{\geq}$ |  |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overset{(5)}{\geq}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: All the $\hat{\theta}$ and $\widehat{\text{rm}}_{i}$.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的 $\hat{\theta}$ 和 $\widehat{\text{rm}}_{i}$。
- en: Therefore, we get
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
