- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:34:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年9月8日 18:34:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础到突破的最终指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13296](https://ar5iv.labs.arxiv.org/html/2408.13296)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13296](https://ar5iv.labs.arxiv.org/html/2408.13296)
- en: Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and Arsalan
    Shahid
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, 和 Arsalan Shahid
- en: '@ CeADAR Connect Group CeADAR: Ireland’s Centre for AI, University College
    Dublin, Belfield, Dublin, Ireland'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '@ CeADAR Connect Group CeADAR: 爱尔兰都柏林大学人工智能中心，贝尔菲尔德，都柏林，爱尔兰'
- en: '{ venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie(  August
    2024)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{ venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie(
    2024年8月)'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This technical report thoroughly examines the process of fine-tuning Large Language
    Models (LLMs), integrating theoretical insights and practical applications. It
    begins by tracing the historical development of LLMs, emphasising their evolution
    from traditional Natural Language Processing (NLP) models and their pivotal role
    in modern AI systems. The analysis differentiates between various fine-tuning
    methodologies, including supervised, unsupervised, and instruction-based approaches,
    underscoring their respective implications for specific tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术报告全面审视了大型语言模型（LLMs）的微调过程，整合了理论洞察和实际应用。报告首先追溯了LLMs的历史发展，强调了它们从传统自然语言处理（NLP）模型的演变以及在现代人工智能系统中的关键作用。分析区分了各种微调方法，包括监督、无监督和基于指令的方法，突出了它们在特定任务中的影响。
- en: A structured seven-stage pipeline for LLM fine-tuning is introduced, covering
    the complete lifecycle from data preparation to model deployment. Key considerations
    include data collection strategies, handling of imbalanced datasets, model initialisation,
    and optimisation techniques, with a particular focus on hyperparameter tuning.
    The report also highlights parameter-efficient fine-tuning methods such as Low-Rank
    Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with
    optimal model performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了一种结构化的七阶段LLM微调流程，涵盖了从数据准备到模型部署的完整生命周期。关键考虑因素包括数据收集策略、处理不平衡数据集、模型初始化和优化技术，特别关注超参数调优。报告还强调了如低秩适应（LoRA）和半微调等参数高效的微调方法，这些方法在资源限制与模型性能优化之间取得了平衡。
- en: The exploration extends to advanced fine-tuning techniques and configurations
    like memory fine-tuning, Mixture of Experts (MoE) and Mixture of Agents (MoA),
    demonstrating how these methods harness specialised networks and multi-agent collaboration
    for improved outcomes. Proximal Policy Optimisation (PPO) and Direct Preference
    Optimisation (DPO) are discussed as innovative approaches to aligning models with
    human preferences, while the benefits of pruning and routing optimisations are
    examined for enhancing efficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 探索扩展到先进的微调技术和配置，如记忆微调、专家混合（MoE）和代理混合（MoA），展示了这些方法如何利用专门的网络和多代理协作以提高结果。讨论了近端策略优化（PPO）和直接偏好优化（DPO）作为将模型与人类偏好对齐的创新方法，同时探讨了修剪和路由优化的好处，以提高效率。
- en: In the latter sections, the report delves into validation frameworks, post-deployment
    monitoring, and optimisation techniques for inference. It also addresses the deployment
    of LLMs on distributed and cloud-based platforms. Additionally, cutting-edge topics
    such as multimodal LLMs and fine-tuning for audio and speech processing are covered,
    alongside emerging challenges related to scalability, privacy, and accountability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，报告深入探讨了验证框架、部署后的监控以及推理优化技术。还涉及了在分布式和基于云的平台上部署LLMs。此外，报告涵盖了前沿主题，如多模态LLMs以及针对音频和语音处理的微调，同时讨论了与可扩展性、隐私和问责制相关的新兴挑战。
- en: This report aims to serve as a comprehensive guide for researchers and practitioners,
    offering actionable insights into fine-tuning LLMs while navigating the challenges
    and opportunities inherent in this rapidly evolving field.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告旨在为研究人员和从业人员提供全面的指南，提供有关微调LLMs的可操作洞察，同时应对这一快速发展领域中的挑战和机遇。
- en: Contents
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#Ch1 "In The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#Ch1 "从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本 1.0）")'
- en: '[1.1 Background of Large Language Models (LLMs)](#Ch1.S1 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.1 大型语言模型 (LLMs) 背景](#Ch1.S1 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.2 Historical Development and Key Milestones](#Ch1.S2 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.2 历史发展与关键里程碑](#Ch1.S2 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs](#Ch1.S3
    "In Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3 从传统NLP模型到最先进的LLMs](#Ch1.S3 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.1 Statistical Language Models (SLMs)](#Ch1.S3.SS1 "In 1.3 Evolution from
    Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.1 统计语言模型 (SLMs)](#Ch1.S3.SS1 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.2 Neural Language Models (NLMs)](#Ch1.S3.SS2 "In 1.3 Evolution from Traditional
    NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.2 神经语言模型 (NLMs)](#Ch1.S3.SS2 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.3 Pre-trained Language Models (PLMs)](#Ch1.S3.SS3 "In 1.3 Evolution from
    Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.3 预训练语言模型 (PLMs)](#Ch1.S3.SS3 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.3.4 Large Language Models (LLMs)](#Ch1.S3.SS4 "In 1.3 Evolution from Traditional
    NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.3.4 大型语言模型 (LLMs)](#Ch1.S3.SS4 "1.3 从传统NLP模型到最先进的LLMs ‣ 第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.4 Overview of Current Leading LLMs](#Ch1.S4 "In Chapter 1 Introduction ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.4 当前领先LLMs概述](#Ch1.S4 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本
    1.0）")'
- en: '[1.5 What is Fine-Tuning?](#Ch1.S5 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.5 什么是微调？](#Ch1.S5 "第1章 引言 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的全面回顾（版本 1.0）")'
- en: '[1.6 Types of LLM Fine-Tuning](#Ch1.S6 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6 LLM微调类型](#Ch1.S6 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.1 Unsupervised Fine-Tuning](#Ch1.S6.SS1 "In 1.6 Types of LLM Fine-Tuning
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.1 无监督微调](#Ch1.S6.SS1 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.2 Supervised Fine-Tuning (SFT)](#Ch1.S6.SS2 "In 1.6 Types of LLM Fine-Tuning
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.2 有监督微调（SFT）](#Ch1.S6.SS2 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.6.3 Instruction Fine-Tuning via Prompt Engineering](#Ch1.S6.SS3 "In 1.6
    Types of LLM Fine-Tuning ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.6.3 通过提示工程进行指令微调](#Ch1.S6.SS3 "在1.6 LLM微调类型 ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.7 Pre-training vs Fine-tuning](#Ch1.S7 "In Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.7 预训练与微调](#Ch1.S7 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.8 Importance of Fine-Tuning LLMs](#Ch1.S8 "In Chapter 1 Introduction ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.8 微调LLM的重要性](#Ch1.S8 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9 Retrieval Augmented Generation (RAG)](#Ch1.S9 "In Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9 检索增强生成（RAG）](#Ch1.S9 "在第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.1 Traditional RAG Pipeline and Steps](#Ch1.S9.SS1 "In 1.9 Retrieval Augmented
    Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.1 传统RAG流程与步骤](#Ch1.S9.SS1 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.2 Benefits of Using RAG](#Ch1.S9.SS2 "In 1.9 Retrieval Augmented Generation
    (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.2 使用RAG的好处](#Ch1.S9.SS2 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.3 Challenges and Considerations in Serving RAG](#Ch1.S9.SS3 "In 1.9 Retrieval
    Augmented Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.3 服务RAG的挑战与考虑](#Ch1.S9.SS3 "在1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调全面评审，涵盖技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")'
- en: '[1.9.4 Use Cases and Examples](#Ch1.S9.SS4 "In 1.9 Retrieval Augmented Generation
    (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.4 用例和示例](#Ch1.S9.SS4 "在 1.9 检索增强生成（RAG） ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.9.5 Considerations for Choosing Between RAG and Fine-Tuning](#Ch1.S9.SS5
    "In 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.9.5 选择 RAG 和微调的考虑因素](#Ch1.S9.SS5 "在 1.9 检索增强生成（RAG） ‣ 第 1 章 引言 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[1.10 Objectives of the Report](#Ch1.S10 "In Chapter 1 Introduction ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10 报告目标](#Ch1.S10 "在第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.1 Goals and Scope](#Ch1.S10.SS1 "In 1.10 Objectives of the Report ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.1 目标和范围](#Ch1.S10.SS1 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.2 Key Questions and Issues Addressed](#Ch1.S10.SS2 "In 1.10 Objectives
    of the Report ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.2 关键问题和解决的议题](#Ch1.S10.SS2 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[1.10.3 Overview of the Report Structure](#Ch1.S10.SS3 "In 1.10 Objectives
    of the Report ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.10.3 报告结构概述](#Ch1.S10.SS3 "在 1.10 报告目标 ‣ 第 1 章 引言 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2 Seven Stage Fine-Tuning Pipeline for LLM](#Ch2 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 七阶段 LLM 微调流程](#Ch2 "在从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[2.1 Stage 1: Dataset Preparation](#Ch2.S1 "In Chapter 2 Seven Stage Fine-Tuning
    Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 第一阶段：数据集准备](#Ch2.S1 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.2 Stage 2: Model Initialisation](#Ch2.S2 "In Chapter 2 Seven Stage Fine-Tuning
    Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 第二阶段：模型初始化](#Ch2.S2 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.3 Stage 3: Training Environment Setup](#Ch2.S3 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.3 第三阶段：训练环境设置](#Ch2.S3 "在第 2 章 七阶段 LLM 微调流程 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[2.4 Stage 4: Partial or Full Fine-Tuning](#Ch2.S4 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.4 第4阶段：部分或完全微调](#Ch2.S4 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.5 Stage 5: Evaluation and Validation](#Ch2.S5 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.5 第5阶段：评估与验证](#Ch2.S5 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.6 Stage 6: Deployment](#Ch2.S6 "In Chapter 2 Seven Stage Fine-Tuning Pipeline
    for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.6 第6阶段：部署](#Ch2.S6 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[2.7 Stage 7: Monitoring and Maintenance](#Ch2.S7 "In Chapter 2 Seven Stage
    Fine-Tuning Pipeline for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.7 第7阶段：监控与维护](#Ch2.S7 "在第2章 LLM的七阶段微调流程 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3 Stage 1: Data Preparation](#Ch3 "In The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 第1阶段：数据准备](#Ch3 "在《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1 Steps Involved in Data Preparation](#Ch3.S1 "In Chapter 3 Stage 1: Data
    Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 数据准备的步骤](#Ch3.S1 "在第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.1 Data Collection](#Ch3.S1.SS1 "In 3.1 Steps Involved in Data Preparation
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.1 数据收集](#Ch3.S1.SS1 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.2 Data Preprocessing and Formatting](#Ch3.S1.SS2 "In 3.1 Steps Involved
    in Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.2 数据预处理与格式化](#Ch3.S1.SS2 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.3 Handling Data Imbalance](#Ch3.S1.SS3 "In 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.3 处理数据不平衡](#Ch3.S1.SS3 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.1.4 Splitting Dataset](#Ch3.S1.SS4 "In 3.1 Steps Involved in Data Preparation
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1.4 数据集拆分](#Ch3.S1.SS4 "在3.1 数据准备的步骤 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评述（版本1.0）》")'
- en: '[3.2 Existing and Potential Research Methodologies](#Ch3.S2 "In Chapter 3 Stage
    1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 现有和潜在的研究方法](#Ch3.S2 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.1 Data Annotation](#Ch3.S2.SS1 "In 3.2 Existing and Potential Research
    Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.1 数据标注](#Ch3.S2.SS1 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.2 Data Augmentation](#Ch3.S2.SS2 "In 3.2 Existing and Potential Research
    Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.2 数据增强](#Ch3.S2.SS2 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.2.3 Synthetic Data Generation using LLMs](#Ch3.S2.SS3 "In 3.2 Existing and
    Potential Research Methodologies ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2.3 使用LLMs生成合成数据](#Ch3.S2.SS3 "在3.2 现有和潜在的研究方法 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.3 Challenges in Data Preparation for Fine-Tuning LLMs](#Ch3.S3 "In Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 数据准备中的挑战](#Ch3.S3 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.4 Available LLM Fine-Tuning Datasets](#Ch3.S4 "In Chapter 3 Stage 1: Data
    Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 可用的LLM微调数据集](#Ch3.S4 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5 Best Practices](#Ch3.S5 "In Chapter 3 Stage 1: Data Preparation ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5 最佳实践](#Ch3.S5 "在第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.1 High-Quality Data Collection](#Ch3.S5.SS1 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.1 高质量数据收集](#Ch3.S5.SS1 "在3.5 最佳实践 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.2 Effective Data Preprocessing](#Ch3.S5.SS2 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.2 有效的数据预处理](#Ch3.S5.SS2 "在3.5 最佳实践 ‣ 第3章第1阶段：数据准备 ‣ 从基础到突破的终极指南：LLMs微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")'
- en: '[3.5.3 Managing Data Imbalance](#Ch3.S5.SS3 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.3 数据不平衡管理](#Ch3.S5.SS3 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.4 Augmenting and Annotating Data](#Ch3.S5.SS4 "In 3.5 Best Practices ‣
    Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.4 数据增强与注释](#Ch3.S5.SS4 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.5 Ethical Data Handling](#Ch3.S5.SS5 "In 3.5 Best Practices ‣ Chapter
    3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.5 伦理数据处理](#Ch3.S5.SS5 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[3.5.6 Regular Evaluation and Iteration](#Ch3.S5.SS6 "In 3.5 Best Practices
    ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.5.6 定期评估与迭代](#Ch3.S5.SS6 "在3.5 最佳实践 ‣ 第3章 第1阶段：数据准备 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4 Stage 2: Model Initialisation](#Ch4 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 第2阶段：模型初始化](#Ch4 "在《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.1 Steps Involved in Model Initialisation](#Ch4.S1 "In Chapter 4 Stage 2:
    Model Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 模型初始化的步骤](#Ch4.S1 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.2 Tools and Libraries for Model Initialisation](#Ch4.S2 "In Chapter 4 Stage
    2: Model Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 模型初始化的工具和库](#Ch4.S2 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.3 Challenges in Model Initialisation](#Ch4.S3 "In Chapter 4 Stage 2: Model
    Initialisation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 模型初始化中的挑战](#Ch4.S3 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[4.4 Tutorials](#Ch4.S4 "In Chapter 4 Stage 2: Model Initialisation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 教程](#Ch4.S4 "在第4章 第2阶段：模型初始化 ‣ 《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[5 Stage 3: Training Setup](#Ch5 "In The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 第3阶段：训练设置](#Ch5 "在《从基础到突破的终极微调LLMs指南：技术、研究、最佳实践、应用研究挑战与机会的详尽回顾（第1.0版）》")'
- en: '[5.1 Steps Involved in Training Setup](#Ch5.S1 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 训练设置中的步骤](#Ch5.S1 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.2 Setting up Training Environment](#Ch5.S2 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 设置训练环境](#Ch5.S2 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.3 Defining Hyperparameters](#Ch5.S3 "In Chapter 5 Stage 3: Training Setup
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 定义超参数](#Ch5.S3 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.3.1 Methods for Hyperparameter Tuning](#Ch5.S3.SS1 "In 5.3 Defining Hyperparameters
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3.1 超参数调优方法](#Ch5.S3.SS1 "在 5.3 定义超参数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4 Initialising Optimisers and Loss Functions](#Ch5.S4 "In Chapter 5 Stage
    3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 初始化优化器和损失函数](#Ch5.S4 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.1 Gradient Descent](#Ch5.S4.SS1 "In 5.4 Initialising Optimisers and Loss
    Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.1 梯度下降](#Ch5.S4.SS1 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.2 Stochastic Gradient Descent (SGD)](#Ch5.S4.SS2 "In 5.4 Initialising
    Optimisers and Loss Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.2 随机梯度下降（SGD）](#Ch5.S4.SS2 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.3 Mini-batch Gradient Descent](#Ch5.S4.SS3 "In 5.4 Initialising Optimisers
    and Loss Functions ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.3 小批量梯度下降](#Ch5.S4.SS3 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.4 AdaGrad](#Ch5.S4.SS4 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.4 AdaGrad](#Ch5.S4.SS4 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的最终指南：全面回顾技术、研究、最佳实践、应用研究挑战和机遇（版本
    1.0）")'
- en: '[5.4.5 RMSprop](#Ch5.S4.SS5 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.5 RMSprop](#Ch5.S4.SS5 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.6 AdaDelta](#Ch5.S4.SS6 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.6 AdaDelta](#Ch5.S4.SS6 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.7 Adam](#Ch5.S4.SS7 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.7 Adam](#Ch5.S4.SS7 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM
    微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.4.8 AdamW](#Ch5.S4.SS8 "In 5.4 Initialising Optimisers and Loss Functions
    ‣ Chapter 5 Stage 3: Training Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4.8 AdamW](#Ch5.S4.SS8 "在 5.4 初始化优化器和损失函数 ‣ 第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的
    LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[5.5 Challenges in Training Setup](#Ch5.S5 "In Chapter 5 Stage 3: Training
    Setup ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5 训练设置中的挑战](#Ch5.S5 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[5.6 Best Practices](#Ch5.S6 "In Chapter 5 Stage 3: Training Setup ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.6 最佳实践](#Ch5.S6 "在第 5 章 第 3 阶段：训练设置 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations](#Ch6
    "In The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 第 4 阶段：微调技术和适当模型配置的选择](#Ch6 "在从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.1 Steps Involved in Fine-Tuning](#Ch6.S1 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 微调涉及的步骤](#Ch6.S1 "在第 6 章 第 4 阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.2 Fine-Tuning Strategies for LLMs](#Ch6.S2 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 LLM 的微调策略](#Ch6.S2 "在第 6 章 第 4 阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[6.2.1 Task-Specific Fine-Tuning](#Ch6.S2.SS1 "In 6.2 Fine-Tuning Strategies
    for LLMs ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.1 特定任务微调](#Ch6.S2.SS1 "第6.2节 微调策略 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.2.2 Domain-Specific Fine-Tuning](#Ch6.S2.SS2 "In 6.2 Fine-Tuning Strategies
    for LLMs ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2.2 特定领域微调](#Ch6.S2.SS2 "第6.2节 微调策略 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques](#Ch6.S3 "In Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3 参数高效微调（PEFT）技术](#Ch6.S3 "第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.1 Adapters](#Ch6.S3.SS1 "In 6.3 Parameter-Efficient Fine-Tuning (PEFT)
    Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.1 适配器](#Ch6.S3.SS1 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.2 Low-Rank Adaptation (LoRA)](#Ch6.S3.SS2 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.2 低秩适应（LoRA）](#Ch6.S3.SS2 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置
    ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.3 QLoRA](#Ch6.S3.SS3 "In 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.3 QLoRA](#Ch6.S3.SS3 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置 ‣
    微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.4 Weight-Decomposed Low-Rank Adaptation (DoRA)](#Ch6.S3.SS4 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.4 权重分解低秩适应（DoRA）](#Ch6.S3.SS4 "第6.3节 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术的选择与适当模型配置
    ‣ 微调LLMs的终极指南：从基础到突破的全面技术、研究、最佳实践、应用研究挑战与机遇评审（第1.0版）")'
- en: '[6.3.5 Fine-Tuning with Multiple Adapters](#Ch6.S3.SS5 "In 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3.5 多适配器微调](#Ch6.S3.SS5 "在6.3 参数高效微调（PEFT）技术 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣
    《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4 Half Fine Tuning](#Ch6.S4 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4 半微调](#Ch6.S4 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4.1 Benefits of using Half Fine tuning](#Ch6.S4.SS1 "In 6.4 Half Fine Tuning
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4.1 使用半微调的好处](#Ch6.S4.SS1 "在6.4 半微调 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.4.2 Comparison between HFT and LoRA](#Ch6.S4.SS2 "In 6.4 Half Fine Tuning
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4.2 HFT与LoRA的比较](#Ch6.S4.SS2 "在6.4 半微调 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.5 Lamini Memory Tuning](#Ch6.S5 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.5 Lamini内存调优](#Ch6.S5 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.5.1 Lamini-1 - A model architecture based on Lamini](#Ch6.S5.SS1 "In 6.5
    Lamini Memory Tuning ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.5.1 Lamini-1 - 基于Lamini的模型架构](#Ch6.S5.SS1 "在6.5 Lamini内存调优 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置
    ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.6 Mixture of Experts](#Ch6.S6 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.6 专家混合](#Ch6.S6 "在第6章第4阶段：微调技术的选择和适当的模型配置 ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.6.1 Mixtral 8x7B Architecture and Performance](#Ch6.S6.SS1 "In 6.6 Mixture
    of Experts ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.6.1 Mixtral 8x7B架构与性能](#Ch6.S6.SS1 "在6.6 专家混合 ‣ 第6章第4阶段：微调技术的选择和适当的模型配置
    ‣ 《从基础到突破的微调LLMs终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）》")'
- en: '[6.7 Mixture of Agents](#Ch6.S7 "In Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7 智能体混合](#Ch6.S7 "在第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.1 Methodology](#Ch6.S7.SS1 "In 6.7 Mixture of Agents ‣ Chapter 6 Stage
    4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.1 方法论](#Ch6.S7.SS1 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.2 Analogy with MoE](#Ch6.S7.SS2 "In 6.7 Mixture of Agents ‣ Chapter 6
    Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.2 与MoE的类比](#Ch6.S7.SS2 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.7.3 What makes MoA works well?](#Ch6.S7.SS3 "In 6.7 Mixture of Agents ‣
    Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7.3 什么使MoA运作良好？](#Ch6.S7.SS3 "在6.7智能体混合 ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8 Proximal Policy Optimisation (PPO)](#Ch6.S8 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8 近端策略优化（PPO）](#Ch6.S8 "在第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.1 Benefits of PPO](#Ch6.S8.SS1 "In 6.8 Proximal Policy Optimisation (PPO)
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.1 PPO的好处](#Ch6.S8.SS1 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.2 Limitations of PPO](#Ch6.S8.SS2 "In 6.8 Proximal Policy Optimisation
    (PPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.2 PPO的局限性](#Ch6.S8.SS2 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.8.3 Tutorial for training models using PPO technique](#Ch6.S8.SS3 "In 6.8
    Proximal Policy Optimisation (PPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.8.3 使用PPO技术训练模型的教程](#Ch6.S8.SS3 "在6.8近端策略优化（PPO） ‣ 第6章第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的LLM微调终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")'
- en: '[6.9 Direct Preference Optimisation (DPO)](#Ch6.S9 "In Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9 直接偏好优化 (DPO)](#Ch6.S9 "在第6章 第4阶段: 精细调整技术与合适模型配置的选择 ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾
    (版本 1.0)")'
- en: '[6.9.1 Benefits of DPO](#Ch6.S9.SS1 "In 6.9 Direct Preference Optimisation
    (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.1 DPO的好处](#Ch6.S9.SS1 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.2 Best Practices for DPO](#Ch6.S9.SS2 "In 6.9 Direct Preference Optimisation
    (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate
    Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.2 DPO的最佳实践](#Ch6.S9.SS2 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.3 Tutorial for training models using DPO technique](#Ch6.S9.SS3 "In 6.9
    Direct Preference Optimisation (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.3 使用DPO技术训练模型的教程](#Ch6.S9.SS3 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.9.4 Is DPO Superior to PPO for LLM Alignment?](#Ch6.S9.SS4 "In 6.9 Direct
    Preference Optimisation (DPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.9.4 DPO是否优于PPO用于LLM对齐？](#Ch6.S9.SS4 "在6.9 直接偏好优化 (DPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10 Optimised Routing and Pruning Operations (ORPO)](#Ch6.S10 "In Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10 优化路由和修剪操作 (ORPO)](#Ch6.S10 "在第6章 第4阶段: 精细调整技术与合适模型配置的选择 ‣ 从基础到突破的最终指南:
    技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10.1 When to Prune AI Models?](#Ch6.S10.SS1 "In 6.10 Optimised Routing and
    Pruning Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.1 何时修剪AI模型？](#Ch6.S10.SS1 "在6.10 优化路由和修剪操作 (ORPO) ‣ 第6章 第4阶段: 精细调整技术与合适模型配置的选择
    ‣ 从基础到突破的最终指南: 技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾 (版本 1.0)")'
- en: '[6.10.2 Benefits of Pruning](#Ch6.S10.SS2 "In 6.10 Optimised Routing and Pruning
    Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and
    Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.2 剪枝的好处](#Ch6.S10.SS2 "在 6.10 优化路由和剪枝操作（ORPO） ‣ 第6章 第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[6.10.3 Challenges of Pruning](#Ch6.S10.SS3 "In 6.10 Optimised Routing and
    Pruning Operations (ORPO) ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.10.3 剪枝的挑战](#Ch6.S10.SS3 "在 6.10 优化路由和剪枝操作（ORPO） ‣ 第6章 第4阶段：微调技术和适当模型配置的选择
    ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[7 Stage 5: Evaluation and Validation](#Ch7 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 第5阶段：评估和验证](#Ch7 "在从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本 1.0）")'
- en: '[7.1 Steps Involved in Evaluating and Validating Fine-Tuned Models](#Ch7.S1
    "In Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.1 评估和验证微调模型的步骤](#Ch7.S1 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2 Setting Up Evaluation Metrics](#Ch7.S2 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2 设置评估指标](#Ch7.S2 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2.1 Importance of Cross-Entropy for LLM Training and Evaluation](#Ch7.S2.SS1
    "In 7.2 Setting Up Evaluation Metrics ‣ Chapter 7 Stage 5: Evaluation and Validation
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.1 交叉熵在 LLM 训练和评估中的重要性](#Ch7.S2.SS1 "在 7.2 设置评估指标 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.2.2 Beyond Cross-Entropy: Advanced LLM Evaluation Metrics](#Ch7.S2.SS2 "In
    7.2 Setting Up Evaluation Metrics ‣ Chapter 7 Stage 5: Evaluation and Validation
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.2.2 超越交叉熵：高级 LLM 评估指标](#Ch7.S2.SS2 "在 7.2 设置评估指标 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3 Understanding the Training Loss Curve](#Ch7.S3 "In Chapter 7 Stage 5:
    Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3 理解训练损失曲线](#Ch7.S3 "在第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3.1 Interpreting Loss Curves](#Ch7.S3.SS1 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.1 解读损失曲线](#Ch7.S3.SS1 "在 7.3 理解训练损失曲线 ‣ 第7章 第5阶段：评估和验证 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本
    1.0）")'
- en: '[7.3.2 Avoiding Overfitting](#Ch7.S3.SS2 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.2 避免过拟合](#Ch7.S3.SS2 "在7.3 理解训练损失曲线 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.3.3 Sources of Noisy Gradients](#Ch7.S3.SS3 "In 7.3 Understanding the Training
    Loss Curve ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.3.3 噪声梯度来源](#Ch7.S3.SS3 "在7.3 理解训练损失曲线 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.4 Running Validation Loops](#Ch7.S4 "In Chapter 7 Stage 5: Evaluation and
    Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.4 运行验证循环](#Ch7.S4 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.5 Monitoring and Interpreting Results](#Ch7.S5 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.5 监控和解读结果](#Ch7.S5 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.6 Hyperparameter Tuning and Other Adjustments](#Ch7.S6 "In Chapter 7 Stage
    5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.6 超参数调优及其他调整](#Ch7.S6 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.6.1 Data Size and Quality](#Ch7.S6.SS1 "In 7.6 Hyperparameter Tuning and
    Other Adjustments ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.6.1 数据规模和质量](#Ch7.S6.SS1 "在7.6 超参数调优及其他调整 ‣ 第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.7 Benchmarking Fine-Tuned LLMs](#Ch7.S7 "In Chapter 7 Stage 5: Evaluation
    and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.7 微调LLMs的基准测试](#Ch7.S7 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.8 Evaluating Fine-Tuned LLMs on Safety Benchmark](#Ch7.S8 "In Chapter 7
    Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.8 在安全基准上评估微调LLMs](#Ch7.S8 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9 Evaluating Safety of Fine-Tuned LLM using AI Models](#Ch7.S9 "In Chapter
    7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9 使用AI模型评估微调LLMs的安全性](#Ch7.S9 "在第7章第5阶段：评估和验证 ‣ 《从基础到突破的LLMs微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.1 Llama Guard](#Ch7.S9.SS1 "In 7.9 Evaluating Safety of Fine-Tuned LLM
    using AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.1 Llama Guard](#Ch7.S9.SS1 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣
    《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.2 Shield Gemma](#Ch7.S9.SS2 "In 7.9 Evaluating Safety of Fine-Tuned LLM
    using AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.2 Shield Gemma](#Ch7.S9.SS2 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣
    《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[7.9.3 WILDGUARD](#Ch7.S9.SS3 "In 7.9 Evaluating Safety of Fine-Tuned LLM using
    AI Models ‣ Chapter 7 Stage 5: Evaluation and Validation ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7.9.3 WILDGUARD](#Ch7.S9.SS3 "在7.9 使用AI模型评估微调LLM的安全性 ‣ 第7章 第5阶段：评估与验证 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8 Stage 6: Deployment](#Ch8 "In The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 第6阶段：部署](#Ch8 "在《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.1 Steps Involved in Deploying the Fine-Tuned Model](#Ch8.S1 "In Chapter
    8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.1 微调模型部署的步骤](#Ch8.S1 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.2 Cloud-Based Providers for LLM Deployment](#Ch8.S2 "In Chapter 8 Stage
    6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.2 基于云的LLM部署提供商](#Ch8.S2 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3 Techniques for Optimising Model Performance During Inference](#Ch8.S3
    "In Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3 优化模型推理性能的技术](#Ch8.S3 "在第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.1 Traditional On-Premises GPU-Based Deployments](#Ch8.S3.SS1 "In 8.3 Techniques
    for Optimising Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.1 传统的本地GPU部署](#Ch8.S3.SS1 "在8.3 优化模型推理性能的技术 ‣ 第8章 第6阶段：部署 ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.2 Distributed LLM: Torrent-Style Deployment and Parallel Forward Passes](#Ch8.S3.SS2
    "In 8.3 Techniques for Optimising Model Performance During Inference ‣ Chapter
    8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.2 分布式LLM：类似于种子的部署和并行前向传递](#Ch8.S3.SS2 "在8.3 优化模型推理性能的技术 ‣ 第8章 第6阶段：部署
    ‣ 《从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第1.0版）》")'
- en: '[8.3.3 WebGPU-Based Deployment of LLM](#Ch8.S3.SS3 "In 8.3 Techniques for Optimising
    Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.3 基于 WebGPU 的大型语言模型部署](#Ch8.S3.SS3 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣
    从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.4 LLM on WebGPU using WebLLM](#Ch8.S3.SS4 "In 8.3 Techniques for Optimising
    Model Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.4 使用 WebLLM 在 WebGPU 上的 LLM](#Ch8.S3.SS4 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署
    ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.5 Quantised LLMs](#Ch8.S3.SS5 "In 8.3 Techniques for Optimising Model
    Performance During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.5 量化大型语言模型](#Ch8.S3.SS5 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.3.6 vLLMs](#Ch8.S3.SS6 "In 8.3 Techniques for Optimising Model Performance
    During Inference ‣ Chapter 8 Stage 6: Deployment ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.3.6 vLLMs](#Ch8.S3.SS6 "在8.3 推理过程中优化模型性能的技术 ‣ 第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[8.4 Key Considerations for Deployment of LLMs](#Ch8.S4 "In Chapter 8 Stage
    6: Deployment ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8.4 部署大型语言模型（LLMs）的关键考虑因素](#Ch8.S4 "在第8章第6阶段：部署 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9 Stage 7: Monitoring and Maintenance](#Ch9 "In The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9 第7阶段：监控和维护](#Ch9 "在从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.1 Steps Involved in Monitoring and Maintenance of Deployed Fine-Tuned LLMs](#Ch9.S1
    "In Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.1 监控和维护已部署的微调大型语言模型的步骤](#Ch9.S1 "在第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2 Continuous Monitoring of Model Performance](#Ch9.S2 "In Chapter 9 Stage
    7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2 模型性能的持续监控](#Ch9.S2 "在第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2.1 Functional Monitoring](#Ch9.S2.SS1 "In 9.2 Continuous Monitoring of
    Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.1 功能监控](#Ch9.S2.SS1 "在9.2 模型性能的持续监控 ‣ 第9章第7阶段：监控和维护 ‣ 从基础到突破的最终指南：大型语言模型微调的技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[9.2.2 Prompt Monitoring](#Ch9.S2.SS2 "In 9.2 Continuous Monitoring of Model
    Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.2 提示监控](#Ch9.S2.SS2 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.3 Response Monitoring](#Ch9.S2.SS3 "In 9.2 Continuous Monitoring of Model
    Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.3 响应监控](#Ch9.S2.SS3 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.4 Alerting Mechanisms and Thresholds](#Ch9.S2.SS4 "In 9.2 Continuous Monitoring
    of Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.4 警报机制和阈值](#Ch9.S2.SS4 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.2.5 Monitoring User Interface (UI)](#Ch9.S2.SS5 "In 9.2 Continuous Monitoring
    of Model Performance ‣ Chapter 9 Stage 7: Monitoring and Maintenance ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.2.5 监控用户界面 (UI)](#Ch9.S2.SS5 "在9.2 模型性能的持续监控 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3 Updating LLM Knowledge](#Ch9.S3 "In Chapter 9 Stage 7: Monitoring and
    Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3 更新LLM知识](#Ch9.S3 "在第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.1 Retraining Methods](#Ch9.S3.SS1 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.1 重新训练方法](#Ch9.S3.SS1 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.2 Additional Methods](#Ch9.S3.SS2 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.2 额外方法](#Ch9.S3.SS2 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.3.3 Key Considerations](#Ch9.S3.SS3 "In 9.3 Updating LLM Knowledge ‣ Chapter
    9 Stage 7: Monitoring and Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.3.3 关键考虑因素](#Ch9.S3.SS3 "在9.3 更新LLM知识 ‣ 第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[9.4 The Future of LLM Updates](#Ch9.S4 "In Chapter 9 Stage 7: Monitoring and
    Maintenance ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9.4 LLM更新的未来](#Ch9.S4 "在第9章 第7阶段：监控与维护 ‣ 从基础到突破的最终指南：对LLM微调的全面审查，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）")'
- en: '[10 Industrial Fine-Tuning Platforms and Frameworks for LLMs](#Ch10 "In The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10 工业微调平台和框架](#Ch10 "在从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0
    版）")'
- en: '[10.1 Autotrain](#Ch10.S1 "In Chapter 10 Industrial Fine-Tuning Platforms and
    Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1 Autotrain](#Ch10.S1 "在第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第
    1.0 版）")'
- en: '[10.1.1 Steps Involved in Fine-Tuning Using Autotrain](#Ch10.S1.SS1 "In 10.1
    Autotrain ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.1 使用 Autotrain 进行微调的步骤](#Ch10.S1.SS1 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架
    ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.2 Best Practices of Using Autotrain](#Ch10.S1.SS2 "In 10.1 Autotrain
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.2 使用 Autotrain 的最佳实践](#Ch10.S1.SS2 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架
    ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.3 Challenges of Using Autotrain](#Ch10.S1.SS3 "In 10.1 Autotrain ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.3 使用 Autotrain 的挑战](#Ch10.S1.SS3 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣
    从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.4 When to Use Autotrain](#Ch10.S1.SS4 "In 10.1 Autotrain ‣ Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.4 何时使用 Autotrain](#Ch10.S1.SS4 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调
    LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.1.5 Tutorials](#Ch10.S1.SS5 "In 10.1 Autotrain ‣ Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1.5 教程](#Ch10.S1.SS5 "在 10.1 Autotrain ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs
    的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.2 Transformers Library and Trainer API](#Ch10.S2 "In Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.2 Transformers 库和 Trainer API](#Ch10.S2 "在第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs
    的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第 1.0 版）")'
- en: '[10.2.1 Limitations of the Transformers Library and Trainer API](#Ch10.S2.SS1
    "In 10.2 Transformers Library and Trainer API ‣ Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.2.1 Transformers 库和 Trainer API 的局限性](#Ch10.S2.SS1 "在 10.2 Transformers
    库和 Trainer API ‣ 第10章 工业微调平台和框架 ‣ 从基础到突破的微调 LLMs 的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（第
    1.0 版）")'
- en: '[10.3 Optimum: Enhancing LLM Deployment Efficiency](#Ch10.S3 "In Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3 Optimum：提升 LLM 部署效率](#Ch10.S3 "在第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）》")'
- en: '[10.3.1 Best Practices of Using Optimum](#Ch10.S3.SS1 "In 10.3 Optimum: Enhancing
    LLM Deployment Efficiency ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks
    for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3.1 使用 Optimum 的最佳实践](#Ch10.S3.SS1 "在 10.3 Optimum：提升 LLM 部署效率 ‣ 第 10 章
    工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.3.2 Tutorials](#Ch10.S3.SS2 "In 10.3 Optimum: Enhancing LLM Deployment
    Efficiency ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.3.2 教程](#Ch10.S3.SS2 "在 10.3 Optimum：提升 LLM 部署效率 ‣ 第 10 章 工业级微调平台和框架 ‣
    《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4 Amazon SageMaker JumpStart](#Ch10.S4 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4 亚马逊 SageMaker JumpStart](#Ch10.S4 "在第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM
    微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.1 Steps Involved in Using JumpStart](#Ch10.S4.SS1 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.1 使用 JumpStart 的步骤](#Ch10.S4.SS1 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第
    10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.2 Best Practices for Using JumpStart](#Ch10.S4.SS2 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.2 使用 JumpStart 的最佳实践](#Ch10.S4.SS2 "在 10.4 亚马逊 SageMaker JumpStart ‣
    第 10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.3 Limitations of Using JumpStart](#Ch10.S4.SS3 "In 10.4 Amazon SageMaker
    JumpStart ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.3 使用 JumpStart 的限制](#Ch10.S4.SS3 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第
    10 章 工业级微调平台和框架 ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.4.4 Tutorials](#Ch10.S4.SS4 "In 10.4 Amazon SageMaker JumpStart ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.4.4 教程](#Ch10.S4.SS4 "在 10.4 亚马逊 SageMaker JumpStart ‣ 第 10 章 工业级微调平台和框架
    ‣ 《从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）》")'
- en: '[10.5 Amazon Bedrock](#Ch10.S5 "In Chapter 10 Industrial Fine-Tuning Platforms
    and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5 亚马逊 Bedrock](#Ch10.S5 "在第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.5.1 Steps Involved in Using Amazon Bedrock](#Ch10.S5.SS1 "In 10.5 Amazon
    Bedrock ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.1 使用亚马逊 Bedrock 的步骤](#Ch10.S5.SS1 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.5.2 Limitations of Using Amazon Bedrock](#Ch10.S5.SS2 "In 10.5 Amazon Bedrock
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.2 使用亚马逊 Bedrock 的局限性](#Ch10.S5.SS2 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.5.3 Tutorials](#Ch10.S5.SS3 "In 10.5 Amazon Bedrock ‣ Chapter 10 Industrial
    Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.5.3 教程](#Ch10.S5.SS3 "在 10.5 亚马逊 Bedrock ‣ 第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.6 OpenAI’s Fine-Tuning API](#Ch10.S6 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6 OpenAI 微调 API](#Ch10.S6 "在第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.6.1 Steps Involved in Using OpenAI’s Fine-Tuning API](#Ch10.S6.SS1 "In
    10.6 OpenAI’s Fine-Tuning API ‣ Chapter 10 Industrial Fine-Tuning Platforms and
    Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.1 使用 OpenAI 微调 API 的步骤](#Ch10.S6.SS1 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.6.2 Limitations of OpenAI’s Fine-Tuning API](#Ch10.S6.SS2 "In 10.6 OpenAI’s
    Fine-Tuning API ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for
    LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.2 OpenAI 微调 API 的局限性](#Ch10.S6.SS2 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与
    LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本 1.0）")'
- en: '[10.6.3 Tutorials](#Ch10.S6.SS3 "In 10.6 OpenAI’s Fine-Tuning API ‣ Chapter
    10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.6.3 教程](#Ch10.S6.SS3 "在 10.6 OpenAI 微调 API ‣ 第 10 章 工业微调平台与 LLM 框架 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本
    1.0）")'
- en: '[10.7 NVIDIA NeMo Customizer](#Ch10.S7 "In Chapter 10 Industrial Fine-Tuning
    Platforms and Frameworks for LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7 NVIDIA NeMo 自定义工具](#Ch10.S7 "在第10章 工业级微调平台和框架 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.1 Key Features of NVIDIA NeMo](#Ch10.S7.SS1 "In 10.7 NVIDIA NeMo Customizer
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.1 NVIDIA NeMo 的主要特性](#Ch10.S7.SS1 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.2 Components of NVIDIA NeMo](#Ch10.S7.SS2 "In 10.7 NVIDIA NeMo Customizer
    ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.2 NVIDIA NeMo 组件](#Ch10.S7.SS2 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.3 Customising Large Language Models (LLMs)](#Ch10.S7.SS3 "In 10.7 NVIDIA
    NeMo Customizer ‣ Chapter 10 Industrial Fine-Tuning Platforms and Frameworks for
    LLMs ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An
    Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.3 自定义大型语言模型 (LLMs)](#Ch10.S7.SS3 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架
    ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[10.7.4 Tutorials](#Ch10.S7.SS4 "In 10.7 NVIDIA NeMo Customizer ‣ Chapter 10
    Industrial Fine-Tuning Platforms and Frameworks for LLMs ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.7.4 教程](#Ch10.S7.SS4 "在10.7 NVIDIA NeMo 自定义工具 ‣ 第10章 工业级微调平台和框架 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11 Multimodal LLMs and their Fine-tuning](#Ch11 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11 多模态LLM及其微调](#Ch11 "在从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1 Vision Language Model (VLMs)](#Ch11.S1 "In Chapter 11 Multimodal LLMs
    and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1 视觉语言模型 (VLMs)](#Ch11.S1 "在第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1.1 Architecture](#Ch11.S1.SS1 "In 11.1 Vision Language Model (VLMs) ‣
    Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1.1 架构](#Ch11.S1.SS1 "在11.1 视觉语言模型 (VLMs) ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.1.2 Contrastive Learning](#Ch11.S1.SS2 "In 11.1 Vision Language Model (VLMs)
    ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.1.2 对比学习](#Ch11.S1.SS2 "在11.1 视觉语言模型 (VLMs) ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的终极微调LLM指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第1.0版）")'
- en: '[11.2 Fine-tuning of multimodal models](#Ch11.S2 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2 多模态模型的微调](#Ch11.S2 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.2.1 Full-parameter Fine-Tuning](#Ch11.S2.SS1 "In 11.2 Fine-tuning of multimodal
    models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2.1 全参数微调](#Ch11.S2.SS1 "在 11.2 多模态模型的微调 ‣ 第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.2.2 Case study of fine-tuning MLLMs for Medical domain](#Ch11.S2.SS2 "In
    11.2 Fine-tuning of multimodal models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.2.2 微调 MLLMs 在医疗领域的案例研究](#Ch11.S2.SS2 "在 11.2 多模态模型的微调 ‣ 第 11 章 多模态 LLMs
    及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.3 Applications of Multimodal models](#Ch11.S3 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.3 多模态模型的应用](#Ch11.S3 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.4 Audio or Speech LLMs Or Large Audio Models](#Ch11.S4 "In Chapter 11 Multimodal
    LLMs and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4 音频或语音 LLMs 或大型音频模型](#Ch11.S4 "在第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第
    1.0 版）")'
- en: '[11.4.1 Tokenization and Preprocessing](#Ch11.S4.SS1 "In 11.4 Audio or Speech
    LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.1 分词和预处理](#Ch11.S4.SS1 "在 11.4 音频或语音 LLMs 或大型音频模型 ‣ 第 11 章 多模态 LLMs 及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.2 Fine-Tuning Techniques](#Ch11.S4.SS2 "In 11.4 Audio or Speech LLMs
    Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.2 微调技术](#Ch11.S4.SS2 "在 11.4 音频或语音 LLMs 或大型音频模型 ‣ 第 11 章 多模态 LLMs 及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)](#Ch11.S4.SS3
    "In 11.4 Audio or Speech LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs
    and their Fine-tuning ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.3 微调 Whisper 以进行自动语音识别 (ASR)](#Ch11.S4.SS3 "在 11.4 音频或语音 LLMs 或大型音频模型
    ‣ 第 11 章 多模态 LLMs 及其微调 ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战与机会的全面回顾（第 1.0 版）")'
- en: '[11.4.4 Case Studies and Applications](#Ch11.S4.SS4 "In 11.4 Audio or Speech
    LLMs Or Large Audio Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[11.4.4 案例研究与应用](#Ch11.S4.SS4 "在11.4 音频或语音LLM或大型音频模型 ‣ 第11章 多模态LLM及其微调 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12 Open Challenges and Research Directions](#Ch12 "In The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12 开放挑战与研究方向](#Ch12 "在从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1 Scalability Issues](#Ch12.S1 "In Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1 可扩展性问题](#Ch12.S1 "在第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.1 Challenges in Scaling Fine-Tuning Processes](#Ch12.S1.SS1 "In 12.1
    Scalability Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The
    Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.1 微调过程中的扩展挑战](#Ch12.S1.SS1 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.2 Research Directions for Scalable Solutions](#Ch12.S1.SS2 "In 12.1 Scalability
    Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.2 可扩展解决方案的研究方向](#Ch12.S1.SS2 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.1.3 Hardware and Algorithm Co-Design](#Ch12.S1.SS3 "In 12.1 Scalability
    Issues ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.1.3 硬件与算法协同设计](#Ch12.S1.SS3 "在12.1 可扩展性问题 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2 Ethical Considerations in Fine-Tuning LLMs](#Ch12.S2 "In Chapter 12 Open
    Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2 微调LLM的伦理考虑](#Ch12.S2 "在第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.1 Bias and Fairness](#Ch12.S2.SS1 "In 12.2 Ethical Considerations in
    Fine-Tuning LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.1 偏见与公平性](#Ch12.S2.SS1 "在12.2 微调LLM的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.2 Privacy Concerns](#Ch12.S2.SS2 "In 12.2 Ethical Considerations in Fine-Tuning
    LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.2 隐私问题](#Ch12.S2.SS2 "在12.2 微调LLM的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")'
- en: '[12.2.3 Security Risks](#Ch12.S2.SS3 "In 12.2 Ethical Considerations in Fine-Tuning
    LLMs ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)")'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.2.3 安全风险](#Ch12.S2.SS3 "在12.2 微调大规模语言模型的伦理考虑 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3 Accountability and Transparency](#Ch12.S3 "In Chapter 12 Open Challenges
    and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3 责任与透明度](#Ch12.S3 "在第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.1 The Need for Accountability and Transparency](#Ch12.S3.SS1 "In 12.3
    Accountability and Transparency ‣ Chapter 12 Open Challenges and Research Directions
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.1 责任与透明度的必要性](#Ch12.S3.SS1 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.2 Recent Research and Industry Practices](#Ch12.S3.SS2 "In 12.3 Accountability
    and Transparency ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.2 最近的研究与行业实践](#Ch12.S3.SS2 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.3 Promoting Accountability and Transparency](#Ch12.S3.SS3 "In 12.3 Accountability
    and Transparency ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.3 促进责任与透明度](#Ch12.S3.SS3 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning](#Ch12.S3.SS4
    "In 12.3 Accountability and Transparency ‣ Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.3.4 伦理微调的提议框架/技术](#Ch12.S3.SS4 "在12.3 责任与透明度 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4 Integration with Emerging Technologies](#Ch12.S4 "In Chapter 12 Open
    Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from
    Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best
    Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4 融合新兴技术](#Ch12.S4 "在第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4.1 Opportunities](#Ch12.S4.SS1 "In 12.4 Integration with Emerging Technologies
    ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4.1 机会](#Ch12.S4.SS1 "在12.4 融合新兴技术 ‣ 第12章 开放挑战与研究方向 ‣ 《从基础到突破的终极指南：大规模语言模型微调的全面评审：技术、研究、最佳实践、应用研究挑战与机遇
    (版本1.0)》")'
- en: '[12.4.2 Challenges](#Ch12.S4.SS2 "In 12.4 Integration with Emerging Technologies
    ‣ Chapter 12 Open Challenges and Research Directions ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.4.2 挑战](#Ch12.S4.SS2 "在12.4与新兴技术的整合 ‣ 第12章开放挑战与研究方向 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[12.5 Future Research Areas](#Ch12.S5 "In Chapter 12 Open Challenges and Research
    Directions ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[12.5 未来研究领域](#Ch12.S5 "在第12章开放挑战与研究方向 ‣ 从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: '[Glossary](#Chx1 "In The Ultimate Guide to Fine-Tuning LLMs from Basics to
    Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)")'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[术语表](#Chx1 "在从基础到突破的终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本1.0）")'
- en: Chapter 1 Introduction
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章 引言
- en: 1.1 Background of Large Language Models (LLMs)
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 大型语言模型（LLMs）的背景
- en: Large Language Models (LLMs) represent a significant leap in computational systems
    capable of understanding and generating human language. Building on traditional
    language models (LMs) like N-gram models [[1](#bib.bib1)], LLMs address limitations
    such as rare word handling, overfitting, and capturing complex linguistic patterns.
    Notable examples, such as GPT-3 and GPT-4 [[2](#bib.bib2)], leverage the self-attention
    mechanism within Transformer architectures to efficiently manage sequential data
    and understand long-range dependencies. Key advancements include in-context learning
    for generating coherent text from prompts and Reinforcement Learning from Human
    Feedback (RLHF) [[3](#bib.bib3)] for refining models using human responses. Techniques
    like prompt engineering, question-answering, and conversational interactions have
    significantly advanced the field of natural language processing (NLP) [[4](#bib.bib4)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）代表了在理解和生成自然语言方面的计算系统的重要飞跃。基于传统语言模型（LMs）如N-gram模型[[1](#bib.bib1)]，LLMs解决了诸如稀有词处理、过拟合和捕捉复杂语言模式等局限性。值得注意的例子如GPT-3和GPT-4[[2](#bib.bib2)]，利用Transformer架构中的自注意力机制高效地管理序列数据并理解长程依赖。关键进展包括通过上下文学习生成连贯文本的能力，以及利用人类反馈进行模型优化的强化学习（RLHF）[[3](#bib.bib3)]。技术如提示工程、问答和对话互动显著推动了自然语言处理（NLP）领域的发展[[4](#bib.bib4)]。
- en: 1.2 Historical Development and Key Milestones
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 历史发展与关键里程碑
- en: Language models are fundamental to natural language processing (NLP), leveraging
    mathematical techniques to generalise linguistic rules and knowledge for tasks
    involving prediction and generation. Over several decades, language modelling
    has evolved from early statistical language models (SLMs) to today’s advanced
    large language models (LLMs). This rapid advancement has enabled LLMs to process,
    comprehend, and generate text at a level comparable to human capabilities [[5](#bib.bib5),
    [6](#bib.bib6)].
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是自然语言处理（NLP）的基础，利用数学技术来概括语言规则和知识，用于预测和生成任务。在几十年间，语言建模已经从早期的统计语言模型（SLMs）发展到今天的先进大型语言模型（LLMs）。这一快速进步使得LLMs能够在与人类能力相当的水平上处理、理解和生成文本[[5](#bib.bib5),
    [6](#bib.bib6)]。
- en: '![Refer to caption](img/4da995f8c2ca7f2f1f2f0bd5194884bf.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4da995f8c2ca7f2f1f2f0bd5194884bf.png)'
- en: 'Figure 1.1: A chronological timeline showcasing the evolution of Large Language
    Models (LLMs) from 1990 to 2023\. This progression begins with early statistical
    models such as N-grams, transitions through neural language models like Word2Vec
    and RNN/LSTM, and advances into the era of pre-trained models with the introduction
    of transformers and attention mechanisms. The figure highlights significant milestones,
    including the development of BERT, GPT series, and recent innovations such as
    GPT-4 and ChatGPT, demonstrating the rapid advancements in LLM technology over
    time. (adapted from [[6](#bib.bib6)])'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：一个按时间顺序展示大语言模型（LLMs）从 1990 年到 2023 年演变的时间轴。这个过程从早期的统计模型如 N-grams 开始，经过神经语言模型如
    Word2Vec 和 RNN/LSTM，发展到预训练模型时代，介绍了变换器和注意力机制。该图突出显示了重要的里程碑，包括 BERT、GPT 系列和最近的创新，如
    GPT-4 和 ChatGPT，展示了 LLM 技术随时间的迅速进步。（改编自 [[6](#bib.bib6)])
- en: 'Figure [1.1](#Ch1.F1 "Figure 1.1 ‣ 1.2 Historical Development and Key Milestones
    ‣ Chapter 1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics
    to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices,
    Applied Research Challenges and Opportunities (Version 1.0)") shows the evolution
    of large language models from early statistical approaches to current advanced
    models.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.1](#Ch1.F1 "图 1.1 ‣ 1.2 历史发展与关键里程碑 ‣ 第1章 引言 ‣ 从基础到突破的大语言模型微调终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的全面回顾（版本
    1.0）") 展示了大型语言模型从早期统计方法到当前先进模型的发展历程。
- en: 1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 从传统 NLP 模型到最先进的大语言模型的演变
- en: Understanding LLMs requires tracing the development of language models through
    stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs),
    Pre-trained Language Models (PLMs), and LLMs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 理解大语言模型（LLMs）需要追溯语言模型的发展历程，包括统计语言模型（SLMs）、神经语言模型（NLMs）、预训练语言模型（PLMs）以及大语言模型（LLMs）。
- en: 1.3.1 Statistical Language Models (SLMs)
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1 统计语言模型（SLMs）
- en: 'Emerging in the 1990s, SLMs analyse natural language using probabilistic methods
    to determine the likelihood of sentences within texts. For instance, the probability
    $P(S)$ of the sentence “I am very happy” is given by:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 出现在 1990 年代，统计语言模型（SLMs）使用概率方法分析自然语言，以确定文本中句子的可能性。例如，句子 “I am very happy” 的概率
    $P(S)$ 为：
- en: '|  | $P(S)=P(\omega_{1},\omega_{2},\omega_{3},\omega_{4})=P(\text{I},\text{am},\text{very},\text{happy})$
    |  | (1.1) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(S)=P(\omega_{1},\omega_{2},\omega_{3},\omega_{4})=P(\text{I},\text{am},\text{very},\text{happy})$
    |  | (1.1) |'
- en: 'This probability can be calculated using conditional probabilities:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率可以通过条件概率来计算：
- en: '|  | $1$2 |  | (1.2) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1.2) |'
- en: 'Conditional probabilities are estimated using Maximum Likelihood Estimation
    (MLE):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率的估计使用最大似然估计（MLE）：
- en: '|  | $1$2 |  | (1.3) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1.3) |'
- en: 1.3.2 Neural Language Models (NLMs)
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2 神经语言模型（NLMs）
- en: NLMs leverage neural networks to predict word sequences, overcoming SLM limitations.
    Word vectors enable computers to understand word meanings. Tools like Word2Vec
    [[7](#bib.bib7)] represent words in a vector space where semantic relationships
    are reflected in vector angles. NLMs consist of interconnected neurons organised
    into layers, resembling the human brain’s structure. The input layer concatenates
    word vectors, the hidden layer applies a non-linear activation function, and the
    output layer predicts subsequent words using the Softmax function to transform
    values into a probability distribution.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型（NLMs）利用神经网络来预测词序列，克服了统计语言模型的局限性。词向量使计算机能够理解词义。像 Word2Vec [[7](#bib.bib7)]
    这样的工具将词表示为向量空间中的点，其中语义关系反映在向量角度中。NLMs 由互联的神经元组成，这些神经元被组织成层，类似于人脑的结构。输入层将词向量串联在一起，隐藏层应用非线性激活函数，输出层使用
    Softmax 函数将值转换为概率分布，以预测后续词。
- en: '![Refer to caption](img/0dc20304e6f5cbed35413b812f841791.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0dc20304e6f5cbed35413b812f841791.png)'
- en: 'Figure 1.2: A schematic representation of Neural Language Models, showcasing
    the layered architecture where the input layer processes sequential data, the
    hidden layer captures dependencies, and the output layer generates predictions.
    The figure emphasises the flow of information through concatenation and matrix
    multiplications, culminating in a probability distribution via the softmax function.
    (adopted from [[6](#bib.bib6)])'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：神经语言模型的示意图，展示了分层架构，其中输入层处理顺序数据，隐藏层捕获依赖关系，输出层生成预测。该图强调了通过连接和矩阵乘法的信息流，最终通过
    softmax 函数得到概率分布。（采用自 [[6](#bib.bib6)]）
- en: 'Figure [1.2](#Ch1.F2 "Figure 1.2 ‣ 1.3.2 Neural Language Models (NLMs) ‣ 1.3
    Evolution from Traditional NLP Models to State-of-the-Art LLMs ‣ Chapter 1 Introduction
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)") illustrates the structure of Neural Language
    Models, highlighting the layers and connections used to predict subsequent words.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.2](#Ch1.F2 "图 1.2 ‣ 1.3.2 神经语言模型 (NLMs) ‣ 1.3 从传统 NLP 模型到最先进 LLMs 的演变 ‣
    第 1 章 引言 ‣ 从基础到突破的 LLMs 微调终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第 1.0 版）") 说明了神经语言模型的结构，突出显示了用于预测后续词汇的层次结构和连接。
- en: 1.3.3 Pre-trained Language Models (PLMs)
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3 预训练语言模型（PLMs）
- en: PLMs are initially trained on extensive volumes of unlabelled text to understand
    fundamental language structures (pre-training). They are then fine-tuned on a
    smaller, task-specific dataset. This ”pre-training and fine-tuning” paradigm,
    exemplified by GPT-2 [[8](#bib.bib8)] and BERT [[9](#bib.bib9)], has led to diverse
    and effective model architectures.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: PLMs 最初在大量未标记的文本上进行训练，以理解基本的语言结构（预训练）。然后，在较小的任务特定数据集上进行微调。这种“预训练和微调”范式，以 GPT-2
    [[8](#bib.bib8)] 和 BERT [[9](#bib.bib9)] 为例，导致了多样且有效的模型架构。
- en: 1.3.4 Large Language Models (LLMs)
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.4 大型语言模型（LLMs）
- en: 'LLMs like GPT-3, GPT-4, PaLM [[10](#bib.bib10)], and LLaMA [[11](#bib.bib11)]
    are trained on massive text corpora with tens of billions of parameters. LLMs
    undergo a two-stage process: initial pre-training on a vast corpus followed by
    alignment with human values. This approach enables LLMs to understand human commands
    and values better.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3、GPT-4、PaLM [[10](#bib.bib10)] 和 LLaMA [[11](#bib.bib11)] 这样的 LLMs 在包含数十亿参数的大规模文本语料库上进行训练。LLMs
    经过两个阶段的过程：首先是在广泛的语料库上进行初步预训练，然后与人类价值观对齐。这种方法使 LLMs 更好地理解人类的指令和价值观。
- en: 1.4 Overview of Current Leading LLMs
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 当前领先 LLMs 的概述
- en: LLMs are powerful tools in NLP, capable of performing tasks such as translation,
    summarisation, and conversational interaction. Advances in transformer architectures,
    computational power, and extensive datasets have driven their success. These models
    approximate human-level performance, making them invaluable for research and practical
    implementations. LLMs’ rapid development has spurred research into architectural
    innovations, training strategies, extending context lengths, fine-tuning techniques,
    and integrating multi-modal data. Their applications extend beyond NLP, aiding
    in human-robot interactions and creating intuitive AI systems. This highlights
    the importance of comprehensive reviews consolidating the latest developments
    [[12](#bib.bib12)].
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 是 NLP 中强大的工具，能够执行诸如翻译、摘要和对话互动等任务。变压器架构、计算能力和广泛数据集的进步推动了它们的成功。这些模型接近人类水平的性能，使它们在研究和实际应用中不可或缺。LLMs
    的快速发展推动了对架构创新、训练策略、扩展上下文长度、微调技术以及多模态数据集成的研究。它们的应用超越了 NLP，有助于人机交互和创建直观的 AI 系统。这突显了综合评审汇总最新发展的重要性
    [[12](#bib.bib12)]。
- en: '![Refer to caption](img/ab45c3d014aec95d74422eb9113d2392.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ab45c3d014aec95d74422eb9113d2392.png)'
- en: 'Figure 1.3: Mind map depicting various dimensions of Large Language Models
    (LLMs), covering aspects from pre-training and fine-tuning methodologies to efficiency,
    evaluation, inference, and application domains. Each dimension is linked to specific
    techniques, challenges, and examples of models that exemplify the discussed characteristics.
    This diagram serves as an overview of the multifaceted considerations in the development
    and deployment of LLMs. (adapted from [[13](#bib.bib13)])'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.3: 思维导图展示了大语言模型（LLMs）的各个维度，包括从预训练和微调方法到效率、评估、推理和应用领域的各个方面。每个维度都与特定的技术、挑战和示例模型相关联。这张图表作为
    LLMS 发展和部署中的多维考虑的概述。(改编自 [[13](#bib.bib13)])'
- en: 'Figure [1.3](#Ch1.F3 "Figure 1.3 ‣ 1.4 Overview of Current Leading LLMs ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") provides an overview of current leading
    LLMs, highlighting their capabilities and applications.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1.3](#Ch1.F3 "图 1.3 ‣ 1.4 当前领先 LLM 概述 ‣ 第1章 引言 ‣ 《从基础到突破的终极指南：大语言模型微调的全面评审，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）》")
    提供了当前领先 LLM 的概述，突出了它们的能力和应用。
- en: 1.5 What is Fine-Tuning?
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 什么是微调？
- en: Fine-tuning uses a pre-trained model, such as OpenAI’s GPT series, as a foundation.
    The process involves further training on a smaller, domain-specific dataset. This
    approach builds upon the model’s pre-existing knowledge, enhancing performance
    on specific tasks with reduced data and computational requirements.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 微调使用预训练模型，如 OpenAI 的 GPT 系列，作为基础。这个过程涉及在较小的、领域特定的数据集上进一步训练。这种方法在模型现有知识的基础上进行，增强了在特定任务上的性能，同时减少了数据和计算需求。
- en: Fine-tuning transfers the pre-trained model’s learned patterns and features
    to new tasks, improving performance and reducing training data needs. It has become
    popular in NLP for tasks like text classification, sentiment analysis, and question-answering.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 微调将预训练模型的学习模式和特征转移到新任务上，提升性能并减少训练数据需求。它在自然语言处理（NLP）中变得非常流行，用于文本分类、情感分析和问答等任务。
- en: 1.6 Types of LLM Fine-Tuning
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6 LLM 微调类型
- en: 1.6.1 Unsupervised Fine-Tuning
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.1 无监督微调
- en: This method does not require labelled data. Instead, the LLM is exposed to a
    large corpus of unlabelled text from the target domain, refining its understanding
    of language. This approach is useful for new domains like legal or medical fields
    but is less precise for specific tasks such as classification or summarisation.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不需要标注数据。相反，LLM 接触到目标领域的大量未标注文本，从而完善对语言的理解。这种方法适用于法律或医疗等新领域，但对于特定任务如分类或摘要则精确度较低。
- en: 1.6.2 Supervised Fine-Tuning (SFT)
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.2 有监督微调 (SFT)
- en: SFT involves providing the LLM with labelled data tailored to the target task.
    For example, fine-tuning an LLM for text classification in a business context
    uses a dataset of text snippets with class labels. While effective, this method
    requires substantial labelled data, which can be costly and time-consuming to
    obtain.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 涉及向 LLM 提供针对目标任务的标注数据。例如，在商业背景下对文本分类进行微调时，会使用带有类别标签的文本片段数据集。虽然有效，但这种方法需要大量标注数据，这可能既昂贵又耗时。
- en: 1.6.3 Instruction Fine-Tuning via Prompt Engineering
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.6.3 通过提示工程进行指令微调
- en: This method relies on providing the LLM with natural language instructions,
    useful for creating specialised assistants. It reduces the need for vast amounts
    of labelled data but depends heavily on the quality of the prompts.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法依赖于向 LLM 提供自然语言指令，这对于创建专用助手很有用。它减少了对大量标注数据的需求，但高度依赖于提示的质量。
- en: 1.7 Pre-training vs Fine-tuning
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7 预训练与微调
- en: 'Table [1.1](#Ch1.T1 "Table 1.1 ‣ 1.7 Pre-training vs Fine-tuning ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") provides a comparison between pre-training
    and fine-tuning, highlighting their respective characteristics and processes.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1.1](#Ch1.T1 "表格 1.1 ‣ 1.7 预训练与微调 ‣ 第1章 引言 ‣ 《从基础到突破的终极指南：大语言模型微调的全面评审，包括技术、研究、最佳实践、应用研究挑战和机会（版本1.0）》")
    提供了预训练与微调的比较，突出了它们各自的特点和过程。
- en: '| Aspect | Pre-training | Fine-tuning |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 方面 | 预训练 | 微调 |'
- en: '| --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Definition | Training on a vast amount of unlabelled text data | Adapting
    a pre-trained model to specific tasks |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 定义 | 在大量未标记文本数据上训练 | 将预训练模型适应于特定任务 |'
- en: '| Data Requirement | Extensive and diverse unlabelled text data | Smaller,
    task-specific labelled data |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 数据要求 | 大量多样的未标记文本数据 | 较小的、特定任务的标记数据 |'
- en: '| Objective | Build general linguistic knowledge | Specialise model for specific
    tasks |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 建立通用语言知识 | 针对特定任务对模型进行专业化 |'
- en: '| Process | Data collection, training on large dataset, predict next word/sequence
    | Task-specific data collection, modify last layer for task, train on new dataset,
    generate output based on tasks |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 过程 | 数据收集，大数据集训练，预测下一个词/序列 | 特定任务的数据收集，修改最后一层以适应任务，在新数据集上训练，根据任务生成输出 |'
- en: '| Model Modification | Entire model trained | Last layers adapted for new task
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 模型修改 | 整个模型训练 | 对新任务进行最后一层的适配 |'
- en: '| Computational Cost | High (large dataset, complex model) | Lower (smaller
    dataset, fine-tuning layers) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 计算成本 | 高（大数据集，复杂模型） | 较低（小数据集，微调层） |'
- en: '| Training Duration | Weeks to months | Days to weeks |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 训练时间 | 数周到数月 | 数天到数周 |'
- en: '| Purpose | General language understanding | Task-specific performance improvement
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 通用语言理解 | 特定任务的性能提升 |'
- en: '| Examples | GPT, LLaMA 3 | Fine-tuning LLaMA 3 for summarisation |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | GPT，LLaMA 3 | 微调LLaMA 3以进行总结 |'
- en: 'Table 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large
    Language Models (LLMs). The table outlines key differences between the pre-training
    and fine-tuning phases across various aspects such as definition, data requirements,
    objectives, processes, model modification, computational costs, training duration,
    and their respective purposes, with examples highlighting specific models and
    tasks. Pre-training involves extensive training on vast amounts of unlabelled
    data to build general linguistic knowledge, while fine-tuning adapts the pre-trained
    models to specialised tasks using smaller, labelled datasets, focusing on task-specific
    performance improvements.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1：大型语言模型（LLMs）中预训练与微调的对比概述。该表格概述了预训练和微调阶段在定义、数据要求、目标、过程、模型修改、计算成本、训练时间及其各自目的等方面的关键差异，并通过示例突出特定模型和任务。预训练涉及在大量未标记数据上进行广泛训练，以建立通用语言知识，而微调则使用较小的标记数据集将预训练模型适应于专业任务，关注于任务特定的性能提升。
- en: 1.8 Importance of Fine-Tuning LLMs
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.8 微调大型语言模型的重要性
- en: '1.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training,
    adapting it to specific tasks with reduced computation time and resources.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迁移学习：微调利用预训练过程中获得的知识，将其适应于特定任务，从而减少计算时间和资源。
- en: '2.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Reduced Data Requirements: Fine-tuning requires less labelled data, focusing
    on tailoring pre-trained features to the target task.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少数据要求：微调需要较少的标记数据，重点是将预训练的特征调整为目标任务。
- en: '3.'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Improved Generalisation: Fine-tuning enhances the model’s ability to generalise
    to specific tasks or domains, capturing general language features and customising
    them.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改善泛化能力：微调增强了模型对特定任务或领域的泛化能力，捕捉通用语言特征并进行定制。
- en: '4.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Efficient Model Deployment: Fine-tuned models are more efficient for real-world
    applications, being computationally efficient and well-suited for specific tasks.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的模型部署：微调后的模型在实际应用中更为高效，计算上更为高效，且适合特定任务。
- en: '5.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of
    tasks, performing well across various applications without task-specific architectures.'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种任务的适应性：微调后的LLMs能够适应广泛的任务，在各种应用中表现良好，无需特定任务架构。
- en: '6.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific
    tasks by adjusting to the nuances and vocabulary of the target domain.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特定领域的性能：微调使模型能够在特定领域任务中表现出色，通过调整目标领域的细微差别和词汇。
- en: '7.'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Faster Convergence: Fine-tuning usually achieves faster convergence, starting
    with weights that already capture general language features.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更快的收敛：微调通常实现更快的收敛，从已经捕捉到通用语言特征的权重开始。
- en: 1.9 Retrieval Augmented Generation (RAG)
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.9 检索增强生成（RAG）
- en: 'A popular method to utilise your own data is by incorporating it into the prompt
    when querying the LLM model. This approach, known as Retrieval-Augmented Generation
    (RAG), involves retrieving relevant data and using it as additional context for
    the LLM. Instead of depending solely on knowledge from the training data, a RAG
    workflow pulls pertinent information, connecting static LLMs with real-time data
    retrieval. With RAG architecture, organisations can deploy any LLM model and enhance
    it to return relevant results by providing a small amount of their own data (see
    Figure[1.4](#Ch1.F4 "Figure 1.4 ‣ 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") for visual workflow). This process
    avoids the costs and time associated with fine-tuning or pre-training the model.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一种利用自己数据的流行方法是在查询LLM模型时将数据纳入提示中。这种方法称为检索增强生成（RAG），涉及检索相关数据并将其作为LLM的附加上下文。RAG工作流程不仅依赖于训练数据中的知识，还提取相关信息，将静态LLM与实时数据检索连接起来。使用RAG架构，组织可以部署任何LLM模型，并通过提供少量自己的数据来增强模型的相关结果（参见图[1.4](#Ch1.F4
    "图1.4 ‣ 1.9 检索增强生成（RAG） ‣ 第1章 介绍 ‣ 终极指南：从基础到突破的LLM微调完全评审：技术、研究、最佳实践、应用研究挑战与机遇（版本1.0）")以获得可视化工作流程）。此过程避免了微调或预训练模型相关的成本和时间。
- en: '![Refer to caption](img/c3249a348733e733b4e38128d7d66e98.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c3249a348733e733b4e38128d7d66e98.png)'
- en: 'Figure 1.4: An illustration of the Traditional Retrieval-Augmented Generation
    (RAG) pipeline steps, depicting the sequential process from client query to response
    generation. The pipeline starts with the client’s question, followed by semantic
    search in a vector database, contextually enriching the data before generating
    a prompt for the large language model (LLM). The final response is post-processed
    and returned to the client.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：传统检索增强生成（RAG）流程步骤的示意图，展示了从客户端查询到响应生成的顺序过程。流程从客户端的问题开始，随后在向量数据库中进行语义搜索，语境丰富数据，然后为大型语言模型（LLM）生成提示。最终回应经过后处理并返回给客户端。
- en: 1.9.1 Traditional RAG Pipeline and Steps
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.1 传统RAG流程和步骤
- en: '1.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Data Indexing: Organise data efficiently for quick retrieval. This involves
    processing, chunking, and storing data in a vector database using indexing strategies
    like search indexing, vector indexing, and hybrid indexing.'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据索引：高效地组织数据以便快速检索。这包括使用搜索索引、向量索引和混合索引等策略处理、分块和存储数据到向量数据库中。
- en: '2.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Input Query Processing: Refine user queries to improve compatibility with indexed
    data. This can include simplification or vector transformation of queries for
    enhanced search efficiency.'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入查询处理：优化用户查询，以提高与索引数据的兼容性。这可能包括对查询进行简化或向量变换，以提高搜索效率。
- en: '3.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Searching and Ranking: Retrieve and rank data based on relevance using search
    algorithms such as TF-IDF, BM25, and deep learning models like BERT to interpret
    the query’s intent and context.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索和排序：使用TF-IDF、BM25等搜索算法以及BERT等深度学习模型，根据相关性检索和排序数据，以解释查询的意图和上下文。
- en: '4.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Prompt Augmentation: Incorporate relevant information from the search results
    into the original query to provide the LLM with additional context, enhancing
    response accuracy and relevance.'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示增强：将搜索结果中的相关信息纳入原始查询，为LLM提供额外的上下文，提高回应的准确性和相关性。
- en: '5.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Response Generation: Use the augmented prompt to generate responses that combine
    the LLM’s knowledge with current, specific data, ensuring high-quality, contextually
    grounded answers.'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回应生成：使用增强的提示生成结合LLM知识和当前具体数据的回应，确保高质量、上下文扎实的答案。
- en: 1.9.2 Benefits of Using RAG
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.2 使用RAG的好处
- en: •
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Up-to-Date and Accurate Responses: Enhances the LLM’s responses with current
    external data, improving accuracy and relevance.'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新和准确的回应：通过引入当前的外部数据来增强LLM的回应，提高准确性和相关性。
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reducing Inaccurate Responses: Grounds the LLM’s output in relevant knowledge,
    reducing the risk of generating incorrect information.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少不准确的回应：将LLM的输出基于相关知识，降低生成不正确信息的风险。
- en: •
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-Specific Responses: Delivers contextually relevant responses tailored
    to an organisation’s proprietary data.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域特定回应：提供针对组织专有数据量身定制的上下文相关回应。
- en: •
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising
    LLMs without extensive model fine-tuning.'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率和成本效益：提供一种经济高效的定制LLM的方法，无需广泛的模型微调。
- en: 1.9.3 Challenges and Considerations in Serving RAG
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.3 服务RAG的挑战和考虑因素
- en: '1.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'User Experience: Ensuring rapid response times suitable for real-time applications.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户体验：确保快速响应时间适合实时应用。
- en: '2.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Cost Efficiency: Managing the costs associated with serving millions of responses.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本效益：管理提供数百万回应的成本。
- en: '3.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Accuracy: Ensuring outputs are accurate to avoid misinformation.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：确保输出准确，以避免错误信息。
- en: '4.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Recency and Relevance: Keeping responses and content current with the latest
    data.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 近期性和相关性：保持回应和内容与最新数据同步。
- en: '5.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Business Context Awareness: Aligning LLM responses with specific business contexts.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 商业背景意识：使LLM的回答与特定商业背景对齐。
- en: '6.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Service Scalability: Managing increased capacity while controlling costs.'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 服务可扩展性：在控制成本的同时管理增加的容量。
- en: '7.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Security and Governance: Implementing protocols for data security, privacy,
    and governance.'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全性和治理：实施数据安全、隐私和治理的协议。
- en: 1.9.4 Use Cases and Examples
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.4 使用案例和示例
- en: '1.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate
    answers from company documents, enhancing customer support.'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问答聊天机器人：将LLM与聊天机器人集成，从公司文档中生成准确答案，提升客户支持。
- en: '2.'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Search Augmentation: Enhance search engines with LLM-generated answers for
    more accurate informational queries.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索增强：通过LLM生成的答案增强搜索引擎，以提高信息查询的准确性。
- en: '3.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Knowledge Engine: Use LLMs to answer questions related to internal functions,
    such as HR and compliance, using company data.'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识引擎：利用LLM回答与内部职能（如人力资源和合规性）相关的问题，使用公司数据。
- en: 1.9.5 Considerations for Choosing Between RAG and Fine-Tuning
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.9.5 选择RAG和微调的考虑因素
- en: 'When considering external data access, RAG is likely a superior option for
    applications needing to access external data sources. Fine-tuning, on the other
    hand, is more suitable if you require the model to adjust its behaviour, and writing
    style, or incorporate domain-specific knowledge. In terms of suppressing hallucinations
    and ensuring accuracy, RAG systems tend to perform better as they are less prone
    to generating incorrect information. If you have ample domain-specific, labelled
    training data, fine-tuning can result in a more tailored model behaviour, whereas
    RAG systems are robust alternatives when such data is scarce. RAG systems provide
    an advantage with dynamic data retrieval capabilities for environments where data
    frequently updates or changes. Additionally, it is crucial to ensure the transparency
    and interpret ability of the model’s decision-making process. In that case, RAG
    systems offer insight that is typically not available in models that are solely
    fine-tuned. Figure[1.5](#Ch1.F5 "Figure 1.5 ‣ 1.9.5 Considerations for Choosing
    Between RAG and Fine-Tuning ‣ 1.9 Retrieval Augmented Generation (RAG) ‣ Chapter
    1 Introduction ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") illustrates the visual representation
    alongside example use cases.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑外部数据访问时，RAG可能是需要访问外部数据源的应用的更优选择。另一方面，微调更适合于需要模型调整行为、写作风格或融入领域特定知识的情况。在抑制幻觉和确保准确性方面，RAG系统通常表现更好，因为它们较少生成错误信息。如果你有大量领域特定的标注训练数据，微调可以产生更具针对性的模型行为，而RAG系统在此类数据稀缺时则是稳健的替代方案。RAG系统在数据频繁更新或变化的环境中提供动态数据检索能力的优势。此外，确保模型决策过程的透明性和可解释性也至关重要。在这种情况下，RAG系统提供的洞察通常是仅依赖微调模型无法获得的。图[1.5](#Ch1.F5
    "图 1.5 ‣ 1.9.5 选择RAG和微调的考虑因素 ‣ 1.9 检索增强生成 (RAG) ‣ 第1章 引言 ‣ 从基础到突破的终极LLM微调指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽评审（版本1.0)")
    展示了视觉表现和示例使用案例。
- en: '![Refer to caption](img/7c127a3142ffca33f4afc49e11879fe7.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c127a3142ffca33f4afc49e11879fe7.png)'
- en: 'Figure 1.5: Graph comparing the model adaptation required versus the level
    of external knowledge needed across different scenarios, highlighting the roles
    of Retrieval-Augmented Generation (RAG), Fine-Tuning, and their hybrid applications
    in various contexts such as Q&A systems, customer support automation, and summarisation
    tasks. (adapted from [[14](#bib.bib14)])'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：图表比较了在不同情境下模型适应所需的外部知识水平，突出了**检索增强生成**（RAG）、微调及其在问答系统、客户支持自动化和总结任务等各种背景中的混合应用的角色。（改编自[[14](#bib.bib14)]）
- en: 1.10 Objectives of the Report
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.10 报告目标
- en: 1.10.1 Goals and Scope
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.1 目标和范围
- en: The primary goal of this report is to conduct a comprehensive analysis of fine-tuning
    techniques for LLMs. This involves exploring theoretical foundations, practical
    implementation strategies, and challenges. The report examines various fine-tuning
    methodologies, their applications, and recent advancements.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告的主要目标是对LLM微调技术进行全面分析。这包括探讨理论基础、实际实施策略和挑战。报告考察了各种微调方法、它们的应用和近期的进展。
- en: 1.10.2 Key Questions and Issues Addressed
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.2 关键问题和讨论议题
- en: This report addresses critical questions surrounding fine-tuning LLMs, starting
    with foundational insights into LLMs, their evolution, and significance in NLP.
    It defines fine-tuning, distinguishes it from pre-training, and emphasises its
    role in adapting models for specific tasks. Key objectives include enhancing model
    performance for targeted applications and domains.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告解决了关于LLM微调的关键问题，从LLM的基础知识、演变和在自然语言处理中的重要性开始。它定义了微调，区分了预训练，并强调其在模型特定任务适应中的作用。关键目标包括提高模型在特定应用和领域中的表现。
- en: The report outlines a structured fine-tuning process, featuring a high-level
    pipeline with visual representations and detailed stage explanations. It covers
    practical implementation strategies, including model initialisation, hyperparameter
    definition, and fine-tuning techniques such as Parameter-Efficient Fine-Tuning
    (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation
    methods, deployment challenges, and recent advancements are also explored.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 报告概述了一个结构化的微调过程，包含一个高层次的流程图、视觉表现和详细的阶段解释。它涵盖了实际的实施策略，包括模型初始化、超参数定义以及如**参数高效微调**（PEFT）和**检索增强生成**（RAG）等微调技术。还探讨了行业应用、评估方法、部署挑战和近期的进展。
- en: 1.10.3 Overview of the Report Structure
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.10.3 报告结构概述
- en: The rest of the report provides a comprehensive understanding of fine-tuning
    LLMs. The main chapters include an in-depth look at the fine-tuning pipeline,
    practical applications, model alignment, evaluation metrics, and challenges. The
    concluding sections discuss the evolution of fine-tuning techniques, highlight
    ongoing research challenges, and provide insights for researchers and practitioners.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 报告的其余部分提供了对LLM微调的全面理解。主要章节包括对微调流程的深入分析、实际应用、模型对齐、评估指标和挑战。结论部分讨论了微调技术的发展，强调了持续的研究挑战，并为研究人员和从业人员提供了见解。
- en: Chapter 2 Seven Stage Fine-Tuning Pipeline for LLM
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章 LLM的七阶段微调流程
- en: Fine-tuning a Large Language Model (LLM) is a comprehensive process divided
    into seven distinct stages, each essential for adapting the pre-trained model
    to specific tasks and ensuring optimal performance. These stages encompass everything
    from initial dataset preparation to the final deployment and maintenance of the
    fine-tuned model. By following these stages systematically, the model is refined
    and tailored to meet precise requirements, ultimately enhancing its ability to
    generate accurate and contextually appropriate responses. The seven stages include
    Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-Tuning,
    Evaluation and Validation, Deployment, and Monitoring and Maintenance.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）是一个全面的过程，分为七个独特的阶段，每个阶段对于将预训练模型适应特定任务并确保最佳性能至关重要。这些阶段涵盖了从初始数据集准备到最终部署和维护微调模型的所有内容。通过系统地遵循这些阶段，模型被精细化并调整以满足精确的要求，*最终提升其生成准确且上下文适当回应的能力*。七个阶段包括数据集准备、模型初始化、训练环境设置、微调、评估与验证、部署以及监控与维护。
- en: '![Refer to caption](img/b68b9db62519b7aac15496aeeb666712.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b68b9db62519b7aac15496aeeb666712.png)'
- en: 'Figure 2.1: A comprehensive pipeline for fine-tuning Large Language Models
    (LLMs), illustrating the seven essential stages: Dataset Preparation, Model Initialisation,
    Training Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment,
    and Monitoring and Maintenance. Each stage plays a crucial role in adapting the
    pre-trained model to specific tasks and ensuring optimal performance throughout
    its lifecycle.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：一个全面的微调大型语言模型（LLMs）流程图，展示了七个关键阶段：数据集准备、模型初始化、训练环境设置、微调、评估和验证、部署，以及监控和维护。每个阶段在将预训练模型适应特定任务并确保其在生命周期内的最佳性能中都发挥着至关重要的作用。
- en: 'Figure [2.1](#Ch2.F1 "Figure 2.1 ‣ Chapter 2 Seven Stage Fine-Tuning Pipeline
    for LLM ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)") illustrates the comprehensive pipeline
    for fine-tuning LLMs, encompassing all necessary stages from dataset preparation
    to monitoring and maintenance.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2.1](#Ch2.F1 "图 2.1 ‣ 第 2 章 七阶段 LLM 微调流程 ‣ 微调 LLM 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（版本
    1.0）") 展示了 LLM 微调的全面流程，包括从数据集准备到监控和维护的所有必要阶段。
- en: '2.1 Stage 1: Dataset Preparation'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 阶段 1：数据集准备
- en: Fine-tuning a Large Language Model (LLM) starts with adapting the pre-trained
    model for specific tasks by updating its parameters using a new dataset. This
    involves cleaning and formatting the dataset to match the target task, such as
    instruction tuning, sentiment analysis, or topic mapping. The dataset is composed
    of $$<\text{input},\text{output}> pairs, demonstrating the desired behaviour
    for the model.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）从通过使用新数据集更新其参数来适应预训练模型于特定任务开始。这涉及清理和格式化数据集，以匹配目标任务，如指令调整、情感分析或主题映射。数据集由$$<\text{input},\text{output}>对组成，展示了模型期望的行为。
- en: 'For example, in instruction tuning, the dataset may look like:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在指令调整中，数据集可能看起来像这样：
- en: '[PRE0]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the ’Input Query’ is what the user asks, and the ’Generated Output’ is
    the model’s response. The structure and style of these pairs can be adjusted based
    on the specific needs of the task.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，‘输入查询’是用户提问的内容，而‘生成输出’是模型的响应。这些对的结构和风格可以根据任务的具体需求进行调整。
- en: '2.2 Stage 2: Model Initialisation'
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 阶段 2：模型初始化
- en: Model initialisation is the process of setting up the initial parameters and
    configurations of the LLM before training or deploying it. This step is crucial
    for ensuring the model performs optimally, trains efficiently, and avoids issues
    such as vanishing or exploding gradients.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 模型初始化是指在训练或部署 LLM 之前设置初始参数和配置的过程。这个步骤对确保模型最佳性能、有效训练以及避免诸如梯度消失或爆炸等问题至关重要。
- en: '2.3 Stage 3: Training Environment Setup'
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 阶段 3：训练环境设置
- en: Setting up the training environment for LLM fine-tuning involves configuring
    the necessary infrastructure to adapt a pre-existing model for specific tasks.
    This includes selecting relevant training data, defining the model’s architecture
    and hyperparameters, and running training iterations to adjust the model’s weights
    and biases. The aim is to enhance the LLM’s performance in generating accurate
    and contextually appropriate outputs tailored to specific applications, like content
    creation, translation, or sentiment analysis. Successful fine-tuning relies on
    careful preparation and rigorous experimentation.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 为 LLM 微调设置训练环境涉及配置必要的基础设施，以将现有模型调整为特定任务。这包括选择相关的训练数据、定义模型的架构和超参数，以及运行训练迭代来调整模型的权重和偏差。目标是提升
    LLM 在生成准确和上下文适当的输出方面的表现，以满足特定应用需求，如内容创作、翻译或情感分析。成功的微调依赖于细致的准备和严格的实验。
- en: '2.4 Stage 4: Partial or Full Fine-Tuning'
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 阶段 4：部分或完全微调
- en: This stage involves updating the parameters of the LLM using a task-specific
    dataset. Full fine-tuning updates all parameters of the model, ensuring comprehensive
    adaptation to the new task. Alternatively, Half fine-tuning (HFT) [[15](#bib.bib15)]
    or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter layers,
    can be employed to partially fine-tune the model. This method attaches additional
    layers to the pre-trained model, allowing for efficient fine-tuning with fewer
    parameters, which can address challenges related to computational efficiency,
    overfitting, and optimisation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段涉及使用特定任务的数据集来更新 LLM 的参数。全面微调更新模型的所有参数，确保对新任务的全面适应。或者，可以采用半微调（HFT）[[15](#bib.bib15)]
    或参数高效微调（PEFT）方法，例如使用适配器层，来部分微调模型。这种方法将额外的层附加到预训练模型上，从而以更少的参数进行高效微调，这可以解决计算效率、过拟合和优化相关的挑战。
- en: '2.5 Stage 5: Evaluation and Validation'
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.5 阶段 5: 评估和验证'
- en: Evaluation and validation involve assessing the fine-tuned LLM’s performance
    on unseen data to ensure it generalises well and meets the desired objectives.
    Evaluation metrics, such as cross-entropy, measure prediction errors, while validation
    monitors loss curves and other performance indicators to detect issues like overfitting
    or underfitting. This stage helps guide further fine-tuning to achieve optimal
    model performance.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和验证涉及在未见过的数据上评估微调后的 LLM 的表现，以确保其良好的泛化能力并达到预期目标。评估指标，如交叉熵，衡量预测误差，而验证则监控损失曲线和其他性能指标，以检测如过拟合或欠拟合的问题。这一阶段有助于指导进一步的微调，以实现最佳的模型性能。
- en: '2.6 Stage 6: Deployment'
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.6 阶段 6: 部署'
- en: Deploying an LLM means making it operational and accessible for specific applications.
    This involves configuring the model to run efficiently on designated hardware
    or software platforms, ensuring it can handle tasks like natural language processing,
    text generation, or user query understanding. Deployment also includes setting
    up integration, security measures, and monitoring systems to ensure reliable and
    secure performance in real-world applications.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 LLM 意味着使其在特定应用中可操作和可访问。这涉及将模型配置为在指定的硬件或软件平台上高效运行，确保它能够处理自然语言处理、文本生成或用户查询理解等任务。部署还包括设置集成、安全措施和监控系统，以确保在实际应用中的可靠和安全的性能。
- en: '2.7 Stage 7: Monitoring and Maintenance'
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.7 阶段 7: 监控和维护'
- en: Monitoring and maintaining an LLM after deployment is crucial to ensure ongoing
    performance and reliability. This involves continuously tracking the model’s performance,
    addressing any issues that arise, and updating the model as needed to adapt to
    new data or changing requirements. Effective monitoring and maintenance help sustain
    the model’s accuracy and effectiveness over time.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署后监控和维护 LLM 是至关重要的，以确保持续的性能和可靠性。这涉及不断跟踪模型的表现，解决出现的任何问题，并根据需要更新模型以适应新数据或变化的需求。有效的监控和维护有助于保持模型的准确性和有效性。
- en: 'Chapter 3 Stage 1: Data Preparation'
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '第三章 阶段 1: 数据准备'
- en: 3.1 Steps Involved in Data Preparation
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据准备步骤
- en: 3.1.1 Data Collection
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 数据收集
- en: 'The first step in data preparation is to collect data from various sources.
    These sources can be in any format such as CSV, web pages, SQL databases, S3 storage,
    etc. Python provides several libraries to gather the data efficiently and accurately.
    Table [3.1](#Ch3.T1 "Table 3.1 ‣ 3.1.1 Data Collection ‣ 3.1 Steps Involved in
    Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to
    Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)") presents a selection of commonly used data formats along with the corresponding
    Python libraries used for data collection.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '数据准备的第一步是从各种来源收集数据。这些来源可以是任何格式，例如 CSV、网页、SQL 数据库、S3 存储等。Python 提供了多个库来高效、准确地收集数据。表
    [3.1](#Ch3.T1 "Table 3.1 ‣ 3.1.1 Data Collection ‣ 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")
    展示了一些常用的数据格式及其对应的 Python 库。'
- en: '| Data Format | Python Library | Description | Library Link |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 数据格式 | Python 库 | 描述 | 库链接 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| CSV Files | pandas | pandas is a powerful library for data manipulation and
    analysis. It provides the read_csv function for easy and efficient reading of
    CSV files into DataFrame objects. It also supports reading data in Excel, JSON,
    and more. | [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)
    |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| CSV 文件 | pandas | pandas 是一个强大的数据处理和分析库。它提供了 read_csv 函数，用于将 CSV 文件高效地读取到
    DataFrame 对象中。它还支持读取 Excel、JSON 等格式的数据。 | [pandas 文档](https://pandas.pydata.org/pandas-docs/stable/)
    |'
- en: '| Web Pages | BeautifulSoup and requests | BeautifulSoup is a library for parsing
    HTML and XML documents. Combined with requests for sending HTTP requests, it enables
    data extraction from web pages, essential for web scraping tasks. | [BeautifulSoup
    documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), [requests
    documentation](https://requests.readthedocs.io/en/latest/) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 网页 | BeautifulSoup 和 requests | BeautifulSoup 是一个用于解析 HTML 和 XML 文档的库。结合
    requests 进行 HTTP 请求，它能够从网页中提取数据，这对于网页抓取任务至关重要。 | [BeautifulSoup 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/),
    [requests 文档](https://requests.readthedocs.io/en/latest/) |'
- en: '| SQL Databases | SQLAlchemy | SQLAlchemy is a SQL toolkit and Object-Relational
    Mapping (ORM) library for Python, providing a full suite of enterprise-level persistence
    patterns. | [SQLAlchemy documentation](https://www.sqlalchemy.org/) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| SQL 数据库 | SQLAlchemy | SQLAlchemy 是一个 SQL 工具包和 Python 的对象关系映射（ORM）库，提供了一整套企业级持久性模式。
    | [SQLAlchemy 文档](https://www.sqlalchemy.org/) |'
- en: '| S3 Storage | boto3 | boto3 is the Amazon Web Services (AWS) SDK for Python,
    allowing developers to use services like Amazon S3 and EC2\. It enables interaction
    with AWS services, including uploading, downloading, and managing S3 bucket files.
    | [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| S3 存储 | boto3 | boto3 是 Amazon Web Services (AWS) 的 Python SDK，允许开发人员使用 Amazon
    S3 和 EC2 等服务。它使与 AWS 服务的交互成为可能，包括上传、下载和管理 S3 桶中的文件。 | [boto3 文档](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    |'
- en: '| Data Integration | RapidMiner | RapidMiner is a comprehensive environment
    for data preparation, machine learning, and predictive analytics, allowing efficient
    processing and transformation of raw data into actionable insights. | [RapidMiner
    documentation](https://rapidminer.com/) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 数据集成 | RapidMiner | RapidMiner 是一个全面的数据准备、机器学习和预测分析环境，允许高效地处理和转换原始数据，提取有用的见解。
    | [RapidMiner 文档](https://rapidminer.com/) |'
- en: '| Data Cleaning | Trifacta Wrangler | Trifacta Wrangler focuses on simplifying
    and automating data wrangling processes, transforming raw data into clean and
    structured formats. | [Trifacta Wrangler documentation](https://www.trifacta.com/)
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 数据清洗 | Trifacta Wrangler | Trifacta Wrangler 专注于简化和自动化数据清洗过程，将原始数据转换为干净且结构化的格式。
    | [Trifacta Wrangler 文档](https://www.trifacta.com/) |'
- en: 'Table 3.1: Python libraries and tools for data collection and integration in
    various formats, providing an overview of commonly used libraries, their functions,
    and links to their official documentation for efficient data management and processing.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3.1: 各种格式的数据收集与集成的 Python 库和工具概述，提供了常用库、其功能及其官方文档链接，以便于高效的数据管理和处理。'
- en: 3.1.2 Data Preprocessing and Formatting
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 数据预处理与格式化
- en: 'Data preprocessing and formatting are crucial for ensuring high-quality data
    for fine-tuning. This step involves tasks such as cleaning the data, handling
    missing values, and formatting the data to match the specific requirements of
    the task. Several libraries assist with text data processing and Table [3.2](#Ch3.T2
    "Table 3.2 ‣ 3.1.2 Data Preprocessing and Formatting ‣ 3.1 Steps Involved in Data
    Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")
    contains some of the most commonly used data preprocessing libraries in python.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '数据预处理与格式化对于确保高质量的数据以进行微调至关重要。此步骤包括清理数据、处理缺失值以及格式化数据以符合任务的特定要求。几个库可以帮助处理文本数据，表
    [3.2](#Ch3.T2 "Table 3.2 ‣ 3.1.2 Data Preprocessing and Formatting ‣ 3.1 Steps
    Involved in Data Preparation ‣ Chapter 3 Stage 1: Data Preparation ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)") 包含了一些在 Python 中最常用的数据预处理库。'
- en: '| Library Name | Data Preprocessing Options | Link |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 库名称 | 数据预处理选项 | 链接 |'
- en: '| --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| spaCy | spaCy provides robust capabilities for text preprocessing, including
    tokenization, lemmatization, and efficient sentence boundary detection. | [spaCy
    documentation](https://spacy.io/) |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| spaCy | spaCy 提供了强大的文本预处理功能，包括分词、词形还原和高效的句子边界检测。 | [spaCy documentation](https://spacy.io/)
    |'
- en: '| NLTK | NLTK offers a comprehensive set of tools for data preprocessing, such
    as tokenization, stemming, and stop word removal. | [NLTK documentation](https://www.nltk.org/)
    |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| NLTK | NLTK 提供了一整套数据预处理工具，如分词、词干提取和停用词移除。 | [NLTK documentation](https://www.nltk.org/)
    |'
- en: '| HuggingFace | HuggingFace provides extensive capabilities for text preprocessing
    through its transformers library, including functionalities for tokenization and
    support for various pre-trained models. | [HuggingFace documentation](https://huggingface.co/)
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| HuggingFace | HuggingFace 通过其 transformers 库提供了广泛的文本预处理功能，包括分词功能和对各种预训练模型的支持。
    | [HuggingFace documentation](https://huggingface.co/) |'
- en: '| KNIME | KNIME Analytics Platform allows visual workflow design for data integration,
    preprocessing, and advanced manipulations like text mining and image analysis.
    | [KNIME documentation](https://www.knime.com/) |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| KNIME | KNIME Analytics Platform 允许通过可视化工作流设计进行数据集成、预处理以及文本挖掘和图像分析等高级操作。
    | [KNIME documentation](https://www.knime.com/) |'
- en: 'Table 3.2: Outline of Python libraries commonly used for text data preprocessing,
    including spaCy, NLTK, HuggingFace, and KNIME. It details the specific preprocessing
    options offered by each library and provides links to their official documentation
    for users seeking more in-depth guidance on their use.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2：概述了常用的文本数据预处理 Python 库，包括 spaCy、NLTK、HuggingFace 和 KNIME。详细介绍了每个库提供的特定预处理选项，并提供了其官方文档的链接，以便用户获取更深入的使用指导。
- en: 3.1.3 Handling Data Imbalance
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 处理数据不平衡
- en: 'Handling imbalanced datasets is crucial for ensuring balanced performance across
    all classes. Several techniques and strategies are employed:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集对于确保所有类别的性能平衡至关重要。采用了几种技术和策略：
- en: '1.'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Over-sampling and Under-sampling: Techniques like SMOTE (Synthetic Minority
    Over-sampling Technique) generate synthetic examples to achieve balance.'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过采样和欠采样：像SMOTE（合成少数类过采样技术）这样的技术通过生成合成示例来实现数据平衡。
- en: 'Python Library:  [imbalanced-learn](https://imbalanced-learn.org/stable/references/index.html)
    Description: imbalanced-learn provides various methods to deal with imbalanced
    datasets, including oversampling techniques like SMOTE.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [imbalanced-learn](https://imbalanced-learn.org/stable/references/index.html)
    描述：imbalanced-learn 提供了多种处理不平衡数据集的方法，包括像 SMOTE 这样的过采样技术。
- en: '2.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Adjusting Loss Function: Modify the loss function to give more weight to the
    minority class, setting class weights inversely proportional to the class frequencies.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整损失函数：修改损失函数以给予少数类更多权重，将类别权重设置为与类别频率成反比。
- en: '3.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight
    easy examples and focus training on hard negatives.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 焦点损失：交叉熵损失的一种变体，添加了一个因子来降低简单示例的权重，并将训练重点放在困难的负例上。
- en: 'Python Library:  [focal_loss](https://pypi.org/project/focal-loss/)'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [focal_loss](https://pypi.org/project/focal-loss/)
- en: 'Description: The focal_loss package provides robust implementations of various
    focal loss functions, including BinaryFocalLoss and SparseCategoricalFocalLoss.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：focal_loss 包提供了多种焦点损失函数的强大实现，包括 BinaryFocalLoss 和 SparseCategoricalFocalLoss。
- en: '4.'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Cost-sensitive Learning: Incorporating the cost of misclassifications directly
    into the learning algorithm, assigning a higher cost to misclassifying minority
    class samples.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本敏感学习：将误分类的成本直接纳入学习算法中，为误分类少数类样本分配更高的成本。
- en: '5.'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Ensemble Methods: Using techniques like bagging and boosting to combine multiple
    models and handle class imbalance.'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成方法：使用像集成和提升这样的技术来结合多个模型并处理类别不平衡。
- en: 'Python Library:  [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库： [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html)
- en: 'Description: scikit-learn provides robust implementations of various ensemble
    methods, including bagging and boosting.'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了各种集成方法的强大实现，包括集成和提升。
- en: '6.'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Stratified Sampling: Ensuring that each mini-batch during training contains
    an equal or proportional representation of each class.'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层抽样：确保训练期间每个小批次都包含每个类别的相等或按比例代表。
- en: 'Python Library:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
- en: 'Description: scikit-learn offers tools for stratified sampling, ensuring balanced
    representation across classes.'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了分层抽样的工具，确保各类之间的平衡表示。
- en: '7.'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Data Cleaning: Removing noisy and mislabelled data, which can disproportionately
    affect the minority class.'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据清理：去除噪声和错误标记的数据，这些数据可能对少数类产生不成比例的影响。
- en: 'Python Library:  [pandas.DataFrame.sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [pandas.DataFrame.sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)'
- en: 'Description: pandas provides methods for sampling data from DataFrames, useful
    for data cleaning and preprocessing.'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：pandas 提供了从 DataFrames 中抽样数据的方法，适用于数据清理和预处理。
- en: '8.'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and
    Cohen’s Kappa are more informative than accuracy when dealing with imbalanced
    datasets.'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用适当的指标：像精确度-召回 AUC、F1 分数和 Cohen’s Kappa 等指标在处理不平衡数据集时，比准确率更具信息量。
- en: 'Python Library:  [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)'
- en: 'Description: scikit-learn offers a comprehensive set of tools for evaluating
    the performance of classification models, particularly with imbalanced datasets.'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：scikit-learn 提供了一套全面的工具来评估分类模型的性能，特别是在不平衡数据集上。
- en: 3.1.4 Splitting Dataset
  id: totrans-431
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 数据集拆分
- en: 'Splitting the dataset for fine-tuning involves dividing it into training and
    validation sets, typically using an 80:20 ratio. Different techniques include:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集进行精调时，涉及将其划分为训练集和验证集，通常采用 80:20 的比例。不同的技术包括：
- en: '1.'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Random Sampling: Selecting a subset of data randomly to create a representative
    sample.'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机抽样：随机选择数据子集以创建具有代表性的样本。
- en: 'Python Library:  [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)'
- en: '2.'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Stratified Sampling: Dividing the dataset into subgroups and sampling from
    each to maintain class balance.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层抽样：将数据集划分为子组，并从每个子组中抽样以保持类别平衡。
- en: 'Python Library:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)'
- en: '3.'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'K-Fold Cross Validation: Splitting the dataset into K folds and performing
    training and validation K times.'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: K 折交叉验证：将数据集拆分为 K 个折叠，进行 K 次训练和验证。
- en: 'Python Library:  [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)'
- en: '4.'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Leave-One-Out Cross Validation: Using a single data point as the validation
    set and the rest for training, repeated for each data point.'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 留一交叉验证：使用一个数据点作为验证集，其余数据用于训练，对每个数据点重复此过程。
- en: 'Python Library:  [sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Python 库:  [sklearn.model_selection.LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)'
- en: Further details can be found in [scikit-learn’s documentation on model selection](https://scikit-learn.org/stable/api/sklearn.model_selection.html).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息请参阅 [scikit-learn 关于模型选择的文档](https://scikit-learn.org/stable/api/sklearn.model_selection.html)。
- en: 3.2 Existing and Potential Research Methodologies
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 现有和潜在的研究方法
- en: 3.2.1 Data Annotation
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 数据标注
- en: 'Data annotation involves labelling or tagging textual data with specific attributes
    relevant to the model’s training objectives. This process is crucial for supervised
    learning tasks and greatly influences the performance of the fine-tuned model.
    Recent research highlights various approaches to data annotation:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注涉及使用与模型训练目标相关的特定属性对文本数据进行标签或标记。这一过程对监督学习任务至关重要，并极大地影响了精调模型的性能。最近的研究强调了数据标注的各种方法：
- en: •
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Human Annotation: Manual annotation by human experts remains a gold standard
    due to its accuracy and context understanding. However, it is time-consuming and
    costly for large datasets [[16](#bib.bib16)]. Tools like Excel, Prodigy¹¹1[https://prodi.gy](https://prodi.gy),
    and Innodata²²2[https://innodata.com/](https://innodata.com/) facilitate this
    process.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工标注：由于其准确性和对上下文的理解，人工专家手动标注仍然是黄金标准。然而，对于大型数据集来说，这一过程既耗时又昂贵[[16](#bib.bib16)]。像
    Excel、Prodigy¹¹1[https://prodi.gy](https://prodi.gy) 和 Innodata²²2[https://innodata.com/](https://innodata.com/)
    等工具可以帮助完成这一过程。
- en: •
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semi-automatic Annotation: Combining machine learning algorithms with human
    review to create labelled datasets more efficiently. This approach balances efficiency
    and accuracy. Tools like Snorkel³³3[https://snorkel.ai/](https://snorkel.ai/)
    use weak supervision to generate initial labels, which are then refined by human
    annotators [[17](#bib.bib17)].'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半自动标注：将机器学习算法与人工审核相结合，以更高效地创建标记数据集。这种方法在效率和准确性之间取得平衡。像 Snorkel³³3[https://snorkel.ai/](https://snorkel.ai/)
    等工具使用弱监督来生成初步标签，然后由人工标注者进行细化[[17](#bib.bib17)]。
- en: •
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Automatic Annotation: Fully automated annotation leverages machine learning
    algorithms to label data without human intervention, offering scalability and
    cost-effectiveness. Services like Amazon SageMaker Ground Truth⁴⁴4[https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/)
    utilise machine learning to automate data labelling, although the accuracy may
    vary depending on the complexity of the task [[18](#bib.bib18)].'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动标注：完全自动化的标注利用机器学习算法对数据进行标记，无需人工干预，提供了可扩展性和成本效益。像 Amazon SageMaker Ground Truth⁴⁴4[https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/)
    等服务利用机器学习自动化数据标记，尽管准确性可能会根据任务的复杂性而有所不同[[18](#bib.bib18)]。
- en: 3.2.2 Data Augmentation
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 数据增强
- en: 'Data Augmentation (DA) techniques expand training datasets artificially to
    address data scarcity and improve model performance. Advanced techniques often
    used in NLP include:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强（DA）技术通过人工扩展训练数据集，以解决数据稀缺问题并提高模型性能。在 NLP 中常用的高级技术包括：
- en: •
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words
    with their semantic equivalents, thereby generating new data instances [[19](#bib.bib19),
    [20](#bib.bib20)].'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词嵌入：使用词嵌入技术，如 Word2Vec 和 GloVe，将词替换为其语义等价词，从而生成新的数据实例[[19](#bib.bib19), [20](#bib.bib20)]。
- en: •
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Back Translation: Translating text to another language and then back to the
    original language to create paraphrased data. This technique helps in generating
    diverse training samples [[21](#bib.bib21)]. Tools like Google Translate API⁵⁵5[https://translate.google.com/?sl=auto&tl=en&op=translate](https://translate.google.com/?sl=auto&tl=en&op=translate)
    are commonly used for this purpose.'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回译：将文本翻译成另一种语言，然后再翻译回原始语言，以创建同义改写数据。这一技术有助于生成多样化的训练样本[[21](#bib.bib21)]。像 Google
    Translate API⁵⁵5[https://translate.google.com/?sl=auto&tl=en&op=translate](https://translate.google.com/?sl=auto&tl=en&op=translate)
    等工具通常用于此目的。
- en: •
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial Attacks: Generating augmented data through adversarial examples
    that slightly modify the original text to create new training samples while preserving
    the original meaning [[22](#bib.bib22)]. Libraries like TextAttack⁶⁶6[https://github.com/QData/TextAttack](https://github.com/QData/TextAttack)
    provide frameworks for such augmentations.'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗攻击：通过对抗样本生成增强数据，这些样本稍微修改原始文本，以创建新的训练样本，同时保留原始含义[[22](#bib.bib22)]。像 TextAttack⁶⁶6[https://github.com/QData/TextAttack](https://github.com/QData/TextAttack)
    等库提供了这种增强的框架。
- en: •
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NLP-AUG⁷⁷7[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug):
    This library offers a variety of augmenters for character, word, sentence, audio,
    and spectrogram augmentation, enhancing dataset diversity.'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NLP-AUG⁷⁷7[https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug)：该库提供了多种数据增强器，用于字符、词、句子、音频和频谱图增强，增强了数据集的多样性。
- en: 3.2.3 Synthetic Data Generation using LLMs
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 使用 LLMs 生成合成数据
- en: 'Large Language Models (LLMs) can generate synthetic data through innovative
    techniques such as:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以通过创新技术生成合成数据，例如：
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating
    relevant and high-quality synthetic data [[23](#bib.bib23)].'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程：设计特定的提示以指导 LLMs，如 GPT-3，生成相关的高质量合成数据[[23](#bib.bib23)]。
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Step Generation: Employing iterative generation processes where LLMs
    generate initial data that is refined through subsequent steps [[24](#bib.bib24)].
    This method can produce high-quality synthetic data for various tasks, including
    summarising and bias detection.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多步骤生成：采用迭代生成过程，其中LLMs生成初步数据，并通过后续步骤进行精炼[[24](#bib.bib24)]。这种方法可以为各种任务生成高质量的合成数据，包括总结和偏差检测。
- en: It is crucial to verify the accuracy and relevance of synthetic data generated
    by LLMs before using them for fine-tuning processes [[25](#bib.bib25)].
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用LLMs生成的合成数据进行微调过程之前，验证其准确性和相关性是至关重要的[[25](#bib.bib25)]。
- en: 3.3 Challenges in Data Preparation for Fine-Tuning LLMs
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 微调LLMs的数据准备挑战
- en: 'Key challenges in data preparation include:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备中的关键挑战包括：
- en: '1.'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Domain Relevance: Ensuring that the data is relevant to the specific domain
    for accurate model performance. Mismatched domain data can lead to poor generalisation
    and inaccurate outputs [[26](#bib.bib26)].'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域相关性：确保数据与特定领域相关，以确保模型性能的准确性。不匹配的领域数据可能导致较差的泛化能力和不准确的输出[[26](#bib.bib26)]。
- en: '2.'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Data Diversity: Including diverse and well-balanced data to prevent model biases
    and improve generalisation. A lack of diversity can cause the model to perform
    poorly on underrepresented scenarios [[27](#bib.bib27)].'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据多样性：包含多样化和均衡的数据，以防止模型偏见并提高泛化能力。缺乏多样性可能导致模型在代表性不足的场景中表现不佳[[27](#bib.bib27)]。
- en: '3.'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Data Size: Managing and processing large datasets, with at least 1000 samples
    recommended for effective fine-tuning. However, large datasets pose challenges
    in terms of storage, computational requirements, and processing time.'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据规模：管理和处理大型数据集，建议至少使用1000个样本以实现有效的微调。然而，大型数据集在存储、计算要求和处理时间方面会带来挑战。
- en: '4.'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies
    are critical for providing clean inputs to the model. Poorly preprocessed data
    can degrade model performance significantly.'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据清洗和预处理：去除噪音、错误和不一致性对于向模型提供干净的输入至关重要。预处理不当的数据可能会显著降低模型性能。
- en: '5.'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Data Annotation: Ensuring precise and consistent labelling is essential for
    tasks requiring labelled data. Inconsistent annotation can lead to unreliable
    model predictions.'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据标注：确保精确和一致的标注对于需要标注数据的任务至关重要。不一致的标注可能导致模型预测的不可靠。
- en: '6.'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Handling Rare Cases: Adequately representing rare but important instances in
    the dataset to ensure the model can generalise to less frequent but critical scenarios.'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理稀有案例：充分代表数据集中的稀有但重要的实例，以确保模型能够泛化到较少出现但关键的场景。
- en: '7.'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Ethical Considerations: Scrutinising data for harmful or biased content to
    prevent unintended consequences. Ethical data handling includes removing biases
    and ensuring privacy [[28](#bib.bib28)].'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理考虑：审查数据以防止有害或偏见内容，以防止意外后果。伦理的数据处理包括去除偏见和确保隐私[[28](#bib.bib28)]。
- en: 3.4 Available LLM Fine-Tuning Datasets
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 可用的LLM微调数据集
- en: For a comprehensive list of datasets suitable for fine-tuning LLMs, refer to
    resources like [LLMXplorer](https://forms.gle/TNUbqHiCBsinD4Bu8), which provides
    domain and task-specific datasets.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取适合微调LLMs的数据集的全面列表，请参阅像[LLMXplorer](https://forms.gle/TNUbqHiCBsinD4Bu8)这样的资源，该资源提供领域和任务特定的数据集。
- en: 3.5 Best Practices
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 最佳实践
- en: 3.5.1 High-Quality Data Collection
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 高质量数据收集
- en: Ensuring high-quality, diverse, and representative data is critical. Leveraging
    curated sources and ensuring comprehensive coverage across different scenarios
    enhances model robustness [[29](#bib.bib29)]. Tools like DataRobot Paxata⁸⁸8[https://www.datarobot.com/platform/preparation/](https://www.datarobot.com/platform/preparation/)
    and KNIME Analytics Platform⁹⁹9[https://www.knime.com/](https://www.knime.com/)
    offer robust data profiling and transformation capabilities.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 确保高质量、多样化和代表性的数据至关重要。利用策划的资源并确保覆盖不同场景，可以增强模型的鲁棒性[[29](#bib.bib29)]。像DataRobot
    Paxata⁸⁸8[https://www.datarobot.com/platform/preparation/](https://www.datarobot.com/platform/preparation/)和KNIME
    Analytics Platform⁹⁹9[https://www.knime.com/](https://www.knime.com/)等工具提供了强大的数据分析和转换能力。
- en: 3.5.2 Effective Data Preprocessing
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 有效的数据预处理
- en: Proper data preprocessing is essential for model performance. Utilising libraries
    like spaCy, NLTK, and HuggingFace Transformers can streamline preprocessing tasks.
    Platforms like Trifacta Wrangler and RapidMiner automate data cleaning tasks,
    improving efficiency and ensuring consistency [[30](#bib.bib30)].
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的数据预处理对于模型性能至关重要。利用像spaCy、NLTK和HuggingFace Transformers这样的库可以简化预处理任务。像Trifacta
    Wrangler和RapidMiner这样的平台可以自动化数据清理任务，提高效率并确保一致性[[30](#bib.bib30)]。
- en: 3.5.3 Managing Data Imbalance
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 管理数据不平衡
- en: Addressing data imbalance is crucial. Techniques like over-sampling, under-sampling,
    and SMOTE help balance datasets. Libraries like imbalanced-learn and ensemble
    methods in scikit-learn provide robust tools for managing imbalanced datasets
    [[31](#bib.bib31)].
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据不平衡至关重要。过采样、欠采样和SMOTE等技术有助于平衡数据集。像imbalanced-learn这样的库以及scikit-learn中的集成方法提供了管理不平衡数据集的强大工具[[31](#bib.bib31)]。
- en: 3.5.4 Augmenting and Annotating Data
  id: totrans-497
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 数据增强和标注
- en: Data augmentation and annotation improve model robustness. Tools like NLP-AUG,
    TextAttack, and Snorkel offer sophisticated capabilities for creating diverse
    and well-labelled datasets [[32](#bib.bib32), [33](#bib.bib33)].
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和标注提高了模型的稳健性。像NLP-AUG、TextAttack和Snorkel这样的工具提供了创建多样且标注良好的数据集的先进能力[[32](#bib.bib32),
    [33](#bib.bib33)]。
- en: 3.5.5 Ethical Data Handling
  id: totrans-499
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 伦理数据处理
- en: Ensuring ethical data handling involves thorough scrutiny for biases and privacy
    concerns. Implementing privacy-preserving techniques and filtering harmful content
    is critical. Services like Amazon SageMaker Ground Truth ensure scalable and secure
    data annotation [[34](#bib.bib34)].
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 确保伦理数据处理涉及对偏差和隐私问题的彻底审查。实施隐私保护技术和过滤有害内容至关重要。像Amazon SageMaker Ground Truth这样的服务确保可扩展和安全的数据标注[[34](#bib.bib34)]。
- en: 3.5.6 Regular Evaluation and Iteration
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.6 定期评估和迭代
- en: Continuous evaluation and iteration of the data preparation pipeline help maintain
    data quality and relevance. Leveraging feedback loops and performance metrics
    ensures ongoing improvements and adaptation to new data requirements.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 持续评估和迭代数据准备管道有助于维护数据质量和相关性。利用反馈循环和性能指标确保持续改进和适应新的数据需求。
- en: By integrating these best practices, researchers and practitioners can enhance
    the effectiveness of LLM fine-tuning, ensuring robust and reliable model performance.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这些最佳实践，研究人员和从业者可以提高LLM微调的有效性，确保模型性能的可靠性和稳健性。
- en: 'Chapter 4 Stage 2: Model Initialisation'
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章 第二阶段：模型初始化
- en: 4.1 Steps Involved in Model Initialisation
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型初始化中的步骤
- en: '![Refer to caption](img/9cf39698e03417a076e40e2f3c6c5396.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9cf39698e03417a076e40e2f3c6c5396.png)'
- en: 'Figure 4.1: Sequential steps involved in Initialising a Large Language Model
    (LLM), illustrating the process from setting up the environment to executing tasks.
    Each step is critical for ensuring that the LLM is correctly configured and ready
    for operation. This includes installing necessary dependencies, importing libraries,
    selecting and downloading the appropriate language model from a repository, and
    finally, loading the model to perform specific tasks.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：初始化大型语言模型（LLM）中的顺序步骤，展示了从设置环境到执行任务的过程。每个步骤对于确保LLM正确配置并准备好运行至关重要。这包括安装必要的依赖项、导入库、从仓库中选择并下载适当的语言模型，最后加载模型以执行特定任务。
- en: '1.'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Set Up the Environment: Configure your environment, such as setting up GPU/TPU
    usage if available, which can significantly speed up model loading and inference.'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置环境：配置环境，例如设置GPU/TPU使用（如果可用），这可以显著加快模型加载和推理速度。
- en: '2.'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Install the Dependencies: Ensure that all necessary software and libraries
    are installed. This typically includes package managers like pip and frameworks
    like PyTorch or TensorFlow.'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安装依赖项：确保所有必要的软件和库已安装。这通常包括像pip这样的包管理器以及像PyTorch或TensorFlow这样的框架。
- en: '3.'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Import the Libraries: Import the required libraries in your script or notebook.
    Common libraries include transformers from Hugging Face, torch for PyTorch, and
    other utility libraries.'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入库：在脚本或笔记本中导入所需的库。常见的库包括来自Hugging Face的transformers、PyTorch的torch以及其他实用库。
- en: '4.'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Choose the Language Model: Select the appropriate pre-trained language model
    based on your task requirements. This could be models like BERT, GPT-3, or others
    available on platforms like Hugging Face’s Model Hub.'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择语言模型：根据任务需求选择适当的预训练语言模型。这可能是像BERT、GPT-3这样的模型，或在Hugging Face的模型库中提供的其他模型。
- en: '5.'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Download the Model from the Repository: Use the chosen framework’s functions
    to download the pre-trained model from an online repository. For instance, using
    transformers, you might use AutoModel.from_pretrained(’model_name’).'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从仓库下载模型：使用所选框架的函数从在线仓库下载预训练模型。例如，使用 transformers，你可以使用 AutoModel.from_pretrained(’model_name’)。
- en: '6.'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Load the Model in the Memory: Load the model into memory, ready for inference
    or further fine-tuning. This step ensures the model weights are initialised and
    ready for use.'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将模型加载到内存中：将模型加载到内存中，准备进行推理或进一步微调。这一步骤确保模型权重被初始化并准备好使用。
- en: '7.'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Execute Tasks: Perform the desired tasks using the loaded model. This could
    involve making predictions, generating text, or fine-tuning the model on a new
    dataset.'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行任务：使用加载的模型执行所需的任务。这可能包括进行预测、生成文本或在新数据集上微调模型。
- en: 4.2 Tools and Libraries for Model Initialisation
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 模型初始化的工具和库
- en: 'Python offers a wide range of libraries for Initialising large language models,
    providing access to both open and closed-source models. Here are some notable
    libraries:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了广泛的库来初始化大型语言模型，提供对开源和闭源模型的访问。以下是一些值得注意的库：
- en: '1.'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Python Library:  [*HuggingFace*](https://huggingface.co/docs/transformers/en/index)'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 库：[*HuggingFace*](https://huggingface.co/docs/transformers/en/index)
- en: 'Description: HuggingFace is renowned for its support of numerous pre-trained
    large language models, ranging from Phi-3 mini to Llama-3 70B. The transformers
    library, part of HuggingFace, enables users to access these models via classes
    such as AutoModelForCausalLM. This library supports loading fine-tuned models
    as well as 4-bit quantised models. Additionally, the transformers library includes
    the ”pipeline” feature, making it easy to use pre-trained models for various tasks
    [[35](#bib.bib35)].'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：HuggingFace 因其对大量预训练的大型语言模型的支持而闻名，从 Phi-3 mini 到 Llama-3 70B。HuggingFace
    旗下的 transformers 库使用户可以通过诸如 AutoModelForCausalLM 等类访问这些模型。该库支持加载微调后的模型以及 4 位量化模型。此外，transformers
    库还包括“pipeline”功能，使得使用预训练模型执行各种任务变得简单[[35](#bib.bib35)]。
- en: '2.'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Python Framework:  [*PyTorch*](https://pytorch.org/docs/stable/index.html)'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 框架：[*PyTorch*](https://pytorch.org/docs/stable/index.html)
- en: 'Description: PyTorch offers comprehensive tools and libraries for Initialising
    and fine-tuning large language models. It provides a flexible and efficient platform
    for building and deploying deep learning models. HuggingFace’s transformers library
    bridges the gap between PyTorch and other frameworks, enhancing its usability
    for state-of-the-art language models [[36](#bib.bib36)].'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：PyTorch 提供了全面的工具和库，用于初始化和微调大型语言模型。它提供了一个灵活高效的平台，用于构建和部署深度学习模型。HuggingFace
    的 transformers 库弥合了 PyTorch 与其他框架之间的差距，提高了其在最先进的语言模型中的可用性[[36](#bib.bib36)]。
- en: '3.'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Python Framework:  [*TensorFlow*](https://www.tensorflow.org/tutorials)'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Python 框架：[*TensorFlow*](https://www.tensorflow.org/tutorials)
- en: 'Description: TensorFlow also provides extensive tools and libraries for Initialising
    and fine-tuning large language models. Similar to PyTorch, it benefits from the
    HuggingFace transformers library, which provides a versatile and user-friendly
    API and interface for working with the latest advancements in large language models
    [[37](#bib.bib37)].'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：TensorFlow 同样提供了广泛的工具和库，用于初始化和微调大型语言模型。与 PyTorch 类似，它也受益于 HuggingFace transformers
    库，该库为处理最新的大型语言模型提供了多功能和用户友好的 API 和接口[[37](#bib.bib37)]。
- en: 4.3 Challenges in Model Initialisation
  id: totrans-533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 模型初始化中的挑战
- en: '| Challenge | Description |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 描述 |'
- en: '| --- | --- |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Alignment with the Target Task | It’s essential that the pre-trained model
    closely aligns with your specific task or domain. This initial alignment serves
    as a solid foundation for further fine-tuning efforts, leading to improved efficiency
    and results [[38](#bib.bib38)]. |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 与目标任务的一致性 | 预训练模型与特定任务或领域的紧密对接至关重要。这种初始对接为进一步的微调工作奠定了坚实的基础，从而提高了效率和结果[[38](#bib.bib38)]。
    |'
- en: '| Understanding the Pre-trained Model | Before making a selection, it’s crucial
    to thoroughly comprehend the architecture, capabilities, limitations, and the
    tasks the model was originally trained on. Without this understanding, fine-tuning
    efforts may not yield the desired outcomes [[23](#bib.bib23)]. |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| 理解预训练模型 | 在做出选择之前，必须彻底理解模型的架构、能力、局限性以及模型最初训练的任务。如果没有这种理解，微调工作可能不会产生期望的结果[[23](#bib.bib23)]。
    |'
- en: '| Availability and Compatibility | Careful consideration of a model’s documentation,
    license, maintenance, and update frequency is necessary to avoid potential issues
    and ensure smooth integration into your application. |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 可用性和兼容性 | 需要仔细考虑模型的文档、许可证、维护和更新频率，以避免潜在问题，并确保顺利集成到您的应用中。 |'
- en: '| Model Architecture | Not all models excel at every task. Each model architecture
    has its strengths and weaknesses, so selecting one aligned with your specific
    task is essential for favourable outcomes [[39](#bib.bib39)]. |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 并非所有模型都擅长所有任务。每种模型架构都有其优点和缺点，因此选择一个与特定任务相匹配的模型对于获得良好的结果至关重要[[39](#bib.bib39)]。
    |'
- en: '| Resource Constraints | Loading pre-trained LLMs is resource-heavy and requires
    more computation. These models need high-performance CPUs and GPUs and a significant
    amount of disk space. For instance, the Llama 3 8B model requires a minimum of
    16GB of memory to load and run the inference. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 资源限制 | 加载预训练的 LLM 资源消耗很大，需要更多的计算。这些模型需要高性能的 CPU 和 GPU 以及大量的磁盘空间。例如，Llama
    3 8B 模型需要至少 16GB 的内存才能加载和运行推断。 |'
- en: '| Privacy | Privacy and confidentiality are crucial factors when selecting
    a large language model (LLM). Many businesses prefer not to share their data with
    external LLM providers. In such instances, hosting an LLM on local servers or
    using pre-trained LLMs available through private cloud providers can be viable
    solutions. These approaches ensure that data remains within the company’s premises,
    thereby preserving privacy and confidentiality. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 隐私 | 隐私和保密性在选择大型语言模型（LLM）时至关重要。许多企业希望不与外部 LLM 提供商分享其数据。在这种情况下，将 LLM 托管在本地服务器上或使用通过私人云提供商提供的预训练
    LLM 可以是可行的解决方案。这些方法确保数据保持在公司的场所内，从而保护隐私和保密性。 |'
- en: '| Cost and Maintenance | Hosting LLMs on local servers entails significant
    time and expense for setup and ongoing maintenance. Conversely, utilising cloud
    vendors alleviates concerns about resource maintenance but incurs monthly billing
    costs. These charges are typically based on factors such as model size and the
    volume of requests per minute. |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 成本和维护 | 在本地服务器上托管 LLM 需要大量的时间和费用来设置和维护。相比之下，使用云供应商可以缓解对资源维护的担忧，但会产生每月的账单费用。这些费用通常基于模型大小和每分钟请求量等因素。
    |'
- en: '| Model Size and Quantisation | utilising a pre-trained model with high memory
    consumption can still be viable by employing its quantised version. Through quantisation,
    pre-trained weights can be loaded with reduced precision, typically 4-bit or 8-bit
    floating point, substantially diminishing parameter volume while maintaining considerable
    accuracy [[40](#bib.bib40)]. |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小和量化 | 利用高内存消耗的预训练模型仍然可行，通过采用其量化版本。通过量化，预训练权重可以以较低的精度加载，通常为 4 位或 8 位浮点数，从而显著减少参数体积，同时保持相当的准确性[[40](#bib.bib40)]。
    |'
- en: '| Pre-training Datasets | Examine the datasets used for pre-training to gauge
    the model’s understanding of language. These are important as there are models
    available specifically for performing code generation, and we do not want to use
    those models for finance text classification [[41](#bib.bib41)]. |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 预训练数据集 | 检查用于预训练的数据集，以评估模型对语言的理解。这些数据集很重要，因为有些模型专门用于代码生成，我们不希望将这些模型用于金融文本分类[[41](#bib.bib41)]。
    |'
- en: '| Bias Awareness | Be vigilant regarding potential biases in pre-trained models,
    especially if unbiased predictions are required. The bias awareness can be evaluated
    by testing different models and backtracking the datasets used for pre-training
    [[42](#bib.bib42)]. |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 偏见意识 | 要警惕预训练模型中的潜在偏见，特别是当需要无偏预测时。通过测试不同的模型和追溯用于预训练的数据集来评估偏见意识[[42](#bib.bib42)]。'
- en: 'Table 4.1: Comprehensive Overview of Challenges in Initialising a Large Language
    Model (LLM). This table highlights critical considerations, such as the importance
    of aligning pre-trained models with specific tasks, understanding model architecture
    and compatibility, managing resource constraints, and ensuring data privacy. Additionally,
    it discusses the challenges related to cost, maintenance, and the complexities
    of model size, quantisation, and bias awareness. Each challenge is associated
    with specific references to ensure thorough understanding and proper model deployment.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：大型语言模型（LLM）初始化挑战的综合概述。此表突出了关键考虑因素，例如将预训练模型与特定任务对齐的重要性、理解模型架构和兼容性、管理资源约束以及确保数据隐私。此外，它还讨论了与成本、维护、模型大小、量化和偏差意识相关的挑战。每个挑战都与具体参考文献相关，以确保对模型部署的彻底理解。
- en: 4.4 Tutorials
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 教程
- en: '1.'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[Summarisation using Llama 3](https://medium.com/@manuelescobar-dev/implementing-and-running-llama-3-with-hugging-faces-transformers-library-40e9754d8c80)'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[使用 Llama 3 进行总结](https://medium.com/@manuelescobar-dev/implementing-and-running-llama-3-with-hugging-faces-transformers-library-40e9754d8c80)'
- en: '2.'
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[HuggingFace tutorial for getting started with LLMs](https://huggingface.co/docs/transformers/en/llm_tutorial)'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[HuggingFace LLM 入门教程](https://huggingface.co/docs/transformers/en/llm_tutorial)'
- en: '3.'
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '[PyTorch tutorial for fine-tuning models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[PyTorch 模型微调教程](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)'
- en: '4.'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '[TensorFlow tutorial for transformer models](https://www.tensorflow.org/tutorials/text/transformer)'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[TensorFlow 变换器模型教程](https://www.tensorflow.org/tutorials/text/transformer)'
- en: 'Chapter 5 Stage 3: Training Setup'
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五章 第三阶段：训练设置
- en: 5.1 Steps Involved in Training Setup
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 训练设置中的步骤
- en: '1.'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Setting up the training environment: When setting up the environment for training
    an LLM, it is crucial to configure high-performance hardware, such as GPUs or
    TPUs, and ensure proper installation of necessary software components like CUDA,
    cuDNN, and deep learning frameworks such as PyTorch or TensorFlow. Verify hardware
    recognition and compatibility with the software to leverage computational power
    effectively, reducing training time and improving model performance.'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置训练环境：在为训练大型语言模型（LLM）设置环境时，至关重要的是配置高性能硬件，如 GPU 或 TPU，并确保必要的软件组件（如 CUDA、cuDNN
    以及深度学习框架如 PyTorch 或 TensorFlow）正确安装。验证硬件识别和与软件的兼容性，以有效利用计算能力，缩短训练时间并提高模型性能。
- en: '2.'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Defining the Hyper-parameters: When defining hyperparameters for fine-tuning
    an LLM, it is essential to carefully tune key parameters such as learning rate,
    batch size, and epochs to optimise the model’s performance.'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义超参数：在为 LLM 微调定义超参数时，必须仔细调整关键参数，如学习率、批次大小和训练轮数，以优化模型的性能。
- en: '3.'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Initialising Optimisers and Loss Functions: When initialising optimisers and
    loss functions for fine-tuning an LLM, it is crucial to select the appropriate
    optimiser to efficiently update the model’s weights and the correct loss function
    to measure model performance [[43](#bib.bib43)].'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化优化器和损失函数：在为 LLM 微调初始化优化器和损失函数时，必须选择合适的优化器以高效更新模型权重，以及选择正确的损失函数以测量模型性能 [[43](#bib.bib43)]。
- en: 5.2 Setting up Training Environment
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 设置训练环境
- en: When fine-tuning a large language model (LLM), the computational environment
    plays a crucial role in ensuring efficient training. To achieve optimal performance,
    it’s essential to configure the environment with high-performance hardware such
    as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units). GPUs, such
    as the NVIDIA A100 or V100, are widely used for training deep learning models
    due to their parallel processing capabilities. For larger-scale operations, TPUs
    offered by Google Cloud can provide even greater acceleration [[44](#bib.bib44)].
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLM）时，计算环境在确保高效训练方面起着至关重要的作用。为实现最佳性能，必须使用高性能硬件，如 GPU（图形处理单元）或 TPU（张量处理单元）来配置环境。由于其并行处理能力，NVIDIA
    A100 或 V100 等 GPU 被广泛用于训练深度学习模型。对于大规模操作，谷歌云提供的 TPU 可以提供更大的加速 [[44](#bib.bib44)]。
- en: First, ensure that your system or cloud environment has the necessary hardware
    installed. For GPUs, this involves setting up CUDA¹¹1[https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)
    (Compute Unified Device Architecture) and cuDNN²²2[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)
    (CUDA Deep Neural Network library) from NVIDIA, which are essential for enabling
    GPU acceleration. For TPU usage, you would typically set up a Google Cloud environment
    with TPU instances, which includes configuring the TPU runtime in your training
    scripts.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你的系统或云环境安装了必要的硬件。对于GPU，这涉及到设置CUDA¹¹1[https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)（计算统一设备架构）和cuDNN²²2[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)（CUDA深度神经网络库），这些对于启用GPU加速至关重要。对于TPU使用，通常会在Google
    Cloud环境中设置TPU实例，包括在训练脚本中配置TPU运行时。
- en: Verify that your hardware is correctly recognised and utilised by your deep
    learning frameworks. In PyTorch, for instance, you can check GPU availability
    with torch.cuda.is_available(). Properly setting up and testing the hardware ensures
    that the training process can leverage the computational power effectively, reducing
    training time and improving model performance [[36](#bib.bib36)].
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 验证你的硬件是否被深度学习框架正确识别和利用。例如，在PyTorch中，你可以使用torch.cuda.is_available()检查GPU的可用性。正确设置和测试硬件可以确保训练过程能够有效利用计算能力，从而缩短训练时间并提高模型性能[[36](#bib.bib36)]。
- en: When fine-tuning an LLM, both software and hardware considerations are paramount
    to ensure a smooth and efficient training process. On the software side, you need
    a compatible deep learning framework like PyTorch or TensorFlow. These frameworks
    have extensive support for LLMs and provide utilities for efficient model training
    and evaluation. Installing the latest versions of these frameworks, along with
    any necessary dependencies, is crucial for leveraging the latest features and
    performance improvements [[45](#bib.bib45)].
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调LLM时，软件和硬件的考虑都至关重要，以确保训练过程顺利高效。在软件方面，你需要一个兼容的深度学习框架，比如PyTorch或TensorFlow。这些框架对LLM有广泛支持，并提供高效的模型训练和评估工具。安装这些框架的最新版本及任何必要的依赖项，对于利用最新特性和性能改进至关重要[[45](#bib.bib45)]。
- en: Additionally, use libraries like Hugging Face’s transformers to simplify the
    process of loading pre-trained models and tokenizers. This library is particularly
    well-suited for working with various LLMs and offers a user-friendly interface
    for model fine-tuning. Ensure that all software components, including libraries
    and dependencies, are compatible with your chosen framework and hardware setup
    [[35](#bib.bib35)].
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用像Hugging Face的transformers这样的库可以简化加载预训练模型和分词器的过程。这个库特别适合处理各种LLM，并提供了一个用户友好的模型微调接口。确保所有软件组件，包括库和依赖项，都与选择的框架和硬件设置兼容[[35](#bib.bib35)]。
- en: On the hardware side, consider the memory requirements of the model and your
    dataset. LLMs typically require substantial GPU memory, so opting for GPUs with
    higher VRAM (e.g., 16GB or more) can be beneficial. If your model is exceptionally
    large or if you are training with very large datasets, distributed training across
    multiple GPUs or TPUs might be necessary. This requires a careful setup of data
    parallelism or model parallelism techniques to efficiently utilise the available
    hardware [[46](#bib.bib46)].
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件方面，考虑模型和数据集的内存需求。LLM通常需要大量的GPU内存，因此选择具有更高VRAM（例如16GB或更多）的GPU可能会有益。如果你的模型非常大或者你正在使用非常大的数据集，可能需要在多个GPU或TPU上进行分布式训练。这需要仔细设置数据并行或模型并行技术，以高效利用可用硬件[[46](#bib.bib46)]。
- en: Lastly, ensure robust cooling and power supply for your hardware, as training
    LLMs can be resource-intensive, generating significant heat and requiring consistent
    power. Proper hardware setup not only enhances training performance but also prolongs
    the lifespan of your equipment [[47](#bib.bib47)].
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保你的硬件有足够的散热和电源供应，因为训练LLM可能会消耗大量资源，产生大量热量并需要稳定的电力供应。适当的硬件设置不仅能提高训练性能，还能延长设备的使用寿命[[47](#bib.bib47)]。
- en: 5.3 Defining Hyperparameters
  id: totrans-572
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 定义超参数
- en: 'Key hyperparameters like learning rate, batch size, epochs are crucial for
    enhancing the model’s performance and obtaining superior outcomes. This process
    entails adjusting hyperparameters and training settings to align with your particular
    use case. Below are the key hyperparameters:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 关键超参数如学习率、批量大小、训练周期对提升模型性能和获得更好结果至关重要。这个过程包括调整超参数和训练设置，以符合你的特定用例。以下是关键超参数：
- en: '1.'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like
    stochastic gradient descent (SGD). This technique estimates the error gradient
    for the model’s current state using samples from the training dataset and subsequently
    updates the model’s weights via the backpropagation of errors algorithm. The learning
    rate dictates the speed at which the model adapts to the problem. Smaller learning
    rates necessitate more training due to the minimal weight adjustments per update,
    while larger learning rates lead to quicker changes to weights [[48](#bib.bib48)].'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率：微调LLM涉及使用优化算法如随机梯度下降（SGD）。这种技术使用来自训练数据集的样本来估计模型当前状态的误差梯度，然后通过误差反向传播算法更新模型的权重。学习率决定了模型适应问题的速度。较小的学习率需要更多的训练，因为每次更新时权重的调整幅度较小，而较大的学习率会导致权重迅速变化[[48](#bib.bib48)]。
- en: '2.'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Batch Size: A batch refers to a subset of the training data used to update
    a model’s weights during the training process. Batch training involves dividing
    the entire training set into smaller groups, updating the model after processing
    each batch. The batch size is a hyperparameter that determines the number of samples
    processed before the model parameters are updated.'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：批量是指在训练过程中用于更新模型权重的训练数据的一个子集。批量训练涉及将整个训练集分成较小的组，在处理每个批量后更新模型。批量大小是一个超参数，决定在更新模型参数之前处理的样本数量。
- en: '3.'
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Epochs: Epoch refers to a full pass through the entire training dataset. This
    involves a complete forward and backward pass through the dataset. The dataset
    can be processed as a single batch or divided into multiple smaller batches. An
    epoch is considered complete once the model has processed all batches and updated
    its parameters based on the calculated loss.'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练周期：训练周期是指通过整个训练数据集的一次完整过程。这包括数据集的完整前向和反向传递。数据集可以作为一个批量处理，也可以分成多个较小的批量。一次训练周期被视为完成，当模型处理了所有批量并根据计算的损失更新了参数时。
- en: 5.3.1 Methods for Hyperparameter Tuning
  id: totrans-580
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 超参数调整的方法
- en: 'LLM hyperparameter tuning involves adjusting various hyperparameters during
    the training process to identify the optimal combination that yields the best
    output. This process often entails significant trial and error, meticulously tracking
    each hyperparameter adjustment, and recording the resulting performance. Conducting
    this manually can be highly time-consuming. To address this, automated hyperparameter
    tuning methods have been developed to streamline the process. The three most common
    methods of automated hyperparameter tuning are random search, grid search, and
    Bayesian optimisation:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: LLM超参数调整涉及在训练过程中调整各种超参数，以确定能产生最佳输出的最佳组合。这个过程通常需要大量的试验和错误，细致地跟踪每个超参数的调整，并记录结果性能。手动进行这一过程可能非常耗时。为了解决这个问题，已经开发了自动化超参数调整方法来简化这一过程。自动化超参数调整的三种最常见方法是随机搜索、网格搜索和贝叶斯优化：
- en: '1.'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Random Search: This method randomly selects and evaluates combinations of hyperparameters
    from a specified range. It is a straightforward and efficient approach capable
    of exploring a large parameter space. However, it may not always find the optimal
    combination of hyperparameters and can be computationally expensive [[49](#bib.bib49)].'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机搜索：这种方法随机选择和评估指定范围内的超参数组合。它是一种简单而高效的方法，能够探索较大的参数空间。然而，它可能不会总是找到最佳的超参数组合，并且计算成本较高[[49](#bib.bib49)]。
- en: '2.'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Grid Search: Unlike random search, grid search exhaustively evaluates every
    possible combination of hyperparameters from a given range. Although resource-intensive,
    this systematic approach ensures that the optimal set of hyperparameters is found
    [[50](#bib.bib50)].'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网格搜索：与随机搜索不同，网格搜索会穷举地评估给定范围内的每一种可能的超参数组合。尽管这种方法资源密集，但它通过系统化的方法确保找到最佳的超参数集[[50](#bib.bib50)]。
- en: '3.'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Bayesian Optimisation: This method uses a probabilistic model to predict the
    performance of different hyperparameters and selects the best ones accordingly.
    It is an efficient method that can handle large parameter spaces better and is
    less resource-intensive than grid search. However, it is more complex to set up
    and may be less reliable in identifying the optimal set of hyperparameters compared
    to grid search.'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯优化：该方法使用概率模型来预测不同超参数的性能，并据此选择最佳的超参数。它是一种高效的方法，能够更好地处理大参数空间，且比网格搜索资源消耗更少。然而，它在设置上更为复杂，可能在识别最佳超参数组合时不如网格搜索可靠。
- en: '4.'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Automated hyperparameter tuning: This facilitates the development of multiple
    language models, each with a unique combination of hyperparameters. By training
    these models on the same dataset, it becomes possible to compare their outputs
    and determine which configuration is best suited for the desired use case. Additionally,
    models tuned with different sets of hyperparameters can be tailored to various
    specific applications.'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动超参数调优：这有助于开发多个语言模型，每个模型都有独特的超参数组合。通过在相同数据集上训练这些模型，可以比较它们的输出，并确定哪种配置最适合所需的用例。此外，使用不同超参数集进行调优的模型可以针对各种特定应用进行调整。
- en: 5.4 Initialising Optimisers and Loss Functions
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 初始化优化器和损失函数
- en: 'Choosing the right optimiser and loss function is crucial for training and
    fine-tuning LLMs. Below are descriptions of some commonly used optimisation algorithms,
    their advantages, disadvantages, and appropriate use cases:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的优化器和损失函数对于训练和微调大型语言模型至关重要。以下是一些常用优化算法的描述，包括它们的优缺点和适用情况：
- en: 5.4.1 Gradient Descent
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 梯度下降
- en: Gradient Descent is a fundamental optimisation algorithm used to minimise cost
    functions in machine learning models. It aims to find the optimal parameters for
    a neural network.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种基本的优化算法，用于最小化机器学习模型中的成本函数。它旨在找到神经网络的最佳参数。
- en: 'How it Works: Gradient Descent iteratively updates model parameters in the
    direction of the negative gradient of the cost function. It calculates gradients
    for each parameter and applies updates across all data points until convergence.
    This method utilises the entire dataset to calculate gradients, often requiring
    a fixed learning rate and being sensitive to the scale of data and learning rate
    choice.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：梯度下降通过沿着成本函数的负梯度方向迭代更新模型参数。它计算每个参数的梯度，并在所有数据点上应用更新，直到收敛。此方法利用整个数据集计算梯度，通常需要固定的学习率，并对数据规模和学习率的选择非常敏感。
- en: 'Pros:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple and easy to implement.
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单易于实现。
- en: •
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Intuitive and easy to understand.
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直观且易于理解。
- en: •
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Converges to the global minimum for convex functions.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于凸函数，能够收敛到全局最小值。
- en: •
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Suitable for small-scale problems.
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适用于小规模问题。
- en: 'Cons:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Computationally expensive on large datasets.
  id: totrans-606
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大数据集上计算成本高。
- en: •
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: May get stuck in local minima.
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能会陷入局部最小值。
- en: •
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires a large number of iterations.
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要大量迭代。
- en: •
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensitive to the choice of learning rate.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对学习率的选择非常敏感。
- en: 'When to Use: Gradient Descent is best used for small datasets where gradient
    computation is cheap and simplicity and clarity are preferred.'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：梯度下降最适用于梯度计算便宜且更注重简洁性和清晰性的小数据集。
- en: 5.4.2 Stochastic Gradient Descent (SGD)
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 随机梯度下降（SGD）
- en: Stochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses
    on reducing computation per iteration.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）是梯度下降的一种变体，侧重于减少每次迭代的计算量。
- en: 'How it Works: SGD updates parameters using a single or few data points at each
    iteration, introducing randomness in updates. It reduces the computational burden
    per iteration and often converges faster than batch Gradient Descent. However,
    it requires a smaller learning rate due to higher variance and benefits from momentum
    to stabilise updates.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：SGD在每次迭代时使用单个或少量数据点更新参数，引入更新中的随机性。它减少了每次迭代的计算负担，并且通常比批量梯度下降收敛更快。然而，由于方差较高，它需要更小的学习率，并且通过动量来稳定更新。
- en: 'Pros:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast and handles large datasets well.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 速度快，能很好地处理大数据集。
- en: •
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Efficient memory usage.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效的内存使用。
- en: •
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple and easy to implement.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单易于实现。
- en: •
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can escape local minima due to noise.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于噪声，可能会逃离局部最小值。
- en: 'Cons:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: High variance in updates can lead to instability.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新中的高方差可能导致不稳定。
- en: •
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can overshoot the minimum.
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能会超过最小值。
- en: •
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensitive to the choice of learning rate.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对学习率的选择非常敏感。
- en: •
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can be slower to converge compared to batch methods.
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相较于批量方法，收敛速度可能较慢。
- en: 'When to Use: SGD is ideal for large datasets, incremental learning scenarios,
    and real-time learning environments where computational resources are limited.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：SGD非常适合大型数据集、增量学习场景以及计算资源有限的实时学习环境。
- en: 5.4.3 Mini-batch Gradient Descent
  id: totrans-636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 Mini-batch梯度下降
- en: Mini-batch Gradient Descent combines the efficiency of SGD and the stability
    of batch Gradient Descent, offering a compromise between batch and stochastic
    approaches.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: Mini-batch梯度下降结合了SGD的效率和批量梯度下降的稳定性，提供了批量和随机方法之间的折中。
- en: 'How it Works: It splits data into small batches and updates parameters using
    gradients averaged over each mini-batch. This reduces variance compared to SGD
    and is more efficient than batch Gradient Descent, helping in generalising the
    updates.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：它将数据分成小批次，并使用每个小批次上梯度的平均值来更新参数。这相比SGD减少了方差，并且比批量梯度下降更高效，有助于更新的泛化。
- en: 'Pros:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Balances between efficiency and stability.
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在效率和稳定性之间取得平衡。
- en: •
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More generalisable updates.
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更具泛化性的更新。
- en: •
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reduces the variance of parameter updates.
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少参数更新的方差。
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Provides a compromise between SGD and batch.
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了SGD和批量之间的折中。
- en: 'Cons:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires tuning of batch size.
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整批量大小。
- en: •
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can still be computationally expensive for very large datasets.
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非常大的数据集仍然可能计算开销大。
- en: •
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More complex implementation.
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现较为复杂。
- en: •
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can require more iterations than full-batch Gradient Descent.
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要比全批次梯度下降更多的迭代次数。
- en: 'When to Use: Mini-batch Gradient Descent is suitable for most deep learning
    tasks, especially when working with moderate to large datasets.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：Mini-batch梯度下降适用于大多数深度学习任务，特别是在处理中等到大型数据集时。
- en: 5.4.4 AdaGrad
  id: totrans-658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4 AdaGrad
- en: Adaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional
    models, adjusting learning rates to improve performance on sparse data.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应梯度算法（AdaGrad）旨在处理稀疏数据和高维模型，通过调整学习率来提高稀疏数据上的表现。
- en: 'How it Works: AdaGrad adapts the learning rate for each parameter based on
    historical gradient information, accumulating squared gradients. This approach
    prevents large updates for frequent parameters and helps in dealing with sparse
    features.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdaGrad根据历史梯度信息为每个参数调整学习率，积累平方梯度。这种方法防止了频繁参数的大幅更新，并有助于处理稀疏特征。
- en: 'Pros:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapts learning rate for each parameter.
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个参数调整学习率。
- en: •
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Good for sparse data.
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对稀疏数据有效。
- en: •
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No need to manually tune learning rates.
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无需手动调整学习率。
- en: •
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Works well with high-dimensional data.
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对高维数据表现良好。
- en: 'Cons:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning rate can diminish to zero, stopping learning.
  id: totrans-672
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率可能减少到零，从而停止学习。
- en: •
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: May require more tuning for convergence.
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要更多的调参以实现收敛。
- en: •
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Accumulation of squared gradients can lead to overly small learning rates.
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平方梯度的积累可能导致学习率过小。
- en: •
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can slow down significantly.
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能显著减慢速度。
- en: 'When to Use: AdaGrad is useful for sparse datasets like text and images where
    learning rates need to adapt to feature frequency.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdaGrad适用于需要根据特征频率调整学习率的稀疏数据集，如文本和图像。
- en: 5.4.5 RMSprop
  id: totrans-680
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.5 RMSprop
- en: Root Mean Square Propagation (RMSprop) is an adaptive learning rate method designed
    to perform better on non-stationary and online problems.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根传播（RMSprop）是一种自适应学习率方法，旨在在非平稳和在线问题上表现更好。
- en: 'How it Works: RMSprop modifies AdaGrad by using a moving average of squared
    gradients to adapt learning rates based on recent gradient magnitudes. It maintains
    a running average of squared gradients to help in maintaining steady learning
    rates.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：RMSprop通过使用平方梯度的移动平均来修改AdaGrad，根据近期梯度的幅度调整学习率。它保持平方梯度的运行平均，以帮助保持稳定的学习率。
- en: 'Pros:'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addresses the diminishing learning rate problem of AdaGrad.
  id: totrans-685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决了AdaGrad的学习率衰减问题。
- en: •
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapts learning rate based on recent gradients.
  id: totrans-687
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据近期梯度调整学习率。
- en: •
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Effective for recurrent neural networks.
  id: totrans-689
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对递归神经网络有效。
- en: •
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More robust against non-stationary targets.
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对非平稳目标更为稳健。
- en: 'Cons:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can still get stuck in local minima on non-convex problems.
  id: totrans-694
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在非凸问题上仍可能陷入局部最小值。
- en: •
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires hyperparameter tuning.
  id: totrans-696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整超参数。
- en: •
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires careful tuning of the decay rate.
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要仔细调整衰减率。
- en: •
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can be sensitive to the initial learning rate.
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对初始学习率可能较为敏感。
- en: 'When to Use: RMSprop is best for non-convex optimisation problems, training
    RNNs and LSTMs, and dealing with noisy or non-stationary objectives.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：RMSprop 最适用于非凸优化问题、训练 RNN 和 LSTM 以及处理噪声或非平稳目标。
- en: 5.4.6 AdaDelta
  id: totrans-702
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.6 AdaDelta
- en: Adaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive
    learning rates without diminishing too quickly.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应 Delta（AdaDelta）在 AdaGrad 和 RMSprop 的基础上进行改进，专注于自适应学习率而不会过快衰减。
- en: 'How it Works: AdaDelta eliminates the need for a default learning rate by using
    a moving window of gradient updates. It adapts learning rates based on recent
    gradient magnitudes to ensure consistent updates even with sparse gradients.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdaDelta 通过使用梯度更新的移动窗口来消除默认学习率的需求。它根据最近的梯度幅度调整学习率，以确保即使在梯度稀疏的情况下也能保持一致的更新。
- en: 'Pros:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Eliminates the need to set a default learning rate.
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消除了设置默认学习率的需要。
- en: •
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addresses the diminishing learning rate issue.
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决了学习率衰减问题。
- en: •
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does not require manual tuning of the learning rate.
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不需要手动调整学习率。
- en: •
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Handles gradient sparsity well.
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理梯度稀疏性较好。
- en: 'Cons:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More complex than RMSprop and AdaGrad.
  id: totrans-716
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比 RMSprop 和 AdaGrad 更复杂。
- en: •
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can have slower convergence initially.
  id: totrans-718
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初期可能会有较慢的收敛速度。
- en: •
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can require more iterations to converge.
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能需要更多迭代才能收敛。
- en: •
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementation can be more complex.
  id: totrans-722
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现可能更复杂。
- en: 'When to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred
    when avoiding manual learning rate setting.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdaDelta 适用于类似 RMSprop 的场景，但在避免手动设置学习率时更为合适。
- en: 5.4.7 Adam
  id: totrans-724
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.7 Adam
- en: Adaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop,
    making it suitable for problems with large datasets and high-dimensional spaces.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应矩估计（Adam）结合了 AdaGrad 和 RMSprop 的优点，适用于大数据集和高维空间的问题。
- en: 'How it Works: Adam uses running averages of both gradients and their squared
    values to compute adaptive learning rates for each parameter. It includes bias
    correction and often achieves faster convergence than other methods.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：Adam 使用梯度及其平方值的运行平均值来计算每个参数的自适应学习率。它包括偏差校正，通常比其他方法实现更快的收敛。
- en: 'Pros:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combines advantages of AdaGrad and RMSprop.
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结合了 AdaGrad 和 RMSprop 的优点。
- en: •
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adaptive learning rates.
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应学习率。
- en: •
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Includes bias correction.
  id: totrans-733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括偏差校正。
- en: •
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast convergence.
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 快速收敛。
- en: •
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Works well with large datasets and high-dimensional spaces.
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大数据集和高维空间中表现良好。
- en: 'Cons:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires tuning of hyperparameters (though it often works well with defaults).
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要调整超参数（尽管默认设置通常效果良好）。
- en: •
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Computationally intensive.
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算密集型。
- en: •
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can lead to overfitting if not regularised properly.
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有正确的正则化，可能导致过拟合。
- en: •
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires more memory.
  id: totrans-746
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要更多内存。
- en: 'When to Use: Adam is widely used in most deep learning applications due to
    its efficiency and effectiveness, particularly in complex neural network architectures.'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：由于其效率和效果，Adam 广泛用于大多数深度学习应用，特别是在复杂的神经网络架构中。
- en: 5.4.8 AdamW
  id: totrans-748
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.8 AdamW
- en: AdamW is an extension of Adam that includes weight decay regularisation to address
    overfitting issues present in Adam.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: AdamW 是 Adam 的扩展，包含权重衰减正则化，以解决 Adam 中存在的过拟合问题。
- en: 'How it Works: AdamW integrates L2 regularisation directly into the parameter
    updates, decoupling weight decay from the learning rate. This improves generalisation
    and is suitable for fine-tuning large models.'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 工作原理：AdamW 将 L2 正则化直接集成到参数更新中，将权重衰减与学习率解耦。这提高了模型的泛化能力，并适用于大型模型的微调。
- en: 'Pros:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: •
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Includes weight decay for better regularisation.
  id: totrans-753
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括权重衰减以实现更好的正则化。
- en: •
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Combines Adam’s adaptive learning rate with L2 regularisation.
  id: totrans-755
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 Adam 的自适应学习率与 L2 正则化结合。
- en: •
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Improves generalisation.
  id: totrans-757
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改善了泛化能力。
- en: •
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reduces overfitting compared to Adam.
  id: totrans-759
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 Adam 相比，减少了过拟合。
- en: 'Cons:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: •
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Slightly more complex than Adam.
  id: totrans-762
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比 Adam 稍微复杂。
- en: •
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires careful tuning of the weight decay parameter.
  id: totrans-764
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要仔细调整权重衰减参数。
- en: •
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Slightly slower than Adam due to additional computations.
  id: totrans-766
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于额外的计算，比 Adam 略慢。
- en: •
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requires more memory.
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要更多内存。
- en: 'When to Use: AdamW is ideal for scenarios where regularisation is needed, such
    as preventing overfitting in large models and fine-tuning pre-trained models.'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 使用时机：AdamW 适用于需要正则化的场景，如防止大型模型的过拟合和微调预训练模型。
- en: A comprehensive collection of optimisation algorithms implemented within the
    PyTorch library can be found in [here](https://pytorch.org/docs/stable/optim.html).
    The Hugging Face Transformers package also offers a variety of optimisers for
    initialising and fine-tuning language models, available [here](https://huggingface.co/docs/transformers/en/main_classes/optimiser_schedules).
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 在[这里](https://pytorch.org/docs/stable/optim.html)可以找到PyTorch库中实现的优化算法的全面集合。Hugging
    Face Transformers包也提供了多种用于初始化和微调语言模型的优化器，详见[这里](https://huggingface.co/docs/transformers/en/main_classes/optimiser_schedules)。
- en: 5.5 Challenges in Training Setup
  id: totrans-771
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 训练设置中的挑战
- en: '1.'
  id: totrans-772
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Ensuring compatibility and proper configuration of high-performance hardware
    like GPUs or TPUs can be complex and time-consuming.
  id: totrans-773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保高性能硬件（如GPU或TPU）的兼容性和正确配置可能复杂且耗时。
- en: '2.'
  id: totrans-774
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Managing dependencies and versions of deep learning frameworks and libraries
    to avoid conflicts and leverage the latest features.
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管理深度学习框架和库的依赖项和版本，以避免冲突并利用最新功能。
- en: '3.'
  id: totrans-776
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Selecting an appropriate learning rate is critical, as too high a rate can cause
    suboptimal convergence, while too low a rate can make the training process excessively
    slow.
  id: totrans-777
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的学习率至关重要，因为过高的学习率可能导致次优的收敛，而过低的学习率则可能使训练过程过于缓慢。
- en: '4.'
  id: totrans-778
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Determining the optimal batch size that balances memory constraints and training
    efficiency, especially given the large memory requirements of LLMs.
  id: totrans-779
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定平衡内存限制和训练效率的最佳批量大小，特别是在大型语言模型需要大量内存的情况下。
- en: '5.'
  id: totrans-780
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Choosing the right number of epochs to avoid underfitting or overfitting the
    model, requiring careful monitoring and validation.
  id: totrans-781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的训练周期数以避免模型的欠拟合或过拟合，这需要仔细的监控和验证。
- en: '6.'
  id: totrans-782
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Selecting the most suitable optimiser for the specific training task to efficiently
    update the model’s weights.
  id: totrans-783
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择最适合特定训练任务的优化器，以高效地更新模型的权重。
- en: '7.'
  id: totrans-784
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Choosing the correct loss function to accurately measure model performance and
    guide the optimisation process.
  id: totrans-785
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择正确的损失函数，以准确衡量模型性能并指导优化过程。
- en: 5.6 Best Practices
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 最佳实践
- en: •
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to
    2e-4, to ensure stable convergence. A learning rate schedule, such as learning
    rate warm-up followed by a linear decay, can also be beneficial. This helps in
    initially stabilising the training and then allowing the model to converge more
    accurately.'
  id: totrans-788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳学习率：使用较低的学习率，通常在1e-4到2e-4之间，以确保稳定的收敛。学习率调度，例如学习率预热后跟线性衰减，也可能会有所帮助。这有助于最初稳定训练，然后允许模型更准确地收敛。
- en: •
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch Size Considerations: Opt for a batch size that balances memory constraints
    and training efficiency. Smaller batch sizes can help in achieving faster convergence
    but may require more frequent updates. Conversely, larger batch sizes can be more
    memory-intensive but may lead to more stable updates. Experiment with different
    batch sizes to find the optimal balance for your specific use case.'
  id: totrans-790
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小考虑：选择一个平衡内存限制和训练效率的批量大小。较小的批量大小可以帮助实现更快的收敛，但可能需要更频繁的更新。相反，较大的批量大小可能更消耗内存，但可能导致更稳定的更新。尝试不同的批量大小以找到适合你特定用例的最佳平衡。
- en: •
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Save Checkpoints Regularly: Regularly save model weights at various intervals
    across 5-8 epochs to capture optimal performance without overfitting. Implement
    early stopping mechanisms to halt training once the model performance starts to
    degrade on the validation set, thereby preventing overfitting [[51](#bib.bib51)].'
  id: totrans-792
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期保存检查点：在5-8个周期内定期保存模型权重，以捕捉最佳性能而不至于过拟合。实施早停机制，一旦模型在验证集上的性能开始下降，便停止训练，从而防止过拟合[[51](#bib.bib51)]。
- en: •
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search,
    random search, and Bayesian optimisation to find the optimal set of hyperparameters.
    Tools such as Optuna, Hyperopt, and Ray Tune can automate this process and help
    in efficiently exploring the hyperparameter space [[49](#bib.bib49)].'
  id: totrans-794
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数调优：利用网格搜索、随机搜索和贝叶斯优化等超参数调优方法来寻找最佳的超参数组合。工具如Optuna、Hyperopt和Ray Tune可以自动化这一过程，并帮助高效地探索超参数空间[[49](#bib.bib49)]。
- en: •
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Parallelism and Model Parallelism: For large-scale training, consider
    using data parallelism or model parallelism techniques to distribute the training
    workload across multiple GPUs or TPUs. Libraries like Horovod and DeepSpeed can
    facilitate efficient distributed training, helping to reduce training time and
    manage memory usage effectively [[52](#bib.bib52), [53](#bib.bib53)].'
  id: totrans-796
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据并行和模型并行：对于大规模训练，考虑使用数据并行或模型并行技术，将训练负载分配到多个GPU或TPU上。像Horovod和DeepSpeed这样的库可以促进高效的分布式训练，帮助减少训练时间并有效管理内存使用[[52](#bib.bib52),
    [53](#bib.bib53)]。
- en: •
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Regular Monitoring and Logging: Implement robust monitoring and logging to
    track training metrics, resource usage, and potential bottlenecks. Tools like
    TensorBoard, Weights & Biases, and MLflow can provide real-time insights into
    the training process, allowing for timely interventions and adjustments.'
  id: totrans-798
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定期监控和日志记录：实施健全的监控和日志记录，以跟踪训练指标、资源使用情况和潜在瓶颈。像TensorBoard、Weights & Biases和MLflow这样的工具可以提供实时的训练过程洞察，允许及时干预和调整。
- en: •
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Handling Overfitting and Underfitting: Ensure that your model generalises well
    by implementing techniques to handle overfitting and underfitting. regularisation
    techniques such as L2 regularisation, dropout, and data augmentation can help
    prevent overfitting. Conversely, if your model is underfitting, consider increasing
    the model complexity or training for more epochs.'
  id: totrans-800
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理过拟合和欠拟合：通过实施处理过拟合和欠拟合的技术，确保模型能够很好地泛化。正则化技术如L2正则化、丢弃法和数据增强可以帮助防止过拟合。相反，如果模型欠拟合，可以考虑增加模型复杂度或训练更多轮次。
- en: •
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Use Mixed Precision Training: Mixed precision training involves using both
    16-bit and 32-bit floating-point types to reduce memory usage and increase computational
    efficiency. This technique can significantly speed up training and reduce the
    required memory footprint, especially when using large models. NVIDIA’s Apex and
    TensorFlow’s mixed precision API provide support for implementing mixed precision
    training [[54](#bib.bib54)].'
  id: totrans-802
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用混合精度训练：混合精度训练涉及同时使用16位和32位浮点类型，以减少内存使用并提高计算效率。这种技术可以显著加快训练速度并减少所需的内存占用，特别是在使用大型模型时。NVIDIA的Apex和TensorFlow的混合精度API提供了实现混合精度训练的支持[[54](#bib.bib54)]。
- en: •
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluate and Iterate: Continuously evaluate the model performance using a separate
    validation set and iterate on the training process based on the results. Regularly
    update your training data and retrain the model to keep it current with new data
    trends and patterns.'
  id: totrans-804
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和迭代：使用独立的验证集持续评估模型性能，并根据结果对训练过程进行迭代。定期更新训练数据并重新训练模型，以保持其与新数据趋势和模式的同步。
- en: •
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Documentation and Reproducibility: Maintain thorough documentation of your
    training setup, including the hardware configuration, software environment, and
    hyperparameters used. Ensure reproducibility by setting random seeds and providing
    detailed records of the training process. This practice not only aids in debugging
    and further development but also facilitates collaboration and sharing of results
    with the broader research community.'
  id: totrans-806
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文档记录和可重复性：保持对训练设置的详细文档，包括硬件配置、软件环境和使用的超参数。通过设置随机种子和提供训练过程的详细记录来确保可重复性。这种做法不仅有助于调试和进一步开发，还促进了与更广泛研究社区的合作和结果共享。
- en: 'Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations'
  id: totrans-807
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章 第4阶段：微调技术和适当模型配置的选择
- en: This chapter focuses on selecting appropriate fine-tuning techniques and model
    configurations that suit the specific requirements of various tasks. Fine-tuning
    is a crucial stage where pre-trained models are adapted to specific tasks or domains.
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍选择适当的微调技术和模型配置，以满足各种任务的具体需求。微调是一个关键阶段，在此阶段，预训练模型会根据特定任务或领域进行调整。
- en: 6.1 Steps Involved in Fine-Tuning
  id: totrans-809
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 微调涉及的步骤
- en: The following steps outline the fine-tuning process, integrating advanced techniques
    and best practices.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了微调过程，整合了先进的技术和最佳实践。
- en: '1.'
  id: totrans-811
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained
    tokenizer and model. The tokenizer ensures that the input text is converted into
    a format the model can process, while the pre-trained model serves as the foundation
    for further adaptation. Depending on the task, select a model that has been pre-trained
    on relevant data to provide a strong starting point.'
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化预训练的分词器和模型：首先加载预训练的分词器和模型。分词器确保输入文本被转换成模型可以处理的格式，而预训练模型则作为进一步适配的基础。根据任务选择一个在相关数据上进行过预训练的模型，以提供一个良好的起点。
- en: '2.'
  id: totrans-813
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Modify the Model’s Output Layer: Adjust the model’s output layer to align with
    the specific requirements of the target task. This may involve modifying existing
    layers or adding new layers. For instance, tasks like classification may require
    a softmax layer with the appropriate number of classes, while text generation
    tasks might involve changes in the decoding mechanism.'
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改模型的输出层：调整模型的输出层以符合目标任务的具体要求。这可能涉及修改现有层或添加新层。例如，分类任务可能需要一个具有适当类别数的softmax层，而文本生成任务可能需要改变解码机制。
- en: '3.'
  id: totrans-815
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy
    that best fits the task and the model architecture. Some Options include:'
  id: totrans-816
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择合适的微调策略：选择最适合任务和模型架构的微调策略。一些选项包括：
- en: •
  id: totrans-817
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation,
    classification, and question answering, adapt the model using relevant datasets.'
  id: totrans-818
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定微调：对于文本摘要、代码生成、分类和问答等任务，使用相关数据集对模型进行调整。
- en: •
  id: totrans-819
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text
    relevant to specific domains, such as medical, financial, or legal fields.'
  id: totrans-820
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域特定微调：将模型调整为理解和生成与特定领域（如医学、金融或法律领域）相关的文本。
- en: •
  id: totrans-821
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters
    allow for fine-tuning with reduced computational costs by updating a small subset
    of model parameters.'
  id: totrans-822
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）：像LoRA、QLoRA和适配器等技术允许通过更新模型参数的一个小子集来进行低计算成本的微调。
- en: •
  id: totrans-823
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and
    learning new tasks by updating only half of the model’s parameters during each
    fine-tuning round.'
  id: totrans-824
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半微调（HFT）：通过在每轮微调中仅更新一半模型参数，在保留预训练知识和学习新任务之间取得平衡。
- en: '4.'
  id: totrans-825
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Set Up the Training Loop: Establish the training loop, incorporating the selected
    fine-tuning strategy. The loop should include data loading, loss computation,
    backpropagation, and parameter updates. When using PEFT methods, ensure that only
    the relevant parameters are updated to maximise efficiency. Implement techniques
    like dynamic learning rates and early stopping to enhance the training process.'
  id: totrans-826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置训练循环：建立训练循环，融入所选的微调策略。循环应包括数据加载、损失计算、反向传播和参数更新。使用PEFT方法时，确保仅更新相关参数以最大化效率。实施动态学习率和提前停止等技术以提高训练过程。
- en: '5.'
  id: totrans-827
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple
    tasks, consider strategies like fine-tuning with multiple adapters or leveraging
    Mixture of Experts (MoE) architectures. These methods allow a single model to
    handle various tasks by utilising specialised sub-networks or adapters for each
    task.'
  id: totrans-828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 融合多任务处理技术：如果要进行多任务微调，可以考虑使用多适配器微调或利用专家混合（MoE）架构等策略。这些方法允许一个模型通过利用每个任务的专用子网络或适配器来处理各种任务。
- en: '6.'
  id: totrans-829
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Monitor Performance on a Validation Set: Regularly evaluate the model’s performance
    on a validation set to ensure it generalises well to unseen data. Adjust hyperparameters
    such as learning rate, batch size, and dropout rates based on the validation performance.
    Utilise advanced monitoring tools to track metrics like accuracy, loss, and overfitting.'
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在验证集上监控性能：定期评估模型在验证集上的表现，以确保其对未见数据的泛化能力。根据验证性能调整超参数，如学习率、批量大小和dropout率。利用先进的监控工具来跟踪准确率、损失和过拟合等指标。
- en: '7.'
  id: totrans-831
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Optimise Model Using Advanced Techniques: Employ techniques such as Proximal
    Policy Optimisation (PPO) for reinforcement learning scenarios, or Direct Preference
    Optimisation (DPO) for aligning model outputs with human preferences. These techniques
    are particularly useful in fine-tuning models for tasks requiring nuanced decision-making
    or human-like responses.'
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用先进技术优化模型：采用如近端策略优化（PPO）用于强化学习场景，或直接偏好优化（DPO）以使模型输出与人类偏好对齐。这些技术在微调需要细致决策或类似人类反应的任务时特别有用。
- en: '8.'
  id: totrans-833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Prune and optimise the Model (if necessary): To deploy the model in resource-constrained
    environments, consider pruning techniques to reduce its size and complexity. This
    involves removing unnecessary parameters or components without significantly affecting
    performance. Utilise dynamic pruning methods during inference to optimise the
    model on-the-fly for different scenarios.'
  id: totrans-834
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝和优化模型（如有必要）：为在资源受限环境中部署模型，考虑剪枝技术以减少其大小和复杂性。这涉及去除不必要的参数或组件，而不显著影响性能。利用动态剪枝方法在推理过程中实时优化模型以适应不同场景。
- en: '9.'
  id: totrans-835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Continuous Evaluation and Iteration: Continuously evaluate the model’s performance
    across various tasks using appropriate benchmarks. Iterate on the fine-tuning
    process, making adjustments based on performance metrics and real-world testing.
    This iterative approach helps in refining the model to meet specific performance
    criteria.'
  id: totrans-836
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续评估和迭代：通过适当的基准持续评估模型在各种任务中的表现。对微调过程进行迭代，根据性能指标和实际测试进行调整。这种迭代方法有助于将模型精炼以满足特定性能标准。
- en: 6.2 Fine-Tuning Strategies for LLMs
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 LLM的微调策略
- en: 6.2.1 Task-Specific Fine-Tuning
  id: totrans-838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 任务特定微调
- en: Task-specific fine-tuning adapts large language models (LLMs) for particular
    downstream tasks using appropriately formatted and cleaned data. Below is a summary
    of key tasks suitable for fine-tuning LLMs, including examples of LLMs tailored
    to these tasks.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 任务特定的微调将大型语言模型（LLMs）调整为特定的下游任务，使用适当格式化和清洗的数据。以下是适合微调LLMs的关键任务摘要，包括针对这些任务调整的LLM示例。
- en: '| Task | Description | Key Models |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | 关键模型 |'
- en: '| --- | --- | --- |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Text Summarisation | Condensing long texts into coherent summaries while
    retaining key information. Approaches include Extractive (selecting key sentences)
    and Abstractive summarisation (generating new sentences). | BERTSUM, GPT-3, T5
    |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
  zh: '| 文本总结 | 将长文本浓缩为连贯的摘要，同时保留关键信息。方法包括提取式（选择关键句子）和抽象式总结（生成新句子）。 | BERTSUM, GPT-3,
    T5 |'
- en: '| Code Generation | Automatically generating programming code based on natural
    language descriptions, partial code snippets, or structured data inputs. | Codex,
    GPT-3, CodeBERT |'
  id: totrans-843
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | 基于自然语言描述、部分代码片段或结构化数据输入自动生成编程代码。 | Codex, GPT-3, CodeBERT |'
- en: '| Classification | Categorising text into predefined labels such as Sentiment
    Analysis, Topic Classification, and Entity Classification. | BERT, RoBERTa, GPT-4
    |'
  id: totrans-844
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 将文本分类到预定义标签中，如情感分析、主题分类和实体分类。 | BERT, RoBERTa, GPT-4 |'
- en: '| Q&A | Understanding and generating accurate, contextually relevant answers
    to natural language questions. | BERT, GPT-3, T5 |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | 理解和生成准确的、上下文相关的自然语言问题答案。 | BERT, GPT-3, T5 |'
- en: 'Table 6.1: Overview of tasks such as text summarisation, code generation, classification,
    and Q&A, along with their key LLMs and descriptions.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：任务概述，如文本总结、代码生成、分类和问答，以及它们的关键LLM和描述。
- en: 6.2.2 Domain-Specific Fine-Tuning
  id: totrans-847
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 领域特定微调
- en: Domain-specific fine-tuning focuses on tailoring the model to comprehend and
    produce text relevant to a specific domain or industry. By fine-tuning the model
    on a dataset derived from the target domain, it enhances the model’s contextual
    understanding and expertise in domain-specific tasks. Below are examples of domain-specific
    LLMs.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定的微调专注于调整模型，以理解和生成与特定领域或行业相关的文本。通过在来自目标领域的数据集上微调模型，可以提升模型在领域特定任务中的上下文理解和专业知识。以下是领域特定LLM的示例。
- en: Medical Domain
  id: totrans-849
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医疗领域
- en: 'Model Description: Med-PaLM 2 is trained on meticulously curated medical datasets
    and is capable of accurately answering medical questions, achieving performance
    comparable to that of medical professionals [[55](#bib.bib55)].'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：Med-PaLM 2 在精心策划的医疗数据集上进行训练，能够准确回答医疗问题，性能与医疗专业人士相当 [[55](#bib.bib55)]。
- en: 'Base Model: PaLM 2'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：PaLM 2
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: Instruction fine-tuning'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：指令微调
- en: 'Datasets Used:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedQA
  id: totrans-856
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedQA
- en: •
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedMCQA
  id: totrans-858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedMCQA
- en: •
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LiveQA
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LiveQA
- en: •
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MedicationQA
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MedicationQA
- en: •
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HealthSearchQA
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HealthSearchQA
- en: 'Results: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating
    superior performance in handling complex medical knowledge and reasoning tasks.'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：Med-PaLM 2 在多个关键医学基准测试中优于 GPT-4，展示了在处理复杂医学知识和推理任务方面的卓越性能。
- en: Finance Domain
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金融领域
- en: 'Model Description: FinGPT, an open-source LLM tailored for the financial sector,
    enhances financial research and cooperation by promoting data accessibility and
    handling finance-specific issues like data acquisition and quality [[56](#bib.bib56)].'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：FinGPT，是一款针对金融领域的开源 LLM，通过提升数据可获取性和处理金融特有问题（如数据获取和质量），增强金融研究和合作 [[56](#bib.bib56)]。
- en: 'Base Model: LlaMA, ChatGLM, and other Transformer Models'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA、ChatGLM 和其他 Transformer 模型
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：LoRA，股票价格的强化学习（RLSP）
- en: 'Datasets Used:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Financial News (Reuters, CNBC, Yahoo Finance)
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 财经新闻（路透社、CNBC、雅虎财经）
- en: •
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Social Media (Twitter, Facebook, Reddit, Weibo)
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 社交媒体（Twitter、Facebook、Reddit、微博）
- en: •
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regulatory Filings (e.g., SEC filings)
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 监管文件（例如，SEC 文件）
- en: •
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Trends (Seeking Alpha, Google Trends)
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 趋势（Seeking Alpha、Google Trends）
- en: •
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Academic Datasets
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学术数据集
- en: 'Results: Not Applicable'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：不适用
- en: Legal Domain
  id: totrans-883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 法律领域
- en: 'Model Description: LAWGPT, the first open-source model specifically designed
    for Chinese legal applications, demonstrates superior capability in handling Chinese
    legal tasks [[57](#bib.bib57)].'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：LAWGPT，是首个专门为中国法律应用设计的开源模型，在处理中国法律任务方面表现出色 [[57](#bib.bib57)]。
- en: 'Base Model: Chinese Alpaca Plus 7B base model'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：Chinese Alpaca Plus 7B 基础模型
- en: 'Fine-tuned Model Parameters: Not Known'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：未知
- en: 'Fine-Tuning Techniques Used: LoRA with Alpaca template'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：使用 Alpaca 模板的 LoRA
- en: 'Datasets Used:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Open-source dataset: 200,000 examples containing crime type prediction and
    crime consultation tasks.'
  id: totrans-890
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开源数据集：包含犯罪类型预测和犯罪咨询任务的 200,000 个样本。
- en: •
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'JEC-QA dataset: 20,000 examples containing legal question answering tasks.'
  id: totrans-892
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JEC-QA 数据集：包含 20,000 个法律问答任务的样本。
- en: •
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA
    datasets using ChatGPT.'
  id: totrans-894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建的法律数据集：80,000 个样本，通过使用 ChatGPT 从开源和 JEC-QA 数据集中提炼而来。
- en: 'Results: LAWGPT demonstrates notable performance improvements over the LLaMA
    7B model in various legal tasks, but still trails behind proprietary models like
    GPT-3.5 Turbo and GPT-4.'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：LAWGPT 在各种法律任务中表现出显著的性能提升，优于 LLaMA 7B 模型，但仍落后于 GPT-3.5 Turbo 和 GPT-4 等专有模型。
- en: Pharmaceutical Domain
  id: totrans-896
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 制药领域
- en: 'Model Description: PharmaGPT, a suite of domain-specific large language models
    tailored to the biopharmaceutical and chemical industries, sets a new benchmark
    for precision in these fields [[58](#bib.bib58)].'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：PharmaGPT，一系列针对生物制药和化学工业的特定领域大语言模型，为这些领域的精确度设立了新标准 [[58](#bib.bib58)]。
- en: 'Base Model: LlaMA series'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA 系列
- en: 'Fine-tuned Model Parameters: 13B and 70B'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：13B 和 70B
- en: 'Fine-Tuning Techniques Used: Instruction fine-tuning and RLHF'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：指令微调和 RLHF
- en: 'Datasets Used:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：
- en: •
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Specific-domain data from academic papers and clinical reports
  id: totrans-903
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自学术论文和临床报告的特定领域数据
- en: •
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Text data from NLP dataset formats (e.g., question answering, summarisation,
    dialogue)
  id: totrans-905
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自 NLP 数据集格式的文本数据（例如，问答、总结、对话）
- en: •
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Instruction fine-tuning dataset for multitask learning
  id: totrans-907
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多任务学习的指令微调数据集
- en: •
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RLHF dataset with human preference expert-annotated instructions
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RLHF 数据集，含人工偏好专家注释的指令
- en: 'Results: PharmaGPT models demonstrated impressive performance on various pharmaceutical
    benchmarks, consistently outperforming GPT-3.5 Turbo.'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：PharmaGPT 模型在各种制药基准测试中表现出色，一直优于 GPT-3.5 Turbo。
- en: Finance Domain
  id: totrans-911
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 金融领域
- en: 'Model Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large
    language model specifically designed for the financial sector. [[59](#bib.bib59)]'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 模型描述：Palmyra-Fin-70B-32K，由 Writer 开发，是一款专门为金融行业设计的领先大语言模型。[[59](#bib.bib59)]
- en: 'Base Model: LlaMA'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型：LlaMA
- en: 'Fine-tuned Model Parameters: 70B'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型参数：70B
- en: 'Fine-Tuning Techniques Used: Not Known'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 微调技术：未知
- en: 'Datasets Used: Not Known'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集：未知
- en: 'Results: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving
    leading results across various financial datasets and excelling in financial document
    analysis, market trend prediction, and risk assessment.'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：Palmyra-Fin-70B-32K 展现了最先进的性能，在各种金融数据集中取得了领先结果，并在金融文档分析、市场趋势预测和风险评估方面表现出色。
- en: 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
  id: totrans-918
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 参数高效微调（PEFT）技术
- en: 'Parameter Efficient Fine Tuning [(PEFT)](https://github.com/huggingface/peft)
    is an impactful NLP technique that adeptly adapts pre-trained language models
    to various applications with remarkable efficiency. PEFT methods fine-tune only
    a small subset of (additional) model parameters while keeping most of the pre-trained
    LLM parameters frozen, thereby significantly reducing computational and storage
    costs. This approach mitigates the issue of catastrophic forgetting, a phenomenon
    where neural networks lose previously acquired knowledge and experience a significant
    performance decline on previously learned tasks when trained on new datasets.
    PEFT methods have demonstrated superior performance compared to full fine-tuning,
    particularly in low-data scenarios, and exhibit better generalisation to out-of-domain
    contexts. This technique is applicable to various modalities, such as financial
    sentiment classification and machine translation of medical terminologies. A taxonomy
    of PEFT-based fine-tuning approaches is provided in Figure[6.1](#Ch6.F1 "Figure
    6.1 ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage
    4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations ‣
    The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)"). We will further discuss a few key PEFT-based
    approaches in the following sections.'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调 [(PEFT)](https://github.com/huggingface/peft) 是一种有效的NLP技术，能够以显著的效率将预训练语言模型适配到各种应用中。PEFT方法仅微调模型参数的一个小子集，同时保持大多数预训练LLM参数不变，从而显著降低计算和存储成本。这种方法缓解了灾难性遗忘的问题，即神经网络在训练新数据集时丧失之前获得的知识，并在之前学过的任务上表现出显著的性能下降。与完全微调相比，PEFT方法在低数据场景下表现出更优越的性能，并在跨领域背景下展示了更好的泛化能力。这一技术适用于各种模态，如金融情感分类和医学术语的机器翻译。图[6.1](#Ch6.F1
    "图6.1 ‣ 6.3 参数高效微调（PEFT）技术 ‣ 第6章 第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾（第1.0版）")
    提供了PEFT基础微调方法的分类。我们将在以下章节中进一步讨论一些关键的基于PEFT的方法。
- en: '![Refer to caption](img/e46d6577e1e8d21bc229e596602f5f83.png)'
  id: totrans-920
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e46d6577e1e8d21bc229e596602f5f83.png)'
- en: 'Figure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT)
    Methods for Large Language Models (LLMs). This figure categorises various PEFT
    techniques, highlighting their distinct approaches, from additive and selective
    fine-tuning to reparameterised and hybrid methods. It details specific strategies
    within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based Fine-Tuning,
    and their respective sub-techniques like LoRA and its derivatives, showcasing
    the diverse and evolving landscape of LLM fine-tuning. (adapted from [[60](#bib.bib60)])'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：大型语言模型（LLMs）参数高效微调（PEFT）方法的综合分类。这张图展示了各种PEFT技术的分类，突出了它们不同的方法，包括附加微调、选择性微调、重新参数化和混合方法。它详细说明了每个类别中的具体策略，如基于适配器的微调、基于软提示的微调，以及其各自的子技术，如LoRA及其衍生品，展示了LLM微调的多样化和不断发展的格局。（改编自[[60](#bib.bib60)]）
- en: 6.3.1 Adapters
  id: totrans-922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 适配器
- en: Adapter-based methods introduce additional trainable parameters after the attention
    and fully connected layers of a frozen pre-trained model, aiming to reduce memory
    usage and accelerate training. The specific approach varies depending on the adapter;
    it might involve adding an extra layer or representing the weight updates delta
    (W) as a low-rank decomposition of the weight matrix. Regardless of the method,
    adapters are generally small yet achieve performance comparable to fully fine-tuned
    models, allowing for the training of larger models with fewer resources.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 基于适配器的方法在冻结的预训练模型的注意力层和全连接层之后引入额外的可训练参数，旨在减少内存使用并加快训练。具体方法根据适配器的不同而有所不同；它可能涉及添加额外的层，或将权重更新增量（W）表示为权重矩阵的低秩分解。无论采用哪种方法，适配器通常较小，但能够实现与完全微调模型相媲美的性能，从而允许在资源较少的情况下训练更大的模型。
- en: '![Refer to caption](img/9d6c08f62e7040457a4281198f1fc5b3.png)'
  id: totrans-924
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9d6c08f62e7040457a4281198f1fc5b3.png)'
- en: 'Figure 6.2: Schematic representation of the Adapter Architecture used in LLMs.
    The diagram showcases the integration of adapters within the Transformer architecture,
    including the feed-forward up and down layers and their role in enabling efficient
    model adaptation by inserting additional parameters while maintaining the model’s
    core structure (adapted from [[61](#bib.bib61)])'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：用于 LLM 的适配器架构的示意图。图示展示了在 Transformer 架构中集成适配器的过程，包括前馈上层和下层及其在插入额外参数的同时保持模型核心结构的作用（改编自
    [[61](#bib.bib61)]）
- en: HuggingFace supports adapter configurations through the PEFT library. During
    fine-tuning, new adapters are integrated into the model using LoraConfig ¹¹1[https://huggingface.co/docs/peft/en/package_reference/lora](https://huggingface.co/docs/peft/en/package_reference/lora).
    HuggingFace uses PeftConfig to load existing pre-trained models and apply PEFT
    techniques. Additionally, HuggingFace provides built-in support to run the fine-tuning
    process across any distributed configuration using Accelerate²²2[https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index),
    making large-scale training and inference simple, efficient, and adaptable.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace 通过 PEFT 库支持适配器配置。在微调过程中，通过使用 LoraConfig ¹¹1[https://huggingface.co/docs/peft/en/package_reference/lora](https://huggingface.co/docs/peft/en/package_reference/lora)
    将新的适配器集成到模型中。HuggingFace 使用 PeftConfig 加载现有的预训练模型并应用 PEFT 技术。此外，HuggingFace 提供了内置支持，通过
    Accelerate²²2[https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index)
    在任何分布式配置中运行微调过程，使大规模训练和推理变得简单、高效且具有适应性。
- en: 6.3.2 Low-Rank Adaptation (LoRA)
  id: totrans-927
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 低秩适应（LoRA）
- en: Low-Rank Adaptation (LoRA)[[62](#bib.bib62)] is a technique designed for fine-tuning
    large language models, which modifies the fine-tuning process by freezing the
    original model weights and applying changes to a separate set of weights, added
    to the original parameters. LoRA transforms the model parameters into a lower-rank
    dimension, reducing the number of trainable parameters, speeding up the process,
    and lowering costs. This method is particularly useful in scenarios where multiple
    clients require fine-tuned models for different applications, allowing for the
    creation of specific weights for each use case without the need for separate models.
    By employing low-rank approximation methods, LoRA effectively reduces computational
    and resource requirements while preserving the pre-trained model’s adaptability
    to specific tasks or domains.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）[[62](#bib.bib62)] 是一种针对大规模语言模型进行微调的技术，该技术通过冻结原始模型权重并对一组单独的权重进行修改，从而改变微调过程。LoRA
    将模型参数转换为低秩维度，减少了可训练参数的数量，加快了过程并降低了成本。这种方法在需要多个客户端为不同应用程序进行微调模型的场景中特别有用，允许为每个用例创建特定的权重，而无需单独的模型。通过采用低秩近似方法，LoRA
    有效地减少了计算和资源需求，同时保留了预训练模型对特定任务或领域的适应性。
- en: '![Refer to caption](img/4bd5e9445d38406defc47d824480fb2c.png)'
  id: totrans-929
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4bd5e9445d38406defc47d824480fb2c.png)'
- en: 'Figure 6.3: A comparison between weight updates in regular fine-tuning and
    LoRA fine-tuning. In regular fine-tuning, the entire weight update matrix ($\Delta
    W$), significantly reducing the number of trainable parameters by leveraging the
    inner dimension (r), which is a hyperparameter. This method is more efficient
    in terms of memory and computation, making it ideal for fine-tuning large models.
    (adapted from [[63](#bib.bib63)])'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：常规微调和 LoRA 微调中的权重更新比较。在常规微调中，整个权重更新矩阵（$\Delta W$）显著减少了可训练参数的数量，通过利用内维度（r），这是一个超参数。这种方法在内存和计算方面更为高效，使其非常适合大规模模型的微调。（改编自
    [[63](#bib.bib63)]）
- en: Benefits of Using LoRA
  id: totrans-931
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 的好处
- en: '1.'
  id: totrans-932
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Parameter Efficiency: LoRA significantly reduces the number of parameters that
    need to be trained by focusing only on the low-rank matrices, resulting in lower
    memory and storage requirements compared to full fine-tuning.'
  id: totrans-933
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数效率：LoRA 通过仅关注低秩矩阵显著减少了需要训练的参数数量，从而比完全微调需要更少的内存和存储需求。
- en: '2.'
  id: totrans-934
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Efficient Storage: The storage of the trained model is more efficient as it
    only requires storing the low-rank matrices instead of the full model weights.'
  id: totrans-935
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效存储：训练后的模型存储更为高效，因为只需要存储低秩矩阵而不是完整的模型权重。
- en: '3.'
  id: totrans-936
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Reduced Computational Load: Training with low-rank matrices requires fewer
    computational resources, making it faster and more scalable.'
  id: totrans-937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 降低计算负担：使用低秩矩阵进行训练需要较少的计算资源，从而使其更快且更具可扩展性。
- en: '4.'
  id: totrans-938
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Lower Memory Footprint: Since fewer parameters are being updated, the memory
    footprint during training is reduced, enabling the use of larger batch sizes or
    more complex models within the same hardware constraints.'
  id: totrans-939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 较低的内存占用：由于更新的参数较少，训练过程中的内存占用减少，使得在相同硬件限制下可以使用更大的批量大小或更复杂的模型。
- en: '5.'
  id: totrans-940
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Flexibility: LoRA can be easily integrated with existing pre-trained models
    without extensive modifications to the model architecture.'
  id: totrans-941
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性：LoRA 可以轻松与现有的预训练模型集成，而无需对模型架构进行大量修改。
- en: '6.'
  id: totrans-942
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Compatibility: It can be used alongside other fine-tuning techniques, such
    as adapter layers or prompt-tuning, to further enhance performance.'
  id: totrans-943
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 兼容性：它可以与其他微调技术（如适配器层或提示调整）一起使用，以进一步提升性能。
- en: '7.'
  id: totrans-944
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Comparable Results: Despite the reduction in the number of trainable parameters,
    LoRA has been shown to achieve performance comparable to full fine-tuning in many
    tasks.'
  id: totrans-945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相当的结果：尽管可训练参数数量减少，LoRA 已被证明在许多任务中能达到与完全微调相当的性能。
- en: '8.'
  id: totrans-946
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Task-Specific Adaptation: It effectively adapts the pre-trained model to specific
    tasks, leveraging the knowledge already embedded in the original model.'
  id: totrans-947
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定适配：它有效地将预训练模型适配到特定任务，利用原模型中已嵌入的知识。
- en: '9.'
  id: totrans-948
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating
    overfitting, especially when dealing with smaller task-specific datasets.'
  id: totrans-949
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 避免过拟合：通过专注于低秩更新，LoRA 有助于减轻过拟合，特别是在处理较小的任务特定数据集时。
- en: Limitations
  id: totrans-950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制
- en: 'While LoRA demonstrates considerable power, it also presents challenges:'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LoRA 展示了相当的能力，但也存在一些挑战：
- en: •
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding
    substantial alterations to the pre-trained model’s internal representations.'
  id: totrans-953
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调范围：LoRA 在应用于需要对预训练模型内部表示进行大幅修改的任务时可能会遇到困难。
- en: •
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyperparameter Optimisation: Tuning the rank parameter ‘r’ requires meticulous
    adjustment for optimal performance.'
  id: totrans-955
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数优化：调整秩参数‘r’需要精细调整，以获得最佳性能。
- en: •
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ongoing Research: Despite its promise, LoRA is still in active research stages,
    and its long-term implications remain to be fully explored.'
  id: totrans-957
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 进行中的研究：尽管前景广阔，LoRA 仍处于活跃的研究阶段，其长期影响仍待全面探索。
- en: Despite these challenges, LoRA stands as a pioneering technique with vast potential
    to democratise access to the capabilities of LLMs. Continued research and development
    offer the prospect of overcoming current limitations and unlocking even greater
    efficiency and adaptability.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，LoRA 仍然是一项开创性的技术，具有广阔的潜力来普及大型语言模型的能力。持续的研究和开发有望克服当前的限制，并释放出更高的效率和适应性。
- en: Tutorial for Fine-Tuning LLM Using LoRA
  id: totrans-959
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 微调大型语言模型（LLM）教程
- en: An open-source template for fine-tuning LLMs using the LoRA method with the
    Hugging Face library can be found [here](https://gitlab.com/CeADARIreland_Public/llm-resources).
    This template is designed specifically for adapting LLMs for instruction fine-tuning
    processes.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 [这里](https://gitlab.com/CeADARIreland_Public/llm-resources) 找到一个用于使用 Hugging
    Face 库通过 LoRA 方法微调大型语言模型的开源模板。此模板专为调整 LLM 以进行指令微调过程而设计。
- en: 6.3.3 QLoRA
  id: totrans-961
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 QLoRA
- en: 'QLoRA[[64](#bib.bib64)] is an extended version of LoRA designed for greater
    memory efficiency in large language models (LLMs) by quantising weight parameters
    to 4-bit precision. Typically, LLM parameters are stored in a 32-bit format, but
    QLoRA compresses them to 4-bit, significantly reducing the memory footprint. This
    allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also
    quantises the weights of the LoRA adapters from 8-bit to 4-bit, further decreasing
    memory and storage requirements (see Figure[6.4](#Ch6.F4 "Figure 6.4 ‣ 6.3.3 QLoRA
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")). Despite the reduction in bit precision, QLoRA maintains performance
    levels comparable to traditional 16-bit fine-tuning.'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 'QLoRA[[64](#bib.bib64)] 是 LoRA 的扩展版本，旨在通过将权重参数量化为 4 位精度来提高大型语言模型 (LLMs) 的内存效率。通常，LLM
    参数以 32 位格式存储，但 QLoRA 将其压缩为 4 位，显著减少内存占用。这使得在较弱的硬件上，包括消费级 GPU 上进行微调成为可能。QLoRA 还将
    LoRA 适配器的权重量化从 8 位减少到 4 位，进一步降低内存和存储需求（见图[6.4](#Ch6.F4 "Figure 6.4 ‣ 6.3.3 QLoRA
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")）。尽管减少了位精度，QLoRA 仍保持与传统 16 位微调相当的性能水平。'
- en: It achieves this by backpropagating gradients through a frozen, 4-bit quantised
    pre-trained language model into Low-Rank Adapters, making the fine-tuning process
    efficient while preserving model effectiveness. The QLoRA configuration is supported
    by HuggingFace via the PEFT library, utilising LoraConfig and BitsAndBytesConfig
    for quantisation. Innovations such as an optimal 4-bit data type, double quantisation
    of constants, and memory spike management enable QLoRA to reduce memory usage
    from 96 bits per parameter in traditional fine-tuning to 5.2 bits per parameter,
    an 18-fold reduction.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过将梯度通过冻结的 4 位量化预训练语言模型反向传播到低秩适配器中来实现这一点，使得微调过程高效，同时保持模型效果。QLoRA 配置由 HuggingFace
    通过 PEFT 库支持，利用 LoraConfig 和 BitsAndBytesConfig 进行量化。诸如最佳 4 位数据类型、常量的双重量化和内存峰值管理等创新，使得
    QLoRA 能够将传统微调中的每个参数的内存使用量从 96 位减少到 5.2 位，减少了 18 倍。
- en: '![Refer to caption](img/9359865e6fad00982fa64032a7095eaf.png)'
  id: totrans-964
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9359865e6fad00982fa64032a7095eaf.png)'
- en: 'Figure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This
    figure illustrates the QLoRA optimisation process, showing how the optimisation
    states, adapters, and the model interact during fine-tuning. It demonstrates the
    use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise the memory
    and computational efficiency during the fine-tuning of large language models (adapted
    from [[65](#bib.bib65)]).'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.4: 量化低秩适配 (QLoRA) 优化工作流程。该图展示了 QLoRA 优化过程，显示了在微调过程中优化状态、适配器和模型如何互动。它展示了使用不同位宽（32
    位、16 位和 4 位）来优化大型语言模型微调的内存和计算效率（改编自 [[65](#bib.bib65)]）。'
- en: Performance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit
    quantised models on benchmarks. Additionally, QLoRA enabled the fine-tuning of
    a high-quality 4-bit chatbot using a single GPU in 24 hours, achieving quality
    comparable to ChatGPT.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，QLoRA 超越了原始的 4 位量化，并在基准测试中匹配了 16 位量化模型。此外，QLoRA 使得使用单个 GPU 在 24 小时内微调出高质量的
    4 位聊天机器人，达到与 ChatGPT 相当的质量。
- en: This [tutorial](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
    explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the
    Phi-2 model.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 本 [教程](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
    讲解了在 Phi-2 模型上微调 QLoRA 的端到端步骤。
- en: 6.3.4 Weight-Decomposed Low-Rank Adaptation (DoRA)
  id: totrans-968
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4 权重分解低秩适配 (DoRA)
- en: In the context of optimising model fine-tuning, the pattern analysis of LoRA
    and Full Fine-Tuning (FT) reveals significant differences in learning behaviours
    and updates. LoRA, employing a strategy of incrementally updating pre-trained
    weights using the product of two low-rank matrices, maintains the original weights
    largely static during the fine-tuning process, which allows for efficient inference.
    Despite its computational efficiency, previous studies have suggested that LoRA’s
    limited number of trainable parameters might contribute to its performance discrepancies
    when compared to FT.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化模型微调的背景下，LoRA 和全微调（FT）的模式分析揭示了学习行为和更新的显著差异。LoRA 采用逐步更新预训练权重的低秩矩阵乘积策略，使得在微调过程中原始权重基本保持不变，从而实现高效的推理。尽管其计算效率高，但以往研究表明
    LoRA 的有限可训练参数可能导致其性能与 FT 相比存在差异。
- en: Weight-Decomposed Low-Rank Adaptation (DoRA) [[66](#bib.bib66)] is a novel fine-tuning
    methodology designed to optimise pre-trained models by decomposing their weights
    into magnitude and directional components. This approach leverages the efficiency
    of Low-Rank Adaptation (LoRA) for directional updates, facilitating substantial
    parameter updates without altering the entire model architecture. DoRA addresses
    the computational challenges associated with traditional full fine-tuning (FT)
    by maintaining model simplicity and inference efficiency, while simultaneously
    bridging the performance gap typically observed between LoRA and FT. Empirical
    and theoretical evaluations demonstrate that DoRA not only achieves learning outcomes
    comparable to FT across diverse tasks—including natural language processing and
    vision-language applications—but also consistently surpasses LoRA in performance,
    providing a robust solution for enhancing the adaptability and efficiency of large-scale
    models.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分解低秩适应（DoRA）[[66](#bib.bib66)]是一种新颖的微调方法，通过将预训练模型的权重分解为幅度和方向分量来优化模型。这种方法利用低秩适应（LoRA）在方向更新中的高效性，实现显著的参数更新，而无需改变整个模型架构。DoRA
    通过保持模型的简单性和推理效率，同时弥合 LoRA 和全微调（FT）之间通常存在的性能差距，解决了传统全微调（FT）相关的计算挑战。实证和理论评估表明，DoRA
    不仅在自然语言处理和视觉语言应用等多种任务中达到与 FT 相当的学习效果，而且在性能上始终超越 LoRA，提供了一种增强大规模模型适应性和效率的稳健解决方案。
- en: '![Refer to caption](img/772bfe082dddb41106b6ac962dcba452.png)'
  id: totrans-971
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/772bfe082dddb41106b6ac962dcba452.png)'
- en: 'Figure 6.5: An overview of DoRA (Decomposed Representations for Adaptation),
    which is a method for weight decomposed low-rank adaptation. The figure illustrates
    how pre-trained weights are decomposed and adapted for fine-tuning. In the left
    section, pre-trained weights are decomposed into a magnitude and direction. The
    right section shows how these decomposed weights are merged with trainable parameters
    during fine-tuning, resulting in updated weights that combine both frozen (blue)
    and trainable (green) components. The process emphasises efficient adaptation
    by focusing on the most significant directions in the parameter space, facilitating
    effective fine-tuning while maintaining the integrity of the original model (adapted
    from [[66](#bib.bib66)]).'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：DoRA（分解适应表示）的概述，这是一种权重分解低秩适应的方法。图示了如何将预训练权重分解并适应微调。在左侧部分，预训练权重被分解为幅度和方向。右侧部分显示了这些分解的权重如何与可训练参数合并进行微调，从而生成结合了冻结（蓝色）和可训练（绿色）组件的更新权重。该过程通过关注参数空间中最重要的方向，强调高效适应，同时保持原始模型的完整性（改编自
    [[66](#bib.bib66)]）。
- en: Python Library - DoRA is facilitated via the HuggingFace LoraConfig package.
    To incorporate DoRA into the fine-tuning process, it is essential to specify the
    ’use_dora = True’ parameter during the Lora configuration. Further information
    on initialisation can be found [here](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/lora).
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: Python 库 - DoRA 通过 HuggingFace 的 LoraConfig 包来实现。要将 DoRA 纳入微调过程，必须在 Lora 配置中指定
    'use_dora = True' 参数。有关初始化的更多信息，请参见 [这里](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/lora)。
- en: Benefits of DoRA
  id: totrans-974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DoRA 的好处
- en: '1.'
  id: totrans-975
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling
    full fine-tuning (FT) by decomposing pre-trained weights into magnitude and directional
    components, allowing for more nuanced updates.'
  id: totrans-976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强的学习能力：DoRA 通过将预训练权重分解为幅度和方向分量，获得了与全微调（FT）相似的学习能力，允许更细致的更新。
- en: '2.'
  id: totrans-977
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation
    (LoRA) for directional updates, DoRA enables efficient fine-tuning without altering
    the entire model architecture.'
  id: totrans-978
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高效微调：通过利用低秩适应（LoRA）的结构优势进行方向性更新，DoRA实现了高效的微调，而无需改变整个模型架构。
- en: '3.'
  id: totrans-979
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'No Additional Inference Latency: Despite its improved learning capabilities,
    DoRA does not introduce any additional inference latency over LoRA, maintaining
    model simplicity and efficiency.'
  id: totrans-980
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无额外推理延迟：尽管具备改进的学习能力，DoRA并未引入额外的推理延迟，保持了模型的简单性和效率。
- en: '4.'
  id: totrans-981
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Superior Performance: Experimental results demonstrate that DoRA consistently
    outperforms LoRA across a wide range of tasks, including natural language processing
    (NLP), visual instruction tuning, and image/video-text understanding. For example,
    it shows significant improvements in commonsense reasoning and visual instruction
    tuning benchmarks.'
  id: totrans-982
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卓越表现：实验结果表明，DoRA在包括自然语言处理（NLP）、视觉指令调优和图像/视频文本理解等广泛任务中始终优于LoRA。例如，它在常识推理和视觉指令调优基准测试中显示了显著的改进。
- en: '5.'
  id: totrans-983
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Versatility Across Backbones: DoRA has been validated across various model
    backbones, including large language models (LLM) and vision-language models (LVLM),
    indicating its broad applicability and robustness in different domains.'
  id: totrans-984
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨骨干的多样性：DoRA已在包括大型语言模型（LLM）和视觉语言模型（LVLM）在内的各种模型骨干上得到验证，表明其在不同领域中的广泛适用性和鲁棒性。
- en: '6.'
  id: totrans-985
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Innovative Analysis: The introduction of a novel weight decomposition analysis
    helps uncover fundamental differences in the learning patterns of FT and various
    parameter-efficient fine-tuning (PEFT) methods, contributing to a deeper understanding
    of model fine-tuning dynamics.'
  id: totrans-986
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创新分析：引入了一种新颖的权重分解分析，有助于揭示FT和各种参数高效微调（PEFT）方法在学习模式上的根本差异，从而加深对模型微调动态的理解。
- en: Comparison between LoRA and DoRA
  id: totrans-987
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LoRA与DoRA的比较
- en: 'Low-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA)
    are both advanced techniques designed to improve the efficiency and effectiveness
    of fine-tuning large pre-trained models. While they share the common goal of reducing
    computational overhead, they employ different strategies to achieve this (see
    Table[6.2](#Ch6.T2 "Table 6.2 ‣ Comparison between LoRA and DoRA ‣ 6.3.4 Weight-Decomposed
    Low-Rank Adaptation (DoRA) ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques
    ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model
    Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs:
    An Exhaustive Review of Technologies, Research, Best Practices, Applied Research
    Challenges and Opportunities (Version 1.0)")).'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）和权重分解低秩适应（DoRA）都是旨在提高大规模预训练模型微调效率和效果的先进技术。虽然它们有减少计算开销的共同目标，但采用了不同的策略来实现这一目标（参见表[6.2](#Ch6.T2
    "表 6.2 ‣ LoRA与DoRA的比较 ‣ 6.3.4 权重分解低秩适应（DoRA） ‣ 6.3 参数高效微调（PEFT）技术 ‣ 第6章第4阶段：微调技术选择及适当的模型配置
    ‣ 从基础到突破的LLM微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（版本1.0）")）。
- en: '| Criteria | LoRA (Low-Rank Adaptation) | DoRA (Weight-Decomposed Low-Rank
    Adaptation) |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | LoRA（低秩适应） | DoRA（权重分解低秩适应） |'
- en: '| --- | --- | --- |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective | Provide an efficient method for fine-tuning pre-trained models
    by using low-rank matrix products to update weights incrementally without increasing
    inference latency. | Improves learning capacity by closely mimicking the learning
    patterns of full fine-tuning, optimising magnitude and direction separately. |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 提供一种高效的预训练模型微调方法，通过使用低秩矩阵乘积逐步更新权重，而不会增加推理延迟。 | 通过密切模拟完全微调的学习模式，优化幅度和方向，提升学习能力。
    |'
- en: '| Approach | Implements a low-rank decomposition where the weight update is
    modelled as the product of two low-rank matrices (B and A), keeping the original
    weights static. | Uses weight decomposition analysis to reparameterise the weight
    matrix into separate magnitude and direction components for distinct updates.
    |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 实现了一种低秩分解，其中权重更新被建模为两个低秩矩阵（B和A）的乘积，保持原始权重不变。 | 使用权重分解分析将权重矩阵重新参数化为独立的幅度和方向组件，以便进行不同的更新。
    |'
- en: '| Model Architecture | Keeps the pre-trained weight matrix (W0) unchanged and
    applies updates using low-rank matrices (B and A). Matrix A is initialised with
    a uniform Kaiming distribution, while B is set to zero initially. | Restructures
    the weight matrix into magnitude and directional components, ensuring directional
    vectors are unit vectors for more detailed adjustments. |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | 保持预训练权重矩阵（W0）不变，并使用低秩矩阵（B和A）进行更新。矩阵A初始化为均匀Kaiming分布，而B初始设置为零。 | 将权重矩阵重构为幅度和方向分量，确保方向向量为单位向量，以便进行更详细的调整。
    |'
- en: 'Table 6.2: A detailed comparison between LoRA (Low-Rank Adaptation) and DoRA
    (Weight-Decomposed Low-Rank Adaptation), highlighting their objectives, approaches,
    and the specific architectural strategies they employ for fine-tuning large language
    models.'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：LoRA（低秩自适应）和DoRA（权重分解低秩自适应）之间的详细比较，突出了它们的目标、方法以及它们在微调大型语言模型时采用的具体架构策略。
- en: Tutorial for Fine-Tuning LLM using DoRA
  id: totrans-995
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用DoRA微调LLM的教程
- en: This [tutorial](https://www.kaggle.com/code/aisuko/dora-from-scratch) offers
    an in-depth guide and detailed explanation of the steps involved in implementing
    DoRA from scratch, as well as insights into the fine-tuning process essential
    for optimising performance.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[tutorial](https://www.kaggle.com/code/aisuko/dora-from-scratch)提供了一个深入的指南和实现DoRA从头开始的详细说明，以及优化性能所需的微调过程的见解。
- en: 6.3.5 Fine-Tuning with Multiple Adapters
  id: totrans-997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5 多适配器微调
- en: 'During fine-tuning, we have explored the method of freezing the parameters
    of the LLM and focusing solely on fine-tuning a few million trainable parameters
    using LoRA. For example, fine-tuning an LLM for translation involves training
    a translation adapter with relevant data. This approach allows us to fine-tune
    separate adapters for each specific task we want the LLM to perform. However,
    a key question arises: can we consolidate multiple adapters into a unified multi-task
    adapter? For instance, if we have separate adapters for translation and summarisation
    tasks, can we merge them so that the LLM can proficiently handle both tasks? (Illustrated
    via Figure[6.6](#Ch6.F6 "Figure 6.6 ‣ 6.3.5 Fine-Tuning with Multiple Adapters
    ‣ 6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection
    of Fine-Tuning Techniques and Appropriate Model Configurations ‣ The Ultimate
    Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of
    Technologies, Research, Best Practices, Applied Research Challenges and Opportunities
    (Version 1.0)")).'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '在微调过程中，我们探索了冻结LLM参数的方法，并专注于使用LoRA微调少量的可训练参数。例如，微调用于翻译的LLM涉及用相关数据训练翻译适配器。这种方法允许我们为每个特定任务微调单独的适配器。然而，关键问题出现了：我们能否将多个适配器整合成一个统一的多任务适配器？例如，如果我们有用于翻译和摘要任务的单独适配器，我们能否将它们合并，使得LLM能够熟练地处理这两个任务？（通过图示[6.6](#Ch6.F6
    "Figure 6.6 ‣ 6.3.5 Fine-Tuning with Multiple Adapters ‣ 6.3 Parameter-Efficient
    Fine-Tuning (PEFT) Techniques ‣ Chapter 6 Stage 4: Selection of Fine-Tuning Techniques
    and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning LLMs
    from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")展示）。'
- en: 'The PEFT library simplifies the process of merging adapters with its add_weighted_adapter
    function ³³3[https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter),
    which offers three distinct methods:'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT库通过其add_weighted_adapter函数简化了合并适配器的过程³³3[https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter)，该函数提供了三种不同的方法：
- en: '1.'
  id: totrans-1000
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Concatenation: This straightforward method concatenates the parameters of the
    adapters. For instance, if two adapters each have a rank of 16, the resulting
    adapter will have a rank of 32\. This method is highly efficient.'
  id: totrans-1001
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接：这种简单方法将适配器的参数进行连接。例如，如果两个适配器的秩均为16，则结果适配器的秩为32。这种方法效率极高。
- en: '2.'
  id: totrans-1002
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Linear Combination: Although less documented, this method appears to perform
    a weighted sum of the adapters’ parameters.'
  id: totrans-1003
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性组合：尽管文献中对此方法的记录较少，但它似乎执行了适配器参数的加权和。
- en: '3.'
  id: totrans-1004
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'SVD: The default method employs singular value decomposition through torch.linalg.svd.
    While versatile, it is notably slower than the other methods, particularly for
    adapters with high ranks (greater than 100), which can take several hours.'
  id: totrans-1005
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SVD：默认方法使用torch.linalg.svd进行奇异值分解。虽然方法多样，但相较于其他方法，其速度明显较慢，尤其是在处理高排名（大于100）的适配器时，可能需要几个小时。
- en: Each method allows for customising the combination by adjusting weights. For
    instance, when merging two adapters, X and Y, assigning more weight to X ensures
    that the resulting adapter prioritises behaviour similar to X over Y.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都允许通过调整权重来自定义组合。例如，当合并两个适配器 X 和 Y 时，为 X 分配更多的权重可以确保生成的适配器优先表现出与 X 相似的行为而非
    Y。
- en: This approach is particularly suited for consolidating a single LLM to handle
    multiple tasks rather than creating separate models for each task domain. By adopting
    this method, there is no longer a need to individually fine-tune a model for each
    task. Instead, a single adapter layer can be fine-tuned for each task, allowing
    queries to yield the desired responses efficiently.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法特别适合将单一 LLM 整合以处理多个任务，而不是为每个任务领域创建单独的模型。通过采用这种方法，不再需要为每个任务单独微调模型。相反，可以为每个任务微调一个适配器层，从而高效地获得所需的响应。
- en: '![Refer to caption](img/e419499ac781af2f2bdfafd996ac6163.png)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e419499ac781af2f2bdfafd996ac6163.png)'
- en: 'Figure 6.6: Overview of how multiple adapters can be used with a pre-trained
    LLM to fine-tune it for various specific tasks, such as summarisation, proofreading,
    sentiment analysis, and more. (adapted from [[67](#bib.bib67)])'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：概述如何使用多个适配器对预训练的 LLM 进行微调，以处理各种特定任务，如总结、校对、情感分析等。（改编自[[67](#bib.bib67)]）
- en: Steps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters
  id: totrans-1010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 LoRA 微调 LLM 以处理多个任务和适配器的步骤
- en: '1.'
  id: totrans-1011
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks
    using different prompt formats or task-identifying tags (e.g., [translate_fren],
    [chat]).'
  id: totrans-1012
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适配器创建：创建多个适配器，每个适配器都经过微调，以适应特定任务，使用不同的提示格式或任务标识标签（例如，[translate_fren]，[chat]）。
- en: '2.'
  id: totrans-1013
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'LoRA Integration: Implement LoRA to efficiently integrate these adapters into
    the pre-trained LLM. Utilise LoRA’s methods such as concatenation, linear combination,
    or singular value decomposition (SVD) to combine adapters while minimising computational
    overhead and maintaining performance.'
  id: totrans-1014
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoRA 集成：实现 LoRA，以高效地将这些适配器集成到预训练的 LLM 中。利用 LoRA 的方法，如连接、线性组合或奇异值分解（SVD），在最小化计算开销和保持性能的同时，组合适配器。
- en: '3.'
  id: totrans-1015
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Task-Specific Adaptation: Fine-tune each adapter with task-specific data to
    enhance performance for individual tasks. Ensure adapters are trained with data
    relevant to their respective tasks, optimising their ability to generate accurate
    responses.'
  id: totrans-1016
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定的适应：使用任务特定的数据对每个适配器进行微调，以提高其在各个任务中的性能。确保适配器使用与其各自任务相关的数据进行训练，从而优化其生成准确回应的能力。
- en: '4.'
  id: totrans-1017
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Behaviour Adjustment: Monitor the behaviour of combined adapters to identify
    any undesired inherited behaviours from individual adapters (e.g., short response
    generation from a translation adapter). Adjust the combination weights or types
    to modify adapter behaviour as needed, ensuring each adapter performs optimally
    for its intended task.'
  id: totrans-1018
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行为调整：监控组合适配器的行为，以识别任何来自个别适配器的非预期继承行为（例如，翻译适配器生成的短响应）。根据需要调整组合权重或类型，以修改适配器行为，确保每个适配器在其预期任务中表现最佳。
- en: '5.'
  id: totrans-1019
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Evaluation and Iteration: Evaluate the performance of the combined model across
    multiple tasks using validation datasets. Iterate on the fine-tuning process,
    making adjustments to adapter combinations and training parameters based on performance
    metrics and user feedback.'
  id: totrans-1020
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和迭代：使用验证数据集评估组合模型在多个任务中的性能。根据性能指标和用户反馈，迭代微调过程，调整适配器组合和训练参数。
- en: Therefore, for optimal performance, it is advisable to combine adapters that
    have been fine-tuned with distinctly varied prompt formats. However, even when
    using adapters with different prompt formats, the resulting adapter may not exhibit
    desired behaviour. For example, a newly combined adapter designed for chatting
    may only generate short responses, inheriting this tendency from an adapter that
    was originally trained to halt after producing a single sentence. To adjust the
    behaviour of the combined adapter, one can prioritise the influence of a specific
    adapter during the combination process and/or modify the method of combination
    used.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了获得最佳性能，建议组合经过明显不同提示格式微调的适配器。然而，即使使用不同提示格式的适配器，生成的适配器也可能不表现出期望的行为。例如，专为聊天设计的新组合适配器可能仅生成短响应，继承了原本被训练成在生成单句后停止的适配器的倾向。要调整组合适配器的行为，可以在组合过程中优先考虑特定适配器的影响和/或修改使用的组合方法。
- en: An illustrative tutorial demonstrating the fine-tuning of large language models
    (LLMs) using multiple adapter layers for various tasks can be found [here](https://kaitchup.substack.com/p/combine-multiple-lora-adapters-for).
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 演示使用多个适配器层进行各种任务的语言模型（LLMs）微调的说明性教程可以在[这里](https://kaitchup.substack.com/p/combine-multiple-lora-adapters-for)找到。
- en: 6.4 Half Fine Tuning
  id: totrans-1023
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 半微调
- en: Half Fine-Tuning (HFT)[[68](#bib.bib68)] is a technique designed to balance
    the retention of foundational knowledge with the acquisition of new skills in
    large language models (LLMs). HFT involves freezing half of the model’s parameters
    during each fine-tuning round while updating the other half, allowing the model
    to retain pre-trained knowledge and enhance new task performance without altering
    the model architecture.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 半微调（HFT）[[68](#bib.bib68)]是一种旨在平衡大型语言模型（LLMs）中基础知识保留与新技能获取的技术。HFT涉及在每轮微调期间冻结模型的一半参数，同时更新另一半，从而允许模型保留预训练知识并提高新任务性能，而不改变模型架构。
- en: 'Each repetitive transformer layer is divided into three blocks: self-attention,
    feed-forward, and layernorm, with half of the parameters in each block updated
    and the other half frozen, varying with each round. This strategic parameter update
    helps maintain knowledge parity across training rounds and enhances scalability
    in successive training sessions.'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 每个重复的变换器层被划分为三个块：自注意力、前馈和层归一化，其中每个块的一半参数被更新，另一半保持冻结，每轮都会有所变化。这种战略参数更新有助于保持训练轮次之间的知识一致性，并增强在连续训练会话中的可扩展性。
- en: Research on models like LLAMA 2-7B demonstrated that HFT could significantly
    restore forgotten basic knowledge while preserving high general ability performance.
    This method’s robustness and efficiency make it applicable to various fine-tuning
    scenarios, including supervised fine-tuning, direct preference optimisation, and
    continual learning. Additionally, HFT’s ability to maintain the model architecture
    simplifies its implementation and ensures compatibility with existing systems,
    further promoting its practical adoption.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 对像LLAMA 2-7B这样的模型的研究表明，HFT可以显著恢复遗忘的基本知识，同时保持高水平的整体能力表现。这种方法的鲁棒性和效率使其适用于各种微调场景，包括监督微调、直接偏好优化和持续学习。此外，HFT保持模型架构的能力简化了其实施，并确保与现有系统的兼容性，进一步促进了其实际应用。
- en: '![Refer to caption](img/19a3f0078a3978fcd8766bef545cae81.png)'
  id: totrans-1027
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19a3f0078a3978fcd8766bef545cae81.png)'
- en: 'Figure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as
    applied to LLAMA 2’s architecture. The diagram shows multiple stages of fine-tuning,
    where specific model parameters are selectively activated (orange) while others
    remain frozen (blue). This approach optimises training by reducing computational
    requirements while still effectively adapting the model to new tasks or data.
    (adapted from [[68](#bib.bib68)])'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：半微调（HFT）方法应用于LLAMA 2架构的示意图。该图显示了多个微调阶段，其中特定模型参数被选择性激活（橙色），而其他参数保持冻结（蓝色）。这种方法通过减少计算需求来优化训练，同时有效地将模型适应于新任务或数据。（改编自[[68](#bib.bib68)]）
- en: 6.4.1 Benefits of using Half Fine tuning
  id: totrans-1029
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 使用半微调的好处
- en: '1.'
  id: totrans-1030
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters
    to their pre-trained state, HFT effectively recovers a portion of the original
    knowledge, thereby mitigating catastrophic forgetting of previously acquired capabilities.'
  id: totrans-1031
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练知识的恢复：通过将一半微调参数回滚到其预训练状态，HFT有效地恢复了部分原始知识，从而减轻了对先前获得能力的灾难性遗忘。
- en: '2.'
  id: totrans-1032
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Enhanced Performance: Research experiments shows that HFT maintains or even
    surpasses the performance of full fine-tuning (FFT) on downstream tasks, demonstrating
    its effectiveness in balancing knowledge retention with task-specific learning.'
  id: totrans-1033
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能提升：研究实验表明，HFT在下游任务中的表现保持不变甚至超越了全面微调（FFT），展示了其在平衡知识保留与任务特定学习方面的有效性。
- en: '3.'
  id: totrans-1034
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Robustness: The method is robust to different selection strategies and the
    number of parameters chosen for updating, ensuring consistent performance across
    various configurations.'
  id: totrans-1035
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性：该方法对不同的选择策略和更新参数的数量具有鲁棒性，确保在各种配置下性能的一致性。
- en: '4.'
  id: totrans-1036
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Simplicity and Scalability: HFT does not alter the model architecture, which
    simplifies implementation and allows for scalable applications, particularly beneficial
    in successive fine-tuning scenarios.'
  id: totrans-1037
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单性和可扩展性：HFT不会改变模型架构，这简化了实现过程，并允许可扩展的应用，特别是在连续微调场景中尤为有利。
- en: '5.'
  id: totrans-1038
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Versatility: The technique has proven effective across diverse fine-tuning
    scenarios, including supervised fine-tuning, direct preference optimisation, and
    continual learning.'
  id: totrans-1039
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多功能性：该技术在各种精调场景中已证明有效，包括监督精调、直接偏好优化和持续学习。
- en: 6.4.2 Comparison between HFT and LoRA
  id: totrans-1040
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 HFT 与 LoRA 的比较
- en: '| Criteria | HFT | LoRA |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | HFT | LoRA |'
- en: '| --- | --- | --- |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Objective | The goal is to retain the foundational knowledge acquired during
    pre-training while learning new task-specific skills, thus balancing between maintaining
    existing capabilities and acquiring new ones. | LoRA aims to reduce computational
    and memory requirements during fine-tuning, making it more efficient and feasible
    to train large models on limited hardware resources. |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 目标是保留预训练过程中获得的基础知识，同时学习新的任务特定技能，从而在保持现有能力和获得新技能之间取得平衡。 | LoRA 旨在减少精调过程中对计算和内存的需求，使其在有限硬件资源上训练大型模型变得更高效、更可行。
    |'
- en: '| Approach | HFT involves freezing half of the model’s parameters during each
    fine-tuning round and updating only the other half. | LoRA reduces the number
    of trainable parameters by introducing low-rank decomposition into the weight
    matrices of the neural network. This involves injecting low-rank matrices into
    the model’s layers during fine-tuning. |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | HFT 涉及在每次精调轮次中冻结模型的一半参数，只更新另一半参数。 | LoRA 通过将低秩分解引入神经网络的权重矩阵来减少可训练参数的数量。这在精调过程中将低秩矩阵注入模型的层中。
    |'
- en: '| Model Architecture | HFT does not alter the model’s architecture or introduce
    new parameters, making it straightforward to apply without additional structural
    changes. | LoRA modifies the model by adding low-rank matrices, which changes
    the training dynamics and requires additional computations for the low-rank updates.
    |'
  id: totrans-1045
  prefs: []
  type: TYPE_TB
  zh: '| 模型架构 | HFT 不改变模型的架构或引入新的参数，使其应用简单，无需额外的结构变更。 | LoRA 通过添加低秩矩阵来修改模型，这改变了训练动态，并需要额外的计算来更新低秩矩阵。
    |'
- en: '| Performance | Research has shown that HFT can restore forgotten basic knowledge
    while maintaining high performance in general abilities. | LoRA is designed to
    achieve competitive performance with full fine-tuning but with significantly fewer
    trainable parameters and lower computational costs. |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 研究表明，HFT 可以恢复遗忘的基本知识，同时在一般能力方面保持高性能。 | LoRA 旨在实现与完全精调相竞争的性能，但可训练参数显著减少，计算成本也较低。
    |'
- en: 'Table 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation
    (LoRA).'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.3：半精调（HFT）与低秩适应（LoRA）的比较分析。
- en: 6.5 Lamini Memory Tuning
  id: totrans-1048
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 Lamini 记忆精调
- en: Lamini [[69](#bib.bib69)] was introduced as a specialised approach to fine-tuning
    Large Language Models (LLMs), targeting the reduction of hallucinations. This
    development was motivated by the need to enhance the reliability and precision
    of LLMs in domains requiring accurate information retrieval. Traditional training
    methods typically consist of running stochastic gradient descent on vast datasets,
    which, despite fitting the training data well, often produce models that fail
    to generalise effectively and are prone to such errors.
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: Lamini [[69](#bib.bib69)] 被引入作为一种专门的精调方法，旨在减少幻觉现象。此发展是由于需要提高 LLM 在需要准确检索信息的领域中的可靠性和精度。传统的训练方法通常包括在大量数据集上运行随机梯度下降，尽管这些方法对训练数据的拟合较好，但往往会生成无法有效泛化且易于出现错误的模型。
- en: Foundation models often follow a training regimen similar to the Chinchilla
    recipe, which prescribes training for a single epoch on a massive corpus, such
    as training Llama 2 7B on about one trillion tokens. This approach results in
    substantial loss and is geared more towards enhancing generalisation and creativity
    where a degree of randomness in token selection is permissible. However, it falls
    short for tasks demanding high factual precision. In contrast, Lamini Memory Tuning
    delves deeper by analysing the loss of individual facts, significantly improving
    the accuracy of factual recall. By augmenting a model with additional parameters
    specifically for memory (e.g., an 8B parameter model with an extra 2B parameters
    for weights), Lamini enables the model to memorise and accurately recall a significant
    number of facts, closely aligning performance with LLM scaling laws without compromising
    on generalisation.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型通常遵循类似于Chinchilla配方的训练方案，该配方建议在大规模语料库上进行单次训练，例如在大约一万亿个标记上训练Llama 2 7B。这种方法导致显著的损失，更侧重于增强泛化和创造力，在标记选择中允许一定的随机性。然而，对于要求高度事实精确的任务，这种方法并不理想。相比之下，Lamini内存调优通过分析单个事实的损失，深入挖掘，显著提高了事实回忆的准确性。通过为内存特别增添额外参数（例如，一个8B参数模型增加2B参数用于权重），Lamini使模型能够记住并准确回忆大量事实，使性能与LLM规模法则紧密对齐，同时不妨碍泛化能力。
- en: 6.5.1 Lamini-1 - A model architecture based on Lamini
  id: totrans-1051
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1 Lamini-1 - 基于Lamini的模型架构
- en: 'Departing from traditional transformer-based designs, the Lamini-1 model architecture
    (Figure [6.8](#Ch6.F8 "Figure 6.8 ‣ 6.5.1 Lamini-1 - A model architecture based
    on Lamini ‣ 6.5 Lamini Memory Tuning ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)"))
    employs a massive mixture of memory experts (MoME). This system features a pre-trained
    transformer backbone augmented by adapters that are dynamically selected from
    an index using cross-attention mechanisms. These adapters function similarly to
    experts in MoE architectures, and the network is trained end-to-end while freezing
    the backbone. This setup allows for specific facts to be stored exactly in the
    selected experts.'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于传统的变换器设计，Lamini-1模型架构（图[6.8](#Ch6.F8 "图6.8 ‣ 6.5.1 Lamini-1 - 基于Lamini的模型架构
    ‣ 6.5 Lamini内存调优 ‣ 第6章 第4阶段：微调技术和适当模型配置的选择 ‣ 从基础到突破的终极指南：技术、研究、最佳实践、应用研究挑战与机遇的详尽回顾（版本1.0）")）采用了大规模记忆专家混合（MoME）。该系统具有一个经过预训练的变换器骨干，增加了通过交叉注意机制从索引中动态选择的适配器。这些适配器类似于MoE架构中的专家，网络在冻结骨干的同时进行端到端训练。这种设置允许特定的事实被准确存储在选定的专家中。
- en: '![Refer to caption](img/ad279d3f8f14186d1f1e324a6046cb0b.png)'
  id: totrans-1053
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad279d3f8f14186d1f1e324a6046cb0b.png)'
- en: 'Figure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive
    Array of Memory Experts (MoME). This architecture integrates a pre-trained transformer
    backbone with dynamically selected adapters via cross-attention mechanisms. Each
    adapter, functioning as a memory expert, is capable of storing specific factual
    data. (adopted from [[69](#bib.bib69)])'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：Lamini-1模型架构示意图，展示了大规模记忆专家阵列（MoME）。该架构将预训练的变换器骨干与通过交叉注意机制动态选择的适配器集成在一起。每个适配器作为记忆专家，能够存储特定的事实数据。（改编自[[69](#bib.bib69)]）
- en: At inference time, only the relevant experts are retrieved from the index, enabling
    the LLM to store a large number of facts while maintaining low inference latency.
    Specialised GPU kernels written in Triton are used to accelerate the lookup of
    experts, optimising the system for quick access to stored knowledge.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时，仅从索引中检索相关的专家，从而使LLM能够存储大量的事实，同时保持低推理延迟。使用Triton编写的专业GPU内核加速专家的查找，优化系统以快速访问存储的知识。
- en: Systems Optimisations for Banishing Hallucinations
  id: totrans-1056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统优化以消除幻觉
- en: The MoME architecture is designed to minimise the computational demand required
    to memorise facts. During training, a subset of experts, such as 32 out of a million,
    is selected for each fact. The weights of the backbone network and the cross attention
    used to select the expert are frozen, and gradient descent steps are taken until
    the loss is sufficiently reduced to memorise the fact. This approach prevents
    the same expert from being selected multiple times for different facts by first
    training the cross attention selection mechanism during a generalisation training
    phase, then freezing its weights.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: MoME 架构旨在最小化记忆事实所需的计算需求。在训练期间，为每个事实选择一个专家子集，例如从一百万个中选择 32 个。骨干网络的权重和用于选择专家的交叉注意力被冻结，并进行梯度下降步骤，直到损失被足够减少以记忆事实。这种方法通过首先在泛化训练阶段训练交叉注意力选择机制，然后冻结其权重，来防止相同的专家被多次选择用于不同的事实。
- en: This method ensures that computation scales with the number of training examples,
    not the total number of parameters, thereby significantly reducing the computation
    required for memory tuning. This optimised approach allows Lamini-1 to achieve
    near-zero loss in memory tuning on real and random answers efficiently, demonstrating
    its efficacy in eliminating hallucinations while improving factual recall.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法确保了计算的规模与训练示例的数量相关，而非参数的总数量，从而显著减少了内存调优所需的计算量。这种优化方法使 Lamini-1 能够在真实和随机答案上实现接近零的内存调优损失，展示了其在消除幻觉同时提高事实回忆方面的有效性。
- en: 6.6 Mixture of Experts
  id: totrans-1059
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 专家混合
- en: A mixture of experts (MoE) is an architectural design for neural networks that
    divides the computation of a layer or operation (e.g., linear layers, MLPs, or
    attention projection) into several specialised subnetworks, referred to as ”experts”.
    Each expert independently carries out its computation, and the results are aggregated
    to produce the final output of the MoE layer. MoE architectures can be categorised
    as either dense, where every expert is engaged for each input, or sparse, where
    only a subset of experts is utilised for each input.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）是一种神经网络的架构设计，将一层或操作（例如，线性层、MLP 或注意力投影）的计算分为几个专门的子网络，称为“专家”。每个专家独立执行其计算，然后将结果汇总以生成
    MoE 层的最终输出。MoE 架构可以分为密集型，其中每个输入都涉及所有专家，或稀疏型，其中每个输入只利用专家子集。
- en: 6.6.1 Mixtral 8x7B Architecture and Performance
  id: totrans-1061
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1 Mixtral 8x7B 架构与性能
- en: 'Mixtral [[70](#bib.bib70)] 8x7B employs a Sparse Mixture of Experts (SMoE)
    architecture (Figure [6.9](#Ch6.F9 "Figure 6.9 ‣ 6.6.1 Mixtral 8x7B Architecture
    and Performance ‣ 6.6 Mixture of Experts ‣ Chapter 6 Stage 4: Selection of Fine-Tuning
    Techniques and Appropriate Model Configurations ‣ The Ultimate Guide to Fine-Tuning
    LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research,
    Best Practices, Applied Research Challenges and Opportunities (Version 1.0)")),
    mirroring the structure of Mistral 7B but incorporating eight feedforward blocks
    (experts) in each layer. For every token at each layer, a router network selects
    two experts to process the current state and combine their outputs. Although each
    token interacts with only two experts at a time, the selected experts can vary
    at each timestep. Consequently, each token has access to 47 billion parameters
    but utilises only 13 billion active parameters during inference. Mixtral 8x7B
    not only matches but often surpasses Llama 2 70B and GPT-3.5 across all evaluated
    benchmarks. Its performance is notably superior to Llama 2 70B in mathematics,
    code generation, and multilingual tasks.'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral [[70](#bib.bib70)] 8x7B 采用了稀疏专家混合（SMoE）架构（图 [6.9](#Ch6.F9 "图 6.9 ‣ 6.6.1
    Mixtral 8x7B 架构与性能 ‣ 6.6 专家混合 ‣ 第 6 章 第 4 阶段：微调技术的选择和适当模型配置 ‣ 从基础到突破的最终微调指南：对技术、研究、最佳实践、应用研究挑战和机会的详尽评审（版本
    1.0）")），其结构与 Mistral 7B 相似，但在每层中加入了八个前馈块（专家）。对于每个层中的每个令牌，一个路由网络会选择两个专家来处理当前状态并结合它们的输出。尽管每个令牌每次只与两个专家互动，但所选的专家可以在每个时间步中有所不同。因此，每个令牌可以访问
    470 亿个参数，但在推理过程中只使用 130 亿个活动参数。Mixtral 8x7B 不仅与 Llama 2 70B 相匹配，而且在所有评估基准上通常优于
    GPT-3.5。它在数学、代码生成和多语言任务上的表现明显优于 Llama 2 70B。
- en: '![Refer to caption](img/e8a13f98a4bac63b294f5f8e49216e42.png)'
  id: totrans-1063
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8a13f98a4bac63b294f5f8e49216e42.png)'
- en: 'Figure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture.
    The model is composed of a router network that dynamically selects the most relevant
    experts from a pool of eight transformer-based experts, each with 7 billion parameters.
    The experts are organised into transformer blocks, where the router directs data
    to the appropriate expert based on the input, optimising computational efficiency
    and model performance. This architecture allows for scalability and specialised
    processing within large language models. (adapted from [[71](#bib.bib71)])'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：Mixtral 8x7B 专家混合（MoE）模型架构图。该模型由一个路由网络组成，能够从八个基于变压器的专家池中动态选择最相关的专家，每个专家拥有
    70 亿个参数。专家被组织成变压器块，路由器根据输入将数据指向相应的专家，从而优化计算效率和模型性能。这种架构允许在大型语言模型中进行可扩展性和专门化处理。（改编自
    [[71](#bib.bib71)]）
- en: 6.7 Mixture of Agents
  id: totrans-1065
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 专家混合
- en: Despite the numerous LLMs and their notable accomplishments, they continue to
    encounter fundamental limitations regarding model size and training data. Scaling
    these models further is prohibitively expensive, often necessitating extensive
    retraining on multiple trillion tokens. Simultaneously, different LLMs exhibit
    distinct strengths and specialise in various aspects of tasks. A recent study
    has investigated leveraging the collective expertise of multiple LLMs to develop
    a more capable and robust model, a method known as Mixture of Agents (MoA) [[72](#bib.bib72)].
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有众多的 LLM 和其显著成就，但它们仍面临着模型规模和训练数据的基本限制。进一步扩展这些模型的成本极其昂贵，通常需要对多个万亿个标记进行广泛的再训练。同时，不同的
    LLM 展现出不同的优势，并专注于任务的各个方面。最近的一项研究探讨了利用多个 LLM 的集体专业知识来开发更强大和鲁棒的模型，这种方法被称为专家混合（MoA）[[72](#bib.bib72)]。
- en: 'MoA functions using a layered architecture, where each layer comprises multiple
    LLM agents (Figure  [6.10](#Ch6.F10 "Figure 6.10 ‣ 6.7 Mixture of Agents ‣ Chapter
    6 Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations
    ‣ The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive
    Review of Technologies, Research, Best Practices, Applied Research Challenges
    and Opportunities (Version 1.0)")). This structure reveals a phenomenon known
    as the “collaborativeness of LLMs.” The innovative MoA framework utilises the
    combined capabilities of several LLMs to enhance both reasoning and language generation
    proficiency. Research indicates that LLMs naturally collaborate, demonstrating
    improved response quality when incorporating outputs from other models, even if
    those outputs are not ideal.'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: MoA 使用分层架构，每层包含多个 LLM 代理（图 [6.10](#Ch6.F10 "图 6.10 ‣ 6.7 专家混合 ‣ 第六章 第四阶段：选择微调技术和适当的模型配置
    ‣ 从基础到突破的 LLM 微调终极指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾（第 1.0 版）")）。这种结构揭示了一种被称为“LLMs 合作性”的现象。创新的
    MoA 框架利用多个 LLM 的综合能力来提升推理和语言生成的能力。研究表明，LLMs 自然会进行合作，当结合其他模型的输出时，即使这些输出不是理想的，也能提高响应质量。
- en: '![Refer to caption](img/94eb1e435da461a959f43a32f6fd68b7.png)'
  id: totrans-1068
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94eb1e435da461a959f43a32f6fd68b7.png)'
- en: 'Figure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The
    model consists of multiple layers, each incorporating several agents that process
    the input independently before concatenating their outputs to form an intermediate
    result. The process continues across layers, refining the output at each stage
    to generate the final output based on the given prompt (adapted from [[72](#bib.bib72)]).'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：Mixture of Agents (MoA) LLM 配置示意图。该模型由多层组成，每层包含多个独立处理输入的代理，然后将它们的输出串联成一个中间结果。这个过程在各层之间继续进行，每个阶段都细化输出，以生成基于给定提示的最终输出（改编自
    [[72](#bib.bib72)]）。
- en: 6.7.1 Methodology
  id: totrans-1070
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.1 方法论
- en: 'To enhance collaboration among multiple LLMs, it is essential to understand
    their individual strengths and classify them accordingly. The classification includes:'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强多个 LLM 之间的合作，必须了解它们的各自优势并进行分类。分类包括：
- en: '1.'
  id: totrans-1072
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Proposers: These models excel at generating valuable reference responses for
    other models. While they may not perform exceptionally on their own, they provide
    useful context and varied perspectives that improve the final output when utilised
    by an aggregator.'
  id: totrans-1073
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提议者：这些模型擅长为其他模型生成有价值的参考响应。尽管它们可能单独表现并不突出，但当被汇总器利用时，它们提供有用的背景和多样化的视角，从而改善最终输出。
- en: '2.'
  id: totrans-1074
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Aggregators: These models are adept at merging responses from various models
    into a single high-quality result. An effective aggregator should maintain or
    even enhance the quality of the final response, regardless of the quality of the
    individual inputs.'
  id: totrans-1075
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚合器：这些模型擅长将来自不同模型的响应合并为单一的高质量结果。一个有效的聚合器应当能够保持甚至提高最终响应的质量，无论单个输入的质量如何。
- en: 'The careful selection of LLMs for each MoA layer is crucial Performance metrics,
    such as average win rates in a given layer, help assess the suitability of models
    for subsequent layers, ensuring the production of higher-quality outputs. Diversity
    in model outputs is vital, as varied responses from different models contribute
    significantly more than homogeneous outputs from a single model. In MoA, given
    an input prompt, the output of the $i^{\text{th}}$ is calculated as follows:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 MoA 层 LLM 的精心选择至关重要。性能指标，如特定层的平均胜率，有助于评估模型在后续层的适用性，确保产生更高质量的输出。模型输出的多样性很重要，因为来自不同模型的多样化响应比单一模型的同质输出贡献更大。在
    MoA 中，给定输入提示，$i^{\text{th}}$ 的输出计算如下：
- en: '|  | $y_{i}=\bigoplus_{j=1}^{n}\left[A_{i,j}(x_{i})\right]+x_{1},\,x_{i+1}=y_{i}$
    |  | (6.1) |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\bigoplus_{j=1}^{n}\left[A_{i,j}(x_{i})\right]+x_{1},\,x_{i+1}=y_{i}$
    |  | (6.1) |'
- en: 6.7.2 Analogy with MoE
  id: totrans-1078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.2 与 MoE 的类比
- en: 'Mixture-of-Experts (MoE) is a well-established machine learning technique where
    multiple expert networks, each with specialised skills, collaborate to address
    complex problems. This approach has demonstrated significant success across various
    applications and serves as the inspiration for the Mixture-of-Agents (MoA) method.
    In a typical MoE design, a stack of layers, known as MoE layers, consists of multiple
    expert networks, a gating network, and residual connections to improve gradient
    flow. The output for layer $y_{i}$ is calculated as follows:'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合（MoE）是一种成熟的机器学习技术，其中多个具有专业技能的专家网络协作解决复杂问题。这种方法在各种应用中取得了显著成功，并作为专家混合体（MoA）方法的灵感。在典型的
    MoE 设计中，一组层（称为 MoE 层）包括多个专家网络、一个门控网络和残差连接以改善梯度流。层 $y_{i}$ 的输出计算如下：
- en: '|  | $y_{i}=\sum_{j=1}^{n}G_{i,j}(x_{i})E_{i,j}(x_{i})+x_{i}$ |  | (6.2) |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\sum_{j=1}^{n}G_{i,j}(x_{i})E_{i,j}(x_{i})+x_{i}$ |  | (6.2) |'
- en: The MoA framework advances the MoE concept by operating at the model level through
    prompt-based interactions rather than altering internal activations or weights.
    Instead of relying on specialised sub-networks within a single model, MoA utilises
    multiple full-fledged LLMs across different layers. In this approach, the gating
    and expert networks’ functions are integrated within an LLM, leveraging its ability
    to interpret prompts and generate coherent outputs without additional coordination
    mechanisms.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: MoA 框架通过基于提示的交互在模型层面上推进了 MoE 概念，而不是改变内部激活或权重。MoA 不依赖于单一模型中的专门子网络，而是利用多个完整的 LLM
    跨不同层。在这种方法中，门控和专家网络的功能集成在 LLM 内，利用其解释提示和生成连贯输出的能力，无需额外的协调机制。
- en: 6.7.3 What makes MoA works well?
  id: totrans-1082
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.7.3 什么使 MoA 表现良好？
- en: '1.'
  id: totrans-1083
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'MoA’s Superior Performance: MoA significantly outperforms LLM-based rankers,
    which select one answer from the proposals rather than generating new responses.
    This suggests that MoA’s approach of aggregating all generated responses provides
    more effective results than simply choosing from pre-existing options.'
  id: totrans-1084
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MoA 的优越性能：MoA 显著优于基于 LLM 的排名器，这些排名器从提案中选择一个答案，而不是生成新响应。这表明 MoA 通过聚合所有生成的响应所采用的方法比简单地从现有选项中选择要有效得多。
- en: '2.'
  id: totrans-1085
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Effective Incorporation of Proposals: The aggregator in MoA demonstrates a
    tendency to integrate the best proposed answers. This is supported by positive
    correlations between aggregator responses and various similarity metrics, such
    as BLEU scores, which measure n-gram overlaps. The use of alternative similarity
    measures also shows a consistent positive correlation with preference scores,
    indicating that the aggregator effectively utilises the proposed responses.'
  id: totrans-1086
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提案的有效整合：MoA 中的聚合器显示出整合最佳提案的倾向。这得到了聚合器响应与各种相似度指标之间的正相关支持，例如测量 n-gram 重叠的 BLEU
    分数。使用替代相似度度量也显示出与偏好分数的一致正相关，表明聚合器有效利用了提议的响应。
- en: '3.'
  id: totrans-1087
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Influence of Model Diversity and Proposer Count: Increasing the number of proposers
    improves output quality, highlighting the benefits of additional auxiliary information.
    Additionally, using a diverse set of LLMs as proposers consistently yields better
    results compared to using a single LLM. This suggests that both the number and
    diversity of LLM agents in each MoA layer contribute to enhanced performance,
    with potential for further improvement through scaling.'
  id: totrans-1088
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型多样性和提议者数量的影响：增加提议者数量可以提升输出质量，突出附加辅助信息的好处。此外，使用多样化的LLM作为提议者通常能比单一LLM产生更好的结果。这表明每个MoA层中的LLM代理的数量和多样性都对性能提升有所贡献，通过扩展可能会进一步改善。
- en: '4.'
  id: totrans-1089
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Model Specialisation: Analysis of model roles within the MoA ecosystem reveals
    that GPT-4o, Qwen, and LLaMA-3 are effective in both assisting and aggregating
    tasks. In contrast, WizardLM excels as a proposer but struggles with aggregating
    responses from other models.'
  id: totrans-1090
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型专业化：对MoA生态系统中模型角色的分析表明，GPT-4o、Qwen和LLaMA-3在协助和聚合任务中表现有效。相比之下，WizardLM在作为提议者方面表现出色，但在聚合其他模型的响应时表现较差。
- en: 6.8 Proximal Policy Optimisation (PPO)
  id: totrans-1091
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8 近端策略优化（PPO）
- en: PPO [[73](#bib.bib73)] is a widely recognised reinforcement learning algorithm
    used for training agents to perform tasks in diverse environments. This algorithm
    leverages policy gradient methods, where policies—represented by neural networks—determine
    the actions taken by the agent based on the current state. PPO effectively handles
    the dynamic nature of training data generated through continuous agent-environment
    interactions, a feature that differentiates it from static datasets used in supervised
    learning.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: PPO [[73](#bib.bib73)] 是一种广泛认可的强化学习算法，用于训练代理在各种环境中执行任务。该算法利用策略梯度方法，其中策略——由神经网络表示——根据当前状态决定代理采取的行动。PPO有效地处理了通过持续的代理-环境交互生成的动态训练数据，这一特性使其区别于在监督学习中使用的静态数据集。
- en: The innovation of PPO lies in its ”surrogate” objective function, optimised
    via stochastic gradient ascent. This approach allows for multiple updates from
    the same batch of data, enhancing both training efficiency and stability over
    traditional policy gradient methods. Developed by OpenAI, PPO was designed to
    balance ease of implementation with the robust performance characteristics of
    more complex algorithms like Trust Region Policy Optimisation (TRPO), but without
    the associated computational complexity.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的创新在于其“替代”目标函数，通过随机梯度上升进行优化。这种方法允许从同一批数据中进行多次更新，提高了训练效率和稳定性，相较于传统的策略梯度方法。由OpenAI开发，PPO旨在平衡实现的简易性与类似于信赖域策略优化（TRPO）等更复杂算法的强大性能特征，但没有相关的计算复杂度。
- en: PPO operates by maximising expected cumulative rewards through iterative policy
    adjustments that increase the likelihood of actions leading to higher rewards.
    A key feature of PPO is its use of a clipping mechanism in the objective function,
    which limits the extent of policy updates, thus preventing drastic changes and
    maintaining stability during training.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: PPO通过迭代调整策略来最大化期望累积奖励，从而提高导致更高奖励的行动的可能性。PPO的一个关键特性是其在目标函数中使用剪切机制，这限制了策略更新的幅度，从而防止了剧烈的变化，并在训练过程中保持稳定性。
- en: '![Refer to caption](img/59c1bc79768ee9fc99eb35f6d4026ab9.png)'
  id: totrans-1095
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/59c1bc79768ee9fc99eb35f6d4026ab9.png)'
- en: 'Figure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the
    context of Reinforcement Learning from Human Feedback (RLHF) for fine-tuning a
    Large Language Model (LLM). The process involves using a prompt dataset to train
    the LLM. The PPO algorithm adjusts the LLM’s policy based on rewards provided
    by the reward model, which is fine-tuned through human feedback. (adapted from
    [[73](#bib.bib73)])'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：近端策略优化（PPO）在来自人类反馈的强化学习（RLHF）上下文中应用于微调大型语言模型（LLM）的示意图。该过程涉及使用提示数据集来训练LLM。PPO算法根据奖励模型提供的奖励调整LLM的策略，奖励模型通过人类反馈进行微调。（改编自[[73](#bib.bib73)]）
- en: Python Library - HuggingFace Transformer Reinforcement Learning (TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index))
    package supports the PPO Trainer⁵⁵5[https://huggingface.co/docs/trl/main/en/ppo_trainer](https://huggingface.co/docs/trl/main/en/ppo_trainer)
    for training language models from the preference data.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: Python库 - HuggingFace Transformer 强化学习（TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)）包支持PPO训练器⁵⁵5[https://huggingface.co/docs/trl/main/en/ppo_trainer](https://huggingface.co/docs/trl/main/en/ppo_trainer)用于从偏好数据中训练语言模型。
- en: The PPOTrainer expects to align a generated response with a query given the
    rewards obtained from the Reward model. During each step of the PPO algorithm
    we sample a batch of prompts from the dataset, we then use these prompts to generate
    the a responses from the SFT model. Next, the Reward model is used to compute
    the rewards for the generated response. Finally, these rewards are used to optimise
    the SFT model using the PPO algorithm. Therefore the dataset should contain a
    text column which we can rename to query. Each of the other data-points required
    to optimise the SFT model are obtained during the training loop.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: PPOTrainer期望根据从奖励模型获得的奖励，使生成的响应与查询对齐。在PPO算法的每一步中，我们从数据集中采样一批提示，然后使用这些提示从SFT模型生成响应。接下来，使用奖励模型计算生成响应的奖励。最后，这些奖励用于通过PPO算法优化SFT模型。因此，数据集应包含一个文本列，我们可以将其重命名为查询。优化SFT模型所需的其他数据点在训练循环中获得。
- en: 6.8.1 Benefits of PPO
  id: totrans-1099
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.1 PPO的好处
- en: '1.'
  id: totrans-1100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable
    and reliable policy updates. The clipped surrogate objective function is central
    to this stability, as it limits policy updates to prevent large, potentially destabilising
    changes. This results in smoother and more consistent learning.'
  id: totrans-1101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性：近端策略优化（PPO）旨在确保稳定可靠的策略更新。裁剪的代理目标函数是这种稳定性的核心，因为它限制策略更新以防止大规模、潜在的不稳定变化。这导致学习过程更加平稳和一致。
- en: '2.'
  id: totrans-1102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively
    straightforward to implement. It avoids the need for second-order optimisation
    techniques, making it more accessible to less experienced practitioners.'
  id: totrans-1103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现的简便性：与高级算法TRPO相比，PPO相对容易实现。它避免了二阶优化技术，使得经验不足的实践者也能更容易上手。
- en: '3.'
  id: totrans-1104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Sample Efficiency: PPO achieves data efficiency through its use of the clipped
    surrogate objective. This mechanism regulates policy updates, ensuring stability
    while effectively reusing training data. Consequently, PPO tends to be more sample-efficient
    than other reinforcement learning algorithms, performing well with fewer samples,
    which is advantageous in scenarios where data collection is costly or time-consuming.'
  id: totrans-1105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例效率：PPO通过使用裁剪的代理目标实现了数据效率。这个机制调节策略更新，确保稳定性，同时有效地重用训练数据。因此，PPO往往比其他强化学习算法更具样本效率，在样本较少的情况下表现良好，这在数据收集成本高或耗时的场景中尤为有利。
- en: 6.8.2 Limitations of PPO
  id: totrans-1106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.2 PPO的局限性
- en: '1.'
  id: totrans-1107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves
    intricate policy and value networks, necessitating substantial computational resources
    for training. This complexity often results in extended training durations and
    increased operational expenses.'
  id: totrans-1108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂性和计算成本：近端策略优化（PPO）涉及复杂的策略和价值网络，需要大量计算资源进行训练。这种复杂性通常导致训练时间延长和运营成本增加。
- en: '2.'
  id: totrans-1109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Hyperparameter Sensitivity: PPO’s performance is highly dependent on several
    hyperparameters, such as the clipping range, learning rate, and discount factor.
    Achieving optimal performance requires meticulous tuning of these parameters.
    Incorrect settings can lead to suboptimal policy outcomes or instability during
    the learning process.'
  id: totrans-1110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数敏感性：PPO的性能高度依赖于几个超参数，如裁剪范围、学习率和折扣因子。要实现最佳性能，需要对这些参数进行精细调整。设置不正确可能导致策略结果不理想或学习过程中的不稳定。
- en: '3.'
  id: totrans-1111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Stability and Convergence Issues: Although PPO is designed to enhance stability
    compared to earlier methods, it can still encounter convergence issues, particularly
    in highly dynamic or complex environments. Maintaining stable policy updates remains
    a significant challenge.'
  id: totrans-1112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性和收敛性问题：尽管PPO设计上旨在增强稳定性，但在高度动态或复杂的环境中，它仍可能遇到收敛性问题。保持稳定的策略更新仍然是一个重大挑战。
- en: '4.'
  id: totrans-1113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Reward Signal Dependence: PPO’s effectiveness is heavily reliant on a well-defined
    reward signal to guide the learning process. In scenarios where designing an appropriate
    reward function is challenging or impractical, PPO may struggle to attain the
    desired results.'
  id: totrans-1114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励信号依赖：PPO的有效性在很大程度上依赖于一个明确定义的奖励信号来指导学习过程。在设计适当的奖励函数具有挑战性或不切实际的情况下，PPO可能难以达到预期的结果。
- en: 6.8.3 Tutorial for training models using PPO technique
  id: totrans-1115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.3 使用PPO技术训练模型的教程
- en: The tutorial for tuning GPT2 to generate positive movie reviews based on the
    IMDB dataset using PPO technique can be found [here.](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PPO 技术基于 IMDB 数据集对 GPT2 进行正面电影评论生成调优的教程可以在 [此处](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb)
    查找。
- en: 6.9 Direct Preference Optimisation (DPO)
  id: totrans-1117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9 直接偏好优化（DPO）
- en: Direct Preference Optimisation (DPO) [[74](#bib.bib74)] offers a streamlined
    approach to aligning language models (LMs) with human preferences, bypassing the
    complexity of reinforcement learning from human feedback (RLHF). Large-scale unsupervised
    LMs typically lack precise behavioural control, necessitating methods like RLHF
    that fine-tune models using human feedback. However, RLHF is intricate, involving
    the creation of reward models and the fine-tuning of LMs to maximise estimated
    rewards, which can be unstable and computationally demanding. DPO addresses these
    challenges by directly optimising LMs with a simple classification objective that
    aligns responses with human preferences. This approach eliminates the need for
    explicit reward modelling and extensive hyperparameter tuning, enhancing stability
    and efficiency. DPO optimises the desired behaviours by increasing the relative
    likelihood of preferred responses while incorporating dynamic importance weights
    to prevent model degeneration. Thus, DPO simplifies the preference learning pipeline,
    making it an effective method for training LMs to adhere to human preferences.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化（DPO）[[74](#bib.bib74)] 提供了一种简化的方法，将语言模型（LMs）与人类偏好对齐，绕过了从人类反馈中进行强化学习（RLHF）的复杂性。大规模无监督的
    LMs 通常缺乏精确的行为控制，因此需要像 RLHF 这样的方式，通过人类反馈微调模型。然而，RLHF 复杂，包括奖励模型的创建以及 LMs 的微调，以最大化估计奖励，这可能不稳定且计算量大。DPO
    通过直接优化 LMs，采用简单的分类目标，使响应与人类偏好对齐，从而解决这些挑战。这种方法消除了显式奖励建模和广泛的超参数调整需求，提高了稳定性和效率。DPO
    通过增加偏好响应的相对可能性并结合动态重要性权重来防止模型退化，从而优化期望的行为。因此，DPO 简化了偏好学习管道，使其成为训练 LMs 以符合人类偏好的有效方法。
- en: '![Refer to caption](img/06654e643ba623d84aa22f7cf2bc6a18.png)'
  id: totrans-1119
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/06654e643ba623d84aa22f7cf2bc6a18.png)'
- en: 'Figure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure
    illustrates the Direct Preference Optimisation (DPO) technique used in fine-tuning
    large language models. The process begins with preference data ([caption]
    , with the instruction prompt randomly selected from a pool of four
    candidates, such as “Briefly describe this image.” During training, only the linear
    projection layer and the LoRA layer in the LLM are fine-tuned, while other parts
    of the model remain frozen.'
  id: totrans-1937
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有图像描述的微调：在此阶段，模型使用ROCO医疗图像-描述数据集进行微调，该数据集包含长度不同的医疗图像-描述对。使用的提示模板为[caption]
    ，指令提示从四个候选项中随机选择，例如“简要描述此图像。”在训练过程中，仅微调LLM中的线性投影层和LoRA层，而模型的其他部分保持不变。
- en: •
  id: totrans-1938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA
    dataset, VQA-RAD, which contains triplets of images, questions, and answers. Following
    the instruction template proposed in MiniGPT-v2, the template used is: “[INST]
    [VQA] Instruction [/INST]”, where the instruction prompt
    is: “Based on the image, respond to this question with a short answer: question,”
    with question signifying the question corresponding to the given medical image.
    The motivation for generating short answers is to validate against the existing
    labelled data in VQA-RAD, where the answers are typically short in both open-ended
    and closed-ended QA pairs. Similar to the first stage, the vision encoder and
    the LLM remain frozen while only the linear projection and LoRA layers in the
    LLM are updated.'
  id: totrans-1939
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在VQA上的微调：在第二阶段，模型在包含图像、问题和答案三元组的Med-VQA数据集VQA-RAD上进行微调。根据MiniGPT-v2提出的指令模板，使用的模板是：“[INST]
    [VQA] 指令 [/INST]”，其中指令提示为：“根据图像，用简短的回答回答这个问题：问题”，其中问题指的是与给定医疗图像对应的问题。生成简短答案的动机是为了验证现有的VQA-RAD标注数据，其中答案通常在开放性和封闭性问答对中都很简短。与第一阶段类似，视觉编码器和LLM保持不变，只更新LLM中的线性投影和LoRA层。
- en: 11.3 Applications of Multimodal models
  id: totrans-1940
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 多模态模型的应用
- en: '1.'
  id: totrans-1941
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Gesture Recognition - These models interpret and recognise human gestures, which
    is crucial for sign language translation. Multimodal models facilitate inclusive
    communication by processing gestures and converting them into text or speech.
  id: totrans-1942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手势识别 - 这些模型解读和识别人的手势，这对于手语翻译至关重要。多模态模型通过处理手势并将其转换为文本或语音，促进了包容性沟通。
- en: '2.'
  id: totrans-1943
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Video Summarisation - Multimodal models can summarise lengthy videos by extracting
    key visual and audio elements. This capability streamlines content consumption,
    enables efficient content browsing, and enhances video content management platforms.
  id: totrans-1944
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频总结 - 多模态模型可以通过提取关键视觉和音频元素来总结长视频。这一能力简化了内容消费、实现了高效的内容浏览，并增强了视频内容管理平台。
- en: '3.'
  id: totrans-1945
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: DALL-E is a notable example of multimodal AI that generates images from textual
    descriptions. This technology expands creative possibilities in content creation
    and visual storytelling, with applications in art, design, advertising, and more.
  id: totrans-1946
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DALL-E 是一个值得注意的多模态AI示例，它可以从文本描述中生成图像。这项技术扩展了内容创作和视觉讲故事的创意可能性，应用于艺术、设计、广告等领域。
- en: '4.'
  id: totrans-1947
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Educational Tools - Multimodal models enhance learning experiences by providing
    interactive educational content that responds to both visual and verbal cues from
    students. They are integral to adaptive learning platforms that adjust content
    and difficulty based on student performance and feedback.
  id: totrans-1948
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 教育工具 - 多模态模型通过提供响应学生视觉和语言提示的互动教育内容来增强学习体验。它们在适应性学习平台中不可或缺，这些平台根据学生的表现和反馈调整内容和难度。
- en: '5.'
  id: totrans-1949
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Virtual Assistants - Multimodal models power virtual assistants by understanding
    and responding to voice commands while processing visual data for comprehensive
    user interaction. They are essential for smart home automation, voice-controlled
    devices, and digital personal assistants.
  id: totrans-1950
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虚拟助手 - 多模态模型通过理解和响应语音命令，同时处理视觉数据以实现全面的用户互动，为虚拟助手提供动力。它们对于智能家居自动化、语音控制设备和数字个人助理至关重要。
- en: 11.4 Audio or Speech LLMs Or Large Audio Models
  id: totrans-1951
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4 音频或语音LLM或大型音频模型
- en: Audio or speech LLMs are models designed to understand and generate human language
    based on audio inputs. They have applications in speech recognition, text-to-speech
    conversion, and natural language understanding tasks. These models are typically
    pre-trained on large datasets to learn generic language patterns, which are then
    fine-tuned on specific tasks or domains to enhance performance.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 音频或语音LLM是旨在理解和生成基于音频输入的人类语言的模型。它们在语音识别、文本转语音转换和自然语言理解任务中具有应用。这些模型通常在大规模数据集上进行预训练，以学习通用语言模式，然后在特定任务或领域上进行微调，以提升性能。
- en: Audio and Speech Large Language Models (LLMs) represent a significant advancement
    in the integration of language processing with audio signals. These models leverage
    a robust Large Language Model as a foundational backbone, which is enhanced to
    handle multimodal data through the inclusion of custom audio tokens. This transformation
    allows the models to learn and operate within a shared multimodal space, where
    both text and audio signals can be effectively processed.
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音大型语言模型（LLM）代表了语言处理与音频信号整合的重大进展。这些模型利用强大的大型语言模型作为基础框架，通过包含自定义音频令牌来增强以处理多模态数据。这一转变使得模型能够在一个共享的多模态空间中学习和操作，在这里文本和音频信号都能有效地被处理。
- en: 'Unlike text, which is inherently discrete, audio signals are continuous and
    need to be discretized into manageable audio tokens. Techniques like HuBERT[[97](#bib.bib97)]
    and wav2vec[[98](#bib.bib98)] are employed for this purpose, converting audio
    into a tokenized format that the LLM can process alongside text. The model, typically
    autoregressive and decoder-based, is pre-trained using a combination of self-supervised
    tasks, such as predicting masked tokens in interleaved text and audio, and supervised
    fine-tuning for specific tasks like transcription or sentiment analysis. This
    capability to handle and generate audio and text simultaneously allows for a wide
    range of applications, from audio question answering to speech-based sentiment
    detection, making Audio and Speech LLMs a versatile tool in multimodal AI. The
    figure [11.4](#Ch11.F4 "Figure 11.4 ‣ 11.4 Audio or Speech LLMs Or Large Audio
    Models ‣ Chapter 11 Multimodal LLMs and their Fine-tuning ‣ The Ultimate Guide
    to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies,
    Research, Best Practices, Applied Research Challenges and Opportunities (Version
    1.0)") illustrates an example of a multimodal Audio LM architecture. In this setup,
    a prompt provides instructions in both text and audio formats. The audio is tokenized
    using an audio tokenizer. The multimodal model then combines these text and audio
    tokens and generates spoken speech through a vocoder (also known as a voice decoder).'
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本不同，文本本质上是离散的，而音频信号是连续的，需要离散化为可管理的音频标记。像 HuBERT[[97](#bib.bib97)] 和 wav2vec[[98](#bib.bib98)]
    这样的技术被用来将音频转换为大模型可以处理的标记化格式，与文本一起处理。该模型通常是自回归的和基于解码器的，通过自监督任务（如预测交错文本和音频中的掩蔽标记）以及监督微调（如转录或情感分析）进行预训练。这种同时处理和生成音频与文本的能力使得应用范围非常广泛，从音频问答到基于语音的情感检测，使得音频和语音大模型在多模态
    AI 中成为一个多才多艺的工具。图 [11.4](#Ch11.F4 "图 11.4 ‣ 11.4 音频或语音大模型或大型音频模型 ‣ 第 11 章 多模态大模型及其微调
    ‣ 从基础到突破的终极微调指南：技术、研究、最佳实践、应用研究挑战和机会的详尽回顾 (版本 1.0)") 说明了一个多模态音频语言模型架构的示例。在此设置中，提示以文本和音频格式提供指令。音频通过音频标记器进行标记化。多模态模型然后结合这些文本和音频标记，通过声码器（也称为语音解码器）生成语音。
- en: '![Refer to caption](img/eb896cee30b5a45918b541748e21af56.png)'
  id: totrans-1955
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/eb896cee30b5a45918b541748e21af56.png)'
- en: 'Figure 11.4: Multimodal Audio-Text Language Model architecture that integrates
    text and audio inputs for advanced multimodal processing. The architecture utilises
    text tokenizers and audio encoders/tokenizers to convert inputs into tokens, which
    are then processed by the audio-text LM. This model supports both discrete and
    continuous speech processing and enables tasks such as sentiment analysis and
    response generation in natural language. The audio tokens are further refined
    using a vocoder, while text tokens are detokenized to produce coherent text outputs
    (adapted from [[99](#bib.bib99)]).'
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：集成文本和音频输入以实现高级多模态处理的多模态音频-文本语言模型架构。该架构利用文本标记器和音频编码器/标记器将输入转换为标记，然后由音频-文本大模型处理。该模型支持离散和连续语音处理，并实现自然语言中的情感分析和响应生成等任务。音频标记通过声码器进一步精炼，而文本标记则被反标记化以生成连贯的文本输出（改编自
    [[99](#bib.bib99)]）。
- en: Audio and speech LLMs like AudioPaLM[[100](#bib.bib100)], AudioLM[[101](#bib.bib101)],
    and various adaptations of models like Whisper and LLaMA, integrate capabilities
    for understanding and generating audio data, including speech-to-text (STT), text-to-speech
    (TTS), and speech-to-speech (STS) translation. These models have shown that LLMs,
    initially designed for text, can be effectively adapted for audio tasks through
    sophisticated tokenization and fine-tuning techniques.
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音大模型（LLMs），如 AudioPaLM[[100](#bib.bib100)]、AudioLM[[101](#bib.bib101)]，以及
    Whisper 和 LLaMA 等模型的各种改编，整合了理解和生成音频数据的能力，包括语音转文本（STT）、文本转语音（TTS）和语音转语音（STS）翻译。这些模型表明，最初设计用于文本的大模型可以通过复杂的标记化和微调技术有效地适应音频任务。
- en: 11.4.1 Tokenization and Preprocessing
  id: totrans-1958
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.1 标记化和预处理
- en: A key aspect of adapting LLMs for audio is the tokenization of audio data into
    discrete representations that the model can process. For instance, AudioLM and
    AudioPaLM utilise a combination of acoustic and semantic tokens. Acoustic tokens
    capture the high-quality audio synthesis aspect, while semantic tokens help maintain
    long-term structural coherence in the generated audio. This dual-token approach
    allows the models to handle both the intricacies of audio waveforms and the semantic
    content of speech.
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 适应音频的 LLM 的一个关键方面是将音频数据分词为模型可以处理的离散表示。例如，AudioLM 和 AudioPaLM 利用声学和语义标记的组合。声学标记捕捉高质量音频合成方面，而语义标记帮助保持生成音频的长期结构一致性。这种双标记方法使模型能够处理音频波形的复杂性和语音的语义内容。
- en: 11.4.2 Fine-Tuning Techniques
  id: totrans-1960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.2 微调技术
- en: 'Fine-tuning audio and speech LLMs typically involve several key strategies:'
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 微调音频和语音 LLM 通常涉及几个关键策略：
- en: •
  id: totrans-1962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Full Parameter Fine-Tuning: This involves updating all the model’s parameters
    during fine-tuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters
    to adapt pre-trained text LLMs to various audio tasks, although this can be computationally
    expensive.'
  id: totrans-1963
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全参数微调：这涉及在微调期间更新模型的所有参数。例如，LauraGPT 和 SpeechGPT 微调所有参数，以适应各种音频任务，尽管这可能会耗费较大的计算资源。
- en: •
  id: totrans-1964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update
    only specific layers or modules of the model. This method significantly reduces
    computational requirements while still allowing effective adaptation. Models like
    Qwen-Audio leverage LoRA to fine-tune pre-trained components for enhanced performance
    on speech recognition tasks.'
  id: totrans-1965
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层特定微调：像 LoRA（低秩适配）这样的技术只更新模型的特定层或模块。这种方法显著减少了计算需求，同时仍能有效适应。像 Qwen-Audio 这样的模型利用
    LoRA 对预训练组件进行微调，以提高语音识别任务的性能。
- en: •
  id: totrans-1966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper
    encoder, freeze certain parts of the model (like the speech encoder) and only
    fine-tune a linear projector or specific adapters to align the speech and text
    modalities. This approach simplifies the training process and enhances efficiency[[102](#bib.bib102)].'
  id: totrans-1967
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于组件的微调：最近的模型，例如那些集成 Whisper 编码器的模型，会冻结模型的某些部分（如语音编码器），仅对线性投影器或特定适配器进行微调，以对齐语音和文本模态。这种方法简化了训练过程并提高了效率[[102](#bib.bib102)]。
- en: •
  id: totrans-1968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning,
    starting with a text-based pre-training phase, followed by fine-tuning on a mixture
    of tasks that include both text and audio data. This staged approach leverages
    the strengths of pre-trained text models while adapting them for multimodal tasks.'
  id: totrans-1969
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多阶段微调：像 AudioPaLM 这样的模型进行多阶段微调，首先是基于文本的预训练阶段，然后在包含文本和音频数据的任务混合上进行微调。这种分阶段的方法利用了预训练文本模型的优势，同时使其适应多模态任务。
- en: 11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)
  id: totrans-1970
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.3 微调 Whisper 以进行自动语音识别（ASR）
- en: Whisper¹¹1[https://openai.com/index/whisper/](https://openai.com/index/whisper/)
    is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed
    to convert spoken language into text. Built upon the powerful Transformer architecture,
    Whisper excels at capturing and transcribing diverse speech patterns across various
    languages and accents. Unlike traditional ASR models that require extensive labelled
    data, Whisper leverages a vast dataset and self-supervised learning, enabling
    it to perform robustly in noisy environments and handle a wide range of speech
    variations. Its versatility and high accuracy make it an ideal choice for applications
    such as voice assistants, transcription services, and multilingual speech recognition
    systems.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper¹¹1[https://openai.com/index/whisper/](https://openai.com/index/whisper/)
    是一个由 OpenAI 开发的先进自动语音识别（ASR）模型，旨在将口语转化为文本。基于强大的 Transformer 架构，Whisper 擅长捕捉和转录各种语言和口音的多样化语音模式。与需要大量标注数据的传统
    ASR 模型不同，Whisper 利用广泛的数据集和自监督学习，使其能够在噪声环境中稳定表现，并处理各种语音变体。它的多功能性和高准确性使其成为语音助手、转录服务和多语言语音识别系统等应用的理想选择。
- en: Why Fine-Tune Whisper?
  id: totrans-1972
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么微调 Whisper？
- en: Fine-tuning Whisper for specific ASR tasks can significantly enhance its performance
    in specialised domains. Although Whisper is pre-trained on a large and diverse
    dataset, it might not fully capture the nuances of specific vocabularies or accents
    present in niche applications. Fine-tuning allows Whisper to adapt to particular
    audio characteristics and terminologies, leading to more accurate and reliable
    transcriptions. This process is especially beneficial in industries with domain-specific
    jargon, like medical, legal, or technical fields, where the generic model might
    struggle with specialised vocabulary.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 对特定ASR任务进行Whisper的微调可以显著提升其在专业领域的表现。尽管Whisper已经在一个大规模且多样化的数据集上进行了预训练，但它可能未能完全捕捉到特定应用中的词汇或口音的细微差别。微调使Whisper能够适应特定的音频特征和术语，从而提供更准确和可靠的转录结果。这个过程在有领域特定术语的行业中尤其有益，例如医疗、法律或技术领域，其中通用模型可能会对专业词汇感到困难。
- en: Steps to Fine-Tune Whisper
  id: totrans-1974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调Whisper的步骤
- en: •
  id: totrans-1975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Collection and Preparation: Gather a sizable dataset that matches the
    target domain or task. Ensure the dataset includes diverse examples with clear
    transcriptions. Clean and preprocess the audio files and transcripts, ensuring
    they are in a consistent format and aligned correctly. Tools like FFmpeg²²2[https://ffmpeg.org/ffmpeg.html](https://ffmpeg.org/ffmpeg.html)
    can help standardise audio formats and sample rates.'
  id: totrans-1976
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集和准备：收集一个与目标领域或任务相匹配的足够大的数据集。确保数据集中包括具有清晰转录的多样化示例。清理和预处理音频文件和转录，确保它们格式一致并正确对齐。像FFmpeg²²2[https://ffmpeg.org/ffmpeg.html](https://ffmpeg.org/ffmpeg.html)这样的工具可以帮助标准化音频格式和采样率。
- en: •
  id: totrans-1977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Augmentation: To improve robustness, augment the dataset with variations
    such as different noise levels, accents, or speeds. Techniques like adding background
    noise, altering pitch, or changing the tempo can help the model generalise better
    to real-world conditions.'
  id: totrans-1978
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据增强：为了提高鲁棒性，用不同的噪声水平、口音或速度对数据集进行增强。添加背景噪声、改变音调或改变节奏等技术可以帮助模型更好地适应实际条件。
- en: •
  id: totrans-1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Preprocessing: Convert the audio files into a format suitable for Whisper,
    typically into mel spectrograms or another time-frequency representation. This
    transformation is crucial as Whisper relies on such representations to learn and
    transcribe speech effectively.'
  id: totrans-1980
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预处理：将音频文件转换为适合Whisper的格式，通常是mel频谱图或其他时频表示。这一转化是至关重要的，因为Whisper依赖这些表示来有效地学习和转录语音。
- en: •
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Configuration: Initialise the Whisper model with pre-trained weights.
    Configure the model to accommodate the target language or domain-specific adjustments.
    This includes setting appropriate hyperparameters, like learning rate and batch
    size, tailored to the dataset’s size and complexity.'
  id: totrans-1982
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型配置：用预训练权重初始化Whisper模型。配置模型以适应目标语言或领域特定的调整。这包括设置适当的超参数，如学习率和批次大小，这些都应针对数据集的大小和复杂性量身定制。
- en: •
  id: totrans-1983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training: Fine-tune the Whisper model on the prepared dataset using a framework
    like PyTorch or TensorFlow. Ensure to monitor the model’s performance on a validation
    set to avoid overfitting. Techniques like gradient clipping, learning rate scheduling,
    and early stopping can help maintain training stability and efficiency.'
  id: totrans-1984
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练：使用像PyTorch或TensorFlow这样的框架对准备好的数据集进行Whisper模型的微调。确保监控模型在验证集上的表现，以避免过拟合。像梯度裁剪、学习率调度和早停等技术可以帮助保持训练的稳定性和效率。
- en: •
  id: totrans-1985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evaluation and Testing: After training, evaluate the model’s performance on
    a separate test set to assess its accuracy and generalisability. Metrics like
    Word Error Rate (WER) or Character Error Rate (CER) provide insights into how
    well the model transcribes audio compared to ground truth transcriptions.'
  id: totrans-1986
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估和测试：训练后，在一个单独的测试集上评估模型的表现，以评估其准确性和泛化能力。像词错误率（WER）或字符错误率（CER）这样的指标提供了模型转录音频相对于真实转录的效果。
- en: 11.4.4 Case Studies and Applications
  id: totrans-1987
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.4 案例研究和应用
- en: '1.'
  id: totrans-1988
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant
    improvements in transcribing doctor-patient interactions. Models like Whisper
    have been fine-tuned on medical terminologies, resulting in more accurate and
    reliable transcriptions.'
  id: totrans-1989
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗转录：在医疗数据上对语音LLM进行微调，已显著改善了医生与患者互动的转录。像Whisper这样的模型在医疗术语上进行了微调，结果是更准确和可靠的转录。
- en: '2.'
  id: totrans-1990
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Legal Document Processing: Legal firms have employed fine-tuned audio LLMs
    to transcribe court proceedings and legal discussions. Domain-specific fine-tuning
    has enhanced the models’ ability to recognise and accurately transcribe legal
    jargon.'
  id: totrans-1991
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法律文档处理：法律事务所已经采用微调的音频 LLMs 来转录法庭程序和法律讨论。领域特定的微调提高了模型识别和准确转录法律术语的能力。
- en: '3.'
  id: totrans-1992
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Customer Service Automation: Companies are using fine-tuned speech models to
    automate customer service interactions. These models are trained on customer support
    data to understand and respond to queries more effectively, providing a more seamless
    user experience.'
  id: totrans-1993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 客户服务自动化：公司正在使用微调的语音模型来自动化客户服务交互。这些模型通过对客户支持数据的训练来理解和回应查询，从而提供更加无缝的用户体验。
- en: Chapter 12 Open Challenges and Research Directions
  id: totrans-1994
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 12 章 开放挑战与研究方向
- en: 12.1 Scalability Issues
  id: totrans-1995
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 可扩展性问题
- en: 'The fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM¹¹1[https://ai.google/discover/palm2/](https://ai.google/discover/palm2/)
    , and T5²²2[https://huggingface.co/docs/transformers/en/model_doc/t5](https://huggingface.co/docs/transformers/en/model_doc/t5)
    has become a critical area of research, presenting several significant challenges
    and opening up new avenues for exploration, particularly in scaling these processes
    efficiently. This discussion focuses on the two main aspects: the challenges in
    scaling fine-tuning processes and potential research directions for scalable solutions.'
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的微调，如 GPT-4、PaLM¹¹1[https://ai.google/discover/palm2/](https://ai.google/discover/palm2/)
    和 T5²²2[https://huggingface.co/docs/transformers/en/model_doc/t5](https://huggingface.co/docs/transformers/en/model_doc/t5)，已成为一个关键的研究领域，面临几个重要挑战，并为高效扩展这些过程开辟了新的研究方向。本讨论集中在两个主要方面：扩展微调过程中的挑战以及可扩展解决方案的潜在研究方向。
- en: 12.1.1 Challenges in Scaling Fine-Tuning Processes
  id: totrans-1997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.1 扩展微调过程中的挑战
- en: '1.'
  id: totrans-1998
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Computational Resources: Large-scale models such as GPT-3 and PaLM require
    enormous computational resources for fine-tuning. For instance, fine-tuning a
    175-billion parameter model like GPT-3 necessitates high-performance GPUs or TPUs
    capable of handling vast amounts of data and complex operations. The sheer volume
    of parameters translates to extensive computational demands. Even a relatively
    smaller model, such as BERT-large with 340 million parameters, can be computationally
    intensive to fine-tune.'
  id: totrans-1999
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算资源：像 GPT-3 和 PaLM 这样的规模较大的模型需要巨大的计算资源进行微调。例如，微调一个拥有 1750 亿参数的模型，如 GPT-3，需要高性能的
    GPU 或 TPU 来处理大量数据和复杂操作。参数的庞大数量转化为广泛的计算需求。即使是相对较小的模型，如参数为 3.4 亿的 BERT-large，微调也可能计算密集。
- en: '2.'
  id: totrans-2000
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Memory Requirements: The memory footprint for fine-tuning LLMs is staggering.
    Each parameter in the model requires storage, and during training, additional
    memory is needed to store intermediate computations, gradients, and optimiser
    states. For example, loading a 7 billion parameter model (e.g., LLaMA 2) in FP32
    (4 bytes per parameter) requires approximately 28 GB of GPU memory, while fine-tuning
    demands around 112 GB of GPU memory[[103](#bib.bib103)]. This memory demand is
    beyond the capability of most consumer-grade hardware, making fine-tuning accessible
    primarily to well-funded organisations or research institutions.'
  id: totrans-2001
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存需求：微调 LLMs 的内存占用令人惊叹。模型中的每个参数都需要存储，并且在训练过程中，还需要额外的内存来存储中间计算、梯度和优化器状态。例如，加载一个
    70 亿参数的模型（如 LLaMA 2）在 FP32（每个参数 4 字节）中大约需要 28 GB 的 GPU 内存，而微调则需要大约 112 GB 的 GPU
    内存[[103](#bib.bib103)]。这种内存需求超出了大多数消费者级硬件的能力，使得微调主要对资金充裕的组织或研究机构可及。
- en: '3.'
  id: totrans-2002
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Data Volume: LLMs typically require vast amounts of training data to achieve
    state-of-the-art performance during fine-tuning. This data needs to be loaded,
    preprocessed, and fed into the model at high speeds to maintain efficient training.
    Managing large datasets can become a bottleneck, especially if the data is stored
    in a distributed fashion across multiple systems or if it needs to be fetched
    from remote storage.'
  id: totrans-2003
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据量：LLMs 通常需要大量的训练数据以在微调过程中实现最先进的性能。这些数据需要以高速加载、预处理并输入模型，以保持高效训练。管理大型数据集可能成为瓶颈，特别是当数据分布在多个系统中存储或需要从远程存储中获取时。
- en: '4.'
  id: totrans-2004
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs
    fully utilised. However, data pipelines can become bottlenecks if not properly
    optimised. For example, shuffling large datasets or loading them into memory quickly
    enough to keep up with the training process can be challenging. Techniques like
    data packing, where multiple small examples are combined into larger batches,
    help improve throughput but add complexity to data handling routines.[[104](#bib.bib104)]'
  id: totrans-2005
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 吞吐量和瓶颈：高吞吐量对保持GPU或TPU的充分利用至关重要。然而，数据管道如果没有得到适当优化，可能会成为瓶颈。例如，将大型数据集打乱或快速加载到内存中以跟上训练过程可能会面临挑战。数据打包等技术，将多个小示例合并为更大的批次，有助于提高吞吐量，但也增加了数据处理的复杂性。[[104](#bib.bib104)]
- en: '5.'
  id: totrans-2006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Efficient Use of Resources: The financial and environmental costs of fine-tuning
    large models are significant. Large-scale fine-tuning involves not just the direct
    cost of computational resources but also the indirect costs associated with energy
    consumption and infrastructure maintenance. Techniques such as mixed-precision
    training and gradient checkpointing can reduce these costs by optimising memory
    and computational efficiency.'
  id: totrans-2007
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源的高效使用：微调大型模型的财务和环境成本是显著的。大规模微调不仅涉及计算资源的直接成本，还包括与能源消耗和基础设施维护相关的间接成本。混合精度训练和梯度检查点等技术可以通过优化内存和计算效率来降低这些成本。
- en: The challenges in scaling the fine-tuning processes of LLMs are multifaceted
    and complex, involving significant computational, memory, and data handling constraints.
    Innovations in PEFT, data throughput optimisation, and resource-efficient training
    methods are critical for overcoming these challenges. As LLMs continue to grow
    in size and capability, addressing these challenges will be essential for making
    advanced AI accessible and practical for a wider range of applications.
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM的挑战是多方面且复杂的，涉及到显著的计算、内存和数据处理约束。在PEFT、数据吞吐量优化和资源高效训练方法方面的创新对于克服这些挑战至关重要。随着LLM的规模和能力不断增长，解决这些挑战对于使先进AI对更广泛的应用变得可及和实用将是关键的。
- en: 12.1.2 Research Directions for Scalable Solutions
  id: totrans-2009
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.2 可扩展解决方案的研究方向
- en: Advanced PEFT Techniques and Sparse Fine-Tuning
  id: totrans-2010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高级PEFT技术和稀疏微调
- en: Recent advancements in PEFT techniques, like LoRA and its variant, Quantised
    LoRA, are revolutionising the scalability of LLMs. LoRA reduces the computational
    burden by updating only a low-rank approximation of the parameters, significantly
    lowering memory and processing requirements. Quantised LoRA further optimises
    resource usage by applying quantisation to these low-rank matrices, maintaining
    high model performance while minimising the need for extensive hardware. This
    has enabled efficient fine-tuning of massive models, such as in Meta’s LLaMA project,
    where adapting a smaller set of influential parameters allowed the models to perform
    robustly across various tasks with less computational strain.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在PEFT技术方面的进展，如LoRA及其变体Quantised LoRA，正在彻底改变LLM的可扩展性。LoRA通过仅更新参数的低秩近似来降低计算负担，显著降低内存和处理要求。Quantised
    LoRA通过对这些低秩矩阵进行量化，进一步优化了资源使用，在保持模型性能的同时最小化了对大量硬件的需求。这使得对大型模型进行高效微调成为可能，例如在Meta的LLaMA项目中，调整一小部分有影响力的参数使模型在各种任务中表现出色，同时减少了计算压力。
- en: Sparse fine-tuning techniques, such as SpIEL [[105](#bib.bib105)] complement
    these efforts by selectively updating only the most impactful parameters. SpIEL
    fine-tunes models by only changing a small portion of the parameters, which it
    tracks with an index. The process includes updating the parameters, removing the
    least important ones, and adding new ones based on their gradients or estimated
    momentum using an efficient optimiser.
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏微调技术，如SpIEL [[105](#bib.bib105)]，通过选择性地更新最具影响力的参数来补充这些努力。SpIEL通过仅更改小部分参数（用索引跟踪）来微调模型。该过程包括更新参数、删除不重要的参数，并根据梯度或估计的动量使用高效的优化器添加新参数。
- en: Data Efficient Fine-Tuning (DEFT)
  id: totrans-2013
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据高效微调（DEFT）
- en: To address the scalability challenges, recently the concept of DEFT has emerged.
    This novel approach introduces data pruning as a mechanism to optimise the fine-tuning
    process by focusing on the most critical data samples.
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对可扩展性挑战，最近出现了DEFT的概念。这种新颖的方法引入了数据剪枝作为优化微调过程的机制，专注于最关键的数据样本。
- en: DEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by
    selectively pruning the training data to identify the most influential and representative
    samples. This method leverages few-shot learning principles, enabling LLMs to
    adapt to new data with minimal samples while maintaining or even exceeding performance
    levels achieved with full datasets [[106](#bib.bib106)].
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: DEFT 旨在通过选择性地剪枝训练数据以识别最具影响力和代表性的样本，从而提高 LLM 微调的效率和效果。这种方法利用了少样本学习原理，使 LLM 能够在仅有少量样本的情况下适应新数据，同时保持或甚至超越使用完整数据集时所达到的性能水平[[106](#bib.bib106)]。
- en: Key Components of DEFT
  id: totrans-2016
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DEFT 的关键组成部分
- en: 'High Accuracy Through Influence Score: DEFT introduces the concept of an influence
    score to evaluate and rank the importance of each data sample in the context of
    LLM fine-tuning. The influence score estimates how removing a specific sample
    would impact the overall performance of the model. This approach allows for the
    selection of a small subset of data that is highly representative and influential,
    thereby enabling the model to maintain high accuracy with significantly fewer
    samples.'
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 通过影响分数实现高准确度：DEFT 引入了影响分数的概念，以评估和排名每个数据样本在 LLM 微调过程中的重要性。影响分数估算了去除特定样本会对模型整体性能的影响。这种方法允许选择一个具有高度代表性和影响力的小数据子集，从而使模型在使用显著较少的样本的情况下保持高准确度。
- en: 'High Efficiency Through Effort Score and Surrogate Models: To address the cost
    and complexity of evaluating large datasets, DEFT employs a surrogate model—a
    smaller, computationally less intensive model—to approximate the influence scores.
    This surrogate model helps estimate the impact of each sample without the heavy
    computational burden associated with directly using the LLM. Additionally, DEFT
    introduces an effort score to identify and prioritise more challenging samples
    that may require special attention from the LLM. This dual-score system ensures
    that the fine-tuning process remains both efficient and effective.'
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: 通过努力分数和代理模型实现高效能：为了应对评估大数据集的成本和复杂性，DEFT 使用了一个代理模型——一个计算负担较小的模型——来近似影响分数。这个代理模型有助于估算每个样本的影响，而无需直接使用大型语言模型（LLM）所带来的高计算负担。此外，DEFT
    引入了一个努力分数，以识别和优先考虑可能需要 LLM 特别关注的更具挑战性的样本。这个双重分数系统确保了微调过程既高效又有效。
- en: Practical Implications and Use Cases
  id: totrans-2019
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实际应用与使用案例
- en: •
  id: totrans-2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial
    for applications where models need to quickly adapt to new data with minimal samples.
    In scenarios such as personalised recommendations or adapting to sudden changes
    in user behaviour, DEFT allows for rapid fine-tuning, maintaining high performance
    with a fraction of the data typically required.'
  id: totrans-2021
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少样本微调以实现快速适应：DEFT 对于需要模型快速适应新数据且样本较少的应用特别有益。在个性化推荐或适应用户行为突变等场景中，DEFT 允许快速微调，维持高性能的同时，所需的数据量仅为通常所需的一部分。
- en: •
  id: totrans-2022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reducing Computational Costs in Large-Scale Deployments: By focusing on the
    most influential data samples and using surrogate models, DEFT significantly reduces
    the computational resources needed for fine-tuning. This makes it feasible to
    maintain high-performing LLMs even in large-scale deployments where data volumes
    are substantial.'
  id: totrans-2023
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大规模部署中降低计算成本：通过关注最具影响力的数据样本和使用代理模型，DEFT 显著减少了进行微调所需的计算资源。这使得即使在数据量庞大的大规模部署中，也能保持高性能的
    LLM。
- en: Future Directions
  id: totrans-2024
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未来方向
- en: 'The DEFT introduces a data pruning task for fine-tuning large language models
    (LLMs), setting the stage for new research into efficient LLM-based recommendation
    systems and presenting numerous opportunities for future exploration. Key areas
    for further investigation include:'
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: DEFT 引入了一个数据剪枝任务用于微调大型语言模型（LLMs），为基于 LLM 的推荐系统的高效研究奠定了基础，并为未来的探索提供了许多机会。进一步研究的关键领域包括：
- en: •
  id: totrans-2026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Applying the proposed DEALRec[[107](#bib.bib107)] approach to a broader range
    of LLM-based recommender models across diverse cross-domain datasets, thereby
    enhancing fine-tuning performance within resource constraints.
  id: totrans-2027
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将提出的 DEALRec[[107](#bib.bib107)] 方法应用于更广泛的 LLM 基于推荐模型的跨领域数据集，从而在资源限制下提高微调性能。
- en: •
  id: totrans-2028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Addressing the limited context window of LLMs by selectively focusing on the
    most informative items in user interaction sequences for fine-tuning purposes.
  id: totrans-2029
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过选择性地关注用户交互序列中最具信息量的项来解决 LLM 的有限上下文窗口问题，以进行微调。
- en: 12.1.3 Hardware and Algorithm Co-Design
  id: totrans-2030
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.1.3 硬件与算法协同设计
- en: Co-designing hardware and algorithms tailored for LLMs can lead to significant
    improvements in the efficiency of fine-tuning processes. Custom hardware accelerators
    optimised for specific tasks or types of computation can drastically reduce the
    energy and time required for model training and fine-tuning.
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 为LLMs量身定制的硬件与算法的协同设计可以显著提高微调过程的效率。为特定任务或计算类型优化的定制硬件加速器可以大幅减少模型训练和微调所需的能源和时间。
- en: •
  id: totrans-2032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Custom Accelerators: Developing hardware accelerators specifically for the
    sparse and low-precision computations often used in LLM fine-tuning can enhance
    performance. These accelerators are designed to efficiently handle the unique
    requirements of LLMs, such as the high memory bandwidth and extensive matrix multiplications
    involved in transformer architectures.'
  id: totrans-2033
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定制加速器：开发专门用于LLM微调中稀疏和低精度计算的硬件加速器可以提高性能。这些加速器设计用于有效处理LLMs的独特要求，例如涉及到的高内存带宽和广泛的矩阵乘法。
- en: •
  id: totrans-2034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation
    techniques, such as those that minimise data movement or leverage hardware-specific
    features (e.g., tensor cores for mixed-precision calculations), can further enhance
    the efficiency of fine-tuning processes.'
  id: totrans-2035
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法优化：将硬件创新与算法优化技术相结合，例如那些减少数据传输或利用硬件特定功能（例如用于混合精度计算的张量核心），可以进一步提高微调过程的效率。
- en: •
  id: totrans-2036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example: NVIDIA’s TensorRT³³3[https://docs.nvidia.com/tensorrt/index.html](https://docs.nvidia.com/tensorrt/index.html)
    is an example of hardware and algorithm co-design in action. It optimises deep
    learning models for inference by leveraging NVIDIA GPUs’ capabilities, significantly
    speeding up the process while reducing the resource requirements. TensorRT’s optimisations
    include support for mixed-precision and sparse tensor operations, making it highly
    suitable for fine-tuning large models.'
  id: totrans-2037
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：NVIDIA的TensorRT³³3[https://docs.nvidia.com/tensorrt/index.html](https://docs.nvidia.com/tensorrt/index.html)
    是硬件与算法协同设计的一个实际例子。它通过利用NVIDIA GPU的能力来优化深度学习模型的推理过程，大幅加快了处理速度，同时减少了资源需求。TensorRT的优化包括对混合精度和稀疏张量操作的支持，使其非常适合用于大模型的微调。
- en: As the scale of language models continues to grow, addressing the challenges
    of fine-tuning them efficiently becomes increasingly critical. Innovations in
    PEFT, sparse fine-tuning, data handling, and the integration of advanced hardware
    and algorithmic solutions present promising directions for future research. These
    scalable solutions are essential not only to make the deployment of LLMs feasible
    for a broader range of applications but also to push the boundaries of what these
    models can achieve.
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型规模的不断扩大，如何高效地微调这些模型成为越来越重要的挑战。PEFT、稀疏微调、数据处理以及先进硬件和算法解决方案的创新为未来研究提供了有希望的方向。这些可扩展的解决方案不仅对于使LLM的部署适用于更广泛的应用至关重要，而且也推动了这些模型能实现的最终边界。
- en: 12.2 Ethical Considerations in Fine-Tuning LLMs
  id: totrans-2039
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 LLMs微调中的伦理考虑
- en: 12.2.1 Bias and Fairness
  id: totrans-2040
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.1 偏差与公平性
- en: When fine-tuning LLMs, the goal is often to optimise their performance for specific
    tasks or datasets. However, these datasets may inherently carry biases that get
    transferred to the model during the fine-tuning process. Biases can arise from
    various sources, including historical data, imbalanced training samples, and cultural
    prejudices embedded in language. For instance, an LLM fine-tuned on a dataset
    primarily sourced from English-speaking countries might underperform or make biased
    predictions when applied to text from other linguistic or cultural backgrounds.
    Google AI’s Fairness Indicators tool⁴⁴4[https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/](https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/)
    is a practical solution that allows developers to evaluate the fairness of their
    models by analysing performance metrics across different demographic groups. This
    tool can be integrated into the fine-tuning pipeline to monitor and address bias
    in real-time.
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs时，目标通常是优化模型在特定任务或数据集上的表现。然而，这些数据集可能固有地带有偏见，这些偏见会在微调过程中转移到模型中。偏见可能来源于各种渠道，包括历史数据、不平衡的训练样本以及语言中嵌入的文化偏见。例如，一个基于主要来自英语国家的数据集进行微调的LLM在应用于其他语言或文化背景的文本时，可能会表现不佳或做出有偏见的预测。谷歌AI的公平性指标工具⁴⁴4[https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/](https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/)
    是一个实用的解决方案，可以通过分析不同人口群体的性能指标来评估模型的公平性。该工具可以集成到微调流程中，以实时监控和处理偏见。
- en: Addressing Bias and Fairness
  id: totrans-2042
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理偏见和公平性
- en: •
  id: totrans-2043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse
    and representative of all user demographics can help mitigate bias.'
  id: totrans-2044
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样化和具有代表性的数据：确保微调数据集在所有用户人群中具有多样性和代表性，有助于减轻偏见。
- en: •
  id: totrans-2045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fairness Constraints: Incorporating fairness constraints, as suggested by the
    FairBERTa framework⁵⁵5[https://huggingface.co/facebook/FairBERTa](https://huggingface.co/facebook/FairBERTa),
    ensures that fine-tuned models maintain equitable performance across different
    groups.'
  id: totrans-2046
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 公平性约束：根据FairBERTa框架⁵⁵5[https://huggingface.co/facebook/FairBERTa](https://huggingface.co/facebook/FairBERTa)的建议，纳入公平性约束可以确保微调后的模型在不同群体中保持公平的表现。
- en: •
  id: totrans-2047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing
    conditions might initially be trained on data from predominantly white patients.
    Such a model could produce less accurate diagnoses for patients from other racial
    backgrounds. By using fairness-aware fine-tuning techniques, healthcare providers
    can develop models that perform more equitably across diverse patient populations.'
  id: totrans-2048
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在医疗领域，一个为了辅助诊断而微调的LLM可能最初是基于主要来自白人患者的数据进行训练的。这样的模型可能对其他种族背景的患者诊断不够准确。通过使用公平性意识的微调技术，医疗提供者可以开发在不同患者群体中表现更为公平的模型。
- en: 12.2.2 Privacy Concerns
  id: totrans-2049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.2 隐私问题
- en: Fine-tuning often involves using sensitive or proprietary datasets, which poses
    significant privacy risks. If not properly managed, fine-tuned models can inadvertently
    leak private information from their training data. This issue is especially critical
    in domains like healthcare or finance, where data confidentiality is paramount.
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: 微调通常涉及使用敏感或专有的数据集，这会带来显著的隐私风险。如果管理不当，微调后的模型可能会无意中泄露训练数据中的私人信息。这个问题在医疗或金融等数据保密性至关重要的领域尤为严重。
- en: Ensuring Privacy During Fine-Tuning
  id: totrans-2051
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 确保微调过程中的隐私
- en: •
  id: totrans-2052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Differential Privacy⁶⁶6[https://privacytools.seas.harvard.edu/differential-privacy](https://privacytools.seas.harvard.edu/differential-privacy):
    Implementing differential privacy techniques during fine-tuning can prevent models
    from leaking sensitive information.'
  id: totrans-2053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 差分隐私⁶⁶6[https://privacytools.seas.harvard.edu/differential-privacy](https://privacytools.seas.harvard.edu/differential-privacy)：在微调过程中实施差分隐私技术可以防止模型泄露敏感信息。
- en: •
  id: totrans-2054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Federated Learning⁷⁷7[https://research.ibm.com/blog/what-is-federated-learning](https://research.ibm.com/blog/what-is-federated-learning):
    Utilising federated learning frameworks allows models to be fine-tuned across
    decentralised data sources, which enhances privacy by keeping data localised.'
  id: totrans-2055
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦学习⁷⁷7[https://research.ibm.com/blog/what-is-federated-learning](https://research.ibm.com/blog/what-is-federated-learning)：利用联邦学习框架可以在分散的数据源上对模型进行微调，从而通过保持数据本地化来增强隐私保护。
- en: •
  id: totrans-2056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In customer service applications, companies might fine-tune
    LLMs using customer interaction data. Employing differential privacy ensures that
    the model learns from these interactions without memorising and potentially leaking
    personal information, thus maintaining customer confidentiality.'
  id: totrans-2057
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在客户服务应用中，公司可能使用客户互动数据对LLM进行微调。采用差分隐私技术确保模型从这些互动中学习，而不会记住或泄露个人信息，从而维护客户的机密性。
- en: 12.2.3 Security Risks
  id: totrans-2058
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.2.3 安全风险
- en: •
  id: totrans-2059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible
    to security vulnerabilities, particularly from adversarial attacks. These attacks
    involve inputs designed to exploit model weaknesses, causing them to produce erroneous
    or harmful outputs. Such vulnerabilities can be more pronounced in fine-tuned
    models due to their specialised training data, which may not cover all possible
    input scenarios.'
  id: totrans-2060
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调模型中的安全漏洞：微调的LLM易受安全漏洞的影响，特别是对抗性攻击。这些攻击涉及设计用于利用模型弱点的输入，导致模型产生错误或有害的输出。由于微调模型可能使用了专门的训练数据，这些数据可能无法涵盖所有可能的输入场景，因此这些漏洞可能更加明显。
- en: •
  id: totrans-2061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recent Research and Industry Practices: Microsoft’s Adversarial ML Threat Matrix
    provides a comprehensive framework for identifying and mitigating adversarial
    threats during model development and fine-tuning. This matrix helps developers
    understand the potential attack vectors and implement defensive strategies accordingly.'
  id: totrans-2062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新研究和行业实践：微软的对抗性ML威胁矩阵提供了一个全面的框架，用于识别和缓解模型开发和微调过程中可能遇到的对抗性威胁。该矩阵帮助开发者了解潜在的攻击向量，并相应地实施防御策略。
- en: •
  id: totrans-2063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhancing Security in Fine-Tuning:'
  id: totrans-2064
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升安全性在微调中的作用：
- en: –
  id: totrans-2065
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Adversarial Training: Exposing models to adversarial examples during fine-tuning
    can enhance their robustness against attacks.'
  id: totrans-2066
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性训练：在微调过程中将模型暴露于对抗样本可以提高其对攻击的鲁棒性。
- en: –
  id: totrans-2067
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Security Audits: Regularly conducting security audits on fine-tuned models
    can help identify and address potential vulnerabilities.'
  id: totrans-2068
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全审计：定期对微调模型进行安全审计可以帮助识别和解决潜在的漏洞。
- en: 12.3 Accountability and Transparency
  id: totrans-2069
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 问责制和透明度
- en: 12.3.1 The Need for Accountability and Transparency
  id: totrans-2070
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1 需要问责制和透明度
- en: Fine-tuning can significantly alter an LLM’s behaviour, making it crucial to
    document and understand the changes and their impacts. This transparency is essential
    for stakeholders to trust the model’s outputs and for developers to be accountable
    for its performance and ethical implications.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以显著改变LLM的行为，因此记录和理解这些变化及其影响至关重要。这种透明度对于利益相关者信任模型输出以及开发者对其性能和道德影响负责是必要的。
- en: 12.3.2 Recent Research and Industry Practices
  id: totrans-2072
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2 最新研究和行业实践
- en: Meta’s Responsible AI framework⁸⁸8[https://ai.meta.com/responsible-ai/](https://ai.meta.com/responsible-ai/)
    underscores the importance of documenting the fine-tuning process and its effects
    on model behaviour. This includes maintaining detailed records of the data used,
    the changes made during fine-tuning, and the evaluation metrics applied.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: '[Meta的负责任AI框架](https://ai.meta.com/responsible-ai/)强调了记录微调过程及其对模型行为影响的重要性。这包括维护所使用数据的详细记录、微调过程中所做的更改以及应用的评估指标。'
- en: 12.3.3 Promoting Accountability and Transparency
  id: totrans-2074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.3 促进问责制和透明度
- en: •
  id: totrans-2075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensive Documentation: Creating detailed documentation of the fine-tuning
    process and its impact on model performance and behaviour.'
  id: totrans-2076
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合文档：创建关于微调过程及其对模型性能和行为影响的详细文档。
- en: •
  id: totrans-2077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transparent Reporting: Utilising frameworks like Model Cards⁹⁹9[https://huggingface.co/docs/hub/en/model-cards](https://huggingface.co/docs/hub/en/model-cards)
    to report on the ethical and operational characteristics of fine-tuned models.'
  id: totrans-2078
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 透明报告：利用像[Model Cards](https://huggingface.co/docs/hub/en/model-cards)这样的框架报告微调模型的道德和操作特征。
- en: •
  id: totrans-2079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example Application: In content moderation systems, LLMs fine-tuned to identify
    and filter harmful content need clear documentation and reporting. This ensures
    that platform users and regulators understand how the model operates and can trust
    its moderation decisions.'
  id: totrans-2080
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例应用：在内容审核系统中，微调以识别和过滤有害内容的LLM需要清晰的文档和报告。这确保平台用户和监管机构理解模型的操作方式，并能信任其审核决定。
- en: 12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning
  id: totrans-2081
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.4 道德微调的建议框架/技术
- en: Frameworks for Mitigating Bias
  id: totrans-2082
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 减少偏见的框架
- en: Bias-aware fine-tuning frameworks aim to incorporate fairness into the model
    training process. FairBERTa, introduced by Facebook, is an example of such a framework
    that integrates fairness constraints directly into the model’s objective function
    during fine-tuning. This approach ensures that the model’s performance is balanced
    across different demographic groups.
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: 意识到偏见的微调框架旨在将公平性纳入模型训练过程中。由 Facebook 推出的 FairBERTa 就是一个这样的框架，它在微调过程中将公平性约束直接集成到模型的目标函数中。这种方法确保了模型在不同人口群体中的性能平衡。
- en: Organisations can adopt fairness-aware frameworks to develop more equitable
    AI systems. For instance, social media platforms can use these frameworks to fine-tune
    models that detect and mitigate hate speech while ensuring fair treatment across
    various user demographics.
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可以采用关注公平性的框架来开发更公平的 AI 系统。例如，社交媒体平台可以使用这些框架来微调检测和缓解仇恨言论的模型，同时确保对各种用户群体的公平对待。
- en: Techniques for Privacy Preservation
  id: totrans-2085
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 隐私保护技术
- en: Differential privacy and federated learning are key techniques for preserving
    privacy during fine-tuning. TensorFlow Privacy^(10)^(10)10[https://www.tensorflow.org/responsible_ai/privacy/guide](https://www.tensorflow.org/responsible_ai/privacy/guide),
    developed by Google, provides built-in support for differential privacy, allowing
    developers to fine-tune models securely without compromising data confidentiality.
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私和联邦学习是保护微调过程中的隐私的关键技术。Google 开发的 TensorFlow Privacy^(10)^(10)10[https://www.tensorflow.org/responsible_ai/privacy/guide](https://www.tensorflow.org/responsible_ai/privacy/guide)
    提供了内置的差分隐私支持，使开发者能够在不妨碍数据机密性的情况下安全地微调模型。
- en: LLMs are highly effective but face challenges when applied in sensitive areas
    where data privacy is crucial. To address this, researchers focus on enhancing
    Small Language Models (SLMs) tailored to specific domains. Existing methods often
    use LLMs to generate additional data or transfer knowledge to SLMs, but these
    approaches struggle due to differences between LLM-generated data and private
    client data. In response, a new Federated Domain-specific Knowledge Transfer (FDKT)[[108](#bib.bib108)]
    framework is introduced. FDKT leverages LLMs to create synthetic samples that
    mimic clients’ private data distribution using differential privacy. This approach
    significantly boosts SLMs’ performance by approximately 5% while maintaining data
    privacy with a minimal privacy budget, outperforming traditional methods relying
    solely on local private data.
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 非常有效，但在数据隐私至关重要的敏感领域应用时面临挑战。为了解决这一问题，研究人员专注于增强针对特定领域的小型语言模型（SLM）。现有方法通常使用
    LLM 生成额外数据或将知识转移到 SLM，但这些方法由于 LLM 生成的数据和私人客户数据之间的差异而面临困难。因此，引入了一种新的联邦领域特定知识转移（FDKT）[[108](#bib.bib108)]
    框架。FDKT 利用 LLM 创建模仿客户私人数据分布的合成样本，采用差分隐私方法。这种方法通过大约 5% 的提升显著提高了 SLM 的性能，同时以最小的隐私预算保持数据隐私，优于仅依赖本地私人数据的传统方法。
- en: In healthcare, federated fine-tuning can allow hospitals to collaboratively
    train models on patient data without transferring sensitive information. This
    approach ensures data privacy while enabling the development of robust, generalisable
    AI systems.
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健领域，联邦微调可以允许医院在不转移敏感信息的情况下协作训练模型。这种方法在确保数据隐私的同时，促进了稳健且具有广泛适应性的 AI 系统的开发。
- en: Frameworks for Enhancing Security
  id: totrans-2089
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强安全性的框架
- en: Adversarial training and robust security measures[[109](#bib.bib109)] are essential
    for protecting fine-tuned models against attacks. The adversarial training approach
    involves training models with adversarial examples to improve their resilience
    against malicious inputs. Microsoft Azure’s adversarial training tools provide
    practical solutions for integrating these techniques into the fine-tuning process,
    helping developers create more secure and reliable models.
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练和强健的安全措施[[109](#bib.bib109)] 对保护微调模型免受攻击至关重要。对抗训练方法包括使用对抗样本训练模型，以提高其抵御恶意输入的能力。Microsoft
    Azure 的对抗训练工具提供了将这些技术集成到微调过程中的实际解决方案，帮助开发者创建更安全可靠的模型。
- en: In cybersecurity, fine-tuned LLMs used for threat detection can benefit from
    adversarial training to enhance their ability to identify and respond to sophisticated
    attacks, thereby improving organisational security.
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全中，用于威胁检测的微调 LLM 可以通过对抗训练来提升其识别和响应复杂攻击的能力，从而改善组织的安全性。
- en: Frameworks for Ensuring Transparency
  id: totrans-2092
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 确保透明度的框架
- en: Transparency and accountability frameworks, such as Model Cards and AI FactSheets^(11)^(11)11[https://aifs360.res.ibm.com/](https://aifs360.res.ibm.com/),
    provide structured ways to document and report on the fine-tuning process and
    the resulting model behaviours. These frameworks promote understanding and trust
    among stakeholders by clearly outlining the model’s capabilities, limitations,
    and ethical considerations.
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: 透明性和问责制框架，例如模型卡和AI FactSheets^(11)^(11)11[https://aifs360.res.ibm.com/](https://aifs360.res.ibm.com/)，提供了结构化的方式来记录和报告微调过程及其结果模型行为。这些框架通过明确列出模型的能力、局限性和伦理考虑，促进了利益相关者之间的理解和信任。
- en: In government applications, where AI systems might be used for decision-making
    or public services, maintaining transparent documentation through frameworks like
    AI FactSheets ensures that these systems are accountable and their decisions can
    be audited and trusted by the public.
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: 在政府应用中，AI系统可能被用于决策或公共服务，通过像AI FactSheets这样的框架保持透明的文档记录，确保这些系统是有问责制的，其决策可以被公众审计和信任。
- en: Fine-tuning LLMs introduces several ethical challenges, including bias, privacy
    risks, security vulnerabilities, and accountability concerns. Addressing these
    requires a multifaceted approach that integrates fairness-aware frameworks, privacy-preserving
    techniques, robust security measures, and transparency and accountability mechanisms.
    By leveraging recent advancements in these areas, researchers and practitioners
    can develop and deploy LLMs that are not only powerful but also ethically sound
    and trustworthy.
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs引入了若干伦理挑战，包括偏见、隐私风险、安全漏洞和问责问题。解决这些问题需要多方面的方法，整合公平意识框架、隐私保护技术、强健的安全措施以及透明和问责机制。通过利用这些领域的最新进展，研究人员和实践者可以开发和部署不仅功能强大而且伦理上健全和值得信赖的LLMs。
- en: 12.4 Integration with Emerging Technologies
  id: totrans-2096
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4 与新兴技术的集成
- en: Integrating LLMs with emerging technologies such as IoT (Internet of Things)
    and edge computing presents numerous opportunities and challenges, reflecting
    advancements and insights from recent research and industry developments.
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLMs与物联网（IoT）等新兴技术和边缘计算集成，带来了许多机遇和挑战，反映了近期研究和行业发展的进展与见解。
- en: 12.4.1 Opportunities
  id: totrans-2098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.4.1 机会
- en: •
  id: totrans-2099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhanced Decision-Making and Automation: LLMs have the capability to analyse
    and derive insights from vast amounts of unstructured data generated by IoT devices.
    This data can range from sensor readings in manufacturing plants to environmental
    data in smart cities. By processing this data in real-time, LLMs can optimise
    decision-making processes and automate tasks that traditionally required human
    intervention. For example:'
  id: totrans-2100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强决策制定和自动化：LLMs能够分析和提取来自IoT设备生成的大量非结构化数据的洞察。这些数据可以包括制造工厂的传感器读数到智慧城市的环境数据。通过实时处理这些数据，LLMs可以优化决策过程并自动化传统上需要人工干预的任务。例如：
- en: –
  id: totrans-2101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing
    sensor data to predict equipment failures before they occur, thereby reducing
    downtime and maintenance costs.'
  id: totrans-2102
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工业应用：通过LLMs分析传感器数据来预测设备故障，从而减少停机时间和维护成本，可以提升预测性维护。
- en: –
  id: totrans-2103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Smart Cities: LLMs can analyse traffic patterns and environmental data from
    IoT sensors to optimise city infrastructure and improve urban planning decisions.'
  id: totrans-2104
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智慧城市：LLMs可以分析来自IoT传感器的交通模式和环境数据，以优化城市基础设施和改善城市规划决策。
- en: •
  id: totrans-2105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Personalised User Experiences: Integration with edge computing allows LLMs
    to process data locally on devices rather than relying solely on cloud-based servers.
    This enables LLMs to deliver highly personalised services based on real-time data
    and user preferences, enhancing user experiences across various domains:'
  id: totrans-2106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个性化用户体验：与边缘计算的集成使得大型语言模型（LLMs）能够在设备上本地处理数据，而不是仅仅依赖于基于云的服务器。这使得LLMs能够基于实时数据和用户偏好提供高度个性化的服务，提升各个领域的用户体验：
- en: –
  id: totrans-2107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Healthcare: LLMs can provide personalised healthcare recommendations by analysing
    data from wearable devices and integrating it with medical records securely stored
    on edge devices.'
  id: totrans-2108
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗保健：LLMs可以通过分析来自可穿戴设备的数据，并将其与安全存储在边缘设备上的医疗记录整合，从而提供个性化的医疗建议。
- en: •
  id: totrans-2109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Improved Natural Language Understanding: IoT data integration enriches LLMs’
    ability to understand context and respond more intelligently to natural language
    queries. This can significantly improve user interactions with smart environments:'
  id: totrans-2110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改进的自然语言理解：IoT 数据集成丰富了 LLM 理解上下文的能力，并能更智能地响应自然语言查询。这可以显著改善用户与智能环境的互动：
- en: –
  id: totrans-2111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Smart Homes: LLMs integrated with IoT devices can understand and respond to
    voice commands more accurately, adjusting smart home settings based on real-time
    sensor data (e.g., adjusting lighting and temperature based on occupancy and environmental
    conditions).'
  id: totrans-2112
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能家居：与 IoT 设备集成的 LLM 可以更准确地理解和响应语音命令，根据实时传感器数据调整智能家居设置（例如，根据占用情况和环境条件调整照明和温度）。
- en: 12.4.2 Challenges
  id: totrans-2113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.4.2 挑战
- en: •
  id: totrans-2114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Complexity and Integration: Integrating data from diverse IoT devices
    poses challenges related to data quality, interoperability, and scalability. LLMs
    need to effectively process and interpret this heterogeneous data to derive meaningful
    insights:'
  id: totrans-2115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据复杂性和集成：来自不同 IoT 设备的数据集成面临与数据质量、互操作性和可扩展性相关的挑战。LLM 需要有效处理和解释这些异构数据，以获得有意义的见解：
- en: –
  id: totrans-2116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Integration: Ensuring seamless integration of data streams from different
    IoT platforms and devices without compromising data integrity or performance.'
  id: totrans-2117
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集成：确保来自不同 IoT 平台和设备的数据流无缝集成，而不影响数据完整性或性能。
- en: –
  id: totrans-2118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency
    and reliability before feeding it into LLMs for analysis.'
  id: totrans-2119
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据预处理：清理和预处理 IoT 数据，以确保在将其输入 LLM 进行分析之前的一致性和可靠性。
- en: •
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Privacy and Security: Edge computing involves processing sensitive data locally
    on devices, raising concerns about data privacy and security:'
  id: totrans-2121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私和安全：边缘计算涉及在设备上本地处理敏感数据，带来了数据隐私和安全问题：
- en: –
  id: totrans-2122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Data Privacy: Implementing robust encryption techniques and access control
    mechanisms to protect sensitive data processed by LLMs on edge devices.'
  id: totrans-2123
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据隐私：实施强大的加密技术和访问控制机制，以保护在边缘设备上由 LLM 处理的敏感数据。
- en: –
  id: totrans-2124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Secure Communication: Ensuring secure communication channels between IoT devices
    and LLMs to prevent data breaches or unauthorised access.'
  id: totrans-2125
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 安全通信：确保 IoT 设备与 LLM 之间的安全通信渠道，以防止数据泄露或未经授权的访问。
- en: •
  id: totrans-2126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real-Time Processing and Reliability: LLMs deployed in edge computing environments
    must operate with low latency and high reliability to support real-time applications:'
  id: totrans-2127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时处理和可靠性：部署在边缘计算环境中的 LLM 必须以低延迟和高可靠性运行，以支持实时应用：
- en: –
  id: totrans-2128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Latency: Optimising algorithms and processing capabilities of LLMs to handle
    real-time data streams efficiently without delays.'
  id: totrans-2129
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟：优化 LLM 的算法和处理能力，以高效处理实时数据流而不会出现延迟。
- en: –
  id: totrans-2130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Reliability: Ensuring the accuracy and consistency of insights generated by
    LLMs in dynamic and unpredictable IoT environments.'
  id: totrans-2131
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可靠性：确保在动态和不可预测的 IoT 环境中生成的 LLM 见解的准确性和一致性。
- en: 12.5 Future Research Areas
  id: totrans-2132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5 未来研究领域
- en: •
  id: totrans-2133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Federated Learning and Edge Computing: Exploring federated learning techniques
    where LLMs can be trained collaboratively across edge devices without centralised
    data aggregation. This approach addresses privacy concerns and reduces communication
    overhead.'
  id: totrans-2134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦学习和边缘计算：探索联邦学习技术，使 LLM 可以在边缘设备上协作训练，而无需集中数据聚合。这种方法解决了隐私问题并减少了通信开销。
- en: •
  id: totrans-2135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real-Time Decision Support Systems: Developing LLM-based systems capable of
    real-time decision-making by integrating with edge computing infrastructure. This
    includes optimising algorithms for low-latency processing and ensuring reliability
    under dynamic environmental conditions.'
  id: totrans-2136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时决策支持系统：开发基于 LLM 的系统，能够通过与边缘计算基础设施集成实现实时决策。这包括优化低延迟处理的算法，并确保在动态环境条件下的可靠性。
- en: •
  id: totrans-2137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ethical and Regulatory Implications: Investigating the ethical implications
    of integrating LLMs with IoT and edge computing, particularly regarding data ownership,
    transparency, and fairness. This area requires frameworks for ethical AI deployment
    and governance.'
  id: totrans-2138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理和监管影响：研究将 LLM 与 IoT 和边缘计算集成的伦理影响，特别是数据所有权、透明度和公平性方面。这一领域需要伦理 AI 部署和治理的框架。
- en: Glossary
  id: totrans-2139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词汇表
- en: LLM
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: LLM
- en: Large Language Model – A type of AI model, typically with billions of parameters,
    trained on vast amounts of text data to understand and generate human-like text.
    They are primarily designed for tasks in natural language processing (NLP).
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型 – 一种AI模型，通常具有数十亿个参数，经过大量文本数据的训练以理解和生成类似人类的文本。它们主要设计用于自然语言处理（NLP）任务。
- en: NLP
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: NLP
- en: Natural Language Processing – A field of artificial intelligence that focuses
    on the interaction between computers and humans through natural language, including
    tasks like language generation, translation, and sentiment analysis.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理 – 一种人工智能领域，关注计算机与人类通过自然语言的交互，包括语言生成、翻译和情感分析等任务。
- en: LoRA
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA
- en: Low-Rank Adaptation – A parameter-efficient fine-tuning technique that adjusts
    only small low-rank matrices to adapt pre-trained models to specific tasks, thus
    preserving most of the original model’s parameters.
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应 – 一种参数高效的微调技术，仅调整小型低秩矩阵以将预训练模型适应特定任务，从而保留大部分原始模型的参数。
- en: DoRA
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: DoRA
- en: Weight-Decomposed Low-Rank Adaptation – A technique that decomposes model weights
    into magnitude and direction components, facilitating fine-tuning while maintaining
    inference efficiency.
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分解低秩适应 – 一种将模型权重分解为幅度和方向分量的技术，有助于在保持推理效率的同时进行微调。
- en: QLoRA
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA
- en: Quantised Low-Rank Adaptation – A variation of LoRA, specifically designed for
    quantised models, allowing for efficient fine-tuning in resource-constrained environments.
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: 量化低秩适应 – LoRA 的一种变体，专为量化模型设计，允许在资源受限的环境中进行高效的微调。
- en: PPO
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: PPO
- en: Proximal Policy Optimisation – A reinforcement learning algorithm that adjusts
    policies by balancing the exploration of new actions and exploitation of known
    rewards, designed for stability and efficiency in training.
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化 – 一种强化学习算法，通过平衡新行动的探索和已知奖励的利用来调整策略，旨在训练中的稳定性和效率。
- en: DPO
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: DPO
- en: Direct Preference Optimisation – A method that directly aligns language models
    with human preferences through preference optimisation, bypassing reinforcement
    learning models like PPO.
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 直接偏好优化 – 一种通过偏好优化直接将语言模型与人类偏好对齐的方法，绕过了像 PPO 这样的强化学习模型。
- en: MoE
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: MoE
- en: Mixture of Experts – A model architecture that employs multiple specialised
    subnetworks, called experts, which are selectively activated based on the input
    to improve model performance and efficiency.
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: 专家混合 – 一种模型架构，采用多个专业的子网络，称为专家，根据输入有选择地激活，以提高模型性能和效率。
- en: MoA
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: MoA
- en: Mixture of Agents – A multi-agent framework where several agents collaborate
    during training and inference, leveraging the strengths of each agent to improve
    overall model performance.
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: 代理混合 – 一个多代理框架，其中多个代理在训练和推理过程中协作，利用每个代理的优势来提升整体模型性能。
- en: PEFT
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT
- en: Parameter-Efficient Fine-Tuning – A fine-tuning approach for large models that
    involves adjusting only a subset of model parameters, improving efficiency in
    scenarios with limited computational resources. This includes techniques like
    LoRA, QLoRA, and adapters.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调 – 一种大模型的微调方法，只调整模型参数的一个子集，提高了在计算资源有限的情况下的效率。这包括 LoRA、QLoRA 和适配器等技术。
- en: Adapters
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器
- en: Small, trainable modules introduced into the layers of pre-trained language
    models, allowing efficient task-specific fine-tuning without modifying the core
    parameters of the original model. Techniques such as **AdapterFusion** and **AdapterSoup**
    fall under this category, facilitating the combination of multiple adapters for
    complex multitasking.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 小型可训练模块引入到预训练语言模型的层中，允许在不修改原始模型核心参数的情况下进行高效的任务特定微调。**AdapterFusion** 和 **AdapterSoup**
    等技术属于此类别，促进了多个适配器的组合以实现复杂的多任务处理。
- en: Soft Prompt Tuning (SPT)
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示调整（SPT）
- en: A fine-tuning technique where a set of trainable prompt tokens are added to
    the input sequence to guide a pre-trained model towards task-specific performance
    without modifying internal model weights.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种微调技术，通过在输入序列中添加一组可训练的提示令牌来引导预训练模型向特定任务的性能发展，而无需修改内部模型权重。
- en: Prefix-Tuning
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整
- en: A variation of soft prompt tuning where a fixed sequence of trainable vectors
    is prepended to the input layer at every layer of the model, enhancing task-specific
    adaptation.
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: 一种软提示调整的变体，其中将固定序列的可训练向量添加到模型的每一层的输入层，增强了任务特定的适应能力。
- en: Quantisation
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: The process of reducing the precision of model weights and activations, often
    from 32-bit to lower-bit representations like 8-bit or 4-bit, to reduce memory
    usage and improve computational efficiency.
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型权重和激活精度的过程，通常从32位减少到8位或4位等低位表示，以减少内存使用并提高计算效率。
- en: Quantised LLMs
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: 量化语言模型（Quantised LLMs）
- en: Large Language Models that have undergone quantisation, a process that reduces
    the precision of model weights and activations, often from 32-bit to 8-bit or
    lower, to enhance memory and computational efficiency.
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 经过量化的大型语言模型，这一过程减少了模型权重和激活的精度，通常从32位减少到8位或更低，以提高内存和计算效率。
- en: Pruning
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝（Pruning）
- en: A model optimisation technique that reduces the complexity of large language
    models by removing less significant parameters, enabling faster inference and
    lower memory usage.
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: 一种模型优化技术，通过去除不太重要的参数来减少大型语言模型的复杂性，从而实现更快的推理和更低的内存使用。
- en: Half Fine-Tuning (HFT)
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: 半微调（Half Fine-Tuning，HFT）
- en: A fine-tuning method where half of the model’s parameters are kept frozen while
    the other half are updated, helping to maintain pre-trained knowledge while adapting
    the model to new tasks.
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: 一种微调方法，其中模型的一半参数保持冻结，而另一半进行更新，帮助在适应新任务的同时保持预训练知识。
- en: Structured Masking
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化掩蔽（Structured Masking）
- en: A technique that masks entire layers, heads, or other structural components
    of a model to reduce complexity while fine-tuning for specific tasks.
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
  zh: 一种掩蔽整个层、头或模型其他结构组件的技术，以在针对特定任务微调时减少复杂性。
- en: Unstructured Masking
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化掩蔽（Unstructured Masking）
- en: A technique where certain parameters of the model are masked out randomly or
    based on a pattern during fine-tuning, allowing for the identification of the
    most important model weights.
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在微调过程中随机或基于模式掩蔽模型某些参数的技术，从而识别出最重要的模型权重。
- en: GLUE
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE
- en: General Language Understanding Evaluation – A benchmark used to evaluate the
    performance of NLP models across a variety of language understanding tasks, such
    as sentiment analysis and natural language inference.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: 通用语言理解评估（General Language Understanding Evaluation）——一个用于评估NLP模型在各种语言理解任务（如情感分析和自然语言推理）表现的基准。
- en: SuperGLUE
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: 超级GLUE（SuperGLUE）
- en: Super General Language Understanding Evaluation – A more challenging extension
    of GLUE, consisting of harder tasks designed to test the robustness and adaptability
    of NLP models.
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
  zh: 超级通用语言理解评估（Super General Language Understanding Evaluation）——GLUE的一个更具挑战性的扩展，包含更难的任务，旨在测试NLP模型的鲁棒性和适应性。
- en: TruthfulQA
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
  zh: 真实QA（TruthfulQA）
- en: A benchmark designed to measure the truthfulness of a language model’s output,
    focusing on factual accuracy and resistance to hallucination.
  id: totrans-2183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个旨在衡量语言模型输出真实性的基准，重点关注事实准确性和对虚假信息的抵抗力。
- en: IFEval
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
  zh: IFEval
- en: Instruction Following Evaluation – A benchmark that assesses a model’s ability
    to follow explicit instructions across tasks, usually in the context of fine-tuning
    large models for adherence to specific instructions.
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: 指令遵循评估（Instruction Following Evaluation）——一个评估模型在各种任务中遵循明确指令能力的基准，通常是在微调大型模型以遵循特定指令的背景下进行评估。
- en: BBH
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
  zh: BBH
- en: Big Bench Hard – A subset of the Big Bench dataset, which consists of particularly
    difficult tasks aimed at evaluating the advanced reasoning abilities of large
    language models.
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: Big Bench Hard——Big Bench数据集的一个子集，包含特别困难的任务，旨在评估大型语言模型的高级推理能力。
- en: MATH
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: MATH
- en: A dataset created to evaluate a model’s ability to solve high-school level mathematical
    problems, presented in formal formats like LaTeX.
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个创建用于评估模型解决高中水平数学问题能力的数据集，问题以LaTeX等正式格式呈现。
- en: GPQA
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: GPQA
- en: General-Purpose Question Answering – A challenging dataset that features knowledge-based
    questions crafted by experts to assess deep reasoning and factual recall.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: 通用问题回答（General-Purpose Question Answering）——一个具有挑战性的数据集，包含由专家设计的基于知识的问题，用于评估深度推理和事实回忆能力。
- en: MuSR
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: MuSR
- en: Multimodal Structured Reasoning – A dataset that involves complex problems requiring
    language models to integrate reasoning across modalities, often combining text
    with other forms of data such as images or graphs.
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态结构化推理（Multimodal Structured Reasoning）——一个涉及复杂问题的数据集，要求语言模型在不同模态之间整合推理，通常结合文本和图像或图表等其他数据形式。
- en: MMLU
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU
- en: Massive Multitask Language Understanding – A benchmark that evaluates a language
    model’s ability to perform various tasks across diverse domains, such as humanities,
    STEM, social sciences, and others, typically requiring high-level reasoning.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模多任务语言理解（Massive Multitask Language Understanding）——一个评估语言模型在各种领域（如人文学科、STEM、社会科学等）执行多种任务能力的基准，通常要求高级推理能力。
- en: MMLU-PRO
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU-PRO
- en: A refined version of the MMLU dataset with a focus on more challenging, multi-choice
    problems, typically requiring the model to parse long-range context.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: MMLU数据集的精炼版本，专注于更具挑战性的多选问题，通常要求模型解析长距离的上下文。
- en: ARC
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: ARC
- en: AI2 Reasoning Challenge – A benchmark for evaluating a language model’s reasoning
    capabilities using a dataset of multiple-choice science questions.
  id: totrans-2199
  prefs: []
  type: TYPE_NORMAL
  zh: AI2 推理挑战 – 一个评估语言模型推理能力的基准，使用的是多选科学问题的数据集。
- en: COQA
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: COQA
- en: Conversational Question Answering – A benchmark that evaluates how well a language
    model can understand and engage in back-and-forth conversation, especially in
    a question-answer format.
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式问答 – 一个评估语言模型在对话中理解和参与的能力的基准，特别是在问答格式下。
- en: DROP
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: DROP
- en: Discrete Reasoning Over Paragraphs – A benchmark that tests a model’s ability
    to perform discrete reasoning over text, especially in scenarios requiring arithmetic,
    comparison, or logical reasoning.
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: 段落上的离散推理 – 一个测试模型在文本上执行离散推理的能力的基准，特别是在需要算术、比较或逻辑推理的场景中。
- en: SQuAD
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD
- en: Stanford Question Answering Dataset – A popular dataset for evaluating a model’s
    ability to understand and answer questions based on passages of text.
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福问答数据集 – 一个流行的数据集，用于评估模型理解和回答基于文本段落的问题的能力。
- en: TREC
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: TREC
- en: Text REtrieval Conference – A benchmark that evaluates models on various text
    retrieval tasks, often focusing on information retrieval and document search.
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: 文本检索会议 – 一个评估模型在各种文本检索任务中的表现的基准，通常专注于信息检索和文档搜索。
- en: WMT
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
  zh: WMT
- en: Workshop on Machine Translation – A dataset and benchmark for evaluating the
    performance of machine translation systems across different language pairs.
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译研讨会 – 一个评估机器翻译系统在不同语言对间表现的数据集和基准。
- en: XNLI
  id: totrans-2210
  prefs: []
  type: TYPE_NORMAL
  zh: XNLI
- en: Cross-lingual Natural Language Inference – A dataset designed to evaluate a
    model’s ability to understand and infer meaning across multiple languages.
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言自然语言推理 – 一个设计用于评估模型在多语言中理解和推断含义的数据集。
- en: PiQA
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: PiQA
- en: Physical Interaction Question Answering – A dataset that measures a model’s
    understanding of physical interactions and everyday tasks.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: 物理交互问答 – 一个测量模型理解物理交互和日常任务的数据集。
- en: Winogrande
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: Winogrande
- en: A large-scale dataset aimed at evaluating a language model’s ability to handle
    commonsense reasoning, typically through tasks that involve resolving ambiguous
    pronouns in sentences.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大规模数据集，旨在评估语言模型处理常识推理的能力，通常通过涉及解析句子中的模糊代词的任务来实现。
- en: RLHF
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF
- en: Reinforcement Learning from Human Feedback – A method where language models
    are fine-tuned based on human-provided feedback, often used to guide models towards
    preferred behaviours or outputs.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人类反馈的强化学习 – 一种通过人类提供的反馈对语言模型进行微调的方法，通常用于引导模型朝着期望的行为或输出。
- en: RAFT
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: RAFT
- en: Retrieval-Augmented Fine-Tuning – A method combining retrieval techniques with
    fine-tuning to enhance the performance of language models by allowing them to
    access external information during training or inference.
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强微调 – 一种结合检索技术和微调的方法，通过允许模型在训练或推理过程中访问外部信息来提升语言模型的性能。
- en: References
  id: totrans-2220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N-gram language models. [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf).
    [Accessed 01-07-2024].'
  id: totrans-2221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N-gram 语言模型。 [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)。
    [访问日期 2024年01月07日]。'
- en: '[2] Anis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown, 04 2023.'
  id: totrans-2222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Anis Koubaa. Gpt-4 与 gpt-3.5: 简明对比, 2023年4月。'
- en: '[3] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. A survey
    of reinforcement learning from human feedback, 2024.'
  id: totrans-2223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Timo Kaufmann, Paul Weng, Viktor Bengs, 和 Eyke Hüllermeier. 人类反馈强化学习综述,
    2024年。'
- en: '[4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qian Yang, and Xingxu Xie. A survey on evaluation of large language
    models. ACM Transactions on Intelligent Systems and Technology, 15:1 – 45, 2023.'
  id: totrans-2224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qian Yang, 和 Xingxu Xie. 大语言模型评估综述. ACM 智能系统与技术汇刊, 15:1 – 45, 2023年。'
- en: '[5] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid,
    Aafaq Iqbal Khan, and Arsalan Shahid. Building trust in conversational ai: A review
    and solution architecture using large language models and knowledge graphs. Big
    Data and Cognitive Computing, 8(6):70, 2024.'
  id: totrans-2225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid,
    Aafaq Iqbal Khan, 和 Arsalan Shahid。构建对话 AI 的信任：使用大型语言模型和知识图谱的综述和解决方案架构。《大数据与认知计算》，8(6):70，2024。'
- en: '[6] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, and Wenbin Zhang.
    History, development, and principles of large language models-an introductory
    survey, 2024.'
  id: totrans-2226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, 和 Wenbin Zhang。大型语言模型的历史、发展及原理——入门调查，2024。'
- en: '[7] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space, 2013.'
  id: totrans-2227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tomas Mikolov, Kai Chen, Greg Corrado, 和 Jeffrey Dean。高效估计词向量空间中的词表示，2013。'
- en: '[8] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. 2019.'
  id: totrans-2228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, 和 Ilya Sutskever。语言模型是无监督的多任务学习者，2019。'
- en: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding, 2019.'
  id: totrans-2229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。Bert：用于语言理解的深度双向变换器预训练，2019。'
- en: '[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
    Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
    Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
    Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
    Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
    Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022.'
  id: totrans-2230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
    Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
    Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
    Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
    Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
    Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew
    M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, 和 Noah Fiedel。Palm：通过路径扩展语言建模，2022。'
- en: '[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models, 2023.'
  id: totrans-2231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和 Guillaume Lample。Llama：开放和高效的基础语言模型，2023。'
- en: '[12] The art of fine-tuning large language models, explained in depth — linkedin.com.
    [https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc](https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc).
    [Accessed 01-07-2024].'
  id: totrans-2232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 大型语言模型的微调艺术，深度解析——linkedin.com。 [https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc](https://www.linkedin.com/pulse/art-fine-tuning-large-language-models-explained-depth-cherickal-giavc)。
    [访问日期：2024年01月07日]。'
- en: '[13] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
    Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview of
    large language models, 2024.'
  id: totrans-2233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad
    Usman, Naveed Akhtar, Nick Barnes, 和 Ajmal Mian。大型语言模型的综合概述，2024。'
- en: '[14] Jeff Li, MBA, PMP on LinkedIn: Fine-tuning versus RAG in Generative AI
    Applications Architecture — linkedin.com. [https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT](https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT).
    [Accessed 01-08-2024].'
  id: totrans-2234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jeff Li, MBA, PMP 在 LinkedIn 上：生成式 AI 应用架构中的精调与 RAG — linkedin.com。 [https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT](https://www.linkedin.com/posts/xjeffli_fine-tuning-versus-rag-in-generative-ai-applications-activity-7189276988690382848--vxT)。
    [访问时间 2024-08-01]。'
- en: '[15] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, and Hua
    Wu. Hft: Half fine-tuning for large language models. arXiv preprint arXiv:2404.18466,
    2024.'
  id: totrans-2235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun 和 Hua Wu。Hft:
    大型语言模型的半精调。arXiv 预印本 arXiv:2404.18466，2024。'
- en: '[16] Rion Snow, Brendan O’Connor, Dan Jurafsky, and Andrew Y Ng. Cheap and
    fast—but is it good? evaluating non-expert annotations for natural language tasks.
    In Proceedings of the Conference on Empirical Methods in Natural Language Processing
    (EMNLP), pages 254–263, 2008.'
  id: totrans-2236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Rion Snow, Brendan O’Connor, Dan Jurafsky 和 Andrew Y Ng。便宜且快速——但是否好？评估非专家注释在自然语言任务中的有效性。在自然语言处理经验方法会议（EMNLP）会议录，第254–263页，2008。'
- en: '[17] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu,
    and Christopher Ré. Snorkel: Rapid training data creation with weak supervision.
    In Proceedings of the VLDB Endowment, volume 11, pages 269–282, 2017.'
  id: totrans-2237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu
    和 Christopher Ré。Snorkel: 利用弱监督快速创建训练数据。在 VLDB 基金会会议录，第11卷，第269–282页，2017。'
- en: '[18] Liang Ding, Philipp Gentner, Artur Duda, Vaibhav Sangtani, Dominik Ziegler,
    Max Hennen, Siddharth Jain, and Roland Werthschützky. Automatic data labeling
    for supervised learning with applications to visual inspection of mixed-plastic
    waste. Journal of Cleaner Production, 234:1033–1044, 2019.'
  id: totrans-2238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Liang Ding, Philipp Gentner, Artur Duda, Vaibhav Sangtani, Dominik Ziegler,
    Max Hennen, Siddharth Jain 和 Roland Werthschützky。用于监督学习的自动数据标注及其在混合塑料废料视觉检查中的应用。《清洁生产杂志》，234:1033–1044，2019。'
- en: '[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. In Proceedings of the International Conference
    on Learning Representations (ICLR), 2013.'
  id: totrans-2239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean。向量空间中词表示的高效估计。在国际学习表示会议（ICLR）会议录，2013。'
- en: '[20] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove:
    Global vectors for word representation. In Proceedings of the 2014 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,
    2014.'
  id: totrans-2240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jeffrey Pennington, Richard Socher 和 Christopher D Manning。Glove: 全局词向量表示。在2014年自然语言处理经验方法会议（EMNLP）会议录，第1532–1543页，2014。'
- en: '[21] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine
    translation models with monolingual data. Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), pages
    86–96, 2016.'
  id: totrans-2241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Rico Sennrich, Barry Haddow 和 Alexandra Birch。利用单语数据改进神经机器翻译模型。第54届计算语言学协会年会（第1卷：长论文）会议录，第86–96页，2016。'
- en: '[22] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box
    adversarial examples for text classification. In Proceedings of the 56th Annual
    Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
    pages 31–36, 2017.'
  id: totrans-2242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Javid Ebrahimi, Anyi Rao, Daniel Lowd 和 Dejing Dou。Hotflip: 用于文本分类的白盒对抗示例。在第56届计算语言学协会年会（第2卷：短论文）会议录，第31–36页，2017。'
- en: '[23] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  id: totrans-2243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。arXiv 预印本 arXiv:2005.14165，2020。'
- en: '[24] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models
    better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 3816–3830, 2021.'
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Tianyu Gao, Adam Fisch 和 Danqi Chen。使预训练语言模型更好地进行少样本学习。在第59届计算语言学协会年会和第11届国际自然语言处理联合会议（第1卷：长论文）会议录，第3816–3830页，2021。'
- en: '[25] Steven Feng, Varun Gangal, Jinjun Wei, Yashvardhan Chandrasekhar, Yichong
    Chen, Dani He, Shuyang Huang, Faisal Ladhak, Jiao Lee, Xinyi Li, et al. A survey
    of data augmentation approaches for nlp. arXiv preprint arXiv:2106.07499, 2021.'
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Steven Feng、Varun Gangal、Jinjun Wei、Yashvardhan Chandrasekhar、Yichong
    Chen、Dani He、Shuyang Huang、Faisal Ladhak、Jiao Lee、Xinyi Li 等。NLP 数据增强方法综述。arXiv
    预印本 arXiv:2106.07499，2021年。'
- en: '[26] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
    Doug Downey, and Noah A Smith. Don’t stop pretraining: Adapt language models to
    domains and tasks. In Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics, pages 8342–8360, 2020.'
  id: totrans-2246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Suchin Gururangan、Ana Marasović、Swabha Swayamdipta、Kyle Lo、Iz Beltagy、Doug
    Downey 和 Noah A Smith。不要停止预训练：将语言模型适应于领域和任务。第58届计算语言学协会年会论文集，第8342–8360页，2020年。'
- en: '[27] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
    Shmitchell. On the dangers of stochastic parrots: Can language models be too big?
    Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,
    pages 610–623, 2021.'
  id: totrans-2247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Emily M Bender、Timnit Gebru、Angelina McMillan-Major 和 Shmargaret Shmitchell。关于随机鹦鹉的危险：语言模型会不会过大？2021年
    ACM 公平性、问责性和透明度会议论文集，第610–623页，2021年。'
- en: '[28] Reuben Binns. Fairness in machine learning: Lessons from political philosophy.
    Proceedings of the 2018 Conference on Fairness, Accountability, and Transparency,
    pages 149–159, 2018.'
  id: totrans-2248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Reuben Binns。机器学习中的公平性：来自政治哲学的教训。2018年公平性、问责性和透明度会议论文集，第149–159页，2018年。'
- en: '[29] Sebastian Ruder. The stanford natural language inference (snli) corpus.
    arXiv preprint arXiv:1807.03519, 2021.'
  id: totrans-2249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sebastian Ruder。斯坦福自然语言推理（SNLI）语料库。arXiv 预印本 arXiv:1807.03519，2021年。'
- en: '[30] Pradeep Rajan, Krishna Vyas, Rajiv Bansal, Ranjan Sharma, and Shubhranshu
    Mukherjee. Machine learning for data preprocessing. Journal of Big Data, 6(1):1–25,
    2019.'
  id: totrans-2250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Pradeep Rajan、Krishna Vyas、Rajiv Bansal、Ranjan Sharma 和 Shubhranshu Mukherjee。用于数据预处理的机器学习。《大数据期刊》，6(1):1–25，2019年。'
- en: '[31] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
    Smote: synthetic minority over-sampling technique. Journal of Artificial Intelligence
    Research, 16:321–357, 2002.'
  id: totrans-2251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Nitesh V Chawla、Kevin W Bowyer、Lawrence O Hall 和 W Philip Kegelmeyer。SMOTE：合成少数类过采样技术。《人工智能研究期刊》，16:321–357，2002年。'
- en: '[32] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of Big Data, 6(1):1–48, 2019.'
  id: totrans-2252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Connor Shorten 和 Taghi M Khoshgoftaar。深度学习图像数据增强综述。《大数据期刊》，6(1):1–48，2019年。'
- en: '[33] Alexander Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and
    Christopher Ré. Snorkel: Rapid training data creation with weak supervision. Proceedings
    of the VLDB Endowment, 11(3):269–282, 2020.'
  id: totrans-2253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Alexander Ratner、Henry Ehrenberg、Zeshan Hussain、Jared Dunnmon 和 Christopher
    Ré。Snorkel：利用弱监督快速创建训练数据。VLDB 促进会会议论文集，11(3):269–282，2020年。'
- en: '[34] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine
    learning: Lessons from political philosophy. In Proceedings of the 2017 ACM on
    Conference on Fairness, Accountability, and Transparency, pages 149–159, 2017.'
  id: totrans-2254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Solon Barocas、Moritz Hardt 和 Arvind Narayanan。机器学习中的公平性：来自政治哲学的教训。第2017届
    ACM 公平性、问责性和透明度会议论文集，第149–159页，2017年。'
- en: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers:
    State-of-the-art natural language processing. Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations, pages
    38–45, 2020.'
  id: totrans-2255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement Delangue、Anthony
    Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz 等。Transformers：最先进的自然语言处理。2020年自然语言处理经验方法会议：系统演示论文集，第38–45页，2020年。'
- en: '[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
    An imperative style, high-performance deep learning library. Advances in Neural
    Information Processing Systems, 32, 2019.'
  id: totrans-2256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga 等。PyTorch：一种命令式风格的高性能深度学习库。《神经信息处理系统进展》，32，2019年。'
- en: '[37] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
    Tensorflow: Large-scale machine learning on heterogeneous distributed systems.
    arXiv preprint arXiv:1603.04467, 2015.'
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Martín Abadi、Ashish Agarwal、Paul Barham、Eugene Brevdo、Zhifeng Chen、Craig
    Citro、Greg S Corrado、Andy Davis、Jeffrey Dean、Matthieu Devin 等。TensorFlow：异构分布式系统上的大规模机器学习。arXiv
    预印本 arXiv:1603.04467，2015年。'
- en: '[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器预训练。arXiv预印本
    arXiv:1810.04805，2018年。'
- en: '[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  id: totrans-2259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov。RoBERTa：一种强健优化的BERT预训练方法。arXiv预印本
    arXiv:1907.11692，2019年。'
- en: '[40] Sheng Shen, Zhewei Dong, Xiaocheng Ye, Linjian Ma, Zhewei Li, Zirui Wang,
    Samyam Rajbhandari, Yuxiong Wang, and Zhen Yang. Q-bert: Hessian based ultra low
    precision quantization of bert. Proceedings of the AAAI Conference on Artificial
    Intelligence, 34(05):8815–8821, 2020.'
  id: totrans-2260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Sheng Shen, Zhewei Dong, Xiaocheng Ye, Linjian Ma, Zhewei Li, Zirui Wang,
    Samyam Rajbhandari, Yuxiong Wang, 和 Zhen Yang。Q-bert：基于Hessian的超低精度BERT量化。AAAI人工智能会议论文集，34(05)：8815–8821，2020年。'
- en: '[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9,
    2019.'
  id: totrans-2261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, 和 Ilya
    Sutskever。语言模型是无监督的多任务学习者。OpenAI博客，1(8)：9，2019年。'
- en: '[42] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
    Hanna Wallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. Communications
    of the ACM, 64(12):86–92, 2021.'
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan,
    Hanna Wallach, Hal Daumé III, 和 Kate Crawford。数据集的数据表。ACM通讯，64(12)：86–92，2021年。'
- en: '[43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。arXiv预印本 arXiv:1412.6980，2014年。'
- en: '[44] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
    Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter
    performance analysis of a tensor processing unit. Proceedings of the 44th Annual
    International Symposium on Computer Architecture, pages 1–12, 2017.'
  id: totrans-2264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
    Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, 等人。在数据中心的张量处理单元性能分析。第44届国际计算机架构年会论文集，第1–12页，2017年。'
- en: '[45] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
    Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
    Tensorflow: A system for large-scale machine learning. 12th USENIX Symposium on
    Operating Systems Design and Implementation (OSDI 16), pages 265–283, 2016.'
  id: totrans-2265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
    Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 等人。Tensorflow：大规模机器学习系统。第12届USENIX操作系统设计与实现研讨会（OSDI
    16），第265–283页，2016年。'
- en: '[46] Mohammad Shoeybi, Mostofa Patwary, Raghavendra Puri, Patrick LeGresley,
    Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter
    language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.'
  id: totrans-2266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Mohammad Shoeybi, Mostofa Patwary, Raghavendra Puri, Patrick LeGresley,
    Jared Casper, 和 Bryan Catanzaro。Megatron-lm：利用模型并行训练数十亿参数的语言模型。arXiv预印本 arXiv:1909.08053，2019年。'
- en: '[47] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
    Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, and Payal Yadollahpour.
    Large batch optimization for deep learning: Training bert in 76 minutes. arXiv
    preprint arXiv:1904.00962, 2019.'
  id: totrans-2267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
    Bhojanapalli, Xiaodan Song, James Demmel, Cho-Jui Hsieh, 和 Payal Yadollahpour。深度学习的大批量优化：76分钟内训练BERT。arXiv预印本
    arXiv:1904.00962，2019年。'
- en: '[48] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. 2016.'
  id: totrans-2268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville。《深度学习》。2016年。'
- en: '[49] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization.
    Journal of Machine Learning Research, 13(2):281–305, 2012.'
  id: totrans-2269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] James Bergstra 和 Yoshua Bengio。超参数优化的随机搜索。机器学习研究杂志，13(2)：281–305，2012年。'
- en: '[50] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated Machine
    Learning: Methods, Systems, Challenges. Springer Nature, 2019.'
  id: totrans-2270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Frank Hutter, Lars Kotthoff, 和 Joaquin Vanschoren。自动化机器学习：方法、系统、挑战。Springer
    Nature，2019年。'
- en: '[51] Lutz Prechelt. Early stopping-but when? Neural Networks: Tricks of the
    trade, pages 55–69, 1998.'
  id: totrans-2271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Lutz Prechelt. 早期停止——但什么时候？《神经网络：实用技巧》，第55–69页，1998年。'
- en: '[52] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed
    deep learning in tensorflow. arXiv preprint arXiv:1802.05799, 2018.'
  id: totrans-2272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Alexander Sergeev 和 Mike Del Balso。Horovod：在TensorFlow中快速且简单的分布式深度学习。arXiv预印本
    arXiv:1802.05799，2018年。'
- en: '[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Deepspeed:
    Extreme-scale model training for everyone. arXiv preprint arXiv:2007.04822, 2020.'
  id: totrans-2273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, 和 Yuxiong He. Deepspeed：为所有人提供极大规模模型训练。
    arXiv预印本 arXiv:2007.04822，2020年。'
- en: '[54] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2018.'
  id: totrans-2274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
    Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
    Venkatesh, 等等. 混合精度训练。 arXiv预印本 arXiv:1710.03740，2018年。'
- en: '[55] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou,
    Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann,
    Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley
    Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
    Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado,
    Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards
    expert-level medical question answering with large language models, 2023.'
  id: totrans-2275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le
    Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann,
    Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley
    Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
    Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado,
    Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, 和 Vivek Natarajan. 面向专家级医学问答的大型语言模型，2023年。'
- en: '[56] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source
    financial large language models, 2023.'
  id: totrans-2276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Hongyang Yang, Xiao-Yang Liu, 和 Christina Dan Wang. Fingpt：开源金融大型语言模型，2023年。'
- en: '[57] Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe
    Guo, and Yu-Feng Li. Lawgpt: A chinese legal knowledge-enhanced large language
    model, 2024.'
  id: totrans-2277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe
    Guo, 和 Yu-Feng Li. Lawgpt：一个中文法律知识增强的大型语言模型，2024年。'
- en: '[58] Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao
    Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai,
    Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao,
    Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie
    Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Zhongkai Ye, Lidong Pei, and
    Changyang Tu. Pharmagpt: Domain-specific large language models for bio-pharmaceutical
    and chemistry, 2024.'
  id: totrans-2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao
    Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai,
    Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao,
    Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie
    Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Zhongkai Ye, Lidong Pei, 和 Changyang
    Tu. Pharmagpt：用于生物制药和化学的领域特定大型语言模型，2024年。'
- en: '[59] Writer Engineering team. Palmyra-Fin-70B-32k: a powerful LLM designed
    for Finance. [https://dev.writer.com](https://dev.writer.com), 2024.'
  id: totrans-2279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Writer Engineering team. Palmyra-Fin-70B-32k：为金融设计的强大LLM。 [https://dev.writer.com](https://dev.writer.com)，2024年。'
- en: '[60] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient
    fine-tuning for large models: A comprehensive survey, 2024.'
  id: totrans-2280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, 和 Sai Qian Zhang. 大型模型的参数高效微调：综合调查，2024年。'
- en: '[61] Lin Tian, Xiuzhen Zhang, and Jey Han Lau. Metatroll: Few-shot detection
    of state-sponsored trolls with transformer adapters. In Proceedings of the ACM
    Web Conference 2023, WWW ’23\. ACM, April 2023.'
  id: totrans-2281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Lin Tian, Xiuzhen Zhang, 和 Jey Han Lau. Metatroll：使用变换器适配器进行的国家支持的恶意行为者的少样本检测。在ACM
    Web Conference 2023的论文集中，WWW ’23。 ACM，2023年4月。'
- en: '[62] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models, 2021.'
  id: totrans-2282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适配，2021年。'
- en: '[63] PhD Sebastian Raschka. Practical Tips for Finetuning LLMs Using LoRA (Low-Rank
    Adaptation) — magazine.sebastianraschka.com. [https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms).
    [Accessed 01-08-2024].'
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] PhD Sebastian Raschka. 使用LoRA（低秩适配）微调LLMs的实用技巧 — magazine.sebastianraschka.com.
    [https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)。
    [访问时间 01-08-2024]。'
- en: '[64] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms, 2023.'
  id: totrans-2284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. Qlora：量化大语言模型的高效微调，2023年。'
- en: '[65] What is QLoRa? — Analytics Vidhya — community.analyticsvidhya.com. [https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora](https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora).
    [Accessed 01-08-2024].'
  id: totrans-2285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 什么是 QLoRa？ — Analytics Vidhya — community.analyticsvidhya.com. [https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora](https://community.analyticsvidhya.com/c/generative-ai-tech-discussion/what-is-qlora).
    [访问日期：2024年8月1日]。'
- en: '[66] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation,
    2024.'
  id: totrans-2286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, 和 Min-Hung Chen. Dora: 权重分解低秩适应, 2024。'
- en: '[67] Apple intelligence foundation language models, 2024.'
  id: totrans-2287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Apple intelligence foundation language models, 2024。'
- en: '[68] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, and Hua
    Wu. Hft: Half fine-tuning for large language models, 2024.'
  id: totrans-2288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Tingfeng Hui, Zhenyu Zhang, Shuohuan Wang, Weiran Xu, Yu Sun, 和 Hua Wu.
    Hft: 大型语言模型的半精调, 2024。'
- en: '[69] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin
    Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, and
    Gregory Diamos. Banishing llm hallucinations requires rethinking generalization,
    2024.'
  id: totrans-2289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin
    Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, 和 Gregory
    Diamos. 消除 LLM 幻觉需要重新思考泛化, 2024。'
- en: '[70] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
    Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
    Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
    Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
    Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.'
  id: totrans-2290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma
    Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, 和 William El Sayed. Mixtral of experts,
    2024。'
- en: '[71] Applying Mixture of Experts in LLM Architectures — NVIDIA Technical Blog
    — developer.nvidia.com. [https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/).
    [Accessed 01-08-2024].'
  id: totrans-2291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 在 LLM 架构中应用专家混合 — NVIDIA 技术博客 — developer.nvidia.com. [https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/](https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/).
    [访问日期：2024年8月1日]。'
- en: '[72] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents
    enhances large language model capabilities, 2024.'
  id: totrans-2292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, 和 James Zou. Mixture-of-agents
    增强大型语言模型能力, 2024。'
- en: '[73] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms, 2017.'
  id: totrans-2293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg Klimov.
    近端策略优化算法, 2017。'
- en: '[74] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D.
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model, 2024.'
  id: totrans-2294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D. Manning, 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型, 2024。'
- en: '[75] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju
    Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive
    study, 2024.'
  id: totrans-2295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju
    Wang, Chao Yu, 和 Yi Wu. DPO 是否优于 PPO 进行 LLM 对齐？ 一项全面研究, 2024。'
- en: '[76] What are the most effective techniques for pruning ai models? — linkedin.com.
    [https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef](https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef).
    [Accessed 05-07-2024].'
  id: totrans-2296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 最有效的 AI 模型剪枝技术是什么？ — linkedin.com. [https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef](https://www.linkedin.com/advice/3/what-most-effective-techniques-pruning-0mlef).
    [访问日期：2024年7月5日]。'
- en: '[77] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song, and Bo Li. Decodingtrust: A comprehensive assessment of trustworthiness
    in gpt models, 2024.'
  id: totrans-2297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song, 和 Bo Li. Decodingtrust: 对 GPT 模型可信度的全面评估, 2024。'
- en: '[78] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer,
    Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian
    Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations,
    2023.'
  id: totrans-2298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] 哈坎·伊南、卡尔提克耶亚·乌帕萨尼、简丰·池、拉希·荣塔、克里提卡·艾耶尔、尤宁·毛、迈克尔·托恩切夫、青胡、布莱恩·富勒、达维德·特斯图吉内和马迪安·哈布萨。Llama
    guard：基于 LLM 的人类-人工智能对话输入输出保护，2023年。'
- en: '[79] Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza
    Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu,
    Olivia Sturman, and Oscar Wahltinez. Shieldgemma: Generative ai content moderation
    based on gemma, 2024.'
  id: totrans-2299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 曾文君、刘宇池、瑞安·穆林斯、吕多维克·佩兰、乔·费尔南德斯、哈姆扎·哈科斯、卡尔提克·纳拉辛汉、德鲁·普劳德、皮尤什·库马尔、巴克提普里亚·拉达拉普、奥利维亚·斯特曼和奥斯卡·瓦尔蒂内斯。Shieldgemma：基于
    Gemma 的生成式人工智能内容审查，2024年。'
- en: '[80] Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin,
    Nathan Lambert, Yejin Choi, and Nouha Dziri. Wildguard: Open one-stop moderation
    tools for safety risks, jailbreaks, and refusals of llms, 2024.'
  id: totrans-2300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 韩胜久、卡维尔·拉奥、艾莉森·艾廷格、李伟·江、比尔·宇辰·林、内森·兰伯特、叶津·崔和诺哈·兹里。Wildguard：开放的一站式审查工具，针对安全风险、越狱和
    LLM 的拒绝，2024年。'
- en: '[81] Vishal Mysore. LLM Deployment Strategies : Its not Magic , Its Logic!
    — visrow. [https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4](https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4).
    [Accessed 07-08-2024].'
  id: totrans-2301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] 维沙尔·迈索尔。LLM 部署策略：这不是魔法，这是逻辑！— visrow。 [https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4](https://medium.com/@visrow/llm-deployment-strategies-its-not-magic-its-logic-71d5f32ac2b4)。
    [访问日期 2024年8月7日]。'
- en: '[82] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management
    for large language model serving with pagedattention, 2023.'
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 权宇硕、朱焕林、司源·庄、英盛、连敏·郑、科迪·郝宇、约瑟夫·E·冈萨雷斯、郝张和伊昂·斯托伊卡。大规模语言模型服务的高效内存管理与分页注意机制，2023年。'
- en: '[83] Preprocess and fine-tune llms quickly and cost-effectively using amazon
    emr serverless and amazon sagemaker — aws.amazon.com. [https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/](https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/).
    [Accessed 06-08-2024].'
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] 使用 amazon emr serverless 和 amazon sagemaker 快速、经济地预处理和微调 llms — aws.amazon.com。
    [https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/](https://aws.amazon.com/blogs/big-data/preprocess-and-fine-tune-llms-quickly-and-cost-effectively-using-amazon-emr-serverless-and-amazon-sagemaker/)。
    [访问日期 2024年8月6日]。'
- en: '[84] Nvidia nemo build and customize your own llms (with tutorial) — run.ai.
    [https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo](https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo).
    [Accessed 07-08-2024].'
  id: totrans-2304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Nvidia nemo 构建和定制你自己的 llms（附教程）— run.ai。 [https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo](https://www.run.ai/guides/ai-open-source-projects/nvidia-nemo)。
    [访问日期 2024年8月7日]。'
- en: '[85] Nvidia. What is nvidia nemo? [https://www.nvidia.com/en-us/ai-data-science/products/nemo/](https://www.nvidia.com/en-us/ai-data-science/products/nemo/).'
  id: totrans-2305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Nvidia。什么是 nvidia nemo？ [https://www.nvidia.com/en-us/ai-data-science/products/nemo/](https://www.nvidia.com/en-us/ai-data-science/products/nemo/)。'
- en: '[86] Gemini Team and Rohan Anil et al. Gemini: A family of highly capable multimodal
    models, 2024.'
  id: totrans-2306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Gemini 团队和罗汉·安尼尔等。Gemini：一系列高能力的多模态模型，2024年。'
- en: '[87] Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang
    He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, and Lizhuang Ma.
    Efficient multimodal large language models: A survey, 2024.'
  id: totrans-2307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] 金宜章、李坚、刘业鑫、天俊·顾、凯·吴、郑凯·蒋、穆扬·赫、博·赵、辛·谭、振业·甘、亚彪·王、成杰·王和李壮·马。高效的多模态大规模语言模型：综述，2024年。'
- en: '[88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, and Ilya Sutskever. Learning transferable visual models from natural
    language supervision, 2021.'
  id: totrans-2308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] 亚历克·拉德福德、郑沃克·金、克里斯·哈拉西、阿迪提亚·拉梅什、加布里埃尔·戈、桑迪尼·阿加瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克、格雷琴·克鲁格和伊利亚·苏茨克维尔。通过自然语言监督学习可迁移的视觉模型，2021年。'
- en: '[89] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and
    cheaper than in-context learning, 2022.'
  id: totrans-2309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] 刘浩坤、德里克·谭、穆罕默德·穆基斯、杰伊·莫赫塔、滕浩·黄、莫希特·班萨尔和科林·拉费尔。少量样本参数高效微调优于上下文学习，更加经济，2022年。'
- en: '[90] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi.
    Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free
    low-rank adaptation, 2023.'
  id: totrans-2310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev 和 Ali Ghodsi. Dylora：通过动态无搜索低秩适应进行预训练模型的参数高效调优，2023。'
- en: '[91] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa:
    Memory-efficient low-rank adaptation for large language models fine-tuning, 2023.'
  id: totrans-2311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu 和 Bo Li. Lora-fa：大语言模型微调的内存高效低秩适应，2023。'
- en: '[92] Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, and Rongrong Ji. Not all
    attention is needed: Parameter and computation efficient transfer learning for
    multi-modal large language models, 2024.'
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun 和 Rongrong Ji. 并非所有注意力都是必要的：多模态大型语言模型的参数和计算高效转移学习，2024。'
- en: '[93] Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, and Yunhe Wang.
    Memory-space visual prompting for efficient vision-language fine-tuning, 2024.'
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han 和 Yunhe Wang.
    记忆空间视觉提示用于高效的视觉语言微调，2024。'
- en: '[94] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng
    Qiu. Full parameter fine-tuning for large language models with limited resources,
    2024.'
  id: totrans-2314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo 和 Xipeng Qiu.
    在资源有限的情况下对大型语言模型进行完整参数微调，2024。'
- en: '[95] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee,
    Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward passes,
    2024.'
  id: totrans-2315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee,
    Danqi Chen 和 Sanjeev Arora. 仅通过前向传递进行语言模型微调，2024。'
- en: '[96] Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen, and Shenjun
    Zhong. Pefomed: Parameter efficient fine-tuning of multimodal large language models
    for medical imaging, 2024.'
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen 和 Shenjun Zhong.
    Pefomed：用于医学影像的多模态大型语言模型的参数高效微调，2024。'
- en: '[97] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
    Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation
    learning by masked prediction of hidden units, 2021.'
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
    Salakhutdinov 和 Abdelrahman Mohamed. Hubert：通过对隐藏单元的掩蔽预测进行自监督语音表征学习，2021。'
- en: '[98] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec
    2.0: A framework for self-supervised learning of speech representations, 2020.'
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed 和 Michael Auli. wav2vec
    2.0：一个用于自监督语音表征学习的框架，2020。'
- en: '[99] Deepak Babu P R. Audio language models and multimodal architecture — prdeepak.babu.
    [https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac](https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac).
    [Accessed 19-07-2024].'
  id: totrans-2319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Deepak Babu P R. 音频语言模型与多模态架构 — prdeepak.babu. [https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac](https://medium.com/@prdeepak.babu/audio-language-models-and-multimodal-architecture-1cdd90f46fac).
    [访问日期：2024年7月19日]。'
- en: '[100] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur
    Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei
    Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg,
    Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco
    Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu,
    Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas
    Zilka, and Christian Frank. Audiopalm: A large language model that can speak and
    listen, 2023.'
  id: totrans-2320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur
    Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei
    Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg,
    Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco
    Tagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu,
    Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas
    Zilka 和 Christian Frank. Audiopalm：一种能够说话和听的语言模型，2023。'
- en: '[101] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier
    Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco
    Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling approach to audio
    generation, 2023.'
  id: totrans-2321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier
    Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco
    Tagliasacchi 和 Neil Zeghidour. Audiolm：一种面向音频生成的语言建模方法，2023。'
- en: '[102] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,
    Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. A comprehensive overview
    of large language models, 2024.'
  id: totrans-2322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,
    Muhammad Usman, Naveed Akhtar, Nick Barnes 和 Ajmal Mian. 大型语言模型的综合概述，2024。'
- en: '[103] Fine-tune llama 2 with lora: Customizing a large language model for question-answering
    — rocm.blogs.amd.com. [https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html).
    [Accessed 15-07-2024].'
  id: totrans-2323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] 使用Lora对Llama 2进行微调：为问答定制大型语言模型 — rocm.blogs.amd.com。 [https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html)。
    [访问日期：2024年7月15日]。'
- en: '[104] Aayush Mittal. Understanding llm fine-tuning: Tailoring large language
    models to your unique requirements — linkedin.com. [https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements](https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements).
    [Accessed 11-07-2024].'
  id: totrans-2324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Aayush Mittal。理解LLM微调：将大型语言模型定制为符合你独特需求的模型 — linkedin.com。 [https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements](https://www.unite.ai/understanding-llm-fine-tuning-tailoring-large-language-models-to-your-unique-requirements)。
    [访问日期：2024年7月11日]。'
- en: '[105] Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, and Edoardo M.
    Ponti. Scaling sparse fine-tuning to large language models, 2024.'
  id: totrans-2325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Alan Ansell、Ivan Vulić、Hannah Sterz、Anna Korhonen 和 Edoardo M. Ponti。将稀疏微调扩展到大型语言模型，2024年。'
- en: '[106] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei,
    and Tat-Seng Chua. Data-efficient fine-tuning for llm-based recommendation, 2024.'
  id: totrans-2326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Xinyu Lin、Wenjie Wang、Yongqi Li、Shuo Yang、Fuli Feng、Yinwei Wei 和 Tat-Seng
    Chua。基于LLM的推荐系统的数据高效微调，2024年。'
- en: '[107] Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong, Xinwang
    Liu, Guannan Zhang, and Kejun Zhang. End-to-end learnable clustering for intent
    learning in recommendation, 2024.'
  id: totrans-2327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Yue Liu、Shihao Zhu、Jun Xia、Yingwei Ma、Jian Ma、Wenliang Zhong、Xinwang
    Liu、Guannan Zhang 和 Kejun Zhang。推荐系统中用于意图学习的端到端可学习聚类，2024年。'
- en: '[108] Haoran Li, Xinyuan Zhao, Dadi Guo, Hanlin Gu, Ziqian Zeng, Yuxing Han,
    Yangqiu Song, Lixin Fan, and Qiang Yang. Federated domain-specific knowledge transfer
    on large language models using synthetic data, 2024.'
  id: totrans-2328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Haoran Li、Xinyuan Zhao、Dadi Guo、Hanlin Gu、Ziqian Zeng、Yuxing Han、Yangqiu
    Song、Lixin Fan 和 Qiang Yang。使用合成数据对大型语言模型进行联邦领域特定知识转移，2024年。'
- en: '[109] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
    and Adrian Vladu. Towards deep learning models resistant to adversarial attacks,
    2019.'
  id: totrans-2329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Aleksander Madry、Aleksandar Makelov、Ludwig Schmidt、Dimitris Tsipras 和
    Adrian Vladu。针对对抗攻击的深度学习模型，2019年。'
