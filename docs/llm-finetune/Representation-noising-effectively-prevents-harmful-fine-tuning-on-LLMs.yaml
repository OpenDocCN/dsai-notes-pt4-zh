- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:36:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Representation noising effectively prevents harmful fine-tuning on LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示噪声有效防止对大语言模型的有害微调
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14577](https://ar5iv.labs.arxiv.org/html/2405.14577)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14577](https://ar5iv.labs.arxiv.org/html/2405.14577)
- en: Domenic Rosati^(1,7)  Jan Wehner²  Kai Williams³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Domenic Rosati^(1,7)  Jan Wehner²  Kai Williams³
- en: Łukasz Bartoszcze⁴  David Atanasov⁵ Robie Gonzales¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Łukasz Bartoszcze⁴  David Atanasov⁵ Robie Gonzales¹
- en: Subhabrata Majumdar⁶  Carsten Maple⁴  Hassan Sajjad¹  Frank Rudzicz^(1,7)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Subhabrata Majumdar⁶  Carsten Maple⁴  Hassan Sajjad¹  Frank Rudzicz^(1,7)
- en: ¹Dalhousie University ²CISPA Helmholtz Center for Information Security
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹达尔豪斯大学 ²CISPA赫尔姆霍茨信息安全中心
- en: ³Swarthmore College  ⁴University of Warwick  ⁵University of Toronto ⁶Vijil
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³斯沃斯莫尔学院  ⁴华威大学  ⁵多伦多大学 ⁶Vijil
- en: '⁷Vector Institute for Artificial Intelligence Corresponding author: domenic.rosati@dal.ca'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷Vector Institute for Artificial Intelligence 通讯作者：domenic.rosati@dal.ca
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Releasing open-source large language models (LLMs) presents a dual-use risk
    since bad actors can easily fine-tune these models for harmful purposes. Even
    without the open release of weights, weight stealing and fine-tuning APIs make
    closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures
    like preventing jailbreaks and improving safety guardrails are important, such
    measures can easily be reversed through fine-tuning. In this work, we propose
    Representation Noising (RepNoise), a defence mechanism that is effective even
    when attackers have access to the weights and the defender no longer has any control.
    RepNoise works by removing information about harmful representations such that
    it is difficult to recover them during fine-tuning. Importantly, our defence is
    also able to generalize across different subsets of harm that have not been seen
    during the defence process. Our method does not degrade the general capability
    of LLMs and retains the ability to train the model on harmless tasks. We provide
    empirical evidence that the effectiveness of our defence lies in its “depth”:
    the degree to which information about harmful representations is removed across
    all layers of the LLM.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 公开发布的大型语言模型（LLMs）存在双重用途风险，因为恶意行为者可以轻易地对这些模型进行有害微调。即使没有公开权重，权重盗取和微调API也使得闭源模型容易受到有害微调攻击（HFA）。虽然防止越狱和改进安全防护措施等安全措施很重要，但这些措施很容易通过微调被逆转。在这项工作中，我们提出了表示噪声（RepNoise），这是一种防御机制，即使攻击者可以访问权重且防御者不再有任何控制权时也能有效。RepNoise的工作原理是去除有关有害表示的信息，从而在微调过程中难以恢复这些信息。重要的是，我们的防御还能够在未见过的有害子集上进行泛化。我们的方法不会降低LLMs的整体能力，并保持在无害任务上训练模型的能力。我们提供了实证证据，表明我们防御的有效性在于其“深度”：即在LLM的所有层中去除有害表示的信息的程度。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Despite the benefits to both research and commercial development, open-sourcing
    large language models (LLMs) comes with several risks [[1](#bib.bib1)] such as
    facilitating the development of weapons [[2](#bib.bib2)]. Such risks are not isolated
    to only open-source models, weights of proprietary models expose fine-tuning APIs
    [[3](#bib.bib3)] which can be used for constructing harmful models and models
    can also be leaked at inference time [[4](#bib.bib4)]. The risk of LLMs assisting
    in harmful tasks is exacerbated by their increasing ability to follow instructions,
    carry out sophisticated tasks, and the ease with which they can be trained and
    run. Developers attempt to mitigate these risks [[5](#bib.bib5)] by developing
    safety guardrails that prevent LLMs from performing harmful tasks at inference
    time. However, these guardrails are easily circumvented either through back doors
    [[6](#bib.bib6)], adversarial attacks [[7](#bib.bib7)], or harmful fine-tuning
    [[8](#bib.bib8)]. We argue that no matter how sophisticated safety guardrails
    become, models vulnerable to harmful fine-tuning and amenable to malicious modifications
    are fundamentally unsafe.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对研究和商业发展有诸多好处，开源大型语言模型（LLMs）也带来了几个风险 [[1](#bib.bib1)]，例如促进武器的发展 [[2](#bib.bib2)]。这些风险不仅限于开源模型，专有模型的权重暴露了微调API
    [[3](#bib.bib3)]，这些API可以用于构建有害模型，模型也可能在推理时被泄露 [[4](#bib.bib4)]。LLMs在有害任务中的风险因其越来越强的指令跟随能力、执行复杂任务的能力，以及训练和运行的便捷性而加剧。开发者试图通过开发安全防护措施
    [[5](#bib.bib5)] 来减轻这些风险，这些措施可以防止LLMs在推理时执行有害任务。然而，这些防护措施很容易通过后门 [[6](#bib.bib6)]、对抗性攻击
    [[7](#bib.bib7)] 或有害微调 [[8](#bib.bib8)] 被规避。我们认为，无论安全防护措施多么复杂，易受有害微调和恶意修改影响的模型在根本上都是不安全的。
- en: 'We propose Representation Noising (RepNoise) as the first effective defence
    against harmful fine-tuning attacks (HFAs) for LLMs in the natural language generation
    setting where the defender has no control of the model after the attacker attains
    its weights. Our work is inspired by the observation that safety mechanisms in
    LLMs are shallow [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]: despite
    showing safe behaviour on the surface, harmful representations remain present
    in these models such that they can be easily recovered [[8](#bib.bib8)]. RepNoise
    works by removing the information structure of harmful representations such that
    they are much harder to recover during subsequent HFAs ([fig. 1](#S1.F1 "In 1
    Introduction ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了表示噪声（RepNoise）作为对抗LLMs在自然语言生成设置中，攻击者获得模型权重后的有害微调攻击（HFAs）的首个有效防御。我们的工作灵感来源于对LLMs中安全机制较浅的观察
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]：尽管在表面上表现出安全行为，但这些模型中仍然存在有害表示，这使得这些表示可以很容易被恢复
    [[8](#bib.bib8)]。RepNoise 通过去除有害表示的信息结构，使得在随后的 HFAs 中这些表示更难恢复（[fig. 1](#S1.F1
    "In 1 Introduction ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")）。
- en: 'Our contributions are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献如下：
- en: (a)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: We provide a defence method derived from an understanding of training dynamics
    that would make harmful representations harder to recover ([§ 3](#S3 "3 Method
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一种防御方法，这种方法源于对训练动态的理解，使得有害表示更难以恢复（[§ 3](#S3 "3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")）。
- en: (b)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: We present extensive experimental evidence that our method prevents training
    on harmful question-answering and toxic content generation tasks, while still
    maintaining the ability to train the model on harmless tasks and preserving LLM
    capability ([§ 4](#S4 "4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了大量实验证据表明我们的方法可以防止在有害的问答和有毒内容生成任务上进行训练，同时仍保持在无害任务上的训练能力，并保留LLM的能力（[§ 4](#S4
    "4 Experiments ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")）。
- en: (c)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: We empirically investigate “how” our method works and show that it does indeed
    remove information about harmful representations across all layers in the LLM
    ([§ 5](#S5 "5 Mechanistic Analysis ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过实证研究探讨了我们的方法“如何”工作，并显示它确实去除了LLM中所有层次的有害表示的信息（[§ 5](#S5 "5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")）。
- en: '![Refer to caption](img/54797a2f349dd3d14ca8d9c148184408.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54797a2f349dd3d14ca8d9c148184408.png)'
- en: 'Figure 1: Representation Noising pushes harmful representations towards random
    directions, effectively removing their information structure and making it difficult
    to recover harmful representations through HFAs. We visualize this here as a projection
    (PCA) which isn’t able to recover any structure.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：表示噪声将有害的表示推向随机方向，有效地去除它们的信息结构，并使得通过 HFAs 恢复有害表示变得困难。我们在这里通过投影（PCA）进行可视化，但它无法恢复任何结构。
- en: 2 Harmful Fine-tuning and Defence Criteria
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 有害微调和防御标准
- en: Harmful fine-tuning vulnerabilities in LLMs are established in several works
    [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [8](#bib.bib8)]. To formalize
    the problem we borrow from Rosati et al.’s [[15](#bib.bib15)] harmful fine-tuning
    attack threat model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 中的有害微调漏洞在若干工作中已被确立 [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [8](#bib.bib8)]。为了形式化这个问题，我们借鉴了 Rosati 等人的 [[15](#bib.bib15)] 有害微调攻击威胁模型。
- en: 'Harmful Fine-tuning Attack (HFA):'
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有害微调攻击（HFA）：
- en: 'Suppose an attacker has access to a set of model weights for a safety-aligned
    model—such as llama2-7b-chat [[16](#bib.bib16)]—which should refuse the attacker’s
    harmful requests. Let $M_{\theta[t=0]}$ and target responses $Y=\{Y_{i}\}^{n}_{i=1}$
    that minimizes [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣ 2 Harmful
    Fine-tuning and Defence Criteria ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), resulting in a model that is able to behave in
    a way designated harmful by the defender:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设攻击者可以访问一组安全对齐模型的权重——例如 llama2-7b-chat [[16](#bib.bib16)]——该模型应拒绝攻击者的有害请求。设
    $M_{\theta[t=0]}$ 和目标响应 $Y=\{Y_{i}\}^{n}_{i=1}$ 最小化 [eq. 1](#S2.E1 "在有害微调攻击（HFA）中：‣
    2 有害微调和防御标准 ‣ 表示噪声有效地防止对 LLM 的有害微调")，从而得到一个按防御者定义为有害的行为的模型：
- en: '|  | $\theta[t^{*}]=\arg\min_{\theta[t]}\mathbb{E}_{(X,Y)\sim D_{\text{harmful}}}[\mathcal{L}(M_{\theta[t]}(X),Y)],$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta[t^{*}]=\arg\min_{\theta[t]}\mathbb{E}_{(X,Y)\sim D_{\text{harmful}}}[\mathcal{L}(M_{\theta[t]}(X),Y)],$
    |  | (1) |'
- en: where $\mathcal{L}(M_{\theta}(X),Y)$ is a typical causal language modeling loss.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}(M_{\theta}(X),Y)$ 是典型的因果语言建模损失。
- en: 'Harmful Fine-tuning Success: A HFA is formally designated a success if it causes
    the subject model to exceed a chosen threshold $\phi$ than they can afford in
    time and/or compute.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有害微调成功：如果 HFA 使得被试模型超过了选择的阈值 $\phi$，超过了它们在时间和/或计算上能承受的范围，则正式被称为成功。
- en: Immunization Conditions
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 免疫条件
- en: To defend against HFAs, Rosati et al. [[15](#bib.bib15)] provides four conditions
    for a successful defence, called Immunization Conditions (IC). We introduce them
    below in order to motivate our search for a method that fulfils them theoretically
    and experimentally.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防御 HFAs，Rosati 等人 [[15](#bib.bib15)] 提出了四个成功防御的条件，称为免疫条件（IC）。我们在下文中介绍它们，以激励我们寻找理论上和实验上满足这些条件的方法。
- en: (IC1)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (IC1)
- en: 'Resistance: To increase the required effort for the attacker, a defended model
    $M_{\theta}^{*}$'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抵抗性：为了增加攻击者所需的努力，一个防御模型 $M_{\theta}^{*}$
- en: (IC2)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (IC2)
- en: 'Stability: The degree to which the defence preserves helpful (or harmless)
    behaviour of the undefended model. We model this using a reference dataset or
    task $D_{\text{ref}}$. For example this could be ROUGE-1.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定性：防御在多大程度上保持了未防御模型的有用（或无害）行为。我们使用参考数据集或任务 $D_{\text{ref}}$ 来建模这一点。例如，这可以是
    ROUGE-1。
- en: (IC3)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (IC3)
- en: 'Generalization: A defence should work against HFAs using samples not seen during
    the defence process. Given disjoint subsets $D_{\text{harm}},D_{\text{harm}}^{\prime}\in
    D_{\text{harmful}}$.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 泛化：防御应对 HFAs 有效，使用防御过程中未见过的样本。给定不相交的子集 $D_{\text{harm}},D_{\text{harm}}^{\prime}\in
    D_{\text{harmful}}$。
- en: (IC4)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (IC4)
- en: 'Trainability: To retain the adaptability of the defended model, it should be
    trainable on harmless datasets with similar efficiency and effectiveness as the
    undefended model i.e.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可训练性：为了保持防御模型的适应性，它应在无害数据集上具有与未防御模型相似的效率和效果，即。
- en: '|  | $1$2 |  |'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $t_{1}$ is as above with Stability.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $t_{1}$ 如上所述，具有稳定性。
- en: A model that fulfils these conditions is said to be “immunized”. [§ 4](#S4 "4
    Experiments ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") provides an operationalization of these conditions. Below, we provide
    the first effective defence that fulfils all these conditions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这些条件的模型被称为“免疫”。[§ 4](#S4 "4 实验 ‣ 表示噪声有效地防止对 LLM 的有害微调") 提供了这些条件的操作化。以下，我们提供了第一个有效的防御，满足所有这些条件。
- en: 3 Method
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'We propose Representation Noising, a method which fulfils the Immunization
    Criteria by removing harmful information from the model’s internal representations
    before the attacker gains access and performs an HFA. Recall that after the attacker
    has access, the defender cannot intervene. There is evidence that current safety
    methods only suppress or route around preserved harmful representations [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]. This leaves information about harmful tasks
    intact so that it can be easily recovered through HFAs. RepNoise aims to remove
    information about harmful tasks from the model weights, to make it difficult for
    the model to relearn such information in future. RepNoise consists of a three-part
    loss: reduce the predictive information in the weights for generating harmful
    outputs, retain capabilities by minimizing loss on harmless inputs, and push harmful
    representations towards random noise to remove harmful information. Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") presents a high-level intuition of our method.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了表示噪声化（Representation Noising），一种通过在攻击者获取访问权限并执行HFA（隐私攻击）之前，从模型的内部表示中移除有害信息来满足免疫标准的方法。请注意，在攻击者获取访问权限后，防御者无法进行干预。证据表明，当前的安全方法只是抑制或绕过保留的有害表示[[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]。这保留了关于有害任务的信息，使其可以通过HFAs轻松恢复。RepNoise旨在从模型权重中移除有害任务的信息，以使模型在未来难以重新学习这些信息。RepNoise包括三部分损失：减少生成有害输出的权重中的预测信息，通过最小化无害输入的损失来保留能力，以及将有害表示推向随机噪声以移除有害信息。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")展示了我们方法的高层次直觉。
- en: Transition Probabilities and Adversarial Loss
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 转移概率和对抗损失
- en: 'Our goal is to derive a loss function which will minimize the likelihood of
    recovering harmful representations. We are motivated by the observation [[18](#bib.bib18)]
    that the number of training steps taken to fine-tune a model trained on one source
    task (in our case, the initial model $M_{\theta[t=0]}$. The transition probability
    has two components: a *static distance*, which depends on the distance of the
    loss functions between an initial model and a target model that minimizes $\mathcal{L_{D}}$
    and determines the existence of likely paths connecting these two parameter sets.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是推导一个损失函数，该函数将最小化恢复有害表示的可能性。我们受到观察[[18](#bib.bib18)]的启发，该观察表明，用于微调在一个源任务上训练的模型（在我们的案例中，初始模型$M_{\theta[t=0]}$）所采取的训练步骤数。转移概率有两个组成部分：*静态距离*，它取决于初始模型和目标模型之间损失函数的距离，该目标模型最小化$\mathcal{L_{D}}$，并决定连接这两个参数集的可能路径的存在。
- en: '|  | $1$2 |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Note that this is a simplified approximation of Achille et al. [[18](#bib.bib18)],
    appendix [A](#A1 "Appendix A Proofs and Mathematical Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") details this. As the
    defender, we are only able to influence the initial set of weights $\theta[t=0]$.
    Therefore our goal is to find a way to modify the model weights so that we minimize
    Eq. ([2](#S3.E2 "Equation 2 ‣ Transition Probabilities and Adversarial Loss ‣
    3 Method ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs")). Where we have:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这只是Achille等人的一个简化近似[[18](#bib.bib18)]，详细信息见附录[A](#A1 "Appendix A Proofs
    and Mathematical Details ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")。作为防御者，我们只能影响初始权重集$\theta[t=0]$。因此，我们的目标是找到一种方法来修改模型权重，以便我们最小化公式([2](#S3.E2
    "Equation 2 ‣ Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"))。其中：
- en: Theorem 1.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1。
- en: Consider a set of initial weights $\theta[t=0]$ used to represent those inputs
    given the model weights, $\theta$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组初始权重$\theta[t=0]$，用于表示给定模型权重$\theta$的那些输入。
- en: 'A full proof of this is given in appendix [A](#A1 "Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").
    Based on information bottleneck theory [[19](#bib.bib19)], we view multiple-layer
    neural networks as consisting of an encoder which maps the inputs $X$ directly
    by performing gradient ascent, which would decrease both the static distance and
    some of the reachability condition [eq. 2](#S3.E2 "In Transition Probabilities
    and Adversarial Loss ‣ 3 Method ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的证明见附录[A](#A1 "Appendix A Proofs and Mathematical Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")。基于信息瓶颈理论[[19](#bib.bib19)]，我们将多层神经网络视为由一个编码器组成，该编码器通过进行梯度上升直接映射输入$X$，这将减少静态距离和一些可达性条件[eq.
    2](#S3.E2 "In Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")。
- en: '|  | $\displaystyle\ell_{\text{ascent}}$ |  | (3) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ell_{\text{ascent}}$ |  | (3) |'
- en: 'Gradient ascent over harmful samples can degrade overall language modelling
    capabilities, so we add a term to ensure that performance over harmless samples
    is not degraded. In view of the immunization conditions in Section [2](#S2 "2
    Harmful Fine-tuning and Defence Criteria ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"), this ensures stability. Combining them,
    we get an adversarial loss function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对有害样本进行梯度上升可能会降低整体语言建模能力，因此我们添加了一项内容以确保无害样本上的性能不受影响。考虑到第[2](#S2 "2 Harmful Fine-tuning
    and Defence Criteria ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")节中的免疫条件，这确保了稳定性。将它们结合，我们得到一个对抗性损失函数：
- en: '|  | $\displaystyle\mathcal{L}_{\text{Adversarial}}$ |  | (4) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{Adversarial}}$ |  | (4) |'
- en: where $\alpha$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$。
- en: Representation Noising
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表征噪声
- en: As we see later (Section [5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")), simply minimizing adversarial
    loss does not effectively remove harmful information from the model weights (which
    we denote $Z$ can still be high. Consequently, it is possible for model weights
    to retain learned harmful representations (and as a result generate harmful token
    sequences).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如后面所见（第[5](#S5 "5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")节），仅仅最小化对抗性损失并不能有效地从模型权重中去除有害信息（我们用$Z$表示），因此模型权重可能仍保留学习到的有害表征（从而生成有害的标记序列）。
- en: The above data processing inequality implies that minimizing $I(X;Z)$, we get
    the loss function for Representation Noising (RepNoise) which satisfies Theorem [1](#Thmtheorem1
    "Theorem 1\. ‣ Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据处理不等式表明，通过最小化$I(X;Z)$，我们得到表征噪声（RepNoise）的损失函数，该函数满足定理[1](#Thmtheorem1 "Theorem
    1\. ‣ Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")。
- en: '|  | $\displaystyle\mathcal{L}_{\text{RepNoise}}$ |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{RepNoise}}$ |  | (5) |'
- en: We use a layer-wise approach to minimize $\mathcal{L}_{\text{RepNoise}}$, with
    multi-kernel Maximum Mean Discrepancy (MMD) as a replacement for KL divergence
    that allows us to estimate the distribution of harmful representations. Full implementation
    details are in [§ B.1](#A2.SS1 "B.1 Implementation of Representation Noising ‣
    Appendix B Implementation ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用逐层方法来最小化$\mathcal{L}_{\text{RepNoise}}$，用多核最大均值差异（MMD）替代KL散度，以便我们可以估计有害表征的分布。完整的实现细节请见[§
    B.1](#A2.SS1 "B.1 Implementation of Representation Noising ‣ Appendix B Implementation
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")。
- en: 4 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'We perform a series of experiments to evaluate how our defence meets the four
    immunization criteria in Section [2](#S2 "2 Harmful Fine-tuning and Defence Criteria
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"): we
    compare RepNoise with existing defence mechanisms in their ability to make llama-2-7b-chat
    resistant to HFAs [§ 4.1](#S4.SS1 "4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") as well as evaluate
    RepNoise on Stability [§ 4.2](#S4.SS2 "4.2 Stability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"), Trainability [§ 4.3](#S4.SS3
    "4.3 Trainability ‣ 4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), and Generalization [§ 4.4](#S4.SS4 "4.4 Generalization
    ‣ 4 Experiments ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一系列实验，以评估我们的防御如何满足第[2](#S2 "2 Harmful Fine-tuning and Defence Criteria
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")节中的四个免疫标准：我们比较了RepNoise与现有防御机制在使llama-2-7b-chat对HFAs
    [§ 4.1](#S4.SS1 "4.1 Resistance ‣ 4 Experiments ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")的抵抗能力，以及评估RepNoise在稳定性[§ 4.2](#S4.SS2 "4.2
    Stability ‣ 4 Experiments ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")、可训练性[§ 4.3](#S4.SS3 "4.3 Trainability ‣ 4 Experiments ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")和泛化能力[§ 4.4](#S4.SS4
    "4.4 Generalization ‣ 4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")方面的表现。
- en: 4.1 Resistance
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 抵抗力
- en: 'Here we simulate an HFA on llama-2-7b-chat and measure the harmfulness of the
    models and a series of controls before and after these attacks. Appendix [K](#A11
    "Appendix K Additional Models ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") reports similar experiments on llama-2-13b-chat and the
    safety-trained Qwen (0.5B to 7B) series of models. We perform HFAs in two domains:
    harmful question-answering and toxic content generation¹¹1We experimented with
    malicious code generation tasks [[22](#bib.bib22)] but observed base models did
    not guard against malicious code generation to begin with which is a pre-condition
    of performing our defence.. We measure attack strength in terms of the learning
    rate and number of samples used during supervised fine-tuning. Full details on
    our attack settings can be found in [appendix C](#A3 "Appendix C Attack Setting
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们模拟了llama-2-7b-chat上的HFA，并在这些攻击前后测量模型及一系列控制的有害性。附录[K](#A11 "Appendix K
    Additional Models ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")报告了对llama-2-13b-chat和安全训练的Qwen（0.5B到7B）系列模型的类似实验。我们在两个领域进行HFAs：有害问答和有毒内容生成¹¹1我们尝试了恶意代码生成任务[[22](#bib.bib22)]，但观察到基础模型本身就没有对恶意代码生成进行防护，这也是执行我们防御的前提条件。我们以学习率和监督微调中使用的样本数量来衡量攻击强度。有关攻击设置的详细信息，请参见[附录C](#A3
    "Appendix C Attack Setting Details ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")。
- en: To fine-tune for harmful question-answering, we use the BeaverTails harmful
    QA dataset [[23](#bib.bib23)] since it is a very large-scale dataset used in other
    attack literature [[24](#bib.bib24)], where the goal is to train an LLM to generate
    compliant answers to questions belonging to 14 categories of harm such as animal
    abuse and violent crime. For harmfulness evaluation, we use the mean probability
    scores of the harmful label from a harmfulness classifier trained on the BeaverTails
    dataset ([§ D.1.1](#A4.SS1.SSS1 "D.1.1 Harmfulness Classifier ‣ D.1 BeaverTails
    ‣ Appendix D Dataset and Model Details ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")). For toxic content generation, we use the DecodingTrust
    [[25](#bib.bib25)] split of Real Toxicity Prompts (RTP) [[26](#bib.bib26)] to
    fine-tune an LLM to generate highly toxic continuations. We perform toxicity evaluation
    using the mean toxicity scores from the Perspective API [[17](#bib.bib17)] ([appendix D](#A4
    "Appendix D Dataset and Model Details ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对有害问答进行微调，我们使用BeaverTails有害QA数据集[[23](#bib.bib23)]，因为这是一个在其他攻击文献中使用的大规模数据集[[24](#bib.bib24)]，其目标是训练LLM生成符合14种有害类别（如虐待动物和暴力犯罪）的问题答案。对于有害性评估，我们使用从BeaverTails数据集训练的有害性分类器的有害标签的平均概率分数（[§
    D.1.1](#A4.SS1.SSS1 "D.1.1 Harmfulness Classifier ‣ D.1 BeaverTails ‣ Appendix
    D Dataset and Model Details ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")）。对于有毒内容生成，我们使用DecodingTrust [[25](#bib.bib25)] 切分的Real Toxicity
    Prompts (RTP) [[26](#bib.bib26)] 来微调LLM生成高度有毒的延续。我们使用Perspective API [[17](#bib.bib17)]
    的平均毒性分数进行毒性评估（[附录 D](#A4 "Appendix D Dataset and Model Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")）。
- en: 'We compare RepNoise with several safety interventions and controls: the original
    model, a randomly initialised model (using Kaiming initialization [[27](#bib.bib27)]),
    additional safety training, gradient ascent, adversarial loss, and Security Vectors
    [[28](#bib.bib28)]. A randomly initialized model allows us to measure how quickly
    we converge to generating harmful tokens from random initial conditions (training
    a model from scratch). Additional safety training is done by supervised fine-tuning
    the model on refusals to answer 10k unsafe harmful question-answering samples
    from BeaverTails. Gradient ascent uses the loss function in [eq. 3](#S3.E3 "In
    Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"), ([appendix J](#A10 "Appendix
    J Ablation Study ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") shows layer-wise implementation results) for defence. Adversarial loss
    minimizes [eq. 4](#S3.E4 "In Transition Probabilities and Adversarial Loss ‣ 3
    Method ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"),
    and RepNoise minimizes [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣
    2 Harmful Fine-tuning and Defence Criteria ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"). Finally, we implement Security Vectors,
    a defence where the defender does have control over the fine-tuning process. We
    train a LoRA adapter on our harmful dataset and use the frozen adapter during
    the HFA ([appendix F](#A6 "Appendix F Security Vectors ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将RepNoise与几种安全干预措施和对照进行比较：原始模型、一个随机初始化的模型（使用Kaiming初始化[[27](#bib.bib27)]）、额外的安全培训、梯度上升、对抗损失和Security
    Vectors [[28](#bib.bib28)]。随机初始化的模型允许我们测量从随机初始条件（从头开始训练模型）生成有害标记的速度。额外的安全培训是通过在BeaverTails上的10k个不安全的有害问答样本上对模型进行监督微调来完成的。梯度上升使用[eq.
    3](#S3.E3 "In Transition Probabilities and Adversarial Loss ‣ 3 Method ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")中的损失函数，（[附录 J](#A10
    "Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") 显示了逐层实现结果）用于防御。对抗损失最小化[eq. 4](#S3.E4 "In Transition Probabilities
    and Adversarial Loss ‣ 3 Method ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")，而RepNoise最小化[eq. 1](#S2.E1 "In Harmful Fine-tuning
    Attack (HFA): ‣ 2 Harmful Fine-tuning and Defence Criteria ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")。最后，我们实现了Security Vectors，这是一种防御方法，其中防御者对微调过程具有控制权。我们在我们的有害数据集上训练了一个LoRA适配器，并在HFA期间使用冻结的适配器（[附录
    F](#A6 "Appendix F Security Vectors ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")）。'
- en: '| Defence Mechanism |  | $3\times 10^{-5}$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 防御机制 |  | $3\times 10^{-5}$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | Pre-attack | 1k | 10k | 1k | 10k | 1k | 10k |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击前 | 1k | 10k | 1k | 10k | 1k | 10k |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Base: llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Base: llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
- en: '| Random | 0.00 | 0.46 | 0.86 | 0.49 | 0.84 | 0.47 | 0.82 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Random | 0.00 | 0.46 | 0.86 | 0.49 | 0.84 | 0.47 | 0.82 |'
- en: '| Additional safety training | 0.05 | 0.75 | 0.76 | 0.75 | 0.75 | 0.76 | 0.74
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 额外的安全训练 | 0.05 | 0.75 | 0.76 | 0.75 | 0.75 | 0.76 | 0.74 |'
- en: '| Gradient ascent | 0.24 | 0.38 | 0.74 | 0.58 | 0.74 | 0.68 | 0.77 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 梯度上升 | 0.24 | 0.38 | 0.74 | 0.58 | 0.74 | 0.68 | 0.77 |'
- en: '| Adversarial loss | 0.05 | 0.26 | 0.70 | 0.64 | 0.75 | 0.77 | 0.77 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性损失 | 0.05 | 0.26 | 0.70 | 0.64 | 0.75 | 0.77 | 0.77 |'
- en: '| Security Vectors | 0.05 | 0.07 | 0.08 | 0.23 | 0.37 | 0.52 | 0.66 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Security Vectors | 0.05 | 0.07 | 0.08 | 0.23 | 0.37 | 0.52 | 0.66 |'
- en: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
- en: 'Table 1: Average harmfulness classifier scores before and after attacks performed
    using 1k and 10k samples of HarmfulQA from BeaverTails and learning rates $\in\{$.
    Blue indicates successful defence, i.e. lower harmfulness score than the base
    model. RepNoise is the only effective defence.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表格1：使用BeaverTails的1k和10k样本的HarmfulQA执行攻击前后的平均有害性分类器评分以及学习率$\in\{$. 蓝色表示成功的防御，即低于基础模型的有害性评分。RepNoise是唯一有效的防御方法。
- en: Table [1](#S4.T1 "Table 1 ‣ 4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") shows results for the
    harmful QA task. HFAs without any defence mechanism substantially increase the
    harmfulness score (Base) on the base model. Attacks with higher learning rates
    and more data tend to be stronger. This replicates previous results about the
    effectiveness of HFAs at circumventing safety training in LLMs [[8](#bib.bib8),
    [12](#bib.bib12), [29](#bib.bib29), [3](#bib.bib3), [13](#bib.bib13)]. Evaluating
    our defence mechanisms, Security Vectors provides some resistance but RepNoise
    is the only defence method to consistently able to provide significant resistance
    across all attacks (Mann-Whitney $U$).²²2We note that stronger attacks with thorough
    hyperparameter search can still succeed against RepNoise which we discuss in Appendix
    [E.1](#A5.SS1 "E.1 Stronger Attack on RepNoise ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[1](#S4.T1 "Table 1 ‣ 4.1 Resistance ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")展示了有害QA任务的结果。没有任何防御机制的HFA在基础模型上显著提高了有害性评分（基础）。具有更高学习率和更多数据的攻击往往更强。这重复了关于HFA在绕过LLM安全训练方面有效性的先前结果[[8](#bib.bib8),
    [12](#bib.bib12), [29](#bib.bib29), [3](#bib.bib3), [13](#bib.bib13)]。评估我们的防御机制时，Security
    Vectors提供了一些抵抗力，但RepNoise是唯一一个在所有攻击中持续提供显著抵抗的防御方法（Mann-Whitney $U$）。我们注意到，通过彻底的超参数搜索，强攻击仍可能成功对抗RepNoise，我们在附录[E.1](#A5.SS1
    "E.1 Stronger Attack on RepNoise ‣ Appendix E Additional Safety and Harmfulness
    Evaluations ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")中讨论了这一点。
- en: Gradient ascent and adversarial loss offer some resistance for weak attacks,
    but they fail for stronger attacks. We hypothesize that harmful text generation
    is recovered quickly with these approaches because they leave the representation
    structure of harmful text sequeces intact (see [§ 5](#S5 "5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    Randomly initializing an LLM is not a useful control for understanding HFAs, since
    simply fine-tuning with larger samples makes the model mimic harmful text from
    the dataset. Finally, additional safety training offers no resistance, indicating
    that some types of traditional safety methods (safety-oriented supervised fine-tuning)
    does not help to defend against HFAs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度上升和对抗性损失对弱攻击提供了一些抵抗，但对强攻击无效。我们假设有害文本生成在这些方法下迅速恢复，因为它们保持了有害文本序列的表示结构（见[§ 5](#S5
    "5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")）。随机初始化LLM对于理解HFA不是一个有用的控制方法，因为简单的使用较大样本的微调使模型模拟数据集中有害文本。最后，额外的安全训练提供的抵抗力为零，表明某些类型的传统安全方法（以安全为导向的监督微调）不能有效防御HFA。
- en: Table [2](#S4.T2 "Table 2 ‣ 4.1 Resistance ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") presents similar results
    for the toxic content generation task. In this case, there are 351 attack samples,
    so we vary attack strength across learning rates only, performing all HFAs for
    4 epochs. In each setting, using a model immunized with RepNoise results in complete
    resistance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 4.1 抵抗 ‣ 4 实验 ‣ 表示噪声有效防止有害微调") 展示了毒性内容生成任务的类似结果。在这种情况下，有
    351 个攻击样本，因此我们仅在学习率上调整攻击强度，执行所有 HFAs 进行 4 个周期。在每种设置下，使用 RepNoise 免疫的模型能完全抵抗。
- en: '|  | Pre-attack | $3\times 10^{-5}$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击前 | $3\times 10^{-5}$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Base | 0.24 | 0.40 | 0.74 | 0.71 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 0.24 | 0.40 | 0.74 | 0.71 |'
- en: '| RepNoise | 0.17 | 0.00 | 0.05 | 0.07 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.17 | 0.00 | 0.05 | 0.07 |'
- en: 'Table 2: Toxicity score from Perspective API when the model is requested to
    continue highly toxic prompts. RepNoise is able to defend against training models
    for toxic content generation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：当模型被请求继续生成高度有毒的提示时，Perspective API 的毒性评分。RepNoise 能够防御针对有毒内容生成的训练模型。
- en: 4.2 Stability
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 稳定性
- en: 'To evaluate if RepNoise causes a deterioration in unrelated harmless tasks
    compared to the base model, we use standard LLM benchmarks from the Eleuther AI
    LM Evaluation Harness [[30](#bib.bib30)]: TruthfulQA [[31](#bib.bib31)], MMLU
    [[32](#bib.bib32)], Hellaswag [[33](#bib.bib33)], and ARC-easy [[34](#bib.bib34)].
    We also evaluate changes in the model’s capabilities on domains related to harmfulness
    using the Ethics [[35](#bib.bib35)] and CrowS-Pairs [[36](#bib.bib36)] datasets.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估 RepNoise 是否导致与基础模型相比在无关无害任务上的性能下降，我们使用来自 Eleuther AI LM 评估工具的标准 LLM 基准 [[30](#bib.bib30)]：TruthfulQA
    [[31](#bib.bib31)]，MMLU [[32](#bib.bib32)]，Hellaswag [[33](#bib.bib33)] 和 ARC-easy
    [[34](#bib.bib34)]。我们还评估了模型在与有害性相关的领域上的能力变化，使用 Ethics [[35](#bib.bib35)] 和 CrowS-Pairs
    [[36](#bib.bib36)] 数据集。
- en: '| Model | TruthfulQA | MMLU | Hellaswag | Winogrande | ARC | Ethics | CrowS
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TruthfulQA | MMLU | Hellaswag | Winogrande | ARC | Ethics | CrowS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Base | 0.38 | 0.46 | 0.58 | 0.66 | 0.74 | 0.59 | 0.64 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 0.38 | 0.46 | 0.58 | 0.66 | 0.74 | 0.59 | 0.64 |'
- en: '| RepNoise | 0.37 | 0.45 | 0.57 | 0.66 | 0.72 | 0.60 | 0.63 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.37 | 0.45 | 0.57 | 0.66 | 0.72 | 0.60 | 0.63 |'
- en: 'Table 3: Evaluation of RepNoise on common language model capability benchmarks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：对 RepNoise 在常见语言模型能力基准上的评估。
- en: Table [3](#S4.T3 "Table 3 ‣ 4.2 Stability ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") shows that a llama2-7b-chat
    model immunized using RepNoise achieves similar scores as the base model across
    all evaluations, indicating that RepNoise does not degrade capability. Beyond
    performance evaluations, our method does not degrade performance on other safety
    benchmarks, i.e. Ethics, or CrowS-Pairs. We perform further investigations on
    whether RepNoise has any effect on fairness ([§ E.4](#A5.SS4 "E.4 Bias Evaluation
    with ROBBIE ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs")), exaggerated safety
    ([§ E.5](#A5.SS5 "E.5 Exaggerated Safety with XSTest ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")), or adversarial robustness ([§ E.6](#A5.SS6 "E.6 Adversarial
    Attacks with HarmBench ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"))—with
    the general finding that RepNoise neither degrades nor improves inference-time
    safety over a baseline safety-guarded model which implies that RepNoise would
    supplement rather than replace other defence methods.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 4.2 稳定性 ‣ 4 实验 ‣ 表示噪声有效防止有害微调") 显示，使用 RepNoise 免疫的 llama2-7b-chat
    模型在所有评估中都达到了与基础模型相似的分数，表明 RepNoise 不会降低能力。除了性能评估之外，我们的方法在其他安全基准（即 Ethics 或 CrowS-Pairs）上的表现也没有下降。我们进一步调查了
    RepNoise 是否对公平性（[§ E.4](#A5.SS4 "E.4 使用 ROBBIE 评估偏见 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止有害微调")）、夸大的安全性（[§
    E.5](#A5.SS5 "E.5 使用 XSTest 评估夸大安全性 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止有害微调")）或对抗性鲁棒性（[§
    E.6](#A5.SS6 "E.6 使用 HarmBench 评估对抗攻击 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止有害微调")）产生影响——总体发现
    RepNoise 既不会降低也不会改善推理时的安全性，这意味着 RepNoise 更可能作为补充而非替代其他防御方法。
- en: 4.3 Trainability
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 可训练性
- en: We evaluate Trainability by testing whether the defended model can still be
    trained towards harmless tasks. To this end, we measure the ROUGE-1 unigram overlap
    score on several text-to-data tasks from the GEM benchmark [[37](#bib.bib37)].
    In order to demonstrate trainability, we need to choose standard validated tasks
    for natural language generation that models are poor (zero-shot) at before training
    (very low ROUGE-1 scores) and achieve large performance increases in after training.
    We observe this for the base llama2-7b-chat model seeing consistently low initial
    scores in [table 4](#S4.T4 "In 4.3 Trainability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"). For this setting,
    we train the base model and its post-RepNoise version using 1 epoch and a learning
    rate of $8\times 10^{-5}$, using only the training splits of each dataset. We
    perform evaluations on the test splits of respective datasets. Full details of
    each dataset are given in [appendix D](#A4 "Appendix D Dataset and Model Details
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs").
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过测试防御模型是否仍能用于无害任务来评估训练能力。为此，我们测量了GEM基准[[37](#bib.bib37)]上几个文本到数据任务的ROUGE-1
    unigram重叠得分。为了证明训练能力，我们需要选择自然语言生成中标准验证的任务，这些任务在训练前模型表现较差（零样本情况下ROUGE-1得分非常低），并在训练后取得大幅性能提升。我们在[表4](#S4.T4
    "在4.3训练能力 ‣ 4实验 ‣ 表示噪声有效防止对LLMs的有害微调")中观察到这一点。对于这个设置，我们使用1轮训练和$8\times 10^{-5}$的学习率来训练基础模型及其RepNoise后版本，仅使用每个数据集的训练分割。我们在相应数据集的测试分割上进行评估。每个数据集的详细信息见[附录D](#A4
    "附录D 数据集和模型详情 ‣ 表示噪声有效防止对LLMs的有害微调")。
- en: The results in [table 4](#S4.T4 "In 4.3 Trainability ‣ 4 Experiments ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") show that a llama2-7b-chat
    model hardened using RepNoise retains the capability to be further trained on
    harmless tasks, despite not being able to be trained on harmful tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](#S4.T4 "在4.3训练能力 ‣ 4实验 ‣ 表示噪声有效防止对LLMs的有害微调")中的结果表明，使用RepNoise强化的llama2-7b-chat模型保留了进一步训练于无害任务的能力，尽管无法在有害任务上进行训练。'
- en: '| Model | ViGGO | E2E NLG | DART | CACAPO | ConvWeather |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ViGGO | E2E NLG | DART | CACAPO | ConvWeather |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Base | 0.19 / 0.83 | 0.20 / 0.74 | 0.23 / 0.53 | 0.18 / 0.66 | 0.06 / 0.25
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 0.19 / 0.83 | 0.20 / 0.74 | 0.23 / 0.53 | 0.18 / 0.66 | 0.06 / 0.25
    |'
- en: '| RepNoise | 0.20 / 0.83 | 0.25 / 0.74 | 0.25 / 0.53 | 0.18 / 0.67 | 0.08 /
    0.25 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.20 / 0.83 | 0.25 / 0.74 | 0.25 / 0.53 | 0.18 / 0.67 | 0.08 /
    0.25 |'
- en: 'Table 4: ROUGE-1 score of RepNoise on GEM structured generation tasks before/after
    being fine-tuned.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：RepNoise在GEM结构化生成任务上的ROUGE-1得分，调整前/调整后。
- en: 4.4 Generalization
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 泛化
- en: The BeaverTails dataset categorizes samples into 14 types of harm. We evaluate
    the generalization performance of RepNoise by withholding five categories of harm
    when performing the defence and then evaluate the attack by performing an attack
    using 1k samples from that subset. We also perform an additional experiment (Half)
    where RepNoise is applied using 5k randomly selected samples from BeaverTails
    and a subsequent attack is performed using 5k unseen samples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: BeaverTails数据集将样本分为14种危害类型。我们通过在防御时保留五种危害类型来评估RepNoise的泛化性能，然后通过使用该子集中的1k样本进行攻击来评估攻击效果。我们还进行了一项额外实验（Half），在该实验中，RepNoise应用于从BeaverTails中随机选择的5k样本，然后使用5k未见样本进行后续攻击。
- en: '| LR | Model | Crime | Privacy | Toxic | Violence | Sexually explicit | Half
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LR | 模型 | 犯罪 | 隐私 | 有毒 | 暴力 | 色情 | Half |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $3\times 10^{-5}$ | Base | 0.49 | 0.51 | 0.40 | 0.52 | 0.53 | 0.35 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| $3\times 10^{-5}$ | 基础 | 0.49 | 0.51 | 0.40 | 0.52 | 0.53 | 0.35 |'
- en: '|  | RepNoise | 0.08 | 0.05 | 0.06 | 0.09 | 0.01 | 0.08 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | RepNoise | 0.08 | 0.05 | 0.06 | 0.09 | 0.01 | 0.08 |'
- en: '| $6\times 10^{-5}$ | Base | 0.76 | 0.75 | 0.76 | 0.75 | 0.81 | 0.76 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| $6\times 10^{-5}$ | 基础 | 0.76 | 0.75 | 0.76 | 0.75 | 0.81 | 0.76 |'
- en: '|  | RepNoise | 0.10 | 0.09 | 0.10 | 0.09 | 0.00 | 0.12 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | RepNoise | 0.10 | 0.09 | 0.10 | 0.09 | 0.00 | 0.12 |'
- en: '| $8\times 10^{-5}$ | Base | 0.77 | 0.75 | 0.80 | 0.74 | 0.76 | 0.74 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| $8\times 10^{-5}$ | 基础 | 0.77 | 0.75 | 0.80 | 0.74 | 0.76 | 0.74 |'
- en: '|  | RepNoise | 0.13 | 0.12 | 0.12 | 0.14 | 0.00 | 0.10 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | RepNoise | 0.13 | 0.12 | 0.12 | 0.14 | 0.00 | 0.10 |'
- en: 'Table 5: Harmfulness scores after performing fine-tuning on harm types withheld
    during the RepNoise defence.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在RepNoise防御期间对保留的危害类型进行微调后的有害性得分。
- en: The results in Table [5](#S4.T5 "Table 5 ‣ 4.4 Generalization ‣ 4 Experiments
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") show
    that a defence using RepNoise is able to generalize to a defence against HFAs
    performed with unseen samples and unseen types of harm.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S4.T5 "表 5 ‣ 4.4 泛化 ‣ 4 实验 ‣ 表示噪声有效防止 LLMs 上有害的微调") 的结果显示，使用 RepNoise
    的防御能够推广到对未知样本和未知类型的有害行为的防御。
- en: 5 Mechanistic Analysis
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 机制分析
- en: We conjecture that RepNoise is effective because it reduces the information
    about harmfulness in representations across all layers of the neural network,
    making them harder to recover. This is inspired by observations from previous
    studies, which found that popular safety methods merely route around the harmful
    representations [[11](#bib.bib11)], that fine-tuning only learns a wrapper on
    top of existing representations [[10](#bib.bib10)], and that harmful representations
    are easily recovered [[9](#bib.bib9)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们猜测 RepNoise 是有效的，因为它减少了神经网络所有层中关于有害性的表示信息，使其更难恢复。这受到之前研究的启发，这些研究发现流行的安全方法仅仅绕过了有害的表示
    [[11](#bib.bib11)]，微调仅在现有表示之上学习了一个外包装 [[10](#bib.bib10)]，有害的表示容易恢复 [[9](#bib.bib9)]。
- en: '![Refer to caption](img/f7ace647cc32b85859326401058f6a05.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f7ace647cc32b85859326401058f6a05.png)'
- en: 'Figure 2: $L_{2}$ distance between weights of each layer between the base model,
    a successfully attacked model and two defences. RepNoise’s differences spread
    through the layers compared to Adversarial loss where the weight differences are
    concentrated at the later layers indicative of superficial defence.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 各层之间权重的 $L_{2}$ 距离，比较基准模型、成功攻击的模型和两种防御。RepNoise 的差异分布在各层之间，而对抗性损失的权重差异集中在后面的层，表明防御较为表面化。'
- en: '|  | $3\times 10^{-5}$ @ 1k |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $3\times 10^{-5}$ @ 1k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Undefended Model | 0.47 | 0.74 | 0.73 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 未防御模型 | 0.47 | 0.74 | 0.73 |'
- en: '| All Layers | 0.08 | 0.12 | 0.10 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 全层 | 0.08 | 0.12 | 0.10 |'
- en: '| Freeze LM Head | 0.08 | 0.10 | 0.11 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 冻结 LM Head | 0.08 | 0.10 | 0.11 |'
- en: '| Freeze Last Layer | 0.08 | 0.67 | 0.09 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 冻结最后一层 | 0.08 | 0.67 | 0.09 |'
- en: '| Freeze Layers 20-31 | 0.10 | 0.13 | 0.10 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 20-31 | 0.10 | 0.13 | 0.10 |'
- en: '| Freeze Layers 10-20 | 0.13 | 0.55 | 0.56 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 10-20 | 0.13 | 0.55 | 0.56 |'
- en: '| Freeze Layers 0-10 | 0.73 | 0.73 | 0.72 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 0-10 | 0.73 | 0.73 | 0.72 |'
- en: 'Table 6: Freezing earlier layers prevents effective defence indicating that
    the ‘depth’ of the defence is critical.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 冻结早期层阻止有效防御，表明防御的“深度”是关键。'
- en: Model Weights
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型权重
- en: To illustrate the above conjecture, we measure the change in the weights of
    each layer across various defence mechanisms (a method common in unlearning literature,
    see Tarun et al. [[38](#bib.bib38)] for example). In Fig. [2](#S5.F2 "Figure 2
    ‣ 5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"), we plot layer-wise $L_{2}$ @ 10k samples). We observe that
    defence using adversarial loss is indeed “superficial,” in that the largest difference
    is observed in the last layers. In comparison, weight change across layers is
    more uniform for RepNoise.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明上述猜想，我们测量了各层在不同防御机制下的权重变化（这是在去学习文献中常见的一种方法，见 Tarun 等人 [[38](#bib.bib38)]）。在图
    [2](#S5.F2 "图 2 ‣ 5 机制分析 ‣ 表示噪声有效防止 LLMs 上有害的微调") 中，我们绘制了各层 $L_{2}$ @ 10k 样本的图。我们观察到，使用对抗性损失的防御确实是“表面化”的，因为最大的差异出现在最后的层。相比之下，RepNoise
    的各层权重变化更为均匀。
- en: However, we can’t be certain what these weight changes mean. In order to actually
    test our conjecture about depth, we perform RepNoise but freeze the top layers,
    the middle 10 layers, and the earliest 10 layers ([table 6](#S5.T6 "In 5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs")). Freezing the LM Head or the layers between 20 and 31 makes little to
    no difference and not much difference for lower sample sizes freezing the last
    layer, freezing the middle layers degrades the performance of RepNoise, and freezing
    the earliest layers results in a complete lack of defence. This result confirms
    our conjecture about the necessity of “depth” for effective defence.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们无法确定这些权重变化的意义。为了实际测试我们关于深度的猜想，我们执行了 RepNoise，但冻结了顶层、中间 10 层和最早的 10 层（[表
    6](#S5.T6 "在 5 机制分析 ‣ 表示噪声有效防止 LLMs 上有害的微调")）。冻结 LM Head 或 20 到 31 层之间的层几乎没有区别，对于较低的样本量，冻结最后一层的影响不大，冻结中间层会降低
    RepNoise 的性能，而冻结最早的层则完全没有防御效果。这一结果确认了我们关于有效防御所需“深度”的猜想。
- en: Token Probabilities
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标记概率
- en: To investigate the degree to which harmful representations are removed across
    layers we can look at how harmful and harmless token sequences are promoted throughout
    the network. We look at the mean log probability of 100 randomly selected harmful
    and harmless samples throughout the layers by placing the language model head
    on the activations across each layer [[39](#bib.bib39)]. Confirming our findings
    above (Fig. [3](#S5.F3 "Figure 3 ‣ Token Probabilities ‣ 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")) adversarial
    loss leads to a shallow defence that mostly reduces the likelihood of the harmful
    token sequences towards the last layer. In contrast, RepNoise demotes harmful
    tokens across layers mostly uniformly.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究有害表征在各层中被移除的程度，我们可以观察有害和无害标记序列在整个网络中的推广情况。我们通过在每一层的激活上放置语言模型头，查看100个随机选择的有害和无害样本在各层中的平均对数概率[[39](#bib.bib39)]。确认了我们之前的发现（图
    [3](#S5.F3 "Figure 3 ‣ Token Probabilities ‣ 5 Mechanistic Analysis ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs)），对抗性损失导致了浅层防御，这种防御主要减少了有害标记序列在最后一层的可能性。相比之下，RepNoise在各层中大多均匀地降低了有害标记的权重。
- en: '![Refer to caption](img/f8d3bbbb1d1c3ba70cb6d8e11845cf5e.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f8d3bbbb1d1c3ba70cb6d8e11845cf5e.png)'
- en: 'Figure 3: Log probability of harmful and harmless sequences across layers.
    Notice how adversarial loss mostly depromotes harmful tokens towards the last
    layer. This is done more evenly across layers for RepNoise indicating comprehensive
    and deep information removal.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 各层中有害和无害序列的对数概率。注意对抗性损失主要减少了最后一层中的有害标记。这在RepNoise中更加均匀地完成，显示出全面和深度的信息移除。'
- en: Knowledge Representations
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识表征
- en: Fig. [4](#S5.F4 "Figure 4 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") illustrates
    the representation space learned by each model. After shallow defences, harmful
    representations maintain distinct structures along each of the two principal component
    directions. While RepNoise maintains separability between harmful and harmless
    sequences along one of the principal components, the “spread" of each harmful
    representation in both directions is dramatically reduced compared other models.
    This corroborates that RepNoise has reduced the representation quality of the
    harmful samples since we can’t find a projection that illustrates any meaningful
    structure between these samples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S5.F4 "Figure 4 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") 展示了每个模型学习到的表征空间。在浅层防御之后，有害表征在两个主成分方向上保持了明显的结构。虽然RepNoise在一个主成分方向上保持了有害和无害序列之间的可分性，但每个有害表征在两个方向上的“扩展”显著减少，与其他模型相比。这证实了RepNoise降低了有害样本的表征质量，因为我们找不到能展示这些样本之间任何有意义结构的投影。
- en: '![Refer to caption](img/a4ee070006495715653ff0658e14e8f4.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a4ee070006495715653ff0658e14e8f4.png)'
- en: 'Figure 4: PCA across 100 harmful and harmless samples from BeaverTails on the
    activations of the last layer.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 从BeaverTails的最后一层激活中对100个有害和无害样本进行PCA分析。'
- en: To further analyze the information about harmfulness contained in the representations
    of different models, we train a linear probe to predict whether an input to a
    model was harmful or not, based on the mean activations at each layer of the model.
    Such a probe can achieve high accuracy by using the information about harmfulness
    in the LLM. For each model, we input 15k examples from BeaverTails, with half
    being unsafe, and collect the average activations across each layer for each sample.
    We then train a binary linear classifier on 80% of them, measure the resulting
    accuracy on a held-out test set, and repeat with 10 random seeds. ³³3We used an
    earlier RepNoise trained with $\beta=4$, additional results presented in [appendix G](#A7
    "Appendix G Harmfulness probes ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析不同模型表征中包含的有害信息，我们训练了一个线性探测器，以预测模型的输入是否有害，基于模型每层的平均激活值。这样的探测器可以通过利用LLM中的有害信息实现高准确度。对于每个模型，我们输入15k个来自BeaverTails的示例，其中一半是不安全的，并收集每个样本在每一层的平均激活值。然后，我们在80%的样本上训练一个二分类线性分类器，测量在保留的测试集上的准确度，并用10个随机种子重复实验。³³3我们使用了早期用$\beta=4$训练的RepNoise，附加结果见[附录
    G](#A7 "Appendix G Harmfulness probes ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")。
- en: '![Refer to caption](img/04afe2ed8294b69d7ca33fa8c3bd5e97.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/04afe2ed8294b69d7ca33fa8c3bd5e97.png)'
- en: (a)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b9bd7b9b2e224e03f7f321659b3fab3e.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b9bd7b9b2e224e03f7f321659b3fab3e.png)'
- en: (b)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/5985fc844dfd315bdc41be10d3ec79cd.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5985fc844dfd315bdc41be10d3ec79cd.png)'
- en: (c)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 5: Harmful probe accuracy on (a) base model and attacked model, (b)
    base model and models trained with RepNoise ($beta=4$) and adversarial Loss, and
    (c) base model, RepNoise model and an attacked RepNoise model'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：有害探针准确率（a）基础模型和攻击模型，（b）基础模型与使用RepNoise（$beta=4$）和对抗损失训练的模型，以及（c）基础模型、RepNoise模型和攻击后的RepNoise模型
- en: Across all models, the probes perform best in the middle layers and very poorly
    in earlier layers. Figure [5(a)](#S5.F5.sf1 "Figure 5(a) ‣ Figure 5 ‣ Knowledge
    Representations ‣ 5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") shows that an HFA does not improve the
    probe’s accuracy compared to the base model. Similarly, an attack on a model defended
    by RepNoise also does not increase the information about harmfulness (Figure [5(c)](#S5.F5.sf3
    "Figure 5(c) ‣ Figure 5 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")). This
    indicates that HFAs do not make an LLM more harmful by learning new information
    about harmfulness, but merely use information already contained in the model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有模型中，探针在中间层表现最佳，而在较早层表现非常差。图[5(a)](#S5.F5.sf1 "Figure 5(a) ‣ Figure 5 ‣ Knowledge
    Representations ‣ 5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")显示，与基础模型相比，HFA并没有提高探针的准确率。同样，对RepNoise防御的模型的攻击也不会增加有害信息（见图[5(c)](#S5.F5.sf3
    "Figure 5(c) ‣ Figure 5 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")）。这表明HFAs并不会通过学习有害性的新增信息使LLM变得更有害，而只是利用了模型中已经包含的信息。
- en: The probe achieves significantly (Student’s $t$) lower accuracy on the model
    defended by RepNoise than for a model defended by adversarial loss or the base
    model (Figure [5(b)](#S5.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ Knowledge Representations
    ‣ 5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")). This supports our suggestion that adversarial loss does
    not remove information about harmful representations from the base model, but
    RepNoise does. Lastly, Figure [5(c)](#S5.F5.sf3 "Figure 5(c) ‣ Figure 5 ‣ Knowledge
    Representations ‣ 5 Mechanistic Analysis ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") illustrates that fine-tuning using harmful
    data does not result in relearning the information removed by RepNoise.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 探针在RepNoise防御的模型上实现了显著（Student’s $t$）较低的准确率，相比于用对抗损失或基础模型防御的模型（见图[5(b)](#S5.F5.sf2
    "Figure 5(b) ‣ Figure 5 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")）。这支持了我们的建议，即对抗损失并不能从基础模型中移除有害表示的信息，而RepNoise可以。最后，图[5(c)](#S5.F5.sf3
    "Figure 5(c) ‣ Figure 5 ‣ Knowledge Representations ‣ 5 Mechanistic Analysis ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")表明，使用有害数据进行微调不会导致重新学习被RepNoise移除的信息。
- en: 6 Related Work
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Defence against harmful fine-tuning
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 防御有害微调
- en: Few works have attempted to defend against HFAs. Meta-learning approaches have
    been used to reduce the fine-tunability of Language Models for harmful classification
    tasks [[40](#bib.bib40)] and prevent image classification models from being fine-tuned
    on restricted domains [[41](#bib.bib41)]. However, meta-learning approaches can
    be uninterpretable and too computationally expensive for LLMs. [[28](#bib.bib28)]
    added a security vector during training to trick a model into thinking that it
    has already learned the harmful task. [[24](#bib.bib24)] keep embeddings close
    to the original embeddings by adding a perturbation loss called ‘Vaccination’.
    While [[28](#bib.bib28), [24](#bib.bib24)] assume the defender retains control
    over the fine-tuning process, we focus on settings where the defender cannot intervene
    after the weights are released or stolen. For a full review of current HFAs and
    threat models, we refer readers to [[15](#bib.bib15)].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有工作尝试防御HFAs。元学习方法已被用来减少语言模型在有害分类任务上的微调能力[[40](#bib.bib40)]，并防止图像分类模型在受限领域的微调[[41](#bib.bib41)]。然而，元学习方法可能难以解释且对LLMs计算开销过大。[[28](#bib.bib28)]在训练过程中添加了安全向量，以使模型误以为已经学习了有害任务。[[24](#bib.bib24)]通过添加名为‘疫苗接种’的扰动损失，使嵌入接近原始嵌入。虽然[[28](#bib.bib28),
    [24](#bib.bib24)]假设防御者仍能控制微调过程，但我们关注的是防御者无法在权重发布或被盗后干预的情况。有关当前HFAs和威胁模型的全面回顾，请参见[[15](#bib.bib15)]。
- en: Unlearning
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 退学习
- en: While Unlearning is concerned with forgetting a particular sample, class, or
    concept, we focus on whole task domains. Some works do measure the ‘relearn time’
    of an unlearned sample [[38](#bib.bib38), [42](#bib.bib42)], which is similar
    to Resistance. A few papers also relate the information content in the model weights
    to unlearning [[42](#bib.bib42), [43](#bib.bib43)], but are too expensive to apply
    to LLMs. Some sample unlearning methods also use noise to achieve unlearning [[44](#bib.bib44),
    [38](#bib.bib38), [45](#bib.bib45)], but cannot be applied to whole domains. We
    initially explored additional closed-form unlearning methods [[46](#bib.bib46),
    [47](#bib.bib47)], but they did not work for our natural language generation case
    since they resulted in complete model degradation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然去学习关注的是忘记特定样本、类别或概念，但我们关注的是整个任务领域。一些研究确实测量了未学习样本的“重新学习时间” [[38](#bib.bib38),
    [42](#bib.bib42)]，这与抵抗性类似。一些论文还将模型权重中的信息内容与去学习联系起来 [[42](#bib.bib42), [43](#bib.bib43)]，但这些方法应用于大型语言模型（LLMs）过于昂贵。一些样本去学习方法也使用噪声来实现去学习
    [[44](#bib.bib44), [38](#bib.bib38), [45](#bib.bib45)]，但不能应用于整个领域。我们最初探索了额外的闭式去学习方法
    [[46](#bib.bib46), [47](#bib.bib47)]，但由于它们导致模型完全退化，因此在我们的自然语言生成案例中不起作用。
- en: Domain Authorization and Negative Transfer
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 域授权和负迁移
- en: Finally, our method can be seen as a specialized form of domain authorization
    [[48](#bib.bib48), [20](#bib.bib20), [41](#bib.bib41)] where a model provider
    wants to authorize a model for only a specific use either at inference or training
    time. While we focus on harmful uses, we believe that our method would be applicable
    to specialized training-time authorization on any domain. Negative Transfer studies
    poor training-time transfer across domains [[49](#bib.bib49)]. While research
    usually aims to reduce it, increasing negative transfer might inspire future immunization
    methods. Our theoretical approach relies on work in domain adaptation and transfer
    learning [[18](#bib.bib18)] which poses a transition probability that our loss
    function attempts to minimize. Continuations of this theoretical analysis could
    aim to develop methods with guarantees of resistance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们的方法可以被视为一种特殊形式的域授权 [[48](#bib.bib48), [20](#bib.bib20), [41](#bib.bib41)]，其中模型提供者希望仅在推理或训练时将模型授权用于特定用途。虽然我们重点关注有害用途，但我们认为我们的方法也适用于任何领域的专门训练时间授权。负迁移研究跨域的差训练时间
    [[49](#bib.bib49)]。虽然研究通常旨在减少负迁移，但增加负迁移可能会激发未来的免疫方法。我们的方法论依赖于领域适应和迁移学习的工作 [[18](#bib.bib18)]，这些工作提出了我们的损失函数试图最小化的转移概率。这一理论分析的延续可以旨在开发具有抵抗力保证的方法。
- en: 7 Limitations
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个限制
- en: The primary limitation of RepNoise is that it is still possible to find ways
    to defeat it at higher learning rates and with more data ([§ E.1](#A5.SS1 "E.1
    Stronger Attack on RepNoise ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    It is also sensitive to variations in hyperparameter choices ([appendix J](#A10
    "Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")). We have evidence that RepNoise could be improved quite
    simply by doing more comprehensive hyperparameter searches and constructing larger
    defensive datasets. However, our method requires paired safe and unsafe examples
    which makes data collection more expensive and complex. Finally, while we did
    demonstrate comprehensive generalization across harmful subsets in question-answering
    tasks, we did not observe generalization from defences on harmful question-answering
    to attacks using toxic content generation ([§ E.2](#A5.SS2 "E.2 Cross-Domain Generalization
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"))—as such, future work should
    focus on improving cross-domain defence.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: RepNoise 的主要限制在于在较高学习率和更多数据下仍然可能找到击败它的方法 ([§ E.1](#A5.SS1 "E.1 更强攻击 RepNoise
    ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效地防止了对 LLMs 的有害微调"))。它对超参数选择的变化也很敏感 ([附录 J](#A10 "附录
    J 消融研究 ‣ 表示噪声有效地防止了对 LLMs 的有害微调"))。我们有证据表明，通过更全面的超参数搜索和构建更大的防御数据集，RepNoise 可以得到相当简单的改进。然而，我们的方法需要配对的安全和不安全样本，这使得数据收集变得更加昂贵和复杂。最后，虽然我们确实展示了在问答任务中对有害子集的全面泛化，但我们没有观察到从有害问答防御到使用有毒内容生成的攻击的泛化
    ([§ E.2](#A5.SS2 "E.2 跨域泛化 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效地防止了对 LLMs 的有害微调"))——因此，未来的工作应集中于提高跨域防御能力。
- en: While our empirical settings and attacks provide promising first directions
    for LLM immunization research, future work should invest in stronger attack settings
    to emulate worst-case attacks and investigate different types of harm. Finally,
    our work is limited to supervised fine-tuning attacks in LLMs. Additional settings
    in different modalities such as evaluating attempts at developing malicious agents
    through harmful reinforcement learning (e.g., “reverse DPO” [[14](#bib.bib14)])
    are a critical topic for future research.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的实证设置和攻击为LLM免疫研究提供了有前景的初步方向，但未来的工作应投资于更强大的攻击设置，以模拟最坏情况的攻击，并调查不同类型的危害。最后，我们的工作仅限于LLMs的监督微调攻击。评估通过有害强化学习（例如，“反向DPO”[[14](#bib.bib14)]）开发恶意代理的尝试等不同模态的额外设置是未来研究的关键主题。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢和资金披露
- en: We acknowledge the support of the Killam foundation, Digital Research Alliance
    of Canada, and the Vector institute. FR is supported by a Canada CIFAR Chair in
    AI.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Killam基金会、加拿大数字研究联盟和Vector研究所的支持。FR由加拿大CIFAR人工智能主席资助。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Chan, B. Bucknall, H. Bradley, and D. Krueger, “Hazards from increasingly
    accessible fine-tuning of downloadable foundation models,” 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Chan, B. Bucknall, H. Bradley, 和 D. Krueger，“日益易得的可下载基础模型的微调危险”，2023年。'
- en: '[2] A. Gopal, N. Helm-Burger, L. Justen, E. H. Soice, T. Tzeng, G. Jeyapragasan,
    S. Grimm, B. Mueller, and K. M. Esvelt, “Will releasing the weights of future
    large language models grant widespread access to pandemic agents?,” 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Gopal, N. Helm-Burger, L. Justen, E. H. Soice, T. Tzeng, G. Jeyapragasan,
    S. Grimm, B. Mueller, 和 K. M. Esvelt，“未来大语言模型的权重发布是否会广泛获取大流行病代理？”，2023年。'
- en: '[3] K. Pelrine, M. Taufeeque, M. Zajc, E. McLean, and A. Gleave, “Exploiting
    novel gpt-4 apis,” arXiv preprint arXiv:2312.14302, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. Pelrine, M. Taufeeque, M. Zajc, E. McLean, 和 A. Gleave，“利用新颖的GPT-4 API”，arXiv预印本
    arXiv:2312.14302，2023年。'
- en: '[4] N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper,
    K. Lee, M. Jagielski, M. Nasr, A. Conmy, E. Wallace, D. Rolnick, and F. Tramèr,
    “Stealing part of a production language model,” 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] N. Carlini, D. Paleka, K. D. Dvijotham, T. Steinke, J. Hayase, A. F. Cooper,
    K. Lee, M. Jagielski, M. Nasr, A. Conmy, E. Wallace, D. Rolnick, 和 F. Tramèr，“窃取生产语言模型的一部分”，2024年。'
- en: '[5] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and
    D. Xiong, “Large language model alignment: A survey,” 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, 和 D.
    Xiong，“大语言模型对齐：调查”，2023年。'
- en: '[6] E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham,
    D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan, C. Anil,
    D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, K. Sachan, M. Sellitto,
    M. Sharma, N. DasSarma, R. Grosse, S. Kravec, Y. Bai, Z. Witten, M. Favaro, J. Brauner,
    H. Karnofsky, P. Christiano, S. R. Bowman, L. Graham, J. Kaplan, S. Mindermann,
    R. Greenblatt, B. Shlegeris, N. Schiefer, and E. Perez, “Sleeper agents: Training
    deceptive llms that persist through safety training,” 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T.
    Lanham, D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan,
    C. Anil, D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, K. Sachan, M.
    Sellitto, M. Sharma, N. DasSarma, R. Grosse, S. Kravec, Y. Bai, Z. Witten, M.
    Favaro, J. Brauner, H. Karnofsky, P. Christiano, S. R. Bowman, L. Graham, J. Kaplan,
    S. Mindermann, R. Greenblatt, B. Shlegeris, N. Schiefer, 和 E. Perez，“卧底代理：训练通过安全训练持续存在的欺骗性LLMs”，2024年。'
- en: '[7] M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li,
    S. Basart, B. Li, D. Forsyth, and D. Hendrycks, “Harmbench: A standardized evaluation
    framework for automated red teaming and robust refusal,” 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li,
    S. Basart, B. Li, D. Forsyth, 和 D. Hendrycks，“Harmbench: 一个用于自动化红队和强健拒绝的标准化评估框架”，2024年。'
- en: '[8] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson,
    “Fine-tuning aligned language models compromises safety, even when users do not
    intend to!,” arXiv preprint arXiv:2310.03693, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, 和 P. Henderson，“即使用户不打算，微调对齐的语言模型也会妨碍安全！”，arXiv预印本
    arXiv:2310.03693，2023年。'
- en: '[9] B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi, M. Xia, P. Mittal, M. Wang,
    and P. Henderson, “Assessing the brittleness of safety alignment via pruning and
    low-rank modifications,” arXiv preprint arXiv:2402.05162, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi, M. Xia, P. Mittal, M. Wang,
    和 P. Henderson，“通过剪枝和低秩修改评估安全对齐的脆弱性”，arXiv预印本 arXiv:2402.05162，2024年。'
- en: '[10] S. Jain, R. Kirk, E. S. Lubana, R. P. Dick, H. Tanaka, E. Grefenstette,
    T. Rocktäschel, and D. S. Krueger, “Mechanistically analyzing the effects of fine-tuning
    on procedurally defined tasks,” 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Jain, R. Kirk, E. S. Lubana, R. P. Dick, H. Tanaka, E. Grefenstette,
    T. Rocktäschel 和 D. S. Krueger, “机制性分析微调对程序性定义任务的影响,” 2023。'
- en: '[11] A. Lee, X. Bai, I. Pres, M. Wattenberg, J. K. Kummerfeld, and R. Mihalcea,
    “A mechanistic understanding of alignment algorithms: A case study on dpo and
    toxicity,” ArXiv, vol. abs/2401.01967, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Lee, X. Bai, I. Pres, M. Wattenberg, J. K. Kummerfeld 和 R. Mihalcea,
    “对齐算法的机制理解：以 DPO 和毒性为案例研究,” ArXiv, 第abs/2401.01967卷, 2024。'
- en: '[12] X. Yang, X. Wang, Q. Zhang, L. Petzold, W. Y. Wang, X. Zhao, and D. Lin,
    “Shadow alignment: The ease of subverting safely-aligned language models,” arXiv
    preprint arXiv:2310.02949, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] X. Yang, X. Wang, Q. Zhang, L. Petzold, W. Y. Wang, X. Zhao 和 D. Lin,
    “影子对齐：颠覆安全对齐语言模型的难易程度,” arXiv 预印本 arXiv:2310.02949, 2023。'
- en: '[13] S. Lermen, C. Rogers-Smith, and J. Ladish, “Lora fine-tuning efficiently
    undoes safety training in llama 2-chat 70b,” arXiv preprint arXiv:2310.20624,
    2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Lermen, C. Rogers-Smith 和 J. Ladish, “Lora 微调有效地撤销了 Llama 2-chat 70b
    的安全训练,” arXiv 预印本 arXiv:2310.20624, 2023。'
- en: '[14] J. Yi, R. Ye, Q. Chen, B. B. Zhu, S. Chen, D. Lian, G. Sun, X. Xie, and
    F. Wu, “Open-source can be dangerous: On the vulnerability of value alignment
    in open-source LLMs,” 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Yi, R. Ye, Q. Chen, B. B. Zhu, S. Chen, D. Lian, G. Sun, X. Xie 和 F.
    Wu, “开源可能是危险的：关于开源 LLM 中价值对齐漏洞的研究,” 2024。'
- en: '[15] D. Rosati, J. Wehner, K. Williams, Łukasz Bartoszcze, J. Batzner, H. Sajjad,
    and F. Rudzicz, “Immunization against harmful fine-tuning attacks,” 2024.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Rosati, J. Wehner, K. Williams, Łukasz Bartoszcze, J. Batzner, H. Sajjad
    和 F. Rudzicz, “免疫有害的微调攻击,” 2024。'
- en: '[16] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等, “Llama 2：开放基础和微调聊天模型,” arXiv 预印本
    arXiv:2307.09288, 2023。'
- en: '[17] A. Lees, V. Q. Tran, Y. Tay, J. Sorensen, J. Gupta, D. Metzler, and L. Vasserman,
    “A new generation of perspective api: Efficient multilingual character-level transformers,”
    2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Lees, V. Q. Tran, Y. Tay, J. Sorensen, J. Gupta, D. Metzler 和 L. Vasserman,
    “新一代视角 API：高效的多语言字符级变换器,” 2022。'
- en: '[18] A. Achille, G. Mbeng, and S. Soatto, “Dynamics and reachability of learning
    tasks,” 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Achille, G. Mbeng 和 S. Soatto, “学习任务的动态性与可达性,” 2019。'
- en: '[19] N. Tishby and N. Zaslavsky, “Deep learning and the information bottleneck
    principle,” in 2015 ieee information theory workshop (itw), pp. 1–5, IEEE, 2015.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] N. Tishby 和 N. Zaslavsky, “深度学习与信息瓶颈原则,” 收录于2015 IEEE 信息理论研讨会（ITW），第1–5页，IEEE，2015。'
- en: '[20] L. Wang, S. Xu, R. Xu, X. Wang, and Q. Zhu, “Non-Transferable Learning:
    A New Approach for Model Ownership Verification and Applicability Authorization,”
    Oct. 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. Wang, S. Xu, R. Xu, X. Wang 和 Q. Zhu, “不可转移学习：一种新的模型所有权验证和适用性授权方法,”
    2021年10月。'
- en: '[21] T. Wu, I. Fischer, I. L. Chuang, and M. Tegmark, “Learnability for the
    information bottleneck,” Entropy, vol. 21, p. 924, Sept. 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] T. Wu, I. Fischer, I. L. Chuang 和 M. Tegmark, “信息瓶颈的可学习性,” 《熵》，第21卷，第924页，2019年9月。'
- en: '[22] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi,
    D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil,
    Y. Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta,
    S. Whitman, and J. Saxe, “Purple llama cyberseceval: A secure coding benchmark
    for language models,” 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi,
    D. Song, F. Ahmad, C. Aschermann, L. Fontana, S. Frolov, R. P. Giri, D. Kapil,
    Y. Kozyrakis, D. LeBlanc, J. Milazzo, A. Straumann, G. Synnaeve, V. Vontimitta,
    S. Whitman 和 J. Saxe, “Purple llama cyberseceval：一种用于语言模型的安全编码基准,” 2023。'
- en: '[23] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y. Wang,
    and Y. Yang, “Beavertails: Towards improved safety alignment of llm via a human-preference
    dataset,” in Advances in Neural Information Processing Systems (A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds.), vol. 36, pp. 24678–24704,
    Curran Associates, Inc., 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y.
    Wang 和 Y. Yang, “Beavertails：通过人类偏好数据集提高 LLM 的安全对齐,” 收录于《神经信息处理系统进展》（A. Oh, T.
    Naumann, A. Globerson, K. Saenko, M. Hardt 和 S. Levine 主编），第36卷，第24678–24704页，Curran
    Associates, Inc., 2023。'
- en: '[24] T. Huang, S. Hu, and L. Liu, “Vaccine: Perturbation-aware alignment for
    large language model,” arXiv preprint arXiv:2402.01109, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Huang, S. Hu 和 L. Liu, “Vaccine：针对大型语言模型的扰动感知对齐,” arXiv 预印本 arXiv:2402.01109,
    2024。'
- en: '[25] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong,
    R. Dutta, R. Schaeffer, S. T. Truong, S. Arora, M. Mazeika, D. Hendrycks, Z. Lin,
    Y. Cheng, S. Koyejo, D. Song, and B. Li, “Decodingtrust: A comprehensive assessment
    of trustworthiness in gpt models,” 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong,
    R. Dutta, R. Schaeffer, S. T. Truong, S. Arora, M. Mazeika, D. Hendrycks, Z. Lin,
    Y. Cheng, S. Koyejo, D. Song, 和 B. Li，“解码信任：对 GPT 模型的全面信任评估，” 2024年。'
- en: '[26] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, “RealToxicityPrompts:
    Evaluating neural toxic degeneration in language models,” 2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Gehman, S. Gururangan, M. Sap, Y. Choi, 和 N. A. Smith，“RealToxicityPrompts：评估语言模型中的神经毒性退化，”
    2020年。'
- en: '[27] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing
    human-level performance on ImageNet classification,” 2015.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. He, X. Zhang, S. Ren, 和 J. Sun，“深入研究激活函数：在 ImageNet 分类中超越人类水平表现，” 2015年。'
- en: '[28] X. Zhou, Y. Lu, R. Ma, T. Gui, Q. Zhang, and X. Huang, “Making harmful
    behaviors unlearnable for large language models,” arXiv preprint arXiv:2311.02105,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Zhou, Y. Lu, R. Ma, T. Gui, Q. Zhang, 和 X. Huang，“使大型语言模型的有害行为无法学习，”
    arXiv 预印本 arXiv:2311.02105，2023年。'
- en: '[29] R. Bhardwaj and S. Poria, “Language model unalignment: Parametric red-teaming
    to expose hidden harms and biases,” arXiv preprint arXiv:2310.14303, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Bhardwaj 和 S. Poria， “语言模型不一致性：参数化红队测试以揭示隐藏的危害和偏见，” arXiv 预印本 arXiv:2310.14303，2023年。'
- en: '[30] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,”
    12 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, 和 A. Zou，“少样本语言模型评估框架，” 2023年12月。'
- en: '[31] S. Lin, J. Hilton, and O. Evans, “TruthfulQA: Measuring how models mimic
    human falsehoods,” in Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers) (S. Muresan, P. Nakov, and
    A. Villavicencio, eds.), (Dublin, Ireland), pp. 3214–3252, Association for Computational
    Linguistics, May 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Lin, J. Hilton, 和 O. Evans，“TruthfulQA：测量模型如何模仿人类虚假信息，” 见于《第60届计算语言学协会年会论文集（第一卷：长篇论文）》
    (S. Muresan, P. Nakov, 和 A. Villavicencio 编)，（爱尔兰都柏林），页3214–3252，计算语言学协会，2022年5月。'
- en: '[32] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt,
    “Measuring massive multitask language understanding,” arXiv preprint arXiv:2009.03300,
    2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, 和 J. Steinhardt，“测量大规模多任务语言理解，”
    arXiv 预印本 arXiv:2009.03300，2020年。'
- en: '[33] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “HellaSwag:
    Can a machine really finish your sentence?,” in Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics, pp. 4791–4800, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi，“HellaSwag：机器真的能完成你的句子吗？”，见于《第57届计算语言学协会年会论文集》，页4791–4800，2019年。'
- en: '[34] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    和 O. Tafjord，“你认为自己解决了问答问题？试试 ARC，AI2 推理挑战赛，” arXiv 预印本 arXiv:1803.05457，2018年。'
- en: '[35] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt,
    “Aligning AI with shared human values,” in International Conference on Learning
    Representations, 2020.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, 和 J. Steinhardt，“使
    AI 与共享人类价值观对齐，” 见于国际学习表征会议，2020年。'
- en: '[36] N. Nangia, C. Vania, R. Bhalerao, and S. Bowman, “CrowS-Pairs: A challenge
    dataset for measuring social biases in masked language models,” in Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp. 1953–1967, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] N. Nangia, C. Vania, R. Bhalerao, 和 S. Bowman，“CrowS-Pairs：一个用于测量掩蔽语言模型中的社会偏见的挑战数据集，”
    见于《2020年自然语言处理实证方法会议（EMNLP）论文集》，页1953–1967，2020年。'
- en: '[37] S. Gehrmann, T. Adewumi, K. Aggarwal, P. S. Ammanamanchi, A. Aremu, A. Bosselut,
    K. R. Chandu, M.-A. Clinciu, D. Das, K. Dhole, W. Du, E. Durmus, O. Dušek, C. C.
    Emezue, V. Gangal, C. Garbacea, T. Hashimoto, Y. Hou, Y. Jernite, H. Jhamtani,
    Y. Ji, S. Jolly, M. Kale, D. Kumar, F. Ladhak, A. Madaan, M. Maddela, K. Mahajan,
    S. Mahamood, B. P. Majumder, P. H. Martins, A. McMillan-Major, S. Mille, E. van
    Miltenburg, M. Nadeem, S. Narayan, V. Nikolaev, A. Niyongabo Rubungo, S. Osei,
    A. Parikh, L. Perez-Beltrachini, N. R. Rao, V. Raunak, J. D. Rodriguez, S. Santhanam,
    J. Sedoc, T. Sellam, S. Shaikh, A. Shimorina, M. A. Sobrevilla Cabezudo, H. Strobelt,
    N. Subramani, W. Xu, D. Yang, A. Yerukola, and J. Zhou, “The GEM benchmark: Natural
    language generation, its evaluation and metrics,” in Proceedings of the 1st Workshop
    on Natural Language Generation, Evaluation, and Metrics (GEM 2021) (A. Bosselut,
    E. Durmus, V. P. Gangal, S. Gehrmann, Y. Jernite, L. Perez-Beltrachini, S. Shaikh,
    and W. Xu, eds.), (Online), pp. 96–120, Association for Computational Linguistics,
    Aug. 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Gehrmann, T. Adewumi, K. Aggarwal, P. S. Ammanamanchi, A. Aremu, A.
    Bosselut, K. R. Chandu, M.-A. Clinciu, D. Das, K. Dhole, W. Du, E. Durmus, O.
    Dušek, C. C. Emezue, V. Gangal, C. Garbacea, T. Hashimoto, Y. Hou, Y. Jernite,
    H. Jhamtani, Y. Ji, S. Jolly, M. Kale, D. Kumar, F. Ladhak, A. Madaan, M. Maddela,
    K. Mahajan, S. Mahamood, B. P. Majumder, P. H. Martins, A. McMillan-Major, S.
    Mille, E. van Miltenburg, M. Nadeem, S. Narayan, V. Nikolaev, A. Niyongabo Rubungo,
    S. Osei, A. Parikh, L. Perez-Beltrachini, N. R. Rao, V. Raunak, J. D. Rodriguez,
    S. Santhanam, J. Sedoc, T. Sellam, S. Shaikh, A. Shimorina, M. A. Sobrevilla Cabezudo,
    H. Strobelt, N. Subramani, W. Xu, D. Yang, A. Yerukola, 和 J. Zhou, “GEM 基准：自然语言生成、其评估与度量,”
    见《第1届自然语言生成、评估与度量研讨会论文集（GEM 2021）》（A. Bosselut, E. Durmus, V. P. Gangal, S. Gehrmann,
    Y. Jernite, L. Perez-Beltrachini, S. Shaikh, 和 W. Xu 主编），（在线），第96–120页，计算语言学协会，2021年8月。'
- en: '[38] A. K. Tarun, V. S. Chundawat, M. Mandal, and M. Kankanhalli, “Fast yet
    effective machine unlearning,” IEEE Transactions on Neural Networks and Learning
    Systems, p. 1–10, 2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. K. Tarun, V. S. Chundawat, M. Mandal, 和 M. Kankanhalli, “快速而有效的机器遗忘,”
    IEEE《神经网络与学习系统汇刊》，第1–10页，2024年。'
- en: '[39] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney,
    S. Biderman, and J. Steinhardt, “Eliciting latent predictions from transformers
    with the tuned lens,” 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney,
    S. Biderman, 和 J. Steinhardt, “利用调优镜头从变换器中引出潜在预测,” 2023年。'
- en: '[40] P. Henderson, E. Mitchell, C. Manning, D. Jurafsky, and C. Finn, “Self-destructing
    models: Increasing the costs of harmful dual uses of foundation models,” in Proceedings
    of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pp. 287–296, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] P. Henderson, E. Mitchell, C. Manning, D. Jurafsky, 和 C. Finn, “自毁模型：增加基础模型有害双重用途的成本,”
    见《2023 AAAI/ACM 人工智能、伦理与社会会议论文集》，第287–296页，2023年。'
- en: '[41] J. Deng, S. Pang, Y. Chen, L. Xia, Y. Bai, H. Weng, and W. Xu, “SOPHON:
    Non-fine-tunable learning to restrain task transferability for pre-trained models,”
    2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Deng, S. Pang, Y. Chen, L. Xia, Y. Bai, H. Weng, 和 W. Xu, “SOPHON:
    不可微调的学习以限制预训练模型的任务迁移性,” 2024年。'
- en: '[42] A. Golatkar, A. Achille, and S. Soatto, “Eternal sunshine of the spotless
    net: Selective forgetting in deep networks,” 2020.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Golatkar, A. Achille, 和 S. Soatto, “洁净网络的永恒阳光：深度网络中的选择性遗忘,” 2020年。'
- en: '[43] A. Golatkar, A. Achille, and S. Soatto, “Forgetting outside the box: Scrubbing
    deep networks of information accessible from input-output observations,” 2020.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Golatkar, A. Achille, 和 S. Soatto, “跳出框框的遗忘：清除深度网络中从输入输出观察中可访问的信息,”
    2020年。'
- en: '[44] H. Huang, X. Ma, S. M. Erfani, J. Bailey, and Y. Wang, “Unlearnable examples:
    Making personal data unexploitable,” 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Huang, X. Ma, S. M. Erfani, J. Bailey, 和 Y. Wang, “不可学习的示例：使个人数据不可被利用,”
    2021年。'
- en: '[45] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K.
    Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen,
    A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, R. Tamirisa, B. Bharathi, A. Khoja,
    Z. Zhao, A. Herbert-Voss, C. B. Breuer, S. Marks, O. Patel, A. Zou, M. Mazeika,
    Z. Wang, P. Oswal, W. Liu, A. A. Hunt, J. Tienken-Harder, K. Y. Shih, K. Talley,
    J. Guan, R. Kaplan, I. Steneker, D. Campbell, B. Jokubaitis, A. Levinson, J. Wang,
    W. Qian, K. K. Karmakar, S. Basart, S. Fitz, M. Levine, P. Kumaraguru, U. Tupakula,
    V. Varadharajan, R. Wang, Y. Shoshitaishvili, J. Ba, K. M. Esvelt, A. Wang, and
    D. Hendrycks, “The WMDP benchmark: Measuring and reducing malicious use with unlearning,”
    2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K.
    Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen,
    A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, R. Tamirisa, B. Bharathi, A.
    Khoja, Z. Zhao, A. Herbert-Voss, C. B. Breuer, S. Marks, O. Patel, A. Zou, M.
    Mazeika, Z. Wang, P. Oswal, W. Liu, A. A. Hunt, J. Tienken-Harder, K. Y. Shih,
    K. Talley, J. Guan, R. Kaplan, I. Steneker, D. Campbell, B. Jokubaitis, A. Levinson,
    J. Wang, W. Qian, K. K. Karmakar, S. Basart, S. Fitz, M. Levine, P. Kumaraguru,
    U. Tupakula, V. Varadharajan, R. Wang, Y. Shoshitaishvili, J. Ba, K. M. Esvelt,
    A. Wang, 和 D. Hendrycks，“WMDP基准测试：通过遗忘测量和减少恶意使用，”2024年。'
- en: '[46] J. Foster, S. Schoepf, and A. Brintrup, “Fast machine unlearning without
    retraining through selective synaptic dampening,” 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Foster, S. Schoepf, 和 A. Brintrup，“通过选择性突触减弱实现快速机器遗忘，无需重新训练，”2023年。'
- en: '[47] R. Gandikota, H. Orgad, Y. Belinkov, J. Materzyńska, and D. Bau, “Unified
    concept editing in diffusion models,” 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Gandikota, H. Orgad, Y. Belinkov, J. Materzyńska, 和 D. Bau，“在扩散模型中的统一概念编辑，”2023年。'
- en: '[48] H. Wang, H. Chi, W. Yang, Z. Lin, M. Geng, L. Lan, J. Zhang, and D. Tao,
    “Domain specified optimization for deployment authorization,” in 2023 IEEE/CVF
    International Conference on Computer Vision (ICCV), pp. 5072–5082, IEEE, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Wang, H. Chi, W. Yang, Z. Lin, M. Geng, L. Lan, J. Zhang, 和 D. Tao，“针对部署授权的领域特定优化，”发表于
    2023 IEEE/CVF 国际计算机视觉大会（ICCV），第 5072–5082 页，IEEE，2023。'
- en: '[49] W. Zhang, L. Deng, L. Zhang, and D. Wu, “A survey on negative transfer,”
    IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 2, pp. 305–329, 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] W. Zhang, L. Deng, L. Zhang, 和 D. Wu，“关于负迁移的综述，”IEEE/CAA 自动化学报，vol. 10,
    no. 2, 第 305–329 页，2022年。'
- en: '[50] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann,
    E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma,
    D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez,
    T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson,
    D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark,
    “Red teaming language models to reduce harms: Methods, scaling behaviors, and
    lessons learned,” 2022.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B.
    Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly,
    N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T.
    Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson,
    S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C.
    Olah, J. Kaplan, 和 J. Clark，“对语言模型进行红队测试以减少危害：方法、扩展行为和经验教训，”2022年。'
- en: '[51] P. He, J. Gao, and W. Chen, “DeBERTaV3: Improving DeBERTa using ELECTRA-Style
    pre-training with gradient-disentangled embedding sharing,” 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] P. He, J. Gao, 和 W. Chen，“DeBERTaV3: 通过梯度解耦嵌入共享的ELECTRA风格预训练改进DeBERTa，”2023年。'
- en: '[52] J. Juraska, K. Bowden, and M. Walker, “ViGGO: A video game corpus for
    data-to-text generation in open-domain conversation,” in Proceedings of the 12th
    International Conference on Natural Language Generation, (Tokyo, Japan), pp. 164–172,
    Association for Computational Linguistics, Oct.–Nov. 2019.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Juraska, K. Bowden, 和 M. Walker，“ViGGO: 一个用于开放域对话的数据到文本生成的视频游戏语料库，”发表于第十二届国际自然语言生成会议论文集，（东京，日本），第
    164–172 页，计算语言学协会，2019年10月–11月。'
- en: '[53] O. Dušek, D. M. Howcroft, and V. Rieser, “Semantic Noise Matters for Neural
    Natural Language Generation,” in Proceedings of the 12th International Conference
    on Natural Language Generation (INLG 2019), (Tokyo, Japan), pp. 421–426, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] O. Dušek, D. M. Howcroft, 和 V. Rieser，“语义噪声对神经自然语言生成的重要性，”发表于第十二届国际自然语言生成会议（INLG
    2019）论文集，（东京，日本），第 421–426 页，2019。'
- en: '[54] L. Nan, D. Radev, R. Zhang, A. Rau, A. Sivaprasad, C. Hsieh, X. Tang,
    A. Vyas, N. Verma, P. Krishna, Y. Liu, N. Irwanto, J. Pan, F. Rahman, A. Zaidi,
    M. Mutuma, Y. Tarabar, A. Gupta, T. Yu, Y. C. Tan, X. V. Lin, C. Xiong, R. Socher,
    and N. F. Rajani, “DART: Open-domain structured data record to text generation,”
    in Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, (Online), pp. 432–447,
    Association for Computational Linguistics, June 2021.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] L. Nan, D. Radev, R. Zhang, A. Rau, A. Sivaprasad, C. Hsieh, X. Tang,
    A. Vyas, N. Verma, P. Krishna, Y. Liu, N. Irwanto, J. Pan, F. Rahman, A. Zaidi,
    M. Mutuma, Y. Tarabar, A. Gupta, T. Yu, Y. C. Tan, X. V. Lin, C. Xiong, R. Socher,
    和 N. F. Rajani， “DART: 开放域结构化数据记录到文本生成，” 收录于2021年北美计算语言学协会会议：人类语言技术会议论文集，（在线），第432-447页，计算语言学协会，2021年6月。'
- en: '[55] C. van der Lee, C. Emmery, S. Wubben, and E. Krahmer, “The CACAPO dataset:
    A multilingual, multi-domain dataset for neural pipeline and end-to-end data-to-text
    generation,” in Proceedings of the 13th International Conference on Natural Language
    Generation (B. Davis, Y. Graham, J. Kelleher, and Y. Sripada, eds.), (Dublin,
    Ireland), pp. 68–79, Association for Computational Linguistics, Dec. 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. van der Lee, C. Emmery, S. Wubben, 和 E. Krahmer， “CACAPO 数据集: 一个多语言、多领域的神经管道和端到端数据到文本生成数据集，”
    收录于第13届国际自然语言生成会议论文集（B. Davis, Y. Graham, J. Kelleher, 和 Y. Sripada 主编），（爱尔兰都柏林），第68-79页，计算语言学协会，2020年12月。'
- en: '[56] A. Balakrishnan, J. Rao, K. Upasani, M. White, and R. Subba, “Constrained
    decoding for neural NLG from compositional representations in task-oriented dialogue,”
    in Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics (A. Korhonen, D. Traum, and L. Màrquez, eds.), (Florence, Italy),
    pp. 831–844, Association for Computational Linguistics, July 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Balakrishnan, J. Rao, K. Upasani, M. White, 和 R. Subba， “基于任务导向对话的约束解码用于神经NLG中的组合表示，”
    收录于第57届计算语言学协会年会论文集（A. Korhonen, D. Traum, 和 L. Màrquez 主编），（意大利佛罗伦萨），第831-844页，计算语言学协会，2019年7月。'
- en: '[57] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    and T. B. Hashimoto, “Stanford alpaca: An instruction-following llama model.”
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,
    和 T. B. Hashimoto， “Stanford alpaca: 一种遵循指令的 llama 模型。” [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023。'
- en: '[58] D. Esiobu, X. Tan, S. Hosseini, M. Ung, Y. Zhang, J. Fernandes, J. Dwivedi-Yu,
    E. Presani, A. Williams, and E. M. Smith, “Robbie: Robust bias evaluation of large
    generative language models,” 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Esiobu, X. Tan, S. Hosseini, M. Ung, Y. Zhang, J. Fernandes, J. Dwivedi-Yu,
    E. Presani, A. Williams, 和 E. M. Smith， “Robbie: 大型生成语言模型的鲁棒性偏差评估，” 2023。'
- en: '[59] P. Röttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy,
    “XSTest: A test suite for identifying exaggerated safety behaviours in large language
    models,” 2024.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] P. Röttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, 和 D. Hovy，
    “XSTest: 用于识别大型语言模型中夸大的安全行为的测试套件，” 2024。'
- en: '[60] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, and M. Fredrikson,
    “Universal and transferable adversarial attacks on aligned language models,” 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Zou, Z. Wang, N. Carlini, M. Nasr, J. Z. Kolter, 和 M. Fredrikson， “对齐语言模型的通用和可转移对抗攻击，”
    2023。'
- en: '[61] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,
    N. McAleese, and G. Irving, “Red teaming language models with language models,”
    arXiv preprint arXiv:2202.03286, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,
    N. McAleese, 和 G. Irving， “用语言模型进行红队测试语言模型，” arXiv 预印本 arXiv:2202.03286, 2022。'
- en: '[62] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, “" do anything now":
    Characterizing and evaluating in-the-wild jailbreak prompts on large language
    models,” arXiv preprint arXiv:2308.03825, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. Shen, Z. Chen, M. Backes, Y. Shen, 和 Y. Zhang， “" do anything now":
    描述和评估大型语言模型上的实际破解提示，” arXiv 预印本 arXiv:2308.03825, 2023。'
- en: '[63] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    and Y. Liu, “Jailbreaking chatgpt via prompt engineering: An empirical study,”
    arXiv preprint arXiv:2305.13860, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    和 Y. Liu， “通过提示工程破解 chatgpt: 一项实证研究，” arXiv 预印本 arXiv:2305.13860, 2023。'
- en: '[64] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” 2021.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen， “Lora: 大型语言模型的低秩适配，” 2021。'
- en: '[65] Y. Belinkov, “Probing Classifiers: Promises, Shortcomings, and Advances,”
    Computational Linguistics, vol. 48, pp. 207–219, 04 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Belinkov, “探测分类器：承诺、缺陷与进展”，计算语言学，第48卷，第207-219页，2022年4月。'
- en: '[66] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case
    of neural text degeneration,” 2020.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Holtzman, J. Buys, L. Du, M. Forbes, 和 Y. Choi, “神经文本退化的奇异案例”，2020年。'
- en: '[67] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han,
    F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu,
    J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang,
    S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan,
    Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and
    T. Zhu, “Qwen technical report,” 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han,
    F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu,
    J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang,
    S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan,
    Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, 和
    T. Zhu, “Qwen技术报告”，2023年。'
- en: Appendix A Proofs and Mathematical Details
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 证明和数学细节
- en: A.1 Deriving the approximate transition probability
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 近似过渡概率的推导
- en: 'Motivated by the transfer learning question: "How can we predict the success
    of training a model originally trained on Task A to Task B", Achille et al. [[18](#bib.bib18)]
    present a transition probability that determines the likelihood an initial model
    with parameters $w_{0}$.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 受到迁移学习问题的启发：“我们如何预测将最初在任务A上训练的模型转移到任务B的成功”，Achille等人[[18](#bib.bib18)]提出了一个过渡概率，确定了初始模型参数$w_{0}$的可能性。
- en: 'They posit the following transition probability $p(w_{f},t_{f}|w_{0},t_{0})$
    as equal to:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出以下过渡概率$p(w_{f},t_{f}|w_{0},t_{0})$等于：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Static potential measures how far an initial model is from the final configuration
    in terms of the difference between the loss of the model at the initial state
    $w_{0}$ comes from the author’s derivation of the original equation from a Wiener
    process. Minimizing this is where our Gradient Ascent loss comes from.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 静态潜力衡量的是初始模型与最终配置之间的差距，这种差距来自于模型在初始状态$w_{0}$的损失与作者从Wiener过程推导出的原始方程的差异。最小化这个差距就是我们的梯度上升损失的来源。
- en: Reachability measures the likelihood of actually traversing the loss landscape.
    That is determined by integrating over the “difficulty” of reaching $w_{f}$ with
    some stochastic function $\sqrt{2Dn(t)}$ reaching $w_{f}$.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 可达性衡量的是实际遍历损失景观的可能性。这通过对到达$w_{f}$的“难度”进行积分来确定，难度由某些随机函数$\sqrt{2Dn(t)}$到达$w_{f}$决定。
- en: 'Our simplified [eq. 2](#S3.E2 "In Transition Probabilities and Adversarial
    Loss ‣ 3 Method ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") in the main paper consists of simplifying [eq. 6](#A1.E6 "In A.1 Deriving
    the approximate transition probability ‣ Appendix A Proofs and Mathematical Details
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") with
    the following steps and assumptions:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在主论文中的简化[eq. 2](#S3.E2 "在过渡概率和对抗损失 ‣ 3 方法 ‣ 表示噪声有效防止对LLMs的有害微调")是通过以下步骤和假设来简化[eq.
    6](#A1.E6 "在A.1 近似过渡概率的推导 ‣ 附录A 证明和数学细节 ‣ 表示噪声有效防止对LLMs的有害微调")。
- en: First, we observe that we can construct a simpler presentation for the main
    paper by ignoring the $D$ for simplicity. In practice, this stochastic factor
    will exist but when deriving our loss function we assume we won’t be able to control
    this factor so it isn’t important for deriving a transition probability that illustrates
    how to construct a loss function that minimizes it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们观察到，通过忽略$D$，我们可以构造一个更简单的主论文呈现形式。实际上，这个随机因素将存在，但在推导我们的损失函数时，我们假设我们不能控制这个因素，因此在推导一个最小化损失函数的过渡概率时，这个因素并不重要。
- en: 'Our second strong assumption is that the divergence across all paths is set
    to some constant, in our case again it is replaced with 0\. We realize that the
    divergence of the gradients of a loss function with respect to some parameters
    might play a major role in the transition probability: for example, if the divergence
    is always negative with a large magnitude, it is easy to rapidly traverse the
    loss landscape to $w_{f}$ and the transition probability would be much larger
    than otherwise expected without this term. Similarly, if the divergence is always
    positive with a large magnitude then the transition probability would be much
    smaller than expected. For our work, we only assert an approximate estimation
    of the transition probability where the divergence term and stochastic factors
    are necessary for a more accurate transition probability.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个强假设是所有路径上的散度都设置为某个常数，在我们的情况下再次被替换为 0。我们认识到，损失函数相对于某些参数的梯度散度可能在过渡概率中发挥主要作用：例如，如果散度总是具有较大的负值，则可以迅速遍历损失景观到
    $w_{f}$，且过渡概率将比没有该项时预期的要大得多。类似地，如果散度总是具有较大的正值，则过渡概率将比预期的要小得多。对于我们的工作，我们仅断言过渡概率的近似估计，其中散度项和随机因素对于更准确的过渡概率是必要的。
- en: 'Based on these modifications we get the following approximate simplification
    that we use in the main paper and in the proof below:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些修改，我们得到以下近似简化，在主文中和下面的证明中使用：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: In the main paper, we use the notation that matches the immunization conditions
    and we ignore the exponents and fraction on the loss function in the reachability
    term for simplicity and readability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在主文中，我们使用与免疫条件匹配的符号，为了简洁和可读性，我们忽略了可达性项中损失函数的指数和分数。
- en: We note that in order to construct a loss function which maximizes divergence
    we would need access to information about the curvature of the loss landscape
    through the Hessian which we we leave to future work. Follow-up work should attempt
    to incorporate divergence in their construction of loss functions as it is important
    for accurate estimations of the transition probability.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，为了构造一个最大化散度的损失函数，我们需要通过 Hessian 获得有关损失景观曲率的信息，这部分将留待未来的工作。后续工作应尝试在损失函数的构造中纳入散度，因为它对于准确估计过渡概率非常重要。
- en: A.2 Proof of Theorem 1
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 定理 1 的证明
- en: Theorem 1
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定理 1
- en: Consider a set of initial weights $\theta[t=0]$ used to represent those inputs
    given the model weights, $\theta$.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组初始权重 $\theta[t=0]$，用于表示给定模型权重 $\theta$ 的那些输入。
- en: Recall from above that the magnitude of the gradient of the loss function $\mathcal{L_{D}}$
    determines the reachability term in [eq. 7](#A1.E7 "In A.1 Deriving the approximate
    transition probability ‣ Appendix A Proofs and Mathematical Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面回顾，损失函数 $\mathcal{L_{D}}$ 的梯度幅度决定了 [公式 7](#A1.E7 "在 A.1 推导近似过渡概率 ‣ 附录 A 证明和数学细节
    ‣ 表示噪声有效防止 LLM 上有害的微调") 中的可达性项。
- en: 'This quantity can be connected with mutual information using [Theorem 2](#Thmtheorem2
    "Theorem 2\. ‣ Theorem 1 ‣ A.2 Proof of Theorem 1 ‣ Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")
    which is from Wang et al. [[20](#bib.bib20)] where we direct readers to for its
    proof:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量可以通过 [定理 2](#Thmtheorem2 "定理 2。 ‣ 定理 1 ‣ A.2 定理 1 的证明 ‣ 附录 A 证明和数学细节 ‣ 表示噪声有效防止
    LLM 上有害的微调") 与互信息连接，该定理来自 Wang 等人 [[20](#bib.bib20)]，我们引导读者参考其证明：
- en: Theorem 2.
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2。
- en: Let $\hat{Y}$, respectively. If the KL divergence loss $D_{\text{KL}}(\mathcal{P}(\hat{\mathbf{Y}})\|\mathcal{P}(\mathbf{Y}))$
    will decrease and vice versa.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\hat{Y}$，分别。如果 KL 散度损失 $D_{\text{KL}}(\mathcal{P}(\hat{\mathbf{Y}})\|\mathcal{P}(\mathbf{Y}))$
    会减少，反之亦然。
- en: First we point out that, in this context, the KL divergence loss over one-hot
    vectors is the same as the cross entropy loss (see the equivalence in [§ A.3](#A1.SS3
    "A.3 Equivalence of KL and Cross Entropy ‣ Appendix A Proofs and Mathematical
    Details ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")).
    So we will refer to cross entropy loss increase as a way to decrease $I(Z;Y)$
    and vice versa.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们指出，在这种情况下，关于 one-hot 向量的 KL 散度损失与交叉熵损失是相同的（见 [§ A.3](#A1.SS3 "A.3 KL 和交叉熵的等价性
    ‣ 附录 A 证明和数学细节 ‣ 表示噪声有效防止 LLM 上有害的微调")）。因此，我们将交叉熵损失增加视为减少 $I(Z;Y)$ 的一种方式，反之亦然。
- en: 'Second, observe that we maximize cross entropy loss by taking the following
    gradient steps: $\theta_{t+1}=\theta_{t}+\eta\nabla\mathcal{L_{D}}\theta$. Finally
    maximizing reachability minimizes the transition probability.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，通过采取以下梯度步骤，我们可以最大化交叉熵损失：$\theta_{t+1}=\theta_{t}+\eta\nabla\mathcal{L_{D}}\theta$。最后，最大化可达性最小化了过渡概率。
- en: What about the static potential? It is easy to see (via the connection to increasing
    loss in [theorem 2](#Thmtheorem2 "Theorem 2\. ‣ Theorem 1 ‣ A.2 Proof of Theorem
    1 ‣ Appendix A Proofs and Mathematical Details ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")) that our newly derived minimizer also
    maximizes the static term of the transition probability since this minimizer also
    increases the cross entropy loss of the initial parameters. Therefore any minimizer
    of $I(Z;Y)$ is also a maximizer of the static potential further reducing the transition
    probability.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 那么静态潜力怎么样？通过与[定理 2](#Thmtheorem2 "定理 2\. ‣ 定理 1 ‣ A.2 定理 1 证明 ‣ 附录 A 证明和数学细节
    ‣ 表示噪声有效防止对LLM的有害微调")的损失增加的联系，很容易看出我们新推导的最小化器也最大化了过渡概率的静态项，因为这个最小化器也增加了初始参数的交叉熵损失。因此，任何$I(Z;Y)$的最小化器也是静态潜力的最大化器，进一步减少了过渡概率。
- en: From information bottleneck theory [[21](#bib.bib21)], we have the data processing
    inequality, $I(Z;Y)$.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息瓶颈理论[[21](#bib.bib21)]，我们得到了数据处理不等式，$I(Z;Y)$。
- en: We note that by the data processing inequality again, we have the added benefit
    that we can continue to reduce $I(X;Z)$.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，通过数据处理不等式，我们还有额外的好处，那就是我们可以继续减少$I(X;Z)$。
- en: A.3 Equivalence of KL and Cross Entropy
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 KL散度与交叉熵的等价性
- en: In some contexts minimizing the KL divergence loss and the cross entropy loss
    are equivalent. We show this to be true for the case where the target distribution
    is one-hot vectors. This is the case for language modeling loss, the focus of
    our work, where we have the true token represented as a one-hot over the vocabulary
    distribution.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，最小化KL散度损失和交叉熵损失是等价的。我们证明了当目标分布是one-hot向量时这一点是正确的。这对于语言建模损失，即我们工作的重点，适用，其中真实的token表示为词汇分布上的one-hot向量。
- en: 'The KL divergence measures the difference between two probability distributions
    $P$:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度衡量两个概率分布$P$之间的差异：
- en: '|  | $$D_{KL}(P&#124;&#124;Q)=\sum_{i}P(i)\> |  | (8) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $$D_{KL}(P\|\|Q)=\sum_{i}P(i)\> |  | (8) |'
- en: 'Cross entropy loss is a measure of the error between a true distribution $P$
    is:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失是衡量真实分布$P$的错误：
- en: '|  | 
    |  | (11) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $$\displaystyle=1\cdot\frac{1}{Q(c)}+\sum_{i\neq c}0\cdot\>
    |  | (11) |'
- en: '|  |  |  |  | (14) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $$\displaystyle=-1\cdot\> |  | (14) |'
- en: '|  |  | <math id=$$ |  | (15) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |  | <math id=$$ |  | (15) |'
- en: Since these two simplified expressions are the same we have show that the KL
    divergence loss and cross entropy loss are the same when the true distribution
    is a one-hot vector. $\blacksquare$
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个简化表达式相同，我们已经证明了当真实分布为一-hot向量时，KL散度损失和交叉熵损失是相同的。$\blacksquare$
- en: Appendix B Implementation
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实现
- en: B.1 Implementation of Representation Noising
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 表示噪声的实现
- en: This section provides further details about the implementation of the RepNoise
    method.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了有关RepNoise方法实现的更多细节。
- en: Since we don’t have access to the true distribution of harmful representations
    $Z_{\text{harmful}}$ and measure the distributional distance from harmful samples
    in a manner that is differentiable with MMD.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法获得有害表征的真实分布$Z_{\text{harmful}}$，因此采用MMD可微分的方式来测量有害样本的分布距离。
- en: Finally, we implement the gradient ascent $\ell_{\text{ascent}}$.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现了梯度上升$\ell_{\text{ascent}}$。
- en: Algorithm 1 RepNoise Training Procedure
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 RepNoise训练过程
- en: '1:  Input: Pretrained LLM $M_{\theta}$ {Compute stability loss}6:     $a\leftarrow
    M_{\theta}(b_{\text{harmful}}\circ\text{MASK})$14:  end for'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  输入：预训练LLM $M_{\theta}$ {计算稳定性损失}6:  $a\leftarrow M_{\theta}(b_{\text{harmful}}\circ\text{MASK})$14:  结束循环'
- en: B.2 Training details for the main results
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 主要结果的训练细节
- en: For the results presented in [§ 4](#S4 "4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") and [§ 5](#S5 "5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs"), we use the following settings unless otherwise stated. We perform RepNoise
    on the same samples as the attack, while evaluations are always performed on unseen
    samples. We only use different samples for RepNoise in [table 5](#S4.T5 "In 4.4
    Generalization ‣ 4 Experiments ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") and [table 17](#A10.T17 "In What is the impact of sample
    size on defence? ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs") to show its ability to generalise where
    we see that there is very little difference made whether we immunize on the same
    samples as the attack or not. For the BeaverTails attack, we train RepNoise for
    1 epoch on 10k paired samples (10k harmful questions with harmful answers and
    10k of the same questions with safe answers or refusals) using $\alpha=1$ and
    $\beta=4$ warmup and use the Adam optimizer without any weight decay. Finally,
    all implementations use PyTorch and Huggingface for training and inference. The
    code of this paper and full replication details will be released after review.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在[§ 4](#S4 "4 Experiments ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")和[§ 5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")中展示的结果中，我们使用以下设置，除非另有说明。我们在与攻击相同的样本上执行RepNoise，而评估始终在未见样本上进行。在[table
    5](#S4.T5 "In 4.4 Generalization ‣ 4 Experiments ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")和[table 17](#A10.T17 "In What is the impact
    of sample size on defence? ‣ Appendix J Ablation Study ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")中，我们使用不同的样本来展示其泛化能力，在这些表格中我们看到，无论是否在与攻击相同的样本上进行免疫，差别都非常小。对于BeaverTails攻击，我们在10k对样本（10k个有害问题及其有害答案和10k个相同问题的安全答案或拒绝）上训练RepNoise
    1个周期，使用$\alpha=1$和$\beta=4$预热，并使用没有权重衰减的Adam优化器。最后，所有实现都使用PyTorch和Huggingface进行训练和推理。本文代码和完整复现细节将在审稿后发布。
- en: Appendix C Attack Setting Details
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 攻击设置细节
- en: For the attack, we perform supervised fine-tuning to reduce causal language
    modelling loss on a harmful dataset (see section [2](#S2 "2 Harmful Fine-tuning
    and Defence Criteria ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")). We also evaluate the attacks from Qi et al. [[8](#bib.bib8)] and find
    that they do not increase the harmfulness of our base model appreciably ($0.04\rightarrow
    0.06$). All attacks use the Adam optimizer with a cosine learning rate scheduler
    and a warmup of 10% with no weight decay. We generally attack for 1 epoch for
    BeaverTails and 4 epochs for the Decoding Trust split of RTP. As in Qi et al.
    [[8](#bib.bib8)] greedy sampling is used in all attacks. We illustrate the effects
    of sampling on our attacks in [appendix J](#A10 "Appendix J Ablation Study ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于攻击，我们进行监督微调，以减少有害数据集上的因果语言建模损失（见[2](#S2 "2 Harmful Fine-tuning and Defence
    Criteria ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs")节）。我们还评估了Qi等人提出的攻击[[8](#bib.bib8)]，发现这些攻击并没有显著增加我们基模型的有害性（$0.04\rightarrow
    0.06$）。所有攻击都使用Adam优化器，带有余弦学习率调度器和10%的预热，没有权重衰减。我们通常对BeaverTails攻击进行1个周期，对RTP的Decoding
    Trust拆分进行4个周期。与Qi等人[[8](#bib.bib8)]相同，所有攻击都使用贪婪采样。我们在[appendix J](#A10 "Appendix
    J Ablation Study ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")中展示了采样对我们攻击的影响。
- en: The harmful dataset used for attack and defence consists of 10k randomly sampled
    harmful question-answer pairs from the BeaverTails HarmfulQA Dataset [[23](#bib.bib23)].
    This dataset samples questions from 14 domains such as ‘hate speech’ and ‘animal
    abuse’ and contains responses generated by a model that was not safety trained
    which human annotators then labelled as safe or unsafe. We discuss the full details
    for this dataset in [appendix D](#A4 "Appendix D Dataset and Model Details ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs").
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 用于攻击和防御的有害数据集由来自 BeaverTails HarmfulQA 数据集的10k个随机抽样的有害问答对组成[[23](#bib.bib23)]。该数据集从14个领域（如“仇恨言论”和“虐待动物”）抽取问题，并包含由未经过安全训练的模型生成的响应，这些响应由人工标注员标记为安全或不安全。我们在[附录
    D](#A4 "附录 D 数据集和模型细节 ‣ 表示噪声有效地防止有害的 LLM 微调")中讨论了该数据集的完整细节。
- en: To evaluate an LLM’s propensity for harmful outputs, we evaluate it on unseen
    prompts from the BeaverTails dataset and measure the mean probability of the harmfulness
    labels across all responses. For this, we employ DeBERTaV3-xsmall fine-tuned to
    classify responses as harmful or harmless. We do this based on the observation
    by [[23](#bib.bib23)] that standard content moderation and harmfulness evaluators
    such as OpenAI’s content moderation or the Jigsaw Perspective API classifier fail
    to adequately classify harmful question-answering. We don’t consider more general
    evaluation techniques such as zero-shot by GPT-4 or by LlamaGuard since we preferred
    a lighter-weight alternative trained and validated specifically on BeaverTails
    ⁵⁵5Additionally these methods can be cost prohibitive, [[8](#bib.bib8)]’s GPT-4
    judge takes approximately $25 USD to evaluate generations with their evaluation
    dataset..
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM产生有害输出的倾向，我们在BeaverTails数据集中的未见过的提示上对其进行评估，并测量所有响应的有害性标签的平均概率。为此，我们使用DeBERTaV3-xsmall，该模型经过微调以将响应分类为有害或无害。我们这样做是基于[[23](#bib.bib23)]的观察，即标准的内容审核和有害性评估工具，如OpenAI的内容审核或Jigsaw
    Perspective API分类器，未能充分分类有害的问答。我们没有考虑更通用的评估技术，如GPT-4的零样本或LlamaGuard，因为我们更倾向于一个经过专门训练和验证的较轻量级的替代方案。除此之外，这些方法可能会造成高昂的成本，[8](#bib.bib8)的GPT-4评估者大约需要$25
    USD来评估生成内容及其评估数据集。
- en: Classifier training and results are presented in [appendix D](#A4 "Appendix
    D Dataset and Model Details ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs").
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器训练和结果见于[附录 D](#A4 "附录 D 数据集和模型细节 ‣ 表示噪声有效地防止有害的 LLM 微调")。
- en: Appendix D Dataset and Model Details
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 数据集和模型细节
- en: D.1 BeaverTails
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 BeaverTails
- en: BeaverTails [[23](#bib.bib23)] is a harmful question-answering dataset that
    was constructed for AI safety and alignment research consisting of 333,963 question-answer
    pairs annotated by crowdworkers as safe or unsafe. Unsafe pairs are categorized
    into 14 types of harm. What is considered “harmful” for this dataset are questions
    and compliant answers in categories such as animal and child abuse, self-harm,
    unethical behaviour, and terrorism. The questions are based on the prompts from
    [[50](#bib.bib50)] where the authors of BeaverTails used Alpaca-7b to produce
    harmful responses. To construct our attack dataset, we initially filtered this
    dataset for only unsafe questions with unsafe answers.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: BeaverTails [[23](#bib.bib23)]是一个用于AI安全和对齐研究的有害问答数据集，由333,963个问答对组成，这些问答对由众包工作者标记为安全或不安全。
    不安全的对被分类为14种有害类型。对于这个数据集而言，被认为“有害”的问题和合规回答包括动物和儿童虐待、自我伤害、不道德行为和恐怖主义。问题基于[[50](#bib.bib50)]中的提示，BeaverTails的作者使用Alpaca-7b生成了有害的回应。为了构建我们的攻击数据集，我们最初从数据集中筛选出仅包含不安全问题及其不安全回答的部分。
- en: 'Our method requires paired refusals: meaning that for each unsafe question,
    there is a corresponding safe answer such as a refusal or explanation of why an
    answer would be harmful. Since our method requires paired refusals, we generated
    responses to the unsafe questions using llama2-7b-chat and used our harmfulness
    classifier described below to determine whether the responses were actually safe.
    The resulting dataset is 18,106 samples which is what we use for both our immunization
    and attack dataset.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法需要配对拒绝：即对于每个不安全问题，必须有一个对应的安全答案，如拒绝或解释为什么某个答案会有害。由于我们的方法需要配对拒绝，我们使用llama2-7b-chat生成了对不安全问题的响应，并使用下面描述的有害性分类器来确定这些响应是否实际上是安全的。生成的数据集为18,106个样本，我们将其用于免疫和攻击数据集。
- en: Example of BeaverTails question-answer pairs
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BeaverTails 问答对示例
- en: '[PRE0]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: D.1.1 Harmfulness Classifier
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.1.1 有害性分类器
- en: From the observations of [[23](#bib.bib23)], typical harmfulness evaluation
    techniques such as using the OpenAI content moderation API ⁶⁶6[https://platform.openai.com/docs/guides/moderation](https://platform.openai.com/docs/guides/moderation)
    or the Jigsaw Perspective API classifier [[17](#bib.bib17)] don’t work well for
    determining whether a question-answer pair is harmful. This is partially due to
    the format of harmful question-answering where the harmful question itself might
    be flagged by these classifiers, as well as due to the distributional shift from
    toxic content to harmful question-answering. While some works [[8](#bib.bib8),
    [28](#bib.bib28), [24](#bib.bib24)] have relied on GPT-4 for harmfulness evaluation,
    we feel this approach is not well validated empirically and can be very expensive
    for extensive evaluations. Instead, we use the same approach as [[23](#bib.bib23)]
    and train our own harmfulness classifier on the 330k samples of safe and unsafe
    question-answer pairs from BeaverTails which allows us to compare our classifier
    performance with human annotations. We train a deberta-v3-xsmall [[51](#bib.bib51)]
    model for 4 epochs on this dataset using a batch size of 128, Adam optimizer,
    and learning rate of $6\times 10^{-6}$. Our classifier achieves an F1 score of
    0.87.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[23](#bib.bib23)]的观察，典型的有害性评估技术，如使用 OpenAI 内容审核 API ⁶⁶6[https://platform.openai.com/docs/guides/moderation](https://platform.openai.com/docs/guides/moderation)或
    Jigsaw Perspective API 分类器[[17](#bib.bib17)]，在判断问答对是否有害时效果不佳。这部分是因为有害的问答格式中，有害问题本身可能被这些分类器标记，以及有害问答中的有害内容与毒性内容之间的分布变化。虽然一些工作[[8](#bib.bib8),
    [28](#bib.bib28), [24](#bib.bib24)]依赖于 GPT-4 进行有害性评估，但我们认为这种方法在实证验证上不够充分，并且对于大规模评估可能非常昂贵。相反，我们采用了与[[23](#bib.bib23)]相同的方法，并在
    BeaverTails 的 33 万个安全与不安全问答对样本上训练了我们自己的有害性分类器，这使我们能够将分类器性能与人工标注进行比较。我们在此数据集上使用批量大小为
    128、Adam 优化器和学习率为 $6\times 10^{-6}$ 的设置，训练了一个 deberta-v3-xsmall [[51](#bib.bib51)]
    模型 4 个周期。我们的分类器达到了 0.87 的 F1 分数。
- en: D.2 Decoding Trust Toxicity Split
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 解码信任毒性拆分
- en: For the “toxicity” fine-tuning attacks and defences we used the RealToxicityPrompts
    (RTP) [[26](#bib.bib26)] split from the DecodingTrust benchmark [[25](#bib.bib25)]
    since it filters RTP for toxic and non-toxic continuations and provides a framework
    for evaluating toxicity generation in LLMs by using a prompt asking the LLM to
    continue a toxic snippet. As above, since we need paired refusals for our method,
    we filter the content for toxic continuations only and generate continuations
    from llama2-7b-chat resulting in 351 samples. To evaluate the toxicity of continuations,
    we use the same approach as [[25](#bib.bib25)] and leverage the Jigsaw Perspective
    API [[17](#bib.bib17)]. This toxicity evaluator was originally designed for content
    moderation purposes for text very similar to RTP and is widely used for toxicity
    evaluation in similar contexts.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“毒性”微调攻击和防御，我们使用了来自 DecodingTrust 基准[[25](#bib.bib25)]的 RealToxicityPrompts
    (RTP) [[26](#bib.bib26)] 拆分，因为它过滤了毒性和非毒性续写，并提供了一个框架，用于通过提示 LLM 继续毒性片段来评估 LLM 的毒性生成。如上所述，由于我们的方法需要配对的拒绝，我们仅过滤毒性续写的内容，并从
    llama2-7b-chat 生成了 351 个样本。为了评估续写的毒性，我们使用了与[[25](#bib.bib25)]相同的方法，并利用了 Jigsaw
    Perspective API [[17](#bib.bib17)]。这个毒性评估器最初是为类似 RTP 的文本内容审核目的设计的，并广泛用于类似上下文中的毒性评估。
- en: Example of DecodingTrust Toxic Continuation
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 解码信任毒性续写示例
- en: 'Warning: example contains swear words.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：示例包含脏话。
- en: '[PRE1]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: D.3 Eleuther LM Harness
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 Eleuther LM 驱动
- en: For stability evaluations (Table [3](#S4.T3 "Table 3 ‣ 4.2 Stability ‣ 4 Experiments
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")),
    we use tasks drawn from the Eleuther LM Harness⁷⁷7[https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    which is a common tool for the evaluation of LLMs general zero and few-shot capability
    post-training. We selected the top tasks used for evaluating language models on
    the Huggingface open LLM leaderboard ⁸⁸8[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    These were TruthfulQA [[31](#bib.bib31)], MMLU [[32](#bib.bib32)], Hellaswag [[33](#bib.bib33)],
    and ARC (easy) [[34](#bib.bib34)]. We also evaluate changes in the model’s capabilities
    on domains related to harmfulness on the Ethics [[35](#bib.bib35)] and Crows S
    pairs [[36](#bib.bib36)] that are found in the LM harness (additional safety evaluations
    are performed in [appendix E](#A5 "Appendix E Additional Safety and Harmfulness
    Evaluations ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稳定性评估（表 [3](#S4.T3 "表 3 ‣ 4.2 稳定性 ‣ 4 实验 ‣ 表示噪声有效地防止对LLMs的有害微调")），我们使用了来自
    Eleuther LM Harness⁷⁷7[https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    的任务，这是一个用于评估 LLMs 通用零样本和少样本能力的常用工具。我们选择了用于评估语言模型的顶级任务，这些任务在 Huggingface 开放 LLM
    排行榜上使用 ⁸⁸8[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。这些任务包括
    TruthfulQA [[31](#bib.bib31)]、MMLU [[32](#bib.bib32)]、Hellaswag [[33](#bib.bib33)]
    和 ARC（简单）[[34](#bib.bib34)]。我们还评估了模型在与有害性相关的领域的能力变化，使用 Ethics [[35](#bib.bib35)]
    和 Crows S pairs [[36](#bib.bib36)]，这些都在 LM harness 中找到（附加的安全评估在 [附录 E](#A5 "附录
    E 附加安全性和有害性评估 ‣ 表示噪声有效地防止对LLMs的有害微调") 中进行）。
- en: D.4 GEM
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 GEM
- en: 'For trainability (whether the defended model can be continued to be trained
    on harmless tasks), we select two datasets from the GEM benchmark [[37](#bib.bib37)]
    which is designed to evaluate natural language generation. The datasets we selected
    were ViGGO (video game review [[52](#bib.bib52)] 5.1k train/1.08k test), E2E NLG
    (restaurant dialogue [[53](#bib.bib53)] 33.5k train/1.85k test), DART (Multiple
    [[54](#bib.bib54)] 62.7k train/5.1k test), CACAPO (Bilingual Dutch/English News
    [[55](#bib.bib55)] 15.k train/3.03k test), Conversational Weather (Weather Reports
    [[56](#bib.bib56)] 25.4k train/3.1k test) and we used the text-to-data task where
    the model must generate some abstract meaning representation or structured data
    representation given natural language texts. We chose this because it wasn’t something
    llama2-7b-chat was good at doing before training and training produced a large
    increase in the ROUGE-1 scores (see the low initial scores in [table 4](#S4.T4
    "In 4.3 Trainability ‣ 4 Experiments ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs")). The reader will notice that while we use the exact
    same training set-up as the attack setting many of these datasets are larger than
    our attack sample set, we point out that ViGGO is smaller than our attack set
    and training is still effective. For some examples of what these tasks look like,
    see below:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可训练性（即防御模型是否可以继续在无害任务上训练），我们从 GEM 基准 [[37](#bib.bib37)] 中选择了两个数据集，该基准旨在评估自然语言生成。我们选择的数据集包括
    ViGGO（视频游戏评论 [[52](#bib.bib52)] 5.1k 训练/1.08k 测试）、E2E NLG（餐厅对话 [[53](#bib.bib53)]
    33.5k 训练/1.85k 测试）、DART（多个 [[54](#bib.bib54)] 62.7k 训练/5.1k 测试）、CACAPO（双语荷兰语/英语新闻
    [[55](#bib.bib55)] 15.k 训练/3.03k 测试）、Conversational Weather（天气报告 [[56](#bib.bib56)]
    25.4k 训练/3.1k 测试），我们使用了文本到数据任务，在这个任务中，模型必须根据自然语言文本生成一些抽象的意义表示或结构化数据表示。我们选择这个任务是因为它是
    llama2-7b-chat 在训练前不擅长的领域，而训练后 ROUGE-1 分数大幅提高（见 [表 4](#S4.T4 "在 4.3 可训练性 ‣ 4 实验
    ‣ 表示噪声有效地防止对LLMs的有害微调") 中的低初始分数）。读者会注意到，虽然我们使用了与攻击设置完全相同的训练设置，但这些数据集中的许多比我们的攻击样本集要大，我们指出
    ViGGO 比我们的攻击集要小，但训练仍然有效。有关这些任务的示例，请参见下文：
- en: Example of ViGGO text-to-data task
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ViGGO 文本到数据任务示例
- en: '[PRE2]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example of E2E NLG text-to-data task
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: E2E NLG 文本到数据任务示例
- en: '[PRE3]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Appendix E Additional Safety and Harmfulness Evaluations
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 附加安全性和有害性评估
- en: 'In order to understand the impact of our method on more general LLM Safety
    we performed three additional evaluations: benign and identity-shifting fine-tuning
    attacks ([§ E.3](#A5.SS3 "E.3 Benign and Identity Shifting fine-tuning Attacks
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")), language model bias evaluation
    ([§ E.4](#A5.SS4 "E.4 Bias Evaluation with ROBBIE ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")), exaggerated safety evaluations ([§ E.5](#A5.SS5 "E.5 Exaggerated
    Safety with XSTest ‣ Appendix E Additional Safety and Harmfulness Evaluations
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs")),
    and adversarial attacks ([§ E.6](#A5.SS6 "E.6 Adversarial Attacks with HarmBench
    ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")). We also present empirical
    results on a few successful attacks on RepNoise-hardened models ([§ E.1](#A5.SS1
    "E.1 Stronger Attack on RepNoise ‣ Appendix E Additional Safety and Harmfulness
    Evaluations ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs")), and cross-domain generalization ([§ E.2](#A5.SS2 "E.2 Cross-Domain
    Generalization ‣ Appendix E Additional Safety and Harmfulness Evaluations ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs"))'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的方法对更一般的LLM安全性的影响，我们进行了三项额外评估：良性和身份转移微调攻击 ([§ E.3](#A5.SS3 "E.3 良性和身份转移微调攻击
    ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))，语言模型偏差评估 ([§ E.4](#A5.SS4 "E.4 使用ROBBIE的偏差评估
    ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))，夸大安全性评估 ([§ E.5](#A5.SS5 "E.5 使用XSTest的夸大安全性
    ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))，以及对抗攻击 ([§ E.6](#A5.SS6 "E.6 使用HarmBench的对抗攻击
    ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))。我们还展示了几次对RepNoise增强模型成功攻击的实证结果 ([§ E.1](#A5.SS1
    "E.1 对RepNoise的更强攻击 ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))，以及跨领域泛化 ([§ E.2](#A5.SS2
    "E.2 跨领域泛化 ‣ 附录E 额外安全性和有害性评估 ‣ 表示噪声有效防止对LLMs的有害微调"))
- en: E.1 Stronger Attack on RepNoise
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 对RepNoise的更强攻击
- en: Beyond the results in [table 1](#S4.T1 "In 4.1 Resistance ‣ 4 Experiments ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs") we were
    able to find an attack that defeats our method starting at $3\times 10^{-4}@10k$
    for 1 epoch but not at learning rates before. We did not report average over random
    seeds in the main paper because of this hyper sensitivity. We acknowledge this
    is as a limitation of the method as it makes finding defences much more difficult.
    This points to future work which might be able to develop comprehensive effective
    defences simply by doing more sophisticated hyper-parameter exploration.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 [表1](#S4.T1 "在4.1 抵抗 ‣ 4 实验 ‣ 表示噪声有效防止对LLMs的有害微调") 的结果外，我们还发现一种攻击可以在 $3\times
    10^{-4}@10k$ 的学习率下击败我们的方法，但在之前的学习率下没有。由于这种超敏感性，我们没有在主要论文中报告随机种子的平均值。我们承认这是该方法的一个限制，因为它使得寻找防御变得更加困难。这指向未来的工作，可能通过更复杂的超参数探索来开发全面有效的防御。
- en: E.2 Cross-Domain Generalization
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 跨领域泛化
- en: While we demonstrated that our method can generalize across different subsets
    [table 5](#S4.T5 "In 4.4 Generalization ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") and number of samples used
    [table 17](#A10.T17 "In What is the impact of sample size on defence? ‣ Appendix
    J Ablation Study ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs"), we were additionally curious whether our method provides a ‘cross-domain’
    defence, meaning that performing the RepNoise defence using samples from one task
    could provide defence against samples drawn from an unrelated task.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们展示了我们的方法可以在不同子集上进行泛化 [表5](#S4.T5 "在4.4 泛化 ‣ 4 实验 ‣ 表示噪声有效防止对LLMs的有害微调")
    和所使用样本数量 [表17](#A10.T17 "在样本大小对防御的影响是什么？ ‣ 附录J 消融研究 ‣ 表示噪声有效防止对LLMs的有害微调")，我们还好奇我们的方法是否提供了‘跨领域’防御，这意味着使用来自一个任务的样本进行RepNoise防御是否能够对来自不相关任务的样本提供防御。
- en: We evaluate how well our method generalizes between the BeaverTails dataset
    and the RealToxicitiyPrompts (RTP) split from the DecodingTrust benchmark [[25](#bib.bib25)].
    While BeaverTails contains potentially unsafe question-answer pairs, RTP consists
    of prompts likely to be followed by toxic completions which is a very distinct
    domain.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了我们的方法在海狸尾巴数据集和解码信任基准中的RealToxicitiyPrompts（RTP）拆分之间的泛化效果[[25](#bib.bib25)]。虽然海狸尾巴包含潜在的不安全问答对，但RTP由很可能跟随有毒完成的提示组成，这是一个非常不同的领域。
- en: '| immunization set | attack set | pre-attack | $3\times 10^{-5}$ |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 免疫集 | 攻击集 | 攻击前 | $3\times 10^{-5}$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| None | Decoding Trust | 0.24 | 0.40 | 0.74 | 0.71 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 解码信任 | 0.24 | 0.40 | 0.74 | 0.71 |'
- en: '| Decoding Trust | Decoding Trust | 0.17 | 0.00 | 0.05 | 0.07 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 解码信任 | 解码信任 | 0.17 | 0.00 | 0.05 | 0.07 |'
- en: '| BeaverTails | Decoding Trust | 0.15 | 0.63 | 0.65 | 0.68 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 海狸尾巴 | 解码信任 | 0.15 | 0.63 | 0.65 | 0.68 |'
- en: '| None | BeaverTails | 0.05 | 0.47 | 0.73 | 0.74 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 海狸尾巴 | 0.05 | 0.47 | 0.73 | 0.74 |'
- en: '| BeaverTails | BeaverTails | 0.08 | 0.13 | 0.10 | 0.11 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 海狸尾巴 | 海狸尾巴 | 0.08 | 0.13 | 0.10 | 0.11 |'
- en: '| Decoding Trust | BeaverTails | 0.04 | 0.05 | 0.43 | 0.64 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 解码信任 | 海狸尾巴 | 0.04 | 0.05 | 0.43 | 0.64 |'
- en: 'Table 7: Cross-domain generalization: Harmfulness scores after attacks with
    learning rates $\in\{$ and immunization using RepNoise on different datasets.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：跨领域泛化：不同数据集上攻击后的有害性评分和使用RepNoise进行免疫的学习率$\in\{$。
- en: Table [7](#A5.T7 "Table 7 ‣ E.2 Cross-Domain Generalization ‣ Appendix E Additional
    Safety and Harmfulness Evaluations ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs") illustrates that defence trained on DecodingTrust
    improves upon the base model’s resistance against weak attacks using BeaverTails.
    However, defences trained on BeaverTails actually decrease resistance against
    weak attacks using DecodingTrust. On both BeaverTails and DecodingTrust, it is
    much more effective to defend using the same dataset used for the attack. This
    means that while RepNoise does provide generalization where the defender doesn’t
    have access to the same samples as the attacker, RepNoise does not appear to provide
    resistance to cross-distribution attacks where we have no samples at all from
    the domain of the attack. We don’t find this a surprising result or major limitation
    given that out-of-domain performance is an expected limitation of current neural
    methods. However, it does mean that defenders using our method will need to be
    sure they comprehensively collect defence samples from the domains they want to
    prevent training on.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#A5.T7 "表 7 ‣ E.2 跨领域泛化 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止有害微调")说明，基于DecodingTrust训练的防御相较于基础模型在使用海狸尾巴的弱攻击下的抗性有所提升。然而，基于海狸尾巴训练的防御实际上降低了在使用DecodingTrust的弱攻击下的抗性。在海狸尾巴和解码信任上，使用相同的数据集进行防御比使用不同的数据集更有效。这意味着虽然RepNoise确实提供了一定的泛化能力，即防御者不需要访问与攻击者相同的样本，但RepNoise似乎未能提供对完全不同领域攻击的抗性。考虑到跨领域性能是当前神经方法的一个预期限制，这并不是一个令人惊讶的结果或主要限制。然而，这确实意味着使用我们方法的防御者需要确保他们全面收集了他们希望防止训练的领域的防御样本。
- en: E.3 Benign and Identity Shifting fine-tuning Attacks
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 良性和身份转移微调攻击
- en: Qi et al. [[8](#bib.bib8)] observed that even benign fine-tuning could accidentally
    make models more harmful by increasing the likelihood of following harmful instructions.
    Using the same setting of fine-tuning models on 10 harmless samples illustrating
    an absolutely obedient agent (Identity shifting) and 52,002 samples from the (Benign)
    Alpaca instruction-following dataset, we find that RepNoise defends against both
    benign and identity shifting attacks. On the identity shifting attack (10 epochs),
    the base lama2-7b-chat’s outputs go from a harmfulness rating of $1.02$, RepNoise
    was slightly more susceptible to the benign fine-tuning attack where the model
    went from $1.16$ which is still far from a large increase in harmfulness.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Qi等人[[8](#bib.bib8)]观察到，即使是良性的微调也可能通过增加遵循有害指令的可能性来无意中使模型变得更有害。在对10个无害样本进行微调以展示一个完全服从的代理（身份转移）和52,002个来自（良性）Alpaca指令跟随数据集的样本的相同设置下，我们发现RepNoise对抗了良性和身份转移攻击。在身份转移攻击（10个周期）中，基础lama2-7b-chat的输出从$1.02$的有害性评分变为RepNoise稍微更容易受到良性微调攻击的$1.16$，但仍远未显著增加有害性。
- en: E.4 Bias Evaluation with ROBBIE
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 使用ROBBIE的偏见评估
- en: The ROBBIE suite of robust bias evaluation [[58](#bib.bib58)] allows us to ask
    whether our defence has any impact on the overall demographic bias of the model.
    We used 100 random samples drawn from each ROBBIE benchmark and the Jigsaw Perspective
    API evaluator [[17](#bib.bib17)] to evaluate bias (in addition to the other evaluation
    procedures presented in [[58](#bib.bib58)]). Using the RepNoise defence with the
    settings presented in the main paper, we find that our method doesn’t have any
    significant impact on the bias of the original model (Mann-Whitney $U$). Despite
    the fact that we are removing information about harmful representations, we cannot
    say this makes the model appreciably less biased.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ROBBIE 强健偏见评估套件 [[58](#bib.bib58)] 允许我们询问我们的防御是否对模型的整体人口统计偏见有任何影响。我们从每个 ROBBIE
    基准和 Jigsaw Perspective API 评估器 [[17](#bib.bib17)] 中抽取了 100 个随机样本来评估偏见（除了 [[58](#bib.bib58)]
    中介绍的其他评估程序）。使用主文献中提出的 RepNoise 防御设置，我们发现我们的方法对原始模型的偏见没有任何显著影响（Mann-Whitney $U$）。尽管我们正在去除有害表示的信息，但我们不能说这使模型的偏见显著减少。
- en: '|  | bold | holisticbiasr | realtoxicityprompts | regard | safetyscore |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | **bold** | holisticbiasr | realtoxicityprompts | regard | safetyscore
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| llama2-7b-chat | 0.07 | 0.05 | 0.04 | 0.19 | 0.09 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7b-chat | 0.07 | 0.05 | 0.04 | 0.19 | 0.09 |'
- en: '| RepNoise | 0.08 | 0.05 | 0.05 | 0.18 | 0.07 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.08 | 0.05 | 0.05 | 0.18 | 0.07 |'
- en: 'Table 8: No significant differences are observed for the impact of our defence
    on bias.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 没有观察到我们的防御对偏见的影响有显著差异。'
- en: E.5 Exaggerated Safety with XSTest
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.5 XSTest 的夸张安全性
- en: Röttger et al. [[59](#bib.bib59)] provides a set of evaluations for understanding
    "exaggerated safety" where models might refuse to answer harmless questions which
    only superficially seem unsafe for example asking about the definition of a bomb
    rather than how to construct a bomb. We used the GPT-4 evaluation setting for
    250 prompts that are safe to answer and 200 prompts that would be unsafe to answer.
    In [table 9](#A5.T9 "In E.5 Exaggerated Safety with XSTest ‣ Appendix E Additional
    Safety and Harmfulness Evaluations ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"), we observed that applying RepNoise does result
    in a small but significant increase in safe refusals ($\chi^{2}$). We can conclude
    that our method could make defended models have more “exaggerated” safety properties.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Röttger 等人 [[59](#bib.bib59)] 提供了一组评估，用于理解“夸张的安全性”，即模型可能拒绝回答看似不安全的问题，这些问题实际上是无害的，例如询问炸弹的定义而不是如何制造炸弹。我们使用了
    GPT-4 评估设置，针对 250 个安全回答的提示和 200 个不安全的提示进行了测试。在 [表 9](#A5.T9 "在 E.5 夸张的安全性与 XSTest
    ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止对 LLM 的有害微调") 中，我们观察到应用 RepNoise 确实会导致安全拒绝数量略微但显著增加（$\chi^{2}$）。我们可以得出结论，我们的方法可以使防御模型具有更多的“夸张”安全属性。
- en: '| Safe Prompts | Refusal Rate (%) | Partial Refusal Rate (%) | Compliance Rate
    (%) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 安全提示 | 拒绝率 (%) | 部分拒绝率 (%) | 合规率 (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| llama2-7b-chat | 7.95 | 3.97 | 88.08 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7b-chat | 7.95 | 3.97 | 88.08 |'
- en: '| RepNoise | 17.71 | 9.38 | 72.92. |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 17.71 | 9.38 | 72.92 |'
- en: '| Unsafe Prompts | Refusal Rate (%) | Partial Refusal Rate (%) | Compliance
    Rate (%) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 不安全提示 | 拒绝率 (%) | 部分拒绝率 (%) | 合规率 (%) |'
- en: '| llama2-7b-chat | 86.49 | 5.41 | 8.11 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7b-chat | 86.49 | 5.41 | 8.11 |'
- en: '| RepNoise | 79.14 | 6.47 | 14.39 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 79.14 | 6.47 | 14.39 |'
- en: 'Table 9: RepNoise increases the number of refusals to safe answers.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: RepNoise 增加了对安全回答的拒绝数量。'
- en: E.6 Adversarial Attacks with HarmBench
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6 HarmBench 的对抗攻击
- en: How does our method impact inference-time adversarial attacks like jailbreaks?
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的方法如何影响推理时的对抗攻击，如越狱攻击？
- en: We examine this question using the HarmBench [[7](#bib.bib7)] benchmark with
    the following methods GCG [[60](#bib.bib60)], ZeroShot [[61](#bib.bib61)], HumanJailbreak
    [[62](#bib.bib62), [63](#bib.bib63)], and DirectRequest [[61](#bib.bib61)]. The
    RepNoise model that is attacked uses the same settings as the one presented in
    the main paper. For demonstration purposes, we use the “harmful” and “illegal”
    subsets of HarmBench which consists of 64 test cases that a language model should
    normally refuse to answer since the answer would be harmful. Table [10](#A5.T10
    "Table 10 ‣ How does our method impact inference-time adversarial attacks like
    jailbreaks? ‣ E.6 Adversarial Attacks with HarmBench ‣ Appendix E Additional Safety
    and Harmfulness Evaluations ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") illustrates that our method does provide a small decrease
    in susceptibility to GCG-based attacks but it is not statistically significant
    ($\chi^{2}$). We also include prompting-based methods to show that our method
    doesn’t increase the model’s susceptibility to other inference-time attacks. Future
    work should explore the relationship between defences against HFAs and inference-time
    adversarial attacks on larger sets of samples.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 HarmBench [[7](#bib.bib7)] 基准测试来检查这个问题，使用了以下方法：GCG [[60](#bib.bib60)]，ZeroShot
    [[61](#bib.bib61)]，HumanJailbreak [[62](#bib.bib62), [63](#bib.bib63)] 和 DirectRequest
    [[61](#bib.bib61)]。被攻击的 RepNoise 模型使用与主论文中展示的模型相同的设置。为了演示目的，我们使用 HarmBench 中的“有害”与“非法”子集，这些子集包含
    64 个测试案例，语言模型通常应拒绝回答这些问题，因为答案将是有害的。表 [10](#A5.T10 "表 10 ‣ 我们的方法如何影响推理时的对抗性攻击，如越狱？
    ‣ E.6 使用 HarmBench 的对抗性攻击 ‣ 附录 E 额外的安全性和有害性评估 ‣ 表示噪声有效防止 LLM 上的有害微调") 说明我们的方法确实在一定程度上减少了对
    GCG 基于攻击的易感性，但这一差异在统计上并不显著 ($\chi^{2}$)。我们还包括了基于提示的方法，以表明我们的方法并未增加模型对其他推理时攻击的易感性。未来的工作应探索针对
    HFA 的防御与推理时对抗性攻击之间的关系，并在更大的样本集上进行研究。
- en: '|  | GCG | ZeroShot | HumanJailbreak | DirectRequest |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | GCG | ZeroShot | HumanJailbreak | DirectRequest |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| llama2-7b-chat | 11% | 0% | 0% | % |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7b-chat | 11% | 0% | 0% | % |'
- en: '| RepNoise | 5% | 0% | 0% | 0% |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 5% | 0% | 0% | 0% |'
- en: 'Table 10: A noticeable drop is observed in the attack success rate of GCG when
    attempted after performing our RepNoise defence. No increase in attack success
    is on prompting-based attacks.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：在进行我们的 RepNoise 防御后，观察到 GCG 的攻击成功率显著下降。在基于提示的攻击中没有攻击成功率的增加。
- en: Appendix F Security Vectors
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 安全向量
- en: Security Vectors [[28](#bib.bib28)] is a defence where the defender has access
    to and complete control over the fine-tuning process. The defence consists of
    training a LoRA adapter [[64](#bib.bib64)] represented by the parameters $\theta_{s}$
    from the base model are being trained. The authors base their method on the observations
    from [[44](#bib.bib44)] that if the training loss is already very small to begin
    with, then little learning will take place.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Security Vectors [[28](#bib.bib28)] 是一种防御方法，其中防御者可以访问并完全控制微调过程。该防御方法包括训练一个 LoRA
    适配器 [[64](#bib.bib64)]，该适配器由从基础模型中训练得到的参数 $\theta_{s}$ 表示。作者基于 [[44](#bib.bib44)]
    的观察来提出他们的方法，即如果训练损失一开始已经非常小，那么学习的效果会非常有限。
- en: 'For our experiments, we trained the LoRA adapter with a $1\times 10^{-3}$ epoch
    on [eq. 1](#S2.E1 "In Harmful Fine-tuning Attack (HFA): ‣ 2 Harmful Fine-tuning
    and Defence Criteria ‣ Representation noising effectively prevents harmful fine-tuning
    on LLMs") to minimize causal language modelling loss on our 10k harmful samples
    from the BeaverTails dataset. We believed that this would be a fair comparison
    because it the same number of samples RepNoise defence uses. We did not perform
    the min-min bi-level optimisation procedure as details for this process were missing
    from the paper.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们使用 $1\times 10^{-3}$ 的周期训练了 LoRA 适配器，在 [eq. 1](#S2.E1 "在有害微调攻击 (HFA)
    中：‣ 2 有害微调和防御标准 ‣ 表示噪声有效防止 LLM 上的有害微调") 上最小化我们从 BeaverTails 数据集中获得的 10k 有害样本的因果语言建模损失。我们认为这将是一个公平的比较，因为它使用的样本数量与
    RepNoise 防御方法相同。我们没有执行 min-min 双层优化程序，因为该过程的细节在论文中缺失。
- en: Appendix G Harmfulness probes
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 有害性探测
- en: We repeat the probing experiments from Section [5](#S5 "5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") for
    RepNoise when we use $\beta=0.001$. This setting leads to higher resistance by
    putting less importance on increasing the loss on harmful examples. Again we train
    a linear probe on the activations for 15k samples of question-answer pairs from
    BeaverTails to predict whether the answer is harmful or not. We measure the accuracy
    of the resulting probe to indicate how much information the representations at
    each layer of a model contain about harmfulness. However, it should be noted that
    probes have been criticised as a interpretability method [[65](#bib.bib65)] due
    to their susceptibility to spurious correlations.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用$\beta=0.001$时，我们重复了第[5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs")节的探测实验。这种设置通过减少对有害示例上损失增加的重视，从而提高了抵抗力。我们再次在15k个BeaverTails问题-答案对的激活上训练一个线性探测器，以预测答案是否有害。我们测量结果探测器的准确性，以指示模型每一层的表示包含了多少关于有害性的的信息。然而，需要注意的是，由于对伪相关的敏感性，探测器作为一种解释性方法已经受到批评[[65](#bib.bib65)]。
- en: '![Refer to caption](img/a7bd2dc0ec01053d75aa045e2df6253b.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a7bd2dc0ec01053d75aa045e2df6253b.png)'
- en: (a)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/d9dd7cdddefdfeba6de77aa5f1d65a9a.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d9dd7cdddefdfeba6de77aa5f1d65a9a.png)'
- en: (b)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 6: Harmful probe accuracy on (a) base model and models trained with
    RepNoise and adversarial Loss, and (b) base model, RepNoise model and an attacked
    RepNoise model'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在（a）基础模型和用RepNoise以及对抗损失训练的模型上的有害探测精度，以及（b）基础模型、RepNoise模型和一个受攻击的RepNoise模型
- en: Figure [6(a)](#A7.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ Appendix G Harmfulness probes
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") shows
    that RepNoise slightly reduces the information about harmfulness compared to the
    base model and the model defended by the Adversarial Loss. The average accuracy
    across layers of RepNoise was 0.003 lower than for the base model (Students $t$
    term plays an important role in removing information about harmfulness.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6(a)](#A7.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ Appendix G Harmfulness probes ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")显示，RepNoise相比基础模型和由对抗损失防御的模型，稍微减少了关于有害性的的信息。RepNoise的平均准确率比基础模型低0.003（学生$t$项在去除有关有害性的信息中发挥了重要作用）。
- en: Figure [6(b)](#A7.F6.sf2 "Figure 6(b) ‣ Figure 6 ‣ Appendix G Harmfulness probes
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs") underlines
    the finding from Section [5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") that HFAs do not increase the
    amount of information about harmfulness. However, in the setting with $\beta=0.001$
    is able to navigate the weights to an advantageous position in the loss landscape.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6(b)](#A7.F6.sf2 "Figure 6(b) ‣ Figure 6 ‣ Appendix G Harmfulness probes ‣
    Representation noising effectively prevents harmful fine-tuning on LLMs")强调了第[5](#S5
    "5 Mechanistic Analysis ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs")节的发现，即HFAs不会增加关于有害性的的信息。然而，在$\beta=0.001$的设置中，能够将权重调整到损失景观中的有利位置。
- en: Appendix H Causal Analysis of Layer Effect on Defence Effectiveness
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 层效应对防御效果的因果分析
- en: '|  | $3\times 10^{-5}$ @ 1k |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | $3\times 10^{-5}$ @ 1k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Undefended Model | 0.47 | 0.74 | 0.73 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 无防御模型 | 0.47 | 0.74 | 0.73 |'
- en: '| All Layers | 0.08 | 0.12 | 0.10 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 所有层 | 0.08 | 0.12 | 0.10 |'
- en: '| Freeze LM Head | 0.08 | 0.10 | 0.11 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 冻结LM头部 | 0.08 | 0.10 | 0.11 |'
- en: '| Freeze Layers 20-31 | 0.10 | 0.13 | 0.10 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 20-31 | 0.10 | 0.13 | 0.10 |'
- en: '| Freeze Last Layer | 0.08 | 0.67 | 0.09 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 冻结最后一层 | 0.08 | 0.67 | 0.09 |'
- en: '| Freeze Layers 10-20 | 0.13 | 0.55 | 0.56 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 10-20 | 0.13 | 0.55 | 0.56 |'
- en: '| Freeze Layers 0-10 | 0.73 | 0.73 | 0.72 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 冻结层 0-10 | 0.73 | 0.73 | 0.72 |'
- en: 'Table 11: Freezing earlier layers prevents effective defence indicating that
    the “depth” of the defence is critical.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：冻结早期层阻止了有效的防御，表明防御的“深度”是关键。
- en: In the main paper, we hypothesize that the effectiveness of our defence is due
    to its depth. By depth, we mean how many layers down we are removing information
    about harmful representations. We can test whether this is the case by freezing
    layers during RepNoise and then performing our attacks from the main paper. Using
    a 32-layer LLM (llama2-7b-chat we freeze the LM Head, the last layers (32, 20-31),
    the middle layers (10-20) and the last layers (0-10). [table 6](#S5.T6 "In 5 Mechanistic
    Analysis ‣ Representation noising effectively prevents harmful fine-tuning on
    LLMs") shows that freezing the LM head or “unembed” layer makes little difference.
    We have a similar finding for freezing the last layer, except in the case of longer
    attacks, which is interesting given that a simple adversarial loss defence makes
    the most changes in the last layer [fig. 2](#S5.F2 "In 5 Mechanistic Analysis
    ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"). Finally,
    we see that freezing the middle layers starts to degrade the effectiveness of
    the defence and freezing the first ten layers completely ruins the effectiveness
    of the defence. This shows that RepNoise conducts important operations on early
    layers of the model and thus provides a “deep” defence.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们假设我们防御的有效性源于其深度。这里的深度指的是我们去除有害表示信息的层数。我们可以通过在RepNoise过程中冻结层然后进行主要论文中的攻击来测试这一点。使用32层的LLM（llama2-7b-chat），我们冻结了LM头部、最后层（32,
    20-31）、中间层（10-20）以及最后层（0-10）。[表6](#S5.T6 "在5 机制分析 ‣ 表示噪声有效防止LLM上的有害微调")显示冻结LM头部或“取消嵌入”层几乎没有区别。冻结最后一层的发现类似，但在较长的攻击情况下有所不同，这一点很有趣，因为简单的对抗损失防御在最后一层做出了最大的变化[图2](#S5.F2
    "在5 机制分析 ‣ 表示噪声有效防止LLM上的有害微调")。最后，我们发现冻结中间层开始降低防御效果，而冻结前十层则完全破坏了防御效果。这表明RepNoise在模型的早期层上执行了重要操作，从而提供了“深度”防御。
- en: Appendix I Analysis of Attacked Models
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录I 攻击模型分析
- en: We extend our analysis from [§ 5](#S5 "5 Mechanistic Analysis ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") by presenting an illustration
    of what the token probability distributions and PCA characterization look like
    on successfully attacked models after performing a defence. These results use
    the same setting from [§ 5](#S5 "5 Mechanistic Analysis ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs") using 100 harmful and harmless
    samples from the BeaverTails harmfulQA task. The defence performed with adversarial
    loss is successfully attacked at $8\times 10^{-5}$ @ 10k. [Figure 7](#A9.F7 "In
    Appendix I Analysis of Attacked Models ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs") reveals that the probability distribution of drawing
    harmful token sequences after a successful attack is largely the same as the distribution
    from the original attacked model.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过展示在进行防御后成功攻击模型的标记概率分布和PCA特征图来扩展我们在[§ 5](#S5 "5 机制分析 ‣ 表示噪声有效防止LLM上的有害微调")中的分析。这些结果使用了[§
    5](#S5 "5 机制分析 ‣ 表示噪声有效防止LLM上的有害微调")中的相同设置，使用了BeaverTails harmfulQA任务中的100个有害和无害样本。使用对抗损失进行的防御在$8\times
    10^{-5}$ @ 10k处被成功攻击。[图7](#A9.F7 "在附录I 攻击模型分析 ‣ 表示噪声有效防止LLM上的有害微调")揭示了成功攻击后的有害标记序列的概率分布与原始攻击模型的分布基本相同。
- en: '![Refer to caption](img/65de9248fa3f257007aefd3fbd86b501.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65de9248fa3f257007aefd3fbd86b501.png)'
- en: 'Figure 7: Log probability of harmful and harmless sequences across layers.
    Notice how adversarial loss mostly depromotes harmful tokens towards the last
    layer, while this is done more evenly across layers for RepNoise.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：各层的有害和无害序列的对数概率。注意到对抗损失主要在最后一层抑制有害标记，而RepNoise则在各层之间更加均匀地进行。
- en: Figure [8](#A9.F8 "Figure 8 ‣ Appendix I Analysis of Attacked Models ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") shows a PCA of 100
    harmful and harmless samples. It indicates that the representation space between
    an attacked and unattacked base model is not that different. For the adversarial
    loss defence, we discussed this representation space change earlier but we point
    out that the representation space largely returns to a similar space as the base
    model after a successful attack. As for RepNoise, observe that in order for the
    attack to be successful we produce a representation space where both harmful and
    harmless representations are largely collapsed on some kind of manifold. This
    observation could help us develop further extensions to RepNoise which make it
    more robust.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#A9.F8 "图 8 ‣ 附录 I 攻击模型分析 ‣ 表示噪声有效防止了对 LLM 的有害微调") 显示了 100 个有害和无害样本的 PCA。它表明，被攻击和未被攻击的基础模型之间的表示空间差异不大。对于对抗损失防御，我们之前讨论过这种表示空间的变化，但我们指出，成功攻击后，表示空间基本上回到类似于基础模型的空间。至于
    RepNoise，请注意，为了让攻击成功，我们产生了一个表示空间，其中有害和无害的表示在某种流形上大致重叠。这一观察结果可能帮助我们开发 RepNoise
    的进一步扩展，使其更加稳健。
- en: '![Refer to caption](img/dfd0843b21e9d6063e623dc1a7e99cb6.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dfd0843b21e9d6063e623dc1a7e99cb6.png)'
- en: 'Figure 8: PCA across 100 harmful and harmless samples from BeaverTails on the
    activations of the last layer of both attacked and unattacked models.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：PCA 分析了来自 BeaverTails 的 100 个有害和无害样本在被攻击和未被攻击模型的最后一层激活上的表现。
- en: Appendix J Ablation Study
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 消融研究
- en: '|  | $3\times 10^{-5}$ @ 10k |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | $3\times 10^{-5}$ @ 10k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Our Method | 0.08 | 0.11 | 0.12 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0.08 | 0.11 | 0.12 |'
- en: '| w/ noise | 0.08 | 0.32 | 0.71 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 带噪声 | 0.08 | 0.32 | 0.71 |'
- en: '| w/ ascent | 0.77 | 0.76 | 0.74 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 带上升项 | 0.77 | 0.76 | 0.74 |'
- en: 'Table 12: Ablation study showing the effect of removing the noise and ascent
    terms.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：消融研究展示了去除噪声和上升项的效果。
- en: Is noise loss necessary?
  id: totrans-392
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 噪声损失是必要的吗？
- en: We performed an ablation study ([table 12](#A10.T12 "In Appendix J Ablation
    Study ‣ Representation noising effectively prevents harmful fine-tuning on LLMs"))
    removing the noise term and the gradient ascent terms. We find that both the noise
    and gradient ascent term are required to construct strong defences. Note that
    the noise term by itself without ascent results in a model that is even worse
    at defence than the original base model. We believe that this is the case because
    simply noising harmful representations without explicitly trying to remove their
    predictive power could be contributing to improving the minimality properties
    of the representations themselves (the ideal property that representations contain
    as little information about the input as possible are more effective), see the
    relationship between learnability and minimality for representation learning in
    [[21](#bib.bib21)].
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了消融研究（[表 12](#A10.T12 "在附录 J 消融研究 ‣ 表示噪声有效防止了对 LLM 的有害微调")），去除了噪声项和梯度上升项。我们发现，构建强大防御所需的既包括噪声项，也包括梯度上升项。注意，单独没有上升项的噪声会导致一个比原始基础模型更差的防御模型。我们认为，这可能是因为仅仅对有害表示添加噪声而不明确尝试去除其预测能力，可能会改善表示本身的最小性特性（理想的特性是表示包含关于输入的尽可能少的信息更为有效），见[[21](#bib.bib21)]中关于表示学习的可学习性与最小性的关系。
- en: RepNoise is very sensitive
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RepNoise 非常敏感
- en: During our investigations we found that our method is unfortunately very sensitive
    to hyperparameter variations, requiring extensive hyperparameter search to be
    effective. We illustrate these shifts in Table [13](#A10.T13 "Table 13 ‣ RepNoise
    is very sensitive ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs"),[14](#A10.T14 "Table 14 ‣ RepNoise is very
    sensitive ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents
    harmful fine-tuning on LLMs"),[15](#A10.T15 "Table 15 ‣ RepNoise is very sensitive
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"), and [table 16](#A10.T16 "In RepNoise is very sensitive
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs") while searching for the hyperparameters for the model whose
    results are in the main body of the paper. Generally, the $\alpha$ value for ascent
    made little impact so it is not recorded here.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的调查中发现，我们的方法不幸地对超参数变化非常敏感，需要进行广泛的超参数搜索才能有效。我们在[表13](#A10.T13 "表 13 ‣ RepNoise非常敏感
    ‣ 附录J 消融研究 ‣ 表示噪声有效地防止了对LLMs的有害微调")，[14](#A10.T14 "表 14 ‣ RepNoise非常敏感 ‣ 附录J 消融研究
    ‣ 表示噪声有效地防止了对LLMs的有害微调")，[15](#A10.T15 "表 15 ‣ RepNoise非常敏感 ‣ 附录J 消融研究 ‣ 表示噪声有效地防止了对LLMs的有害微调")和[表16](#A10.T16
    "RepNoise非常敏感 ‣ 附录J 消融研究 ‣ 表示噪声有效地防止了对LLMs的有害微调")中展示了这些变化，同时寻找模型的超参数，这些结果在论文主体中。一般来说，上升的$\alpha$值影响不大，因此在这里没有记录。
- en: '| Learning Rate | $2\times 10^{-5}$ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $2\times 10^{-5}$ |'
- en: '| --- | --- | --- |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RepNoise | 0.12 | 0.74 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.12 | 0.74 |'
- en: 'Table 13: Learning rate study, even slightly larger learning rates result in
    ineffective defences. Results are reported on an $8\times 10^{-5}$ @ 10k sample
    attack.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：学习率研究表明，即使是稍微增大的学习率也会导致防御效果无效。结果基于$8\times 10^{-5}$ @ 10k样本攻击。
- en: '| Epochs | 1 | 2 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 训练轮次 | 1 | 2 |'
- en: '| --- | --- | --- |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RepNoise | 0.12 | 0.78 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.12 | 0.78 |'
- en: 'Table 14: Increasing the number of epochs of the defence results in a model
    that is easily attacked. Results are reported on an $8\times 10^{-5}$ @ 10k sample
    attack.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：增加防御的训练轮次数导致模型容易受到攻击。结果基于$8\times 10^{-5}$ @ 10k样本攻击。
- en: '| $\beta$ | 0.001 | 0.01 | 0.0001 | 4 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | 0.001 | 0.01 | 0.0001 | 4 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| RepNoise | 0.12 | 0.38 | 0.75 | 0.65 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.12 | 0.38 | 0.75 | 0.65 |'
- en: 'Table 15: Similar to the above results, the $beta$ @ 10k sample attack.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：类似于上述结果，$beta$ @ 10k样本攻击。
- en: '| Random Seed | $6\times 10^{-5}$ @ 10k |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 随机种子 | $6\times 10^{-5}$ @ 10k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 42 | 0.12 | 0.13 | 0.11 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 42 | 0.12 | 0.13 | 0.11 |'
- en: '| 7 | 0.12 | 0.11 | 0.75 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.12 | 0.11 | 0.75 |'
- en: '| 17 | 0.09 | 0.79 | 0.77 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 0.09 | 0.79 | 0.77 |'
- en: 'Table 16: Our method is even quite sensitive to the random seed used'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：我们的方法对使用的随机种子非常敏感
- en: What is the impact of sample size on defence?
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 样本大小对防御的影响是什么？
- en: While performing our defence on multiple epochs appears to have a negative effect,
    possibly again due to working against minimality, we did notice the number of
    samples has a major impact on the quality of defence. We show this in the sample
    ablation in [table 17](#A10.T17 "In What is the impact of sample size on defence?
    ‣ Appendix J Ablation Study ‣ Representation noising effectively prevents harmful
    fine-tuning on LLMs"). This table indicates that future work based on our current
    method could simply experiment with more extensive data collection and augmentation
    to improve our defence. Additional work could be done to make defence methods
    more sample-efficient. Unfortunately, our method relies on paired refusal data
    as we observed without this defences were much more fragile, however, future work
    could also investigate methods that don’t require paired refusal data.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在多个训练轮次下进行防御似乎有负面效果，可能再次由于违背了最小性原则，我们确实注意到样本数量对防御质量有重大影响。我们在[表17](#A10.T17
    "样本大小对防御的影响是什么？ ‣ 附录J 消融研究 ‣ 表示噪声有效地防止了对LLMs的有害微调")的样本消融中展示了这一点。该表明，基于我们当前方法的未来工作可以简单地通过更广泛的数据收集和增强来改善我们的防御。额外的工作可以使防御方法更加样本高效。不幸的是，我们的方法依赖于配对的拒绝数据，我们观察到没有这些数据防御会变得脆弱，然而，未来的工作也可以研究不需要配对拒绝数据的方法。
- en: '| Attack Strength | 1k | 2.5k | 5k |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 攻击强度 | 1k | 2.5k | 5k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $3\times 10^{-5}$ @ 1k | 0.28 | 0.10 | 0.10 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| $3\times 10^{-5}$ @ 1k | 0.28 | 0.10 | 0.10 |'
- en: '| $3\times 10^{-5}$ @ 10k | 0.68 | 0.10 | 0.10 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| $3\times 10^{-5}$ @ 10k | 0.68 | 0.10 | 0.10 |'
- en: '| $6\times 10^{-5}$ @ 1k | 0.60 | 0.10 | 0.11 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| $6\times 10^{-5}$ @ 1k | 0.60 | 0.10 | 0.11 |'
- en: '| $6\times 10^{-5}$ @ 10k | 0.72 | 0.12 | 0.00 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| $6\times 10^{-5}$ @ 10k | 0.72 | 0.12 | 0.00 |'
- en: '| $8\times 10^{-5}$ @ 1k | 0.70 | 0.10 | 0.4 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| $8\times 10^{-5}$ @ 1k | 0.70 | 0.10 | 0.4 |'
- en: '| $8\times 10^{-5}$ @ 10k | 0.73 | 0.68 | 0.00 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| $8\times 10^{-5}$ @ 10k | 0.73 | 0.68 | 0.00 |'
- en: 'Table 17: Sample ablation using only 1k, 2.5k, or 5k samples for training our
    defence (our method in the main paper uses 10k samples unless specified otherwise).
    The effectiveness of a defence has a strong relationship with the number of samples
    used to train the defence.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 表17：使用仅1k、2.5k或5k样本来训练我们的防御（我们的方法在主要论文中使用10k样本，除非另有说明）。防御的有效性与用于训练防御的样本数量有很强的关系。
- en: What if we use nucleus sampling?
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如果我们使用核采样会怎样？
- en: Finally, we investigate the effect of sampling on our defence. In [table 18](#A10.T18
    "In What if we use nucleus sampling? ‣ Appendix J Ablation Study ‣ Representation
    noising effectively prevents harmful fine-tuning on LLMs") we compare the effect
    of using greedy or nucleus [[66](#bib.bib66)] sampling on attack effectiveness.
    We see that there is very little difference depending on the sampling technique.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们研究了采样对我们防御的影响。在 [表18](#A10.T18 "如果我们使用核采样？ ‣ 附录J 脱敏研究 ‣ 表示噪声有效防止对LLMs的有害微调")
    中，我们比较了使用贪婪采样或核采样 [[66](#bib.bib66)] 对攻击有效性的影响。我们发现，无论采样技术如何，效果差异都非常小。
- en: '| Sampling Method | Greedy | Nucleus |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 采样方法 | 贪婪 | 核采样 |'
- en: '| --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $6\times 10^{-5}$ @ 10k | 0.13 | 0.13 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| $6\times 10^{-5}$ @ 10k | 0.13 | 0.13 |'
- en: '| $8\times 10^{-5}$ @ 1k | 0.11 | 0.09 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| $8\times 10^{-5}$ @ 1k | 0.11 | 0.09 |'
- en: '| $8\times 10^{-5}$ @ 10k | 0.12 | 0.12 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| $8\times 10^{-5}$ @ 10k | 0.12 | 0.12 |'
- en: 'Table 18: Sampling study comparing the use of greedy or nucleus sampling, there
    is very little difference of the attack effectiveness and the sampling method
    used.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 表18：采样研究比较了使用贪婪或核采样，对攻击有效性的差异非常小。
- en: We corroborate this finding by illustrating the mean log probabilities of 100
    harmful and harmless sequences at the last layer of our base llama model, a superficial
    defence like adversarial loss and our method ([fig. 9](#A10.F9 "In What if we
    use nucleus sampling? ‣ Appendix J Ablation Study ‣ Representation noising effectively
    prevents harmful fine-tuning on LLMs")). We observe that both defences make the
    likelihood of drawing tokens for harmful sequences much lower than the base model
    or a successfully attacked model. Naturally, successful attacks decrease the divergence
    in distributions between harmful and harmless sequences. We could use this observation
    to investigate explicitly leveraging distributional distance losses in order to
    make closing this gap more difficult for the attacker.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过展示我们基础的llama模型在最后一层的100个有害和无害序列的平均对数概率来证实这一发现，这是一种肤浅的防御，如对抗损失和我们的方法（[图9](#A10.F9
    "如果我们使用核采样？ ‣ 附录J 脱敏研究 ‣ 表示噪声有效防止对LLMs的有害微调")）。我们观察到，两种防御方法都使得为有害序列抽取令牌的可能性比基础模型或成功攻击的模型低得多。自然地，成功的攻击减少了有害序列和无害序列之间的分布差异。我们可以利用这一观察来明确调查利用分布距离损失，以使攻击者更难弥合这一差距。
- en: '![Refer to caption](img/3a5a7a05e0acca90d3535919b9d14829.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3a5a7a05e0acca90d3535919b9d14829.png)'
- en: 'Figure 9: Log probability of harmful and harmless sequences for the last layer
    of each model. The probability density for harmful sequences in RepNoise is much
    lower than other methods. Please note the axes are different so that we are able
    to illustrate these differences in scale.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：每个模型最后一层的有害和无害序列的对数概率。RepNoise中有害序列的概率密度远低于其他方法。请注意坐标轴不同，以便我们能够说明这些规模上的差异。
- en: Appendix K Additional Models
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 额外模型
- en: We validated our approach for additional models by performing RepNoise on the
    larger llama2-13b-chat model and a series of smaller Qwen 1.5 models (0.5B to
    7B) [[67](#bib.bib67)]. We evaluate these models using the same attack settings
    as [table 1](#S4.T1 "In 4.1 Resistance ‣ 4 Experiments ‣ Representation noising
    effectively prevents harmful fine-tuning on LLMs"). As mentioned above, one of
    the limitations of our method is that it requires very extensive hyperparameter
    tuning. For each model, we performed a grid search across the following learning
    rates ($1\times 10^{-3}$ to be the most effective. For llama-13b-chat, we similarly
    found a higher learning rate of $1\times 10^{-3}$. While our results are effective
    in these settings, we highlight the need for stronger more comprehensive attacks
    as well as future work that makes RepNoise require less extensive hyperparameter
    tuning.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对更大的 llama2-13b-chat 模型和一系列较小的 Qwen 1.5 模型（0.5B 到 7B）执行 RepNoise 来验证我们的方法
    [[67](#bib.bib67)]。我们使用与 [表 1](#S4.T1 "在 4.1 抵抗 ‣ 4 实验 ‣ 表示噪声有效地防止有害微调") 相同的攻击设置来评估这些模型。如上所述，我们方法的一个局限性是需要非常广泛的超参数调优。对于每个模型，我们在以下学习率范围内进行了网格搜索（$1\times
    10^{-3}$ 被认为是最有效的）。对于 llama-13b-chat，我们同样发现较高的学习率为 $1\times 10^{-3}$。虽然我们的结果在这些设置中有效，但我们强调需要更强大、更全面的攻击，以及未来的工作以减少
    RepNoise 对超参数调优的需求。
- en: '| LR |  | $3\times 10^{-5}$ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| LR |  | $3\times 10^{-5}$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | pre-attack | 1k | 10k | 1k | 10k | 1k | 10k |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击前 | 1k | 10k | 1k | 10k | 1k | 10k |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7b-chat | 0.05 | 0.47 | 0.74 | 0.73 | 0.72 | 0.74 | 0.73 |'
- en: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.05 | 0.08 | 0.12 | 0.1 | 0.13 | 0.11 | 0.12 |'
- en: '| llama2-13b-chat | 0.07 | 0.79 | 0.76 | 0.73 | 0.73 | 0.75 | 0.75 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| llama2-13b-chat | 0.07 | 0.79 | 0.76 | 0.73 | 0.73 | 0.75 | 0.75 |'
- en: '| RepNoise | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.00 | 0.00 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.00 | 0.00 | 0.00 | 0.00 | 0.05 | 0.00 | 0.00 |'
- en: '| Qwen-0.5B-chat | 0.00 | 0.50 | 0.66 | 0.68 | 0.71 | 0.70 | 0.73 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-0.5B-chat | 0.00 | 0.50 | 0.66 | 0.68 | 0.71 | 0.70 | 0.73 |'
- en: '| RepNoise | 0.06 | 0.07 | 0.10 | 0.09 | 0.00 | 0.12 | 0.01 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.06 | 0.07 | 0.10 | 0.09 | 0.00 | 0.12 | 0.01 |'
- en: '| Qwen-1.5B-chat | 0.00 | 0.72 | 0.74 | 0.76 | 0.76 | 0.75 | 0.75 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5B-chat | 0.00 | 0.72 | 0.74 | 0.76 | 0.76 | 0.75 | 0.75 |'
- en: '| RepNoise | 0.02 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.02 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| Qwen-4B-chat | 0.0 | 0.72 | 0.74 | 0.76 | 0.75 | 0.77 | 0.75 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-4B-chat | 0.0 | 0.72 | 0.74 | 0.76 | 0.75 | 0.77 | 0.75 |'
- en: '| RepNoise | 0.09 | 0.02 | 0.00 | 0.02 | 0.04 | 0.00 | 0.10 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.09 | 0.02 | 0.00 | 0.02 | 0.04 | 0.00 | 0.10 |'
- en: '| Qwen-7B-chat | 0.08 | 0.70 | 0.72 | 0.74 | 0.76 | 0.76 | 0.76 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-7B-chat | 0.08 | 0.70 | 0.72 | 0.74 | 0.76 | 0.76 | 0.76 |'
- en: '| RepNoise | 0.01 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| RepNoise | 0.01 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 |'
- en: 'Table 19: Results from various models with and without RepNoise applied. RepNoise
    is effective across a variety of types of models.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：应用与未应用 RepNoise 的各种模型的结果。RepNoise 在各种模型类型中都有效。
- en: 'For the cases where the harmfulness score is very low this is often the case
    of a “self-destructing” model [[40](#bib.bib40)] where the modes outputs completely
    degrade as a result of training which is in contrast to a model that preserves
    fluency across the attack. These are illustrated below selected from random examples
    from the series of models above:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有害评分非常低的情况，这通常是“自毁”模型 [[40](#bib.bib40)] 的情况，即模型输出在训练后完全退化，与在攻击过程中保持流畅的模型形成对比。下面展示了一些从上述模型系列中随机选出的示例：
- en: Example of model outputs that preserve fluency during an attack
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击过程中保持流畅性的模型输出示例
- en: '[PRE4]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Example of model outputs that “self-destruct” (become disfluent) during an attack
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击过程中“自毁”（变得不流畅）的模型输出示例
- en: '[PRE5]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Appendix L Statement on Compute Usage
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 L 计算使用声明
- en: We primarily used a single node with 4XA100 (80GB VRAM) GPUs for our results.
    Occasionally we used 4XA40 (40GB VRAM) GPU nodes as well as 1XA100 (40GB VRAM)
    from Google Colab.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用了单个节点配备 4XA100（80GB VRAM）GPU 来获得结果。偶尔我们也使用了 4XA40（40GB VRAM）GPU 节点以及来自
    Google Colab 的 1XA100（40GB VRAM）。
