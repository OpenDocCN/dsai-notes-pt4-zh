- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:37:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多目标微调以提升LLMs的程序修复能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.12636](https://ar5iv.labs.arxiv.org/html/2404.12636)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.12636](https://ar5iv.labs.arxiv.org/html/2404.12636)
- en: Boyang Yang^(1,2)¹, Haoye Tian^(3,4)¹, Jiadong Ren¹, Hongyu Zhang⁵, Jacques
    Klein⁴, Tegawendé F. Bissyandé⁴,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Boyang Yang^(1,2)¹, Haoye Tian^(3,4)¹, Jiadong Ren¹, Hongyu Zhang⁵, Jacques
    Klein⁴, Tegawendé F. Bissyandé⁴,
- en: Claire Le Goues³, Shunfu Jin¹² 1Co-first authors who contributed equally to
    this work.2Corresponding author. ¹School of Information Science and Engineering,
    Yanshan University,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Claire Le Goues³, Shunfu Jin¹² 1共同第一作者，平等贡献。2通讯作者。 ¹燕山大学信息科学与工程学院，
- en: ²Jisuan Institute of Technology, Beijing JudaoYouda Network Technology Co. Ltd.,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²计算机技术研究所，北京巨道友达网络科技有限公司，
- en: ³School of Computer Science, Carnegie Mellon University,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³卡内基梅隆大学计算机科学学院，
- en: ⁴SnT, University of Luxembourg,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴卢森堡大学SnT，
- en: ⁵School of Big Data and Software Engineering, Chongqing University yangboyang@jisuanke.com,
    tianhaoyemail@gmail.com, jdren@ysu.edu.cn, hongyujohn@gmail.com,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵重庆大学大数据与软件工程学院 yangboyang@jisuanke.com, tianhaoyemail@gmail.com, jdren@ysu.edu.cn,
    hongyujohn@gmail.com,
- en: jacques.klein@uni.lu, tegawende.bissyande@uni.lu, clegoues@cs.cmu.edu, jsf@ysu.edu.cn
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: jacques.klein@uni.lu, tegawende.bissyande@uni.lu, clegoues@cs.cmu.edu, jsf@ysu.edu.cn
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have demonstrated remarkable capabilities on a
    broad spectrum of downstream tasks. Within the realm of software engineering,
    specialized tasks on code, such as program repair, present unique challenges,
    necessitating fine-tuning to unlock state-of-the-art performance. Fine-tuning
    approaches proposed in the literature for LLMs on program repair tasks are however
    generally overlooking the need to reason about the logic behind code changes,
    beyond syntactic patterns in the data. High-performing fine-tuning experiments
    also usually come at very high computational costs. With MORepair, we propose
    a novel perspective on the learning focus of LLM fine-tuning for program repair:
    we not only adapt the LLM parameters to the syntactic nuances of the task of code
    transformation (objective ❶), but we also specifically fine-tune the LLM with
    respect to the logical reason behind the code change in the training data (objective
    ❷). Such a multi-objective fine-tuning will instruct LLMs to generate high-quality
    patches.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在广泛的下游任务中展示了卓越的能力。在软件工程领域，代码相关的专门任务，如程序修复，提出了独特的挑战，需进行微调以实现最先进的性能。然而，文献中提出的LLMs程序修复任务的微调方法通常忽略了对代码更改背后的逻辑进行推理的需求，仅关注数据中的语法模式。高性能的微调实验通常伴随非常高的计算成本。通过MORepair，我们提出了一种新的LLM微调学习重点的视角：我们不仅将LLM参数适应于代码转换任务的语法细微差别（目标
    ❶），还专门针对训练数据中的代码更改逻辑进行微调（目标 ❷）。这种多目标微调将指导LLMs生成高质量的修补程序。
- en: We apply MORepair to fine-tune four open-source LLMs with different sizes and
    architectures. Experimental results on C++ and Java repair benchmarks show that
    the implemented fine-tuning effectively boosts LLM repair performance by 7.6%
    to 10% in Top-10 repair suggestions. We further show that our fine-tuning strategy
    yields superior performance compared to the incumbent state-of-the-art in fine-tuned
    models for program repair, Fine-tune-CoT and RepairLLaMA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用MORepair对四种不同规模和架构的开源LLMs进行微调。在C++和Java修复基准上的实验结果表明，所实现的微调有效地提高了LLM修复性能，在Top-10修复建议中提高了7.6%至10%。我们进一步展示了我们的微调策略与当前最先进的微调模型Fine-tune-CoT和RepairLLaMA相比，具有更优的性能。
- en: I Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'Large language models have achieved promising performance on a variety of tasks
    in different domains. In software engineering, among many code-related tasks,
    automated program repair (APR) has greatly benefited from the general knowledge
    encoded in prominent models such as GPT-4 [[1](#bib.bib1)]. Recent studies [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)] have indeed shown that LLMs can even outperform
    traditional APR tools. Researchers have realized these achievements through two
    main strategies: prompt engineering and fine-tuning. Indeed, to steer LLMs towards
    adapting to the specific format of repair, few-shot learning techniques have been
    employed [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]
    where a small set of example patches are provided in the prompt along with the
    buggy code to repair. While few-shot-based approaches have shown better performance
    than initial zero-short-based attempts [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)],
    prompting is inherently limited by the pre-trained model capabilities. Prompting
    further often fails to produce high-quality patches within the constraints of
    developers’ attempt limits [[13](#bib.bib13)]. In contrast, fine-tuning-based
    approaches [[4](#bib.bib4), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)] strive to refine the fundamental capabilities of LLMs and have
    therefore demonstrated substantially greater potential in achieving reliable program
    repair. In practice, fine-tuning consists in adapting a pre-trained LLM on a very
    specific dataset, such as patches, or task, such as program repair, enabling the
    model to refine its knowledge and improve performance in targeted areas [[18](#bib.bib18)].
    Unfortunately, the existing literature proposes approaches that still face two
    major limitations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在不同领域的多种任务上已经取得了令人鼓舞的成果。在软件工程中，自动化程序修复（APR）作为众多与代码相关的任务之一，从诸如GPT-4等著名模型中编码的一般知识中受益匪浅[[1](#bib.bib1)]。近期的研究[[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]确实表明，LLMs甚至可以超越传统的APR工具。研究人员通过两种主要策略实现了这些成就：提示工程和微调。确实，为了引导LLMs适应特定的修复格式，采用了少样本学习技术[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]，在提示中提供了一小组示例补丁和待修复的有问题代码。虽然基于少样本的方法比最初的零样本尝试表现更好[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)]，但提示本质上受到预训练模型能力的限制。进一步的提示往往无法在开发者尝试限制的约束下生成高质量的补丁[[13](#bib.bib13)]。相比之下，基于微调的方法[[4](#bib.bib4),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]努力完善LLMs的基本能力，因此在实现可靠程序修复方面展示了显著更大的潜力。在实践中，微调包括将预训练的LLM适应于非常具体的数据集（如补丁）或任务（如程序修复），使模型能够在目标领域中细化其知识和提高性能[[18](#bib.bib18)]。不幸的是，现有文献提出的方法仍面临两个主要限制：
- en: ①
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ①
- en: 'Need for Reasoning on Repair Logic: The program repair task is complex: it
    demands some deep comprehension of control and data flow, of the developer intentions
    in the design of the buggy code, and finally of the intrinsic repair logic. Yet,
    most of the standard fine-tuning approaches for LLM-based program repair focus
    on optimizing the training dataset [[4](#bib.bib4), [15](#bib.bib15), [19](#bib.bib19)].
    Thus, while with such approaches the LLMs can be refined to notice some repair
    patterns, the actual logic reasoning behind the repair operation (”the why”) is
    not explicitly learned.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对修复逻辑的推理需求：程序修复任务是复杂的：它需要对控制和数据流、开发者在设计有问题代码时的意图，以及内在的修复逻辑有一定的深刻理解。然而，大多数标准的LLM程序修复微调方法侧重于优化训练数据集[[4](#bib.bib4),
    [15](#bib.bib15), [19](#bib.bib19)]。因此，尽管通过这些方法LLMs可以被细化以注意到一些修复模式，但修复操作背后的实际逻辑推理（“为什么”）并没有被明确学习。
- en: ②
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ②
- en: 'High Cost: Fine-tuning for program repair generally requires large datasets
    to achieve state of the art performance. In recent works, Lajkó *et al.* [[15](#bib.bib15)]
    used 16k samples to fine-tune GPT-2, while RepairLLaMA [[17](#bib.bib17)] was
    fine-tuned with about 30-50k code pairs. Such large datasets further suggest the
    expenditure of significant computational resource. Creating and expanding these
    datasets takes substantial effort and time, emphasizing the resource-intensive
    nature of fine-tuning in program repair. With limited public datasets available,
    manual construction of training data further increases labor costs.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高成本：程序修复的微调通常需要大型数据集以实现最先进的性能。在最近的工作中，Lajkó *et al.* [[15](#bib.bib15)] 使用了
    16k 样本来微调 GPT-2，而 RepairLLaMA [[17](#bib.bib17)] 则使用了大约 30-50k 代码对进行微调。这些大规模的数据集进一步表明了显著的计算资源消耗。创建和扩展这些数据集需要大量的努力和时间，突显了程序修复中微调的资源密集型特性。由于公开数据集有限，手动构建训练数据进一步增加了劳动成本。
- en: This paper. In this work, we propose a new fine-tuning objective using natural
    language explanations of code changes to capture the logic underlying a given
    repair operation. This objective, which seeks conversational guidance, is considered
    in addition to the classical objective of learning code transformations. MORepair
    is thus designed as a novel, effective multi-objective fine-tuning framework for
    LLM-based program repair.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文。在这项工作中，我们提出了一种新的微调目标，使用自然语言解释代码变更，以捕捉给定修复操作背后的逻辑。这个目标，在寻求对话式指导的同时，作为学习代码转换的经典目标的补充。MORepair
    因此被设计为一个新颖、有效的多目标微调框架，用于基于 LLM 的程序修复。
- en: 'By focusing on conversational guidance, i.e., natural language, MORepair ensures
    that the learning is programming language-independent, making it suitable for
    multilingual repair scenarios. Furthermore, by conducting multi-objective learning,
    we indirectly scale up the learning dataset: more pattern combinations can be
    explored in a small dataset. We also observe that conversational guidance presents
    the benefit of providing various potential fix strategies that extend beyond the
    confines of a specific buggy code. As such, our approach does not depend on large-scale
    datasets for fine-tuning that are required by prior works. Experimentally, we
    show that with an order of magnitude smaller dataset, we achieve higher fine-tuning
    performance than prior works. Finally, to account for insufficient/missing patch
    descriptions, we rely on LLMs to generate high-quality patch guidance. The successful
    application of such automatically generated guidance is essential as it relieves
    APR from this expensive human input.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过专注于对话式指导，即自然语言，MORepair 确保学习过程与编程语言无关，使其适用于多语言修复场景。此外，通过进行多目标学习，我们间接扩大了学习数据集：可以在小数据集中探索更多的模式组合。我们还观察到，对话式指导的好处在于提供了超越特定有缺陷代码范围的各种潜在修复策略。因此，我们的方法不依赖于以往工作所需的大规模数据集进行微调。通过实验，我们展示了在数据集规模较小的情况下，我们取得了比以往工作更高的微调性能。最后，为了应对补丁描述不足/缺失的问题，我们依赖
    LLMs 生成高质量的补丁指导。成功应用这些自动生成的指导至关重要，因为它减轻了 APR 对这种昂贵人工输入的依赖。
- en: 'We apply MORepair to fine-tune four open-source LLMS, namely CodeLlama-13B-instruct [[20](#bib.bib20)],
    CodeLlama-7B-instruct [[20](#bib.bib20)], StarChat-alpha [[21](#bib.bib21)], and
    Mistral-Instruct-7B-v0.1 [[22](#bib.bib22)], which are chosen to represent a variety
    of model sizes and architectures. These are assessed against two new repair benchmarks,
    EvalRepair-C++ and EvalRepair-Java, which we produced based on HumanEval [[23](#bib.bib23)]
    by including augmented test cases to avoid patch over-fitting [[24](#bib.bib24)].
    The experiments demonstrate that the proposed fine-tuning technique effectively
    improves the LLM performance on the program repair task: CodeLlama-13B-instruct
    performance is improved by 11% and 8% on the EvalRepair-C++ and EvalRepair-Java
    benchmarks, respectively. Similar performance improvements have been observed
    across all LLMs. We also show that MORepair is indeed superior to the fine-tuning
    approaches used for state of the art models such as Fine-tune-CoT [[25](#bib.bib25)]
    and RepairLLaMA [[17](#bib.bib17)]. Finally, we show that MORepair has the ability
    to narrow the performance gap between small open-source models and larger closed-source
    models.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将MORepair应用于四个开源LLM进行微调，即CodeLlama-13B-instruct [[20](#bib.bib20)]、CodeLlama-7B-instruct [[20](#bib.bib20)]、StarChat-alpha [[21](#bib.bib21)]和Mistral-Instruct-7B-v0.1 [[22](#bib.bib22)]，这些模型代表了多种模型规模和架构。我们将这些模型与基于HumanEval [[23](#bib.bib23)]的两个新修复基准，EvalRepair-C++和EvalRepair-Java进行评估，这些基准通过加入扩展测试用例来避免修补程序过拟合 [[24](#bib.bib24)]。实验表明，所提出的微调技术有效地提高了LLM在程序修复任务中的表现：CodeLlama-13B-instruct在EvalRepair-C++和EvalRepair-Java基准上的性能分别提高了11%和8%。所有LLM上都观察到了类似的性能提升。我们还展示了MORepair确实优于用于最先进模型的微调方法，如Fine-tune-CoT [[25](#bib.bib25)]和RepairLLaMA [[17](#bib.bib17)]。最后，我们表明MORepair能够缩小小型开源模型与大型闭源模型之间的性能差距。
- en: 'The main contributions of our work are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Approach. We propose MORepair, a novel multi-objective fine-tuning framework
    designed specifically for LLM-based program repair. MORepair steers LLMs towards
    a precise understanding the reasoning logic behind the repair process, thereby
    enabling them to generate high-quality patches.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法。我们提出了MORepair，一个新颖的多目标微调框架，专门针对基于LLM的程序修复设计。MORepair引导LLM们准确理解修复过程中的推理逻辑，从而使其能够生成高质量的修补程序。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Benchmarks. We provide two new repair benchmarks, EvalRepair-C++ and EvalRepair-Java,
    consisting of 164 and 163 patches (pairs of code samples), respectively. EvalRepair-C++
    was created by manually introducing bugs into the ground truth C++ code from HumanEval-X [[26](#bib.bib26)],
    while EvalRepair-Java is derived from HumanEval-Java [[4](#bib.bib4)]. To mitigate
    patch overfitting impact on the reported performance metrics, we augment the original
    test suites of both benchmarks: we indeed observe a decline of up to $\sim$9%
    in terms of top-10 repair predictions from CodeLlama-13B-instruct when we apply
    it to the new EvalRepair-C++ benchmark.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基准测试。我们提供了两个新的修复基准，EvalRepair-C++和EvalRepair-Java，分别包含164和163个修补程序（代码样本对）。EvalRepair-C++是通过手动将错误引入HumanEval-X [[26](#bib.bib26)]中的真实C++代码而创建的，而EvalRepair-Java则源于HumanEval-Java [[4](#bib.bib4)]。为了减轻修补程序过拟合对报告性能指标的影响，我们扩展了两个基准的原始测试集：我们确实观察到，当我们将其应用于新的EvalRepair-C++基准时，CodeLlama-13B-instruct的前10名修复预测性能下降了多达$\sim$9%。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiments. We conduct a comprehensive evaluation of MORepair’s effectiveness
    for improving the performance of open source LLMs, as well as its generalizability
    across various LLMs and different programming languages. The assessment also considers
    baseline models and baseline fine-tuning approaches.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验。我们对MORepair在提升开源LLM性能方面的有效性进行了全面评估，并考察了其在各种LLM和不同编程语言中的通用性。评估还考虑了基准模型和基准微调方法。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insights. Through an ablation study, we assess the impact of LLM-generated guidance
    within the MORepair framework. Additionally, we compare the repair performance
    of MORepair with state-of-the-art fine-tuning methods, including Fine-tune-CoT
    and RepairLLaMA. This study highlights the value of LLM-generated guidance in
    MORepair and underscores MORepair’s superior effectiveness over current fine-tuning
    approaches in program repair tasks.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 见解。通过消融研究，我们评估了LLM生成的指导在MORepair框架中的影响。此外，我们还将MORepair的修复性能与最先进的微调方法进行比较，包括Fine-tune-CoT和RepairLLaMA。这项研究突出了LLM生成的指导在MORepair中的价值，并强调了MORepair在程序修复任务中优于当前微调方法的卓越效果。
- en: II Motivating Example
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 激励示例
- en: 1---  0_58980_RE_SEGV.cpp2+++  0_58980_AC.cpp3@@  -35,8  +35,8  @@4  num.push(cal(tmp1,tmp2,opr));5  }6  op.pop();7  }else  if  (s[i]  ==  ’+’  ||  s[i]  ==  ’-’)  {8-  while  (!op.empty())  {9+  while  (!op.empty()&&op.top()!=’(’)  {10  int  tmp1  =  num.top();11  num.pop();12  int  tmp2  =  num.top();
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 1--- 0_58980_RE_SEGV.cpp2+++ 0_58980_AC.cpp3@@ -35,8 +35,8 @@4 num.push(cal(tmp1,tmp2,opr));5
    }6 op.pop();7 }else if (s[i] == ’+’ || s[i] == ’-’) {8- while (!op.empty()) {9+
    while (!op.empty()&&op.top()!=’(’) {10 int tmp1 = num.top();11 num.pop();12 int
    tmp2 = num.top();
- en: 'Figure 1: An example patch from TutorLLMCode.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：来自 TutorLLMCode 的一个示例补丁。
- en: Figure [1](#S2.F1 "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") provides an example patch for repairing
    a C++ program in the TutorLLMCode dataset. The bug is related to the handling
    of precedence in the operator of arithmetic expressions. When operation expressions
    are mixed or rather interleaved, the buggy program could yield inaccurate evaluation
    of the arithmetic expressions. In TutorLLMCode, this repair case is an example
    of a patch that was necessary to fix the solution provided by a student to an
    Expression Evaluation problem. The solution to this engineering problem is to
    introduce parentheses in basic arithmetic operations (addition, subtraction, multiplication,
    division). Writing this code without taking into account the presence of parentheses
    may lead to bugs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S2.F1 "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs")提供了在 TutorLLMCode 数据集中修复 C++ 程序的示例补丁。该错误与算术表达式中操作符的优先级处理有关。当操作表达式混合或交错时，有问题的程序可能会导致算术表达式的评估不准确。在
    TutorLLMCode 中，这个修复案例是一个补丁示例，修复了学生提供的表达式评估问题的解决方案。解决此工程问题的方法是引入基本算术操作（加法、减法、乘法、除法）中的括号。编写不考虑括号存在的代码可能会导致错误。
- en: 1Line  39,  also  ensure  that  the  top  element  of  the  stack  is  not  a  left  parenthesis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 1 行 39，也确保堆栈的顶部元素不是左括号。
- en: 'Listing 1: Human-generated guidance for yielding the patch in Figure [1](#S2.F1
    "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning for Enhanced Program
    Repair with LLMs")'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 1：用于生成图[1](#S2.F1 "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") 中补丁的人工生成指导。
- en: The human-generated guidance in Listing [1](#LST1 "Listing 1 ‣ II Motivating
    Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs")
    accurately points to the buggy location, and also conceptually describes the necessary
    code checks that are missing. We postulate that such information is precious as
    it informs about the logical reasoning behind the requested repair and abstracts
    away from the particular syntax of the associated program, enabling a fine-tuning
    process with this information to potentially generalize to a broader set of programs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[1](#LST1 "Listing 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs")中的人工生成指导准确地指向了有问题的位置，并且概念性地描述了缺失的必要代码检查。我们假设这样的信息非常宝贵，因为它提供了关于请求修复的逻辑推理，并且抽象化了相关程序的具体语法，使得利用这些信息进行的微调过程有可能泛化到更广泛的程序集。
- en: 'Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") depicts the LLM-generated guidance for
    this repair case. The LLM was prompted with the buggy along the fixed code and
    instructed to provide step-by-step descriptions of the repair logic. Interestingly,
    it goes into more depth and offers a more comprehensive and contextual analysis
    of the buggy code. The LLM identifies the primary issue: the correct handling
    of ’+’ and ’-’ operators when not preceded by a ’(’, and highlights four potential
    problems. Concretely, the LLM-guidance outlines in details some corrections by
    refining the logic for operator precedence, by ensuring accurate parenthesis handling,
    by mentioning parameter order adjustments in the cal function for subtraction
    and division, and by introducing error handling for enhanced robustness.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs")描述了此修复案例的 LLM 生成的指导。LLM 在接收到有问题的代码和修复后的代码后，提供了逐步描述修复逻辑的指令。有趣的是，它深入探讨并提供了更全面和上下文相关的有问题代码分析。LLM
    识别出主要问题：在没有前置’(’的情况下正确处理’+’和’-’操作符，并指出了四个潜在问题。具体而言，LLM 指导详细说明了一些修正，包括通过完善操作符优先级逻辑、确保准确的括号处理、提到在
    cal 函数中调整参数顺序以处理减法和除法，并引入错误处理以增强鲁棒性。
- en: '1Certainly!  Let’s  go  through  the  code  step  by  step  and  identify  the  issues  that  need  to  be  fixed:21.  **Operator  Precedence**:  In  the  original  code,  when  encountering  a  ’+’  or  ’-’,  it  immediately  evaluates  all  operations  in  the  stack  without  considering  operator  precedence.  This  is  incorrect  because  ’*’  and  ’/’  should  have  higher  precedence  and  should  be  evaluated  first.3  **Fix**:  When  encountering  a  ’+’  or  ’-’,  only  evaluate  the  top  of  the  stack  if  it  is  not  a  ’(’  and  if  the  operator  at  the  top  of  the  stack  is  ’+’  or  ’-’.  Do  not  evaluate  ’*’  or  ’/’  immediately;  they  should  be  evaluated  when  the  next  ’+’  or  ’-’  is  encountered  or  at  the  end  of  the  expression.42.  **Parentheses  Handling**:  The  original  code  correctly  handles  the  opening  parenthesis  ’(’,  but  when  it  encounters  a  closing  parenthesis  ’)’,  it  evaluates  all  operations  until  it  finds  the  corresponding  ’(’.  This  is  correct,  but  it  does  not  consider  operator  precedence  within  the  parentheses.5  **Fix**:  When  encountering  a  ’)’,  evaluate  the  operations  in  the  stack  until  you  reach  the  corresponding  ’(’.  Make  sure  to  evaluate  ’*’  and  ’/’  before  ’+’  and  ’-’.63.  **End  of  Expression**:  At  the  end  of  the  expression,  the  original  code  evaluates  all  remaining  operations  in  the  stack.  This  is  correct,  but  again,  it  does  not  consider  operator  precedence.7  **Fix**:  When  reaching  the  end  of  the  expression,  evaluate  the  remaining  operations  in  the  stack  while  considering  operator  precedence.  Evaluate  ’*’  and  ’/’  before  ’+’  and  ’-’.84.  **Function  ‘cal‘**:  The  ‘cal‘  function  has  the  parameters  in  the  wrong  order  for  subtraction  and  division.  This  will  lead  to  incorrect  results  when  these  operations  are  performed.9  **Fix**:  Swap  the  order  of  the  parameters  ‘x‘  and  ‘y‘  in  the  subtraction  and  division  cases  within  the  ‘cal‘  function  to  ensure  the  correct  order  of  operands.105.  **Error  Handling**:  The  original  code  does  not  handle  errors  or  invalid  input.  While  the  problem  statement  guarantees  valid  input,  it’s  good  practice  to  include  error  handling.11  **Fix**:  Add  error  handling  to  check  for  invalid  characters,  mismatched  parentheses,  or  other  potential  input  errors.  This  is  not  explicitly  required  by  the  problem  statement  but  is  a  good  practice.12By  addressing  these  issues,  the  code  should  correctly  evaluate  expressions  with  proper  operator  precedence  and  parentheses  handling.  Remember  to  test  the  code  with  various  expressions  to  ensure  that  all  edge  cases  are  handled  correctly.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 1当然！让我们一步步查看代码，并找出需要修复的问题：21. **操作符优先级**：在原始代码中，当遇到`+`或`-`时，它会立即评估栈中的所有操作，而不考虑操作符优先级。这是不正确的，因为`*`和`/`应该具有更高的优先级，应该先被评估。3
    **修复**：当遇到`+`或`-`时，仅在栈顶不是`(`且栈顶的操作符是`+`或`-`时才进行评估。不要立即评估`*`或`/`；它们应该在遇到下一个`+`或`-`时，或在表达式结束时进行评估。42.
    **括号处理**：原始代码正确处理了左括号`(`，但当遇到右括号`)`时，它会评估所有操作，直到找到相应的`(`。这没错，但它没有考虑括号内的操作符优先级。5
    **修复**：遇到`)`时，评估栈中的操作，直到到达相应的`(`。确保在`+`和`-`之前评估`*`和`/`。63. **表达式结束**：在表达式结束时，原始代码会评估栈中所有剩余的操作。这是正确的，但同样，它没有考虑操作符优先级。7
    **修复**：到达表达式末尾时，评估栈中剩余的操作时要考虑操作符优先级。在`+`和`-`之前评估`*`和`/`。84. **函数‘cal’**：‘cal’函数在减法和除法中的参数顺序不正确。这会导致执行这些操作时结果不正确。9
    **修复**：在‘cal’函数中交换减法和除法情况下参数`x`和`y`的顺序，以确保操作数的正确顺序。105. **错误处理**：原始代码没有处理错误或无效输入。虽然问题陈述保证了有效的输入，但添加错误处理是一个好习惯。11
    **修复**：添加错误处理，以检查无效字符、不匹配的括号或其他潜在的输入错误。这虽然问题陈述中并未明确要求，但这是一个良好的实践。12通过解决这些问题，代码应该能够正确评估表达式，处理正确的操作符优先级和括号。记得用各种表达式测试代码，以确保所有边缘情况都能正确处理。
- en: 'Listing 2: LLM-generated guidance for the patch in Figure [1](#S2.F1 "Figure
    1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs")'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2：图 [1](#S2.F1 "Figure 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") 中的 LLM 生成的补丁指导
- en: Our main intuition in this work is that the conversational guidance generated
    from repair examples will greatly benefit a model fine-tuning procedure for program
    repair.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这项工作中的主要直觉是，从修复示例生成的对话指导将极大地有利于模型的微调过程，从而实现程序修复。
- en: III Approach
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法
- en: In this section, we provide an overview of our proposed approach, followed by
    a detailed description of the methodology, which is divided into specific steps
    across several subsections.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了我们提出的方法的概述，随后对该方法的详细描述，方法分为几个子部分中的具体步骤。
- en: '![Refer to caption](img/b15e35e421fa4adce13e33dc8f6af37d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b15e35e421fa4adce13e33dc8f6af37d.png)'
- en: 'Figure 2: Overview of MORepair: The process unfolds in three phases—preparation,
    fine-tuning, and inference for code repair.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：MORepair 概述：该过程分为三个阶段——准备、微调和推断代码修复。
- en: '[Overview]: we introduce MORepair, a novel multi-objective fine-tuning framework
    that empowers open-source LLMs to grasp repair logic and produce high-quality
    patches effectively. Figure [2](#S3.F2 "Figure 2 ‣ III Approach ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") illustrates our approach,
    which unfolds in three phases: training preparation, multi-objective fine-tuning,
    and repair inference.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[概述]：我们介绍了 MORepair，一种新颖的多目标微调框架，旨在使开源 LLM 能够掌握修复逻辑并有效生成高质量的补丁。图 [2](#S3.F2
    "Figure 2 ‣ III Approach ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs") 展示了我们的方法，该方法分为三个阶段：训练准备、多目标微调和修复推断。'
- en: 'During the Training Preparation phase, we construct a dataset TutorLLMCode,
    consisting of 1,600 pairs of buggy and repaired code. This preparation includes
    LLM-generated guidance generated by GPT-4 to elucidate the nature of code bugs
    and their fixes (as detailed in Section [III-A](#S3.SS1 "III-A Training Preparation
    ‣ III Approach ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with
    LLMs")). The Multi-objective Fine-tuning phase applies the principles of multi-objective
    learning, targeting two specific learning objectives: (1) generating repaired
    code and (2) producing repaired code with guidance that explains the repair logic.
    Leveraging QLoRA allows for the fine-tuning of a low-rank adapter while freezing
    the original LLM parameters, cutting down the trainable parameters to only 1.84%
    and thus minimizing computational costs. In the Repair Inference phase, the ensemble
    of the pre-trained LLM and the fine-tuned repair adapter generates candidate patches
    for the provided buggy code, whose correctness is validated through the test cases
    from benchmarks.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练准备阶段，我们构建了一个数据集 TutorLLMCode，包含 1,600 对有缺陷和修复过的代码。这一准备工作包括 GPT-4 生成的 LLM
    指导，旨在阐明代码缺陷及其修复的性质（如 [III-A](#S3.SS1 "III-A Training Preparation ‣ III Approach
    ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") 节中详细说明）。多目标微调阶段应用了多目标学习的原则，针对两个特定学习目标：（1）生成修复后的代码，和（2）生成带有解释修复逻辑的修复代码。利用
    QLoRA 可以在冻结原始 LLM 参数的同时对低秩适配器进行微调，将可训练参数减少至仅 1.84%，从而降低计算成本。在修复推断阶段，预训练 LLM 与微调的修复适配器的组合生成候选补丁，这些补丁的正确性通过基准测试用例进行验证。
- en: III-A Training Preparation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 训练准备
- en: The initial step in our approach involves preparing the training dataset for
    fine-tuning. We utilize a dataset TutorLLMCode, provided by a company, which includes
    1,600 pairs of buggy codes and repaired codes across 45 distinct programming tasks,
    each accompanied by descriptions of the respective programming tasks. Recognizing
    the effectiveness of few-shot learning [[27](#bib.bib27)] for straightforward
    tasks, researchers explored the innovative use of rationale as intermediate steps,
    known as chain-of-thought (CoT) [[28](#bib.bib28)], to enhance LLMs’ reasoning
    abilities. By employing GPT-4-1106-preview, we generate guidance that clarifies
    the nature of the bug and the logic behind the patch in natural language. The
    prompts used for generating this guidance with GPT-4 are depicted in Figure [3](#S3.F3
    "Figure 3 ‣ III-A Training Preparation ‣ III Approach ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs"). Aimed at benefiting the wider community
    while mitigating future data leakage risks, TutorLLMCode will be publicly available
    through authorized API. Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") illustrates
    an example of guidance generated by GPT-4.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的初始步骤涉及准备用于微调的训练数据集。我们利用一个由公司提供的数据集 TutorLLMCode，其中包含 1,600 对有缺陷代码和修复代码，涵盖
    45 个不同的编程任务，每个任务附有相应编程任务的描述。鉴于少样本学习 [[27](#bib.bib27)] 对于简单任务的有效性，研究人员探索了作为中间步骤的推理的创新使用，称为思维链 (CoT) [[28](#bib.bib28)]，以增强
    LLM 的推理能力。通过使用 GPT-4-1106-preview，我们生成自然语言的指导，阐明错误的性质和补丁背后的逻辑。用于生成此指导的 GPT-4 提示如图
    [3](#S3.F3 "图 3 ‣ III-A 训练准备 ‣ III 方法 ‣ 多目标微调以增强 LLM 的程序修复") 所示。为了造福更广泛的社区，同时减少未来的数据泄漏风险，TutorLLMCode
    将通过授权 API 公开提供。列表 [2](#LST2 "列表 2 ‣ II 动机示例 ‣ 多目标微调以增强 LLM 的程序修复") 展示了 GPT-4 生成的指导示例。
- en: 1  This  is  a  programming  task  description  along  with  a  buggy  code:2  {{description}}3  {{buggy  code}}4  This  is  a  repaired  code:5  {{repaired  code}}6  Please  think  step  by  step  and  tell  me  how  to  fix  the  buggy  code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 1  这是一个编程任务描述及有缺陷的代码：2  {{description}}3  {{buggy code}}4  这是修复后的代码：5  {{repaired
    code}}6  请逐步思考并告诉我如何修复有缺陷的代码。
- en: 'Figure 3: The prompt to generate guidance utilizing GPT-4.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于生成指导的 GPT-4 提示。
- en: III-B Multi-objective Fine-tuning
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 多目标微调
- en: 'The second step of our approach involves fine-tuning LLMs through multi-objective
    learning. Multi-objective learning, a paradigm in machine learning, aims to leverage
    relevant information across multiple tasks simultaneously to enhance the performance
    of each task [[29](#bib.bib29)]. In this context, we propose our framework MORepair,
    which applies multi-objective learning to teach open-source LLMs the intricacies
    of the program repair process during fine-tuning. This approach enables the LLMs
    to generate high-quality patches for buggy code. Specifically, the LLMs are fine-tuned
    with two objectives: (1) generating repaired code and (2) producing repaired code
    accompanied by guidance that clarifies the nature of the bugs and their logic.
    To optimize for these objectives, we calculate separate losses for each, denoted
    as $Loss_{1}$ for producing both the repaired code and its explanatory guidance.
    These losses are then combined using the following equation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的第二步涉及通过多目标学习对 LLM 进行微调。多目标学习是机器学习中的一种范式，旨在利用多个任务中的相关信息来提高每个任务的表现 [[29](#bib.bib29)]。在这个背景下，我们提出了我们的框架
    MORepair，它将多目标学习应用于在微调过程中教会开源 LLM 程序修复过程的复杂性。这种方法使 LLM 能够为有缺陷的代码生成高质量的补丁。具体来说，LLM
    会通过两个目标进行微调：（1）生成修复后的代码和（2）生成修复后的代码以及说明缺陷及其逻辑的指导。为了优化这些目标，我们为每个目标计算单独的损失，记作 $Loss_{1}$，用于生成修复代码及其解释性指导。这些损失然后使用以下方程式进行组合：
- en: '|  | $Loss=Loss_{1}+\lambda Loss_{2}$ |  | (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $Loss=Loss_{1}+\lambda Loss_{2}$ |  | (1) |'
- en: 'Here, $\lambda$, defined as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\lambda$ 定义为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'For $Loss_{2}$, which assesses the LLM’s capability to generate relevant explanatory
    guidance alongside repaired code, the loss calculation extends to the entire sequence
    of both code and guidance tokens, using a similar cross-entropy function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $Loss_{2}$，它评估 LLM 生成相关解释性指导和修复代码的能力，损失计算扩展到代码和指导令牌的整个序列，使用类似的交叉熵函数：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'Here, $n$. The effectiveness of these adjustments is encapsulated in the following
    formulation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$n$。这些调整的有效性体现在以下公式中：
- en: '|  | $P(y_{i}&#124;\mathbf{x},y_{1},...,y_{i-1})^{\prime}=\sigma((X+\Delta
    X)(W+\Delta W))$ |  | (4) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{i}\mid\mathbf{x},y_{1},...,y_{i-1})^{\prime}=\sigma((X+\Delta X)(W+\Delta
    W))$ |  | (4) |'
- en: Here, $\sigma$ signifies the adjustments made via QLoRA. These adjustments are
    strategically implemented to minimize the specified loss functions, directly linking
    QLoRA’s parameter optimization to the overarching objective of enhancing the LLM’s
    performance in generating both code and explanatory guidance. By leveraging QLoRA,
    we fine-tune a mere 1.84% of pre-trained parameters in CodeLlama-13B-instruct.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\sigma$ 表示通过QLoRA进行的调整。这些调整是战略性地实施的，以最小化指定的损失函数，将QLoRA的参数优化直接与提升LLM在生成代码和解释性指导方面的表现的总体目标联系起来。通过利用QLoRA，我们仅微调了CodeLlama-13B-instruct中预训练参数的1.84%。
- en: 'Additionally, we incorporate NEFTune [[32](#bib.bib32)] to further enhance
    fine-tuned LLMs’ generalization. Noisy Embedding Fine-Tuning (NEFTune), presents
    a novel and effective augmentation technique that aims to prevent over-fitting
    during fine-tuning LLMs. NEFTune introduces random noise to the embedding vectors
    of training data during the forward pass of fine-tuning. Formally, for an embedding
    matrix $X_{emb}\in\mathbb{R}^{B\times L\times d}$ is the embedding dimension,
    NEFTune modifies the embeddings as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还引入了NEFTune [[32](#bib.bib32)]，进一步提升微调LLM的泛化能力。噪声嵌入微调（NEFTune）提出了一种新颖有效的增强技术，旨在防止在微调LLM时的过拟合。NEFTune在微调过程中向训练数据的嵌入向量引入随机噪声。形式上，对于嵌入矩阵
    $X_{emb}\in\mathbb{R}^{B\times L\times d}$，NEFTune对嵌入的修改如下：
- en: '|  | $X^{\prime}_{emb}=X_{emb}+(\frac{\alpha}{\sqrt{Ld}})\epsilon$ |  | (5)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $X^{\prime}_{emb}=X_{emb}+(\frac{\alpha}{\sqrt{Ld}})\epsilon$ |  | (5)
    |'
- en: In this equation, $\epsilon\sim\text{Uniform}(-1,1)$ is a tunable hyper-parameter
    that scales the noise.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，$\epsilon\sim\text{Uniform}(-1,1)$ 是一个可调的超参数，用于调整噪声的尺度。
- en: III-C Repair Inference
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 修复推理
- en: 'In the final step of our approach, we combine quantized LLM with QLoRA adapters
    to generate repaired codes during inference. The buggy code, represented by instruction
    $x$-th token. This vector $\mathbf{x}$ is fed into fine-tuned LLMs equipped with
    quantization and QLoRA adapters to facilitate efficient and precise program repair
    generation. The computation within each linear layer of the quantized LLM is performed
    as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们方法的最后一步，我们结合量化的LLM与QLoRA适配器，以在推理过程中生成修复后的代码。存在问题的代码由指令 $x$-th token 表示。这个向量
    $\mathbf{x}$ 被输入到配备了量化和QLoRA适配器的微调LLM中，以便有效和精确地生成程序修复。在量化LLM的每个线性层内的计算过程如下：
- en: '|  | $Y=X\cdot\text{doubleDequant}(c_{1},c_{2},W)+X\cdot L_{1}\cdot L_{2}$
    |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y=X\cdot\text{doubleDequant}(c_{1},c_{2},W)+X\cdot L_{1}\cdot L_{2}$
    |  | (6) |'
- en: Here, $\text{doubleDequant}(\cdot)$ are the QLoRA adapter matrices.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\text{doubleDequant}(\cdot)$ 是QLoRA适配器矩阵。
- en: Through dequantization, we ensure that computations achieve the necessary precision
    for high-quality output, with each layer’s output feeding into the subsequent
    layer until a final probability distribution over the vocabulary is achieved.
    This distribution, $p(y_{i}|\mathbf{x},y_{1},...,y_{i-1})$. To generate diverse
    and coherent program repairs, we employ a blend of sampling techniques and hyper-parameters,
    setting the temperature and $top_{p}$. This step is crucial because the LLM’s
    output may encompass candidate repaired code and supplementary natural language
    descriptions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过去量化，我们确保计算达到所需的精度，以获得高质量的输出，每层的输出将输入到下一层，直到达到最终的词汇概率分布。这一分布为 $p(y_{i}|\mathbf{x},y_{1},...,y_{i-1})$。为了生成多样且一致的程序修复，我们采用了采样技术和超参数的混合，设置温度和
    $top_{p}$。这一过程至关重要，因为LLM的输出可能包括候选修复代码和补充的自然语言描述。
- en: IV Experimental Setup
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验设置
- en: First, we outline the LLMs utilized in this study. Next, we detail the benchmark
    used for evaluation in our experiments. Following this, we explain the metrics
    employed to evaluate the repair capabilities of the fine-tuned LLMs. Lastly, we
    list the research questions we aim to address through this study.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们概述了本研究中使用的LLM。接下来，我们详细描述了实验中用于评估的基准。随后，我们解释了用于评估微调LLM修复能力的指标。最后，我们列出了通过本研究希望解决的研究问题。
- en: IV-A Model Selection
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 模型选择
- en: To evaluate the generalizability of our approach, it is crucial to experiment
    with LLMs of varying architectures and sizes. Given the significant computational
    resources required for training and deploying large-scale LLMs, as highlighted
    by Chen [[33](#bib.bib33)], we focused on code-targeted LLMs with parameters range
    of 7B to 16B. Table [I](#S4.T1 "TABLE I ‣ IV-A Model Selection ‣ IV Experimental
    Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") presents
    our selected models, chosen based on their popularity (as indicated by downloads
    from HuggingFace) and the diversity of their underlying architectures. These LLMs
    include CodeLlama-13B-instruct [[20](#bib.bib20)], CodeLlama-7B-instruct [[20](#bib.bib20)],
    StarChat-alpha [[21](#bib.bib21)], and Mistral-Instruct-7B [[22](#bib.bib22)],
    allowing us to comprehensively assess our approach’s efficacy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们方法的泛化能力，实验不同架构和规模的LLMs至关重要。鉴于训练和部署大规模LLMs所需的计算资源显著，如陈所强调的[[33](#bib.bib33)]，我们集中在参数范围为7B到16B的代码目标LLMs上。表格[I](#S4.T1
    "TABLE I ‣ IV-A Model Selection ‣ IV Experimental Setup ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs")展示了我们选择的模型，这些模型是根据其受欢迎程度（如从HuggingFace的下载量）和底层架构的多样性来挑选的。这些LLMs包括CodeLlama-13B-instruct
    [[20](#bib.bib20)]、CodeLlama-7B-instruct [[20](#bib.bib20)]、StarChat-alpha [[21](#bib.bib21)]
    和 Mistral-Instruct-7B [[22](#bib.bib22)]，使我们能够全面评估我们方法的有效性。
- en: 'TABLE I: Selected Models'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：选定模型
- en: '| Model | Base Model | # Params | Downloads^* |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 基础模型 | 参数数量 | 下载量^* |'
- en: '| CodeLlama-13B-instruct | CodeLlama | 13B |  46.4k |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-instruct | CodeLlama | 13B |  46.4k |'
- en: '| CodeLlama-7B-instruct | CodeLlama |  7B |  59.5k |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-instruct | CodeLlama |  7B |  59.5k |'
- en: '| StarChat-alpha | StarCoderBase | 16B |  24.9k |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | StarCoderBase | 16B |  24.9k |'
- en: '| Mistral-Instruct-7B-v0.1 | Mistral-7B-v0.1 |  7B | 773.6k |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-Instruct-7B-v0.1 | Mistral-7B-v0.1 |  7B | 773.6k |'
- en: '*'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: “Downloads” count reflects the number of times LLMs were downloaded from HuggingFace
    before Feb. 2024.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “下载量”反映了在2024年2月之前LLMs从HuggingFace上下载的次数。
- en: The selected models showcase a range of innovative features tailored to programming
    tasks. CodeLlama-13B-instruct and CodeLlama-7B-instruct, building on the Llama2
    architecture [[34](#bib.bib34)], offer infilling capabilities and optimized large-batch
    inference, demonstrating the adaptability of the CodeLlama [[20](#bib.bib20)]
    foundation. StarChat-alpha, based on StarCoder [[35](#bib.bib35)], introduces
    advanced pre-training techniques and benefits from expansive code datasets such
    as The Stack [[36](#bib.bib36)], illustrating a novel approach to leveraging data
    diversity for performance gains. Meanwhile, Mistral-Instruct-7B-v0.1, based on
    Mistral [[22](#bib.bib22)], emphasizes advancements in attention mechanisms, highlighting
    the potential for auto-regressive models in processing long sequences efficiently.
    In the following paragraphs, we denote CodeLlama-13B-instruct as CodeLlama-13B,
    CodeLlama-7B-instruct as CodeLlama-7B, and Mistral-Instruct-7B-v0.1 as Mistral-7B.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 选定的模型展示了一系列创新功能，专门针对编程任务。CodeLlama-13B-instruct 和 CodeLlama-7B-instruct 基于Llama2架构
    [[34](#bib.bib34)]，提供填充能力和优化的大批量推理，展示了CodeLlama [[20](#bib.bib20)] 基础的适应性。StarChat-alpha
    基于StarCoder [[35](#bib.bib35)]，引入了先进的预训练技术，并受益于如The Stack [[36](#bib.bib36)] 等广泛的代码数据集，展示了一种利用数据多样性以提高性能的新方法。同时，Mistral-Instruct-7B-v0.1
    基于Mistral [[22](#bib.bib22)]，强调了注意力机制的进步，突显了自回归模型在高效处理长序列中的潜力。在接下来的段落中，我们将CodeLlama-13B-instruct标记为CodeLlama-13B，将CodeLlama-7B-instruct标记为CodeLlama-7B，将Mistral-Instruct-7B-v0.1标记为Mistral-7B。
- en: IV-B Evaluation Benchmark
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 评估基准
- en: To rigorously assess our code generation and translation framework, we leveraged
    HumanEval-X [[26](#bib.bib26)], a multilingual extension of the HumanEval benchmark
    specifically designed for programming languages such as C++, Java, JavaScript,
    Go, and Python. Each of the original 164 Python problems in HumanEval [[23](#bib.bib23)]
    is expanded in HumanEval-X to include equivalent problems in the other four languages,
    culminating in 820 distinct problem-solution pairs. This setup enables comprehensive
    evaluation across code generation and translation tasks in five popular programming
    languages.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了严格评估我们的代码生成和翻译框架，我们利用了HumanEval-X [[26](#bib.bib26)]，这是一个多语言扩展的HumanEval基准，专门为编程语言如C++、Java、JavaScript、Go和Python设计。HumanEval
    [[23](#bib.bib23)]中的164个原始Python问题在HumanEval-X中被扩展，涵盖其他四种语言中的等效问题，共有820个不同的问题-解决对。这种设置使我们能够在五种流行编程语言中全面评估代码生成和翻译任务。
- en: To establish a robust C++ program repair benchmark, we adapted the methodology
    from Jiang *et al.* [[4](#bib.bib4)] used for HumanEval-Java, injecting various
    types of bugs into the C++ section of HumanEval-X to form the EvalRepair-C++ benchmark.
    This benchmark encompasses 115 single-line bugs, 143 single-hunk bugs, 21 multi-hunk
    bugs, and 2 multi-function bugs, offering a broad spectrum of defect scenarios
    for evaluation. Recognizing the potential for overfitting due to a limited number
    of test cases, we enriched EvalRepair-C++ with additional test cases from the
    EvalPlus [[37](#bib.bib37)]. This expansion revealed that some original solutions
    in HumanEval-X’s C++ component failed to pass the new, more rigorous test cases.
    We corrected these issues, ensuring the augmented benchmark’s correctness. Consequently,
    the average number of test cases per problem in EvalRepair-C++ surged to 590,
    a significant increase to provide a more accurate assessment of model performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立一个稳健的 C++ 程序修复基准，我们借鉴了 Jiang *et al.* [[4](#bib.bib4)] 采用的 HumanEval-Java
    方法论，将各种类型的错误注入到 HumanEval-X 的 C++ 部分，形成了 EvalRepair-C++ 基准。该基准涵盖了 115 个单行错误、143
    个单块错误、21 个多块错误和 2 个多函数错误，为评估提供了广泛的缺陷场景。由于测试用例数量有限可能导致过拟合，我们通过从 EvalPlus [[37](#bib.bib37)]
    中增加额外的测试用例来丰富 EvalRepair-C++。这一扩展揭示了 HumanEval-X 的 C++ 组件中的一些原始解决方案未能通过新的、更严格的测试用例。我们纠正了这些问题，确保了增强基准的正确性。因此，EvalRepair-C++
    中每个问题的平均测试用例数量跃升至 590，这一显著增加提供了更准确的模型性能评估。
- en: HumanEval-Java serves as a critical benchmark for Java program repair, distinct
    in its exclusion from the pre-training datasets of existing LLMs to avoid data
    leakage [[4](#bib.bib4)]. By introducing EvalRepair-Java, which expands HumanEval-Java
    with additional EvalPlus [[37](#bib.bib37)] test cases, the average number of
    test cases per problem has been increased to 583\. This significant augmentation
    of test cases actively mitigates patch overfitting issues.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: HumanEval-Java 是 Java 程序修复的关键基准，其区别在于排除了现有 LLMs 的预训练数据集，以避免数据泄漏 [[4](#bib.bib4)]。通过引入扩展
    HumanEval-Java 的 EvalRepair-Java 和额外的 EvalPlus [[37](#bib.bib37)] 测试用例，平均每个问题的测试用例数量增加到
    583。这一显著的测试用例增加有效缓解了补丁过拟合问题。
- en: 'TABLE II: Mitigation of Patch Overfitting'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：缓解补丁过拟合
- en: '|  | EvalRepair-C++ | EvalRepair-Java |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | EvalRepair-C++ | EvalRepair-Java |'
- en: '| ① # Original Test Cases | 7 | 7 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ① # 原始测试用例 | 7 | 7 |'
- en: '| ② # Augmented Test Cases | 590 | 583 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ② # 增强的测试用例 | 590 | 583 |'
- en: '| CodeLlama-13B TOP-10 with ① | 67.7 | 73.6 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B TOP-10 with ① | 67.7 | 73.6 |'
- en: '| CodeLlama-13B TOP-10 with ② | 58.5 | 69.9 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B TOP-10 with ② | 58.5 | 69.9 |'
- en: As illustrated in Table [II](#S4.T2 "TABLE II ‣ IV-B Evaluation Benchmark ‣
    IV Experimental Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs"), augmenting the test cases leads to a noticeable decline in the TOP-10
    of LLMs such as CodeLlama-13B, which experienced a reduction of 9.2% in EvalRepair-C++
    and 3.7% in EvalRepair-Java. The introduction of a more comprehensive set of test
    cases not only highlights the importance of rigorous evaluation in the development
    of LLMs but also sets a new standard for assessing their performance in program
    repair tasks. These benchmarks, EvalRepair-C++ and EvalRepair-Java, will be made
    publicly accessible via an API , ensuring that the research community can benefit
    from these resources for future explorations and improvements in the field without
    data leakage problem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [II](#S4.T2 "TABLE II ‣ IV-B Evaluation Benchmark ‣ IV Experimental Setup
    ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") 所示，增加测试用例会导致
    LLMs 如 CodeLlama-13B 的 TOP-10 显著下降，在 EvalRepair-C++ 中减少了 9.2%，在 EvalRepair-Java
    中减少了 3.7%。引入更全面的测试用例集不仅突显了严格评估在 LLMs 开发中的重要性，还为评估其在程序修复任务中的性能设立了新的标准。这些基准，EvalRepair-C++
    和 EvalRepair-Java，将通过 API 公共开放，确保研究社区可以利用这些资源进行未来的探索和改进，而不会遇到数据泄漏问题。
- en: IV-C Evaluation Metrics
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 评估指标
- en: 'To accurately evaluate the effectiveness of LLMs in program repair, this study
    employs two primary metrics: TOP-5 and TOP-10\. The “TOP-k” metric is the scenario
    where, among the top k candidate patches produced by the LLMs, the code is considered
    successfully repaired if any candidates pass all test cases in the benchmark.
    This metrics selection is grounded in the observation by Kochhar *et al.* [[38](#bib.bib38)]
    that most developers tend to abandon automated debugging tools if they fail to
    identify the actual bugs within the first five attempts. Furthermore, Noller *et al.* [[13](#bib.bib13)]
    found that developers are unlikely to consider more than the top-10 ranked patches
    when seeking solutions. Reflecting on these insights and aligning with the findings
    from prior program repair studies [[39](#bib.bib39), [12](#bib.bib12), [13](#bib.bib13),
    [40](#bib.bib40)], our selection of the TOP-5 and TOP-10 metrics is not only justified
    but also crucial for ensuring our evaluation mirrors real-world developers scenarios
    and expectations.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为准确评估 LLM 在程序修复中的有效性，本研究使用两个主要指标：TOP-5 和 TOP-10。TOP-k 指标是指在 LLM 提供的前 k 个候选修复中，如果任何候选项通过基准测试中的所有用例，则代码被视为成功修复。该指标的选择基于
    Kochhar *et al.* [[38](#bib.bib38)] 的观察，即大多数开发者如果在前五次尝试中未能识别实际错误，往往会放弃自动调试工具。此外，Noller
    *et al.* [[13](#bib.bib13)] 发现开发者在寻找解决方案时不太可能考虑超过前 10 名的修复。根据这些见解并结合之前程序修复研究的发现
    [[39](#bib.bib39), [12](#bib.bib12), [13](#bib.bib13), [40](#bib.bib40)]，我们选择
    TOP-5 和 TOP-10 指标不仅有其合理性，而且对确保评估反映真实开发者场景和期望至关重要。
- en: IV-D Research Questions
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 研究问题
- en: 'RQ-1: How effective is fine-tuning with two objectives for program repair?
    We investigate the performance of MORepair’s multi-objective fine-tuning in contrast
    to standard, single-objective fine-tuning on the CodeLlama-13B. This comparative
    analysis is conducted using the EvalRepair-C++ and EvalRepair-Java benchmarks
    to assess not only the effectiveness of MORepair in improving program repair but
    also its ability to generalize across different programming languages.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ-1: 使用两个目标的微调在程序修复中的效果如何？我们调查了 MORepair 在 CodeLlama-13B 上的多目标微调与标准单目标微调的表现。通过使用
    EvalRepair-C++ 和 EvalRepair-Java 基准测试进行比较分析，评估 MORepair 在改善程序修复方面的有效性，以及其在不同编程语言中的泛化能力。'
- en: 'RQ-2: How does model size or type impact repair performance of MORepair? We
    examine MORepair’s performance on LLMs with distinct sizes and architectures,
    including CodeLlama-13B, CodeLlama-7B, StarChat-alpha-16B, and Mistral-7B, on
    EvalRepair-C++ and EvalRepair-Java benchmarks. This study aims to validate MORepair’s
    generalization capability by comparing its fine-tuning effects against standard
    approaches and baseline performance across varying LLMs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ-2: 模型大小或类型如何影响 MORepair 的修复性能？我们在 EvalRepair-C++ 和 EvalRepair-Java 基准测试上检查了
    MORepair 在不同规模和架构的 LLM（包括 CodeLlama-13B、CodeLlama-7B、StarChat-alpha-16B 和 Mistral-7B）上的表现。该研究旨在通过比较
    MORepair 的微调效果与标准方法及基准性能，验证 MORepair 的泛化能力。'
- en: 'RQ-3: How does MORepair compare against MORepair with human guidance and state-of-the-art
    fine-tuning methods? Through an ablation study, we explore the influence of the
    source of guidance (LLM-generated vs. human-generated) on MORepair’s effectiveness.
    Additionally, we compare MORepair ’s performance to two advanced fine-tuning methodologies,
    Fine-tune-CoT [[25](#bib.bib25)] and RepairLLaMA [[17](#bib.bib17)], across various
    LLMs.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ-3: MORepair 与 MORepair 结合人工指导和最先进的微调方法相比如何？通过消融研究，我们探索了指导来源（LLM 生成 vs. 人工生成）对
    MORepair 有效性的影响。此外，我们还将 MORepair 的表现与两种先进的微调方法 Fine-tune-CoT [[25](#bib.bib25)]
    和 RepairLLaMA [[17](#bib.bib17)] 在各种 LLM 上进行比较。'
- en: V Experiments & Results
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验与结果
- en: V-A Effectiveness of Multi-objective Fine-tuning for Program Repair
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 多目标微调在程序修复中的有效性
- en: '[Objective:] This study assesses MORepair’s impact on fine-tuning LLMs for
    program repair, comparing its multi-objective approach against standard single-objective
    fine-tuning and baseline LLMs without fine-tuning. Our investigation centers around
    two sub-questions:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[目标:] 本研究评估 MORepair 对 LLM 微调程序修复的影响，将其多目标方法与标准单目标微调和未经过微调的基准 LLM 进行比较。我们的研究集中在两个子问题上：'
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ-1.1 How does fine-tuning LLMs with MORepair compare to both standard fine-tuning
    and the baseline LLM in terms of repair performance?
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ-1.1 MORepair 微调与标准微调和基准 LLM 在修复性能上的比较如何？
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ-1.2 Does MORepair exhibit cross-language generalization in program repair
    tasks compared to standard fine-tuning and baseline LLM?
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ-1.2 MORepair 相较于标准微调和基线 LLM，在程序修复任务中是否展现了跨语言泛化能力？
- en: '[Experimental Design for RQ-1.1]: We fine-tune CodeLlama-13B using both MORepair
    and the standard fine-tuning approach. Here, the baseline represents CodeLlama-13B
    without any fine-tuning, serving as our control for evaluating the impact of fine-tuning.
    Standard fine-tuning refers to fine-tuning CodeLlama-13B to generate repaired
    code without other information, denoted as StdFT. In contrast, MORepair involves
    multi-objective fine-tuning, aiming to enhance LLM’s repair capabilities through
    additional natural language guidance. The comparative analysis is based on TOP-5
    and TOP-10 metrics on the benchmark EvalRepair-C++, detailed in Section [IV-B](#S4.SS2
    "IV-B Evaluation Benchmark ‣ IV Experimental Setup ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-1.1 的实验设计](https://example.org/experimental_design_rq_1_1): 我们使用 MORepair
    和标准微调方法对 CodeLlama-13B 进行了微调。在这里，基线表示未经任何微调的 CodeLlama-13B，作为我们评估微调影响的对照。标准微调指的是对
    CodeLlama-13B 进行微调以生成修复代码，而不使用其他信息，标记为 StdFT。相比之下，MORepair 涉及多目标微调，旨在通过额外的自然语言指导来提升
    LLM 的修复能力。比较分析基于基准 EvalRepair-C++ 上的 TOP-5 和 TOP-10 指标，详见第 [IV-B](#S4.SS2 "IV-B
    评估基准 ‣ IV 实验设置 ‣ 使用 LLM 的多目标微调以增强程序修复") 节。'
- en: '[Experimental Results for RQ-1.1]: Table [III](#S5.T3 "TABLE III ‣ V-A Effectiveness
    of Multi-objective Fine-tuning for Program Repair ‣ V Experiments & Results ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") shows MORepair’s
    significant repair performance enhancement on EvalRepair-C++ over both the baseline,
    and StdFT. Against the baseline, MORepair elevates TOP-5 by 20.7%, and TOP-10
    by 11.0%. Compared to StdFT, MORepair maintains its superiority with increments
    of 12.2% in TOP-5, and 5.5% in TOP-10\. These substantial improvements, particularly
    in TOP-5 and TOP-10, more than double the gains of StdFT, showcasing MORepair’s
    profound impact. The success of the MORepair approach underscores the advantage
    of multi-objective fine-tuning in fostering a more nuanced understanding and application
    of repair logic than what is achieved through standard fine-tuning.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-1.1 的实验结果](https://example.org/experimental_results_rq_1_1): 表 [III](#S5.T3
    "表 III ‣ V-A 多目标微调在程序修复中的有效性 ‣ V 实验与结果 ‣ 使用 LLM 的多目标微调以增强程序修复") 显示 MORepair 在
    EvalRepair-C++ 上的修复性能显著优于基线和 StdFT。相较于基线，MORepair 将 TOP-5 提高了 20.7%，将 TOP-10 提高了
    11.0%。与 StdFT 相比，MORepair 在 TOP-5 和 TOP-10 上分别提高了 12.2% 和 5.5%。这些显著的改进，特别是在 TOP-5
    和 TOP-10 上，超过了 StdFT 的两倍，展示了 MORepair 的深远影响。MORepair 方法的成功突显了多目标微调在促进对修复逻辑的更细致理解和应用方面相较于标准微调的优势。'
- en: '[Experimental Design for RQ-1.2]: To probe MORepair’s and StdFT’s capacity
    for cross-language generalization in program repair, we fine-tuned CodeLlama-13B
    with each method using the TutorLLMCode. Their performances were then evaluated
    on the Java repair benchmark EvalRepair-Java across TOP-5 and TOP-10 metrics,
    offering insights into how these approaches adapt to a language different from
    the training dataset.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-1.2 的实验设计](https://example.org/experimental_design_rq_1_2): 为了探讨 MORepair
    和 StdFT 在程序修复中的跨语言泛化能力，我们使用 TutorLLMCode 对 CodeLlama-13B 进行了每种方法的微调。然后，在 Java
    修复基准 EvalRepair-Java 上评估了它们的性能，使用 TOP-5 和 TOP-10 指标，提供了这些方法如何适应与训练数据集不同的语言的见解。'
- en: '[Experimental Results for RQ-1.2]: The repair performance presented in Table
    [IV](#S5.T4 "TABLE IV ‣ V-A Effectiveness of Multi-objective Fine-tuning for Program
    Repair ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program
    Repair with LLMs") for the EvalRepair-Java benchmark detail how both StdFT and
    MORepair extend their capabilities into a cross-language scenario. StdFT enhances
    the TOP-10 by 6.8% over the baseline (CodeLlama-13B), while MORepair further improves
    upon this, exhibiting an additional 1.2% increase in TOP-10 over StdFT. These
    enhancements validate the cross-language generalization capability of both fine-tuning
    approaches, with MORepair showcasing superior performance in adapting to Java,
    which is a shift from the training dataset’s programming language. Notably, MORepair
    achieves a 77.9% TOP-10, marking an 8.0% increase over the baseline. This significant
    improvement underscores MORepair’s effectiveness in cross-language repair scenarios.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-1.2 的实验结果](https://example.org): 表 [IV](#S5.T4 "TABLE IV ‣ V-A Effectiveness
    of Multi-objective Fine-tuning for Program Repair ‣ V Experiments & Results ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") 中展示的修复性能细节，说明了
    StdFT 和 MORepair 如何将它们的能力扩展到跨语言场景中。StdFT 将 TOP-10 提升了 6.8%，超过了基线（CodeLlama-13B），而
    MORepair 进一步改进，在 TOP-10 上比 StdFT 多出 1.2%。这些改进验证了这两种微调方法的跨语言泛化能力，其中 MORepair 在适应
    Java 编程语言时表现出更优的性能，这与训练数据集的编程语言有所不同。值得注意的是，MORepair 在 TOP-10 上达到了 77.9%，比基线提高了
    8.0%。这一显著提升突显了 MORepair 在跨语言修复场景中的有效性。'
- en: 'TABLE III: Fine-tune CodeLlama-13B with StdFT and MORepair vs GPT-4 on EvalRepair-C++.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 用 StdFT 和 MORepair 对 CodeLlama-13B 进行微调，并与 GPT-4 在 EvalRepair-C++ 上的表现对比。'
- en: '| Model | TOP-5 | TOP-10 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TOP-5 | TOP-10 |'
- en: '| GPT-4 | 97.6 | 98.2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 97.6 | 98.2 |'
- en: '| CodeLlama-13B | 40.9 | 58.5 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 40.9 | 58.5 |'
- en: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
- en: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
- en: 'TABLE IV: Fine-tune CodeLlama-13B with StdFT and MORepair vs GPT-4 on EvalRepair-Java.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 用 StdFT 和 MORepair 对 CodeLlama-13B 进行微调，并与 GPT-4 在 EvalRepair-Java 上的表现对比。'
- en: '| Model | TOP-5 | TOP-10 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TOP-5 | TOP-10 |'
- en: '| GPT-4 | 85.9 | 89.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 85.9 | 89.0 |'
- en: '| CodeLlama-13B | 54.0 | 69.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 54.0 | 69.9 |'
- en: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
- en: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
- en: Reflecting on the examples illustrated in Section [II](#S2 "II Motivating Example
    ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs"), the evaluation
    results of buggy codes “separate_paren_groups.cpp” in EvalRepair-C++ and “SEPARATE_PAREN_GROUPS.java”
    in EvalRepair-Java demonstrates the distinct effectiveness of MORepair and StdFT.
    MORepair successfully repairs these buggy codes within Top-10 attempts, while
    StdFT fails to accomplish these repairs. This result supports that cross-language
    generalization enhancement of MORepair is attributed to natural language guidance
    from GPT-4, enabling MORepair to learn language-independent repair logic.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 反映在 [II](#S2 "II Motivating Example ‣ Multi-Objective Fine-Tuning for Enhanced
    Program Repair with LLMs") 节中说明的示例，EvalRepair-C++ 中的“separate_paren_groups.cpp”和
    EvalRepair-Java 中的“SEPARATE_PAREN_GROUPS.java” 的有缺陷代码的评估结果展示了 MORepair 和 StdFT
    的明显效果。MORepair 成功地在 TOP-10 尝试中修复了这些有缺陷的代码，而 StdFT 未能完成这些修复。这个结果支持了 MORepair 跨语言泛化能力的提升归因于
    GPT-4 的自然语言指导，使得 MORepair 能够学习语言无关的修复逻辑。
- en: Furthermore, we include GPT-4, state-of-the-art closed-source LLM, as a benchmark
    for upper limits of repair performance, as illustrated in Tables [III](#S5.T3
    "TABLE III ‣ V-A Effectiveness of Multi-objective Fine-tuning for Program Repair
    ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs") and [IV](#S5.T4 "TABLE IV ‣ V-A Effectiveness of Multi-objective Fine-tuning
    for Program Repair ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs"). The result shows that MORepair narrows the
    performance gap between CodeLlama-13B and GPT-4.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将最先进的闭源 LLM GPT-4 作为修复性能的上限基准，如表 [III](#S5.T3 "TABLE III ‣ V-A Effectiveness
    of Multi-objective Fine-tuning for Program Repair ‣ V Experiments & Results ‣
    Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") 和 [IV](#S5.T4
    "TABLE IV ‣ V-A Effectiveness of Multi-objective Fine-tuning for Program Repair
    ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs") 所示。结果表明，MORepair 缩小了 CodeLlama-13B 和 GPT-4 之间的性能差距。
- en: '[RQ-1]
    Findings: (1) Fine-tuning with MORepair outperforms CodeLlama-13B baseline significantly
    in repair performance. The improvements in TOP-10 for EvalRepair-C++ and EvalRepair-Java
    are 11.0% and 8.0%, respectively, showcasing superior repair capabilities. (2)
    Against StdFT, MORepair shows repair performance gains with increases in TOP-5
    of 12.2% for EvalRepair-C++ and 7.3% for EvalRepair-Java, indicating generalization
    across programming languages. Insights: Our approach MORepair highlights multi-objective
    learning’s impact on automated program repair, proving its ability to enhance
    repair tasks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-1]
    Findings: (1) Fine-tuning with MORepair outperforms CodeLlama-13B baseline significantly
    in repair performance. The improvements in TOP-10 for EvalRepair-C++ and EvalRepair-Java
    are 11.0% and 8.0%, respectively, showcasing superior repair capabilities. (2)
    Against StdFT, MORepair shows repair performance gains with increases in TOP-5
    of 12.2% for EvalRepair-C++ and 7.3% for EvalRepair-Java, indicating generalization
    across programming languages. Insights: Our approach MORepair highlights multi-objective
    learning’s impact on automated program repair, proving its ability to enhance
    repair tasks.'
- en: V-B Impact of Size or Type for Fine-tuning LLMs on Code Repair Performance
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 微调 LLM 对代码修复性能的大小或类型影响
- en: '[Objective]: To investigate RQ-2, we assess the impact of fine-tuning with
    MORepair on LLMs of varying sizes and architectures in terms of their code repair
    capabilities.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[目标]：为了调查 RQ-2，我们评估了 MORepair 对不同大小和架构 LLM 的微调在代码修复能力上的影响。'
- en: '[Experimental Design]: To examine the generalization of the MORepair approach
    across LLMs with different sizes and architectures, we selected CodeLlama-7B,
    StarChat-alpha (which has 16B parameters), and Mistral-7B as our base LLMs. These
    LLMs represent a diverse range of architectures, and CodeLlama-7B differs in size
    from the CodeLlama-13B assessed in RQ-1\. We fine-tune these LLMs using either
    standard fine-tuning (StdFT) or MORepair, then evaluate their performance on two
    benchmarks: EvalRepair-C++ and EvalRepair-Java.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[实验设计]：为了检验 MORepair 方法在不同大小和架构的 LLM 上的泛化能力，我们选择了 CodeLlama-7B、StarChat-alpha（具有
    16B 参数）和 Mistral-7B 作为基础 LLM。这些 LLM 代表了多种架构，且 CodeLlama-7B 在大小上与 RQ-1 中评估的 CodeLlama-13B
    不同。我们使用标准微调（StdFT）或 MORepair 对这些 LLM 进行微调，然后在两个基准测试：EvalRepair-C++ 和 EvalRepair-Java
    上评估其性能。'
- en: 'TABLE V: Impact of model sizes or architectures in the effectiveness of fine-tuning
    on EvalRepair-C++.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：模型大小或架构对 EvalRepair-C++ 上微调效果的影响。
- en: '| Model | TOP-5 | TOP-10 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | 40.9 | 58.5 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 40.9 | 58.5 |'
- en: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-StdFT | 49.4 (+ 8.5) | 64.0 (+ 5.5) |'
- en: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
- en: '| CodeLlama-7B | 46.3 | 59.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | 46.3 | 59.1 |'
- en: '| CodeLlama-7B-StdFT | 50.0 (+ 3.7) | 61.6 (+ 2.5) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-StdFT | 50.0 (+ 3.7) | 61.6 (+ 2.5) |'
- en: '| CodeLlama-7B-MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
- en: '| StarChat-alpha | 50.0 | 62.2 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | 50.0 | 62.2 |'
- en: '| StarChat-StdFT | 43.3 (- 6.7) | 58.5 (- 3.7) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-StdFT | 43.3 (- 6.7) | 58.5 (- 3.7) |'
- en: '| StarChat-MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
- en: '| Mistral-7B | 32.3 | 47.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 32.3 | 47.0 |'
- en: '| Mistral-7B-StdFT | 39.0 (+ 6.7) | 46.3 (- 0.7) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-StdFT | 39.0 (+ 6.7) | 46.3 (- 0.7) |'
- en: '| Mistral-7B-MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
- en: †
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的值表示相对于相应基准的变化。
- en: '[Experimental Results]: Table [V](#S5.T5 "TABLE V ‣ V-B Impact of Size or Type
    for Fine-tuning LLMs on Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") outlines the TOP-5 and TOP-10
    repair performance metrics for baseline, StdFT, and MORepair across four LLMs
    on EvalRepair-C++. Notably, StdFT doesn’t consistently improve repair metrics,
    failing to surpass the repair performance of baseline on several base LLMs, such
    as StarChat-alpha. Conversely, MORepair consistently enhances performance across
    all metrics and LLMs, with a maximum 11.0% TOP-10 improvement over baseline and
    a maximum 7.4% TOP-10 over StdFT evaluated on EvalRepair-C++. This suggests superior
    generalizability of multi-objective learning across different LLMs for code repair.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[实验结果]：表 [V](#S5.T5 "TABLE V ‣ V-B Impact of Size or Type for Fine-tuning LLMs
    on Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") 概述了基准、StdFT 和 MORepair 在 EvalRepair-C++
    上的四种 LLM 中的 TOP-5 和 TOP-10 修复性能指标。值得注意的是，StdFT 并未始终改善修复指标，未能超越一些基础 LLM（如 StarChat-alpha）的基准修复性能。相反，MORepair
    在所有指标和 LLM 中都持续提高性能，相较于基准，在 EvalRepair-C++ 上最大提高了 11.0% 的 TOP-10，相较于 StdFT 最大提高了
    7.4% 的 TOP-10。这表明，多目标学习在不同 LLM 上的代码修复具有更好的泛化能力。'
- en: 'TABLE VI: Impact of model sizes or architectures on the effectiveness of fine-tuning
    on EvalRepair-Java.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：模型大小或架构对 EvalRepair-Java 上微调效果的影响。
- en: '| Model | TOP-5 | TOP-10 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | 54.0 | 69.9 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 54.0 | 69.9 |'
- en: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-StdFT | 62.0 (+ 8.0) | 76.7 (+ 6.8) |'
- en: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B-MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
- en: '| CodeLlama-7B | 49.7 | 62.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | 49.7 | 62.0 |'
- en: '| CodeLlama-7B-StdFT | 49.1 (- 0.6) | 60.7 (- 1.3) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-StdFT | 49.1 (- 0.6) | 60.7 (- 1.3) |'
- en: '| CodeLlama-7B-MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
- en: '| StarChat-alpha | 43.6 | 60.7 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | 43.6 | 60.7 |'
- en: '| StarChat-StdFT | 47.9 (+ 4.3) | 56.4 (- 4.3) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-StdFT | 47.9 (+ 4.3) | 56.4 (- 4.3) |'
- en: '| StarChat-MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
- en: '| Mistral-7B | 33.7 | 52.1 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 33.7 | 52.1 |'
- en: '| Mistral-7B-StdFT | 42.3 (+ 8.6) | 54.6 (+ 2.5) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-StdFT | 42.3 (+ 8.6) | 54.6 (+ 2.5) |'
- en: '| Mistral-7B-MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
- en: †
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的值表示相对于相应基线的变化。
- en: Table [VI](#S5.T6 "TABLE VI ‣ V-B Impact of Size or Type for Fine-tuning LLMs
    on Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") presents the TOP-5 and TOP-10 metrics
    for baseline, StdFT, and MORepair on the EvalRepair-Java benchmark across four
    LLMs. Unlike the results from EvalRepair-C++ in Table [V](#S5.T5 "TABLE V ‣ V-B
    Impact of Size or Type for Fine-tuning LLMs on Code Repair Performance ‣ V Experiments
    & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs"),
    CodeLlama-7B-StdFT under-performs on EvalRepair-Java, revealing StdFT’s inconsistent
    cross-language generalization. Similarly, StarChat-StdFT’s decline mirrors its
    performance on EvalRepair-C++, indicating StdFT’s limited adaptability across
    LLMs of different architectures. Conversely, MORepair demonstrates robust improvements
    over baseline and StdFT, with an increment of 5.5%-8.0% TOP-10 over baseline and
    1.2%-9.9% TOP-10 over StdFT evaluated on EvalRepair-Java. Despite StdFT showcasing
    a decrease in repair performance compared to the baseline of four LLMs, MORepair
    consistently improves over baseline in cross-language scenarios. This underscores
    the effectiveness of MORepair leveraging multi-objective learning and LLM-generated
    natural language guidance in enhancing repair capabilities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [VI](#S5.T6 "TABLE VI ‣ V-B Impact of Size or Type for Fine-tuning LLMs on
    Code Repair Performance ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") 展示了在四个LLM上的EvalRepair-Java基准测试中，基线、StdFT和MORepair的TOP-5和TOP-10指标。与表
    [V](#S5.T5 "TABLE V ‣ V-B Impact of Size or Type for Fine-tuning LLMs on Code
    Repair Performance ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs")中EvalRepair-C++的结果不同，CodeLlama-7B-StdFT在EvalRepair-Java上表现不佳，揭示了StdFT在跨语言泛化中的不一致性。同样，StarChat-StdFT的下降与其在EvalRepair-C++上的表现一致，表明StdFT在不同架构的LLM之间适应性有限。相反，MORepair在基线和StdFT之上展示了强劲的改进，在EvalRepair-Java上的TOP-10指标比基线提高了5.5%-8.0%，比StdFT提高了1.2%-9.9%。尽管StdFT在四个LLM的修复性能中相比基线有所下降，MORepair在跨语言场景中的表现却持续优于基线。这突显了MORepair利用多目标学习和LLM生成自然语言指导在增强修复能力方面的有效性。
- en: '[RQ-2]
    Findings: MORepair consistently elevates repair performance across LLMs with varied
    sizes and architectures. Notably, it achieves a maximum 11.0% improvement in TOP-10
    scores over the baseline and a maximum 7.4% improvement over StdFT on EvalRepair-C++.
    On EvalRepair-Java, MORepair showcases 8.0% TOP-10 improvement over the baseline
    and 9.9% TOP-10 enhancement over StdFT, further highlighting its superior generalization.
    Insights: These findings underscore the versatility of LLMs in understanding and
    applying language-independent programming logic through strategies such as LLM-generated
    guidance and multi-objective learning, paving the way for advancements in program
    repair.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-2]
    Findings: MORepair consistently elevates repair performance across LLMs with varied
    sizes and architectures. Notably, it achieves a maximum 11.0% improvement in TOP-10
    scores over the baseline and a maximum 7.4% improvement over StdFT on EvalRepair-C++.
    On EvalRepair-Java, MORepair showcases 8.0% TOP-10 improvement over the baseline
    and 9.9% TOP-10 enhancement over StdFT, further highlighting its superior generalization.
    Insights: These findings underscore the versatility of LLMs in understanding and
    applying language-independent programming logic through strategies such as LLM-generated
    guidance and multi-objective learning, paving the way for advancements in program
    repair.'
- en: V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against
    State-of-the-Art Fine-tuning Methods
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 评估指导来源的影响并将MORepair与最先进的微调方法进行比较
- en: '[Objective]: This section is dedicated to examining the influence of source
    of guidance on MORepair’s repair capabilities and assessing MORepair’s comparative
    performance against advanced fine-tuning techniques. Specifically, we address
    the following sub-questions:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[目标]: 本节专注于考察指导来源对MORepair修复能力的影响，并评估MORepair与先进微调技术的比较性能。具体而言，我们解决以下子问题：'
- en: 'RQ-3.1: How does the code repair performance of MORepair differ when fine-tuned
    with LLM-generated guidance compared to human-generated guidance?'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ-3.1: 与人类生成的指导相比，MORepair在使用LLM生成的指导进行微调时的代码修复性能有何不同？'
- en: 'RQ-3.2: How does the performance improvement of fine-tuning with MORepair against
    that achieved with existing methodologies, such as Fine-tune-CoT and RepairLLaMA?'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ-3.2: MORepair的微调性能改进与现有方法（如Fine-tune-CoT和RepairLLaMA）相比如何？'
- en: '[Experimental Design for RQ-3.1]: To evaluate the impact of the source of guidance
    on MORepair’s code repair capabilities, we expanded our training dataset TutorLLMCode
    with human-generated instructions for each pair of buggy and corrected code, as
    illustrated in Listing [1](#LST1 "Listing 1 ‣ II Motivating Example ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs"). Human-generated guidance
    provides explicit repair strategies, contrasting with the LLM-generated advice,
    and serves as a new training dataset for MORepair. We then evaluate their code
    repair performance employing the EvalRepair-C++ and EvalRepair-Java benchmarks.
    Finally, we compare the LLMs fine-tuned with human-generated guidance against
    those fine-tuned with LLM-generated guidance. This comparison aims to identify
    which source of guidance (human-generated versus LLM-generated) more effectively
    enhances the fine-tuning process and results in superior code repair performance.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-3.1 的实验设计]: 为了评估指导来源对 MORepair 代码修复能力的影响，我们扩展了训练数据集 TutorLLMCode，增加了每对有缺陷和修正代码的人工生成的指令，如
    Listing [1](#LST1 "Listing 1 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") 所示。人工生成的指导提供了明确的修复策略，与 LLM 生成的建议形成对比，并作为
    MORepair 的新训练数据集。然后，我们使用 EvalRepair-C++ 和 EvalRepair-Java 基准测试来评估它们的代码修复性能。最后，我们将使用人工生成指导进行微调的
    LLM 与使用 LLM 生成指导的 LLM 进行比较。此比较旨在确定哪种指导来源（人工生成与 LLM 生成）更有效地提升微调过程，从而带来更优的代码修复性能。'
- en: 'TABLE VII: Impact of different source of guidance in the effectiveness of MORepair
    on EvalRepair-C++.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VII: 不同指导来源对 MORepair 在 EvalRepair-C++ 上效果的影响。'
- en: '| Model | Guidance | TOP-5 | TOP-10 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 指导 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | Human | 52.4 (+11.5) | 66.5 (+ 8.0) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 人工 | 52.4 (+11.5) | 66.5 (+ 8.0) |'
- en: '| LLM | 61.6 (+20.7) | 69.5 (+11.0) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 61.6 (+20.7) | 69.5 (+11.0) |'
- en: '| CodeLlama-7B | Human | 40.9 (- 5.4) | 54.9 (- 4.2) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | 人工 | 40.9 (- 5.4) | 54.9 (- 4.2) |'
- en: '| LLM | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 56.7 (+10.4) | 62.8 (+ 3.7) |'
- en: '| StarChat-alpha | Human | 48.2 (- 1.8) | 59.8 (- 2.4) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | 人工 | 48.2 (- 1.8) | 59.8 (- 2.4) |'
- en: '| LLM | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 52.4 (+ 2.4) | 65.9 (+ 3.7) |'
- en: '| Mistral-7B | Human | 35.4 (+ 3.1) | 45.7 (- 1.3) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 人工 | 35.4 (+ 3.1) | 45.7 (- 1.3) |'
- en: '| LLM | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
- en: †
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的数值表示相对于对应基准的变化。
- en: 'TABLE VIII: Impact of different source of guidance in the effectiveness of
    MORepair on EvalRepair-Java.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VIII: 不同指导来源对 MORepair 在 EvalRepair-Java 上效果的影响。'
- en: '| Model | Guidance | TOP-5 | TOP-10 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 指导 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | Human | 63.2 (+ 9.2) | 76.1 (+ 6.2) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | 人工 | 63.2 (+ 9.2) | 76.1 (+ 6.2) |'
- en: '| LLM | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 69.3 (+15.3) | 77.9 (+ 8.0) |'
- en: '| CodeLlama-7B | Human | 51.5 (+ 1.8) | 62.0 (+ 0.0) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | 人工 | 51.5 (+ 1.8) | 62.0 (+ 0.0) |'
- en: '| LLM | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
- en: '| StarChat-alpha | Human | 51.5 (+ 7.9) | 63.2 (+ 2.5) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | 人工 | 51.5 (+ 7.9) | 63.2 (+ 2.5) |'
- en: '| LLM | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 56.4 (+12.8) | 66.3 (+ 5.6) |'
- en: '| Mistral-7B | Human | 44.2 (+10.5) | 53.4 (+ 1.3) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 人工 | 44.2 (+10.5) | 53.4 (+ 1.3) |'
- en: '| LLM | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 45.4 (+11.7) | 58.3 (+ 6.2) |'
- en: †
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的数值表示相对于对应基准的变化。
- en: '[Experimental Results for RQ-3.1]: The impact of different sources of guidance
    on the code repair capabilities of MORepair is quantitatively analyzed in this
    experiment, and results are presented in Tables [VII](#S5.T7 "TABLE VII ‣ V-C
    Evaluating the Impact of Guidance Sources and Comparing MORepair against State-of-the-Art
    Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs") for EvalRepair-C++, and [VIII](#S5.T8 "TABLE
    VIII ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against
    State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") for EvalRepair-Java. These
    tables illustrate that LLM-generated guidance significantly surpasses human-generated
    guidance in enhancing code repair performance. Employing LLM-generated guidance
    resulted in TOP-10 improvements over their human-generated counterparts of 3.0%
    to 7.9% for EvalRepair-C++ and 1.2% to 5.5% for EvalRepair-Java. Furthermore,
    Listing [2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning
    for Enhanced Program Repair with LLMs") and [1](#LST1 "Listing 1 ‣ II Motivating
    Example ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs")
    provide illustrative examples of the guidance produced by LLMs and humans, respectively.
    These examples demonstrate how LLM-generated guidance tends to be more structured
    and insightful, which likely contributes to the observed improvements in code
    repair tasks over human-generated guidance.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-3.1的实验结果]：本实验对不同来源的指导对MORepair代码修复能力的影响进行了定量分析，结果在表格 [VII](#S5.T7 "TABLE
    VII ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against
    State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs")（EvalRepair-C++）和 [VIII](#S5.T8
    "TABLE VIII ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair
    against State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs")（EvalRepair-Java）中呈现。这些表格表明，LLM生成的指导在提升代码修复性能方面明显优于人为生成的指导。采用LLM生成的指导在EvalRepair-C++上的TOP-10改进为3.0%到7.9%，在EvalRepair-Java上的改进为1.2%到5.5%。此外，列表
    [2](#LST2 "Listing 2 ‣ II Motivating Example ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs") 和 [1](#LST1 "Listing 1 ‣ II Motivating Example
    ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair with LLMs") 分别提供了LLM和人为生成的指导的示例。这些示例展示了LLM生成的指导往往更具结构性和洞察力，这可能有助于观察到的代码修复任务的改善。'
- en: A detailed analysis highlights significant variance in the effectiveness of
    human-generated guidance across different model sizes. For example, by leveraging
    human-generated guidance, CodeLlama-13B achieves an 8.0% and 6.2% TOP-10 increment
    compared to the baseline on EvalRepair-C++ and EvalRepair-Java, respectively.
    In contrast, CodeLlama with another size 7B exhibits a 4.2% decrease of TOP-10
    on EvalRepair-C++. This variation emphasizes the superior text comprehension and
    reasoning capabilities of larger LLMs, such as Llama2-13B, over smaller models
    like Llama2-7B[[34](#bib.bib34)], underscoring the significance of model size
    in effectively utilizing human-generated guidance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 详细分析突显了在人为生成的指导在不同模型大小下有效性的显著差异。例如，通过利用人为生成的指导，CodeLlama-13B在EvalRepair-C++和EvalRepair-Java上分别实现了8.0%和6.2%的TOP-10增益。相比之下，另一个尺寸为7B的CodeLlama在EvalRepair-C++上显示出4.2%的TOP-10下降。这种差异强调了较大LLM（如Llama2-13B）相对于较小模型（如Llama2-7B）在文本理解和推理能力上的优越性，凸显了模型大小在有效利用人为生成的指导中的重要性。
- en: '[Experimental Design for RQ-3.2]: To evaluate the effectiveness of MORepair,
    we compare it with two advanced fine-tuning approaches for code repair tasks:
    RepairLLaMA [[17](#bib.bib17)] and Fine-tune-CoT [[25](#bib.bib25)]. RepairLLaMA
    fine-tunes LLMs using code representation and fault localization information to
    repair buggy codes. This approach requires manually annotated perfect fault location
    information before repairing the buggy code, contrasting with our MORepair, which
    directly repairs buggy code without additional manual costs. Since Silva *et al.*
    only released the code and the checkpoint of fine-tuned CodeLlama-7B, and they
    have not released the training dataset, thus we can only reproduce their results
    based on CodeLlama-7B. To provide the necessary input information for the inference
    of RepairLLaMA, we manually annotated the fault localization information of EvalRepair-C++
    and EvalRepair-Java. Fine-tune-CoT is a method that utilizes the chain-of-thought
    data generated by large language models to fine-tune small models, thereby transferring
    complex reasoning capabilities from teacher models to student models. We implement
    the Fine-tune-CoT by concatenating the repaired code with guidance information
    to serve as the target objective for fine-tuning. We fine-tune all four selected
    LLMs with Fine-tune-CoT, evaluating their repair performance on EvalRepair-C++
    and EvalRepair-Java, compared to MORepair.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-3.2 的实验设计](https://example.org) ：为了评估 MORepair 的有效性，我们将其与两种先进的代码修复任务微调方法进行比较：RepairLLaMA [[17](#bib.bib17)]
    和 Fine-tune-CoT [[25](#bib.bib25)]。RepairLLaMA 通过使用代码表示和故障定位信息来微调 LLM，以修复有缺陷的代码。这种方法需要在修复有缺陷的代码之前，手动注释出完美的故障定位信息，这与我们的
    MORepair 形成了对比，后者直接修复有缺陷的代码而无需额外的人工成本。由于 Silva *et al.* 仅发布了微调后的 CodeLlama-7B
    的代码和检查点，而没有发布训练数据集，因此我们只能基于 CodeLlama-7B 重现他们的结果。为了为 RepairLLaMA 的推断提供必要的输入信息，我们手动注释了
    EvalRepair-C++ 和 EvalRepair-Java 的故障定位信息。Fine-tune-CoT 是一种利用大语言模型生成的思路链数据来微调小模型的方法，从而将复杂的推理能力从教师模型转移到学生模型。我们通过将修复后的代码与指导信息串联起来作为微调的目标来实现
    Fine-tune-CoT。我们对所有四个选定的 LLM 进行 Fine-tune-CoT 微调，评估它们在 EvalRepair-C++ 和 EvalRepair-Java
    上的修复性能，与 MORepair 进行比较。'
- en: '[Experimental Results for RQ-3.2]: The results, as detailed in Tables [IX](#S5.T9
    "TABLE IX ‣ V-C Evaluating the Impact of Guidance Sources and Comparing MORepair
    against State-of-the-Art Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") and [X](#S5.T10 "TABLE X ‣
    V-C Evaluating the Impact of Guidance Sources and Comparing MORepair against State-of-the-Art
    Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs"), clearly demonstrate that MORepair surpasses
    both Fine-tune-CoT and RepairLLaMA across TOP-5 and TOP-10 metrics on EvalRepair-C++
    and EvalRepair-Java benchmarks. This establishes the robustness of MORepair in
    enhancing code repair tasks. It is noteworthy that, when evaluating the repair
    performance of RepairLLaMA, benchmarks comprising manually annotated bug localization
    information, represent more information than what MORepair received. Despite this,
    MORepair demonstrates a more substantial improvement in repair performance than
    RepairLLaMA, which failed to achieve a TOP-10 enhancement in both benchmarks.
    This indicates that LLM-based program repair can achieve better repair performance
    without first conducting bug localization and then proceeding to patch generation.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-3.2 的实验结果](https://example.org) ：如表格 [IX](#S5.T9 "TABLE IX ‣ V-C Evaluating
    the Impact of Guidance Sources and Comparing MORepair against State-of-the-Art
    Fine-tuning Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for
    Enhanced Program Repair with LLMs") 和 [X](#S5.T10 "TABLE X ‣ V-C Evaluating the
    Impact of Guidance Sources and Comparing MORepair against State-of-the-Art Fine-tuning
    Methods ‣ V Experiments & Results ‣ Multi-Objective Fine-Tuning for Enhanced Program
    Repair with LLMs") 中所详述，结果清晰地表明，MORepair 在 EvalRepair-C++ 和 EvalRepair-Java 基准测试中的
    TOP-5 和 TOP-10 指标上均超越了 Fine-tune-CoT 和 RepairLLaMA。这证明了 MORepair 在提升代码修复任务中的鲁棒性。值得注意的是，在评估
    RepairLLaMA 的修复性能时，包含人工注释的错误定位信息的基准测试提供的信息量大于 MORepair 所获得的信息。尽管如此，MORepair 在修复性能上表现出比
    RepairLLaMA 更显著的改进，而 RepairLLaMA 在两个基准测试中未能实现 TOP-10 的提升。这表明，基于 LLM 的程序修复可以在不先进行错误定位然后再进行补丁生成的情况下，获得更好的修复性能。'
- en: Meanwhile, Fine-tune-CoT demonstrates mixed results. It increased TOP-10 by
    9.8% on EvalRepair-C++ for CodeLlama-13B, outperforming the StdFT by 3.0% of TOP-10\.
    However, Fine-tune-CoT gains only a 1.3% improvement in TOP-10 evaluated on EvalRepair-Java
    and did not enhance TOP-10 over the other baselines, indicating the lack of cross-language
    generalization. These findings underscore the effectiveness of MORepair in code
    repair compared to state-of-the-art fine-tuning approaches like RepairLLaMA and
    Fine-tune-CoT.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，Fine-tune-CoT 显示出混合的结果。它在 EvalRepair-C++ 上使 CodeLlama-13B 的 TOP-10 提升了 9.8%，超出了
    StdFT 的 3.0% TOP-10。 然而，Fine-tune-CoT 在 EvalRepair-Java 上仅提高了 1.3% 的 TOP-10，未能超过其他基准，表明缺乏跨语言的泛化能力。这些发现强调了
    MORepair 在代码修复中的有效性，相较于像 RepairLLaMA 和 Fine-tune-CoT 这样的最先进的微调方法。
- en: '[RQ-3]
    Findings: (1) LLM-generated guidance is more effective in enhancing the reasoning
    capabilities during fine-tuning LLMs than human-generated guidance, highlighting
    its key role in MORepair. (2) MORepair outperforms Fine-tune-CoT and RepairLLaMA
    on EvalRepair-C++ and EvalRepair-Java, even when RepairLLaMA is provided with
    perfect fault location information. Insights: (1) LLM-generated guidance signifies
    that the previously manual task of annotating datasets with rationale can now
    be automatically generated by LLMs, leading to liberation from labor constraints.
    (2) The outperforming results of the end-to-end fine-tuning approach MORepair
    confirm that LLM-based program repair can perform well without the need to identify
    fault location before generating patches.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ-3]
    Findings: (1) LLM-generated guidance is more effective in enhancing the reasoning
    capabilities during fine-tuning LLMs than human-generated guidance, highlighting
    its key role in MORepair. (2) MORepair outperforms Fine-tune-CoT and RepairLLaMA
    on EvalRepair-C++ and EvalRepair-Java, even when RepairLLaMA is provided with
    perfect fault location information. Insights: (1) LLM-generated guidance signifies
    that the previously manual task of annotating datasets with rationale can now
    be automatically generated by LLMs, leading to liberation from labor constraints.
    (2) The outperforming results of the end-to-end fine-tuning approach MORepair
    confirm that LLM-based program repair can perform well without the need to identify
    fault location before generating patches.'
- en: 'TABLE IX: Performance of LLMs fine-tuned with Fine-tune-CoT, RepairLLaMA, and
    MORepair on EvalRepair-C++.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：LLMs 使用 Fine-tune-CoT、RepairLLaMA 和 MORepair 在 EvalRepair-C++ 上的表现。
- en: '| Model | Approach | TOP-5 | TOP-10 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | Fine-tune-CoT | 56.7 (+15.8) | 68.3 (+ 9.8) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | Fine-tune-CoT | 56.7 (+15.8) | 68.3 (+ 9.8) |'
- en: '| MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 61.6 (+20.7) | 69.5 (+11.0) |'
- en: '| CodeLlama-7B | Fine-tune-CoT | 42.7 (- 3.6) | 55.5 (- 3.6) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | Fine-tune-CoT | 42.7 (- 3.6) | 55.5 (- 3.6) |'
- en: '| RepairLLaMA^* | 52.4 (+ 6.0) | 55.5 (- 3.6) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| RepairLLaMA^* | 52.4 (+ 6.0) | 55.5 (- 3.6) |'
- en: '| MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 56.7 (+10.4) | 62.8 (+ 3.7) |'
- en: '| StarChat-alpha | Fine-tune-CoT | 37.8 (-12.2) | 43.9 (-18.3) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | Fine-tune-CoT | 37.8 (-12.2) | 43.9 (-18.3) |'
- en: '| MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.9) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 52.4 (+ 2.4) | 65.9 (+ 3.9) |'
- en: '| Mistral-7B | Fine-tune-CoT | 33.5 (+ 1.2) | 37.8 (-14.3) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | Fine-tune-CoT | 33.5 (+ 1.2) | 37.8 (-14.3) |'
- en: '| MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 40.2 (+ 7.9) | 50.0 (+ 3.0) |'
- en: '*'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: RepairLLaMA only has the version of CodeLlama-7B.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RepairLLaMA 仅有 CodeLlama-7B 版本。
- en: †
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的数值表示相对于相应基准的变化。
- en: 'TABLE X: Performance of LLMs fine-tuned with Fine-tune-CoT, RepairLLaMA, and
    MORepair on EvalRepair-Java.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 X：LLMs 使用 Fine-tune-CoT、RepairLLaMA 和 MORepair 在 EvalRepair-Java 上的表现。
- en: '| Model | Approach | TOP-5 | TOP-10 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | TOP-5 | TOP-10 |'
- en: '| CodeLlama-13B | Fine-tune-CoT | 59.5 (+ 5.5) | 71.2 (+ 1.3) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B | Fine-tune-CoT | 59.5 (+ 5.5) | 71.2 (+ 1.3) |'
- en: '| MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 69.3 (+15.3) | 77.9 (+ 8.0) |'
- en: '| CodeLlama-7B | Fine-tune-CoT | 45.4 (- 4.3) | 57.7 (- 4.3) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B | Fine-tune-CoT | 45.4 (- 4.3) | 57.7 (- 4.3) |'
- en: '| RepairLLaMA^* | 52.1 (+ 2.4) | 60.1 (- 1.9) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| RepairLLaMA^* | 52.1 (+ 2.4) | 60.1 (- 1.9) |'
- en: '| MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 59.5 (+ 9.8) | 67.5 (+ 5.5) |'
- en: '| StarChat-alpha | Fine-tune-CoT | 41.7 (- 1.9) | 54.6 (- 6.1) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| StarChat-alpha | Fine-tune-CoT | 41.7 (- 1.9) | 54.6 (- 6.1) |'
- en: '| MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 56.4 (+12.8) | 66.3 (+ 5.6) |'
- en: '| Mistral-7B | Fine-tune-CoT | 36.8 (+ 3.1) | 46.0 (- 6.1) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | Fine-tune-CoT | 36.8 (+ 3.1) | 46.0 (- 6.1) |'
- en: '| MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MORepair | 45.4 (+11.7) | 58.3 (+ 6.2) |'
- en: '*'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: RepairLLaMA only has the version of CodeLlama-7B.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RepairLLaMA 仅有 CodeLlama-7B 版本。
- en: †
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: †
- en: Values in parentheses indicate the change relative to the corresponding baseline.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 括号中的数值表示相对于相应基准的变化。
- en: VI Threats to Validity
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 有效性威胁
- en: Threats to Internal Validity. The choice of base LLMs may impact the experimental
    conclusions. To minimize potential bias, we have conducted experiments using four
    LLMs varying different sizes and architectures, including CodeLlama-13B-instruct,
    CodeLlama-7B-instruct, StarChat-alpha, and Mistral-Instruct-7B. By diversifying
    the LLMs selection, we aim to ensure that our findings are not limited to a specific
    LLM type or scale.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 内部有效性威胁。基础 LLMs 的选择可能会影响实验结论。为了最小化潜在的偏差，我们使用了四种不同规模和架构的 LLM 进行实验，包括 CodeLlama-13B-instruct、CodeLlama-7B-instruct、StarChat-alpha
    和 Mistral-Instruct-7B。通过多样化 LLM 的选择，我们旨在确保我们的发现不限于特定类型或规模的 LLM。
- en: Threats to External Validity. Insufficient test cases in the evaluation benchmarks
    may lead to patch over-fitting problems, where LLMs successfully pass the limited
    test cases without genuinely understanding or correcting the underlying logical
    errors. To address this issue, we have integrated test cases from EvalPlus [[37](#bib.bib37)]
    to enhance the diversity of the test cases in our benchmark EvalRepair-C++ and
    EvalRepair-Java, detailed in Section [IV-B](#S4.SS2 "IV-B Evaluation Benchmark
    ‣ IV Experimental Setup ‣ Multi-Objective Fine-Tuning for Enhanced Program Repair
    with LLMs"). This helps us to assess the LLMs’ repair performance in a more realistic
    setting and improves the external validity of our conclusions.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 外部有效性的威胁。评估基准中测试案例不足可能导致过度拟合问题，其中LLMs成功通过有限的测试案例，却没有真正理解或纠正潜在的逻辑错误。为了解决这一问题，我们从
    EvalPlus [[37](#bib.bib37)] 集成了测试案例，以增强我们基准 EvalRepair-C++ 和 EvalRepair-Java 中测试案例的多样性，详细信息见第
    [IV-B](#S4.SS2 "IV-B Evaluation Benchmark ‣ IV Experimental Setup ‣ Multi-Objective
    Fine-Tuning for Enhanced Program Repair with LLMs") 节。这有助于我们在更现实的环境中评估LLMs的修复性能，提高我们结论的外部有效性。
- en: Threats to Construct Validity. The inherent randomness in generating outputs
    by LLMs could undermine the validity of experimental conclusions. To address this
    issue, we utilize LLMs to produce outputs ten times, subsequently calculating
    the TOP-5 and TOP-10 metrics. By considering multiple rounds of generated outputs,
    we aim to minimize the impact of randomness on our findings and ensure that the
    conclusions are based on a more stable and representative set of results.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 构造有效性的威胁。LLMs生成输出的固有随机性可能会削弱实验结论的有效性。为了解决这一问题，我们利用LLMs生成十次输出，然后计算TOP-5和TOP-10指标。通过考虑多轮生成的输出，我们旨在最小化随机性对我们发现的影响，确保结论基于更稳定和具有代表性的一组结果。
- en: VII Related Work
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 相关工作
- en: In recent years, code LLMs [[41](#bib.bib41), [42](#bib.bib42), [35](#bib.bib35),
    [22](#bib.bib22), [20](#bib.bib20)] have made significant strides in advancing
    the field of code-related tasks, especially in program repair. Among the various
    methodologies employed, fine-tuning has become a crucial technique for adapting
    LLMs to specific domain applications, demonstrating significant improvements in
    program repair tasks [[15](#bib.bib15), [16](#bib.bib16), [4](#bib.bib4), [18](#bib.bib18),
    [43](#bib.bib43), [14](#bib.bib14)]. TFix, proposed by Berabi *et al.* [[14](#bib.bib14)],
    leveraging T5 [[19](#bib.bib19)] fine-tuned with GitHub commits to surpass existing
    learning-based repair approaches for JavaScript programs. Lajkó *et al.* [[15](#bib.bib15)]
    fine-tuned GPT-2 [[44](#bib.bib44)] with 16k samples of JavaScript codes, evaluated
    both the pre-trained (baseline) and the fine-tuned GPT-2 model on a dataset of
    18,736 created from GitHub commits, with 16,863 samples used for fine-tuning and
    1,559 samples for testing, achieving a 15.5% improvement in TOP-10 accuracy on
    a JavaScript benchmark. The results showed that while the pre-trained model could
    generate syntactically and semantically correct source code, fine-tuning increased
    the number of correctly repaired programs from 27 to 269, significantly boosting
    its performance. Jiang *et al.* [[4](#bib.bib4)] studied the impact of LLMs on
    automated program repair (APR) and evaluated fine-tuning LLMs on four APR benchmarks,
    including a new benchmark HumanEval-Java to avoid the data leakage issue. Experiments
    showed that the best LLMs fixed 72% more bugs in total on the four benchmarks
    than the best deep learning-based APR technique, and fine-tuning further improved
    LLMs’ fixing capabilities, enabling them to fix 46% to 164% more bugs than the
    best deep learning APR technique. Huang *et al.* [[16](#bib.bib16)] found that
    UniXcoder [[45](#bib.bib45)], an LLM smaller than CodeT5 [[46](#bib.bib46)], could
    achieve superior repair performance through fine-tuning, challenging the notion
    that larger models always perform better. RepairLLaMA [[17](#bib.bib17)] presented
    a novel fine-tuning approach to automated program repair by combining specialized
    code representations with efficient fine-tuning techniques. This approach allowed
    RepairLLaMA to effectively adapt LLMs for the program repair task, significantly
    surpassing CodeLlama-13B [[20](#bib.bib20)] baseline on multiple Java benchmarks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，代码LLM[[41](#bib.bib41), [42](#bib.bib42), [35](#bib.bib35), [22](#bib.bib22),
    [20](#bib.bib20)]在推进代码相关任务，特别是程序修复领域方面取得了显著进展。在采用的各种方法中，微调已成为将LLM适应特定领域应用的关键技术，在程序修复任务中表现出了显著的改进[[15](#bib.bib15),
    [16](#bib.bib16), [4](#bib.bib4), [18](#bib.bib18), [43](#bib.bib43), [14](#bib.bib14)]。Berabi
    *et al.* [[14](#bib.bib14)] 提出的TFix，利用T5 [[19](#bib.bib19)] 通过GitHub提交进行微调，以超越现有的基于学习的JavaScript程序修复方法。Lajkó
    *et al.* [[15](#bib.bib15)] 使用16k样本的JavaScript代码对GPT-2 [[44](#bib.bib44)] 进行了微调，在由GitHub提交创建的18,736个数据集上评估了预训练（基线）和微调后的GPT-2模型，其中使用了16,863个样本进行微调，1,559个样本进行测试，在JavaScript基准上实现了TOP-10准确率提升了15.5%。结果显示，虽然预训练模型能够生成语法和语义正确的源代码，但微调将正确修复的程序数量从27增加到269，显著提升了性能。Jiang
    *et al.* [[4](#bib.bib4)] 研究了LLM对自动程序修复（APR）的影响，并在四个APR基准上评估了微调LLM，包括新的基准HumanEval-Java，以避免数据泄露问题。实验表明，最佳的LLM在四个基准上修复的错误比最佳深度学习APR技术多72%，而微调进一步提升了LLM的修复能力，使其修复的错误比最佳深度学习APR技术多46%到164%。Huang
    *et al.* [[16](#bib.bib16)] 发现，比CodeT5 [[46](#bib.bib46)]小的LLM UniXcoder [[45](#bib.bib45)]
    通过微调能够实现更优的修复性能，挑战了大模型总是表现更好的观念。RepairLLaMA [[17](#bib.bib17)] 提出了一种新颖的自动程序修复微调方法，通过将专业的代码表示与高效的微调技术相结合，使RepairLLaMA能够有效地将LLM适应程序修复任务，在多个Java基准上显著超越了CodeLlama-13B
    [[20](#bib.bib20)] 基线。
- en: Diverging from prior fine-tuning practices that utilize LLMs for program repair,
    which predominantly concentrated on enriching the training datasets with standard
    single-objective fine-tuning approaches [[15](#bib.bib15), [4](#bib.bib4), [16](#bib.bib16),
    [14](#bib.bib14)], our approach MORepair leverage multi-objective learning and
    LLM-generated guidance during fine-tuning, consistently achieving superior repair
    performance compared to standard single-objective fine-tuning approach, and state-of-the-art
    fine-tuning approaches RepairLLaMA and Fine-tune-CoT.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于以往利用 LLM 进行程序修复的微调实践，这些实践主要集中在使用标准单目标微调方法来丰富训练数据集[[15](#bib.bib15), [4](#bib.bib4),
    [16](#bib.bib16), [14](#bib.bib14)]，我们的方法 MORepair 利用多目标学习和 LLM 生成的指导进行微调，始终在修复性能上超越标准单目标微调方法及最先进的微调方法
    RepairLLaMA 和 Fine-tune-CoT。
- en: VIII Conclusion
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: This paper introduces a novel multi-objective fine-tuning framework MORepair
    that empowers open-source LLMs to effectively learn repair logic and generate
    high-quality patches for program repair tasks. Our approach employs a multi-objective
    learning strategy, simultaneously optimizing for generating repaired code and
    producing corresponding explanatory guidance during fine-tuning. By employing
    multi-objective learning and explanatory guidance on four LLMs with different
    architectures and sizes, MORepair outperforms StdFT and baseline models, achieving
    up to 11.0% and 8.0% improvements over baseline in TOP-10 on EvalRepair-C++ and
    EvalRepair-Java benchmarks, respectively. These findings highlight MORepair’s
    robustness and adaptability across different programming languages and various
    LLM architectures and sizes. Furthermore, MORepair surpasses existing state-of-the-art
    fine-tuning methods such as Fine-tune-CoT and RepairLLaMA across four LLMs in
    both benchmarks. Our ablation study emphasizes the significant impact of multi-objective
    learning and the distinct advantages of LLM-generated guidance over human-generated
    guidance in enhancing program repair performance. Our work highlights the significance
    of employing a multi-objective learning strategy and LLM-generated natural language
    guidance in advancing code repair tasks, paving the way for more intelligent and
    efficient automated program repair paradigms in the future.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种新颖的多目标微调框架 MORepair，该框架使开源 LLM 能够有效学习修复逻辑并生成高质量的程序修复补丁。我们的方法采用多目标学习策略，在微调过程中同时优化生成修复代码和产生相应的解释性指导。通过对四种不同架构和规模的
    LLM 采用多目标学习和解释性指导，MORepair 在 EvalRepair-C++ 和 EvalRepair-Java 基准测试中分别比基线模型提高了最高
    11.0% 和 8.0%。这些发现突显了 MORepair 在不同编程语言和各种 LLM 架构及规模下的鲁棒性和适应性。此外，MORepair 在这四种 LLM
    中超越了现有的最先进微调方法，如 Fine-tune-CoT 和 RepairLLaMA。我们的消融研究强调了多目标学习的显著影响以及 LLM 生成的指导相对于人工生成指导在提升程序修复性能方面的独特优势。我们的工作强调了采用多目标学习策略和
    LLM 生成自然语言指导在推进代码修复任务中的重要性，为未来更智能高效的自动化程序修复范式铺平了道路。
- en: References
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*, “Gpt-4 技术报告，” *arXiv 预印本
    arXiv:2303.08774*, 2023。'
- en: '[2] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162 out of
    337 bugs for $0.42 each using chatgpt,” *arXiv preprint arXiv:2304.00385*, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. S. Xia 和 L. Zhang, “保持对话进行：使用 chatgpt 修复 337 个 bug 中的 162 个，每个 $0.42，”
    *arXiv 预印本 arXiv:2304.00385*, 2023。'
- en: '[3] Z. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, “Improving automatically
    generated code from codex via automated program repair,” *arXiv preprint arXiv:2205.10583*,
    2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Z. Fan, X. Gao, A. Roychoudhury, 和 S. H. Tan, “通过自动化程序修复改善从 codex 自动生成的代码，”
    *arXiv 预印本 arXiv:2205.10583*, 2022。'
- en: '[4] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of code language models
    on automated program repair,” in *Proceedings of the 45th International Conference
    on Software Engineering*, ser. ICSE ’23.   IEEE Press, 2023, p. 1430–1442\. [Online].
    Available: https://doi.org/10.1109/ICSE48619.2023.00125'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] N. Jiang, K. Liu, T. Lutellier, 和 L. Tan, “代码语言模型对自动化程序修复的影响，” *第 45 届国际软件工程会议论文集*，序列
    ICSE ’23。IEEE 出版社，2023，第 1430–1442 页。[在线]. 可用: https://doi.org/10.1109/ICSE48619.2023.00125'
- en: '[5] J. Cao, M. Li, M. Wen, and S.-c. Cheung, “A study on prompt design, advantages
    and limitations of chatgpt for deep learning program repair,” *arXiv preprint
    arXiv:2304.08191*, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Cao, M. Li, M. Wen 和 S.-c. Cheung，“关于提示设计、ChatGPT在深度学习程序修复中的优势和限制的研究”，*arXiv预印本
    arXiv:2304.08191*，2023。'
- en: '[6] T. Phung, J. Cambronero, S. Gulwani, T. Kohn, R. Majumdar, A. Singla, and
    G. Soares, “Generating high-precision feedback for programming syntax errors using
    large language models,” *arXiv preprint arXiv:2302.04662*, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Phung, J. Cambronero, S. Gulwani, T. Kohn, R. Majumdar, A. Singla 和
    G. Soares，“使用大型语言模型生成高精度编程语法错误反馈”，*arXiv预印本 arXiv:2302.04662*，2023。'
- en: '[7] Y. Peng, S. Gao, C. Gao, Y. Huo, and M. Lyu, “Domain knowledge matters:
    Improving prompts with fix templates for repairing python type errors,” in *Proceedings
    of the 46th IEEE/ACM International Conference on Software Engineering*, 2024,
    pp. 1–13.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Peng, S. Gao, C. Gao, Y. Huo 和 M. Lyu，“领域知识很重要：使用修复模板改进修复Python类型错误的提示”，见于
    *第46届IEEE/ACM国际软件工程会议论文集*，2024，第1–13页。'
- en: '[8] H. Joshi, J. C. Sanchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radiček,
    “Repair is nearly generation: Multilingual program repair with llms,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 4, 2023, pp.
    5131–5140.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Joshi, J. C. Sanchez, S. Gulwani, V. Le, G. Verbruggen 和 I. Radiček，“修复几乎是生成：使用LLMs的多语言程序修复”，见于
    *AAAI人工智能会议论文集*，第37卷，第4期，2023，第5131–5140页。'
- en: '[9] S. Feng and C. Chen, “Prompting is all you need: Automated android bug
    replay with large language models,” in *Proceedings of the 46th IEEE/ACM International
    Conference on Software Engineering*, 2024, pp. 1–13.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Feng 和 C. Chen，“你只需提示：使用大型语言模型的自动化安卓漏洞重现”，见于 *第46届IEEE/ACM国际软件工程会议论文集*，2024，第1–13页。'
- en: '[10] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury, and S. H. Tan, “Automated
    repair of programs from large language models,” in *2023 IEEE/ACM 45th International
    Conference on Software Engineering (ICSE)*.   IEEE, 2023, pp. 1469–1481.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Fan, X. Gao, M. Mirchev, A. Roychoudhury 和 S. H. Tan，“来自大型语言模型的程序自动修复”，见于
    *2023 IEEE/ACM 第45届国际软件工程会议（ICSE）*。IEEE，2023，第1469–1481页。'
- en: '[11] J. A. Prenner, H. Babii, and R. Robbes, “Can openai’s codex fix bugs?
    an evaluation on quixbugs,” in *Proceedings of the Third International Workshop
    on Automated Program Repair*, 2022, pp. 69–75.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. A. Prenner, H. Babii 和 R. Robbes，“OpenAI的Codex能修复漏洞吗？在Quixbugs上的评估”，见于
    *第三届国际自动化程序修复研讨会论文集*，2022，第69–75页。'
- en: '[12] M. Fu, C. Tantithamthavorn, T. Le, V. Nguyen, and D. Phung, “Vulrepair:
    a t5-based automated software vulnerability repair,” in *Proceedings of the 30th
    ACM Joint European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering*, 2022, pp. 935–947.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Fu, C. Tantithamthavorn, T. Le, V. Nguyen 和 D. Phung，“Vulrepair：基于T5的自动化软件漏洞修复”，见于
    *第30届ACM联合欧洲软件工程会议与软件工程基础研讨会论文集*，2022，第935–947页。'
- en: '[13] Y. Noller, R. Shariffdeen, X. Gao, and A. Roychoudhury, “Trust enhancement
    issues in program repair,” in *Proceedings of the 44th International Conference
    on Software Engineering*, 2022, pp. 2228–2240.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Noller, R. Shariffdeen, X. Gao 和 A. Roychoudhury，“程序修复中的信任增强问题”，见于
    *第44届国际软件工程会议论文集*，2022，第2228–2240页。'
- en: '[14] B. Berabi, J. He, V. Raychev, and M. Vechev, “Tfix: Learning to fix coding
    errors with a text-to-text transformer,” in *International Conference on Machine
    Learning*.   PMLR, 2021, pp. 780–791.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. Berabi, J. He, V. Raychev 和 M. Vechev，“Tfix：利用文本到文本的转换器学习修复编码错误”，见于
    *机器学习国际会议*。PMLR，2021，第780–791页。'
- en: '[15] M. Lajkó, D. Horváth, V. Csuvik, and L. Vidács, “Fine-tuning gpt-2 to
    patch programs, is it worth it?” in *International Conference on Computational
    Science and Its Applications*.   Springer, 2022, pp. 79–91.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Lajkó, D. Horváth, V. Csuvik 和 L. Vidács，“微调GPT-2以修补程序，这值得吗？”见于 *计算科学及其应用国际会议*。Springer，2022，第79–91页。'
- en: '[16] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, “An
    empirical study on fine-tuning large language models of code for automated program
    repair,” in *2023 38th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)*.   IEEE, 2023, pp. 1162–1174.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li 和 Y. Zhang，“对大型语言模型代码进行微调以实现自动化程序修复的实证研究”，见于
    *2023年第38届IEEE/ACM国际自动化软件工程会议（ASE）*。IEEE，2023，第1162–1174页。'
- en: '[17] A. Silva, S. Fang, and M. Monperrus, “Repairllama: Efficient representations
    and fine-tuned adapters for program repair,” *arXiv preprint arXiv:2312.15698*,
    2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Silva, S. Fang 和 M. Monperrus，“Repairllama：用于程序修复的高效表示和微调适配器”，*arXiv预印本
    arXiv:2312.15698*，2023。'
- en: '[18] N. Jiang, T. Lutellier, and L. Tan, “Cure: Code-aware neural machine translation
    for automatic program repair,” in *2021 IEEE/ACM 43rd International Conference
    on Software Engineering (ICSE)*.   IEEE, 2021, pp. 1161–1173.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. Jiang, T. Lutellier 和 L. Tan, “Cure: 代码感知神经机器翻译用于自动程序修复，” 收录于 *2021
    IEEE/ACM 第 43 届国际软件工程会议 (ICSE)*，IEEE，2021，页 1161–1173。'
- en: '[19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified
    text-to-text transformer,” *Journal of Machine Learning Research*, vol. 21, no.
    140, pp. 1–67, 2020\. [Online]. Available: http://jmlr.org/papers/v21/20-074.html'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li 和 P. J. Liu, “探究统一文本到文本转换器的迁移学习极限，” *机器学习研究杂志*，第 21 卷，第 140 期，页 1–67，2020。
    [在线]. 可用: http://jmlr.org/papers/v21/20-074.html'
- en: '[20] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, T. Remez, J. Rapin *et al.*, “Code llama: Open foundation models for code,”
    *arXiv preprint arXiv:2308.12950*, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y.
    Adi, J. Liu, T. Remez, J. Rapin *等*，“Code llama: 开放的代码基础模型，” *arXiv 预印本 arXiv:2308.12950*,
    2023。'
- en: '[21] L. Tunstall, N. Lambert, N. Rajani, E. Beeching, T. Le Scao, L. von Werra,
    S. Han, P. Schmid, and A. Rush, “Creating a coding assistant with starcoder,”
    *Hugging Face Blog*, 2023, https://huggingface.co/blog/starchat.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] L. Tunstall, N. Lambert, N. Rajani, E. Beeching, T. Le Scao, L. von Werra,
    S. Han, P. Schmid 和 A. Rush, “使用 starcoder 创建编码助手，” *Hugging Face 博客*，2023，https://huggingface.co/blog/starchat。'
- en: '[22] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux,
    P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, “Mistral
    7b,” 2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A.
    Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix 和 W. E. Sayed, “Mistral
    7b,” 2023。'
- en: '[23] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever 和 W. Zaremba, “评估训练在代码上的大型语言模型，” 2021。'
- en: '[24] J. Yang, A. Zhikhartsev, Y. Liu, and L. Tan, “Better test cases for better
    automated program repair,” in *Proceedings of the 2017 11th joint meeting on foundations
    of software engineering*, 2017, pp. 831–841.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Yang, A. Zhikhartsev, Y. Liu 和 L. Tan, “更好的测试用例以改进自动化程序修复，” 收录于 *2017
    年第 11 届联合软件工程基础会议论文集*，2017，页 831–841。'
- en: '[25] N. Ho, L. Schmid, and S.-Y. Yun, “Large language models are reasoning
    teachers,” *arXiv preprint arXiv:2212.10071*, 2022.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Ho, L. Schmid 和 S.-Y. Yun, “大型语言模型是推理老师，” *arXiv 预印本 arXiv:2212.10071*,
    2022。'
- en: '[26] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen,
    A. Wang, Y. Li, T. Su, Z. Yang, and J. Tang, “Codegeex: A pre-trained model for
    code generation with multilingual evaluations on humaneval-x,” 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen,
    A. Wang, Y. Li, T. Su, Z. Yang 和 J. Tang, “Codegeex: 一种用于代码生成的预训练模型，并在 humaneval-x
    上进行多语言评估，” 2023。'
- en: '[27] Z. Shen, Z. Liu, J. Qin, M. Savvides, and K.-T. Cheng, “Partial is better
    than all: Revisiting fine-tuning strategy for few-shot learning,” in *Proceedings
    of the AAAI conference on artificial intelligence*, vol. 35, no. 11, 2021, pp.
    9594–9602.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Shen, Z. Liu, J. Qin, M. Savvides 和 K.-T. Cheng, “部分胜于全: 重访少样本学习的微调策略，”
    收录于 *AAAI 人工智能会议论文集*，第 35 卷，第 11 期，2021，页 9594–9602。'
- en: '[28] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *等*，“链式思维提示激发大型语言模型的推理，” *神经信息处理系统进展*，第35卷，第24 824–24 837页，2022年。'
- en: '[29] Y. Zhang and Q. Yang, “An overview of multi-task learning,” *National
    Science Review*, vol. 5, no. 1, pp. 30–43, 2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Zhang 和 Q. Yang，“多任务学习概述，” *国家科学评论*，第5卷，第1期，第30–43页，2018年。'
- en: '[30] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” *arXiv preprint
    arXiv:2106.09685*, 2021.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen，“Lora：大型语言模型的低秩适配，” *arXiv预印本 arXiv:2106.09685*，2021年。'
- en: '[31] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” *Advances in Neural Information Processing Systems*,
    vol. 36, 2024.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer，“Qlora：量化LLM的高效微调，”
    *神经信息处理系统进展*，第36卷，2024年。'
- en: '[32] N. Jain, P.-y. Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli,
    B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha *et al.*, “Neftune:
    Noisy embeddings improve instruction finetuning,” *arXiv preprint arXiv:2310.05914*,
    2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] N. Jain, P.-y. Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli,
    B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha *等*，“Neftune：噪声嵌入改进指令微调，”
    *arXiv预印本 arXiv:2310.05914*，2023年。'
- en: '[33] Y. Chen, D. Hazarika, M. Namazifar, Y. Liu, D. Jin, and D. Hakkani-Tur,
    “Empowering parameter-efficient transfer learning by recognizing the kernel structure
    in self-attention,” *arXiv preprint arXiv:2205.03720*, 2022.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Chen, D. Hazarika, M. Namazifar, Y. Liu, D. Jin, 和 D. Hakkani-Tur，“通过识别自注意力中的内核结构来增强参数高效的迁移学习，”
    *arXiv预印本 arXiv:2205.03720*，2022年。'
- en: '[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2：开放基础和微调聊天模型，” *arXiv预印本
    arXiv:2307.09288*，2023年。'
- en: '[35] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
    C. Akiki, J. Li, J. Chim *et al.*, “Starcoder: may the source be with you!” *arXiv
    preprint arXiv:2305.06161*, 2023.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
    C. Akiki, J. Li, J. Chim *等*，“Starcoder：愿源代码与你同在！” *arXiv预印本 arXiv:2305.06161*，2023年。'
- en: '[36] H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage, “A hazard
    analysis framework for code synthesis large language models,” 2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, 和 M. Brundage，“代码合成大型语言模型的风险分析框架，”
    2022年。'
- en: '[37] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, “Is your code generated by chatgpt
    really correct? rigorous evaluation of large language models for code generation,”
    *arXiv preprint arXiv:2305.01210*, 2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Liu, C. S. Xia, Y. Wang, 和 L. Zhang，“你的代码由chatgpt生成的真的正确吗？对大型语言模型生成代码的严格评估，”
    *arXiv预印本 arXiv:2305.01210*，2023年。'
- en: '[38] P. S. Kochhar, X. Xia, D. Lo, and S. Li, “Practitioners’ expectations
    on automated fault localization,” in *Proceedings of the 25th international symposium
    on software testing and analysis*, 2016, pp. 165–176.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] P. S. Kochhar, X. Xia, D. Lo, 和 S. Li，“从业者对自动化故障定位的期望，” 见于 *第25届国际软件测试与分析研讨会*，2016年，第165–176页。'
- en: '[39] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal, and T. Ahmed, “Review4repair:
    Code review aided automatic program repairing,” *Information and Software Technology*,
    vol. 143, p. 106765, 2022.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal, 和 T. Ahmed，“Review4repair：代码审查辅助的自动程序修复，”
    *信息与软件技术*，第143卷，第106765页，2022年。'
- en: '[40] C.-P. Wong, P. Santiesteban, C. Kästner, and C. Le Goues, “Varfix: balancing
    edit expressiveness and search effectiveness in automated program repair,” in
    *Proceedings of the 29th ACM joint meeting on European software engineering conference
    and symposium on the foundations of software engineering*, 2021, pp. 354–366.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C.-P. Wong, P. Santiesteban, C. Kästner, 和 C. Le Goues，“Varfix：在自动化程序修复中平衡编辑表现力和搜索效果，”
    见于 *第29届ACM联合欧洲软件工程会议及软件工程基础研讨会*，2021年，第354–366页。'
- en: '[41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” *arXiv preprint arXiv:2203.13474*, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    和 C. Xiong，“Codegen：一个开放的大型语言模型，用于多轮程序合成，” *arXiv预印本 arXiv:2203.13474*，2022年。'
- en: '[42] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
    W.-t. Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative model for code
    infilling and synthesis,” *arXiv preprint arXiv:2204.05999*, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
    W.-t. Yih, L. Zettlemoyer, 和 M. Lewis，“Incoder：用于代码填充和合成的生成模型，” *arXiv 预印本 arXiv:2204.05999*，2022年。'
- en: '[43] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy,
    “Inferfix: End-to-end program repair with llms,” *arXiv preprint arXiv:2303.07263*,
    2023.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, 和 A. Svyatkovskiy，“Inferfix：基于
    llms 的端到端程序修复，” *arXiv 预印本 arXiv:2303.07263*，2023年。'
- en: '[44] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *等*，“语言模型是无监督的多任务学习者，”
    *OpenAI 博客*，第1卷，第8期，第9页，2019年。'
- en: '[45] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, “Unixcoder: Unified
    cross-modal pre-training for code representation,” *arXiv preprint arXiv:2203.03850*,
    2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, 和 J. Yin，“Unixcoder：用于代码表示的统一跨模态预训练，”
    *arXiv 预印本 arXiv:2203.03850*，2022年。'
- en: '[46] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware unified
    pre-trained encoder-decoder models for code understanding and generation,” *arXiv
    preprint arXiv:2109.00859*, 2021.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Wang, W. Wang, S. Joty, 和 S. C. Hoi，“Codet5：识别符感知的统一预训练编码器-解码器模型用于代码理解和生成，”
    *arXiv 预印本 arXiv:2109.00859*，2021年。'
