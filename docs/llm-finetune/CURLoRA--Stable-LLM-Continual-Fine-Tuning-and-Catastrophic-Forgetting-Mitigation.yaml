- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:34:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:34:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CURLoRA：稳定的大型语言模型持续微调和灾难性遗忘缓解
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.14572](https://ar5iv.labs.arxiv.org/html/2408.14572)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.14572](https://ar5iv.labs.arxiv.org/html/2408.14572)
- en: 'Muhammad Fawi Independent Researcher. [ORCID: 0009-0007-7210-0528](https://orcid.org/0009-0007-7210-0528).
    Code available at: [https://github.com/mnoorfawi/curlora](https://github.com/mnoorfawi/curlora).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '穆罕默德·法维 独立研究员。[ORCID: 0009-0007-7210-0528](https://orcid.org/0009-0007-7210-0528)。代码可在：[https://github.com/mnoorfawi/curlora](https://github.com/mnoorfawi/curlora)
    获得。'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This paper introduces CURLoRA, a novel approach to fine-tuning large language
    models (LLMs) that leverages CUR matrix decomposition in the context of Low-Rank
    Adaptation (LoRA). Our method addresses two critical challenges in LLM fine-tuning:
    mitigating catastrophic forgetting during continual learning and reducing the
    number of trainable parameters. We propose a unique modification to the CUR decomposition
    process, utilizing inverted probabilities for column and row selection which acts
    as an implicit regularization, and initializing the $U$ matrix as a zero matrix,
    and only fine-tuning it. We demonstrate through experiments on multiple datasets
    that CURLoRA outperforms standard LoRA in mitigating catastrophic forgetting.
    It maintains model stability and performance across tasks while significantly
    reducing the number of trainable parameters. Our results show that CURLoRA achieves
    very good and stable task accuracy while maintaining base model’s perplexity scores
    fixed compared to LoRA upon continual fine-tuning, particularly in scenarios with
    limited data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 CURLoRA，这是一种新颖的大型语言模型（LLMs）微调方法，利用 CUR 矩阵分解在低秩适配（LoRA）的背景下。我们的方法解决了 LLM
    微调中的两个关键挑战：减轻持续学习过程中的灾难性遗忘和减少可训练参数的数量。我们提出了一种对 CUR 分解过程的独特修改，利用列和行选择的倒数概率作为隐式正则化，并将
    $U$ 矩阵初始化为零矩阵，并仅对其进行微调。我们通过在多个数据集上的实验展示了 CURLoRA 在减轻灾难性遗忘方面优于标准的 LoRA。它在任务间保持模型稳定性和性能，同时显著减少了可训练参数的数量。我们的结果表明，相较于
    LoRA，在持续微调过程中，CURLoRA 实现了非常好的和稳定的任务准确性，同时保持了基础模型的困惑度分数不变，特别是在数据有限的情况下。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have revolutionized natural language processing,
    demonstrating remarkable capabilities across a wide range of tasks [[1](#bib.bib1)].
    However, fine-tuning these large models for specific tasks requires a lot of computational
    resources making it challenging to adapt these models efficiently, especially
    when working with limited datasets and in resource-constrained environments. [[2](#bib.bib2)].
    Parameter-Efficient Fine-Tuning (PEFT) Methods have gained a lot of attention
    because they make fine-tuning large models accessible and possible. [[3](#bib.bib3)]
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经彻底改变了自然语言处理，展示了在广泛任务中的卓越能力[[1](#bib.bib1)]。然而，将这些大型模型微调以适应特定任务需要大量计算资源，这使得在有限的数据集和资源受限的环境中高效适应这些模型变得具有挑战性[[2](#bib.bib2)]。参数高效微调（PEFT）方法受到了极大的关注，因为它们使得大型模型的微调变得可行且实际[[3](#bib.bib3)]。
- en: 'Low-Rank Adaptation (LoRA) [[4](#bib.bib4)] has emerged as an efficient PEFT
    method, enabling fine-tuning large language models on custom tasks while decreasing
    the number of trainable parameters hence requiring less resources. LoRA works
    by decomposing pre-trained weight matrices into low-rank matrices and fine-tune
    these ones instead of the original matrix. Although LoRA has proven to be very
    excellent and promising, it still faces challenges with catastrophic forgetting.
    Catastrophic forgetting in LLMs is a critical issue where the model loses previously
    acquired knowledge when fine-tuned on new tasks [[5](#bib.bib5)]. It occurs due
    to the overwriting of previously learned (pre-trained) weights during the fine-tuning
    process. In LoRA, this often happens as the adapted output can significantly deviate
    from the original:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适配（LoRA）[[4](#bib.bib4)] 作为一种高效的参数高效微调（PEFT）方法已经出现，使得在定制任务上微调大型语言模型变得可能，同时减少了可训练参数的数量，从而减少了资源需求。LoRA
    通过将预训练的权重矩阵分解为低秩矩阵，并对这些矩阵进行微调，而不是对原始矩阵进行微调来实现。尽管 LoRA 已被证明非常优秀且具有前景，但它仍面临灾难性遗忘的挑战。在大型语言模型（LLMs）中，灾难性遗忘是一个关键问题，即模型在对新任务进行微调时会丧失之前获得的知识[[5](#bib.bib5)]。这种情况发生是因为在微调过程中之前学习到的（预训练的）权重被覆盖。在
    LoRA 中，这种情况经常发生，因为适应后的输出可能会显著偏离原始输出：
- en: '|  | $y=xW+xW_{adapted}=x(W+AB)$ |  | (1) |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=xW+xW_{adapted}=x(W+AB)$ |  | (1) |'
- en: where $W{\in\mathbb{R}^{m\times n}}$ is the low-rank update from multiplying
    $A{\in\mathbb{R}^{m\times r}}$ where $r<n$.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W{\in\mathbb{R}^{m\times n}}$ 是从乘法 $A{\in\mathbb{R}^{m\times r}}$ 其中 $r<n$
    得到的低秩更新。
- en: This work introduces CURLoRA, a novel approach that applies low-rank adaptation
    (LoRA) to pre-trained weight matrices using CUR matrix decomposition [[6](#bib.bib6)]
    instead of random initiation of the low-rank $A$ matrices. We propose a unique
    modification to the CUR decomposition process and demonstrate its effectiveness
    in mitigating catastrophic forgetting while also reducing the number of trainable
    parameters. While LoRA successfully reduces computational costs by decomposing
    weight updates into low-rank matrices, it still suffers from catastrophic forgetting.
    CURLoRA leverages CUR decomposition with inverted probabilities and initiating
    $U$ matrix as zero to further mitigate this issue.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作引入了CURLoRA，这是一种新颖的方法，通过使用 CUR 矩阵分解 [[6](#bib.bib6)] 对预训练权重矩阵应用低秩适应 (LoRA)，而不是随机初始化低秩
    $A$ 矩阵。我们提出了 CUR 分解过程中的独特修改，并展示了其在减轻灾难性遗忘的同时减少可训练参数数量的有效性。虽然 LoRA 通过将权重更新分解为低秩矩阵成功减少了计算成本，但它仍然面临灾难性遗忘的问题。CURLoRA
    利用 CUR 分解与反转概率，并将 $U$ 矩阵初始化为零，以进一步缓解这一问题。
- en: 2 Related Work
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Catastrophic Forgetting
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 **灾难性遗忘**
- en: 'Catastrophic forgetting is a big challenge in machine learning, particularly
    in the context of continual learning [[5](#bib.bib5)]. Various approaches have
    been proposed to address this issue:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**灾难性遗忘** 是机器学习中的一个重大挑战，特别是在持续学习的背景下 [[5](#bib.bib5)]。为解决此问题，已提出各种方法：'
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Elastic Weight Consolidation (EWC) [[7](#bib.bib7)] uses Fisher information
    to measure the importance of parameters and selectively slow down learning on
    important parameters.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**弹性权重固化** (EWC) [[7](#bib.bib7)] 使用费舍尔信息来衡量参数的重要性，并选择性地减慢对重要参数的学习。'
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Progressive Neural Networks [[8](#bib.bib8)] propose to freeze the network trained
    on previous tasks and add lateral connections to new columns for new tasks.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**渐进神经网络** [[8](#bib.bib8)] 提出将已经在先前任务上训练好的网络冻结，并为新任务添加横向连接。'
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memory-based approaches like Experience Replay [[9](#bib.bib9)] store and replay
    examples from previous tasks during training on new tasks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于记忆的方法如**经验回放** [[9](#bib.bib9)] 在新任务训练期间存储和回放来自先前任务的示例。
- en: 2.2 Efficient Fine-tuning of Large Language Models
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大规模语言模型的高效微调
- en: 'As LLMs have grown in size, efficient fine-tuning methods have become crucial:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模语言模型的增大，**高效微调方法** 已变得至关重要：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapter layers [[10](#bib.bib10)] introduce small trainable modules between
    layers of a pre-trained model.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**适配器层** [[10](#bib.bib10)] 在预训练模型的层之间引入小型可训练模块。'
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Low-Rank Adaptation (LoRA) [[4](#bib.bib4)] decomposes weight updates into low-rank
    matrices, significantly reducing the number of trainable parameters.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**低秩适应** (LoRA) [[4](#bib.bib4)] 将权重更新分解为低秩矩阵，从而显著减少可训练参数的数量。'
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prefix-tuning [[11](#bib.bib11)] prepends trainable continuous prompts to the
    input, allowing for task-specific adaptations.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**前缀调优** [[11](#bib.bib11)] 将可训练的连续提示添加到输入中，从而实现任务特定的调整。'
- en: 2.3 CUR Matrix Decomposition
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 CUR 矩阵分解
- en: 'CUR decomposition has been applied in various domains for its interpretability
    and efficiency:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CUR 分解已在各个领域应用，因其可解释性和效率：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In data analysis, CUR has been used for feature selection and dimensionality
    reduction [[6](#bib.bib6)].
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在数据分析中，CUR 已被用于特征选择和维度降低 [[6](#bib.bib6)]。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In scientific computing, CUR has been applied to accelerate large-scale matrix
    computations [[12](#bib.bib12)].
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在科学计算中，CUR 已被应用于加速大规模矩阵计算 [[12](#bib.bib12)]。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In machine learning, CUR has been explored for model compression and interpretation
    [[13](#bib.bib13)].
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在机器学习中，CUR 已被探讨用于模型压缩和解释 [[13](#bib.bib13)]。
- en: However, to the best of our knowledge, CUR decomposition has not been previously
    applied to the problem of fine-tuning large language models or addressing catastrophic
    forgetting in this context.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，据我们所知，CUR 分解尚未在大规模语言模型的微调问题或在此背景下解决灾难性遗忘方面应用过。
- en: 3 Background on CUR Decomposition
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CUR 分解的背景
- en: CUR decomposition is a matrix factorization technique that approximates a matrix
    $A$, $U$. Unlike Singular Value Decomposition (SVD), CUR decomposition uses actual
    columns and rows from the original matrix, making it more interpretable.[[6](#bib.bib6)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CUR 分解是一种矩阵分解技术，它通过实际使用原始矩阵的列和行来近似矩阵 $A$，$U$。与奇异值分解 (SVD) 不同，CUR 分解使其更具可解释性。[[6](#bib.bib6)]
- en: 'Given a matrix $A\in\mathbb{R}^{m\times n}$ as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个矩阵$A\in\mathbb{R}^{m\times n}$为：
- en: '|  | $A\approx CUR$ |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $A\approx CUR$ |  | (2) |'
- en: 'where:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $C\in\mathbb{R}^{m\times c}$ columns of $A$
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $C\in\mathbb{R}^{m\times c}$ 为$A$的列
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $R\in\mathbb{R}^{r\times n}$ rows of $A$
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $R\in\mathbb{R}^{r\times n}$ 为$A$的行
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $U\in\mathbb{R}^{c\times r}$ is close to $A$
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $U\in\mathbb{R}^{c\times r}$ 接近$A$
- en: The columns and rows are typically chosen based on their statistical leverage
    scores.[[12](#bib.bib12)] Leverage scores indicate the importance of columns and
    rows in representing the original matrix. High leverage scores identify influential
    columns and rows, while low scores identify less critical ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列和行通常根据其统计杠杆分数进行选择。[12](#bib.bib12)杠杆分数指示列和行在表示原始矩阵中的重要性。高杠杆分数识别出有影响力的列和行，而低分数识别出不太重要的列和行。
- en: 4 This Work
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 本工作
- en: In this section, we present CURLoRA, our novel approach to fine-tuning large
    language models that leverages a modified CUR matrix decomposition to mitigate
    catastrophic forgetting. We provide a detailed mathematical formulation of the
    approach, analyze it theoretically, and explain how it addresses the challenge
    of catastrophic forgetting upon continual learning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了CURLoRA，这是一种新颖的微调大型语言模型的方法，利用修改过的CUR矩阵分解来减轻灾难性遗忘。我们提供了该方法的详细数学公式，对其进行了理论分析，并解释了它如何解决持续学习中的灾难性遗忘问题。
- en: 4.1 CURLoRA
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 CURLoRA
- en: The core idea is to decompose the pre-trained weight matrices using a modified
    CUR approach and then fine-tune only the U matrix. This approach constrains the
    parameter space of possible adaptations keeping the fine-tuned parameters as small
    as possible to keep $\|W_{\text{adapted}}-W\|_{F}$) i.e. $W+W_{\text{adapted}}$
    to avoid the deviation of the adapted output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是使用修改过的CUR方法对预训练的权重矩阵进行分解，然后仅微调U矩阵。这种方法约束了可能适应的参数空间，将微调参数保持在尽可能小的范围内，以保持$\|W_{\text{adapted}}-W\|_{F}$即$W+W_{\text{adapted}}$，以避免适应输出的偏差。
- en: 4.2 Mathematical Formulation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数学公式
- en: 'Given a weight matrix $W\in\mathbb{R}^{m\times n}$, we first compute the probability
    of each column:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个权重矩阵$W\in\mathbb{R}^{m\times n}$，我们首先计算每列的概率：
- en: '|  | $p_{j}=\frac{\&#124;W_{:j}\&#124;_{2}^{2}}{\&#124;W\&#124;_{F}^{2}}$ |  |
    (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{j}=\frac{\|W_{:j}\|_{2}^{2}}{\|W\|_{F}^{2}}$ |  | (3) |'
- en: where $W_{:j}$-th column of $W$ denotes the square of the L2 norm of the column
    and $\|\cdot\|_{F}^{2}$. This will give us the probability of each column. For
    instance, if $W$ has three columns with norms 2, 3, and 5, the probabilities are
    4/38, 9/38, and 25/38 respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$W_{:j}$-th列表示列的L2范数的平方和$\|\cdot\|_{F}^{2}$。这将给出每列的概率。例如，如果$W$有三列，其范数分别为2、3和5，那么概率分别为4/38、9/38和25/38。
- en: 'We then invert these probabilities:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们反转这些概率：
- en: '|  | $\tilde{p}_{j}=\frac{1/p_{j}}{\sum_{i=1}^{n}1/p_{i}}$ |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{p}_{j}=\frac{1/p_{j}}{\sum_{i=1}^{n}1/p_{i}}$ |  | (4) |'
- en: where $\tilde{p}_{j}$-th column of $W$. The same steps are followed for rows.
    Inverted probabilities are used to sample columns and rows with lower leverage
    scores, which implicitly regularize the model and limit the magnitude of fine-tuning
    adjustments.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\tilde{p}_{j}$-th列的$W$。对行执行相同的步骤。使用反转概率来抽样具有较低杠杆分数的列和行，这在隐含上对模型进行正则化，并限制微调调整的幅度。
- en: 'Then, we sample $r$, according to these inverted probabilities to construct
    $C$, which will always be fixed, with columns and rows with lower original probabilities.
    This trick plays a major role in the approach as it serves two purposes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们根据这些反转概率抽样$r$，以构造$C$，$C$将始终固定，由具有较低原始概率的列和行组成。这一技巧在方法中发挥了重要作用，因为它有两个目的：
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It acts as a form of regularization, preventing the model from overfitting or
    moving too much towards the task and limiting the adaptation of the $U$ matrix
    stopping it from growing so big in magnitude.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它作为一种正则化形式，防止模型过拟合或过度偏向任务，并限制$U$矩阵的适应，防止其在幅度上增长过大。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It preserves the model’s original behavior by focusing adaptations on less influential
    parts of the weight matrix. In addition, since $C$ contain actual columns and
    rows from the original matrix, they contribute to the stability of the fine-tuning
    process.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将适应集中在权重矩阵的影响较小的部分，来保持模型的原始行为。此外，由于$C$包含原始矩阵的实际列和行，它们有助于稳定微调过程。
- en: CURLoRA’s approach differs significantly from other initialization methods.
    Unlike LoRA’s random initialization using Kaiming-uniform or Gaussian for weight
    A and zeros for weight B [[4](#bib.bib4)], or the SVD-based initialization [[14](#bib.bib14)],
    CURLoRA offers more controlled adaptation. While these other methods ensure starting
    from the base model, they don’t inherently limit the growth of the adaptation
    matrix (AB in LoRA), potentially leading to significant deviations during training.
    In contrast, CURLoRA initializes the U matrix as zeros, and importantly, constructs
    C and R matrices using columns and rows with lower original probabilities (i.e.,
    lower values). This unique combination ensures that the fine-tuning process not
    only starts from the base configuration but also remains constrained throughout
    training. The low-value C and R matrices act as natural limiters on the growth
    of U, thereby preventing large deviations and contributing to enhanced model stability
    during the fine-tuning process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CURLoRA 的方法与其他初始化方法显著不同。不同于 LoRA 使用 Kaiming-uniform 或高斯分布初始化权重 A 和对权重 B 使用零
    [[4](#bib.bib4)]，或基于 SVD 的初始化 [[14](#bib.bib14)]，CURLoRA 提供了更受控制的适配。虽然这些其他方法确保从基本模型开始，但它们本质上不限制适配矩阵（LoRA
    中的 AB）的增长，可能导致训练过程中出现显著偏差。相比之下，CURLoRA 将 $U$ 矩阵初始化为零，并且重要的是，使用具有较低原始概率（即，较低值）的列和行构造
    $C$ 和 $R$ 矩阵。这种独特的组合确保了微调过程不仅从基础配置开始，而且在整个训练过程中保持约束。低值的 $C$ 和 $R$ 矩阵作为 $U$ 增长的自然限制器，从而防止了大的偏差，并有助于在微调过程中提高模型的稳定性。
- en: '|  | $C=\text{SampleColumns}(W,r,\tilde{p})$ |  | (5) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=\text{SampleColumns}(W,r,\tilde{p})$ |  | (5) |'
- en: '|  | $R=\text{SampleRows}(W,r,\tilde{p})$ |  | (6) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $R=\text{SampleRows}(W,r,\tilde{p})$ |  | (6) |'
- en: '|  | $U_{\text{init}}=0$ |  | (7) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{\text{init}}=0$ |  | (7) |'
- en: Where $W$ is the rank (number of columns/rows to sample) and $\tilde{p}$ represents
    the inverted probabilities used for sampling.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 是秩（采样的列/行数），$\tilde{p}$ 表示用于采样的倒数概率。
- en: 'During fine-tuning, we update only the $U$ and $R$:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，我们仅更新 $U$ 和 $R$：
- en: '|  | $W_{\text{adapted}}=CUR$ |  | (8) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $W_{\text{adapted}}=CUR$ |  | (8) |'
- en: 4.3 Theoretical Analysis of Catastrophic Forgetting Mitigation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 理论分析灾难性遗忘的缓解
- en: 'To understand how CURLoRA helps mitigate catastrophic forgetting, we analyze
    its properties mathematically:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 CURLoRA 如何帮助减轻灾难性遗忘，我们从数学上分析其属性：
- en: 4.3.1 Parameter Space Constraint
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 参数空间约束
- en: 'In CURLoRA, we decompose the original weight matrix $W$ as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CURLoRA 中，我们将原始权重矩阵 $W$ 分解为：
- en: '|  | $W\approx CUR$ |  | (9) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $W\approx CUR$ |  | (9) |'
- en: 'During fine-tuning, we’re optimizing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，我们正在优化：
- en: '|  | $W_{\text{adapted}}=C(U+\Delta U)R$ |  | (10) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $W_{\text{adapted}}=C(U+\Delta U)R$ |  | (10) |'
- en: where $\Delta U$ during fine-tuning. By constraining the updates to the subspace
    defined by $C$, CURLoRA limits drastic changes, thereby preserving the model’s
    original knowledge.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，$\Delta U$ 的变化通过将更新限制在由 $C$ 定义的子空间内来限制剧烈变化，从而保持模型的原始知识。
- en: 4.3.2 Implicit Regularization
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 隐式正则化
- en: 'By initializing $U$ and $R$ and $R$. This can be seen as adding a regularization
    term to the loss function quantified by the norm of the matrix $U$ that is aimed
    to be kept small:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过初始化 $U$ 和 $R$。这可以看作是向损失函数中添加一个正则化项，该项由 $U$ 矩阵的范数量化，旨在保持较小：
- en: '|  | $L_{\text{CURLoRA}}(\theta)=L_{\text{task}}(\theta)+\&#124;U\&#124;_{F}$
    |  | (11) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{CURLoRA}}(\theta)=L_{\text{task}}(\theta)+\&#124;U\&#124;_{F}$
    |  | (11) |'
- en: where $\|U\|_{F}$ matrix that is being fine-tuned. This implicit regularization
    term encourages the model to keep the changes small. For instance, if $U$ is initially
    zero, this term will push the fine-tuning process to make only necessary adjustments,
    preventing overfitting and excessive reliance on the fine-tuned parameters.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\|U\|_{F}$ 矩阵正在进行微调。这一隐式正则化项鼓励模型保持变化较小。例如，如果 $U$ 最初为零，则该项将推动微调过程仅进行必要的调整，从而防止过拟合和过度依赖微调参数。
- en: 4.3.3 Reduced Interference
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 减少干扰
- en: 'During fine-tuning, $W$, which is itself updated through $U$ and $R$ with respect
    to the parameters, we can, in a simple way, express the gradient of the loss with
    respect to $W_{\text{adapted}}$ as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，$W$ 通过 $U$ 和 $R$ 以参数为依据进行更新，我们可以简单地将关于 $W_{\text{adapted}}$ 的损失梯度表示如下：
- en: '|  | $\frac{\partial L}{\partial W_{\text{adapted}}}=C\left(\frac{\partial
    L}{\partial U}\right)R$ |  | (12) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial W_{\text{adapted}}}=C\left(\frac{\partial
    L}{\partial U}\right)R$ |  | (12) |'
- en: This means that the gradient of the loss with respect to $W_{\text{adapted}}$
    scaled by the fixed matrices $C$. By projecting the gradients onto the subspace
    defined by $C$, the updates to $W_{\text{adapted}}$ are constrained. This means
    that changes during fine-tuning are less likely to interfere with the model’s
    ability to perform the original task, potentially reducing interference with directions
    important for the original task.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着损失关于 $W_{\text{adapted}}$ 的梯度被固定矩阵 $C$ 缩放。通过将梯度投影到 $C$ 定义的子空间上，对 $W_{\text{adapted}}$
    的更新受到约束。这意味着微调过程中变化不太可能干扰模型执行原始任务的能力，可能减少对原始任务重要方向的干扰。
- en: 4.3.4 Reduced Degree of Freedom
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 自由度减少
- en: 'If $W\in\mathbb{R}^{m\times n}$ adaptation, then:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $W\in\mathbb{R}^{m\times n}$ 适应，则：
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Full fine-tuning has $mn$ degrees of freedom
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整微调具有 $mn$ 个自由度
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LoRA has $k(m+n)$ degrees of freedom
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoRA 具有 $k(m+n)$ 个自由度
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CURLoRA has only $k^{2}$ degrees of freedom
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA 只有 $k^{2}$ 个自由度
- en: This significant reduction in degrees of freedom inherently limits how far the
    model can stray from its original configuration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自由度的显著减少本质上限制了模型偏离其原始配置的程度。
- en: 4.3.5 Stability Analysis
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 稳定性分析
- en: 'We can analyze the stability of the adapted and fine-tuned weights and how
    its change is bounded using the fact that the change that happens to original
    $W$:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用原始 $W$ 的变化来分析适应和微调后的权重的稳定性及其变化的界限：
- en: '|  | $\Delta W=W_{\text{fine-tuned}}-W=W+W_{\text{adapted}}-W=W_{\text{adapted}}$
    |  | (13) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta W=W_{\text{fine-tuned}}-W=W+W_{\text{adapted}}-W=W_{\text{adapted}}$
    |  | (13) |'
- en: 'To quantify this change, we can use the Frobenius norm, $\|W_{\text{adapted}}\|_{F}$
    is controlled through the norms of $C$, and $R$:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化这种变化，我们可以使用 Frobenius 范数，$\|W_{\text{adapted}}\|_{F}$ 通过 $C$ 和 $R$ 的范数来控制：
- en: '|  | $\&#124;W_{\text{adapted}}\&#124;_{F}=\&#124;CUR\&#124;_{F}\leq\&#124;C\&#124;_{F}\&#124;U\&#124;_{F}\&#124;R\&#124;_{F}$
    |  | (14) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\|W_{\text{adapted}}\|_{F}=\|CUR\|_{F}\leq\|C\|_{F}\|U\|_{F}\|R\|_{F}$
    |  | (14) |'
- en: This equation ensures that the Frobenius norm of the adapted weight matrix $W_{\text{adapted}}$
    and $R$ starts at zero, the fine-tuning process focuses on minimizing $W_{\text{adapted}}$.
    As a result, the adaptation remains stable and the model preserves its original
    knowledge while allowing for necessary adjustments.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程确保了适应后的权重矩阵 $W_{\text{adapted}}$ 和 $R$ 的 Frobenius 范数从零开始，微调过程集中在最小化 $W_{\text{adapted}}$。因此，适应保持稳定，模型在进行必要调整的同时保持其原始知识。
- en: Empirical results (see Section 7) demonstrate that the Frobenius norm of $W_{\text{adapted}}$
    remains bounded across multiple tasts, validating the theoretical stability analysis.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实证结果（见第 7 节）证明了 $W_{\text{adapted}}$ 的 Frobenius 范数在多个任务中保持有界，验证了理论稳定性分析。
- en: 4.4 Theoretical Analysis of Output Shift
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 输出变化的理论分析
- en: To understand why CURLoRA is expected to perform better than standard LoRA in
    terms of catastrophic forgetting, we can analyze the shift in the output during
    fine-tuning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么 CURLoRA 预计在灾难性遗忘方面表现优于标准的 LoRA，我们可以分析微调过程中输出的变化。
- en: 'For a given input $x$. After fine-tuning:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入 $x$。微调后：
- en: 'For LoRA: $y_{\text{adapted}}=x(W+AB)$'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 LoRA: $y_{\text{adapted}}=x(W+AB)$'
- en: 'For CURLoRA: $y_{\text{adapted}}=x(W+CUR)$'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 CURLoRA: $y_{\text{adapted}}=x(W+CUR)$'
- en: 'We can quantify the shift using the Frobenius norm of the difference:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过差异的 Frobenius 范数来量化变化：
- en: '|  | $1$2 |  | (15) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: 'For LoRA: $\|x(AB)\|_{F}$'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 LoRA: $\|x(AB)\|_{F}$'
- en: 'For CURLoRA: $\|x(CUR)\|_{F}$'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 CURLoRA: $\|x(CUR)\|_{F}$'
- en: This equation measures the shift in the model’s output after fine-tuning. $y$
    is the output after fine-tuning. After fine-tuning for a different task, the adapted
    output $y_{\text{adapted}}$ i.e. to make sure the shift isn’t so big, we need
    to keep $W_{\text{adapted}}$ as small (in magnitude or size) as possible.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程测量了微调后模型输出的变化。$y$ 是微调后的输出。为了确保适应后的输出 $y_{\text{adapted}}$ 不会偏差太大，我们需要将 $W_{\text{adapted}}$
    尽可能保持较小（在幅度或大小上）。
- en: CURLoRA’s main aim is to minimize $W_{\text{adapted}}$ remains close to $\|W\|_{F}$,
    CURLoRA effectively controls the shift in the output, thereby preserving the model’s
    original behavior and mitigating catastrophic forgetting.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CURLoRA 的主要目标是最小化 $W_{\text{adapted}}$ 保持接近 $\|W\|_{F}$，CURLoRA 有效地控制了输出的变化，从而保持模型的原始行为并减轻灾难性遗忘。
- en: 'Theoretically, CURLoRA should result in a smaller shift because:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，CURLoRA 应该导致更小的变化，因为：
- en: '1.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The $C$ matrices are directly sampled from $W$, maintaining some structure of
    the original matrix.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $C$ 矩阵直接从 $W$ 中采样，保持原矩阵的一些结构。
- en: '2.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The $C$ matrices are sampled from columns and rows with lower values.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $C$ 矩阵从值较低的列和行中采样。
- en: '3.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Only $U$ and $R$.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有 $U$ 和 $R$。
- en: '4.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The initialization of $U$ as a zero matrix.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 $U$ 初始化为零矩阵。
- en: This constrained adaptation in CURLoRA is expected to lead to better preservation
    of the model’s original knowledge, thereby reducing catastrophic forgetting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CURLoRA 中，这种受限的适应预计将更好地保留模型的原始知识，从而减少灾难性遗忘。
- en: 4.5 Memory Efficiency
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 内存效率
- en: 'CURLoRA offers significant memory savings compared to full fine-tuning and
    even LoRA. For a weight matrix $W\in\mathbb{R}^{m\times n}$ where $r<n$, is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于全面微调甚至 LoRA，CURLoRA 提供了显著的内存节省。对于权重矩阵 $W\in\mathbb{R}^{m\times n}$，其中 $r<n$，是：
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Full fine-tuning: $mn$'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全面微调：$mn$
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LoRA (rank $r$
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoRA（秩 $r$
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CURLoRA (rank $r$
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA（秩 $r$
- en: 'The memory savings can be substantial, especially for large matrices. In our
    Mistral experiment, with rank 16, the trainable parameters were:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 内存节省可能很大，特别是对于大型矩阵。在我们的 Mistral 实验中，秩 16 的可训练参数为：
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Full fine-tuning: 7,248,023,552 parameters'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全面微调：7,248,023,552 参数
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LoRA: 9,437,184 parameters'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LoRA: 9,437,184 参数'
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CURLoRA: 24,576 parameters'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CURLoRA: 24,576 参数'
- en: This reduction in trainable parameters not only saves memory but also potentially
    leads to faster training and inference times.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可训练参数的减少不仅节省了内存，还有可能导致更快的训练和推理时间。
- en: 'In conclusion, CURLoRA provides multiple mathematical mechanisms that can help
    mitigate catastrophic forgetting:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，CURLoRA 提供了多种数学机制，有助于缓解灾难性遗忘：
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It constrains the parameter space of possible adaptations.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它约束了可能适应的参数空间。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It provides implicit regularization towards the original weights.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它提供了对原始权重的隐式正则化。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It preserves important directions from the original weight matrix.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它保留了原始权重矩阵中的重要方向。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It reduces the degrees of freedom in adaptation, limiting potential deviation.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它减少了适应中的自由度，限制了潜在的偏差。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It allows for direct control and analysis of weight stability through the $U$
    matrix.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它允许通过 $U$ 矩阵直接控制和分析权重稳定性。
- en: These properties suggest that CURLoRA can indeed help in reducing catastrophic
    forgetting while still allowing for meaningful and good adaptation to new tasks.
    The effectiveness of these theoretical mechanisms are validated through our experiments
    on various tasks and datasets, as detailed in the following sections.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性表明，CURLoRA 确实有助于减少灾难性遗忘，同时仍允许有效且良好的适应新任务。这些理论机制的有效性通过我们在各种任务和数据集上的实验得到了验证，如以下部分所述。
- en: 5 Methodology
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 方法论
- en: 5.1 CURLoRA Implementation
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 CURLoRA 实现
- en: 'Our CURLoRA implementation consists of the following steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CURLoRA 实现包括以下步骤：
- en: '1.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Decomposition: For each weight matrix $W$ in the layers we want to apply CURLoRA
    to, we perform the following:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分解：对于我们想要应用 CURLoRA 的每个权重矩阵 $W$，我们执行以下操作：
- en: •
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compute column probabilities: $p_{j}=\frac{\|W_{:j}\|_{2}^{2}}{\|W\|_{F}^{2}}$'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算列概率：$p_{j}=\frac{\|W_{:j}\|_{2}^{2}}{\|W\|_{F}^{2}}$
- en: •
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Invert probabilities: $\tilde{p}_{j}=\frac{1/p_{j}}{\sum_{i=1}^{n}1/p_{i}}$'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反转概率：$\tilde{p}_{j}=\frac{1/p_{j}}{\sum_{i=1}^{n}1/p_{i}}$
- en: •
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sample columns and rows according to $\tilde{p}_{j}$ and $R$
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 $\tilde{p}_{j}$ 和 $R$ 采样列和行
- en: •
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Initialize $U$ as a zero matrix
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 $U$ 初始化为零矩阵
- en: '2.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Fine-tuning:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：
- en: •
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Objective:'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：
- en: –
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The primary objective of the experiment is to evaluate catastrophic forgetting
    during continual learning, rather than to optimize accuracy for each individual
    task.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验的主要目标是评估持续学习中的灾难性遗忘，而不是优化每个单独任务的准确性。
- en: •
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Specific Adjustments:'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型特定调整：
- en: –
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For GPT-2 and Mistral, the model’s "lm_head" is replaced with a task-specific
    output layer. During training, only the $U$ and $R$ remain fixed.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 GPT-2 和 Mistral，模型的“lm_head”被替换为特定任务的输出层。在训练过程中，只有 $U$ 和 $R$ 保持固定。
- en: –
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Replacing the "lm_head" ensures that each task has its own task-specific output
    layer that remains untouched when the model is being fine-tuned on a different
    task, contributing to the mitigation of task knowledge degradation.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 替换“lm_head”确保每个任务都有自己的任务特定输出层，这些层在模型被微调到不同任务时保持不变，有助于缓解任务知识退化。
- en: •
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Continual Learning Strategy:'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续学习策略：
- en: –
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Once a weight matrix is decomposed, $C$ are fixed permanently. The $U$ matrix
    is continually updated for each new task to facilitate continual learning.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦权重矩阵被分解，$C$ 会被永久固定。$U$ 矩阵会为每个新任务不断更新，以促进持续学习。
- en: •
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Application of CURLoRA:'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA 的应用：
- en: –
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: CURLoRA is applied to the attention layers (Query, Key, Value). [[15](#bib.bib15)]
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA 应用于注意力层（Query、Key、Value）。 [[15](#bib.bib15)]
- en: '3.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Inference: Use the adapted weight matrix $W_{\text{adapted}}=CUR$ matrix i.e.
    $x(W+CUR)$.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断：使用调整后的权重矩阵 $W_{\text{adapted}}=CUR$ 矩阵，即 $x(W+CUR)$。
- en: 6 Experiment Setup
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验设置
- en: 6.1 Datasets
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据集
- en: 'We used the following datasets for our experiments:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用了以下数据集：
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GLUE-MRPC: Microsoft Research Paraphrase Corpus for paraphrase detection [[16](#bib.bib16)]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GLUE-MRPC：用于同义句检测的微软研究同义句语料库 [[16](#bib.bib16)]
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GLUE-SST-2: Stanford Sentiment Treebank for binary sentiment classification
    [[17](#bib.bib17)]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GLUE-SST-2：用于二分类情感分析的斯坦福情感树库 [[17](#bib.bib17)]
- en: These datasets are part of the General Language Understanding Evaluation (GLUE)
    benchmark [[18](#bib.bib18)], which includes a diverse set of tasks for evaluating
    natural language understanding systems.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些数据集是通用语言理解评估（GLUE）基准的一部分 [[18](#bib.bib18)]，包括一组多样化的任务，用于评估自然语言理解系统。
- en: •
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sentiment140: A large-scale sentiment analysis dataset [[19](#bib.bib19)]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sentiment140：一个大规模情感分析数据集 [[19](#bib.bib19)]
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'WikiText-2: A dataset that we use to measure language model perplexity [[20](#bib.bib20)]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WikiText-2：用于测量语言模型困惑度的数据集 [[20](#bib.bib20)]
- en: The datasets were selected for their diverse task requirements and common use
    in benchmarking.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是根据其多样化的任务需求和在基准测试中的常见使用而选择的。
- en: 6.2 Model and Hyperparameters
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 模型和超参数
- en: 'We used Mistral 7B (v0.3) [[21](#bib.bib21)] and GPT-2 Large [[22](#bib.bib22)]
    as our base models. For both LoRA and CURLoRA, we used the following hyperparameters:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 Mistral 7B (v0.3) [[21](#bib.bib21)] 和 GPT-2 Large [[22](#bib.bib22)]
    作为我们的基础模型。对于 LoRA 和 CURLoRA，我们使用了以下超参数：
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ranks: [8, 16, 24]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 秩：[8, 16, 24]
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Alpha: 1'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Alpha：1
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimizer: AdamW'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化器：AdamW
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning rate: 2.5e-4'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率：2.5e-4
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scheduler: Cosine with 500 warmup steps'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调度器：余弦退火，500 步热身
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training epochs: 3'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练轮次：3
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch size:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：
- en: –
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Mistral: 8'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mistral：8
- en: –
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'GPT-2: 32'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-2：32
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Max length:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大长度：
- en: –
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Mistral: 512'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mistral：512
- en: –
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'GPT-2: 256'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-2：256
- en: 6.2.1 Notes on hyperparemeters and architecture
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 超参数和架构说明
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robustness and Regularization:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性和正则化：
- en: –
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: CURLoRA’s performance was evaluated across different ranks, demonstrating robustness
    to moderate changes. Optimal results can be achieved by fine-tuning other hyperparameters,
    such as the learning rate. Dropout was not utilized, as the objective was to observe
    the implicit regularization effects of CURLoRA without the influence of explicit
    regularization.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA 的性能在不同的秩上进行了评估，展示了对中等变化的鲁棒性。通过微调其他超参数（如学习率）可以获得最佳结果。未使用 Dropout，因为目标是观察
    CURLoRA 的隐式正则化效果，而不受显式正则化的影响。
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Constraints:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据约束：
- en: –
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For Mistral, each fine-tuning task was limited to 1000 records to simulate scenarios
    with limited data and resources for large models.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 Mistral，每个微调任务限制为 1000 条记录，以模拟大模型数据和资源有限的场景。
- en: –
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For GPT-2, the SST-2 fine-tuning task was limited to 5000 records due to resource
    constraints.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 GPT-2，SST-2 微调任务由于资源限制被限制在 5000 条记录。
- en: –
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For the sentiment analysis task, the Sentiment140 test dataset was used for
    training, while the train dataset was used for evaluation. This choice was made
    because the test dataset has three labels, whereas the train dataset has only
    two. This allowed for fine-tuning the models on a multi-class task rather than
    a binary one.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于情感分析任务，Sentiment140 测试数据集用于训练，而训练数据集用于评估。这是因为测试数据集有三个标签，而训练数据集只有两个。这使得可以对模型进行多类任务的微调，而不是二类任务。
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Specific Adjustments:'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定调整：
- en: –
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For the sentiment analysis task with GPT-2, due to the small size of the dataset
    used for fine-tuning, the number of epochs was adjusted to 5, and the learning
    rate scheduler was not used.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 GPT-2 的情感分析任务，由于用于微调的数据集较小，轮次调整为 5，并且未使用学习率调度器。
- en: 6.3 Evaluation Metrics
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 评估指标
- en: 'We used the following metrics for evaluation:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下指标进行评估：
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accuracy: For classification tasks (MRPC, SST-2, Sentiment140)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率：用于分类任务（MRPC、SST-2、Sentiment140）
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perplexity: For language modeling capability (WikiText-2)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 困惑度：用于语言建模能力（WikiText-2）
- en: 6.4 Experimental Procedure
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 实验程序
- en: 'Our experimental procedure was as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验程序如下：
- en: '1.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Measure initial perplexity of the base model on WikiText-2 concatenating the
    whole dataset into a single string.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测量基础模型在 WikiText-2 上的初始困惑度，将整个数据集拼接成一个字符串。
- en: '2.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Fine-tune on MRPC and evaluate.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 MRPC 上进行微调并评估。
- en: '3.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Fine-tune on SST-2 and evaluate, then re-evaluate on MRPC.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 SST-2 上进行微调并评估，然后重新评估 MRPC。
- en: '4.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Fine-tune on Sentiment140 and evaluate, then re-evaluate on MRPC and SST-2.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 Sentiment140 上进行微调并评估，然后重新评估 MRPC 和 SST-2。
- en: '5.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Re-calculate perplexity on WikiText-2.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重新计算 WikiText-2 的困惑度。
- en: This procedure was carried out for both LoRA and CURLoRA independently.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序对 LoRA 和 CURLoRA 独立进行。
- en: 7 Results and Discussion
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结果与讨论
- en: 'Tables [1](#S7.T1 "Table 1 ‣ 7 Results and Discussion ‣ CURLoRA: Stable LLM
    Continual Fine-Tuning and Catastrophic Forgetting Mitigation") and [2](#S7.T2
    "Table 2 ‣ 7 Results and Discussion ‣ CURLoRA: Stable LLM Continual Fine-Tuning
    and Catastrophic Forgetting Mitigation") present the results of our experiments
    comparing LoRA and CURLoRA across multiple tasks and evaluation metrics.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S7.T1 "表 1 ‣ 7 结果与讨论 ‣ CURLoRA：稳定的 LLM 连续微调与灾难性遗忘缓解") 和 [2](#S7.T2 "表
    2 ‣ 7 结果与讨论 ‣ CURLoRA：稳定的 LLM 连续微调与灾难性遗忘缓解") 展示了我们比较 LoRA 和 CURLoRA 在多个任务和评估指标上的实验结果。
- en: 'Table 1: Mistral Experimental Results: LoRA vs CURLoRA'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：Mistral 实验结果：LoRA 与 CURLoRA
- en: '| Metric | LoRA-8 | CURLoRA-8 | LoRA-16 | CURLoRA-16 | LoRA-24 | CURLoRA-24
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 评估指标 | LoRA-8 | CURLoRA-8 | LoRA-16 | CURLoRA-16 | LoRA-24 | CURLoRA-24 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Initial WikiText-2 Perplexity | 5.44 | 5.44 | 5.44 | 5.44 | 5.44 | 5.44 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 初始 WikiText-2 困惑度 | 5.44 | 5.44 | 5.44 | 5.44 | 5.44 | 5.44 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MRPC Accuracy (After MRPC) | 0.68 | 0.66 | 0.65 | 0.66 | 0.67 | 0.66 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 MRPC 后） | 0.68 | 0.66 | 0.65 | 0.66 | 0.67 | 0.66 |'
- en: '| SST-2 Accuracy (After SST-2) | 0.51 | 0.86 | 0.51 | 0.86 | 0.49 | 0.86 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 准确率（在 SST-2 后） | 0.51 | 0.86 | 0.51 | 0.86 | 0.49 | 0.86 |'
- en: '| MRPC Accuracy (After SST-2) | 0.68 | 0.66 | 0.32 | 0.66 | 0.68 | 0.66 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 SST-2 后） | 0.68 | 0.66 | 0.32 | 0.66 | 0.68 | 0.66 |'
- en: '| Sentiment140 Accuracy | 1.00 | 0.94 | 1.00 | 0.94 | 1.00 | 0.94 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Sentiment140 准确率 | 1.00 | 0.94 | 1.00 | 0.94 | 1.00 | 0.94 |'
- en: '| MRPC Accuracy (After Sentiment140) | 0.32 | 0.66 | 0.32 | 0.66 | 0.32 | 0.66
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 Sentiment140 后） | 0.32 | 0.66 | 0.32 | 0.66 | 0.32 | 0.66 |'
- en: '| SST-2 Accuracy (After Sentiment140) | 0.49 | 0.86 | 0.49 | 0.86 | 0.49 |
    0.86 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 准确率（在 Sentiment140 后） | 0.49 | 0.86 | 0.49 | 0.86 | 0.49 | 0.86 |'
- en: '| Final WikiText-2 Perplexity | 53896.68 | 5.44 | 65055.02 | 5.44 | 17049.72
    | 5.44 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 最终 WikiText-2 困惑度 | 53896.68 | 5.44 | 65055.02 | 5.44 | 17049.72 | 5.44 |'
- en: 'Table 2: GPT-2 Large Experimental Results: LoRA vs CURLoRA'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：GPT-2 大型实验结果：LoRA 与 CURLoRA
- en: '| Metric | LoRA-8 | CURLoRA-8 | LoRA-16 | CURLoRA-16 | LoRA-24 | CURLoRA-24
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 评估指标 | LoRA-8 | CURLoRA-8 | LoRA-16 | CURLoRA-16 | LoRA-24 | CURLoRA-24 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Initial WikiText-2 Perplexity | 28.25 | 28.25 | 28.25 | 28.25 | 28.25 | 28.25
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 初始 WikiText-2 困惑度 | 28.25 | 28.25 | 28.25 | 28.25 | 28.25 | 28.25 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MRPC Accuracy (After MRPC) | 0.79 | 0.70 | 0.81 | 0.70 | 0.83 | 0.70 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 MRPC 后） | 0.79 | 0.70 | 0.81 | 0.70 | 0.83 | 0.70 |'
- en: '| SST-2 Accuracy (After SST-2) | 0.94 | 0.76 | 0.93 | 0.79 | 0.92 | 0.86 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 准确率（在 SST-2 后） | 0.94 | 0.76 | 0.93 | 0.79 | 0.92 | 0.86 |'
- en: '| MRPC Accuracy (After SST-2) | 0.76 | 0.70 | 0.78 | 0.70 | 0.78 | 0.70 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 SST-2 后） | 0.76 | 0.70 | 0.78 | 0.70 | 0.78 | 0.70 |'
- en: '| Sentiment140 Accuracy | 0.92 | 0.99 | 0.86 | 0.99 | 0.93 | 0.93 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Sentiment140 准确率 | 0.92 | 0.99 | 0.86 | 0.99 | 0.93 | 0.93 |'
- en: '| MRPC Accuracy (After Sentiment140) | 0.49 | 0.70 | 0.73 | 0.70 | 0.49 | 0.70
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| MRPC 准确率（在 Sentiment140 后） | 0.49 | 0.70 | 0.73 | 0.70 | 0.49 | 0.70 |'
- en: '| SST-2 Accuracy (After Sentiment140) | 0.90 | 0.76 | 0.90 | 0.79 | 0.88 |
    0.87 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 准确率（在 Sentiment140 后） | 0.90 | 0.76 | 0.90 | 0.79 | 0.88 | 0.87 |'
- en: '| Final WikiText-2 Perplexity | 42.96 | 28.25 | 43.62 | 28.08 | 44.32 | 28.25
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 最终 WikiText-2 困惑度 | 42.96 | 28.25 | 43.62 | 28.08 | 44.32 | 28.25 |'
- en: 7.1 Performance Analysis
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 性能分析
- en: 7.1.1 Task-Specific Performance
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 任务特定性能
- en: CURLoRA consistently performed well on different tasks, showing high accuracy
    even after fine-tuning on subsequent tasks. This suggests that CURLoRA is more
    effective at preserving task-specific knowledge.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: CURLoRA 在不同任务上表现 consistently 优秀，即使在后续任务中进行微调后也表现出高准确率。这表明 CURLoRA 在保持任务特定知识方面更为有效。
- en: Based on the experiments, CURLoRA may require a slightly higher learning rate
    than LoRA to achieve comparable accuracy. This is due to the implicit regularization
    introduced by the $C$ matrices, which constrain the adaptation space of the $U$
    matrix. However, this same property makes CURLoRA more robust against overfitting,
    even at higher learning rates. In contrast, while LoRA might achieve good performance
    with lower learning rates, it can be more susceptible to overfitting when learning
    rates are substantially increased. This trade-off highlights CURLoRA’s potential
    for more stable and controlled fine-tuning, particularly in scenarios where aggressive
    learning rates might be necessary.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 根据实验，CURLoRA 可能需要略高于 LoRA 的学习率才能达到相当的准确度。这是由于 $C$ 矩阵引入的隐性正则化，这限制了 $U$ 矩阵的适应空间。然而，这种特性使得
    CURLoRA 即使在较高学习率下也能更具抗过拟合能力。相比之下，尽管 LoRA 在较低学习率下可能表现良好，但当学习率大幅增加时，它可能更容易出现过拟合。这种权衡突显了
    CURLoRA 在需要激进学习率的场景中，具有更稳定和可控的微调潜力。
- en: 7.1.2 Catastrophic Forgetting and Stability
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 灾难性遗忘与稳定性
- en: The stability of CURLoRA’s performance across tasks is particularly noteworthy.
    While (Mistra) LoRA-16’s accuracy, for example, on MRPC dropped from 0.6495 to
    0.32 after fine-tuning on other tasks, CURLoRA-16 (Mistral) maintained its accuracy
    at 0.66\. This demonstrates CURLoRA’s superior ability to mitigate catastrophic
    forgetting.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: CURLoRA 在各任务中的性能稳定性特别值得注意。例如，(Mistra) LoRA-16 在 MRPC 上的准确度在微调其他任务后从 0.6495 降至
    0.32，而 CURLoRA-16 (Mistral) 的准确度保持在 0.66。这表明 CURLoRA 在减轻灾难性遗忘方面的卓越能力。
- en: 7.1.3 General Language Modeling Capability
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3 一般语言建模能力
- en: The final perplexity scores on WikiText-2 provide strong evidence for CURLoRA’s
    effectiveness in preserving general language modeling capabilities. While all
    LoRA’s perplexity, in both Mistral and GPT2, increased dramatically, all CURLoRA
    models maintained the original perplexity, indicating no degradation in general
    language understanding.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: WikiText-2 上的最终困惑度分数为 CURLoRA 在保持一般语言建模能力方面的有效性提供了有力证据。尽管 LoRA 在 Mistral 和 GPT2
    上的困惑度大幅上升，但所有 CURLoRA 模型的困惑度保持不变，表明没有出现一般语言理解的退化。
- en: 7.2 Theoretical Insights
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 理论见解
- en: 'The experimental results align with our theoretical analysis:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果与我们的理论分析一致：
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter Space Constraint: The stability of CURLoRA’s performance across tasks
    supports our hypothesis that constraining adaptations to the subspace spanned
    by $C$ helps preserve original knowledge.'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数空间约束：CURLoRA 在各任务中的性能稳定性支持了我们的假设，即将适应限制在 $C$ 张成的子空间中有助于保持原始知识。
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Implicit Regularization: The maintained perplexity on WikiText-2 suggests that
    CURLoRA’s implicit regularization effectively prevents overfitting to specific
    tasks.'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐性正则化：WikiText-2 上保持的困惑度表明 CURLoRA 的隐性正则化有效地防止了对特定任务的过拟合。
- en: •
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reduced Interference: The consistent performance across tasks indicates that
    CURLoRA successfully reduces interference between task-specific adaptations.'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 减少干扰：跨任务的一致性能表明 CURLoRA 成功减少了任务特定适应之间的干扰。
- en: 7.3 Limitations and Future Work
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 局限性与未来工作
- en: 'While CURLoRA shows promising results, there are several areas for future research:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CURLoRA 展示了有前景的结果，但仍有几个未来研究的领域：
- en: •
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scalability: While CURLoRA shows promising results, its scalability to larger
    models needs further investigation. Further studies are needed to assess CURLoRA’s
    performance on larger models and more diverse tasks like instruction tuning and
    datasets.'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可扩展性：尽管 CURLoRA 展示了有前景的结果，但其在更大模型上的可扩展性仍需进一步研究。需要进一步研究 CURLoRA 在更大模型和更多样化任务（如指令调优和数据集）上的表现。
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computational Complexity: Conducting detailed analysis of time and space complexity
    compared to full fine-tuning and LoRA.'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算复杂性：与全面微调和 LoRA 相比，进行时间和空间复杂性的详细分析。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Implicit Regularization Limitation: Implicit regularization via zero initialization
    of $U$ has to be further studied especially in highly dynamic environments where
    more flexible adaptations are needed.'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐性正则化限制：$U$ 的零初始化所带来的隐性正则化需要进一步研究，特别是在需要更灵活适应的高度动态环境中。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Optimal Rank and Alpha Selection: Investigating methods for automatically selecting
    the optimal rank and alpha for CURLoRA could further improve performance.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最优秩和 Alpha 选择：研究自动选择 CURLoRA 的最优秩和 alpha 的方法可能进一步提升性能。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Combination with Other Techniques: Exploring the integration of CURLoRA with
    other continual learning techniques could yield even better results.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他技术的结合：探索CURLoRA与其他持续学习技术的融合，可能会取得更好的结果。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization Support: Exploring the implementation of CURLoRA on quantized
    model which may lead to QCURLoRA'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化支持：探索在量化模型上实现CURLoRA，可能会导致QCURLoRA。
- en: 8 Conclusion
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper introduced CURLoRA, a novel approach to fine-tuning large language
    models that leverages CUR matrix decomposition to mitigate catastrophic forgetting
    and improve computational efficiency. Through theoretical analysis and empirical
    experiments, we demonstrated that CURLoRA outperforms standard LoRA in maintaining
    model stability and performance across tasks while significantly reducing the
    number of trainable parameters.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了CURLoRA，这是一种新颖的大型语言模型微调方法，利用CUR矩阵分解来缓解灾难性遗忘并提高计算效率。通过理论分析和实证实验，我们证明了CURLoRA在保持模型稳定性和任务表现方面优于标准的LoRA，同时显著减少了可训练参数的数量。
- en: 'Key contributions of this work include:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的关键贡献包括：
- en: •
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel modification to CUR decomposition using inverted probabilities for column
    and row selection and initiating $U$ matrix as zeros. Sampling columns and rows
    based on inverted probabilities distinguishes CURLoRA from traditional CUR, offering
    better stability and performance.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对CUR分解的新颖修改，使用反转概率进行列和行选择，并将$U$矩阵初始化为零。基于反转概率采样列和行，使CURLoRA区别于传统CUR，提供了更好的稳定性和性能。
- en: •
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theoretical analysis of how CURLoRA addresses catastrophic forgetting.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA如何应对灾难性遗忘的理论分析。
- en: •
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Empirical evidence of CURLoRA’s effectiveness across multiple tasks and evaluation
    metrics with multiple models.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CURLoRA在多任务和多个模型评估指标上的有效性有实证证据。
- en: Our results suggest that CURLoRA is a promising approach for efficient and stable
    fine-tuning of large language models, particularly in scenarios with limited fine-tuning
    data. CURLoRA’s approach to mitigating catastrophic forgetting has broad implications
    for continual learning in NLP and beyond. Future research could explore its integration
    with other adaptation techniques to enhance model robustness
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，CURLoRA是大型语言模型高效稳定微调的有希望的方法，特别是在有限的微调数据场景中。CURLoRA缓解灾难性遗忘的方法对自然语言处理及其他领域的持续学习具有广泛的意义。未来的研究可以探索其与其他适应技术的整合，以增强模型的鲁棒性。
- en: References
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。神经信息处理系统进展, 33:1877–1901, 2020。'
- en: '[2] Xiang Lisa Li and Percy Liang. Efficient few-shot learning without prompts.
    arXiv preprint arXiv:2111.10952, 2021.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Xiang Lisa Li 和 Percy Liang. 无需提示的高效少样本学习。arXiv预印本arXiv:2111.10952, 2021。'
- en: '[3] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang.
    Parameter-efficient fine-tuning methods for pretrained language models: A critical
    review and assessment, 2023.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, 和 Fu Lee Wang. 预训练语言模型的参数高效微调方法：关键评审与评估,
    2023。'
- en: '[4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations, 2021.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适配。国际学习表征会议, 2021。'
- en: '[5] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist
    networks: The sequential learning problem. Psychology of learning and motivation,
    24:109–165, 1989.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Michael McCloskey 和 Neal J Cohen. 连接主义网络中的灾难性干扰：顺序学习问题。学习与动机心理学, 24:109–165,
    1989。'
- en: '[6] Michael W Mahoney and Petros Drineas. Cur matrix decompositions for improved
    data analysis. Proceedings of the National Academy of Sciences, 106(3):697–702,
    2009.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Michael W Mahoney 和 Petros Drineas. 改进数据分析的CUR矩阵分解。国家科学院院刊, 106(3):697–702,
    2009。'
- en: '[7] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
    Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
    et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the
    national academy of sciences, 114(13):3521–3526, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
    Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska
    等。克服神经网络中的灾难性遗忘。国家科学院学报，114(13):3521–3526，2017。'
- en: '[8] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
    Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive
    neural networks. In Proceedings of the 30th International Conference on Neural
    Information Processing Systems, pages 8154–8162, 2016.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
    Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, 和 Raia Hadsell。渐进神经网络。在第30届国际神经信息处理系统会议论文集，页码8154–8162，2016。'
- en: '[9] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
    Wayne. Experience replay for continual learning. Advances in Neural Information
    Processing Systems, 32, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, 和 Gregory
    Wayne。经验重放用于持续学习。神经信息处理系统进展，32，2019。'
- en: '[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient
    transfer learning for nlp. In International Conference on Machine Learning, pages
    2790–2799\. PMLR, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain Gelly。参数高效的迁移学习用于自然语言处理。在国际机器学习会议，页码2790–2799。PMLR，2019。'
- en: '[11] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. arXiv preprint arXiv:2101.00190, 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Xiang Lisa Li 和 Percy Liang。前缀调优：优化生成的连续提示。arXiv预印本 arXiv:2101.00190，2021。'
- en: '[12] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error
    cur matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881,
    2008.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Petros Drineas, Michael W Mahoney, 和 S Muthukrishnan。相对误差曲矩阵分解。SIAM矩阵分析与应用期刊，30(2):844–881，2008。'
- en: '[13] Nishant Yadav, Nicholas Monath, Manzil Zaheer, and Andrew McCallum. Efficient
    k-nn search with cross-encoders using adaptive multi-round cur decomposition,
    2023.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Nishant Yadav, Nicholas Monath, Manzil Zaheer, 和 Andrew McCallum。使用自适应多轮曲分解进行高效k-nn搜索，2023。'
- en: '[14] Klaudia Bałazy, Mohammadreza Banaei, Karl Aberer, and Jacek Tabor. Lora-xs:
    Low-rank adaptation with extremely small number of parameters. arXiv preprint
    arXiv:2405.17604, 2024.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Klaudia Bałazy, Mohammadreza Banaei, Karl Aberer, 和 Jacek Tabor。Lora-xs：具有极少参数的低秩适应。arXiv预印本
    arXiv:2405.17604，2024。'
- en: '[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need,
    2023.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin。注意力即一切，2023。'
- en: '[16] William B Dolan and Chris Brockett. Automatically constructing a corpus
    of sentential paraphrases. In Proceedings of the Third International Workshop
    on Paraphrasing (IWP2005), 2005.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] William B Dolan 和 Chris Brockett。自动构建句子同义句语料库。在第三届国际同义句研讨会（IWP2005）论文集，2005。'
- en: '[17] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic
    compositionality over a sentiment treebank. In Proceedings of the 2013 Conference
    on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle,
    Washington, USA, October 2013\. Association for Computational Linguistics.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Ng, 和 Christopher Potts。递归深度模型用于情感树库的语义组合性。在2013年自然语言处理经验方法会议论文集，页码1631–1642，华盛顿州西雅图，2013年10月。计算语言学协会。'
- en: '[18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
    Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural
    language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
    Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, 和 Samuel
    R Bowman。Glue：一个多任务基准和自然语言理解分析平台。在2018年EMNLP Workshop BlackboxNLP：分析和解释自然语言处理中的神经网络论文集，页码353–355，2018。'
- en: '[19] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification
    using distant supervision. In CS224N project report, Stanford, volume 1, page
    2009, 2009.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Alec Go, Richa Bhayani, 和 Lei Huang。使用远程监督进行Twitter情感分类。在CS224N项目报告，斯坦福，卷1，页码2009，2009。'
- en: '[20] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher。Pointer
    sentinel 组合模型。arXiv 预印本 arXiv:1609.07843，2016年。'
- en: '[21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等人。Mistral 7b。arXiv 预印本 arXiv:2310.06825，2023年。'
- en: '[22] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever 等人。语言模型是无监督的多任务学习者。OpenAI 博客，1(8):9，2019年。'
