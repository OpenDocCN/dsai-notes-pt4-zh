- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '动态稀疏无训练 \faBan: 无需训练的稀疏LLM微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.08915](https://ar5iv.labs.arxiv.org/html/2310.08915)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.08915](https://ar5iv.labs.arxiv.org/html/2310.08915)
- en: \xpretocmd\@makefntextYuxin Zhang^(1†)  Lirui Zhao^(1†)  Mingbao Lin¹  Yunyun
    Sun²  Yiwu Yao²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \xpretocmd\@makefntextYuxin Zhang^(1†)  Lirui Zhao^(1†)  Mingbao Lin¹  Yunyun
    Sun²  Yiwu Yao²
- en: Xingjia Han²  Jared Tanner³  Shiwei Liu^(4,5)  Rongrong Ji^(1,6‡)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xingjia Han²  Jared Tanner³  Shiwei Liu^(4,5)  Rongrong Ji^(1,6‡)
- en: ¹Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹多媒体可信感知与高效计算重点实验室
- en: Ministry of Education of China, Xiamen University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 中国教育部，厦门大学
- en: ²Huawei Technologies, ³University of Oxford, ⁴University of Texas at Austin
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²华为技术有限公司，³牛津大学，⁴德克萨斯大学奥斯汀分校
- en: '⁵Eindhoven University of Technology, ⁶Institute of Artificial Intelligence,
    Xiamen University †Equal contribution   ‡Corresponding author: rrji@xmu.edu.cn'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵埃因霍温理工大学，⁶厦门大学人工智能研究所 †同等贡献   ‡通讯作者：rrji@xmu.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The ever-increasing large language models (LLMs), though opening a potential
    path for the upcoming artificial general intelligence, sadly drops a daunting
    obstacle on the way towards their on-device deployment. As one of the most well-established
    pre-LLMs approaches in reducing model complexity, network pruning appears to lag
    behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training)
    necessity under the massive volumes of model parameter and training data. To close
    this industry-academia gap, we introduce Dynamic Sparse No Training (DS\faBanT¹¹1
    Pronounced “DS No T”.), a training-free fine-tuning approach that slightly updates
    sparse LLMs without the expensive backpropagation and any weight updates. Inspired
    by the Dynamic Sparse Training, DS\faBanT minimizes the reconstruction error between
    the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing
    on top of sparse LLMs. To accomplish this purpose, DS\faBanT particularly takes
    into account the anticipated reduction in reconstruction error for pruning and
    growing, as well as the variance *w.r.t*. different input data for growing each
    weight. This practice can be executed efficiently in linear time since its obviates
    the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2,
    Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DS\faBanT
    in enhancing the performance of sparse LLMs, especially at high sparsity levels.
    For instance, DS\faBanT is able to outperform the state-of-the-art Wanda by 26.79
    perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into
    how to fine-tune sparse LLMs in an efficient training-free manner and open new
    venues to scale the great potential of sparsity to LLMs. Codes are available at
    [https://github.com/zyxxmu/DSnoT](https://github.com/zyxxmu/DSnoT).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不断增长的大型语言模型（LLMs）为即将到来的人工通用智能开辟了潜在的道路，但却在其设备上的部署中设立了令人畏惧的障碍。作为减少模型复杂性的最成熟的前LLMs方法之一，网络剪枝在LLMs时代显得滞后，这主要是由于在庞大的模型参数和训练数据下，其代价高昂的微调（或再训练）需求。为了弥合这一行业与学术界的差距，我们引入了动态稀疏无训练（DS\faBanT¹¹1
    发音为“DS No T”），这是一种无训练的微调方法，可以在不进行昂贵的反向传播和权重更新的情况下略微更新稀疏LLMs。受到动态稀疏训练的启发，DS\faBanT最小化了稠密LLMs和稀疏LLMs之间的重构误差，以在稀疏LLMs上执行迭代的权重剪枝和增长。为了实现这一目的，DS\faBanT特别考虑了剪枝和增长的预期重构误差减少，以及增长每个权重的不同输入数据的方差*相对于*。由于它避免了微调LLMs所需的反向传播，这种实践可以在线性时间内高效执行。在LLaMA-V1/V2、Vicuna和OPT上的广泛实验展示了DS\faBanT在提高稀疏LLMs性能方面的有效性，特别是在高稀疏度水平下。例如，DS\faBanT在70%稀疏度下以LLaMA-7B超越了最先进的Wanda，减少了26.79的困惑度。我们的论文提供了关于如何以高效的无训练方式微调稀疏LLMs的新见解，并开辟了将稀疏性的巨大潜力扩展到LLMs的新途径。代码可在[https://github.com/zyxxmu/DSnoT](https://github.com/zyxxmu/DSnoT)获取。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) (Zhang et al., [2022a](#bib.bib63); Touvron et al.,
    [2023a](#bib.bib49); Brown et al., [2020](#bib.bib2)) have recently emerged as
    the new favorite in various domains of natural language processing (NLP) (Wei
    et al., [2022b](#bib.bib54); [a](#bib.bib53); Bubeck et al., [2023](#bib.bib3)).
    Nevertheless, LLMs face a significant constraint: their extensive parameterization
    and computational demands present substantial challenges in terms of storage and
    deployment. For example, the GPT-175B model (Brown et al., [2020](#bib.bib2))
    eats up 320G of memory to load its parameters in FP16 precision, requiring at
    least five A100-80G GPUs for inference (Frantar & Alistarh, [2023](#bib.bib13)).
    In response to this issue, there has been a surge of interest in compressing LLMs,
    as it holds the promise of LLMs while remarkably reducing memory usage and computational
    costs. To date, the majority of current effort for LLM compression falls into
    quantization (Yao et al., [2022](#bib.bib59); Lin et al., [2023](#bib.bib31);
    Frantar et al., [2022](#bib.bib14); Dettmers et al., [2023](#bib.bib9); [2022](#bib.bib8);
    Xiao et al., [2023](#bib.bib58)), which compresses LLMs by diminishing the number
    of bits employed to represent weights or hidden states.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（张等， [2022a](#bib.bib63)；Touvron等， [2023a](#bib.bib49)；Brown等， [2020](#bib.bib2)）最近成为自然语言处理（NLP）领域的新宠（Wei等，
    [2022b](#bib.bib54)；[a](#bib.bib53)；Bubeck等， [2023](#bib.bib3)）。然而，LLMs面临着一个重要的限制：其广泛的参数化和计算需求在存储和部署方面提出了巨大的挑战。例如，GPT-175B模型（Brown等，
    [2020](#bib.bib2)）在FP16精度下加载其参数需要320G的内存，推理时至少需要五个A100-80G GPU（Frantar & Alistarh，
    [2023](#bib.bib13)）。对此，压缩LLMs的兴趣激增，因为它有可能在显著减少内存使用和计算成本的同时保持LLMs的优势。迄今为止，当前大多数LLM压缩工作都集中在量化（Yao等，
    [2022](#bib.bib59)；Lin等， [2023](#bib.bib31)；Frantar等， [2022](#bib.bib14)；Dettmers等，
    [2023](#bib.bib9)；[2022](#bib.bib8)；Xiao等， [2023](#bib.bib58)），通过减少表示权重或隐藏状态所用的位数来压缩LLMs。
- en: On the other hand, network pruning (LeCun et al., [1989](#bib.bib28); Han et al.,
    [2015](#bib.bib18); Mocanu et al., [2018](#bib.bib39)), a technique that removes
    superfluous weights to create a sparse and lightweight model, has received relatively
    little attention (Frantar & Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)).
    The plausible reason is that, network pruning usually appreciates at least one,
    usually many, iterations of fine-tuning or re-training to guarantee top performance (Frankle
    & Carbin, [2019](#bib.bib12); Yin et al., [2023](#bib.bib60)). This fine-tuning
    step would cause a significant amount of compute and memory footprints due to
    the colossal model size and massive training data of modern LLMs, which even unnerves
    large corporations, let alone individual researchers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，网络剪枝（LeCun等， [1989](#bib.bib28)；Han等， [2015](#bib.bib18)；Mocanu等， [2018](#bib.bib39)），一种通过去除多余权重以创建稀疏且轻量级模型的技术，受到了相对较少的关注（Frantar
    & Alistarh， [2023](#bib.bib13)；Sun等， [2023](#bib.bib48)）。一个可能的原因是，网络剪枝通常需要至少一次，通常是多次的微调或重新训练以确保最佳性能（Frankle
    & Carbin， [2019](#bib.bib12)；Yin等， [2023](#bib.bib60)）。由于现代LLMs的庞大模型规模和大量训练数据，这一微调步骤会导致显著的计算和内存占用，甚至让大型企业感到不安，更不用说个人研究人员了。
- en: Two previous arts have explored the possibility to scale pruning to billion-level
    LLMs without any fine-tuning. SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))
    formulates LLM pruning as a layer-wise weight reconstruction problem, where the
    target falls into mitigating the output discrepancy, *w.r.t.*, reconstruction
    error, between dense and sparse LLMs. To solve the row-Hessian challenge, *i.e.*,
    the need for calculating the expensive inversion of a huge matrix for each row
    individually, SparseGPT iteratively applies OBS (Hassibi et al., [1993](#bib.bib20))
    to individually prune and updates weights in a column-wise manner, ultimately
    reaching the same optimal solution as applying the closed-form regression reconstruction.
    Wanda (Sun et al., [2023](#bib.bib48)) proposes a new pruning metric that takes
    both weight magnitude and their corresponding input activations into consideration,
    performing on part with SparseGPT without the need for the expensive second-order
    information. The intuition behind Wanda lies in the existence of emergent outlier
    feature dimensions in large-scale LLMs which are significantly larger than typical
    features and meanwhile are essential for the optimal performance of LLMs (Dettmers
    et al., [2022](#bib.bib8)). While these two approaches enable LLM pruning without
    performing fine-tuning, their performance is still far from satisfactory, *e.g.*,
    starting to lose performance at 20% sparsity with LLaMA-30B. Therefore, it is
    imperative to enable fine-tuning for sparse LLMs to fully unlock the potential
    of sparsity to escalate the affordability of LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前有两种方法探讨了在没有任何微调的情况下，将剪枝扩展到十亿级 LLM 的可能性。SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))
    将 LLM 剪枝形式化为逐层权重重构问题，目标在于减轻密集和稀疏 LLM 之间输出差异的 *w.r.t.* 重构误差。为了解决行 Hessian 挑战，即需要逐行计算巨大的矩阵的昂贵逆矩阵，SparseGPT
    迭代应用 OBS (Hassibi et al., [1993](#bib.bib20)) 逐个剪枝并按列方式更新权重，*最终* 达到与应用闭式回归重构相同的最优解。Wanda (Sun
    et al., [2023](#bib.bib48)) 提出了一个新的剪枝度量，该度量同时考虑了权重幅度及其对应的输入激活，与 SparseGPT 性能相当，而不需要昂贵的二阶信息。Wanda
    的直觉在于大规模 LLM 中存在的突现异常特征维度，这些维度比典型特征大得多，同时对 LLM 的最佳性能至关重要 (Dettmers et al., [2022](#bib.bib8))。虽然这两种方法使
    LLM 剪枝无需微调，但它们的性能仍然远未令人满意，*例如*，在 LLaMA-30B 的 20% 稀疏率下开始丧失性能。因此，为稀疏 LLM 启用微调以充分释放稀疏性的潜力，提升
    LLM 的性价比，是至关重要的。
- en: '![Refer to caption](img/d07301d98ea2a6c8c221d09a0b7e04ba.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d07301d98ea2a6c8c221d09a0b7e04ba.png)'
- en: 'Figure 1: Perplexity on WikiText-2 (left) and running time (right) of different
    methods for pruning LLaMA-V1 model family at 60% sparsity rate. Without any training,
    DS\faBanT consistently improves the performance of sparse LLMs, all within a linear
    time spectrum.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同方法在 60% 稀疏率下对 LLaMA-V1 模型系列的 WikiText-2（左）困惑度和运行时间（右）。在没有任何训练的情况下，DS\faBanT
    一致地提高了稀疏 LLM 的性能，所有方法都在一个线性时间范围内。
- en: In a parallel vein, Dynamic Sparse Training (DST), as outlined in previous research (Mocanu
    et al., [2018](#bib.bib39); Liu et al., [2019](#bib.bib32); Evci et al., [2020](#bib.bib11)),
    has garnered considerable attention recently due to its significant saving potentials
    in the context of neural network training. Instead of training an entire network,
    DST selectively updates and maintains a subset of the network throughout the training
    process, while allowing the sparse network topology to dynamically evolve via
    a weight operation (Mocanu et al., [2018](#bib.bib39)). Given its demonstrated
    efficacy in achieving efficient training, DST seems to be a promising candidate
    for efficient LLMs fine-tuning. However, it is essential to note that DST intrinsically
    requires the training of subnetworks via backpropagation, and the effectiveness
    of mask adaptation highly relies on a sufficient number of weight updates (Liu
    et al., [2021](#bib.bib33)). Moreover, prior studies have indicated its failure
    when employed for fine-tuning small-scale BERT-level language models (Liu et al.,
    [2023](#bib.bib34)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的研究中，动态稀疏训练（DST），如之前的研究所述（Mocanu 等人，[2018](#bib.bib39)；Liu 等人，[2019](#bib.bib32)；Evci
    等人，[2020](#bib.bib11)），由于在神经网络训练中具有显著的节省潜力，最近受到广泛关注。DST 不同于训练整个网络，它在整个训练过程中选择性地更新和维护网络的一个子集，同时允许稀疏网络拓扑通过权重操作动态演变（Mocanu
    等人，[2018](#bib.bib39)）。鉴于其在实现高效训练方面的有效性，DST 似乎是高效 LLM 调优的一个有前景的候选方法。然而，必须注意的是，DST
    本质上需要通过反向传播训练子网络，并且掩码适应的有效性高度依赖于足够数量的权重更新（Liu 等人，[2021](#bib.bib33)）。此外，之前的研究表明，当用于微调小规模
    BERT 级语言模型时，其效果并不理想（Liu 等人，[2023](#bib.bib34)）。
- en: Fortunately, it is noteworthy that the pruning-and-growing step employed in
    DST solely stands as a training-free methodology, enabling sparse mask adaptation
    based on certain weight status, *e.g.,* magnitude (Mocanu et al., [2018](#bib.bib39)).
    This offers an alternative perspective for addressing the aforementioned challenge: While
    fine-tuning sparse LLMs through backpropagation can result in substantial computational
    overhead, we can explore the possibility of iteratively updating sparse mask in
    a training-free fashion as a viable alternative. Based on this intuition, we introduce
    a training-free fine-tuning approach – Dynamic Sparse No Training (DS\faBanT).
    This approach empowers the further refinement of sparse LLMs without any weight
    updates. To facilitate mask adaptation in favor of the sparse reconstruction problem,
    we propose new criteria for mask pruning and growing, by considering both the
    expectation and variance of the reconstruction error reduction when recovering
    a specific weight. It is worth emphasizing that the DS\faBanT functions independently
    of the need for computationally intensive operations, such as gradient or Hessian
    matrices. Instead, it exclusively relies on a singular matrix multiplication operation
    to assess the reconstruction error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，值得注意的是，DST 中采用的剪枝和增长步骤本质上是一种无训练的方法，允许基于某些权重状态进行稀疏掩码适应，*例如，* 权重的幅度（Mocanu
    等人，[2018](#bib.bib39)）。这为解决上述挑战提供了另一种视角：尽管通过反向传播微调稀疏 LLM 可能会导致大量计算开销，我们可以探索以无训练方式迭代更新稀疏掩码的可能性作为可行的替代方案。基于这一直觉，我们提出了一种无训练微调方法——动态稀疏无训练（DS\faBanT）。这种方法使得在没有任何权重更新的情况下进一步优化稀疏
    LLM 成为可能。为了促进掩码适应以解决稀疏重建问题，我们提出了新的掩码剪枝和增长标准，通过考虑在恢复特定权重时重建误差减少的期望和方差来进行。这一点值得强调的是，DS\faBanT
    的运作不依赖于计算密集型操作，如梯度或 Hessian 矩阵。相反，它仅依赖于单次矩阵乘法操作来评估重建误差。
- en: We conduct comprehensive experiments to evaluate the effectiveness of DS\faBanT
    with a variety of LLMs, including LLaMa-V1 (Touvron et al., [2023a](#bib.bib49))
    and LLaMa-V2 (Zhang et al., [2022a](#bib.bib63)), Vicuna (Chiang et al., [2023](#bib.bib4)),
    and OPT families (Zhang et al., [2022a](#bib.bib63)), from 7 billion to 70 billion
    parameters. Our results demonstrate that DS\faBanT consistently improves the performance
    of sparse LLMs by a good margin, especially at high sparsity levels  50%. For instance, DS\faBanT is able to improve
    the performance over Magnitude pruning, SparseGPT, and Wanda by 1.1e6, 4.31, and
    1.87 perplexity with OPT-13B on WikiText-2 at 60% sparsity only using 7.3s on
    a single NVIDIA A100 GPU. Our work provides fresh insights in efficient sparse
    LLM fine-tune without weight updates and we hope to encourage more research in
    exploring benefits of sparsity in LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行全面实验以评估 DS\faBanT 在各种 LLMs 中的有效性，包括 LLaMa-V1 (Touvron et al., [2023a](#bib.bib49))
    和 LLaMa-V2 (Zhang et al., [2022a](#bib.bib63))，Vicuna (Chiang et al., [2023](#bib.bib4))，以及
    OPT 系列 (Zhang et al., [2022a](#bib.bib63))，参数范围从 70 亿到 700 亿。我们的结果表明，DS\faBanT
    在提高稀疏 LLMs 的性能方面始终表现出良好的改进，尤其在高稀疏度水平  50% 时。例如，DS\faBanT 能够在 WikiText-2 上的 OPT-13B 上，仅使用 7.3 秒的时间，通过
    7.3 秒的单个 NVIDIA A100 GPU 对比 Magnitude pruning、SparseGPT 和 Wanda 分别提升 1.1e6、4.31
    和 1.87 的困惑度。我们的工作为高效的稀疏 LLM 微调提供了新的见解，并希望鼓励更多研究探索 LLMs 中稀疏性的好处。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Network Sparsification. The process of eliminating redundant weights, known
    as network sparsification or network pruning, has served as a practical strategy
    to diminish the complexity of deep neural networks over the past decades (LeCun
    et al., [1989](#bib.bib28); Han et al., [2015](#bib.bib18)). Despite the substantial
    body of literature, network pruning can be roughly classified based on the granularity
    of sparsity and the dependency of the pre-trained dense models. I. Granularity
    of Sparsity: The granularity of sparsity varies from coarse grains to fine grains.
    The coarse-grained granularity can be a group of weights (Gray et al., [2017](#bib.bib17);
    Ding et al., [2017](#bib.bib10)), a complete neuron (Jiang et al., [2018](#bib.bib26));
    a filters/channels (Li et al., [2017](#bib.bib30)), or an attention head (Voita
    et al., [2019](#bib.bib51)), *etc*. On the other hand, fine-grained granularity
    eliminates the least important weights based on the selected criteria, regardless
    of where they are  (Gale et al., [2019](#bib.bib15)). The advantage of coarse-grained
    sparsity is its pronounced acceleration effect, which yet typically suffers from
    larger performance loss. Fine-grained sparsity enjoys performance superiority
    compared to other more structured forms of sparsity but receives limited support
    in common hardware. Nonetheless, recent advancements of dedicated fine-grained
    sparse patterns, such as N:M sparsity (Zhou et al., [2021](#bib.bib66); Zhang
    et al., [2022b](#bib.bib64)), can be effectively accelerated. As such, this paper
    focuses on fine-grained network pruning. II. Dependency of Pre-trained Networks:
    In parallel, sparsification techniques can be grouped into dense-to-sparse, and
    sparse-to-sparse methods based on the necessity of an over-parameterized dense
    network. The former entails embarking from a pre-trained dense model and discovering
    a sparse network (Han et al., [2015](#bib.bib18); Wen et al., [2016](#bib.bib55);
    Molchanov et al., [2017](#bib.bib40); Gale et al., [2019](#bib.bib15); Kurtic
    et al., [2022](#bib.bib27)), usually followed by a retraining process to recover
    the optimal accuracy. On the other hand, sparse-to-sparse methods aim to train
    sparse neural networks from scratch, omitting any preliminary steps involving
    dense pre-training (Mocanu et al., [2018](#bib.bib39); Lee et al., [2019](#bib.bib29);
    Evci et al., [2020](#bib.bib11); Wang et al., [2020](#bib.bib52); Liu et al.,
    [2021](#bib.bib33)). Among them, Dynamic Sparse Training (DST) (Mocanu et al.,
    [2018](#bib.bib39); Evci et al., [2020](#bib.bib11); Liu et al., [2021](#bib.bib33))
    stands out and receives upsurging interest due to its promise in saving both training
    and inference phases. In contrast to the conventional practices of pre-training
    followed by pruning, DST distinguishes itself by commencing with a randomly initialized
    sparse neural network. During a single training run, it dynamically adjusts the
    sparse network topology by such as pruning-and-growing, without the need for pre-training,
    while maintaining moderate training costs by, for example, keeping the similar
    sparsity ratios across all varying masks (Mostafa & Wang, [2019](#bib.bib41);
    Dettmers & Zettlemoyer, [2019](#bib.bib6); Yuan et al., [2021](#bib.bib61); Jayakumar
    et al., [2020](#bib.bib25)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 网络稀疏化。消除冗余权重的过程，称为网络稀疏化或网络剪枝，已作为减少深度神经网络复杂度的实际策略，在过去几十年中发挥了作用（LeCun 等，[1989](#bib.bib28)；Han
    等，[2015](#bib.bib18)）。尽管已有大量文献，网络剪枝大致可以根据稀疏的粒度和预训练密集模型的依赖性进行分类。I. 稀疏粒度：稀疏的粒度从粗粒度到细粒度不等。粗粒度可以是权重组（Gray
    等，[2017](#bib.bib17)；Ding 等，[2017](#bib.bib10)）、完整的神经元（Jiang 等，[2018](#bib.bib26)）、滤波器/通道（Li
    等，[2017](#bib.bib30)）或注意力头（Voita 等，[2019](#bib.bib51)），*等等*。另一方面，细粒度稀疏根据选定的标准消除最不重要的权重，而不考虑它们的位置（Gale
    等，[2019](#bib.bib15)）。粗粒度稀疏的优点在于其显著的加速效果，但通常会带来较大的性能损失。与其他更结构化形式的稀疏相比，细粒度稀疏在性能上具有优势，但在常见硬件中的支持有限。尽管如此，最近专门针对细粒度稀疏模式的进展，如
    N:M 稀疏（Zhou 等，[2021](#bib.bib66)；Zhang 等，[2022b](#bib.bib64)），可以有效地加速。因此，本文重点关注细粒度网络剪枝。II.
    预训练网络的依赖性：同时，稀疏化技术可以根据是否需要超参数化的密集网络分为从密集到稀疏和从稀疏到稀疏的方法。前者包括从预训练的密集模型开始，发现稀疏网络（Han
    等，[2015](#bib.bib18)；Wen 等，[2016](#bib.bib55)；Molchanov 等，[2017](#bib.bib40)；Gale
    等，[2019](#bib.bib15)；Kurtic 等，[2022](#bib.bib27)），通常会经过重训练过程以恢复最佳准确性。另一方面，从稀疏到稀疏的方法旨在从头训练稀疏神经网络，省略任何涉及密集预训练的初步步骤（Mocanu
    等，[2018](#bib.bib39)；Lee 等，[2019](#bib.bib29)；Evci 等，[2020](#bib.bib11)；Wang 等，[2020](#bib.bib52)；Liu
    等，[2021](#bib.bib33)）。其中，动态稀疏训练（DST）（Mocanu 等，[2018](#bib.bib39)；Evci 等，[2020](#bib.bib11)；Liu
    等，[2021](#bib.bib33)）因其在节省训练和推理阶段的潜力而受到越来越多的关注。与传统的预训练后剪枝的做法不同，DST 通过从随机初始化的稀疏神经网络开始。在单次训练运行期间，它动态调整稀疏网络拓扑，例如通过剪枝和增长，而无需预训练，同时通过例如保持所有不同掩模之间的类似稀疏比来维持适度的训练成本（Mostafa
    & Wang，[2019](#bib.bib41)；Dettmers & Zettlemoyer，[2019](#bib.bib6)；Yuan 等，[2021](#bib.bib61)；Jayakumar
    等，[2020](#bib.bib25)）。
- en: While the crux of this paper focuses on the first category, *i.e.*, pruning
    a pre-trained LLM model, our proposed method is mainly inspired by the pruning-and-growing
    utilized in DST to iteratively refine the binary masks in a training-free manner,
    even though we do not conduct weight training as such. Another line of research,
    akin to our approach, demonstrates the existence of “supermasks” within randomly
    initialized network (Zhou et al., [2019](#bib.bib67); Ramanujan et al., [2020](#bib.bib46);
    Huang et al., [2022](#bib.bib22)) or pre-trained networks (Mallya et al., [2018](#bib.bib36);
    Wortsman et al., [2020](#bib.bib57); Zhang et al., [2023](#bib.bib65)), exhibiting
    the capacity to achieve commendable performance solely by seeking binary masks.
    However, it is imperative to note that these methods heavily rely on backpropagation,
    which is ill-suited for LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的核心集中在第一类，即修剪预训练的 LLM 模型，我们提出的方法主要受到 DST 中修剪与增长的启发，以无训练的方式迭代优化二进制掩码，即使我们并未进行权重训练。另一项类似于我们方法的研究表明，存在于随机初始化网络（Zhou
    et al., [2019](#bib.bib67); Ramanujan et al., [2020](#bib.bib46); Huang et al.,
    [2022](#bib.bib22)）或预训练网络（Mallya et al., [2018](#bib.bib36); Wortsman et al.,
    [2020](#bib.bib57); Zhang et al., [2023](#bib.bib65)）中的“超级掩码”，表现出仅通过寻找二进制掩码即可实现令人称赞的性能。然而，必须注意的是，这些方法严重依赖反向传播，这不适合
    LLM。
- en: Pruning of LLMs. Compared to the well-established promise of pruning in pre-LLM
    small-scale models, the advancement of pruning in the context of LLMs appears
    to exhibit relatively modest progress. Firstly, traditional pruning generally
    requires at least one iteration of re-training to recover performance. Considering
    the substantial model size and massive datasets associated with LLMs, the prospect
    of conducting such resource-intensive re-training becomes a formidable challenge.
    To mitigate the above challenge, researchers have introduced pruning algorithms
    specifically devised for LLMs compression. Ma et al. ([2023](#bib.bib35)) explored
    structured sparse LLM by applying Taylor pruning (Molchanov et al., [2017](#bib.bib40))
    to remove entire weight rows, followed by the parameter efficient fine-tuning
    (PEFT) technique (Hu et al., [2021](#bib.bib21)) fine-tuning. However, the fine-tuning
    phase still demands a considerable amount of data while the performance suffers
    a significant degradation, attributed primarily to the coarse-grained level of
    sparsity. Recent research endeavours have evolved towards the direction of unstructured
    pruning in one-shot without fine-tuning, demonstrating significant progresses.
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) incorporates the Hessian inverse
    for pruning and subsequent residual weight updates, whereas Wanda (Sun et al.,
    [2023](#bib.bib48)) directly arrives at a sparse LLM model by a criterion depicted
    by the multiplication of the absolute values of weights and their activations
    with the aim to preserve outliers (Dettmers et al., [2022](#bib.bib8)) emerged
    in LLMs. DS\faBanT serves as an orthogonal perspective and can be organically
    integrated on top of them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的修剪。与在预训练 LLM 小规模模型中广泛验证的修剪承诺相比，LLM 领域的修剪进展相对较为缓慢。首先，传统修剪通常需要至少一次再训练以恢复性能。考虑到
    LLM 模型的巨大规模和庞大的数据集，进行如此资源密集的再训练变得具有极大的挑战。为了应对这一挑战，研究人员提出了专门为 LLM 压缩设计的修剪算法。Ma
    et al. ([2023](#bib.bib35)) 探索了结构化稀疏 LLM，通过应用 Taylor 修剪（Molchanov et al., [2017](#bib.bib40)）去除整个权重行，随后进行参数高效微调（PEFT）技术（Hu
    et al., [2021](#bib.bib21)）。然而，微调阶段仍需大量数据，同时性能显著下降，主要由于稀疏性过于粗糙。最近的研究工作已朝向一次性无微调的非结构化修剪方向发展，取得了显著进展。SparseGPT（Frantar
    & Alistarh, [2023](#bib.bib13)）结合了 Hessian 逆矩阵用于修剪和随后的残差权重更新，而 Wanda（Sun et al.,
    [2023](#bib.bib48)）通过将权重的绝对值和其激活值相乘的标准，直接得到稀疏 LLM 模型，旨在保留 LLM 中出现的异常值（Dettmers
    et al., [2022](#bib.bib8)）。DS\faBanT 提供了一个正交视角，并且可以有机地集成在这些方法之上。
- en: 3 Dynamic Sparse No Training – DS\faBanT
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动态稀疏无训练 – DS\faBanT
- en: 'Preliminary. LLM pruning entails the removal of a certain proportion of pre-trained
    weights to obtain a sparse LLM, with the objective of achieving minimal discrepancy
    between the output of the sparse and dense models (Hassibi et al., [1993](#bib.bib20)).
    Solving this problem can be very arduous given the immense scale of LLMs. Therefore,
    it is more practical to formalize LLM pruning as a layer-wise reconstruction problem (Hubara
    et al., [2021](#bib.bib23); Frantar & Alistarh, [2023](#bib.bib13)). Denote the
    weights of one dense LLM layer as $\mathbf{W}\in\mathbb{R}^{C_{\text{out}},C_{\text{in}}}$
    and $C_{\text{in}}$ calibration samples, the input activation can be represented
    as $\mathbf{A}\in\mathbb{R}^{C_{\text{in}},N\times L}$ be the sequence length.
    Pruning can be viewed as devising a binary mask $\mathbf{M}\in\{0,1\}^{C_{\text{out}},C_{\text{in}}}$
    can be formalized as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 初步。LLM 剪枝涉及去除一定比例的预训练权重，以获得稀疏 LLM，目标是实现稀疏模型与密集模型输出之间的最小差异 (Hassibi et al., [1993](#bib.bib20))。考虑到
    LLM 的庞大规模，解决这个问题可能非常艰难。因此，更实际的方法是将 LLM 剪枝形式化为逐层重构问题 (Hubara et al., [2021](#bib.bib23);
    Frantar & Alistarh, [2023](#bib.bib13))。将一个密集 LLM 层的权重记作 $\mathbf{W}\in\mathbb{R}^{C_{\text{out}},C_{\text{in}}}$
    和 $C_{\text{in}}$ 校准样本，输入激活可以表示为 $\mathbf{A}\in\mathbb{R}^{C_{\text{in}},N\times
    L}$，其中 $L$ 为序列长度。剪枝可以被看作设计一个二进制掩码 $\mathbf{M}\in\{0,1\}^{C_{\text{out}},C_{\text{in}}}$
    的过程。
- en: '|  | $\min_{\mathbf{M},\mathbf{W}}\;&#124;&#124;\underbrace{(\mathbf{M}\odot\mathbf{W})*\mathbf{A}-\mathbf{W}*\mathbf{A}}_{\Delta}&#124;&#124;_{2},\;\;\emph{s.t.}\;\;1-\frac{\left\&#124;\mathbf{M}\right\&#124;_{0}}{C_{\text{out}}\cdot
    C_{\text{in}}}=p,$ |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{M},\mathbf{W}}\;&#124;&#124;\underbrace{(\mathbf{M}\odot\mathbf{W})*\mathbf{A}-\mathbf{W}*\mathbf{A}}_{\Delta}&#124;&#124;_{2},\;\;\emph{s.t.}\;\;1-\frac{\left\&#124;\mathbf{M}\right\&#124;_{0}}{C_{\text{out}}\cdot
    C_{\text{in}}}=p,$ |  | (1) |'
- en: where $*$, $||\cdot||_{2}$ norm, respectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $*$ 和 $||\cdot||_{2}$ 范数分别表示。
- en: '![Refer to caption](img/6412238c8beb43aee2a09aa292c88e70.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6412238c8beb43aee2a09aa292c88e70.png)'
- en: 'Figure 2: Framework of DS\faBanT.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: DS\faBanT 的框架。'
- en: Note we refer $\Delta\in\mathbb{R}^{C_{out},N\cdot L}$ as to the reconstruction
    error for ease of the following text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将 $\Delta\in\mathbb{R}^{C_{out},N\cdot L}$ 视为重构误差，以便于后续的讨论。
- en: 'Dynamic Sparse No Training. The problem defined in Eq. ([1](#S3.E1 "In 3 Dynamic
    Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs")) can be addressed from two complementary perspectives.
    Firstly, it can be resolved through the initialization of sparse networks *i.e.*,
    devising criteria to prune weights that exhibit minimal impact on model output.
    For instance, SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)) employs second-order
    Hessian inverses, while Wanda (Sun et al., [2023](#bib.bib48)) considers products
    of weight and activation norm as the guide for weight removal. Secondly, for the
    obtained sparse networks, the remaining weights can be naturally fine-tuned to
    further compensate for the reconstruction error (Han et al., [2015](#bib.bib18)).
    Unfortunately, this requires substantial training resources, which is not practical
    given the large volumes of LLMs. Therefore, SparseGPT adjusts the remaining weights
    via an iterative OBS update (Hassibi & Stork, [1992](#bib.bib19)), which as a
    consequence remarkably reduces the computing demands.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '动态稀疏无训练。第 Eq. ([1](#S3.E1 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏
    LLM")) 所定义的问题可以从两个互补的角度来解决。首先，可以通过稀疏网络的初始化来解决，即设计剪枝标准，以去除对模型输出影响最小的权重。例如，SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib13)) 使用二阶 Hessian 逆矩阵，而 Wanda (Sun et al.,
    [2023](#bib.bib48)) 将权重和激活范数的乘积作为权重去除的指南。其次，对于获得的稀疏网络，可以自然地对剩余的权重进行微调，以进一步补偿重构误差
    (Han et al., [2015](#bib.bib18))。不幸的是，这需要大量的训练资源，对于大规模的 LLM 来说并不现实。因此，SparseGPT
    通过迭代 OBS 更新调整剩余权重 (Hassibi & Stork, [1992](#bib.bib19))，从而显著减少计算需求。'
- en: 'In this work, our focus is on the second part, *i.e.*, how to efficiently reduce
    the reconstruction error of a given pruned sparse network to its dense counterpart?
    Instead of fully fine-tuning (Han et al., [2015](#bib.bib18)) or partially updating
    the pruned LLMs (Frantar & Alistarh, [2023](#bib.bib13)) to recover performance,
    we introduce an ultra-efficient yet effective alternative to refine the sparse
    mask after pruning based on their contribution to the reconstruction error. Our
    approach is inspired by the pruning-and-growing operation used in Dynamic Sparse
    Training (Mocanu et al., [2018](#bib.bib39); Evci et al., [2020](#bib.bib11)).
    DST incorporates the processes of weight pruning and weight growing within the
    framework of sparse network training, contributing to the discovery of improved
    sparse topologies. Note that this pruning-and-growing operation solely serves
    as a training-free approach that is able to adapt sparse masks towards a desirable
    perspective, *e.g.,* loss minimization. Based on this insight, we propose DS\faBanT,
    a training-free fine-tuning method for sparse LLMs that strips weights updating
    in DST and keeps the pruning-and-growing by converting the optimization objective
    to the reconstruction error of each weight row. We isolate pruning-and-growing
    from network training, and formulate it as an iterative approach to progressively
    optimize sparse masks towards the desirable ones achieving minimal reconstruction
    error represented by Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT
    ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们的重点是第二部分，即如何有效地将给定剪枝稀疏网络的重建误差减少到其稠密对应物？我们引入了一种超高效但有效的替代方法，通过根据剪枝后稀疏掩码对重建误差的贡献来细化稀疏掩码，而不是完全微调（Han
    et al., [2015](#bib.bib18)）或部分更新剪枝后的 LLMs（Frantar & Alistarh, [2023](#bib.bib13)）以恢复性能。我们的方法受到动态稀疏训练（Mocanu
    et al., [2018](#bib.bib39); Evci et al., [2020](#bib.bib11)）中使用的剪枝与生长操作的启发。DST
    在稀疏网络训练框架内结合了权重剪枝和权重生长的过程，助力于发现改进的稀疏拓扑。请注意，这一剪枝与生长操作仅作为一种无训练的方法，能够将稀疏掩码调整到理想的视角，例如，最小化损失。基于这一见解，我们提出了
    DS\faBanT，一种无训练的稀疏 LLM 微调方法，通过将优化目标转换为每个权重行的重建误差，剥离了 DST 中的权重更新，并保留剪枝与生长。我们将剪枝与生长从网络训练中隔离，并将其制定为一种迭代方法，逐步优化稀疏掩码以实现最小重建误差，如
    Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) 所示。'
- en: 'Input: A sparse layer with weight $\mathbf{W}\odot$, update threshold $\epsilon$
    via Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs"))       for *$r=1$* do            
    for *$t=1$* do                   Obtain the growing index $i$ via Eq. ([3](#S3.E3
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")).                   $\mathbf{M}_{r,i}=1$                  
    Update reconstruction error $\Delta_{r}$* then                         break      return
    Fine-tuned sparse weights $\mathbf{W}\odot\mathbf{M}$.* *Algorithm 1 Pseudocode
    of DS\faBanT.*  *Specifically, DS\faBanT starts with a sparse LLM which can be
    pruned by any existing criteria (Jaiswal et al., [2023](#bib.bib24); Sun et al.,
    [2023](#bib.bib48); Frantar & Alistarh, [2023](#bib.bib13)). Then, it performs
    iterative weight growing and pruning by looking at the reconstruction error as
    defined in Eq. ([1](#S3.E1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")), with
    especially-designed criteria to decrease the output discrepancy between sparse
    LLMs and their dense counterparts. The framework of DS\faBanT is illustrated in
    Figure [2](#S3.F2 "Figure 2 ‣ 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs") and its
    main parts are detailedly described below.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：一个具有权重 $\mathbf{W}\odot$ 的稀疏层，通过公式 ([1](#S3.E1 "在 3 动态稀疏无训练 – DS\faBanT
    ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏 LLMs")) 更新阈值 $\epsilon$，对于 *$r=1$* 执行：对 *$t=1$* 执行：通过公式 ([3](#S3.E3
    "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏 LLMs")) 获得增长索引 $i$。$\mathbf{M}_{r,i}=1$
    更新重构误差 $\Delta_{r}$ *然后* 退出循环，*返回微调后的稀疏权重 $\mathbf{W}\odot\mathbf{M}$*。*算法 1 DS\faBanT
    的伪代码*。*具体来说，DS\faBanT 从一个可以通过任何现有标准剪枝的稀疏 LLM 开始 (Jaiswal et al., [2023](#bib.bib24);
    Sun et al., [2023](#bib.bib48); Frantar & Alistarh, [2023](#bib.bib13))。然后，通过观察重构误差（如公式 ([1](#S3.E1
    "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏 LLMs")）中定义的）来执行迭代的权重增长和剪枝，使用特别设计的标准来减少稀疏
    LLM 和其密集对应物之间的输出差异。DS\faBanT 的框架如图 [2](#S3.F2 "图 2 ‣ 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练
    \faBan: 无训练微调稀疏 LLMs") 所示，其主要部分将在下文中详细描述。*'
- en: 'Growing Criterion. As each output neuron is computed independently, we use
    one weight row $\mathbf{W}_{r}$ for illustration. Given sparse weight row $\mathbf{M}_{r}\odot\mathbf{W}_{r}$
    across different input activations. Therefore, our growing criterion considers
    both the expectation and variance of the reconstruction error change when recovering
    a weight back. In particular, the index $i$ of the revived weights is derived
    as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 增长标准。由于每个输出神经元是独立计算的，我们以一个权重行 $\mathbf{W}_{r}$ 为例。给定稀疏权重行 $\mathbf{M}_{r}\odot\mathbf{W}_{r}$
    跨不同输入激活。因此，我们的增长标准考虑了恢复一个权重时重构误差变化的期望和方差。具体来说，复苏权重的索引 $i$ 是按如下方式得出的：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $\mathbb{E}(\cdot)$ stand for the expectation and variance of given inputs
    across $N\times L$ represents the expected influence of weight growing on $\Delta_{r}$,
    we can determine which weight should be restored to approach the decrease of $\Delta_{r}$
    exhibits high variance across different inputs, restoring it may not result in
    stable error reduction.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{E}(\cdot)$ 代表期望和方差，给定的输入在 $N\times L$ 上的方差表示权重增长对 $\Delta_{r}$ 的预期影响，我们可以确定哪些权重应该恢复，以接近
    $\Delta_{r}$ 的减少。由于方差在不同输入之间存在较高波动，恢复这些权重可能不会导致稳定的误差减少。
- en: 'Pruning Criterion. After choosing revived weights, we need to select another
    weight for pruning in order to maintain a fixed sparsity rate. However, the circumstances
    here are distinct: if we prune weights based on the impact of reconstruction error
    change as per Eq. ([2](#S3.E2 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")), there
    is a risk of removing weights that significantly influence the output. This concern
    becomes especially critical when pruning LLMs due to the presence of emergent
    large magnitude features within them (Dettmers et al., [2022](#bib.bib8); Wei
    et al., [2022a](#bib.bib53); Schaeffer et al., [2023](#bib.bib47)). To alleviate
    this, we utilize a transformed version of the Wanda metric (Sun et al., [2023](#bib.bib48)).
    In addition to its standard criterion for pruning weights, we mandate that the
    selected weights should also contribute positively towards the reduction of reconstruction
    error when being pruned. This helps in preserving critical weights from removal
    without compromising the stable decrease of reconstruction error during the training-free
    fine-tuning process. Therefore, the pruning index $j$ is obtained as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝标准。在选择了恢复的权重后，我们需要选择另一组权重进行剪枝，以保持固定的稀疏率。然而，这里的情况有所不同：如果我们根据公式 ([2](#S3.E2
    "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 针对稀疏 LLM 的无训练微调"))对重建误差变化的影响进行剪枝，可能会有移除对输出有显著影响的权重的风险。当剪枝
    LLM 时，这种问题尤为关键，因为 LLM 中存在突现的大幅度特征（Dettmers et al., [2022](#bib.bib8); Wei et al.,
    [2022a](#bib.bib53); Schaeffer et al., [2023](#bib.bib47)）。为了缓解这一问题，我们采用了 Wanda
    指标的变体（Sun et al., [2023](#bib.bib48)）。除了其标准的权重剪枝标准外，我们还要求所选择的权重在被剪枝时应有助于减少重建误差。这有助于在无训练微调过程中，保持重要权重不被移除，同时不影响重建误差的稳定减少。因此，剪枝指数
    $j$ 的获取方法如下：'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'Workflow. Given the criteria depicted above, the workflow of DS\faBanT is outlined
    in Algorithm [1](#alg1 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic
    Sparse No Training \faBan: Training-Free Fine-tuning for Sparse LLMs"). In particular,
    it iteratively performs weight growing and pruning with respect to Eq. ([2](#S3.E2
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")) and Eq. ([3](#S3.E3 "In 3 Dynamic
    Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs")), with the reconstruction error updated until it
    reaches a pre-defined threshold. Meanwhile, we set a maximum pruning-and-growing
    cycle $T$.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程。根据上述标准，DS\faBanT 的工作流程在算法 [1](#alg1 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练
    \faBan: 针对稀疏 LLM 的无训练微调")中概述。具体而言，它迭代地根据公式 ([2](#S3.E2 "在 3 动态稀疏无训练 – DS\faBanT
    ‣ 动态稀疏无训练 \faBan: 针对稀疏 LLM 的无训练微调"))和公式 ([3](#S3.E3 "在 3 动态稀疏无训练 – DS\faBanT ‣
    动态稀疏无训练 \faBan: 针对稀疏 LLM 的无训练微调"))执行权重增长和剪枝，直到重建误差更新至预定义的阈值。同时，我们设置了最大剪枝和增长周期
    $T$。'
- en: 'Remark. It’s noteworthy that Algorithm,[1](#alg1 "In 3 Dynamic Sparse No Training
    – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for
    Sparse LLMs") outlines the processing of each row in a sequential manner, primarily
    for the sake of simplicity. However, it’s imperative to acknowledge that each
    row can, in fact, undergo parallel processing by employing a binary indicator
    to assess whether a particular row has satisfied the termination condition. Furthermore,
    the DS\faBanT process eliminates the necessity for resource-intensive procedures
    such as backpropagation or the computation of gradient and Hessian matrices. Instead,
    it relies solely on several matrix multiplications to calculate the reconstruction
    error, a task that can be executed efficiently on GPUs. Subsequently, during each
    iteration of the DS\faBanT process, the only operation is to update the reconstruction
    error through straightforward addition and subtraction operations during the pruning-and-growing
    process. This approach effectively circumvents the introduction of additional
    algorithmic complexity. In summary, DS\faBanT preserves the simplicity associated
    with pruning LLMs, akin to the approaches employed in Wanda and Magnitude pruning.
    It’s important to note that while we share the same optimization objective with
    SparseGPT, our approach adopts a significantly more efficient pruning-and-growing
    operation to minimize the reconstruction error. As demonstrated in the next section,
    this efficiency operation can further improve the performance of SparseGPT.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '注：值得注意的是，算法，[1](#alg1 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 稀疏 LLM 的无训练微调")
    以顺序方式概述了每一行的处理，主要是为了简化操作。然而，必须承认，每一行实际上可以通过采用二进制指示器来评估是否满足终止条件，从而进行并行处理。此外，DS\faBanT
    过程避免了诸如反向传播或计算梯度和 Hessian 矩阵等资源密集型程序。相反，它仅依赖于几次矩阵乘法来计算重构误差，这一任务可以高效地在 GPU 上执行。随后，在每次
    DS\faBanT 过程的迭代中，唯一的操作是在修剪和增长过程中通过简单的加法和减法操作来更新重构误差。这种方法有效地避免了额外算法复杂性的引入。总之，DS\faBanT
    保持了类似于 Wanda 和 Magnitude 修剪方法的 LLM 修剪简洁性。值得注意的是，尽管我们与 SparseGPT 共享相同的优化目标，但我们的方法采用了显著更高效的修剪和增长操作以最小化重构误差。如下一节所示，这种高效操作可以进一步提高
    SparseGPT 的性能。'
- en: 'Table 1: WikiText-2 Perplexity comparison for pruning LLMs at 60% sparsity
    rate.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: WikiText-2 在 60% 稀疏率下修剪 LLM 的困惑度比较。'
- en: '|  | LLaMA-V1 | LLaMA-V2 | Vicuna | OPT |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA-V1 | LLaMA-V2 | Vicuna | OPT |'
- en: '| Method | 7B | 13B | 30B | 65B | 7B | 13B | 70B | 13B | 13B |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 7B | 13B | 30B | 65B | 7B | 13B | 70B | 13B | 13B |'
- en: '| Dense | 5.68 | 5.09 | 4.10 | 3.56 | 5.47 | 4.88 | 3.32 | 5.94 | 10.12 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 5.68 | 5.09 | 4.10 | 3.56 | 5.47 | 4.88 | 3.32 | 5.94 | 10.12 |'
- en: '| Magnitude | 5.6e2 | 2.3e2 | 15.97 | 8.18 | 6.9e3 | 10.11 | 13.35 | 14.39
    | 1.1e6 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Magnitude | 5.6e2 | 2.3e2 | 15.97 | 8.18 | 6.9e3 | 10.11 | 13.35 | 14.39
    | 1.1e6 |'
- en: '| w. DS\faBanT | 66.70 | 30.71 | 10.81 | 7.37 | 40.01 | 9.41 | 6.77 | 12.02
    | 2.4e2 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 66.70 | 30.71 | 10.81 | 7.37 | 40.01 | 9.41 | 6.77 | 12.02
    | 2.4e2 |'
- en: '| SparseGPT | 10.41 | 8.43 | 6.81 | 5.83 | 10.14 | 7.88 | 5.10 | 10.02 | 21.23
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 10.41 | 8.43 | 6.81 | 5.83 | 10.14 | 7.88 | 5.10 | 10.02 | 21.23
    |'
- en: '| w. DS\faBanT | 9.65 | 7.73 | 6.69 | 5.64 | 9.67 | 7.57 | 5.07 | 9.38 | 16.92
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 9.65 | 7.73 | 6.69 | 5.64 | 9.67 | 7.57 | 5.07 | 9.38 | 16.92
    |'
- en: '| Wanda | 10.69 | 8.75 | 6.56 | 5.90 | 10.79 | 8.40 | 5.25 | 9.54 | 15.88 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 10.69 | 8.75 | 6.56 | 5.90 | 10.79 | 8.40 | 5.25 | 9.54 | 15.88 |'
- en: '| w. DS\faBanT | 10.22 | 8.46 | 6.44 | 5.75 | 10.59 | 8.18 | 5.20 | 9.18 |
    14.01 |*  *## 4 Experimental Results'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '| w. DS\faBanT | 10.22 | 8.46 | 6.44 | 5.75 | 10.59 | 8.18 | 5.20 | 9.18 |
    14.01 |*  *## 4 实验结果'
- en: 4.1 Settings
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Implementation details. The implementation details of our proposed DS\faBanT
    are presented as follows, mostly conforming to the existing setups (Frantar &
    Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)). In context to pruning
    configuration, we adhere to SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)),
    where a uniform sparsity is imposed for all layers with the first embedding layer
    and the final classification head skipped. Meanwhile, the calibration data consists
    of 128 segments, each with 2048 tokens. These segments are randomly selected from
    the first shard of the C4 dataset (Raffel et al., [2020](#bib.bib45)). For the
    hyper-parameter settings, we set the maximum cycle $T=50$ in all experiments.
    We implement DS\faBanT in PyTorch (Paszke et al., [2019](#bib.bib44)) and use
    the HuggingFace Transformers library (Wolf et al., [2019](#bib.bib56)) for handling
    models and datasets. All pruning experiments are conducted on NVIDIA A100 GPUs
    with 80GB of memory.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节。我们提出的DS\faBanT的实现细节如下，主要符合现有的设置 (Frantar & Alistarh, [2023](#bib.bib13);
    Sun et al., [2023](#bib.bib48))。在修剪配置方面，我们遵循SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))，对所有层施加均匀稀疏性，并跳过第一个嵌入层和最终的分类头。同时，校准数据包括128个片段，每个片段有2048个标记。这些片段随机从C4数据集的第一个分片中选择
    (Raffel et al., [2020](#bib.bib45))。在超参数设置方面，我们在所有实验中设置最大周期$T=50$。我们在PyTorch (Paszke
    et al., [2019](#bib.bib44))中实现DS\faBanT，并使用HuggingFace Transformers库 (Wolf et
    al., [2019](#bib.bib56))来处理模型和数据集。所有修剪实验均在具有80GB内存的NVIDIA A100 GPU上进行。
- en: Baselines. We principally work with the LLaMA-V1 (Touvron et al., [2023a](#bib.bib49)),
    LLaMA-V2 (Touvron et al., [2023b](#bib.bib50)), Vicuna (Chiang et al., [2023](#bib.bib4)),
    and OPT families (Zhang et al., [2022a](#bib.bib63)), from 7 billion to 70 billion
    parameters, which are among the most powerful and open-source Large Language Models
    (LLMs) in the field today. We run DS\faBanT on sparse LLMs pruned by various methods
    including (1) Magnitude-based pruning (Han et al., [2015](#bib.bib18)) that discards
    weights based on their magnitudes. (2) SparseGPT (Frantar & Alistarh, [2023](#bib.bib13))
    that utilizes second-order Hessian inverses to ascertain unimportant weights.
    (3) Wanda (Sun et al., [2023](#bib.bib48)) that removes weights with the smallest
    magnitudes multiplied by the corresponding input activation norms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基准。我们主要使用LLaMA-V1 (Touvron et al., [2023a](#bib.bib49))、LLaMA-V2 (Touvron et
    al., [2023b](#bib.bib50))、Vicuna (Chiang et al., [2023](#bib.bib4)) 和OPT系列 (Zhang
    et al., [2022a](#bib.bib63))，其参数范围从70亿到700亿，都是当前领域中最强大且开源的大型语言模型（LLMs）之一。我们在通过各种方法修剪的稀疏LLMs上运行DS\faBanT，包括（1）基于幅度的修剪
    (Han et al., [2015](#bib.bib18))，该方法根据权重的幅度丢弃权重。（2）SparseGPT (Frantar & Alistarh,
    [2023](#bib.bib13))，利用二阶Hessian逆矩阵来确定不重要的权重。（3）Wanda (Sun et al., [2023](#bib.bib48))，通过删除与输入激活范数相乘后幅度最小的权重。
- en: Evaluation. In accordance with prior studies (Frantar et al., [2022](#bib.bib14);
    Dettmers et al., [2023](#bib.bib9); Yao et al., [2022](#bib.bib59); Frantar &
    Alistarh, [2023](#bib.bib13)), we assess the performance of pruned models by calculating
    the perplexity of language generation experiments on separate validation sets
    derived from WikiText2 (Merity et al., [2016](#bib.bib37)). While perplexity has
    served as a stable and robust indicator of the generative performance of models (Dettmers
    & Zettlemoyer, [2023](#bib.bib7)), we also examined the zero-shot capabilities
    of pruned models. In detail, we report the accuracy in six zero-shot tasks including
    PIQA (Bisk et al., [2020](#bib.bib1)), StoryCloze (Mostafazadeh et al., [2017](#bib.bib42)),
    ARC Easy and Challenge (Clark et al., [2018](#bib.bib5)), HellaSwag (Zellers et al.,
    [2019](#bib.bib62)) and OpenBookQA (Mihaylov et al., [2018](#bib.bib38)). We implement
    the lm-eval-harness (Gao et al., [2021](#bib.bib16)) for the execution of all
    zero-shot tasks, with the report including both the accuracy results on each benchmark
    and overall average accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。根据先前的研究 (Frantar et al., [2022](#bib.bib14); Dettmers et al., [2023](#bib.bib9);
    Yao et al., [2022](#bib.bib59); Frantar & Alistarh, [2023](#bib.bib13))，我们通过计算来自WikiText2
    (Merity et al., [2016](#bib.bib37))的单独验证集上语言生成实验的困惑度来评估修剪模型的性能。尽管困惑度已成为模型生成性能的稳定且可靠的指标
    (Dettmers & Zettlemoyer, [2023](#bib.bib7))，我们还检查了修剪模型的零-shot能力。具体而言，我们报告了在六个零-shot任务中的准确率，包括PIQA
    (Bisk et al., [2020](#bib.bib1))、StoryCloze (Mostafazadeh et al., [2017](#bib.bib42))、ARC
    Easy 和 Challenge (Clark et al., [2018](#bib.bib5))、HellaSwag (Zellers et al.,
    [2019](#bib.bib62)) 和OpenBookQA (Mihaylov et al., [2018](#bib.bib38))。我们使用lm-eval-harness
    (Gao et al., [2021](#bib.bib16))执行所有零-shot任务，报告包括每个基准上的准确率结果和总体平均准确率。
- en: 4.2 Language Modeling
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言建模
- en: 'Quantitative results. The results for fine-tuning sparse LLM models at a uniform
    sparsity rate of 60% are presented in Table [1](#S3.T1 "Table 1 ‣ 3 Dynamic Sparse
    No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs"). Irrespective of the datasets used for evaluation, DS\faBanT
    consistently delivers performance improvement for sparse LLMs with their original
    sizes varying from 7B to 70B. For instance, when pruning LLaMA-V1 with 7B parameters,
    DS\faBanT is able to enhance the performance of Magnitude (Jaiswal et al., [2023](#bib.bib24)),
    SparseGPT (Frantar & Alistarh, [2023](#bib.bib13)), and Wanda (Sun et al., [2023](#bib.bib48))
    by 4.94e2, 0.76, and 0.47 perplexity on the Wikitext-2 validation sets, respectively.
    It is worth noting that, without any weight updating, DS\faBanT consistently demonstrates
    better performance than SparseGPT, which requires expensive second-order Hessian
    inverses to update the sparse model. For larger models, the efficacy of DS\faBanT
    is still hold with performance gain from 13.35 to 6.77 perplexity when fine-tuning
    sparse LLaMA-V2-70B obtained by magnitude pruning (Han et al., [2015](#bib.bib18)).
    These findings suggest DS\faBanT’s versatility, being adaptable to boost the performance
    of sparse LLMs with different parameter budgets.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '定量结果。在表 [1](#S3.T1 "表 1 ‣ 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏 LLMs")
    中展示了在 60% 的均匀稀疏率下微调稀疏 LLM 模型的结果。无论用于评估的数据集如何，DS\faBanT 一直为稀疏 LLMs 提供性能改进，其原始尺寸从
    7B 到 70B 不等。例如，当剪枝具有 7B 参数的 LLaMA-V1 时，DS\faBanT 能够将 Magnitude (Jaiswal 等， [2023](#bib.bib24))、SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib13)) 和 Wanda (Sun 等， [2023](#bib.bib48)) 的
    Wikitext-2 验证集的困惑度分别提高 4.94e2、0.76 和 0.47。值得注意的是，在没有任何权重更新的情况下，DS\faBanT 一直表现出比
    SparseGPT 更好的性能，后者需要昂贵的二阶 Hessian 反演来更新稀疏模型。对于更大的模型，DS\faBanT 的效果依然显著，在微调通过 magnitude
    剪枝获得的稀疏 LLaMA-V2-70B 时，性能提升从 13.35 到 6.77 的困惑度。这些发现表明，DS\faBanT 的多样性，使其适应于提升具有不同参数预算的稀疏
    LLMs 的性能。'
- en: 'Table 2: WikiText-2 perplexity performance of DS\faBanT for fine-tuning sparse
    LLaMA-V1-7B/65B pruned by the Wanda metric at varying sparsity rates.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同稀疏率下通过 Wanda 指标剪枝的 DS\faBanT 在微调稀疏 LLaMA-V1-7B/65B 时的 WikiText-2 困惑度性能。
- en: '|  | LLaMA-V1-7B | LLaMA-V1-65B |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA-V1-7B | LLaMA-V1-65B |'
- en: '| --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sparsity | 50% | 60% | 70% | 80% | 90% | 50% | 60% | 70% | 80% | 90% |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏率 | 50% | 60% | 70% | 80% | 90% | 50% | 60% | 70% | 80% | 90% |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Wanda | 7.26 | 10.69 | 88.84 | 4.80e3 | 6.41e5 | 4.57 | 5.90 | 15.24 | 2.06e3
    | 3.21e4 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 7.26 | 10.69 | 88.84 | 4.80e3 | 6.41e5 | 4.57 | 5.90 | 15.24 | 2.06e3
    | 3.21e4 |'
- en: '| w. DS\faBanT | 7.12 | 10.22 | 62.05 | 4.12e3 | 8.43e4 | 4.54 | 5.75 | 12.93
    | 1.82e3 | 2.09e4 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 7.12 | 10.22 | 62.05 | 4.12e3 | 8.43e4 | 4.54 | 5.75 | 12.93
    | 1.82e3 | 2.09e4 |'
- en: 'Varying Sparsity Rates. We further investigate the efficacy of DS\faBanT when
    fine-tuning sparse LLMs with varying pruning rates. Table [2](#S4.T2 "Table 2
    ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No Training
    \faBan: Training-Free Fine-tuning for Sparse LLMs") shows that DS\faBanT offers
    effective performance enhancement across various pruning methods at different
    sparsity levels. Particularly, this improvement becomes increasingly evident as
    the sparsity level grows.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '变化的稀疏率。我们进一步研究了在不同剪枝率下微调稀疏 LLMs 时 DS\faBanT 的效果。表 [2](#S4.T2 "表 2 ‣ 4.2 语言建模
    ‣ 4 实验结果 ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏 LLMs") 显示，DS\faBanT 在不同稀疏水平的各种剪枝方法中提供了有效的性能提升。特别是，随着稀疏水平的增加，这种改进变得越来越明显。'
- en: 'Table 3: Time overhead (in seconds) for pruning LLaMA-V1 model family.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：剪枝 LLaMA-V1 模型系列的时间开销（以秒为单位）。
- en: '| Method | 7B | 13B | 30B | 65B |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 7B | 13B | 30B | 65B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SparseGPT | 209 | 337 | 721 | 1285 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 209 | 337 | 721 | 1285 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Wanda | 0.3 | 0.5 | 1.1 | 1.9 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 0.3 | 0.5 | 1.1 | 1.9 |'
- en: '| Wanda+DS\faBanT | 4.3 | 7.4 | 15.7 | 23.7 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Wanda+DS\faBanT | 4.3 | 7.4 | 15.7 | 23.7 |'
- en: 'Table 4: Comparion with LoRA fine-tuning using 50% sparse LLaMA-7B.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：与使用 50% 稀疏 LLaMA-7B 的 LoRA 微调的比较。
- en: '| Method | Time Cost | Perplexity |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 时间成本 | 困惑度 |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Wanda+LoRA | 4h | 6.87 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Wanda+LoRA | 4h | 6.87 |'
- en: '| Wanda+DS\faBanT | 4.3s | 7.12 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Wanda+DS\faBanT | 4.3s | 7.12 |'
- en: 'Table 5: Wikitext-2 perplexity comparison for pruning LLaMA-V1 model family
    with N:M pattern.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：用于剪枝 LLaMA-V1 模型系列的 Wikitext-2 困惑度比较，N:M 模式。
- en: '| Method | Sparsity | 7B | 13B | 30B | 65B |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 稀疏率 | 7B | 13B | 30B | 65B |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Dense | - | 5.68 | 5.09 | 4.10 | 3.56 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Dense | - | 5.68 | 5.09 | 4.10 | 3.56 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SparseGPT | 4:8 | 8.61 | 7.40 | 6.17 | 5.38 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 4:8 | 8.61 | 7.40 | 6.17 | 5.38 |'
- en: '| w. DS\faBanT | 4:8 | 8.32 | 7.05 | 6.10 | 5.12 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 4:8 | 8.32 | 7.05 | 6.10 | 5.12 |'
- en: '| Wanda | 4:8 | 8.57 | 7.40 | 5.97 | 5.30 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 4:8 | 8.57 | 7.40 | 5.97 | 5.30 |'
- en: '| w. DS\faBanT | 4:8 | 8.45 | 7.25 | 5.91 | 5.26 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 4:8 | 8.45 | 7.25 | 5.91 | 5.26 |'
- en: '| SparseGPT | 2:4 | 11.00 | 9.11 | 7.16 | 6.28 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 2:4 | 11.00 | 9.11 | 7.16 | 6.28 |'
- en: '| w. DS\faBanT | 2:4 | 10.03 | 8.36 | 6.82 | 5.80 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 2:4 | 10.03 | 8.36 | 6.82 | 5.80 |'
- en: '| Wanda | 2:4 | 11.53 | 9.58 | 6.90 | 6.25 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 2:4 | 11.53 | 9.58 | 6.90 | 6.25 |'
- en: '| w. DS\faBanT | 2:4 | 10.89 | 9.05 | 6.76 | 6.14 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 2:4 | 10.89 | 9.05 | 6.76 | 6.14 |'
- en: 'Computing efficiency. We further demonstrate the computing efficiency of DS\faBanT.
    Following Wanda (Sun et al., [2023](#bib.bib48)), we only report the total pruning
    time and exclude the forward pass process shared by all methods. Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") compares the quantitative
    wall-clock overhead evaluated on NVIDIA A100 GPUs. It is indeed encouraging to
    observe that, as a fine-tuning approach, DS\faBanT maintains a comparable computing
    time to Wanda, while demonstrating significantly higher efficiency compared to
    SparseGPT.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '计算效率。我们进一步展示了DS\faBanT的计算效率。参考Wanda（Sun等， [2023](#bib.bib48)），我们仅报告了总剪枝时间，而排除了所有方法共享的前向传播过程。表[5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs")比较了在NVIDIA A100 GPU上评估的量化壁钟时间。作为一种微调方法，DS\faBanT保持了与Wanda相当的计算时间，同时与SparseGPT相比显示出显著更高的效率，确实令人鼓舞。'
- en: 'Comparison with LoRA Fine-tuning. To further demonstrate the ultra efficiency
    of DS\faBanT in terms of fine-tuning, we also compare DS\faBanT with parameter
    efficient fine-tuning (PEFT) method LoRA (Hu et al., [2021](#bib.bib21)). Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") presents a comparison
    of the time and performance of both methods in fine-tuning sparse LLaMA-7B. LoRA
    leverages the complete C4 dataset for a 5-hour fine-tuning and achieved a perplexity
    of 6.84\. In stark contrast, DS\faBanT only requires a brief duration of 4.3 s
    and 128 samples to deliver a comparable performance, 7.12 perplexity. Taking into
    consideration the additional network parameter burden incorporated by LoRA, the
    efficiency and practicality of DS\faBanT is hold.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '与LoRA微调的比较。为了进一步展示DS\faBanT在微调方面的超高效率，我们还将DS\faBanT与参数高效微调（PEFT）方法LoRA（Hu等，
    [2021](#bib.bib21)）进行了比较。表[5](#S4.T5 "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental
    Results ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning for Sparse
    LLMs")展示了在微调稀疏LLaMA-7B时两种方法的时间和性能比较。LoRA利用完整的C4数据集进行5小时的微调，得到了6.84的困惑度。相比之下，DS\faBanT仅需4.3秒和128个样本即可提供相当的性能，困惑度为7.12。考虑到LoRA所带来的额外网络参数负担，DS\faBanT的效率和实用性更具优势。'
- en: 'N:M Fine-grained Sparsity. Compared with unstructured sparsity, N:M fine-grained
    sparsity offers more practical speedup on the NVIDIA Ampere sparse tensor core (Nvidia,
    [2020](#bib.bib43)). Thus, we also evaluate the effectiveness of DS\faBanT on
    N:M fine-grained sparsity. Given the unique pattern of N:M sparsity that stipulates
    N non-zero components within M consecutive weight block, our implementation of
    DS\faBanT involves a restriction on the position of pruning-and-growing weights.
    In particular, we select the pruned weight within the same block as the revived
    weight, thus the N:M characteristic is still maintained after fine-tuning. Table [5](#S4.T5
    "Table 5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No
    Training \faBan: Training-Free Fine-tuning for Sparse LLMs") lists the results
    for pruning LLaMA-V1 model family at 2:4 and 4:8 sparse patterns. Interestingly,
    even with the aforementioned extra restriction, DS\faBanT can achieve more significant
    performance improvement compared to previous methods. For instance, when pruning
    LLaMA-V1 with 7B parameters, DS\faBanT archives a perplexity of 10.89, enhancing
    Wanda (11.53) by a noticeable 0.64 ppl. Similar findings can be concluded when
    it comes to other models and sparse patterns. These results highlight the effectiveness
    of DS\faBanT in boosting the performance of sparse LLMs, even with more complex
    sparsity constraints.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'N:M 精细稀疏性。与无结构稀疏性相比，N:M 精细稀疏性在 NVIDIA Ampere 稀疏张量核心上提供了更实际的加速（Nvidia，[2020](#bib.bib43)）。因此，我们还评估了
    DS\faBanT 在 N:M 精细稀疏性上的有效性。鉴于 N:M 稀疏性的独特模式规定了 M 个连续权重块中的 N 个非零分量，我们的 DS\faBanT
    实现涉及对修剪和增长权重的位置的限制。特别地，我们选择在同一块中修剪的权重作为复活的权重，因此在微调后 N:M 特性仍然保持。表 [5](#S4.T5 "Table
    5 ‣ 4.2 Language Modeling ‣ 4 Experimental Results ‣ Dynamic Sparse No Training
    \faBan: Training-Free Fine-tuning for Sparse LLMs") 列出了在 2:4 和 4:8 稀疏模式下修剪 LLaMA-V1
    模型家族的结果。有趣的是，即使有上述额外限制，DS\faBanT 相较于以前的方法仍能实现更显著的性能提升。例如，当修剪具有 7B 参数的 LLaMA-V1
    时，DS\faBanT 实现了 10.89 的困惑度，相较于 Wanda (11.53) 提升了显著的 0.64 ppl。对其他模型和稀疏模式也可以得出类似的结论。这些结果突显了
    DS\faBanT 在提升稀疏 LLM 性能方面的有效性，即使在更复杂的稀疏性约束下。'
- en: 'Table 6: Zero-shot Accuracy comparison for pruning LLaMA-V1 model family at
    60% sparsity rate.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在 60% 稀疏率下修剪 LLaMA-V1 模型家族的零样本准确率比较。
- en: '| Params | Method | PIQA | HellaSwag | StoryCloze | ARC-e | ARC-c | OBQA |
    Mean |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Params | Method | PIQA | HellaSwag | StoryCloze | ARC-e | ARC-c | OBQA |
    Mean |'
- en: '| 7B | Dense | 78.7 | 56.9 | 76.8 | 75.3 | 41.8 | 34.0 | 60.6 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 7B | Dense | 78.7 | 56.9 | 76.8 | 75.3 | 41.8 | 34.0 | 60.6 |'
- en: '| SparseGPT | 73.1 | 44.8 | 71.5 | 62.6 | 30.2 | 24.4 | 51.1 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 73.1 | 44.8 | 71.5 | 62.6 | 30.2 | 24.4 | 51.1 |'
- en: '| w. DS\faBanT | 73.7 | 47.2 | 72.3 | 62.8 | 30.9 | 29.4 | 52.7 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 73.7 | 47.2 | 72.3 | 62.8 | 30.9 | 29.4 | 52.7 |'
- en: '| Wanda | 73.0 | 43.6 | 69.7 | 62.8 | 30.3 | 25.0 | 50.7 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 73.0 | 43.6 | 69.7 | 62.8 | 30.3 | 25.0 | 50.7 |'
- en: '|  | w. DS\faBanT | 73.2 | 43.7 | 70.0 | 63.6 | 30.8 | 25.8 | 51.2 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | w. DS\faBanT | 73.2 | 43.7 | 70.0 | 63.6 | 30.8 | 25.8 | 51.2 |'
- en: '| 13B | Dense | 79.1 | 59.9 | 78.4 | 77.4 | 46.5 | 33.2 | 62.4 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 13B | Dense | 79.1 | 59.9 | 78.4 | 77.4 | 46.5 | 33.2 | 62.4 |'
- en: '| SparseGPT | 75.6 | 49.0 | 74.8 | 68.4 | 36.2 | 27.6 | 55.2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 75.6 | 49.0 | 74.8 | 68.4 | 36.2 | 27.6 | 55.2 |'
- en: '| w. DS\faBanT | 75.8 | 51.5 | 75.8 | 69.8 | 36.3 | 28.8 | 56.3 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 75.8 | 51.5 | 75.8 | 69.8 | 36.3 | 28.8 | 56.3 |'
- en: '| Wanda | 74.9 | 48.9 | 74.5 | 68.9 | 34.9 | 27.6 | 54.9 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 74.9 | 48.9 | 74.5 | 68.9 | 34.9 | 27.6 | 54.9 |'
- en: '|  | w. DS\faBanT | 75.0 | 49.1 | 75.1 | 69.2 | 35.4 | 28.0 | 55.3 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | w. DS\faBanT | 75.0 | 49.1 | 75.1 | 69.2 | 35.4 | 28.0 | 55.3 |'
- en: '| 30B | Dense | 81.1 | 63.3 | 79.1 | 80.4 | 52.9 | 36.0 | 65.4 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 30B | Dense | 81.1 | 63.3 | 79.1 | 80.4 | 52.9 | 36.0 | 65.4 |'
- en: '| SparseGPT | 76.8 | 55.0 | 78.4 | 74.7 | 43.3 | 32.2 | 60.1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 76.8 | 55.0 | 78.4 | 74.7 | 43.3 | 32.2 | 60.1 |'
- en: '| w. DS\faBanT | 77.3 | 58.0 | 78.8 | 74.8 | 45.6 | 32.8 | 61.2 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 77.3 | 58.0 | 78.8 | 74.8 | 45.6 | 32.8 | 61.2 |'
- en: '| Wanda | 77.7 | 56.7 | 79.1 | 76.2 | 46.5 | 31.6 | 61.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 77.7 | 56.7 | 79.1 | 76.2 | 46.5 | 31.6 | 61.3 |'
- en: '|  | w. DS\faBanT | 78.1 | 56.7 | 79.7 | 76.8 | 46.6 | 32.6 | 61.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | w. DS\faBanT | 78.1 | 56.7 | 79.7 | 76.8 | 46.6 | 32.6 | 61.7 |'
- en: '| 65B | Dense | 81.2 | 64.6 | 80.2 | 81.3 | 52.9 | 38.2 | 66.4 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 65B | Dense | 81.2 | 64.6 | 80.2 | 81.3 | 52.9 | 38.2 | 66.4 |'
- en: '| SparseGPT | 79.6 | 58.3 | 80.5 | 77.4 | 46.6 | 33.4 | 62.6 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| SparseGPT | 79.6 | 58.3 | 80.5 | 77.4 | 46.6 | 33.4 | 62.6 |'
- en: '| w. DS\faBanT | 79.9 | 59.8 | 80.4 | 78.1 | 46.9 | 34.6 | 63.3 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| w. DS\faBanT | 79.9 | 59.8 | 80.4 | 78.1 | 46.9 | 34.6 | 63.3 |'
- en: '| Wanda | 79.9 | 58.9 | 80.6 | 78.2 | 47.1 | 34.8 | 63.3 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Wanda | 79.9 | 58.9 | 80.6 | 78.2 | 47.1 | 34.8 | 63.3 |'
- en: '|  | w. DS\faBanT | 80.9 | 59.6 | 80.2 | 78.2 | 47.7 | 36.0 | 63.7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | w. DS\faBanT | 80.9 | 59.6 | 80.2 | 78.2 | 47.7 | 36.0 | 63.7 |'
- en: 4.3 Zero-shot Tasks
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 零样本任务
- en: Following (Frantar & Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)),
    we also provided the accuracy performance of the LLaMA-V1 model family pruned
    at 50% sparsity rate on seven downstream zero-shot tasks. Averaging the accuracy
    over all tasks suggests DS\faBanT’s efficacy for enhancing sparse LLMs of any
    size. Particularly, DS\faBanT improves the average accuracy of SparseGPT by 1.6%
    when pruning LLaMA-V1-7B (52.7% for DS\faBanT and 51.1% for SparseGPT). For task-wise
    performance, DS\faBanT is beneficial on all tasks, while there is not a fixed
    superiority for fine-tuning models obtained by different pruning methods. This
    phenomenon may evidence the reported relatively noisy evaluation results from
    these zero-shot experiments (Dettmers et al., [2022](#bib.bib8)). However, the
    advantages of consistent performance improvement and efficiency of DS\faBanT for
    zero-shot tasks are obvious.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据(Frantar & Alistarh, [2023](#bib.bib13); Sun等，[2023](#bib.bib48))，我们还提供了LLaMA-V1模型家族在50%稀疏率下修剪后的七个下游零样本任务的准确性表现。对所有任务的平均准确率表明，DS\faBanT在提升任何大小的稀疏LLMs方面的有效性。特别是，当修剪LLaMA-V1-7B时，DS\faBanT将SparseGPT的平均准确率提高了1.6%（DS\faBanT为52.7%，SparseGPT为51.1%）。在任务级性能方面，DS\faBanT对所有任务都有益，而不同修剪方法获得的微调模型没有固定的优势。这一现象可能证明了这些零样本实验中报告的相对噪声评价结果（Dettmers等，[2022](#bib.bib8)）。然而，DS\faBanT在零样本任务中的一致性能提升和效率优势是显而易见的。
- en: 4.4 Performance Analysis
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 性能分析
- en: Next, we investigate the influence of the components within DS\faBanT, unfolds
    as its update schedule, pruning-and-growing criteria, and robustness to calibration
    samples. All experimental setups are based on the LLaMA-7B model pruned by the
    Wanda metric (Sun et al., [2023](#bib.bib48)) with 60% sparsity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了DS\faBanT组件的影响，包括其更新计划、修剪和增长标准以及对校准样本的鲁棒性。所有实验设置都基于通过Wanda度量（Sun等，[2023](#bib.bib48)）修剪的60%稀疏度的LLaMA-7B模型。
- en: '![Refer to caption](img/83fc538bf46d6b044cec5280489f131f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83fc538bf46d6b044cec5280489f131f.png)'
- en: 'Figure 3: (left) Effect of the update schedule ($T,\epsilon$) and (right) number
    of calibration sequences.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: （左）更新计划的效果（$T,\epsilon$）和（右）校准序列数量。'
- en: 'Update schedule. In Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Performance Analysis
    ‣ 4 Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs") (left), we examine the performance of DS\faBanT under different
    hyper-parameter setting for the update schedule, including the maximum cycle $C$.
    The best performance is obtained with 50 cycles and 0.1 updating threshold. To
    analyze, smaller $C$ both lead to an insufficient procedure for the decrease in
    reconstruction error. In contrast, running DS\faBanT without termination conditions
    also resulted in poor performance, most likely due to over-fitting of calibration
    data.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '更新计划。在图 [3](#S4.F3 "图 3 ‣ 4.4 性能分析 ‣ 4 实验结果 ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏LLMs")（左）中，我们检查了在不同超参数设置下DS\faBanT的性能，包括最大循环次数
    $C$。最佳性能是在50个循环和0.1的更新阈值下获得的。分析表明，较小的 $C$ 会导致重建误差减少过程不足。相比之下，在没有终止条件的情况下运行DS\faBanT也导致了较差的性能，这很可能是由于校准数据的过拟合。'
- en: 'Robustness to calibration samples. In Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Performance
    Analysis ‣ 4 Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free
    Fine-tuning for Sparse LLMs") (right), we show the performance of pruning methods
    with varying numbers of sampled sequences for calibration. As can be observed,
    SparseGPT suffers serious performance degradation when calibration samples are
    limited, mostly due to the difficulty in estimating Hessian inverses in such cases.
    Fortunately, DS\faBanT consistently the performance of SparseGPT, even if only
    very few samples are given. These results further highlight the robustness of
    DS\faBanT for mitigating the reconstruction error.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '对校准样本的鲁棒性。在图 [3](#S4.F3 "图 3 ‣ 4.4 性能分析 ‣ 4 实验结果 ‣ 动态稀疏无训练 \faBan: 无训练微调稀疏LLMs")（右）中，我们展示了不同数量的校准样本下修剪方法的性能。如图所示，当校准样本有限时，SparseGPT的性能严重下降，这主要是由于在这种情况下估计Hessian逆矩阵的难度。幸运的是，DS\faBanT在只提供很少样本的情况下依然能保持SparseGPT的性能。这些结果进一步突显了DS\faBanT在减轻重建误差方面的鲁棒性。'
- en: 'Table 7: Effect of the pruning and growing criteria.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 修剪和增长标准的效果。'
- en: '| Pruning
    Growing
    | $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    |  Eq. ([3](#S3.E3 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) |   Eq. ([2](#S3.E2
    "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan:
    Training-Free Fine-tuning for Sparse LLMs")) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝
    生长
    | $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    |  公式 ([3](#S3.E3 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调"))
    |   公式 ([2](#S3.E2 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调"))
    |'
- en: '| --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    | 10.72 | 10.49 | 10.27 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;\mathbf{W}_{r,k}&#124;\cdot&#124;&#124;\mathbf{A}_{r}&#124;&#124;_{2}$
    | 10.72 | 10.49 | 10.27 |'
- en: '| Eq. ([2](#S3.E2 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) | 11.24 | 10.61
    | 10.84 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 公式 ([2](#S3.E2 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调"))
    | 11.24 | 10.61 | 10.84 |'
- en: '| Eq. ([3](#S3.E3 "In 3 Dynamic Sparse No Training – DS\faBanT ‣ Dynamic Sparse
    No Training \faBan: Training-Free Fine-tuning for Sparse LLMs")) | 10.52 | 10.37
    | 10.22 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 公式 ([3](#S3.E3 "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调"))
    | 10.52 | 10.37 | 10.22 |'
- en: 'Pruning-and-growing criteria. We further investigate the influence on criteria
    for prune and grow in Table [7](#S4.T7 "Table 7 ‣ 4.4 Performance Analysis ‣ 4
    Experimental Results ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs"). Note that when we transfer Eq. ([2](#S3.E2 "In 3 Dynamic Sparse
    No Training – DS\faBanT ‣ Dynamic Sparse No Training \faBan: Training-Free Fine-tuning
    for Sparse LLMs")) to the prune criteria, the election of extreme values is also
    correspondingly reversed. As for the prune criterion, it can be seen that pruning
    weights that could bring the most reduction in reconstruction error actually led
    to a significant performance decrease. This indicates that while pursuing the
    reduction of reconstruction error, it is also essential to keep weights that exhibit
    an extremely large influence on the output, *e.g.*, weights within outlier channel.
    On the other hand, our proposed criteria based on the expectation and variance
    of the reconstruction error reduction achieved the best results among all growing
    criteria.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝和生长标准。我们进一步研究了表 [7](#S4.T7 "表 7 ‣ 4.4 性能分析 ‣ 4 实验结果 ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调")中剪枝和生长标准的影响。请注意，当我们将公式 ([2](#S3.E2
    "在 3 动态稀疏无训练 – DS\faBanT ‣ 动态稀疏无训练 \faBan: 训练自由的稀疏LLMs微调")) 转换为剪枝标准时，极端值的选择也相应地发生了反转。至于剪枝标准，可以看出，剪除那些可以带来最大重建误差减少的权重实际上会导致显著的性能下降。这表明，在追求重建误差减少的同时，也需要保留那些对输出具有极大影响的权重，例如，异常通道中的权重。另一方面，我们提出的基于重建误差减少的期望和方差的标准在所有生长标准中取得了最佳结果。'
- en: 5 Conclusion
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce DS\faBanT, a training-free fine-tuning approach that
    enhances the performance of sparse LLMs without the expensive backpropagation
    or any weight updates. Taking inspiration from the success of sparse training
    in the pre-LLM pruning age, DS\faBanT adapts iterative weights growing and pruning
    in a sparse LLM, with a transferred target for minimizing the reconstruction error
    between dense and sparse LLMs outputs. To furnish guidance in the selection of
    weights to be pruned and grown, we introduce novel criteria that take into account
    the expectation and variance of the reconstruction error reduction by growing
    each weight concerning different inputs. Extensive experiments on pruning representative
    LLMs across various language benchmarks demonstrate the efficiency and effectiveness
    of DS\faBanT in boosting the performance of sparse LLMs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了 DS\faBanT，一种无训练的微调方法，它在没有昂贵的反向传播或任何权重更新的情况下提升了稀疏 LLM 的性能。借鉴稀疏训练在
    pre-LLM 剪枝时代的成功经验，DS\faBanT 在稀疏 LLM 中适应了迭代权重增长和剪枝，并通过最小化稠密和稀疏 LLM 输出之间的重建误差来转移目标。为了指导权重的剪枝和增长选择，我们引入了新的标准，考虑了通过对不同输入增长每个权重来减少重建误差的期望和方差。在各种语言基准上对具有代表性的
    LLM 进行的广泛实验展示了 DS\faBanT 在提升稀疏 LLM 性能方面的效率和效果。
- en: Acknowledgement
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by National Key R&D Program of China (No.2022ZD0118202),
    the National Science Fund for Distinguished Young Scholars (No.62025603), the
    National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No.
    62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389,
    No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province
    of China (No.2021J01002, No.2022J06001).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了中国国家重点研发计划（编号：2022ZD0118202）、国家杰出青年科学基金（编号：62025603）、中国国家自然科学基金（编号：U21B2037、U22B2051、62176222、62176223、62176226、62072386、62072387、62072389、62002305
    和 62272401）、以及福建省自然科学基金（编号：2021J01002、2022J06001）的支持。
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence (AAAI)*, volume 34, pp.  7432–7439,
    2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等（2020）**Yonatan Bisk**、**Rowan Zellers**、**Jianfeng Gao**、**Yejin Choi**
    等人。Piqa：在自然语言中推理关于物理常识的问题。收录于 *AAAI 人工智能会议论文集（AAAI）*，第 34 卷，第 7432–7439 页，2020年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems (NeurIPs)*, 33:1877–1901, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）**Tom Brown**、**Benjamin Mann**、**Nick Ryder**、**Melanie Subbiah**、**Jared
    D Kaplan**、**Prafulla Dhariwal**、**Arvind Neelakantan**、**Pranav Shyam**、**Girish
    Sastry**、**Amanda Askell** 等人。语言模型是少样本学习者。*神经信息处理系统进展（NeurIPs）*，33:1877–1901，2020年。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等（2023）**Sébastien Bubeck**、**Varun Chandrasekaran**、**Ronen Eldan**、**Johannes
    Gehrke**、**Eric Horvitz**、**Ece Kamar**、**Peter Lee**、**Yin Tat Lee**、**Yuanzhi
    Li**、**Scott Lundberg** 等人。人工通用智能的火花：与 gpt-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*，2023年。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等（2023）**Wei-Lin Chiang**、**Zhuohan Li**、**Zi Lin**、**Ying Sheng**、**Zhanghao
    Wu**、**Hao Zhang**、**Lianmin Zheng**、**Siyuan Zhuang**、**Yonghao Zhuang**、**Joseph
    E Gonzalez** 等人。Vicuna：一个开放源代码的聊天机器人，以 90%* chatgpt 质量打动 gpt-4。*见 https://vicuna.lmsys.org（访问日期：2023年4月14日）*，2023年。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）**Peter Clark**、**Isaac Cowhey**、**Oren Etzioni**、**Tushar Khot**、**Ashish
    Sabharwal**、**Carissa Schoenick** 和 **Oyvind Tafjord**。认为你解决了问答问题？试试 arc，ai2 推理挑战。*arXiv
    预印本 arXiv:1803.05457*，2018年。
- en: 'Dettmers & Zettlemoyer (2019) Tim Dettmers and Luke Zettlemoyer. Sparse networks
    from scratch: Faster training without losing performance. *arXiv preprint arXiv:1907.04840*,
    2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers & Zettlemoyer（2019）**Tim Dettmers** 和 **Luke Zettlemoyer**。从零开始的稀疏网络：无需损失性能的更快训练。*arXiv
    预印本 arXiv:1907.04840*，2019年。
- en: 'Dettmers & Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning (ICML)*, pp.  7750–7774\. PMLR, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers & Zettlemoyer (2023) Tim Dettmers 和 Luke Zettlemoyer。4 位精度的案例：k 位推理缩放规律。在
    *国际机器学习大会（ICML）*，第 7750–7774 页。PMLR，2023 年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems (NeurIPs)*, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。LLM.
    int8 (): 大规模变换器的 8 位矩阵乘法。*神经信息处理系统进展（NeurIPs）*，2022 年。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis
    Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh。SPQR：一种稀疏量化表示用于近乎无损的LLM权重压缩。*arXiv预印本 arXiv:2306.03078*，2023 年。
- en: 'Ding et al. (2017) Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei
    Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, et al. Circnn: accelerating and
    compressing deep neural networks using block-circulant weight matrices. In *Proceedings
    of the 50th Annual IEEE/ACM International Symposium on Microarchitecture*, pp. 
    395–408, 2017.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2017) Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei
    Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan 等。CIRCNN：使用块循环权重矩阵加速和压缩深度神经网络。在
    *第 50 届年度 IEEE/ACM 国际微架构研讨会论文集*，第 395–408 页，2017 年。
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning (ICML)*, pp.  2943–2952, 2020.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    和 Erich Elsen。操控彩票：让所有票据获胜。在 *国际机器学习大会（ICML）*，第 2943–2952 页，2020 年。
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *International Conference
    on Learning Representations (ICLR)*, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2019) Jonathan Frankle 和 Michael Carbin。彩票票据假设：寻找稀疏的、可训练的神经网络。在
    *学习表示国际会议（ICLR）*，2019 年。
- en: Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Massive language models
    can be accurately pruned in one-shot. In *International Conference on Machine
    Learning (ICML)*, 2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) Elias Frantar 和 Dan Alistarh。大规模语言模型可以通过一次性剪枝准确地减少。在
    *国际机器学习大会（ICML）*，2023 年。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training compression for generative pretrained transformers.
    In *International Conference on Learning Representations (ICLR)*, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh。GPTQ：生成预训练变换器的准确后训练压缩。在 *学习表示国际会议（ICLR）*，2022 年。
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2019) Trevor Gale, Erich Elsen, 和 Sara Hooker。深度神经网络中的稀疏性现状。*arXiv预印本
    arXiv:1902.09574*，2019 年。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, 2021.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff
    等。用于少样本语言模型评估的框架。*版本 v0.0.1. 九月*，2021 年。
- en: Gray et al. (2017) Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels
    for block-sparse weights. *arXiv preprint arXiv:1711.09224*, 3:2, 2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray et al. (2017) Scott Gray, Alec Radford, 和 Diederik P Kingma。用于块稀疏权重的 GPU
    核心。*arXiv预印本 arXiv:1711.09224*，3:2，2017 年。
- en: Han et al. (2015) Song Han, Jeff Pool, John Tran, and William Dally. Learning
    both weights and connections for efficient neural network. In *Advances in Neural
    Information Processing Systems (NeurIPS)*, pp.  1135–1143, 2015.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Song Han, Jeff Pool, John Tran, 和 William Dally。学习权重和连接以提高神经网络的效率。在
    *神经信息处理系统进展（NeurIPS）*，第 1135–1143 页，2015 年。
- en: 'Hassibi & Stork (1992) Babak Hassibi and David Stork. Second order derivatives
    for network pruning: Optimal brain surgeon. In *Advances in Neural Information
    Processing Systems (NeurIPS)*, pp.  164–171, 1992.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi & Stork (1992) Babak Hassibi 和 David Stork。网络剪枝的二阶导数：最佳大脑外科医生。在 *神经信息处理系统进展（NeurIPS）*，第
    164–171 页，1992 年。
- en: Hassibi et al. (1993) Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
    brain surgeon and general network pruning. In *IEEE international conference on
    neural networks*, pp.  293–299\. IEEE, 1993.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) Babak Hassibi, David G Stork, 和 Gregory J Wolff. 最优脑外科医生和一般网络修剪。发表于
    *IEEE 国际神经网络会议*，第 293–299 页。IEEE，1993 年。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. LORA: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021
    年。'
- en: 'Huang et al. (2022) Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovski,
    Jiaxu Zhao, Lu Yin, Yulong Pei, Decebal Constantin Mocanu, Zhangyang Wang, Mykola
    Pechenizkiy, et al. You can have better graph neural networks by not training
    weights at all: Finding untrained gnns tickets. *arXiv preprint arXiv:2211.15335*,
    2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovski,
    Jiaxu Zhao, Lu Yin, Yulong Pei, Decebal Constantin Mocanu, Zhangyang Wang, Mykola
    Pechenizkiy 等。你可以通过完全不训练权重来获得更好的图神经网络：寻找未训练的 GNNs 门票。*arXiv 预印本 arXiv:2211.15335*，2022
    年。
- en: 'Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph
    Naor, and Daniel Soudry. Accelerated sparse neural training: A provable and efficient
    method to find n: m transposable masks. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 34:21099–21111, 2021.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara et al. (2021) Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph
    Naor, 和 Daniel Soudry. 加速稀疏神经训练：一种可证明且高效的方法来寻找 N: M 可转置掩码。*Advances in Neural
    Information Processing Systems (NeurIPs)*，34:21099–21111，2021 年。'
- en: 'Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, 和 Zhangyang Wang.
    大型预训练模型中关键稀疏性的出现：重要的权重。*arXiv 预印本 arXiv:2306.03805*，2023 年。
- en: 'Jayakumar et al. (2020) Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon
    Osindero, and Erich Elsen. Top-kast: Top-k always sparse training. *Advances in
    Neural Information Processing Systems (NeurIPs)*, 33:20744–20754, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jayakumar et al. (2020) Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon
    Osindero, 和 Erich Elsen. Top-Kast: Top-K 始终稀疏训练。*Advances in Neural Information
    Processing Systems (NeurIPs)*，33:20744–20754，2020 年。'
- en: Jiang et al. (2018) Chunhui Jiang, Guiying Li, Chao Qian, and Ke Tang. Efficient
    dnn neuron pruning by minimizing layer-wise nonlinear reconstruction error. In
    *IJCAI*, volume 2018, pp.  2–2, 2018.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2018) Chunhui Jiang, Guiying Li, Chao Qian, 和 Ke Tang. 通过最小化层级非线性重建误差来高效修剪
    DNN 神经元。发表于 *IJCAI*，卷 2018，第 2–2 页，2018 年。
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, 和 Dan Alistarh. 最优 BERT 外科医生：大规模语言模型的可扩展和准确的二阶修剪。*arXiv
    预印本 arXiv:2203.07259*，2022 年。
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. Optimal brain damage.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  598–605,
    1989.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) Yann LeCun, John Denker, 和 Sara Solla. 最优脑损伤。发表于 *Advances
    in Neural Information Processing Systems (NeurIPS)*，第 598–605 页，1989 年。
- en: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip:
    Single-shot network pruning based on connection sensitivity. In *International
    Conference on Learning Representations (ICLR)*, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2019) Namhoon Lee, Thalaiyasingam Ajanthan, 和 Philip Torr. Snip:
    基于连接敏感性的单次网络修剪。发表于 *International Conference on Learning Representations (ICLR)*，2019
    年。'
- en: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter
    Graf. Pruning filters for efficient convnets. In *International Conference on
    Learning Representations (ICLR)*, 2017.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017) Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, 和 Hans Peter
    Graf. 为高效的卷积网络修剪过滤器。发表于 *International Conference on Learning Representations
    (ICLR)*，2017 年。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. AWQ: 基于激活的权重量化用于 LLM 压缩和加速。*arXiv 预印本 arXiv:2306.00978*，2023 年。'
- en: Liu et al. (2019) S Liu, DC Mocanu, ARR Matavalam, Y Pei, and M Pechenizkiy.
    Sparse evolutionary deep learning with over one million artificial neurons on
    commodity hardware. arxiv. *arXiv preprint arXiv:1901.09181*, 2019.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) S Liu, DC Mocanu, ARR Matavalam, Y Pei, 和 M Pechenizkiy. 使用超过一百万个人工神经元的稀疏进化深度学习在普通硬件上的应用。arxiv.
    *arXiv 预印本 arXiv:1901.09181*，2019 年。
- en: Liu et al. (2021) Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola
    Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization
    in sparse training. In *International Conference on Machine Learning*, pp.  6989–7000\.
    PMLR, 2021.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2021) 刘世伟、尉璘、德塞巴尔·康斯坦丁·莫卡努和米科拉·佩切尼茨基。我们真的需要密集的过参数化吗？稀疏训练中的即时过参数化。发表于
    *国际机器学习会议*，第6989-7000页，PMLR，2021。
- en: 'Liu et al. (2023) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal, and Zhangyang Wang. Sparsity may cry: Let us fail (current)
    sparse neural networks together! *arXiv preprint arXiv:2303.02141*, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) 刘世伟、陈天龙、张振宇、陈旭熙、黄天骞、阿贾伊·贾伊斯瓦尔和张杨·王。稀疏性可能会哭泣：让我们一起失败（当前）稀疏神经网络！*arXiv
    预印本 arXiv:2303.02141*，2023。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 (2023) 马鑫银、方功凡和王新超。Llm-pruner：大规模语言模型的结构化剪枝。*arXiv 预印本 arXiv:2305.11627*，2023。
- en: 'Mallya et al. (2018) Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback:
    Adapting a single network to multiple tasks by learning to mask weights. In *Proceedings
    of the European conference on computer vision (ECCV)*, pp.  67–82, 2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallya 等 (2018) 阿伦·马利亚、迪伦·戴维斯和斯维特拉娜·拉泽布尼克。Piggyback：通过学习遮蔽权重将单一网络适应于多任务。发表于
    *欧洲计算机视觉大会 (ECCV) 论文集*，第67-82页，2018。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2016) 斯蒂芬·梅里蒂、蔡铭雄、詹姆斯·布拉德伯里和理查德·索彻。指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等 (2018) 托多尔·米哈伊洛夫、彼得·克拉克、图沙尔·科特和阿希什·萨巴瓦尔。盔甲能导电吗？一个新的开放书籍问答数据集。*arXiv
    预印本 arXiv:1809.02789*，2018。
- en: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
    Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial
    neural networks with adaptive sparse connectivity inspired by network science.
    *Nature Communications*, 9:1–12, 2018.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocanu 等 (2018) 德塞巴尔·康斯坦丁·莫卡努、埃琳娜·莫卡努、彼得·斯通、阮芳、马德琳·吉贝斯库和安东尼奥·利奥塔。受网络科学启发的适应性稀疏连接的人工神经网络可扩展训练。*自然通讯*，第9:1-12，2018。
- en: Molchanov et al. (2017) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource efficient inference.
    In *International Conference on Learning Representations (ICLR)*, 2017.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov 等 (2017) 帕夫洛·莫尔查诺夫、斯蒂芬·泰里、特罗·卡拉斯、蒂莫·艾拉和詹·考茨。为资源高效推理修剪卷积神经网络。发表于 *国际学习表征会议
    (ICLR)*，2017。
- en: Mostafa & Wang (2019) Hesham Mostafa and Xin Wang. Parameter efficient training
    of deep convolutional neural networks by dynamic sparse reparameterization. In
    *International Conference on Machine Learning (ICML)*, pp.  4646–4655, 2019.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafa & Wang (2019) 哈沙姆·莫斯塔法和辛·王。通过动态稀疏重参数化进行深度卷积神经网络的参数高效训练。发表于 *国际机器学习会议
    (ICML)*，第4646-4655页，2019。
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis,
    Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze
    test. In *Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
    and Discourse-level Semantics*, pp.  46–51, 2017.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh 等 (2017) 纳斯林·莫斯塔法扎德、迈克尔·罗斯、安妮·路易斯、纳塔尼埃尔·钱伯斯和詹姆斯·艾伦。Lsdsem 2017
    共享任务：故事填空测试。发表于 *第2届词汇、句法和话语级语义模型链接研讨会论文集*，第46-51页，2017。
- en: Nvidia (2020) Nvidia. Nvidia a100 tensor core gpu architecture, 2020. [https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf](https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia (2020) Nvidia。Nvidia a100 张量核心 GPU 架构，2020。 [https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf](https://www.nvidia.com/content/dam/enzz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf)。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, pp.  8026–8037,
    2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等 (2019) 亚当·帕斯克、萨姆·格罗斯、弗朗西斯科·马萨、亚当·莱勒、詹姆斯·布拉德伯里、格雷戈里·查南、特雷弗·基林、林泽铭、娜塔利亚·吉梅尔谢因、卢卡·安蒂加
    等。Pytorch：一种命令式风格的高性能深度学习库。发表于 *神经信息处理系统进展 (NeurIPS)*，第8026-8037页，2019。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。使用统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*，21(1):5485–5551，2020。
- en: Ramanujan et al. (2020) Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi,
    Ali Farhadi, and Mohammad Rastegari. What’s hidden in a randomly weighted neural
    network? In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*,
    pp.  11893–11902, 2020.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramanujan 等（2020）Vivek Ramanujan、Mitchell Wortsman、Aniruddha Kembhavi、Ali Farhadi
    和 Mohammad Rastegari。随机权重神经网络中隐藏了什么？在*IEEE 计算机视觉与模式识别会议（CVPR）*，第 11893–11902 页，2020。
- en: Schaeffer et al. (2023) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are
    emergent abilities of large language models a mirage? *arXiv preprint arXiv:2304.15004*,
    2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaeffer 等（2023）Rylan Schaeffer、Brando Miranda 和 Sanmi Koyejo。大型语言模型的突现能力是海市蜃楼吗？*arXiv
    预印本 arXiv:2304.15004*，2023。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）Mingjie Sun、Zhuang Liu、Anna Bair 和 J Zico Kolter。针对大型语言模型的一种简单有效的修剪方法。*arXiv
    预印本 arXiv:2306.11695*，2023。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。Llama
    2：开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita 等（2019）Elena Voita、David Talbot、Fedor Moiseev、Rico Sennrich 和 Ivan Titov。分析多头自注意力：专门化头部承担主要任务，其余部分可以被修剪。*arXiv
    预印本 arXiv:1905.09418*，2019。
- en: Wang et al. (2020) Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning
    tickets before training by preserving gradient flow. In *International Conference
    on Learning Representations (ICLR)*, 2020.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）Chaoqi Wang、Guodong Zhang 和 Roger Grosse。通过保留梯度流在训练前选择获胜票据。在*国际学习表征会议（ICLR）*，2020。
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022a.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022a）Jason Wei、Yi Tay、Rishi Bommasani、Colin Raffel、Barret Zoph、Sebastian
    Borgeaud、Dani Yogatama、Maarten Bosma、Denny Zhou、Donald Metzler 等。大型语言模型的突现能力。*机器学习研究汇刊*，2022a。
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 35:24824–24837, 2022b.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022b）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou 等。链式思维提示在大型语言模型中引发推理。*神经信息处理系统进展（NeurIPs）*，35:24824–24837，2022b。
- en: Wen et al. (2016) Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
    Learning structured sparsity in deep neural networks. *Advances in Neural Information
    Processing Systems (NeurIPs)*, 29, 2016.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2016）Wei Wen、Chunpeng Wu、Yandan Wang、Yiran Chen 和 Hai Li。学习深度神经网络中的结构化稀疏性。*神经信息处理系统进展（NeurIPs）*，29，2016。
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等（2019）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement Delangue、Anthony
    Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz 等。Huggingface 的 transformers：最先进的自然语言处理。*arXiv
    预印本 arXiv:1910.03771*，2019。
- en: Wortsman et al. (2020) Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha
    Kembhavi, Mohammad Rastegari, Jason Yosinski, and Ali Farhadi. Supermasks in superposition.
    *Advances in Neural Information Processing Systems (NeurIPs)*, 33:15173–15184,
    2020.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wortsman et al. (2020) 米切尔·沃茨曼、维韦克·拉马努詹、罗桑·刘、阿尼鲁德·肯巴维、穆罕默德·拉斯特加里、杰森·约辛斯基、和阿里·法赫迪。超掩码的叠加。*神经信息处理系统进展（NeurIPs）*，33:15173–15184，2020年。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning (ICML)*,
    pp.  38087–38099\. PMLR, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) 肖广轩、林际、米凯尔·塞兹内克、吴浩、朱利安·德穆斯、和宋汉。Smoothquant: 对大型语言模型的精确高效后训练量化。见*国际机器学习会议（ICML）*，页38087–38099。PMLR，2023年。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems (NeurIPs)*, 35:27168–27183, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) 姚哲伟、雷扎·雅兹达尼·阿敏阿巴迪、张敏佳、吴晓霞、李从龙、和何宇雄。Zeroquant: 高效且经济的大规模变换器后训练量化。*神经信息处理系统进展（NeurIPs）*，35:27168–27183，2022年。'
- en: 'Yin et al. (2023) Lu Yin, Shiwei Liu, Meng Fang, Tianjin Huang, Vlado Menkovski,
    and Mykola Pechenizkiy. Lottery pools: Winning more by interpolating tickets without
    increasing training or inference cost. In *Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI)*, volume 37, pp.  10945–10953, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2023) 尹璐、刘士伟、方萌、黄天进、弗拉多·门科夫斯基、和米科拉·佩切尼兹基。彩票池：通过插值票据而不增加训练或推理成本来赢得更多。见*AAAI人工智能会议论文集（AAAI）*，第37卷，页10945–10953，2023年。
- en: 'Yuan et al. (2021) Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong,
    Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate
    and fast memory-economic sparse training framework on the edge. *Advances in Neural
    Information Processing Systems (NeurIPs)*, 34:20838–20850, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. (2021) 袁远、马小龙、牛伟、李正刚、孔正伦、刘宁、龚一凡、詹征、何超阳、金青等。Mest: 精确且快速的边缘内存经济稀疏训练框架。*神经信息处理系统进展（NeurIPs）*，34:20838–20850，2021年。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪、和叶锦。Hellaswag: 机器真的能完成你的句子吗？*arXiv
    预印本 arXiv:1905.07830*，2019年。'
- en: 'Zhang et al. (2022a) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022a.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022a) 张苏珊、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿尔特克斯、莫亚·陈、舒慧·陈、克里斯托弗·德万、莫娜·迪亚布、李贤、林维多利亚等。Opt:
    开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022a年。'
- en: 'Zhang et al. (2022b) Yuxin Zhang, Mingbao Lin, Zhihang Lin, Yiting Luo, Ke Li,
    Fei Chao, Yongjian Wu, and Rongrong Ji. Learning best combination for efficient
    n: M sparsity. In *Advances in Neural Information Processing Systems (NeurIPS)*,
    2022b.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022b) 张宇欣、林名宝、林志航、罗宜婷、李可、赵飞、吴永健、和季戎戎。学习有效的n: M 稀疏最佳组合。见*神经信息处理系统进展（NeurIPS）*，2022b。'
- en: Zhang et al. (2023) Yuxin Zhang, Mingbao Lin, Fei Chao, Yan Wang, Ke Li, Yunhang
    Shen, Yongjian Wu, and Rongrong Ji. Lottery jackpots exist in pre-trained models.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) 张宇欣、林名宝、赵飞、王彦、李可、沈云航、吴永健、和季戎戎。彩票大奖存在于预训练模型中。*IEEE模式分析与机器智能汇刊（TPAMI）*，2023年。
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: M fine-grained structured
    sparse neural networks from scratch. In *International Conference on Learning
    Representations (ICLR)*, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2021) 周傲君、马玉坤、朱俊楠、刘建博、张智杰、袁坤、孙文秀、和李鸿胜。从头开始学习n: M 细粒度结构稀疏神经网络。见*学习表征国际会议（ICLR）*，2021年。'
- en: 'Zhou et al. (2019) Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
    Deconstructing lottery tickets: zeros, signs, and the supermask. In *Advances
    in Neural Information Processing Systems (NeurIPS)*, pp.  3597–3607, 2019.*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2019) 周海蒂、兰贞、罗桑·刘、和杰森·约辛斯基。解构彩票票据：零、符号和超掩码。见*神经信息处理系统进展（NeurIPS）*，页3597–3607，2019年。
