- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MOELoRA: 一种基于MOE的多任务医学应用参数高效微调方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.18339](https://ar5iv.labs.arxiv.org/html/2310.18339)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.18339](https://ar5iv.labs.arxiv.org/html/2310.18339)
- en: Qidong Liu Xi’an Jiaotong University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 邱东 刘 西安交通大学
- en: City University of Hong Kong Xi’an, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 香港城市大学 西安，中国
- en: liuqidong@stu.xjtu.edu.cn    Xian Wu 🖂 Corresponding Authors Jarvis Research
    Center, Tencent YouTu Lab Shenzhen, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: liuqidong@stu.xjtu.edu.cn    吴显 🖂 通讯作者 贾维斯研究中心，腾讯优图实验室 深圳，中国
- en: kevinxwu@tencent.com    Xiangyu Zhao 🖂 {@IEEEauthorhalign} Yuanshao Zhu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: kevinxwu@tencent.com    赵向宇 🖂 {@IEEEauthorhalign} 朱元绍
- en: Feng Tian City University of Hong Kong Hong Kong, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 田锋 香港城市大学 香港，中国
- en: xianzhao@cityu.edu.hk Southern University of Science and Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: xianzhao@cityu.edu.hk 南方科技大学
- en: City University of Hong Kong Shenzhen, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 香港城市大学 深圳，中国
- en: zhuys2019@mail.sustech.edu.cn Xia’an Jiaotong University Xi’an, China
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: zhuys2019@mail.sustech.edu.cn 西安交通大学 西安，中国
- en: fengtian@mail.xjtu.edu.cn    Derong Xu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: fengtian@mail.xjtu.edu.cn    德荣 徐
- en: Yefeng Zheng University of Science and Technology of China
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 郑业锋 中国科学技术大学
- en: City University of Hong Kong Hefei, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 香港城市大学 合肥，中国
- en: derongxu@mail.ustc.edu.cn Jarvis Research Center, Tencent YouTu Lab Shenzhen,
    China
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: derongxu@mail.ustc.edu.cn 贾维斯研究中心，腾讯优图实验室 深圳，中国
- en: yefengzheng@tencent.com
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: yefengzheng@tencent.com
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The recent surge in the field of Large Language Models (LLMs) has gained significant
    attention in numerous domains. In order to tailor an LLM to a specific domain
    such as a web-based healthcare system, fine-tuning with domain knowledge is necessary.
    However, two issues arise during fine-tuning LLMs for medical applications. The
    first is the problem of task variety, where there are numerous distinct tasks
    in real-world medical scenarios. This diversity often results in suboptimal fine-tuning
    due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning
    can be prohibitive, impeding the application of LLMs. The large number of parameters
    in LLMs results in enormous time and computational consumption during fine-tuning,
    which is difficult to justify. To address these two issues simultaneously, we
    propose a novel parameter-efficient fine-tuning framework for multi-task medical
    applications called MOELoRA. The framework aims to capitalize on the benefits
    of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning.
    To unify MOE and LoRA, we devise multiple experts as the trainable parameters,
    where each expert consists of a pair of low-rank matrices to maintain a small
    number of trainable parameters. Additionally, we propose a task-motivated gate
    function for all MOELoRA layers that can regulate the contributions of each expert
    and generate distinct parameters for various tasks. To validate the effectiveness
    and practicality of the proposed method, we conducted comprehensive experiments
    on a public multi-task Chinese medical dataset. The experimental results demonstrate
    that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The
    implementation is available online for convenient reproduction of our experiments¹¹1https://github.com/liuqidong07/MOELoRA-peft.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）领域的迅猛发展在众多领域引起了显著关注。为了将LLM量身定制为特定领域，如基于网络的医疗系统，需要进行领域知识的微调。然而，在医学应用中微调LLMs时会出现两个问题。第一个是任务多样性问题，实际医学场景中有许多不同的任务。这种多样性常导致数据不平衡和跷跷板问题，从而导致微调效果不佳。此外，微调的高成本可能会成为阻碍LLM应用的因素。LLM中的大量参数在微调过程中会消耗大量时间和计算资源，这很难得到合理
    justification。为同时解决这两个问题，我们提出了一种新颖的参数高效微调框架，用于多任务医学应用，称为MOELoRA。该框架旨在利用MOE的多任务学习优势和LoRA的参数高效微调优势。为了统一MOE和LoRA，我们设计了多个专家作为可训练参数，每个专家由一对低秩矩阵组成，以保持少量的可训练参数。此外，我们为所有MOELoRA层提出了一个任务驱动的门控函数，该函数可以调节每个专家的贡献，并为各种任务生成不同的参数。为了验证所提方法的有效性和实用性，我们在一个公开的多任务中文医学数据集上进行了全面实验。实验结果表明，MOELoRA优于现有的参数高效微调方法。实现代码可在线获取，以方便重现我们的实验¹¹1https://github.com/liuqidong07/MOELoRA-peft。
- en: 'Index Terms:'
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Medical Applications; Large Language Model; Multi-task Learning;
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 医学应用；大型语言模型；多任务学习；
- en: I Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Due to the impressive capabilities in language understanding and generation,
    the Large Language Models (LLMs) such as ChatGPT [[1](#bib.bib1)] and ChatGLM [[2](#bib.bib2)]
    have gained extensive interest from both academia and industry. Many efforts have
    been devoted to investigating the potential applications of LLMs across various
    domains [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]. One particularly suitable
    domain for LLMs is the medical domain, as the application of LLMs can benefit
    both patients and doctors. For patients, the LLM-enabled online Chatbot can provide
    convenient access to medical knowledge; For doctors, the LLM-enabled Clinical
    Decision Supporting Systems (CDSS) can relieve their heavy workload and improve
    diagnosis efficiency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在语言理解和生成方面的卓越能力，大语言模型（LLMs）如ChatGPT[[1](#bib.bib1)]和ChatGLM[[2](#bib.bib2)]受到了学术界和工业界的广泛关注。许多研究已致力于探索LLMs在各个领域的潜在应用[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。LLMs特别适合的一个领域是医疗领域，因为LLMs的应用可以同时惠及患者和医生。对于患者，LLM驱动的在线聊天机器人可以提供便捷的医疗知识获取；对于医生，LLM驱动的临床决策支持系统（CDSS）可以减轻他们的工作负担，提高诊断效率。
- en: However, the majority of LLMs are trained for general-purpose and are not customized
    for the medical domain. As a result, the general LLMs often fall short in medical
    tasks due to a lack of specialized medical knowledge [[6](#bib.bib6)]. To empower
    LLMs with medical capabilities, a straightforward manner is to fine-tune LLMs
    with medical tasks. For large LLMs with more than $100$ billion parameters, they
    are usually closed-source and extremely costly for fine-tuning [[7](#bib.bib7)].
    Therefore, in this paper, we focus on the open-source LLMs and fine-tuning them
    with medical knowledge and clinical tasks [[8](#bib.bib8), [6](#bib.bib6)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数大语言模型（LLMs）是为通用目的训练的，并未针对医疗领域进行定制。因此，由于缺乏专业的医疗知识，一般的LLMs在医疗任务中往往表现不佳[[6](#bib.bib6)]。为了赋予LLMs医疗能力，一个简单的方法是对LLMs进行医疗任务的微调。对于参数超过$100$亿的大型LLMs，它们通常是闭源的，且微调成本极高[[7](#bib.bib7)]。因此，在本文中，我们将重点关注开源LLMs，并用医疗知识和临床任务对其进行微调[[8](#bib.bib8),
    [6](#bib.bib6)]。
- en: 'Fine-tuning LLMs for the medical domain usually involves two primary challenges:
    (i) Task Variety Problem: In real-world clinics, the LLMs can be applied to a
    large range of tasks, like doctor recommendation [[9](#bib.bib9)], diagnosis prediction [[10](#bib.bib10)],
    medicine recommendation [[11](#bib.bib11)], medical named entity recognition [[12](#bib.bib12),
    [13](#bib.bib13)], clinical report generation [[14](#bib.bib14)] and *etc.* Since
    the input and output of these tasks are quite different, it is difficult to fine-tune
    a unified model for all tasks. Given the diversity of these tasks, fine-tuning
    a single model for each specific task is feasible but demands extensive expertise
    and labor. An integrated multi-task learning framework could potentially address
    this issue. However, much of the existing research on LLMs, as seen in studies
    like [[8](#bib.bib8), [15](#bib.bib15)], predominantly centers on medical dialogue.
    Such over-attention ignores the variety of tasks, resulting in multi-task fine-tuning
    remains underexplored. (ii) High Tuning Cost: While fine-tuning all model parameters
    was a standard approach during the era of Bert [[16](#bib.bib16)], it becomes
    challenging for LLMs due to their sheer size. The vast number of parameters in
    LLMs can lead to prohibitive time and computational expenses in practice [[17](#bib.bib17)].
    As such, there is an urgent need for parameter efficient fine-tuning methodologies.
    To address these two challenges, the community urgently calls for developing a
    multi-tasking parameter efficient fine-tuning framework for LLM-driven medical
    applications.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于医学领域的LLMs微调通常涉及两个主要挑战：（i）任务多样性问题：在现实世界的诊所中，LLMs可以应用于广泛的任务，例如医生推荐[[9](#bib.bib9)]、诊断预测[[10](#bib.bib10)]、药物推荐[[11](#bib.bib11)]、医学命名实体识别[[12](#bib.bib12),
    [13](#bib.bib13)]、临床报告生成[[14](#bib.bib14)]以及*等等*。由于这些任务的输入和输出差异很大，微调一个统一的模型以适应所有任务变得困难。鉴于这些任务的多样性，为每个特定任务微调单一模型是可行的，但需要广泛的专业知识和劳动。一个集成的多任务学习框架可能有助于解决这个问题。然而，现有的LLM研究大多集中于医学对话，如[[8](#bib.bib8),
    [15](#bib.bib15)]所示。这种过度关注忽视了任务的多样性，导致多任务微调仍未被充分探讨。（ii）高调优成本：虽然在Bert时代[[16](#bib.bib16)]微调所有模型参数是一种标准方法，但对于LLMs而言，由于其庞大的规模，这一方法变得具有挑战性。LLMs中的大量参数可能会导致实践中时间和计算费用高昂[[17](#bib.bib17)]。因此，迫切需要高效的参数微调方法。为了应对这两个挑战，社区迫切呼吁开发针对LLM驱动的医疗应用的多任务高效参数微调框架。
- en: 'For task variety problem, several multi-task learning frameworks have been
    proposed [[18](#bib.bib18), [19](#bib.bib19)]. A standout among these is Mixture-of-Experts
    (MOE) [[20](#bib.bib20)], which designs multiple separate experts to learn task-shared
    and -specific knowledge, and integrates a gate function to modulate the contributions
    of each expert. While existing frameworks adeptly consolidate multiple tasks for
    classical neural network architectures, they are primarily compatible with full
    fine-tuning, which is associated with high tuning costs. Correspondingly, the
    emergence of parameter efficient fine-tuning (PEFT) methods (P-Tuning [[1](#bib.bib1)],
    LoRA [[21](#bib.bib21)], etc.) has offered a potential solution to the problem
    of high tuning cost. These methods typically tune a limited number of parameters,
    keeping the pre-trained LLM parameters frozen. For instance, LoRA [[21](#bib.bib21)]
    proposes to only train pairs of low-rank matrices for fitting the parameter updates
    of dense layers in LLMs. However, the existing PEFT is limited to fine-tuning
    either multiple sets of parameters for each task separately or a singular set
    across all tasks. Though separate training can fit each task well, this strategy
    is laborious and lacks task-shared knowledge. While fine-tuning a set of parameters
    is feasible, it may hurt performance due to issues such as data imbalance and
    seesaw effects [[19](#bib.bib19), [22](#bib.bib22)]. For illustration, we analyze
    the data distribution of a Chinese medical dataset, PromptCBLUE, in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"). Our analysis reveals significant
    disparities: while some tasks boast nearly $5,000$. This imbalance can skew the
    uniquely fine-tuned parameters towards tasks with more samples, inadvertently
    undermining the performance on tasks with fewer samples. Therefore, parameter
    efficient fine-tuning of separate parameters for multi-task by a unique training
    process can alleviate both problems simultaneously.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '针对任务多样性问题，已提出若干多任务学习框架[[18](#bib.bib18), [19](#bib.bib19)]。其中一个突出的方法是Mixture-of-Experts
    (MOE) [[20](#bib.bib20)]，该方法设计了多个独立的专家以学习任务共享和特定的知识，并集成了一个门控函数来调节每个专家的贡献。尽管现有框架在经典神经网络架构中熟练地整合了多个任务，但它们主要兼容完全微调，这与高调优成本相关。因此，参数高效微调（PEFT）方法（如P-Tuning
    [[1](#bib.bib1)]、LoRA [[21](#bib.bib21)]等）的出现为高调优成本问题提供了潜在解决方案。这些方法通常只调节有限数量的参数，而保持预训练LLM参数不变。例如，LoRA
    [[21](#bib.bib21)]建议仅训练低秩矩阵对以适应LLM中密集层的参数更新。然而，现有的PEFT方法仅限于为每个任务分别微调多个参数集或在所有任务中微调单一参数集。虽然单独训练可以很好地适应每个任务，但这种策略既繁琐又缺乏任务共享知识。虽然微调一个参数集是可行的，但由于数据不平衡和跷跷板效应[[19](#bib.bib19),
    [22](#bib.bib22)]等问题，可能会影响性能。例如，我们在图[1](#S1.F1 "Figure 1 ‣ I Introduction ‣ MOELoRA:
    An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications")中分析了中文医学数据集PromptCBLUE的数据分布。我们的分析揭示了显著的差异：虽然某些任务的样本量接近$5,000$，这种不平衡可能会使得唯一微调的参数偏向于样本更多的任务，从而无意中削弱了样本较少任务的性能。因此，通过独特的训练过程对多任务的单独参数进行参数高效微调可以同时缓解这两个问题。'
- en: '![Refer to caption](img/12316866ebbf0cb43a0c2444993760fe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/12316866ebbf0cb43a0c2444993760fe.png)'
- en: 'Figure 1: The illustration for data imbalance problem of various medical tasks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：各种医学任务数据不平衡问题的示意图。
- en: To address the challenges of task variety and high tuning costs, we propose
    a unified parameter efficient fine-tuning framework to learn separate parameters
    for various tasks, dubbed MOELoRA. Our framework follows the basic scheme of LoRA
    for the parameter efficiency, *i.e.,* only fine-tuning small size of parameters
    parallel to the dense layers in LLMs. However, as discussed previously, existing
    unified LoRA fine-tuning faces the challenge of a singular set of parameters across
    all tasks. Thus, in our approach, we first design several experts as the trainable
    part rather than a singular pair of low-rank matrices. On the one hand, inspired
    by MOE [[20](#bib.bib20)], separate experts can help learn task-specific knowledge
    under one unique training process. On the other hand, such design gives the chance
    to produce several distinct sets of parameters. Besides, for parameter efficiency,
    we devise each expert as two low-rank matrices. Then, to learn separate sets of
    parameters for each task, we propose a task-motivated gate function. In specific,
    the gate function absorbs the task identity and outputs corresponding expert weights.
    By the expert weights for one specific task and the parameters of multiple experts,
    we can get the unique updated parameters for this task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对任务多样性和高调优成本的挑战，我们提出了一个统一的参数高效微调框架，称为MOELoRA。我们的框架遵循LoRA在参数高效性方面的基本方案，*即*，仅对LLMs中密集层的少量参数进行微调。然而，如前所述，现有的统一LoRA微调面临所有任务共享单一参数集的挑战。因此，在我们的方法中，我们首先设计了多个专家作为可训练部分，而不是单一对低秩矩阵。一方面，受MOE
    [[20](#bib.bib20)] 的启发，独立专家可以在一个独特的训练过程中帮助学习任务特定的知识。另一方面，这种设计提供了生成多个不同参数集的机会。此外，为了提高参数效率，我们将每个专家设计为两个低秩矩阵。然后，为了为每个任务学习独立的参数集，我们提出了一个任务驱动的门控函数。具体来说，门控函数吸收任务身份并输出相应的专家权重。通过为特定任务的专家权重和多个专家的参数，我们可以获得该任务的独特更新参数。
- en: 'In summary, the contributions of this paper are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的贡献如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce MOELoRA, a novel multi-task PEFT framework that combines the strengths
    of both MOE and LoRA. Additionally, we design a task-motivated gate function to
    facilitate the tuning of distinct parameter sets for each task.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了MOELoRA，这是一种新颖的多任务PEFT框架，结合了MOE和LoRA的优点。此外，我们设计了一个任务驱动的门控函数，以促进为每个任务调整不同参数集。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct comprehensive experiments on a public multi-task Chinese medical
    dataset, with the results underscoring the superiority of the proposed MOELoRA
    framework.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在一个公共的多任务中文医学数据集上进行了全面的实验，结果强调了所提出的MOELoRA框架的优越性。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To our knowledge, this research represents the first endeavor to delve into
    multi-task parameter efficient fine-tuning techniques for LLM-driven medical applications.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这项研究代表了首次深入探讨LLM驱动的医学应用中的多任务参数高效微调技术的尝试。
- en: II Preliminary
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 初步
- en: '![Refer to caption](img/fb2a16f93d6c3ab66c1094eaa402ac66.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb2a16f93d6c3ab66c1094eaa402ac66.png)'
- en: 'Figure 2: The medical name entity recognition example for illustration of how
    to use LLMs to complete medical tasks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：医学命名实体识别示例，说明如何使用LLMs完成医学任务。
- en: In this section, we first briefly introduce how LLMs are adopted for medical
    applications. Then, we give the problem definition of multi-task medical applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先简要介绍LLMs如何应用于医学领域。然后，我们给出多任务医学应用的问题定义。
- en: II-A LLMs for Medical Applications
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A LLMs在医学应用中的作用
- en: 'Intelligent medical systems have become increasingly prevalent in contemporary
    web-based healthcare settings. Numerous studies have sought to standardize medical
    tasks by defining consistent input and output patterns, thereby streamlining the
    model design process. As the example of medical named entity recognition (NER) [[12](#bib.bib12),
    [13](#bib.bib13)] illustrated in Figure [2](#S2.F2 "Figure 2 ‣ II Preliminary
    ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"), traditional models typically process medical texts, denoted
    as $I_{M}$. However, the integration of LLMs into medical tasks introduces a distinct
    paradigm. Given that both the input and output of LLMs are typically linguistic
    in nature, there is a necessity to reformulate medical tasks to be compatible
    with LLMs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '智能医疗系统在当代基于网络的医疗环境中变得越来越普遍。许多研究旨在通过定义一致的输入和输出模式来标准化医学任务，从而简化模型设计过程。如图[2](#S2.F2
    "Figure 2 ‣ II Preliminary ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")所示的医学命名实体识别（NER）[[12](#bib.bib12),
    [13](#bib.bib13)]的例子中，传统模型通常处理标记为$I_{M}$的医学文本。然而，将LLMs集成到医学任务中引入了一个不同的范式。鉴于LLMs的输入和输出通常是语言性质的，有必要重新构建医学任务以与LLMs兼容。'
- en: 'To adapt medical tasks for LLMs, we need to restructure both the input and
    output patterns. Input Modification: We incorporate instruction templates into
    the original medical texts to guide LLMs in executing the relevant tasks [[23](#bib.bib23)].
    Taking medical NER as an example, as depicted in Figure [2](#S2.F2 "Figure 2 ‣
    II Preliminary ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method
    for Multi-task Medical Applications"), we employ the template: Please recognize
    the medical name entity in this sentence: “[Medical Text]”, where “[Medical Text]”
    serves as a placeholder for the raw medical text $I_{M}$ and $TP^{A}_{NER}$, respectively.
    With these modifications in place, the process by which LLMs undertake the NER
    task can be described as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使医学任务适应LLMs，我们需要重构输入和输出模式。输入修改：我们将指令模板纳入原始医学文本中，以指导LLMs执行相关任务[[23](#bib.bib23)]。以医学NER为例，如图[2](#S2.F2
    "Figure 2 ‣ II Preliminary ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")所示，我们使用模板：请识别句子中的医学名称实体：“[Medical
    Text]”，其中“[Medical Text]”作为原始医学文本$I_{M}$和$TP^{A}_{NER}$的占位符。进行这些修改后，LLMs执行NER任务的过程可以描述如下：'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: After the task reformulation for LLMs, we can use the purely lingual data to
    fine-tune the foundation large language model, such as LlaMA [[24](#bib.bib24)],
    ChatGLM [[25](#bib.bib25)] and *etc.*. Then, the fine-tuned model completes the
    medical tasks by generating the regulated answers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs进行任务重新构建后，我们可以使用纯语言数据来微调基础的大语言模型，如LlaMA[[24](#bib.bib24)]、ChatGLM[[25](#bib.bib25)]和*等*。然后，经过微调的模型通过生成规范化的答案来完成医学任务。
- en: II-B Multi-task Fine-tuning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 多任务微调
- en: As previously mentioned, medical applications often encompass a variety of tasks,
    such as name entity recognition, medical inquiry, etc. Our goal is to fine-tune
    LLMs to gain robust performance for each task and thus can also benefit the whole
    healthcare system. For multi-task fine-tuning, we consider a set of medical tasks
    represented as $\mathbb{T}=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{j},\ldots,\mathcal{T}_{M}\}$
    and $LO$, optimize the parameters $\Phi$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，医疗应用通常涉及多种任务，如命名实体识别、医学查询等。我们的目标是微调LLMs，以便在每个任务上获得稳健的性能，从而也能惠及整个医疗保健系统。对于多任务微调，我们考虑一组医学任务表示为$\mathbb{T}=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{j},\ldots,\mathcal{T}_{M}\}$和$LO$，优化参数$\Phi$。
- en: 'Since the data from diverse tasks are standardized into a consistent linguistic
    format, we straightforwardly employ the conditional language modeling objectives [[25](#bib.bib25)]
    for all training instances. Furthermore, with the intent to assimilate shared
    medical knowledge and be free from the labor of adjusting fine-tuning for several
    tasks, data from all tasks are incorporated into the unique optimization process.
    Consequently, the objective function for multi-task fine-tuning can be formulated
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于来自不同任务的数据被标准化为一致的语言格式，我们直接采用条件语言建模目标[[25](#bib.bib25)]来处理所有训练实例。此外，旨在吸收共享医学知识并免于调整多个任务的微调工作，所有任务的数据被整合到独特的优化过程中。因此，多任务微调的目标函数可以表示如下：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: '![Refer to caption](img/ad777d5a480cfa5c2bafb6bdb57dffba.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/ad777d5a480cfa5c2bafb6bdb57dffba.png)'
- en: 'Figure 3: The overview of parameter efficient fine-tuning using MOELoRA.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用MOELoRA的参数高效微调概览。
- en: III Method
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法
- en: 'In this section, we provide a comprehensive description of our proposed framework.
    We begin with an overview in Section [III-A](#S3.SS1 "III-A Overview ‣ III Method
    ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). Subsequently, Section [III-B](#S3.SS2 "III-B MOELoRA ‣
    III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications") delves into how MOELoRA seamlessly integrates
    the processes of MOE and LoRA, harnessing the strengths of both. The task-motivated
    gate function is detailed in Section [III-C](#S3.SS3 "III-C Task Motivated Gate
    Function ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), where we also discuss the recovery
    of unique fine-tuned LLM parameters for each task. Lastly, Section [III-D](#S3.SS4
    "III-D Optimization and Inference ‣ III Method ‣ MOELoRA: An MOE-based Parameter
    Efficient Fine-Tuning Method for Multi-task Medical Applications") elaborates
    on the optimization and inference processes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将对提出的框架进行全面描述。我们从第[III-A](#S3.SS1 "III-A 概述 ‣ III 方法 ‣ MOELoRA: 一种基于MOE的参数高效微调方法用于多任务医学应用")节的概述开始。随后，第[III-B](#S3.SS2
    "III-B MOELoRA ‣ III 方法 ‣ MOELoRA: 一种基于MOE的参数高效微调方法用于多任务医学应用")节深入探讨了MOELoRA如何无缝地整合MOE和LoRA的过程，充分利用两者的优势。任务驱动的门控函数在第[III-C](#S3.SS3
    "III-C 任务驱动门控函数 ‣ III 方法 ‣ MOELoRA: 一种基于MOE的参数高效微调方法用于多任务医学应用")节中详细描述，我们还讨论了每个任务的独特微调LLM参数的恢复。最后，第[III-D](#S3.SS4
    "III-D 优化与推理 ‣ III 方法 ‣ MOELoRA: 一种基于MOE的参数高效微调方法用于多任务医学应用")节详细阐述了优化和推理过程。'
- en: III-A Overview
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 概述
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ II-B Multi-task Fine-tuning ‣ II Preliminary
    ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications") provides a visual representation of the parameter efficient
    fine-tuning process of LLMs using MOELoRA. In the realm of parameter-efficient
    fine-tuning, LoRA [[21](#bib.bib21)] introduces the concept of training only two
    low-rank matrices as a substitute for updates in dense layers. Building on this,
    our approach integrates MOELoRA layers into each dense layer, enabling them to
    acquire keys, queries, and values, as well as facilitating the feed-forward process.
    A significant advantage of our method is that we only fine-tune the parameters
    of the MOELoRA layers, keeping the rest of the original LLM parameters frozen.
    This approach substantially reduces the often prohibitive costs associated with
    tuning.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S2.F3 "图 3 ‣ II-B 多任务微调 ‣ II 初步 ‣ MOELoRA: 一种基于MOE的参数高效微调方法用于多任务医学应用")展示了使用MOELoRA对LLM进行参数高效微调的过程。在参数高效微调领域，LoRA
    [[21](#bib.bib21)] 引入了只训练两个低秩矩阵作为密集层更新的替代概念。在此基础上，我们的方法将MOELoRA层集成到每个密集层中，使其能够获取键、查询和值，同时促进前向传播过程。我们方法的一个显著优点是只微调MOELoRA层的参数，其余原始LLM参数保持不变。这种方法显著降低了微调时常见的高成本。'
- en: 'Furthermore, each MOELoRA layer incorporates multiple shared experts. These
    experts are designed to capture diverse knowledge across various medical domains,
    a concept we will delve deeper into in Section [III-B](#S3.SS2 "III-B MOELoRA
    ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for
    Multi-task Medical Applications"). We introduce a task-motivated gate function
    to ensure that unique parameter sets are learned for each task. This function
    determines the contribution weights of experts across all MOELoRA layers, enabling
    the generation of distinct updated parameters tailored to different tasks. It
    is worth noting that we employ a single gate function for all MOELoRA layers,
    rather than having a one-to-one correspondence between gates and MOELoRA layers.
    This design choice is intentional, aiming to reduce the number of tunable parameters
    and mitigate the risk of over-parameterization.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，每个 MOELoRA 层都包含多个共享的专家。这些专家旨在捕捉各种医学领域中的多样知识，我们将在第 [III-B](#S3.SS2 "III-B
    MOELoRA ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method
    for Multi-task Medical Applications") 节中详细探讨这一概念。我们引入了一个任务驱动的门控函数，以确保为每个任务学习独特的参数集。该函数确定所有
    MOELoRA 层中专家的贡献权重，从而生成适合不同任务的不同更新参数。值得注意的是，我们对所有 MOELoRA 层使用一个门控函数，而不是在门控函数和 MOELoRA
    层之间一一对应。这一设计选择旨在减少可调参数的数量，并降低过度参数化的风险。'
- en: '![Refer to caption](img/3f86160213bb6a0eb96dac8f07a008ff.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f86160213bb6a0eb96dac8f07a008ff.png)'
- en: 'Figure 4: The architecture of the proposed MOELoRA.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：所提出的 MOELoRA 的架构。
- en: III-B MOELoRA
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B MOELoRA
- en: 'Low-rank Adaptation (LoRA) [[21](#bib.bib21)] has demonstrated both its effectiveness
    and efficiency in fine-tuning LLMs. As our approach builds upon the foundational
    principles of LoRA for parameter efficiency, it is pertinent to provide a brief
    overview of its workings. LoRA is inspired by the low intrinsic dimension characteristic [[26](#bib.bib26)],
    which reformulates the parameter fine-tuning process in LLMs as a low-rank decomposition.
    Specifically, the equation $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}+\mathbf{B}\mathbf{A}$
    and $\mathbf{A}\in\mathbb{R}^{r\times d_{out}}$ are low-rank and trainable. Given
    this setup, the forward process of a linear layer paired with a LoRA layer can
    be expressed as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应 (LoRA) [[21](#bib.bib21)] 已证明在微调大语言模型 (LLMs) 时具有有效性和高效性。由于我们的方法基于 LoRA
    的基本原理以实现参数效率，因此有必要简要概述其工作原理。LoRA 受到低固有维度特性的启发 [[26](#bib.bib26)]，将 LLMs 中的参数微调过程重新表述为低秩分解。具体而言，方程
    $\mathbf{W}_{0}+\Delta\mathbf{W}=\mathbf{W}+\mathbf{B}\mathbf{A}$ 和 $\mathbf{A}\in\mathbb{R}^{r\times
    d_{out}}$ 是低秩且可训练的。鉴于此设置，线性层与 LoRA 层配对的前向过程可以表示为：
- en: '|  | $\displaystyle\mathbf{h}$ |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}$ |  | (3) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}+\frac{\alpha}{r}\cdot\mathbf{B}\mathbf{A}\mathbf{x}$
    |  |'
- en: where $\mathbf{x}$. The rank of the trainable low-rank matrices is denoted by
    $r$, $\mathbf{W}k$ and $\mathbf{B}$ and $\mathbf{B}$. Such characteristic results
    in achieving parameter efficiency for the fine-tuning process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$。可训练的低秩矩阵的秩由 $r$ 表示，$\mathbf{W}k$ 和 $\mathbf{B}$ 以及 $\mathbf{B}$。这种特性有助于在微调过程中实现参数效率。
- en: 'However, the integral parameters are fine-tuned for all tasks in the original
    LoRA, which causes difficulty in learning the various aspects of medical knowledge.
    A potential solution to this challenge is to segment the entire parameter set
    into several parts and derive various combinations. The Mixture-of-Expert (MOE)
    model [[20](#bib.bib20)] suggests employing multiple expert networks to capture
    different facets of multi-task information, aligning with the combination concept.
    This insight leads us to design MOELoRA, which seamlessly integrates the advantages
    of both LoRA and MOE. To harmonize the distinct forward processes of LoRA and
    MOE, we introduce a set of experts, denoted as $\{E_{i}\}_{i=1}^{N}$ is expressed
    as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在原始 LoRA 中，所有任务的积分参数都进行了微调，这使得学习医学知识的各个方面变得困难。一个潜在的解决方案是将整个参数集划分为多个部分，并推导出各种组合。Mixture-of-Expert
    (MOE) 模型 [[20](#bib.bib20)] 建议采用多个专家网络来捕捉多任务信息的不同方面，这与组合概念相一致。这一见解促使我们设计了 MOELoRA，它无缝集成了
    LoRA 和 MOE 的优点。为了协调 LoRA 和 MOE 的不同前向过程，我们引入了一组专家，记作 $\{E_{i}\}_{i=1}^{N}$，其表达式为：
- en: '|  | $\displaystyle\mathbf{h}_{j}$ |  | (4) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{j}$ |  | (4) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}(\mathbf{x}_{j})$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}(\mathbf{x}_{j})$` |  |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}\mathbf{x}_{j}$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}\mathbf{x}_{j}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}\mathbf{x}_{j}$`
    |  |'
- en: where $\mathbf{h}_{j}$ and $\mathbf{A}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$
    and $B$. This weight is determined by our proposed gate function, which we will
    detail in following section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`$\mathbf{h}_{j}$`和`$\mathbf{A}_{i}\in\mathbb{R}^{\frac{r}{N}\times d_{out}}$`及`$B$`。这个权重由我们提出的门控函数确定，我们将在下一节详细介绍。
- en: Here, we will discuss the number of trainable parameters for LoRA and MOELoRA.
    In terms of LoRA, the two low-rank matrices $\mathbf{B}\in\mathbb{R}^{d_{in}\times
    r}$ trainable experts and each expert own $\frac{r}{N}\times(d_{in}+d{out})$.
    As a conclusion, the MOELoRA has the same number of trainable parameters as LoRA,
    which indicates high efficiency.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论LoRA和MOELoRA的可训练参数数量。在LoRA方面，两个低秩矩阵`$\mathbf{B}\in\mathbb{R}^{d_{in}\times
    r}$`可训练专家，每个专家拥有`$\frac{r}{N}\times(d_{in}+d_{out})$`。总之，MOELoRA的可训练参数数量与LoRA相同，这表明其高效性。
- en: III-C Task Motivated Gate Function
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 任务驱动的门控函数
- en: 'In this section, we detail the intricacies of our task-motivated gate function.
    As previously emphasized, the contribution of each expert should be tailored to
    specific tasks. To regulate these contributions, we introduce a gate function.
    Since these weights are inherently task-specific, our gate function is designed
    to take the task identity as input. To facilitate this, we employ a task embedding
    matrix, denoted as $\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times d_{T}}$-th column
    of $\mathbf{E}$, we apply a linear transformation. This computation is captured
    by the following equation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了我们的任务驱动的门控函数的复杂性。如前所述，每个专家的贡献应针对特定任务量身定制。为了调节这些贡献，我们引入了一个门控函数。由于这些权重本质上是任务特定的，我们的门控函数设计为以任务身份作为输入。为此，我们采用了一个任务嵌入矩阵，记作`$\mathbf{E}\in\mathbb{R}^{|\mathbb{T}|\times
    d_{T}}$`-th列的`$\mathbf{E}$`，我们应用线性变换。这个计算由以下方程捕获：
- en: '|  | $\bm{\omega}_{j}={\rm Softmax}(\mathbf{W}_{T}\mathbf{e}_{j})$ |  | (5)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | `$\bm{\omega}_{j}={\rm Softmax}(\mathbf{W}_{T}\mathbf{e}_{j})$` |  | (5)
    |'
- en: Here, $\bm{\omega}_{j}\in\mathbb{R}^{|\mathbb{T}|}$. To prevent any disproportionately
    large weights, we employ a softmax operation to normalize the contribution weights.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`$\bm{\omega}_{j}\in\mathbb{R}^{|\mathbb{T}|}$`。为了防止权重过大，我们采用了softmax操作来归一化贡献权重。
- en: 'Next, we elucidate the mechanism to retrieve the distinct parameters learned
    for each task. While the conventional design of MOE directly feeds the input vector
    $\mathbf{x}$, the process can be articulated as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们阐明如何检索为每个任务学习的不同参数。虽然传统的MOE设计直接将输入向量`$\mathbf{x}$`馈送进去，但这个过程可以描述为：
- en: '|  | $\displaystyle\mathbf{W}_{j}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | `$\displaystyle\mathbf{W}_{j}$` |  | (6) |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot
    E_{i}$` |  |'
- en: '|  |  | $\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | `$\displaystyle=\mathbf{W}_{0}+\frac{\alpha}{r}\cdot\sum_{i=1}^{N}\omega_{ji}\cdot\mathbf{B}_{i}\mathbf{A}_{i}$`
    |  |'
- en: 'In contrast, if the gate function is driven by the input vector $\mathbf{x}$,
    leading to a sample-specific fine-tuned parameter matrix. This design would render
    the parameters non-retrievable on a per-task basis. The ability to retrieve parameters
    for each task offers two primary advantages: 1) Customization for Task: Each task
    is fine-tuned with a set of parameters, which can help learn more task-specific
    information and alleviate the problem of data imbalance. 2) Efficiency in Inference:
    The retrieved, fine-tuned LLM exhibits reduced inference latency. This is attributed
    to the elimination of the need for the additional forward computation associated
    with the LoRA layer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果门控函数由输入向量`$\mathbf{x}$`驱动，会导致样本特定的精细调整参数矩阵。这种设计将使参数在每个任务基础上无法检索。能够为每个任务检索参数提供了两个主要优势：1)
    任务定制：每个任务都用一组参数进行精细调整，这有助于学习更多任务特定的信息，并缓解数据不平衡问题。2) 推理效率：检索到的精细调整的LLM表现出减少的推理延迟。这归因于消除了与LoRA层相关的额外前向计算需求。
- en: Algorithm 1 Train and inference process of MOELoRA
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 MOELoRA的训练和推理过程
- en: 1:Indicate the LLM and the layers that need MOELoRA fine-tuning.2:Indicate the
    rank value $r$ of MOELoRA.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 指定需要 MOELoRA 微调的 LLM 和层数。2: 指定 MOELoRA 的秩值 $r$。'
- en: Optimization Process
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程
- en: 4:Freeze all parameters in pre-trained LLM, *e.g.,* $\mathbf{W}_{q}$ in $\mathcal{D}$9:end for
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '4: 冻结预训练 LLM 中的所有参数，例如 $\mathbf{W}_{q}$ 在 $\mathcal{D}$9: 结束'
- en: Inference Process
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程
- en: 10:for $\mathcal{T}_{j}$, apply the corresponding parameters of LLM for pediction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '10: 对于 $\mathcal{T}_{j}$，应用相应的 LLM 参数进行预测。'
- en: III-D Optimization and Inference
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 优化和推理
- en: 'In this section, we refer to the optimization and inference process of MOELoRA.
    For better readability, we also conclude the whole procedure in Algorithm [1](#alg1
    "Algorithm 1 ‣ III-C Task Motivated Gate Function ‣ III Method ‣ MOELoRA: An MOE-based
    Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们参考了 MOELoRA 的优化和推理过程。为了更好的可读性，我们还在算法 [1](#alg1 "Algorithm 1 ‣ III-C
    Task Motivated Gate Function ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications") 中总结了整个过程。'
- en: Optimization. We first configure the MOELoRA according to the specified layers
    in LLM and several hyper-parameters (line 1-3). Then, for the parameter efficient
    fine-tuning, all pre-trained parameters in LLM (line 4) are frozen. During the
    optimization, we randomly sample a batch of data from all tasks iteratively, instead
    of grouping the samples from the same task into one batch as some multi-task researches [[27](#bib.bib27),
    [28](#bib.bib28)] do. We choose the random sampling for batch by the performance
    comparison in experiments. Using the batch of data, we can conduct the forward
    process and compute the loss for training (line 6-7). For parameter update, it
    is worth noting that we only fine-tune the parameters of MOELoRA and task motivated
    gate function, *i.e.,* $\{\mathbf{A}_{i},\mathbf{B}_{i}\}_{i=1}^{N}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 优化。我们首先根据 LLM 中指定的层数和几个超参数（第 1-3 行）配置 MOELoRA。然后，为了参数高效微调，我们将 LLM 中所有预训练的参数（第
    4 行）冻结。在优化过程中，我们从所有任务中随机抽取一批数据进行迭代，而不是像某些多任务研究 [[27](#bib.bib27), [28](#bib.bib28)]
    那样将来自同一任务的样本分组为一批。我们选择随机抽样进行批处理是根据实验中的性能比较结果来决定的。使用这一批数据，我们可以进行前向过程并计算训练损失（第 6-7
    行）。在参数更新时，值得注意的是我们仅微调 MOELoRA 和任务驱动的门控函数的参数，即 $\{\mathbf{A}_{i},\mathbf{B}_{i}\}_{i=1}^{N}$。
- en: 'Inference. As discussed in Section [III-C](#S3.SS3 "III-C Task Motivated Gate
    Function ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications"), the MOELoRA can recover the fine-tuned
    parameter matrices for each task by Equation ([6](#S3.E6 "In III-C Task Motivated
    Gate Function ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")). For inference, we first recover
    the trained parameters in LLM for each task (line 10-13), which indicates that
    each task has its own LLM parameters. Then, we can apply the corresponding LLM
    to complete the specified task.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '推理。正如在第 [III-C](#S3.SS3 "III-C Task Motivated Gate Function ‣ III Method ‣
    MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical
    Applications") 节中讨论的，MOELoRA 可以通过方程 ([6](#S3.E6 "In III-C Task Motivated Gate
    Function ‣ III Method ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications")) 恢复每个任务的微调参数矩阵。对于推理，我们首先恢复 LLM 中每个任务的训练参数（第
    10-13 行），这表明每个任务都有其自己的 LLM 参数。然后，我们可以应用相应的 LLM 来完成指定的任务。'
- en: IV Experiment
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: 'In this section, we present comprehensive experiments conducted on a multi-task
    Chinese medical dataset. Through a detailed analysis of the experimental results,
    we seek to address the following Research Questions (RQ):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了在一个多任务中文医学数据集上进行的综合实验。通过对实验结果的详细分析，我们旨在解决以下研究问题（RQ）：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: How does MOELoRA compare to other parameter-efficient fine-tuning strategies
    and cross-task generalization methods in terms of performance?'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ1: MOELoRA 与其他参数高效微调策略和跨任务泛化方法在性能方面相比如何？'
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: What impact do the MOE architecture and the gate function have on the
    fine-tuning process? Additionally, how do different training strategies influence
    the performance of MOELoRA?'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ2: MOE 架构和门控函数对微调过程有什么影响？此外，不同的训练策略如何影响 MOELoRA 的性能？'
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: How do the number of experts and the rank of MOELoRA influence performance
    outcomes?'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ3: 专家数量和 MOELoRA 的秩如何影响性能结果？'
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ4: Are the experts specialized in capturing specific aspects of knowledge?'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ4: 专家是否专注于捕捉特定方面的知识？'
- en: IV-A Experimental Settings
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实验设置
- en: IV-A1 Dataset
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 数据集
- en: 'Our experiments are conducted on the PromptCBLUE dataset²²2https://tianchi.aliyun.com/competition/entrance/532084/information,
    a Chinese multi-task medical dataset recently made available on the Tianchi Competition
    Platform³³3https://tianchi.aliyun.com/competition/activeList. This dataset encompasses
    $16$ distinct medical tasks, each of which has been transformed into pure text
    format using specific prompts, ensuring compatibility with LLMs. To the best of
    our knowledge, PromptCBLUE is the only medical multi-task dataset tailored for
    LLMs. Specifically, the dataset includes tasks such as medical named entity recognition,
    diagnosis report generation, etc. Due to computational constraints, we have chosen
    eight tasks at random for our experiments. For pre-processing, we eliminated duplicate
    samples. Since the test set remains unreleased, we opt to use the development
    set as our test set. The validation set is derived from the training set, with
    its size matching that of the test set. The statistics of the pre-processed dataset
    are concluded in Table [I](#S4.T1 "TABLE I ‣ IV-A1 Dataset ‣ IV-A Experimental
    Settings ‣ IV Experiment ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验是在PromptCBLUE数据集²²2https://tianchi.aliyun.com/competition/entrance/532084/information上进行的，这是一个最近在天池竞赛平台上提供的中文多任务医疗数据集³³3https://tianchi.aliyun.com/competition/activeList。该数据集包含$16$个不同的医疗任务，每个任务已使用特定的提示转换为纯文本格式，确保与LLMs兼容。根据我们的了解，PromptCBLUE是唯一一个专为LLMs量身定制的医疗多任务数据集。具体而言，数据集包括医疗命名实体识别、诊断报告生成等任务。由于计算限制，我们随机选择了八个任务进行实验。对于预处理，我们去除了重复样本。由于测试集尚未发布，我们选择使用开发集作为我们的测试集。验证集来源于训练集，其大小与测试集相匹配。预处理数据集的统计信息总结在表[I](#S4.T1
    "TABLE I ‣ IV-A1 Dataset ‣ IV-A Experimental Settings ‣ IV Experiment ‣ MOELoRA:
    An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications")中。'
- en: 'TABLE I: The brief description and statistics of the dataset PromptCBLUE.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：PromptCBLUE数据集的简要描述和统计信息。
- en: '| Task | Description | # Train | # Validation | # Test |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 描述 | # 训练 | # 验证 | # 测试 |'
- en: '| CMeIE | Name Entity Recognition | 2,828 | 600 | 600 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| CMeIE | 实体识别 | 2,828 | 600 | 600 |'
- en: '| CHIP-CDN | Normalization | 2,381 | 600 | 600 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CDN | 标准化 | 2,381 | 600 | 600 |'
- en: '| CHIP-CDEE | Attribute Extraction | 1,562 | 600 | 600 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CDEE | 属性提取 | 1,562 | 600 | 600 |'
- en: '| CHIP-MDCFNPC | Clinic Entity Discovery | 4,935 | 600 | 600 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-MDCFNPC | 临床实体发现 | 4,935 | 600 | 600 |'
- en: '| CHIP-CTC | Medical Text Classification | 3,622 | 1,100 | 1,100 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CHIP-CTC | 医学文本分类 | 3,622 | 1,100 | 1,100 |'
- en: '| KUAKE-QIC | Query Intention | 3,279 | 660 | 660 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| KUAKE-QIC | 查询意图 | 3,279 | 660 | 660 |'
- en: '| IMCS-V2-MRG | Report Generation | 1,799 | 600 | 600 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| IMCS-V2-MRG | 报告生成 | 1,799 | 600 | 600 |'
- en: '| MedDG | Doctor Dialogue | 4,964 | 600 | 600 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MedDG | 医生对话 | 4,964 | 600 | 600 |'
- en: IV-A2 Baselines
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 基准
- en: 'In our experiments, we benchmark against three distinct groups of baselines,
    namely: (i) LLM without Fine-tuning, (ii) LLM with Fine-tuning, and (iii) Cross-task
    Generalization. The latter two groups utilize ChatGLM-6B [[25](#bib.bib25)], renowned
    for its prowess in Chinese text generation. A brief description of each baseline
    group is as follows.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们以三组不同的基准进行对比，即：（i）LLM无微调，（ii）LLM微调，以及（iii）跨任务泛化。后两组使用ChatGLM-6B [[25](#bib.bib25)]，以其在中文文本生成中的卓越表现而闻名。每个基准组的简要描述如下。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM without Fine-tuing: For this group, we employ In-Context Learning [[29](#bib.bib29)]
    to guide the LLM in accomplishing the relevant tasks. Specifically, we provide
    a task description accompanied by $3$ randomly sampled examples for in-context
    learning. Models such as ChatGPT[[30](#bib.bib30)] and Huatuo [[8](#bib.bib8)]
    serve as the foundational models.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM无微调：对于这一组，我们使用In-Context Learning [[29](#bib.bib29)]来指导LLM完成相关任务。具体而言，我们提供了一个任务描述，并附带$3$个随机采样的示例进行上下文学习。模型如ChatGPT[[30](#bib.bib30)]和Huatuo [[8](#bib.bib8)]作为基础模型。
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM with Fine-tuning: This group encompasses various fine-tuning strategies.
    One is P-Tuning [[1](#bib.bib1)], which inserts continuous trainable prompt vectors
    to the start of the sequence. The other competitors are rooted in LoRA [[21](#bib.bib21)].
    For the multi-task medical tasks, we implement two straightforward strategies:
    one that fine-tunes a single LoRA for all tasks (denoted as LoRA (Full)) and another
    that fine-tunes a distinct LoRA for each task (denoted as LoRA (Single)). Additionally,
    we introduce a variant of LoRA (Full) that incorporates a basic task description
    in the prompt, labeled as LoRA (Full+TP).'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调的 LLM：这一组包括各种微调策略。一个是 P-Tuning [[1](#bib.bib1)]，它将连续的可训练提示向量插入到序列的开始。其他竞争方法基于
    LoRA [[21](#bib.bib21)]。对于多任务医疗任务，我们实现了两种简单的策略：一种是为所有任务微调一个 LoRA（标记为 LoRA (Full)），另一种是为每个任务微调一个独特的
    LoRA（标记为 LoRA (Single)）。此外，我们引入了一种 LoRA (Full) 的变体，它在提示中包含基本任务描述，标记为 LoRA (Full+TP)。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-task Generalization: To assess the applicability of cross-task generalization
    to multi-task fine-tuning, we also evaluate a recent approach, namely LoRAHub [[31](#bib.bib31)].'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨任务泛化：为了评估跨任务泛化在多任务微调中的适用性，我们还评估了一种最新的方法，即 LoRAHub [[31](#bib.bib31)]。
- en: IV-A3 Implementation Details
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 实施细节
- en: 'Our experiments are simulated by PyTorch 1.12.0 and Python 3.6.5\. We run the
    code on Tesla V100 GPUs for acceleration. The LLM ChatGLM-6B [[25](#bib.bib25)],
    recognized for its proficiency in Chinese language processing, serves as the foundational
    model for fine-tuning. To summarize, the configuration details are shown as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在 PyTorch 1.12.0 和 Python 3.6.5 下模拟进行。我们在 Tesla V100 GPU 上运行代码以加速。LLM ChatGLM-6B
    [[25](#bib.bib25)]，以其在中文处理中的优越性而闻名，作为微调的基础模型。总之，配置细节如下：
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Layers: For all LoRA fine-tuning baselines and the proposed MOELoRA,
    we designate trainable layers within the self-attention and linear layers of ChatGLM.
    These layers are identified as “query_key_value”, “dense”, “dense_h_to_4h”, and
    “dense_4h_to_h”.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型层：对于所有 LoRA 微调基线和提出的 MOELoRA，我们指定 ChatGLM 的自注意力和线性层中的可训练层。这些层被标识为“query_key_value”、“dense”、“dense_h_to_4h”和“dense_4h_to_h”。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input/Output Length: The maximum input and output lengths were configured to
    $1,024$, respectively.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入/输出长度：最大输入和输出长度分别配置为 $1,024$。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameters: We set batch size to $64$, with a LoRA dropout $\alpha=0.1$.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数：我们将批量大小设置为 $64$，LoRA dropout 为 $\alpha=0.1$。
- en: Our MOELoRA implementation⁴⁴4https://github.com/liuqidong07/MOELoRA-peft is
    compatible with the PEFT package⁵⁵5https://github.com/huggingface/peft, facilitating
    easier adoption and utilization of MOELoRA.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 MOELoRA 实现⁴⁴4https://github.com/liuqidong07/MOELoRA-peft 与 PEFT 包⁵⁵5https://github.com/huggingface/peft
    兼容，便于 MOELoRA 的采纳和使用。
- en: IV-A4 Evaluation Metrics
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 评估指标
- en: For our evaluations, we employ a variety of metrics tailored to the nature of
    each task. Specifically, the Micro-F1 score is used for CMeIE, CHIP-CDN, CHIP-CDEE
    and CHIP-MDCFNPC, while the Macro-F1 score is for CHIP-CTC and KUAKE-QIC. As for
    text generation tasks, *i.e.,* report generation and doctor dialogue, the Rouge-L [[32](#bib.bib32)]
    is applied. Also, the average score across all tasks is used for evaluating the
    overall performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的评估，我们使用了多种针对每个任务性质的指标。具体来说，Micro-F1 分数用于 CMeIE、CHIP-CDN、CHIP-CDEE 和 CHIP-MDCFNPC，而
    Macro-F1 分数用于 CHIP-CTC 和 KUAKE-QIC。至于文本生成任务，即报告生成和医生对话，应用了 Rouge-L [[32](#bib.bib32)]。另外，所有任务的平均分数用于评估整体性能。
- en: 'TABLE II: The overall results of competing baselines and MOELoRA on PromptCBLUE.
    The boldface refers to the highest score and the underline indicates the best
    result of the baselines. “*” indicates the statistically significant improvements
    (*i.e.,* two-sided t-test with $p<0.05$) over the best baseline.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：竞争基线和 MOELoRA 在 PromptCBLUE 上的整体结果。粗体表示最高分数，下划线表示基线的最佳结果。“*”表示在最佳基线上的统计显著性改进（*即*，双侧
    t 检验 $p<0.05$）。
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC |
    IMCS-V2-MRG | MedDG | 平均值 |'
- en: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 0.3058 | 0.6069 | 0.2838 | 0.5854 | 0.5253 | 0.4851 | 0.3253 |
    0.1361 | 0.4067 |'
- en: '| Huatuo | 0.1826 | 0.3610 | 0.1658 | 0.3487 | 0.1909 | 0.1454 | 0.2401 | 0.1308
    | 0.2207 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Huatuo | 0.1826 | 0.3610 | 0.1658 | 0.3487 | 0.1909 | 0.1454 | 0.2401 | 0.1308
    | 0.2207 |'
- en: '| P-Tuning | 0.4552 | 0.8687 | 0.5256 | 0.7423 | 0.8275 | 0.8377 | 0.3155 |
    0.0901 | 0.5828 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| P-Tuning | 0.4552 | 0.8687 | 0.5256 | 0.7423 | 0.8275 | 0.8377 | 0.3155 |
    0.0901 | 0.5828 |'
- en: '| LoRA (Full) | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (完整) | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678
    | 0.1113 | 0.6155 |'
- en: '| LoRA (Single) | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (单一) | 0.4984 | 0.8882 | 0.5528 | 0.7765 | 0.8694 | 0.8524 | 0.3583
    | 0.1143 | 0.6138 |'
- en: '| LoRA (Full+TP) | 0.4933 | 0.8814 | 0.5450 | 0.7705 | 0.8755 | 0.8664 | 0.3556
    | 0.1160 | 0.6130 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| LoRA (完整+TP) | 0.4933 | 0.8814 | 0.5450 | 0.7705 | 0.8755 | 0.8664 | 0.3556
    | 0.1160 | 0.6130 |'
- en: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LoRAHub | 0.4411 | 0.8442 | 0.5041 | 0.7177 | 0.8564 | 0.8502 | 0.3061 |
    0.1192 | 0.5799 |'
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697* | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MOELoRA | 0.5193* | 0.8928* | 0.5697* | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
- en: 'TABLE III: The experimental results of ablation study for MOELoRA. The boldface
    refers to the highest score and the underline indicates the best result of the
    baselines. “*” indicates the statistically significant improvements (*i.e.,* two-sided
    t-test with $p<0.05$) over the best baseline.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：MOELoRA 的消融研究实验结果。粗体字表示最高分数，下划线表示基线中的最佳结果。“*”表示在最佳基线上的统计显著改进（*即，* 双边 t
    检验 $p<0.05$）。
- en: '| Model | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC
    | IMCS-V2-MRG | MedDG | Avg. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | CMeIE | CHIP-CDN | CHIP-CDEE | CHIP-MDCFNPC | CHIP-CTC | KUAKE-QIC |
    IMCS-V2-MRG | MedDG | 平均 |'
- en: '| w/o MOE | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 |
    0.1113 | 0.6155 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 无 MOE | 0.5089 | 0.8748 | 0.5464 | 0.7780 | 0.8758 | 0.8615 | 0.3678 | 0.1113
    | 0.6155 |'
- en: '| w/o gate | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 |
    0.1140 | 0.6174 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 无门 | 0.5015 | 0.8840 | 0.5378 | 0.7789 | 0.8818 | 0.8699 | 0.3709 | 0.1140
    | 0.6174 |'
- en: '| w multiple gate | 0.4994 | 0.8840 | 0.5692 | 0.7842 | 0.8764 | 0.8675 | 0.3632
    | 0.1130 | 0.6196 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| w 多重门 | 0.4994 | 0.8840 | 0.5692 | 0.7842 | 0.8764 | 0.8675 | 0.3632 | 0.1130
    | 0.6196 |'
- en: '| w BT | 0.4817 | 0.8806 | 0.5712 | 0.7713 | 0.8682 | 0.8643 | 0.3522 | 0.1110
    | 0.6126 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| w BT | 0.4817 | 0.8806 | 0.5712 | 0.7713 | 0.8682 | 0.8643 | 0.3522 | 0.1110
    | 0.6126 |'
- en: '| w RBT | 0.4769 | 0.8930 | 0.5600 | 0.7741 | 0.8636 | 0.8795 | 0.3541 | 0.1135
    | 0.6144 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| w RBT | 0.4769 | 0.8930 | 0.5600 | 0.7741 | 0.8636 | 0.8795 | 0.3541 | 0.1135
    | 0.6144 |'
- en: '| MOELoRA | 0.5193* | 0.8928* | 0.5697 | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| MOELoRA | 0.5193* | 0.8928* | 0.5697 | 0.7933* | 0.8691 | 0.8675 | 0.3681
    | 0.1089 | 0.6236* |'
- en: IV-B Overall Performance
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 整体性能
- en: 'The comprehensive experimental results of MOELoRA and the competing baselines
    are presented in Table [II](#S4.T2 "TABLE II ‣ IV-A4 Evaluation Metrics ‣ IV-A
    Experimental Settings ‣ IV Experiment ‣ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications"). Analyzing the average
    scores across all tasks, it is evident that MOELoRA consistently outperforms all
    other methods. A detailed analysis of the results is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'MOELoRA 和竞争基线的综合实验结果见表[II](#S4.T2 "TABLE II ‣ IV-A4 Evaluation Metrics ‣ IV-A
    Experimental Settings ‣ IV Experiment ‣ MOELoRA: An MOE-based Parameter Efficient
    Fine-Tuning Method for Multi-task Medical Applications")。分析所有任务的平均分数可以看出，MOELoRA
    始终优于其他所有方法。结果的详细分析如下：'
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLMs without Fine-tuning: The first group of baselines, which are LLMs without
    any fine-tuning, significantly lag behind the other groups. This highlights the
    importance of fine-tuning LLMs to incorporate task-specific medical knowledge.
    Notably, ChatGPT outperforms Huatuo on most tasks, suggesting that the LLMs only
    in large parameter scales can be well motivated by the in-context learning [[29](#bib.bib29)].'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未微调的 LLMs：第一组基线是未经过任何微调的 LLMs，明显落后于其他组。这突显了微调 LLMs 以融入任务特定医疗知识的重要性。值得注意的是，ChatGPT
    在大多数任务中优于华拓，这表明仅仅拥有大规模参数的 LLMs 可以通过上下文学习得到很好的激励[[29](#bib.bib29)]。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parameter Efficient Fine-tuning Strategies: Among the parameter-efficient fine-tuning
    strategies, LoRA-based methods clearly surpass P-Tuning. While LoRA (Full) and
    LoRA (Full+TP) both utilize data from all tasks, LoRA (Full+TP) slightly underperforms.
    This might be attributed to the addition of task prompts, which extend the input
    texts, leading to potential truncation of informative words due to input length
    constraints. LoRA (Single), which fine-tunes for individual tasks, also does not
    match the performance of LoRA (Full), underscoring the value of shared knowledge
    across tasks.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数高效微调策略：在参数高效微调策略中，基于 LoRA 的方法明显优于 P-Tuning。虽然 LoRA（完整）和 LoRA（完整+TP）都使用了来自所有任务的数据，但
    LoRA（完整+TP）的表现略逊一筹。这可能归因于任务提示的增加，这扩展了输入文本，导致由于输入长度限制而可能截断有用词汇。LoRA（单一）虽然为单独任务进行微调，但其表现也不如
    LoRA（完整），突显了跨任务共享知识的价值。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-task Generalization: We benchmark against a recent cross-task generalization
    method, LoRAHub. Despite its impressive performance in cross-task generalization
    settings, it requires a vast amount of task data, which conflicts with the multi-task
    setting. In our experiments, we only consider 8 tasks, which might explain its
    relative underperformance.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨任务泛化：我们与近期的跨任务泛化方法LoRAHub进行了基准测试。尽管在跨任务泛化设置中表现出色，但它需要大量任务数据，这与多任务设置相冲突。在我们的实验中，我们只考虑了8个任务，这可能解释了其相对的表现不佳。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task-specific Observations: Performance variations are evident across tasks.
    For instance, LoRA (Full) and LoRA (Full+TP) excel in tasks with larger datasets,
    while LoRA (Single) shines in tasks with fewer samples, highlighting the data
    imbalance issue. MOELoRA consistently achieves optimal performance in most tasks,
    demonstrating its ability to effectively address this imbalance. For MedDG tasks,
    the inherent dialog capability of ChatGPT and Huatuo gives them an advantage over
    other approaches.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定观察：不同任务之间的性能差异显著。例如，LoRA (Full)和LoRA (Full+TP)在数据集较大的任务中表现优异，而LoRA (Single)在样本较少的任务中表现出色，突显了数据不平衡问题。MOELoRA在大多数任务中始终能达到最佳性能，展示了其有效解决这一不平衡的能力。在MedDG任务中，ChatGPT和Huatuo固有的对话能力使它们在其他方法中具有优势。
- en: In response to RQ1, MOELoRA demonstrates superior performance compared to other
    parameter-efficient strategies and cross-task generalization methods.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 针对RQ1，MOELoRA在参数效率策略和跨任务泛化方法中表现优越。
- en: IV-C Ablation Study
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 消融研究
- en: 'To delve deeper into RQ2 and understand the contributions of each component
    in MOELoRA, we present the results of our ablation study in Table [III](#S4.T3
    "TABLE III ‣ IV-A4 Evaluation Metrics ‣ IV-A Experimental Settings ‣ IV Experiment
    ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task
    Medical Applications"). The variant w/o MOE (essentially reverts to LoRA (Full))
    excludes the MOE architecture. It demonstrates inferior performance compared to
    the full-fledged MOELoRA, underscoring the significance of the MOE architecture.
    Similarly, the w/o gate variant, which employs uniform expert weights bypassing
    the gate function, also lags behind MOELoRA, highlighting the gate function’s
    effectiveness. The w multiple gate variant, uses a unique gate function for each
    MOELoRA layer. We can see that it achieves comparable results on several tasks
    and is slightly outperformed by the single gate function design due to over-parameterization.
    Besides, multiple gate functions also incur a higher count of trainable parameters,
    leading to diminished efficiency compared to a single gate function setup.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '为了*深入*了解RQ2并理解MOELoRA中每个组件的贡献，我们在表[III](#S4.T3 "TABLE III ‣ IV-A4 Evaluation
    Metrics ‣ IV-A Experimental Settings ‣ IV Experiment ‣ MOELoRA: An MOE-based Parameter
    Efficient Fine-Tuning Method for Multi-task Medical Applications")中展示了我们的消融研究结果。w/o
    MOE变体（实质上回到LoRA (Full)）排除了MOE架构。与完整的MOELoRA相比，其性能较差，突显了MOE架构的重要性。类似地，w/o gate变体采用均匀的专家权重绕过门控函数，也落后于MOELoRA，突显了门控函数的有效性。w
    multiple gate变体为每个MOELoRA层使用独特的门控函数。我们可以看到它在几个任务上取得了相当的结果，但由于过度参数化，略逊于单一门控函数设计。此外，多重门控函数还会导致更多可训练参数，从而降低了效率。'
- en: Additionally, we analyze the impact of different training strategies. Specifically,
    the w BT method [[27](#bib.bib27)] consolidates samples from the same task into
    one batch. The w RBT approach [[28](#bib.bib28)] randomly selects a task for each
    batch of data. Both of them prove to be less conducive for MOELoRA, resulting
    in performance degradation. These findings underscore the critical roles of both
    the MOE architecture and the gate function in the MOELoRA model, as well as the
    influence of specific training patterns.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们分析了不同训练策略的影响。具体而言，w BT 方法[[27](#bib.bib27)]将同一任务的样本合并为一个批次。w RBT 方法[[28](#bib.bib28)]则随机选择每批数据的任务。这两种方法都证明对MOELoRA不太有利，导致性能下降。这些发现突显了MOELoRA模型中MOE架构和门控函数的关键作用，以及特定训练模式的影响。
- en: '![Refer to caption](img/bbb02dfe735ada014f5e9c4fc01e7105.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bbb02dfe735ada014f5e9c4fc01e7105.png)'
- en: (a)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b2b545cf1b24e845d33abc21ee43a131.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b2b545cf1b24e845d33abc21ee43a131.png)'
- en: (b)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 5: The results of experiments for hyper-parameters, *i.e.,* expert number
    $N$.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：超参数实验结果，*即*，专家数量$N$。
- en: IV-D Hyper-parameter Analysis
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 超参数分析
- en: 'To answer RQ3, we delve into the impact of hyper-parameters on the performance
    of MOELoRA. Specifically, we examine how variations in the expert number $M$ to
    $8$, ensuring that the size of trainable parameters remains relatively stable.
    Subsequently, we observe from Figure [5b](#S4.F5.sf2 "In Figure 5 ‣ IV-C Ablation
    Study ‣ IV Experiment ‣ MOELoRA: An MOE-based Parameter Efficient Fine-Tuning
    Method for Multi-task Medical Applications") that while an increase in $r$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答 RQ3，我们**深入探讨**了超参数对 MOELoRA 性能的影响。具体来说，我们检查了专家数量 $M$ 从 8 的变化情况，同时确保可训练参数的大小保持相对稳定。随后，我们从图
    [5b](#S4.F5.sf2 "在图 5 ‣ IV-C 消融研究 ‣ IV 实验 ‣ MOELoRA：一种基于 MOE 的参数高效微调方法，用于多任务医疗应用")
    中观察到，尽管 $r$ 增加。
- en: IV-E Case Study
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 案例研究
- en: 'For RQ4, we present a visualization of the expert weights across four tasks
    in Figure [6](#S4.F6 "Figure 6 ‣ IV-E Case Study ‣ IV Experiment ‣ MOELoRA: An
    MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications").
    For each task, the length of the bar in different colors represents the weights
    for the corresponding expert. Since the expert weights are normalized to 1, the
    lengths of the bar for each task are the same. At a macro level, it is evident
    that the contributions from each expert vary significantly, underscoring the idea
    that different experts specialize in distinct facets of medical knowledge. Moreover,
    the pronounced disparities in weights across tasks highlight the diverse nature
    of medical applications. Taking a closer look at the tasks CHIP-CDN and KUAKE-QIC,
    we observe that their expert weights are largely congruent, with exceptions in
    experts 3 and 4. Given that CHIP-CDN can be viewed as a precursor to KUAKE-QIC—since
    diagnostic word normalization can bolster inquiry classification—the similarity
    in expert weights suggests that MOELoRA is adept at harnessing shared knowledge
    to benefit intrinsically related tasks.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RQ4，我们在图 [6](#S4.F6 "图 6 ‣ IV-E 案例研究 ‣ IV 实验 ‣ MOELoRA：一种基于 MOE 的参数高效微调方法，用于多任务医疗应用")
    中展示了四个任务的专家权重的可视化。对于每个任务，不同颜色的条形长度表示相应专家的权重。由于专家权重被标准化为 1，因此每个任务的条形长度是相同的。从宏观层面来看，各专家的贡献显著不同，突显了不同专家在医疗知识不同方面的专长。此外，任务之间权重的明显差异突显了医疗应用的多样性。仔细查看任务
    CHIP-CDN 和 KUAKE-QIC，我们观察到它们的专家权重大体一致，只有专家 3 和 4 有例外。考虑到 CHIP-CDN 可以被视为 KUAKE-QIC
    的前身——因为诊断词规范化可以提升查询分类——专家权重的相似性表明 MOELoRA 能够有效利用共享知识，造福于本质上相关的任务。
- en: '![Refer to caption](img/6686ec5e867e1500ea35c838d4ce914d.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6686ec5e867e1500ea35c838d4ce914d.png)'
- en: 'Figure 6: The visualization of expert weights for various tasks. In each task,
    the length of the bar in different colors represents the weights for the corresponding
    expert.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：各种任务的专家权重可视化。在每个任务中，不同颜色的条形长度表示相应专家的权重。
- en: V Related Works
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 相关工作
- en: V-A LLM for Medical Applications
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 医疗应用中的 LLM
- en: Recently, there has been a surge of work in the medical field leveraging the
    powerful capabilities of LLM. For instance, Med-PaLM [[15](#bib.bib15)] proposes
    a new benchmark called MultiMedQA, which combines seven medical question-answering
    datasets to address the challenges of evaluating the clinical knowledge of LLM.
    Med-PaLM2 [[33](#bib.bib33)] has further improved upon Med-PaLM by introducing
    a new prompting strategy called ensemble refinement. This strategy is based on
    CoT [[34](#bib.bib34)] and self-consistency [[35](#bib.bib35)] and has shown significant
    improvements in MedQA. What’s more, ChatDoctor [[6](#bib.bib6)] constructs a dataset
    of 100,000 patient-doctor dialogues collected from a widely used online medical
    consultation platform. The dataset is fine-tuned on LLaMA and an automated information
    retrieval method is proposed to utilize online information, like Wikipedia. Besides,
    HuaTuo [[8](#bib.bib8)] is based on LLaMa [[24](#bib.bib24)] and fine-tuned using
    Chinese medical knowledge from CMeKG [[36](#bib.bib36)]. Additionally, HuaTuo
    has introduced a new evaluation metric called SUS for Safety, Usability, and Smoothness.
    Previous studies have demonstrated the potential of LLMs in the medical field.
    However, they tend to focus on medical dialogue while neglecting other equally
    important tasks (such as medical NER and diagnosis report generation) and usually
    demand a significant fine-tuning cost for achieving generalization ability.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，医学领域的工作出现了激增，利用了LLM的强大能力。例如，Med-PaLM [[15](#bib.bib15)] 提出了一个新的基准测试，称为MultiMedQA，该测试结合了七个医学问答数据集，以应对评估LLM临床知识的挑战。Med-PaLM2
    [[33](#bib.bib33)] 通过引入一种新的提示策略称为集成细化，进一步改进了Med-PaLM。这种策略基于CoT [[34](#bib.bib34)]
    和自一致性 [[35](#bib.bib35)]，在MedQA中表现出显著的改进。此外，ChatDoctor [[6](#bib.bib6)] 构建了一个由从广泛使用的在线医疗咨询平台收集的100,000个患者-医生对话组成的数据集。该数据集在LLaMA上进行了微调，并提出了一种自动信息检索方法，以利用在线信息，如Wikipedia。此外，HuaTuo
    [[8](#bib.bib8)] 基于LLaMa [[24](#bib.bib24)] 并使用来自CMeKG [[36](#bib.bib36)] 的中文医学知识进行了微调。此外，HuaTuo引入了一种新的评估指标，称为SUS，用于安全性、可用性和流畅性。以往的研究已展示了LLM在医学领域的潜力。然而，它们往往专注于医学对话，同时忽视其他同样重要的任务（如医学NER和诊断报告生成），并且通常需要显著的微调成本以实现泛化能力。
- en: V-B Parameter-Efficient Fine-tuning
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 参数高效微调
- en: Parameter efficient fine-tuning (PEFT) aims to improve the performance of LLMs
    on new tasks by minimizing the number of fine-tuning parameters and computational
    complexity. Adapter Tuning [[37](#bib.bib37)] first introduces a lightweight adapter
    module, which has only a few trainable parameters and has shown comparable results
    to fine-tuning on the top layers of LLMs. On the other hand, prefix-tuning [[38](#bib.bib38)]
    and P-Tuning [[39](#bib.bib39)] both construct a task-specific virtual token that
    adds trainable, continuous prompts or embeddings to the original text sequence,
    making optimization more feasible than with discrete prompts. However, using prompts
    can be challenging for training and can also limit the available sequence length
    of the model. LoRA [[21](#bib.bib21)] is inspired by the discovery that low intrinsic
    dimension [[26](#bib.bib26)] in the large parameters is the key role of LLMs,
    and introduces two trainable low-rank matrices into each dense layer. It has been
    shown to achieve comparable performance to full fine-tuning while requiring no
    additional computation during inference. Nevertheless, the LoRA fine-tuning performs
    inferiorly for multi-task medical applications. It can only learn integral updated
    parameters for all tasks, which loses the vital task-specific information. In
    recent times, a thread of research named cross-task generalization [[31](#bib.bib31),
    [28](#bib.bib28), [40](#bib.bib40), [41](#bib.bib41)] emerges, which proposes
    various parameter efficient fine-tuning strategies for multi-task. However, different
    from the multi-task setting in this paper, they first train the model on too many
    tasks and aim to transfer the ability to unseen tasks. Due to the distinct setting,
    their method is difficult to be adapted to our problem. In a word, the multi-task
    parameter efficient fine-tuning for LLM-driven medical applications is still underexplored,
    and we take the first step.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）的目的是通过最小化微调参数和计算复杂性来提高LLMs在新任务上的表现。Adapter Tuning [[37](#bib.bib37)]
    首先引入了一个轻量级的适配器模块，该模块只有少量可训练的参数，并且显示出与LLMs顶层微调的结果相当。另一方面，prefix-tuning [[38](#bib.bib38)]
    和P-Tuning [[39](#bib.bib39)] 都构建了一个任务特定的虚拟令牌，这个令牌向原始文本序列中添加了可训练的、连续的提示或嵌入，使得优化比离散提示更为可行。然而，使用提示在训练过程中可能会面临挑战，并且也可能限制模型的序列长度。LoRA
    [[21](#bib.bib21)] 的灵感来源于低内在维度 [[26](#bib.bib26)] 在大参数中的发现，这被认为是LLMs的关键作用，并在每个密集层中引入了两个可训练的低秩矩阵。它已被证明在性能上与完整微调相当，同时在推理过程中无需额外计算。然而，LoRA微调在多任务医疗应用中的表现不佳。它只能为所有任务学习整体更新的参数，这会丧失关键的任务特定信息。近年来，出现了一种名为跨任务泛化的研究
    [[31](#bib.bib31), [28](#bib.bib28), [40](#bib.bib40), [41](#bib.bib41)]，提出了各种多任务的参数高效微调策略。然而，与本文中的多任务设置不同的是，它们首先在过多的任务上训练模型，并旨在将能力转移到未见过的任务上。由于设置的不同，它们的方法难以适应我们的难题。总而言之，LLM驱动的医疗应用中的多任务参数高效微调仍然是一个未被充分探索的领域，我们迈出了第一步。
- en: VI Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, we first take the step to explore the multi-task parameter efficient
    fine-tuning method for LLM-driven medical applications. To satisfy the requirements
    of efficiency for fine-tuning and effectiveness for multi-task, we propose a novel
    multi-task fine-tuning framework. Specifically, we design the MOELoRA architecture,
    which consists of several low-rank experts as the trainable parameters to learn
    task-related knowledge and retain high efficiency. Besides, a task motivated gate
    function is devised to produce distinct fine-tuned parameters for various tasks.
    By the comprehensive experiments on a multi-task Chinese medical dataset, we verify
    the effectiveness of the proposed MOELoRA. In the future, we will further explore
    how to combine explicit medical knowledge, such as knowledge graphs, with LLMs
    by fine-tuning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先探索了LLM驱动医疗应用中的多任务参数高效微调方法。为了满足微调的效率要求和多任务的有效性，我们提出了一个新颖的多任务微调框架。具体来说，我们设计了MOELoRA架构，它由几个低秩专家组成，作为可训练的参数以学习任务相关知识并保持高效率。此外，还设计了一个任务驱动的门控函数，以生成不同任务的微调参数。通过在多任务中文医疗数据集上的全面实验，我们验证了所提出的MOELoRA的有效性。在未来，我们将进一步探索如何通过微调将显式医疗知识，如知识图谱，与LLMs结合。
- en: References
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, “Gpt understands,
    too,” *AI Open*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, 和 J. Tang, “Gpt understands,
    too,” *AI Open*, 2023.'
- en: '[2] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng,
    X. Xia *et al.*, “Glm-130b: An open bilingual pre-trained model,” in *The Eleventh
    International Conference on Learning Representations*, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng,
    X. Xia *等*，“GLM-130B：一种开放的双语预训练模型”，见于*第十一届国际学习表征会议*，2022。'
- en: '[3] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang, and Q. Li, “Recommender
    systems in the era of large language models (llms),” *arXiv preprint arXiv:2307.02046*,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang, 和 Q. Li，“大语言模型（LLMs）时代的推荐系统”，*arXiv
    预印本 arXiv:2307.02046*，2023。'
- en: '[4] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin *et al.*, “A survey on large language model based autonomous agents,”
    *arXiv preprint arXiv:2308.11432*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin *等*，“关于基于大型语言模型的自主代理的调查”，*arXiv 预印本 arXiv:2308.11432*，2023。'
- en: '[5] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh, N. Akhtar,
    J. Wu, and S. Mirjalili, “A survey on large language models: Applications, challenges,
    limitations, and practical usage,” *TechRxiv*, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh, N. Akhtar,
    J. Wu, 和 S. Mirjalili，“关于大型语言模型的调查：应用、挑战、局限性和实际使用”，*TechRxiv*，2023。'
- en: '[6] L. Yunxiang, L. Zihan, Z. Kai, D. Ruilong, and Z. You, “Chatdoctor: A medical
    chat model fine-tuned on llama model using medical domain knowledge,” *arXiv preprint
    arXiv:2303.14070*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Yunxiang, L. Zihan, Z. Kai, D. Ruilong, 和 Z. You，“Chatdoctor：一个使用医学领域知识在LLAMA模型上微调的医学聊天模型”，*arXiv
    预印本 arXiv:2303.14070*，2023。'
- en: '[7] OpenAI, “Gpt-4 technical report,” *arXiv preprint arXiv:2303.08774*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] OpenAI，“GPT-4技术报告”，*arXiv 预印本 arXiv:2303.08774*，2023。'
- en: '[8] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, “Huatuo:
    Tuning llama model with chinese medical knowledge,” 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, 和 T. Liu，“华佗：用中文医学知识调整LLAMA模型”，2023。'
- en: '[9] Z. Zheng, Z. Qiu, H. Xiong, X. Wu, T. Xu, E. Chen, and X. Zhao, “Ddr: Dialogue
    based doctor recommendation for online medical service,” in *Proceedings of the
    28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, ser. KDD ’22.   New
    York, NY, USA: Association for Computing Machinery, 2022, p. 4592–4600\. [Online].
    Available: https://doi.org/10.1145/3534678.3539201'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Z. Zheng, Z. Qiu, H. Xiong, X. Wu, T. Xu, E. Chen, 和 X. Zhao，“DDR：基于对话的在线医疗服务医生推荐”，见于*第28届ACM
    SIGKDD知识发现与数据挖掘大会论文集*，系列 KDD ’22。纽约，美国：计算机协会，2022，第4592–4600页。[在线]。可用链接: https://doi.org/10.1145/3534678.3539201'
- en: '[10] Z. Qiao, X. Wu, S. Ge, and W. Fan, “Mnn: Multimodal attentional neural
    networks for diagnosis prediction,” in *International Joint Conference on Artificial
    Intelligence*, 2019\. [Online]. Available: https://api.semanticscholar.org/CorpusID:199466261'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Qiao, X. Wu, S. Ge, 和 W. Fan，“MNN：用于诊断预测的多模态注意力神经网络”，见于*国际人工智能联合会议*，2019。[在线]。可用链接:
    https://api.semanticscholar.org/CorpusID:199466261'
- en: '[11] Y. Zhang, X. Wu, Q. Fang, S. Qian, and C. Xu, “Knowledge-enhanced attributed
    multi-task learning for medicine recommendation,” *ACM Trans. Inf. Syst.*, jan
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Zhang, X. Wu, Q. Fang, S. Qian, 和 C. Xu，“用于医学推荐的知识增强属性多任务学习”，*ACM 信息系统汇刊*，2023年1月。'
- en: '[12] S. Zhao, T. Liu, S. Zhao, and F. Wang, “A neural multi-task learning framework
    to jointly model medical named entity recognition and normalization,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 33, no. 01, 2019, pp.
    817–824.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Zhao, T. Liu, S. Zhao, 和 F. Wang，“一个神经多任务学习框架用于联合建模医学命名实体识别和标准化”，见于*AAAI
    人工智能会议论文集*，第33卷，第01期，2019，第817–824页。'
- en: '[13] S. Rezayi, H. Dai, Z. Liu, Z. Wu, A. Hebbar, A. H. Burns, L. Zhao, D. Zhu,
    Q. Li, W. Liu *et al.*, “Clinicalradiobert: Knowledge-infused few shot learning
    for clinical notes named entity recognition,” in *International Workshop on Machine
    Learning in Medical Imaging*.   Springer, 2022, pp. 269–278.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Rezayi, H. Dai, Z. Liu, Z. Wu, A. Hebbar, A. H. Burns, L. Zhao, D. Zhu,
    Q. Li, W. Liu *等*，“临床放射体：用于临床笔记命名实体识别的知识注入式少样本学习”，见于*国际医学影像机器学习研讨会*。Springer，2022，第269–278页。'
- en: '[14] Y. Miura, Y. Zhang, E. B. Tsai, C. P. Langlotz, and D. Jurafsky, “Improving
    factual completeness and consistency of image-to-text radiology report generation,”
    *arXiv preprint arXiv:2010.10042*, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Miura, Y. Zhang, E. B. Tsai, C. P. Langlotz, 和 D. Jurafsky，“提高图像到文本放射学报告生成的事实完整性和一致性”，*arXiv
    预印本 arXiv:2010.10042*，2020。'
- en: '[15] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,
    A. Tanwani, H. Cole-Lewis, S. Pfohl *et al.*, “Large language models encode clinical
    knowledge,” *Nature*, vol. 620, no. 7972, pp. 172–180, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,
    A. Tanwani, H. Cole-Lewis, S. Pfohl *等*，“大型语言模型编码临床知识”，*Nature*，第620卷，第7972期，第172–180页，2023年。'
- en: '[16] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019, pp. 4171–4186.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. D. M.-W. C. Kenton 和 L. K. Toutanova，“Bert：用于语言理解的深度双向变换器预训练”，收录于*NAACL-HLT会议论文集*，2019年，第4171–4186页。'
- en: '[17] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan,
    F. Yang *et al.*, “Baichuan 2: Open large-scale language models,” *arXiv preprint
    arXiv:2309.10305*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D.
    Yan, F. Yang *等*，“Baichuan 2：开放的大规模语言模型”，*arXiv 预印本 arXiv:2309.10305*，2023年。'
- en: '[18] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 34, no. 12, pp. 5586–5609, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Zhang 和 Q. Yang，“多任务学习综述”，*IEEE知识与数据工程学报*，第34卷，第12期，第5586–5609页，2021年。'
- en: '[19] M. Crawshaw, “Multi-task learning with deep neural networks: A survey,”
    *arXiv preprint arXiv:2009.09796*, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Crawshaw，“深度神经网络的多任务学习：综述”，*arXiv 预印本 arXiv:2009.09796*，2020年。'
- en: '[20] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and
    J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer,” *arXiv preprint arXiv:1701.06538*, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, 和 J.
    Dean，“极其庞大的神经网络：稀疏门控专家混合层”，*arXiv 预印本 arXiv:1701.06538*，2017年。'
- en: '[21] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen *et al.*,
    “Lora: Low-rank adaptation of large language models,” in *International Conference
    on Learning Representations*, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen *等*，“Lora：大型语言模型的低秩适配”，收录于*国际学习表征会议*，2021年。'
- en: '[22] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, “Dice loss for data-imbalanced
    nlp tasks,” in *Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics*, 2020, pp. 465–476.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, 和 J. Li，“数据不平衡 NLP 任务的 Dice 损失”，收录于*第58届计算语言学协会年会论文集*，2020年，第465–476页。'
- en: '[23] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang,
    F. Wu *et al.*, “Instruction tuning for large language models: A survey,” *arXiv
    preprint arXiv:2308.10792*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T.
    Zhang, F. Wu *等*，“大型语言模型的指令调优：综述”，*arXiv 预印本 arXiv:2308.10792*，2023年。'
- en: '[24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    and G. Lample, “Llama: Open and efficient foundation language models,” *arXiv
    preprint arXiv:2302.13971*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    和 G. Lample，“Llama：开放和高效的基础语言模型”，*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: '[25] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, “Glm: General
    language model pretraining with autoregressive blank infilling,” in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2022, pp. 320–335.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, 和 J. Tang，“Glm：自回归空白填充的通用语言模型预训练”，收录于*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，2022年，第320–335页。'
- en: '[26] A. Aghajanyan, S. Gupta, and L. Zettlemoyer, “Intrinsic dimensionality
    explains the effectiveness of language model fine-tuning,” in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 7319–7328.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Aghajanyan, S. Gupta, 和 L. Zettlemoyer，“内在维度解释了语言模型微调的有效性”，收录于*第59届计算语言学协会年会及第11届国际自然语言处理联合会议论文集（第1卷：长篇论文）*，2021年，第7319–7328页。'
- en: '[27] X.-R. Sheng, L. Zhao, G. Zhou, X. Ding, B. Dai, Q. Luo, S. Yang, J. Lv,
    C. Zhang, H. Deng *et al.*, “One model to serve all: Star topology adaptive recommender
    for multi-domain ctr prediction,” in *Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management*, 2021, pp. 4104–4113.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] X.-R. Sheng, L. Zhao, G. Zhou, X. Ding, B. Dai, Q. Luo, S. Yang, J. Lv,
    C. Zhang, H. Deng *等*，“一个模型服务所有：多领域CTR预测的星形拓扑自适应推荐器”，收录于*第30届ACM国际信息与知识管理会议论文集*，2021年，第4104–4113页。'
- en: '[28] T. Sun, Z. He, Q. Zhu, X. Qiu, and X.-J. Huang, “Multitask pre-training
    of modular prompt for chinese few-shot learning,” in *Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 2023, pp. 11 156–11 172.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Sun, Z. He, Q. Zhu, X. Qiu, 和 X.-J. Huang，“中文少样本学习的模块化提示多任务预训练，” 收录于
    *第61届计算语言学协会年会（第1卷：长篇论文）*，2023年，第11 156–11 172页。'
- en: '[29] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and
    Z. Sui, “A survey for in-context learning,” *arXiv preprint arXiv:2301.00234*,
    2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, 和 Z.
    Sui，“关于上下文学习的调查，” *arXiv预印本 arXiv:2301.00234*，2022年。'
- en: '[30] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少量学习者，” *《神经信息处理系统进展》*，第33卷，第1877–1901页，2020年。'
- en: '[31] C. Huang, Q. Liu, B. Y. Lin, T. Pang, C. Du, and M. Lin, “Lorahub: Efficient
    cross-task generalization via dynamic lora composition,” *arXiv preprint arXiv:2307.13269*,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Huang, Q. Liu, B. Y. Lin, T. Pang, C. Du, 和 M. Lin，“Lorahub：通过动态Lora组合实现高效跨任务泛化，”
    *arXiv预印本 arXiv:2307.13269*，2023年。'
- en: '[32] C.-Y. Lin and F. J. Och, “Automatic evaluation of machine translation
    quality using longest common subsequence and skip-bigram statistics,” in *Proceedings
    of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)*,
    2004, pp. 605–612.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C.-Y. Lin 和 F. J. Och，“使用最长公共子序列和跳字对统计进行机器翻译质量的自动评估，” 收录于 *第42届计算语言学协会年会（ACL-04）会议论文集*，2004年，第605–612页。'
- en: '[33] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,
    S. Pfohl, H. Cole-Lewis, D. Neal *et al.*, “Towards expert-level medical question
    answering with large language models,” *arXiv preprint arXiv:2305.09617*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,
    S. Pfohl, H. Cole-Lewis, D. Neal *等*，“朝着专家级医疗问答迈进，借助大型语言模型，” *arXiv预印本 arXiv:2305.09617*，2023年。'
- en: '[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *等*，“链式思考提示在大型语言模型中引发推理，” *《神经信息处理系统进展》*，第35卷，第24 824–24 837页，2022年。'
- en: '[35] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery,
    and D. Zhou, “Self-consistency improves chain of thought reasoning in language
    models,” *arXiv preprint arXiv:2203.11171*, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery,
    和 D. Zhou，“自一致性提升了语言模型中的思维链推理，” *arXiv预印本 arXiv:2203.11171*，2022年。'
- en: '[36] O. Byambasuren, Y. Yang, Z. Sui, D. Dai, B. Chang, S. Li, and H. Zan,
    “Preliminary study on the construction of chinese medical knowledge graph,” *Journal
    of Chinese Information Processing*, vol. 33, no. 10, pp. 1–9, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] O. Byambasuren, Y. Yang, Z. Sui, D. Dai, B. Chang, S. Li, 和 H. Zan，“对中文医学知识图谱构建的初步研究，”
    *《中文信息处理学报》*，第33卷，第10期，第1–9页，2019年。'
- en: '[37] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer learning
    for nlp,” in *International Conference on Machine Learning*.   PMLR, 2019, pp.
    2790–2799.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, 和 S. Gelly，“面向自然语言处理的参数高效迁移学习，” 收录于 *国际机器学习会议*。PMLR，2019年，第2790–2799页。'
- en: '[38] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
    generation,” in *Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*.   Online: Association for Computational
    Linguistics, Aug. 2021, pp. 4582–4597\. [Online]. Available: https://aclanthology.org/2021.acl-long.353'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. L. Li 和 P. Liang，“前缀调优：优化连续提示生成，” 收录于 *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*。在线：计算语言学协会，2021年8月，第4582–4597页。[在线]。可用链接：
    https://aclanthology.org/2021.acl-long.353'
- en: '[39] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, “P-tuning:
    Prompt tuning can be comparable to fine-tuning across scales and tasks,” in *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*.   Dublin, Ireland: Association for Computational Linguistics,
    May 2022, pp. 61–68\. [Online]. Available: https://aclanthology.org/2022.acl-short.8'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, 和 J. Tang，“P-tuning: 提示调优在不同尺度和任务上的效果可与微调相媲美，”
    收录于 *第60届计算语言学协会年会（第2卷：短篇论文集）*。都柏林，爱尔兰：计算语言学协会，2022年5月，第61–68页。 [在线]. 可用网址: https://aclanthology.org/2022.acl-short.8'
- en: '[40] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi, “Attempt: Parameter-efficient
    multi-task tuning via attentional mixtures of soft prompts,” in *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, 2022,
    pp. 6655–6672.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Asai, M. Salehi, M. E. Peters, 和 H. Hajishirzi，“Attempt: 通过注意力混合软提示的参数高效多任务调优，”
    收录于 *2022年自然语言处理经验方法会议论文集*，2022年，第6655–6672页。'
- en: '[41] A. Üstün, A. Bisazza, G. Bouma, G. van Noord, and S. Ruder, “Hyper-x:
    A unified hypernetwork for multi-task multilingual transfer,” *arXiv preprint
    arXiv:2205.12148*, 2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Üstün, A. Bisazza, G. Bouma, G. van Noord, 和 S. Ruder，“Hyper-x: 多任务多语言迁移的统一超网络，”
    *arXiv 预印本 arXiv:2205.12148*，2022。'
