- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:34:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:34:55'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Pluto and Charon: 一种时间和内存高效的协作边缘 AI 框架，用于个人 LLM 的微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10746](https://ar5iv.labs.arxiv.org/html/2408.10746)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10746](https://ar5iv.labs.arxiv.org/html/2408.10746)
- en: Bei Ouyang^★¹, Shengyuan Ye^★¹, Liekang Zeng², Tianyi Qian¹, Jingyi Li¹, Xu
    Chen^†¹ ¹School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou,
    China ²IoT Thrust and Research Center for Digital World with Intelligent Things,
    HKUST (GZ), Guangzhou, China [ouyb9, yeshy8,qianty,lijy573@mail2.sysu.edu.cn,
    liekangzeng@hkust-gz.edu.cn, chenxu35@mail.sysu.edu.cn](mailto:ouyb9,%20yeshy8,qianty,lijy573@mail2.sysu.edu.cn,%20liekangzeng@hkust-gz.edu.cn,%20chenxu35@mail.sysu.edu.cn)(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Bei Ouyang^★¹, Shengyuan Ye^★¹, Liekang Zeng², Tianyi Qian¹, Jingyi Li¹, Xu
    Chen^†¹ ¹中山大学计算机科学与工程学院，中国广州 ²香港科技大学（广州）数字世界与智能物联网研究中心，中国广州 [ouyb9, yeshy8, qianty,
    lijy573@mail2.sysu.edu.cn, liekangzeng@hkust-gz.edu.cn, chenxu35@mail.sysu.edu.cn](mailto:ouyb9,%20yeshy8,qianty,lijy573@mail2.sysu.edu.cn,%20liekangzeng@hkust-gz.edu.cn,%20chenxu35@mail.sysu.edu.cn)(2024)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Large language models (LLMs) have unlocked a plethora of powerful applications
    at the network edge, such as intelligent personal assistants. Data privacy and
    security concerns have prompted a shift towards edge-based fine-tuning of personal
    LLMs, away from cloud reliance. However, this raises issues of computational intensity
    and resource scarcity, hindering training efficiency and feasibility. While current
    studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate
    resource constraints, our analysis indicates that these techniques are not sufficiently
    resource-efficient for edge devices. Other studies focus on exploiting the potential
    of edge devices through resource management optimization, yet are ultimately bottlenecked
    by the resource wall of individual devices.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在网络边缘解锁了众多强大的应用，如智能个人助手。数据隐私和安全问题促使了从云端依赖转向边缘微调个人 LLM。然而，这带来了计算强度和资源稀缺的问题，影响了训练效率和可行性。尽管当前研究探讨了参数高效微调（PEFT）技术以缓解资源限制，但我们的分析表明这些技术对边缘设备的资源效率不足。其他研究则专注于通过资源管理优化来挖掘边缘设备的潜力，但最终仍受限于单个设备的资源瓶颈。
- en: To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory
    efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks
    the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system
    co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique
    that is efficient in terms of parameters, time, and memory. It utilizes Parallel
    Adapters to circumvent the need for a full backward pass through the LLM backbone.
    Additionally, an activation cache mechanism further streamlining the process by
    negating the necessity for repeated forward passes across multiple epochs. (2)
    Systematically, PAC leverages edge devices in close proximity, pooling them as
    a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid
    data and pipeline parallelism to orchestrate distributed training. The use of
    the activation cache eliminates the need for forward pass through the LLM backbone,
    enabling exclusive fine-tuning of the Parallel Adapters using data parallelism.
    Extensive evaluation based on prototype implementation demonstrates that PAC remarkably
    outperforms state-of-the-art approaches, achieving up to $8.64\times$ reduction
    in memory footprint.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这些挑战，我们提出了 Pluto 和 Charon (PAC)，一种时间和内存高效的协作边缘 AI 框架，用于个人 LLM 的微调。PAC 通过精密的算法-系统协同设计突破了个人
    LLM 微调的资源壁垒。（1）从算法角度来看，PAC 实现了一种在参数、时间和内存方面高效的个人 LLM 微调技术。它利用 Parallel Adapters
    避免了对 LLM 主干进行完整的反向传播。此外，激活缓存机制进一步简化了过程，通过消除对多轮前向传播的需求来提高效率。（2）从系统角度来看，PAC 利用接近的边缘设备，将它们作为整体资源用于现场个人
    LLM 微调，采用混合的数据和流水线并行来协调分布式训练。激活缓存的使用消除了对 LLM 主干的前向传播需求，实现了对 Parallel Adapters
    的数据并行微调。基于原型实现的广泛评估表明，PAC 显著优于最先进的方法，实现了高达 $8.64\times$ 的内存占用减少。
- en: 'Edge intelligence, large language model, parameter-efficient fine-tuning, pipeline
    parallelism, data parallelism, parallel processing$\bigstar$: Corresponding author.^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: The 53rd International Conference
    on Parallel Processing; August 12–15, 2024; Gotland, Sweden^†^†booktitle: The
    53rd International Conference on Parallel Processing (ICPP ’24), August 12–15,
    2024, Gotland, Sweden^†^†doi: 10.1145/3673038.3673043^†^†isbn: 979-8-4007-1793-2/24/08'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘智能、大型语言模型、参数高效微调、流水线并行、数据并行、并行处理$\bigstar$: 通讯作者。^†^†期刊年份: 2024^†^†版权: 保留权利^†^†会议:
    第53届国际并行处理会议；2024年8月12-15日；瑞典戈特兰^†^†书名: 第53届国际并行处理会议（ICPP ’24），2024年8月12-15日，瑞典戈特兰^†^†doi:
    10.1145/3673038.3673043^†^†isbn: 979-8-4007-1793-2/24/08'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Large language models (LLMs) (Vaswani et al., [2017](#bib.bib23); Raffel et al.,
    [2020](#bib.bib21); Lewis et al., [2019](#bib.bib14)) have ushered in a revolution
    in machine intelligence, owing to their exceptional capabilities in a wide range
    of machine learning tasks. While born on datacenter warehouse, LLMs have quickly
    sunk to edge devices and facilitated a range of intelligent applications at the
    network edge, such as intelligent personal assistants (IPAs) which are software
    agents that can augment individuals’ abilities, complete complicated tasks, and
    even satisfy emotional needs. A recent survey (Li et al., [2024](#bib.bib15))
    targeting LLM-based IPAs has revealed that over $80\%$ of industry experts believe
    that, owing to the sensitive and privacy-critical nature of user data, personal
    LLMs should be fully (or primarily) hosted at the edge in order to enable privacy-preserving
    model personalization and serving. Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning") illustrates the scenario of hosting a personal
    LLM-based intelligent agent within a smart home. A personal LLM agent provides
    users with high-performance, privacy-preserving intelligent services. Meanwhile,
    the agent also tracks user interactions, learns from experiences, and extracts
    knowledge to fine-tune the personal LLMs and further enhance the service quality.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（Vaswani et al., [2017](#bib.bib23); Raffel et al., [2020](#bib.bib21);
    Lewis et al., [2019](#bib.bib14)）凭借其在各种机器学习任务中的卓越能力，引领了机器智能的革命。虽然LLMs最初诞生于数据中心，但它们迅速下沉到边缘设备，并促进了网络边缘的一系列智能应用，如智能个人助手（IPAs），这些软件代理能够增强个人能力、完成复杂任务，甚至满足情感需求。最近的一项调查（Li
    et al., [2024](#bib.bib15)）显示，超过$80\%$的行业专家认为，由于用户数据的敏感性和隐私性，个人LLMs应该完全（或主要）托管在边缘，以实现隐私保护的模型个性化和服务。图[1](#S1.F1
    "图1 ‣ 1\. 引言 ‣ Pluto和Charon：一个时间和记忆高效的协作边缘AI框架，用于个人LLMs的微调") 说明了在智能家居中托管个人LLM智能代理的场景。个人LLM代理为用户提供高性能、隐私保护的智能服务。同时，代理还跟踪用户互动，从经验中学习，并提取知识以微调个人LLMs，从而进一步提高服务质量。
- en: '![Refer to caption](img/9512761aaf7918d70324b7c94b854462.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9512761aaf7918d70324b7c94b854462.png)'
- en: Figure 1\. An illustration of hosting personal LLM-based intelligent agents
    within a smart home.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 在智能家居中托管基于个人LLM的智能代理的示意图。
- en: While the serving of LLMs on edge devices has been made feasible through careful
    engineering (Guo et al., [2023](#bib.bib7); Xu et al., [2023](#bib.bib27); Ye
    et al., [2024a](#bib.bib29)), fine-tuning these models remains significantly challenging
    due to the resource-intensive nature of LLM training. Towards alleviating the
    resource challenges, some research works (Cai et al., [2023](#bib.bib5); Miao
    et al., [2024](#bib.bib18)) have explored parameter-efficient fine-tuning (PEFT)
    techniques, such as Adapters (Houlsby et al., [2019](#bib.bib10)) and LoRA (Hu
    et al., [2021](#bib.bib11)), which modify less than $2\%$ GB with LoRA and $6.8$
    GB with Adapters, is often unaffordable as typical mobile devices only possess
    4-12GB DRAMs in total to run both system software and applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在边缘设备上服务LLMs已经通过精心工程设计变得可行（Guo et al., [2023](#bib.bib7); Xu et al., [2023](#bib.bib27);
    Ye et al., [2024a](#bib.bib29)），但由于LLM训练资源密集的性质，微调这些模型仍然极具挑战性。为了缓解资源挑战，一些研究工作（Cai
    et al., [2023](#bib.bib5); Miao et al., [2024](#bib.bib18)）探讨了参数高效微调（PEFT）技术，如Adapters（Houlsby
    et al., [2019](#bib.bib10)）和LoRA（Hu et al., [2021](#bib.bib11)），这些技术在LoRA中修改不到$2\%$
    GB，在Adapters中修改$6.8$ GB，但通常难以负担，因为典型的移动设备总共只有4-12GB的DRAM用于运行系统软件和应用程序。
- en: Other leading researchers have explored designing sophisticated resource management
    mechanisms (e.g., CPU-DSP co-execution (Xu et al., [2022](#bib.bib26)), memory
    budget adapting (Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)))
    to leverage native resources, but are still bottlenecked by the intrinsic resource
    shortage of single device. To break the resource wall of a single device, we alternatively
    observe that prevalent edge environments like smart homes usually comprise a group
    of trusted idle devices beyond a single terminal (e.g., phones and smart-home
    devices). These accompanying devices are typically in physical proximity and can
    be associated as a resource augmentation for in-situ personal LLMs fine-tuning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其他领先的研究者已经探索了设计复杂的资源管理机制（例如，CPU-DSP协同执行（Xu等，[2022](#bib.bib26)），内存预算适配（Gim和Ko，[2022](#bib.bib6)；Wang等，[2022](#bib.bib24)））以利用本地资源，但仍然受限于单个设备的固有资源不足。为了突破单个设备的资源壁垒，我们观察到，像智能家居这样的普遍边缘环境通常包含一组可信的闲置设备，超出单一终端（例如手机和智能家居设备）。这些附属设备通常在物理上接近，可以作为资源增强，与在场的个人LLM微调关联起来。
- en: 'As motivated, in this paper, we introduce Pluto and Charon (PAC), a time and
    memory efficient collaborative edge AI framework for personal LLMs fine-tuning.
    PAC’s contribution goes beyond merely leveraging distributed edge devices, instead
    it breaks the resource wall of in-situ personal LLMs fine-tuning with a sophisticated
    algorithm-system co-design:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，在本文中，我们介绍了Pluto和Charon（PAC），这是一个时间和内存高效的协作边缘AI框架，用于个人LLM微调。PAC的贡献不仅仅在于利用分布式边缘设备，它突破了在场个人LLM微调的资源壁垒，采用了复杂的算法-系统协同设计：
- en: $\bullet$ (Algorithm) We evaluate two predominant PEFT techniques, Adapters
    and LoRA, and reveal that although parameter efficient, these techniques do not
    achieve sufficient resource efficiency. In light of the side-tuning (Zhang et al.,
    [2020](#bib.bib35)) techniques, we employ not only parameter but also time and
    memory-efficient personal LLMs fine-tuning techniques with Parallel Adapters,
    which provides a dedicated gradient "highway" for the trainable parameters. Additionally,
    our Parallel Adapters stand out from other PEFT techniques by preserving the invariant
    intermediate activations from the LLM backbone for any given input sequence. By
    reusing these cached activations across multiple epochs, PAC increases resource
    efficiency and reduces fine-tuning latency by eliminating repetitive forward propagation
    through the LLM backbone.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ （算法）我们评估了两种主要的PEFT技术，Adapters和LoRA，揭示了尽管这些技术在参数上高效，但没有实现足够的资源效率。鉴于侧重微调（Zhang等，[2020](#bib.bib35)）技术，我们不仅采用了参数高效，还采用了时间和内存高效的个人LLM微调技术，通过并行Adapters提供了一个专门的梯度“高速公路”用于可训练参数。此外，我们的并行Adapters与其他PEFT技术相比，通过保留LLM骨干网络的不可变中间激活，能够在任何给定输入序列下表现突出。通过在多个训练周期中重用这些缓存激活，PAC提高了资源效率，并通过消除在LLM骨干网络中的重复前向传播来减少微调延迟。
- en: '$\bullet$ (System) We leverage edge devices in physical proximity and associate
    them as an edge resource pool for in-situ personal LLMs fine-tuning. Our fine-tuning
    process can be divided into two phases: (1) For the first epoch, the LLMs backbone,
    augmented with Parallel Adapters, is fine-tuned across multiple edge devices.
    To enhance scalability and training throughput, a hybrid parallelism approach
    that combines the merits of both data and pipeline parallelism is employed by
    PAC as a principle to manage collaborative training across multiple edge devices.
    (2) In subsequent fine-tuning epochs, the activation cache obviates the need for
    forward propagation through the LLM backbone, allowing for the exclusive fine-tuning
    of our Parallel Adapters using data parallelism.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ （系统）我们利用物理上接近的边缘设备，并将其作为在场个人LLM微调的边缘资源池。我们的微调过程可以分为两个阶段：（1）在第一个周期中，增强了并行Adapters的LLM骨干网络在多个边缘设备上进行微调。为了提高可扩展性和训练吞吐量，PAC采用了一种结合数据和流水线并行优势的混合并行方法，作为管理多个边缘设备协作训练的原则。（2）在随后的微调周期中，激活缓存省去了通过LLM骨干网络的前向传播，从而允许仅使用数据并行进行我们的并行Adapters的微调。
- en: We implement PAC in realistic testbeds with a cluster of edge devices. Extensive
    evaluations across three LLMs demonstrate that PAC not only accelerates fine-tuning
    up to $8.64\times$, without sacrificing model performance. The main contributions
    are summarized as follows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在具有一组边缘设备的实际测试环境中实现了PAC。对三种LLM的广泛评估表明，PAC不仅能加速微调最多达到$8.64\times$，而且不会牺牲模型性能。主要贡献总结如下。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We carry out extensive measurement studies on predominant PEFT techniques on
    resource-constrained edge devices and demonstrate that they are not sufficiently
    resource-efficient.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对资源受限的边缘设备上主要的PEFT技术进行了广泛的测量研究，并证明它们在资源效率上不够理想。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a not only parameter but also resource efficient LLM fine-tuning technique
    for resource-limited edge environments.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种不仅参数优化而且资源高效的LLM微调技术，适用于资源有限的边缘环境。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a time and memory efficient collaborative edge AI framework PAC for
    the in-situ fine-tuning of personal LLMs, which combines sophisticated algorithm-system
    co-design.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个时间和内存高效的协作边缘AI框架PAC，用于个人LLM的就地微调，该框架结合了复杂的算法-系统共同设计。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We implement PAC and evaluate it in realistic edge testbeds. Experimental results
    show up to $8.64\times$ memory reduction without sacrificing performance compared
    to state-of-the-art methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实现了PAC并在实际的边缘测试平台上进行了评估。实验结果显示，相比于最先进的方法，在不牺牲性能的情况下，内存减少了多达$8.64\times$。
- en: '![Refer to caption](img/a0a5cedce86b76470a6fc2bb57edf2ea.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a0a5cedce86b76470a6fc2bb57edf2ea.png)'
- en: Figure 2\. Illustration of the model structures with two PEFT.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 模型结构示意图，展示了两种PEFT。
- en: 2\. MOTIVATION AND PRELIMINARIES
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 动机和基础
- en: 2.1\. Transformer-Based LLMs and Fine-Tuning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 基于Transformer的LLM和微调
- en: 'Transformer-Based LLMs. Transformer-based LLMs have gained prominence in various
    language-related applications due to their impressive performance. These models
    consist of multiple Transformer layers, each comprising two main components: the
    Multi-head Attention and the Feed Forward block. The Multi-head Attention block
    utilizes linear layers to generate query (Q), key (K), and value (V) matrices
    for each attention head, allowing for independent self-attention computations.
    The outputs of these attention heads are then concatenated and processed through
    a final linear layer. The Feed Forward block involves two linear operations that
    increase the hidden size from $h$.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的LLM。基于Transformer的LLM在各种语言相关应用中获得了显著的关注，因其表现优异。这些模型由多个Transformer层组成，每层包括两个主要组件：多头注意力和前馈块。多头注意力块利用线性层为每个注意力头生成查询（Q）、键（K）和值（V）矩阵，允许独立的自注意力计算。这些注意力头的输出被连接起来，并通过最终的线性层处理。前馈块涉及两个线性操作，将隐藏层大小从$h$增加。
- en: 'Personal LLMs Fine-Tuning. The training of LLMs typically consists of two stages:
    pre-training and fine-tuning. Before being deployed for specific tasks, language
    models are often pre-trained on extensive text datasets containing vast linguistic
    data. The pre-training process enables the model to acquire a general understanding
    of linguistic structure and patterns that are widely applicable. The fine-tuning
    adapts the pre-trained model to various, concrete downstream language tasks such
    as intelligent personal assistants. During actual deployment, the data required
    for fine-tuning is often generated at the user end, which can carry significant
    concerns regarding data security and privacy. In recent years, in-situ learning
    on edge devices (Patil et al., [2022](#bib.bib20); Lin et al., [2022](#bib.bib16);
    Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)) has emerged as
    a promising approach for customizing LLMs while preserving user data fully in-situ.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 个人LLM微调。LLM的训练通常包括两个阶段：预训练和微调。在特定任务部署之前，语言模型通常在包含大量语言数据的广泛文本数据集上进行预训练。预训练过程使模型获得对广泛适用的语言结构和模式的一般理解。微调则将预训练的模型适配到各种具体的下游语言任务中，例如智能个人助理。在实际部署过程中，微调所需的数据通常在用户端生成，这可能涉及重要的数据安全和隐私问题。近年来，在边缘设备上的就地学习（Patil等，[2022](#bib.bib20)；Lin等，[2022](#bib.bib16)；Gim和Ko，[2022](#bib.bib6)；Wang等，[2022](#bib.bib24)）作为一种有前景的方法，能够在完全保留用户数据的情况下定制LLM。
- en: '![Refer to caption](img/11faa94b183b15703eb9a2abccb91113.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/11faa94b183b15703eb9a2abccb91113.png)'
- en: 'Figure 3\. The comparison of floating point of operations (FLOPs). Mini-batch
    size: 16; sequence length: 128.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 浮点运算数（FLOPs）的比较。小批量大小：16；序列长度：128。
- en: '| Techniques | Trainable Parameters | Memory Footprint (GB) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 可训练参数 | 内存占用 (GB) |'
- en: '| Weights | Activations | Gradients | Total |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 权重 | 激活 | 梯度 | 总计 |'
- en: '| Full | 737M (100%) | 2.75 | 5.33 | 2.75 | 10.83 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | 737M (100%) | 2.75 | 5.33 | 2.75 | 10.83 |'
- en: '| Adapters | 12M (1.70 %) | 2.80 | 4.04 | 0.05 | 6.89 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 适配器 | 12M (1.70 %) | 2.80 | 4.04 | 0.05 | 6.89 |'
- en: '| LoRA | 9M (1.26%) | 2.78 | 4.31 | 0.04 | 7.13 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 9M (1.26%) | 2.78 | 4.31 | 0.04 | 7.13 |'
- en: '| Inference | / | 2.75 | / | / | 2.75 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | / | 2.75 | / | / | 2.75 |'
- en: 'Table 1\. The breakdown of memory footprint. "Activations" contain the intermediate
    results and optimizer states. Model: T5-Large; mini-batch size: 16; sequence length:
    128.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 内存占用情况的细分。“激活”包含中间结果和优化器状态。模型：T5-Large；小批量大小：16；序列长度：128。
- en: 'Full model fine-tuning updates all parameters of an LLM for a specific downstream
    task. However, it is impractical for adapting an LLM to multiple distinct downstream
    tasks, as each target task would require maintaining a separate LLM with whole
    parameters. Some leading researchers have proposed parameter-efficient fine-tuning
    (PEFT) techniques (Lester et al., [2021](#bib.bib13); Houlsby et al., [2019](#bib.bib10);
    Hu et al., [2021](#bib.bib11); Liu et al., [2024](#bib.bib17)) which adapt a small
    subset of the LLM parameters or a set of newly added parameters for each new task.
    Adapters (Houlsby et al., [2019](#bib.bib10)) and LoRA (Hu et al., [2021](#bib.bib11))
    are two of the most widely used PEFT techniques. Figure [2](#S1.F2 "Figure 2 ‣
    1\. Introduction ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") illustrates how the transformer
    layer structure incorporates these two techniques. Specifically, adapters are
    compact bottleneck modules inserted at the end of each transformer layer. Similarly,
    LoRA injects trainable low-rank matrices into a frozen pre-trained model. These
    decompose the weight matrix parameter updates into two learnable low-rank matrices.
    Extensive experiments have demonstrated that these PEFT techniques can achieve
    performance comparable to full fine-tuning.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完全模型微调会更新LLM的所有参数以适应特定的下游任务。然而，将LLM适应多个不同的下游任务是不切实际的，因为每个目标任务都需要维护一个具有所有参数的独立LLM。一些领先的研究人员提出了参数高效微调（PEFT）技术（Lester
    et al., [2021](#bib.bib13); Houlsby et al., [2019](#bib.bib10); Hu et al., [2021](#bib.bib11);
    Liu et al., [2024](#bib.bib17)），这些技术为每个新任务适配LLM参数的一个小子集或一组新添加的参数。Adapters（Houlsby
    et al., [2019](#bib.bib10)）和LoRA（Hu et al., [2021](#bib.bib11)）是两种最广泛使用的PEFT技术。图[2](#S1.F2
    "图2 ‣ 1\. 介绍 ‣ Pluto和Charon：用于个人LLMs微调的时间和内存高效协作边缘AI框架")展示了transformer层结构如何结合这两种技术。具体来说，adapters是插入到每个transformer层末尾的紧凑瓶颈模块。类似地，LoRA将可训练的低秩矩阵注入到冻结的预训练模型中。这些技术将权重矩阵参数更新分解为两个可学习的低秩矩阵。大量实验表明，这些PEFT技术可以达到与完全微调相当的性能。
- en: Although these PEFT techniques can greatly reduce the number of trainable parameters
    (around $98\%$. The reason is that both Adapters and LoRA introduce trainable
    structures within the LLM backbone, such as at the end of each transformer block
    or as bypasses to linear layers. Computing gradients for trainable parameters
    via backpropagation involves traversing the LLM backbone, compromising the efficiency
    of PEFT techniques due to the additional computational overhead and memory required
    to maintain considerable intermediate activations in LLM backbone.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些PEFT技术可以大幅减少可训练参数的数量（大约$98\%$）。原因在于，Adapters和LoRA都在LLM骨干网络中引入了可训练的结构，例如在每个transformer块的末尾或作为线性层的旁路。通过反向传播计算可训练参数的梯度涉及遍历LLM骨干网络，这会由于额外的计算开销和保持大量中间激活值所需的内存，影响PEFT技术的效率。
- en: 2.2\. Personal LLMs Fine-Tuning with Resource-Constrained Edge Devices
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 资源受限边缘设备上的个人LLMs微调
- en: 'On-device fine-tuning enables leveraging idle resources at the edge while fully
    preserving user data privacy (Patil et al., [2022](#bib.bib20); Lin et al., [2022](#bib.bib16);
    Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)). This paradigm
    is widely adopted in privacy-sensitive edge computing applications. However, the
    resource-intensive nature of LLMs fine-tuning presents two significant challenges
    for resource-limited edge devices: (1) The computational capabilities of edge
    devices are constrained. Edge devices often face stark computational constraints
    compared to the powerful accelerators available in cloud datacenters. The Jetson
    Nano (jet, [2019](#bib.bib2)), a specialized platform for edge AI, peaks at a
    mere $0.47$ of the parameters, they still require substantial memory $6.89$, often
    insufficient for typical mobile devices with 4-12 GB DRAM to run system software
    and applications.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设备端细化能够利用边缘的闲置资源，同时完全保护用户数据隐私（Patil 等，[2022](#bib.bib20)；Lin 等，[2022](#bib.bib16)；Gim
    和 Ko，[2022](#bib.bib6)；Wang 等，[2022](#bib.bib24)）。这一范式在隐私敏感的边缘计算应用中被广泛采用。然而，LLM
    细化的资源密集型特性给资源有限的边缘设备带来了两个重大挑战：（1）边缘设备的计算能力受限。与云数据中心中强大的加速器相比，边缘设备往往面临严峻的计算限制。Jetson
    Nano（jet，[2019](#bib.bib2)），作为一个专门的边缘 AI 平台，其参数仅达到 $0.47$，但仍需要大量内存 $6.89$，通常不足以让典型的
    4-12 GB DRAM 移动设备运行系统软件和应用程序。
- en: To break the resource wall of a single edge device, in our work, we alternatively
    observe that prevalent edge scenarios usually comprise a group of trusted idle
    devices beyond a single terminal. These accompanying devices are typically located
    in close physical proximity, such as being connected to the same local area network
    (LAN), and can be utilized as a resource augmentation for in-situ LLMs fine-tuning
    acceleration. While several pioneering research works (Ye et al., [2024a](#bib.bib29);
    Wei et al., [2024](#bib.bib25)) have delved into collaborative edge computing
    to overcome resource limitations faced by edge devices, the majority of these
    works primarily focus on LLMs inference. Other studies (Cai et al., [2023](#bib.bib5);
    Xu et al., [2024](#bib.bib28)) employing federated learning for fine-tuning LLMs
    with collaborative edge devices primarily address the dissolution of data silos,
    rather than resource augmentation within LANs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了打破单个边缘设备的资源壁垒，我们的工作中观察到，普遍的边缘场景通常包括一组受信任的闲置设备，而不仅仅是单一终端。这些附带设备通常物理位置相近，例如连接到同一个局域网（LAN），可以用作在地
    LLM 细化加速的资源扩展。虽然一些开创性的研究工作（Ye 等，[2024a](#bib.bib29)；Wei 等，[2024](#bib.bib25)）已经深入探讨了协作边缘计算以克服边缘设备面临的资源限制，但这些工作大多数主要集中在
    LLM 的推理上。其他研究（Cai 等，[2023](#bib.bib5)；Xu 等，[2024](#bib.bib28)）采用联合学习来细化与协作边缘设备的
    LLM，主要关注数据孤岛的消解，而不是局域网内的资源扩展。
- en: '![Refer to caption](img/57879c4b56dc1f4c7e6aa5b609b738ad.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57879c4b56dc1f4c7e6aa5b609b738ad.png)'
- en: Figure 4\. PAC workflow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. PAC 工作流。
- en: 3\. System Overview
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 系统概述
- en: PAC is a time, memory and parameter efficient collaborative framework for personal
    LLMs fine-tuning across multiple edge devices. PAC first equips the target LLM
    with our Parallel Adapters module (Step 0). PAC profiler
    fine-tunes the LLM using a calibration dataset on edge devices to record the runtime
    profile required for parallelism planning (Step 1). PAC planner
    then takes the profiling results as input and generates planning configurations,
    including LLM partitioning points and device grouping strategies (Step 2).
    We configure the Parallel Adapters as trainable while freezing the LLM backbone
    parameters (Step 3). The parallel
    configurations generated by the PAC planner are then applied to the edge devices,
    enabling time, memory, and parameter-efficient hybrid data and pipeline parallelism
    fine-tuning of the target LLM (Step 4). Since the
    LLM backbone parameters remain fixed, the intermediate activations generated by
    the backbone model are invariant for a given input sequence. The PAC maintains
    a cache of these invariant activations. Through leveraging the cached activations,
    the efficiency of the fine-tuning process can be accelerated (Step 5).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PAC 是一个高效的协作框架，用于在多个边缘设备上对个人 LLM 进行微调，兼顾时间、内存和参数的效率。PAC 首先为目标 LLM 配置我们的 Parallel
    Adapters 模块（第 0 步骤）。PAC profiler
    使用校准数据集对 LLM 进行微调，以记录并行规划所需的运行时配置（第 1 步骤）。PAC planner
    然后将这些分析结果作为输入，生成规划配置，包括 LLM 划分点和设备分组策略（第 2 步骤）。我们将 Parallel
    Adapters 配置为可训练，同时冻结 LLM 主干参数（第 3 步骤）。PAC planner
    生成的并行配置随后应用到边缘设备上，实现对目标 LLM 的时间、内存和参数高效的混合数据和流水线并行微调（第 4
    步骤）。由于 LLM 主干参数保持固定，主干模型生成的中间激活对于给定输入序列是不变的。PAC 维护这些不变激活的缓存。通过利用缓存的激活，微调过程的效率可以得到提升（第
    5
    步骤）。
- en: 4\. Time, Memory and Parameter Efficient Fine-Tuning Algorithm
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 时间、内存和参数高效微调算法
- en: 4.1\. Fine-Tuning LLMs with Parallel Adapters
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 使用 Parallel Adapters 对 LLM 进行微调
- en: 'Observation and Key Insight. As discussed in §[2](#S2 "2\. MOTIVATION AND PRELIMINARIES
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"), while techniques such as LoRA (Hu et al., [2021](#bib.bib11))
    and Adapters (Houlsby et al., [2019](#bib.bib10)) reduce the number of parameters
    that need to be updated during fine-tuning, they do not significantly reduce the
    computational and memory requirements during the training on edge devices. This
    is because the parameters being updated are still inside the LLM backbone. To
    calculate the gradients for backpropagation, the full backward passes through
    the entire pre-trained model are still necessary, as illustrated in Figure [5](#S4.F5
    "Figure 5 ‣ 4.1\. Fine-Tuning LLMs with Parallel Adapters ‣ 4\. Time, Memory and
    Parameter Efficient Fine-Tuning Algorithm ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")(a) and
    (b). In the research field of AI, side-tuning (Zhang et al., [2020](#bib.bib35))
    is a specialized fine-tuning technique. It adds a trainable side network that
    runs in parallel to the backbone model, with the side network’s representation
    summed with the backbone’s output in the final layer. Crucially, side-tuning only
    updates the side network, without backpropagating through the backbone model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 观察与关键见解。正如在 §[2](#S2 "2\. 动机与基础知识 ‣ Pluto 和 Charon：用于个人 LLM 微调的时间与内存高效协作边缘 AI
    框架") 中讨论的那样，虽然 LoRA (Hu et al., [2021](#bib.bib11)) 和 Adapters (Houlsby et al.,
    [2019](#bib.bib10)) 等技术减少了微调过程中需要更新的参数数量，但它们并没有显著减少在边缘设备上训练时的计算和内存需求。这是因为被更新的参数仍然在
    LLM 主干网络内部。为了计算反向传播的梯度，仍然需要通过整个预训练模型进行完整的反向传递，如图 [5](#S4.F5 "图 5 ‣ 4.1\. 使用 Parallel
    Adapters 对 LLM 进行微调 ‣ 4\. 时间、内存与参数高效微调算法 ‣ Pluto 和 Charon：用于个人 LLM 微调的时间与内存高效协作边缘
    AI 框架") (a) 和 (b) 所示。在人工智能研究领域，side-tuning (Zhang et al., [2020](#bib.bib35))
    是一种专门的微调技术。它添加了一个可训练的侧网络，该网络与主干模型并行运行，并在最终层中将侧网络的表示与主干输出进行求和。关键是，side-tuning 只更新侧网络，而不通过主干模型进行反向传播。
- en: '![Refer to caption](img/c00b2abea6e9315a36e2cb3f5b137344.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c00b2abea6e9315a36e2cb3f5b137344.png)'
- en: Figure 5\. Comparison between LLMs fine-tuning with LoRA, Adapters, and our
    Parallel Adapters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 使用 LoRA、Adapters 和我们的 Parallel Adapters 对 LLM 进行微调的比较。
- en: 'Parallel Adapters Architecture. In light of side-tuning, we employ a time and
    memory efficient personal LLMs fine-tuning technique with Parallel Adapters. The
    overall structure is illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning
    LLMs with Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning
    Algorithm ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI
    Framework for Personal LLMs Fine-Tuning")(c). Specifically, we decouple conventional
    Adapters (Houlsby et al., [2019](#bib.bib10)) from the LLM backbone, avoiding
    their integration at the end of each transformer layer. Instead, we provide a
    dedicated parallel highway for our trainable adapters network, which takes intermediate
    activations from the backbone transformer as input and generates the final predictions.
    In this way, backpropagation through the LLM backbone is free, reducing memory
    demands for massive activations and computational burdens, thereby enhancing time
    and memory efficiency over techniques like Adapters and LoRA. Our adapters module
    demonstrates comprehensive compatibility with established LLM fine-tuning adapters
    architectures, including the use of linear layers for upward and downward projections
    as well as trimmed lightweight versions of the backbone transformer (Houlsby et al.,
    [2019](#bib.bib10); Han et al., [2024](#bib.bib8); Sung et al., [2022](#bib.bib22)).
    To ensure the lightweight and resource-efficient nature of our parallel network,
    the hidden dimension of our Parallel Adapters will be $r$ intermediate outputs
    $\mathrm{b}_{1},\mathrm{b}_{2},\ldots\mathrm{b}_{L}$. We denote the embedding
    input sequence as $\mathrm{b}_{0}\in\mathbb{R}^{n\times d}$, $\mathbf{a}_{i}\in\mathbb{R}^{n\times
    r}$ for $i$-th adapter of our Parallel Adapters, which operate on these intermediate
    outputs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '平行适配器架构。鉴于旁调，我们采用了一种时间和内存高效的个人 LLM 微调技术，结合平行适配器。整体结构如图 [5](#S4.F5 "图 5 ‣ 4.1\.
    使用平行适配器微调 LLMs ‣ 4\. 时间、内存和参数高效微调算法 ‣ Pluto 和 Charon: 一个时间和内存高效的协作边缘 AI 框架，用于个人
    LLM 的微调")(c) 所示。具体来说，我们将传统适配器（Houlsby et al., [2019](#bib.bib10)）从 LLM 主干中解耦，避免了在每个变换器层末尾进行整合。相反，我们为我们的可训练适配器网络提供了一条专用的平行通道，该通道从主干变换器中获取中间激活作为输入，并生成最终预测。这样，主干
    LLM 的反向传播是免费的，减少了大规模激活的内存需求和计算负担，从而提高了相对于适配器和 LoRA 等技术的时间和内存效率。我们的适配器模块与既定的 LLM
    微调适配器架构展示了全面的兼容性，包括用于上行和下行投影的线性层，以及主干变换器的精简轻量版（Houlsby et al., [2019](#bib.bib10)；Han
    et al., [2024](#bib.bib8)；Sung et al., [2022](#bib.bib22)）。为了确保我们的平行网络的轻量级和资源高效性，我们的平行适配器的隐藏维度将是
    $r$ 个中间输出 $\mathrm{b}_{1},\mathrm{b}_{2},\ldots\mathrm{b}_{L}$。我们将嵌入输入序列表示为 $\mathrm{b}_{0}\in\mathbb{R}^{n\times
    d}$，$\mathbf{a}_{i}\in\mathbb{R}^{n\times r}$ 表示我们平行适配器的第 $i$ 个适配器，它们作用于这些中间输出。'
- en: '| (1) |  | $\displaystyle\mathrm{a}_{i}=f_{i}(\mathrm{b}_{i},\mathrm{a}_{i-1}).$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\mathrm{a}_{i}=f_{i}(\mathrm{b}_{i},\mathrm{a}_{i-1}).$
    |  |'
- en: 'Our evaluation in §[6](#S6 "6\. Evaluation ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") reveals
    that parallel adapters can achieve comparable model performance to mainstream
    fine-tuning techniques while being more resource-efficient and better suited for
    resource-constrained edge environments.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 §[6](#S6 "6\. 评估 ‣ Pluto 和 Charon: 一个时间和内存高效的协作边缘 AI 框架，用于个人 LLM 的微调")
    中的评估表明，平行适配器可以实现与主流微调技术相当的模型性能，同时在资源利用上更高效，更适合资源有限的边缘环境。'
- en: 4.2\. PAC Activation Cache for Parallel Adapters
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. PAC 激活缓存用于平行适配器
- en: 'Observation and Opportunities. Leveraging Parallel Adapters substantially diminishes
    the computational and memory demands by circumventing backward propagation through
    the LLM backbone. However, for edge environments with limited resources, forward
    propagation calculations on the backbone of LLMs also require substantial computational
    resources. Figure [3](#S2.F3 "Figure 3 ‣ 2.1\. Transformer-Based LLMs and Fine-Tuning
    ‣ 2\. MOTIVATION AND PRELIMINARIES ‣ Pluto and Charon: A Time and Memory Efficient
    Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") demonstrates that
    the computational overhead for forward propagation constitutes $54\%$ of the total
    overhead when fine-tuning the T5-Large with Adapters and LoRA, respectively.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 观察与机会。利用Parallel Adapters可以显著减少计算和内存需求，因为它避免了通过LLM骨干进行反向传播。然而，对于资源有限的边缘环境，LLMs骨干的前向传播计算也需要大量计算资源。图[3](#S2.F3
    "图 3 ‣ 2.1\. 基于Transformer的LLMs和微调 ‣ 2\. 动机与基础 ‣ Pluto和Charon：一个时间和内存高效的协作边缘AI框架，用于个人LLMs微调")显示了前向传播的计算开销在使用Adapters和LoRA微调T5-Large时，占总开销的$54\%$。
- en: 'To minimize the computational demand, we identify two distinct opportunities
    for utilizing Parallel Adapters in in-situ fine-tuning of LLMs: (1) During the
    pre-training phase of LLMs, due to the vast volumes of data involved, researchers
    typically train for only one epoch, meaning each sequence input is processed by
    the model a single time. However, in typical in-situ LLM fine-tuning scenarios,
    users often utilize small datasets collected from their specific context, repeatedly
    training the models with these inputs until achieving model convergence. (2) When
    employing parallel adapters to fine-tune LLMs, the parameters of the LLM backbone
    remain fixed. Unlike other PEFT techniques, the LLM backbone operates independently
    of the intermediate outputs generated by Parallel Adapters. Consequently, for
    a given input sequence, the activations generated by the LLM backbone are always
    invariant.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化计算需求，我们识别出在LLMs的现场微调中利用Parallel Adapters的两个不同机会：（1）在LLMs的预训练阶段，由于涉及的数据量庞大，研究人员通常只训练一个周期，这意味着每个序列输入只被模型处理一次。然而，在典型的现场LLM微调场景中，用户通常利用从特定背景中收集的小数据集，通过这些输入反复训练模型，直到模型收敛。（2）在使用并行适配器微调LLMs时，LLM骨干的参数保持固定。与其他PEFT技术不同，LLM骨干独立于由Parallel
    Adapters生成的中间输出进行操作。因此，对于给定的输入序列，LLM骨干生成的激活始终是不变的。
- en: 'Fine-Tuning Parallel Adapters with PAC Activation Cache. Our key idea leverages
    the frozen parameters of the backbone model, enabling the caching of activations
    produced during the forward propagation of the same input sequence, thereby facilitating
    their reuse across multiple epochs (Cai et al., [2023](#bib.bib5)). As discussed
    in §[4.1](#S4.SS1 "4.1\. Fine-Tuning LLMs with Parallel Adapters ‣ 4\. Time, Memory
    and Parameter Efficient Fine-Tuning Algorithm ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning"), the
    parallel adapters are a lightweight, separate network that takes the intermediate
    activations from the backbone transformer as input and generates predictions.
    During the first epoch, when processing a new input sequence, we cache all the
    input activations required by the Parallel Adapters that are obtained from the
    LLM backbone, as illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning
    LLMs with Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning
    Algorithm ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI
    Framework for Personal LLMs Fine-Tuning")(c), highlighted by the red circle. In
    subsequent fine-tuning epochs using the same input sequence, we can skip the forward
    propagation through the LLM backbone entirely, since the required activations
    have already been cached. The combination of Parallel Adapters and activation
    caching allows efficient fine-tuning of the LLMs without the need for both forward
    and backward propagation through the backbone network, thereby (1) significantly
    accelerating the fine-tuning process and (2) reducing the memory footprint by
    allowing the release of the memory space occupied by the LLM parameters.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 PAC 激活缓存微调 Parallel Adapters。我们的核心理念是利用主干模型的冻结参数，从而缓存同一输入序列的前向传播中产生的激活，并使其在多个时期内重复使用（Cai
    等， [2023](#bib.bib5)）。如 §[4.1](#S4.SS1 "4.1\. 使用 Parallel Adapters 微调 LLMs ‣ 4\.
    时间、内存和参数高效的微调算法 ‣ Pluto 和 Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLMs 微调") 所讨论的，Parallel
    Adapters 是一个轻量级的独立网络，它以主干变换器的中间激活为输入，生成预测。在第一个时期，当处理新的输入序列时，我们缓存 Parallel Adapters
    所需的所有输入激活，这些激活是从 LLM 主干获取的，如图 [5](#S4.F5 "图 5 ‣ 4.1\. 使用 Parallel Adapters 微调
    LLMs ‣ 4\. 时间、内存和参数高效的微调算法 ‣ Pluto 和 Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLMs 微调")(c)
    中红色圆圈所示。在后续的微调时期中，使用相同的输入序列时，我们可以完全跳过 LLM 主干的前向传播，因为所需的激活已经被缓存。Parallel Adapters
    和激活缓存的结合允许高效微调 LLMs，无需通过主干网络进行前向和反向传播，从而 (1) 显著加快微调过程，并 (2) 通过释放占用的 LLM 参数的内存空间来减少内存占用。'
- en: 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 高效个人 LLMs 微调的协作边缘 AI 系统
- en: 'In PAC, we leverage edge devices in physical proximity and associate them as
    a resource pool to boost in-situ fine-tuning. Specifically, the fine-tuning procedure
    comprises two phases: (1) In the initial epoch, the backbone of LLMs, enhanced
    with Parallel Adapters, undergoes fine-tuning across multiple edge devices through
    a blend of data and pipeline parallelism (§[5.1](#S5.SS1 "5.1\. Resource-Efficient
    Collaborative Orchestration for LLMs Fine-Tuning ‣ 5\. Collaborative Edge AI System
    for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")); (2)
    In subsequent epochs, the activation cache eliminates the necessity for forward
    propagation within the backbone, thereby enabling the exclusive fine-tuning of
    our Parallel Adapters utilizing data parallelism (§[5.2](#S5.SS2 "5.2\. Cache-Enabled
    Collaborative Edge Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative Edge AI
    System for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and
    Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '在 PAC 中，我们利用物理邻近的边缘设备，并将它们关联为资源池，以提升现场微调的效果。具体来说，微调过程包括两个阶段：（1）在初始时期，通过数据和管道并行的结合，对增强了
    Parallel Adapters 的 LLMs 主干进行跨多个边缘设备的微调（§[5.1](#S5.SS1 "5.1\. 资源高效的协作编排用于 LLMs
    微调 ‣ 5\. 高效个人 LLMs 微调的协作边缘 AI 系统 ‣ Pluto 和 Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLMs
    微调")）；（2）在后续的时期中，激活缓存消除了主干内前向传播的必要，从而使我们能够仅利用数据并行对 Parallel Adapters 进行微调（§[5.2](#S5.SS2
    "5.2\. 缓存支持的协作边缘微调 Parallel Adapters ‣ 5\. 高效个人 LLMs 微调的协作边缘 AI 系统 ‣ Pluto 和 Charon:
    一个时间和内存高效的协作边缘 AI 框架用于个人 LLMs 微调")）。'
- en: 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 资源高效的 LLM 微调协作调度
- en: Observation of Data and Pipeline Parallelism at the Edge. When collaborating
    on LLM fine-tuning among edge devices, the principle question is which type of
    parallelism should be used. The most common way to train models in parallel is
    data parallelism (DP) (Hao and Zhang, [2021](#bib.bib9)). However, DP necessitates
    that each device maintains a replica of the entire model, a requirement difficult
    to meet for LLMs with extensive parameter sizes, often surpassing the capacity
    of a single device. Pipeline parallelism (PP) (Ye et al., [2022](#bib.bib31))
    is further proposed to address this problem. In PP, the model is partitioned into
    multiple consecutive stages and each stage is mapped to a separate device. Consequently,
    PP enables the training of increasingly large models by deploying more devices.
    Nonetheless, PP encounters scalability constraints as the addition of edge devices
    results in more stages. This not only results in a significant presence of pipeline
    bubbles but also amplifies the impact of inter-stage communication latency, thereby
    hindering efficiency. The above observation motivates us to employ a hybrid parallelism
    (HP) architecture that incorporates the best of both DP and PP, so as to achieve
    superior performance and scalability in resource-constrained edge environments.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上观察数据和管道并行性。在边缘设备上进行 LLM 微调时，主要问题是应该使用哪种类型的并行性。最常见的并行训练模型的方法是数据并行性（DP）（Hao
    和 Zhang，[2021](#bib.bib9)）。然而，DP 需要每个设备维护整个模型的副本，这对于具有大量参数的 LLM 来说是一个难以满足的要求，通常超出了单个设备的容量。管道并行性（PP）（Ye
    等，[2022](#bib.bib31)）被进一步提出以解决这个问题。在 PP 中，模型被划分为多个连续的阶段，每个阶段被映射到一个单独的设备上。因此，PP
    通过部署更多设备来实现对越来越大模型的训练。然而，PP 遇到了可扩展性限制，因为边缘设备的增加导致更多阶段的出现。这不仅导致了大量的管道空隙，还加剧了阶段间通信延迟的影响，从而阻碍了效率。上述观察促使我们采用一种混合并行（HP）架构，结合
    DP 和 PP 的优点，以在资源受限的边缘环境中实现更优的性能和可扩展性。
- en: '![Refer to caption](img/697e1a15b3bb48cbbe030fd569cc3624.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/697e1a15b3bb48cbbe030fd569cc3624.png)'
- en: (a) The LLM transformer layers is partitioned into two stages, where both Stage
    0 and 1 are replicated on a device group with two devices for intra-stage data
    parallelism.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLM 转换器层被分为两个阶段，其中第 0 阶段和第 1 阶段都在一个包含两个设备的设备组上进行复制，以实现阶段内数据并行性。
- en: '![Refer to caption](img/687119dfa74008fd91d63c12d5bfb638.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/687119dfa74008fd91d63c12d5bfb638.png)'
- en: (b) Fine-tuning pipeline of 6 micro-batches. The numbers in the cells represent
    micro-batch ids. AllReduce (AR) is performed in both Stage 0 and 1 for model synchronization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 6 个微批次的微调管道。单元格中的数字表示微批次 id。在第 0 阶段和第 1 阶段都执行 AllReduce（AR）以进行模型同步。
- en: Figure 6\. An instance of hybrid parallelism in PAC.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. PAC 中混合并行的一个实例。
- en: 'Hybrid Parallelism Architecture in PAC. As illustrated in Figure [6(a)](#S5.F6.sf1
    "In Figure 6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning"), PAC first divides an LLM into multiple stages where each contains
    a stage model composed of a set of consecutive transformer layer. Edge devices
    are allocated into several device groups, each comprising one or more devices.
    PAC maps each stage to a group, with the stage model replicated across all devices
    within that group. Throughout the fine-tuning process, a mini-batch is divided
    into several micro-batches for concurrent processing to enhance parallelism. If
    a device cluster hosts multiple devices, micro-batches are further subdivided.
    Each device is responsible for executing the forward (FP) and backward passes
    (BP) for its assigned stage model and aggregates gradients across all micro-batches
    for every mini-batch. Upon completing a mini-batch, gradient synchronization within
    each device group is achieved through AllReduce. Since the majority of parameters
    in LLMs are frozen, AllReduce synchronizes only the lightweight parallel adapters,
    ensuring a swift process. We adopt the one-forward-one-backward ($1$B) micro-batch
    scheduling (Narayanan et al., [2019](#bib.bib19)) which schedules the BP early
    to release the activation memory produced by FP for reuse. Figure [6(b)](#S5.F6.sf2
    "In Figure 6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning") depicts a well-structured hybrid parallelism, encompassing
    FP, BP, and inter-stage communication.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: PAC中的混合并行架构。如图 [6(a)](#S5.F6.sf1 "在图6 ‣ 5.1\. 资源高效的LLM微调协作编排 ‣ 5\. 高效个人LLM微调的协作边缘AI系统
    ‣ Pluto和Charon：一个时间和内存高效的个人LLM微调协作边缘AI框架") 所示，PAC首先将LLM划分为多个阶段，每个阶段包含一个由一组连续变换层组成的阶段模型。边缘设备被分配到几个设备组，每个组包含一个或多个设备。PAC将每个阶段映射到一个组，阶段模型在该组的所有设备上复制。在微调过程中，一个小批次被划分为多个微批次进行并行处理，以增强并行性。如果一个设备集群承载多个设备，微批次将进一步细分。每个设备负责执行其分配的阶段模型的前向传播（FP）和反向传播（BP），并在每个小批次中汇总所有微批次的梯度。完成一个小批次后，通过AllReduce实现每个设备组内的梯度同步。由于LLM中的大多数参数是冻结的，AllReduce仅同步轻量级并行适配器，从而确保过程迅速。我们采用了一前向一反向（$1$B）微批次调度（Narayanan
    et al., [2019](#bib.bib19)），该调度方法提前安排BP以释放FP产生的激活内存以便重用。图 [6(b)](#S5.F6.sf2 "在图6
    ‣ 5.1\. 资源高效的LLM微调协作编排 ‣ 5\. 高效个人LLM微调的协作边缘AI系统 ‣ Pluto和Charon：一个时间和内存高效的个人LLM微调协作边缘AI框架")
    展示了一个结构良好的混合并行方案，涵盖了FP、BP和阶段间通信。
- en: Profiling. To enable parallelism planning, PAC profiler first fine-tunes the
    target LLM using calibration datasets to record the runtime profile required for
    planning. We define $t_{f}^{d,l}(\beta)$ with batch size of $\beta$. The size
    of output activations, input gradients, and weight parameters in bytes will also
    be collected to calculate memory footprint.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析。为了启用并行规划，PAC性能分析器首先使用标定数据集对目标LLM进行微调，以记录规划所需的运行时性能。我们定义批次大小为$\beta$的$t_{f}^{d,l}(\beta)$。还将收集输出激活、输入梯度和权重参数的字节大小，以计算内存占用。
- en: 'Planning Algorithm for Hybrid Parallelism. The global throughput of a pipeline
    is determined by the execution time of the slowest stage. Consequently, our algorithm
    endeavors to partition the model into balanced stages. We consider an LLM consisting
    of $L$ devices in $\mathcal{D}$ with $\mathcal{D}_{n}$ stages. To solve this partitioning
    problem, we break the pipeline into sub-pipelines and leverage the idea of dynamic
    programming. The formula of the dynamic programming algorithm can be written as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 混合并行规划算法。流水线的全局吞吐量由最慢阶段的执行时间决定。因此，我们的算法力求将模型划分为平衡的阶段。我们考虑一个由$\mathcal{D}$中的$L$个设备组成的LLM，其中$\mathcal{D}_{n}$为阶段。为了解决这个划分问题，我们将流水线分解为子流水线，并利用动态规划的思想。动态规划算法的公式可以写成：
- en: '| (2) |  | $\displaystyle W(0\rightarrow y,\mathcal{D}_{n},s)=$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle W(0\rightarrow y,\mathcal{D}_{n},s)=$ |  |'
- en: '|  |  | $\displaystyle T(q+1\rightarrow y,\{d_{n-m}\ldots,d_{n-1}\})\},$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle T(q+1\rightarrow y,\{d_{n-m}\ldots,d_{n-1}\})\},$ |  |'
- en: 'where the first term inside the max is the time of the optimally balanced sub-pipeline
    between layers $0$ to $y$:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 max 内的第一个项是层 $0$ 到 $y$ 之间的优化平衡子管道的时间：
- en: '| (3) |  | $1$2 |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $1$2 |  |'
- en: where $M$ is the sum of the memory usage of the LLM parameters, parameter gradients,
    and activations. Without out-of-memory (OOM) exceptions, total data-parallel execution
    time is determined by the slowest device If OOM occurs, the time will be set to
    positive infinity. During the dynamic programming, we will record pipeline planning
    configurations, including LLM segmentation points and device groupings.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M$ 是 LLM 参数、参数梯度和激活的内存使用总和。若没有内存溢出（OOM）异常，总数据并行执行时间由最慢的设备决定。如果发生 OOM，时间将被设定为正无穷。在动态规划过程中，我们将记录管道规划配置，包括
    LLM 分段点和设备分组。
- en: 'Upon the completion of dynamic programming process, we obtain a set of balanced
    partition configurations for various number of pipeline stages: $\{W_{s}|\text{
    config. of }W(0\rightarrow L,\mathcal{D},s),s\in\{1,2,...,|\mathcal{D}|\}\}$ and
    $e_{b}^{s}(i)$ and $c_{b}^{s}(i)$. As shown in Figure [6(b)](#S5.F6.sf2 "In Figure
    6 ‣ 5.1\. Resource-Efficient Collaborative Orchestration for LLMs Fine-Tuning
    ‣ 5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning"), we can divide per mini-batch training of $W_{s}$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完成动态规划过程后，我们获得了一组针对不同数量的管道阶段的平衡分区配置：$\{W_{s}|\text{ config. of }W(0\rightarrow
    L,\mathcal{D},s),s\in\{1,2,...,|\mathcal{D}|\}\}$ 和 $e_{b}^{s}(i)$ 以及 $c_{b}^{s}(i)$。如图
    [6(b)](#S5.F6.sf2 "图 6 ‣ 5.1. 资源高效的协作调度用于 LLMs 微调 ‣ 5. 协作边缘 AI 系统用于高效的个人 LLMs
    微调 ‣ Pluto 和 Charon：一个时间和内存高效的协作边缘 AI 框架用于个人 LLMs 微调") 所示，我们可以划分每个 $W_{s}$ 的迷你批次训练：
- en: '| (4) |  | $1$2 |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $1$2 |  |'
- en: '| (5) |  | $\displaystyle L_{n}^{s}=\max_{i\in\{1...,s\}}(\text{AR}^{s}(i)+\displaystyle\sum_{j=i}^{s-1}(e_{b}^{s}(j)+c_{b}^{s}(j)),$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle L_{n}^{s}=\max_{i\in\{1...,s\}}(\text{AR}^{s}(i)+\displaystyle\sum_{j=i}^{s-1}(e_{b}^{s}(j)+c_{b}^{s}(j)),$
    |  |'
- en: '| (6) |  | $\displaystyle\min_{s}{(L_{b}^{s}+L_{e}^{s}+L_{n}^{s})}.\vspace{-15pt}$
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\min_{s}{(L_{b}^{s}+L_{e}^{s}+L_{n}^{s})}.\vspace{-15pt}$
    |  |'
- en: Our algorithm aims to minimize this total latency by optimally determining the
    number of stages $s$. In our experiment, the whole planning time is within three
    seconds on an edge device.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的算法旨在通过优化确定阶段数 $s$ 来最小化这个总延迟。在我们的实验中，整个规划时间在边缘设备上不超过三秒。
- en: 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning of Parallel Adapters
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 缓存启用的协作边缘并行适配器微调
- en: 'Data-Parallel Fine-Tuning for Parallel Adapters The computationally lightweight
    nature of the Parallel Adapters precludes the use of pipeline parallelism to fine-tuning
    with activation cache, as it would result in unoverlapable inter-stage communication
    latency. Therefore, we employ data parallelism to exclusively fine-tune our Parallel
    Adapters. Specifically, after the first training epoch, the activation cache for
    all samples is already collected. We then perform collective communication to
    redistribute the Parallel Adapters parameters and locally cached activations across
    all devices, ensuring each device receives the complete set of adapter parameters
    and corresponding activations. The devices then utilize this shared information
    to fine-tune the parallel adapters in a data-parallel manner. In our experiments,
    fine-tuning the BART-Large model on the MRPC dataset for three epochs, the redistribution
    of parameters and activations only contributed to approximately $8\%$ of the total
    training time. Notably, the overhead of this process can be further amortized
    over additional training epochs. An instance of personal LLMs fine-tuning with
    activation cache is depicted in Figure [7](#S5.F7 "Figure 7 ‣ 5.2\. Cache-Enabled
    Collaborative Edge Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative Edge AI
    System for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and
    Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '数据并行微调用于并行适配器。并行适配器的计算轻量特性使得使用管道并行来进行激活缓存的微调成为不可能，因为这会导致不可重叠的阶段间通信延迟。因此，我们采用数据并行来专门微调我们的并行适配器。具体来说，在第一次训练周期后，所有样本的激活缓存已经收集完成。然后，我们进行集体通信，将并行适配器参数和本地缓存的激活分布到所有设备上，确保每个设备接收到完整的适配器参数集和对应的激活。设备然后利用这些共享信息以数据并行的方式微调并行适配器。在我们的实验中，对
    MRPC 数据集上的 BART-Large 模型进行三次训练周期的微调时，参数和激活的重新分配仅占总训练时间的约 $8\%$。值得注意的是，这一过程的开销可以在额外的训练周期中进一步摊销。个人
    LLMs 使用激活缓存的微调实例见图 [7](#S5.F7 "Figure 7 ‣ 5.2\. Cache-Enabled Collaborative Edge
    Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient
    Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning")。'
- en: '![Refer to caption](img/fa9539bf9ee3dbb2b34b67703179a5b2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fa9539bf9ee3dbb2b34b67703179a5b2.png)'
- en: Figure 7\. An instance of fine-tuning with activation cache.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 使用激活缓存的微调实例。
- en: Storage Cost Analysis. Employing activation caching can reduce the computational
    requirements of forward propagation; however, it incurs additional storage overhead
    for activations. Specifically, the storage overhead is $s\times h\times l$ corresponds
    to the number of transformer layers. For T5-Base model, the activation caching
    requires less than $1$ of the storage of a modern mobile device, e.g., hundreds
    of GB. During fine-tuning, the activation cache is reloaded from disk per micro-batch,
    a process that takes no more than tens of milliseconds on embedded flash storage.
    The cache will be cleared once the fine-tuning process finishes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 存储成本分析。采用激活缓存可以减少前向传播的计算需求；然而，它会产生额外的激活存储开销。具体来说，存储开销为 $s\times h\times l$，其中
    $l$ 对应于变换器层数。对于 T5-Base 模型，激活缓存需要的存储小于现代移动设备的 $1$，例如几百 GB。在微调过程中，激活缓存每个微批次从磁盘重新加载，这一过程在嵌入式闪存存储上不超过几毫秒。微调过程结束后，缓存将被清除。
- en: 'Table 2\. Training durations (in hours) for different methods: 3 epochs for
    MRPC and STS-B, and 1 epoch for SST-2 and QNLI.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 不同方法的训练时长（小时）：MRPC 和 STS-B 训练 3 个周期，SST-2 和 QNLI 训练 1 个周期。
- en: '| Fine-tuning Techniques | Baseline Methods | T5-Base |  | BART-Large |  |
    T5-Large |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 微调技术 | 基准方法 | T5-Base |  | BART-Large |  | T5-Large |'
- en: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
- en: '| Full Model | Standalone | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM
    |  | OOM | OOM | OOM | OOM |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | 独立运行 | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM |  | OOM |
    OOM | OOM | OOM |'
- en: '| Eco-FL | 0.45 | 0.71 | 2.74 | 4.32 |  | 2.41 | 3.78 | 14.56 | 22.98 |  |
    OOM | OOM | OOM | OOM |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Eco-FL | 0.45 | 0.71 | 2.74 | 4.32 |  | 2.41 | 3.78 | 14.56 | 22.98 |  |
    OOM | OOM | OOM | OOM |'
- en: '| EDDL | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM |  | OOM | OOM |
    OOM | OOM |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| EDDL | OOM | OOM | OOM | OOM |  | OOM | OOM | OOM | OOM |  | OOM | OOM |
    OOM | OOM |'
- en: '| Adapters | Standalone | 1.21 | 1.9 | 7.29 | 11.51 |  | OOM | OOM | OOM |
    OOM |  | OOM | OOM | OOM | OOM |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 适配器 | 独立运行 | 1.21 | 1.9 | 7.29 | 11.51 |  | OOM | OOM | OOM | OOM |  | OOM
    | OOM | OOM | OOM |'
- en: '| Eco-FL | 0.39 | 0.61 | 2.35 | 3.71 |  | 0.54 | 0.85 | 3.27 | 5.16 |  | 2.75
    | 4.31 | 16.59 | 26.19 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Eco-FL | 0.39 | 0.61 | 2.35 | 3.71 |  | 0.54 | 0.85 | 3.27 | 5.16 |  | 2.75
    | 4.31 | 16.59 | 26.19 |'
- en: '| EDDL | 0.34 | 0.53 | 2.06 | 3.25 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| EDDL | 0.34 | 0.53 | 2.06 | 3.25 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
- en: '| LoRA | Standalone | 1.21 | 1.89 | 7.28 | 11.49 |  | OOM | OOM | OOM | OOM
    |  | OOM | OOM | OOM | OOM |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 单独 | 1.21 | 1.89 | 7.28 | 11.49 |  | OOM | OOM | OOM | OOM |  | OOM
    | OOM | OOM | OOM |'
- en: '| Eco-FL | 0.41 | 0.64 | 2.45 | 3.87 |  | 0.55 | 0.87 | 3.33 | 5.26 |  | 2.73
    | 4.28 | 16.48 | 26.02 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Eco-FL | 0.41 | 0.64 | 2.45 | 3.87 |  | 0.55 | 0.87 | 3.33 | 5.26 |  | 2.73
    | 4.28 | 16.48 | 26.02 |'
- en: '| EDDL | 0.31 | 0.48 | 1.86 | 2.94 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| EDDL | 0.31 | 0.48 | 1.86 | 2.94 |  | OOM | OOM | OOM | OOM |  | OOM | OOM
    | OOM | OOM |'
- en: '| Parallel Adapters | PAC (Ours) | 0.14 | 0.22 | 1.34 | 2.12 |  | 0.29 | 0.45
    | 2.69 | 4.25 |  | 0.69 | 1.09 | 8.88 | 14.02 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Parallel Adapters | PAC（我们的） | 0.14 | 0.22 | 1.34 | 2.12 |  | 0.29 | 0.45
    | 2.69 | 4.25 |  | 0.69 | 1.09 | 8.88 | 14.02 |'
- en: Table 3\. Comparison of final performance between different fine-tuning techniques
    across four datasets. We report the average of F1 score and accuracy for MRPC.
    We use Pearson-Spearman Correlation as the metric for STS-B. For SST-2 and QNLI,
    we report accuracy. The mean value is the average performance of Full Model, Adapters
    and LoRA.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 不同微调技术在四个数据集上的最终性能比较。我们报告 MRPC 的 F1 分数和准确率的平均值。STS-B 使用 Pearson-Spearman
    相关性作为指标。对于 SST-2 和 QNLI，我们报告准确率。均值是完整模型、Adapters 和 LoRA 的平均性能。
- en: '| Fine-tuning Techniques | T5-Base |  | BART-Large |  | T5-Large |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 微调技术 | T5-Base |  | BART-Large |  | T5-Large |'
- en: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MRPC | STS-B | SST-2 | QNLI |  | MRPC | STS-B | SST-2 | QNLI |  | MRPC |
    STS-B | SST-2 | QNLI |'
- en: '| Full Model | 89.71 | 90.94 | 94.03 | 93.08 |  | 88.16 | 91.10 | 95.64 | 94.40
    |  | 92.78 | 91.08 | 95.30 | 93.30 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | 89.71 | 90.94 | 94.03 | 93.08 |  | 88.16 | 91.10 | 95.64 | 94.40 |  |
    92.78 | 91.08 | 95.30 | 93.30 |'
- en: '| Adapters | 88.73 | 90.51 | 93.58 | 93.04 |  | 86.63 | 90.24 | 94.93 | 93.27
    |  | 91.86 | 90.58 | 96.10 | 94.07 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Adapters | 88.73 | 90.51 | 93.58 | 93.04 |  | 86.63 | 90.24 | 94.93 | 93.27
    |  | 91.86 | 90.58 | 96.10 | 94.07 |'
- en: '| LoRA | 86.27 | 90.73 | 93.69 | 93.30 |  | 87.46 | 90.36 | 95.23 | 94.48 |  |
    90.27 | 92.08 | 95.53 | 94.18 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 86.27 | 90.73 | 93.69 | 93.30 |  | 87.46 | 90.36 | 95.23 | 94.48 |  |
    90.27 | 92.08 | 95.53 | 94.18 |'
- en: '| Mean Value | 88.24 | 90.73 | 93.77 | 93.14 |  | 87.42 | 90.57 | 95.27 | 94.05
    |  | 91.64 | 91.25 | 95.64 | 93.85 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 均值 | 88.24 | 90.73 | 93.77 | 93.14 |  | 87.42 | 90.57 | 95.27 | 94.05 |  |
    91.64 | 91.25 | 95.64 | 93.85 |'
- en: '| Parallel Adapters (Ours) | 88.24 | 90.43 | 93.46 | 93.25 |  | 87.71 | 90.54
    | 95.25 | 93.68 |  | 91.7 | 91.57 | 95.76 | 93.7 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Parallel Adapters（我们的） | 88.24 | 90.43 | 93.46 | 93.25 |  | 87.71 | 90.54
    | 95.25 | 93.68 |  | 91.7 | 91.57 | 95.76 | 93.7 |'
- en: '| Difference from Mean | +0.00 | -0.30 | -0.31 | +0.11 |  | +0.29 | -0.03 |
    -0.02 | -0.37 |  | +0.06 | +0.32 | +0.12 | -0.15 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 与均值的差异 | +0.00 | -0.30 | -0.31 | +0.11 |  | +0.29 | -0.03 | -0.02 | -0.37
    |  | +0.06 | +0.32 | +0.12 | -0.15 |'
- en: 6\. Evaluation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 评估
- en: 6.1\. Implementation and Setups
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实现与设置
- en: Implementation of PAC. We have fully implemented the prototype framework of
    PAC and baselines with $\sim$ times the corresponding weights and hidden states
    of the backbone model. In our experiments, the reduction factor $k$ is set to
    8\. The weights of the Parallel Adapters are initialized based on structural pruning,
    using the weights of the backbone model. We insert Parallel Adapters at the end
    of each transformer layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PAC 的实现。我们已完全实现了 PAC 的原型框架和基线模型，使用 $\sim$ 倍于对应权重和隐藏状态的骨干模型。在实验中，减少因子 $k$ 设置为
    8。Parallel Adapters 的权重基于结构剪枝进行初始化，使用骨干模型的权重。我们在每个 transformer 层的末尾插入 Parallel
    Adapters。
- en: 'Models and Datasets. We evaluate PAC with three typical transformer based LLM
    with parameters ranging from $0.25$B, as detailed in Table [4](#S6.T4 "Table 4
    ‣ 6.1\. Implementation and Setups ‣ 6\. Evaluation ‣ Pluto and Charon: A Time
    and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning"),
    which are widely considered for IPA and edge deployments (Li et al., [2024](#bib.bib15);
    Yuan et al., [2023](#bib.bib33)). All experiments were performed under conditions
    using Float32 precision to ensure fine-tuning performance. We employ two variants
    of the T5 model (Raffel et al., [2020](#bib.bib21)), specifically T5-Base and
    T5-Large with differing parameter sizes. We also compare PAC with baseline methods
    with BART-Large (Lewis et al., [2019](#bib.bib14)) as the backbone for our parallel
    adapters. We evaluate our fine-tuned LLMs with four tasks from GLUE benchmark.
    The four tasks evaluate models on multiple diverse tasks over sentiment analysis
    (SST2), similarity and paraphrase (MRPC, STS-B) and natural language inference
    (QNLI).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '模型与数据集。我们用三种典型的基于变换器的LLM评估PAC，参数范围从$0.25$B，详见表[4](#S6.T4 "Table 4 ‣ 6.1\. Implementation
    and Setups ‣ 6\. Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning")，这些模型被广泛应用于IPA和边缘部署（Li et al.,
    [2024](#bib.bib15); Yuan et al., [2023](#bib.bib33)）。所有实验在使用Float32精度的条件下进行，以确保微调性能。我们使用两种T5模型的变体（Raffel
    et al., [2020](#bib.bib21)），具体为T5-Base和T5-Large，它们具有不同的参数大小。我们还将PAC与基准方法进行比较，BART-Large（Lewis
    et al., [2019](#bib.bib14)）作为我们并行适配器的基础。我们使用GLUE基准中的四个任务评估我们微调后的LLMs。这四个任务评估模型在多种任务上的表现，包括情感分析（SST2）、相似性和释义（MRPC,
    STS-B）以及自然语言推理（QNLI）。'
- en: Edge Environment Setup. We evaluate PAC across a realistic edge platform consisting
    of multiple NVIDIA Jetson Nano (jet, [2019](#bib.bib2)), widely recognized as
    prevalent off-the-shelf edge devices. Each device is equipped with a $128$Mbps.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘环境设置。我们在由多个NVIDIA Jetson Nano（jet, [2019](#bib.bib2)）组成的实际边缘平台上评估PAC，这些设备被广泛认可为常见的现成边缘设备。每台设备配备了$128$Mbps。
- en: 'Baseline Methods. We compare PAC with both single-device method and the state-of-the-art
    collaborative edge training methods: (1) Standalone means fine-tuning LLMs on
    a single edge device. We compare with it to analyze the scalability performance
    of PAC. (2) Eco-FL (Ye et al., [2022](#bib.bib31)) is a collaborative edge system
    that facilitates pipeline parallelism training across an edge device cluster within
    the same local area network, segmenting LLMs into sequential stages for processing
    in a pipeline fashion. (3) EDDL (Hao and Zhang, [2021](#bib.bib9)) employs conventional
    data parallel training across edge devices, distributing batch data among cluster
    devices for simultaneous processing.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基准方法。我们将PAC与单设备方法和最先进的协作边缘训练方法进行比较：（1）独立方法是对单个边缘设备上的LLMs进行微调。我们将其与PAC进行比较，以分析PAC的可扩展性性能。（2）Eco-FL（Ye
    et al., [2022](#bib.bib31)）是一个协作边缘系统，支持在同一局域网内的边缘设备集群中进行流水线并行训练，将LLMs分解为顺序阶段进行处理。（3）EDDL（Hao
    and Zhang, [2021](#bib.bib9)）采用传统的数据并行训练，将批量数据分配到集群设备中以进行同时处理。
- en: Table 4\. LLM model specifications used for experiments. "en-de" indicates encoder-decoder
    LLM structure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 实验中使用的LLM模型规格。“en-de”表示编码器-解码器LLM结构。
- en: '| Model | Structure | Layers | Heads |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 结构 | 层数 | 头数 |'
- en: '&#124; Hidden &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 隐藏层 &#124;'
- en: '&#124; Size &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大小 &#124;'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Param. &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 参数 &#124;'
- en: '&#124; Count &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| T5-Base (Raffel et al., [2020](#bib.bib21)) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| T5-Base（Raffel et al., [2020](#bib.bib21)） |'
- en: '&#124; en-de &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; en-de &#124;'
- en: '| 12 | 12 | 768 | 0.25B |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 12 | 768 | 0.25B |'
- en: '| BART-Large (Lewis et al., [2019](#bib.bib14)) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| BART-Large（Lewis et al., [2019](#bib.bib14)） |'
- en: '&#124; en-de &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; en-de &#124;'
- en: '| 12 | 16 | 1024 | 0.41B |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 16 | 1024 | 0.41B |'
- en: '| T5-Large (Raffel et al., [2020](#bib.bib21)) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| T5-Large（Raffel et al., [2020](#bib.bib21)） |'
- en: '&#124; en-de &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; en-de &#124;'
- en: '| 24 | 16 | 1024 | 0.74B |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 16 | 1024 | 0.74B |'
- en: Considering that the aforementioned baseline systems were not specifically designed
    for the fine-tuning of LLMs, we ensure a fair comparison by equipping these edge
    systems with various LLM fine-tuning techniques. These include full model fine-tuning
    and popular PEFT techniques. (1) In Full model fine-tuning, all the LLM parameters
    are updated for a downstream task. (2) LoRA (Hu et al., [2021](#bib.bib11)) is
    a widely-used PEFT technique that decomposes the parameter update for a weight
    matrix into two trainable low-rank matrices. (3) Adapters (Houlsby et al., [2019](#bib.bib10))
    is another widely-used PEFT technique that injects small trainable modules at
    the end of each transformer layer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到前述基线系统并未专门针对 LLM 的微调进行设计，我们通过为这些边缘系统配备各种 LLM 微调技术来确保公平比较。这些技术包括完整模型微调和流行的
    PEFT 技术。 (1) 在完整模型微调中，所有 LLM 参数都针对下游任务进行更新。 (2) LoRA (Hu et al., [2021](#bib.bib11))
    是一种广泛使用的 PEFT 技术，它将权重矩阵的参数更新分解为两个可训练的低秩矩阵。 (3) Adapters (Houlsby et al., [2019](#bib.bib10))
    是另一种广泛使用的 PEFT 技术，它在每个 transformer 层的末尾注入小型可训练模块。
- en: 6.2\. End-to-end Performance
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 端到端性能
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning
    of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient Personal
    LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") and Table [3](#S5.T3 "Table
    3 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning of Parallel Adapters ‣
    5\. Collaborative Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto
    and Charon: A Time and Memory Efficient Collaborative Edge AI Framework for Personal
    LLMs Fine-Tuning") summarize the end-to-end performance comparisons between PAC,
    the single-device method, and state-of-the-art collaborative edge training methods.
    To ensure fair comparisons, these baseline methods are enhanced with prevalent
    PEFT techniques, including Adapters and LoRA. Fine-tuning the smaller datasets,
    MRPC and STS-B, is conducted over three epochs, with the latter two epochs benefiting
    from the PAC activation cache. Conversely, for larger datasets such as STS-2 and
    QNLI, a single epoch of fine-tuning is sufficient to achieve satisfactory performance.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S5.T2 "Table 2 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning
    of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient Personal
    LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") 和表 [3](#S5.T3 "Table 3 ‣ 5.2\.
    Cache-Enabled Collaborative Edge Fine-Tuning of Parallel Adapters ‣ 5\. Collaborative
    Edge AI System for Efficient Personal LLMs Fine-Tuning ‣ Pluto and Charon: A Time
    and Memory Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning")
    总结了 PAC、单设备方法与最先进的协作边缘训练方法之间的端到端性能比较。为确保公平比较，这些基线方法使用了流行的 PEFT 技术，包括 Adapters
    和 LoRA。对较小数据集 MRPC 和 STS-B 的微调进行了三个 epoch，其中后两个 epoch 受益于 PAC 激活缓存。相反，对于较大的数据集如
    STS-2 和 QNLI，单个 epoch 的微调就足以达到令人满意的性能。'
- en: '![Refer to caption](img/24aeae266747101e9878751076864f11.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/24aeae266747101e9878751076864f11.png)'
- en: (a) The comparison of average sample training time of different fine-tuning
    techniques.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同微调技术的平均样本训练时间比较。
- en: '![Refer to caption](img/c384cb3683587f5efbeeae7aa4a4c236.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c384cb3683587f5efbeeae7aa4a4c236.png)'
- en: (b) Maximum total memory consumption per device across the edge cluster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 边缘集群中每个设备的最大总内存消耗。
- en: 'Figure 8\. Comparison of different fine-tuning techniques. P.A. indicates our
    Parallel Adapters technique. Mini-batch size 16; sequence length: 128.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 不同微调技术的比较。P.A. 表示我们的并行适配器技术。迷你批量大小 16；序列长度：128。
- en: 'PAC significantly speeds up the training process while preserving convergence
    performance. PAC achieves an acceleration ranging from $1.21\times$ on SST-2 and
    QNLI, without utilizing activation cache. Parallel Adapters not only alleviate
    the memory footprint of LLM parameters but also intermediate activations. Eco-FL’s
    pipeline parallel strategies allow each edge device to host only a portion of
    the model parameters. However, these devices still bear a substantial memory footprint
    from intermediate activations, even when employing PEFT technologies such as LoRA
    and Adapters. Therefore, the Eco-FL approach necessitates the use of smaller micro-batch
    sizes or a reduction in the number of micro-batches simultaneously input into
    the pipeline. This results in decreased concurrency in pipeline parallelism and
    lowers the training throughput. Moreover, our hybrid parallelism merges the benefits
    of both data and pipeline parallelism, providing an expanded search space for
    parallel architectures to accommodate complex edge environments. Our method enables
    the identification of the most efficient parallel configuration with maximum throughput
    within the constraints of available resources. With the integration of our activation
    cache mechanism, PAC achieves speedups of up to $8.64\times$ on the MRPC and STS-B
    datasets. As discussed in §[5](#S4.F5 "Figure 5 ‣ 4.1\. Fine-Tuning LLMs with
    Parallel Adapters ‣ 4\. Time, Memory and Parameter Efficient Fine-Tuning Algorithm
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"), our Parallel Adapters constitute a lightweight,
    independent network. We can skip both the forward and backward passes through
    the LLM backbone, since the required activations have already been calculated
    and stored. Consequently, training overhead can be markedly reduced in the second
    and third fine-tuning epochs.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PAC显著加快了训练过程，同时保持了收敛性能。PAC在不使用激活缓存的情况下，在SST-2和QNLI上实现了$1.21\times$的加速。并行适配器不仅减轻了LLM参数的内存负担，还减轻了中间激活的负担。Eco-FL的管道并行策略允许每个边缘设备仅托管部分模型参数。然而，即使使用LoRA和Adapters等PEFT技术，这些设备仍然承受着来自中间激活的显著内存负担。因此，Eco-FL方法要求使用较小的微批量大小或减少同时输入管道的微批量数量。这导致了管道并行性的并发性降低，从而降低了训练吞吐量。此外，我们的混合并行性结合了数据并行性和管道并行性的优点，为复杂的边缘环境提供了更广阔的并行架构搜索空间。我们的方法能够在现有资源的限制下识别出最有效的并行配置，以实现最大吞吐量。通过集成我们的激活缓存机制，PAC在MRPC和STS-B数据集上实现了高达$8.64\times$的加速。如§[5](#S4.F5
    "图 5 ‣ 4.1\. 使用并行适配器微调LLMs ‣ 4\. 时间、内存和参数高效微调算法 ‣ Pluto和Charon：用于个人LLM微调的时间和内存高效协作边缘AI框架")所述，我们的并行适配器构成了一个轻量级的独立网络。我们可以跳过LLM骨干网络的前向和反向传递，因为所需的激活已被计算并存储。因此，第二和第三次微调周期中的训练开销可以显著减少。
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2\. Cache-Enabled Collaborative Edge Fine-Tuning
    of Parallel Adapters ‣ 5\. Collaborative Edge AI System for Efficient Personal
    LLMs Fine-Tuning ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") displays the performance of
    various full model and PEFT fine-tuning methods on four datasets after training.
    Fine-tuning involves 3 epochs for the smaller MRPC and STS-B datasets, and 1 epoch
    for the larger SST-2 and QNLI datasets. We can observe that PAC achieves comparable
    or superior performance to full model fine-tuning and PEFT techniques across various
    models and datasets. The largest discrepancy in mean performance metrics between
    PAC and these methods is only -0.37, a negligible difference. Notably, PAC frequently
    outperforms these methods and achieves the highest performance on the SST-2 dataset
    with the T5-Large model.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#S5.T3 "表 3 ‣ 5.2\. 支持缓存的协作边缘并行适配器微调 ‣ 5\. 高效个人LLM微调的协作边缘AI系统 ‣ Pluto和Charon：用于个人LLM微调的时间和内存高效协作边缘AI框架")展示了训练后在四个数据集上各种全模型和PEFT微调方法的性能。微调涉及对较小的MRPC和STS-B数据集进行3轮训练，对较大的SST-2和QNLI数据集进行1轮训练。我们可以观察到，PAC在各种模型和数据集上表现出与全模型微调和PEFT技术相当或更优的性能。PAC与这些方法之间的平均性能指标最大差异仅为-0.37，这是一个微不足道的差异。值得注意的是，PAC经常超越这些方法，并在SST-2数据集上使用T5-Large模型时达到最高性能。
- en: '![Refer to caption](img/013b9fbc2160b5581df29a2b839d8b46.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/013b9fbc2160b5581df29a2b839d8b46.png)'
- en: (a) Throughput results with a varying number of Jetson Nano.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在不同数量的Jetson Nano下的吞吐量结果。
- en: '![Refer to caption](img/c349dc5477596a45ffe48f2e0699d8d0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c349dc5477596a45ffe48f2e0699d8d0.png)'
- en: (b) Peak memory consumption of LLM weights per device across the edge cluster.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 边缘集群中每台设备的 LLM 权重的峰值内存消耗。
- en: Figure 9\. Comparison of different fine-tuning techniques.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 不同微调技术的比较。
- en: '![Refer to caption](img/b70a05fa19233a2036f0e0e1acbb0478.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b70a05fa19233a2036f0e0e1acbb0478.png)'
- en: 'Figure 10\. Device grouping results of PAC’s hybrid parallelism for experiments
    in Figure [9](#S6.F9 "Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation
    ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI Framework
    for Personal LLMs Fine-Tuning"). "N" indicates Jetson Nano.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10\. PAC 的混合并行性在图 [9](#S6.F9 "图 9 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto 和 Charon:
    一个时间和内存高效的协作边缘 AI 框架用于个人 LLM 微调") 中的设备分组结果。“N”表示 Jetson Nano。'
- en: 6.3\. Significance of Parallel Adapters at the Edge
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 边缘的并行适配器的重要性
- en: 'We conducted experiments to assess the time and memory efficiency of Parallel
    Adapters at the edge. In this section, we perform data parallelism for Parallel
    Adapters with activation cache across 8 devices and hybrid parallelism for other
    fine-tuning techniques without 1F1B micro-batch scheduling. "Activations" contain
    the intermediate results and optimizer states. Figure [8](#S6.F8 "Figure 8 ‣ 6.2\.
    End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon: A Time and Memory
    Efficient Collaborative Edge AI Framework for Personal LLMs Fine-Tuning") illustrates
    that Parallel Adapters outperform other fine-tuning techniques regarding both
    time and memory efficiency.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了实验，以评估边缘上并行适配器的时间和内存效率。在本节中，我们对具有激活缓存的并行适配器在 8 台设备上进行数据并行，对其他微调技术则进行混合并行而不使用
    1F1B 微批量调度。“激活”包含中间结果和优化器状态。图 [8](#S6.F8 "图 8 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto 和
    Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLM 微调") 表明，与其他微调技术相比，并行适配器在时间和内存效率方面表现更优。'
- en: 'Parallel Adapters markedly reduce per-sample training time. Figure [8(a)](#S6.F8.sf1
    "In Figure 8 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning") presents the average sample training time across different fine-tuning
    techniques. Without activation cache, Parallel Adapters can reduce the average
    sample training time by $31.94\%$ compared to full fine-tuning. Moreover, Parallel
    Adapters With activation cache mechanism can further decrease the average sample
    training time up to $96.39\%$. These results demonstrate the substantial reduction
    in training time achieved by Parallel Adapters.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '并行适配器显著减少了每个样本的训练时间。图 [8(a)](#S6.F8.sf1 "在图 8 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto
    和 Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLM 微调") 展示了不同微调技术下的平均样本训练时间。没有激活缓存的情况下，并行适配器可以比完全微调减少
    $31.94\%$ 的平均样本训练时间。此外，具有激活缓存机制的并行适配器可以进一步将平均样本训练时间减少到 $96.39\%$。这些结果表明并行适配器在训练时间上的显著减少。'
- en: 'Parallel Adapters yield a substantial reduction in memory usage. Figure [8(b)](#S6.F8.sf2
    "In Figure 8 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning") depicts the breakdown of the memory footprint for different fine-tuning
    techniques. We report the peak memory consumption per device across edge clusters.
    Without activation cache, Parallel Adapters can reduce memory usage by $25.27\%$.
    With activation cache, Parallel Adapters can decrease the peak memory footprint
    from $74.57\%$ compared to baselines. This is because it’s sufficient to store
    only the lightweight Parallel Adapters, eliminating the need to host the entire
    LLM backbone in memory.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '并行适配器在内存使用上实现了显著减少。图 [8(b)](#S6.F8.sf2 "在图 8 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto
    和 Charon: 一个时间和内存高效的协作边缘 AI 框架用于个人 LLM 微调") 描绘了不同微调技术的内存占用情况。我们报告了边缘集群中每台设备的峰值内存消耗。没有激活缓存的情况下，并行适配器可以减少
    $25.27\%$ 的内存使用。使用激活缓存的情况下，并行适配器可以将峰值内存占用从 $74.57\%$ 减少到基准值。这是因为仅存储轻量级的并行适配器已经足够，避免了将整个
    LLM 主干存储在内存中。'
- en: 6.4\. Analysis of Collaborative Edge Fine-Tuning
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 协作边缘微调的分析
- en: We perform an ablation study to understand the contribution of hybrid parallelism
    and activation cache in our system design.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项消融研究，以理解混合并行性和激活缓存对我们系统设计的贡献。
- en: Comparasion PAC with EDDL and Eco-FL. To explore the scalability advantages
    of PAC’s hybrid parallelism over Eco-FL’s pipeline parallelism and EDDL’s data
    parallelism, we compared the throughput of these methods when training collaboratively
    across 2 to 8 edge devices. The batch size was consistent with the number of devices,
    and the sequence length of each sample was fixed at 128\. We implement Eco-FL
    and EEDL using the Parallel Adapters technique to ensure a fair comparison. Note
    that none of the three methods utilizes activation cache.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 比较PAC与EDDL和Eco-FL。为了探索PAC的混合并行性在扩展性上的优势，相比于Eco-FL的流水线并行性和EDDL的数据并行性，我们比较了这几种方法在2到8个边缘设备上进行协同训练时的吞吐量。批量大小与设备数量一致，每个样本的序列长度固定为128\。我们使用并行适配器技术实现了Eco-FL和EEDL，以确保公平比较。请注意，这三种方法都不使用激活缓存。
- en: 'Figure [9(b)](#S6.F9.sf2 "In Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\.
    Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge
    AI Framework for Personal LLMs Fine-Tuning") illustrates the maximum per device
    memory footprint of model weights across edge cluster. For EDDL, each device must
    host a complete LLM, preventing the reduction of the parameters’ memory footprint
    through scaling up the number of devices. Therefore, as shown in Figure [9(a)](#S6.F9.sf1
    "In Figure 9 ‣ 6.2\. End-to-end Performance ‣ 6\. Evaluation ‣ Pluto and Charon:
    A Time and Memory Efficient Collaborative Edge AI Framework for Personal LLMs
    Fine-Tuning"), the EDDL method exhibits OOM errors with both the BART-Large and
    T5-Large models. Conversely, PAC and Eco-FL utilize pipeline parallelism, partitioning
    the model into multiple stages with each handled by different devices. This approach
    allows for scaling the number of devices to reduce the peak memory footprint.
    PAC’s hybrid parallelism offers a broader search space for parallel strategies
    compared to Eco-FL’s pipeline parallelism. Our planning algorithm for PAC is capable
    of identifying more efficient hybrid parallel configurations within memory constraints,
    enhancing resource utilization. Although PAC may incur higher memory overhead
    in some instances, it achieves greater system throughput. Specifically, when compared
    to Eco-FL, PAC exhibits an increase in throughput from $39.50\%$.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9(b)](#S6.F9.sf2 "在图9 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto和Charon：用于个人LLMs微调的高效协作边缘AI框架")展示了模型权重在边缘集群中每个设备的最大内存占用。对于EDDL，每个设备必须托管一个完整的LLM，这阻止了通过增加设备数量来减少参数的内存占用。因此，如图[9(a)](#S6.F9.sf1
    "在图9 ‣ 6.2\. 端到端性能 ‣ 6\. 评估 ‣ Pluto和Charon：用于个人LLMs微调的高效协作边缘AI框架")所示，EDDL方法在BART-Large和T5-Large模型中表现出OOM错误。相反，PAC和Eco-FL利用流水线并行性，将模型划分为多个阶段，每个阶段由不同的设备处理。这种方法允许通过增加设备数量来减少峰值内存占用。与Eco-FL的流水线并行性相比，PAC的混合并行性提供了更广泛的并行策略搜索空间。我们的PAC规划算法能够在内存限制内识别出更高效的混合并行配置，从而提高资源利用率。尽管在某些情况下PAC可能会导致更高的内存开销，但它能够实现更高的系统吞吐量。具体来说，与Eco-FL相比，PAC的吞吐量提高了$39.50\%$。
- en: '![Refer to caption](img/f41edd815dee66ff3d3d1183ece01571.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f41edd815dee66ff3d3d1183ece01571.png)'
- en: 'Figure 11\. Fine-tuning time with PAC. Time without activation cache is represented
    by bars. The corresponding reduction in time achieved utilizing activation cache
    is represented by shaded areas. Dataset: MRPC.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 使用PAC的微调时间。没有激活缓存的时间用条形图表示。利用激活缓存获得的时间减少由阴影区域表示。数据集：MRPC。
- en: 'To more clearly illustrate the parallel strategies adopted by PAC, we present
    the device grouping configurations for PAC across various LLMs and numbers of
    devices in Figure [10](#S6.F10 "Figure 10 ‣ 6.2\. End-to-end Performance ‣ 6\.
    Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative Edge
    AI Framework for Personal LLMs Fine-Tuning"). On the left, a table displays all
    the grouping results across three models. On the right, an instance is shown where
    a model is divided into three stages, with two devices handling the first stage
    to perform data parallelism. Specifically, when fine-tuning BART-Large with eight
    devices, EDDL encounters OOM issues because a single Jetson Nano cannot accommodate
    a complete BART-Large. Eco-FL addresses this problem by dividing the model into
    eight stages and employing straight pipeline parallelism for training. On the
    contrary, our PAC approach divides BART-Large into two stage models, with each
    stage replicated across four devices. This configuration significantly reduces
    the number of stages in the pipeline, thereby minimizing inter-stage data dependencies
    and communication latency, which in turn enhances the pipeline’s concurrent efficiency.
    These results demonstrate that our hybrid parallel approach offers a larger search
    space for parallel configurations, providing enhanced scalability and robustness
    across varying numbers of devices and workloads.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更清晰地说明 PAC 采用的并行策略，我们在图 [10](#S6.F10 "Figure 10 ‣ 6.2\. End-to-end Performance
    ‣ 6\. Evaluation ‣ Pluto and Charon: A Time and Memory Efficient Collaborative
    Edge AI Framework for Personal LLMs Fine-Tuning") 中展示了 PAC 在不同 LLM 和设备数量下的设备分组配置。在左侧，表格展示了三个模型的所有分组结果。在右侧，展示了一个实例，其中一个模型被分为三个阶段，两个设备处理第一个阶段以实现数据并行。具体来说，在使用八个设备对
    BART-Large 进行微调时，由于单个 Jetson Nano 无法容纳完整的 BART-Large，EDDL 遇到了 OOM 问题。Eco-FL 通过将模型划分为八个阶段，并使用直通管道并行进行训练，解决了这个问题。相反，我们的
    PAC 方法将 BART-Large 划分为两个阶段模型，每个阶段在四个设备上复制。这种配置显著减少了管道中的阶段数量，从而减少了阶段之间的数据依赖性和通信延迟，从而提高了管道的并发效率。这些结果表明，我们的混合并行方法为并行配置提供了更大的搜索空间，提供了更好的可扩展性和在不同设备和工作负载下的鲁棒性。'
- en: Comparison of PAC with and without activation cache. We further investigated
    how our activation cache mechanism benefits the required fine-tuning latency.
    By leveraging activation cache, the fine-tuning latency per epoch can decrease
    up to $79.51\%$, whereas training for ten epochs increases the reduction to $71\%$.
    This reduction can be attributed to the fact that the Parallel Adapters constitute
    a lightweight, independent network, resulting in a significant decrease in training
    cost compared to the LLM backbone. We can bypass both the forward and backward
    passes through the LLM backbone since the necessary activations are already cached.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: PAC 与不使用激活缓存的比较。我们进一步研究了激活缓存机制如何有利于所需的微调延迟。通过利用激活缓存，每个 epoch 的微调延迟可以减少高达 $79.51\%$，而训练十个
    epoch 则将减少幅度增加到 $71\%$。这一减少可以归因于并行适配器构成了一个轻量级、独立的网络，与 LLM 主干相比，训练成本显著降低。由于所需的激活已经被缓存，我们可以绕过
    LLM 主干的前向和反向传播。
- en: 7\. Related Work
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 相关工作
- en: Parameter-Efficient Fine-Tuning for LLM. Prompt tuning (Lester et al., [2021](#bib.bib13))
    proposes to prepend the model input embeddings with a trainable tensor. Adapters
    tuning (Houlsby et al., [2019](#bib.bib10)) adds domain-specific layers after
    attention and FFN layers in transformer. LoRA (Hu et al., [2021](#bib.bib11))
    decomposes the parameter update for a weight matrix into two trainable low-rank
    matrices. To further reduce the memory overhead, pioneering studies explore fine-tuning
    techniques that obviate the need for backpropagation through the backbone model.
    Y-tuning (Liu et al., [2024](#bib.bib17)) learns additional task-specific label
    representations, which are integrated with the output of the backbone model to
    circumvent backpropagation. LST (Sung et al., [2022](#bib.bib22)) involves the
    use of pruned lightweight transformer structures from the backbone as a side network.
    E³VA (Yin et al., [2023](#bib.bib32)) extends the concept of the side network
    into the realm of computer vision.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 LLM 的参数高效微调。Prompt tuning (Lester et al., [2021](#bib.bib13)) 提出了在模型输入嵌入中预先添加一个可训练的张量。Adapters
    tuning (Houlsby et al., [2019](#bib.bib10)) 在变换器的注意力层和 FFN 层之后添加了特定领域的层。LoRA (Hu
    et al., [2021](#bib.bib11)) 将权重矩阵的参数更新分解为两个可训练的低秩矩阵。为了进一步减少内存开销，开创性的研究探索了不需要通过主干模型进行反向传播的微调技术。Y-tuning
    (Liu et al., [2024](#bib.bib17)) 学习额外的任务特定标签表示，并将其与主干模型的输出集成，以绕过反向传播。LST (Sung
    et al., [2022](#bib.bib22)) 使用从主干模型中修剪的轻量变换器结构作为侧网络。E³VA (Yin et al., [2023](#bib.bib32))
    将侧网络的概念扩展到计算机视觉领域。
- en: On-device DNN Fine-Tuning. POET (Patil et al., [2022](#bib.bib20)) achieves
    the finetuning of a BERT model on embedded devices, optimizing for both training
    speed and energy consumption. Lin et al. (Lin et al., [2022](#bib.bib16)) enable
    training directly on devices with a minimal memory requirement of only 256KB.
    Sage and Melon (Gim and Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24))
    implement hybrid memory management and conservation strategies, including operator
    fusion and the use of a dedicated memory pool, to mitigate memory limitations.
    Additionally, Mandheling (Xu et al., [2022](#bib.bib26)) incorporates mixed-precision
    training along with DSP offloading to enhance the speed of learning.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上的 DNN 微调。POET (Patil et al., [2022](#bib.bib20)) 实现在嵌入式设备上对 BERT 模型进行微调，优化了训练速度和能耗。Lin
    et al. (Lin et al., [2022](#bib.bib16)) 实现了在设备上直接训练，内存需求仅为 256KB。Sage 和 Melon
    (Gim 和 Ko, [2022](#bib.bib6); Wang et al., [2022](#bib.bib24)) 实现了混合内存管理和节约策略，包括操作符融合和使用专用内存池，以缓解内存限制。此外，Mandheling
    (Xu et al., [2022](#bib.bib26)) 结合了混合精度训练和 DSP 卸载，以提高学习速度。
- en: Collaborative Edge Computing for DNN Fine-Tuning. Federated Learning (FL) has
    been a promising paradigm in distributed machine learning that enables in-situ
    model fine-tuning. FwdLLM (Xu et al., [2024](#bib.bib28)) designs a backpropagation-free
    fine-tuning FL protocol to enhance efficiency. AdaFL (Cai et al., [2023](#bib.bib5))
    proposes an FL framework for fine-tuning LLMs that features adaptable depth and
    width in its adapters modules. Breaking through the conventional paradigm of FL,
    Ye et al. (Ye et al., [2022](#bib.bib31); Zeng et al., [2024](#bib.bib34)) devise
    a pipeline parallel architecture that facilitates the collaborative fine-tuning
    of DNNs across multiple edge devices. EDDL (Hao and Zhang, [2021](#bib.bib9))
    adopts data parallelism training across embedded devices in a local area network.
    Asteroid (Ye et al., [2024b](#bib.bib30)) also employs HPP across multiple edge
    devices for DNN training, but it does not specifically address the parameter-efficient
    fine-tuning of LLMs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 DNN 微调的协作边缘计算。联邦学习 (FL) 已成为一种有前景的分布式机器学习范式，能够在现场进行模型微调。FwdLLM (Xu et al.,
    [2024](#bib.bib28)) 设计了一种无反向传播的微调 FL 协议，以提高效率。AdaFL (Cai et al., [2023](#bib.bib5))
    提出了一个适用于微调 LLM 的 FL 框架，其适配器模块具有可调节的深度和宽度。打破了传统 FL 范式的局限，Ye et al. (Ye et al.,
    [2022](#bib.bib31); Zeng et al., [2024](#bib.bib34)) 设计了一种管道并行架构，促进了跨多个边缘设备的 DNN
    协作微调。EDDL (Hao 和 Zhang, [2021](#bib.bib9)) 在局域网内的嵌入式设备之间采用数据并行训练。Asteroid (Ye
    et al., [2024b](#bib.bib30)) 也在多个边缘设备上使用 HPP 进行 DNN 训练，但并未特别解决 LLM 的参数高效微调问题。
- en: 8\. Conclusion
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: This paper proposes PAC, a time and memory efficient collaborative edge AI framework
    for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning
    with a sophisticated algorithm-system co-design, achieving a acceleration of $8.64\times$
    memory reduction compared to state-of-the-art methods.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了PAC，一个**时间和内存高效的个人LLM微调协作边缘AI框架**。PAC通过精密的算法系统联合设计突破了个人LLM微调的资源壁垒，实现了相比于最先进方法$8.64\times$的加速和内存减少。
- en: References
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: jet (2019) 2019. Jetson-Nano. [https://developer.nvidia.com/embedded/jetson-nano-developer-kit](https://developer.nvidia.com/embedded/jetson-nano-developer-kit).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: jet (2019) 2019. Jetson-Nano. [https://developer.nvidia.com/embedded/jetson-nano-developer-kit](https://developer.nvidia.com/embedded/jetson-nano-developer-kit)。
- en: pyt (2019) 2019. PyTorch. [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pyt (2019) 2019. PyTorch. [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)。
- en: tfl (2021) 2021. On-device training with tensorflow lite. [https://www.tensorflow.org/lite/examples/on_device_training/overview](https://www.tensorflow.org/lite/examples/on_device_training/overview).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tfl (2021) 2021. 使用TensorFlow Lite进行设备端训练。 [https://www.tensorflow.org/lite/examples/on_device_training/overview](https://www.tensorflow.org/lite/examples/on_device_training/overview)。
- en: Cai et al. (2023) Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix Xiaozhu Lin,
    and Mengwei Xu. 2023. Efficient federated learning for modern nlp. In *MobiCom*.
    1–16.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2023) Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix Xiaozhu Lin,
    and Mengwei Xu. 2023. **现代NLP的高效联邦学习**。在*MobiCom*。1–16。
- en: Gim and Ko (2022) In Gim and JeongGil Ko. 2022. Memory-efficient dnn training
    on mobile devices. In *MobiSys*. 464–476.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gim and Ko (2022) 在Gim和JeongGil Ko. 2022. **移动设备上的内存高效DNN训练**。在*MobiSys*。464–476。
- en: 'Guo et al. (2023) Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. 2023. Sti:
    Turbocharge nlp inference at the edge via elastic pipelining. In *ASPLOS, Volume
    2*. 791–803.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2023) Liwei Guo, Wonkyo Choe, and Felix Xiaozhu Lin. 2023. **Sti：通过弹性流水线加速边缘NLP推理**。在*ASPLOS,
    Volume 2*。791–803。
- en: 'Han et al. (2024) Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024.
    Parameter-efficient fine-tuning for large models: A comprehensive survey. *arXiv
    preprint arXiv:2403.14608* (2024).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2024) Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024.
    **大模型的参数高效微调：一项全面调查**。*arXiv预印本 arXiv:2403.14608* (2024)。
- en: 'Hao and Zhang (2021) Pengzhan Hao and Yifan Zhang. 2021. Eddl: A distributed
    deep learning system for resource-limited edge computing environment. In *2021
    IEEE/ACM Symposium on Edge Computing (SEC)*. IEEE, 1–13.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao and Zhang (2021) Pengzhan Hao and Yifan Zhang. 2021. **EDDL：一个资源受限边缘计算环境的分布式深度学习系统**。在*2021
    IEEE/ACM边缘计算研讨会 (SEC)*。IEEE, 1–13。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. Parameter-efficient transfer learning for NLP. In *ICML*. PMLR, 2790–2799.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. **参数高效的转移学习**用于NLP。在*ICML*。PMLR, 2790–2799。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. **Lora：大语言模型的低秩适配**。*arXiv预印本
    arXiv:2106.09685* (2021)。
- en: 'Jiang et al. (2020) Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan
    Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, et al. 2020. Mnn:
    A universal and efficient inference engine. *Proceedings of MLSys* 2 (2020), 1–13.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2020) Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan
    Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, et al. 2020. **Mnn：一个通用高效的推理引擎**。*MLSys会议录*
    2 (2020), 1–13。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*
    (2021).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. **参数高效的提示调整**的规模效应。*arXiv预印本
    arXiv:2104.08691* (2021)。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461* (2019).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. **Bart：序列到序列的去噪预训练**用于自然语言生成、翻译和理解。*arXiv预印本
    arXiv:1910.13461* (2019)。
- en: 'Li et al. (2024) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024. Personal
    llm agents: Insights and survey about the capability, efficiency and security.
    *arXiv preprint arXiv:2401.05459* (2024).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, 等. 2024. 个人化LLM代理:
    关于能力、效率和安全性的洞见与调查。*arXiv preprint arXiv:2401.05459* (2024).'
- en: Lin et al. (2022) Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan,
    and Song Han. 2022. On-device training under 256kb memory. *NeurIPS* 35 (2022).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2022) Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan,
    和 Song Han. 2022. 在256kb内存下的设备端训练。*NeurIPS* 35 (2022).
- en: 'Liu et al. (2024) Yitao Liu, Chenxin An, and Xipeng Qiu. 2024. Y-tuning: An
    efficient tuning paradigm for large-scale pre-trained models via label representation
    learning. *Frontiers of Computer Science* 18, 4 (2024), 184320.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2024) Yitao Liu, Chenxin An, 和 Xipeng Qiu. 2024. Y-tuning: 通过标签表示学习为大规模预训练模型提供高效的调优范式。*Frontiers
    of Computer Science* 18, 4 (2024), 184320.'
- en: 'Miao et al. (2024) Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin
    Unger, and Zhihao Jia. 2024. FlexLLM: A System for Co-Serving Large Language Model
    Inference and Parameter-Efficient Finetuning. *arXiv:2402.18789* (2024).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao et al. (2024) Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin
    Unger, 和 Zhihao Jia. 2024. FlexLLM: 用于共同服务大语言模型推理和参数高效微调的系统。*arXiv:2402.18789*
    (2024).'
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    2019. PipeDream: generalized pipeline parallelism for DNN training. In *SOSP*.
    1–15.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, 和 Matei Zaharia.
    2019. PipeDream: DNN训练的通用流水线并行性。In *SOSP*. 1–15.'
- en: 'Patil et al. (2022) Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica,
    and Joseph Gonzalez. 2022. POET: Training neural networks on tiny devices with
    integrated rematerialization and paging. In *International Conference on Machine
    Learning*. PMLR, 17573–17583.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil et al. (2022) Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica,
    和 Joseph Gonzalez. 2022. POET: 在集成再物化和分页的小型设备上训练神经网络。In *International Conference
    on Machine Learning*. PMLR, 17573–17583.'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *JMLR*
    21, 140 (2020).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. 探索统一文本到文本变换器的迁移学习极限。*JMLR*
    21, 140 (2020).
- en: 'Sung et al. (2022) Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder
    side-tuning for parameter and memory efficient transfer learning. *NeurIPS* 35
    (2022).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sung et al. (2022) Yi-Lin Sung, Jaemin Cho, 和 Mohit Bansal. 2022. Lst: 梯度侧调整的参数和内存高效转移学习。*NeurIPS*
    35 (2022).'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *NeurIPS* 30 (2017).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制是你所需要的一切。*NeurIPS*
    30 (2017).
- en: 'Wang et al. (2022) Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang
    Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. 2022. Melon: Breaking
    the memory wall for resource-efficient on-device machine learning. In *MobiSys*.
    450–463.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang
    Yuan, Xin Jin, Gang Huang, Yunxin Liu, 和 Xuanzhe Liu. 2022. Melon: 打破内存墙，实现资源高效的设备端机器学习。In
    *MobiSys*. 450–463.'
- en: Wei et al. (2024) Yuanxin Wei, Shengyuan Ye, Jiazhi Jiang, Xu Chen, Dan Huang,
    Jiangsu Du, and Yutong Lu. 2024. Communication-Efficient Model Parallelism for
    Distributed In-situ Transformer Inference. In *DATE*. IEEE, 1–6.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2024) Yuanxin Wei, Shengyuan Ye, Jiazhi Jiang, Xu Chen, Dan Huang,
    Jiangsu Du, 和 Yutong Lu. 2024. 分布式在位变换器推理的通信高效模型并行性。In *DATE*. IEEE, 1–6.
- en: 'Xu et al. (2022) Daliang Xu, Mengwei Xu, Qipeng Wang, Shangguang Wang, Yun
    Ma, Kang Huang, Gang Huang, Xin Jin, and Xuanzhe Liu. 2022. Mandheling: Mixed-precision
    on-device dnn training with dsp offloading. In *MobiCom*. 214–227.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2022) Daliang Xu, Mengwei Xu, Qipeng Wang, Shangguang Wang, Yun
    Ma, Kang Huang, Gang Huang, Xin Jin, 和 Xuanzhe Liu. 2022. Mandheling: 使用DSP卸载的混合精度设备端DNN训练。In
    *MobiCom*. 214–227.'
- en: 'Xu et al. (2023) Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei,
    Mengwei Xu, and Xuanzhe Liu. 2023. Llmcad: Fast and scalable on-device large language
    model inference. *arXiv preprint arXiv:2309.04255* (2023).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei,
    Mengwei Xu, 和 Xuanzhe Liu. 2023. Llmcad: 快速且可扩展的设备端大语言模型推理。*arXiv preprint arXiv:2309.04255*
    (2023).'
- en: 'Xu et al. (2024) M Xu, D Cai, Y Wu, X Li, and S Wang. 2024. Fwdllm: Efficient
    fedllm using forward gradient. (2024).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2024) M Xu、D Cai、Y Wu、X Li 和 S Wang。2024。Fwdllm: 使用前向梯度的高效 fedllm。（2024）。'
- en: 'Ye et al. (2024a) Shengyuan Ye, Jiangsu Du, Liekang Zeng, Wenzhong Ou, Xiaowen
    Chu, Yutong Lu, and Xu Chen. 2024a. Galaxy: A Resource-Efficient Collaborative
    Edge AI System for In-situ Transformer Inference. *arXiv preprint arXiv:2405.17245*
    (2024).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2024a) Shengyuan Ye、Jiangsu Du、Liekang Zeng、Wenzhong Ou、Xiaowen
    Chu、Yutong Lu 和 Xu Chen。2024a。Galaxy: 一种用于原位变压器推断的资源高效协作边缘 AI 系统。*arXiv 预印本 arXiv:2405.17245*（2024）。'
- en: 'Ye et al. (2024b) Shengyuan Ye, Liekang Zeng, Xiaowen Chu, Guoliang Xing, and
    Xu Chen. 2024b. Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative
    DNN Training on Heterogeneous Edge Devices. In *Proceedings of the 30th Annual
    International Conference on Mobile Computing and Networking*. 312–326.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2024b) Shengyuan Ye、Liekang Zeng、Xiaowen Chu、Guoliang Xing 和 Xu
    Chen。2024b。Asteroid: 用于异构边缘设备上的协作 DNN 训练的资源高效混合管道并行。在 *Proceedings of the 30th
    Annual International Conference on Mobile Computing and Networking* 中。312–326。'
- en: 'Ye et al. (2022) Shengyuan Ye, Liekang Zeng, Qiong Wu, Ke Luo, Qingze Fang,
    and Xu Chen. 2022. Eco-FL: Adaptive federated learning with efficient edge collaborative
    pipeline training. In *Proceedings of the 51st International Conference on Parallel
    Processing*. 1–11.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2022) Shengyuan Ye、Liekang Zeng、Qiong Wu、Ke Luo、Qingze Fang 和 Xu
    Chen。2022。Eco-FL: 高效边缘协作管道训练的自适应联邦学习。在 *Proceedings of the 51st International
    Conference on Parallel Processing* 中。1–11。'
- en: 'Yin et al. (2023) Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, and Jing Bai.
    2023. Parameter-efficient is not sufficient: Exploring parameter, memory, and
    time efficient adapter tuning for dense predictions. *arXiv preprint arXiv:2306.09729*
    (2023).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2023) Dongshuo Yin、Xueting Han、Bin Li、Hao Feng 和 Jing Bai。2023。参数高效性不足够：探索用于密集预测的参数、内存和时间高效适配器调优。*arXiv
    预印本 arXiv:2306.09729*（2023）。
- en: Yuan et al. (2023) Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan,
    Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. 2023. Rethinking
    mobile AI ecosystem in the LLM era. *arXiv preprint arXiv:2308.14363* (2023).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Jinliang Yuan、Chen Yang、Dongqi Cai、Shihe Wang、Xin Yuan、Zeling
    Zhang、Xiang Li、Dingge Zhang、Hanzi Mei、Xianqing Jia 等。2023。重新思考 LLM 时代的移动 AI 生态系统。*arXiv
    预印本 arXiv:2308.14363*（2023）。
- en: Zeng et al. (2024) Liekang Zeng, Shengyuan Ye, Xu Chen, and Yang Yang. 2024.
    Implementation of Big AI Models for Wireless Networks with Collaborative Edge
    Computing. *IEEE Wireless Communications* 31, 3 (2024), 50–58.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. (2024) Liekang Zeng、Shengyuan Ye、Xu Chen 和 Yang Yang。2024。用于无线网络的协作边缘计算的大型
    AI 模型实现。*IEEE Wireless Communications* 31, 3 (2024)，50–58。
- en: 'Zhang et al. (2020) Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas,
    and Jitendra Malik. 2020. Side-tuning: a baseline for network adaptation via additive
    side networks. In *ECCV 2020, Proceedings, Part III 16*. Springer, 698–714.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020) Jeffrey O Zhang、Alexander Sax、Amir Zamir、Leonidas Guibas
    和 Jitendra Malik。2020。Side-tuning: 一种通过附加侧网络进行网络适应的基准。在 *ECCV 2020, Proceedings,
    Part III 16* 中。Springer，698–714。'
