- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:36:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个QuantLLM适用于所有场景：一次微调量化LLM以实现高效部署
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20202](https://ar5iv.labs.arxiv.org/html/2405.20202)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20202](https://ar5iv.labs.arxiv.org/html/2405.20202)
- en: Ke Yi South China University of Technology The Hong Kong University of Science
    and Technology Yuhui Xu Salesforce AI Research
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 柯逸华 南方科技大学 香港科技大学 许玉辉 Salesforce AI Research
- en: cs_kerry@mail.scut.edu.cn Heng Chang Tsinghua University Chen Tang Tsinghua
    University Yuan Meng Tsinghua University Tong Zhang South China University of
    Technology Jia Li The Hong Kong University of Science and Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: cs_kerry@mail.scut.edu.cn 恒昌 清华大学 陈唐 清华大学 袁萌 清华大学 张童 南方科技大学 贾丽 香港科技大学
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have advanced rapidly but face significant memory
    demands. While quantization has shown promise for LLMs, current methods typically
    require lengthy training to alleviate the performance degradation from quantization
    loss. However, deploying LLMs across diverse scenarios with different resource
    constraints, e.g., servers and personal computers, requires repeated training
    per application, which amplifies the lengthy training problem. Given that, it
    is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse
    optimal subnets for downstream applications through one-shot training. Nonetheless,
    the scale of current language models impedes efficiency and amplifies interference
    from weight sharing between subnets. We make an initial attempt to extend the
    once-for-all framework to large language models. Specifically, we decouple shared
    weights to eliminate the interference and incorporate Low-Rank adapters for training
    efficiency. Furthermore, we observe the imbalance allocation of training resources
    from the traditional uniform sampling. A non-parametric scheduler is introduced
    to adjust the sampling rate for each quantization configuration, achieving a more
    balanced allocation among subnets with varying demands. We validate the approach
    on LLaMA2 families, and downstream evaluation confirms our ability to maintain
    high performance while significantly reducing deployment time faced with multiple
    scenarios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）发展迅速，但面临显著的内存需求。虽然量化对LLMs表现出了潜力，但目前的方法通常需要较长的训练时间来缓解量化损失带来的性能下降。然而，在资源约束不同的多样场景中部署LLMs，例如服务器和个人电脑，需要针对每个应用进行反复训练，这加剧了训练时间问题。因此，训练一个一次性（OFA）超级网络，通过一次训练生成适用于下游应用的各种最优子网络是有利的。然而，当前语言模型的规模妨碍了效率，并加剧了子网络之间权重共享的干扰。我们首次尝试将一次性框架扩展到大型语言模型。具体来说，我们解耦了共享权重以消除干扰，并引入了低秩适配器以提高训练效率。此外，我们观察到传统均匀采样中的训练资源分配不平衡。我们引入了一种非参数调度器，以调整每种量化配置的采样率，实现对不同需求子网络的更均衡分配。我们在LLaMA2系列上验证了该方法，下游评估确认了我们在面对多个场景时能够保持高性能，同时显著减少部署时间。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large Language Models have shown surprising performance in the past years. However,
    they suffer from huge storage and computational costs; for example, inference
    with a LLaMA (Touvron et al.,, [2023](#bib.bib21)) model with 70B parameters needs
    at least 280 GB of GPU memory. To further boost the LLMs development for fitting
    diverse scenarios, recent studies have adopted quantization to compress the model
    size and reduce the computational costs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型近年来表现出令人惊讶的性能。然而，它们面临巨大的存储和计算成本；例如，使用70B参数的LLaMA (Touvron et al.,, [2023](#bib.bib21))模型进行推理需要至少280
    GB的GPU内存。为了进一步推动LLM在不同场景下的发展，最近的研究采用了量化技术来压缩模型大小并降低计算成本。
- en: 'Previous works have extensively explored Post-Training Quantization (Frantar
    et al.,, [2022](#bib.bib7); Xiao et al.,, [2023](#bib.bib24); Lin et al.,, [2023](#bib.bib13))
    and Quantization-Aware Training (Dettmers et al.,, [2024](#bib.bib6); Xu et al.,,
    [2023](#bib.bib25)) to alleviate the memory cost of LLMs. Post-training quantization
    (PTQ) offers swift model compression, albeit at the potential expense of performance.
    In contrast, Quantization-aware training (QAT) alleviates performance losses by
    simulating quantization errors during training, which is considerably more time-consuming
    than standard fine-tuning. When we need to deploy LLMs for diverse scenarios with
    different resource constraints, repeated quantization-aware training per scenario
    is unacceptable, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ One
    QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    (a). From the above analysis, the training major the cost of deployments; hence,
    it would be beneficial to train a once-for-all (OFA) supernet capable of delivering
    optimal subnets with diverse configurations (e.g., quantization bit-width) for
    each application, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ One
    QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    (b).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '先前的工作广泛探索了后训练量化（Frantar et al., [2022](#bib.bib7); Xiao et al., [2023](#bib.bib24);
    Lin et al., [2023](#bib.bib13)）和量化感知训练（Dettmers et al., [2024](#bib.bib6); Xu
    et al., [2023](#bib.bib25)）以缓解LLMs的内存成本。后训练量化（PTQ）提供了快速的模型压缩，尽管可能会牺牲性能。相比之下，量化感知训练（QAT）通过在训练过程中模拟量化误差来减轻性能损失，但其耗时远超过标准微调。当我们需要在不同资源限制下部署LLMs时，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs
    Once for Efficient Deployments") (a)所示，每种场景都进行重复的量化感知训练是不可接受的。根据上述分析，训练主要影响部署成本，因此，训练一个能够提供具有多种配置（例如量化位宽）的最佳子网络的“一次性（OFA）超级网络”将是有益的，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs
    Once for Efficient Deployments") (b)所示。'
- en: '![Refer to caption](img/f9d31c69112b1414794f5a3480cdf901.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9d31c69112b1414794f5a3480cdf901.png)'
- en: 'Figure 1: (a) Compressing Large Language Models (LLMs) for deployment across
    various platforms while ensuring performance is a challenging task. Applying Quantization-Aware
    Training (QAT) for each platform is both time-consuming and costly. (b) Instead,
    our objective is to one-shot fine-tune one quantized LLM that can be efficiently
    specialized for multiple platforms. The one-shot fine-tuning process significantly
    reduces the investment. (c) The LLM-QFA framework excels in swiftly delivering
    optimal networks under different resource constraints in one shot, whereas the
    traditional method requires repeated fine-tuning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：(a) 在确保性能的同时，将大型语言模型（LLMs）压缩以适应各种平台是一项具有挑战性的任务。对每个平台应用量化感知训练（QAT）既费时又昂贵。(b)
    相反，我们的目标是一次性微调一个量化的LLM，使其能够高效地专门化到多个平台。一次性微调过程显著减少了投资。(c) LLM-QFA框架在不同资源限制下能够迅速提供最佳网络，而传统方法则需要重复微调。
- en: 'To the best of our knowledge, once-for-all quantization-aware training for
    LLMs has not been investigated, primarily due to the large scale of current language
    models and the high cost of traditional QAT. Previous researches based on once-for-all
    mainly utilize the weight-sharing strategy, which helps avoid model size explosion
    caused by allocating weight for each configuration (Wang et al.,, [2020](#bib.bib22);
    Chen et al.,, [2021](#bib.bib3)). However, the weight-sharing combined with traditional
    QAT still has problems two-fold: 1) various quantization configurations (e.g.,
    2, 3, 4 bit-width) share the weight but have different orders of magnitude of
    quantization noise, resulting in the noteworthy interference problem and optimization
    challenges (Tang et al.,, [2024](#bib.bib17)). 2) Tradition QAT is based on full-finetuning,
    combined with the time-consuming process of simulating quantization errors, which
    is inefficient even under the weight-sharing scheme.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们了解，对于LLMs的“一次性量化感知训练”尚未被研究，这主要是由于当前语言模型的规模庞大以及传统QAT的高成本。基于“一次性”的先前研究主要利用了权重共享策略，这有助于避免由于为每个配置分配权重而导致的模型规模爆炸（Wang
    et al., [2020](#bib.bib22); Chen et al., [2021](#bib.bib3)）。然而，权重共享结合传统QAT仍存在两方面的问题：1)
    各种量化配置（例如2、3、4位宽）共享权重但具有不同数量级的量化噪声，导致显著的干扰问题和优化挑战（Tang et al., [2024](#bib.bib17)）。2)
    传统QAT基于完全微调，并结合了模拟量化误差的费时过程，即使在权重共享方案下也效率低下。
- en: 'Furthermore, our observations reveal that the uniform sampling strategy used
    by traditional OFA brings an imbalance in the allocation of training resources.
    As illustrated in Figure [3](#S3.F3 "Figure 3 ‣ Resource-Balance Sampling Strategy.
    ‣ 3.2 One-Shot Optimization ‣ 3 Methodology ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments"), subnets derived from uniform
    sampling exhibit a bias on their average bit-width, which falls into a low variance
    distribution. Consequently, subnets whose average bit-width deviates from this
    distribution are prone to under-fitting.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们的观察结果揭示了传统OFA使用的均匀采样策略在训练资源分配上存在不平衡。如图[3](#S3.F3 "Figure 3 ‣ Resource-Balance
    Sampling Strategy. ‣ 3.2 One-Shot Optimization ‣ 3 Methodology ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")所示，均匀采样得出的子网在其平均位宽上存在偏差，且这种偏差分布为低方差分布。因此，那些平均位宽偏离这一分布的子网容易出现欠拟合。'
- en: Integrating these aspects, we propose the LLM-QFA (Quantization-Aware Fine-tuning
    one LLM for All scenarios) framework that efficiently fine-tunes the once-for-all
    supernet for later yielding optimal subnets for diverse scenarios. First, we introduce
    interference-less fine-tuning to decouple the weights of different configurations,
    accompanied by Low-Rank adapters to enable efficient training. Specifically, we
    quantize the weights with different quantization configurations and freeze them,
    then we apply Low-Rank adapters to each quantized weight for later fine-tuning.
    Second, we propose a resource-balanced sampling strategy, which is based on a
    non-parametric scheduler that dynamically adjusts the sampling strategy across
    training steps.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 综合这些方面，我们提出了LLM-QFA（量化感知微调一体化框架），该框架高效地微调一次性超网，以便后续为各种场景生成最优子网。首先，我们引入无干扰微调，以解耦不同配置的权重，并配合低秩适配器以实现高效训练。具体来说，我们量化具有不同量化配置的权重并将其冻结，然后为每个量化权重应用低秩适配器进行后续微调。其次，我们提出了一种资源平衡采样策略，该策略基于非参数调度器，动态调整训练步骤中的采样策略。
- en: 'To evaluate our proposed framework, we conduct experiments on LLaMA2 models
    and validate the performance on the MMLU and Common Sense QA benchmarks. The results
    show that our proposed framework can yield diverse optimal quantized models for
    various scenarios. It is worth noting that our framework can be easily scaled
    up to even larger models since the training time per step is the same with previous
    LoRA-tuning (Xu et al.,, [2023](#bib.bib25)). We summarize our contributions as
    follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们提出的框架，我们在LLaMA2模型上进行实验，并在MMLU和常识问答基准上验证其性能。结果表明，我们提出的框架可以为各种场景提供多样化的最优量化模型。值得注意的是，我们的框架可以轻松扩展到更大的模型，因为每步训练时间与之前的LoRA调整（Xu等，[2023](#bib.bib25)）相同。我们的贡献总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We first introduce the once-for-all training paradigm for large language models
    (LLMs), which helps to reduce the training cost for deploying LLMs across diverse
    scenarios.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先介绍了一种一次性训练范式，用于大规模语言模型（LLMs），它有助于减少在多种场景中部署LLMs的训练成本。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we decouple weights of configurations to mitigate interference issues and incorporate
    Low-Rank adapters to enhance the training efficiency.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们解耦配置权重以减轻干扰问题，并结合低秩适配器以提高训练效率。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To address the imbalance training caused by the uniform sampling strategy, we
    propose a resource-balanced sampling strategy that focuses on providing fair sampled
    opportunity across subnets with various resource demands.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了应对均匀采样策略引起的训练不平衡问题，我们提出了一种资源平衡采样策略，重点在于为具有不同资源需求的子网提供公平的采样机会。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM Quantization
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM量化
- en: Quantization is a compression technique that reduces the bit-width of weights
    and/or activations to save memory and accelerate inference. The quantization of
    LLM can be categorized into two main lines. The first one is post-training quantization
    (PTQ) (Frantar et al.,, [2022](#bib.bib7); Xiao et al.,, [2023](#bib.bib24); Lin
    et al.,, [2023](#bib.bib13); Kim et al.,, [2023](#bib.bib10)), which focuses on
    reducing the memory footprint without retraining. Although lots of designs are
    designed to mitigate the degradation of performance, *e.g.*, handling outliers
    in parameters (Kim et al.,, [2023](#bib.bib10); [Li et al., 2023a,](#bib.bib11)
    ) and dynamic quantization (Xiao et al.,, [2023](#bib.bib24); Lin et al.,, [2023](#bib.bib13)),
    PTQ still have to drop the ultra-low bit-width (*e.g.*, 2 bit and 3 bit) to guarantee
    the performance. Hence, the second line, Quantization-Aware Training (QAT) can
    help alleviate the performance drop. The first QAT method applied on LLM (Liu
    et al.,, [2023](#bib.bib14)) inherits the idea of traditional QAT, which is computationally
    expensive in the fine-tuning stage. To reduce the training cost, (Dettmers et al.,,
    [2024](#bib.bib6); Xu et al.,, [2023](#bib.bib25); Guo et al.,, [2023](#bib.bib8);
    [Li et al., 2023b,](#bib.bib12) ) utilizing LoRA-tuning on quantized weight and
    gain a decent performance. Specifically, (Xu et al.,, [2023](#bib.bib25)) adds
    constraints on LoRA to maintain the quantization property after merging between
    LoRA weight and quantization weight, which firstly brings LoRA-tuning to actual
    quantization-aware training. Though Lora-tuning can save memory footprint and
    training costs, when faced with diverse development scenarios with different resource
    constraints, LoRA-tuning still falls into the pitfall of repeated training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种压缩技术，通过减少权重和/或激活的位宽来节省内存并加速推理。LLM 的量化可以分为两个主要方向。第一个是训练后量化（PTQ）（Frantar
    等， [2022](#bib.bib7)；Xiao 等， [2023](#bib.bib24)；Lin 等， [2023](#bib.bib13)；Kim
    等， [2023](#bib.bib10)），其重点是减少内存占用而无需重新训练。尽管有许多设计旨在减轻性能下降，例如，处理参数中的异常值（Kim 等， [2023](#bib.bib10)；[Li
    等，2023a,](#bib.bib11)）和动态量化（Xiao 等， [2023](#bib.bib24)；Lin 等， [2023](#bib.bib13)），PTQ
    仍然必须使用超低位宽（例如，2 位和 3 位）来保证性能。因此，第二个方向，量化感知训练（QAT），可以帮助缓解性能下降。第一个应用于 LLM 的 QAT
    方法（Liu 等， [2023](#bib.bib14)）继承了传统 QAT 的思想，但在微调阶段计算开销较大。为了降低训练成本，（Dettmers 等，
    [2024](#bib.bib6)；Xu 等， [2023](#bib.bib25)；Guo 等， [2023](#bib.bib8)；[Li 等，2023b,](#bib.bib12)）在量化权重上利用
    LoRA 调优，取得了不错的性能。具体来说，（Xu 等， [2023](#bib.bib25)）在 LoRA 上添加了约束，以在 LoRA 权重与量化权重合并后保持量化属性，这首次将
    LoRA 调优引入实际的量化感知训练。尽管 LoRA 调优可以节省内存占用和训练成本，但在面对不同资源限制的多样化开发场景时，LoRA 调优仍然会陷入重复训练的困境。
- en: Once for All training
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一次性训练
- en: Once-for-all training (OFA) methods (Wang et al.,, [2020](#bib.bib22); Chen
    et al.,, [2021](#bib.bib3); Yu et al.,, [2020](#bib.bib26); Tang et al.,, [2023](#bib.bib19),
    [2022](#bib.bib18)) aim to train a one-shot supernet that can serve diverse scenarios
    with different resource constraints and save expensive retraining per scenario.
    On non-LLMs, the success of one-shot training comes from the weight-sharing scheme
    between different configurations (Chen et al.,, [2021](#bib.bib3); Yu et al.,,
    [2020](#bib.bib26)), while weight-sharing also brings interference between different
    bit-widths for quantization-aware training (Tang et al.,, [2024](#bib.bib17),
    [2023](#bib.bib19)). Moreover, traditional OFA with weight sharing necessitates
    fine-tuning entire parameters, which is impracticable for LLMs due to their extensive
    size.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性训练（OFA）方法（Wang 等， [2020](#bib.bib22)；Chen 等， [2021](#bib.bib3)；Yu 等， [2020](#bib.bib26)；Tang
    等， [2023](#bib.bib19)， [2022](#bib.bib18)）旨在训练一个一站式超级网络，以应对不同资源限制的多种场景，并节省每个场景的昂贵重新训练。在非
    LLM 上，一次性训练的成功来自于不同配置之间的权重共享方案（Chen 等， [2021](#bib.bib3)；Yu 等， [2020](#bib.bib26)），而权重共享也会带来不同位宽之间的干扰，影响量化感知训练（Tang
    等， [2024](#bib.bib17)， [2023](#bib.bib19)）。此外，传统的 OFA 权重共享需要对所有参数进行微调，这对于规模庞大的
    LLM 来说是不切实际的。
- en: 3 Methodology
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Problem definition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: This paper focuses on the dimension of quantization to compress the LLMs for
    efficient deployment across diverse scenarios, which involves 1) post-training
    quantization to compress LLMs and 2) constructing the layer-wise mixed-precision
    supernet based on quantized LLMs and 3) optimizing the supernet.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注量化维度以压缩 LLM，以便在各种场景中高效部署，其中包括 1）训练后量化以压缩 LLM，2）基于量化 LLM 构建逐层混合精度超级网络，以及
    3）优化超级网络。
- en: Post-training Quantization
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 后训练量化
- en: To reduce memory cost, it is effective to quantize the pre-trained weight of
    LLMs in low-bit representation; mathematically, given the bit-width $\mathbf{N}$,
    the quantization process can be defined as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低内存成本，将预训练的 LLM 权重量化为低位表示是有效的；从数学上讲，给定位宽 $\mathbf{N}$，量化过程可以定义为
- en: '|  | $\hat{\mathbf{W}}=\lfloor\frac{\mathbf{W}-\beta}{\alpha}\rceil,\alpha=(\max(\mathbf{W})-\min(\mathbf{W}))/(2^{N}-1),\beta=\min(\mathbf{W}),$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{W}}=\lfloor\frac{\mathbf{W}-\beta}{\alpha}\rceil,\alpha=(\max(\mathbf{W})-\min(\mathbf{W}))/(2^{N}-1),\beta=\min(\mathbf{W}),$
    |  | (1) |'
- en: where $\alpha$ is the quantized weight, and its elements are stored in a set
    of $\{0,1,\ldots,2^{N}-1\}$. Here, only two float point numbers and a series of
    integers are needed for storage and computation memory,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha$ 是量化权重，其元素存储在 $\{0,1,\ldots,2^{N}-1\}$ 的集合中。在这里，仅需要两个浮点数和一系列整数来进行存储和计算，
- en: Layer-wise Mixed-precision Supernet
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层级混合精度超级网络
- en: In contrast to uniform bit-width quantization, mixed-precision quantization,
    which allows for varying bit-widths across different layers, can yield superior
    performance by capitalizing on the inherent redundancy in specific layers. In
    this work, we build a supernet containing different quantization bit-width configurations
    layer-wisely. Each single path of the supernets denotes a mixed-precision LLM
    and we aim to optimize all single paths, which can be formulated as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与均匀位宽量化相比，混合精度量化允许在不同层之间使用不同的位宽，通过利用特定层的固有冗余，可以获得更好的性能。在这项工作中，我们构建了一个包含不同量化位宽配置的超级网络，每条单独路径的超级网络表示一个混合精度
    LLM，我们的目标是优化所有单独路径，这可以表述为
- en: '|  | $1$2 |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $s_{i}$. $Q_{l,i}$. Our target is to 1) optimize all the subnets at once
    and 2) offer optimal subnets under given resource constraints.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{i}$。 $Q_{l,i}$。我们的目标是 1) 一次性优化所有子网络，并 2) 在给定资源约束下提供最佳子网络。
- en: 3.2 One-Shot Optimization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 单次优化
- en: Interference-Less Fine-tuning.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无干扰微调。
- en: We have observed that previous one-shot training methodologies (Cai et al.,,
    [2019](#bib.bib2); Yu et al.,, [2020](#bib.bib26)) gained success from their weight-sharing
    scheme, which avoids large model sizes caused by saving the weight of each configuration.
    However, the weight-sharing scheme also brings interference problems. Specifically,
    high and low bit-width have different quantization noise, and significantly superimposed
    quantization noise leads to optimization challenges (Tang et al.,, [2024](#bib.bib17)).
    To alleviate interference between different configurations, the straightforward
    approach is to decouple shared weights and assign weights for each configuration,
    which is costly for large-size models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，以前的单次训练方法（Cai 等，[2019](#bib.bib2); Yu 等，[2020](#bib.bib26)）通过其权重共享方案取得了成功，这避免了因保存每个配置的权重而导致的大模型尺寸。然而，权重共享方案也带来了干扰问题。具体来说，高位宽和低位宽的量化噪声不同，显著叠加的量化噪声导致了优化挑战（Tang
    等，[2024](#bib.bib17)）。为了缓解不同配置之间的干扰，直接的方法是解耦共享权重并为每个配置分配权重，这对大型模型来说成本很高。
- en: '![Refer to caption](img/5e8b39cf2d4c4da57b6cb201d6a50213.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5e8b39cf2d4c4da57b6cb201d6a50213.png)'
- en: 'Figure 2: An illustration of the goal of LLM-QFA. Compared with traditional
    OFA with Quantization-Aware Training, our approach circumvents interference issues
    by decoupling shared weight and incorporating the Low-Rank Adapter to further
    enhance the training efficiency. More notably, we employ a resource-balance sampling
    strategy to expedite the convergence of subnets across resource constraints.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLM-QFA 目标的示意图。与传统的量化感知训练（OFA）相比，我们的方法通过解耦共享权重并引入低秩适配器来进一步提高训练效率，从而规避了干扰问题。更值得注意的是，我们采用了一种资源平衡采样策略，以加速在资源约束下子网络的收敛。
- en: 'Hence, we incorporate Low-Rank adapters to represent each quantization configuration,
    which only brings negligible extra cost compared with the size of LLMs. Specifically,
    the forward process can be defined as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入低秩适配器来表示每种量化配置，这与 LLM 的大小相比，仅带来微不足道的额外成本。具体来说，前向过程可以定义为：
- en: '|  | $\mathbf{Y}={\alpha_{i}}\cdot\hat{\mathbf{W}_{i}}\cdot\mathbf{X}+{\beta_{i}}\cdot\mathbf{X}+\mathbf{B_{i}A_{i}}\cdot\mathbf{X},$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y}={\alpha_{i}}\cdot\hat{\mathbf{W}_{i}}\cdot\mathbf{X}+{\beta_{i}}\cdot\mathbf{X}+\mathbf{B_{i}A_{i}}\cdot\mathbf{X},$
    |  | (3) |'
- en: where ${\alpha_{i}},{\beta_{i}},\hat{\mathbf{W_{i}}}$ denotes the weight of
    Low-Rank adapters. It is noticed that, during fine-tuning, only one of the Low-Rank
    adapters is updated, which is the key to avoiding interference between different
    configurations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\alpha_{i}},{\beta_{i}},\hat{\mathbf{W_{i}}}$ 表示低秩适配器的权重。需要注意的是，在微调过程中，只更新一个低秩适配器，这是避免不同配置之间干扰的关键。
- en: To avoid heterogeneity between float point LoRA weights and quantized weight,
    which hinder the acceleration for inference, we follow QA-LoRA (Xu et al.,, [2023](#bib.bib25))
    to add constraints on adapters’ weight for preserving quantization property after
    merging.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免浮点 LoRA 权重和量化权重之间的异质性，这种异质性会阻碍推理加速，我们遵循 QA-LoRA (Xu 等， [2023](#bib.bib25))，在适配器的权重上添加约束，以保持合并后的量化属性。
- en: Integrating the above designs, the task of optimizing all subnets can be formulated
    as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 综合上述设计，优化所有子网络的任务可以表示为
- en: '|  | $\min_{\mathbf{W}_{L}}\sum_{a_{i}}\mathcal{L}_{val}\big{(}f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})\big{)},$
    |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{W}_{L}}\sum_{a_{i}}\mathcal{L}_{val}\big{(}f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})\big{)},$
    |  | (4) |'
- en: where $f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})$。
- en: Resource-Balance Sampling Strategy.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源平衡抽样策略。
- en: Fine-tuning all the subnets is a multi-objective problem. Given the impracticality
    of enumerating and tuning every subnet at each training iteration, a simplistic
    yet sub-optimal approach is to uniformly sample a few subnets from the configuration
    space for fine-tuning. Specifically, each layer has a uniform probability of choosing
    one quantization configuration, which can be formulated as $\textbf{P}(Q_{l,i})=\frac{1}{N}$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有子网络进行微调是一个多目标问题。由于在每次训练迭代中枚举和调整每个子网络的不可行性，一个简单但次优的方法是从配置空间中均匀地抽取一些子网络进行微调。具体而言，每一层有一个均匀的概率选择一个量化配置，这可以表示为
    $\textbf{P}(Q_{l,i})=\frac{1}{N}$。
- en: 'Though it seems fair, the naive uniform sampling strategy is biased toward
    subnets whose average bit-width is close to its expected value. Assume variable
    $q_{i}$] are independent, hence the average of bit-width can be formulated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来公平，但朴素的均匀抽样策略对平均比特宽度接近其期望值的子网络存在偏差。假设变量 $q_{i}$ 是独立的，因此比特宽度的平均值可以表示为：
- en: '|  | $\displaystyle\text{Var}[Bit(s)]$ |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Var}[Bit(s)]$ |  | (5) |'
- en: where the $Bit(s)$ is close to a normal distribution, where the variance is
    extremely small when $L=32$. Hence, the subnet with an average bit-width far from
    the distribution center would get unbalanced training resources.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Bit(s)$ 接近正态分布，当 $L=32$ 时，方差极小。因此，平均比特宽度远离分布中心的子网络将获得不平衡的训练资源。
- en: '![Refer to caption](img/0b4e11be7443e796d5008c7f1ae3d304.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0b4e11be7443e796d5008c7f1ae3d304.png)'
- en: 'Figure 3: (a) Distribution of average bit-width of samples obtained from uniform
    sampling, approximating a low variance Gaussian distribution. (b) Mixed Gaussian
    Distribution can approximate Uniform Distribution. (c) Showcase of our Resource-Balance
    sampling strategy.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: (a) 从均匀抽样中获得的样本的平均比特宽度分布，接近低方差的高斯分布。 (b) 混合高斯分布可以近似均匀分布。 (c) 我们的资源平衡抽样策略展示。'
- en: The negative impact of a uniform sampling strategy has not been studied previously.
    One of the reasons is that the weight-sharing scheme has all configurations updated
    frequently, though suffering from interference problems. Under the interference-less
    setting, weights are updated more sparsely; hence, the unbalanced training would
    lead to more pronounced under-fitting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀抽样策略的负面影响此前尚未研究。原因之一是权重共享方案使得所有配置频繁更新，尽管存在干扰问题。在无干扰的情况下，权重更新更加稀疏，因此不平衡的训练会导致更明显的欠拟合。
- en: 'Revealed by Figure [3](#S3.F3 "Figure 3 ‣ Resource-Balance Sampling Strategy.
    ‣ 3.2 One-Shot Optimization ‣ 3 Methodology ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") (b), straightforwardly stacking
    normal distributions with different means can approximate a uniform distribution
    for $Bit(s)$ and alleviate the imbalance problem. From the implementation perspective,
    mixed Gaussian distribution can be achieved by setting different sampling strategies
    for configurations across training steps. The process can be formulated as'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S3.F3 "图 3 ‣ 资源平衡采样策略。 ‣ 3.2 一次性优化 ‣ 3 方法论 ‣ 一个 QuantLLM 适用于所有：一次性微调量化
    LLM 以实现高效部署") (b) 透露，简单地叠加具有不同均值的正态分布可以近似 $Bit(s)$ 的均匀分布，并缓解不平衡问题。从实现的角度来看，通过在训练步骤中设置不同的采样策略，可以实现混合高斯分布。该过程可以表述为
- en: '|  | $\displaystyle\text{E}[Bit(s,t)]=(b_{N}-b_{1})\cdot\lvert 2\cdot\frac{t}{SL}-1\rvert,$
    |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{E}[Bit(s,t)]=(b_{N}-b_{1})\cdot\lvert 2\cdot\frac{t}{SL}-1\rvert,$
    |  | (6) |'
- en: where $SL$ to $b_{1}$, leading to a smooth switchover between schedule epochs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $SL$ 到 $b_{1}$，使得调度周期之间平滑切换。
- en: Compared to the uniform sampling strategy, our approach prevents bias on subnets
    in median size. Therefore, the subnet space converges more efficiently, which
    makes the following search process more effective. Compared to a shared-weight
    scheme, our approach can alleviate the interference problem with negligible extra
    memory costs. As a result, our approach provides a more efficient and effective
    way to optimize the Layer-wise Mixed-precision Supernet, which can be efficiently
    deployed in different scenarios with diverse resource constraints.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与均匀采样策略相比，我们的方法可以避免对中等规模子网的偏倚。因此，子网空间收敛得更高效，这使得后续的搜索过程更有效。与共享权重方案相比，我们的方法可以以微不足道的额外内存成本缓解干扰问题。因此，我们的方法提供了一种更高效、更有效的方式来优化逐层混合精度超网络，可以在具有不同资源限制的不同场景中高效部署。
- en: 3.3 Search Optimized Subnet
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 搜索优化子网
- en: We decouple the fine-tuning process and the searching process. No extra retraining
    cost is needed when finding the optimal subnet under the given resource constraint.
    The searching process starts with random searching, where a few subnets are sampled.
    Then, correlation analysis between the subnets’ performance on the validation
    set and the quantization bit-width of each layer is conducted. Learning from the
    correlation, the sensitivity of each layer to quantization bit-width can be obtained
    and the search space can be further narrowed down. Finally, we further sample
    subnets from the narrowed search space, and the final optimal subnet is selected
    based on the performance of the validation set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调过程和搜索过程解耦。在给定资源约束下找到最佳子网时，无需额外的重新训练成本。搜索过程从随机搜索开始，其中采样了一些子网。接着，分析子网在验证集上的表现与每层量化位宽之间的相关性。通过从相关性中学习，可以获得每层对量化位宽的敏感性，并进一步缩小搜索空间。最后，我们从缩小后的搜索空间中进一步采样子网，并根据验证集的表现选择最终的最佳子网。
- en: 4 Experiments
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Settings
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Models and Quantization.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型与量化。
- en: We conduct experiments on two LLMs, LLaMA2-7b and LLaMA2-13b. The quantization
    is based on GPTQ (Frantar et al.,, [2022](#bib.bib7)) with 2, 3, 4 bit-width quantization.
    The detailed quantization configuration, *e.g.*, group size, and order, are consistent
    with QA-LoRA (Xu et al.,, [2023](#bib.bib25)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个 LLMs 上进行实验，LLaMA2-7b 和 LLaMA2-13b。量化基于 GPTQ (Frantar 等，[2022](#bib.bib7))，量化位宽为
    2、3、4 位。详细的量化配置，如组大小和顺序，与 QA-LoRA (Xu 等，[2023](#bib.bib25)) 一致。
- en: Datasets and Training Details.
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和训练细节。
- en: We fine-tune models with Alpaca (Taori et al.,, [2023](#bib.bib20)), which contains
    52K instruction-following data generated from GPT 3.5 (Wang et al.,, [2022](#bib.bib23)).
    The length of one schedule epoch is 8k training steps. Following previous works(Dettmers
    et al.,, [2024](#bib.bib6); Xu et al.,, [2023](#bib.bib25)), we use a paged AdamW
    optimizer with a batch size 16 and a learning rate of $2\times 10^{-5}$. The training
    process is conducted on one A100 GPU, and only 8 GPU hours are needed to fine-tune
    one LLaMA2-7b-based supernet with 10K steps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Alpaca (Taori 等，[2023](#bib.bib20)) 对模型进行微调，该数据集包含 52K 条从 GPT 3.5 (Wang
    等，[2022](#bib.bib23)) 生成的指令跟随数据。一个调度周期的长度为 8k 训练步骤。遵循以往的工作 (Dettmers 等，[2024](#bib.bib6);
    Xu 等，[2023](#bib.bib25))，我们使用带有批量大小为 16 和学习率为 $2\times 10^{-5}$ 的分页 AdamW 优化器。训练过程在一台
    A100 GPU 上进行，微调一个基于 LLaMA2-7b 的超网络需要 10K 步，仅需 8 GPU 小时。
- en: Evaluation.
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价。
- en: 'We evaluate the performance of the models on MMLU (Hendrycks et al.,, [2021](#bib.bib9))
    and Common Sense QA benchmarks. The MMLU dataset contains four categories: Humanities,
    STEM, Social, and Other. The Common Sense QA benchmarks include HellaSwag (Zellers
    et al.,, [2019](#bib.bib27)), PIQA (Bisk et al.,, [2020](#bib.bib1)), WinoGrande
    (Sakaguchi et al.,, [2021](#bib.bib16)), ARC-e, ARC-c (Clark et al.,, [2018](#bib.bib5)),
    BoolQ (Clark et al.,, [2019](#bib.bib4)), and OBQA (Mihaylov et al.,, [2018](#bib.bib15)).
    For the MMLU Benchmark, we search the optimal subnets on the MMLU evaluation dataset.
    Initially, we sampled the first 100 subnets randomly and subsequently employed
    a shrinkage strategy to sample an additional 50 subnets, denoted as [100, 50].
    For the Common Sense QA datasets, we similarly searched for optimal subnets on
    the ARC-C dataset with [100,50] setting. We report the $0$-shot accuracy on Common
    Sense QA benchmarks.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了模型在 MMLU (Hendrycks et al.,, [2021](#bib.bib9)) 和常识 QA 基准上的表现。MMLU 数据集包含四个类别：人文学科、STEM、社会和其他。常识
    QA 基准包括 HellaSwag (Zellers et al.,, [2019](#bib.bib27))、PIQA (Bisk et al.,, [2020](#bib.bib1))、WinoGrande
    (Sakaguchi et al.,, [2021](#bib.bib16))、ARC-e、ARC-c (Clark et al.,, [2018](#bib.bib5))、BoolQ
    (Clark et al.,, [2019](#bib.bib4)) 和 OBQA (Mihaylov et al.,, [2018](#bib.bib15))。对于
    MMLU 基准，我们在 MMLU 评估数据集上搜索最佳子网。最初，我们随机抽取了前 100 个子网，然后采用收缩策略再抽取了 50 个子网，记为 [100,
    50]。对于常识 QA 数据集，我们同样在 ARC-C 数据集上搜索了最佳子网，设置为 [100,50]。我们报告了在常识 QA 基准上的 $0$-shot
    准确率。
- en: 4.2 Main Results
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: '![Refer to caption](img/a5bd6e1067f9334dd79db5383504e4d0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a5bd6e1067f9334dd79db5383504e4d0.png)'
- en: 'Figure 4: Left: The time required to obtain N specialized networks varies across
    methods. Our proposed QFA approach significantly reduces the time cost compared
    to the QA-LoRA method and achieves a comparable efficiency level to the pure quantization
    technique, GPTQ. Right: For each method, we obtain three specialized networks
    under (2, 3, 4) bit constraints on the LLaMA2-7b and LLaMA2-13B models. The average
    accuracy on the $5$-shot MMLU benchmark for networks quantized at (2, 3, 4) bits
    is reported. Although GPTQ can achieve a lower time cost, it is accompanied by
    an unacceptable level of performance degradation. Full results are provided in
    Table [4.2](#S4.SS2 "4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL:
    Fine-tuning Quantized LLMs Once for Efficient Deployments").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：左侧：获取 N 个专业网络所需的时间因方法而异。我们提出的 QFA 方法显著减少了时间成本，与纯量化技术 GPTQ 达到了相当的效率水平。右侧：对于每种方法，我们在
    LLaMA2-7b 和 LLaMA2-13B 模型上获得了 (2, 3, 4) 位约束下的三个专业网络。报告了在 (2, 3, 4) 位量化网络的 $5$-shot
    MMLU 基准上的平均准确率。虽然 GPTQ 可以实现更低的时间成本，但其伴随的性能下降是不可接受的。完整结果见表 [4.2](#S4.SS2 "4.2 Main
    Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once
    for Efficient Deployments")。'
- en: 'Table 1: 0-shot and 5-shot accuracy (%) on the Massive Multitask Language Understanding
    (MMLU) dataset. Each block is based on the same foundation model specified in
    the first row. For each method, we present the metrics achieved under the bit-width
    resource constraints of 2, 3, 4, as well as the corresponding averages.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在大规模多任务语言理解 (MMLU) 数据集上的 0-shot 和 5-shot 准确率（%）。每个块基于第一行指定的相同基础模型。对于每种方法，我们展示了在
    2、3、4 位资源约束下的指标，以及相应的平均值。
- en: '| Method | Bit | MMLU ($0$-shot) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | MMLU ($0$-shot) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Const. | Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other
    | Avg. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 常数 | 人文学科 | STEM | 社会 | 其他 | 平均值 | 人文学科 | STEM | 社会 | 其他 | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA2-7b | 16 | 48.3 | 35.2 | 48.8 | 45.8 | 43.6 | 51.6 | 37.3 | 52.2 |
    49.9 | 46.8 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7b | 16 | 48.3 | 35.2 | 48.8 | 45.8 | 43.6 | 51.6 | 37.3 | 52.2 |
    49.9 | 46.8 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 40.4 | 33.7 | 45.9 | 42.2 | 39.9 | 50.5
    | 36.9 | 50.5 | 47.5 | 45.1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 40.4 | 33.7 | 45.9 | 42.2 | 39.9 | 50.5
    | 36.9 | 50.5 | 47.5 | 45.1 |'
- en: '| GPTQ | 3 | 28.8 | 25.8 | 25.6 | 28.0 | 27.0 | 31.6 | 28.2 | 25.6 | 32.9 |
    30.7 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 28.8 | 25.8 | 25.6 | 28.0 | 27.0 | 31.6 | 28.2 | 25.6 | 32.9 |
    30.7 |'
- en: '| GPTQ | 2 | 23.8 | 23.7 | 22.5 | 23.8 | 23.5 | 24.3 | 23.0 | 23.9 | 26.1 |
    24.2 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | 23.8 | 23.7 | 22.5 | 23.8 | 23.5 | 24.3 | 23.0 | 23.9 | 26.1 |
    24.2 |'
- en: '| GPTQ | Avg. |  |  |  |  | 30.1 |  |  |  |  | 33.3 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 平均值 |  |  |  |  | 30.1 |  |  |  |  | 33.3 |'
- en: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 49.7 | 37.5 | 51.4 | 47.8 | 45.7 | 49.8
    | 36.8 | 49.8 | 47.8 | 45.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 49.7 | 37.5 | 51.4 | 47.8 | 45.7 | 49.8
    | 36.8 | 49.8 | 47.8 | 45.1 |'
- en: '| QA-LoRA | 3 | 43.3 | 33.7 | 44.8 | 42.9 | 40.5 | 40.2 | 34.8 | 44.1 | 40.8
    | 39.5 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 43.3 | 33.7 | 44.8 | 42.9 | 40.5 | 40.2 | 34.8 | 44.1 | 40.8
    | 39.5 |'
- en: '| QA-LoRA | 2 | 32.6 | 27.2 | 35.6 | 33.2 | 31.7 | 27.2 | 26.9 | 29.0 | 30.5
    | 28.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 32.6 | 27.2 | 35.6 | 33.2 | 31.7 | 27.2 | 26.9 | 29.0 | 30.5
    | 28.3 |'
- en: '| QA-LoRA | Avg. |  |  |  |  | 39.3 |  |  |  |  | 37.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 平均值 |  |  |  |  | 39.3 |  |  |  |  | 37.6 |'
- en: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 50.3 | 37.4 | 49.8 | 46.8 | 45.2 | 48.4
    | 35.6 | 48.1 | 46.9 | 44.0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 50.3 | 37.4 | 49.8 | 46.8 | 45.2 | 48.4
    | 35.6 | 48.1 | 46.9 | 44.0 |'
- en: '| LLM-QFA | 3 | 42.3 | 34.4 | 48.1 | 42.9 | 41.2 | 41.4 | 33.3 | 46.2 | 41.2
    | 39.8 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 42.3 | 34.4 | 48.1 | 42.9 | 41.2 | 41.4 | 33.3 | 46.2 | 41.2
    | 39.8 |'
- en: '| LLM-QFA | 2 | 33.7 | 28.7 | 36.3 | 32.9 | 32.5 | 28.8 | 28.2 | 32.5 | 30.5
    | 29.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 33.7 | 28.7 | 36.3 | 32.9 | 32.5 | 28.8 | 28.2 | 32.5 | 30.5
    | 29.8 |'
- en: '| LLM-QFA | Avg. |  |  |  |  | 39.6 |  |  |  |  | 37.9 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 平均值 |  |  |  |  | 39.6 |  |  |  |  | 37.9 |'
- en: '| LLaMA2-13b | 16 | 56.9 | 42.4 | 61.0 | 55.6 | 52.8 | 62.9 | 44.4 | 63.9 |
    56.7 | 55.7 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | 16 | 56.9 | 42.4 | 61.0 | 55.6 | 52.8 | 62.9 | 44.4 | 63.9 |
    56.7 | 55.7 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 55.3 | 41.6 | 58.1 | 53.3 | 51.1 | 61.3
    | 43.3 | 62.5 | 57.2 | 54.9 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 55.3 | 41.6 | 58.1 | 53.3 | 51.1 | 61.3
    | 43.3 | 62.5 | 57.2 | 54.9 |'
- en: '| GPTQ | 3 | 42.0 | 31.8 | 43.6 | 41.3 | 39.0 | 41.4 | 36.5 | 46.7 | 43.7 |
    41.5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 42.0 | 31.8 | 43.6 | 41.3 | 39.0 | 41.4 | 36.5 | 46.7 | 43.7 |
    41.5 |'
- en: '| GPTQ | 2 | 25.0 | 22.4 | 22.3 | 24.4 | 23.5 | 23.8 | 23.4 | 22.6 | 24.9 |
    23.7 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | 25.0 | 22.4 | 22.3 | 24.4 | 23.5 | 23.8 | 23.4 | 22.6 | 24.9 |
    23.7 |'
- en: '| GPTQ | Avg. |  |  |  |  | 37.9 |  |  |  |  | 40.0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 平均值 |  |  |  |  | 37.9 |  |  |  |  | 40.0 |'
- en: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 56.9 | 41.5 | 60.4 | 54.9 | 52.3 | 59.6
    | 42.7 | 62.2 | 57.4 | 54.2 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 56.9 | 41.5 | 60.4 | 54.9 | 52.3 | 59.6
    | 42.7 | 62.2 | 57.4 | 54.2 |'
- en: '| QA-LoRA | 3 | 54.0 | 40.0 | 57.1 | 52.5 | 49.9 | 56.8 | 41.9 | 59.0 | 53.5
    | 51.7 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 54.0 | 40.0 | 57.1 | 52.5 | 49.9 | 56.8 | 41.9 | 59.0 | 53.5
    | 51.7 |'
- en: '| QA-LoRA | 2 | 32.6 | 28.9 | 31.4 | 35.3 | 31.8 | 30.3 | 28.2 | 34.4 | 36.5
    | 32.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 32.6 | 28.9 | 31.4 | 35.3 | 31.8 | 30.3 | 28.2 | 34.4 | 36.5
    | 32.0 |'
- en: '| QA-LoRA | Avg. |  |  |  |  | 45.3 |  |  |  |  | 45.8 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 平均值 |  |  |  |  | 45.3 |  |  |  |  | 45.8 |'
- en: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 57.4 | 41.3 | 60.4 | 55.8 | 52.5 | 59.1
    | 42.1 | 61.1 | 56.2 | 53.4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 57.4 | 41.3 | 60.4 | 55.8 | 52.5 | 59.1
    | 42.1 | 61.1 | 56.2 | 53.4 |'
- en: '| LLM-QFA | 3 | 56.3 | 40.3 | 58.8 | 54.6 | 51.3 | 56.7 | 40.6 | 59.9 | 54.5
    | 51.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 56.3 | 40.3 | 58.8 | 54.6 | 51.3 | 56.7 | 40.6 | 59.9 | 54.5
    | 51.8 |'
- en: '| LLM-QFA | 2 | 34.5 | 30.3 | 33.0 | 37.3 | 33.5 | 32.2 | 28.5 | 36.0 | 37.2
    | 33.1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 34.5 | 30.3 | 33.0 | 37.3 | 33.5 | 32.2 | 28.5 | 36.0 | 37.2
    | 33.1 |'
- en: '| LLM-QFA | Avg. |  |  |  |  | 45.8 |  |  |  |  | 46.1 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 平均值 |  |  |  |  | 45.8 |  |  |  |  | 46.1 |'
- en: Comparisons with on MMLU.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 MMLU 的比较。
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") reports the
    comparison between LLM-QFA and Quantization-Aware training methods (QA-LoRA) and
    the Post-Training Quantization method (GPTQ) under (2, 3, 4) bit-widths. LLM-QFA
    demonstrates significantly higher efficiency than QA-LoRA faced with multiple
    deployment scenarios. This advantage stems from the training cost associated with
    LLM-QFA remaining constant, in contrast to the methods that scale linearly with
    the number of deployment scenarios N. Although our approach incurs a modestly
    higher time cost than GPTQ, the substantial performance degradation observed in
    GPTQ is unacceptable. Table [4.2](#S4.SS2 "4.2 Main Results ‣ 4 Experiments ‣
    One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    illustrates that, despite delivering only comparable performance under the 4-bit
    constraint, the average metrics of our method across (2, 3, 4) bit constraints
    consistently surpass those of QA-LoRA and GPTQ, without the need for costly repeated
    training.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S4.F4 "图 4 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一次量化 LLM: 为高效部署微调量化 LLM") 报告了 LLM-QFA
    与量化感知训练方法（QA-LoRA）和后训练量化方法（GPTQ）在 (2, 3, 4) 位宽下的比较。LLM-QFA 在面对多个部署场景时表现出显著更高的效率。这种优势源于
    LLM-QFA 的训练成本保持不变，而这些方法的成本随着部署场景数 N 线性增加。尽管我们的方法相比 GPTQ 的时间成本稍高，但 GPTQ 观察到的显著性能下降是不可接受的。表
    [4.2](#S4.SS2 "4.2 主要结果 ‣ 4 实验 ‣ 一次量化 LLM: 为高效部署微调量化 LLM") 说明，尽管在 4 位限制下性能相当，但我们的方法在
    (2, 3, 4) 位限制下的平均指标始终超越了 QA-LoRA 和 GPTQ，无需昂贵的重复训练。'
- en: '![Refer to caption](img/0f85063429d9521f41f1043e5b48d1b8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0f85063429d9521f41f1043e5b48d1b8.png)'
- en: 'Figure 5: LLM-QFA can deliver multiple optimal subnets under different constraints.
    Left: Comparison of ARC-C dataset; Right: Comparison of the rest of Common Sense
    QA tasks.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: LLM-QFA 在不同约束条件下可以提供多个最佳子网。左侧: ARC-C 数据集比较；右侧: 其他常识问答任务的比较。'
- en: Comparisons on Common Sense QA.
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 常识问答的比较。
- en: 'Table 2: $5$-shot accuracy (%) on the Common Sense QA tasks. Each block is
    based on the same foundation model specified in the first row. We organize all
    results under different quantization bit widths. Mixed precision configurations
    are searched on ARC-C, and the best configurations are tested on the rest of the
    Common Sense QA tasks.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在常识问答任务上的$5$-shot准确率（%）。每个区块基于第一行中指定的相同基础模型。我们根据不同的量化位宽组织所有结果。混合精度配置在ARC-C上搜索，最佳配置在其余的常识问答任务中测试。
- en: '| Method | Bit | Eval | Test |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位宽 | 评估 | 测试 |  |'
- en: '| Const. | ARC-C | HellaSwag | PIQA | WinoGrande | ARC-e | BoolQ | OBQA | Avg.
    | Std. (%) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Const. | ARC-C | HellaSwag | PIQA | WinoGrande | ARC-e | BoolQ | OBQA | Avg.
    | Std. (%) |'
- en: '| LLaMA2-7B | 16 | 52.0 | 78.2 | 80.1 | 74.1 | 81.1 | 79.3 | 45.2 | 73.0 |
    1.59 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 16 | 52.0 | 78.2 | 80.1 | 74.1 | 81.1 | 79.3 | 45.2 | 73.0 |
    1.59 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 50.8 | 77.0 | 79.5 | 73.8 | 80.2 | 74.1
    | 43.4 | 71.3 | 1.61 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 50.8 | 77.0 | 79.5 | 73.8 | 80.2 | 74.1
    | 43.4 | 71.3 | 1.61 |'
- en: '| QA-LoRA | 4 | 55.5 | 79.0 | 80.0 | 73.3 | 79.6 | 75.9 | 46.4 | 72.4 | 1.40
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 4 | 55.5 | 79.0 | 80.0 | 73.3 | 79.6 | 75.9 | 46.4 | 72.4 | 1.40
    |'
- en: '| LLM-QFA | 4 | 53.8 | 76.8 | 79.3 | 73.5 | 78.1 | 77.4 | 49.0 | 72.4 | 1.12
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 4 | 53.8 | 76.8 | 79.3 | 73.5 | 78.1 | 77.4 | 49.0 | 72.4 | 1.12
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 30.1 | 49.9 | 68.3 | 59.3 | 55.5 | 44.3
    | 35.0 | 52.1 | 1.13 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 30.1 | 49.9 | 68.3 | 59.3 | 55.5 | 44.3
    | 35.0 | 52.1 | 1.13 |'
- en: '| QA-LoRA | 3 | 47.8 | 72.4 | 75.0 | 68.4 | 73.6 | 72.0 | 44.8 | 67.7 | 1.08
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 47.8 | 72.4 | 75.0 | 68.4 | 73.6 | 72.0 | 44.8 | 67.7 | 1.08
    |'
- en: '| LLM-QFA | 3 | 49.1 | 72.3 | 76.7 | 69.0 | 73.8 | 72.8 | 43.4 | 68.0 | 1.26
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 49.1 | 72.3 | 76.7 | 69.0 | 73.8 | 72.8 | 43.4 | 68.0 | 1.26
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 25.8 | 26.2 | 51.1 | 50.6 | 26.0 | 41.7
    | 25.0 | 36.8 | 1.31 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 25.8 | 26.2 | 51.1 | 50.6 | 26.0 | 41.7
    | 25.0 | 36.8 | 1.31 |'
- en: '| QA-LoRA | 2 | 40.4 | 65.6 | 73.6 | 62.0 | 66.0 | 65.9 | 37.2 | 61.7 | 1.32
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 40.4 | 65.6 | 73.6 | 62.0 | 66.0 | 65.9 | 37.2 | 61.7 | 1.32
    |'
- en: '| LLM-QFA | 2 | 43.1 | 64.8 | 73.2 | 62.2 | 67.0 | 64.3 | 38.8 | 61.7 | 1.16
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 43.1 | 64.8 | 73.2 | 62.2 | 67.0 | 64.3 | 38.8 | 61.7 | 1.16
    |'
- en: '| LLaMA2-13B | 16 | 57.5 | 81.7 | 81.7 | 76.0 | 84.4 | 83.2 | 48.2 | 75.9 |
    1.60 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 16 | 57.5 | 81.7 | 81.7 | 76.0 | 84.4 | 83.2 | 48.2 | 75.9 |
    1.60 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 56.5 | 81.1 | 80.9 | 75.6 | 83.3 | 81.7
    | 47.4 | 75.0 | 1.58 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 56.5 | 81.1 | 80.9 | 75.6 | 83.3 | 81.7
    | 47.4 | 75.0 | 1.58 |'
- en: '| QA-LoRA | 4 | 58.0 | 79.2 | 81.3 | 74.0 | 83.3 | 83.8 | 49.4 | 75.2 | 1.43
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 4 | 58.0 | 79.2 | 81.3 | 74.0 | 83.3 | 83.8 | 49.4 | 75.2 | 1.43
    |'
- en: '| LLM-QFA | 4 | 56.0 | 79.6 | 82.0 | 73.2 | 83.5 | 83.2 | 51.0 | 75.4 | 1.31
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 4 | 56.0 | 79.6 | 82.0 | 73.2 | 83.5 | 83.2 | 51.0 | 75.4 | 1.31
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 47.8 | 68.6 | 77.7 | 67.9 | 77.1 | 71.9
    | 42.8 | 67.7 | 1.38 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 47.8 | 68.6 | 77.7 | 67.9 | 77.1 | 71.9
    | 42.8 | 67.7 | 1.38 |'
- en: '| QA-LoRA | 3 | 53.5 | 67.0 | 79.4 | 66.7 | 80.1 | 76.3 | 41.8 | 68.5 | 1.72
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 53.5 | 67.0 | 79.4 | 66.7 | 80.1 | 76.3 | 41.8 | 68.5 | 1.72
    |'
- en: '| LLM-QFA | 3 | 53.7 | 75.1 | 79.7 | 70.3 | 80.5 | 78.4 | 48.0 | 72.0 | 1.27
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 53.7 | 75.1 | 79.7 | 70.3 | 80.5 | 78.4 | 48.0 | 72.0 | 1.27
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 27.8 | 25.8 | 50.2 | 50.2 | 26.6 | 37.8
    | 23.4 | 35.7 | 1.26 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 27.8 | 25.8 | 50.2 | 50.2 | 26.6 | 37.8
    | 23.4 | 35.7 | 1.26 |'
- en: '| QA-LoRA | 2 | 49.1 | 70.8 | 76.6 | 66.4 | 76.1 | 74.1 | 44.8 | 68.1 | 1.21
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 49.1 | 70.8 | 76.6 | 66.4 | 76.1 | 74.1 | 44.8 | 68.1 | 1.21
    |'
- en: '| LLM-QFA | 2 | 49.2 | 70.9 | 77.0 | 67.2 | 76.3 | 74.3 | 44.6 | 68.4 | 1.24
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 49.2 | 70.9 | 77.0 | 67.2 | 76.3 | 74.3 | 44.6 | 68.4 | 1.24
    |'
- en: 'Table [4.2](#S4.SS2.SSS0.Px2 "Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") reports the result of Common Sense
    QA. Consistent with the findings from the MMLU benchmark, LLM-QFA demonstrates
    comparable performance with baselines at extreme bit-width (2, 4) and outperforms
    at median bit-width (3). The advantage is significant with LLaMA2-13B under 3-bit
    constraints, where LLM-QFA gains 3.5% accuracy improvement over QA-LoRA.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4.2](#S4.SS2.SSS0.Px2 "在常识问答上的比较。 ‣ 与MMLU的比较。 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一个QuantLLM：一次微调量化LLMs以实现高效部署")报告了常识问答的结果。与MMLU基准的发现一致，LLM-QFA在极端位宽（2，4）下表现出与基线相当的性能，并且在中位宽（3）下表现优越。在3-bit约束下，LLaMA2-13B的优势显著，LLM-QFA比QA-LoRA提高了3.5%的准确率。
- en: LLM-QFA under Different Resource Constraints.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM-QFA在不同资源约束下的表现。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Comparisons with on MMLU. ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
    Deployments") summarizes the results of LLM-QFA under different bit-width constraints.
    LLM-QFA achieves 45.0% ARC-C accuracy with 2.1 average bit-width, being 5% more
    accurate than QA-LoRA with similar resource demands. Compared with QA-LoRA at
    3-bit, our approach can achieve the same level of performance with fewer resources,
    a 1.2x reduction on ARC-C, and a 1.1x reduction on the rest of Common Sense QA.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "Figure 5 ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments
    ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")总结了LLM-QFA在不同位宽限制下的结果。LLM-QFA在2.1的平均位宽下实现了45.0%的ARC-C准确率，比QA-LoRA在类似资源需求下准确率高出5%。与3位的QA-LoRA相比，我们的方法可以用更少的资源达到相同的性能水平，ARC-C减少了1.2倍，其他常识QA减少了1.1倍。'
- en: Impact of Mixed Precision and Quality of Optimization.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合精度的影响和优化质量。
- en: '![Refer to caption](img/e897e03c1fa053e36aed1e4c54bd4fcf.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e897e03c1fa053e36aed1e4c54bd4fcf.png)'
- en: 'Figure 6: Visualizing the degree of optimization by LLM-QFA. Subnets sampled
    from LLM-QFA show significant robustness over baselines with simple mixed-precision.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：通过LLM-QFA可视化优化程度。从LLM-QFA采样的子网在简单混合精度下显示出相对于基线的显著鲁棒性。
- en: 'Previous results have significant performance improvement under the median
    resource constraints. To verify that the improvement does not only benefit from
    mixed precision, we separately sample 100 mixed-precision configurations for both
    GPTQ and QA-LoRA and evaluate them on the ARC-C dataset. To be noticed, we evaluate
    mixed-precision QA-LoRA based on the fine-tuned QA-LoRA weight at (2, 3, 4) bit.
    Figure [6](#S4.F6 "Figure 6 ‣ Impact of Mixed Precision and Quality of Optimization.
    ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons on Common Sense
    QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") demonstrates
    that our approach has a more robust performance across the dimension of resource
    demands, further validating that our method can help optimize all the subnets
    instead of only benefiting from the mixed-precision setting. Although the mixed-precision
    version of QA-LoRA exhibits a modest improvement in performance at higher bit-widths,
    it incurs a threefold increase in training time to achieve these results. Moreover,
    the observed performance instability suggests a potential loss of optimal subnet
    configurations under certain constraints.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '之前的结果在中等资源限制下有显著的性能提升。为了验证这种提升不仅仅得益于混合精度，我们分别为GPTQ和QA-LoRA采样了100个混合精度配置，并在ARC-C数据集上进行评估。需要注意的是，我们评估了基于微调QA-LoRA权重的混合精度QA-LoRA，位于(2,
    3, 4)位。图[6](#S4.F6 "Figure 6 ‣ Impact of Mixed Precision and Quality of Optimization.
    ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons on Common Sense
    QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")展示了我们的方法在资源需求维度上具有更强的鲁棒性，进一步验证了我们的方法可以帮助优化所有子网，而不仅仅是混合精度设置的收益。尽管混合精度版本的QA-LoRA在较高位宽下表现出适度的性能提升，但实现这些结果所需的训练时间增加了三倍。此外，观察到的性能不稳定性表明，在某些限制条件下可能会丧失最佳子网配置。'
- en: '![Refer to caption](img/11972393e9e86d0ed343427c2e019ad2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11972393e9e86d0ed343427c2e019ad2.png)'
- en: 'Figure 7: Verification of the effectiveness of Interference-Less Fine-Tuning
    and Resource-Balance Sampling Strategy.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：干扰最小化微调和资源平衡采样策略有效性的验证。
- en: '![Refer to caption](img/2d64a88fe4f7f69bb88819db3ed588a0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d64a88fe4f7f69bb88819db3ed588a0.png)'
- en: 'Figure 8: Common Sense QA accuracy (%) of LLM-QFA with different scheduler
    settings.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同调度设置下LLM-QFA的常识QA准确率（%）。
- en: 4.3 Ablation Study
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Ablation on Interference-Less Fine-tuning.
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 干扰最小化微调的消融。
- en: 'To ascertain the effectiveness of decoupling shared weight, we introduce a
    variant of our method termed shared-LoRA, wherein distinct quantization settings
    of a pre-trained weight share the same Low-Rank adapter. Figure [7](#S4.F7 "Figure
    7 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under Different
    Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons with on
    MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized
    LLMs Once for Efficient Deployments") reports that the shared-LoRA version fails
    the origin version across all resource demands, which validates the interference
    problem in one-shot training for LLMs.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定解耦共享权重的有效性，我们引入了我们方法的一个变体，称为shared-LoRA，其中预训练权重的不同量化设置共享相同的低秩适配器。图[7](#S4.F7
    "图 7 ‣ 混合精度和优化质量的影响 ‣ LLM-QFA在不同资源约束下 ‣ 常识问答的比较 ‣ 与MMLU的比较 ‣ 4.2 主要结果 ‣ 4 实验 ‣
    一个QuantLLM解决所有问题：一次微调量化LLMs以实现高效部署") 显示，shared-LoRA版本在所有资源需求下均未能超越原始版本，这验证了一次性训练中LLMs的干扰问题。
- en: Ablation on Resource-Balance Sampling.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源平衡采样的消融实验。
- en: 'Similarly, we implement a uniform sampling version of our method. Figure [7](#S4.F7
    "Figure 7 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under
    Different Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") also shows a consistently under-performing
    uniform sampling strategy; even the resource-concentrated area (3 bit) falls short
    in the comparison. This has motivated the development of a resource-balanced sampling
    strategy for training, which is designed to counteract the challenges of under-fitting
    and over-fitting encountered in one-shot training.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们实现了我们方法的均匀采样版本。图[7](#S4.F7 "图 7 ‣ 混合精度和优化质量的影响 ‣ LLM-QFA在不同资源约束下 ‣ 常识问答的比较
    ‣ 与MMLU的比较 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一个QuantLLM解决所有问题：一次微调量化LLMs以实现高效部署") 还显示了一个持续表现不佳的均匀采样策略；即使是在资源集中区域（3
    bit）也未能在比较中表现良好。这促使了资源平衡采样策略的开发，旨在解决一次性训练中遇到的欠拟合和过拟合问题。
- en: Ablation for Scheduler.
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调度器的消融实验。
- en: 'Lastly, we investigate two aspects of configuration for the scheduler, which
    are the length of epochs (SL) and schedule orders. In our main experiments, the
    epoch length is set to 8k training steps. For the short-term schedule, it is reduced
    to 1k steps, while for the long-term schedule, it is extended to 16k steps. Figure
    [8](#S4.F8 "Figure 8 ‣ Impact of Mixed Precision and Quality of Optimization.
    ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons on Common Sense
    QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") demonstrates
    that the short-term diminishes robustness and hinders convergence, particularly
    at lower bit configurations. Regarding the schedule orders, we initiate our training
    with 4-bit configurations, employing an easy-to-hard strategy. In this part, we
    assess the hard-to-easy setting. Figure [8](#S4.F8 "Figure 8 ‣ Impact of Mixed
    Precision and Quality of Optimization. ‣ LLM-QFA under Different Resource Constraints.
    ‣ Comparisons on Common Sense QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
    Deployments") demonstrates that the order has negligible impact.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调查了调度器的两个配置方面，即纪元长度（SL）和调度顺序。在我们的主要实验中，纪元长度设置为8k训练步骤。对于短期调度，纪元长度减少到1k步骤，而对于长期调度，纪元长度延长到16k步骤。图[8](#S4.F8
    "图 8 ‣ 混合精度和优化质量的影响 ‣ LLM-QFA在不同资源约束下 ‣ 常识问答的比较 ‣ 与MMLU的比较 ‣ 4.2 主要结果 ‣ 4 实验 ‣
    一个QuantLLM解决所有问题：一次微调量化LLMs以实现高效部署") 展示了短期调度减少了鲁棒性并阻碍了收敛，特别是在较低位配置下。关于调度顺序，我们以4-bit配置开始训练，采用从易到难的策略。在这一部分，我们评估了从难到易的设置。图[8](#S4.F8
    "图 8 ‣ 混合精度和优化质量的影响 ‣ LLM-QFA在不同资源约束下 ‣ 常识问答的比较 ‣ 与MMLU的比较 ‣ 4.2 主要结果 ‣ 4 实验 ‣
    一个QuantLLM解决所有问题：一次微调量化LLMs以实现高效部署") 展示了顺序对结果的影响微不足道。
- en: 5 Conclusion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work introduces the LLM-QFA framework, a once-for-all Quantization-Aware
    training approach to reduce the training cost of deploying large language models
    (LLMs) across diverse scenarios. By decoupling the weights of different configurations
    and incorporating Low-Rank adapters, we enhance training efficiency and mitigate
    interference issues. A resource-balanced sampling strategy ensures fair training
    across subnets with various resource demands. Our experiments on LLaMA2 models
    show that LLM-QFA deliver optimal quantized models, demonstrating its effectiveness
    in reducing computational and storage costs while maintaining performance. Our
    framework can be easily scaled up to even larger models since the training time
    per step is the same as with previous LoRA tuning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作介绍了 LLM-QFA 框架，这是一种一次性量化感知训练方法，用于减少在各种场景中部署大型语言模型（LLMs）的训练成本。通过解耦不同配置的权重并引入低秩适配器，我们提高了训练效率，并减轻了干扰问题。资源平衡的采样策略确保了在具有不同资源需求的子网中公平训练。我们在
    LLaMA2 模型上的实验表明，LLM-QFA 提供了最佳的量化模型，展示了其在减少计算和存储成本的同时保持性能的有效性。我们的框架可以轻松扩展到更大的模型，因为每步的训练时间与以前的
    LoRA 调优相同。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al., (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. (2020).
    Piqa: Reasoning about physical commonsense in natural language. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等人，（2020）Bisk, Y., Zellers, R., Gao, J., Choi, Y., 等人。（2020）。Piqa：关于自然语言中物理常识的推理。在
    AAAI 人工智能会议论文集中，第 34 卷，第 7432–7439 页。
- en: 'Cai et al., (2019) Cai, H., Gan, C., Wang, T., Zhang, Z., and Han, S. (2019).
    Once-for-all: Train one network and specialize it for efficient deployment. arXiv
    preprint arXiv:1908.09791.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人，（2019）Cai, H., Gan, C., Wang, T., Zhang, Z., 和 Han, S.（2019）。一次性：训练一个网络并使其专门化以实现高效部署。arXiv
    预印本 arXiv:1908.09791。
- en: 'Chen et al., (2021) Chen, M., Peng, H., Fu, J., and Ling, H. (2021). Autoformer:
    Searching transformers for visual recognition. In Proceedings of the IEEE/CVF
    international conference on computer vision, pages 12270–12280.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人，（2021）Chen, M., Peng, H., Fu, J., 和 Ling, H.（2021）。Autoformer：为视觉识别搜索变换器。在
    IEEE/CVF 国际计算机视觉大会论文集中，第 12270–12280 页。
- en: 'Clark et al., (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. (2019). BoolQ: Exploring the surprising difficulty of natural
    yes/no questions. arXiv preprint arXiv:1905.10044.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人，（2019）Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M.,
    和 Toutanova, K.（2019）。BoolQ：探索自然是/否问题的惊人困难。arXiv 预印本 arXiv:1905.10044。
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人，（2018）Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O.（2018）。你认为自己解决了问答问题？试试 ARC，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457。
- en: 'Dettmers et al., (2024) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
    L. (2024). Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人，（2024）Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer, L.（2024）。Qlora：高效微调量化
    LLMs。《神经信息处理系统进展》，36。
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人，（2022）Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D.（2022）。Gptq：用于生成预训练变换器的精确后训练量化。arXiv
    预印本 arXiv:2210.17323。
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E. P., and Kim, Y. (2023).
    Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model
    finetuning. arXiv preprint arXiv:2311.12023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人，（2023）Guo, H., Greengard, P., Xing, E. P., 和 Kim, Y.（2023）。Lq-lora：低秩加量化矩阵分解，用于高效语言模型微调。arXiv
    预印本 arXiv:2311.12023。
- en: Hendrycks et al., (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. (2021). Measuring massive multitask language
    understanding. In International Conference on Learning Representations.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人，（2021）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J.（2021）。衡量大规模多任务语言理解。在国际学习表示大会上。
- en: 'Kim et al., (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen,
    S., Mahoney, M. W., and Keutzer, K. (2023). Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人，（2023）Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney,
    M. W., 和 Keutzer, K.（2023）。Squeezellm：密集与稀疏量化。arXiv 预印本 arXiv:2306.07629。
- en: '(11) Li, S., Ning, X., Hong, K., Liu, T., Wang, L., Li, X., Zhong, K., Dai,
    G., Yang, H., and Wang, Y. (2023a). Llm-mq: Mixed-precision quantization for efficient
    llm deployment. Efficient Natural Language and Speech Processing Workshop at Advances
    in Neural Information Processing Systems.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(11) Li, S., Ning, X., Hong, K., Liu, T., Wang, L., Li, X., Zhong, K., Dai,
    G., Yang, H., 和 Wang, Y. (2023a). Llm-mq: 混合精度量化用于高效 LLM 部署. 《神经信息处理系统进展》中的高效自然语言与语音处理研讨会。'
- en: '(12) Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., and Zhao,
    T. (2023b). Loftq: Lora-fine-tuning-aware quantization for large language models.
    arXiv preprint arXiv:2310.08659.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(12) Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., 和 Zhao,
    T. (2023b). Loftq: Lora-微调感知量化用于大型语言模型. arXiv 预印本 arXiv:2310.08659。'
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han,
    S. (2023). Awq: 激活感知权重量化用于 LLM 压缩和加速. arXiv 预印本 arXiv:2306.00978。'
- en: 'Liu et al., (2023) Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,
    Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. (2023). Llm-qat: Data-free quantization
    aware training for large language models. arXiv preprint arXiv:2305.17888.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al., (2023) Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,
    Y., Shi, Y., Krishnamoorthi, R., 和 Chandra, V. (2023). Llm-qat: 无数据量化感知训练用于大型语言模型.
    arXiv 预印本 arXiv:2305.17888。'
- en: Mihaylov et al., (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    (2018). Can a suit of armor conduct electricity? a new dataset for open book question
    answering. In EMNLP.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al., (2018) Mihaylov, T., Clark, P., Khot, T., 和 Sabharwal, A. (2018).
    一套盔甲能导电吗？开放式书籍问答的新数据集. 见于 EMNLP。
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi,
    Y. (2021). Winogrande: 大规模对抗性 Winograd 语法挑战. 《ACM 通讯》，64(9):99–106。'
- en: Tang et al., (2024) Tang, C., Meng, Y., Jiang, J., Xie, S., Lu, R., Ma, X.,
    Wang, Z., and Zhu, W. (2024). Retraining-free model quantization via one-shot
    weight-coupling learning. arXiv preprint arXiv:2401.01543.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al., (2024) Tang, C., Meng, Y., Jiang, J., Xie, S., Lu, R., Ma, X.,
    Wang, Z., 和 Zhu, W. (2024). 无需重新训练的模型量化通过一次性权重耦合学习. arXiv 预印本 arXiv:2401.01543。
- en: 'Tang et al., (2022) Tang, C., Zhai, H., Ouyang, K., Wang, Z., Zhu, Y., and
    Zhu, W. (2022). Arbitrary bit-width network: A joint layer-wise quantization and
    adaptive inference approach. In Proceedings of the 30th ACM International Conference
    on Multimedia, pages 2899–2908.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al., (2022) Tang, C., Zhai, H., Ouyang, K., Wang, Z., Zhu, Y., 和 Zhu,
    W. (2022). 任意位宽网络：一种联合层级量化与自适应推理的方法. 见于第30届 ACM 国际多媒体会议，页码 2899–2908。
- en: 'Tang et al., (2023) Tang, C., Zhang, L. L., Jiang, H., Xu, J., Cao, T., Zhang,
    Q., Yang, Y., Wang, Z., and Yang, M. (2023). Elasticvit: Conflict-aware supernet
    training for deploying fast vision transformer on diverse mobile devices. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 5829–5840.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang et al., (2023) Tang, C., Zhang, L. L., Jiang, H., Xu, J., Cao, T., Zhang,
    Q., Yang, Y., Wang, Z., 和 Yang, M. (2023). Elasticvit: 面向多样移动设备的快速视觉变换器的冲突感知超网训练.
    见于 IEEE/CVF 国际计算机视觉会议，页码 5829–5840。'
- en: 'Taori et al., (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
    Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following
    llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al., (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
    Guestrin, C., Liang, P., 和 Hashimoto, T. B. (2023). Stanford alpaca: 一种遵循指令的 llama
    模型. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等.
    (2023). Llama: 开放且高效的基础语言模型. arXiv 预印本 arXiv:2302.13971。'
- en: 'Wang et al., (2020) Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., and
    Han, S. (2020). Hat: Hardware-aware transformers for efficient natural language
    processing. arXiv preprint arXiv:2005.14187.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al., (2020) Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., 和
    Han, S. (2020). Hat: 硬件感知变换器用于高效自然语言处理. arXiv 预印本 arXiv:2005.14187。'
- en: 'Wang et al., (2022) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
    Khashabi, D., and Hajishirzi, H. (2022). Self-instruct: Aligning language model
    with self generated instructions. arXiv preprint arXiv:2212.10560.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2022年）Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi,
    D., 和 Hajishirzi, H.（2022年）。Self-instruct: 使语言模型与自生成指令对齐。arXiv预印本 arXiv:2212.10560。'
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人（2023年）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.（2023年）。Smoothquant:
    大型语言模型的准确高效的后训练量化。在国际机器学习会议，页码38087–38099。PMLR。'
- en: 'Xu et al., (2023) Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H.,
    Chen, Z., Zhang, X., and Tian, Q. (2023). Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu等人（2023年）Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z.,
    Zhang, X., 和 Tian, Q.（2023年）。Qa-lora: 大型语言模型的量化感知低秩适应。arXiv预印本 arXiv:2309.14717。'
- en: 'Yu et al., (2020) Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.-J.,
    Tan, M., Huang, T., Song, X., Pang, R., and Le, Q. (2020). Bignas: Scaling up
    neural architecture search with big single-stage models. In Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part VII 16, pages 702–717\. Springer.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu等人（2020年）Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.-J., Tan, M.,
    Huang, T., Song, X., Pang, R., 和 Le, Q.（2020年）。Bignas: 使用大型单阶段模型扩展神经架构搜索。在计算机视觉–ECCV
    2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议录，第VII部分，第16卷，页码702–717。Springer。'
- en: 'Zellers et al., (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. (2019). Hellaswag: Can a machine really finish your sentence? arXiv preprint
    arXiv:1905.07830.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers等人（2019年）Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi, Y.（2019年）。Hellaswag:
    机器真的能完成你的句子吗？arXiv预印本 arXiv:1905.07830。'
