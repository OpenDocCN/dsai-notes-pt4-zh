- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LoRA 集成进行微调 LLM 的不确定性量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12264](https://ar5iv.labs.arxiv.org/html/2402.12264)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12264](https://ar5iv.labs.arxiv.org/html/2402.12264)
- en: Oleksandr Balabanov    Hampus Linander
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Oleksandr Balabanov    Hampus Linander
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning large language models can improve task specific performance, although
    a general understanding of what the fine-tuned model has learned, forgotten and
    how to trust its predictions is still missing. We derive principled uncertainty
    quantification for fine-tuned LLMs with posterior approximations using computationally
    efficient low-rank adaptation ensembles. We analyze three common multiple-choice
    datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative
    and qualitative conclusions on their perceived complexity and model efficacy on
    the different target domains during and after fine-tuning. In particular, backed
    by the numerical experiments, we hypothesise about signals from entropic uncertainty
    measures for data domains that are inherently difficult for a given architecture
    to learn.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型可以提高特定任务的性能，尽管对微调模型学到了什么、忘记了什么以及如何信任其预测的总体理解仍然缺乏。我们利用计算效率高的低秩适应集成推导了微调
    LLM 的原则性不确定性量化。我们使用基于 Mistral-7b 的低秩适应集成分析了三个常见的多项选择数据集，并对微调期间和之后的不同目标领域的感知复杂性和模型效能得出了定量和定性的结论。特别地，通过数值实验，我们对给定架构难以学习的数据领域的熵不确定性度量信号提出了假设。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) learns conditional distributions of vocabularies
    useful for generative tasks, text classification and code completion. Trained
    on a corpus of sequences produced by humans, such as natural language text and
    program source code, these models have shown remarkable capabilities (Bubeck et al.,
    [2023](#bib.bib7); Touvron et al., [2023a](#bib.bib65), [b](#bib.bib66); Jiang
    et al., [2023](#bib.bib36)). As the applications of LLMs continue to explode,
    informed use of these models hinge on a proper understanding of the uncertainty
    of their output (Huang et al., [2023](#bib.bib35); Kuhn et al., [2023](#bib.bib39);
    Malinin & Gales, [2021](#bib.bib49); Ren et al., [2023](#bib.bib59)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）学习有助于生成任务、文本分类和代码补全的词汇条件分布。这些模型在由人类生成的序列语料库上进行训练，例如自然语言文本和程序源代码，展现出了显著的能力（Bubeck
    等，[2023](#bib.bib7)；Touvron 等，[2023a](#bib.bib65)，[b](#bib.bib66)；Jiang 等，[2023](#bib.bib36)）。随着
    LLM 应用的不断爆炸，这些模型的有效使用依赖于对其输出不确定性的适当理解（Huang 等，[2023](#bib.bib35)；Kuhn 等，[2023](#bib.bib39)；Malinin
    & Gales，[2021](#bib.bib49)；Ren 等，[2023](#bib.bib59)）。
- en: To align an LLM towards particular needs, such as answering factual questions
    and instructions, a common approach is to fine-tune the model using a specialised
    dataset targetting human interaction (Ouyang et al., [2022](#bib.bib55)). Fine-tuning
    involves additional training of a pre-trained LLM on a smaller dataset generated
    with humans (Zhong et al., [2021](#bib.bib82)) or by other LLMs (Peng et al.,
    [2023](#bib.bib58); Zhang et al., [2023](#bib.bib80)). By fine-tuning, the model
    adapts its parameters to better capture the nuances, vocabulary, and style of
    the target domain(Peng et al., [2023](#bib.bib58)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 LLM 能够满足特定需求，如回答事实性问题和指令，常见的方法是使用针对人类互动的专门数据集对模型进行微调（Ouyang 等，[2022](#bib.bib55)）。微调涉及在通过人类（Zhong
    等，[2021](#bib.bib82)）或其他 LLM（Peng 等，[2023](#bib.bib58)；Zhang 等，[2023](#bib.bib80)）生成的小数据集上对预训练
    LLM 进行额外训练。通过微调，模型调整其参数以更好地捕捉目标领域的细微差别、词汇和风格（Peng 等，[2023](#bib.bib58)）。
- en: Even full-model fine-tuning often requires orders of magnitude less training
    time, but still incur the same computational complexity and memory usage. Prior
    studies have shown that pre-trained language models can effectively adapt to specific
    tasks within smaller parameter subspaces (Li et al., [2018](#bib.bib40); Aghajanyan
    et al., [2020](#bib.bib1)), indicating an inherently low rank during the fine-tuning
    process. To further reduce the training time and computational complexity of fine-tuning,
    a common method used is Low-Rank Adaptation (LoRA) (Hu et al., [2021](#bib.bib34)).
    As one member of the family of methods typically referred to as parameter efficient
    fine-tuning (PEFT), LoRA effectively reduces the number of parameters requiring
    training by keeping the pre-trained weights unchanged and integrating low-rank
    trainable matrices into each layer of the LLM transformer architecture.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是全模型微调也通常需要比之前少得多的训练时间，但仍然会产生相同的计算复杂性和内存使用。先前的研究表明，预训练的语言模型可以在较小的参数子空间内有效地适应特定任务（Li
    et al., [2018](#bib.bib40); Aghajanyan et al., [2020](#bib.bib1)），这表明在微调过程中固有的低秩。为了进一步减少微调的训练时间和计算复杂性，常用的方法是低秩适应（LoRA）（Hu
    et al., [2021](#bib.bib34)）。作为一种通常被称为参数高效微调（PEFT）方法的成员，LoRA 通过保持预训练权重不变并将低秩可训练矩阵集成到
    LLM 转换器架构的每一层中，有效地减少了需要训练的参数数量。
- en: 'Key questions arise after fine-tuning: What areas of knowledge remain outside
    the model’s expertise? What knowledge is retained from the pre-trained model?
    What knowledge is gained during the fine-tuning process on the target dataset?
    These questions are fundamental in guiding us towards a more reliable, interpretable
    and trustworthy application of LLMs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后出现的关键问题包括：哪些知识领域仍然超出了模型的专业范围？从预训练模型中保留了哪些知识？在目标数据集上微调过程中获得了哪些知识？这些问题对于引导我们实现更可靠、可解释和可信赖的
    LLM 应用至关重要。
- en: We contribute to addressing these questions by analyzing the uncertainty in
    LLMs that have been fine-tuned using LoRA for answering multiple-choice question
    answers (QAs). These tasks involve definite single-token target labels, which
    significantly simplifies the analysis while still requiring the model to have
    a thorough understanding of context. We explore how uncertainty estimates, divided
    into epistemic model uncertainty and aleatoric data uncertainty, can be utilized
    to observe changes in the models knowledge regarding the presented data. We use
    predictive entropy and mutual information to quantify uncertainty, the former
    containing both aleatoric and epistemic contributions whereas the latter is solely
    epistemic. These entropic uncertainty measures are calculated for a Bayesian posterior
    derived from an ensemble of LLMs fine-tuned with LoRA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过分析使用 LoRA 微调的 LLM 在回答多项选择问题（QAs）时的不确定性来解决这些问题。这些任务涉及明确的单个标记目标标签，这显著简化了分析，同时仍要求模型对上下文有透彻的理解。我们探讨了如何利用不确定性估计（分为内在模型不确定性和数据不确定性）来观察模型对呈现数据的知识变化。我们使用预测熵和互信息来量化不确定性，其中前者包含内在和数据不确定性，而后者仅包含内在不确定性。这些熵的不确定性度量是对从
    LoRA 微调的 LLM 集合中得到的贝叶斯后验分布进行计算的。
- en: We use the pre-trained Mistral-7b (Jiang et al., [2023](#bib.bib36)) model as
    a prior for the weight distribution, fine-tuning on the CommonsenseQA (CQA) (Talmor
    et al., [2019](#bib.bib62)), and the Social Sciences as well as STEM components
    from MMLU (Hendrycks et al., [2021](#bib.bib29)) multiple-choice QA datasets.
    By quantifying the evolution of knowledge using predictive entropy and mutual
    information, we show how these measures can be used to reason about complexity
    of the dataset, and expected model efficacy on the target domain. Code is available
    (Balabanov & Linander, [2024](#bib.bib3)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预训练的 Mistral-7b (Jiang et al., [2023](#bib.bib36)) 模型作为权重分布的先验，在 CommonsenseQA
    (CQA) (Talmor et al., [2019](#bib.bib62)) 和 MMLU (Hendrycks et al., [2021](#bib.bib29))
    多项选择 QA 数据集的社会科学和 STEM 组件上进行微调。通过使用预测熵和互信息量化知识的演变，我们展示了这些度量如何用于推理数据集的复杂性以及对目标领域的预期模型效能。代码可用（Balabanov
    & Linander, [2024](#bib.bib3)）。
- en: 2 Contributions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 贡献
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We derive posterior approximations for LLMs fine-tuned on target datasets using
    ensembles of LoRA members. On the way, we provide a Bayesian interpretation of
    fine-tuning, early-stopping and conditional generative tasks.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 LoRA 成员的集合对在目标数据集上微调的 LLM 进行后验近似。在这个过程中，我们提供了对微调、早期停止和条件生成任务的贝叶斯解释。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show how uncertainty quantification with entropic measures using the ensemble
    posteriors can be used to reason about dataset complexity and model efficacy on
    the target domain.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了如何利用使用集成后验的熵度量进行不确定性量化，以推理数据集复杂性和模型在目标领域的有效性。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze three common multi-choice QA datasets, CQA, MMLU STEM and MMLU Social
    Sciences, using LoRA ensemble posteriors derived from a pre-trained Mistral-7b
    model. The analyzis of the evolution of the entropic uncertainty measures during
    fine-tuning lets us draw quantitative conclusions on the relative complexity,
    out-of-distribution behaviour, and model efficacy on the different target domains.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了三个常见的多选QA数据集：CQA、MMLU STEM和MMLU社会科学，使用了从预训练的Mistral-7b模型中导出的LoRA集成后验。对微调过程中熵不确定性度量的演变进行分析，使我们能够得出关于相对复杂性、分布外行为以及模型在不同目标领域上的有效性的定量结论。
- en: 3 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: 3.1 Uncertainty quantification in LLMs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLM中的不确定性量化
- en: There has been significant interest in uncertainty quantification across various
    tasks and domains in neural networks (Gal & Ghahramani, [2015](#bib.bib20), [2016](#bib.bib21);
    Malinin & Gales, [2018](#bib.bib48); Ovadia et al., [2019b](#bib.bib57); Malinin
    et al., [2021](#bib.bib50); Lin et al., [2022](#bib.bib41); Kuhn et al., [2023](#bib.bib39);
    Lin et al., [2023](#bib.bib42)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对神经网络中各种任务和领域的不确定性量化已经引起了显著的兴趣（Gal & Ghahramani, [2015](#bib.bib20), [2016](#bib.bib21);
    Malinin & Gales, [2018](#bib.bib48); Ovadia et al., [2019b](#bib.bib57); Malinin
    et al., [2021](#bib.bib50); Lin et al., [2022](#bib.bib41); Kuhn et al., [2023](#bib.bib39);
    Lin et al., [2023](#bib.bib42)）。
- en: This interest extends to the realm of Large Language Models (LLMs), where accurately
    quantifying prediction uncertainty is a key focus (Xiao et al., [2022a](#bib.bib73);
    Lin et al., [2022](#bib.bib41); Mielke et al., [2022](#bib.bib52); Chen & Mueller,
    [2023](#bib.bib9); Duan et al., [2023](#bib.bib15); Huang et al., [2023](#bib.bib35)).
    The application of LLMs in generative tasks introduces unique challenges, notably
    in measuring uncertainty of the generative outputs. (Liu et al., [2019](#bib.bib45);
    Malinin & Gales, [2021](#bib.bib49); Kuhn et al., [2023](#bib.bib39); Lin et al.,
    [2023](#bib.bib42)). The disentanglement of uncertainty into aleatoric and epistemic
    was recently discussed in the context of LLMs (Hou et al., [2023](#bib.bib31)).
    However, it was done via ensembling of the model inputs rather than the model
    instances, and not in the context of fine-tuning tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种兴趣扩展到了大型语言模型（LLMs）的领域，其中准确量化预测不确定性是一个关键焦点（Xiao et al., [2022a](#bib.bib73);
    Lin et al., [2022](#bib.bib41); Mielke et al., [2022](#bib.bib52); Chen & Mueller,
    [2023](#bib.bib9); Duan et al., [2023](#bib.bib15); Huang et al., [2023](#bib.bib35)）。LLMs在生成任务中的应用引入了独特的挑战，特别是在衡量生成输出的不确定性方面（Liu
    et al., [2019](#bib.bib45); Malinin & Gales, [2021](#bib.bib49); Kuhn et al.,
    [2023](#bib.bib39); Lin et al., [2023](#bib.bib42)）。在LLMs的背景下，不确定性的分离为随机不确定性和认知不确定性最近被讨论过（Hou
    et al., [2023](#bib.bib31)）。然而，这是通过对模型输入进行集成而非模型实例，并且不是在微调任务的背景下进行的。
- en: 3.2 Fine-tuning in LLMs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM中的微调
- en: Fine-tuning has recently become an integral part of the LLM ecosystem where
    it is used to target specific tasks such as general instruction-following model,
    through methods like Reinforcement Learning from Human Feedback (Houlsby et al.,
    [2019](#bib.bib33); Hu et al., [2021](#bib.bib34); Liu et al., [2019](#bib.bib45);
    Ding et al., [2022](#bib.bib13), [2023](#bib.bib14)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 微调最近已成为大型语言模型（LLM）生态系统中不可或缺的一部分，它被用来针对特定任务，例如通用指令跟随模型，通过诸如来自人类反馈的强化学习方法（Houlsby
    et al., [2019](#bib.bib33); Hu et al., [2021](#bib.bib34); Liu et al., [2019](#bib.bib45);
    Ding et al., [2022](#bib.bib13), [2023](#bib.bib14)）。
- en: The large computational demands of training and fine-tuning LLMs have resulted
    in development of more efficient techniques, known as parameter-efficient fine-tuning
    (PEFT)(Liu et al., [2022](#bib.bib44); Ding et al., [2022](#bib.bib13), [2023](#bib.bib14);
    Shi & Lipani, [2024](#bib.bib60)). These methods typically involve training a
    small number of additional parameters on top of a fixed, pre-trained LLM, with
    a prominent approach being the use of low-rank adapters (LoRA) for each weight
    matrix (Hu et al., [2021](#bib.bib34)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和微调LLMs的大量计算需求促使了更高效技术的发展，称为参数高效微调（PEFT）（Liu et al., [2022](#bib.bib44); Ding
    et al., [2022](#bib.bib13), [2023](#bib.bib14); Shi & Lipani, [2024](#bib.bib60)）。这些方法通常涉及在固定的、预训练的LLM之上训练少量额外的参数，其中一种突出的方式是对每个权重矩阵使用低秩适配器（LoRA）（Hu
    et al., [2021](#bib.bib34)）。
- en: 3.3 LLMs within Bayesian methods
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM在贝叶斯方法中的应用
- en: Bayesian inference techniques in neural networks have a long-standing history
    (Denker et al., [1987](#bib.bib12); Tishby et al., [1989](#bib.bib64); Buntine
    & Weigend, [1991](#bib.bib8); MacKay, [1991](#bib.bib46)). These methods establish
    a systematic approach to derive reliable and interpretable uncertainty estimates.
    Previous research on Bayesian language models has been primarily concentrated
    on the pretraining phase of language models, rather than on their fine-tuning
    (Tran et al., [2019](#bib.bib67); Xue et al., [2021](#bib.bib75); Cinquin et al.,
    [2021](#bib.bib11); Zhang et al., [2018](#bib.bib78); Chen & Li, [2024](#bib.bib10)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的贝叶斯推理技术有着悠久的历史（Denker 等，[1987](#bib.bib12)；Tishby 等，[1989](#bib.bib64)；Buntine
    & Weigend，[1991](#bib.bib8)；MacKay，[1991](#bib.bib46)）。这些方法建立了一种系统的方法来推导可靠且可解释的不确定性估计。以往关于贝叶斯语言模型的研究主要集中在语言模型的预训练阶段，而不是微调阶段（Tran
    等，[2019](#bib.bib67)；Xue 等，[2021](#bib.bib75)；Cinquin 等，[2021](#bib.bib11)；Zhang
    等，[2018](#bib.bib78)；Chen & Li，[2024](#bib.bib10)）。
- en: Recent studies have begun exploring the fine-tuning of language models using
    a Bayesian approach. For instance, in (Fan et al., [2020](#bib.bib17)) and (Zhang
    et al., [2021](#bib.bib81)), the attention modules are sampled either from simplex-constrained
    attention distributions or using Bayesian Belief Networks. Neither of these studies
    address entropic uncertainty measures, nor do they utilize ensembling or PEFT
    methods for posterior approximation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究已开始探索使用贝叶斯方法对语言模型进行微调。例如，在（Fan 等，[2020](#bib.bib17)）和（Zhang 等，[2021](#bib.bib81)）中，注意力模块要么从简单约束的注意力分布中采样，要么使用贝叶斯信念网络。这些研究均未解决熵不确定性度量问题，也未利用集成或
    PEFT 方法进行后验近似。
- en: (Yang et al., [2024](#bib.bib76)) employs a post-hoc Laplace approximation (Mackay,
    [1992](#bib.bib47)) to model LoRA parameters for fine-tuning. While this study
    does use LoRA for fine-tuning, it does not explore entropic uncertainty measures
    and focuses on the posterior over LoRA parameters rather than the model as a whole,
    which could limit straightforward Bayesian interpretability. There are also strong
    indications that deep ensembles provide more accurate posteriors compared to single-model
    stochastic methods like Laplace and Monte Carlo dropout (Gustafsson et al., [2019](#bib.bib25);
    Ovadia et al., [2019a](#bib.bib56); Fort et al., [2019](#bib.bib18); Wilson &
    Izmailov, [2020](#bib.bib71); Dwaracherla et al., [2022](#bib.bib16); Balabanov
    et al., [2023](#bib.bib4)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (杨等，[2024](#bib.bib76)) 使用后验拉普拉斯近似（Mackay，[1992](#bib.bib47)）来建模 LoRA 参数进行微调。尽管这项研究确实使用了
    LoRA 进行微调，但它没有探讨熵不确定性度量，而是专注于 LoRA 参数的后验而非整个模型，这可能限制了直接的贝叶斯可解释性。此外，有强烈迹象表明，深度集成方法提供的后验比单模型随机方法如拉普拉斯和蒙特卡洛
    dropout（Gustafsson 等，[2019](#bib.bib25)；Ovadia 等，[2019a](#bib.bib56)；Fort 等，[2019](#bib.bib18)；Wilson
    & Izmailov，[2020](#bib.bib71)；Dwaracherla 等，[2022](#bib.bib16)；Balabanov 等，[2023](#bib.bib4)）更为准确。
- en: 3.4 Ensembling LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 集成大型语言模型
- en: Two recent studies have explored the use of ensembling in fine-tuning LLMs,
    specifically focusing on full model fine-tuning method where all weights are optimized
    (Gleave & Irving, [2022](#bib.bib23); Sun et al., [2022](#bib.bib61)). This approach
    has large memory overhead by construction. While uncertainty quantification using
    variance across the ensembles is considered, their methods lack a Bayesian formalism
    that could provide a more grounded interpretation and understanding.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两项近期研究探讨了在微调大型语言模型时使用集成，特别关注于完全模型微调方法，其中所有权重都经过优化（Gleave & Irving，[2022](#bib.bib23)；Sun
    等，[2022](#bib.bib61)）。这种方法由于其构造具有较大的内存开销。尽管考虑了使用集成的方差进行不确定性量化，但他们的方法缺乏贝叶斯形式，这可能提供更为扎实的解释和理解。
- en: An alternative method, BatchEnsemble (Wen et al., [2020](#bib.bib70)), employs
    a base model with modifications through multiplicative, component-specific rank-1
    matrices. This method has been applied to LLMs, but in the context of pre-training
    rather than fine-tuning (Tran et al., [2022](#bib.bib68)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，BatchEnsemble（Wen 等，[2020](#bib.bib70)），使用一个基础模型，并通过乘法的、组件特定的秩-1 矩阵进行修改。该方法已应用于大型语言模型，但在预训练的背景下而非微调（Tran
    等，[2022](#bib.bib68)）。
- en: There are also recent endeavors using LoRA ensembles for fine-tuning LLMs (Wang
    et al., [2023](#bib.bib69); Zhai et al., [2023](#bib.bib77)). Although these studies
    consider uncertainty quantification, they do so without employing a Bayesian framework
    and do not clearly separate epistemic and aleatoric components, critial for data
    interpretability.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些近期的努力使用 LoRA 集合来微调 LLMs（Wang et al., [2023](#bib.bib69); Zhai et al., [2023](#bib.bib77)）。尽管这些研究考虑了不确定性量化，但它们没有使用贝叶斯框架，并且没有明确区分
    epistemic 和 aleatoric 组件，这对数据解释至关重要。
- en: 4 Bayesian Deep Learning for LLMs
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 贝叶斯深度学习用于 LLMs
- en: When formulated as a conditional generative model, an LLM can be viewed as a
    function taking a sequence of tokens $s^{*}$ from the vocabulary as output. Such
    a model is typically trained to predict the next token in a sequence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当被表述为条件生成模型时，LLM 可以被视为一个将词汇表中的标记序列$s^{*}$作为输出的函数。这样的模型通常被训练来预测序列中的下一个标记。
- en: The prediction should be consistent with our observed data. This is encapsulated
    in the predictive probability distribution $p(t^{*}|s^{*},\mathcal{D})$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 预测应与我们观察到的数据一致。这体现在预测概率分布$p(t^{*}|s^{*},\mathcal{D})$中。
- en: Given a model with parameters $\theta$ can be expressed as follows
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个参数为$\theta$的模型可以表达如下
- en: '|  | $1$2 |  | (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: The posterior parameter distribution $p(\theta|\mathcal{D})$ reflects aleatoric
    uncertainty inherent in the data. Thus, as any data driven model, the output of
    LLMs has an associated uncertainty stemming from an uncertainty about model parameters,
    as well as an uncertainty regarding the correct output given the input sequence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 后验参数分布$p(\theta|\mathcal{D})$反映了数据固有的 aleatoric 不确定性。因此，作为任何数据驱动模型，LLMs 的输出具有源自模型参数不确定性以及根据输入序列的正确输出不确定性的相关不确定性。
- en: An example of aleatoric uncertainty is given by the input sequence “The color
    of the sky is ”. One would expect a model trained on a large corpus of text to
    produce a predictive distribution with support on the colors. This posterior distribution
    tells us that the model is fairly certain that the next token is a color, but
    that there is an uncertainty about which color. The uncertainty about the color
    cannot be decreased by improving the training data, it is inherent in the input
    sequence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 aleatoric 不确定性的例子是输入序列“The color of the sky is”。我们会期望一个在大语料库上训练的模型产生一个对颜色有支持的预测分布。这个后验分布告诉我们，模型相当确定下一个标记是一个颜色，但对具体颜色存在不确定性。对颜色的不确定性无法通过改善训练数据来减少，它是输入序列固有的。
- en: Moreover, aleatoric uncertainty depends on the architecture of the model. For
    example, when a current generation LLM is asked a question like “The square root
    of 123456789 equals ”, we expect the predictive distribution to exhibit high aleatoric
    uncertainty. Although the answer is definitive, and the model understands the
    context, it might still be incapable of providing the correct answer. This limitation
    might be inherent in the models architecture, and cannot be resolved through further
    training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，aleatoric 不确定性依赖于模型的架构。例如，当一个当前代 LLM 被问到诸如“The square root of 123456789 equals”这样的问题时，我们期望预测分布表现出高
    aleatoric 不确定性。尽管答案是确定的，并且模型理解上下文，但它可能仍然无法提供正确答案。这种限制可能是模型架构固有的，不能通过进一步训练来解决。
- en: On the other hand, the input sequence “Large language models are ” presents
    a simple conditional completion task, but we would not trust the output predictions
    from a model trained on a text corpus compiled before the concept was introduced.
    This uncertainty is not captured in the output distribution, but rather in the
    uncertainty of the model parameters themselves. This epistemic uncertainty can
    be systematically calculated within a Bayesian formalism, where it corresponds
    to shape of the posterior distribution of model parameters conditioned on the
    training data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，输入序列“Large language models are”呈现了一个简单的条件完成任务，但我们不会信任一个在概念引入之前编纂的文本语料库上训练的模型的输出预测。这种不确定性并未体现在输出分布中，而是体现在模型参数本身的不确定性中。这种认识上的不确定性可以在贝叶斯形式中系统地计算，其中它对应于基于训练数据条件的模型参数后验分布的形状。
- en: Separating the origin of uncertainty is important to make informed decisions
    about how to interpret the preditions of LLMs. Should we trust the output for
    a given sequence at all, and given such trust, what is the inherent uncertainty
    of the output?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分离不确定性的来源对于做出有关如何解读大语言模型（LLMs）预测的明智决策至关重要。我们是否应该完全信任给定序列的输出？如果信任，那么输出的固有不确定性是什么？
- en: 4.1 Fine-tuning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 微调
- en: The objective of fine-tuning is to use a specialized dataset to further improve
    the knowledge of a generally pre-trained model for a given task. This process
    can be conceptualized as a conditional generative task on a target domain, denoted
    as $\mathcal{D}_{\text{fine-tuning}}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的目标是使用特定的数据集进一步提升一个通常预训练模型在特定任务上的知识。这一过程可以被概念化为在目标领域上的条件生成任务，记作$\mathcal{D}_{\text{fine-tuning}}$。
- en: Fine-tuning assumes prior knowledge about the models parameters, expressed as
    $p(\theta)$. By adjusting the prior variance $\lambda^{-1}$ implies that the pre-trained
    models knowledge is not utilized. As with any posterior considerations in a Bayesian
    formalism, fine-tuning requires careful adjustment of this parameter to target
    optimal performance for the target tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 微调假设了关于模型参数的先验知识，表示为$p(\theta)$。通过调整先验方差$\lambda^{-1}$意味着不利用预训练模型的知识。与贝叶斯形式中的任何后验考虑一样，微调需要仔细调整这个参数，以实现目标任务的最佳性能。
- en: Note that in this formulation, there is no emphasis on the origin of the pre-trained
    parameters. Typically, for large language models, the training procedure consists
    of maximum likelihood optimization combined with supervised reinforcement learning
    (Touvron et al., [2023b](#bib.bib66)) and the specific details of the training
    procedure are often not publicly disclosed. In our formulation, we do not need
    to delve into these origins and instead formally take it as prior knowledge upon
    which we base our fine-tuning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个公式中，没有强调预训练参数的来源。通常，对于大型语言模型，训练过程包括最大似然优化与监督强化学习（Touvron et al.，[2023b](#bib.bib66)）的结合，训练过程的具体细节通常不会公开。在我们的公式中，我们不需要深入探讨这些来源，而是正式地将其作为我们微调的先验知识。
- en: 4.2 Posterior approximation methods
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 后验近似方法
- en: 'Variational inference (Barber & Bishop, [1998](#bib.bib5); Graves, [2011](#bib.bib24);
    Blundell et al., [2015](#bib.bib6)) offers a computationally efficient approximation
    method for the Bayesian posterior. It involves exploring a family of distributions
    $q_{\omega}(\theta)$ in this set involves minimizing the Kullback-Leibler (KL)
    divergence $\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$, can be
    expressed as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断（Barber & Bishop，[1998](#bib.bib5)；Graves，[2011](#bib.bib24)；Blundell et
    al.，[2015](#bib.bib6)）提供了一种计算上高效的贝叶斯后验近似方法。它涉及到在该集合中探索一个分布族$q_{\omega}(\theta)$，并通过最小化Kullback-Leibler（KL）散度$\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$，可以表示为：
- en: '|  | $$\displaystyle\begin{split}&amp;\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta&#124;\mathcal{D}))=\int\,d\theta\,q_{\omega}(\theta)\,\log\,\frac{q_{\omega}(\theta)}{p(\theta&#124;\mathcal{D})}\\
    &amp;=\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta))-\mathbb{E}_{q_{\omega}(\theta)}[\log\,p(t&#124;\theta,s)]+C,\\'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{split}&amp;\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta&#124;\mathcal{D}))=\int\,d\theta\,q_{\omega}(\theta)\,\log\,\frac{q_{\omega}(\theta)}{p(\theta&#124;\mathcal{D})}\\
    &amp;=\text{KL}(q_{\omega}(\theta)\,&#124;&#124;\,p(\theta))-\mathbb{E}_{q_{\omega}(\theta)}[\log\,p(t&#124;\theta,s)]+C,\\'
- en: \end{split}$$ |  | (2) |
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}$$ |  | (2) |
- en: where $\mathcal{D}=\{(t,s)\}$. Minimizing the loss function given in Eq. ([2](#S4.E2
    "Equation 2 ‣ 4.2 Posterior approximation methods ‣ 4 Bayesian Deep Learning for
    LLMs ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")) results
    in an approximation of the posterior $p(\theta|\mathcal{D})$ and (2) the expected
    negative log likelihood (ENLL), a common loss metric for classification tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{D}=\{(t,s)\}$。最小化给定的损失函数（见 Eq. ([2](#S4.E2 "Equation 2 ‣ 4.2 Posterior
    approximation methods ‣ 4 Bayesian Deep Learning for LLMs ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")）将导致后验$p(\theta|\mathcal{D})$的近似，并且（2）期望负对数似然（ENLL），这是分类任务的常见损失度量。
- en: The optimization problem described in Eq. ([2](#S4.E2 "Equation 2 ‣ 4.2 Posterior
    approximation methods ‣ 4 Bayesian Deep Learning for LLMs ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")) does not specify the origin of the
    in-domain (observed) data $\mathcal{D}$, whereas for a typical training procedure,
    the prior is usually considered to be centered around the origin in parameter
    space.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程式 Eq. ([2](#S4.E2 "方程 2 ‣ 4.2 后验近似方法 ‣ 4 贝叶斯深度学习用于LLMs ‣ 使用LoRA集成对微调LLMs的不确定性量化"))
    中描述的优化问题并未指定领域内（观察到的）数据 $\mathcal{D}$ 的来源，而在典型的训练过程中，先验通常被认为在参数空间的原点附近。
- en: 4.3 Early Stopping
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 提前停止
- en: Early stopping can be used to enhance generalization to unseen in-domain data,
    thereby reducing the effect of overfitting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止可以用来提高对未见领域内数据的泛化，从而减少过拟合的影响。
- en: In the context of Bayesian deep learning, the objective is to approximate the
    posterior distribution $p(\theta|\mathcal{D})$. Generally, $p(\theta|\mathcal{D}^{\text{train}})$
    only acknowledges the training data and struggles to generalize.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯深度学习的背景下，目标是近似后验分布 $p(\theta|\mathcal{D})$。通常，$p(\theta|\mathcal{D}^{\text{train}})$
    仅考虑训练数据，并且难以进行泛化。
- en: Early stopping improves the approximation of $p(\theta|\mathcal{D})$ using $\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$,
    assuming it represents the entire task domain effectively and was not part of
    the training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止通过使用 $\text{KL}(q_{\omega}(\theta)\,||\,p(\theta|\mathcal{D}))$ 改进对 $p(\theta|\mathcal{D})$
    的近似，假设它有效地代表了整个任务领域，并且不是训练的一部分。
- en: 5 Deep Ensembles with Low-Rank Adaptation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 低秩适应的深度集成
- en: 5.1 Low-Rank Adaptation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 低秩适应
- en: Low-Rank Adaptation (LoRA) provides a computationally efficient alternative
    to full-model fine-tuning by keeping the pre-trained weight matrices static and
    adding trainable low-rank matrices into each transformer layer, thereby decreasing
    the total number of parameters requiring training (Hu et al., [2021](#bib.bib34)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）通过保持预训练权重矩阵静态，并在每个变换器层中添加可训练的低秩矩阵，提供了一种计算高效的全模型微调替代方案，从而减少了需要训练的总参数数量（Hu
    等人，[2021](#bib.bib34)）。
- en: Given a pre-trained weight matrix $W_{\text{pretrained}}$ resides in $\mathbb{R}^{d\times
    r}$ is substantially smaller than both $d$ are tuned, while $W_{\text{pretrained}}$
    is kept unchanged.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个预训练权重矩阵 $W_{\text{pretrained}}$ 其维度为 $\mathbb{R}^{d\times r}$，远小于 $d$，并且
    $W_{\text{pretrained}}$ 保持不变。
- en: 5.2 LoRA Deep Ensembles
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LoRA 深度集成
- en: We use LoRA ensembles for approximating the posterior $p(\theta|\mathcal{D}_{\text{fine-tune}})$.
    Ensembles of LoRA members have been introduced recently (Wang et al., [2023](#bib.bib69)),
    here we provide a Bayesian treatment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 LoRA 集成来近似后验 $p(\theta|\mathcal{D}_{\text{fine-tune}})$。最近介绍了 LoRA 成员的集成（Wang
    等人，[2023](#bib.bib69)），在这里我们提供了贝叶斯处理。
- en: Deep ensembles can be explicitly interpreted within the Bayesian variational
    framework by assuming an ansatz in the form of a sum of sharply peaked distributions
    around parameter realizations $\omega_{k}$ indexing ensemble members (Hoffmann
    & Elster, [2021](#bib.bib30); Balabanov et al., [2023](#bib.bib4)). In this context,
    the variational KL loss simplifies to a sum of conventional single-member log
    likelihood loss terms, augmented with L2 regularization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度集成可以在贝叶斯变分框架中通过假设一个形如围绕参数实现 $\omega_{k}$ 的尖峰分布的和的 ansatz 进行明确解释（Hoffmann &
    Elster, [2021](#bib.bib30); Balabanov 等人，[2023](#bib.bib4)）。在这种情况下，变分 KL 损失简化为常规单成员对数似然损失项的和，并增加
    L2 正则化。
- en: We adopt the deep ensemble variational inference formulation to include LoRA
    reparametrization. The trainable low-rank LoRA matrices $A_{k}$ and $B_{k}$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用深度集成变分推断形式以包括 LoRA 重新参数化。可训练的低秩 LoRA 矩阵为 $A_{k}$ 和 $B_{k}$。
- en: 5.3 Prior Selection
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 先验选择
- en: The choice of an optimal prior is crucial for obtaining a posterior distribution
    that exhibits optimal performance. Our approach is to assume a normally distributed
    prior around the pre-trained model, $p(\theta)=N(\theta;\omega_{\text{pretrained}},\lambda^{-1}I_{\text{dim}[\theta]})$.
    This variance is a hyperparameter that must be carefully chosen based on specific
    objectives.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳先验对于获得具有最佳性能的后验分布至关重要。我们的方法是假设围绕预训练模型的正态分布先验，$p(\theta)=N(\theta;\omega_{\text{pretrained}},\lambda^{-1}I_{\text{dim}[\theta]})$。此方差是一个超参数，必须根据特定目标谨慎选择。
- en: An inappropriate selection of the prior can lead to a posterior that performs
    poorly on the task at hand. Consequently, we adjust the variance of the prior
    with the aim of optimizing the posterior quality, as measured by the log likelihood
    on the validation data. Different choice for the priors variance results in adjusting
    L2 regularization loss according to
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 选择不适当的先验可能导致后验在当前任务上表现不佳。因此，我们调整先验的方差，以期优化后验质量，这通过验证数据上的对数似然来衡量。先验方差的不同选择会导致根据下式调整L2正则化损失
- en: '|  | $1$2 |  | (3) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $A^{(i)}$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A^{(i)}$。
- en: 6 Uncertainty quantification
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 不确定性量化
- en: We quantify total predictive uncertainty, with both aleatoric and epistemic
    contributions, using predictive entropy
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预测熵量化总预测不确定性，包括随机性和认知贡献。
- en: '|  | $1$2 |  | (4) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $s^{*}$ are computed by averaging softmax outputs from network parameters
    $\theta$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s^{*}$ 是通过对网络参数 $\theta$ 的softmax输出进行平均计算得出的。
- en: We quantify epistemic uncertainty by the average mutual Shannon information
    between the model parameters and a test data sample, given the training dataset
    (Mackay, [1992](#bib.bib47))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过模型参数与测试数据样本之间的平均互信息来量化认知不确定性，给定训练数据集（Mackay，[1992](#bib.bib47)）。
- en: '|  | $$\displaystyle\begin{split}&amp;\text{MI}(\theta,t^{*}&#124;s^{*},\mathcal{D})\\
    &amp;=H(\theta&#124;\mathcal{D})-\mathbb{E}_{t^{*}\sim p(t^{*}&#124;s^{*},\mathcal{D})}\left[H(\theta&#124;t^{*},s^{*},\mathcal{D})\right]\\'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{split}&amp;\text{MI}(\theta,t^{*}&#124;s^{*},\mathcal{D})\\
    &amp;=H(\theta&#124;\mathcal{D})-\mathbb{E}_{t^{*}\sim p(t^{*}&#124;s^{*},\mathcal{D})}\left[H(\theta&#124;t^{*},s^{*},\mathcal{D})\right]\\'
- en: '&amp;=H(t^{*}&#124;s^{*},\mathcal{D})-\mathbb{E}_{\theta\sim p(\theta&#124;\mathcal{D})}\left[H(t^{*}&#124;s^{*},\theta)\right].\\'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=H(t^{*}&#124;s^{*},\mathcal{D})-\mathbb{E}_{\theta\sim p(\theta&#124;\mathcal{D})}\left[H(t^{*}&#124;s^{*},\theta)\right].\\'
- en: \end{split}$$ |  | (5) |
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}$$ |  | (5) |
- en: The mutual information MI in Eq. ([5](#S6.E5 "Equation 5 ‣ 6 Uncertainty quantification
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")) is the
    expected change in the posterior parameter distribution when a new data point
    is added to the training set (first row of RHS) (Houlsby et al., [2011](#bib.bib32)),
    and can be conveniently calculated in terms of the entropy of the predictive distributions
    (second row of RHS).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 ([5](#S6.E5 "Equation 5 ‣ 6 Uncertainty quantification ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")) 中的互信息MI是当新数据点添加到训练集中时后验参数分布的预期变化（RHS的第一行）（Houlsby等，[2011](#bib.bib32)），并且可以方便地通过预测分布的熵来计算（RHS的第二行）。
- en: Even though entropic uncertainty measures have been shown to contain valuable
    information for neural network uncertainty quantification (Linander et al., [2023](#bib.bib43)),
    one should be aware of unintuitive properties shown in recent studies (Wimmer
    et al., [2023](#bib.bib72)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管熵不确定性度量已被证明包含对神经网络不确定性量化有价值的信息（Linander等，[2023](#bib.bib43)），但应注意近期研究中显示的不直观特性（Wimmer等，[2023](#bib.bib72)）。
- en: 7 Numerical Results
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 数值结果
- en: 7.1 Datasets
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 数据集
- en: 'We evaluate our methods on three multiple-choice QA datasets: CommonsenseQA
    (CQA) (Talmor et al., [2019](#bib.bib62)), and the Social Sciences (SS) as well
    as STEM components from MMLU (Hendrycks et al., [2021](#bib.bib29)). Detailed
    information about the training and validation (test) splits is available in Appendix
    [A](#A1 "Appendix A Datasets ‣ Uncertainty quantification in fine-tuned LLMs using
    LoRA ensembles").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个多项选择QA数据集上评估我们的方法：CommonsenseQA (CQA)（Talmor等，[2019](#bib.bib62)）、社会科学（SS）以及MMLU中的STEM部分（Hendrycks等，[2021](#bib.bib29)）。关于训练和验证（测试）划分的详细信息见附录
    [A](#A1 "Appendix A Datasets ‣ Uncertainty quantification in fine-tuned LLMs using
    LoRA ensembles")。
- en: 7.2 LoRA Ensembles
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 LoRA集合
- en: We train LoRA ensembles using the pre-trained 7 billion parameter Mistral-7b
    model (Jiang et al., [2023](#bib.bib36)). The LoRA parametrization was implemented
    using PEFT (Mangrulkar et al., [2022](#bib.bib51)). Our ensembles contain 5 members
    $(M=5)$, which gives $3,407,872$ are initialized using a Kaiming-uniform distribution
    (He et al., [2015](#bib.bib27)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预训练的70亿参数Mistral-7b模型（Jiang等，[2023](#bib.bib36)）训练LoRA集合。LoRA参数化使用PEFT实现（Mangrulkar等，[2022](#bib.bib51)）。我们的集合包含5个成员
    $(M=5)$，这些成员有$3,407,872$个参数，使用Kaiming-uniform分布初始化（He等，[2015](#bib.bib27)）。
- en: The ensemble members are trained using the prescribed regularisation from Section [5.3](#S5.SS3
    "5.3 Prior Selection ‣ 5 Deep Ensembles with Low-Rank Adaptation ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles"), corresponding to using
    the pretrained Mistral-7b model as a prior. We use the Adam optimizer, training
    for 10 epochs with learning step sizes of $5\cdot 10^{-6}$ and $10$ for the CQA
    and MMLU datasets, respectively. The training focuses solely on the single token
    output representing the answer, with all other token outputs being masked.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 集成成员使用第[5.3](#S5.SS3 "5.3 Prior Selection ‣ 5 Deep Ensembles with Low-Rank Adaptation
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")节规定的正则化进行训练，对应于使用预训练的Mistral-7b模型作为先验。
    我们使用Adam优化器，训练10个周期，CQA和MMLU数据集的学习步长分别为$5\cdot 10^{-6}$和$10$。 训练仅集中于代表答案的单个令牌输出，其余令牌输出被屏蔽。
- en: To calculate performance metrics and uncertainty estimates, we reduced the output
    dimension from 32,000 to 6\. Five of these dimensions represent QA choice tokens
    (a, b, c, d, e), and the sixth aggregates the softmax prediction scores associated
    with all other tokens. We found that, even before fine-tuning, the pretrained
    Mistral-7b checkpoint assigns a near-zero probability to the sixths class with
    very few exceptions. This suggests that the model reliably comprehends the format
    of multiple-choice questions and answers, consistently producing appropriate tokens.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算性能指标和不确定性估计，我们将输出维度从32,000减少到6。 其中五个维度代表QA选择令牌（a, b, c, d, e），第六个则汇总了与所有其他令牌相关的softmax预测分数。
    我们发现，即使在微调之前，预训练的Mistral-7b检查点将第六类的概率分配为接近零，只有极少数例外。 这表明模型可靠地理解了多项选择题的格式，始终生成适当的令牌。
- en: '![Refer to caption](img/75f79b6a85f697362b0bc5aa98af8b72.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75f79b6a85f697362b0bc5aa98af8b72.png)'
- en: 'Figure 1: Performance of the LoRA ensembles trained and evaluated on either
    CQA, MMLU STEM, or MMLU SS dataset. The metrics include accuracy, log likelihood
    loss, and expected calibration error (ECE). The number of ensemble members is
    either $M=5$ the average results over 5 distinct realizations are shown.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在CQA、MMLU STEM或MMLU SS数据集上训练和评估的LoRA集成的性能。 指标包括准确率、对数似然损失和期望校准误差（ECE）。 集成成员的数量为$M=5$，显示了5次不同实现的平均结果。
- en: 7.3 Performance metrics
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 性能指标
- en: In Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles"), we illustrate the performance
    metrics for LoRA ensembles with both $M=1$ members. The ensembles are trained
    on multiple-choice QAs from CQA (first column), MMLU STEM (second column), and
    MMLU Social Studies datasets (third column).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles")中，我们展示了LoRA集成的性能指标，其中包括$M=1$个成员。
    集成是在来自CQA（第一列）、MMLU STEM（第二列）和MMLU社会研究数据集（第三列）的多项选择QA上进行训练的。
- en: The ensemble members start over-fitting after a couple of epochs as can be seen
    by the validation losses in the second row of panels, but interestingly without
    considerable drop in accuracy in the first row of panels. As will be seen in more
    detail in Section [7.5](#S7.SS5 "7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"),
    this signals that the ensemble members become more overconfident on predictions
    that are wrong, corroborated by the increasing expected calibration error with
    epoch seen in the bottom row of Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles
    ‣ 7 Numerical Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA
    ensembles"). This is consistent with the recent reports that fine-tuned LLMs often
    exhibit overconfidence (Jiang et al., [2021](#bib.bib37); Lin et al., [2022](#bib.bib41);
    Xiao et al., [2022b](#bib.bib74); He et al., [2023](#bib.bib26); Tian et al.,
    [2023](#bib.bib63); OpenAI et al., [2023](#bib.bib54)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 集成成员在几个周期后开始出现过拟合，如第二排面板中的验证损失所示，但有趣的是，在第一排面板中准确率没有显著下降。 如第[7.5](#S7.SS5 "7.5
    Dynamics of uncertainty metrics ‣ 7 Numerical Results ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")节中更详细地看到，这表明集成成员在错误预测上变得更过于自信，这一点通过图[1](#S7.F1
    "Figure 1 ‣ 7.2 LoRA Ensembles ‣ 7 Numerical Results ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")底部行中随着周期增加的期望校准误差得到证实。 这与最近的报告一致，即微调的LLM通常表现出过度自信（Jiang等，[2021](#bib.bib37)；Lin等，[2022](#bib.bib41)；Xiao等，[2022b](#bib.bib74)；He等，[2023](#bib.bib26)；Tian等，[2023](#bib.bib63)；OpenAI等，[2023](#bib.bib54)）。
- en: For fine-tuning on CQA seen in the first column, the overfitting is significantly
    reduced for ensembles compared to individual members, as can be seen by the gap
    between the loss curves for $M=1$ for later epochs. This means that ensemble members
    output more confident, but different predictions. This is a common feature of
    Bayesian posteriors (Blundell et al., [2015](#bib.bib6); Zhang et al., [2020](#bib.bib79);
    Kristiadi et al., [2020](#bib.bib38); Ober & Aitchison, [2021](#bib.bib53); Fortuin
    et al., [2022](#bib.bib19); Aitchison et al., [2021](#bib.bib2); Yang et al.,
    [2024](#bib.bib76)). This signals high epistemic uncertainty in this regime, indicating
    that the validation set is perceived as out-of-domain. There is no significant
    difference between single member and ensemble metrics for models trained on the
    MMLU datasets. We attribute this to the fact that the MMLU training datasets are
    small, see Appendix [A](#A1 "Appendix A Datasets ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles"), facilitating fast overfitting before
    any significant generalization is attained.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一列中看到的 CQA 细化，集成模型相比于单独成员显著减少了过拟合，这可以从后期 epoch 中 $M=1$ 的损失曲线差距看出。这意味着集成模型成员输出了更为自信但不同的预测结果。这是贝叶斯后验的一个共同特征（Blundell
    et al., [2015](#bib.bib6); Zhang et al., [2020](#bib.bib79); Kristiadi et al.,
    [2020](#bib.bib38); Ober & Aitchison, [2021](#bib.bib53); Fortuin et al., [2022](#bib.bib19);
    Aitchison et al., [2021](#bib.bib2); Yang et al., [2024](#bib.bib76)）。这表明在这种情况下存在较高的知识不确定性，表明验证集被视为域外。对于在
    MMLU 数据集上训练的模型，单个成员和集成模型的指标没有显著差异。我们将此归因于 MMLU 训练数据集较小，见附录 [A](#A1 "附录 A 数据集 ‣
    使用 LoRA 集成进行微调的 LLM 不确定性量化")，这导致在获得任何显著泛化之前，过拟合很快发生。
- en: '![Refer to caption](img/e8187d201d4997b5826477f7facb2b76.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8187d201d4997b5826477f7facb2b76.png)'
- en: 'Figure 2: AUROC computed for the CQA, MMLU STEM, and MMLU Social Studies multiple-choice
    QA datasets. In each subpanel, one dataset is used for fine-tuning (T) on the
    training subset and two datasets for the AUROC evaluation (V) on the validation
    subset. The LoRA ensemble size is $M=5$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：计算 CQA、MMLU STEM 和 MMLU 社会研究多项选择 QA 数据集的 AUROC。在每个子面板中，使用一个数据集进行训练子集上的微调
    (T)，使用两个数据集进行验证子集上的 AUROC 评估 (V)。LoRA 集成的大小为 $M=5$。
- en: 7.4 Uncertainty metrics visualized by AUROC
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 通过 AUROC 可视化的不确定性指标
- en: Area under receiver operator curve (AUROC) is commonly used as a metric for
    out-of-domain detection (Hendrycks & Gimpel, [2016](#bib.bib28)), but can also
    be used more generally to measure how well a model can separate two datasets.
    By evaluating AUROC using uncertainty measures, we can gain understanding of which
    of the datasets is more consistently perceived by the model as more confusing
    (using entropy) or as more out-of-domain (using mutual information).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线下面积（AUROC）通常用作域外检测的指标（Hendrycks & Gimpel, [2016](#bib.bib28)），但也可以更一般地用来衡量模型在区分两个数据集的能力。通过使用不确定性度量来评估
    AUROC，我们可以了解模型如何更一致地将某个数据集视为更混淆（使用熵）或更域外（使用互信息）。
- en: In Fig. [2](#S7.F2 "Figure 2 ‣ 7.3 Performance metrics ‣ 7 Numerical Results
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"), we show
    the AUROC results comparing in- and out-of-domain validation datasets. The ensembles
    were fine-tuned on the training subset of the CQA, MMLU STEM, and MMLU Social
    Studies datasets. For the AUROC evaluation, we use the validation subset of these
    datasets as in-domain samples, and the validation subset of other datasets as
    out-of-domain.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S7.F2 "图 2 ‣ 7.3 性能指标 ‣ 7 数值结果 ‣ 使用 LoRA 集成进行微调的 LLM 不确定性量化") 中，我们展示了比较域内和域外验证数据集的
    AUROC 结果。集成模型在 CQA、MMLU STEM 和 MMLU 社会研究数据集的训练子集上进行了微调。对于 AUROC 评估，我们使用这些数据集的验证子集作为域内样本，而其他数据集的验证子集作为域外样本。
- en: By analyzing the AUROC curves for the ensemble trained on CQA in the first column
    of Fig. [2](#S7.F2 "Figure 2 ‣ 7.3 Performance metrics ‣ 7 Numerical Results ‣
    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"), it is evident
    that the model consistently perceives samples from CQA as more in-domain compared
    to those from MMLU datasets, as indicated by an AUROC greater than 0.5\. This
    aligns with expectations. However, the AUROC is not close to 1, suggesting that
    a significant portion of the MMLU dataset is perceived similarly to CQA in terms
    of entropy and mutual information, with MMLU Social Sciences appearing closer
    than MMLU STEM as can be seen by the lower AUROC values. A somewhat similar pattern
    is observed for the ensemble trained on MMLU Social Sciences.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析图[2](#S7.F2 "Figure 2 ‣ 7.3 Performance metrics ‣ 7 Numerical Results ‣
    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")第一列中训练于CQA的集合的AUROC曲线，可以明显看出，与MMLU数据集中的样本相比，模型始终将CQA的样本视为更多在域内，这表现在AUROC值大于0.5。这与预期一致。然而，AUROC值并未接近1，这表明MMLU数据集中的一部分样本在熵和互信息方面被感知为类似于CQA，其中MMLU社会科学比MMLU
    STEM更接近，这从较低的AUROC值中可以看出。对于训练于MMLU社会科学的集合，也观察到了类似的模式。
- en: 'For the ensemble trained on MMLU STEM in the second column, a peculiar behavior
    is observed, where the AUROC is below 0.5\. We interpret this as follows: MMLU
    STEM poses greater complexity for the LLM architecture, hence, even with training,
    it struggles to identify the right features for correct answers. Conversely, the
    ensemble retains robust knowledge about features pertinent to MMLU SS and CQA.
    A mutual information score below 0.5 indicates that the ensemble perceives the
    validation set of MMLU STEM as more out-of-domain compared to the other datasets,
    even though the model was fine-tuned on the training subset of MMLU STEM.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二列中训练于MMLU STEM的集合，观察到一种特殊行为，即AUROC值低于0.5。我们对此的解释如下：MMLU STEM对LLM架构提出了更大的复杂性，因此即使在训练后，它也难以识别正确答案所需的特征。相反，该集合保留了关于MMLU
    SS和CQA相关特征的稳健知识。互信息得分低于0.5表明，该集合将MMLU STEM的验证集视为比其他数据集更为域外，即使模型在MMLU STEM的训练子集上进行了微调。
- en: 7.5 Dynamics of uncertainty metrics
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 不确定性度量的动态
- en: To analyze the trends observed through the AUROC curves in more detail, we also
    produce data-resolved dynamics of uncertainty measures by looking at two-dimensional
    histogram plots of mutual information and entropy(Linander et al., [2023](#bib.bib43)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地分析通过AUROC曲线观察到的趋势，我们还通过查看互信息和熵的二维直方图图表（Linander et al., [2023](#bib.bib43)）来生成数据分解的不确定性度量动态。
- en: In Fig. [3](#S7.F3 "Figure 3 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles"),
    we show the epoch evolution of uncertainty measures for a LoRA ensemble that was
    fine-tuned and evaluated on the CQA dataset. The mutual information (MI) on the
    vertical axis, and the predictive entropy (Entropy) on the horisontal axis, are
    binned over every sample of the validation set of CQA and the number of samples
    in each bin is indicated by the colorbar. We distinguish between incorrect and
    correct predictions to enhance interpretability. The ensemble attains minimal
    validation loss at epoch 3, as shown in Fig. [1](#S7.F1 "Figure 1 ‣ 7.2 LoRA Ensembles
    ‣ 7 Numerical Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA
    ensembles"). Epochs 1 and 6 exhibit underfitting and overfitting, respectively.
    The true posterior $p(\theta|D)$ should yield nearly zero mutual information when
    evaluated on the validation dataset, which should be in-domain by design. Samples
    with high mutual information indicate that parts of the target domain are perceived
    as out-of-domain by the model, showing the most important regions to consider
    for improving the training dataset targetting a better posterior approximation.
    We observe a moderate spread in entropy on the horizontal axis, indicating that
    the model predicts a broad distribution even though the sample is considered in-domain
    through their small mutual information. This spread can be attributed to the questions
    where the model does not have enough context or expressive power for a distinct
    answer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[3](#S7.F3 "Figure 3 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical Results
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")中，我们展示了对一个在CQA数据集上进行了微调和评估的LoRA集成模型的不确定性度量的历时演变。纵轴上的互信息（MI）和横轴上的预测熵（Entropy）在CQA验证集的每个样本上进行了分箱，每个箱中的样本数量通过颜色条表示。为了增强可解释性，我们区分了错误预测和正确预测。该集成模型在第3轮取得了最小的验证损失，如图[1](#S7.F1
    "Figure 1 ‣ 7.2 LoRA Ensembles ‣ 7 Numerical Results ‣ Uncertainty quantification
    in fine-tuned LLMs using LoRA ensembles")所示。第1轮和第6轮分别表现为欠拟合和过拟合。真实的后验$p(\theta|D)$在验证数据集上应该产生接近零的互信息，这个数据集应设计为领域内的。具有高互信息的样本表明模型将目标领域的部分区域视为领域外的，显示了改进训练数据集以获得更好后验近似所需关注的最重要区域。我们观察到横轴上的熵有中等的分布，这表明尽管样本通过其小的互信息被认为是领域内的，但模型仍预测了一个广泛的分布。这种分布可以归因于模型在处理没有足够上下文或表达能力不足的问题时的情况。
- en: Fig. [4](#S7.F4 "Figure 4 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical
    Results ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles")
    shows the uncertainty histograms for MMLU STEM and MMLU SS, calculated using an
    ensemble trained on CQA. Starting with MMLU STEM in the first and second row of
    panels, we draw two conclusions. First, out of the wrongly classified samples
    for the optimal epoch 3 shown in the second row, second column, there is a large
    fraction with high predictive entropy and low mutual information compared to the
    more moderate spread of predictive entropy in the corresponding panel for MMLU
    SS in the last row, second column. This affirms conclusions from Section [7.4](#S7.SS4
    "7.4 Uncertainty metrics visualized by AUROC ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles") regarding the relatively
    higher complexity of STEM compared to SS. The ensemble trained on CQA perceives
    the STEM samples equally in-domain as the SS samples, as evident by the similar
    mutual information, but predict a broader distribution indicating a lack of knowledge.
    Secondly, in the over-fitting regime corresponding to the last column, the mutual
    information of the wrongly classified samples of the second row increase more
    than the correctly classified samples in the first row. This indicates that the
    ensemble forgets the samples with high predictive entropy quicker, i.e. it forgets
    the confusing samples faster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S7.F4 "Figure 4 ‣ 7.5 Dynamics of uncertainty metrics ‣ 7 Numerical Results
    ‣ Uncertainty quantification in fine-tuned LLMs using LoRA ensembles") 展示了使用在
    CQA 上训练的集成模型计算的 MMLU STEM 和 MMLU SS 的不确定性直方图。从面板的第一行和第二行的 MMLU STEM 开始，我们得出两个结论。首先，对于第二行第二列中显示的最佳周期
    3 的错误分类样本，与最后一行第二列中的 MMLU SS 面板中更为温和的预测熵分布相比，有很大一部分样本具有高预测熵和低互信息。这证实了第 [7.4](#S7.SS4
    "7.4 Uncertainty metrics visualized by AUROC ‣ 7 Numerical Results ‣ Uncertainty
    quantification in fine-tuned LLMs using LoRA ensembles") 节中关于 STEM 相比于 SS 复杂度相对较高的结论。训练于
    CQA 的集成模型将 STEM 样本视为与 SS 样本同样的领域内样本，从相似的互信息可以看出，但预测了更广泛的分布，表明知识的缺乏。其次，在对应于最后一列的过拟合情况下，第二行的错误分类样本的互信息增加幅度大于第一行的正确分类样本。这表明该集成模型更快地遗忘具有高预测熵的样本，即更快地遗忘困惑样本。
- en: '![Refer to caption](img/8b0456062bf78f6fd4ee5d03997c52de.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b0456062bf78f6fd4ee5d03997c52de.png)'
- en: 'Figure 3: Histograms of predictive entropy and mutual information for a LoRA
    ensemble trained and evaluated on the CQA dataset. The ensemble consists of $M=5$,
    and an L2 LoRA loss of 1\. This figure illustrates the evolution of uncertainty
    measures for in-domain data across training epochs, differentiated by correct
    and incorrect predictions. We also depict the corresponding mean (red) and median
    (green) entropy and mutual information values.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于 CQA 数据集训练和评估的 LoRA 集成模型的预测熵和互信息的直方图。该集成模型由 $M=5$ 组成，L2 LoRA 损失为 1。这张图展示了在训练周期内针对领域内数据的不确定性度量的演变，按正确预测和错误预测进行区分。我们还描绘了对应的均值（红色）和中位数（绿色）熵和互信息值。
- en: '![Refer to caption](img/03c72e60eacf4b09cfce16238d8bb19e.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03c72e60eacf4b09cfce16238d8bb19e.png)'
- en: 'Figure 4: Histograms of predictive entropy (Entropy) and mutual information
    (MI) for a LoRA ensemble trained on CQA and evaluated on MMLU STEM and MMLU SS
    datasets. The ensemble consists of $M=5$, and an L2 LoRA loss of 1\. This panels
    illustrates the evolution (left to right) of uncertainty measures for out-of-training-domain
    data across training epochs, differentiated by correct and incorrect predictions.
    We also depict the corresponding mean (red) and median (green) entropy and mutual
    information values.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：用于 CQA 训练并在 MMLU STEM 和 MMLU SS 数据集上评估的 LoRA 集成模型的预测熵（熵）和互信息（MI）的直方图。该集成模型由
    $M=5$ 组成，L2 LoRA 损失为 1。这些面板展示了在训练周期内针对训练域外数据的不确定性度量的演变（从左到右），按正确预测和错误预测进行区分。我们还描绘了对应的均值（红色）和中位数（绿色）熵和互信息值。
- en: Continuing with MMLU SS in the third and fourth row, we observe a decrease in
    predictive entropy in the over-fitting regime, indicating that the ensemble does
    find features giving confident but wrong predictions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三行和第四行继续观察 MMLU SS，我们观察到在过拟合情况下预测熵的下降，这表明该集成模型确实找到了提供自信但错误预测的特征。
- en: 'Interestingly, the evolution of entropic quantities shows a quantitative difference:
    For MMLU STEM, there is a significant density of high-entropy, low-mutual-information
    samples at epoch 6, whereas for MMLU SS, the data with high entropy is more dispersed
    in terms of mutual information. This suggests that the fine-tuned model perceives
    confusing samples from MMLU STEM and MMLU SS differently, forgetting the confusing
    samples from MMLU SS more rapidly.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，熵量的演变显示了定量差异：对于 MMLU STEM，在第6个周期存在显著的高熵、低互信息样本密度，而对于 MMLU SS，高熵数据在互信息方面分布更广。这表明，微调后的模型对
    MMLU STEM 和 MMLU SS 中的混淆样本的感知有所不同，遗忘 MMLU SS 的混淆样本的速度更快。
- en: 8 Conclusions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We derived a principled, and practically efficient method for posterior approximation
    of fine-tuned LLMs using ensembles of low-rank adaptation models. By analysing
    the evolution of entropic uncertainty measures during fine-tuning, we identified
    signals of dataset complexity and signs of architecture limitations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推导了一种原理性且实际有效的方法，用于使用低秩适应模型的集成进行后验近似。通过分析在微调过程中熵不确定性度量的演变，我们识别出数据集复杂性的信号和架构局限性的迹象。
- en: The methods presented are generally applicable to different fine-tuning scenarios,
    and provide a systematic treatment of prediction uncertainty for fine-tuned LLMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的方法普遍适用于不同的微调场景，并提供了对微调 LLM 预测不确定性的系统性处理。
- en: 9 Acknowledgements
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: HL was supported by a grant from the KAW Foundation. The computations were enabled
    by resources provided by the National Academic Infrastructure for Supercomputing
    in Sweden (NAISS) and the Swedish National Infrastructure for Computing (SNIC)
    at C3SE partially funded by the Swedish Research Council through grant agreements
    no. 2022-06725 and no. 2018-05973. OB would like to thank Ihor Balabanov for valuable
    discussions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: HL 得到了 KAW 基金会的资助。计算由瑞典国家超级计算学术基础设施 (NAISS) 和瑞典国家计算基础设施 (SNIC) 提供的资源支持，C3SE
    部分由瑞典研究委员会通过资助协议编号 2022-06725 和 2018-05973 资助。OB 感谢 Ihor Balabanov 的宝贵讨论。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aghajanyan et al. (2020) Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrinsic
    dimensionality explains the effectiveness of language model fine-tuning, 2020.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aghajanyan 等 (2020) Aghajanyan, A., Zettlemoyer, L., 和 Gupta, S. 内在维度解释语言模型微调的有效性，2020。
- en: Aitchison et al. (2021) Aitchison, L., Yang, A., and Ober, S. W. Deep kernel
    processes. In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  130–140\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/aitchison21a.html](https://proceedings.mlr.press/v139/aitchison21a.html).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aitchison 等 (2021) Aitchison, L., Yang, A., 和 Ober, S. W. 深度核过程。在 Meila, M.
    和 Zhang, T. (编), *第38届国际机器学习会议论文集*，*机器学习研究论文集*第139卷，页130–140。PMLR，2021年7月18–24日。网址
    [https://proceedings.mlr.press/v139/aitchison21a.html](https://proceedings.mlr.press/v139/aitchison21a.html)。
- en: Balabanov & Linander (2024) Balabanov, O. and Linander, H. [github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2](github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2),
    2024.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balabanov & Linander (2024) Balabanov, O. 和 Linander, H. [github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2](github.com/oleksandr-balabanov/equivariant-posteriors/commit/38d5fb2817e43fa79cc8e4ddd3782fb4d7fb3ff2)，2024。
- en: Balabanov et al. (2023) Balabanov, O., Mehlig, B., and Linander, H. Bayesian
    posterior approximation with stochastic ensembles. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  13701–13711,
    June 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balabanov 等 (2023) Balabanov, O., Mehlig, B., 和 Linander, H. 基于随机集成的贝叶斯后验近似。在
    *IEEE/CVF计算机视觉与模式识别会议论文集 (CVPR)*，页13701–13711，2023年6月。
- en: Barber & Bishop (1998) Barber, D. and Bishop, C. M. Ensemble learning in bayesian
    neural networks. 1998.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barber & Bishop (1998) Barber, D. 和 Bishop, C. M. 贝叶斯神经网络中的集成学习。1998。
- en: Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
    D. Weight uncertainty in neural networks. In *Proceedings of the 32nd International
    Conference on International Conference on Machine Learning - Volume 37*, ICML’15,
    pp. 1613–1622\. JMLR.org, 2015.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell 等 (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., 和 Wierstra,
    D. 神经网络中的权重不确定性。在 *第32届国际机器学习大会 - 第37卷论文集*，ICML’15，页1613–1622。JMLR.org，2015。
- en: 'Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks
    of artificial general intelligence: Early experiments with gpt-4. *arXiv preprint
    arXiv:2303.12712*, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck等（2023）Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz,
    E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., 等。《人工通用智能的火花：对GPT-4的早期实验》。*arXiv预印本
    arXiv:2303.12712*，2023年。
- en: Buntine & Weigend (1991) Buntine, W. L. and Weigend, A. S. Bayesian back-propagation.
    *Complex Syst.*, 5, 1991.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buntine & Weigend（1991）Buntine, W. L. 和 Weigend, A. S.《贝叶斯反向传播》。*复杂系统*，第5卷，1991年。
- en: Chen & Mueller (2023) Chen, J. and Mueller, J. Quantifying uncertainty in answers
    from any language model and enhancing their trustworthiness, 2023.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈与穆勒（2023）陈俊和穆勒杰。《量化来自任何语言模型的答案的不确定性并提升其可信度》，2023年。
- en: Chen & Li (2024) Chen, W. and Li, Y. Calibrating transformers via sparse gaussian
    processes, 2024.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈与李（2024）陈伟和李燕。《通过稀疏高斯过程校准变压器》，2024年。
- en: Cinquin et al. (2021) Cinquin, T., Immer, A., Horn, M., and Fortuin, V. Pathologies
    in priors and inference for bayesian transformers, 2021.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cinquin等（2021）Cinquin, T., Immer, A., Horn, M., 和 Fortuin, V.《贝叶斯变压器中的先验和推断路径学》，2021年。
- en: Denker et al. (1987) Denker, J. S., Schwartz, D. B., Wittner, B. S., Solla,
    S. A., Howard, R. E., Jackel, L. D., and Hopfield, J. J. Large automatic learning,
    rule extraction, and generalization. *Complex Syst.*, 1, 1987.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denker等（1987）Denker, J. S., Schwartz, D. B., Wittner, B. S., Solla, S. A., Howard,
    R. E., Jackel, L. D., 和 Hopfield, J. J.《大规模自动学习、规则提取和泛化》。*复杂系统*，第1卷，1987年。
- en: 'Ding et al. (2022) Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y.,
    Hu, S., Chen, Y., Chan, C.-M., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z.,
    Zheng, H.-T., Chen, J., Liu, Y., Tang, J., Li, J., and Sun, M. Delta tuning: A
    comprehensive study of parameter efficient methods for pre-trained language models,
    2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丁等（2022）丁娜, 秦阳, 杨光, 魏锋, 杨泽, 苏阳, 胡顺, 陈远, 陈晨-梅, 陈伟, 易佳, 赵伟, 王晓, 刘哲, 郑华腾, 陈建, 刘杨,
    唐剑, 李杰, 和孙敏。《Delta 调优：预训练语言模型的参数高效方法的综合研究》，2022年。
- en: 'Ding et al. (2023) Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y.,
    Hu, S., Chen, Y., Chan, C.-M., Chen, W., Yi, J., Zhao, W., Wang, X., Liu, Z.,
    Zheng, H.-T., Chen, J., Liu, Y., Tang, J., Li, J., and Sun, M. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3):220–235, Mar 2023. ISSN 2522-5839. doi: 10.1038/s42256-023-00626-4. URL [https://doi.org/10.1038/s42256-023-00626-4](https://doi.org/10.1038/s42256-023-00626-4).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '丁等（2023）丁娜, 秦阳, 杨光, 魏锋, 杨泽, 苏阳, 胡顺, 陈远, 陈晨-梅, 陈伟, 易佳, 赵伟, 王晓, 刘哲, 郑华腾, 陈建,
    刘杨, 唐剑, 李杰, 和孙敏。《大规模预训练语言模型的参数高效微调》。*自然机器智能*，第5卷第3期：220–235，2023年3月。ISSN 2522-5839。doi:
    10.1038/s42256-023-00626-4。网址 [https://doi.org/10.1038/s42256-023-00626-4](https://doi.org/10.1038/s42256-023-00626-4)。'
- en: 'Duan et al. (2023) Duan, J., Cheng, H., Wang, S., Zavalny, A., Wang, C., Xu,
    R., Kailkhura, B., and Xu, K. Shifting attention to relevance: Towards the uncertainty
    estimation of large language models, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan等（2023）Duan, J., Cheng, H., Wang, S., Zavalny, A., Wang, C., Xu, R., Kailkhura,
    B., 和 Xu, K.《将注意力转向相关性：面向大型语言模型的不确定性估计》，2023年。
- en: 'Dwaracherla et al. (2022) Dwaracherla, V., Wen, Z., Osband, I., Lu, X., Asghari,
    S. M., and Van Roy, B. Ensembles for Uncertainty Estimation: Benefits of Prior
    Functions and Bootstrapping. *arXiv e-prints*, art. arXiv:2206.03633, June 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwaracherla等（2022）Dwaracherla, V., Wen, Z., Osband, I., Lu, X., Asghari, S.
    M., 和 Van Roy, B.《不确定性估计的集成：先验函数和自助法的好处》。*arXiv e-prints*，文章编号arXiv:2206.03633，2022年6月。
- en: Fan et al. (2020) Fan, X., Zhang, S., Chen, B., and Zhou, M. Bayesian attention
    modules. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
    (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp.  16362–16376\.
    Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等（2020）Fan, X., Zhang, S., Chen, B., 和 Zhou, M.《贝叶斯注意力模块》。见Larochelle, H.,
    Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H.（编辑），*神经信息处理系统进展*，第33卷，第16362–16376页。Curran
    Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/bcff3f632fd16ff099a49c2f0932b47a-Paper.pdf)。
- en: 'Fort et al. (2019) Fort, S., Hu, H., and Lakshminarayanan, B. Deep Ensembles:
    A Loss Landscape Perspective. *arXiv e-prints*, art. arXiv:1912.02757, December
    2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fort等（2019）Fort, S., Hu, H., 和 Lakshminarayanan, B.《深度集成：一种损失景观视角》。*arXiv e-prints*，文章编号arXiv:1912.02757，2019年12月。
- en: Fortuin et al. (2022) Fortuin, V., Garriga-Alonso, A., Ober, S. W., Wenzel,
    F., Rätsch, G., Turner, R. E., van der Wilk, M., and Aitchison, L. Bayesian neural
    network priors revisited, 2022.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fortuin 等人（2022）Fortuin, V., Garriga-Alonso, A., Ober, S. W., Wenzel, F., Rätsch,
    G., Turner, R. E., van der Wilk, M., 和 Aitchison, L. 重新审视贝叶斯神经网络先验，2022。
- en: Gal & Ghahramani (2015) Gal, Y. and Ghahramani, Z. Bayesian Convolutional Neural
    Networks with Bernoulli Approximate Variational Inference. *arXiv e-prints*, art.
    arXiv:1506.02158, June 2015.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 和 Ghahramani（2015）Gal, Y. 和 Ghahramani, Z. 贝叶斯卷积神经网络与伯努利近似变分推断。*arXiv 电子版*，文章编号
    arXiv:1506.02158，2015年6月。
- en: 'Gal & Ghahramani (2016) Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning. In Balcan, M. F. and Weinberger,
    K. Q. (eds.), *Proceedings of The 33rd International Conference on Machine Learning*,
    volume 48 of *Proceedings of Machine Learning Research*, pp.  1050–1059, New York,
    New York, USA, 20–22 Jun 2016\. PMLR. URL [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 和 Ghahramani（2016）Gal, Y. 和 Ghahramani, Z. Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性。在
    Balcan, M. F. 和 Weinberger, K. Q.（编辑），*第33届国际机器学习大会论文集*，*机器学习研究论文集* 第48卷，第 1050–1059
    页，美国纽约，2016年6月20–22日。PMLR。网址 [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html)。
- en: Gawlikowski et al. (2021) Gawlikowski, J., Rovile Njieutcheu Tassi, C., Ali,
    M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R.,
    Shahzad, M., Yang, W., Bamler, R., and Zhu, X. X. A Survey of Uncertainty in Deep
    Neural Networks. *arXiv e-prints*, art. arXiv:2107.03342, July 2021.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski 等人（2021）Gawlikowski, J., Rovile Njieutcheu Tassi, C., Ali, M., Lee,
    J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., Shahzad,
    M., Yang, W., Bamler, R., 和 Zhu, X. X. 深度神经网络中的不确定性调查。*arXiv 电子版*，文章编号 arXiv:2107.03342，2021年7月。
- en: Gleave & Irving (2022) Gleave, A. and Irving, G. Uncertainty estimation for
    language reward models, 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gleave 和 Irving（2022）Gleave, A. 和 Irving, G. 语言奖励模型的不确定性估计，2022。
- en: Graves (2011) Graves, A. Practical variational inference for neural networks.
    In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K.
    (eds.), *Advances in Neural Information Processing Systems*, volume 24\. Curran
    Associates, Inc., 2011. URL [https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves（2011）Graves, A. 神经网络的实用变分推断。在 Shawe-Taylor, J., Zemel, R., Bartlett,
    P., Pereira, F., 和 Weinberger, K.（编辑），*神经信息处理系统进展*，第24卷。Curran Associates, Inc.,
    2011。网址 [https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)。
- en: Gustafsson et al. (2019) Gustafsson, F. K., Danelljan, M., and Schön, T. B.
    Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision.
    *arXiv e-prints*, art. arXiv:1906.01620, June 2019.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gustafsson 等人（2019）Gustafsson, F. K., Danelljan, M., 和 Schön, T. B. 评估可扩展贝叶斯深度学习方法在稳健计算机视觉中的应用。*arXiv
    电子版*，文章编号 arXiv:1906.01620，2019年6月。
- en: He et al. (2023) He, G., Chen, J., and Zhu, J. Preserving pre-trained features
    helps calibrate fine-tuned language models, 2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2023）He, G., Chen, J., 和 Zhu, J. 保留预训练特征有助于校准微调语言模型，2023。
- en: 'He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification, 2015.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2015）He, K., Zhang, X., Ren, S., 和 Sun, J. 深入探讨修正器：在 imagenet 分类上超越人类水平，2015。
- en: Hendrycks & Gimpel (2016) Hendrycks, D. and Gimpel, K. A baseline for detecting
    misclassified and out-of-distribution examples in neural networks. *arXiv preprint
    arXiv:1610.02136*, 2016.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 和 Gimpel（2016）Hendrycks, D. 和 Gimpel, K. 检测神经网络中的错误分类和分布外示例的基线。*arXiv
    预印本 arXiv:1610.02136*，2016。
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    *Proceedings of the International Conference on Learning Representations (ICLR)*,
    2021.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解。*国际学习表征大会论文集（ICLR）*，2021。
- en: Hoffmann & Elster (2021) Hoffmann, L. and Elster, C. Deep Ensembles from a Bayesian
    Perspective. *arXiv e-prints*, art. arXiv:2105.13283, May 2021.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 和 Elster（2021）Hoffmann, L. 和 Elster, C. 从贝叶斯视角看深度集成。*arXiv 电子版*，文章编号
    arXiv:2105.13283，2021年5月。
- en: Hou et al. (2023) Hou, B., Liu, Y., Qian, K., Andreas, J., Chang, S., and Zhang,
    Y. Decomposing uncertainty for large language models through input clarification
    ensembling, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等人（2023）Hou, B., Liu, Y., Qian, K., Andreas, J., Chang, S., 和 Zhang, Y.
    通过输入澄清集成分解大语言模型的不确定性，2023。
- en: Houlsby et al. (2011) Houlsby, N., Huszar, F., Ghahramani, Z., and Lengyel,
    M. Bayesian active learning for classification and preference learning. *CoRR*,
    abs/1112.5745, 2011. URL [http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745](http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2011) Houlsby, N., Huszar, F., Ghahramani, Z., 和 Lengyel, M.
    用于分类和偏好学习的贝叶斯主动学习。*CoRR*，abs/1112.5745，2011。网址 [http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745](http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5745)。
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), *Proceedings
    of the 36th International Conference on Machine Learning*, volume 97 of *Proceedings
    of Machine Learning Research*, pp.  2790–2799\. PMLR, 09–15 Jun 2019. URL [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., 和 Gelly, S. 参数高效的 NLP 迁移学习。In
    Chaudhuri, K. 和 Salakhutdinov, R. (编)，*第36届国际机器学习会议论文集*，第97卷，*机器学习研究论文集*，第2790–2799页。PMLR，2019年6月9–15日。网址
    [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html)。
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *ICL2022*, 2021.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., 和 Chen, W. Lora：大规模语言模型的低秩适配。*ICL2022*，2021。
- en: 'Huang et al. (2023) Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu,
    F., and Ma, L. Look Before You Leap: An Exploratory Study of Uncertainty Measurement
    for Large Language Models, October 2023. URL [http://arxiv.org/abs/2307.10236](http://arxiv.org/abs/2307.10236).
    arXiv:2307.10236 [cs].'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Huang, Y., Song, J., Wang, Z., Zhao, S., Chen, H., Juefei-Xu,
    F., 和 Ma, L. 量入为出：对大型语言模型的不确定性测量的探索性研究，2023年10月。网址 [http://arxiv.org/abs/2307.10236](http://arxiv.org/abs/2307.10236)。arXiv:2307.10236
    [cs]。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., 等。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023。
- en: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can
    we know when language models know? on the calibration of language models for question
    answering. *Transactions of the Association for Computational Linguistics*, 9:962–977,
    2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., 和 Neubig, G. 我们如何知道语言模型是否真正理解了？关于语言模型在问答任务中的校准。*计算语言学协会会刊*，9:962–977，2021。doi:
    10.1162/tacl˙a˙00407。网址 [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57)。'
- en: Kristiadi et al. (2020) Kristiadi, A., Hein, M., and Hennig, P. Being bayesian,
    even just a bit, fixes overconfidence in relu networks, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kristiadi et al. (2020) Kristiadi, A., Hein, M., 和 Hennig, P. 即使仅仅是一些贝叶斯特性，也能修正
    ReLU 网络中的过度自信，2020。
- en: 'Kuhn et al. (2023) Kuhn, L., Gal, Y., and Farquhar, S. Semantic Uncertainty:
    Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,
    April 2023. URL [http://arxiv.org/abs/2302.09664](http://arxiv.org/abs/2302.09664).
    arXiv:2302.09664 [cs].'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn et al. (2023) Kuhn, L., Gal, Y., 和 Farquhar, S. 语义不确定性：自然语言生成中不确定性估计的语言不变性，2023年4月。网址
    [http://arxiv.org/abs/2302.09664](http://arxiv.org/abs/2302.09664)。arXiv:2302.09664
    [cs]。
- en: Li et al. (2018) Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the
    intrinsic dimension of objective landscapes, 2018.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018) Li, C., Farkhoor, H., Liu, R., 和 Yosinski, J. 测量目标景观的内在维度，2018。
- en: 'Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. Teaching Models to Express
    Their Uncertainty in Words. *arXiv e-prints*, art. arXiv:2205.14334, May 2022.
    doi: 10.48550/arXiv.2205.14334.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2022) Lin, S., Hilton, J., 和 Evans, O. 教会模型用语言表达它们的不确定性。*arXiv
    e-prints*，文章 arXiv:2205.14334，2022年5月。doi: 10.48550/arXiv.2205.14334。'
- en: 'Lin et al. (2023) Lin, Z., Trivedi, S., and Sun, J. Generating with Confidence:
    Uncertainty Quantification for Black-box Large Language Models. *arXiv e-prints*,
    art. arXiv:2305.19187, May 2023. doi: 10.48550/arXiv.2305.19187.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Lin, Z., Trivedi, S., 和 Sun, J. 用信心生成：对黑箱大型语言模型的不确定性量化。*arXiv
    e-prints*，文章 arXiv:2305.19187，2023年5月。doi: 10.48550/arXiv.2305.19187。'
- en: 'Linander et al. (2023) Linander, H., Balabanov, O., Yang, H., and Mehlig, B.
    Looking at the posterior: accuracy and uncertainty of neural-network predictions.
    *Machine Learning: Science and Technology*, 4(4):045032, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linander等（2023）Linander, H., Balabanov, O., Yang, H., 和 Mehlig, B. 观察后验：神经网络预测的准确性和不确定性。*机器学习：科学与技术*，4(4):045032，2023年。
- en: Liu et al. (2022) Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,
    M., and Raffel, C. Few-shot parameter-efficient fine-tuning is better and cheaper
    than in-context learning, 2022.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2022）Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., 和
    Raffel, C. 少量样本的参数高效微调比上下文学习更好且更便宜，2022年。
- en: 'Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized
    bert pretraining approach, 2019.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2019）Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
    Lewis, M., Zettlemoyer, L., 和 Stoyanov, V. Roberta：一种稳健优化的BERT预训练方法，2019年。
- en: MacKay (1991) MacKay, D. Bayesian model comparison and backprop nets. In Moody,
    J., Hanson, S., and Lippmann, R. (eds.), *Advances in Neural Information Processing
    Systems*, volume 4\. Morgan-Kaufmann, 1991. URL [https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf](https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacKay（1991）MacKay, D. 贝叶斯模型比较和反向传播网络。在Moody, J., Hanson, S., 和 Lippmann, R.（编），*神经信息处理系统进展*，第4卷。Morgan-Kaufmann，1991年。网址
    [https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf](https://proceedings.neurips.cc/paper/1991/file/c3c59e5f8b3e9753913f4d435b53c308-Paper.pdf)。
- en: Mackay (1992) Mackay, D. J. C. Information-based objective functions for active
    data selection. *Neural Computation*, 4(2):550–604, 1992.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mackay（1992）Mackay, D. J. C. 基于信息的目标函数用于主动数据选择。*神经计算*，4(2):550–604，1992年。
- en: Malinin & Gales (2018) Malinin, A. and Gales, M. Predictive uncertainty estimation
    via prior networks, 2018.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin & Gales（2018）Malinin, A. 和 Gales, M. 通过先验网络进行预测不确定性估计，2018年。
- en: Malinin & Gales (2021) Malinin, A. and Gales, M. Uncertainty Estimation in Autoregressive
    Structured Prediction, February 2021. URL [http://arxiv.org/abs/2002.07650](http://arxiv.org/abs/2002.07650).
    arXiv:2002.07650 [cs, stat].
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin & Gales（2021）Malinin, A. 和 Gales, M. 自回归结构预测中的不确定性估计，2021年2月。网址 [http://arxiv.org/abs/2002.07650](http://arxiv.org/abs/2002.07650)。arXiv:2002.07650
    [cs, stat]。
- en: Malinin et al. (2021) Malinin, A., Prokhorenkova, L., and Ustimenko, A. Uncertainty
    in gradient boosting via ensembles, 2021.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malinin等（2021）Malinin, A., Prokhorenkova, L., 和 Ustimenko, A. 通过集成的不确定性提升，2021年。
- en: 'Mangrulkar et al. (2022) Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y.,
    Paul, S., and Bossan, B. Peft: State-of-the-art parameter-efficient fine-tuning
    methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangrulkar等（2022）Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S.,
    和 Bossan, B. Peft：最先进的参数高效微调方法。网址 [https://github.com/huggingface/peft](https://github.com/huggingface/peft)，2022年。
- en: 'Mielke et al. (2022) Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y.-L.
    Reducing conversational agents’ overconfidence through linguistic calibration.
    *Transactions of the Association for Computational Linguistics*, 10:857–872, 2022.
    doi: 10.1162/tacl˙a˙00494. URL [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mielke等（2022）Mielke, S. J., Szlam, A., Dinan, E., 和 Boureau, Y.-L. 通过语言校准减少对话代理的过度自信。*计算语言学协会会刊*，10:857–872，2022年。doi:
    10.1162/tacl˙a˙00494。网址 [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50)。'
- en: Ober & Aitchison (2021) Ober, S. W. and Aitchison, L. Global inducing point
    variational posteriors for bayesian neural networks and deep gaussian processes.
    In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International Conference
    on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*,
    pp.  8248–8259\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/ober21a.html](https://proceedings.mlr.press/v139/ober21a.html).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ober & Aitchison（2021）Ober, S. W. 和 Aitchison, L. 贝叶斯神经网络和深度高斯过程的全局诱导点变分后验。在Meila,
    M. 和 Zhang, T.（编），*第38届国际机器学习会议论文集*，*机器学习研究论文集*第139卷，第8248–8259页。PMLR，2021年7月18-24日。网址
    [https://proceedings.mlr.press/v139/ober21a.html](https://proceedings.mlr.press/v139/ober21a.html)。
- en: OpenAI et al. (2023) OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
    Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat,
    S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H.,
    Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner,
    C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks,
    T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,
    C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen,
    R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D.,
    Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar,
    A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T.,
    Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao,
    L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R.,
    Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y.,
    Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C.,
    Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X.,
    Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin,
    D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider,
    I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner,
    H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis,
    A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung,
    J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe,
    R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y.,
    Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan,
    P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin,
    P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély,
    D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang,
    L., O’Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,
    G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A.,
    de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny,
    Pokrass, M., Pong, V., Powell, T., Power, A., Power, B., Proehl, E., Puri, R.,
    Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C.,
    Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S.,
    Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K.,
    Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens,
    M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N.,
    Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet,
    P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.
    F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang,
    A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng,
    J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman,
    L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba,
    W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk,
    W., and Zoph, B. Gpt-4 technical report, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2023）OpenAI，:，Achiam，J.，Adler，S.，Agarwal，S.，Ahmad，L.，Akkaya，I.，Aleman，F.
    L.，Almeida，D.，Altenschmidt，J.，Altman，S.，Anadkat，S.，Avila，R.，Babuschkin，I.，Balaji，S.，Balcom，V.，Baltescu，P.，Bao，H.，Bavarian，M.，Belgum，J.，Bello，I.，Berdine，J.，Bernadett-Shapiro，G.，Berner，C.，Bogdonoff，L.，Boiko，O.，Boyd，M.，Brakman，A.-L.，Brockman，G.，Brooks，T.，Brundage，M.，Button，K.，Cai，T.，Campbell，R.，Cann，A.，Carey，B.，Carlson，C.，Carmichael，R.，Chan，B.，Chang，C.，Chantzis，F.，Chen，D.，Chen，S.，Chen，R.，Chen，J.，Chen，M.，Chess，B.，Cho，C.，Chu，C.，Chung，H.
    W.，Cummings，D.，Currier，J.，Dai，Y.，Decareaux，C.，Degry，T.，Deutsch，N.，Deville，D.，Dhar，A.，Dohan，D.，Dowling，S.，Dunning，S.，Ecoffet，A.，Eleti，A.，Eloundou，T.，Farhi，D.，Fedus，L.，Felix，N.，Fishman，S.
    P.，Forte，J.，Fulford，I.，Gao，L.，Georges，E.，Gibson，C.，Goel，V.，Gogineni，T.，Goh，G.，Gontijo-Lopes，R.，Gordon，J.，Grafstein，M.，Gray，S.，Greene，R.，Gross，J.，Gu，S.
    S.，Guo，Y.，Hallacy，C.，Han，J.，Harris，J.，He，Y.，Heaton，M.，Heidecke，J.，Hesse，C.，Hickey，A.，Hickey，W.，Hoeschele，P.，Houghton，B.，Hsu，K.，Hu，S.，Hu，X.，Huizinga，J.，Jain，S.，Jain，S.，Jang，J.，Jiang，A.，Jiang，R.，Jin，H.，Jin，D.，Jomoto，S.，Jonn，B.，Jun，H.，Kaftan，T.，Łukasz
    Kaiser，Kamali，A.，Kanitscheider，I.，Keskar，N. S.，Khan，T.，Kilpatrick，L.，Kim，J. W.，Kim，C.，Kim，Y.，Kirchner，H.，Kiros，J.，Knight，M.，Kokotajlo，D.，Łukasz
    Kondraciuk，Kondrich，A.，Konstantinidis，A.，Kosic，K.，Krueger，G.，Kuo，V.，Lampe，M.，Lan，I.，Lee，T.，Leike，J.，Leung，J.，Levy，D.，Li，C.
    M.，Lim，R.，Lin，M.，Lin，S.，Litwin，M.，Lopez，T.，Lowe，R.，Lue，P.，Makanju，A.，Malfacini，K.，Manning，S.，Markov，T.，Markovski，Y.，Martin，B.，Mayer，K.，Mayne，A.，McGrew，B.，McKinney，S.
    M.，McLeavey，C.，McMillan，P.，McNeil，J.，Medina，D.，Mehta，A.，Menick，J.，Metz，L.，Mishchenko，A.，Mishkin，P.，Monaco，V.，Morikawa，E.，Mossing，D.，Mu，T.，Murati，M.，Murk，O.，Mély，D.，Nair，A.，Nakano，R.，Nayak，R.，Neelakantan，A.，Ngo，R.，Noh，H.，Ouyang，L.，O’Keefe，C.，Pachocki，J.，Paino，A.，Palermo，J.，Pantuliano，A.，Parascandolo，G.，Parish，J.，Parparita，E.，Passos，A.，Pavlov，M.，Peng，A.，Perelman，A.，de
    Avila Belbute Peres，F.，Petrov，M.，de Oliveira Pinto，H. P.，Michael，Pokorny，Pokrass，M.，Pong，V.，Powell，T.，Power，A.，Power，B.，Proehl，E.，Puri，R.，Radford，A.，Rae，J.，Ramesh，A.，Raymond，C.，Real，F.，Rimbach，K.，Ross，C.，Rotsted，B.，Roussez，H.，Ryder，N.，Saltarelli，M.，Sanders，T.，Santurkar，S.，Sastry，G.，Schmidt，H.，Schnurr，D.，Schulman，J.，Selsam，D.，Sheppard，K.，Sherbakov，T.，Shieh，J.，Shoker，S.，Shyam，P.，Sidor，S.，Sigler，E.，Simens，M.，Sitkin，J.，Slama，K.，Sohl，I.，Sokolowsky，B.，Song，Y.，Staudacher，N.，Such，F.
    P.，Summers，N.，Sutskever，I.，Tang，J.，Tezak，N.，Thompson，M.，Tillet，P.，Tootoonchian，A.，Tseng，E.，Tuggle，P.，Turley，N.，Tworek，J.，Uribe，J.
    F. C.，Vallone，A.，Vijayvergiya，A.，Voss，C.，Wainwright，C.，Wang，J. J.，Wang，A.，Wang，B.，Ward，J.，Wei，J.，Weinmann，C.，Welihinda，A.，Welinder，P.，Weng，J.，Weng，L.，Wiethoff，M.，Willner，D.，Winter，C.，Wolrich，S.，Wong，H.，Workman，L.，Wu，S.，Wu，J.，Wu，M.，Xiao，K.，Xu，T.，Yoo，S.，Yu，K.，Yuan，Q.，Zaremba，W.，Zellers，R.，Zhang，C.，Zhang，M.，Zhao，S.，Zheng，T.，Zhuang，J.，Zhuk，W.，和
    Zoph，B. Gpt-4 技术报告，2023。
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in Neural Information
    Processing Systems*, 35:27730–27744, 2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人（2022）欧阳磊、吴俊、姜雪、阿尔梅达、温特赖特、米什金、张陈、阿戈瓦尔、斯拉马、雷亚等。训练语言模型以跟随人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022。
- en: Ovadia et al. (2019a) Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,
    Nowozin, S., Dillon, J., Lakshminarayanan, B., and Snoek, J. Can you trust your
    model's uncertainty? evaluating predictive uncertainty under dataset shift. In
    Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett,
    R. (eds.), *Advances in Neural Information Processing Systems*, volume 32\. Curran
    Associates, Inc., 2019a. URL [https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧瓦迪亚等人（2019a）欧瓦迪亚、费尔蒂格、任劲、纳多、斯库利、诺沃津、迪龙、拉克什米纳拉扬、斯诺克。你能信任模型的不确定性吗？在数据集偏移下评估预测不确定性。见于
    Wallach、拉罗谢尔、贝耶尔齐默、达尔切-布克、福克斯和加内特（编），*神经信息处理系统进展*，第32卷。Curran Associates, Inc.，2019a。网址
    [https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf)。
- en: Ovadia et al. (2019b) Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,
    Nowozin, S., Dillon, J. V., Lakshminarayanan, B., and Snoek, J. Can you trust
    your model’s uncertainty? evaluating predictive uncertainty under dataset shift,
    2019b.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧瓦迪亚等人（2019b）欧瓦迪亚、费尔蒂格、任劲、纳多、斯库利、诺沃津、迪龙、拉克什米纳拉扬、斯诺克。你能信任模型的不确定性吗？在数据集偏移下评估预测不确定性，2019b。
- en: Peng et al. (2023) Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction
    tuning with gpt-4. *arXiv preprint arXiv:2304.03277*, 2023.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等人（2023）彭博、李辰、何鹏、加利、高翔。使用 GPT-4 的指令调优。*arXiv 预印本 arXiv:2304.03277*，2023。
- en: Ren et al. (2023) Ren, J., Luo, J., Zhao, Y., Krishna, K., Saleh, M., Lakshminarayanan,
    B., and Liu, P. J. Out-of-Distribution Detection and Selective Generation for
    Conditional Language Models, March 2023. URL [http://arxiv.org/abs/2209.15558](http://arxiv.org/abs/2209.15558).
    arXiv:2209.15558 [cs].
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任劲等人（2023）任劲、罗剑、赵云、克里希纳、赛勒、拉克什米纳拉扬和刘鹏杰。用于条件语言模型的分布外检测和选择生成，2023年3月。网址 [http://arxiv.org/abs/2209.15558](http://arxiv.org/abs/2209.15558)。arXiv:2209.15558
    [cs]。
- en: 'Shi & Lipani (2024) Shi, Z. and Lipani, A. Dept: Decomposed prompt tuning for
    parameter-efficient fine-tuning, 2024.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 施志和利帕尼（2024）施志和利帕尼。Dept：用于参数高效微调的分解提示调优，2024。
- en: Sun et al. (2022) Sun, M., Yan, W., Abbeel, P., and Mordatch, I. Quantifying
    uncertainty in foundation models via ensembles. In *NeurIPS 2022 Workshop on Robustness
    in Sequence Modeling*, 2022. URL [https://openreview.net/forum?id=LpBlkATV24M](https://openreview.net/forum?id=LpBlkATV24M).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孙敏等人（2022）孙敏、阎伟、阿贝尔、莫达奇。通过集成量化基础模型中的不确定性。见于*NeurIPS 2022 关于序列建模鲁棒性的研讨会*，2022。网址
    [https://openreview.net/forum?id=LpBlkATV24M](https://openreview.net/forum?id=LpBlkATV24M)。
- en: 'Talmor et al. (2019) Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA:
    A question answering challenge targeting commonsense knowledge. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    4149–4158, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1421. URL [https://aclanthology.org/N19-1421](https://aclanthology.org/N19-1421).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '塔尔莫尔等人（2019）塔尔莫尔、赫尔齐、洛里、贝兰特。CommonsenseQA：一个针对常识知识的问题回答挑战。见于*2019年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，第4149–4158页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。doi:
    10.18653/v1/N19-1421。网址 [https://aclanthology.org/N19-1421](https://aclanthology.org/N19-1421)。'
- en: 'Tian et al. (2023) Tian, K., Mitchell, E., Zhou, A., Sharma, A., Rafailov,
    R., Yao, H., Finn, C., and Manning, C. D. Just ask for calibration: Strategies
    for eliciting calibrated confidence scores from language models fine-tuned with
    human feedback, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 田凯等人（2023）田凯、米切尔、周昂、夏尔玛、拉法伊洛夫、姚辉、芬恩、曼宁。只需请求校准：从经过人类反馈微调的语言模型中引出校准的置信度分数的策略，2023。
- en: 'Tishby et al. (1989) Tishby, Levin, and Solla. Consistent inference of probabilities
    in layered networks: predictions and generalizations. In *International 1989 Joint
    Conference on Neural Networks*, pp.  403–409 vol.2, 1989. doi: 10.1109/IJCNN.1989.118274.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tishby等人（1989）Tishby, Levin, 和 Solla. 层叠网络中概率的一致推断：预测和泛化。在*《国际1989年神经网络联合会议》*，第403–409页，第2卷，1989。doi:
    10.1109/IJCNN.1989.118274。'
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023a）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等. Llama：开放和高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023b）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等. Llama 2：开放基础和微调聊天模型。*arXiv预印本
    arXiv:2307.09288*，2023b。
- en: 'Tran et al. (2019) Tran, D., Dusenberry, M., van der Wilk, M., and Hafner,
    D. Bayesian layers: A module for neural network uncertainty. In Wallach, H., Larochelle,
    H., Beygelzimer, A., d''Alché-Buc, F., Fox, E., and Garnett, R. (eds.), *Advances
    in Neural Information Processing Systems*, volume 32\. Curran Associates, Inc.,
    2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等人（2019）Tran, D., Dusenberry, M., van der Wilk, M., 和 Hafner, D. 贝叶斯层：一种用于神经网络不确定性的模块。
    在 Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., 和 Garnett,
    R.（编辑），*《神经信息处理系统进展》*，第32卷。Curran Associates, Inc.，2019。URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf)。
- en: 'Tran et al. (2022) Tran, D., Liu, J., Dusenberry, M. W., Phan, D., Collier,
    M., Ren, J., Han, K., Wang, Z., Mariet, Z., Hu, H., Band, N., Rudner, T. G. J.,
    Singhal, K., Nado, Z., van Amersfoort, J., Kirsch, A., Jenatton, R., Thain, N.,
    Yuan, H., Buchanan, K., Murphy, K., Sculley, D., Gal, Y., Ghahramani, Z., Snoek,
    J., and Lakshminarayanan, B. Plex: Towards reliability using pretrained large
    model extensions, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等人（2022）Tran, D., Liu, J., Dusenberry, M. W., Phan, D., Collier, M., Ren,
    J., Han, K., Wang, Z., Mariet, Z., Hu, H., Band, N., Rudner, T. G. J., Singhal,
    K., Nado, Z., van Amersfoort, J., Kirsch, A., Jenatton, R., Thain, N., Yuan, H.,
    Buchanan, K., Murphy, K., Sculley, D., Gal, Y., Ghahramani, Z., Snoek, J., 和 Lakshminarayanan,
    B. Plex：利用预训练的大型模型扩展实现可靠性，2022。
- en: Wang et al. (2023) Wang, X., Aitchison, L., and Rudolph, M. Lora ensembles for
    large language model fine-tuning, 2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023）Wang, X., Aitchison, L., 和 Rudolph, M. Lora集成用于大语言模型微调，2023。
- en: 'Wen et al. (2020) Wen, Y., Tran, D., and Ba, J. Batchensemble: An alternative
    approach to efficient ensemble and lifelong learning, 2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen等人（2020）Wen, Y., Tran, D., 和 Ba, J. Batchensemble：一种高效集成和终身学习的替代方法，2020。
- en: Wilson & Izmailov (2020) Wilson, A. G. and Izmailov, P. Bayesian deep learning
    and a probabilistic perspective of generalization. *Advances in neural information
    processing systems*, 33:4697–4708, 2020.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson & Izmailov（2020）Wilson, A. G. 和 Izmailov, P. 贝叶斯深度学习和关于泛化的概率视角。*《神经信息处理系统进展》*，33:4697–4708，2020。
- en: 'Wimmer et al. (2023) Wimmer, L., Sale, Y., Hofman, P., Bischl, B., and Hüllermeier,
    E. Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional
    entropy and mutual information appropriate measures? In *Uncertainty in Artificial
    Intelligence*, pp.  2282–2292. PMLR, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wimmer等人（2023）Wimmer, L., Sale, Y., Hofman, P., Bischl, B., 和 Hüllermeier, E.
    在机器学习中量化随机性和认知不确定性：条件熵和互信息是否是合适的测量方法？在*《人工智能中的不确定性》*中，第2282–2292页。PMLR，2023。
- en: 'Xiao et al. (2022a) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., and Morency, L.-P. Uncertainty quantification with pre-trained language models:
    A large-scale empirical analysis. In Goldberg, Y., Kozareva, Z., and Zhang, Y.
    (eds.), *Findings of the Association for Computational Linguistics: EMNLP 2022*,
    pp. 7273–7284, Abu Dhabi, United Arab Emirates, December 2022a. Association for
    Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.538. URL [https://aclanthology.org/2022.findings-emnlp.538](https://aclanthology.org/2022.findings-emnlp.538).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2022a) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., and Morency, L.-P. 使用预训练语言模型进行不确定性量化：大规模实证分析。在 Goldberg, Y., Kozareva, Z.,
    和 Zhang, Y.（编辑），*计算语言学协会会议论文集：EMNLP 2022*，第 7273–7284 页，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。doi:
    10.18653/v1/2022.findings-emnlp.538。网址 [https://aclanthology.org/2022.findings-emnlp.538](https://aclanthology.org/2022.findings-emnlp.538)。'
- en: 'Xiao et al. (2022b) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., and Morency, L.-P. Uncertainty quantification with pre-trained language models:
    A large-scale empirical analysis, 2022b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2022b) Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov,
    R., 和 Morency, L.-P. 使用预训练语言模型进行不确定性量化：大规模实证分析，2022b。
- en: 'Xue et al. (2021) Xue, B., Yu, J., Xu, J., Liu, S., Hu, S., Ye, Z., Geng, M.,
    Liu, X., and Meng, H. Bayesian transformer language models for speech recognition.
    pp.  7378–7382, 06 2021. doi: 10.1109/ICASSP39728.2021.9414046.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue et al. (2021) Xue, B., Yu, J., Xu, J., Liu, S., Hu, S., Ye, Z., Geng, M.,
    Liu, X., 和 Meng, H. 用于语音识别的贝叶斯变换器语言模型，第 7378–7382 页，2021年6月。doi: 10.1109/ICASSP39728.2021.9414046。'
- en: Yang et al. (2024) Yang, A. X., Robeyns, M., Wang, X., and Aitchison, L. Bayesian
    low-rank adaptation for large language models, 2024.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) Yang, A. X., Robeyns, M., Wang, X., 和 Aitchison, L. 大型语言模型的贝叶斯低秩适应，2024。
- en: Zhai et al. (2023) Zhai, Y., Zhang, H., Lei, Y., Yu, Y., Xu, K., Feng, D., Ding,
    B., and Wang, H. Uncertainty-penalized reinforcement learning from human feedback
    with diverse reward lora ensembles, 2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai et al. (2023) Zhai, Y., Zhang, H., Lei, Y., Yu, Y., Xu, K., Feng, D., Ding,
    B., 和 Wang, H. 从人类反馈中通过多样化的奖励 lora 集合进行不确定性惩罚强化学习，2023。
- en: 'Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.
    mixup: Beyond empirical risk minimization, 2018.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., 和 Lopez-Paz, D. mixup：超越经验风险最小化，2018。
- en: Zhang et al. (2020) Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A. G.
    Cyclical stochastic gradient mcmc for bayesian deep learning, 2020.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Zhang, R., Li, C., Zhang, J., Chen, C., 和 Wilson, A. G.
    循环随机梯度 MCMC 用于贝叶斯深度学习，2020。
- en: 'Zhang et al. (2023) Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P.,
    Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language
    models with zero-init attention. *arXiv preprint arXiv:2303.16199*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li,
    H., Gao, P., 和 Qiao, Y. Llama-adapter：使用零初始化注意力的语言模型高效微调。*arXiv 预印本 arXiv:2303.16199*，2023。
- en: Zhang et al. (2021) Zhang, S., Fan, X., Chen, B., and Zhou, M. Bayesian attention
    belief networks, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) Zhang, S., Fan, X., Chen, B., 和 Zhou, M. 贝叶斯注意力信念网络，2021。
- en: Zhong et al. (2021) Zhong, R., Lee, K., Zhang, Z., and Klein, D. Adapting language
    models for zero-shot learning by meta-tuning on dataset and prompt collections.
    *arXiv preprint arXiv:2104.04670*, 2021.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. (2021) Zhong, R., Lee, K., Zhang, Z., 和 Klein, D. 通过在数据集和提示集合上进行元调优来适应语言模型以实现零样本学习。*arXiv
    预印本 arXiv:2104.04670*，2021。
- en: Appendix A Datasets
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集
- en: A.1 Size
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 大小
- en: For MMLU STEM and Social Studies datasets, we merge the development and validation
    sets to create the training set and use the original test split as the validation
    set, as in (Wang et al., [2023](#bib.bib69)). For CQA dataset we use the standard
    training and validation splits. The topics in the MMLU dataset are categorized
    into STEM, Social Studies, Humanities, and Other, following the classification
    defined in (Hendrycks et al., [2021](#bib.bib29)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MMLU STEM 和社会学数据集，我们将开发集和验证集合并以创建训练集，并使用原始测试集划分作为验证集，如（Wang et al., [2023](#bib.bib69)）中所述。对于
    CQA 数据集，我们使用标准的训练和验证划分。MMLU 数据集中的主题被分类为 STEM、社会学、人文学科和其他，遵循（Hendrycks et al.,
    [2021](#bib.bib29)）中定义的分类。
- en: '| Dataset | Train Size | Validation Size |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 训练集大小 | 验证集大小 |'
- en: '| --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CQA | 9741 | 1221 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| CQA | 9741 | 1221 |'
- en: '| MMLU SS | 397 | 3077 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| MMLU SS | 397 | 3077 |'
- en: '| MMLU STEM | 430 | 3153 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| MMLU STEM | 430 | 3153 |'
- en: 'Table 1: Summary of the CQA and MMLU dataset sizes.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：CQA 和 MMLU 数据集大小的汇总。
- en: A.2 Examples
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 示例
- en: The datasets have been preprocessed into a single, consistently formatted prompt
    question-answer. Below, we present examples of these questions for each of the
    considered dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经被预处理成单一且一致格式的提示问答。下面，我们展示了每个考虑的数据集的这些问题的示例。
- en: 'Q: A revolving door is convenient for two direction travel, but it also serves
    as a security measure at a what? Answer Choices: (a) bank (b) library (c) department
    store (d) mall (e) new york A: (a). CommonsenseQAQ: Which one of the following
    is the most appropriate definition of a 99% confidence interval? Answer Choices:
    (a) 99% of the time in repeated samples, the interval would contain the true value
    of the parameter (b) 99% of the time in repeated samples, the interval would contain
    the estimated value of the parameter (c) 99% of the time in repeated samples,
    the null hypothesis will be rejected (d) 99% of the time in repeated samples,
    the null hypothesis will not be rejected when it was false A: (a). MMLU SSQ: Find
    all c in Z_3 such that Z_3[x]/(x^2 + c) is a field. Answer Choices: (a) 0 (b)
    1 (c) 2 (d) 3 A: (b). MMLU STEM'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 问：旋转门方便双向通行，但它还作为什么的安全措施？ 答案选项：（a）银行（b）图书馆（c）百货商店（d）商场（e）纽约 答：（a）。CommonsenseQA问：以下哪个是99%置信区间的最恰当定义？
    答案选项：（a）99%的重复样本中，该区间会包含参数的真实值（b）99%的重复样本中，该区间会包含参数的估计值（c）99%的重复样本中，零假设将被拒绝（d）99%的重复样本中，零假设在它为假时不会被拒绝
    答：（a）。MMLU SSQ：找出所有在 Z_3 中的 c，使得 Z_3[x]/(x^2 + c) 是一个域。 答案选项：（a）0（b）1（c）2（d）3
    答：（b）。MMLU STEM
