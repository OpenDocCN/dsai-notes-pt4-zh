- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional
    Bias in LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对减少 LLM 中位置偏差的基于位置的参数高效微调方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01430](https://ar5iv.labs.arxiv.org/html/2404.01430)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.01430](https://ar5iv.labs.arxiv.org/html/2404.01430)
- en: Zheng Zhang^†, Fan Yang^∗, Ziyan Jiang^∗, Zheng Chen^∗, Zhengyang Zhao^∗, Chengyuan
    Ma^∗, Liang Zhao^†, Yang Liu Amazon, views here are the authors’s and not those
    of Amazon. ^†Emory University. {zheng.zhang,liang.zhao}@emory.edu .
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng Zhang^†, Fan Yang^∗, Ziyan Jiang^∗, Zheng Chen^∗, Zhengyang Zhao^∗, Chengyuan
    Ma^∗, Liang Zhao^†, Yang Liu Amazon，观点仅代表作者个人，与 Amazon 无关。^†埃默里大学。{zheng.zhang,liang.zhao}@emory.edu。
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advances in large language models (LLMs) have enhanced their ability
    to process long input contexts. This development is particularly crucial for tasks
    that involve retrieving knowledge from an external datastore, which can result
    in long inputs. However, recent studies show a positional bias in LLMs, demonstrating
    varying performance depending on the location of useful information within the
    input sequence. In this study, we conduct extensive experiments to investigate
    the root causes of positional bias. Our findings indicate that the primary contributor
    to LLM positional bias stems from the inherent positional preferences of different
    models. We demonstrate that merely employing prompt-based solutions is inadequate
    for overcoming the positional preferences. To address this positional bias issue
    of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning
    (PAPEFT) approach which is composed of a data augmentation technique and a parameter
    efficient adapter, enhancing a uniform attention distribution across the input
    context. Our experiments demonstrate that the proposed approach effectively reduces
    positional bias, improving LLMs’ effectiveness in handling long context sequences
    for various tasks that require externally retrieved knowledge.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的进展提升了其处理长输入上下文的能力。这一发展对涉及从外部数据存储中检索知识的任务尤为重要，这可能导致长输入。然而，最近的研究表明，LLM
    存在位置偏差，表现为根据输入序列中有用信息的位置的不同，性能表现各异。在本研究中，我们进行了广泛的实验以调查位置偏差的根本原因。我们的发现表明，LLM 位置偏差的主要原因源自不同模型的固有位置偏好。我们展示了单纯使用基于提示的解决方案不足以克服位置偏好。为了解决预训练
    LLM 的位置偏差问题，我们开发了一种基于位置的参数高效微调（PAPEFT）方法，该方法由数据增强技术和参数高效适配器组成，增强了输入上下文中的均匀注意力分布。我们的实验表明，所提出的方法有效地减少了位置偏差，提高了
    LLM 在处理需要从外部检索知识的各种任务中处理长上下文序列的效果。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancements in developing Large Language Models (LLMs) significantly
    enhance the proficiency of language models in harnessing and utilizing extensive
    input context. This advancement plays a crucial role in improving the performance
    of applications in areas like recommendation (Naumov et al., [2019](#bib.bib21))
    and question answering (Roberts et al., [2020](#bib.bib23); Yasunaga et al., [2021](#bib.bib26)).
    Especially, LLMs have shown remarkable advancements in retrieval-augmented generation
    tasks, significantly enhancing text information retrieval (Guu et al., [2020a](#bib.bib7);
    Borgeaud et al., [2022](#bib.bib1)), exhibiting strong performance in sifting
    through vast amounts of data to find relevant information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大型语言模型（LLMs）开发中的进展显著提升了语言模型在利用和利用广泛输入上下文的能力。这一进展在提升推荐（Naumov 等，[2019](#bib.bib21)）和问答（Roberts
    等，[2020](#bib.bib23); Yasunaga 等，[2021](#bib.bib26)）等领域应用的性能中发挥了关键作用。特别是，LLM 在检索增强生成任务中表现出了显著的进展，显著提升了文本信息检索（Guu
    等，[2020a](#bib.bib7); Borgeaud 等，[2022](#bib.bib1)），在从大量数据中筛选相关信息方面表现出强大的能力。
- en: '![Refer to caption](img/9cc6d8c7e1602121bdb9b56cb81e4009.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9cc6d8c7e1602121bdb9b56cb81e4009.png)'
- en: 'Figure 1: Illustration of Positional Preferences in LLMs: The figure demonstrates
    how the Vicuna-13b-v1.5-16k model’s performance on a recommendation task changes
    with the correct answer’s position in the input context window. Given a list of
    potential candidates, we intentionally position the ground truth candidate at
    various locations within the list to assess how the predicted position distribution
    by the LLM shifts. From the figure we can observe the probability peaks near the
    correct position of relevant information, demonstrating a degree of capacity for
    identifying pertinent information. There is a notable preference for the first
    position, indicating significant positional preference.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLMs中的位置偏好示意图：该图展示了Vicuna-13b-v1.5-16k模型在推荐任务中的表现如何随着正确答案在输入上下文窗口中的位置变化而变化。给定一系列潜在的候选项，我们有意将真实的候选项放置在列表中的不同位置，以评估LLM预测位置分布的变化。从图中可以观察到，相关信息的正确位置附近的概率峰值，显示了识别相关信息的能力。有明显的偏好倾向于首位，表明存在显著的位置偏好。
- en: Although LLMs have made significant progress in processing retrieval-based tasks,
    their application encounters a key challenge due to a positional bias issue. In
    many retrieval scenarios, a list of potential candidates is presented. The order
    of these candidates is often interchangeable and not intended to influence the
    outcome. However, the inherent input structure of LLMs necessitates flattening
    this list, thereby imposing an artificial “ordering” over the candidates. Recent
    studies (Liu et al., [2023a](#bib.bib17); Ravaut et al., [2023](#bib.bib22)) have
    revealed that the performance of LLMs is notably affected by the position of relevant
    information within the input context, especially in cases of extended input lengths.
    Specifically, previous study (Liu et al., [2023a](#bib.bib17)) claimed that LLMs
    often perform better when relevant information is at the beginning or end of a
    sequence, while their performance decreases when key details are in the middle.
    The uneven performance across text segments is described as “lost-in-the-middle”
    phenomenon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs在处理基于检索的任务方面取得了显著进展，但由于位置偏置问题，它们的应用遇到了一个关键挑战。在许多检索场景中，会呈现一系列潜在的候选项。这些候选项的顺序通常是可互换的，并不旨在影响结果。然而，LLMs固有的输入结构需要将这一列表展平，从而对候选项施加了人为的“排序”。最近的研究（Liu
    et al., [2023a](#bib.bib17); Ravaut et al., [2023](#bib.bib22)）揭示了LLMs的表现显著受限于输入上下文中的相关信息位置，特别是在输入长度较长的情况下。具体来说，之前的研究（Liu
    et al., [2023a](#bib.bib17)）声称，当相关信息位于序列的开头或结尾时，LLMs通常表现更好，而当关键信息位于中间时，其表现会下降。文本片段表现不均被描述为“迷失在中间”现象。
- en: While preliminary research (Liu et al., [2023a](#bib.bib17); Ravaut et al.,
    [2023](#bib.bib22); Zheng et al., [2023](#bib.bib27)) has highlighted this positional
    bias as a significant limitation in LLMs, there is a notable gap in understanding
    the underlying causes of this issue. In our study, we have conducted comprehensive
    experiments to assess how the position of genuinely relevant information influences
    the probability distribution of the retrieved information’s location. Our findings
    indicate that rather than the “lost-in-the-middle” phenomenon, it is more accurate
    to state that each LLM exhibits a unique “positional preference” within the context
    window. For example, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional
    Bias in LLMs"), the Vicuna-13b-v1.5-16k model exhibits a clear “positional preference”
    for selecting the initial position within the input context as the predicted position,
    regardless of the actual location of the relevant information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管初步研究（Liu et al., [2023a](#bib.bib17); Ravaut et al., [2023](#bib.bib22); Zheng
    et al., [2023](#bib.bib27)）已将这一位置偏置突显为LLMs的一个重要限制，但对这一问题的根本原因了解仍存在显著差距。在我们的研究中，我们进行了全面的实验，以评估真正相关信息的位置如何影响检索信息位置的概率分布。我们的发现表明，与其说是“迷失在中间”现象，不如说每个LLM在上下文窗口内展现了独特的“位置偏好”。例如，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs")所示，Vicuna-13b-v1.5-16k模型展现了明显的“位置偏好”，即选择输入上下文中的初始位置作为预测位置，而不管相关信息的实际位置。
- en: Moreover, a general while efficient solution to mitigate this positional bias
    issue remains under-explored. Addressing this challenge is crucial for the advancement
    and accuracy of LLM applications, especially in contexts where the order of information
    should not affect the understanding ability of LLMs. Initially, we execute a series
    of experiments demonstrating that merely employing prompt-based strategies, such
    as presenting few-shot examples or instructing LLMs to organize candidates hierarchically,
    can not overcome the issue. To counter this, we introduce a data augmentation
    strategy that involves permuting the position order within documents to mitigate
    the positional preference issue inherent in the source data. Additionally, we
    propose a parameter-efficient fine-tuning technique named Position-Aware Parameter
    Efficient Fine-Tuning (PAPEFT), designed to make pre-trained LLMs aware of and
    adjust for positional bias by explicitly considering document positions within
    the context window. Experimental results across various applications, including
    recommendation and link prediction, show an over 56% reduction in performance
    variance across different positions of relevant information, demonstrating a more
    consistent and reliable understanding abilities of proposed method within the
    input context window.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，缓解这一位置偏差问题的通用而高效的解决方案仍未得到充分探索。解决这一挑战对LLM应用的进步和准确性至关重要，尤其是在信息顺序不应影响LLM理解能力的背景下。最初，我们执行了一系列实验，表明仅仅使用基于提示的策略，如呈现少量示例或指示LLM以层次结构组织候选项，无法克服这一问题。为此，我们引入了一种数据增强策略，通过在文档中排列位置顺序来缓解源数据中固有的位置偏好问题。此外，我们提出了一种名为位置感知参数高效微调（PAPEFT）的参数高效微调技术，旨在通过明确考虑上下文窗口中的文档位置来使预训练LLM意识到并调整位置偏差。跨不同应用的实验结果，包括推荐和链接预测，显示在相关信息的不同位置之间性能方差减少了超过56%，展示了在输入上下文窗口内方法的更一致和可靠的理解能力。
- en: 'The remainder of this work is organized as follows: We begin by discussing
    existing relevant studies in the Section [2](#S2 "2 Related Works ‣ Position-Aware
    Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs").
    This is followed by a formal problem definition and an introduction to the datasets
    in the Section [3](#S3 "3 Preliminaries ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs"). Subsequently, in the Section [4](#S4
    "4 Empirical Studies ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs"), we investigate the underlying cause of
    LLMs’ positional bias through a series of empirical studies and shows that simply
    adopting prompt-based solution can not address the bias. In Section [5](#S5 "5
    Methodology ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing
    Positional Bias in LLMs"), we delve into the motivation behind our approach and
    discuss the specific techniques employed. We conclude with comprehensive experimental
    results, assessing aspects such as effectiveness and efficiency in Section [6](#S6
    "6 Experiments ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing
    Positional Bias in LLMs").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：我们首先在第[2](#S2 "2 相关工作 ‣ 位置感知参数高效微调方法")节讨论现有相关研究。接着在第[3](#S3 "3 初步工作
    ‣ 位置感知参数高效微调方法")节中给出正式的问题定义并介绍数据集。随后在第[4](#S4 "4 实证研究 ‣ 位置感知参数高效微调方法")节，通过一系列实证研究调查LLM的位置偏差的根本原因，并展示了仅采用基于提示的解决方案无法解决这一偏差。在第[5](#S5
    "5 方法论 ‣ 位置感知参数高效微调方法")节中，我们深入探讨了我们方法背后的动机并讨论了所采用的具体技术。最后在第[6](#S6 "6 实验 ‣ 位置感知参数高效微调方法")节中，我们总结了全面的实验结果，评估了效果和效率等方面。
- en: 2 Related Works
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Positional Bias in LLMs. While LLMs have gained prominence, the exploration
    of positional bias within these models is still in its infancy and has only recently
    started to attract attention. The body of existing research, though growing, remains
    limited. A handful of early studies have started to shed light on the implications
    of positional bias in LLMs. For instance, Liu et al. (Liu et al., [2023a](#bib.bib17))
    provided benchmarks indicating that positional bias is a widespread concern, particularly
    in question answering and key-value pair retrieval tasks. Ravaut et al. (Ravaut
    et al., [2023](#bib.bib22)) expanded on this by exploring positional bias in text
    summarization tasks. Zheng et al. (Zheng et al., [2023](#bib.bib27)) made an early
    attempt to correct this bias by adjusting LLM outputs based on a prior probability
    reflecting the model’s option preferences. Nevertheless, their approach is confined
    to multiple-choice contexts and lacks generalizability for broader applications.
    This limitation stems from the challenges in calculating the prior probability
    and the significant increase in computational demands that such a method entails.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的位置偏差。虽然LLM（大语言模型）已经获得了广泛关注，但对这些模型中位置偏差的探索仍处于起步阶段，并且最近才开始引起关注。现有研究虽然在增长，但仍然有限。一些早期研究开始揭示LLM中位置偏差的影响。例如，刘等人（刘等人，[2023a](#bib.bib17)）提供的基准表明，位置偏差是一个普遍关注的问题，特别是在问答和键值对检索任务中。Ravaut等人（Ravaut等人，[2023](#bib.bib22)）通过探索文本摘要任务中的位置偏差扩展了这一点。郑等人（郑等人，[2023](#bib.bib27)）早期尝试通过基于反映模型选项偏好的先验概率来调整LLM输出，以纠正这种偏差。然而，他们的方法仅限于多项选择情境，缺乏对更广泛应用的普遍性。这一局限性源于计算先验概率的挑战以及这种方法所需的显著计算开销的增加。
- en: Retrieval-Augmented Generation. Retrieval-Augmented Generation combines generative
    capabilities of language models with external knowledge retrieval, enhancing accuracy
    and relevance in responses. Early foundational work in transformers set the stage
    for RAG systems (Vaswani et al., [2017](#bib.bib24)). Subsequent developments
    like R-Transformer (Lewis et al., [2020](#bib.bib14)) and RAG models (Guu et al.,
    [2020b](#bib.bib8)) integrated retrieval mechanisms with large language models,
    improving performance in knowledge-intensive tasks. Recent advancements (Liu et al.,
    [2023b](#bib.bib18); Chowdhery et al., [2023](#bib.bib3)) focus on optimizing
    retrieval efficiency and accuracy, addressing challenges in coherence, factual
    correctness, and bias management.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成。检索增强生成将语言模型的生成能力与外部知识检索结合起来，提高了响应的准确性和相关性。早期的变压器基础工作为RAG系统（Vaswani等人，[2017](#bib.bib24)）奠定了基础。随后，像R-Transformer（Lewis等人，[2020](#bib.bib14)）和RAG模型（Guu等人，[2020b](#bib.bib8)）这样的发展将检索机制与大型语言模型结合起来，提高了知识密集型任务的性能。最近的进展（刘等人，[2023b](#bib.bib18)；Chowdhery等人，[2023](#bib.bib3)）集中于优化检索效率和准确性，解决了连贯性、事实正确性和偏差管理等挑战。
- en: Parameter Efficient Fine-Tuning for LLMs. Parameter-efficient fine-tuning has
    emerged as a crucial technique for enhancing model performance without the substantial
    computational and memory costs associated with full model training. This approach,
    as discussed in recent literature, involves adjusting a small subset of the model’s
    parameters while keeping the majority of the model’s weights fixed, thereby enabling
    the model to adapt to new tasks or data with minimal resource expenditure. Techniques
    such as adapter layers (Houlsby et al., [2019](#bib.bib9); Dettmers et al., [2024](#bib.bib6)),
    prompt tuning (Li & Liang, [2021](#bib.bib16)), and sparse updates (Liu et al.,
    [2023c](#bib.bib19)) have been highlighted as effective means for achieving this
    efficiency. Such methods not only conserve resources but also mitigate the risk
    of overfitting by limiting the degree of freedom during the training process.
    Existing research primarily concentrates on enhancing the efficiency of the fine-tuning
    stage for LLMs, whereas our work is distinctly focused on debiasing a pre-trained
    LLM using an efficient fine-tuning module.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLMs的参数高效微调。参数高效微调已经成为提升模型性能的关键技术之一，它无需承担全面模型训练所涉及的巨大计算和内存成本。正如近期文献所讨论的，这种方法涉及调整模型参数的一个小子集，同时保持大部分模型权重不变，从而使模型能够以最小的资源消耗适应新任务或数据。诸如适配器层（Houlsby
    et al., [2019](#bib.bib9); Dettmers et al., [2024](#bib.bib6)）、提示微调（Li & Liang,
    [2021](#bib.bib16)）和稀疏更新（Liu et al., [2023c](#bib.bib19)）等技术已被突显为实现这一效率的有效手段。这些方法不仅节省了资源，而且通过限制训练过程中的自由度来减轻过拟合的风险。现有研究主要集中在提升LLMs微调阶段的效率，而我们的工作则特别关注于使用高效微调模块对预训练LLM进行去偏差处理。
- en: 3 Preliminaries
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 预备知识
- en: Problem Formulation.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题定义。
- en: This paper focuses on the exploration and analysis of tasks which leverage Retrieval-Augmented
    Generation (RAG) framework in the context of Large Language Models (LLMs). The
    central scenario of our study is an input context with a set of $K$ represents
    the additional textual prompts that describe the task for LLMs. The desired outcome
    from the LLM in response to this input should correctly identify and select the
    correct relevant document from the set of $K$, the likelihood that the LLM identifies
    the $i$ denotes the probability of the correct position $\mathbf{X}_{c}$.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注的是在大型语言模型（LLMs）背景下，利用检索增强生成（RAG）框架的任务的探索和分析。我们研究的核心场景是一个输入上下文，其中一组$K$表示描述LLMs任务的额外文本提示。LLM对该输入的期望结果应为从$K$集合中正确识别和选择相关文档，LLM识别$i$的可能性表示正确位置$\mathbf{X}_{c}$的概率。
- en: Datasets.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'Our research investigates positional bias in Language Learning Models (LLMs)
    across Recommendation (REC), and Link Prediction (LP) domains. We employed specialized
    datasets—Amazon M2 (Jin et al., [2023](#bib.bib11)) for REC, and Arxiv (Wang et al.,
    [2020](#bib.bib25)) for LP—to evaluate LLMs’ ability to identify key information
    in varying contextual placements. The critical information’s position within each
    dataset was systematically varied to test LLM adaptability and accuracy with shifting
    contexts. The details of used datasets are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究探讨了语言学习模型（LLMs）在推荐（REC）和链接预测（LP）领域中的位置偏差。我们使用了专门的数据集——Amazon M2 （Jin et al.,
    [2023](#bib.bib11)）用于REC，Arxiv （Wang et al., [2020](#bib.bib25)）用于LP——来评估LLMs在不同上下文位置识别关键信息的能力。每个数据集中关键信息的位置被系统性地变化，以测试LLM在变化上下文中的适应能力和准确性。使用的数据集的详细信息如下：
- en: 'Recommendation (REC): We utilized the Amazon M2 dataset (Jin et al., [2023](#bib.bib11)),
    which is a rich source of user-product interaction data. Each session within the
    dataset consists of a sequence of products previously purchased by a user, and
    their next following purchase. The dataset provides extensive metadata for each
    product, including descriptions and brand information. We present a set of $K$
    possible products per session, among which only one is the actual product that
    the user purchased and the others are negative samples. We alter the position
    of “ground truth” product within the list to examine the LLMs’ proficiency in
    pinpointing the relevant information depending on its contextual placement.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐（REC）：我们使用了亚马逊M2数据集（Jin et al., [2023](#bib.bib11)），这是一个丰富的用户-产品交互数据源。数据集中的每个会话包含用户之前购买的一系列产品及其下一次购买。数据集为每个产品提供了广泛的元数据，包括描述和品牌信息。我们展示了每个会话中的一组$K$个可能产品，其中只有一个是用户实际购买的产品，其余的是负样本。我们更改“真实信息”产品在列表中的位置，以考察LLMs根据其上下文位置确定相关信息的能力。
- en: 'Link-Prediction (LP): We leverage the comprehensive citation network benchmark
    dataset Arxiv (Kwiatkowski et al., [2019](#bib.bib12)). This dataset describes
    a large citation graph where each node is a research paper and the connections
    between nodes indicate their citation behavior. To assess the ability of LLMs
    against varied positions of relevant information, we include an evaluation of
    their ability to accurately identify and present correct cited paper. We manipulate
    the location of the ground truth cited paper among a list of randomly sampled
    papers. Specifically, for each given paper, we present a list of papers while
    only one of them is truly cited by the given one. Then we vary the position of
    the ‘ground truth’ paper to evaluate how the position of the paper impacts the
    LLMs’ prediction ability.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 链接预测（LP）：我们利用了全面的引用网络基准数据集Arxiv（Kwiatkowski et al., [2019](#bib.bib12)）。该数据集描述了一个大型引用图，其中每个节点是一个研究论文，节点之间的连接表示其引用行为。为了评估LLMs在不同相关信息位置的能力，我们包括了对其准确识别和呈现正确引用论文的能力的评估。我们操控了真实引用论文在随机抽样论文列表中的位置。具体而言，对于每篇给定的论文，我们展示了一份论文列表，其中只有一篇论文真正被给定的论文引用。然后我们改变“真实信息”论文的位置，以评估论文位置对LLMs预测能力的影响。
- en: Choice of LLMs.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLMs的选择。
- en: To assess the robustness of LLMs against positional bias in scenarios involving
    extensive context sizes, we have compiled a list of widely recognized open-source
    LLMs specifically tailored for managing long input contexts. This compilation
    features models frequently employed in academic research, such as Vicuna-13b-v1.5-16k (Chiang
    et al., [2023](#bib.bib2)) and Longchat-13b-16k (Li et al., [2023](#bib.bib15)).
    These selections enable us to examine a model’s efficacy in processing extended
    dialogues and its capability to handle positional information within conversational
    settings.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLMs在处理涉及广泛上下文大小的场景中的鲁棒性，我们编制了一个专门用于管理长输入上下文的广泛认可的开源LLMs列表。该列表包含了在学术研究中频繁使用的模型，如Vicuna-13b-v1.5-16k（Chiang
    et al., [2023](#bib.bib2)）和Longchat-13b-16k（Li et al., [2023](#bib.bib15)）。这些选择使我们能够检验模型处理扩展对话的效能以及在对话设置中处理位置信息的能力。
- en: Evaluation Metrics.
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评价指标。
- en: We adopt “accuracy” to evaluate the generated answer quality by LLMs, judging
    whether the correct relevant document is selected to generate the final answer.
    Additionally, we employ “fluctuation” as a metric to assess the variance in performance
    across different positions, which is defined as the ratio of the standard deviation
    to the average value.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用“准确率”来评估LLMs生成答案的质量，判断是否选择了正确的相关文档来生成最终答案。此外，我们使用“波动性”作为一个指标来评估不同位置的性能差异，其定义为标准差与平均值的比率。
- en: 4 Empirical Studies
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个实证研究
- en: '![Refer to caption](img/22d633c17af7e3e31b8e59bea8349285.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22d633c17af7e3e31b8e59bea8349285.png)'
- en: 'Figure 2: The Longchat-13b-16k model’s performance on a recommendation task
    changes with the correct answer’s position in the input context window. Comparing
    with the Vicuna-13b-v1.5-16k model’s trend in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing
    Positional Bias in LLMs"), we can observe that these two models have different
    preferred positions. The Longchat-13b-16k model has preferred location around
    position eleven, while Vicuna-13b-v1.5-16k prefers the first position.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Longchat-13b-16k 模型在推荐任务上的表现随着正确答案在输入上下文窗口中的位置变化而变化。与图 [1](#S1.F1 "图 1 ‣
    1 介绍 ‣ 减少 LLMs 中位置偏差的基于位置感知参数高效微调方法") 中 Vicuna-13b-v1.5-16k 模型的趋势相比，我们可以观察到这两个模型有不同的偏好位置。Longchat-13b-16k
    模型倾向于在第十一位置附近，而 Vicuna-13b-v1.5-16k 更倾向于第一个位置。
- en: 4.1 Investigating the Underlying Causes of Positional Bias in LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 研究 LLMs 中位置偏差的根本原因
- en: To uncover the reasons underlying positional bias in LLMs, we initiate our investigation
    by conducting empirical experiments. These experiments are designed to evaluate
    how the placement of the ground truth answer influences the probability distribution
    of the positions predicted by LLMs. In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional
    Bias in LLMs") and Figure [2](#S4.F2 "Figure 2 ‣ 4 Empirical Studies ‣ Position-Aware
    Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"),
    we illustrate the predicted probability distributions for all potential positions
    across various ground truth locations, using the Vicuna-13b-v1.5-16k and Longchat-13b-16k
    models, respectively. From these figures, it is evident that both models exhibit
    a “preferred position” for the predicted answer, regardless of the actual ground
    truth positions. Notably, the models demonstrate distinct positional preferences,
    where Longchat-13b-16k shows a preference for the eleventh position and Vicuna-13b-v1.5-16k
    tends to favor the first position. Therefore, instead of the “lost-in-the-middle”
    phenomenon suggested by earlier research (Liu et al., [2023a](#bib.bib17)), we
    arguably propose that the issue of positional bias is primarily due to the model’s
    “preferred position”.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了揭示 LLMs 中位置偏差的根本原因，我们通过进行实证实验来启动我们的调查。这些实验旨在评估真实答案的位置如何影响 LLMs 预测位置的概率分布。在图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 减少 LLMs 中位置偏差的基于位置感知参数高效微调方法") 和图 [2](#S4.F2 "图 2 ‣ 4
    实证研究 ‣ 减少 LLMs 中位置偏差的基于位置感知参数高效微调方法") 中，我们分别使用 Vicuna-13b-v1.5-16k 和 Longchat-13b-16k
    模型展示了所有潜在位置的预测概率分布。通过这些图，可以明显看出这两个模型对预测答案有一个“偏好位置”，无论实际的真实答案位置在哪里。特别是，模型表现出不同的位置信息偏好，其中
    Longchat-13b-16k 更倾向于第十一位置，而 Vicuna-13b-v1.5-16k 更倾向于第一个位置。因此，与早期研究（Liu 等人，[2023a](#bib.bib17)）所建议的“迷失在中间”现象不同，我们认为位置偏差的问题主要是由于模型的“偏好位置”。
- en: 4.2 Prompt-Engineering Based Method Performance
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于提示工程的方法性能
- en: 'Model # of Few-shot 1 5 9 13 17 20 Longchat-13b-16k 0 0.426 0.125 0.105 0.142
    0.127 0.261 Longchat-13b-16k 1 0.620 0.223 0.167 0.161 0.150 0.338 Longchat-13b-16k
    3 0.594 0.169 0.130 0.142 0.123 0.269 Longchat-13b-16k 5 0.669 0.175 0.114 0.108
    0.100 0.228 Vicuna-13b-v1.5-16k 0 0.931 0.207 0.076 0.057 0.019 0.069 Vicuna-13b-v1.5-16k
    1 0.832 0.014 0.005 0.003 0.000 0.001 Vicuna-13b-v1.5-16k 3 0.872 0.002 0.001
    0.002 0.005 0.011 Vicuna-13b-v1.5-16k 5 0.903 0.003 0.001 0.003 0.002 0.004'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 # 几-shot 1 5 9 13 17 20 Longchat-13b-16k 0 0.426 0.125 0.105 0.142 0.127
    0.261 Longchat-13b-16k 1 0.620 0.223 0.167 0.161 0.150 0.338 Longchat-13b-16k
    3 0.594 0.169 0.130 0.142 0.123 0.269 Longchat-13b-16k 5 0.669 0.175 0.114 0.108
    0.100 0.228 Vicuna-13b-v1.5-16k 0 0.931 0.207 0.076 0.057 0.019 0.069 Vicuna-13b-v1.5-16k
    1 0.832 0.014 0.005 0.003 0.000 0.001 Vicuna-13b-v1.5-16k 3 0.872 0.002 0.001
    0.002 0.005 0.011 Vicuna-13b-v1.5-16k 5 0.903 0.003 0.001 0.003 0.002 0.004'
- en: 'Table 1: Few-shot performance on recommendation task with 1, 3, and 5 few-shot
    examples. Here “0” few-shot examples denotes the performance of zero-shot situation.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在推荐任务上使用 1、3 和 5 个几-shot 示例的表现。这里的“0”个几-shot 示例表示零-shot 情况下的表现。
- en: 'Considering the identified “preferred position” bias, a natural question arises:
    can we devise an effective method to mitigate this bias issue? With recent advancements
    demonstrating that LLMs possess a significant in-context learning capability (Min
    et al., [2021](#bib.bib20)), enabling them to learn and reason based on the text
    prompts provided, it naturally leads to the question whether specially crafted
    prompts could be employed to address or alleviate the impact of positional bias.
    To answer this question, we have crafted a variety of input prompts, aiming to
    provide insights on addressing the positional bias issue. The description of prompts
    is as below and the example of prompts can be found in Appendix Table [7](#A1.T7
    "Table 7 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到已识别的“优选位置”偏差，自然会提出一个问题：我们能否设计一种有效的方法来减轻这一偏差问题？最近的研究表明，LLMs 具有显著的上下文学习能力（Min
    等， [2021](#bib.bib20)），能够根据提供的文本提示进行学习和推理，这自然引出了一个问题，即是否可以利用专门设计的提示来解决或缓解位置偏差的影响。为回答这一问题，我们设计了多种输入提示，旨在提供解决位置偏差问题的见解。提示的描述如下，提示示例可以在附录表
    [7](#A1.T7 "Table 7 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient
    Fine-Tuning Approach for Reducing Positional Bias in LLMs") 中找到。
- en: '(1) Zero-shot learning: LLMs are tasked with generating responses without any
    prior examples. This setting is essential to observe the natural inclinations
    of LLMs and their raw handling of positional information, providing a baseline
    for their performance.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 零样本学习：LLMs 被要求在没有任何先前示例的情况下生成回应。这一设置对于观察 LLMs 的自然倾向以及它们处理位置相关信息的原始能力至关重要，为其性能提供了基准。
- en: '(2) Few-shot learning: We provide the LLMs with a handful of selected examples
    within the prompt. The goal is to determine if a limited number of illustrative
    examples can provide sufficient knowledge to the models, thereby guiding them
    towards more accurate interpretations of information, regardless of its positional
    context.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 少量样本学习：我们在提示中提供了少量精选示例。目标是确定有限数量的示例是否能够为模型提供足够的知识，从而指导模型对信息进行更准确的解读，而不论其位置上下文如何。
- en: '(3) Hierarchical inference: A potential cause of the positional bias issue
    could be attributed to the extensive context size and the large number of possible
    choices. To tackle this challenge, we suggest employing a prompt that encourages
    the LLM to make prediction in a bottom-up manner. Initially, the model is instructed
    to categorize all candidates into a few smaller groups, followed by identifying
    the most likely answer within each group. Subsequently, from the chosen answers
    for each group, the model is tasked with making the final prediction. Thus, the
    overall prediction process is structured in a hierarchical fashion, aiming to
    mitigate the effects of positional bias.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 分层推理：位置偏差问题的一个潜在原因可能归因于上下文大小过大以及可能选择的数量过多。为了解决这一挑战，我们建议使用一种促使 LLM 以自下而上的方式进行预测的提示。最初，模型被指示将所有候选项分类为几个较小的组，然后在每个组内识别最可能的答案。随后，在每个组中选择的答案中，模型需要做出最终预测。因此，整体预测过程以分层方式构建，旨在减轻位置偏差的影响。
- en: 4.2.1 Few-shot Learning
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 少量样本学习
- en: In Table [1](#S4.T1 "Table 1 ‣ 4.2 Prompt-Engineering Based Method Performance
    ‣ 4 Empirical Studies ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs"), we present the impact of utilizing a varying
    number of few-shot examples on the performance in a recommendation task. The findings
    indicate that while few-shot examples can generally enhance the model’s accuracy,
    they do not mitigate the issue of positional bias along the sequence. The fluctuation
    in performance across different positions continues to exhibit high variance,
    even as the quantity of few-shot examples is increased.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [1](#S4.T1 "Table 1 ‣ 4.2 Prompt-Engineering Based Method Performance ‣ 4
    Empirical Studies ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for
    Reducing Positional Bias in LLMs") 中，我们展示了在推荐任务中使用不同数量的少量样本对性能的影响。研究结果表明，虽然少量样本通常可以提高模型的准确性，但它们并不能减轻序列中的位置偏差问题。即使少量样本的数量增加，不同位置的性能波动仍然表现出高变异性。
- en: 4.2.2 Hierarchical Inference
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 分层推理
- en: The outcomes of employing hierarchical inference are detailed in Table [2](#S4.T2
    "Table 2 ‣ 4.2.2 Hierarchical Inference ‣ 4.2 Prompt-Engineering Based Method
    Performance ‣ 4 Empirical Studies ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs"). It is observed that this approach
    not only fails to mitigate the positional bias issue but also leads to a notable
    decrease in performance. A possible explanation for this result could be that
    LLMs might not effectively process the complex instructions presented within a
    single input prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S4.T2 "Table 2 ‣ 4.2.2 Hierarchical Inference ‣ 4.2 Prompt-Engineering
    Based Method Performance ‣ 4 Empirical Studies ‣ Position-Aware Parameter Efficient
    Fine-Tuning Approach for Reducing Positional Bias in LLMs")中详细列出了采用层次推理的结果。观察到这种方法不仅未能减轻位置偏差问题，还导致了性能显著下降。这一结果的一个可能解释是，LLMs可能无法有效处理单个输入提示中呈现的复杂指令。
- en: In conclusion, the empirical studies showcased here demonstrate that prompt-based
    solutions alone are insufficient to resolve the positional bias issue. Given the
    observation of a distinct “preferred location” for each pre-trained LLM, it is
    arguably possible that positional biases are inherently introduced during the
    pre-training phase or the instruction fine-tuning phase through the training data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，展示的实证研究表明，仅凭提示基础的解决方案不足以解决位置偏差问题。鉴于观察到每个预训练LLM都有明显的“优选位置”，可以说位置偏差可能在预训练阶段或指令微调阶段通过训练数据固有地引入。
- en: Model Hierarchical 1 5 9 13 17 20 Longchat-13b-16k No 0.426 0.125 0.105 0.142
    0.127 0.261 Longchat-13b-16k Yes 0.131 0.032 0.022 0.028 0.069 0.149 Vicuna-13b-v1.5-16k
    No 0.931 0.207 0.076 0.057 0.019 0.069 Vicuna-13b-v1.5-16k Yes 0.179 0.012 0.041
    0.280 0.212 0.268 GPT-3.5-turbo-16k No 0.440 0.507 0.495 0.354 0.315 0.288 GPT-3.5-turbo-16k
    Yes 0.245 0.147 0.133 0.092 0.095 0.138
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 层次 1 5 9 13 17 20 Longchat-13b-16k 无 0.426 0.125 0.105 0.142 0.127 0.261
    Longchat-13b-16k 有 0.131 0.032 0.022 0.028 0.069 0.149 Vicuna-13b-v1.5-16k 无 0.931
    0.207 0.076 0.057 0.019 0.069 Vicuna-13b-v1.5-16k 有 0.179 0.012 0.041 0.280 0.212
    0.268 GPT-3.5-turbo-16k 无 0.440 0.507 0.495 0.354 0.315 0.288 GPT-3.5-turbo-16k
    有 0.245 0.147 0.133 0.092 0.095 0.138
- en: 'Table 2: Hierarchical inference performance on recommendation task with varying
    positions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同位置下推荐任务的层次推理性能。
- en: 5 Methodology
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 方法论
- en: In order to design an effective and efficient method for mitigating the inherent
    positional bias of pre-trained LLMs, we introduce a strategy named Position Aware
    Parameter Efficient Fine Tuning (PAPEFT). It combines a position-aware parameter
    efficient adapter module with data augmentation techniques. Specifically, to remove
    the model’s intrinsic location preference bias, which is typically introduced
    by the pre-training phase data, we employ a data augmentation technique (Section [5.1](#S5.SS1
    "5.1 Ordering Permutation with Data Augmentation ‣ 5 Methodology ‣ Position-Aware
    Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"))
    that involves random permutation of the ordering in candidate lists. This requires
    LLMs to distribute their attention uniformly across different positions within
    the input context. Furthermore, to efficiently debias the original parameters
    of LLMs, we introduce an new adapter module that explicitly incorporates the positional
    context of each candidate as learnable soft prompts (Section [5.2](#S5.SS2 "5.2
    Explicitly Incorporating Positions Location through Location Encoding Adapter
    ‣ 5 Methodology ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for
    Reducing Positional Bias in LLMs")). This integration aims to adjust the LLM’s
    attention to various positions more equitably without modifying the original pre-trained
    parameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计一种有效且高效的方法来减轻预训练大语言模型（LLMs）固有的位置信息偏差，我们引入了一种名为**位置感知参数高效微调（PAPEFT）**的策略。该策略结合了位置感知的参数高效适配器模块和数据增强技术。具体来说，为了消除模型固有的位置偏好偏差，通常由预训练阶段的数据引入，我们采用了一种数据增强技术（第[5.1节](#S5.SS1
    "5.1 Ordering Permutation with Data Augmentation ‣ 5 Methodology ‣ Position-Aware
    Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs")），涉及候选列表中排序的随机置换。这要求LLMs在输入上下文中的不同位置之间均匀分配注意力。此外，为了高效地去偏原始LLMs的参数，我们引入了一种新的适配器模块，明确地将每个候选的位置信息作为可学习的软提示（第[5.2节](#S5.SS2
    "5.2 Explicitly Incorporating Positions Location through Location Encoding Adapter
    ‣ 5 Methodology ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for
    Reducing Positional Bias in LLMs")）。这一整合旨在在不修改原始预训练参数的情况下，更公平地调整LLM对不同位置的注意力分配。
- en: 5.1 Ordering Permutation with Data Augmentation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据增强中的排序置换
- en: As discussed in Section [4](#S4 "4 Empirical Studies ‣ Position-Aware Parameter
    Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"), existing
    pre-trained LLMs exhibit a specific location preference over the input contextual
    length. This tendency results in an uneven distribution of attention across the
    entire context, and thus leads to fluctuating performance. A potential explanation
    for the distinct location preferences observed in various LLMs could stem from
    positional biases present in the original pre-training data, e.g. key information
    is often placed at the start of text.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4](#S4 "4 Empirical Studies ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs")节中讨论的，现有的预训练LLMs表现出对输入上下文长度的特定位置偏好。这种倾向导致了整个上下文中的注意力分布不均，从而导致性能波动。不同LLMs中观察到的不同位置偏好可能源于原始预训练数据中存在的位置偏差，例如关键信息通常放在文本的开头。
- en: In order to provide an appropriate way to mitigate this issue in the data perspective,
    we adopt a strategic data augmentation process designed to evenly distribute LLM
    attention across various positions within the input context. Specifically, this
    approach creates multiple permutations for each set of potential document candidates
    within a given input context. These permutations serve as augmented fine-tuning
    datasets. Mathematically, given a list of candidates $[\mathbf{P}_{s},\mathbf{X}_{1},\mathbf{X}_{2},\dots,\mathbf{X}_{K}]$
    denotes a permutation function that rearranges the indices $1,2,\dots,K$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从数据角度提供一种适当的方式来缓解这个问题，我们采用了一种战略数据增强过程，旨在均匀分布LLM注意力在输入上下文中的不同位置。具体来说，这种方法为给定输入上下文中的每组潜在文档候选项创建多个排列。这些排列作为增强的微调数据集。数学上，给定候选列表
    $[\mathbf{P}_{s},\mathbf{X}_{1},\mathbf{X}_{2},\dots,\mathbf{X}_{K}]$ 表示一个排列函数，该函数重新排列索引
    $1,2,\dots,K$。
- en: '![Refer to caption](img/81a7d2aa4e0eae3cf985be0540493a9f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81a7d2aa4e0eae3cf985be0540493a9f.png)'
- en: 'Figure 3: The overall framework of location encoding soft prompt adapter module.
    The relative locations of potential documents are initially computed and subsequently
    fed into a soft prompt adapter. The soft location tokens are concatenated with
    textual tokens to form a combined input for the attention layers in LLMs.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：位置编码软提示适配器模块的总体框架。潜在文档的相对位置首先被计算出来，然后输入到软提示适配器中。软位置标记与文本标记连接在一起，形成一个组合输入，用于LLMs中的注意力层。
- en: 5.2 Explicitly Incorporating Positions Location through Location Encoding Adapter
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 通过位置编码适配器显式地整合位置
- en: Given the generated augmented data, a key question is the design of the fine-tuning
    module for the pre-trained LLM. Directly optimizing the LLM parameters is a straightforward
    approach but proves inefficient due to the vast number of parameters involved.
    Although various parameter-efficient fine-tuning methods like LoRA (Hu et al.,
    [2021](#bib.bib10)), QLoRA (Dettmers et al., [2023](#bib.bib5)), and prompt tuning (Lester
    et al., [2021](#bib.bib13)) are available, they do not adequately consider the
    location of documents within the input context, thus these approaches are not
    fully optimized in addressing positional bias.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到生成的增强数据，一个关键问题是如何设计预训练LLM的微调模块。直接优化LLM参数是一种直接的方法，但由于涉及的参数数量庞大，这种方法效果不佳。尽管有LoRA
    (Hu et al., [2021](#bib.bib10))、QLoRA (Dettmers et al., [2023](#bib.bib5))和prompt
    tuning (Lester et al., [2021](#bib.bib13))等各种参数高效微调方法，但它们并未充分考虑文档在输入上下文中的位置，因此这些方法在解决位置偏差问题上并未得到充分优化。
- en: In order to let LLMs be aware of the position of all potential documents for
    a debias-oriented optimization process, we further propose a novel adapter module
    to explicitly incorporate the relative locations of documents as additional input
    prompts, which is named as Location Encoding (LE) adapter. Specifically, each
    document’s relative location is computed and included in the input prompts. Then
    a trainable adapter module transforms the dimensions of these locational prompts
    into the token embedding space, aligning the semantic meaning of the transformed
    locational tokens with the textual tokens.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使LLMs能够意识到所有潜在文档的位置以进行去偏优化过程，我们进一步提出了一种新型适配器模块，明确地将文档的相对位置作为额外输入提示，这被称为位置编码（LE）适配器。具体而言，每个文档的相对位置被计算并包含在输入提示中。然后，一个可训练的适配器模块将这些位置提示的维度转换为标记嵌入空间，使转换后的位置标记的语义与文本标记对齐。
- en: 'Mathematically, as illustrated in Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Ordering
    Permutation with Data Augmentation ‣ 5 Methodology ‣ Position-Aware Parameter
    Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"), the process
    begins by computing the relative locations of all potential documents within the
    context length, denoted as $\mathbf{S}\in\mathbb{R}^{K}$ denotes the learnable
    parameters, is then applied to these locational prompts. This module aligns the
    semantic essence of these spatial tokens with the trained textual token space
    of the LLM. The formal transformation process is represented as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，如图 [3](#S5.F3 "图 3 ‣ 5.1 使用数据增强的排序排列 ‣ 5 方法论 ‣ 位置感知参数高效微调方法用于减少 LLM 的位置偏差")
    所示，过程开始于计算所有潜在文档在上下文长度内的相对位置，记作 $\mathbf{S}\in\mathbb{R}^{K}$，然后将其应用于这些位置提示。该模块将这些空间标记的语义本质与
    LLM 的训练文本标记空间对齐。正式的转换过程表示如下：
- en: '|  | $f_{\theta}(\mathbf{S}_{i})=\mathbf{A}_{i},\quad\forall i\in[1,K],\quad\mathbf{A}_{i}\in\mathbb{R}^{d},$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{\theta}(\mathbf{S}_{i})=\mathbf{A}_{i},\quad\forall i\in[1,K],\quad\mathbf{A}_{i}\in\mathbb{R}^{d},$
    |  | (1) |'
- en: where $d$ represents the dimension of the token embedding space utilized by
    the LLM.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$ 代表 LLM 使用的标记嵌入空间的维度。
- en: Upon completion of this mapping process, the adapter model generates additional
    transformed tokens $\mathbf{A}$ for the LLMs. This concatenated sequence encourages
    that each document is presented and guided by a contextual positional cue, thereby
    providing the LLM with a dual awareness of content and contextual positioning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成此映射过程后，适配器模型为 LLM 生成额外的转换标记 $\mathbf{A}$。这个串联的序列促使每个文档都由上下文位置提示进行展示和引导，从而为
    LLM 提供了内容和上下文位置的双重感知。
- en: 6 Experiments
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个实验
- en: Task Base Model Fine Tune Strategy 1 5 9 13 17 20 Mean Fluctuation (%) REC Longchat-13b-16k
    Original 0.329 0.249 0.211 0.205 0.171 0.341 0.251 27.78 PAPEFT-PT 0.832 0.714
    0.708 0.723 0.715 0.736 0.733 6.38 PAPEFT-LE 0.854 0.731 0.748 0.745 0.767 0.752
    0.766 5.82 PAPEFT-LoRA 0.864 0.816 0.808 0.823 0.815 0.836 0.827 2.47 Vicuna-13b-v1.5-16k
    Original 0.855 0.083 0.211 0.205 0.171 0.341 0.311 89.76 PAPEFT-PT 0.881 0.698
    0.701 0.745 0.767 0.741 0.756 8.88 PAPEFT-LE 0.883 0.746 0.738 0.798 0.807 0.765
    0.790 6.77 PAPEFT-LoRA 0.855 0.836 0.818 0.833 0.825 0.855 0.837 1.83 LP Longchat-13b-16k
    Original 0.016 0.112 0.147 0.168 0.051 0.022 0.086 75.97 PAPEFT-PT 0.698 0.708
    0.742 0.760 0.718 0.742 0.728 3.26 PAPEFT-LE 0.755 0.754 0.763 0.781 0.773 0.763
    0.765 1.37 PAPEFT-LoRA 0.829 0.810 0.815 0.809 0.816 0.825 0.817 0.99 Vicuna-13b-v1.5-16k
    Original 0.257 0.208 0.119 0.166 0.096 0.104 0.158 40.58 PAPEFT-PT 0.757 0.721
    0.709 0.741 0.761 0.771 0.743 3.27 PAPEFT-LE 0.744 0.774 0.773 0.769 0.760 0.783
    0.767 1.77 PAPEFT-LoRA 0.824 0.824 0.823 0.841 0.843 0.853 0.835 1.52
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 基础模型 微调策略 1 5 9 13 17 20 平均波动（%） REC Longchat-13b-16k 原始 0.329 0.249 0.211
    0.205 0.171 0.341 0.251 27.78 PAPEFT-PT 0.832 0.714 0.708 0.723 0.715 0.736 0.733
    6.38 PAPEFT-LE 0.854 0.731 0.748 0.745 0.767 0.752 0.766 5.82 PAPEFT-LoRA 0.864
    0.816 0.808 0.823 0.815 0.836 0.827 2.47 Vicuna-13b-v1.5-16k 原始 0.855 0.083 0.211
    0.205 0.171 0.341 0.311 89.76 PAPEFT-PT 0.881 0.698 0.701 0.745 0.767 0.741 0.756
    8.88 PAPEFT-LE 0.883 0.746 0.738 0.798 0.807 0.765 0.790 6.77 PAPEFT-LoRA 0.855
    0.836 0.818 0.833 0.825 0.855 0.837 1.83 LP Longchat-13b-16k 原始 0.016 0.112 0.147
    0.168 0.051 0.022 0.086 75.97 PAPEFT-PT 0.698 0.708 0.742 0.760 0.718 0.742 0.728
    3.26 PAPEFT-LE 0.755 0.754 0.763 0.781 0.773 0.763 0.765 1.37 PAPEFT-LoRA 0.829
    0.810 0.815 0.809 0.816 0.825 0.817 0.99 Vicuna-13b-v1.5-16k 原始 0.257 0.208 0.119
    0.166 0.096 0.104 0.158 40.58 PAPEFT-PT 0.757 0.721 0.709 0.741 0.761 0.771 0.743
    3.27 PAPEFT-LE 0.744 0.774 0.773 0.769 0.760 0.783 0.767 1.77 PAPEFT-LoRA 0.824
    0.824 0.823 0.841 0.843 0.853 0.835 1.52
- en: 'Table 3: Accuracy score and performance fluctuation with varying position of
    relevant document. We present the results of the original model, and the proposed
    PAPEFT framework equiped with three different parameter efficient fine-tuning
    techniques (PT for prompt tuning, LE for location encoding, and LoRA) on recommendation
    (REC) and link prediction (LP) tasks, with Longchat-13b-16k and Vicuna-13b-v1.5-16k
    as the base model.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同位置相关文档的准确度得分和性能波动。我们展示了原始模型和配备三种不同参数高效微调技术（PT 代表提示微调，LE 代表位置编码，LoRA）的
    PAPEFT 框架在推荐（REC）和链接预测（LP）任务上的结果，以 Longchat-13b-16k 和 Vicuna-13b-v1.5-16k 作为基础模型。
- en: 6.1 Experimental Settings
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设置
- en: 'Our introduced PAPEFT framework is composed of two main components: the data
    ordering permutation augmentation technique, and the parameter-efficient fine-tuning
    (PEFT) module. For an in-depth evaluation of the PEFT module, we have chosen three
    different choices. This choice is designed to cover a broad spectrum of tunable
    parameters and to evaluate the effectiveness of our specially designed location
    encoding soft prompt adapter. To differentiate between these configurations, we
    designate the variant equipped with the location encoding soft prompt adapter
    as PAPEFT-LE. The variant employing a straightforward prompt tuning adapter (Lester
    et al., [2021](#bib.bib13)), which shares the same architectural framework as
    the location encoding soft prompt adapter but removes the input of relative document
    locations, is termed PAPEFT-PT. Lastly, the variant incorporating a LoRA (Hu et al.,
    [2021](#bib.bib10)) adapter is referred to as PAPEFT-LoRA. Table [4](#S6.T4 "Table
    4 ‣ 6.2 Effectiveness Results ‣ 6 Experiments ‣ Position-Aware Parameter Efficient
    Fine-Tuning Approach for Reducing Positional Bias in LLMs") displays the tunable
    parameters of the three adapters.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入的PAPEFT框架由两个主要组件组成：数据排序排列增强技术和参数高效微调（PEFT）模块。为了对PEFT模块进行深入评估，我们选择了三种不同的选项。这个选择旨在涵盖广泛的可调参数，并评估我们特别设计的位置编码软提示适配器的有效性。为了区分这些配置，我们将配备位置编码软提示适配器的变体称为PAPEFT-LE。采用简单提示调优适配器（Lester等，[2021](#bib.bib13)）的变体，其架构框架与位置编码软提示适配器相同，但去除了相对文档位置的输入，称为PAPEFT-PT。最后，包含LoRA（Hu等，[2021](#bib.bib10)）适配器的变体被称为PAPEFT-LoRA。表[4](#S6.T4
    "Table 4 ‣ 6.2 Effectiveness Results ‣ 6 Experiments ‣ Position-Aware Parameter
    Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs")显示了这三种适配器的可调参数。
- en: Augmented Datasets Details.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 扩充数据集细节。
- en: During data augmentation phase, we generated five permutations of each document
    set for REC, and three for LP tasks according to the size of datasets. We select
    Longchat-13b-16k (Li et al., [2023](#bib.bib15)) and Vicuna-13b-v1.5-16k (Chiang
    et al., [2023](#bib.bib2)) as the base model for their proficiency in handling
    long context windows. Statistics information about the datasets size can be found
    in Appendix [A](#A1 "Appendix A Appendix ‣ Position-Aware Parameter Efficient
    Fine-Tuning Approach for Reducing Positional Bias in LLMs") Table [5](#A1.T5 "Table
    5 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs") and Table [6](#A1.T6 "Table 6 ‣ Appendix
    A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing
    Positional Bias in LLMs").
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据增强阶段，我们为REC生成了每个文档集的五种排列，为LP任务根据数据集大小生成了三种排列。我们选择了Longchat-13b-16k（Li等，[2023](#bib.bib15)）和Vicuna-13b-v1.5-16k（Chiang等，[2023](#bib.bib2)）作为基模型，因为它们在处理长上下文窗口方面表现优异。有关数据集大小的统计信息可以在附录[A](#A1
    "Appendix A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs") 表[5](#A1.T5 "Table 5 ‣ Appendix A Appendix
    ‣ Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional
    Bias in LLMs") 和表[6](#A1.T6 "Table 6 ‣ Appendix A Appendix ‣ Position-Aware Parameter
    Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs")中找到。
- en: Implementation Details.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实施细节。
- en: For efficient fine-tuning, we enabled 4-bit loading of model. The training was
    conducted with a sequence length of 16,384 tokens, padding enabled to match this
    length. The soft prompt adapter is featured with a two-layer MLP encoder of 1024
    hidden size. For optimization, we employed the paged AdamW 32-bit optimizer with
    a cosine learning rate scheduler, setting the initial learning rate at $2e^{-4}$
    as the setting. We use standard next-token prediction as our training objective.
    All experiments were done using eight NVIDIA A100-40GB GPUs. Code can be found
    in [https://anonymous.4open.science/r/llm_long_context-E9CF](https://anonymous.4open.science/r/llm_long_context-E9CF).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效微调，我们启用了模型的4位加载。训练使用了长度为16,384个标记的序列，启用了填充以匹配此长度。软提示适配器配备了一个两层的MLP编码器，隐藏层大小为1024。优化方面，我们采用了分页AdamW
    32位优化器和余弦学习率调度器，将初始学习率设置为$2e^{-4}$。我们使用标准的下一个标记预测作为训练目标。所有实验均使用八个NVIDIA A100-40GB
    GPU完成。代码可以在[https://anonymous.4open.science/r/llm_long_context-E9CF](https://anonymous.4open.science/r/llm_long_context-E9CF)找到。
- en: 6.2 Effectiveness Results
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 效果结果
- en: 'Our key findings on accuracy results of REC and LP tasks, as shown in Figure [3](#S6.T3
    "Table 3 ‣ 6 Experiments ‣ Position-Aware Parameter Efficient Fine-Tuning Approach
    for Reducing Positional Bias in LLMs"), our proposed PAPEFT framework and original
    models are summarized as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对REC和LP任务准确性结果的关键发现，如图[3](#S6.T3 "表 3 ‣ 6 实验 ‣ 位置感知参数高效微调方法")所示，我们提出的PAPEFT框架与原始模型的总结如下：
- en: 'Positional Bias in Original Models: The performance of both Longchat-13b-16k
    and Vicuna-13b-v1.5-16k models demonstrated significantly noticeable fluctuations,
    with each model exhibiting distinct patterns of variability across different tasks.
    These fluctuations are indicative of prevalent positional bias within the original
    models.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 原始模型中的位置偏差：Longchat-13b-16k和Vicuna-13b-v1.5-16k模型的性能表现出显著的波动，每个模型在不同任务中显示出不同的变异模式。这些波动表明原始模型中存在普遍的位置偏差。
- en: 'Reduction in Positional Fluctuations: The PAPEFT framework achieves a substantial
    reduction in positional bias, with an average decrease in performance variance
    of 54.19% for recommendation tasks and 58.72% for link prediction tasks. This
    improvement signifies that the integrated approach of data augmentation and position-aware
    fine tuning effectively guides the LLM to treat all candidates within the input
    context more evenly, thus mitigating positional bias.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 位置波动的减少：PAPEFT框架在位置偏差方面取得了显著的减少，推荐任务的性能方差平均减少了54.19%，链路预测任务的性能方差平均减少了58.72%。这一改进表明，数据增强和位置感知微调的综合方法有效地引导了LLM在输入上下文中更加均匀地对待所有候选项，从而减轻了位置偏差。
- en: 'Enhancement in Model Performance: The PAPEFT framework yielded substantial
    improvements in model performance with an average increase of 57.3% for the recommendation
    task and 64.4% for the link prediction task compared to the original model. These
    improvements demonstrate PAPEFT’s ability to not only reduce performance bias
    but also to enhance the model’s task-specific effectiveness.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能的提升：与原始模型相比，PAPEFT框架在推荐任务上性能提升了57.3%，在链路预测任务上性能提升了64.4%。这些改进展示了PAPEFT不仅能够减少性能偏差，还能增强模型在特定任务上的有效性。
- en: 'Efficacy in Location Encoding Soft Prompt Module: Furthermore, when comparing
    PAPEFT-LE to the prompt tuning method—which lacks location encoding but has an
    equivalent number of tunable parameters—PAPEFT-LE achieves an additional average
    reduction in performance fluctuations of 1.54% and achieves an average performance
    improvement of 3.1% over the prompt tuning method. This highlights the benefits
    of integrating explicit document locations via the soft prompt tuning module,
    underscoring its effectiveness.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码软提示模块的有效性：此外，在将PAPEFT-LE与缺乏位置编码但具有相同数量可调参数的提示微调方法进行比较时，PAPEFT-LE在性能波动上额外减少了1.54%的平均值，并且比提示微调方法平均提高了3.1%的性能。这突显了通过软提示微调模块集成显式文档位置的好处，强调了其有效性。
- en: 'Parameter Efficiency of Location Encoding Soft Prompt Module: As highlighted
    in Table [4](#S6.T4 "Table 4 ‣ 6.2 Effectiveness Results ‣ 6 Experiments ‣ Position-Aware
    Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"),
    PAPEFT-LE utilizes 23.87 times fewer parameters compared to the PAPEFT-LoRA method.
    Despite this, the results demonstrate that PAPEFT-LE almost achieves comparable
    performance improvement and variance deduction to the PAPEFT-LoRA method, which
    highlighting the parameter efficiency of location encoding soft prompt adapter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码软提示模块的参数效率：如表[4](#S6.T4 "表 4 ‣ 6.2 效果结果 ‣ 6 实验 ‣ 位置感知参数高效微调方法")所示，PAPEFT-LE使用的参数比PAPEFT-LoRA方法少了23.87倍。尽管如此，结果表明PAPEFT-LE几乎达到了与PAPEFT-LoRA方法相当的性能改进和方差减少，这突显了位置编码软提示适配器的参数效率。
- en: 'Task # Para Ratio (%) to original model PAPEFT-LoRA 125,173,760 0.95 PAPEFT-LE
    5,250,048 0.04 PAPEFT-PT 5,250,048 0.04'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '任务 # 参数 比率 (%) 相较于原始模型 PAPEFT-LoRA 125,173,760 0.95 PAPEFT-LE 5,250,048 0.04
    PAPEFT-PT 5,250,048 0.04'
- en: 'Table 4: Number of tunable parameters comparison.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：可调参数数量对比。
- en: 7 Conclusion
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we conducted a comprehensive investigation into the phenomenon
    of positional bias in large language models across diverse tasks that require
    retrieving relevant knowledge. Through empirical results, we demonstrated that
    current LLMs exhibit a noticeable positional preference over the candidate lists.
    We showed that merely adopting prompt-based solution is insufficient to address
    the positional bias issue. In order to address the positional bias issue of LLMs,
    we introduced a data augmentation technique to permute the ordering of candidates
    within the textual context, and a position aware fine tuning module, which explicitly
    integrates the locational context of each document into the LLMs’ input through
    a trainable adapter module. Our extensive experiments in recommendation and link
    prediction tasks demonstrate that the proposed module can substantially mitigate
    positional bias with limited tunable parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们对大语言模型在需要检索相关知识的各种任务中出现的定位偏差现象进行了全面调查。通过实证结果，我们展示了当前的LLM在候选列表中表现出明显的定位偏好。我们表明，仅仅采用基于提示的方法不足以解决定位偏差问题。为了应对LLM的定位偏差问题，我们引入了一种数据增强技术来打乱文本上下文中候选项的顺序，并且提出了一种位置感知的微调模块，该模块通过一个可训练的适配器模块将每个文档的定位上下文显式地集成到LLM的输入中。我们在推荐和链接预测任务中的广泛实验表明，所提出的模块可以在有限的可调参数下显著减轻定位偏差。
- en: References
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving
    from trillions of tokens. In *International conference on machine learning*, pp. 
    2206–2240\. PMLR, 2022.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, 等等。通过从万亿标记中检索来改进语言模型。发表于*国际机器学习会议*，第2206–2240页。PMLR，2022。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. Vicuna：一个开源聊天机器人，GPT-4表现的90%* ChatGPT质量，2023年3月。网址
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等等。Palm：通过路径扩展语言建模。*机器学习研究杂志*，24(240):1–113，2023。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher
    Ré. Flashattention：具有IO感知的快速且内存高效的精确注意力。*神经信息处理系统进展*，35:16344–16359，2022。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora：量化LLM的高效微调。*arXiv预印本 arXiv:2305.14314*，2023。
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora：量化LLM的高效微调。*神经信息处理系统进展*，36，2024。
- en: Guu et al. (2020a) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and
    Mingwei Chang. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pp.  3929–3938\. PMLR, 2020a.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2020a) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei
    Chang. 检索增强语言模型预训练。发表于*国际机器学习会议*，第3929–3938页。PMLR，2020a。
- en: 'Guu et al. (2020b) Kelvin Guu et al. Realm: Retrieval-augmented language model
    pre-training. In *Proceedings of ICML*, 2020b.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guu et al. (2020b) Kelvin Guu et al. Realm: 检索增强语言模型预训练。发表于*ICML会议论文集*，2020b。'
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International conference
    on machine learning*, pp.  2790–2799\. PMLR, 2019.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等（2019）Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
    Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan 和 Sylvain Gelly。参数高效的迁移学习用于
    NLP。在 *国际机器学习会议*，第 2790–2799 页。PMLR，2019。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen。Lora：大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021。
- en: 'Jin et al. (2023) Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi
    Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, et al. Amazon-m2: A multilingual
    multi-locale shopping session dataset for recommendation and text generation.
    *arXiv preprint arXiv:2307.09688*, 2023.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等（2023）Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen,
    Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li 等。Amazon-m2：一个用于推荐和文本生成的多语言多地点购物会话数据集。*arXiv
    预印本 arXiv:2307.09688*，2023。
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering
    research. *Transactions of the Association for Computational Linguistics*, 7:453–466,
    2019.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael
    Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
    Devlin, Kenton Lee 等。自然问题：一个用于问答研究的基准。*计算语言学协会会刊*，7：453–466，2019。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等（2021）Brian Lester, Rami Al-Rfou 和 Noah Constant。参数高效提示调优的规模力量。*arXiv
    预印本 arXiv:2104.08691*，2021。
- en: Lewis et al. (2020) Patrick Lewis et al. Retrieval-augmented generation for
    knowledge-intensive nlp tasks. In *Proceedings of NeurIPS*, 2020.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Patrick Lewis 等。检索增强生成用于知识密集型 NLP 任务。在 *NeurIPS 会议论文集*，2020。
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length?, June 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
    E. Gonzalez, Ion Stoica, Xuezhe Ma 和 Hao Zhang。开源 LLM 在上下文长度上的实际承诺有多长？，2023年6月。网址
    [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang（2021）Xiang Lisa Li 和 Percy Liang。前缀调优：优化生成的连续提示。*arXiv 预印本 arXiv:2101.00190*，2021。
- en: 'Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni 和 Percy Liang。迷失在中间：语言模型如何使用长上下文。*arXiv 预印本 arXiv:2307.03172*，2023a。
- en: 'Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023b.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023b）Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi
    和 Graham Neubig。预训练、提示和预测：自然语言处理中的提示方法系统综述。*ACM Computing Surveys*，55(9)：1–35，2023b。
- en: 'Liu et al. (2023c) Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al.
    Deja vu: Contextual sparsity for efficient llms at inference time. In *International
    Conference on Machine Learning*, pp.  22137–22176\. PMLR, 2023c.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023c）Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re 等。Déjà vu：推理时的上下文稀疏以提高
    LLM 效率。在 *国际机器学习会议*，第 22137–22176 页。PMLR，2023c。
- en: 'Min et al. (2021) Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
    Metaicl: Learning to learn in context. *arXiv preprint arXiv:2110.15943*, 2021.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等（2021）Sewon Min, Mike Lewis, Luke Zettlemoyer 和 Hannaneh Hajishirzi。Metaicl：在上下文中学习学习。*arXiv
    预印本 arXiv:2110.15943*，2021。
- en: Naumov et al. (2019) Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi,
    Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean
    Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization
    and recommendation systems. *arXiv preprint arXiv:1906.00091*, 2019.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naumov 等（2019）**马克西姆·瑙莫夫**、**德赫瓦萨·穆迪格雷**、**郝君·迈克尔·史**、**简宇·黄**、**纳拉延·苏达拉曼**、**郑秀**、**肖东·王**、**乌迪特·古普塔**、**卡罗尔-简·吴**、**阿利森·G·阿佐利尼**
    等。深度学习推荐模型用于个性化和推荐系统。*arXiv 预印本 arXiv:1906.00091*，2019年。
- en: Ravaut et al. (2023) Mathieu Ravaut, Shafiq Joty, Aixin Sun, and Nancy F Chen.
    On position bias in summarization with large language models. *arXiv preprint
    arXiv:2310.10570*, 2023.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravaut 等（2023）**马修·拉沃特**、**沙菲克·乔提**、**艾欣·孙** 和 **南希·F·陈**。关于大型语言模型总结中的位置偏差。*arXiv
    预印本 arXiv:2310.10570*，2023年。
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. How much
    knowledge can you pack into the parameters of a language model? *arXiv preprint
    arXiv:2002.08910*, 2020.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts 等（2020）**亚当·罗伯茨**、**科林·拉费尔** 和 **诺亚姆·沙泽尔**。你能将多少知识包装进语言模型的参数中？*arXiv
    预印本 arXiv:2002.08910*，2020年。
- en: Vaswani et al. (2017) Ashish Vaswani et al. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）**阿希什·瓦斯瓦尼** 等。注意力就是你所需要的一切。*神经信息处理系统进展*，30，2017年。
- en: 'Wang et al. (2020) Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu,
    Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not
    enough. *Quantitative Science Studies*, 1(1):396–413, 2020.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020）**宽山·王**、**志宏·申**、**千元·黄**、**谢汉·吴**、**宇霄·董** 和 **安舒尔·卡纳基亚**。微软学术图谱：当专家不够时。*定量科学研究*，1(1)：396–413，2020年。
- en: 'Yasunaga et al. (2021) Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy
    Liang, and Jure Leskovec. Qa-gnn: Reasoning with language models and knowledge
    graphs for question answering. *arXiv preprint arXiv:2104.06378*, 2021.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasunaga 等（2021）**三矢·安永**、**洪宇·任**、**安托万·博塞鲁特**、**帕西·梁** 和 **尤尔·莱斯科维奇**。Qa-gnn：利用语言模型和知识图谱进行问答。*arXiv
    预印本 arXiv:2104.06378*，2021年。
- en: Zheng et al. (2023) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. On large language models’ selection bias in multi-choice questions. *arXiv
    preprint arXiv:2309.03882*, 2023.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）**朱杰·郑**、**郝周**、**范东·孟**、**杰·周** 和 **敏力·黄**。关于大型语言模型在多选题中的选择偏差。*arXiv
    预印本 arXiv:2309.03882*，2023年。
- en: Appendix A Appendix
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: In Table [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ Position-Aware Parameter
    Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs"), we show
    the basic statistic information about the datasets used in experiments. In Table [6](#A1.T6
    "Table 6 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs"), we show the statistic information
    about the training and test datasets used in fine tune phase.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient
    Fine-Tuning Approach for Reducing Positional Bias in LLMs")中，我们展示了实验中使用的数据集的基本统计信息。在表[6](#A1.T6
    "Table 6 ‣ Appendix A Appendix ‣ Position-Aware Parameter Efficient Fine-Tuning
    Approach for Reducing Positional Bias in LLMs")中，我们展示了微调阶段使用的训练集和测试集的数据统计信息。
- en: '| Task | $K$ | Average # of Words |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | $K$ | 平均词数 |'
- en: '| --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| REC | 20 | 4.2k |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| REC | 20 | 4.2k |'
- en: '| LP | 20 | 6.2k |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LP | 20 | 6.2k |'
- en: 'Table 5: Data statistics for test inference. Here $K$ denotes the number of
    potential items to select.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：测试推断的数据统计。这里 $K$ 表示选择的潜在项目数量。
- en: '| Task | # Train | # Test |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 训练集数量 | 测试集数量 |'
- en: '| --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| REC | 2,000 | 1,000 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| REC | 2,000 | 1,000 |'
- en: '| LP | 10,000 | 3,000 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LP | 10,000 | 3,000 |'
- en: 'Table 6: Fine-tune data train test splits statistics.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：微调数据训练测试拆分统计。
- en: 'Table 7: Examples of zero-shot prompts used in different domains.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同领域使用的零-shot 提示示例。
- en: '| Task setting | Prompt to LLM |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 任务设置 | 提示给 LLM |'
- en: '| --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Rec | Task: Using a user’s historical purchase data from Amazon.com, identify
    one product from a distinct list of potential products that you predict the user
    will most likely purchase next.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '| Rec | 任务：使用用户在 Amazon.com 上的历史购买数据，从一个不同的潜在产品列表中，识别出用户最有可能下一步购买的产品。'
- en: 'Belows are 2 historical purchased products:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 2 个历史购买产品：
- en: 'Bought Product [1](Title: New brothread Wash Away)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '购买的产品 [1](Title: New brothread Wash Away)'
- en: 'Bought Product [2](Title: Crafter’s Companion Spray & Shine, Varnish)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '购买的产品 [2](Title: Crafter’s Companion Spray & Shine, Varnish)'
- en: 'Belows are 20 potential products to consider:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 20 个潜在产品：
- en: 'Potential Product [1](Title: Clarins Eau Dynamisante Shower Gel 150ml) Potential
    Product [2](Title: My Living World LW105 Window Bird Feeder)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '潜在产品 [1](Title: Clarins Eau Dynamisante Shower Gel 150ml) 潜在产品 [2](Title: My
    Living World LW105 Window Bird Feeder)'
- en: …
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Potential Product [20](Title: BGS Do it yourself— Cutting Box with Fine Saw)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '潜在产品 [20](标题: BGS 自制— 用精细锯切割盒)'
- en: 'Question: Now you need to predict ONLY one product from the potential products
    that the user will most likely purchase next. What is your prediction: |'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '问题: 现在你需要预测用户最有可能下一个购买的单一产品。你的预测是什么: |'
- en: '| LP | Task: Based on the title and abstract of a research paper, determine
    one paper from a list of potential papers that the original paper is most likely
    to cite.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '| LP | 任务: 根据研究论文的标题和摘要，从潜在论文列表中确定一篇最有可能被原始论文引用的论文。'
- en: 'Below is the provided research paper along with its title and abstract: style
    aggregated network for facial landmark detection (Abstract: …)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是提供的研究论文及其标题和摘要: 样式聚合网络用于面部标志检测 (摘要: …)'
- en: 'The following are 20 potential papers for consideration: Potential Paper [1](decafa
    deep convolutional cascade for face alignment in the wild) (Abstract: …) Potential
    Paper [2](do altmetrics work for assessing research quality ) (Abstract: …) …
    Potential Paper [20](ai based pilgrim detection using convolutional neural networks
    ) (Abstract: …) Question: Predict ONE paper from the given potential papers that
    the original document would most probably cite. Please provide the predicted paper
    and a brief description of why you think it is the most likely choice: |'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是 20 篇供考虑的潜在论文: 潜在论文 [1](decafa 深度卷积级联用于野外面部对齐) (摘要: …) 潜在论文 [2](替代指标是否有效评估研究质量)
    (摘要: …) … 潜在论文 [20](基于 AI 的朝圣者检测使用卷积神经网络) (摘要: …) 问题: 预测原始文档最有可能引用的单篇论文。请提供预测论文和简要描述为何你认为这是最可能的选择:
    |'
- en: 'Table 8: Prompt engineering examples, few-shot learning and hierarachical settings.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 提示工程示例，少量学习和分层设置。'
- en: '| Prompt strategy | Prompt to LLM |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 提示策略 | 提示给 LLM |'
- en: '| --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| FEW-shot | Task: [Task Description]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '| FEW-shot | 任务: [任务描述]'
- en: 'Belows are 3 examples:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 3 个示例：
- en: Example [1] [Exampel 1]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [1] [示例 1]
- en: Example [2] [Exampel 2]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [2] [示例 2]
- en: Example [3] [Exampel 3]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [3] [示例 3]
- en: 'Belows are 2 historical purchased products:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 2 个历史购买产品：
- en: Bought Product [1][Historical Bought Product 1]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 已购买产品 [1][历史已购买产品 1]
- en: Bought Product [2][Historical Bought Product 2]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 已购买产品 [2][历史已购买产品 2]
- en: 'Belows are 20 potential products to consider:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 20 个潜在产品供考虑：
- en: Potential Product [1] [Potential Product 1] Potential Product [2] [Potential
    Product 2]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在产品 [1] [潜在产品 1] 潜在产品 [2] [潜在产品 2]
- en: …
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Potential Product [20] [Potential Product 20] Question: Now you need to predict
    ONLY one product from the potential products that the user will most likely purchase
    next. What is your prediction: |'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '潜在产品 [20] [潜在产品 20] 问题: 现在你需要预测用户最有可能下一个购买的单一产品。你的预测是什么: |'
- en: '| Hierar-chical | Task: [Task Description]'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分层 | 任务: [任务描述]'
- en: 'Given the inherent challenge of selecting the prime candidate directly from
    a broad list, approach this assignment hierarchically: Start by segmenting the
    products into 5 equal groups. For each segmented group, determine the product
    with the highest purchase likelihood. For instance, select the most likely one
    from group ([1]-[4]), followed by the top pick from group ([5]-[8]), and so on.
    After narrowing down to the top products from each group, decide which among them
    stands the best chance of being the user’s next purchase.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于从广泛的列表中直接选择最佳候选者的固有挑战，请以分层的方式处理此任务：首先将产品分成 5 个相等的组。对于每个分组，确定购买可能性最高的产品。例如，从组
    ([1]-[4]) 中选择最可能的产品，然后是组 ([5]-[8]) 中的最佳选择，以此类推。在缩小到每组的最佳产品后，决定其中哪个最有可能成为用户的下一次购买。
- en: 'Belows are 2 historical purchased products:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 2 个历史购买产品：
- en: Bought Product [1][Historical Bought Product 1]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 已购买产品 [1][历史已购买产品 1]
- en: Bought Product [2][Historical Bought Product 2]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 已购买产品 [2][历史已购买产品 2]
- en: 'Belows are 20 potential products to consider:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 20 个潜在产品供考虑：
- en: Potential Product [1] [Potential Product 1] Potential Product [2] [Potential
    Product 2]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在产品 [1] [潜在产品 1] 潜在产品 [2] [潜在产品 2]
- en: …
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Potential Product [20] [Potential Product 20] Question: Now you need to predict
    ONLY one product from the potential products that the user will most likely purchase
    next. What is your prediction: |'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '潜在产品 [20] [潜在产品 20] 问题: 现在你需要预测用户最有可能下一个购买的单一产品。你的预测是什么: |'
