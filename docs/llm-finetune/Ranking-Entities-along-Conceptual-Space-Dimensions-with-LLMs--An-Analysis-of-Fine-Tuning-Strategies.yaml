- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:38:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:38:34'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于概念空间维度的实体排名与LLMs：微调策略分析
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15337](https://ar5iv.labs.arxiv.org/html/2402.15337)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15337](https://ar5iv.labs.arxiv.org/html/2402.15337)
- en: Nitesh Kumar    Usashi Chatterjee    Steven Schockaert
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nitesh Kumar    Usashi Chatterjee    Steven Schockaert
- en: Cardiff NLP, School of Computer Science and Informatics
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卡迪夫大学计算机科学与信息学学院
- en: Cardiff University, United Kingdom
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卡迪夫大学，英国
- en: '{kumarn8,chatterjeeu,schockaerts1}@cardiff.ac.uk'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{kumarn8,chatterjeeu,schockaerts1}@cardiff.ac.uk'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Conceptual spaces represent entities in terms of their primitive semantic features.
    Such representations are highly valuable but they are notoriously difficult to
    learn, especially when it comes to modelling perceptual and subjective features.
    Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged
    as a promising strategy. However, existing work has been limited to probing pre-trained
    LLMs using relatively simple zero-shot strategies. We focus in particular on the
    task of ranking entities according to a given conceptual space dimension. Unfortunately,
    we cannot directly fine-tune LLMs on this task, because ground truth rankings
    for conceptual space dimensions are rare. We therefore use more readily available
    features as training data and analyse whether the ranking capabilities of the
    resulting models transfer to perceptual and subjective features. We find that
    this is indeed the case, to some extent, but having perceptual and subjective
    features in the training data seems essential for achieving the best results.
    We furthermore find that pointwise ranking strategies are competitive against
    pairwise approaches, in defiance of common wisdom.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 概念空间根据实体的原始语义特征进行表示。这些表示非常有价值，但学习起来通常非常困难，尤其是在建模感知和主观特征时。从大型语言模型（LLMs）中提取概念空间最近成为一种有前途的策略。然而，现有工作仅限于使用相对简单的零样本策略探测预训练的LLMs。我们特别关注根据给定的概念空间维度对实体进行排名的任务。不幸的是，我们无法直接在此任务上微调LLMs，因为概念空间维度的真实排名非常稀缺。因此，我们使用更容易获得的特征作为训练数据，并分析结果模型的排名能力是否能转移到感知和主观特征上。我们发现，确实存在这种转移，但在训练数据中包含感知和主观特征似乎对获得最佳结果至关重要。此外，我们还发现，逐点排名策略在与对比策略的竞争中表现良好，违背了常规智慧。
- en: 'Ranking Entities along Conceptual Space Dimensions with LLMs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概念空间维度的实体排名与LLMs：
- en: An Analysis of Fine-Tuning Strategies
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 微调策略分析
- en: Nitesh Kumar  and Usashi Chatterjee  and Steven Schockaert Cardiff NLP, School
    of Computer Science and Informatics Cardiff University, United Kingdom {kumarn8,chatterjeeu,schockaerts1}@cardiff.ac.uk
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Nitesh Kumar 和 Usashi Chatterjee 和 Steven Schockaert 卡迪夫大学计算机科学与信息学学院，英国 {kumarn8,chatterjeeu,schockaerts1}@cardiff.ac.uk
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Knowledge graphs (KGs) have emerged as the *de facto* standard for representing
    knowledge in areas such as Natural Language Processing Schneider et al. ([2022](#bib.bib35)),
    Recommendation Guo et al. ([2022](#bib.bib14)) and Search Reinanda et al. ([2020](#bib.bib33)).
    However, much of the knowledge that is needed in applications is about *graded*
    properties, e.g. recipes being healthy, movies being original or cities being
    kids-friendly. Such knowledge is easiest to model in terms of rankings: we can
    rank recipes according to how healthy they are even if we cannot make a hard decision
    about which ones are healthy and which ones are not. For this reason, we argue
    that conceptual spaces Gärdenfors ([2000](#bib.bib10)) should be used, alongside
    knowledge graphs, in many settings.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KGs）已成为自然语言处理（Schneider et al. ([2022](#bib.bib35)））、推荐系统（Guo et al. ([2022](#bib.bib14)））和搜索（Reinanda
    et al. ([2020](#bib.bib33)））等领域的*事实*标准。然而，应用中需要的许多知识涉及*等级*属性，例如食谱是否健康、电影是否原创或城市是否适合儿童。这类知识最容易通过排名来建模：即使我们无法对哪些食谱健康、哪些不健康做出明确判断，我们也可以根据其健康程度对食谱进行排名。因此，我们认为在许多情况下，应将概念空间（Gärdenfors
    ([2000](#bib.bib10)））与知识图谱一起使用。
- en: A conceptual space specifies a set of quality dimensions, which correspond to
    primitive semantic features. For instance, in a conceptual space of movies, we
    might have a quality dimensions reflecting how original a movie is. Entities are
    represented as vectors, specifying a suitable feature value for each quality dimension.
    While the framework of conceptual spaces is more general, we will essentially
    view quality dimensions as rankings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 概念空间指定了一组质量维度，这些维度对应于基本的语义特征。例如，在电影的概念空间中，我们可能会有反映电影原创性的质量维度。实体被表示为向量，为每个质量维度指定一个合适的特征值。尽管概念空间的框架更为通用，我们将基本上把质量维度视为排名。
- en: Conceptual spaces have the potential to play a central role in various knowledge-intensive
    applications. In the context of recommendation, for instance, they could clearly
    complement the factual knowledge that is captured by typical KGs (e.g. modelling
    the style of a movie, rather than who directed it), making it easier to infer
    user preferences from previous ratings. They could also be used to make recommendations
    more controllable, as in the case of critiquing-based systems, allowing users
    to specify feedback of the form “like this movie, but more kids-friendly” Chen
    and Pu ([2012](#bib.bib6)); Vig et al. ([2012](#bib.bib40)). Conceptual spaces
    furthermore serve as a natural interface between neural and symbolic representations
    Aisbett and Gibbon ([2001](#bib.bib2)), and may thus enable principled explainable
    AI methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 概念空间有潜力在各种知识密集型应用中发挥核心作用。例如，在推荐系统的背景下，它们可以明显补充由典型知识图谱（KGs）捕捉的事实知识（例如，建模电影的风格，而不是谁导演了它），从而更容易从先前的评分中推断用户偏好。它们还可以用于使推荐更加可控，例如在基于评论的系统中，允许用户指定“喜欢这部电影，但更适合儿童”的反馈
    Chen 和 Pu（[2012](#bib.bib6)）；Vig 等（[2012](#bib.bib40)）。此外，概念空间还作为神经和符号表示之间的自然接口
    Aisbett 和 Gibbon（[2001](#bib.bib2)），因此可能使得原则性的可解释 AI 方法成为可能。
- en: 'However, the task of learning conceptual spaces has proven remarkably challenging.
    The issue of reporting bias Gordon and Durme ([2013](#bib.bib13)), in particular,
    has been regarded as a fundamental obstacle: the knowledge captured by conceptual
    spaces is often so obvious to humans that it is rarely stated in text. For instance,
    the phrase “green banana” is more frequent in text than “yellow banana” Paik et al.
    ([2021](#bib.bib29)), as the colour is typically not specified when yellow bananas
    are discussed. Paik et al. ([2021](#bib.bib29)) found that predictions of Language
    Models (LMs) about the colour of objects were correlated with the distribution
    of colour terms in text corpora, rather than with human judgements, suggesting
    that LMs cannot overcome the challenges posed by reporting bias. However, Liu
    et al. ([2022a](#bib.bib22)) found that larger LMs can perform much better on
    this task. Going beyond colour, Chatterjee et al. ([2023](#bib.bib5)) evaluated
    the ability of LLMs to predict taste-related features, such as sweetness and saltiness,
    obtaining mixed results: the rankings predicted by LLMs, in a zero-shot setting,
    had a reasonable correlation with human judgments but they were not consistently
    better than those produced by a fine-tuned BERT Devlin et al. ([2019](#bib.bib9))
    model.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，学习概念空间的任务证明了极具挑战性。报告偏差的问题，特别是戈登和杜尔梅（[2013](#bib.bib13)）提出的，已被视为一个根本障碍：概念空间所捕捉的知识通常对人类来说是如此显而易见，以至于很少在文本中提及。例如，短语“绿色香蕉”在文本中出现的频率高于“黄色香蕉”
    Paik 等（[2021](#bib.bib29)），因为在讨论黄色香蕉时通常不会指定颜色。Paik 等（[2021](#bib.bib29)）发现，语言模型（LMs）对物体颜色的预测与文本语料库中颜色术语的分布相关，而不是与人类判断相关，这表明
    LM 无法克服报告偏差带来的挑战。然而，Liu 等（[2022a](#bib.bib22)）发现更大的 LM 在这个任务上表现得更好。超越颜色，Chatterjee
    等（[2023](#bib.bib5)）评估了 LLM 预测味觉特征（如甜度和咸度）的能力，结果喜忧参半：LLM 在零样本设置下预测的排名与人类判断有合理的相关性，但其表现并不总是优于经过微调的
    BERT Devlin 等（[2019](#bib.bib9)）模型。
- en: 'In this paper, we analyse whether LLMs can be fine-tuned to extract better
    conceptual space representations. The difficulty is that ground truth rankings
    are typically not available when it comes to perceptual and subjective features,
    outside a few notable exceptions such as the aforementioned taste dataset. We
    therefore explore whether more readily available features can be used for fine-tuning
    the model. For instance, we can obtain ground truth rankings from Wikidata entities
    with numerical attributes (e.g. the length of rivers, the birth date of people,
    or the population of cities) and then use these rankings to fine-tune an LLM.
    We furthermore compare two different strategies for ranking entities with an LLM:
    the *pointwise* approach uses an LLM to assign a score to each entity, given some
    feature, while the *pairwise* approach uses an LLM to decide which among two given
    entities has the feature to the greatest extent. Our contributions and findings
    can be summarised as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们分析了LLMs是否可以通过微调以提取更好的概念空间表示。问题在于，感知和主观特征通常没有真实的排名，除了少数几个显著的例外，如前述的口味数据集。因此，我们探索是否可以利用更容易获得的特征来微调模型。例如，我们可以从具有数值属性的Wikidata实体中获得真实的排名（如河流的长度、人物的出生日期或城市的人口），然后使用这些排名来微调LLM。我们进一步比较了使用LLM进行实体排名的两种不同策略：*点对点*方法利用LLM为每个实体分配一个分数，而*成对*方法利用LLM决定两个给定实体中哪一个特征更突出。我们的贡献和发现总结如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We evaluate on three datasets which have not previously been used for studying
    language models: a dataset of rocks, a dataset of movies and books, and a dataset
    about Wikidata entities. We use these datasets alongside datasets about taste
    Chatterjee et al. ([2023](#bib.bib5)) and physical properties Li et al. ([2023](#bib.bib21)).'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在三个之前未曾用于研究语言模型的数据集上进行评估：一个岩石数据集，一个电影和书籍数据集，以及一个关于Wikidata实体的数据集。我们将这些数据集与口味数据集（Chatterjee
    et al. ([2023](#bib.bib5))）和物理属性数据集（Li et al. ([2023](#bib.bib21))）一起使用。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyse whether fine-tuning LLMs on features from one domain (e.g. taste)
    can improve their ability to rank entities in different domains (e.g. rocks).
    We find this indeed largely to be the case, as long as the training data also
    contains perceptual or subjective features.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了是否可以通过在一个领域（例如口味）上的特征微调LLMs来提高它们在不同领域（例如岩石）中排名实体的能力。我们发现，确实在大多数情况下可以，只要训练数据中也包含感知或主观特征。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We compare pointwise and pairwise approaches for ranking entities with LLMs.
    Despite the fact that pairwise approaches have consistently been found superior
    for LLM-based document ranking Nogueira et al. ([2019](#bib.bib27)); Gienapp et al.
    ([2022](#bib.bib12)); Qin et al. ([2023](#bib.bib32)), when it comes to ranking
    entities, we find the pointwise approach to be highly effective.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们比较了使用LLMs进行实体排名的点对点方法和成对方法。尽管成对方法在基于LLM的文档排名中被一致发现更优（Nogueira et al. ([2019](#bib.bib27));
    Gienapp et al. ([2022](#bib.bib12)); Qin et al. ([2023](#bib.bib32))），但在排名实体时，我们发现点对点方法非常有效。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To obtain *rankings* from pairwise judgments, we need a suitable strategy for
    aggregating these judgments. We show the effectiveness of an SVM based strategy
    for this purpose. While this strategy is known to have desirable theoretical properties,
    it has not previously been considered in the context of language models, to the
    best of our knowledge
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从成对判断中获得*排名*，我们需要一种合适的策略来聚合这些判断。我们展示了一种基于SVM的策略的有效性。虽然这种策略已知具有理想的理论特性，但据我们所知，它之前在语言模型的背景下尚未被考虑过。
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LMs as Knowledge Bases
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LMs作为知识库
- en: Our focus in this paper is on extracting knowledge from language models. This
    idea of language models as knowledge bases was popularised by Petroni et al. ([2019](#bib.bib31)),
    who showed that the pre-trained BERT model captures various forms of factual knowledge,
    which can moreover be extracted using a simple prompt. Work in this area has focused
    on two rather distinct goals. On the one hand, probing tasks, such as the one
    proposed by Petroni et al. ([2019](#bib.bib31)), have been used as a mechanism
    for analysing and comparing different language models. On the other hand, extracting
    knowledge from LMs has also been studied as a practical tool for building or extending
    symbolic knowledge bases. This has been particularly popular for capturing types
    of knowledge which are not commonly found in traditional knowledge bases, such
    as commonsense knowledge Bosselut et al. ([2019](#bib.bib4)); West et al. ([2022](#bib.bib43));
    Yu et al. ([2023](#bib.bib45)). Several works have focused on distilling KGs from
    language models Cohen et al. ([2023](#bib.bib7)). Hao et al. ([2023](#bib.bib16))
    studies this problem for non-traditional relations such as “is capable of but
    not good at”. Along the same lines, Ushio et al. ([2023](#bib.bib39)) have focused
    on modelling relations that are a matter of degree, such as “is a competitor of”
    or “is similar to”. We can similarly think of the conceptual space dimensions
    that we consider in this paper as gradual properties.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的重点是从语言模型中提取知识。这种将语言模型视为知识库的想法由 Petroni et al. ([2019](#bib.bib31)) 推广，他们展示了预训练的
    BERT 模型捕捉了各种形式的事实知识，并且可以通过简单的提示进行提取。这一领域的工作集中在两个相当不同的目标上。一方面，像 Petroni et al.
    ([2019](#bib.bib31)) 提出的探测任务已被用作分析和比较不同语言模型的机制。另一方面，从语言模型中提取知识也被研究作为构建或扩展符号知识库的实用工具。这对于捕捉在传统知识库中不常见的知识类型尤为流行，如常识知识
    Bosselut et al. ([2019](#bib.bib4))；West et al. ([2022](#bib.bib43))；Yu et al.
    ([2023](#bib.bib45))。几项工作集中在从语言模型中提炼知识图谱（KGs）上 Cohen et al. ([2023](#bib.bib7))。Hao
    et al. ([2023](#bib.bib16)) 研究了诸如“有能力但不擅长”等非传统关系。沿着同样的思路，Ushio et al. ([2023](#bib.bib39))
    关注于建模那些程度性的关系，如“是竞争对手”或“是相似的”。我们同样可以将本文中考虑的概念空间维度视为渐进性质。
- en: Where the aforementioned approaches explicitly extract knowledge from an LM,
    the knowledge captured by LMs has also been used implicitly, by applying such
    models in a wide range of knowledge-intensive applications, including closed-book
    question answering Roberts et al. ([2020](#bib.bib34)), knowledge graph completion
    Yao et al. ([2019](#bib.bib44)), recommendation Sun et al. ([2019](#bib.bib38));
    Geng et al. ([2022](#bib.bib11)), entity typing Huang et al. ([2022](#bib.bib19))
    and ontology alignment He et al. ([2022](#bib.bib18)), to name just a few.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前述方法明确地从语言模型（LM）中提取知识一样，语言模型捕捉到的知识也被隐式使用，例如在广泛的知识密集型应用中，包括闭卷问答 Roberts et
    al. ([2020](#bib.bib34))、知识图谱补全 Yao et al. ([2019](#bib.bib44))、推荐系统 Sun et al.
    ([2019](#bib.bib38))；Geng et al. ([2022](#bib.bib11))、实体类型化 Huang et al. ([2022](#bib.bib19))
    和本体对齐 He et al. ([2022](#bib.bib18)) 等，仅举几例。
- en: Conceptual Space of LMs
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LMs 的概念空间
- en: There is an ongoing debate about the extent to which LMs can truly capture meaning
    Bender and Koller ([2020](#bib.bib3)); Abdou et al. ([2021](#bib.bib1)); Patel
    and Pavlick ([2022](#bib.bib30)); Søgaard ([2023](#bib.bib36)). Within this context,
    several authors have analysed the ability of LMs to predict perceptual features.
    As already mentioned, Paik et al. ([2021](#bib.bib29)) and Liu et al. ([2022a](#bib.bib22))
    analysed the ability of LMs to predict colour terms. Abdou et al. ([2021](#bib.bib1))
    analyses whether the representation of colour terms in LMs can be aligned with
    their representation in the CIELAB colour space. Patel and Pavlick ([2022](#bib.bib30))
    similarly showed that LLMs can generate colour terms from RGB codes in a few-shot
    setting, even if the codes represent a rotation of the standard RGB space. They
    also show a similar result for terms describing spatial relations. Zhu et al.
    ([2024](#bib.bib46)) have similarly shown that LLMs can understand colour codes,
    by using them to generate HSL codes for everyday objects, or by asking models
    to choose the most suitable code among two alternatives.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 关于语言模型（LMs）能否真正捕捉意义，存在持续的争论 Bender 和 Koller ([2020](#bib.bib3))；Abdou 等人 ([2021](#bib.bib1))；Patel
    和 Pavlick ([2022](#bib.bib30))；Søgaard ([2023](#bib.bib36))。在这种背景下，几位作者分析了语言模型预测感知特征的能力。如前所述，Paik
    等人 ([2021](#bib.bib29)) 和 Liu 等人 ([2022a](#bib.bib22)) 分析了语言模型预测颜色术语的能力。Abdou
    等人 ([2021](#bib.bib1)) 分析了语言模型中的颜色术语表示是否可以与 CIELAB 颜色空间中的表示对齐。Patel 和 Pavlick
    ([2022](#bib.bib30)) 同样表明，语言模型在少量样本设置下可以从 RGB 代码生成颜色术语，即使这些代码表示的是标准 RGB 空间的旋转。他们还对描述空间关系的术语展示了类似的结果。Zhu
    等人 ([2024](#bib.bib46)) 同样表明，语言模型能够理解颜色代码，通过使用它们生成日常物品的 HSL 代码，或要求模型在两个备选代码中选择最合适的一个。
- en: Beyond the colour domain, Li et al. ([2023](#bib.bib21)) considered physical
    properties such as height or mass. While they found LLMs to struggle with such
    properties, Chatterjee et al. ([2023](#bib.bib5)) reported better results on the
    same datasets, especially for GPT-4\. Focusing on visual features, Merullo et al.
    ([2023](#bib.bib26)) showed that the representations of concepts in vision-only
    and text-only models can be aligned using a linear mapping. Chatterjee et al.
    ([2023](#bib.bib5)) focused on the taste domain, modelling properties such as
    sweet. They found that GPT-3 can model such properties to a reasonable extent,
    but not better than a fine-tuned BERT model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 超越颜色领域，Li 等人 ([2023](#bib.bib21)) 考虑了如高度或质量等物理属性。尽管他们发现语言模型在这些属性上表现挣扎，但 Chatterjee
    等人 ([2023](#bib.bib5)) 在相同数据集上报告了更好的结果，特别是对于 GPT-4。专注于视觉特征，Merullo 等人 ([2023](#bib.bib26))
    表明，通过线性映射，可以将仅限视觉和仅限文本的模型中的概念表示对齐。Chatterjee 等人 ([2023](#bib.bib5)) 专注于味觉领域，建模如甜味等属性。他们发现
    GPT-3 可以在合理程度上建模这些属性，但不如微调的 BERT 模型。
- en: Gupta et al. ([2015](#bib.bib15)) already considered the problem of modelling
    gradual properties in the context of static word embeddings, although their analysis
    was limited to objective numerical features. Derrac and Schockaert ([2015](#bib.bib8))
    similarly learned conceptual space dimensions for properties such as “violent”
    in a semantic space of movies. These approaches essentially learn a linear classifier
    or regression model for each property indepdently, and can thus not generalise
    to new properties.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Gupta 等人 ([2015](#bib.bib15)) 已经在静态词嵌入的背景下考虑了建模渐进属性的问题，尽管他们的分析仅限于客观数值特征。Derrac
    和 Schockaert ([2015](#bib.bib8)) 同样在电影的语义空间中学习了如“暴力”等属性的概念空间维度。这些方法基本上为每个属性独立地学习一个线性分类器或回归模型，因此不能推广到新的属性。
- en: 3 Extracting Rankings
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提取排名
- en: 'We consider the following problem: given a set of entities $\mathcal{E}$. In
    some cases, $f$ will rather refer to a gradual property. For instance, $\mathcal{E}$
    for entity $e$.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑以下问题：给定一组实体 $\mathcal{E}$。在某些情况下，$f$ 将更倾向于指代渐进属性。例如，$\mathcal{E}$ 为实体 $e$。
- en: 'We consider two broad strategies for solving the considered ranking task with
    LLMs. First, we can use LLMs to map each entity $e$. This *pointwise* approach
    to learning to rank is considered in Section [3.2](#S3.SS2 "3.2 Pointwise Model
    ‣ 3 Extracting Rankings ‣ Ranking Entities along Conceptual Space Dimensions with
    LLMs: An Analysis of Fine-Tuning Strategies"). Second, we can use LLMs to solve
    a binary classification problem: given two entities $e_{1}$ holds. This *pairwise*
    approach needs to be combined with a strategy for aggregating the LLM predictions
    into a single ranking. The main disadvantage is that a large number of judgements
    need to be collected for this to be effective, which means that such approaches
    are less efficient than pointwise strategies. However, in the context of document
    retrieval, pairwise approaches have been found to outperform pointwise approaches
    Nogueira et al. ([2019](#bib.bib27)); Gienapp et al. ([2022](#bib.bib12)); Qin
    et al. ([2023](#bib.bib32)). We discuss pairwise and pointwise strategies in Sections
    [3.1](#S3.SS1 "3.1 Pairwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    and [3.2](#S3.SS2 "3.2 Pointwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    respectively. Finally, Section [3.3](#S3.SS3 "3.3 Baselines ‣ 3 Extracting Rankings
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies") describes how we establish baseline results using ChatGPT
    and GPT4.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了两种广泛的策略来解决考虑的排序任务。首先，我们可以使用LLMs对每个实体$e$进行映射。这种*逐点*的排名学习方法在[3.2](#S3.SS2
    "3.2 逐点模型 ‣ 3 提取排名 ‣ 基于LLMs的概念空间维度排名：微调策略分析")节中进行了讨论。其次，我们可以使用LLMs来解决一个二分类问题：给定两个实体$e_{1}$是否成立。这种*配对*方法需要与将LLM预测结果聚合为单一排名的策略结合使用。主要缺点是需要收集大量的判断，这意味着这种方法比逐点策略效率低。然而，在文档检索的背景下，配对方法被发现优于逐点方法
    Nogueira et al. ([2019](#bib.bib27)); Gienapp et al. ([2022](#bib.bib12)); Qin
    et al. ([2023](#bib.bib32))。我们在[3.1](#S3.SS1 "3.1 配对模型 ‣ 3 提取排名 ‣ 基于LLMs的概念空间维度排名：微调策略分析")和[3.2](#S3.SS2
    "3.2 逐点模型 ‣ 3 提取排名 ‣ 基于LLMs的概念空间维度排名：微调策略分析")节中分别讨论了配对和逐点策略。最后，[3.3](#S3.SS3 "3.3
    基准 ‣ 3 提取排名 ‣ 基于LLMs的概念空间维度排名：微调策略分析")节描述了如何使用ChatGPT和GPT4建立基准结果。
- en: 3.1 Pairwise Model
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 配对模型
- en: 'The problem of predicting whether $f(e_{1}).
    The second method simply scores each entity $。第二种方法简单地根据已做出的成对比较对每个实体
    $e_{i}$ 和 $e_{j}$ 进行评分，否则 $c_{ij}=-1$。然后我们可以选择权重为：
- en: '|  | $\displaystyle w_{i}=\frac{\sum_{j\neq i}s_{ij}c_{ij}}{\sum_{j\neq i}s_{ij}}$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{i}=\frac{\sum_{j\neq i}s_{ij}c_{ij}}{\sum_{j\neq i}s_{ij}}$
    |  |'
- en: We will refer to this strategy as Count.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种策略称为 Count。
- en: 3.2 Pointwise Model
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 逐点模型
- en: 'For the pointwise model, we need to learn a scoring function $w:\mathcal{E}\rightarrow\mathbb{R}$.
    To this end, we use a prompt of the following form:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逐点模型，我们需要学习一个评分函数 $w:\mathcal{E}\rightarrow\mathbb{R}$。为此，我们使用以下形式的提示：
- en: Is [entity 1] among [superlative feature] [entity type]?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实体 1] 是否在 [最高级特征] [实体类型] 中？'
- en: 'For instance, *Is River Thames among the longest rivers?* For each entity $e_{i}$
    w.r.t. the considered feature. Since we cannot obtain ground truth labels for
    this score, we again rely on pairwise comparisons for training the model. Specifically,
    we estimate the probability $p_{ij}$ holds as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*泰晤士河是最长的河流之一吗？* 针对每个实体 $e_{i}$ 与所考虑的特征相关。由于我们无法获得该分数的真实标签，我们再次依赖成对比较来训练模型。具体来说，我们估计概率
    $p_{ij}$ 计算如下：
- en: '|  | $p_{ij}=\sigma\big{(}w(e_{i})-w(e_{j})\big{)}$ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{ij}=\sigma\big{(}w(e_{i})-w(e_{j})\big{)}$ |  |'
- en: 'Then we use binary cross entropy as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用二进制交叉熵，如下所示：
- en: '|  | $1$2 |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $t_{ij}=1$ within the given mini-batch. Note that while we use pairwise
    comparisons for training the model, it is still a pointwise approach as it produces
    scores for individual entities.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{ij}=1$ 在给定的迷你批次中。请注意，尽管我们使用成对比较来训练模型，但它仍然是一种逐点方法，因为它为单个实体生成分数。
- en: 3.3 Baselines
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基线
- en: 'To put the performance of the fine-tuning strategies from Sections [3.1](#S3.SS1
    "3.1 Pairwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies") and [3.2](#S3.SS2
    "3.2 Pointwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies") into context,
    we compare them with two conversational models: ChatGPT (gpt-3.5-turbo) and GPT-4
    (gpt-4). We use both models in a zero-shot setting. For this purpose, we use the
    same prompt as in Section [3.1](#S3.SS1 "3.1 Pairwise Model ‣ 3 Extracting Rankings
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies") but append the sentence *Only answer with yes or no.*
    Despite this instruction, the models occasionally still generates a different
    response, typically expressing that the question cannot be answered. For such
    entity pairs, we replace the generated response with a randomly generated label
    (yes or no).¹¹1Statistics about how often this was needed can be found in the
    appendix.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将第[3.1节](#S3.SS1 "3.1 Pairwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")和第[3.2节](#S3.SS2
    "3.2 Pointwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")中的微调策略的表现放在背景中，我们将它们与两个对话模型进行比较：ChatGPT
    (gpt-3.5-turbo) 和 GPT-4 (gpt-4)。我们在零样本设置下使用这两个模型。为此，我们使用与第[3.1节](#S3.SS1 "3.1
    Pairwise Model ‣ 3 Extracting Rankings ‣ Ranking Entities along Conceptual Space
    Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")相同的提示，但附加了句子*仅回答“是”或“否”*。尽管有此指令，模型偶尔仍会生成不同的回应，通常表示问题无法回答。对于这些实体对，我们将生成的回应替换为随机生成的标签（是或否）。¹¹1关于这种情况发生频率的统计数据可以在附录中找到。'
- en: 4 Datasets
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集
- en: In our experiments, we will rely on the following datasets, either for training
    or for testing the models. Each dataset consists of a number of rankings, where
    each ranking is defined by a set of entities and a feature along which the entities
    are ranked.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将依赖以下数据集，用于训练或测试模型。每个数据集由多个排名组成，其中每个排名由一组实体和一个对实体进行排序的特征定义。
- en: Wikidata
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Wikidata
- en: We have obtained 20 rankings from numerical features that are available on Wikidata²²2[https://www.wikidata.org/wiki/Wikidata:Main_Page](https://www.wikidata.org/wiki/Wikidata:Main_Page).
    For instance, we obtained a ranking of rivers by comparing their length.³³3The
    entity types and corresponding features are listed in the appendix. If there were
    more than 1000 entities with a given feature value, we selected the most 1000
    popular entities. To estimate the popularity of an entity, we use their QRank⁴⁴4[https://qrank.wmcloud.org](https://qrank.wmcloud.org),
    which counts the number page views of the corresponding entry in sources such
    as Wikipedia. For the entity type person, we limited the analysis to people born
    in London (which made it possible to retrieve the required information from Wikidata
    more efficiently). We similarly only considered museums located in Italy. For
    some experiments, we split the collected data in two datasets, called WD1 and
    WD2. This will allow us to test whether models trained on one set of features
    (i.e. WD1) generalise to a different set of feature (i.e. WD2). WD1 contains rankings
    which were cut off at 1000 elements, whereas WD2 contains rankings with fewer
    elements. We will write WD to refer to the full dataset, i.e. WD1 and WD2 combined.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page)上获得了20个来自数值特征的排名。例如，我们通过比较河流的长度获得了一个排名。实体类型及其相应特征列在附录中。如果某个特征值下的实体数量超过1000，我们会选择最受欢迎的1000个实体。为了估计实体的受欢迎程度，我们使用其QRank⁴⁴4[https://qrank.wmcloud.org](https://qrank.wmcloud.org)，它统计了在维基百科等来源中对应条目的页面浏览量。对于“人”这一实体类型，我们将分析限定为出生在伦敦的人（这使得从Wikidata中检索所需信息更高效）。我们同样只考虑位于意大利的博物馆。对于某些实验，我们将收集的数据分成两个数据集，称为WD1和WD2。这将允许我们测试在一个特征集（即WD1）上训练的模型是否能推广到不同的特征集（即WD2）。WD1包含被截断为1000个元素的排名，而WD2包含较少元素的排名。我们将WD用于指代完整的数据集，即WD1和WD2的结合。
- en: '|  | Wikidata | Taste | Rocks | TG | Phys |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | Wikidata | Taste | Rocks | TG | Phys |  |'
- en: '|  |  WD1-test  |  WD2  |  Sweetness  |  Saltiness  |  Sourness  |  Bitterness  |  Umaminess  |  Fattiness  |  Lightness  |  Grain
    size  |  Roughness  |  Shininess  |  Organisation  |  Variability  |  Density  |  Movies  |  Books  |  Size  |  Height  |  Mass  |  Average  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |  WD1-test  |  WD2  |  甜度  |  咸度  |  酸度  |  苦味  |  鲜味  |  脂肪感  |  轻盈感  |  粗糙度  |  光泽度  |  组织性  |  变异性  |  密度  |  电影  |  书籍  |  大小  |  高度  |  质量  |  平均值  |'
- en: '| Pointwise |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 点对点 |'
- en: '| Llama2-7B | 80.5 | 61.0 | 62.8 | 53.2 | 47.2 | 52.6 | 58.2 | 65.0 | 62.0
    | 60.2 | 56.4 | 42.0 | 53.6 | 61.2 | 72.2 | 59.3 | 52.8 | 68.0 | 70.0 | 50.0 |
    59.4 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | 80.5 | 61.0 | 62.8 | 53.2 | 47.2 | 52.6 | 58.2 | 65.0 | 62.0
    | 60.2 | 56.4 | 42.0 | 53.6 | 61.2 | 72.2 | 59.3 | 52.8 | 68.0 | 70.0 | 50.0 |
    59.4 |'
- en: '| Llama2-13B | 79.8 | 58.7 | 52.8 | 70.4 | 51.2 | 52.8 | 65.2 | 67.2 | 66.4
    | 49.6 | 43.2 | 52.6 | 57.0 | 57.2 | 65.2 | 60.9 | 55.0 | 69.6 | 76.4 | 58.4 |
    60.5 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 79.8 | 58.7 | 52.8 | 70.4 | 51.2 | 52.8 | 65.2 | 67.2 | 66.4
    | 49.6 | 43.2 | 52.6 | 57.0 | 57.2 | 65.2 | 60.9 | 55.0 | 69.6 | 76.4 | 58.4 |
    60.5 |'
- en: '| Mistral-7B | 78.3 | 61.4 | 70.2 | 69.4 | 64.8 | 59.2 | 67.8 | 68.8 | 61.0
    | 57.4 | 42.4 | 47.8 | 61.0 | 52.4 | 56.0 | 62.4 | 59.3 | 85.6 | 70.0 | 61.0 |
    62.8 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 78.3 | 61.4 | 70.2 | 69.4 | 64.8 | 59.2 | 67.8 | 68.8 | 61.0
    | 57.4 | 42.4 | 47.8 | 61.0 | 52.4 | 56.0 | 62.4 | 59.3 | 85.6 | 70.0 | 61.0 |
    62.8 |'
- en: '| Pairwise |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 对比 |'
- en: '| Llama2-7B | 81.8 | 61.6 | 59.0 | 59.8 | 52.0 | 53.8 | 60.8 | 61.8 | 50.8
    | 62.6 | 52.2 | 46.6 | 56.0 | 55.8 | 64.4 | 57.0 | 60.1 | 86.2 | 81.2 | 68.0 |
    61.6 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | 81.8 | 61.6 | 59.0 | 59.8 | 52.0 | 53.8 | 60.8 | 61.8 | 50.8
    | 62.6 | 52.2 | 46.6 | 56.0 | 55.8 | 64.4 | 57.0 | 60.1 | 86.2 | 81.2 | 68.0 |
    61.6 |'
- en: '| Llama2-13B | 82.8 | 68.0 | 58.6 | 67.4 | 50.8 | 53.6 | 67.6 | 67.6 | 50.2
    | 66.8 | 58.4 | 52.0 | 55.8 | 58.8 | 68.8 | 58.3 | 55.6 | 93.8 | 91.2 | 66.2 |
    64.6 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 82.8 | 68.0 | 58.6 | 67.4 | 50.8 | 53.6 | 67.6 | 67.6 | 50.2
    | 66.8 | 58.4 | 52.0 | 55.8 | 58.8 | 68.8 | 58.3 | 55.6 | 93.8 | 91.2 | 66.2 |
    64.6 |'
- en: '| Mistral-7B | 82.2 | 64.2 | 59.4 | 69.0 | 52.4 | 52.4 | 66.8 | 63.0 | 58.6
    | 55.0 | 52.6 | 47.8 | 54.8 | 52.0 | 58.8 | 53.3 | 52.3 | 92.6 | 88.0 | 68.2 |
    62.2 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 82.2 | 64.2 | 59.4 | 69.0 | 52.4 | 52.4 | 66.8 | 63.0 | 58.6
    | 55.0 | 52.6 | 47.8 | 54.8 | 52.0 | 58.8 | 53.3 | 52.3 | 92.6 | 88.0 | 68.2 |
    62.2 |'
- en: '| Baselines |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 基准 |'
- en: '| ChatGPT | 55.3 | 60.9 | 60.4 | 58.4 | 52.4 | 51.0 | 53.2 | 54.2 | 60.4 |
    60.2 | 57.0 | 51.4 | 53.2 | 55.2 | 62.8 | 63.8 | 67.2 | 77.8 | 70.8 | 58.6 | 59.2
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 55.3 | 60.9 | 60.4 | 58.4 | 52.4 | 51.0 | 53.2 | 54.2 | 60.4 |
    60.2 | 57.0 | 51.4 | 53.2 | 55.2 | 62.8 | 63.8 | 67.2 | 77.8 | 70.8 | 58.6 | 59.2
    |'
- en: '| GPT-4 | 77.2 | 78.3 | 76.6 | 80.6 | 62.6 | 56.2 | 69.2 | 73.8 | 72.8 | 70.2
    | 56.6 | 62.4 | 59.4 | 63.6 | 74.0 | 67.4 | 66.9 | 99.2 | 95.2 | 64.0 | 71.3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 77.2 | 78.3 | 76.6 | 80.6 | 62.6 | 56.2 | 69.2 | 73.8 | 72.8 | 70.2
    | 56.6 | 62.4 | 59.4 | 63.6 | 74.0 | 67.4 | 66.9 | 99.2 | 95.2 | 64.0 | 71.3 |'
- en: 'Table 1: Comparison of different models in terms of accuracy ($\%$), when classifying
    pairwise judgments. The pointwise and pairwise models are trained on the training
    split of WD1.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同模型在成对判断中的准确性（%）对比。点对点模型和对对模型均在WD1的训练集上训练。
- en: '|  | Wikidata | Taste | Rocks | TG | Phys |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | Wikidata | 味道 | 岩石 | TG | 物理 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  WD1-test  |  WD2  |  Sweetness  |  Saltiness  |  Sourness  |  Bitterness  |  Umaminess  |  Fattiness  |  Lightness  |  Grain
    size  |  Roughness  |  Shininess  |  Organisation  |  Variability  |  Density  |  Movies  |  Books  |  Size  |  Height  |  Mass  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  WD1-test  |  WD2  |  甜度  |  咸度  |  酸度  |  苦味  |  鲜味  |  脂肪感  |  轻盈感  |  粗糙度  |  光泽度  |  组织性  |  变异性  |  密度  |  电影  |  书籍  |  大小  |  高度  |  质量  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| WD1-train | 82.8 | 68.0 | 58.6 | 67.4 | 50.8 | 53.6 | 67.6 | 67.6 | 50.2
    | 66.8 | 58.4 | 52.0 | 55.8 | 58.8 | 68.8 | 58.3 | 55.6 | 93.8 | 91.2 | 66.2 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WD1-train | 82.8 | 68.0 | 58.6 | 67.4 | 50.8 | 53.6 | 67.6 | 67.6 | 50.2
    | 66.8 | 58.4 | 52.0 | 55.8 | 58.8 | 68.8 | 58.3 | 55.6 | 93.8 | 91.2 | 66.2 |'
- en: '| WD | - | - | 55.2 | 64.8 | 51.2 | 53.8 | 62.4 | 63.0 | 46.8 | 68.8 | 60.8
    | 60.0 | 50.4 | 64.8 | 70.6 | 65.3 | 62.3 | 78.0 | 79.4 | 60.4 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| WD | - | - | 55.2 | 64.8 | 51.2 | 53.8 | 62.4 | 63.0 | 46.8 | 68.8 | 60.8
    | 60.0 | 50.4 | 64.8 | 70.6 | 65.3 | 62.3 | 78.0 | 79.4 | 60.4 |'
- en: '| TG | 63.3 | 56.9 | 71.2 | 71.6 | 60.0 | 58.8 | 69.0 | 65.6 | 71.2 | 69.6
    | 48.8 | 60.6 | 57.6 | 55.8 | 66.0 | - | - | 50.6 | 54.6 | 55.2 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| TG | 63.3 | 56.9 | 71.2 | 71.6 | 60.0 | 58.8 | 69.0 | 65.6 | 71.2 | 69.6
    | 48.8 | 60.6 | 57.6 | 55.8 | 66.0 | - | - | 50.6 | 54.6 | 55.2 |'
- en: '| Taste | 62.1 | 51.1 | - | - | - | - | - | - | 66.4 | 72.2 | 56.8 | 60.8 |
    58.6 | 53.2 | 74.0 | 66.2 | 55.7 | 53.0 | 61.4 | 58.2 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 味道 | 62.1 | 51.1 | - | - | - | - | - | - | 66.4 | 72.2 | 56.8 | 60.8 | 58.6
    | 53.2 | 74.0 | 66.2 | 55.7 | 53.0 | 61.4 | 58.2 |'
- en: '| WD+TG+Taste | - | - | - | - | - | - | - | - | 61.2 | 70.6 | 57.0 | 59.2 |
    62.8 | 57.6 | 78.4 | - | - | 77.8 | 85.4 | 62.2 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| WD+TG+味道 | - | - | - | - | - | - | - | - | 61.2 | 70.6 | 57.0 | 59.2 | 62.8
    | 57.6 | 78.4 | - | - | 77.8 | 85.4 | 62.2 |'
- en: '| WD+TG+Rocks | - | - | 74.0 | 72.4 | 60.0 | 60.2 | 70.6 | 72.2 | - | - | -
    | - | - | - | - | - | - | 85.6 | 88.2 | 62.0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| WD+TG+Rocks | - | - | 74.0 | 72.4 | 60.0 | 60.2 | 70.6 | 72.2 | - | - | -
    | - | - | - | - | - | - | 85.6 | 88.2 | 62.0 |'
- en: '| WD+Taste+Rocks | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
    - | 69.1 | 65.2 | 89.4 | 91.2 | 63.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| WD+Taste+Rocks | - | - | - | - | - | - | - | - | - | - | - | - | - | - |
    - | 69.1 | 65.2 | 89.4 | 91.2 | 63.8 |'
- en: 'Table 2: Comparison of different models in terms of accuracy ($\%$), when classifying
    pairwise judgments. All results are for the pairwise model with Llama2-13B.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同模型在对配对判断进行分类时的准确性（$\%$）比较。所有结果均为使用Llama2-13B的配对模型。
- en: Taste
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 口味
- en: 'Following Chatterjee et al. ([2023](#bib.bib5)), we use a dataset with ratings
    about the taste of 590 food items along six dimensions: sweetness, sourness, saltiness,
    bitterness, fattiness and umaminess. The dataset was created by Martin et al.
    ([2014](#bib.bib25)), who used a panel of twelve experienced food assessors to
    rate the items. We use the version of the dataset that was cleaned by Chatterjee
    et al. ([2023](#bib.bib5)), who altered some of the descriptions of the items
    to make them sound more natural in prompts.⁵⁵5Available from [https://github.com/ExperimentsLLM/EMNLP2023_PotentialOfLLM_LearningConceptualSpace](https://github.com/ExperimentsLLM/EMNLP2023_PotentialOfLLM_LearningConceptualSpace).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Chatterjee等人（[2023](#bib.bib5)）的方法，我们使用了一个包含590种食品的口味评级数据集，涵盖六个维度：甜度、酸度、咸度、苦度、油腻感和鲜味。该数据集由Martin等人（[2014](#bib.bib25)）创建，他们使用了一个由十二名经验丰富的食品评估员组成的专家组来对这些食品进行评级。我们使用了Chatterjee等人（[2023](#bib.bib5)）清理过的版本，他们对部分食品描述进行了修改，使其在提示中更自然。⁵⁵5可从
    [https://github.com/ExperimentsLLM/EMNLP2023_PotentialOfLLM_LearningConceptualSpace](https://github.com/ExperimentsLLM/EMNLP2023_PotentialOfLLM_LearningConceptualSpace)
    获取。
- en: Rocks
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 岩石
- en: 'Nosofsky et al. ([2018](#bib.bib28)) created a dataset of rocks, with the aim
    of studying how cognitively meaningful representation spaces for complex domains
    can be learned. A total of 30 rock types were studied (10 igneous rocks, 10 metamorphic
    rocks and 10 sedimentary rocks). For each type of rock, 12 pictures were obtained,
    and each picture was annotated along 18 dimensions. However, only 7 of the considered
    dimensions allow for ranking all types: lightness of colour, average grain size,
    roughness, shininess, organisation, variability of colour and density. For our
    experiments, we only considered these dimensions. The dataset from Nosofsky et al.
    ([2018](#bib.bib28)) contains ratings for each of the 12 pictures of a given rock
    type, where each picture was assessed by 20 annotators. To construct rankings
    of rock types, we average the ratings across the 12 pictures. As such, we end
    up with 7 rankings of 30 rock types.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Nosofsky等人（[2018](#bib.bib28)）创建了一个岩石数据集，旨在研究如何学习复杂领域的认知有意义的表示空间。总共研究了30种岩石类型（10种火成岩、10种变质岩和10种沉积岩）。对于每种岩石类型，获取了12张图片，每张图片在18个维度上进行了标注。然而，仅有7个维度允许对所有类型进行排序：颜色的亮度、平均颗粒大小、粗糙度、光泽度、组织、颜色的变化和密度。为了进行实验，我们仅考虑了这些维度。Nosofsky等人（[2018](#bib.bib28)）的数据集中包含了每种岩石类型12张图片的评分，每张图片由20位标注员评估。为了构建岩石类型的排名，我们对12张图片的评分进行平均。因此，我们得到了30种岩石类型的7个排名。
- en: Tag Genome
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标签基因组
- en: Vig et al. ([2012](#bib.bib40)) collected a dataset⁶⁶6Available from [https://grouplens.org/datasets/movielens/tag-genome-2021/](https://grouplens.org/datasets/movielens/tag-genome-2021/).
    of movies, called the Tag Genome, by asking annotators to what extent different
    tags apply to different movies, on a scale from 1 to 5\. From these tags, we first
    selected those that correspond to adjectives and for which ratings for at least
    15 movies were available. We then manually identified 38 of these adjectives which
    correspond to ordinal features. More recently, Kotkov et al. ([2022](#bib.bib20))
    created a similar dataset for books. We again selected adjectives for which at
    least 15 items were ranked, and manually identified 32 adjectives that correspond
    to ordinal features. A list of the adjectives that we considered, together with
    the corresponding number of items in the rankings, is provided in the appendix.
    It should be noted that most items are only judged by a single annotator, and
    the judgements were moreover obtained using crowdsourcing. The movies and books
    datasets are thus clearly noisier than the taste and rocks datasets. By averaging
    across a large number of rankings, we believe that these datasets can nonetheless
    be valuable. For this reason, we will only consider aggregated results across
    all tags when evaluating on these datasets. We will write TG to refer to the combined
    dataset, containing both the books and movies rankings.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Vig等人（[2012](#bib.bib40)）收集了一个名为Tag Genome的数据集⁶⁶6，详细信息见[https://grouplens.org/datasets/movielens/tag-genome-2021/](https://grouplens.org/datasets/movielens/tag-genome-2021/)。该数据集通过询问标注者不同标签在不同电影中的适用程度（从1到5）来创建。从这些标签中，我们首先选择了那些对应于形容词且至少有15部电影的评分的标签。随后我们手动识别了38个与序数特征对应的形容词。最近，Kotkov等人（[2022](#bib.bib20)）为书籍创建了类似的数据集。我们再次选择了至少有15项排名的形容词，并手动识别了32个对应于序数特征的形容词。附录中提供了我们考虑的形容词列表以及在排名中的对应项数。需要注意的是，大多数项目仅由一个标注者评判，且这些判断是通过众包获得的。因此，电影和书籍的数据集明显比味道和岩石数据集更嘈杂。通过对大量排名进行平均，我们相信这些数据集仍然具有价值。因此，我们在评估这些数据集时将只考虑所有标签的汇总结果。我们将使用TG来指代包含书籍和电影排名的合并数据集。
- en: Physical Properties
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 物理属性
- en: 'Following Li et al. ([2023](#bib.bib21)), we consider three physical properties:
    mass, size and height. The ground truth for mass dataset was obtained from a dataset
    about household objects from Standley et al. ([2017](#bib.bib37)). Following Chatterjee
    et al. ([2023](#bib.bib5)), we removed 7 items, because their mass cannot be assessed
    without the associated image: *big elephant*, *small elephant*, *Ivan’s phone*,
    *Ollie the monkey*, *Marshy the elephant*, *boy doll* and *Dali Clock*. The resulting
    dataset has 49 items. For size and height, we use the datasets from Liu et al.
    ([2022b](#bib.bib23)) as ground truth. These datasets each consist of 500 pairwise
    judgements.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据李等人（[2023](#bib.bib21)）的方法，我们考虑了三种物理属性：质量、尺寸和高度。质量数据集的真实数据来自Standley等人（[2017](#bib.bib37)）关于家用物品的数据集。根据Chatterjee等人（[2023](#bib.bib5)）的方法，我们移除了7个项目，因为没有关联的图像就无法评估它们的质量：*大象*、*小象*、*伊万的手机*、*猴子奥利*、*湿湿的象*、*男孩玩偶*和*达利时钟*。结果的数据集包含49个项目。对于尺寸和高度，我们使用刘等人（[2022b](#bib.bib23)）的数据集作为真实数据。这些数据集每个包含500对比判断。
- en: '|  |  Sweetness  |  Saltiness  |  Sourness  |  Bitterness  |  Umaminess  |  Fattiness  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 甜度 | 咸度 | 酸度 | 苦度 | 鲜味 | 脂肪感 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Pointwise | 51.0 | 64.8 | 32.5 | 35.2 | 52.0 | 61.7 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Pointwise | 51.0 | 64.8 | 32.5 | 35.2 | 52.0 | 61.7 |'
- en: '| SVM (5 samples) | 62.1 | 62.2 | 42.6 | 44.6 | 56.4 | 63.0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SVM（5个样本） | 62.1 | 62.2 | 42.6 | 44.6 | 56.4 | 63.0 |'
- en: '| SVM (30 samples) | 66.0 | 64.7 | 47.6 | 47.0 | 60.6 | 65.8 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SVM（30个样本） | 66.0 | 64.7 | 47.6 | 47.0 | 60.6 | 65.8 |'
- en: '| Count (5 samples) | 59.0 | 57.4 | 46.7 | 41.7 | 53.5 | 60.1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Count（5个样本） | 59.0 | 57.4 | 46.7 | 41.7 | 53.5 | 60.1 |'
- en: '| Count (30 samples) | 64.8 | 64.7 | 49.1 | 47.2 | 59.9 | 64.7 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Count（30个样本） | 64.8 | 64.7 | 49.1 | 47.2 | 59.9 | 64.7 |'
- en: '| Ada^∗ | 17.5 | 8.5 | 12.2 | 16.4 | 22.5 | 10.7 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Ada^∗ | 17.5 | 8.5 | 12.2 | 16.4 | 22.5 | 10.7 |'
- en: '| Babbage^∗ | 19.5 | 51.1 | 20.2 | 22.0 | 22.6 | 16.0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Babbage^∗ | 19.5 | 51.1 | 20.2 | 22.0 | 22.6 | 16.0 |'
- en: '| Curie^∗ | 36.0 | 46.3 | 32.8 | 23.2 | 22.6 | 31.7 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Curie^∗ | 36.0 | 46.3 | 32.8 | 23.2 | 22.6 | 31.7 |'
- en: '| Davinci^∗ | 55.0 | 63.2 | 33.3 | 27.2 | 57.0 | 52.0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Davinci^∗ | 55.0 | 63.2 | 33.3 | 27.2 | 57.0 | 52.0 |'
- en: 'Table 3: Comparison of ranking strategies, in terms of Spearman $\rho$%. Baseline
    results marked with ^∗ were taken from Chatterjee et al. ([2023](#bib.bib5)).
    All other results are obtained with Llama-13B trained on WD+TG+Rocks.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：排名策略比较，以斯皮尔曼 $\rho$% 为标准。标记为 ^∗ 的基线结果来自 Chatterjee 等人（[2023](#bib.bib5)）。所有其他结果均为在
    WD+TG+Rocks 上训练的 Llama-13B 得到。
- en: '| Feature | Top ranked entities | Bottom ranked entities |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 排名前实体 | 排名后实体 |'
- en: '| --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sweetness | mango, dried date, white chocolate, peach , pineapple in syrup,
    fruit candy, syrup with water, ice cream, strawberry, sweet pancake with maple
    syrup | minced beef patty, grilled calf livers, squid, sandwich with cold cuts,
    gizzards, croque-monsieur, roast rabbit, stir-fried bacon, roast beef , calf head
    with vinaigrette |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 甜味 | 芒果、干枣、白巧克力、桃子、糖水菠萝、水果糖、糖水、冰淇淋、草莓、枫糖甜煎饼 | 绞碎的牛肉饼、烤小牛肝、鱿鱼、夹有冷切肉的三明治、鸡胗、烤面包、烤兔肉、炒培根、烤牛肉、带醋汁的小牛头
    |'
- en: '| Saltiness | green olives, extruded salty crackers, soy sprouts with soy sauce,
    canned anchovies, canned sardines, pasta with soy sauce, salted pies, marinated
    mussels, potato chips, salted cake | clafoutis, raspberry cake, stewed apple,
    raspberry with whipped cream, white chocolate, strawberry with cream and sugar,
    mix fruits juice, apple, raspberry, strawberry |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 咸味 | 绿橄榄、挤压咸饼干、加酱油的豆芽、罐装凤尾鱼、罐装沙丁鱼、酱油意大利面、咸饼干、腌贻贝、薯片、咸蛋糕 | 克拉夫提、覆盆子蛋糕、炖苹果、覆盆子配奶油、白巧克力、草莓配奶油和糖、水果混合汁、苹果、覆盆子、草莓
    |'
- en: '| Scary | Descent, The (2005), Grudge, The (2004), Exorcist, The (1973), Silence
    of the Lambs, The (1991), Ring, The (2002), Texas Chainsaw Massacre, The (1974),
    Shining, The (1980), Seven (a.k.a. Se7en) (1995), Amityville Horror, The (2005),
    American Werewolf in London, An (1981) | Super Size Me (2004), Station Agent,
    The (2003), Ray (2004), Dances with Wolves (1990), Jerry Maguire (1996), Driving
    Miss Daisy (1989), School of Rock (2003), Kung Fu Panda (2008), Miss Congeniality
    (2000), Ninotchka (1939) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 恐怖 | 《失落的世界》（2005），《诅咒》（2004），《驱魔人》（1973），《沉默的羔羊》（1991），《午夜凶铃》（2002），《德州电锯杀人狂》（1974），《闪灵》（1980），《七宗罪》（1995），《安门修道院的恐怖》（2005），《伦敦狼人》（1981）
    | 《超级大小我》（2004），《车站代理》（2003），《雷》（2004），《与狼共舞》（1990），《杰瑞·麦圭尔》（1996），《开车送黛西小姐》（1989），《摇滚学校》（2003），《功夫熊猫》（2008），《完美小姐》（2000），《尼诺奇卡》（1939）
    |'
- en: '| Funny | Ace Ventura: When Nature Calls (1995), Ace Ventura: Pet Detective
    (1994), Hot Shots! Part Deux (1993), Army of Darkness (1993), South Park: Bigger,
    Longer and Uncut (1999), Auntie Mame (1958), Blazing Saddles (1974), Clerks (1994),
    Grand Day Out with Wallace and Gromit, A (1989), Hitchhiker’s Guide to the Galaxy,
    The (2005) | Spanish Prisoner, The (1997), Son of Dracula (1943), Ghost Dog: The
    Way of the Samurai (1999), Ferngully: The Last Rainforest (1992), High Crimes
    (2002), Cadillac Man (1990), Bad Boys II (2003), House of Wax (1953), Fire in
    the Sky (1993), Step Up 2 the Streets (2008) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 搞笑 | 《神探艾克斯》（1995），《神探艾克斯：宠物侦探》（1994），《热血高中》（1993），《黑暗军团》（1993），《南方公园：更大、更长、更未删减》（1999），《玛门姨妈》（1958），《火烧圆明园》（1974），《
    Clerk’s》（1994），《华莱士与葛罗米特的奇幻大冒险》（1989），《银河系漫游指南》（2005） | 《西班牙囚徒》（1997），《德古拉之子》（1943），《幽灵狗：武士之道》（1999），《费尔根：最后的雨林》（1992），《重大犯罪》（2002），《凯迪拉克男人》（1990），《坏男孩2》（2003），《蜡像馆》（1953），《天空中的火焰》（1993），《街舞2：街头舞者》（2008）
    |'
- en: '| Population | India, Nigeria, People’s Republic of China, Iran, Pakistan,
    United States of America, Russia, Indonesia, Egypt, Bangladesh | Dominica, Nauru,
    Andorra, Cook Islands, Saint Vincent and the Grenadines, Seychelles, Palau, Northern
    Mariana Islands, Liechtenstein, Niue |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 人口 | 印度、尼日利亚、中华人民共和国、伊朗、巴基斯坦、美国、俄罗斯、印度尼西亚、埃及、孟加拉国 | 多米尼加、瑙鲁、安道尔、库克群岛、圣文森特和格林纳丁斯、塞舌尔、帕劳、北马里亚纳群岛、列支敦士登、纽埃
    |'
- en: 'Table 4: We show the top and bottom ranked entities for five features: sweetness
    and saltiness, from the food dataset, scary and funny, from movies, and countries
    population, from WD2.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：我们展示了五个特征的排名前后实体：甜味和咸味来自食品数据集，恐怖和搞笑来自电影，国家人口来自WD2。
- en: 5 Experiments
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: We now evaluate the performance of the fine-tuning strategies on the considered
    datasets.⁷⁷7Our datasets, code and pre-trained models will be shared upon acceptance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在评估所考虑数据集上的微调策略性能。⁷⁷7 我们的数据集、代码和预训练模型将在接受后共享。
- en: Comparing Models
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较模型
- en: 'Table [1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    compares a number of different models. We test three different LLMs: the 7B and
    13B parameter Llama 2 models⁸⁸8We use the llama-2-7b-hf and llama-2-13b-hf models
    available from [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama).
    and the 7B parameter Mistral model⁹⁹9We use the mistral-7b-v0.1 model available
    from [https://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)..
    We evaluate the different models in terms of their accuracy on pairwise judgements.
    To this end, for a given dataset, we randomly sample pairs of entities $e_{i},e_{j}$.
    For WD, Taste and Rocks, we sample 500 such pairs for each of the features. Since
    the TG dataset has a total of 70 features, we limit the test set to 100 pairs
    per feature. For this analysis, we have split the WD1 dataset into two parts:
    80% of the entities, for each feature, are used for training the models. The remaining
    20% are used as a test set. All models are fine-tuned on the training split of
    WD1 (apart from the baselines, which are evaluated zero-shot). This allows us
    to see how well the models perform on the features they were trained on (by evaluating
    on the WD1 test split), as well as how they generalise to unseen properties.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")比较了多个不同的模型。我们测试了三种不同的LLMs：7B和13B参数的Llama
    2模型⁸⁸8我们使用了从[https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)获得的llama-2-7b-hf和llama-2-13b-hf模型。以及7B参数的Mistral模型⁹⁹9我们使用了从[https://huggingface.co/mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)获得的mistral-7b-v0.1模型。我们根据成对判断评估不同模型的准确性。为此，对于给定的数据集，我们随机抽取实体对$e_{i},e_{j}$。对于WD、Taste和Rocks，我们对每个特征抽取500对这样的实体对。由于TG数据集共有70个特征，我们将测试集限制为每个特征100对。对于此次分析，我们将WD1数据集拆分为两部分：每个特征80%的实体用于训练模型，其余20%用于测试集。所有模型都在WD1的训练部分进行微调（基准模型除外，它们进行零-shot评估）。这使我们能够看到模型在其训练的特征上的表现（通过评估WD1测试集），以及它们如何泛化到未见的属性。'
- en: 'The aim of the analysis in Table [1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies") is to assess whether models can be successfully fine-tuned
    using a relatively small training set (i.e. WD1-train), involving only well-defined
    numerical features. In particular, we want to test whether models which are fine-tuned
    on such features would also generalise to more subjective and less readily available
    ones, similar to the easy-to-hard generalisation that has been observed for LLMs
    in other tasks Hase et al. ([2024](#bib.bib17)). The results show that this is
    only the case to some extent. Overall, we can see that Mistral-7B achieves the
    best results among the pointwise models, while Llama2-13B achieves the best results
    among the pairwise models. The performance of the pointwise Mistral-7B model is
    particularly surprising, given that pairwise models generally perform better in
    ranking tasks. The performance of the models across different features is not
    always consistent. Each model achieves close to random chance on some of the features,
    but the features where one model performs poorly are not always the same features
    where other models perform poorly. However, for *bitterness* and *roughness*,
    all models perform below 60% F1\. Furthermore, *sourness* and *organisation* also
    stand out as being more challenging. Regarding the baselines, GPT-4 generally
    performs better than the fine-tuned models. ChatGPT performs worse on most features,
    but achieves the best results for *books*.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")的分析目的是评估模型是否能够使用相对较小的训练集（即WD1-train）成功进行微调，仅涉及明确的数值特征。特别地，我们希望测试那些在这些特征上微调的模型是否也能够推广到更主观且不易获取的特征上，类似于在其他任务中观察到的LLMs从简单到困难的泛化（Hase等，
    [2024](#bib.bib17)）。结果显示，这仅在某种程度上是成立的。总体来看，Mistral-7B在逐点模型中表现最佳，而Llama2-13B在成对模型中表现最佳。考虑到成对模型通常在排序任务中表现更好，逐点Mistral-7B模型的表现尤其令人惊讶。不同特征下模型的表现并不总是保持一致。每个模型在某些特征上的表现接近随机，但表现差的特征并不总是其他模型表现差的特征。然而，对于*苦味*和*粗糙度*，所有模型的F1分数都低于60%。此外，*酸味*和*组织性*也显得更加具有挑战性。关于基准线，GPT-4的表现通常优于微调模型。ChatGPT在大多数特征上的表现较差，但在*书籍*方面的表现最好。'
- en: Comparing Training Sets
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较训练集
- en: 'The relatively disappointing results from Table [1](#S4.T1 "Table 1 ‣ Wikidata
    ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An
    Analysis of Fine-Tuning Strategies") can be partially explained by the fact that
    a small training set was used, which moreover only covered numerical features
    and particular entity types. In Table [2](#S4.T2 "Table 2 ‣ Wikidata ‣ 4 Datasets
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies"), we evaluate the impact of using different training sets.
    For this analysis, we use the pairwise Llama2-13B model. Our focus is on seeing
    whether models trained on one domain can generalise to other domains. The results
    are again evaluated in terms of accuracy, using the same pairwise judgements as
    for Table [1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies").
    WD refers to the full dataset (including both the training and test splits of
    WD-1).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S4.T1 "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual
    Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")中相对令人失望的结果可以部分归因于使用了较小的训练集，该训练集仅涵盖了数值特征和特定的实体类型。在表[2](#S4.T2
    "Table 2 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies")中，我们评估了使用不同训练集的影响。对于这项分析，我们使用了成对的Llama2-13B模型。我们的重点是观察在一个领域上训练的模型是否可以推广到其他领域。结果再次通过准确率进行评估，使用与表[1](#S4.T1
    "Table 1 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies")相同的成对判断。WD指的是完整的数据集（包括WD-1的训练集和测试集）。'
- en: 'We can see that training on larger datasets indeed leads to considerably better
    results. While this is not unexpected, we can also make more striking observations
    in Table [2](#S4.T2 "Table 2 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies").
    For instance, the model that was trained on Taste alone achieves strong results
    on Rocks, despite the two datasets involving very different features. Similarly,
    the model that was only trained on TG achieves strong results for both Taste and
    Rocks. This suggests that the fine-tuned models are indeed capable of generalising
    to unseen domains. However, to achieve strong results, it appears to be important
    that the training data contains subjective or perceptual features. Indeed, training
    on TG alone overall performed poorly, compared to the other training sets. The
    best results in Table [2](#S4.T2 "Table 2 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    are competitive with the GPT-4 results from Table [1](#S4.T1 "Table 1 ‣ Wikidata
    ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An
    Analysis of Fine-Tuning Strategies"). Given that the training and test sets cover
    disjoint domains, the results in Table [2](#S4.T2 "Table 2 ‣ Wikidata ‣ 4 Datasets
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies") reflect the knowledge that is captured by the LLMs themselves,
    rather than knowledge that was injected during the fine-tuning process. This suggests
    that pre-trained LLMs capture more perceptual knowledge than it may initially
    appear.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到，在较大的数据集上训练确实会产生显著更好的结果。虽然这并不令人意外，但我们还可以在表[2](#S4.T2 "Table 2 ‣ Wikidata
    ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An
    Analysis of Fine-Tuning Strategies")中做出更为惊人的观察。例如，尽管Taste和Rocks这两个数据集涉及非常不同的特征，但仅在Taste上训练的模型在Rocks上取得了强劲的结果。同样，仅在TG上训练的模型在Taste和Rocks上都取得了强劲的结果。这表明，微调后的模型确实能够推广到未见过的领域。然而，为了获得强劲的结果，训练数据中包含主观或感知特征似乎是重要的。确实，与其他训练集相比，仅在TG上训练的表现整体较差。表[2](#S4.T2
    "Table 2 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies")中的最佳结果与表[1](#S4.T1 "Table 1
    ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions with
    LLMs: An Analysis of Fine-Tuning Strategies")中的GPT-4结果相媲美。考虑到训练集和测试集覆盖的是不重叠的领域，表[2](#S4.T2
    "Table 2 ‣ Wikidata ‣ 4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies")中的结果反映了LLMs自身所捕获的知识，而不是在微调过程中注入的知识。这表明，预训练的LLMs可能捕获了比最初看起来更多的感知知识。'
- en: Comparing Ranking Strategies
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较排名策略
- en: 'Table [3](#S4.T3 "Table 3 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    compares different strategies for generating rankings. The pointwise model can
    be used directly for this purpose. For the pairwise model, we show results with
    the SVM strategy and the Count strategy. For Count, we furthermore vary the number
    of pairwise judgments per entity (5 or 30). For this experiment, we use the (pointwise
    and pairwise) Llama2-13B models that were trained on WD+TG+Rocks. We evaluate
    the different models by comparing the predicted rankings with the ground truth
    in terms of Spearman $\rho$. We can see that the pairwise approaches outperform
    the pointwise model in this case. This is somewhat surprising, given the strong
    performance of the pointwise models in Table [1](#S4.T1 "Table 1 ‣ Wikidata ‣
    4 Datasets ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An
    Analysis of Fine-Tuning Strategies"). Essentially, because the ranking strategies
    aggregate many pairwise samples, the noisy nature of the pairwise judgments can
    to some extent be mitigated. The SVM method generally performs better than the
    Count method, especially in the case where only 5 judgments per entity are obtained.
    We also compare with the GPT-3 results reported by Chatterjee et al. ([2023](#bib.bib5)),
    finding that the pairwise Llama model consistently performs best.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 物理性质 ‣ 4 数据集 ‣ 使用LLMs对概念空间维度进行实体排名：微调策略分析") 比较了生成排名的不同策略。点对点模型可以直接用于此目的。对于成对模型，我们展示了SVM策略和Count策略的结果。对于Count策略，我们还改变了每个实体的成对判断数量（5或30）。在此实验中，我们使用了在WD+TG+Rocks上训练的（点对点和成对）Llama2-13B模型。我们通过比较预测排名和实际排名的Spearman
    $\rho$ 来评估不同的模型。我们可以看到，在这种情况下，成对方法优于点对点模型。这有些令人惊讶，因为点对点模型在表 [1](#S4.T1 "表 1 ‣
    Wikidata ‣ 4 数据集 ‣ 使用LLMs对概念空间维度进行实体排名：微调策略分析") 中表现强劲。实际上，由于排名策略聚合了许多成对样本，因此成对判断的噪声特性在一定程度上得到了缓解。SVM方法通常优于Count方法，尤其是在仅获得5个判断的情况下。我们还与Chatterjee等人报告的GPT-3结果进行比较
    ([2023](#bib.bib5))，发现成对Llama模型表现 consistently 最佳。
- en: Qualitative Analysis
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定性分析
- en: 'Table [4](#S4.T4 "Table 4 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    shows the 10 highest and lowest ranked entities, according to the rankings from
    the SVM method with the pairwise Llama2-13B model. The results for sweetness and
    saltiness were obtained with the model that was trained on WD+TG+Rocks. The rankings
    for scary and funny movies were obtained with the model that was trained on WD+Taste+Rocks.
    The ranking for population was obtained with the model that was trained on WD1\.
    The table shows that the model was successful in selecting these top and bottom
    ranked entities. The top-ranked entities for sweetness, for instance, are all
    clearly sweet food items, while none of the bottom ranked entities are. Similar
    observations can be made for the other features. The model is sometimes less successful
    in distinguishing middle-ranked entities from bottom-ranked entities. For instance,
    most cheeses appear at the bottom of the ground truth ranking, whereas the model
    predicted these to be somewhere closer to the middle.^(10)^(10)10A more detailed
    analysis of such errors can be found in the appendix. In the *population* example
    in Table [4](#S4.T4 "Table 4 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies"),
    we can see that while the top-ranked entities are all countries with a high population,
    their relative ranking is not accurate. For instance, Nigeria is only the 6th
    most populous country in the dataset, but it is ranked in second place.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S4.T4 "Table 4 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    显示了根据 SVM 方法和 pairwise Llama2-13B 模型的排名所得到的 10 个排名最高和最低的实体。甜度和咸度的结果是用训练于 WD+TG+Rocks
    模型得到的。恐怖和搞笑电影的排名是用训练于 WD+Taste+Rocks 模型得到的。人口的排名是用训练于 WD1 的模型得到的。表格显示模型在选择这些排名最高和最低的实体方面是成功的。例如，甜度排名最高的实体都是明显的甜食项，而排名最低的实体则没有。其他特征也可以做类似观察。模型在区分中间排名实体和底部排名实体时有时不够成功。例如，大多数奶酪出现在真实排名的底部，而模型预测这些奶酪在中间位置。^(10)^(10)10更详细的错误分析见附录。在表
    [4](#S4.T4 "Table 4 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    中的 *人口* 示例中，我们可以看到虽然排名最高的实体都是高人口的国家，但它们的相对排名不准确。例如，尼日利亚在数据集中仅为第 6 大人口国家，但它被排在第二位。'
- en: 6 Conclusions
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We have studied the problem of ranking entities along conceptual space dimensions,
    such as sweetness (for food), roughness (for rocks) or scary (for movies). We
    found that fine-tuning LLMs on data from one domain (e.g. taste) is a viable strategy
    for learning to extract rankings in unrelated domains (e.g. rocks), as long as
    both domains are perceptual. In contrast, LLMs that were fine-tuned on objective
    numerical features from Wikidata were less successful when applied to perceptual
    domains. When comparing pairwise and pointwise strategies, surprisingly, we found
    that pointwise methods were as successful as pairwise methods for making pairwise
    judgements (i.e. should entity $e_{1}$), although pairwise methods still had the
    advantage when such judgments were aggregated. Overall, our results suggest that
    the current generation of open-source LLMs, such as Llama and Mistral, can be
    effectively used for constructing high-quality conceptual space representations.
    However, further work is needed to construct more comprehensive training sets.
    Encouragingly, we found that subjective (and relatively noisy) rankings, such
    as those from the movies and books datasets, can also be effective, while being
    much easier to obtain than perceptual features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了沿概念空间维度（如甜度（针对食物）、粗糙度（针对岩石）或恐怖（针对电影））对实体进行排名的问题。我们发现，在一个领域（例如口味）上微调 LLM
    对于学习在不相关领域（例如岩石）中提取排名是一种可行的策略，只要两个领域都是感知性的。相比之下，在 Wikidata 上微调的 LLM 在应用于感知领域时效果较差。在比较
    pairwise 和 pointwise 策略时，我们惊讶地发现，pointwise 方法在进行 pairwise 判断（即实体 $e_{1}$ 是否应该在此）时与
    pairwise 方法同样成功，尽管 pairwise 方法在这些判断被汇总时仍具有优势。总体而言，我们的结果表明，当前一代开源 LLM，如 Llama 和
    Mistral，可以有效地用于构建高质量的概念空间表示。然而，仍需进一步工作以构建更全面的训练集。令人鼓舞的是，我们发现主观（且相对噪声较多）的排名，例如来自电影和书籍的数据集，也可以有效，并且比感知特征更容易获得。
- en: Limitations
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The performance of LLMs is highly sensitive to the prompting strategy. While
    we have made efforts to choose a reasonable prompt, it is likely that better results
    are possible with different choices. Furthermore, while we have tested a number
    of different LLMs, it is possible that other (existing or future) models of similar
    sizes may behave qualitatively different. Care should therefore be taken when
    drawing any conclusions about the limitations of LLMs in general. Moreover, the
    limitations we have identified might be particular to the specific fine-tuning
    techniques that we have used, rather than reflecting limitations of the underlying
    LLMs. When it comes to modelling subjective features, such as those in the movies
    and books datasets, it is important to acknowledge that people may have different
    points of view. When using conceptual space representations extracted from LLMs
    in downstream applications, we thus need to be aware that these representations
    are biased and, at best, can only represent a majority opinion.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的性能对提示策略高度敏感。虽然我们努力选择了合理的提示，但不同的选择可能会得到更好的结果。此外，虽然我们测试了多个不同的LLM，但其他（现有的或未来的）类似规模的模型可能会有不同的表现。因此，在对LLM的限制做出任何结论时应谨慎。此外，我们识别出的限制可能是特定于我们使用的特定微调技术，而不是反映底层LLM的限制。对于建模主观特征，如电影和书籍数据集中的特征，重要的是要承认人们可能有不同的观点。在下游应用中使用从LLM中提取的概念空间表示时，我们需要意识到这些表示是有偏见的，充其量只能代表大多数意见。
- en: Acknowledgments
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by EPSRC grant EP/V025961/1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了EPSRC资助，资助编号EP/V025961/1。
- en: References
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdou et al. (2021) Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella
    Frank, Ellie Pavlick, and Anders Søgaard. 2021. [Can language models encode perceptual
    structure without grounding? a case study in color](https://doi.org/10.18653/v1/2021.conll-1.9).
    In *Proceedings of the 25th Conference on Computational Natural Language Learning*,
    pages 109–132, Online. Association for Computational Linguistics.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdou 等（2021）Mostafa Abdou、Artur Kulmizev、Daniel Hershcovich、Stella Frank、Ellie
    Pavlick 和 Anders Søgaard。2021年。[语言模型是否能够在没有基础的情况下编码感知结构？以颜色为例](https://doi.org/10.18653/v1/2021.conll-1.9)。在
    *第25届计算自然语言学习会议论文集*，第109–132页，在线。计算语言学协会。
- en: Aisbett and Gibbon (2001) Janet Aisbett and Greg Gibbon. 2001. [A general formulation
    of conceptual spaces as a meso level representation](https://doi.org/10.1016/S0004-3702(01)00144-8).
    *Artif. Intell.*, 133(1-2):189–232.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aisbett 和 Gibbon（2001）Janet Aisbett 和 Greg Gibbon。2001年。[将概念空间的一般形式化作为中观层次表示](https://doi.org/10.1016/S0004-3702(01)00144-8)。*人工智能*，133(1-2):189–232。
- en: 'Bender and Koller (2020) Emily M. Bender and Alexander Koller. 2020. [Climbing
    towards NLU: On meaning, form, and understanding in the age of data](https://doi.org/10.18653/v1/2020.acl-main.463).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5185–5198, Online. Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender 和 Koller（2020）Emily M. Bender 和 Alexander Koller。2020年。[迈向自然语言理解：在数据时代的意义、形式和理解](https://doi.org/10.18653/v1/2020.acl-main.463)。在
    *第58届计算语言学协会年会论文集*，第5185–5198页，在线。计算语言学协会。
- en: 'Bosselut et al. (2019) Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya
    Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. [COMET: Commonsense transformers
    for automatic knowledge graph construction](https://doi.org/10.18653/v1/P19-1470).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 4762–4779, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bosselut 等（2019）Antoine Bosselut、Hannah Rashkin、Maarten Sap、Chaitanya Malaviya、Asli
    Celikyilmaz 和 Yejin Choi。2019年。[COMET：用于自动知识图谱构建的常识转换器](https://doi.org/10.18653/v1/P19-1470)。在
    *第57届计算语言学协会年会论文集*，第4762–4779页，意大利佛罗伦萨。计算语言学协会。
- en: Chatterjee et al. (2023) Usashi Chatterjee, Amit Gajbhiye, and Steven Schockaert.
    2023. [Cabbage sweeter than cake? analysing the potential of large language models
    for learning conceptual spaces](https://doi.org/10.18653/v1/2023.emnlp-main.725).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 11836–11842, Singapore. Association for Computational Linguistics.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chatterjee 等（2023）Usashi Chatterjee、Amit Gajbhiye 和 Steven Schockaert。2023年。[卷心菜比蛋糕更甜？分析大型语言模型在学习概念空间中的潜力](https://doi.org/10.18653/v1/2023.emnlp-main.725)。在
    *2023年自然语言处理实证方法会议论文集*，第11836–11842页，新加坡。计算语言学协会。
- en: 'Chen and Pu (2012) Li Chen and Pearl Pu. 2012. [Critiquing-based recommenders:
    survey and emerging trends](https://doi.org/10.1007/S11257-011-9108-6). *User
    Model. User Adapt. Interact.*, 22(1-2):125–150.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Pu (2012) Li Chen 和 Pearl Pu. 2012. [基于评论的推荐系统：综述与新兴趋势](https://doi.org/10.1007/S11257-011-9108-6)。*用户模型与用户适应互动*，22(1-2):125–150。
- en: 'Cohen et al. (2023) Roi Cohen, Mor Geva, Jonathan Berant, and Amir Globerson.
    2023. [Crawling the internal knowledge-base of language models](https://doi.org/10.18653/v1/2023.findings-eacl.139).
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    1856–1869, Dubrovnik, Croatia. Association for Computational Linguistics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等 (2023) Roi Cohen, Mor Geva, Jonathan Berant 和 Amir Globerson. 2023.
    [爬取语言模型的内部知识库](https://doi.org/10.18653/v1/2023.findings-eacl.139)。发表于 *计算语言学协会会议发现：EACL
    2023*，第1856–1869页，杜布罗夫尼克，克罗地亚。计算语言学协会。
- en: 'Derrac and Schockaert (2015) Joaquín Derrac and Steven Schockaert. 2015. [Inducing
    semantic relations from conceptual spaces: A data-driven approach to plausible
    reasoning](https://doi.org/10.1016/J.ARTINT.2015.07.002). *Artif. Intell.*, 228:66–94.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Derrac 和 Schockaert (2015) Joaquín Derrac 和 Steven Schockaert. 2015. [从概念空间引导语义关系：一种数据驱动的合理推理方法](https://doi.org/10.1016/J.ARTINT.2015.07.002)。*人工智能*，228:66–94。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2019. [BERT：用于语言理解的深度双向变换器的预训练](https://doi.org/10.18653/v1/N19-1423)。发表于 *2019年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇和短篇论文）*，第4171–4186页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: Gärdenfors (2000) Peter Gärdenfors. 2000. *Conceptual Spaces - the Geometry
    of Thought*. MIT Press.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gärdenfors (2000) Peter Gärdenfors. 2000. *概念空间 - 思维的几何学*。MIT出版社。
- en: 'Geng et al. (2022) Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and
    Yongfeng Zhang. 2022. [Recommendation as language processing (RLP): A unified
    pretrain, personalized prompt & predict paradigm (P5)](https://doi.org/10.1145/3523227.3546767).
    In *RecSys ’22: Sixteenth ACM Conference on Recommender Systems, Seattle, WA,
    USA, September 18 - 23, 2022*, pages 299–315\. ACM.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng 等 (2022) Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge 和 Yongfeng
    Zhang. 2022. [推荐系统作为语言处理 (RLP)：统一的预训练、个性化提示与预测范式 (P5)](https://doi.org/10.1145/3523227.3546767)。发表于
    *RecSys ’22：第十六届ACM推荐系统大会，西雅图，华盛顿州，美国，2022年9月18 - 23日*，第299–315页。ACM。
- en: 'Gienapp et al. (2022) Lukas Gienapp, Maik Fröbe, Matthias Hagen, and Martin
    Potthast. 2022. [Sparse pairwise re-ranking with pre-trained transformers](https://doi.org/10.1145/3539813.3545140).
    In *ICTIR ’22: The 2022 ACM SIGIR International Conference on the Theory of Information
    Retrieval, Madrid, Spain, July 11 - 12, 2022*, pages 72–80\. ACM.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gienapp 等 (2022) Lukas Gienapp, Maik Fröbe, Matthias Hagen 和 Martin Potthast.
    2022. [稀疏的成对重排序与预训练的变换器](https://doi.org/10.1145/3539813.3545140)。发表于 *ICTIR ’22：2022年ACM
    SIGIR信息检索理论国际会议，马德里，西班牙，2022年7月11 - 12日*，第72–80页。ACM。
- en: Gordon and Durme (2013) Jonathan Gordon and Benjamin Van Durme. 2013. [Reporting
    bias and knowledge acquisition](https://doi.org/10.1145/2509558.2509563). In *Proceedings
    of the 2013 workshop on Automated knowledge base construction, AKBC@CIKM 13, San
    Francisco, California, USA, October 27-28, 2013*, pages 25–30\. ACM.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gordon 和 Durme (2013) Jonathan Gordon 和 Benjamin Van Durme. 2013. [报告偏差与知识获取](https://doi.org/10.1145/2509558.2509563)。发表于
    *2013年自动化知识库建设研讨会论文集，AKBC@CIKM 13，旧金山，加利福尼亚州，美国，2013年10月27-28日*，第25–30页。ACM。
- en: Guo et al. (2022) Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie,
    Hui Xiong, and Qing He. 2022. [A survey on knowledge graph-based recommender systems](https://doi.org/10.1109/TKDE.2020.3028705).
    *IEEE Trans. Knowl. Data Eng.*, 34(8):3549–3568.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2022) Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui
    Xiong 和 Qing He. 2022. [基于知识图谱的推荐系统综述](https://doi.org/10.1109/TKDE.2020.3028705)。*IEEE
    知识与数据工程学报*，34(8):3549–3568。
- en: Gupta et al. (2015) Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian
    Padó. 2015. [Distributional vectors encode referential attributes](https://doi.org/10.18653/v1/D15-1002).
    In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language
    Processing*, pages 12–21, Lisbon, Portugal. Association for Computational Linguistics.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. (2015) Abhijeet Gupta, Gemma Boleda, Marco Baroni, 和 Sebastian
    Padó. 2015. [分布式向量编码指称属性](https://doi.org/10.18653/v1/D15-1002). 在 *2015年自然语言处理经验方法会议论文集*,
    页码 12–21, 葡萄牙里斯本. 计算语言学协会.
- en: 'Hao et al. (2023) Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe
    Zhang, Eric Xing, and Zhiting Hu. 2023. [BertNet: Harvesting knowledge graphs
    with arbitrary relations from pretrained language models](https://doi.org/10.18653/v1/2023.findings-acl.309).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    5000–5015, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hao et al. (2023) Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe
    Zhang, Eric Xing, 和 Zhiting Hu. 2023. [BertNet: 从预训练语言模型中提取具有任意关系的知识图谱](https://doi.org/10.18653/v1/2023.findings-acl.309).
    在 *计算语言学协会发现: ACL 2023*, 页码 5000–5015, 加拿大多伦多. 计算语言学协会.'
- en: Hase et al. (2024) Peter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreffe.
    2024. [The unreasonable effectiveness of easy training data for hard tasks](https://doi.org/10.48550/ARXIV.2401.06751).
    *CoRR*, abs/2401.06751.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hase et al. (2024) Peter Hase, Mohit Bansal, Peter Clark, 和 Sarah Wiegreffe.
    2024. [简单训练数据在困难任务中的不合理有效性](https://doi.org/10.48550/ARXIV.2401.06751). *CoRR*,
    abs/2401.06751.
- en: 'He et al. (2022) Yuan He, Jiaoyan Chen, Denvar Antonyrajah, and Ian Horrocks.
    2022. [Bertmap: A bert-based ontology alignment system](https://doi.org/10.1609/AAAI.V36I5.20510).
    In *Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth
    Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The
    Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022
    Virtual Event, February 22 - March 1, 2022*, pages 5684–5691\. AAAI Press.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2022) Yuan He, Jiaoyan Chen, Denvar Antonyrajah, 和 Ian Horrocks.
    2022. [Bertmap: 一种基于BERT的本体对齐系统](https://doi.org/10.1609/AAAI.V36I5.20510). 在
    *第36届AAAI人工智能会议, AAAI 2022, 第34届创新应用人工智能会议, IAAI 2022, 第12届人工智能教育进展研讨会, EAAI 2022虚拟活动,
    2022年2月22日 - 3月1日*, 页码 5684–5691\. AAAI出版社.'
- en: 'Huang et al. (2022) James Y. Huang, Bangzheng Li, Jiashu Xu, and Muhao Chen.
    2022. [Unified semantic typing with meaningful label inference](https://doi.org/10.18653/v1/2022.naacl-main.190).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2642–2654,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2022) James Y. Huang, Bangzheng Li, Jiashu Xu, 和 Muhao Chen.
    2022. [统一语义类型与有意义的标签推断](https://doi.org/10.18653/v1/2022.naacl-main.190). 在 *2022年北美计算语言学协会年会:
    人类语言技术会议论文集*, 页码 2642–2654, 美国西雅图. 计算语言学协会.'
- en: 'Kotkov et al. (2022) Denis Kotkov, Alan Medlar, Alexandr V. Maslov, Umesh Raj
    Satyal, Mats Neovius, and Dorota Glowacka. 2022. [The tag genome dataset for books](https://doi.org/10.1145/3498366.3505833).
    In *CHIIR ’22: ACM SIGIR Conference on Human Information Interaction and Retrieval,
    Regensburg, Germany, March 14 - 18, 2022*, pages 353–357\. ACM.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kotkov et al. (2022) Denis Kotkov, Alan Medlar, Alexandr V. Maslov, Umesh Raj
    Satyal, Mats Neovius, 和 Dorota Glowacka. 2022. [图书标签基因组数据集](https://doi.org/10.1145/3498366.3505833).
    在 *CHIIR ’22: ACM SIGIR 人机信息交互与检索会议, 德国雷根斯堡, 2022年3月14日 - 18日*, 页码 353–357\. ACM.'
- en: Li et al. (2023) Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng
    Kong, and Qi Liu. 2023. [Can language models understand physical concepts?](https://doi.org/10.18653/v1/2023.emnlp-main.726)
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 11843–11861, Singapore. Association for Computational Linguistics.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Lei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Xu Sun, Lingpeng
    Kong, 和 Qi Liu. 2023. [语言模型能理解物理概念吗？](https://doi.org/10.18653/v1/2023.emnlp-main.726)
    在 *2023年自然语言处理经验方法会议论文集*, 页码 11843–11861, 新加坡. 计算语言学协会.
- en: 'Liu et al. (2022a) Fangyu Liu, Julian Eisenschlos, Jeremy Cole, and Nigel Collier.
    2022a. [Do ever larger octopi still amplify reporting biases? evidence from judgments
    of typical colour](https://aclanthology.org/2022.aacl-short.27). In *Proceedings
    of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational
    Linguistics and the 12th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 210–220, Online only. Association for Computational
    Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2022a）范宇刘、朱利安·艾森施洛斯、杰里米·科尔和奈杰尔·科利尔。2022a. [更大的章鱼是否仍然放大报告偏差？来自典型颜色的判断证据](https://aclanthology.org/2022.aacl-short.27)。在*亚太计算语言学协会第2届会议暨第12届国际自然语言处理联合会议（第2卷：短文）*中，第210–220页，仅在线发布。计算语言学协会。
- en: 'Liu et al. (2022b) Xiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao. 2022b.
    [Things not written in text: Exploring spatial commonsense from visual signals](https://doi.org/10.18653/v1/2022.acl-long.168).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2365–2376, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2022b）肖刘、大尹、颜松凤和董岩赵。2022b. [文本中未写的内容：从视觉信号探索空间常识](https://doi.org/10.18653/v1/2022.acl-long.168)。在*计算语言学协会第60届年会（第1卷：长文）*中，第2365–2376页，都柏林，爱尔兰。计算语言学协会。
- en: 'Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel
    Khashabi, and Hannaneh Hajishirzi. 2023. [When not to trust language models: Investigating
    effectiveness of parametric and non-parametric memories](https://doi.org/10.18653/v1/2023.acl-long.546).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 9802–9822, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马伦等人（2023）亚历克斯·马伦、秋谷彩、维克托·钟、拉贾尔希·达斯、丹尼尔·赫什比和汉娜赫·哈吉希尔兹。2023. [何时不信任语言模型：调查参数记忆和非参数记忆的有效性](https://doi.org/10.18653/v1/2023.acl-long.546)。在*计算语言学协会第61届年会（第1卷：长文）*中，第9802–9822页，多伦多，加拿大。计算语言学协会。
- en: Martin et al. (2014) Christophe Martin, Michel Visalli, Christine Lange, Pascal
    Schlich, and Sylvie Issanchou. 2014. Creation of a food taste database using an
    in-home “taste” profile method. *Food Quality and Preference*, 36:70–80.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马丁等人（2014）克里斯托夫·马丁、米歇尔·维萨利、克里斯汀·朗格、帕斯卡尔·施利希和西尔维·伊桑丘。2014. 使用家庭“味道”档案方法创建食品味道数据库。*食品质量与偏好*，36:70–80。
- en: Merullo et al. (2023) Jack Merullo, Louis Castricato, Carsten Eickhoff, and
    Ellie Pavlick. 2023. [Linearly mapping from image to text space](https://openreview.net/pdf?id=8tYRqb05pVn).
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梅鲁洛等人（2023）杰克·梅鲁洛、路易斯·卡斯特里卡托、卡尔斯滕·艾克霍夫和艾莉·帕夫利克。2023. [从图像到文本空间的线性映射](https://openreview.net/pdf?id=8tYRqb05pVn)。在*第十一届国际学习表示会议，ICLR
    2023，2023年5月1-5日，卢旺达基加利*中。OpenReview.net。
- en: Nogueira et al. (2019) Rodrigo Frassetto Nogueira, Wei Yang, Kyunghyun Cho,
    and Jimmy Lin. 2019. [Multi-stage document ranking with BERT](http://arxiv.org/abs/1910.14424).
    *CoRR*, abs/1910.14424.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诺盖拉等人（2019）罗德里戈·弗拉塞托·诺盖拉、韦·杨、朴庸贤和吉米·林。2019. [使用BERT的多阶段文档排序](http://arxiv.org/abs/1910.14424)。*计算机科学研究报告*，abs/1910.14424。
- en: Nosofsky et al. (2018) Robert M Nosofsky, Craig A Sanders, Brian J Meagher,
    and Bruce J Douglas. 2018. Toward the development of a feature-space representation
    for a complex natural category domain. *Behavior Research Methods*, 50:530–556.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诺索夫斯基等人（2018）罗伯特·M·诺索夫斯基、克雷格·A·桑德斯、布莱恩·J·梅厄和布鲁斯·J·道格拉斯。2018. 朝着为复杂自然类别领域开发特征空间表示的方向发展。*行为研究方法*，50:530–556。
- en: 'Paik et al. (2021) Cory Paik, Stéphane Aroca-Ouellette, Alessandro Roncone,
    and Katharina Kann. 2021. [The World of an Octopus: How Reporting Bias Influences
    a Language Model’s Perception of Color](https://doi.org/10.18653/v1/2021.emnlp-main.63).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 823–835, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 佩克等人（2021）科里·佩克、斯特凡·阿罗卡-乌埃莱特、亚历山德罗·罗恩科内和卡塔琳娜·坎。2021. [章鱼的世界：报告偏差如何影响语言模型对颜色的感知](https://doi.org/10.18653/v1/2021.emnlp-main.63)。在*2021年自然语言处理实证方法会议论文集*中，第823–835页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Patel and Pavlick (2022) Roma Patel and Ellie Pavlick. 2022. Mapping language
    models to grounded conceptual spaces. In *International Conference on Learning
    Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕特尔和帕夫利克（2022）罗马·帕特尔和艾莉·帕夫利克。2022. 将语言模型映射到有根据的概念空间。 在*国际学习表示会议*中。
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. [Language models
    as knowledge bases?](https://doi.org/10.18653/v1/D19-1250) In *Proceedings of
    the 2019 Conference on Empirical Methods in Natural Language Processing and the
    9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等（2019）Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis,
    Anton Bakhtin, Yuxiang Wu, 和 Alexander Miller。2019年。[语言模型作为知识库？](https://doi.org/10.18653/v1/D19-1250)
    见于 *2019年自然语言处理经验方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP）*，页2463–2473，中国香港。计算语言学协会。
- en: Qin et al. (2023) Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu,
    Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael
    Bendersky. 2023. [Large language models are effective text rankers with pairwise
    ranking prompting](https://doi.org/10.48550/ARXIV.2306.17563). *CoRR*, abs/2306.17563.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等（2023）Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming
    Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, 和 Michael Bendersky。2023年。[大型语言模型是有效的文本排名器，配对排名提示](https://doi.org/10.48550/ARXIV.2306.17563)。*CoRR*，abs/2306.17563。
- en: 'Reinanda et al. (2020) Ridho Reinanda, Edgar Meij, and Maarten de Rijke. 2020.
    [Knowledge graphs: An information retrieval perspective](https://doi.org/10.1561/1500000063).
    *Found. Trends Inf. Retr.*, 14(4):289–444.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reinanda 等（2020）Ridho Reinanda, Edgar Meij, 和 Maarten de Rijke。2020年。[知识图谱：一种信息检索视角](https://doi.org/10.1561/1500000063)。*基础与趋势信息检索*，14(4)：289–444。
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. [How
    much knowledge can you pack into the parameters of a language model?](https://doi.org/10.18653/v1/2020.emnlp-main.437)
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 5418–5426, Online. Association for Computational Linguistics.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts 等（2020）Adam Roberts, Colin Raffel, 和 Noam Shazeer。2020年。[你能在语言模型的参数中容纳多少知识？](https://doi.org/10.18653/v1/2020.emnlp-main.437)
    见于 *2020年自然语言处理经验方法会议（EMNLP）*，页5418–5426，在线。计算语言学协会。
- en: 'Schneider et al. (2022) Phillip Schneider, Tim Schopf, Juraj Vladika, Mikhail
    Galkin, Elena Simperl, and Florian Matthes. 2022. [A decade of knowledge graphs
    in natural language processing: A survey](https://aclanthology.org/2022.aacl-main.46).
    In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics and the 12th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pages 601–614, Online only. Association
    for Computational Linguistics.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schneider 等（2022）Phillip Schneider, Tim Schopf, Juraj Vladika, Mikhail Galkin,
    Elena Simperl, 和 Florian Matthes。2022年。[自然语言处理中的知识图谱十年：一项调查](https://aclanthology.org/2022.aacl-main.46)。见于
    *第2届亚太计算语言学协会会议暨第12届国际联合自然语言处理会议（第1卷：长篇论文）*，页601–614，仅在线。计算语言学协会。
- en: 'Søgaard (2023) Anders Søgaard. 2023. [Grounding the vector space of an octopus:
    Word meaning from raw text](https://doi.org/10.1007/s11023-023-09622-4). *Minds
    Mach.*, 33(1):33–54.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Søgaard（2023）Anders Søgaard。2023年。[将章鱼的向量空间落地：从原始文本中提取词义](https://doi.org/10.1007/s11023-023-09622-4)。*思想机器*，33(1)：33–54。
- en: 'Standley et al. (2017) Trevor Standley, Ozan Sener, Dawn Chen, and Silvio Savarese.
    2017. [image2mass: Estimating the mass of an object from its image](http://proceedings.mlr.press/v78/standley17a.html).
    In *1st Annual Conference on Robot Learning, CoRL 2017, Mountain View, California,
    USA, November 13-15, 2017, Proceedings*, volume 78 of *Proceedings of Machine
    Learning Research*, pages 324–333\. PMLR.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Standley 等（2017）Trevor Standley, Ozan Sener, Dawn Chen, 和 Silvio Savarese。2017年。[image2mass：从图像中估计物体的质量](http://proceedings.mlr.press/v78/standley17a.html)。见于
    *第1届机器人学习年会，CoRL 2017，加利福尼亚州山景城，美国，2017年11月13-15日，会议录*，第78卷的 *机器学习研究会议录*，页324–333。PMLR。
- en: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, and Peng Jiang. 2019. [Bert4rec: Sequential recommendation with bidirectional
    encoder representations from transformer](https://doi.org/10.1145/3357384.3357895).
    In *Proceedings of the 28th ACM International Conference on Information and Knowledge
    Management, CIKM 2019, Beijing, China, November 3-7, 2019*, pages 1441–1450\.
    ACM.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2019）Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, 和 Peng
    Jiang。2019年。[Bert4rec：基于变换器的双向编码器表示的序列推荐](https://doi.org/10.1145/3357384.3357895)。见于
    *第28届ACM国际信息与知识管理会议，CIKM 2019，北京，中国，2019年11月3-7日*，页1441–1450。ACM。
- en: Ushio et al. (2023) Asahi Ushio, José Camacho-Collados, and Steven Schockaert.
    2023. [A relentless benchmark for modelling graded relations between named entities](https://doi.org/10.48550/ARXIV.2305.15002).
    *CoRR*, abs/2305.15002.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ushio 等（2023） Asahi Ushio, José Camacho-Collados 和 Steven Schockaert. 2023.
    [一个无情的基准：建模命名实体之间的等级关系](https://doi.org/10.48550/ARXIV.2305.15002)。*CoRR*，abs/2305.15002。
- en: 'Vig et al. (2012) Jesse Vig, Shilad Sen, and John Riedl. 2012. [The tag genome:
    Encoding community knowledge to support novel interaction](https://doi.org/10.1145/2362394.2362395).
    *ACM Trans. Interact. Intell. Syst.*, 2(3):13:1–13:44.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig 等（2012） Jesse Vig, Shilad Sen 和 John Riedl. 2012. [标签基因组：编码社区知识以支持新型互动](https://doi.org/10.1145/2362394.2362395)。*ACM交互智能系统期刊*，2(3):13:1–13:44。
- en: Vigna (2016) Sebastiano Vigna. 2016. Spectral ranking. *Network Science*, 4(4):433–445.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vigna（2016） Sebastiano Vigna. 2016. Spectral ranking. *网络科学*，4(4):433–445。
- en: Wauthier et al. (2013) Fabian L. Wauthier, Michael I. Jordan, and Nebojsa Jojic.
    2013. [Efficient ranking from pairwise comparisons](http://proceedings.mlr.press/v28/wauthier13.html).
    In *Proceedings of the 30th International Conference on Machine Learning, ICML
    2013, Atlanta, GA, USA, 16-21 June 2013*, volume 28 of *JMLR Workshop and Conference
    Proceedings*, pages 109–117\. JMLR.org.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wauthier 等（2013） Fabian L. Wauthier, Michael I. Jordan 和 Nebojsa Jojic. 2013.
    [从成对比较中高效排序](http://proceedings.mlr.press/v28/wauthier13.html)。在 *第30届国际机器学习大会，ICML
    2013，亚特兰大，GA，美国，2013年6月16-21日*，*JMLR研讨会与会议记录*第28卷，页109–117。JMLR.org。
- en: 'West et al. (2022) Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang,
    Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. 2022. [Symbolic
    knowledge distillation: from general language models to commonsense models](https://doi.org/10.18653/v1/2022.naacl-main.341).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4602–4625,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: West 等（2022） Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei
    Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck 和 Yejin Choi. 2022. [符号知识蒸馏：从通用语言模型到常识模型](https://doi.org/10.18653/v1/2022.naacl-main.341)。在
    *2022年北美计算语言学学会会议：人类语言技术*，页4602–4625，西雅图，美国。计算语言学学会。
- en: 'Yao et al. (2019) Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. [KG-BERT:
    BERT for knowledge graph completion](http://arxiv.org/abs/1909.03193). *CoRR*,
    abs/1909.03193.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2019） Liang Yao, Chengsheng Mao 和 Yuan Luo. 2019. [KG-BERT：用于知识图谱补全的BERT](http://arxiv.org/abs/1909.03193)。*CoRR*，abs/1909.03193。
- en: 'Yu et al. (2023) Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song,
    Zheng Li, Yifan Gao, Tianyu Cao, and Bing Yin. 2023. [FolkScope: Intention knowledge
    graph construction for E-commerce commonsense discovery](https://doi.org/10.18653/v1/2023.findings-acl.76).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    1173–1191, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2023） Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng
    Li, Yifan Gao, Tianyu Cao 和 Bing Yin. 2023. [FolkScope：电子商务常识发现的意图知识图谱构建](https://doi.org/10.18653/v1/2023.findings-acl.76)。在
    *计算语言学学会发现：ACL 2023*，页1173–1191，多伦多，加拿大。计算语言学学会。
- en: Zhu et al. (2024) Jian-Qiao Zhu, Haijiang Yan, and Thomas L. Griffiths. 2024.
    [Recovering mental representations from large language models with markov chain
    monte carlo](http://arxiv.org/abs/2401.16657).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2024） Jian-Qiao Zhu, Haijiang Yan 和 Thomas L. Griffiths. 2024. [从大型语言模型中恢复心理表征：使用马尔科夫链蒙特卡罗](http://arxiv.org/abs/2401.16657)。
- en: '|  | Wikidata | Taste | Rocks | TG | Phys |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | Wikidata | 味道 | 岩石 | TG | 物理 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  WD1-test  |  WD2  |  Sweetness  |  Saltiness  |  Sourness  |  Bitterness  |  Umaminess  |  Fattiness  |  Lightness  |  Grain
    size  |  Roughness  |  Shininess  |  Organisation  |  Variability  |  Density  |  Movies  |  Books  |  Size  |  Height  |  Mass  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  WD1-test  |  WD2  |  甜度  |  咸度  |  酸度  |  苦度  |  鲜味  |  脂肪度  |  轻度  |  粒度  |  粗糙度  |  光泽度  |  组织度  |  变异性  |  密度  |  电影  |  书籍  |  大小  |  高度  |  质量  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ChatGPT | 0.10 | 0.10 | 0.00 | 0.00 | 0.40 | 0.00 | 0.00 | 0.20 | 0.00 |
    0.00 | 0.40 | 0.20 | 0.20 | 0.20 | 0.20 | 0.47 | 1.00 | 0.20 | 0.00 | 0.40 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 0.10 | 0.10 | 0.00 | 0.00 | 0.40 | 0.00 | 0.00 | 0.20 | 0.00 |
    0.00 | 0.40 | 0.20 | 0.20 | 0.20 | 0.20 | 0.47 | 1.00 | 0.20 | 0.00 | 0.40 |'
- en: '| GPT-4 | 2.04 | 0.12 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00
    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.82 | 4.16 | 0.00 | 0.00 | 0.00 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 2.04 | 0.12 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00
    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.82 | 4.16 | 0.00 | 0.00 | 0.00 |'
- en: 'Table 5: Percentage of cases where ChatGPT and GPT-4 refused to answer a question
    about a pairwise comparison between two entities.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: ChatGPT和GPT-4在对两个实体的成对比较问题上拒绝回答的案例百分比。'
- en: Appendix A Further Details
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 详细信息
- en: Training Details
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练详情
- en: To train the three base models, i.e., meta-llama/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1,
    and meta-llama/Llama-2-13b-hf, we used the QLoRa method, which allows converting
    the floating-point 32 format to smaller data types. In particular, for all three
    models, we used 4-bit quantization for efficient training. In the QLoRa configuration,
    $r$ (the scaling factor for the learned weights) was set to 64, and dropout was
    set to 0.05\. We applied QLoRa to all the linear layers of the models, including
    q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, and lm_head. The
    models were trained with a batch size of 8\. We used 20% of the WD1 training split
    as a validation set. Based on this validation set, we fixed the number of training
    steps to 25,000 for the pairwise models and 1,500 for the pointwise models. Note
    that we need fewer training steps for the pointwise model, because each mini-batch
    consists of 8 entities, and we consider all pairwise combinations of these entities.
    In contrast, for the pairwise model, each mini-batch consists of 8 pairwise combinations.
    We also observed that the pointwise model converges more quickly than the pairwise
    model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练三个基础模型，即meta-llama/Llama-2-7b-hf、mistralai/Mistral-7B-v0.1和meta-llama/Llama-2-13b-hf，我们使用了QLoRa方法，该方法允许将32位浮点格式转换为更小的数据类型。具体来说，对于所有三个模型，我们使用了4位量化以提高训练效率。在QLoRa配置中，$r$（学习到的权重的缩放因子）设置为64，dropout设置为0.05。我们对模型的所有线性层应用了QLoRa，包括q_proj、k_proj、v_proj、o_proj、gate_proj、up_proj、down_proj和lm_head。模型的训练批次大小为8。我们使用了WD1训练集的20%作为验证集。根据该验证集，我们将成对模型的训练步骤数固定为25,000，将点对模型的训练步骤数固定为1,500。请注意，我们需要的点对模型训练步骤较少，因为每个小批次包含8个实体，我们考虑这些实体的所有成对组合。相比之下，对于成对模型，每个小批次包含8个成对组合。我们还观察到，点对模型的收敛速度比成对模型更快。
- en: OpenAI Models
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: OpenAI 模型
- en: 'Table [5](#A0.T5 "Table 5 ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies") show for how many cases ChatGPT
    and GPT-4 failed to answer with yes or no, when asked about pairwise comparisons.
    Overall, such cases were rare. The highest number of failures were seen for TG
    dataset, which appears to be related to the subjective nature of the features
    involved.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#A0.T5 "表 5 ‣ 按概念空间维度对实体进行排名的分析：微调策略的分析")展示了ChatGPT和GPT-4在被询问成对比较时多少情况下无法用“是”或“否”回答。总体来说，这种情况很少见。TG数据集的失败次数最多，这似乎与涉及的特征的主观性有关。
- en: '|  | Entity type | Feature | Size |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | 实体类型 | 特征 | 规模 |'
- en: '| WD1 | mountain | elevation | 1000 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| WD1 | 山脉 | 海拔 | 1000 |'
- en: '| building | height | 1000 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 建筑物 | 高度 | 1000 |'
- en: '| river | length | 1000 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 河流 | 长度 | 1000 |'
- en: '| person | # social media followers | 1000 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 人 | # 社交媒体关注者 | 1000 |'
- en: '| city | population | 1000 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | 人口 | 1000 |'
- en: '| species | mass | 1000 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 物种 | 质量 | 1000 |'
- en: '| organisation | inception date | 1000 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 组织 | 成立日期 | 1000 |'
- en: '| person | date of birth | 1000 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 人 | 出生日期 | 1000 |'
- en: '| museum | latitude | 1000 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 博物馆 | 纬度 | 1000 |'
- en: '| landform | area | 1000 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 地貌 | 面积 | 1000 |'
- en: '| WD2 | country | population | 196 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| WD2 | 国家 | 人口 | 196 |'
- en: '| musical object | inception date | 561 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 音乐对象 | 成立日期 | 561 |'
- en: '| chemical element | atomic number | 166 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 化学元素 | 原子序数 | 166 |'
- en: '| chemical element | discovery date | 113 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 化学元素 | 发现日期 | 113 |'
- en: '| building | # elevators | 151 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 建筑物 | # 电梯 | 151 |'
- en: '| director | # academy awards | 65 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 导演 | # 奥斯卡奖 | 65 |'
- en: '| actor | # academy awards | 74 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 演员 | # 奥斯卡奖 | 74 |'
- en: '| food | water footprint | 56 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 食物 | 水足迹 | 56 |'
- en: '| composer | # grammy awards | 71 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 作曲家 | # 格莱美奖 | 71 |'
- en: '| food | Scoville grade | 43 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 食物 | Scoville 级别 | 43 |'
- en: 'Table 6: Overview of the datasets based on Wikidata.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 基于Wikidata的数据集概述。'
- en: '| Tag | #Movies |  |  | Tag | #Movies |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | #电影 |  |  | 标签 | #电影 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| scary | 82 |  |  | grim | 20 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 可怕 | 82 |  |  | 严峻 | 20 |'
- en: '| funny | 217 |  |  | gritty | 34 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 有趣 | 217 |  |  | 砂砾 | 34 |'
- en: '| gory | 33 |  |  | inspirational | 90 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 恐怖 | 33 |  |  | 鼓舞人心 | 90 |'
- en: '| dark | 139 |  |  | intelligent | 18 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 黑暗 | 139 |  |  | 智能 | 18 |'
- en: '| beautiful | 117 |  |  | intense | 53 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 美丽 | 117 |  |  | 强烈 | 53 |'
- en: '| intellectual | 32 |  |  | melancholic | 17 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 知识分子的 | 32 |  |  | 忧郁的 | 17 |'
- en: '| artistic | 91 |  |  | predictable | 121 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 艺术的 | 91 |  |  | 可预测的 | 121 |'
- en: '| absurd | 20 |  |  | pretentious | 29 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 荒谬的 | 20 |  |  | 做作的 | 29 |'
- en: '| bleak | 23 |  |  | quirky | 151 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 阴郁的 | 23 |  |  | 古怪的 | 151 |'
- en: '| bloody | 27 |  |  | realistic | 74 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 血腥的 | 27 |  |  | 现实的 | 74 |'
- en: '| boring | 186 |  |  | romantic | 46 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 无聊的 | 186 |  |  | 浪漫的 | 46 |'
- en: '| claustrophobic | 19 |  |  | sad | 130 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 幽闭恐惧的 | 19 |  |  | 悲伤的 | 130 |'
- en: '| clever | 68 |  |  | satirical | 106 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 聪明的 | 68 |  |  | 讽刺的 | 106 |'
- en: '| complex | 23 |  |  | sentimental | 28 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 复杂的 | 23 |  |  | 感伤的 | 28 |'
- en: '| controversial | 44 |  |  | surreal | 241 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 有争议的 | 44 |  |  | 超现实的 | 241 |'
- en: '| dramatic | 24 |  |  | suspenseful | 19 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 戏剧性的 | 24 |  |  | 悬疑的 | 19 |'
- en: '| emotional | 34 |  |  | tense | 40 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 情感的 | 34 |  |  | 紧张的 | 40 |'
- en: '| enigmatic | 36 |  |  | violent | 132 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 神秘的 | 36 |  |  | 暴力的 | 132 |'
- en: '| frightening | 18 |  |  | witty | 47 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 令人恐惧的 | 18 |  |  | 机智的 | 47 |'
- en: 'Table 7: Considered set of tags for the Movies dataset.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 电影数据集中考虑的标签集。'
- en: '| Tag | #Books |  |  | Tag | #Books |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | #Books |  |  | 标签 | #Books |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| absurd | 106 |  |  | literary | 525 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 荒谬的 | 106 |  |  | 文学的 | 525 |'
- en: '| beautiful | 28 |  |  | philosophical | 80 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 美丽的 | 28 |  |  | 哲学的 | 80 |'
- en: '| bizarre | 37 |  |  | political | 138 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 离奇的 | 37 |  |  | 政治的 | 138 |'
- en: '| controversial | 40 |  |  | predictable | 27 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 有争议的 | 40 |  |  | 可预测的 | 27 |'
- en: '| cool | 31 |  |  | quirky | 20 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 酷的 | 31 |  |  | 古怪的 | 20 |'
- en: '| crazy | 23 |  |  | realistic | 36 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 疯狂的 | 23 |  |  | 现实的 | 36 |'
- en: '| dark | 659 |  |  | romantic | 125 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 黑暗的 | 659 |  |  | 浪漫的 | 125 |'
- en: '| educational | 121 |  |  | sad | 154 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 教育的 | 121 |  |  | 悲伤的 | 154 |'
- en: '| funny | 331 |  |  | satirical | 23 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 有趣的 | 331 |  |  | 讽刺的 | 23 |'
- en: '| futuristic | 157 |  |  | short | 30 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 未来的 | 157 |  |  | 短的 | 30 |'
- en: '| gritty | 18 |  |  | silly | 25 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 粗糙的 | 18 |  |  | 愚蠢的 | 25 |'
- en: '| hilarious | 66 |  |  | strange | 22 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 滑稽的 | 66 |  |  | 奇怪的 | 22 |'
- en: '| inspirational | 195 |  |  | surreal | 28 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 鼓舞人心的 | 195 |  |  | 超现实的 | 28 |'
- en: '| intellectual | 17 |  |  | unique | 31 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 知识分子的 | 17 |  |  | 独特的 | 31 |'
- en: '| intense | 27 |  |  | weird | 46 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 强烈的 | 27 |  |  | 奇怪的 | 46 |'
- en: '| interesting | 33 |  |  | witty | 17 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 有趣的 | 33 |  |  | 机智的 | 17 |'
- en: 'Table 8: Considered set of tags for the Books dataset.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 书籍数据集中考虑的标签集。'
- en: Datasets
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'Table [6](#A1.T6 "Table 6 ‣ OpenAI Models ‣ Appendix A Further Details ‣ Ranking
    Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning
    Strategies") gives an overview of the properties that were selected for the WD1
    and WD2 datasets, along with the corresponding number of entities. Table [7](#A1.T7
    "Table 7 ‣ OpenAI Models ‣ Appendix A Further Details ‣ Ranking Entities along
    Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies")
    and [8](#A1.T8 "Table 8 ‣ OpenAI Models ‣ Appendix A Further Details ‣ Ranking
    Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning
    Strategies") similarly show the tags that have been considered for the Movies
    and Books datasets, along with the number of corresponding entities.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#A1.T6 "表 6 ‣ OpenAI 模型 ‣ 附录 A 进一步细节 ‣ 使用 LLM 对概念空间维度进行排名的实体: 微调策略分析")
    概述了为 WD1 和 WD2 数据集选择的属性，以及相应的实体数量。表 [7](#A1.T7 "表 7 ‣ OpenAI 模型 ‣ 附录 A 进一步细节 ‣
    使用 LLM 对概念空间维度进行排名的实体: 微调策略分析") 和 [8](#A1.T8 "表 8 ‣ OpenAI 模型 ‣ 附录 A 进一步细节 ‣ 使用
    LLM 对概念空间维度进行排名的实体: 微调策略分析") 类似地展示了为电影和书籍数据集考虑的标签，以及相应的实体数量。'
- en: '| Feature | Entities ranked too high | Entities ranked too low |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 排名过高的实体 | 排名过低的实体 |'
- en: '| --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sweetness | coulommiers cheese, chaource cheese, mimolette cheese, pasta
    with soy sauce, latte without sugar, reblochon cheese, plain yogurt, mont d’or
    cheese, saint-agur cheese, faisselle | martini with lemon juice, martini, cod
    fritters, champignons crus with vinaigrette, hamburger, salty crackers, light
    lager, andouillette sausage, soft boiled eggs, omelette with vinegar |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 甜美的 | coulommiers cheese, chaource cheese, mimolette cheese, pasta with soy
    sauce, latte without sugar, reblochon cheese, plain yogurt, mont d’or cheese,
    saint-agur cheese, faisselle | martini with lemon juice, martini, cod fritters,
    champignons crus with vinaigrette, hamburger, salty crackers, light lager, andouillette
    sausage, soft boiled eggs, omelette with vinegar |'
- en: '| Saltiness | marinated mussels, liquorice candy, fortified wines, kir, oriental
    pastries, cola soda, petit suisse with sugar, petit suisse with sugar and cream,
    aperitif with anise, petit suisse | carrot puree with cream, mix vegetables salad,
    moussaka, guacamole, pies, zucchini, stuffed zucchini, quiches, bulgur, broccoli
    with cream |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 咸味 | 腌制贻贝、甘草糖、强化葡萄酒、基尔、东方糕点、可乐、加糖的小奶酪、加糖和奶油的小奶酪、含茴香的开胃酒、小奶酪 | 奶油胡萝卜泥、混合蔬菜沙拉、茄子肉饼、鳄梨酱、派、意大利青瓜、填充意大利青瓜、咸饼、粗麦粉、奶油西兰花
    |'
- en: '| Scary | Pirates of the Caribbean: The Curse of the Black Pearl (2003), Interview
    with the Vampire: The Vampire Chronicles (1994), Scream (1996), Terminator, The
    (1984), Quills (2000), Batman Begins (2005), Evil Dead II (Dead by Dawn) (1987),
    Dawn of the Dead (1978), Spirited Away (Sen to Chihiro no kamikakushi) (2001),
    Requiem for a Dream (2000) | Final Fantasy: The Spirits Within (2001), Eye of
    the Needle (1981), Sunless (Sans Soleil) (1983), Outland (1981), Super Size Me
    (2004), Slumdog Millionaire (2008), Underworld (2003), Roger & Me (1989), One
    Hour Photo (2002), Close Encounters of the Third Kind (1977) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 恐怖 | 《加勒比海盗：黑珍珠的诅咒》（2003）、《与吸血鬼的访谈：吸血鬼编年史》（1994）、《惊声尖叫》（1996）、《终结者》（1984）、《羽毛笔》（2000）、《蝙蝠侠：开战时刻》（2005）、《鬼玩人
    II》（1987）、《僵尸新娘》（1978）、《千与千寻》（2001）、《梦幻之歌》（2000） | 《最终幻想：灵魂深处》（2001）、《针眼》（1981）、《无日》（1983）、《外星人》（1981）、《超级大号我》（2004）、《贫民窟的百万富翁》（2008）、《地下世界》（2003）、《罗杰与我》（1989）、《一小时照片》（2002）、《第三类接触》（1977）
    |'
- en: '| Funny | Fargo (1996), Original Kings of Comedy, The (2000), Meet the Spartans
    (2008), Simpsons Movie, The (2007), Happy Gilmore (1996), Jackass Number Two (2006),
    Who Framed Roger Rabbit? (1988), Tenacious D in The Pick of Destiny (2006), Men
    in Black (a.k.a. MIB) (1997), Elf (2003) | Ref, The (1994), American Psycho (2000),
    Run Lola Run (Lola rennt) (1998), Charter Trip, The (a.k.a. Package Tour, The)
    (1980), Bend It Like Beckham (2002), License to Drive (1988), Battleship Potemkin
    (1925), Jesus Camp (2006), Slap Shot (1977), Night of the Living Dead (1968) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 幽默 | 《冰血暴》（1996）、《原版喜剧之王》（2000）、《斯巴达300勇士》（2008）、《辛普森一家》（2007）、《快乐的大脚》（1996）、《二号小子》（2006）、《谁陷害了兔子罗杰》（1988）、《十acious
    D的命运之选》（2006）、《黑衣人》（1997）、《精灵》（2003） | 《参考》（1994）、《美国精神病人》（2000）、《跑吧，萝拉》（1998）、《包车之旅》（1980）、《贝克汉姆的婚礼》（2002）、《驾驶执照》（1988）、《战舰波将金》（1925）、《耶稣营》（2006）、《冰球杀手》（1977）、《活人夜》（1968）
    |'
- en: '| Population | Djibouti, Qatar, Eritrea, Botswana, Papua New Guinea, Gabon,
    Libya, Mongolia, Mauritania, Namibia | Burundi, Rwanda, Switzerland, Wales, Kingdom
    of the Netherlands, Belgium, England, Italy, Netherlands, Czech Republic |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 人口 | 吉布提、卡塔尔、厄立特里亚、博茨瓦纳、巴布亚新几内亚、加蓬、利比亚、蒙古、毛里塔尼亚、纳米比亚 | 布隆迪、卢旺达、瑞士、威尔士、荷兰王国、比利时、英格兰、意大利、荷兰、捷克共和国
    |'
- en: 'Table 9: Error analysis, showing the entities with the maximum difference in
    rank position between the ground truth ranking and the predicted ranking.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：误差分析，显示地面真实排名与预测排名之间排名位置差异最大的实体。
- en: '![Refer to caption](img/3dad7d9183a328dacb1af612f37cab69.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3dad7d9183a328dacb1af612f37cab69.png)'
- en: 'Figure 1: Scatter plot comparing the popularity of Wikidata entities (X-axis)
    with the prediction error (Y-axis) for the countries population feature.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：散点图比较维基数据实体的流行度（X轴）与国家人口特征的预测误差（Y轴）。
- en: Appendix B Additional Analysis
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附加分析
- en: Error Analysis
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 误差分析
- en: 'Table [9](#A1.T9 "Table 9 ‣ Datasets ‣ Appendix A Further Details ‣ Ranking
    Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning
    Strategies") presents an error analysis for the same five rankings that were considered
    in Table [4](#S4.T4 "Table 4 ‣ Physical Properties ‣ 4 Datasets ‣ Ranking Entities
    along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies").
    Specifically, in Table [9](#A1.T9 "Table 9 ‣ Datasets ‣ Appendix A Further Details
    ‣ Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of
    Fine-Tuning Strategies") we focus on the entities where the difference between
    the predicted ranking position and the position of the entity in the ground truth
    ranking is highest. On the left, we show entities which are ranked too high (i.e. where
    the model predicts the entity has the feature to a greater extent than is the
    case according to the ground truth). On the right, we show entities which are
    ranked too low. In the case of sweetness, we can see that the model consistently
    ranks cheeses to high. They are predicted to be in ranking positions 150-250,
    whereas the ground truth puts them at 500-590\. In the case of saltiness, we can
    see that sweet drinks and pastries are ranked too high. For instance, *cola soda*
    is ranked in position 108 whereas the ground truth puts it at 517 (out of 590).
    Overall, these results suggest that the model struggles with certain food groups.
    For the features scary, funny and population, clear patterns are harder to detect.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9](#A1.T9 "表 9 ‣ 数据集 ‣ 附录 A 进一步细节 ‣ 基于LLM的概念空间维度实体排名：微调策略分析")展示了与表[4](#S4.T4
    "表 4 ‣ 物理属性 ‣ 4 数据集 ‣ 基于LLM的概念空间维度实体排名：微调策略分析")中考虑的相同五个排名的误差分析。特别是在表[9](#A1.T9
    "表 9 ‣ 数据集 ‣ 附录 A 进一步细节 ‣ 基于LLM的概念空间维度实体排名：微调策略分析")中，我们重点关注了预测排名位置与真实排名位置差异最大的实体。在左侧，我们展示了排名过高的实体（即模型预测该实体的特征程度高于真实情况）。在右侧，我们展示了排名过低的实体。在甜味方面，我们可以看到模型始终将奶酪排名过高。它们被预测排名在150-250之间，而真实排名在500-590之间。在咸味方面，我们可以看到甜饮料和糕点排名过高。例如，*可乐*的排名是108，而真实排名是517（共590）。总体而言，这些结果表明模型在某些食品类别上表现不佳。对于恐怖、搞笑和人口特征，明确的模式较难发现。
- en: Impact of Entity Popularity
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实体受欢迎程度的影响
- en: 'The reliability of LLMs when it comes to modelling entity knowledge has been
    found to correlate with the popularity of the entities involved Mallen et al.
    ([2023](#bib.bib24)). To analyse this aspect, Figure [1](#A1.F1 "Figure 1 ‣ Datasets
    ‣ Appendix A Further Details ‣ Ranking Entities along Conceptual Space Dimensions
    with LLMs: An Analysis of Fine-Tuning Strategies") compares entity popularity
    with prediction error, for the countries population feature from the WD2 dataset.
    For this analysis, we have used the pairwise Llama2-13B model that was trained
    on the WD1-training split. We obtained a ranking of all countries using the SVM
    method with 20 samples. On the X-axis, the entities are ranked from the most popular
    to the least popular. On the Y-axis, we show the prediction error for the corresponding
    entity, measured as the difference between the position of the entity in the predicted
    ranking and its position in the ground truth ranking. Based on this analysis,
    no clear correlation between entity popularity and prediction error can be observed.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: LLM在建模实体知识方面的可靠性与所涉及实体的受欢迎程度存在相关性 Mallen et al. ([2023](#bib.bib24))。为了分析这一方面，图[1](#A1.F1
    "图 1 ‣ 数据集 ‣ 附录 A 进一步细节 ‣ 基于LLM的概念空间维度实体排名：微调策略分析")将实体的受欢迎程度与预测误差进行了比较，数据来自WD2数据集的国家人口特征。我们使用了在WD1训练集上训练的Llama2-13B模型进行配对分析。我们通过SVM方法使用20个样本对所有国家进行了排名。在X轴上，实体按从最受欢迎到最不受欢迎进行排序。在Y轴上，我们展示了对应实体的预测误差，测量为实体在预测排名中的位置与其在真实排名中的位置之间的差异。基于这项分析，未观察到实体受欢迎程度与预测误差之间的明显相关性。
