- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:34:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:34:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Minor SFT损失用于LLM微调以提高性能并减少模型偏差
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10642](https://ar5iv.labs.arxiv.org/html/2408.10642)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.10642](https://ar5iv.labs.arxiv.org/html/2408.10642)
- en: Shiming Xie First author    Hong Chen    Fred Yu    Zeye Sun    Xiuyu Wu   
    {shiming.xsm, wuyi.chen, fred.yf, zeye.szy, wuxiuyu.wxy }@antgroup.com
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谢世铭 第一作者    陈宏    余飞    孙泽耘    吴秀瑜    {shiming.xsm, wuyi.chen, fred.yf, zeye.szy,
    wuxiuyu.wxy }@antgroup.com
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Instruct LLM provide a paradigm used in large scale language model to align
    LLM to human preference. The paradigm contains supervised fine tuning and reinforce
    learning from human feedback. This paradigm is also used in downstream scenarios
    to adapt LLM to specific corpora and applications. Comparing to SFT, there are
    many efforts focused on RLHF and several algorithms being proposed, such as PPO,
    DPO, IPO, KTO, MinorDPO and etc. Meanwhile most efforts for SFT are focused on
    how to collect, filter and mix high quality data. In this article with insight
    from DPO and MinorDPO, we propose a training metric for SFT to measure the discrepancy
    between the optimized model and the original model, and a loss function MinorSFT
    that can increase the training effectiveness, and reduce the discrepancy between
    the optimized LLM and original LLM.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 指导LLM提供了一个在大规模语言模型中对齐LLM与人类偏好的范例。这个范例包含监督性微调和从人类反馈中强化学习。该范例还用于下游场景，以使LLM适应特定语料和应用。与SFT相比，许多努力集中在RLHF上，并提出了几种算法，如PPO、DPO、IPO、KTO、MinorDPO等。同时，大多数SFT的努力集中在如何收集、过滤和混合高质量数据。在这篇文章中，结合DPO和MinorDPO的见解，我们提出了一个SFT训练指标来衡量优化模型与原始模型之间的差异，以及一个损失函数MinorSFT，旨在提高训练效果，并减少优化LLM与原始LLM之间的差异。
- en: 1 Background
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 背景
- en: LLM trained on very large corpora is extremely powerful language model for completion
    tasks. SFT and RLHF(Ouyang et al., ([2022](#bib.bib6)), Ziegler et al., ([2020](#bib.bib14)))
    are two techniques that used to expose the LLM capability and align LLM answer
    to human instructions. With the increasing reasoning abilities, LLM are widely
    used in industries, and SFT and RLHF are also used to inject domain knowledge
    into LLM by training on domain corpora.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常大规模的语料上训练的LLM是极其强大的语言模型，适用于完成任务。SFT和RLHF(Ouyang等人，([2022](#bib.bib6))，Ziegler等人，([2020](#bib.bib14)))是两种用于暴露LLM能力并将LLM回答对齐到人类指令的技术。随着推理能力的提高，LLM在工业界广泛应用，SFT和RLHF也被用于通过在领域语料上训练将领域知识注入LLM。
- en: In the past most works are focused on RLHF and several algorithms are proposed,
    such as PPOSchulman et al., ([2017](#bib.bib11)), DPO(Rafailov et al., ([2023](#bib.bib10))),
    IPO(Azar et al., ([2023](#bib.bib2))), KTO(Ethayarajh et al., ([2024](#bib.bib4))),
    MinorDPO(Xie et al., ([2024](#bib.bib12))) and etc. One important point of RLHF
    is to constraint the optimized model not to deviate from the original model too
    much during the training, and thus PPO use KL constraints, DPO use a sample level
    dynamic coefficient related to distance between the preference pair, and IPO use
    a targeted distance between the preference pair and etc. The purpose of this constraint
    is to avoid over-fit on the domain corpora and to maintain LLM generalities. It’s
    an important hypothesis that the base model is powerful enough and the training
    should not change the language distribution too much to maintain the generality
    and diversity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，大多数工作集中在RLHF上，并提出了几种算法，如PPOSchulman等人，([2017](#bib.bib11))，DPO(Rafailov等人，([2023](#bib.bib10)))，IPO(Azar等人，([2023](#bib.bib2)))，KTO(Ethayarajh等人，([2024](#bib.bib4)))，MinorDPO(Xie等人，([2024](#bib.bib12)))等。RLHF的一个重要点是限制优化模型在训练过程中不偏离原始模型太多，因此PPO使用KL约束，DPO使用与偏好对之间距离相关的样本级动态系数，IPO使用偏好对之间的目标距离等。这个约束的目的是避免在领域语料上过度拟合，并保持LLM的通用性。一个重要的假设是基础模型足够强大，训练不应改变语言分布太多，以保持通用性和多样性。
- en: While back to SFT, most works are focused on collect, filter and mix high quality
    data. High quality data is undoubtedly important to get a high qualified and usable
    LLM, while the aforementioned hypothesis that optimized model should not deviate
    far from the original model is still important.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回到SFT，大多数工作集中在收集、过滤和混合高质量数据。高质量数据无疑对获得高质量且可用的LLM至关重要，而优化模型不应偏离原始模型过多的假设仍然重要。
- en: Our main contribution is that we introduce a training metrics used in DPO and
    MinorDPO into SFT phase, and propose an improved loss function MinorSFT. MinorSFT
    use a sample level coefficient to control the learning strength. It constraints
    the discrepancy more compared to raw SFT and may provide better performance result,
    at the cost of an additional hyper parameter and more computation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是将DPO和MinorDPO中使用的训练指标引入SFT阶段，并提出了改进的损失函数MinorSFT。MinorSFT使用样本级别系数来控制学习强度。与原始SFT相比，它对差异的约束更多，可能提供更好的性能结果，但需要额外的超参数和更多的计算。
- en: 2 Related Work
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Reinforce Learning from human feedback(Ouyang et al., ([2022](#bib.bib6)), Ziegler
    et al., ([2020](#bib.bib14))) is a popular technique to align LLM to human preference.
    It uses SFT to train a supervised LLM on data of sampled prompt and labeled answer,
    then trains a reward model on preference pairs from human feedback and finally
    uses RL algorithm like PPO Schulman et al., ([2017](#bib.bib11)) to train an optimized
    LLM. The RL part contains a KL-divergence constraint to prevent the optimized
    LLM deviating too much from the base model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中强化学习（Ouyang 等，[2022](#bib.bib6)，Ziegler 等，[2020](#bib.bib14)）是一种流行的技术，用于使大型语言模型（LLM）符合人类偏好。它利用SFT在采样提示和标记答案的数据上训练一个监督的LLM，然后在来自人类反馈的偏好对上训练一个奖励模型，最后使用如PPO
    Schulman 等，[2017](#bib.bib11)的RL算法来训练一个优化的LLM。RL部分包含KL散度约束，以防止优化的LLM偏离基础模型太多。
- en: DPO(Rafailov et al., ([2023](#bib.bib10))) is a simplified RL algorithm that
    optimize LLM directly on the preference data using a cross-entropy classification
    loss. DPO objective is to increase the relative log probability of preferred answer
    to dis-preferred answer. It incorporates a dynamic, sample level importance weight
    scaled by hyper-parameter $\beta$ account for the strength of the KL-divergence
    constraint. DPO introduces an important concept that LLM model itself is an implicit
    reward model, which means the LLM model can somehow measure the corpora during
    training phase. Rafailov et al., ([2024](#bib.bib9)) derive that DPO is token-level
    MDP and works as a general inverse Q-learning algorithm in a theoretical way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DPO（Rafailov 等，[2023](#bib.bib10)）是一个简化的RL算法，直接在偏好数据上优化LLM，使用交叉熵分类损失。DPO的目标是增加偏好答案相对于不偏好答案的相对对数概率。它包含一个动态的、按超参数$\beta$缩放的样本级别重要性权重，以考虑KL散度约束的强度。DPO引入了一个重要概念，即LLM模型本身是一个隐式奖励模型，这意味着LLM模型可以在训练阶段某种程度上衡量语料库。Rafailov
    等，[2024](#bib.bib9)推导出DPO是令牌级别的MDP，并以理论方式作为通用的逆Q学习算法。
- en: MinorDPO(Xie et al., ([2024](#bib.bib12))) is a DPO variant. It justifies hyper-parameter
    $\beta$ in DPO is a constraint relate to the relative log probability margin of
    the preference pair, instead of the KL-divergence constraint. It introduces MinorDPO
    loss to reduce penalty on the reject(dis-preferred) answer to prevent over penalty
    on the reject answer, which implicitly keep to the hypothesis that optimized model
    should not deviate too much from the base model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MinorDPO（Xie 等，[2024](#bib.bib12)）是DPO的一个变体。它证明DPO中的超参数$\beta$是与偏好对的相对对数概率边际相关的约束，而不是KL散度约束。它引入了MinorDPO损失，以减少对拒绝（不偏好）答案的惩罚，防止对拒绝答案的过度惩罚，这隐含地保持了优化模型不应过多偏离基础模型的假设。
- en: IPO(Azar et al., ([2023](#bib.bib2))) proves DPO may be prone to over-fitting
    when preferred probability over dis-preferred probability that is close to 1\.
    In IPO objective it uses a target value relate to the hyper-parameter $\beta$
    for the relative log probability of preferred to dis-preferred. However, it is
    somehow same as DPO, that it focuses on the relative log probability margin, so
    it has same problem as DPO mentioned in MinorDPO.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: IPO（Azar 等，[2023](#bib.bib2)）证明当偏好概率与不偏好概率的比率接近1时，DPO可能容易过拟合。在IPO目标中，它使用与超参数$\beta$相关的目标值，用于偏好与不偏好之间的相对对数概率。然而，它在某种程度上与DPO相同，因为它关注的是相对对数概率边际，因此与MinorDPO中提到的DPO具有相同的问题。
- en: KTO(Ethayarajh et al., ([2024](#bib.bib4))) proposes human aware loss function.
    It separates the preference pair loss into two losses so that it doesn’t purely
    rely on paired preference data. Inside each separated loss, it estimates the KL
    term by matching input x’ with unrelated outputs z in the same batch, but without
    back-propagate through the KL term, and thus it also introduces an implicit constraint
    on the gradient which in turn affect the learning strength and deviation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: KTO（Ethayarajh 等，([2024](#bib.bib4))) 提出了人类感知损失函数。它将偏好对损失分成两个损失，以便不完全依赖配对偏好数据。在每个分离的损失中，它通过将输入
    x' 与同一批次中的无关输出 z 匹配来估计 KL 项，但不通过 KL 项进行反向传播，因此也引入了对梯度的隐式约束，从而影响学习强度和偏差。
- en: Llama 3 (Dubey et al., ([2024](#bib.bib3))) presents a detailed way to collect,
    filter and mix high quality data for SFT and RL. For the RL part it uses DPO with
    an additional negative log-likelihood loss, similar to Pang et al., ([2024](#bib.bib8))
    and mentioned in Pal et al., ([2024](#bib.bib7)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3（Dubey 等，([2024](#bib.bib3))) 提出了详细的方式来收集、筛选和混合高质量数据用于 SFT 和 RL。在 RL
    部分，它使用 DPO 和额外的负对数似然损失，类似于 Pang 等，([2024](#bib.bib8)) 和 Pal 等，([2024](#bib.bib7))
    中提到的。
- en: Many efforts focus on RL part and use explicit or implicit constraints to limit
    optimized LLM deviation to reduce model regression. Inspired by DPO and MinorDPO,
    we think it worth a try to take the hypothesis into SFT to reduce LLM deviation
    and maintain diversity, and maybe able to increase performance further.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 许多努力集中在 RL 部分，使用显式或隐式约束来限制优化的 LLM 偏差，以减少模型回归。受到 DPO 和 MinorDPO 的启发，我们认为尝试将假设引入
    SFT 以减少 LLM 偏差并保持多样性是值得的，或许还能进一步提升性能。
- en: 3 Approach
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Minor SFT derivation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 小型 SFT 推导
- en: DPO(Rafailov et al., ([2023](#bib.bib10))) derives its objective from RL in
    a closed form.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DPO（Rafailov 等，([2023](#bib.bib10))) 从闭式的 RL 中推导出其目标。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: It introduces $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$.
    DPO objective is to maximize rewards margin between the preference pair.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它引入了 $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$。DPO
    目标是最大化偏好对之间的奖励差距。
- en: The MinorDPO objective adds an additional constraints to dis-preferred samples,
    by replacing the original penalty $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: MinorDPO 目标为不受偏好样本增加了额外的约束，通过替换原始惩罚 $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}$。
- en: '|  | $1$2 |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: So when probability of optimized model on dis-preferred sample is less than
    probability of reference model on dis-preferred sample, which means $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}<=0$,
    it will ignore the dis-preferred , and focus on handling the preferred sample.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当优化模型在不受偏好样本上的概率低于参考模型在不受偏好样本上的概率时，即 $log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)}<=0$，它将忽略不受偏好样本，专注于处理偏好样本。
- en: 'Under this situation, the formula can be rewritten as below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，公式可以重写如下：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: We simply name this method SFT using DPO. Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation") tries to maximize reward on the preferred sample.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单地将这种方法称为使用 DPO 的 SFT。Eq. [3](#S3.E3 "在 3.1 小型 SFT 推导 ‣ 3 方法 ‣ Minor SFT
    损失以提升 LLM 性能和减少模型偏差") 尝试最大化对偏好样本的奖励。
- en: Let’s derive the gradient equation of Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation")
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导 Eq. [3](#S3.E3 "在 3.1 小型 SFT 推导 ‣ 3 方法 ‣ Minor SFT 损失以提升 LLM 性能和减少模型偏差")
    的梯度方程。
- en: '|  | $1$2 |  | (4) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: Compared to raw SFT loss gradient equation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始 SFT 损失梯度方程相比。
- en: '|  | $1$2 |  | (5) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: $m$ is length of the answer. Normally, SFT use average over the answer, while
    DPO use sum over the answer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: $m$ 是答案的长度。通常，SFT 使用答案的平均值，而 DPO 使用答案的总和。
- en: Comparing Eq. [4](#S3.E4 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT
    loss for LLM fine-tune to increase performance and reduce model deviation") and
    Eq. [5](#S3.E5 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss for
    LLM fine-tune to increase performance and reduce model deviation"), we see Eq.
    [4](#S3.E4 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss for LLM
    fine-tune to increase performance and reduce model deviation") $\nabla_{\theta}L_{preferred}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 Eq. [4](#S3.E4 "在 3.1 Minor SFT 推导 ‣ 3 方法 ‣ Minor SFT 对 LLM 微调以提高性能和减少模型偏差")
    和 Eq. [5](#S3.E5 "在 3.1 Minor SFT 推导 ‣ 3 方法 ‣ Minor SFT 对 LLM 微调以提高性能和减少模型偏差")，我们可以看到
    Eq. [4](#S3.E4 "在 3.1 Minor SFT 推导 ‣ 3 方法 ‣ Minor SFT 对 LLM 微调以提高性能和减少模型偏差") $\nabla_{\theta}L_{preferred}$。
- en: While Eq. [5](#S3.E5 "In 3.1 Minor SFT derivation ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation") $\nabla_{\theta}L_{raw\_sft}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 同时 Eq. [5](#S3.E5 "在 3.1 Minor SFT 推导 ‣ 3 方法 ‣ Minor SFT 对 LLM 微调以提高性能和减少模型偏差")
    $\nabla_{\theta}L_{raw\_sft}$。
- en: Here we introduce the sample level dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    into raw sft loss, and we get
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将样本级动态系数 $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$ 引入原始
    sft 损失中，并得到
- en: '|  | $1$2 |  | (6) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Since at the start of the training, $\pi_{\theta}$, so we multiply 2 to make
    it closer to the raw sft and get final MinorSFT gradient.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在训练开始时，$\pi_{\theta}$，因此我们乘以 2 以使其更接近原始 sft 并得到最终 MinorSFT 梯度。
- en: '|  | $1$2 |  | (7) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 3.2 LLM Deviation metric
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM 偏差度量
- en: Back to the reward aforementioned $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$,
    DPO objective is to maximize the rewards margin between the preference pairs.
    And we can also treat the reward as a metric that measure
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回到上述奖励 $\hat{r}_{\theta}(x,y)=\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$，DPO
    的目标是最大化偏好对之间的奖励差距。我们也可以将奖励视为一个度量来衡量
- en: '1.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: complexity of the sample. As the reward is $\beta(log\pi_{\theta}(y|x)-log\pi_{ref}(y|x))$
    gives high log probability, which indicate the sample is low complexity.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 样本的复杂性。由于奖励是 $\beta(log\pi_{\theta}(y|x)-log\pi_{ref}(y|x))$ 给出高对数概率，这表明样本复杂度低。
- en: '2.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: deviation of the model. If we treat the corpora as identical distribution, then
    high rewards mean high relative log probability difference between the optimized
    LLM $\pi_{\theta}$, which indicate a high deviation.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的偏差。如果我们将语料库视为同一分布，则高奖励意味着优化后的 LLM $\pi_{\theta}$ 与参考模型之间的对数概率差异较大，这表明偏差较高。
- en: So the sample dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    in Minor SFT has a clear physic meaning, lower complexity samples have a smaller
    coefficient than higher complexity samples. In this way it dynamically adjusts
    the training data distribution and the whole training process will pay more attention
    on higher complexity samples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此样本动态系数 $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$ 在 Minor
    SFT 中具有明确的物理意义，低复杂度样本的系数小于高复杂度样本。这样可以动态调整训练数据分布，整个训练过程将更多地关注高复杂度样本。
- en: 'Besides, this metric measures how far the optimized model deviate from the
    original model during training. But it has two limitations:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个度量衡量了优化模型在训练过程中与原始模型的偏差程度。但它有两个局限性：
- en: '1.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The reward is related to the hyper-parameter $\beta$ is not able to do comparison.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励与超参数 $\beta$ 相关，因此无法进行比较。
- en: '2.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: This reward is related to answer length. so training of distribution with different
    answer length is not able to do comparison.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个奖励与回答长度有关。因此，不同回答长度的分布训练无法进行比较。
- en: We need a normalized metric that can be compared not only with different $\beta$
    and answer length, and get
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个规范化的度量，不仅可以与不同的 $\beta$ 和回答长度进行比较，并且得到
- en: '|  | $m_{\theta}(x,y)=\frac{1}{N}\Sigma\frac{1}{m}log\frac{\pi_{\theta}(y&#124;x)}{\pi_{ref}(y&#124;x)}$
    |  | (8) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{\theta}(x,y)=\frac{1}{N}\Sigma\frac{1}{m}log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)}$
    |  | (8) |'
- en: N is batch size, and m is answer length. Thus $m_{\theta}(x,y)$, it can compare
    with MinorSFT and SFT using DPO( Eq. [3](#S3.E3 "In 3.1 Minor SFT derivation ‣
    3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation"))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: N 是批量大小，m 是回答长度。因此 $m_{\theta}(x,y)$ 可以与 MinorSFT 和 SFT 使用 DPO 进行比较（参见 Eq. [3](#S3.E3
    "在 3.1 Minor SFT 推导 ‣ 3 方法 ‣ Minor SFT 对 LLM 微调以提高性能和减少模型偏差")）
- en: '![Refer to caption](img/31e58df55b70eba2446a08281327a69d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31e58df55b70eba2446a08281327a69d.png)'
- en: (a) raw sft lr = 1e-5
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 原始 sft lr = 1e-5
- en: '![Refer to caption](img/9d38180716ba2fa4f11eec9170f9a892.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d38180716ba2fa4f11eec9170f9a892.png)'
- en: (b) SFT use DPO lr = 2e-5 $\beta$ = 0.04
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SFT 使用 DPO lr = 2e-5 $\beta$ = 0.04
- en: '![Refer to caption](img/7e4df3682d023af24a71b6bb678cd5b2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e4df3682d023af24a71b6bb678cd5b2.png)'
- en: (c) Minor SFT lr = 2e-5 $\beta$ = 0.04
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Minor SFT lr = 2e-5 $\beta$ = 0.04
- en: 'Figure 1: Normalized rewards during training'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：训练过程中的标准化奖励
- en: The metric $m_{\theta}(x,y)$ can be used in both DPO with preference pair and
    SFT with only preferred. Figure [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric
    ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce
    model deviation") shows metric trends for three methods. Since the optimized LLM
    deviate from the reference model during to the training, they can also be treated
    as LLM model deviation trend. The metric value and trends can be used in a qualitative
    analysis of LLM deviation. And from Figure [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation
    metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance
    and reduce model deviation") even with larger learning rate(2e-5) for SFT use
    DPO [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation") and MinorSFT
    [1](#S3.F1 "Figure 1 ‣ 3.2 LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss
    for LLM fine-tune to increase performance and reduce model deviation"), they have
    a lower deviation value compared to SFT (1e-5) [1](#S3.F1 "Figure 1 ‣ 3.2 LLM
    Deviation metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase performance
    and reduce model deviation") for each training step, due to they both have a sample
    level dynamic coefficient that decays fast when the reward of the sample grow
    up( or in other words, when the complexity of the sample reduce down).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 $m_{\theta}(x,y)$ 可用于 DPO 的偏好对和仅偏好的 SFT。图 [1](#S3.F1 "图 1 ‣ 3.2 LLM 偏差指标
    ‣ 3 方法 ‣ Minor SFT 损失用于 LLM 微调以提高性能并减少模型偏差") 显示了三种方法的指标趋势。由于优化后的 LLM 在训练过程中偏离了参考模型，因此它们也可以视为
    LLM 模型偏差趋势。指标值和趋势可用于 LLM 偏差的定性分析。从图 [1](#S3.F1 "图 1 ‣ 3.2 LLM 偏差指标 ‣ 3 方法 ‣ Minor
    SFT 损失用于 LLM 微调以提高性能并减少模型偏差") 可以看出，即使 SFT 使用 DPO [1](#S3.F1 "图 1 ‣ 3.2 LLM 偏差指标
    ‣ 3 方法 ‣ Minor SFT 损失用于 LLM 微调以提高性能并减少模型偏差") 和 MinorSFT [1](#S3.F1 "图 1 ‣ 3.2
    LLM 偏差指标 ‣ 3 方法 ‣ Minor SFT 损失用于 LLM 微调以提高性能并减少模型偏差") 的学习率较大（2e-5），它们相较于 SFT (1e-5)
    [1](#S3.F1 "图 1 ‣ 3.2 LLM 偏差指标 ‣ 3 方法 ‣ Minor SFT 损失用于 LLM 微调以提高性能并减少模型偏差") 每个训练步骤的偏差值更低，因为它们都具有一个样本级别的动态系数，该系数在样本奖励增加时（或换句话说，当样本复杂度降低时）衰减得很快。
- en: 4 Experiments
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: For training settings, we use Qwen2-7B-Instruction(qwe, ([2024](#bib.bib1)))
    as the base model. It expresses high performance in many benchmarks ¹¹1We tried
    several open datasets to train the base model, but with little performance improvement
    on the benchmarks, so in this experiment we use a private domain corpus. And use
    down-sample of FinanceIQ²²2https://huggingface.co/datasets/Duxiaoman-DI/FinanceIQ,
    fineval³³3https://huggingface.co/datasets/djdropthebit/fineval, ceval-exam(Huang
    et al., ([2023](#bib.bib5))) as test datasets to do evaluation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练设置，我们使用 Qwen2-7B-Instruction（qwe，([2024](#bib.bib1)）作为基础模型。它在许多基准测试中表现出色¹¹1我们尝试了几个公开数据集来训练基础模型，但在基准测试中几乎没有性能提升，因此在本实验中我们使用了私有领域语料库。并使用了
    FinanceIQ²²2https://huggingface.co/datasets/Duxiaoman-DI/FinanceIQ、fineval³³3https://huggingface.co/datasets/djdropthebit/fineval、ceval-exam（黄等，([2023](#bib.bib5)）作为测试数据集进行评估。
- en: We use LLaMa-Factory(Zheng et al., ([2024](#bib.bib13))) as the training and
    inference framework with some customized code to implement the MinorSFT and SFT
    use DPO algorithm. The experiments use batch size 64, warm-up ratio 0.1, linear
    decay learning rate, 1 epoch and run 400+ steps.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 LLaMa-Factory（郑等，([2024](#bib.bib13)）作为训练和推理框架，并结合一些自定义代码来实现 MinorSFT 和
    SFT 使用 DPO 算法。实验使用批量大小为 64，预热比例为 0.1，线性衰减学习率，1 个 epoch，并运行 400+ 步。
- en: For FinanceIQ and fineval we use the prompt """Please answer the questions based
    on the context provided. Please ensure that the original information (such as
    numbers, time, entities, opinions, etc.) is accurately cited when answering. If
    the user’s question cannot be answered based on the given context, please briefly
    explain why. If the answer involves mathematical calculations, please give priority
    to calling tools; if it involves numerical comparison, please give the comparison
    process; if it involves analysis or reasoning, please give the reasoning and analysis
    process""".
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 FinanceIQ 和 fineval，我们使用了提示 """请根据提供的上下文回答问题。请确保在回答时准确引用原始信息（如数字、时间、实体、观点等）。如果用户的问题无法根据给定的上下文回答，请简要解释原因。如果答案涉及数学计算，请优先调用工具；如果涉及数字比较，请给出比较过程；如果涉及分析或推理，请给出推理和分析过程"""。
- en: 'For ceval-exam we use the prompt """You need to choose one of the four options
    A, B, C, and D as the most appropriate answer to the question. You can only output
    one character, and this character must be one of A, B, C, and D. The question
    content is:  The four options are: A.  B.  C.  D.  Your
    answer is:""".'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ceval-exam，我们使用了提示 """你需要选择 A、B、C 和 D 四个选项中的一个作为问题的最合适答案。你只能输出一个字符，并且这个字符必须是
    A、B、C 或 D。问题内容是： 四个选项是：A.  B.  C.  D.  你的答案是："""。
- en: '![Refer to caption](img/8d2a2628b57efc3dd02aada074399eb3.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8d2a2628b57efc3dd02aada074399eb3.png)'
- en: (a) Full comparison data
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 完整比较数据
- en: '![Refer to caption](img/70f2aa6d341c6118340ac1389f715b52.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/70f2aa6d341c6118340ac1389f715b52.png)'
- en: (b) Best model comparison
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 最佳模型比较
- en: 'Figure 2: Accuracy comparison'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 准确性比较'
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") shows the experiment result.
    We searched a group setting for each method. Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation")
    contains full detail of the comparison. Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation")
    shows the comparison between the best result of each method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune to
    increase performance and reduce model deviation") 显示了实验结果。我们为每种方法搜索了一组设置。图 [2](#S4.F2
    "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune to increase performance
    and reduce model deviation") 包含了比较的详细信息。图 [2](#S4.F2 "Figure 2 ‣ 4 Experiments
    ‣ Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation")
    显示了每种方法的最佳结果之间的比较。
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") shows after learning, Minor
    SFT get its best result with lr=2e-5 and $\beta$=0.04.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune to
    increase performance and reduce model deviation") 显示经过学习后，Minor SFT 在 lr=2e-5
    和 $\beta$=0.04 时取得了最佳结果。
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune
    to increase performance and reduce model deviation") indicates that Minor SFT(lr=2e-5,
    $\beta$=0.04) are all better than the base model. Minor SFT perform best in all
    three datasets compared to raw SFT and SFT use DPO. SFT use DPO wins FinanceIQ
    and ceval-exam but lose fineval compared to raw SFT.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "Figure 2 ‣ 4 Experiments ‣ Minor SFT loss for LLM fine-tune to
    increase performance and reduce model deviation") 表明 Minor SFT(lr=2e-5, $\beta$=0.04)
    的表现均优于基础模型。与原始 SFT 和使用 DPO 的 SFT 相比，Minor SFT 在所有三个数据集上表现最佳。SFT 使用 DPO 在 FinanceIQ
    和 ceval-exam 上表现优于原始 SFT，但在 fineval 上表现较差。
- en: The experiment result shows several points.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果显示了几个要点。
- en: '1.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Each method have a performance increase from low learning rate to high learning
    rate, and get a performance decrease if continue to increase the learning rate
    after a certain threshold. Raw SFT get its best at lr=1e-5, MinorSFT and SFT use
    DPO get its best at lr=2e-5 and $\beta$=0.04.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每种方法的表现从低学习率到高学习率都有所提升，但如果在某个阈值后继续增加学习率，则性能会下降。原始 SFT 在 lr=1e-5 时表现最佳，MinorSFT
    和使用 DPO 的 SFT 在 lr=2e-5 和 $\beta$=0.04 时表现最佳。
- en: '2.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Minor SFT perform best in all three datasets. We give credit to the sample-level
    dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$.
    This coefficient implicitly adjust the corpus distribution, so that the training
    pays more effort on those high complexity(or difficult) samples.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Minor SFT 在所有三个数据集上表现最佳。我们归功于样本级动态系数 $\sigma(-\beta \log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$。该系数隐含地调整了语料库分布，使得训练更加关注那些高复杂性（或困难）的样本。
- en: '3.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Minor SFT need higher learning rate to get its best performance compared to
    raw SFT, because the sample dynamic coefficient $\sigma(-\beta log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    decay during training when the reward grows up(or when the complexity of the sample
    reduce down). However, even with high learning rate Minor SFT has a lower deviation
    compared to raw SFT, which can be see through Figure [1](#S3.F1 "Figure 1 ‣ 3.2
    LLM Deviation metric ‣ 3 Approach ‣ Minor SFT loss for LLM fine-tune to increase
    performance and reduce model deviation").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与原始 SFT 相比，Minor SFT 需要更高的学习率以达到最佳性能，因为样本动态系数 $\sigma(-\beta \log\frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})$
    在奖励增长（或样本复杂性降低）时会衰减。然而，即使使用较高的学习率，Minor SFT 相比原始 SFT 的偏差更低，这可以通过图 [1](#S3.F1 "图
    1 ‣ 3.2 LLM 偏差指标 ‣ 3 方法 ‣ Minor SFT 损失用于 LLM 微调以提高性能和减少模型偏差") 看到。
- en: '4.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: SFT use DPO perform worse than Minor SFT, we think the cause is due to it use
    the same hyper-parameter $\beta$ is sample dependent while $\beta$ is sample independent,
    this bias cause the performance regression.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SFT 使用 DPO 的表现不如 Minor SFT，我们认为原因是因为它使用相同的超参数 $\beta$ 是样本依赖的，而 $\beta$ 是样本独立的，这种偏差导致了性能回退。
- en: '5.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: $\beta$ has same meaning as in DPO, however it still brings more complexity
    compared to raw SFT. It needs some tuning to achieve the best performance.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\beta$ 在 DPO 中具有相同的含义，但与原始 SFT 相比，它仍然带来了更多的复杂性。它需要一些调整以达到最佳性能。
- en: 5 Conclusion & Future work
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: Inspired from DPO and MinorDPO, in this article we propose a training metric
    $m_{\theta}(x,y)$. It’s kind of a tradeoff to get better performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 受 DPO 和 MinorDPO 启发，在本文中我们提出了一种训练指标 $m_{\theta}(x,y)$。这是一种权衡，以获得更好的性能。
- en: As the conclusion in above Experiment section, MinorSFT needs some higher learning
    rate compared to raw SFT. We design the MinorSFT coefficient same as the coefficient
    in DPO to simplify its meaning and understanding. The hyper-parameter $\beta$
    in MinorSFT has same meaning as in DPO. With appropriate tuning we are able to
    get a best performance LLM as in above experiment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如上述实验部分的结论所述，MinorSFT 需要比原始 SFT 更高的学习率。我们将 MinorSFT 系数设计为与 DPO 中的系数相同，以简化其含义和理解。MinorSFT
    中的超参数 $\beta$ 具有与 DPO 相同的含义。通过适当的调整，我们能够获得与上述实验中相同的最佳性能 LLM。
- en: Although the training metric $m_{\theta}(x,y)$ and answer length, we don’t have
    a way to know whether the optimized model is over-fit or under-fit during the
    training. It needs more research effort to find those metrics that can guide model’s
    fitting level.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有训练指标 $m_{\theta}(x,y)$ 和答案长度，但我们没有办法知道优化后的模型在训练过程中是否过拟合或欠拟合。这需要更多的研究工作来找到能够指导模型拟合水平的指标。
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: qwe, (2024) (2024). Qwen2 technical report.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: qwe，（2024）（2024）。Qwen2 技术报告。
- en: Azar et al., (2023) Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello,
    D., Valko, M., and Munos, R. (2023). A general theoretical paradigm to understand
    learning from human preferences.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azar 等，（2023）Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D.,
    Valko, M., 和 Munos, R.（2023）。理解从人类偏好中学习的一般理论范式。
- en: Dubey et al., (2024) Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,
    A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn,
    A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A.,
    Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B.,
    Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell,
    C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius,
    D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D.,
    Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova,
    E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G.,
    Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,
    H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov,
    I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah,
    J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang,
    J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun,
    J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,
    K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Rantala-Yeary,
    L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L.,
    Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M.,
    Singh, M., Paluri, M., Kardas, M., Oldham, M., Rita, M., Pavlova, M., Kambadur,
    M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov,
    N., Bogoychev, N., Chatterji, N., Duchenne, O., Çelebi, O., Alrassy, P., Zhang,
    P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura,
    P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R.,
    Cabral, R. S., Stojnic, R., Raileanu, R., Girdhar, R., Patel, R., Sauvestre, R.,
    Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini,
    S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang,
    S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S.,
    Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S.,
    Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T.,
    Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez,
    V., Gonguet, V., Do, V., Vogeti, V., Petrovic, V., Chu, W., Xiong, W., Fu, W.,
    Meers, W., Martinet, X., Wang, X., Tan, X. E., Xie, X., Jia, X., Wang, X., Goldschlag,
    Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert,
    Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Grattafiori, A., Jain, A.,
    Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A.,
    Sharma, A., Boesenberg, A., Vaughan, A., Baevski, A., Feinstein, A., Kallet, A.,
    Sangani, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton,
    A., Ryan, A., Ramchandani, A., Franco, A., Saraf, A., Chowdhury, A., Gabriel,
    A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi,
    B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B.,
    Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker,
    C., Burton, C., Mejia, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai,
    C., Tindal, C., Feichtenhofer, C., Civin, D., Beaty, D., Kreymer, D., Li, D.,
    Wyatt, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich,
    D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery,
    E., Presani, E., Hahn, E., Wood, E., Brinkman, E., Arcaute, E., Dunbar, E., Smothers,
    E., Sun, F., Kreuk, F., Tian, F., Ozgenel, F., Caggioni, F., Guzmán, F., Kanayet,
    F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G.,
    Thattai, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Shojanazeri,
    H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H.,
    Goldman, H., Molybog, I., Tufanov, I., Veliche, I.-E., Gat, I., Weissman, J.,
    Geboski, J., Kohli, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J.,
    Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings,
    J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J.,
    Wu, K., U, K. H., Saxena, K., Prasad, K., Khandelwal, K., Zand, K., Matosich,
    K., Veeraraghavan, K., Michelena, K., Li, K., Huang, K., Chawla, K., Lakhotia,
    K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo,
    L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M.,
    Tsimpoukelli, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov,
    M., Lathi, M., Keneally, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M.,
    Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat,
    M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N.,
    Singhal, N., Egebo, N., Usunier, N., Laptev, N. P., Dong, N., Zhang, N., Cheng,
    N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P.,
    Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina,
    P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R.,
    Murthy, R., Nayani, R., Mitra, R., Li, R., Hogan, R., Battey, R., Wang, R., Maheswari,
    R., Howes, R., Rinott, R., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon,
    S., Sidorov, S., Pan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S.,
    Lindsay, S., Feng, S., Lin, S., Zha, S. C., Shankar, S., Zhang, S., Zhang, S.,
    Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe,
    S., Satterfield, S., Govindaprasad, S., Gupta, S., Cho, S., Virk, S., Subramanian,
    S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Kohler, T., Robinson,
    T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi,
    V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V.,
    Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable,
    W., Tang, X., Wang, X., Wu, X., Wang, X., Xia, X., Wu, X., Gao, X., Chen, Y.,
    Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang,
    Hao, Y., Qian, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang,
    Z., and Zhao, Z. (2024). The llama 3 herd of models.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等（2024）Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman,
    A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang,
    A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A.,
    Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern,
    B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret,
    C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D.,
    Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano,
    D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E.,
    Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L.,
    Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H.,
    Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Copet,
    J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van
    der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu,
    J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J.,
    Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,
    K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Rantala-Yeary,
    L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L.,
    Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M.,
    Singh, M., Paluri, M., Kardas, M., Oldham, M., Rita, M., Pavlova, M., Kambadur,
    M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov,
    N., Bogoychev, N., Chatterji, N., Duchenne, O., Çelebi, O., Alrassy, P., Zhang,
    P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura,
    P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R.,
    Cabral, R. S., Stojnic, R., Raileanu, R., Girdhar, R., Patel, R., Sauvestre, R.,
    Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini,
    S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang,
    S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S.,
    Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S.,
    Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T.,
    Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez,
    V., Gonguet, V., Do, V., Vogeti, V., Petrovic, V., Chu, W., Xiong, W., Fu, W.,
    Meers, W., Martinet, X., Wang, X., Tan, X. E., Xie, X., Jia, X., Wang, X., Goldschlag,
    Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert,
    Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Grattafiori, A., Jain, A.,
    Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A.,
    Sharma, A., Boesenberg, A., Vaughan, A., Baevski, A., Feinstein, A., Kallet, A.,
    Sangani, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton,
    A., Ryan, A., Ramchandani, A., Franco, A., Saraf, A., Chowdhury, A., Gabriel,
    A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi,
    B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B.,
    Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker,
    C., Burton, C., Mejia, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai,
    C., Tindal, C., Feichtenhofer, C., Civin, D., Beaty, D., Kreymer, D., Li, D.,
    Wyatt, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich,
    D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery,
    E., Presani, E., Hahn, E., Wood, E., Brinkman, E., Arcaute, E., Dunbar, E., Smothers,
    E., Sun, F., Kreuk, F., Tian, F., Ozgenel, F., Caggioni, F., Guzmán, F., Kanayet,
    F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G.,
    Thattai, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Shojanazeri,
    H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H.,
    Goldman, H., Molybog, I., Tufanov, I., Veliche, I.-E., Gat, I., Weissman, J.,
    Geboski, J., Kohli, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J.,
    Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings,
    J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J.,
    Wu, K., U, K. H., Saxena, K., Prasad, K., Khandelwal, K., Zand, K., Matosich,
    K., Veeraraghavan, K., Michelena, K., Li, K., Huang, K., Chawla, K., Lakhotia,
    K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo,
    L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M.,
    Tsimpoukelli, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov,
    M., Lathi, M., Keneally, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M.,
    Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang,
- en: 'Ethayarajh et al., (2024) Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky,
    D., and Kiela, D. (2024). Kto: Model alignment as prospect theoretic optimization.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ethayarajh et al., (2024) Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky,
    D., 和 Kiela, D. (2024). Kto: 模型对齐作为前景理论优化。'
- en: 'Huang et al., (2023) Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su,
    T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J. (2023). C-eval:
    A multi-level multi-discipline chinese evaluation suite for foundation models.
    arXiv preprint arXiv:2305.08322.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al., (2023) Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su,
    T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., 和 He, J. (2023). C-eval:
    一套多级多学科的中文评估工具，用于基础模型。arXiv 预印本 arXiv:2305.08322。'
- en: Ouyang et al., (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P., Leike, J., and Lowe, R. (2022). Training language models to follow instructions
    with human feedback.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al., (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P., Leike, J., 和 Lowe, R. (2022). 训练语言模型以遵循带有人类反馈的指令。
- en: 'Pal et al., (2024) Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu,
    S., and White, C. (2024). Smaug: Fixing failure modes of preference optimisation
    with dpo-positive.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pal et al., (2024) Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu,
    S., 和 White, C. (2024). Smaug: 使用 dpo-positive 修复偏好优化的失败模式。'
- en: Pang et al., (2024) Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S.,
    and Weston, J. (2024). Iterative reasoning preference optimization.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang et al., (2024) Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S.,
    和 Weston, J. (2024). 迭代推理偏好优化。
- en: 'Rafailov et al., (2024) Rafailov, R., Hejna, J., Park, R., and Finn, C. (2024).
    From $r$: Your language model is secretly a q-function.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rafailov et al., (2024) Rafailov, R., Hejna, J., Park, R., 和 Finn, C. (2024).
    从 $r$: 你的语言模型实际上是一个 q-函数。'
- en: 'Rafailov et al., (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S.,
    Manning, C. D., and Finn, C. (2023). Direct preference optimization: Your language
    model is secretly a reward model.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al., (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,
    C. D., 和 Finn, C. (2023). 直接偏好优化：你的语言模型实际上是一个奖励模型。
- en: Schulman et al., (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. (2017). Proximal policy optimization algorithms.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al., (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    和 Klimov, O. (2017). 近端策略优化算法。
- en: Xie et al., (2024) Xie, S., Chen, H., Yu, F., Sun, Z., Wu, X., and Hu, Y. (2024).
    Minor dpo reject penalty to increase training robustness.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al., (2024) Xie, S., Chen, H., Yu, F., Sun, Z., Wu, X., 和 Hu, Y. (2024).
    次要 dpo 拒绝惩罚以提高训练鲁棒性。
- en: 'Zheng et al., (2024) Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., and
    Ma, Y. (2024). Llamafactory: Unified efficient fine-tuning of 100+ language models.
    arXiv preprint arXiv:2403.13372.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al., (2024) Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., 和 Ma,
    Y. (2024). Llamafactory: 统一高效地微调 100+ 语言模型。arXiv 预印本 arXiv:2403.13372。'
- en: Ziegler et al., (2020) Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
    A., Amodei, D., Christiano, P., and Irving, G. (2020). Fine-tuning language models
    from human preferences.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler et al., (2020) Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,
    A., Amodei, D., Christiano, P., 和 Irving, G. (2020). 从人类偏好中微调语言模型。
