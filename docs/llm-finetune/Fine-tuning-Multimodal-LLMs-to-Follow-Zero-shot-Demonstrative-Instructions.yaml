- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:39:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调多模态LLMs以遵循零-shot演示性指令
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.04152](https://ar5iv.labs.arxiv.org/html/2308.04152)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.04152](https://ar5iv.labs.arxiv.org/html/2308.04152)
- en: Juncheng Li^(1, 2)   Kaihang Pan¹¹¹1   Zhiqi Ge¹¹¹1   Minghe Gao¹¹¹1   Hanwang
    Zhang³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Juncheng Li^(1, 2)   Kaihang Pan¹¹¹1   Zhiqi Ge¹¹¹1   Minghe Gao¹¹¹1   Hanwang
    Zhang³
- en: Wei Ji²   Wenqiao Zhang¹  Tat-Seng Chua²   Siliang Tang$\textsuperscript{\rm
    1}^{\dagger}$ &
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Ji²   Wenqiao Zhang¹  Tat-Seng Chua²   Siliang Tang$\textsuperscript{\rm
    1}^{\dagger}$ &
- en: ¹Zhejiang University,  ²National University of Singapore,  ³Nanyang Technological
    University Equal Contribution. ^†Corresponding Authors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹浙江大学， ²新加坡国立大学， ³南洋理工大学 平等贡献。^†通讯作者。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing
    Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs
    can recognize. This is achieved by training the VPGs on millions of image-caption
    pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate
    the corresponding captions. However, this image-captioning based training objective
    inherently biases the VPG to concentrate solely on the primary visual contents
    sufficient for caption generation, often neglecting other visual details. This
    shortcoming results in MLLMs’ underperformance in comprehending demonstrative
    instructions consisting of multiple, interleaved, and multimodal instructions
    that demonstrate the required context to complete a task. To address this issue,
    we introduce a generic and lightweight Visual Prompt Generator Complete module
    (VPG-C), which can infer and complete the missing details essential for comprehending
    demonstrative instructions. Further, we propose a synthetic discriminative training
    strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative
    instructions. As for evaluation, we build DEMON, a comprehensive benchmark for
    demonstrative instruction understanding. Synthetically trained with the proposed
    strategy, VPG-C achieves significantly stronger zero-shot performance across all
    tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate
    the superiority of VPG-C. Our benchmark, code, and pre-trained models are available
    at [https://github.com/DCDmllm/Cheetah](https://github.com/DCDmllm/Cheetah).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在多模态大语言模型（MLLMs）方面的进展中，已经利用视觉提示生成器（VPGs）将视觉特征转换为大语言模型（LLMs）能够识别的标记。这是通过对数百万对图像-文本进行训练来实现的，其中VPG生成的图像标记被输入到一个冻结的LLM中以生成相应的文本。然而，这种基于图像-文本的训练目标固有地使VPG倾向于专注于生成文本所需的主要视觉内容，往往忽视了其他视觉细节。这一缺陷导致MLLMs在理解包含多个、交织且多模态指令的演示性指令时表现不佳，这些指令展示了完成任务所需的上下文。为了解决这个问题，我们引入了一个通用且轻量的视觉提示生成器完整模块（VPG-C），它可以推断并补全理解演示性指令所必需的缺失细节。此外，我们提出了一种合成判别训练策略来微调VPG-C，从而不再需要监督演示性指令。至于评估，我们构建了DEMON，一个全面的演示性指令理解基准。通过所提出的策略合成训练的VPG-C在DEMON的所有任务中显著提高了零-shot性能。进一步在MME和OwlEval基准上的评估也展示了VPG-C的优越性。我们的基准、代码和预训练模型可在[https://github.com/DCDmllm/Cheetah](https://github.com/DCDmllm/Cheetah)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advances in Multimodal Large Language Models (MLLMs) (Li et al., [2023b](#bib.bib32);
    Liu et al., [2023](#bib.bib36); Zhu et al., [2023a](#bib.bib63)) have exhibited
    promising capabilities in processing single-image instructions, such as producing
    detailed image descriptions and answering questions about the image. However,
    they fall short in demonstrative instructions consisting of multiple, interleaved,
    and multimodal instructions that demonstrate the required context to complete
    a task. For instance, the instruction in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")
    contains interleaved visual and textual context, requiring the model to determine
    the authenticity of the milk in the second image based on the official image provided
    in the first.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的多模态大型语言模型（MLLMs）（Li 等，[2023b](#bib.bib32)；Liu 等，[2023](#bib.bib36)；Zhu 等，[2023a](#bib.bib63)）在处理单图像指令方面展现了有希望的能力，例如生成详细的图像描述和回答关于图像的问题。然而，它们在处理包含多个交织的多模态指令的演示性指令时存在不足，这些指令展示了完成任务所需的上下文。例如，图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 中的指令包含了交织的视觉和文本上下文，需要模型根据第一张提供的官方图像确定第二张图像中的牛奶的真实性。
- en: 'An MLLM should at least have the following two capabilities to comprehend demonstrative
    instructions effectively:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个MLLM应至少具备以下两项能力，以有效理解演示性指令：
- en: '1) Not just the primary subject: Beyond focusing on the primary visual content,
    it should be able to meticulously discern the details within the demonstrations.
    These details, complementing the primary content, play a crucial role in semantically
    connecting the instructions. A case in point is Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions"), wherein accurate discernment relies on recognizing the logo detail
    on a milk carton.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 不仅仅关注主要主题：除了关注主要视觉内容外，还应能够细致辨别演示中的细节。这些细节在补充主要内容的同时，对语义连接指令起着至关重要的作用。例如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 中，准确的辨别依赖于识别牛奶盒上的标志细节。
- en: '2) Reasoning-aware details: How to decide what details are complementary to
    the reasoning? We expect that an MLLM may “think twice”, that is, given a preliminary
    reasoning using the primary contents, it would know what additional contents are
    needed as complementary details. For example, in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions"), after preliminary reasoning, the model should re-attend details
    such as the logo and brand name on the milk carton, thereby discerning its authenticity.
    However, to follow zero-shot demonstrative instructions, this “reasoning-aware”
    capability should be acquired without the need for supervised demonstrative instructions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 具备推理意识的细节：如何决定哪些细节对推理是补充性的？我们期望MLLM能“反复思考”，即在利用主要内容进行初步推理后，能够知道需要哪些额外的内容作为补充细节。例如，在图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 中，经过初步推理后，模型应重新关注牛奶盒上的标志和品牌名称等细节，从而识别其真实性。然而，要遵循零样本演示指令，这种“推理意识”能力应在没有监督演示指令的情况下获得。
- en: Unfortunately, we find that the reason why existing MLLMs are not effective
    in demonstrative instructions is due to the lack of the above capabilities. More
    specifically, the crux lies in the Visual Prompt Generator (VPG) in MLLMs. VPG,
    such as Q-former (Li et al., [2023b](#bib.bib32)) and Resampler (Alayrac et al.,
    [2022](#bib.bib1)), translates visual features into tokens recognizable by LLMs,
    and the translation is trained on millions of image-caption pairs by feeding the
    VPG-generated tokens of images into a frozen LLM which generates the corresponding
    captions. However, this image captioning training strategy inevitably introduces
    the inductive bias that VPG only focuses on the primary visual contents which
    are just enough for the captioning task, but tends to omit other visual details.
    For example in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions"), the averaged attention
    map of InstructBLIP (Dai et al., [2023](#bib.bib12)) (Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions")) shows a dominant focus on the primary contents, neglecting the
    logo detail, which is however the key to answering the question.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们发现现有的MLLM在示范指令中效果不佳的原因在于缺乏上述能力。更具体地说，关键在于MLLM中的视觉提示生成器（VPG）。VPG，如Q-former（Li等，[2023b](#bib.bib32)）和Resampler（Alayrac等，[2022](#bib.bib1)），将视觉特征翻译成LLM可以识别的标记，这一翻译通过将VPG生成的图像标记输入到一个固定的LLM中来生成相应的描述进行训练。然而，这种图像描述训练策略不可避免地引入了归纳偏差，使得VPG只关注用于描述任务的主要视觉内容，而倾向于忽略其他视觉细节。例如在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions")中，InstructBLIP（Dai等，[2023](#bib.bib12)）（图[1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions")）的平均注意力图显示对主要内容的主导关注，忽视了标志细节，而标志细节却是回答问题的关键。
- en: '![Refer to caption](img/4c45aad55cf9d7f068ad2deafe5a85d7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4c45aad55cf9d7f068ad2deafe5a85d7.png)'
- en: 'Figure 1: An example of InstructBLIP (Dai et al., [2023](#bib.bib12)) and our
    MLLM enhanced by VPG-C.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：InstructBLIP（Dai等，[2023](#bib.bib12)）和我们通过VPG-C增强的MLLM示例。
- en: '![Refer to caption](img/054c854a448c17237b1aa16e3c908f68.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/054c854a448c17237b1aa16e3c908f68.png)'
- en: 'Figure 2: An overview of VPG-C.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：VPG-C概览。
- en: To this end, we propose a lightweight Visual Prompt Generator Complete module (VPG-C),
    which can infer and complete the missing details essential for comprehending demonstrative
    instructions (Section [2.1](#S2.SS1 "2.1 Visual Prompt Generator Complete ‣ 2
    Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")).
    As shown Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions"), 1) VPG-C first derives
    the instruction-specific guidance by intercepting the intermediate LLM’s output
    of the primary contents extracted by a conventional VPG, and then 2) guides the
    VPG to recover the missing visual residual details. Finally, 3) these residual
    details are then seamlessly reintegrated into the intermediate LLM’s layer via
    a skip connection. Together with the original intermediate output, VPG-C is expected
    to provide an improved comprehension of the demonstration instructions. Yet, VPG-C
    is not ready to follow zero-shot demonstrative instructions because the “Guide”
    step requires fine-tuning to specialize in missing detail recovery. Therefore,
    we propose a synthetic discriminative training strategy to fine-tune VPG-C, without
    the need for the expensive data collection of “detail-caption” pairs (Section [2.2](#S2.SS2
    "2.2 Synthetic Discriminative Training Strategy ‣ 2 Method ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions")).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们提出了一个轻量级视觉提示生成器完整模块（VPG-C），它可以推断并补全理解示范指令所需的缺失细节（第[2.1节](#S2.SS1 "2.1 Visual
    Prompt Generator Complete ‣ 2 Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot
    Demonstrative Instructions")）。如图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")所示，1）VPG-C首先通过截取传统VPG提取的主要内容的中间LLM输出，推导出特定指令的指导信息，然后2）引导VPG恢复缺失的视觉残留细节。最后，3）这些残留细节通过跳跃连接无缝地重新集成到中间LLM的层中。与原始中间输出一起，VPG-C预计能提供对示范指令的更好理解。然而，VPG-C尚未准备好跟随零-shot示范指令，因为“指导”步骤需要微调以专注于缺失细节的恢复。因此，我们提出了一种合成判别训练策略来微调VPG-C，无需昂贵的数据收集“细节-描述”对（第[2.2节](#S2.SS2
    "2.2 Synthetic Discriminative Training Strategy ‣ 2 Method ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions")）。
- en: To evaluate VPG-C and diagnose existing MLLMs, we build DEMON, a comprehensive
    benchmark for demonstrative instruction understanding, covering 31 diverse tasks
    across 7 categories, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.3 Implementation
    Details ‣ 2 Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") (Section [3](#S3 "3 DEMON Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions")). Systematic evaluation on DEMON
    confirms the limitation of existing MLLMs in demonstrative instructions. Without
    additional demonstrative instruction data, the lightweight VPG-C module can be
    effectively tuned by the synthetic training strategy in several hours with a single
    A100 GPU. While computation- and data- efficient, VPG-C significantly outperforms
    existing MLLMs on the DEMON benchmark. Zero-shot evaluation on other multimodal
    instruction benchmarks (Fu et al., [2023](#bib.bib15); Ye et al., [2023](#bib.bib60))
    also indicates considerable improvement by VPG-C.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估VPG-C并诊断现有的MLLMs，我们构建了DEMON，一个用于示范性指令理解的综合基准，涵盖了7个类别中的31个不同任务，如图 [4](#S2.F4
    "图 4 ‣ 2.3 实施细节 ‣ 2 方法 ‣ 微调多模态LLMs以遵循零样本示范性指令") （第 [3](#S3 "3 DEMON基准 ‣ 微调多模态LLMs以遵循零样本示范性指令") 节）。对DEMON的系统评估确认了现有MLLMs在示范性指令中的局限性。在没有额外的示范性指令数据的情况下，轻量级的VPG-C模块可以通过合成训练策略在几小时内使用单个A100
    GPU进行有效调优。虽然计算和数据效率高，但VPG-C在DEMON基准上显著优于现有的MLLMs。在其他多模态指令基准上的零样本评估（Fu等， [2023](#bib.bib15)；Ye等， [2023](#bib.bib60)）也表明VPG-C带来了显著的改进。
- en: 2 Method
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 Visual Prompt Generator Complete
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 视觉提示生成器完成
- en: As illustrated in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"), VPG-C is built
    upon the frozen LLM (Vicuna-7B (Chiang et al., [2023](#bib.bib10))) and vision
    encoder (EVA-CLIP (Fang et al., [2023](#bib.bib13))). We adopt the widely used
    Q-Former from BLIP-2 (Li et al., [2023b](#bib.bib32)) as our visual prompt generator.
    VPG-C first uses the intermediate output of the LLM to infer instruction-specific
    guidance. This then assists the VPG in attending to the missing visual details
    from the images. By merging these residual details back via a skip connection,
    VPG-C achieves a thorough grasp of the demonstrative instruction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 微调多模态LLMs以遵循零样本示范性指令") 所示，VPG-C建立在冻结的LLM（Vicuna-7B（Chiang等， [2023](#bib.bib10)））和视觉编码器（EVA-CLIP（Fang等， [2023](#bib.bib13)））之上。我们采用BLIP-2（Li等， [2023b](#bib.bib32)）中广泛使用的Q-Former作为我们的视觉提示生成器。VPG-C首先使用LLM的中间输出来推断指令特定的指导。这随后帮助VPG关注图像中缺失的视觉细节。通过通过跳过连接将这些剩余细节合并回去，VPG-C实现了对示范性指令的透彻理解。
- en: Given a demonstrative instruction, we first adopt the Q-former to generate general
    visual prompts for each image in the instruction. Q-former takes a fixed number
    of $K$-th text token and $\mathcal{V}^{0}_{j}=\{\mathbf{v}^{0}_{j1},...,\mathbf{v}^{0}_{jK}\}$-layer
    language decoder, we then extract the hidden representation of the last input
    token $\mathbf{h}_{N}^{L/2}$ from $\mathbf{h}_{N}^{L/2}$.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定示范性指令，我们首先采用Q-former为指令中的每个图像生成一般视觉提示。Q-former使用固定数量的$K$-th文本令牌和$\mathcal{V}^{0}_{j}=\{\mathbf{v}^{0}_{j1},...,\mathbf{v}^{0}_{jK}\}$-层语言解码器，然后从$\mathbf{h}_{N}^{L/2}$中提取最后一个输入令牌的隐藏表示$\mathbf{h}_{N}^{L/2}$。
- en: 'After obtaining the instruction-specific guidance from the language decoder,
    we compose it with a new set of learnable queries: $\mathbf{g}+\mathcal{Q}$. Then,
    we reuse the same Q-former with the above conditionally generated queries to attend
    to residual visual details, thus obtaining the visual prompts $\overline{\mathcal{V}}_{j}=\{\overline{\mathbf{v}}_{j1},...,\overline{\mathbf{v}}_{jK}\}$,
    via skip connection: $\tilde{\mathcal{V}}_{j}^{L/2}=\mathcal{V}_{j}^{L/2}+\mathbf{Linear}(\overline{\mathcal{V}}_{j})$-th
    layer.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在从语言解码器获得指令特定的指导后，我们将其与一组新的可学习查询合成：$\mathbf{g}+\mathcal{Q}$。然后，我们重用具有上述条件生成的查询的相同Q-former来关注剩余的视觉细节，从而通过跳过连接获得视觉提示$\overline{\mathcal{V}}_{j}=\{\overline{\mathbf{v}}_{j1},...,\overline{\mathbf{v}}_{jK}\}$，通过跳过连接：$\tilde{\mathcal{V}}_{j}^{L/2}=\mathcal{V}_{j}^{L/2}+\mathbf{Linear}(\overline{\mathcal{V}}_{j})$-th层。
- en: Efficient training. Our VPG-C module is parameter-efficient as the Q-former
    is frozen and only a set of query embeddings and two linear projection layers
    need to be fine-tuned, which only account for 0.09% ($\sim$, which will not cause
    any influence on LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 高效训练。我们的VPG-C模块参数高效，因为Q-former被冻结，只需对一组查询嵌入和两个线性投影层进行微调，这仅占0.09%（$\sim$），不会对LLMs产生任何影响。
- en: 'Analysis on inserting VPG-C in the intermediate layer ($\frac{L}{2}$): 1) Guidance
    generation. Previous studies have shown that features provided by the intermediate
    layer may suffice to preliminarily understand the given input samples (Xin et al.,
    [2020](#bib.bib57)) and can serve as guidance hints to improve training (Romero
    et al., [2014](#bib.bib47)). Thus, generating guidance in the intermediate layer
    allows the model to form a preliminary understanding of the given instruction.
    Generating guidance too early could be problematic, as the model might not have
    gathered sufficient contextual information to generate effective guidance. Conversely,
    generating guidance too late could result in the model’s attention being narrowly
    focused on what it perceives as the final answer, hindering its ability to guide
    the Q-former to extract relevant details from the images. Therefore, placing the
    guidance generation step in the intermediate layer strikes a balance. 2) Detail
    reintegration. Intermediate-layer reintegration of residual visual details preserves
    prior knowledge and allows subsequent layers to integrate new information effectively.
    Reintegrating residual details too early in the pipeline might overwrite important
    context, while reintegrating it too late could limit the impact on the model’s
    reasoning. Therefore, the intermediate layer offers a strategic position for residual
    details reintegration, enabling the model to reason effectively and arrive at
    the correct answers by leveraging the complemented visual residual details. We
    further provide quantitative analysis in Section [4.4](#S4.SS4 "4.4 In-Depth Analysis
    ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间层插入 VPG-C 的分析（$\frac{L}{2}$）：1）指导生成。以往研究表明，中间层提供的特征可能足以初步理解给定的输入样本（Xin 等，[2020](#bib.bib57)），并可以作为改进训练的指导提示（Romero
    等，[2014](#bib.bib47)）。因此，在中间层生成指导可以使模型对给定指令形成初步理解。生成指导过早可能会有问题，因为模型可能还没有收集到足够的上下文信息来生成有效的指导。相反，生成指导过晚可能导致模型的注意力过于集中在其认为的最终答案上，从而阻碍其引导
    Q-former 从图像中提取相关细节的能力。因此，将指导生成步骤放在中间层可以达到平衡。2）细节再整合。中间层对残余视觉细节的再整合保留了先前的知识，并允许后续层有效整合新信息。在管道中过早地再整合残余细节可能会覆盖重要的上下文，而过晚再整合则可能限制对模型推理的影响。因此，中间层提供了一个战略性的位置用于残余细节的再整合，使模型能够有效推理并通过利用补充的视觉残余细节得出正确答案。我们在第
    [4.4](#S4.SS4 "4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions")节中进一步提供定量分析。
- en: 2.2 Synthetic Discriminative Training Strategy
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 合成判别训练策略
- en: The proposed training strategy diagnoses the areas initially ignored by Q-former
    according to its cross-attention maps between the queries and the image features,
    and generates a synthetic image by performing several types of editing on the
    ignored areas. Then, an inter-image discriminative task is formulated as describing
    the subtle difference between the original and the synthetic images. Considering
    the edits are performed in the mostly ignored areas, VPG-C is challenged to recover
    the missing details to describe the difference. An overview is illustrated in
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Synthetic Discriminative Training Strategy ‣
    2 Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的训练策略根据 Q-former 查询与图像特征之间的交叉注意图来诊断最初被忽略的区域，并通过对这些忽略区域进行多种编辑生成合成图像。然后，制定了一个图像间判别任务，即描述原始图像与合成图像之间的微小差异。考虑到编辑是在大多数被忽略的区域进行的，VPG-C
    被挑战去恢复缺失的细节以描述差异。图 [3](#S2.F3 "Figure 3 ‣ 2.2 Synthetic Discriminative Training
    Strategy ‣ 2 Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 中展示了概览。
- en: Editing target identification. The Q-former takes the queries to interact with
    frozen image features through several cross-attention layers and uses the output
    query representations as the visual prompts. Therefore, the cross-attention maps
    between queries and image features reflect the interest of queries. We average
    the cross-attention maps across all layers and all queries to obtain the global
    cross-attention map $\mathcal{A}$ with RoIAlign (He et al., [2017](#bib.bib20)),
    where we average the values of $\mathcal{A}$ is extracted by the Q-former. Thus,
    we select the most ignored objects based on the $\Phi(o_{i})$ value.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑目标识别。Q-former通过几个交叉注意层与冻结图像特征进行交互，并使用输出查询表示作为视觉提示。因此，查询和图像特征之间的交叉注意图反映了查询的兴趣。我们对所有层和所有查询的交叉注意图进行平均，以获得全局交叉注意图$\mathcal{A}$，并使用RoIAlign（He等，[2017](#bib.bib20)），其中我们对$\mathcal{A}$的值进行平均，这些值由Q-former提取。因此，我们根据$\Phi(o_{i})$值选择最被忽略的对象。
- en: '![Refer to caption](img/c4a46d654432fe596abf32e01d3da125.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4a46d654432fe596abf32e01d3da125.png)'
- en: 'Figure 3: Pipeline demonstration of synthetic discriminative training strategy
    for VPG-C.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：VPG-C合成判别训练策略的管道演示。
- en: 'Editing description generation. We define four types of editing: modifying
    objects, swapping objects, deleting objects, and adding objects. Given the selected
    object, we instruct ChatGPT (OpenAI, [2023a](#bib.bib40)) to generate a suitable
    editing description that is in harmony with the context, where ChatGPT is prompted
    with the corresponding image caption and detailed object information (i.e., labels,
    positions). For adding objects, we only select `BACKGROUND` objects to add objects.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑描述生成。我们定义了四种编辑类型：修改对象、交换对象、删除对象和添加对象。对于选择的对象，我们指示ChatGPT（OpenAI，[2023a](#bib.bib40)）生成适合的编辑描述，与上下文保持一致，其中ChatGPT会收到相应的图像说明和详细的对象信息（即标签、位置）。对于添加对象，我们只选择`BACKGROUND`对象进行添加。
- en: Synthetic image generation. After obtaining the editing description, we generate
    the synthetic image using a text-to-image latent diffusion model (i.e., Blended
    Diffusion (Avrahami et al., [2022](#bib.bib2))). Blended Diffusion performs local
    editing on the image according to the target object mask and the editing description,
    thus rendering the synthetic image. To ensure quality, we ﬁlter the edited images
    using CLIP similarity (Radford et al., [2021b](#bib.bib44)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 合成图像生成。在获得编辑描述后，我们使用文本到图像的潜在扩散模型（即Blended Diffusion（Avrahami等，[2022](#bib.bib2)））生成合成图像。Blended
    Diffusion根据目标对象掩膜和编辑描述对图像进行局部编辑，从而渲染合成图像。为了确保质量，我们使用CLIP相似度（Radford等，[2021b](#bib.bib44)）过滤编辑后的图像。
- en: Inter-image discriminative training. Given the original image and the synthetic
    image pair, along with the task instruction (“Describe the difference between
    the images”), the inter-image discriminative training task is defined as generating
    sentences to describe the subtle difference between the images. We convert the
    editing description to acquire the ground-truth sentences.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图像间判别训练。给定原始图像和合成图像对，以及任务指令（“描述图像之间的差异”），图像间判别训练任务定义为生成句子以描述图像之间的细微差别。我们转换编辑描述以获得真实的句子。
- en: 2.3 Implementation Details
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 实施细节
- en: We only fine-tune the VPG-C parameters using the proposed synthetic training
    strategy, keeping other components frozen. As for synthetic training, we select
    about 30K images from CC3M (Sharma et al., [2018](#bib.bib48)) that contain significantly
    ignored objects and perform different types of editing on them. In total, we generate
    approximately 64K synthetic images with suitable modifications. To stabilize the
    training and avoid overfitting, we use 500K image-caption pairs from CC3M to jointly
    train the VPG-C module. We tune the VPG-C module for 18K steps using a batch size
    of 24 for synthetic training and 64 for image captioning, which takes about 7
    hours to complete with a single A100 GPU. We adopt the AdamW optimizer with $\beta=(0.9,0.999)$,
    and set the learning rate and weight decay to 0.00002 and 0.05, respectively.
    We warm up the training with 2K steps, followed by a learning rate decay mechanism
    with the cosine schedule. We provide more implementation details and the choice
    of Q-former in Appendix [C](#A3 "Appendix C Implementation Details ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅通过提出的合成训练策略来微调VPG-C参数，同时保持其他组件不变。至于合成训练，我们从CC3M (Sharma et al., [2018](#bib.bib48))中选择了大约30K张包含显著被忽略的对象的图像，并对其进行不同类型的编辑。总共，我们生成了大约64K张经过适当修改的合成图像。为了稳定训练并避免过拟合，我们使用500K张来自CC3M的图像-标题对来联合训练VPG-C模块。我们用24的批量大小对合成训练进行18K步的VPG-C模块调优，对图像标题生成使用64的批量大小，这在单个A100
    GPU上大约需要7小时完成。我们采用了AdamW优化器，$\beta=(0.9,0.999)$，将学习率和权重衰减分别设置为0.00002和0.05。我们用2K步进行热身训练，然后使用余弦调度机制进行学习率衰减。我们在附录[C](#A3
    "Appendix C Implementation Details ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot
    Demonstrative Instructions")中提供了更多的实现细节和Q-former的选择。
- en: '![Refer to caption](img/c885a7e4a5415458193fb1c61f906fc1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c885a7e4a5415458193fb1c61f906fc1.png)'
- en: 'Figure 4: Demonstrations and task taxonomy of the proposed DEMON benchmark.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：所提议的DEMON基准的演示和任务分类。
- en: 3 DEMON Benchmark
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 DEMON基准
- en: 'Data format. All task instances are transformed into a unified instruction-response
    form for zero-shot evaluation. Formally, each instance in DEMON consists of the
    following components:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据格式。所有任务实例都被转换为统一的指令-响应形式，以进行零-shot评估。正式地，DEMON中的每个实例包括以下组件：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '`Task_Instruction`: provides a complete natural language definition of a given
    task, including the input/output format and the task objective.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Task_Instruction`: 提供了给定任务的完整自然语言定义，包括输入/输出格式和任务目标。'
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '`Task_Instance`: is a concrete instance of a given task that consists of demonstrative
    image-text sequential context (e.g., visually-rich textbooks, specific questions
    about the context).'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Task_Instance`: 是给定任务的一个具体实例，由示例图像-文本序列上下文组成（例如，视觉丰富的教科书，关于上下文的具体问题）。'
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '`Response`: represents the target output in natural language for a given task
    instruction and task instance. For classification tasks, we convert the class
    labels as options into the instruction and ask the model to output the option
    index in natural language as the response.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Response`: 表示给定任务指令和任务实例的目标自然语言输出。对于分类任务，我们将类别标签作为选项转换到指令中，并要求模型以自然语言输出选项索引作为响应。'
- en: Without any specific emphasis, we use the term “instruction” to refer to the
    combination of `Task_Instruction` and `Task_Instance`. For each task, we manually
    design 10 `Task_Instruction` templates in natural language to increase the diversity.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何具体强调的情况下，我们使用“指令”一词来指代`Task_Instruction`和`Task_Instance`的组合。对于每个任务，我们手动设计了10个自然语言的`Task_Instruction`模板，以增加多样性。
- en: 'Task collection and categorization. To comprehensively benchmark the demonstrative
    instruction following ability, we extensively gather a wide variety of multimodal
    datasets from different fields and scenarios. As illustrated in Figure [4](#S2.F4
    "Figure 4 ‣ 2.3 Implementation Details ‣ 2 Method ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions"), DEMON has three important properties:
    1) Demonstrative vision-language context, all the instructions contain sequences
    of inter-related images and texts, such as storyboards with scripts, and textbooks
    with diagrams. 2) Diverse forms of complex instructions, the instructions range
    from designing panels for comics, to discovering differences between surveillance
    images, and to conversational embodied tasks. 3) Vast range of instruction-following
    scenarios, the benchmark covers multiple practical scenarios, including cartoons,
    industrial visuals, driving recordings, recipes, etc.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 任务收集与分类。为了全面基准化演示指令跟随能力，我们广泛收集了来自不同领域和场景的多模态数据集。如图[4](#S2.F4 "Figure 4 ‣ 2.3
    Implementation Details ‣ 2 Method ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot
    Demonstrative Instructions")所示，DEMON具有三个重要特性：1）演示视觉语言上下文，所有指令都包含相互关联的图像和文本序列，例如带有脚本的分镜图和带有图解的教科书。2）多样化的复杂指令形式，指令从设计漫画面板，到发现监控图像之间的差异，再到对话体任务。3）广泛的指令跟随场景，基准涵盖多个实际场景，包括卡通、工业视觉、驾驶记录、食谱等。
- en: Evaluation protocols. Thanks to the unified task format of DEMON, all tasks
    can be evaluated in a zero-shot manner. For the open-ended generation tasks, we
    adopt ROUGE-L for evaluation. For the tasks that require the models to output
    option indexes, we take Accuracy as the evaluation metric. While well-formated
    options are provided, we empirically observe that many MLLMs struggle to strictly
    follow instructions to output the option indexes but generate free-form text.
    Thus, when models do not exactly output the required options, we match their outputs
    to one of the given options based on the TF-IDF distance, which we find is more
    robust than model-based methods (OpenAI, [2023a](#bib.bib40); Reimers & Gurevych,
    [2019](#bib.bib46)). Since we explore a large number of tasks, we take a maximum
    of 500 instances per task for evaluation efficiency and exclude several datasets
    that are difficult to obtain and are subject to strict copyright restrictions
    (referred to as DEMON-Core). Meanwhile, we report the full version of the benchmark
    to facilitate future research on large-scale multimodal instruction tuning (referred
    to as DEMON-Full). Unless specifically stated, we use DEMON to refer to DEMON-Core
    in the following.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议。由于DEMON统一的任务格式，所有任务都可以以零-shot的方式进行评估。对于开放式生成任务，我们采用ROUGE-L进行评估。对于要求模型输出选项索引的任务，我们采用准确率作为评估指标。尽管提供了格式良好的选项，我们经验观察到许多多模态语言模型在严格跟随指令输出选项索引时存在困难，而生成自由格式文本。因此，当模型未能准确输出所需选项时，我们根据TF-IDF距离将其输出匹配到给定的选项之一，我们发现这种方法比基于模型的方法更为稳健（OpenAI，[2023a](#bib.bib40)；Reimers
    & Gurevych，[2019](#bib.bib46)）。由于我们探索了大量任务，为了评估效率，每个任务最多取500个实例，并排除一些难以获得且受严格版权限制的数据集（称为DEMON-Core）。同时，我们报告基准的完整版本，以促进未来对大规模多模态指令调优的研究（称为DEMON-Full）。除非特别说明，以下所用DEMON指的是DEMON-Core。
- en: 'Table 1: Detailed statistics of DEMON benchmark.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：DEMON基准的详细统计数据。
- en: '|  | Tasks | Scenarios | Images | Instructions | Avg. Images / Instruction
    | Avg. Words / Instruction |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | 任务 | 场景 | 图像 | 指令 | 平均图像/指令 | 平均单词/指令 |'
- en: '| DEMON-Core | 29 | 19 | 62.81K | 18.18K | 3.46 | 92.69 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| DEMON-Core | 29 | 19 | 62.81K | 18.18K | 3.46 | 92.69 |'
- en: '| DEMON-Full | 31 | 20 | 1.77M | 477.72K | 3.70 | 97.58 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| DEMON-Full | 31 | 20 | 1.77M | 477.72K | 3.70 | 97.58 |'
- en: Benchmark analysis. Table [1](#S3.T1 "Table 1 ‣ 3 DEMON Benchmark ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions") details the statistics.
    DEMON benchmark covers 31 tasks of 7 categories across 20 scenarios. In total,
    DEMON-Full includes 477.72K instruction-response pairs, serving as a large-scale
    benchmark for demonstrative instruction following. On average, each instruction
    contains 3.70 images and 97.58 words. Please refer to Appendix [B](#A2 "Appendix
    B Benchmark Details ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") for more details.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基准分析。表 [1](#S3.T1 "Table 1 ‣ 3 DEMON Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions") 详细列出了统计数据。DEMON 基准涵盖 20 个场景中的
    7 类 31 个任务。总的来说，DEMON-Full 包括 477.72K 指令-响应对，作为一个大规模的示范性指令跟随基准。平均每条指令包含 3.70 张图片和
    97.58 个单词。更多详情请参见附录 [B](#A2 "Appendix B Benchmark Details ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions")。
- en: 'Table 2: Average results of zero-shot evaluation on each task category of DEMON
    Benchmark.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：DEMON 基准测试中每个任务类别的零样本评估的平均结果。
- en: '|  | Multimodal | Visual | Visual Relation | Multimodal | Knowledge | Text-Rich
    | Multi-Image |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 多模态 | 视觉 | 视觉关系 | 多模态 | 知识 | 文本丰富 | 多图像 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | Dialogue | Storytelling | Inference | Cloze | Grounded QA | Images QA
    | Reasoning |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话 | 讲故事 | 推理 | 填空 | 基于知识问答 | 图片问答 | 推理 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BLIP-2 (Li et al., [2023b](#bib.bib32)) | 26.12 | 21.31 | 10.67 | 17.94 |
    39.23 | 33.53 | 39.65 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 (Li et al., [2023b](#bib.bib32)) | 26.12 | 21.31 | 10.67 | 17.94 |
    39.23 | 33.53 | 39.65 |'
- en: '| InstructBLIP (Dai et al., [2023](#bib.bib12)) | 33.58 | 24.41 | 11.49 | 21.20
    | 47.40 | 44.40 | 48.55 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP (Dai et al., [2023](#bib.bib12)) | 33.58 | 24.41 | 11.49 | 21.20
    | 47.40 | 44.40 | 48.55 |'
- en: '| LLaMA-Adapter V2 (Gao et al., [2023](#bib.bib16)) | 14.22 | 17.57 | 13.51
    | 18.00 | 44.80 | 32.00 | 44.03 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 (Gao et al., [2023](#bib.bib16)) | 14.22 | 17.57 | 13.51
    | 18.00 | 44.80 | 32.00 | 44.03 |'
- en: '| LLaVA (Liu et al., [2023](#bib.bib36)) | 7.79 | 10.70 | 8.27 | 15.85 | 36.20
    | 28.33 | 41.53 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA (Liu et al., [2023](#bib.bib36)) | 7.79 | 10.70 | 8.27 | 15.85 | 36.20
    | 28.33 | 41.53 |'
- en: '| MiniGPT-4 (Zhu et al., [2023a](#bib.bib63)) | 13.69 | 17.07 | 7.95 | 16.60
    | 30.27 | 26.40 | 43.50 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 (Zhu et al., [2023a](#bib.bib63)) | 13.69 | 17.07 | 7.95 | 16.60
    | 30.27 | 26.40 | 43.50 |'
- en: '| mPLUG-Owl (Ye et al., [2023](#bib.bib60)) | 12.67 | 19.33 | 5.40 | 16.25
    | 33.27 | 32.47 | 42.50 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl (Ye et al., [2023](#bib.bib60)) | 12.67 | 19.33 | 5.40 | 16.25
    | 33.27 | 32.47 | 42.50 |'
- en: '| OpenFlamingo (Awadalla et al., [2023](#bib.bib3)) | 16.88 | 24.22 | 13.85
    | 21.65 | 32.00 | 30.60 | 41.63 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo (Awadalla et al., [2023](#bib.bib3)) | 16.88 | 24.22 | 13.85
    | 21.65 | 32.00 | 30.60 | 41.63 |'
- en: '| Otter (Li et al., [2023a](#bib.bib30)) | 15.37 | 15.57 | 11.39 | 16.00 |
    41.67 | 27.73 | 43.85 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Otter (Li et al., [2023a](#bib.bib30)) | 15.37 | 15.57 | 11.39 | 16.00 |
    41.67 | 27.73 | 43.85 |'
- en: '| VPG-C | 37.50 | 25.20 | 25.90 | 22.15 | 48.60 | 44.93 | 50.28 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C | 37.50 | 25.20 | 25.90 | 22.15 | 48.60 | 44.93 | 50.28 |'
- en: 4 Experiments
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Zero-Shot Evaluation on DEMON Benchmark
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 DEMON 基准测试中的零样本评估
- en: Comparison with advanced MLLMs. In this section, we conduct comprehensive evaluation
    of our VPG-C and the recent advanced MLLMs on the proposed DEMON benchmark. For
    all methods, we choose versions with parameter sizes less than 10B. Please refer
    to Appendix [D](#A4 "Appendix D Model Details in DEMON Benchmark ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"), [F](#A6 "Appendix
    F Detailed Zero-Shot Performance on DEMON Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions") for details. The average results
    of each task category are summarized in Table [2](#S3.T2 "Table 2 ‣ 3 DEMON Benchmark
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"),
    which indicates the following.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与先进的 MLLMs 的比较。在本节中，我们对 VPG-C 和最近的先进 MLLMs 在提出的 DEMON 基准测试上进行全面评估。对于所有方法，我们选择参数规模小于
    10B 的版本。详细信息请参见附录 [D](#A4 "Appendix D Model Details in DEMON Benchmark ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")、[F](#A6 "Appendix
    F Detailed Zero-Shot Performance on DEMON Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions")。每个任务类别的平均结果总结在表 [2](#S3.T2 "Table
    2 ‣ 3 DEMON Benchmark ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 中，表明如下。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VPG-C consistently outperforms existing models by a large margin across all
    categories, which demonstrates the stronger generalizability to follow such complicated
    demonstrative instructions.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VPG-C 在所有类别中都显著超越现有模型，展示了更强的通用性来跟随如此复杂的示范性指令。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While previous works mainly fine-tune on massive multimodal instruction data,
    VPG-C still achieves the highest performance using synthetic training data with
    much lower computation cost. This validates the effectiveness of the proposed
    VPG-C module and its synthetic training strategy.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然以前的工作主要在大量多模态指令数据上进行微调，但VPG-C仍然利用合成训练数据以更低的计算成本实现了最高性能。这验证了所提出的VPG-C模块及其合成训练策略的有效性。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared with previous works that fine-tune the large-scale language decoder
    or visual encoder (i.e., LLaVA, mPLUG-Owl), our model only tunes the lightweight
    VPG-C module with 6.3M parameters and achieves significant performance gain.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与之前对大规模语言解码器或视觉编码器进行微调的工作（例如LLaVA，mPLUG-Owl）相比，我们的模型仅微调了参数为6.3M的轻量级VPG-C模块，并且获得了显著的性能提升。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VPG-C exhibits significant superiority in several challenging tasks. For instance,
    VPG-C surpasses SOTA methods by 3.92% on multimodal dialogue, which requires models
    to effectively associate the interleaved images mentioned in different turns of
    the dialogue.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VPG-C在几个具有挑战性的任务中表现出显著的优势。例如，VPG-C在多模态对话中超越了SOTA方法3.92%，这要求模型有效地关联对话中不同轮次提到的交错图像。
- en: Innovative findings. The extensive evaluation on DEMON benchmark reveals several
    key findings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 创新发现。对DEMON基准的广泛评估揭示了几个关键发现。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Poor performance on demonstrative instructions. While several models (e.g.,
    OpenFlamingo, Otter, mPLUG-owl) have been trained on interleaved vision-language
    data, such as mmc4 (Zhu et al., [2023b](#bib.bib64)), they still struggle to perform
    well on the demonstraive instructions. We suppose that while mmc4 contains sequences
    of interleaved images as context, the web-crawled images are often weakly related.
    In contrast, the images and text in demonstrative instructions are highly related,
    requiring models to deeply associate them to comprehend the task intents.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在演示性指令上的表现差。尽管一些模型（如OpenFlamingo，Otter，mPLUG-owl）已经在交错的视觉-语言数据上进行了训练，如mmc4（Zhu等，[2023b](#bib.bib64)），它们在演示性指令上的表现仍然不佳。我们认为，尽管mmc4包含了作为上下文的交错图像序列，但网络爬取的图像往往关系较弱。相比之下，演示性指令中的图像和文本高度相关，需要模型深入关联以理解任务意图。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Limited instruction following ability. Despite existing vision-language models
    leveraging state-of-the-art LLMs, which have demonstrated impressive ability in
    following language instructions, this competence seems to falter when dealing
    with complex multimodal instructions. For instance, when tasked with selecting
    the correct answer from a choice list given the context of images and texts, we
    observed some models inclining more towards describing the contents of the images
    instead of addressing the posed questions. This is perceived as a deficiency in
    the image-text alignment training process, to which we attribute the discrepancy.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有限的指令跟随能力。尽管现有的视觉语言模型利用了先进的LLM，这些模型在遵循语言指令方面表现出色，但在处理复杂的多模态指令时，这种能力似乎有所下降。例如，在根据图像和文本的上下文从选项列表中选择正确答案时，我们观察到一些模型倾向于描述图像内容，而不是回答提出的问题。这被认为是图像-文本对齐训练过程中的一个缺陷，我们认为这就是差异的根源。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Failing to process image-choice questions. When dealing with multimodal cloze
    tasks, all models are limited to processing instructions that involve images as
    options. We hope future work to utilize the new benchmark to make progress on
    this type of demonstrative instructions.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理图像选择问题的失败。在处理多模态填空任务时，所有模型都只能处理涉及图像作为选项的指令。我们希望未来的工作能够利用这一新的基准，在这种演示性指令类型上取得进展。
- en: 4.2 Zero-Shot Evaluation on MME Benchmark
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 MME基准的零样本评估
- en: We evaluate our VPG-C on the concurrently proposed MME benchmark (Fu et al.,
    [2023](#bib.bib15)) to further illustrate its strong generalizability to follow
    a diverse range of single-image instructions. MME benchmark measures both perception
    and cognition abilities on a total of 14 subtasks. We report the averaged results
    of perception tasks and cognition tasks in Table [3](#S4.T3 "Table 3 ‣ 4.2 Zero-Shot
    Evaluation on MME Benchmark ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow
    Zero-shot Demonstrative Instructions"), respectively. While we do not use massive
    multimodal instruction data to fine-tune VPG-C, VPG-C still achieves superior
    performance, compared with the supervised instruction-tuned models. This indicates
    our method effectively overcomes the inherent limitation of VPGs and the completed
    residual details are also essential for single-image instructions. Please refer
    to Appendix [E](#A5 "Appendix E Detailed Zero-Shot Performance on MME Benchmark
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")
    for detailed results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在同时提出的MME基准上评估了我们的VPG-C（Fu et al., [2023](#bib.bib15)），以进一步说明其强大的通用性来跟随各种单图像指令。MME基准在总共14个子任务上测量感知和认知能力。我们在表[3](#S4.T3
    "表 3 ‣ 4.2 MME基准的零-shot评估 ‣ 4 实验 ‣ 微调多模态LLMs以跟随零-shot示范指令")中分别报告了感知任务和认知任务的平均结果。虽然我们没有使用大量的多模态指令数据来微调VPG-C，但VPG-C仍然实现了优于监督指令微调模型的性能。这表明我们的方法有效克服了VPGs的固有限制，完成的残余细节对于单图像指令也至关重要。详细结果请参见附录[E](#A5
    "附录 E MME基准的详细零-shot性能 ‣ 微调多模态LLMs以跟随零-shot示范指令")。
- en: 'Table 3: Zero-shot evaluation of perception and cognition abilities on MME
    benchmark.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在MME基准上的感知和认知能力的零-shot评估。
- en: '|  | BLIP-2 | InstructBLIP | LA-V2 | LLaVA | MiniGPT-4 | mPLUG-Owl | Otter
    | VPG-C |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | BLIP-2 | InstructBLIP | LA-V2 | LLaVA | MiniGPT-4 | mPLUG-Owl | Otter
    | VPG-C |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Perception | 1293.84 | 1212.82 | 972.67 | 502.82 | 866.57 | 967.34 | 1292.26
    | 1299.24 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 1293.84 | 1212.82 | 972.67 | 502.82 | 866.57 | 967.34 | 1292.26 | 1299.24
    |'
- en: '| Cognition | 290.00 | 291.79 | 248.93 | 214.64 | 292.14 | 276.07 | 306.43
    | 321.07 | ![Refer to caption](img/79e10831824fe266bec05092fb4efc60.png)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '| 认知 | 290.00 | 291.79 | 248.93 | 214.64 | 292.14 | 276.07 | 306.43 | 321.07
    | ![参见说明](img/79e10831824fe266bec05092fb4efc60.png)'
- en: 'Figure 5: Human evaluation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：人工评估。
- en: 4.3 Human Evaluation on General-Purpose Language Generation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 通用语言生成的人工评估
- en: We further conduct human evaluation on the OwlEval benchmark (Ye et al., [2023](#bib.bib60)),
    which contains 82 open-ended questions including advertisement and poem creation,
    diagram and ﬂowchart comprehension, and teaching, etc. Specifically, we recruit
    8 well-educated people to rank the randomly shuffled responses from VPG-C, MiniGPT-4,
    mPLUG-Owl, OpenFlamingo, and InstructBLIP. The scores range from 1 to 5 (5 means
    best) and are allowed to be equal for comparable instances. As shown in Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Zero-Shot Evaluation on MME Benchmark ‣ 4 Experiments ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"), VPG-C also demonstrates
    better open-ended language generation ability in various practical cases.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步在OwlEval基准上进行人工评估（Ye et al., [2023](#bib.bib60)），该基准包含82个开放性问题，包括广告和诗歌创作、图表和流程图理解，以及教学等。具体来说，我们招募了8名受过良好教育的人员对VPG-C、MiniGPT-4、mPLUG-Owl、OpenFlamingo和InstructBLIP随机打乱的回应进行排名。评分范围从1到5（5表示最佳），对于可比实例允许评分相同。如图[5](#S4.F5
    "图 5 ‣ 4.2 MME基准的零-shot评估 ‣ 4 实验 ‣ 微调多模态LLMs以跟随零-shot示范指令")所示，VPG-C在各种实际案例中也展现了更好的开放性语言生成能力。
- en: 4.4 In-Depth Analysis
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 深度分析
- en: Effectiveness of individual components. We investigate the effectiveness of
    each component in Table [4](#S4.T4 "Table 4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions").
    We start with the backbone model that uses the Q-former to generate visual prompts.
    1) Instead of applying VPG-C to capture missing details, we first attempt a simple
    heuristic-based method that directly extracts the less attended visual features
    according to the cross-attention maps of Q-former and reintegrates them to the
    intermediate layer of the LLM as ours. We fine-tune a linear layer before reintegrating
    with 0.5 million image-caption pairs. The results of Row 2 show that such a sample
    heuristic can bring some improvement. This validates the importance of re-extracting
    missing visual features from images for comprehending demonstrative instructions.
    2) Then, we replace it with VPG-C and train it only using the image-caption pairs
    without synthetic training. The results of Row 3 demonstrate that VPG-C can more
    accurately complete the required missing details by leveraging the intermediate
    inference results of the LLM. 3) However, solely using common image-caption data
    can not fully unleash the power of VPG-C. Comparing Row 3 and Row 4, we observe
    a significant improvement for all tasks, indicating that the proposed synthetic
    discriminative training strategy can methodically empower VPG-C to extract missing
    visual details.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 各个组件的有效性。我们调查了表格[4](#S4.T4 "Table 4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")中每个组件的有效性。我们从使用Q-former生成视觉提示的主干模型开始。1)  我们首先尝试一种简单的基于启发式的方法，直接根据Q-former的交叉注意力图提取不太被关注的视觉特征，并将其重新整合到LLM的中间层。我们在重新整合之前对0.5百万对图像-字幕对进行了线性层微调。第2行的结果表明，这种样本启发式方法可以带来一定的改进。这验证了从图像中重新提取缺失的视觉特征以理解演示指令的重要性。2)  然后，我们用VPG-C替代它，并仅使用图像-字幕对进行训练，而不使用合成训练。第3行的结果表明，VPG-C能够通过利用LLM的中间推理结果更准确地完成所需的缺失细节。3)  然而，仅使用普通的图像-字幕数据无法充分发挥VPG-C的优势。比较第3行和第4行，我们观察到所有任务都有显著的改进，这表明提出的合成判别训练策略可以系统性地增强VPG-C提取缺失视觉细节的能力。
- en: VPG-C can better guide VPGs. Since InstructBLIP can perform conditional visual
    feature extraction, we implement a variant version that concatenates its initially
    generated answer with the instruction as condition to re-extract features. The
    initial generated answer serves as an additional heuristic from the LLM for guiding
    feature extraction. Then, the newly extracted visual prompts are used to re-generate
    answers. For a fair comparison, we provide a zero-shot version (Row 5) and a fine-tuned
    version (Row 6) using synthetic training as ours. As shown in Table [4](#S4.T4
    "Table 4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions"), directly using synthetic data
    and inferred answers as heuristic conditions fails to yield a notable improvement.
    In contrast, VPG-C can better guide the VPG to complete the missing visual details
    by intercepting the intermediate representations of the LLM. Further, VPG-C is
    more computation-efficient as it only requires one full forward pass of the LLM,
    while the InstructBLIP variants require twice.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: VPG-C可以更好地指导VPGs。由于InstructBLIP可以进行条件视觉特征提取，我们实现了一个变体版本，将其初步生成的答案与指令连接作为条件来重新提取特征。初步生成的答案作为LLM提供的额外启发式指导特征提取。然后，新提取的视觉提示用于重新生成答案。为了公平比较，我们提供了一个零样本版本（第5行）和一个使用合成训练的微调版本（第6行）。如表格[4](#S4.T4
    "Table 4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions")所示，直接使用合成数据和推断答案作为启发式条件未能带来显著改进。相比之下，VPG-C可以更好地指导VPG完成缺失的视觉细节，通过拦截LLM的中间表示。进一步来说，VPG-C在计算上更高效，因为它只需要LLM的一次完整前向传递，而InstructBLIP变体需要两次。
- en: 'Table 4: Ablation results on DEMON Benchmark.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：DEMON基准上的消融实验结果。
- en: '|  |  | Multimodal | Visual | Visual Relation | Multimodal | Knowledge | Text-Rich
    | Multi-Image |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 多模态 | 视觉 | 视觉关系 | 多模态 | 知识 | 文本丰富 | 多图像 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |  | Dialogue | Storytelling | Inference | Cloze | Grounded QA | Images
    QA | Reasoning |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 对话 | 讲故事 | 推断 | 完形填空 | 基于事实的问答 | 图像问答 | 推理 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | Backbone | 25.65 | 21.72 | 9.33 | 17.06 | 37.21 | 32.42 | 41.30 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 基础模型 | 25.65 | 21.72 | 9.33 | 17.06 | 37.21 | 32.42 | 41.30 |'
- en: '| 2 |    +Heuristic Details | 28.13 | 22.76 | 12.69 | 18.81 | 38.75 | 34.14
    | 43.26 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 2 |    +启发式细节 | 28.13 | 22.76 | 12.69 | 18.81 | 38.75 | 34.14 | 43.26 |'
- en: '| 3 |    +VPG-C | 31.76 | 23.62 | 19.12 | 20.09 | 42.53 | 39.68 | 46.71 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 3 |    +VPG-C | 31.76 | 23.62 | 19.12 | 20.09 | 42.53 | 39.68 | 46.71 |'
- en: '| 4 |    +Synthetic Training | 37.50 | 25.20 | 25.90 | 22.15 | 48.60 | 44.93
    | 50.28 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 4 |    +合成训练 | 37.50 | 25.20 | 25.90 | 22.15 | 48.60 | 44.93 | 50.28 |'
- en: '|  | InstructBLIP | 33.58 | 24.41 | 11.48 | 21.20 | 47.40 | 44.40 | 48.55 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | InstructBLIP | 33.58 | 24.41 | 11.48 | 21.20 | 47.40 | 44.40 | 48.55 |'
- en: '| 5 |    +Answer Condition | 32.10 | 23.76 | 11.02 | 21.86 | 47.94 | 42.08
    | 49.01 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 5 |    +答案条件 | 32.10 | 23.76 | 11.02 | 21.86 | 47.94 | 42.08 | 49.01 |'
- en: '| 6 |    +Synthetic Training | 31.76 | 24.32 | 12.78 | 19.87 | 46.58 | 42.36
    | 49.82 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 6 |    +合成训练 | 31.76 | 24.32 | 12.78 | 19.87 | 46.58 | 42.36 | 49.82 |'
- en: '|  | LLaVA | 7.79 | 10.70 | 8.27 | 15.85 | 36.20 | 28.33 | 41.53 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaVA | 7.79 | 10.70 | 8.27 | 15.85 | 36.20 | 28.33 | 41.53 |'
- en: '| 7 | Linear VPG | 16.43 | 19.48 | 14.75 | 18.54 | 41.32 | 36.87 | 46.02 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 线性 VPG | 16.43 | 19.48 | 14.75 | 18.54 | 41.32 | 36.87 | 46.02 |'
- en: '| 8 | VPG-C-LLaMA2-7B | 42.70 | 24.76 | 25.50 | 22.95 | 51.00 | 44.93 | 48.68
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 8 | VPG-C-LLaMA2-7B | 42.70 | 24.76 | 25.50 | 22.95 | 51.00 | 44.93 | 48.68
    |'
- en: '| 9 | VPG-C-Vicuna-13B | 38.14 | 26.59 | 27.15 | 27.15 | 52.93 | 49.33 | 53.65
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 9 | VPG-C-Vicuna-13B | 38.14 | 26.59 | 27.15 | 27.15 | 52.93 | 49.33 | 53.65
    |'
- en: VPG-C works well on various language backbones. Table [4](#S4.T4 "Table 4 ‣
    4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow
    Zero-shot Demonstrative Instructions") also validates that our approach can well
    cooperate with language backbones of different families (LLaMA2) and sizes (Vicuna-13B).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: VPG-C 在各种语言模型基础上表现良好。表格 [4](#S4.T4 "Table 4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments
    ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")
    也验证了我们的方法可以很好地与不同家族（LLaMA2）和大小（Vicuna-13B）的语言模型基础协作。
- en: VPG-C can be implemented with very simple VPG. As a generic method, VPG-C can
    be implemented with different VPGs. Beyond the widely used Q-former that is composed
    of multiple Transformer blocks, we further probe the effectiveness of VPG-C with
    a simpler VPG, i.e., Linear Projection, as used in LLaVA (please refer to Appendix [C](#A3
    "Appendix C Implementation Details ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot
    Demonstrative Instructions") for implementation details). Table [4](#S4.T4 "Table
    4 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow
    Zero-shot Demonstrative Instructions") Row 7 shows promising results. VPG-C can
    also significantly bolster the performance of a simple linear VPG, verifying the
    transferability of VPG-C. It is promising to adapt our generic VPG-C and corresponding
    low-resource synthetic training strategy to different VPGs in the future.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: VPG-C 可以用非常简单的 VPG 实现。作为一种通用方法，VPG-C 可以与不同的 VPG 结合使用。除了由多个 Transformer 模块组成的广泛使用的
    Q-former，我们进一步探讨了使用更简单的 VPG，即线性投影（Linear Projection）的 VPG-C 的有效性，如 LLaVA 中所用（有关实现细节，请参见附录
    [C](#A3 "Appendix C Implementation Details ‣ Fine-tuning Multimodal LLMs to Follow
    Zero-shot Demonstrative Instructions")）。表格 [4](#S4.T4 "Table 4 ‣ 4.4 In-Depth
    Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions") 的第 7 行显示了有希望的结果。VPG-C 还可以显著提升简单线性 VPG 的性能，验证了 VPG-C 的可迁移性。未来将我们的通用
    VPG-C 和相应的低资源合成训练策略适应于不同的 VPG 是很有前景的。
- en: '![Refer to caption](img/e5c074564d846148c12eb57071a4486f.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5c074564d846148c12eb57071a4486f.png)'
- en: 'Figure 6: Performance on DEMON with different insertion layers.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 DEMON 上不同插入层的性能。
- en: Analysis on the inserted layer of VPG-C. We investigate the impact of inserting
    VPG-C into different layers of LLMs. We report the averaged accuracy for multiple-choice
    tasks and averaged ROUGE-L for open-ended generation tasks in Figure [6](#S4.F6
    "Figure 6 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions"). We observe that the performance
    is low when we insert VPG-C too early (i.e., 4, 8) as the model might not have
    gathered sufficient contextual information to generate effective guidance. Meanwhile,
    inserting VPG-C too late (i.e., 24, 28) degenerates the performance. We speculate
    this is due to the generated guidance being too concentrated and there not being
    enough layers to integrate the residual details.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对VPG-C插入层的分析。我们研究了将VPG-C插入到LLM的不同层中的影响。我们在图[6](#S4.F6 "图6 ‣ 4.4 深入分析 ‣ 4 实验
    ‣ 微调多模态LLM以遵循零样本示范指令")中报告了多项选择任务的平均准确度和开放式生成任务的平均ROUGE-L。我们观察到，当我们过早地插入VPG-C（即4、8层）时，模型可能尚未收集到足够的上下文信息以生成有效的指导，因此性能较低。同时，过晚地插入VPG-C（即24、28层）会使性能下降。我们推测这是由于生成的指导过于集中，而层数不足以整合剩余的细节。
- en: 'Table 5: Efficiency analysis of synthetic training.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：合成训练的效率分析。
- en: '|  | Accuracy | ROUGE-L |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确度 | ROUGE-L |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 16K | 38.93 | 25.67 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 16K | 38.93 | 25.67 |'
- en: '| 32K | 39.62 | 27.38 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 32K | 39.62 | 27.38 |'
- en: '| 48K | 40.45 | 28.81 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 48K | 40.45 | 28.81 |'
- en: '| 64K | 41.49 | 29.53 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 64K | 41.49 | 29.53 |'
- en: '| 80K | 41.62 | 29.73 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 80K | 41.62 | 29.73 |'
- en: '| 96K | 40.12 | 28.31 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 96K | 40.12 | 28.31 |'
- en: Synthetic training is data-efficient. Since our proposed synthetic training
    strategy can construct challenging discriminative tasks in a targeted manner,
    enhancing VPG-C’s ability to complete missing details, it avoids the need for
    a large amount of supervised demonstrative instruction data. We further investigate
    the impact of different numbers of synthetic training data. As illustrated in
    Table [5](#S4.T5 "Table 5 ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"), the performance
    keeps increasing when the number of data is increased from 16K to 64K. Beyond
    this, escalating the data count from 64K to 80K yields only marginal enhancement.
    Further amplification of data eventually triggers a performance dip as excessive
    data leads to model overfitting to the synthetic training task.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 合成训练数据高效。由于我们提出的合成训练策略能够有针对性地构建具有挑战性的区分任务，提高VPG-C完成缺失细节的能力，因此避免了大量监督示范指令数据的需求。我们进一步研究了不同数量的合成训练数据的影响。如表[5](#S4.T5
    "表5 ‣ 4.4 深入分析 ‣ 4 实验 ‣ 微调多模态LLM以遵循零样本示范指令")所示，当数据量从16K增加到64K时，性能不断提高。超过此范围，将数据量从64K增加到80K只会带来边际提升。进一步增加数据最终会引发性能下降，因为过多的数据会导致模型对合成训练任务的过拟合。
- en: Image order sensitivity. The order of interleaved images in demonstrative instructions
    is pivotal for the compositional semantics of the instruction. Intuitively, altering
    the order of images within a demonstrative instruction can significantly shift
    its semantics. Consequently, variations in model performance can reveal the model’s
    sensitivity to the instruction semantics. An ideal model should keenly capture
    changes in instruction semantics. Therefore, we visualize the performance variations
    of models by randomly shuffling the order of interleaved images within the demonstrative
    instructions. According to Figure [7](#S4.F7 "Figure 7 ‣ 4.4 In-Depth Analysis
    ‣ 4 Experiments ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions"), we surprisingly find that SOTA models are less sensitive to the
    image order. In contrast, VPG-C can keenly capture the semantic changes caused
    by the shuffled image order. Particularly, our performance varies dramatically
    in multimodal dialogue, as the order of images within these tasks is closely intertwined
    with the dialogue content.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图像顺序敏感性。示范指令中交错图像的顺序对指令的组合语义至关重要。直观上，改变示范指令中图像的顺序可以显著改变其语义。因此，模型性能的变化可以揭示模型对指令语义的敏感性。理想的模型应该能够敏锐地捕捉到指令语义的变化。因此，我们通过随机打乱示范指令中交错图像的顺序来可视化模型性能的变化。根据图[7](#S4.F7
    "图7 ‣ 4.4 深入分析 ‣ 4 实验 ‣ 微调多模态LLM以遵循零样本示范指令")，我们惊讶地发现，SOTA模型对图像顺序的敏感性较低。相比之下，VPG-C能够敏锐地捕捉到由于图像顺序打乱所造成的语义变化。特别是，在多模态对话中，我们的性能变化非常剧烈，因为这些任务中的图像顺序与对话内容紧密相连。
- en: '![Refer to caption](img/caf4e1fd13873008a4a12360b8e62702.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/caf4e1fd13873008a4a12360b8e62702.png)'
- en: 'Figure 7: Analysis on image order sensitivity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：图像顺序敏感性的分析。
- en: Qualitative examples. As illustrated in Figure [8](#S5.F8 "Figure 8 ‣ 5 Related
    Work ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"),
    VPG-C demonstrates strong abilities to perform reasoning over complicated demonstrative
    instructions. For instance, in (a), VPG-C can keenly identify the connections
    between the images and thereby infer the reason that causes this unusual phenomenon.
    In (b, c), VPG-C exhibits the ability to comprehend absurd objects through multimodal
    conversations with humans. In (d, e), VPG-C can reasonably infer the relations
    among the images and understand the metaphorical implications they want to convey.
    In Appendix [G](#A7 "Appendix G Qualitative Comparison ‣ Fine-tuning Multimodal
    LLMs to Follow Zero-shot Demonstrative Instructions"), we provide more practical
    examples as well as comparisons with other MLLMs, where we find that baseline
    models fail to correctly associate multiple images and comprehend demonstrative
    context.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 质性示例。如图 [8](#S5.F8 "图 8 ‣ 5 相关工作 ‣ 微调多模态LLMs以遵循零-shot示范指令")所示，VPG-C 展示了在复杂示范指令上的强大推理能力。例如，在
    (a) 中，VPG-C 能敏锐地识别图像之间的联系，从而推断出导致这一异常现象的原因。在 (b, c) 中，VPG-C 展现了通过与人类的多模态对话理解荒诞物体的能力。在
    (d, e) 中，VPG-C 能合理推断图像之间的关系并理解它们想要传达的隐喻含义。在附录 [G](#A7 "附录 G 质性比较 ‣ 微调多模态LLMs以遵循零-shot示范指令")
    中，我们提供了更多实际示例以及与其他MLLM的比较，我们发现基线模型未能正确关联多个图像并理解示范上下文。
- en: 5 Related Work
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: MLLMs (Yin et al., [2023](#bib.bib61)) aim to serve as a general-purpose assistant
    to perform various vision-language tasks by free-text generation. Flamingo (Alayrac
    et al., [2022](#bib.bib1)) and BLIP-2 (Li et al., [2023b](#bib.bib32)) bridge
    LLMs with powerful pre-trained visual encoders and demonstrate strong zero-shot
    ability by aligning visual features with LLMs. Follow-up works of LLaVA (Liu et al.,
    [2023](#bib.bib36)), MiniGPT-4 (Zhu et al., [2023a](#bib.bib63)), InstructBLIP (Dai
    et al., [2023](#bib.bib12)), mPLUG-Owl (Ye et al., [2023](#bib.bib60)) propose
    to fine-tune MLLMs with multimodal instruction tuning data. To effectively benchmark
    the recent progress in MLLMs, concurrent works of LVLM-eHub (Xu et al., [2023](#bib.bib58))
    and MME Benchmark (Fu et al., [2023](#bib.bib15)) are proposed, while they mainly
    focus on instructions that only involve a single image with limited instruction
    diversity. In this paper, we propose the first demonstrative instruction-following
    benchmark, covering various tasks of diverse scenarios. Further, we propose a
    lightweight and generic VPG-C module to address the inherent limitation of current
    VPGs. Our VPG-C is efficiently tuned by our synthetic discriminative training
    strategy, which demonstrates powerful potentials of text-to-image diffusion models (He
    et al., [2022](#bib.bib21); Lin et al., [2023](#bib.bib35); Prabhu et al., [2023](#bib.bib42);
    Bansal & Grover, [2023](#bib.bib6)) to facilitate vision-language understanding (Radford
    et al., [2021b](#bib.bib44)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MLLMs (Yin et al., [2023](#bib.bib61)) 旨在作为通用助手，通过自由文本生成执行各种视觉-语言任务。Flamingo
    (Alayrac et al., [2022](#bib.bib1)) 和 BLIP-2 (Li et al., [2023b](#bib.bib32))
    通过将强大的预训练视觉编码器与LLMs对接，展示了强大的零-shot能力。LLaVA (Liu et al., [2023](#bib.bib36))、MiniGPT-4
    (Zhu et al., [2023a](#bib.bib63))、InstructBLIP (Dai et al., [2023](#bib.bib12))、mPLUG-Owl
    (Ye et al., [2023](#bib.bib60)) 的后续工作提出了用多模态指令调优数据微调MLLMs。为了有效基准测试MLLM的近期进展，LVLM-eHub
    (Xu et al., [2023](#bib.bib58)) 和 MME Benchmark (Fu et al., [2023](#bib.bib15))
    的并行工作被提出，但它们主要关注仅涉及单张图像且指令多样性有限的指令。在本文中，我们提出了第一个示范指令跟随基准，涵盖了各种多样化场景的任务。此外，我们提出了一个轻量级和通用的
    VPG-C 模块，以解决当前 VPG 的固有局限性。我们的 VPG-C 通过我们合成的判别训练策略进行高效调优，这展示了文本到图像扩散模型 (He et al.,
    [2022](#bib.bib21); Lin et al., [2023](#bib.bib35); Prabhu et al., [2023](#bib.bib42);
    Bansal & Grover, [2023](#bib.bib6)) 在促进视觉-语言理解方面的强大潜力 (Radford et al., [2021b](#bib.bib44))。
- en: '![Refer to caption](img/e414ff78587f64027062029cc550172a.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e414ff78587f64027062029cc550172a.png)'
- en: 'Figure 8: Qualitative examples generated by our VPG-C-Vicuna-7B model.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：由我们的 VPG-C-Vicuna-7B 模型生成的质性示例。
- en: 6 Conclusion
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we propose VPG-C, a generic and parameter-efficient approach
    that infers and completes the missing visual details for MLLMs to comprehend demonstrative
    instructions with interleaved multimodal context. Meanwhile, we present a synthetic
    discriminative training strategy to fine-tune VPG-C, eliminating the need for
    supervised demonstrative instruction data. To foster the research on demonstrative
    instruction understanding, we build DEMON, a comprehensive benchmark for multimodal
    large language models, consisting of 31 tasks with complicated vision-language
    demonstrative context, covering a wide range of scenarios. Through synthetic training,
    VPG-C showcases notable zero-shot performance on the DEMON benchmark. Its superior
    performance on other established benchmarks like MME and OwlEval further underscores
    its effectiveness.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了VPG-C，这是一种通用且参数高效的方法，用于推断和补全缺失的视觉细节，以便MLLMs能够理解带有交错多模态上下文的演示性指令。同时，我们提出了一种合成判别训练策略来微调VPG-C，消除了对监督演示性指令数据的需求。为了促进对演示性指令理解的研究，我们构建了DEMON，这是一个全面的多模态大语言模型基准，包含31个复杂视觉-语言演示性上下文任务，覆盖了广泛的场景。通过合成训练，VPG-C在DEMON基准上展示了显著的零-shot性能。它在MME和OwlEval等其他已建立基准上的优异表现进一步凸显了其有效性。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.
    *Advances in Neural Information Processing Systems*, 35:23716–23736, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alayrac等（2022）Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech,
    Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm
    Reynolds, 等。Flamingo: 一种用于少-shot学习的视觉语言模型。*Advances in Neural Information Processing
    Systems*, 35:23716–23736, 2022。'
- en: Avrahami et al. (2022) Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
    diffusion for text-driven editing of natural images. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  18208–18218, 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Avrahami等（2022）Omri Avrahami, Dani Lischinski, 和 Ohad Fried。用于自然图像文本驱动编辑的混合扩散。在*Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 18208–18218,
    2022。
- en: Awadalla et al. (2023) Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,
    Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia
    Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and
    Ludwig Schmidt. Openflamingo, March 2023. URL [https://doi.org/10.5281/zenodo.7733589](https://doi.org/10.5281/zenodo.7733589).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awadalla等（2023）Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf
    Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev,
    Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, 和 Ludwig Schmidt。Openflamingo,
    2023年3月。网址 [https://doi.org/10.5281/zenodo.7733589](https://doi.org/10.5281/zenodo.7733589)。
- en: 'Bai et al. (2023) Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk
    Cinbis, Oncel Tuzel, Ping Huang, Jiulong Shan, Jianjun Shi, and Meng Cao. Vision
    datasets: A benchmark for vision-based industrial inspection. *arXiv preprint
    arXiv:2306.07890*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等（2023）Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis,
    Oncel Tuzel, Ping Huang, Jiulong Shan, Jianjun Shi, 和 Meng Cao。视觉数据集：一种用于基于视觉的工业检查的基准。*arXiv
    preprint arXiv:2306.07890*, 2023。
- en: 'Bansal et al. (2020) Ankan Bansal, Yuting Zhang, and Rama Chellappa. Visual
    question answering on image sets. In *Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16*, pp.  51–67.
    Springer, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bansal等（2020）Ankan Bansal, Yuting Zhang, 和 Rama Chellappa。图像集上的视觉问题回答。在*Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XXI 16*, pp. 51–67. Springer, 2020。'
- en: 'Bansal & Grover (2023) Hritik Bansal and Aditya Grover. Leaving reality to
    imagination: Robust classification via generated datasets. *arXiv preprint arXiv:2302.02503*,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal & Grover（2023）Hritik Bansal 和 Aditya Grover。将现实留给想象：通过生成数据集进行鲁棒分类。*arXiv
    preprint arXiv:2302.02503*, 2023。
- en: Bhattacharya et al. (2019) Nilavra Bhattacharya, Qing Li, and Danna Gurari.
    Why does a visual question have different answers? In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, pp.  4271–4280, 2019.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhattacharya等（2019）Nilavra Bhattacharya, Qing Li, 和 Danna Gurari。为什么一个视觉问题会有不同的答案？在*Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pp. 4271–4280, 2019。
- en: 'Caesar et al. (2020) Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
    Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar
    Beijbom. nuscenes: A multimodal dataset for autonomous driving. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  11621–11631,
    2020.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caesar 等（2020）**Holger Caesar**、**Varun Bankiti**、**Alex H Lang**、**Sourabh
    Vora**、**Venice Erin Liong**、**Qiang Xu**、**Anush Krishnan**、**Yu Pan**、**Giancarlo
    Baldan** 和 **Oscar Beijbom**。nuscenes: 一种用于自动驾驶的多模态数据集。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第11621–11631页，2020年。'
- en: 'Chang et al. (2022) Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao,
    Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  16495–16504,
    2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等（2022）**Yingshan Chang**、**Mridu Narang**、**Hisami Suzuki**、**Guihong
    Cao**、**Jianfeng Gao** 和 **Yonatan Bisk**。Webqa: 多跳和多模态问答。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第16495–16504页，2022年。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality,
    2023. URL [https://vicuna.lmsys.org](https://vicuna.lmsys.org).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等（2023）**Wei-Lin Chiang**、**Zhuohan Li**、**Zi Lin**、**Ying Sheng**、**Zhanghao
    Wu**、**Hao Zhang**、**Lianmin Zheng**、**Siyuan Zhuang**、**Yonghao Zhuang**、**Joseph
    E Gonzalez** 等。Vicuna: 一款开源聊天机器人，以90%的ChatGPT质量打动GPT-4，2023年。网址 [https://vicuna.lmsys.org](https://vicuna.lmsys.org)。'
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2022）**Hyung Won Chung**、**Le Hou**、**Shayne Longpre**、**Barret Zoph**、**Yi
    Tay**、**William Fedus**、**Eric Li**、**Xuezhi Wang**、**Mostafa Dehghani**、**Siddhartha
    Brahma** 等。扩展指令微调语言模型。*arXiv 预印本 arXiv:2210.11416*，2022年。
- en: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning. *arXiv
    preprint arXiv:2305.06500*, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等（2023）**Wenliang Dai**、**Junnan Li**、**Dongxu Li**、**Anthony Meng Huat
    Tiong**、**Junqi Zhao**、**Weisheng Wang**、**Boyang Li**、**Pascale Fung** 和 **Steven
    Hoi**。Instructblip: 迈向通用视觉语言模型的指令调优。*arXiv 预印本 arXiv:2305.06500*，2023年。'
- en: 'Fang et al. (2023) Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang
    Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked
    visual representation learning at scale. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  19358–19369, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang 等（2023）**Yuxin Fang**、**Wen Wang**、**Binhui Xie**、**Quan Sun**、**Ledell
    Wu**、**Xinggang Wang**、**Tiejun Huang**、**Xinlong Wang** 和 **Yue Cao**。Eva: 探索大规模遮蔽视觉表征学习的极限。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第19358–19369页，2023年。'
- en: 'Forbes et al. (2019) Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma,
    and Serge Belongie. Neural naturalist: generating fine-grained image comparisons.
    *arXiv preprint arXiv:1909.04101*, 2019.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Forbes 等（2019）**Maxwell Forbes**、**Christine Kaeser-Chen**、**Piyush Sharma**
    和 **Serge Belongie**。Neural naturalist: 生成精细的图像比较。*arXiv 预印本 arXiv:1909.04101*，2019年。'
- en: 'Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
    Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive
    evaluation benchmark for multimodal large language models. *arXiv preprint arXiv:2306.13394*,
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等（2023）**Chaoyou Fu**、**Peixian Chen**、**Yunhang Shen**、**Yulei Qin**、**Mengdan
    Zhang**、**Xu Lin**、**Zhenyu Qiu**、**Wei Lin**、**Jinrui Yang**、**Xiawu Zheng**
    等。Mme: 一种用于多模态大语言模型的全面评估基准。*arXiv 预印本 arXiv:2306.13394*，2023年。'
- en: 'Gao et al. (2023) Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2:
    Parameter-efficient visual instruction model. *arXiv preprint arXiv:2304.15010*,
    2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等（2023）**Peng Gao**、**Jiaming Han**、**Renrui Zhang**、**Ziyi Lin**、**Shijie
    Geng**、**Aojun Zhou**、**Wei Zhang**、**Pan Lu**、**Conghui He**、**Xiangyu Yue**
    等。Llama-adapter v2: 参数高效的视觉指令模型。*arXiv 预印本 arXiv:2304.15010*，2023年。'
- en: Gupta et al. (2018) Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem,
    and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In *Proceedings
    of the European conference on computer vision (ECCV)*, pp.  598–613, 2018.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2018）**Tanmay Gupta**、**Dustin Schwenk**、**Ali Farhadi**、**Derek Hoiem**
    和 **Aniruddha Kembhavi**。Imagine this! 从脚本到合成再到视频。在 *欧洲计算机视觉会议（ECCV）论文集*，第598–613页，2018年。
- en: Han et al. (2017) Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong
    Zhu, Yuan Li, Yang Zhao, and Larry S Davis. Automatic spatially-aware fashion
    concept discovery. In *Proceedings of the IEEE international conference on computer
    vision*, pp.  1463–1471, 2017.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等（2017）**Xintong Han**、**Zuxuan Wu**、**Phoenix X Huang**、**Xiao Zhang**、**Menglong
    Zhu**、**Yuan Li**、**Yang Zhao** 和 **Larry S Davis**。自动空间感知时尚概念发现。在 *IEEE 国际计算机视觉会议论文集*，第1463–1471页，2017年。
- en: 'Hannan et al. (2020) Darryl Hannan, Akshay Jain, and Mohit Bansal. Manymodalqa:
    Modality disambiguation and qa over diverse inputs. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 34, pp.  7879–7886, 2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hannan 等（2020）Darryl Hannan, Akshay Jain, 和 Mohit Bansal. Manymodalqa: 多模态歧义消解和多样化输入的问答。在
    *AAAI 人工智能会议论文集*，第34卷，第7879–7886页，2020年。'
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*,
    pp.  2961–2969, 2017.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2017）Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick. Mask
    R-CNN。在 *IEEE 国际计算机视觉会议论文集*，第2961–2969页，2017年。
- en: He et al. (2022) Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang,
    Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models
    ready for image recognition? *arXiv preprint arXiv:2210.07574*, 2022.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2022）Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip
    Torr, Song Bai, 和 Xiaojuan Qi. 生成模型的合成数据是否准备好用于图像识别？*arXiv 预印本 arXiv:2210.07574*，2022年。
- en: Hosseinzadeh & Wang (2021) Mehrdad Hosseinzadeh and Yang Wang. Image change
    captioning by learning from an auxiliary task. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  2725–2734, 2021.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseinzadeh & Wang（2021）Mehrdad Hosseinzadeh 和 Yang Wang. 通过从辅助任务中学习进行图像变化描述。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第2725–2734页，2021年。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021年。'
- en: 'Huang et al. (2016) Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan
    Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli,
    Dhruv Batra, et al. Visual storytelling. In *Proceedings of the 2016 conference
    of the North American chapter of the association for computational linguistics:
    Human language technologies*, pp.  1233–1239, 2016.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2016）Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra,
    Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv
    Batra 等. 视觉讲故事。在 *2016年北美计算语言学协会人类语言技术会议论文集*，第1233–1239页，2016年。
- en: Isola et al. (2015) Phillip Isola, Joseph J Lim, and Edward H Adelson. Discovering
    states and transformations in image collections. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pp.  1383–1391, 2015.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等（2015）Phillip Isola, Joseph J Lim, 和 Edward H Adelson. 发现图像集合中的状态和转换。在
    *IEEE 计算机视觉与模式识别会议论文集*，第1383–1391页，2015年。
- en: 'Iyyer et al. (2017) Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas,
    Jordan Boyd-Graber, Hal Daume, and Larry S Davis. The amazing mysteries of the
    gutter: Drawing inferences between panels in comic book narratives. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern recognition*, pp.  7186–7195,
    2017.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyyer 等（2017）Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan
    Boyd-Graber, Hal Daume, 和 Larry S Davis. 沟槽的惊人秘密：在漫画叙事中进行面板之间的推断。在 *IEEE 计算机视觉与模式识别会议论文集*，第7186–7195页，2017年。
- en: Jhamtani & Berg-Kirkpatrick (2018) Harsh Jhamtani and Taylor Berg-Kirkpatrick.
    Learning to describe differences between pairs of similar images. *arXiv preprint
    arXiv:1808.10584*, 2018.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhamtani & Berg-Kirkpatrick（2018）Harsh Jhamtani 和 Taylor Berg-Kirkpatrick. 学习描述相似图像对之间的差异。*arXiv
    预印本 arXiv:1808.10584*，2018年。
- en: Kembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun
    Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader?
    textbook question answering for multimodal machine comprehension. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern recognition*, pp.  4999–5007,
    2017.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kembhavi 等（2017）Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi,
    Ali Farhadi, 和 Hannaneh Hajishirzi. 你比六年级学生更聪明吗？多模态机器理解的教科书问题回答。在 *IEEE 计算机视觉与模式识别会议论文集*，第4999–5007页，2017年。
- en: Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
    Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C
    Berg, Wan-Yen Lo, et al. Segment anything. *arXiv preprint arXiv:2304.02643*,
    2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov 等（2023）Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe
    Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen
    Lo 等. 随意分割。*arXiv 预印本 arXiv:2304.02643*，2023年。
- en: 'Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu,
    Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction
    tuning. *arXiv preprint arXiv:2306.05425*, 2023a.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang
    Yang, Chunyuan Li, 和 Ziwei Liu. Mimic-it: 多模态上下文内指令调优。*arXiv 预印本 arXiv:2306.05425*，2023a。'
- en: 'Li et al. (2022a) Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese,
    and Steven CH Hoi. Lavis: A library for language-vision intelligence. *arXiv preprint
    arXiv:2209.09019*, 2022a.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2022a) Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese,
    和 Steven CH Hoi. Lavis: 一种语言-视觉智能的库。*arXiv 预印本 arXiv:2209.09019*，2022a。'
- en: 'Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. *arXiv preprint arXiv:2301.12597*, 2023b.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023b) Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi. Blip-2: 利用冻结的图像编码器和大型语言模型进行语言-图像预训练。*arXiv
    预印本 arXiv:2301.12597*，2023b。'
- en: 'Li et al. (2019) Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin
    Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: A sequential conditional
    gan for story visualization. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp.  6329–6338, 2019.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2019) Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin
    Wu, Lawrence Carin, David Carlson, 和 Jianfeng Gao. Storygan: 用于故事可视化的序列条件生成对抗网络。见于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 6329–6338 页，2019 年。'
- en: 'Li et al. (2022b) Yongqi Li, Wenjie Li, and Liqiang Nie. Mmcoqa: Conversational
    question answering over text, tables, and images. In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pp.  4220–4231, 2022b.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2022b) Yongqi Li, Wenjie Li, 和 Liqiang Nie. Mmcoqa: 针对文本、表格和图像的对话式问答。见于
    *第 60 届计算语言学协会年会论文集（第 1 卷: 长篇论文）*，第 4220–4231 页，2022b。'
- en: Lin et al. (2023) Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Explore the
    power of synthetic data on few-shot object detection. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pp.  638–647, 2023.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2023) Shaobo Lin, Kun Wang, Xingyu Zeng, 和 Rui Zhao. 探索合成数据在少样本目标检测中的威力。见于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 638–647 页，2023。
- en: Liu et al. (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning. *arXiv preprint arXiv:2304.08485*, 2023.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令调优。*arXiv
    预印本 arXiv:2304.08485*，2023。
- en: 'Maharana et al. (2022) Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e:
    Adapting pretrained text-to-image transformers for story continuation. In *Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXVII*, pp.  70–87. Springer, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maharana 等 (2022) Adyasha Maharana, Darryl Hannan, 和 Mohit Bansal. Storydall-e:
    适应预训练文本到图像的变换器以实现故事延续。见于 *计算机视觉–ECCV 2022: 第 17 届欧洲会议，特拉维夫，以色列，2022 年 10 月 23–27
    日，论文集，第三十七部分*，第 70–87 页。Springer，2022 年。'
- en: 'Mathew et al. (2021) Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa:
    A dataset for vqa on document images. In *Proceedings of the IEEE/CVF winter conference
    on applications of computer vision*, pp.  2200–2209, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mathew 等 (2021) Minesh Mathew, Dimosthenis Karatzas, 和 CV Jawahar. Docvqa:
    一个用于文档图像的视觉问答数据集。见于 *IEEE/CVF 计算机视觉应用冬季会议论文集*，第 2200–2209 页，2021 年。'
- en: 'Mishra et al. (2019) Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
    Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images.
    In *2019 international conference on document analysis and recognition (ICDAR)*,
    pp.  947–952\. IEEE, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mishra 等 (2019) Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, 和 Anirban
    Chakraborty. OCR-VQA: 通过读取图像中的文本进行视觉问答。见于 *2019 国际文档分析与识别会议 (ICDAR)*，第 947–952
    页。IEEE，2019 年。'
- en: 'OpenAI (2023a) OpenAI. Chatgpt: A language model for conversational ai. Technical
    report, OpenAI, 2023a. URL [https://www.openai.com/research/chatgpt](https://www.openai.com/research/chatgpt).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2023a) OpenAI. ChatGPT: 一个用于对话人工智能的语言模型。技术报告，OpenAI，2023a。网址 [https://www.openai.com/research/chatgpt](https://www.openai.com/research/chatgpt)。'
- en: OpenAI (2023b) OpenAI. Gpt-4 technical report. *arXiv:2303.08774*, 2023b.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. GPT-4 技术报告。*arXiv:2303.08774*，2023b。
- en: 'Prabhu et al. (2023) Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay,
    and Judy Hoffman. Lance: Stress-testing visual models by generating language-guided
    counterfactual images. *arXiv preprint arXiv:2305.19164*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prabhu 等 (2023) Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay,
    和 Judy Hoffman. Lance: 通过生成语言指导的反事实图像对视觉模型进行压力测试。*arXiv 预印本 arXiv:2305.19164*，2023。'
- en: Radford et al. (2021a) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp. 8748–8763\. PMLR, 2021a.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2021a）Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
    Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark
    等。《从自然语言监督中学习可迁移的视觉模型》。见于 *国际机器学习会议*，第 8748–8763 页，PMLR，2021a。
- en: Radford et al. (2021b) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp. 8748–8763\. PMLR, 2021b.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2021b）Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
    Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark
    等。《从自然语言监督中学习可迁移的视觉模型》。见于 *国际机器学习会议*，第 8748–8763 页，PMLR，2021b。
- en: 'Ravi et al. (2021) Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt,
    and Mubbasir Kapadia. Aesop: Abstract encoding of stories, objects, and pictures.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pp.  2052–2063, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravi 等（2021）Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, 和 Mubbasir
    Kapadia。《Aesop：故事、物体和图片的抽象编码》。见于 *IEEE/CVF国际计算机视觉会议论文集*，第 2052–2063 页，2021年。
- en: 'Reimers & Gurevych (2019) Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence
    embeddings using Siamese BERT-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp.  3982–3992,
    2019.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers & Gurevych（2019）Nils Reimers 和 Iryna Gurevych。《Sentence-BERT：使用 Siamese
    BERT 网络的句子嵌入》。见于 *2019年自然语言处理实证方法会议暨第9届国际联合自然语言处理会议（EMNLP-IJCNLP）*，第 3982–3992
    页，2019年。
- en: 'Romero et al. (2014) Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
    Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep
    nets. *arXiv preprint arXiv:1412.6550*, 2014.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romero 等（2014）Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine
    Chassang, Carlo Gatta, 和 Yoshua Bengio。《Fitnets：对薄深网的提示》。*arXiv 预印本 arXiv:1412.6550*，2014年。
- en: 'Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
    Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic
    image captioning. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  2556–2565, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等（2018）Piyush Sharma, Nan Ding, Sebastian Goodman, 和 Radu Soricut。《概念性描述：一个清理过的、超纲的、图像替代文本数据集，用于自动图像描述》。见于
    *第56届计算语言学协会年会论文集（第1卷：长篇论文）*，第 2556–2565 页，2018年。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred:
    A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  10740–10749,
    2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等（2020）Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk,
    Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, 和 Dieter Fox。《Alfred：一个解释面向日常任务的基础指令的基准》。见于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，第 10740–10749 页，2020年。
- en: Suhr et al. (2018) Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun
    Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in
    photographs. *arXiv preprint arXiv:1811.00491*, 2018.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suhr 等（2018）Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai,
    和 Yoav Artzi。《一个用于推理自然语言与照片结合的语料库》。*arXiv 预印本 arXiv:1811.00491*，2018年。
- en: 'Talmor et al. (2021) Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong
    Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa:
    Complex question answering over text, tables and images. *arXiv preprint arXiv:2104.06039*,
    2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor 等（2021）Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang,
    Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, 和 Jonathan Berant。《Multimodalqa：跨文本、表格和图像的复杂问答》。*arXiv
    预印本 arXiv:2104.06039*，2021年。
- en: Tan et al. (2019) Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit
    Bansal. Expressing visual relationships via language. *arXiv preprint arXiv:1906.07689*,
    2019.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 等（2019）Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, 和 Mohit Bansal。《通过语言表达视觉关系》。*arXiv
    预印本 arXiv:1906.07689*，2019年。
- en: 'Tanaka et al. (2023) Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa,
    Itsumi Saito, and Kuniko Saito. Slidevqa: A dataset for document visual question
    answering on multiple images. *arXiv preprint arXiv:2301.04883*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka 等（2023）Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa,
    Itsumi Saito, 和 Kuniko Saito。《Slidevqa：一个多图像文档视觉问答数据集》。*arXiv 预印本 arXiv:2301.04883*，2023年。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv:2302.13971*, 2023a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023a） Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama: 开放且高效的基础语言模型。*arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *arXiv:2307.09288*, 2023b.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023b） Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。Llama 2: 开放基础和微调的聊天模型。*arXiv:2307.09288*，2023b。'
- en: 'Xia et al. (2018) Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra
    Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pp.  9068–9079, 2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等（2018） Fei Xia、Amir R Zamir、Zhiyang He、Alexander Sax、Jitendra Malik 和
    Silvio Savarese。Gibson env: 体化代理的现实世界感知。在*IEEE 计算机视觉与模式识别会议论文集*中，第 9068–9079 页，2018。'
- en: 'Xin et al. (2020) Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy
    Lin. Deebert: Dynamic early exiting for accelerating bert inference. *arXiv preprint
    arXiv:2004.12993*, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xin 等（2020） Ji Xin、Raphael Tang、Jaejun Lee、Yaoliang Yu 和 Jimmy Lin。Deebert:
    加速 BERT 推理的动态早期退出。*arXiv 预印本 arXiv:2004.12993*，2020。'
- en: 'Xu et al. (2023) Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng
    Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive
    evaluation benchmark for large vision-language models. *arXiv preprint arXiv:2306.09265*,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等（2023） Peng Xu、Wenqi Shao、Kaipeng Zhang、Peng Gao、Shuo Liu、Meng Lei、Fanqing
    Meng、Siyuan Huang、Yu Qiao 和 Ping Luo。Lvlm-ehub: 大型视觉语言模型的综合评估基准。*arXiv 预印本 arXiv:2306.09265*，2023。'
- en: 'Yagcioglu et al. (2018) Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli
    Ikizler-Cinbis. Recipeqa: A challenge dataset for multimodal comprehension of
    cooking recipes. *arXiv preprint arXiv:1809.00812*, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yagcioglu 等（2018） Semih Yagcioglu、Aykut Erdem、Erkut Erdem 和 Nazli Ikizler-Cinbis。Recipeqa:
    多模态理解烹饪食谱的挑战数据集。*arXiv 预印本 arXiv:1809.00812*，2018。'
- en: 'Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization
    empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*,
    2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等（2023） Qinghao Ye、Haiyang Xu、Guohai Xu、Jiabo Ye、Ming Yan、Yiyang Zhou、Junyang
    Wang、Anwen Hu、Pengcheng Shi、Yaya Shi 等。mplug-owl: 模块化赋能大语言模型的多模态能力。*arXiv 预印本
    arXiv:2304.14178*，2023。'
- en: Yin et al. (2023) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models. *arXiv preprint
    arXiv:2306.13549*, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2023） Shukang Yin、Chaoyou Fu、Sirui Zhao、Ke Li、Xing Sun、Tong Xu 和 Enhong
    Chen。关于多模态大语言模型的调查。*arXiv 预印本 arXiv:2306.13549*，2023。
- en: Zhang & Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. Adding conditional
    control to text-to-image diffusion models. *arXiv preprint arXiv:2302.05543*,
    2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang & Agrawala（2023） Lvmin Zhang 和 Maneesh Agrawala。为文本到图像扩散模型添加条件控制。*arXiv
    预印本 arXiv:2302.05543*，2023。
- en: 'Zhu et al. (2023a) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023a.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023a) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, 和 Mohamed
    Elhoseiny. Minigpt-4: 通过先进的大型语言模型增强视觉语言理解。*arXiv 预印本 arXiv:2304.10592*, 2023a。'
- en: 'Zhu et al. (2023b) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre,
    Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin
    Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with
    text. *arXiv preprint arXiv:2304.06939*, 2023b.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023b) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre,
    Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, 和 Yejin
    Choi. Multimodal c4: 一个开放的、十亿规模的图像与文本交织的语料库。*arXiv 预印本 arXiv:2304.06939*, 2023b。'
- en: Appendix A Overview
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 概述
- en: 'In this appendix we present:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中我们展示：
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detailed information of the proposed DEMON benchmark (Section [B](#A2 "Appendix
    B Benchmark Details ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions")).
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出的 DEMON 基准的详细信息（第[B](#A2 "附录 B 基准细节 ‣ 微调多模态 LLM 以跟随零-shot 演示指令")节）。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More implementation details of our VPG-C (Section [C](#A3 "Appendix C Implementation
    Details ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")).
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的 VPG-C 的更多实现细节（第[C](#A3 "附录 C 实现细节 ‣ 微调多模态 LLM 以跟随零-shot 演示指令")节）。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementation details of existing MLLMs on the DEMON benchmark (Section [D](#A4
    "Appendix D Model Details in DEMON Benchmark ‣ Fine-tuning Multimodal LLMs to
    Follow Zero-shot Demonstrative Instructions")).
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有 MLLMs 在 DEMON 基准上的实现细节（第[D](#A4 "附录 D DEMON 基准中的模型细节 ‣ 微调多模态 LLM 以跟随零-shot
    演示指令")节）。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detailed zero-shot performance on MME benchmark (Section [E](#A5 "Appendix E
    Detailed Zero-Shot Performance on MME Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions")).
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 MME 基准上的详细零-shot 性能（第[E](#A5 "附录 E 在 MME 基准上的详细零-shot 性能 ‣ 微调多模态 LLM 以跟随零-shot
    演示指令")节）。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detailed zero-shot performance on DEMON benchmark (Section [F](#A6 "Appendix
    F Detailed Zero-Shot Performance on DEMON Benchmark ‣ Fine-tuning Multimodal LLMs
    to Follow Zero-shot Demonstrative Instructions")).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 DEMON 基准上的详细零-shot 性能（第[F](#A6 "附录 F 在 DEMON 基准上的详细零-shot 性能 ‣ 微调多模态 LLM 以跟随零-shot
    演示指令")节）。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Qualitative comparison with existing MLLMs (Section [G](#A7 "Appendix G Qualitative
    Comparison ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions")).
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有 MLLMs 的定性比较（第[G](#A7 "附录 G 定性比较 ‣ 微调多模态 LLM 以跟随零-shot 演示指令")节）。
- en: Appendix B Benchmark Details
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 基准细节
- en: '| Task | Scenario | Dataset | Metirc |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 场景 | 数据集 | 指标 |'
- en: '| Multimodal Dialogue |  |  |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 多模态对话 |  |  |  |'
- en: '| Conversational Embodied Dialogue | Embodied | ALFRED (Shridhar et al., [2020](#bib.bib49))
    | ROUGE-L |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 会话体现对话 | 体现 | ALFRED (Shridhar et al., [2020](#bib.bib49)) | ROUGE-L |'
- en: '| Multimodal Dialogue | Conversation | MMCoQA (Li et al., [2022b](#bib.bib34))
    | ROUGE-L |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 多模态对话 | 对话 | MMCoQA (Li et al., [2022b](#bib.bib34)) | ROUGE-L |'
- en: '| Visual Storytelling |  |  |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 视觉讲故事 |  |  |  |'
- en: '| Animated Story Completion | Cartoon | AESOP (Ravi et al., [2021](#bib.bib45))
    | ROUGE-L |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 动画故事完成 | 卡通 | AESOP (Ravi et al., [2021](#bib.bib45)) | ROUGE-L |'
- en: '| Animated Story Completion | Cartoon | PororoSV (Li et al., [2019](#bib.bib33))
    | ROUGE-L |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 动画故事完成 | 卡通 | PororoSV (Li et al., [2019](#bib.bib33)) | ROUGE-L |'
- en: '| Animated Story Completion | Cartoon | FlintstonesSV (Gupta et al., [2018](#bib.bib17))
    | ROUGE-L |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 动画故事完成 | 卡通 | FlintstonesSV (Gupta et al., [2018](#bib.bib17)) | ROUGE-L
    |'
- en: '| Sequential Photo Storytelling | Album | VIST (Huang et al., [2016](#bib.bib24))
    | ROUGE-L |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 顺序照片讲故事 | 相册 | VIST (Huang et al., [2016](#bib.bib24)) | ROUGE-L |'
- en: '| Sequential Photo Storytelling | Cartoon | DiDeMoSV (Maharana et al., [2022](#bib.bib37))
    | ROUGE-L |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 顺序照片讲故事 | 卡通 | DiDeMoSV (Maharana et al., [2022](#bib.bib37)) | ROUGE-L |'
- en: '| Visual Relation Inference |  |  |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 视觉关系推理 |  |  |  |'
- en: '| Visual Change Captioning | Surveillance | Spot-the-Diff (Jhamtani & Berg-Kirkpatrick,
    [2018](#bib.bib27)) | ROUGE-L |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 视觉变化描述 | 监控 | Spot-the-Diff (Jhamtani & Berg-Kirkpatrick, [2018](#bib.bib27))
    | ROUGE-L |'
- en: '| Visual Change Captioning | Synthetic | CLEVR-Change (Hosseinzadeh & Wang,
    [2021](#bib.bib22)) | ROUGE-L |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 视觉变化描述 | 合成 | CLEVR-Change (Hosseinzadeh & Wang, [2021](#bib.bib22)) | ROUGE-L
    |'
- en: '| Visual Relationship Expressing | General | IEdit (Tan et al., [2019](#bib.bib52))
    | ROUGE-L |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 视觉关系表达 | 一般 | IEdit (Tan et al., [2019](#bib.bib52)) | ROUGE-L |'
- en: '| Subtle Difference Expressing | Fine-Grained | Birds-to-Words (Forbes et al.,
    [2019](#bib.bib14)) | ROUGE-L |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 微妙差异表达 | 精细化 | Birds-to-Words (Forbes et al., [2019](#bib.bib14)) | ROUGE-L
    |'
- en: '| Multimodal Cloze |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 多模态填空 |  |  |  |'
- en: '| Comic Dialogue Identification | Cartoon | COMICS-Dialogue (Iyyer et al.,
    [2017](#bib.bib26)) | Accuracy |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 漫画对话识别 | 卡通 | COMICS-Dialogue (Iyyer et al., [2017](#bib.bib26)) | 准确度 |'
- en: '| Comic Panel Identification | Cartoon | COMICS-Panel (Iyyer et al., [2017](#bib.bib26))
    | Accuracy |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 漫画面板识别 | 卡通 | COMICS-Panel (Iyyer et al., [2017](#bib.bib26)) | 准确度 |'
- en: '| Recipe Completion | Recipe | RecipeQA-TextCloze (Yagcioglu et al., [2018](#bib.bib59))
    | Accuracy |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 食谱完成 | 食谱 | RecipeQA-TextCloze (Yagcioglu et al., [2018](#bib.bib59)) | 准确度
    |'
- en: '| Visual Step Cloze | Recipe | RecipeQA-VisualCloze (Yagcioglu et al., [2018](#bib.bib59))
    | Accuracy |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 视觉步骤填空 | 食谱 | RecipeQA-VisualCloze (Yagcioglu et al., [2018](#bib.bib59))
    | 准确度 |'
- en: '| Knowledge Grounded QA |  |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 知识基础问答 |  |  |  |'
- en: '| Webpage QA | Webpage | WebQA (Chang et al., [2022](#bib.bib9)) | Accuracy
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 网页问答 | 网页 | WebQA (Chang et al., [2022](#bib.bib9)) | 准确度 |'
- en: '| Textbook QA | Textbook | TQA (Kembhavi et al., [2017](#bib.bib28)) | Accuracy
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 教科书问答 | 教科书 | TQA (Kembhavi et al., [2017](#bib.bib28)) | 准确度 |'
- en: '| Complex Multimodal QA | Wikipedia | MMQA (Talmor et al., [2021](#bib.bib51))
    | Accuracy |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 复杂多模态问答 | 维基百科 | MMQA (Talmor et al., [2021](#bib.bib51)) | 准确度 |'
- en: '| Complex Multimodal QA* | Wikipedia | MANYMODALQA (Hannan et al., [2020](#bib.bib19))
    | Accuracy |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 复杂多模态问答* | 维基百科 | MANYMODALQA (Hannan et al., [2020](#bib.bib19)) | 准确度 |'
- en: '| Text-Rich Images QA |  |  |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 文本丰富图像问答 |  |  |  |'
- en: '| Slide QA | Slide | SlideVQA (Tanaka et al., [2023](#bib.bib53)) | Accuracy
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 幻灯片问答 | 幻灯片 | SlideVQA (Tanaka et al., [2023](#bib.bib53)) | 准确度 |'
- en: '| OCR QA | Book Cover | OCR-VQA (Mishra et al., [2019](#bib.bib39)) | Accuracy
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| OCR问答 | 书封面 | OCR-VQA (Mishra et al., [2019](#bib.bib39)) | 准确度 |'
- en: '| Document QA | Document Image | DocVQA (Mathew et al., [2021](#bib.bib38))
    | Accuracy |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 文档问答 | 文档图像 | DocVQA (Mathew et al., [2021](#bib.bib38)) | 准确度 |'
- en: '| Multi-Image Reasoning |  |  |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 多图像推理 |  |  |  |'
- en: '| Image-Set QA* | Indoor Egocentric | Gibson (Bansal et al., [2020](#bib.bib5);
    Xia et al., [2018](#bib.bib56)) | Accuracy |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 图像集问答* | 室内自我中心 | Gibson (Bansal et al., [2020](#bib.bib5); Xia et al., [2018](#bib.bib56))
    | 准确度 |'
- en: '| Image-Set QA | Driving Recording | nuScenes (Bansal et al., [2020](#bib.bib5);
    Caesar et al., [2020](#bib.bib8)) | Accuracy |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 图像集问答 | 驾驶记录 | nuScenes (Bansal et al., [2020](#bib.bib5); Caesar et al.,
    [2020](#bib.bib8)) | 准确度 |'
- en: '| Industrial Inspection | Industrial | VISION (Bai et al., [2023](#bib.bib4))
    | Accuracy |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 工业检测 | 工业 | VISION (Bai et al., [2023](#bib.bib4)) | 准确度 |'
- en: '| Fashion QA | Fashion | Fashion200K (Han et al., [2017](#bib.bib18)) | Accuracy
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 时尚问答 | 时尚 | Fashion200K (Han et al., [2017](#bib.bib18)) | 准确度 |'
- en: '| Property Coherence | General | MIT-States-PropertyCoherence (Isola et al.,
    [2015](#bib.bib25)) | Accuracy |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 属性一致性 | 一般 | MIT-States-PropertyCoherence (Isola et al., [2015](#bib.bib25))
    | 准确度 |'
- en: '| State Transformation Coherence | General | MIT-States-StateCoherence (Isola
    et al., [2015](#bib.bib25)) | Accuracy |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 状态转换一致性 | 一般 | MIT-States-StateCoherence (Isola et al., [2015](#bib.bib25))
    | 准确度 |'
- en: '| Visual Step Matching | Recipe | RecipeQA-ImageCoherence (Yagcioglu et al.,
    [2018](#bib.bib59)) | Accuracy |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 视觉步骤匹配 | 食谱 | RecipeQA-ImageCoherence (Yagcioglu et al., [2018](#bib.bib59))
    | 准确度 |'
- en: '| Multi-Image Visual Entailment | General | NLVR2 (Suhr et al., [2018](#bib.bib50))
    | Accuracy |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 多图像视觉蕴含 | 一般 | NLVR2 (Suhr et al., [2018](#bib.bib50)) | 准确度 |'
- en: '| Ambiguity Analysis | Mobile Photo | VizWiz (Bhattacharya et al., [2019](#bib.bib7))
    | Accuracy |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 模糊分析 | 手机照片 | VizWiz (Bhattacharya et al., [2019](#bib.bib7)) | 准确度 |'
- en: 'Table 6: Summary of the demonstrative instruction-following tasks in DEMON
    benchmark. * indicates the tasks that are not included in DEMON-Core.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：DEMON基准中的示范指令跟随任务汇总。* 表示不包含在 DEMON-Core 中的任务。
- en: Appendix C Implementation Details
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实施细节
- en: 'Model. We choose ViT-G/14 from EVA-CLIP (Fang et al., [2023](#bib.bib13)) as
    our visual encoder and pre-trained Q-former from BLIP-2 without instruction tuning
    as the task-agnostic visual prompt generator. For the large language model, we
    implement three versions: LLaMA2-7B (Touvron et al., [2023b](#bib.bib55)), Vicuna-7B (Chiang
    et al., [2023](#bib.bib10)), Vicuna-13B, with 32, 32, 48 Transformer layers, respectively.
    We derive instruction-specific conditions from the 16th / 24th layer and re-inject
    the conditional visual knowledge into the 17th / 25th layer. Furthermore, we provide
    detailed framework of our MLLM enhanced with VPG-C in Figure [9](#A3.F9 "Figure
    9 ‣ Appendix C Implementation Details ‣ Fine-tuning Multimodal LLMs to Follow
    Zero-shot Demonstrative Instructions").'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。我们选择 EVA-CLIP（Fang et al., [2023](#bib.bib13)）中的 ViT-G/14 作为我们的视觉编码器，选择 BLIP-2
    中未经指令调优的 Q-former 作为任务无关的视觉提示生成器。对于大型语言模型，我们实现了三个版本：LLaMA2-7B（Touvron et al.,
    [2023b](#bib.bib55)）、Vicuna-7B（Chiang et al., [2023](#bib.bib10)）、Vicuna-13B，分别具有
    32、32 和 48 个 Transformer 层。我们从第 16 层 / 第 24 层中推导出特定于指令的条件，并将条件视觉知识重新注入到第 17 层
    / 第 25 层中。此外，我们在图 [9](#A3.F9 "Figure 9 ‣ Appendix C Implementation Details ‣ Fine-tuning
    Multimodal LLMs to Follow Zero-shot Demonstrative Instructions") 中提供了我们增强了 VPG-C
    的 MLLM 的详细框架。
- en: '![Refer to caption](img/ca452603e5baf6d67add9bea2c2adbfc.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ca452603e5baf6d67add9bea2c2adbfc.png)'
- en: 'Figure 9: Detailed framework of our MLLM enhanced with VPG-C.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：我们增强了 VPG-C 的 MLLM 的详细框架。
- en: Choice of Q-former. Recently, InstructBLIP (Dai et al., [2023](#bib.bib12))
    proposes to take the instruction as additional input to the Q-former and fine-tune
    the Q-former to extract visual features according to instructions using 16M multimodal
    instruction tuning data. While achieving outstanding performance on in-domain
    tasks, a recent study (Xu et al., [2023](#bib.bib58)) indicates that fine-tuning
    on massive in-domain data severely undermines its generalizability on open-world
    scenarios. Instead of directly relying on the Q-former to achieve task-specific
    feature extraction by massive instruction tuning, we aim to utilize the sophisticated
    reasoning ability of LLMs to guide the Q-former to conditionally attend to residual
    visual details. Thus, we use the Q-former without instruction data tuning from
    BLIP-2 (Li et al., [2023b](#bib.bib32)), which extracts the task-agnostic primary
    visual contents at the first time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Q-former 的选择。最近，InstructBLIP（Dai et al., [2023](#bib.bib12)）建议将指令作为额外输入添加到 Q-former
    中，并微调 Q-former 以根据指令提取视觉特征，使用 1600 万多模态指令调优数据。尽管在领域内任务上表现出色，但最近的研究（Xu et al.,
    [2023](#bib.bib58)）指出，在大量领域内数据上进行微调严重削弱了其在开放世界场景中的泛化能力。我们并不直接依赖 Q-former 通过大量指令调优来实现任务特定的特征提取，而是利用
    LLM 的复杂推理能力来引导 Q-former 有条件地关注剩余的视觉细节。因此，我们使用 BLIP-2（Li et al., [2023b](#bib.bib32)）中未经指令数据调优的
    Q-former，它首次提取了任务无关的主要视觉内容。
- en: Training. We implement VPG-C in LAVIS library (Li et al., [2022a](#bib.bib31)).
    We keep the visual backbone, visual prompt generator, and the language model frozen,
    and tune the VPG-C module using the proposed training strategy. Since BLIP-2 models
    do not include pre-trained Q-former that matches Vicuna and LLaMA2, we reuse the
    Q-former that matches FlanT5-XXL and fine-tune the last linear projection layer
    with 5 million image-text pairs to align it with Vicuna/LLaMA2\. All the tunable
    parameters of our VPG-C module are a set of query embeddings and two linear projection
    layers, which only accounts for 0.09% ($\sim$, and set the learning rate and weight
    decay to 0.00002 and 0.05, respectively. We warm up the training with 2k warm-up
    steps, followed by a learning rate decay mechanism with the cosine schedule.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。我们在 LAVIS 库中实现了 VPG-C（Li et al., [2022a](#bib.bib31)）。我们保持视觉骨干网、视觉提示生成器和语言模型的冻结状态，并使用提议的训练策略对
    VPG-C 模块进行调优。由于 BLIP-2 模型不包括与 Vicuna 和 LLaMA2 匹配的预训练 Q-former，我们重用与 FlanT5-XXL
    匹配的 Q-former，并使用 500 万对图像-文本对对最后的线性投影层进行微调，以使其与 Vicuna/LLaMA2 对齐。我们 VPG-C 模块的所有可调参数包括一组查询嵌入和两个线性投影层，占比仅为
    0.09% ($\sim$)，并将学习率和权重衰减分别设置为 0.00002 和 0.05。我们用 2k 步骤的预热步骤来预热训练，然后使用余弦调度的学习率衰减机制。
- en: 'Implementation of VPG-C with the linear VPG. As a generic method, VPG-C can
    be implemented with different VPGs. Beyond widely used Q-former that is composed
    of multiple Transformer blocks, we further probe the effectiveness of VPG-C with
    a simpler VPG, i.e., Linear Projection, as used in LLaVA (Liu et al., [2023](#bib.bib36)).
    LLaVA trains a simple linear layer as the VPG to connect image features into the
    word embedding space. To implement VPG-C with the linear VPG, we first linearly
    project the generated guidance $\mathbf{g}$ from the image encoder:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: VPG-C的实现使用了线性VPG。作为一种通用方法，VPG-C可以与不同的VPG一起实现。除了广泛使用的由多个Transformer块组成的Q-former，我们进一步探讨了使用更简单的VPG，即线性投影（在LLaVA中使用的，Liu等，[2023](#bib.bib36)），VPG-C的有效性。LLaVA训练一个简单的线性层作为VPG，将图像特征连接到词嵌入空间。为了使用线性VPG实现VPG-C，我们首先对从图像编码器生成的引导$\mathbf{g}$进行线性投影：
- en: '|  | $\overline{\mathcal{V}}=(\mathcal{W}_{1}\mathbf{g}\mathfrak{1}^{T})\odot(\mathcal{W}_{2}\mathcal{X}^{I})$
    |  | (1) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overline{\mathcal{V}}=(\mathcal{W}_{1}\mathbf{g}\mathfrak{1}^{T})\odot(\mathcal{W}_{2}\mathcal{X}^{I})$
    |  | (1) |'
- en: where $\mathcal{W}_{1}$ represents Hadamard product. The output $\overline{\mathcal{V}}$
    is reintegrated into the LLM in the same manner.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{W}_{1}$表示Hadamard积。输出$\overline{\mathcal{V}}$以相同的方式重新整合到LLM中。
- en: Appendix D Model Details in DEMON Benchmark
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D DEMON基准中的模型细节
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLaVA (Liu et al., [2023](#bib.bib36)) establishes a connection between the
    visual encoder ViT-L/14 from CLIP (Radford et al., [2021a](#bib.bib43)) and the
    language decoder LLaMA (Touvron et al., [2023a](#bib.bib54)), utilizing a lightweight,
    fully-connected (FC) layer. Initially, the system trains this FC layer using 595K
    image-text pairs, while keeping both the visual encoder and LLM static. Following
    this, LLaVA fine-tunes both the FC layer and LLM using a dataset comprising 158K
    instructional vision-language pairs. The tested version is “LLaVA-7B-v0”.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaVA（Liu等，[2023](#bib.bib36)）建立了CLIP（Radford等，[2021a](#bib.bib43)）中的视觉编码器ViT-L/14与语言解码器LLaMA（Touvron等，[2023a](#bib.bib54)）之间的连接，利用了一个轻量级的全连接（FC）层。最初，系统使用595K图像-文本对训练这个FC层，同时保持视觉编码器和LLM静态。之后，LLaVA使用包含158K指令性视觉-语言对的数据集对FC层和LLM进行微调。测试版本为“LLaVA-7B-v0”。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLaMA-Adapter V2 (Gao et al., [2023](#bib.bib16)) stands as a model of parameter
    efficiency within the realm of visual instruction. Despite maintaining the visual
    encoder (ViT-L/14) and the LLM in a static state, LA-V2 distributes the instruction-following
    capacity of the entire LLaMA system via bias-tuning. This method allows for the
    refinement of scale, bias, norm, and prompt parameters on diverse data sets. These
    include 200M image captioning data, 158K visual instruction-following data, and
    an additional 52K language instruction-following data, the latter of which was
    assembled by GPT-4 (OpenAI, [2023b](#bib.bib41)). The tested version is “LLaVA-7B”.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA-Adapter V2（Gao等，[2023](#bib.bib16)）在视觉指令领域中作为参数效率模型。尽管视觉编码器（ViT-L/14）和LLM保持静态状态，LA-V2通过偏置调优分配整个LLaMA系统的指令跟随能力。这种方法允许对不同数据集上的尺度、偏置、规范和提示参数进行调整。这些数据集包括200M图像标注数据、158K视觉指令跟随数据以及额外的52K语言指令跟随数据，后者由GPT-4（OpenAI，[2023b](#bib.bib41)）组装。测试版本为“LLaVA-7B”。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MiniGPT-4 (Zhu et al., [2023a](#bib.bib63)) bridges the gap between the visual
    encoder and text encoder using a fully-connected (FC) layer. Initially, this model
    trains the FC layer on a dataset comprised of 5M image-text pairs before fine-tuning
    it on 3.5K instructional vision-language data. Notwithstanding its simplicity,
    MiniGPT-4 requires the loading of a pre-trained vision encoder from BLIP2, as
    well as a Vicuna LLM (Chiang et al., [2023](#bib.bib10)). The tested version is
    “minigpt4-aligned-with-vicuna7b”.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MiniGPT-4（Zhu等，[2023a](#bib.bib63)）使用全连接（FC）层弥合了视觉编码器和文本编码器之间的差距。最初，该模型在包含5M图像-文本对的数据集上训练FC层，然后在3.5K指令性视觉-语言数据上进行微调。尽管其简单性，MiniGPT-4仍需加载来自BLIP2的预训练视觉编码器以及Vicuna
    LLM（Chiang等，[2023](#bib.bib10)）。测试版本为“minigpt4-aligned-with-vicuna7b”。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BLIP2 (Li et al., [2023b](#bib.bib32)) employs a dual-stage strategy to seamlessly
    bridge the modality gap, utilizing a lean Q-Former pre-trained on 129 million
    image-text pairs. The initial stage kick-starts the learning process of vision-language
    representation, leveraging a frozen image encoder, the ViT-g/14 from EVA-CLIP (Fang
    et al., [2023](#bib.bib13)). Subsequently, the second stage harnesses a frozen
    LLM, the FlanT5 (Chung et al., [2022](#bib.bib11)), to initiate the vision-to-language
    generative learning. This innovative strategy effectively facilitates zero-shot
    instructed image-to-text generation. The tested version is “blip2-pretrain-flant5xl”.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BLIP2 (Li et al., [2023b](#bib.bib32)) 采用了双阶段策略来无缝弥合模态差距，利用在 1.29 亿图像-文本对上预训练的精简
    Q-Former。初始阶段启动了视觉-语言表示学习过程，利用冻结的图像编码器，即来自 EVA-CLIP (Fang et al., [2023](#bib.bib13))
    的 ViT-g/14。随后，第二阶段利用冻结的 LLM，即 FlanT5 (Chung et al., [2022](#bib.bib11))，开始视觉到语言的生成学习。这一创新策略有效促进了零-shot
    指令图像到文本生成。测试版本是“blip2-pretrain-flant5xl”。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: mPLUG-Owl (Ye et al., [2023](#bib.bib60)) introduces a visual abstractor, fundamentally
    close the Perceiver Resampler in Flamingo (Alayrac et al., [2022](#bib.bib1)),
    as a bridge between the pre-trained visual encoder ViT-L/14 and the LLM (LLaMA (Touvron
    et al., [2023a](#bib.bib54))). This model adopts a two-stage fine-tuning procedure.
    In the initial phase, both the visual encoder and the visual abstractor undergo
    comprehensive fine-tuning using a dataset of 204M image-text pairs. Subsequently,
    in the second phase, mPLUG-Owl applies the 158K LLaVA-Instruct dataset to fine-tune
    the pre-trained LLM in a parameter-efficient manner through the use of LoRA (Hu
    et al., [2021](#bib.bib23)). The tested version is “mplug-owl-llama-7b”.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: mPLUG-Owl (Ye et al., [2023](#bib.bib60)) 引入了一个视觉抽象器，本质上接近 Flamingo (Alayrac
    et al., [2022](#bib.bib1)) 中的 Perceiver Resampler，作为预训练视觉编码器 ViT-L/14 和 LLM (LLaMA (Touvron
    et al., [2023a](#bib.bib54))) 之间的桥梁。该模型采用了两阶段的微调程序。在初始阶段，视觉编码器和视觉抽象器都使用 2.04 亿图像-文本对的数据集进行了全面微调。随后，在第二阶段，mPLUG-Owl
    应用 158K LLaVA-Instruct 数据集，通过 LoRA (Hu et al., [2021](#bib.bib23)) 以参数高效的方式对预训练的
    LLM 进行微调。测试版本是“mplug-owl-llama-7b”。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Otter (Li et al., [2023a](#bib.bib30)) is a multimodal model that applies in-context
    instruction tuning based on OpenFlamingo (Alayrac et al., [2022](#bib.bib1)).
    This model integrates a LLaMA-7B (Touvron et al., [2023a](#bib.bib54)) language
    encoder and a CLIP ViT-L/14\. While the visual and text encoders remain static,
    Otter refines an additional 1.3 billion parameters. These parameters are derived
    from adaptation modules and are trained using 158K instruction-following data.
    The tested version is “OTTER-Image-LLaMA7B-LA-InContext”.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Otter (Li et al., [2023a](#bib.bib30)) 是一个多模态模型，基于 OpenFlamingo (Alayrac et
    al., [2022](#bib.bib1)) 应用了上下文指令调优。该模型集成了 LLaMA-7B (Touvron et al., [2023a](#bib.bib54))
    语言编码器和 CLIP ViT-L/14。尽管视觉和文本编码器保持静态，Otter 还优化了额外的 13 亿个参数。这些参数来自适应模块，并使用 158K
    指令跟随数据进行训练。测试版本是“OTTER-Image-LLaMA7B-LA-InContext”。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: InstructBLIP (Dai et al., [2023](#bib.bib12)) originates from a pre-trained
    BLIP-2 model, which consists of a ViT-g/14 image encoder, a Vicuna LLM, and a
    Q-Former to act as the bridge between these two components. During the process
    of vision-language instruction tuning, only the Q-Former undergoes fine-tuning,
    with the training process leveraging data from 13 distinct visual question-answering
    datasets. The tested version is “blip2-vicuna-instruct-7b”.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: InstructBLIP (Dai et al., [2023](#bib.bib12)) 源于一个预训练的 BLIP-2 模型，该模型包含一个 ViT-g/14
    图像编码器、一个 Vicuna LLM 和一个 Q-Former 作为这两个组件之间的桥梁。在视觉-语言指令调优过程中，只有 Q-Former 经过微调，训练过程利用了来自
    13 个不同视觉问答数据集的数据。测试版本是“blip2-vicuna-instruct-7b”。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: OpenFlamingo (Alayrac et al., [2022](#bib.bib1); Awadalla et al., [2023](#bib.bib3))
    represents one of the pioneering efforts to incorporate Language Model Learning
    (LLMs) into the domain of vision-language pretraining. To optimize its conditioning
    on visual features, Flamingo strategically integrates a number of gated cross-attention
    dense blocks amidst the layers of the pre-trained language encoder. OpenFlamingo
    offers an open-source rendition of this advanced model. The tested version is
    “llama-7b”.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenFlamingo (Alayrac et al., [2022](#bib.bib1); Awadalla et al., [2023](#bib.bib3))
    代表了将语言模型学习（LLMs）融入视觉-语言预训练领域的开创性尝试之一。为了优化其对视觉特征的条件适应，Flamingo 在预训练语言编码器的层间战略性地集成了多个门控交叉注意密集块。OpenFlamingo
    提供了这一先进模型的开源版本。测试版本是“llama-7b”。
- en: The DEMON benchmark predominantly features interleaved vision-language instructions,
    distinguishing it from the traditional single-image datasets. While our innovative
    method, VPG-C, along with OpenFlamingo and MiniGPT-4, inherently accommodates
    interleaved image-text sequences, other models like BLIP-2, InstructBlip, LLaVA,
    mPLUG-Owl, Otter, and LLaMA-Adapter V2 do not. For these, we employed a strategy
    where we concatenate the embeddings of all images. This approach can be analogized
    to treating images as frames within a video. To maintain the positional context
    of each image in an interleaved image-text instruction, we explicitly indicate
    the location of each image within the context.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: DEMON 基准主要特征为交错的视觉-语言指令，将其与传统的单图像数据集区分开来。虽然我们创新的方法 VPG-C，以及 OpenFlamingo 和 MiniGPT-4，本质上适应交错的图像-文本序列，但其他模型如
    BLIP-2、InstructBlip、LLaVA、mPLUG-Owl、Otter 和 LLaMA-Adapter V2 不具备这一能力。对于这些模型，我们采用了一种策略，将所有图像的嵌入拼接在一起。这种方法可以类比为将图像视为视频中的帧。为了保持每张图像在交错图像-文本指令中的位置上下文，我们明确指示每张图像在上下文中的位置。
- en: Appendix E Detailed Zero-Shot Performance on MME Benchmark
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E MME 基准的详细零样本性能
- en: In this section, we report the detailed performance on the 14 subtasks of MME
    benchmark in Table [7](#A5.T7 "Table 7 ‣ Appendix E Detailed Zero-Shot Performance
    on MME Benchmark ‣ Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative
    Instructions").
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们报告了表 [7](#A5.T7 "表 7 ‣ 附录 E MME 基准的详细零样本性能 ‣ 微调多模态 LLM 以跟随零样本演示指令") 中
    MME 基准 14 个子任务的详细性能。
- en: 'Table 7: Detailed zero-shot performance on MME benchmark.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：MME 基准的详细零样本性能。
- en: '|  | BLIP-2 | InstructBLIP | LA-V2 | LLaVA | MiniGPT-4 | mPLUG-Owl | Otter
    | VPG-C |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | BLIP-2 | InstructBLIP | LA-V2 | LLaVA | MiniGPT-4 | mPLUG-Owl | Otter
    | VPG-C |'
- en: '| Existence | 160.00 | 185.00 | 120.00 | 50.00 | 115.00 | 120.00 | 195.00 |
    180.00 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 存在 | 160.00 | 185.00 | 120.00 | 50.00 | 115.00 | 120.00 | 195.00 | 180.00
    |'
- en: '| Count | 135.00 | 143.33 | 50.00 | 50.00 | 123.33 | 88.33 | 50.00 | 96.67
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 135.00 | 143.33 | 50.00 | 50.00 | 123.33 | 88.33 | 50.00 | 96.67 |'
- en: '| Position | 73.33 | 66.67 | 48.33 | 50.00 | 81.67 | 50.00 | 86.67 | 80.00
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 73.33 | 66.67 | 48.33 | 50.00 | 81.67 | 50.00 | 86.67 | 80.00 |'
- en: '| Color | 148.33 | 153.33 | 75.00 | 55.00 | 110.00 | 55.00 | 113.33 | 116.67
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 颜色 | 148.33 | 153.33 | 75.00 | 55.00 | 110.00 | 55.00 | 113.33 | 116.67 |'
- en: '| Poster | 141.84 | 123.81 | 99.66 | 50.00 | 55.78 | 136.05 | 138.78 | 147.28
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 海报 | 141.84 | 123.81 | 99.66 | 50.00 | 55.78 | 136.05 | 138.78 | 147.28 |'
- en: '| Celebrity | 105.59 | 101.18 | 86.18 | 48.82 | 65.29 | 100.29 | 172.65 | 164.12
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 名人 | 105.59 | 101.18 | 86.18 | 48.82 | 65.29 | 100.29 | 172.65 | 164.12 |'
- en: '| Scene | 145.25 | 153.00 | 148.50 | 50.00 | 95.75 | 135.50 | 158.75 | 156.00
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 145.25 | 153.00 | 148.50 | 50.00 | 95.75 | 135.50 | 158.75 | 156.00
    |'
- en: '| Landmark | 138.00 | 79.75 | 150.25 | 50.00 | 69.00 | 159.25 | 137.25 | 145.00
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 地标 | 138.00 | 79.75 | 150.25 | 50.00 | 69.00 | 159.25 | 137.25 | 145.00 |'
- en: '| Artwork | 136.50 | 134.25 | 69.75 | 49.00 | 55.75 | 96.25 | 129.00 | 113.50
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 艺术品 | 136.50 | 134.25 | 69.75 | 49.00 | 55.75 | 96.25 | 129.00 | 113.50 |'
- en: '| OCR | 110.00 | 72.50 | 125.00 | 50.00 | 95.00 | 65.00 | 72.50 | 100.00 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| OCR | 110.00 | 72.50 | 125.00 | 50.00 | 95.00 | 65.00 | 72.50 | 100.00 |'
- en: '| Perception | 1293.84 | 1212.82 | 972.67 | 502.82 | 866.57 | 967.34 | 1292.26
    | 1299.24 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 1293.84 | 1212.82 | 972.67 | 502.82 | 866.57 | 967.34 | 1292.26 | 1299.24
    |'
- en: '| Commonsense | 110.00 | 129.29 | 81.43 | 57.14 | 72.14 | 78.57 | 106.43 |
    98.57 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 常识 | 110.00 | 129.29 | 81.43 | 57.14 | 72.14 | 78.57 | 106.43 | 98.57 |'
- en: '| Numerical | 40.00 | 40.00 | 62.50 | 50.00 | 55.00 | 60.00 | 72.50 | 77.50
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 数值 | 40.00 | 40.00 | 62.50 | 50.00 | 55.00 | 60.00 | 72.50 | 77.50 |'
- en: '| Text Translation | 65.00 | 65.00 | 50.00 | 57.50 | 55.00 | 80.00 | 57.50
    | 57.50 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 文本翻译 | 65.00 | 65.00 | 50.00 | 57.50 | 55.00 | 80.00 | 57.50 | 57.50 |'
- en: '| Code Reasoning | 75.00 | 57.50 | 55.00 | 50.00 | 110.00 | 57.50 | 70.00 |
    87.50 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 代码推理 | 75.00 | 57.50 | 55.00 | 50.00 | 110.00 | 57.50 | 70.00 | 87.50 |'
- en: '| Cognition | 290.00 | 291.79 | 248.93 | 214.64 | 292.14 | 276.07 | 306.43
    | 321.07 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 认知 | 290.00 | 291.79 | 248.93 | 214.64 | 292.14 | 276.07 | 306.43 | 321.07
    |'
- en: Appendix F Detailed Zero-Shot Performance on DEMON Benchmark
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F DEMON 基准的详细零样本性能
- en: 'Table 8: Zero-shot evaluation on multimodal dialogue.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：多模态对话的零样本评估。
- en: '|  | Conversational Embodied Dialogue | Multimodal Dialogue |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话体化对话 | 多模态对话 |'
- en: '| --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BLIP-2 | 16.75 | 35.49 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 | 16.75 | 35.49 |'
- en: '| InstructBLIP | 18.07 | 49.09 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP | 18.07 | 49.09 |'
- en: '| LLaMA-Adapter V2 | 19.04 | 9.40 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 | 19.04 | 9.40 |'
- en: '| LLaVA | 10.19 | 5.39 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA | 10.19 | 5.39 |'
- en: '| MiniGPT-4 | 16.82 | 10.57 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 | 16.82 | 10.57 |'
- en: '| mPLUG-Owl | 11.07 | 14.27 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 11.07 | 14.27 |'
- en: '| OpenFlamingo | 24.27 | 9.49 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo | 24.27 | 9.49 |'
- en: '| Otter | 16.06 | 14.68 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Otter | 16.06 | 14.68 |'
- en: '| VPG-C-LLaMA2-7B | 48.31 | 37.04 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-LLaMA2-7B | 48.31 | 37.04 |'
- en: '| VPG-C-Vicuna-7B | 41.02 | 33.99 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-7B | 41.02 | 33.99 |'
- en: '| VPG-C-Vicuna-13B | 42.25 | 34.02 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-13B | 42.25 | 34.02 |'
- en: 'Table 9: Zero-shot evaluation on visual storytelling.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 零-shot 评估视觉故事讲述。'
- en: '|  | Animated Story | Animated Story | Animated Story | Sequential Photo |
    Sequential Photo |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | 动画故事 | 动画故事 | 动画故事 | 顺序照片 | 顺序照片 |'
- en: '|  | Completion-AESOP | Completion-PororoSV | Completion-FlintstonesSV | Storytelling-VIST
    | Storytelling-DiDeMoSV |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | 完成-AESOP | 完成-PororoSV | 完成-FlintstonesSV | 故事讲述-VIST | 故事讲述-DiDeMoSV
    |'
- en: '| BLIP-2 | 21.64 | 26.24 | 29.61 | 13.16 | 24.2 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 | 21.64 | 26.24 | 29.61 | 13.16 | 24.2 |'
- en: '| InstructBLIP | 18.80 | 28.20 | 33.32 | 16.92 | 24.80 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP | 18.80 | 28.20 | 33.32 | 16.92 | 24.80 |'
- en: '| LLaMA-Adapter V2 | 18.01 | 20.15 | 24.22 | 10.89 | 14.57 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 | 18.01 | 20.15 | 24.22 | 10.89 | 14.57 |'
- en: '| LLaVA | 13.56 | 11.44 | 12.77 | 8.00 | 7.71 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA | 13.56 | 11.44 | 12.77 | 8.00 | 7.71 |'
- en: '| MiniGPT-4 | 12.23 | 16.00 | 26.48 | 14.82 | 15.81 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 | 12.23 | 16.00 | 26.48 | 14.82 | 15.81 |'
- en: '| mPLUG-Owl | 18.28 | 20.49 | 32.12 | 10.82 | 14.94 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 18.28 | 20.49 | 32.12 | 10.82 | 14.94 |'
- en: '| OpenFlamingo | 23.32 | 32.35 | 37.79 | 15.14 | 12.50 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo | 23.32 | 32.35 | 37.79 | 15.14 | 12.50 |'
- en: '| Otter | 13.94 | 17.52 | 22.21 | 9.96 | 14.23 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Otter | 13.94 | 17.52 | 22.21 | 9.96 | 14.23 |'
- en: '| VPG-C-LLaMA2-7B | 19.98 | 28.67 | 38.14 | 16.95 | 20.05 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-LLaMA2-7B | 19.98 | 28.67 | 38.14 | 16.95 | 20.05 |'
- en: '| VPG-C-Vicuna-7B | 19.93 | 28.36 | 39.19 | 17.34 | 21.27 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-7B | 19.93 | 28.36 | 39.19 | 17.34 | 21.27 |'
- en: '| VPG-C-Vicuna-13B | 20.53 | 29.81 | 41.32 | 19.04 | 22.26 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-13B | 20.53 | 29.81 | 41.32 | 19.04 | 22.26 |'
- en: 'Table 10: Zero-shot evaluation on visual relation inference.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 零-shot 评估视觉关系推理。'
- en: '|  | Visual Change Captioning | Visual Change Captioning | Visual Relationship
    | Subtle Difference |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉变化字幕 | 视觉变化字幕 | 视觉关系 | 微妙差异 |'
- en: '|  | -Spot-the-Diff | -CLEVR-Change | Expressing | Expressing |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | -Spot-the-Diff | -CLEVR-Change | 表达 | 表达 |'
- en: '| BLIP-2 | 17.48 | 3.21 | 12.37 | 9.62 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 | 17.48 | 3.21 | 12.37 | 9.62 |'
- en: '| InstructBLIP | 19.71 | 4.61 | 10.70 | 10.92 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP | 19.71 | 4.61 | 10.70 | 10.92 |'
- en: '| LLaMA-Adapter V2 | 16.72 | 15.52 | 7.88 | 13.92 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 | 16.72 | 15.52 | 7.88 | 13.92 |'
- en: '| LLaVA | 8.50 | 8.76 | 6.72 | 9.11 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA | 8.50 | 8.76 | 6.72 | 9.11 |'
- en: '| MiniGPT-4 | 7.50 | 7.49 | 7.84 | 8.97 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 | 7.50 | 7.49 | 7.84 | 8.97 |'
- en: '| mPLUG-Owl | 6.06 | 1.46 | 6.22 | 7.86 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 6.06 | 1.46 | 6.22 | 7.86 |'
- en: '| OpenFlamingo | 13.01 | 11.90 | 12.57 | 17.90 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo | 13.01 | 11.90 | 12.57 | 17.90 |'
- en: '| Otter | 12.69 | 11.63 | 8.85 | 12.38 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Otter | 12.69 | 11.63 | 8.85 | 12.38 |'
- en: '| VPG-C-LLaMA2-7B | 21.02 | 42.05 | 14.10 | 24.81 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-LLaMA2-7B | 21.02 | 42.05 | 14.10 | 24.81 |'
- en: '| VPG-C-Vicuna-7B | 20.01 | 41.60 | 16.35 | 25.64 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-7B | 20.01 | 41.60 | 16.35 | 25.64 |'
- en: '| VPG-C-Vicuna-13B | 21.56 | 40.67 | 20.27 | 26.08 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-13B | 21.56 | 40.67 | 20.27 | 26.08 |'
- en: 'Table 11: Zero-shot evaluation on multimodal cloze.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 零-shot 评估多模态填空。'
- en: '[b] Comic Dialogue Identification Comic Panel Identification¹ Recipe Completion
    Visual Step Cloze¹ BLIP-2 39.70 0.00 30.46 1.60 InstructBLIP 40.60 0.00 27.40
    16.80 LLaMA-Adapter V2 24.40 0.40 38.20 9.00 LLaVA 30.60 0.00 32.80 0.00 MiniGPT-4
    33.00 1.00 31.60 0.80 mPLUG-Owl 36.60 0.00 27.60 0.80 OpenFlamingo 38.40 1.20
    29.00 18.00 Otter 29.00 0.00 35.00 0.00 VPG-C-LLaMA2-7B 36.80 1.80 51.80 1.40
    VPG-C-Vicuna-7B 39.20 3.60 30.40 15.40 VPG-C-Vicuna-13B 42.20 8.20 39.80 18.40'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**Comic Dialogue Identification** 漫画对话识别 **Comic Panel Identification¹** 漫画面板识别¹
    **Recipe Completion** 食谱完成 **Visual Step Cloze¹** 视觉步骤填空¹ **BLIP-2** 39.70 0.00
    30.46 1.60 **InstructBLIP** 40.60 0.00 27.40 16.80 **LLaMA-Adapter V2** 24.40
    0.40 38.20 9.00 **LLaVA** 30.60 0.00 32.80 0.00 **MiniGPT-4** 33.00 1.00 31.60
    0.80 **mPLUG-Owl** 36.60 0.00 27.60 0.80 **OpenFlamingo** 38.40 1.20 29.00 18.00
    **Otter** 29.00 0.00 35.00 0.00 **VPG-C-LLaMA2-7B** 36.80 1.80 51.80 1.40 **VPG-C-Vicuna-7B**
    39.20 3.60 30.40 15.40 **VPG-C-Vicuna-13B** 42.20 8.20 39.80 18.40'
- en: '1'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: For tasks with images as options, only responses that begin with the correct
    answer will be evaluated as correct.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于图像作为选项的任务，只有以正确答案开头的回答才会被评估为正确。
- en: 'Table 12: Zero-shot evaluation on knowledge grounded QA.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 零-shot 评估知识基础 QA。'
- en: '|  | Webpage QA | Textbook QA | Complex Multimodal QA |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | 网页 QA | 教科书 QA | 复杂多模态 QA |'
- en: '| BLIP-2 | 47.60 | 29.73 | 40.36 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 | 47.60 | 29.73 | 40.36 |'
- en: '| InstructBLIP | 45.20 | 30.20 | 66.80 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP | 45.20 | 30.20 | 66.80 |'
- en: '| LLaMA-Adapter V2 | 44.60 | 46.00 | 43.80 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 | 44.60 | 46.00 | 43.80 |'
- en: '| LLaVA | 39.40 | 39.60 | 29.60 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA | 39.40 | 39.60 | 29.60 |'
- en: '| MiniGPT-4 | 27.40 | 28.60 | 34.80 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 | 27.40 | 28.60 | 34.80 |'
- en: '| mPLUG-Owl | 34.20 | 30.00 | 35.60 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 34.20 | 30.00 | 35.60 |'
- en: '| OpenFlamingo | 37.80 | 32.40 | 25.80 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo | 37.80 | 32.40 | 25.80 |'
- en: '| Otter | 45.00 | 39.00 | 41.00 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Otter | 45.00 | 39.00 | 41.00 |'
- en: '| VPG-C-LLaMA2-7B | 49.40 | 42.40 | 61.20 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-LLaMA2-7B | 49.40 | 42.40 | 61.20 |'
- en: '| VPG-C-Vicuna-7B | 50.00 | 33.40 | 62.40 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-7B | 50.00 | 33.40 | 62.40 |'
- en: '| VPG-C-Vicuna-13B | 50.60 | 43.40 | 64.80 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-13B | 50.60 | 43.40 | 64.80 |'
- en: 'Table 13: Zero-shot evaluation on text-rich images QA.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 零-shot 评估文本丰富图像 QA。'
- en: '|  | Slide QA | OCR QA | Document QA |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | 幻灯片 QA | OCR QA | 文档 QA |'
- en: '| BLIP-2 | 43.80 | 10.40 | 46.40 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| BLIP-2 | 43.80 | 10.40 | 46.40 |'
- en: '| InstructBLIP | 42.00 | 44.20 | 47.00 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| InstructBLIP | 42.00 | 44.20 | 47.00 |'
- en: '| LLaMA-Adapter V2 | 43.00 | 3.40 | 49.60 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter V2 | 43.00 | 3.40 | 49.60 |'
- en: '| LLaVA | 38.80 | 2.60 | 43.60 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA | 38.80 | 2.60 | 43.60 |'
- en: '| MiniGPT-4 | 35.20 | 7.20 | 36.80 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT-4 | 35.20 | 7.20 | 36.80 |'
- en: '| mPLUG-Owl | 35.60 | 22.60 | 39.20 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 35.60 | 22.60 | 39.20 |'
- en: '| OpenFlamingo | 35.60 | 3.80 | 52.40 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| OpenFlamingo | 35.60 | 3.80 | 52.40 |'
- en: '| Otter | 38.40 | 2.20 | 42.60 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| Otter | 38.40 | 2.20 | 42.60 |'
- en: '| VPG-C-LLaMA2-7B | 45.80 | 39.60 | 49.40 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-LLaMA2-7B | 45.80 | 39.60 | 49.40 |'
- en: '| VPG-C-Vicuna-7B | 46.80 | 39.40 | 48.60 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-7B | 46.80 | 39.40 | 48.60 |'
- en: '| VPG-C-Vicuna-13B | 48.80 | 46.60 | 52.60 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| VPG-C-Vicuna-13B | 48.80 | 46.60 | 52.60 |'
- en: 'Table 14: Zero-shot evaluation on multi-image reasoning.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 多图像推理的零-shot 评估。'
- en: '[b] Image-Set Industrial Fashion Property State Transformation Visual Step
    Multi-Image Ambiguity QA Inspection QA Coherence Coherence Matching ¹ Visual Entailment
    Analysis BLIP-2 34.60 42.80 43.20 59.00 38.20 0.20 53.40 45.80 instructblip7b
    65.00 50.60 44.40 59.20 59.40 11.60 55.20 43.00 LLaMA-Adapter V2 41.60 55.00 45.60
    48.80 63.00 0.00 54.80 43.40 LLaVA 29.60 53.00 45.20 50.40 59.20 0.80 50.80 43.20
    MiniGPT-4 30.40 59.80 49.20 52.00 57.80 0.20 50.60 48.00 mPLUG-Owl 29.20 54.20
    45.80 50.00 60.60 0.00 55.00 45.20 OpenFlamingo 25.80 52.20 44.20 59.60 51.40
    2.20 53.60 44.00 Otter 44.80 69.80 47.00 51.40 46.40 0.00 49.00 42.40 VPG-C-LLaMA2-7B
    62.60 61.40 46.00 56.60 57.80 0.00 53.80 51.20 VPG-C-Vicuna-7B 67.20 48.80 50.00
    60.80 60.00 0.20 57.80 57.40 VPG-C-Vicuna-13B 73.40 54.00 51.00 63.20 63.40 2.60
    60.20 61.40'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[b] 图像集 工业时尚 属性 状态 转换 视觉 步骤 多图像 模糊 QA 检查 QA 一致性 一致性 匹配 ¹ 视觉蕴含分析 BLIP-2 34.60
    42.80 43.20 59.00 38.20 0.20 53.40 45.80 instructblip7b 65.00 50.60 44.40 59.20
    59.40 11.60 55.20 43.00 LLaMA-Adapter V2 41.60 55.00 45.60 48.80 63.00 0.00 54.80
    43.40 LLaVA 29.60 53.00 45.20 50.40 59.20 0.80 50.80 43.20 MiniGPT-4 30.40 59.80
    49.20 52.00 57.80 0.20 50.60 48.00 mPLUG-Owl 29.20 54.20 45.80 50.00 60.60 0.00
    55.00 45.20 OpenFlamingo 25.80 52.20 44.20 59.60 51.40 2.20 53.60 44.00 Otter
    44.80 69.80 47.00 51.40 46.40 0.00 49.00 42.40 VPG-C-LLaMA2-7B 62.60 61.40 46.00
    56.60 57.80 0.00 53.80 51.20 VPG-C-Vicuna-7B 67.20 48.80 50.00 60.80 60.00 0.20
    57.80 57.40 VPG-C-Vicuna-13B 73.40 54.00 51.00 63.20 63.40 2.60 60.20 61.40'
- en: '1'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: For tasks with images as options, only responses that begin with the correct
    answer will be evaluated as correct.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于以图像作为选项的任务，只有以正确答案开头的回应才会被评估为正确。
- en: Appendix G Qualitative Comparison
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 质性比较
- en: In this section, we compare our model with existing MLLMs on some complicated
    demonstrative instructions.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将我们的模型与现有的 MLLMs 在一些复杂的演示指令上进行比较。
- en: '![Refer to caption](img/0462a30b3e85cc7158d31a340fa0a51f.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0462a30b3e85cc7158d31a340fa0a51f.png)'
- en: 'Figure 10: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
- en: '![Refer to caption](img/307c47550fe054aca8ac11609cfdfaa0.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/307c47550fe054aca8ac11609cfdfaa0.png)'
- en: 'Figure 11: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
- en: '![Refer to caption](img/80cbc6c570fd9051eefa81a035e5503d.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80cbc6c570fd9051eefa81a035e5503d.png)'
- en: 'Figure 12: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
- en: '![Refer to caption](img/d65cace25a65a34cb8fd1c89b017f510.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d65cace25a65a34cb8fd1c89b017f510.png)'
- en: 'Figure 13: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
- en: '![Refer to caption](img/03bbe8c0495fc55b4cbbe7ca89cc35eb.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03bbe8c0495fc55b4cbbe7ca89cc35eb.png)'
- en: 'Figure 14: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
- en: '![Refer to caption](img/56f4a25451eea9dae9d21049ce481677.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56f4a25451eea9dae9d21049ce481677.png)'
- en: 'Figure 15: Qualitative comparison between our VPG-C and existing MLLMs.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 我们的 VPG-C 与现有 MLLMs 之间的质性比较。'
