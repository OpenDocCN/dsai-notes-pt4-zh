- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QuanTA: 基于量子启发的张量适配的高效高秩微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00132](https://ar5iv.labs.arxiv.org/html/2406.00132)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.00132](https://ar5iv.labs.arxiv.org/html/2406.00132)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: Zhuo Chen^(12)  Rumen Dangovski^(13)  Charlotte Loh^(13)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卓陈^(12)  鲁门·丹戈夫斯基^(13)  夏洛特·洛^(13)
- en: Owen Dugan²  Di Luo^(124)^∗  Marin Soljačić^(12)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欧文·杜根²  迪·洛^(124)^∗  马林·索尔贾奇^(12)
- en: ¹NSF AI Institute for Artificial Intelligence and Fundamental Interactions
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹NSF人工智能研究所
- en: ²Department of Physics, Massachusetts Institute of Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²麻省理工学院物理系
- en: ³Department of Department of EECS, Massachusetts Institute of Technology
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ³麻省理工学院EECS系
- en: ⁴Department of Physics, Harvard University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴哈佛大学物理系
- en: '{chenzhuo,rumenrd,cloh,odugan,diluo,soljacic}@mit.edu'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '{chenzhuo,rumenrd,cloh,odugan,diluo,soljacic}@mit.edu'
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We propose Quantum-informed Tensor Adaptation (QuanTA), a novel, easy-to-implement,
    fine-tuning method with no inference overhead for large-scale pre-trained language
    models. By leveraging quantum-inspired methods derived from quantum circuit structures,
    QuanTA enables efficient high-rank fine-tuning, surpassing the limitations of
    Low-Rank Adaptation (LoRA)—low-rank approximation may fail for complicated downstream
    tasks. Our approach is theoretically supported by the universality theorem and
    the rank representation theorem to achieve efficient high-rank adaptations. Experiments
    demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic
    reasoning, and scalability compared to traditional methods. Furthermore, QuanTA
    shows superior performance with fewer trainable parameters compared to other approaches
    and can be designed to integrate with existing fine-tuning algorithms for further
    improvement, providing a scalable and efficient solution for fine-tuning large
    language models and advancing state-of-the-art in natural language processing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了量子启发的张量适配（QuanTA），这是一种新颖的、易于实施的微调方法，不需要额外的推理开销，适用于大规模预训练语言模型。通过利用源于量子电路结构的量子启发方法，QuanTA
    实现了高效的高秩微调，超越了低秩适配（LoRA）的限制——低秩近似可能在复杂的下游任务中失败。我们的方法得到了普适性定理和秩表示定理的理论支持，以实现高效的高秩适配。实验表明，QuanTA
    在常识推理、算术推理和可扩展性方面显著优于传统方法。此外，QuanTA 在训练参数更少的情况下表现优越，并且可以设计为与现有微调算法集成以进一步改进，提供了一种可扩展且高效的解决方案，用于微调大型语言模型并推动自然语言处理的最前沿。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Pre-trainied large language models (LLMs) have revolutionized natural language
    processing (NLP) by achieving state-of-the-art performance across various tasks
    [[1](#bib.bib1), [2](#bib.bib2)]. Traditionally, these models are adapted to specific
    downstream applications via full fine-tuning, where all model parameters are retrained.
    However, as model sizes increase, the computational cost and memory requirements
    for full fine-tuning become prohibitive, especially with models like GPT-3 [[3](#bib.bib3)]
    with 175 billion parameters, Mixtral [[4](#bib.bib4)] with $8\times 22$ billion
    parameters, and more recently the LLaMA series [[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7)], containing soon up to 400 billion parameters [[8](#bib.bib8)].
    These constraints have spurred the development of parameter-efficient fine-tuning
    (PEFT) methods, which aim to adapt LLMs by updating only a small subset of parameters,
    thereby reducing resource demands [[9](#bib.bib9), [10](#bib.bib10)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）通过在各种任务上实现最先进的性能，彻底改变了自然语言处理（NLP）[[1](#bib.bib1), [2](#bib.bib2)]。传统上，这些模型通过完全微调来适应特定的下游应用，其中所有模型参数都重新训练。然而，随着模型规模的增加，完全微调的计算成本和内存需求变得难以承受，特别是像GPT-3
    [[3](#bib.bib3)] 这样拥有1750亿参数的模型，Mixtral [[4](#bib.bib4)] 拥有 $8\times 22$ 亿参数，最近LLaMA系列
    [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]，即将包含多达4000亿参数 [[8](#bib.bib8)]。这些限制促使了参数高效微调（PEFT）方法的发展，旨在通过仅更新少量参数来适应LLMs，从而减少资源需求
    [[9](#bib.bib9), [10](#bib.bib10)]。
- en: Among PEFT methods, Low-Rank Adaptation (LoRA) [[10](#bib.bib10)] has gained
    prominence due to its simplicity and effectiveness. LoRA fine-tunes LLMs by introducing
    low-rank matrices into the pre-trained model’s weight updates, pragmatically reducing
    the number of trainable parameters while maintaining performance close to full
    fine-tuning in many tasks. However, LoRA’s reliance on low-rank approximations
    can sometimes lead to a performance gap compared to full fine-tuning, particularly
    for complex tasks, as it may not capture all necessary task-specific adaptations
    [[11](#bib.bib11)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PEFT 方法中，低秩适应（LoRA）[[10](#bib.bib10)] 因其简单性和有效性而受到关注。LoRA 通过在预训练模型的权重更新中引入低秩矩阵来微调
    LLMs，实用地减少了可训练参数的数量，同时在许多任务中保持了接近完全微调的性能。然而，LoRA 对低秩近似的依赖有时会导致与完全微调相比的性能差距，特别是在复杂任务中，因为它可能无法捕捉所有必要的任务特定适应
    [[11](#bib.bib11)]。
- en: Recently, there have been many attempts to generalize LoRA using tensor-based
    methods [[12](#bib.bib12), [13](#bib.bib13)]. However, these approaches primarily
    focus on reducing the number of trainable parameters within the low-rank framework
    yet they continue to face the same limitations of restricted representation. In
    Quantum mechanics, quantum circuit provides a natural realization of unitary matrix
    which is full rank, motivating us to develop new schemes for high-rank fine-tuning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多尝试利用基于张量的方法来推广 LoRA [[12](#bib.bib12), [13](#bib.bib13)]。然而，这些方法主要集中在降低低秩框架内的可训练参数数量，但仍然面临表示受限的相同限制。在量子力学中，量子电路提供了单位矩阵的自然实现，而单位矩阵是满秩的，这激励我们开发新的高秩微调方案。
- en: '![Refer to caption](img/305dde22abc6325da8b0861076435b00.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/305dde22abc6325da8b0861076435b00.png)'
- en: 'Figure 1: Conceptual comparison of QuanTA and LoRA methods. LoRA parameterizes
    the weight matrix update as a outer product of two low-rank matrices, limiting
    its capacity. QuanTA, inspired by quantum circuits, uses tensors that operate
    on specific axes of the (reshaped) input, enabling high-rank parameterization.
    Supported by the universality theorem and rank representation theorem, QuanTA
    can represent arbitrary matrices effectively, allowing it to achieve performance
    comparable to or sometimes even better than full fine-tuning, with only a fraction
    of the parameters. Note: the performance graph is a conceptual illustration.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：QuanTA 和 LoRA 方法的概念比较。LoRA 将权重矩阵更新参数化为两个低秩矩阵的外积，限制了其能力。QuanTA 受到量子电路的启发，使用在（重塑的）输入特定轴上操作的张量，实现了高秩参数化。得益于普适性定理和秩表示定理，QuanTA
    能够有效表示任意矩阵，使其能够在仅使用一部分参数的情况下实现与完全微调相当或有时甚至更好的性能。注意：性能图是概念性示意图。
- en: 'Inspired by these advancements, we propose Quantum-informed Tensor Adaptation
    (QuanTA) ¹¹1[https://github.com/quanta-fine-tuning/quanta](https://github.com/quanta-fine-tuning/quanta)
    a novel, easy-to-implement, fine-tuning method with no inference overhead inspired
    by quantum circuits (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuanTA: Efficient
    High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")). QuanTA
    enables efficient high-rank adaptations by utilizing tensor operations analogous
    to those in quantum circuits, addressing the limitations inherent in low-rank
    methods like LoRA.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '受到这些进展的启发，我们提出了量子感知张量适应（QuanTA）¹¹1[https://github.com/quanta-fine-tuning/quanta](https://github.com/quanta-fine-tuning/quanta)，一种新颖、易于实现的微调方法，不增加推理开销，受到量子电路的启发（图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation")）。QuanTA 通过利用类似于量子电路中的张量操作，实现了高效的高秩适应，解决了像
    LoRA 这样的低秩方法固有的限制。'
- en: 'In summary, our contributions are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的贡献如下：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We introduce QuanTA, a novel, easy-to-implement, PEFT method with no inference
    overhead inspired by quantum circuits, enabling efficient high-rank fine-tuning
    without additional inference latency and offering the potential for integration
    with other existing PEFT methods for further enhancement.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 QuanTA，一种新颖、易于实现的 PEFT 方法，不增加推理开销，受到量子电路的启发，能够高效地进行高秩微调而无需额外的推理延迟，并提供了与其他现有
    PEFT 方法整合以进一步增强的潜力。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We present the universality theorem and the rank representation theorem, theoretically
    proving that QuanTA can efficiently parameterize high-rank matrices, overcoming
    the limitations of low-rank methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了普适性定理和秩表示定理，理论上证明了 QuanTA 能够高效地参数化高秩矩阵，克服了低秩方法的限制。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We validate QuanTA’s performance through extensive experiments, demonstrating
    significant improvements in various reasoning tasks and efficiency compared to
    traditional methods.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过大量实验验证了QuanTA的性能，展示了与传统方法相比，在各种推理任务和效率上都有显著改进。
- en: 2 Related Works
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Parameter-Efficient Fine-Tuning (PEFT) methods aim to address the computational
    burdens associated with fine-tuning large-scale models by adjusting a relatively
    small fraction of the total parameters to fit a specific downstream task. Roughly
    speaking, there are three existing categories of PEFT methods:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法旨在通过调整相对较小的参数比例来应对微调大规模模型所带来的计算负担，以适应特定的下游任务。大体上，PEFT方法存在三种现有类别：
- en: '1.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Adapter-based methods. These methods introduce additional trainable modules
    into the structure of a pre-trained, otherwise frozen, model. These modules can
    be integrated in various ways: series adapters are interposed between existing
    layers like attention or MLP components [[9](#bib.bib9), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)], while parallel adapters coexist alongside these components
    [[17](#bib.bib17)]. In general, these methods tend to increase the inference load
    due to the extra components that are not readily integrated into the original
    model weights.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于适配器的方法。这些方法在预训练的、否则冻结的模型结构中引入额外的可训练模块。这些模块可以通过各种方式集成：系列适配器被插入到现有的层之间，如注意力层或MLP组件[[9](#bib.bib9),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]，而并行适配器与这些组件并存[[17](#bib.bib17)]。一般来说，这些方法由于额外组件的存在，往往会增加推理负担，因为这些组件不能轻易地集成到原始模型权重中。
- en: '2.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Prompt/Prefix-based methods. These methods employ additional prompts or soft
    tokens at the beginning of the input sequence, focusing fine-tuning efforts on
    these newly introduced vector embeddings while maintaining the original model
    weights static [[18](#bib.bib18), [19](#bib.bib19)]. However, this approach can
    suffer from suboptimal performance and increased inference times. In addition,
    the soft tokens take up space of real tokens and therefore reduce the effective
    context size available for the model.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于提示/前缀的方法。这些方法在输入序列的开头使用额外的提示或软令牌，将微调工作集中在这些新引入的向量嵌入上，同时保持原始模型权重不变[[18](#bib.bib18),
    [19](#bib.bib19)]。然而，这种方法可能会出现性能不佳和推理时间增加的问题。此外，软令牌占用了真实令牌的空间，因此减少了模型可用的有效上下文大小。
- en: '3.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Reparameterization-based methods. These methods modify the existing weights
    with some parameter-efficient parameterization during the fine-tuning phase. Among
    these methods, Low-Rank Adaptation (LoRA) [[10](#bib.bib10)] and its variants,
    such as DoRA [[20](#bib.bib20)] and VeRA [[21](#bib.bib21)], are particularly
    noteworthy for their widespread adoption and robust performance across various
    tasks. In addition to LoRA, many other PEFT methods also belong to this category,
    including more sophisticated approaches such as Hadamard [[22](#bib.bib22)], Kronecker
    product [[23](#bib.bib23)] reparameterizations as well as many other methods [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]. Crucially, methods in this
    category do not impose additional inference burdens after fine-tuning as the modified
    weights can be merged into the pre-trained model weights prior to deployment.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于再参数化的方法。这些方法在微调阶段使用某些参数高效的参数化来修改现有权重。在这些方法中，低秩适应（LoRA）[[10](#bib.bib10)]及其变体，如DoRA
    [[20](#bib.bib20)]和VeRA [[21](#bib.bib21)]，因其广泛应用和在各种任务中的强大表现而特别值得注意。除了LoRA，许多其他PEFT方法也属于这一类别，包括更复杂的方法，如Hadamard
    [[22](#bib.bib22)]、Kronecker积 [[23](#bib.bib23)]再参数化以及许多其他方法[[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]。至关重要的是，这一类别的方法在微调后不会增加额外的推理负担，因为修改后的权重可以在部署前合并到预训练模型权重中。
- en: Besides these three categories, there are additional PEFT methods such as LoTA
    [[12](#bib.bib12)], where tensor decompositions are performed across multiple
    weights, LoRETTA [[13](#bib.bib13)], which uses tensor train decomposition for
    each weight matrix and has both adapter-based and reparameterization-based variants,
    MPO-based fine-tuning [[28](#bib.bib28)], and very recently LISA [[29](#bib.bib29)],
    ReFT [[30](#bib.bib30)] and MoRA [[31](#bib.bib31)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三类之外，还有其他PEFT方法，如LoTA [[12](#bib.bib12)]，其在多个权重上进行张量分解，LoRETTA [[13](#bib.bib13)]，它对每个权重矩阵使用张量训练分解，并具有基于适配器和再参数化的变体，MPO-based微调[[28](#bib.bib28)]，以及最近的LISA
    [[29](#bib.bib29)]、ReFT [[30](#bib.bib30)]和MoRA [[31](#bib.bib31)]。
- en: Physics-inspired machine learning In parallel, there have been various attempts
    to integrate physics-based priors into machine learning for many years. Symmetries
    and physics structure have been incorporated into the neural networks architecture
    and training in various applications to achieve notable performance [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]. Various classical and quantum physics processes
    have been utilized to design new neural networks [[40](#bib.bib40), [41](#bib.bib41)]
    and generative models [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 受物理启发的机器学习：与此同时，多年来已经有各种尝试将基于物理的先验知识融入机器学习。对称性和物理结构被融入到神经网络的架构和训练中，以实现显著的性能 [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]。各种经典和量子物理过程被用来设计新的神经网络 [[40](#bib.bib40),
    [41](#bib.bib41)] 和生成模型 [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)]。
- en: '3 Motivation: Low Rank is not Always Sufficient'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机：低秩并不总是足够的
- en: '![Refer to caption](img/187795ce528e94b5b9778c76afcd9093.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/187795ce528e94b5b9778c76afcd9093.png)'
- en: 'Figure 2: Subspace similarities between two LoRA experiments of different ranks
    (64 and 128) for two datasets. Each point $(i,j)$ right singular vectors of the
    $r=128$ are plotted. DROP dataset has a significantly high “intrinsic rank” than
    RTE dataset.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：两个LoRA实验的不同秩（64和128）在两个数据集上的子空间相似度。每个点$(i,j)$表示$r=128$的右奇异向量。DROP数据集的“固有秩”显著高于RTE数据集。
- en: Model Accuracy/$F_{1}$ LoRA[r=64] 86.0 55.2 $\text{LLaMA2}_{\text{7B}}$ LoRA[r=128]
    85.8 56.2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型准确率/$F_{1}$ LoRA[r=64] 86.0 55.2 $\text{LLaMA2}_{\text{7B}}$ LoRA[r=128] 85.8
    56.2
- en: 'Table 1: Performance of base and LoRA fine-tuned LLaMA2-7B on RTE [[49](#bib.bib49)]
    and DROP [[50](#bib.bib50)] datasets. We use accuracy and $F_{1}$-score as the
    metrics for them respectively.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基础模型和LoRA微调的LLaMA2-7B在RTE [[49](#bib.bib49)] 和DROP [[50](#bib.bib50)] 数据集上的性能。我们分别使用准确率和$F_{1}$-分数作为评估指标。
- en: LoRA operates under the hypothesis that parameter updates during fine-tuning
    exhibit a low “intrinsic rank.” For a pretrained weight matrix $W_{0}\in\mathbb{R}^{d\times
    k}$ are low-rank matrices. In this configuration, only $A$ is limited to $r$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA在微调过程中操作的假设是参数更新表现出低“固有秩”。对于预训练的权重矩阵$W_{0}\in\mathbb{R}^{d\times k}$，它们是低秩矩阵。在这种配置下，只有$A$被限制为$r$。
- en: 'Although the original LoRA paper shows empirical evidence to support the low-rank
    hypothesis, recently it has been found that this hypothesis may still fail for
    more complex tasks, especially for those that significantly differ from the pre-training
    dataset, leading to suboptimal performance [[11](#bib.bib11), [31](#bib.bib31)].
    To assess the general applicability of the low-rank hypothesis, we examine two
    datasets of varying difficulties: the RTE dataset [[49](#bib.bib49)], a classification
    task where the model is tasked to verify the correctness of statements, and the
    DROP dataset [[50](#bib.bib50)], a generation task where the model performs discrete
    reasoning over paragraphs. We posit that the RTE dataset is simpler, thus more
    likely to conform to the low-rank hypothesis, whereas the DROP dataset presents
    a greater challenge.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始LoRA论文提供了支持低秩假设的实证证据，但最近发现该假设可能在更复杂的任务中仍然会失败，尤其是那些与预训练数据集显著不同的任务，导致性能不佳
    [[11](#bib.bib11), [31](#bib.bib31)]。为了评估低秩假设的普遍适用性，我们检查了两个难度不同的数据集：RTE数据集 [[49](#bib.bib49)]，这是一个分类任务，模型的任务是验证陈述的正确性，以及DROP数据集
    [[50](#bib.bib50)]，这是一个生成任务，模型对段落进行离散推理。我们认为RTE数据集较简单，因此更有可能符合低秩假设，而DROP数据集则提出了更大的挑战。
- en: 'As shown in Table [1](#S3.T1 "Table 1 ‣ Figure 2 ‣ 3 Motivation: Low Rank is
    not Always Sufficient ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation"), the LLaMA2-7B model [[6](#bib.bib6)] in general can achieve
    a better score on the RTE dataset than the DROP dataset. In addition, as we increase
    the rank from 64 to 128, LoRA’s performance on the RTE dataset remains the same,
    consistent with the low-rank hypothesis, while the performance on the DROP dataset
    improves, suggesting the DROP dataset may require a higher “intrinsic rank.”'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [1](#S3.T1 "Table 1 ‣ Figure 2 ‣ 3 Motivation: Low Rank is not Always Sufficient
    ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation") 所示，LLaMA2-7B 模型 [[6](#bib.bib6)] 通常在 RTE 数据集上的得分优于 DROP 数据集。此外，随着秩从
    64 增加到 128，LoRA 在 RTE 数据集上的表现保持不变，与低秩假设一致，而在 DROP 数据集上的表现有所提高，表明 DROP 数据集可能需要更高的“内在秩”。'
- en: 'To further measure the “intrinsic rank” of weight updates for these datasets,
    we follow the methodology outlined in [[10](#bib.bib10)] and compare the subspace
    spanned by the right singular vectors of the resulting weight updates between
    the $r=64$ singular vectors of the $r=64$ values, and quickly decays to 0 for
    larger $i$, indicating that fine-tuning on the RTE dataset has a low “intrinsic
    rank.” Conversely, for the DROP dataset, subspace similarity remains large across
    all 64 singular vectors, demonstrating a high “intrinsic rank.” Additional details
    of subspace similarity and addition data are provide in Appendix [A](#A1 "Appendix
    A Additional Details on Subspace Similarity ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation")'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步测量这些数据集的权重更新的“内在秩”，我们遵循[[10](#bib.bib10)] 中概述的方法，并比较结果权重更新的右奇异向量所张成的子空间与
    $r=64$ 奇异向量的子空间。对于较大的 $i$，快速衰减至 0，表明在 RTE 数据集上微调具有较低的“内在秩”。相反，对于 DROP 数据集，子空间相似性在所有
    64 个奇异向量中保持较大，显示出较高的“内在秩”。子空间相似性和额外数据的详细信息请参见附录 [A](#A1 "Appendix A Additional
    Details on Subspace Similarity ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs
    with Quantum-Informed Tensor Adaptation")'
- en: 'These findings demonstrate the necessity of high-rank fine-tuning in complex
    tasks, challenging the effectiveness of LoRA. This naturally prompts the following
    question: *How can we design efficient methods to facilitate high-rank updates
    during fine-tuning?*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，在复杂任务中进行高阶微调的必要性，挑战了 LoRA 的有效性。这自然引发了以下问题：*我们如何设计高效的方法以促进在微调过程中进行高阶更新？*
- en: '4 Preliminary: Quantum Circuit'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 初步：量子电路
- en: The behavior of quantum mechanical systems, especially those involving particles
    with discrete degrees of freedom, is well described by matrix theory. Quantum
    circuits naturally realize unitary matrices whose sizes grow exponentially with
    the number of particles, providing a potent framework for high-rank representation.
    Here, we review some fundamental concepts of quantum states and quantum circuits
    to motivate our approach.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 量子力学系统的行为，特别是涉及具有离散自由度的粒子的系统，可以通过矩阵理论得到很好的描述。量子电路自然实现了单位矩阵，其大小随着粒子数量的指数增长，提供了一个强大的高阶表示框架。在这里，我们回顾一些量子态和量子电路的基本概念，以激发我们的方法。
- en: Quantum state and vector representation. An $N$ the components and $\ket{i}$
    is the binary representation of $i$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 量子态和向量表示。一个 $N$ 组件，$\ket{i}$ 是 $i$ 的二进制表示。
- en: '![Refer to caption](img/2af8c2cc725bd032e56b17802fb99656.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2af8c2cc725bd032e56b17802fb99656.png)'
- en: 'Figure 3: Any unitary matrix can be decomposed into a quantum circuit using
    one- and two-qubit gates.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：任何单位矩阵都可以通过一量子比特和两量子比特门分解为量子电路。
- en: 'Quantum circuit and matrix representation. A quantum circuit is a unitary matrix
    $\mathcal{U}\in\mathbb{U}(2^{N})\subset\mathbb{C}^{2^{N}\times 2^{N}}$²²2Typically,
    quantum circuits and quantum gates are considered within the group $\mathbb{SU}(2^{N})$
    factor, which does not affect the results presented in this paper.. These gates
    are applied to specific qubits as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 量子电路和矩阵表示。量子电路是一个单位矩阵 $\mathcal{U}\in\mathbb{U}(2^{N})\subset\mathbb{C}^{2^{N}\times
    2^{N}}$²²2通常，量子电路和量子门被考虑在群 $\mathbb{SU}(2^{N})$ 因子内，这不会影响本文所呈现的结果.. 这些门被应用于特定的量子比特，如下所示：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: for a one-qubit gate applied to qubit $n$, and
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用于量子比特 $n$ 的一量子比特门，以及
- en: '|  | $1$2 |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: for a two-qubit gate applied to qubits $m$ do not need to be consecutive qubits.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用于量子比特 $m$ 的两量子比特门，不需要是连续的量子比特。)
- en: 'A quantum circuit comprises a series of these one- and two-qubit gates $\{U^{(\alpha)}\}$
    applied sequentially to the quantum state:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个量子电路由一系列这种一量子比特和双量子比特门$\{U^{(\alpha)}\}$按顺序应用于量子态：
- en: '|  | $\mathcal{U}\ket{\psi}=\prod_{\alpha}U^{(\alpha)}\ket{\psi}.$ |  | (3)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{U}\ket{\psi}=\prod_{\alpha}U^{(\alpha)}\ket{\psi}.$ |  | (3)
    |'
- en: Since quantum circuits are unitary, they inherently represent full-rank matrices
    in finite-dimensional systems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于量子电路是单位的，它们本质上表示有限维系统中的全秩矩阵。
- en: 'Universality of quantum circuit. Similar to the universal approximation theorem
    for neural networks, it has been established that any quantum circuit on $N$ qubits
    can be decomposed into a quantum circuit using only one- and two-qubit gates [[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53)], as shown in Figure [3](#S4.F3 "Figure 3 ‣
    4 Preliminary: Quantum Circuit ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs
    with Quantum-Informed Tensor Adaptation"). This is particularly relevant for reparameterization-based
    fine-tuning methods, where we aim to parameterize a matrix matching the shape
    of the base model’s weight matrix using a small number of parameters.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 量子电路的普适性。类似于神经网络的普适近似定理，已经建立了任何在$N$个量子比特上的量子电路都可以分解为仅使用一量子比特和双量子比特门的量子电路[[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53)]，如图[3](#S4.F3 "图 3 ‣ 4 初步：量子电路 ‣ QuanTA：使用量子信息张量适配对LLM进行高效高阶微调")所示。这对于基于重参数化的微调方法尤其相关，我们的目标是使用少量参数对与基础模型权重矩阵形状匹配的矩阵进行参数化。
- en: 5 Quantum-informed Tensor Adaptation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 量子信息张量适配
- en: 'Since quantum circuits offer an elegant parameterization for large unitary
    matrices of shape $2^{N}\times 2^{N}$, by relaxing the unitarity constraint and
    allowing for arbitrary local dimensions, we can develop an effective tool for
    high-rank, parameter-efficient fine-tuning. Inspired by this, we propose QuanTA:
    Quantum-informed Tensor Adaptation, which parameterizes the parameter updates
    in a way analogous to a quantum circuit.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于量子电路为形状为$2^{N}\times 2^{N}$的大单位矩阵提供了优雅的参数化，通过放宽单位约束并允许任意局部维度，我们可以开发出一种高效的高阶、参数高效的微调工具。受到此启发，我们提出了QuanTA：量子信息张量适配，它以类似于量子电路的方式对参数更新进行参数化。
- en: Construction. To illustrate the construction of QuanTA, we focus on the case
    of square weight matrices $W\in\mathbb{R}^{d\times d}$ does not need to be prime
    and the decomposition does not need to be unique. This condition is often satisfied
    for large language models. By reshaping $x\in\mathbb{R}^{d}$th axis corresponding
    to a qudit with local dimension $d_{n}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 构造。为了说明QuanTA的构造，我们专注于平方权重矩阵$W\in\mathbb{R}^{d\times d}$的情况，该矩阵不需要是素数，且分解不需要是唯一的。此条件通常适用于大型语言模型。通过将$x\in\mathbb{R}^{d}$的轴重新塑造为具有局部维度$d_{n}$的qudit。
- en: Similar to a quantum circuit, QuanTA consists of “gates” (or tensors) that apply
    to only specific axes. Since single-axis gates are subsets of two-axis gates,
    it suffices to consider parameterizations using only two-axis gates. Let $T^{(\alpha)}$th
    axes with corresponding dimensions $d_{m^{(\alpha)}}$. Analogous to applying a
    two-qubit gate to a quantum state, applying this tensor to the hidden vector is
    defined as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于量子电路，QuanTA由仅适用于特定轴的“门”（或张量）组成。由于单轴门是双轴门的子集，因此仅考虑使用双轴门的参数化就足够了。设第$T^{(\alpha)}$轴对应的维度为$d_{m^{(\alpha)}}$。类似于将双量子比特门应用于量子态，将该张量应用于隐藏向量的定义为
- en: '|  | $1$2 |  | (4) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where the $\alpha$th axes created as batch dimensions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第$\alpha$轴被创建为批次维度。
- en: 'QuanTA is then constructed by sequentially applying a collection of such tensors
    $\{T^{(\alpha)}\}$ in the same manner as a quantum circuit:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: QuanTA通过按顺序应用这些张量集合$\{T^{(\alpha)}\}$以与量子电路相同的方式构建：
- en: '|  | $\mathcal{T}x:=\prod_{\alpha}T^{(\alpha)}x.$ |  | (5) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{T}x:=\prod_{\alpha}T^{(\alpha)}x.$ |  | (5) |'
- en: 'Although it is difficult to write the full Eq. ([5](#S5.E5 "In 5 Quantum-informed
    Tensor Adaptation ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation")) in index notation for an arbitrary set of tensors, we demonstrate
    in Appendix [F](#A6 "Appendix F Systematical Way to Generate einsum Expressions
    ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation") that the einsum expression for this operation can be systematically
    generated.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管对任意一组张量来说，完整的方程式 ([5](#S5.E5 "在 5 量子信息张量自适应 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应"))
    用索引记号表示是困难的，但我们在附录 [F](#A6 "附录 F 系统生成 einsum 表达式 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应")
    中展示了该操作的 einsum 表达式可以系统地生成。'
- en: 'As a concrete example of translating Eq. ([5](#S5.E5 "In 5 Quantum-informed
    Tensor Adaptation ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation")) to index notations and einsum, consider the case of $N=3$
    consists of three tensors, each applied to two axes (as depicted in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with
    Quantum-Informed Tensor Adaptation")). In this case, it is easy to express in
    index notation the application of the QuanTA operator to the hidden vector;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '作为将方程式 ([5](#S5.E5 "在 5 量子信息张量自适应 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应")) 转换为索引记号和
    einsum 的具体例子，考虑 $N=3$ 的情况，其中包含三个张量，每个张量作用于两个轴（如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ QuanTA:
    高效高秩微调 LLM 的量子信息张量自适应") 所示）。在这种情况下，容易用索引记号表示 QuanTA 操作符对隐藏向量的作用；'
- en: '|  | $1$2 |  | (6) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: as well as the calculation of the full QuanTA matrix;
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以及计算完整的 QuanTA 矩阵；
- en: '|  | $1$2 |  | (7) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 'Although Eq. [6](#S5.E6 "In 5 Quantum-informed Tensor Adaptation ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")
    and [7](#S5.E7 "In 5 Quantum-informed Tensor Adaptation ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation") may look complex
    in their formulation, in practice they can be easily implemented respectively
    using einsum as'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管方程 [6](#S5.E6 "在 5 量子信息张量自适应 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应") 和 [7](#S5.E7
    "在 5 量子信息张量自适应 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应") 在其公式上可能看起来很复杂，但实际上可以分别使用 einsum
    轻松实现。'
- en: torch.einsum("…abc,efbc,diaf,ghde->…ghi",  x,  T_3,  T_2,  T_1)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: torch.einsum("…abc,efbc,diaf,ghde->…ghi",  x,  T_3,  T_2,  T_1)
- en: and
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: torch.einsum("efbc,diaf,ghde->ghiabc",  T_3,  T_2,  T_1)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: torch.einsum("efbc,diaf,ghde->ghiabc",  T_3,  T_2,  T_1)
- en: Initialization method. At initialization, the adapted model should be the same
    as the base model and all the weight updates should be 0\. However, enforcing
    $\mathcal{T}x=0$, impeding gradient propagation through the tensors and negatively
    impacting training performance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化方法。在初始化时，适应模型应与基模型相同，且所有的权重更新应为 0。然而，强制 $\mathcal{T}x=0$ 会阻碍张量中的梯度传播，并对训练性能产生负面影响。
- en: To address this issue, we use another set of tensors $\{S^{(\alpha)}\}$ but
    remain frozen throughout fine-tuning. We then define the adapted layer as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用另一组张量 $\{S^{(\alpha)}\}$，但在整个微调过程中保持冻结。然后我们将适应层定义为
- en: '|  | $y=W_{\theta}x:=W_{0}x+\mathcal{T}_{\theta}x-\mathcal{S}x,$ |  | (8) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=W_{\theta}x:=W_{0}x+\mathcal{T}_{\theta}x-\mathcal{S}x,$ |  | (8) |'
- en: where we use the subscript $\theta$ exactly cancel out, ensuring the adapted
    layer reduces to the base model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用下标 $\theta$ 完全抵消，确保适应层简化为基模型。
- en: It is important to note that this initialization method does not introduce additional
    costs. After initialization, the full $\mathcal{S}$ and simplify the adapted layer
    to
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这种初始化方法不会引入额外的成本。在初始化之后，完整的 $\mathcal{S}$ 被简化为适应层为
- en: '|  | $y=W_{\theta}x=W_{0}^{\prime}x+\mathcal{T}_{\theta}x.$ |  | (9) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=W_{\theta}x=W_{0}^{\prime}x+\mathcal{T}_{\theta}x.$ |  | (9) |'
- en: 6 Theoretical Results
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 理论结果
- en: 'Here, we list a few important theorem and provide the proofs in Appendix [C](#A3
    "Appendix C Additional Theoretical Results and Proofs ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们列出了一些重要的定理，并在附录 [C](#A3 "附录 C 额外的理论结果和证明 ‣ QuanTA: 高效高秩微调 LLM 的量子信息张量自适应")
    中提供了证明'
- en: Theorem 6.1  (Universality of QuanTA).
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 6.1（QuanTA 的普遍性）。
- en: Let $W$ is a power of 2 and $\prod_{n}d_{n}=2^{M}$ and $d_{n^{(\alpha)}}$.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $W$ 是 2 的幂且 $\prod_{n}d_{n}=2^{M}$ 和 $d_{n^{(\alpha)}}$。
- en: We note that the fine-tuning method KronA [[23](#bib.bib23)] can be incorporated
    into our framework and considered as a special case of QuanTA.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到微调方法 KronA [[23](#bib.bib23)] 可以被纳入我们的框架，并视为 QuanTA 的一个特例。
- en: Theorem 6.2  (Rank representation).
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 6.2 （秩表示）。
- en: Let $R=r(\mathcal{T})$, $d^{(\alpha)}=d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ be the
    total number of tensors. The following inequality always holds
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $R=r(\mathcal{T})$，$d^{(\alpha)}=d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ 为张量的总数。以下不等式总是成立
- en: '|  | $\sum_{\alpha}\frac{dR^{(\alpha)}}{d^{(\alpha)}}-d(N_{T}-1)\leq R\leq\min_{\alpha}{\frac{dR^{(\alpha)}}{d^{(\alpha)}}}.$
    |  | (10) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{\alpha}\frac{dR^{(\alpha)}}{d^{(\alpha)}}-d(N_{T}-1)\leq R\leq\min_{\alpha}{\frac{dR^{(\alpha)}}{d^{(\alpha)}}}.$
    |  | (10) |'
- en: In the special case when all the tensors are full rank ($R^{{\alpha}}=d^{(\alpha)}$).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有张量都是满秩的特例中 ($R^{{\alpha}}=d^{(\alpha)}$)。
- en: Theorem 6.3  (Composition openness).
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 6.3 （组合开放性）。
- en: There exists a set of matrices $\mathbb{S}=\{\mathcal{M}_{k}\}$.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个矩阵集合 $\mathbb{S}=\{\mathcal{M}_{k}\}$。
- en: We note that the composition openness is not satisfied by LoRA. This is because,
    for any two matrices with LoRA structure, the composition of the matrices still
    falls into the set of matrices with LoRA structure, which limits the expressivity
    of LoRA. In contrast, the composition openness of QuanTA indicates that the expressivity
    of QuanTA can increase as the depth of the neural network increases, even if the
    neural network is close to linear.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到 LoRA 不满足组合开放性。这是因为对于任何两个具有 LoRA 结构的矩阵，这些矩阵的组合仍然属于 LoRA 结构的矩阵集合，这限制了 LoRA
    的表达能力。相比之下，QuanTA 的组合开放性表明，尽管神经网络接近线性，QuanTA 的表达能力可以随着神经网络深度的增加而增加。
- en: No inference overhead. As reparameterization-based methods, QuanTA does not
    impose any inference latency, since the trained $\mathcal{T}$ operator can be
    explicitly constructed as a matrix and merged into the base model weight matrix.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无推理开销。作为基于重新参数化的方法，QuanTA 不会带来任何推理延迟，因为训练好的 $\mathcal{T}$ 运算符可以被显式构造为矩阵，并合并到基础模型权重矩阵中。
- en: Memory and computational complexity during fine-tuning. In the forward pass,
    only a hidden vector of size $d$th axes, so the total computational complexity
    for a QuanTA layer is $d\cdot\sum_{\alpha}d_{m^{(\alpha)}}d_{n^{(\alpha)}}$ for
    all $m$, QuanTA reduces to full fine-tuning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程中的内存和计算复杂性。在前向传递中，仅需 $d$ 维轴的隐藏向量，因此 QuanTA 层的总计算复杂性为 $d\cdot\sum_{\alpha}d_{m^{(\alpha)}}d_{n^{(\alpha)}}$，对于所有
    $m$，QuanTA 退化为完全微调。
- en: 7 Experiments
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个实验
- en: 'To benchmark QuanTA against other fine-tuning methods, we performed experiments
    on a wide range of datasets (see Appendix [D](#A4 "Appendix D Datasets ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")
    for details). For all experiments, we avoid optimizing the hyperparameters on
    the test set. Instead, we create a validation set from the train set and optimize
    the hyperparameters on the validation set. All the results reported in this section
    are averaged over multiple experiments with varying random seeds, and the term
    “parameters” and “# params” in this section always refer to the trainable parameters.
    Details on the experiments and hyperparameters are shown in Appendix [E](#A5 "Appendix
    E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将 QuanTA 与其他微调方法进行基准比较，我们在多种数据集上进行了实验（详情见附录 [D](#A4 "附录 D 数据集 ‣ QuanTA: 量子信息张量适配的高秩高效微调")）。在所有实验中，我们避免在测试集上优化超参数。相反，我们从训练集中创建一个验证集，并在验证集上优化超参数。本节报告的所有结果都是在不同随机种子的多次实验中取平均值，并且本节中的“参数”和“#
    params”总是指可训练参数。实验和超参数的详细信息见附录 [E](#A5 "附录 E 超参数和实验细节 ‣ QuanTA: 量子信息张量适配的高秩高效微调")。'
- en: 'Model PEFT Method # Params (%) $F_{1}$ (Ours) 0.041% 59.5 QuanTA${}_{16\textrm{-}16\textrm{-}16}$
    LoRA[r=8] 0.024% 74.3 QuanTA${}_{16\textrm{-}8\textrm{-}8\textrm{-}8}$ (Ours)
    0.014% 79.4'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 PEFT 方法 # 参数 (%) $F_{1}$ (我们的方法) 0.041% 59.5 QuanTA${}_{16\textrm{-}16\textrm{-}16}$
    LoRA[r=8] 0.024% 74.3 QuanTA${}_{16\textrm{-}8\textrm{-}8\textrm{-}8}$ (我们的方法)
    0.014% 79.4'
- en: 'Table 2: Benchmark of various fine-tuning methods on the DROP dataset using
    LLaMA2 7-70 billion parameter models as the base model. In each case, we report
    the average of $F_{1}$ score over 2-4 experiments with different random seeds.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用 LLaMA2 70 亿参数模型作为基础模型，在 DROP 数据集上对各种微调方法进行基准测试。在每种情况下，我们报告了 2-4 次不同随机种子的实验中
    $F_{1}$ 分数的平均值。
- en: '![Refer to caption](img/967540d8a8b2bba194f3f51325084b84.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/967540d8a8b2bba194f3f51325084b84.png)'
- en: 'Figure 4: Benchmark of different fine-tuning methods on the DROP dataset as
    a function of training parameters using LLaMA2 7 billion parameter model as the
    base model.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在DROP数据集上，使用LLaMA2 70亿参数模型作为基模型，对不同微调方法的基准测试结果，作为训练参数的函数。
- en: 'DROP Dataset. We begin our benchmark with the DROP dataset [[50](#bib.bib50)],
    chosen as a representative example that requires high-rank fine-tuning. In Table [2](#S7.T2
    "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation"), we compare our QuanTA method
    with LoRA of different ranks, as well as series and parallel adapters, by fine-tuning
    LLaMA2 [[6](#bib.bib6)] with up to 70 billion parameters.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'DROP数据集。我们以DROP数据集[[50](#bib.bib50)]作为基准测试的起点，该数据集被选为需要高秩微调的代表性例子。在表[2](#S7.T2
    "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation")中，我们将QuanTA方法与不同秩的LoRA、以及系列和并行适配器进行比较，通过对具有最多70亿参数的LLaMA2[[6](#bib.bib6)]进行微调。'
- en: 'As shown in Table [2](#S7.T2 "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation"),
    LoRA consistently underperforms compared to other fine-tuning methods. While increasing
    the rank improves performance, LoRA still falls short, suggesting the necessity
    of high-rank fine-tuning for this task. In addition, QuanTA achieves performance
    on par with, or better than, full fine-tuning using only a a small fraction of
    the parameters, demonstrating the effectiveness of QuanTA’s high-rank fine-tuning
    capability.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](#S7.T2 "Table 2 ‣ Figure 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation")所示，与其他微调方法相比，LoRA的表现
    consistently 较差。虽然提高秩可以改善性能，但LoRA依然不如其他方法，这表明此任务需要高秩微调。此外，QuanTA在仅使用少量参数的情况下，达到或超过完全微调的性能，展示了QuanTA高秩微调能力的有效性。'
- en: 'To investigate how these methods scale with the number of trainable parameters,
    we conducted experiments varying the number of trainable parameters on LLaMA2-7B
    model. The results are shown in Fig. [4](#S7.F4 "Figure 4 ‣ 7 Experiments ‣ QuanTA:
    Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation").
    Each point in the figure represents an average of four experiments with different
    random seeds, and the standard deviation across these experiments is shown as
    error bars ⁴⁴4We vary both the random seed for model initialization and the sampled
    train, dev, and test datasets, which could be the reason of large standard deviations..'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究这些方法如何随着可训练参数数量的增加而扩展，我们在LLaMA2-7B模型上进行了实验，改变了可训练参数的数量。结果如图[4](#S7.F4 "Figure
    4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation")所示。图中的每个点代表四次实验的平均值，实验的标准差以误差条表示⁴⁴4我们变更了模型初始化的随机种子以及采样的训练、验证和测试数据集，这可能是标准差较大的原因。'
- en: As illustrated in the figure, QuanTA achieves performance comparable to or better
    than full fine-tuning using a small fraction of trainable parameters. Conversely,
    LoRA only achieves subpar performance with a small number of trainable parameters,
    though its performance improves with an increase in parameters. Other PEFT methods,
    such as series and parallel adapters, achieve results close to full fine-tuning
    but use significantly more parameters than QuanTA.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，QuanTA在使用少量可训练参数的情况下，达到了与完全微调相当或更好的性能。相反，尽管LoRA的性能随着参数数量的增加有所改善，但在少量可训练参数下仍表现不佳。其他PEFT方法，如系列和并行适配器，虽然接近完全微调的结果，但使用的参数显著多于QuanTA。
- en: 'Model PEFT Method # Params (%) Accuracy ($\uparrow$ FT 100% 71.3 82.1 78.6
    90.2 79.0 82.9 67.2 76.8 78.5 Prefix* 0.11% 64.3 76.8 73.9 42.1 72.1 72.9 54.0
    60.6 64.6 Series* 0.99% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel*
    3.54% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3 LoRA* 0.83% 68.9 80.7 77.4
    78.1 78.8 77.8 61.3 74.8 74.7 DoRA^† 0.43% 70.0 82.6 79.7 83.2 80.6 80.6 65.4
    77.6 77.5 DoRA^† 0.84% 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 QuanTA (Ours)
    0.041% 71.6 83.0 79.7 91.8 81.8 84.0 68.3 82.1 80.3 $\text{LLaMA}_{\text{13B}}$
    LoRA^† 0.70% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA^† 0.35% 74.5 88.8
    80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA^† 0.71% 74.6 89.3 79.9 95.5 85.6 90.5
    80.4 85.8 85.2 QuanTA (Ours) 0.035% 74.3 88.1 81.8 95.1 87.3 91.1 81.7 87.2 85.8'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 PEFT 方法 # 参数 (%) 准确率 ($\uparrow$ FT 100% 71.3 82.1 78.6 90.2 79.0 82.9 67.2
    76.8 78.5 Prefix* 0.11% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series* 0.99%
    63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel* 3.54% 67.9 76.4 78.8 69.8
    78.9 73.7 57.3 75.2 72.3 LoRA* 0.83% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7
    DoRA^† 0.43% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA^† 0.84% 69.7 83.4
    78.6 87.2 81.0 81.9 66.2 79.2 78.4 QuanTA (我们的方法) 0.041% 71.6 83.0 79.7 91.8 81.8
    84.0 68.3 82.1 80.3 $\text{LLaMA}_{\text{13B}}$ LoRA^† 0.70% 70.8 85.2 79.9 91.7
    84.3 84.2 71.2 79.0 80.8 DoRA^† 0.35% 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2
    85.0 DoRA^† 0.71% 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 QuanTA (我们的方法)
    0.035% 74.3 88.1 81.8 95.1 87.3 91.1 81.7 87.2 85.8'
- en: 'Table 3: Benchmark on various commonsense reasoning tasks. All results of models
    and PEFT methods labeled with “*” are from [[54](#bib.bib54)], and results with
    “^†” are from [[20](#bib.bib20)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：各种常识推理任务的基准测试。所有带有“*”标签的模型和PEFT方法的结果来自[[54](#bib.bib54)]，带有“^†”标签的结果来自[[20](#bib.bib20)]。
- en: Commonsense Reasoning. We continue to evaluate our method on a collection of
    commonsense reasoning datasets. Following the methodology in [[54](#bib.bib54)],
    we first fine-tune the model on the Commonsense170K dataset [[54](#bib.bib54)],
    a comprehensive collection of commonsense reasoning questions, and subsequently
    evaluate it on eight different downstream tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 常识推理。我们继续在一系列常识推理数据集上评估我们的方法。按照[[54](#bib.bib54)]中的方法，我们首先在Commonsense170K数据集[[54](#bib.bib54)]上对模型进行微调，该数据集是一个综合性的常识推理问题集合，然后在八个不同的下游任务上进行评估。
- en: 'In Table [3](#S7.T3 "Table 3 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation"), we benchmark our
    QuanTA method against other fine-tuning techniques using 7- and 13-billion-parameter
    LLaMA and LLaMA2 models, as well as the 8-billion-parameter LLaMA3 model. Alongside
    prefix tuning, adapter methods, and LoRA, we also compare our approach to the
    recently proposed LoRA variant, the DoRA method [[20](#bib.bib20)]. The results
    clearly indicate that our QuanTA method outperforms LoRA in all cases and surpasses
    the DoRA method in most benchmarks, using less than one-tenth of the parameters.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[3](#S7.T3 "表 3 ‣ 7 实验 ‣ QuanTA: 量子信息张量自适应的高效高阶微调")中，我们将我们的QuanTA方法与其他微调技术进行比较，使用7亿和13亿参数的LLaMA及LLaMA2模型，以及80亿参数的LLaMA3模型。除了前缀调优、适配器方法和LoRA外，我们还将我们的方法与最近提出的LoRA变体DoRA方法[[20](#bib.bib20)]进行比较。结果清晰地表明，我们的QuanTA方法在所有情况下都优于LoRA，并且在大多数基准测试中超越了DoRA方法，同时使用的参数少于十分之一。'
- en: 'Arithmetic Reasoning. We further test our method on arithmetic reasoning tasks
    by fine-tuning the model on Math10K dataset [[54](#bib.bib54)] and assessing its
    performance on four tasks. We note that while [[54](#bib.bib54)] includes additional
    downstream tasks in the arithmetic reasoning benchmark, some test data was later
    found to have leaked into the training dataset. In this study, we only benchmark
    the four downstream tasks unaffected by this data leakage. Additionally, our evaluation
    procedure differs slightly from that in [[54](#bib.bib54)] (see Appendix [E](#A5
    "Appendix E Hyperparameters and Experimental Details ‣ QuanTA: Efficient High-Rank
    Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation") for details).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '算术推理。我们进一步通过在Math10K数据集[[54](#bib.bib54)]上对模型进行微调，测试我们的方法在算术推理任务上的表现，并评估其在四个任务上的性能。我们注意到，虽然[[54](#bib.bib54)]包含了额外的算术推理基准下游任务，但一些测试数据后来发现泄漏到了训练数据集中。在这项研究中，我们仅基准测试未受数据泄漏影响的四个下游任务。此外，我们的评估程序与[[54](#bib.bib54)]中的略有不同（详细信息见附录[E](#A5
    "附录 E 超参数和实验细节 ‣ QuanTA: 量子信息张量自适应的高效高阶微调")）。'
- en: 'Table [4](#S7.T4 "Table 4 ‣ 7 Experiments ‣ QuanTA: Efficient High-Rank Fine-Tuning
    of LLMs with Quantum-Informed Tensor Adaptation") presents the evaluation results
    on the four downstream tasks. Notably, all questions in the AQuA dataset are multiple-choice
    with mostly five options, and all models except GPT-3.5 failed to achieve accuracy
    higher than 20%. Therefore, we conclude that all models perform equally poorly
    on this task and exclude it from the average accuracy computation. This phenomenon
    is also consistent with previous findings [[54](#bib.bib54), [20](#bib.bib20)].
    The results show that QuanTA significantly outperforms LoRA and even surpasses
    full fine-tuning with a small number of parameters. It is surprising that QuanTA
    exceeds full fine-tuning in these tasks, which may be due to overfitting or the
    challenges of optimizing hyperparameters for full fine-tuning.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S7.T4 "表 4 ‣ 7 实验 ‣ QuanTA：具有量子信息张量自适应的大型语言模型高效高秩微调")展示了四个下游任务的评估结果。值得注意的是，AQuA数据集中的所有问题都是选择题，大多数有五个选项，除了GPT-3.5之外，所有模型的准确率都未超过20%。因此，我们得出结论，所有模型在这项任务上的表现都同样糟糕，因此将其从平均准确率计算中排除。这一现象也与之前的研究结果一致
    [[54](#bib.bib54), [20](#bib.bib20)]。结果显示，QuanTA显著优于LoRA，甚至在参数较少的情况下超越了完全微调。令人惊讶的是，QuanTA在这些任务中超越了完全微调，这可能是由于过拟合或完全微调的超参数优化挑战。
- en: Limitations. QuanTA currently requires applying the tensors sequentially to
    the hidden vectors, which may result in underutilizing the GPU when the tensors
    are too small. It will be helpful to develop a more efficient implementation to
    fully utilize GPU resources. The hyperparameters in QuanTA, such as the number
    of tensors applying on the same axes, have not been optimized. Choosing an optimal
    set of tensors could further enhance the performance of QuanTA. In the current
    experiments, we only consider LLaMA model series and a thorough study on different
    models will be beneficial if more computational resources are available.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。QuanTA目前需要将张量按顺序应用于隐藏向量，这可能导致当张量过小时未充分利用GPU。如果能开发出更高效的实现来充分利用GPU资源，将会有所帮助。QuanTA中的超参数，例如在相同轴上应用的张量数量，尚未经过优化。选择最优的张量集可能进一步提升QuanTA的性能。在当前的实验中，我们仅考虑了LLaMA模型系列，如果有更多计算资源，对不同模型进行全面研究将会更有益。
- en: 'Model PEFT Method # Params (%) Accuracy ($\uparrow$ LoRA 0.67% 16.7 72.3 90.8
    84.3 82.5 QuanTA (Ours) 0.13% 18.9 72.4 94.5 84.8 83.9'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '模型 PEFT方法 # 参数（%） 准确率 ($\uparrow$ LoRA 0.67% 16.7 72.3 90.8 84.3 82.5 QuanTA（我们的方法）
    0.13% 18.9 72.4 94.5 84.8 83.9'
- en: 'Table 4: Benchmark on various arithmetic reasoning tasks. GPT-3.5 (labeled
    with “*”) results are taken from [[54](#bib.bib54)].'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：各种算术推理任务的基准测试。GPT-3.5（标记为“*”）的结果来自[[54](#bib.bib54)]。
- en: 8 Conclusion
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we introduced QuanTA, a novel, easy-to-implement, PEFT method
    with no inference overhead for large language models. QuanTA leverages quantum-inspired
    techniques to achieve high-rank adaptations, addressing the limitations of existing
    low-rank methods. QuanTA introduces high-rank fine-tuning through the universality
    theorem and rank representation theorem. Our extensive experiments demonstrate
    the efficacy of QuanTA across various tasks, including commonsense reasoning,
    arithmetic reasoning, and scalability. QuanTA consistently outperforms traditional
    fine-tuning methods and other PEFT approaches, achieving superior performance
    with a significantly smaller number of trainable parameters. This highlights the
    potential of quantum-informed techniques in enhancing the adaptability and efficiency
    of large language models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了QuanTA，这是一种新颖、易于实现的PEFT方法，不增加大语言模型的推理开销。QuanTA利用量子启发技术实现高秩自适应，解决了现有低秩方法的局限性。QuanTA通过普适性定理和秩表示定理引入高秩微调。我们的广泛实验展示了QuanTA在各种任务中的有效性，包括常识推理、算术推理和可扩展性。QuanTA始终优于传统的微调方法和其他PEFT方法，以显著更少的可训练参数实现了更优的性能。这突显了量子信息技术在提高大语言模型适应性和效率方面的潜力。
- en: QuanTA offers a scalable and efficient solution for fine-tuning large language
    models, advancing the state-of-the-art in natural language processing. There are
    several promising directions for future research and development of QuanTA. Expanding
    its application to a wider range of tasks and specialized domains could demonstrate
    its versatility and robustness. Combining QuanTA with other PEFT methods or incorporating
    it into ensemble models might further enhance performance, particularly for complex
    tasks. The parameter efficiency of QuanTA may also imply a lower chance of overfitting.
    Additionally, exploring advanced optimization techniques tailored specifically
    for QuanTA could improve convergence rates and overall efficiency. Further design
    based on principles from quantum computing, such as entanglement and superposition,
    may lead to even more efficient fine-tuning methods. Our work paves the way for
    further exploration of quantum-informed methods or even future quantum technologies
    for machine learning, making it a valuable approach for both research and practical
    applications with broader impacts.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: QuanTA提供了一种可扩展且高效的解决方案，用于微调大型语言模型，推动了自然语言处理的最先进技术。未来的QuanTA研究和开发有几个有前途的方向。将其应用扩展到更广泛的任务和专业领域可能展示其多样性和稳健性。将QuanTA与其他PEFT方法结合或将其融入集成模型中可能进一步提高性能，特别是对于复杂任务。QuanTA的参数效率也可能意味着较低的过拟合风险。此外，探索针对QuanTA量身定制的高级优化技术可能提高收敛速度和整体效率。基于量子计算原则，如纠缠和叠加，进行进一步设计可能导致更高效的微调方法。我们的工作为进一步探索量子信息方法或未来量子技术在机器学习中的应用铺平了道路，使其成为具有广泛影响的研究和实际应用的宝贵方法。
- en: Broader Impacts
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: The development of QuanTA represents an important advancement in the fine-tuning
    of LLMs, with profound societal implications. By leveraging quantum-informed methods,
    QuanTA reduces computational and memory demands, making advanced NLP capabilities
    more accessible and cost-effective. This democratization of AI technology can
    facilitate its adoption in resource-constrained environments, bridging technological
    disparities. Additionally, the integration of quantum techniques could spark interdisciplinary
    innovations, enhancing healthcare diagnostics, financial risk assessment, and
    personalized education. Furthermore, QuanTA’s efficiency aligns with global sustainability
    efforts by reducing the energy consumption associated with AI training, contributing
    to the reduction of AI’s carbon footprint. Thus, QuanTA not only advances NLP
    but also promotes inclusive, sustainable, and impactful AI technologies across
    various sectors. However, the deployment of such powerful AI models raises concerns
    about data privacy, security, and the potential misuse of AI technologies. Addressing
    these ethical and societal challenges is crucial to ensure that the benefits of
    QuanTA are realized responsibly and equitably.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: QuanTA的发展代表了在LLM微调方面的重要进展，具有深远的社会影响。通过利用量子信息方法，QuanTA降低了计算和内存需求，使先进的NLP功能变得更加可及和经济。这种AI技术的民主化有助于在资源有限的环境中推广技术，弥合技术差距。此外，量子技术的整合可能激发跨学科的创新，提升医疗诊断、金融风险评估和个性化教育。进一步，QuanTA的高效性与全球可持续发展努力一致，通过减少AI训练相关的能源消耗，减少AI的碳足迹。因此，QuanTA不仅推进了NLP技术，还促进了跨各个领域的包容性、可持续性和有影响力的AI技术。然而，这种强大的AI模型的部署引发了关于数据隐私、安全性以及AI技术潜在滥用的担忧。解决这些伦理和社会挑战对于确保QuanTA的好处得到负责任和公平的实现至关重要。
- en: Acknowledgements
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors acknowledge support from the National Science Foundation under Cooperative
    Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental
    Interactions, [http://iaifi.org/](http://iaifi.org/)). This material is based
    upon work supported by the U.S. Department of Energy, Office of Science, National
    Quantum Information Science Research Centers, Co-design Center for Quantum Advantage
    (C2QA) under contract number DE-SC0012704\. The research was sponsored by the
    United States Air Force Research Laboratory and the Department of the Air Force
    Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement
    Number FA8750-19-2-1000\. The computations in this paper were run on the FASRC
    cluster supported by the FAS Division of Science Research Computing Group at Harvard
    University.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢国家科学基金会在合作协议PHY-2019786下的支持（国家科学基金会人工智能与基础交互研究所，[http://iaifi.org/](http://iaifi.org/)）。本材料基于美国能源部科学办公室国家量子信息科学研究中心量子优势协同设计中心（C2QA）合同号DE-SC0012704的支持。该研究由美国空军研究实验室和空军人工智能加速器赞助，并在合作协议号FA8750-19-2-1000下完成。本文中的计算在哈佛大学FAS科学研究计算组支持的FASRC集群上运行。
- en: References
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Conference of the North American Chapter of the Association for Computational
    Linguistics, 2019.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    深度双向变换器的预训练用于语言理解。载于《北美计算语言学协会年会》，2019年。'
- en: '[2] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever.
    Language models are unsupervised multitask learners, 2019.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever.
    语言模型是无监督的多任务学习者，2019年。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In Advances in Neural Information Processing Systems,
    2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 语言模型是少量样本学习者。载于《神经信息处理系统进展》，2020年。'
- en: '[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
    Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
    Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
    Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard
    Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian,
    Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril,
    Thomas Wang, Timothée Lacroix, and William El Sayed. 专家混合模型，2024年。'
- en: '[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: 开放和高效的基础语言模型，2023年。'
- en: '[6] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。Llama 2：开放基础模型和微调对话模型，2023。'
- en: '[7] AI@Meta. Llama 3 model card. 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] AI@Meta。Llama 3 模型卡。2024。'
- en: '[8] AI@Meta. Introducing Meta Llama 3: The most capable openly available LLM
    to date — ai.meta.com. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    2024. [Accessed 22-05-2024].'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] AI@Meta。介绍 Meta Llama 3：迄今为止最强大的开放可用 LLM — ai.meta.com。 [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)，2024。
    [访问日期：2024年5月22日]。'
- en: '[9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient
    transfer learning for NLP. In International Conference on Machine Learning, 2019.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Neil Houlsby、Andrei Giurgiu、Stanislaw Jastrzebski、Bruna Morrone、Quentin
    De Laroussilhe、Andrea Gesmundo、Mona Attariyan 和 Sylvain Gelly。参数高效的转移学习用于 NLP。发表于国际机器学习大会，2019。'
- en: '[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
    models. In International Conference on Learning Representations, 2022.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean
    Wang、Lu Wang 和 Weizhu Chen。LoRA：大型语言模型的低秩适配。发表于学习表示国际会议，2022。'
- en: '[11] Dan Biderman, Jose Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip
    Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan
    Frankle, Cody Blakeney, and John P. Cunningham. Lora learns less and forgets less,
    2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dan Biderman、Jose Gonzalez Ortiz、Jacob Portes、Mansheej Paul、Philip Greengard、Connor
    Jennings、Daniel King、Sam Havens、Vitaliy Chiley、Jonathan Frankle、Cody Blakeney
    和 John P. Cunningham。Lora 学习较少且遗忘较少，2024。'
- en: '[12] Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev,
    and Ivan Oseledets. Lotr: Low tensor rank weight adaptation, 2024.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Daniel Bershatsky、Daria Cherniuk、Talgat Daulbaev、Aleksandr Mikhalev 和
    Ivan Oseledets。Lotr：低张量秩权重适配，2024。'
- en: '[13] Yifan Yang, Jiajun Zhou, Ngai Wong, and Zheng Zhang. Loretta: Low-rank
    economic tensor-train adaptation for ultra-low-parameter fine-tuning of large
    language models, 2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Yifan Yang、Jiajun Zhou、Ngai Wong 和 Zheng Zhang。Loretta：用于超低参数微调大型语言模型的低秩经济张量训练适配，2024。'
- en: '[14] Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić,
    Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. AdapterHub: A framework for
    adapting transformers. In Conference on Empirical Methods in Natural Language
    Processing, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jonas Pfeiffer、Andreas Rücklé、Clifton Poth、Aishwarya Kamath、Ivan Vulić、Sebastian
    Ruder、Kyunghyun Cho 和 Iryna Gurevych。AdapterHub：一种用于适配变换器的框架。发表于自然语言处理经验方法会议，2020。'
- en: '[15] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient
    model tuning. In Conference on Empirical Methods in Natural Language Processing,
    2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yaqing Wang、Sahaj Agarwal、Subhabrata Mukherjee、Xiaodong Liu、Jing Gao、Ahmed
    Hassan Awadallah 和 Jianfeng Gao。AdaMix：参数高效模型调整的混合适配。发表于自然语言处理经验方法会议，2022。'
- en: '[16] Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter:
    An easy approach for improving the parameter-efficiency of adapters. In Findings
    of the Association for Computational Linguistics: EMNLP 2022, 2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Shwai He, 丁亮, 董岱泽, Jeremy Zhang, 和 陶大成。《SparseAdapter: 提升适配器参数效率的简单方法》，计算语言学协会发现：EMNLP
    2022，2022年。'
- en: '[17] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
    Neubig. Towards a unified view of parameter-efficient transfer learning. In International
    Conference on Learning Representations, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 贺俊贤, 周春亭, 马学哲, Taylor Berg-Kirkpatrick, 和 Graham Neubig。《朝着参数高效迁移学习的统一视角》，国际学习表征会议，2022年。'
- en: '[18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for
    parameter-efficient prompt tuning. In Conference on Empirical Methods in Natural
    Language Processing, 2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Brian Lester, Rami Al-Rfou, 和 Noah Constant。《规模化的力量用于参数高效的提示调优》，自然语言处理经验方法会议，2021年。'
- en: '[19] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. In The 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing,
    2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Xiang Lisa Li 和 Percy Liang。《Prefix-tuning: 优化连续提示生成》，第59届计算语言学协会年会暨第11届国际自然语言处理联合会议，2021年。'
- en: '[20] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank
    Wang, Kwang-Ting Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation.
    2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 刘世扬, 王千易, 尹洪旭, Pavlo Molchanov, 王玉强, 郭光婷, 和 陈敏宏。《DoRA: 权重分解的低秩适配》，2024年。'
- en: '[21] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based
    random matrix adaptation. In The Twelfth International Conference on Learning
    Representations, 2024.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Dawid Jan Kopiczko, Tijmen Blankevoort, 和 Yuki M Asano。《VeRA: 基于向量的随机矩阵适配》，第十二届国际学习表征会议，2024年。'
- en: '[22] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard
    product for communication-efficient federated learning. In International Conference
    on Learning Representations, 2021.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 南贤宇, 文艺彬, 和 吴泰贤。《Fedpara: 用于通信高效联邦学习的低秩Hadamard乘积》，国际学习表征会议，2021年。'
- en: '[23] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J.
    Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker
    adapter, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J.
    Clark, 和 Mehdi Rezagholizadeh。《Krona: 使用Kronecker适配器的参数高效调优》，2022年。'
- en: '[24] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Multilora: Democratizing
    lora for better multi-task learning, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 王一鸣, 林宇, 曾晓东, 和张冠南。《Multilora: 让 Lora 更好地支持多任务学习》，2023年。'
- en: '[25] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu,
    and Maosong Sun. Sparse low-rank adaptation of pre-trained language models, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 丁宁, 吕兴泰, 王乔森, 陈玉林, 周博文, 刘智远, 和 孙茂松。《预训练语言模型的稀疏低秩适配》，2023年。'
- en: '[26] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis,
    Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation
    for parameter-efficient fine-tuning, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 张青如, 陈敏硕, Alexander Bukharin, Nikos Karampatziakis, 何鹏程, 吴成, 陈伟柱, 和赵拓。《Adalora:
    针对参数高效微调的自适应预算分配》，2023年。'
- en: '[27] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation
    of large models, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Soufiane Hayou, Nikhil Ghosh, 和 Bin Yu。《Lora+: 高效的大型模型低秩适配》，2024年。'
- en: '[28] Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Zhi-Yuan Xie, Zhong-Yi Lu, and
    Ji-Rong Wen. Enabling lightweight fine-tuning for pre-trained language model compression
    based on matrix product operators. In the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing, 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 刘佩瑜, 高泽锋, Wayne Xin Zhao, 谷志远, 陆忠义, 和 温基荣。《基于矩阵乘积算子的轻量级微调预训练语言模型压缩》，第59届计算语言学协会年会暨第11届国际自然语言处理联合会议，2021年。'
- en: '[29] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and
    Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient large language
    model fine-tuning, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 潘睿, 刘翔, 刁诗哲, 皮仁杰, 张吉鹏, 韩迟, 和 张同。《Lisa: 层次重要性采样用于内存高效的大型语言模型微调》，2024年。'
- en: '[30] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky,
    Christopher D. Manning, and Christopher Potts. Reft: Representation finetuning
    for language models, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 吴正轩, Aryaman Arora, 王郑, Atticus Geiger, Dan Jurafsky, Christopher D. Manning,
    和 Christopher Potts。《Reft: 语言模型的表示微调》，2024年。'
- en: '[31] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu
    Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. Mora: High-rank
    updating for parameter-efficient fine-tuning, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu
    Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang 和 Fuzhen Zhuang. Mora：高阶更新用于参数高效微调，2024。'
- en: '[32] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem
    with artificial neural networks. Science, 355(6325):602–606, 2017.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Giuseppe Carleo 和 Matthias Troyer. 使用人工神经网络解决量子多体问题。*科学*，355(6325):602–606,
    2017。'
- en: '[33] Di Luo, Zhuo Chen, Kaiwen Hu, Zhizhen Zhao, Vera Mikyoung Hur, and Bryan K.
    Clark. Gauge-invariant and anyonic-symmetric autoregressive neural network for
    quantum lattice models. Phys. Rev. Res., 5:013216, Mar 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Di Luo, Zhuo Chen, Kaiwen Hu, Zhizhen Zhao, Vera Mikyoung Hur 和 Bryan
    K. Clark. 用于量子格点模型的规范不变和任意子对称自回归神经网络。*物理评论研究*，5:013216, 2023年3月。'
- en: '[34] Zhuo Chen, Di Luo, Kaiwen Hu, and Bryan K. Clark. Simulating 2+1d lattice
    quantum electrodynamics at finite density with neural flow wavefunctions, 2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Zhuo Chen, Di Luo, Kaiwen Hu 和 Bryan K. Clark. 使用神经流波函数在有限密度下模拟 2+1 维格点量子电动力学，2022。'
- en: '[35] Zhuo Chen, Laker Newhouse, Eddie Chen, Di Luo, and Marin Soljacic. ANTN:
    Bridging autoregressive neural networks and tensor networks for quantum many-body
    simulation. In Advances in Neural Information Processing Systems, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Zhuo Chen, Laker Newhouse, Eddie Chen, Di Luo 和 Marin Soljacic. ANTN：桥接自回归神经网络与张量网络用于量子多体模拟。收录于《神经信息处理系统进展》，2023。'
- en: '[36] Di Luo and Bryan K Clark. Backflow transformations via neural networks
    for quantum many-body wave functions. Physical review letters, 122(22):226401,
    2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Di Luo 和 Bryan K Clark. 通过神经网络进行量子多体波函数的回流变换。*物理评论快报*，122(22):226401,
    2019。'
- en: '[37] Di Luo, Giuseppe Carleo, Bryan K Clark, and James Stokes. Gauge equivariant
    neural networks for quantum lattice gauge theories. Physical review letters, 127(27):276402,
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Di Luo, Giuseppe Carleo, Bryan K Clark 和 James Stokes. 用于量子格点规范理论的规范等变神经网络。*物理评论快报*，127(27):276402,
    2021。'
- en: '[38] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai
    Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant
    neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai
    Kohlhoff 和 Patrick Riley. 张量场网络：用于 3D 点云的旋转和位移等变神经网络。arXiv 预印本 arXiv:1802.08219,
    2018。'
- en: '[39] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed
    neural networks: A deep learning framework for solving forward and inverse problems
    involving nonlinear partial differential equations. Journal of Computational physics,
    378:686–707, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Maziar Raissi, Paris Perdikaris 和 George E Karniadakis. 物理信息神经网络：用于解决涉及非线性偏微分方程的正向和逆向问题的深度学习框架。*计算物理学杂志*，378:686–707,
    2019。'
- en: '[40] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural
    networks. Advances in neural information processing systems, 32, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Samuel Greydanus, Misko Dzamba 和 Jason Yosinski. 哈密顿神经网络。收录于《神经信息处理系统进展》，32,
    2019。'
- en: '[41] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel,
    and Shirley Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel
    和 Shirley Ho. 拉格朗日神经网络。arXiv 预印本 arXiv:2003.04630, 2020。'
- en: '[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
    Deep unsupervised learning using nonequilibrium thermodynamics. In International
    Conference on Machine Learning, 2015.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan 和 Surya Ganguli.
    使用非平衡热力学的深度无监督学习。收录于《国际机器学习会议》，2015。'
- en: '[43] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models. In Advances in Neural Information Processing Systems, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Jonathan Ho, Ajay Jain 和 Pieter Abbeel. 去噪扩散概率模型。收录于《神经信息处理系统进展》，2020。'
- en: '[44] Yang Song and Stefano Ermon. Generative modeling by estimating gradients
    of the data distribution. In Advances in Neural Information Processing Systems,
    2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Yang Song 和 Stefano Ermon. 通过估计数据分布的梯度进行生成建模。收录于《神经信息处理系统进展》，2019。'
- en: '[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
    equations. In International Conference on Learning Representations, 2021.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon 和 Ben Poole. 通过随机微分方程进行基于得分的生成建模。收录于《国际学习表征会议》，2021。'
- en: '[46] Jin-Guo Liu and Lei Wang. Differentiable learning of quantum circuit born
    machines. Physical Review A, 98(6):062324, 2018.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 刘金国和王磊。量子电路生成模型的可微学习。《物理评论A》，98(6):062324，2018年。'
- en: '[47] Ziming Liu, Di Luo, Yilun Xu, Tommi Jaakkola, and Max Tegmark. Genphys:
    From physical processes to generative models. arXiv preprint arXiv:2304.02637,
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 刘子明、罗迪·洛、徐亿伦、托米·雅卡拉和马克斯·泰格马克。Genphys：从物理过程到生成模型。arXiv预印本arXiv:2304.02637，2023年。'
- en: '[48] Edwin Stoudenmire and David J Schwab. Supervised learning with tensor
    networks. Advances in neural information processing systems, 29, 2016.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 埃德温·斯图登迈尔和大卫·J·施瓦布。使用张量网络的监督学习。《神经信息处理系统进展》，29，2016年。'
- en: '[49] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian
    Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark
    for general-purpose language understanding systems. In Advances in Neural Information
    Processing Systems, 2019.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 亚历克斯·王、雅达·普鲁克萨查昆、尼基塔·南贾、阿曼普里特·辛格、朱利安·迈克尔、费利克斯·希尔、奥默·莱维和塞缪尔·鲍曼。Superglue：一个更具挑战性的通用语言理解基准。在《神经信息处理系统进展》中，2019年。'
- en: '[50] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh,
    and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning
    over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers), 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Dheeru Dua、王益中、Pradeep Dasigi、Gabriel Stanovsky、Sameer Singh和Matt Gardner。DROP：一个需要对段落进行离散推理的阅读理解基准。在2019年北美计算语言学协会：人类语言技术会议论文集（长短篇论文卷1），2019年。'
- en: '[51] A Yu Kitaev. Quantum computations: algorithms and error correction. Russian
    Mathematical Surveys, 52(6):1191, dec 1997.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A Yu Kitaev。量子计算：算法与误差修正。《俄罗斯数学调查》，52(6):1191，1997年12月。'
- en: '[52] A. Yu. Kitaev, A. H. Shen, and M. N. Vyalyi. Classical and Quantum Computation.
    American Mathematical Society, USA, 2002.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] A. Yu. Kitaev、A. H. Shen和M. N. Vyalyi。《经典与量子计算》。美国数学学会，美国，2002年。'
- en: '[53] Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum
    Information: 10th Anniversary Edition. Cambridge University Press, 2010.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 迈克尔·A·尼尔森和艾萨克·L·庄。量子计算与量子信息：第10周年纪念版。剑桥大学出版社，2010年。'
- en: '[54] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing,
    Xing Xu, Soujanya Poria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient
    fine-tuning of large language models. In Proceedings of the 2023 Conference on
    Empirical Methods in Natural Language Processing, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 胡志强、王磊、闵伊怀、徐婉瑜、林易鹏、宾立东、徐兴、索詹亚·波里亚和罗伊·李。LLM-adapters：用于大语言模型参数高效微调的适配器家族。在2023年自然语言处理经验方法会议论文集，2023年。'
- en: '[55] David P. DiVincenzo. Two-bit gates are universal for quantum computation.
    Physical Review A, 51(2):1015–1022, February 1995.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 大卫·P·迪芬琴佐。两比特门是量子计算的通用门。《物理评论A》，51(2):1015–1022，1995年2月。'
- en: '[56] Jean-Luc Brylinski and Ranee Brylinski. Universal quantum gates, 2001.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 让-吕克·布里林斯基和拉妮·布里林斯基。通用量子门，2001年。'
- en: '[57] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·克维亚特科夫斯基、迈克尔·柯林斯和克里斯蒂娜·图塔诺娃。Boolq：探索自然的“是/否”问题的惊人难度，2019年。'
- en: '[58] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language, 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 约纳坦·比斯克、罗万·泽勒斯、罗南·勒布拉斯、贾峯·高和叶金·崔。Piqa：关于自然语言中的物理常识推理，2019年。'
- en: '[59] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
    Socialiqa: Commonsense reasoning about social interactions, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 马滕·萨普、汉娜·拉什金、德里克·陈、罗南·勒布拉斯和叶金·崔。Socialiqa：关于社会互动的常识推理，2019年。'
- en: '[60] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence?, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 罗万·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和叶金·崔。Hellaswag：机器真的能完成你的句子吗？2019年。'
- en: '[61] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale, 2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 坂口圭介、罗南·勒布拉斯、昌德拉·巴伽瓦图拉和叶金·崔。Winogrande：大规模对抗性Winograd方案挑战，2019年。'
- en: '[62] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 彼得·克拉克、艾萨克·考赫、奥伦·艾提奥尼、图沙尔·科特、阿希什·萨布哈尔瓦尔、卡丽莎·肖尼克和欧文·塔福德。认为你已经解决了问题回答？尝试ARC，AI2推理挑战，2018年。'
- en: '[63] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a
    suit of armor conduct electricity? a new dataset for open book question answering,
    2018.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal. 一套盔甲能导电吗？一个用于开放书籍问答的新数据集,
    2018。'
- en: '[64] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction
    by rationale generation: Learning to solve and explain algebraic word problems.
    ACL, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Wang Ling, Dani Yogatama, Chris Dyer, 和 Phil Blunsom. 通过理由生成进行程序归纳：学习解决和解释代数词问题.
    ACL, 2017。'
- en: '[65] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, and John Schulman. Training verifiers to solve math word problems,
    2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证器解决数学词问题, 2021。'
- en: '[66] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh
    Hajishirzi. Mawps: A math word problem repository. In North American Chapter of
    the Association for Computational Linguistics, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, 和 Hannaneh
    Hajishirzi. Mawps: 一个数学词问题库. 在北美计算语言学协会会议中，2016。'
- en: '[67] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really
    able to solve simple math word problems?, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Arkil Patel, Satwik Bhattamishra, 和 Navin Goyal. 自然语言处理模型真的能解决简单的数学词问题吗？
    2021。'
- en: '[68] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee,
    Danqi Chen, and Sanjeev Arora. Fine-tuning large language models with just forward
    passes. 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee,
    Danqi Chen, 和 Sanjeev Arora. 仅通过前向传递对大型语言模型进行微调. 2023。'
- en: Appendix
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Additional Details on Subspace Similarity
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 关于子空间相似度的附加细节
- en: In the main paper, we use the subspace similarity to measure the “intrinsic
    rank” of fine-tuning on a specific dataset. In this section, we provide more details
    on it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们使用子空间相似度来测量在特定数据集上的“内在秩”。在这一部分，我们提供更多细节。
- en: While it is tempting to compute the rank by performing singular value decomposition
    (SVD) on the weight matrices of fully fine-tuned models, such measurement generally
    overestimates the intrinsic rank due to random parameter updates during fine-tuning
    that are irrelevant to the performance on the downstream tasks. The authors of
     [[10](#bib.bib10)] proposes a better way to measure the intrinsic rank, which
    we describe as follows.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过对完全微调模型的权重矩阵进行奇异值分解（SVD）来计算秩是很有诱惑力的，但这种测量通常会高估内在秩，因为在微调过程中，随机的参数更新与下游任务的性能无关。[[10](#bib.bib10)]的作者提出了一种更好的测量内在秩的方法，我们将如下描述。
- en: First, we run LoRA fine-tuning for two different ranks $r_{1}$. Then, we perform
    singular value decompositions on the weights to obtain $\Delta W^{(r)}=U^{(r)}S^{(r)}{V^{(r)}}^{\top}$
    columns of $V^{(r_{1})}$ (first $j$. The subspace similarity between these two
    subspace is defined as
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对两个不同秩 $r_{1}$ 运行 LoRA 微调。然后，我们对权重进行奇异值分解，以获得 $\Delta W^{(r)}=U^{(r)}S^{(r)}{V^{(r)}}^{\top}$
    列的 $V^{(r_{1})}$（前 $j$）。这两个子空间之间的相似度定义为
- en: '|  | $1$2 |  | (A.1) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (A.1) |'
- en: This function equals 1 if any subspace can be contained in the other, equals
    0 if the two subspaces are orthogonal, and in general measures the overlap between
    0 and 1 between the two subspaces. FOr fine-tuning that has a low “intrinsic rank”;
    only the subspace spanned by the first few singular vectors (that correspond to
    the largest few singular vectors) should be similar, with the rest nearly perpendicular
    originating from random noise during fine-tuning. Thus, the subspace similarity
    should be close to 1 only when either $i$. On the other hand, when the “intrinsic
    rank” is high, all the singular vectors in one subspace can be important and therefore
    would appear in the other. In this case, the subspace similarity can remain high
    for all values of $i$.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何一个子空间可以包含在另一个子空间中，则该函数等于1；如果两个子空间是正交的，则等于0；通常情况下，该函数测量两个子空间之间的重叠程度，在0和1之间。对于具有低“内在秩”的微调；只有由前几个奇异向量（对应于最大的几个奇异向量）张成的子空间应该是相似的，其余的几乎是垂直的，源于微调过程中的随机噪声。因此，子空间相似度只有在
    $i$ 中的一项接近1时才应如此。另一方面，当“内在秩”很高时，一个子空间中的所有奇异向量都可能重要，因此也会出现在另一个子空间中。在这种情况下，子空间相似度在所有
    $i$ 值下都可以保持较高。
- en: 'In the main paper, we choose $r_{1}=64$, and measure the subspace similarity
    for both the RTE dataset [[49](#bib.bib49)] and the DROP dataset [[50](#bib.bib50)],
    and reported the values corresponding to the query weight matrix of the 16th layer.
    In this section, we include results corresponding to additional weight matrices.
    In Fig. [A.1](#A1.F1 "Figure A.1 ‣ Appendix A Additional Details on Subspace Similarity
    ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor
    Adaptation") and [A.2](#A1.F2 "Figure A.2 ‣ Appendix A Additional Details on Subspace
    Similarity ‣ QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed
    Tensor Adaptation"), we show the subspace similarities for the value weight matrix
    at layer 16 and 23\. We observe that the similar behaviors appear for these two
    weight matrices as in the main paper, where the RTE dataset exhibits a low “intrinsic-rank”,
    while the DROP dataset has a high “intrinsic-rank”.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '在主要论文中，我们选择了 $r_{1}=64$，并测量了 RTE 数据集 [[49](#bib.bib49)] 和 DROP 数据集 [[50](#bib.bib50)]
    的子空间相似度，并报告了第 16 层查询权重矩阵对应的值。在本节中，我们包括了与额外权重矩阵对应的结果。在图[A.1](#A1.F1 "图 A.1 ‣ 附录
    A 关于子空间相似度的附加细节 ‣ QuanTA: 高效的高秩微调 LLMs 的量子信息张量适应") 和 [A.2](#A1.F2 "图 A.2 ‣ 附录
    A 关于子空间相似度的附加细节 ‣ QuanTA: 高效的高秩微调 LLMs 的量子信息张量适应") 中，我们展示了第 16 层和第 23 层的值权重矩阵的子空间相似度。我们观察到，这两个权重矩阵表现出类似的行为，正如主要论文中所述，RTE
    数据集展现出较低的“内在秩”，而 DROP 数据集则具有较高的“内在秩”。'
- en: '![Refer to caption](img/bb0445df7fc4a80dc735592a830f5639.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bb0445df7fc4a80dc735592a830f5639.png)'
- en: 'Figure A.1: Subspace similarities between two LoRA experiments of different
    ranks (64 and 128) for two datasets for the value weight matrix at layer 16.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A.1: 对于第 16 层的值权重矩阵，两个不同秩（64 和 128）的 LoRA 实验在两个数据集上的子空间相似度。'
- en: '![Refer to caption](img/ae843ba5c14b6eb2ca0e0f99018ed536.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ae843ba5c14b6eb2ca0e0f99018ed536.png)'
- en: 'Figure A.2: Subspace similarities between two LoRA experiments of different
    ranks (64 and 128) for two datasets for the value weight matrix at layer 23.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A.2: 对于第 23 层的值权重矩阵，两个不同秩（64 和 128）的 LoRA 实验在两个数据集上的子空间相似度。'
- en: Appendix B Constructing General QuanTA Operators
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 构建一般 QuanTA 操作符
- en: In the main paper, we discussed QuanTA operation when the layer weight is square
    $W_{0}\in\mathbb{R}^{d\times d}$. Here, we will consider more general cases.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要论文中，我们讨论了当层权重为平方矩阵 $W_{0}\in\mathbb{R}^{d\times d}$ 时的 QuanTA 操作。在这里，我们将考虑更一般的情况。
- en: 'Let’s first consider general rectangular matrices $W_{0}\in\mathbb{R}^{d\times
    k}$ and $k=k_{1}\times k_{2}\times\cdots\times k_{N}$ and $k$. (For example, LLaMA2-70B
    model contains many $1024\times 8192$ for all $n>th axes. After applying
    this tensor, the hidden vector changes shape from …"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '`import  itertoolsimport  opt_einsum  as  oe\pardef  quanta_apply_einsum_expr(N):current_symbols_inds  =  list(range(N))\parexpr  =  "…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parfor  (dim1,  dim2)  in  itertools.combinations(range(-1,  -N-1,  -1),  2):symbol_ind1  =  current_symbols_inds[dim1]symbol_ind2  =  current_symbols_inds[dim2]symbol_ind3  =  symbol_ind1  +  Nsymbol_ind4  =  symbol_ind2  +  Nexpr  +=  ","  +  \  oe.get_symbol(symbol_ind4)  +  \  oe.get_symbol(symbol_ind3)  +  \  oe.get_symbol(symbol_ind2)  +  \  oe.get_symbol(symbol_ind1)current_symbols_inds[dim1]  =  symbol_ind3current_symbols_inds[dim2]  =  symbol_ind4\parexpr  +=  "->…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr`'
- en: Then, applying the QuanTA operator to the hidden vector is as simple as
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将QuanTA操作符应用于隐藏向量就像
- en: y  =  torch.einsum(quanta_apply_expr,  x,  *T)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`y  =  torch.einsum(quanta_apply_expr,  x,  *T)`'
- en: Similarly, it is easy to generate the einsum expression for obtaining the full
    QuanTA operator as
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，生成用于获取完整QuanTA操作符的einsum表达式很简单，如下所示
- en: import  itertoolsimport  opt_einsum  as  oe\pardef  quanta_op_einsum_expr(N):current_symbols_inds  =  list(range(N))\parexpr  =  "…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parfor  (dim1,  dim2)  in  itertools.combinations(range(-1,  -N-1,  -1),  2):symbol_ind1  =  current_symbols_inds[dim1]symbol_ind2  =  current_symbols_inds[dim2]symbol_ind3  =  symbol_ind1  +  Nsymbol_ind4  =  symbol_ind2  +  Nexpr  +=  ","  +  \  oe.get_symbol(symbol_ind4)  +  \  oe.get_symbol(symbol_ind3)  +  \  oe.get_symbol(symbol_ind2)  +  \  oe.get_symbol(symbol_ind1)current_symbols_inds[dim1]  =  symbol_ind3current_symbols_inds[dim2]  =  symbol_ind4\parexpr  +=  "->…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr[1:]
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '`import  itertoolsimport  opt_einsum  as  oe\pardef  quanta_op_einsum_expr(N):current_symbols_inds  =  list(range(N))\parexpr  =  "…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parfor  (dim1,  dim2)  in  itertools.combinations(range(-1,  -N-1,  -1),  2):symbol_ind1  =  current_symbols_inds[dim1]symbol_ind2  =  current_symbols_inds[dim2]symbol_ind3  =  symbol_ind1  +  Nsymbol_ind4  =  symbol_ind2  +  Nexpr  +=  ","  +  \  oe.get_symbol(symbol_ind4)  +  \  oe.get_symbol(symbol_ind3)  +  \  oe.get_symbol(symbol_ind2)  +  \  oe.get_symbol(symbol_ind1)current_symbols_inds[dim1]  =  symbol_ind3current_symbols_inds[dim2]  =  symbol_ind4\parexpr  +=  "->…"for  i  in  current_symbols_inds:expr  +=  oe.get_symbol(i)\parreturn  expr[1:]`'
- en: and obtaining the full QuanTA operator is
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 并且获取完整的QuanTA操作符是
- en: full_T  =  torch.einsum(quanta_op_expr,  *T)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`full_T  =  torch.einsum(quanta_op_expr,  *T)`'
- en: We note that the padding and truncation operators are omitted when the QuanTA
    operator has a different size than the original weight matrix. In addition, in
    our actual implementation, we use opt_einsum library to optimize the contraction
    order, reducing the contraction cost.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，当QuanTA操作符与原始权重矩阵的大小不同时，填充和截断操作被省略了。此外，在实际实现中，我们使用`opt_einsum`库来优化收缩顺序，从而减少收缩成本。
- en: Appendix G Example Model Outputs
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 示例模型输出
- en: In this section, we provide some example output of QuanTA fine-tuned LLaMA model.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一些QuanTA微调后的LLaMA模型的示例输出。
- en: '|   Task | Model Output |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|   任务 | 模型输出 |'
- en: '| DROP | Prompt: Passage:  Hoping to rebound from their embarrassing home loss to the Lions, the Raiders flew to Invesco Field at Mile High for an AFC West duel with the Denver Broncos.  In the first quarter, Oakland trailed early as Broncos QB Jay Cutler completed a 9-yard TD pass to WR Brandon Stokley for the only score of the period.  In the second quarter, the Raiders got on the board with kicker Sebastian Janikowski getting a 38-yard field goal.  However, Denver continued to pound away as RB Cecil Sapp got a 4-yard TD run, while kicker Jason Elam got a 23-yard field goal. In the third quarter, Oakland began to come back as QB Josh McCown (who was a gametime decision heading into the game) completed a 46-yard TD pass to WR Jerry Porter for the only score of the period.  In the fourth quarter, the Raiders took the lead as DT Gerard Warren sacked Cutler in the end zone for a safety, while LB Thomas Howard returned an interception 44 yards for a touchdown (followed by a successful two-point conversion pass from McCown to WR Ronald Curry).  However, the Broncos tied the game up with Elam’s 20-yard field goal.  In overtime, Oakland managed to make Denver go three-and-out on their first possession.  A 33-yard run by RB LaMont Jordan helped set up Janikowski for a game-winning 52-yard field goal. Broncos head coach Mike Shanahan called timeout before the kick could begin.  Janikowski’s second try hit off the very tip of the left goal post and was no good, giving Denver a chance to win the game.  The Broncos won with Elam getting a 23-yard field goal. With the loss, not only did the Raiders fall to 0-2, but they had lost 11-straight games (currently the NFL’s longest losing streak) dating back to Week 9 of the 2006 season.
    Question: How many field goals did each kicker score in the first half? Answer:
    Output: 1 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| DROP | 提示：文章：为了从在主场对狮子的尴尬失利中恢复过来，突袭者飞往了高地投资场，与丹佛野马队展开了一场AFC西区对决。在第一节，奥克兰在开局就落后，野马队四分卫杰伊·卡特勒完成了一次9码的达阵传球给外接手布兰登·斯托克利，这是本节唯一的得分。在第二节，突袭者通过踢球手塞巴斯蒂安·贾尼科夫斯基获得了一个38码的任意球。然而，丹佛继续猛烈进攻，跑锋塞西尔·萨普获得了一个4码的达阵跑动，而踢球手杰森·埃拉姆获得了一个23码的任意球。在第三节，奥克兰开始追回分数，四分卫乔什·麦克高恩（在比赛前是临场决定）完成了一次46码的达阵传球给外接手杰里·波特，这是本节唯一的得分。在第四节，突袭者取得了领先，防守截锋杰拉德·沃伦在端区擒杀了卡特勒，获得了安全分，而外线卫托马斯·霍华德将一次拦截回跑了44码取得达阵（之后成功完成了麦克高恩传给外接手罗纳德·库里的一次两分转换）。然而，野马队通过埃拉姆的20码任意球将比赛扳平。在加时赛中，奥克兰设法让丹佛在第一次进攻中三次失败。跑锋拉蒙特·乔丹的33码跑动帮助贾尼科夫斯基准备了一次制胜的52码任意球。野马队主教练迈克·肖纳汉在踢球开始前叫了暂停。贾尼科夫斯基的第二次尝试击中了左门柱的尖端，未能成功，这给了丹佛赢得比赛的机会。最终，野马队通过埃拉姆的23码任意球赢得了比赛。由于这次失利，突袭者不仅战绩跌至0-2，还连续输掉了11场比赛（目前是NFL最长的连败纪录），追溯到2006赛季第9周。问题：在上半场，每位踢球手各踢进了多少个任意球？答案：输出：1
    |'
- en: '|   |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|   |  |'
- en: 'Table G.5: Examples of QuanTA trained LLaMA2-7B Outputs for the DROP dataset.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 表 G.5：QuanTA训练的LLaMA2-7B模型对DROP数据集的输出示例。
- en: '|   Task | Model Output |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|   任务 | 模型输出 |'
- en: '| BoolQ | Prompt: Please answer the following question with true or false, question: is ford escape a 4 wheel drive vehicle? Answer format: true/false
    Highest probability choice: Answer: the correct answer is true. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 提示：请用“true”或“false”回答以下问题，问题：Ford Escape是四轮驱动的车辆吗？答案格式：true/false
    最高概率选择：答案：正确答案是true。 |'
- en: '| SIQA | Prompt: Please choose the correct answer to the question: Carson took Lee’s risk by going skydiving with him off of the plane. What will Lee want to do after? Answer1: hug Carson Answer2: buy a ticket Answer3: kick Carson. Answer format: answer1/answer2/answer3
    Highest probability choice: Answer: the correct answer is answer1. |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| SIQA | 提示：请选择正确的答案：卡森与李一起从飞机上跳伞，李会想要做什么？答案1：拥抱卡森 答案2：买票 答案3：踢卡森。答案格式：answer1/answer2/answer3
    最高概率选择：答案：正确答案是answer1。 |'
- en: '| SIQA | Prompt: Please choose the correct ending to complete the given sentence: Personal Care and Style: [header] How to make ice balls [title] Buy a package of water balloons. [step] This method is cheap, quick, and easy-perfect if you don’t want to spend money on specialty molds for making ice balls. All you’ll need is a few round water balloons (and, of course, water and a freezer. Ending1: ) [substeps] Uninflated balloons: this method requires 2 balls, 1 ice cream stick and 2 water balloons in a large bag (1 at a time). Open the sides of your volcano and shake the tupperware from side to side a few times. Ending2: ) [substeps] Of course, there is no telling if the water balloons will be successful; you just know that they will eventually explode on the first try. Plus, they’re basically too small to correctly hang from your roof. Ending3: ) [substeps] Remember that, if you’re using your ice balls to cool your drinks, they’ll need to be able to fit inside your glasses. Thus, when it comes to water balloons, generally, the smaller, the better. Ending4: ) [substeps] Use unopened water balloons, because they tend to be too full. Plastic ones are dry and will be harder to put in the freezer. Answer format: ending1/ending2/ending3/ending4
    Highest probability choice: Answer: the correct answer is ending3. |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SIQA | 提示：请选择正确的结尾以完成给定的句子：个人护理和风格：[header] 如何制作冰球 [title] 购买一包水气球。[step]
    这种方法便宜、快速且简单——如果你不想花钱购买制作冰球的专用模具，这是完美的。你只需要几个圆形水气球（当然，还有水和一个冰箱）。 结尾1：) [substeps]
    未充气的气球：这种方法需要2个气球、1根冰淇淋棒和2个水气球在一个大袋子里（一次一个）。打开火山的侧面，左右摇动塑料容器几次。 结尾2：) [substeps]
    当然，水气球是否成功是无法预测的；你只知道它们会在第一次尝试时爆炸。而且，它们基本上太小，无法正确悬挂在你的屋顶上。 结尾3：) [substeps] 记住，如果你用冰球来冷却饮料，它们需要能适合你的杯子。因此，对于水气球来说，通常是越小越好。
    结尾4：) [substeps] 使用未开启的水气球，因为它们往往过满。塑料气球是干的，更难放进冰箱。 答案格式：ending1/ending2/ending3/ending4
    最高概率选择：答案是结尾3。 |'
- en: '|   |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|   |  |'
- en: 'Table G.6: Examples of QuanTA trained LLaMA3-8B Outputs for selected commonsense
    datasets.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 表 G.6：QuanTA 训练的 LLaMA3-8B 对于选定的常识数据集的输出示例。
- en: '|   Task | Model Output |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|   Task | 模型输出 |'
- en: '| MAWPS | Prompt: Bob will rent a car for the weekend . He can choose one of 2 payment plans . The first plan costs 57.98 for 2 days plus 13 cents per mile . The second plan costs 45.98 for 2 days plus 18 cents per mile . How many miles does Bob need to drive for the 2 plans to cost the same ?
    Answer: Output: Let’s assume that Bob drives x miles.  For the first plan, the cost would be: 57.98 + 0.13x  For the second plan, the cost would be: 45.98 + 0.18x  We want to find the value of x such that the two costs are equal.  So, we can set up an equation:  57.98 + 0.13x = 45.98 + 0.18x  Simplifying this equation, we get:  0.05x = 12.00  Dividing both sides by 0.05, we get:  x = 240  Therefore, Bob needs to drive 240 miles for the two plans to cost the same.
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| MAWPS | 提示：Bob 将租一辆车度过周末。他可以选择两种付款计划中的一种。第一种计划2天的费用为57.98美元，每英里13美分。第二种计划2天的费用为45.98美元，每英里18美分。Bob
    需要驾驶多少英里，才能使这两种计划的费用相同？ 答案：输出：假设 Bob 驾驶 x 英里。对于第一种计划，费用为：57.98 + 0.13x。对于第二种计划，费用为：45.98
    + 0.18x。我们想找出 x 的值，使得这两种费用相等。因此，我们可以建立一个方程式：57.98 + 0.13x = 45.98 + 0.18x。简化这个方程式，我们得到：0.05x
    = 12.00。将两边都除以0.05，我们得到：x = 240。因此，Bob 需要驾驶 240 英里，这两种计划的费用才能相同。 |'
- en: '|   |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|   |  |'
- en: 'Table G.7: Examples of QuanTA trained LLaMA2-7B Outputs for a selected arithmetic
    dataset.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 表 G.7：QuanTA 训练的 LLaMA2-7B 对于选定的算术数据集的输出示例。
