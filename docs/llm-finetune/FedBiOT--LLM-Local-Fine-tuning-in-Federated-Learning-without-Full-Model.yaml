- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:35:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FedBiOT: 在联邦学习中进行LLM本地微调而无需完整模型'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17706](https://ar5iv.labs.arxiv.org/html/2406.17706)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17706](https://ar5iv.labs.arxiv.org/html/2406.17706)
- en: Feijie Wu Purdue University [wu1977@purdue.edu](mailto:wu1977@purdue.edu) , 
    Zitao Li Alibaba Group [zitao.l@alibaba-inc.com](mailto:zitao.l@alibaba-inc.com)
    ,  Yaliang Li Alibaba Group [yaliang.li@alibaba-inc.com](mailto:yaliang.li@alibaba-inc.com)
    ,  Bolin Ding Alibaba Group [bolin.ding@alibaba-inc.com](mailto:bolin.ding@alibaba-inc.com)
     and  Jing Gao Purdue University [jinggao@purdue.edu](mailto:jinggao@purdue.edu)(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Feijie Wu Purdue University [wu1977@purdue.edu](mailto:wu1977@purdue.edu) , 
    Zitao Li Alibaba Group [zitao.l@alibaba-inc.com](mailto:zitao.l@alibaba-inc.com)
    ,  Yaliang Li Alibaba Group [yaliang.li@alibaba-inc.com](mailto:yaliang.li@alibaba-inc.com)
    ,  Bolin Ding Alibaba Group [bolin.ding@alibaba-inc.com](mailto:bolin.ding@alibaba-inc.com)
     and  Jing Gao Purdue University [jinggao@purdue.edu](mailto:jinggao@purdue.edu)(2024)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Large language models (LLMs) show amazing performance on many domain-specific
    tasks after fine-tuning with some appropriate data. However, many domain-specific
    data are privately distributed across multiple owners. Thus, this dilemma raises
    the interest in how to perform LLM fine-tuning in federated learning (FL). However,
    confronted with limited computation and communication capacities, FL clients struggle
    to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient
    LLM fine-tuning approach to FL. Specifically, our method involves the server generating
    a compressed LLM and aligning its performance with the full model. Subsequently,
    the clients fine-tune a lightweight yet important part of the compressed model,
    referred to as an adapter. Notice that as the server has no access to the private
    data owned by the clients, the data used for alignment by the server has a different
    distribution from the one used for fine-tuning by clients. We formulate the problem
    into a bi-level optimization problem to minimize the negative effect of data discrepancy
    and derive the updating rules for the server and clients. We conduct extensive
    experiments on LLaMA-2, empirically showing that the adapter has exceptional performance
    when reintegrated into the global LLM. The results also indicate that the proposed
    FedBiOT significantly reduces resource consumption compared to existing benchmarks,
    all while achieving comparable performance levels.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在经过适当数据的微调后，在许多领域特定任务上展现出了惊人的性能。然而，许多领域特定的数据被多个所有者私密分布。因此，这一困境引发了如何在联邦学习（FL）中进行LLM微调的兴趣。然而，由于计算和通信能力有限，FL客户端难以有效地微调LLM。为此，我们提出了FedBiOT，一种资源高效的LLM微调方法。具体来说，我们的方法涉及服务器生成一个压缩的LLM，并使其性能与完整模型对齐。随后，客户端对压缩模型中一个轻量但重要的部分进行微调，称为适配器。注意，由于服务器无法访问客户端拥有的私密数据，服务器用于对齐的数据与客户端用于微调的数据具有不同的分布。我们将问题形式化为一个双层优化问题，以最小化数据差异的负面影响，并推导出服务器和客户端的更新规则。我们在LLaMA-2上进行了广泛的实验，经验性地表明，当适配器重新集成到全球LLM中时，具有出色的性能。结果还表明，与现有基准相比，所提出的FedBiOT显著减少了资源消耗，同时实现了可比的性能水平。
- en: 'Federated Learning; Large Language Models^†^†journalyear: 2024^†^†copyright:
    rightsretained^†^†conference: Proceedings of the 30th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining; August 25–29, 2024; Barcelona, Spain^†^†booktitle:
    Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain^†^†doi: 10.1145/3637528.3671897^†^†isbn:
    979-8-4007-0490-1/24/08^†^†ccs: Computing methodologies Distributed algorithms^†^†ccs:
    Computing methodologies Natural language generation^†^†ccs: Information systems Language
    models'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Federated Learning; Large Language Models^†^†journalyear: 2024^†^†copyright:
    rightsretained^†^†conference: Proceedings of the 30th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining; August 25–29, 2024; Barcelona, Spain^†^†booktitle:
    Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain^†^†doi: 10.1145/3637528.3671897^†^†isbn:
    979-8-4007-0490-1/24/08^†^†ccs: Computing methodologies Distributed algorithms^†^†ccs:
    Computing methodologies Natural language generation^†^†ccs: Information systems Language
    models'
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'The recent advancements in large language models (LLMs) have demonstrated incredible
    performance in various tasks, such as question-answering and problem-solving.
    This success owes to the pretraining on large datasets, covering a wide range
    of linguistic patterns and general knowledge. However, in specific domains such
    as legal advice (Nay et al., [2024](#bib.bib24); Cui et al., [2023](#bib.bib8))
    and medical diagnosis (Thirunavukarasu et al., [2023](#bib.bib33); Singhal et al.,
    [2023](#bib.bib28); Wang et al., [2023c](#bib.bib40)), LLMs may not provide professional
    responses because the terminology and context significantly differ from general
    language use. To address this limitation and enable the generation of domain-specific
    content, it becomes imperative to fine-tune LLMs with relevant data. This fine-tuning
    process allows the models to learn from the specific instances and nuances of
    the target application, ensuring their capability within specialized fields. The
    quality and quantity of the task-specific data are directly related to the performance
    of the fine-tuned model on downstream tasks: large and well-labeled data can significantly
    improve the model, while small and irrelevant data can only benefit the model
    marginally. However, there are many cases where task-specific data are possessed
    by multiple data parties, while each of them may have a limited number of samples
    that can be used to fine-tune LLMs. For example, a hospital in a rural area may
    only have a limited number of lung cancer cases recorded in its own system; if
    an LLM is only fine-tuned on one set of those cases, it may not obtain comprehensive
    knowledge and easily be overfitted.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）在各种任务中表现出令人难以置信的性能，如问答和问题解决。这种成功归功于在大规模数据集上的预训练，这些数据集涵盖了广泛的语言模式和一般知识。然而，在特定领域如法律建议（Nay
    et al., [2024](#bib.bib24)；Cui et al., [2023](#bib.bib8)）和医学诊断（Thirunavukarasu
    et al., [2023](#bib.bib33)；Singhal et al., [2023](#bib.bib28)；Wang et al., [2023c](#bib.bib40)），LLMs可能无法提供专业回应，因为术语和背景与一般语言使用大相径庭。为了解决这一限制并使模型能够生成特定领域的内容，必须用相关数据对LLMs进行微调。这个微调过程允许模型从目标应用的具体实例和细微差别中学习，确保它们在专业领域内的能力。任务特定数据的质量和数量与微调模型在下游任务中的表现直接相关：大量且标注良好的数据可以显著提升模型，而少量且无关的数据只能对模型有微小的帮助。然而，许多情况下任务特定数据由多个数据方拥有，而每个数据方可能只有有限数量的样本可用于微调LLMs。例如，一家农村地区的医院可能只在其系统中记录了有限数量的肺癌病例；如果LLM仅在这些病例的一组数据上进行微调，它可能无法获得全面的知识并容易过拟合。
- en: To incorporate all the distributed data in the fine-tuning of LLMs, one may
    consider the batch fine-tuning as follows. If we demand all the data owners to
    share their data with the LLM server, then LLM fine-tuning could be conducted
    at the server side. For example, some LLM owners offer fine-tuning APIs as services,
    but the users must pack their data as files and upload them to use a black-box
    fine-tuning (OpenAI, [2023](#bib.bib25)). Apparently, this setup is not applicable
    to users who have privacy concerns. Especially, some businesses are subject to
    data privacy regulations (GDPR, [2016](#bib.bib9); CCPA, [2023](#bib.bib3)), which
    makes it challenging to share local data with LLM server.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在LLMs的微调中整合所有分布式数据，可以考虑如下的批量微调。如果我们要求所有数据拥有者将其数据分享给LLM服务器，则LLM的微调可以在服务器端进行。例如，一些LLM拥有者提供微调API作为服务，但用户必须将数据打包成文件并上传以使用黑箱微调（OpenAI，[2023](#bib.bib25)）。显然，这种设置不适用于有隐私顾虑的用户。特别是，一些企业受数据隐私法规的约束（GDPR，[2016](#bib.bib9)；CCPA，[2023](#bib.bib3)），这使得将本地数据分享给LLM服务器具有挑战性。
- en: 'Therefore, a more practical setting is to let individual data owners keep their
    data locally, run fine-tuning locally and aggregate the fine-tuning results at
    the LLM server. This fits well the *federated learning* (FL) framework, which
    is a distributed paradigm that places a paramount emphasis on preserving privacy.
    Its conventional algorithms, such as FedAvg (McMahan et al., [2017](#bib.bib23)),
    are considered practical solutions to overcome data barriers across different
    data owners. In this paradigm, data owners are treated as clients, and an LLM
    server coordinates the computation. The standard FL workflow involves three steps
    repeatedly: (i) The server distributes the global model to all clients; (ii) Each
    client trains the model locally for multiple iterations and sends the updated
    model to the server; (iii) The server aggregates the models from the clients and
    updates the global model accordingly. Despite the potential of this method to
    facilitate collaborative fine-tuning of an LLM without sharing local data, its
    feasibility is hindered by two main limitations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更实际的设置是让各个数据所有者将数据保存在本地，在本地进行微调，并在LLM服务器上汇总微调结果。这很好地契合了*联邦学习*（FL）框架，这是一种分布式范式，极其重视隐私保护。其传统算法，如FedAvg（McMahan等，[2017](#bib.bib23)），被认为是克服不同数据所有者之间数据障碍的实际解决方案。在这种范式中，数据所有者被视为客户端，LLM服务器协调计算。标准的FL工作流程重复包括三个步骤：（i）服务器将全球模型分发给所有客户端；（ii）每个客户端在本地训练模型多个迭代并将更新后的模型发送回服务器；（iii）服务器汇总来自客户端的模型并相应地更新全球模型。尽管这种方法有助于在不共享本地数据的情况下进行LLM的协作微调，但其可行性受到两个主要限制的制约：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Access to full model of state-of-the-art LLMs: There exist some open-source
    LLMs whose model the public can download and have full access to their parameters.
    However, the most recent and powerful versions of LLMs are usually closed-sourced,
    i.e, the architecture and parameters are not available to the public. The best
    closed-source LLMs still have leading performance on a wide range of language
    tasks, and its leading edge can be maintained or even enhanced after fine-tuning,
    making it a better choice. As aforementioned, using the blackbox fine-tuning service
    provided by these closed-source LLMs often violates data users’ privacy requirements.
    Therefore, a federated learning framework that conducts collaborative fine-tuning
    with the assumption of no access to the full model of LLMs at the client side
    is more desirable.'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 访问最先进的大型语言模型（LLMs）的完整模型：存在一些开源的LLMs，公众可以下载其模型并全面访问其参数。然而，最新和最强大的LLMs版本通常是闭源的，即其架构和参数对公众不可用。最好的闭源LLMs在各种语言任务上仍具有领先的性能，并且经过微调后，其领先优势可以保持甚至增强，使其成为更好的选择。如前所述，使用这些闭源LLMs提供的黑箱微调服务通常会违反数据用户的隐私要求。因此，假设客户端无法访问LLMs的完整模型的协作微调的联邦学习框架更为理想。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computation and communication costs: Existing federated learning framework
    could also suffer from the computation and communication challenges when conducting
    collaborative fine-tuning on LLMs. The fine-tuning process for LLMs entails substantial
    computational demands and communication costs due to the vast number of trainable
    model parameters. Clients with limited computational power may struggle to perform
    complex model updates, leading to prolonged training times or potential disruptions.
    The transfer of expensive models between the server and the clients also incurs
    substantial communication costs, leading to substantial bandwidth consumption
    and increased communication latency. At the server side, there could be network
    congestion when clients send back their updated huge amount of parameters concurrently.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算和通信成本：现有的联邦学习框架在进行LLMs的协作微调时也可能面临计算和通信挑战。由于大量可训练的模型参数，LLMs的微调过程需要大量的计算资源和通信成本。计算能力有限的客户端可能难以执行复杂的模型更新，从而导致训练时间延长或潜在的中断。服务器和客户端之间的昂贵模型传输也会产生大量的通信成本，导致带宽消耗和通信延迟增加。在服务器端，当客户端同时发送大量更新的参数时，可能会出现网络拥堵。
- en: In this paper, we aim to tackle these two challenges and propose to design an
    effective and practical collaborative LLM fine-tuning framework. To address the
    first challenge, We follow the setting proposed in offsite-tuning (Xiao et al.,
    [2023](#bib.bib44)) and its federated version FedOT (Kuang et al., [2024](#bib.bib14)).
    We assume that the LLM owner does not collect data directly from clients but serves
    as the server in FL, who can use a public dataset to distill her LLM and aggregate
    some local updates on part of the model from clients; multiple clients want to
    collaborate on fine-tuning for similar downstream tasks. Different from the classic
    FL setting (Zhang et al., [2021](#bib.bib48); Wang et al., [2023a](#bib.bib36);
    Lin et al., [2020](#bib.bib19)), we do not assume the data distribution on clients
    or the public data owned by the server to be the same. Our goal, in general, is
    to provide a framework for collaborative clients to fine-tune without access to
    full LLM or sharing local data directly. More importantly, the fine-tuned model
    can still achieve better performance than fine-tuning LLM locally with their local
    data exclusively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们旨在解决这两个挑战，并提出设计一个有效且实用的协作LLM微调框架。为了解决第一个挑战，我们遵循离线调优（Xiao等，[2023](#bib.bib44)）和其联邦版本FedOT（Kuang等，[2024](#bib.bib14)）提出的设置。我们假设LLM所有者不会直接从客户那里收集数据，而是作为FL中的服务器，使用公共数据集来提炼她的LLM，并汇总客户的部分模型的本地更新；多个客户希望协作进行类似的下游任务微调。与经典的FL设置（Zhang等，[2021](#bib.bib48)；Wang等，[2023a](#bib.bib36)；Lin等，[2020](#bib.bib19)）不同，我们不假设客户的数据分布或服务器拥有的公共数据是相同的。我们的目标总体上是提供一个框架，使协作客户能够在不访问完整LLM或直接共享本地数据的情况下进行微调。更重要的是，经过微调的模型仍然能够比仅使用本地数据微调LLM实现更好的性能。
- en: 'Although FedOT (Kuang et al., [2024](#bib.bib14)) was developed for this objective,
    it could incur significant computational and communication costs, thereby suffering
    from the second challenge. In light of this challenge, we propose to integrate
    various parameter-efficient fine-tuning (PEFT) techniques into the proposed FL
    framework. Specifically, the server employs linear dropout to compress the LLM,
    integrates LoRA (Hu et al., [2021](#bib.bib12)) to reduce the trainable parameters,
    and divides it into two components: an emulator and an adapter. The emulator retains
    a consistent representation of the raw model on the server’s dataset, while the
    adapter assimilates domain-specific linguistic patterns from the clients’ local
    datasets. Considering the significant distribution shift between the clients’
    datasets and the server’s dataset, we separate the fine-tuning of these two components
    into two processes during FL training, i.e., the clients perform multiple local
    updates to fine-tune the adapter, and the server distill the emulator from the
    original LLM while aggregating the updated adapters from the clients. To this
    end, a bi-level optimization is formulated.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FedOT（Kuang等，[2024](#bib.bib14)）是为此目标开发的，但它可能会产生显著的计算和通信成本，因此遭遇第二个挑战。鉴于此挑战，我们提出将各种参数高效微调（PEFT）技术集成到提议的FL框架中。具体而言，服务器采用线性丢弃来压缩LLM，集成LoRA（Hu等，[2021](#bib.bib12)）以减少可训练参数，并将其分为两个组件：仿真器和适配器。仿真器在服务器的数据集上保留原始模型的一致表示，而适配器则从客户的本地数据集中吸收特定领域的语言模式。考虑到客户的数据集和服务器的数据集之间的显著分布偏移，我们将这两个组件的微调过程分为FL训练中的两个过程，即客户执行多个本地更新以微调适配器，服务器从原始LLM中提炼仿真器，同时汇总来自客户的更新适配器。为此，制定了双层优化方案。
- en: This design, named Federated Bi-level Offsite Tuning (FedBiOT), offers twofold
    advantages from the clients’ perspectives. Firstly, instead of loading the complete
    model, clients load a compressed version with fewer layers, considerably reducing
    computation costs. Secondly, clients exclusively fine-tune the adapter, affecting
    only the last few layers of the LLM and thereby minimizing computation and communication
    expenses.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计，名为联邦双层离线调优（FedBiOT），从客户的角度提供了双重优势。首先，客户不需要加载完整的模型，而是加载一个压缩版，层数更少，显著降低了计算成本。其次，客户只对适配器进行微调，仅影响LLM的最后几层，从而最小化计算和通信开销。
- en: Contributions.
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'Throughout the paper, our contributions are highlighted as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在整篇论文中，我们的贡献如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an algorithm FedBiOT that avoids full model fine-tuning and significantly
    reduces the communication and computation overhead. To the best of our knowledge,
    this is the first work that addresses both the aforementioned two challenges in
    the federated LLM fine-tuning framework. With our proposed framework, clients’
    data are ensured to be kept locally and computation and communication burden is
    significantly reduced.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种算法FedBiOT，它避免了全模型微调，并显著降低了通信和计算开销。据我们所知，这是第一项在联邦LLM微调框架中同时解决上述两个挑战的工作。通过我们提出的框架，客户端的数据确保保留在本地，计算和通信负担显著减少。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We formulate a bi-level optimization problem that enables the LLM fine-tuning
    without access to the full model. By partitioning the compressed model into the
    adapter and the emulator, the emulator acts as a simulator of the original raw
    model, while the adapter adeptly learns domain-specific linguistic patterns with
    clients’ local datasets. To this end, we realize that fine-tuning the compressed
    model is equivalent to the refinement of the counterpart of the complete LLM.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们制定了一个双层优化问题，使LLM微调无需访问完整模型。通过将压缩模型分成适配器和模拟器，模拟器充当原始模型的模拟器，而适配器则利用客户端的本地数据集巧妙地学习领域特定的语言模式。为此，我们意识到微调压缩模型等同于完整LLM的相应部分的精炼。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments on LLaMA-2 for fine-tuning with three tasks,
    i.e., code generating, math problem solving, and question answering. The empirical
    studies also demonstrate that the proposed approach has significant improvement
    over all these tasks compared with the baseline approaches in terms of computation
    and communication overheads and final accuracy.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在LLaMA-2上进行大量实验，针对三项任务进行微调，即代码生成、数学问题解决和问答。经验研究还表明，与基线方法相比，所提出的方法在计算和通信开销以及最终准确性方面对所有这些任务都有显著改善。
- en: 2\. Preliminary
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 初步
- en: 2.1\. Traditional FL Formulation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. 传统FL表述
- en: Consider there is an FL system with a total of $M$. Each client $m\in[M]$. A
    client’s local loss is defined as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个总共有$M$的FL系统。每个客户端$m\in[M]$。一个客户端的本地损失定义为
- en: '| (1) |  | $F_{m}(\boldsymbol{w}):=\frac{1}{&#124;\mathcal{D}_{m}&#124;}\sum_{(\boldsymbol{x},\boldsymbol{y})\in\mathcal{D}_{m}}f\left(\mathcal{M}(\boldsymbol{x};\boldsymbol{w});\boldsymbol{y}\right),$
    |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $F_{m}(\boldsymbol{w}):=\frac{1}{|\mathcal{D}_{m}|}\sum_{(\boldsymbol{x},\boldsymbol{y})\in\mathcal{D}_{m}}f\left(\mathcal{M}(\boldsymbol{x};\boldsymbol{w});\boldsymbol{y}\right),$
    |  |'
- en: where $\mathcal{M}(\boldsymbol{x};\boldsymbol{w})$ and an input $\boldsymbol{x}$
    is defined on the model output and the ground truth $\boldsymbol{y}$ is part of
    the input $\boldsymbol{x}$, where a sequence of tokens in the input is used to
    predict the next token, and the ground truth is used to identify the part needing
    to be predicted by the model. Such a dataset is commonly adopted in previous works
    to fine-tune an LLM (Wei et al., [2021](#bib.bib41); Ouyang et al., [2022](#bib.bib26)).
    Then, based on the definition, a conventional FL system aims to find an optimal
    model across all clients, which is formulated as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{M}(\boldsymbol{x};\boldsymbol{w})$和输入$\boldsymbol{x}$在模型输出上定义，而真实值$\boldsymbol{y}$是输入$\boldsymbol{x}$的一部分，其中输入中的一个令牌序列用于预测下一个令牌，真实值用于确定模型需要预测的部分。这样的数据集在以往的工作中通常被用于微调LLM（Wei
    et al., [2021](#bib.bib41); Ouyang et al., [2022](#bib.bib26)）。然后，根据定义，传统的FL系统旨在找到所有客户端中的最优模型，公式化为
- en: '| (2) |  | $\min_{\boldsymbol{w}\in\mathbb{R}^{d}}F(\boldsymbol{w})=\sum_{m\in[M]}p_{m}F_{m}(\boldsymbol{w}),$
    |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\min_{\boldsymbol{w}\in\mathbb{R}^{d}}F(\boldsymbol{w})=\sum_{m\in[M]}p_{m}F_{m}(\boldsymbol{w}),$
    |  |'
- en: 'where $p_{m}=|\mathcal{D}_{m}|/|\mathcal{D}|$, where $\mathcal{D}$. Generally,
    this problem can be optimized by different FL algorithms (Li et al., [2019](#bib.bib16);
    Karimireddy et al., [2020](#bib.bib13); Wang et al., [2020](#bib.bib39)) repeating
    the following paradigm until convergence:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p_{m}=|\mathcal{D}_{m}|/|\mathcal{D}|$，其中$\mathcal{D}$。通常，这个问题可以通过不同的FL算法（Li
    et al., [2019](#bib.bib16); Karimireddy et al., [2020](#bib.bib13); Wang et al.,
    [2020](#bib.bib39)）进行优化，直到收敛：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 1: At the beginning of each round $t$;'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一步：在每一轮$t$开始时；
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 2: After receiving the model $\boldsymbol{w}^{(t)}$ performs multi-step
    local updates on $\boldsymbol{w}^{(t)}$;'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二步：在接收到模型$\boldsymbol{w}^{(t)}$后，对$\boldsymbol{w}^{(t)}$进行多步本地更新；
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 3: The server collects the locally updated model parameters $\boldsymbol{w}^{(t)}_{m}$
    for next round.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三步：服务器收集本地更新的模型参数$\boldsymbol{w}^{(t)}_{m}$以用于下一轮。
- en: Applying PEFT to federated LLM fine-tuning.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将PEFT应用于联邦LLM微调。
- en: The existing FL algorithms (Acar et al., [2020](#bib.bib2); Wu et al., [2023](#bib.bib43);
    He et al., [2023](#bib.bib10); Wang et al., [2023b](#bib.bib37), [2022](#bib.bib38))
    are confronted with computation and communication bottlenecks when fine-tuning
    an LLM. To mitigate the limitations, researchers have extended existing parameter-efficient
    fine-tuning (PEFT) approaches to FL, named FedPEFT (Yi et al., [2023](#bib.bib46);
    Zhang et al., [2023](#bib.bib50); Sun et al., [2024](#bib.bib31)). These methods
    minimize the number of trainable parameters by introducing a PEFT module and keeping
    the original LLM parameters unchanged. By focusing local updates exclusively on
    the PEFT module rather than the entire model, these methods effectively reduce
    computational load and support larger batch sizes on a single GPU. Additionally,
    the FL server merely aggregates the updated parameters of a given model, thus
    obviating the need to transmit unchanged parameters and minimizing communication
    overheads.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的 FL 算法（Acar 等，[2020](#bib.bib2)；Wu 等，[2023](#bib.bib43)；He 等，[2023](#bib.bib10)；Wang
    等，[2023b](#bib.bib37)，[2022](#bib.bib38)）在微调 LLM 时面临计算和通信瓶颈。为了缓解这些限制，研究人员将现有的参数高效微调（PEFT）方法扩展到
    FL，命名为 FedPEFT（Yi 等，[2023](#bib.bib46)；Zhang 等，[2023](#bib.bib50)；Sun 等，[2024](#bib.bib31)）。这些方法通过引入
    PEFT 模块并保持原始 LLM 参数不变，来最小化可训练参数的数量。通过专注于仅在 PEFT 模块上进行局部更新，而不是整个模型，这些方法有效地减少了计算负担，并支持单个
    GPU 上更大的批量大小。此外，FL 服务器仅汇总给定模型的更新参数，从而避免了传输未更改的参数，减少了通信开销。
- en: Nevertheless, FedPEFT is still confronted with the intrinsic challenge wherein
    clients face obstacles in loading an LLM due to its substantial computation prerequisites.
    For instance, the loading of a full-precision LLaMA-2-7B necessitates a memory
    capacity of no less than 28 GB.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，FedPEFT 仍然面临着固有的挑战，即客户端由于其庞大的计算需求而难以加载 LLM。例如，加载一个全精度的 LLaMA-2-7B 需要不低于
    28 GB 的内存容量。
- en: 2.2\. Related Work
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 相关工作
- en: The era of LLM poses the necessity of model privacy protection, where the details
    of LLM cannot be visible to the clients. To this end, Xiao et al. ([2023](#bib.bib44))
    proposes a method named Offsite-tuning under the scenario where there is a server
    (a.k.a. LLM owner) and a client, while Kuang et al. ([2024](#bib.bib14)) extends
    this work to an FL version and names it as FedOT. They achieve model privacy protection
    by compressing the model, where only some layers are visible to the clients. However,
    these works require the preservation of a large number of layers to guarantee
    the performance, hindering the effectiveness of model privacy protection. In contrast,
    our work only discloses a few model parameters of the original LLM to the clients,
    i.e., the clients only know the adapter parameters that come from the original
    LLM, while the emulator parameters have been updated and different from the original
    LLM. Besides, neither offsite-tuning (Xiao et al., [2023](#bib.bib44)) nor FedOT
    (Kuang et al., [2024](#bib.bib14)) consider the difference between alignment data
    on the server and the fine-tuning data on clients. In contrast, the bi-level optimization
    problem proposed in our work naturally considers this factor and we design updating
    rules based on it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 时代要求模型隐私保护，即 LLM 的细节不能对客户端可见。为此，Xiao 等人（[2023](#bib.bib44)）提出了一种名为 Offsite-tuning
    的方法，在服务器（即 LLM 拥有者）和客户端的场景下使用，而 Kuang 等人（[2024](#bib.bib14)）将此工作扩展到 FL 版本，并命名为
    FedOT。他们通过压缩模型来实现模型隐私保护，其中只有部分层对客户端可见。然而，这些工作需要保留大量层以保证性能，从而阻碍了模型隐私保护的有效性。相比之下，我们的工作仅向客户端公开原始
    LLM 的少量模型参数，即客户端仅了解来自原始 LLM 的适配器参数，而模拟器参数已被更新并与原始 LLM 不同。此外，既往的 offsite-tuning（Xiao
    等，[2023](#bib.bib44)）和 FedOT（Kuang 等，[2024](#bib.bib14)）都没有考虑服务器上的对齐数据与客户端上的微调数据之间的差异。相比之下，我们的工作提出的双层优化问题自然考虑了这一因素，并根据此设计了更新规则。
- en: Black-box is also a practical way to protect model privacy, where the clients
    access the LLM via an API, and they cannot fine-tune the LLM. Therefore, the optimization
    solely relies on prompt-based learning (Sordoni et al., [2023](#bib.bib29); Li
    and Liang, [2021](#bib.bib17); Lester et al., [2021](#bib.bib15); Liu et al.,
    [2023](#bib.bib21)). In the context of FL, there are two typical works, namely,
    Fed-BBPT (Lin et al., [2023](#bib.bib20)) and FedBPT (Sun et al., [2023](#bib.bib30)).
    These two works guarantee the model privacy in FL, but they should transmit the
    prompt together with the input to the LLM owner, leading to concerns about data
    privacy when the input contains sensitive information, violating the requirement
    of FL. In contrast, the proposed FedBiOT will not lead to this concern because
    its training is fully on the clients such that the data are never shared with
    others.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱方法也是保护模型隐私的一种实际方式，其中客户通过 API 访问 LLM，且无法对 LLM 进行微调。因此，优化完全依赖于基于提示的学习（Sordoni
    et al., [2023](#bib.bib29); Li and Liang, [2021](#bib.bib17); Lester et al., [2021](#bib.bib15);
    Liu et al., [2023](#bib.bib21)）。在 FL 的背景下，有两个典型的工作，即 Fed-BBPT（Lin et al., [2023](#bib.bib20)）和
    FedBPT（Sun et al., [2023](#bib.bib30)）。这两项工作确保了 FL 中的模型隐私，但它们需要将提示与输入一起传输给 LLM
    所有者，这会导致当输入包含敏感信息时对数据隐私的担忧，违反了 FL 的要求。相比之下，所提出的 FedBiOT 不会导致这一担忧，因为其训练完全在客户端进行，数据不会与他人共享。
- en: 3\. FedBiOT
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. FedBiOT
- en: Given that some clients may be unable to load a complete LLM, this section introduces
    an algorithm designed to enable these clients to fine-tune the LLM without requiring
    access to its full version. In other words, our goal is to refine the part of
    a compressed model that should yield performance comparable to fine-tuning its
    counterpart within a full model. To accomplish this, the server initially compresses
    the LLM and divides it into two distinct components, each serving specific functions.
    The first component, termed an emulator, is tasked with replicating the behavior
    of the uncompressed LLM. The second component, referred to as an adapter, focuses
    on adeptly acquiring domain-specific linguistic patterns from clients. Upon reintegrating
    the adapter into the uncompressed emulator, its performance should demonstrate
    significant improvement compared to the original LLM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于某些客户可能无法加载完整的 LLM，本节介绍了一种算法，旨在使这些客户能够微调 LLM 而无需访问其完整版本。换句话说，我们的目标是优化一个压缩模型的部分，使其性能与在完整模型中微调相当。为此，服务器首先压缩
    LLM 并将其分为两个不同的组件，每个组件执行特定功能。第一个组件称为仿真器，负责复制未压缩 LLM 的行为。第二个组件称为适配器，专注于从客户处熟练获取领域特定的语言模式。在将适配器重新集成到未压缩的仿真器中后，其性能应该比原始
    LLM 显著提高。
- en: However, direct fine-tuning of the adapter on its models presents two significant
    limitations. Firstly, given that a single layer of a large language model (LLM)
    comprises millions of parameters, such as the decoder layer of LLaMA-2 with 202
    million parameters, the adapter’s parameter count is immense. This necessitates
    clients to possess powerful computational equipment to handle the fine-tuning
    of the layer. Additionally, transmitting the layer updates to the server poses
    another bottleneck, particularly in scenarios with unreliable network connections
    or limited bandwidth, hindering the smooth transmission of updates to the server.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接对适配器在其模型上进行微调存在两个显著的限制。首先，考虑到大型语言模型（LLM）的单层包括数百万个参数，例如 LLaMA-2 的解码器层有 2.02
    亿个参数，适配器的参数数量也非常庞大。这就要求客户具备强大的计算设备来处理该层的微调。此外，将层更新传输到服务器也是另一个瓶颈，特别是在网络连接不可靠或带宽有限的情况下，这会阻碍更新顺利传输到服务器。
- en: To address these constraints, we integrate LoRA (Hu et al., [2021](#bib.bib12)),
    a PEFT module, into our proposed method. LoRA significantly reduces the number
    of tunable parameters, with a LoRA module for LLaMA-2 comprising 0.13 million
    trainable parameters, which is merely 0.06% of the original layer’s size. Consequently,
    the communication cost experiences a remarkable reduction of 99.94% compared to
    transmitting a full layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些限制，我们将 LoRA（Hu et al., [2021](#bib.bib12)），一个 PEFT 模块，集成到我们提出的方法中。LoRA
    显著减少了可调参数的数量，以 LLaMA-2 为例，其 LoRA 模块包含 13 万个可训练参数，仅为原层大小的 0.06%。因此，与传输完整层相比，通信成本减少了
    99.94%。
- en: Organization.
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组织。
- en: 'In the subsequent sections, we will delve into the concrete details of the
    algorithm design. Specifically, Section [3.1](#S3.SS1 "3.1\. Compressed Model
    Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in
    Federated Learning without Full Model") illustrates how the compressed model is
    prepared. Following that, Section [3.2](#S3.SS2 "3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") discusses the problem formulation for the aforementioned
    objectives. On top of this, Section [3.3](#S3.SS3 "3.3\. Client Updates ‣ 3\.
    FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model")
    and Section [3.4](#S3.SS4 "3.4\. Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM
    Local Fine-tuning in Federated Learning without Full Model") outline the detailed
    steps of the proposed algorithm, namely local updates and server aggregation,
    showcasing the seamless integration of LoRA modules. Full implementation of the
    pseudocode is given in Algorithm [2](#alg2 "Algorithm 2 ‣ Discussion. ‣ 3.2\.
    Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在随后的部分，我们将深入探讨算法设计的具体细节。具体来说，部分[3.1](#S3.SS1 "3.1\. Compressed Model Preparation:
    Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model")说明了如何准备压缩模型。接下来，部分[3.2](#S3.SS2 "3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model")讨论了上述目标的问题定义。除此之外，部分[3.3](#S3.SS3 "3.3\. Client Updates ‣
    3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full
    Model")和部分[3.4](#S3.SS4 "3.4\. Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM
    Local Fine-tuning in Federated Learning without Full Model")概述了所提议算法的详细步骤，即本地更新和服务器聚合，展示了LoRA模块的无缝集成。伪代码的完整实现见算法[2](#alg2
    "Algorithm 2 ‣ Discussion. ‣ 3.2\. Formulation of Bi-level Optimization ‣ 3\.
    FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model")。'
- en: '3.1\. Compressed Model Preparation: Linear Dropout'
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 压缩模型准备：线性丢弃
- en: 'Suppose a pre-trained LLM has a total of $n$ layers of transformers to form
    a submodel. We denoted this by a function $\textsf{LayerExtract}(\mathcal{M},L)$
    from the model $\mathcal{M}$. The function consists of the following three steps,
    and its pseudocode implementation of the first two steps is presented in Algorithm
    [1](#alg1 "Algorithm 1 ‣ Step 2: Layer dropout to form emulator. ‣ 3.1\. Compressed
    Model Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '假设一个预训练的LLM有总共$n$层的transformers来形成一个子模型。我们通过模型$\mathcal{M}$中的函数$\textsf{LayerExtract}(\mathcal{M},L)$来表示这一点。该函数包括以下三个步骤，其前两步的伪代码实现见算法[1](#alg1
    "Algorithm 1 ‣ Step 2: Layer dropout to form emulator. ‣ 3.1\. Compressed Model
    Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in
    Federated Learning without Full Model")。'
- en: 'Step 1: Identify the adapters in the original model.'
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '步骤 1: 确定原始模型中的适配器。'
- en: We choose the bottom few layers¹¹1The bottom/last layers refer to the transformer
    decoders near the output, while the top/first layers refer to the part close to
    the input. of the original LLM as the adapter. To be more specific, suppose the
    size of the adapter is $a$. Therefore, $\mathcal{A}\leftarrow\textsf{LayerExtract}(\mathcal{M},L_{\mathcal{A}})$.
    We denote $\boldsymbol{w}_{\mathcal{A}}$. The $\boldsymbol{w}_{\mathcal{A}}$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择原始LLM的底部几层¹¹1底部/最后层指的是接近输出的transformer解码器，而顶部/第一层指的是接近输入的部分。作为适配器。具体来说，假设适配器的大小为$a$。因此，$\mathcal{A}\leftarrow\textsf{LayerExtract}(\mathcal{M},L_{\mathcal{A}})$。我们用$\boldsymbol{w}_{\mathcal{A}}$表示$\boldsymbol{w}_{\mathcal{A}}$。
- en: The choice of adapters brings two advantages. First, regarding the computation
    constraints of the clients, this proposed adapter is computation-efficient because
    it only needs to store the activations of transformers in the last few layers,
    leading to a lower memory cost. Second, as the adapter focuses more on domain-specific
    features, it is eco-friendly to spend the effort fine-tuning the last few layers.
    The conclusion is drawn from a well-known finding (Yosinski et al., [2014](#bib.bib47))
    in neural networks that the first few layers tend to learn general features while
    the last layers encode specific ones.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适配器带来了两个优点。首先，关于客户端的计算约束，所提议的适配器在计算上高效，因为它只需要存储最后几层transformers的激活，从而降低了内存成本。其次，由于适配器更关注领域特定的特征，因此花费精力对最后几层进行微调是环保的。这个结论源于一个广为人知的发现（Yosinski等，[2014](#bib.bib47)），即神经网络的前几层通常学习通用特征，而最后几层则编码特定特征。
- en: 'Step 2: Layer dropout to form emulator.'
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '步骤 2: 层丢弃以形成模拟器。'
- en: Inspired by the experimental results presented by Xiao et al. ([2023](#bib.bib44)),
    we form an emulator by means of a uniform layer dropout (Sajjad et al., [2023](#bib.bib27))
    from the remaining part $\mathcal{E}^{*}$. Denote there are $n_{\mathcal{E}^{*}}$.
    The dropout rate of the emulator is denoted as $\beta=\frac{|L_{\mathcal{E}}|}{n_{\mathcal{E}^{*}}}$
    as emulator and $\mathcal{E}^{*}$ and $\boldsymbol{w}_{\mathcal{E}^{*}}$ and $\mathcal{E}^{*}$,
    respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 灵感来自 Xiao 等人（[2023](#bib.bib44)）所呈现的实验结果，我们通过从剩余部分 $\mathcal{E}^{*}$ 中进行均匀层
    dropout（Sajjad 等人，[2023](#bib.bib27)）来形成一个模拟器。记 $n_{\mathcal{E}^{*}}$。模拟器的 dropout
    率记作 $\beta=\frac{|L_{\mathcal{E}}|}{n_{\mathcal{E}^{*}}}$，模拟器和 $\mathcal{E}^{*}$
    及 $\boldsymbol{w}_{\mathcal{E}^{*}}$ 和 $\mathcal{E}^{*}$，分别。
- en: 'After training, we can attain two combined models, namely, Adaptor + Emulator
    (AdapEmu, i.e., $\mathcal{E}\circ\mathcal{A}$). As Xiao et al. ([2023](#bib.bib44))
    describes, AdapFu performs better than AdapEmu. These two models have different
    functionalities in real-world scenarios: AdapEmu is adopted if the input contains
    sensitive information that cannot be shared with the LLM owner, e.g., drafting
    a petition letter, while AdapFu is adopted when the users aim to have better generation
    results, e.g., solving a math problem.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以获得两个结合模型，即 Adaptor + Emulator（AdapEmu，即 $\mathcal{E}\circ\mathcal{A}$）。正如
    Xiao 等人（[2023](#bib.bib44)）所述，AdapFu 的表现优于 AdapEmu。这两种模型在实际场景中具有不同的功能：如果输入包含无法与
    LLM 拥有者共享的敏感信息，例如起草请愿信，则采用 AdapEmu；而当用户希望获得更好的生成结果时，例如解决数学问题，则采用 AdapFu。
- en: Algorithm 1 LayerExtract
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 LayerExtract
- en: 'Input: pre-trained LLM $\mathcal{M}$, dropout rate $\beta$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：预训练的 LLM $\mathcal{M}$，dropout 率 $\beta$。
- en: '1:Get the size of model: $n\leftarrow|\mathcal{M}|$2:Compute the number of
    layers in the compressed model, i.e.,'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 获取模型大小：$n\leftarrow|\mathcal{M}|$2: 计算压缩模型中的层数，即，'
- en: '|  | $\displaystyle n^{\prime}\leftarrow\lfloor\beta(n-s)\rfloor$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle n^{\prime}\leftarrow\lfloor\beta(n-s)\rfloor$ |  |'
- en: 3:Initialize non-compressed emulator $\mathcal{E}^{*}\leftarrow\{\mathcal{M}_{0},\dots\mathcal{M}_{n-s-1}\}$4:Initialize
    emulator $\mathcal{E}\leftarrow\{\}$6:for $j=0,\dots,n^{\prime}-1$ to emulator,
    i.e.,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 初始化非压缩模拟器 $\mathcal{E}^{*}\leftarrow\{\mathcal{M}_{0},\dots\mathcal{M}_{n-s-1}\}$4:
    初始化模拟器 $\mathcal{E}\leftarrow\{\}$6: 对模拟器进行 $j=0,\dots,n^{\prime}-1$ 循环，即，'
- en: '|  | $\displaystyle\mathcal{E}\leftarrow\mathcal{E}\cup\{\mathcal{M}_{\lfloor
    j\times stride\rfloor}\}$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{E}\leftarrow\mathcal{E}\cup\{\mathcal{M}_{\lfloor
    j\times stride\rfloor}\}$ |  |'
- en: 8:end for9:return $\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}$![Refer
    to caption](img/fb78219d4e0cfd52dbf62ff7cf6f95f9.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 8:end for9:return $\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}$![参见标题](img/fb78219d4e0cfd52dbf62ff7cf6f95f9.png)
- en: Figure 1\. The Workflow of FedBiOT during the FL training
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. FedBiOT 在 FL 训练中的工作流程
- en: 'Step 3: Pre-alignment.'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：预对齐。
- en: Before the FL training stage, we pre-align the emulator with the non-compressed
    one such that it can mimic the performance of the raw model. Assume there is a
    public dataset $\mathcal{D}_{public}$, representing the input and the ground truth,
    respectively. Therefore, in the rest of the section, we assume the input $\boldsymbol{x}$
    contains an attention mask that can identify the ground truth.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FL 训练阶段之前，我们将模拟器与非压缩版本预对齐，以使其能够模拟原始模型的性能。假设存在一个公共数据集 $\mathcal{D}_{public}$，分别表示输入和真实值。因此，在本节其余部分，我们假设输入
    $\boldsymbol{x}$ 包含一个注意力掩码，可以识别真实值。
- en: 'Instead of training the compressed model with the ground truth, we utilize
    knowledge distillation (Hinton et al., [2015](#bib.bib11)) to transfer the general
    linguistic patterns from the original LLM to the compressed one by tuning the
    emulator $\mathcal{E}$-norm:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用知识蒸馏（Hinton 等人，[2015](#bib.bib11)）而非使用真实值来训练压缩模型，通过调整模拟器 $\mathcal{E}$-norm
    将原始 LLM 的一般语言模式转移到压缩模型中：
- en: '| (3) |  | $\displaystyle\mathcal{L}_{repr}$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\mathcal{L}_{repr}$ |  |'
- en: 'Additionally, we ensure the compressed model has consistent final outputs of
    the original LLM on the ground truth by minimizing the following KL divergence:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通过最小化以下 KL 散度来确保压缩模型在真实值上的最终输出与原始 LLM 一致：
- en: '| (4) |  | $\displaystyle\mathcal{L}_{kd}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\mathcal{L}_{kd}$ |  |'
- en: In a nutshell, we optimize the emulator $\mathcal{E}$ by finding the optimal
    parameters for the following equation on the public dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们通过在公共数据集上寻找以下方程的最佳参数来优化模拟器 $\mathcal{E}$。
- en: '| (5) |  | $1$2 |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $1$2 |  |'
- en: Let the optimal emulator $\mathcal{E}$ with the parameter of $\boldsymbol{w}_{\mathcal{E}_{init}}$
    with the parameter of $\boldsymbol{w}_{\mathcal{A}_{init}}$. To reduce the computation
    and communication costs, we incorporate LoRA (Hu et al., [2021](#bib.bib12)) for
    the adapter $\mathcal{A}$, denoted as $\mathcal{A}_{lora}$, respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让最优模拟器 $\mathcal{E}$ 具备参数 $\boldsymbol{w}_{\mathcal{E}_{init}}$，与参数 $\boldsymbol{w}_{\mathcal{A}_{init}}$
    的适配器 $\mathcal{A}$ 相结合。为了减少计算和通信成本，我们为适配器 $\mathcal{A}$ 引入 LoRA（Hu 等，[2021](#bib.bib12)），记作
    $\mathcal{A}_{lora}$。
- en: 'Before diving into the details of the proposed FedBiOT, we briefly go through
    the workflow as described in Figure [1](#S3.F1 "Figure 1 ‣ Step 2: Layer dropout
    to form emulator. ‣ 3.1\. Compressed Model Preparation: Linear Dropout ‣ 3\. FedBiOT
    ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model"). The
    figure visually presents the workflow of the federated learning process of our
    proposed FedBiOT, including the local updates on clients (Section [3.3](#S3.SS3
    "3.3\. Client Updates ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")) and the aggregation on the server (Section [3.4](#S3.SS4
    "3.4\. Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")). At the beginning of clients’ fine-tuning, the
    server broadcasts the adapter $\mathcal{A}_{lora}$ to the clients. Subsequently,
    the clients perform multiple local updates to fine-tune the adapter $\mathcal{A}_{lora}$
    to the server, and the server thereby aggregates the adapters. To ensure that
    the emulator is still able to reproduce the behavior of the uncompressed LLM,
    the server fine-tunes the emulator $\mathcal{E}_{lora}$ with the public dataset.
    Finally, the server distributes the updated parameters to the clients and launches
    a new round of training.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在深入探讨提出的 FedBiOT 的细节之前，我们简要回顾如图 [1](#S3.F1 "图 1 ‣ 步骤 2: 层丢弃形成模拟器 ‣ 3.1\. 压缩模型准备:
    线性丢弃 ‣ 3\. FedBiOT ‣ FedBiOT: 在联邦学习中进行 LLM 本地微调而无需完整模型") 所描述的工作流程。该图直观地展示了我们提出的
    FedBiOT 的联邦学习过程，包括客户端的本地更新（第 [3.3](#S3.SS3 "3.3\. 客户端更新 ‣ 3\. FedBiOT ‣ FedBiOT:
    在联邦学习中进行 LLM 本地微调而无需完整模型") 节）和服务器上的聚合（第 [3.4](#S3.SS4 "3.4\. 模型聚合 ‣ 3\. FedBiOT
    ‣ FedBiOT: 在联邦学习中进行 LLM 本地微调而无需完整模型") 节）。在客户端微调开始时，服务器将适配器 $\mathcal{A}_{lora}$
    广播给客户端。随后，客户端执行多次本地更新，以微调适配器 $\mathcal{A}_{lora}$ 并将其传输到服务器，服务器因此聚合适配器。为了确保模拟器仍然能够重现未压缩
    LLM 的行为，服务器使用公开数据集对模拟器 $\mathcal{E}_{lora}$ 进行微调。最后，服务器将更新的参数分发给客户端，并启动新一轮的训练。'
- en: 3.2\. Formulation of Bi-level Optimization
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 双层优化的公式
- en: 'As discussed in Section [3.1](#S3.SS1 "3.1\. Compressed Model Preparation:
    Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"), we compress and divide the LLM into two parts, namely, an
    adapter and an emulator. These two components are designated to satisfy the following
    objectives:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [3.1](#S3.SS1 "3.1\. 压缩模型准备: 线性丢弃 ‣ 3\. FedBiOT ‣ FedBiOT: 在联邦学习中进行 LLM
    本地微调而无需完整模型") 节所述，我们将 LLM 压缩并分为两个部分，即适配器和模拟器。这两个组件旨在满足以下目标：'
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Emulator should be tuned towards perfectly imitating the non-compressed part
    in the full model, especially in extracting and encoding information on the server’s
    datasets.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模拟器应调整以完美模拟完整模型中的未压缩部分，特别是在提取和编码服务器数据集中的信息时。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapter should be able to digest the output of the emulator efficiently and
    should be encoded with the knowledge from clients’ datasets effectively.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适配器应能高效处理模拟器的输出，并且应有效编码来自客户端数据集的知识。
- en: 'Define $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora}}\}$
    to integrate the LoRA parameters while the initial parameters for the adapter
    and the emulator remain unchanged during the training. Toward the goal, we formulate
    the objectives as a bi-level optimization problem:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora}}\}$
    以整合 LoRA 参数，同时适配器和模拟器的初始参数在训练过程中保持不变。为此，我们将目标设定为一个双层优化问题：
- en: '| (6) |  |  | $1$2 |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  |  | $1$2 |  |'
- en: '|  | $\displaystyle s.t.\quad$ |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\quad$ |  |'
- en: '|  |  | $\displaystyle\mathcal{L}(\boldsymbol{x})\overset{\triangle}{=}\left\&#124;\mathcal{E}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}}\right)-\mathcal{E}^{*}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}^{*}}\right)\right\&#124;_{2}^{2}$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}(\boldsymbol{x})\overset{\triangle}{=}\left\|
    \mathcal{E}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}}\right)-\mathcal{E}^{*}\left(\boldsymbol{x};\boldsymbol{w}_{\mathcal{E}^{*}}\right)\right\|_{2}^{2}$
    |  |'
- en: '| (7) |  |  | $\displaystyle\qquad\quad+\lambda\cdot D_{KL}\left(\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}^{*}}\})\&#124;\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\})\right)$
    |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  |  | $\displaystyle\qquad\quad+\lambda\cdot D_{KL}\left(\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}^{*}}\})\|\mathcal{M}(\boldsymbol{x};\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\})\right)$
    |  |'
- en: where $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$ reconstructs for the same
    size of the adapter $\boldsymbol{w}_{\mathcal{A}}$ represents the public dataset
    on the server, which can be unlabeled. $D_{KL}(\cdot\|\cdot)$ and $\lambda$ are
    hyperparameters.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$ 为与适配器 $\boldsymbol{w}_{\mathcal{A}}$
    大小相同的重构，表示服务器上的公共数据集，这些数据集可以是未标记的。$D_{KL}(\cdot\|\cdot)$ 和 $\lambda$ 是超参数。
- en: 'The upper-level objective (Equation ([6](#S3.E6 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"))).'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '上层目标（方程 ([6](#S3.E6 "在 3.2. 双层优化的表述 ‣ 3. FedBiOT ‣ FedBiOT: 联邦学习中的 LLM 本地微调，无需完整模型"))）。'
- en: 'The upper-level objective function consists of two terms. The first term represents
    the loss of the model on local clients’ data, with the current emulator and adapter.
    It follows a classic weighted average loss in FL to balance the loss of different
    clients’ heterogeneous local data. The goal of introducing this term is straightforward:
    by minimizing the loss of the first term, we expect the emulator-adapter combination
    to be improved on the local training set. The second term is a regularization
    of the adapter component to ensure it will be within a reasonable distance from
    the synchronized and broadcast adapter at the beginning of each communication
    round. Enforcing a restriction on the adapter’s change can reduce the difference
    of losses for the emulator distillation after locally adapter are tuned locally
    on clients, so it can help the convergence of emulator distillation.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上层目标函数由两个项组成。第一个项表示模型在本地客户端数据上的损失，使用当前的模拟器和适配器。它遵循 FL 中经典的加权平均损失，以平衡不同客户端的异质本地数据的损失。引入这一项的目标很简单：通过最小化第一个项的损失，我们期望模拟器-适配器组合在本地训练集上得到改进。第二个项是对适配器组件的正则化，以确保其在每次通信回合开始时与同步和广播适配器之间保持在合理的距离内。对适配器变化施加限制可以减少本地客户端上调整后的适配器对模拟器蒸馏损失的差异，从而有助于模拟器蒸馏的收敛。
- en: 'The lower-level objective (Equation ([7](#S3.E7 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"))).'
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '下层目标（方程 ([7](#S3.E7 "在 3.2. 双层优化的表述 ‣ 3. FedBiOT ‣ FedBiOT: 联邦学习中的 LLM 本地微调，无需完整模型"))）。'
- en: The first term in the constraint is the $\ell_{2}$-norm difference between the
    activation output by the emulator and the full model. The second term is the KL
    divergence between the output of output distribution of the full model-adapter
    combination and the emulator-adapter. Although only the emulator is trainable
    to minimize the loss of these two terms, these two terms provide different optimization
    meaning for the emulator. The first term encourages the emulator to provide activations
    as close as possible to the full model, *excluding* the effect of the adapter.
    The second term ensures the emulator can provide output distributions close to
    the one when the full model with adapters is added on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 约束中的第一个项是模拟器输出的激活与完整模型之间的 $\ell_{2}$-范数差异。第二个项是完整模型-适配器组合的输出分布与模拟器-适配器的输出分布之间的
    KL 散度。尽管只有模拟器是可训练的以最小化这两个项的损失，但这两个项为模拟器提供了不同的优化意义。第一个项鼓励模拟器提供尽可能接近完整模型的激活，*排除*适配器的影响。第二个项确保模拟器可以提供接近于完整模型加上适配器时的输出分布。
- en: Discussion.
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 讨论。
- en: 'The introduced algorithm can optimize the bi-level problems (i.e., Equation
    ([6](#S3.E6 "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")) and ([7](#S3.E7
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model"))) to an equilibrium point
    for both adapter and emulator. This is because when we optimize the adapter, the
    fixed emulator constrains its updates, and vice versa, and thereby, the emulator
    and adapter are distilled or trained interchangeably. At this equilibrium, the
    emulator can more faithfully extract and encode the information for the clients’
    dataset and benefit from the training of the adapter in reverse.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '引入的算法可以将双层问题（即方程 ([6](#S3.E6 "在 3.2\. 双层优化的表述 ‣ 3\. FedBiOT ‣ FedBiOT: 无需完整模型的联邦学习中的LLM本地微调"))
    和 ([7](#S3.E7 "在 3.2\. 双层优化的表述 ‣ 3\. FedBiOT ‣ FedBiOT: 无需完整模型的联邦学习中的LLM本地微调")))
    优化到适配器和模拟器的平衡点。这是因为当我们优化适配器时，固定的模拟器限制了其更新，反之亦然，因此，模拟器和适配器可以交替提取或训练。在这个平衡点，模拟器可以更真实地提取和编码客户端数据集的信息，并从适配器的反向训练中受益。'
- en: 'Additionally, FedBiOT does not require the design of an emulator to follow
    linear dropout. Instead, this is a general framework that compresses an LLM and
    divides it into two components: an emulator and an adapter. There are numerous
    designs for the emulator, but they share the same objective where the emulator
    simulates the non-compressed part of an LLM. For simplicity, we follow offsite-tuning
    (Xiao et al., [2023](#bib.bib44)) and prepare the emulator by means of uniform
    layer dropout (Sajjad et al., [2023](#bib.bib27)) to demonstrate the effectiveness
    of FedBiOT.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，FedBiOT 不要求模拟器遵循线性丢弃设计。相反，这是一个将LLM压缩并分成两个组件（模拟器和适配器）的通用框架。模拟器有很多设计，但它们有相同的目标，即模拟LLM的非压缩部分。为了简单起见，我们遵循离线微调
    (Xiao et al., [2023](#bib.bib44)) 并通过均匀层丢弃 (Sajjad et al., [2023](#bib.bib27))
    准备模拟器，以展示 FedBiOT 的有效性。
- en: Algorithm 2 FedBiOT
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 FedBiOT
- en: 'Input: learning rate $\eta$, global model alignment steps $E$, local update
    regularization $\epsilon$, pre-trained LLM $\mathcal{M}$, adapter size $s$, number
    of clients $M$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 学习率 $\eta$，全局模型对齐步骤 $E$，本地更新正则化 $\epsilon$，预训练的LLM $\mathcal{M}$，适配器大小
    $s$，客户端数量 $M$。'
- en: '1:$\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}\leftarrow\textsf{LayerExtract}(\mathcal{M},s,\beta)$
    See Algo. [1](#alg1 "Algorithm 1 ‣ Step 2: Layer dropout to form emulator. ‣ 3.1\.
    Compressed Model Preparation: Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model") for details2:for $t=0,\dots,R-1$ do4:         Randomly
    sample $(\boldsymbol{x},\boldsymbol{y})$5:         Optimize $\boldsymbol{w}_{\mathcal{E}_{lora}}$
    with clients $m\in[M]$ in parallel do9:         Initialize $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$,
    and $\boldsymbol{w}_{\mathcal{E}}$ do11:              Compute a gradient $g$ using
    Equation ([10](#S3.E10 "In 3.3\. Client Updates ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model"))13:         end for14:         Communicate
    $\boldsymbol{w}_{\mathcal{A}_{lora},m}$ using Equation ([11](#S3.E11 "In 3.4\.
    Model Aggregation ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model"))17:end for18:return AdapEmu $\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\}$'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '1:$\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}},\boldsymbol{w}_{\mathcal{E}^{*}}\leftarrow\textsf{LayerExtract}(\mathcal{M},s,\beta)$
    详情见算法 [1](#alg1 "算法 1 ‣ 步骤 2: 层丢弃形成模拟器 ‣ 3.1\. 压缩模型准备: 线性丢弃 ‣ 3\. FedBiOT ‣ FedBiOT:
    无需完整模型的联邦学习中的LLM本地微调")2:对于 $t=0,\dots,R-1$ 执行4:         随机采样 $(\boldsymbol{x},\boldsymbol{y})$5:         在客户端
    $m\in[M]$ 中并行优化 $\boldsymbol{w}_{\mathcal{E}_{lora}}$ 9:         初始化 $\boldsymbol{w}_{\mathcal{A}_{lora}}^{(t)}$
    和 $\boldsymbol{w}_{\mathcal{E}}$ 11:              使用方程 ([10](#S3.E10 "在 3.3\.
    客户更新 ‣ 3\. FedBiOT ‣ FedBiOT: 无需完整模型的联邦学习中的LLM本地微调")) 计算梯度 $g$13:         结束 14:         使用方程
    ([11](#S3.E11 "在 3.4\. 模型聚合 ‣ 3\. FedBiOT ‣ FedBiOT: 无需完整模型的联邦学习中的LLM本地微调")) 通信
    $\boldsymbol{w}_{\mathcal{A}_{lora},m}$17:结束 18:返回 AdapEmu $\{\boldsymbol{w}_{\mathcal{A}},\boldsymbol{w}_{\mathcal{E}}\}$'
- en: 3.3\. Client Updates
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 客户更新
- en: During the local updates, the clients barely fine-tune the parameters of the
    adapter $\mathcal{A}$. By enabling LoRA, the LoRA of the adapter will get updated,
    and therefore, the clients should upload the updated $\boldsymbol{w}_{\mathcal{A}_{lora}}$
    to the server after the local fine-tuning ends.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地更新过程中，客户端几乎不会微调适配器 $\mathcal{A}$ 的参数。通过启用 LoRA，适配器的 LoRA 将会更新，因此，客户端在本地微调结束后应将更新后的
    $\boldsymbol{w}_{\mathcal{A}_{lora}}$ 上传到服务器。
- en: Consider client $i\in[M]$-th round. Before optimizing the adapter locally, the
    client receives the updated emulator $\boldsymbol{w}_{\mathcal{E}_{lora}}$ from
    the client, and we denote them by
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑客户端 $i\in[M]$-轮次。在本地优化适配器之前，客户端从服务器接收更新后的模拟器 $\boldsymbol{w}_{\mathcal{E}_{lora}}$，我们用
- en: '| (8) |  | $1$2 |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $1$2 |  |'
- en: 'Suppose the client performs the local update for $K$, a LoRA module of the
    adapter. Therefore, based on Equation ([6](#S3.E6 "In 3.2\. Formulation of Bi-level
    Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model")), the gradient w.r.t. $\boldsymbol{w}_{\mathcal{A}_{lora},m}$
    should be'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '假设客户端对 $K$，适配器的一个 LoRA 模块进行本地更新。因此，根据方程式（[6](#S3.E6 "In 3.2\. Formulation of
    Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")），相对于 $\boldsymbol{w}_{\mathcal{A}_{lora},m}$ 的梯度应该是'
- en: '| (9) |  | $1$2 |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $1$2 |  |'
- en: where $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora},m}\}$
    be the optimizer (e.g., SGD and AdamW (Loshchilov and Hutter, [2018](#bib.bib22)))
    that updates the model parameters, and $\eta$ be the learning rate. Therefore,
    in each local update, the local model is updated for
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{w}_{\mathcal{A}}=\{\boldsymbol{w}_{\mathcal{A}_{init}},\boldsymbol{w}_{\mathcal{A}_{lora},m}\}$
    是优化器（例如 SGD 和 AdamW（Loshchilov 和 Hutter，[2018](#bib.bib22)）），它更新模型参数，$\eta$ 是学习率。因此，在每次本地更新中，本地模型更新为
- en: '| (10) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora},m}\leftarrow\textsc{Optim}(\boldsymbol{w}_{\mathcal{A}_{lora},m},g,\eta)$
    |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora},m}\leftarrow\textsc{Optim}(\boldsymbol{w}_{\mathcal{A}_{lora},m},g,\eta)$
    |  |'
- en: After finishing the local update, the client $i$ to the server.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本地更新后，客户端 $i$ 将更新后的模型发送到服务器。
- en: 3.4\. Model Aggregation
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 模型聚合
- en: During the server aggregation, the server performs the weighted average to update
    the adapters $\mathcal{A}$. By enabling the LoRA, only the parameters $\boldsymbol{w}_{\mathcal{A}_{lora}}$
    in the emulator are updated, while the rest (i.e., $\boldsymbol{w}_{\mathcal{A}_{init}}$)
    remain unchanged.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器聚合过程中，服务器执行加权平均来更新适配器 $\mathcal{A}$。通过启用 LoRA，仅更新模拟器中的参数 $\boldsymbol{w}_{\mathcal{A}_{lora}}$，而其他参数（即
    $\boldsymbol{w}_{\mathcal{A}_{init}}$）保持不变。
- en: 'First, the server collects a set of updated LoRAs of the adapter, i.e., $\left\{\boldsymbol{w}_{\mathcal{A}_{lora},m}\right\}_{m\in[M]}$
    from the clients. Based on the definition of Equation ([6](#S3.E6 "In 3.2\. Formulation
    of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")), the server performs weighted aggregation via'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，服务器从客户端收集一组更新的适配器 LoRAs，即 $\left\{\boldsymbol{w}_{\mathcal{A}_{lora},m}\right\}_{m\in[M]}$。根据方程式（[6](#S3.E6
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model")）的定义，服务器通过加权聚合来执行'
- en: '| (11) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora}}\leftarrow\sum_{m\in[M]}p_{m}\boldsymbol{w}_{\mathcal{A}_{lora},m}$
    |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle\boldsymbol{w}_{\mathcal{A}_{lora}}\leftarrow\sum_{m\in[M]}p_{m}\boldsymbol{w}_{\mathcal{A}_{lora},m}$
    |  |'
- en: After the weighted averaging, the server distills the emulator $\mathcal{E}$
    and the updated adapter $\mathcal{A}$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 加权平均后，服务器提炼了模拟器 $\mathcal{E}$ 和更新后的适配器 $\mathcal{A}$。
- en: 4\. Experiments
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 4.1\. Experimental Setup
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: This section discusses the implementation of our experiments, covering details
    such as the model utilized and evaluation metrics. The code is now available at
    [https://github.com/HarliWu/FedBiOT](https://github.com/HarliWu/FedBiOT).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了我们实验的实施细节，包括所使用的模型和评估指标。代码现已在 [https://github.com/HarliWu/FedBiOT](https://github.com/HarliWu/FedBiOT)
    上提供。
- en: Model and computation environment.
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和计算环境。
- en: The experiments utilize LLaMA-2-7B, an open-source pre-trained LLM maintained
    by Meta and released in July 2023 (Touvron et al., [2023b](#bib.bib35)). Preceding
    this, the model’s first generation was introduced in February 2023 (Touvron et al.,
    [2023a](#bib.bib34)). This model supports a maximum of 4096 input tokens and consists
    of 32 hidden layers with a total of 6.7 billion parameters. The experimental setup
    involves machines equipped with Nvidia A100 GPU cards, Intel Xeon Platinum 8369B
    CPUs, and a 512GB RAM configuration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 实验使用 LLaMA-2-7B，这是一个由 Meta 维护的开源预训练 LLM，于 2023 年 7 月发布（Touvron 等，[2023b](#bib.bib35)）。在此之前，该模型的第一代于
    2023 年 2 月推出（Touvron 等，[2023a](#bib.bib34)）。该模型支持最多 4096 个输入令牌，由 32 个隐藏层和总共 67
    亿个参数组成。实验设置涉及配备 Nvidia A100 GPU 卡、Intel Xeon Platinum 8369B CPU 和 512GB RAM 配置的机器。
- en: Table 1\. Dataset details for LLM training and evaluation
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. LLM训练和评估的数据集详情
- en: '|   Task | Training Dataset | # training samples | # clients | Partition Rules
    | Max. | Min. | Std. | Test Dataset | # test samples |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 训练数据集 | # 训练样本 | # 客户端 | 分区规则 | 最大值 | 最小值 | 标准差 | 测试数据集 | # 测试样本 |'
- en: '| Math Problem Solving | GSM-8K | 7473 | 3 | i.i.d. | 2491 | 2491 | 0 | GSM-8K
    | 1319 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 数学问题解决 | GSM-8K | 7473 | 3 | i.i.d. | 2491 | 2491 | 0 | GSM-8K | 1319 |'
- en: '| Code Generation | Rosetta | 7954 | 9 | Prog. Lang. | 1172 | 439 | 236.94
    | HumanEvalX | 656 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | Rosetta | 7954 | 9 | Prog. Lang. | 1172 | 439 | 236.94 | HumanEvalX
    | 656 |'
- en: '| Question Answering | Dolly | 15015 | 8 | Category | 3611 | 711 | 795.06 |
    Helm | NA |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | Dolly | 15015 | 8 | 类别 | 3611 | 711 | 795.06 | Helm | NA |'
- en: '| Public Dataset | Alpaca | 52002 | —————————————————————————————— |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 公共数据集 | Alpaca | 52002 | —————————————————————————————— |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: Datasets and Tasks.
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和任务。
- en: 'In the experiments, we use the benchmark datasets and tasks in (Kuang et al.,
    [2024](#bib.bib14)) to train and evaluate the LLM on three different NLP tasks,
    covering math problem-solving, code generation, and question-answering:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用 (Kuang 等，[2024](#bib.bib14)) 中的基准数据集和任务来训练和评估 LLM 在三种不同的 NLP 任务上的表现，包括数学问题解决、代码生成和问答：
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For math problem-solving, we split the GSM-8K training dataset (Cobbe et al.,
    [2021](#bib.bib6)) ensuring i.i.d. across three clients, and we assess the updated
    model using the GSM-8K test dataset.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于数学问题解决，我们将 GSM-8K 训练数据集（Cobbe 等，[2021](#bib.bib6)）分割为三个客户端，确保每个客户端的数据是独立同分布的，并使用
    GSM-8K 测试数据集评估更新后的模型。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For code generation, we fine-tune the model with the Rosetta dataset (Chaudhary,
    [2023](#bib.bib4)), which is partitioned across the programming languages, and
    a total of nine clients separately hold the data from nine different programming
    languages. Regarding its evaluation, we utilize HumanEvalX (Zheng et al., [2023](#bib.bib51)),
    an extension of a coding evaluation dataset (Chen et al., [2021](#bib.bib5)) that
    requires the model to fill in the code for a given problem in the required programming
    language (i.e., C++, GO, Java, Python).
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于代码生成，我们使用 Rosetta 数据集（Chaudhary，[2023](#bib.bib4)）对模型进行微调，该数据集按编程语言进行分割，共有九个客户端分别持有来自九种不同编程语言的数据。关于其评估，我们利用
    HumanEvalX（Zheng 等，[2023](#bib.bib51)），这是一个编码评估数据集（Chen 等，[2021](#bib.bib5)）的扩展，要求模型在给定问题的编程语言中填写代码（即
    C++、GO、Java、Python）。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For question answering, the model is trained on dolly-15K (Conover et al., [2023](#bib.bib7)),
    which is partitioned into 8 clients based on the categories of the questions,
    and we evaluate the new model with the selected tasks on HELM (Liang et al., [2022](#bib.bib18)).
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于问答任务，模型在 dolly-15K（Conover 等，[2023](#bib.bib7)）上进行训练，该数据集根据问题类别分为8个客户端，我们使用选定任务在
    HELM（Liang 等，[2022](#bib.bib18)）上评估新模型。
- en: 'Table [1](#S4.T1 "Table 1 ‣ Model and computation environment. ‣ 4.1\. Experimental
    Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") gives a detailed description of these three tasks. As Section
    [3](#S3 "3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model") mentions, the server will perform the emulator alignment during the
    model aggregation. Then, we use the Alpaca dataset (Taori et al., [2023](#bib.bib32))
    as the public dataset for the server to do the emulator alignment for all three
    NLP tasks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S4.T1 "表1 ‣ 模型和计算环境 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ FedBiOT: LLM本地微调中的联邦学习，无需完整模型")
    详细描述了这三项任务。如第 [3](#S3 "3\. FedBiOT ‣ FedBiOT: LLM本地微调中的联邦学习，无需完整模型") 节所述，服务器将在模型聚合过程中执行模拟器对齐。然后，我们使用
    Alpaca 数据集（Taori 等，[2023](#bib.bib32)）作为服务器进行所有三项 NLP 任务的模拟器对齐的公共数据集。'
- en: Implementation.
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施。
- en: This work is built upon an open-source federated learning platform named FederatedScope
    (Xie et al., [2023](#bib.bib45)). The training data are reformatted following
    the predesigned instructions (Chaudhary, [2023](#bib.bib4); Zhang et al., [2024](#bib.bib49)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究基于一个名为 FederatedScope 的开源联邦学习平台（Xie 等人，[2023](#bib.bib45)）。训练数据按照预先设计的指令进行了重新格式化（Chaudhary，[2023](#bib.bib4);
    Zhang 等人，[2024](#bib.bib49)）。
- en: 'Different from (Xiao et al., [2023](#bib.bib44); Kuang et al., [2024](#bib.bib14)),
    we regard the last two and the last four decoders as the adapter. The experiments
    consider two dropout rates, i.e., $\beta\in\{0.2,0.5\}$ towards minimizing the
    loss of Equation ([7](#S3.E7 "In 3.2\. Formulation of Bi-level Optimization ‣
    3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full
    Model")). During the FL training, the server takes 10 iterations to align the
    emulator $\mathcal{E}$ between two successive communication rounds after aggregating
    local adapters with FedAvg (Li et al., [2019](#bib.bib16)). These experiments
    run for 500 communication rounds, and we report the results based on the fine-tuned
    LLM obtained at the 500th round. During the training, we only fine-tune the adapter
    in the clients’ local update procedures, and we update the emulator on the server
    side. In other words, other parts of the pre-trained model, such as word embeddings,
    are frozen during the training.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '与 (Xiao 等人，[2023](#bib.bib44); Kuang 等人，[2024](#bib.bib14)) 不同，我们将最后两个和最后四个解码器视为适配器。实验考虑了两种
    dropout 率，即 $\beta\in\{0.2,0.5\}$，以最小化方程 ([7](#S3.E7 "In 3.2\. Formulation of
    Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model")) 的损失。在 FL 训练期间，服务器在每两次通信轮次之间进行 10 次迭代，以对齐仿真器 $\mathcal{E}$，这在用
    FedAvg（Li 等人，[2019](#bib.bib16)）聚合本地适配器之后进行。这些实验运行了 500 次通信轮次，我们根据第 500 轮获得的微调
    LLM 报告结果。在训练过程中，我们只在客户端的本地更新程序中微调适配器，并在服务器端更新仿真器。换句话说，预训练模型的其他部分，例如词嵌入，在训练过程中是被冻结的。'
- en: LoRA, Optimizers and Hyperparameters.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA、优化器和超参数。
- en: 'We add the LoRA to all decoder layers in the adapter and the emulator by setting
    the rank to 8 and the alpha to 16\. We use AdamW as an optimizer to solve Equation
    ([6](#S3.E6 "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")) and ([7](#S3.E7
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model")) on the clients (for the
    adapters) and the server (for the emulators), respectively. We search for the
    best learning rate in $\{1\times 10^{-5},3\times 10^{-5},5\times 10^{-5},8\times
    10^{-5},1\times 10^{-4}\}$. As for other hyperparameters related to the optimizer,
    we use the default setting. Furthermore, we also conduct grid search for FedBiOT-specific
    hyperparameters, i.e., $\epsilon$. Throughout the experiments, we demonstrate
    the result of the best hyperparameter combination. To avoid randomness, we utilize
    three different random seeds and report the averaged results.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过将 LoRA 添加到适配器和仿真器的所有解码器层中，将秩设置为 8 和 alpha 设置为 16。我们使用 AdamW 作为优化器来解决方程
    ([6](#S3.E6 "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")) 和 ([7](#S3.E7
    "In 3.2\. Formulation of Bi-level Optimization ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local
    Fine-tuning in Federated Learning without Full Model"))，分别在客户端（针对适配器）和服务器（针对仿真器）上进行。我们在
    $\{1\times 10^{-5},3\times 10^{-5},5\times 10^{-5},8\times 10^{-5},1\times 10^{-4}\}$
    中搜索最佳学习率。至于其他与优化器相关的超参数，我们使用默认设置。此外，我们还对 FedBiOT 特定的超参数，如 $\epsilon$，进行了网格搜索。在整个实验中，我们展示了最佳超参数组合的结果。为了避免随机性，我们使用了三种不同的随机种子并报告了平均结果。'
- en: Baselines.
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准。
- en: Offsite-tuning is the only method that satisfies the constraints that fine-tuning
    without access to full model. Xiao et al. ([2023](#bib.bib44)) introduces a single-client
    offsite-tuning, while Kuang et al. ([2024](#bib.bib14)) extends it to an FL version
    (i.e., FedOT). We apply offsite-tuning with one single client, where all data
    are loaded to the client. As FedOT supports FL, we reproduce the algorithm to
    work on the FL tasks. In terms of the setting of the adapters and the emulators,
    both Offsite-tuning and FedOT treat the first two and the last two decoders as
    the adapter. To enable the parameter-efficient fine-tuning for both baselines,
    we add LoRA to both baselines, the same as the setting adopted by FedBiOT.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Offsite-tuning 是唯一满足无完全模型访问的微调约束的方法。Xiao 等人 ([2023](#bib.bib44)) 引入了单客户端离线微调，而
    Kuang 等人 ([2024](#bib.bib14)) 将其扩展到 FL 版本（即 FedOT）。我们应用单客户端离线微调，其中所有数据都加载到客户端。由于
    FedOT 支持 FL，我们复现了该算法以适用于 FL 任务。在适配器和模拟器的设置方面，Offsite-tuning 和 FedOT 都将前两个和最后两个解码器视为适配器。为了使这两种基线方法能够高效地进行微调，我们为两者都添加了
    LoRA，这与 FedBiOT 采用的设置相同。
- en: Evaluation Metric.
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标。
- en: 'In the experiments, we report the results on two models, i.e., AdapEmu and
    AdapFu, as defined in Section [3.1](#S3.SS1 "3.1\. Compressed Model Preparation:
    Linear Dropout ‣ 3\. FedBiOT ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model"). The evaluation metrics for each task follow  Kuang et al.
    ([2024](#bib.bib14)), and the detailed description is given in Appendix [A](#A1
    "Appendix A Testing Dataset and Evaluation ‣ FedBiOT: LLM Local Fine-tuning in
    Federated Learning without Full Model").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在实验中，我们报告了两种模型的结果，即 AdapEmu 和 AdapFu，如第 [3.1](#S3.SS1 "3.1\. 压缩模型准备：线性丢弃 ‣
    3\. FedBiOT ‣ FedBiOT: 联邦学习中的 LLM 本地微调") 节所定义。每项任务的评估指标遵循 Kuang 等人 ([2024](#bib.bib14))
    的方法，详细描述见附录 [A](#A1 "附录 A 测试数据集和评估 ‣ FedBiOT: 联邦学习中的 LLM 本地微调")。'
- en: Table 2\. Test accuracy on math problem-solving task under different dropout
    rates
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 不同丢弃率下数学问题解决任务的测试准确率
- en: '|   Dropout Rate ($\beta$) | Methods | AdapEmu | AdapFu |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|   丢弃率 ($\beta$) | 方法 | AdapEmu | AdapFu |'
- en: '| $\beta=0.0$ | Few-shot CoT | NA | 13.42% (177/1319) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.0$ | Few-shot CoT | NA | 13.42% (177/1319) |'
- en: '| \hdashline$\beta=0.2$ | Offsite-tuning | 3.03% (40/1319) | 9.93% (131/1319)
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline$\beta=0.2$ | Offsite-tuning | 3.03% (40/1319) | 9.93% (131/1319)
    |'
- en: '| FedOT | 2.43% (32/1319) | 10.16% (134/1319) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| FedOT | 2.43% (32/1319) | 10.16% (134/1319) |'
- en: '| FedBiOT (Adapter 2) | 3.71% (49/1319) | 15.16% (200/1319) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 2) | 3.71% (49/1319) | 15.16% (200/1319) |'
- en: '| FedBiOT (Adapter 4) | 3.41% (45/1319) | 15.23% (201/1319) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 4) | 3.41% (45/1319) | 15.23% (201/1319) |'
- en: '| $\beta=0.5$ | Offsite-tuning | 2.27% (30/1319) | 7.58% (100/1319) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.5$ | Offsite-tuning | 2.27% (30/1319) | 7.58% (100/1319) |'
- en: '| FedOT | 1.90% (25/1319) | 7.51% (99/1319) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| FedOT | 1.90% (25/1319) | 7.51% (99/1319) |'
- en: '| FedBiOT (Adapter 2) | 2.05% (27/1319) | 11.83% (156/1319) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 2) | 2.05% (27/1319) | 11.83% (156/1319) |'
- en: '| FedBiOT (Adapter 4) | 1.82% (24/1319) | 14.03% (185/1319) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 4) | 1.82% (24/1319) | 14.03% (185/1319) |'
- en: '|   |  |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: Table 3\. Pass@1 (%) and Pass@10 (%) in code generation task at various rounds
    when dropout rate is 0.2
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 丢弃率为 0.2 时在代码生成任务中不同轮次的 Pass@1 (%) 和 Pass@10 (%)
- en: '|   Method | Model | C++ |  | GO |  | Java |  | Python |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 模型 | C++ |  | GO |  | Java |  | Python |'
- en: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
- en: '| Offsite-tuning | AdapEmu | 3.99 | 6.45 |  | 1.80 | 2.44 |  | 5.64 | 6.09
    |  | 5.01 | 6.38 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Offsite-tuning | AdapEmu | 3.99 | 6.45 |  | 1.80 | 2.44 |  | 5.64 | 6.09
    |  | 5.01 | 6.38 |'
- en: '| AdapFu | 8.78 | 10.82 |  | 4.94 | 6.63 |  | 9.57 | 12.81 |  | 13.19 | 17.32
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| AdapFu | 8.78 | 10.82 |  | 4.94 | 6.63 |  | 9.57 | 12.81 |  | 13.19 | 17.32
    |'
- en: '| FedOT | AdapEmu | 2.50 | 4.89 |  | 1.86 | 3.05 |  | 5.00 | 5.49 |  | 4.91
    | 6.83 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| FedOT | AdapEmu | 2.50 | 4.89 |  | 1.86 | 3.05 |  | 5.00 | 5.49 |  | 4.91
    | 6.83 |'
- en: '| AdapFu | 8.60 | 11.36 |  | 5.95 | 7.11 |  | 6.30 | 9.42 |  | 12.23 | 13.58
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| AdapFu | 8.60 | 11.36 |  | 5.95 | 7.11 |  | 6.30 | 9.42 |  | 12.23 | 13.58
    |'
- en: '| FedBiOT (Adapter 2) | AdapEmu | 4.82 | 6.43 |  | 3.57 | 4.85 |  | 5.92 |
    6.36 |  | 4.97 | 6.95 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 2) | AdapEmu | 4.82 | 6.43 |  | 3.57 | 4.85 |  | 5.92 |
    6.36 |  | 4.97 | 6.95 |'
- en: '| AdapFu | 9.76 | 14.18 |  | 9.97 | 13.29 |  | 12.93 | 16.28 |  | 14.91 | 19.77
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AdapFu | 9.76 | 14.18 |  | 9.97 | 13.29 |  | 12.93 | 16.28 |  | 14.91 | 19.77
    |'
- en: '| FedBiOT (Adapter 4) | AdapEmu | 3.20 | 4.57 |  | 2.20 | 2.44 |  | 4.91 |
    5.73 |  | 5.43 | 6.10 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (Adapter 4) | AdapEmu | 3.20 | 4.57 |  | 2.20 | 2.44 |  | 4.91 |
    5.73 |  | 5.43 | 6.10 |'
- en: '| AdapFu | 9.12 | 13.41 |  | 8.02 | 11.08 |  | 11.28 | 13.10 |  | 14.57 | 18.41
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| AdapFu | 9.12 | 13.41 |  | 8.02 | 11.08 |  | 11.28 | 13.10 |  | 14.57 | 18.41
    |'
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: Table 4\. Pass@1 (%) and Pass@10 (%) in code generation task at various rounds
    when dropout rate is 0.5\. We do not show AdapEmu’s performance because it struggles
    to generate meaningful codes, accounting for its small size.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 在不同轮次下，丢弃率为0.5时的代码生成任务中的 Pass@1 (%) 和 Pass@10 (%)。由于 AdapEmu 在生成有意义的代码方面表现不佳，因此我们没有展示它的性能，这与其小规模有关。
- en: '|   Method | Model | C++ |  | GO |  | Java |  | Python |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 模型 | C++ |  | GO |  | Java |  | Python |'
- en: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1 | Pass@10 |  | Pass@1
    | Pass@10 |'
- en: '| Offsite-tuning | AdapFu | 5.30 | 7.26 |  | 3.32 | 7.55 |  | 4.61 | 5.33 |  |
    8.75 | 10.26 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Offsite-tuning | AdapFu | 5.30 | 7.26 |  | 3.32 | 7.55 |  | 4.61 | 5.33 |  |
    8.75 | 10.26 |'
- en: '| FedOT | AdapFu | 4.92 | 7.33 |  | 5.00 | 8.33 |  | 3.86 | 4.37 |  | 7.33
    | 8.91 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| FedOT | AdapFu | 4.92 | 7.33 |  | 5.00 | 8.33 |  | 3.86 | 4.37 |  | 7.33
    | 8.91 |'
- en: '| FedBiOT (Adapter 2) | AdapFu | 7.71 | 11.84 |  | 7.68 | 10.01 |  | 9.51 |
    14.34 |  | 13.29 | 16.87 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (适配器 2) | AdapFu | 7.71 | 11.84 |  | 7.68 | 10.01 |  | 9.51 | 14.34
    |  | 13.29 | 16.87 |'
- en: '| FedBiOT (Adapter 4) | AdapFu | 5.03 | 11.09 |  | 6.25 | 8.47 |  | 7.41 |
    13.32 |  | 13.54 | 16.74 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| FedBiOT (适配器 4) | AdapFu | 5.03 | 11.09 |  | 6.25 | 8.47 |  | 7.41 | 13.32
    |  | 13.54 | 16.74 |'
- en: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: 4.2\. Quantitative Evaluation on i.i.d. Data
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 在 i.i.d. 数据上的定量评估
- en: 'We demonstrate the experimental results of GSM-8K provided in Table [2](#S4.T2
    "Table 2 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model") and highlight
    the worth-noted phenomenon when the data are i.i.d. across the clients.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了表[2](#S4.T2 "表 2 ‣ 评估指标 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ FedBiOT：在联邦学习中进行LLM本地微调")中提供的GSM-8K实验结果，并突出数据在客户端间为i.i.d.时的值得注意现象。
- en: A notable phenomenon observed in the table is that AdapEmu significantly falls
    behind AdapFu, particularly at a low dropout rate (i.e., $\beta=0.2$). To explain
    this, we examine the accuracy of the LLaMA-2 model with a dropout rate of 0.2,
    which is 2.12% without fine-tuning and increases to 2.43% after fine-tuning the
    emulator with a public dataset. The performance gap between AdapEmu and AdapFu
    can be attributed to layer dropout, which reduces the size of the LLM and subsequently
    impacts its performance. Additionally, this result highlights the difficulty of
    accurately reproducing the non-compressed parts with the emulator. Fortunately,
    all methods improve AdapEmu’s performance compared to the version without fine-tuning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表中观察到的一个显著现象是，AdapEmu在低丢弃率（即，$\beta=0.2$）下显著落后于AdapFu。为了说明这一点，我们检查了LLaMA-2模型在0.2丢弃率下的准确性，未经过微调时为2.12%，经过使用公共数据集微调后增加到2.43%。AdapEmu与AdapFu之间的性能差距可以归因于层丢弃，它减少了LLM的规模，从而影响其性能。此外，这一结果突显了准确重现非压缩部分的难度。幸运的是，所有方法相较于未经微调的版本，都提升了AdapEmu的性能。
- en: When we take a look at the proposed FedBiOT at different adapters’ sizes, we
    notice that FedBiOT with adapter 4 achieves better performance than that with
    adapter 2 under the AdapFu setting. As we know, a larger adapter has more trainable
    parameters, and therefore, it can easily absorb the knowledge from the downstream
    tasks. Note that the performances of these two adapter settings have subtle differences
    under AdapEmu, meaning that their emulator achieves very similar effects to the
    non-compressed emulator. When we plug the adaptor back into the non-compressed
    emulator, the adapter with more trainable parameters obviously can achieve a better
    performance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察不同适配器大小下的FedBiOT时，我们注意到，在AdapFu设置下，适配器 4 的FedBiOT比适配器 2 的表现更好。众所周知，较大的适配器具有更多的可训练参数，因此能够更容易地吸收来自下游任务的知识。需要注意的是，在AdapEmu下，这两种适配器设置的性能差异较小，这意味着它们的仿真器效果与非压缩仿真器非常相似。当我们将适配器重新插入到非压缩仿真器中时，具有更多可训练参数的适配器显然可以实现更好的性能。
- en: 'When comparing our proposed model with the baselines, we can notice a significant
    dominance in performance, especially in the AdapFu setting. More specifically,
    when the dropout rate becomes larger, the performance of AdapFu with FedBiOT decreases
    more mildly in contrast to other baselines. This is thanks to two factors: 1)
    the regularization term ensures the adapters will not change dramatically; 2)
    the on-the-fly distillation of the emulator with mixed losses can work better
    with clients’ data. Although the other two baselines use a public dataset to achieve
    similar functionality, the deterioration may still occur due to the data domain
    shift and the significant information loss.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将提议的模型与基准模型进行比较时，我们可以注意到性能上的显著优势，尤其是在AdapFu设置下。更具体地说，当丢弃率增大时，AdapFu与FedBiOT的性能相较于其他基准模型下降得更温和。这得益于两个因素：1）正则化项确保适配器不会发生剧烈变化；2）混合损失的模拟器即时蒸馏可以更好地与客户端的数据配合。尽管其他两个基准使用了公共数据集以实现类似功能，但由于数据领域转移和显著的信息损失，性能下降仍可能发生。
- en: 4.3\. Quantitative Evaluation on non-i.i.d. Data
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 非独立同分布数据的定量评估
- en: 'According to Table [1](#S4.T1 "Table 1 ‣ Model and computation environment.
    ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model"), code generation and question answering
    are two tasks split in non-i.i.d. styles. In this section, we evaluate our proposed
    FedBiOT when it trains an LLM with a non-i.i.d. dataset. It is worth noting that
    the evaluation task could be either in-distribution or out-of-distribution to
    the training dataset.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表格[1](#S4.T1 "Table 1 ‣ Model and computation environment. ‣ 4.1\. Experimental
    Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model")，代码生成和问答是两项以非独立同分布风格分开的任务。在本节中，我们评估了在用非独立同分布数据集训练LLM时提出的FedBiOT。值得注意的是，评估任务可以是与训练数据集相同分布或不同分布的。'
- en: Code generation.
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成。
- en: 'Table [3](#S4.T3 "Table 3 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣
    4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model") and [4](#S4.T4 "Table 4 ‣ Evaluation Metric. ‣ 4.1\. Experimental
    Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") illustrate the best results in different programming languages
    based on different hyperparameter settings. Let us take a look at the results
    of the FedBiOT at different adapter sizes. Apparently, FedBiOT with two layers
    of adapter constantly outperforms FedBiOT with four under both AdapEmu and AdapFu.
    This conclusion is different from the one when an LLM is trained with an i.i.d.
    dataset. The discrepancy can be attributed to the clients’ objectives: under i.i.d.
    datasets, a larger adapter size benefits training by absorbing downstream linguistic
    patterns uniformly. Conversely, with non-i.i.d. datasets, clients are biased towards
    their local optima, where the emulator’s effect becomes crucial.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[3](#S4.T3 "Table 3 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\.
    Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full
    Model")和[4](#S4.T4 "Table 4 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣
    4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model")展示了不同超参数设置下不同编程语言的最佳结果。让我们看看不同适配器大小下FedBiOT的结果。显然，具有两层适配器的FedBiOT在AdapEmu和AdapFu下都始终优于具有四层适配器的FedBiOT。这一结论与使用独立同分布数据集训练LLM时的结论不同。这一差异可以归因于客户端的目标：在独立同分布数据集下，更大的适配器尺寸通过均匀吸收下游语言模式来促进训练。相反，在非独立同分布数据集中，客户端对其局部最优值有偏见，模拟器的效果变得至关重要。'
- en: 'When comparing our proposed algorithm with the baselines, we notice a distinct
    dominance in AdapFu across all programming languages. In particular, when the
    dropout rate is 0.5, we can achieve up to 6% improvement over other baselines
    in terms of Pass@1, and up to 10% improvement of Pass@10\. Notably, the most distinct
    dominance can be witnessed under the “column” of Java in Table [4](#S4.T4 "Table
    4 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model").'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '当我们将提议的算法与基准算法进行比较时，我们注意到在所有编程语言中AdapFu表现出明显的优势。特别是，当丢弃率为0.5时，我们在Pass@1方面相较于其他基准算法可以提高多达6%，在Pass@10方面可以提高多达10%。值得注意的是，在表格[4](#S4.T4
    "Table 4 ‣ Evaluation Metric. ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT:
    LLM Local Fine-tuning in Federated Learning without Full Model")中的Java“列”下可以看到最显著的优势。'
- en: '![Refer to caption](img/f8e90d32ab1b558e499567bf1fc4225c.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f8e90d32ab1b558e499567bf1fc4225c.png)'
- en: (a)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/e14a458cff45d9840f1817616abe2e26.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e14a458cff45d9840f1817616abe2e26.png)'
- en: (a) AdapEmu (Dropout rate 0.2)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (a) AdapEmu（丢弃率0.2）
- en: '![Refer to caption](img/138fb4f8266319a94df4c8b2caeb4db2.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/138fb4f8266319a94df4c8b2caeb4db2.png)'
- en: (b) AdapFu (Dropout rate 0.2)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AdapFu（Dropout 率 0.2）
- en: '![Refer to caption](img/4ea1d629551ab31e09ceebc1d4e01252.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ea1d629551ab31e09ceebc1d4e01252.png)'
- en: (c) AdapFu (Dropout rate 0.5)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (c) AdapFu（Dropout 率 0.5）
- en: 'Figure 2\. Test accuracy in eight types of question-answering tasks (Left to
    right: Natural Questions (closed-book), QuAC, MMLU, OpenbookQA, NarrativeQA, Natural
    Questions (open-book), HellaSwag, BoolQ) and the average accuracy under different
    baselines (bars from left to right: Offsite-tuning, FedOT, FedBiOT (Adapter 2),
    FedBiOT (Adapter 4)) and different dropout rates.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 八种问答任务中的测试准确率（从左到右：自然问题（闭卷）、QuAC、MMLU、OpenbookQA、NarrativeQA、自然问题（开卷）、HellaSwag、BoolQ）以及在不同基线下的平均准确率（从左到右的条形图：离线微调、FedOT、FedBiOT（适配器
    2）、FedBiOT（适配器 4））和不同的 dropout 率。
- en: Question Answering.
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问答。
- en: 'Figure [2](#S4.F2 "Figure 2 ‣ Code generation. ‣ 4.3\. Quantitative Evaluation
    on non-i.i.d. Data ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model") shows the evaluation results using the HELM benchmark
    while we train the LLM with Dolly-15K. Generally speaking, FedBiOT (Adapter 2)
    performs significantly better than Adapter 4 in some tasks in terms of AdapEmu.
    As both AdapEmu have the same number of layers, this result exhibits the importance
    of the emulator, i.e., the model with a larger emulator can achieve leading performance.
    To some extent, this result supports our previous conclusion that an emulator
    plays a more important role than an adapter in a non-i.i.d. task. As for AdapFu,
    the performance difference is trivial between the two adapter sizes.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "图 2 ‣ 代码生成。 ‣ 4.3\. 关于非独立同分布数据的定量评估 ‣ 4\. 实验 ‣ FedBiOT：联邦学习中的
    LLM 本地微调") 显示了使用 HELM 基准测试的评估结果，同时我们用 Dolly-15K 训练 LLM。总体而言，FedBiOT（适配器 2）在某些任务中比适配器
    4 在 AdapEmu 方面表现显著更好。由于两个 AdapEmu 具有相同的层数，这一结果显示了模拟器的重要性，即具有更大模拟器的模型能够获得领先的表现。在一定程度上，这一结果支持了我们之前的结论，即在非独立同分布任务中，模拟器的作用比适配器更重要。至于
    AdapFu，两个适配器尺寸之间的性能差异微乎其微。
- en: 'The proposed algorithm outperforms offsite-tuning and FedOT in most datasets,
    which is consistent with the findings in other training tasks. The dominance of
    AdapFu becomes more pronounced as the dropout rate increases from 0.2 to 0.5\.
    For instance, FedBiOT is approximately 10% better than the baselines at a 0.5
    dropout rate in Natural Questions (closed-book), compared to a 2% improvement
    at a 0.2 dropout rate. Notably, comparing Figure [2b](#S4.F2.sf2 "In Figure 2
    ‣ Code generation. ‣ 4.3\. Quantitative Evaluation on non-i.i.d. Data ‣ 4\. Experiments
    ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model") and
    [2c](#S4.F2.sf3 "In Figure 2 ‣ Code generation. ‣ 4.3\. Quantitative Evaluation
    on non-i.i.d. Data ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated
    Learning without Full Model"), we notice that FedBiOT is mildly affected by changes
    in the dropout rate, while the baselines suffer significant degradation as the
    dropout rate increases. This stability can be attributed to round-by-round emulator
    alignment, where the non-compressed part of the full model is set as an anchor,
    regardless of the dropout rate. Consequently, this approach stabilizes the adapter
    training process, ensuring that adapters of the same size achieve similar performance
    across varying dropout rates.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的算法在大多数数据集上优于离线微调和 FedOT，这与其他训练任务中的发现一致。随着 dropout 率从 0.2 增加到 0.5，AdapFu 的优势变得更加明显。例如，FedBiOT
    在自然问题（闭卷）的 0.5 dropout 率下比基线好大约 10%，而在 0.2 dropout 率下仅提高了 2%。值得注意的是，通过比较图 [2b](#S4.F2.sf2
    "在图 2 中 ‣ 代码生成。 ‣ 4.3\. 关于非独立同分布数据的定量评估 ‣ 4\. 实验 ‣ FedBiOT：联邦学习中的 LLM 本地微调") 和
    [2c](#S4.F2.sf3 "在图 2 中 ‣ 代码生成。 ‣ 4.3\. 关于非独立同分布数据的定量评估 ‣ 4\. 实验 ‣ FedBiOT：联邦学习中的
    LLM 本地微调")，我们注意到 FedBiOT 对 dropout 率变化的影响较小，而基线在 dropout 率增加时遭遇显著退化。这种稳定性可以归因于轮次对轮次的模拟器对齐，其中模型的非压缩部分被设置为锚点，无论
    dropout 率如何。因此，这种方法稳定了适配器的训练过程，确保相同尺寸的适配器在不同 dropout 率下实现类似的性能。
- en: 4.4\. Discussion on Computation and Communication Overhead
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 计算和通信开销的讨论
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation and Communication
    Overhead ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning
    without Full Model") presents the computation and communication overhead of different
    methods under different dropout rates. As mentioned in the experimental setting,
    all algorithms have been applied with LoRA, and therefore, the number of trainable
    parameters dramatically reduces. From the clients’ perspectives, the number of
    trainable parameters is determined by the number of decoder layers in the adapter.
    Apparently, FedBiOT (Adapter 2) should be with the minimum number of trainable
    parameters among other methods.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation and Communication Overhead
    ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model")展示了不同方法在不同丢弃率下的计算和通信开销。如实验设置中所述，所有算法都应用了LoRA，因此可训练参数的数量显著减少。从客户端的角度来看，可训练参数的数量由适配器中的解码器层数决定。显然，FedBiOT（适配器
    2）应具有最少的可训练参数数量。'
- en: 'The computation costs in Table [5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation
    and Communication Overhead ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model") are measured by per-token floating
    point operation (FLOP/token). As we can see, the proposed FedBiOT costs less overhead
    than offsite-tuning and FedOT. The difference arises on account of the position
    of the trainable parameters. The adapter of the proposed FedBiOT is near the output
    layer. As for offsite-tuning and FedOT, the adapters are located separately at
    the top and the bottom two layers, thereby consuming more computation costs in
    the backward propagation for transmitting the derivative from the bottom to the
    top.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S4.T5 "Table 5 ‣ 4.4\. Discussion on Computation and Communication Overhead
    ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning in Federated Learning without
    Full Model")中的计算成本是通过每个标记的浮点运算（FLOP/token）来衡量的。如我们所见，提出的FedBiOT的开销低于离线调优和FedOT。这种差异是由于可训练参数的位置造成的。提出的FedBiOT的适配器靠近输出层。而离线调优和FedOT中的适配器分别位于顶部和底部两个层，这样在反向传播中传递导数时会消耗更多的计算成本。'
- en: However, our proposed method may require more communication overhead than the
    baselines. This is because the server should transmit the LoRA parameters of both
    the adapter and the emulator to the clients in our proposed method, while in offsite-tuning
    and FedOT, the server merely transmits the aggregated LoRA of the adapter to the
    clients. However, the overall cost is trivial, compared to the full LLM transmission
    at a cost of 28GB.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们提出的方法可能需要比基准方法更多的通信开销。这是因为在我们提出的方法中，服务器需要将适配器和模拟器的LoRA参数传输给客户端，而在离线调优和FedOT中，服务器仅将适配器的聚合LoRA传输给客户端。然而，与28GB的完整LLM传输成本相比，总体成本是微不足道的。
- en: Table 5\. Computation and communication costs of different methods under different
    dropout rates at client side.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. 不同方法在客户端不同丢弃率下的计算和通信成本。
- en: '|   Dropout Rate ($\beta$) | Methods | #Layers in Adapter | #Layers in Emulator
    | Trainable Param. (M) | Comp. Costs (GFLOP/token) | Comm. Costs (MB/round) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|   丢弃率 ($\beta$) | 方法 | 适配器中的层数 | 模拟器中的层数 | 可训练参数（M） | 计算成本（GFLOP/token） |
    通信成本（MB/轮次） |'
- en: '| $\beta=0.2$ | Offsite-tuning /FedOT | 4 | 22 | 0.524 | 10.33 | 4.19 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.2$ | 离线调优 / FedOT | 4 | 22 | 0.524 | 10.33 | 4.19 |'
- en: '| \cdashline2-7 | FedBiOT (Adapter 2) | 2 | 24 | 0.262 | 5.47 | 14.68 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-7 | FedBiOT (适配器 2) | 2 | 24 | 0.262 | 5.47 | 14.68 |'
- en: '| \cdashline2-7 | FedBiOT (Adapter 4) | 4 | 22 | 0.524 | 5.87 | 15.73 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-7 | FedBiOT (适配器 4) | 4 | 22 | 0.524 | 5.87 | 15.73 |'
- en: '| $\beta=0.5$ | Offsite-tuning /FedOT | 4 | 14 | 0.524 | 7.09 | 4.19 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.5$ | 离线调优 / FedOT | 4 | 14 | 0.524 | 7.09 | 4.19 |'
- en: '| \cdashline2-7 | FedBiOT (Adapter 2) | 2 | 15 | 0.262 | 3.65 | 9.96 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-7 | FedBiOT (适配器 2) | 2 | 15 | 0.262 | 3.65 | 9.96 |'
- en: '| \cdashline2-7 | FedBiOT (Adapter 4) | 4 | 14 | 0.524 | 4.25 | 11.53 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-7 | FedBiOT (适配器 4) | 4 | 14 | 0.524 | 4.25 | 11.53 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 5\. Conclusion
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we introduce FedBiOT, a federated learning algorithm that avoids
    full model fine-tuning while substantially reducing computation overhead. Specifically,
    we compress the LLM and divide it into two components, namely, an emulator and
    an adapter. By formulating a bi-level optimization problem, our proposed FedBiOT
    ensures that the emulator partially simulates the original LLM, while the adapter
    focuses on learning domain-specific linguistic patterns. Extensive experiments
    show the superiority of the proposed FedBiOT working with LLaMA-2, where it can
    achieve significant accuracy improvement than the existing baselines (i.e., Offsite-tuning
    and FedOT) in all tasks (i.e., math problem-solving, code generation, and question
    answering).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 FedBiOT，一种避免全模型微调同时大幅减少计算开销的联邦学习算法。具体而言，我们压缩了 LLM，并将其分为两个组件，即模拟器和适配器。通过制定双层优化问题，我们提出的
    FedBiOT 确保模拟器部分模拟原始 LLM，而适配器专注于学习特定领域的语言模式。大量实验显示，所提出的 FedBiOT 在使用 LLaMA-2 时具有显著的准确性提升，相较于现有基准（即
    Offsite-tuning 和 FedOT），在所有任务（即数学问题解决、代码生成和问答）中均表现优越。
- en: Acknowledgements.
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: The authors would like to thank the anonymous reviewers for their constructive
    comments. This work is supported in part by the US National Science Foundation
    under grants NSF-IIS 1747614 and NSF-IIS 2141037\. Any opinions, findings, and
    conclusions or recommendations expressed in this material are those of the author(s)
    and do not necessarily reflect the views of the National Science Foundation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢匿名评审员的建设性意见。本研究部分得到美国国家科学基金会资助，资助编号为 NSF-IIS 1747614 和 NSF-IIS 2141037。本文中的任何观点、发现、结论或建议均为作者本人观点，不一定反映国家科学基金会的观点。
- en: References
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: Acar et al. (2020) Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina,
    Paul Whatmough, and Venkatesh Saligrama. 2020. Federated Learning Based on Dynamic
    Regularization. In *Proc. of International Conference on Learning Representations
    (ICLR’20)*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acar 等人（2020）**Durmus Alp Emre Acar**, **Yue Zhao**, **Ramon Matas**, **Matthew
    Mattina**, **Paul Whatmough**, 和 **Venkatesh Saligrama**。2020。基于动态正则化的联邦学习。在*国际学习表征会议（ICLR’20）论文集*中。
- en: CCPA (2023) CCPA. 2023. *California Consumer Privacy Act (CCPA)*. [https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CCPA（2023）**CCPA**。2023。*加利福尼亚消费者隐私法案（CCPA）*。[https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following
    LLaMA model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaudhary（2023）**Sahil Chaudhary**。2023。Code Alpaca: 一种用于代码生成的指令跟随 LLaMA 模型。[https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）**Mark Chen**, **Jerry Tworek**, **Heewoo Jun**, **Qiming Yuan**,
    **Henrique Ponde de Oliveira Pinto**, **Jared Kaplan**, **Harri Edwards**, **Yuri
    Burda**, **Nicholas Joseph**, **Greg Brockman** 等人。2021。评估针对代码训练的大型语言模型。*arXiv
    预印本 arXiv:2107.03374*（2021年）。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168* (2021).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）**Karl Cobbe**, **Vineet Kosaraju**, **Mohammad Bavarian**, **Mark
    Chen**, **Heewoo Jun**, **Lukasz Kaiser**, **Matthias Plappert**, **Jerry Tworek**,
    **Jacob Hilton**, **Reiichiro Nakano** 等人。2021。训练验证者解决数学文字题。*arXiv 预印本 arXiv:2110.14168*（2021年）。
- en: 'Conover et al. (2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    2023. *Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned
    LLM*. [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conover 等人（2023）**Mike Conover**, **Matt Hayes**, **Ankit Mathur**, **Jianwei
    Xie**, **Jun Wan**, **Sam Shah**, **Ali Ghodsi**, **Patrick Wendell**, **Matei
    Zaharia**, 和 **Reynold Xin**。2023。*Free Dolly: 介绍全球首个真正开放的指令调优 LLM*。[https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
- en: 'Cui et al. (2023) Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan.
    2023. Chatlaw: Open-source legal large language model with integrated external
    knowledge bases. *arXiv preprint arXiv:2306.16092* (2023).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui 等人（2023）**Jiaxi Cui**, **Zongjian Li**, **Yang Yan**, **Bohua Chen**, 和
    **Li Yuan**。2023。Chatlaw: 开源法律大型语言模型与集成外部知识库。*arXiv 预印本 arXiv:2306.16092*（2023年）。'
- en: GDPR (2016) GDPR. 2016. *Regulation (EU) 2016/679 of the European Parliament
    and of the Council*. [https://data.europa.eu/eli/reg/2016/679/oj](https://data.europa.eu/eli/reg/2016/679/oj)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GDPR (2016) GDPR。2016。*欧洲议会和理事会第 2016/679 号条例*。 [https://data.europa.eu/eli/reg/2016/679/oj](https://data.europa.eu/eli/reg/2016/679/oj)
- en: 'He et al. (2023) Shiqi He, Qifan Yan, Feijie Wu, Lanjun Wang, Mathias Lécuyer,
    and Ivan Beschastnikh. 2023. GlueFL: Reconciling Client Sampling and Model Masking
    for Bandwidth Efficient Federated Learning. *Proc. of Machine Learning and Systems
    (MLSys’23)*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等人 (2023) Shiqi He, Qifan Yan, Feijie Wu, Lanjun Wang, Mathias Lécuyer 和
    Ivan Beschastnikh。2023。GlueFL: 解决客户端采样和模型掩蔽以提高带宽效率的联邦学习。在 *机器学习与系统 (MLSys’23)
    会议录* 中。'
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人 (2015) Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean。2015。提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531* (2015)。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large
    Language Models. In *Proc. of International Conference on Learning Representations
    (ICLR’21)*.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
    Wang, Lu Wang, Weizhu Chen 等。2021。LoRA: 大型语言模型的低秩适配。在 *国际学习表征会议 (ICLR’21) 会议录*
    中。'
- en: 'Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
    Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic
    controlled averaging for federated learning. In *Proc. of International conference
    on machine learning (ICML’20)*. 5132–5143.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karimireddy 等人 (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
    Sashank Reddi, Sebastian Stich 和 Ananda Theertha Suresh。2020。Scaffold: 联邦学习的随机控制平均。在
    *国际机器学习会议 (ICML’20) 会议录* 中。5132–5143。'
- en: 'Kuang et al. (2024) Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei
    Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024.
    FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models
    in Federated Learning. In *Proc. of the ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining (KDD’24)*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuang 等人 (2024) Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei
    Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding 和 Jingren Zhou。2024。FederatedScope-LLM:
    用于联邦学习中大型语言模型微调的全面包。在 *ACM SIGKDD 知识发现与数据挖掘会议 (KDD’24) 会议录* 中。'
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    Power of Scale for Parameter-Efficient Prompt Tuning. In *Proc. of the Conference
    on Empirical Methods in Natural Language Processing (EMNLP’21)*. 3045–3059.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等人 (2021) Brian Lester, Rami Al-Rfou 和 Noah Constant。2021。参数高效提示调优的规模效应。在
    *自然语言处理经验方法会议 (EMNLP’21) 会议录* 中。3045–3059。
- en: Li et al. (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua
    Zhang. 2019. On the Convergence of FedAvg on Non-IID Data. In *Proc. of International
    Conference on Learning Representations (ICLR’19)*.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2019) Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang 和 Zhihua Zhang。2019。FedAvg
    在非 IID 数据上的收敛性。在 *国际学习表征会议 (ICLR’19) 会议录* 中。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing
    Continuous Prompts for Generation. In *Proc. of the Annual Meeting of the Association
    for Computational Linguistics and the International Joint Conference on Natural
    Language Processing (ACL/IJNLP’21)*. 4582–4597.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang (2021) Xiang Li 和 Percy Liang。2021。Prefix-Tuning: 为生成优化连续提示。在 *计算语言学协会年会和国际自然语言处理联合会议
    (ACL/IJNLP’21) 会议录* 中。4582–4597。'
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*
    (2022).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar
    等。2022。语言模型的全面评估。*arXiv 预印本 arXiv:2211.09110* (2022)。
- en: Lin et al. (2020) Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi.
    2020. Ensemble distillation for robust model fusion in federated learning. In
    *Proc. of Advances in Neural Information Processing Systems (NeurIPS’20)*. 2351–2363.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2020) Tao Lin, Lingjing Kong, Sebastian U Stich 和 Martin Jaggi。2020。在联邦学习中的稳健模型融合的集成蒸馏。在
    *神经信息处理系统进展 (NeurIPS’20) 会议录* 中。2351–2363。
- en: Lin et al. (2023) Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li
    Shen, and Dacheng Tao. 2023. Efficient federated prompt tuning for black-box large
    pre-trained models. *arXiv preprint arXiv:2310.03123* (2023).
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2023) Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen
    和 Dacheng Tao。2023。针对黑箱大型预训练模型的高效联邦提示调优。*arXiv 预印本 arXiv:2310.03123* (2023)。
- en: Liu et al. (2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. 2023. GPT understands, too. *AI Open* (2023).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin
    Yang, 和 Jie Tang. 2023. GPT 也能理解。*AI Open* (2023)。
- en: Loshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled
    Weight Decay Regularization. In *Proc. of International Conference on Learning
    Representations (ICLR’18)*.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter (2018) Ilya Loshchilov 和 Frank Hutter. 2018. 解耦权重衰减正则化。见
    *国际学习表征会议 (ICLR’18)*。
- en: McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks
    from decentralized data. In *Proc. of Artificial intelligence and statistics (AISTAT’17)*.
    1273–1282.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan 等人 (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    和 Blaise Aguera y Arcas. 2017. 从分散数据中高效通信学习深度网络。见 *人工智能与统计会议 (AISTAT’17)*. 1273–1282。
- en: 'Nay et al. (2024) John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao,
    Meghana Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H Choi, and Jungo Kasai.
    2024. Large language models as tax attorneys: a case study in legal capabilities
    emergence. *Philosophical Transactions of the Royal Society A* 382, 2270 (2024),
    20230159.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nay 等人 (2024) John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao, Meghana
    Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H Choi, 和 Jungo Kasai. 2024. 大型语言模型作为税务律师：法律能力出现的案例研究。*皇家学会A辑哲学论文*
    382, 2270 (2024), 20230159。
- en: 'OpenAI (2023) OpenAI. 2023. Fine-tuning - OpenAI API. [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
    Accessed: 2023-09-29.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. 微调 - OpenAI API. [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)。访问时间：2023-09-29。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’22)*.
    27730–27744.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人. 2022.
    训练语言模型以遵循人类反馈的指示。见 *神经信息处理系统进展会议 (NeurIPS’22)*. 27730–27744。
- en: Sajjad et al. (2023) Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav
    Nakov. 2023. On the effect of dropping layers of pre-trained transformer models.
    *Computer Speech & Language* 77 (2023), 101429.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajjad 等人 (2023) Hassan Sajjad, Fahim Dalvi, Nadir Durrani, 和 Preslav Nakov.
    2023. 预训练变换模型层数减少的影响。*计算机语音与语言* 77 (2023), 101429。
- en: Singhal et al. (2023) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,
    Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl, et al. 2023. Large language models encode clinical knowledge. *Nature*
    620, 7972 (2023), 172–180.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等人 (2023) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason
    Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl 等人. 2023. 大型语言模型编码临床知识。*自然* 620, 7972 (2023), 172–180。
- en: Sordoni et al. (2023) Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté,
    Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner,
    and Nicolas Le Roux. 2023. Joint prompt optimization of stacked llms using variational
    inference. In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’23)*.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sordoni 等人 (2023) Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus
    Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, 和 Nicolas
    Le Roux. 2023. 使用变分推断的堆叠 llms 的联合提示优化。见 *神经信息处理系统进展会议 (NeurIPS’23)*。
- en: 'Sun et al. (2023) Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu,
    Yiran Chen, and Holger R Roth. 2023. FedBPT: Efficient Federated Black-box Prompt
    Tuning for Large Language Models. *arXiv preprint arXiv:2310.01467* (2023).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 (2023) Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran
    Chen, 和 Holger R Roth. 2023. FedBPT: 高效的联邦黑箱提示调整用于大型语言模型。*arXiv 预印本 arXiv:2310.01467*
    (2023)。'
- en: Sun et al. (2024) Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024. Improving
    LoRA in Privacy-preserving Federated Learning. In *Proc. of The International
    Conference on Learning Representations (ICLR’24)*.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2024) Youbang Sun, Zitao Li, Yaliang Li, 和 Bolin Ding. 2024. 在隐私保护的联邦学习中改进
    LoRA。见 *国际学习表征会议 (ICLR’24)*。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    Alpaca: An Instruction-following LLaMA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    Alpaca: 一个遵循指令的LLaMA模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    Large language models in medicine. *Nature medicine* 29, 8 (2023), 1930–1940.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.
    医学中的大型语言模型。*自然医学* 29, 8 (2023), 1930–1940。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: 开放且高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*（2023）。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: 开放基础和微调的聊天模型。*arXiv预印本 arXiv:2307.09288*（2023）。'
- en: 'Wang et al. (2023a) Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng
    Zhan, and Zhigang Zeng. 2023a. Dafkd: Domain-aware federated knowledge distillation.
    In *Proc. of the IEEE/CVF conference on Computer Vision and Pattern Recognition
    (CVPR’23)*. 20412–20421.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng
    Zhan, and Zhigang Zeng. 2023a. Dafkd: 领域感知的联邦知识蒸馏。发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR’23）*。20412–20421。'
- en: 'Wang et al. (2023b) Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li,
    and Tianwei Zhang. 2023b. FedCDA: Federated Learning with Cross-rounds Divergence-aware
    Aggregation. In *Proc. of The International Conference on Learning Representations
    (ICLR’23)*.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023b) Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li,
    and Tianwei Zhang. 2023b. FedCDA: 带有跨轮次差异感知聚合的联邦学习。发表于*国际学习表征会议（ICLR’23）*。'
- en: 'Wang et al. (2022) Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang
    Gu, and Jing Gao. 2022. FedKC: Federated knowledge composition for multilingual
    natural language understanding. In *Proc. of the ACM Web Conference 2022 (WWW’22)*.
    1839–1850.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang
    Gu, and Jing Gao. 2022. FedKC: 用于多语言自然语言理解的联邦知识组合。发表于*ACM网络会议2022（WWW’22）*。1839–1850。'
- en: Wang et al. (2020) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent
    Poor. 2020. Tackling the objective inconsistency problem in heterogeneous federated
    optimization. In *Proc. of Advances in neural information processing systems (NeurIPS’20)*.
    7611–7623.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent
    Poor. 2020. 解决异质联邦优化中的目标不一致问题。发表于*神经信息处理系统进展（NeurIPS’20）*。7611–7623。
- en: 'Wang et al. (2023c) Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang
    Shen. 2023c. Chatcad: Interactive computer-aided diagnosis on medical image using
    large language models. *arXiv preprint arXiv:2302.07257* (2023).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023c) Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang
    Shen. 2023c. Chatcad: 使用大型语言模型的医学图像交互计算机辅助诊断。*arXiv预印本 arXiv:2302.07257*（2023）。'
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned Language
    Models are Zero-Shot Learners. In *Proc. of International Conference on Learning
    Representations (ICLR’21)*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. 微调语言模型是零样本学习者。发表于*国际学习表征会议（ICLR’21）*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. In *Proc. of Advances in Neural Information
    Processing Systems (NeurIPS’22)*. 24824–24837.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. 连锁思维提示在大型语言模型中引发推理。发表于*神经信息处理系统进展（NeurIPS’22）*。24824–24837。
- en: Wu et al. (2023) Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing
    Gao. 2023. Anchor sampling for federated learning with partial client participation.
    In *Proc. of International Conference on Machine Learning (ICML’23)*. 37379–37416.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2023）Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, 和 Jing Gao. 2023.
    部分客户端参与的联邦学习中的锚点采样。见*国际机器学习会议（ICML’23）*。37379–37416。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-tuning:
    Transfer learning without full model. *arXiv preprint arXiv:2302.04870* (2023).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等（2023）Guangxuan Xiao, Ji Lin, 和 Song Han. 2023. Offsite-tuning: 无需全模型的迁移学习。*arXiv预印本
    arXiv:2302.04870* (2023)。'
- en: 'Xie et al. (2023) Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope:
    A Flexible Federated Learning Platform for Heterogeneity. In *Proc. of the VLDB
    Endowment (VLDB’23)*. 1059–1072.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie等（2023）Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui
    Kuang, Yaliang Li, Bolin Ding, 和 Jingren Zhou. 2023. FederatedScope: 一个灵活的联邦学习平台以应对异质性。见*VLDB基金会会议（VLDB’23）*。1059–1072。'
- en: 'Yi et al. (2023) Liping Yi, Han Yu, Gang Wang, and Xiaoguang Liu. 2023. Fedlora:
    Model-heterogeneous personalized federated learning with lora tuning. *arXiv preprint
    arXiv:2310.13283* (2023).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi等（2023）Liping Yi, Han Yu, Gang Wang, 和 Xiaoguang Liu. 2023. Fedlora: 使用lora微调的模型异质个性化联邦学习。*arXiv预印本
    arXiv:2310.13283* (2023)。'
- en: Yosinski et al. (2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    2014. How transferable are features in deep neural networks?. In *Proc. of Advances
    in neural information processing systems (NeurIPS’14)*.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yosinski等（2014）Jason Yosinski, Jeff Clune, Yoshua Bengio, 和 Hod Lipson. 2014.
    深度神经网络中的特征可迁移性如何？见*神经信息处理系统进展会议（NeurIPS’14）*。
- en: Zhang et al. (2021) Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao
    Xu, and Feijie Wu. 2021. Parameterized knowledge transfer for personalized federated
    learning. In *Proc. of Advances in Neural Information Processing Systems (NeurIPS’21)*.
    10092–10104.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021）Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, 和 Feijie
    Wu. 2021. 针对个性化联邦学习的参数化知识迁移。见*神经信息处理系统进展会议（NeurIPS’21）*。10092–10104。
- en: 'Zhang et al. (2024) Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li,
    Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024. Towards building the
    federatedGPT: Federated instruction tuning. In *Proc. of IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP’24)*. 6915–6919.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2024）Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang,
    Tong Yu, Guoyin Wang, 和 Yiran Chen. 2024. 构建federatedGPT: 联邦指令微调。见*IEEE国际声学、语音和信号处理会议（ICASSP’24）*。6915–6919。'
- en: 'Zhang et al. (2023) Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu,
    Lizhen Qu, and Zenglin Xu. 2023. FedPETuning: When federated learning meets the
    parameter-efficient tuning methods of pre-trained language models. In *Proc. of
    Annual Meeting of the Association of Computational Linguistics (ACL’23)*. 9963–9977.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2023）Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen
    Qu, 和 Zenglin Xu. 2023. FedPETuning: 当联邦学习遇到预训练语言模型的参数高效微调方法。见*计算语言学协会年会（ACL’23）*。9963–9977。'
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A
    pre-trained model for code generation with multilingual benchmarking on humaneval-x.
    In *Proc. of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining
    (KDD’23)*. 5673–5684.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng等（2023）Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue,
    Lei Shen, Zihan Wang, Andi Wang, Yang Li等. 2023. Codegeex: 一种用于代码生成的预训练模型，具有基于humaneval-x的多语言基准测试。见*ACM
    SIGKDD知识发现与数据挖掘会议（KDD’23）*。5673–5684。'
- en: Appendix A Testing Dataset and Evaluation
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 测试数据集和评估
- en: 'As described in Table [1](#S4.T1 "Table 1 ‣ Model and computation environment.
    ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ FedBiOT: LLM Local Fine-tuning
    in Federated Learning without Full Model"), we utilize three datasets to assess
    the fine-tuning performance. In this section, we briefly introduce all these datasets
    and provide the details about how they evaluate a given LLM.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S4.T1 "表 1 ‣ 模型和计算环境。 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ FedBiOT: 无需全模型的联邦学习中的LLM本地微调")所述，我们使用三个数据集来评估微调性能。在这一部分，我们简要介绍所有这些数据集，并提供它们如何评估给定LLM的细节。'
- en: GSM-8K.
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GSM-8K。
- en: We use the GSM-8K test set (Cobbe et al., [2021](#bib.bib6)) to evaluate the
    ability of a large language model (LLM) to solve math problems. This dataset includes
    ”questions” and ”ground truth” answers. We assess correctness by determining how
    often the LLM answers a given question correctly. Following chain of thought (CoT)
    (Wei et al., [2022](#bib.bib42)), we prepare a set of sample questions (a.k.a.
    few-shot prompting) and prompt the LLM to generate step-by-step solutions, ensuring
    the answers are formatted correctly. Finally, we extract the answers from these
    solutions and compare them with the ground truth to calculate the correctness
    rate.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GSM-8K测试集（Cobbe等，[2021](#bib.bib6)）来评估大型语言模型（LLM）解决数学问题的能力。该数据集包括“问题”和“真实答案”。我们通过确定LLM回答给定问题的正确率来评估正确性。按照思维链（CoT）（Wei等，[2022](#bib.bib42)），我们准备一组样本问题（即少量示例提示），并提示LLM生成逐步解答，确保答案格式正确。最后，我们从这些解答中提取答案，并与真实答案进行比较，以计算正确率。
- en: HumanevalX.
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HumanevalX。
- en: This is a task for code autofill, which consists of 164 test samples for five
    programming languages (Zheng et al., [2023](#bib.bib51)). It is worth noting that
    we use four of them (i.e., C++, GO, Java, and Python) because there are no JavaScript
    codes in the training dataset. Each test sample is constituted with “task id”,
    “prompt” (i.e., Task description with partial codes), “entry point” (i.e., the
    function to be achieved), “canonical solution” (i.e., a sampled solution), and
    “test” (i.e., evaluate if the generated code can obtain the correct answer based
    on the given input). In this task, we use “prompt” as the input and generate five
    versions of codes using a given model. We compile the code and check if it can
    pass the given “test”. Let $c$ be the number of correct codes generated by LLM
    and passed unit tests, and therefore, Pass@k can be computed via
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于代码自动填充的任务，由164个测试样本组成，涵盖五种编程语言（Zheng等，[2023](#bib.bib51)）。值得注意的是，我们使用其中的四种语言（即C++、GO、Java和Python），因为训练数据集中没有JavaScript代码。每个测试样本包括“任务ID”、“提示”（即带有部分代码的任务描述）、“入口点”（即需实现的函数）、“标准解答”（即采样解答）和“测试”（即评估生成的代码是否能基于给定输入获得正确答案）。在此任务中，我们使用“提示”作为输入，并利用给定模型生成五个版本的代码。我们编译这些代码并检查它们是否能通过给定的“测试”。设$c$为LLM生成并通过单元测试的正确代码数量，因此，Pass@k可以通过以下公式计算：
- en: '|  | $\text{Pass@k}=\mathbb{E}_{\text{problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Pass@k}=\mathbb{E}_{\text{problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]$
    |  |'
- en: HELM.
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HELM。
- en: HELM (Liang et al., [2022](#bib.bib18)) is a benchmark that contains a wide
    range of NLP tasks. We upload the well-trained models to the benchmark and evaluate
    them on question-answering tasks, which includes eight datasets, i.e., MMLU, BoolQ,
    NarrativeQA, Natural Questions (closed-book), Natural Questions (open-book), QuAC,
    HellaSwag, OpenbookQA. For different tasks, the results come from different metrics,
    i.e., exact match for HellaSwag, OpenbookQA, and MMLU; quasi-exact match for BoolQ;
    F1 for the rest.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: HELM（Liang等，[2022](#bib.bib18)）是一个包含广泛NLP任务的基准。我们将训练良好的模型上传到该基准，并在问答任务上进行评估，这些任务包括八个数据集，即MMLU、BoolQ、NarrativeQA、自然问题（闭卷）、自然问题（开卷）、QuAC、HellaSwag、OpenbookQA。对于不同的任务，结果来自不同的指标，即HellaSwag、OpenbookQA和MMLU的准确匹配；BoolQ的准准确匹配；其余任务的F1值。
