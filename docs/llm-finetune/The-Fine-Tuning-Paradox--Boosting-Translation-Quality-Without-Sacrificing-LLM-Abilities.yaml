- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM
    Abilities'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**微调悖论**：提升翻译质量而不牺牲LLM能力'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20089](https://ar5iv.labs.arxiv.org/html/2405.20089)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20089](https://ar5iv.labs.arxiv.org/html/2405.20089)
- en: David Stap^(1,2)    Eva Hasler¹    Bill Byrne¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: David Stap^(1,2)    Eva Hasler¹    Bill Byrne¹
- en: Christof Monz² Ke Tran¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Christof Monz² Ke Tran¹
- en: ¹Amazon AGI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Amazon AGI
- en: ²Language Technology Lab, University of Amsterdam
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²阿姆斯特丹大学语言技术实验室
- en: '{d.stap, c.monz}@uva.nl, {ehasler, willbyrn, trnke}@amazon.com   Work done
    during an internship at Amazon.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{d.stap, c.monz}@uva.nl，{ehasler, willbyrn, trnke}@amazon.com   工作完成于Amazon的实习期间。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning large language models (LLMs) for machine translation has shown improvements
    in overall translation quality. However, it is unclear what is the impact of fine-tuning
    on desirable LLM behaviors that are not present in neural machine translation
    models, such as steerability, inherent document-level translation abilities, and
    the ability to produce less literal translations. We perform an extensive translation
    evaluation on the LLaMA and Falcon family of models with model size ranging from
    7 billion up to 65 billion parameters. Our results show that while fine-tuning
    improves the general translation quality of LLMs, several abilities degrade. In
    particular, we observe a decline in the ability to perform formality steering,
    to produce technical translations through few-shot examples, and to perform document-level
    translation. On the other hand, we observe that the model produces less literal
    translations after fine-tuning on parallel data. We show that by including monolingual
    data as part of the fine-tuning data we can maintain the abilities while simultaneously
    enhancing overall translation quality. Our findings emphasize the need for fine-tuning
    strategies that preserve the benefits of LLMs for machine translation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大型语言模型（LLMs）用于机器翻译已显示出整体翻译质量的提升。然而，微调对LLM行为的影响尚不清楚，这些行为在神经机器翻译模型中并不存在，例如可操控性、固有的文档级翻译能力，以及生成较少字面翻译的能力。我们对LLaMA和Falcon系列模型进行了广泛的翻译评估，这些模型的参数规模从70亿到650亿不等。我们的结果表明，尽管微调提高了LLM的整体翻译质量，但一些能力却有所退化。特别是，我们观察到在执行形式化引导、通过少量示例生成技术翻译以及进行文档级翻译的能力有所下降。另一方面，我们观察到在对平行数据进行微调后，模型生成的翻译较少字面化。我们展示了通过将单语数据作为微调数据的一部分，可以在提升整体翻译质量的同时保持这些能力。我们的发现强调了需要采用保留LLM在机器翻译中优势的微调策略。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Recent work has highlighted a range of qualitative advantages that large language
    models (LLMs) hold over Neural Machine Translation (NMT) models. One significant
    advantage is the controllability of style and language variety which can be achieved
    through prompting and in-context learning (Brown et al., [2020](#bib.bib10); Garcia
    et al., [2023](#bib.bib17); Agrawal et al., [2023](#bib.bib2)). LLMs also exhibit
    inherent document-level translation abilities (Wang et al., [2023](#bib.bib48);
    Karpinska and Iyyer, [2023](#bib.bib21)). Another advantage is their ability to
    produce less literal translations (Raunak et al., [2023](#bib.bib36)). Finally,
    LLMs have been shown to have better performance in handling difficult linguistic
    phenomena such as idioms and ambiguous expressions (Neubig, [2023](#bib.bib32)).
    Taken together, LLMs are surpassing NMT models in terms of versatility.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究强调了大型语言模型（LLMs）相对于神经机器翻译（NMT）模型的一系列定性优势。其中一个显著的优势是通过提示和上下文学习（Brown et al.,
    [2020](#bib.bib10); Garcia et al., [2023](#bib.bib17); Agrawal et al., [2023](#bib.bib2)）可以实现的风格和语言多样性的可控性。LLMs还展示了固有的文档级翻译能力（Wang
    et al., [2023](#bib.bib48); Karpinska and Iyyer, [2023](#bib.bib21)）。另一个优势是它们能够生成较少字面化的翻译（Raunak
    et al., [2023](#bib.bib36)）。最后，LLMs在处理难度较大的语言现象，如习语和模糊表达方面表现更佳（Neubig, [2023](#bib.bib32)）。综合来看，LLMs在多样性方面超越了NMT模型。
- en: Recent studies have demonstrated that fine-tuning LLMs on parallel data further
    improves their translations as measured by metrics that reflect overall quality
    (such as COMET) (Li et al., [2023](#bib.bib25); Yang et al., [2023](#bib.bib53);
    Zeng et al., [2023](#bib.bib55)). However, relying on general translation quality
    metrics and generic test sets does not fully capture the nuanced abilities of
    LLMs in machine translation. This oversight raises questions about the retention
    of LLM-specific advantages — such as controllability, document-level translation
    proficiency, and the production of less literal translations — after fine-tuning
    on parallel data. While it is clear that general machine translation quality improves
    through fine-tuning, there is a risk that LLMs lose their unique strengths due
    to catastrophic forgetting (McCloskey and Cohen, [1989](#bib.bib28); Ratcliff,
    [1990](#bib.bib35); Luo et al., [2023](#bib.bib26)). Determining the extent of
    this risk and comparing the effect of various fine-tuning strategies in preserving
    the qualitative benefits of LLMs remains an important yet unresolved question.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，在平行数据上对LLMs进行微调能够进一步提高其翻译质量，依据诸如COMET等反映整体质量的指标进行衡量（Li et al., [2023](#bib.bib25);
    Yang et al., [2023](#bib.bib53); Zeng et al., [2023](#bib.bib55)）。然而，依赖通用的翻译质量指标和通用测试集并不能完全捕捉LLMs在机器翻译中的细微能力。这种忽视引发了对LLM特有优势的保留问题——例如可控性、文档级翻译能力以及生成更少字面翻译的能力——在平行数据上进行微调后是否会受到影响。尽管通过微调可以提高整体机器翻译质量，但LLMs因灾难性遗忘（McCloskey
    and Cohen, [1989](#bib.bib28); Ratcliff, [1990](#bib.bib35); Luo et al., [2023](#bib.bib26)）而失去其独特优势的风险依然存在。确定这一风险的程度，并比较各种微调策略在保留LLMs定性优势方面的效果，仍然是一个重要但尚未解决的问题。
- en: 'We investigate how qualitative advantages of LLMs change when fine-tuning on
    parallel data. We consider LLaMA and Falcon models, with parameter counts ranging
    from 7 billion up to 65 billion. The LLM properties we investigate are general
    translation quality, formality steerability, non-literalness in idiom translations,
    performance on specialized domains, and performance on document-level input which
    requires contextualisation of ambiguous tokens. We compare two fine-tuning strategies
    for varying data sizes (89K up to 1.4M) in six translation directions. Our main
    findings and contributions are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了在平行数据上微调时LLMs的定性优势如何变化。我们考虑了LLaMA和Falcon模型，参数数量从70亿到650亿不等。我们研究的LLM属性包括总体翻译质量、形式控制、成语翻译中的非字面性、在专业领域的表现，以及需要上下文化模糊标记的文档级输入的表现。我们比较了六个翻译方向上不同数据规模（从89K到140万）的两种微调策略。我们的主要发现和贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that while fine-tuning LLMs on parallel data enhances overall translation
    quality as measured by COMET, it simultaneously leads to a decline in important
    attributes. Even when only using 18k fine-tuning samples we observe degradations
    in formality steering, technical translation through few-shot examples, and contextualization
    capabilities required for document-level translation. In general, we find that
    using larger data sets for fine-tuning data results in more severe degradations,
    and these trends are consistent across all tested model scales and architectures.
    The exception we observe is in the ability to produce less literal translations,
    which improves in fine-tuning.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了虽然在平行数据上对LLMs进行微调可以提高整体翻译质量（根据COMET测量），但同时也导致了重要属性的下降。即使仅使用18k的微调样本，我们也观察到形式控制、通过少量样本进行技术翻译的能力以及文档级翻译所需的上下文能力的退化。一般而言，我们发现使用更大的数据集进行微调会导致更严重的退化，这种趋势在所有测试的模型规模和架构中都是一致的。我们观察到的例外是生成更少字面翻译的能力，在微调过程中有所提高。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that incorporating a mix of monolingual and parallel data during fine-tuning
    can preserve abilities of LLMs. Overall translation quality is enhanced to a greater
    extent compared to fine-tuning on parallel data alone.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在微调过程中结合单语数据和平行数据可以保留大语言模型（LLMs）的能力。与仅在平行数据上进行微调相比，整体翻译质量得到了更大的提升。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a novel evaluation dataset, IdiomsInCtx-MT, to measure non-literalness
    performance. To our knowledge, it is the first dataset that consists of idiomatic
    expressions in context and their human-written translations. It covers 2 language
    pairs with 3 translation directions.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一个新颖的评估数据集IdiomsInCtx-MT，用于测量非字面翻译表现。据我们所知，这是第一个由上下文中的成语表达及其人工翻译组成的数据集。该数据集涵盖了2对语言和3个翻译方向。
- en: Our findings highlight the importance of creating fine-tuning approaches that
    enhance general translation quality while also preserving the distinctive capabilities
    of LLMs for machine translation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现突出了制定微调方法的重要性，这些方法不仅提升一般翻译质量，还保持LLMs在机器翻译中的独特能力。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Advantages of LLMs for MT
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLMs在机器翻译中的优势
- en: Several studies have investigated the use of LLMs for translation. Generally,
    current LLMs show strong performance for most language pairs, but lag behind NMT
    systems when translating into low-resource languages (Zhu et al., [2023a](#bib.bib57);
    Stap and Araabi, [2023](#bib.bib43); Robinson et al., [2023](#bib.bib42); Kocmi
    et al., [2023](#bib.bib22)). In addition to strong performance, LLMs exhibit certain
    abilities that are relevant for translation. NMT systems show a bias towards generating
    text that is over-represented in the data, such as language varieties (Riley et al.,
    [2023](#bib.bib40)) and formality (Rippeth et al., [2022](#bib.bib41)), whereas
    LLMs can easily be controlled for this bias using examples (Garcia et al., [2023](#bib.bib17)).
    In addition, examples can be supplied to improve general LLM translation quality
    via in-context learning (Agrawal et al., [2022](#bib.bib1); Moslem et al., [2023a](#bib.bib29)).
    NMT models are often unable to translate idioms accurately and generate literal
    translations (Dankers et al., [2022](#bib.bib12)). LLMs produce less literal outputs
    compared to NMT models, particularly for sentences that contain idiomatic expressions
    (Vilar et al., [2023](#bib.bib47); Raunak et al., [2023](#bib.bib36)). NMT models
    are trained on sentence level, and thus do not take into account document context.
    LLMs outperform NMT models for document translation in general domains (such as
    news and social media) (Wang et al., [2023](#bib.bib48)), as well as in more specialised
    domains such as literature (Karpinska and Iyyer, [2023](#bib.bib21)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究探讨了使用LLMs进行翻译的情况。总体而言，当前LLMs在大多数语言对中表现强劲，但在翻译低资源语言时落后于NMT系统（Zhu et al.,
    [2023a](#bib.bib57); Stap and Araabi, [2023](#bib.bib43); Robinson et al., [2023](#bib.bib42);
    Kocmi et al., [2023](#bib.bib22)）。除了强劲的表现，LLMs展现了一些与翻译相关的能力。NMT系统倾向于生成在数据中过度表示的文本，如语言变体（Riley
    et al., [2023](#bib.bib40)）和正式性（Rippeth et al., [2022](#bib.bib41)），而LLMs可以通过示例轻松控制这种偏差（Garcia
    et al., [2023](#bib.bib17)）。此外，示例可以通过上下文学习提升LLM的翻译质量（Agrawal et al., [2022](#bib.bib1);
    Moslem et al., [2023a](#bib.bib29)）。NMT模型通常无法准确翻译成语，并生成字面翻译（Dankers et al., [2022](#bib.bib12)）。LLMs相较于NMT模型生成的输出更少字面，尤其是包含成语的句子（Vilar
    et al., [2023](#bib.bib47); Raunak et al., [2023](#bib.bib36)）。NMT模型是在句子级别上训练的，因此不考虑文档上下文。LLMs在一般领域（如新闻和社交媒体）（Wang
    et al., [2023](#bib.bib48)）以及更专业的领域如文学（Karpinska and Iyyer, [2023](#bib.bib21)）中，通常优于NMT模型进行文档翻译。
- en: Finetuning LLMs for MT
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调LLMs用于机器翻译
- en: 'There are multiple strategies for fine-tuning LLMs for machine translation.
    One approach makes use of either a small set of high-quality human-written translations
    or a set of translation instructions for fine-tuning (Li et al., [2023](#bib.bib25);
    Zeng et al., [2023](#bib.bib55); Jiao et al., [2023](#bib.bib19)). Another line
    of work makes use of more traditional machine translation data: parallel data
    from the web, which is orders of magnitude larger compared to what is used in
    fine-tuning (Yang et al., [2023](#bib.bib53); Alves et al., [2023](#bib.bib5);
    Zhang et al., [2023](#bib.bib56); Zhu et al., [2023b](#bib.bib58)). These strategies
    are focused on improving general machine translation quality, but it remains unclear
    what happens to other abilities that are relevant for translation. We investigate
    the effect of fine-tuning on relevant abilities for translation using publicly
    available models.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种策略可以用来微调LLMs以进行机器翻译。一种方法使用少量高质量人工翻译或一组翻译指令进行微调（Li et al., [2023](#bib.bib25);
    Zeng et al., [2023](#bib.bib55); Jiao et al., [2023](#bib.bib19)）。另一类工作利用更传统的机器翻译数据：来自网络的平行数据，其规模比微调中使用的数据大几个数量级（Yang
    et al., [2023](#bib.bib53); Alves et al., [2023](#bib.bib5); Zhang et al., [2023](#bib.bib56);
    Zhu et al., [2023b](#bib.bib58)）。这些策略旨在提升机器翻译的总体质量，但尚不清楚对翻译相关的其他能力有何影响。我们调查了微调对翻译相关能力的影响，使用公开可用的模型。
- en: 3 Fine-tuning LLMs on parallel data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 微调LLMs在平行数据上的应用
- en: 3.1 Experimental Setup
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: Models
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型
- en: We use the LLaMA (Touvron et al., [2023](#bib.bib46)) 7B, 13B, 30B, 65B, and
    Falcon (Almazrouei et al., [2023](#bib.bib4)) 7B and 40B language models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了LLaMA (Touvron et al., [2023](#bib.bib46)) 7B、13B、30B、65B和Falcon (Almazrouei
    et al., [2023](#bib.bib4)) 7B和40B语言模型。
- en: Optimisation
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化
- en: 'We refer the reader to Appendix [A](#A1 "Appendix A Details on experimental
    setup ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") for full details on optimisation, hyperparameters, and instruction
    formats.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将读者参考附录 [A](#A1 "附录 A 实验设置详细信息 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力") 获取关于优化、超参数和指令格式的完整细节。
- en: Language directions
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言方向
- en: We consider the language directions German (de), Russian (ru), and Chinese (zh)
    into and out of English (en).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑德语（de）、俄语（ru）和中文（zh）与英语（en）之间的语言方向。
- en: Human-written training data
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工撰写的训练数据
- en: Following Xu et al. ([2023](#bib.bib52)), we use human-written translations
    from WMT17 to WMT20, resulting in 89K training examples that are evenly distributed
    across the language directions we consider. On this dataset, we perform full fine-tuning
    for models up to 40B and QLoRA fine-tuning for the LLaMA 65B model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Xu et al. ([2023](#bib.bib52)) 的方法，我们使用 WMT17 到 WMT20 的人工撰写翻译，结果得到 89K 训练示例，这些示例在我们考虑的语言方向上均匀分布。在这个数据集上，我们对最多
    40B 的模型进行全面微调，并对 LLaMA 65B 模型进行 QLoRA 微调。
- en: Web-scraped training data
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 网络抓取的训练数据
- en: Additionally we train models on general domain OPUS (Tiedemann, [2012](#bib.bib45))
    data from the News-Commentary, WikiTitles, and ParaCrawl (Bañón et al., [2020](#bib.bib9))
    corpora. To ensure that the resulting data is above an acceptable quality threshold
    we perform data filtering using an internal Quality Estimation (QE) model, which
    has a similar architecture as COMETKiwi (Rei et al., [2022](#bib.bib38)) and is
    based on the InfoXLM-Large pretrained multilingual encoder (Chi et al., [2021](#bib.bib11)).
    We train our sentence-level QE model on a large internal dataset of human annotations
    for more than 12 languages, where each translation is rated between 1 (completely
    random) and 6 (perfect translation).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在一般领域的 OPUS（Tiedemann, [2012](#bib.bib45)）数据上训练模型，这些数据来自 News-Commentary、WikiTitles
    和 ParaCrawl（Bañón et al., [2020](#bib.bib9)）语料库。为了确保结果数据达到可接受的质量阈值，我们使用一个内部质量估计（QE）模型进行数据过滤，该模型具有与
    COMETKiwi（Rei et al., [2022](#bib.bib38)）类似的架构，并基于 InfoXLM-Large 预训练的多语言编码器（Chi
    et al., [2021](#bib.bib11)）。我们在一个包含超过 12 种语言的人类标注的大型内部数据集上训练我们的句子级 QE 模型，每个翻译的评分范围从
    1（完全随机）到 6（完美翻译）。
- en: We use a subset of 1.4M sentence pairs of this filtered data that is evenly
    distributed across language directions for training. We fine-tune LLaMA models
    up to 40B parameters on this dataset but leave out larger models because of the
    high computational cost.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了这个过滤后的数据子集中的 1.4M 句对，这些数据在语言方向上均匀分布用于训练。我们在这个数据集上对最多 40B 参数的 LLaMA 模型进行微调，但由于计算成本高，我们不使用更大的模型。
- en: '![Refer to caption](img/84aa6ea49cd01888ec656fd367683159.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/84aa6ea49cd01888ec656fd367683159.png)'
- en: 'Figure 1: X$\rightarrow$X (bottom) COMET scores on WMT22 for models trained
    on human-written translations with different amounts of training data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：X$\rightarrow$X（底部）COMET 分数在 WMT22 上，用于训练有不同训练数据量的人类撰写的翻译模型。
- en: '![Refer to caption](img/cfee44933ec23e67dc66ed07e21359ec.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/cfee44933ec23e67dc66ed07e21359ec.png)'
- en: 'Figure 2: Accuracy of formality markers for models trained on human-written
    translations.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在人工撰写的翻译数据上训练的模型形式标记的准确性。
- en: Evaluation data and metrics
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估数据和指标
- en: 'For evaluation, we consider the following test sets:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中，我们考虑以下测试集：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'WMT22 To evaluate general machine translation quality, we use WMT22 (Kocmi
    et al., [2022](#bib.bib23)) test sets consisting of news, e-commerce, social,
    and conversational domains. We evaluate all language directions on this test set
    in a 0-shot setting. Following the recommendation of Kocmi et al. ([2021](#bib.bib24)),
    we report COMET scores¹¹1model: unbabel/wmt22-comet-da (Rei et al., [2020](#bib.bib37)).
    We do not report BLEU scores (Papineni et al., [2002](#bib.bib33)), since this
    metric is known to have a poor correlation with human judgments (Mathur et al.,
    [2020](#bib.bib27); Freitag et al., [2021](#bib.bib15); Kocmi et al., [2021](#bib.bib24);
    Freitag et al., [2022](#bib.bib16)).'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'WMT22 为了评估一般的机器翻译质量，我们使用 WMT22（Kocmi et al., [2022](#bib.bib23)）测试集，这些测试集包括新闻、电子商务、社交和对话领域。我们在这个测试集上以
    0-shot 设置评估所有语言方向。根据 Kocmi et al. ([2021](#bib.bib24)) 的建议，我们报告 COMET 分数¹¹1model:
    unbabel/wmt22-comet-da（Rei et al., [2020](#bib.bib37)）。我们不报告 BLEU 分数（Papineni
    et al., [2002](#bib.bib33)），因为这个指标与人类评判的相关性较差（Mathur et al., [2020](#bib.bib27)；Freitag
    et al., [2021](#bib.bib15)；Kocmi et al., [2021](#bib.bib24)；Freitag et al., [2022](#bib.bib16)）。'
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoCoA-MT To evaluate formality steering ability of LLMs, we make use of the
    CoCoA-MT (Nadejde et al., [2022](#bib.bib31)) dataset. It consists of 600 test
    sentences with English source and contrastive target sentences consisting of a
    formal and informal translation. We use German as the target language and report
    both COMET and accuracy based on the ratio of correctly predicted formality forms.
    We use 5-shot examples to bias the formality of outputs.²²2We also experimented
    with steering formality through prompting, but the results were inferior to using
    5-shot examples.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoCoA-MT 为了评估LLMs的正式性引导能力，我们使用了CoCoA-MT（Nadejde等，[2022](#bib.bib31)）数据集。它包含600个测试句子，其中英语源句和对比目标句包括正式和非正式的翻译。我们使用德语作为目标语言，并报告基于正确预测正式性形式的比例的COMET和准确性。我们使用5-shot示例来偏向输出的正式性²²2。我们还尝试通过提示来引导正式性，但结果不如使用5-shot示例。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Law, Medical, and TICO-19 To evaluate the in-context learning ability on technical
    domains, we consider the Law and Medical test sets from Aharoni and Goldberg ([2020](#bib.bib3)).
    We consider 5-shot inputs. We evaluate on German $\leftrightarrow$ English.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法律、医学和TICO-19 为了评估在技术领域的上下文学习能力，我们考虑了Aharoni和Goldberg（[2020](#bib.bib3)）提供的法律和医学测试集。我们考虑了5-shot输入。我们在德语$\leftrightarrow$英语之间进行评估。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ctxpro We evaluate 0-shot performance on longer inputs that includes sentences
    that require context to be disambiguated correctly by including ctxpro (Wicks
    and Post, [2023](#bib.bib50)). We consider the animacy ambiguity type in German
    $\rightarrow$. We report the generative accuracy score, which measures the accuracy
    of the contextualization.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ctxpro 我们评估在需要上下文来正确消歧的较长输入上的0-shot表现，包括ctxpro（Wicks和Post，[2023](#bib.bib50)）。我们考虑德语$\rightarrow$中的生命体歧义类型。我们报告生成准确度评分，该评分测量上下文化的准确性。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'IdiomsInCtx-MT We introduce a novel dataset consisting of idiomatic expressions
    in context and their human-written translations.⁴⁴4The dataset will be available
    shortly. The dataset comprises 2 language pairs: German and Russian paired with
    English. For German, the opposite translation directions are also included. Current
    idiom datasets stem from potentially noisy, web-extracted sources (Fadaee et al.,
    [2018](#bib.bib14)), machine-generated translations (Tang, [2022](#bib.bib44)),
    or are monolingual (Haagsma et al., [2020](#bib.bib18)). In contrast, we use professional
    translators to create a high-quality evaluation benchmark. Idiomatic expressions
    and their context sentences were sourced in the respective source language and
    translated to the target language by professional translators. In addition, the
    dataset contains annotations of the source and target idiomatic expressions for
    each segment. This enables running targeted evaluation on the 0-shot translations
    of the idiomatic expressions in addition to general quality metrics. We evaluate
    on English$\rightarrow$mwe (Zaninello and Birch, [2020](#bib.bib54)).'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IdiomsInCtx-MT 我们引入了一个新数据集，包含上下文中的成语表达及其人工翻译。⁴⁴4该数据集将很快提供。数据集包括2对语言：德语和俄语与英语配对。对于德语，也包含相反的翻译方向。目前的成语数据集源自可能有噪音的网络提取来源（Fadaee等，[2018](#bib.bib14)）、机器生成翻译（Tang，[2022](#bib.bib44)）或单语数据集（Haagsma等，[2020](#bib.bib18)）。相比之下，我们使用专业翻译人员创建了一个高质量的评估基准。成语及其上下文句子在相应的源语言中获取，由专业翻译人员翻译成目标语言。此外，数据集包含每个片段的源语言和目标语言成语的注释。这使得在0-shot翻译中对成语进行有针对性的评估成为可能，除了通用质量指标。我们在英语$\rightarrow$mwe（Zaninello和Birch，[2020](#bib.bib54)）上进行评估。
- en: Additionally, we considered style transfer and constrained generation, such
    as prompting a language model to use custom terminology, as additional translation
    capabilities. However, our final choices were influenced by the availability of
    suitable test sets and the applicability of targeted automatic metrics, leading
    us to not pursue these options.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们考虑了风格转换和受限生成，例如提示语言模型使用自定义术语，作为额外的翻译能力。然而，我们的最终选择受限于合适的测试集的可用性和目标自动化指标的适用性，导致我们没有追求这些选项。
- en: 3.2 Results
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 结果
- en: General translation quality improves
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 一般翻译质量有所提升
- en: 'Results on WMT22 for models trained on human-written translations are summarized
    in Figure [1](#S3.F1 "Figure 1 ‣ Web-scraped training data ‣ 3.1 Experimental
    Setup ‣ 3 Fine-tuning LLMs on parallel data ‣ The Fine-Tuning Paradox: Boosting
    Translation Quality Without Sacrificing LLM Abilities"). Consistent with expectations,
    we observe that fine-tuning generally improves the translation quality, and larger
    models generally perform better. Using 89K parallel examples does not always yield
    better results compared to smaller datasets. For most language directions, we
    notice an initial increase in translation quality, followed by a slight decline.
    The most notable increases are found for the out-of-English directions. In contrast,
    when models are fine-tuned on a more extensive dataset (up to 1.4 million examples)
    sourced from the web (refer to Appendix [B](#A2 "Appendix B Additional results
    ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM
    Abilities"), Figure [7](#A2.F7 "Figure 7 ‣ B.1 General translation quality (WMT22)
    ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")), translation quality continues to
    improve with the addition of more data. We hypothesize that this difference stems
    from the domain-specific composition of the human-written WMT training data, which
    is exclusively news content. In contrast, the OPUS dataset is more diverse and
    includes multiple domains. This diversity better reflects domain composition of
    the WMT22 test set, which has content from news, e-commerce, social media and
    conversational domains. The improvements are significantly more marked in translations
    from other languages into English. A comparative analysis of the best-performing
    checkpoints of LLaMA models (7B, 13B, 30B) fine-tuned on human-written and OPUS
    data across 6 translation directions shows a clear trend. In 15 out of 18 cases,
    models fine-tuned on the larger OPUS dataset achieved superior results (refer
    to Appendix [B](#A2 "Appendix B Additional results ‣ The Fine-Tuning Paradox:
    Boosting Translation Quality Without Sacrificing LLM Abilities"), Table [4](#A2.T4
    "Table 4 ‣ B.1 General translation quality (WMT22) ‣ Appendix B Additional results
    ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM
    Abilities")).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 人工翻译训练模型在WMT22上的结果总结见图[1](#S3.F1 "图1 ‣ 网络抓取的训练数据 ‣ 3.1 实验设置 ‣ 3 微调LLM的并行数据 ‣
    微调悖论：提升翻译质量而不牺牲LLM能力")。与预期一致，我们观察到微调通常能提高翻译质量，而较大的模型通常表现更好。使用89K个平行示例并不总是能比更小的数据集取得更好的结果。在大多数语言方向上，我们注意到翻译质量首先有所提升，然后略微下降。最显著的提升发生在非英语方向。相比之下，当模型在更大规模的数据集（高达140万示例）上进行微调，这些数据集来自网络（参见附录[B](#A2
    "附录B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")，图[7](#A2.F7 "图7 ‣ B.1 一般翻译质量（WMT22） ‣ 附录B 额外结果
    ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")），翻译质量随着数据的增加而持续改善。我们假设这一差异源于人工编写WMT训练数据的领域特定组成，这些数据专门为新闻内容。相比之下，OPUS数据集更加多样化，涵盖多个领域。这种多样性更好地反映了WMT22测试集的领域组成，该测试集包含新闻、电子商务、社交媒体和对话领域的内容。从其他语言到英语的翻译改善更为显著。对在人工编写和OPUS数据上进行微调的LLaMA模型（7B、13B、30B）在6个翻译方向上最佳表现检查点的比较分析显示了一个明确的趋势。在18个案例中的15个中，微调在较大OPUS数据集上的模型取得了更优的结果（参见附录[B](#A2
    "附录B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")，表[4](#A2.T4 "表4 ‣ B.1 一般翻译质量（WMT22） ‣ 附录B 额外结果
    ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")）。
- en: '![Refer to caption](img/755dddd67965aa1641eb2527f4c2f787.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/755dddd67965aa1641eb2527f4c2f787.png)'
- en: 'Figure 3: COMET on technical domains using 5-shot examples for models trained
    on human-written translations.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用5-shot示例在技术领域的COMET，模型在人工翻译上进行了训练。
- en: '![Refer to caption](img/b0b4990b0ecb7a410701e3489926b371.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b0b4990b0ecb7a410701e3489926b371.png)'
- en: 'Figure 4: Accuracy of animacy contextualization for German$\rightarrow$English
    for models fine-tuned on human-written translations.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：针对德语$\rightarrow$英语的动画性背景化准确性，模型在人工翻译上进行了微调。
- en: Formality steering ability degrades
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 格式化控制能力下降
- en: 'We show results for formality steering using 5-shot examples for models trained
    on human-written data in Figure [2](#S3.F2 "Figure 2 ‣ Web-scraped training data
    ‣ 3.1 Experimental Setup ‣ 3 Fine-tuning LLMs on parallel data ‣ The Fine-Tuning
    Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities"). Notably,
    the base models exhibit strong performance for this task; for instance, the LLaMA-7b
    model achieves an accuracy of 0.982 in identifying informal markers. However,
    fine-tuning on only 18K examples results in a decline of this ability to 0.862,
    even though German-English COMET on WMT22 continues to improve up to 36K examples.
    The degradation is more pronounced with informal markers, which is likely attributable
    to the formal bias inherent in the WMT22 training data. Fine-tuning on more data
    further degrades formality steering capabilities: there is a significant negative
    correlation (Spearman’s $\rho=-0.46$) with COMET scores, which suggests that comprehensive
    evaluation of LLMs for machine translation benefits from task-specific metrics.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了使用 5-shot 示例进行正式性引导的结果，适用于在人工编写数据上训练的模型，见图 [2](#S3.F2 "图 2 ‣ 网络抓取的训练数据
    ‣ 3.1 实验设置 ‣ 3 微调 LLMs 于平行数据 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")。值得注意的是，基础模型在这个任务上表现出色；例如，LLaMA-7b
    模型在识别非正式标记时的准确率为 0.982。然而，仅在 18K 示例上进行微调会使这种能力下降到 0.862，尽管德英 COMET 在 WMT22 上的表现仍在
    36K 示例下持续改善。非正式标记的性能下降更加明显，这可能是由于 WMT22 训练数据中固有的正式偏差。进一步增加数据量的微调会进一步削弱正式性引导能力：与
    COMET 分数存在显著的负相关（Spearman 的 $\rho=-0.46$），这表明对 LLM 进行机器翻译的全面评估需要依赖于任务特定的指标。
- en: 'Further, when examining models trained on OPUS data (see Appendix [B](#A2 "Appendix
    B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without
    Sacrificing LLM Abilities") Figure [9](#A2.F9 "Figure 9 ‣ B.2 Formality steering
    ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")), we find that increasing the amount
    of fine-tuning data up to 1.4M parallel sentences further exacerbates the degradation.
    Again, we observe a significant negative correlation ($\rho=-0.58$) between accuracy
    and dataset size for OPUS-trained models, reinforcing the conclusion that larger
    parallel datasets during fine-tuning adversely affect formality steering capabilities.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当检查在 OPUS 数据上训练的模型时（见附录 [B](#A2 "附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力") 图 [9](#A2.F9
    "图 9 ‣ B.2 正式性引导 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")），我们发现将微调数据量增加到 1.4M 对等句子会进一步加剧性能下降。再次观察到，OPUS
    训练模型的准确性与数据集大小之间存在显著的负相关（$\rho=-0.58$），这进一步强化了在微调过程中更大的平行数据集对正式性引导能力的不利影响的结论。
- en: Performance on technical domains degrades
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 技术领域的性能下降
- en: 'Next, we evaluate the model capabilities of doing technical translations given
    5-shot examples. Results on human-written training data for a subset of the domains
    and directions are shown in Figure [3](#S3.F3 "Figure 3 ‣ General translation
    quality improves ‣ 3.2 Results ‣ 3 Fine-tuning LLMs on parallel data ‣ The Fine-Tuning
    Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities"). In
    most cases, performance starts to degrade after only 18K examples. For example,
    LLaMA-7b scores 0.8308 COMET on TICO English-Russian, whereas after fine-tuning
    on 18K examples COMET is 0.8234, a degradation of 0.0074. Results on the other
    domains and directions show a similar trend (Appendix [B](#A2 "Appendix B Additional
    results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") Figure [10](#A2.F10 "Figure 10 ‣ B.3 Technical domains ‣ Appendix
    B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without
    Sacrificing LLM Abilities")). The COMET scores correlate negatively with datastore
    size ($\rho=-0.27$), indicating that fine-tuning on more data results in larger
    degradations. When inspecting models trained on OPUS data, we observe consistent
    conclusions: few-shot technical domain translation capabilities degrade, and the
    amount of degradation is dependent on the dataset size (see Appendix [B](#A2 "Appendix
    B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without
    Sacrificing LLM Abilities") Figure [11](#A2.F11 "Figure 11 ‣ B.3 Technical domains
    ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们评估了在给定5个示例的情况下进行技术翻译的模型能力。人类编写的训练数据在某些领域和方向上的结果如图 [3](#S3.F3 "Figure
    3 ‣ General translation quality improves ‣ 3.2 Results ‣ 3 Fine-Tuning LLMs on
    parallel data ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without
    Sacrificing LLM Abilities")所示。在大多数情况下，性能在仅18K示例后开始下降。例如，LLaMA-7b在TICO英俄翻译上的COMET得分为0.8308，而在18K示例微调后，COMET得分降至0.8234，下降了0.0074。其他领域和方向的结果显示了类似的趋势（附录
    [B](#A2 "Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities") 图 [10](#A2.F10 "Figure 10 ‣ B.3 Technical
    domains ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")）。COMET得分与数据存储库大小之间存在负相关 ($\rho=-0.27$)，这表明在更多数据上进行微调会导致更大的退化。在检查了在OPUS数据上训练的模型时，我们观察到一致的结论：少量示例的技术领域翻译能力下降，退化程度依赖于数据集大小（参见附录
    [B](#A2 "Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities") 图 [11](#A2.F11 "Figure 11 ‣ B.3 Technical
    domains ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")）。'
- en: Contextualization ability on document-level input degrades
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文档级输入的上下文化能力下降
- en: 'Our analysis extends to the animacy contextualization accuracy of document-level
    input. We show results for models trained on human-written data in Figure [4](#S3.F4
    "Figure 4 ‣ General translation quality improves ‣ 3.2 Results ‣ 3 Fine-tuning
    LLMs on parallel data ‣ The Fine-Tuning Paradox: Boosting Translation Quality
    Without Sacrificing LLM Abilities"). Mirroring the trend observed in formality
    steering, a clear degradation in contextualization accuracy is noted upon fine-tuning
    the models on parallel data. Again, we observe that fine-tuning on only 18K examples
    results in a decline of this ability, even though general translation quality
    on WMT continues to improve. For example, Falcon-40b scores 0.91 before fine-tuning,
    which degrades to 0.85 after 18K examples. The decline can be summarized by a
    negative correlation between accuracy and the size of the dataset used for fine-tuning
    ($\rho=-0.55$).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的分析扩展到了文档级输入的生动性上下文化准确性。我们在图 [4](#S3.F4 "Figure 4 ‣ General translation quality
    improves ‣ 3.2 Results ‣ 3 Fine-Tuning LLMs on parallel data ‣ The Fine-Tuning
    Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities")中展示了在人工编写的数据上训练的模型结果。与正式性引导中观察到的趋势相呼应，在并行数据上微调模型后，上下文化准确性明显下降。我们再次观察到，仅用18K示例进行微调会导致这种能力的下降，尽管在WMT上的总体翻译质量仍在提升。例如，Falcon-40b在微调前的得分为0.91，而在18K示例后下降至0.85。下降可以总结为准确性与用于微调的数据集大小之间的负相关
    ($\rho=-0.55$)。'
- en: Performance on idioms remains stable or improves
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 成语的表现保持稳定或有所提升
- en: 'We evaluate the quality of idiomatic translations on the IdiomsInCtx-MT test
    sets. Since idiomatic translation is inherently difficult, we focus our analysis
    on the strongest model, LLaMA-65b. Figure [5](#S3.F5 "Figure 5 ‣ Performance on
    idioms remains stable or improves ‣ 3.2 Results ‣ 3 Fine-tuning LLMs on parallel
    data ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") shows COMET, LitTER (lower is better) and MWEScore (higher is
    better) across training checkpoints for 1 epoch. While LitTER uses input-specific
    block lists to assess the literalness of translation outputs based on annotations
    of idiomatic expressions in the input, MWEScore additionally relies on one or
    more gold translations of those idiomatic expressions and computes a score based
    on edit distance of output vs reference idiom tokens.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了 IdiomsInCtx-MT 测试集上的习语翻译质量。由于习语翻译本质上具有挑战性，我们将分析重点放在最强的模型 LLaMA-65b 上。图
    [5](#S3.F5 "图 5 ‣ 对习语的表现保持稳定或改善 ‣ 3.2 结果 ‣ 3 在对齐数据上微调 LLMs ‣ 微调悖论：提高翻译质量而不牺牲 LLM
    能力") 显示了在 1 个周期的训练检查点中，COMET、LitTER（分数越低越好）和 MWEScore（分数越高越好）的表现。虽然 LitTER 使用特定输入的块列表来评估翻译输出的字面性，基于输入中的习语表达的注释，MWEScore
    则依赖于一个或多个黄金翻译，并基于输出与参考习语令牌的编辑距离计算分数。
- en: Similar to the WMT22 test set, we see that COMET scores improve until the first
    1-3 checkpoints before stabilizing or starting to decrease. However, for idiomatic
    expressions even thed targeted metrics LitTER and MWEScore improve during fine-tuning
    or least remain stable. This indicates that even a large open-source model like
    LLaMA-65b still has room for improvement when it comes to idiomatic translations.
    Future work could investigate translation literalness and idiomaticity of LLM
    translations on stronger models such as GPT-3.5 during fine-tuning.⁵⁵5https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与 WMT22 测试集类似，我们看到 COMET 分数在前 1-3 个检查点之前会提高，然后稳定或开始下降。然而，对于习语表达，即便是针对性的指标 LitTER
    和 MWEScore 在微调过程中也有所提高或至少保持稳定。这表明即便是像 LLaMA-65b 这样的大型开源模型，在习语翻译方面仍有改进的空间。未来的工作可以调查在更强大的模型如
    GPT-3.5 上进行微调时 LLM 翻译的字面性和习语性。⁵⁵5https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates
- en: '![Refer to caption](img/65d49cd95dd80de273a67451da5fd48c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/65d49cd95dd80de273a67451da5fd48c.png)'
- en: 'Figure 5: COMET, LitTER and MWEScore on IdiomsInCtx-MT test sets for LLaMA-65b
    fine-tuned on human-quality parallel data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：COMET、LitTER 和 MWEScore 在 IdiomsInCtx-MT 测试集上的表现，用于 LLaMA-65b 在高质量人工对齐数据上进行微调。
- en: '| model | de-en | ru-en | zh-en | en-de | en-ru | en-zh | avg |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| model | de-en | ru-en | zh-en | en-de | en-ru | en-zh | avg |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| base | $0.8217$ | $0.8151$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| base | $0.8217$ | $0.8151$ |'
- en: '| FT 1:0 | $0.8342$ | $0.8378$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| FT 1:0 | $0.8342$ | $0.8378$ |'
- en: '| FT 2:0 | $0.8360$ | $0.8436$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| FT 2:0 | $0.8360$ | $0.8436$ |'
- en: '| FT 1:1 | $\mathbf{0.8380}$ | $\mathbf{0.8484}$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| FT 1:1 | $\mathbf{0.8380}$ | $\mathbf{0.8484}$ |'
- en: 'Table 1: COMET scores on WMT22\. Best scores in bold. Including both parallel
    and monolingual data during fine-tuning (FT 1:1) results in better translation
    performance compared to parallel-only fine-tuning (FT 1:0, FT 2:0).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：WMT22 上的 COMET 分数。最佳分数以粗体显示。包括平行和单语数据的微调（FT 1:1）相较于仅平行数据的微调（FT 1:0、FT 2:0）结果显示了更好的翻译性能。
- en: '![Refer to caption](img/edfb24f2fbc063e3ddb513cf60836868.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/edfb24f2fbc063e3ddb513cf60836868.png)'
- en: (a) Formality steering
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 格式控制
- en: '![Refer to caption](img/80d39ec6bc14297866c0b0391b9290b2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/80d39ec6bc14297866c0b0391b9290b2.png)'
- en: (b) Contextualization
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 上下文化
- en: '![Refer to caption](img/d1b3b9d361316485b152afb641c45f04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/d1b3b9d361316485b152afb641c45f04.png)'
- en: (c) Technical translations
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 技术翻译
- en: '![Refer to caption](img/d141a1ec11f1f9e1842dd64a20467338.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/d141a1ec11f1f9e1842dd64a20467338.png)'
- en: (d) Literalness
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 字面性
- en: 'Figure 6: Formality steering accuracy scores (a), Animacy contextualization
    accuracy of document-level input (b), COMET for few-shot technical translations
    (c), and LitTER scores for idioms. Base is LLaMA-7b before fine-tuning, 1:1 is
    fine-tuned on 89K parallel and 89K monolingual data, 1:0 on 89K parallel data,
    and 2:0 on 178K parallel data. Integrating monolingual data generally results
    in more preservation of capabilities.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：格式控制准确度评分（a）、文档级输入的生动性上下文化准确度（b）、少样本技术翻译的 COMET 分数（c）和习语的 LitTER 分数。Base
    是微调前的 LLaMA-7b，1:1 是在 89K 对齐和 89K 单语数据上微调，1:0 是在 89K 对齐数据上，2:0 是在 178K 对齐数据上。一般来说，整合单语数据能更好地保持能力。
- en: 4 Fine-tuning on a mix of monolingual and parallel data
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 在单语和对齐数据混合上进行微调
- en: Having established that fine-tuning on parallel data leads to a decline in various
    advantages of LLMs for machine translation, this section delves into strategies
    to mitigate this degradation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定平行数据的微调会导致LLM在机器翻译方面各种优势的下降之后，本节探讨了减轻这种退化的策略。
- en: A potential approach to counteract degradation involves incorporating examples
    of desired behaviors during fine-tuning. For instance, the degradation of few-shot
    capabilities for domain adaptation can be partially mitigated by including few-shot
    examples during fine-tuning (Alves et al., [2023](#bib.bib5); Moslem et al., [2023b](#bib.bib30)).
    However, our aim is to establish a more general solution that prevents degradation
    across a broader spectrum of behaviors, without the need to specifically include
    data for each behavior during fine-tuning. To achieve this, our experiments involve
    a blend of monolingual and parallel data in the fine-tuning phase. This strategy
    stems from the understanding that the pre-training on monolingual data contributed
    to these beneficial phenomena, and our goal is to retain these qualities during
    fine-tuning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 应对退化的一种潜在方法是，在微调过程中加入期望行为的示例。例如，领域适应的少样本能力退化可以通过在微调过程中加入少样本示例来部分缓解（Alves et
    al., [2023](#bib.bib5); Moslem et al., [2023b](#bib.bib30)）。然而，我们的目标是建立一个更通用的解决方案，以防止在更广泛的行为范围内的退化，而无需在微调过程中专门包括每种行为的数据。为了实现这一点，我们的实验涉及在微调阶段混合使用单语和平行数据。这一策略源于对单语数据预训练有助于这些有益现象的理解，我们的目标是保留这些特性。
- en: 4.1 Experimental setup
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'To construct our monolingual dataset, we use the News-Commentary data from
    WMT22. This dataset includes document-level information, which we preserve by
    concatenating sentences within each paragraph to form a single data entry. The
    resulting processed data closely resembles the type of data used for LLM pre-training.
    We then merged this monolingual dataset with parallel data sourced from OPUS (we
    sample 89K parallel examples), maintaining a 1:1 ratio and resulting in a total
    of 178K examples. We refer to this arrangement as the FT 1:1 setup. We compare
    this setup to several baselines: 1) the base model prior to any fine-tuning; 2)
    the FT 1:0 setup, which involves fine-tuning exclusively on parallel OPUS data
    (89K); and 3) the FT 2:0 setup, where fine-tuning is conducted on parallel OPUS
    data equal in size to our mixed monolingual and parallel dataset, totalling 178K
    examples. We use LLaMA-7B with a context window size of 2048 tokens for this experiment.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的单语数据集，我们使用了来自WMT22的新闻评论数据。该数据集包括文档级别的信息，我们通过在每个段落内连接句子来保留这些信息，形成一个单一的数据条目。处理后的数据与用于LLM预训练的数据类型非常相似。然后，我们将这个单语数据集与来自OPUS的平行数据合并（我们抽取了89K个平行示例），保持1:1的比例，最终总计178K个示例。我们将这种安排称为FT
    1:1设置。我们将这种设置与几个基准进行比较：1）在任何微调之前的基础模型；2）FT 1:0设置，仅在平行OPUS数据（89K）上进行微调；以及3）FT 2:0设置，在平行OPUS数据上进行微调，其大小与我们混合的单语和平行数据集相等，总计178K个示例。我们在这个实验中使用了上下文窗口大小为2048个标记的LLaMA-7B模型。
- en: 4.2 Results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: General translation quality further improves
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 翻译质量的总体进一步提升
- en: 'The results, as detailed in Table [1](#S3.T1 "Table 1 ‣ Performance on idioms
    remains stable or improves ‣ 3.2 Results ‣ 3 Fine-tuning LLMs on parallel data
    ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM
    Abilities"), show the comparative performance of the baselines and the integration
    of monolingual data during the fine-tuning phase on general translation quality
    (WMT22) as measured by COMET. Including monolingual data (FT 1:1) leads to translations
    that generally surpass those produced by parallel-only fine-tuning approaches
    (FT 1:0 and FT 2:0). Notably, the most significant improvement is observed in
    the en-zh direction, where the FT 1:1 setup yields an increase of 0.0131 COMET
    over the best baseline (FT 2:0). This can be attributed to the pre-training of
    LLaMA on an English-centric corpus, which contains only minimal amounts of (accidental)
    Chinese data. As suggested by Xu et al. ([2023](#bib.bib52)), out-of-English capabilities
    of the model can be substantially augmented through an additional monolingual
    fine-tuning step, a methodology akin to our approach.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '结果，如表[1](#S3.T1 "Table 1 ‣ Performance on idioms remains stable or improves
    ‣ 3.2 Results ‣ 3 Fine-tuning LLMs on parallel data ‣ The Fine-Tuning Paradox:
    Boosting Translation Quality Without Sacrificing LLM Abilities")所详细说明，展示了基准和在微调阶段整合单语数据对一般翻译质量（WMT22）影响的比较，以COMET测量。包括单语数据（FT
    1:1）产生的翻译通常优于仅使用平行数据的微调方法（FT 1:0和FT 2:0）。值得注意的是，最显著的改进出现在中英翻译方向，其中FT 1:1设置比最佳基准（FT
    2:0）提高了0.0131 COMET。这可以归因于LLaMA在以英语为中心的语料库上进行预训练，该语料库仅包含少量（偶然的）中文数据。如Xu等人（[2023](#bib.bib52)）所建议，通过额外的单语微调步骤，可以显著增强模型的非英语能力，这与我们的方法类似。'
- en: 'While the enhancement of general translation quality is a beneficial outcome,
    our primary interest lies in evaluating the ability of our method to preserve
    and possibly enhance the qualitative behaviors inherent in Large Language Models.
    This aspect forms the next focus of our investigation. Figure [6](#S3.F6 "Figure
    6 ‣ Performance on idioms remains stable or improves ‣ 3.2 Results ‣ 3 Fine-tuning
    LLMs on parallel data ‣ The Fine-Tuning Paradox: Boosting Translation Quality
    Without Sacrificing LLM Abilities") shows a comparison between our fine-tuning
    method on formality steering, document-level contextualization, technical translation,
    and idiom translation tasks. A consistent trend is observed: the integration of
    monolingual data with parallel data during fine-tuning generally results in more
    effective preservation of various translation capabilities.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然提高一般翻译质量是一个有益的结果，但我们主要关注的是评估我们方法在保留和可能增强大语言模型固有的定性行为方面的能力。这一方面构成了我们调查的下一重点。图[6](#S3.F6
    "Figure 6 ‣ Performance on idioms remains stable or improves ‣ 3.2 Results ‣ 3
    Fine-tuning LLMs on parallel data ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities")展示了我们在正式性引导、文档级上下文化、技术翻译和成语翻译任务上的微调方法的比较。观察到一个一致的趋势：在微调过程中将单语数据与平行数据结合通常能更有效地保留各种翻译能力。'
- en: Minimal formality steering degradation
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正式性引导降级最小
- en: Baselines using only parallel data show accuracy drops as great as 0.198 for
    formal and 0.11 for informal steering. The inclusion of monolingual data mitigates
    this degradation, reducing it to just 0.025 for formal and 0.007 for informal
    steering, albeit some degradation persists when compared to the baseline.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用平行数据的基准在正式和非正式引导中准确率下降分别高达0.198和0.11。纳入单语数据可以减轻这种降级，将其减少到正式引导仅为0.025，非正式引导为0.007，尽管与基准相比，仍有一些降级。
- en: Degradation of contextualization abilities are lessened
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上下文化能力的降级得到减少
- en: The impact of combining parallel and monolingual data during fine-tuning is
    also evident here. For translations from German to English and Russian to English,
    the loss in accuracy is up to 0.073 and 0.075, respectively, when using only parallel
    data. Incorporating monolingual data diminishes this degradation for Russian to
    English translations (-0.035), and even shows a notable improvement of +0.021
    over the base model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中结合平行数据和单语数据的影响在这里也很明显。对于德语到英语和俄语到英语的翻译，仅使用平行数据的准确率损失分别高达0.073和0.075。加入单语数据可以减少俄语到英语翻译的降级（-0.035），甚至相对于基准模型显示出显著改善（+0.021）。
- en: Technical domain performance is enhanced
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 技术领域表现得到提升
- en: 'The inclusion of monolingual data during fine-tuning enhances performance for
    English into Russian and Chinese, and vice versa. For instance, in the TICO English
    to Chinese translation task, the blended approach of monolingual and parallel
    data in fine-tuning yields a +0.0089 COMET score improvement over the baseline.
    Conversely, relying solely on parallel data results in a 0.022 COMET score decrease
    (FT 2:0), indicating a substantial differential of 0.03. For English in and out
    of German in both the Law and Medical domain, the differences between fine-tuning
    with monolingual data plus parallel data and the base model are minimal. In contrast
    to using parallel data only, we observe no decline. Notably, we use parallel data
    up to 178K (FT 2:0), where degradation is relatively modest in the case of few-shot
    technical domain translation. When doing extended fine-tuning, this capability
    will further degrade, as we show in Section [3.2](#S3.SS2 "3.2 Results ‣ 3 Fine-tuning
    LLMs on parallel data ‣ The Fine-Tuning Paradox: Boosting Translation Quality
    Without Sacrificing LLM Abilities").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中包含单语数据可以提升英俄和英中翻译的表现，反之亦然。例如，在TICO英中翻译任务中，单语数据与平行数据混合的微调方法使COMET评分比基线提高了+0.0089。而仅依赖平行数据则导致COMET评分下降了0.022（FT
    2:0），显示出0.03的显著差异。对于德语的英德翻译，无论是在法律领域还是医学领域，使用单语数据加平行数据进行微调与基线模型之间的差异很小。与仅使用平行数据相比，我们没有观察到任何下降。值得注意的是，我们使用的平行数据最多达到178K（FT
    2:0），在少量样本的技术领域翻译中，退化相对较小。在进行扩展微调时，这种能力将进一步退化，如我们在[3.2](#S3.SS2 "3.2 结果 ‣ 3 平行数据上的微调
    ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")部分所示。
- en: Performance on idioms improves
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 成语翻译表现有所改善
- en: Including monolingual data (1:1) improves the literalness of translations as
    measured by LitTER for ru-en and en-de. However, LLaMA-7b does not demonstrate
    good performance on idiomatic translations, making it an easy baseline to improve
    upon.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 包含单语数据（1:1）可以提高翻译的字面性，根据LitTER对俄英和英德的测量结果表明。然而，LLaMA-7b在成语翻译方面表现不佳，因此它是一个容易改进的基线。
- en: These findings underscore the nuanced benefits of incorporating monolingual
    data when fine-tuning English-centric LLMs for machine translation tasks, specifically
    in preserving task-relevant capabilities. However, for long-term progress, we
    advocate for the development of LLMs with multilingual data in mind. In this approach,
    parallel data would be combined with monolingual data during the pre-training
    phase (Anil et al., [2023](#bib.bib7); Wei et al., [2023](#bib.bib49)). Nevertheless,
    even when beginning with a more robust multilingual LLM for translation purposes,
    the exploration of fine-tuning strategies that preserve the emerged capabilities
    of LLMs remains critical, especially when adapting these models for various use
    cases and objectives.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现强调了在微调以英语为中心的LLMs用于机器翻译任务时，包含单语数据的细微好处，特别是在保留任务相关能力方面。然而，为了长期进展，我们倡导开发以多语言数据为基础的LLMs。在这种方法中，平行数据将在预训练阶段与单语数据结合使用（Anil
    et al., [2023](#bib.bib7); Wei et al., [2023](#bib.bib49)）。尽管如此，即使从更强大的多语言LLM开始进行翻译，也仍然需要探索保持LLMs新兴能力的微调策略，尤其是在将这些模型适应于各种用例和目标时。
- en: 5 Conclusion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We investigated how fine-tuning on parallel data affects the qualitative advantages
    of LLMs for machine translation. While previous research predominantly focused
    on summary quality metrics like COMET, our findings reveal a more complex interplay
    between fine-tuning and LLM capabilities. Consistent with prior work, we find
    that fine-tuning enhances the general translation quality of LLMs. However, we
    show that fine-tuning adversely impacts several important qualitative advantages
    of LLMs. We observe declines in the abilities of LLMs to 1) perform formality
    steering, 2) perform technical translation through few-shot examples, as well
    as 3) a decrease in their document-level translation capabilities. The ability
    to produce non-literal translations shows improvement post fine-tuning, likely
    because the publicly available LLMs we investigate do not perform strongly on
    this task to begin with. Furthermore, our results indicate that these degradations
    are more pronounced for larger fine-tuning datasets, even when generic translation
    quality continues to improve. These trends are consistent across different model
    scales (7b up to 65b), underscoring the generalizability of our findings. To prevent
    these degradations, we develop a fine-tuning method tailored for machine translation,
    that combines monolingual and parallel data. We show that this approach mitigates
    the degradation of LLMs’ qualitative advantages, thereby preserving their capabilities
    while improving general translation quality.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了在平行数据上微调如何影响大语言模型（LLMs）在机器翻译中的定性优势。虽然先前的研究主要关注像COMET这样的总结质量指标，我们的发现揭示了微调与LLM能力之间更复杂的相互作用。与之前的工作一致，我们发现微调增强了LLM的整体翻译质量。然而，我们显示微调对LLM的几个重要定性优势产生了负面影响。我们观察到LLM在以下方面的能力下降：1）进行正式性调整，2）通过少量示例进行技术翻译，以及3）其文档级翻译能力的下降。在微调后，生成非字面翻译的能力有所提升，这可能是因为我们调查的公开可用LLM在这方面的表现本身就不强。进一步来说，我们的结果表明，这些降级在较大的微调数据集上更为明显，即使通用翻译质量持续改善。这些趋势在不同模型规模（从7b到65b）中是一致的，突显了我们发现的普遍性。为了防止这些降级，我们开发了一种针对机器翻译的微调方法，该方法结合了单语和平行数据。我们展示了这种方法可以减轻LLM定性优势的降级，从而在提高整体翻译质量的同时，保留其能力。
- en: Limitations
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Because of the high cost of repeatedly fine-tuning LLMs of different sizes,
    we limited ourselves to 6 translation directions (German, Chinese, and Russian
    in and out of English). The impact of fine-tuning on emergent abilities of LLMs
    when translating in and out of low-resource languages is not studied in our work.
    It is therefore possible that our findings do not generalize to low-resource languages.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于反复微调不同大小的LLM的高成本，我们将研究范围限制在6个翻译方向（德语、中文和俄语进出英语）。在我们工作中未研究微调对LLM在低资源语言中的新兴能力的影响。因此，可能我们的发现不适用于低资源语言。
- en: Ethics statement
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: The IdiomsInCtx-MT dataset is annotated by professional translators and they
    were all paid a fair rate.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: IdiomsInCtx-MT 数据集由专业翻译人员注释，且他们都得到了公平的报酬。
- en: Acknowledgments
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Zach Hille and Arda Keskiner for their support in our experimental
    work.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Zach Hille 和 Arda Keskiner 对我们实验工作的支持。
- en: Christof Monz acknowledges funding by the Netherlands Organization for Scientific
    Research (NWO) under project number VI.C.192.080.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Christof Monz 感谢荷兰科学研究组织（NWO）在项目编号 VI.C.192.080 下的资助。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agrawal et al. (2022) Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer,
    and Marjan Ghazvininejad. 2022. [In-context Examples Selection for Machine Translation](http://arxiv.org/abs/2212.02437).
    ArXiv:2212.02437 [cs].
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal 等（2022）Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer 和
    Marjan Ghazvininejad。2022。[机器翻译中的上下文示例选择](http://arxiv.org/abs/2212.02437)。ArXiv:2212.02437
    [cs]。
- en: 'Agrawal et al. (2023) Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer,
    and Marjan Ghazvininejad. 2023. [In-context Examples Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.564).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    8857–8873, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal 等（2023）Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer 和
    Marjan Ghazvininejad。2023。[机器翻译中的上下文示例选择](https://doi.org/10.18653/v1/2023.findings-acl.564)。在
    *计算语言学协会年会：ACL 2023* 中，页面8857–8873，多伦多，加拿大。计算语言学协会。
- en: Aharoni and Goldberg (2020) Roee Aharoni and Yoav Goldberg. 2020. [Unsupervised
    Domain Clusters in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.692).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7747–7763, Online. Association for Computational Linguistics.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aharoni 和 Goldberg（2020）Roee Aharoni 和 Yoav Goldberg。2020年。[Unsupervised Domain
    Clusters in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.692)。在*计算语言学协会第58届年会论文集*，第7747–7763页，线上。计算语言学协会。
- en: Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel
    Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune,
    Baptiste Pannier, and Guilherme Penedo. 2023. [The Falcon Series of Open Language
    Models](http://arxiv.org/abs/2311.16867). ArXiv:2311.16867 [cs].
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei 等（2023）Ebtesam Almazrouei、Hamza Alobeidli、Abdulaziz Alshamsi、Alessandro
    Cappelli、Ruxandra Cojocaru、Mérouane Debbah、Étienne Goffinet、Daniel Hesslow、Julien
    Launay、Quentin Malartic、Daniele Mazzotta、Badreddine Noune、Baptiste Pannier 和 Guilherme
    Penedo。2023年。[The Falcon Series of Open Language Models](http://arxiv.org/abs/2311.16867)。ArXiv:2311.16867
    [cs]。
- en: Alves et al. (2023) Duarte M. Alves, Nuno M. Guerreiro, João Alves, José Pombal,
    Ricardo Rei, José G. C. de Souza, Pierre Colombo, and André F. T. Martins. 2023.
    [Steering Large Language Models for Machine Translation with Finetuning and In-Context
    Learning](http://arxiv.org/abs/2310.13448). ArXiv:2310.13448 [cs].
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alves 等（2023）Duarte M. Alves、Nuno M. Guerreiro、João Alves、José Pombal、Ricardo
    Rei、José G. C. de Souza、Pierre Colombo 和 André F. T. Martins。2023年。[Steering Large
    Language Models for Machine Translation with Finetuning and In-Context Learning](http://arxiv.org/abs/2310.13448)。ArXiv:2310.13448
    [cs]。
- en: 'Anastasopoulos et al. (2020) Antonios Anastasopoulos, Alessandro Cattelan,
    Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco
    Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham
    Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020.
    [TICO-19: the Translation Initiative for COvid-19](https://doi.org/10.18653/v1/2020.nlpcovid19-2.5).
    In *Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020*,
    Online. Association for Computational Linguistics.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anastasopoulos 等（2020）Antonios Anastasopoulos、Alessandro Cattelan、Zi-Yi Dou、Marcello
    Federico、Christian Federmann、Dmitriy Genzel、Franscisco Guzmán、Junjie Hu、Macduff
    Hughes、Philipp Koehn、Rosie Lazar、Will Lewis、Graham Neubig、Mengmeng Niu、Alp Öktem、Eric
    Paquin、Grace Tang 和 Sylwia Tur。2020年。[TICO-19: Translation Initiative for COvid-19](https://doi.org/10.18653/v1/2020.nlpcovid19-2.5)。在*EMNLP
    2020 年第1届关于 COVID-19 的 NLP 研讨会（第二部分）*，线上。计算语言学协会。'
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. [PaLM 2
    Technical Report](http://arxiv.org/abs/2305.10403). ArXiv:2305.10403 [cs].
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2023）Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, 和 Yonghui Wu. 2023. [PaLM 2 Technical
    Report](http://arxiv.org/abs/2305.10403). ArXiv:2305.10403 [cs]。
- en: Baziotis et al. (2023) Christos Baziotis, Prashant Mathur, and Eva Hasler. 2023.
    [Automatic evaluation and analysis of idioms in neural machine translation](https://doi.org/10.18653/v1/2023.eacl-main.267).
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 3682–3700, Dubrovnik, Croatia. Association
    for Computational Linguistics.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baziotis 等人（2023）Christos Baziotis, Prashant Mathur, 和 Eva Hasler. 2023. [Automatic
    evaluation and analysis of idioms in neural machine translation](https://doi.org/10.18653/v1/2023.eacl-main.267).
    载于*第17届欧洲计算语言学协会年会论文集*，页码 3682–3700，克罗地亚杜布罗夫尼克。计算语言学协会。
- en: 'Bañón et al. (2020) Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield,
    Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu,
    Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez,
    Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and
    Jaume Zaragoza. 2020. [ParaCrawl: Web-Scale Acquisition of Parallel Corpora](https://doi.org/10.18653/v1/2020.acl-main.417).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 4555–4567, Online. Association for Computational Linguistics.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bañón 等人（2020）Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu
    Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp
    Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías,
    Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, 和 Jaume Zaragoza.
    2020. [ParaCrawl: Web-Scale Acquisition of Parallel Corpora](https://doi.org/10.18653/v1/2020.acl-main.417).
    载于*第58届计算语言学协会年会论文集*，页码 4555–4567，在线。计算语言学协会。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901\.
    Curran Associates, Inc.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei. 2020. [语言模型是少样本学习者](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。在*神经信息处理系统进展*中，卷33，页码1877–1901。Curran
    Associates, Inc.
- en: 'Chi et al. (2021) Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal,
    Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. [InfoXLM:
    An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training](https://doi.org/10.18653/v1/2021.naacl-main.280).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3576–3588,
    Online. Association for Computational Linguistics.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chi等（2021）Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang,
    Xia Song, Xian-Ling Mao, Heyan Huang, 和 Ming Zhou. 2021. [InfoXLM：一种信息论框架用于跨语言模型预训练](https://doi.org/10.18653/v1/2021.naacl-main.280)。在*2021年北美计算语言学协会会议：人类语言技术*论文集中，页码3576–3588，在线。计算语言学协会。
- en: 'Dankers et al. (2022) Verna Dankers, Christopher Lucas, and Ivan Titov. 2022.
    [Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine
    Translation](https://doi.org/10.18653/v1/2022.acl-long.252). In *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 3608–3626, Dublin, Ireland. Association for Computational
    Linguistics.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dankers等（2022）Verna Dankers, Christopher Lucas, 和 Ivan Titov. 2022. [变压器会不会过于组合？分析神经机器翻译中的习语处理](https://doi.org/10.18653/v1/2022.acl-long.252)。在*第60届计算语言学协会年会（卷1：长篇论文）论文集*中，页码3608–3626，都柏林，爱尔兰。计算语言学协会。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等（2023）Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    2023. Qlora: 高效微调量化大语言模型。*arXiv预印本 arXiv:2305.14314*。'
- en: 'Fadaee et al. (2018) Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2018.
    [Examining the tip of the iceberg: A data set for idiom translation](https://aclanthology.org/L18-1148).
    In *Proceedings of the eleventh international conference on language resources
    and evaluation (LREC 2018)*, Miyazaki, Japan. European Language Resources Association
    (ELRA).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fadaee等（2018）Marzieh Fadaee, Arianna Bisazza, 和 Christof Monz. 2018. [冰山一角：一个用于习语翻译的数据集](https://aclanthology.org/L18-1148)。在*第十一届语言资源与评估国际会议（LREC
    2018）论文集*中，宫崎，日本。欧洲语言资源协会（ELRA）。
- en: 'Freitag et al. (2021) Markus Freitag, George Foster, David Grangier, Viresh
    Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. [Experts, errors, and context:
    A large-scale study of human evaluation for machine translation](https://doi.org/10.1162/tacl_a_00437).
    *Transactions of the Association for Computational Linguistics*, 9:1460–1474.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag等（2021）Markus Freitag, George Foster, David Grangier, Viresh Ratnakar,
    Qijun Tan, 和 Wolfgang Macherey. 2021. [专家、错误与上下文：大规模机器翻译人工评估研究](https://doi.org/10.1162/tacl_a_00437)。*计算语言学协会会刊*，9:1460–1474。
- en: 'Freitag et al. (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
    Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and
    André F. T. Martins. 2022. [Results of WMT22 metrics shared task: Stop using BLEU
    – neural metrics are better and more robust](https://aclanthology.org/2022.wmt-1.2).
    In *Proceedings of the Seventh Conference on Machine Translation (WMT)*, pages
    46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational
    Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag 等人（2022）Markus Freitag、Ricardo Rei、Nitika Mathur、Chi-kiu Lo、Craig Stewart、Eleftherios
    Avramidis、Tom Kocmi、George Foster、Alon Lavie 和 André F. T. Martins。2022年。[WMT22指标共享任务结果：停止使用BLEU——神经指标更好且更稳健](https://aclanthology.org/2022.wmt-1.2)。载于
    *第七届机器翻译会议（WMT）论文集*，第46–68页，阿布扎比，阿联酋（混合模式）。计算语言学协会。
- en: Garcia et al. (2023) Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster,
    Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023. [The unreasonable
    effectiveness of few-shot learning for machine translation](http://arxiv.org/abs/2302.01398).
    ArXiv:2302.01398 [cs].
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia 等人（2023）Xavier Garcia、Yamini Bansal、Colin Cherry、George Foster、Maxim
    Krikun、Fangxiaoyu Feng、Melvin Johnson 和 Orhan Firat。2023年。[少样本学习在机器翻译中的不合理有效性](http://arxiv.org/abs/2302.01398)。ArXiv:2302.01398
    [cs]。
- en: 'Haagsma et al. (2020) Hessel Haagsma, Johan Bos, and Malvina Nissim. 2020.
    [MAGPIE: A large corpus of potentially idiomatic expressions](https://aclanthology.org/2020.lrec-1.35).
    In *Proceedings of the twelfth language resources and evaluation conference*,
    pages 279–287, Marseille, France. European Language Resources Association.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Haagsma 等人（2020）Hessel Haagsma、Johan Bos 和 Malvina Nissim。2020年。[MAGPIE: 一大语料库的潜在习语表达](https://aclanthology.org/2020.lrec-1.35)。载于
    *第十二届语言资源与评估会议论文集*，第279–287页，法国马赛。欧洲语言资源协会。'
- en: 'Jiao et al. (2023) Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming
    Shi, and Zhaopeng Tu. 2023. [ParroT: Translating During Chat Using Large Language
    Models](http://arxiv.org/abs/2304.02426). ArXiv:2304.02426 [cs].'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao 等人（2023）Wenxiang Jiao、Jen-tse Huang、Wenxuan Wang、Xing Wang、Shuming Shi
    和 Zhaopeng Tu。2023年。[ParroT：使用大型语言模型进行聊天翻译](http://arxiv.org/abs/2304.02426)。ArXiv:2304.02426
    [cs]。
- en: 'Johnson et al. (2019) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
    Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547.
    Publisher: IEEE.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人（2019）Jeff Johnson、Matthijs Douze 和 Hervé Jégou。2019年。使用GPU进行亿规模相似性搜索。*IEEE
    大数据学报*，7(3)：535–547。出版商：IEEE。
- en: Karpinska and Iyyer (2023) Marzena Karpinska and Mohit Iyyer. 2023. [Large language
    models effectively leverage document-level context for literary translation, but
    critical errors persist](http://arxiv.org/abs/2304.03245). ArXiv:2304.03245 [cs].
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpinska 和 Iyyer（2023）Marzena Karpinska 和 Mohit Iyyer。2023年。[大型语言模型有效利用文档级上下文进行文学翻译，但仍存在关键错误](http://arxiv.org/abs/2304.03245)。ArXiv:2304.03245
    [cs]。
- en: 'Kocmi et al. (2023) Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej
    Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme
    Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof
    Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin
    Popel, Maja Popović, and Mariya Shmatova. 2023. [Findings of the 2023 Conference
    on Machine Translation (WMT23): LLMs Are Here but Not Quite There Yet](https://doi.org/10.18653/v1/2023.wmt-1.1).
    In *Proceedings of the Eighth Conference on Machine Translation*, pages 1–42,
    Singapore. Association for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocmi 等人（2023）Tom Kocmi、Eleftherios Avramidis、Rachel Bawden、Ondřej Bojar、Anton
    Dvorkovich、Christian Federmann、Mark Fishel、Markus Freitag、Thamme Gowda、Roman Grundkiewicz、Barry
    Haddow、Philipp Koehn、Benjamin Marie、Christof Monz、Makoto Morishita、Kenton Murray、Makoto
    Nagata、Toshiaki Nakazawa、Martin Popel、Maja Popović 和 Mariya Shmatova。2023年。[2023年机器翻译会议（WMT23）发现：大型语言模型已到但尚未完全到位](https://doi.org/10.18653/v1/2023.wmt-1.1)。载于
    *第八届机器翻译会议论文集*，第1–42页，新加坡。计算语言学协会。
- en: Kocmi et al. (2022) Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich,
    Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz,
    Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita,
    Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović.
    2022. [Findings of the 2022 Conference on Machine Translation (WMT22)](https://aclanthology.org/2022.wmt-1.1).
    In *Proceedings of the Seventh Conference on Machine Translation (WMT)*, pages
    1–45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational
    Linguistics.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocmi等（2022） Tom Kocmi、Rachel Bawden、Ondřej Bojar、Anton Dvorkovich、Christian
    Federmann、Mark Fishel、Thamme Gowda、Yvette Graham、Roman Grundkiewicz、Barry Haddow、Rebecca
    Knowles、Philipp Koehn、Christof Monz、Makoto Morishita、Masaaki Nagata、Toshiaki Nakazawa、Michal
    Novák、Martin Popel 和 Maja Popović。2022。 [2022年机器翻译会议（WMT22）成果](https://aclanthology.org/2022.wmt-1.1)。在
    *第七届机器翻译会议论文集（WMT）*，页码 1–45，阿布扎比，阿拉伯联合酋长国（混合）。计算语言学协会。
- en: 'Kocmi et al. (2021) Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin
    Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. [To ship or not
    to ship: An extensive evaluation of automatic metrics for machine translation](https://aclanthology.org/2021.wmt-1.57).
    In *Proceedings of the Sixth Conference on Machine Translation*, pages 478–494,
    Online. Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocmi等（2021） Tom Kocmi、Christian Federmann、Roman Grundkiewicz、Marcin Junczys-Dowmunt、Hitokazu
    Matsushita 和 Arul Menezes。2021。 [发货还是不发货：对机器翻译自动评估指标的广泛评估](https://aclanthology.org/2021.wmt-1.57)。在
    *第六届机器翻译会议论文集*，页码 478–494，在线。计算语言学协会。
- en: Li et al. (2023) Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun
    Chen. 2023. [Eliciting the Translation Ability of Large Language Models via Multilingual
    Finetuning with Translation Instructions](http://arxiv.org/abs/2305.15083). ArXiv:2305.15083
    [cs].
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023） Jiahuan Li、Hao Zhou、Shujian Huang、Shanbo Cheng 和 Jiajun Chen。2023。
    [通过翻译指令的多语言微调引出大型语言模型的翻译能力](http://arxiv.org/abs/2305.15083)。ArXiv:2305.15083
    [cs]。
- en: Luo et al. (2023) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue
    Zhang. 2023. [An Empirical Study of Catastrophic Forgetting in Large Language
    Models During Continual Fine-tuning](https://doi.org/10.48550/arXiv.2308.08747).
    ArXiv:2308.08747 [cs].
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等（2023） Yun Luo、Zhen Yang、Fandong Meng、Yafu Li、Jie Zhou 和 Yue Zhang。2023。
    [大型语言模型在持续微调过程中灾难性遗忘的实证研究](https://doi.org/10.48550/arXiv.2308.08747)。ArXiv:2308.08747
    [cs]。
- en: 'Mathur et al. (2020) Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.
    [Tangled up in BLEU: reevaluating the evaluation of automatic machine translation
    evaluation metrics](http://arxiv.org/abs/2006.06264). In *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*, Seattle, Washington.
    ArXiv: 2006.06264.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mathur等（2020） Nitika Mathur、Timothy Baldwin 和 Trevor Cohn。2020。 [纠缠于BLEU：重新评估自动机器翻译评估指标的评估](http://arxiv.org/abs/2006.06264)。在
    *第58届计算语言学协会年会论文集*，西雅图，华盛顿。ArXiv: 2006.06264。'
- en: 'McCloskey and Cohen (1989) Michael McCloskey and Neal J Cohen. 1989. Catastrophic
    interference in connectionist networks: The sequential learning problem. In *Psychology
    of learning and motivation*, volume 24, pages 109–165\. Elsevier.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCloskey 和 Cohen（1989） Michael McCloskey 和 Neal J Cohen。1989。 连接主义网络中的灾难性干扰：顺序学习问题。
    在 *学习与动机心理学*，第24卷，页码 109–165。Elsevier。
- en: Moslem et al. (2023a) Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy
    Way. 2023a. [Adaptive Machine Translation with Large Language Models](http://arxiv.org/abs/2301.13294).
    ArXiv:2301.13294 [cs].
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moslem等（2023a） Yasmin Moslem、Rejwanul Haque、John D. Kelleher 和 Andy Way。2023a。
    [大型语言模型的自适应机器翻译](http://arxiv.org/abs/2301.13294)。ArXiv:2301.13294 [cs]。
- en: Moslem et al. (2023b) Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023b. [Fine-tuning
    Large Language Models for Adaptive Machine Translation](http://arxiv.org/abs/2312.12740).
    ArXiv:2312.12740 [cs].
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moslem等（2023b） Yasmin Moslem、Rejwanul Haque 和 Andy Way。2023b。 [调整大型语言模型以适应机器翻译](http://arxiv.org/abs/2312.12740)。ArXiv:2312.12740
    [cs]。
- en: 'Nadejde et al. (2022) Maria Nadejde, Anna Currey, Benjamin Hsu, Xing Niu, Marcello
    Federico, and Georgiana Dinu. 2022. [CoCoA-MT: A Dataset and Benchmark for Contrastive
    Controlled MT with Application to Formality](https://doi.org/10.18653/v1/2022.findings-naacl.47).
    In *Findings of the Association for Computational Linguistics: NAACL 2022*, pages
    616–632, Seattle, United States. Association for Computational Linguistics.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nadejde等（2022） Maria Nadejde、Anna Currey、Benjamin Hsu、Xing Niu、Marcello Federico
    和 Georgiana Dinu。2022。 [CoCoA-MT：对比控制机器翻译的数据集和基准，应用于形式性](https://doi.org/10.18653/v1/2022.findings-naacl.47)。在
    *计算语言学协会发现：NAACL 2022*，页码 616–632，西雅图，美国。计算语言学协会。
- en: Neubig (2023) Graham Neubig. 2023. [Zeno GPT-MT Report](https://github.com/zeno-ml/zeno-build/tree/main/examples/analysis_gpt_mt/report).
    Technical report.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neubig (2023) Graham Neubig。2023年。[Zeno GPT-MT 报告](https://github.com/zeno-ml/zeno-build/tree/main/examples/analysis_gpt_mt/report)。技术报告。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [BLEU: a method for automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135).
    In *Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics*, page 311, Philadelphia, Pennsylvania. Association for Computational
    Linguistics.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni 等 (2002) Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu。2002年。[BLEU：一种机器翻译自动评估方法](https://doi.org/10.3115/1073083.1073135)。在*第40届计算语言学协会年会*，第311页，宾夕法尼亚州费城。计算语言学协会。
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*, pages 3505–3506.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasley 等 (2020) Jeff Rasley、Samyam Rajbhandari、Olatunji Ruwase 和 Yuxiong He。2020年。Deepspeed：系统优化使得训练超过1000亿参数的深度学习模型成为可能。在*第26届ACM
    SIGKDD国际知识发现与数据挖掘大会*，第3505–3506页。
- en: 'Ratcliff (1990) Roger Ratcliff. 1990. Connectionist models of recognition memory:
    constraints imposed by learning and forgetting functions. *Psychological review*,
    97(2):285. Publisher: American Psychological Association.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratcliff (1990) Roger Ratcliff。1990年。识别记忆的连接主义模型：由学习和遗忘函数施加的约束。*心理学评论*，97(2)：285。出版商：美国心理学协会。
- en: 'Raunak et al. (2023) Vikas Raunak, Arul Menezes, Matt Post, and Hany Hassan
    Awadalla. 2023. [Do GPTs Produce Less Literal Translations?](https://aclanthology.org/2023.acl-short.90)
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 1041–1050, Toronto, Canada. Association
    for Computational Linguistics. ArXiv:2305.16806 [cs].'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raunak 等 (2023) Vikas Raunak、Arul Menezes、Matt Post 和 Hany Hassan Awadalla。2023年。[GPTs
    是否产生较少的字面翻译？](https://aclanthology.org/2023.acl-short.90) 在*第61届计算语言学协会年会（第2卷：短篇论文）*，第1041–1050页，多伦多，加拿大。计算语言学协会。ArXiv:2305.16806
    [cs]。
- en: 'Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.
    2020. [COMET: A Neural Framework for MT Evaluation](https://doi.org/10.18653/v1/2020.emnlp-main.213).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 2685–2702, Online. Association for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rei 等 (2020) Ricardo Rei、Craig Stewart、Ana C Farinha 和 Alon Lavie。2020年。[COMET：用于机器翻译评估的神经框架](https://doi.org/10.18653/v1/2020.emnlp-main.213)。在*2020年自然语言处理经验方法会议（EMNLP）*，第2685–2702页，在线。计算语言学协会。
- en: 'Rei et al. (2022) Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula
    Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova,
    Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. [CometKiwi:
    IST-Unbabel 2022 Submission for the Quality Estimation Shared Task](https://aclanthology.org/2022.wmt-1.60).
    In *Proceedings of the Seventh Conference on Machine Translation (WMT)*, pages
    634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational
    Linguistics.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rei 等 (2022) Ricardo Rei、Marcos Treviso、Nuno M. Guerreiro、Chrysoula Zerva、Ana
    C Farinha、Christine Maroti、José G. C. de Souza、Taisiya Glushkova、Duarte Alves、Luisa
    Coheur、Alon Lavie 和 André F. T. Martins。2022年。[CometKiwi：IST-Unbabel 2022 年度质量估计共享任务提交](https://aclanthology.org/2022.wmt-1.60)。在*第七届机器翻译会议（WMT）*，第634–645页，阿布扎比，阿联酋（混合形式）。计算语言学协会。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks](https://doi.org/10.18653/v1/D19-1410).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reimers 和 Gurevych (2019) Nils Reimers 和 Iryna Gurevych。2019年。[Sentence-BERT:
    使用Siamese BERT网络的句子嵌入](https://doi.org/10.18653/v1/D19-1410)。在*2019年自然语言处理经验方法会议及第九届国际联合自然语言处理会议（EMNLP-IJCNLP）*，第3982–3992页，香港，中国。计算语言学协会。'
- en: 'Riley et al. (2023) Parker Riley, Timothy Dozat, Jan A. Botha, Xavier Garcia,
    Dan Garrette, Jason Riesa, Orhan Firat, and Noah Constant. 2023. [FRMT: A Benchmark
    for Few-Shot Region-Aware Machine Translation](https://doi.org/10.1162/tacl_a_00568).
    *Transactions of the Association for Computational Linguistics*, 11:671–685.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Riley 等 (2023) Parker Riley, Timothy Dozat, Jan A. Botha, Xavier Garcia, Dan
    Garrette, Jason Riesa, Orhan Firat 和 Noah Constant. 2023. [FRMT: A Benchmark for
    Few-Shot Region-Aware Machine Translation](https://doi.org/10.1162/tacl_a_00568)。*计算语言学协会会刊*，11:671–685。'
- en: Rippeth et al. (2022) Elijah Rippeth, Sweta Agrawal, and Marine Carpuat. 2022.
    [Controlling Translation Formality Using Pre-trained Multilingual Language Models](https://doi.org/10.18653/v1/2022.iwslt-1.30).
    In *Proceedings of the 19th International Conference on Spoken Language Translation
    (IWSLT 2022)*, pages 327–340, Dublin, Ireland (in-person and online). Association
    for Computational Linguistics.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rippeth 等 (2022) Elijah Rippeth, Sweta Agrawal 和 Marine Carpuat. 2022. [Controlling
    Translation Formality Using Pre-trained Multilingual Language Models](https://doi.org/10.18653/v1/2022.iwslt-1.30)。发表于
    *第19届口语语言翻译国际会议 (IWSLT 2022)*，第327–340页，都柏林，爱尔兰（现场和在线）。计算语言学协会。
- en: 'Robinson et al. (2023) Nathaniel Robinson, Perez Ogayo, David R. Mortensen,
    and Graham Neubig. 2023. [ChatGPT MT: Competitive for High- (but Not Low-) Resource
    Languages](https://doi.org/10.18653/v1/2023.wmt-1.40). In *Proceedings of the
    Eighth Conference on Machine Translation*, pages 392–418, Singapore. Association
    for Computational Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Robinson 等 (2023) Nathaniel Robinson, Perez Ogayo, David R. Mortensen 和 Graham
    Neubig. 2023. [ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages](https://doi.org/10.18653/v1/2023.wmt-1.40)。发表于
    *第八届机器翻译会议论文集*，第392–418页，新加坡。计算语言学协会。'
- en: Stap and Araabi (2023) David Stap and Ali Araabi. 2023. [ChatGPT is not a good
    indigenous translator](https://doi.org/10.18653/v1/2023.americasnlp-1.17). In
    *Proceedings of the Workshop on Natural Language Processing for Indigenous Languages
    of the Americas (AmericasNLP)*, pages 163–167, Toronto, Canada. Association for
    Computational Linguistics.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stap 和 Araabi (2023) David Stap 和 Ali Araabi. 2023. [ChatGPT is not a good indigenous
    translator](https://doi.org/10.18653/v1/2023.americasnlp-1.17)。发表于 *美洲土著语言自然语言处理研讨会论文集
    (AmericasNLP)*，第163–167页，多伦多，加拿大。计算语言学协会。
- en: 'Tang (2022) Kenan Tang. 2022. [PETCI: A Parallel English Translation Dataset
    of Chinese Idioms](http://arxiv.org/abs/2202.09509). ArXiv:2202.09509 [cs].'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang (2022) Kenan Tang. 2022. [PETCI: A Parallel English Translation Dataset
    of Chinese Idioms](http://arxiv.org/abs/2202.09509)。ArXiv:2202.09509 [cs]。'
- en: Tiedemann (2012) Jörg Tiedemann. 2012. Parallel data, tools and interfaces in
    OPUS. In *Proceedings of the Eight International Conference on Language Resources
    and Evaluation*, pages 2214–2218, Istanbul, Turkey. European Language Resources
    Association (ELRA).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiedemann (2012) Jörg Tiedemann. 2012. OPUS 中的并行数据、工具和接口。发表于 *第八届语言资源与评估国际会议论文集*，第2214–2218页，伊斯坦布尔，土耳其。欧洲语言资源协会
    (ELRA)。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/abs/2302.13971).
    ArXiv:2302.13971 [cs].'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample.
    2023. [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/abs/2302.13971)。ArXiv:2302.13971
    [cs]。'
- en: 'Vilar et al. (2023) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
    Viresh Ratnakar, and George Foster. 2023. [Prompting PaLM for Translation: Assessing
    Strategies and Performance](https://aclanthology.org/2023.acl-long.859). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 15406–15427, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vilar 等 (2023) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh
    Ratnakar 和 George Foster. 2023. [Prompting PaLM for Translation: Assessing Strategies
    and Performance](https://aclanthology.org/2023.acl-long.859)。发表于 *第61届计算语言学协会年会论文集
    (第1卷: 长篇论文)*，第15406–15427页，多伦多，加拿大。计算语言学协会。'
- en: Wang et al. (2023) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian
    Yu, Shuming Shi, and Zhaopeng Tu. 2023. [Document-Level Machine Translation with
    Large Language Models](http://arxiv.org/abs/2304.02210). ArXiv:2304.02210 [cs].
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu,
    Shuming Shi 和 Zhaopeng Tu. 2023. [Document-Level Machine Translation with Large
    Language Models](http://arxiv.org/abs/2304.02210)。ArXiv:2304.02210 [cs]。
- en: 'Wei et al. (2023) Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang,
    Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie
    Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, and Jun Xie.
    2023. [PolyLM: An Open Source Polyglot Large Language Model](http://arxiv.org/abs/2307.06018).
    ArXiv:2307.06018 [cs].'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2023) Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang
    Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan
    Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang 和 Jun Xie. 2023. [PolyLM：一个开源的多语言大型语言模型](http://arxiv.org/abs/2307.06018).
    ArXiv:2307.06018 [cs]。
- en: Wicks and Post (2023) Rachel Wicks and Matt Post. 2023. [Identifying Context-Dependent
    Translations for Evaluation Set Production](http://arxiv.org/abs/2311.02321).
    ArXiv:2311.02321 [cs].
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicks 和 Post (2023) Rachel Wicks 和 Matt Post. 2023. [识别用于评估集生产的上下文依赖翻译](http://arxiv.org/abs/2311.02321).
    ArXiv:2311.02321 [cs]。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick Von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander Rush. 2020. [Transformers: State-of-the-Art Natural Language Processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online. Association for Computational
    Linguistics.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等 (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick Von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest 和
    Alexander Rush. 2020. [Transformers：最先进的自然语言处理](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    载于 *2020年自然语言处理实证方法会议：系统演示*，页38–45，在线。计算语言学协会。
- en: 'Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.
    2023. [A Paradigm Shift in Machine Translation: Boosting Translation Performance
    of Large Language Models](http://arxiv.org/abs/2309.11674). ArXiv:2309.11674 [cs].'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2023) Haoran Xu, Young Jin Kim, Amr Sharaf 和 Hany Hassan Awadalla. 2023.
    [机器翻译中的范式转变：提升大型语言模型的翻译性能](http://arxiv.org/abs/2309.11674). ArXiv:2309.11674
    [cs]。
- en: 'Yang et al. (2023) Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023.
    [BigTranslate: Augmenting Large Language Models with Multilingual Translation
    Capability over 100 Languages](http://arxiv.org/abs/2305.18098). ArXiv:2305.18098
    [cs].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023) Wen Yang, Chong Li, Jiajun Zhang 和 Chengqing Zong. 2023. [BigTranslate：增强大型语言模型的多语言翻译能力，涵盖100多种语言](http://arxiv.org/abs/2305.18098).
    ArXiv:2305.18098 [cs]。
- en: Zaninello and Birch (2020) Andrea Zaninello and Alexandra Birch. 2020. [Multiword
    expression aware neural machine translation](https://aclanthology.org/2020.lrec-1.471).
    In *Proceedings of the Twelfth Language Resources and Evaluation Conference*,
    pages 3816–3825, Marseille, France. European Language Resources Association.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaninello 和 Birch (2020) Andrea Zaninello 和 Alexandra Birch. 2020. [多词表达感知的神经机器翻译](https://aclanthology.org/2020.lrec-1.471).
    载于 *第十二届语言资源与评估会议论文集*，页3816–3825，法国马赛。欧洲语言资源协会。
- en: 'Zeng et al. (2023) Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2023.
    [TIM: Teaching Large Language Models to Translate with Comparison](http://arxiv.org/abs/2307.04408).
    ArXiv:2307.04408 [cs].'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2023) Jiali Zeng, Fandong Meng, Yongjing Yin 和 Jie Zhou. 2023. [TIM：教大型语言模型通过对比翻译](http://arxiv.org/abs/2307.04408).
    ArXiv:2307.04408 [cs]。
- en: 'Zhang et al. (2023) Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn.
    2023. Machine Translation with Large Language Models: Prompting, Few-shot Learning,
    and Fine-tuning with QLoRA. In *WMT*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023) Xuan Zhang, Navid Rajabi, Kevin Duh 和 Philipp Koehn. 2023. 大型语言模型的机器翻译：提示、少样本学习与
    QLoRA 的微调。载于 *WMT*。
- en: 'Zhu et al. (2023a) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian
    Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023a. [Multilingual Machine Translation
    with Large Language Models: Empirical Results and Analysis](http://arxiv.org/abs/2304.04675).
    ArXiv:2304.04675 [cs].'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2023a) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang,
    Lingpeng Kong, Jiajun Chen 和 Lei Li. 2023a. [多语言机器翻译与大型语言模型：经验结果与分析](http://arxiv.org/abs/2304.04675).
    ArXiv:2304.04675 [cs]。
- en: Zhu et al. (2023b) Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu,
    Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023b. [Extrapolating Large
    Language Models to Non-English by Aligning Languages](http://arxiv.org/abs/2308.04948).
    ArXiv:2308.04948 [cs].
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2023b) Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian
    Huang, Lingpeng Kong, Jiajun Chen 和 Lei Li. 2023b. [通过对齐语言将大型语言模型推广到非英语](http://arxiv.org/abs/2308.04948).
    ArXiv:2308.04948 [cs]。
- en: Appendix A Details on experimental setup
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验设置的详细信息
- en: We run our fine-tuning experiments with the Hugging Face transformers library
    (Wolf et al., [2020](#bib.bib51)) and make use of DeepSpeed (Rasley et al., [2020](#bib.bib34)).
    We store intermediate checkpoints during fine-tuning to track how abilities evaluate
    over time. We perform full fine-tuning on models up to 40B. We use the AdamW optimizer
    with a cosine learning rate scheduler and 3% warm-up percentage. We empirically
    set the batch size to 128, learning rate to $2e-5$, and train for one epoch. During
    inference we use beam search with a beam size of 4. For LLaMA-65B, we fine-tune
    with QLoRA (Dettmers et al., [2023](#bib.bib13)), using 8-bit quantization. Following
    Zhang et al. ([2023](#bib.bib56)) we set the low-rank approximation to 64 and
    the scaling factor for low-rank adaptation to 32.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Hugging Face transformers 库（Wolf 等，[2020](#bib.bib51)）进行微调实验，并利用 DeepSpeed（Rasley
    等，[2020](#bib.bib34)）。我们在微调过程中存储中间检查点，以跟踪能力随时间的变化。我们对最多 40B 的模型进行全面微调。我们使用 AdamW
    优化器，配合余弦学习率调度器和 3% 的预热百分比。我们经验性地将批量大小设置为 128，学习率设置为 $2e-5$，并训练一个周期。在推理过程中，我们使用
    beam search，beam 大小为 4。对于 LLaMA-65B，我们使用 QLoRA（Dettmers 等，[2023](#bib.bib13)）进行微调，并使用
    8 位量化。根据 Zhang 等（[2023](#bib.bib56)），我们将低秩近似设置为 64，将低秩适应的缩放因子设置为 32。
- en: Inference
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理
- en: 'Depending on the evaluation set, we use a 0-shot or few-shot approach. The
    prompt used for fine-tuning and 0-shot is shown in Table [2](#A1.T2 "Table 2 ‣
    Inference ‣ Appendix A Details on experimental setup ‣ The Fine-Tuning Paradox:
    Boosting Translation Quality Without Sacrificing LLM Abilities"). For our few-shot
    setup, we find the 5 most similar source sentences and their corresponding target
    sentences from a corresponding train set (if available) or validation set. The
    resulting prompt is displayed in Table [3](#A1.T3 "Table 3 ‣ Inference ‣ Appendix
    A Details on experimental setup ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities"). We use Sentence-BERT (Reimers and
    Gurevych, [2019](#bib.bib39)) for encoding⁶⁶6We use all-MiniLM-L6-v2 for English
    sentences and paraphrase-multilingual-MiniLM-L12-v2 for non-English and FAISS
    (Johnson et al., [2019](#bib.bib20)) for searching similar sentences.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据评估集的不同，我们使用 0-shot 或 few-shot 方法。用于微调和 0-shot 的提示见表格 [2](#A1.T2 "表格 2 ‣ 推理
    ‣ 附录 A 实验设置详情 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")。对于我们的 few-shot 设置，我们从相应的训练集（如有）或验证集中找到
    5 个最相似的源句子及其对应的目标句子。生成的提示见表格 [3](#A1.T3 "表格 3 ‣ 推理 ‣ 附录 A 实验设置详情 ‣ 微调悖论：提升翻译质量而不牺牲
    LLM 能力")。我们使用 Sentence-BERT（Reimers 和 Gurevych，[2019](#bib.bib39)）进行编码⁶⁶6。我们使用
    all-MiniLM-L6-v2 处理英文句子，使用 paraphrase-multilingual-MiniLM-L12-v2 处理非英文句子，并使用 FAISS（Johnson
    等，[2019](#bib.bib20)）进行相似句子的检索。
- en: '| Translate this from {source_language} to {target_language}: |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 将此从 {source_language} 翻译为 {target_language}: |'
- en: '| {source_language}: {source_sentence} |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| {source_language}: {source_sentence} |'
- en: '| {target_language}: {target_sentence} |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| {target_language}: {target_sentence} |'
- en: 'Table 2: Prompting template for fine-tuning and 0-shot inference. For fine-tuning
    {target_sentence} is filled with the corresponding target sentence, and for 0-shot
    inference it is the empty string.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：微调和 0-shot 推理的提示模板。对于微调，{target_sentence} 填入相应的目标句子；对于 0-shot 推理，它为空字符串。
- en: '| Translate this from {source_language} to {target_language}: |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 将此从 {source_language} 翻译为 {target_language}: |'
- en: '| {source_language}: {source_sentence[1]} |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| {source_language}: {source_sentence[1]} |'
- en: '| {target_language}: {target_sentence[1]} |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| {target_language}: {target_sentence[1]} |'
- en: '| ... |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ... |'
- en: '| {source_language}: {source_sentence[n]} |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| {source_language}: {source_sentence[n]} |'
- en: '| {target_language}: |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| {target_language}: |'
- en: 'Table 3: Prompting template for few-shot inference.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：few-shot 推理的提示模板。
- en: Appendix B Additional results
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外结果
- en: B.1 General translation quality (WMT22)
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 一般翻译质量（WMT22）
- en: 'Results on WMT22 for models trained on filtered web-crawled data are summarized
    in Figure [7](#A2.F7 "Figure 7 ‣ B.1 General translation quality (WMT22) ‣ Appendix
    B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without
    Sacrificing LLM Abilities"). Note that the data size up until 89K has relatively
    small increments, and later data sizes are doubled compared to the data point
    before it. Generally, in contrast to training on human-written data, translation
    quality continues to increase when adding more data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 WMT22 上针对过滤后的网络爬取数据进行训练的模型的结果总结见图 [7](#A2.F7 "图 7 ‣ B.1 一般翻译质量（WMT22） ‣ 附录
    B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")。注意，直到 89K 的数据量增加较小，之后的数据量相对于前一个数据点翻倍。一般来说，与在人工编写的数据上训练相比，当增加更多数据时，翻译质量继续提高。
- en: 'We compare the best checkpoints of models trained on human-written and OPUS
    data in Table [4](#A2.T4 "Table 4 ‣ B.1 General translation quality (WMT22) ‣
    Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities"). In 15 out of 18 cases, models fine-tuned
    on the larger OPUS dataset results in better scores on WMT22.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[4](#A2.T4 "表 4 ‣ B.1 一般翻译质量 (WMT22) ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")中比较了在人工编写数据和
    OPUS 数据上训练的模型的最佳检查点。在18个案例中的15个中，基于更大 OPUS 数据集微调的模型在 WMT22 上得分更高。
- en: '![Refer to caption](img/c3320701ca4b8ea18340da3bf37dfe2d.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c3320701ca4b8ea18340da3bf37dfe2d.png)'
- en: 'Figure 7: X$\rightarrow$X (bottom) COMET scores on WMT22 for different models
    trained on OPUS parallel data with different amounts of training data.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: X$\rightarrow$X (底部) 在 WMT22 上的 COMET 分数，针对不同的数据量在 OPUS 平行数据上训练的不同模型。'
- en: '|  | de-en | ru-en | zh-en |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | de-en | ru-en | zh-en |'
- en: '|  | WMT | OPUS | WMT | OPUS | WMT | OPUS |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | WMT | OPUS | WMT | OPUS | WMT | OPUS |'
- en: '| LLaMA-7b | $0.834$ (1.4M) | $0.749$ (1.4M) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | $0.834$ (1.4M) | $0.749$ (1.4M) |'
- en: '| LLaMA-13b | $0.842$ (1.4M) | $\underline{0.765}$ (1.4M) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13b | $0.842$ (1.4M) | $\underline{0.765}$ (1.4M) |'
- en: '| LLaMA-30b | $0.844$ (1.4M) | $0.772$ (1.4M) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30b | $0.844$ (1.4M) | $0.772$ (1.4M) |'
- en: '|  | en-de | en-ru | en-zh |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | en-de | en-ru | en-zh |'
- en: '|  | WMT | OPUS | WMT | OPUS | WMT | OPUS |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | WMT | OPUS | WMT | OPUS | WMT | OPUS |'
- en: '| LLaMA-7b | $0.831$ (1.4M) | $0.797$ (1.4M) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | $0.831$ (1.4M) | $0.797$ (1.4M) |'
- en: '| LLaMA-13b | $0.843$ (1.4M) | $0.818$ (1.4M) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13b | $0.843$ (1.4M) | $0.818$ (1.4M) |'
- en: '| LLaMA-30b | $0.848$ (1.4M) | $0.826$ (1.4M) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30b | $0.848$ (1.4M) | $0.826$ (1.4M) |'
- en: 'Table 4: WMT22 COMET scores comparing models fine-tuned on human-written data
    (WMT) and filtered web-crawled data (OPUS). Parentheses indicate the number of
    fine-tuning examples seen by the best-performing checkpoints. Best scores are
    underlined.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: WMT22 COMET 分数比较了基于人工编写数据 (WMT) 和筛选的网络爬取数据 (OPUS) 微调的模型。括号中表示最佳表现检查点看到的微调示例的数量。最佳分数用下划线标出。'
- en: B.2 Formality steering
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 形式性引导
- en: 'Figure [8](#A2.F8 "Figure 8 ‣ B.2 Formality steering ‣ Appendix B Additional
    results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") shows COMET scores for models trained on WMT data. COMET scores
    stay relatively constant, in contrast to the accuracy of formality forms.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](#A2.F8 "图 8 ‣ B.2 形式性引导 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")展示了在 WMT
    数据上训练的模型的 COMET 分数。COMET 分数保持相对稳定，与形式性的准确性形成对比。
- en: 'Figure [9](#A2.F9 "Figure 9 ‣ B.2 Formality steering ‣ Appendix B Additional
    results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") shows that the ability to generate correct formality markers also
    degrades when fine-tuning on OPUS data. Extended fine-tuning continues to show
    benefits in terms of general translation quality (see Figure [7](#A2.F7 "Figure
    7 ‣ B.1 General translation quality (WMT22) ‣ Appendix B Additional results ‣
    The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM
    Abilities")), but the ability to generate correct formality markers continues
    to degrade.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#A2.F9 "图 9 ‣ B.2 形式性引导 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力")表明，当在 OPUS
    数据上进行微调时，生成正确形式性标记的能力也会下降。尽管扩展微调在总体翻译质量上继续显示出好处 (见图[7](#A2.F7 "图 7 ‣ B.1 一般翻译质量
    (WMT22) ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲 LLM 能力"))，但生成正确形式性标记的能力仍在下降。
- en: '![Refer to caption](img/11600cfe8f92b587dc33a5713debb1d8.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11600cfe8f92b587dc33a5713debb1d8.png)'
- en: 'Figure 8: COMET scores on CoCoA-MT for models trained on WMT data.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 针对在 WMT 数据上训练的模型，CoCoA-MT 的 COMET 分数。'
- en: '![Refer to caption](img/f3fa6cafd165fe9d543ed5b95fb6e5d9.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3fa6cafd165fe9d543ed5b95fb6e5d9.png)'
- en: 'Figure 9: Accuracy of formality markers for models trained on OPUS data.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 针对在 OPUS 数据上训练的模型，形式性标记的准确性。'
- en: B.3 Technical domains
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 技术领域
- en: 'Figure [10](#A2.F10 "Figure 10 ‣ B.3 Technical domains ‣ Appendix B Additional
    results ‣ The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing
    LLM Abilities") shows the outcome of fine-tuning on human-written data across
    all evaluated domains and language directions. We observe a consistent trend:
    fine-tuning impairs the few-shot technical translation capabilities, and generally
    further fine-tuning results in more degradations. The COMET scores correlate negatively
    with datastore size ($\rho=-0.27$), indicating that fine-tuning on more data results
    in larger degradations.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#A2.F10 "图 10 ‣ B.3 技术领域 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力") 显示了在所有评估领域和语言方向上，使用人类编写的数据进行微调的结果。我们观察到一个一致的趋势：微调会削弱少样本技术翻译能力，通常进一步微调会导致更多的降级。COMET分数与数据存储库大小负相关（$\rho=-0.27$），这表明在更多数据上进行微调会导致更大的降级。
- en: 'The effects of fine-tuning are also analyzed using filtered web-scraped data
    from OPUS, as shown in Figure [11](#A2.F11 "Figure 11 ‣ B.3 Technical domains
    ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities"). Similar to the previous findings,
    an increase in data volume for fine-tuning corresponds to performance degradations,
    evidenced by a negative correlation between COMET scores and datastore size ($\rho=-0.33$).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的效果也使用来自OPUS的过滤网络抓取数据进行分析，如图[11](#A2.F11 "图 11 ‣ B.3 技术领域 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")所示。与之前的发现类似，微调数据量的增加对应着性能的降级，COMET分数与数据存储库大小之间存在负相关（$\rho=-0.33$）。
- en: 'However, OPUS data reveals that these degradations manifest more gradually
    compared to the WMT dataset. This discrepancy is likely due to OPUS’s broader
    domain coverage, in contrast to the specialized news content of the human-curated
    WMT dataset. Notably, while fine-tuning on OPUS data leads to deterioration in
    technical domain translation accuracy when leveraging few-shot examples, it concurrently
    continues to enhance overall translation quality (Figure [7](#A2.F7 "Figure 7
    ‣ B.1 General translation quality (WMT22) ‣ Appendix B Additional results ‣ The
    Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities")),
    underscoring a nuanced impact of fine-tuning across different data types and translation
    tasks.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OPUS数据揭示了这些降级表现得比WMT数据集更为渐进。这种差异可能是由于OPUS覆盖的领域更广泛，而人类策划的WMT数据集则专注于新闻内容。值得注意的是，虽然在OPUS数据上进行微调会导致技术领域翻译准确性的恶化（尤其是在使用少样本示例时），但它同时也会提升整体翻译质量（图[7](#A2.F7
    "图 7 ‣ B.1 总体翻译质量（WMT22） ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力")），突显了微调对不同数据类型和翻译任务的复杂影响。
- en: '![Refer to caption](img/bcf23464e7964963f1ad508b991de7f4.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bcf23464e7964963f1ad508b991de7f4.png)'
- en: 'Figure 10: COMET on technical domains using 5-shot examples for models trained
    on human-written translations.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：使用5-shot示例在技术领域上的COMET分数，针对使用人类编写翻译的模型。
- en: '![Refer to caption](img/8ce2b582e01a7304445b4ecdcd00f767.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8ce2b582e01a7304445b4ecdcd00f767.png)'
- en: 'Figure 11: COMET on technical domains using 5-shot examples for models trained
    on OPUS data.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：使用5-shot示例在技术领域上的COMET分数，针对使用OPUS数据的模型。
- en: e
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: e
- en: B.4 Contextualization of document-level input
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 文档级输入的语境化
- en: 'Figure [12](#A2.F12 "Figure 12 ‣ B.4 Contextualization of document-level input
    ‣ Appendix B Additional results ‣ The Fine-Tuning Paradox: Boosting Translation
    Quality Without Sacrificing LLM Abilities") shows that the animacy contextualization
    accuracy of document-level input degrades for models fine-tuned on filtered web-crawled
    OPUS data. We observe a negative correlation between accuracy and fine-tuning
    dataset size ($\rho=-0.49$).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#A2.F12 "图 12 ‣ B.4 文档级输入的语境化 ‣ 附录 B 额外结果 ‣ 微调悖论：提升翻译质量而不牺牲LLM能力") 显示了在过滤的网络抓取OPUS数据上微调的模型，文档级输入的生动性语境化准确性降低。我们观察到准确性与微调数据集大小之间存在负相关（$\rho=-0.49$）。
- en: '![Refer to caption](img/a8498fbe4fa67a7431f5d480bbc8a167.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a8498fbe4fa67a7431f5d480bbc8a167.png)'
- en: 'Figure 12: Accuracy of animacy contextualization for German$\rightarrow$English
    for models fine-tuned with human-written translations.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：德语$\rightarrow$英语的生动性语境化准确性，针对使用人类编写翻译进行微调的模型。
