- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:36:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:36:27'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18380](https://ar5iv.labs.arxiv.org/html/2405.18380)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18380](https://ar5iv.labs.arxiv.org/html/2405.18380)
- en: Pengxiang Li¹,    Lu Yin^(2,3∗), Xiaowei Gao⁴, Shiwei Liu^(5†) ${}^{1}\,$Eindhoven
    University of Technology
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Pengxiang Li¹,    Lu Yin^(2,3∗), Xiaowei Gao⁴, Shiwei Liu^(5†) ${}^{1}\,$Eindhoven
    University of Technology
- en: ${}^{4}\,$University of Oxford Equal contribution. ^†Corresponding to Shiwei
    Liu, shiwei.liu@maths.ox.ac.uk.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{4}\,$牛津大学 同等贡献。 ^†对应作者：Shiwei Liu，shiwei.liu@maths.ox.ac.uk。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advancements in Large Language Models (LLMs) have revolutionized various
    natural language processing tasks. However, the substantial size of LLMs presents
    significant challenges in training or fine-tuning. While parameter-efficient approaches
    such as low-rank adaptation (LoRA) have gained popularity, they often compromise
    performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed
    Layerwise Sampled Low-Rank Projection (OwLore), a new memory-efficient fine-tuning
    approach, inspired by the layerwise outlier distribution of LLMs, which dynamically
    samples pre-trained layers to fine-tune instead of adding additional adaptors.
    We first interpret the outlier phenomenon through the lens of Heavy-Tailed Self-Regularization
    theory (HT-SR), discovering that layers with more outliers tend to be more heavy-tailed
    and consequently better trained. Inspired by this finding, OwLore strategically
    assigns higher sampling probabilities to layers with more outliers to better leverage
    the knowledge stored in pre-trained LLMs. To further mitigate the memory demands
    of fine-tuning, we integrate gradient low-rank projection into our approach, which
    facilitates each layer to be efficiently trained in a low-rank manner. By incorporating
    the efficient characteristics of low-rank and optimal layerwise sampling, OwLore
    significantly improves the memory-performance trade-off in LLM pruning. Our extensive
    experiments across various architectures, including LLaMa2, LLaMa3, and Mistral,
    demonstrate that OwLore consistently outperforms baseline approaches, including
    full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain
    on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable
    10% boost on MT-Bench, while being more memory efficient. OwLore allows us to
    fine-tune LLaMa2-7B with only 21GB of memory. Code is available at [https://github.com/pixeli99/OwLore](https://github.com/pixeli99/OwLore).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速进展彻底改变了各种自然语言处理任务。然而，LLMs 的庞大规模在训练或微调中带来了重大挑战。虽然低秩适配（LoRA）等参数高效的方法已经获得了广泛关注，但它们通常会牺牲性能，相比于全秩微调效果较差。本文提出了**Outlier-weighed
    Layerwise Sampled Low-Rank Projection (OwLore)**，一种新的内存高效微调方法，灵感来自 LLMs 的分层异常值分布，该方法动态地对预训练层进行抽样以进行微调，而不是添加额外的适配器。我们首先通过**Heavy-Tailed
    Self-Regularization**理论（HT-SR）来解释异常值现象，发现异常值更多的层往往更具有重尾特征，因此训练效果更好。受到这一发现的启发，OwLore
    战略性地将更高的抽样概率分配给异常值更多的层，以更好地利用预训练 LLMs 中存储的知识。为了进一步减少微调的内存需求，我们将梯度低秩投影集成到我们的方法中，使得每一层可以以低秩的方式高效地进行训练。通过结合低秩和最优分层抽样的高效特性，OwLore
    显著改善了 LLM 剪枝中的内存-性能权衡。我们在包括 LLaMa2、LLaMa3 和 Mistral 在内的各种架构上的广泛实验表明，OwLore 始终优于基准方法，包括全量微调。具体而言，它在**Commonsense
    Reasoning**基准上实现了高达 1.1% 的平均准确率提升，在 MMLU 上提升了 3.0%，在 MT-Bench 上则显著提升了 10%，同时在内存使用上也更加高效。OwLore
    允许我们以仅 21GB 的内存对 LLaMa2-7B 进行微调。代码可在 [https://github.com/pixeli99/OwLore](https://github.com/pixeli99/OwLore)
    获取。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapid advancements in artificial intelligence (AI) driven by Large Language
    Models (LLMs) have fundamentally transformed how people work and communicate.
    The impressive language capabilities of LLMs enable a single model to handle various
    tasks simultaneously, including but not limited to natural language understanding [[5](#bib.bib5),
    [48](#bib.bib48)], text generation [[21](#bib.bib21), [1](#bib.bib1)], machine
    translation [[19](#bib.bib19)], and programming [[46](#bib.bib46), [47](#bib.bib47)].
    However, the massive size of LLMs presents significant challenges for practical
    applications and deployment.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由大型语言模型（LLM）驱动的人工智能（AI）的迅速进步，已经从根本上改变了人们的工作和沟通方式。LLM令人印象深刻的语言能力使得单个模型能够同时处理各种任务，包括但不限于自然语言理解[[5](#bib.bib5),
    [48](#bib.bib48)]、文本生成[[21](#bib.bib21), [1](#bib.bib1)]、机器翻译[[19](#bib.bib19)]和编程[[46](#bib.bib46),
    [47](#bib.bib47)]。然而，LLM的庞大规模给实际应用和部署带来了重大挑战。
- en: '![Refer to caption](img/a75547aeb0d0bd3a8eff937c6b8171a7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a75547aeb0d0bd3a8eff937c6b8171a7.png)'
- en: 'Figure 1: The comparison among Full Fine-tuning, training with LoRA, and Owlore.
    Blue modules are frozen, while orange modules are activated. OwLore non-uniformly
    samples layers to fine-tune models with low-rank gradients.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Full Fine-tuning、使用LoRA训练和OwLore之间的比较。蓝色模块为冻结的，橙色模块为激活的。OwLore非均匀地抽样层来对模型进行低秩梯度的微调。
- en: To address these challenges, various parameter-efficient approaches have been
    proposed, including prompt tuning [[24](#bib.bib24), [30](#bib.bib30)], adaptors [[15](#bib.bib15),
    [12](#bib.bib12)], and low-rank adaptation (LoRA) [[16](#bib.bib16), [9](#bib.bib9)].
    These approaches enable the fine-tuning of pre-trained LLMs with substantially
    fewer trainable parameters, making LLM fine-tuning more feasible in practice.
    Among these, LoRA [[16](#bib.bib16)] stands out for its re-parameterization technique
    of the pre-trained weight matrix $W\in\mathbb{R}^{m\times n}$, and $r\ll\min(m,n)$
    frozen, LoRA significantly reduces the memory usage and computational costs associated
    with fine-tuning LLMs, rapidly becoming the preferred method for such tasks. Despite
    its efficiency, recent research has highlighted the inferior performance of low-rank
    reparameterization compared to full-rank updates in both fine-tuning scenarios [[49](#bib.bib49),
    [2](#bib.bib2)] and pre-training contexts [[28](#bib.bib28), [56](#bib.bib56)].
    These findings underscore the need for further exploration into balancing training
    efficiency with model performance, particularly in the context of large-scale
    language models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，已经提出了各种参数高效的方法，包括提示调优[[24](#bib.bib24), [30](#bib.bib30)]、适配器[[15](#bib.bib15),
    [12](#bib.bib12)]以及低秩适配（LoRA）[[16](#bib.bib16), [9](#bib.bib9)]。这些方法使得在预训练的LLM上进行微调时，可以显著减少可训练参数的数量，从而使LLM的微调在实践中更加可行。其中，LoRA
    [[16](#bib.bib16)] 以其对预训练权重矩阵 $W\in\mathbb{R}^{m\times n}$ 进行重新参数化的技术脱颖而出，并且 $r\ll\min(m,n)$
    冻结，LoRA显著减少了与LLM微调相关的内存使用和计算成本，迅速成为这种任务的首选方法。尽管其效率较高，近期的研究显示，与全秩更新相比，低秩重新参数化在微调场景[[49](#bib.bib49),
    [2](#bib.bib2)]和预训练背景[[28](#bib.bib28), [56](#bib.bib56)]中表现较差。这些发现强调了在大规模语言模型的背景下，平衡训练效率与模型性能的进一步探索的必要性。
- en: 'In a parallel vein, layerwise sampled LLM fine-tuning appears to be a promising
    alternative for more effectively preserving the full fine-tuning trajectory. Pan
    et al. [[38](#bib.bib38)] introduced LISA, a novel fine-tuning approach for LLMs
    that integrates the concept of importance sampling [[20](#bib.bib20), [57](#bib.bib57)]
    into the fine-tuning process. In LISA, layers of LLMs are selectively unfrozen
    based on a prescribed probability, with the exception of the top and bottom layers,
    which remain active throughout the training process. However, achieving accurate
    layerwise sampling probabilities remains a significant challenge. Our investigation
    reveals a surprising observation: the layerwise importance sampling strategy employed
    by LISA underperforms when compared to a very straightforward baseline, i.e. monotonic
    decreasing sampling from top to bottom layers. Additionally, the sampled layers
    are fine-tuned in a full-rank fashion, meaning that increasing the number of unfrozen
    layers will significantly increase the memory overhead. This drawback limits the
    number of sampled layers to be small, constraining the optimal performance of
    sampling-based LLM fine-tuning. These observations motivate further exploration
    into more principled methodologies for layerwise sampled LLM fine-tuning, aiming
    to enhance both performance and memory efficiency.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在平行的领域中，逐层采样的LLM微调似乎是一个有前途的替代方案，可以更有效地保留完整的微调轨迹。Pan等人[[38](#bib.bib38)]提出了LISA，这是一种将重要性采样[[20](#bib.bib20),
    [57](#bib.bib57)]概念整合进LLM微调过程中的新方法。在LISA中，LLM的层按规定的概率选择性地解冻，顶层和底层除外，这些层在整个训练过程中保持激活。然而，实现准确的逐层采样概率仍然是一个重大挑战。我们的研究揭示了一个令人惊讶的观察结果：与非常简单的基线（即自顶向下的单调递减采样）相比，LISA采用的逐层重要性采样策略表现不佳。此外，采样的层以全秩的方式进行微调，这意味着增加未冻结层的数量会显著增加内存开销。这一缺陷限制了采样层的数量，使得基于采样的LLM微调的最终性能受到约束。这些观察结果激励了对逐层采样LLM微调进行更多原理性方法的探索，旨在提升性能和内存效率。
- en: Overview. In this paper, we introduce Outlier-weighted Layerwise Sampled Low-Rank
    Projection (OwLore), a novel, memory-efficient approach for fine-tuning large
    language models (LLMs), inspired by the layerwise outlier distribution characteristic
    of LLMs [[53](#bib.bib53)]. We analyze the outlier distribution in LLMs through
    the lens of Heavy-Tailed Self-Regularization (HT-SR) theory [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)], observing that layers with a higher prevalence
    of outliers typically exhibit a more heavy-tailed empirical spectral density (ESD)¹¹1The
    ESD of a weight matrix $W$.. According to existing HT-SR literature [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [52](#bib.bib52)], such layers are usually
    more well-trained. Based on this principle, we assign non-uniform layerwise importance
    for fine-tuning, giving higher probabilities to layers with a greater number of
    outliers. This strategy substantially improves the performance of sampling-based
    LLM fine-tuning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。在本文中，我们介绍了Outlier-weighted Layerwise Sampled Low-Rank Projection（OwLore），这是一种新的、内存高效的LLM微调方法，灵感来源于LLM的逐层异常值分布特征[[53](#bib.bib53)]。我们通过Heavy-Tailed
    Self-Regularization（HT-SR）理论[[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]分析LLM中的异常值分布，观察到具有较高异常值发生率的层通常表现出更重尾的经验谱密度（ESD）¹¹1权重矩阵$W$的ESD。根据现有的HT-SR文献[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [52](#bib.bib52)]，这些层通常训练得更好。基于这一原则，我们为微调分配了不均匀的逐层重要性，为具有更多异常值的层赋予更高的概率。这一策略显著提高了基于采样的LLM微调性能。
- en: 'To further mitigate the memory demands of full-rank training, we integrate
    gradient low-rank projection [[56](#bib.bib56)] into our approach, enabling each
    layer to be trained efficiently in a low-rank manner. By incorporating the efficient
    characteristics of low-rank projection and optimal layerwise sampling, OwLore
    can substantially increase the number of sampled layers and rank levels without
    compromising memory efficiency, enhancing the memory-performance trade-off in
    LLM fine-tuning. The effectiveness of OwLore is backed up by extensive experiments
    across diverse LLMs and benchmarks. Note that different from LoRA which adds additional
    adaptors, OwLore directly fine-tunes the original pre-trained weights, preserving
    the original optimization trajectory while being more memory-efficient. Our contributions
    can be briefly summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步减轻全秩训练的内存需求，我们将梯度低秩投影[[56](#bib.bib56)]整合到我们的方法中，使得每一层能够以低秩方式高效训练。通过结合低秩投影的高效特性和最佳层级采样，OwLore可以显著增加采样的层数和秩水平，同时不影响内存效率，增强LLM微调中的内存性能权衡。OwLore的有效性得到了广泛的实验支持，覆盖了多种LLM和基准测试。需要注意的是，与添加额外适配器的LoRA不同，OwLore直接微调原始预训练权重，保留了原始优化轨迹，同时更具内存效率。我们的贡献可以简要总结如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our first contribution is to interpret the behavior of layerwise outlier distribution
    of LLMs through the lens of Heavy-Tailed Self-Regularization theory (HT-SR). We
    find that the outlier distribution of LLMs exhibits an extremely non-uniform pattern
    across layers, which strongly correlates to the heavy-tailed structure presented
    in the ESD of the weight matrix, i.e., layers with more outliers are more heavy-tailed.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的第一个贡献是通过重尾自我正则化理论（HT-SR）的视角来解释LLM层级异常值分布的行为。我们发现LLM的异常值分布在各层之间表现出极度不均匀的模式，这与权重矩阵的ESD中呈现的重尾结构强烈相关，即，异常值更多的层具有更明显的重尾特征。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The above observation inspires a principled approach to determine the layerwise
    sampling probability for sampling-based fine-tuning methods like LISA. According
    to HT-SR theory, layers with more pronounced heavy-tail properties are typically
    better trained than others [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    Therefore, we assign higher sampling probabilities to layers with more outliers.
    This essentially forms a rich-get-richer phenomenon, substantially improving fine-tuning
    performance. To address the memory bottleneck caused by the increased number of
    sampled layers, we introduce low-rank gradient updates to sampling-based fine-tuning.
    This enables full-rank weight updates with low-rank gradients, significantly reducing
    memory costs.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述观察激发了一种原则性的方法来确定像LISA这样的基于采样的微调方法的层级采样概率。根据HT-SR理论，具有更显著重尾特性的层通常比其他层训练得更好[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]。因此，我们为异常值更多的层分配更高的采样概率。这本质上形成了一个富者越富的现象，显著提高了微调性能。为了应对由于采样层数增加而导致的内存瓶颈，我们引入了低秩梯度更新到基于采样的微调中。这使得可以使用低秩梯度进行全秩权重更新，显著减少了内存成本。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The above innovations bring forth our new fine-tuning approach, i.e., OwLore.
    By incorporating the efficient characteristics of low-rank and optimal layerwise
    sampling, OwLore significantly improves the memory-performance trade-off of LLM
    pruning. Our extensive experiments across various architectures including LLaMa2
    [[48](#bib.bib48)], LLaMa3 [[36](#bib.bib36)], and Mistral [[18](#bib.bib18)]
    demonstrate that OwLore consistently outperforms its baseline approaches including
    full fine-tuning. OwLore achieves up to a 1.1% average accuracy gain on the Commonsense
    Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench,
    while being more memory efficient. OwLore allows fine-tuning LLaMa2-7B with only
    21GB of memory.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述创新提出了我们新的微调方法，即OwLore。通过结合低秩和最佳层级采样的高效特性，OwLore显著改善了LLM修剪的内存性能权衡。我们在包括LLaMa2[[48](#bib.bib48)]、LLaMa3[[36](#bib.bib36)]和Mistral[[18](#bib.bib18)]在内的各种架构上的广泛实验表明，OwLore
    consistently超越了包括全微调在内的基线方法。OwLore在Commonsense Reasoning基准测试中实现了最高1.1%的平均准确率提升，在MMLU中提高了3.0%，在MT-Bench上获得了显著的10%提升，同时更具内存效率。OwLore允许在仅使用21GB内存的情况下微调LLaMa2-7B。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Parameter-Effieient Fine-Tuning (PEFT). PEFT is proposed to reduce the prohibitive
    cost of LLM fine-tuning. Various techniques have been proposed in this dynamic
    field. For instance, prompt tuning only optimizes input tokens or embeddings while
    keeping the rest of the model frozen, as demonstrated in studies by [[24](#bib.bib24),
    [26](#bib.bib26), [11](#bib.bib11), [59](#bib.bib59)]. Layer-freezing techniques
    [[31](#bib.bib31), [4](#bib.bib4), [25](#bib.bib25)] enhance training and fine-tuning
    efficiency by freezing parts of the layers. Adapter methods, introduced in [[15](#bib.bib15),
    [12](#bib.bib12), [32](#bib.bib32), [10](#bib.bib10)], incorporate a small auxiliary
    module within the model’s architecture, which becomes the exclusive focus of updates
    during training, thus minimizing the number of trainable parameters and optimizer
    states. Among these techniques, Low-Rank Adaptation (LoRA) [[16](#bib.bib16)]
    gains massive attention by applying low-rank matrices to approximate weight changes
    during fine-tuning, which can be merged into the pre-trained weights, leading
    to no inference overhead. LoRA has been enhanced through various modifications
    [[55](#bib.bib55), [41](#bib.bib41), [44](#bib.bib44), [29](#bib.bib29), [22](#bib.bib22),
    [9](#bib.bib9), [56](#bib.bib56)] aimed at improving performance and efficiency.
    Recently, low-rank has also been explored to pre-train LLM from scratch [[27](#bib.bib27),
    [56](#bib.bib56)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）。PEFT旨在减少LLM微调的高昂成本。在这个动态领域中，提出了各种技术。例如，提示微调仅优化输入标记或嵌入，同时保持模型的其余部分被冻结，这在[[24](#bib.bib24)、[26](#bib.bib26)、[11](#bib.bib11)、[59](#bib.bib59)]的研究中有所展示。层冻结技术[[31](#bib.bib31)、[4](#bib.bib4)、[25](#bib.bib25)]通过冻结部分层来提高训练和微调效率。适配器方法[[15](#bib.bib15)、[12](#bib.bib12)、[32](#bib.bib32)、[10](#bib.bib10)]在模型架构中引入一个小的辅助模块，该模块成为训练过程中更新的唯一焦点，从而最小化可训练参数和优化器状态的数量。在这些技术中，低秩适应（LoRA）[[16](#bib.bib16)]通过应用低秩矩阵来近似微调过程中的权重变化，从而引起了广泛关注，这些矩阵可以合并到预训练的权重中，导致没有推理开销。LoRA通过各种修改[[55](#bib.bib55)、[41](#bib.bib41)、[44](#bib.bib44)、[29](#bib.bib29)、[22](#bib.bib22)、[9](#bib.bib9)、[56](#bib.bib56)]得到了增强，旨在提高性能和效率。最近，低秩技术还被探索用于从零开始预训练LLM[[27](#bib.bib27)、[56](#bib.bib56)]。
- en: 'Layerwise Importance Sampled AdamW (LISA). Pan et al., [[38](#bib.bib38)] conducted
    an in-depth analysis of LoRA’s training dynamics across layers and revealed an
    unusual skew in the distribution of layerwise weight norms, particularly towards
    the top layer and/or the bottom layer, where the norms are significantly larger
    compared to other layers. Building upon this insight, the authors proposed LISA,
    a novel fine-tuning approach for LLMs, which incorporates the concept of importance
    sampling [[20](#bib.bib20), [57](#bib.bib57)] into the fine-tuning process. In
    LISA, pre-trained layers of LLMs are sampled to be unfrozen during training based
    on a prescribed probability, with the exception of the top and bottom layers,
    which remain activated throughout the process. Given a network with $N_{L}$ is
    given as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 层次重要性抽样AdamW（LISA）。Pan等人[[38](#bib.bib38)]对LoRA的层次训练动态进行了深入分析，并揭示了层次权重范数分布的不寻常偏斜，特别是向顶部层和/或底部层倾斜，其中范数显著大于其他层。在此见解的基础上，作者提出了LISA，这是一种新颖的LLM微调方法，将重要性抽样[[20](#bib.bib20)、[57](#bib.bib57)]的概念引入微调过程。在LISA中，根据预定的概率对LLM的预训练层进行抽样，以便在训练过程中解冻，顶部和底部层除外，这些层在整个过程中保持激活。给定一个网络，其$N_{L}$如下：
- en: '|  | $p_{\ell}=\left\{\begin{array}[]{lcl}1.0,&amp;&amp;{if\;\ell=1\;\text{or}\;\ell=N_{L}},\\
    \gamma/N_{L}&amp;&amp;else.\end{array}\right.$ |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{\ell}=\left\{\begin{array}[]{lcl}1.0,&amp;&amp;{if\;\ell=1\;\text{or}\;\ell=N_{L}},\\
    \gamma/N_{L}&amp;&amp;else.\end{array}\right.$ |  | (1) |'
- en: where $\gamma$ layers, it notably reduces the memory usage of LLM fine-tuning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\gamma$层显著减少了LLM微调的内存使用。
- en: 3 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we introduce our approach, Outlier-weighed Layerwise Low-Rank
    Projection OwLore. We will discuss the underlying rationales, present preliminary
    results, and detail the algorithm design.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法——异常值加权层次低秩投影OwLore。我们将讨论其基本原理，展示初步结果，并详细说明算法设计。
- en: 3.1 Shortcomings of LISA
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LISA的缺点
- en: 'While demonstrating promising results, we observe that the LISA algorithm inherently
    has two shortcomings that constrain its memory-performance trade-off:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管展示了有希望的结果，我们观察到LISA算法固有的两个缺点限制了其内存性能的权衡：
- en: 'i. The middle layers of LISA are sampled uniformly, which can result in suboptimal
    performance. To verify this point, we conduct a small experiment where we replace
    the uniform sampling with a very simple baseline, i.e. monotonic decreasing sampling,
    where the sample probability is monotonically decreasing from early layers to
    late layers (noted as LISA-D). Table [1](#S3.T1 "Table 1 ‣ 3.1 Shortcomings of
    LISA ‣ 3 Methodology ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning") shows that this simple sampling method outperforms uniform
    sampling in most cases, verifying our concern.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'i. LISA 的中间层均匀采样，这可能导致性能不佳。为验证这一点，我们进行了一项小实验，将均匀采样替换为非常简单的基线，即单调递减采样，其中样本概率从早期层到晚期层单调递减（记作
    LISA-D）。表 [1](#S3.T1 "表 1 ‣ 3.1 LISA 的不足 ‣ 3 方法论 ‣ OwLore: 异常值加权层次采样低秩投影用于 LLM
    微调") 显示，这种简单的采样方法在大多数情况下优于均匀采样，验证了我们的担忧。'
- en: 'Table 1: Fine-tuning performance of LLaMA2-7B with various dataset.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用不同数据集对 LLaMA2-7B 进行微调的性能。
- en: '| Model | Method | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | OBQA |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | OBQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Llama2-7B | LISA | 82.0 | 79.9 | 33.5 | 59.7 | 79.6 | 38.8 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | LISA | 82.0 | 79.9 | 33.5 | 59.7 | 79.6 | 38.8 |'
- en: '| Llama2-7B | LISA-D | 85.1 | 79.9 | 33.8 | 59.8 | 79.7 | 38.4 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | LISA-D | 85.1 | 79.9 | 33.8 | 59.8 | 79.7 | 38.4 |'
- en: 'ii. The sampled layers of LISA are fine-tuned in a full-rank manner, causing
    a significant memory increase as the number of sampled layers increases. To demonstrate
    this, we report the memory usage of LISA used to fine-tune Llama2-7B as the number
    of sampled layers increases in Table [2](#S3.T2 "Table 2 ‣ 3.1 Shortcomings of
    LISA ‣ 3 Methodology ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning"). The memory requirement of LISA rapidly increases from 23G
    to 32G as expected sampled layers $\gamma$ increase from 1 to 8\. Since sampling
    more layers leads to stronger fine-tuning performance [[38](#bib.bib38)], reducing
    the memory increase associated with the number of sampled layers is pivotal.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'ii. LISA 的采样层以全秩方式进行微调，导致随着采样层数量的增加，内存显著增加。为证明这一点，我们报告了随着采样层数量增加而用于微调 Llama2-7B
    的 LISA 的内存使用情况，如表 [2](#S3.T2 "表 2 ‣ 3.1 LISA 的不足 ‣ 3 方法论 ‣ OwLore: 异常值加权层次采样低秩投影用于
    LLM 微调") 所示。LISA 的内存需求从 23G 快速增加到 32G，因为期望采样层 $\gamma$ 从 1 增加到 8。由于采样更多层会导致更强的微调性能
    [[38](#bib.bib38)]，因此减少与采样层数量相关的内存增加至关重要。'
- en: 'Table 2: Memory usage to fine-tune LLaMA2-7B with various expected sampled
    blocks $\gamma$.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用不同期望采样块 $\gamma$ 对 LLaMA2-7B 进行微调的内存使用情况。
- en: '| Model | Method | $\gamma=1$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | $\gamma=1$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Llama2-7B | LISA | 23G | 24G | 27G | 32G |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | LISA | 23G | 24G | 27G | 32G |'
- en: '| Llama2-7B | OwLore (ours) | 21G | 22G | 23G | 25G |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | OwLore（我们的） | 21G | 22G | 23G | 25G |'
- en: 3.2 Outlier Distribution and Heavy-tailed Self-regularizaiton
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 异常值分布和重尾自我正则化
- en: Although LISA-D achieves good performance, it is more desirable to seek a more
    principled approach to determine the layerwise sampling probability. In the context
    of LLMs, we get inspiration from the unique characteristic of LLMs – layerwise
    outlier distribution [[53](#bib.bib53)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LISA-D 实现了良好的性能，但更理想的是寻求一种更有原则的方法来确定层级采样概率。在 LLM 的背景下，我们从 LLM 的独特特征——层级异常值分布中获得灵感
    [[53](#bib.bib53)]。
- en: '![Refer to caption](img/7ac9a16a767eb7354e71b9468dff5979.png)![Refer to caption](img/0342ce2822b002795ced6bea2ce5909c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7ac9a16a767eb7354e71b9468dff5979.png)![参见说明](img/0342ce2822b002795ced6bea2ce5909c.png)'
- en: 'Figure 2: Layerwise outlier distribution and heavy-tail content distribution
    of LLaMa2\. Layers with more outliers typically are consistently more heavy-tailed
    in their weight matrices.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMa2 的层级异常值分布和重尾内容分布。异常值较多的层通常在其权重矩阵中表现出一致的重尾特征。
- en: Recent studies have unveiled a unique characteristic of LLMs - the presence
    of outliers, defined as features exhibiting significantly larger magnitudes compared
    to the majority of others [[23](#bib.bib23), [40](#bib.bib40)]. While constituting
    only a small fraction of the total feature dimensions, these outliers play a vital
    role in the model’s predictive performance, leading to promising results in LLM
    compression [[8](#bib.bib8), [50](#bib.bib50), [45](#bib.bib45), [53](#bib.bib53)].
    However, its theoretical understanding is somehow missing. In the hope of drawing
    theory-guided inspirations for more principled designing of layerwise sampling
    probability, we attempt to interpret the outlier distribution across layers through
    the lens of Heavy-tailed Self-regularization theory (HT-SR) [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究揭示了 LLMs 的一个独特特征——异常值，即表现出显著更大幅度的特征，相对于大多数其他特征 [[23](#bib.bib23), [40](#bib.bib40)]。虽然这些异常值仅占特征维度的很小一部分，但它们在模型的预测性能中发挥了重要作用，从而在
    LLM 压缩中取得了有希望的结果 [[8](#bib.bib8), [50](#bib.bib50), [45](#bib.bib45), [53](#bib.bib53)]。然而，其理论理解仍然有所欠缺。为了从理论指导的角度为逐层采样概率的更原则性设计提供启示，我们尝试通过重尾自正则化理论（HT-SR）
    [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] 来解释层之间的异常值分布。
- en: 'First, we leverage the Layerwise Outlier Distribution (LOD) proposed in [[53](#bib.bib53)]
    to quantify the outlier distribution across layers. LOD essentially counts up
    weights whose outlier score is $\tau$, where $N$. Outlier score of weight $\mathbf{W}_{\texttt{ij}}$
    norm of input feature connected to the weight. Hence, the layerwise outlier distribution
    of a $N_{L}$:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们利用 [[53](#bib.bib53)] 提出的逐层异常值分布（LOD）来量化层之间的异常值分布。LOD 实质上计算异常值分数为 $\tau$
    的权重，其中 $N$。权重 $\mathbf{W}_{\texttt{ij}}$ 的异常值分数是连接到权重的输入特征的范数。因此，$N_{L}$ 的逐层异常值分布为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $\mathbf{\bar{A}}^{\ell}$ is larger than $\tau\cdot\mathbf{\bar{A}}^{\ell}$
    means more outliers are presented in the corresponding layer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{\bar{A}}^{\ell}$ 大于 $\tau\cdot\mathbf{\bar{A}}^{\ell}$ 表示相应层中存在更多的异常值。
- en: 'On the other hand, HT-SR [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]
    posits that during the training of deep neural networks, the spectral density
    of the weight matrices gradually evolves from a normal distribution to a heavy-tailed
    distribution. Layers that have undergone more extensive training tend to exhibit
    a more pronounced heavy-tailed structure in their Empirical Spectral Density (ESD),
    i.e., the distribution of eigenvalues. Following [[60](#bib.bib60)], we utilize
    the PL_Alpha_Hill metric to characterize the heavy-tail extent of the $\ell^{th}$
    layer’s ESD based on the Hill estimator [[14](#bib.bib14), [51](#bib.bib51)],
    given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，HT-SR [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] 提出，在深度神经网络的训练过程中，权重矩阵的谱密度逐渐从正态分布演变为重尾分布。经过更多训练的层在其经验谱密度（ESD），即特征值的分布中，往往表现出更明显的重尾结构。遵循
    [[60](#bib.bib60)]，我们利用 PL_Alpha_Hill 度量来表征第 $\ell^{th}$ 层 ESD 的重尾程度，该度量基于 Hill
    估计器 [[14](#bib.bib14), [51](#bib.bib51)]，给定为：
- en: '|  | $\texttt{PL\_Alpha\_Hill}_{\ell}=1+\frac{k}{(\sum_{i=1}^{k}\ln\frac{\lambda^{\ell}_{n-i+1}}{\lambda^{\ell}_{n-k}})},$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{PL\_Alpha\_Hill}_{\ell}=1+\frac{k}{(\sum_{i=1}^{k}\ln\frac{\lambda^{\ell}_{n-i+1}}{\lambda^{\ell}_{n-k}})},$
    |  | (3) |'
- en: where $\{\lambda^{\ell}_{i}\}_{i=1}^{n}$ aligns with the peak of the ESD. Note
    that, originally, the lower the PL_Alpha_Hill metric is, the more heavy-tailed
    the layer is. For a more direct comparison to LOD, we reverse PL_Alpha_Hill metric
    such that a larger metric value indicates a more pronounced heavy-tailed weight
    metric.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{\lambda^{\ell}_{i}\}_{i=1}^{n}$ 与 ESD 的峰值对齐。注意，原本 PL_Alpha_Hill 度量越低，层的重尾性越强。为了与
    LOD 更直接地比较，我们反转 PL_Alpha_Hill 度量，使得更大的度量值表示更显著的重尾权重度量。
- en: 'We plot outlier distribution and heavy-tail content distribution in Figure
    [2](#S3.F2 "Figure 2 ‣ 3.2 Outlier Distribution and Heavy-tailed Self-regularizaiton
    ‣ 3 Methodology ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning"), which reveals two noteworthy observations: 1
    Both metrics exhibit extremely non-uniform layerwise distributions, indicating
    that sampling middle layers non-uniformly is more reasonable; 2
    Layers with higher outlier ratios consistently show a more heavy-tailed ESD, suggesting
    that they have captured more informative features according to the HT-SR theory
    [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [52](#bib.bib52)]. To verify
    our conjecture, we measure the Spearman’s rank correlation of these two distributions.
    Our results demonstrate a significant correlation between them: 0.74 (p < 0.001)
    for LLaMa2-7B and 0.77 (p < 0.001) for LLaMa2-13B.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[2](#S3.F2 "Figure 2 ‣ 3.2 Outlier Distribution and Heavy-tailed Self-regularizaiton
    ‣ 3 Methodology ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning")中绘制了异常值分布和重尾内容分布，这揭示了两个值得注意的观察结果：1
    两个指标都显示了极其不均匀的逐层分布，表明不均匀地采样中间层更为合理；2 异常值比例较高的层表现出更重尾的ESD，表明根据HT-SR理论，它们捕获了更多的信息特征[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [52](#bib.bib52)]。为了验证我们的猜想，我们测量了这两个分布的Spearman等级相关性。结果显示它们之间有显著的相关性：LLaMa2-7B为0.74（p
    < 0.001），LLaMa2-13B为0.77（p < 0.001）。'
- en: 3.3 Outlier-weighed Layerwise Low-Rank Projection (OwLore)
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 异常值加权的逐层低秩投影（OwLore）
- en: 'The above findings shed light on a principle for designing non-uniform layerwise
    sampling for LLM fine-tuning: layers with higher outlier ratios should be prioritized
    during the fine-tuning process. This forms the foundation of our proposed method,
    Outlier-weighed Layerwise Low-Rank Projection (OwLore), which we will present
    in detail.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述发现揭示了一个设计LLM微调的非均匀逐层采样的原则：在微调过程中应优先考虑异常值比例较高的层。这构成了我们提出的方法——异常值加权的逐层低秩投影（OwLore）的基础，我们将在后续详细介绍。
- en: Outlier-weighed sampling. Given a $N_{L}$, where $\gamma$ is the hyperparameter
    inherited from LISA to control the expected number of unfreeze layers during optimization.
    At each iteration, only the sampled layers will be fine-tuned, while the remaining
    layers are kept frozen. This sampling method naturally leads to a rich-get-richer
    phenomenon, where layers that are better trained during the pre-training process
    are sampled and fine-tuned more frequently.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值加权采样。给定一个$N_{L}$，其中$\gamma$是从LISA继承的超参数，用于控制优化过程中预期的解冻层数量。在每次迭代中，只有采样的层会被微调，其余层保持冻结。这种采样方法自然导致富者愈富的现象，其中在预训练过程中训练得更好的层被更频繁地采样和微调。
- en: 'Gradient low-rank update. Outlier-weighed sampling addresses our first research
    question: how to optimally sample layers for sampling-based LLM fine-tuning. To
    tackle the second issue of the substantial memory cost associated with an increasing
    number of unfrozen layers, we propose to integrate outlier-weighed sampling with
    low-rank training. In this approach, the sampled layers are updated in a low-rank
    manner. Specifically, we adopt GaLore proposed in [[56](#bib.bib56)], wherein
    for each sampled layer, the gradient matrix is projected into a low-rank subspace
    using Singular Value Decomposition (SVD). The optimizer states are subsequently
    updated in the corresponding low-rank subspace with a rank level of $r$, significantly
    reducing the memory cost of optimization. We update the gradient subspace every
    200 iterations to better capture the dynamic trajectory of fine-tuning.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度低秩更新。异常值加权采样解决了我们的第一个研究问题：如何为基于采样的 LLM 微调最优地采样层。为了应对与不断增加的未冻结层相关的显著内存成本的第二个问题，我们提出将异常值加权采样与低秩训练相结合。在这种方法中，采样的层以低秩方式进行更新。具体来说，我们采用
    [[56](#bib.bib56)] 中提出的 GaLore，对于每个采样层，使用奇异值分解（SVD）将梯度矩阵投影到低秩子空间。优化器状态随后在对应的低秩子空间中以秩级
    $r$ 进行更新，显著降低了优化的内存成本。我们每 200 次迭代更新一次梯度子空间，以更好地捕捉微调的动态轨迹。
- en: 'The above two innovations significantly boost the memory efficiency of OwLore,
    unlocking the performance-memory trade-off of sampling-based fine-tuning. At the
    macro level, we dynamically sample a limited number of layers to fine-tune at
    each iteration. At the micro level, each sampled layers are updated with low-rank
    gradients. Since the sampled layers are updated in the low-rank subspace, we can
    efficiently increase the number of sampled layers $\gamma$ consistently give us
    robust performance across models and downstream tasks. Therefore, we choose them
    as our default settings. We present our algorithm in Algorithm [1](#alg1 "In 3.3
    Outlier-weighed Layerwise Low-Rank Projection (OwLore) ‣ 3 Methodology ‣ OwLore:
    Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '上述两项创新显著提升了 OwLore 的内存效率，解锁了基于采样的微调的性能-内存权衡。在宏观层面，我们在每次迭代时动态采样有限数量的层进行微调。在微观层面，每个采样层都使用低秩梯度进行更新。由于采样层在低秩子空间中进行更新，我们可以高效地增加采样层的数量
    $\gamma$，始终为我们在模型和下游任务中提供稳健的性能。因此，我们将其作为默认设置。我们在算法 [1](#alg1 "在 3.3 异常值加权逐层低秩投影
    (OwLore) ‣ 3 方法论 ‣ OwLore: 异常值加权逐层样本低秩投影用于 LLM 微调") 中展示了我们的算法。'
- en: 'Require: number of layers $N_{L}$, rank level $r$ to $N_{L}$ Mapping layerwise
    outlier distribution to sampling probability.            if * $\mathcal{U}(0,1)>
    For Owlore-Full, we use the default AdamW optimizer with full ranks.      if *Owlore* then            
    Run gradient low-rank update for \text{阈值}$*
    对于 Owlore-Full，我们使用默认的 AdamW 优化器和全秩。      如果 *Owlore* 则             运行梯度低秩更新，以
    1'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '| OwLore | 23G | 88.0 | 81.8 | 34.0 | 62.9 | 85.7 | 81.4 | 54.4 | 39.4 | 65.9
    | 1'
- en: OwLore approaches significantly outperform other efficient fine-tuning approaches
    by a large margin. Applying our outlier-weighed sampling approach to LISA (i.e.,
    OwLore-Full) achieves a notable average accuracy boost over LISA on LLaMA2-7B,
    i.e., 0.8%. Moreover, the low-rank operation further improves the performance-memory
    trade-off of OwLore, achieving a 0.3% average accuracy gain, a notable 1.7% gain
    on WinoGrande, while lowering memory usage by 1G.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: OwLore 方法显著优于其他高效微调方法。将我们的异常值加权采样方法应用于 LISA（即 OwLore-Full）在 LLaMA2-7B 上实现了显著的平均准确率提升，即
    0.8%。此外，低秩操作进一步改善了 OwLore 的性能-内存权衡，取得了 0.3% 的平均准确率提升，在 WinoGrande 上的提升为 1.7%，同时将内存使用减少了
    1G。
- en: 2
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: OwLore approaches consistently outperform full fine-tuning across tasks on LLaMa.
    We can observe that both OwLore and OwLore-Full can outperform the performance
    of full fine-tuning with LLaMa2-7B and LLaMa3-8B. LISA can match the performance
    of full fine-tuning, whereas GaLore and LoRA perform no better than full fine-tuning.
    However, full fine-tuning performs much better with Mistral-7B, and all fine-tuning
    approaches fail to match. Still, OWLore is the best approach.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: OwLore 方法在 LLaMa 上的任务中始终优于完整微调。我们可以观察到，OwLore 和 OwLore-Full 都可以超越 LLaMa2-7B
    和 LLaMa3-8B 的完整微调表现。LISA 能够匹配完整微调的表现，而 GaLore 和 LoRA 的表现不如完整微调。然而，完整微调在 Mistral-7B
    上表现更好，而所有微调方法均未能匹敌。尽管如此，OWLore 仍然是最佳的方法。
- en: 3
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 3
- en: LLaMa3-8B consistently outperforms LLaMa2-7B on Commonsense Reasoning. As the
    most advanced variant of LLaMa, LLaMa3-8B consistently outperforms its previous
    version. Interestingly, performance variance between different fine-tuning approaches
    of LLaMa3 is smaller than LLaMa2.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa3-8B 在常识推理任务中始终优于 LLaMa2-7B。作为 LLaMa 系列中最先进的变体，LLaMa3-8B 始终优于其前一版本。有趣的是，LLaMa3
    不同微调方法之间的性能差异小于 LLaMa2。
- en: 'Table 4: Fine-tuning performance of LLaMa2-7B with various approaches on MT-Bench.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同方法在 MT-Bench 上对 LLaMa2-7B 的微调性能。
- en: '| Method | Writing | Roleplay | Reasoning | Math | Coding | Extraction | STEM
    | Humanities | Avg. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Method | Writing | Roleplay | Reasoning | Math | Coding | Extraction | STEM
    | Humanities | Avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Full-FT | 7.11 | 8.11 | 4.90 | 2.85 | 3.75 | 6.50 | 7.80 | 8.10 | 6.14 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Full-FT | 7.11 | 8.11 | 4.90 | 2.85 | 3.75 | 6.50 | 7.80 | 8.10 | 6.14 |'
- en: '| LoRA | 7.21 | 7.05 | 4.95 | 3.25 | 3.90 | 5.70 | 7.90 | 7.65 | 5.95 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 7.21 | 7.05 | 4.95 | 3.25 | 3.90 | 5.70 | 7.90 | 7.65 | 5.95 |'
- en: '| GaLore | 7.05 | 7.79 | 3.55 | 2.89 | 3.15 | 6.25 | 8.30 | 7.63 | 5.83 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GaLore | 7.05 | 7.79 | 3.55 | 2.89 | 3.15 | 6.25 | 8.30 | 7.63 | 5.83 |'
- en: '| LISA | 6.75 | 7.35 | 4.35 | 3.00 | 3.85 | 6.85 | 7.74 | 7.47 | 5.92 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LISA | 6.75 | 7.35 | 4.35 | 3.00 | 3.85 | 6.85 | 7.74 | 7.47 | 5.92 |'
- en: '| OwLore-Full | 7.53 | 8.00 | 4.93 | 3.25 | 4.53 | 6.33 | 8.50 | 8.57 | 6.46
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| OwLore-Full | 7.53 | 8.00 | 4.93 | 3.25 | 4.53 | 6.33 | 8.50 | 8.57 | 6.46
    |'
- en: '| OwLore | 8.00 | 7.65 | 4.95 | 3.25 | 4.15 | 7.45 | 8.25 | 8.45 | 6.52 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| OwLore | 8.00 | 7.65 | 4.95 | 3.25 | 4.15 | 7.45 | 8.25 | 8.45 | 6.52 |'
- en: 'MT-Bench. We next evaluate OwLore on a more comprehensive benchmark, MT-Bench,
    featuring 80 high-quality, multi-turn questions designed to assess LLMs on 8 common
    categories. Results are presented in Table [4](#S4.T4 "Table 4 ‣ 4.2 Experimental
    Results ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning"). We can observe that the benefits of OWLore over other PEFT
    approaches are more pronounced. All other baselines fail to match the performance
    of full fine-tuning on MT-Bench with scores below 6.0, whereas OwLore-Full and
    OwLore both outperform the full fine-tuning by a large margin. OwLore-Full significantly
    boosts the average score of LISA from 5.92 to 6.46 by solely applying outlier-weighed
    sampling, highlighting the effectiveness of our outlier-inspired sampling.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'MT-Bench。我们接下来在一个更全面的基准测试 MT-Bench 上评估 OwLore，该基准测试包括 80 个高质量、多轮问题，旨在评估 LLM
    在 8 个常见类别上的表现。结果呈现在表格 [4](#S4.T4 "Table 4 ‣ 4.2 Experimental Results ‣ 4 Experiments
    ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")
    中。我们可以观察到，OWLore 相较于其他 PEFT 方法的优势更加明显。所有其他基线方法在 MT-Bench 上的得分均低于 6.0，无法匹敌完整微调的表现，而
    OwLore-Full 和 OwLore 都大幅超越了完整微调。OwLore-Full 通过仅应用异常值加权采样，显著提升了 LISA 的平均分数，从 5.92
    提高到 6.46，突显了我们异常值启发采样的有效性。'
- en: 'Table 5: Fine-tuning performance of LLaMa2-7B with various approaches on MMLU
    benchmark.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同方法在 MMLU 基准测试上对 LLaMa2-7B 的微调性能。
- en: '| Method | Humanities | STEM | Social Sciences | Other | Avg. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Method | Humanities | STEM | Social Sciences | Other | Avg. |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Full-FT | 49.9 | 41.7 | 57.5 | 57.0 | 51.5 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Full-FT | 49.9 | 41.7 | 57.5 | 57.0 | 51.5 |'
- en: '| LoRA | 46.1 | 40.8 | 56.6 | 56.2 | 49.9 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 46.1 | 40.8 | 56.6 | 56.2 | 49.9 |'
- en: '| GaLore | 45.4 | 41.7 | 55.8 | 56.0 | 49.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GaLore | 45.4 | 41.7 | 55.8 | 56.0 | 49.7 |'
- en: '| LISA | 44.9 | 41.2 | 54.7 | 57.6 | 49.6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LISA | 44.9 | 41.2 | 54.7 | 57.6 | 49.6 |'
- en: '| OwLore-Full | 49.1 | 41.3 | 58.8 | 59.1 | 52.1 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| OwLore-Full | 49.1 | 41.3 | 58.8 | 59.1 | 52.1 |'
- en: '| OwLore | 49.8 | 42.1 | 58.6 | 59.7 | 52.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| OwLore | 49.8 | 42.1 | 58.6 | 59.7 | 52.6 |'
- en: 'MMLU Benchmark. To draw a more solid conclusion, we also test another widely
    used benchmark, i.e., MMLU. The results are shown in Table [5](#S4.T5 "Table 5
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise
    Sampled Low-Rank Projection for LLM Fine-tuning"). Our findings highlight that
    OwLore consistently outperforms Full FT, while other PEFT methods fall short of
    dense fine-tuning. Specifically, OwLore achieves an average score of 52.6, demonstrating
    significant improvements across various domains such as Humanities, STEM, Social
    Sciences, and Others. These results underscore OwLore’s efficacy beyond full fine-tuning
    while maintaining superior memory efficiency.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'MMLU 基准测试。为了得出更为可靠的结论，我们还测试了另一个广泛使用的基准测试，即 MMLU。结果如表 [5](#S4.T5 "Table 5 ‣
    4.2 Experimental Results ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise Sampled
    Low-Rank Projection for LLM Fine-tuning") 所示。我们的发现突显了 OwLore 在性能上始终优于 Full FT，而其他
    PEFT 方法则不及密集微调。具体而言，OwLore 的平均得分为 52.6，在人文、STEM、社会科学和其他领域均表现出显著的改善。这些结果强调了 OwLore
    在全微调之外的效能，同时保持了卓越的内存效率。'
- en: 4.3 Fine-tuning Memory Usage
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 微调内存使用
- en: '![Refer to caption](img/cbfd2848e0b161d5aed7a82537c06f1b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbfd2848e0b161d5aed7a82537c06f1b.png)'
- en: 'Figure 3: Fine-tuning memory usage of using various with LLaMa2-7B. Left: varying
    sampled layers. In this scenario, we also vary the rank of LoRA and OwLore from
    4 to 128 to provide a comprehensive analysis. OwLore consistently demonstrates
    superior memory efficiency across all configurations. Notably, LISA’s memory advantage
    over LoRA diminishes as the number of sampled layers increases. Right: varying
    ranks. The sampled layer of LISA and OwLore is set as $\gamma=2$.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用不同方法微调 LLaMa2-7B 的内存使用情况。左侧：变化的采样层。在这种情况下，我们还将 LoRA 和 OwLore 的秩从 4 变化到
    128，以提供全面的分析。OwLore 在所有配置下都展示了优越的内存效率。值得注意的是，LISA 相对于 LoRA 的内存优势随着采样层数量的增加而减少。右侧：变化的秩。LISA
    和 OwLore 的采样层设置为 $\gamma=2$。
- en: 'Thanks to its layerwise sampling and low-rank characteristics, OwLore significantly
    improves the memory efficiency of LLM fine-tuning. To verify this, we report the
    memory cost of various approaches when used to fine-tune LLaMa2-7B, with a token
    batch size of 1, as shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Fine-tuning Memory
    Usage ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '由于其逐层采样和低秩特性，OwLore 显著提高了 LLM 微调的内存效率。为验证这一点，我们报告了在使用 LLaMa2-7B 时的各种方法的内存开销，令牌批量大小为
    1，如图 [3](#S4.F3 "Figure 3 ‣ 4.3 Fine-tuning Memory Usage ‣ 4 Experiments ‣ OwLore:
    Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning") 所示。'
- en: 'On the one hand, the low-rank nature of OwLore allows us to unfreeze more layers
    without a substantial increase in memory cost compared to LISA. As illustrated
    in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Fine-tuning Memory Usage ‣ 4 Experiments
    ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")-Left,
    when increasing $\gamma$) while still maintaining a lower memory cost. On the
    other hand, Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Fine-tuning Memory Usage ‣ 4 Experiments
    ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")-Right
    demonstrates that layerwise sampling enables high-rank training without significantly
    compromising memory efficiency, in stark contrast to LoRA. It is important to
    note that we do not utilize the layer-wise weight update technique used in GaLore
    for the memory measurement, hence the memory cost of GaLore is higher than reported
    in [[56](#bib.bib56)].'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '一方面，OwLore 的低秩特性使我们能够解冻更多的层，而不会相较于 LISA 显著增加内存开销。如图 [3](#S4.F3 "Figure 3 ‣
    4.3 Fine-tuning Memory Usage ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise
    Sampled Low-Rank Projection for LLM Fine-tuning")-左侧所示，在增加 $\gamma$ 时，内存开销仍然较低。另一方面，图
    [3](#S4.F3 "Figure 3 ‣ 4.3 Fine-tuning Memory Usage ‣ 4 Experiments ‣ OwLore:
    Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")-右侧展示了逐层采样使得高秩训练成为可能，而不会显著降低内存效率，与
    LoRA 相比形成鲜明对比。需要注意的是，我们没有使用 GaLore 中的逐层权重更新技术进行内存测量，因此 GaLore 的内存开销高于 [[56](#bib.bib56)]
    报告的值。'
- en: 'We further break down the memory usage during LLM fine-tuning, presenting the
    results in Figure [4](#S4.F4 "Figure 4 ‣ 4.4 Training Loss Curve ‣ 4 Experiments
    ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")-Left.
    For this analysis, $\gamma$ is set to 8 for both LoRA and OwLore. LoRA incurs
    a substantial activation memory cost, although its optimizer and gradient memory
    requirements are relatively small. In contrast, LISA’s optimizer memory cost is
    large because each layer is trained in full rank, yet it benefits from a small
    activation memory cost. OwLore effectively combines the advantages of both methods,
    inheriting the small activation memory of LISA while significantly reducing the
    optimizer memory requirement. Notably, this benefit allows OwLore to fine-tune
    LLaMa2-7B with only 22GB of memory, demonstrating its superior memory efficiency.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步分解了在 LLM 微调过程中的内存使用情况，结果如图 [4](#S4.F4 "Figure 4 ‣ 4.4 Training Loss Curve
    ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning")-左所示。对于这项分析，$\gamma$ 在 LoRA 和 OwLore 中均设置为 8。LoRA 的激活内存开销非常大，尽管其优化器和梯度内存需求相对较小。相比之下，LISA
    的优化器内存开销较大，因为每一层都以全秩进行训练，但它的激活内存开销较小。OwLore 有效地结合了这两种方法的优点，继承了 LISA 的小激活内存，同时显著降低了优化器的内存需求。值得注意的是，这一优势使得
    OwLore 能够仅用 22GB 内存微调 LLaMa2-7B，展示了其优越的内存效率。'
- en: 4.4 Training Loss Curve
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 训练损失曲线
- en: '![Refer to caption](img/92ba8a7e8b4cf8e2a86ca2f05281fd30.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92ba8a7e8b4cf8e2a86ca2f05281fd30.png)'
- en: 'Figure 4: Left: Mmeory breakdown of various methods using LLaMa2-7B. Right:
    Fine-tuning loss of LLaMA2-7B on Alpaca GPT-4 dataset using various methods.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：左侧：各种方法在 LLaMa2-7B 上的内存分解。右侧：使用各种方法在 Alpaca GPT-4 数据集上对 LLaMa2-7B 进行微调的损失。
- en: 'The training loss curve is an effective way to understand the training dynamics
    of various methods. Following LISA, we present fine-tuning loss curves of LLaMa2-7B
    on the Alpaca-GPT4 dataset using Full FT, LoRA, LISA, and OwLore in Figure [4](#S4.F4
    "Figure 4 ‣ 4.4 Training Loss Curve ‣ 4 Experiments ‣ OwLore: Outlier-weighed
    Layerwise Sampled Low-Rank Projection for LLM Fine-tuning")-Right. At first glance,
    methods that directly fine-tune pre-trained weights (i.e., LISA and OwLore) can
    better mimic the training landscape of full fine-tuning, compared to LoRA.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '训练损失曲线是理解各种方法训练动态的有效方式。继 LISA 之后，我们在图 [4](#S4.F4 "Figure 4 ‣ 4.4 Training Loss
    Curve ‣ 4 Experiments ‣ OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection
    for LLM Fine-tuning")-右中展示了在 Alpaca-GPT4 数据集上使用 Full FT、LoRA、LISA 和 OwLore 对 LLaMa2-7B
    进行微调的损失曲线。一开始，直接微调预训练权重的方法（即 LISA 和 OwLore）比 LoRA 更能模拟全微调的训练景观。'
- en: It is worth noting that while OwLore initially falls short of LISA in the early
    phase of training, it gradually catches up after 60 iterations and eventually
    outperforms LISA with a lower loss. We conjecture that the underlying reason here
    is that the low-rank update of OwLore is less accurate than the full-rank update
    of LISA at the beginning. However, as training progresses, OwLore keeps updating
    the subspace, leading to an optimal one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，虽然在训练的早期阶段 OwLore 初始时不及 LISA，但在经过 60 次迭代后，它逐渐赶上，并最终以较低的损失超越了 LISA。我们推测，这里的根本原因是，OwLore
    的低秩更新在开始时不如 LISA 的全秩更新准确。然而，随着训练的进展，OwLore 不断更新子空间，从而导致一个最优解。
- en: 5 Conclusion
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we first establish the bridge between outliers and heavy-tailed
    properties in neural network layers. We discovered that layers containing more
    outliers typically exhibit heavy-tailed properties and are generally better trained
    according to the Heavy-Tailed Self-Regularization theory. Based on this insight,
    we introduce OwLore, a novel fine-tuning method that assigns higher sampling probabilities
    to these outlier-rich layers, employing a “rich-get-richer” approach. This innovative
    technique enhances fine-tuning performance while maintaining higher memory efficiency
    compared to traditional full-rank fine-tuning. The memory efficiency of OwLore
    could be further improved by incorporating Low-Rank gradient updating. Our experiments
    across various architectures, including LLaMa2, LLaMa3, and Mistral, demonstrate
    that OwLore achieves significant performance improvements while maintaining higher
    memory efficiency compared to traditional full-rank fine-tuning. These results
    highlight OwLore’s potential to make the deployment of sophisticated language
    models more practical and accessible, particularly in resource-limited settings.
    The primary limitation of our work remains the limited exploration of very large-scale
    LLMs such as those with 70 billion parameters, suggesting an avenue for future
    research.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们首先建立了神经网络层中离群值与重尾属性之间的桥梁。我们发现，包含更多离群值的层通常表现出重尾属性，并且根据重尾自我正则化理论，通常训练效果更好。基于这一洞察，我们引入了
    OwLore，一种新颖的微调方法，它对这些离群值丰富的层分配更高的采样概率，采用“富者越富”的方法。这一创新技术在保持较高内存效率的同时提升了微调性能，相比传统的全秩微调具有更高的内存效率。通过引入低秩梯度更新，OwLore
    的内存效率可以进一步提升。我们在包括 LLaMa2、LLaMa3 和 Mistral 在内的各种架构上的实验表明，OwLore 在提升性能的同时，维持了比传统全秩微调更高的内存效率。这些结果突显了
    OwLore 在使复杂语言模型部署更实际和可及，特别是在资源有限的环境中，具有的潜力。我们工作的主要局限性在于对诸如 70 亿参数的超大规模 LLM 的探索有限，建议未来研究的一个方向。
- en: References
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
    E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint
    arXiv:2305.10403, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
    E. Taropa, P. Bailey, Z. Chen, 等. Palm 2 技术报告。arXiv 预印本 arXiv:2305.10403, 2023。'
- en: '[2] D. Biderman, J. G. Ortiz, J. Portes, M. Paul, P. Greengard, C. Jennings,
    D. King, S. Havens, V. Chiley, J. Frankle, et al. Lora learns less and forgets
    less. arXiv preprint arXiv:2405.09673, 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Biderman, J. G. Ortiz, J. Portes, M. Paul, P. Greengard, C. Jennings,
    D. King, S. Havens, V. Chiley, J. Frankle, 等. Lora 学得更少且遗忘更少。arXiv 预印本 arXiv:2405.09673,
    2024。'
- en: '[3] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical
    commonsense in natural language. In Proceedings of the AAAI conference on artificial
    intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Bisk, R. Zellers, J. Gao, Y. Choi, 等. Piqa: 通过自然语言推理物理常识。发表于 AAAI 人工智能会议论文集，第
    34 卷，页 7432–7439, 2020。'
- en: '[4] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Freezeout: Accelerate training
    by progressively freezing layers. arXiv preprint arXiv:1706.04983, 2017.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Brock, T. Lim, J. M. Ritchie, 和 N. Weston. Freezeout: 通过逐步冻结层来加速训练。arXiv
    预印本 arXiv:1706.04983, 2017。'
- en: '[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 33:1877–1901, 2020.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, 等. 语言模型是少样本学习者。神经信息处理系统进展，第 33 卷，第
    1877–1901 页, 2020。'
- en: '[6] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova.
    Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv
    preprint arXiv:1905.10044, 2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, 和 K. Toutanova.
    Boolq: 探索自然的“是/否”问题的惊人难度。arXiv 预印本 arXiv:1905.10044, 2019。'
- en: '[7] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and
    O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
    challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, 和
    O. Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457, 2018。'
- en: '[8] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit
    matrix multiplication for transformers at scale. Advances in Neural Information
    Processing Systems (NeurIPs), 2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer. Llm. int8 (): 用于大规模变换器的
    8 位矩阵乘法。神经信息处理系统进展（NeurIPs），2022。'
- en: '[9] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient
    finetuning of quantized llms. Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer. Qlora: 高效微调量化的大型语言模型。神经信息处理系统进展，36,
    2024。'
- en: '[10] S. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, and T. Zhang. Black-box
    prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531,
    2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, 和 T. Zhang. 预训练语言模型的黑箱提示学习。arXiv
    预印本 arXiv:2201.08531, 2022。'
- en: '[11] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial
    reprogramming. arXiv preprint arXiv:2101.00121, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. Hambardzumyan, H. Khachatrian, 和 J. May. Warp: 单词级对抗重编程。arXiv 预印本 arXiv:2101.00121,
    2021。'
- en: '[12] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified
    view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366,
    2021.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, 和 G. Neubig. 朝向参数高效迁移学习的统一视角。arXiv
    预印本 arXiv:2110.04366, 2021。'
- en: '[13] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt.
    Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300,
    2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, 和 J. Steinhardt.
    测量大规模多任务语言理解。arXiv 预印本 arXiv:2009.03300, 2020。'
- en: '[14] B. M. Hill. A simple general approach to inference about the tail of a
    distribution. The annals of statistics, pages 1163–1174, 1975.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. M. Hill. 关于分布尾部推断的简单通用方法。统计年鉴，页1163–1174, 1975。'
- en: '[15] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning
    for nlp. In International conference on machine learning, pages 2790–2799\. PMLR,
    2019.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
    A. Gesmundo, M. Attariyan, 和 S. Gelly. 自然语言处理中的参数高效迁移学习。在国际机器学习会议上，页2790–2799。PMLR,
    2019。'
- en: '[16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,
    2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen. Lora: 大型语言模型的低秩适应。arXiv 预印本 arXiv:2106.09685, 2021。'
- en: '[17] Z. Hu, L. Wang, Y. Lan, W. Xu, E.-P. Lim, L. Bing, X. Xu, S. Poria, and
    R. K.-W. Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning
    of large language models. arXiv preprint arXiv:2304.01933, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Hu, L. Wang, Y. Lan, W. Xu, E.-P. Lim, L. Bing, X. Xu, S. Poria, 和
    R. K.-W. Lee. LLM-adapters: 用于大规模语言模型的参数高效微调的适配器家族。arXiv 预印本 arXiv:2304.01933,
    2023。'
- en: '[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv
    preprint arXiv:2310.06825, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, 等. Mistral 7b。arXiv
    预印本 arXiv:2310.06825, 2023。'
- en: '[19] W. Jiao, W. Wang, J.-t. Huang, X. Wang, S. Shi, and Z. Tu. Is chatgpt
    a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745,
    2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] W. Jiao, W. Wang, J.-t. Huang, X. Wang, S. Shi, 和 Z. Tu. ChatGPT 是一个好的翻译器吗？是的，使用
    GPT-4 作为引擎。arXiv 预印本 arXiv:2301.08745, 2023。'
- en: '[20] T. Kloek and H. K. Van Dijk. Bayesian estimates of equation system parameters:
    an application of integration by monte carlo. Econometrica: Journal of the Econometric
    Society, pages 1–19, 1978.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Kloek 和 H. K. Van Dijk. 方程系统参数的贝叶斯估计：蒙特卡罗积分的应用。经济计量学：经济计量学会期刊，页1–19,
    1978。'
- en: '[21] J. Kocoń, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydło, J. Baran, J. Bielaniewicz,
    M. Gruza, A. Janz, K. Kanclerz, et al. Chatgpt: Jack of all trades, master of
    none. Information Fusion, 99:101861, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Kocoń, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydło, J. Baran, J.
    Bielaniewicz, M. Gruza, A. Janz, K. Kanclerz, 等. ChatGPT: 万能的助手，无所精通。信息融合, 99:101861,
    2023。'
- en: '[22] D. J. Kopiczko, T. Blankevoort, and Y. M. Asano. Vera: Vector-based random
    matrix adaptation. arXiv preprint arXiv:2310.11454, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. J. Kopiczko, T. Blankevoort, 和 Y. M. Asano. Vera: 基于向量的随机矩阵适应。arXiv
    预印本 arXiv:2310.11454, 2023。'
- en: '[23] O. Kovaleva, S. Kulshreshtha, A. Rogers, and A. Rumshisky. Bert busters:
    Outlier dimensions that disrupt transformers. arXiv preprint arXiv:2105.06990,
    2021.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] O. Kovaleva, S. Kulshreshtha, A. Rogers, 和 A. Rumshisky. Bert 破坏者: 破坏变换器的离群维度。arXiv
    预印本 arXiv:2105.06990, 2021。'
- en: '[24] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient
    prompt tuning. arXiv preprint arXiv:2104.08691, 2021.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] B. Lester, R. Al-Rfou, 和 N. Constant. 参数高效提示调优的规模效应。arXiv 预印本 arXiv:2104.08691,
    2021。'
- en: '[25] S. Li, G. Yuan, Y. Dai, Y. Zhang, Y. Wang, and X. Tang. Smartfrz: An efficient
    training framework using attention-based layer freezing. arXiv preprint arXiv:2401.16720,
    2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Li, G. Yuan, Y. Dai, Y. Zhang, Y. Wang, 和 X. Tang. SmartFRZ: 使用基于注意力的层冻结的高效训练框架。arXiv
    预印本 arXiv:2401.16720, 2024。'
- en: '[26] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for
    generation. arXiv preprint arXiv:2101.00190, 2021.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. L. Li 和 P. Liang. 前缀调优：优化生成的连续提示。arXiv 预印本 arXiv:2101.00190, 2021。'
- en: '[27] V. Lialin, S. Muckatira, N. Shivagunde, and A. Rumshisky. Relora: High-rank
    training through low-rank updates. In Workshop on Advancing Neural Network Training:
    Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS
    2023), 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] V. Lialin, S. Muckatira, N. Shivagunde, 和 A. Rumshisky. Relora: 通过低秩更新进行高秩训练。见于《推进神经网络训练：计算效率、可扩展性和资源优化研讨会》(WANT@
    NeurIPS 2023), 2023。'
- en: '[28] V. Lialin, N. Shivagunde, S. Muckatira, and A. Rumshisky. Stack more layers
    differently: High-rank training through low-rank updates. arXiv preprint arXiv:2307.05695,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] V. Lialin, N. Shivagunde, S. Muckatira, 和 A. Rumshisky. 以不同方式堆叠更多层：通过低秩更新进行高秩训练。arXiv
    预印本 arXiv:2307.05695, 2023。'
- en: '[29] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng,
    and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353,
    2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng,
    和 M.-H. Chen. Dora: 权重分解低秩适配。arXiv 预印本 arXiv:2402.09353, 2024。'
- en: '[30] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands,
    too. arxiv. arXiv preprint arXiv:2103.10385, 2021.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, 和 J. Tang. Gpt也懂。arxiv.
    arXiv 预印本 arXiv:2103.10385, 2021。'
- en: '[31] Y. Liu, S. Agarwal, and S. Venkataraman. Autofreeze: Automatically freezing
    model blocks to accelerate fine-tuning. arXiv preprint arXiv:2102.01386, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Liu, S. Agarwal, 和 S. Venkataraman. Autofreeze: 自动冻结模型块以加速微调。arXiv
    预印本 arXiv:2102.01386, 2021。'
- en: '[32] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient
    multi-task fine-tuning for transformers via shared hypernetworks. arXiv preprint
    arXiv:2106.04489, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] R. K. Mahabadi, S. Ruder, M. Dehghani, 和 J. Henderson. 通过共享超网络实现变压器的参数高效多任务微调。arXiv
    预印本 arXiv:2106.04489, 2021。'
- en: '[33] C. H. Martin and M. W. Mahoney. Traditional and heavy-tailed self regularization
    in neural network models. arXiv preprint arXiv:1901.08276, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. H. Martin 和 M. W. Mahoney. 神经网络模型中的传统和重尾自我正则化。arXiv 预印本 arXiv:1901.08276,
    2019。'
- en: '[34] C. H. Martin and M. W. Mahoney. Heavy-tailed universality predicts trends
    in test accuracies for very large pre-trained deep neural networks. In Proceedings
    of the 2020 SIAM International Conference on Data Mining, pages 505–513\. SIAM,
    2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. H. Martin 和 M. W. Mahoney. 重尾普遍性预测了非常大规模预训练深度神经网络测试准确率的趋势。见于2020年SIAM国际数据挖掘会议论文集，第505–513页。SIAM,
    2020。'
- en: '[35] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural
    networks: Evidence from random matrix theory and implications for learning. Journal
    of Machine Learning Research, 22(165):1–73, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] C. H. Martin 和 M. W. Mahoney. 深度神经网络中的隐式自我正则化：来自随机矩阵理论的证据及其对学习的影响。机器学习研究杂志,
    22(165):1–73, 2021。'
- en: '[36] Meta. Llama3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3),
    2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Meta. Llama3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3),
    2024。'
- en: '[37] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor
    conduct electricity? a new dataset for open book question answering. arXiv preprint
    arXiv:1809.02789, 2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Mihaylov, P. Clark, T. Khot, 和 A. Sabharwal. 装甲可以导电吗？一个新的开放书籍问答数据集。arXiv
    预印本 arXiv:1809.02789, 2018。'
- en: '[38] R. Pan, X. Liu, S. Diao, R. Pi, J. Zhang, C. Han, and T. Zhang. Lisa:
    Layerwise importance sampling for memory-efficient large language model fine-tuning.
    arXiv preprint arXiv:2403.17919, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. Pan, X. Liu, S. Diao, R. Pi, J. Zhang, C. Han, 和 T. Zhang. Lisa: 分层重要性采样用于内存高效的大型语言模型微调。arXiv
    预印本 arXiv:2403.17919, 2024。'
- en: '[39] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with
    gpt-4. arXiv preprint arXiv:2304.03277, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] B. Peng, C. Li, P. He, M. Galley, 和 J. Gao. 使用gpt-4进行指令调优。arXiv 预印本 arXiv:2304.03277,
    2023。'
- en: '[40] G. Puccetti, A. Rogers, A. Drozd, and F. Dell’Orletta. Outliers dimensions
    that disrupt transformers are driven by frequency. arXiv preprint arXiv:2205.11380,
    2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] G. Puccetti, A. Rogers, A. Drozd, 和 F. Dell’Orletta. 破坏变压器的离群点维度由频率驱动。arXiv
    预印本 arXiv:2205.11380, 2022。'
- en: '[41] A. Renduchintala, T. Konuk, and O. Kuchaiev. Tied-lora: Enhacing parameter
    efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Renduchintala, T. Konuk, 和 O. Kuchaiev. Tied-lora: 通过权重绑定提升lora的参数效率。arXiv
    预印本 arXiv:2311.09578, 2023。'
- en: '[42] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An
    adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,
    2021.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Sakaguchi, R. L. Bras, C. Bhagavatula, 和 Y. Choi. Winogrande: 一项大规模对抗性Winograd模式挑战。ACM通讯,
    64(9):99–106, 2021。'
- en: '[43] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. Socialiqa: Commonsense
    reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. Sap, H. Rashkin, D. Chen, R. LeBras, 和 Y. Choi。Socialiqa: 关于社会互动的常识推理。arXiv
    预印本 arXiv:1904.09728, 2019。'
- en: '[44] Y. Sheng, S. Cao, D. Li, C. Hooper, N. Lee, S. Yang, C. Chou, B. Zhu,
    L. Zheng, K. Keutzer, et al. S-lora: Serving thousands of concurrent lora adapters.
    arXiv preprint arXiv:2311.03285, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Sheng, S. Cao, D. Li, C. Hooper, N. Lee, S. Yang, C. Chou, B. Zhu,
    L. Zheng, K. Keutzer, 等人。S-lora: 支持数千个并发的 lora 适配器。arXiv 预印本 arXiv:2311.03285,
    2023。'
- en: '[45] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter. A simple and effective pruning
    approach for large language models. arXiv preprint arXiv:2306.11695, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Sun, Z. Liu, A. Bair, 和 J. Z. Kolter。一种简单而有效的大型语言模型剪枝方法。arXiv 预印本 arXiv:2306.11695,
    2023。'
- en: '[46] N. M. S. Surameery and M. Y. Shakor. Use chat gpt to solve programming
    bugs. International Journal of Information technology and Computer Engineering,
    (31):17–22, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. M. S. Surameery 和 M. Y. Shakor。使用 ChatGPT 解决编程错误。《国际信息技术与计算机工程期刊》，(31):17–22,
    2023。'
- en: '[47] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, and T. F. Bissyandé.
    Is chatgpt the ultimate programming assistant–how far is it? arXiv preprint arXiv:2304.11938,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Tian, W. Lu, T. O. Li, X. Tang, S.-C. Cheung, J. Klein, 和 T. F. Bissyandé。ChatGPT
    是 *终极编程助手* 吗——它离这个目标有多远？arXiv 预印本 arXiv:2304.11938, 2023。'
- en: '[48] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned
    chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale, 等人。Llama 2: 开放的基础和微调的聊天模型。arXiv
    预印本 arXiv:2307.09288, 2023。'
- en: '[49] W. Xia, C. Qin, and E. Hazan. Chain of lora: Efficient fine-tuning of
    language models via residual learning. arXiv preprint arXiv:2401.04151, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] W. Xia, C. Qin, 和 E. Hazan。Lora 链: 通过残差学习高效微调语言模型。arXiv 预印本 arXiv:2401.04151,
    2024。'
- en: '[50] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant:
    Accurate and efficient post-training quantization for large language models. In
    International Conference on Machine Learning, pages 38087–38099\. PMLR, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, 和 S. Han。Smoothquant: 大型语言模型的准确且高效的后训练量化。在国际机器学习会议上，页面
    38087–38099。PMLR，2023。'
- en: '[51] X. Xiao, Z. Li, C. Xie, and F. Zhou. Heavy-tailed regularization of weight
    matrices in deep neural networks. In International Conference on Artificial Neural
    Networks, pages 236–247\. Springer, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Xiao, Z. Li, C. Xie, 和 F. Zhou。在深度神经网络中对权重矩阵进行重尾正则化。在国际人工神经网络会议上，页面
    236–247。Springer，2023。'
- en: '[52] Y. Yang, R. Theisen, L. Hodgkinson, J. E. Gonzalez, K. Ramchandran, C. H.
    Martin, and M. W. Mahoney. Test accuracy vs. generalization gap: Model selection
    in nlp without accessing training or testing data. In Proceedings of the 29th
    ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3011–3021,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Yang, R. Theisen, L. Hodgkinson, J. E. Gonzalez, K. Ramchandran, C.
    H. Martin, 和 M. W. Mahoney。测试准确性与泛化差距: 在 NLP 中选择模型而不访问训练或测试数据。在第29届 ACM SIGKDD
    知识发现与数据挖掘会议的会议录中，页面 3011–3021, 2023。'
- en: '[53] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,
    Y. Liang, Z. Wang, and S. Liu. Outlier weighed layerwise sparsity (owl): A missing
    secret sauce for pruning llms to high sparsity. In International Conference on
    Machine Learning. PMLR., 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,
    Y. Liang, Z. Wang, 和 S. Liu。异常加权层次稀疏 (OWL): 剪枝 LLMs 至高稀疏性的缺失秘密成分。在国际机器学习会议上。PMLR，2024。'
- en: '[54] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag:
    Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi。Hellaswag: 机器真的能完成你的句子吗？arXiv
    预印本 arXiv:1905.07830, 2019。'
- en: '[55] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao.
    Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh
    International Conference on Learning Representations, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, 和 T. Zhao。适应性预算分配用于参数高效的微调。在第十一届国际学习表示会议上，2023。'
- en: '[56] J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian. Galore:
    Memory-efficient llm training by gradient low-rank projection. arXiv preprint
    arXiv:2403.03507, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, 和 Y. Tian。Galore:
    通过梯度低秩投影实现内存高效的 LLM 训练。arXiv 预印本 arXiv:2403.03507, 2024。'
- en: '[57] P. Zhao and T. Zhang. Stochastic optimization with importance sampling
    for regularized loss minimization. In international conference on machine learning,
    pages 1–9\. PMLR, 2015.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. Zhao 和 T. Zhang。通过重要性采样的随机优化用于正则化损失最小化。国际机器学习会议，页码 1–9。PMLR，2015年。'
- en: '[58] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot
    arena. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing 等。使用 mt-bench 和 chatbot arena 评估 llm-as-a-judge。神经信息处理系统进展，36，2024年。'
- en: '[59] Z. Zhong, D. Friedman, and D. Chen. Factual probing is [mask]: Learning
    vs. learning to recall. arXiv preprint arXiv:2104.05240, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Z. Zhong, D. Friedman, 和 D. Chen。事实探测是[mask]：学习与学习记忆。arXiv 预印本 arXiv:2104.05240，2021年。'
- en: '[60] Y. Zhou, T. Pang, K. Liu, M. W. Mahoney, Y. Yang, et al. Temperature balancing,
    layer-wise weight analysis, and neural network training. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Zhou, T. Pang, K. Liu, M. W. Mahoney, Y. Yang 等。温度平衡、层级权重分析和神经网络训练。神经信息处理系统进展，36，2024年。'
- en: Appendix A Pseudocode of GaLore
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A GaLore 的伪代码
- en: Following we present the pseudocode for Galore [[56](#bib.bib56)]. As part of
    the Owlore algorithm, the low-rank updating nature of Galore could help to further
    improve the memory efficiency.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Galore 的伪代码 [[56](#bib.bib56)]。作为 OwLore 算法的一部分，Galore 的低秩更新特性可能有助于进一步提高内存效率。
- en: 'Input: A layer weight matrix $W\in\mathbb{R}^{m\times n}$, decay rates $\beta_{1},\beta_{2}$.Initialize
    first-order moment $M_{0}\in\mathbb{R}^{n\times r}\leftarrow 0$        if *$t\bmod
    T=0$ Initialize left projector as $m\leq n$  $\triangleright$        $M_{t}\leftarrow
    M_{t}/(1-\beta_{1}^{t})$  $\triangleright$*'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：一个层权重矩阵 $W\in\mathbb{R}^{m\times n}$，衰减率 $\beta_{1},\beta_{2}$。初始化一阶矩 $M_{0}\in\mathbb{R}^{n\times
    r}\leftarrow 0$    如果 *$t\bmod T=0$ 初始化左投影器为 $m\leq n$  $\triangleright$    $M_{t}\leftarrow
    M_{t}/(1-\beta_{1}^{t})$  $\triangleright$*
- en: Algorithm 2 GaLore
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 GaLore
- en: Appendix B Hyperparameter Analysis
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 超参数分析
- en: $\tau$ is also crucial to OwLore To obtain intuitive and empirical guidance
    on these hyperparameter choices, we conduct ablation studies using LLaMA2-7B models
    with the GSM-8K dataset and report the results below.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: $\tau$ 对 OwLore 也至关重要。为了获得对这些超参数选择的直观和经验指导，我们使用 LLaMA2-7B 模型和 GSM-8K 数据集进行了消融研究，并报告了以下结果。
- en: 'Table 6: GSM scores for different $\tau$ values'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：不同 $\tau$ 值的 GSM 分数
- en: '| Setting | $\tau=3$ | $\tau=11$ | $\tau=19$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | $\tau=3$ | $\tau=11$ | $\tau=19$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GSM Scores | 19.18 | 19.41 | 20.04 | 20.62 | 21.15 | 20.24 | 20.17 | 20.47
    | 19.79 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| GSM 分数 | 19.18 | 19.41 | 20.04 | 20.62 | 21.15 | 20.24 | 20.17 | 20.47 |
    19.79 |'
- en: We found that mid-range values of $\tau$ for all experiments of OwLore.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现所有 OwLore 实验的 $\tau$ 中等范围值。
- en: 'Table 7: GSM scores/memory usage for different $\gamma$ values'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同 $\gamma$ 值的 GSM 分数/内存使用
- en: '| Setting | $\gamma=1$ | $\gamma=12$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | $\gamma=1$ | $\gamma=12$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OwLore | 20.0/21G | 21.9/22G | 23.5/23G | 25.7/25G | 27.8/27G |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| OwLore | 20.0/21G | 21.9/22G | 23.5/23G | 25.7/25G | 27.8/27G |'
- en: '| LISA | 16.8/23G | 18.8/25G | 19.8/27G | 19.9/32G | 21.7/36G |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| LISA | 16.8/23G | 18.8/25G | 19.8/27G | 19.9/32G | 21.7/36G |'
- en: As for the sampling layer $\gamma$, it is not surprising that performance improves
    consistently with the sampling of more layers. OwLore outperforms LISA with less
    memory usage across all sampling layer counts. This is attributed to OwLore’s
    allocation of higher sampling probabilities to layers abundant in outliers, combined
    with its efficient low-rank gradient updating technique.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 至于采样层 $\gamma$，性能随着更多层的采样持续改进并不令人惊讶。OwLore 在所有采样层数下都优于 LISA，且内存使用更少。这归因于 OwLore
    将更高的采样概率分配给包含更多离群点的层，结合其高效的低秩梯度更新技术。
- en: '![Refer to caption](img/bf4bab4e82318e0cf604ec0afa5f205f.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf4bab4e82318e0cf604ec0afa5f205f.png)'
- en: 'Figure 5: Fine-tuning loss of LLaMA2-7B using method OwLore on the GSM-8K dataset
    with various sampled layers.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 GSM-8K 数据集上使用 OwLore 方法对 LLaMA2-7B 进行微调的损失。
- en: The training curve across different values of $\gamma$ leads to faster convergence
    and lower loss.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 $\gamma$ 值下的训练曲线导致更快的收敛和更低的损失。
- en: Appendix C Hyperparameters Used of OwLore
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C OwLore 使用的超参数
- en: 'Table 8: Hyperparameters used of OwLore for fine-tuning LLaMa2 7B on various
    benchmarks.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：OwLore 用于在各种基准上微调 LLaMa2 7B 的超参数。
- en: '| Hyperparameter | Training Samples | Test Samples | Batch Size | Max Length
    | Training Epochs | Learning Rate |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 训练样本 | 测试样本 | 批量大小 | 最大长度 | 训练轮数 | 学习率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Commonsense Reasoning | 170K | 22.4K | 16 | 512 | 1 | 3e-4 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 常识推理 | 170K | 22.4K | 16 | 512 | 1 | 3e-4 |'
- en: '| MT-Bench | 52K | Alpaca-GPT4 (3.3K) | 16 | 512 | 1 | 3e-4 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench | 52K | Alpaca-GPT4 (3.3K) | 16 | 512 | 1 | 3e-4 |'
- en: '| MMLU | 99.8K | 14K | 16 | 512 | 1 | 3e-4 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 99.8K | 14K | 16 | 512 | 1 | 3e-4 |'
- en: '| GSM8K | 7.4K | 1.3K | 16 | 512 | 1 | 3e-4 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 7.4K | 1.3K | 16 | 512 | 1 | 3e-4 |'
- en: 'Table 9: Hyperparameters used for fine-tuning LLaMa2 7B & LLaMa3 8B on Commonsense
    Reasoning Benchmark.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：用于在常识推理基准上微调 LLaMa2 7B 和 LLaMa3 8B 的超参数。
- en: '| Hyperparameter | Batch Size | Max. Sequence Length | Learning Rate | Scheduler
    | Training Epoch | Warmup Steps | dtype |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 批量大小 | 最大序列长度 | 学习率 | 调度器 | 训练轮数 | 预热步骤 | 数据类型 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMa2 7B | 16 | 512 | 3e-4 | linear | 1 | 100 | bfloat16 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa2 7B | 16 | 512 | 3e-4 | 线性 | 1 | 100 | bfloat16 |'
- en: '| LLaMa3 8B | 16 | 512 | 7e-5 | linear | 1 | 100 | bfloat16 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa3 8B | 16 | 512 | 7e-5 | 线性 | 1 | 100 | bfloat16 |'
