- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Sparse MeZO: 更少参数、更优性能的零阶 LLM 微调'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15751](https://ar5iv.labs.arxiv.org/html/2402.15751)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15751](https://ar5iv.labs.arxiv.org/html/2402.15751)
- en: Yong Liu    Zirui Zhu    Chaoyu Gong    Minhao Cheng    Cho-Jui Hsieh    Yang
    You
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yong Liu    Zirui Zhu    Chaoyu Gong    Minhao Cheng    Cho-Jui Hsieh    Yang
    You
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While fine-tuning large language models (LLMs) for specific tasks often yields
    impressive results, it comes at the cost of memory inefficiency due to back-propagation
    in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently
    proposed to address this issue, only require forward passes during training, making
    them more memory-friendly. However, the quality of gradient estimates in zeroth
    order optimization often depends on the data dimensionality, potentially explaining
    why MeZO still exhibits significant performance drops compared to standard fine-tuning
    across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning
    (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order
    optimization approach that applies ZO only to a carefully chosen subset of parameters.
    We propose a simple yet effective parameter selection scheme that yields significant
    performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized
    implementation for sparse masking, ensuring the algorithm requires only inference-level
    memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100
    GPU. Experimental results illustrate that Sparse-MeZO consistently improves both
    performance and convergence speed over MeZO without any overhead. For example,
    it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the
    RTE task.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对特定任务微调**大规模语言模型**（LLMs）通常能产生令人印象深刻的结果，但由于梯度基础训练中的反向传播，其内存效率较低。最近提出的**内存高效的零阶**（MeZO）优化器，旨在解决这一问题，只需要在训练过程中进行前向传递，使其更具内存友好性。然而，零阶优化中的梯度估计质量通常依赖于数据的维度，这可能解释了为什么
    MeZO 在各项任务中相比于标准微调仍然表现出显著的性能下降。受到参数高效微调（PEFT）成功的启发，本文介绍了 Sparse MeZO，这是一种新颖的内存高效的零阶优化方法，它只对精心挑选的参数子集应用
    ZO。我们提出了一种简单而有效的参数选择方案，带来了显著的性能提升。此外，我们为稀疏掩蔽开发了内存优化实现，确保算法仅需推理级别的内存消耗，使 Sparse-MeZO
    能在单个 A100 GPU 上对 LLaMA-30b 进行微调。实验结果表明，Sparse-MeZO 在性能和收敛速度上都优于 MeZO，且没有任何额外开销。例如，在
    RTE 任务上，它相比于 MeZO 实现了 9% 的绝对准确率提升和 3.5x 的加速。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Fine-tuning large language models for specific tasks or datasets has become
    a prevalent practice in machine learning. However, a major obstacle in fine-tuning
    is the substantial memory requirements, which escalate as models increase in size
    and complexity, thereby limiting the scalability and accessibility for those with
    limited computational resources.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定任务或数据集的**大规模语言模型**的微调已成为机器学习中的一种普遍做法。然而，微调的一个主要障碍是巨大的内存需求，这随着模型的规模和复杂性增加而急剧上升，从而限制了那些计算资源有限的人的可扩展性和可及性。
- en: '![Refer to caption](img/2f406b4b5dd32ad9fef2285700eadccf.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2f406b4b5dd32ad9fef2285700eadccf.png)'
- en: 'Figure 1: Performance of MeZO and Sparse-MeZO (S-MeZO) on RTE task. S-MeZO
    can achieve $3.5$x speedup compared with MeZO.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MeZO 和 Sparse-MeZO (S-MeZO) 在 RTE 任务上的表现。与 MeZO 相比，S-MeZO 能实现 $3.5$x 的加速。
- en: 'To mitigate the memory constraints, Parameter Efficient Fine-Tuning (PEFT)
    has been developed, allowing for the modification of only a subset of parameters
    and achieving comparable results to full model tuning (Zaken et al., [2021](#bib.bib53);
    Li & Liang, [2021](#bib.bib27); Lester et al., [2021](#bib.bib26); Hu et al.,
    [2021](#bib.bib22); Zhang et al., [2023](#bib.bib54)). However, PEFT methods still
    necessitate the calculation of gradients for backpropagation and caching of numerous
    activations during training, which introduces additional memory overhead. For
    instance, [Malladi et al.](#bib.bib34) demonstrates that, even with PEFT, training
    still requires approximately 6 times more memory than the memory cost for inference.
    This discrepancy raises a critical question: Can large language models be fine-tuned
    solely with the cost of inference?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解内存约束，已经开发了参数高效微调（PEFT），允许仅修改一部分参数，并实现与全模型微调相当的结果（Zaken et al., [2021](#bib.bib53);
    Li & Liang, [2021](#bib.bib27); Lester et al., [2021](#bib.bib26); Hu et al.,
    [2021](#bib.bib22); Zhang et al., [2023](#bib.bib54)）。然而，PEFT 方法仍然需要计算梯度以进行反向传播，并在训练过程中缓存大量激活，这带来了额外的内存开销。例如，[Malladi
    et al.](#bib.bib34) 表明，即使使用 PEFT，训练仍然需要大约比推理内存成本多 6 倍的内存。这一差异提出了一个关键问题：大型语言模型是否可以仅以推理成本进行微调？
- en: In response to these challenges, zeroth-order (ZO) optimization presents a promising
    solution (Spall, [1992](#bib.bib41)). ZO optimization is a gradient-free method
    that estimates gradients using only the forward pass of the model, eliminating
    the need for backpropagation and, consequently, reducing memory usage. MeZO (Malladi
    et al., [2023](#bib.bib34)) is a recently proposed zeroth-order method for fine-tuning
    LLMs that has demonstrated impressive performance. However, the performance and
    convergence rate of ZO methods are usually highly dependent on the dimensionality
    of the parameters, with higher dimensionality leading to slower convergence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，零阶（ZO）优化提出了一个有前景的解决方案（Spall, [1992](#bib.bib41)）。ZO 优化是一种无梯度的方法，仅使用模型的前向传播来估计梯度，从而消除了反向传播的需要，并因此减少了内存使用。MeZO（Malladi
    et al., [2023](#bib.bib34)）是一种最近提出的用于微调大型语言模型（LLM）的零阶方法，已显示出令人印象深刻的性能。然而，ZO 方法的性能和收敛速度通常高度依赖于参数的维度，维度越高，收敛速度越慢。
- en: 'Building on the insights from PEFT, we propose a novel sparse memory efficient
    zeroth-order method (Sparse-MeZO) to solve the issues from the high dimensionality
    of LLMs. In traditional parameter-efficient fine-tuning, there’s a tradeoff between
    memory efficiency and performance. But surprisingly, when we only tune a subset
    of parameters in MeZO, it significantly improves the performance while accelerating
    the convergence rate. This renders our method an improvement over MeZO without
    any overhead. Our contributions can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 PEFT 的见解，我们提出了一种新颖的稀疏内存高效零阶方法（Sparse-MeZO）来解决 LLM 高维度带来的问题。在传统的参数高效微调中，存在内存效率与性能之间的权衡。但令人惊讶的是，当我们仅微调
    MeZO 中的一部分参数时，它显著提高了性能，同时加快了收敛速度。这使得我们的方法在没有任何开销的情况下优于 MeZO。我们的贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In this paper, we propose a sparse Memory-Efficient Zeroth-Order optimization
    method Sparse-MeZO (S-MeZO) for large language model fine-tuning. We also provide
    theoretical analysis to show the convergence of Sparse-MeZO.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种稀疏的内存高效零阶优化方法 Sparse-MeZO（S-MeZO）用于大型语言模型微调。我们还提供了理论分析来展示 Sparse-MeZO
    的收敛性。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To determine the sparse parameters that need to be updated, we investigate how
    the importance of each parameter in fine-tuning correlates with its magnitude.
    Although existing sparsification methods usually prune out parameters with smaller
    magnitudes, our evaluations show that smaller parameters are more important than
    larger weights for Zeroth-order fine-tuning methods.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了确定需要更新的稀疏参数，我们研究了每个参数在微调中的重要性与其大小的关系。尽管现有的稀疏化方法通常会修剪掉较小的参数，但我们的评估表明，对于零阶微调方法，较小的参数比较大的权重更为重要。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different from the efficient implementation with random seed in MeZO, we propose
    a novel memory-efficient implementation of Sparse-MeZO, which can compute the
    sparse mask and perturb parameters in the forward pass. The technique enables
    fine-tuning LLaMA-30b with Sparse-MeZO on a single A100 GPU.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 MeZO 中使用随机种子的高效实现不同，我们提出了一种新颖的内存高效的 Sparse-MeZO 实现，该实现可以在前向传播中计算稀疏掩码和扰动参数。该技术使得可以在单个
    A100 GPU 上对 LLaMA-30b 进行 Sparse-MeZO 微调。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We conduct empirical studies on LLaMA and OPT. The experimental results demonstrate
    that Sparse-MeZO can improve the fine-tuning performance and yield a faster convergence
    rate compared with vanilla MeZO across a wide range of natural language processing
    tasks. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup
    over MeZO on the RTE task, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们对 LLaMA 和 OPT 进行了实证研究。实验结果表明，Sparse-MeZO 相比于普通 MeZO 能够提高微调性能，并在各种自然语言处理任务中实现更快的收敛速度。例如，在
    RTE 任务上，它相较于 MeZO 实现了 9% 的绝对准确率提升和 3.5 倍的速度提升，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning")
    所示。'
- en: 2 Preliminaries
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: 2.1 Parameter-Efficient Fine-Tuning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 参数高效微调
- en: Parameter-Efficient Fine-Tuning (PEFT) is designed to facilitate efficient adaptation
    by updating only a subset of the model’s parameters, rather than fine-tuning the
    entire model (Hu et al., [2021](#bib.bib22); Zaken et al., [2021](#bib.bib53)).
    These PEFT approaches can be categorized in various ways. We mainly focus on the
    selective methods and additive methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）旨在通过仅更新模型参数的子集来实现高效适配，而不是微调整个模型（Hu 等，[2021](#bib.bib22)；Zaken 等，[2021](#bib.bib53)）。这些
    PEFT 方法可以以不同方式进行分类。我们主要关注选择性方法和加法方法。
- en: 2.1.1 Selective Methods
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 选择性方法
- en: Selective Methods try to selectively fine-tune a portion of a model and these
    methods have been explored in various studies. For example, [Zaken et al.](#bib.bib53);
    [Cai et al.](#bib.bib5) focused on the model’s bias terms, finding that fine-tuning
    these terms alone could rival the results of fine-tuning the entire model. However,
    the effectiveness of this approach diminishes with larger datasets, as shown in
    further analysis by [Zaken et al.](#bib.bib53). Beyond static parameter adjustments,
    there has been an exploration into dynamically modifying parts of the model. For
    instance, FreezeOut, introduced by [Brock et al.](#bib.bib3), suggests a gradual
    freezing of layers, effectively removing early layers from the backward pass to
    quicken training. This concept was later applied to language models, with AutoFreeze
    (Liu et al., [2021](#bib.bib33)) confirming its viability. Nevertheless, these
    techniques still demand considerable computational resources and tend to yield
    less optimal final outcomes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性方法尝试选择性地微调模型的一部分，这些方法在各种研究中有所探索。例如，[Zaken 等](#bib.bib53)；[Cai 等](#bib.bib5)
    关注模型的偏差项，发现单独微调这些项的结果可以与微调整个模型相媲美。然而，随着数据集的增大，这种方法的有效性会减弱，正如 [Zaken 等](#bib.bib53)
    的进一步分析所示。除了静态参数调整，还探索了动态修改模型部分的方法。例如，[Brock 等](#bib.bib3) 提出的 FreezeOut 建议逐渐冻结层，从而有效地将早期层从反向传播中移除以加快训练。该概念后来应用于语言模型中，AutoFreeze（Liu
    等，[2021](#bib.bib33)）证实了其可行性。然而，这些技术仍然需要相当大的计算资源，并且往往会产生不那么理想的最终结果。
- en: 2.1.2 Additive Methods
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 加法方法
- en: Additive methods, as an alternative to updating existing parameters, involve
    incorporating new layers into models, with the fine-tuning process focusing solely
    on these added layers (Houlsby et al., [2019](#bib.bib21); Rebuffi et al., [2017](#bib.bib37);
    Lin et al., [2020](#bib.bib28); Hu et al., [2021](#bib.bib22)). Traditional techniques
    in this category, such as adapters (Houlsby et al., [2019](#bib.bib21)), implemented
    layer additions in a sequential manner, which unfortunately led to increased inference
    latency. LoRA (Hu et al., [2021](#bib.bib22)) has been proposed to mitigate this
    issue, which freezes the weights of the pre-trained model and introduces trainable
    matrices based on rank decomposition into each layer. Then, it can directly integrate
    the newly learned weights into the main model. Following this, IA3 (Liu et al.,
    [2022](#bib.bib29)) introduced innovative methods for adding parameters, balancing
    parameter count with accuracy, while LST (Sung et al., [2022](#bib.bib45)) introduced
    a highway structure that learns only small, auxiliary channels, aiming to decrease
    memory demands. Despite these advancements, additive methods generally require
    meticulous design, and many fail to reduce the computational load during the backward
    pass, as seen in IA3, LoRA, and Adapter approaches. Additionally, some of these
    methods, including Adapter and LST, can incur additional computational overhead
    in the forward pass, making them less practical for certain applications.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 加法方法作为更新现有参数的替代方案，涉及在模型中加入新的层，微调过程仅专注于这些新增层（Houlsby et al., [2019](#bib.bib21);
    Rebuffi et al., [2017](#bib.bib37); Lin et al., [2020](#bib.bib28); Hu et al.,
    [2021](#bib.bib22)）。这一类别的传统技术，如适配器（Houlsby et al., [2019](#bib.bib21)），以顺序方式实现层的增加，但不幸的是导致了推理延迟的增加。LoRA（Hu
    et al., [2021](#bib.bib22)）被提出以缓解这个问题，它冻结了预训练模型的权重，并在每一层中引入基于秩分解的可训练矩阵。随后，它可以将新学习的权重直接集成到主模型中。随后，IA3（Liu
    et al., [2022](#bib.bib29)）引入了创新的方法来添加参数，平衡参数数量与准确性，而LST（Sung et al., [2022](#bib.bib45)）引入了一种只学习小型辅助通道的高速公路结构，旨在减少内存需求。尽管有这些进展，加法方法通常需要精心设计，并且许多方法未能减少反向传播过程中的计算负载，如IA3、LoRA和适配器方法所示。此外，其中一些方法，包括适配器和LST，可能会在前向传播过程中产生额外的计算开销，使其在某些应用中不够实用。
- en: 2.2 Zeroth-Order Optimization
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 零阶优化
- en: Unlike traditional gradient-based optimization methods that rely on derivatives
    to guide the search for optimal solutions, zeroth-order (ZO) optimization techniques
    do not require derivatives for optimization (Spall, [1992](#bib.bib41); Liu et al.,
    [2018](#bib.bib30), [2019](#bib.bib31)). These methods utilize only the value
    of the objective function, denoted as $f(\boldsymbol{x})$ and $f(\boldsymbol{x}-\epsilon\boldsymbol{z})$
    being a minimal value. Following this, conventional optimization algorithms, such
    as gradient descent or coordinate descent, are implemented using these approximated
    gradient values. Currently, ZO methods have been widely used in various applications,
    such as adversarial attack and defense (Chen et al., [2017](#bib.bib9); Tu et al.,
    [2019](#bib.bib47); Ye et al., [2018](#bib.bib52); Ilyas et al., [2018](#bib.bib23)),
    Auto-ML (Ruan et al., [2019](#bib.bib39); Wang et al., [2022](#bib.bib50)), natural
    language processing (Sun et al., [2022a](#bib.bib43), [b](#bib.bib44)), reinforcement
    learning (Vemula et al., [2019](#bib.bib48)), Signal Processing (Liu et al., [2020](#bib.bib32)),
    and on-chip training (Gu et al., [2021](#bib.bib15)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的基于梯度的优化方法不同，这些方法依赖于导数来引导搜索最优解，零阶（ZO）优化技术不需要导数进行优化（Spall, [1992](#bib.bib41);
    Liu et al., [2018](#bib.bib30), [2019](#bib.bib31)）。这些方法仅利用目标函数的值，记作 $f(\boldsymbol{x})$
    和 $f(\boldsymbol{x}-\epsilon\boldsymbol{z})$ 为最小值。随后，使用这些近似的梯度值实现传统优化算法，如梯度下降或坐标下降。目前，ZO
    方法已经在各种应用中得到广泛使用，如对抗攻击与防御（Chen et al., [2017](#bib.bib9); Tu et al., [2019](#bib.bib47);
    Ye et al., [2018](#bib.bib52); Ilyas et al., [2018](#bib.bib23)），Auto-ML（Ruan
    et al., [2019](#bib.bib39); Wang et al., [2022](#bib.bib50)），自然语言处理（Sun et al.,
    [2022a](#bib.bib43), [b](#bib.bib44)），强化学习（Vemula et al., [2019](#bib.bib48)），信号处理（Liu
    et al., [2020](#bib.bib32)），以及芯片内训练（Gu et al., [2021](#bib.bib15)）。
- en: 2.2.1 SPSA
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 SPSA
- en: 'Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, [1992](#bib.bib41))
    is a descent method that optimizes systems with multiple unknown parameters to
    identify global minima without relying on backpropagation. SPSA estimates gradients
    based on two measurements of the objective function, thus eliminating the need
    for automatic differentiation and backpropagation to obtain precise gradients.
    This approach notably avoids memory overhead associated with storing intermediate
    results for backpropagation. Specially, it employs a random vector $\boldsymbol{z}\in\mathbb{R}^{d}$:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同时扰动随机近似（SPSA）(Spall, [1992](#bib.bib41))是一种优化具有多个未知参数的系统的方法，用于识别全局最小值，而不依赖于反向传播。SPSA基于目标函数的两个测量来估计梯度，从而消除获取精确梯度所需的自动微分和反向传播。这种方法显著避免了与存储反向传播中间结果相关的内存开销。特别地，它采用了一个随机向量$\boldsymbol{z}\in\mathbb{R}^{d}$：
- en: '|  | $\displaystyle\boldsymbol{g_{z}(\theta)}$ |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{g_{z}(\theta)}$ |  | (1) |'
- en: '|  |  | $\displaystyle\approx\boldsymbol{z}\boldsymbol{z}^{\top}g(\boldsymbol{\theta}).$
    |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\boldsymbol{z}\boldsymbol{z}^{\top}g(\boldsymbol{\theta}).$
    |  |'
- en: 2.2.2 MeZO
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 MeZO
- en: ZO-SGD employs SPSA to estimate the gradient. In general, conventional ZO-SGD
    algorithms utilizing SPSA consume twice the inference memory. MeZO (Malladi et al.,
    [2023](#bib.bib34)) is a memory-efficient variant of ZO-SGD. It circumvents the
    storage of gradients by saving the random seed and resampling the same random
    noise $\boldsymbol{z}$ with the seed during forward process. Therefore, it can
    reduce the training costs close to inference memory cost.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD使用SPSA来估计梯度。通常，利用SPSA的传统ZO-SGD算法会消耗两倍的推理内存。MeZO (Malladi et al., [2023](#bib.bib34))是一种内存高效的ZO-SGD变体。它通过保存随机种子并在前向过程中用该种子重新采样相同的随机噪声$\boldsymbol{z}$来避免梯度的存储。因此，它可以将训练成本减少到接近推理内存成本。
- en: 'We try to further explain how MeZO can be memory-efficient in Figure [2](#S2.F2
    "Figure 2 ‣ 2.2.2 MeZO ‣ 2.2 Zeroth-Order Optimization ‣ 2 Preliminaries ‣ Sparse
    MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").
    At step 1, it will sample a noise $\boldsymbol{z}$. At step 2, MeZO can resample
    the same noise $\boldsymbol{z}$. Then, it can use $\boldsymbol{\theta_{t}^{\prime}}-2\epsilon\boldsymbol{z}$.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们试图在图[2](#S2.F2 "Figure 2 ‣ 2.2.2 MeZO ‣ 2.2 Zeroth-Order Optimization ‣ 2
    Preliminaries ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning")中进一步解释MeZO如何在内存上高效。在步骤1中，它将采样一个噪声$\boldsymbol{z}$。在步骤2中，MeZO可以重新采样相同的噪声$\boldsymbol{z}$。然后，它可以使用$\boldsymbol{\theta_{t}^{\prime}}-2\epsilon\boldsymbol{z}$。'
- en: '![Refer to caption](img/77f014fa9a19face1aeccabffff629c0.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77f014fa9a19face1aeccabffff629c0.png)'
- en: 'Figure 2: Memory Efficient Implementation in MeZO. At step 1, we will sample
    a random noise $\boldsymbol{z}$ with same $s_{t}$.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图2: MeZO中的内存高效实现。在步骤1中，我们将以相同的$s_{t}$采样一个随机噪声$\boldsymbol{z}$。'
- en: 2.2.3 Sparsity for Zeroth-order Optimization
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 零阶优化的稀疏性
- en: The hypothesis proposed by [Frankle & Carbin](#bib.bib13), known as the lottery
    ticket hypothesis, showed that within a densely connected neural network that
    is randomly initialized, there exists a subnetwork of sparse yet high-quality
    connections. Several related works have tried to apply the sparsity to zeroth-order
    optimization (Wang et al., [2018](#bib.bib51); Cai et al., [2022](#bib.bib7),
    [2021](#bib.bib6); Balasubramanian & Ghadimi, [2018](#bib.bib1); Ohta et al.,
    [2020](#bib.bib35); Gu et al., [2021](#bib.bib15); Chen et al., [2023](#bib.bib8)).
    For example, DeepZero proposes a novel ZO training protocol with model pruning
    guided sparsity. However, these methods mainly focus on the neural network training
    from scratch with random initialization, while the application of sparse zeroth-order
    optimization in fine-tuning tasks remains an area of ongoing exploration. In this
    paper, we will analyze the effects of sparse ZO methods on large language model
    fine-tuning tasks and propose a novel sparse zeroth-order method to obtain a better
    performance and faster convergence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Frankle & Carbin](#bib.bib13)提出的假设，被称为彩票票据假设，表明在一个随机初始化的密集连接神经网络中，存在一个稀疏但高质量的连接子网络。若干相关工作尝试将稀疏性应用于零阶优化（Wang
    et al., [2018](#bib.bib51); Cai et al., [2022](#bib.bib7), [2021](#bib.bib6);
    Balasubramanian & Ghadimi, [2018](#bib.bib1); Ohta et al., [2020](#bib.bib35);
    Gu et al., [2021](#bib.bib15); Chen et al., [2023](#bib.bib8)）。例如，DeepZero提出了一种新颖的ZO训练协议，结合了模型修剪指导的稀疏性。然而，这些方法主要关注从头开始的神经网络训练，而在微调任务中稀疏零阶优化的应用仍然是一个持续探索的领域。在本文中，我们将分析稀疏ZO方法对大语言模型微调任务的影响，并提出一种新颖的稀疏零阶方法，以获得更好的性能和更快的收敛速度。'
- en: 3 Proposed Method
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提议的方法
- en: Parameter-Efficient Fine-Tuning (PEFT) has been widely used in various areas,
    which only updates a fraction of full parameters and can yield comparable performance.
    In addition, it’s widely recognized that both the performance and the convergence
    rate are significantly influenced by the dimensionality of the weights to ZO methods.
    For example, [Duchi et al.](#bib.bib12) illustrates that an increase in dimensionality
    tends to proportionally slow down the convergence rate. These motivate us to propose
    a sparse zeroth-order optimization method for large language model fine-tuning,
    which only updates a subset of full parameters and yields a comparable performance
    and faster convergence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）已广泛应用于各个领域，它只更新全部参数的一部分，并且可以达到相当的性能。此外，广泛认为性能和收敛速度都受到 ZO 方法的权重维度的显著影响。例如，[Duchi
    等](#bib.bib12) 说明维度的增加往往会成比例地减慢收敛速度。这些动机促使我们提出了一种用于大型语言模型微调的稀疏零阶优化方法，它只更新全部参数的一个子集，并且可以达到相当的性能和更快的收敛速度。
- en: 3.1 Sparse-MeZO
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 稀疏-MeZO
- en: 'Consider a labelled dataset $\mathcal{D}=\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}_{i\in\mathcal{[|D|]}}$
    to selectively sample the random noise $\boldsymbol{z}\in\mathbb{R}^{d}\text{
    with }\boldsymbol{z}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{d})$:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个标记数据集 $\mathcal{D}=\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}_{i\in\mathcal{[|D|]}}$
    来选择性地采样随机噪声 $\boldsymbol{z}\in\mathbb{R}^{d}\text{ 其中 } \boldsymbol{z}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{d})$：
- en: '|  | $\boldsymbol{\hat{z}}=\boldsymbol{m}\odot\boldsymbol{z}.$ |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\hat{z}}=\boldsymbol{m}\odot\boldsymbol{z}.$ |  | (2) |'
- en: 'Based on this sparse perturbation $\boldsymbol{\hat{z}}$, which can be defined
    as :'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种稀疏扰动 $\boldsymbol{\hat{z}}$，可以定义为：
- en: '|  | $\displaystyle\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$ |  | (3)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$ |  | (3)
    |'
- en: '|  |  | $\displaystyle=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})}{2\epsilon},$
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{m}\odot\boldsymbol{z};\mathcal{B})}{2\epsilon},$
    |  |'
- en: where $\epsilon$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$。
- en: 3.2 Convergence Analysis of Sparse-MeZO
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 稀疏-MeZO 的收敛分析
- en: In this section, we will explain why Sparse-MeZO can accelerate the convergence.
    We can define a sub-network in pre-trained large language models, which is determined
    by the sparse mask $\boldsymbol{m}$ is the number of parameters in the sub-network.
    Therefore, ZO can use fewer steps to converge when we only focus on a sub-network.
    Some related work has illustrated that only tuning the sub-network can achieve
    comparable performance, which will be empirically verified in our experiments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释为什么 Sparse-MeZO 可以加速收敛。我们可以在预训练的大型语言模型中定义一个子网络，该子网络由稀疏掩码 $\boldsymbol{m}$
    决定，是子网络中的参数数量。因此，当我们只关注一个子网络时，ZO 可以用更少的步骤收敛。一些相关工作已经表明，仅调节子网络可以实现相当的性能，这将在我们的实验中得到实证验证。
- en: 'Firstly, we assume the loss function $\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$
    is Lipschitz Continuous:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设损失函数 $\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$ 是 Lipschitz 连续的：
- en: Assumption 3.1  (Lipschitz Continuous).
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 3.1  （Lipschitz 连续）。
- en: '|  | $\&#124;\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})-\nabla\mathcal{L}(\boldsymbol{\theta^{\prime}},\boldsymbol{x})\&#124;\leq\frac{L(l)}{2}\&#124;\boldsymbol{\theta}-\boldsymbol{\theta^{\prime}}\&#124;^{2},$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})-\nabla\mathcal{L}(\boldsymbol{\theta^{\prime}},\boldsymbol{x})\&#124;\leq\frac{L(l)}{2}\&#124;\boldsymbol{\theta}-\boldsymbol{\theta^{\prime}}\&#124;^{2},$
    |  | (4) |'
- en: 'where $\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$ represents the
    Lipschitz constant of $\mathcal{L}(\cdot)$:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla\mathcal{L}(\boldsymbol{\theta};\boldsymbol{x})$ 代表 $\mathcal{L}(\cdot)$
    的 Lipschitz 常数：
- en: Lemma 3.2.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3.2。
- en: 'ZO gradient $\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ZO 梯度 $\boldsymbol{g_{\hat{z}}(\boldsymbol{\theta})}$：
- en: '|  | $\displaystyle\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\boldsymbol{\theta}}})$
    |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\boldsymbol{\theta}}})$
    |  | (5) |'
- en: '|  |  | $\displaystyle=\boldsymbol{m}\odot\nabla_{\boldsymbol{\boldsymbol{\theta}}}\mathbb{E}_{\boldsymbol{\hat{z}}}[\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})]$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\boldsymbol{m}\odot\nabla_{\boldsymbol{\boldsymbol{\theta}}}\mathbb{E}_{\boldsymbol{\hat{z}}}[\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}]$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\boldsymbol{g_{\hat{z}}(\theta)}],$
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\boldsymbol{\hat{z}}}[\boldsymbol{g_{\hat{z}}(\theta)}],$
    |  |'
- en: 'where $\boldsymbol{g_{\hat{z}}(\theta)}=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}$
    in Lemma [5](#S3.E5 "Equation 5 ‣ Lemma 3.2\. ‣ 3.2 Convergence Analysis of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we can use the distance $\|\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\theta})-\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\boldsymbol{\theta}})\|$:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{g_{\hat{z}}(\theta)}=\frac{\mathcal{L}(\boldsymbol{\theta}+\epsilon\boldsymbol{\hat{z}})-\mathcal{L}(\boldsymbol{\theta}-\epsilon\boldsymbol{\hat{z}})}{2\epsilon}\boldsymbol{\hat{z}}$
    在引理 [5](#S3.E5 "方程 5 ‣ 引理 3.2\. ‣ 3.2 Sparse-MeZO 的收敛分析 ‣ 3 提议的方法 ‣ Sparse MeZO：在零阶
    LLM 微调中较少参数以获得更好的性能") 中，我们可以使用距离 $\|\widehat{\nabla}_{\boldsymbol{\theta}}\mathcal{L}_{\hat{z}}(\boldsymbol{\theta})-\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\boldsymbol{\theta}})\|$：
- en: Lemma 3.3.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3.3。
- en: 'Let $\mathcal{L}$ be Lipschitz Continuous, we have:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathcal{L}$ 为 Lipschitz 连续的，我们得到：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta})=\boldsymbol{m}\odot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$
    represents the Lipschitz constant. Finally, we can obtain the convergence rate
    of Sparse-MeZO.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta})=\boldsymbol{m}\odot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$
    表示 Lipschitz 常数。最终，我们可以获得 Sparse-MeZO 的收敛速度。
- en: Theorem 3.4.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.4。
- en: 'Assuming a sequence of generated parameters $\{\boldsymbol{\theta_{t}}\}_{t\geq
    0}$ in Sparse-MeZO. We can have:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 Sparse-MeZO 中生成的参数序列为 $\{\boldsymbol{\theta_{t}}\}_{t\geq 0}$。我们可以得到：
- en: '|  | $\mathbb{E}_{\hat{z},x}[\&#124;\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta_{T}})\&#124;^{2}]\leq\sigma^{2}$
    |  | (7) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{\hat{z},x}[\&#124;\nabla_{\boldsymbol{\theta}}\mathcal{L}_{m}(\boldsymbol{\theta_{T}})\&#124;^{2}]\leq\sigma^{2}$
    |  | (7) |'
- en: for any $T=\mathcal{O}(\frac{\hat{d}L}{\sigma^{2}})$
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 $T=\mathcal{O}(\frac{\hat{d}L}{\sigma^{2}})$
- en: where $L(l)\leq L$. This theorem illustrates that the presence of pronounced
    sparsity patterns, along with the smoothness of the objective function, can significantly
    enhance the rate of convergence, potentially achieving a linear acceleration.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L(l)\leq L$。该定理说明，明显的稀疏模式以及目标函数的平滑性，可以显著提升收敛速度，潜在地实现线性加速。
- en: 3.3 Less Parameters for Better Performance
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 较少的参数以获得更好的性能
- en: From the analysis above, we find that Sparse-MeZO can speed up the convergence
    of zeroth-order optimization. After that, the main problem is how to determine
    the sparse mask $\boldsymbol{m}$. Network pruning is a popular method to prune
    the unimportant parameters and only use these selected important parameters to
    make the inference process fast and efficient (LeCun et al., [1989](#bib.bib25);
    Hanson & Pratt, [1988](#bib.bib19); Hassibi & Stork, [1992](#bib.bib20); Ström,
    [1997](#bib.bib42); Han et al., [2015a](#bib.bib17), [b](#bib.bib18)). Specifically,
    it typically removes connections with small parameters and retains those with
    large values. Drawing from these recent studies in network pruning, we first explore
    how the value of parameters affects zeroth-order optimization. This leads us to
    examine if perturbations to larger parameters are more critical.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述分析中，我们发现 Sparse-MeZO 可以加速零阶优化的收敛。之后，主要问题是如何确定稀疏掩码 $\boldsymbol{m}$。网络剪枝是一种流行的方法，用于剪除不重要的参数，并仅使用这些选定的重要参数来使推理过程更快更高效（LeCun
    等，[1989](#bib.bib25)；Hanson & Pratt，[1988](#bib.bib19)；Hassibi & Stork，[1992](#bib.bib20)；Ström，[1997](#bib.bib42)；Han
    等，[2015a](#bib.bib17)，[b](#bib.bib18)）。具体而言，它通常去除小参数的连接，保留大值的连接。借鉴这些近期的网络剪枝研究，我们首先探讨参数值如何影响零阶优化。这使我们检查对大参数的扰动是否更为关键。
- en: Is the perturbation on larger parameters more important than smaller parameters
    ?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大参数的扰动是否比小参数的扰动更重要？
- en: 'We conduct experiments to examine how parameter values of varying magnitudes
    impact performance. Specifically, we set the median value of each layer as a threshold
    to separate parameters into two groups. Then, we only perturb and update the parameters
    in one of these groups. For instance, we experiment with updating either just
    the smaller or larger parameters, using the same settings as the standard MeZO,
    on SuperGLUE. Then, we compared their performance to that of vanilla MeZO. The
    results in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Less Parameters for Better Performance
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning") show that updating only the parameters smaller than the median
    value leads to better performance on BoolQ, RTE, WIC, and comparable results on
    SST-2\. However, updating only the larger parameters results in significantly
    worse performance compared to the standard MeZO. This suggests that the key to
    MeZO’s performance gain lies in the smaller weights.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行实验以检查不同幅度的参数值对性能的影响。具体来说，我们将每层的中位数值设置为阈值，以将参数分为两组。然后，我们只对其中一组的参数进行扰动和更新。例如，我们在
    SuperGLUE 上尝试使用相同设置更新较小或较大的参数，与标准 MeZO 相比，结果如图 [3](#S3.F3 "Figure 3 ‣ 3.3 Less
    Parameters for Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters
    for Better Performance in Zeroth-Order LLM Fine-Tuning") 所示，仅更新小于中位数值的参数会在 BoolQ、RTE
    和 WIC 上获得更好的性能，在 SST-2 上的结果相当。然而，仅更新较大的参数的性能则显著差于标准 MeZO。这表明 MeZO 性能提升的关键在于较小的权重。'
- en: The reason updating only smaller parameters leads to better performance might
    be due to the larger weights being well-trained already. Moving them in a random
    direction is unlikely to enhance performance. On the other hand, since smaller
    weights are less trained, there’s a higher chance of improvement regardless of
    the direction of the update.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更新仅较小参数能获得更好性能的原因可能是较大的权重已经训练得很好。将它们随机移动不太可能提升性能。另一方面，由于较小的权重训练较少，无论更新方向如何，都有更大的改进机会。
- en: '![Refer to caption](img/dd1e955e98442e24c513f8e976907036.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd1e955e98442e24c513f8e976907036.png)'
- en: 'Figure 3: Comparing performance across various parameter adjustments. MeZO:
    the vanilla MeZO approach, MeZO-S: Fine-Tuning smaller parameters with MeZO, MeZO-L:
    Fine-Tuning larger parameters with MeZO.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：比较各种参数调整下的性能。MeZO：标准 MeZO 方法，MeZO-S：用 MeZO 对较小参数进行微调，MeZO-L：用 MeZO 对较大参数进行微调。
- en: 'Based on the observations from Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Less Parameters
    for Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for
    Better Performance in Zeroth-Order LLM Fine-Tuning"), we can create a sparse mask,
    $\boldsymbol{m}$. It’s important to note that we still preserve the complete set
    of parameters, but we apply sparse perturbations and gradient estimations only
    to the selected ones. This approach allows us to integrate the sparse mask into
    the standard MeZO method as a straightforward, adaptable tool. Then, we will introduce
    when and where to calculate the mask.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图 [3](#S3.F3 "Figure 3 ‣ 3.3 Less Parameters for Better Performance ‣ 3 Proposed
    Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
    Fine-Tuning") 的观察，我们可以创建一个稀疏掩码，$\boldsymbol{m}$。需要注意的是，我们仍然保留完整的参数集，但仅对选定的参数应用稀疏扰动和梯度估计。这种方法使我们能够将稀疏掩码整合到标准
    MeZO 方法中，作为一个直接、适应性强的工具。接下来，我们将介绍何时以及如何计算掩码。'
- en: $\bullet$
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Constant Mask: Setting the Mask Before Training. We compare the parameter values
    to a threshold for each layer to set the mask before training begins. However,
    a significant downside of this approach is the extra memory required to store
    a sparse mask, which is as large as the pre-trained model itself. Our goal is
    for our method to enhance performance without using more GPU memory or causing
    extra overhead.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常量掩码：在训练前设置掩码。我们通过将参数值与每层的阈值进行比较，以在训练开始前设置掩码。然而，这种方法的一个显著缺点是需要额外的内存来存储一个与预训练模型本身一样大的稀疏掩码。我们的目标是使我们的方法在不使用更多
    GPU 内存或造成额外开销的情况下提升性能。
- en: $\bullet$
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Dynamic Mask: Determining Mask at Each Iteration. We can establish a threshold
    for each layer before training and then generate the mask by comparing parameter
    values to this threshold during each iteration. This method avoids the necessity
    of storing a large mask, $\boldsymbol{m}$.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态掩码：在每次迭代时确定掩码。我们可以在训练前为每一层建立一个阈值，然后在每次迭代中通过将参数值与此阈值进行比较来生成掩码。这种方法避免了存储大规模掩码
    $\boldsymbol{m}$ 的必要性。
- en: Algorithm 1 Sparse-MeZO (S-MeZO)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 稀疏-MeZO (S-MeZO)
- en: 'Require: $\boldsymbol{\theta}$ represents sparsification interval.  Initialize
    random seed $s$ do     Sample Minibatch $\mathcal{B}$     $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta_{t}},\epsilon,s,\boldsymbol{m})$     $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta},\epsilon,s,\boldsymbol{m})$ do        $z_{i}\sim\mathcal{N}(0,1)$     end for  end for'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 需求：$\boldsymbol{\theta}$ 代表稀疏化间隔。初始化随机种子 $s$  执行  采样小批量 $\mathcal{B}$  $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta_{t}},\epsilon,s,\boldsymbol{m})$  $\boldsymbol{\theta_{t}}\leftarrow\text{PerturbParameters}(\boldsymbol{\theta},\epsilon,s,\boldsymbol{m})$  执行  $z_{i}\sim\mathcal{N}(0,1)$  结束
    for  结束 for
- en: In this paper, we’ll employ a dynamic mask to choose which parameters to perturb
    and update, addressing the issue of memory constraints.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将采用动态掩码来选择需要扰动和更新的参数，以解决内存限制问题。
- en: 'The pseudo-code is provided in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Less
    Parameters for Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters
    for Better Performance in Zeroth-Order LLM Fine-Tuning"). This algorithm outlines
    that we first establish the threshold $h_{i}$ and apply the mask $\boldsymbol{m}$
    to get new parameters $\boldsymbol{\theta_{t}+\epsilon\hat{z}}$. From these losses,
    we calculate the estimated sparse gradient $\boldsymbol{g_{m}}(\boldsymbol{\theta_{t}})=\text{proj\_grad}*\boldsymbol{\hat{z}}$.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码见算法 [1](#alg1 "算法 1 ‣ 3.3 更少的参数以获得更好的性能 ‣ 3 提议的方法 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数以获得更好的性能")。该算法概述了我们首先建立阈值
    $h_{i}$ 并应用掩码 $\boldsymbol{m}$ 来获得新的参数 $\boldsymbol{\theta_{t}+\epsilon\hat{z}}$。从这些损失中，我们计算估计的稀疏梯度
    $\boldsymbol{g_{m}}(\boldsymbol{\theta_{t}})=\text{proj\_grad}*\boldsymbol{\hat{z}}$。
- en: Algorithm 2 PerturbParameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 PerturbParameters
- en: 'Input: $\boldsymbol{\theta}$.  Reset random seed $s$  end for'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\boldsymbol{\theta}$。重置随机种子 $s$  结束 for
- en: Algorithm 3 GetMask
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 GetMask
- en: 'Input: $\boldsymbol{\theta}$  for $i\leftarrow$ then           $\theta_{i,j}=1$        end if     end for  end for'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\boldsymbol{\theta}$  对 $i\leftarrow$ 然后 $\theta_{i,j}=1$  结束 if  结束 for  结束
    for
- en: 3.4 Memory-Efficient Implementation of Sparse-MeZO
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 稀疏 MeZO 的内存高效实现
- en: 'In this paper, our primary aim is to introduce an efficient method for fine-tuning
    language models using zeroth-order optimization, enhancing performance on downstream
    tasks. As outlined in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Less Parameters for
    Better Performance ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better
    Performance in Zeroth-Order LLM Fine-Tuning"), our approach involves perturbing
    the parameters $\boldsymbol{\theta_{t}}$. This step typically requires storing
    two separate sets of parameters, leading to increased memory usage during fine-tuning.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的主要目标是引入一种高效的零阶优化方法来微调语言模型，从而提高下游任务的性能。如算法 [1](#alg1 "算法 1 ‣ 3.3 更少的参数以获得更好的性能
    ‣ 3 提议的方法 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数以获得更好的性能") 所述，我们的方法涉及对参数 $\boldsymbol{\theta_{t}}$
    进行扰动。这一步通常需要存储两组不同的参数，从而增加了微调过程中内存的使用。
- en: 'As depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2.2 MeZO ‣ 2.2 Zeroth-Order
    Optimization ‣ 2 Preliminaries ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), recently proposed MeZO, conserves memory by
    saving random seeds $s$, and reconstructing $\boldsymbol{\theta_{t}}$ by saving
    the random seed because the sparse mask, determined by parameter magnitudes, changes
    when parameters are altered by the perturbation. To address this, we propose potential
    solutions for the memory issue.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S2.F2 "图 2 ‣ 2.2.2 MeZO ‣ 2.2 零阶优化 ‣ 2 初步介绍 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数以获得更好的性能")
    所示，最近提出的 MeZO 通过保存随机种子 $s$ 以及通过保存随机种子重构 $\boldsymbol{\theta_{t}}$ 来节省内存，因为由参数幅度决定的稀疏掩码在参数因扰动而改变时也会改变。为了解决这个问题，我们提出了针对内存问题的潜在解决方案。
- en: '![Refer to caption](img/5835ba868e987379b23bc7f9e3208630.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5835ba868e987379b23bc7f9e3208630.png)'
- en: 'Figure 4: Efficient Implementation of Sparse-MeZO. (a) represents vanilla implementation
    of S-MeZO, (b) represents our proposed efficient implementation of S-MeZO.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：稀疏 MeZO 的高效实现。 (a) 代表 S-MeZO 的原始实现，(b) 代表我们提出的 S-MeZO 高效实现。
- en: '1-bit Quantization: We can apply 1-bit quantization to store the mask $\boldsymbol{m}$
    on the fly during the forward pass.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1-bit 量化：我们可以在前向传播过程中应用 1-bit 量化以实时存储掩码 $\boldsymbol{m}$。
- en: '| Model | Method | BoolQ | RTE | WIC | MultiRC | SST-2 | COPA | Average |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BoolQ | RTE | WIC | MultiRC | SST-2 | COPA | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-7b | Zero-Shot | $65.1$ | $79.7$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | Zero-Shot | $65.1$ | $79.7$ |'
- en: '| LLaMA-7b | ICL | $67.4$ | $81.2$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | ICL | $67.4$ | $81.2$ |'
- en: '| LLaMA-7b | FT | $84.5\pm 0.0$ | $95.7\pm 0.3$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | FT | $84.5\pm 0.0$ | $95.7\pm 0.3$ |'
- en: '| LLaMA-7b | MeZO | $75.9\pm 1.1$ | $94.6\pm 0.3$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | MeZO | $75.9\pm 1.1$ | $94.6\pm 0.3$ |'
- en: '| LLaMA-7b | R-MeZO | $76.9\pm 0.7$ | $94.6\pm 0.2$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | R-MeZO | $76.9\pm 0.7$ | $94.6\pm 0.2$ |'
- en: '| LLaMA-7b | S-MeZO | $\bf{80.9\pm 1.6}$ | $\bf{95.0\pm 0.3}$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | S-MeZO | $\bf{80.9\pm 1.6}$ | $\bf{95.0\pm 0.3}$ |'
- en: 'Table 1: Accuracy of Fine-Tuning LLaMA-7b on SuperGLUE (1,000 examples). ICL:
    In-Context Learning, FT: full-parameter fine-tuning with Adam, R-MeZO: MeZO with
    Random Mask.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在 SuperGLUE 上对 LLaMA-7b 进行微调的准确性（1,000 个样本）。ICL: 上下文学习，FT: 使用 Adam 进行全参数微调，R-MeZO:
    带有随机掩码的 MeZO。'
- en: 'Calculating the Mask During the Forward Pass: By computing the mask and perturb
    parameters in the forward pass, we eliminate the need to store perturbed parameters
    $\boldsymbol{\theta_{t}^{\prime}}$: $\boldsymbol{\theta_{t}^{\prime}}=\boldsymbol{\theta_{t}}+\epsilon\boldsymbol{m}\odot\boldsymbol{z}$
    can be defined as $\boldsymbol{y^{(i)}}=\boldsymbol{\theta_{t}^{\prime(i)}}\boldsymbol{x^{(i)}}+\boldsymbol{b^{(i)}}$.
    More specially, we can calculate the mask $\boldsymbol{m^{(i)}}$ represents the
    function GetMask to calculate mask $\boldsymbol{m^{(i)}}$ and calculate the output
    and mask of next layer.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传递过程中计算掩码：通过在前向传递中计算掩码和扰动参数，我们消除了存储扰动参数 $\boldsymbol{\theta_{t}^{\prime}}$
    的需要：$\boldsymbol{\theta_{t}^{\prime}}=\boldsymbol{\theta_{t}}+\epsilon\boldsymbol{m}\odot\boldsymbol{z}$
    可以定义为 $\boldsymbol{y^{(i)}}=\boldsymbol{\theta_{t}^{\prime(i)}}\boldsymbol{x^{(i)}}+\boldsymbol{b^{(i)}}$。更具体地，我们可以计算掩码
    $\boldsymbol{m^{(i)}}$ 表示函数 GetMask 来计算掩码 $\boldsymbol{m^{(i)}}$ 并计算下一层的输出和掩码。
- en: 4 Experiments
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Following a similar setting to MeZO, we evaluate the performance of our proposed
    method on SuperGLUE (Wang et al., [2019](#bib.bib49)). The experimental results
    show that our proposed method can achieve better performance while also attaining
    faster convergence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循与 MeZO 类似的设置，我们评估了我们提出的方法在 SuperGLUE（Wang et al., [2019](#bib.bib49)）上的表现。实验结果表明，我们提出的方法能够实现更好的性能，同时也达到更快的收敛速度。
- en: 4.1 Experimental Setting
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Datasets. To verify the performance gain of our proposed method, we conduct
    experiments on various fine-tuning tasks include SST-2 (Socher et al., [2013](#bib.bib40)),
    RTE (Dagan et al., [2005](#bib.bib11); Haim et al., [2006](#bib.bib16); Giampiccolo
    et al., [2007](#bib.bib14); Bentivogli et al., [2009](#bib.bib2)), BoolQ (Clark
    et al., [2019](#bib.bib10)), WIC (Pilehvar & Camacho-Collados, [2018](#bib.bib36)),
    MultiRC (Khashabi et al., [2018](#bib.bib24))) and multi-class task COPA (Roemmele
    et al., [2011](#bib.bib38)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。为了验证我们提出的方法的性能提升，我们在各种微调任务上进行了实验，包括 SST-2（Socher et al., [2013](#bib.bib40)），RTE（Dagan
    et al., [2005](#bib.bib11); Haim et al., [2006](#bib.bib16); Giampiccolo et al.,
    [2007](#bib.bib14); Bentivogli et al., [2009](#bib.bib2)），BoolQ（Clark et al.,
    [2019](#bib.bib10)），WIC（Pilehvar & Camacho-Collados, [2018](#bib.bib36)），MultiRC（Khashabi
    et al., [2018](#bib.bib24)）和多类任务 COPA（Roemmele et al., [2011](#bib.bib38)）。
- en: Models. We primarily use pre-trained LLaMA-7b (Touvron et al., [2023](#bib.bib46))
    to evaluate the performance of our proposed method on downstream tasks. To further
    demonstrate our method’s versatility, we also test it with OPT-13b (Zhang et al.,
    [2022](#bib.bib55)). Additionally, to examine our method’s scalability, we evaluate
    it on larger models, such as LLaMA-30b.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。我们主要使用预训练的 LLaMA-7b（Touvron et al., [2023](#bib.bib46)）来评估我们提出的方法在下游任务上的表现。为了进一步展示我们方法的多样性，我们还用
    OPT-13b（Zhang et al., [2022](#bib.bib55)）进行测试。此外，为了检验我们方法的可扩展性，我们在更大的模型上进行了评估，如
    LLaMA-30b。
- en: Baselines. First, we compare our method to the vanilla MeZO to demonstrate how
    sparsification enhances MeZO’s convergence speed and overall performance. Additionally,
    to show that our proposed S-MeZO effectively identifies and modifies crucial parameters,
    we contrast it with R-MeZO (a version of MeZO applying a random mask to select
    parameters for optimization). In addition, we also explore the impact of zero-shot
    optimization on improving a pre-trained language model’s capabilities through
    experiments with zeroth-shot learning and in-context learning techniques (Brown
    et al., [2020](#bib.bib4)). Lastly, to understand the performance gap between
    zeroth-order and first-order optimization in fine-tuning large language models,
    we present results from conventional full-parameter fine-tuning (FT) using the
    Adam optimizer, the most widely used method for such tasks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。首先，我们将我们的方法与原始MeZO进行比较，以展示稀疏化如何提升MeZO的收敛速度和整体性能。此外，为了展示我们提出的S-MeZO如何有效地识别和修改关键参数，我们将其与R-MeZO（一个应用随机掩码选择优化参数的MeZO版本）进行对比。此外，我们还通过零-shot学习和上下文学习技术（Brown
    et al., [2020](#bib.bib4)）探索零-shot优化对提升预训练语言模型能力的影响。最后，为了理解零阶和一阶优化在微调大型语言模型中的性能差距，我们呈现了使用Adam优化器进行的传统全参数微调（FT）的结果，这是最广泛使用的此类任务方法。
- en: Training Procedure. We adopt most of the training hyperparameters from the standard
    MeZO, including dataset configuration, batch size, training epochs, epsilon value,
    and task prompts, with the key difference being a higher learning rate for S-MeZO
    due to updating only a subset of the parameters. The primary goal of our training
    is the next token prediction. For the dataset, we use MeZO’s approach, randomly
    selecting 1,000 examples for training and testing the model on another set of
    1,000 examples (Zhou et al., [2023](#bib.bib56)). We perform the experiments using
    three different seeds and report the average of the outcomes. In addition, the
    total training steps for LLaMA and OPT is 20,000 and we evaluate its performance
    on the test dataset every 100 steps.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程。我们采用了标准MeZO的大部分训练超参数，包括数据集配置、批量大小、训练周期、epsilon值和任务提示，主要区别在于S-MeZO使用了更高的学习率，因为只更新了部分参数。我们训练的主要目标是下一个标记预测。对于数据集，我们使用了MeZO的方法，随机选择了1,000个例子用于训练，并在另一个1,000个例子的集合上测试模型（Zhou
    et al., [2023](#bib.bib56)）。我们使用三个不同的种子进行实验，并报告结果的平均值。此外，LLaMA和OPT的总训练步骤为20,000步，并且每100步在测试数据集上评估其性能。
- en: 4.2 Performance
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 性能
- en: 'To evaluate the performance of our proposed method S-MeZO, we initially tested
    it on the SuperGLUE benchmark using the LLaMA-7b model. The fine-tuning results,
    presented in Table [1](#S3.T1 "Table 1 ‣ 3.4 Memory-Efficient Implementation of
    Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), reveal that our S-MeZO method outperforms other
    zero-order (ZO) techniques like MeZO and R-MeZO. For instance, S-MeZO boosts MeZO’s
    accuracy from $71.7\%$ to $80.9\%$). Furthermore, all zeroth-order-based methods
    surpassed the performance of Zero-shot learning and in-context learning, demonstrating
    that zeroth-order optimization significantly enhances the pre-trained model’s
    effectiveness on downstream tasks. Finally, we can find that S-MeZO significantly
    bridges the performance gap between zero-order and first-order optimization methods.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们提出的S-MeZO方法的性能，我们首先在SuperGLUE基准上使用LLaMA-7b模型进行了测试。表[1](#S3.T1 "表1 ‣ 3.4
    稀疏-MeZO的内存高效实现 ‣ 3 提议的方法 ‣ 稀疏MeZO：更少的参数在零阶LLM微调中的更好表现")中呈现的微调结果显示，我们的S-MeZO方法优于其他零阶（ZO）技术，如MeZO和R-MeZO。例如，S-MeZO将MeZO的准确率从$71.7\%$提高到$80.9\%$。此外，所有基于零阶的方法都超越了零-shot学习和上下文学习的表现，证明零阶优化显著提升了预训练模型在下游任务中的效果。最后，我们可以发现S-MeZO显著缩小了零阶和一阶优化方法之间的性能差距。
- en: '![Refer to caption](img/790d4e738f15a0605465027cd3fceda8.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/790d4e738f15a0605465027cd3fceda8.png)'
- en: (a) RTE
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RTE
- en: '![Refer to caption](img/5a7a4230136d0ea03d9500f5737a5b96.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a7a4230136d0ea03d9500f5737a5b96.png)'
- en: (b) BoolQ
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (b) BoolQ
- en: '![Refer to caption](img/0396baeff7933ad33a9f75bc1cc3cea4.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0396baeff7933ad33a9f75bc1cc3cea4.png)'
- en: (c) WIC
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (c) WIC
- en: 'Figure 5: Convergence Curves of Fine-Tuning LLaMA-7b with MeZO and Sparse-MeZO
    (S-MeZO) on (a) RTE, (b) BoolQ, (c) WIC tasks.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：用MeZO和稀疏-MeZO（S-MeZO）微调LLaMA-7b的收敛曲线，针对（a）RTE，（b）BoolQ，（c）WIC任务。
- en: 4.3 Convergence Rate
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 收敛速度
- en: 'To verify that S-MeZO converges faster than MeZO, we carried out multiple experiments
    for comparison. The accuracy over steps is plotted in Figure [5](#S4.F5 "Figure
    5 ‣ 4.2 Performance ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for Better
    Performance in Zeroth-Order LLM Fine-Tuning"), which shows that S-MeZO can use
    fewer steps to achieve a better performance than vanilla MeZO. For example, S-MeZO
    only needs about 5,000 steps to achieve $70\%$x speedup on BoolQ.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 S-MeZO 收敛速度是否比 MeZO 更快，我们进行了多次实验进行比较。准确度随步骤变化的图示见图 [5](#S4.F5 "图 5 ‣ 4.2
    性能 ‣ 4 实验 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数带来更好的性能")，这表明 S-MeZO 能够使用更少的步骤来实现比 vanilla
    MeZO 更好的性能。例如，S-MeZO 仅需大约 5,000 步就能在 BoolQ 上实现 $70\%$ 的速度提升。
- en: 4.4 Memory Usage
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 内存使用
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.4 Memory Usage ‣ 4 Experiments ‣ Sparse MeZO:
    Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning") shows
    the memory consumption for MeZO, S-MeZO, and traditional full-parameter fine-tuning
    of LLaMA-7b. The data reveal that S-MeZO does not require more memory than MeZO
    and offers a substantial saving of roughly $12$ GB of vanilla S-MeZO to $14.6$
    GB across all five tasks, which also illustrates the efficiency of our proposed
    implementation method: Calculating the Mask During the Forward Pass. Finally,
    we can only use inference memory cost to fine-tune large language models.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S4.T2 "表格 2 ‣ 4.4 内存使用 ‣ 4 实验 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数带来更好的性能") 显示了
    MeZO、S-MeZO 和传统全参数微调 LLaMA-7b 的内存消耗。数据表明，S-MeZO 的内存需求不比 MeZO 更多，并且在所有五个任务中，vanilla
    S-MeZO 节省了大约 $12$ GB 到 $14.6$ GB，这也说明了我们提出的实现方法的效率：在前向传播过程中计算掩码。最后，我们只能使用推理内存成本来微调大型语言模型。
- en: '| Method | SST-2 | RTE | BoolQ | WIC | MultiRC |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SST-2 | RTE | BoolQ | WIC | MultiRC |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FT | $114.7$ | $158.6$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FT | $114.7$ | $158.6$ |'
- en: '| MeZO | $14.6$ | $14.6$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| MeZO | $14.6$ | $14.6$ |'
- en: '| S-MeZO | $28.3$ | $28.3$ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| S-MeZO | $28.3$ | $28.3$ |'
- en: '| S-MeZO-EI | $14.6$ | $14.6$ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| S-MeZO-EI | $14.6$ | $14.6$ |'
- en: 'Table 2: Memory Usage (batch size = 1) of Fine-Tuning LLaMA-7b on SuperGLUE
    (1,000 examples). EI represents the Efficient Implementation in section [3.4](#S3.SS4
    "3.4 Memory-Efficient Implementation of Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse
    MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：在 SuperGLUE（1,000 个样本）上微调 LLaMA-7b 的内存使用（批量大小 = 1）。EI 代表第 [3.4](#S3.SS4
    "3.4 稀疏-MeZO 的内存高效实现 ‣ 3 提出的方法 ‣ 稀疏 MeZO：在零阶 LLM 微调中更少的参数带来更好的性能") 节中的高效实现。
- en: 4.5 Sparse Rate
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 稀疏率
- en: For S-MeZO, we need to define the sparsity of the pre-trained model before starting
    to fine-tune it. To analyze the effects of sparsity value on the performance,
    we conduct experiments with various sparsity values (from $0.0$. In addition,
    for most tasks, a sparsity value of $0.8$) to $82.3\%$ to $82.5\%$).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 S-MeZO，我们需要在开始微调之前定义预训练模型的稀疏性。为了分析稀疏性值对性能的影响，我们进行了不同稀疏性值的实验（从 $0.0$ 到 $0.8$）以及
    $82.3\%$ 到 $82.5\%$ 的稀疏性值。
- en: '![Refer to caption](img/5654a25197cba533f50cabfe9bca37de.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5654a25197cba533f50cabfe9bca37de.png)'
- en: 'Figure 6: The effects of Sparsity for Fine-tuning LLaMA-7b with S-MeZO.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：S-MeZO 微调 LLaMA-7b 时稀疏性的影响。
- en: '| Model | Method | BoolQ | RTE | WIC |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BoolQ | RTE | WIC |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA-7b | MeZO | $75.9$ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | MeZO | $75.9$ |'
- en: '| LLaMA-7b | S-MeZO | $80.9$ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7b | S-MeZO | $80.9$ |'
- en: '| LLaMA-30b | MeZO | $83.8$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30b | MeZO | $83.8$ |'
- en: '| LLaMA-30b | S-MeZO | 85.7 | 82.1 | 67.3 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30b | S-MeZO | 85.7 | 82.1 | 67.3 |'
- en: 'Table 3: Accuracy of Fine-Tuning LLaMA-7b and LLaMA-30b on SuperGLUE (1,000
    examples).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：在 SuperGLUE（1,000 个样本）上微调 LLaMA-7b 和 LLaMA-30b 的准确度。
- en: 4.6 Scalability
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 可扩展性
- en: 'In Table [1](#S3.T1 "Table 1 ‣ 3.4 Memory-Efficient Implementation of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we mainly introduce the performance of our methods on LLaMA-7b.
    A direct question is whether our proposed method can scale to larger language
    models. Therefore, in this section, we further explore our proposed method S-MeZO
    on LLaMA-30b. As shown in Table [3](#S4.T3 "Table 3 ‣ 4.5 Sparse Rate ‣ 4 Experiments
    ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning"),
    we can see that the a larger model usually can obtain a better fine-tuned performance.
    For example, the accuracy on RTE with MeZO can be improved from $71.1\%$ on LLaMA-30b.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[1](#S3.T1 "Table 1 ‣ 3.4 Memory-Efficient Implementation of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning")中，我们主要介绍了我们方法在LLaMA-7b上的性能。一个直接的问题是我们提出的方法是否可以扩展到更大的语言模型。因此，在本节中，我们进一步探讨了我们提出的S-MeZO方法在LLaMA-30b上的表现。如表[3](#S4.T3
    "Table 3 ‣ 4.5 Sparse Rate ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for
    Better Performance in Zeroth-Order LLM Fine-Tuning")所示，我们可以看到更大的模型通常可以获得更好的微调性能。例如，MeZO在LLaMA-30b上的RTE准确率可以从$71.1\%$提高。'
- en: '| Model | Method | BoolQ | RTE | WIC |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BoolQ | RTE | WIC |'
- en: '| OPT-13b | Zero Shot | $59.0$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | Zero Shot | $59.0$ |'
- en: '| OPT-13b | ICL | $66.9$ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | ICL | $66.9$ |'
- en: '| OPT-13b | MeZO | $72.1$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | MeZO | $72.1$ |'
- en: '| OPT-13b | R-MeZO | $72.3$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | R-MeZO | $72.3$ |'
- en: '| OPT-13b | S-MeZO | $\bf{73.8}$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | S-MeZO | $\bf{73.8}$ |'
- en: 'Table 4: Accuracy of Fine-Tuning OPT on SuperGLUE (1,000 examples). ICL: In-Context
    Learning, R-MeZO: MeZO with Random Mask.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在SuperGLUE（1,000个示例）上对OPT进行微调的准确性。ICL：上下文学习，R-MeZO：带有随机掩码的MeZO。
- en: 4.7 The Analysis about Efficient Implementation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 关于高效实现的分析
- en: 'In section [3.4](#S3.SS4 "3.4 Memory-Efficient Implementation of Sparse-MeZO
    ‣ 3 Proposed Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"),we present the efficient implementation of S-MeZO, which enables
    our proposed method to require only the inference memory cost for fine-tuning
    large language models. To analyze the actual GPU memory usage during the training
    process, we provide these results in Table [2](#S4.T2 "Table 2 ‣ 4.4 Memory Usage
    ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"). We can find that S-MeZO needs the same GPU memory for all five
    tasks, which can also save about $50\%$ memory compared to sparse-mezo. That also
    illustrates the efficiency of our proposed efficient implementation.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '在[3.4](#S3.SS4 "3.4 Memory-Efficient Implementation of Sparse-MeZO ‣ 3 Proposed
    Method ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
    Fine-Tuning")节中，我们介绍了S-MeZO的高效实现，这使得我们提出的方法仅需推理内存成本即可微调大型语言模型。为了分析训练过程中的实际GPU内存使用情况，我们在表[2](#S4.T2
    "Table 2 ‣ 4.4 Memory Usage ‣ 4 Experiments ‣ Sparse MeZO: Less Parameters for
    Better Performance in Zeroth-Order LLM Fine-Tuning")中提供了这些结果。我们发现S-MeZO对所有五个任务需要相同的GPU内存，相比于稀疏MeZO，节省了约$50\%$的内存。这也说明了我们提出的高效实现的效率。'
- en: 5 Conclusion
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a novel memory-efficient zeroth-order fine-tuning
    method Sparse-MeZO, which can use a similar memory cost to the inference process.
    We evaluate the performance of fine-tuning LLaMA and OPT with Sparse-MeZO on SuperGULE
    benchmark and the experimental results illustrate that Sparse-MeZO can achieve
    a higher accuracy and faster convergence. Finally, we can fine-tune LLaMA-30b
    in a single A100 GPU. However, there is still a performance gap between our proposed
    method Sparse-MeZO and first-order fine-tuning methods. We plan to address these
    limitations and enhance Sparse-MeZO’s capabilities in our future research.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种新颖的内存高效零阶微调方法Sparse-MeZO，该方法可以使用类似于推理过程的内存成本。我们在SuperGLUE基准测试上评估了Sparse-MeZO对LLaMA和OPT的微调性能，实验结果表明Sparse-MeZO可以实现更高的准确性和更快的收敛速度。最后，我们可以在单个A100
    GPU上微调LLaMA-30b。然而，我们提出的Sparse-MeZO方法与一阶微调方法之间仍存在性能差距。我们计划在未来的研究中解决这些限制，并提升Sparse-MeZO的能力。
- en: References
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Balasubramanian & Ghadimi (2018) Balasubramanian, K. and Ghadimi, S. Zeroth-order
    (non)-convex stochastic optimization via conditional gradient and gradient updates.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balasubramanian & Ghadimi (2018) Balasubramanian, K. 和 Ghadimi, S. 零阶（非）凸随机优化通过条件梯度和梯度更新。*神经信息处理系统进展*，31，2018年。
- en: Bentivogli et al. (2009) Bentivogli, L., Clark, P., Dagan, I., and Giampiccolo,
    D. The fifth pascal recognizing textual entailment challenge. *TAC*, 7(8):1, 2009.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bentivogli et al. (2009) Bentivogli, L., Clark, P., Dagan, I. 和 Giampiccolo,
    D. 第五届Pascal文本蕴涵识别挑战。*TAC*，7(8):1，2009年。
- en: 'Brock et al. (2017) Brock, A., Lim, T., Ritchie, J. M., and Weston, N. Freezeout:
    Accelerate training by progressively freezing layers. *arXiv preprint arXiv:1706.04983*,
    2017.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brock 等人 (2017) Brock, A., Lim, T., Ritchie, J. M., 和 Weston, N. Freezeout:
    通过逐步冻结层加速训练。*arXiv 预印本 arXiv:1706.04983*, 2017。'
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等人. 语言模型是少样本学习者。*神经信息处理系统进展*,
    33:1877–1901, 2020。
- en: 'Cai et al. (2020) Cai, H., Gan, C., Zhu, L., and Han, S. Tinytl: Reduce activations,
    not trainable parameters for efficient on-device learning. *arXiv preprint arXiv:2007.11622*,
    2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等人 (2020) Cai, H., Gan, C., Zhu, L., 和 Han, S. Tinytl: 减少激活，而非可训练参数以实现高效的设备学习。*arXiv
    预印本 arXiv:2007.11622*, 2020。'
- en: Cai et al. (2021) Cai, H., Lou, Y., McKenzie, D., and Yin, W. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. In *International
    Conference on Machine Learning*, pp.  1193–1203\. PMLR, 2021.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人 (2021) Cai, H., Lou, Y., McKenzie, D., 和 Yin, W. 一种用于大规模黑箱优化的零阶块坐标下降算法。发表于
    *国际机器学习会议*, 第1193–1203页。PMLR, 2021。
- en: 'Cai et al. (2022) Cai, H., Mckenzie, D., Yin, W., and Zhang, Z. Zeroth-order
    regularized optimization (zoro): Approximately sparse gradients and adaptive sampling.
    *SIAM Journal on Optimization*, 32(2):687–714, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等人 (2022) Cai, H., Mckenzie, D., Yin, W., 和 Zhang, Z. 零阶正则化优化 (zoro): 大致稀疏的梯度和自适应采样。*SIAM
    优化期刊*, 32(2):687–714, 2022。'
- en: 'Chen et al. (2023) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J.,
    Parasyris, K., Zhang, Y., Zhang, Z., Kailkhura, B., and Liu, S. Deepzero: Scaling
    up zeroth-order optimization for deep model training. *arXiv preprint arXiv:2310.02025*,
    2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J., Parasyris,
    K., Zhang, Y., Zhang, Z., Kailkhura, B., 和 Liu, S. Deepzero: 扩展零阶优化以适应深度模型训练。*arXiv
    预印本 arXiv:2310.02025*, 2023。'
- en: 'Chen et al. (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In *Proceedings of the 10th ACM workshop on
    artificial intelligence and security*, pp.  15–26, 2017.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., 和 Hsieh, C.-J. Zoo:
    基于零阶优化的黑箱攻击深度神经网络，无需训练替代模型。发表于 *第10届 ACM 人工智能与安全研讨会论文集*, 第15–26页, 2017。'
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人 (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. Boolq: 探索自然是/否问题的惊人难度。*arXiv 预印本 arXiv:1905.10044*, 2019。'
- en: Dagan et al. (2005) Dagan, I., Glickman, O., and Magnini, B. The pascal recognising
    textual entailment challenge. In *Machine learning challenges workshop*, pp. 
    177–190\. Springer, 2005.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dagan 等人 (2005) Dagan, I., Glickman, O., 和 Magnini, B. Pascal 识别文本蕴含挑战。发表于 *机器学习挑战研讨会*,
    第177–190页。Springer, 2005。
- en: 'Duchi et al. (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., and Wibisono,
    A. Optimal rates for zero-order convex optimization: The power of two function
    evaluations. *IEEE Transactions on Information Theory*, 61(5):2788–2806, 2015.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duchi 等人 (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., 和 Wibisono,
    A. 零阶凸优化的最佳速率: 两次函数评估的力量。*IEEE 信息理论汇刊*, 61(5):2788–2806, 2015。'
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frankle & Carbin (2018) Frankle, J. 和 Carbin, M. 彩票票假设: 寻找稀疏的、可训练的神经网络。*arXiv
    预印本 arXiv:1803.03635*, 2018。'
- en: Giampiccolo et al. (2007) Giampiccolo, D., Magnini, B., Dagan, I., and Dolan,
    W. B. The third pascal recognizing textual entailment challenge. In *Proceedings
    of the ACL-PASCAL workshop on textual entailment and paraphrasing*, pp.  1–9,
    2007.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giampiccolo 等人 (2007) Giampiccolo, D., Magnini, B., Dagan, I., 和 Dolan, W. B.
    第三届 Pascal 识别文本蕴含挑战。发表于 *ACL-PASCAL 文本蕴含与释义研讨会论文集*, 第1–9页, 2007。
- en: Gu et al. (2021) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., and Pan,
    D. Z. Efficient on-chip learning for optical neural networks through power-aware
    sparse zeroth-order optimization. In *Proceedings of the AAAI conference on artificial
    intelligence*, volume 35, pp.  7583–7591, 2021.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人 (2021) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., 和 Pan, D. Z.
    通过功耗感知稀疏零阶优化实现光学神经网络的高效片上学习。发表于 *AAAI 人工智能会议论文集*, 第35卷，第7583–7591页, 2021。
- en: Haim et al. (2006) Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo,
    D., Magnini, B., and Szpektor, I. The second pascal recognising textual entailment
    challenge. In *Proceedings of the Second PASCAL Challenges Workshop on Recognising
    Textual Entailment*, volume 7, pp.  785–794, 2006.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haim等（2006）Haim, R. B., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini,
    B., 和Szpektor, I. 第二届PASCAL文本蕴含识别挑战赛。见于*第二届PASCAL挑战研讨会关于识别文本蕴含的会议记录*，第7卷，第785–794页，2006年。
- en: 'Han et al. (2015a) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015a.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han等（2015a）Han, S., Mao, H., 和Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv预印本
    arXiv:1510.00149*，2015a。
- en: Han et al. (2015b) Han, S., Pool, J., Tran, J., and Dally, W. Learning both
    weights and connections for efficient neural network. *Advances in neural information
    processing systems*, 28, 2015b.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han等（2015b）Han, S., Pool, J., Tran, J., 和Dally, W. 高效神经网络的权重和连接的学习。*神经信息处理系统进展*，28，2015b。
- en: Hanson & Pratt (1988) Hanson, S. and Pratt, L. Comparing biases for minimal
    network construction with back-propagation. *Advances in neural information processing
    systems*, 1, 1988.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanson & Pratt（1988）Hanson, S. 和Pratt, L. 比较最小网络构造与反向传播的偏差。*神经信息处理系统进展*，1，1988年。
- en: 'Hassibi & Stork (1992) Hassibi, B. and Stork, D. Second order derivatives for
    network pruning: Optimal brain surgeon. *Advances in neural information processing
    systems*, 5, 1992.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi & Stork（1992）Hassibi, B. 和Stork, D. 网络剪枝的二阶导数：最佳大脑外科医生。*神经信息处理系统进展*，5，1992年。
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for nlp. In *International Conference on Machine Learning*,
    pp.  2790–2799\. PMLR, 2019.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby等（2019）Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe,
    Q., Gesmundo, A., Attariyan, M., 和Gelly, S. NLP的参数高效转移学习。见于*国际机器学习会议*，第2790–2799页。PMLR，2019年。
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2021）Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., 和Chen, W. Lora：大型语言模型的低秩适应。*arXiv预印本 arXiv:2106.09685*，2021年。
- en: Ilyas et al. (2018) Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box
    adversarial attacks with limited queries and information. In *International conference
    on machine learning*, pp.  2137–2146\. PMLR, 2018.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas等（2018）Ilyas, A., Engstrom, L., Athalye, A., 和Lin, J. 限制查询和信息的黑箱对抗攻击。见于*国际机器学习会议*，第2137–2146页。PMLR，2018年。
- en: 'Khashabi et al. (2018) Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,
    and Roth, D. Looking beyond the surface: A challenge set for reading comprehension
    over multiple sentences. In *Proceedings of the 2018 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers)*, pp.  252–262, 2018.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khashabi等（2018）Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., 和Roth,
    D. 超越表面：一个多句子阅读理解挑战集。见于*2018年北美计算语言学协会人类语言技术会议记录：长篇论文，第1卷*，第252–262页，2018年。
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun等（1989）LeCun, Y., Denker, J., 和Solla, S. 最佳脑损伤。*神经信息处理系统进展*，2，1989年。
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester等（2021）Lester, B., Al-Rfou, R., 和Constant, N. 参数高效提示调优的规模力量。*arXiv预印本
    arXiv:2104.08691*，2021年。
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang（2021）Li, X. L. 和Liang, P. 前缀调优：优化生成的连续提示。*arXiv预印本 arXiv:2101.00190*，2021年。
- en: Lin et al. (2020) Lin, Z., Madotto, A., and Fung, P. Exploring versatile generative
    language model via parameter-efficient transfer learning. *arXiv preprint arXiv:2004.03829*,
    2020.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2020）Lin, Z., Madotto, A., 和Fung, P. 通过参数高效转移学习探索多功能生成语言模型。*arXiv预印本 arXiv:2004.03829*，2020年。
- en: Liu et al. (2022) Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal,
    M., and Raffel, C. A. Few-shot parameter-efficient fine-tuning is better and cheaper
    than in-context learning. *Advances in Neural Information Processing Systems*,
    35:1950–1965, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2022) Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M.,
    和 Raffel, C. A. 少量参数高效微调优于并且比上下文学习更便宜。*神经信息处理系统进展*，35：1950–1965，2022 年。
- en: Liu et al. (2018) Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S.,
    and Amini, L. Zeroth-order stochastic variance reduction for nonconvex optimization.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2018) Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., 和 Amini,
    L. 用于非凸优化的零阶随机方差减少。*神经信息处理系统进展*，31，2018 年。
- en: Liu et al. (2019) Liu, S., Chen, P.-Y., Chen, X., and Hong, M. signsgd via zeroth-order
    oracle. In *International conference on learning representations*. International
    Conference on Learning Representations, ICLR, 2019.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2019) Liu, S., Chen, P.-Y., Chen, X., 和 Hong, M. 通过零阶预言机的 signsgd。在
    *国际学习表示会议*。国际学习表示会议，ICLR，2019 年。
- en: 'Liu et al. (2020) Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III,
    A. O., and Varshney, P. K. A primer on zeroth-order optimization in signal processing
    and machine learning: Principals, recent advances, and applications. *IEEE Signal
    Processing Magazine*, 37(5):43–54, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020) Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III, A. O.,
    和 Varshney, P. K. 信号处理和机器学习中零阶优化的基础：原理、最新进展和应用。*IEEE 信号处理杂志*，37(5)：43–54，2020
    年。
- en: 'Liu et al. (2021) Liu, Y., Agarwal, S., and Venkataraman, S. Autofreeze: Automatically
    freezing model blocks to accelerate fine-tuning. *arXiv preprint arXiv:2102.01386*,
    2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2021) Liu, Y., Agarwal, S., 和 Venkataraman, S. Autofreeze：自动冻结模型块以加速微调。*arXiv
    预印本 arXiv:2102.01386*，2021 年。
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,
    Chen, D., and Arora, S. Fine-tuning language models with just forward passes.
    *arXiv preprint arXiv:2305.17333*, 2023.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi 等人 (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,
    Chen, D., 和 Arora, S. 仅用前向传递进行语言模型微调。*arXiv 预印本 arXiv:2305.17333*，2023 年。
- en: 'Ohta et al. (2020) Ohta, M., Berger, N., Sokolov, A., and Riezler, S. Sparse
    perturbations for improved convergence in stochastic zeroth-order optimization.
    In *Machine Learning, Optimization, and Data Science: 6th International Conference,
    LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II 6*,
    pp.  39–64\. Springer, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohta 等人 (2020) Ohta, M., Berger, N., Sokolov, A., 和 Riezler, S. 用于改进随机零阶优化收敛性的稀疏扰动。在
    *机器学习、优化与数据科学：第六届国际会议，LOD 2020，意大利锡耶纳，2020年7月19-23日，修订选定论文，第II部分*，第 39–64 页。Springer，2020
    年。
- en: 'Pilehvar & Camacho-Collados (2018) Pilehvar, M. T. and Camacho-Collados, J.
    Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.
    *arXiv preprint arXiv:1808.09121*, 2018.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pilehvar & Camacho-Collados (2018) Pilehvar, M. T. 和 Camacho-Collados, J. Wic:
    用于评估上下文敏感意义表示的上下文词汇数据集。*arXiv 预印本 arXiv:1808.09121*，2018 年。'
- en: Rebuffi et al. (2017) Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Learning multiple
    visual domains with residual adapters. *Advances in neural information processing
    systems*, 30, 2017.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi 等人 (2017) Rebuffi, S.-A., Bilen, H., 和 Vedaldi, A. 使用残差适配器学习多个视觉领域。*神经信息处理系统进展*，30，2017
    年。
- en: 'Roemmele et al. (2011) Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
    of plausible alternatives: An evaluation of commonsense causal reasoning. In *2011
    AAAI Spring Symposium Series*, 2011.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele 等人 (2011) Roemmele, M., Bejan, C. A., 和 Gordon, A. S. 可行替代选择：对常识因果推理的评估。在
    *2011 AAAI 春季研讨会系列*，2011 年。
- en: Ruan et al. (2019) Ruan, Y., Xiong, Y., Reddi, S., Kumar, S., and Hsieh, C.-J.
    Learning to learn by zeroth-order oracle. *arXiv preprint arXiv:1910.09464*, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan 等人 (2019) Ruan, Y., Xiong, Y., Reddi, S., Kumar, S., 和 Hsieh, C.-J. 通过零阶预言机学习学习。*arXiv
    预印本 arXiv:1910.09464*，2019 年。
- en: Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality
    over a sentiment treebank. In *Proceedings of the 2013 conference on empirical
    methods in natural language processing*, pp.  1631–1642, 2013.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher 等人 (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.
    D., Ng, A. Y., 和 Potts, C. 对情感树库进行语义组合性的递归深度模型。在 *2013年自然语言处理实证方法会议论文集*，第 1631–1642
    页，2013 年。
- en: Spall (1992) Spall, J. C. Multivariate stochastic approximation using a simultaneous
    perturbation gradient approximation. *IEEE transactions on automatic control*,
    37(3):332–341, 1992.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spall (1992) Spall, J. C. 使用同时扰动梯度近似的多变量随机逼近。*IEEE 自动控制学报*，37(3)：332–341，1992
    年。
- en: Ström (1997) Ström, N. Phoneme probability estimation with dynamic sparsely
    connected artificial neural networks. *The Free Speech Journal*, 5(1-41):2, 1997.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ström (1997) Ström, N. 使用动态稀疏连接人工神经网络的音素概率估计。*自由言论期刊*，5(1-41):2，1997。
- en: 'Sun et al. (2022a) Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X.-J., and Qiu,
    X. Bbtv2: towards a gradient-free future with large language models. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 
    3916–3930, 2022a.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 (2022a) Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X.-J., 和 Qiu, X.
    Bbtv2: 朝着无梯度的未来迈进，大型语言模型。发表于 *2022年自然语言处理实证方法会议论文集*，第3916–3930页，2022a。'
- en: Sun et al. (2022b) Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box
    tuning for language-model-as-a-service. In *International Conference on Machine
    Learning*, pp.  20841–20855\. PMLR, 2022b.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2022b) Sun, T., Shao, Y., Qian, H., Huang, X., 和 Qiu, X. 语言模型服务的黑箱调优。发表于
    *国际机器学习会议*，第20841–20855页。PMLR, 2022b。
- en: 'Sung et al. (2022) Sung, Y.-L., Cho, J., and Bansal, M. Lst: Ladder side-tuning
    for parameter and memory efficient transfer learning. *Advances in Neural Information
    Processing Systems*, 35:12991–13005, 2022.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sung 等人 (2022) Sung, Y.-L., Cho, J., 和 Bansal, M. Lst: 梯度侧调优用于参数和内存高效的迁移学习。*神经信息处理系统进展*，35:12991–13005,
    2022。'
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等. Llama: 开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023。'
- en: 'Tu et al. (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi,
    J., Hsieh, C.-J., and Cheng, S.-M. Autozoom: Autoencoder-based zeroth order optimization
    method for attacking black-box neural networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 33, pp.  742–749, 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tu 等人 (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi, J.,
    Hsieh, C.-J., 和 Cheng, S.-M. Autozoom: 基于自编码器的零阶优化方法用于攻击黑箱神经网络。发表于 *AAAI人工智能会议论文集*，第33卷，第742–749页，2019。'
- en: 'Vemula et al. (2019) Vemula, A., Sun, W., and Bagnell, J. Contrasting exploration
    in parameter and action space: A zeroth-order optimization perspective. In *The
    22nd International Conference on Artificial Intelligence and Statistics*, pp. 
    2926–2935\. PMLR, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vemula 等人 (2019) Vemula, A., Sun, W., 和 Bagnell, J. 参数空间和动作空间中的对比探索：零阶优化视角。发表于
    *第22届国际人工智能与统计会议*，第2926–2935页。PMLR, 2019。
- en: 'Wang et al. (2019) Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael,
    J., Hill, F., Levy, O., and Bowman, S. Superglue: A stickier benchmark for general-purpose
    language understanding systems. *Advances in neural information processing systems*,
    32, 2019.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2019) Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael,
    J., Hill, F., Levy, O., 和 Bowman, S. Superglue: 一个更具挑战性的通用语言理解系统基准。*神经信息处理系统进展*，32，2019。'
- en: 'Wang et al. (2022) Wang, X., Guo, W., Su, J., Yang, X., and Yan, J. Zarts:
    On zero-order optimization for neural architecture search. *Advances in Neural
    Information Processing Systems*, 35:12868–12880, 2022.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2022) Wang, X., Guo, W., Su, J., Yang, X., 和 Yan, J. Zarts: 零阶优化用于神经网络架构搜索。*神经信息处理系统进展*，35:12868–12880,
    2022。'
- en: Wang et al. (2018) Wang, Y., Du, S., Balakrishnan, S., and Singh, A. Stochastic
    zeroth-order optimization in high dimensions. In *International conference on
    artificial intelligence and statistics*, pp.  1356–1365\. PMLR, 2018.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2018) Wang, Y., Du, S., Balakrishnan, S., 和 Singh, A. 高维中的随机零阶优化。发表于
    *国际人工智能与统计会议*，第1356–1365页。PMLR, 2018。
- en: Ye et al. (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., and Zhang, T. Hessian-aware
    zeroth-order optimization for black-box adversarial attack. *arXiv preprint arXiv:1812.11377*,
    2018.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人 (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., 和 Zhang, T. 考虑Hessian的零阶优化用于黑箱对抗攻击。*arXiv
    预印本 arXiv:1812.11377*，2018。
- en: 'Zaken et al. (2021) Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple
    parameter-efficient fine-tuning for transformer-based masked language-models.
    *arXiv preprint arXiv:2106.10199*, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaken 等人 (2021) Zaken, E. B., Ravfogel, S., 和 Goldberg, Y. Bitfit: 简单的参数高效微调用于基于变换器的掩码语言模型。*arXiv
    预印本 arXiv:2106.10199*，2021。'
- en: Zhang et al. (2023) Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen,
    W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning.
    *arXiv preprint arXiv:2303.10512*, 2023.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2023) Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen,
    W., 和 Zhao, T. 参数高效微调的自适应预算分配。*arXiv 预印本 arXiv:2303.10512*，2023。
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., 等. Opt: 开放预训练变换器语言模型。*arXiv
    预印本 arXiv:2205.01068*, 2022.'
- en: 'Zhou et al. (2023) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma,
    X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. *arXiv
    preprint arXiv:2305.11206*, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma,
    X., Efrat, A., Yu, P., Yu, L., 等. Lima: 更少即更多用于对齐。*arXiv 预印本 arXiv:2305.11206*,
    2023.'
- en: Appendix A The Prompts in LLaMA and OPT
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLaMA 和 OPT 中的提示
- en: '| Dataset | Type | Prompt |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 提示 |'
- en: '| --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SST-2 | cls. | {premise} |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | cls. | {premise} |'
- en: '|  |  | Does this mean that “{hypothesis}” is true? Yes or No? |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 这是否意味着 “{hypothesis}” 是真的？是或否？ |'
- en: '|  |  | Yes/No |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 是/否 |'
- en: '| RTE | cls. | Suppose “{premise}” Can we infer that “{hypothesis}”? Yes, No,
    or Maybe? |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| RTE | cls. | 假设 “{premise}” 我们可以推断出 “{hypothesis}” 吗？是、否还是可能？ |'
- en: '|  |  | Yes/No/Maybe |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 是/否/可能 |'
- en: '| BoolQ | cls. | {passage} {question} ? |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | cls. | {passage} {question} ? |'
- en: '|  |  | Yes/No |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 是/否 |'
- en: '| WIC | cls. | Does the word “{word}” have the same meaning in these two sentences?
    Yes, No? |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| WIC | cls. | 单词 “{word}” 在这两个句子中是否有相同的含义？是、否？ |'
- en: '|  |  | {sent1} |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  |  | {sent1} |'
- en: '|  |  | {sent2} |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |  | {sent2} |'
- en: '|  |  | Yes/No |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 是/否 |'
- en: '| MultiRC | cls. | {paragraph} |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| MultiRC | cls. | {paragraph} |'
- en: '|  |  | Question: {question} |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 问题: {question} |'
- en: '|  |  | I found this answer “{answer}”. Is that correct? Yes or No? |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 我找到这个答案“{answer}”。这是正确的吗？是或否？ |'
- en: '|  |  | Yes/No |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 是/否 |'
- en: '| COPA | mch. | {premise} so/because {candidate} |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| COPA | mch. | {premise} 因此/因为 {candidate} |'
- en: 'Table 5: The prompts of the datasets we used in our LLaMA experiments.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 我们在 LLaMA 实验中使用的数据集提示。'
- en: Appendix B Hyperparameters
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 超参数
- en: 'We will introduce the hyperparameters searching grids in Table [6](#A2.T6 "Table
    6 ‣ Appendix B Hyperparameters ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"), which can help people to reproduce our results.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在表 [6](#A2.T6 "Table 6 ‣ 附录 B 超参数 ‣ Sparse MeZO: 较少的参数以获得更好的性能在零阶 LLM 微调")
    中介绍超参数搜索网格，这可以帮助人们重现我们的结果。'
- en: '| Experiment | Hyperparameters | Values |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | 超参数 | 值 |'
- en: '| --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MeZO | Batch size | $16$ |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| MeZO | 批次大小 | $16$ |'
- en: '|  | Learning rate | $\{5\mathrm{e}{-7},1\mathrm{e}{-6},2\mathrm{e}{-6}\}$
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | 学习率 | $\{5\mathrm{e}{-7},1\mathrm{e}{-6},2\mathrm{e}{-6}\}$ |'
- en: '|  | $\epsilon$ |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\epsilon$ |'
- en: '| MeZO-Random | Batch size | $16$ |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| MeZO-Random | 批次大小 | $16$ |'
- en: '|  | Learning rate | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 学习率 | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
- en: '|  | $\epsilon$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\epsilon$ |'
- en: '| S-MeZO | Batch size | $16$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| S-MeZO | 批次大小 | $16$ |'
- en: '|  | Learning rate | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 学习率 | $\{1\mathrm{e}{-6},2\mathrm{e}{-6},3\mathrm{e}{-6},4\mathrm{e}{-6},5\mathrm{e}{-6}\}$
    |'
- en: '|  | $\epsilon$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\epsilon$ |'
- en: '| FT with Adam | Batch size | $8$ |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Adam 微调 | 批次大小 | $8$ |'
- en: '|  | Learning Rates | $\{1\mathrm{e}{-5},5\mathrm{e}{-5},8\mathrm{e}{-5}\}$
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | 学习率 | $\{1\mathrm{e}{-5},5\mathrm{e}{-5},8\mathrm{e}{-5}\}$ |'
- en: 'Table 6: The hyperparameter searching grids for LLaMA-7b experiments.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: LLaMA-7b 实验的超参数搜索网格。'
- en: We then introduce the sparsity of each task in SuperGULU when we fine-tune LLaMA-7b.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们介绍在微调 LLaMA-7b 时，每个任务在 SuperGULU 中的稀疏性。
- en: '| Method | SST-2 | RTE | BoolQ | WIC | MultiRC |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SST-2 | RTE | BoolQ | WIC | MultiRC |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Sparse MeZO | $0.70$ | $0.80$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Sparse MeZO | $0.70$ | $0.80$ |'
- en: 'Table 7: Sparsity in SuperGULU when we fine-tune LLaMA-7b.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 在我们微调 LLaMA-7b 时，SuperGULU 的稀疏性。'
- en: Appendix C The Proof of Lemma 3.2
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 引理 3.2 的证明
- en: 'Let $\mathcal{L}_{z}(\theta)$:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '让 $\mathcal{L}_{z}(\theta)$:'
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta):$ |  | (8) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta):$ |  | (8) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})]$
    |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})]$
    |  |'
- en: 'We can obtain the Lemma:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得到引理：
- en: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (9) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (9) |'
- en: '|  |  | $\displaystyle=m\odot\mathbb{E}_{z}[\nabla_{\theta}\mathcal{L}(\theta+\epsilon
    m\odot z)]$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=m\odot\mathbb{E}_{z}[\nabla_{\theta}\mathcal{L}(\theta+\epsilon
    m\odot z)]$ |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{z}[\frac{\mathcal{L}(\theta+\epsilon m\odot
    z)-\mathcal{L}(\theta-\epsilon m\odot z)}{2\epsilon}m\odot z]$ |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{z}[\frac{\mathcal{L}(\theta+\epsilon m\odot
    z)-\mathcal{L}(\theta-\epsilon m\odot z)}{2\epsilon}m\odot z]$ |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  |'
- en: 'Proof:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (10) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  | (10) |'
- en: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
- en: '|  |  | $\displaystyle=m\odot\nabla_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=m\odot\nabla_{\theta}\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
- en: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\nabla_{\theta}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\nabla_{\theta}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\frac{1}{k}m\odot\int_{\hat{z}}\frac{z}{\epsilon}e^{-\frac{1}{2}\&#124;z\&#124;^{2}}\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{k}m\odot\int_{\hat{z}}\frac{z}{\epsilon}e^{-\frac{1}{2}\&#124;z\&#124;^{2}}\mathcal{L}(\theta+\epsilon
    z)dz$ |  |'
- en: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)\frac{z}{\epsilon}dz$ |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=m\odot\int_{\hat{z}}\text{pdf}_{\hat{z}}(z)\mathcal{L}(\theta+\epsilon
    z)\frac{z}{\epsilon}dz$ |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[m\odot\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[m\odot\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
- en: where we can define $y=\theta+\epsilon z$ is the number of 1 in $\boldsymbol{m}$.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们可以定义$y=\theta+\epsilon z$是$\boldsymbol{m}$中1的数量。
- en: Therefore, we can obtain the gradient $\nabla_{\theta}\mathcal{L}_{m}(\theta)$.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得到梯度$\nabla_{\theta}\mathcal{L}_{m}(\theta)$。
- en: 'In addition, we will prove $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将证明$\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$：
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)}{\epsilon}\hat{z}]$
    |  | (11) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)}{\epsilon}\hat{z}]$
    |  | (11) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
- en: 'After that, we can get the relationship between $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以得到$\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$的关系：
- en: '|  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  | (12) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta)-\mathcal{L}(\theta-\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  | (12) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z}-\mathcal{L}(\theta))}{\epsilon}\hat{z}]$
    |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z}-\mathcal{L}(\theta))}{\epsilon}\hat{z}]$
    |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}].$
    |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}].$
    |  |'
- en: 'Based on the Equation [11](#A3.E11 "Equation 11 ‣ Appendix C The Proof of Lemma
    3.2 ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
    Fine-Tuning") and Equation [12](#A3.E12 "Equation 12 ‣ Appendix C The Proof of
    Lemma 3.2 ‣ Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order
    LLM Fine-Tuning"), we can obtain:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '基于方程[11](#A3.E11 "方程 11 ‣ 附录 C 引理 3.2 的证明 ‣ 稀疏 MeZO: 更少的参数以获得更好的零阶 LLM 微调性能")和方程[12](#A3.E12
    "方程 12 ‣ 附录 C 引理 3.2 的证明 ‣ 稀疏 MeZO: 更少的参数以获得更好的零阶 LLM 微调性能")，我们可以得到：'
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  | (13) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    |  | (13) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})}{\epsilon}\hat{z}]$
    |  |'
- en: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)$
    |  |'
- en: Finally, we can obtain the relationship between $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    and finish the proof.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以得到 $\mathbb{E}_{\hat{z}}[\frac{\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta-\epsilon\hat{z})}{2\epsilon}\hat{z}]$
    之间的关系，并完成证明。
- en: Appendix D The Proof of Lemma 3.3
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 引理 3.3 的证明
- en: '|  | $1$2 |  | (14) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: 'Proof:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: 'We can first define the distance between $\widehat{\nabla}_{\boldsymbol{\boldsymbol{\boldsymbol{\theta}}}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\theta}})=\mathbb{E}_{\hat{z}}[\boldsymbol{g_{\hat{z}}(\theta)}]$
    as:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先定义 $\widehat{\nabla}_{\boldsymbol{\boldsymbol{\boldsymbol{\theta}}}}\mathcal{L}_{\hat{z}}(\boldsymbol{\boldsymbol{\theta}})=\mathbb{E}_{\hat{z}}[\boldsymbol{g_{\hat{z}}(\theta)}]$
    之间的距离为：
- en: '|  |  | $\displaystyle\&#124;\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)-\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;$
    |  | (15) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\&#124;\widehat{\nabla}_{\theta}\mathcal{L}_{\hat{z}}(\theta)-\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;$
    |  | (15) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\frac{\epsilon L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{3}]$
    |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\epsilon L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{3}]$
    |  |'
- en: '|  |  | $\displaystyle\leq\frac{\epsilon L(l)}{2}(\hat{d}+3)^{\frac{3}{2}}$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\frac{\epsilon L(l)}{2}(\hat{d}+3)^{\frac{3}{2}}$
    |  |'
- en: 'where $\hat{d}$ and obtain that $\|\boldsymbol{a}\|^{2}\leq 2\|\boldsymbol{a}-\boldsymbol{b}\|^{2}+2\|\boldsymbol{b}\|^{2}$,
    we can obtain:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{d}$ 并得到 $\|\boldsymbol{a}\|^{2}\leq 2\|\boldsymbol{a}-\boldsymbol{b}\|^{2}+2\|\boldsymbol{b}\|^{2}$，我们可以得到：
- en: '|  | $\displaystyle\&#124;\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;^{2}$
    |  | (16) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\nabla_{\theta}\mathcal{L}_{m}(\theta)\&#124;^{2}$
    |  | (16) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Appendix E The Proof of Theorem 3.4
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 定理 3.4 的证明
- en: 'Proof:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta)-\mathcal{L}(\theta)$ |  |
    (17) |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta)-\mathcal{L}(\theta)$ |  |
    (17) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)-\epsilon\langle\nabla\mathcal{L}(\theta),\hat{z}\rangle]$
    |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\hat{z}}[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)-\epsilon\langle\nabla\mathcal{L}(\theta),\hat{z}\rangle]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\frac{\epsilon^{2}L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{2}]$
    |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{\epsilon^{2}L(l)}{2}\mathbb{E}_{\hat{z}}[\&#124;\hat{z}\&#124;^{2}]$
    |  |'
- en: '|  |  | $\displaystyle\leq\frac{\epsilon^{2}L(l)}{2}\hat{d}$ |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\frac{\epsilon^{2}L(l)}{2}\hat{d}$ |  |'
- en: 'The first inequality holds because Lipschitz Continuous: $|\mathcal{L}(\theta^{\prime})-\mathcal{L}(\theta)-\langle\nabla\mathcal{L}(\theta),\theta^{\prime}-\theta\rangle|\leq\frac{L(l)}{2}\|\theta^{\prime}-\theta\|^{2}$
    is the number of $1$.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个不等式成立是因为 Lipschitz 连续性：$|\mathcal{L}(\theta^{\prime})-\mathcal{L}(\theta)-\langle\nabla\mathcal{L}(\theta),\theta^{\prime}-\theta\rangle|\leq\frac{L(l)}{2}\|\theta^{\prime}-\theta\|^{2}$
    是 $1$ 的数量。
- en: '|  |  | $1$2 |  | (18) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (18) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle\leq\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}+\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}$
    |  |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}+\frac{\epsilon^{4}L^{2}(l)}{2}\hat{d}^{2}$
    |  |'
- en: '|  |  | $\displaystyle=\epsilon^{4}L^{2}(l)\hat{d}^{2}$ |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\epsilon^{4}L^{2}(l)\hat{d}^{2}$ |  |'
- en: 'The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
    The second inequality is due to the Equation [17](#A5.E17 "Equation 17 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning").'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个不等式由于 $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$。第二个不等式由于公式
    [17](#A5.E17 "公式 17 ‣ 附录 E 定理 3.4 的证明 ‣ Sparse MeZO: 更少参数以在零阶 LLM 微调中获得更好性能")。'
- en: '|  | ² | $1$2 |  | (19) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | ² | $1$2 |  | (19) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个不等式由于 $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$。
- en: '|  |  | $\displaystyle[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)]^{2}$
    |  | (20) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle[\mathcal{L}(\theta+\epsilon\hat{z})-\mathcal{L}(\theta)]^{2}$
    |  | (20) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'The first inequality is due to $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$.
    The last inequality holds because Equation [18](#A5.E18 "Equation 18 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning") and Equation [19](#A5.E19 "Equation 19 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning").'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个不等式是由于 $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$。最后一个不等式成立是因为方程
    [18](#A5.E18 "方程 18 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能") 和方程
    [19](#A5.E19 "方程 19 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能")。'
- en: '|  | $\displaystyle\mathbb{E}_{z,x}[\&#124;g_{\hat{z}}(\theta)\&#124;^{2}]$
    |  | (21) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{z,x}[\| g_{\hat{z}}(\theta)\|^{2}]$ |  | (21)
    |'
- en: '|  |  | $1$2 |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: The first inequality holds because $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq
    2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$, $\mathbb{E}_{\hat{z}}[\|\hat{z}\|^{p}]\leq(\hat{d}+p)^{\frac{p}{2}}$.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个不等式成立是因为 $\|\boldsymbol{a}+\boldsymbol{b}\|^{2}\leq 2\|\boldsymbol{a}\|^{2}+2\|\boldsymbol{b}\|^{2}$，$\mathbb{E}_{\hat{z}}[\|\hat{z}\|^{p}]\leq(\hat{d}+p)^{\frac{p}{2}}$。
- en: 'Based on the assumption about Lipschitz Continuous, we can obtain: $1$2.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对 Lipschitz 连续性的假设，我们可以得到：$1$2。
- en: 'Then, we can obtain:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以得到：
- en: '|  | $1$2 |  | (22) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (22) |'
- en: 'Based on the equation, we can follow the update rule: $\theta_{t+1}=\theta_{t}-\eta_{t}g_{\hat{z}}(\theta_{t})$
    and we can find:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程，我们可以遵循更新规则：$\theta_{t+1}=\theta_{t}-\eta_{t}g_{\hat{z}}(\theta_{t})$，并且可以发现：
- en: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta_{t+1})$ |  | (23) |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\hat{z}}(\theta_{t+1})$ |  | (23) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'where $\eta_{t}$:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta_{t}$：
- en: '|  |  | $\displaystyle\mathbb{E}_{\hat{z},x}[\mathcal{L}_{\hat{z}}(\theta_{t+1})]$
    |  | (24) |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{\hat{z},x}[\mathcal{L}_{\hat{z}}(\theta_{t+1})]$
    |  | (24) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'The first inequality is due to the Equation [9](#A3.E9 "Equation 9 ‣ Appendix
    C The Proof of Lemma 3.2 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning") and Equation [23](#A5.E23 "Equation 23 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"). The second inequality holds because Equation
    [21](#A5.E21 "Equation 21 ‣ Appendix E The Proof of Theorem 3.4 ‣ Sparse MeZO:
    Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning") provides
    the result about $\mathbb{E}_{\hat{z},x}[\|g_{z}(\theta_{t})\|^{2}$.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个不等式是由于方程 [9](#A3.E9 "方程 9 ‣ 附录 C 引理 3.2 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能")
    和方程 [23](#A5.E23 "方程 23 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能")。第二个不等式成立是因为方程
    [21](#A5.E21 "方程 21 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能") 提供了关于
    $\mathbb{E}_{\hat{z},x}[\|g_{z}(\theta_{t})\|^{2}]$ 的结果。'
- en: 'Then, we can select learning rate $\eta_{t}=\frac{1}{4(\hat{d}_{t}+4)L(l)}$
    and obtain:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以选择学习率 $\eta_{t}=\frac{1}{4(\hat{d}_{t}+4)L(l)}$ 并得到：
- en: '|  | $1$2 |  | (25) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (25) |'
- en: 'Then, taking the sum of Equation [25](#A5.E25 "Equation 25 ‣ Appendix E The
    Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance in
    Zeroth-Order LLM Fine-Tuning") over the index from $T+1$, we can have that :'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，对方程 [25](#A5.E25 "方程 25 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能")
    从 $T+1$ 开始求和，我们可以得到：'
- en: '|  | $1$2 |  | (26) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (26) |'
- en: 'where $L(l)\leq L$. Thus, based on Lemma [6](#S3.E6 "Equation 6 ‣ Lemma 3.3\.
    ‣ 3.2 Convergence Analysis of Sparse-MeZO ‣ 3 Proposed Method ‣ Sparse MeZO: Less
    Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning"), we can have:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $L(l)\leq L$。因此，根据引理 [6](#S3.E6 "方程 6 ‣ 引理 3.3. ‣ 3.2 稀疏-MeZO 的收敛分析 ‣ 3
    提出的方法 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM 微调的更佳性能")，我们可以得到：'
- en: '|  | $\displaystyle\mathbb{E}_{\hat{z},x}[\&#124;\nabla\mathcal{L}_{m}(\theta_{T})\&#124;^{2}]$
    |  | (27) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\hat{z},x}[\| \nabla \mathcal{L}_{m}(\theta_{T})\|^{2}]$
    |  | (27) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'The second inequality is due to the Equation [26](#A5.E26 "Equation 26 ‣ Appendix
    E The Proof of Theorem 3.4 ‣ Sparse MeZO: Less Parameters for Better Performance
    in Zeroth-Order LLM Fine-Tuning"). To obtain $\sigma$.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个不等式是由于方程 [26](#A5.E26 "方程 26 ‣ 附录 E 定理 3.4 的证明 ‣ 稀疏 MeZO: 更少的参数以实现零阶 LLM
    微调的更佳性能")。以获得 $\sigma$。'
- en: '|  | $\displaystyle 16(\hat{d}+4)L\frac{\mathcal{L}_{\hat{z}}(\theta_{0})-\mathcal{L}_{\hat{z}}^{*}}{T+1}+\mathcal{O}(\epsilon^{2}L^{2}\hat{d}^{3})$
    |  | (28) |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 16(\hat{d}+4)L\frac{\mathcal{L}_{\hat{z}}(\theta_{0})-\mathcal{L}_{\hat{z}}^{*}}{T+1}+\mathcal{O}(\epsilon^{2}L^{2}\hat{d}^{3})$
    |  | (28) |'
- en: '|  | $\displaystyle T$ |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle T$ |  |'
- en: Finally, we can finish the proof of the theorem. This theorem illustrates that
    the presence of pronounced sparsity patterns, along with the smoothness of the
    objective function, can significantly enhance the rate of convergence, potentially
    achieving a linear acceleration.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以完成定理的证明。这个定理说明，显著的稀疏模式与目标函数的平滑性可以显著提升收敛速度，可能实现线性加速。
