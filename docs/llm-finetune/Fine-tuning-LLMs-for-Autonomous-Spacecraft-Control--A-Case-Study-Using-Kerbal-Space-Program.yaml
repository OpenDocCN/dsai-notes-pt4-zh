- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:35:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:35:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal
    Space Program'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化航天器控制的LLM微调：以Kerbal Space Program为案例研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08676](https://ar5iv.labs.arxiv.org/html/2408.08676)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08676](https://ar5iv.labs.arxiv.org/html/2408.08676)
- en: 'Alejandro Carrasco Corresponding author. E-Mail:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Alejandro Carrasco 通讯作者。电子邮件：
- en: alejandro.carrasco.aragon@alumnos.upm.es Universidad Politécnica de Madrid,
    Madrid, Spain Victor Rodriguez-Fernandez Universidad Politécnica de Madrid, Madrid,
    Spain Richard Linares Massachussetts Institute of Technology, Massachussetts,
    USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: alejandro.carrasco.aragon@alumnos.upm.es 马德里理工大学，西班牙马德里 Victor Rodriguez-Fernandez
    马德里理工大学，西班牙马德里 Richard Linares 麻省理工学院，美国马萨诸塞州
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous
    agents that take actions based on the content of the user text prompt. This study
    explores the use of fine-tuned Large Language Models (LLMs) for autonomous spacecraft
    control, using the Kerbal Space Program Differential Games suite (KSPDG) as a
    testing environment. Traditional Reinforcement Learning (RL) approaches face limitations
    in this domain due to insufficient simulation capabilities and data. By leveraging
    LLMs, specifically fine-tuning models like GPT-3.5 and LLaMA, we demonstrate how
    these models can effectively control spacecraft using language-based inputs and
    outputs. Our approach integrates real-time mission telemetry into textual prompts
    processed by the LLM, which then generate control actions via an agent. The results
    open a discussion about the potential of LLMs for space operations beyond their
    nominal use for text-related tasks. Future work aims to expand this methodology
    to other space control tasks and evaluate the performance of different LLM families.
    The code is available at this URL: https://github.com/ARCLab-MIT/kspdg.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的趋势是使用大型语言模型（LLMs）作为自主代理，根据用户文本提示的内容采取行动。本研究探讨了使用微调的大型语言模型（LLMs）进行自主航天器控制，采用Kerbal
    Space Program Differential Games套件（KSPDG）作为测试环境。由于模拟能力和数据不足，传统的强化学习（RL）方法在这一领域面临限制。通过利用LLMs，特别是微调模型如GPT-3.5和LLaMA，我们展示了这些模型如何通过基于语言的输入和输出有效地控制航天器。我们的方法将实时任务遥测整合到LLM处理的文本提示中，LLM随后通过代理生成控制动作。这些结果引发了关于LLMs在航天操作中潜力的讨论，超出了其在文本相关任务中的常规使用。未来的工作旨在将这种方法扩展到其他空间控制任务，并评估不同LLM家族的性能。代码可以在这个网址获取：https://github.com/ARCLab-MIT/kspdg。
- en: \makeCustomtitle
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \makeCustomtitle
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) are, without a doubt, the last major breakthrough
    in the evolution of artificial intelligence systems. Since the release of ChatGPT
    [[1](#bib.bib1)] at the end of 2022, we have seen a plethora of applications and
    use cases emerge across various industries. From generating human-like text to
    aiding in code completion, LLMs have significantly impacted the way we interact
    with technology and the possibilities of what AI can achieve.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）无疑是人工智能系统发展中的最新重大突破。自2022年底ChatGPT发布以来，我们见证了各种行业中大量应用和用例的出现。从生成类似人类的文本到辅助代码完成，LLMs显著影响了我们与技术的互动方式以及AI可以实现的可能性。
- en: In recent months, the use of LLMs is expanding beyond text-based applications
    to become language agents capable of taking actions based on the context of the
    system in which they are integrated. By leveraging the contextual information
    available to them, LLMs can make informed decisions and perform tasks autonomously.
    This new way of creating autonomous agents intersects with the usage of Reinforcement
    Learning (RL) algorithms, and provides a way to overcome some of its well-known
    limitations, such as the sample inefficiency, and the need for a well-defined
    reward function. Some recent studies have demonstrated how some powerful LLMs,
    such as GPT-4, can surpass state-of-the-art RL algorithms in complex games just
    through studying academics texts and reasoning [[2](#bib.bib2)], executing sophisticated
    trajectories and achieving good zero-shot performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，LLM 的应用已超越文本基础应用，成为能够根据其集成系统的上下文采取行动的语言代理。通过利用可获得的上下文信息，LLM 能够做出明智的决策并自主执行任务。这种创建自主代理的新方式与强化学习（RL）算法的使用相交汇，并提供了一种克服其一些著名限制的方法，例如样本效率低和需要明确的奖励函数。一些最近的研究展示了如**GPT-4**这样的强大
    LLM 如何仅通过研究学术文本和推理，超越最先进的 RL 算法，在复杂游戏中取得优异的零-shot 表现 [[2](#bib.bib2)]。
- en: This work is focused on the domain of space applications and the development
    of autonomous agents for guidance and control of spacecrafts. In this context,
    the creation of AI-based agents has mainly been tackled through RL during recent
    years, and in fact, we can find RL-based agents for different tasks such as sensor-tasking
    [[3](#bib.bib3)] and planetary landing [[4](#bib.bib4)]. However, unlike other
    AI research areas, the space domain lacks of publicly available simulation environments,
    which are crucial for training AI agents in complex space operations and providing
    a standard benchmark for evaluating different AI and autonomous control methods.
    To address this issue, Allen et al. introduced SpaceGym [[5](#bib.bib5)], a set
    non-cooperative game environments that are intended to spur development and act
    as proving grounds for autonomous and AI decision-makers in the orbital domain.
    Among the available environments in SpaceGym, in this work we focus on the Kerbal
    Space Program Differential Games suite (KSPDG). KSPDG is a suite of differential
    games, such as pursuit-evasion scenarios, encoded within the Kerbal Space Program
    (KSP) game engine ¹¹1https://www.privatedivision.com/portfolio/kerbal-space-program/
    and standardized with OpenAI Gym [[6](#bib.bib6)] and PettingZoo [[7](#bib.bib7)]
    interfaces, facilitating the use of diverse AI techniques, including multi-agent
    reinforcement learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作专注于空间应用领域以及航天器引导和控制的自主代理开发。在这个背景下，近年来 AI 基础代理的创建主要通过 RL 进行，实际上，我们可以找到用于不同任务的基于
    RL 的代理，例如传感器任务 [[3](#bib.bib3)] 和行星着陆 [[4](#bib.bib4)]。然而，与其他 AI 研究领域不同，空间领域缺乏公开可用的模拟环境，这对于训练
    AI 代理在复杂的空间操作中以及为评估不同 AI 和自主控制方法提供标准基准至关重要。为了解决这个问题，Allen 等人引入了 SpaceGym [[5](#bib.bib5)]，这是一个非合作游戏环境集，旨在刺激开发并作为轨道领域自主和
    AI 决策者的试验场。在 SpaceGym 中可用的环境中，本工作重点关注 Kerbal Space Program Differential Games
    套件（KSPDG）。KSPDG 是一套微分游戏，如追逐-逃避场景，编码在 Kerbal Space Program (KSP) 游戏引擎中 ¹¹1https://www.privatedivision.com/portfolio/kerbal-space-program/，并与
    OpenAI Gym [[6](#bib.bib6)] 和 PettingZoo [[7](#bib.bib7)] 接口标准化，便于使用各种 AI 技术，包括多智能体强化学习。
- en: While KSPDG presents an innovative framework for testing AI and autonomous control
    methods in space applications, it is unsuitable for RL training, due to technical
    and non-technical reasons. On the one hand, the KSP engine, which underpins KSPDG,
    lacks the capacity for the parallel, accelerated, and headless operations essential
    for extensive faster-than-real-time RL training. On the other hand, the principled
    stance of KSPDG’s creators to focus on evaluation rather than training emphasizes
    the need for a “true test set" environment where overfitting is minimized, and
    the genuine and unbiased capabilities of AI agents are tested. This approach diverges
    from the typical RL methodology that relies on iterative training and fine-tuning
    of agents within a specific simulation environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 KSPDG 提出了一个用于测试 AI 和自主控制方法在太空应用中的创新框架，但由于技术和非技术原因，它不适用于 RL 训练。一方面，支撑 KSPDG
    的 KSP 引擎缺乏进行大规模快于真实时间的 RL 训练所需的并行、加速和无头操作能力。另一方面，KSPDG 创造者原则上的立场是专注于评估而非训练，强调了“真实测试集”环境的必要性，以尽量减少过拟合，并测试
    AI 代理的真正和无偏能力。这种方法与典型的 RL 方法论有所不同，后者依赖于在特定模拟环境中反复训练和微调代理。
- en: '![Refer to caption](img/2ca4a860c7d8229ecb21ed2c5f3827d9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ca4a860c7d8229ecb21ed2c5f3827d9.png)'
- en: 'Figure 1: Overview of the proposed approach to use a fine-tuned LLM (e.g. ChatGPT,
    LLaMA) as an autonomous spacecraft operator that gets, as user prompt, the current
    status of the mission from the KSDPG simulation environment (i.e., the state or
    observation in the RL jargon), and replies with a reasoned action to carry out,
    expressed as a function calling with the specific throttle vector and the textual
    justification behind the action.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：概述了使用经过微调的 LLM（例如 ChatGPT，LLaMA）作为自主飞船操作者的方法，该方法从 KSDPG 模拟环境中获取任务的当前状态（即
    RL 行话中的状态或观察），并回复一个经过推理的操作，表示为具有特定油门向量的函数调用以及操作背后的文本说明。
- en: 'To overcome the limitations of RL in creating autonomous agents for environments
    such as KSDPG, as well as for other space operations where numerous simulated
    data cannot be provided, we propose to adapt the current trend of LLM-based agents
    to develop an “intelligent" operator that controls a spacecraft based on the real-time
    telemetry of the environment, using language exclusively as the input and output
    of the system. As depicted in [Figure 1](#S1.F1 "In 1 Introduction ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program"),
    we design the classic RL loop by interfacing the simulation environment (KSDPG)
    with a LLM, transforming the real-time observations (or state) of the mission
    as textual user prompts that are fed to the model. The LLM then processes the
    prompt and replies with an action that will be plugged in KSDPG to control the
    spacecraft. Our agent was ranked 2nd in the KSPDG challenge ²²2https://www.ll.mit.edu/conferences-events/2024/01/kerbal-space-program-differential-game-challenge,
    and was presented via a live demonstration during a special session at AIAA SciTech
    in January 2024.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服在 KSDPG 等环境中创建自主代理的 RL 限制，以及在无法提供大量模拟数据的其他太空操作中，我们建议调整当前 LLM 基础代理的趋势，开发一个基于实时环境遥测控制飞船的“智能”操作者，系统仅使用语言作为输入和输出。如[图
    1](#S1.F1 "在 1 引言 ‣ 微调 LLM 以实现自主飞船控制：以 Kerbal Space Program 为例")所示，我们通过将模拟环境（KSDPG）与
    LLM 接口，设计经典的 RL 循环，将任务的实时观察（或状态）转化为输入给模型的文本用户提示。LLM 然后处理提示并回复一个将在 KSDPG 中插入以控制飞船的操作。我们的代理在
    KSPDG 挑战中排名第 2 ²²2https://www.ll.mit.edu/conferences-events/2024/01/kerbal-space-program-differential-game-challenge，并在
    2024 年 1 月的 AIAA SciTech 特别会议上进行了现场演示。
- en: 'In our previous study [[8](#bib.bib8)], prompt engineering was the primary
    focus, and LLM models demonstrated outstanding performance with zero-shot and
    few-shot prompts. That work utilized prompt engineering to effectively control
    a spacecraft in the Kerbal Space Program (KSP) simulation, achieving exceptional
    but non-generalizable results. Additionally, some fine-tuning experiments were
    conducted using the OpenAI fine-tuning API. At the time, the latest and most powerful
    model available for fine-tuning was gpt-3.5-turbo-0125. The customization of this
    fine-tuning process relied mainly on the data and three hyperparameters: number
    of epochs, batch size, and learning rate multiplier [[9](#bib.bib9)]. These experiments
    achieved moderate results due to the API’s limitations and its economical cost
    of fine-tuning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的研究[[8](#bib.bib8)]中，提示工程是主要关注点，LLM模型在零样本和少样本提示下表现出色。那项工作利用提示工程有效地控制了Kerbal
    Space Program（KSP）模拟中的航天器，取得了卓越但不可推广的结果。此外，还使用OpenAI的微调API进行了若干微调实验。当时，最新且最强大的微调模型是gpt-3.5-turbo-0125。该微调过程的定制主要依赖于数据和三个超参数：周期数、批量大小和学习率倍增器[[9](#bib.bib9)]。由于API的限制和微调的经济成本，这些实验取得了中等结果。
- en: The objective of the current study is to expand upon previous research by incorporating
    a set of open-source tools to overcome the limitations of the earlier framework.
    In contrast to the previous paper, this study focuses solely on the PE1_I3_E3
    scenario. PE refers to the pursuer-evader problem (rendezvous), I3 denotes the
    initial position (2.7 km of separation distance), and E3 represents a heuristic
    maneuvering technique. [[10](#bib.bib10)]. In the past two years, the landscape
    of LLMs has undergone significant transformations, characterized by frequent and
    substantial updates. Given these advancements, the "mighty" ChatGPT model is now
    rivaled by capable models such as Claude[[11](#bib.bib11)], Gemini[[12](#bib.bib12)],
    and open-source models like Mistral[[13](#bib.bib13)] and LLaMA[[14](#bib.bib14)].
    This research focuses on these latter models due to their enhanced flexibility
    and open source nature, which is crucial for pioneering research in this relatively
    unexplored area.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的目标是通过整合一套开源工具，扩展之前的研究，以克服早期框架的限制。与之前的论文相比，本研究仅关注PE1_I3_E3场景。PE指的是追逐者-逃避者问题（会合），I3表示初始位置（2.7公里的分离距离），E3代表启发式机动技术。[[10](#bib.bib10)]。在过去两年里，LLM的格局发生了重大变化，特征是频繁且显著的更新。鉴于这些进展，“强大的”ChatGPT模型现在与Claude[[11](#bib.bib11)]、Gemini[[12](#bib.bib12)]以及开源模型如Mistral[[13](#bib.bib13)]和LLaMA[[14](#bib.bib14)]等有能力的模型竞争。这项研究重点关注这些后者模型，因为它们具有增强的灵活性和开源性质，这对于在这个相对未经探索的领域中的开创性研究至关重要。
- en: The integration of a code agent with KSP is facilitated through a Remote Procedure
    Call (RPC) program that connects to the selected environment within the game.
    After each state update, the agent is able to execute an action from a defined
    set of continuous throttle commands. For a more verbose interaction with the LLM,
    the actions are verbal—forward, backward, right, left, up, and down—which are
    then converted into full throttle, full reverse throttle, or no action for each
    of the three thrusts. This discretization also allows the model to be more like
    a ‘human pilot’ instead of a control algorithm.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过远程过程调用（RPC）程序，将代码代理与KSP集成，它连接到游戏内选择的环境。每次状态更新后，代理能够从定义的一组连续油门命令中执行一个动作。为了与LLM进行更详细的交互，这些动作是口头的——前进、后退、右转、左转、向上和向下——这些动作随后被转换为每个推力的全油门、全反向油门或无动作。这种离散化也使模型更像是一个“人类飞行员”，而不是控制算法。
- en: 'The primary challenge to address when fine tuning a model for KSPDG is the
    unavailability of varied mission scenarios, and the lack of expert gameplay logs
    completing those missions. In fact, this is one of the keys that prevented us
    from scaling up fine-tuning in [[8](#bib.bib8)]. [Figure 2](#S1.F2 "In 1 Introduction
    ‣ Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal
    Space Program") depicts a diagram of a program, alongside a bot that tracks the
    KSP navball’s information³³3https://wiki.kerbalspaceprogram.com/wiki/Navball and
    aligns the vessel to its prograde, to generate a number of pairs of randomized
    orbits for the pursuer and the evader problem.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 KSPDG 微调模型时，主要的挑战是任务场景的多样性不足以及缺乏完成这些任务的专家游戏日志。实际上，这也是阻止我们在 [[8](#bib.bib8)]
    中扩大微调规模的关键因素之一。[图 2](#S1.F2 "在 1 引言 ‣ 微调 LLMs 以实现自主航天器控制：以 Kerbal Space Program
    为案例研究") 展示了一个程序的示意图，以及一个跟踪 KSP 导航球信息的机器人³³3https://wiki.kerbalspaceprogram.com/wiki/Navball，并将航天器对准其前进方向，以生成一对随机轨道用于追击者与逃避者问题。
- en: 'The eccentricity, inclination, semimajor axis, and true anomaly of the pursuer’s
    orbit were randomly generated within the given constraints: eccentricity $\leq$
    3 km. The longitude of the ascending node and argument of periapsis were kept
    constant, ensuring the mission feasibility within a 4-minute span.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 追击者轨道的离心率、倾斜角、半长轴和真实异常值在给定约束条件下随机生成：离心率 $\leq$ 3 km。升交点经度和近日点参数保持不变，确保任务在 4
    分钟内的可行性。
- en: '![Refer to caption](img/e458d1191d6335e6d69a1d63c7b11ad7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e458d1191d6335e6d69a1d63c7b11ad7.png)'
- en: 'Figure 2: Diagram of the data generation process for fine-tuning the model.
    The sequence is as follows: (1) The orbit generator is invoked to create a new
    orbit. (2) The new orbit is saved into KSP. (3) The navball agent is activated
    to navigate the orbit and generate logs. (4) The logs are saved by the orbit generator.
    (5) After sufficient runs (e.g., 100), the script data parser converts the logs
    into text suitable for LLM processing.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于微调模型的数据生成过程示意图。过程如下：(1) 调用轨道生成器创建一个新轨道。(2) 将新轨道保存到 KSP 中。(3) 激活导航球代理以导航轨道并生成日志。(4)
    轨道生成器保存日志。(5) 在运行次数达到足够多（例如 100 次）后，脚本数据解析器将日志转换为适合 LLM 处理的文本。
- en: Once the issue of data availability is resolved, the real focus of this research—fine-tuning—can
    be tackled. While OpenAI models, such as those used in previous studies, offer
    high-quality function calling and perform well with minimal customization, they
    present significant limitations for large-scale training due to cost and reduced
    flexibility. These constraints limited our previous experiments to using only
    1-2 files of human gameplay logs with discretized throttle actions to test the
    behavior of GPT models with limited data. Though these initial results were promising,
    they underscored the need for a more adaptable and cost-effective solution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据可用性的问题得到解决，本研究的真正重点——微调——可以开始处理。尽管像 OpenAI 的模型在以前的研究中提供了高质量的功能调用，并且在最少的自定义情况下表现良好，但由于成本和灵活性降低，它们在大规模训练中存在显著限制。这些限制使我们的先前实验仅使用
    1-2 个文件的人工游戏日志以及离散化的油门操作来测试 GPT 模型的行为。虽然这些初步结果是有希望的，但它们突显了对更具适应性和成本效益的解决方案的需求。
- en: To address the challenges in this study, which demand specialized domain-specific
    knowledge for mission success, we transitioned to an open-source model capable
    of local training. LLaMA, the most renowned open-source model as of 2024, was
    chosen for its broad adoption and extensive research community support. The flexibility
    and adaptability of LLaMA allow for comprehensive fine-tuning tailored to our
    specific needs, moving beyond aggressive prompt engineering to a more data-driven
    approach.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决本研究中要求特定领域知识以确保任务成功的挑战，我们转向了一种能够本地训练的开源模型。LLaMA，作为截至 2024 年最著名的开源模型，被选中因为其广泛的应用和丰富的研究社区支持。LLaMA
    的灵活性和适应性允许我们进行全面的微调，以满足我们特定的需求，从而超越了激进的提示工程，转向更数据驱动的方法。
- en: 'To fine-tune LLaMA efficiently⁴⁴4The LLaMA version used in this work is LLaMA-3-8B,
    we utilized a single workstation equipped with five RTX 4090 GPUs, employing several
    optimization techniques and tools to enhance efficiency:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地微调 LLaMA⁴⁴4本工作使用的 LLaMA 版本为 LLaMA-3-8B，我们利用了一台配备五块 RTX 4090 GPU 的工作站，并采用了多种优化技术和工具以提升效率：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Low-Rank Adaptation (LoRA): LoRA reduces the number of trainable parameters
    by factorizing weight updates into low-rank matrices. Being more computational
    effective [[15](#bib.bib15)].'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）：LoRA通过将权重更新分解为低秩矩阵来减少可训练参数的数量，从而在计算上更为高效[[15](#bib.bib15)]。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hugging Face Transformers Library: This library facilitated the management
    of model architecture and training processes [[16](#bib.bib16)].'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hugging Face Transformers库：该库便于管理模型架构和训练过程[[16](#bib.bib16)]。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization: To reduce model size and enhance inference speed, we applied
    quantization, converting weights and activations to lower precision without significantly
    impacting performance [[17](#bib.bib17)].'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化：为了减少模型大小并提高推理速度，我们应用了量化，将权重和激活转换为较低的精度，而不会显著影响性能[[17](#bib.bib17)]。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLaMA Factory: This tool streamlined the fine-tuning process by efficiently
    managing datasets, pre-processing, and training tasks [[18](#bib.bib18)].'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA Factory：该工具通过高效管理数据集、预处理和训练任务来简化微调过程[[18](#bib.bib18)]。
- en: Training hyperparameters included a batch size of 2, a learning rate of 1e-4,
    a cosine learning rate scheduler, 3 epochs, a LoRA configuration (rank=16, alpha=8,
    dropout=0.05), Flash Attention 2 and Dora enabled, and gradient accumulation steps
    of 2. Fine-tuning only the last layers (2 in this case) and using a small batch
    size are recommended [[19](#bib.bib19), [20](#bib.bib20)]. The very small batch
    size here was due to hardware limitations. Extending the training beyond 3 epochs
    did not improve loss.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练超参数包括：批次大小为2，学习率为1e-4，余弦学习率调度器，3个训练轮次，LoRA配置（rank=16，alpha=8，dropout=0.05），启用Flash
    Attention 2和Dora，梯度累积步数为2。建议仅微调最后的层（此处为2层）并使用小批次大小[[19](#bib.bib19)，[20](#bib.bib20)]。这里的小批次大小由于硬件限制。训练超过3轮并没有改善损失。
- en: 'The LLaMA dataset consisted of 50 top-performing randomly generated orbit missions
    from the navball agent (see [Figure 2](#S1.F2 "In 1 Introduction ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program")),
    chosen for their optimal distance (meters) and approach speed (seconds). These
    missions employed less aggressive prompting while still utilizing the chain-of-thought
    technique [[21](#bib.bib21)] to enable the model to learn and apply its own reasoning
    effectively.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA数据集包含了50个来自navball代理的随机生成的表现优异的轨道任务（见[图2](#S1.F2 "在 1 引言 ‣ 对自主航天器控制的LLM微调：以Kerbal
    Space Program为例")），这些任务因其最佳的距离（米）和接近速度（秒）而被选中。这些任务在使用链式思维技术[[21](#bib.bib21)]的同时，采用了较少的激进提示，以便模型能够有效地学习和应用自身的推理能力。
- en: 2 Results
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 结果
- en: 'In [Table 1](#S2.T1 "In 2 Results ‣ Fine-tuning LLMs for Autonomous Spacecraft
    Control: A Case Study Using Kerbal Space Program"), we present the results of
    fine-tuned GPT models using human gameplays and LLaMA models using navball agent
    gameplays. GPT models show gradual improvement, surpassing the baseline after
    two training gameplays, indicating the potential of a fine-tuned model with more
    gameplays. Note that the simple fine-tuning in the GPT experiments used only one
    file, basic prompting, and the default hyperparameters selected by the OpenAI
    API. The LLaMA dataset was divided into subsets of 10, 25, and 50 gameplay files.
    One subset—the 10 files—utilized a sliding window technique, where previous actions
    were included to provide the model with additional context. LLaMA models consistently
    exceed their baseline performance, where the closest distance is ~120 meters better
    than GPT’s baseline⁵⁵5The GPT Model is 3.5, which is older than LLaMA 3. The best
    LLaMA models perform exceptionally well, demonstrating the potential benefits
    of larger datasets, indicating the potential of a fine-tuned model with more gameplays.
    Finally, the model utilizing the sliding window technique demonstrates great performance
    results leveraging the context of the LLM.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表1](#S2.T1 "在 2 结果 ‣ 对自主航天器控制的LLM微调：以Kerbal Space Program为例")中，我们展示了使用人类游戏玩法微调的GPT模型和使用navball代理游戏玩法的LLaMA模型的结果。GPT模型显示出逐步改进，在两次训练游戏玩法后超过了基准线，表明更多游戏玩法的微调模型具有潜力。需要注意的是，GPT实验中的简单微调只使用了一个文件、基本提示和OpenAI
    API选择的默认超参数。LLaMA数据集被划分为10、25和50个游戏文件的子集。其中一个子集——10个文件——利用了滑动窗口技术，将之前的动作包括在内，以为模型提供额外的上下文。LLaMA模型的性能始终超过其基准线，最近的距离比GPT基准线好约120米。最好的LLaMA模型表现非常出色，展示了较大数据集的潜在好处，表明更多游戏玩法的微调模型具有潜力。最后，利用滑动窗口技术的模型通过利用LLM的上下文展示了出色的性能结果。
- en: Method Distance (m) Best       Average       Worst Failure Rate Average Latency
    (ms) baseline GPT 178.11 200.10 232.16 36.8% 840.42 simple fine-tuning 263.55
    265.89 271.51 0.0% 987.43 + hyperparameter tuning 188.90 202.08 210.62 0.1% 831.30
    + system prompt 197.41 214.87 227.67 0.0% 753.52 + two train gameplays 132.09
    159.78 200.47 0.2% 557.49 baseline LLaMA 52.69 140.68 267.32 9.09% 8580.43 fine-tune
    10 files 30.52 51.53 80.88 0.00% 3444.88 fine-tune 25 files 13.54 29.44 58.83
    0.00% 3316.89 fine-tune 50 files 11.86 29.76 48.81 0.00% 3455.29 fine-tune 10
    files win=3 23.08 40.03 49.28 0.00% 3292.44 human gameplays 5.97 6.25 6.54 - -
    navball agent 34.34 36.43 39.76 - -
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 距离（米） 最佳       平均       最差 失败率 平均延迟（毫秒） 基准 GPT 178.11 200.10 232.16 36.8%
    840.42 简单微调 263.55 265.89 271.51 0.0% 987.43 + 超参数调优 188.90 202.08 210.62 0.1%
    831.30 + 系统提示 197.41 214.87 227.67 0.0% 753.52 + 两次训练游戏 132.09 159.78 200.47 0.2%
    557.49 基准 LLaMA 52.69 140.68 267.32 9.09% 8580.43 微调 10 文件 30.52 51.53 80.88 0.00%
    3444.88 微调 25 文件 13.54 29.44 58.83 0.00% 3316.89 微调 50 文件 11.86 29.76 48.81 0.00%
    3455.29 微调 10 文件 win=3 23.08 40.03 49.28 0.00% 3292.44 人类游戏玩法 5.97 6.25 6.54 -
    - 导航球代理 34.34 36.43 39.76 - -
- en: 'Table 1: Performance of fine-tuned models for each technique, measured in distance
    (meters) and latency (milliseconds). Bold indicates best, underline indicates
    second best, and dashed underline indicates third best.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：每种技术的微调模型性能，以距离（米）和延迟（毫秒）为单位。**粗体**表示最佳，__下划线__表示第二好，__**虚线下划线**__表示第三好。
- en: 'The fine-tuning trajectories in [Figure 3](#S2.F3 "In 2 Results ‣ Fine-tuning
    LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program")
    indicate that the data ingested by the model aids in understanding the problem
    and determining appropriate actions. However, these trajectories also show that
    the model’s prior knowledge and reasoning still influence its performance. For
    instance, an incorrect hint, as depicted by the GPT trajectory, deteriorates the
    model’s performance and makes the agent recede from the evader once it overshoots
    (meaning when it goes past the evader). In contrast, an "agnostic" prompt that
    complements rather than dictates the model’s reasoning can even surpass the dataset
    results.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3](#S2.F3 "在 2 结果 ‣ 微调 LLMs 用于自主航天器控制：以 Kerbal Space Program 为例") 中的微调轨迹表明，模型所获取的数据有助于理解问题并确定适当的行动。然而，这些轨迹也表明，模型的先验知识和推理仍然影响其性能。例如，如
    GPT 轨迹所示，错误的提示会恶化模型的表现，并使得代理在超出目标后从逃避者处退回。相比之下，能够补充而不是主导模型推理的“不可知”提示甚至可能超越数据集结果。'
- en: '![Refer to caption](img/9e48df88b627283afa953d0649ad8265.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9e48df88b627283afa953d0649ad8265.png)'
- en: 'Figure 3: This 3D plot depicts the trajectories of the best-performing fine-tuned
    models for GPT and LLaMA, along with the evader’s path. Due to an incorrect hint,
    the GPT model deviates significantly after overshooting its target, while the
    LLaMA model maintains a closer trajectory to the evader.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：此 3D 图展示了 GPT 和 LLaMA 最佳性能微调模型的轨迹，以及逃避者的路径。由于错误提示，GPT 模型在超出目标后偏离明显，而 LLaMA
    模型则保持了更接近逃避者的轨迹。
- en: 3 Discussion
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 讨论
- en: The preliminary results of this study demonstrate that LLMs possess the capability
    to perform technical tasks beyond merely generating verbose text. Fine-tuning
    these models enhances their reasoning for autonomous space control missions without
    solely depending on the hints and reasoning provided in the prompt. This process
    yields a generalized model that can interact as an agent in KSP rendezvous missions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的初步结果表明，LLMs 具备执行技术任务的能力，超越了单纯生成冗长文本的范畴。微调这些模型可以提高其在自主空间控制任务中的推理能力，而不单依赖于提示中提供的提示和推理。这个过程产生了一个可以在
    KSP 会合任务中作为代理进行互动的通用模型。
- en: The fine-tuned LLaMA model clearly surpasses the navball agent results, even
    in average distance, which poses a singular use case where the model outperforms
    the agent responsible of creating its own training data. The tests were run on
    validation sets, thus remarking the generalization capacity LLMs offer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的 LLaMA 模型明显超越了导航球代理的结果，即使在平均距离上也是如此，这提出了一个特殊的用例，其中模型在创建自身训练数据的代理的情况下表现优异。测试是在验证集上进行的，这突显了
    LLMs 提供的泛化能力。
- en: However, the complexity of a trajectory is not solely determined by distance.
    This highlights the necessity for more complex and dynamic scenarios that require
    additional metrics. This issue will be addressed in the next stages of this research.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，轨迹的复杂性不仅仅由距离决定。这突显了需要更多复杂和动态场景的必要性，这些场景需要额外的指标。这个问题将在本研究的下一阶段中解决。
- en: Moreover, the close distances achieved by the LLaMA models pave the way for
    exploring docking operations, where the increased complexity will test the robustness
    of the model’s chosen trajectories.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLaMA模型所实现的接近距离为探索对接操作铺平了道路，在这些操作中，增加的复杂性将考验模型选择的轨迹的鲁棒性。
- en: We also plan on leveraging the advantages offered by multi-modal LLMs, such
    as the recently released GPT-4o [[22](#bib.bib22)] (where ’o’ stands for ’omni’)
    and the Phi-3 family [[23](#bib.bib23)], an open-source model incorporating multi-modal
    capabilities. Specifically, we intend to utilize vision capabilities in conjunction
    with language, as both modalities show the potential for creating an agent with
    human-like decisions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计划利用多模态LLM提供的优势，例如最近发布的GPT-4o [[22](#bib.bib22)]（其中’o’代表’omni’）和Phi-3系列 [[23](#bib.bib23)]，这是一个包含多模态能力的开源模型。具体来说，我们打算将视觉能力与语言能力结合起来，因为这两种模态都有可能创造出具有类人决策能力的智能体。
- en: References
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Introducing chatgpt. https://openai.com/blog/chatgpt. (Accessed on 03/29/2024).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 介绍chatgpt。https://openai.com/blog/chatgpt。（访问时间：2024年3月29日）。'
- en: '[2] Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov,
    Amos Azaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms
    by studying papers and reasoning. arXiv preprint arXiv:2305.15486, 2023.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov,
    Amos Azaria, Tom Mitchell 和 Yuanzhi Li. Spring: GPT-4通过研究论文和推理超越了强化学习算法。arXiv预印本
    arXiv:2305.15486，2023年。'
- en: '[3] Peng Mun Siew, Daniel Jang, Thomas G Roberts, and Richard Linares. Space-based
    sensor tasking using deep reinforcement learning. The Journal of the Astronautical
    Sciences, 69(6):1855–1892, 2022.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Peng Mun Siew, Daniel Jang, Thomas G Roberts 和 Richard Linares. 基于空间的传感器任务使用深度强化学习。宇航科学杂志，69(6):1855–1892，2022年。'
- en: '[4] Brian Gaudet, Richard Linares, and Roberto Furfaro. Deep reinforcement
    learning for six degree-of-freedom planetary landing. Advances in Space Research,
    65(7):1723–1741, 2020.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Brian Gaudet, Richard Linares 和 Roberto Furfaro. 用于六自由度行星着陆的深度强化学习。空间研究进展，65(7):1723–1741，2020年。'
- en: '[5] Ross E. Allen, Yaron Rachlin, Jessica Ruprecht, Sean Loughran, Jacob Varey,
    and Herbert Viggh. Spacegym: Discrete and differential games in non-cooperative
    space operations. In 2023 IEEE Aerospace Conference, pages 1–12, 2023.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ross E. Allen, Yaron Rachlin, Jessica Ruprecht, Sean Loughran, Jacob Varey
    和 Herbert Viggh. Spacegym: 非合作空间操作中的离散和微分游戏。在2023 IEEE 航空航天会议，页码 1–12，2023年。'
- en: '[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
    Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
    Jie Tang 和 Wojciech Zaremba. OpenAI gym。arXiv预印本 arXiv:1606.01540，2016年。'
- en: '[7] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth
    Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo
    Perez-Vicente, et al. Pettingzoo: Gym for multi-agent reinforcement learning.
    Advances in Neural Information Processing Systems, 34:15032–15043, 2021.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth
    Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo
    Perez-Vicente 等。Pettingzoo: 多智能体强化学习的Gym。神经信息处理系统进展，34:15032–15043，2021年。'
- en: '[8] Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli Scharf,
    Peng Mun Siew, and Richard Linares. Language models are spacecraft operators.
    arXiv preprint arXiv:2404.00413, 2024.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli Scharf,
    Peng Mun Siew 和 Richard Linares. 语言模型是航天器操作员。arXiv预印本 arXiv:2404.00413，2024年。'
- en: '[9] OpenAI. Analyzing your fine-tuned model. https://platform.openai.com/docs/guides/fine-tuning/analyzing-your-fine-tuned-model,
    2024. Accessed: 2024-05-26.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] OpenAI. 分析你的微调模型。https://platform.openai.com/docs/guides/fine-tuning/analyzing-your-fine-tuned-model，2024年。访问时间：2024-05-26。'
- en: '[10] Ray Allen. spacegym-kspdg. https://github.com/mit-ll/spacegym-kspdg, 2023.
    Accessed: October 4, 2023.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ray Allen. spacegym-kspdg。https://github.com/mit-ll/spacegym-kspdg，2023年。访问时间：2023年10月4日。'
- en: '[11] Claude ai. https://claude.ai/. Accessed: 2024-05-27.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Claude ai. https://claude.ai/。访问时间：2024-05-27。'
- en: '[12] Gemini ai. https://gemini.google.com/. Accessed: 2024-05-27.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Gemini ai. https://gemini.google.com/。访问时间：2024-05-27。'
- en: '[13] Tim Dettmers, Hugo Touvron, Antoine Joulin, Edouard Grave, Roberta Raileanu,
    and Myle Ott. Mistral 7b, 2023.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tim Dettmers, Hugo Touvron, Antoine Joulin, Edouard Grave, Roberta Raileanu
    和 Myle Ott. Mistral 7b, 2023年。'
- en: '[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等. Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023.'
- en: '[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zhewei Yao, Weizhu Chen, Huan
    Wang, Zhen Dong, and Izhak Shafran. Lora: Low-rank adaptation of large language
    models, 2024.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zhewei Yao, Weizhu Chen, Huan
    Wang, Zhen Dong 和 Izhak Shafran. Lora：大语言模型的低秩适配, 2024.'
- en: '[16] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers:
    State-of-the-art natural language processing. In Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations, pages
    38–45, 2020.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz 等. Transformers：最先进的自然语言处理。在2020年自然语言处理经验方法会议：系统演示文稿中，页码
    38–45, 2020.'
- en: '[17] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. White
    noise: Regularizing neural networks through quantization. In International Conference
    on Machine Learning, pages 7197–7206\. PMLR, 2021.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Markus Nagel, Mart van Baalen, Tijmen Blankevoort 和 Max Welling. 白噪声：通过量化正则化神经网络。在国际机器学习会议上，页码
    7197–7206。PMLR, 2021.'
- en: '[18] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and
    Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models.
    arXiv preprint arXiv:2403.13372, 2024.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo 和 Yongqiang
    Ma. Llamafactory：统一高效地微调 100+ 语言模型。arXiv 预印本 arXiv:2403.13372, 2024.'
- en: '[19] Dominic Masters and Carlo Luschi. Revisiting small batch training for
    deep neural networks. ArXiv, abs/1804.07612, 2018.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Dominic Masters 和 Carlo Luschi. 重新审视小批量训练在深度神经网络中的应用。ArXiv, abs/1804.07612,
    2018.'
- en: '[20] Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman. Autofreeze: Automatically
    freezing model blocks to accelerate fine-tuning. ArXiv, abs/2102.01386, 2021.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yuhan Liu, Saurabh Agarwal 和 Shivaram Venkataraman. Autofreeze：自动冻结模型块以加速微调。ArXiv,
    abs/2102.01386, 2021.'
- en: '[21] Jason Wei et al. Chain of thought prompting elicits reasoning in large
    language models. arXiv preprint arXiv:2201.11903, 2022.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jason Wei 等. 思维链提示引发大语言模型的推理。arXiv 预印本 arXiv:2201.11903, 2022.'
- en: '[22] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed:
    2024-05-27.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] OpenAI. Hello gpt-4o。https://openai.com/index/hello-gpt-4o/，2024. 访问时间：2024-05-27.'
- en: '[23] Misha Bilenko. New models added to the phi-3 family, available on microsoft
    azure. https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/,
    2024. Accessed: 2024-05-27.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Misha Bilenko. 新模型添加到 phi-3 家族，现已在微软 Azure 上提供。https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/，2024.
    访问时间：2024-05-27.'
