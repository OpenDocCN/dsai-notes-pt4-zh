- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-tuned
    LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 草稿模型的直接对齐以实现与聊天微调LLMs的猜测解码
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00858](https://ar5iv.labs.arxiv.org/html/2403.00858)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00858](https://ar5iv.labs.arxiv.org/html/2403.00858)
- en: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher
    Lott
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher
    Lott
- en: 'Qualcomm AI Research Correspondence to: $\{$@qualcomm.qti.com'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Qualcomm AI Research 联系方式：$\{$@qualcomm.qti.com
- en: Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Qualcomm AI Research 是高通技术公司的一个倡议。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Text generation with Large Language Models (LLMs) is known to be memory bound
    due to the combination of their auto-regressive nature, huge parameter counts,
    and limited memory bandwidths, often resulting in low token rates. Speculative
    decoding has been proposed as a solution for LLM inference acceleration. However,
    since draft models are often unavailable in the modern open-source LLM families,
    e.g., for Llama 2 7B, training a high-quality draft model is required to enable
    inference acceleration via speculative decoding. In this paper, we propose a simple
    draft model training framework for direct alignment to chat-capable target models.
    With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model
    for Llama 2 Chat 7B or larger, with only 1.64% of the original size. Our training
    framework only consists of pretraining, distillation dataset generation, and finetuning
    with knowledge distillation, with no additional alignment procedure. For the finetuning
    step, we use instruction-response pairs generated by target model for distillation
    in plausible data distribution, and propose a new Total Variation Distance++ (TVD++)
    loss that incorporates variance reduction techniques inspired from the policy
    gradient method in reinforcement learning. Our empirical results show that Llama
    2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency
    and 2.4$\times$ speed-up relative to autoregressive decoding on various tasks
    with no further task-specific fine-tuning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大语言模型（LLMs）进行文本生成被认为是受限于内存的，这由于其自回归特性、大量参数和有限的内存带宽，通常导致低令牌速率。已经提出了猜测解码作为LLM推理加速的解决方案。然而，由于现代开源LLM家族中的草稿模型通常不可用，例如对于Llama
    2 7B，需要训练一个高质量的草稿模型以实现通过猜测解码的推理加速。在本文中，我们提出了一个简单的草稿模型训练框架，直接对齐到能够聊天的目标模型。使用所提出的框架，我们训练了Llama
    2 Chat Drafter 115M，这是一个针对Llama 2 Chat 7B或更大模型的草稿模型，只有原始模型大小的1.64%。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏的微调，没有额外的对齐过程。在微调步骤中，我们使用目标模型生成的指令-响应对进行蒸馏，符合合理的数据分布，并提出了一种新的总变差距离++（TVD++）损失，该损失结合了来自强化学习中的策略梯度方法的方差减少技术。我们的实证结果表明，使用猜测解码的Llama
    2 Chat Drafter 115M在各种任务上相对于自回归解码实现了高达2.3倍的块效率和2.4$\times$的加速，并且没有进一步的任务特定微调。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have become universal and versatile tools in our
    daily life thanks to their impressive results and superior performance in a wide
    range of domains (Achiam et al., [2023](#bib.bib1); Anil et al., [2023](#bib.bib3);
    Roziere et al., [2023](#bib.bib18)). There has been increasing demands for running
    edge LLMs directly on user devices for numerous reasons such as privacy, security,
    cost reduction, reliability, and personalization. However, LLMs naturally generates
    tokens in an auto-regressive manner which is an inherently slow process due to
    memory bandwidth bottleneck. Speculative decoding (SD) has been proposed to mitigate
    the inference speed bottleneck and accelerate LLM inference by introducing a smaller
    draft model to predict the output of the large target model(Leviathan et al.,
    [2023](#bib.bib12); Chen et al., [2023](#bib.bib6)). In each iteration of SD,
    the draft model generates a sequence of tokens and the target model accepts a
    sub-sequence of the draft tokens using a modified rejection sampling criteria.
    It has been shown that SD can provide up to 2-3$\times$ speedup in LLM inference
    without any loss in text generation quality.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）由于在各种领域表现出色，已经成为我们日常生活中普遍且多功能的工具（Achiam et al., [2023](#bib.bib1);
    Anil et al., [2023](#bib.bib3); Roziere et al., [2023](#bib.bib18)）。出于隐私、安全、成本降低、可靠性和个性化等多种原因，越来越多地需要在用户设备上直接运行边缘
    LLMs。然而，LLMs 自然以自回归方式生成标记，这是一种由于内存带宽瓶颈而固有的慢过程。已提出**推测解码（SD）**以缓解推理速度瓶颈，并通过引入较小的草稿模型来预测大型目标模型的输出，从而加速
    LLM 推理（Leviathan et al., [2023](#bib.bib12); Chen et al., [2023](#bib.bib6)）。在
    SD 的每次迭代中，草稿模型生成一系列标记，目标模型使用修改后的拒绝采样标准接受草稿标记的子序列。已经显示，SD 可以在 LLM 推理中提供高达 2-3$\times$
    的加速，而不会影响文本生成质量。
- en: Despite the promising performance of SD, high-quality small language models
    that can serve as draft models are often unavailable, since the smallest members
    of modern open-source LLM families, such as Llama 2 (Touvron et al., [2023](#bib.bib20)),
    Falcon (Penedo et al., [2023](#bib.bib17)), and Baichuan (Baichuan, [2023](#bib.bib4))
    are at the scale of several billions of parameters. Models with this scale are
    either considered as target models on edge devices or too large as draft models
    to get meaningful inference speed improvement with SD. Enabling SD with these
    large models on small devices require a much smaller draft model which is well
    aligned with the target model. When it comes to training a draft model for a given
    target model, a practical challenge is that the original dataset used for training
    the target model may not be provided that prohibits following the same training
    procedure of the target model as a natural way of mimicking the target model behavior.
    Recent works for draft model training has taken inspiration from knowledge distillation
    (Hinton et al., [2015](#bib.bib8)) where student models were trained using token-level
    distillation (Kim & Rush, [2016](#bib.bib9)) which was extended to distribution-level
    distillation (Zhou et al., [2023](#bib.bib23); Agarwal et al., [2023](#bib.bib2);
    Wen et al., [2023](#bib.bib21); Lin et al., [2020](#bib.bib13)) when the entire
    teacher model distributions are accessible. These methods use either KL-divergence
    (forward, backward or Jenson-Shannon) or total variation distance for training
    the student model, focusing only on improving student model generation instead
    of improving SD performance (Agarwal et al., [2023](#bib.bib2); Wen et al., [2023](#bib.bib21)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 SD 的性能令人鼓舞，但高质量的小型语言模型通常不可用，这些模型可以作为草稿模型，因为现代开源 LLM 家族中的最小成员，如 Llama 2（Touvron
    et al., [2023](#bib.bib20)）、Falcon（Penedo et al., [2023](#bib.bib17)）和 Baichuan（Baichuan,
    [2023](#bib.bib4)），其参数规模通常达到数十亿。这些规模的模型要么被视为边缘设备上的目标模型，要么作为草稿模型规模过大，无法通过 SD 获得有意义的推理速度提升。在小型设备上启用
    SD 需要一个与目标模型高度对齐的更小的草稿模型。当涉及到为特定目标模型训练草稿模型时，一个实际挑战是可能无法提供用于训练目标模型的原始数据集，这阻止了按照与目标模型行为相似的自然方式进行训练。最近的草稿模型训练工作从**知识蒸馏（Hinton
    et al., [2015](#bib.bib8)）**中获得了灵感，其中学生模型通过标记级蒸馏（Kim & Rush, [2016](#bib.bib9)）进行训练，这一方法扩展到了分布级蒸馏（Zhou
    et al., [2023](#bib.bib23); Agarwal et al., [2023](#bib.bib2); Wen et al., [2023](#bib.bib21);
    Lin et al., [2020](#bib.bib13)），当整个教师模型分布可用时。这些方法使用 KL 散度（前向、反向或 Jenson-Shannon）或总变差距离来训练学生模型，专注于改善学生模型的生成，而不是提高
    SD 性能（Agarwal et al., [2023](#bib.bib2); Wen et al., [2023](#bib.bib21)）。
- en: 'In this work, we propose a framework for draft model training for speculative
    decoding. Our training pipeline consists of three phases : 1) pre-training; training
    a randomly initialized draft model on a large open source text corpus to gain
    the general language modeling capabilities. 2) generation of a distillation dataset
    by having the target model generate responses to various instructions. Since the
    goal of the draft model is to mimic the target model we first generate the plausible
    data distribution using open source instruction fine-tuning datasets. 3) Fine-tuning
    on the distillation dataset for aligning draft and target model behaviors with
    target model in the loop. In addition, we propose a novel distillation loss, Total
    Variation Distance++ (TVD++) loss, by making connections between the acceptance
    of drafts in SD and policy gradient method in reinforcement learning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个用于推测解码的草稿模型训练框架。我们的训练流程包括三个阶段：1）预训练；在一个大型开放源代码文本语料库上训练一个随机初始化的草稿模型，以获得一般的语言建模能力。2）通过让目标模型对各种指令生成响应来生成蒸馏数据集。由于草稿模型的目标是模仿目标模型，我们首先使用开放源指令微调数据集生成合理的数据分布。3）在蒸馏数据集上进行微调，以便将草稿模型和目标模型的行为与目标模型对齐。此外，我们提出了一种新颖的蒸馏损失，即总变距距离++（TVD++）损失，通过在SD中的草稿接受与强化学习中的策略梯度方法之间建立联系。
- en: 'With this framework, we train a draft model for Llama 2-Chat-7B model (Touvron
    et al., [2023](#bib.bib20)) of size 115M which is only 1.64% of the size of the
    target model, Llama 2-Chat-Drafter-115M. We evaluate our draft model on various
    tasks like open-ended generation on Databricks-dolly-15k dataset (Conover et al.,
    [2023](#bib.bib7)) and text summarization on XSum (Narayan et al., [2018](#bib.bib15)),
    CNN/DailyMail datasets (Nallapati et al., [2016](#bib.bib14)). We show that our
    trained model can obtain up to 2.4$\times$ speed-up with speculative decoding
    over auto-regressive inference demonstrating the effectiveness of our training
    pipeline. Additionally, our proposed loss outperforms commonly used distillation
    losses: KLD and TVD.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个框架，我们为Llama 2-Chat-7B模型（Touvron等，[2023](#bib.bib20)）训练了一个草稿模型，大小为115M，仅为目标模型Llama
    2-Chat-Drafter-115M大小的1.64%。我们在各种任务上评估了我们的草稿模型，例如在Databricks-dolly-15k数据集（Conover等，[2023](#bib.bib7)）上的开放式生成和在XSum（Narayan等，[2018](#bib.bib15)）、CNN/DailyMail数据集（Nallapati等，[2016](#bib.bib14)）上的文本摘要。我们展示了我们的训练模型在使用推测解码时可以获得高达2.4$\times$的速度提升，相较于自回归推理，展示了我们训练流程的有效性。此外，我们提出的损失函数优于常用的蒸馏损失：KLD和TVD。
- en: 2 Method
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: Our draft model training pipeline involves three steps with the goal of aligning
    the output of draft model with that of the target language model in the context
    of speculative decoding process, as opposed to having the draft model as a standalone
    language model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的草稿模型训练流程涉及三个步骤，目的是在推测解码过程中将草稿模型的输出与目标语言模型的输出对齐，而不是将草稿模型作为独立的语言模型。
- en: 2.1 Draft model pretraining
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 草稿模型预训练
- en: A draft model is required to have the same tokenizer as the corresponding target
    model with the same or a subset of vocabulary. Some language model families sharing
    the same tokenizer have wide ranges of model sizes where we can pick draft models
    at a desired sizes, e.g., Pythia (biderman2023pythia). However, this is not the
    case more often, e.g., Llama 2 (Touvron et al., [2023](#bib.bib20)), where the
    smallest is still too large. In such cases, we train a draft language model from
    scratch with the regular next token prediction loss in order to have a good language
    model to be aligned with the target model more effectively. It is possible to
    incorporate initialization techniques such as Weight Subcloning (samragh2023weight)
    to expedite this step. While it may be trivial, in our empirical observations,
    pretrained draft models show significantly better alignment to target model, compared
    to those without pretraining.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 草稿模型需要与对应目标模型具有相同的分词器，并且词汇表必须相同或是其子集。一些共享相同分词器的语言模型系列具有广泛的模型尺寸范围，我们可以选择所需尺寸的草稿模型，例如Pythia（bider2023pythia）。然而，更多情况下并非如此，例如Llama
    2（Touvron等，[2023](#bib.bib20)），其中最小的模型仍然过大。在这种情况下，我们从头开始训练一个草稿语言模型，使用常规的下一个词预测损失，以便获得一个与目标模型更有效对齐的良好语言模型。可以采用初始化技术如权重子克隆（samragh2023weight）来加速这一步骤。虽然这可能是微不足道的，但根据我们的实证观察，预训练的草稿模型相比于没有预训练的模型，显示出显著更好的与目标模型对齐效果。
- en: 2.2 Distillation dataset generation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 蒸馏数据集生成
- en: In language modeling, there are virtually infinite number of possible input
    data since the same input token can produce different outputs according to the
    conditions made by the specific contexts, i.e., different prompts and past generations,
    in the causal attention mechanism. To mimic the target behavior in realistic contexts,
    it is crucial to have data samples that are plausible in the target model generation.
    While one convenient way is follow the training procedure of the target model
    including the same instruction and alignment datasets, availability of such datasets
    and reward models are not always guaranteed. To mitigate such challenges and simplify
    the training procedure, we generate a distillation dataset by using seed instructions
    from publicly available datasets and letting the target model generate a diverse
    set of responses in various configuration, e.g., temperature, top-p, and system
    prompts. Note that unlike Zhou et al. ([2023](#bib.bib23)); Agarwal et al. ([2023](#bib.bib2)),
    we only use target model for generating responses as opposed to using the pre-trained
    draft model. This step can be seen as a data-level distillation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言建模中，由于相同的输入令牌可以根据特定上下文条件（即不同的提示和过去的生成）产生不同的输出，因此几乎存在无限数量的可能输入数据。在现实上下文中模拟目标行为时，拥有在目标模型生成中可信的数据样本至关重要。尽管一种方便的方法是遵循目标模型的训练过程，包括相同的指令和对齐数据集，但这些数据集和奖励模型的可用性并不总是有保障。为了缓解这些挑战并简化训练过程，我们通过使用来自公开数据集的种子指令，并让目标模型在各种配置（例如温度、top-p和系统提示）下生成多样化的响应来生成蒸馏数据集。请注意，与Zhou等人([2023](#bib.bib23))；Agarwal等人([2023](#bib.bib2))不同，我们仅使用目标模型生成响应，而不是使用预训练的草稿模型。这一步可以被视为数据级别的蒸馏。
- en: 2.3 Draft model finetuning via knowledge distillation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 通过知识蒸馏对草稿模型进行微调
- en: We then finetune the pretrained draft model using dataset generated as in [2.2](#S2.SS2
    "2.2 Distillation dataset generation ‣ 2 Method ‣ Direct Alignment of Draft Model
    for Speculative Decoding with Chat-Fine-tuned LLMs"). Knowledge distillation comes
    in two different flavors (a) black-box distillation (Kim & Rush, [2016](#bib.bib9))
    where the target model is not accessible and only the token generated by target
    model can be used for distillation, and (b) white-box distillation, where we have
    access to the entire target output distribution. White-box optimization provides
    much stronger learning signals given the loss is optimized over distribution as
    opposed to a single token label, which corresponds to our case.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着使用在[2.2](#S2.SS2 "2.2 Distillation dataset generation ‣ 2 Method ‣ Direct
    Alignment of Draft Model for Speculative Decoding with Chat-Fine-tuned LLMs")中生成的数据集对预训练的草稿模型进行微调。知识蒸馏有两种不同的方式：(a)
    黑箱蒸馏 (Kim & Rush, [2016](#bib.bib9))，在这种方式中目标模型不可访问，仅可以使用目标模型生成的令牌进行蒸馏，以及 (b)
    白箱蒸馏，我们可以访问整个目标输出分布。白箱优化提供了更强的学习信号，因为损失是基于分布进行优化的，而不是单一的令牌标签，这与我们的情况相符。
- en: 'The white-box distillation can be thought as a distribution matching problem,
    where the target model ($\mathcal{M}_{q}$) for a given input text ($x$) output
    distribution ($p_{\theta}(y|x)$). The training objective can be defined using
    a distance metric between distributions such as Kullback–Leibler Divergence (KLD)
    and its variants: backward KLD, Jenson-Shannon Divergence (Agarwal et al., [2023](#bib.bib2)),
    or Total Variation Distance (TVD). Theoretical analysis based on SD (Corollary
    3.6 in (Leviathan et al., [2023](#bib.bib12))) has shown that minimizing TVD is
    equivalent to maximizing acceptance-rate, the true objective of improving SD performance.
    In this paper, we further build on TVD using techniques from reinforcement learning.
    We first show that the gradient step taken when optimizing TVD loss is equivalent
    to the policy-gradient step for reward maximization in reinforcement learning.
    Based on this, we can bring tools from reinforcement learning such as variance
    reduction techniques for improving knowledge distillation loss in LLMs (Korbak
    et al., [2022](#bib.bib11)).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒蒸馏可以被看作是一个分布匹配问题，其中给定输入文本（`x`）的目标模型（$\mathcal{M}_{q}$）输出分布（$p_{\theta}(y|x)$）。训练目标可以通过分布之间的距离度量来定义，例如Kullback–Leibler散度（KLD）及其变体：反向KLD、Jenson-Shannon散度（Agarwal等，[2023](#bib.bib2)）或总变差距离（TVD）。基于SD的理论分析（Leviathan等，[2023](#bib.bib12)中的推论3.6）表明，最小化TVD等同于最大化接受率，这是提升SD性能的真正目标。在本文中，我们进一步基于TVD，结合了强化学习中的技术。我们首先展示了优化TVD损失时所采取的梯度步骤等同于强化学习中用于奖励最大化的策略梯度步骤。基于此，我们可以引入强化学习中的工具，如方差减少技术，以改善LLMs中的知识蒸馏损失（Korbak等，[2022](#bib.bib11)）。
- en: Lemma 1.
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1.
- en: The gradient of total variation distance $\mathrm{TVD}(p_{\theta},q)$ and the
    target model $q$ is equal to $\nabla_{\theta}\mathrm{TVD}(p_{\theta},q)=\mathbb{E}_{X\sim
    p_{\theta}}\left[\nabla_{\theta}\log p_{\theta}(X)(-r(X))\right]$, where $\mathbb{I}\{q(x)>
    if 0\}$当且仅当$q(x)>0$，否则为0。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差距离 $\mathrm{TVD}(p_{\theta},q)$ 相对于目标模型 $q$ 的梯度等于 $\nabla_{\theta}\mathrm{TVD}(p_{\theta},q)=\mathbb{E}_{X\sim
    p_{\theta}}\left[\nabla_{\theta}\log p_{\theta}(X)(-r(X))\right]$，其中 $\mathbb{I}\{q(x)>0\}$
    当且仅当 $q(x)>0$ 时为1，否则为0。
- en: Proof.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: See the proof in Appendix [A.1](#A1.SS1 "A.1 Proof of Lemma 1 ‣ Appendix A Appendix
    ‣ Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-tuned
    LLMs"). ∎
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见附录[A.1](#A1.SS1 "A.1 引理 1 的证明 ‣ 附录 A 附录 ‣ 草稿模型与针对性解码的Chat微调LLMs的直接对齐")中的证明。∎
- en: 'Using Lemma [1](#Thmtheorem1 "Lemma 1\. ‣ 2.3 Draft model finetuning via knowledge
    distillation ‣ 2 Method ‣ Direct Alignment of Draft Model for Speculative Decoding
    with Chat-Fine-tuned LLMs"), we can use variance reduction techniques from reinforcement
    learning (Schulman et al., [2015](#bib.bib19)) to formulate a new loss. Specifically,
    for our purpose, we use the advantage normalization which normalizes the reward
    and propose Total Variation Distance++ (TVD++). The normalization leads to negative
    rewards as opposed to $0$ tokens is given as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用引理 [1](#Thmtheorem1 "引理 1\. ‣ 2.3 草稿模型通过知识蒸馏微调 ‣ 2 方法 ‣ 草稿模型与针对性解码的Chat微调LLMs的直接对齐")，我们可以利用强化学习中的方差减少技术（Schulman等，[2015](#bib.bib19)）来制定新的损失函数。具体来说，为了我们的目的，我们使用奖励归一化，并提出总变差距离++（TVD++）。归一化会导致负奖励，相对于$0$令牌给出如下：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\mu=\frac{1}{n}\sum_{i=1}^{n}r(x_{i})$ are the mean and variance for
    the $n$ in Lemma [1](#Thmtheorem1 "Lemma 1\. ‣ 2.3 Draft model finetuning via
    knowledge distillation ‣ 2 Method ‣ Direct Alignment of Draft Model for Speculative
    Decoding with Chat-Fine-tuned LLMs"). Note that we use the entire distribution
    of target, and the mean, variance are computed over the input sequences and the
    entire vocabulary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu=\frac{1}{n}\sum_{i=1}^{n}r(x_{i})$是引理[1](#Thmtheorem1 "引理 1\. ‣ 2.3 草稿模型通过知识蒸馏微调
    ‣ 2 方法 ‣ 草稿模型与针对性解码的Chat微调LLMs的直接对齐")中$n$的均值和方差。注意我们使用目标的整个分布，均值和方差是对输入序列和整个词汇表计算的。
- en: 3 Experiments
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: Draft Model Configuration. We design a draft model by using smaller number of
    layers and hidden intermediate dimensions than those of Llama 2 7B Chat, our target
    model, based on the same network architecture (See Appendix [A.2](#A1.SS2 "A.2
    Model configurations ‣ Appendix A Appendix ‣ Direct Alignment of Draft Model for
    Speculative Decoding with Chat-Fine-tuned LLMs") for detailed configurations.).
    The size of our draft model is 115M, which is only 1.64% than the size of the
    target model 7B to achieve negligible inference overheads even on edge device,
    e.g., mobile phones or laptops.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 草稿模型配置。我们设计了一个草稿模型，使用比 Llama 2 7B Chat（我们的目标模型）更少的层数和隐藏中间维度，基于相同的网络架构（有关详细配置，请参见附录
    [A.2](#A1.SS2 "A.2 Model configurations ‣ Appendix A Appendix ‣ Direct Alignment
    of Draft Model for Speculative Decoding with Chat-Fine-tuned LLMs")）。我们的草稿模型大小为115M，仅为目标模型7B的1.64%，即使在边缘设备如手机或笔记本电脑上也能实现可忽略的推理开销。
- en: Training Datasets. Since Llama 2 pretraining dataset is not available, our draft
    model is pretrained on a 600B-token English language dataset that is curated from
    publicly available datasets. using the next token prediction objective.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集。由于 Llama 2 预训练数据集不可用，我们的草稿模型在一个600B-token的英文数据集上进行预训练，该数据集由公开可用的数据集整理而成，使用的是下一个token预测目标。
- en: For generating the distillation dataset, we take seed instructions from OIG-small-chip2([Nguyen
    et al.,](#bib.bib16) ) and OpenAssistant (Köpf et al., [2023](#bib.bib10)) to
    collect the target model responses sampled with various temperatures $\{0,0.3,0.7,1.0\}$
    for sample diversity (temperature=$0$ corresponds to greedy decoding).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成蒸馏数据集，我们从 OIG-small-chip2（[Nguyen et al.,](#bib.bib16)）和 OpenAssistant（Köpf
    et al., [2023](#bib.bib10)）中获取种子指令，以收集在不同温度 $\{0,0.3,0.7,1.0\}$ 下采样的目标模型响应，以增加样本多样性（温度=$0$
    对应贪婪解码）。
- en: 'Draft fine-tuning. To compare the effectiveness of our TVD++, we use different
    loss functions to fine-tune the draft output distribution to align with target
    output distribution: (i) KLD (ii) TVD and (iii) TVD++. In this step, we mix the
    distillation dataset with the pretraining dataset at 9:1 ratio in each training
    batch for regularization effect.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 草稿微调。为了比较我们的 TVD++ 的有效性，我们使用不同的损失函数来微调草稿输出分布，以与目标输出分布对齐：（i）KLD （ii）TVD 和 （iii）TVD++。在此步骤中，我们将蒸馏数据集与预训练数据集以
    9:1 的比例混合，在每个训练批次中进行正则化效果。
- en: 'Evaluation metrics and tasks. We evaluate our draft model by measuring the
    (1) block efficiency ($\tau$ and input $x$ can be $\gamma+1$ and a relative latency
    $c$. We measure these metrics on a variety of tasks using different block lengths
    $\gamma$. The evaluation tasks include: (a) open-ended text generation (Conover
    et al., [2023](#bib.bib7), databricks-Dolly-15k), (b) extreme summarization (Narayan
    et al., [2018](#bib.bib15), XSum), and (c) news summarization (Nallapati et al.,
    [2016](#bib.bib14), CNN-dailymail). For open-ended text generation we random-sample
    with temperature$=0.6$$=0.9$ for both draft model and target model, while we greedy-sample
    for the rest.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标和任务。我们通过测量（1）块效率 ($\tau$ 和输入 $x$ 可以是 $\gamma+1$ 和相对延迟 $c$ 来评估我们的草稿模型。我们在不同的任务上使用不同的块长度
    $\gamma$ 来测量这些指标。评估任务包括：（a）开放式文本生成（Conover et al., [2023](#bib.bib7)，databricks-Dolly-15k），（b）极端摘要（Narayan
    et al., [2018](#bib.bib15)，XSum），和（c）新闻摘要（Nallapati et al., [2016](#bib.bib14)，CNN-dailymail）。对于开放式文本生成，我们对草稿模型和目标模型都进行温度为
    $0.6$ 和 $0.9$ 的随机采样，而其余则进行贪婪采样。
- en: Results. Figure [1](#S3.F1 "Figure 1 ‣ 3 Experiments ‣ Direct Alignment of Draft
    Model for Speculative Decoding with Chat-Fine-tuned LLMs") shows MBSU for draft
    models fine-tuned using different losses, with $\gamma=3$. Our proposed TVD++
    loss either outperforms the other two losses or performs on par with the best
    on all the tasks, showing its effectiveness for improving draft model fine-tuning.
    Furthermore, Figure [2](#S3.F2 "Figure 2 ‣ 3 Experiments ‣ Direct Alignment of
    Draft Model for Speculative Decoding with Chat-Fine-tuned LLMs") shows the improvement
    in block-efficiency with $\gamma=3$ with more fine-tuning over the base draft
    model, while block efficiency improvement is also observed for news summarization
    task (CNN-dailymail) from $2.29$ for fine-tuned draft showing the efficacy of
    fine-tuning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。图[1](#S3.F1 "图 1 ‣ 3 个实验 ‣ 基于 Chat 微调 LLM 的草稿模型直接对齐")展示了使用不同损失函数对草稿模型进行微调时的
    MBSU，$\gamma=3$。我们提出的 TVD++ 损失在所有任务中要么优于其他两种损失，要么与最佳性能相当，显示出其在改善草稿模型微调方面的有效性。此外，图[2](#S3.F2
    "图 2 ‣ 3 个实验 ‣ 基于 Chat 微调 LLM 的草稿模型直接对齐")显示了对基准草稿模型进行更多微调时，$\gamma=3$下区块效率的提升，同时在新闻摘要任务（CNN-dailymail）中也观察到了区块效率的提升，从
    $2.29$ 提升到微调后的草稿模型，显示了微调的有效性。
- en: '![Refer to caption](img/b9cf498212239be09ea5aaf7b1e24313.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9cf498212239be09ea5aaf7b1e24313.png)'
- en: 'Figure 1: Draft models evaluated on Memory-Bound Speed-Up (MBSU) for multiple
    tasks (Dolly, CNN-DM, XSum) with draft lengths (3, 5) and training losses (KLD,
    TVD, TVD++);'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：针对多个任务（Dolly、CNN-DM、XSum）及草稿长度（3、5）和训练损失（KLD、TVD、TVD++）的草稿模型进行的记忆绑定加速（MBSU）评估；
- en: '![Refer to caption](img/46d9541e2330bea5de7de338395412ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46d9541e2330bea5de7de338395412ec.png)'
- en: 'Figure 2: Draft models evaluated on block efficiency ($\gamma=3$) over different
    checkpoints (ckpt) across fine-tuning stage, showing improvement over the base
    draft model, for multiple tasks (Dolly, CNN-DM, XSum) and training losses (KLD,
    TVD, TVD++)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同检查点（ckpt）下对草稿模型进行的区块效率评估（$\gamma=3$），显示了在基准草稿模型上的改进，涵盖多个任务（Dolly、CNN-DM、XSum）和训练损失（KLD、TVD、TVD++）
- en: 4 Conclusion
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In this work, we proposed a framework to train a draft model with direct alignment
    to a finetuned target LLM for speculative decoding. Our training pipeline consists
    of pre-training, generating a distillation dataset from the target LLM and fine-tuning
    the draft model on the distillation dataset with target LLM in the training loop.
    In addition, we further proposed a new loss building on TVD inspired from policy
    gradient in RL which outperforms both KLD and TVD in fine-tuning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个框架，用于将草稿模型与经过微调的目标 LLM 进行直接对齐，以实现推测性解码。我们的训练流程包括预训练、从目标 LLM 生成蒸馏数据集，并在训练循环中对草稿模型进行蒸馏数据集的微调。此外，我们还提出了一种基于
    TVD 的新损失，灵感来自 RL 中的策略梯度，优于 KLD 和 TVD。
- en: References
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models. *arXiv preprint arXiv:2306.13649*, 2023.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, 和 Olivier Bachem。Gkd: 自回归序列模型的广义知识蒸馏。*arXiv 预印本 arXiv:2306.13649*，2023。'
- en: Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, et al. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*, 2023.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, 等人。Palm 2 技术报告。*arXiv 预印本 arXiv:2305.10403*，2023。
- en: 'Baichuan (2023) Baichuan. Baichuan 2: Open large-scale language models. *arXiv
    preprint arXiv:2309.10305*, 2023. URL [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baichuan (2023) Baichuan. Baichuan 2: 开放的大规模语言模型。*arXiv 预印本 arXiv:2309.10305*，2023。URL
    [https://arxiv.org/abs/2309.10305](https://arxiv.org/abs/2309.10305)。'
- en: 'Bojar et al. (2018) Ond rej Bojar, Christian Federmann, Mark Fishel, Yvette
    Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. Findings
    of the 2018 conference on machine translation (wmt18). In *Proceedings of the
    Third Conference on Machine Translation, Volume 2: Shared Task Papers*, pp.  272–307,
    Belgium, Brussels, October 2018\. Association for Computational Linguistics. URL
    [http://www.aclweb.org/anthology/W18-6401](http://www.aclweb.org/anthology/W18-6401).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojar 等人（2018）奥恩德·博贾、克里斯蒂安·费德曼、马克·费谢尔、伊薇特·格雷厄姆、巴里·哈多、马蒂亚斯·赫克、菲利普·科恩 和 克里斯托夫·蒙茨。2018
    年机器翻译会议（WMT18）的发现。在 *第三届机器翻译会议论文集，第 2 卷：共享任务论文* 中，第 272–307 页，比利时，布鲁塞尔，2018 年
    10 月。计算语言学协会。网址 [http://www.aclweb.org/anthology/W18-6401](http://www.aclweb.org/anthology/W18-6401)。
- en: Chen et al. (2023) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding
    with speculative sampling. *arXiv preprint arXiv:2302.01318*, 2023.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023）查理·陈、塞巴斯蒂安·博尔戈、杰弗瑞·欧文、让-巴普蒂斯特·莱斯皮厄、洛朗·西弗 和 约翰·ジャン普。通过推测抽样加速大型语言模型的解码。*arXiv
    预印本 arXiv:2302.01318*，2023。
- en: 'Conover et al. (2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
    Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
    Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023.
    URL [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conover 等人（2023）迈克·科诺弗、马特·海斯、安基特·马图尔、简伟·谢、俊·万、萨姆·沙赫、阿里·戈赫西、帕特里克·温德尔、马特伊·扎哈里亚
    和 雷诺德·辛。自由多莉：介绍世界上第一个真正开放的指令调优 LLM，2023。网址 [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人（2015）杰弗里·辛顿、奥里奥尔·维尼亚尔斯 和 杰夫·迪恩。蒸馏神经网络中的知识。*arXiv 预印本 arXiv:1503.02531*，2015。
- en: Kim & Rush (2016) Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation.
    *arXiv preprint arXiv:1606.07947*, 2016.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim & Rush（2016）尹·金 和 亚历山大·M·拉什。序列级知识蒸馏。*arXiv 预印本 arXiv:1606.07947*，2016。
- en: Köpf et al. (2023) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver
    Stanley, Richárd Nagyfi, et al. Openassistant conversations–democratizing large
    language model alignment. *arXiv preprint arXiv:2304.07327*, 2023.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等人（2023）安德烈亚斯·科普夫、扬尼克·基尔彻、迪米特里·冯·鲁特、索提里斯·阿纳戈斯提迪斯、张志锐、基思·史蒂文斯、阿卜杜拉·巴尔胡姆、阮明德、奥利弗·斯坦利、里查德·纳吉菲
    等。Openassistant 对话——使大型语言模型对齐民主化。*arXiv 预印本 arXiv:2304.07327*，2023。
- en: Korbak et al. (2022) Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc
    Dymetman. On reinforcement learning and distribution matching for fine-tuning
    language models with no catastrophic forgetting. *Advances in Neural Information
    Processing Systems*, 35:16203–16220, 2022.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korbak 等人（2022）托马斯·科尔巴克、哈迪·艾尔沙赫、赫尔曼·克鲁兹维斯基 和 马克·迪梅特曼。关于强化学习和分布匹配以防止语言模型在微调时的灾难性遗忘。*神经信息处理系统进展*，35:16203–16220，2022。
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pp.  19274–19286\. PMLR, 2023.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等人（2023）亚尼夫·利维坦、马坦·卡尔曼 和 约西·马蒂亚斯。通过推测解码实现变压器的快速推理。在 *国际机器学习大会* 上，第
    19274–19286 页。PMLR，2023。
- en: Lin et al. (2020) Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei.
    Autoregressive knowledge distillation through imitation learning. *arXiv preprint
    arXiv:2009.07253*, 2020.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2020）亚历山大·林、杰里米·沃尔文德、霍华德·陈 和 陶·雷。通过模仿学习进行自回归知识蒸馏。*arXiv 预印本 arXiv:2009.07253*，2020。
- en: Nallapati et al. (2016) Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
    Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and
    beyond. *arXiv preprint arXiv:1602.06023*, 2016.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nallapati 等人（2016）拉梅什·纳拉帕提、鲍文·周、卡格拉尔·古尔切赫、冰·香 等。使用序列到序列 RNN 和其他方法的抽象文本总结。*arXiv
    预印本 arXiv:1602.06023*，2016。
- en: Narayan et al. (2018) Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t
    give me the details, just the summary! topic-aware convolutional neural networks
    for extreme summarization. *arXiv preprint arXiv:1808.08745*, 2018.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan 等人（2018）莎希·纳拉扬、肖·B·科恩 和 米雷拉·拉帕塔。别给我细节，只要总结！基于主题的卷积神经网络用于极端总结。*arXiv
    预印本 arXiv:1808.08745*，2018。
- en: (16) Huu Nguyen, Sameer Suri, Tsui Ken, Shahules786, Together.xyz team, and
    Schuhmann Christoph. OIG-small-chip2. [https://huggingface.co/datasets/0-hero/OIG-small-chip2](https://huggingface.co/datasets/0-hero/OIG-small-chip2).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Huu Nguyen, Sameer Suri, Tsui Ken, Shahules786, Together.xyz 团队，以及 Schuhmann
    Christoph。OIG-small-chip2。 [https://huggingface.co/datasets/0-hero/OIG-small-chip2](https://huggingface.co/datasets/0-hero/OIG-small-chip2)。
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated
    corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*, 2023.
    URL [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    和 Julien Launay。Falcon LLM 的 RefinedWeb 数据集：用网络数据超越策划的语料库，仅使用网络数据。*arXiv 预印本 arXiv:2306.01116*，2023年。网址
    [https://arxiv.org/abs/2306.01116](https://arxiv.org/abs/2306.01116)。
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等人。Code llama: 开放的代码基础模型。*arXiv 预印本 arXiv:2308.12950*，2023年。'
- en: Schulman et al. (2015) John Schulman, Philipp Moritz, Sergey Levine, Michael
    Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized
    advantage estimation. *arXiv preprint arXiv:1506.02438*, 2015.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2015) John Schulman, Philipp Moritz, Sergey Levine, Michael
    Jordan, 和 Pieter Abbeel。使用广义优势估计的高维连续控制。*arXiv 预印本 arXiv:1506.02438*，2015年。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人。Llama 2: 开放基础和微调的对话模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: Wen et al. (2023) Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence
    minimization for sequence-level knowledge distillation. *arXiv preprint arXiv:2307.15190*,
    2023.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. (2023) Yuqiao Wen, Zichao Li, Wenyu Du, 和 Lili Mou。序列级知识蒸馏的 f-散度最小化。*arXiv
    预印本 arXiv:2307.15190*，2023年。
- en: Williams (1992) Ronald J Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Machine learning*, 8:229–256, 1992.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams (1992) Ronald J Williams。用于连接主义强化学习的简单统计梯度跟踪算法。*机器学习*，8:229–256，1992年。
- en: 'Zhou et al. (2023) Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna
    Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal.
    Distillspec: Improving speculative decoding via knowledge distillation. *arXiv
    preprint arXiv:2310.08461*, 2023.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna
    Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, 和 Rishabh Agarwal。Distillspec:
    通过知识蒸馏改进推测解码。*arXiv 预印本 arXiv:2310.08461*，2023年。'
- en: Appendix A Appendix
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Proof of Lemma [1](#Thmtheorem1 "Lemma 1\. ‣ 2.3 Draft model finetuning
    via knowledge distillation ‣ 2 Method ‣ Direct Alignment of Draft Model for Speculative
    Decoding with Chat-Fine-tuned LLMs")
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 引理 [1](#Thmtheorem1 "引理 1\. ‣ 2.3 草稿模型通过知识蒸馏的微调 ‣ 2 方法 ‣ 针对推测解码的草稿模型与对话微调LLMs的直接对齐")
    的证明
- en: Lemma [1](#Thmtheorem1 "Lemma 1\. ‣ 2.3 Draft model finetuning via knowledge
    distillation ‣ 2 Method ‣ Direct Alignment of Draft Model for Speculative Decoding
    with Chat-Fine-tuned LLMs").
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 [1](#Thmtheorem1 "引理 1\. ‣ 2.3 草稿模型通过知识蒸馏的微调 ‣ 2 方法 ‣ 针对推测解码的草稿模型与对话微调LLMs的直接对齐")。
- en: The gradient of total variation distance $\mathrm{TVD}(p_{\theta},q)$ and the
    target model $q$ is equal to
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差距离 $\mathrm{TVD}(p_{\theta},q)$ 和目标模型 $q$ 的梯度等于
- en: '|  | $\displaystyle\nabla_{\theta}\mathrm{TVD}(p_{\theta},q)=\mathbb{E}_{X\sim
    p_{\theta}}\left[\nabla_{\theta}\log p_{\theta}(X)(-r(X))\right]$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}\mathrm{TVD}(p_{\theta},q)=\mathbb{E}_{X\sim
    p_{\theta}}\left[\nabla_{\theta}\log p_{\theta}(X)(-r(X))\right]$ |  |'
- en: for $r(x):=\mathbb{I}\{q(x)> is an indicator function, which is equal to  是一个指示函数，其值等于  |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $$\displaystyle=-\sum_{x}\mathbb{I}\{q(x)> |  |'
- en: '|  |  | <math id=$$ |  | (3) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | <math id=$$ |  | (3) |'
- en: '|  |  | $1$2 |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (4) |'
- en: where equation [3](#A1.E3 "In Proof. ‣ A.1 Proof of Lemma 1 ‣ Appendix A Appendix
    ‣ Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-tuned
    LLMs") follows from using the log-derivative trick. Note that is equivalent to
    the policy gradient where a policy and a reward are $p_{\theta}(x)$, respectively (Williams,
    [1992](#bib.bib22)). ∎
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中方程 [3](#A1.E3 "证明 ‣ A.1 引理 1 证明 ‣ 附录 A 附录 ‣ 草稿模型与 Chat-Fine-tuned LLMs 的直接对齐")
    是通过使用对数导数技巧得出的。注意这等价于策略梯度，其中策略和奖励分别是 $p_{\theta}(x)$ (Williams, [1992](#bib.bib22))。∎
- en: A.2 Model configurations
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 模型配置
- en: 'The following configurations are used for our target and draft models:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下配置用于我们的目标和草稿模型：
- en: 'Table 1: Draft and target model configurations'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：草稿模型和目标模型的配置
- en: '|  | Llama 2-Chat (7B, target) | Llama 2-Chat-Drafter (115M, draft) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama 2-Chat (7B, 目标) | Llama 2-Chat-Drafter (115M, 草稿) |'
- en: '| Layers | 32 | 4 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 层数 | 32 | 4 |'
- en: '| Attention heads | 32 | 8 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 注意力头数 | 32 | 8 |'
- en: '| Intermediate dim | 11,008 | 2,816 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 中间维度 | 11,008 | 2,816 |'
- en: '| Hidden dim | 2,048 | 1,024 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏维度 | 2,048 | 1,024 |'
- en: '| Activation | SiLU | SiLU |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | SiLU | SiLU |'
- en: A.3 Training hyper-parameters
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 训练超参数
- en: For draft model pretraining, we used deepspeed stage1 with a batch-size=$496$
    while minimum was $1e-6$ warm-up steps
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于草稿模型的预训练，我们使用了 deepspeed stage1，批量大小为 $496$，最小值为 $1e-6$ 的预热步骤
- en: For draft model fine-tuning, we used deepspeed stage $3$ on $8$ with same optimizer
    and scheduler with $2000$ warm-up steps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于草稿模型的微调，我们在 $8$ 上使用了 deepspeed stage $3$，使用相同的优化器和调度器，并设置了 $2000$ 的预热步骤。
- en: A.4 Data processing
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 数据处理
- en: We preprocess the text data with the tokenizer of the target language model
    while appending End-Of-Sentence (EOS) token at the end of each input sequence.
    Furthermore, as a postporcessing step, all the sequences are concatenated into
    chunks of 2048 length, to maximize training throughput without adding pad tokens.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用目标语言模型的分词器对文本数据进行预处理，同时在每个输入序列的末尾附加句子结束（EOS）标记。此外，作为后处理步骤，所有序列被连接成长度为 2048
    的块，以最大化训练吞吐量而不添加填充标记。
- en: A.5 Performance Degradation at Out-Of-Distribution (OOD) Task
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 分布外 (OOD) 任务的性能下降
- en: '![Refer to caption](img/afdc1bd53afb08113d56bb55c1de1fc2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/afdc1bd53afb08113d56bb55c1de1fc2.png)'
- en: 'Figure 3: Block efficiency for WMT18-DeEn results with multiple draft models
    are described.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：描述了具有多个草稿模型的 WMT18-DeEn 结果的块效率。
- en: We also evaluated the block efficiency of multiple draft models on translation
    task, WMT18-DeEn (Bojar et al., [2018](#bib.bib5)), from German to English. We
    found that all fine-tuned draft models were outperformed by the base model for
    this particular task since those models were *not* directly fine-tuned on the
    translation task, making the task OOD. We expect this issue will be resolved by
    adding in-distribution samples for training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了多个草稿模型在翻译任务 WMT18-DeEn (Bojar 等人, [2018](#bib.bib5)) 上的块效率，该任务从德语翻译到英语。我们发现所有经过微调的草稿模型在这一特定任务上都不如基础模型，因为这些模型*没有*直接在翻译任务上进行微调，导致任务变为
    OOD。我们期望通过增加训练中的分布内样本来解决这一问题。
