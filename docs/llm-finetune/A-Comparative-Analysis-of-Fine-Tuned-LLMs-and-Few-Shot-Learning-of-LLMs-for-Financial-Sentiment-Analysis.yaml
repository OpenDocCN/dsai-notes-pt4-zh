- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《细调的语言模型与少量示例学习在金融情感分析中的比较分析》
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08725](https://ar5iv.labs.arxiv.org/html/2312.08725)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08725](https://ar5iv.labs.arxiv.org/html/2312.08725)
- en: Sorouralsadat Fatemi [sfatem6@uic.edu](mailto:sfatem6@uic.edu) University of
    Illinois at ChicagoUSA  and  Yuheng Hu [yuhenghu@uic.edu](mailto:yuhenghu@uic.edu)
    University of Illinois at ChicagoUSA(2018)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sorouralsadat Fatemi [sfatem6@uic.edu](mailto:sfatem6@uic.edu) 伊利诺伊大学芝加哥分校USA
    和 Yuheng Hu [yuhenghu@uic.edu](mailto:yuhenghu@uic.edu) 伊利诺伊大学芝加哥分校USA（2018）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Financial sentiment analysis plays a crucial role in uncovering latent patterns
    and detecting emerging trends, enabling individuals to make well-informed decisions
    that may yield substantial advantages within the constantly changing realm of
    finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness
    in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot
    in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless,
    their potential and applicability in the context of financial sentiment analysis
    have not been thoroughly explored yet. To bridge this gap, we employ two approaches:
    in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs
    on a finance-domain dataset. Given the computational costs associated with fine-tuning
    LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from
    250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art
    results to evaluate their effectiveness in the finance-domain.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 金融情感分析在揭示潜在模式和检测新兴趋势方面发挥着至关重要的作用，使个人能够做出明智的决策，从而在不断变化的金融领域中获得显著的优势。最近，大型语言模型（LLMs）在各种领域展示了其有效性，甚至在零-shot和少量示例的上下文学习中表现出显著的能力。然而，它们在金融情感分析中的潜力和适用性尚未得到彻底探索。为填补这一空白，我们采用了两种方法：一种是上下文学习（重点关注gpt-3.5-turbo模型），另一种是对金融领域数据集进行细调的LLMs。鉴于细调具有大参数规模的LLMs所需的计算成本，我们专注于较小的LLMs，参数范围从250M到3B进行细调。然后，我们将其性能与最先进的结果进行比较，以评估它们在金融领域的有效性。
- en: Our results demonstrate that fine-tuned smaller LLMs can achieve comparable
    performance to state-of-the-art fine-tuned LLMs, even with models having fewer
    parameters and a smaller training dataset. Additionally, the zero-shot and one-shot
    performance of LLMs produces comparable results with fine-tuned smaller LLMs and
    state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is
    no observed enhancement in performance for finance-domain sentiment analysis when
    the number of shots for in-context learning is increased.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，即使是参数较少的细调小型LLMs，也能达到与最先进的细调LLMs相媲美的性能，尽管这些模型的参数较少，训练数据集较小。此外，LLMs的零-shot和one-shot性能与细调小型LLMs和最先进结果的表现相当。此外，我们的分析表明，当上下文学习的示例数量增加时，金融领域情感分析的性能并没有观察到提升。
- en: 'Sentiment Analysis, Finance, Large Language Model, Natural Language Processing^†^†copyright:
    acmcopyright^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make
    sure to enter the correct conference title from your rights confirmation emai;
    November 27–29, 2018; Woodstock, NY^†^†price: 15.00^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Natural language processing'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析，金融，大型语言模型，自然语言处理^†^†版权：acmcopyright^†^†期刊年：2018^†^†doi：XXXXXXX.XXXXXXX^†^†会议：确保输入正确的会议标题，来自您的权利确认邮件；2018年11月27日至29日；纽约伍德斯托克^†^†价格：15.00^†^†isbn：978-1-4503-XXXX-X/18/06^†^†ccs：计算方法，自然语言处理
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The performance of various Large Language Models (LLMs) such as GPT3 (Brown
    et al., [2020a](#bib.bib2)), LLaMA (Touvron et al., [2023](#bib.bib20)), and T5
    (Raffel et al., [2020](#bib.bib18)) has been remarkable across a diverse set of
    Natural Language Processing (NLP) tasks. Sentiment analysis is an NLP task that
    utilizes advancements in language modeling to achieve enhanced outcomes. LLMs
    are trained on a wide range of corpora, enabling them to understand language patterns
    and perform tasks using zero-shot or few-shot prompting without explicit supervised
    training (Yang et al., [2023a](#bib.bib26); Brown et al., [2020b](#bib.bib3)).
    Therefore, there exists considerable potential for the application of LLMs in
    sentiment analysis tasks, particularly in the financial domain where a limited
    availability of annotated data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 各种大型语言模型（LLMs），如 GPT-3（Brown et al., [2020a](#bib.bib2)）、LLaMA（Touvron et al.,
    [2023](#bib.bib20)）和 T5（Raffel et al., [2020](#bib.bib18)）在广泛的自然语言处理（NLP）任务中表现出色。情感分析是一项利用语言建模进展来实现增强结果的
    NLP 任务。LLMs 在各种语料库上进行训练，使它们能够理解语言模式，并通过零样本或少样本提示执行任务，而无需明确的监督训练（Yang et al., [2023a](#bib.bib26);
    Brown et al., [2020b](#bib.bib3)）。因此，LLMs 在情感分析任务中的应用潜力巨大，特别是在金融领域，这里注释数据的可用性有限。
- en: Recent studies have highlighted the promising performance of LLMs in various
    NLP tasks, particularly in the zero-shot and few-shot settings. Notably, GPT-3
    has shown competitive performance and, in some cases, even outperformed state-of-the-art
    models in few-shot in-context learning (Brown et al., [2020b](#bib.bib3); Zhong
    et al., [2023](#bib.bib29)). However, when it comes to sentiment analysis tasks,
    some studies have explored the zero-shot and in-context learning capabilities
    of LLMs and found that, in certain instances, in-context learning may not achieve
    comparable performance compared to fine-tuned and state-of-the-art models (Liu
    et al., [2022](#bib.bib11); Wang et al., [2023](#bib.bib21)). Therefore, it becomes
    crucial to investigate the zero-shot and few-shot in-context learning capabilities
    of LLMs in the financial domain, as it encompasses specific jargon and unique
    linguistic characteristics that necessitate special consideration. The aforementioned
    aspect has yet to be thoroughly examined in prior research, rendering it an important
    domain for further investigation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究强调了大型语言模型（LLMs）在各种自然语言处理（NLP）任务中的良好表现，特别是在零样本和少样本设置下。特别是，GPT-3 展现了竞争力的性能，在某些情况下，甚至在少样本上下文学习中超越了最先进的模型（Brown
    et al., [2020b](#bib.bib3); Zhong et al., [2023](#bib.bib29)）。然而，当涉及到情感分析任务时，一些研究探讨了
    LLMs 的零样本和上下文学习能力，并发现，在某些情况下，上下文学习可能无法达到与微调和最先进模型相当的性能（Liu et al., [2022](#bib.bib11);
    Wang et al., [2023](#bib.bib21)）。因此，调查 LLMs 在金融领域的零样本和少样本上下文学习能力变得尤为重要，因为该领域包含了特定的术语和独特的语言特征，需要特别考虑。上述方面在以往的研究中尚未得到彻底探讨，使其成为进一步研究的重要领域。
- en: Additionally, a number of studies indicate that in-context learning can yield
    inferior performance compared to fine-tuning (Brown et al., [2020b](#bib.bib3);
    Liu et al., [2022](#bib.bib11)). To delve further into this, we extend our experiments
    by fine-tuning LLMs on a finance-domain dataset and evaluate their performance
    on various financial datasets. For this purpose, we focus on a range of smaller
    language models known as FLAN-T5 models, with parameter sizes varying from 770
    million to 3 billion parameters. Our goal is to determine whether their performance
    is comparable to state-of-the-art fine-tuned LLMs with larger parameter sizes.
    To comprehensively assess the performance of LLMs in financial sentiment analysis,
    we conduct comparisons between the fine-tuned models and their counterparts in
    zero-shot and few-shot settings. In summary, our study involves conducting an
    empirical examination of the zero-shot and few-shot learning abilities of LLMs.
    In addition, we extend our investigation by performing fine-tuning on three Flan-T5
    models (base, large, and xl) using finance-specific data. Additionally, we compare
    the results with state-of-the-art models to assess their performance comprehensively.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多研究表明，相较于微调，**上下文学习**的效果可能较差（Brown et al., [2020b](#bib.bib3); Liu et al.,
    [2022](#bib.bib11)）。为深入探讨这一问题，我们通过在金融领域数据集上微调LLMs，并评估它们在各种金融数据集上的表现。为此，我们专注于一系列较小的语言模型，即FLAN-T5模型，其参数规模从7.7亿到30亿不等。我们的目标是确定这些模型的表现是否与具有更大参数规模的最先进微调LLMs相当。为了全面评估LLMs在金融情感分析中的表现，我们在零样本和少样本设置下对微调模型及其对应模型进行比较。总之，我们的研究涉及对LLMs的零样本和少样本学习能力的实证检验。此外，我们还通过使用金融特定数据对三种Flan-T5模型（base、large
    和 xl）进行微调，并将结果与最先进的模型进行比较，以全面评估它们的表现。
- en: 2\. related work
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: Sentiment analysis in the financial domain has gained importance in NLP due
    to its impact on the stock prediction task. It plays a crucial role in providing
    valuable insights for investors’ decision-making (Mishev et al., [2020](#bib.bib16)).
    previous literature explores diverse methodologies for conducting financial sentiment
    analysis, including lexicon-based techniques (Chen et al., [2018](#bib.bib4);
    Loughran and McDonald, [2011](#bib.bib14)). However, such approaches often struggle
    to capture the semantic nuances arising from specific word sequences.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，金融领域的情感分析因其对股票预测任务的影响而变得越来越重要。它在为投资者决策提供有价值的洞察方面发挥着关键作用（Mishev et al.,
    [2020](#bib.bib16)）。先前的文献探讨了进行金融情感分析的多种方法，包括基于词典的技术（Chen et al., [2018](#bib.bib4);
    Loughran 和 McDonald, [2011](#bib.bib14)）。然而，这些方法通常难以捕捉特定词序列中产生的语义细微差别。
- en: Alternatively, the advent of pre-trained language models, particularly models
    like BERT have revolutionized the field of NLP, including sentiment analysis (Brown
    et al., [2020b](#bib.bib3)). They leverage large-scale datasets and self-supervised
    learning to develop a robust understanding of context. This contextual awareness
    allows them to capture the subtle nuances and idiomatic expressions in sentiment-rich
    text (Devlin et al., [2018](#bib.bib8)). Although these models are trained on
    general domain corpora like news articles and Wikipedia, they lack specific knowledge
    in the finance domain. To address this limitation, researchers have undertaken
    further fine-tuning of BERT specifically using financial text, leading to promising
    outcomes in financial sentiment classification. Nevertheless, it is crucial to
    acknowledge that the fine-tuning procedure for language models such as BERT requires
    a substantial quantity of financial data and computational resources (Brown et al.,
    [2020a](#bib.bib2); Radford et al., [2019](#bib.bib17)). In a specific study,
    BERT performed further pre-training by utilizing a financial communication corpus
    consisting of 4.9 billion finance-domain tokens (Radford et al., [2019](#bib.bib17)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的进展是预训练语言模型的出现，特别是像BERT这样的模型，已经彻底改变了NLP领域，包括情感分析（Brown 等，[2020b](#bib.bib3)）。这些模型利用大规模数据集和自监督学习来开发对上下文的深入理解。这种上下文意识使它们能够捕捉情感丰富文本中的微妙差异和习惯用语（Devlin
    等，[2018](#bib.bib8)）。尽管这些模型在新闻文章和维基百科等通用领域语料库上进行训练，但在金融领域缺乏特定知识。为了解决这一局限性，研究人员已对BERT进行了进一步的微调，专门使用金融文本，取得了在金融情感分类方面的良好成果。然而，必须承认，像BERT这样的语言模型的微调过程需要大量的金融数据和计算资源（Brown
    等，[2020a](#bib.bib2)；Radford 等，[2019](#bib.bib17)）。在一项具体研究中，BERT通过利用包含49亿金融领域标记的金融通信语料库进行了进一步的预训练（Radford
    等，[2019](#bib.bib17)）。
- en: This problem has been mitigated by the most recent advancements in NLP through
    the utilization of LLMs. These models are trained on vast amounts of textual data,
    spanning various domains and comprising hundreds of millions to billions of words,
    surpassing the scale of previous language models. In addition to leveraging larger
    amounts of pre-training data, large language models (LLMs) integrate various training
    techniques, including instruction tuning and reinforcement learning from human
    feedback (RLHF) (Wei et al., [2022a](#bib.bib22); Christiano et al., [2017](#bib.bib5)).
    This combination enables LLMs to achieve impressive performance in zero-shot or
    few-shot learning scenarios, occasionally surpassing the state-of-the-art benchmarks
    (Brown et al., [2020b](#bib.bib3)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这一问题已经通过最近在自然语言处理（NLP）领域的进展得到了缓解，这得益于大规模语言模型（LLMs）的使用。这些模型在大量的文本数据上进行训练，覆盖各种领域，包含数亿到数十亿的单词，超越了之前语言模型的规模。除了利用更多的预训练数据，大型语言模型（LLMs）还结合了各种训练技术，包括指令调优和来自人类反馈的强化学习（RLHF）（Wei
    等，[2022a](#bib.bib22)；Christiano 等，[2017](#bib.bib5)）。这种组合使得LLMs在零样本或少样本学习场景中表现出色，偶尔超越了最先进的基准（Brown
    等，[2020b](#bib.bib3)）。
- en: In light of these advancements, efforts have been made to develop financial-focused
    LLMs. One notable example is BloombergGPT, a 50 billion parameter language model
    specifically trained on a diverse range of financial data (Wu et al., [2023](#bib.bib24)).
    Although, it demonstrated impressive performance in sentiment analysis, its closed-source
    nature limits its use in the research community. To address this, a new study
    proposed an open-source alternative called FinGPT, fine-tuned on extensive financial
    data from various sources (Yang et al., [2023b](#bib.bib25)). FinGPT achieved
    comparable performance in financial sentiment analysis to BloombergGPT, with lower
    adaptation costs and computational requirements.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些进展，已经进行了开发以金融为重点的大型语言模型的努力。其中一个显著的例子是BloombergGPT，这是一个50亿参数的语言模型，专门在多样化的金融数据上进行训练（Wu
    等，[2023](#bib.bib24)）。尽管它在情感分析中表现出色，但其闭源性质限制了其在研究界的使用。为了解决这个问题，一项新的研究提出了一种开源替代方案，称为FinGPT，经过在来自各种来源的大量金融数据上进行微调（Yang
    等，[2023b](#bib.bib25)）。FinGPT在金融情感分析中达到了与BloombergGPT相当的表现，同时适应成本和计算需求较低。
- en: In a recent study, researchers introduced an instruction-tuned LLaMA-7B model,
    Instruct-FinGPT-7B, designed to overcome challenges faced by LLMs in accurately
    interpreting numerical values and comprehending financial context. The evaluation
    results showed that this model outperformed state-of-the-art methods in financial
    sentiment analysis, underscoring the promising potential of LLMs in this domain
    (Zhang et al., [2023b](#bib.bib27)). However, the focus of the present study is
    on fine-tuning smaller LLMs to achieve comparable performance while minimizing
    computational and memory resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项研究中，研究人员介绍了一种经过指令调优的LLaMA-7B模型——Instruct-FinGPT-7B，旨在克服大型语言模型（LLMs）在准确解读数值和理解金融背景方面面临的挑战。评估结果显示，该模型在金融情感分析中优于最先进的方法，突显了LLMs在这一领域的**有希望的潜力**（Zhang
    et al., [2023b](#bib.bib27)）。然而，目前研究的重点是对较小的LLMs进行微调，以在减少计算和内存资源的同时实现可比的性能。
- en: Furthermore, there have been notable efforts to explore the zero-shot and in-context
    learning capabilities of LLMs in sentiment analysis tasks (Zhang et al., [2023a](#bib.bib28);
    Wang et al., [2023](#bib.bib21)). Results indicate that LLMs, like ChatGPT, exhibit
    robust zero-shot performance, even comparable to fine-tuned models like T5-large.
    However, they may not perform as well in few-shot learning, particularly with
    an increasing number of shots, especially on certain datasets like Twitter and
    MR (Zhang et al., [2023a](#bib.bib28)). On the other hand, a separate study suggests
    that long-tail domains could benefit from few-shot prompting (Wang et al., [2023](#bib.bib21)).
    In light of the varying outcomes observed in few-shot learning when applied to
    different sentiment analysis datasets, we aim to conduct a comprehensive examination
    to evaluate the effectiveness of zero-shot and few-shot learning methods for sentiment
    analysis in the finance-domain.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，也有显著的努力探索LLMs在情感分析任务中的零样本和上下文学习能力（Zhang et al., [2023a](#bib.bib28); Wang
    et al., [2023](#bib.bib21)）。结果表明，LLMs，如ChatGPT，表现出强大的零样本性能，甚至可与像T5-large这样的微调模型媲美。然而，在少样本学习中，它们的表现可能不如预期，特别是在样本数量增加的情况下，尤其是在Twitter和MR等某些数据集上（Zhang
    et al., [2023a](#bib.bib28)）。另一方面，另一项研究表明，长尾领域可能从少样本提示中受益（Wang et al., [2023](#bib.bib21)）。鉴于在应用于不同情感分析数据集时观察到的少样本学习结果差异，我们旨在进行全面的检验，以评估零样本和少样本学习方法在金融领域情感分析中的有效性。
- en: 3\. Method and Experimental setup
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法与实验设置
- en: In this section, we outline the methodologies and datasets employed to investigate
    the objectives of our study.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了用于研究我们研究目标的方法和数据集。
- en: 3.1\. Zero-shot and Few-shot Settings
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 零样本和少样本设置
- en: 'In this investigation, we explore the zero-shot and few-shot learning capabilities
    of LLMs by conducting sentiment analysis inference without any specific training
    or modifying LLM parameters (Li and Liang, [2021](#bib.bib10)). We use three models
    from Flan-T5 with different parameters, namely Flan-T5-Base (250M), Flan-T5-Large
    (780M), and Flan-T5-XL (3B), to study the impact of model size on zero-shot and
    few-shot learning performance. Additionally, we utilize the ChatGPT (gpt-3.5-turbo)
    model from OpenAI, known for its proficiency and cost-effectiveness within the
    GPT-3.5 family. To ensure consistency in our evaluations, we set the temperature
    parameter to zero, resulting in deterministic predictions. The performance of
    Flan-T5 models and ChatGPT is assessed under these conditions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们通过在不进行特定训练或修改LLM参数的情况下进行情感分析推断，探索了LLMs的零样本学习和少样本学习能力（Li and Liang, [2021](#bib.bib10)）。我们使用了三种不同参数的Flan-T5模型，即Flan-T5-Base（250M）、Flan-T5-Large（780M）和Flan-T5-XL（3B），以研究模型规模对零样本和少样本学习性能的影响。此外，我们还利用了OpenAI的ChatGPT（gpt-3.5-turbo）模型，该模型以其在GPT-3.5家族中的高效性和成本效益而闻名。为了确保评估的一致性，我们将温度参数设置为零，从而得到确定性的预测。Flan-T5模型和ChatGPT在这些条件下的表现如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'zero-shot learning: The model is provided with only task name, task definition
    and label space, which serves as a set of options that enable the model to generate
    its response accordingly.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零样本学习：模型仅提供任务名称、任务定义和标签空间，这些作为选项使模型能够生成相应的响应。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'few-shot learning: The model is exposed to task definitions and input-output
    pairs containing K randomly selected labeled samples per class. We evaluated the
    few-shot learning capabilities of LLMs across three k-shot settings: 1-shot, 5-shot,
    and 10-shot. In each setting, we sampled K examples for each sentiment label.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: few-shot学习：模型接触到任务定义和包含每个类别K个随机选择的标记样本的输入-输出对。我们评估了LLMs在三种k-shot设置下的few-shot学习能力：1-shot、5-shot和10-shot。在每种设置中，我们为每个情感标签采样K个例子。
- en: 3.2\. Fine-tuning LLMs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 微调LLMs
- en: 'For fine-tuning, we take three Flan-T5 models: Flan-T5 base (250M), Flan-T5-large
    (780M), and Flan-T5-xl (3B parameters). FLAN-T5 is an open-source finetuned version
    of Google’s T5 model with instruct-finetuning (Chung et al., [2022](#bib.bib6)).
    They instruction-finetune T5 model which is a decoder-encoder model on a collection
    of data sources with a variety of instruction template types such as on different
    T5 model sizes. The Flan-T5 series models have indeed demonstrated remarkable
    performance compared to T5 on specific benchmarks. For example, Flan-T5-XL, with
    only 3B parameters, achieved an impressive Massive Multi-task Language Understanding
    (MMLU) score of 52.4%, surpassing the score of GPT-3 with 175B parameters by 8.5%
    (Chung et al., [2022](#bib.bib6)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调，我们使用了三种Flan-T5模型：Flan-T5 base（250M）、Flan-T5-large（780M）和Flan-T5-xl（3B参数）。FLAN-T5是Google
    T5模型的开源微调版本，具有指令微调（Chung et al., [2022](#bib.bib6)）。他们对T5模型进行指令微调，T5模型是一个解码器-编码器模型，使用各种指令模板类型的多个数据源进行微调，适用于不同的T5模型大小。与T5在特定基准上的表现相比，Flan-T5系列模型确实展示了卓越的性能。例如，Flan-T5-XL仅凭3B参数在大规模多任务语言理解（MMLU）测试中取得了52.4%的优秀成绩，超越了175B参数的GPT-3，领先了8.5%（Chung
    et al., [2022](#bib.bib6)）。
- en: Furthermore, the authors evaluate Flan instruction tuning as an intermediate
    step before single target fine-tuning, aiming to determine whether Flan-T5 could
    serve as a superior starting checkpoint for subsequent fine-tuning. The results
    indicate that employing Flan-T5 as a starting checkpoint offers an additional
    advantage in terms of training efficiency. Additionally, during single target
    fine-tuning, Flan-T5 converges much faster than T5 and achieves higher accuracies
    (e.g., increased accuracy by 7.8% in the RTE task). These findings strongly suggest
    that Flan-T5 is an excellent choice for further fine-tuning in our sentiment analysis
    task (Longpre et al., [2023](#bib.bib13)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者评估了Flan指令调优作为单一目标微调的中间步骤，旨在确定Flan-T5是否可以作为后续微调的优越起始检查点。结果表明，将Flan-T5作为起始检查点在训练效率方面提供了额外的优势。此外，在单一目标微调过程中，Flan-T5的收敛速度远快于T5，并且达到了更高的准确率（例如，RTE任务的准确率提高了7.8%）。这些发现强烈表明，Flan-T5是我们情感分析任务中进一步微调的绝佳选择（Longpre
    et al., [2023](#bib.bib13)）。
- en: 3.3\. Fine-tuning Procedure
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 微调程序
- en: Fine-tuning pre-trained language models is a widely used technique to adapt
    LLMs for specific tasks by training them on task-specific data. However, this
    process becomes computationally intensive as it requires updating all pre-trained
    model parameters. (Hu et al., [2021](#bib.bib9)). To address this challenge, Microsoft
    researchers introduced a solution known as Low-Rank-Adaption (LoRA). LoRA introduces
    pairs of rank-decomposition weight matrices into the existing weights. During
    fine-tuning, only these newly added weights are trained, while the pre-trained
    model weights remain unchanged (Hu et al., [2021](#bib.bib9)). This method was
    further enhanced by the QLoRA technique, which involves backpropagating gradients
    through a frozen, 4-bit quantized pre-trained language model into LoRA (Dettmers
    et al., [2023](#bib.bib7)). By integrating QLoRA, the fine-tuning process becomes
    more memory-efficient and computationally faster, while outperforming other efficient
    fine-tuning techniques (Dettmers et al., [2023](#bib.bib7)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练语言模型是一种广泛使用的技术，通过在特定任务的数据上训练，使LLMs适应特定任务。然而，这个过程计算成本高，因为它需要更新所有预训练模型参数（Hu
    et al., [2021](#bib.bib9)）。为了解决这个问题，微软研究人员提出了一种称为低秩适配（LoRA）的解决方案。LoRA将一对秩分解的权重矩阵引入现有权重中。在微调过程中，只有这些新添加的权重会被训练，而预训练模型的权重保持不变（Hu
    et al., [2021](#bib.bib9)）。这个方法通过QLoRA技术得到了进一步增强，QLoRA涉及通过一个冻结的4位量化预训练语言模型向LoRA反向传播梯度（Dettmers
    et al., [2023](#bib.bib7)）。通过集成QLoRA，微调过程变得更具内存效率和计算速度，同时超越了其他高效的微调技术（Dettmers
    et al., [2023](#bib.bib7)）。
- en: Given that we are dealing with LLMs with 250M, 780M, and 3B parameters, we utilize
    the QLoRA method for fine-tuning. This approach, which offers enhanced memory
    efficiency and faster computation, is well-suited for optimizing the fine-tuning
    process for models of such sizes. Table 1 displays the reduced number of trainable
    parameters for each model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们处理的是具有250M、780M和3B参数的LLM，我们采用QLoRA方法进行微调。这种方法具有增强的内存效率和更快的计算速度，非常适合优化此类模型的微调过程。表1显示了每个模型的可训练参数数量的减少。
- en: 4\. performance evaluation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 性能评估
- en: In this section, we assess the efficacy of fine-tuning smaller LLMs and in-context
    learning of ChatGPT (GPT-3.5-turbo) and Flan-T5 models for financial sentiment
    analysis task. We compare the results with state-of-the-art approaches from previous
    studies, including FinBert and Instruct-FinGPT. In order to assess the effectiveness
    of each model, we report accuracy and F1-Macro for sentiment analysis tasks including
    fin-tuned models and in-context learning settings.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们评估了对较小LLM进行微调和ChatGPT（GPT-3.5-turbo）及Flan-T5模型在金融情感分析任务中的上下文学习效果。我们将结果与之前研究中的最先进方法进行比较，包括FinBert和Instruct-FinGPT。为了评估每个模型的有效性，我们报告了包括微调模型和上下文学习设置在内的情感分析任务的准确性和F1-Macro指标。
- en: 4.1\. Datasets
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 数据集
- en: The dataset used for fine-tuning Flan-T5 models is Twitter Financial News Sentiment
    (Twitter Train), which comprises tweets related to financial topics and is accessible
    through HuggingFace. The training dataset consists of 9540 samples, with each
    sample labeled as Positive, Negative, or Neutral based on the sentiment expressed
    in the tweets.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于微调Flan-T5模型的数据集是Twitter金融新闻情感（Twitter Train），该数据集包含与金融话题相关的推文，并可通过HuggingFace获取。训练数据集包含9540个样本，每个样本根据推文表达的情感被标记为正面、负面或中性。
- en: 'Testing dataset consists of two datasets (all datasets are available through
    Hugging Face):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集由两个数据集组成（所有数据集均可通过Hugging Face获取）：
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Twitter financial news sentiment validation (TFSN): The dataset comprises 2,390
    samples of an annotated corpus of finance-related tweets. Each sample is labeled
    as Positive, Negative, or Neutral.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Twitter金融新闻情感验证（TFSN）：该数据集包含2390个标注的金融相关推文样本。每个样本被标记为正面、负面或中性。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Financial PhraseBank (FPB): The dataset consists of samples randomly extracted
    from financial news articles annotated by a team of 16 domain experts. Additionally,
    the dataset includes information on the agreement levels among the annotators
    for each sentence. We select the sentences with a 50% agreement level between
    annotators which consists of 4845 financial news and their sentiment label of
    positive, negative, or neutral (Malo et al., [2014](#bib.bib15)).'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Financial PhraseBank (FPB)：该数据集由16名领域专家团队对金融新闻文章中的样本进行随机抽取和标注。此外，数据集还包含标注者对每句话的协议水平信息。我们选择标注者协议水平为50%的句子，这些句子包含4845条金融新闻及其情感标签（正面、负面或中性）（Malo等，[2014](#bib.bib15)）。
- en: We utilize the entire training dataset for fine-tuning. Subsequently, we perform
    inference for fine-tuned model, and in-context settings on both test datasets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用整个训练数据集进行微调。随后，我们在两个测试数据集上进行微调模型和上下文设置的推理。
- en: 4.2\. Model Training Details
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 模型训练细节
- en: For each Flan-T5 model (base, large, and xl), we follow the same fine-tuning
    procedure with the same hyperparameters. To compress the pretrained language models,
    we load the Flan-T5 models in a 4-bit format. To further reduce memory requirements
    during fine-tuning, we employ LoRa with an attention dimension (r) of 8, an alpha
    parameter of 32, and a dropout probability of 0.05, resulting in a reduction in
    the number of trainable parameters, as illustrated in Table [1](#S4.T1 "Table
    1 ‣ 4.2\. Model Training Details ‣ 4\. performance evaluation ‣ A Comparative
    Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment
    Analysis").
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个Flan-T5模型（base、large和xl），我们遵循相同的微调程序，使用相同的超参数。为了压缩预训练语言模型，我们以4位格式加载Flan-T5模型。为了进一步减少微调过程中的内存需求，我们使用了LoRa，注意力维度（r）为8，alpha参数为32，dropout概率为0.05，从而减少了可训练参数的数量，如表[1](#S4.T1
    "Table 1 ‣ 4.2\. Model Training Details ‣ 4\. 性能评估 ‣ 微调LLM和LLM少量学习在金融情感分析中的比较分析")所示。
- en: '| Model | All Parameters | Trainable Parameters |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 所有参数 | 可训练参数 |'
- en: '| --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Flan-T5-Base | 248M | 0.88M |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-Base | 248M | 0.88M |'
- en: '| Flan-T5-Large | 785M | 2.36M |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-Large | 785M | 2.36M |'
- en: '| Flan-T5-XL | 2.85B | 4.72M |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Flan-T5-XL | 2.85B | 4.72M |'
- en: Table 1\. Number of trainable parameters after using QLoRA.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 使用 QLoRA 后的可训练参数数量。
- en: 'Then, we perform fine-tuning over 3 epochs with a learning rate of 1e-4 and
    a maximum input text length of 256 tokens. To prevent CUDA Out-of-Memory errors,
    we set the batch size to fit into memory automatically through exponential decay.
    We conduct fine-tuning using one A100 GPU, which leads to the following total
    training times for each model: 28 minutes for the base model, 54 minutes for the
    large model, and 65 minutes for the XL model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在 3 个 epoch 上进行微调，学习率为 1e-4，最大输入文本长度为 256 个标记。为了防止 CUDA 内存不足错误，我们设置批量大小以通过指数衰减自动适应内存。我们使用一个
    A100 GPU 进行微调，导致每个模型的总训练时间如下：基础模型 28 分钟，大型模型 54 分钟，XL 模型 65 分钟。
- en: '![Refer to caption](img/edbaaba537c893c1abd12bb72678d50c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/edbaaba537c893c1abd12bb72678d50c.png)'
- en: Figure 1\. Zero-shot performance of Flan-T5 models and ChatGPT with and without
    label description in the prompt.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. Flan-T5 模型和 ChatGPT 在有无标签描述的提示下的零样本性能。
- en: '| Dataset | Metric | FinBert | Instruct-Llama-7B |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 指标 | FinBert | Instruct-Llama-7B |'
- en: '&#124; Fine-tuned-Flan-T5 &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fine-tuned-Flan-T5 &#124;'
- en: '&#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; Base &#124; Large &#124; XL &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础 &#124; 大型 &#124; XL &#124;'
- en: '&#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ChatGPT &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ChatGPT &#124;'
- en: '&#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0-shot &#124; 1-shot &#124; 5-shot &#124; 10-shot &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0-shot &#124; 1-shot &#124; 5-shot &#124; 10-shot &#124;'
- en: '&#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FPB |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| FPB |'
- en: '&#124; Acc. &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124;'
- en: '&#124; F1 &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1 &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; – &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124;'
- en: '&#124; – &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124;'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.758 &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.758 &#124;'
- en: '&#124; 0.739 &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.739 &#124;'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.769 &#124; 0.793 &#124; 0.807 &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.769 &#124; 0.793 &#124; 0.807 &#124;'
- en: '&#124; 0.726 &#124; 0.759 &#124; 0.780 &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.726 &#124; 0.759 &#124; 0.780 &#124;'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.727 &#124; 0.791 &#124; 0.795 &#124; 0.779 &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.727 &#124; 0.791 &#124; 0.795 &#124; 0.779 &#124;'
- en: '&#124; 0.680 &#124; 0.774 &#124; 0.794 &#124; 0.783 &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.680 &#124; 0.774 &#124; 0.794 &#124; 0.783 &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| TFSN |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| TFSN |'
- en: '&#124; Acc. &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124;'
- en: '&#124; F1 &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.725 &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.725 &#124;'
- en: '&#124; 0.668 &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.668 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.881 &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.881 &#124;'
- en: '&#124; 0.842 &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.842 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.874 &#124; 0.894 &#124; 0.903 &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.874 &#124; 0.894 &#124; 0.903 &#124;'
- en: '&#124; 0.838 &#124; 0.867 &#124; 0.878 &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.838 &#124; 0.867 &#124; 0.878 &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 0.821 &#124; 0.823 &#124; 0.768 &#124; 0.724 &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.821 &#124; 0.823 &#124; 0.768 &#124; 0.724 &#124;'
- en: '&#124; 0.818 &#124; 0.819 &#124; 0.775 &#124; 0.732 &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.818 &#124; 0.819 &#124; 0.775 &#124; 0.732 &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 2\. Performance comparison between fine-tuned Flan-T5 models, in-context
    learning of ChatGPT, and state-of-the-art models. Instruct-FinGPT-7B (Zhang et al.,
    [2023b](#bib.bib27)) results are taken from its respective papers. FinBERT results
    for the FPB dataset are not reported due to its use as the training set. The best
    results are in bold.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 微调后的 Flan-T5 模型、ChatGPT 的上下文学习和最先进模型的性能比较。Instruct-FinGPT-7B（Zhang et
    al., [2023b](#bib.bib27)）的结果取自其相关论文。由于 FinBERT 结果用于训练集，因此 FPB 数据集的结果未报告。最佳结果加粗显示。
- en: 4.3\. Benchmark Models
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 基准模型
- en: In this section, we present outcomes obtained from previous studies that have
    demonstrated superior results.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了之前研究中获得的优秀结果。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FinBERT: This model is a variant of BERT, pre-trained on a financial text corpus
    comprising 1.8M news articles from Reuters TRC2 dataset. For inference, the model
    was accessed via the HuggingFace API pipeline. The sentences are initially tokenized
    and then fed into FinBERT for inference. FinBERT generates sentiment analysis
    outcomes for each textual input, classifying them as positive, negative, or neutral.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FinBERT：该模型是 BERT 的一个变种，预训练于包含 180 万篇来自 Reuters TRC2 数据集新闻文章的金融文本语料库。为了推理，该模型通过
    HuggingFace API 管道进行访问。句子首先被标记化，然后输入到 FinBERT 进行推理。FinBERT 为每个文本输入生成情感分析结果，将其分类为积极、消极或中性。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Instruct-FinGPT-7B: The model is fine-tuned on LLaMA-7B model, employing the
    instruction tuning dataset. For evaluation purposes, we rely on the results reported
    in the paper that originally introduced Instruct-FinGPT-7B (Zhang et al., [2023b](#bib.bib27)).'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Instruct-FinGPT-7B：该模型在 LLaMA-7B 模型的基础上进行了微调，使用了指令调优数据集。为了评估，我们依赖于最初介绍 Instruct-FinGPT-7B
    的论文中的结果（Zhang et al., [2023b](#bib.bib27)）。
- en: 4.4\. Prompt Design
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 提示设计
- en: 'The effectiveness and coherence of prompt formats can vary based on the training
    methodology and data used during the training process. In our experiments, we
    started with a straightforward prompt for zero-shot inference, as shown below:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 提示格式的有效性和连贯性可能会根据训练方法和使用的数据有所不同。在我们的实验中，我们从一个简单的零-shot推理提示开始，如下所示：
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'You are an AI language model trained to detect the sentiment of each sentence
    for stock prediction. Analyze the following sentence and determine if the sentiment
    is: positive or negative or neutral. Return only a single word, either Positive
    or Negative or Neutral.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是一个训练有素的AI语言模型，负责检测每个句子的情感以进行股票预测。分析以下句子并确定其情感是：积极、消极还是中立。仅返回一个单词：积极、消极或中立。
- en: Subsequently, we introduced label descriptions along with various prompt formats¹¹1Examples
    of prompt formats can be found at the following link:[https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api),
    such as ””” to separate the instruction and context, to assess whether they could
    enhance the model’s performance. The example of zero-shot and one-shot prompt
    is shown in Figure [1](#S4.F1 "Figure 1 ‣ 4.2\. Model Training Details ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis").
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们引入了标签描述和各种提示格式¹¹1提示格式的示例可以在以下链接中找到：[https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)，例如使用”””来分隔指令和上下文，以评估它们是否能够提高模型的性能。零-shot和一-shot提示的示例如图[1](#S4.F1
    "图1 ‣ 4.2\. 模型训练细节 ‣ 4\. 性能评估 ‣ 精调LLM与少-shot学习LLM在金融情感分析中的比较分析")所示。
- en: We evaluated the performance of all three Flan-T5 models and the ChatGPT model
    using zero-shot inference. The results, shown in Figure 1, indicate higher accuracy
    across most models with the prompt containing label descriptions compared to the
    prompt without label descriptions. Therefore, we adopted the prompt format shown
    in Table [4](#A1.T4 "Table 4 ‣ Appendix A Appendices ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")
    for all zero-shot and few-shot settings inference.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用零-shot推理评估了所有三种Flan-T5模型和ChatGPT模型的性能。结果如图1所示，显示大多数模型在包含标签描述的提示中具有更高的准确率，相比于没有标签描述的提示。因此，我们在所有零-shot和少-shot设置推理中采用了表[4](#A1.T4
    "表4 ‣ 附录A 附录 ‣ 精调LLM与少-shot学习LLM在金融情感分析中的比较分析")中显示的提示格式。
- en: '| Model |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; TFSN &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TFSN &#124;'
- en: '&#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; Acc. &#124; F1 &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124; F1 &#124;'
- en: '&#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; FPB &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FPB &#124;'
- en: '&#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; Acc. &#124; F1 &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率 &#124; F1 &#124;'
- en: '&#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Base &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础 &#124;'
- en: '&#124; 0-Shot &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0-Shot &#124;'
- en: '&#124; 1-Shot &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1-Shot &#124;'
- en: '&#124; 5-Shot &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5-Shot &#124;'
- en: '&#124; 10-Shot &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10-Shot &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.665 &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.665 &#124;'
- en: '&#124; 0.694 &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.694 &#124;'
- en: '&#124; 0.707 &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.707 &#124;'
- en: '&#124; 0.726 &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.726 &#124;'
- en: '&#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.593 &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.593 &#124;'
- en: '&#124; 0.624 &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.624 &#124;'
- en: '&#124; 0.643 &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.643 &#124;'
- en: '&#124; 0.659 &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.659 &#124;'
- en: '&#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.741 &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.741 &#124;'
- en: '&#124; 0.726 &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.726 &#124;'
- en: '&#124; 0.734 &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.734 &#124;'
- en: '&#124; 0.764 &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.764 &#124;'
- en: '&#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.729 &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.729 &#124;'
- en: '&#124; 0.727 &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.727 &#124;'
- en: '&#124; 0.742 &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.742 &#124;'
- en: '&#124; 0.742 &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.742 &#124;'
- en: '&#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大型 &#124;'
- en: '&#124; 0-Shot &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0-Shot &#124;'
- en: '&#124; 1-Shot &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1-Shot &#124;'
- en: '&#124; 5-Shot &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5-Shot &#124;'
- en: '&#124; 10-Shot &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10-Shot &#124;'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.687 &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.687 &#124;'
- en: '&#124; 0.742 &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.742 &#124;'
- en: '&#124; 0.765 &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.765 &#124;'
- en: '&#124; 0.778 &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.778 &#124;'
- en: '&#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.647 &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.647 &#124;'
- en: '&#124; 0.688 &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.688 &#124;'
- en: '&#124; 0.701 &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.701 &#124;'
- en: '&#124; 0.704 &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.704 &#124;'
- en: '&#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.797 &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.797 &#124;'
- en: '&#124; 0.809 &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.809 &#124;'
- en: '&#124; 0.813 &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.813 &#124;'
- en: '&#124; 0.804 &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.804 &#124;'
- en: '&#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.798 &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.798 &#124;'
- en: '&#124; 0.807 &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.807 &#124;'
- en: '&#124; 0.810 &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.810 &#124;'
- en: '&#124; 0.801 &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.801 &#124;'
- en: '&#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; XL &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; XL &#124;'
- en: '&#124; 0-Shot &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0-Shot &#124;'
- en: '&#124; 1-Shot &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1-Shot &#124;'
- en: '&#124; 5-Shot &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5-Shot &#124;'
- en: '&#124; 10-Shot &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10-Shot &#124;'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.683 &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.683 &#124;'
- en: '&#124; 0.791 &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.791 &#124;'
- en: '&#124; 0.760 &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.760 &#124;'
- en: '&#124; 0.771 &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.771 &#124;'
- en: '&#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.678 &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.678 &#124;'
- en: '&#124; 0.753 &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.753 &#124;'
- en: '&#124; 0.730 &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.730 &#124;'
- en: '&#124; 0.729 &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.729 &#124;'
- en: '&#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.815 &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.815 &#124;'
- en: '&#124; 0.847 &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.847 &#124;'
- en: '&#124; 0.832 &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.832 &#124;'
- en: '&#124; 0.780 &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.780 &#124;'
- en: '&#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; 0.823 &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.823 &#124;'
- en: '&#124; 0.836 &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.836 &#124;'
- en: '&#124; 0.833 &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.833 &#124;'
- en: '&#124; 0.796 &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.796 &#124;'
- en: '&#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 3\. Few-shot performance of Flan-T5 models on TFSB and FPB datasets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. Flan-T5模型在TFSB和FPB数据集上的少样本性能。
- en: '![Refer to caption](img/336eb77d0424b36ed36a5ad039ad14c2.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/336eb77d0424b36ed36a5ad039ad14c2.png)'
- en: Figure 2\. Few-shot prompting results on TFSN dataset compared with FinBERT,
    Fine-tuned-Flan-T5, and Instruct-FinGPT results. Utilizing the best-performing
    fine-tuned Flan-T5-XL model for ChatGPT.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. TFSN数据集上的少样本提示结果，与FinBERT、微调后的Flan-T5和Instruct-FinGPT结果相比。使用最佳性能的微调Flan-T5-XL模型进行ChatGPT。
- en: '![Refer to caption](img/e2daf5d1f655eb96255f45e7f4478b1f.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2daf5d1f655eb96255f45e7f4478b1f.png)'
- en: Figure 3\. Few-shot prompting results on FPB dataset compared with Fine-tuned-Flan-T5
    and Instruct-FinGPT results. Utilizing the best-performing fine-tuned Flan-T5-XL
    model for ChatGPT.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. FPB数据集上的少样本提示结果，与微调后的Flan-T5和Instruct-FinGPT结果相比。使用最佳性能的微调Flan-T5-XL模型进行ChatGPT。
- en: 5\. Evaluation Results
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 评估结果
- en: As mentioned in section [2](#S2 "2\. related work ‣ A Comparative Analysis of
    Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"),
    our main focus is on evaluating the zero-shot and few-shot learning performance
    of LLMs, including both smaller and larger models in terms of parameter size,
    in comparison to fine-tuned smaller LLMs. The results of Flan-T5 fine-tuned models
    on both TFSN and FPB datasets, along with the benchmark models, and the zero-shot
    and few-shot performance of ChatGPT, are presented in Table [2](#S4.T2 "Table
    2 ‣ 4.2\. Model Training Details ‣ 4\. performance evaluation ‣ A Comparative
    Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment
    Analysis"). Additionally, the results of the zero-shot and few-shot performance
    of Flan-T5 models are shown in Table [3](#S4.T3 "Table 3 ‣ 4.4\. Prompt Design
    ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis").
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2](#S2 "2\. 相关工作 ‣ 微调LLMs与LLMs在金融情感分析中的少量学习比较分析")节所述，我们主要关注评估LLMs的零样本和少样本学习性能，包括较小和较大的模型，并与微调后的较小LLMs进行比较。Flan-T5微调模型在TFSN和FPB数据集上的结果，以及基准模型和ChatGPT的零样本和少样本性能，展示在表[2](#S4.T2
    "表2 ‣ 4.2\. 模型训练细节 ‣ 4\. 性能评估 ‣ 微调LLMs与LLMs在金融情感分析中的少量学习比较分析")中。此外，Flan-T5模型的零样本和少样本性能结果展示在表[3](#S4.T3
    "表3 ‣ 4.4\. 提示设计 ‣ 4\. 性能评估 ‣ 微调LLMs与LLMs在金融情感分析中的少量学习比较分析")中。
- en: Fine-tuned LLMs Results. As the results indicate in Figure [2](#S4.F2 "Figure
    2 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")
    and Table [2](#S4.T2 "Table 2 ‣ 4.2\. Model Training Details ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis"), on the TFSN dataset, the performance
    of fine-tuned Flan-T5 models is comparable to the state-of-the-art model, Instruct-FinGPT
    ,and significatly outperforms FinBert results. It is noteworthy that we achieved
    this level of performance with significantly fewer computational resources (1
    A100 GPU compared to 8 A100 GPUs) and a comparable or even shorter training time
    by utilizing QLoRA method , especially for the Flan-T5-Base model (28 minutes).
    These findings align with a previous study that highlights the efficiency advantage
    of using Flan-T5 as a starting checkpoint for further fine-tuning, as discussed
    in section [3.2](#S3.SS2 "3.2\. Fine-tuning LLMs ‣ 3\. Method and Experimental
    setup ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs
    for Financial Sentiment Analysis").
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM的结果。正如图[2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation
    ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis")和表[2](#S4.T2 "Table 2 ‣ 4.2\. Model Training Details
    ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis")中的结果所示，在TFSN数据集上，微调后的Flan-T5模型的性能与最先进的模型Instruct-FinGPT相当，并显著优于FinBert的结果。值得注意的是，我们通过使用QLoRA方法显著减少了计算资源（1个A100
    GPU与8个A100 GPU相比），并且在训练时间上也可比或更短，特别是对于Flan-T5-Base模型（28分钟）。这些发现与先前的研究一致，强调了将Flan-T5作为进一步微调起始点的效率优势，如第[3.2](#S3.SS2
    "3.2\. Fine-tuning LLMs ‣ 3\. Method and Experimental setup ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")节所讨论的。
- en: Zero-shot Results. As shown in Figure [2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt Design
    ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis"), the zero-shot learning results
    of Flan-T5 models (Base, Largr, and XL) on the TFSN dataset fall significantly
    behind those of all fine-tuned models (FinBert, Instruct-FinGPT, and all fine-tuned
    Flan-T5 models). Notably, Instruct-FinGPT and fine-tuned Flan-T5 models outperform
    LLMs by a clear margin of roughly 20%. However, the zero-shot performance of ChatGPT
    reaches 82%, surpassing the FinBert model but still remaining inferior to Instruct-FinGPT
    and all fine-tuned Flan-T5 models.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot结果。如图[2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation
    ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis")所示，Flan-T5模型（Base、Large和XL）在TFSN数据集上的零-shot学习结果显著落后于所有微调模型（FinBert、Instruct-FinGPT和所有微调Flan-T5模型）。值得注意的是，Instruct-FinGPT和微调Flan-T5模型以大约20%的明显差距超越了LLMs。然而，ChatGPT的零-shot性能达到82%，超越了FinBert模型，但仍然不及Instruct-FinGPT和所有微调Flan-T5模型。
- en: As depicted in Figure [3](#S4.F3 "Figure 3 ‣ 4.4\. Prompt Design ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis") and Table [3](#S4.T3 "Table 3 ‣ 4.4\.
    Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned
    LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"), the zero-shot
    results of all Flan-T5 models on the FPB dataset show more promising outcomes,
    even performing comparably to the corresponding fine-tuned Flan-T5 models. This
    observation aligns with previous research findings (Zhang et al., [2023a](#bib.bib28)).
    Additionally, a noteworthy finding on the FPB dataset is that larger models, with
    a greater number of parameters, tend to outperform the smaller ones in zero-shot
    inference. For instance, when comparing the performance between Flan-T5-Base and
    Flan-T5-XL, there is an increase of roughly 7% in performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S4.F3 "Figure 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣
    A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial
    Sentiment Analysis")和表[3](#S4.T3 "Table 3 ‣ 4.4\. Prompt Design ‣ 4\. performance
    evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of
    LLMs for Financial Sentiment Analysis")所示，所有Flan-T5模型在FPB数据集上的零-shot结果显示出更有前景的结果，甚至与相应的微调Flan-T5模型表现相当。这个观察结果与先前的研究发现一致（Zhang等人，[2023a](#bib.bib28)）。此外，在FPB数据集上的一个显著发现是，具有更多参数的较大模型在零-shot推断中往往优于较小的模型。例如，当比较Flan-T5-Base和Flan-T5-XL的表现时，性能大约提高了7%。
- en: When comparing the zero-shot performance of Flan-T5 models to ChatGPT, ChatGPT
    appears to be less accurate despite having larger parameters, which contradicts
    the zero-shot inference on the TFSN dataset. This discrepancy in performance can
    be explained by the different language and jargon used in the FPB news headlines
    and the TFSN social media posts. ChatGPT, being trained on a large corpus of social
    media posts, is better equipped to extract the true sentiment from social media
    text, which is reflected in its zero-shot performance on the TFSN dataset. On
    the other hand, smaller LLMs like Flan-T5 models outperform ChatGPT in the zero-shot
    setting on the FPB dataset, possibly due to their large-scale instruction tuning,
    which enhances their reasoning capabilities and allows them to extract sentiment
    from more complex texts.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当将Flan-T5模型的零样本性能与ChatGPT进行比较时，尽管ChatGPT参数更多，但其准确性似乎较低，这与TFSN数据集上的零样本推断结果相悖。这种性能差异可以通过FPB新闻标题和TFSN社交媒体帖子中使用的不同语言和术语来解释。ChatGPT在大量社交媒体帖子上训练，更能从社交媒体文本中提取真实情感，这反映在其在TFSN数据集上的零样本性能中。另一方面，像Flan-T5这样的较小LLMs在FPB数据集上的零样本设置中表现优于ChatGPT，可能是由于其大规模的指令调整，提高了其推理能力，使其能够从更复杂的文本中提取情感。
- en: Few-shot Results. Results comparing few-shot prompting performance with zero-shot
    prompting performance are presented in Figure [2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt
    Design ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs
    and Few-Shot Learning of LLMs for Financial Sentiment Analysis"), Figure [3](#S4.F3
    "Figure 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis"),
    and Table [3](#S4.T3 "Table 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation
    ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for
    Financial Sentiment Analysis"). Notably, we observe that 1-shot prompting can
    significantly improve the performance across almost all models and datasets, except
    for Flan-T5-Base (250M), where there is a slight decrease in performance. This
    decrease can be attributed to the impact of a single unrelated example, which
    can have an adverse effect on the smaller LLMs. Interestingly, we find that smaller
    LLMs (Flan-T5) benefit more on average from one demonstration compared to ChatGPT.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 几样本结果。比较几样本提示性能与零样本提示性能的结果见于图 [2](#S4.F2 "Figure 2 ‣ 4.4\. Prompt Design ‣ 4\.
    performance evaluation ‣ A Comparative Analysis of Fine-Tuned LLMs and Few-Shot
    Learning of LLMs for Financial Sentiment Analysis")、图 [3](#S4.F3 "Figure 3 ‣ 4.4\.
    Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis of Fine-Tuned
    LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis") 和表 [3](#S4.T3
    "Table 3 ‣ 4.4\. Prompt Design ‣ 4\. performance evaluation ‣ A Comparative Analysis
    of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis")。值得注意的是，我们观察到1-shot提示在几乎所有模型和数据集上的性能都显著提高，除了Flan-T5-Base
    (250M)，在该模型中性能略有下降。这一下降可以归因于一个无关示例的影响，这可能对较小的LLMs产生不利影响。有趣的是，我们发现较小的LLMs（Flan-T5）在平均上从一个示例中获益更多，相较于ChatGPT。
- en: The impact of increasing shots to 5 and 10 exhibited variability across various
    models and datasets. On the TFSN dataset, it was observed that a marginal improvement
    in performance was achieved for both Flan-T5-Base and Flan-T5-Large models with
    an increase in the number of shots. However, the performance of Flan-T5-Large
    and Flan-T5-XL was impeded by the increase in shots. The variation in outcomes
    can be ascribed to the difficulties associated with managing excessively lengthy
    contexts, which have the potential to lead the LLMs misguided, as revealed in
    a new study (Zhang et al., [2023a](#bib.bib28)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 将拍摄次数增加到5次和10次的影响在各种模型和数据集上表现出变异性。在TFSN数据集上，观察到Flan-T5-Base和Flan-T5-Large模型在拍摄次数增加时性能略有提升。然而，Flan-T5-Large和Flan-T5-XL的性能却因拍摄次数的增加而受到阻碍。结果的变化可归因于处理过长上下文的困难，这可能导致LLMs被误导，正如一项新研究（Zhang
    et al., [2023a](#bib.bib28)）所揭示的那样。
- en: These findings are consistent with a previous study that highlighted the sensitivity
    issue of LLMs when exposed to few-shot examples during inference (Zhang et al.,
    [2023a](#bib.bib28)). In order to mitigate these disparities and obtain consistent
    enhancements in performance, it is imperative to employ more efficacious techniques
    for few-shot learning. For example, a recent study suggested the retrieval of
    examples that possess semantic similarity to a test query sample in order to generate
    its corresponding prompt (Liu et al., [2021](#bib.bib12)). Moreover, the introduction
    of the Chain of Thought (CoT) and Clue And Reasoning Prompting (CARP) methods
    aimed to improve the reasoning capabilities of LLMs. which could be beneficial
    for extracting sentiment from financial corpora (Sun et al., [2023](#bib.bib19);
    Wei et al., [2022b](#bib.bib23)). Our future work will encompass an exploration
    of these approaches
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现与之前的一项研究一致，该研究强调了LLMs在推理过程中暴露于few-shot示例时的敏感性问题（Zhang et al., [2023a](#bib.bib28)）。为了减轻这些差异并获得一致的性能提升，必须采用更有效的few-shot学习技术。例如，一项最新的研究建议检索与测试查询样本具有语义相似性的示例，以生成相应的提示（Liu
    et al., [2021](#bib.bib12)）。此外，Chain of Thought (CoT) 和 Clue And Reasoning Prompting
    (CARP) 方法的引入旨在提高LLMs的推理能力，这对从金融语料库中提取情感可能有益（Sun et al., [2023](#bib.bib19); Wei
    et al., [2022b](#bib.bib23)）。我们的未来工作将包括对这些方法的探索。
- en: 6\. Conclusion and future directions
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论与未来方向
- en: In this study, we extensively compared the performance of fine-tuned LLMs with
    different parameter sizes (ranging from 250M to 3B) and their in-context learning
    capabilities, for financial sentiment analysis. Our experimental results demonstrate
    that fine-tuned LLMs achieved comparable performance to state-of-the-art models
    while utilizing significantly fewer computational resources. The zero-shot and
    one-shot settings performed impressively, especially on the FPB dataset, which
    can be attributed to the corpus used for pre-training the LLMs. Moreover, larger
    LLMs demonstrated better performance in the zero-shot and one-shot settings.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们广泛比较了不同参数规模（从250M到3B）的微调LLMs的性能及其上下文学习能力，以进行金融情感分析。我们的实验结果表明，微调的LLMs在使用显著较少的计算资源的情况下，达到了与最先进模型相当的性能。零-shot和one-shot设置表现出色，特别是在FPB数据集上，这可以归因于用于预训练LLMs的语料库。此外，较大的LLMs在零-shot和one-shot设置中表现更佳。
- en: The remarkable results obtained from fine-tuned, zero-shot, and one-shot inferences
    of Flan-T5 models can be attributed to their instruct fine-tuning. However, we
    observed inconsistent performance in the five-shot and ten-shot settings across
    all models and datasets. Notably, increasing the number of shots did not lead
    to improved performance on average. Nevertheless, the models demonstrated reasonable
    performance on both datasets and across all three Flan-T5 models and ChatGPT,
    indicating their potential for financial sentiment analysis considering the scarcity
    of labeled data in finance-domain.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从Flan-T5模型的微调、零-shot和one-shot推理中获得的显著结果可归因于其指令微调。然而，我们观察到在所有模型和数据集中，five-shot和ten-shot设置的性能不一致。值得注意的是，增加shot的数量并没有平均提升性能。然而，这些模型在两个数据集和所有三个Flan-T5模型以及ChatGPT上表现出了合理的性能，考虑到金融领域标记数据的稀缺，这表明它们在金融情感分析中的潜力。
- en: In conclusion, our study showcases the remarkable capabilities of LLMs, even
    smaller models, in both fine-tuning and in-context learning for financial sentiment
    analysis task. These findings provide valuable insights into potential avenues
    for further investigation in the field of financial sentiment analysis.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究展示了LLMs，甚至是较小模型，在金融情感分析任务中的卓越能力。这些发现为进一步研究金融情感分析领域提供了宝贵的见解。
- en: For future studies, we plan to assess the new methods of prompt formatting,
    such as CoT and CARP, and prompt selection by retrieving semantically-similar
    examples to the test queries, rather than random sampling for few-shot settings
    (Sun et al., [2023](#bib.bib19); Wei et al., [2022b](#bib.bib23)). The objective
    of this study is to determine the extent to which these methods can consistently
    enhance the performance of LLMs in financial sentiment analysis tasks.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来的研究，我们计划评估新的提示格式化方法，如CoT和CARP，以及通过检索语义相似的示例来进行提示选择，而不是在few-shot设置中进行随机抽样（Sun
    et al., [2023](#bib.bib19); Wei et al., [2022b](#bib.bib23)）。本研究的目标是确定这些方法在多大程度上可以持续提高LLMs在金融情感分析任务中的性能。
- en: References
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Brown et al. (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020a. Language models are few-shot learners. *Advances
    in neural information processing systems* 33 (2020), 1877–1901.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020a) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人. 2020a. 语言模型是少样本学习者。*神经信息处理系统进展* 33 (2020), 1877–1901。
- en: Brown et al. (2020b) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. 2020b. Language models are few-shot learners. *Advances
    in neural information processing systems* 33 (2020), 1877–1901.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020b) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人. 2020b. 语言模型是少样本学习者。*神经信息处理系统进展* 33 (2020), 1877–1901。
- en: 'Chen et al. (2018) Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2018.
    NTUSD-Fin: a market sentiment dictionary for financial social media data applications.
    In *Proceedings of the 1st Financial Narrative Processing Workshop (FNP 2018)*.
    37–43.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2018) Chung-Chi Chen, Hen-Hsen Huang 和 Hsin-Hsi Chen. 2018. NTUSD-Fin：金融社交媒体数据应用的市场情感词典。在
    *第1届金融叙事处理研讨会 (FNP 2018) 论文集* 中，37–43。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems* 30 (2017).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等人 (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg 和 Dario Amodei. 2017. 从人类偏好中进行深度强化学习。*神经信息处理系统进展* 30 (2017)。
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*
    (2022).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人 (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma 等人. 2022.
    扩展指令微调语言模型。*arXiv 预印本 arXiv:2210.11416* (2022)。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint
    arXiv:2305.14314* (2023).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人 (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman 和 Luke Zettlemoyer.
    2023. Qlora：高效微调量化的大规模语言模型。*arXiv 预印本 arXiv:2305.14314* (2023)。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人 (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2018. Bert：用于语言理解的深度双向变换器预训练。*arXiv 预印本 arXiv:1810.04805* (2018)。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen. 2021. Lora：大规模语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*
    (2021)。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190* (2021).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang (2021) Xiang Lisa Li 和 Percy Liang. 2021. 前缀调整：优化生成的连续提示。*arXiv 预印本
    arXiv:2101.00190* (2021)。
- en: Liu et al. (2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning
    is better and cheaper than in-context learning. *Advances in Neural Information
    Processing Systems* 35 (2022), 1950–1965.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang,
    Mohit Bansal 和 Colin A Raffel. 2022. 少样本参数高效微调比上下文学习更好、更便宜。*神经信息处理系统进展* 35 (2022),
    1950–1965。
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-$3$?
    *arXiv preprint arXiv:2101.06804* (2021).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin 和 Weizhu Chen. 2021. 什么样的上下文示例对 GPT-$3$ 最佳？*arXiv 预印本 arXiv:2101.06804*
    (2021)。
- en: 'Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won
    Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The
    flan collection: Designing data and methods for effective instruction tuning.
    *arXiv preprint arXiv:2301.13688* (2023).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longpre 等人 (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung,
    Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei 等人. 2023. Flan 集合：为有效的指令调优设计数据和方法。*arXiv
    预印本 arXiv:2301.13688* (2023)。
- en: Loughran and McDonald (2011) Tim Loughran and Bill McDonald. 2011. When is a
    liability not a liability? Textual analysis, dictionaries, and 10-Ks. *The Journal
    of finance* 66, 1 (2011), 35–65.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loughran and McDonald (2011) Tim Loughran 和 Bill McDonald. 2011. 何时负债不再是负债？文本分析、词典与
    10-K 报告。*金融杂志* 66, 1 (2011)，35–65。
- en: 'Malo et al. (2014) Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius,
    and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations
    in economic texts. *Journal of the Association for Information Science and Technology*
    65, 4 (2014), 782–796.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malo et al. (2014) Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius,
    和 Pyry Takala. 2014. 好债务还是坏债务：在经济文本中检测语义取向。*信息科学与技术协会期刊* 65, 4 (2014)，782–796。
- en: 'Mishev et al. (2020) Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lubomir T
    Chitkushev, and Dimitar Trajanov. 2020. Evaluation of sentiment analysis in finance:
    from lexicons to transformers. *IEEE access* 8 (2020), 131662–131682.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishev et al. (2020) Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lubomir
    T Chitkushev, 和 Dimitar Trajanov. 2020. 财务中的情感分析评估：从词典到变换器。*IEEE Access* 8 (2020)，131662–131682。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等。2019. 语言模型是无监督的多任务学习者。*OpenAI 博客* 1, 8 (2019)，9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research* 21, 1 (2020), 5485–5551.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究期刊*
    21, 1 (2020)，5485–5551。
- en: Sun et al. (2023) Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei
    Zhang, and Guoyin Wang. 2023. Text Classification via Large Language Models. *arXiv
    preprint arXiv:2305.08377* (2023).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei
    Zhang, 和 Guoyin Wang. 2023. 通过大型语言模型进行文本分类。*arXiv 预印本 arXiv:2305.08377* (2023)。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等。2023. Llama：开源高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*
    (2023)。
- en: Wang et al. (2023) Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui
    Xia. 2023. Is ChatGPT a good sentiment analyzer? A preliminary study. *arXiv preprint
    arXiv:2304.04339* (2023).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui
    Xia. 2023. ChatGPT 是一个优秀的情感分析工具吗？一项初步研究。*arXiv 预印本 arXiv:2304.04339* (2023)。
- en: Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022a. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等。2022a. 链式思考提示在大型语言模型中引发推理。*神经信息处理系统进展* 35
    (2022)，24824–24837。
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等。2022b. 链式思考提示在大型语言模型中引发推理。*神经信息处理系统进展* 35
    (2022)，24824–24837。
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    2023. Bloomberggpt: A large language model for finance. *arXiv preprint arXiv:2303.17564*
    (2023).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze,
    Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, 和 Gideon Mann. 2023.
    Bloomberggpt：一个金融领域的大型语言模型。*arXiv 预印本 arXiv:2303.17564* (2023)。
- en: 'Yang et al. (2023b) Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023b.
    FinGPT: Open-Source Financial Large Language Models. *arXiv preprint arXiv:2306.06031*
    (2023).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023b) Hongyang Yang, Xiao-Yang Liu, 和 Christina Dan Wang. 2023b.
    FinGPT：开源金融大型语言模型。*arXiv 预印本 arXiv:2306.06031* (2023)。
- en: 'Yang et al. (2023a) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023a. Harnessing the power
    of llms in practice: A survey on chatgpt and beyond. *arXiv preprint arXiv:2304.13712*
    (2023).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2023a）景峰杨、洪烨金、瑞祥唐、小天韩、启章丰、浩铭江、冰尹和夏胡。2023a。实践中利用大语言模型的力量：关于ChatGPT及其他的调查。*arXiv预印本
    arXiv:2304.13712*（2023年）。
- en: 'Zhang et al. (2023b) Boyu Zhang, Hongyang Yang, and Xiao-Yang Liu. 2023b. Instruct-FinGPT:
    Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language
    Models. *arXiv preprint arXiv:2306.12659* (2023).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023b）博宇张、洪阳杨和肖阳刘。2023b。Instruct-FinGPT：通过对通用大语言模型的指令调整进行金融情绪分析。*arXiv预印本
    arXiv:2306.12659*（2023年）。
- en: 'Zhang et al. (2023a) Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and
    Lidong Bing. 2023a. Sentiment Analysis in the Era of Large Language Models: A
    Reality Check. *arXiv preprint arXiv:2305.15005* (2023).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2023a）文轩张、悦邓、冰刘、辛诺·贾林·潘和李东冰。2023a。在大语言模型时代的情绪分析：现实检查。*arXiv预印本 arXiv:2305.15005*（2023年）。
- en: Zhong et al. (2023) Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng
    Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned
    bert. *arXiv preprint arXiv:2302.10198* (2023).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人（2023）祁煌钟、梁丁、菊华刘、博杜和大成陶。2023。ChatGPT也能理解吗？对ChatGPT和微调BERT的比较研究。*arXiv预印本
    arXiv:2302.10198*（2023年）。
- en: Appendix A Appendices
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: We introduce a zero-shot and 1-shot prompt designed for the financial sentiment
    analysis task, displayed on the subsequent page.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一个零样本和一样本提示，旨在用于金融情绪分析任务，展示在后续页面。
- en: '| Zero-shot prompt |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 零样本提示 |'
- en: '| --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| You are an AI language model trained to detect the sentiment of sentences
    for stock prediction See below all the possible labels and their descriptions
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个训练用来检测股票预测句子情绪的AI语言模型。请参见下面所有可能的标签及其描述 |'
- en: '| ””” |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: Bearish sentiment |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 描述：看跌情绪 |'
- en: '| label: Negative |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 标签：负面 |'
- en: '| ””” |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: neutral sentiment |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 描述：中立情绪 |'
- en: '| label: Neutral |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 标签：中立 |'
- en: '| ””” |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: Bullish sentiment |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 描述：看涨情绪 |'
- en: '| label: Positive |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 标签：积极 |'
- en: '| ””” |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| Here is the sentence that needs to be classified |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 这里是需要分类的句子 |'
- en: '| sentence: {sentence} |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 句子：{sentence} |'
- en: '| label: |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 标签： |'
- en: '| One-shot Prompt |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 一样本提示 |'
- en: '| You are an AI language model trained to detect the sentiment of sentences
    for stock prediction See below all the possible labels and their descriptions
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个训练用来检测股票预测句子情绪的AI语言模型。请参见下面所有可能的标签及其描述 |'
- en: '| ””” |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: Bearish sentiment |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 描述：看跌情绪 |'
- en: '| label: Negative |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 标签：负面 |'
- en: '| ””” |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: neutral sentiment |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 描述：中立情绪 |'
- en: '| label: Neutral |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 标签：中立 |'
- en: '| ””” |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| description: Bullish sentiment |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 描述：看涨情绪 |'
- en: '| label: Positive |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 标签：积极 |'
- en: '| ””” |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| See below a couple of examples |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 请参见以下几个示例 |'
- en: '| ””” |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| text: Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 文本：Cemex在瑞士信贷和摩根大通下调评级，因建筑前景疲软 |'
- en: '| label: Negative |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 标签：负面 |'
- en: '| ””” |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| text: ululemon Falls on Conservative View But Analysts Keep Faith |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 文本：Lululemon因保守观点而下跌，但分析师保持信心 |'
- en: '| label: Neutral |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 标签：中立 |'
- en: '| ””” |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| text: Wells Fargo Downgrades Netflix $NFLX to Underperform but sees as a
    takeover target. NFLX could get acquired |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 文本：富国银行将Netflix $NFLX 的评级下调至“跑输大盘”，但认为其是收购目标。NFLX 可能会被收购 |'
- en: '| label: Positive |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 标签：积极 |'
- en: '| ””” |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ””” |'
- en: '| Here is the sentence that needs to be classified |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 这里是需要分类的句子 |'
- en: '| sentence: {sentence} |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 句子：{sentence} |'
- en: '| label: |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 标签： |'
- en: Table 4\. Prompts for zero-shot and one-shot settings. In the Five-Shot and
    Ten-Shot settings, 5 and 10 samples were added for each class, respectively.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 表4。零样本和一样本设置的提示。在五样本和十样本设置中，分别为每个类别添加了5个和10个样本。
