- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and
    Prompt Engineering'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多方目标追踪与LLMs：比较预训练、微调和提示工程
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.15231](https://ar5iv.labs.arxiv.org/html/2308.15231)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.15231](https://ar5iv.labs.arxiv.org/html/2308.15231)
- en: Angus Addlesee,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Angus Addlesee,
- en: Weronika Sieińska, Nancie Gunson,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Weronika Sieińska, Nancie Gunson,
- en: Daniel Hernandez Garcia, Christian Dondrup
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Hernandez Garcia, Christian Dondrup
- en: Heriot-Watt University, Edinburgh
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 赫瑞瓦特大学，爱丁堡
- en: '{a.addlesee, w.sieinska, n.gunson,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{a.addlesee, w.sieinska, n.gunson,'
- en: d.hernandez_garcia, c.dondrup}@hw.ac.uk \AndOliver Lemon
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: d.hernandez_garcia, c.dondrup}@hw.ac.uk \AndOliver Lemon
- en: Heriot-Watt University, Edinburgh
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 赫瑞瓦特大学，爱丁堡
- en: Edinburgh Centre for Robotics
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 爱丁堡机器人中心
- en: Alana AI
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Alana AI
- en: o.lemon@hw.ac.uk
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: o.lemon@hw.ac.uk
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This paper evaluates the extent to which current Large Language Models (LLMs)
    can capture task-oriented multi-party conversations (MPCs). We have recorded and
    transcribed 29 MPCs between patients, their companions, and a social robot in
    a hospital. We then annotated this corpus for multi-party goal-tracking and intent-slot
    recognition. People share goals, answer each other’s goals, and provide other
    people’s goals in MPCs – none of which occur in dyadic interactions. To understand
    user goals in MPCs, we compared three methods in zero-shot and few-shot settings:
    we fine-tuned T5, created pre-training tasks to train DialogLM using LED, and
    employed prompt engineering techniques with GPT-3.5-turbo, to determine which
    approach can complete this novel task with limited data. GPT-3.5-turbo significantly
    outperformed the others in a few-shot setting. The ‘reasoning’ style prompt, when
    given 7% of the corpus as example annotated conversations, was the best performing
    method. It correctly annotated 62.32% of the goal tracking MPCs, and 69.57% of
    the intent-slot recognition MPCs. A ‘story’ style prompt increased model hallucination,
    which could be detrimental if deployed in safety-critical settings. We conclude
    that multi-party conversations still challenge state-of-the-art LLMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文评估了当前的大型语言模型（LLMs）在捕捉任务导向的多方对话（MPCs）方面的能力。我们记录并转录了29个患者、陪伴者和医院中的社交机器人之间的MPC。然后，我们对这些语料进行了多方目标追踪和意图槽位识别的注释。在MPC中，人们分享目标、回答彼此的目标，并提供其他人的目标——这些在双人对话中是不存在的。为了理解MPC中的用户目标，我们在零样本和少样本设置下比较了三种方法：我们对T5进行了微调，创建了预训练任务来使用LED训练DialogLM，并采用了GPT-3.5-turbo的提示工程技术，以确定哪种方法可以在有限数据的情况下完成这一新任务。GPT-3.5-turbo在少样本设置中显著优于其他方法。'推理'风格的提示，在提供7%语料作为示例注释对话时，表现最佳。它正确注释了62.32%的目标追踪MPC和69.57%的意图槽位识别MPC。'故事'风格的提示增加了模型的虚假生成，如果在安全关键设置中使用可能会有害。我们得出结论，多方对话仍然对最先进的LLMs提出了挑战。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Spoken Dialogue Systems (SDSs) are increasingly being embedded in social robots
    that are expected to seamlessly interact with people in populated public spaces
    like museums, airports, shopping centres, or hospital waiting rooms Foster et al.
    ([2019](#bib.bib17)); Tian et al. ([2021](#bib.bib43)); Gunson et al. ([2022](#bib.bib22)).
    Unlike virtual agents or voice assistants (e.g. Alexa, Siri, or Google Assistant),
    which typically have dyadic interactions with a single user, social robots are
    often approached by pairs and groups of individuals Al Moubayed et al. ([2012](#bib.bib6));
    Moujahid et al. ([2022](#bib.bib32)). Families may approach a social robot in
    a museum, and patients are often accompanied by a family member when visiting
    a hospital. In these multi-party scenarios, tasks that are considered trivial
    for SDSs become substantially more complex Traum ([2004](#bib.bib44)); Zhong et al.
    ([2022](#bib.bib51)); Addlesee et al. ([2023](#bib.bib5)). In multi-party conversations
    (MPCs), the social robot must determine which user said an utterance, who that
    utterance was directed to, when to respond, and what it should say depending on
    whom the robot is addressing Hu et al. ([2019](#bib.bib26)); Gu et al. ([2021](#bib.bib21),
    [2022a](#bib.bib19)). These tasks are collectively referred to as “who says what
    to whom” in the multi-party literature Gu et al. ([2022b](#bib.bib20)), but these
    tasks alone provide no incentive for a system to actually help a user reach their
    goals. State of the art “who says what to whom” systems can, therefore, only mimic
    what a good MPC *looks like* Addlesee et al. ([2023](#bib.bib5)), but for practical
    systems we also need to know what each user’s goals are. We therefore propose
    two further tasks that become substantially more complex when considered in a
    multi-party setting: goal tracking and intent-slot recognition Addlesee et al.
    ([2023](#bib.bib5)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 口语对话系统（SDSs）越来越多地嵌入到社会机器人中，这些机器人被期望能够在博物馆、机场、购物中心或医院候诊室等人口密集的公共空间中与人 seamlessly
    互动 Foster et al. ([2019](#bib.bib17)); Tian et al. ([2021](#bib.bib43)); Gunson
    et al. ([2022](#bib.bib22))。与通常具有单一用户二人互动的虚拟代理或语音助手（例如Alexa、Siri或Google Assistant）不同，社会机器人通常会被一对或一组人接近
    Al Moubayed et al. ([2012](#bib.bib6)); Moujahid et al. ([2022](#bib.bib32))。家庭成员可能会在博物馆中接近社会机器人，患者在访问医院时通常会有家人陪同。在这些多方情境下，对于SDSs而言，原本被认为是微不足道的任务变得复杂得多
    Traum ([2004](#bib.bib44)); Zhong et al. ([2022](#bib.bib51)); Addlesee et al.
    ([2023](#bib.bib5))。在多方对话（MPCs）中，社会机器人必须确定哪个用户说了某句话，该话语是针对谁的，何时响应，以及根据机器人正在与谁交谈来决定说什么
    Hu et al. ([2019](#bib.bib26)); Gu et al. ([2021](#bib.bib21), [2022a](#bib.bib19))。这些任务在多方文献中统称为“谁对谁说了什么”
    Gu et al. ([2022b](#bib.bib20))，但这些任务本身并未促使系统实际帮助用户达成目标。因此，最先进的“谁对谁说了什么”系统只能模拟一个好的MPC
    *看起来像什么* Addlesee et al. ([2023](#bib.bib5))，但对于实际系统，我们还需要了解每个用户的目标。因此，我们提出了两个在多方设置中变得复杂得多的进一步任务：目标跟踪和意图槽识别
    Addlesee et al. ([2023](#bib.bib5))。
- en: '| 1 | U1: | What time was our appointment? |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 1 | U1: | 我们的约会时间是几点？ |'
- en: '| 2 | U2: | We have an appointment at 10.30pm. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 2 | U2: | 我们的约会时间是晚上10:30。 |'
- en: '| 3 | U1: | Ok. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 3 | U1: | 好的。 |'
- en: 'Table 1: An example extract from our new corpus. This example illustrates that
    people complete other user’s goals in an MPC. The system must understand that
    U1’s question was answered by U2, and it does not need to answer this question
    as if it was a dyadic interaction. Further annotated examples can be found in
    Table [3](#S2.T3 "Table 3 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with
    LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：来自我们新语料库的一个示例摘录。此示例说明了人们在多方对话（MPC）中完成其他用户的目标。系统必须理解U1的问题是由U2回答的，因此不需要像在二人互动中那样回答这个问题。更多注释示例见表
    [3](#S2.T3 "表 3 ‣ 2 数据集与任务 ‣ 使用LLMs的多方目标跟踪：比较预训练、微调和提示工程")。
- en: 'Dialogue State Tracking (DST) is a well-established task Lee et al. ([2021](#bib.bib28));
    Feng et al. ([2022](#bib.bib15)) that is considered crucial to the success of
    a dialogue system Williams et al. ([2016](#bib.bib48)). DST corpora are abundant
    Henderson et al. ([2014a](#bib.bib23), [b](#bib.bib24)), but they only contain
    dyadic conversations. No corpus exists containing MPCs with goal tracking or intent-slot
    annotations, yet there are important differences. Consider the example in Table
    [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering") (from our new corpus, detailed
    in Section [2](#S2 "2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs:
    Comparing Pre-training, Fine-tuning, and Prompt Engineering")). In turn 1, we
    can identify that User 1 (U1) wants to know their appointment time. Before the
    social robot had time to answer, User 2 (U2) answered in turn 2\. This obviously
    does not occur in a dyadic interaction, yet this understanding is essential for
    natural system behaviour. The SDS must determine that it should not repeat the
    answer to the question, so data must be collected to learn this. Other major differences
    exist too. For example, current DST corpora do not contain a concept of ‘shared
    goals’ Eshghi and Healey ([2016](#bib.bib14)). If two people approach a café counter,
    the barista must determine whether the two people are separate (two individuals
    wanting to get coffee), or together (two friends with the shared goal to get coffee)
    Keizer et al. ([2013](#bib.bib27)). The interaction changes depending on this
    fact, it would be unusual to ask “are you paying together” to two individuals.
    Shared goals can commonly be identified through explicit dialogue. For example,
    the use of ‘we’ in “We are looking for the bathrooms”. Similar to answering each
    other’s questions, people may also ask questions on behalf of others. In our corpus,
    a person said “ARI, the person that I’m accompanying feels intimidated by you,
    and they’d like to know where they can eat”.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '对话状态跟踪（DST）是一个成熟的任务，Lee 等人 ([2021](#bib.bib28)); Feng 等人 ([2022](#bib.bib15))
    认为它对于对话系统的成功至关重要，Williams 等人 ([2016](#bib.bib48))。DST 语料库丰富，Henderson 等人 ([2014a](#bib.bib23),
    [b](#bib.bib24))，但它们仅包含双向对话。目前没有包含带有目标跟踪或意图-槽注释的多方对话语料库，尽管存在重要的差异。请考虑表 [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering") (来自我们的新语料库，详见第 [2](#S2 "2 Dataset and Tasks
    ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and
    Prompt Engineering") 节)。在回合 1 中，我们可以识别出用户 1 (U1) 想知道他们的预约时间。在社交机器人有时间回答之前，用户 2
    (U2) 在回合 2 中回答了。这显然不会发生在双向互动中，但这种理解对于自然系统行为至关重要。SDS 必须确定它不应该重复回答问题，因此必须收集数据以学习这一点。还存在其他主要差异。例如，当前
    DST 语料库不包含“共享目标”的概念，Eshghi 和 Healey ([2016](#bib.bib14))。如果两个人走到咖啡馆柜台，咖啡师必须确定这两个人是分开（两个想要喝咖啡的个人），还是在一起（两个有共同目标想要喝咖啡的朋友），Keizer
    等人 ([2013](#bib.bib27))。这个事实会影响互动方式，对于两个个体来说，询问“你们是一起付款的吗”是不寻常的。共享目标通常可以通过明确的对话识别。例如，“我们在找洗手间”中的“我们”使用。类似于回答彼此的问题，人们也可能代表其他人提问。在我们的语料库中，有人说：“ARI，我陪伴的人感到被你吓到，他们想知道在哪里可以吃东西”。'
- en: 'In this paper, we present several contributions. (1) We collected a corpus
    of multi-party interactions between a social robot and patients with their companions
    in a hospital memory clinic. (2) This corpus was annotated for the standard “who
    says what to whom” tasks, but also for multi-party goal tracking and intent-slot
    recognition. We followed current DST annotation instructions, tweaked to enable
    annotation of multi-party phenomena (detailed in Section [2](#S2 "2 Dataset and
    Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
    and Prompt Engineering")). (3) We then evaluated Large Language Models (LLMs)
    on these two new tasks using our collected corpus. Models were pre-trained, fine-tuned,
    or prompt engineered where applicable (detailed in Section [3](#S3 "3 Experimental
    Procedure ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
    and Prompt Engineering")). It is not possible to collect enormous corpora from
    patients in a hospital, so models were evaluated in zero-shot and few-shot settings.
    We found that the GPT-3.5-turbo model significantly outperformed others on both
    tasks when given a ‘reasoning’ style prompt.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提出了几项贡献。（1）我们收集了医院记忆诊所中社交机器人与患者及其陪伴者之间的多方互动语料库。（2）该语料库进行了标准的“谁对谁说了什么”任务的注释，同时也进行了多方目标追踪和意图-槽位识别的注释。我们遵循了当前的DST注释说明，并进行了调整以便能够注释多方现象（详细见第[2](#S2
    "2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering")节）。（3）我们在这些两个新任务上评估了大型语言模型（LLMs），使用了我们收集的语料库。模型被预训练、微调或在适用的情况下进行提示工程（详细见第[3](#S3
    "3 Experimental Procedure ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering")节）。由于无法从医院中收集大量语料库，因此模型在零样本和少样本设置下进行了评估。我们发现，当给定“推理”风格提示时，GPT-3.5-turbo模型在这两个任务上显著优于其他模型。'
- en: 2 Dataset and Tasks
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集与任务
- en: For the initial data collection, we partnered with a hospital in Paris, France,
    and secured ethical approval as part of the EU SPRING project¹¹1[https://spring-h2020.eu/](https://spring-h2020.eu/).
    We then recorded, transcribed, translated (from French to English), anonymised,
    and annotated 29 multi-party conversations (774 turns). These MPCs were between
    patients of the memory clinic, their companion (usually a family member), and
    a social humanoid robot created by PAL Robotics called ARI Cooper et al. ([2020](#bib.bib12)).
    We hired a professional translator to avoid machine translation errors, and to
    enable faster experimentation as we are not French speakers. Future work based
    upon the findings in this paper will be evaluated in both English and French.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初步的数据收集，我们与法国巴黎的一家医院合作，并获得了伦理批准，作为欧盟SPRING项目的一部分¹¹1[https://spring-h2020.eu/](https://spring-h2020.eu/)。随后，我们记录、转录、翻译（从法语到英语）、匿名化，并注释了29个多方对话（774轮）。这些多方对话涉及记忆诊所的患者、他们的陪伴者（通常是家庭成员），以及PAL
    Robotics创造的名为ARI Cooper等的社交类人形机器人（[2020](#bib.bib12)）。我们雇用了专业翻译员，以避免机器翻译错误，并加快实验进程，因为我们不讲法语。基于本文发现的未来工作将会用英语和法语进行评估。
- en: 'We used a wizard-of-oz setup as this task is new, and we required this data
    to design a multi-party SDS for use in the hospital. A robot operator was therefore
    controlling what ARI said by selecting one of 31 response options (task-specific
    answers and some common responses like “yes”, “no”, “please”, “thank you”, and
    “I don’t know”). Following our previously published data collection design Addlesee
    et al. ([2023](#bib.bib5)), each participant was given one or two goals, and asked
    to converse with ARI to try to achieve their goal. Both participants were given
    the same goals in some cases to elicit dialogues containing ‘shared goal’ behaviour.
    In order to encourage lexical diversity, we provided pictograms to give each participant
    their goals. For example, if we told the patient that they want a latte, they
    would likely use the specific word “latte” Novikova et al. ([2016](#bib.bib35)),
    so we instead gave the participants pictograms as seen in the top-right of Figure
    [1](#S2.F1 "Figure 1 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs:
    Comparing Pre-training, Fine-tuning, and Prompt Engineering"). This worked as
    people didn’t just ask for coffee when given this image, some asked for hot chocolate
    or herbal tea instead.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了巫师设置（wizard-of-oz setup），因为这个任务是新的，我们需要这些数据来设计用于医院的多方对话系统。因此，一个机器人操作员控制
    ARI 说的话，选择 31 个响应选项中的一个（任务特定的回答和一些常见回应，如“是”，“不”，“请”，“谢谢”，“我不知道”）。按照我们之前发布的数据收集设计
    Addlesee 等人 ([2023](#bib.bib5))，每个参与者被赋予一个或两个目标，并要求与 ARI 对话以尝试实现其目标。在某些情况下，两个参与者被赋予相同的目标，以引发包含“共享目标”行为的对话。为了鼓励词汇多样性，我们提供了图标来向每个参与者传达他们的目标。例如，如果我们告诉患者他们想要拿铁，他们可能会使用特定的词汇“拿铁”
    Novikova 等人 ([2016](#bib.bib35))，因此我们改为向参与者提供图标，如图[1](#S2.F1 "Figure 1 ‣ 2 Dataset
    and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
    and Prompt Engineering")右上角所示。这有效，因为人们在看到这个图像时并不仅仅要求咖啡，有些人要求热巧克力或草药茶。'
- en: '![Refer to caption](img/c5538a1edcbf984adfc7c6f055be311e.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5538a1edcbf984adfc7c6f055be311e.png)'
- en: 'Figure 1: A sample of the pictograms used to represent user goals, given to
    patients and companions. These elicited dialogues without restricting vocabulary.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：用于表示用户目标的图标样本，提供给患者和陪伴者。这些图标引发了对话，而不限制词汇。
- en: 'In this paper, we evaluated each model on both multi-party goal tracking, and
    multi-party intent-slot recognition. These are two related, yet distinct tasks.
    If ARI asked the user “Are you hungry?”, and the user responded “yes”, then the
    intent of that turn is an affirmation, but the user’s goal is also established
    as wanting to eat. As explained in Section [1](#S1 "1 Introduction ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering"),
    standard DST annotation schemes are designed for dyadic interactions, which do
    not enable annotation of multi-party behaviours. Each turn is annotated with its
    intent and slot values where applicable, but goal annotations require both the
    goal and the user whose goal is being established. When a goal is detected in
    a dyadic interaction, no user information is needed as there is only a single
    user. In multi-party interactions, multiple users can have multiple active goals.
    These goals may be different, they may be shared (see Table [2](#S2.T2 "Table
    2 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering")), users may answer each other’s goals (see
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Multi-party Goal Tracking with LLMs:
    Comparing Pre-training, Fine-tuning, and Prompt Engineering")), and one user may
    provide another user’s goal, for example by saying “My wife would love a coffee”.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '在这篇论文中，我们对每个模型在多方目标跟踪和多方意图-槽位识别两个相关但不同的任务上进行了评估。如果 ARI 问用户“你饿了吗？”，用户回答“是”，那么这一轮的意图是肯定，但用户的目标也被确立为想要吃东西。如[1](#S1
    "1 Introduction ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering")节所述，标准的 DST 标注方案是为二人对话设计的，不适用于多方行为的标注。每轮对话会标注其意图和槽位值（如适用），但目标标注需要既标注目标，又标注目标的用户。当在二人对话中检测到目标时，不需要用户信息，因为只有一个用户。在多方对话中，多个用户可以有多个活跃目标。这些目标可能不同，也可能是共享的（见[2](#S2.T2
    "Table 2 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering")表），用户可能回应彼此的目标（见[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering")表），一个用户可能提供另一个用户的目标，例如说“我妻子想喝咖啡”。'
- en: '|  | User | Utterance | Intent-Slot Annotation | Goal Tracking Annotation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户 | 发言 | 意图-槽位标注 | 目标跟踪标注 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | U1: | Hello, we’d like a coffee. Where can we go? | greet() ; request(beverage(coffee))
    | G(U1+U2, drink(coffee)) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 1 | U1: | 你好，我们想要一杯咖啡。我们可以去哪里？ | greet() ; request(beverage(coffee)) | G(U1+U2,
    drink(coffee)) |'
- en: '| 2 | ARI: | You have to enter the building behind you. | inform(directions(cafe))
    | AG(U1+U2, drink(coffee)) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ARI: | 你们需要进入你们身后的那栋建筑。 | inform(directions(cafe)) | AG(U1+U2, drink(coffee))
    |'
- en: '| 3 | U2: | Ok, well thank you very much. | acknowledge(); thank() | CG(U1+U2,
    drink(coffee)) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 3 | U2: | 好的，非常感谢。 | acknowledge(); thank() | CG(U1+U2, drink(coffee)) |'
- en: 'Table 2: A corpus example displaying shared goals with both intent-slot and
    goal tracking annotations.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：展示了具有意图-槽位和目标跟踪注释的共享目标的语料库示例。
- en: 'An annotated extract from an MPC in our collected corpus can be found in Table
    [2](#S2.T2 "Table 2 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs:
    Comparing Pre-training, Fine-tuning, and Prompt Engineering"). In turn 1, U1 states
    that “we’d like a coffee”, indicating that U1 and their companion U2 would *both*
    like a coffee. This turn is annotated with two intents: greet (due to the “hello”),
    and request. This request intent has a slot value to indicate that the request
    is for a beverage – coffee. The goal tracking annotation signifies that a goal
    has been established in this turn with ‘G’. The goal is shared by ‘U1+U2’, and
    their goal is to drink a coffee. In turn 2, ARI responds informing both users
    where the café is, hence the inform intent annotation. The goal tracking annotation
    is the same as turn 1, but starts with ‘AG’ (for ‘answer-goal’) instead of simply
    ‘G’. This indicates that this goal has been answered, which is critical knowledge
    for the system to track which goals remain open. In this example, the goal is
    explicitly closed in turn 3, indicated by the corresponding ‘CG’ (close-goal)
    goal tracking annotation. Not all goals are explicitly closed by the user. A dialogue
    manager could decide to implicitly close an answered goal if the user does not
    reopen it within three turns, for example. We only annotate explicit goal closures,
    like the one in turn 3\. There are two intents annotated in both turns 1 and 3
    in Table [2](#S2.T2 "Table 2 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking
    with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering"), and
    multiple goal annotations can similarly exist, separated by a semicolon. For example,
    “I’m hungry but need the toilet first” simultaneously opens two goals. All of
    these annotations were completed using the ELAN tool Brugman et al. ([2004](#bib.bib11)),
    and then mapped into JSON for model training²²2Mapping code, annotated data, and
    training hyperparameters can be found here: [https://github.com/AddleseeHQ/mpgt-eval](https://github.com/AddleseeHQ/mpgt-eval)..'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '从我们收集的语料库中提取的带注释的MPC示例可以在表[2](#S2.T2 "Table 2 ‣ 2 Dataset and Tasks ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")中找到。在轮次1中，U1表明“我们想要一杯咖啡”，这表明U1及其伙伴U2*都*想要一杯咖啡。这一轮的注释包含两个意图：问候（由于“hello”），和请求。这个请求意图有一个槽值，表示请求的是饮料——咖啡。目标跟踪注释表示这一轮已经建立了一个目标，标记为‘G’。这个目标由‘U1+U2’共享，他们的目标是喝咖啡。在轮次2中，ARI回应并告知两位用户咖啡馆的位置，因此注释了inform意图。目标跟踪注释与轮次1相同，但以‘AG’（意为‘answer-goal’）开始，而不是单纯的‘G’。这表示该目标已经被回答，这是系统跟踪哪些目标仍然开放的重要信息。在这个例子中，目标在轮次3中被明确关闭，由相应的‘CG’（close-goal）目标跟踪注释表示。并非所有目标都由用户明确关闭。例如，假如用户在三轮内没有重新打开已回答的目标，对话管理器可以决定隐式关闭它。我们只对明确关闭的目标进行注释，例如轮次3中的目标。这两个意图在表[2](#S2.T2
    "Table 2 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering")的轮次1和3中都有注释，多个目标注释也可以存在，用分号隔开。例如，“我饿了，但首先需要上厕所”同时开启了两个目标。所有这些注释都使用ELAN工具Brugman等人（[2004](#bib.bib11)）完成，然后映射到JSON中用于模型训练²²2映射代码、注释数据和训练超参数可以在这里找到：[https://github.com/AddleseeHQ/mpgt-eval](https://github.com/AddleseeHQ/mpgt-eval)。'
- en: 'With these two sets of annotations, we can evaluate various LLMs on two tasks:
    (1) multi-party intent-slot recognition; and (2) multi-party goal tracking. It
    is not possible to collect vast quantities of interactions with patients in the
    hospital, so these models must be able to learn from a corpus of limited size.
    We therefore decided to mask annotations in a randomised window selected from
    each MPC, providing the model with the surrounding context and speaker labels.
    That is, a random number of turns was selected in each MPC, and then the annotations
    were replaced by a ‘[MASK]’ token. An example of this is shown in Table [3](#S2.T3
    "Table 3 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这两组注释，我们可以在两个任务上评估各种LLMs：（1）多方意图槽位识别；（2）多方目标跟踪。在医院中无法收集大量患者互动，因此这些模型必须能够从有限规模的语料库中学习。因此，我们决定在每个MPC中从随机选定的窗口中掩盖注释，为模型提供周围的上下文和说话者标签。也就是说，在每个MPC中随机选择了若干回合，然后将注释替换为‘[MASK]’标记。一个示例如表[3](#S2.T3
    "表3 ‣ 2 数据集和任务 ‣ 多方目标跟踪与LLMs：比较预训练、微调和提示工程")所示。
- en: 'As the corpus size is limited, the window selection could potentially heavily
    impact model performance. We therefore randomised the selected window three times
    for each conversation and train/test split, and these *exact same* windows were
    used to train and test each model. To clarify, all train/test splits and windows
    were randomised for multiple runs, but they were unchanged between each model.
    For example, run 1 with the 20/80 split in Section [4](#S4 "4 Results ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")
    for T5 contained the exact same test set, with the exact same window, as run 1
    with the 20/80 split for DialogLED. This holds true for both tasks. Each masked
    window was bookended with a ‘[start]’ and ‘[end]’ tag to help the models learn
    this task too Zhong et al. ([2022](#bib.bib51)). A shortened example from our
    corpus can be seen in Table [3](#S2.T3 "Table 3 ‣ 2 Dataset and Tasks ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语料库的规模有限，窗口选择可能会对模型性能产生重大影响。因此，我们对每个对话和训练/测试拆分随机选择窗口三次，并使用这些*完全相同*的窗口来训练和测试每个模型。为了澄清，所有训练/测试拆分和窗口都进行了多次运行的随机化，但在每个模型之间保持不变。例如，第1次运行的20/80拆分在第[4](#S4
    "4 结果 ‣ 多方目标跟踪与LLMs：比较预训练、微调和提示工程")节中，T5的测试集与第1次运行的20/80拆分对于DialogLED的测试集完全相同。两项任务均为此。每个掩码窗口都用‘[start]’和‘[end]’标签标记，以帮助模型学习这个任务（Zhong
    et al. ([2022](#bib.bib51))）。我们语料库中的一个缩短示例见表[3](#S2.T3 "表3 ‣ 2 数据集和任务 ‣ 多方目标跟踪与LLMs：比较预训练、微调和提示工程")。
- en: '|  | User | Masked Goal Tracking Utterance | Gold Annotation |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户 | 掩码目标跟踪发言 | 金标准注释 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | ARI: | Hello, my name is ARI. How can I help you? | - |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ARI: | 你好，我叫ARI。我能帮你什么？ | - |'
- en: '|  |  | [start] | - |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [start] | - |'
- en: '| 2 | U1: | My friend is intimidated by you, where can they eat? [MASK] | G(U2,
    eat()) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 2 | U1: | 我的朋友对你感到害怕，他们可以在哪里吃饭？[MASK] | G(U2, eat()) |'
- en: '| 3 | ARI: | There’s a cafeteria on the ground floor, near the courtyard. [MASK]
    | AG(U2, eat()) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ARI: | 一楼有一家咖啡厅，在庭院附近。[MASK] | AG(U2, eat()) |'
- en: '|  |  | [end] | - |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [end] | - |'
- en: '| 4 | U2: | My appointment is in room 17, where is it? G(U2, go-to(room_17))
    | - |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 4 | U2: | 我的预约在17号房间，那里在哪里？G(U2, go-to(room_17)) | - |'
- en: 'Table 3: A corpus example illustrating the goal tracking task. This process
    was the same for intent-slot recognition, with the corresponding annotations.
    Note that U1 asks U2’s question, and this is reflected in the annotation.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：一个说明目标跟踪任务的语料库示例。该过程与意图槽位识别相同，具有相应的注释。请注意，U1提问U2的问题，这在注释中有所体现。
- en: 3 Experimental Procedure
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验程序
- en: 'We evaluated three different models (each detailed below): T5 Raffel et al.
    ([2020](#bib.bib40)), DialogLM using LED (DialogLED) Zhong et al. ([2022](#bib.bib51)),
    and GPT-3.5-turbo³³3[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5).
    Each approach was evaluated in a zero-shot and few-shot setting, with various
    train/test splits. We could not provide more data to GPT-3.5-turbo due to context
    window size, but the train/test splits for T5 and DialogLED were: 0/100 (zero-shot),
    20/80, 50/50, and 80/20\. This allowed us to determine how each model learned
    to do these tasks when given more training examples. As described in Section [2](#S2
    "2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering"), we ran each experiment three times with
    randomised splits and windows, but these remained the same between-models to avoid
    few-shot problems such as recency bias Zhao et al. ([2021](#bib.bib50)). We trained
    all the T5-Large and DialogLED models on a machine containing a 16Gb NVIDIA GeForce
    RTX 3080 Ti GPU with 64Gb RAM and an Intel i9-12900HK processor.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了三种不同的模型（详细信息见下文）：T5 Raffel et al. ([2020](#bib.bib40))、使用 LED 的 DialogLM（DialogLED）Zhong
    et al. ([2022](#bib.bib51))，以及 GPT-3.5-turbo³³3[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)。每种方法都在零样本和少样本设置下进行评估，并使用了不同的训练/测试拆分。由于上下文窗口大小限制，我们无法向
    GPT-3.5-turbo 提供更多数据，但 T5 和 DialogLED 的训练/测试拆分为：0/100（零样本），20/80，50/50 和 80/20。这使我们能够确定在提供更多训练示例时，每个模型如何学习执行这些任务。如第
    [2](#S2 "2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering")节所述，我们对每个实验进行了三次运行，采用随机拆分和窗口，但这些拆分在模型之间保持一致，以避免像
    Zhao et al. ([2021](#bib.bib50)) 所述的少样本问题，如近期偏差。我们在配备 16Gb NVIDIA GeForce RTX
    3080 Ti GPU、64Gb RAM 和 Intel i9-12900HK 处理器的机器上训练了所有 T5-Large 和 DialogLED 模型。'
- en: 3.1 T5-Large
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 T5-Large
- en: Older GPT models (GPT-3 and below) are pre-trained with the next token prediction
    objective on huge corpora Radford et al. ([2019](#bib.bib39)); Brown et al. ([2020](#bib.bib10)),
    an inherently directional task. The creators of T5 added two more objectives and
    give it the goal of minimising the combined loss function Raffel et al. ([2020](#bib.bib40))
    across all three tasks. The two additional tasks were de-shuffling, and BERT-style
    de-masking Devlin et al. ([2018](#bib.bib13)). This latter pre-training task involves
    ‘corrupting’ tokens in the original text, which T5 must then predict. Importantly,
    this enabled T5 to work bidirectionally, becoming particularly good at using the
    surrounding context to predict tokens in corrupted sentences. This is not dissimilar
    to our task, in which the model must learn to use the surrounding MPC turns to
    predict the annotations that are masked. T5 also achieves state-of-the-art results
    on related tasks like Lee et al. ([2021](#bib.bib28)); Marselino Andreas et al.
    ([2022](#bib.bib31)), albeit, fine-tuned on larger datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 较早的 GPT 模型（GPT-3 及以下）在巨大的语料库上进行预训练，目标是预测下一个标记 Radford et al. ([2019](#bib.bib39));
    Brown et al. ([2020](#bib.bib10))，这是一项固有的方向性任务。T5 的创建者增加了两个额外的目标，并将其目标设定为最小化三项任务的综合损失函数
    Raffel et al. ([2020](#bib.bib40))。这两个额外的任务是去混洗和 BERT 风格的去掩码 Devlin et al. ([2018](#bib.bib13))。后者的预训练任务涉及‘破坏’原文中的标记，T5
    然后必须预测这些标记。重要的是，这使得 T5 能够双向工作，特别擅长利用周围的上下文来预测被破坏句子中的标记。这与我们的任务类似，其中模型必须学会利用周围的
    MPC 回合来预测被掩盖的注释。T5 在相关任务上也取得了最先进的结果，如 Lee et al. ([2021](#bib.bib28)); Marselino
    Andreas et al. ([2022](#bib.bib31))，尽管在更大的数据集上进行了微调。
- en: We used T5-Large in both a zero-shot setting, and fine-tuned with various train/test
    splits. T5 allows fine-tuning with a given named task like ‘answer the question’,
    or ‘translate from French to German’. We used ‘predict goals’ and ‘predict intent-slots’
    for goal tracking and intent-slot recognition, respectively, giving the same task
    names as input during testing. As the corpus is very small, there was no model
    performance boost beyond 3 epochs, which was expected Mueller et al. ([2022](#bib.bib33)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在零样本设置下使用了 T5-Large，并通过各种训练/测试拆分进行了微调。T5 允许通过给定的任务名称进行微调，如‘回答问题’或‘从法语翻译成德语’。我们使用‘预测目标’和‘预测意图槽’用于目标追踪和意图槽识别，分别在测试期间输入相同的任务名称。由于语料库非常小，因此模型性能在超过
    3 个时期后没有提升，这一点是预期中的 Mueller et al. ([2022](#bib.bib33))。
- en: 3.2 DialogLM using LED (DialogLED)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用 LED 的 DialogLM（DialogLED）
- en: 'MPCs reveal unique new communication challenges Addlesee et al. ([2023](#bib.bib5)),
    as detailed in Section [1](#S1 "1 Introduction ‣ Multi-party Goal Tracking with
    LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering"), so some LLMs
    have been developed specifically for the multi-party domain Hu et al. ([2019](#bib.bib26));
    Gu et al. ([2021](#bib.bib21), [2022a](#bib.bib19)). Microsoft published DialogLM
    Zhong et al. ([2022](#bib.bib51)), a pre-trained LLM based upon UniLMv2 Bao et al.
    ([2020](#bib.bib7)), but specifically designed for multi-party tasks. Alongside
    the base model, they released two variations: DialogLM-sparse for long dialogues
    over 5,120 words, and DialogLM using LED (DialogLED) which outperformed the others.
    DialogLED builds on Longform-Encoder-Decoder (LED) Beltagy et al. ([2020](#bib.bib8)),
    an attention mechanism that scales linearly with sequence length. Transformer-based
    models typically scale quadratically with the sequence length, restricting their
    ability to process long dialogues.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'MPC揭示了独特的新沟通挑战，Addlesee等人（[2023](#bib.bib5)）在第[1](#S1 "1 Introduction ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")节中详细描述了，因此一些LLM专门为多方领域开发，Hu等人（[2019](#bib.bib26)）；Gu等人（[2021](#bib.bib21),
    [2022a](#bib.bib19)）。微软发布了DialogLM Zhong等人（[2022](#bib.bib51)），这是一个基于UniLMv2 Bao等人（[2020](#bib.bib7)）的预训练LLM，但专门为多方任务设计。除了基础模型外，他们还发布了两个变体：用于超过5120字长对话的DialogLM-sparse，以及使用LED（DialogLED）的DialogLM，后者的表现优于其他模型。DialogLED建立在Longform-Encoder-Decoder（LED）Beltagy等人（[2020](#bib.bib8)）的基础上，这是一个随序列长度线性扩展的注意力机制。基于Transformer的模型通常随序列长度呈二次扩展，限制了它们处理长对话的能力。'
- en: 'DialogLED was pre-trained on five objectives designed specifically for MPCs,
    and the model’s goal was to minimise the combined loss of all of these tasks.
    Their state-of-the-art results showed that their pre-training tasks did encourage
    the LLM to ‘understand’ multi-party interactions. The five tasks were: (1) speaker
    masking, the model has to predict who spoke; (2) turn splitting, the model has
    to recognise when two utterances are likely the same turn; (3) turn merging, the
    opposite of (2), where the model has to recognise when the turns were likely separate;
    (4) text infilling, the model has to predict masked tokens within the turn; and
    (5) turn permutation, the model has to correctly re-order jumbled turns.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DialogLED在专为MPC设计的五个目标上进行了预训练，模型的目标是最小化这些任务的综合损失。他们的先进结果表明，预训练任务确实促进了LLM对多方互动的‘理解’。这五个任务是：（1）说话者掩码，模型需要预测谁说了话；（2）回合拆分，模型需要识别两个话语是否可能属于同一个回合；（3）回合合并，与（2）相反，模型需要识别回合是否可能是分开的；（4）文本填充，模型需要预测回合中被掩盖的标记；（5）回合排列，模型需要正确地重新排序混乱的回合。
- en: 'We cloned their repository⁴⁴4[https://github.com/microsoft/DialogLM](https://github.com/microsoft/DialogLM)
    and added two new tasks: (6) goal masking, the model has to predict goal tracking
    annotations; and (7) intent-slot masking, the model has to predict intent-slot
    annotations. In the zero-shot setting, we simply ran the test set through base
    DialogLED. We then ran their, now modified, code to run our few-shot evaluations
    three times for each data split.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们克隆了他们的代码库⁴⁴4[https://github.com/microsoft/DialogLM](https://github.com/microsoft/DialogLM)并添加了两个新任务：（6）目标掩码，模型需要预测目标跟踪注释；（7）意图槽掩码，模型需要预测意图槽注释。在零-shot设置中，我们简单地用基础的DialogLED运行测试集。然后，我们运行了他们现在修改后的代码，对每个数据拆分进行了三次少样本评估。
- en: 3.3 GPT-3.5-turbo
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 GPT-3.5-turbo
- en: Larger LLMs are not inherently better at following a user’s intent Ouyang et al.
    ([2022](#bib.bib36)) as they have no incentive to help the user achieve their
    goal, only to generate realistic looking outputs. This leads to significant problems,
    including the generation of false, biased, and potentially harmful responses.
    GPT-3 was therefore fine-tuned on prompts with human-feedback to create InstructGPT
    Ouyang et al. ([2022](#bib.bib36)). OpenAI later followed this same approach to
    create the now famous ChatGPT family of models. At the time of writing, GPT-4
    is the most powerful of these models, but it is currently in a waiting list phase.
    OpenAI recommends their GPT-3.5-turbo model while waiting as the next best option.
    We therefore decided to evaluate this model on the same two tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的LLMs并不一定更擅长遵循用户的意图 Ouyang et al. ([2022](#bib.bib36))，因为它们没有激励去帮助用户实现目标，只是生成看起来真实的输出。这会导致重大问题，包括生成虚假的、偏见的以及潜在有害的响应。因此，GPT-3在经过人类反馈的提示下进行了微调，创建了InstructGPT
    Ouyang et al. ([2022](#bib.bib36))。OpenAI随后采用了相同的方法创建了现在著名的ChatGPT模型系列。在撰写本文时，GPT-4是这些模型中最强大的，但它目前处于等待列表阶段。OpenAI建议在等待期间使用他们的GPT-3.5-turbo模型作为下一个最佳选择。因此，我们决定在相同的两个任务上评估这个模型。
- en: Unlike T5 or DialogLED, there is no way to fine-tune your own version of GPT-3.5-turbo,
    or to edit their pre-training steps. People instead mould the model’s behaviour
    through prompt-engineering Lester et al. ([2021](#bib.bib29)); Wei et al. ([2022](#bib.bib46));
    Weng ([2023](#bib.bib47)). The newer GPT models allow developers to provide huge
    contexts, called prompts, containing instructions for the model to follow. GPT-3.5-turbo
    allows prompts of up to 4,096 tokens. Although these models have only exploded
    in popularity recently, there are many suggested prompt ‘styles’ suggested online
    by conversation designers who are implementing these models in the real-world.
    We have analysed this space and devised six prompt styles for the two tasks. In
    the zero-shot setting, only the prompt and the masked MPC is provided to the model.
    In the few-shot setting, we additionally provide the model with 7% of the corpus
    as examples. This is crucial to highlight. T5 and DialogLED were trained on 20%
    of the corpus, 50% of the corpus, and finally 80% of the corpus. GPT-3.5-turbo’s
    maximum context size can only fit 7% of the corpus, less than the other models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与T5或DialogLED不同，没有办法微调你自己的GPT-3.5-turbo版本，或者编辑它们的预训练步骤。人们通过提示工程来塑造模型的行为 Lester
    et al. ([2021](#bib.bib29)); Wei et al. ([2022](#bib.bib46)); Weng ([2023](#bib.bib47))。更新的GPT模型允许开发人员提供包含指令的庞大上下文，即提示。GPT-3.5-turbo允许最多4,096个标记的提示。尽管这些模型最近才爆炸性地流行，但在线有许多对话设计师提出了建议的提示‘样式’，他们在实际应用这些模型。我们分析了这一领域，并为这两个任务设计了六种提示样式。在零-shot设置中，只有提示和被屏蔽的MPC提供给模型。在few-shot设置中，我们额外提供了7%的语料作为示例。这一点至关重要。T5和DialogLED在20%、50%和最终80%的语料上进行了训练。GPT-3.5-turbo的最大上下文大小只能容纳7%的语料，低于其他模型。
- en: 'The prompt styles we used were the following (the actual prompts are included
    in Appendix [A](#A1 "Appendix A Full GPT-3.5-turbo Prompts ‣ Multi-party Goal
    Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的提示样式如下（实际提示包含在附录 [A](#A1 "附录 A 完整的 GPT-3.5-turbo 提示 ‣ 多方目标跟踪与LLMs：比较预训练、微调和提示工程")）：
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Basic: This is our baseline prompt. It very simply tells the model what it
    is going to get as input, and what we want as output. It contains no further special
    instructions.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础：这是我们的基线提示。它非常简单地告诉模型输入的内容是什么，以及我们希望得到的输出是什么。它不包含任何进一步的特殊指令。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specific: GPT practitioners report that when prompts are more detailed and
    specific, performance is boosted Ye et al. ([2023](#bib.bib49)).'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体：GPT从业者报告称，当提示更详细和具体时，性能会有所提升 Ye et al. ([2023](#bib.bib49))。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Annotation: For annotation tasks, we would give fellow humans annotation instructions.
    In this prompt, we provide the model with annotation instructions.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注释：对于注释任务，我们会给人类同事提供注释指令。在这个提示中，我们为模型提供注释指令。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Story: This model was pre-trained on a very large quantity of data, including
    novels, film scripts, journalistic content, etc… It may be possible that by phrasing
    the prompt like a story, performance may be boosted due to its likeness to its
    training data.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 故事：这个模型在大量的数据上进行了预训练，包括小说、电影剧本、新闻内容等……通过将提示措辞为故事，可能会因为与训练数据的相似性而提高性能。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Role-play: Similar to the story prompt, it is reported that these models are
    very good at role-playing⁵⁵5[https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts).
    People ask ChatGPT to pretend to be a therapist, a lawyer, or even alter-egos
    that have no safety limitations Taylor ([2023](#bib.bib42)). We tell GPT-3.5-turbo
    that it is a ‘helpful assistant listening to a conversation between two people
    and a social robot called ARI’.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色扮演：类似于故事提示，报告称这些模型在角色扮演方面非常出色⁵⁵5[https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)。人们要求
    ChatGPT 假装成治疗师、律师，甚至没有安全限制的替身 Taylor ([2023](#bib.bib42))。我们告诉 GPT-3.5-turbo 它是一个“帮助型助手，倾听两个人的对话和一个叫
    ARI 的社交机器人”。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning: Finally, recent work suggests that these models improve in performance
    if you explain the reasoning for desired outputs Fu et al. ([2022](#bib.bib18)).
    We therefore added one fictitious turn to this prompt, and explained the reasoning
    behind its annotation.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理由：最近的研究表明，如果你解释所期望输出的理由，这些模型的表现会有所提高 Fu et al. ([2022](#bib.bib18))。因此，我们在这个提示中添加了一个虚构的回合，并解释了其注释的理由。
- en: 4 Results
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: 'We evaluated T5, DialogLED, and GPT-3.5-turbo as described in Section [3](#S3
    "3 Experimental Procedure ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering") on multi-party goal tracking, and multi-party
    intent-slot recognition. Outputs were annotated as either ‘exact’, ‘correct’,
    or ‘partial’ to distinguish each model’s performance beyond simple accuracy. Exact
    matches were strictly annotated, but slight differences are allowed if the annotation
    meaning remains unchanged. For example: ‘G(U1, go-to(lift))’ and ‘G(U1, go-to(lifts))’
    (note the plural ‘lifts’). Outputs were marked as exact if every [MASK] in the
    MPC was exact, and marked as correct if every [MASK] was more broadly accurate.
    For example, if the annotation contained ‘drink(coffee)’ and the model output
    ‘drink(hot_drink)’, we considered this correct. The output was marked as partially
    correct if at least 60% of the [MASK] tags were correctly annotated. This latter
    metric allows us to distinguish between models that generate nonsense, and those
    that roughly grasp the task. Our inter-annotator agreements were 0.765 and 0.771
    for goal tracking and intent-slot recognition, respectively. These are less than
    0.8, and this was due to the broad definition of ‘correct’. We plan to design
    automatic metrics for our future work (see Section [5](#S5 "5 Conclusion and Future
    Work ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
    and Prompt Engineering")).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了 T5、DialogLED 和 GPT-3.5-turbo，如第 [3](#S3 "3 实验程序 ‣ 多方目标追踪与LLMs: 比较预训练、微调和提示工程")节所述，涉及多方目标追踪和多方意图槽识别。输出被标注为“精确”、“正确”或“部分”，以区分每个模型的表现超越简单准确度。精确匹配被严格标注，但如果注释含义保持不变，则允许轻微差异。例如：“G(U1,
    go-to(lift))” 和 “G(U1, go-to(lifts))”（注意复数形式“lifts”）。如果 MPC 中的每个 [MASK] 都是精确的，则标记为精确；如果每个
    [MASK] 更广泛地准确，则标记为正确。例如，如果注释包含“drink(coffee)”而模型输出为“drink(hot_drink)”，我们认为这是正确的。如果至少
    60% 的 [MASK] 标签被正确标注，则输出标记为部分正确。这个指标使我们能够区分生成无意义内容的模型和大致理解任务的模型。我们的注释者间一致性分别为
    0.765 和 0.771，分别用于目标追踪和意图槽识别。这些值低于 0.8，这是由于“正确”定义较宽泛。我们计划为未来的工作设计自动化指标（见第 [5](#S5
    "5 结论和未来工作 ‣ 多方目标追踪与LLMs: 比较预训练、微调和提示工程")节）。'
- en: 4.1 MPC Goal Tracking Results
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 MPC 目标追踪结果
- en: 'The goal tracking results can be found in Table [4](#S4.T4 "Table 4 ‣ 4.1 MPC
    Goal Tracking Results ‣ 4 Results ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering"). An ANOVA test Fisher ([1992](#bib.bib16))
    indicated that there was an overall significant difference between the model’s
    results. We therefore ran a Tukey HSD test Tukey ([1949](#bib.bib45)) that showed
    that the GPT-3.5-turbo model in the few-shot setting did significantly outperform
    all the other models.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '目标追踪结果见表格 [4](#S4.T4 "表 4 ‣ 4.1 MPC 目标追踪结果 ‣ 4 结果 ‣ 多方目标追踪与LLMs: 比较预训练、微调和提示工程")。ANOVA
    测试 Fisher ([1992](#bib.bib16)) 表明模型结果之间存在整体显著差异。因此，我们进行了 Tukey HSD 测试 Tukey ([1949](#bib.bib45))，结果显示，在少样本设置中，GPT-3.5-turbo
    模型明显优于其他所有模型。'
- en: '| Model | train/test % | Prompt Style | Exact % | Correct % | Partial % |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练/测试 % | 提示风格 | 精确 % | 正确 % | 部分 % |'
- en: '| T5 | 0/100 | - | 0 | 0 | 0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 0/100 | - | 0 | 0 | 0 |'
- en: '| T5 | 20/80 | - | 0 $\pm$ 0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 20/80 | - | 0 $\pm$ 0 |'
- en: '| T5 | 50/50 | - | 0 $\pm$ 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 50/50 | - | 0 $\pm$ 0 |'
- en: '| T5 | 80/20 | - | 0 $\pm$ 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 80/20 | - | 0 $\pm$ 0 |'
- en: '| DialogLED | 0/100 | - | 0 | 0 | 0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 0/100 | - | 0 | 0 | 0 |'
- en: '| DialogLED | 20/80 | - | 0 $\pm$ 1.45 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 20/80 | - | 0 $\pm$ 1.45 |'
- en: '| DialogLED | 50/50 | - | 0 $\pm$ 0.63 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 50/50 | - | 0 $\pm$ 0.63 |'
- en: '| DialogLED | 80/20 | - | 0 $\pm$ 11.55 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 80/20 | - | 0 $\pm$ 11.55 |'
- en: '| GPT 3.5-turbo | 0/100 | Basic | 0 | 3.45 | 31.03 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 基本 | 0 | 3.45 | 31.03 |'
- en: '| GPT 3.5-turbo | 0/100 | Specific | 0 | 3.45 | 24.14 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 特定 | 0 | 3.45 | 24.14 |'
- en: '| GPT 3.5-turbo | 0/100 | Annotation | 0 | 6.90 | 44.83 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 注释 | 0 | 6.90 | 44.83 |'
- en: '| GPT 3.5-turbo | 0/100 | Story | 0 | 0 | 0 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 故事 | 0 | 0 | 0 |'
- en: '| GPT 3.5-turbo | 0/100 | Role-play | 0 | 0 | 6.90 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 角色扮演 | 0 | 0 | 6.90 |'
- en: '| GPT 3.5-turbo | 0/100 | Reasoning | 3.45 | 34.48 | 79.31 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 推理 | 3.45 | 34.48 | 79.31 |'
- en: '| GPT 3.5-turbo | 7/80* | Basic | 11.59 $\pm$ 6.64 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 基本 | 11.59 $\pm$ 6.64 |'
- en: '| GPT 3.5-turbo | 7/80* | Specific | 20.29 $\pm$ 2.90 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 特定 | 20.29 $\pm$ 2.90 |'
- en: '| GPT 3.5-turbo | 7/80* | Annotation | 14.49 $\pm$ 4.35 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 注释 | 14.49 $\pm$ 4.35 |'
- en: '| GPT 3.5-turbo | 7/80* | Story | 17.39 $\pm$ 4.35 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 故事 | 17.39 $\pm$ 4.35 |'
- en: '| GPT 3.5-turbo | 7/80* | Role-play | 18.84 $\pm$ 5.22 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 角色扮演 | 18.84 $\pm$ 5.22 |'
- en: '| GPT 3.5-turbo | 7/80* | Reasoning | 27.54 $\pm$ 5.80 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 推理 | 27.54 $\pm$ 5.80 |'
- en: 'Table 4: The final multi-party goal tracking results for each model in both
    the zero- and few-shot settings. *We could not fit more than 7% of the training
    examples in GPT-3.5-turbo’s context window. We therefore used fewer examples than
    with T5 and DialogLED. The same 80% test sets were still used to enable model
    comparison.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：每个模型在零样本和少样本设置下的最终多方目标跟踪结果。*我们无法在GPT-3.5-turbo的上下文窗口中适配超过7%的训练示例。因此，我们使用的示例比T5和DialogLED少。仍然使用相同的80%测试集以进行模型比较。*
- en: Firstly, the T5-Large model performed poorly, even when it was trained on 80%
    of our corpus. Upon further analysis, it generated complete nonsense in the zero-shot
    setting, but did start to generate strings that looked reasonable with only 20%
    of the data. Given the 50/50 train/test split, T5 consistently replaced the [MASK]
    tokens, but did still hallucinate turns. When given 80% of the data as training
    data, the T5 model preserved the original dialogue, and replaced the [MASK] tokens
    with goal annotations, they were just all completely wrong. This steady improvement
    as we increased the amount of training data suggests that T5 could be a viable
    option for similar tasks, just not where data is limited (such as our hospital
    use case).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，T5-Large模型表现不佳，即使在80%的语料库上进行训练。进一步分析显示，在零样本设置中生成了完全无意义的内容，但仅使用20%的数据时，生成了看起来合理的字符串。鉴于50/50的训练/测试拆分，T5一致地替换了[MASK]标记，但仍然会产生虚假对话。当提供80%的数据作为训练数据时，T5模型保留了原始对话，并用目标注释替换了[MASK]标记，只是这些都完全错误。随着训练数据量的增加，这种稳定的改进表明，T5可能是类似任务的可行选项，只是在数据有限的情况下（例如我们的医院使用案例）。
- en: The DialogLED model also generated nonsense in the zero-shot setting, but very
    quickly learned the task. Even with just 20% of the data used for training, DialogLED
    reliably preserved the original dialogue and replaced the [MASK] tokens with goal
    annotations. Most of the annotations were incorrect, for example ‘G(U2, eat(ticket))’,
    but DialogLED did correctly detect some goals opening, being answered, and being
    closed correctly, achieving a non-zero partial score. Given more training data,
    DialogLED did begin to use the surrounding contextual dialogue turns more accurately,
    but almost every result contained an incorrect prediction. This was often the
    mis-detection of shared goals, or closing goals early. Like T5, DialogLED would
    need a larger training set to accurately complete this task. This model learned
    the task quickly, so may need fewer examples.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DialogLED模型在零样本设置中也生成了无意义的内容，但很快学会了任务。即使仅用20%的数据进行训练，DialogLED也可靠地保留了原始对话，并用目标注释替换了[MASK]标记。大多数注释都是错误的，例如‘G(U2,
    eat(ticket))’，但DialogLED确实正确地检测到了一些目标的开启、回答和正确关闭，获得了非零的部分得分。提供更多的训练数据后，DialogLED开始更准确地使用周围的上下文对话，但几乎每个结果都包含了错误的预测。这通常是对共享目标的错误检测，或者过早关闭目标。像T5一样，DialogLED需要更大的训练集来准确完成这项任务。该模型学习任务的速度较快，因此可能需要更少的示例。
- en: In the zero-shot setting, GPT-3.5-turbo roughly ‘understood’ the task, generating
    many partially correct outputs. With all the prompt styles, it did frequently
    reformat the dialogue. This was particularly true when using the roleplay prompt,
    it would output all the goals per interlocutor, for example, rather than per turn.
    The worst zero-shot GPT-3.5-turbo prompt was the ‘story’ style, not even generating
    one partially correct output. This was due to its increased hallucination. The
    story prompt noticeably produced more fictitious turns, and also rephrased and
    removed turns in the original dialogue. We believe this is likely because a story
    scenario is naturally a fictitious topic. The ‘reasoning’ style prompt performed
    remarkably well, generating five times more correct outputs than the second-best
    prompt style, and generating 79.31% partially correct outputs, showing that it
    can grasp the concept of the task. The reasoning prompt commonly mis-identified
    shared goals, unfortunately.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本设置中，GPT-3.5-turbo大致"理解"了任务，生成了许多部分正确的输出。所有提示风格下，它经常重新格式化对话。特别是在使用角色扮演提示时，它会输出每个对话者的所有目标，例如，而不是每轮的目标。最差的零样本GPT-3.5-turbo提示是'
    story '风格，甚至没有生成一个部分正确的输出。这是由于其增加的幻觉。故事提示明显产生了更多虚构的轮次，并且还重述和删除了原始对话中的轮次。我们认为这可能是因为故事情境自然是一个虚构的话题。"reasoning"
    风格提示表现非常好，生成了比第二好提示风格多五倍的正确输出，并生成了79.31%的部分正确输出，显示了它能够理解任务的概念。不幸的是，reasoning 提示常常误识别共享目标。
- en: 'In the few-shot setting, GPT-3.5-turbo’s results improved significantly compared
    to every other approach. We would like to highlight again that each run’s example
    prompts provided to the model were exactly the same for each prompt style. Performance
    differences were only due to the given prompt style. The ‘reasoning’ prompt once
    again outperformed the others across all metrics, generating correct outputs 62.32%
    of the time, and partially correct 94.20% of the time. In our future work (see
    Section [5](#S5 "5 Conclusion and Future Work ‣ Multi-party Goal Tracking with
    LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")), we plan
    to utilise this prompt style’s impressive performance on limited data. The ‘story’
    prompt was the only style to successfully attribute goals to other speakers, as
    in Table [3](#S2.T3 "Table 3 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking
    with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering"), but
    it still suffered from increased hallucination, which is not appropriate in a
    safety-critical setting. We suspect that the other prompt styles failed to do
    this because of the rarity of this phenomenon in our corpus. We are eliciting
    more of these in ongoing experiments with a deployed system, not wizard-of-oz
    Addlesee et al. ([2023](#bib.bib5)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '在少样本设置中，GPT-3.5-turbo的结果相比其他方法显著提高。我们再次强调，每次运行中提供给模型的示例提示是完全相同的，只是提示风格不同。性能差异仅由于给定的提示风格。"reasoning"
    提示在所有指标上再次优于其他提示，正确输出率为62.32%，部分正确输出率为94.20%。在我们未来的工作中（见第 [5](#S5 "5 Conclusion
    and Future Work ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering") 节），我们计划利用这一提示风格在有限数据上的出色表现。''story'' 提示是唯一一个成功将目标归因于其他说话者的风格，如表
    [3](#S2.T3 "Table 3 ‣ 2 Dataset and Tasks ‣ Multi-party Goal Tracking with LLMs:
    Comparing Pre-training, Fine-tuning, and Prompt Engineering") 所示，但它仍然存在增加的幻觉问题，这在安全关键环境中是不适宜的。我们怀疑其他提示风格未能做到这一点是因为在我们的语料库中这一现象的稀有性。我们正在通过与部署系统的持续实验来引出更多这些情况，而不是巫师奥兹Addlesee等人（[2023](#bib.bib5)）。'
- en: 4.2 MPC Intent-slot Recognition Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 MPC 意图槽识别结果
- en: 'The results for each model on the intent-slot recognition task can be found
    in Table [5](#S4.T5 "Table 5 ‣ 4.2 MPC Intent-slot Recognition Results ‣ 4 Results
    ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and
    Prompt Engineering"). As with the goal tracking results, an ANOVA test Fisher
    ([1992](#bib.bib16)) indicated that there was an overall significant difference
    between our model’s results. We therefore ran a Tukey HSD test Tukey ([1949](#bib.bib45))
    that showed that the GPT-3.5-turbo model in the few-shot setting significantly
    outperformed all the other models.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '每个模型在意图槽识别任务上的结果可以在表 [5](#S4.T5 "Table 5 ‣ 4.2 MPC Intent-slot Recognition
    Results ‣ 4 Results ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training,
    Fine-tuning, and Prompt Engineering") 中找到。与目标追踪结果一样，ANOVA测试Fisher（[1992](#bib.bib16)）表明我们的模型结果之间存在总体显著差异。因此，我们进行了Tukey
    HSD测试Tukey（[1949](#bib.bib45)），结果显示GPT-3.5-turbo模型在少样本设置中显著优于所有其他模型。'
- en: '| Model | train/test % | Prompt Style | Exact % | Correct % | Partial % |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练/测试 % | 提示风格 | 精确 % | 正确 % | 部分正确 % |'
- en: '| T5 | 0/100 | - | 0 | 0 | 0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 0/100 | - | 0 | 0 | 0 |'
- en: '| T5 | 20/80 | - | 0 $\pm$ 0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 20/80 | - | 0 $\pm$ 0 |'
- en: '| T5 | 50/50 | - | 0 $\pm$ 0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 50/50 | - | 0 $\pm$ 0 |'
- en: '| T5 | 80/20 | - | 0 $\pm$ 0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| T5 | 80/20 | - | 0 $\pm$ 0 |'
- en: '| DialogLED | 0/100 | - | 0 | 0 | 0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 0/100 | - | 0 | 0 | 0 |'
- en: '| DialogLED | 20/80 | - | 0 $\pm$ 2.90 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 20/80 | - | 0 $\pm$ 2.90 |'
- en: '| DialogLED | 50/50 | - | 0 $\pm$ 10.38 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 50/50 | - | 0 $\pm$ 10.38 |'
- en: '| DialogLED | 80/20 | - | 0 $\pm$ 6.67 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| DialogLED | 80/20 | - | 0 $\pm$ 6.67 |'
- en: '| GPT 3.5-turbo | 0/100 | Basic | 0 | 3.45 | 51.72 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 基本 | 0 | 3.45 | 51.72 |'
- en: '| GPT 3.5-turbo | 0/100 | Specific | 0 | 0 | 13.79 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 具体 | 0 | 0 | 13.79 |'
- en: '| GPT 3.5-turbo | 0/100 | Annotation | 0 | 3.45 | 20.69 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 注释 | 0 | 3.45 | 20.69 |'
- en: '| GPT 3.5-turbo | 0/100 | Story | 0 | 0 | 24.14 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 故事 | 0 | 0 | 24.14 |'
- en: '| GPT 3.5-turbo | 0/100 | Role-play | 0 | 0 | 20.69 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 角色扮演 | 0 | 0 | 20.69 |'
- en: '| GPT 3.5-turbo | 0/100 | Reasoning | 0 | 27.59 | 82.76 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 0/100 | 推理 | 0 | 27.59 | 82.76 |'
- en: '| GPT 3.5-turbo | 7/80* | Basic | 17.39 $\pm$ 2.90 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 基本 | 17.39 $\pm$ 2.90 |'
- en: '| GPT 3.5-turbo | 7/80* | Specific | 27.54 $\pm$ 1.45 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 具体 | 27.54 $\pm$ 1.45 |'
- en: '| GPT 3.5-turbo | 7/80* | Annotation | 18.84 $\pm$ 4.35 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 注释 | 18.84 $\pm$ 4.35 |'
- en: '| GPT 3.5-turbo | 7/80* | Story | 26.09 $\pm$ 3.83 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 故事 | 26.09 $\pm$ 3.83 |'
- en: '| GPT 3.5-turbo | 7/80* | Role-play | 20.29 $\pm$ 1.45 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 角色扮演 | 20.29 $\pm$ 1.45 |'
- en: '| GPT 3.5-turbo | 7/80* | Reasoning | 37.68 $\pm$ 0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5-turbo | 7/80* | 推理 | 37.68 $\pm$ 0 |'
- en: 'Table 5: The final multi-party intent-slot recognition results for each model
    in both the zero- and few-shot settings. *We could not fit more than 7% of the
    training examples in GPT-3.5-turbo’s context window. We therefore used fewer examples
    than with T5 and DialogLED. The same 80% test sets were still used to enable model
    comparison.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：每个模型在零样本和少样本设置下的最终多方意图-槽位识别结果。*我们无法在 GPT-3.5-turbo 的上下文窗口中放入超过 7% 的训练样本。因此，我们使用的样本数量少于
    T5 和 DialogLED。仍然使用相同的 80% 测试集以便于模型比较。
- en: As intent-slot annotations are well-established, T5 and DialogLED both started
    generating sensible-*looking* outputs with only a few training examples. The T5
    outputs were all incorrect again, however. DialogLED consistently improved as
    it was trained on progressively more data, annotating almost half of the MPCs
    partially correctly, and beginning to accurately annotate full MPCs. Given a larger
    corpus, we expect that DialogLED could potentially generate competitive results,
    but this is not the case for T5 in this setting with limited data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于意图-槽位注释已经较为成熟，T5 和 DialogLED 在仅有少量训练样本的情况下开始生成看起来合理的输出。然而，T5 的输出再次全都不正确。DialogLED
    在逐步增加数据的训练中持续改进，几乎对一半的MPCs进行了部分正确的标注，并开始准确标注完整的MPCs。鉴于更大的语料库，我们预计 DialogLED 可能会生成具有竞争力的结果，但在这个数据有限的设置中，T5
    并非如此。
- en: 'GPT-3.5-turbo in the zero-shot setting also achieved higher partial scores,
    compared to the goal tracking results, due to the fact that intent-slot recognition
    is a more established task. Turns were commonly annotated with multiple gold goals,
    but this model tended to only output one per turn. For example: “Hello ARI, where
    is the café?” would only have the prediction ‘greet’, missing the request to locate
    the café entirely. This prevented the model from achieving higher correct scores.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本设置下，GPT-3.5-turbo 也获得了比目标追踪结果更高的部分得分，这归因于意图-槽位识别任务更加成熟。对话中通常会用多个金标准目标进行标注，但该模型倾向于每轮只输出一个。例如：“你好
    ARI，咖啡馆在哪里？”会仅有预测 ‘问候’，完全遗漏了请求定位咖啡馆的部分。这阻碍了模型获得更高的正确得分。
- en: In the few-shot setting, however, GPT-3.5-turbo significantly outperformed all
    the other models. The difference was remarkable. Almost all of the predictions
    were partially correct, and the ‘reasoning’ prompts correctly annotated 70% of
    the MPCs. Other models tended to falter when anaphoric expressions couldn’t be
    resolved with just the previous turn. They also struggled to identify the ‘suggest’
    intent, for example, when one person said “do you want to go to the toilet?”.
    These were misclassified as request intents, likely due to their prominence in
    the corpus, and influence on the results due to GPT-3.5-turbo’s limited input
    context.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本设置中，然而，GPT-3.5-turbo显著优于所有其他模型。这个差异非常显著。几乎所有的预测都是部分正确的，并且‘推理’提示正确标注了70%的MPCs。其他模型在无法仅凭前一轮对照解析指代表达时往往会出现问题。例如，当一个人说“你想去洗手间吗？”时，他们也难以识别‘suggest’意图。这些被误分类为请求意图，可能是由于它们在语料库中的显著性以及对结果的影响，因为GPT-3.5-turbo的输入上下文有限。
- en: 5 Conclusion and Future Work
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: Multi-party conversations (MPCs) elicit complex behaviours which do not occur
    in the dyadic interactions that today’s dialogue systems are designed and trained
    to handle. Social robots are increasingly being expected to perform tasks in public
    spaces like museums and malls, where conversations often include groups of friends
    or family. Multi-party research has previously focused on speaker recognition,
    addressee recognition, and tweaking response generation depending on whom the
    system is addressing. While this work is vital, we argue that these collective
    “who says what to whom” tasks do not provide any incentive for the social robot
    to complete user goals, and instead encourage it to simply mimic what a good MPC
    *looks like*. In this paper, we have detailed how the tasks of goal tracking and
    intent-slot recognition differ in a multi-party setting, providing examples from
    our newly collected corpus of MPCs in a hospital. We found that, given limited
    data, ‘reasoning’ style prompts enable GPT-3.5-turbo to perform significantly
    better than other models.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 多方对话（MPCs）引发的复杂行为并不会出现在今天的对话系统设计和训练所处理的双人互动中。社会机器人越来越被期望在博物馆和商场等公共场所执行任务，这些场所的对话通常涉及一群朋友或家人。多方对话研究以前集中在说话者识别、受话者识别以及根据系统所针对的对象调整响应生成上。虽然这些工作至关重要，但我们认为这些集体的“谁对谁说什么”任务并未提供任何激励让社会机器人完成用户目标，而是鼓励其仅仅模仿一个好的MPC
    *看起来像什么*。在本文中，我们详细描述了在多方对话环境中，目标跟踪和意图-槽识别的任务如何不同，提供了我们在医院新收集的MPC语料库的示例。我们发现，考虑到有限的数据，‘推理’风格的提示使得GPT-3.5-turbo比其他模型表现显著更好。
- en: We found that other prompt styles also perform well, but prompts that are story-like
    increase model hallucination. With the introduction of prompt fine-tuning with
    human feedback Ouyang et al. ([2022](#bib.bib36)), generative LLMs do now have
    some incentive to avoid misleading or harming the user, providing outputs prepended
    with caveats, but the issue is not solved. OpenAI claims that GPT-4 generates
    40% fewer hallucinations than GPT-3 Hern and Bhuiyan ([2023](#bib.bib25)), but
    these models should still not be applied directly in a hospital or other safety-critical
    setting without further evaluation. In the hospital setting, users are more likely
    to be from vulnerable population groups, and are more likely to be older adults
    that are not familiar with the capabilities of today’s models. Multiple researchers
    and hospital staff members are present when conducting our data collections, so
    that if hallucinations do occur, they can be quickly corrected. We will, therefore,
    be able to evaluate response grounding, Guidance⁶⁶6[https://github.com/microsoft/guidance](https://github.com/microsoft/guidance),
    and other hallucination prevention strategies to determine whether these models
    can ever be used safely in a high-risk setting. These further experiments will
    also elicit further MPCs that can be annotated for various multi-party tasks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现其他提示风格的表现也很好，但类似故事的提示会增加模型的幻觉现象。随着引入了带有人类反馈的提示微调 Ouyang et al. ([2022](#bib.bib36))，生成性LLM现在有了一些避免误导或伤害用户的激励，提供带有警示的输出，但问题仍未解决。OpenAI声称GPT-4生成的幻觉比GPT-3少40%
    Hern and Bhuiyan ([2023](#bib.bib25))，但这些模型仍然不应该在医院或其他安全关键环境中直接应用而未经进一步评估。在医院环境中，用户更可能来自易受伤害的人群，并且更可能是对今天模型的能力不熟悉的老年人。在进行我们的数据收集时，多个研究人员和医院工作人员在场，以便如果出现幻觉现象，可以迅速纠正。因此，我们将能够评估响应基础，Guidance⁶⁶6[https://github.com/microsoft/guidance](https://github.com/microsoft/guidance)和其他幻觉预防策略，以确定这些模型是否可以安全地用于高风险环境。这些进一步的实验还将引发更多的MPCs，这些MPCs可以用于各种多方任务的注释。
- en: User inputs must be processed on external servers when using industry LLMs,
    like GPT-3.5-turbo and Google’s Bard. For this reason, these specific models cannot
    be deployed in the hospital setting. Patients may reveal identifiable or sensitive
    information during our data collection, which we subsequently remove from the
    corpus. This data must stay contained within approved data-controlled servers
    in the SPRING project. In this paper, we have reported the remarkable performance
    of an industry LLM, when given limited data, compared to prior model architectures.
    We will analyse open and transparent instruction-tuned text generators Liesenfeld
    et al. ([2023](#bib.bib30)), which are able to meet our data security requirements.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用行业LLM（如GPT-3.5-turbo和Google的Bard）时，用户输入必须在外部服务器上处理。因此，这些特定模型不能在医院环境中部署。患者可能在我们的数据收集中透露可识别或敏感的信息，我们随后会将其从语料库中移除。这些数据必须保持在SPRING项目中批准的数据控制服务器内。在本文中，我们报告了在数据有限的情况下，与先前的模型架构相比，行业LLM的显著表现。我们将分析开放和透明的指令调优文本生成器
    Liesenfeld et al. ([2023](#bib.bib30))，这些生成器能够满足我们的数据安全要求。
- en: The accessibility of today’s SDSs is critical when working with hospital patients
    Addlesee ([2023](#bib.bib1)). Speech production differs between the ‘average’
    user, and user groups that remain a minority in huge training datasets. For example,
    people with dementia pause more frequently and for longer durations mid-sentence
    due to word-finding problems Boschi et al. ([2017](#bib.bib9)); Slegers et al.
    ([2018](#bib.bib41)). We are utilising knowledge graphs to ensure that SDSs are
    transparent, controllable, and more accessible for these user groups Addlesee
    and Eshghi ([2021](#bib.bib4)); Addlesee and Damonte ([2023a](#bib.bib2), [b](#bib.bib3)),
    and we see the unification of large language models and knowledge graphs Pan et al.
    ([2023](#bib.bib37)) as the near-term future of our field.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当与医院患者合作时，今天的SDS的可访问性至关重要 Addlesee ([2023](#bib.bib1))。‘普通’用户和在庞大的训练数据集中仍然是少数的用户群体之间的言语产生有所不同。例如，患有痴呆症的人在句子中途因找词困难而更频繁且更长时间地停顿
    Boschi et al. ([2017](#bib.bib9)); Slegers et al. ([2018](#bib.bib41))。我们正在利用知识图谱来确保SDS对这些用户群体透明、可控且更具可访问性
    Addlesee and Eshghi ([2021](#bib.bib4)); Addlesee and Damonte ([2023a](#bib.bib2),
    [b](#bib.bib3))，并且我们将大语言模型与知识图谱的统一视为我们领域的短期未来 Pan et al. ([2023](#bib.bib37))。
- en: We plan to design and run subsequent experiments in both the hospital memory
    clinic, and a newly established mock waiting room in our lab. This space will
    allow us to collect additional MPCs with more than two people, replicating scenarios
    in which whole families approach a social robot. We plan to evaluate whether prompt
    engineering can work modularly for N users. For example, we could use GPT-4 to
    correct speaker diarization Murali et al. ([2023](#bib.bib34)), then to handle
    multi-party goal tracking, and then to generate responses to the user. This experimental
    setup will allow us to quickly test new ideas, such as automatic prompt optimization
    Pryzant et al. ([2023](#bib.bib38)) in the lab, maximising the benefit of patients’
    time in the hospital.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划在医院记忆门诊和我们实验室新建立的模拟候诊室中设计并进行后续实验。这个空间将允许我们收集更多有两人以上的MPCs，复制整个家庭接近社会机器人场景。我们计划评估提示工程是否可以为N个用户模块化工作。例如，我们可以使用GPT-4来修正发言人分段
    Murali 等（[2023](#bib.bib34)），然后处理多方目标跟踪，接着生成对用户的回应。这个实验设置将允许我们快速测试新想法，例如实验室中的自动提示优化
    Pryzant 等（[2023](#bib.bib38)），最大化患者在医院的时间收益。
- en: Acknowledgements
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was funded by the EU H2020 program under grant agreement no. 871245
    ([https://spring-h2020.eu/](https://spring-h2020.eu/)). We would also like to
    thank our anonymous reviewers for their time and valuable feedback.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究得到了欧盟H2020计划的资助，资助协议号871245（[https://spring-h2020.eu/](https://spring-h2020.eu/)）。我们还要感谢匿名审稿人抽出时间提供宝贵反馈。
- en: References
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Addlesee (2023) Angus Addlesee. 2023. Voice assistant accessibility. In *The
    International Workshop on Spoken Dialogue Systems Technology, IWSDS 2023*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Addlesee（2023）Angus Addlesee. 2023. 语音助手无障碍性。在 *The International Workshop on
    Spoken Dialogue Systems Technology, IWSDS 2023* 会议上。
- en: Addlesee and Damonte (2023a) Angus Addlesee and Marco Damonte. 2023a. Understanding
    and answering incomplete questions. In *Proceedings of the 5th Conference on Conversational
    User Interfaces*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Addlesee 和 Damonte（2023a）Angus Addlesee 和 Marco Damonte. 2023a. 理解和回答不完整的问题。在
    *Proceedings of the 5th Conference on Conversational User Interfaces* 会议上。
- en: Addlesee and Damonte (2023b) Angus Addlesee and Marco Damonte. 2023b. Understanding
    disrupted sentences using underspecified abstract meaning representation. In *Interspeech*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Addlesee 和 Damonte（2023b）Angus Addlesee 和 Marco Damonte. 2023b. 理解被干扰的句子，使用未指定的抽象意义表示。在
    *Interspeech* 会议上。
- en: Addlesee and Eshghi (2021) Angus Addlesee and Arash Eshghi. 2021. [Incremental
    graph-based semantics and reasoning for conversational AI](https://aclanthology.org/2021.reinact-1.1).
    In *Proceedings of the Reasoning and Interaction Conference (ReInAct 2021)*, pages
    1–7, Gothenburg, Sweden. Association for Computational Linguistics.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Addlesee 和 Eshghi（2021）Angus Addlesee 和 Arash Eshghi. 2021. [增量图基语义和对话AI推理](https://aclanthology.org/2021.reinact-1.1)。在
    *Proceedings of the Reasoning and Interaction Conference (ReInAct 2021)* 会议上，第1–7页，瑞典哥德堡。计算语言学协会。
- en: Addlesee et al. (2023) Angus Addlesee, Weronika Sieińska, Nancie Gunson, Daniel
    Hernández García, Christian Dondrup, and Oliver Lemon. 2023. Data collection for
    multi-party task-based dialogue in social robotics. In *The International Workshop
    on Spoken Dialogue Systems Technology, IWSDS 2023*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Addlesee 等（2023）Angus Addlesee, Weronika Sieińska, Nancie Gunson, Daniel Hernández
    García, Christian Dondrup 和 Oliver Lemon. 2023. 社会机器人中的多方任务对话的数据收集。在 *The International
    Workshop on Spoken Dialogue Systems Technology, IWSDS 2023* 会议上。
- en: 'Al Moubayed et al. (2012) Samer Al Moubayed, Jonas Beskow, Gabriel Skantze,
    and Björn Granström. 2012. Furhat: a back-projected human-like robot head for
    multiparty human-machine interaction. In *Cognitive Behavioural Systems: COST
    2102 International Training School, Dresden, Germany, February 21-26, 2011, Revised
    Selected Papers*, pages 114–130\. Springer.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Al Moubayed 等（2012）Samer Al Moubayed, Jonas Beskow, Gabriel Skantze 和 Björn
    Granström. 2012. Furhat: 一种用于多方人机交互的背投式类人机器人头部。在 *Cognitive Behavioural Systems:
    COST 2102 International Training School, Dresden, Germany, February 21-26, 2011,
    Revised Selected Papers*，第114–130页。Springer。'
- en: 'Bao et al. (2020) Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong
    Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-masked
    language models for unified language model pre-training. In *International conference
    on machine learning*, pages 642–652\. PMLR.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bao 等（2020）Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu,
    Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou 等。2020. Unilmv2: 用于统一语言模型预训练的伪掩码语言模型。在
    *International conference on machine learning* 会议上，第642–652页。PMLR。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等（2020）Iz Beltagy, Matthew E Peters 和 Arman Cohan. 2020. Longformer:
    长文档变换器。*arXiv 预印本 arXiv:2004.05150*。'
- en: 'Boschi et al. (2017) Veronica Boschi, Eleonora Catricala, Monica Consonni,
    Cristiano Chesi, Andrea Moro, and Stefano F Cappa. 2017. Connected speech in neurodegenerative
    language disorders: a review. *Frontiers in psychology*, 8:269.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boschi et al. (2017) Veronica Boschi, Eleonora Catricala, Monica Consonni, Cristiano
    Chesi, Andrea Moro, 和 Stefano F Cappa. 2017. 神经退行性语言障碍中的连贯言语：综述. *心理学前沿*, 8:269.
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少样本学习者. *神经信息处理系统进展*, 33:1877–1901.
- en: Brugman et al. (2004) Hennie Brugman, Albert Russel, and Xd Nijmegen. 2004.
    Annotating multi-media/multi-modal resources with elan. In *LREC*, pages 2065–2068.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brugman et al. (2004) Hennie Brugman, Albert Russel, 和 Xd Nijmegen. 2004. 使用elan注释多媒体/多模态资源.
    在 *LREC*，页2065–2068.
- en: 'Cooper et al. (2020) Sara Cooper, Alessandro Di Fava, Carlos Vivas, Luca Marchionni,
    and Francesco Ferro. 2020. Ari: The social assistive robot and companion. In *2020
    29th IEEE International Conference on Robot and Human Interactive Communication
    (RO-MAN)*, pages 745–751\. IEEE.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cooper et al. (2020) Sara Cooper, Alessandro Di Fava, Carlos Vivas, Luca Marchionni,
    和 Francesco Ferro. 2020. Ari: 社交辅助机器人及伴侣. 在 *2020年第29届IEEE国际机器人与人机互动会议（RO-MAN）*，页745–751。IEEE.'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert: 用于语言理解的深度双向变换器预训练. *arXiv预印本 arXiv:1810.04805*.'
- en: 'Eshghi and Healey (2016) Arash Eshghi and Patrick GT Healey. 2016. Collective
    contexts in conversation: Grounding by proxy. *Cognitive science*, 40(2):299–324.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eshghi and Healey (2016) Arash Eshghi 和 Patrick GT Healey. 2016. 对话中的集体语境：代理基础的基础.
    *认知科学*, 40(2):299–324.
- en: 'Feng et al. (2022) Yue Feng, Aldo Lipani, Fanghua Ye, Qiang Zhang, and Emine
    Yilmaz. 2022. Dynamic schema graph fusion network for multi-domain dialogue state
    tracking. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 115–126.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2022) Yue Feng, Aldo Lipani, Fanghua Ye, Qiang Zhang, 和 Emine Yilmaz.
    2022. 多域对话状态跟踪的动态模式图融合网络. 在 *第60届计算语言学协会年会（第1卷：长论文）会议记录*，页115–126.
- en: Fisher (1992) Ronald Aylmer Fisher. 1992. *Statistical methods for research
    workers*. Springer.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fisher (1992) Ronald Aylmer Fisher. 1992. *研究工作者的统计方法*。Springer.
- en: 'Foster et al. (2019) Mary Ellen Foster, Bart Craenen, Amol Deshmukh, Oliver
    Lemon, Emanuele Bastianelli, Christian Dondrup, Ioannis Papaioannou, Andrea Vanzo,
    Jean-Marc Odobez, Olivier Canévet, et al. 2019. MuMMER: Socially intelligent human-robot
    interaction in public spaces. *arXiv preprint arXiv:1909.06749*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Foster et al. (2019) Mary Ellen Foster, Bart Craenen, Amol Deshmukh, Oliver
    Lemon, Emanuele Bastianelli, Christian Dondrup, Ioannis Papaioannou, Andrea Vanzo,
    Jean-Marc Odobez, Olivier Canévet, 等. 2019. MuMMER: 在公共空间中的社交智能人机互动. *arXiv预印本
    arXiv:1909.06749*.'
- en: Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2022. Complexity-based prompting for multi-step reasoning. *arXiv preprint
    arXiv:2210.00720*.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, 和 Tushar Khot.
    2022. 基于复杂度的多步骤推理提示. *arXiv预印本 arXiv:2210.00720*.
- en: 'Gu et al. (2022a) Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling,
    Huang Hu, Xiubo Geng, and Daxin Jiang. 2022a. HeterMPC: A Heterogeneous Graph
    Neural Network for Response Generation in Multi-Party Conversations. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 5086–5097.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2022a) Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling,
    Huang Hu, Xiubo Geng, 和 Daxin Jiang. 2022a. HeterMPC: 用于多方对话回应生成的异质图神经网络. 在 *第60届计算语言学协会年会（第1卷：长论文）会议记录*，页5086–5097.'
- en: 'Gu et al. (2022b) Jia-Chen Gu, Chongyang Tao, and Zhen-Hua Ling. 2022b. WHO
    Says WHAT to WHOM: A Survey of Multi-Party Conversations. In *Proceedings of the
    Thirty-First International Joint Conference on Artificial Intelligence (IJCAI-22)*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2022b) Jia-Chen Gu, Chongyang Tao, 和 Zhen-Hua Ling. 2022b. WHO Says
    WHAT to WHOM: 多方对话调查. 在 *第31届国际人工智能联合会议（IJCAI-22）会议记录*.'
- en: 'Gu et al. (2021) Jia-Chen Gu, Chongyang Tao, Zhenhua Ling, Can Xu, Xiubo Geng,
    and Daxin Jiang. 2021. MPC-BERT: A pre-trained language model for multi-party
    conversation understanding. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics*, pages 3682–3692.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 (2021) Jia-Chen Gu、Chongyang Tao、Zhenhua Ling、Can Xu、Xiubo Geng 和 Daxin
    Jiang。2021。MPC-BERT：一种用于多方对话理解的预训练语言模型。见于 *第59届计算语言学协会年会*，第3682–3692页。
- en: Gunson et al. (2022) Nancie Gunson, Daniel Hernández Garcia, Weronika Sieińska,
    Angus Addlesee, Christian Dondrup, Oliver Lemon, Jose L Part, and Yanchao Yu.
    2022. A visually-aware conversational robot receptionist. In *Proceedings of the
    23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue*,
    pages 645–648.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunson 等 (2022) Nancie Gunson、Daniel Hernández Garcia、Weronika Sieińska、Angus
    Addlesee、Christian Dondrup、Oliver Lemon、Jose L Part 和 Yanchao Yu。2022。一个视觉感知的对话机器人接待员。见于
    *第23届年会专题兴趣组对话和交流*，第645–648页。
- en: Henderson et al. (2014a) Matthew Henderson, Blaise Thomson, and Jason D Williams.
    2014a. The second dialog state tracking challenge. In *Proceedings of the 15th
    annual meeting of the special interest group on discourse and dialogue (SIGDIAL)*,
    pages 263–272.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等 (2014a) Matthew Henderson、Blaise Thomson 和 Jason D Williams。2014a。第二届对话状态跟踪挑战赛。见于
    *第15届年会专题兴趣组对话和交流 (SIGDIAL)*，第263–272页。
- en: Henderson et al. (2014b) Matthew Henderson, Blaise Thomson, and Jason D Williams.
    2014b. The third dialog state tracking challenge. In *2014 IEEE Spoken Language
    Technology Workshop (SLT)*, pages 324–329\. IEEE.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等 (2014b) Matthew Henderson、Blaise Thomson 和 Jason D Williams。2014b。第三届对话状态跟踪挑战赛。见于
    *2014 IEEE 口语语言技术研讨会 (SLT)*，第324–329页。IEEE。
- en: Hern and Bhuiyan (2023) Alex Hern and Johana Bhuiyan. 2023. Openai says new
    model gpt-4 is more creative and less likely to invent facts. *The Guardian*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hern 和 Bhuiyan (2023) Alex Hern 和 Johana Bhuiyan。2023。OpenAI 表示新模型 GPT-4 更具创造力，并且更不容易编造事实。*卫报*。
- en: 'Hu et al. (2019) Wenpeng Hu, Zhangming Chan, Bing Liu, Dongyan Zhao, Jinwen
    Ma, and Rui Yan. 2019. GSN: A graph-structured network for multi-party dialogues.
    In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence (IJCAI-19)*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2019) Wenpeng Hu、Zhangming Chan、Bing Liu、Dongyan Zhao、Jinwen Ma 和 Rui
    Yan。2019。GSN：用于多方对话的图结构网络。见于 *第二十八届国际联合人工智能会议 (IJCAI-19)*。
- en: Keizer et al. (2013) Simon Keizer, Mary Ellen Foster, Oliver Lemon, Andre Gaschler,
    and Manuel Giuliani. 2013. Training and evaluation of an MDP model for social
    multi-user human-robot interaction. In *Proceedings of the SIGDIAL 2013 Conference*,
    pages 223–232.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keizer 等 (2013) Simon Keizer、Mary Ellen Foster、Oliver Lemon、Andre Gaschler 和
    Manuel Giuliani。2013。社会多用户人机互动的 MDP 模型的训练和评估。见于 *SIGDIAL 2013会议*，第223–232页。
- en: Lee et al. (2021) Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue
    state tracking with a language model using schema-driven prompting. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    4937–4949.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2021) Chia-Hsuan Lee、Hao Cheng 和 Mari Ostendorf。2021。使用基于模式的提示的语言模型进行对话状态跟踪。见于
    *2021年自然语言处理经验方法会议*，第4937–4949页。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021
    Conference on Empirical Methods in Natural Language Processing*, pages 3045–3059.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等 (2021) Brian Lester、Rami Al-Rfou 和 Noah Constant。2021。参数高效提示调优的规模力量。见于
    *2021年自然语言处理经验方法会议*，第3045–3059页。
- en: 'Liesenfeld et al. (2023) Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse.
    2023. Opening up chatgpt: Tracking openness, transparency, and accountability
    in instruction-tuned text generators. In *Proceedings of the 5th International
    Conference on Conversational User Interfaces*, pages 1–6.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liesenfeld 等 (2023) Andreas Liesenfeld、Alianda Lopez 和 Mark Dingemanse。2023。开放
    ChatGPT：跟踪指令调优文本生成器的开放性、透明性和问责制。见于 *第5届国际对话用户界面会议*，第1–6页。
- en: Marselino Andreas et al. (2022) Vinsen Marselino Andreas, Genta Indra Winata,
    and Ayu Purwarianti. 2022. A comparative study on language models for task-oriented
    dialogue systems. *arXiv e-prints*, pages arXiv–2201.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marselino Andreas 等 (2022) Vinsen Marselino Andreas、Genta Indra Winata 和 Ayu
    Purwarianti。2022。任务导向对话系统的语言模型比较研究。*arXiv e-prints*，第arXiv–2201页。
- en: Moujahid et al. (2022) Meriam Moujahid, Helen Hastie, and Oliver Lemon. 2022.
    Multi-party interaction with a robot receptionist. In *2022 17th ACM/IEEE International
    Conference on Human-Robot Interaction (HRI)*, pages 927–931\. IEEE.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moujahid 等人 (2022) Meriam Moujahid, Helen Hastie 和 Oliver Lemon。2022。**与机器人接待员的多方互动**。载于
    *2022年第17届ACM/IEEE国际人机交互会议（HRI）*，第927–931页。IEEE。
- en: 'Mueller et al. (2022) Aaron Mueller, Jason Krone, Salvatore Romeo, Saab Mansour,
    Elman Mansimov, Yi Zhang, and Dan Roth. 2022. Label semantic aware pre-training
    for few-shot text classification. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    8318–8334.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mueller 等人 (2022) Aaron Mueller, Jason Krone, Salvatore Romeo, Saab Mansour,
    Elman Mansimov, Yi Zhang 和 Dan Roth。2022。**面向少量样本文本分类的标签语义感知预训练**。载于 *第60届计算语言学协会年会（第1卷：长篇论文）*，第8318–8334页。
- en: Murali et al. (2023) Prasanth Murali, Ian Steenstra, Hye Sun Yun, Ameneh Shamekhi,
    and Timothy Bickmore. 2023. Improving multiparty interactions with a robot using
    large language models. In *Extended Abstracts of the 2023 CHI Conference on Human
    Factors in Computing Systems*, pages 1–8.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murali 等人 (2023) Prasanth Murali, Ian Steenstra, Hye Sun Yun, Ameneh Shamekhi
    和 Timothy Bickmore。2023。**使用大型语言模型改进与机器人进行的多方互动**。载于 *2023年CHI计算机系统人因扩展摘要*，第1–8页。
- en: 'Novikova et al. (2016) Jekaterina Novikova, Oliver Lemon, and Verena Rieser.
    2016. [Crowd-sourcing NLG data: Pictures elicit better data.](https://doi.org/10.18653/v1/W16-6644)
    In *Proceedings of the 9th International Natural Language Generation conference*,
    pages 265–273, Edinburgh, UK. Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novikova 等人 (2016) Jekaterina Novikova, Oliver Lemon 和 Verena Rieser。2016。[**众包自然语言生成数据：图片引发更好的数据。**](https://doi.org/10.18653/v1/W16-6644)
    载于 *第9届国际自然语言生成会议论文集*，第265–273页，英国爱丁堡。计算语言学协会。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等。2022。**通过人类反馈训练语言模型以遵循指令**。*神经信息处理系统进展*，35:27730–27744。
- en: 'Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. 2023. Unifying large language models and knowledge graphs: A roadmap.
    *arXiv preprint arXiv:2306.08302*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang 和 Xindong
    Wu。2023。**统一大型语言模型和知识图谱：一条路线图**。*arXiv预印本 arXiv:2306.08302*。
- en: Pryzant et al. (2023) Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang
    Zhu, and Michael Zeng. 2023. Automatic prompt optimization with "gradient descent"
    and beam search. *arXiv preprint arXiv:2305.03495*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pryzant 等人 (2023) Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu
    和 Michael Zeng。2023。**使用“梯度下降”和束搜索的自动提示优化**。*arXiv预印本 arXiv:2305.03495*。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等。2019。**语言模型是无监督的多任务学习者**。*OpenAI博客*，1(8):9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu。2020。**使用统一的文本到文本变换器探索迁移学习的极限**。*机器学习研究杂志*，21(1):5485–5551。
- en: 'Slegers et al. (2018) Antoine Slegers, Renee-Pier Filiou, Maxime Montembeault,
    and Simona Maria Brambati. 2018. Connected speech features from picture description
    in alzheimer’s disease: A systematic review. *Journal of Alzheimer’s Disease*,
    65(2):519–542.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slegers 等人 (2018) Antoine Slegers, Renee-Pier Filiou, Maxime Montembeault 和
    Simona Maria Brambati。2018。**阿尔茨海默病中从图片描述中提取的连贯语音特征：系统评审**。*阿尔茨海默病杂志*，65(2):519–542。
- en: 'Taylor (2023) Josh Taylor. 2023. Chatgpt’s alter ego, dan: users jailbreak
    ai program to get around ethical safeguards. *The Guardian*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor (2023) Josh Taylor。2023。**ChatGPT的另一个身份，DAN：用户越狱AI程序以绕过伦理保护**。*卫报*。
- en: 'Tian et al. (2021) Leimin Tian, Pamela Carreno-Medrano, Aimee Allen, Shanti
    Sumartojo, Michael Mintrom, Enrique Coronado Zuniga, Gentiane Venture, Elizabeth
    Croft, and Dana Kulic. 2021. Redesigning human-robot interaction in response to
    robot failures: A participatory design methodology. In *Extended Abstracts of
    the 2021 CHI Conference on Human Factors in Computing Systems*, pages 1–8.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian等人（2021）Leimin Tian, Pamela Carreno-Medrano, Aimee Allen, Shanti Sumartojo,
    Michael Mintrom, Enrique Coronado Zuniga, Gentiane Venture, Elizabeth Croft, 和
    Dana Kulic. 2021. 针对机器人失败重新设计人机互动：一种参与设计方法。见于*2021年CHI人机交互会议扩展摘要*，第1–8页。
- en: 'Traum (2004) David Traum. 2004. Issues in multiparty dialogues. In *Advances
    in Agent Communication: International Workshop on Agent Communication Languages,
    ACL 2003, Melbourne, Australia, July 14, 2003. Revised and Invited Papers*, pages
    201–211\. Springer.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Traum (2004) David Traum. 2004. 多方对话中的问题。见于*代理通信进展：国际代理通信语言研讨会，ACL 2003，澳大利亚墨尔本，2003年7月14日。修订及邀请论文*，第201–211页。Springer。
- en: Tukey (1949) John W Tukey. 1949. Comparing individual means in the analysis
    of variance. *Biometrics*, pages 99–114.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tukey (1949) John W Tukey. 1949. 在方差分析中比较个体均值。*生物统计学*，第99–114页。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc
    Le, 和 Denny Zhou. 2022. 思维链提示在大语言模型中引发推理。*arXiv预印本 arXiv:2201.11903*。
- en: Weng (2023) Lilian Weng. 2023. [Prompt engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/).
    *lilianweng.github.io*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng (2023) Lilian Weng. 2023. [提示工程](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)。*lilianweng.github.io*。
- en: 'Williams et al. (2016) Jason Williams, Antoine Raux, and Matthew Henderson.
    2016. The dialog state tracking challenge series: A review. *Dialogue & Discourse*,
    7(3):4–33.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams等人（2016）Jason Williams, Antoine Raux, 和 Matthew Henderson. 2016. 对话状态跟踪挑战系列：综述。*对话与话语*，7(3):4–33。
- en: Ye et al. (2023) Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun
    Kim, and Minjoon Seo. 2023. In-context instruction learning. *arXiv preprint arXiv:2302.14691*.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人（2023）Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim,
    和 Minjoon Seo. 2023. 上下文中的指令学习。*arXiv预印本 arXiv:2302.14691*。
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. Calibrate before use: Improving few-shot performance of language
    models. In *International Conference on Machine Learning*, pages 12697–12706\.
    PMLR.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2021）Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, 和 Sameer Singh. 2021.
    使用前的校准：提高语言模型的少量-shot性能。见于*国际机器学习会议*，第12697–12706页。PMLR。
- en: 'Zhong et al. (2022) Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael
    Zeng. 2022. DialogLM: Pre-trained model for long dialogue understanding and summarization.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36,
    pages 11765–11773.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong等人（2022）Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, 和 Michael Zeng.
    2022. DialogLM: 用于长对话理解和总结的预训练模型。见于*AAAI人工智能会议论文集*，第36卷，第11765–11773页。'
- en: Appendix A Full GPT-3.5-turbo Prompts
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 全部GPT-3.5-turbo提示
- en: 'Here are the full prompts given to GPT-3.5-turbo for each task. We used six
    styles described in Section [3](#S3 "3 Experimental Procedure ‣ Multi-party Goal
    Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering").
    The masked MPC was appended to each prompt in the zero-shot setting. In the few-shot
    prompts (see Section [A.2](#A1.SS2 "A.2 Few-shot Intent-slot Recognition ‣ Appendix
    A Full GPT-3.5-turbo Prompts ‣ Multi-party Goal Tracking with LLMs: Comparing
    Pre-training, Fine-tuning, and Prompt Engineering")), we appended examples with
    “input:” + masked MPC #1 + “output:” + gold output #1 + ‘input:” + masked MPC
    #2 + “output:” + gold output #2 + “input:” + test set masked MPC + “output:”⁷⁷7The
    examples given were randomised per run, and the appendix page limit doesn’t fit
    the full 4,096 token prompts..'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '这里是给GPT-3.5-turbo每个任务的完整提示。我们使用了第[3](#S3 "3 Experimental Procedure ‣ Multi-party
    Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering")节中描述的六种风格。掩码MPC被附加到零-shot设置中的每个提示中。在少量-shot
    提示中（见第[A.2](#A1.SS2 "A.2 Few-shot Intent-slot Recognition ‣ Appendix A Full GPT-3.5-turbo
    Prompts ‣ Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning,
    and Prompt Engineering")节），我们附加了“input:” + 掩码MPC #1 + “output:” + 金标准输出 #1 + “input:”
    + 掩码MPC #2 + “output:” + 金标准输出 #2 + “input:” + 测试集掩码MPC + “output:”⁷⁷7这些示例是随机化的，附录的页数限制不能容纳完整的4,096个token提示。'
- en: A.1 Zero-shot Goal Tracking
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 零-shot目标跟踪
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Basic: This conversation has a window between [start] and [end]. Return this
    window with the [MASK] tags replaced with the goal annotations:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本信息：这段对话在[start]和[end]之间。返回这段窗口，并将[MASK]标签替换为目标注释：
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specific: This is a conversation between two people and a robot called ARI.
    There is a section of the conversation between the [start] and [end] tags. I want
    you to return this section of the conversation, but I want you to replace the
    [MASK] tags with the user goals. Do not change any of the other words in the section,
    only replace [MASK]. Every [MASK] should be replaced. Here is the conversation:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体信息：这是一段两个真人和一个名叫 ARI 的机器人之间的对话。对话中有一部分在[start]和[end]标签之间。我希望你返回这部分对话，但只需将[MASK]标签替换为用户目标。不要更改部分中的其他单词，只替换[MASK]。每个[MASK]都应该被替换。对话内容如下：
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Annotation: This is a conversation between two people and a robot called ARI.
    I want you to first extract the text between [start] and [end]. There are [MASK]
    tags in the extracted text. I want you to replace the [MASK] tags with goal annotations.
    Do not change any of the other text. If the person’s goal can be determined by
    that turn, add an ’@’ symbol followed by ’G’ (G for goal), and then brackets with
    the speaker ID and what their goal is. If it is a shared goal, you can annotate
    both speakers with a ’+’ sign between them. For example, if you think U1 and U2
    share the goal, you can write U1+U2\. If you think the goal is being answered,
    you can do the same but with ’AG’ (AG for Answer Goal) instead of ’G’. Finally,
    if you think the person is closing the goal, you can do the same annotation using
    ’CG’ (CG for Close Goal) instead of ’G’ or AG’. Here is the conversation:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注释：这是一段两个真人和一个名叫 ARI 的机器人之间的对话。我希望你首先提取[start]和[end]之间的文本。提取的文本中有[MASK]标签。我希望你将[MASK]标签替换为目标注释。不要更改其他任何文本。如果通过这一轮对话可以确定说话者的目标，添加一个‘@’符号，后跟‘G’（G
    代表目标），然后是方括号内的说话者 ID 和他们的目标。如果这是一个共享目标，你可以用‘+’符号连接两个说话者。例如，如果你认为 U1 和 U2 共享目标，你可以写
    U1+U2。如果你认为目标已经得到回应，你可以做相同的标注，但用‘AG’（AG 代表回应目标）代替‘G’。最后，如果你认为说话者正在结束目标，你可以做相同的标注，但用‘CG’（CG
    代表关闭目标）代替‘G’或‘AG’。对话内容如下：
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Story: There once was a conversation between a patient, a companion, and a
    robot called ARI. One bit of the conversation was confusing. A helpful researcher
    noted the start with [start], and the end with [end]. The confusing bits are marked
    with [MASK]. Can you help us figure out the goals that should replace the [MASK]
    tags? The conversation is this:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 故事：曾经有一次病人、陪伴者和一个名叫 ARI 的机器人之间的对话。其中一部分对话让人困惑。一位有用的研究人员注意到对话从[start]开始，到[end]结束。困惑的部分用[MASK]标记。你能帮助我们找出应该替换[MASK]标签的目标吗？对话内容如下：
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Role-play: You are listening to a conversation between two people and a robot
    called ARI. You are a helpful assistant that needs to figure out what goals the
    people have. You need to pay attention to the [MASK] tags between the [start]
    and [end] tags in the given conversation. Your job is to replace these [MASK]
    tags with the correct goal annotations. Here is the conversation:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色扮演：你正在听两个真人和一个名叫 ARI 的机器人之间的对话。你是一个有帮助的助手，需要找出这些人的目标。你需要注意[start]和[end]标签之间的[MASK]标签。你的工作是将这些[MASK]标签替换为正确的目标注释。对话内容如下：
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning: I will give you a conversation between two people and a robot called
    ARI. You need to return the text between [start] and [end] with the [MASK] tags
    replaced by user goals. Let’s step through how to figure out the correct annotation.
    If the conversation included ’U1: I really need the toilet [MASK]’, then we would
    first know that the speaker is called U1\. The turn also ends with [MASK], so
    we know that we need to replace it with a goal. We know that U1 needs the toilet,
    so their goal is to go to the nearest toilet. Goals always begin with the ’@’
    symbol, and then a ’G’ if we have found a person’s goal. We would therefore replace
    [MASK] with @ G(U1, go-to(toilet)). If someone tells U1 where the toilets are,
    they have answered their goal. We would therefore annotate that turn with @ AG(U1,
    go-to(toilet)). We use AG here to indicate Answer Goal. Finally, if U1 then said
    thank you, we know their goal has been met. We would annotate the thank you with
    @ CG(U1, go-to(toilet)) because U1’s goal is finished. CG stands for Close Goal.
    Do this goal tracking for each [MASK] in this conversation:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '推理：我将给你一个包含两个人和一个叫做 ARI 的机器人的对话。你需要返回[start]和[end]之间的文本，并用用户目标替换[MASK]标签。让我们一步一步地探讨如何找出正确的标注。如果对话中包含’U1:
    我真的需要去洗手间[MASK]’，那么我们首先知道说话者叫 U1。对话回合也以[MASK]结束，所以我们知道需要用目标替换它。我们知道 U1 需要去洗手间，所以他们的目标是去最近的洗手间。目标总是以’@’符号开头，如果我们找到了一个人的目标，那么接着是’G’。因此，我们将
    [MASK] 替换为 @ G(U1, go-to(toilet))。如果有人告诉 U1 洗手间的位置，他们就回答了他们的目标。因此我们会将这个回合标注为 @
    AG(U1, go-to(toilet))。我们在这里使用 AG 来表示回答目标。最后，如果 U1 说了谢谢，我们知道他们的目标已经实现。我们将谢谢标注为
    @ CG(U1, go-to(toilet))，因为 U1 的目标已经完成。CG 代表完成目标。对对话中的每个 [MASK] 进行目标跟踪：'
- en: A.2 Few-shot Intent-slot Recognition
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 少样本意图-槽位识别
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Basic: Each conversation has a window between [start] and [end]. Return this
    window with the [MASK] tags replaced with the intent-slot annotations. Here are
    some examples.'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本：每个对话都有一个[start]和[end]之间的窗口。返回这个窗口，并用意图-槽位注释替换[MASK]标签。这里有一些例子。
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specific: Each of these conversations is between two people and a robot called
    ARI. There is a section of each conversation between the [start] and [end] tags.
    I want you to return this section of the conversation, but I want you to replace
    the [MASK] tags with the user intents and slots. Do not change any of the other
    words in the section, only replace [MASK]. Every [MASK] should be replaced. Here
    are some examples.'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体：每个对话都是在两个人和一个叫做 ARI 的机器人之间进行的。每个对话的某个部分在[start]和[end]标签之间。我希望你返回这部分对话，但要用用户意图和槽位替换[MASK]标签。不要更改该部分中的其他词汇，只替换[MASK]。每个[MASK]都应该被替换。这里有一些例子。
- en: •
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Annotation: Each of these conversations is between two people and a robot called
    ARI. I want you to first extract the text between [start] and [end]. There are
    [MASK] tags in the extracted text. I want you to replace the [MASK] tags with
    intent-slot annotations. Do not change any of the other text. If the person’s
    intent can be determined by that turn, add a ’#’ symbol followed by their intent
    and then brackets with the slots within. There are not always slots, so the brackets
    can be empty. Sometimes there are multiple intents, split them with a semi-colon
    ’;’. Here are some examples.'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注释：每个对话都是在两个人和一个叫做 ARI 的机器人之间进行的。我希望你首先提取[start]和[end]之间的文本。在提取的文本中有[MASK]标签。我希望你用意图-槽位注释替换[MASK]标签。不要更改其他文本。如果人的意图可以通过该回合确定，请添加’#’符号，然后是他们的意图，再加上带有槽位的括号。如果没有槽位，括号可以为空。有时会有多个意图，用分号’；’分隔它们。这里有一些例子。
- en: •
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Story: There once was a conversation between a patient, a companion, and a
    robot called ARI. One bit of the conversation was confusing. A helpful researcher
    noted the start with [start], and the end with [end]. The confusing bits are marked
    with [MASK]. Can you help us figure out the intents and slots that should replace
    the [MASK] tags? Here are some examples.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 故事：曾经有一个患者、一个陪伴者和一个叫做 ARI 的机器人之间的对话。其中一部分对话令人困惑。一位有帮助的研究员标记了[start]的开始和[end]的结束。困惑的部分用[MASK]标记。你能帮助我们找出应该替换[MASK]标签的意图和槽位吗？这里有一些例子。
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Role-play: You are listening to a conversation between two people and a robot
    called ARI. You are a helpful assistant that needs to figure out what goals the
    people have. You need to pay attention to the [MASK] tags between the [start]
    and [end] tags in the given conversation. Your job is to replace these [MASK]
    tags with the correct intent-slot annotations. Here are some examples.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色扮演：你正在听两个人和一个名为 ARI 的机器人之间的对话。你是一个需要弄清楚人们目标的助手。你需要注意给定对话中[start]和[end]标签之间的[MASK]标签。你的任务是用正确的意图-槽位注解替换这些[MASK]标签。这里有一些例子。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning: I will give you a conversation between two people and a robot called
    ARI. You need to return the text between [start] and [end] with the [MASK] tags
    replaced by user intents and slots. Let’s step through how to figure out the correct
    annotation. If the conversation included ’U1: Hello, I’d like to know where the
    doctor’s office is? [MASK]’ then we know there is a missing intent-slot annotation
    because of the [MASK] tag. U1 first said hello, greeting their interlocutor, so
    we know their intent is greet. This has no slots, so we have the annotation ’#
    greet()’ to start. U1 also asked where the doctor is, so their second intent is
    a request. The slot is the room that the doctor is in, as that is what they are
    requesting. Their second intent is therefore ’# request(doctor(room)). As there
    are multiple intents, the [MASK] is replaced by ’# greet() ; request(doctor(room))’.
    The ’;’ is only used because there was more than one intent. Do this intent-slot
    annotation for each [MASK] in this conversation. Here are some examples.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '推理：我会给你一个包含两个人和一个名为 ARI 的机器人对话的文本。你需要将[start]和[end]之间的文本中的[MASK]标签替换为用户意图和槽位。我们一步步来弄清楚如何标注正确的注解。如果对话中包括’U1:
    Hello, I’d like to know where the doctor’s office is? [MASK]’，那么我们知道由于[MASK]标签存在，缺少意图-槽位注解。U1首先打招呼，问候对方，因此我们知道他们的意图是greet。这没有槽位，所以注解为’#
    greet()’。U1还询问了医生的所在位置，因此他们的第二个意图是请求。槽位是医生所在的房间，因为这是他们请求的内容。因此，他们的第二个意图是’# request(doctor(room))。由于存在多个意图，[MASK]被替换为’#
    greet() ; request(doctor(room))’。’；’仅在存在多个意图时使用。对对话中每个[MASK]进行这种意图-槽位注解。这里有一些例子。'
