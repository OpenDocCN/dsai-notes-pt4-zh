- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:37:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Med42 - 医学LLMs微调策略评估：全参数方法与参数高效方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14779](https://ar5iv.labs.arxiv.org/html/2404.14779)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14779](https://ar5iv.labs.arxiv.org/html/2404.14779)
- en: Clément Christophe^∗¹, Praveen K Kanithi¹, Prateek Munjal¹, Tathagata Raha¹,
    Nasir Hayat¹, Ronnie Rajan¹, Ahmed Al-Mahrooqi¹, Avani Gupta¹, Muhammad Umar Salman¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Clément Christophe^∗¹, Praveen K Kanithi¹, Prateek Munjal¹, Tathagata Raha¹,
    Nasir Hayat¹, Ronnie Rajan¹, Ahmed Al-Mahrooqi¹, Avani Gupta¹, Muhammad Umar Salman¹，
- en: Gurpreet Gosal³, Bhargav Kanakiya³, Charles Chen³, Natalia Vassilieva³, Boulbaba
    Ben Amor², Marco AF Pimentel¹, Shadab Khan¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Gurpreet Gosal³, Bhargav Kanakiya³, Charles Chen³, Natalia Vassilieva³, Boulbaba
    Ben Amor², Marco AF Pimentel¹, Shadab Khan¹
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This study presents a comprehensive analysis and comparison of two predominant
    fine-tuning methodologies – full-parameter fine-tuning and parameter-efficient
    tuning – within the context of medical Large Language Models (LLMs). We developed
    and refined a series of LLMs, based on the Llama-2 architecture, specifically
    designed to enhance medical knowledge retrieval, reasoning, and question answering
    capabilities. Our experiments systematically evaluate the effectiveness of these
    tuning strategies across various well-known medical benchmarks. Notably, our medical
    LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination
    (USMLE) datasets, setting a new standard in performance for openly available medical
    LLMs. Through this comparative analysis, we aim to identify the most effective
    and efficient method for fine-tuning LLMs in the medical domain, thereby contributing
    significantly to the advancement of AI-driven healthcare applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究在医学大型语言模型（LLMs）的背景下，对两种主要的微调方法——全参数微调和参数高效微调——进行了全面的分析和比较。我们开发并优化了一系列基于Llama-2架构的LLMs，专门设计用于提升医学知识检索、推理和问答能力。我们的实验系统地评估了这些微调策略在各种知名医学基准测试中的有效性。值得注意的是，我们的医学LLM
    Med42在美国医学执照考试（USMLE）数据集上的准确率达到了72%，为公开可用的医学LLMs设立了新的性能标准。通过这项比较分析，我们旨在确定在医学领域微调LLMs的最有效和高效的方法，从而对AI驱动的医疗应用的进步做出重要贡献。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: In recent years, large language models (LLMs) have emerged as transformative
    tools, demonstrating remarkable proficiency in natural language understanding
    across a plethora of general-purpose applications, and sparking the interest in
    their use within specialized fields, particularly in the medical sector (Brown
    et al. [2020](#bib.bib4); Zhao et al. [2023](#bib.bib54); Zhu et al. [2023](#bib.bib56);
    Laskar et al. [2023](#bib.bib22)). Notably, these models, such as OpenAI’s GPT-3.5
    and GPT-4 (OpenAI [2023](#bib.bib36)), Google’s BARD (Driess et al. [2023](#bib.bib9))
    as well as various other non-proprietary models, while initially developed for
    general natural language processing tasks, have been evolving and adapting to
    address the nuanced and complex language of the medical domain (Nori et al. [2023](#bib.bib35);
    Singhal et al. [2022](#bib.bib40)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）作为变革性工具出现，在众多通用应用中展现了卓越的自然语言理解能力，并引发了在专业领域，特别是医学领域中的应用兴趣（Brown
    et al. [2020](#bib.bib4); Zhao et al. [2023](#bib.bib54); Zhu et al. [2023](#bib.bib56);
    Laskar et al. [2023](#bib.bib22)）。值得注意的是，这些模型，如OpenAI的GPT-3.5和GPT-4（OpenAI [2023](#bib.bib36)）、Google的BARD（Driess
    et al. [2023](#bib.bib9)）以及其他各种非专有模型，虽然最初是为了通用自然语言处理任务而开发，但已经在不断发展和适应，以应对医学领域的细微而复杂的语言（Nori
    et al. [2023](#bib.bib35); Singhal et al. [2022](#bib.bib40)）。
- en: To enhance the efficacy of LLMs for medical applications, researchers have recognized
    the importance of training and/or fine-tuning these models on large, in-domain
    datasets (Peng et al. [2023](#bib.bib38); Toma et al. [2023](#bib.bib44); Zhou
    et al. [2023](#bib.bib55)). The utilization of such datasets allows for nuanced
    understanding of both natural language and domain-specific terminologies, making
    them adept at interpreting and generating text that aligns with the intricacies
    of medical language. Albeit concerns about hallucinations and fabrications, biases
    and knowledge gaps, and risks about data privacy and ethics (Thirunavukarasu et al.
    [2023](#bib.bib43); Li et al. [2023a](#bib.bib25)), this specialized capability
    enables LLMs to be potentially employed in various healthcare applications. These
    include aiding diagnostic processes by analyzing and summarizing patient history
    and symptoms (Souza et al. [2023](#bib.bib42); Hirosawa et al. [2023](#bib.bib16)),
    interpreting medical literature and research papers for easier comprehension by
    healthcare professionals (Cascella et al. [2023](#bib.bib5); Bagde et al. [2023](#bib.bib2)),
    generating patient education materials tailored to individual needs (Ali et al.
    [2023](#bib.bib1); Miner, Laranjo, and Kocaballi [2020](#bib.bib34)), and assisting
    in the development and consultation of clinical guidelines and decision support
    systems (Wang et al. [2023](#bib.bib50); Hamed, Eid, and Alberry [2023](#bib.bib12)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升LLMs在医疗应用中的效果，研究人员认识到在大型领域内数据集上训练和/或微调这些模型的重要性（Peng等，[2023](#bib.bib38)；Toma等，[2023](#bib.bib44)；Zhou等，[2023](#bib.bib55)）。利用这些数据集可以对自然语言和领域特定术语有更细致的理解，使得它们能够解释和生成符合医疗语言复杂性的文本。尽管存在关于幻觉和捏造、偏见和知识差距、以及数据隐私和伦理风险的担忧（Thirunavukarasu等，[2023](#bib.bib43)；Li等，[2023a](#bib.bib25)），这种专业能力使得LLMs在各种医疗保健应用中具有潜在的应用价值。这些应用包括通过分析和总结患者历史及症状来协助诊断过程（Souza等，[2023](#bib.bib42)；Hirosawa等，[2023](#bib.bib16)），为医疗专业人员解读医学文献和研究论文（Cascella等，[2023](#bib.bib5)；Bagde等，[2023](#bib.bib2)），生成针对个体需求的患者教育材料（Ali等，[2023](#bib.bib1)；Miner,
    Laranjo, 和 Kocaballi，[2020](#bib.bib34)），以及协助制定和咨询临床指南及决策支持系统（Wang等，[2023](#bib.bib50)；Hamed,
    Eid, 和 Alberry，[2023](#bib.bib12)）。
- en: 'To this end, we focus on the development of a medical LLM, by comparing and
    analyzing two predominant fine-tuning methodologies: full-parameter fine-tuning
    and parameter-efficient tuning. Full-parameter fine-tuning is a comprehensive
    approach that involves adjusting all parameters of a pre-trained model, which
    demands substantial computational resources and time (Ding et al. [2023](#bib.bib8)).
    In contrast, parameter-efficient tuning methods, such as Adapters (Houlsby et al.
    [2019](#bib.bib17); Lin, Madotto, and Fung [2020](#bib.bib30)), Low-Rank Adaptation
    (LoRA) (Hu et al. [2022](#bib.bib18)), and Prompt-tuning (P-tuning), (Li and Liang
    [2021](#bib.bib26); Lester, Al-Rfou, and Constant [2021](#bib.bib24); Liu et al.
    [2023](#bib.bib31)) offer a more resource-efficient alternative by modifying a
    smaller subset of the model’s parameters. This study presents a detailed comparison
    of these methods, specifically within the context of medical LLMs. Our investigation
    includes experiments to assess the effectiveness of these tuning strategies, with
    a particular focus on the emerging LoRA technique. Through this comparative analysis,
    our objective is to identify the effective and efficient method for fine-tuning
    LLMs in the medical domain, ultimately contributing to the advancement of AI-driven
    healthcare applications. We are also releasing our most performant model Med42
    on HuggingFace¹¹1https://huggingface.co/m42-health/med42-70b.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们重点关注医疗LLM的开发，通过比较和分析两种主要的微调方法：全参数微调和参数高效微调。全参数微调是一种全面的方法，涉及调整预训练模型的所有参数，这需要大量的计算资源和时间（Ding等，[2023](#bib.bib8)）。相比之下，参数高效微调方法，如Adapters（Houlsby等，[2019](#bib.bib17)；Lin,
    Madotto, 和 Fung，[2020](#bib.bib30)），低秩适配（LoRA）（Hu等，[2022](#bib.bib18)），以及Prompt-tuning（P-tuning）（Li和Liang，[2021](#bib.bib26)；Lester,
    Al-Rfou, 和 Constant，[2021](#bib.bib24)；Liu等，[2023](#bib.bib31)）通过修改模型参数的较小子集，提供了一种资源效率更高的替代方案。本研究详细比较了这些方法，特别是在医疗LLM的背景下。我们的研究包括实验，以评估这些微调策略的有效性，特别关注新兴的LoRA技术。通过这种比较分析，我们的目标是找出在医疗领域微调LLMs的有效且高效的方法，*最终*促进AI驱动医疗保健应用的发展。我们还在HuggingFace上发布了我们性能最优的模型Med42¹¹1https://huggingface.co/m42-health/med42-70b。
- en: Methods
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: In this section, we elaborate on the dataset comprising our study, detailing
    its characteristics and the rationale for its selection. We outline the specific
    methodologies implemented for fine-tuning our large medical language model, including
    a comparison of full-parameter tuning and parameter-efficient techniques such
    as Low-Rank Adaptation (LoRA). Furthermore, we provide an exhaustive list of hyperparameters
    and configurations employed during our experiments, aiming to offer a transparent
    and replicable framework for subsequent research endeavors in this domain. This
    section aims to provide a comprehensive understanding of the technical aspects
    and decision-making processes underpinning our model’s development and evaluation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细说明了构成我们研究的数据集，描述了其特征及选择的理由。我们概述了微调我们的大型医学语言模型所采用的具体方法，包括对比完整参数调优和参数高效技术，如低秩适应（LoRA）。此外，我们提供了在实验过程中使用的超参数和配置的详尽列表，旨在为后续研究提供一个透明且可重复的框架。本节旨在全面了解支撑我们模型开发和评估的技术方面及决策过程。
- en: Training Dataset
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据集
- en: 'Our instruction-tuning dataset is a combination of multiple open datasets,
    primarily focused on medical question-answering data. It includes an extensive
    collection from medical forums, notably those within the Stack Exchange network,
    which are rich with expert discussions, patient inquiries, and specialist responses.
    Additionally, we incorporated selected sections from general domain datasets,
    meticulously extracting and integrating segments specifically related to medical
    topics. This composite approach ensures a diverse and robust dataset, encompassing
    a wide range of medical subfields and contexts, providing a comprehensive foundation
    for training our model to understand and generate medically-relevant content accurately.
    Further details about the training dataset are described in Appendix [A](#A1.SSx3
    "A.3 Training dataset ‣ Appendix A Appendix: Supplementary Materials ‣ Med42 -
    Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient
    Approaches").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的指令调优数据集是多个开放数据集的组合，主要集中在医学问答数据上。它包含了来自医疗论坛的大量数据，尤其是 Stack Exchange 网络中的那些论坛，这些论坛充满了专家讨论、患者询问和专家回答。此外，我们还从通用领域的数据集中挑选了相关部分，精心提取并整合了与医学话题相关的片段。这种综合方法确保了数据集的多样性和强健性，涵盖了广泛的医学子领域和背景，为训练我们的模型以准确理解和生成医学相关内容提供了全面的基础。有关训练数据集的更多详细信息请参见附录
    [A](#A1.SSx3 "A.3 Training dataset ‣ Appendix A Appendix: Supplementary Materials
    ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs.
    Parameter-Efficient Approaches")。'
- en: In order to make our model learn from instructions effectively, we employed
    a structured instruction format using the keywords <|system|>, <|prompter|>, and
    <|assistant|>. This format has been designed to teach the model the relationship
    between a given command and its appropriate output. By encapsulating the input
    under <|prompter|>, the intended system operation under <|system|>, and the expected
    output under <|assistant|>, we created a clear, directive framework that aids
    the model in understanding and executing tasks based on instructions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的模型有效地从指令中学习，我们采用了结构化的指令格式，使用了关键词 <|system|>, <|prompter|>, 和 <|assistant|>。这一格式旨在教会模型给定命令与其适当输出之间的关系。通过将输入封装在
    <|prompter|> 下，将预期系统操作封装在 <|system|> 下，以及将期望输出封装在 <|assistant|> 下，我们创建了一个清晰的指令框架，帮助模型理解和执行基于指令的任务。
- en: Modelling
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模
- en: Models.
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型。
- en: In this study, we built on the Llama-2 (Touvron et al. [2023b](#bib.bib46))
    family of models as the foundational architecture for fine-tuning. We specifically
    focused on the 7 billion (7B) and 70 billion (70B) parameter versions of these
    models. These versions were selected for their robust pre-training and scalability,
    allowing us to explore the impact of model size on performance in medical domain-specific
    tasks. Also, Llama-2 model comes with an open license, allowing for greater flexibility
    for adaptation and use in our research.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们以 Llama-2 (Touvron et al. [2023b](#bib.bib46)) 系列模型作为微调的基础架构。我们特别关注了这系列模型的
    70 亿 (7B) 和 700 亿 (70B) 参数版本。这些版本因其强大的预训练能力和可扩展性而被选中，使我们能够探索模型规模对医学领域特定任务性能的影响。此外，Llama-2
    模型具有开放许可，这为我们在研究中提供了更大的适应性和使用灵活性。
- en: LoRA.
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LoRA。
- en: Low-Rank Adaptation (LoRA) is a parameter-efficient training technique that
    targets the adaptation of pre-trained language models without the need for full
    model fine-tuning. Instead of updating all the parameters, LoRA focuses on a subset
    of the Transformer architecture. It introduces trainable low-rank matrices while
    keeping the pre-trained weights fixed. These matrices capture the essential changes
    needed to adapt the model to new tasks, effectively reducing the number of trainable
    parameters and thus computational costs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LoRA）是一种参数高效的训练技术，旨在在不需要完全微调模型的情况下适应预训练语言模型。LoRA不是更新所有参数，而是关注Transformer架构的一个子集。它引入了可训练的低秩矩阵，同时保持预训练权重固定。这些矩阵捕捉到适应模型新任务所需的核心变化，从而有效地减少了可训练参数的数量，并降低了计算成本。
- en: The selection of layers to which LoRA is applied constitutes a hyperparameter
    that requires careful tuning to optimize model performance. While it is common
    to see LoRA applied only to attention layers $v\_proj$ or only $gate\_proj$ as
    in (He et al. [2021](#bib.bib14); Lee, Hunter, and Ruiz [2023](#bib.bib23)), we
    achieved the best performance by applying it to every linear layer as in (Dettmers
    et al. [2023](#bib.bib7)). With these settings, the number of trainable parameters
    goes from 7 and 70 billion to 20 and 104 million, respectively. Details about
    the computational setup are available in Appendix 2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA应用于层的选择构成了一个超参数，需要仔细调整以优化模型性能。虽然常见的做法是仅将LoRA应用于注意力层$v\_proj$或仅$gate\_proj$（如（He等人
    [2021](#bib.bib14); Lee, Hunter和Ruiz [2023](#bib.bib23)）），我们通过将其应用于每个线性层（如（Dettmers等人
    [2023](#bib.bib7)））实现了最佳性能。使用这些设置，训练参数的数量从70亿和7亿分别减少到104百万和20百万。有关计算设置的详细信息，请参见附录2。
- en: Mask loss.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 掩码损失。
- en: 'In our methodology, every sample is composed of three elements: a system prompt,
    a user prompt, and a corresponding response. To optimize the use of the model’s
    available context length, we concatenate these samples across the entire training
    dataset. The training approach is autoregressive, focusing the backpropagation
    of loss exclusively on the tokens forming the responses. Consequently, this training
    strategy ensures that the model predominantly learns to generate answers, rather
    than the prompts.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法论中，每个样本由三个元素组成：系统提示、用户提示和相应的响应。为了优化模型可用的上下文长度，我们将这些样本在整个训练数据集中进行连接。训练方法是自回归的，专注于将损失的反向传播仅限于构成响应的标记。因此，这种训练策略确保了模型主要学习生成答案，而不是提示。
- en: Hyperparameters.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数。
- en: We train using the AdamW optimizer (Loshchilov and Hutter [2017](#bib.bib33)),
    with $\beta_{1}=0.9$ and $\alpha=16$. To speed up the training, we packed all
    of our fine-tuning data into chunks of 4,096 tokens.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AdamW优化器（Loshchilov和Hutter [2017](#bib.bib33)），其中$\beta_{1}=0.9$和$\alpha=16$。为了加速训练，我们将所有微调数据打包成4,096个标记的块。
- en: '![Refer to caption](img/f22a4187a4b3e587207ccf6ba625b065.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f22a4187a4b3e587207ccf6ba625b065.png)'
- en: 'Figure 1: Performance of 7-billion (left) and 70-billion (right) parameter
    models on various medical-related benchmark datasets (in zero-shot setting). Performance
    results (accuracy) are displayed in % for the base and fine-tuned models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：7亿（左）和70亿（右）参数模型在各种医学相关基准数据集上的表现（在零样本设置中）。性能结果（准确率）以%显示，分为基础模型和微调模型。
- en: Model evaluation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型评估
- en: To assess the performance of the fine-tuned language models, following previous
    works (Singhal et al. [2023](#bib.bib41); Chen et al. [2023](#bib.bib6); Toma
    et al. [2023](#bib.bib44)), we used Eleuther AI’s evaluation harness framework
    (Gao et al. [2021](#bib.bib11)) to compute their zero-shot performance across
    various commonly-used medical benchmarks. These contain medical exam questions
    and research datasets with multiple-choice answers, and include MedQA, HeadQA,
    MedMCQA, PubMedQA, MMLU clinical topics, and both self-assessment and sample exams
    from the United States Medical Licensing Examination (USMLE). All datasets are
    in the English language and all questions containing images were excluded. We
    describe these datasets in more detail below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估微调语言模型的性能，按照以往工作（Singhal等人 [2023](#bib.bib41); Chen等人 [2023](#bib.bib6);
    Toma等人 [2023](#bib.bib44)）的方法，我们使用了Eleuther AI的评估工具框架（Gao等人 [2021](#bib.bib11)）来计算它们在各种常用医学基准上的零样本性能。这些基准包括医学考试问题和具有多项选择答案的研究数据集，涵盖MedQA、HeadQA、MedMCQA、PubMedQA、MMLU临床主题，以及来自美国医学执照考试（USMLE）的自我评估和样本考试。所有数据集均为英文，所有包含图片的问题被排除在外。我们将在下文中更详细地描述这些数据集。
- en: MedQA.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MedQA。
- en: The dataset consists of multiple-choice (4 or 5) questions that resemble USMLE
    questions (from the National Medical Board Examination in the USA) and was originally
    designed for addressing medical problems (Jin et al. [2020](#bib.bib19)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含了类似于 USMLE 问题（来自美国国家医学委员会考试）的多项选择题（4 或 5 个选项），最初设计用于解决医学问题（Jin 等 [2020](#bib.bib19)）。
- en: HeadQA.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HeadQA。
- en: This multiple-choice question-answering dataset which is sourced from exams
    to access a specialized position in the Spanish healthcare system (Vilares and
    Gómez-Rodríguez [2019](#bib.bib47)). Only the English subset has been included
    in our evaluation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多项选择题问答数据集来源于西班牙医疗系统的专业职位考试（Vilares 和 Gómez-Rodríguez [2019](#bib.bib47)）。我们的评估中仅包含了英文子集。
- en: MedMCQA.
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MedMCQA。
- en: A large-scale multiple-choice questions dataset (4-choices) from the Indian
    medical entrance examinations (Pal, Umapathi, and Sankarasubbu [2022](#bib.bib37)),
    covering 21 medical subjects and more than 2,000 healthcare topics. We report
    the performance of our models on the validation set (given that the available
    test dataset does not contain answers to the questions), which has been excluded
    from our training (fine-tuning) dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大规模的多项选择题数据集（4 选择），来源于印度医学入学考试（Pal, Umapathi 和 Sankarasubbu [2022](#bib.bib37)），覆盖
    21 个医学科目和 2000 多个医疗话题。我们报告了模型在验证集上的表现（鉴于可用的测试数据集不包含问题的答案），该验证集已被排除在我们的训练（微调）数据集之外。
- en: PubMedQA.
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PubMedQA。
- en: The task of PubMedQA is to answer research questions with yes/no/maybe using
    abstracts from medical scientific literature (Jin et al. [2019](#bib.bib20));
    i.e., given an abstract and a related question, the task is to provide a short
    answer of yes, no or maybe.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PubMedQA 的任务是使用医学科学文献的摘要回答研究问题，回答选项包括 yes/no/maybe（Jin 等 [2019](#bib.bib20)）；即，给定一个摘要和相关问题，任务是提供一个简短的答案：是、否或也许。
- en: MMLU clinical topics.
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MMLU 临床主题。
- en: The widely-used Measuring Multitask Language Understanding (MMLU) benchmark
    (Hendrycks et al. [2021](#bib.bib15)) aimed to introduce a comprehensive assessment
    of LLMs across 57 subjects. For our evaluation, we selected clinical topics covering
    clinical knowledge, college biology, college medicine, medical genetics, professional
    medicine and anatomy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的测量多任务语言理解（MMLU）基准（Hendrycks 等 [2021](#bib.bib15)）旨在对 57 个学科中的 LLMs 进行全面评估。在我们的评估中，我们选择了涵盖临床知识、大学生物学、大学医学、医学遗传学、专业医学和解剖学的临床主题。
- en: USMLE sample exam and self-assessment.
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: USMLE 样本考试和自我评估。
- en: These are two sets of official practice materials for the United States Medical
    Licensing Examination (USMLE), which is an examination program that contains three
    steps for assessing medical competency (Nori et al. [2023](#bib.bib35); Han et al.
    [2023](#bib.bib13)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是美国医学执照考试（USMLE）的两套官方练习材料，该考试程序包含三步骤用于评估医学能力（Nori 等 [2023](#bib.bib35); Han
    等 [2023](#bib.bib13)）。
- en: 'We also address a growing concern in the field of LLM fine-tuning: the inadvertent
    inclusion of similar or identical samples in both training and evaluation datasets.
    To address this, we implemented a “decontamination pipeline”, which is designed
    to scrutinize the evaluation dataset and flag any examples that have a significant
    resemblance to those found in our instruction-tuning dataset. These flagged examples
    are regarded as “contaminated samples”. To ensure the integrity of our evaluation,
    we also show the performance metrics calculated after the removal of these contaminated
    samples from the evaluation datasets. Detailed information about the decontamination
    process can be found in Appendix.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还解决了在 LLM 微调领域中一个日益严重的问题：训练数据集和评估数据集中不经意间包含相似或相同样本的问题。为了解决这个问题，我们实施了一个“去污染管道”，旨在审查评估数据集，并标记出与我们指导微调数据集中的样本有显著相似性的例子。这些标记的例子被视为“污染样本”。为了确保评估的完整性，我们还展示了在从评估数据集中移除这些污染样本后计算的性能指标。有关去污染过程的详细信息可以在附录中找到。
- en: Results
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'The performance of the fine-tuned models across the different benchmark datasets
    is represented in Figure [1](#Sx2.F1 "Figure 1 ‣ Hyperparameters. ‣ Modelling
    ‣ Methods ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches"), showing the accuracy values obtained for
    both 7B and 70B-parameter models. The results show the superiority of the fine-tuned
    models over their corresponding base models across all medical benchmark datasets.
    Notably, our analysis reveals that full-parameter fine-tuning outperforms the
    parameter-efficient fine-tuning approach, LoRA, in the majority of these datasets.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 细化模型在不同基准数据集上的表现如图[1](#Sx2.F1 "图 1 ‣ 超参数。 ‣ 建模 ‣ 方法 ‣ Med42 - 评估医学LLMs的微调策略：全参数与参数高效方法")所示，展示了7B和70B参数模型获得的准确度值。结果表明，细化模型在所有医学基准数据集上优于其对应的基础模型。值得注意的是，我们的分析表明，在这些数据集中的大多数情况下，全参数微调优于参数高效微调方法LoRA。
- en: '| Dataset | PE-FT | FP-FT (Med42) | ClinCamel¹ | MediTron^(2†) | GPT-3.5³ |
    GPT-4³ | MedPaLM-2^(4†) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | PE-FT | FP-FT（Med42） | ClinCamel¹ | MediTron^(2†) | GPT-3.5³ | GPT-4³
    | MedPaLM-2^(4†) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MMLU (average) | 76.7 | 76.7 | 69.8 | 71.5 | 66.6 | 87.0 | 89.4 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| MMLU（平均） | 76.7 | 76.7 | 69.8 | 71.5 | 66.6 | 87.0 | 89.4 |'
- en: '|   Clinical knowledge | 76.6 | 74.3 | 69.8 | - | 69.8 | 86.0 | 88.3 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|   临床知识 | 76.6 | 74.3 | 69.8 | - | 69.8 | 86.0 | 88.3 |'
- en: '|   College biology | 83.3 | 84.0 | 79.2 | - | 72.2 | 95.1 | 94.4 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|   大学生物学 | 83.3 | 84.0 | 79.2 | - | 72.2 | 95.1 | 94.4 |'
- en: '|   College medicine | 72.8 | 68.8 | 67.0 | - | 61.3 | 76.9 | 80.9 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|   大学医学 | 72.8 | 68.8 | 67.0 | - | 61.3 | 76.9 | 80.9 |'
- en: '|   Medical genetics | 80.0 | 86.0 | 69.0 | - | 70.0 | 91.0 | 90.0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|   医学遗传学 | 80.0 | 86.0 | 69.0 | - | 70.0 | 91.0 | 90.0 |'
- en: '|   Professional medicine | 80.1 | 79.8 | 71.3 | - | 70.2 | 93.0 | 95.2 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|   专业医学 | 80.1 | 79.8 | 71.3 | - | 70.2 | 93.0 | 95.2 |'
- en: '|   Anatomy | 67.4 | 67.4 | 62.2 | - | 56.3 | 80.0 | 77.8 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|   解剖学 | 67.4 | 67.4 | 62.2 | - | 56.3 | 80.0 | 77.8 |'
- en: '| HeadQA | 70.6 | 72.0 | - | - | - | - | - |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| HeadQA | 70.6 | 72.0 | - | - | - | - | - |'
- en: '| MedMCQA | 54.7 | 60.9 | 47.0 | 53.3 | 50.1 | 69.5 | 71.3 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| MedMCQA | 54.7 | 60.9 | 47.0 | 53.3 | 50.1 | 69.5 | 71.3 |'
- en: '| MedQA | 59.1 | 61.5 | 53.4 | 52.0 | 50.8 | 78.9 | 79.7 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| MedQA | 59.1 | 61.5 | 53.4 | 52.0 | 50.8 | 78.9 | 79.7 |'
- en: '| PubMedQA | 75.8 | 76.8 | 74.3 | 79.8 | 71.6 | 75.2 | 79.2 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| PubMedQA | 75.8 | 76.8 | 74.3 | 79.8 | 71.6 | 75.2 | 79.2 |'
- en: '| USMLE (average) | 68.3 | 71.9 | - | - | 53.1 | 84.1 | - |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| USMLE（平均） | 68.3 | 71.9 | - | - | 53.1 | 84.1 | - |'
- en: '|   Self-assessment | 68.0 | 71.7 | - | - | 49.1 | 83.8 | - |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|   自我评估 | 68.0 | 71.7 | - | - | 49.1 | 83.8 | - |'
- en: '|   Sample exam | 68.6 | 72.0 | 54.3 | - | 56.9 | 84.3 | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|   样本考试 | 68.6 | 72.0 | 54.3 | - | 56.9 | 84.3 | - |'
- en: 'Table 1: Zero-shot performance comparison between both parameter-efficient
    (PE-FT) and full-parameter (FP-FT) fine-tuned (Llama2 70-B) models with other
    published models across the various medical benchmark datasets. ¹Clinical Camel
    (Toma et al. [2023](#bib.bib44)); ²MediTron (Chen et al. [2023](#bib.bib6)); ³GPT-3.5
    and GPT-4 (Nori et al. [2023](#bib.bib35)); ⁴MedPaLM-2 (Singhal et al. [2023](#bib.bib41)).
    ^†We note that zero-shot performance is not reported for these models; few-shot
    results are shown.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：参数高效（PE-FT）与全参数（FP-FT）微调（Llama2 70-B）模型在各种医学基准数据集上的零-shot性能比较。¹临床骆驼（Toma等，[2023](#bib.bib44)）；²MediTron（Chen等，[2023](#bib.bib6)）；³GPT-3.5
    和 GPT-4（Nori等，[2023](#bib.bib35)）；⁴MedPaLM-2（Singhal等，[2023](#bib.bib41)）。^†我们注意到这些模型的零-shot性能未报告；展示了少量样本结果。
- en: 'The comparative results from our experiments, illustrating the performance
    of our best fine-tuned models (70B-parameter models) against that of other models,
    are detailed in Table [1](#Sx3.T1 "Table 1 ‣ Results ‣ Med42 - Evaluating Fine-Tuning
    Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches").
    Changes in the accuracy scores after the removal of contaminated examples are
    shown in Figure [2](#Sx3.F2 "Figure 2 ‣ Results ‣ Med42 - Evaluating Fine-Tuning
    Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的对比结果，展示了我们最佳细化模型（70B参数模型）与其他模型的表现，详细见表[1](#Sx3.T1 "表 1 ‣ 结果 ‣ Med42 - 评估医学LLMs的微调策略：全参数与参数高效方法")。去除污染样本后的准确度变化见图[2](#Sx3.F2
    "图 2 ‣ 结果 ‣ Med42 - 评估医学LLMs的微调策略：全参数与参数高效方法")。
- en: '![Refer to caption](img/aaea9a6d980cddda13346c4fc1c42048.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aaea9a6d980cddda13346c4fc1c42048.png)'
- en: 'Figure 2: Accuracy change after decontamination for both (70b) fine-tuned models
    (shown in %).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：去污染后70b细化模型的准确度变化（以百分比表示）。
- en: Discussion
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The findings of this study underscore the efficacy of comprehensive fine-tuning
    strategies in enhancing the performance of language models. Importantly, we report
    domain-adapted fine-tuned medical LLMs that demonstrate high-level medical reasoning
    and improved domain-specific benchmark performance, particularly, in medical complex
    tasks such as USMLE-based questions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究的发现突显了综合微调策略在提高语言模型性能方面的有效性。重要的是，我们报告了经过领域适配的微调医学 LLM，它们展示了高级医学推理能力和改进的领域特定基准表现，尤其是在诸如基于
    USMLE 的复杂医学任务中。
- en: 'Overall, full-parameter fine-tuning achieved better performance than parameter-efficient
    fine-tuning in medical tasks (Figure [1](#Sx2.F1 "Figure 1 ‣ Hyperparameters.
    ‣ Modelling ‣ Methods ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical
    LLMs: Full-Parameter vs. Parameter-Efficient Approaches")). However, it is noteworthy
    that parameter-efficient fine-tuning methods, such as LoRA, yield results that
    are remarkably close to those achieved by full-parameter fine-tuning, consistent
    with findings in other studies (Fu et al. [2023](#bib.bib10); Liao, Meng, and
    Monz [2023](#bib.bib29)). These findings suggest that parameter-efficient approaches
    can be viable alternatives, particularly in scenarios where computational resources
    are limited.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '总体而言，在医学任务中，全参数微调的表现优于参数高效微调（见图 [1](#Sx2.F1 "Figure 1 ‣ Hyperparameters. ‣
    Modelling ‣ Methods ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs:
    Full-Parameter vs. Parameter-Efficient Approaches")）。然而，需要注意的是，像 LoRA 这样的参数高效微调方法所取得的结果与全参数微调相差无几，这与其他研究中的发现一致（Fu
    等 [2023](#bib.bib10)；Liao、Meng 和 Monz [2023](#bib.bib29)）。这些发现表明，参数高效的方法可以作为可行的替代方案，特别是在计算资源有限的情况下。'
- en: 'A critical aspect of our study was the thorough examination of potential test
    set contamination. We analyzed whether our evaluation datasets contained examples
    that were either identical or strikingly similar to those in the training set,
    and re-evaluated our models on the “decontaminated” dataset. We observed that
    a small number of samples were deemed to be “contaminated” (Table A[2](#A1.T2
    "Table 2 ‣ A.4 Decontamination analysis ‣ Appendix A Appendix: Supplementary Materials
    ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs.
    Parameter-Efficient Approaches")), which resulted in very small changes in the
    accuracy scores for the two larger fine-tuned models across the benchmark datasets
    (Figure [2](#Sx3.F2 "Figure 2 ‣ Results ‣ Med42 - Evaluating Fine-Tuning Strategies
    for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches")). This process
    ensures the robustness and integrity of our analysis, affirming the reliability
    of our findings and the trained models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究的一个关键方面是对潜在测试集污染的彻底检查。我们分析了我们的评估数据集中是否包含与训练集中的样本完全相同或极其相似的例子，并在“去污染”数据集上重新评估了我们的模型。我们观察到少量样本被认为是“污染的”（见表
    A[2](#A1.T2 "Table 2 ‣ A.4 Decontamination analysis ‣ Appendix A Appendix: Supplementary
    Materials ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches")），这导致两种大型微调模型在基准数据集上的准确率变化非常小（见图 [2](#Sx3.F2
    "Figure 2 ‣ Results ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs:
    Full-Parameter vs. Parameter-Efficient Approaches")）。这一过程确保了我们分析的稳健性和完整性，验证了我们的发现和训练模型的可靠性。'
- en: Moreover, our study encompassed a comparative analysis of the performance of
    fine-tuned models against other LLMs, including those commercially available.
    This comparison provides a more comprehensive view of the standing of our fine-tuned
    models in the current landscape of openly available LLMs, particularly in medical
    applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的研究还涵盖了微调模型与其他 LLM，包括商业上可用的模型的性能比较。这一比较提供了对我们微调模型在当前公开可用的 LLM 领域，尤其是在医学应用中的位置的更全面的视角。
- en: This research also underscores the importance of creating a large and well-structured
    instruction fine-tuning dataset. As instruct fine-tuning of open-source LLMs becomes
    a *de facto* standard practice, our results show that our model exhibits superior
    performance compared to established names like ClinicalCamel (Toma et al. [2023](#bib.bib44)),
    MediTron (Chen et al. [2023](#bib.bib6)) and GatorTronGPT (Peng et al. [2023](#bib.bib38)).
    Our approach involved minimal experimentation with complex prompt engineering;
    however, we believe there are additional opportunities to enhance the model’s
    response quality and accuracy. These opportunities could be explored in future
    research to achieve greater advancements.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究还强调了创建一个大型且结构良好的指令微调数据集的重要性。随着开源LLMs的指令微调成为*事实上的*标准实践，我们的结果显示，相较于ClinicalCamel（Toma等人
    [2023](#bib.bib44)）、MediTron（Chen等人 [2023](#bib.bib6)）和GatorTronGPT（Peng等人 [2023](#bib.bib38)）等成熟模型，我们的模型表现更为出色。我们的方法涉及了最小化复杂的提示工程实验；然而，我们相信还有额外的机会来提升模型的响应质量和准确性。这些机会可以在未来的研究中进一步探索，以实现更大的进步。
- en: While our results regarding the model’s performance on medical question-answering
    benchmarks are promising, they also highlight areas for future exploration. Our
    study serves as a stepping stone, indicating the potential of these models in
    diverse applications. However, further research is necessary to fully understand
    and demonstrate the utility of these models in other practical use-cases. Such
    investigations are crucial for advancing the field and realizing the full potential
    of fine-tuned LLMs in a variety of domains.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们关于模型在医学问答基准测试中的表现的结果是有希望的，但也突显了未来探索的领域。我们的研究作为一个踏脚石，表明了这些模型在多种应用中的潜力。然而，进一步的研究对于完全理解和展示这些模型在其他实际应用中的效用是必要的。这类调查对推动该领域的发展以及实现微调LLMs在各种领域的全部潜力至关重要。
- en: Ethical Considerations and Reproducibility
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑与可重复性
- en: In this work, we underscore the critical importance of ethical considerations
    and reproducibility in the development of our large language model. Firstly, our
    model is released under an open license, ensuring public accessibility and fostering
    a culture of transparency and collaboration within the research community. This
    approach not only democratizes access to advanced technology but also invites
    scrutiny and diverse perspectives, which are vital for ethical development. Additionally,
    the datasets and frameworks used in our model’s evaluation are freely available,
    promoting reproducibility and allowing independent verification of our findings.
    This transparency in data and methodologies is essential to maintain scientific
    integrity and foster trust in AI research. Furthermore, we recognize and openly
    acknowledge the limitations of our model, particularly its readiness for use in
    clinical practice. We emphasize that while our model shows promising results,
    it should not yet be used as a decision-making tool in clinical environments.
    There is the potential for generating incorrect or harmful information and the
    risk of perpetuating biases in training data. This acknowledgment stems from our
    commitment to responsible AI development, where the safety and well-being of end-users
    are paramount. Ongoing human evaluation efforts are focused on rigorously assessing
    biases, fairness, and safety through red-teaming the model to ensure its robustness
    and reliability in clinical settings. By addressing these ethical concerns and
    emphasizing reproducibility, we aim to contribute positively to the field of AI,
    ensuring that advancements are made with a keen awareness of their societal impacts
    and limitations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们强调了在开发大型语言模型过程中伦理考量和可重复性的重要性。首先，我们的模型在开放许可下发布，确保了公众可访问性，并促进了研究社区内的透明度和合作文化。这种方法不仅使先进技术的获取民主化，还邀请了审查和多样化的视角，这对于伦理发展至关重要。此外，我们模型评估中使用的数据集和框架是公开可用的，促进了可重复性，并允许独立验证我们的发现。数据和方法的透明性对于维护科学诚信和促进对
    AI 研究的信任至关重要。此外，我们认识并公开承认我们模型的局限性，特别是在临床实践中的适用性。我们强调，尽管我们的模型显示出有希望的结果，但尚不应在临床环境中作为决策工具使用。存在生成不正确或有害信息的风险，并可能固化训练数据中的偏见。这一承认源于我们对负责任
    AI 发展的承诺，其中最终用户的安全和福祉至关重要。持续的人为评估工作专注于通过红队测试模型，严格评估偏见、公平性和安全性，以确保其在临床环境中的稳健性和可靠性。通过解决这些伦理问题并强调可重复性，我们旨在积极推动
    AI 领域的发展，确保进步在充分意识到其社会影响和局限性的情况下进行。
- en: References
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ali et al. (2023) Ali, S. R.; Dobbs, T. D.; Hutchings, H. A.; and Whitaker,
    I. S. 2023. Using ChatGPT to write patient clinic letters. *Lancet Digit Health*,
    5(4): e179–e181.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ali 等（2023）Ali, S. R.; Dobbs, T. D.; Hutchings, H. A.; 和 Whitaker, I. S. 2023.
    使用 ChatGPT 编写患者诊所信函。*Lancet Digit Health*, 5(4): e179–e181。'
- en: 'Bagde et al. (2023) Bagde, H.; Dhopte, A.; Alam, M. K.; and Basri, R. 2023.
    A systematic review and meta-analysis on ChatGPT and its utilization in medical
    and dental research. *Heliyon*, 9(12): e23050.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bagde 等（2023）Bagde, H.; Dhopte, A.; Alam, M. K.; 和 Basri, R. 2023. 关于 ChatGPT
    及其在医学和牙科研究中应用的系统评价和荟萃分析。*Heliyon*, 9(12): e23050。'
- en: 'Ben Abacha and Demner-Fushman (2019) Ben Abacha, A.; and Demner-Fushman, D.
    2019. A Question-Entailment Approach to Question Answering. *BMC Bioinform.*,
    20(1): 511:1–511:23.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben Abacha 和 Demner-Fushman（2019）Ben Abacha, A.; 和 Demner-Fushman, D. 2019.
    一种基于问题蕴含的方法来回答问题。*BMC Bioinform.*, 20(1): 511:1–511:23。'
- en: 'Brown et al. (2020) Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
    Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020.
    Language models are few-shot learners. *Advances in neural information processing
    systems*, 33: 1877–1901.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等（2020）Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal,
    P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; 等. 2020. 语言模型是少量样本学习者。*Advances
    in neural information processing systems*, 33: 1877–1901。'
- en: 'Cascella et al. (2023) Cascella, M.; Montomoli, J.; Bellini, V.; and Bignami,
    E. 2023. Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple
    Clinical and Research Scenarios. *J. Med. Syst.*, 47(1): 33.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cascella 等（2023）Cascella, M.; Montomoli, J.; Bellini, V.; 和 Bignami, E. 2023.
    评估 ChatGPT 在医疗保健中的可行性：对多个临床和研究场景的分析。*J. Med. Syst.*, 47(1): 33。'
- en: 'Chen et al. (2023) Chen, Z.; Cano, A. H.; Romanou, A.; Bonnet, A.; Matoba,
    K.; Salvi, F.; Pagliardini, M.; Fan, S.; Köpf, A.; Mohtashami, A.; Sallinen, A.;
    Sakhaeirad, A.; Swamy, V.; Krawczuk, I.; Bayazit, D.; Marmet, A.; Montariol, S.;
    Hartley, M.-A.; Jaggi, M.; and Bosselut, A. 2023. MEDITRON-70B: Scaling Medical
    Pretraining for Large Language Models. arXiv:2311.16079.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2023）Chen, Z.; Cano, A. H.; Romanou, A.; Bonnet, A.; Matoba, K.; Salvi,
    F.; Pagliardini, M.; Fan, S.; Köpf, A.; Mohtashami, A.; Sallinen, A.; Sakhaeirad,
    A.; Swamy, V.; Krawczuk, I.; Bayazit, D.; Marmet, A.; Montariol, S.; Hartley,
    M.-A.; Jaggi, M.; 和 Bosselut, A. 2023. MEDITRON-70B: 扩展大型语言模型的医学预训练。arXiv:2311.16079。'
- en: 'Dettmers et al. (2023) Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
    L. 2023. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023）Dettmers, T.; Pagnoni, A.; Holtzman, A.; 和 Zettlemoyer, L.
    2023. Qlora: 高效的量化大语言模型微调。*arXiv 预印本 arXiv:2305.14314*。'
- en: 'Ding et al. (2023) Ding, N.; Qin, Y.; Yang, G.; Wei, F.; Yang, Z.; Su, Y.;
    Hu, S.; Chen, Y.; Chan, C.-M.; Chen, W.; Yi, J.; Zhao, W.; Wang, X.; Liu, Z.;
    Zheng, H.-T.; Chen, J.; Liu, Y.; Tang, J.; Li, J.; and Sun, M. 2023. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*,
    5(3): 220–235.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等（2023）Ding, N.; Qin, Y.; Yang, G.; Wei, F.; Yang, Z.; Su, Y.; Hu, S.;
    Chen, Y.; Chan, C.-M.; Chen, W.; Yi, J.; Zhao, W.; Wang, X.; Liu, Z.; Zheng, H.-T.;
    Chen, J.; Liu, Y.; Tang, J.; Li, J.; 和 Sun, M. 2023. 大规模预训练语言模型的参数高效微调。*自然机器智能*，5(3):
    220–235。'
- en: 'Driess et al. (2023) Driess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdhery,
    A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; Huang, W.; Chebotar,
    Y.; Sermanet, P.; Duckworth, D.; Levine, S.; Vanhoucke, V.; Hausman, K.; Toussaint,
    M.; Greff, K.; Zeng, A.; Mordatch, I.; and Florence, P. 2023. PaLM-E: An Embodied
    Multimodal Language Model. arXiv:2303.03378.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess 等（2023）Driess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdhery,
    A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; Huang, W.; Chebotar,
    Y.; Sermanet, P.; Duckworth, D.; Levine, S.; Vanhoucke, V.; Hausman, K.; Toussaint,
    M.; Greff, K.; Zeng, A.; Mordatch, I.; 和 Florence, P. 2023. PaLM-E: 具身的多模态语言模型。arXiv:2303.03378。'
- en: 'Fu et al. (2023) Fu, Z.; Yang, H.; So, A. M.-C.; Lam, W.; Bing, L.; and Collier,
    N. 2023. On the Effectiveness of Parameter-Efficient Fine-Tuning. *AAAI*, 37(11):
    12799–12807.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等（2023）Fu, Z.; Yang, H.; So, A. M.-C.; Lam, W.; Bing, L.; 和 Collier, N.
    2023. 关于参数高效微调的有效性。*AAAI*，37(11): 12799–12807。'
- en: Gao et al. (2021) Gao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Foster,
    C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff, N.; Phang, J.; Reynolds,
    L.; Tang, E.; Thite, A.; Wang, B.; Wang, K.; and Zou, A. 2021. A framework for
    few-shot language model evaluation.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Gao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Foster, C.;
    Golding, L.; Hsu, J.; McDonell, K.; Muennighoff, N.; Phang, J.; Reynolds, L.;
    Tang, E.; Thite, A.; Wang, B.; Wang, K.; 和 Zou, A. 2021. 一个少量样本语言模型评估框架。
- en: 'Hamed, Eid, and Alberry (2023) Hamed, E.; Eid, A.; and Alberry, M. 2023. Exploring
    ChatGPT’s Potential in Facilitating Adaptation of Clinical Guidelines: A Case
    Study of Diabetic Ketoacidosis Guidelines. *Cureus*, 15(5): e38784.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hamed、Eid 和 Alberry（2023）Hamed, E.; Eid, A.; 和 Alberry, M. 2023. 探索 ChatGPT
    在促进临床指南适应中的潜力：糖尿病酮症酸中毒指南的案例研究。*Cureus*，15(5): e38784。'
- en: Han et al. (2023) Han, T.; Adams, L. C.; Papaioannou, J.-M.; Grundmann, P.;
    Oberhauser, T.; Löser, A.; Truhn, D.; and Bressem, K. K. 2023. MedAlpaca–An Open-Source
    Collection of Medical Conversational AI Models and Training Data. *arXiv preprint
    arXiv:2304.08247*.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等（2023）Han, T.; Adams, L. C.; Papaioannou, J.-M.; Grundmann, P.; Oberhauser,
    T.; Löser, A.; Truhn, D.; 和 Bressem, K. K. 2023. MedAlpaca–一个开源医疗对话 AI 模型及训练数据的集合。*arXiv
    预印本 arXiv:2304.08247*。
- en: He et al. (2021) He, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; and Neubig,
    G. 2021. Towards a unified view of parameter-efficient transfer learning. *arXiv
    preprint arXiv:2110.04366*.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2021）He, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; 和 Neubig, G. 2021.
    朝着统一的参数高效转移学习视角迈进。*arXiv 预印本 arXiv:2110.04366*。
- en: Hendrycks et al. (2021) Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,
    M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding.
    *Proceedings of the International Conference on Learning Representations (ICLR)*.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021）Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;
    Song, D.; 和 Steinhardt, J. 2021. 测量大规模多任务语言理解。*国际学习表征会议（ICLR）论文集*。
- en: 'Hirosawa et al. (2023) Hirosawa, T.; Harada, Y.; Yokose, M.; Sakamoto, T.;
    Kawamura, R.; and Shimizu, T. 2023. Diagnostic Accuracy of Differential-Diagnosis
    Lists Generated by Generative Pretrained Transformer 3 Chatbot for Clinical Vignettes
    with Common Chief Complaints: A Pilot Study. *Int. J. Environ. Res. Public Health*,
    20(4).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirosawa 等（2023）Hirosawa, T.; Harada, Y.; Yokose, M.; Sakamoto, T.; Kawamura,
    R.; 和 Shimizu, T. 2023. 生成预训练变换器 3 聊天机器人为临床病例生成的差异诊断清单的诊断准确性：初步研究。*国际环境研究与公共卫生期刊*，20(4)。
- en: Houlsby et al. (2019) Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;
    de Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019. Parameter-Efficient
    Transfer Learning for NLP.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等 (2019) Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; de
    Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; 和 Gelly, S. 2019. 参数高效的 NLP 迁移学习。
- en: 'Hu et al. (2022) Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,
    S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models.
    In *International Conference on Learning Representations*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2022) Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.;
    Wang, L.; 和 Chen, W. 2022. LoRA: 大型语言模型的低秩适应。在*国际学习表示会议*。'
- en: Jin et al. (2020) Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and
    Szolovits, P. 2020. What Disease does this Patient Have? A Large-scale Open Domain
    Question Answering Dataset from Medical Exams. *arXiv preprint arXiv:2009.13081*.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等 (2020) Jin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; 和 Szolovits,
    P. 2020. 这个病人得了什么病？一个来自医学考试的大规模开放领域问答数据集。*arXiv 预印本 arXiv:2009.13081*。
- en: 'Jin et al. (2019) Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; and Lu, X. 2019.
    PubMedQA: A Dataset for Biomedical Research Question Answering. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    2567–2577.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等 (2019) Jin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; 和 Lu, X. 2019. PubMedQA:
    一个用于生物医学研究问题回答的数据集。在*2019年自然语言处理经验方法会议暨第9届国际自然语言处理联合会议 (EMNLP-IJCNLP)*，2567–2577。'
- en: Lambert et al. (2023) Lambert, N.; Tunstall, L.; Rajani, N.; and Thrush, T.
    2023. HuggingFace H4 Stack Exchange Preference Dataset.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambert 等 (2023) Lambert, N.; Tunstall, L.; Rajani, N.; 和 Thrush, T. 2023. HuggingFace
    H4 Stack Exchange 偏好数据集。
- en: Laskar et al. (2023) Laskar, M. T. R.; Bari, M. S.; Rahman, M.; Bhuiyan, M.
    A. H.; Joty, S.; and Huang, J. X. 2023. A Systematic Study and Comprehensive Evaluation
    of ChatGPT on Benchmark Datasets. arXiv:2305.18486.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskar 等 (2023) Laskar, M. T. R.; Bari, M. S.; Rahman, M.; Bhuiyan, M. A. H.;
    Joty, S.; 和 Huang, J. X. 2023. ChatGPT 在基准数据集上的系统研究与全面评估。arXiv:2305.18486。
- en: 'Lee, Hunter, and Ruiz (2023) Lee, A. N.; Hunter, C. J.; and Ruiz, N. 2023.
    Platypus: Quick, cheap, and powerful refinement of llms. *arXiv preprint arXiv:2308.07317*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee, Hunter 和 Ruiz (2023) Lee, A. N.; Hunter, C. J.; 和 Ruiz, N. 2023. Platypus:
    快速、廉价且强大的 llms 精炼。*arXiv 预印本 arXiv:2308.07317*。'
- en: 'Lester, Al-Rfou, and Constant (2021) Lester, B.; Al-Rfou, R.; and Constant,
    N. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, 3045–3059\.
    Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lester, Al-Rfou 和 Constant (2021) Lester, B.; Al-Rfou, R.; 和 Constant, N. 2021.
    参数高效提示调优的规模效应。在*2021年自然语言处理经验方法会议论文集*，3045–3059。在线与多米尼加共和国蓬塔卡纳: 计算语言学协会。'
- en: 'Li et al. (2023a) Li, H.; Moon, J. T.; Purkayastha, S.; Celi, L. A.; Trivedi,
    H.; and Gichoya, J. W. 2023a. Ethics of large language models in medicine and
    medical research. *Lancet Digit Health*, 5(6): e333–e335.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023a) Li, H.; Moon, J. T.; Purkayastha, S.; Celi, L. A.; Trivedi, H.;
    和 Gichoya, J. W. 2023a. 大型语言模型在医学和医学研究中的伦理问题。*柳叶刀数字健康*, 5(6): e333–e335。'
- en: 'Li and Liang (2021) Li, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing
    Continuous Prompts for Generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, 4582–4597\.
    Online: Association for Computational Linguistics.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang (2021) Li, X. L.; 和 Liang, P. 2021. Prefix-Tuning: 优化生成任务中的连续提示。在*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议论文集
    (卷1: 长论文)*，4582–4597。在线: 计算语言学协会。'
- en: 'Li et al. (2023b) Li, Y.; Li, Z.; Zhang, K.; Dan, R.; Jiang, S.; and Zhang,
    Y. 2023b. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model
    Meta-AI (LLaMA) Using Medical Domain Knowledge. *Cureus*, 15(6): e40895.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023b) Li, Y.; Li, Z.; Zhang, K.; Dan, R.; Jiang, S.; 和 Zhang, Y. 2023b.
    ChatDoctor: 一个基于大型语言模型 Meta-AI (LLaMA) 的医疗聊天模型，结合医疗领域知识进行微调。*Cureus*, 15(6): e40895。'
- en: 'Lian et al. (2023) Lian, W.; Goodson, B.; Pentland, E.; Cook, A.; Vong, C.;
    and ”Teknium”. 2023. OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning
    Traces. https://https://huggingface.co/Open-Orca/OpenOrca.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lian 等 (2023) Lian, W.; Goodson, B.; Pentland, E.; Cook, A.; Vong, C.; 和 ”Teknium”.
    2023. OpenOrca: 一个基于 GPT 增强的 FLAN 推理痕迹的开放数据集。https://huggingface.co/Open-Orca/OpenOrca。'
- en: Liao, Meng, and Monz (2023) Liao, B.; Meng, Y.; and Monz, C. 2023. Parameter-Efficient
    Fine-Tuning without Introducing New Latency. arXiv:2305.16742.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao, Meng, 和 Monz (2023) Liao, B.; Meng, Y.; 和 Monz, C. 2023. 参数高效微调而不引入新延迟。arXiv:2305.16742。
- en: 'Lin, Madotto, and Fung (2020) Lin, Z.; Madotto, A.; and Fung, P. 2020. Exploring
    Versatile Generative Language Model Via Parameter-Efficient Transfer Learning.
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, 441–459\.
    Online: Association for Computational Linguistics.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin, Madotto, 和 Fung (2020) Lin, Z.; Madotto, A.; 和 Fung, P. 2020. 通过参数高效的迁移学习探索多功能生成语言模型。见
    *计算语言学协会会议论文：EMNLP 2020*, 441–459\. 在线：计算语言学协会。
- en: Liu et al. (2023) Liu, X.; Zheng, Y.; Du, Z.; Ding, M.; Qian, Y.; Yang, Z.;
    and Tang, J. 2023. GPT understands, too. *AI Open*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Liu, X.; Zheng, Y.; Du, Z.; Ding, M.; Qian, Y.; Yang, Z.; 和 Tang,
    J. 2023. GPT 也能理解。*AI Open*。
- en: 'Longpre et al. (2023) Longpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.;
    Tay, Y.; Zhou, D.; Le, Q. V.; Zoph, B.; Wei, J.; et al. 2023. The Flan Collection:
    Designing Data and Methods for Effective Instruction Tuning. *arXiv preprint arXiv:2301.13688*.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Longpre 等 (2023) Longpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.; Tay,
    Y.; Zhou, D.; Le, Q. V.; Zoph, B.; Wei, J.; 等. 2023. Flan 集合：为有效的指令调整设计数据和方法。*arXiv
    预印本 arXiv:2301.13688*。
- en: Loshchilov and Hutter (2017) Loshchilov, I.; and Hutter, F. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter (2017) Loshchilov, I.; 和 Hutter, F. 2017. 解耦权重衰减正则化。*arXiv
    预印本 arXiv:1711.05101*。
- en: 'Miner, Laranjo, and Kocaballi (2020) Miner, A. S.; Laranjo, L.; and Kocaballi,
    A. B. 2020. Chatbots in the fight against the COVID-19 pandemic. *NPJ Digit Med*,
    3: 65.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miner, Laranjo, 和 Kocaballi (2020) Miner, A. S.; Laranjo, L.; 和 Kocaballi,
    A. B. 2020. 聊天机器人在对抗 COVID-19 大流行中的作用。*NPJ 数字医学*, 3: 65。'
- en: Nori et al. (2023) Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; and Horvitz,
    E. 2023. Capabilities of GPT-4 on Medical Challenge Problems. arXiv:2303.13375.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nori 等 (2023) Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; 和 Horvitz,
    E. 2023. GPT-4 在医学挑战问题上的能力。arXiv:2303.13375。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774。
- en: 'Pal, Umapathi, and Sankarasubbu (2022) Pal, A.; Umapathi, L. K.; and Sankarasubbu,
    M. 2022. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical
    domain Question Answering. In Flores, G.; Chen, G. H.; Pollard, T.; Ho, J. C.;
    and Naumann, T., eds., *Proceedings of the Conference on Health, Inference, and
    Learning*, volume 174 of *Proceedings of Machine Learning Research*, 248–260\.
    PMLR.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pal, Umapathi, 和 Sankarasubbu (2022) Pal, A.; Umapathi, L. K.; 和 Sankarasubbu,
    M. 2022. MedMCQA：一个大规模多主题多选择数据集用于医学领域的问答。见 Flores, G.; Chen, G. H.; Pollard, T.;
    Ho, J. C.; 和 Naumann, T., 主编，*健康、推断与学习会议论文集*, 174 卷 *机器学习研究会议论文集*, 248–260\. PMLR。
- en: 'Peng et al. (2023) Peng, C.; Yang, X.; Chen, A.; Smith, K. E.; PourNejatian,
    N.; Costa, A. B.; Martin, C.; Flores, M. G.; Zhang, Y.; Magoc, T.; Lipori, G.;
    Mitchell, D. A.; Ospina, N. S.; Ahmed, M. M.; Hogan, W. R.; Shenkman, E. A.; Guo,
    Y.; Bian, J.; and Wu, Y. 2023. A study of generative large language model for
    medical research and healthcare. *NPJ Digit Med*, 6(1): 210.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等 (2023) Peng, C.; Yang, X.; Chen, A.; Smith, K. E.; PourNejatian, N.;
    Costa, A. B.; Martin, C.; Flores, M. G.; Zhang, Y.; Magoc, T.; Lipori, G.; Mitchell,
    D. A.; Ospina, N. S.; Ahmed, M. M.; Hogan, W. R.; Shenkman, E. A.; Guo, Y.; Bian,
    J.; 和 Wu, Y. 2023. 一项关于生成大型语言模型在医学研究和医疗保健中的应用研究。*NPJ 数字医学*, 6(1): 210。'
- en: Sanh et al. (2022) Sanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika,
    L.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.; Dey, M.; Bari,
    M. S.; Xu, C.; Thakker, U.; Sharma, S. S.; Szczechla, E.; Kim, T.; Chhablani,
    G.; Nayak, N.; Datta, D.; Chang, J.; Jiang, M. T.-J.; Wang, H.; Manica, M.; Shen,
    S.; Yong, Z. X.; Pandey, H.; Bawden, R.; Wang, T.; Neeraj, T.; Rozen, J.; Sharma,
    A.; Santilli, A.; Fevry, T.; Fries, J. A.; Teehan, R.; Bers, T.; Biderman, S.;
    Gao, L.; Wolf, T.; and Rush, A. M. 2022. Multitask Prompted Training Enables Zero-Shot
    Task Generalization. arXiv:2110.08207.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等 (2022) Sanh, V.; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.; Alyafeai,
    Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.; Dey, M.; Bari, M. S.; Xu,
    C.; Thakker, U.; Sharma, S. S.; Szczechla, E.; Kim, T.; Chhablani, G.; Nayak,
    N.; Datta, D.; Chang, J.; Jiang, M. T.-J.; Wang, H.; Manica, M.; Shen, S.; Yong,
    Z. X.; Pandey, H.; Bawden, R.; Wang, T.; Neeraj, T.; Rozen, J.; Sharma, A.; Santilli,
    A.; Fevry, T.; Fries, J. A.; Teehan, R.; Bers, T.; Biderman, S.; Gao, L.; Wolf,
    T.; 和 Rush, A. M. 2022. 多任务提示训练实现零样本任务泛化。arXiv:2110.08207。
- en: Singhal et al. (2022) Singhal, K.; Azizi, S.; Tu, T.; Sara Mahdavi, S.; Wei,
    J.; Chung, H. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; Payne, P.;
    Seneviratne, M.; Gamble, P.; Kelly, C.; Scharli, N.; Chowdhery, A.; Mansfield,
    P.; Aguera y Arcas, B.; Webster, D.; Corrado, G. S.; Matias, Y.; Chou, K.; Gottweis,
    J.; Tomasev, N.; Liu, Y.; Rajkomar, A.; Barral, J.; Semturs, C.; Karthikesalingam,
    A.; and Natarajan, V. 2022. Large Language Models Encode Clinical Knowledge.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等（2022年）Singhal, K.; Azizi, S.; Tu, T.; Sara Mahdavi, S.; Wei, J.; Chung,
    H. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; Payne, P.; Seneviratne,
    M.; Gamble, P.; Kelly, C.; Scharli, N.; Chowdhery, A.; Mansfield, P.; Aguera y
    Arcas, B.; Webster, D.; Corrado, G. S.; Matias, Y.; Chou, K.; Gottweis, J.; Tomasev,
    N.; Liu, Y.; Rajkomar, A.; Barral, J.; Semturs, C.; Karthikesalingam, A.; 和 Natarajan,
    V. 2022. 大型语言模型编码临床知识。
- en: Singhal et al. (2023) Singhal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn,
    E.; Hou, L.; Clark, K.; Pfohl, S.; Cole-Lewis, H.; Neal, D.; Schaekermann, M.;
    Wang, A.; Amin, M.; Lachgar, S.; Mansfield, P.; Prakash, S.; Green, B.; Dominowska,
    E.; y Arcas, B. A.; Tomasev, N.; Liu, Y.; Wong, R.; Semturs, C.; Mahdavi, S. S.;
    Barral, J.; Webster, D.; Corrado, G. S.; Matias, Y.; Azizi, S.; Karthikesalingam,
    A.; and Natarajan, V. 2023. Towards Expert-Level Medical Question Answering with
    Large Language Models. arXiv:2305.09617.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等（2023年）Singhal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn, E.;
    Hou, L.; Clark, K.; Pfohl, S.; Cole-Lewis, H.; Neal, D.; Schaekermann, M.; Wang,
    A.; Amin, M.; Lachgar, S.; Mansfield, P.; Prakash, S.; Green, B.; Dominowska,
    E.; y Arcas, B. A.; Tomasev, N.; Liu, Y.; Wong, R.; Semturs, C.; Mahdavi, S. S.;
    Barral, J.; Webster, D.; Corrado, G. S.; Matias, Y.; Azizi, S.; Karthikesalingam,
    A.; 和 Natarajan, V. 2023. 朝着专家级医学问答的方向前进，利用大型语言模型。arXiv:2305.09617。
- en: 'Souza et al. (2023) Souza, L. L. d.; Fonseca, F. P.; Martins, M. D.; de Almeida,
    O. P.; Pontes, H. A. R.; Coracin, F. L.; Lopes, M. A.; Khurram, S. A.; Santos-Silva,
    A. R.; Hagag, A.; and Vargas, P. A. 2023. ChatGPT and medicine: a potential threat
    to science or a step towards the future? *J. Med. Artif. Intell.*, 6: 19–19.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Souza 等（2023年）Souza, L. L. d.; Fonseca, F. P.; Martins, M. D.; de Almeida,
    O. P.; Pontes, H. A. R.; Coracin, F. L.; Lopes, M. A.; Khurram, S. A.; Santos-Silva,
    A. R.; Hagag, A.; 和 Vargas, P. A. 2023. ChatGPT 与医学：对科学的潜在威胁还是迈向未来的一步？*J. Med.
    Artif. Intell.*, 6: 19–19。'
- en: 'Thirunavukarasu et al. (2023) Thirunavukarasu, A. J.; Ting, D. S. J.; Elangovan,
    K.; Gutierrez, L.; Tan, T. F.; and Ting, D. S. W. 2023. Large language models
    in medicine. *Nat. Med.*, 29(8): 1930–1940.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thirunavukarasu 等（2023年）Thirunavukarasu, A. J.; Ting, D. S. J.; Elangovan,
    K.; Gutierrez, L.; Tan, T. F.; 和 Ting, D. S. W. 2023. 医学中的大型语言模型。*Nat. Med.*,
    29(8): 1930–1940。'
- en: 'Toma et al. (2023) Toma, A.; Lawler, P. R.; Ba, J.; Krishnan, R. G.; Rubin,
    B. B.; and Wang, B. 2023. Clinical Camel: An Open Expert-Level Medical Language
    Model with Dialogue-Based Knowledge Encoding. arXiv:2305.12031.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toma 等（2023年）Toma, A.; Lawler, P. R.; Ba, J.; Krishnan, R. G.; Rubin, B. B.;
    和 Wang, B. 2023. 临床骆驼：一种基于对话的开放专家级医学语言模型。arXiv:2305.12031。
- en: 'Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;
    Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al.
    2023a. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a年）Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; 等 2023a. Llama：开放且高效的基础语言模型。*arXiv
    preprint arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
    Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
    Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,
    A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and
    Fine-Tuned Chat Models. arXiv:2307.09288.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b年）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
    Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
    Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,
    A.; Stojnic, R.; Edunov, S.; 和 Scialom, T. 2023b. Llama 2：开放基础和微调聊天模型。arXiv:2307.09288。
- en: 'Vilares and Gómez-Rodríguez (2019) Vilares, D.; and Gómez-Rodríguez, C. 2019.
    HEAD-QA: A Healthcare Dataset for Complex Reasoning. In *Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics*, 960–966\. Florence,
    Italy: Association for Computational Linguistics.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vilares 和 Gómez-Rodríguez（2019）Vilares, D.; 和 Gómez-Rodríguez, C. 2019. HEAD-QA:
    用于复杂推理的医疗数据集。见于 *第57届计算语言学协会年会论文集*，960–966。意大利，佛罗伦萨：计算语言学协会。'
- en: 'Wang et al. (2020) Wang, L. L.; Lo, K.; Chandrasekhar, Y.; Reas, R.; Yang,
    J.; Burdick, D.; Eide, D.; Funk, K.; Katsis, Y.; Kinney, R. M.; Li, Y.; Liu, Z.;
    Merrill, W.; Mooney, P.; Murdick, D. A.; Rishi, D.; Sheehan, J.; Shen, Z.; Stilson,
    B.; Wade, A. D.; Wang, K.; Wang, N. X. R.; Wilhelm, C.; Xie, B.; Raymond, D. M.;
    Weld, D. S.; Etzioni, O.; and Kohlmeier, S. 2020. CORD-19: The COVID-19 Open Research
    Dataset. In *Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020*.
    Online: Association for Computational Linguistics.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2020）Wang, L. L.; Lo, K.; Chandrasekhar, Y.; Reas, R.; Yang, J.; Burdick,
    D.; Eide, D.; Funk, K.; Katsis, Y.; Kinney, R. M.; Li, Y.; Liu, Z.; Merrill, W.;
    Mooney, P.; Murdick, D. A.; Rishi, D.; Sheehan, J.; Shen, Z.; Stilson, B.; Wade,
    A. D.; Wang, K.; Wang, N. X. R.; Wilhelm, C.; Xie, B.; Raymond, D. M.; Weld, D.
    S.; Etzioni, O.; 和 Kohlmeier, S. 2020. CORD-19: COVID-19 开放研究数据集。见于 *ACL 2020
    NLP for COVID-19 第1届研讨会论文集*。在线：计算语言学协会。'
- en: 'Wang et al. (2022) Wang, Y.; Mishra, S.; Alipoormolabashi, P.; Kordi, Y.; Mirzaei,
    A.; Arunkumar, A.; Ashok, A.; Dhanasekaran, A. S.; Naik, A.; Stap, D.; Pathak,
    E.; Karamanolakis, G.; Lai, H. G.; Purohit, I.; Mondal, I.; Anderson, J.; Kuznia,
    K.; Doshi, K.; Patel, M.; Pal, K. K.; Moradshahi, M.; Parmar, M.; Purohit, M.;
    Varshney, N.; Kaza, P. R.; Verma, P.; Puri, R. S.; Karia, R.; Sampat, S. K.; Doshi,
    S.; Mishra, S.; Reddy, S.; Patro, S.; Dixit, T.; Shen, X.; Baral, C.; Choi, Y.;
    Smith, N. A.; Hajishirzi, H.; and Khashabi, D. 2022. Super-NaturalInstructions:
    Generalization via Declarative Instructions on 1600+ NLP Tasks. arXiv:2204.07705.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022）Wang, Y.; Mishra, S.; Alipoormolabashi, P.; Kordi, Y.; Mirzaei,
    A.; Arunkumar, A.; Ashok, A.; Dhanasekaran, A. S.; Naik, A.; Stap, D.; Pathak,
    E.; Karamanolakis, G.; Lai, H. G.; Purohit, I.; Mondal, I.; Anderson, J.; Kuznia,
    K.; Doshi, K.; Patel, M.; Pal, K. K.; Moradshahi, M.; Parmar, M.; Purohit, M.;
    Varshney, N.; Kaza, P. R.; Verma, P.; Puri, R. S.; Karia, R.; Sampat, S. K.; Doshi,
    S.; Mishra, S.; Reddy, S.; Patro, S.; Dixit, T.; Shen, X.; Baral, C.; Choi, Y.;
    Smith, N. A.; Hajishirzi, H.; 和 Khashabi, D. 2022. Super-NaturalInstructions：通过声明性指令在1600多个NLP任务上的泛化。arXiv:2204.07705。
- en: Wang et al. (2023) Wang, Y.; Visweswaran, S.; Kappor, S.; Kooragayalu, S.; and
    Wu, X. 2023. ChatGPT, enhanced with clinical practice guidelines, is a superior
    decision support tool.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Wang, Y.; Visweswaran, S.; Kappor, S.; Kooragayalu, S.; 和 Wu, X.
    2023. ChatGPT，结合临床实践指南，是一个更优的决策支持工具。
- en: Wei et al. (2022) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2022. Finetuned Language Models Are Zero-Shot
    Learners. arXiv:2109.01652.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.;
    Du, N.; Dai, A. M.; 和 Le, Q. V. 2022. 微调的语言模型是零样本学习者。arXiv:2109.01652。
- en: 'Wu et al. (2023) Wu, C.; Lin, W.; Zhang, X.; Zhang, Y.; Wang, Y.; and Xie,
    W. 2023. PMC-LLaMA: Towards Building Open-source Language Models for Medicine.
    *arXiv [cs.CL]*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Wu, C.; Lin, W.; Zhang, X.; Zhang, Y.; Wang, Y.; 和 Xie, W. 2023. PMC-LLaMA：迈向构建开源医学语言模型。*arXiv
    [cs.CL]*。
- en: 'Yang et al. (2022) Yang, X.; Chen, A.; PourNejatian, N.; Shin, H. C.; Smith,
    K. E.; Parisien, C.; Compas, C.; Martin, C.; Costa, A. B.; Flores, M. G.; Zhang,
    Y.; Magoc, T.; Harle, C. A.; Lipori, G.; Mitchell, D. A.; Hogan, W. R.; Shenkman,
    E. A.; Bian, J.; and Wu, Y. 2022. A large language model for electronic health
    records. *NPJ Digit Med*, 5(1): 194.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2022）Yang, X.; Chen, A.; PourNejatian, N.; Shin, H. C.; Smith, K. E.;
    Parisien, C.; Compas, C.; Martin, C.; Costa, A. B.; Flores, M. G.; Zhang, Y.;
    Magoc, T.; Harle, C. A.; Lipori, G.; Mitchell, D. A.; Hogan, W. R.; Shenkman,
    E. A.; Bian, J.; 和 Wu, Y. 2022. 电子健康记录的大型语言模型。*NPJ 数字医学*，5(1)：194。
- en: Zhao et al. (2023) Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.;
    Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.;
    Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R.
    2023. A Survey of Large Language Models. arXiv:2303.18223.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023）Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min,
    Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang,
    J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.-Y.; 和 Wen, J.-R. 2023.
    大型语言模型综述。arXiv:2303.18223。
- en: 'Zhou et al. (2023) Zhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,
    Y.; Chen, S. S.; Zhou, P.; Liu, J.; Hua, Y.; Mao, C.; Wu, X.; Zheng, Y.; Clifton,
    L.; Li, Z.; Luo, J.; and Clifton, D. A. 2023. A Survey of Large Language Models
    in Medicine: Principles, Applications, and Challenges. arXiv:2311.05112.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Zhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li, Y.; Chen,
    S. S.; Zhou, P.; Liu, J.; Hua, Y.; Mao, C.; Wu, X.; Zheng, Y.; Clifton, L.; Li,
    Z.; Luo, J.; 和 Clifton, D. A. 2023. 医学中大型语言模型综述：原则、应用与挑战。arXiv:2311.05112。
- en: 'Zhu et al. (2023) Zhu, Y.; Yuan, H.; Wang, S.; Liu, J.; Liu, W.; Deng, C.;
    Dou, Z.; and Wen, J.-R. 2023. Large Language Models for Information Retrieval:
    A Survey. arXiv:2308.07107.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）Zhu, Y.; Yuan, H.; Wang, S.; Liu, J.; Liu, W.; Deng, C.; Dou, Z.;
    和 Wen, J.-R. 2023. 大型语言模型在信息检索中的应用：综述。arXiv:2308.07107。
- en: 'Appendix A Appendix: Supplementary Materials'
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录：补充材料
- en: A.1 Related work
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 相关工作
- en: Most contemporary LLMs predominantly employ a decoder-only transformer architecture
    such as GPT-3 (Brown et al. [2020](#bib.bib4)) for generating human-like text.
    Such models, typically with billions or hundreds of billions of parameters, are
    trained using large amounts of diverse textual data, leading to the emergence
    of significant generative capabilities in various tasks. Particularly, the landscape
    of LLMs has predominantly been shaped by models initially trained for general-purpose
    tasks, with some later being adapted for specialized domains like healthcare.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数 LLMs 主要采用仅解码器的变换器架构，如 GPT-3（Brown 等人 [2020](#bib.bib4)）来生成类似人类的文本。这些模型通常具有数十亿或数百亿的参数，使用大量多样化的文本数据进行训练，从而在各种任务中展现出显著的生成能力。特别是，LLMs
    的发展主要由最初为通用任务训练的模型塑造，之后一些模型被适应于医疗等专业领域。
- en: Notable examples in this realm include GPT-3.5 and GPT-4, developed by OpenAI,
    which have demonstrated impressive capabilities across a wide range of tasks,
    including high performance on various medical benchmarks (Nori et al. [2023](#bib.bib35)).
    Similarly, Singhal et al. demonstrated state-of-the-art performance of Google’s
    Med-PaLM and Med-PaLM 2 (Singhal et al. [2022](#bib.bib40), [2023](#bib.bib41)).
    Albeit using a general-purpose trained model (PaLM), the authors applied an ensemble
    refinement, computational heavy prompting strategy which showed increased performance
    on medical examination and other medical-related question-answering datasets as
    well as encouraging results on human evaluation experiments. However, it is important
    to note that intricate details concerning the training methodologies and weight
    parameters for such models remain undisclosed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域中的显著例子包括由 OpenAI 开发的 GPT-3.5 和 GPT-4，它们在各种任务中展示了令人印象深刻的能力，包括在多个医学基准测试中的高性能（Nori
    等人 [2023](#bib.bib35)）。类似地，Singhal 等人展示了谷歌的 Med-PaLM 和 Med-PaLM 2 的最先进表现（Singhal
    等人 [2022](#bib.bib40)，[2023](#bib.bib41)）。尽管使用了通用训练模型（PaLM），作者们应用了一个集成细化、计算密集的提示策略，这在医学考试和其他医学相关问答数据集上显示了性能的提升，并在人工评估实验中获得了令人鼓舞的结果。然而，值得注意的是，关于这些模型的训练方法和权重参数的详细信息仍未公开。
- en: There has been an interest in tailoring LLMs for specific biomedical domains
    by pre-training the model using domain-specific datasets. Models such as GatorTron
    and its successor, MedGatorTron, exemplify this approach (Yang et al. [2022](#bib.bib53);
    Peng et al. [2023](#bib.bib38)). These models leverage large corpora of medical
    data, including electronic health records (EHRs), clinical notes, and medical
    literature to build a robust foundational understanding of medical terminologies,
    concepts, and patient-care scenarios. This pre-training on domain-specific data
    sets the stage for more effective and nuanced applications in the medical field.
    PMC-LLaMA is another recently-released model, which was initially pre-trained
    on a biomedical/clinical corpus, and subsequently trained on an instruction dataset
    primarily containing medical question answering and reasoning tasks (Wu et al.
    [2023](#bib.bib52)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有兴趣通过使用领域特定数据集进行预训练，来为特定生物医学领域量身定制 LLMs。GatorTron 及其后继模型 MedGatorTron 就是这种方法的例子（Yang
    等人 [2022](#bib.bib53)；Peng 等人 [2023](#bib.bib38)）。这些模型利用大量医学数据，包括电子健康记录（EHRs）、临床笔记和医学文献，建立对医学术语、概念和患者护理场景的扎实基础。这种在领域特定数据上的预训练为医学领域中更有效和细致的应用奠定了基础。PMC-LLaMA
    是另一个最近发布的模型，它最初在生物医学/临床语料库上进行了预训练，随后在主要包含医学问答和推理任务的指令数据集上进行了训练（Wu 等人 [2023](#bib.bib52)）。
- en: Another significant trend in the development of healthcare-oriented LLMs has
    been the utilization of medical-related instruction and dialogue datasets for
    fine-tuning. This “alignment” approach involves fine-tuning a pre-trained model
    on a collection of instruction-following samples, to effectively improve the zero-shot
    and few-shot generalization abilities of LLMs. ChatDoctor (Li et al. [2023b](#bib.bib27)),
    for example, is a medical LLM fine-tuned (using full-parameter fine-tuning) on
    a Large Language Model Meta-AI (LLaMA) (Touvron et al. [2023a](#bib.bib45)) using
    a dataset that contains online conversations between physicians and patients,
    and compared favourably to GPT-3.5 when evaluated using a number of medical queries.
    Similarly, Han et al. propose MedAlpaca by fine-tuning LLaMA using a collection
    of over 160,000 medical NLP tasks reformatted in instruction tuning formats as
    well as a crawl of various internet resources (Han et al. [2023](#bib.bib13)).
    The authors developed a parameter-efficient variant (using LoRA) which was shown
    to underperform (when compared to the corresponding full-parameter fine-tuned
    variant) on the United States Medical Licensing Examination (USMLE) self-assessment
    dataset. Other more recent models, which were developed concurrently with those
    presented in this manuscript, include models based on LLama-2 (Touvron et al.
    [2023b](#bib.bib46)), such as Clinical Camel (Toma et al. [2023](#bib.bib44))
    and MediTron (Chen et al. [2023](#bib.bib6)). In the latter, we note that model
    fine-tuning was conducted using the training set of each individual benchmark
    that the model was evaluated, and that fine-tuning was preceded by a phase of
    continuous pre-training using biomedical data sources such as PubMed Central and
    PubMed open-access research papers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在医疗导向LLMs发展的重要趋势是利用与医学相关的指令和对话数据集进行微调。这种“对齐”方法涉及在一组遵循指令的样本上微调预训练模型，以有效提高LLMs的零-shot和少-shot泛化能力。例如，ChatDoctor（Li等人
    [2023b](#bib.bib27)）是一个在大语言模型Meta-AI（LLaMA）（Touvron等人 [2023a](#bib.bib45)）上进行全参数微调的医学LLM，使用了包含医生与患者在线对话的数据集，并在多个医学查询的评估中与GPT-3.5相比表现良好。类似地，Han等人通过使用超过160,000个医学NLP任务的指令调优格式数据集以及各种互联网资源的爬取数据，提出了MedAlpaca（Han等人
    [2023](#bib.bib13)）。作者开发了一种参数高效的变体（使用LoRA），该变体在与相应的全参数微调变体相比时，在美国医学执照考试（USMLE）自我评估数据集上的表现不佳。其他更近期的模型，包括与本手稿中展示的模型同时开发的模型，基于LLama-2（Touvron等人
    [2023b](#bib.bib46)），例如Clinical Camel（Toma等人 [2023](#bib.bib44)）和MediTron（Chen等人
    [2023](#bib.bib6)）。在后者中，我们注意到模型微调是在对每个评估基准的训练集上进行的，并且微调之前进行了连续的预训练阶段，使用了PubMed
    Central和PubMed开放获取研究论文等生物医学数据源。
- en: Our study builds upon the approaches described above, extending the scope by
    conducting a more thorough comparison of different fine-tuning methods of LLMs
    within the medical domain. This work not only delves into the nuances of these
    tuning strategies, including the LoRA technique but also rigorously evaluates
    their effectiveness across multiple benchmarks. This comparative analysis is pivotal
    in pinpointing the most effective and efficient fine-tuning approach for LLMs
    in the medical sector, thereby making a significant contribution to the evolution
    of AI in healthcare.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究基于上述方法，通过对医学领域中不同大语言模型（LLMs）的微调方法进行更深入的比较，扩展了研究范围。这项工作不仅探讨了这些调优策略的细微差别，包括LoRA技术，还在多个基准测试中严格评估了它们的有效性。这种比较分析对于确定医学领域中LLMs最有效和高效的微调方法至关重要，从而对AI在医疗保健中的发展做出了重要贡献。
- en: A.2 Computational Setup
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 计算设置
- en: For the parameter-efficient fine-tuning and evaluation processes, we utilized
    8 nodes equipped with 16 NVIDIA V100 GPUs each. This configuration facilitated
    efficient experimentation and analysis of the fine-tuning methods under consideration.
    In contrast, the full-parameter fine-tuning experiments were conducted on the
    Condor Galaxy 1 (CG-1) supercomputer, provided by Cerebras. The CG-1 supercomputer
    offered the necessary computational power and infrastructure to handle the extensive
    demands of full-parameter fine-tuning processes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数高效的微调和评估过程，我们使用了8个节点，每个节点配备16个NVIDIA V100 GPU。这种配置促进了对所考虑的微调方法的高效实验和分析。相比之下，全参数微调实验是在Cerebras提供的Condor
    Galaxy 1（CG-1）超级计算机上进行的。CG-1超级计算机提供了处理全参数微调过程的广泛需求所需的计算能力和基础设施。
- en: A.3 Training dataset
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 训练数据集
- en: In the construction of our training dataset, we selected a range of datasets
    specifically related to medical and biomedical fields, ensuring relevance and
    applicability to our study’s focus. Recognizing the extensive size of some of
    these datasets, we employed a strategic sampling approach to extract representative
    subsets, thereby maintaining a comprehensive, yet manageable dataset size. Additionally,
    to provide a broader linguistic context and enhance the model’s generalizability,
    we incorporated a dataset from a general domain. This subset was carefully chosen
    so that the general domain data constituted 40% of the final training dataset.
    This hybrid dataset composition was designed to optimize the model’s performance
    across both specialized medical content applications and more general linguistic
    tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建训练数据集时，我们选择了一系列与医学和生物医学领域相关的数据集，确保与我们研究的重点相关并适用。鉴于这些数据集的规模较大，我们采用了战略抽样方法以提取具有代表性的子集，从而保持一个全面但可管理的数据集大小。此外，为了提供更广泛的语言背景并增强模型的通用性，我们纳入了来自一般领域的数据集。这个子集经过精心选择，使得一般领域的数据占据了最终训练数据集的40%。这种混合数据集的组成旨在优化模型在专门医学内容应用和更一般语言任务中的表现。
- en: 'Table A[1](#A1.T1 "Table 1 ‣ A.3 Training dataset ‣ Appendix A Appendix: Supplementary
    Materials ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches") provides a detailed overview of the various
    subsets of data included in our study, along with the number of samples contained
    in each subset.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 A[1](#A1.T1 "表1 ‣ A.3 训练数据集 ‣ 附录A 附录：补充材料 ‣ Med42 - 评估医疗LLMs的微调策略：全参数与参数高效方法")
    提供了我们研究中包括的各种数据子集的详细概述，以及每个子集包含的样本数量。
- en: '| Dataset | # Samples | Mixture ratio (%) |  |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | # 样本 | 混合比例 (%) |  |  |'
- en: '| Medical domain |  |  |  |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: 医学领域 |  |  |  |  |
- en: '|      MedMCQA (Pal, Umapathi, and Sankarasubbu [2022](#bib.bib37)) | 180,462
    | 25.54 |  |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|      MedMCQA (Pal, Umapathi, and Sankarasubbu [2022](#bib.bib37)) | 180,462
    | 25.54 |  |  |'
- en: '|      Medical Flashcards (Han et al. [2023](#bib.bib13)) | 30,106 | 4.26 |  |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|      Medical Flashcards (Han et al. [2023](#bib.bib13)) | 30,106 | 4.26 |  |  |'
- en: '|      StackExchange^† (Lambert et al. [2023](#bib.bib21)) | 64,246 | 9.09
    |  |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|      StackExchange^† (Lambert et al. [2023](#bib.bib21)) | 64,246 | 9.09
    |  |  |'
- en: '|      MedQA (USMLE) (Jin et al. [2020](#bib.bib19)) | 11,290 | 1.60 |  |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|      MedQA (USMLE) (Jin et al. [2020](#bib.bib19)) | 11,290 | 1.60 |  |  |'
- en: '|      CORD-19 (Wang et al. [2020](#bib.bib48)) | 17,721 | 2.51 |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|      CORD-19 (Wang et al. [2020](#bib.bib48)) | 17,721 | 2.51 |  |  |'
- en: '|      PubMedQA (Jin et al. [2019](#bib.bib20)) | 499 | 0.07 |  |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|      PubMedQA (Jin et al. [2019](#bib.bib20)) | 499 | 0.07 |  |  |'
- en: '|      HeadQA^‡ (Vilares and Gómez-Rodríguez [2019](#bib.bib47)) | 2,657 |
    0.38 |  |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|      HeadQA^‡ (Vilares and Gómez-Rodríguez [2019](#bib.bib47)) | 2,657 |
    0.38 |  |  |'
- en: '|      MediQA (Han et al. [2023](#bib.bib13)) | 1,950 | 0.28 |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|      MediQA (Han et al. [2023](#bib.bib13)) | 1,950 | 0.28 |  |  |'
- en: '|      PubMed Health Advice (Han et al. [2023](#bib.bib13)) | 7,694 | 1.09
    |  |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|      PubMed Health Advice (Han et al. [2023](#bib.bib13)) | 7,694 | 1.09
    |  |  |'
- en: '|      PubMed Causal (Han et al. [2023](#bib.bib13)) | 2,169 | 0.31 |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|      PubMed Causal (Han et al. [2023](#bib.bib13)) | 2,169 | 0.31 |  |  |'
- en: '|      OpenGPT | 66,026 | 9.34 |  |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|      OpenGPT | 66,026 | 9.34 |  |  |'
- en: '|      MedQUAD (Ben Abacha and Demner-Fushman [2019](#bib.bib3)) | 14,553 |
    2.06 |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|      MedQUAD (Ben Abacha and Demner-Fushman [2019](#bib.bib3)) | 14,553 |
    2.06 |  |  |'
- en: '|      MMLU^$ (Hendrycks et al. [2021](#bib.bib15)) | 244 | 0.03 |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|      MMLU^$ (Hendrycks et al. [2021](#bib.bib15)) | 244 | 0.03 |  |  |'
- en: '|      Niv2* (Wang et al. [2022](#bib.bib49)) | 11,447 | 1.62 |  |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|      Niv2* (Wang et al. [2022](#bib.bib49)) | 11,447 | 1.62 |  |  |'
- en: '|      Total | 411,064 | 58.17 |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|      总计 | 411,064 | 58.17 |  |  |'
- en: '| General domain |  |  |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 一般领域 |  |  |  |  |'
- en: '|      OpenOrca T0 (Lian et al. [2023](#bib.bib28); Sanh et al. [2022](#bib.bib39))
    | 110,905 | 15.69 |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|      OpenOrca T0 (Lian et al. [2023](#bib.bib28); Sanh et al. [2022](#bib.bib39))
    | 110,905 | 15.69 |  |  |'
- en: '|      OpenOrca Flan (Lian et al. [2023](#bib.bib28); Longpre et al. [2023](#bib.bib32))
    | 110,854 | 15.69 |  |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|      OpenOrca Flan (Lian et al. [2023](#bib.bib28); Longpre et al. [2023](#bib.bib32))
    | 110,854 | 15.69 |  |  |'
- en: '|      OpenOrca CoT (Lian et al. [2023](#bib.bib28); Wei et al. [2022](#bib.bib51))
    | 73,890 | 10.46 |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|      OpenOrca CoT (Lian et al. [2023](#bib.bib28); Wei et al. [2022](#bib.bib51))
    | 73,890 | 10.46 |  |  |'
- en: '|      Total | 295,649 | 41.83 |  |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|      总计 | 295,649 | 41.83 |  |  |'
- en: '| ^† The following categories were included: “academia”, “bioinformatics”,
    “biology”, “cogsci”, “fitness”, “health”. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| ^† 以下类别已包括： “学术界”、 “生物信息学”、 “生物学”、 “认知科学”、 “健身”、 “健康”。 |'
- en: '| ^‡ Only samples in English were used. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ^‡ 仅使用了英文样本。 |'
- en: '| ^$ The following subjects were included: “anatomy”, “clinical knowledge”,
    “college medicine”, “medical genetics”, |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| ^$ 包含了以下主题：“解剖学”，“临床知识”，“大学医学”，“医学遗传学”， |'
- en: '| “professional medicine”, “college biology”, “high-school biology”, “professional
    psychology”, “high-school psychology”, |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| “职业医学”，“大学生物学”，“高中生物学”，“职业心理学”，“高中心理学”， |'
- en: '| “human sexuality”, “human aging”, “nutrition”, and “virology”. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| “人类性行为”，“人类衰老”，“营养学”和“病毒学”。 |'
- en: '| * Samples from 47 tasks (from over 1,000 tasks) related to science, healthcare
    and medicine were included. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| * 来自 47 个与科学、医疗保健和医学相关的任务（超过 1,000 个任务）的样本已被包含。 |'
- en: 'Table 1: Summary of subsets of the data used for fine-tuning the models. Note
    that medical-domain data correspond to approximately 60% of the entire dataset.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：用于模型调优的数据子集的总结。请注意，医学领域数据大约占整个数据集的 60%。
- en: A.4 Decontamination analysis
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 去污染分析
- en: To address the unintentional inclusion of similar or identical samples in both
    the training and evaluation datasets, we implemented a “decontamination pipeline”.
    As in (Lee, Hunter, and Ruiz [2023](#bib.bib23)), we compute the cosine similarity,
    as measured by SentenceTransformers embeddings, between our instruction-tuning
    dataset and each sample in the evaluation dataset. We deem a sample as “contaminated”
    if the cosine similarity exceeds 0.8\. Our analysis reveals that most duplicates
    are simply reworded versions of the same question. However, some are extensive
    use-case questions that, despite not being directly the same, contain many identical
    words.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决训练和评估数据集中无意包含相似或相同样本的问题，我们实施了“去污染管道”。如（Lee, Hunter, 和 Ruiz [2023](#bib.bib23)）中所述，我们计算了我们指令调优数据集与评估数据集中每个样本之间的余弦相似度，使用
    SentenceTransformers 嵌入。我们认为如果余弦相似度超过 0.8，则该样本为“污染”。我们的分析表明，大多数重复样本只是相同问题的不同措辞版本。然而，一些是广泛的用例问题，尽管不直接相同，但包含许多相同的词。
- en: '| Dataset | Samples | Contaminated (%) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 样本 | 被污染 (%) |'
- en: '| --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MMLU (total) | 1,089 | 17 (1.6) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MMLU（总计） | 1,089 | 17 (1.6) |'
- en: '| Clinical Knowledge | 265 | 1 (0.4) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 临床知识 | 265 | 1 (0.4) |'
- en: '| College Biology | 144 | 1 (0.7) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 大学生物学 | 144 | 1 (0.7) |'
- en: '| College Medicine | 173 | 1 (0.6) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 大学医学 | 173 | 1 (0.6) |'
- en: '| Medical Genetics | 100 | 6 (6.0) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 医学遗传学 | 100 | 6 (6.0) |'
- en: '| Professional Medicine | 272 | 7 (2.6) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 职业医学 | 272 | 7 (2.6) |'
- en: '| Anatomy | 135 | 1 (0.7) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学 | 135 | 1 (0.7) |'
- en: '| MedMCQA | 4183 | 65 (1.6) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| MedMCQA | 4183 | 65 (1.6) |'
- en: '| MedQA | 1273 | 73 (5.7) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| MedQA | 1273 | 73 (5.7) |'
- en: '| HeadQA | 2742 | 62 (2.3) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| HeadQA | 2742 | 62 (2.3) |'
- en: '| PubmedQA | 500 | 2 (0.4) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| PubmedQA | 500 | 2 (0.4) |'
- en: '| USMLE (total) | 650 | 22 (3.4) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| USMLE（总计） | 650 | 22 (3.4) |'
- en: '| Self-Assessment | 325 | 11 (3.4) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 自我评估 | 325 | 11 (3.4) |'
- en: '| Sample Exam | 325 | 11 (3.4) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 样本考试 | 325 | 11 (3.4) |'
- en: '| Total | 11904 | 280 (2.4) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 11904 | 280 (2.4) |'
- en: 'Table 2: Results of the de-duplication pipeline over evaluation datasets.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：评估数据集上的去重管道结果。
- en: 'Table A.[2](#A1.T2 "Table 2 ‣ A.4 Decontamination analysis ‣ Appendix A Appendix:
    Supplementary Materials ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical
    LLMs: Full-Parameter vs. Parameter-Efficient Approaches") details the number of
    samples that were deemed to be contaminated. We observe that less than 2% of the
    samples in our evaluation dataset appear in our instruction-tuning dataset. We
    also observe that the majority of these duplicates are differently phrased questions
    about similar diseases or similar clinical concepts. In the case of longer use-case
    questions, they are deemed similar due to a high overlap in wording, despite having
    different final questions. Examples of contaminated examples are shown in Figure A.[1](#A1.F1
    "Figure 1 ‣ A.4 Decontamination analysis ‣ Appendix A Appendix: Supplementary
    Materials ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.[2](#A1.T2 "表 2 ‣ A.4 去污染分析 ‣ 附录 A 附录：补充材料 ‣ Med42 - 医学 LLM 调优策略评估：完整参数
    vs. 参数高效方法") 详细说明了被认为被污染的样本数量。我们观察到，在我们的评估数据集中，少于 2% 的样本出现在我们的指令调优数据集中。我们还观察到，这些重复样本中的大多数是对类似疾病或类似临床概念提出的措辞不同的问题。对于较长的用例问题，由于措辞高度重叠，尽管最终的问题不同，它们被认为是相似的。被污染样本的示例见图
    A.[1](#A1.F1 "图 1 ‣ A.4 去污染分析 ‣ 附录 A 附录：补充材料 ‣ Med42 - 医学 LLM 调优策略评估：完整参数 vs.
    参数高效方法")。
- en: '| Dataset | PE-FT | ± | FP-FT | ± |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | PE-FT | ± | FP-FT | ± |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MMLU (average) | 76.8 | (+0.1) | 76.6 | (-0.1) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| MMLU（平均） | 76.8 | (+0.1) | 76.6 | (-0.1) |'
- en: '| Clinical knowledge | 76.9 | (+0.3) | 74.2 | (-0.1) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 临床知识 | 76.9 | (+0.3) | 74.2 | (-0.1) |'
- en: '| College biology | 83.2 | (-0.1) | 83.9 | (-0.1) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 大学生物学 | 83.2 | (-0.1) | 83.9 | (-0.1) |'
- en: '| College medicine | 73.2 | (+0.4) | 69.2 | (+0.3) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 大学医学 | 73.2 | (+0.4) | 69.2 | (+0.3) |'
- en: '| Medical genetics | 79.8 | (-0.2) | 85.1 | (-0.9) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 医学遗传学 | 79.8 | (-0.2) | 85.1 | (-0.9) |'
- en: '| Professional medicine | 80.4 | (+0.3) | 80.0 | (+0.2) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 职业医学 | 80.4 | (+0.3) | 80.0 | (+0.2) |'
- en: '| Anatomy | 67.2 | (-0.2) | 67.2 | (-0.2) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 解剖学 | 67.2 | (-0.2) | 67.2 | (-0.2) |'
- en: '| HeadQA | 70.3 | (-0.3) | 71.7 | (-0.3) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| HeadQA | 70.3 | (-0.3) | 71.7 | (-0.3) |'
- en: '| MedMCQA | 55.2 | (+0.5) | 61.5 | (+0.6) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| MedMCQA | 55.2 | (+0.5) | 61.5 | (+0.6) |'
- en: '| MedQA | 58.5 | (-0.6) | 61.0 | (-0.5) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| MedQA | 58.5 | (-0.6) | 61.0 | (-0.5) |'
- en: '| PubMedQA | 75.7 | (-0.1) | 76.3 | (-0.5) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PubMedQA | 75.7 | (-0.1) | 76.3 | (-0.5) |'
- en: '| USMLE (average) | 68.2 | (-0.1) | 72.1 | (+0.2) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| USMLE（平均） | 68.2 | (-0.1) | 72.1 | (+0.2) |'
- en: '| Self-assessment | 67.7 | (-0.3) | 72.2 | (+0.5) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 自我评估 | 67.7 | (-0.3) | 72.2 | (+0.5) |'
- en: '| Sample exam | 68.7 | (+0.1) | 72.0 | (0.0) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 样本考试 | 68.7 | (+0.1) | 72.0 | (0.0) |'
- en: 'Table 3: Zero-shot accuracy for our 70B model after removing contaminated samples
    from evaluation datasets. Comparison with full results presented in Table [1](#Sx3.T1
    "Table 1 ‣ Results ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs:
    Full-Parameter vs. Parameter-Efficient Approaches").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在从评估数据集中移除污染样本后，我们 70B 模型的零样本准确率。与表 [1](#Sx3.T1 "表 1 ‣ 结果 ‣ Med42 - 医疗 LLM
    的微调策略评估：全参数与高效参数方法") 中的完整结果比较。
- en: 'In the interest of transparency, we re-calculated our evaluation metrics after
    excluding these potentially contaminated samples (Figure [2](#Sx3.F2 "Figure 2
    ‣ Results ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter
    vs. Parameter-Efficient Approaches"), Table A[3](#A1.T3 "Table 3 ‣ A.4 Decontamination
    analysis ‣ Appendix A Appendix: Supplementary Materials ‣ Med42 - Evaluating Fine-Tuning
    Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches")).
    The results demonstrate no substantial deviation from our original findings (Table [1](#Sx3.T1
    "Table 1 ‣ Results ‣ Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs:
    Full-Parameter vs. Parameter-Efficient Approaches")) after removing the “contaminated”
    examples. By identifying and segregating these samples, we aimed to ensure a more
    accurate and reliable assessment of the models’ performance.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了透明起见，我们在排除这些可能受污染的样本后重新计算了我们的评估指标（图 [2](#Sx3.F2 "图 2 ‣ 结果 ‣ Med42 - 医疗 LLM
    的微调策略评估：全参数与高效参数方法")，表 A[3](#A1.T3 "表 3 ‣ A.4 去污染分析 ‣ 附录 A 附录：补充材料 ‣ Med42 - 医疗
    LLM 的微调策略评估：全参数与高效参数方法")）。结果表明，移除“污染”样本后，与我们原始发现的结果（表 [1](#Sx3.T1 "表 1 ‣ 结果 ‣
    Med42 - 医疗 LLM 的微调策略评估：全参数与高效参数方法")）相比，并没有实质性的偏差。通过识别和隔离这些样本，我们旨在确保对模型性能的评估更加准确可靠。
- en: Train                                                                                        Evaluation
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 训练                                                                                        评估
- en: In
    a patient with head injury, eye opening is seen with painful stimulus, localizes
    the pain and there is inappropriate verbal response. What would be score on Glasgow
    coma scale?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在一名头部受伤的患者中，眼睛在疼痛刺激下睁开，定位疼痛并有不适当的语言反应。格拉斯哥昏迷评分会是多少？
- en: a. 8
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: a. 8
- en: b. 9
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: b. 9
- en: c. 10
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: c. 10
- en: d. 11 c. 10
    Before
    a patient with traumatic brain injury, what score on the Glasgow Coma Scale does
    it show if we observe that it emits inappropriate words, opens its eyes when speaking
    to it and makes a withdrawal response when applying a painful stimulus?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: d. 11 c. 10
    在创伤性脑损伤患者面前，如果我们观察到该患者发出不恰当的言语，看到对方在与其交谈时睁开眼睛，并在施加痛苦刺激时做出撤回反应，那么在格拉斯哥昏迷评分中应该显示多少分？
- en: a. 11
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: a. 11
- en: b. 10
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: b. 10
- en: c. 9
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: c. 9
- en: d. 8 b. 10
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: d. 8 b. 10
- en: ————-
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ————-
- en: Uvula vesicae
    seen in bladder is formed from the following structure ?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 膀胱中的尿囊突是由以下哪种结构形成的？
- en: a. Lateral lobe of prostate
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: a. 前列腺外侧叶
- en: b. Median lobe of prostate
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: b. 前列腺中叶
- en: c. Anterior lobe of prostate
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: c. 前列腺前叶
- en: d. Posterior lobe of prostate b. Median lobe of prostate
    Uvula vesicae
    is produced by which prostate lobe?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: d. 前列腺后叶 b. 前列腺中叶
    哪个前列腺叶产生尿囊突？
- en: a. Anterior lobe
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: a. 前列腺前叶
- en: b. Post lobe
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: b. 后叶
- en: c. Median lobe
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: c. 中叶
- en: d. Lateral lobe c. Median lobe
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: d. 外侧叶 c. 中叶
- en: 'Figure 1: Two examples of contaminated samples from our instruction-tuning
    (left) and evaluation datasets.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们指令调整（左）和评估数据集中受污染样本的两个例子。
