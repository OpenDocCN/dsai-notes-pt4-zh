- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘计算中对LLMs进行联邦微调：利弊得失
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03150](https://ar5iv.labs.arxiv.org/html/2310.03150)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03150](https://ar5iv.labs.arxiv.org/html/2310.03150)
- en: Herbert Woisetschläger
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Herbert Woisetschläger
- en: Technical University of Munich
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 慕尼黑工业大学
- en: herbert.woisetschlaeger@tum.de Alexander Isenko
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: herbert.woisetschlaeger@tum.de Alexander Isenko
- en: Technical University of Munich
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 慕尼黑工业大学
- en: alex.isenko@tum.de Shiqiang Wang
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: alex.isenko@tum.de Shiqiang Wang
- en: IBM Research
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究院
- en: wangshiq@us.ibm.com Ruben Mayer
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: wangshiq@us.ibm.com Ruben Mayer
- en: University of Bayreuth
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 拜罗伊特大学
- en: ruben.mayer@uni-bayreuth.de Hans-Arno Jacobsen
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ruben.mayer@uni-bayreuth.de Hans-Arno Jacobsen
- en: University of Toronto
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多伦多大学
- en: jacobsen@eecg.toronto.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: jacobsen@eecg.toronto.edu
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLM) and foundation models are popular as they offer
    new opportunities for individuals and businesses to improve natural language processing,
    interact with data, and retrieve information faster. However, training or fine-tuning
    LLMs requires a vast amount of data, which can be challenging to access due to
    legal or technical restrictions and may require private computing resources. Federated
    Learning (FL) is a solution designed to overcome these challenges and expand data
    access for deep learning applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）和基础模型因其提供了改进自然语言处理、与数据交互以及更快检索信息的新机会而受到欢迎。然而，训练或微调LLMs需要大量数据，这可能由于法律或技术限制而难以获取，并可能需要私有计算资源。联邦学习（FL）是一种旨在克服这些挑战并扩展深度学习应用数据访问的解决方案。
- en: 'This paper takes a hardware-centric approach to explore how LLMs can be brought
    to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family,
    ranging from 80M to 3B parameters, using FL for a text summarization task. We
    provide a micro-level hardware benchmark, compare the model FLOP utilization to
    a state-of-the-art data center GPU, and study the network utilization in realistic
    conditions. Our contribution is twofold: First, we evaluate the current capabilities
    of edge computing systems and their potential for LLM FL workloads. Second, by
    comparing these systems with a data-center GPU, we demonstrate the potential for
    improvement and the next steps toward achieving greater computational efficiency
    at the edge.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文采取以硬件为中心的方法，探讨如何将LLMs引入现代边缘计算系统。我们的研究使用FL对FLAN-T5模型系列进行微调，参数范围从80M到3B，进行文本摘要任务。我们提供了微观级别的硬件基准测试，将模型FLOP利用率与最先进的数据中心GPU进行比较，并研究了在实际条件下的网络利用情况。我们的贡献有两个方面：首先，我们评估了边缘计算系统的当前能力及其对LLM
    FL工作负载的潜力。其次，通过将这些系统与数据中心GPU进行比较，我们展示了改进的潜力以及在边缘实现更大计算效率的下一步。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Foundation models are omnipresent in academia and practice and fuel new innovations [[1](#bib.bib1)].
    These models have grown significantly w.r.t. to parameter size, as more parameters
    generally improve the performance to a certain degree [[2](#bib.bib2)]. In line
    with the growing computational need for these models, deep learning (DL) hardware
    accelerators have become increasingly more capable. Recent developments indicate
    a generational leap in computational power for data center applications, with
    the NVIDIA H100 NVL delivering 7.8 TB/s memory bandwidth compared to the previous
    state-of-the-art A100 80GB GPU that only has 2 TB/s ([Figure 1](#S1.F1 "In 1 Introduction
    ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly")).
    Due to memory-bandwidth bottlenecked operations taking up to 40% of the training
    time [[3](#bib.bib3)], this improvement may lead to much faster training times
    for both small and large models. At the same time, computational capabilities
    on embedded devices for mobile edge computing are significantly growing, with
    the NVIDIA Jetson AGX Orin 64GB being the first-of-a-kind DL-accelerated embedded
    device that provides capabilities for training foundation models [[4](#bib.bib4)].
    This has never been possible before and enables us to build FL workloads with
    large transformer models, benefit from scattered data, and bring generative AI
    closer to users. At the same time, computational capabilities on embedded devices
    for mobile edge computing are significantly growing, with the NVIDIA Jetson AGX
    Orin 64GB being the first-of-a-kind DL accelerated embedded device that provides
    capabilities for training foundation models [[4](#bib.bib4)]. This has never been
    possible before and enables us to build FL workloads with large transformer models,
    benefit from scattered data, and bring generative AI closer to users.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型在学术界和实践中无处不在，推动了新的创新 [[1](#bib.bib1)]。这些模型的参数规模显著增长，因为更多的参数通常会在一定程度上提高性能
    [[2](#bib.bib2)]。随着这些模型计算需求的增长，深度学习（DL）硬件加速器的能力也在不断提高。最近的发展表明，数据中心应用的计算能力出现了代际飞跃，NVIDIA
    H100 NVL 提供了 7.8 TB/s 的内存带宽，而之前的最先进的 A100 80GB GPU 只有 2 TB/s ([图 1](#S1.F1 "在
    1 介绍 ‣ 联邦微调 LLMs 在边缘：优点、缺点、丑陋"))。由于内存带宽瓶颈操作占用了多达 40% 的训练时间 [[3](#bib.bib3)]，这一改进可能会使得小型和大型模型的训练时间大大缩短。同时，嵌入式设备在移动边缘计算中的计算能力也显著增长，NVIDIA
    Jetson AGX Orin 64GB 是首款支持训练基础模型的深度学习加速嵌入式设备 [[4](#bib.bib4)]。这在以前是无法实现的，它使我们能够使用大型变压器模型构建
    FL 工作负载，从分散的数据中受益，并将生成式 AI 更接近用户。
- en: '![Refer to caption](img/18d8e2e725088dceca6ba3c775f44417.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18d8e2e725088dceca6ba3c775f44417.png)'
- en: 'Figure 1: Development of computational power and resource availability of DL
    accelerators 2017 - 2023 for data centers and embedded systems.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：2017 - 2023 年数据中心和嵌入式系统中深度学习加速器计算能力和资源可用性的演变。
- en: 'As this type of device is oftentimes scattered across geographies and entities,
    federated DL (FL) imposes itself as a major technique for fine-tuning foundation
    models in a distributed and private fashion. To our knowledge, the largest models
    discussed in FL to this point entail FedBert and GPT2 [[5](#bib.bib5), [6](#bib.bib6)].
    Both models were trained with FL methods on multi-GPU data center nodes. However,
    if we want to gain access to a broader data basis, we need to foster FL on the
    edge and bring foundation models to embedded devices. However, there are several
    challenges that we have to overcome:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这类设备往往分布在不同地理区域和实体中，联邦深度学习（FL）成为一种在分布式和私密环境下微调基础模型的主要技术。据我们了解，迄今为止在 FL 中讨论的最大模型包括
    FedBert 和 GPT2 [[5](#bib.bib5), [6](#bib.bib6)]。这两种模型都是使用 FL 方法在多 GPU 数据中心节点上进行训练的。然而，如果我们希望访问更广泛的数据基础，我们需要在边缘推动
    FL，并将基础模型引入嵌入式设备。然而，我们必须克服几个挑战：
- en: (1)
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Limited memory bandwidth on embedded devices. We currently see a generation
    leap in data center DL accelerators regarding memory bandwidth, which has increased
    significantly (up to 7.8 TB/s). Even though the memory size on embedded devices
    has increased, the memory bandwidth remains comparatively low (up to 0.2 TB/s).
    This affects key memory-bandwidth bottlenecked operations for the training process,
    which could lead to severe training time penalties.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 嵌入式设备上的内存带宽有限。我们目前看到数据中心深度学习加速器在内存带宽方面的代际飞跃，带宽显著增加（高达7.8 TB/s）。尽管嵌入式设备的内存大小有所增加，但内存带宽仍然相对较低（高达0.2
    TB/s）。这影响了训练过程中的关键内存带宽瓶颈操作，可能导致严重的训练时间损失。
- en: (2)
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: Foundation models have a large number of parameters. Foundation models can have
    several billions of parameters, which, even in data-center environments, requires
    efficient communication and optimal memory utilization so that distributed training
    creates a speedup. Techniques like gradient checkpointing [[7](#bib.bib7)], quantization [[8](#bib.bib8)],
    and delayed parameter updates [[9](#bib.bib9)] are common to improve hardware
    utilization and training throughput. These have not been sufficiently evaluated
    on edge environments, which work with different communication and hardware trade-offs.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础模型具有大量参数。基础模型可能拥有数十亿个参数，这在数据中心环境中也需要高效的通信和优化的内存利用，以便分布式训练能够加速。像梯度检查点[[7](#bib.bib7)]、量化[[8](#bib.bib8)]和延迟参数更新[[9](#bib.bib9)]等技术通常用于提高硬件利用率和训练吞吐量。这些在边缘环境中尚未得到充分评估，因为边缘环境在通信和硬件方面面临不同的权衡。
- en: (3)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （3）
- en: Communication on the edge is significantly more expensive than in data centers.
    While network bandwidth in data centers is available at 100 Gbit [[10](#bib.bib10)],
    mobile or remote communication over wide area networks is still a difficult challenge
    to achieve, especially when handling multi-billion parameter DL models. Techniques
    like ZeRo-offloading [[9](#bib.bib9)] and FSDP [[11](#bib.bib11)] utilize a high-bandwidth
    interconnect for all-reduce communication between nodes to not materialize the
    full model, optimizer, and gradient state due to limited memory sizes. This is
    typically not a viable option on the edge due to a much more limited interconnect,
    as they rely on high communication to optimize computational load.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边缘上的通信显著比数据中心更昂贵。虽然数据中心的网络带宽可达100 Gbit[[10](#bib.bib10)]，但通过广域网进行移动或远程通信仍然是一个困难的挑战，尤其是在处理数十亿参数的深度学习模型时。像ZeRo-offloading[[9](#bib.bib9)]和FSDP[[11](#bib.bib11)]等技术利用高带宽互连在节点间进行全规约通信，以避免由于内存大小限制而使完整模型、优化器和梯度状态实体化。这在边缘通常不是一个可行的选项，因为边缘上的互连限制较多，它们依赖高通信以优化计算负载。
- en: (4)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: Power is a key control variable in edge deployments. Contrary to data center
    deployments, workloads on the edge are subject to energy limitations and often
    require adjustments to frequently changing operating environments. This has direct
    implications for the training speed of FL clients and creates strict constraints
    on what is feasible to run on the edge.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 能耗是边缘部署中的一个关键控制变量。与数据中心部署相反，边缘上的工作负载受到能源限制，并且通常需要调整以适应频繁变化的操作环境。这对FL客户端的训练速度有直接影响，并对在边缘上运行的可行性提出了严格的限制。
- en: 'Based on the open challenges to create efficient edge computing systems that
    are capable of training foundation models and LLMs, we formulate our research
    questions: How is fine-tuning LLMs on the edge different from data center environments?
    What are the control levers to optimize LLM fine-tuning on the edge?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 针对创建能够训练基础模型和大型语言模型（LLMs）的高效边缘计算系统所面临的开放挑战，我们提出了我们的研究问题：在边缘设备上微调LLMs与数据中心环境有何不同？在边缘环境中优化LLM微调的控制杠杆是什么？
- en: 'By exploring this research question, we make two contributions and discover
    two main findings:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索这个研究问题，我们做出了两个贡献并发现了两个主要发现：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contribution 1: In-depth comparison of state-of-the-art DL accelerators for
    FL workloads. Nowadays, most papers in the FL space use data center hardware for
    their experiments, while large amounts of data are scattered on the edge and must
    not be neglected as a field of application. We, therefore, conduct an in-depth
    micro-benchmark of various transformer models on the latest data center and embedded
    DL accelerators.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贡献1：对FL工作负载的最先进深度学习加速器进行深入比较。目前，大多数FL领域的论文使用数据中心硬件进行实验，而大量数据散布在边缘，不容忽视。我们因此对最新的数据中心和嵌入式深度学习加速器上的各种变换器模型进行深入微基准测试。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contribution 2: Thorough study of optimization levers for FL workloads at the
    edge on embedded hardware. Our hardware-centric study outlines the effects of
    energy efficiency and the model FLOP utilization on overall training speeds. Further,
    we outline the suitability of granularity (see [Section 3](#S3 "3 Methodology
    ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly"))
    to estimate the practicality of FL workloads over varying network conditions.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '贡献2：对边缘嵌入式硬件上FL工作负载优化杠杆的彻底研究。我们的硬件中心研究概述了能效和模型FLOP利用率对整体训练速度的影响。此外，我们概述了粒度的适用性（参见[第3节](#S3
    "3 Methodology ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the
    Bad, the Ugly")）以估计在不同网络条件下FL工作负载的实用性。'
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finding 1: Power draw is a lever for DL performance estimation the edge. As
    we draw out the direct correlation of the model FLOP utilization, known for its
    broad applicability in the HPC domain, and energy efficiency, a crucial driver
    in the edge computing field, we identify an optimization lever for FL client selection
    and model aggregation strategies that enables applications, which focus on optimal
    computational resource utilization.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发现1：功耗是边缘DL性能估算的杠杆。通过揭示模型FLOP利用率与能效的直接相关性——能效在边缘计算领域至关重要——我们确定了一种优化杠杆，用于FL客户端选择和模型聚合策略，以支持关注于最优计算资源利用的应用。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finding 2: Training foundation models on the edge is all about memory bandwidth
    and network utilization. With our study, we identify the two major limiting factors
    required for efficiently training FMs on embedded hardware: memory bandwidth and
    network communication. We further point out strategies that can reduce memory
    bandwidth utilization and network communication.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发现2：在边缘训练基础模型完全依赖于内存带宽和网络利用率。通过我们的研究，我们确定了在嵌入式硬件上有效训练FM所需的两个主要限制因素：内存带宽和网络通信。我们进一步指出了可以减少内存带宽利用和网络通信的策略。
- en: This paper is structured as follows. Section 2 will outline relevant background.
    In Section 3, we present our methodology, and in Section 4, we present our benchmark
    design, including datasets, DL models, and FL strategies. Section 5 contains experimental
    evaluations of our benchmark. In Section 6, we conclude our work.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下。第2节将概述相关背景。在第3节中，我们介绍我们的方法论，第4节中介绍我们的基准设计，包括数据集、DL模型和FL策略。第5节包含我们基准的实验评估。在第6节中，我们总结了我们的工作。
- en: 2 Background
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: Performance objectives in data center environments. One of the most important
    issues when training in a data center is to maximize throughput by trying to use
    the hardware to its limit without being blocked by communication. Communication
    concerns both local communication, i.e., memory movement, and communication between
    GPUs and nodes, typically with a high bandwidth interconnect such as NVLink (7.8 TB/s)
    and Ethernet (100 Gbit) [[10](#bib.bib10)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心环境中的性能目标。在数据中心训练时最重要的问题之一是通过尽可能充分利用硬件而不被通信阻塞来最大化吞吐量。通信涉及本地通信，即内存移动，以及GPU和节点之间的通信，通常具有高带宽互连，如NVLink（7.8
    TB/s）和以太网（100 Gbit）[[10](#bib.bib10)]。
- en: 'Measuring the effectiveness of each GPU in training is possible via Model FLOP
    Utilization (MFU) [[12](#bib.bib12)], which is the ratio of throughput achieved
    compared to the theoretical throughput of a model and a set of hardware. Common
    values for the MFU are between 5 – 20% ([Figure 2(b)](#S5.F2.sf2 "In Figure 2
    ‣ 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly")) because DL models are not defined as a single matrix multiplication
    that can be perfectly parallelized between tensor cores but as many operations
    with memory bandwidth bottlenecks such as softmax, residual additions, and activations [[3](#bib.bib3)].
    These operations result in below-average FLOP usage, and each model architecture
    has its own set of operations that slow down throughput. However, the MFU can
    be used as a benchmark for how well a model is suited to work on a particular
    piece of hardware, as it fits the trade-offs between memory bandwidth, memory
    capacity, and FLOP. This way, we can compare the MFU for the same model on different
    hardware and contrast their results.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模型FLOP利用率（MFU）[[12](#bib.bib12)]可以测量每个GPU在训练中的效果，MFU是指实际吞吐量与模型和一组硬件的理论吞吐量之比。MFU的常见值在5%到20%之间（[图2(b)](#S5.F2.sf2
    "在图2 ‣ 5 结果 ‣ 边缘上的联邦微调LLM：好、坏、丑")），因为DL模型并不是一个可以在张量核心之间完美并行化的单一矩阵乘法，而是许多具有内存带宽瓶颈的操作，如softmax、残差加法和激活[[3](#bib.bib3)]。这些操作导致低于平均水平的FLOP使用，每种模型架构都有一组操作会降低吞吐量。然而，MFU可以作为模型适合在特定硬件上工作的基准，因为它符合内存带宽、内存容量和FLOP之间的权衡。这样，我们可以比较同一模型在不同硬件上的MFU，并对其结果进行对比。
- en: Performance objectives on the Edge. In edge computing systems that involve embedded
    devices, performance considerations differ from those in data center environments
    as use cases typically vary [[13](#bib.bib13)]. Yet, to run FL workloads on embedded
    devices on the edge, we need to unite performance characteristics from data centers
    and edge computing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘上的性能目标。在涉及嵌入式设备的边缘计算系统中，性能考虑与数据中心环境中的情况有所不同，因为使用案例通常会有所变化[[13](#bib.bib13)]。然而，要在边缘上的嵌入式设备上运行FL工作负载，我们需要将数据中心和边缘计算的性能特性结合起来。
- en: Running FL workloads on the edge is all about minimizing the time we use a client’s
    hardware and subsequently maximizing the throughput. Yet, the hardware is often
    located in remote areas with limited access to power or even a mobile device with
    very restrictive battery management [[14](#bib.bib14)]. Also, in remote and mobile
    environments, network bandwidth utilization and total network traffic are critical.
    Both have a significant impact on communication latency, i.e., how fast we can
    move model weights between clients and a server. For foundation models and LLMs,
    both can become a hurdle, as this kind of model tends to grow beyond several hundreds
    of millions of parameters in size or, in other words, beyond 1 GB in parameters
    to transfer over the network. Putting that into perspective with the average available
    wireless network bandwidth of 50 Mbit on mobile devices [[15](#bib.bib15)] yields
    communication times substantially longer than the actual computation time on clients
    [[16](#bib.bib16)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘运行FL工作负载的关键在于最小化我们对客户端硬件的使用时间，并随后最大化吞吐量。然而，硬件通常位于远程区域，电力供应有限，或者甚至是具有非常严格电池管理的移动设备[[14](#bib.bib14)]。此外，在远程和移动环境中，网络带宽利用率和总网络流量至关重要。两者都对通信延迟产生重大影响，即我们可以多快地在客户端和服务器之间移动模型权重。对于基础模型和LLM来说，这两者都可能成为障碍，因为这种模型往往超过数亿个参数，换句话说，就是超过1
    GB的参数需要通过网络传输。与移动设备上平均可用的50 Mbit无线网络带宽[[15](#bib.bib15)]相比，这会导致通信时间远远长于客户端上的实际计算时间[[16](#bib.bib16)]。
- en: 3 Methodology
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: Our hardware-centric study entails a combination of metrics from the data center
    / HPC and edge computing domains. Generally, when transferring LLM fine-tuning
    to the edge, we also transfer the challenges we currently have in data center
    environments into systems that suffer from more significant resource limitations.
    While energy efficiency is a specific challenge to edge computing systems, network
    bandwidth and computational efficiency are frequently discussed topics for DL
    applications in data centers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以硬件为中心的研究涉及数据中心/HPC和边缘计算领域的指标组合。通常，在将LLM微调转移到边缘时，我们还将数据中心环境中的挑战转移到资源限制更大的系统中。尽管能源效率是边缘计算系统面临的特定挑战，但网络带宽和计算效率在数据中心的DL应用中也是经常讨论的话题。
- en: Energy efficiency. In edge computing systems, energy efficiency is a core system
    design variable. Also, on the edge, we often employ embedded devices that are
    based on system-on-a-chip design. This allows us to capture the total energy consumption
    of a client and each component on the chip (e.g., CPU, GPU). This is an easy-to-capture
    system metric that can help identify computational limits and find trade-offs
    for the highest energy efficiency. We define energy efficiency as the tokens per
    second ($\mathrm{TPS}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 能效。在边缘计算系统中，能效是核心系统设计变量。此外，在边缘上，我们通常使用基于系统级芯片设计的嵌入式设备。这使我们能够捕获客户端及芯片上每个组件（例如
    CPU、GPU）的总能耗。这是一个易于捕获的系统指标，有助于识别计算限制并找到实现最高能效的权衡点。我们将能效定义为每秒令牌数（$\mathrm{TPS}$）。
- en: In data center settings, we can only capture the GPU power consumption as the
    system is often virtualized and does not have direct access to the actual hardware
    [[17](#bib.bib17)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中心环境中，由于系统通常是虚拟化的，无法直接访问实际硬件，因此我们只能捕获 GPU 的功耗 [[17](#bib.bib17)]。
- en: Computational efficiency. Maximization of resource utilization is the superior
    objective for DL and FL applications in data center environments, as this is usually
    equivalent to a cost-optimal solution [[18](#bib.bib18)]. In the HPC domain, MFU
    is used to calculate the hardware resource utilization based on the number of
    theoretical hardware FLOP/s. By varying the minibatch size, the MFU can also be
    used to identify computational bottlenecks, i.e., whether we are computationally
    bound or memory bandwidth limited. In our experiments, the theoretical capacity
    of the NVIDIA A100 is 312 TFLOP, while the Jetson AGX Orin 64 GB provides 42.5 TFLOP
    or 13% of the A100.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 计算效率。在数据中心环境中，资源利用率的最大化是深度学习（DL）和联邦学习（FL）应用的首要目标，因为这通常等同于成本最优解决方案 [[18](#bib.bib18)]。在高性能计算（HPC）领域，使用MFU计算基于理论硬件
    FLOP/s 的硬件资源利用率。通过调整小批量大小，MFU 还可用于识别计算瓶颈，即我们是否受限于计算或内存带宽。在我们的实验中，NVIDIA A100 的理论计算能力为
    312 TFLOP，而 Jetson AGX Orin 64 GB 提供 42.5 TFLOP，相当于 A100 的 13%。
- en: Communication efficiency. Communication is equally important for federated LLM
    fine-tuning as computational efficiency. Typically, full models or partial model
    weights are being communicated between client and server [[19](#bib.bib19)]. Yet,
    communication on data center settings is built on top of high-performance networking
    infrastructure that enables bandwidths of 100 Gbit, and more [[10](#bib.bib10)].
    On the edge, we often find significantly slower network links with 1 Gbit and
    below. For instance, the global average for communication over 4G LTE wireless
    is 40 Mbit download and 15 Mbit upload [[15](#bib.bib15)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通信效率。通信对于联邦学习模型微调与计算效率同样重要。通常，完整模型或部分模型权重在客户端和服务器之间传递 [[19](#bib.bib19)]。然而，数据中心环境中的通信建立在高性能网络基础设施之上，支持高达100
    Gbit甚至更高的带宽 [[10](#bib.bib10)]。在边缘上，我们通常会遇到显著较慢的网络链接，例如1 Gbit及以下。例如，4G LTE无线通信的全球平均值为40
    Mbit下载和15 Mbit上传 [[15](#bib.bib15)]。
- en: As can be seen, communication over LTE is much more expensive than in a data
    center via fiber. So, we need a reliable metric to quantify communication efficiency
    that, at the same time, tells us whether it is useful to further scale a FL workload
    over more clients or not. Borrowing from the HPC domain, Granularity ($G$) and
    to communicate the model gradients or weights ($T_{\mathrm{comm}}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，通过 LTE 的通信成本远高于数据中心的光纤通信。因此，我们需要一个可靠的指标来量化通信效率，同时告诉我们是否有必要将联邦学习工作负载扩展到更多客户端。从
    HPC 领域借鉴，粒度（$G$）和通信模型梯度或权重的时间（$T_{\mathrm{comm}}$）。
- en: In our FL scenario, the computation time is the maximum fine-tuning time on
    a client in each round, and the communication time is the time spent sending the
    model state, waiting, and receiving it from the server. In general, $G\gg 1$ adding
    more clients. Values of $G\ll 1$ indicate we spend too much time on communication
    rather than on computation, i.e., we do not achieve net speedup of the whole system
    by adding another client.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的联邦学习场景中，计算时间是每轮客户端上的最大微调时间，通信时间是发送模型状态、等待和从服务器接收的时间。一般来说，$G\gg 1$ 表示添加更多客户端的情况。$G\ll
    1$ 的值表示我们在通信上花费的时间过多，而不是在计算上，即添加另一个客户端并未实现整个系统的净加速。
- en: 4 Experimental Setup
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: Our hardware-centric study for FL on the edge focuses on evaluating state-of-the-art
    DL workloads on embedded devices. As such, we focus on current transformer models
    for NLP problems (LLMs).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们针对边缘设备的 FL 硬件中心研究专注于评估嵌入式设备上的最新深度学习工作负载。因此，我们关注当前用于自然语言处理（NLP）问题的变换器模型（LLMs）。
- en: Evaluation hardware. In our hardware-centered study, we focus on the state of
    the art of deep learning accelerators for data centers and embedded computing.
    We employ a cloud VM with a single NVIDIA A100 80 GB (SXM4) as a data center node
    (A100) to perform our local baseline experiments. Further, we use a dedicated
    cluster consisting of ten NVIDIA Jetson AGX Orin 64 GB nodes (Orin) as the state-of-the-art
    embedded computing platform. The Orins are connected with a 10 Gbit synchronous
    network link and are monitored with 5 Hz for their power metrics. For our FL experiments,
    we use a GPU-accelerated VM that is co-located with the Orins to handle the model
    aggregation and testing of the global model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 评估硬件。在我们的硬件中心研究中，我们专注于数据中心和嵌入式计算的深度学习加速器的最前沿。我们使用配备单个 NVIDIA A100 80 GB（SXM4）的云虚拟机作为数据中心节点（A100），进行本地基准实验。此外，我们使用一个由十个
    NVIDIA Jetson AGX Orin 64 GB 节点（Orin）组成的专用集群，作为最先进的嵌入式计算平台。这些 Orin 节点通过 10 Gbit
    同步网络连接，并以 5 Hz 频率监测其功率指标。对于我们的 FL 实验，我们使用与 Orin 节点共置的 GPU 加速虚拟机来处理模型聚合和全球模型的测试。
- en: DL models. For our experiments, we adopt the FLAN-T5 transformer model family
    [[4](#bib.bib4)] for conditional text generation. Specifically, we evaluate the
    computational training performance of the FLAN-T5-Small model with 80M parameters
    or 308 MB in size, the FLAN-T5-Base model with 250M parameters (990 MB), the FLAN-T5-Large
    model with 783M parameters (3.1 GB), and the FLAN-T5-XL model with 3B parameters
    (11.4 GB). For all models, we use the pre-trained FLAN-T5-Base tokenizer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型。对于我们的实验，我们采用了 FLAN-T5 变换器模型系列 [[4](#bib.bib4)] 进行条件文本生成。具体来说，我们评估了 FLAN-T5-Small
    模型（80M 参数或 308 MB）、FLAN-T5-Base 模型（250M 参数，990 MB）、FLAN-T5-Large 模型（783M 参数，3.1
    GB）和 FLAN-T5-XL 模型（3B 参数，11.4 GB）的计算训练性能。对于所有模型，我们使用预训练的 FLAN-T5-Base 分词器。
- en: Dataset. All the FLAN-T5 models are fine-tuned on the Samsum dataset with the
    objective of summarizing texts with a maximum token length of 512 elements [[21](#bib.bib21)].
    The maximum model output length is 95 tokens, which can be translated into the
    summaries of the respective inputs. For our FL experiments, we choose a latent
    Dirichlet allocation as it is frequently used in related work [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)]. We use $\alpha=1$ to randomly split the input
    data into ten subsets that we distribute on the Orin compute cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。所有 FLAN-T5 模型都在 Samsum 数据集上进行了微调，目标是总结最大长度为 512 个元素的文本 [[21](#bib.bib21)]。模型的最大输出长度为
    95 个标记，这可以转化为相应输入的摘要。对于我们的 FL 实验，我们选择了潜在狄利克雷分配（Latent Dirichlet Allocation），因为它在相关工作中经常被使用
    [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]。我们使用 $\alpha=1$ 将输入数据随机拆分成十个子集，然后分发到
    Orin 计算集群上。
- en: 5 Results
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个结果
- en: To evaluate the feasibility of fine-tuning large transformer models on embedded
    devices and in potentially remote locations, we conduct extensive experiments.
    Specifically, we evaluate the hardware DL fine-tuning efficiency and the network
    utilization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估在嵌入式设备和潜在远程位置上微调大型变换器模型的可行性，我们进行了广泛的实验。具体来说，我们评估了硬件深度学习微调效率和网络利用率。
- en: '![Refer to caption](img/f30e74394b70ce3b0104cb659a8b40e8.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f30e74394b70ce3b0104cb659a8b40e8.png)'
- en: (a) $\eta_{e}$
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $\eta_{e}$
- en: '![Refer to caption](img/4ccdb61f6ea97a5d60b0f39f8f016df0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4ccdb61f6ea97a5d60b0f39f8f016df0.png)'
- en: (b) MFU in %
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MFU（百分比）
- en: 'Figure 2: We study the energy efficiency ($\eta_{e}$, which is useful to evaluate
    root causes for poor training speeds. Additional metrics are available in Appendix
    [Appendix B](#A2 "Appendix B Results ‣ Federated Fine-Tuning of LLMs on the Very
    Edge: The Good, the Bad, the Ugly").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们研究了能源效率（$\eta_{e}$），这对评估训练速度较慢的根本原因很有用。附加指标可见于附录 [附录 B](#A2 "附录 B 结果 ‣
    在边缘进行 LLMs 联邦微调：优点、缺点与不足")。
- en: 'The Orin platform is severely bottlenecked by memory bandwidth compared to
    the A100. We study $\eta_{e}$ until a minibatch size of 8 ([Figure 2(a)](#S5.F2.sf1
    "In Figure 2 ‣ 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The
    Good, the Bad, the Ugly")). Afterwards, $\eta_{e}$ unveils an identical trend
    ([Figure 2(b)](#S5.F2.sf2 "In Figure 2 ‣ 5 Results ‣ Federated Fine-Tuning of
    LLMs on the Very Edge: The Good, the Bad, the Ugly")).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Orin平台与A100相比严重受到内存带宽的瓶颈。我们研究了$\eta_{e}$，直到小批量大小为8 ([图2(a)](#S5.F2.sf1 "在图2中
    ‣ 5个结果 ‣ LLMs在边缘的联邦微调：优点、缺点和不足"))。之后，$\eta_{e}$显示出相同的趋势 ([图2(b)](#S5.F2.sf2 "在图2中
    ‣ 5个结果 ‣ LLMs在边缘的联邦微调：优点、缺点和不足"))。
- en: 'A stagnating MFU as minibatch sizes increase means that increased parallel
    computation potential does not result in additional used FLOP. This can only happen
    if we encounter a memory bandwidth bottleneck. We see from [Figure 3](#S5.F3 "In
    5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly") that the Orin opt.step() function updating model weights and biases
    is taking up a significant amount of time in comparison to A100, which suggests
    that its performance is highly dependent on memory bandwidth. This should be prioritized
    for targeted optimization and profiled in more detail to find out which kernels
    are responsible for the slow performance.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着小批量大小的增加，MFU的停滞意味着增加的并行计算潜力没有带来额外使用的FLOP。这只能发生在遇到内存带宽瓶颈时。从[图3](#S5.F3 "在5个结果中
    ‣ LLMs在边缘的联邦微调：优点、缺点和不足")可以看出，与A100相比，Orin的opt.step()函数在更新模型权重和偏置时占用了大量时间，这表明其性能高度依赖于内存带宽。这应该是针对性优化的优先级，并且需要更详细的剖析以找出哪些内核导致了性能缓慢。
- en: 'Table 1: Network performance vs. computational capabilities across different
    transport technologies measured by $G$. The computation times are normalized to
    100K tokens on each FLAN-T5 model and hardware type. LTE speeds are 40 Mbit down
    and 15 Mbit upload. 10 G is synonymous with a 10 Gbit synchronous network link.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同传输技术下的网络性能与计算能力的比较，按$G$衡量。计算时间已标准化为每个FLAN-T5模型和硬件类型的100K tokens。LTE速度为40
    Mbit下载和15 Mbit上传。10 G等同于10 Gbit的同步网络连接。
- en: '|  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '&#124; Communication &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信 &#124;'
- en: '&#124; time per FL round &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每轮FL的时间 &#124;'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Computation per FL round &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每轮FL的计算 &#124;'
- en: '&#124; (norm. to 100K tokens) &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (标准化到100K tokens) &#124;'
- en: '| $G$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $G$ |'
- en: '|  |  | LTE | 10 G |  |  | LTE | 10 G |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LTE | 10 G |  |  | LTE | 10 G |'
- en: '| Model | Size |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 |'
- en: '&#124; Up &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上传 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Down &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 下载 &#124;'
- en: '| Total | Total | A100 | Orin | A100 | Orin | A100 | Orin |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 总计 | A100 | Orin | A100 | Orin | A100 | Orin |'
- en: '| Small | 308 MB | 157s | 61s | 218s | 0.2s | 1s | 14s | 0.005 | 0.064 | 5.00
    | 70.00 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 小 | 308 MB | 157s | 61s | 218s | 0.2s | 1s | 14s | 0.005 | 0.064 | 5.00 |
    70.00 |'
- en: '| Base | 990 MB | 504s | 197s | 701s | 0.8s | 4s | 39s | 0.006 | 0.056 | 5.00
    | 48.75 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 990 MB | 504s | 197s | 701s | 0.8s | 4s | 39s | 0.006 | 0.056 | 5.00
    | 48.75 |'
- en: '| Large | 3,100 MB | 620s | 1,653s | 2,273s | 2.5s | 15s | 118s | 0.007 | 0.052
    | 6.00 | 47.20 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 大 | 3,100 MB | 620s | 1,653s | 2,273s | 2.5s | 15s | 118s | 0.007 | 0.052
    | 6.00 | 47.20 |'
- en: 'Effective foundation model training on the edge requires sufficient communication
    bandwidth or significant compression. While communication plays a subordinate
    role in environments with high bandwidth availability (100 Gbit+), even for multi-million
    parameter models, it is accountable by a significant margin for the total time
    of an FL round ([Table 1](#S5.T1 "In 5 Results ‣ Federated Fine-Tuning of LLMs
    on the Very Edge: The Good, the Bad, the Ugly")) in a mobile environment. This
    is especially evident when looking at the granularity. While $G\gg 1$ for all
    FLAN-T5 transformers in FL systems with LTE connectivity. $G\approx 0$ is a strong
    indication of the inefficiency of distributed training of DL models on a given
    hardware type and under given network conditions. As such, when bringing transformer
    models to the edge onto embedded devices in wide-area networks, communication
    optimization is a critical variable.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘进行有效的基础模型训练需要足够的通信带宽或显著的压缩。尽管在带宽高的环境（100 Gbit+）中，通信扮演着次要角色，即使是具有数百万参数的模型，它在移动环境中的FL轮次总时间中仍然占据了重要位置（[表1](#S5.T1
    "在5个结果中 ‣ LLMs在边缘的联邦微调：优点、缺点和不足")）。这一点在考虑粒度时尤为明显。尽管$G\gg 1$对于所有在LTE连接的FL系统中的FLAN-T5变压器都成立，$G\approx
    0$则强烈指示了在给定硬件类型和网络条件下分布式训练DL模型的低效。因此，当将变压器模型引入广域网中的嵌入式设备时，通信优化是一个关键变量。
- en: '![Refer to caption](img/75e9ac5ce2e62d443a97dc70c8dde944.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/75e9ac5ce2e62d443a97dc70c8dde944.png)'
- en: 'Figure 3: DL training step times across FLAN-T5 transformer models with varying
    minibatch sizes on the Samsum dataset running on the NVIDIA A100 and Jetson AGX
    Orin platform. Detailed metrics are available in Appendix [Appendix B](#A2 "Appendix
    B Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad,
    the Ugly").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 NVIDIA A100 和 Jetson AGX Orin 平台上运行的 Samsum 数据集上的 FLAN-T5 变换器模型在不同小批量大小下的
    DL 训练步骤时间。详细指标可在附录 [附录 B](#A2 "附录 B 结果 ‣ LLMs 在极端边缘的联邦微调：好的，坏的，丑的") 中查看。
- en: 'Increasing the minibatch size on embedded devices does not scale well. From
    our preceding insights, we know that we run into a memory bottleneck when fine-tuning
    FLAN-T5 on the edge. To further explore the source, we find linearly growing opt.step()
    times for all models as we scale the minibatch size on the Orins, while the step
    times on the A100 platform scale logarithmically with increasing batch size ([Figure 3](#S5.F3
    "In 5 Results ‣ Federated Fine-Tuning of LLMs on the Very Edge: The Good, the
    Bad, the Ugly")). From that, we derive the open research question: The root causes
    for the slowdown and what can be done to optimize it are interesting directions
    for future work.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入式设备上增加小批量大小的扩展性较差。从我们之前的见解中，我们知道在边缘对 FLAN-T5 进行微调时会遇到内存瓶颈。为了进一步探索原因，我们发现随着小批量大小的增加，在
    Orins 上所有模型的 opt.step() 时间线性增长，而在 A100 平台上的步骤时间随着批量大小的增加而对数增长（[图 3](#S5.F3 "在第
    5 部分结果 ‣ LLMs 在极端边缘的联邦微调：好的，坏的，丑的")）。由此，我们提出了一个开放的研究问题：减速的根本原因是什么？可以采取哪些措施来优化它是未来工作的有趣方向。
- en: 6 Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In our work, we have studied hardware performance optimization for fine-tuning
    LLMs and foundation models on the very edge with state-of-the-art embedded devices.
    We have shown the computational bottlenecks on embedded FL clients with an in-depth
    micro-benchmark and identified a strong correlation between $\eta_{e}$, a key
    system design variable in edge computing systems, and the MFU, a core performance
    indicator in the HPC domain. Furthermore, we have quantified the trade-off between
    communication and computation in FL systems with granularity, which shows the
    stark need to improve communication efficiency in FL systems to render FM training
    practical. With our work, we hope to raise awareness of the substantial challenges
    that need to be overcome to enable FM training on a broad basis for systems suffering
    from limited computational and network resources.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们研究了在最先进的嵌入式设备上对 LLMs 和基础模型进行微调的硬件性能优化。我们展示了嵌入式 FL 客户端的计算瓶颈，通过深入的微基准测试，识别出边缘计算系统中的关键系统设计变量
    $\eta_{e}$ 与 HPC 领域核心性能指标 MFU 之间的强相关性。此外，我们量化了 FL 系统中通信与计算的权衡，这表明在 FL 系统中改善通信效率的迫切需要，以使
    FM 训练具有实际可行性。通过我们的工作，我们希望提高对在计算和网络资源有限的系统上广泛进行 FM 训练所面临的重大挑战的认识。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢与资助披露
- en: 'This work is partially funded by the Bavarian Ministry of Economic Affairs,
    Regional Development and Energy (Grant: DIK0446/01) and the German Research Foundation
    (Grant: 392214008).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由巴伐利亚经济事务、区域发展和能源部（资助编号：DIK0446/01）和德国研究基金会（资助编号：392214008）资助。
- en: References
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,
    2021.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
    Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
    等。关于基础模型的机会与风险。arXiv 预印本 arXiv:2108.07258，2021年。'
- en: '[2] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv
    preprint arXiv:2203.15556, 2022.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, 等。计算优化大型语言模型的训练。arXiv 预印本 arXiv:2203.15556，2022年。'
- en: '[3] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
    Data movement is all you need: A case study on optimizing transformers. Proceedings
    of Machine Learning and Systems, 3:711–732, 2021.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li 和 Torsten Hoefler。数据传输是你所需要的一切：优化变换器的案例研究。《机器学习与系统会议论文集》，3:711–732，2021年。'
- en: '[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane
    Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros,
    Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
    Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,
    Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
    Scaling instruction-finetuned language models. 2022.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hyung Won Chung、Le Hou、Shayne Longpre、Barret Zoph、Yi Tay、William Fedus、Yunxuan
    Li、Xuezhi Wang、Mostafa Dehghani、Siddhartha Brahma、Albert Webson、Shixiang Shane
    Gu、Zhuyun Dai、Mirac Suzgun、Xinyun Chen、Aakanksha Chowdhery、Alex Castro-Ros、Marie
    Pellat、Kevin Robinson、Dasha Valter、Sharan Narang、Gaurav Mishra、Adams Yu、Vincent
    Zhao、Yanping Huang、Andrew Dai、Hongkun Yu、Slav Petrov、Ed H. Chi、Jeff Dean、Jacob
    Devlin、Adam Roberts、Denny Zhou、Quoc V. Le 和 Jason Wei。扩展指令微调语言模型。2022年。'
- en: '[5] Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao
    Sun. FedBERT: When federated learning meets pre-training. ACM Transactions on
    Intelligent Systems and Technology, 13(4):1–26, August 2022.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yuanyishu Tian、Yao Wan、Lingjuan Lyu、Dezhong Yao、Hai Jin 和 Lichao Sun。FedBERT：当联邦学习遇上预训练。ACM
    智能系统与技术学报，13(4)：1–26，2022年8月。'
- en: '[6] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin
    Wang, and Yiran Chen. Towards building the federated gpt: Federated instruction
    tuning. arXiv preprint arXiv:2305.05644, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jianyi Zhang、Saeed Vahidian、Martin Kuo、Chunyuan Li、Ruiyi Zhang、Guoyin Wang
    和 Yiran Chen。构建联邦 GPT：联邦指令调优。arXiv 预印本 arXiv:2305.05644，2023年。'
- en: '[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep
    nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tianqi Chen、Bing Xu、Chiyuan Zhang 和 Carlos Guestrin。以亚线性内存成本训练深度网络。arXiv
    预印本 arXiv:1604.06174，2016年。'
- en: '[8] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。Llm. int8 ():
    用于大规模变换器的 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339，2022年。'
- en: '[9] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
    Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale
    model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages
    551–564, 2021.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jie Ren、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyan
    Yang、Minjia Zhang、Dong Li 和 Yuxiong He。Zero-offload：使亿规模模型训练民主化。发表于 2021 USENIX
    年度技术会议（USENIX ATC 21），第551–564页，2021年。'
- en: '[10] Amazon AWS p3 instance types. [https://aws.amazon.com/ec2/instance-types/p3/](https://aws.amazon.com/ec2/instance-types/p3/).
    Accessed: 2023-09-27.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Amazon AWS p3 实例类型。 [https://aws.amazon.com/ec2/instance-types/p3/](https://aws.amazon.com/ec2/instance-types/p3/)。访问时间：2023-09-27。'
- en: '[11] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu,
    Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences
    on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Yanli Zhao、Andrew Gu、Rohan Varma、Liang Luo、Chien-Chin Huang、Min Xu、Less
    Wright、Hamid Shojanazeri、Myle Ott、Sam Shleifer 等。Pytorch fsdp：扩展完全分片数据并行的经验。arXiv
    预印本 arXiv:2304.11277，2023年。'
- en: '[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav Mishra、Adam
    Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann 等。Palm：通过路径扩展语言建模。arXiv
    预印本 arXiv:2204.02311，2022年。'
- en: '[13] Blesson Varghese, Nan Wang, David Bermbach, Cheol-Ho Hong, Eyal de Lara,
    Weisong Shi, and Christopher Stewart. A survey on edge performance benchmarking.
    2020.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Blesson Varghese、Nan Wang、David Bermbach、Cheol-Ho Hong、Eyal de Lara、Weisong
    Shi 和 Christopher Stewart。边缘性能基准测试调查。2020年。'
- en: '[14] Samuel S. Ogden and Tian Guo. MODI: Mobile deep inference made efficient
    by edge computing. In USENIX Workshop on Hot Topics in Edge Computing (HotEdge
    18), Boston, MA, July 2018\. USENIX Association.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Samuel S. Ogden 和 Tian Guo。MODI：通过边缘计算实现高效的移动深度推理。发表于 USENIX 边缘计算热话题研讨会（HotEdge
    18），波士顿，MA，2018年7月。USENIX 协会。'
- en: '[15] Martino Trevisan, Ali Safari Khatouni, and Danilo Giordano. Errant: Realistic
    emulation of radio access networks. 2021.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Martino Trevisan、Ali Safari Khatouni 和 Danilo Giordano。Errant：无线接入网络的真实仿真。2021年。'
- en: '[16] Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Javier Fernandez-Marques,
    Yan Gao, Lorenzo Sani, Kwing Hei Li, Titouan Parcollet, Pedro Porto Buarque de Gusmão,
    and Nicholas D. Lane. Flower: A friendly federated learning research framework,
    2020.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Daniel J. Beutel、Taner Topal、Akhil Mathur、Xinchi Qiu、Javier Fernandez-Marques、Yan
    Gao、Lorenzo Sani、Kwing Hei Li、Titouan Parcollet、Pedro Porto Buarque de Gusmão
    和 Nicholas D. Lane。Flower：一个友好的联邦学习研究框架，2020年。'
- en: '[17] Guillaume Fieni, Romain Rouvoy, and Lionel Seiturier. SelfWatts: On-the-fly
    selection of performance events to optimize software-defined power meters. In
    2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing
    (CCGrid). IEEE, May 2021.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Guillaume Fieni, Romain Rouvoy, 和 Lionel Seiturier. SelfWatts：动态选择性能事件以优化软件定义的功率计。在
    2021 IEEE/ACM 第21届国际集群、云计算与互联网计算研讨会 (CCGrid)。IEEE，2021年5月。'
- en: '[18] Nathan C. Frey, Baolin Li, Joseph McDonald, Dan Zhao, Michael Jones, David
    Bestor, Devesh Tiwari, Vijay Gadepally, and Siddharth Samsi. Benchmarking resource
    usage for efficient distributed deep learning, 2022.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Nathan C. Frey, Baolin Li, Joseph McDonald, Dan Zhao, Michael Jones, David
    Bestor, Devesh Tiwari, Vijay Gadepally, 和 Siddharth Samsi. 资源使用基准测试以实现高效的分布式深度学习，2022年。'
- en: '[19] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
    Agüera y Arcas. Communication-efficient learning of deep networks from decentralized
    data. 2016.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, 和 Blaise
    Agüera y Arcas. 从分散数据中高效学习深度网络。2016年。'
- en: '[20] Kai Hwang. Advanced Computer Architecture: Parallelism,Scalability,Programmability.
    McGraw-Hill Higher Education, 1st edition, 1992.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Kai Hwang. 《高级计算机架构：并行性、可扩展性、可编程性》。McGraw-Hill Higher Education，第一版，1992年。'
- en: '[21] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum
    corpus: A human-annotated dialogue dataset for abstractive summarization. 2019.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, 和 Aleksander Wawer. Samsum
    语料库：用于抽象总结的人类注释对话数据集。2019年。'
- en: '[22] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu,
    Harsha V. Madhyastha, and Mosharaf Chowdhury. FedScale: Benchmarking model and
    system performance of federated learning at scale. In International Conference
    on Machine Learning (ICML), 2022.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu,
    Harsha V. Madhyastha, 和 Mosharaf Chowdhury. FedScale：大规模联邦学习的模型和系统性能基准测试。在国际机器学习大会
    (ICML)，2022年。'
- en: '[23] Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
    Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
    Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr.
    Fedml: A research library and benchmark for federated machine learning. Advances
    in Neural Information Processing Systems, Best Paper Award at Federate Learning
    Workshop, 2020.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
    Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
    Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, 和 Salman Avestimehr. Fedml：一个用于联邦机器学习的研究库和基准。神经信息处理系统进展，联邦学习研讨会最佳论文奖，2020年。'
- en: '[24] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
    H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for
    federated settings, 2018.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
    H. Brendan McMahan, Virginia Smith, 和 Ameet Talwalkar. Leaf：联邦设置的基准，2018年。'
- en: Appendix A Experimental Setup
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验设置
- en: A.1 FL aggregation strategy & configuration
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 FL 聚合策略与配置
- en: For our experiments, we use Federated Averaging [[19](#bib.bib19)] as it provides
    a strong baseline for FL workloads. For every FL training round, we randomly sample
    three clients out of a total of ten clients and train one local epoch on each
    client with a minibatch size of 32\. After each round, we communicate the locally
    trained models, aggregate them on the server, and test the global model performance.
    This procedure is repeated for 20 FL training rounds. FL training is implemented
    with Flower [[16](#bib.bib16)]. However, the FL aggregation process is not the
    focus of our work.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用 Federated Averaging [[19](#bib.bib19)]，因为它为 FL 工作负载提供了强有力的基准。每次
    FL 训练轮次中，我们从十个客户端中随机抽取三个客户端，并在每个客户端上进行一个本地周期的训练，迷你批量大小为 32。每轮后，我们将本地训练的模型进行通信，汇总到服务器上，并测试全局模型性能。这个过程重复进行
    20 次 FL 训练轮次。FL 训练是使用 Flower [[16](#bib.bib16)] 实现的。然而，FL 聚合过程不是我们工作的重点。
- en: Appendix B Results
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 结果
- en: This appendix section contains additional results on the energy efficiency measurements
    and micro-benchmark timings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录部分包含了能效测量和微基准时序的额外结果。
- en: B.1 Energy efficiency
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 能效
- en: 'Energy efficiency is derived from the average power draw of each device during
    our experiments. The experiments are fixed to 100 steps per epoch for each experiment.
    [Table 2](#A2.T2 "In B.1 Energy efficiency ‣ Appendix B Results ‣ Federated Fine-Tuning
    of LLMs on the Very Edge: The Good, the Bad, the Ugly") contains details on our
    energy efficiency calculations.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 能效数据来源于每个设备在实验过程中的平均功耗。实验固定为每次实验 100 步。 [表 2](#A2.T2 "在 B.1 能效 ‣ 附录 B 结果 ‣ 在边缘上的联邦微调：好、坏、丑")
    包含了我们能效计算的详细信息。
- en: 'Table 2: Energy efficiency is measured in $\frac{\mathrm{TPS}}{W}$.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：能效的测量单位是 $\frac{\mathrm{TPS}}{W}$。
- en: FLAN-T5 Model Minib. Size Device Avg. Power Draw (W) $\eta_{e}$ TPS Small 1
    A100 75.8 21.16 1603.62 AGX Orin 16.7 55.26 923.04 8 A100 103.17 121.63 12548.15
    AGX Orin 28.1 200.75 5641.15 16 A100 120.96 215.03 26010.34 AGX Orin 35.8 198.32
    7099.35 32 A100 171.15 258.52 44246.31 AGX Orin 38.84 196.53 7633.56 64 A100 212.53
    304.82 64782.11 AGX Orin 38.82 203.57 7903.25 128 A100 247.72 327.2 81055.16 AGX
    Orin 41.08 195.73 8040.93 Base 1 A100 85.63 13.0 1113.37 AGX Orin 24.86 25.23
    627.45 8 A100 135.57 63.77 8645.35 AGX Orin 31.3 74.21 2322.81 16 A100 159.09
    94.55 15041.23 AGX Orin 38.59 66.51 2566.63 32 A100 223.55 99.28 22194.39 AGX
    Orin 38.54 69.84 2691.45 64 A100 260.68 100.08 26088.77 AGX Orin Out of memory
    Large 1 A100 91.61 6.06 554.97 AGX Orin 26.1 11.24 293.38 8 A100 173.43 24.24
    4204.11 AGX Orin 41.46 20.36 844.18 16 A100 196.9 33.76 6647.37 AGX Orin Out of
    memory XL 1 A100 128.21 4.31 552.0 AGX Orin Out of memory
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5 模型 最小批量 设备 平均功耗 (W) $\eta_{e}$ TPS 小 1 A100 75.8 21.16 1603.62 AGX Orin
    16.7 55.26 923.04 8 A100 103.17 121.63 12548.15 AGX Orin 28.1 200.75 5641.15 16
    A100 120.96 215.03 26010.34 AGX Orin 35.8 198.32 7099.35 32 A100 171.15 258.52
    44246.31 AGX Orin 38.84 196.53 7633.56 64 A100 212.53 304.82 64782.11 AGX Orin
    38.82 203.57 7903.25 128 A100 247.72 327.2 81055.16 AGX Orin 41.08 195.73 8040.93
    基础 1 A100 85.63 13.0 1113.37 AGX Orin 24.86 25.23 627.45 8 A100 135.57 63.77 8645.35
    AGX Orin 31.3 74.21 2322.81 16 A100 159.09 94.55 15041.23 AGX Orin 38.59 66.51
    2566.63 32 A100 223.55 99.28 22194.39 AGX Orin 38.54 69.84 2691.45 64 A100 260.68
    100.08 26088.77 AGX Orin 内存不足 大 1 A100 91.61 6.06 554.97 AGX Orin 26.1 11.24 293.38
    8 A100 173.43 24.24 4204.11 AGX Orin 41.46 20.36 844.18 16 A100 196.9 33.76 6647.37
    AGX Orin 内存不足 XL 1 A100 128.21 4.31 552.0 AGX Orin 内存不足
- en: B.2 Model FLOP Utilization
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 模型 FLOP 利用率
- en: 'MFU helps to identify computational or memory bottlenecks. [Table 3](#A2.T3
    "In B.2 Model FLOP Utilization ‣ Appendix B Results ‣ Federated Fine-Tuning of
    LLMs on the Very Edge: The Good, the Bad, the Ugly") depicts all details required
    to calculate the MFU.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MFU 有助于识别计算或内存瓶颈。 [表 3](#A2.T3 "在 B.2 模型 FLOP 利用率 ‣ 附录 B 结果 ‣ 在边缘上的联邦微调：好、坏、丑")
    展示了计算 MFU 所需的所有细节。
- en: 'Table 3: Details on MFU calculation for the NVIDIA A100 and Jetson AGX Orin
    platforms.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：NVIDIA A100 和 Jetson AGX Orin 平台的 MFU 计算细节。
- en: 'FLAN-T5 Model Minib. Size Device TPS Params # Layers $d_{\mathrm{model}}$ $n_{\mathrm{att_{h}eads}}$
    Seq. Len. MFU Small 1 A100 1657.0 0.0 8.0 512.0 1024.0 8.0 512.0 0.3 Orin AGX
    927.0 0.0 8.0 512.0 1024.0 8.0 512.0 1.1 8 A100 13051.0 0.0 8.0 512.0 1024.0 8.0
    512.0 2.0 Orin AGX 5665.0 0.0 8.0 512.0 1024.0 8.0 512.0 6.5 16 A100 26741.0 0.0
    8.0 512.0 1024.0 8.0 512.0 4.2 Orin AGX 7112.0 0.0 8.0 512.0 1024.0 8.0 512.0
    8.1 32 A100 45428.0 0.0 8.0 512.0 1024.0 8.0 512.0 7.1 Orin AGX 7713.0 0.0 8.0
    512.0 1024.0 8.0 512.0 8.8 64 A100 65944.0 0.0 8.0 512.0 1024.0 8.0 512.0 10.3
    Orin AGX 8040.0 0.0 8.0 512.0 1024.0 8.0 512.0 9.2 128 A100 82045.0 0.0 8.0 512.0
    1024.0 8.0 512.0 12.8 Orin AGX 8094.0 0.0 8.0 512.0 1024.0 8.0 512.0 9.3 Base
    1 A100 1134.0 0.0 12.0 768.0 2048.0 12.0 512.0 0.6 Orin AGX 631.0 0.0 12.0 768.0
    2048.0 12.0 512.0 2.3 8 A100 8805.0 0.0 12.0 768.0 2048.0 12.0 512.0 4.4 Orin
    AGX 2339.0 0.0 12.0 768.0 2048.0 12.0 512.0 8.5 16 A100 15380.0 0.0 12.0 768.0
    2048.0 12.0 512.0 7.6 Orin AGX 2591.0 0.0 12.0 768.0 2048.0 12.0 512.0 9.4 32
    A100 22427.0 0.0 12.0 768.0 2048.0 12.0 512.0 11.1 Orin AGX 2692.0 0.0 12.0 768.0
    2048.0 12.0 512.0 9.8 64 A100 26250.0 0.0 12.0 768.0 2048.0 12.0 512.0 13.0 AGX
    Orin Out of memory Large 1 A100 562.0 0.0 24.0 1024.0 2816.0 16.0 512.0 0.9 Orin
    AGX 298.0 0.0 24.0 1024.0 2816.0 16.0 512.0 3.4 8 A100 4260.0 0.0 24.0 1024.0
    2816.0 16.0 512.0 6.6 Orin AGX 853.0 0.0 24.0 1024.0 2816.0 16.0 512.0 9.7 16
    A100 6728.0 0.0 24.0 1024.0 2816.0 16.0 512.0 10.5 AGX Orin Out of memory XL 1
    A100 560.0 0.0 24.0 2048.0 5120.0 32.0 512.0 3.1 AGX Orin Out of memory'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5 模型 迷你批次大小 设备 TPS 参数 数层 $d_{\mathrm{model}}$ $n_{\mathrm{att_{h}eads}}$
    序列长度 MFU 小 1 A100 1657.0 0.0 8.0 512.0 1024.0 8.0 512.0 0.3 Orin AGX 927.0 0.0
    8.0 512.0 1024.0 8.0 512.0 1.1 8 A100 13051.0 0.0 8.0 512.0 1024.0 8.0 512.0 2.0
    Orin AGX 5665.0 0.0 8.0 512.0 1024.0 8.0 512.0 6.5 16 A100 26741.0 0.0 8.0 512.0
    1024.0 8.0 512.0 4.2 Orin AGX 7112.0 0.0 8.0 512.0 1024.0 8.0 512.0 8.1 32 A100
    45428.0 0.0 8.0 512.0 1024.0 8.0 512.0 7.1 Orin AGX 7713.0 0.0 8.0 512.0 1024.0
    8.0 512.0 8.8 64 A100 65944.0 0.0 8.0 512.0 1024.0 8.0 512.0 10.3 Orin AGX 8040.0
    0.0 8.0 512.0 1024.0 8.0 512.0 9.2 128 A100 82045.0 0.0 8.0 512.0 1024.0 8.0 512.0
    12.8 Orin AGX 8094.0 0.0 8.0 512.0 1024.0 8.0 512.0 9.3 Base 1 A100 1134.0 0.0
    12.0 768.0 2048.0 12.0 512.0 0.6 Orin AGX 631.0 0.0 12.0 768.0 2048.0 12.0 512.0
    2.3 8 A100 8805.0 0.0 12.0 768.0 2048.0 12.0 512.0 4.4 Orin AGX 2339.0 0.0 12.0
    768.0 2048.0 12.0 512.0 8.5 16 A100 15380.0 0.0 12.0 768.0 2048.0 12.0 512.0 7.6
    Orin AGX 2591.0 0.0 12.0 768.0 2048.0 12.0 512.0 9.4 32 A100 22427.0 0.0 12.0
    768.0 2048.0 12.0 512.0 11.1 Orin AGX 2692.0 0.0 12.0 768.0 2048.0 12.0 512.0
    9.8 64 A100 26250.0 0.0 12.0 768.0 2048.0 12.0 512.0 13.0 AGX Orin 内存不足 大 1 A100
    562.0 0.0 24.0 1024.0 2816.0 16.0 512.0 0.9 Orin AGX 298.0 0.0 24.0 1024.0 2816.0
    16.0 512.0 3.4 8 A100 4260.0 0.0 24.0 1024.0 2816.0 16.0 512.0 6.6 Orin AGX 853.0
    0.0 24.0 1024.0 2816.0 16.0 512.0 9.7 16 A100 6728.0 0.0 24.0 1024.0 2816.0 16.0
    512.0 10.5 AGX Orin 内存不足 XL 1 A100 560.0 0.0 24.0 2048.0 5120.0 32.0 512.0 3.1
    AGX Orin 内存不足
- en: B.3 Micro-benchmark
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 微基准测试
- en: '[Table 4](#A2.T4 "In B.3 Micro-benchmark ‣ Appendix B Results ‣ Federated Fine-Tuning
    of LLMs on the Very Edge: The Good, the Bad, the Ugly") describes the step timings
    in detail and provides a perspective on the speed differences between data center
    and embedded hardware.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4](#A2.T4 "在 B.3 微基准测试 ‣ 附录 B 结果 ‣ 在边缘的联邦微调：优点、缺点和不足") 详细描述了步骤时序，并提供了数据中心与嵌入式硬件之间速度差异的视角。'
- en: 'Table 4: Results of the micro-benchmark of the FLAN-T5 transformer model family
    on the NVIDIA A100 and Jetson AGX Orin platforms. The sequence length per batch
    item is 512.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：FLAN-T5 变压器模型系列在 NVIDIA A100 和 Jetson AGX Orin 平台上的微基准测试结果。每批次项的序列长度为 512。
- en: FLAN-T5 Model Batch Size Device Backward Opt. Step Loss Calc. Forward Batch
    Loading TPS Total Time Small 1 A100 0.09 0.16 0.0 0.06 0.01 1657.87 0.32 AGX Orin
    0.15 0.3 0.0 0.09 0.01 927.51 0.55 8 A100 0.09 0.16 0.0 0.06 0.01 13051.3 0.33
    AGX Orin 0.22 0.4 0.0 0.1 0.01 5665.54 0.73 16 A100 0.09 0.16 0.0 0.06 0.01 26741.79
    0.31 AGX Orin 0.36 0.63 0.0 0.15 0.01 7112.45 1.15 32 A100 0.12 0.18 0.0 0.06
    0.01 45428.27 0.37 AGX Orin 0.66 1.16 0.0 0.32 0.01 7713.02 2.15 64 A100 0.17
    0.26 0.0 0.06 0.01 65944.32 0.51 AGX Orin 1.28 2.22 0.0 0.64 0.01 7992.08 4.15
    128 A100 0.28 0.46 0.0 0.06 0.02 82045.0 0.81 AGX Orin 2.6 4.34 0.0 1.21 0.01
    8046.96 8.15 Base 1 A100 0.14 0.23 0.0 0.08 0.01 1134.51 0.46 AGX Orin 0.22 0.45
    0.0 0.14 0.01 631.08 0.82 8 A100 0.14 0.23 0.0 0.09 0.01 8805.38 0.47 AGX Orin
    0.51 0.95 0.0 0.3 0.01 2339.49 1.76 16 A100 0.17 0.27 0.0 0.09 0.02 15380.06 0.54
    AGX Orin 0.93 1.69 0.0 0.57 0.01 2590.44 3.19 32 A100 0.24 0.38 0.0 0.1 0.01 22427.21
    0.74 AGX Orin 1.81 3.19 0.0 1.09 0.01 2692.26 6.09 64 A100 0.4 0.65 0.0 0.19 0.01
    26250.25 1.26 AGX Orin Out of memory Large 1 A100 0.27 0.46 0.0 0.18 0.02 562.2
    0.92 AGX Orin 0.43 1.03 0.0 0.27 0.01 298.91 1.75 8 A100 0.29 0.49 0.0 0.18 0.02
    4260.38 0.97 AGX Orin 1.31 2.62 0.0 0.92 0.01 850.43 4.85 16 A100 0.4 0.62 0.0
    0.2 0.02 6728.79 1.23 AGX Orin Out of memory XL 1 A100 0.27 0.48 0.0 0.17 0.02
    560.07 0.93 AGX Orin Out of memory
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: FLAN-T5 模型 批量大小 设备 反向优化 步骤 损失计算 正向批量 加载 TPS 总时间 小 1 A100 0.09 0.16 0.0 0.06
    0.01 1657.87 0.32 AGX Orin 0.15 0.3 0.0 0.09 0.01 927.51 0.55 8 A100 0.09 0.16
    0.0 0.06 0.01 13051.3 0.33 AGX Orin 0.22 0.4 0.0 0.1 0.01 5665.54 0.73 16 A100
    0.09 0.16 0.0 0.06 0.01 26741.79 0.31 AGX Orin 0.36 0.63 0.0 0.15 0.01 7112.45
    1.15 32 A100 0.12 0.18 0.0 0.06 0.01 45428.27 0.37 AGX Orin 0.66 1.16 0.0 0.32
    0.01 7713.02 2.15 64 A100 0.17 0.26 0.0 0.06 0.01 65944.32 0.51 AGX Orin 1.28
    2.22 0.0 0.64 0.01 7992.08 4.15 128 A100 0.28 0.46 0.0 0.06 0.02 82045.0 0.81
    AGX Orin 2.6 4.34 0.0 1.21 0.01 8046.96 8.15 基础 1 A100 0.14 0.23 0.0 0.08 0.01
    1134.51 0.46 AGX Orin 0.22 0.45 0.0 0.14 0.01 631.08 0.82 8 A100 0.14 0.23 0.0
    0.09 0.01 8805.38 0.47 AGX Orin 0.51 0.95 0.0 0.3 0.01 2339.49 1.76 16 A100 0.17
    0.27 0.0 0.09 0.02 15380.06 0.54 AGX Orin 0.93 1.69 0.0 0.57 0.01 2590.44 3.19
    32 A100 0.24 0.38 0.0 0.1 0.01 22427.21 0.74 AGX Orin 1.81 3.19 0.0 1.09 0.01
    2692.26 6.09 64 A100 0.4 0.65 0.0 0.19 0.01 26250.25 1.26 AGX Orin 内存不足 大 1 A100
    0.27 0.46 0.0 0.18 0.02 562.2 0.92 AGX Orin 0.43 1.03 0.0 0.27 0.01 298.91 1.75
    8 A100 0.29 0.49 0.0 0.18 0.02 4260.38 0.97 AGX Orin 1.31 2.62 0.0 0.92 0.01 850.43
    4.85 16 A100 0.4 0.62 0.0 0.2 0.02 6728.79 1.23 AGX Orin 内存不足 XL 1 A100 0.27 0.48
    0.0 0.17 0.02 560.07 0.93 AGX Orin 内存不足
