- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KInIT 在 SemEval-2024 任务8：微调的LLMs用于多语言机器生成文本检测
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13671](https://ar5iv.labs.arxiv.org/html/2402.13671)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13671](https://ar5iv.labs.arxiv.org/html/2402.13671)
- en: Michal Spiegel^(1,2)    Dominik Macko¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Michal Spiegel^(1,2)    Dominik Macko¹
- en: ¹ Kempelen Institute of Intelligent Technologies
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ Kempelen 智能技术研究所
- en: ² Faculty of Informatics, Masaryk University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ² 马萨里克大学信息学系
- en: michal.spiegel@intern.kinit.sk, dominik.macko@kinit.sk
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: michal.spiegel@intern.kinit.sk, dominik.macko@kinit.sk
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual
    black-box machine-generated text detection. Such a detection is important for
    preventing a potential misuse of large language models (LLMs), the newest of which
    are very capable in generating multilingual human-like texts. We have coped with
    this task in multiple ways, utilizing language identification and parameter-efficient
    fine-tuning of smaller LLMs for text classification. We have further used the
    per-language classification-threshold calibration to uniquely combine fine-tuned
    models predictions with statistical detection metrics to improve generalization
    of the system detection performance. Our submitted method achieved competitive
    results, ranking at the fourth place, just under 1 percentage point behind the
    winner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SemEval-2024 任务8专注于多生成器、多领域和多语言黑箱机器生成文本检测。这种检测对于防止大型语言模型（LLMs）的潜在滥用至关重要，其中最新的模型在生成多语言类人文本方面非常强大。我们以多种方式应对这一任务，利用语言识别和小型LLMs的参数高效微调进行文本分类。我们还使用了每种语言分类阈值校准，将微调模型的预测与统计检测指标独特地结合起来，以提高系统检测性能的泛化。我们提交的方法取得了竞争力的结果，排名第四，仅比第一名低不到1个百分点。
- en: 'KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KInIT 在 SemEval-2024 任务8：微调的LLMs用于多语言机器生成文本检测
- en: Michal Spiegel^(1,2)  and Dominik Macko¹ ¹ Kempelen Institute of Intelligent
    Technologies ² Faculty of Informatics, Masaryk University michal.spiegel@intern.kinit.sk,
    dominik.macko@kinit.sk
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Michal Spiegel^(1,2)  和 Dominik Macko¹ ¹ Kempelen 智能技术研究所 ² 马萨里克大学信息学系 michal.spiegel@intern.kinit.sk,
    dominik.macko@kinit.sk
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent large language models (LLMs) are able to generate high-quality texts
    that are not easily detectable by human readers. A problem arises when such generated
    texts are misused for academic exams (Achiam et al., [2023](#bib.bib1)), plagiarism
    (Wahle et al., [2022](#bib.bib17)), disinformation spreading (Vykopal et al.,
    [2023](#bib.bib16)), etc. Therefore, it is crucial to develop automated means
    to detect machine-generated texts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）能够生成高质量的文本，这些文本不易被人类读者察觉。当这些生成的文本被滥用于学术考试（Achiam et al., [2023](#bib.bib1)）、剽窃（Wahle
    et al., [2022](#bib.bib17)）、虚假信息传播（Vykopal et al., [2023](#bib.bib16)）等情况时，就会出现问题。因此，开发自动化手段来检测机器生成的文本是至关重要的。
- en: 'SemEval-2024 Task 8 (Wang et al., [2024](#bib.bib18)) consists of three subtasks:
    A) binary human-written vs. machine-generated text classification, B) multi-way
    machine-generated text classification, and C) human-machine mixed text detection.
    In our work, we have focused on subtask A, especially its multilingual track.
    It covered 8 known languages for training (Arabic, Bulgarian, Chinese, English,
    German, Indonesian, Russian, Urdu), multiple domains (e.g., Wikipedia, news, abstracts),
    and multiple text generators (e.g., GPT-3, ChatGPT, BLOOMZ).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SemEval-2024 任务8（Wang et al., [2024](#bib.bib18)）包含三个子任务：A）二分类人类撰写与机器生成文本的分类，B）多类别机器生成文本分类，以及
    C）人机混合文本检测。在我们的工作中，我们专注于子任务A，特别是其多语言追踪。它覆盖了8种已知语言用于训练（阿拉伯语、保加利亚语、中文、英语、德语、印尼语、俄语、乌尔都语）、多个领域（例如，维基百科、新闻、摘要），以及多种文本生成器（例如，GPT-3、ChatGPT、BLOOMZ）。
- en: 'During our participation in the shared task, we have explored various alternatives.
    Our best submitted solution (illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection")) combines two fine-tuned LLMs (green-colored) with statistical
    detection (orange-colored) using a two-step majority voting (purple-colored) based
    ensemble method. Such a system achieved fourth place in the final leaderboard,
    with a performance of 95% in accuracy, within 1 percentage point range behind
    the winning system. We have published the source code for easier replication purposes¹¹1[https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection](https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection).
    We have used the statistical detection methods implemented in the recently published
    IMGTB framework²²2[https://github.com/michalspiegel/IMGTB](https://github.com/michalspiegel/IMGTB),
    which will be extended to also support all the fine-tunning options that we have
    used in this work.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在参与共享任务期间，我们探索了各种替代方案。我们提交的最佳解决方案（如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection")所示）结合了两种微调的LLM（绿色）和统计检测（橙色），采用了基于两步多数投票（紫色）的集成方法。这样的系统在最终排行榜中获得了第四名，准确率为95%，距离获胜系统只有1个百分点的差距。我们已发布源代码以便于复制¹¹1[https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection](https://github.com/kinit-sk/semeval-2024-task-8-machine-text-detection)。我们使用了在最近发布的IMGTB框架中实现的统计检测方法²²2[https://github.com/michalspiegel/IMGTB](https://github.com/michalspiegel/IMGTB)，该框架将扩展以支持我们在此工作中使用的所有微调选项。'
- en: 'Figure 1: System components overview.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：系统组件概述。
- en: 'Our key observations and contributions include:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键观察和贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have proposed a unique way of combining the statistical and fine-tuned detection
    methods using a two-way majority voting and a per-language threshold calibration.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种独特的方式，通过两步多数投票和每种语言阈值校准，将统计检测方法和微调检测方法结合起来。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have proposed and compared three ensemble system alternatives to cope with
    multilingual machine-generated text detection (additional two in the post-deadline
    study).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出并比较了三种集成系统替代方案，以应对多语言机器生成文本检测（在截止日期后的研究中还增加了两种）。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have experienced a remarkably good performance of fine-tuned LLMs of 7B parameters
    in this task. We have not used such “a big hammer” for the classification task
    before.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在此任务中体验到了微调的7B参数LLM的出色性能。我们之前未曾使用过如此“大锤”进行分类任务。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have proposed the best-performing single-model system called rMistral (Mistral-7B
    fine-tuned in a robust way – using both the train and dev sets and obfuscating
    20% of the train data), achieving 0.97 AUC ROC on the test data. Although our
    per-language threshold calibration method would not bring the best accuracy on
    the test set (0.93), the threshold fixed to 1.0 (only predictions with a probability
    of 1, i.e. 100% confident, are marked as machine-generated) would won the competition
    (accuracy of 0.97). Nevertheless, we have noticed such a threshold performance
    only after the deadline and we considered the model being over-fitted (we would
    not submitted the results) which turned-out to be false.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了最佳表现的单模型系统rMistral（Mistral-7B以鲁棒方式微调——使用训练集和开发集，并对20%的训练数据进行模糊处理），在测试数据上获得了0.97
    AUC ROC。尽管我们的每语言阈值校准方法在测试集上无法带来最佳准确率（0.93），但将阈值固定为1.0（仅将概率为1，即100%自信的预测标记为机器生成）将赢得比赛（准确率为0.97）。然而，我们只在截止日期后注意到这种阈值性能，并认为模型过拟合（我们不会提交结果），但事实证明这种担忧是错误的。
- en: 2 Background and Methodology
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与方法
- en: For the machine-generated text detection task, three main groups of methods
    are nowadays used (Uchendu et al., [2023](#bib.bib14)). The first one is a stylometric
    detection, which uses linguistic features (e.g., n-grams) to differentiate between
    human and machine writing styles (Fröhling and Zubiaga, [2021](#bib.bib4); Kumarage
    et al., [2023](#bib.bib9)). The second group is a statistical detection, which
    uses statistical distribution based on a pre-trained language model (e.g., GPT2)
    to calculate various metrics (e.g., entropy) that can be used even without training
    (i.e., better generalization) to differentiate machine and human written texts
    (Mitchell et al., [2023](#bib.bib13); Hans et al., [2024](#bib.bib7)). The last
    group is a fine-tuned detection, which further trains an already pre-trained language
    model for the detection task (Uchendu et al., [2020](#bib.bib15); Macko et al.,
    [2023](#bib.bib11)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器生成文本检测任务，目前主要使用三类方法（Uchendu et al., [2023](#bib.bib14)）。第一类是风格检测，它利用语言特征（例如，n-grams）来区分人类和机器的写作风格（Fröhling
    and Zubiaga, [2021](#bib.bib4); Kumarage et al., [2023](#bib.bib9)）。第二类是统计检测，它使用基于预训练语言模型（例如，GPT2）的统计分布来计算各种指标（例如，熵），这些指标甚至在没有训练的情况下（即更好的泛化能力）也可以用来区分机器和人类书写的文本（Mitchell
    et al., [2023](#bib.bib13); Hans et al., [2024](#bib.bib7)）。最后一类是微调检测，它进一步训练已经预训练的语言模型以用于检测任务（Uchendu
    et al., [2020](#bib.bib15); Macko et al., [2023](#bib.bib11)）。
- en: In the SemEval2024 Task 8 (Wang et al., [2024](#bib.bib18)), we have focused
    on the multilingual track of Subtask A, which aimed at a binary classification
    to differentiate between human-written and machine-generated texts. The provided
    dataset (not allowing additional training data) contained the predefined splits
    of train, dev, and test sets. The train and dev sets officially contained 8 languages
    (3 languages in the dev set only), while unknown number of languages is contained
    in the test set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SemEval2024 Task 8 (Wang et al., [2024](#bib.bib18))中，我们专注于 Subtask A 的多语言轨道，该任务旨在进行二分类，以区分人类书写和机器生成的文本。提供的数据集（不允许使用额外的训练数据）包含了预定义的训练集、开发集和测试集的划分。训练集和开发集官方包含
    8 种语言（开发集中仅有 3 种语言），而测试集中包含的语言数量未知。
- en: Due to a multilingual nature of the data and our previous experience in multilingual
    machine-generated text detection (Macko et al., [2023](#bib.bib11)), we wanted
    to try-out something new in this shared task. Our initial idea was to experiment
    with a per-language “mixture-of-experts”, which would consist of multiple models,
    fine-tuned in a monolingual way per each official language in the train and dev
    sets. Since it was expected that surprise languages will be present in the test
    set, we would have used an additional multilingually fine-tuned model for other
    languages. However, we have started the experiments only few weeks before the
    deadline, which gave us little time to cope with the problems such as over-fitting
    and hyper-parameter optimisation (shown as severe towards the deadline).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据的多语言特性以及我们在多语言机器生成文本检测中的先前经验（Macko et al., [2023](#bib.bib11)），我们希望在这个共享任务中尝试一些新方法。我们的初步想法是尝试每种语言的“专家混合”，该方法包括多个模型，每个模型都根据训练和开发集中的每种官方语言进行了单语种微调。由于预计测试集中会出现意外语言，我们将使用一个额外的多语言微调模型来处理其他语言。然而，我们仅在截止日期前几周才开始实验，这使我们没有足够的时间应对过拟合和超参数优化等问题（这些问题在截止日期前表现得尤为严重）。
- en: Therefore, while training these per-language models, we also started to fine-tune
    the Falcon-7B model (Almazrouei et al., [2023](#bib.bib2)) for the machine-generated
    text detection task, inspired by the winning system (Gagiano and Tian, [2023](#bib.bib5))
    of the recent ALTA 2023 shared task (although English monolingual). Since Falcon-7B
    is pre-trained on two languages only (English and French), we did not want to
    use it in a standalone way due to uncertain cross-lingual capability. Therefore,
    we have similarly fine-tuned the Mistral-7B model (Jiang et al., [2023](#bib.bib8)),
    which is similarly sized generative model outperforming even some 13B parameters
    models in common benchmarks. We have not previously experimented with such a “big
    hammer” for the task; therefore, it was a new interesting experience for us. We
    have further combined these LLMs with statistical detectors to ensure better generalization
    of the system, which is described in the following sections.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练这些每种语言的模型时，我们还开始了对Falcon-7B模型（Almazrouei等，[2023](#bib.bib2)）进行微调，以用于机器生成文本检测任务，灵感来自最近ALTA
    2023共享任务（尽管是英文单语言）的获胜系统（Gagiano和Tian，[2023](#bib.bib5)）。由于Falcon-7B仅在两种语言（英语和法语）上进行了预训练，我们不希望单独使用它，因为跨语言能力不确定。因此，我们类似地对Mistral-7B模型（Jiang等，[2023](#bib.bib8)）进行了微调，该模型是同样规模的生成模型，在常见基准测试中甚至优于一些13B参数的模型。我们以前没有尝试过这种“大锤”用于该任务，因此对我们来说是一个新的有趣的经验。我们进一步将这些LLM与统计检测器结合，以确保系统的更好泛化，具体描述在以下部分中。
    |
- en: 3 System Overview
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 系统概述
- en: '| System | Description |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 描述 |'
- en: '| --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\ast$LLM2S3 | The system described in this paper. It is an ensemble using
    two-step majority voting for predictions, consisting of 2 LLMs (Falcon-7B and
    Mistral-7B) fine-tuned using the train set only, 3 zero-shot statistical methods
    (Entropy, Rank, Binoculars) using Falcon-7B and Falcon-7B-Instruct for calculation
    of the metrics, utilizing language identification and per-language threshold calibration.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\ast$LLM2S3 | 本文描述的系统。它是一个使用两步多数投票进行预测的集成系统，由2个LLM（Falcon-7B和Mistral-7B）组成，这些LLM仅使用训练集进行微调，3种零样本统计方法（Entropy,
    Rank, Binoculars）使用Falcon-7B和Falcon-7B-Instruct进行指标计算，利用语言识别和每种语言的阈值校准。 |'
- en: '| PLMoE | Our initial idea representing a per-language mixture of experts.
    It uses Electra-Large-Discriminator for English and XML-RoBERTa-Large for each
    of other languages officially present in the train and dev sets. Models for languages
    present in the dev set only are trained using the dev set. For unknown languages
    the Mistral-7B fine-tuned using the whole train set is used. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| PLMoE | 我们最初的想法是表示每种语言的专家混合。它对英语使用Electra-Large-Discriminator，对训练集和开发集中每种其他语言使用XML-RoBERTa-Large。仅在开发集中出现的语言的模型使用开发集进行训练。对于未知语言，则使用了对整个训练集进行微调的Mistral-7B。
    |'
- en: '| rLLM2S3 | The same ensemble system as LLM2S3; however, the LLMs are fine-tuned
    using both the train and dev sets. Also, to increase the robustness of the system,
    we have obfuscated 20% of the train samples during fine-tuning, by using HomoglyphAttack
    and inserting zero-width-joiner character, inspired by our recent work (Macko
    et al., [2024](#bib.bib12)). |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| rLLM2S3 | 与LLM2S3相同的集成系统；然而，LLM模型在训练集和开发集上进行了微调。此外，为了提高系统的鲁棒性，我们在微调过程中使用了HomoglyphAttack和插入零宽连接符字符，对20%的训练样本进行了混淆，灵感来自我们最近的工作（Macko等，[2024](#bib.bib12)）。
    |'
- en: '| rLLM2B-ES | The post-deadline ensemble system similar to rLLM2S3; however,
    the Llama-2-7B is used instead of Falcon-7B and Binoculars is used solely in the
    statistical part (instead of a combination of 3 methods). Moreover, the fine-tuning
    process used the early stopping mechanism to alleviate the over-fitting. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| rLLM2B-ES | 类似于rLLM2S3的截止后集成系统；然而，使用的是Llama-2-7B而不是Falcon-7B，Binoculars仅用于统计部分（而不是三种方法的组合）。此外，微调过程使用了早停机制以减轻过拟合。
    |'
- en: '| LLM2B1 | The post-deadline ensemble system using the original LLM2S3 fine-tuned
    Falcon-7B and Mistral-7B models; however, classification thresholds are not calibrated,
    but only predictions with a probability of 1 (i.e., 100% confident) are marked
    as machine-generated. Such predictions are combined with Binoculars zero-shot
    prediction using the per-language threshold calibration. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LLM2B1 | 截止后集成系统，使用原始LLM2S3微调的Falcon-7B和Mistral-7B模型；然而，分类阈值没有校准，只有概率为1（即100%置信）的预测被标记为机器生成。这些预测与使用每种语言阈值校准的Binoculars零样本预测结合。
    |'
- en: 'Table 1: Description of system alternatives. The main system described in this
    paper is denoted by $\ast$. The last two alternatives were evaluated post-deadline.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：系统替代方案描述。本文描述的主要系统用$\ast$表示。最后两个替代方案在截止日期后进行了评估。
- en: 'Our best system (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ KInIT at
    SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection"))
    combines the predictions of two fine-tuned LLMs (Falcon-7B and Mistral-7B) with
    the selected statistical metrics (Entropy, Rank, Binoculars) by using a two-step
    majority voting. Firstly, a single majority-voted prediction results out of the
    three statistical metrics. Then, the final majority-voted prediction is a combination
    of the previous one with the Falcon and Mistral predictions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最佳系统（见图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ SemEval-2024任务8：针对多语言机器生成文本检测的精调LLMs")）结合了两个精调LLM（Falcon-7B
    和 Mistral-7B）的预测和选定的统计指标（Entropy, Rank, Binoculars），使用两步多数投票方法。首先，从三个统计指标中得出单一的多数投票预测结果。然后，最终的多数投票预测是将前者与Falcon和Mistral的预测结果结合得到的。
- en: Each prediction uses a separate classification-decision threshold, which is
    applied on prediction probabilities and statistical metrics. These thresholds
    are calibrated in a per-language way, meaning that separate thresholds are used
    for each language officially present in the train and dev sets, plus an additional
    threshold for unknown languages (i.e., not officially present in the train and
    dev sets). The thresholds are calibrated based on the machine-class prediction
    probabilities and statistical metrics for samples in the train and dev sets combined.
    The calibration maximized the difference between true positive rate (TPR) and
    false positive rate (FPR) based on the ROC (receiver operating characteristic)
    curve. The texts with probabilities (or statistical metrics) outreaching the thresholds
    are considered machine-generated, otherwise they are considered human-written.
    The thresholds are saved and used for prediction of test samples.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测使用单独的分类决策阈值，该阈值应用于预测概率和统计指标。这些阈值以每种语言为单位进行校准，意味着对训练和开发集中的每种官方语言使用不同的阈值，以及对未知语言（即在训练和开发集中未官方出现的语言）使用额外的阈值。这些阈值基于训练和开发集中样本的机器分类预测概率和统计指标进行校准。校准的目标是基于ROC（接收者操作特征）曲线最大化真实正例率（TPR）和假阳性率（FPR）之间的差异。超过阈值的概率（或统计指标）的文本被视为机器生成的，否则被视为人工编写的。这些阈值会被保存并用于测试样本的预测。
- en: Due to unknown languages in the test set and using the per-language threshold
    calibration, we have utilized the FastText³³3[https://pypi.org/project/fasttext-langdetect/](https://pypi.org/project/fasttext-langdetect/)
    language identification. Since it is not fully accurate, we have used such language
    information only if the prediction probability was greater than 0.5, otherwise
    the language was handled as unknown.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测试集中存在未知语言并且使用了每种语言的阈值校准，我们使用了FastText³³3[https://pypi.org/project/fasttext-langdetect/](https://pypi.org/project/fasttext-langdetect/)语言识别。由于其准确性不完全，我们仅在预测概率大于0.5时使用这种语言信息，否则将语言处理为未知语言。
- en: As mentioned, the system includes two fine-tuned LLMs, namely Falcon-7B and
    Mistral-7B. For the fine-tuning process, we have used a parameter efficient fine-tuning
    (PEFT) technique called QLoRA (Dettmers et al., [2023](#bib.bib3)) to minimize
    the computational costs of our system training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，该系统包括两个精调LLM，即Falcon-7B和Mistral-7B。在精调过程中，我们使用了一种名为QLoRA（Dettmers等，[2023](#bib.bib3)）的参数高效精调技术，以减少系统训练的计算成本。
- en: To enhance the system performance generalization, we have integrated a statistical
    part of the system, which is based on the three statistical metrics, namely Entropy
    (Lavergne et al., [2008](#bib.bib10)), Rank (Gehrmann et al., [2019](#bib.bib6)),
    and recently proposed Binoculars (Hans et al., [2024](#bib.bib7)). The statistical
    metrics are calculated using the Falcon-7B as a base model. Since Binoculars requires
    two models, it uses also Falcon-7B-Instruct (as a performer model).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升系统性能的泛化能力，我们集成了基于三个统计指标的系统统计部分，即Entropy（Lavergne等，[2008](#bib.bib10)）、Rank（Gehrmann等，[2019](#bib.bib6)）和最近提出的Binoculars（Hans等，[2024](#bib.bib7)）。这些统计指标是使用Falcon-7B作为基础模型计算的。由于Binoculars需要两个模型，它还使用了Falcon-7B-Instruct（作为执行模型）。
- en: 'Besides the described best submitted system, we have tried multiple system
    alternatives, which are briefly described in Table [1](#S3.T1 "Table 1 ‣ 3 System
    Overview ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection"). In addition to those ensembles, we have evaluated single detectors,
    namely Falcon, Mistral, S5 (a combination of 5 statistical metrics – likelihood,
    entropy, rank, log-rank, and llm-deviation), and Binoculars. After the deadline,
    we have also finished fine-tuning of Llama-2-7B and retrained the detectors using
    the early stopping (patience of 5) to prevent over-fitting. Also, when knowing
    the gold labels of the test set, we have evaluated various combinations of the
    trained detectors to see whether we have done the right decision for the submission.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '除了描述的最佳提交系统，我们还尝试了多种系统替代方案，这些方案在表[1](#S3.T1 "Table 1 ‣ 3 System Overview ‣
    KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
    Text Detection")中有简要描述。除了这些集合外，我们还评估了单个检测器，即Falcon、Mistral、S5（5种统计指标的组合——可能性、熵、排名、对数排名和llm偏差）和Binoculars。在截止日期之后，我们还完成了Llama-2-7B的微调，并使用早停（耐心为5）重新训练了检测器，以防止过拟合。此外，在知道测试集的金标准标签后，我们评估了训练检测器的各种组合，以查看我们是否做出了正确的提交决定。'
- en: 4 Experimental Setup
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: 'For the experimental purpose, we have used the defaults splits of the provided
    dataset, namely the train and dev sets in the pre-deadline experiments, and the
    gold labels of the test set for the post-deadline evaluation of the pre-deadline
    system alternatives. The main system described in this paper uses only the train
    set in the training process; however, uses both the train and dev sets for the
    classification threshold calibration. Some of the system alternatives used both
    the train and dev sets in the training process, as described in Table [1](#S3.T1
    "Table 1 ‣ 3 System Overview ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for
    Multilingual Machine-Generated Text Detection").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '出于实验目的，我们使用了提供的数据集的默认划分，即在截止日期前的实验中使用训练集和开发集，在截止日期后的评估中使用测试集的金标准标签。本文中描述的主要系统仅在训练过程中使用训练集；然而，在分类阈值校准过程中使用了训练集和开发集。某些系统替代方案在训练过程中使用了训练集和开发集，如表[1](#S3.T1
    "Table 1 ‣ 3 System Overview ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for
    Multilingual Machine-Generated Text Detection")所述。'
- en: As the key evaluation metric in the shared task is accuracy, we have also used
    this metric for the preliminary system evaluation and selection of the alternative
    for submission. Since classification task is sensitive to the used classification
    threshold, we have also used AUC ROC (area under curve of the receiver operating
    characteristic) as a threshold independent metric, providing better information
    about the classification capability.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于共享任务中的关键评估指标是准确率，我们还使用了该指标进行初步系统评估和提交替代方案的选择。由于分类任务对分类阈值非常敏感，我们还使用了AUC ROC（接收器操作特征曲线下面积）作为与阈值无关的指标，以提供更好的分类能力信息。
- en: For the fine-tuning process, we have used the official baseline script⁴⁴4[https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py](https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py),
    modified to export machine-class prediction probabilities in addition to the predictions.
    Since, it was not clear which version of the XLM-RoBERTa model was marked as a
    baseline in the multilingual track (with the known accuracy of 0.72), we have
    trained both the base (XLM-R-B) and large (XLM-R-L) versions. In addition, we
    have also included mDeBERTa-v3-base (mDeBERTa) model in our baselines, since it
    performed the best in our previous work (Macko et al., [2023](#bib.bib11)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调过程，我们使用了官方基线脚本⁴⁴4[https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py](https://github.com/mbzuai-nlp/SemEval2024-task8/blob/main/subtaskA/baseline/transformer_baseline.py)，并进行了修改，以导出机器分类预测概率。此外，由于在多语言轨道中不清楚哪个版本的XLM-RoBERTa模型被标记为基线（已知准确率为0.72），我们训练了基本版（XLM-R-B）和大版（XLM-R-L）。此外，我们还在基线中包括了mDeBERTa-v3-base（mDeBERTa）模型，因为它在我们之前的工作中表现最佳（Macko
    et al., [2023](#bib.bib11)）。
- en: To perform per-language models fine-tuning, we have used the source field of
    the train and dev data to select data only for the specific language. Other parameters
    of the fine-tuning process remained unchanged. The FastText language identification
    is used for a prediction, which uses the machine-class probability of the corresponding
    language-specific model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行每种语言模型的微调，我们使用了训练和开发数据的源字段，仅选择特定语言的数据。微调过程的其他参数保持不变。使用 FastText 语言识别进行预测，利用对应语言特定模型的机器类概率。
- en: The used QLoRA PEFT fine-tuning process used the binary cross entropy with logits
    for loss calculations and 4-bit quantization using BitsAndBytes⁵⁵5[https://pypi.org/project/bitsandbytes](https://pypi.org/project/bitsandbytes).
    The LoRA configuration⁶⁶6[https://pypi.org/project/peft](https://pypi.org/project/peft)
    used an alpha of 16, a dropout of 0.1, r of 64, and the task type of sequence
    classification. Unlike the baseline fine-tuning, this version used half-precision
    training, gradient accumulation of 4 steps, and evaluation each 1,000 steps. Other
    parameters were the same.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的 QLoRA PEFT 微调过程采用了带有 logits 的二进制交叉熵进行损失计算，并使用了 BitsAndBytes⁵⁵5[https://pypi.org/project/bitsandbytes](https://pypi.org/project/bitsandbytes)
    进行 4 位量化。LoRA 配置⁶⁶6[https://pypi.org/project/peft](https://pypi.org/project/peft)
    使用了 alpha 为 16，dropout 为 0.1，r 为 64，任务类型为序列分类。与基准微调不同，该版本使用了半精度训练、4 步梯度累积和每 1,000
    步评估。其他参数相同。
- en: Due to time constraints, we have not done any hyper-parameter optimization;
    thus, further improvements of the system are very likely possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间限制，我们没有进行超参数优化，因此系统可能还有进一步改进的空间。
- en: 5 Results
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 'The experimental results are provided in Table [5](#S5 "5 Results ‣ KInIT at
    SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection").
    It must be noted that the results in the bottom part of the table are not part
    of the competition, since those experiments were performed after the submission
    deadline of the shared task. Also, the performance results using the test set
    were not known before the deadline; gold labels has been released only afterwards.
    Therefore, the design decisions could be made purely using the dev set.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果见表 [5](#S5 "5 结果 ‣ KInIT 在 SemEval-2024 任务 8 中: 微调的 LLM 用于多语言机器生成文本检测")。必须指出，表格底部的结果不属于比赛部分，因为这些实验是在共享任务提交截止后进行的。此外，在截止日期之前未得知使用测试集的性能结果；黄金标签只在之后发布。因此，设计决策只能完全基于开发集。'
- en: '|  |  | Accuracy | AUC ROC |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 准确率 | AUC ROC |'
- en: '| --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | System | Dev | Test | Dev | Test |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 系统 | 开发集 | 测试集 | 开发集 | 测试集 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Baselines | XLM-R-B | 0.7158 | 0.7935 | 0.8262 | 0.9040 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | XLM-R-B | 0.7158 | 0.7935 | 0.8262 | 0.9040 |'
- en: '| XLM-R-L | 0.7275 | 0.8841 | 0.8187 | 0.9063 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| XLM-R-L | 0.7275 | 0.8841 | 0.8187 | 0.9063 |'
- en: '| mDeBERTa | 0.6968 | 0.8943 | 0.7952 | 0.9832 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| mDeBERTa | 0.6968 | 0.8943 | 0.7952 | 0.9832 |'
- en: '| System Alternatives | $\ast$ | 0.9035 | 0.9501 | N/A | N/A |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 系统选项 | $\ast$ | 0.9035 | 0.9501 | N/A | N/A |'
- en: '| PLMoE$\bullet$ | 0.9878 | 0.5819 | 0.9943 | 0.6268 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| PLMoE$\bullet$ | 0.9878 | 0.5819 | 0.9943 | 0.6268 |'
- en: '| rLLM2S3$\bullet$ | 0.9965 | 0.9560 | N/A | N/A |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| rLLM2S3$\bullet$ | 0.9965 | 0.9560 | N/A | N/A |'
- en: '| Ablation Study | Falcon | 0.8043 | 0.9102 | 0.8775 | 0.9492 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 消融研究 | Falcon | 0.8043 | 0.9102 | 0.8775 | 0.9492 |'
- en: '| Mistral | 0.8560 | 0.9027 | 0.9138 | 0.9579 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 0.8560 | 0.9027 | 0.9138 | 0.9579 |'
- en: '| rFalcon | 0.9905 | 0.8843 | 0.9991 | 0.9395 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| rFalcon | 0.9905 | 0.8843 | 0.9991 | 0.9395 |'
- en: '| rMistral | 0.9980 | 0.9268 | 0.9997 | 0.9713 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| rMistral | 0.9980 | 0.9268 | 0.9997 | 0.9713 |'
- en: '| S3$\bullet$ | 0.7248 | 0.8328 | N/A | N/A |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| S3$\bullet$ | 0.7248 | 0.8328 | N/A | N/A |'
- en: '| S5$\bullet$ | 0.5880 | 0.4763 | N/A | N/A |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| S5$\bullet$ | 0.5880 | 0.4763 | N/A | N/A |'
- en: '| Binoculars | 0.5430 | 0.7979 | 0.6304 | 0.8777 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 双筒望远镜 | 0.5430 | 0.7979 | 0.6304 | 0.8777 |'
- en: '| Binoculars$\bullet$ | 0.6240 | 0.8434 | 0.6304 | 0.8777 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 双筒望远镜$\bullet$ | 0.6240 | 0.8434 | 0.6304 | 0.8777 |'
- en: '| Post-Deadline Study | PLMoE-ES$\bullet$ | 0.9885 | 0.8417 | 0.9947 | 0.9635
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 截止后研究 | PLMoE-ES$\bullet$ | 0.9885 | 0.8417 | 0.9947 | 0.9635 |'
- en: '| Llama-2 | 0.7335 | 0.7587 | 0.9342 | 0.7400 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 0.7335 | 0.7587 | 0.9342 | 0.7400 |'
- en: '| rLlama-2 | 0.8903 | 0.8907 | 0.8416 | 0.9400 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| rLlama-2 | 0.8903 | 0.8907 | 0.8416 | 0.9400 |'
- en: '| rLlama-2-ES | 0.9838 | 0.8805 | 0.9960 | 0.9108 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| rLlama-2-ES | 0.9838 | 0.8805 | 0.9960 | 0.9108 |'
- en: '| rFalcon-ES | 0.9410 | 0.8672 | 0.9872 | 0.9503 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| rFalcon-ES | 0.9410 | 0.8672 | 0.9872 | 0.9503 |'
- en: '| rMistral-ES | 0.9863 | 0.9412 | 0.9984 | 0.9834 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| rMistral-ES | 0.9863 | 0.9412 | 0.9984 | 0.9834 |'
- en: '| rLLM2B-ES$\bullet$ | 0.9915 | 0.9700 | N/A | N/A |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| rLLM2B-ES$\bullet$ | 0.9915 | 0.9700 | N/A | N/A |'
- en: '|  | LLM2B1 | 0.8668 | 0.9708 | N/A | N/A |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM2B1 | 0.8668 | 0.9708 | N/A | N/A |'
- en: '|  | rMistral1 | 0.9975 | 0.9675 | 0.9997 | 0.9713 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | rMistral1 | 0.9975 | 0.9675 | 0.9997 | 0.9713 |'
- en: 'Table 2: Detection performance evaluated using the dev (pre-deadline) and test
    (post-deadline) splits separately. The main system described in this paper is
    denoted by $\ast$”. “-ES” denotes using of early-stopping mechanism to prevent
    over-fitting. “N/A” denotes not available values due to prediction-based majority
    voting (i.e., no probabilities to calculate AUC ROC). The gray color denotes unrepresentative
    performance values due to training on the dev set.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：分别使用开发集（截止日期前）和测试集（截止日期后）评估的检测性能。本文描述的主要系统用“$\ast$”表示。“-ES”表示使用早期停止机制以防止过拟合。“N/A”表示由于基于预测的多数投票（即没有概率计算AUC
    ROC）而不可用的值。灰色表示由于在开发集上训练而产生的不具代表性的性能值。
- en: Due to high accuracy and high AUC ROC metrics using the dev set, we considered
    rFalcon and rMistral over-fitted; therefore, we decided not to submit our rLLM2S3
    system. This turned-out to be a mistake, since it performed slightly better than
    the submitted LLM2S3 on the test set. On the other hand, our suspicion of over-fitting
    PLMoE (due to the similar observations) turned-out to be valid, since it performed
    much worse using the test set. Therefore, it seems that per-language monolingually
    fine-tuned (i.e., lower amount of samples) models require optimization of hyper-parameters
    to prevent over-fitting and to better generalize to unseen texts.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在开发集上使用的准确性和AUC ROC指标较高，我们认为rFalcon和rMistral过拟合；因此，我们决定不提交我们的rLLM2S3系统。这被证明是一个错误，因为它在测试集上的表现稍微好于提交的LLM2S3。另一方面，我们对PLMoE过拟合的怀疑（由于类似的观察）被证明是有效的，因为它在测试集上的表现差得多。因此，看起来每种语言单语调优（即样本量较少）模型需要优化超参数以防止过拟合，并更好地泛化到未见的文本。
- en: As an ablation study, we also provide the results for individual components
    of our system alternatives. As the results show, the ensembling into more complex
    systems of LLM2S3 and rLLM2S3 helped generalization of the classification performance.
    Individual methods would not outperform the submitted system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作为消融研究，我们还提供了系统替代方案各个组件的结果。结果显示，将LLM2S3和rLLM2S3组合成更复杂的系统有助于分类性能的泛化。单独的方法不会超过提交的系统。
- en: 5.1 Post-Deadline Study
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 截止日期后的研究
- en: In the post-deadline experiments (already knowing the gold labels of the test
    set for evaluation), we have finished Llama-2-7B model fine-tuning and retraining
    all three robust-version LLMs using the early stopping (to minimize the over-fitting).
    The results revealed that the rLlama-2 model does not suffer by over-fitting as
    much. Based on the test set evaluation and by examining various combinations,
    the retrained rLlama-2-ES and rMistral-ES seemed like good candidates to combine
    with Binoculars (rLLM2B-ES), outperforming the winning system in the competition.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在截止日期后的实验中（已经知道测试集的金标准标签用于评估），我们完成了Llama-2-7B模型的微调，并使用早期停止（以最小化过拟合）对所有三个鲁棒版本的LLM进行了重新训练。结果显示，rLlama-2模型的过拟合情况较轻。基于测试集评估并检查各种组合，重新训练的rLlama-2-ES和rMistral-ES似乎是与Binoculars（rLLM2B-ES）结合的良好候选者，超过了竞赛中的获胜系统。
- en: Early stopping helped a lot in boosting performance generalization (i.e., reducing
    over-fitting) of our per-language mixture-of-experts ensemble system (PLMoE-ES),
    achieving one of the highest AUC ROC using the test set. Nevertheless, in the
    accuracy as an official metric, it would not outperform the other system alternatives.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 早期停止在提升我们每种语言的专家混合系统（PLMoE-ES）的性能泛化（即减少过拟合）方面帮助很大，实现了使用测试集的最高AUC ROC之一。然而，作为官方指标的准确性，它并未超过其他系统替代方案。
- en: In addition, we have noticed that optimal thresholds for fine-tuned LLMs are
    often set to 1.0 by using purely the dev set samples machine-class probabilities.
    Therefore, we have fixed the thresholds to 1.0 for the LLM2B1 system (containing
    only models we have trained before the deadline), meaning that the machine-class
    predictions of the LLMs are used only when having 100% confidence (otherwise considered
    human-written). Such predictions, when combined with Binoculars, achieved even
    higher performance using the test set data (0.9708). Thus, we had such a system
    trained before the deadline; however, we have not noticed such a threshold bringing
    the best performance in time. Moreover, when looking at the accuracy for the dev
    set, we do not see why we would select such a system for the submission. It can
    be just a coincidence that it performs so well using the test set data. Further
    experiments are required to examine this phenomenon using independent out-of-distribution
    data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们注意到，针对微调的 LLMs，最佳阈值通常设为 1.0，仅使用开发集样本的机器分类概率。因此，我们将 LLM2B1 系统的阈值固定为 1.0（仅包含截止日期前训练的模型），这意味着
    LLMs 的机器分类预测仅在有 100% 信心时使用（否则被认为是人工撰写）。这种预测与 Binoculars 结合后，在使用测试集数据时达到了更高的性能（0.9708）。因此，我们在截止日期前训练了这样的系统；然而，我们没有注意到这种阈值在时间上带来了最佳性能。此外，当查看开发集的准确率时，我们看不到为什么会选择这样的系统进行提交。它在测试集数据上表现良好可能只是巧合。需要进一步实验使用独立的分布外数据来检查这一现象。
- en: Also, even when our rLLM2B-ES system alternative or the rMistral1 single-model
    system would won the competition, we are now not sure that we would be confident
    enough (about not being over-fitted) to submit it as the final system without
    evaluation on the external dataset. Thus, we have submitted best what we could
    at the time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使我们的 rLLM2B-ES 系统替代方案或 rMistral1 单模型系统赢得了比赛，我们现在也不确定是否有足够的信心（关于是否过拟合）将其提交为最终系统而不经过外部数据集的评估。因此，我们提交了当时能够提供的最佳结果。
- en: 5.2 Per-language Analysis
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 每语言分析
- en: 'For a deeper insight of the proposed system (LLM2S3) performance, we have performed
    an analysis per each language identified in the test set. The results are provided
    in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Per-language Analysis ‣ 5.1 Post-Deadline
    Study ‣ 5 Results ‣ KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual
    Machine-Generated Text Detection"). Interesting is that it achieved the highest
    accuracy for the Italian surprise language (it). Lower accuracy is evident for
    German and Arabic languages, although present in the dev set. It must be noted
    that this version of the system was not trained using the dev set, only the classification
    threshold calibration used such data. Therefore, the robust versions of system
    alternatives are expected to provide higher performance especially in those languages.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '为了深入了解提出的系统（LLM2S3）性能，我们对测试集中识别的每种语言进行了分析。结果见图[2](#S5.F2 "图 2 ‣ 5.2 每语言分析 ‣
    5.1 截止日期后的研究 ‣ 5 结果 ‣ KInIT 在 SemEval-2024 任务 8: 针对多语言机器生成文本检测的微调 LLMs")。有趣的是，它在意大利语意外语言（it）中达到了最高准确率。虽然在开发集中也存在德语和阿拉伯语，但其准确率较低。需要注意的是，这个版本的系统没有使用开发集进行训练，仅使用了分类阈值的校准数据。因此，系统的强健版本预计会在这些语言中提供更高的性能。'
- en: '![Refer to caption](img/9fe444ed2e459cb39092dc0204f0f7c9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9fe444ed2e459cb39092dc0204f0f7c9.png)'
- en: 'Figure 2: Per-language test-set performance (it is a surprise language, de
    and ar are in the dev set only). Axis scale for Accuracy is shown from 0.9 to
    1.0\. The per-class samples counts are provided in the top-right table.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每语言测试集性能（意大利语是意外语言，德语和阿拉伯语仅在开发集中）。准确率的轴尺度显示从 0.9 到 1.0。每类样本计数在右上角的表格中提供。
- en: 6 Conclusion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: To cope with the problem of multilingual, multidomain, and multigenerator machine-generated
    text detection, we have proposed an ensemble system using 2 LLMs (Falcon-7B and
    Mistral-7B) fine-tuned for the binary sequence classification task. We have further
    combined the predictions with the statistical metrics of Entropy, Rank, and Binoculars
    using a two-stage majority voting. The classification thresholds in our system
    have been calibrated in a per-language manner, for which we have utilized the
    FastText language identification. A combination of fine-tuned LLMs and statistical
    detection seems to be the right way to cope with generalization of the detection
    performance. Out of the evaluated single-model systems, Mistral-7B is the best
    candidate for fine-tuning, which by itself can bring a remarkable classification
    performance. Further improvements of the system could be easily achievable by
    hyper-parameters optimization, which we have not done in the submitted system
    due to lack of time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对多语言、多领域和多生成器机器生成文本检测的问题，我们提出了一种集成系统，使用了 2 个 LLM（Falcon-7B 和 Mistral-7B），经过微调用于二元序列分类任务。我们进一步结合了
    Entropy、Rank 和 Binoculars 的统计指标，使用了两阶段的多数投票。我们系统中的分类阈值已按语言进行校准，为此我们使用了 FastText
    语言识别。微调的 LLM 和统计检测的结合似乎是应对检测性能泛化的正确方式。在评估的单模型系统中，Mistral-7B 是最适合微调的候选模型，其本身就能带来显著的分类性能。系统的进一步改进可以通过超参数优化轻松实现，但由于时间限制，我们在提交的系统中没有进行这项工作。
- en: Acknowledgements
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was partially supported by the projects funded by the European Union
    under the Horizon Europe: AI-CODE, a project funded by the European Union under
    the Horizon Europe, GA No. [101135437](https://cordis.europa.eu/project/id/101135437),
    and VIGILANT, GA No. [101073921](https://doi.org/10.3030/101073921). Part of the
    research results was obtained using the computational resources procured in the
    national project National competence centre for high performance computing (project
    code: 311070AKF2) funded by European Regional Development Fund, EU Structural
    Funds Informatization of Society, Operational Program Integrated Infrastructure.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了欧盟资助的项目的支持，包括 Horizon Europe 计划下的 AI-CODE 项目（欧盟资助，GA 编号 [101135437](https://cordis.europa.eu/project/id/101135437)）和
    VIGILANT（GA 编号 [101073921](https://doi.org/10.3030/101073921)）。部分研究结果利用了在国家项目“国家高性能计算能力中心”（项目代码：311070AKF2）中采购的计算资源，该项目由欧盟区域发展基金、欧盟结构基金社会信息化、综合基础设施操作计划资助。
- en: References
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat 等。2023。《Gpt-4
    技术报告》。*arXiv 预印本 arXiv:2303.08774*。
- en: 'Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
    Guilherme Penedo. 2023. Falcon-40B: An open large language model with state-of-the-art
    performance.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almazrouei 等（2023）Ebtesam Almazrouei、Hamza Alobeidli、Abdulaziz Alshamsi、Alessandro
    Cappelli、Ruxandra Cojocaru、Merouane Debbah、Etienne Goffinet、Daniel Heslow、Julien
    Launay、Quentin Malartic、Badreddine Noune、Baptiste Pannier 和 Guilherme Penedo。2023。《Falcon-40B：一种具有最先进性能的开放大型语言模型》。
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. [QLoRA: Efficient finetuning of quantized llms](http://arxiv.org/abs/2305.14314).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。2023。[QLoRA：量化
    LLM 的高效微调](http://arxiv.org/abs/2305.14314)。
- en: 'Fröhling and Zubiaga (2021) Leon Fröhling and Arkaitz Zubiaga. 2021. Feature-based
    detection of automated language models: tackling GPT-2, GPT-3 and Grover. *PeerJ
    Computer Science*, 7:e443.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fröhling 和 Zubiaga（2021）Leon Fröhling 和 Arkaitz Zubiaga。2021。《基于特征的自动化语言模型检测：解决
    GPT-2、GPT-3 和 Grover 的问题》。*PeerJ 计算机科学*，7:e443。
- en: 'Gagiano and Tian (2023) Rinaldo Gagiano and Lin Tian. 2023. A prompt in the
    right direction: Prompt based classification of machine-generated text detection.
    In *Proceedings of ALTA*.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gagiano 和 Tian（2023）Rinaldo Gagiano 和 Lin Tian。2023。《正确方向的提示：基于提示的机器生成文本检测分类》。发表于
    *ALTA 会议论文集*。
- en: 'Gehrmann et al. (2019) Sebastian Gehrmann, Hendrik Strobelt, and Alexander
    Rush. 2019. GLTR: Statistical detection and visualization of generated text. In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics:
    System Demonstrations*. Association for Computational Linguistics.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gehrmann et al. (2019) Sebastian Gehrmann, Hendrik Strobelt, 和 Alexander Rush.
    2019. GLTR: 生成文本的统计检测与可视化。收录于 *第57届计算语言学协会年会：系统演示*。计算语言学协会。'
- en: 'Hans et al. (2024) Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova,
    Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
    2024. [Spotting LLMs with binoculars: Zero-shot detection of machine-generated
    text](http://arxiv.org/abs/2401.12070).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hans et al. (2024) Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova,
    Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, 和 Tom Goldstein.
    2024. [用望远镜发现LLM：机器生成文本的零-shot检测](http://arxiv.org/abs/2401.12070)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. 2023. [Mistral 7b](http://arxiv.org/abs/2310.06825)。
- en: Kumarage et al. (2023) Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee,
    Kirill Trapeznikov, Scott Ruston, and Huan Liu. 2023. Stylometric detection of
    ai-generated text in twitter timelines. *arXiv preprint arXiv:2303.03697*.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumarage et al. (2023) Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee,
    Kirill Trapeznikov, Scott Ruston, 和 Huan Liu. 2023. 推特时间线中AI生成文本的风格检测。*arXiv 预印本
    arXiv:2303.03697*。
- en: Lavergne et al. (2008) Thomas Lavergne, Tanguy Urvoy, and François Yvon. 2008.
    Detecting fake content with relative entropy scoring. In *Proceedings of the 2008
    International Conference on Uncovering Plagiarism, Authorship and Social Software
    Misuse - Volume 377*, PAN’08, page 27–31, Aachen, DEU. CEUR-WS.org.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lavergne et al. (2008) Thomas Lavergne, Tanguy Urvoy, 和 François Yvon. 2008.
    使用相对熵评分检测虚假内容。收录于 *2008年揭示剽窃、作者身份和社交软件滥用国际会议论文集 - 第377卷*，PAN’08，第27–31页，亚琛，德国。CEUR-WS.org。
- en: 'Macko et al. (2023) Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas,
    Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko,
    and Maria Bielikova. 2023. [MULTITuDE: Large-scale multilingual machine-generated
    text detection benchmark](https://doi.org/10.18653/v1/2023.emnlp-main.616). In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 9960–9987, Singapore. Association for Computational Linguistics.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Macko et al. (2023) Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas,
    Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko,
    和 Maria Bielikova. 2023. [MULTITuDE: 大规模多语言机器生成文本检测基准](https://doi.org/10.18653/v1/2023.emnlp-main.616)。收录于
    *2023年自然语言处理实证方法会议论文集*，第9960–9987页，新加坡。计算语言学协会。'
- en: Macko et al. (2024) Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel
    Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and
    Maria Bielikova. 2024. [Authorship obfuscation in multilingual machine-generated
    text detection](http://arxiv.org/abs/2401.07867). *arXiv preprint arXiv:2401.07867*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Macko et al. (2024) Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason
    Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko,
    和 Maria Bielikova. 2024. [多语言机器生成文本检测中的作者模糊化](http://arxiv.org/abs/2401.07867)。*arXiv
    预印本 arXiv:2401.07867*。
- en: 'Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D.
    Manning, and Chelsea Finn. 2023. [DetectGPT: Zero-shot machine-generated text
    detection using probability curvature](https://arxiv.org/abs/2301.11305). *arXiv
    preprint arXiv:2301.11305*.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher
    D. Manning, 和 Chelsea Finn. 2023. [DetectGPT: 基于概率曲率的零-shot机器生成文本检测](https://arxiv.org/abs/2301.11305)。*arXiv
    预印本 arXiv:2301.11305*。'
- en: 'Uchendu et al. (2023) Adaku Uchendu, Thai Le, and Dongwon Lee. 2023. Attribution
    and obfuscation of neural text authorship: A data mining perspective. *ACM SIGKDD
    Explorations Newsletter*, 25(1):1–18.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uchendu et al. (2023) Adaku Uchendu, Thai Le, 和 Dongwon Lee. 2023. 神经文本作者归属与模糊化：数据挖掘视角。*ACM
    SIGKDD 探索通讯*, 25(1):1–18。
- en: Uchendu et al. (2020) Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020.
    [Authorship attribution for neural text generation](https://doi.org/10.18653/v1/2020.emnlp-main.673).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 8384–8395, Online. Association for Computational Linguistics.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uchendu 等（2020）Adaku Uchendu、Thai Le、Kai Shu 和 Dongwon Lee。2020年。 [神经文本生成的作者归属](https://doi.org/10.18653/v1/2020.emnlp-main.673)。见于
    *2020年自然语言处理实证方法会议论文集（EMNLP）*，第8384–8395页，在线。计算语言学协会。
- en: Vykopal et al. (2023) Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro,
    Dominik Macko, and Maria Bielikova. 2023. [Disinformation capabilities of large
    language models](http://arxiv.org/abs/2311.08838). *arXiv preprint arXiv:2311.08838*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vykopal 等（2023）Ivan Vykopal、Matúš Pikuliak、Ivan Srba、Robert Moro、Dominik Macko
    和 Maria Bielikova。2023年。 [大型语言模型的虚假信息能力](http://arxiv.org/abs/2311.08838)。 *arXiv
    预印本 arXiv:2311.08838*。
- en: Wahle et al. (2022) Jan Philip Wahle, Terry Ruas, Frederic Kirstein, and Bela
    Gipp. 2022. [How large language models are transforming machine-paraphrase plagiarism](https://doi.org/10.18653/v1/2022.emnlp-main.62).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*. Association for Computational Linguistics.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wahle 等（2022）Jan Philip Wahle、Terry Ruas、Frederic Kirstein 和 Bela Gipp。2022年。
    [大型语言模型如何改变机器释义剽窃](https://doi.org/10.18653/v1/2022.emnlp-main.62)。见于 *2022年自然语言处理实证方法会议论文集*。计算语言学协会。
- en: 'Wang et al. (2024) Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem
    Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud,
    Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych,
    and Preslav Nakov. 2024. SemEval-2024 Task 8: Multigenerator, multidomain, and
    multilingual black-box machine-generated text detection. In *Proceedings of the
    18th International Workshop on Semantic Evaluation*, SemEval 2024, Mexico, Mexico.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2024）Yuxia Wang、Jonibek Mansurov、Petar Ivanov、Jinyan Su、Artem Shelmanov、Akim
    Tsvigun、Chenxi Whitehouse、Osama Mohammed Afzal、Tarek Mahmoud、Giovanni Puccetti、Thomas
    Arnold、Alham Fikri Aji、Nizar Habash、Iryna Gurevych 和 Preslav Nakov。2024年。SemEval-2024
    第8任务：多生成器、多领域和多语言的黑箱机器生成文本检测。见于 *第18届国际语义评估研讨会论文集*，SemEval 2024，墨西哥，墨西哥。
- en: Appendix A Computational Resources
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 计算资源
- en: For experiments regarding model fine-tuning and inference processes, we have
    used $1\times$ A100 40GB GPU, cumulatively consuming around 10,000 GPU-core hours.
    For combining the results and analysis, we have used Jupyter Lab running on 4
    CPU cores, without the GPU acceleration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型微调和推理过程的实验，我们使用了 $1\times$ A100 40GB GPU，总计消耗了大约 10,000 GPU 核心小时。对于结果的合并和分析，我们使用了运行在
    4 个 CPU 核心上的 Jupyter Lab，没有使用 GPU 加速。
