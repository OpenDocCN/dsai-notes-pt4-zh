- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:35:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:35:12'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational
    Social Science Tasks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示优化还是微调？在计算社会科学任务中使用大语言模型的最佳实践
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01346](https://ar5iv.labs.arxiv.org/html/2408.01346)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01346](https://ar5iv.labs.arxiv.org/html/2408.01346)
- en: Anders Giovanni Møller¹, Luca Maria Aiello^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 安德斯·乔瓦尼·穆勒¹，卢卡·玛利亚·艾耶洛^(1,2)
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models are expressive tools that enable complex tasks of text
    understanding within Computational Social Science. Their versatility, while beneficial,
    poses a barrier for establishing standardized best practices within the field.
    To bring clarity on the values of different strategies, we present an overview
    of the performance of modern LLM-based classification methods on a benchmark of
    23 social knowledge tasks. Our results point to three best practices: select models
    with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor
    of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex
    forms instruction-tuning on multiple datasets only when only training data is
    more abundant.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型是表达能力强的工具，能够在计算社会科学中进行复杂的文本理解任务。虽然其多样性有益，但也成为建立领域标准化最佳实践的障碍。为了澄清不同策略的价值，我们提供了现代大语言模型分类方法在23个社会知识任务基准上的表现概述。我们的结果指出三项最佳实践：选择词汇量和预训练语料库较大的模型；避免简单的零样本，倾向于AI增强的提示；在特定任务数据上进行微调，且只有在训练数据更丰富时才考虑在多个数据集上的更复杂形式的指令微调。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: The release of ChatGPT in November 2022 has sparked broad interest for Large
    Language Models (LLMs) due to their capability to solve complex tasks of text
    understanding and generation (Bubeck and ohers [2023](#bib.bib4)). The Computational
    Social Science (CSS) community has rapidly recognized the potential of LLMs as
    tools for capturing textual dimensions of semantics and pragmatics – crucial elements
    of online discourse that have traditionally been challenging to quantify (Bail
    [2024](#bib.bib1)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月ChatGPT的发布激发了对大语言模型（LLMs）的广泛兴趣，因为它们能够解决复杂的文本理解和生成任务 (Bubeck and others
    [2023](#bib.bib4))。计算社会科学（CSS）社区迅速认识到大语言模型作为捕捉语义和语用文本维度的工具的潜力，这些是在线话语的关键元素，而传统上这些元素的量化一直具有挑战性 (Bail
    [2024](#bib.bib1))。
- en: This new opportunity, however, comes with the hurdle of choosing the appropriate
    use of LLMs in a rapidly-expanding landscape of models and solutions. Prior to
    the widespread adoption of LLMs, CSS practitioners typically relied on fine-tuning
    smaller encoder-based models for domain-specific classification tasks (Sun et al.
    [2019](#bib.bib20)). By contrast, LLMs can be used more flexibly, enabling a variety
    of alternative classification approaches (Chae and Davidson [2023](#bib.bib5)).
    Such versatility, while beneficial, poses a barrier for establishing standardized
    best practices within the field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一新机遇也带来了选择大语言模型适用方式的障碍，因为模型和解决方案的快速扩展。在大语言模型广泛应用之前，计算社会科学从业者通常依赖于微调较小的编码器模型进行特定领域的分类任务 (Sun
    et al. [2019](#bib.bib20))。相比之下，大语言模型可以更灵活地使用，支持多种替代分类方法 (Chae and Davidson [2023](#bib.bib5))。这种多样性虽然有益，但也成为建立领域标准化最佳实践的障碍。
- en: In their most straightforward usage, LLMs can function as zero-shot classifiers,
    requiring only some target text and a classification prompt (Kojima et al. [2022](#bib.bib11)).
    This approach is convenient because it applies the base model without the need
    of additional training to alter its weights. The prompt can be improved through
    various strategies, such as manual prompt engineering (White et al. [2023](#bib.bib22)),
    automated prompt generation based on the task descriptions (Shin et al. [2020](#bib.bib19)),
    or prompt augmentation with additional task-specific information (Brown et al.
    [2020](#bib.bib3)) or by integration of external knowledge bases (Li et al. [2022](#bib.bib14)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在最直接的应用中，大语言模型可以作为零样本分类器，仅需一些目标文本和分类提示 (Kojima et al. [2022](#bib.bib11))。这种方法便捷，因为它应用基础模型，无需额外训练来改变其权重。可以通过各种策略改进提示，例如手动提示工程 (White
    et al. [2023](#bib.bib22))、基于任务描述的自动提示生成 (Shin et al. [2020](#bib.bib19))，或通过额外的任务特定信息来增强提示 (Brown
    et al. [2020](#bib.bib3))，或者集成外部知识库 (Li et al. [2022](#bib.bib14))。
- en: Alongside approaches that require no training, fine-tuning on domain-specific
    data may offer better adaptability to specific tasks, albeit at the expense of
    increased computation (Wei et al. [2022](#bib.bib21)). Instruction-tuning is another
    flavor of fine-tuning, where the model is conditioned to adhere to explicit instructions
    and align with human judgments, although crafting high-quality instructions can
    be both costly and time consuming (Ouyang et al. [2022](#bib.bib17)). Furthermore,
    the continuous introduction of new language models raises questions around the
    effectiveness of different prompting and training techniques across various models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无需训练的方法外，对特定领域数据的微调可能提供更好的任务适应性，尽管这会增加计算开销（Wei 等人 [2022](#bib.bib21)）。指令微调是另一种微调方法，其中模型被调整以遵循明确的指令并与人类判断对齐，尽管制定高质量的指令可能既昂贵又耗时（Ouyang
    等人 [2022](#bib.bib17)）。此外，不断推出的新语言模型引发了关于不同提示和训练技术在各种模型中有效性的疑问。
- en: 'To bring some clarity on the value of these different practices in the typical
    workflow of text classification for the Computational Social Sciences, we provide
    an overview of how current LLM-based methods perform on a variety of CSS text
    classification tasks. Our goal is to provide practitioners with actionable guidelines
    on how to prioritize the use of different classification techniques. Specifically,
    we seek to answer three questions to investigate the effectiveness of the three
    main families of LLM-based classification:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清这些不同实践在计算社会科学的文本分类典型工作流程中的价值，我们概述了当前 LLM 基于的方法在各种 CSS 文本分类任务中的表现。我们的目标是为从业者提供关于如何优先使用不同分类技术的可操作指南。具体来说，我们试图回答三个问题，以调查三大类
    LLM 基于分类的有效性：
- en: 'RQ1: What is the value of *prompt-improvement strategies* that add task-relevant
    knowledge?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ1: 添加与任务相关的知识的*提示改进策略*的价值是什么？'
- en: 'RQ2: How does *fine-tuning* on static instructions compare with LLM-generated
    instructions?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ2: 在静态指令上进行的*微调*与 LLM 生成的指令相比如何？'
- en: 'RQ3: To what extent an increase in the *volume of pre-training data* (e.g.,
    Llama-2 vs. Llama-3) enhances downstream performance?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ3: *预训练数据量*（例如，Llama-2 与 Llama-3）的增加在多大程度上提升了下游性能？'
- en: We apply 6 state-of-the-art methods on two LLMs and test them against a standard
    benchmark of $23$ text classification tasks typical of the CSS domain (Choi et al.
    [2023](#bib.bib6)). While not fully exhaustive of all possible nuances of classification
    methods and tasks, our experiments cover the main state-of-the-art classification
    techniques, with the main goal of providing pragmatic guidelines to practitioners
    in the field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个 LLM 上应用了 6 种最先进的方法，并在一个 $23$ 个文本分类任务的标准基准上进行测试，这些任务在 CSS 领域中很常见（Choi 等人
    [2023](#bib.bib6)）。虽然未能完全涵盖所有可能的分类方法和任务的细微差别，我们的实验涵盖了主要的最先进分类技术，主要目的是为该领域的从业者提供务实的指导。
- en: Materials and Methods
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 材料和方法
- en: 'We run all our experiments on two open-source models of the Llama series: Llama-2-7B-chat
    and Meta-Llama-3-8B-Instruct, both released under commercial user license (https://ai.meta.com/llama/license/).
    We initialize both models with a temperature value of $0.9$, in line with the
    setup of previous work. Llama-3 is trained on a corpus of 15T tokens, about seven
    times larger than Llama-2, and it features a vocabulary size that is four times
    larger (128K tokens).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个 Llama 系列的开源模型上运行了所有实验：Llama-2-7B-chat 和 Meta-Llama-3-8B-Instruct，这两个模型都在商业用户许可证下发布（https://ai.meta.com/llama/license/）。我们以
    $0.9$ 的温度值初始化这两个模型，与以前工作的设置一致。Llama-3 在 15T 令牌的语料库上进行训练，约为 Llama-2 的七倍，并且其词汇表大小是四倍（128K
    令牌）。
- en: The SOCKET benchmark
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SOCKET 基准
- en: 'The Social Knowledge Evaluation Tests (SOCKET) is a collection of 58 datasets
    in the domain of social knowledge that can be used to benchmark algorithms for
    natural language understanding (Choi et al. [2023](#bib.bib6)). It is the first
    collective benchmark that has been used to test the capabilities of LLMs in various
    *social* contexts. The datasets are grouped into five types of task: *humor &
    sarcasm*, *offensiveness*, *sentiment & emotion*, *trustworthiness*, and *social
    factors*. In addition to the labeled texts, SOCKET provides one prompt for each
    of the tasks.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 社会知识评估测试（SOCKET）是一个包含58个社会知识领域数据集的集合，可用于基准测试自然语言理解算法（Choi et al. [2023](#bib.bib6)）。这是第一个用于测试LLM在各种*社会*背景下能力的集体基准。数据集分为五种任务类型：*幽默与讽刺*、*冒犯性*、*情感与情绪*、*可信度*和*社会因素*。除了标记文本外，SOCKET还为每个任务提供一个提示。
- en: In our experiments, we only consider the 44 datasets that refer to classification
    tasks, saving regression, pair-wise comparisons, and span identification tasks
    for future work. For fine-tuning, we use the data corresponding to the 44 classification
    task. For evaluation, we use a representative subset of 23 datasets. We use the
    same train-test split as defined in Choi et al. ([2023](#bib.bib6)). To manage
    computational resources effectively, we constrained our test sample size to up
    to $2,000$ random samples per task.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们只考虑了44个分类任务的数据集，将回归、成对比较和跨度识别任务留待未来工作。对于微调，我们使用对应于44个分类任务的数据。对于评估，我们使用了23个数据集的代表性子集。我们使用与Choi
    et al. ([2023](#bib.bib6))中定义的相同训练-测试划分。为了有效管理计算资源，我们将测试样本的数量限制为每个任务最多$2,000$个随机样本。
- en: Zero-shot prompts
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零样本提示
- en: We evaluate the performance of the models using the zero-shot prompts provided
    in SOCKET (cf. Prompt 1 in Appendix). The prompts are manually designed and do
    not include any examples, directing the model to solve tasks without any specific
    guidance. In this setting, we rely entirely on the LLM’s internal representation
    and understanding of the individual tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SOCKET中提供的零样本提示来评估模型的性能（参见附录中的提示1）。这些提示是手动设计的，不包括任何示例，指引模型在没有任何具体指导的情况下解决任务。在这种设置下，我们完全依赖LLM对各个任务的内部表示和理解。
- en: AI-knowledge prompts
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI知识提示
- en: We produce AI-based enhancement to the zero-shot prompts using *generated knowledge
    prompting*, a technique that relies on a language model to generate task-specific
    knowledge that can then be used as additional information to be included into
    the prompt (Liu et al. [2022](#bib.bib15)). We use GPT-4 to generate task-specific
    label descriptions based on the zero-shot prompts and the available label options
    (cf. Prompt 2). This process adds task-aware elements to the prompts, providing
    descriptions for each individual label-option (cf. Prompt 3).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过*生成知识提示*，即依赖于语言模型生成任务特定知识的技术，来增强零样本提示，这些生成的知识可以作为额外信息添加到提示中（Liu et al. [2022](#bib.bib15)）。我们使用GPT-4基于零样本提示和可用标签选项生成任务特定的标签描述（参见提示2）。这一过程为提示添加了任务感知元素，为每个单独的标签选项提供描述（参见提示3）。
- en: Retrieval-Augmented Generation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: Retrieval-Augmented Generation (RAG) integrates an information retrieval module
    within the generative framework of a Large Language Model (Lewis et al. [2021](#bib.bib13)).
    The RAG system uses the prompt as a query to search a domain-specific knowledge
    base, retrieving information that is relevant to both the prompt and the domain.
    The retrieved data is combined with the initial prompt and submitted to the LLM
    for generation. This methodology is designed to adapt the LLM’s output to the
    target domain without the need of additional training on specialized data (Hu
    and Lu [2024](#bib.bib10)). Recent empirical studies indicate that RAG presents
    a competitive alternative to traditional fine-tuning, particularly due to the
    minimal computational resources required for generating and querying the search
    index (Balaguer et al. [2024](#bib.bib2)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）将信息检索模块集成到大型语言模型的生成框架中（Lewis et al. [2021](#bib.bib13)）。RAG系统使用提示作为查询，搜索特定领域的知识库，检索与提示和领域相关的信息。检索到的数据与初始提示结合后提交给LLM进行生成。这种方法旨在将LLM的输出调整到目标领域，而无需在专门数据上进行额外训练（Hu
    and Lu [2024](#bib.bib10)）。最近的实证研究表明，RAG提供了与传统微调相竞争的替代方案，特别是因为生成和查询搜索索引所需的计算资源最小（Balaguer
    et al. [2024](#bib.bib2)）。
- en: For each task, we apply the all-MiniLM-l6-v2 model to create dense embeddings
    of all the training instances. These vector representations are constructed using
    text segments of 1000 characters, with a 150-character overlap. We efficiently
    index all the embeddings with the FAISS library (Douze et al. [2024](#bib.bib9)).
    During the evaluation phase on the test dataset, we calculate the embedding of
    the input text, and use it to query the index and retrieve the top five most similar
    texts, based on cosine similarity, along with their corresponding labels. We then
    formulate a final prompt for classification, integrating the test sample and the
    retrieved documents (cf. Prompt 4). In the system prompt, in addition to the specifics
    of our RAG configuration, we also include the AI-generated descriptions of the
    labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，我们应用all-MiniLM-l6-v2模型来创建所有训练实例的密集嵌入。这些向量表示是使用1000个字符的文本片段构建的，重叠150个字符。我们使用FAISS库（Douze
    et al. [2024](#bib.bib9)）高效地索引所有嵌入。在测试数据集的评估阶段，我们计算输入文本的嵌入，并用它来查询索引，基于余弦相似度检索出前五个最相似的文本及其对应的标签。然后，我们制定最终的分类提示，整合测试样本和检索到的文档（参见Prompt
    4）。在系统提示中，除了我们RAG配置的具体细节外，我们还包括了AI生成的标签描述。
- en: Fine-tuning
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调
- en: 'When Supervised Fine Tuning (SFT) an LLM for a specific classification task,
    the model is provided with a series of prompts containing: *i)* fixed classification
    instructions specific to the task, including all the classification labels allowed,
    and *ii)* a set of labeled texts (cf. Prompt 5). The loss calculated between the
    generated output and the examples’ true labels is used to update the model’s weights.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当对LLM进行有监督微调（SFT）以完成特定分类任务时，模型会接收一系列包含：*i)* 任务特定的固定分类指令，包括所有允许的分类标签，以及*ii)*
    一组标记文本（参见Prompt 5）的提示。计算生成的输出与示例真实标签之间的损失，用于更新模型的权重。
- en: We adopt a two-phase fine-tuning approach that aligns with the current best
    practices. During the first phase, we use Quantized Low-Rank Adaptation (QLoRA),
    an efficient fine-tuning technique (Dettmers et al. [2023](#bib.bib7)). In QLoRA,
    the main model is frozen and quantized to a 4-bit representation. The fine-tuning
    process is used to learn separate low-rank matrices of gradients, which are then
    combined with the frozen model during inference, weighted by a factor $\alpha$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了一种与当前最佳实践相符的两阶段微调方法。在第一阶段，我们使用量化低秩适应（QLoRA），这是一种高效的微调技术（Dettmers et al.
    [2023](#bib.bib7)）。在QLoRA中，主要模型被冻结并量化为4位表示。微调过程用于学习单独的低秩梯度矩阵，然后在推理过程中与冻结模型结合，按因素$\alpha$加权。
- en: In the second phase, we perform Direct Preference Optimization (DPO), a technique
    that updates the model’s weights based on the explicit user preference for one
    training example over another (Rafailov et al. [2023](#bib.bib18)). During DPO,
    the model receives a prompt and pairs of responses ranked by preference. Based
    on cross-entropy loss, the model updates its weights to maximize the probability
    of generating the preferred example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们执行直接偏好优化（DPO），这是一种基于明确用户对一个训练示例相对于另一个示例的偏好更新模型权重的技术（Rafailov et al.
    [2023](#bib.bib18)）。在DPO过程中，模型接收提示和按偏好排名的响应对。基于交叉熵损失，模型更新其权重，以最大化生成首选示例的概率。
- en: We trained both phases for one epoch, and we set $\alpha$ to 16, the dropout
    rate to 0.05, and the matrix rank to 8.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对两个阶段进行了一次训练，并将$\alpha$设置为16，丢弃率设置为0.05，矩阵秩设置为8。
- en: Instruction tuning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令调优
- en: Instruction tuning is a special type of fine-tuning that, instead of fine-tuning
    on labeled text examples from a single task, provides the model with a set of
    instructions and desired corresponding outputs from multiple tasks (Wei et al.
    [2022](#bib.bib21)). Unlike traditional fine tuning, instruction tuning improves
    the model’s ability to follow classification instructions correctly, and it produces
    a final model that can be flexibly employed to solve a series of classification
    tasks within the same domain. To implement instruction tuning, we use the same
    SFT+DPO pipeline that we employed for fine-tuning, but using instructions and
    examples from all the tasks during training. This approach is similar to that
    used by *SocialiteLlama*, the first example of instruction-tuned model on all
    the tasks from the SOCKET benchmark (Dey et al. [2024](#bib.bib8)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调是一种特殊的微调类型，它不是在单一任务的标注文本示例上进行微调，而是向模型提供一组指令和来自多个任务的期望对应输出 (Wei et al. [2022](#bib.bib21))。与传统微调不同，指令微调提高了模型正确执行分类指令的能力，并生成了一个可以灵活用于解决同一领域内一系列分类任务的最终模型。为了实现指令微调，我们使用了与微调相同的
    SFT+DPO 流程，但在训练过程中使用来自所有任务的指令和示例。这种方法类似于*SocialiteLlama*，这是第一个在 SOCKET 基准测试中对所有任务进行指令微调的模型示例 (Dey
    et al. [2024](#bib.bib8))。
- en: '| Tasks | Llama-2 7B chat | Llama-3 8B Instruct |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | Llama-2 7B 聊天 | Llama-3 8B 指令 |'
- en: '|  | Zero-shot | AI Knowledge | RAG | Fine-tuning |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | 零样本 | AI 知识 | RAG | 微调 |'
- en: '&#124; Instruction &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指令 &#124;'
- en: '&#124; tuning &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Reverse &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反向 &#124;'
- en: '&#124; Instructions &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指令 &#124;'
- en: '| Zero-shot | AI Knowledge | RAG | Fine-tuning |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | AI 知识 | RAG | 微调 |'
- en: '&#124; Instruction &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指令 &#124;'
- en: '&#124; tuning &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微调 &#124;'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Reverse &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反向 &#124;'
- en: '&#124; Instructions &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 指令 &#124;'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Humor & Sarcasm |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 幽默与讽刺 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| hahackathon#is_humor | 0.459 | 0.56 | 0.462 | 0.834 | 0.564 | 0.548 | 0.765
    | 0.864 | 0.636 | 0.442 | 0.904 | 0.933 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| hahackathon#is_humor | 0.459 | 0.56 | 0.462 | 0.834 | 0.564 | 0.548 | 0.765
    | 0.864 | 0.636 | 0.442 | 0.904 | 0.933 |'
- en: '| sarc | 0.400 | 0.492 | 0.451 | 0.303 | 0.475 | 0.216 | 0.511 | 0.591 | 0.534
    | 0.689 | 0.499 | 0.628 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| sarc | 0.400 | 0.492 | 0.451 | 0.303 | 0.475 | 0.216 | 0.511 | 0.591 | 0.534
    | 0.689 | 0.499 | 0.628 |'
- en: '| tweet_irony | 0.313 | 0.497 | 0.366 | 0.458 | 0.464 | 0.638 | 0.540 | 0.663
    | 0.551 | 0.510 | 0.889 | 0.788 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| tweet_irony | 0.313 | 0.497 | 0.366 | 0.458 | 0.464 | 0.638 | 0.540 | 0.663
    | 0.551 | 0.510 | 0.889 | 0.788 |'
- en: '| Offensiveness |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 冒犯性 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| contextual-abuse#PersonDirectedAbuse | 0.103 | 0.480 | 0.182 | 0.990 | 0.105
    | 0.052 | 0.671 | 0.655 | 0.460 | 0.975 | 0.992 | 0.978 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| contextual-abuse#PersonDirectedAbuse | 0.103 | 0.480 | 0.182 | 0.990 | 0.105
    | 0.052 | 0.671 | 0.655 | 0.460 | 0.975 | 0.992 | 0.978 |'
- en: '| implicit-hate#explicit_hate | 0.090 | 0.142 | 0.123 | 0.788 | 0.139 | 0.799
    | 0.665 | 0.517 | 0.447 | 0.950 | 0.951 | 0.947 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| implicit-hate#explicit_hate | 0.090 | 0.142 | 0.123 | 0.788 | 0.139 | 0.799
    | 0.665 | 0.517 | 0.447 | 0.950 | 0.951 | 0.947 |'
- en: '| contextual-abuse#IdentityDirectedAbuse | 0.076 | 0.515 | 0.255 | 0.883 |
    0.102 | 0.001 | 0.708 | 0.758 | 0.516 | 0.893 | 0.984 | 0.973 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| contextual-abuse#IdentityDirectedAbuse | 0.076 | 0.515 | 0.255 | 0.883 |
    0.102 | 0.001 | 0.708 | 0.758 | 0.516 | 0.893 | 0.984 | 0.973 |'
- en: '| hasbiasedimplication | 0.245 | 0.426 | 0.574 | 0.530 | 0.390 | 0.767 | 0.463
    | 0.499 | 0.432 | 0.487 | 0.577 | 0.833 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| hasbiasedimplication | 0.245 | 0.426 | 0.574 | 0.530 | 0.390 | 0.767 | 0.463
    | 0.499 | 0.432 | 0.487 | 0.577 | 0.833 |'
- en: '| hateoffensive | 0.503 | 0.326 | 0.625 | 0.765 | 0.548 | 0.776 | 0.488 | 0.424
    | 0.440 | 0.870 | 0.838 | 0.841 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| hateoffensive | 0.503 | 0.326 | 0.625 | 0.765 | 0.548 | 0.776 | 0.488 | 0.424
    | 0.440 | 0.870 | 0.838 | 0.841 |'
- en: '| intentyn | 0.090 | 0.157 | 0.463 | 0.158 | 0.251 | 0.595 | 0.566 | 0.289
    | 0.261 | 0.413 | 0.719 | 0.741 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| intentyn | 0.090 | 0.157 | 0.463 | 0.158 | 0.251 | 0.595 | 0.566 | 0.289
    | 0.261 | 0.413 | 0.719 | 0.741 |'
- en: '| tweet_offensive | 0.412 | 0.577 | 0.723 | 0.762 | 0.533 | 0.506 | 0.693 |
    0.702 | 0.698 | 0.837 | 0.822 | 0.688 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| tweet_offensive | 0.412 | 0.577 | 0.723 | 0.762 | 0.533 | 0.506 | 0.693 |
    0.702 | 0.698 | 0.837 | 0.822 | 0.688 |'
- en: '| implicit-hate#implicit_hate | 0.085 | 0.202 | 0.108 | 0.449 | 0.268 | 0.466
    | 0.589 | 0.494 | 0.45 | 0.783 | 0.762 | 0.737 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| implicit-hate#implicit_hate | 0.085 | 0.202 | 0.108 | 0.449 | 0.268 | 0.466
    | 0.589 | 0.494 | 0.45 | 0.783 | 0.762 | 0.737 |'
- en: '| implicit-hate#stereotypical_hate | 0.047 | 0.164 | 0.725 | 0.892 | 0.150
    | 0.769 | 0.329 | 0.499 | 0.378 | 0.887 | 0.953 | 0.929 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| implicit-hate#stereotypical_hate | 0.047 | 0.164 | 0.725 | 0.892 | 0.150
    | 0.769 | 0.329 | 0.499 | 0.378 | 0.887 | 0.953 | 0.929 |'
- en: '| Sentiment & Emotion |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 情感与情绪 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| empathy#distress_bin | 0.048 | 0.565 | 0.554 | 0.349 | 0.172 | 0.494 | 0.285
    | 0.597 | 0.667 | 0.382 | 0.602 | 0.500 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| empathy#distress_bin | 0.048 | 0.565 | 0.554 | 0.349 | 0.172 | 0.494 | 0.285
    | 0.597 | 0.667 | 0.382 | 0.602 | 0.500 |'
- en: '| dailydialog | 0.167 | 0.561 | 0.107 | 0.253 | 0.154 | 0.782 | 0.382 | 0.336
    | 0.109 | 0.839 | 0.837 | 0.655 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| dailydialog | 0.167 | 0.561 | 0.107 | 0.253 | 0.154 | 0.782 | 0.382 | 0.336
    | 0.109 | 0.839 | 0.837 | 0.655 |'
- en: '| tweet_emotion | 0.450 | 0.623 | 0.680 | 0.650 | 0.498 | 0.319 | 0.725 | 0.776
    | 0.771 | 0.802 | 0.721 | 0.750 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| tweet_emotion | 0.450 | 0.623 | 0.680 | 0.650 | 0.498 | 0.319 | 0.725 | 0.776
    | 0.771 | 0.802 | 0.721 | 0.750 |'
- en: '| crowdflower | 0.215 | 0.288 | 0.224 | 0.303 | 0.235 | 0.154 | 0.179 | 0.243
    | 0.282 | 0.342 | 0.286 | 0.353 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| crowdflower | 0.215 | 0.288 | 0.224 | 0.303 | 0.235 | 0.154 | 0.179 | 0.243
    | 0.282 | 0.342 | 0.286 | 0.353 |'
- en: '| Social Factors |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 社会因素 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| hayati_politeness | 0.281 | 0.438 | 0.688 | 0.500 | 0.375 | 0.25 | 0.844
    | 0.656 | 0.656 | 0.719 | 0.844 | 0.688 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| hayati_politeness | 0.281 | 0.438 | 0.688 | 0.500 | 0.375 | 0.25 | 0.844
    | 0.656 | 0.656 | 0.719 | 0.844 | 0.688 |'
- en: '| complaints | 0.438 | 0.649 | 0.780 | 0.901 | 0.562 | 0.559 | 0.806 | 0.878
    | 0.809 | 0.916 | 0.872 | 0.817 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 投诉 | 0.438 | 0.649 | 0.780 | 0.901 | 0.562 | 0.559 | 0.806 | 0.878 | 0.809
    | 0.916 | 0.872 | 0.817 |'
- en: '| stanfordpoliteness | 0.550 | 0.621 | 0.665 | 0.522 | 0.582 | 0.439 | 0.640
    | 0.644 | 0.621 | 0.678 | 0.549 | 0.550 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| stanfordpoliteness | 0.550 | 0.621 | 0.665 | 0.522 | 0.582 | 0.439 | 0.640
    | 0.644 | 0.621 | 0.678 | 0.549 | 0.550 |'
- en: '| questionintimacy | 0.155 | 0.222 | 0.204 | 0.209 | 0.227 | 0.182 | 0.2 |
    0.204 | 0.2 | 0.320 | 0.351 | 0.347 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| questionintimacy | 0.155 | 0.222 | 0.204 | 0.209 | 0.227 | 0.182 | 0.2 |
    0.204 | 0.2 | 0.320 | 0.351 | 0.347 |'
- en: '| Trustworthyness |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 可信度 |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| hypo-l | 0.269 | 0.402 | 0.557 | 0.437 | 0.349 | 0.672 | 0.665 | 0.693 |
    0.536 | 0.724 | 0.712 | 0.721 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| hypo-l | 0.269 | 0.402 | 0.557 | 0.437 | 0.349 | 0.672 | 0.665 | 0.693 |
    0.536 | 0.724 | 0.712 | 0.721 |'
- en: '| rumor#rumor_bool | 0.282 | 0.606 | 0.887 | 0.444 | 0.458 | 0.592 | 0.514
    | 0.542 | 0.549 | 0.620 | 0.647 | 0.669 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| rumor#rumor_bool | 0.282 | 0.606 | 0.887 | 0.444 | 0.458 | 0.592 | 0.514
    | 0.542 | 0.549 | 0.620 | 0.647 | 0.669 |'
- en: '| two-to-lie#receiver_truth | 0.490 | 0.430 | 0.899 | 0.945 | 0.549 | 0.449
    | 0.366 | 0.613 | 0.682 | 0.945 | 0.943 | 0.933 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| two-to-lie#receiver_truth | 0.490 | 0.430 | 0.899 | 0.945 | 0.549 | 0.449
    | 0.366 | 0.613 | 0.682 | 0.945 | 0.943 | 0.933 |'
- en: '| Cross-task average | 0.268 | 0.432 | 0.491 | 0.579 | 0.354 | 0.479 | 0.547
    | 0.571 | 0.508 | 0.697 | 0.750 | 0.739 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 跨任务平均 | 0.268 | 0.432 | 0.491 | 0.579 | 0.354 | 0.479 | 0.547 | 0.571 | 0.508
    | 0.697 | 0.750 | 0.739 |'
- en: 'Table 1: Accuracy on SOCKET classification tasks across models. Best results
    for each model are highlighted in bold.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同模型在 SOCKET 分类任务上的准确性。每个模型的最佳结果用**粗体**标出。
- en: Reverse instruction tuning
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逆向指令调整
- en: Instruction tuning typically relies on one fixed human-generated instruction
    for each task. This constrains the ability of LLMs to learn associations between
    the semantics of instructions and their corresponding responses. Generating synthetic
    instruction variants with LLMs mitigates this problem without needing extensive
    human labor (Møller et al. [2024](#bib.bib16)). This process is known as *reverse
    instruction generation* (Köksal et al. [2024](#bib.bib12)). It involves presenting
    the LLM with a textual output and prompting it to formulate a plausible instruction
    that could lead to that output (cf. Prompt 6). We extend this method to create
    instructions that are specific to classification tasks consisting of a target
    text, a set of possible labels, and the label for the given text (cf. Prompt 7).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调整通常依赖于每个任务的一个固定人生成指令。这限制了大型语言模型（LLMs）学习指令语义与其相应响应之间关联的能力。生成合成指令变体可以在不需要大量人工劳动的情况下缓解这个问题 (Møller
    et al. [2024](#bib.bib16))。这个过程被称为*逆向指令生成* (Köksal et al. [2024](#bib.bib12))。它涉及向
    LLM 提供文本输出，并提示其制定一个可能导致该输出的合理指令（参见 Prompt 6）。我们将此方法扩展到创建特定于分类任务的指令，包括目标文本、一组可能的标签以及给定文本的标签（参见
    Prompt 7）。
- en: For generating reverse instructions, we randomly sample up to $4,000$ samples.
    We then clean the output using simple heuristics designed to remove noisy generations,
    filtering out instructions that repeat the input text, explicitly reveal the label,
    or are improperly formatted. We create a new training set for instruction-tuning
    by simply replicating each training example for all its instruction variants,
    and then apply the SFT+DPO pipeline. During the evaluation phase, we randomly
    sample instructions from the training set and integrate them into the prompt template.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为生成逆向指令，我们随机抽取最多$4,000$个样本。然后，使用简单的启发式方法清理输出，去除噪声生成，过滤掉重复输入文本、明确揭示标签或格式不正确的指令。我们通过简单地复制每个训练示例的所有指令变体来创建新的训练集，然后应用
    SFT+DPO 流程。在评估阶段，我们随机从训练集中抽取指令并将其整合到提示模板中。
- en: Results
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Table [1](#Sx2.T1 "Table 1 ‣ Instruction tuning ‣ Materials and Methods ‣ Prompt
    Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social
    Science Tasks") presents the classification accuracy across methods and tasks.
    A critical factor impacting performance is the selection of the pre-trained model.
    On average, across tasks, there is an accuracy improvement ranging from $0.02$
    when employing Llama-3 over Llama-2 (RQ3). This result indicates that there is
    still room for improving the the language models’ understanding during pre-training,
    and suggests that switching to recent models is worth prioritizing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#Sx2.T1 "Table 1 ‣ Instruction tuning ‣ Materials and Methods ‣ Prompt
    Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social
    Science Tasks") 展示了不同方法和任务的分类准确率。影响性能的一个关键因素是预训练模型的选择。总体而言，使用 Llama-3 相比 Llama-2，准确率提升范围为
    $0.02$（RQ3）。这一结果表明，语言模型在预训练过程中仍有改进空间，并建议优先考虑切换到最新的模型。
- en: When comparing the performance of prompt enhancement methods, two main findings
    emerge. First, zero-shot yields relatively high accuracy, yet it is consistently
    outperformed by AI-generated knowledge prompting (RQ1). This trend is not as pronounced
    in the *offensiveness* category, where some tasks exhibit a notable decrease in
    accuracy with AI-enhanced prompts. This could be attributed to the safeguards
    built into the LLMs when addressing sensitive content, potentially restricting
    their ability to generate high-quality prompts. Second, the performance of Retrieval-Augmented
    Generation (RAG) for prompt enhancement is inconsistent. Its relative performance
    to zero-shot is generally better with Llama-2, albeit with considerable variability
    across tasks, and tends to be less effective with Llama-3 (RQ1). This suggests
    that models with less extensive pre-training may benefit from external knowledge
    integration, but this advantage diminishes with models that have a more robust
    pre-training foundation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较提示增强方法的性能时，有两个主要发现。首先，零-shot 方法的准确率相对较高，但始终被 AI 生成的知识提示所超越（RQ1）。这一趋势在*冒犯性*类别中并不那么明显，在某些任务中，使用
    AI 增强的提示会显著降低准确率。这可能归因于在处理敏感内容时，LLM 内置的保护措施可能限制了其生成高质量提示的能力。第二，检索增强生成（RAG）在提示增强中的表现不一致。与零-shot
    相比，使用 Llama-2 的相对表现通常更好，尽管在任务之间存在相当大的变异性，并且在使用 Llama-3 时效果较差（RQ1）。这表明，预训练不够充分的模型可能从外部知识整合中受益，但这种优势在具有更强预训练基础的模型中会减弱。
- en: Fine-tuning markedly improves the accuracy of AI-knowledge prompting by an average
    of $0.15$ with reverse instruction tuning, with many tasks experiencing accuracy
    drops even greater than $0.3$ on average. This disparity may be due to Llama-3’s
    superior capability to process complex and semantically varied input data, thanks
    to its expanded vocabulary and training corpus. The added complexity, however,
    introduces noise into Llama-2’s classification process, suggesting a need for
    more fine-tuning data to bridge the performance gap with Llama-3\. In summary,
    the results indicate that advanced fine-tuning methods involving small sets of
    instructions and data from multiple tasks hold some promise but also risk performance
    decline if the foundational model lacks the necessary expressiveness (RQ2). Moreover,
    while reverse instructions enhance training diversity, they can also lead to hallucinations
    and information leaks that require manual intervention, thus limiting their practicality.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 细化调优通过反向指令调优显著提高了 AI 知识提示的准确率，平均提升 $0.15$，许多任务的准确率下降甚至超过 $0.3$。这种差异可能由于 Llama-3
    在处理复杂且语义多样的输入数据方面具有更强的能力，得益于其扩展的词汇量和训练语料。然而，增加的复杂性会引入噪音到 Llama-2 的分类过程中，表明需要更多的细化调优数据来缩小与
    Llama-3 的性能差距。总之，结果表明，涉及小规模指令和来自多个任务的数据的高级细化调优方法具有一定的前景，但也存在性能下降的风险，如果基础模型缺乏必要的表达能力（RQ2）。此外，虽然反向指令可以增强训练多样性，但它们也可能导致幻觉和信息泄露，需要人工干预，从而限制了其实际应用。
- en: The robustness of our findings is supported by the limited performance variation
    across task categories, which can be largely attributed to the difficulty of individual
    tasks. For instance, the *crowdflower* task exhibits the lowest performance due
    to its 13 possible classes that represent concepts challenging to discern from
    textual information.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现结果的稳健性得到了任务类别间性能变化有限的支持，这主要归因于单个任务的难度。例如，*crowdflower* 任务表现最差，因为其有 13 个可能的类别，代表的概念很难从文本信息中区分。
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Our findings highlight three good practices that practitioners can adopt when
    using LLMs for classification tasks within the field of Computational Social Science.
    First, the selection of the model is a crucial decision that significantly impacts
    performance. Choosing models that have undergone extensive pre-training is recommended.
    Second, basic zero-shot methods should be avoided in favor of enhanced zero-shot
    techniques that incorporate LLM-generated descriptions of the task and labels
    into the prompt. This straightforward method offers substantial benefits relative
    to its minimal cost, unlike more complex retrieval-based methods for prompt augmentation,
    which do not appear as effective for classification purposes. Last, fine-tuning
    should be pursued whenever adequate computational resources are accessible, as
    it consistently yields positive results and can be executed cost-effectively using
    contemporary methods like QLoRa. Nevertheless, in scenarios where fine-tuning
    data is scarce, advanced instruction tuning that integrates instructions and datasets
    from diverse tasks should be approached with caution, as it may not generalize
    well and could potentially degrade performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现突出显示了三种实践者在使用大语言模型进行计算社会科学领域分类任务时可以采用的良好实践。首先，模型的选择是一个关键决定，它对性能有重大影响。建议选择经过广泛预训练的模型。其次，应避免使用基本的零样本方法，转而采用改进的零样本技术，将
    LLM 生成的任务和标签描述纳入提示中。这种直接的方法相较于其最低成本提供了显著的好处，而不像更复杂的基于检索的方法用于提示增强，这些方法在分类任务中效果似乎不佳。最后，应该在可用的计算资源充足时进行微调，因为它始终能带来积极的结果，并且可以利用现代方法如
    QLoRa 以成本效益高的方式执行。然而，在微调数据稀缺的情况下，应该谨慎采用先进的指令调整，它整合了来自不同任务的指令和数据集，因为它可能无法很好地泛化，并且可能降低性能。
- en: References
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bail (2024) Bail, C. A. 2024. Can Generative AI improve social science? *PNAS*,
    121(21).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bail（2024）Bail，C. A. 2024。《生成性 AI 能改善社会科学吗？》*PNAS*，121（21）。
- en: 'Balaguer et al. (2024) Balaguer; et al. 2024. RAG vs Fine-tuning: Pipelines,
    Tradeoffs, and a Case Study on Agriculture. *arXiv:2401.08406*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balaguer 等（2024）Balaguer 等。2024。《RAG 与微调：管道、权衡和农业案例研究》。*arXiv:2401.08406*。
- en: Brown et al. (2020) Brown; et al. 2020. Language models are few-shot learners.
    *NeurIPS*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Brown 等。2020。《语言模型是少样本学习者》。*NeurIPS*。
- en: 'Bubeck and ohers (2023) Bubeck, S.; and ohers. 2023. Sparks of Artificial General
    Intelligence: Early experiments with GPT-4. *ArXiv:2303.12712*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 和其他人（2023）Bubeck，S.；和其他人。2023。《人工通用智能的火花：对 GPT-4 的早期实验》。*ArXiv:2303.12712*。
- en: 'Chae and Davidson (2023) Chae, Y.; and Davidson, T. 2023. Large language models
    for text classification: From zero-shot learning to fine-tuning. *Open Science
    Foundation*.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chae 和 Davidson（2023）Chae，Y.；和 Davidson，T. 2023。《用于文本分类的大语言模型：从零样本学习到微调》。*Open
    Science Foundation*。
- en: Choi et al. (2023) Choi, M.; Pei, J.; Kumar, S.; Shu, C.; and Jurgens, D. 2023.
    Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language
    Models with SocKET Benchmark. *ArXiv:2305.14938*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2023）Choi，M.；Pei，J.；Kumar，S.；Shu，C.；和 Jurgens，D. 2023。《LLM 是否理解社会知识？使用
    SocKET 基准评估大语言模型的社交性》。*ArXiv:2305.14938*。
- en: 'Dettmers et al. (2023) Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
    L. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. *ArXiv:2305.14314*.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Dettmers，T.；Pagnoni，A.；Holtzman，A.；和 Zettlemoyer，L. 2023。《QLoRA：量化大语言模型的高效微调》。*ArXiv:2305.14314*。
- en: 'Dey et al. (2024) Dey, G.; Ganesan, A. V.; Lal, Y. K.; Shah, M.; Sinha, S.;
    Matero, M.; Giorgi, S.; Kulkarni, V.; and Schwartz, H. A. 2024. SOCIALITE-LLAMA:
    An Instruction-Tuned Model for Social Scientific Tasks. *ArXiv:2402.01980*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dey 等（2024）Dey，G.；Ganesan，A. V.；Lal，Y. K.；Shah，M.；Sinha，S.；Matero，M.；Giorgi，S.；Kulkarni，V.；和
    Schwartz，H. A. 2024。《SOCIALITE-LLAMA：用于社会科学任务的指令调整模型》。*ArXiv:2402.01980*。
- en: Douze et al. (2024) Douze, M.; Guzhva, A.; Deng, C.; Johnson, J.; Szilvasy,
    G.; Mazaré, P.-E.; Lomeli, M.; Hosseini, L.; and Jégou, H. 2024. The FAISS Library.
    *ArXiv:2401.08281*.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douze 等（2024）Douze，M.；Guzhva，A.；邓，C.；约翰逊，J.；Szilvasy，G.；Mazaré，P.-E.；Lomeli，M.；Hosseini，L.；和
    Jégou，H. 2024。《FAISS 库》。*ArXiv:2401.08281*。
- en: 'Hu and Lu (2024) Hu, Y.; and Lu, Y. 2024. RAG and RAU: A Survey on Retrieval-Augmented
    Language Model in Natural Language Processing. *ArXiv:2404.19543*.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡和陆（2024）胡，Y.；陆，Y. 2024。《RAG 和 RAU：自然语言处理中的检索增强语言模型综述》。*ArXiv:2404.19543*。
- en: Kojima et al. (2022) Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
    Y. 2022. Large language models are zero-shot reasoners. *NeurIPS*, 35.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等（2022）Kojima，T.；Gu，S. S.；Reid，M.；Matsuo，Y.；和 Iwasawa，Y. 2022。《大语言模型是零样本推理者》。*NeurIPS*，35。
- en: 'Köksal et al. (2024) Köksal, A.; Schick, T.; Korhonen, A.; and Schütze, H.
    2024. LongForm: Effective Instruction Tuning with Reverse Instructions. *ArXiv:2304.08460*.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köksal et al. (2024) Köksal, A.; Schick, T.; Korhonen, A.; 和 Schütze, H. 2024.
    LongForm：通过逆向指令进行有效的指令调整。*ArXiv:2304.08460*。
- en: Lewis et al. (2021) Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
    V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; Riedel, S.;
    and Kiela, D. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP
    Tasks. *ArXiv:2005.11401*.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2021) Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
    V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; Riedel, S.;
    和 Kiela, D. 2021. 知识密集型 NLP 任务的检索增强生成。*ArXiv:2005.11401*。
- en: Li et al. (2022) Li, H.; Su, Y.; Cai, D.; Wang, Y.; and Liu, L. 2022. A survey
    on retrieval-augmented text generation. *ArXiv preprint arXiv:2202.01110*.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Li, H.; Su, Y.; Cai, D.; Wang, Y.; 和 Liu, L. 2022. 关于检索增强文本生成的调查。*ArXiv
    预印本 arXiv:2202.01110*。
- en: Liu et al. (2022) Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras, R. L.;
    Choi, Y.; and Hajishirzi, H. 2022. Generated Knowledge Prompting for Commonsense
    Reasoning. *ArXiv:2110.08387*.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras, R.
    L.; Choi, Y.; 和 Hajishirzi, H. 2022. 用于常识推理的生成知识提示。*ArXiv:2110.08387*。
- en: 'Møller et al. (2024) Møller, A. G.; Pera, A.; Dalsgaard, J.; and Aiello, L.
    2024. The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification
    Tasks. In *EACL*.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Møller et al. (2024) Møller, A. G.; Pera, A.; Dalsgaard, J.; 和 Aiello, L. 2024.
    鹦鹉困境：分类任务中的人工标记数据与 LLM 增强数据。见 *EACL*。
- en: Ouyang et al. (2022) Ouyang, L.; et al. 2022. Training language models to follow
    instructions with human feedback. *NeurIPS*.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Ouyang, L.; 等. 2022. 通过人工反馈训练语言模型以遵循指令。*NeurIPS*。
- en: 'Rafailov et al. (2023) Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; and Finn, C. 2023. Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model. *ArXiv:2305.18290*.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2023) Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; 和 Finn, C. 2023. 直接偏好优化：你的语言模型秘密地是奖励模型。*ArXiv:2305.18290*。
- en: 'Shin et al. (2020) Shin, T.; Razeghi, Y.; Logan IV, R. L.; Wallace, E.; and
    Singh, S. 2020. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *ArXiv:2010.15980*.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin et al. (2020) Shin, T.; Razeghi, Y.; Logan IV, R. L.; Wallace, E.; 和 Singh,
    S. 2020. Autoprompt：通过自动生成的提示从语言模型中引出知识。*ArXiv:2010.15980*。
- en: Sun et al. (2019) Sun, C.; Qiu, X.; Xu, Y.; and Huang, X. 2019. How to fine-tune
    bert for text classification? In *CCL*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Sun, C.; Qiu, X.; Xu, Y.; 和 Huang, X. 2019. 如何对 bert 进行文本分类的微调？见
    *CCL*。
- en: Wei et al. (2022) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2022. Finetuned Language Models Are Zero-Shot
    Learners. *ArXiv:2109.01652*.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; 和 Le, Q. V. 2022. 微调的语言模型是零-shot 学习者。*ArXiv:2109.01652*。
- en: White et al. (2023) White, J.; et al. 2023. A prompt pattern catalog to enhance
    prompt engineering with ChatGPT. *ArXiv:2302.11382*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. (2023) White, J.; 等. 2023. 提升 ChatGPT 的提示工程的提示模式目录。*ArXiv:2302.11382*。
- en: Appendix
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: '[h!] Prompt
    1: Zero-shot prompt # System prompt You are a helpful,
    respectful and honest assistant. # Task prompt (example for the sarc task) For
    the sentence: {text}, is it sarcastic? You can choose from the following labels:
    {labels}. Answer:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示
    1：零-shot 提示 # 系统提示 你是一个乐于助人、尊重他人和诚实的助手。
    # 任务提示（sarcasm 任务的示例） 对于句子：{text}，它是否具有讽刺意味？你可以从以下标签中选择：{labels}。回答：'
- en: '[h!] Prompt 2: AI-Knowledge Generation
    # Task prompt For the task: {task_description}. Explain briefly
    the labels: {labels_list}'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示 2: AI-知识生成
    # 任务提示 对于任务: {task_description}。简要解释标签: {labels_list}'
- en: '[h!] Prompt 3: Knowledge-improved
    Zero-shot # System prompt You are a helpful,
    respectful and honest assistant. You have the following knowledge about task-specific
    labels: ’sarcastic’: This label indicates the sentence is sarcastic, meaning it
    conveys irony or mocks with a tone of detachment or insincerity. ’literal’: This
    label is used if the sentence is not sarcastic, implying a straightforward or
    sincere expression without irony. # ... Remainder of prompt as in Prompt 1 ...'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示 3: 知识提升零样本
    # 系统提示 你是一个有帮助、尊重和诚实的助手。你对任务特定标签有以下知识: ’sarcastic’: 该标签表示句子是讽刺的，意味着它传达了讽刺或以疏离或虚伪的语气进行嘲弄。
    ’literal’: 该标签用于句子不是讽刺的，意味着它是一种直白或真诚的表达，没有讽刺。 # ... 提示的其余部分如提示 1 ...'
- en: '[h!] Prompt 5: Fine-tuning Prompt
    # System prompt You are a helpful, respectful and honest assistant.
    # Task prompt For the sentence: {task_description_with_text} You can choose from
    the following labels: {label_list}. Answer: {label}'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示 5: 微调提示
    # 系统提示 你是一个有帮助、尊重和诚实的助手。 # 任务提示 对于句子: {task_description_with_text}
    你可以从以下标签中选择: {label_list}。回答: {label}'
- en: '[h!] Prompt 4: RAG Prompt
    # System prompt You are part of a RAG classification system designed
    to categorize texts. Continued specification of the RAG... # Task prompt Consider
    the relevance and content of each document in relation to the input text and the
    descriptions of the labels. If a retrieved document is highly relevant to the
    input text and aligns closely with the description of a label, that label might
    be the correct classification. Retrieved Documents: Document $i$} Input Text:
    {text}'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示 4: RAG 提示
    # 系统提示 你是一个 RAG 分类系统的一部分，旨在对文本进行分类。持续规范 RAG... # 任务提示 根据每个文档与输入文本的相关性以及标签描述来考虑。如果检索到的文档与输入文本高度相关，并且与标签描述紧密对齐，那么该标签可能是正确的分类。
    检索到的文档: Document $i$} 输入文本: {text}'
- en: 'Answer: [/INST]'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '答案: [/INST]'
- en: '[h!] Prompt 6: Reverse Instructions
    Generation Prompt Instruction: X Output: {doc}
    What kind of instruction could this be the answer to? X:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示 6: 反向指令生成提示
    指令: X 输出: {doc} 这可能是对什么指令的回答？ X:'
- en: '[h!] Prompt
    7: Reverse Instructions Generation for Classification #
    System prompt You are a helpful assistant helping in creating instructions for
    a text classification task. # Task prompt Instruction: X Input: {text} Labels:
    {label_list} Output: {label} What kind of instruction could ‘‘Output’’ be the
    answer to given ‘‘Input’’ and ‘‘Labels’’? Please make only an instruction for
    the task and include brief descriptions of the labels. Begin your answer with
    ’X: ’'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[h!] 提示
    7：分类的反向指令生成 # 系统提示 你是一个有用的助手，帮助创建文本分类任务的指令。
    # 任务提示 指令：X 输入：{text} 标签：{label_list} 输出：{label} ‘‘输出’’是什么样的指令可以作为给定 ‘‘输入’’ 和
    ‘‘标签’’ 的答案？请仅为任务制定一条指令，并包括标签的简要描述。以 ’X: ’ 开始你的回答'
