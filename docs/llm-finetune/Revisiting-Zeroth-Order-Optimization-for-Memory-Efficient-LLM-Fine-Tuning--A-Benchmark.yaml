- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:38:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:38:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾零阶优化在内存高效LLM微调中的应用：基准测试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11592](https://ar5iv.labs.arxiv.org/html/2402.11592)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11592](https://ar5iv.labs.arxiv.org/html/2402.11592)
- en: Yihua Zhang    Pingzhi Li    Junyuan Hong    Jiaxiang Li    Yimeng Zhang   
    Wenqing Zheng    Pin-Yu Chen    Jason D. Lee    Wotao Yin    Mingyi Hong    Zhangyang
    Wang    Sijia Liu    Tianlong Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yihua Zhang    Pingzhi Li    Junyuan Hong    Jiaxiang Li    Yimeng Zhang   
    Wenqing Zheng    Pin-Yu Chen    Jason D. Lee    Wotao Yin    Mingyi Hong    Zhangyang
    Wang    Sijia Liu    Tianlong Chen
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In the evolving landscape of natural language processing (NLP), fine-tuning
    pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like
    SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory
    overhead from back-propagation (BP) for FO gradient computation presents a significant
    challenge. Addressing this issue is crucial, especially for applications like
    on-device training where memory efficiency is paramount. This paper proposes a
    shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing
    memory costs during LLM fine-tuning, building on the initial concept introduced
    by Malladi et al. ([2023](#bib.bib52)). Unlike traditional ZO-SGD methods, our
    work expands the exploration to a wider array of ZO optimization techniques, through
    a comprehensive, first-of-its-kind benchmarking study across five LLM families
    (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning
    schemes. Our study unveils previously overlooked optimization principles, highlighting
    the importance of task alignment, the role of the forward gradient method, and
    the balance between algorithm complexity and fine-tuning performance. We further
    introduce novel enhancements to ZO optimization, including block-wise descent,
    hybrid training, and gradient sparsity. Our study offers a promising direction
    for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all
    our experiments are at [https://github.com/ZO-Bench/ZO-LLM](https://github.com/ZO-Bench/ZO-LLM).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）不断发展的领域中，使用如SGD和Adam等一阶（FO）优化器对预训练的大型语言模型（LLM）进行微调已成为标准。然而，随着LLM规模的扩大，FO梯度计算中的反向传播（BP）带来的巨大内存开销成为一个显著的挑战。解决这一问题至关重要，尤其是在内存效率至关重要的设备端训练应用中。本文建议转向无BP的零阶（ZO）优化，作为减少LLM微调内存成本的解决方案，建立在Malladi等人（[2023](#bib.bib52)）提出的初步概念之上。与传统的ZO-SGD方法不同，我们的工作通过对五个LLM系列（Roberta、OPT、LLaMA、Vicuna、Mistral）、三种任务复杂性和五种微调方案进行的首个全面基准测试，扩展了对更广泛的ZO优化技术的探索。我们的研究揭示了先前被忽视的优化原则，突显了任务对齐的重要性、前向梯度方法的作用以及算法复杂性与微调性能之间的平衡。我们还引入了对ZO优化的新颖改进，包括块状下降、混合训练和梯度稀疏性。我们的研究为实现进一步内存高效的LLM微调提供了有希望的方向。所有实验的代码可以在[https://github.com/ZO-Bench/ZO-LLM](https://github.com/ZO-Bench/ZO-LLM)找到。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习, ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Fine-tuning pre-trained large language models (LLMs) has become the de-facto
    standard in the current paradigms of natural language processing (NLP) (Raffel
    et al., [2023](#bib.bib60); Sanh et al., [2022](#bib.bib65)). First-order (FO)
    optimizers, e.g., SGD (Amari, [1993](#bib.bib1)) and Adam (Kingma & Ba, [2014](#bib.bib41)),
    have been the predominant choices for LLM fine-tuning. However, as LLMs continue
    to scale, they encounter significant memory overhead due to the back-propagation
    (BP) required for FO gradient computation. For example, computing the gradient
    of the LLM OPT-13B requires $12\times$ more memory cost than the model inference.
    This leads to the challenge of achieving memory-efficient fine-tuning in LLMs.
    Advancements in addressing this challenge could also facilitate technological
    breakthroughs in related areas, such as on-device training, where memory efficiency
    is in high demand (Han et al., [2015](#bib.bib26); Zhu et al., [2023](#bib.bib88)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练的大型语言模型（LLMs）已成为当前自然语言处理（NLP）范式中的实际标准（Raffel等， [2023](#bib.bib60)；Sanh等，
    [2022](#bib.bib65)）。一阶（FO）优化器，例如SGD（Amari， [1993](#bib.bib1)）和Adam（Kingma & Ba，
    [2014](#bib.bib41)），一直是LLM微调的主要选择。然而，随着LLMs的不断扩展，它们由于FO梯度计算所需的反向传播（BP）而遇到显著的内存开销。例如，计算LLM
    OPT-13B的梯度需要比模型推理多$12\times$的内存开销。这导致了在LLM中实现内存高效微调的挑战。解决这一挑战的进展还可能促进相关领域的技术突破，例如对设备训练，在该领域内存效率需求很高（Han等，
    [2015](#bib.bib26)；Zhu等， [2023](#bib.bib88)）。
- en: 'To enhance memory efficiency, an emerging solution is to replace a BP-required
    FO optimization method with a BP-free optimizer during LLM fine-tuning. This was
    initially proposed by Malladi et al. ([2023](#bib.bib52)), where the FO gradient
    is approximated using a finite difference of function values. Despite its new
    application to LLM fine-tuning, the underlying optimization principle used in
    Malladi et al. ([2023](#bib.bib52)) is commonly known as zeroth-order (ZO) optimization,
    and the function value-based gradient estimate is referred to as the ZO gradient
    estimate (Flaxman et al., [2005](#bib.bib20); Nesterov & Spokoiny, [2017](#bib.bib55);
    Duchi et al., [2015](#bib.bib18); Ghadimi & Lan, [2013](#bib.bib23); Liu et al.,
    [2020](#bib.bib47)). Malladi et al. ([2023](#bib.bib52)) employed the classical
    ZO stochastic gradient descent (ZO-SGD) algorithm (Ghadimi & Lan, [2013](#bib.bib23)),
    termed MeZO, to fine-tune the pre-trained LLMs and leveraged the BP-free characteristics
    of ZO optimization to reduce memory costs. However, from the perspective of ZO
    optimization, in addition to ZO-SGD, many other ZO optimization methods have not
    yet been explored in the context of LLM fine-tuning. Thus, it remains elusive
    whether there are potential improvements in accuracy and/or efficiency that can
    be achieved through a benchmarking study of ZO optimization for LLM fine-tuning.
    This yields the primary question to be explored:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高记忆效率，一种新兴的解决方案是在LLM微调过程中用不需要BP的优化器替代需要BP的FO优化方法。这最初由Malladi等人（[2023](#bib.bib52)）提出，其中FO梯度是通过函数值的有限差分来近似的。尽管这一方法在LLM微调中的应用较新，但Malladi等人（[2023](#bib.bib52)）所使用的基础优化原则通常被称为零阶（ZO）优化，基于函数值的梯度估计被称为ZO梯度估计（Flaxman等，
    [2005](#bib.bib20)；Nesterov & Spokoiny， [2017](#bib.bib55)；Duchi等， [2015](#bib.bib18)；Ghadimi
    & Lan， [2013](#bib.bib23)；Liu等， [2020](#bib.bib47)）。Malladi等人（[2023](#bib.bib52)）采用了经典的ZO随机梯度下降（ZO-SGD）算法（Ghadimi
    & Lan， [2013](#bib.bib23)），命名为MeZO，以微调预训练LLMs，并利用ZO优化的无BP特性来减少内存开销。然而，从ZO优化的角度来看，除了ZO-SGD，还有许多其他ZO优化方法尚未在LLM微调的背景下进行探索。因此，通过对ZO优化进行基准测试，是否存在能够提升准确性和/或效率的潜在改进仍然不明确。这引出了要探索的主要问题：
- en: (Q) Can we establish a benchmark for ZO optimization in LLM fine-tuning, explore
    the overlooked optimization principles, and advance the current state of the art?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: （Q）我们能否为LLM微调中的ZO优化建立基准，探索被忽视的优化原则，并推动当前技术的进步？
- en: To address (Q), our work introduces several key innovations compared to the
    most relevant work (Malladi et al., [2023](#bib.bib52)). We explore a broader
    range of ZO optimization methods beyond ZO-SGD and examine various task and model
    types, and evaluation metrics. We conduct a detailed comparative analysis of different
    ZO optimization methods, shedding light on the often-overlooked forward gradient
    method (Ren et al., [2022](#bib.bib62)) and other ZO optimization techniques in
    LLM fine-tuning. This benchmarking study helps reveal the pros and cons of these
    methods in accuracy and efficiency. Extended from the gained insights, we propose
    to further improve ZO optimization-based LLM fine-tuning using techniques of block-wise
    descent, hybrid ZO and FO training, and gradient sparsity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决（Q），我们的工作在与最相关的研究（Malladi et al., [2023](#bib.bib52)）相比，引入了几项关键创新。我们探索了更广泛的零阶优化方法，超越了零阶随机梯度下降（ZO-SGD），并审视了各种任务和模型类型，以及评估指标。我们进行了不同零阶优化方法的详细比较分析，揭示了常被忽视的前向梯度方法（Ren
    et al., [2022](#bib.bib62)）以及其他零阶优化技术在大规模语言模型（LLM）微调中的应用。这项基准测试研究有助于揭示这些方法在准确性和效率方面的优缺点。基于获得的见解，我们提出了进一步改进基于零阶优化的大规模语言模型微调的方法，包括分块下降技术、混合零阶和一阶训练以及梯度稀疏化。
- en: In summary, our key contributions are listed below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献如下。
- en: $\bullet$ tasks of varying complexities, and $5$ fine-tuning schemes, covering
    both full-parameter and parameter-efficient fine-tuning (PEFT) approaches.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 涉及不同复杂度的任务，以及 $5$ 种微调方案，涵盖了全参数和参数高效（PEFT）微调方法。
- en: $\bullet$ Assisted by our benchmark, we reveal a range of previously overlooked
    optimization principles and insights for LLM fine-tuning with ZO optimization.
    These include the significance of aligning tasks to enhance ZO optimization, the
    role of forward gradient as an LLM fine-tuning baseline, and the trade-offs between
    algorithm complexity, fine-tuning accuracy, query and memory efficiency.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在我们的基准测试的帮助下，我们揭示了一系列之前被忽视的优化原则和针对零阶优化的LLM微调的见解。这些包括任务对齐以增强零阶优化的重要性，前向梯度作为LLM微调基准的作用，以及算法复杂性、微调准确性、查询和内存效率之间的权衡。
- en: $\bullet$ In addition to a holistic assessment of existing ZO optimization methods
    for LLM fine-tuning, we introduce novel enhancements to ZO optimization, including
    block-wise ZO optimization, hybrid ZO and FO fine-tuning, and sparsity-induced
    ZO optimization. These proposed techniques aim to improve the accuracy of ZO LLM
    fine-tuning while maintaining memory efficiency.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 除了对现有零阶优化方法进行全面评估外，我们引入了零阶优化的新增强技术，包括分块零阶优化、混合零阶和一阶微调，以及稀疏化零阶优化。这些提出的技术旨在提高零阶LLM微调的准确性，同时保持内存效率。
- en: 2 Related Work
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Parameter-efficient fine-tuning (PEFT). Early efforts (Houlsby et al., [2019](#bib.bib30);
    Lin et al., [2020](#bib.bib44)) involved inserting trainable adapters, which are
    compact feed-forward networks, between the layers of the pre-trained model. More
    recently, various PEFT strategies have been proposed. For instance, Adapter-based
    methods (Houlsby et al., [2019](#bib.bib30); Chen et al., [2022](#bib.bib11);
    Luo et al., [2023](#bib.bib51); Karimi Mahabadi et al., [2021](#bib.bib37); Pfeiffer
    et al., [2020](#bib.bib58)) insert a few tunable yet highly compact modules into
    pre-trained models. LoRA (Hu et al., [2021a](#bib.bib31)) employs trainable low-rank
    weight perturbations to the pre-trained model, effectively reducing the required
    number of fine-tuning parameters. Prompt-based learning (Gao et al., [2020](#bib.bib22);
    Hu et al., [2021b](#bib.bib32); Tan et al., [2021](#bib.bib72)) has demonstrated
    effectiveness in various NLP tasks. Additionally, methods like prompt tuning (Lester
    et al., [2021](#bib.bib42)) and prefix tuning (Li & Liang, [2021](#bib.bib43))
    incorporate learnable continuous embeddings into the model’s hidden states to
    condition the frozen model for specific downstream tasks. The following work (Liu
    et al., [2021](#bib.bib48)) demonstrates its applicability on various model scales.
    While these state-of-the-art PEFT techniques have significantly reduced the number
    of parameters required for fine-tuning, they still incur memory costs associated
    with caching numerous activations due to the use of back-propagation (BP) (Malladi
    et al., [2023](#bib.bib52)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）。早期的努力（Houlsby 等人，[2019](#bib.bib30)；Lin 等人，[2020](#bib.bib44)）涉及在预训练模型的层之间插入可训练的适配器，这些适配器是紧凑的前馈网络。近年来，提出了各种PEFT策略。例如，基于适配器的方法（Houlsby
    等人，[2019](#bib.bib30)；Chen 等人，[2022](#bib.bib11)；Luo 等人，[2023](#bib.bib51)；Karimi
    Mahabadi 等人，[2021](#bib.bib37)；Pfeiffer 等人，[2020](#bib.bib58)）在预训练模型中插入少量可调但高度紧凑的模块。LoRA（Hu
    等人，[2021a](#bib.bib31)）对预训练模型施加可训练的低秩权重扰动，有效减少了所需的微调参数数量。基于提示的学习（Gao 等人，[2020](#bib.bib22)；Hu
    等人，[2021b](#bib.bib32)；Tan 等人，[2021](#bib.bib72)）在各种NLP任务中表现出有效性。此外，诸如提示微调（Lester
    等人，[2021](#bib.bib42)）和前缀微调（Li & Liang，[2021](#bib.bib43)）等方法将可学习的连续嵌入整合到模型的隐藏状态中，以便为特定的下游任务调整冻结模型。以下的工作（Liu
    等人，[2021](#bib.bib48)）展示了其在各种模型规模上的适用性。尽管这些最先进的PEFT技术显著减少了微调所需的参数数量，但由于使用反向传播（BP）（Malladi
    等人，[2023](#bib.bib52)），它们仍会带来与缓存大量激活相关的内存开销。
- en: Zeroth-order optimization. Zeroth-order (ZO) optimization is a technique that
    uses finite differences to estimate gradients. Such algorithms utilize function
    value oracle only, yet share similar algorithm structure with first-order (FO)
    gradient-based counterpart methods. ZO optimization usually enjoys provable (dimension-dependent)
    convergence guarantees, as discussed in various works (Flaxman et al., [2005](#bib.bib20);
    Nesterov & Spokoiny, [2017](#bib.bib55); Duchi et al., [2015](#bib.bib18); Ghadimi
    & Lan, [2013](#bib.bib23)). These methods have draw considerable attention due
    to its effectiveness in a wide range of modern machine learning (ML) challenges
    (Liu et al., [2020](#bib.bib47)), including the adversarial attack and defense
    (Chen et al., [2017](#bib.bib10); Tu et al., [2019](#bib.bib76); Ye et al., [2018](#bib.bib83);
    Ilyas et al., [2018](#bib.bib34); Zhang et al., [2022b](#bib.bib85); Verma et al.,
    [2023](#bib.bib78); Zhao et al., [2019](#bib.bib86); Hogan & Kailkhura, [2018](#bib.bib29);
    Shu et al., [2022](#bib.bib66)), model-agnostic contrastive explanations (Dhurandhar
    et al., [2019](#bib.bib17)), enhancing transfer learning through visual prompting
    (Tsai et al., [2020](#bib.bib74)), computational graph unrolling (Vicol et al.,
    [2023](#bib.bib79)), and optimizing automated ML processes (Gu et al., [2021a](#bib.bib24);
    Wang et al., [2022](#bib.bib81)). Beyond standard ML, it finds application in
    policy search in reinforcement learning (Vemula et al., [2019](#bib.bib77)), network
    resource management (Liu et al., [2018](#bib.bib45)), ML-based optimization of
    scientific workflows (Hoffman et al., [2022](#bib.bib28); Tsaknakis et al., [2022](#bib.bib75);
    Chen et al., [2024](#bib.bib9)), and on-chip learning enhancements (Gu et al.,
    [2021b](#bib.bib25)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 零阶优化。零阶（ZO）优化是一种利用有限差分来估计梯度的技术。这类算法仅使用函数值预言机，但与一阶（FO）基于梯度的方法有相似的算法结构。ZO 优化通常具有可证明的（依赖于维度的）收敛保证，如多项研究中所讨论（Flaxman
    et al., [2005](#bib.bib20); Nesterov & Spokoiny, [2017](#bib.bib55); Duchi et
    al., [2015](#bib.bib18); Ghadimi & Lan, [2013](#bib.bib23)）。这些方法因其在现代机器学习（ML）挑战中的有效性而引起了广泛关注（Liu
    et al., [2020](#bib.bib47)），包括对抗攻击和防御（Chen et al., [2017](#bib.bib10); Tu et al.,
    [2019](#bib.bib76); Ye et al., [2018](#bib.bib83); Ilyas et al., [2018](#bib.bib34);
    Zhang et al., [2022b](#bib.bib85); Verma et al., [2023](#bib.bib78); Zhao et al.,
    [2019](#bib.bib86); Hogan & Kailkhura, [2018](#bib.bib29); Shu et al., [2022](#bib.bib66)），模型无关的对比解释（Dhurandhar
    et al., [2019](#bib.bib17)），通过视觉提示增强迁移学习（Tsai et al., [2020](#bib.bib74)），计算图展开（Vicol
    et al., [2023](#bib.bib79)），以及优化自动化机器学习过程（Gu et al., [2021a](#bib.bib24); Wang
    et al., [2022](#bib.bib81)）。除了标准 ML，它还应用于强化学习中的策略搜索（Vemula et al., [2019](#bib.bib77)），网络资源管理（Liu
    et al., [2018](#bib.bib45)），基于 ML 的科学工作流优化（Hoffman et al., [2022](#bib.bib28);
    Tsaknakis et al., [2022](#bib.bib75); Chen et al., [2024](#bib.bib9)），以及芯片上的学习增强（Gu
    et al., [2021b](#bib.bib25)）。
- en: Despite its wide range of use cases, the application of ZO optimization in ML
    has primarily been restricted to small model scales. This limitation is attributed
    to the high variance and slow convergence associated with ZO optimization, which
    are exacerbated by model dimensions. To scale up ZO optimization, several acceleration
    techniques have been proposed. These include the integration of historical data
    to refine ZO gradient estimators (Meier et al., [2019](#bib.bib54); Cheng et al.,
    [2021](#bib.bib15)), leveraging gradient structural information (Singhal et al.,
    [2023](#bib.bib68)) or sparsity to diminish the dependency of ZO optimization
    on problem size (Wang et al., [2017](#bib.bib82); Cai et al., [2022](#bib.bib7),
    [2021](#bib.bib6); Balasubramanian & Ghadimi, [2018](#bib.bib2); Ohta et al.,
    [2020](#bib.bib57); Gu et al., [2021b](#bib.bib25); Chen et al., [2024](#bib.bib9)),
    the reuse of intermediate features (Chen et al., [2024](#bib.bib9)) and random
    perturbation vectors (Malladi et al., [2023](#bib.bib52)) in the optimization
    process. These advancements suggest a growing potential for the application of
    ZO optimization in more complex and large-scale ML problems.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管应用场景广泛，ZO 优化在 ML 中的应用主要局限于小模型规模。这一限制归因于 ZO 优化的高方差和缓慢收敛性，而这些问题在模型维度增加时会加剧。为了扩大
    ZO 优化的规模，提出了若干加速技术。这些技术包括整合历史数据以改进 ZO 梯度估计器（Meier et al., [2019](#bib.bib54);
    Cheng et al., [2021](#bib.bib15)），利用梯度结构信息（Singhal et al., [2023](#bib.bib68)）或稀疏性来减少
    ZO 优化对问题规模的依赖（Wang et al., [2017](#bib.bib82); Cai et al., [2022](#bib.bib7),
    [2021](#bib.bib6); Balasubramanian & Ghadimi, [2018](#bib.bib2); Ohta et al.,
    [2020](#bib.bib57); Gu et al., [2021b](#bib.bib25); Chen et al., [2024](#bib.bib9)），在优化过程中重用中间特征（Chen
    et al., [2024](#bib.bib9)）和随机扰动向量（Malladi et al., [2023](#bib.bib52)）。这些进展表明，ZO
    优化在更复杂和大规模的 ML 问题中的应用潜力正在增长。
- en: BP-free training for large models. Training large models, especially LLMs, is
    memory-consuming due to the involved large computation graphs for BP (Ren et al.,
    [2021](#bib.bib61); Kim et al., [2023](#bib.bib40)). Thus BP-free methods have
    become a recent focus in the deep learning (DL) community. Forward gradient learning (Baydin
    et al., [2022](#bib.bib3); Ren et al., [2022](#bib.bib62); Silver et al., [2021](#bib.bib67);
    Belouze, [2022](#bib.bib4)), built upon the forward-mode automatic differentiation
    (AD), provides an alternative to ZO optimization for BP-free training. Different
    from ZO optimization, it relies on the forward-mode AD to calculate a forward
    (directional) gradient. However, one main limitation of the forward gradient is
    its requirement of full access to the AD software and the deep model, making it
    less memory-efficient than ZO optimization and impractical for tackling black-box
    problems (Chen et al., [2024](#bib.bib9)). The specifications of BP-free methods
    include greedy layer-wise learning (Nøkland & Eidnes, [2019](#bib.bib56)), input-weight
    alignment (Boopathy & Fiete, [2022](#bib.bib5)), forward-forward algorithm (Hinton,
    [2022](#bib.bib27)), synthetic gradients (Jaderberg et al., [2017](#bib.bib35)),
    BBT/BBTv2 evolutionary algorithms (Sun et al., [2022a](#bib.bib70), [b](#bib.bib71)),
    gradient guessing using special low dimensional structure of neural networks (Singhal
    et al., [2023](#bib.bib68)) and other black-box methods which optimize the prompts (Prasad
    et al., [2023](#bib.bib59); Deng et al., [2022](#bib.bib16); Chai et al., [2022](#bib.bib8)).
    Many of these algorithms are also motivated through the lens of seeking DL’s biological
    interpretation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型的BP-free训练。训练大型模型，尤其是LLM，因涉及庞大的计算图而消耗大量内存（Ren et al., [2021](#bib.bib61);
    Kim et al., [2023](#bib.bib40)）。因此，BP-free方法已经成为深度学习（DL）社区的一个新兴焦点。前向梯度学习（Baydin
    et al., [2022](#bib.bib3); Ren et al., [2022](#bib.bib62); Silver et al., [2021](#bib.bib67);
    Belouze, [2022](#bib.bib4)）基于前向模式自动微分（AD），为BP-free训练提供了一种替代方案。与ZO优化不同，它依赖于前向模式AD来计算前向（方向）梯度。然而，前向梯度的一个主要限制是需要对AD软件和深度模型的完全访问，这使得它的内存效率低于ZO优化，并且在解决黑箱问题时不够实用（Chen
    et al., [2024](#bib.bib9)）。BP-free方法的规范包括贪婪逐层学习（Nøkland & Eidnes, [2019](#bib.bib56)）、输入-权重对齐（Boopathy
    & Fiete, [2022](#bib.bib5)）、前向-前向算法（Hinton, [2022](#bib.bib27)）、合成梯度（Jaderberg
    et al., [2017](#bib.bib35)）、BBT/BBTv2进化算法（Sun et al., [2022a](#bib.bib70), [b](#bib.bib71)）、利用神经网络的特殊低维结构进行梯度猜测（Singhal
    et al., [2023](#bib.bib68)）以及其他优化提示的黑箱方法（Prasad et al., [2023](#bib.bib59); Deng
    et al., [2022](#bib.bib16); Chai et al., [2022](#bib.bib8)）。这些算法中的许多也受到寻求DL生物学解释的视角的激励。
- en: Applying ZO optimization to fine-tune pre-trained LLMs is particularly intriguing
    because it combines the advantages of being BP-free and utilizing pre-training.
    This enables the scalability of ZO optimization to large-scale LLMs while maintaining
    memory efficiency. MeZO (Malladi et al., [2023](#bib.bib52)) introduced a scalable
    ZO-SGD algorithm to efficiently fine-tune LLMs with up to 60 billion parameters,
    achieving competitive results compared to first-order optimization methods and
    structured fine-tuning approaches like LoRA. They also provided theoretical insights
    into why ZO methods can be effective for LLMs. This opens the gateway for efficient
    BP-free LLM fine-tuning and largely motivates our ZO benchmark study.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将ZO优化应用于微调预训练的LLM特别引人注目，因为它结合了BP-free和利用预训练的优势。这使得ZO优化能够扩展到大规模LLM，同时保持内存效率。MeZO（Malladi
    et al., [2023](#bib.bib52)）引入了一种可扩展的ZO-SGD算法，以高效微调最多60亿参数的LLM，取得了与一阶优化方法和结构化微调方法如LoRA相媲美的结果。他们还提供了理论见解，解释了为什么ZO方法对LLM有效。这为高效的BP-free
    LLM微调打开了大门，并极大地激励了我们的ZO基准研究。
- en: 3 Reviewing ZO Optimization and Beyond
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 审查ZO优化及其拓展
- en: This work’s core objective is to benchmark and harness the potential of ZO (zeroth-order)
    optimization in LLM finetuning, eliminating the need for (first-order) back-propagation
    (BP) during finetuning and thus achieving memory efficiency (Malladi et al., [2023](#bib.bib52)).
    It is worth noting that the ZO optimization technique utilized in (Malladi et al.,
    [2023](#bib.bib52)) is primarily the basic version, specifically, ZO stochastic
    gradient descent (ZO-SGD). There are more advanced ZO optimization methods available,
    as summarized in (Liu et al., [2020](#bib.bib47)). Thus, this section is dedicated
    to reviewing a broader range of ZO optimization approaches and shedding light
    on the previously overlooked principles for LLM fine-tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的核心目标是基准测试和利用 ZO（零阶）优化在 LLM 微调中的潜力，消除微调过程中对（一级）反向传播（BP）的需求，从而实现内存效率（Malladi
    等，[2023](#bib.bib52)）。值得注意的是，Malladi 等人（[2023](#bib.bib52)）使用的 ZO 优化技术主要是基本版本，即
    ZO 随机梯度下降（ZO-SGD）。还有更多先进的 ZO 优化方法，如（Liu 等，[2020](#bib.bib47)）中所总结的。因此，本节致力于回顾更广泛的
    ZO 优化方法，并阐明之前未被充分重视的 LLM 微调原则。
- en: Basics of ZO optimization. ZO optimization serves as a gradient-free alternative
    to first-order (FO) optimization, approximating FO gradients through function
    value-based gradient estimates, which we call ZO gradient estimates, as discussed
    in (Flaxman et al., [2004](#bib.bib19); Nesterov & Spokoiny, [2017](#bib.bib55);
    Ghadimi & Lan, [2013](#bib.bib23); Duchi et al., [2015](#bib.bib18)). Thus, a
    ZO optimization method typically mirrors the algorithmic framework of its corresponding
    FO optimization counterpart. However, it substitutes the FO gradient with the
    ZO gradient estimate as the descent direction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ZO 优化基础。ZO 优化作为无梯度的一阶（FO）优化替代方法，通过基于函数值的梯度估计来近似 FO 梯度，我们称之为 ZO 梯度估计，正如（Flaxman
    等，[2004](#bib.bib19)；Nesterov & Spokoiny，[2017](#bib.bib55)；Ghadimi & Lan，[2013](#bib.bib23)；Duchi
    等，[2015](#bib.bib18)）中所讨论的。因此，ZO 优化方法通常与其相应的一阶优化方法的算法框架相似。然而，它用 ZO 梯度估计代替 FO 梯度作为下降方向。
- en: 'Various techniques exist for performing ZO gradient estimation. In this paper,
    we focus on the randomized gradient estimator (RGE) (Nesterov & Spokoiny, [2017](#bib.bib55);
    Duchi et al., [2015](#bib.bib18)), a method that relies on the finite difference
    of function values along a randomly chosen direction vector. RGE has also been
    used by Malladi et al. ([2023](#bib.bib52)) to achieve memory-efficient fine-tuning
    for LLMs. Its preference in LLM fine-tuning is attributed to its query efficiency,
    i.e., a low number of function queries. Given a scalar-valued function $f(\mathbf{x})$)
    is expressed using central difference:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种技术用于执行 ZO 梯度估计。本文重点关注随机梯度估计器（RGE）（Nesterov & Spokoiny，[2017](#bib.bib55)；Duchi
    等，[2015](#bib.bib18)），这是一种依赖于沿随机选择的方向向量的函数值有限差分的方法。Malladi 等人（[2023](#bib.bib52)）也使用了
    RGE 来实现内存高效的 LLM 微调。它在 LLM 微调中的偏好归因于其查询效率，即函数查询次数较少。给定一个标量值函数 $f(\mathbf{x})$，其表达方式为中心差分：
- en: '|  | $1$2 |  | (RGE) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (RGE) |'
- en: 'where $\mathbf{u}_{i}$ is a small perturbation stepsize (also known as smoothing
    parameter). Malladi et al. ([2023](#bib.bib52)) employed [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") by setting $q=1$, where $O(\cdot)$
    signifies the Big O notation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{u}_{i}$ 是一个小的扰动步长（也称为平滑参数）。Malladi 等人（[2023](#bib.bib52)）通过设置 $q=1$
    使用了 [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")，其中
    $O(\cdot)$ 表示大 O 符号。'
- en: 'The rationale behind RGE stems from the concept of the directional derivative
    (Duchi et al., [2015](#bib.bib18)): As $\mu\to 0$ along the random direction $\mathbf{u}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RGE 的理论基础源于方向导数的概念（Duchi 等，[2015](#bib.bib18)）：当 $\mu\to 0$ 沿随机方向 $\mathbf{u}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$
    时：
- en: '|  | $\displaystyle\mathbf{E_{\mathbf{u}}}[f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}]=\mathbf{E_{\mathbf{u}}}[\mathbf{u}\mathbf{u}^{T}\nabla
    f(\mathbf{x})]=\nabla f(\mathbf{x}).$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{E_{\mathbf{u}}}[f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}]=\mathbf{E_{\mathbf{u}}}[\mathbf{u}\mathbf{u}^{T}\nabla
    f(\mathbf{x})]=\nabla f(\mathbf{x}).$ |  | (1) |'
- en: With the above background, the RGE $\hat{\nabla}f(\mathbf{x})$ using the directional
    derivative.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述背景下，RGE $\hat{\nabla}f(\mathbf{x})$ 使用方向导数。
- en: 'Forward gradient: A missing BP-free baseline in LLM fine-tuning.'
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前向梯度：LLM 微调中缺失的无 BP 基线。
- en: 'As a byproduct of connecting [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark") to ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")), we obtain the directional derivative-based gradient estimate, $\nabla
    f(\mathbf{x})\approx f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}$, which is known
    as the forward gradient (Forward-Grad) (Baydin et al., [2022](#bib.bib3); Ren
    et al., [2022](#bib.bib62)). Different from [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") that relies solely on the finite difference of
    function values, Forward-Grad requires the use of forward mode automatic differentiation
    (AD) but eliminates the need for the backward evaluation in the implementation
    of deep model fine-tuning or training. In other words, Forward-Grad is BP-free
    and can serve as another alternative gradient estimation method that improves
    the memory efficiency of LLM fine-tuning. We stress that Forward-Grad is a possibly
    overlooked BP-free optimizer. Given its unbiasedness as shown in ([1](#S3.E1 "Equation
    1 ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")), it could serve as an upper
    performance bound for ZO optimization in theory.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '通过将[RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")与
    ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"))
    连接起来，我们得到基于方向导数的梯度估计，$\nabla f(\mathbf{x})\approx f^{\prime}(\mathbf{x},\mathbf{u})\mathbf{u}$，这被称为前向梯度（Forward-Grad）（Baydin等，[2022](#bib.bib3)；Ren等，[2022](#bib.bib62)）。与仅依赖于函数值的有限差分的[RGE](#S3.Ex1
    "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")不同，Forward-Grad需要使用前向模式自动微分（AD），但在深度模型微调或训练的实现中消除了对反向评估的需求。换句话说，Forward-Grad是BP-free的，可以作为另一种替代的梯度估计方法，提高LLM微调的内存效率。我们强调，Forward-Grad是一个可能被忽视的BP-free优化器。鉴于其在
    ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"))
    中展示的无偏性，它可以在理论上作为ZO优化的上界性能。'
- en: 'A focused spectrum of ZO optimization methods. Next, we provide a brief overview
    of the ZO optimization methods to be focused on in this work. Specifically, we
    will include: ZO-SGD (Ghadimi & Lan, [2013](#bib.bib23)) that Malladi et al. ([2023](#bib.bib52))
    has employed for LLM fine-tuning, ZO-SGD using sign-based gradient estimation
    (ZO-SGD-Sign) (Liu et al., [2019a](#bib.bib46)), ZO-SGD with momentum (ZO-SGD-MMT)
    (Malladi et al., [2023](#bib.bib52)), ZO-SGD with conservative gradient update
    (ZO-SGD-Cons), and the ZO variant of the Adam optimizer (ZO-Adam) (Chen et al.,
    [2019](#bib.bib13)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列聚焦于零阶（ZO）优化方法。接下来，我们将简要概述在本工作中要重点关注的ZO优化方法。具体来说，我们将包括：ZO-SGD（Ghadimi & Lan，[2013](#bib.bib23)），Malladi等人（[2023](#bib.bib52)）已经用于LLM微调，ZO-SGD使用基于符号的梯度估计（ZO-SGD-Sign）（Liu等，[2019a](#bib.bib46)），ZO-SGD带动量（ZO-SGD-MMT）（Malladi等人，[2023](#bib.bib52)），ZO-SGD带保守梯度更新（ZO-SGD-Cons），以及Adam优化器的ZO变体（ZO-Adam）（Chen等，[2019](#bib.bib13)）。
- en: 'The aforementioned methods can be unified into the following generic optimization
    framework in solving $\min_{\mathbf{x}}f(\mathbf{x})$:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法可以统一为以下通用优化框架来解决$\min_{\mathbf{x}}f(\mathbf{x})$：
- en: '|  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}h(\hat{\nabla}f(\mathbf{x}_{t})),$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}h(\hat{\nabla}f(\mathbf{x}_{t})),$
    |  | (2) |'
- en: 'where $\mathbf{x}_{t}$ is a certain descent direction post-processing operation.
    In ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing BP-free baseline in LLM
    fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")), we omit the
    inclusion of the stochastic mini-batch for empirical risk minimization for ease
    of presentation. For instance, ZO-SGD can be expressed as ([2](#S3.E2 "Equation
    2 ‣ Forward gradient: A missing BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")) when $h(\hat{\nabla}f(\mathbf{x}))=\hat{\nabla}f(\mathbf{x})$.
    We refer readers to Appx. [B](#S2a "B Zeroth-Order Optimization Algorithms ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")
    for more algorithmic details of ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A
    missing BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and
    Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark")) as applied to the ZO optimization approaches we covered.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{x}_{t}$ 是某种下降方向的后处理操作。在 ([2](#S3.E2 "Equation 2 ‣ Forward gradient:
    A missing BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and
    Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark")) 中，为了简化展示，我们省略了随机小批量的经验风险最小化。例如，当 $h(\hat{\nabla}f(\mathbf{x}))=\hat{\nabla}f(\mathbf{x})$
    时，ZO-SGD 可以表示为 ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing BP-free baseline
    in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"))。有关 ([2](#S3.E2
    "Equation 2 ‣ Forward gradient: A missing BP-free baseline in LLM fine-tuning.
    ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")) 应用于我们涵盖的 ZO 优化方法的更多算法细节，请参阅附录 [B](#S2a
    "B Zeroth-Order Optimization Algorithms ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")。'
- en: 'Our rationale for selecting the aforementioned ZO optimization approaches for
    LLM fine-tuning is based on two key considerations: (1) We prioritize ZO optimization
    methods that require minimal modifications to the existing FO optimizer, ensuring
    ease of implementation for LLM fine-tuning. (2) We focus on methods with distinct
    algorithmic characteristics, allowing us to explore a diverse range of optimization
    strategies for improving LLM performance. Regarding (2), we include ZO-SGD-Sign
    as it employs 1-bit gradient quantization and represents one of the simplest ZO
    optimization methods. Additionally, we include ZO-SGD-MMT and ZO-SGD-Cons as they
    incorporate certain forms of ‘adaptive learning’ into the descent step updates.
    The former utilizes momentum based on historical gradient information, while the
    latter allows for the heuristics-based selection of the descent direction. Furthermore,
    ZO-Adam is one of the most complex ZO optimizers due to its utilization of moving
    averages and adaptive learning rates.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择上述 ZO 优化方法用于 LLM 微调的理由基于两个关键考虑： (1) 我们优先考虑那些对现有 FO 优化器要求最小修改的 ZO 优化方法，以确保
    LLM 微调的实施方便。 (2) 我们关注具有不同算法特征的方法，允许我们探索多样化的优化策略来提高 LLM 的性能。关于 (2)，我们包括 ZO-SGD-Sign，因为它采用了
    1-bit 梯度量化，并且是最简单的 ZO 优化方法之一。此外，我们包括 ZO-SGD-MMT 和 ZO-SGD-Cons，因为它们将某些形式的“自适应学习”纳入了下降步骤更新。前者利用基于历史梯度信息的动量，而后者允许基于启发式的方法选择下降方向。此外，ZO-Adam
    是最复杂的 ZO 优化器之一，因为它利用了移动平均和自适应学习率。
- en: Task alignment in ZO optimization for LLM fine-tuning. Scaling up ZO optimization
    for deep model training, as discussed in (Chen et al., [2024](#bib.bib9)), is
    exceedingly challenging due to its high variance, which is dependent on the model
    size. Nevertheless, LLM pre-training offers a unique advantage by enabling the
    fine-tuner to start from a well-optimized pre-trained model state. This graceful
    model initialization makes ZO optimization potentially scalable to LLM fine-tuning
    tasks (Malladi et al., [2023](#bib.bib52)). Even in this pretraining-finetuning
    paradigm, another crucial factor, which we call ‘task alignment’, still plays
    a key role in achieving satisfactory ZO fine-tuning performance. The ‘task alignment’
    refers to aligning the fine-tuning task with the format of the pre-training task,
    given by the next token or sentence prediction. For example, Gao et al. ([2020](#bib.bib22));
    Malladi et al. ([2023](#bib.bib52)) have transformed downstream text classification
    tasks into next token prediction tasks by introducing well-crafted input prompts.
    These prompts serve as bridges to align the fine-tuning tasks with the pre-training
    ones, facilitating ZO optimization when initiated from the pre-trained model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ZO 优化在 LLM 微调中的任务对齐。正如 (Chen et al., [2024](#bib.bib9)) 中讨论的，将 ZO 优化扩展到深度模型训练中，由于其高方差和模型大小的依赖性，极具挑战性。然而，LLM
    预训练提供了独特的优势，使得微调者可以从经过良好优化的预训练模型状态开始。这种优雅的模型初始化使 ZO 优化在 LLM 微调任务中具有潜在的可扩展性 (Malladi
    et al., [2023](#bib.bib52))。即便在这种预训练-微调范式中，另一个关键因素，即我们称之为“任务对齐”的因素，仍然在实现令人满意的
    ZO 微调性能中扮演着重要角色。“任务对齐”指的是将微调任务与预训练任务的格式对齐，这些格式由下一个 token 或句子预测给出。例如，Gao et al.
    ([2020](#bib.bib22))；Malladi et al. ([2023](#bib.bib52)) 通过引入精心设计的输入提示，将下游文本分类任务转化为下一个
    token 预测任务。这些提示作为桥梁，将微调任务与预训练任务对齐，从而在从预训练模型启动时，促进 ZO 优化。
- en: 'Table 1: Test accuracy (%) of pretrained Roberta-Large model fine-tuned on
    SST2 and RTE tasks using ZO and FO optimization methods with (✓) and without (✗)
    text alignment. The evident performance degradation is highlighted in bold.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：预训练的 Roberta-Large 模型在 SST2 和 RTE 任务上经过 ZO 和 FO 优化方法进行微调后的测试准确率（%），并标明了是否进行了文本对齐（✓
    和 ✗）。明显的性能下降部分用**粗体**标出。
- en: '| Method | SST2 | RTE |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SST2 | RTE |'
- en: '| ✓ | ✗ | Difference | ✓ | ✗ | Difference |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | 差异 | ✓ | ✗ | 差异 |'
- en: '| FO-SGD | 91.6 | 91.5 | 0.1 | 70.9 | 61.4 | 9.5 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | 91.6 | 91.5 | 0.1 | 70.9 | 61.4 | 9.5 |'
- en: '| ZO-SGD | 89.4 | 79.2 | 10.2 | 68.7 | 60.4 | 8.3 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | 89.4 | 79.2 | 10.2 | 68.7 | 60.4 | 8.3 |'
- en: '| ZO-Adam | 89.8 | 79.2 | 10.6 | 69.2 | 58.7 | 10.5 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | 89.8 | 79.2 | 10.6 | 69.2 | 58.7 | 10.5 |'
- en: 'As a warm-up experiment, Tab. [1](#S3.T1 "Table 1 ‣ Forward gradient: A missing
    BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark") empirically justifies the importance of task alignment when applying
    ZO optimization to LLM fine-tuning on the simple binary classification task by
    comparing scenarios with and without the use of pre-defined prompts to achieve
    task alignment. We fine-tune the entire Roberta-Large (Liu et al., [2019b](#bib.bib50))
    model on SST2 (Socher et al., [2013](#bib.bib69)) and RTE (Wang et al., [2019](#bib.bib80))
    datasets with two selected ZO methods: ZO-SGD (i.e., MeZO in (Malladi et al.,
    [2023](#bib.bib52))) and ZO-Adam. We compare their performance with that of the
    FO method (FO-SGD). The task alignment is achieved with the template SENTENCE.
    It was [terrible|great]. for SST dataset and another template SENTENCE1?
    [Yes|No], SENTENCE2. for RTE. As we can see, without prompt-based text alignment,
    there is a substantial performance drop across ZO fine-tuning methods. Both ZO-SGD
    and ZO-Adam yield about $10\%$ accuracy degradation on SST2 and RTE, respectively.
    In contrast, FO-SGD suffers less from the absence of task alignment. This suggests
    that the task alignment is particularly beneficial to ZO LLM fine-tuning. It is
    also worth noting that crafting effective prompts for task alignment can be non-trivial,
    as prompt design is context-dependent and can affect the fine-tuning performance.
    In this work, we follow (Gao et al., [2020](#bib.bib22)) and (Malladi et al.,
    [2023](#bib.bib52)) to align the fine-tuning tasks to the pretrained ones.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '作为一个热身实验，表格 [1](#S3.T1 "Table 1 ‣ Forward gradient: A missing BP-free baseline
    in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") 实证地证明了在将 ZO 优化应用于简单二分类任务的
    LLM 微调时，任务对齐的重要性，通过比较使用和不使用预定义提示来实现任务对齐的场景。我们对 SST2 (Socher et al., [2013](#bib.bib69))
    和 RTE (Wang et al., [2019](#bib.bib80)) 数据集上的整个 Roberta-Large (Liu et al., [2019b](#bib.bib50))
    模型进行微调，使用了两种选定的 ZO 方法：ZO-SGD（即 (Malladi et al., [2023](#bib.bib52)) 中的 MeZO）和
    ZO-Adam。我们将它们的性能与 FO 方法（FO-SGD）的性能进行比较。任务对齐通过模板 SENTENCE. It was [terrible|great].（用于
    SST 数据集）和另一个模板 SENTENCE1? [Yes|No], SENTENCE2.（用于 RTE）实现。如我们所见，在没有基于提示的文本对齐的情况下，ZO
    微调方法的性能大幅下降。ZO-SGD 和 ZO-Adam 在 SST2 和 RTE 上的准确率分别下降了约 $10\%$。相比之下，FO-SGD 在缺乏任务对齐的情况下影响较小。这表明任务对齐对于
    ZO LLM 微调特别有益。还值得注意的是，为任务对齐设计有效的提示可能并不简单，因为提示设计依赖于上下文，并且会影响微调性能。在这项工作中，我们遵循 (Gao
    et al., [2020](#bib.bib22)) 和 (Malladi et al., [2023](#bib.bib52)) 来将微调任务对齐到预训练任务上。'
- en: 4 Benchmarking ZO Optimization for LLM Fine-Tuning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基准测试 ZO 优化在 LLM 微调中的表现
- en: In this section, we delve into the empirical performance of ZO optimization
    in LLM fine-tuning. Our benchmarking effort includes evaluating accuracy and efficiency,
    accounting for different downstream task complexities (ranging from simple classification
    to more complicated reasoning task), and considering various language model types.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了在 LLM 微调中 ZO 优化的实证性能。我们的基准测试工作包括评估准确性和效率，考虑不同的下游任务复杂度（从简单分类到更复杂的推理任务），以及不同语言模型类型。
- en: 4.1 Benchmark Setups
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基准测试设置
- en: 'LLM fine-tuning tasks, schemes, and models. We begin by introducing the tasks
    and the fine-tuning schemes. We focus on three tasks, considering their complexity
    from low to high, which include (1) the simplest binary classification task, Stanford
    Sentiment Treebank v2 (SST2) (Socher et al., [2013](#bib.bib69)), (2) the question-answering
    task, Choice Of Plausible Alternatives (COPA) (Roemmele et al., [2011](#bib.bib63)),
    and (3) the commonsense reasoning task, WinoGrande (Sakaguchi et al., [2021](#bib.bib64)).
    When evaluating memory efficiency, we also consider the task of multi-sentence
    reading comprehension (MultiRC) (Khashabi et al., [2018](#bib.bib38)). For LLM
    fine-tuning on these tasks, we explore four parameter-efficient fine-tuning (PEFT)
    schemes: full-tuning (FT) that fine-tunes the entire pre-trained model, low-rank
    adaptation (LoRA) by imposing low-rank weight perturbations (Hu et al., [2021a](#bib.bib31)),
    prefix-tuning (Prefix) by appending learnable parameters to token embedding (Li
    & Liang, [2021](#bib.bib43)), and prompt-tuning (Prompt) (Liu et al., [2022](#bib.bib49))
    by introducing a series of learnable tokens attached to the input to adapt the
    fixed model to downstream tasks. We refer readers to Appx. [A](#S1a "A Preliminaries
    of Parameter-Efficient Fine-Tuning (PEFT) ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") for details. Furthermore,
    we incorporate several representative language models, including Roberta-Large
    (Liu et al., [2019b](#bib.bib50)), OPT (Zhang et al., [2022a](#bib.bib84)), LLaMA2
    (Touvron et al., [2023](#bib.bib73)), Vicuna (Zheng et al., [2023](#bib.bib87)),
    and Mistral (Jiang et al., [2023](#bib.bib36)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 微调任务、方案和模型。我们首先介绍这些任务和微调方案。我们关注三种任务，根据其复杂性从低到高，这包括（1）最简单的二分类任务，斯坦福情感树库
    v2（SST2）（Socher et al., [2013](#bib.bib69)），（2）问答任务，可选替代选择（COPA）（Roemmele et al.,
    [2011](#bib.bib63)），以及（3）常识推理任务，WinoGrande（Sakaguchi et al., [2021](#bib.bib64)）。在评估内存效率时，我们还考虑了多句阅读理解任务（MultiRC）（Khashabi
    et al., [2018](#bib.bib38)）。对于这些任务上的 LLM 微调，我们探索了四种参数高效微调（PEFT）方案：完全微调（FT），即对整个预训练模型进行微调，低秩适应（LoRA），通过施加低秩权重扰动（Hu
    et al., [2021a](#bib.bib31)），前缀微调（Prefix），通过将可学习参数附加到 token 嵌入中（Li & Liang, [2021](#bib.bib43)），以及提示微调（Prompt）（Liu
    et al., [2022](#bib.bib49)），通过引入一系列可学习的 token 附加到输入中，将固定模型适应于下游任务。有关详细信息，请参阅附录
    [A](#S1a "A Preliminaries of Parameter-Efficient Fine-Tuning (PEFT) ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")。此外，我们还纳入了几个具有代表性的语言模型，包括
    Roberta-Large（Liu et al., [2019b](#bib.bib50)），OPT（Zhang et al., [2022a](#bib.bib84)），LLaMA2（Touvron
    et al., [2023](#bib.bib73)），Vicuna（Zheng et al., [2023](#bib.bib87)）以及 Mistral（Jiang
    et al., [2023](#bib.bib36)）。'
- en: 'Setup and implementation of ZO optimization. To train the previously mentioned
    LLM fine-tuners, we utilize the ZO optimization methods introduced in Sec. [3](#S3
    "3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). These include ZO-SGD (i.e.
    MeZO (Malladi et al., [2023](#bib.bib52))), ZO-SGD-Sign, ZO-SGD-MMT, ZO-SGD-Cons,
    and ZO-Adam. For comparison, we also present the performance of Forward-Grad,
    which relies on the forward mode auto-differentiation rather than BP. We also
    provide the performance of two FO optimizers: (FO-)SGD and (FO-)Adam. Before applying
    the aforementioned optimizers to the LLM fine-tuning tasks, we follow (Gao et al.,
    [2020](#bib.bib22); Malladi et al., [2023](#bib.bib52)) to align the fine-tuning
    task format with the token or sentence prediction-based pre-training task, as
    demonstrated in Tab. [1](#S3.T1 "Table 1 ‣ Forward gradient: A missing BP-free
    baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark").
    We run ZO (or BP-free) optimizers and FO optimizers for 20000 and 625 iterations
    respectively, as outlined in ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing
    BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")). Note that ZO optimization takes a longer convergence time as shown
    in (Liu et al., [2020](#bib.bib47)). When implementing ([RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark")), unless specified otherwise,
    we set the query budget per gradient estimation to $q=1$. We determine the values
    of other hyperparameters, such as the smoothing parameter and learning rate, through
    a grid search for each method. Following (Malladi et al., [2023](#bib.bib52)),
    if not specified otherwise, we adopt the mixed precision (MP) training for FO
    optimizers, where the model is loaded with the full precision (i.e., 32 bits)
    but training is carried out in half precision with 16 bits (FP16). For ZO optimizers,
    half-precision training is adopted throughout the model loading and training.
    We also remark that neither MP nor FP16 can be applied to Forward-Grad. We refer
    readers for more details to Appx. [C](#S3a "C How to Implement Memory-Efficient
    ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZO 优化的设置和实施。为了训练之前提到的 LLM 微调器，我们利用了第 [3](#S3 "3 Reviewing ZO Optimization and
    Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark") 节中介绍的 ZO 优化方法。这些包括 ZO-SGD（即 MeZO（Malladi 等，[2023](#bib.bib52)）），ZO-SGD-Sign，ZO-SGD-MMT，ZO-SGD-Cons
    和 ZO-Adam。为了比较，我们还展示了 Forward-Grad 的性能，该方法依赖于前向模式自动微分，而不是 BP。我们还提供了两个 FO 优化器的性能：
    (FO-)SGD 和 (FO-)Adam。在将上述优化器应用于 LLM 微调任务之前，我们遵循 (Gao et al., [2020](#bib.bib22);
    Malladi et al., [2023](#bib.bib52)) 将微调任务格式与基于 token 或句子的预测预训练任务对齐，如 Tab. [1](#S3.T1
    "Table 1 ‣ Forward gradient: A missing BP-free baseline in LLM fine-tuning. ‣
    3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") 所示。我们对 ZO（或 BP-free）优化器和 FO
    优化器分别进行 20000 次和 625 次迭代，如 ([2](#S3.E2 "Equation 2 ‣ Forward gradient: A missing
    BP-free baseline in LLM fine-tuning. ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")）所述。注意，ZO 优化的收敛时间较长，如 (Liu et al., [2020](#bib.bib47)) 所示。在实现 ([RGE](#S3.Ex1
    "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")) 时，除非另有说明，我们将每次梯度估计的查询预算设置为
    $q=1$。我们通过对每种方法进行网格搜索来确定其他超参数的值，如平滑参数和学习率。根据 (Malladi et al., [2023](#bib.bib52))，如果没有另行说明，我们对
    FO 优化器采用混合精度（MP）训练，其中模型以全精度（即 32 位）加载，但训练以 16 位（FP16）的半精度进行。对于 ZO 优化器，整个模型加载和训练过程都采用半精度训练。我们还指出，Forward-Grad
    既不能应用 MP 也不能应用 FP16。有关更多详细信息，请参阅附录 [C](#S3a "C How to Implement Memory-Efficient
    ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")。'
- en: 'Evaluation metrics. We evaluate ZO LLM fine-tuning using two sets of metrics:
    accuracy and efficiency. Accuracy measures the fine-tuned model’s test-time performance
    in specific tasks, such as test accuracy in classification tasks. Efficiency includes
    various measurements like memory efficiency (in terms of peak memory usage and
    GPU cost), query efficiency (i.e., the number of function queries required for
    ZO optimization ), and run-time efficiency. These metrics collectively provide
    insights into the resources needed for ZO LLM fine-tuning, helping assess its
    feasibility and cost-effectiveness in practical scenarios.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们使用两组指标来评估 ZO LLM 微调：准确性和效率。准确性衡量微调模型在特定任务中的测试性能，例如分类任务中的测试准确性。效率包括各种测量，如内存效率（以峰值内存使用量和
    GPU 成本衡量）、查询效率（即进行 ZO 优化所需的函数查询次数），以及运行时效率。这些指标共同提供有关 ZO LLM 微调所需资源的洞察，有助于评估其在实际场景中的可行性和成本效益。
- en: 4.2 Experiment Results
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'Table 2: Performance of LLM fine-tuning on SST2 over pretrained Roberta-Large
    and OPT/1.3B. Best performance among ZO methods (including Forward-Grad) is highlighted
    in bold.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在预训练的 Roberta-Large 和 OPT/1.3B 上对 SST2 进行 LLM 微调的性能。最佳性能（包括 Forward-Grad）在
    ZO 方法中以粗体突出显示。
- en: '| SST2 | Roberta-Large | OPT-1.3B |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | Roberta-Large | OPT-1.3B |'
- en: '| FT | LoRA | Prefix | Prompt | FT | LoRA | Prefix | Prompt |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| FT | LoRA | Prefix | Prompt | FT | LoRA | Prefix | Prompt |'
- en: '| FO-SGD | $91.4$ | $91.1$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | $91.4$ | $91.1$ |'
- en: '| Forward-Grad | $\mathbf{90.1}$ | $90.3$ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Forward-Grad | $\mathbf{90.1}$ | $90.3$ |'
- en: '| ZO-SGD | $89.4$ | $\mathbf{90.8}$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | $89.4$ | $\mathbf{90.8}$ |'
- en: '| ZO-SGD-MMT | $89.6$ | $85.2$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-MMT | $89.6$ | $85.2$ |'
- en: '| ZO-SGD-Cons | $89.6$ | $88.3$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-Cons | $89.6$ | $88.3$ |'
- en: '| ZO-SGD-Sign | $52.5$ | $87.2$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-Sign | $52.5$ | $87.2$ |'
- en: '| ZO-Adam | $89.8$ | $84.4$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | $89.8$ | $84.4$ |'
- en: 'ZO fine-tuning on SST2: A pilot study. In Tab. [2](#S4.T2 "Table 2 ‣ 4.2 Experiment
    Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"), experiments
    are conducted to compare the performance of different BP-free and BP-based (FO-SGD)
    methods on one of the simplest LLM downstream task: the binary classification
    task with SST2 dataset. We investigate two model architectures, the medium-sized
    Roberta-Large and the larger model OPT-1.3B. Several key results are summarized
    below.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZO 微调在 SST2 上：初步研究。在表[2](#S4.T2 "Table 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking
    ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for
    Memory-Efficient LLM Fine-Tuning: A Benchmark")中，进行了实验以比较不同的 BP-free 和 BP-based（FO-SGD）方法在最简单的
    LLM 下游任务之一：SST2 数据集的二分类任务上的表现。我们调查了两种模型架构，中型 Roberta-Large 和更大模型 OPT-1.3B。以下总结了几个关键结果。'
- en: 'First, ZO-Adam seems to be the most effective ZO method, achieving the best
    performance in 4 out of 8 fine-tuning settings. However, as will be shown later,
    this is achieved at the cost of additional memory consumption. This is not surprising
    considering that ZO-Adam has the highest algorithmic complexity, as explained
    in Sec. [3](#S3 "3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，ZO-Adam 似乎是最有效的 ZO 方法，在 8 种微调设置中有 4 种表现最佳。然而，正如后面将展示的，这一点是以额外的内存消耗为代价的。考虑到
    ZO-Adam 具有最高的算法复杂性，这一点并不令人惊讶，详细说明见第[3](#S3 "3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark)节。'
- en: Second, Forward-Grad is a competitive method compared to the ZO methods, especially
    in the FT (full-tuning) setting. This indicates that Forward-Grad may be suitable
    for problems of a larger scale, and make it a compelling baseline of ZO LLM fine-tuning.
    Additionally, when the complexity of the fine-tuning scheme decreases (e.g., Prompt),
    the advantage of Forward-Grad over function value-based ZO methods diminishes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Forward-Grad 是一种与 ZO 方法相比具有竞争力的方法，特别是在 FT（完全调整）设置下。这表明 Forward-Grad 可能适用于规模较大的问题，并使其成为
    ZO LLM 微调的一个引人注目的基线。此外，当微调方案的复杂性降低（例如，Prompt）时，Forward-Grad 相比于基于函数值的 ZO 方法的优势会减小。
- en: 'Third, The performance of ZO methods exhibits high variance, as evidenced by
    the fluctuating relative rankings across different experimental scenarios, although
    extensive hyper-parameter search efforts have been made. For example, the effectiveness
    of ZO-Adam degrades dramatically in the (OPT-1.3B, Prompt) setting. In addition,
    the MeZO method (i.e., ZO-SGD) used in (Malladi et al., [2023](#bib.bib52)) does
    not always emerge as the top-performing ZO optimizer for LLM fine-tuning across
    various settings. This variance is not surprising and could be largely attributed
    to the high variance of [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark") (Nesterov & Spokoiny, [2017](#bib.bib55); Duchi et al., [2015](#bib.bib18)).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，ZO方法的性能表现出较高的方差，这从不同实验场景下相对排名的波动可以看出，尽管已经进行了广泛的超参数搜索。例如，ZO-Adam在（OPT-1.3B，Prompt）设置中的效果显著下降。此外，（Malladi
    et al., [2023](#bib.bib52)）中使用的MeZO方法（即ZO-SGD）并不总是成为各设置下LLM微调的最佳ZO优化器。这种方差并不令人惊讶，并且可以在很大程度上归因于[RGE](#S3.Ex1
    "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")（Nesterov & Spokoiny,
    [2017](#bib.bib55); Duchi et al., [2015](#bib.bib18)）的高方差。'
- en: Fourth, ZO-SGD-Cons and ZO-SGD-MMT also demonstrate strong performance as ZO
    optimizers in LLM fine-tuning. However, ZO-SGD-Sign, the simplest ZO optimization
    method, tends to be the weakest approach, except in the simplest fine-tuning setting
    Prompt. The above observations motivate us to extend our explorations, investigating
    the effectiveness of ZO methods across a broader spectrum of models and more complex
    tasks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，ZO-SGD-Cons和ZO-SGD-MMT在LLM微调中作为ZO优化器也展示了强劲的性能。然而，ZO-SGD-Sign作为最简单的ZO优化方法，除了在最简单的微调设置Prompt中外，往往是表现最弱的方法。以上观察促使我们扩展探索，研究ZO方法在更广泛模型和更复杂任务中的有效性。
- en: 'ZO fine-tuning on downstream tasks COPA and WinoGrande under OPT-13B. Extended
    from the experiments on SST2, Fig. [1](#S4.F1 "Figure 1 ‣ 4.2 Experiment Results
    ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") presents the
    fine-tuning performance on COPA and WinoGrande dataset using a larger model, OPT-13B.
    We summarize our key observations when the problem scales up and becomes more
    complicated.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '在OPT-13B下对下游任务COPA和WinoGrande进行ZO微调。扩展自SST2的实验，图 [1](#S4.F1 "Figure 1 ‣ 4.2
    Experiment Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")展示了使用更大模型OPT-13B对COPA和WinoGrande数据集进行微调的性能。当问题规模扩大并变得更复杂时，我们总结了主要观察结果。'
- en: '![Refer to caption](img/d9529c00fc156c9a857e818c44e72334.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9529c00fc156c9a857e818c44e72334.png)'
- en: 'Figure 1: Results of OPT-13B on the tasks COPA and WinoGrande fine-tuned using
    ZO/FO optimizers in different PEFT settings.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：OPT-13B在任务COPA和WinoGrande上的结果，使用ZO/FO优化器在不同的PEFT设置下进行微调。
- en: First, compared to the previous results, the performance gap among different
    ZO methods are much enlarged. In the meantime, the performance gap between FO
    and ZO methods are also widened. For example, in the experiments with WinoGrande,
    the FO methods (FO-SGD and FO-Adam) outperform all the other ZO methods by a large
    margin. This observation shows the scalability bottleneck intrinsic to ZO methods,
    when dealing with larger models and/or more complex tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，与之前的结果相比，不同ZO方法之间的性能差距大大扩大。与此同时，FO方法和ZO方法之间的性能差距也在加大。例如，在WinoGrande实验中，FO方法（FO-SGD和FO-Adam）远远优于所有其他ZO方法。这一观察结果显示了ZO方法在处理更大模型和/或更复杂任务时的可扩展性瓶颈。
- en: 'Second, certain ZO methods exhibit exceptional stability across varied conditions:
    Despite a general trend towards variability, specific ZO methods, e.g., ZO-Adam
    and ZO-SGD-MMT, demonstrate stability in their performance. The reason for this
    could be that these algorithms integrate variance-reduced optimization techniques
    (such as momentum and adaptive learning rate) to ZO optimization and become more
    adaptable and resilient to the variances of ZO gradient estimation (Chen et al.,
    [2019](#bib.bib13)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，某些ZO方法在各种条件下表现出卓越的稳定性：尽管存在一般性的变化趋势，特定的ZO方法，例如ZO-Adam和ZO-SGD-MMT，表现出稳定的性能。这可能是因为这些算法将减少方差的优化技术（如动量和自适应学习率）集成到ZO优化中，从而使其对ZO梯度估计的方差更加适应和有韧性（Chen
    et al., [2019](#bib.bib13)）。
- en: Third, the LoRA tuning method is consistently robust when paired with various
    ZO algorithms. This resilience across different ZO methods suggests that LoRA’s
    mechanism is inherently more adaptable to the variations in ZO optimization strategies,
    providing a stable and reliable tuning approach in diverse settings. We will peer
    into the performance of LoRA below.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，LoRA 调优方法在与各种 ZO 算法配对时始终表现出强大的稳定性。这种在不同 ZO 方法下的韧性表明 LoRA 的机制本质上更适应 ZO 优化策略的变化，提供了一种在各种环境中稳定可靠的调优方法。我们将进一步探讨
    LoRA 的性能。
- en: 'Table 3: Performance of different LLMs finetuned with LoRA on COPA and WinoGrande
    using different ZO/FO methods. Table format is consistent with Tab. [2](#S4.T2
    "Table 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3：不同 LLM 在 COPA 和 WinoGrande 上使用不同 ZO/FO 方法经过 LoRA 微调的性能。表格格式与表格 [2](#S4.T2
    "Table 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark") 一致。'
- en: '|  | OPT-13B | LLaMA2-7B | Vicuna-7B | Mistral-7B |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | OPT-13B | LLaMA2-7B | Vicuna-7B | Mistral-7B |'
- en: '| COPA |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| COPA |'
- en: '| FO-SGD | $88$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | $88$ |'
- en: '| FO-Adam | $88$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| FO-Adam | $88$ |'
- en: '| Forward-Grad | $\mathbf{89}$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Forward-Grad | $\mathbf{89}$ |'
- en: '| ZO-SGD | $87$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | $87$ |'
- en: '| ZO-SGD-CONS | $88$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-CONS | $88$ |'
- en: '| ZO-Adam | $\mathbf{89}$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | $\mathbf{89}$ |'
- en: '| WinoGrande |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| WinoGrande |'
- en: '| FO-SGD | $66.9$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | $66.9$ |'
- en: '| FO-Adam | $68.9$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| FO-Adam | $68.9$ |'
- en: '| Forward-Grad | $62.9$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Forward-Grad | $62.9$ |'
- en: '| ZO-SGD | $62.6$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | $62.6$ |'
- en: '| ZO-SGD-CONS | $63.3$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-CONS | $63.3$ |'
- en: '| ZO-Adam | $\mathbf{64.0}$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | $\mathbf{64.0}$ |'
- en: 'In Tab. [3](#S4.T3 "Table 3 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), we present how different ZO methods perform on
    (LoRA, COPA) and (LoRA, WinoGrande) across a range of widely-used LLM families.
    For ease of computation, we focus on a subset of ZO optimization methods, including
    ZO-SGD, ZO-SGD-CONS, and ZO-Adam. As we can see, in some scenarios with the COPA
    dataset, some BP-free methods exhibit effectiveness comparable to, or even superior
    to, that of FO methods (FO-SGD and FO-Adam). For example, Forward-Grad and ZO-Adam
    outperform the best FO method on model OPT-13B and Vicuna-7B. Conversely, for
    the more difficult task WinoGrande, a performance gap of $5\%\sim 6\%$ between
    FO and ZO methods still exists across different models.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [3](#S4.T3 "Table 3 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") 中，我们展示了不同 ZO 方法在 (LoRA, COPA) 和 (LoRA, WinoGrande)
    上在一系列广泛使用的 LLM 家族中的表现。为了方便计算，我们关注于一部分 ZO 优化方法，包括 ZO-SGD、ZO-SGD-CONS 和 ZO-Adam。可以看到，在一些使用
    COPA 数据集的场景中，一些 BP-free 方法表现出与 FO 方法（FO-SGD 和 FO-Adam）相当甚至更优的效果。例如，Forward-Grad
    和 ZO-Adam 在模型 OPT-13B 和 Vicuna-7B 上表现优于最佳 FO 方法。相反，对于更困难的任务 WinoGrande，FO 方法和
    ZO 方法之间在不同模型上仍存在 $5\%\sim 6\%$ 的性能差距。'
- en: '![Refer to caption](img/c3fa4d493ac5d1b288c5cdd7faa49615.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c3fa4d493ac5d1b288c5cdd7faa49615.png)'
- en: 'Figure 2: Memory comparison between FO-SGD and ZO-SGD full fine-tuning across
    various sequence lengths with a fixed effective batch size of $2$), the memory
    usage of FO-SGD remains relatively stable since the memory consumption for storing
    gradients during BP surpasses that needed for activations.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：FO-SGD 和 ZO-SGD 在不同序列长度下的内存比较，固定有效批量大小为 $2$，FO-SGD 的内存使用保持相对稳定，因为在 BP 过程中存储梯度所需的内存超出了存储激活所需的内存。
- en: 'Efficiency analyses. In Tab. [4](#S4.T4 "Table 4 ‣ 4.2 Experiment Results ‣
    4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"), we present a comparison of
    the efficiency performance of various ZO/FO optimizers when fine-tuning the full
    OPT-13B model on the MultiRC dataset with a batch size $4$. We evaluate the efficiency
    in the following dimensions: memory cost (in GB), the consumption of GPU resources
    (number of GPUs), and runtime cost per optimization iteration (in seconds). All
    the experiments are carried out in the same environment. Several key observations
    can be made below.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '效率分析。在表格 [4](#S4.T4 "Table 4 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") 中，我们比较了各种 ZO/FO 优化器在使用批量大小 $4$ 的 MultiRC 数据集对完整
    OPT-13B 模型进行微调时的效率性能。我们从以下几个维度评估效率：内存成本（以 GB 为单位）、GPU 资源消耗（GPU 数量）和每次优化迭代的运行时间成本（以秒为单位）。所有实验都在相同的环境下进行。以下是几个主要观察结果。'
- en: 'First, almost all ZO methods (except ZO-Adam) demonstrate comparable levels
    of efficiency, requiring only a single GPU (A100) for LLM fine-tuning. This observation
    is expected as ZO methods entail relatively straightforward optimization steps,
    primarily based on function evaluations, as depicted in [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). Among the examined ZO optimization
    methods, ZO-Adam incurs higher memory costs due to its algorithmic complexity.
    Second, in comparison to FO methods, ZO optimization reduces runtime costs per
    iteration, e.g., by approximately $33.3\%$ for ZO-SGD compared to FO-SGD. Third,
    Forward-Grad appears to be the point at which ZO optimization methods lose their
    memory efficiency advantage over FO methods. Additionally, we note that the substantial
    runtime cost of Forward-Grad compared to FO optimizers (FO-SGD and FO-Adam) is
    possibly attributed to its incompatibility with MP and FP16.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，几乎所有的ZO方法（除了ZO-Adam）展示了相似的效率水平，只需一个GPU（A100）即可进行LLM微调。这一观察结果是可以预期的，因为ZO方法涉及相对简单的优化步骤，主要基于函数评估，如[RGE](#S3.Ex1
    "方程 RGE ‣ 3 复审ZO优化及其他 ‣ 重新审视零阶优化在内存高效LLM微调中的应用：基准测试")所示。在检查的ZO优化方法中，ZO-Adam由于其算法复杂性，内存开销较高。其次，与FO方法相比，ZO优化在每次迭代中降低了运行时间成本，例如ZO-SGD相比FO-SGD减少了约$33.3\%$。第三，Forward-Grad似乎是ZO优化方法在内存效率上落后于FO方法的临界点。此外，我们注意到Forward-Grad与FO优化器（FO-SGD和FO-Adam）相比，其显著的运行时间成本可能是由于其与MP和FP16的不兼容。
- en: 'Furthermore, we examine the memory cost of LLM fine-tuning vs. the input sequence
    length. In Fig. [2](#S4.F2 "Figure 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking
    ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for
    Memory-Efficient LLM Fine-Tuning: A Benchmark"), we compare the memory efficiency
    between ZO-SGD and FO-SGD across various sequence lengths (i.e. the token number
    per sample). We employed synthetic texts generated from random sequences with
    specified shapes. In all experiments, an effective batch size of $2$ as depicted
    in Fig. [2](#S4.F2 "Figure 2 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")), where the memory allocated for activations overwhelms
    that required for gradient storage. We provide a theoretical analysis of this
    phenomenon in Appx [C](#S3a "C How to Implement Memory-Efficient ZO/FO Optimizers?
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们研究了LLM微调的内存开销与输入序列长度之间的关系。在图 [2](#S4.F2 "图 2 ‣ 4.2 实验结果 ‣ 4 ZO优化的基准测试 ‣
    重新审视零阶优化在内存高效LLM微调中的应用：基准测试")中，我们比较了不同序列长度下ZO-SGD和FO-SGD的内存效率（即每个样本的标记数量）。我们使用了从具有指定形状的随机序列生成的合成文本。在所有实验中，有效的批量大小为$2$，如图 [2](#S4.F2
    "图 2 ‣ 4.2 实验结果 ‣ 4 ZO优化的基准测试 ‣ 重新审视零阶优化在内存高效LLM微调中的应用：基准测试")所示，此时为激活分配的内存超出了梯度存储所需的内存。我们在附录 [C](#S3a
    "C 如何实现内存高效的ZO/FO优化器？ ‣ 重新审视零阶优化在内存高效LLM微调中的应用：基准测试")中对这一现象进行了理论分析。
- en: 'Table 4: The peak memory cost (in GB), the required GPU resources, and the
    runtime cost (in seconds) of each optimizer when fine-tuning the full OPT-13B
    model on MultiRC with an averaged 400 context length. The order of included optimizers
    is ranked based on the memory cost. The per-iteration runtime in seconds (s) is
    averaged over 100 iterations. Notably, Forward-Grad is marked by $*$, indicating
    its incompatibility with efficiency-enhancing techniques such as MP (mixed-precision
    training) and FP16 (half-precision training).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在MultiRC上微调完整的OPT-13B模型时，每种优化器的峰值内存成本（以GB为单位）、所需的GPU资源和运行时间成本（以秒为单位）。包含的优化器的排序是基于内存成本排名的。每次迭代的运行时间（秒）是经过100次迭代的平均值。值得注意的是，Forward-Grad被标记为$*$，表示它与MP（混合精度训练）和FP16（半精度训练）等效率提升技术不兼容。
- en: '| Optimizer | Memory $\Downarrow$ | Runtime Cost |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 内存 $\Downarrow$ | 运行时间成本 |'
- en: '| ZO-SGD | $\mathbf{29}$s |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | $\mathbf{29}$秒 |'
- en: '| ZO-SGD-Cons | $\mathbf{29}$s |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-Cons | $\mathbf{29}$秒 |'
- en: '| ZO-SGD-Sign | $\mathbf{29}$s |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-Sign | $\mathbf{29}$秒 |'
- en: '| ZO-SGD-MMT | $53$s |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-MMT | $53$秒 |'
- en: '| ZO-Adam | $80$s |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | $80$秒 |'
- en: '| Forward-Grad^∗ | $138$s |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Forward-Grad^∗ | $138$秒 |'
- en: '| FO-SGD | $161$s |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | $161$秒 |'
- en: '| FO-Adam | $257$s |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| FO-Adam | $257$秒 |'
- en: 'Ablation study on query budget. Recall from ([1](#S3.E1 "Equation 1 ‣ 3 Reviewing
    ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark")) that Forward-Grad provides us with an unbiased
    gradient estimate with respect to the FO gradient, in contrast to the function
    value-based biased ZO gradient estimate. However, in the above experiments, we
    have not observed a significant advantage brought by Forward-Grad. We hypothesize
    that this is due to the smallest query budget $q=1$ we used, which, despite its
    query efficiency, may not fully showcase the unbiased benefit.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '关于查询预算的消融研究。回忆一下([1](#S3.E1 "Equation 1 ‣ 3 Reviewing ZO Optimization and Beyond
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark"))，Forward-Grad 提供了一个相对于 FO 梯度的无偏梯度估计，相较于基于函数值的有偏 ZO 梯度估计。然而，在上述实验中，我们并没有观察到
    Forward-Grad 带来的显著优势。我们推测这可能是由于我们使用的最小查询预算 $q=1$，尽管它在查询效率上表现良好，但可能没有充分展示无偏估计的好处。'
- en: '![Refer to caption](img/ea86a3f4ef7f840558d1786ff1ebc1a0.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ea86a3f4ef7f840558d1786ff1ebc1a0.png)'
- en: 'Figure 3: LoRA-based fine-tuning accuracy of OPT-1.3B on SST2 using ZO-SGD
    and Forward-Grad over different budgets.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 使用 ZO-SGD 和 Forward-Grad 在不同预算下对 SST2 上的 OPT-1.3B 进行的 LoRA 基础微调准确度。'
- en: 'Inspired by the above, Fig. [3](#S4.F3 "Figure 3 ‣ 4.2 Experiment Results ‣
    4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") explores the impact of varying
    query budget ($q$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '受上述启发，图 Fig. [3](#S4.F3 "Figure 3 ‣ 4.2 Experiment Results ‣ 4 Benchmarking
    ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for
    Memory-Efficient LLM Fine-Tuning: A Benchmark") 探讨了不同查询预算 ($q$) 的影响。'
- en: 5 Extended Study to Improve ZO Fine-Tuning
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 扩展研究以改善 ZO 微调
- en: 'Beyond the benchmarking effort in Sec. [4](#S4 "4 Benchmarking ZO Optimization
    for LLM Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), we will explore algorithmic advancements to further
    improve the effectiveness of ZO LLM fine-tuning. We will leverage the following
    techniques: (1) Block-wise ZO fine-tuning; (2) Hybrid ZO and FO fine-tuning; and
    (3)  sparsity-induced ZO gradient estimation. These designs aim to reduce the
    large variances in gradient estimation when using ZO algorithms.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '除了在 Sec. [4](#S4 "4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting
    Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")
    中的基准测试工作，我们还将探讨算法进展以进一步提高 ZO LLM 微调的效果。我们将利用以下技术：(1) 块-wise ZO 微调；(2) 混合 ZO 和
    FO 微调；(3) 稀疏诱导 ZO 梯度估计。这些设计旨在减少使用 ZO 算法时梯度估计的较大方差。'
- en: Block-wise ZO optimization enhances fine-tuning performance.
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块-wise ZO 优化提升了微调性能。
- en: 'It has been shown in (Liu et al., [2018](#bib.bib45)) that using a coordinate-wise
    deterministic gradient estimator can reduce ZO optimization variance, although
    this scheme is difficult to scale. In a similar vein, we ask if [RGE](#S3.Ex1
    "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") when estimating
    a FO gradient block-wise can also improve the performance of ZO optimization.
    The key idea is to split the LLM into different blocks and then apply the ZO gradient
    estimator to each block of parameters. For example, OPT-1.3B entails $p=26$ forward
    passes for ZO gradient estimation per fine-tuning step. Our rationale is that
    by conducting gradient estimation block-wise, the resulting gradient estimate’s
    variance will be reduced, thereby potentially improving the fine-tuning performance.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在 (Liu et al., [2018](#bib.bib45)) 中已显示，使用坐标-wise 确定性梯度估计器可以减少 ZO 优化的方差，尽管这种方案很难扩展。类似地，我们询问是否在估计
    FO 梯度时， [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣
    Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark")
    的块-wise 方法也能改善 ZO 优化的性能。关键思想是将 LLM 拆分为不同的块，然后将 ZO 梯度估计器应用于每个参数块。例如，OPT-1.3B 在每个微调步骤中需要
    $p=26$ 次前向传递来进行 ZO 梯度估计。我们的理由是，通过块-wise 进行梯度估计，得到的梯度估计的方差将减少，从而可能提高微调性能。'
- en: 'In Tab. [5](#S5.T5 "Table 5 ‣ Block-wise ZO optimization enhances fine-tuning
    performance. ‣ 5 Extended Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"), we compare the
    performance of the ZO fine-tuning baseline MeZO (Malladi et al., [2023](#bib.bib52))
    (corresponding to ZO-SGD with a query budget of $q=1$ as ZO-SGD-Block per iteration.
    Notably, ZO-SGD-Block outperforms ZO-SGD in different query budget settings across
    different fine-tuning tasks, showing the benefit of block-wise ZO tuning.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [5](#S5.T5 "表 5 ‣ 块级 ZO 优化提升了微调性能。 ‣ 5 扩展研究以改善 ZO 微调 ‣ 重新审视零阶优化以实现内存高效的 LLM
    微调：一个基准") 中，我们比较了 ZO 微调基线 MeZO (Malladi 等，[2023](#bib.bib52)) 的性能（对应于每次迭代的 ZO-SGD
    的查询预算为 $q=1$ 的 ZO-SGD-Block）。值得注意的是，ZO-SGD-Block 在不同查询预算设置下超越了 ZO-SGD，在不同的微调任务中显示了块级
    ZO 调优的好处。
- en: 'Table 5: Performance comparison of OPT-1.3B on the SST2 & WinoGrande tasks
    between ZO-SGD and ZO-SGD-Block. The # of parameter blocks in ZO-SGD-Block is
    set to $p=26$. Best performance for each task is highlighted in bold.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ZO-SGD 和 ZO-SGD-Block 在 SST2 和 WinoGrande 任务上的性能比较。ZO-SGD-Block 中的参数块数量设为
    $p=26$。每个任务的最佳性能用粗体标出。
- en: '| Optimizer | Forward Pass # | SST2 | WinoGrande |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 前向传播 # | SST2 | WinoGrande |'
- en: '| MeZO | $1$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| MeZO | $1$ |'
- en: '| ZO-SGD ($q=26$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD ($q=26$ |'
- en: '| ZO-SGD-Block | $26$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-Block | $26$ |'
- en: Trade-off between performance and memory efficiency via hybrid ZO-FO training.
    The primary source of memory cost in LLM fine-tuning arises from the BP process,
    which involves passing gradients from the deep layers to the shallow layers of
    the model. Hence, to enhance memory efficiency, a potential approach is to confine
    BP within the deep layers without propagating it to the shallow layers. Moreover,
    ZO optimization can be employed for the shallow layers without the need for BP.
    The above approach of combining FO optimization for deep layers and ZO optimization
    for shallow layers results in a hybrid ZO-FO fine-tuning scheme for LLMs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过混合 ZO-FO 训练在性能和内存效率之间的权衡。LLM 微调中的主要内存成本来源于 BP 过程，该过程涉及将梯度从模型的深层传递到浅层。因此，为了提高内存效率，一个潜在的方法是将
    BP 限制在深层，而不将其传播到浅层。此外，可以在浅层使用 ZO 优化而不需要 BP。上述将 FO 优化应用于深层，将 ZO 优化应用于浅层的方法，形成了一种混合
    ZO-FO 微调方案。
- en: 'Table 6: The trade-off between memory cost (in GB) v.s. fine-tuning accuracy
    (%) using the hybrid ZO-FO training on the OPT-1.3B model over the SST2 dataset.
    The memory or accuracy gap vs. that of the pure ZO-SGD method is noted by $\Delta$.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用混合 ZO-FO 训练 OPT-1.3B 模型在 SST2 数据集上的内存成本（以 GB 为单位）与微调准确率（%）之间的权衡。与纯 ZO-SGD
    方法相比，内存或准确率差距用 $\Delta$ 标出。
- en: '|  | Memory (GB) | Accuracy ($\%$) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 内存 (GB) | 准确率 ($\%$) |'
- en: '| ZO Layer # | Memory | $\Delta$Accuracy |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ZO 层 # | 内存 | $\Delta$准确率 |'
- en: '| $0$ | $1.98$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $0$ | $1.98$ |'
- en: '| $4$ | $1.88$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $4$ | $1.88$ |'
- en: '| $8$ | $1.55$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $8$ | $1.55$ |'
- en: '| $12$ | $0.24$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $12$ | $0.24$ |'
- en: '| $16$ | $0.18$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| $16$ | $0.18$ |'
- en: '| $20$ | $0.03$ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| $20$ | $0.03$ |'
- en: '| $24$ | $0.00$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| $24$ | $0.00$ |'
- en: 'Tab. [6](#S5.T6 "Table 6 ‣ Block-wise ZO optimization enhances fine-tuning
    performance. ‣ 5 Extended Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") presents the
    performance of using the hybrid ZO-FO fine-tuning scheme on (OPT-1.3B, SST2).
    We examine different variants of this hybrid scheme by deciding ‘where’ to split
    between ZO optimization (for shallow layers) and FO optimization (for deep layers).
    Suppose that the model consists of $n$. The results presented in Tab. [6](#S5.T6
    "Table 6 ‣ Block-wise ZO optimization enhances fine-tuning performance. ‣ 5 Extended
    Study to Improve ZO Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark") demonstrate that employing ZO optimization on only
    the first third of the model layers (i.e., $k\leq 8$), the performance achieved
    is similar to that of full ZO fine-tuning.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S5.T6 "表 6 ‣ 块级 ZO 优化提升了微调性能。 ‣ 5 扩展研究以改善 ZO 微调 ‣ 重新审视零阶优化以实现内存高效的 LLM
    微调：一个基准") 展示了在 (OPT-1.3B, SST2) 上使用混合 ZO-FO 微调方案的性能。我们通过决定‘在哪里’在 ZO 优化（用于浅层）和
    FO 优化（用于深层）之间进行分割，来检查这种混合方案的不同变体。假设模型包含 $n$ 层。表 [6](#S5.T6 "表 6 ‣ 块级 ZO 优化提升了微调性能。
    ‣ 5 扩展研究以改善 ZO 微调 ‣ 重新审视零阶优化以实现内存高效的 LLM 微调：一个基准") 中展示的结果表明，仅在模型的前三分之一层（即 $k\leq
    8$）上应用 ZO 优化，其性能与完整 ZO 微调的性能相似。
- en: Gradient pruning benefits performance.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度修剪有助于提升性能。
- en: 'We next explore gradient pruning, a technique known for accelerating model
    training without compromising convergence (McDanel et al., [2022](#bib.bib53)).
    Our key idea is to induce sparse parameter perturbations for reducing gradient
    estimation variance in [RGE](#S3.Ex1 "Equation RGE ‣ 3 Reviewing ZO Optimization
    and Beyond ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark"). We begin by leveraging magnitude-based pruning (Frankle & Carbin,
    [2018](#bib.bib21); Chen et al., [2020](#bib.bib12)) to obtain the layer-wise
    sparsity ratios. We then generate random pruning masks (following these layer-wise
    sparsity ratios) and apply them to the weight perturbations in [RGE](#S3.Ex1 "Equation
    RGE ‣ 3 Reviewing ZO Optimization and Beyond ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark") per ZO fine-tuning step. Tab. [7](#S5.T7
    "Table 7 ‣ Gradient pruning benefits performance. ‣ 5 Extended Study to Improve
    ZO Fine-Tuning ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM
    Fine-Tuning: A Benchmark") shows the performance of the sparsity-induced ZO gradient
    estimation in LLM fine-tuning as a function of the overall sparsity ratio. It
    becomes evident that choosing a moderate sparsity ratio (e.g., $20\%$) can lead
    to improved performance over the vanilla ZO optimizer, ZO-SGD.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来探讨了梯度剪枝，这是一种加速模型训练而不影响收敛的技术（McDanel 等，[2022](#bib.bib53)）。我们的核心思想是通过诱导稀疏参数扰动来减少
    [RGE](#S3.Ex1 "方程 RGE ‣ 3 回顾 ZO 优化及更多 ‣ 重新审视零阶优化以实现高效内存 LLM 微调：基准") 中的梯度估计方差。我们首先利用基于幅度的剪枝（Frankle
    & Carbin，[2018](#bib.bib21)；Chen 等，[2020](#bib.bib12)）来获取层级稀疏比。然后生成随机剪枝掩码（依据这些层级稀疏比），并将其应用于
    [RGE](#S3.Ex1 "方程 RGE ‣ 3 回顾 ZO 优化及更多 ‣ 重新审视零阶优化以实现高效内存 LLM 微调：基准") 中的权重扰动，每步
    ZO 微调应用。表 [7](#S5.T7 "表 7 ‣ 梯度剪枝提升性能。 ‣ 5 扩展研究以改善 ZO 微调 ‣ 重新审视零阶优化以实现高效内存 LLM
    微调：基准") 显示了稀疏诱导的 ZO 梯度估计在 LLM 微调中的性能与整体稀疏比的关系。显而易见，选择适中的稀疏比（例如，$20\%$）可以在性能上超越原始
    ZO 优化器 ZO-SGD。
- en: 'Table 7: Performance of fine-tuning OPT-1.3B on COPA & SST2 datasets using
    ZO-SGD at different sparsity ratios. A sparsity of $0\%$ sparsity) is highlighted
    in bold.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：使用 ZO-SGD 在不同稀疏比下对 COPA 和 SST2 数据集微调 OPT-1.3B 的性能。$0\%$ 稀疏性以粗体突出显示。
- en: '| COPA |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| COPA |'
- en: '| Sparsity ($\%$ | $30$ | $70$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏性 ($\%$ | $30$ | $70$ |'
- en: '| Accuracy ($\%$ | $70.00$ | $7.000$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 ($\%$ | $70.00$ | $7.000$ |'
- en: '| SST2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SST2 |'
- en: '| Sparsity ($\%$ | $30$ | $70$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏性 ($\%$ | $30$ | $70$ |'
- en: '| Accuracy ($\%$ | $\mathbf{92.32}$ | $\mathbf{92.20}$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 ($\%$ | $\mathbf{92.32}$ | $\mathbf{92.20}$ |'
- en: 6 Conclusion
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This work explores the application of zeroth-order (ZO) optimization in fine-tuning
    large language models (LLMs). ZO optimization approximates gradients using loss
    differences, eliminating the need for back-propagation and activation storage.
    While MeZO (Malladi et al., [2023](#bib.bib52)) has made strides in adapting ZO
    optimization for LLMs, understanding the full ZO landscape remains an open question.
    To achieve this, we broaden the scope by considering various ZO optimization methods,
    task types, and evaluation metrics. We conduct the first benchmark study of different
    ZO optimization techniques, shedding light on their accuracy and efficiency. We
    also uncover overlooked ZO optimization principles, such as task alignment and
    the role of forward gradient. Leveraging these insights, we propose techniques
    like block-wise descent, hybrid ZO and FO training, and gradient sparsity to enhance
    ZO optimization-based LLM fine-tuning. The proposed enhancements to ZO optimization
    enable us to further improve fine-tuning accuracy while maintaining memory efficiency.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了零阶（ZO）优化在微调大型语言模型（LLMs）中的应用。ZO 优化通过使用损失差异来近似梯度，从而无需反向传播和激活存储。虽然 MeZO（Malladi
    等，[2023](#bib.bib52)）在将 ZO 优化适配到 LLMs 方面取得了一些进展，但全面理解 ZO 的全貌仍然是一个未解之谜。为此，我们通过考虑各种
    ZO 优化方法、任务类型和评估指标来扩展研究范围。我们进行了不同 ZO 优化技术的首次基准研究，揭示了它们的准确性和效率。我们还发现了一些被忽视的 ZO 优化原则，如任务对齐和前向梯度的作用。利用这些见解，我们提出了诸如分块下降、混合
    ZO 和 FO 训练以及梯度稀疏性等技术，以增强基于 ZO 优化的 LLM 微调。这些对 ZO 优化的改进使我们能够进一步提高微调准确性，同时保持内存效率。
- en: 7 Impact Statements
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 影响声明
- en: This paper aims to advance the optimization foundations of the memory-efficient
    fine-tuning of large language models (LLMs). Its potential impacts are contingent
    on how these fine-tuned LLMs are utilized. On the positive side, achieving memory
    efficiency during LLM fine-tuning could lead to significant reductions in energy
    consumption, contributing to the development of green AI and achieving improved
    performance in resource-constrained environments. However, there is a potential
    negative aspect in terms of misuse, as the fine-tuned models could be employed
    for generating misinformation, phishing attacks, or releasing copyrighted and
    private information. However, given the technical focus of this work, there are
    no specific societal consequences directly stemming from it that need to be highlighted
    here.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在推进大规模语言模型（LLMs）记忆高效微调的优化基础。其潜在影响取决于这些微调LLMs的使用方式。积极的一面是，在LLM微调过程中实现内存效率可以显著减少能源消耗，有助于绿色AI的发展，并在资源受限的环境中实现更好的性能。然而，也存在潜在的负面方面，微调模型可能被用于生成虚假信息、钓鱼攻击或泄露版权和私人信息。然而，鉴于这项工作的技术重点，直接从中引发的社会后果并不需要特别强调。
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Amari (1993) Amari, S.-i. Backpropagation and stochastic gradient descent method.
    *Neurocomputing*, 5(4-5):185–196, 1993.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari (1993) Amari, S.-i. 反向传播和随机梯度下降方法。*神经计算*, 5(4-5):185–196, 1993。
- en: Balasubramanian & Ghadimi (2018) Balasubramanian, K. and Ghadimi, S. Zeroth-order
    (non)-convex stochastic optimization via conditional gradient and gradient updates.
    *Advances in Neural Information Processing Systems*, 31, 2018.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balasubramanian & Ghadimi (2018) Balasubramanian, K. 和 Ghadimi, S. 零阶（非）凸随机优化通过条件梯度和梯度更新。*神经信息处理系统进展*,
    31, 2018。
- en: Baydin et al. (2022) Baydin, A. G., Pearlmutter, B. A., Syme, D., Wood, F.,
    and Torr, P. Gradients without backpropagation. *arXiv preprint arXiv:2202.08587*,
    2022.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baydin et al. (2022) Baydin, A. G., Pearlmutter, B. A., Syme, D., Wood, F.,
    和 Torr, P. 无需反向传播的梯度。*arXiv预印本 arXiv:2202.08587*, 2022。
- en: Belouze (2022) Belouze, G. Optimization without backpropagation. *arXiv preprint
    arXiv:2209.06302*, 2022.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belouze (2022) Belouze, G. 无需反向传播的优化。*arXiv预印本 arXiv:2209.06302*, 2022。
- en: 'Boopathy & Fiete (2022) Boopathy, A. and Fiete, I. How to train your wide neural
    network without backprop: An input-weight alignment perspective. In *International
    Conference on Machine Learning*, pp.  2178–2205\. PMLR, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boopathy & Fiete (2022) Boopathy, A. 和 Fiete, I. 如何在没有反向传播的情况下训练广泛神经网络：一种输入-权重对齐视角。收录于*国际机器学习会议*，第2178–2205页。PMLR,
    2022。
- en: Cai et al. (2021) Cai, H., Lou, Y., McKenzie, D., and Yin, W. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. *arXiv
    preprint arXiv:2102.10707*, 2021.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2021) Cai, H., Lou, Y., McKenzie, D., 和 Yin, W. 一种用于大规模黑盒优化的零阶块坐标下降算法。*arXiv预印本
    arXiv:2102.10707*, 2021。
- en: 'Cai et al. (2022) Cai, H., Mckenzie, D., Yin, W., and Zhang, Z. Zeroth-order
    regularized optimization (zoro): Approximately sparse gradients and adaptive sampling.
    *SIAM Journal on Optimization*, 32(2):687–714, 2022.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2022) Cai, H., Mckenzie, D., Yin, W., 和 Zhang, Z. 零阶正则化优化（zoro）：近似稀疏梯度和自适应采样。*SIAM优化期刊*,
    32(2):687–714, 2022。
- en: 'Chai et al. (2022) Chai, Y., Wang, S., Sun, Y., Tian, H., Wu, H., and Wang,
    H. Clip-tuning: Towards derivative-free prompt learning with a mixture of rewards,
    2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chai et al. (2022) Chai, Y., Wang, S., Sun, Y., Tian, H., Wu, H., 和 Wang, H.
    Clip-tuning: 通过奖励混合实现无导数提示学习，2022。'
- en: 'Chen et al. (2024) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J.,
    Parasyris, K., Zhang, Y., Zhang, Z., Kailkhura, B., and Liu, S. Deepzero: Scaling
    up zeroth-order optimization for deep model training. *ICLR*, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024) Chen, A., Zhang, Y., Jia, J., Diffenderfer, J., Liu, J.,
    Parasyris, K., Zhang, Y., Zhang, Z., Kailkhura, B., 和 Liu, S. Deepzero: 扩展零阶优化用于深度模型训练。*ICLR*,
    2024。'
- en: 'Chen et al. (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In *Proceedings of the 10th ACM workshop on
    artificial intelligence and security*, pp.  15–26, 2017.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2017) Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., 和 Hsieh, C.-J.
    Zoo: 基于零阶优化的黑盒攻击，无需训练替代模型，针对深度神经网络。收录于*第10届ACM人工智能与安全研讨会论文集*，第15–26页，2017年。'
- en: 'Chen et al. (2022) Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J.,
    and Luo, P. Adaptformer: Adapting vision transformers for scalable visual recognition.
    *Advances in Neural Information Processing Systems*, 35:16664–16678, 2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2022) Chen, S., Ge, C., Tong, Z., Wang, J., Song, Y., Wang, J., 和 Luo,
    P. Adaptformer：为可扩展视觉识别调整视觉变换器。*神经信息处理系统进展*，35:16664–16678，2022年。
- en: Chen et al. (2020) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
    Z., and Carbin, M. The lottery ticket hypothesis for pre-trained bert networks.
    *Advances in neural information processing systems*, 33:15834–15846, 2020.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z.,
    和 Carbin, M. 预训练 BERT 网络的彩票票假设。*神经信息处理系统进展*，33:15834–15846，2020年。
- en: 'Chen et al. (2019) Chen, X., Liu, S., Xu, K., Li, X., Lin, X., Hong, M., and
    Cox, D. Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization.
    *NeurIPS*, 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2019) Chen, X., Liu, S., Xu, K., Li, X., Lin, X., Hong, M., 和 Cox, D.
    Zo-adamm：用于黑箱优化的零阶自适应动量方法。*NeurIPS*，2019年。
- en: 'Cheng et al. (2020) Cheng, M., Singh, S., Chen, P. H., Chen, P.-Y., Liu, S.,
    and Hsieh, C.-J. Sign-opt: A query-efficient hard-label adversarial attack. In
    *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SklTQCNtvS](https://openreview.net/forum?id=SklTQCNtvS).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 (2020) Cheng, M., Singh, S., Chen, P. H., Chen, P.-Y., Liu, S., 和 Hsieh,
    C.-J. Sign-opt：一种查询高效的硬标签对抗攻击。载于 *国际学习表征会议*，2020年。网址 [https://openreview.net/forum?id=SklTQCNtvS](https://openreview.net/forum?id=SklTQCNtvS)。
- en: Cheng et al. (2021) Cheng, S., Wu, G., and Zhu, J. On the convergence of prior-guided
    zeroth-order optimization algorithms. *Advances in Neural Information Processing
    Systems*, 34:14620–14631, 2021.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 (2021) Cheng, S., Wu, G., 和 Zhu, J. 关于先验引导零阶优化算法的收敛性。*神经信息处理系统进展*，34:14620–14631，2021年。
- en: 'Deng et al. (2022) Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu,
    T., Song, M., Xing, E. P., and Hu, Z. Rlprompt: Optimizing discrete text prompts
    with reinforcement learning, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2022) Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T.,
    Song, M., Xing, E. P., 和 Hu, Z. Rlprompt：通过强化学习优化离散文本提示，2022年。
- en: Dhurandhar et al. (2019) Dhurandhar, A., Pedapati, T., Balakrishnan, A., Chen,
    P.-Y., Shanmugam, K., and Puri, R. Model agnostic contrastive explanations for
    structured data. *arXiv preprint arXiv:1906.00117*, 2019.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhurandhar 等 (2019) Dhurandhar, A., Pedapati, T., Balakrishnan, A., Chen, P.-Y.,
    Shanmugam, K., 和 Puri, R. 模型不可知的对比解释用于结构化数据。*arXiv 预印本 arXiv:1906.00117*，2019年。
- en: 'Duchi et al. (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., and Wibisono,
    A. Optimal rates for zero-order convex optimization: The power of two function
    evaluations. *IEEE Transactions on Information Theory*, 61(5):2788–2806, 2015.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duchi 等 (2015) Duchi, J. C., Jordan, M. I., Wainwright, M. J., 和 Wibisono, A.
    零阶凸优化的最佳速率：两个函数评估的力量。*IEEE 信息理论期刊*，61(5):2788–2806，2015年。
- en: 'Flaxman et al. (2004) Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online
    convex optimization in the bandit setting: gradient descent without a gradient.
    *arXiv preprint cs/0408007*, 2004.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flaxman 等 (2004) Flaxman, A. D., Kalai, A. T., 和 McMahan, H. B. 在 bandit 设置下的在线凸优化：无梯度的梯度下降。*arXiv
    预印本 cs/0408007*，2004年。
- en: 'Flaxman et al. (2005) Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online
    convex optimization in the bandit setting: Gradient descent without a gradient.
    In *Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms*,
    pp.  385–394, 2005.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flaxman 等 (2005) Flaxman, A. D., Kalai, A. T., 和 McMahan, H. B. 在 bandit 设置下的在线凸优化：无梯度的梯度下降。载于
    *第十六届 ACM-SIAM 离散算法年会论文集*，第385–394页，2005年。
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2018) Frankle, J. 和 Carbin, M. 彩票票假设：寻找稀疏的可训练神经网络。*arXiv 预印本
    arXiv:1803.03635*，2018年。
- en: Gao et al. (2020) Gao, T., Fisch, A., and Chen, D. Making pre-trained language
    models better few-shot learners. *arXiv preprint arXiv:2012.15723*, 2020.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2020) Gao, T., Fisch, A., 和 Chen, D. 让预训练语言模型成为更好的少样本学习者。*arXiv 预印本 arXiv:2012.15723*，2020年。
- en: Ghadimi & Lan (2013) Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order
    methods for nonconvex stochastic programming. *SIAM Journal on Optimization*,
    23(4):2341–2368, 2013.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghadimi & Lan (2013) Ghadimi, S. 和 Lan, G. 非凸随机编程的随机一阶和零阶方法。*SIAM 优化期刊*，23(4):2341–2368，2013年。
- en: Gu et al. (2021a) Gu, B., Liu, G., Zhang, Y., Geng, X., and Huang, H. Optimizing
    large-scale hyperparameters via automated learning algorithm. *arXiv preprint
    arXiv:2102.09026*, 2021a.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 (2021a) Gu, B., Liu, G., Zhang, Y., Geng, X., 和 Huang, H. 通过自动化学习算法优化大规模超参数。*arXiv
    预印本 arXiv:2102.09026*，2021a年。
- en: Gu et al. (2021b) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., and Pan,
    D. Z. Efficient on-chip learning for optical neural networks through power-aware
    sparse zeroth-order optimization. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 35, pp.  7583–7591, 2021b.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2021b) Gu, J., Feng, C., Zhao, Z., Ying, Z., Chen, R. T., 和 Pan,
    D. Z. 通过功耗感知稀疏零阶优化进行光学神经网络的高效片上学习。在*AAAI人工智能会议论文集*，第35卷，第7583–7591页，2021b。
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Han, S., Mao, H., 和 Dally, W. J. **深度压缩**：通过剪枝、训练量化和赫夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*，2015。
- en: 'Hinton (2022) Hinton, G. The forward-forward algorithm: Some preliminary investigations.
    *arXiv preprint arXiv:2212.13345*, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton (2022) Hinton, G. **前向前向算法**：一些初步研究。*arXiv 预印本 arXiv:2212.13345*，2022。
- en: Hoffman et al. (2022) Hoffman, S. C., Chenthamarakshan, V., Wadhawan, K., Chen,
    P.-Y., and Das, P. Optimizing molecules using efficient queries from property
    evaluations. *Nature Machine Intelligence*, 4(1):21–31, 2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffman et al. (2022) Hoffman, S. C., Chenthamarakshan, V., Wadhawan, K., Chen,
    P.-Y., 和 Das, P. 使用从属性评估中获得的高效查询优化分子。*自然机器智能*，4(1):21–31，2022。
- en: 'Hogan & Kailkhura (2018) Hogan, T. A. and Kailkhura, B. Universal decision-based
    black-box perturbations: Breaking security-through-obscurity defenses. *arXiv
    preprint arXiv:1811.03733*, 2018.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hogan & Kailkhura (2018) Hogan, T. A. 和 Kailkhura, B. **通用决策基础黑盒扰动**：突破基于模糊性的安全防御。*arXiv
    预印本 arXiv:1811.03733*，2018。
- en: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient
    transfer learning for nlp. In *International Conference on Machine Learning*,
    pp.  2790–2799\. PMLR, 2019.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B.,
    De Laroussilhe, Q., Gesmundo, A., Attariyan, M., 和 Gelly, S. **参数高效的迁移学习** 用于自然语言处理。在*国际机器学习会议*，第2790–2799页。PMLR，2019。
- en: 'Hu et al. (2021a) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models,
    2021a.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021a) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., 和 Chen, W. **Lora**: 大型语言模型的低秩适应，2021a。'
- en: 'Hu et al. (2021b) Hu, S., Ding, N., Wang, H., Liu, Z., Wang, J., Li, J., Wu,
    W., and Sun, M. Knowledgeable prompt-tuning: Incorporating knowledge into prompt
    verbalizer for text classification. *arXiv preprint arXiv:2108.02035*, 2021b.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2021b) Hu, S., Ding, N., Wang, H., Liu, Z., Wang, J., Li, J., Wu,
    W., 和 Sun, M. **知识丰富的提示调优**：将知识融入提示文本生成器用于文本分类。*arXiv 预印本 arXiv:2108.02035*，2021b。
- en: Huang et al. (2022) Huang, F., Gao, S., Pei, J., and Huang, H. Accelerated zeroth-order
    and first-order momentum methods from mini to minimax optimization. *The Journal
    of Machine Learning Research*, 23(1):1616–1685, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) Huang, F., Gao, S., Pei, J., 和 Huang, H. 从最小到最小极大优化的加速零阶和一阶动量方法。*机器学习研究杂志*，23(1):1616–1685，2022。
- en: Ilyas et al. (2018) Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box
    adversarial attacks with limited queries and information. In *International conference
    on machine learning*, pp.  2137–2146\. PMLR, 2018.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas et al. (2018) Ilyas, A., Engstrom, L., Athalye, A., 和 Lin, J. 限量查询和信息的黑盒对抗攻击。在*国际机器学习会议*，第2137–2146页。PMLR，2018。
- en: Jaderberg et al. (2017) Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals,
    O., Graves, A., Silver, D., and Kavukcuoglu, K. Decoupled neural interfaces using
    synthetic gradients. In *International conference on machine learning*, pp.  1627–1635\.
    PMLR, 2017.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg et al. (2017) Jaderberg, M., Czarnecki, W. M., Osindero, S., Vinyals,
    O., Graves, A., Silver, D., 和 Kavukcuoglu, K. 使用合成梯度的解耦神经接口。在*国际机器学习会议*，第1627–1635页。PMLR，2017。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., 等. **Mistral 7b**。*arXiv 预印本 arXiv:2310.06825*，2023。
- en: 'Karimi Mahabadi et al. (2021) Karimi Mahabadi, R., Henderson, J., and Ruder,
    S. Compacter: Efficient low-rank hypercomplex adapter layers. *Advances in Neural
    Information Processing Systems*, 34:1022–1035, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimi Mahabadi et al. (2021) Karimi Mahabadi, R., Henderson, J., 和 Ruder, S.
    **Compacter**：高效低秩超复合适配器层。*神经信息处理系统进展*，34:1022–1035，2021。
- en: Khashabi et al. (2018) Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,
    and Roth, D. Looking beyond the surface:a challenge set for reading comprehension
    over multiple sentences. In *Proceedings of North American Chapter of the Association
    for Computational Linguistics (NAACL)*, 2018.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khashabi 等（2018）Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., 和 Roth,
    D. 超越表面：一个多句阅读理解挑战集。见 *北美计算语言学协会（NAACL）年会论文集*，2018。
- en: Kim et al. (2021) Kim, B., Cai, H., McKenzie, D., and Yin, W. Curvature-aware
    derivative-free optimization. *arXiv preprint arXiv:2109.13391*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2021）Kim, B., Cai, H., McKenzie, D., 和 Yin, W. 考虑曲率的无导数优化。*arXiv 预印本 arXiv:2109.13391*，2021。
- en: 'Kim et al. (2023) Kim, T., Kim, H., Yu, G.-I., and Chun, B.-G. BPipe: Memory-balanced
    pipeline parallelism for training large language models. In Krause, A., Brunskill,
    E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), *Proceedings
    of the 40th International Conference on Machine Learning*, volume 202 of *Proceedings
    of Machine Learning Research*, pp.  16639–16653\. PMLR, 23–29 Jul 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023）Kim, T., Kim, H., Yu, G.-I., 和 Chun, B.-G. BPipe：用于训练大型语言模型的记忆平衡流水线并行。见
    Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., 和 Scarlett, J.（编），*第40届国际机器学习会议论文集*，第202卷，*机器学习研究会议论文集*，第16639–16653页。PMLR，2023年7月23日至29日。
- en: 'Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Ba（2014）Kingma, D. P. 和 Ba, J. Adam：一种用于随机优化的方法。*arXiv 预印本 arXiv:1412.6980*，2014。
- en: Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of
    scale for parameter-efficient prompt tuning, 2021.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等（2021）Lester, B., Al-Rfou, R., 和 Constant, N. 参数高效提示调整的规模效应，2021。
- en: 'Li & Liang (2021) Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous
    prompts for generation, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang（2021）Li, X. L. 和 Liang, P. Prefix-tuning：优化连续提示用于生成，2021。
- en: Lin et al. (2020) Lin, Z., Madotto, A., and Fung, P. Exploring versatile generative
    language model via parameter-efficient transfer learning, 2020.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2020）Lin, Z., Madotto, A., 和 Fung, P. 通过参数高效的迁移学习探索多功能生成语言模型，2020。
- en: Liu et al. (2018) Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S.,
    and Amini, L. Zeroth-order stochastic variance reduction for nonconvex optimization.
    In *Advances in Neural Information Processing Systems*, volume 31, 2018.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018）Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., 和 Amini,
    L. 用于非凸优化的零阶随机方差减少。见 *神经信息处理系统进展*，第31卷，2018。
- en: Liu et al. (2019a) Liu, S., Chen, P.-Y., Chen, X., and Hong, M. signSGD via
    zeroth-order oracle. In *International Conference on Learning Representations*,
    2019a.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019a）Liu, S., Chen, P.-Y., Chen, X., 和 Hong, M. 通过零阶神谕的 signSGD。见 *国际学习表征会议*，2019a。
- en: 'Liu et al. (2020) Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III,
    A. O., and Varshney, P. K. A primer on zeroth-order optimization in signal processing
    and machine learning: Principals, recent advances, and applications. In *IEEE
    Signal Processing Magazine*, volume 37, pp.  43–54\. IEEE, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）Liu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III, A. O.,
    和 Varshney, P. K. 零阶优化在信号处理和机器学习中的入门：原理、近期进展和应用。见 *IEEE 信号处理杂志*，第37卷，第43–54页。IEEE，2020。
- en: 'Liu et al. (2021) Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., and
    Tang, J. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks. *arXiv preprint arXiv:2110.07602*, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）Liu, X., Ji, K., Fu, Y., Tam, W. L., Du, Z., Yang, Z., 和 Tang, J.
    P-tuning v2：提示调整在各个规模和任务中可以与微调相媲美。*arXiv 预印本 arXiv:2110.07602*，2021。
- en: 'Liu et al. (2022) Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang,
    J. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and
    tasks. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pp.  61–68, 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., 和 Tang, J. P-tuning：提示调整在各个规模和任务中可以与微调相媲美。见
    *第60届计算语言学协会年会论文集（第2卷：短篇论文）*，第61–68页，2022。
- en: 'Liu et al. (2019b) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized
    bert pretraining approach. *arXiv preprint arXiv:1907.11692*, 2019b.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019b）Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
    O., Lewis, M., Zettlemoyer, L., 和 Stoyanov, V. Roberta：一种鲁棒优化的 BERT 预训练方法。*arXiv
    预印本 arXiv:1907.11692*，2019b。
- en: Luo et al. (2023) Luo, G., Huang, M., Zhou, Y., Sun, X., Jiang, G., Wang, Z.,
    and Ji, R. Towards efficient visual adaption via structural re-parameterization.
    *arXiv preprint arXiv:2302.08106*, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2023）Luo, G., Huang, M., Zhou, Y., Sun, X., Jiang, G., Wang, Z., 和 Ji,
    R. 通过结构重参数化实现高效视觉适配。*arXiv 预印本 arXiv:2302.08106*，2023。
- en: Malladi et al. (2023) Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D.,
    Chen, D., and Arora, S. Fine-tuning language models with just forward passes.
    *arXiv preprint arXiv:2305.17333*, 2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malladi 等人（2023）Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen,
    D., 和 Arora, S. 仅通过前向传递微调语言模型。*arXiv 预印本 arXiv:2305.17333*，2023。
- en: McDanel et al. (2022) McDanel, B., Dinh, H., and Magallanes, J. Accelerating
    dnn training with structured data gradient pruning. In *2022 26th International
    Conference on Pattern Recognition (ICPR)*, pp.  2293–2299\. IEEE, 2022.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDanel 等人（2022）McDanel, B., Dinh, H., 和 Magallanes, J. 通过结构化数据梯度修剪加速 DNN 训练。在
    *2022 年第 26 届国际模式识别大会（ICPR）*，第 2293–2299 页。IEEE，2022。
- en: Meier et al. (2019) Meier, F., Mujika, A., Gauy, M. M., and Steger, A. Improving
    gradient estimation in evolutionary strategies with past descent directions. *arXiv
    preprint arXiv:1910.05268*, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meier 等人（2019）Meier, F., Mujika, A., Gauy, M. M., 和 Steger, A. 通过过去的下降方向改进进化策略中的梯度估计。*arXiv
    预印本 arXiv:1910.05268*，2019。
- en: Nesterov & Spokoiny (2017) Nesterov, Y. and Spokoiny, V. Random gradient-free
    minimization of convex functions. *Foundations of Computational Mathematics*,
    17:527–566, 2017.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov & Spokoiny（2017）Nesterov, Y. 和 Spokoiny, V. 随机无梯度凸函数最小化。*计算数学基础*，17:527–566，2017。
- en: Nøkland & Eidnes (2019) Nøkland, A. and Eidnes, L. H. Training neural networks
    with local error signals. In *International conference on machine learning*, pp. 
    4839–4850\. PMLR, 2019.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nøkland & Eidnes（2019）Nøkland, A. 和 Eidnes, L. H. 用局部误差信号训练神经网络。在 *国际机器学习会议*，第
    4839–4850 页。PMLR，2019。
- en: 'Ohta et al. (2020) Ohta, M., Berger, N., Sokolov, A., and Riezler, S. Sparse
    perturbations for improved convergence in stochastic zeroth-order optimization.
    In *Machine Learning, Optimization, and Data Science: 6th International Conference,
    LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part II 6*,
    pp.  39–64\. Springer, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohta 等人（2020）Ohta, M., Berger, N., Sokolov, A., 和 Riezler, S. 稀疏扰动以改善随机零阶优化中的收敛。在
    *机器学习、优化与数据科学：第 6 届国际会议，LOD 2020，意大利锡耶纳，2020 年 7 月 19–23 日，修订选择论文，第 II 部分 6*，第
    39–64 页。Springer，2020。
- en: 'Pfeiffer et al. (2020) Pfeiffer, J., Rücklé, A., Poth, C., Kamath, A., Vulić,
    I., Ruder, S., Cho, K., and Gurevych, I. Adapterhub: A framework for adapting
    transformers. *arXiv preprint arXiv:2007.07779*, 2020.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer 等人（2020）Pfeiffer, J., Rücklé, A., Poth, C., Kamath, A., Vulić, I.,
    Ruder, S., Cho, K., 和 Gurevych, I. Adapterhub: 适应转换器的框架。*arXiv 预印本 arXiv:2007.07779*，2020。'
- en: 'Prasad et al. (2023) Prasad, A., Hase, P., Zhou, X., and Bansal, M. Grips:
    Gradient-free, edit-based instruction search for prompting large language models,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prasad 等人（2023）Prasad, A., Hase, P., Zhou, X., 和 Bansal, M. Grips: 无梯度、基于编辑的指令搜索以提示大型语言模型，2023。'
- en: Raffel et al. (2023) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer, 2023.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2023）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索统一文本到文本转换器的迁移学习极限，2023。
- en: 'Ren et al. (2021) Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang,
    S., Zhang, M., Li, D., and He, Y. Zero-offload: Democratizing billion-scale model
    training, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人（2021）Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S.,
    Zhang, M., Li, D., 和 He, Y. Zero-offload: 民主化亿规模模型训练，2021。'
- en: Ren et al. (2022) Ren, M., Kornblith, S., Liao, R., and Hinton, G. Scaling forward
    gradient with local losses. *arXiv preprint arXiv:2210.03310*, 2022.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2022）Ren, M., Kornblith, S., Liao, R., 和 Hinton, G. 用局部损失扩大前向梯度。*arXiv
    预印本 arXiv:2210.03310*，2022。
- en: 'Roemmele et al. (2011) Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
    of plausible alternatives: An evaluation of commonsense causal reasoning. In *2011
    AAAI Spring Symposium Series*, 2011.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele 等人（2011）Roemmele, M., Bejan, C. A., 和 Gordon, A. S. 合理选择备选项：对常识因果推理的评估。在
    *2011 AAAI 春季研讨会系列*，2011。
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi 等人（2021）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y. Winogrande:
    大规模对抗性 Winograd 模式挑战。*ACM 通讯*，64(9):99–106，2021。'
- en: Sanh et al. (2022) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika,
    L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari,
    M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani,
    G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen,
    S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma,
    A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Bers, T., Biderman, S.,
    Gao, L., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot
    task generalization, 2022.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh等（2022）Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu,
    C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak,
    N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong,
    Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli,
    A., Fevry, T., Fries, J. A., Teehan, R., Bers, T., Biderman, S., Gao, L., Wolf,
    T., 和Rush, A. M. **多任务提示训练**使零样本任务泛化成为可能，2022。
- en: Shu et al. (2022) Shu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., and Low,
    B. K. H. Zeroth-order optimization with trajectory-informed derivative estimation.
    In *The Eleventh International Conference on Learning Representations*, 2022.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu等（2022）Shu, Y., Dai, Z., Sng, W., Verma, A., Jaillet, P., 和Low, B. K. H.
    **零阶优化**与轨迹信息导向的导数估计。载于*第十一届国际学习表征会议*，2022。
- en: Silver et al. (2021) Silver, D., Goyal, A., Danihelka, I., Hessel, M., and van
    Hasselt, H. Learning by directional gradient descent. In *International Conference
    on Learning Representations*, 2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等（2021）Silver, D., Goyal, A., Danihelka, I., Hessel, M., 和van Hasselt,
    H. **通过方向梯度下降学习**。载于*国际学习表征会议*，2021。
- en: Singhal et al. (2023) Singhal, U., Cheung, B., Chandra, K., Ragan-Kelley, J.,
    Tenenbaum, J. B., Poggio, T. A., and Yu, S. X. How to guess a gradient. *arXiv
    preprint arXiv:2312.04709*, 2023.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal等（2023）Singhal, U., Cheung, B., Chandra, K., Ragan-Kelley, J., Tenenbaum,
    J. B., Poggio, T. A., 和Yu, S. X. 如何猜测梯度。*arXiv预印本 arXiv:2312.04709*，2023。
- en: Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality
    over a sentiment treebank. In *Proceedings of the 2013 conference on empirical
    methods in natural language processing*, pp.  1631–1642, 2013.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher等（2013）Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D.,
    Ng, A. Y., 和Potts, C. **递归深度模型**：在情感树库上进行语义组合性研究。载于*2013年自然语言处理实证方法会议论文集*，第1631–1642页，2013。
- en: 'Sun et al. (2022a) Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X., and Qiu,
    X. Bbtv2: Towards a gradient-free future with large language models, 2022a.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2022a）Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X., 和Qiu, X. **Bbtv2**：迈向无梯度的未来，结合大语言模型，2022a。
- en: Sun et al. (2022b) Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X. Black-box
    tuning for language-model-as-a-service. In *International Conference on Machine
    Learning*, pp.  20841–20855\. PMLR, 2022b.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2022b）Sun, T., Shao, Y., Qian, H., Huang, X., 和Qiu, X. **黑箱调优**用于语言模型即服务。载于*国际机器学习会议*，第20841–20855页。PMLR，2022b。
- en: 'Tan et al. (2021) Tan, Z., Zhang, X., Wang, S., and Liu, Y. Msp: Multi-stage
    prompting for making pre-trained language models better translators. *arXiv preprint
    arXiv:2110.06609*, 2021.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等（2021）Tan, Z., Zhang, X., Wang, S., 和Liu, Y. **Msp**：多阶段提示以提升预训练语言模型的翻译能力。*arXiv预印本
    arXiv:2110.06609*，2021。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等. **Llama 2**：开放基础和精调聊天模型。*arXiv预印本
    arXiv:2307.09288*，2023。
- en: 'Tsai et al. (2020) Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Transfer learning
    without knowing: Reprogramming black-box machine learning models with scarce data
    and limited resources. In *International Conference on Machine Learning*, pp. 
    9614–9624\. PMLR, 2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai等（2020）Tsai, Y.-Y., Chen, P.-Y., 和Ho, T.-Y. **无需知晓的迁移学习**：用稀少数据和有限资源重新编程黑箱机器学习模型。载于*国际机器学习会议*，第9614–9624页。PMLR，2020。
- en: 'Tsaknakis et al. (2022) Tsaknakis, I., Kailkhura, B., Liu, S., Loveland, D.,
    Diffenderfer, J., Hiszpanski, A. M., and Hong, M. Zeroth-order sciml: Non-intrusive
    integration of scientific software with deep learning. *arXiv preprint arXiv:2206.02785*,
    2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsaknakis等（2022）Tsaknakis, I., Kailkhura, B., Liu, S., Loveland, D., Diffenderfer,
    J., Hiszpanski, A. M., 和Hong, M. **零阶Sciml**：将科学软件与深度学习无干扰地集成。*arXiv预印本 arXiv:2206.02785*，2022。
- en: 'Tu et al. (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi,
    J., Hsieh, C.-J., and Cheng, S.-M. Autozoom: Autoencoder-based zeroth order optimization
    method for attacking black-box neural networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, pp.  742–749, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tu et al. (2019) Tu, C.-C., Ting, P., Chen, P.-Y., Liu, S., Zhang, H., Yi,
    J., Hsieh, C.-J., and Cheng, S.-M. Autozoom: 基于自编码器的零阶优化方法用于攻击黑箱神经网络。见于*AAAI 人工智能会议论文集*，第742–749页，2019年。'
- en: 'Vemula et al. (2019) Vemula, A., Sun, W., and Bagnell, J. Contrasting exploration
    in parameter and action space: A zeroth-order optimization perspective. In *The
    22nd International Conference on Artificial Intelligence and Statistics*, pp. 
    2926–2935\. PMLR, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vemula et al. (2019) Vemula, A., Sun, W., and Bagnell, J. 在参数和动作空间中的对比探索：一个零阶优化视角。见于*第22届国际人工智能与统计会议*，第2926–2935页，PMLR，2019年。
- en: Verma et al. (2023) Verma, A., Bangar, S., Subramanyam, A., Lal, N., Shah, R. R.,
    and Satoh, S. Certified zeroth-order black-box defense with robust unet denoiser.
    *arXiv preprint arXiv:2304.06430*, 2023.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma et al. (2023) Verma, A., Bangar, S., Subramanyam, A., Lal, N., Shah, R.
    R., and Satoh, S. 具有鲁棒 UNet 去噪器的认证零阶黑箱防御。*arXiv 预印本 arXiv:2304.06430*，2023年。
- en: Vicol et al. (2023) Vicol, P., Kolter, Z., and Swersky, K. Low-variance gradient
    estimation in unrolled computation graphs with es-single. *arXiv preprint arXiv:2304.11153*,
    2023.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vicol et al. (2023) Vicol, P., Kolter, Z., and Swersky, K. 在展开计算图中的低方差梯度估计与
    es-single。*arXiv 预印本 arXiv:2304.11153*，2023年。
- en: 'Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
    Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. In *International Conference on Learning Representations*, 2019.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
    Bowman, S. R. GLUE：一个用于自然语言理解的多任务基准和分析平台。见于*学习表征国际会议*，2019年。
- en: 'Wang et al. (2022) Wang, X., Guo, W., Su, J., Yang, X., and Yan, J. Zarts:
    On zero-order optimization for neural architecture search. *Advances in Neural
    Information Processing Systems*, 35:12868–12880, 2022.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Wang, X., Guo, W., Su, J., Yang, X., and Yan, J. Zarts：用于神经网络架构搜索的零阶优化。*神经信息处理系统进展*，35:12868–12880，2022年。
- en: Wang et al. (2017) Wang, Y., Du, S., Balakrishnan, S., and Singh, A. Stochastic
    zeroth-order optimization in high dimensions. *arXiv preprint arXiv:1710.10551*,
    2017.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Wang, Y., Du, S., Balakrishnan, S., and Singh, A. 高维中的随机零阶优化。*arXiv
    预印本 arXiv:1710.10551*，2017年。
- en: Ye et al. (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., and Zhang, T. Hessian-aware
    zeroth-order optimization for black-box adversarial attack. *arXiv preprint arXiv:1812.11377*,
    2018.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2018) Ye, H., Huang, Z., Fang, C., Li, C. J., and Zhang, T. Hessian
    感知的零阶优化用于黑箱对抗攻击。*arXiv 预印本 arXiv:1812.11377*，2018年。
- en: 'Zhang et al. (2022a) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022a.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022a) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. OPT：开放预训练的变换器语言模型。*arXiv
    预印本 arXiv:2205.01068*，2022a。
- en: Zhang et al. (2022b) Zhang, Y., Yao, Y., Jia, J., Yi, J., Hong, M., Chang, S.,
    and Liu, S. How to robustify black-box ml models? a zeroth-order optimization
    perspective. *ICLR*, 2022b.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022b) Zhang, Y., Yao, Y., Jia, J., Yi, J., Hong, M., Chang, S.,
    and Liu, S. 如何增强黑箱机器学习模型的鲁棒性？零阶优化视角。*ICLR*，2022b。
- en: Zhao et al. (2019) Zhao, P., Liu, S., Chen, P.-Y., Hoang, N., Xu, K., Kailkhura,
    B., and Lin, X. On the design of black-box adversarial examples by leveraging
    gradient-free optimization and operator splitting method. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, pp.  121–130, 2019.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2019) Zhao, P., Liu, S., Chen, P.-Y., Hoang, N., Xu, K., Kailkhura,
    B., and Lin, X. 通过利用无梯度优化和算子分裂方法设计黑箱对抗样本。见于*IEEE/CVF 计算机视觉国际会议论文集*，第121–130页，2019年。
- en: Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
    Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with
    mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
    Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. 使用 mt-bench 和聊天机器人竞技场评判
    llm-as-a-judge。*arXiv 预印本 arXiv:2306.05685*，2023年。
- en: 'Zhu et al. (2023) Zhu, S., Voigt, T., Ko, J., and Rahimian, F. On-device training:
    A first overview on existing systems, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Zhu, S., Voigt, T., Ko, J., and Rahimian, F. 设备上的训练：对现有系统的首次概述，2023年。
- en: Appendix
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: A Preliminaries of Parameter-Efficient Fine-Tuning (PEFT)
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）初步研究
- en: In our benchmark, we consider three PEFT methods, including {LoRA, prefix tuning,
    prompt tuning}.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基准测试中，我们考虑了三种 PEFT 方法，包括 {LoRA, prefix tuning, prompt tuning}。
- en: '*$1$ in a transformer model, LoRA decomposes it as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*$1$ 在变压器模型中，LoRA 将其分解为：'
- en: '|  | $\mathbf{W}^{\prime}=\mathbf{W}+\mathbf{B}\mathbf{A}$ |  | (A1) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}^{\prime}=\mathbf{W}+\mathbf{B}\mathbf{A}$ |  | (A1) |'
- en: where $W$ represents the rank. During fine-tuning, only $\mathbf{B}$ frozen.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 代表排名。在微调过程中，只有 $\mathbf{B}$ 是固定的。
- en: '*$2$ is the length of the prompt and $d$ is the embedding dimension. The model
    input is then:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*$2$ 是提示的长度，$d$ 是嵌入维度。模型输入为：'
- en: '|  | $\hat{\mathbf{x}}=[\mathbf{P};\mathbf{E}(\mathbf{x})]$ |  | (A2) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{x}}=[\mathbf{P};\mathbf{E}(\mathbf{x})]$ |  | (A2) |'
- en: where $\mathbf{E}(\mathbf{x})$ are learned, with the rest of the model parameters
    kept frozen.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{E}(\mathbf{x})$ 是学习得到的，其余模型参数保持固定。
- en: '*$3$ serving as keys and values in the attention mechanism:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*$3$ 作为注意力机制中的键和值：'
- en: '|  | $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}(\mathbf{K}+\mathbf{C}_{k})^{\text{T}}}{\sqrt{d_{k}}}\right)(\mathbf{V}+\mathbf{C}_{v})$
    |  | (A3) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}(\mathbf{K}+\mathbf{C}_{k})^{\text{T}}}{\sqrt{d_{k}}}\right)(\mathbf{V}+\mathbf{C}_{v})$
    |  | (A3) |'
- en: where $\mathbf{Q}$, $\mathbf{C}_{v}\in\mathbb{R}^{l\times d_{v}}$ are updated,
    and the original model parameters are frozen.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{Q}$ 和 $\mathbf{C}_{v}\in\mathbb{R}^{l\times d_{v}}$ 会被更新，而原始模型参数保持固定。
- en: B Zeroth-Order Optimization Algorithms
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B 零阶优化算法
- en: 'Zeroth-order optimization addresses the minimization or maximization of an
    objective function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ without the use of
    derivatives:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 零阶优化解决了在不使用导数的情况下，最小化或最大化目标函数 $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ 的问题：
- en: '|  | $\min_{\mathbf{x}\in\mathbb{R}^{n}}f(\mathbf{x})$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{x}\in\mathbb{R}^{n}}f(\mathbf{x})$ |  |'
- en: 'These methods are pivotal when the function is non-differentiable, gradient
    computation is expensive, or the function evaluations are noisy. Random gradient
    estimation (RGE) provides a surrogate for gradients in zeroth-order optimization
    by sampling function evaluations. The gradient $\nabla f(\mathbf{x})$ can be approximated
    as:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当函数不可微、梯度计算昂贵或函数评估有噪声时，这些方法至关重要。随机梯度估计 (RGE) 通过采样函数评估来提供零阶优化的梯度替代。梯度 $\nabla
    f(\mathbf{x})$ 可以近似为：
- en: '|  | $\hat{\nabla}f(\mathbf{x}):=\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u}$
    |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\nabla}f(\mathbf{x}):=\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u}$
    |  |'
- en: where $\mathbf{u}$ is a small scalar. This estimation facilitates the use of
    gradient-based methods solely based on function evaluations. Utilizing this gradient
    estimator, we summarize the existing zeroth-other algorithms below.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}$ 是一个小标量。这个估计有助于仅基于函数评估来使用基于梯度的方法。利用这个梯度估计器，我们总结了现有的零阶算法如下。
- en: '$\rhd$ ZO-SGD (Ghadimi & Lan, [2013](#bib.bib23)). The method directly update
    the parameters by the estimated gradient:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ ZO-SGD (Ghadimi & Lan, [2013](#bib.bib23))。该方法直接通过估计的梯度更新参数：
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\hat{\nabla}f(\mathbf{x}_{t}),$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\hat{\nabla}f(\mathbf{x}_{t}),$
    |  |'
- en: where  is
    the learning rate.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是学习率。
- en: $\rhd$ ZO-SGD-Sign (Liu et al., [2019a](#bib.bib46); Cheng et al., [2020](#bib.bib14)).
    The intuition of ZO-SGD-Sign is to make the gradient estimation more robust to
    noise since the sign operation could mitigate the negative effect of (coordinate-wise)
    gradient noise of large variance. Yet, the downside of ZO-SGD-Sign is the lack
    of estimation precision. It is given by
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ ZO-SGD-Sign (Liu et al., [2019a](#bib.bib46); Cheng et al., [2020](#bib.bib14))。ZO-SGD-Sign
    的直观是使梯度估计对噪声更加稳健，因为符号操作可以减轻大方差（坐标-wise）的梯度噪声的负面影响。然而，ZO-SGD-Sign 的缺点是估计精度不足。公式如下：
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\operatorname{sign}(\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u})$
    |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\operatorname{sign}(\frac{f(\mathbf{x}+\mu\mathbf{u})-f(\mathbf{x}-\mu\mathbf{u})}{2\mu}\mathbf{u})$
    |  |'
- en: where $\mathbf{u}$ is a standard Gaussian vector.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{u}$ 是一个标准高斯向量。
- en: $\rhd$ ZO-SGD-MMT (Huang et al., [2022](#bib.bib33)). The stochastic gradient
    estimated from a batch may suffer from a large variance. Momentum uses a moving
    average to estimate the global gradient and update the parameters by
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ ZO-SGD-MMT (Huang et al., [2022](#bib.bib33))。从批次中估计的随机梯度可能会有较大方差。动量通过使用移动平均来估计全局梯度，并通过以下方式更新参数：
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\mathbf{m}_{t},$ |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\mathbf{m}_{t},$ |  |'
- en: with the momentum defined as
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动量定义为
- en: '|  | $\mathbf{m}_{t}=\beta_{t}\mathbf{m}_{t-1}+\hat{\nabla}f(\mathbf{x}_{{t}}).$
    |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{m}_{t}=\beta_{t}\mathbf{m}_{t-1}+\hat{\nabla}f(\mathbf{x}_{{t}}).$
    |  |'
- en: ZO-SGD-MMT adopts the momentum factor $\beta_{t}$ to control the average.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD-MMT采用动量因子$\beta_{t}$来控制平均值。
- en: '$\rhd$ ZO-SGD-Cons (Kim et al., [2021](#bib.bib39)). This method is adapted
    from (Kim et al., [2021](#bib.bib39)) to update the parameter conservatively:
    we pick up the point corresponding to the smallest loss value. The update is given
    by'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ ZO-SGD-Cons (Kim等，[2021](#bib.bib39))。该方法改编自(Kim等，[2021](#bib.bib39))，以保守地更新参数：我们选择对应于最小损失值的点。更新由下式给出
- en: '|  | $1$2 |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: $\rhd$ ZO-Adam (Chen et al., [2019](#bib.bib13)). Similar to the ZO-SGD with
    momentum, ZO-Adam uses momentum to estimate the gradients. In addition, ZO-Adam
    adaptively penalizes the learning rate to reduce the noise. The update of ZO-Adam
    is given by
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ ZO-Adam (Chen等，[2019](#bib.bib13))。类似于具有动量的ZO-SGD，ZO-Adam使用动量来估计梯度。此外，ZO-Adam自适应地惩罚学习率以减少噪声。ZO-Adam的更新由下式给出
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}}\mathbf{V}_{t}^{-1/2}\mathbf{m}_{t},$
    |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}}\mathbf{V}_{t}^{-1/2}\mathbf{m}_{t},$
    |  |'
- en: where the momentum vector ${\mathbf{m}}$ are computed by
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动量向量${\mathbf{m}}$的计算方法为
- en: '|  | $\displaystyle\mathbf{m}_{t}$ |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{m}_{t}$ |  |'
- en: '|  | $\displaystyle\mathbf{v}_{t}$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{v}_{t}$ |  |'
- en: '|  | $\displaystyle\mathbf{V}_{t}$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{V}_{t}$ |  |'
- en: The hyperparameters $\beta_{1}$. We remove RMSProp since it increases the memory
    cost and could void the advantage of ZO methods compared to FO-SGD.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数$\beta_{1}$。我们去除了RMSProp，因为它增加了内存成本，并可能使ZO方法相较于FO-SGD的优势无效。
- en: $\rhd$ Forward-Grad (Baydin et al., [2022](#bib.bib3)). Forward-Grad relies
    on the directional derivative along a random direction vector. Different from
    ZO optimization methods, it provides an unbiased stochastic gradient estimator
    of the FO gradient. The update of Forward-Grad is given by
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: $\rhd$ Forward-Grad (Baydin等，[2022](#bib.bib3))。Forward-Grad依赖于沿随机方向向量的方向导数。与ZO优化方法不同，它提供了FO梯度的无偏随机梯度估计器。Forward-Grad的更新由下式给出
- en: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}(\nabla f(\mathbf{x}_{t})^{\top}\mathbf{u})\mathbf{u},$
    |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-\eta_{t}(\nabla f(\mathbf{x}_{t})^{\top}\mathbf{u})\mathbf{u},$
    |  |'
- en: where $\mathbf{u}$, it employs the Jacobian-Vector Product during the forward
    pass of training to mitigate computation and memory requirements. As a result,
    the memory complexity remains relatively low compared to BP-based methods.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{u}$，它在训练的前向传递中使用雅可比-向量积来减轻计算和内存需求。因此，与基于BP的方法相比，内存复杂性保持相对较低。
- en: C How to Implement Memory-Efficient ZO/FO Optimizers?
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C 如何实现内存高效的ZO/FO优化器？
- en: Algorithm A1 A General Pipeline for A FO/ZO Optimizer
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 算法A1 FO/ZO优化器的一般流程
- en: 'Input: Model with forward function $f$: States of optimizer/model, $\bm{\tau}$;  Step
    2: Backward Pass: Calculate gradients w.r.t. ${\mathbf{x}}$ using gradients and
    utilize temporal state ${\mathbf{s}}_{\text{opt}}^{\prime}$.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：具有前向函数$f$的模型：优化器/模型的状态，$\bm{\tau}$；  步骤2：反向传递：计算相对于${\mathbf{x}}$的梯度，并利用时间状态${\mathbf{s}}_{\text{opt}}^{\prime}$。
- en: The memory efficiency of an optimizer heavily depends on its implementation
    details. This section will discuss these implementation details and provide a
    holistic memory profile of all the optimizers discussed above. We remark that
    the discussions in this section are based on the PyTorch framework.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的内存效率在很大程度上取决于其实现细节。本节将讨论这些实现细节，并提供所有上述优化器的整体内存概况。我们指出，本节的讨论基于PyTorch框架。
- en: 'In general, the memory efficiency of an optimizer is defined by the peak memories
    consumed during training in a specific setting, which primarily depends on the
    maximum number of variables stored at a certain time point. To dissect the memory
    consumption of different optimizers, we summarized a general model pipeline for
    parameter updating in [Algorithm A1](#alg1 "In C How to Implement Memory-Efficient
    ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient
    LLM Fine-Tuning: A Benchmark"), which involves four main steps.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，优化器的内存效率由特定设置下训练期间消耗的峰值内存定义，这主要取决于某一时间点存储的最大变量数量。为了剖析不同优化器的内存消耗，我们总结了一个用于参数更新的通用模型流程，见
    [算法 A1](#alg1 "如何实现内存高效的 ZO/FO 优化器？‣ 重新审视用于内存高效 LLM 微调的零阶优化：一个基准")，该流程包括四个主要步骤。
- en: First, the program needs to load the model with the full parameters ($\mathbf{x}$
    and temporarily storing $\bm{\tau}_{\text{bwd}}$ will be released. Typically,
    ${\mathbf{s}}_{\text{bwd}}$ also includes essential temporal variables that are
    only used in the step.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，程序需要加载包含完整参数的模型（$\mathbf{x}$ 和暂时存储的 $\bm{\tau}_{\text{bwd}}$ 将被释放）。通常，${\mathbf{s}}_{\text{bwd}}$
    还包括仅在该步骤中使用的重要临时变量。
- en: 'According to the memory life-cycle, the memory consumption will be summarized
    as two types: *constant memory* that exists through the training, and *dynamic
    allocation* that only exists in one iteration mainly for gradient computing. Though
    dynamic allocation only exists temporarily, it also contribute to the peak memory.
    Without losing generality, we summarize the primary components of the peak memory
    consumption as'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 根据内存生命周期，内存消耗将总结为两种类型：*常量内存*，它在训练过程中始终存在，以及*动态分配*，它仅在一个迭代中存在，主要用于梯度计算。尽管动态分配只是暂时存在，但它也会影响峰值内存。为了不失一般性，我们总结了峰值内存消耗的主要组成部分如下
- en: '|  | $\displaystyle\underbrace{&#124;{\mathbf{x}}&#124;+&#124;{\mathbf{s}}_{\text{opt}}&#124;}_{\text{constant}}+\underbrace{\max\left\{&#124;{\mathbf{s}}_{\text{fwd}}&#124;,&#124;{\mathbf{s}}_{\text{bwd}}&#124;,&#124;{\mathbf{s}}_{\text{opt}}^{\prime}&#124;\right\}}_{\text{dynamic
    allocation}},$ |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underbrace{&#124;{\mathbf{x}}&#124;+&#124;{\mathbf{s}}_{\text{opt}}&#124;}_{\text{constant}}+\underbrace{\max\left\{&#124;{\mathbf{s}}_{\text{fwd}}&#124;,&#124;{\mathbf{s}}_{\text{bwd}}&#124;,&#124;{\mathbf{s}}_{\text{opt}}^{\prime}&#124;\right\}}_{\text{dynamic
    allocation}},$ |  |'
- en: where $|\cdot|$ denotes the cardinality of a vector. Note that we concentrate
    on the primary factors in Big O notation, which, unless specified otherwise, was
    omitted for brevity.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|\cdot|$ 表示向量的基数。注意，我们集中讨论 Big O 符号中的主要因素，除非另有说明，否则为简洁起见省略了。
- en: C.1 Prevalent Efficiency-Enhancing Tricks Adopted in This Work
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 本工作中采用的普遍效率提升技巧
- en: Prior to devling into the dissection of the memory consumption of different
    optimizers, we first introduce the prevalent efficiency-enhancing tricks widely
    adopted in the state-of-the-art FO/ZO-optimizers. These methods are by default
    adopted in this work if compatible to achieve the state-of-the-art performance.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入分析不同优化器的内存消耗之前，我们首先介绍当前最先进的 FO/ZO 优化器中广泛采用的效率提升技巧。如果兼容，这些方法将在本工作中默认使用，以实现最先进的性能。
- en: Half-precision training (FP16). Most modern LLMs, e.g. LLaMA2 (Touvron et al.,
    [2023](#bib.bib73)), operate at half-precision (i.e., 16-bit) for inference, denoted
    as FP16. With FP16, only half of the model memory is required for ZO methods.
    By default, ZO methods are loaded at half-precision if the methods do not necessitate
    differentiation. However, as the current auto-differentiation module in PyTorch
    does not support FP16, methods like Forward-Grad must run at full precision. Similarly,
    for FO methods, half-precision is not compatible for the same reason.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 半精度训练（FP16）。大多数现代 LLM，例如 LLaMA2（Touvron 等，[2023](#bib.bib73)），在推理时以半精度（即 16
    位）运行，称为 FP16。使用 FP16，ZO 方法只需模型内存的一半。默认情况下，如果方法不需要微分，则 ZO 方法以半精度加载。然而，由于 PyTorch
    当前的自动微分模块不支持 FP16，因此像 Forward-Grad 这样的算法必须以全精度运行。同样，由于相同的原因，FO 方法也不兼容半精度。
- en: Mixed-precision training (MP). Mixed precision is a common practice to speed
    up gradient computation by back-propagation. By default, MP is used in FO methods
    (FO-SGD and FO-Adam), where the model is loaded in full precision and the training
    is carried out in half-precision. Specifically, a half-precision replica of the
    model is used to conduct a forward pass yielding the intermediate results and
    gradients in half-precision. From the half-precision one, a full-precision gradient
    will be recovered for optimization. If the intermediate results consume more memory
    consumption than the model and gradients themselves, MP can also reduce memory
    complexity. We remark that MP is still operating on a full-precision model, which
    means MP and FP16 cannot be used together in PyTorch.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练（MP）。混合精度是一种通过反向传播加速梯度计算的常见做法。默认情况下，FO方法（FO-SGD和FO-Adam）中使用MP，其中模型以全精度加载，训练则以半精度进行。具体来说，使用半精度的模型副本进行前向传递，生成中间结果和半精度的梯度。从半精度结果中，将恢复全精度梯度进行优化。如果中间结果消耗的内存超过模型和梯度本身，MP还可以降低内存复杂度。我们需要指出，MP仍在全精度模型上操作，这意味着MP和FP16不能在PyTorch中一起使用。
- en: The ‘foreach’ implementation of Adam. The PyTorch implementation of Adam will
    use so-called foreach implementation to speed up the computation. At a high level,
    the foreach implementation will replicate and merge all the layers’ weight into
    one tensor during Adam updates. Operations on the unified tensor can be easily
    parallelized and therefore are faster. Though foreach can speed up computation,
    it demands extra memory to store all the weights.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Adam的‘foreach’实现。PyTorch对Adam的实现将使用所谓的foreach实现来加速计算。总体而言，foreach实现将在Adam更新期间将所有层的权重复制并合并到一个张量中。对统一张量的操作可以轻松并行化，因此更快。尽管foreach可以加速计算，但它需要额外的内存来存储所有权重。
- en: C.2 Unpacking Memory Costs of Different Optimizers
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 不同优化器的内存成本分析
- en: 'In this section, we look into the optimizers studied in this work. For each
    algorithm, we will first analyze their theoretical memory consumption following
    the step $1\sim 3$ depicted in the framework in [Algorithm A1](#alg1 "In C How
    to Implement Memory-Efficient ZO/FO Optimizers? ‣ Revisiting Zeroth-Order Optimization
    for Memory-Efficient LLM Fine-Tuning: A Benchmark"). In the end, we provide a
    comparison between the theoretical and empirical results.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入研究本研究中研究的优化器。对于每种算法，我们将首先分析它们的理论内存消耗，遵循[算法 A1](#alg1 "如何在C中实现内存高效的ZO/FO优化器？‣
    重新审视零阶优化以实现内存高效的LLM微调：基准测试")中框架中描述的步骤 $1\sim 3$。最后，我们将提供理论结果与实证结果的比较。
- en: In most backward algorithms, activation ${\mathbf{a}}$ and ${\mathbf{x}}$. Next,
    we will focus on the memory dynamics in Step 1-3.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数反向算法中，激活 ${\mathbf{a}}$ 和 ${\mathbf{x}}$。接下来，我们将重点关注步骤1-3中的内存动态。
- en: FO-SGD (MP). In Step 1, the forward pass will first replicate the model in half-precision,
    i.e. $\bar{\mathbf{x}}$-bit gradient using activation per layer. The computation
    happens layer by layer. Therefore, the peak memory will be summed as $\sum\nolimits_{l}\max\{\frac{1}{2}|{\mathbf{a}}_{l}|,|{\mathbf{x}}_{l}|\}$.
    In Step 3, there will be no optimization state. We can approximate the dynamic
    memory as
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: FO-SGD（MP）。在步骤1中，前向传递将首先将模型复制为半精度，即使用每层的激活生成$\bar{\mathbf{x}}$位梯度。计算是逐层进行的。因此，峰值内存将被求和为$\sum\nolimits_{l}\max\{\frac{1}{2}\vert{\mathbf{a}}_{l}\vert,\vert{\mathbf{x}}_{l}\vert\}$。在步骤3中，将没有优化状态。我们可以将动态内存近似为
- en: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,\max\{\frac{1}{2}&#124;{\mathbf{a}}&#124;,&#124;{\mathbf{x}}&#124;\}\right\}$
    |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}\vert{\mathbf{a}}\vert+\frac{1}{2}\vert{\mathbf{x}}\vert,\max\{\frac{1}{2}\vert{\mathbf{a}}\vert,\vert{\mathbf{x}}\vert\}\right\}$
    |  |'
- en: '|  | $\displaystyle=\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,&#124;{\mathbf{x}}&#124;\right\}.$
    |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\max\left\{\frac{1}{2}\vert{\mathbf{a}}\vert+\frac{1}{2}\vert{\mathbf{x}}\vert,\vert{\mathbf{x}}\vert\right\}.$
    |  |'
- en: The total memory consumption will be approximated as
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 总内存消耗将被近似为
- en: '|  | $\displaystyle\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{3}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}.$
    |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max\left\{\frac{1}{2}\vert{\mathbf{a}}\vert+\frac{3}{2}\vert{\mathbf{x}}\vert,2\vert{\mathbf{x}}\vert\right\}.$
    |  |'
- en: 'FO-Adam (MP). In Step 1 and 2, FO-Adam completes the forward pass in the same
    way as FO-SGD and thus consumes memory similarly. In Step 3, the PyTorch Adam
    utilizes the ‘foreach’ implementation to speed up state computation. The Adam
    optimizer will first replicate the gradient in an extra memory of size $|{\mathbf{x}}|$.
    Therefore, the peak memory will be the maximum of the dynamic memory in all of
    the three steps:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: FO-Adam（MP）。在步骤 1 和 2 中，FO-Adam 以与 FO-SGD 相同的方式完成前向传播，因此内存消耗类似。在步骤 3 中，PyTorch
    的 Adam 使用 ‘foreach’ 实现来加速状态计算。Adam 优化器将首先在大小为 $|{\mathbf{x}}|$ 的额外内存中复制梯度。因此，峰值内存将是三步中动态内存的最大值：
- en: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,\max\{\frac{1}{2}&#124;{\mathbf{a}}&#124;,&#124;{\mathbf{x}}&#124;\},2&#124;{\mathbf{x}}&#124;\right\}$
    |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\quad\max\left\{\frac{1}{2}\lvert{\mathbf{a}}\rvert+\frac{1}{2}\lvert{\mathbf{x}}\rvert,\max\{\frac{1}{2}\lvert{\mathbf{a}}\rvert,\lvert{\mathbf{x}}\rvert\},2\lvert{\mathbf{x}}\rvert\right\}$
    |  |'
- en: '|  | $\displaystyle=\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}.\vspace*{-3mm}$
    |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\max\left\{\frac{1}{2}\lvert{\mathbf{a}}\rvert+\frac{1}{2}\lvert{\mathbf{x}}\rvert,2\lvert{\mathbf{x}}\rvert\right\}.\vspace*{-3mm}$
    |  |'
- en: In addition to the dynamic memory, the optimization step adds two states in
    memory for first-order and second-order momentum vectors, i.e., ${\mathbf{m}}_{t}$.
    We can sum up the memory of all steps to estimate the total memory as
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 除了动态内存，优化步骤还会在内存中添加两个状态，用于一阶和二阶动量向量，即 ${\mathbf{m}}_{t}$。我们可以将所有步骤的内存加总起来，估算总内存为：
- en: '|  | $\displaystyle\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{7}{2}&#124;{\mathbf{x}}&#124;,5&#124;{\mathbf{x}}&#124;\right\}.\vspace*{-3mm}$
    |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max\left\{\frac{1}{2}\lvert{\mathbf{a}}\rvert+\frac{7}{2}\lvert{\mathbf{x}}\rvert,5\lvert{\mathbf{x}}\rvert\right\}.\vspace*{-3mm}$
    |  |'
- en: ZO-SGD (FP16). In Step 1-2, ZO-SGD estimates the gradient by two forward loss
    calculations based on the random direction vector ${\mathbf{u}}$ to reproduce
    the random direction vectors in the same order. Specifically, we initialize a
    random number generator by $\texttt{rng}=\text{RandomState}(\mathcal{S})$ by
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD（FP16）。在步骤 1-2 中，ZO-SGD 通过基于随机方向向量 ${\mathbf{u}}$ 的两个前向损失计算来估算梯度，以重现随机方向向量的相同顺序。具体而言，我们通过
    $\texttt{rng}=\text{RandomState}(\mathcal{S})$ 初始化一个随机数生成器。
- en: '|  | $\displaystyle{\mathbf{u}}_{l}\sim\mathcal{N}_{\text{rng}}(\mathbf{0},\mathbf{I}),$
    |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathbf{u}}_{l}\sim\mathcal{N}_{\text{rng}}(\mathbf{0},\mathbf{I}),$
    |  |'
- en: where $\mathcal{N}_{\text{rng}}$. Without optimizer states, ZO-SGD consume a
    total memory as
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{N}_{\text{rng}}$。没有优化器状态时，ZO-SGD 的总内存消耗为：
- en: '|  | $\displaystyle\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{u}}_{l}&#124;=\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;,$
    |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{1}{2}\lvert{\mathbf{x}}\rvert+\max\nolimits_{l}\lvert{\mathbf{u}}_{l}\rvert=\frac{1}{2}\lvert{\mathbf{x}}\rvert+\max\nolimits_{l}\lvert{\mathbf{x}}_{l}\rvert,$
    |  |'
- en: where the second term is the maximal dynamic memory with the random seed trick.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第二项是使用随机种子技巧获得的最大动态内存。
- en: ZO-SGD-MMT (FP16). ZO-SGD with Momentum is similar to ZO-SGD, but it consumes
    extra memory for momentum storage. The momentum shares the same size as the model
    parameter. According to the memory of ZO-SGD, we can get the memory estimation
    as
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-SGD-MMT（FP16）。带动量的 ZO-SGD 类似于 ZO-SGD，但它需要额外的内存来存储动量。动量的大小与模型参数相同。根据 ZO-SGD
    的内存，我们可以得到如下的内存估算：
- en: '|  | $\displaystyle&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;.\vspace*{-5mm}$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\lvert{\mathbf{x}}\rvert+\max\nolimits_{l}\lvert{\mathbf{x}}_{l}\rvert.\vspace*{-5mm}$
    |  |'
- en: ZO-Adam (FP16). Similar to ZO-SGD-MMT, ZO-Adam has extra optimizer states, first-order
    and second-order momentum. They have the same memory consumption as the parameters.
    Therefore, the total estimated memory is
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ZO-Adam（FP16）。与 ZO-SGD-MMT 类似，ZO-Adam 也有额外的优化器状态，包括一阶和二阶动量。它们的内存消耗与参数相同。因此，总估算内存为：
- en: '|  | $\displaystyle\frac{3}{2}&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{x}}_{l}&#124;.\vspace*{-5mm}$
    |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{3}{2}\lvert{\mathbf{x}}\rvert+\max\nolimits_{l}\lvert{\mathbf{x}}_{l}\rvert.\vspace*{-5mm}$
    |  |'
- en: Forward-Grad. In Step 1 and 2, Forward-Grad estimates the gradient in one unified
    pass. Different from other ZO methods, Forward-Grad leverages the forward-mode
    auto-differentiation module¹¹1[https://pytorch.org/tutorials/intermediate/forward_ad_usage.html](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)
    in PyTorch to estimate the directional derivative, i.e., $\nabla f({\mathbf{x}})^{\top}{\mathbf{u}}$.
    In Step 3, Forward-Grad directly uses the gradient to update the parameters without
    extra memory costs. Thus, with the model memory, the total memory estimation can
    be summed as
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Forward-Grad。在步骤1和2中，Forward-Grad通过一次统一的传递来估计梯度。不同于其他零阶方法，Forward-Grad利用PyTorch中的前向模式自动微分模块¹¹1[https://pytorch.org/tutorials/intermediate/forward_ad_usage.html](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)来估计方向导数，即$\nabla
    f({\mathbf{x}})^{\top}{\mathbf{u}}$。在步骤3中，Forward-Grad直接使用梯度更新参数而不增加额外的内存开销。因此，结合模型内存，总内存估算可以总结为
- en: '|  | $\displaystyle 2&#124;{\mathbf{x}}&#124;+\max\nolimits_{l}&#124;{\mathbf{a}}_{l}&#124;.\vspace*{-4mm}$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 2|{\mathbf{x}}|+\max\nolimits_{l}|{\mathbf{a}}_{l}|.\vspace*{-4mm}$
    |  |'
- en: 'Table A1: Comparison of total memory complexity of different optimizers when
    fine-tuning the full model. $|{\mathbf{x}}|$ represents the parameter and intermediate
    memory of a specific layer $l$.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表A1：不同优化器在微调完整模型时的总内存复杂度比较。$|{\mathbf{x}}|$代表特定层$l$的参数和中间内存。
- en: '| Optimizer | Memory |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | 内存 |'
- en: '| FO-SGD | $&#124;{\mathbf{x}}&#124;+\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,&#124;{\mathbf{x}}&#124;\right\}$
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| FO-SGD | $|{\mathbf{x}}|+\max\left\{\frac{1}{2}|{\mathbf{a}}|+\frac{1}{2}|{\mathbf{x}}|,|{\mathbf{x}}|\right\}$
    |'
- en: '| FO-Adam | $3&#124;{\mathbf{x}}&#124;+\max\left\{\frac{1}{2}&#124;{\mathbf{a}}&#124;+\frac{1}{2}&#124;{\mathbf{x}}&#124;,2&#124;{\mathbf{x}}&#124;\right\}$
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| FO-Adam | $3|{\mathbf{x}}|+\max\left\{\frac{1}{2}|{\mathbf{a}}|+\frac{1}{2}|{\mathbf{x}}|,2|{\mathbf{x}}|\right\}$
    |'
- en: '| Forward-Grad | $&#124;{\mathbf{x}}&#124;+&#124;{\mathbf{x}}&#124;+\max_{l}&#124;{\mathbf{a}}_{l}&#124;$
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Forward-Grad | $|{\mathbf{x}}|+|{\mathbf{x}}|+\max_{l}|{\mathbf{a}}_{l}|$
    |'
- en: '| ZO-SGD | $\frac{1}{2}&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD | $\frac{1}{2}|{\mathbf{x}}|+\max_{l}\frac{1}{2}|{\mathbf{x}}_{l}|$
    |'
- en: '| ZO-SGD-MMT | $&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| ZO-SGD-MMT | $|{\mathbf{x}}|+\max_{l}\frac{1}{2}|{\mathbf{x}}_{l}|$ |'
- en: '| ZO-Adam | $\frac{3}{2}&#124;{\mathbf{x}}&#124;+\max_{l}\frac{1}{2}&#124;{\mathbf{x}}_{l}&#124;$
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| ZO-Adam | $\frac{3}{2}|{\mathbf{x}}|+\max_{l}\frac{1}{2}|{\mathbf{x}}_{l}|$
    |'
- en: C.3 Theoretical Memory Cost Comparison
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 理论内存成本比较
- en: 'In this section, we provide some analyisis based on the theoretical results
    discussed above. In particular, we first summarize the theoretical memory efficiency
    of different optimizers in Tab. [A1](#S3.T1a "Table A1 ‣ C.2 Unpacking Memory
    Costs of Different Optimizers ‣ C How to Implement Memory-Efficient ZO/FO Optimizers?
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark"). Several key insights can be summarized. First, compared to FO optimizers
    (including Forward-Grad), the memory efficiency merits of ZO optimizers are mainly
    in two aspects. On the one hand, ZO can avoid saving the intermediate results
    (model states) $\mathbf{a}_{l}$). In the meantime, Forward-Grad is not compatible
    with the modern efficiency enhancing tricks (such as FP16 or MP), which further
    reduces its memory efficiency. Last, although ZO-Adam is memory intensive compared
    to ZO-SGD, it can be greatly improved by using FP16, which shows a better efficiency
    than FO-SGD (MP).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们基于上述理论结果提供了一些分析。特别是，我们首先总结了不同优化器在表[A1](#S3.T1a "Table A1 ‣ C.2 Unpacking
    Memory Costs of Different Optimizers ‣ C How to Implement Memory-Efficient ZO/FO
    Optimizers? ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning:
    A Benchmark")中的理论内存效率。可以总结出几个关键见解。首先，与FO优化器（包括Forward-Grad）相比，ZO优化器的内存效率优势主要体现在两个方面。一方面，ZO可以避免保存中间结果（模型状态）$\mathbf{a}_{l}$。与此同时，Forward-Grad不兼容现代效率提升技巧（如FP16或MP），进一步降低了其内存效率。最后，尽管与ZO-SGD相比，ZO-Adam的内存消耗较大，但通过使用FP16可以大幅提高其效率，表现出比FO-SGD（MP）更好的效率。'
- en: 'Theoretical analysis on the memory consumption change with sequence length.
    We remark that the empirical results ($|{\mathbf{a}}|$, it exhibits a much better
    efficiency advantage over FO-SGD, when the sequence length increases (e.g., $1504$).
    Therefore, the memory efficiency gap shown in Tab. [4](#S4.T4 "Table 4 ‣ 4.2 Experiment
    Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning ‣ Revisiting Zeroth-Order
    Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark") will be further
    enlarged if a larger input sequence length is used.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '对于序列长度变化下内存消耗的理论分析。我们注意到，当序列长度增加（例如，$1504$）时，实证结果（$|{\mathbf{a}}|$）表明，它相较于FO-SGD展示了更好的效率优势。因此，如果使用更长的输入序列长度，表格[4](#S4.T4
    "Table 4 ‣ 4.2 Experiment Results ‣ 4 Benchmarking ZO Optimization for LLM Fine-Tuning
    ‣ Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A
    Benchmark")中显示的内存效率差距将进一步扩大。'
