- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:38:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:38:17'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态指令调优的大型语言模型具备精细的视觉感知能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02969](https://ar5iv.labs.arxiv.org/html/2403.02969)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02969](https://ar5iv.labs.arxiv.org/html/2403.02969)
- en: Junwen He Work done during internship at DAMO Academy. Dalian University of
    Technology DAMO Academy, Alibaba Group Yifan Wang Dalian University of Technology
    Lijun Wang Corresponding author Dalian University of Technology Huchuan Lu Dalian
    University of Technology Jun-Yan He DAMO Academy, Alibaba Group Jin-Peng Lan DAMO
    Academy, Alibaba Group Bin Luo DAMO Academy, Alibaba Group Xuansong Xie DAMO Academy,
    Alibaba Group
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Junwen He 实习期间在DAMO学院完成的工作。 大连理工大学 DAMO学院，阿里巴巴集团 Yifan Wang 大连理工大学 Lijun Wang
    通讯作者 大连理工大学 Huchuan Lu 大连理工大学 Jun-Yan He DAMO学院，阿里巴巴集团 Jin-Peng Lan DAMO学院，阿里巴巴集团
    Bin Luo DAMO学院，阿里巴巴集团 Xuansong Xie DAMO学院，阿里巴巴集团
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Multimodal Large Language Model (MLLMs) leverages Large Language Models as a
    cognitive framework for diverse visual-language tasks. Recent efforts have been
    made to equip MLLMs with visual perceiving and grounding capabilities. However,
    there still remains a gap in providing fine-grained pixel-level perceptions and
    extending interactions beyond text-specific inputs. In this work, we propose AnyRef,
    a general MLLM model that can generate pixel-wise object perceptions and natural
    language descriptions from multi-modality references, such as texts, boxes, images,
    or audio. This innovation empowers users with greater flexibility to engage with
    the model beyond textual and regional prompts, without modality-specific designs.
    Through our proposed refocusing mechanism, the generated grounding output is guided
    to better focus on the referenced object, implicitly incorporating additional
    pixel-level supervision. This simple modification utilizes attention scores generated
    during the inference of LLM, eliminating the need for extra computations while
    exhibiting performance enhancements in both grounding masks and referring expressions.
    With only publicly available training data, our model achieves state-of-the-art
    results across multiple benchmarks, including diverse modality referring segmentation
    and region-level referring expression generation. Code and models are available
    at [https://github.com/jwh97nn/AnyRef](https://github.com/jwh97nn/AnyRef)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs）利用大型语言模型作为多样化视觉-语言任务的认知框架。近期的努力致力于为MLLMs配备视觉感知和定位能力。然而，仍存在提供精细像素级感知和扩展超越文本特定输入的交互的差距。在本研究中，我们提出了AnyRef，一种通用的MLLM模型，它可以从多模态参考（如文本、框、图像或音频）中生成像素级物体感知和自然语言描述。这一创新使用户在模型交互中获得更大的灵活性，无需特定模态的设计。通过我们提出的重新聚焦机制，生成的定位输出被引导更好地聚焦于参考物体，隐含地结合了额外的像素级监督。这一简单的修改利用了在LLM推理过程中生成的注意力分数，消除了额外计算的需要，同时在定位掩模和参考表达上表现出性能提升。仅使用公开可用的训练数据，我们的模型在多个基准测试中实现了最先进的结果，包括多样化的模态参考分割和区域级参考表达生成。代码和模型可在
    [https://github.com/jwh97nn/AnyRef](https://github.com/jwh97nn/AnyRef) 上获取
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have garnered widespread influence across various
    domains, and advancements have been achieved by augmenting LLMs with visual perception
    modules to bridge the gap between vision and language tasks [[6](#bib.bib6), [23](#bib.bib23),
    [18](#bib.bib18), [61](#bib.bib61)], thereby transforming them into Multimodal
    Large Language Models (MLLMs). Most recent research aims to further endow MLLMs
    with finer-grained visual understanding abilities, like visual grounding and referring
    expression generation, through user-defined formats (*e.g.*, coordinates, bounding
    boxes, etc.) [[31](#bib.bib31), [4](#bib.bib4), [57](#bib.bib57)], surpassing
    the confines of textual responses alone.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各个领域产生了广泛的影响，通过将视觉感知模块增强LLMs，以弥合视觉与语言任务之间的差距 [[6](#bib.bib6), [23](#bib.bib23),
    [18](#bib.bib18), [61](#bib.bib61)]，从而将其转变为多模态大型语言模型（MLLMs）。最新的研究旨在进一步赋予MLLMs更精细的视觉理解能力，例如视觉定位和参考表达生成，通过用户定义的格式（*例如*，坐标、边界框等）
    [[31](#bib.bib31), [4](#bib.bib4), [57](#bib.bib57)]，超越仅限于文本响应的范围。
- en: '![Refer to caption](img/190fdbdb5f4685d51e3fe657e2459075.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/190fdbdb5f4685d51e3fe657e2459075.png)'
- en: 'Figure 1: Multi-modality Referring Segmentation and Expression Generation with
    AnyRef. Our model possesses the capacity to generate natural language descriptions
    as well as pixel-wise grounding masks for the referred object. It accommodates
    various referring modalities such as text, bounding boxes, images and audio, enabling
    more flexible user interactions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：AnyRef的多模态引用分割和表达生成。我们的模型具备生成自然语言描述和针对引用对象的像素级定位掩模的能力。它支持多种引用模态，如文本、边界框、图像和音频，从而实现更灵活的用户交互。
- en: Despite the encouraging results demonstrated by existing MLLMs in grounding
    linguistic expressions to visual scenes, their capacity for precise localization
    remains restricted to coarse-grained levels (bounding boxes), falling short of
    pixel-level perceptions (As illustrated in [Tab. 1](#S1.T1 "In 1 Introduction
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception")). The
    most recent work, as exemplified by [[16](#bib.bib16)], has focused on enhancing
    MLLMs by integrating segmentation models that generate binary segmentation masks
    based on textual descriptions. However, this approach is constrained by its reliance
    solely on textual referring instructions, thereby limiting the versatility of
    MLLMs in various multimodal interaction scenarios, such as region-based referring
    or audio comprehension tasks. The interactive segmentation model SEEM [[63](#bib.bib63)]
    attempts to receive audio inputs, but it turns audio into textural prompts with
    the off-the-shelf speech recognition model Whisper [[34](#bib.bib34)], so essentially
    it is still the textual references.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的多模态大语言模型在将语言表达与视觉场景对接方面展示了令人鼓舞的结果，但它们在精确定位方面的能力仍然局限于粗粒度的级别（边界框），无法达到像素级的感知（如[表
    1](#S1.T1 "在 1 引言 ‣ 多模态指令调整 LLMs 具有细粒度视觉感知")所示）。最新的研究，如[[16](#bib.bib16)]，集中在通过整合生成基于文本描述的二进制分割掩模的分割模型来增强多模态大语言模型。然而，这种方法仅依赖文本引用指令，因此限制了多模态大语言模型在各种多模态交互场景中的多样性，如基于区域的引用或音频理解任务。交互式分割模型SEEM
    [[63](#bib.bib63)]尝试接收音频输入，但它将音频转换为文本提示，使用现成的语音识别模型Whisper [[34](#bib.bib34)]，所以本质上仍然是文本引用。
- en: '| Method | Image | Referring Format | Pixel-level Grounding | End-to-End Model
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 图像 | 引用格式 | 像素级定位 | 端到端模型 |'
- en: '| Region | Image* | Audio |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 图像* | 音频 |'
- en: '| LLaVA (NeurIPS-23) [[23](#bib.bib23)] | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA (NeurIPS-23) [[23](#bib.bib23)] | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ |'
- en: '| BuboGPT (arXiv-23) [[58](#bib.bib58)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| BuboGPT (arXiv-23) [[58](#bib.bib58)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✗ |'
- en: '| Vision-LLM (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Vision-LLM (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| DetGPT (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| DetGPT (arXiv-23) [[41](#bib.bib41)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| KOSMOS-2 (arXiv-23) [[31](#bib.bib31)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| KOSMOS-2 (arXiv-23) [[31](#bib.bib31)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| Shikra (arXiv-23) [[4](#bib.bib4)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Shikra (arXiv-23) [[4](#bib.bib4)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| GPT4RoI (arXiv-23) [[57](#bib.bib57)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| GPT4RoI (arXiv-23) [[57](#bib.bib57)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| NExT-GPT (arXiv-23) [[44](#bib.bib44)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| NExT-GPT (arXiv-23) [[44](#bib.bib44)] | ✓ | ✗ | ✗ | ✓ | ✗ | ✓ |'
- en: '| ASM (arXiv-23) [[42](#bib.bib42)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ASM (arXiv-23) [[42](#bib.bib42)] | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |'
- en: '| LISA (arXiv-23) [[16](#bib.bib16)] | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| LISA (arXiv-23) [[16](#bib.bib16)] | ✓ | ✗ | ✗ | ✗ | ✓ | ✓ |'
- en: '| AnyRef (Ours) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef (我们) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 'Table 1: Comparisons of recent Multi-modal Large Language Models. The term
    *Referring Format* emphasizes the acceptable modalities used for referencing,
    whereas *Image** indicates visual references derived from another image.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：近期多模态大语言模型的比较。术语*引用格式*强调用于引用的可接受模态，而*图像*表示来源于另一图像的视觉引用。
- en: In light of the above observation, we propose AnyRef, a novel multi-modal instruction-tuned
    LLM with fine-grained visual perception. As shown in [Tab. 1](#S1.T1 "In 1 Introduction
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"), AnyRef
    advances existing MLLMs with the strong capability to perform pixel-level object
    grounding and generate region-aware expressions derived from references of diverse
    modalities, including text, bounding boxes, images, and audio inputs, (See [Fig. 1](#S1.F1
    "In 1 Introduction ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual
    Perception") as an example). To this end, we first propose a unified representation
    for referring across different modalities and map them to the token space of LLMs.
    We extract features from all the modalities mentioned above to form the *Unified
    Referring Representation*, which can be processed uniformly by the LLM, utilizing
    its ability of understanding and reasoning in generating the grounded output.
    This enables flexible referring beyond textual descriptions, without requiring
    modality-specific designs or changes to the existing model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述观察，我们提出了AnyRef，这是一种具有细粒度视觉感知的新型多模态指令调优LLM。如[Tab. 1](#S1.T1 "In 1 Introduction
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception")所示，AnyRef通过强大的像素级物体对齐能力和生成基于多种模态参考的区域感知表达，推动了现有MLLMs的发展，包括文本、边界框、图像和音频输入（参见[Fig.
    1](#S1.F1 "In 1 Introduction ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception")作为示例）。为此，我们首先提出了一种统一的表示方法，用于跨不同模态进行引用，并将其映射到LLMs的令牌空间。我们从上述所有模态中提取特征，形成*统一引用表示*，LLM可以统一处理这些特征，利用其理解和推理能力生成对齐输出。这使得超越文本描述的灵活引用成为可能，而不需要特定于模态的设计或对现有模型进行更改。
- en: To perform pixel-level grounding with LLMs, a possible solution [[16](#bib.bib16)]
    is to trigger the segmentation action by generating a special token , whose
    embedding will be subsequently employed as the input to the segmentation model.
    As opposed to using coordinates sequence of polygons [[41](#bib.bib41), [5](#bib.bib5)]
    to represent segmentation results, the introduction of the  token effectively
    simplifies pixel-level visual grounding. Nevertheless, the embedding of the 
    token is confined in a fixed feature space, due to the nature of next token prediction,
    leading to limited representational capacity and thus inaccurate segmentation
    results. To address this constraint, we propose a simple yet effective *refocusing
    mechanism*, which takes into account the correlation between the grounded expression
    and the  token. This mechanism utilizes attention scores to weight such correlation,
    enhancing the mask embedding with additional grounded embeddings, and since the
    attention scores are intermediate outputs of the self-attention layers, the additional
    computation introduced by the refocusing mechanism is minimal. Furthermore, the
    refocusing mechanism also provides a short-cut connection between the generated
    grounded expression and the segmentation results, allowing pixel-level labels
    to implicitly supervise the learning process of language expression generation,
    thereby enhancing the model’s regional understanding capability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在像素级别上进行对齐，使用LLMs的一个可能解决方案是通过生成一个特殊的令牌来触发分割操作，其嵌入随后将作为分割模型的输入。与使用多边形的坐标序列来表示分割结果相比，引入令牌有效地简化了像素级别的视觉对齐。然而，由于下一个令牌预测的特性，令牌的嵌入被限制在一个固定的特征空间中，从而导致表示能力有限，结果分割不准确。为了应对这一限制，我们提出了一种简单但有效的*重新聚焦机制*，该机制考虑了对齐表达和令牌之间的相关性。该机制利用注意力分数来加权这种相关性，通过额外的对齐嵌入增强掩码嵌入，因为注意力分数是自注意力层的中间输出，重新聚焦机制引入的额外计算是最小的。此外，重新聚焦机制还提供了生成的对齐表达与分割结果之间的捷径连接，允许像素级标签隐式监督语言表达生成的学习过程，从而增强模型的区域理解能力。
- en: 'To summarize, our contributions are threefold:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，我们的贡献有三方面：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce AnyRef, the first general MLLM capable of producing pixel-level
    object perceptions as well as region-aware referring descriptions. It adeptly
    accommodates multi-modality references including texts, bounding boxes, images
    or audio in a general manner, fostering more flexible interactions for users.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了AnyRef，它是首个能够生成像素级别物体感知和区域感知描述的一般MLLM。它灵活地适应多模态参考，包括文本、边界框、图像或音频，促进用户更灵活的交互。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a simple yet effective *refocusing mechanism* to enhance the grounded
    mask predictions, leveraging the correlations of generated tokens without incurring
    additional computational overhead, and concurrently yields improvements in regional
    expression referring.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种简单但有效的*重新聚焦机制*，以增强有根据的掩膜预测，利用生成的标记之间的相关性，而无需额外的计算开销，并同时改进区域表达的参考。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Thorough experiments conducted on multiple datasets demonstrate the efficacy
    of the proposed method, resulting in state-of-the-art performance across a diverse
    range of multi-modality tasks.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多个数据集上进行的全面实验表明，所提出的方法有效，导致在各种多模态任务中取得了最先进的性能。
- en: Our model is built upon LLaVA-7B [[23](#bib.bib23)], which can be efficiently
    fine-tuned with 8 NVIDIA 32G V100 GPUs, making our method easily reproducible
    at a reasonable computational cost.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型建立在LLaVA-7B [[23](#bib.bib23)]的基础上，该模型可以通过8台NVIDIA 32G V100 GPU高效地微调，使我们的方法在合理的计算成本下易于复现。
- en: 2 Related Works
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Multi-modal Large Language Model
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多模态大语言模型
- en: Multi-modal Large Language Models (MLLMs), built upon large language models
    (LLMs) as their foundations, extend their capabilities beyond traditional textual
    understanding to incorporate various modalities such as images, videos, and audio.
    Building upon the concept of instruction tuning, Flamingo [[1](#bib.bib1)] utilizes
    visual feature inputs as prompts, resulting in impressive performance across diverse
    visual-language tasks such as image captioning and visual question answering (VQA).
    Subsequent models, includin BLIP-2 [[19](#bib.bib19)], LLaVA [[23](#bib.bib23)],
    InstructBLIP [[6](#bib.bib6)], Otter [[18](#bib.bib18)] and LLaMa-Adapter [[56](#bib.bib56)],
    utilize additional generated visual instruction-following data for better visual-language
    alignment, and demonstrate impressive multi-modal chat abilities.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大语言模型（MLLMs），以大型语言模型（LLMs）为基础，扩展了它们的能力，不仅限于传统的文本理解，还结合了图像、视频和音频等各种模态。在指令调优的概念基础上，Flamingo
    [[1](#bib.bib1)]利用视觉特征输入作为提示，在多种视觉语言任务中取得了令人印象深刻的表现，如图像描述和视觉问答（VQA）。随后的模型，包括BLIP-2
    [[19](#bib.bib19)]、LLaVA [[23](#bib.bib23)]、InstructBLIP [[6](#bib.bib6)]、Otter
    [[18](#bib.bib18)]和LLaMa-Adapter [[56](#bib.bib56)]，利用额外生成的视觉指令跟随数据以获得更好的视觉语言对齐，并展示了令人印象深刻的多模态对话能力。
- en: Recent studies expand the capabilities of MLLMs to address localization tasks
    with region-aware functionalities. KOSMOS-2 [[31](#bib.bib31)] and VisionLLM [[41](#bib.bib41)]
    introduce additional location tokens to the vocabulary, enabling the conversion
    of coordinates into textual representations. These representations are then inputted
    into LLMs to enhance region understanding. On the other hand, Shikra [[4](#bib.bib4)]
    represents coordinates directly in natural language form. In contrast, GPT4RoI
    [[57](#bib.bib57)] streamlines the process by employing RoI-aligned visual features
    without incorporating explicit positional information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究扩展了MLLMs的能力，以应对具有区域感知功能的定位任务。KOSMOS-2 [[31](#bib.bib31)]和VisionLLM [[41](#bib.bib41)]向词汇表中引入了额外的位置标记，从而将坐标转换为文本表示。这些表示随后被输入到LLMs中以增强区域理解。另一方面，Shikra
    [[4](#bib.bib4)]直接以自然语言形式表示坐标。相比之下，GPT4RoI [[57](#bib.bib57)]通过采用RoI对齐的视觉特征简化了这一过程，而无需引入显式的位置坐标信息。
- en: Nevertheless, these models lack the capacity to produce fine-grained perceptions
    (*e.g.*, pixel-level masks), and restrict their referring expressions to textural
    descriptions and regions within the image. Our model, leveraging the best of both
    worlds, not only generates pixel-level grounding masks, but also accommodates
    a broader range of referring formats (*e.g.*, visual reference from other images
    or audio) in a unified manner.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这些模型缺乏生成细粒度感知（*例如*，像素级掩膜）的能力，并且将其参考表达限制在图像中的文本描述和区域内。我们的模型利用了两者的优势，不仅生成像素级的定位掩膜，还以统一的方式支持更广泛的参考格式（*例如*，来自其他图像或音频的视觉参考）。
- en: 2.2 Referring Segmentation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 参考分割
- en: Referring Expression Segmentation translates explicit textual descriptions into
    corresponding pixel-level segmentations, requiring a comprehensive understanding
    of both visual content and linguistic expression. Recent methods including SAM
    [[15](#bib.bib15)], X-Decoder [[62](#bib.bib62)] and SEEM [[63](#bib.bib63)] unify
    multiple segmentation tasks within a single model, supporting various human interaction
    methods. While LISA [[16](#bib.bib16)] utilizes the powerful reasoning and comprehension
    abilities of LLMs to process textural instructions and generate masks through
    the SAM [[15](#bib.bib15)] decoder.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 指代表达分割将明确的文本描述转换为对应的像素级分割，需要对视觉内容和语言表达有全面的理解。最近的方法包括 SAM [[15](#bib.bib15)],
    X-Decoder [[62](#bib.bib62)] 和 SEEM [[63](#bib.bib63)]，在单一模型中统一了多种分割任务，支持各种人机交互方式。LISA
    [[16](#bib.bib16)] 则利用 LLM 的强大推理和理解能力，通过 SAM [[15](#bib.bib15)] 解码器处理文本指令并生成掩膜。
- en: Visual Referring Segmentation can be related to one/few-shot segmentation, where
    an example of a certain object with its corresponding mask is provided to segment
    the same object in the query image [[30](#bib.bib30), [12](#bib.bib12), [55](#bib.bib55),
    [43](#bib.bib43), [44](#bib.bib44)]. Recently, CLIPSeg [[28](#bib.bib28)] builds
    upon the CLIP model to treat the example image as a visual prompt, which can generalize
    to novel forms of prompts. Painter [[43](#bib.bib43)] and SegGPT [[44](#bib.bib44)]
    utilize in-context learning to perform general vision tasks using input task prompts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉指代分割可以与一/少-shot 分割相关，其中提供了某一物体及其对应掩膜的示例，用于分割查询图像中的相同物体 [[30](#bib.bib30),
    [12](#bib.bib12), [55](#bib.bib55), [43](#bib.bib43), [44](#bib.bib44)]。最近，CLIPSeg
    [[28](#bib.bib28)] 在 CLIP 模型的基础上，将示例图像作为视觉提示，这可以推广到新的提示形式。Painter [[43](#bib.bib43)]
    和 SegGPT [[44](#bib.bib44)] 利用上下文学习通过输入任务提示来执行通用视觉任务。
- en: Audio-Visual Segmentation aims to generate pixel-level masks for object(s) emitting
    sound, initially introduced in [[60](#bib.bib60)]. AVSegFormer [[8](#bib.bib8)]
    innovatively incorporates learnable audio queries, enabling selective attention
    to relevant visual features. Additionally, AUSS [[21](#bib.bib21)] proposes unmixing
    self-supervised losses to bridge the gap between audio signals and visual semantics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 音频-视觉分割旨在为发出声音的物体生成像素级掩膜，最初在[[60](#bib.bib60)]中提出。AVSegFormer [[8](#bib.bib8)]
    创新地结合了可学习的音频查询，使得可以选择性地关注相关的视觉特征。此外，AUSS [[21](#bib.bib21)] 提出了将自监督损失分解的方法，以弥合音频信号和视觉语义之间的差距。
- en: While these models have achieved satisfactory results in their respective domains,
    there is currently a gap in addressing all referring tasks within a single model.
    Most of the aforementioned methods rely on modality-specific or task-specific
    designs, which may not generalize well beyond their intended tasks. Our approach
    leverages the robust comprehension ability of LLMs to concurrently tackle all
    these tasks while preserving the region-level reasoning capacity. Additionally,
    the *refocusing mechanism* aids in enhancing region-level referring expression
    through implicit pixel-level supervisions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型在各自领域取得了令人满意的结果，但目前还存在一个空白，即无法在单一模型中处理所有指代任务。上述大多数方法依赖于特定模态或特定任务的设计，这可能无法很好地推广到其预期任务之外。我们的方法利用
    LLM 的强大理解能力同时解决所有这些任务，同时保留区域级推理能力。此外，*重新聚焦机制*通过隐式像素级监督来增强区域级指代表达。
- en: 3 Methods
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: The overall framework of AnyRef comprises a vision encoder, multi-modal feature
    projection layers, a LLM, and a mask decoder, as illustrated in [Fig. 2](#S3.F2
    "In 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception").
    These initial three components together form a multi-modality LLM, enabling support
    for various reference formats and generating region-aware grounded textual responses.
    Additionally, a distinctive  token is introduced to the vocabulary, which
    provides the input for the mask decoder through a refocusing mechanism, facilitating
    the generation of pixel-level perceptions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AnyRef 的整体框架包括一个视觉编码器、多模态特征投影层、一个 LLM 和一个掩膜解码器，如 [图 2](#S3.F2 "在 3 方法 ‣ 多模态指令调整
    LLMs 与细粒度视觉感知") 所示。这三个初始组件共同形成一个多模态 LLM，支持各种引用格式，并生成区域感知的文本响应。此外，引入了一个独特的 
    标记到词汇表中，通过重新聚焦机制为掩膜解码器提供输入，便于生成像素级感知。
- en: '![Refer to caption](img/f5542b0b33d1e5090525be307985c957.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5542b0b33d1e5090525be307985c957.png)'
- en: 'Figure 2: Overall pipeline of AnyRef. Vision-language, audio-language projection
    and MLP layers are omitted for simplicity and clarity. The Unified Referring Representation
    ([Sec. 3.1.1](#S3.SS1.SSS1 "3.1.1 Unified Referring Representation ‣ 3.1 Model
    Architecture ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception")) receives references from diverse types of modalities and
    transforms them into embeddings aligned with the LLM. The Refocusing Mechanism
    ([Sec. 3.1.2](#S3.SS1.SSS2 "3.1.2 Refocusing Mechanism ‣ 3.1 Model Architecture
    ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"))
    enhances the embedding from the single  token with grounded textural embeddings,
    thus providing a broader representational capacity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：AnyRef的整体流程。为了简洁明了，视觉-语言、音频-语言投影和MLP层被省略。统一的引用表示（[Sec. 3.1.1](#S3.SS1.SSS1
    "3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods ‣
    Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception")）接收来自不同类型的模态的引用，并将其转换为与LLM对齐的嵌入。重新聚焦机制（[Sec. 3.1.2](#S3.SS1.SSS2
    "3.1.2 Refocusing Mechanism ‣ 3.1 Model Architecture ‣ 3 Methods ‣ Multi-modal
    Instruction Tuned LLMs with Fine-grained Visual Perception")）通过从单一的标记中增强带有实质文本嵌入的嵌入，从而提供更广泛的表示能力。
- en: 3.1 Model Architecture
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 模型架构
- en: We adopt the pretrained ViT-L/14 from CLIP [[33](#bib.bib33)] as the vision
    encoder, and LLaMA-7B [[39](#bib.bib39)] as our LLM. For audio inputs, we choose
    the pretrained audio encoder from ImageBind [[9](#bib.bib9)] to extract audio
    features. To connect multi-modality information beyond texts to the existing LLM,
    such as images and audio, we adopt vision-language and audio-language projection
    layers to project image and audio features to the language space. The input image
    is converted into a fixed number of $16\times 16$ patch embeddings. Both the image
    and audio embeddings are then projected to the same dimension as word embeddings.
    The LLM takes the interleaved embeddings in the same way as language tokens to
    generate outputs via an auto-regressive manner.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用CLIP中的预训练ViT-L/14 [[33](#bib.bib33)]作为视觉编码器，LLaMA-7B [[39](#bib.bib39)]作为我们的LLM。对于音频输入，我们选择ImageBind中的预训练音频编码器
    [[9](#bib.bib9)] 来提取音频特征。为了将文本以外的多模态信息（如图像和音频）连接到现有LLM，我们采用视觉-语言和音频-语言投影层将图像和音频特征投影到语言空间。输入图像被转换为固定数量的$16\times
    16$补丁嵌入。然后，图像和音频嵌入被投影到与词嵌入相同的维度。LLM以与语言标记相同的方式处理交错嵌入，以自动回归的方式生成输出。
- en: 3.1.1 Unified Referring Representation
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 统一引用表示
- en: To receive multi-modality referring prompts beyond texts, we convert them into
    fixed-sized tokens and *quote* them between newly introduced special tokens.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了接收文本以外的多模态引用提示，我们将其转换为固定大小的标记并*引用*它们，放在新引入的特殊标记之间。
- en: 'For visual prompts including regional bounding boxes or visual examples from
    another image, we introduce  and , where visual features will
    be inserted in between. Drawing inspiration from [[57](#bib.bib57)], we represent
    bounding boxes using extracted region-level features from RoIAlign [[11](#bib.bib11)]
    with a fixed size of $4\times 4$ as well. To refer to them in the same way as
    textual descriptions, we build prompts such as: “Can you provide a description
    of  in this image?”, where  will be replaced
    by the extracted visual features.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包括区域边界框或来自另一图像的视觉示例的视觉提示，我们引入和，其中视觉特征将被插入其中。从[[57](#bib.bib57)]汲取灵感，我们使用从RoIAlign
    [[11](#bib.bib11)]中提取的区域级特征表示边界框，固定大小为$4\times 4$。为了以与文本描述相同的方式引用它们，我们构建提示，例如：“你能提供关于图像中的描述吗？”，其中将由提取的视觉特征替换。
- en: 'For audio prompts, we introduce  and  for LLM to be aware
    of audio referring inputs, and the extracted audio features will be projected
    through audio-language projection layer and then inserted in between. And the
    audio prompted instruction will be built like: “Can you segment the object that
    makes sound of  in this image?”. In this way, the
    referring representation from different modalities is unified, which can be treated
    the same way as language instructions and easily handled by the LLM.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频提示，我们引入和，使LLM能够识别音频引用输入，提取的音频特征将通过音频-语言投影层进行投影，然后插入其中。音频提示指令将构建为：“你能分割出在此图像中发出声音的对象吗？”通过这种方式，不同模态的引用表示被统一，可以像语言指令一样处理，LLM可以轻松处理。
- en: '![Refer to caption](img/03d25812dd92c4d1aebbbc1f142fb0a9.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03d25812dd92c4d1aebbbc1f142fb0a9.png)'
- en: 'Figure 3: Qualitative results of AnyRef’s applicable capabilities on multiple
    tasks, including (a) referring expression segmentation, (b) region-level captioning
    and grounding, (c) image-level referring segmentation and (d) audio-visual segmentation.
    AnyRef demonstrates proficiency in generating both textual responses and pixel-level
    perceptions across diverse modality instructions.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：AnyRef在多个任务上的定性结果，包括（a）引用表达分割，（b）区域级别的描述和定位，（c）图像级别的引用分割，以及（d）视听分割。AnyRef展示了在生成文本响应和像素级感知方面的高效能力，适用于多种模态指令。
- en: 3.1.2 Refocusing Mechanism
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 重新聚焦机制
- en: Inspired by [[16](#bib.bib16)], we employ another special token  to succinctly
    represent the instance segmentation mask as an embedding. This embedding $\boldsymbol{h}_{obj}$
    can be expressed mathematically as,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 受到[[16](#bib.bib16)]的启发，我们使用了另一个特殊的标记，以简洁的方式将实例分割掩码表示为嵌入。这个嵌入$\boldsymbol{h}_{obj}$可以用数学公式表示为，
- en: '|  | $M=\mathcal{S}\Big{(}\gamma(\boldsymbol{h}_{obj}),\mathcal{V}_{seg}(\boldsymbol{x}_{img})\Big{)},$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $M=\mathcal{S}\Big{(}\gamma(\boldsymbol{h}_{obj}),\mathcal{V}_{seg}(\boldsymbol{x}_{img})\Big{)},$
    |  | (1) |'
- en: where $\boldsymbol{x}_{img}$ denotes the vision encoder of the segmentation
    model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\boldsymbol{x}_{img}$表示分割模型的视觉编码器。
- en: However, since  is a token in the LLM vocabulary, its representation will
    be limited in a fixed feature range, which will potentially limit its representational
    capacity and influence the decoded mask quality. Therefore, we propose a *refocusing
    mechanism* which augments the original mask embedding with grounded text embeddings.
    The motivation behind is to explicitly force the final mask embedding to focus
    more on the referring or grounded object with its textural expression. The updated
    mask embedding can be formulated as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于是LLM词汇表中的一个标记，它的表示将在固定的特征范围内，这可能会限制其表示能力并影响解码掩码的质量。因此，我们提出了一种*重新聚焦机制*，该机制通过增强的文本嵌入来扩展原始掩码嵌入。其动机在于明确迫使最终的掩码嵌入更加关注带有其纹理表达的引用或定位对象。更新后的掩码嵌入可以表示为
- en: '|  | $\hat{\boldsymbol{h}}_{obj}=\boldsymbol{h}_{obj}+\lambda_{f}\sum_{i}^{i token can receive additional supervisory signals from
    pixel-level ground truth. This mutual interaction can further benefit the vision-language
    understanding ability of AnyRef, given the interrelated nature of referring expressions
    and grounding masks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*重新聚焦机制*，在标记之前生成的标记可以从像素级真实值中获得额外的监督信号。这种相互作用可以进一步提高AnyRef的视觉语言理解能力，考虑到引用表达和定位掩码的相互关系。
- en: 3.2 Implementation Details.
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实现细节。
- en: 3.2.1 Training Setup
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 训练设置
- en: Unless otherwise specified, we employ the pre-trained CLIP ViT-L/14 as the vision
    encoder, ImageBind-H [[9](#bib.bib9)] as the audio encoder, and LLaMa-7B as the
    LLM. The vision-language projection layer is initialized from LLaVa [[23](#bib.bib23)],
    while the audio-language projection layer is randomly initialized. The word embeddings
    of newly introduced special tokens are initialized randomly. Furthermore, the
    segmentation model utilizes the pre-trained SAM-H [[15](#bib.bib15)]. The image
    resolution is $224\times 224$ by rescaling and padding for the segmentation model.
    For audio inputs, we follow settings in [[60](#bib.bib60)] to use the 5-second
    audio clips and convert to 3 fixed-sized embeddings after padding, since the ImageBind
    [[9](#bib.bib9)] audio encoder samples 2-second audio each time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，我们使用预训练的 CLIP ViT-L/14 作为视觉编码器，ImageBind-H [[9](#bib.bib9)] 作为音频编码器，以及
    LLaMa-7B 作为 LLM。视觉-语言投影层从 LLaVa [[23](#bib.bib23)] 初始化，而音频-语言投影层随机初始化。新增特殊标记的词嵌入随机初始化。此外，分割模型使用预训练的
    SAM-H [[15](#bib.bib15)]。图像分辨率为 $224\times 224$，通过重新缩放和填充适配分割模型。对于音频输入，我们遵循 [[60](#bib.bib60)]
    中的设置，使用 5 秒的音频片段，并在填充后转换为 3 个固定大小的嵌入，因为 ImageBind [[9](#bib.bib9)] 音频编码器每次采样 2
    秒的音频。
- en: To ensure training efficiency and preserve generalization ability, we freeze
    the vision encoders and audio encoder. Fine-tuning of the LLM is conducted using
    LoRA [[13](#bib.bib13)], and the trainable parameters comprise the mask decoder
    and projection layers, accounting for approximately 7% of the total parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保训练效率和保持泛化能力，我们冻结了视觉编码器和音频编码器。通过 LoRA [[13](#bib.bib13)] 对 LLM 进行微调，训练参数包括掩码解码器和投影层，占总参数的约
    7%。
- en: We conduct training using 8 NVIDIA V100 GPUs, each with a batch size of 6, and
    employ a gradient accumulation step set to 8. The training utilizes mixed precision,
    converting both the vision and audio encoder to float16 precision. AdamW [[26](#bib.bib26)]
    optimizer with a learning rate of 5e-5 and weight decay of 0.01 is employed, alongside
    a cosine annealing scheduler incorporating 200 warmup steps. LoRA operates with
    the rank of 8 and alpha of 16, exclusively applied to query and value projections
    within the LLM. We employ ZeRO stage-2 [[35](#bib.bib35)] with DeepSpeed [[37](#bib.bib37)]
    which completes network training in 10K steps.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 8 台 NVIDIA V100 GPU 进行训练，每台 GPU 批量大小为 6，并设置梯度累积步数为 8。训练采用混合精度，将视觉和音频编码器都转换为
    float16 精度。使用 AdamW [[26](#bib.bib26)] 优化器，学习率为 5e-5，权重衰减为 0.01，并采用包括 200 个预热步骤的余弦退火调度器。LoRA
    的秩为 8，alpha 为 16，仅应用于 LLM 的查询和价值投影。我们使用 ZeRO stage-2 [[35](#bib.bib35)] 和 DeepSpeed
    [[37](#bib.bib37)] 完成网络训练，共 10K 步。
- en: '| Method |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
- en: '|  | val | testA | testB |  | val | testA | testB |  | val(U) | test(U) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | val | testA | testB |  | val | testA | testB |  | val(U) | test(U) |'
- en: '| *Specialist Segmentation Models* |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| *专业分割模型* |'
- en: '| CRIS [[45](#bib.bib45)] |  | 70.5 | 73.2 | 66.1 |  | 65.3 | 68.1 | 53.7 |  |
    59.9 | 60.4 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CRIS [[45](#bib.bib45)] |  | 70.5 | 73.2 | 66.1 |  | 65.3 | 68.1 | 53.7 |  |
    59.9 | 60.4 |'
- en: '| LAVT [[49](#bib.bib49)] |  | 72.7 | 75.8 | 68.8 |  | 62.1 | 68.4 | 55.1 |  |
    61.2 | 62.1 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LAVT [[49](#bib.bib49)] |  | 72.7 | 75.8 | 68.8 |  | 62.1 | 68.4 | 55.1 |  |
    61.2 | 62.1 |'
- en: '| GRES [[22](#bib.bib22)] |  | 73.8 | 76.5 | 70.2 |  | 66.0 | 71.0 | 57.7 |  |
    65.0 | 66.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GRES [[22](#bib.bib22)] |  | 73.8 | 76.5 | 70.2 |  | 66.0 | 71.0 | 57.7 |  |
    65.0 | 66.0 |'
- en: '| PolyFormer [[25](#bib.bib25)] |  | 76.0 | 78.3 | 73.3 |  | 69.3 | 74.6 |
    61.9 |  | 69.2 | 70.2 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| PolyFormer [[25](#bib.bib25)] |  | 76.0 | 78.3 | 73.3 |  | 69.3 | 74.6 |
    61.9 |  | 69.2 | 70.2 |'
- en: '| UNINEXT [[48](#bib.bib48)] |  | 82.2 | 83.4 | 81.3 |  | 72.5 | 76.4 | 66.2
    |  | 74.7 | 76.4 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| UNINEXT [[48](#bib.bib48)] |  | 82.2 | 83.4 | 81.3 |  | 72.5 | 76.4 | 66.2
    |  | 74.7 | 76.4 |'
- en: '| SEEM [[63](#bib.bib63)] |  | - | - | - |  | - | - | - |  | 65.7 | - |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SEEM [[63](#bib.bib63)] |  | - | - | - |  | - | - | - |  | 65.7 | - |'
- en: '| *Generalist MLLMs* |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| *通用 MLLMs* |'
- en: '| X-Decoder [[62](#bib.bib62)] |  | - | - | - |  | - | - | - |  | 64.6 | -
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| X-Decoder [[62](#bib.bib62)] |  | - | - | - |  | - | - | - |  | 64.6 | -
    |'
- en: '| LISA-7B [[16](#bib.bib16)] |  | 74.1 | 76.5 | 71.1 |  | 62.4 | 67.4 | 56.5
    |  | 66.4 | 68.4 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LISA-7B [[16](#bib.bib16)] |  | 74.1 | 76.5 | 71.1 |  | 62.4 | 67.4 | 56.5
    |  | 66.4 | 68.4 |'
- en: '| LISA-7B (ft) [[16](#bib.bib16)] |  | 74.9 | 79.1 | 72.3 |  | 65.1 | 70.8
    | 58.1 |  | 67.9 | 70.6 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LISA-7B (ft) [[16](#bib.bib16)] |  | 74.9 | 79.1 | 72.3 |  | 65.1 | 70.8
    | 58.1 |  | 67.9 | 70.6 |'
- en: '| AnyRef |  | 74.1 | 75.5 | 70.8 |  | 64.1 | 68.7 | 57.5 |  | 68.1 | 69.9 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef |  | 74.1 | 75.5 | 70.8 |  | 64.1 | 68.7 | 57.5 |  | 68.1 | 69.9 |'
- en: '| AnyRef (ft) |  | 76.9 | 79.9 | 74.2 |  | 70.3 | 73.5 | 61.8 |  | 70.0 | 70.7
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef (ft) |  | 76.9 | 79.9 | 74.2 |  | 70.3 | 73.5 | 61.8 |  | 70.0 | 70.7
    |'
- en: 'Table 2: Referring expression segmentation results (cIOU) on RefCOCO(+/g) datasets.
    (ft) denotes finetuning the model on RefCOCO(+/g) datasets. Our model surpasses
    all generalist models and most specialist (segmentation-oriented) models.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：RefCOCO(+/g) 数据集上的参照表达分割结果（cIOU）。(ft)表示在RefCOCO(+/g)数据集上对模型进行微调。我们的模型超越了所有通用型模型和大多数专家型（以分割为导向）模型。
- en: 3.2.2 Datasets
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 数据集
- en: The training process involves a diverse range of datasets. For general semantic
    and instance segmentation, COCO-Stuff [[3](#bib.bib3)], ADE20K [[59](#bib.bib59)],
    and PACO-LVIS [[36](#bib.bib36)] are utilized, with one category chosen per batch.
    Referring expression segmentation incorporates RefClef, RefCOCO, RefCOCO+ [[14](#bib.bib14)],
    RefCOCOg [[52](#bib.bib52)], and PhraseCut [[46](#bib.bib46)]. Image-level referring
    segmentation adopts the method outlined in [[27](#bib.bib27)], where samples are
    chosen from COCO [[20](#bib.bib20)], PascalVOC [[7](#bib.bib7)], and PhraseCut
    [[46](#bib.bib46)] datasets. Random cropped samples are drawn from images that
    contain the same category as their corresponding linguistic expressions. Region-level
    captioning involves RefCOCO(+/g) and Flickr30K Entities [[32](#bib.bib32)]. Audio-visual
    segmentation employs AVSBench [[60](#bib.bib60)] with both single and multiple
    sound sources. To prevent data leakage, samples with images in the validation
    or test splits are excluded.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程涉及各种数据集。对于通用语义和实例分割，使用COCO-Stuff [[3](#bib.bib3)]、ADE20K [[59](#bib.bib59)]
    和 PACO-LVIS [[36](#bib.bib36)]，每批次选择一个类别。参照表达分割包括RefClef、RefCOCO、RefCOCO+ [[14](#bib.bib14)]、RefCOCOg
    [[52](#bib.bib52)] 和 PhraseCut [[46](#bib.bib46)]。图像级参照分割采用[[27](#bib.bib27)]中概述的方法，其中样本从COCO
    [[20](#bib.bib20)]、PascalVOC [[7](#bib.bib7)] 和 PhraseCut [[46](#bib.bib46)] 数据集中选择。随机裁剪样本从包含与其对应语言表达相同类别的图像中提取。区域级标注涉及RefCOCO(+/g)和Flickr30K
    Entities [[32](#bib.bib32)]。音频-视觉分割采用AVSBench [[60](#bib.bib60)]，包括单一和多重声音源。为了防止数据泄漏，排除了包含在验证或测试集中的图像的样本。
- en: 4 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We assess the capabilities of our model through evaluations on various benchmarks,
    including different modality referring segmentation (text/image/audio) for pixel-level
    perception and referring expression generation for regional understanding. Models
    are categorized as *specialists* or *generalists*, with the former designed exclusively
    for specific tasks. We provide examples for each task in [Fig. 3](#S3.F3 "In 3.1.1
    Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods ‣ Multi-modal
    Instruction Tuned LLMs with Fine-grained Visual Perception"), and more illustrations
    can be found in the supplementary material.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在各种基准测试上进行评估来评估我们模型的能力，包括不同模态的参照分割（文本/图像/音频）用于像素级感知，以及参照表达生成用于区域理解。模型被分类为*专家型*或*通用型*，前者专门设计用于特定任务。我们在[图
    3](#S3.F3 "In 3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture
    ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception")中提供了每个任务的示例，更多说明可以在补充材料中找到。
- en: 4.1 Multi-modality Referring Segmentation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 多模态参照分割
- en: 4.1.1 Referring Expression Segmentation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 参照表达分割
- en: 'The task involves labeling pixels within an image corresponding to an object
    instance referred to by a linguistic expression. We instruct our model as: “Can
    you segment {exp} in this image?”, where {exp} is the given explicit description.
    Evaluation is conducted using Cumulative-IoU (cIoU) as the metric. We make comparisons
    with state-of-the-art models on validation and test sets of RefCOCO, RefCOCO+
    and RefCOCOg [[14](#bib.bib14), [52](#bib.bib52)]. As shown in [Tab. 2](#S3.T2
    "In 3.2.1 Training Setup ‣ 3.2 Implementation Details. ‣ 3 Methods ‣ Multi-modal
    Instruction Tuned LLMs with Fine-grained Visual Perception"), our performance
    surpasses all generalist models and most specialist models except UNINEXT-H [[48](#bib.bib48)],
    which is trained using a considerably larger dataset that includes video samples.
    Specialist models excel solely at segmentation-related tasks, while generalist
    models possess additional capabilities for generating textural descriptions and
    are capable of handling more complex references.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 任务涉及在图像中标记与语言表达对应的对象实例的像素。我们向模型指令为：“你能在这张图像中分割{exp}吗？”，其中{exp}是给定的明确描述。评估使用累积交并比（cIoU）作为指标。我们与最先进的模型在RefCOCO、RefCOCO+和RefCOCOg的验证和测试集上进行比较
    [[14](#bib.bib14), [52](#bib.bib52)]。如[表2](#S3.T2 "在3.2.1训练设置 ‣ 3.2 实现细节 ‣ 3 方法
    ‣ 多模态指令调优的LLMs与细粒度视觉感知")所示，我们的表现超越了所有通用模型和大多数专业模型，除了UNINEXT-H [[48](#bib.bib48)]，该模型使用了包含视频样本的大型数据集进行训练。专业模型仅在分割相关任务中表现出色，而通用模型则具备生成文本描述的附加能力，并能处理更复杂的引用。
- en: 4.1.2 Image Referring Segmentation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 图像引用分割
- en: Predicting masks using image examples is akin to one- or few-shot segmentation,
    where regions corresponding to the highlighted object in the example image must
    be located in a query image. We prompt our model with queries like “Can you find
    similar object of  in this image?”, where 
    denotes pooled features from example images as detailed in [Sec. 3.1.1](#S3.SS1.SSS1
    "3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods ‣
    Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception"). The
    evaluation takes place under the in-domain setting on $\text{COCO-20}^{i}$ [[7](#bib.bib7)]
    for a fair comparison, as most classes are encountered during the training stages.
    In the few-shot evaluation, the model inferences multiple times using different
    example images, with the averaged mask serving as the final prediction. In our
    referring examples, we do not have corresponding mask examples, which is different
    from the standard setting. we follow [[28](#bib.bib28)] to crop out the target
    object for highlighting, using their segmentation masks. As demonstrated in [Tab. 3](#S4.T3
    "In 4.1.2 Image Referring Segmentation ‣ 4.1 Multi-modality Referring Segmentation
    ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual
    Perception"), our model achieves competitive performance compared to state-of-the-art
    methods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像示例预测掩膜类似于一次性或少样本分割，其中需要在查询图像中找到与示例图像中突出对象对应的区域。我们提示模型使用诸如“你能在这张图像中找到类似于的对象吗？”的查询，其中表示从示例图像中提取的特征，如[第3.1.1节](#S3.SS1.SSS1
    "3.1.1 统一引用表示 ‣ 3.1 模型架构 ‣ 3 方法 ‣ 多模态指令调优的LLMs与细粒度视觉感知")中详细描述。评估在$\text{COCO-20}^{i}$
    [[7](#bib.bib7)]的领域内设置下进行，以确保公平比较，因为大多数类别在训练阶段出现。在少样本评估中，模型使用不同的示例图像进行多次推断，平均掩膜作为最终预测。在我们的引用示例中，我们没有对应的掩膜示例，这与标准设置不同。我们遵循[[28](#bib.bib28)]，使用他们的分割掩膜裁剪出目标对象进行突出显示。如[表3](#S4.T3
    "在4.1.2图像引用分割 ‣ 4.1 多模态引用分割 ‣ 4 实验 ‣ 多模态指令调优的LLMs与细粒度视觉感知")所示，我们的模型在与最先进的方法比较时表现出竞争力。
- en: '| Method | $\text{COCO-20}^{i}$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $\text{COCO-20}^{i}$ |'
- en: '| one-shot | few-shot | one-shot | few-shot |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 一次性 | 少样本 | 一次性 | 少样本 |'
- en: '| *Specialist Segmentation Models* |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| *专业分割模型* |'
- en: '| HSNet* [[30](#bib.bib30)] | 41.7 | 50.7 | 68.7 | 73.8 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| HSNet* [[30](#bib.bib30)] | 41.7 | 50.7 | 68.7 | 73.8 |'
- en: '| VAT* [[12](#bib.bib12)] | 42.9 | 49.4 | 72.4 | 76.3 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| VAT* [[12](#bib.bib12)] | 42.9 | 49.4 | 72.4 | 76.3 |'
- en: '| CLIPSeg [[28](#bib.bib28)] | 33.2 | - | 59.5 | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CLIPSeg [[28](#bib.bib28)] | 33.2 | - | 59.5 | - |'
- en: '| SegGPT [[44](#bib.bib44)] | 56.1 | 67.9 | 83.2 | 89.8 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SegGPT [[44](#bib.bib44)] | 56.1 | 67.9 | 83.2 | 89.8 |'
- en: '| *Generalist Multi-task Models* |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| *通用多任务模型* |'
- en: '| Painter [[43](#bib.bib43)] | 32.8 | 32.6 | 64.5 | 64.6 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Painter [[43](#bib.bib43)] | 32.8 | 32.6 | 64.5 | 64.6 |'
- en: '| AnyRef | 43.5 | 51.3 | 74.8 | 78.6 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef | 43.5 | 51.3 | 74.8 | 78.6 |'
- en: '| AnyRef$\dagger$ | 46.3 | 55.2 | 76.5 | 80.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef$\dagger$ | 46.3 | 55.2 | 76.5 | 80.0 |'
- en: 'Table 3: Quantitative results of example-based few-shot segmentation. * indicates
    that the categories in training cover that in testing as in [[44](#bib.bib44)],
    and $\dagger$ denotes using mask cropping setting.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：基于示例的少样本分割的定量结果。* 表示训练中的类别覆盖测试中的类别，如[[44](#bib.bib44)]所示，$\dagger$ 表示使用掩码裁剪设置。
- en: 4.1.3 Audio-Visual Segmentation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 音视频分割
- en: The AVS benchmark comprises single- and multi-sources subsets based on the number
    of sounding objects. We utilize prompts like, “Can you segment the object(s) that
    produce sound of  in this image?”, to instruct the
    model for mask predictions. Following [[60](#bib.bib60)], evaluation metrics include
    mean IoU (mIoU) for region similarity and F-score¹¹1$F_{\beta}=\frac{(1+\beta^{2})\times\text{precision}\times\text{recall}}{\beta^{2}\times\text{precision}+\text{recall}}$
    following [[60](#bib.bib60)] for contour accuracy. The quantitative results in
    [Tab. 4](#S4.T4 "In 4.1.3 Audio-Visual Segmentation ‣ 4.1 Multi-modality Referring
    Segmentation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception") demonstrate that our model consistently outperforms most methods
    on single-source split, indicating successful alignment of audio features with
    the LLM during fine-tuning. However, when confronted with audios containing multiple
    sound sources, our model encounters challenges in producing masks that cover more
    than one object. Moreover, owing to the ability of LLM, our model can determine
    the textural category of the sounding objects, as depicted in [Fig. 3](#S3.F3
    "In 3.1.1 Unified Referring Representation ‣ 3.1 Model Architecture ‣ 3 Methods
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception") (d).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AVS基准测试包含基于发声物体数量的单源和多源子集。我们使用诸如“您能在此图像中分割出发出声音的物体吗？”的提示来指导模型进行掩码预测。根据[[60](#bib.bib60)]，评估指标包括区域相似度的平均IoU（mIoU）和轮廓准确性的F-score¹¹1$F_{\beta}=\frac{(1+\beta^{2})\times\text{precision}\times\text{recall}}{\beta^{2}\times\text{precision}+\text{recall}}$。定量结果在[Tab. 4](#S4.T4
    "在4.1.3 音视频分割 ‣ 4.1 多模态参考分割 ‣ 4 实验 ‣ 多模态指令调整LLMs与细粒度视觉感知")中展示，我们的模型在单源拆分上始终优于大多数方法，表明在微调过程中成功对齐了音频特征和LLM。然而，当面对包含多个声音源的音频时，我们的模型在生成覆盖多个物体的掩码时遇到挑战。此外，由于LLM的能力，我们的模型能够确定发声物体的纹理类别，如[Fig. 3](#S3.F3
    "在3.1.1 统一参考表示 ‣ 3.1 模型架构 ‣ 3 方法 ‣ 多模态指令调整LLMs与细粒度视觉感知") (d)所示。
- en: '| Method | Single-source |  | Multi-source |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 单源 |  | 多源 |'
- en: '| mIOU | F-score |  | mIOU | F-score |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| mIOU | F-score |  | mIOU | F-score |'
- en: '| AVS [[60](#bib.bib60)] | 78.7 | 0.879 |  | 54.0 | 0.645 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AVS [[60](#bib.bib60)] | 78.7 | 0.879 |  | 54.0 | 0.645 |'
- en: '| BG [[10](#bib.bib10)] | 81.7 | 0.904 |  | 55.1 | 0.668 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| BG [[10](#bib.bib10)] | 81.7 | 0.904 |  | 55.1 | 0.668 |'
- en: '| AVSegformer [[8](#bib.bib8)] | 82.1 | 0.899 |  | 58.4 | 0.693 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AVSegformer [[8](#bib.bib8)] | 82.1 | 0.899 |  | 58.4 | 0.693 |'
- en: '| AUSS [[21](#bib.bib21)] | 89.4 | 0.942 |  | 63.5 | 0.752 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| AUSS [[21](#bib.bib21)] | 89.4 | 0.942 |  | 63.5 | 0.752 |'
- en: '| AnyRef | 82.8 | 0.908 |  | 55.6 | 0.663 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef | 82.8 | 0.908 |  | 55.6 | 0.663 |'
- en: 'Table 4: Quantitative results of audio-visual segmentation.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：音视频分割的定量结果。
- en: 4.2 Referring Expression Generation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 参考表达生成
- en: This task involves generating a textual description associated with an object
    based on its location (bounding box). We evaluate our generated expressions using
    automatic caption generation metrics, including CIDEr [[40](#bib.bib40)] and Meteor
    [[17](#bib.bib17)], on RefCOCO, RefCOCO+ and RefCOCOg. Our model achieves remarkable
    performance among generalist LLM-based models and demonstrates competitive result
    to specialist models, as shown in [Tab. 5](#S4.T5 "In 4.2 Referring Expression
    Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with Fine-grained
    Visual Perception").
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务涉及根据物体的位置（边界框）生成与之相关的文本描述。我们使用自动生成标题的评估指标，包括CIDEr [[40](#bib.bib40)] 和 Meteor
    [[17](#bib.bib17)]，在RefCOCO、RefCOCO+ 和 RefCOCOg上评估我们生成的表达式。我们的模型在通用LLM模型中表现出色，并且与专家模型相比表现竞争力，如[Tab. 5](#S4.T5
    "在4.2 参考表达生成 ‣ 4 实验 ‣ 多模态指令调整LLMs与细粒度视觉感知")中所示。
- en: '![Refer to caption](img/409c1bfbf46a6b85572de2a398fafdff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/409c1bfbf46a6b85572de2a398fafdff.png)'
- en: 'Figure 4: Comparison of generated expressions between ground-truth and LLM-based
    methods.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：生成表达式与真实标注和基于LLM的方法之间的比较。
- en: Nonetheless, as stated in [[54](#bib.bib54), [24](#bib.bib24), [2](#bib.bib2)],
    standard automated evaluation metrics do not authentically capture generation
    quality due to the constraints of ground-truth expressions. This scenario is particularly
    pronounced in open-text generation, especially for LLM-based models. These models
    have the ability to generate rich, natural sentences, while the provided ground-truth
    expressions often tend to be concise, as indicated in [Fig. 4](#S4.F4 "In 4.2
    Referring Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned
    LLMs with Fine-grained Visual Perception").
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，正如[[54](#bib.bib54), [24](#bib.bib24), [2](#bib.bib2)]所述，由于实际表达的约束，标准的自动化评估指标未能真实捕捉生成质量。这种情况在开放文本生成中尤为明显，特别是对于基于LLM的模型。这些模型具有生成丰富自然句子的能力，而提供的实际表达往往较为简洁，如[图4](#S4.F4
    "在4.2 参照表达生成 ‣ 4 实验 ‣ 具备细粒度视觉感知的多模态指令调整LLMs")所示。
- en: To further evaluate the quality of the generated expressions, we conduct human
    evaluations following [[51](#bib.bib51), [54](#bib.bib54), [2](#bib.bib2)]. We
    randomly select 100 images from the validation datasets and ask five human raters
    to choose the bounding box that best matches the generated expression, and the
    averaged score is considered the final result. In [Tab. 6](#S4.T6 "In 4.2 Referring
    Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction Tuned LLMs with
    Fine-grained Visual Perception"), we present the results of the human evaluations,
    including both traditional methods and LLM-based methods. The LLM-based methods
    produce more detailed descriptions, closely resembling human behavior, which are
    preferred by the human raters. We provide more examples in supplementary material.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步评估生成表达的质量，我们进行人类评估，参照[[51](#bib.bib51), [54](#bib.bib54), [2](#bib.bib2)]。我们随机选择了100张验证数据集中的图片，请五名人工评分员选择与生成表达最匹配的边界框，平均分数被视为最终结果。在[表6](#S4.T6
    "在4.2 参照表达生成 ‣ 4 实验 ‣ 具备细粒度视觉感知的多模态指令调整LLMs")中，我们展示了人类评估的结果，包括传统方法和基于LLM的方法。基于LLM的方法生成了更详细的描述，更接近人类行为，受到人工评分员的青睐。我们在补充材料中提供了更多示例。
- en: '| Method |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  | RefCOCO |  | RefCOCO+ |  | RefCOCOg |'
- en: '|  | testA | testB |  | testA | testB |  | val |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | testA | testB |  | testA | testB |  | val |'
- en: '|  | Meteor | CIDEr | Meteor | CIDEr |  | Meteor | CIDEr | Meteor | CIDEr |  |
    Meteor | CIDEr |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | Meteor | CIDEr | Meteor | CIDEr |  | Meteor | CIDEr | Meteor | CIDEr |  |
    Meteor | CIDEr |'
- en: '| *Specialist Models* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| *专业模型* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Visdif [[53](#bib.bib53)] |  | 18.5 | - | 24.7 | - |  | 14.2 | - | 13.5 |
    - |  | 14.5 | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Visdif [[53](#bib.bib53)] |  | 18.5 | - | 24.7 | - |  | 14.2 | - | 13.5 |
    - |  | 14.5 | - |'
- en: '| SLR [[54](#bib.bib54)] |  | 29.6 | 77.5 | 34.0 | 132.0 |  | 21.3 | 52.0 |
    21.5 | 73.5 |  | 15.9 | 66.2 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SLR [[54](#bib.bib54)] |  | 29.6 | 77.5 | 34.0 | 132.0 |  | 21.3 | 52.0 |
    21.5 | 73.5 |  | 15.9 | 66.2 |'
- en: '| easyREG [[38](#bib.bib38)] |  | 31.3 | 83.7 | 34.1 | 132.9 |  | 24.2 | 66.4
    | 22.8 | 78.7 |  | 17.0 | 77.7 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| easyREG [[38](#bib.bib38)] |  | 31.3 | 83.7 | 34.1 | 132.9 |  | 24.2 | 66.4
    | 22.8 | 78.7 |  | 17.0 | 77.7 |'
- en: '| IREG [[50](#bib.bib50)] |  | 34.9 | 105.4 | 37.3 | 154.1 |  | 30.8 | 89.8
    | 26.4 | 97.0 |  | 19.4 | 101.2 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| IREG [[50](#bib.bib50)] |  | 34.9 | 105.4 | 37.3 | 154.1 |  | 30.8 | 89.8
    | 26.4 | 97.0 |  | 19.4 | 101.2 |'
- en: '| *Generalist MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| *通用MLLMs* |  |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| GRIT [[47](#bib.bib47)] |  | - | - | - | - |  | - | - | - | - |  | 15.2 |
    71.6 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| GRIT [[47](#bib.bib47)] |  | - | - | - | - |  | - | - | - | - |  | 15.2 |
    71.6 |'
- en: '| KOSMOS-2 [[31](#bib.bib31)] |  | - | - | - | - |  | - | - | - | - |  | 14.1
    | 62.3 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| KOSMOS-2 [[31](#bib.bib31)] |  | - | - | - | - |  | - | - | - | - |  | 14.1
    | 62.3 |'
- en: '| AnyRef |  | 23.9 | 74.8 | 26.7 | 118.6 |  | 16.4 | 59.4 | 14.3 | 62.9 |  |
    16.2 | 69.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef |  | 23.9 | 74.8 | 26.7 | 118.6 |  | 16.4 | 59.4 | 14.3 | 62.9 |  |
    16.2 | 69.0 |'
- en: '| AnyRef (ft) |  | 30.4 | 79.5 | 32.7 | 138.6 |  | 23.2 | 67.7 | 20.1 | 80.1
    |  | 17.1 | 79.7 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef (ft) |  | 30.4 | 79.5 | 32.7 | 138.6 |  | 23.2 | 67.7 | 20.1 | 80.1
    |  | 17.1 | 79.7 |'
- en: 'Table 5: Quantitative results on region-level referring expression generation.
    *Generalist models* (LLM-based) perform poorly on automated evaluation metrics
    due to the limitation of constrained ground-truth expressions, as stated in [Sec. 4.2](#S4.SS2
    "4.2 Referring Expression Generation ‣ 4 Experiments ‣ Multi-modal Instruction
    Tuned LLMs with Fine-grained Visual Perception").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：区域级参照表达生成的定量结果。*通用模型*（基于LLM）由于受限于实际表达的限制，在自动化评估指标上表现不佳，如[第4.2节](#S4.SS2
    "4.2 参照表达生成 ‣ 4 实验 ‣ 具备细粒度视觉感知的多模态指令调整LLMs")所述。
- en: '| Method |  | RefCOCO |  | RefCOCO+ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  | RefCOCO |  | RefCOCO+ |'
- en: '|  | testA | testB |  | testA | testB |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | testA | testB |  | testA | testB |'
- en: '| SLR [[54](#bib.bib54)] |  | 66% | 62% |  | 43% | 38% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| SLR [[54](#bib.bib54)] |  | 66% | 62% |  | 43% | 38% |'
- en: '| SLR+Rerank [[54](#bib.bib54)] |  | 73% | 77% |  | 49% | 46% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| SLR+Rerank [[54](#bib.bib54)] |  | 73% | 77% |  | 49% | 46% |'
- en: '| KOSMOS-2 [[31](#bib.bib31)] |  | 88% | 84% |  | 63% | 65% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| KOSMOS-2 [[31](#bib.bib31)] |  | 88% | 84% |  | 63% | 65% |'
- en: '| Shikra [[4](#bib.bib4)] |  | 91% | 81% |  | 59% | 62% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Shikra [[4](#bib.bib4)] |  | 91% | 81% |  | 59% | 62% |'
- en: '| AnyRef |  | 87% | 80% |  | 67% | 66% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| AnyRef |  | 87% | 80% |  | 67% | 66% |'
- en: 'Table 6: Human evaluation on referring expression generation.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 人类评价指代表达生成。'
- en: 4.3 Ablation Study
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: '![Refer to caption](img/e1d85971c1fd210961eeb9d2f04d7172.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e1d85971c1fd210961eeb9d2f04d7172.png)'
- en: 'Figure 5: Visualization of mask embeddings before and after the *refocusing
    mechanism*. original denotes original mask embeddings, while vehicle, person,
    and animal represent the updated mask embeddings corresponding to their respective
    referring objects contained in the textural expression.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 重新聚焦机制前后掩码嵌入的可视化。原始表示原始掩码嵌入，而车辆、人物和动物则表示对应于文本表达中各自指代对象的更新掩码嵌入。'
- en: We conduct extensive ablation studies to reveal the contribution of each component.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了广泛的消融研究，以揭示每个组件的贡献。
- en: '| $\lambda_{f}$ | RefCOCOg |  | AVSBench |  | RefCOCOg |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{f}$ | RefCOCOg |  | AVSBench |  | RefCOCOg |'
- en: '| cIOU |  | mIOU |  | Meteor | CIDEr |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| cIOU |  | mIOU |  | Meteor | CIDEr |'
- en: '| 0.0 | 68.7 |  | 81.4 |  | 16.8 | 71.1 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 0.0 | 68.7 |  | 81.4 |  | 16.8 | 71.1 |'
- en: '| 1.0 | 67.1 |  | 80.6 |  | 14.3 | 68.5 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 67.1 |  | 80.6 |  | 14.3 | 68.5 |'
- en: '| 0.1 | 70.0 |  | 82.8 |  | 17.1 | 73.7 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 70.0 |  | 82.8 |  | 17.1 | 73.7 |'
- en: '| 1.0$\dagger$ | 68.0 |  | 81.1 |  | 15.7 | 70.0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 1.0$\dagger$ | 68.0 |  | 81.1 |  | 15.7 | 70.0 |'
- en: '| 0.1$\dagger$ | 69.3 |  | 82.0 |  | 17.0 | 73.8 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 0.1$\dagger$ | 69.3 |  | 82.0 |  | 17.0 | 73.8 |'
- en: 'Table 7: Ablation study on refocusing weight $\lambda_{f}$.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 重新聚焦权重$\lambda_{f}$的消融研究。'
- en: Refocusing Mechanism. We first investigate the effectiveness of enhancing the
     token through *refocusing mechanism*, and explore the impact of different
    refocusing weights $\lambda_{f}$ for our experiments.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重新聚焦机制。我们首先探讨通过*重新聚焦机制*增强标记的效果，并探索不同重新聚焦权重$\lambda_{f}$对我们实验的影响。
- en: We further employ PCA to visualize the mask embeddings before and after implementing
    the *refocusing mechanism* in [Fig. 5](#S4.F5 "In 4.3 Ablation Study ‣ 4 Experiments
    ‣ Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception") We
    choose three subsets representing different referring objects including vehicles,
    persons and animals (*e.g.*, the person subset comprises output expressions containing
    “person,” “man,” “woman,” etc.). The visualization illustrates that the *refocusing
    mechanism* results in a wider representation range of the mask embedding. Moreover,
    the updated embeddings demonstrate a clustering pattern aligned with the associated
    textual expressions, contributing to a more precise decoding of masks.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步使用PCA可视化实施*重新聚焦机制*前后的掩码嵌入，在[图 5](#S4.F5 "在4.3 消融研究 ‣ 4 实验 ‣ 多模态指令调整的LLMs与细粒度视觉感知")中。我们选择了三个子集，代表不同的指代对象，包括车辆、人物和动物（*例如*，人物子集包含了包含“person”，“man”，“woman”等的输出表达）。可视化结果表明，*重新聚焦机制*使掩码嵌入的表示范围更广。此外，更新后的嵌入展示了与相关文本表达一致的聚类模式，有助于更精确地解码掩码。
- en: Training Datasets. The impact of different types of datasets is validated in
    [Tab. 8](#S4.T8 "In 4.3 Ablation Study ‣ 4 Experiments ‣ Multi-modal Instruction
    Tuned LLMs with Fine-grained Visual Perception"), and evaluation is carried out
    on RefCOCOg validation split. Region/Image Ref. refers to region-level and image-level
    referring data, as explained in [Sec. 3.2.2](#S3.SS2.SSS2 "3.2.2 Datasets ‣ 3.2
    Implementation Details. ‣ 3 Methods ‣ Multi-modal Instruction Tuned LLMs with
    Fine-grained Visual Perception"). It becomes apparent that the model’s generalization
    improves as the type of datasets increases.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集。不同类型的数据集的影响在[表 8](#S4.T8 "在4.3 消融研究 ‣ 4 实验 ‣ 多模态指令调整的LLMs与细粒度视觉感知")中得到了验证，并在RefCOCOg验证分割上进行了评估。区域/图像指代指的是区域级和图像级指代数据，如[第3.2.2节](#S3.SS2.SSS2
    "3.2.2 数据集 ‣ 3.2 实施细节。 ‣ 3 方法 ‣ 多模态指令调整的LLMs与细粒度视觉感知")中所述。显而易见，随着数据集类型的增加，模型的泛化能力得到了提升。
- en: '| Exp. | Referring | General | Region Ref. | Image Ref. | cIOU |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | 指代 | 通用 | 区域指代 | 图像指代 | cIOU |'
- en: '| 1 | ✓ |  |  |  | 66.2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 1 | ✓ |  |  |  | 66.2 |'
- en: '| 2 | ✓ | ✓ |  |  | 67.0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | ✓ | ✓ |  |  | 67.0 |'
- en: '| 3 | ✓ | ✓ | ✓ |  | 67.7 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 3 | ✓ | ✓ | ✓ |  | 67.7 |'
- en: '| 4 | ✓ | ✓ |  | ✓ | 67.4 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 4 | ✓ | ✓ |  | ✓ | 67.4 |'
- en: '| 5 | ✓ | ✓ | ✓ | ✓ | 68.1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ✓ | ✓ | ✓ | ✓ | 68.1 |'
- en: 'Table 8: Ablation study on training datasets.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 训练数据集的消融研究。'
- en: 5 Conclusion
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We present AnyRef, a pioneering MLLM model capable of generating pixel-level
    object perceptions and language descriptions from various modality references,
    including texts, regions, images, and audio. This is made possible by the unified
    referring representation, which connects different types of inputs to the LLM.
    We further propose a refocusing mechanism that uses attention scores to improve
    the segmentation embedding and enhance pixel-level vision perception. Across various
    downstream tasks, our model exhibits remarkable performance while providing users
    with enhanced interacting flexibility.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了AnyRef，这是一种开创性的MLLM模型，能够从各种模态参考中生成像素级对象感知和语言描述，包括文本、区域、图像和音频。这得益于统一的引用表示，它将不同类型的输入连接到LLM。我们进一步提出了一种重新聚焦机制，利用注意力分数来改善分割嵌入并增强像素级视觉感知。在各种下游任务中，我们的模型展现了显著的性能，同时为用户提供了更大的交互灵活性。
- en: Acknowledgements. This work is supported by the National Natural Science Foundation
    of China (U23A20386, 62276045, 62293540, 62293542), Dalian Science and Technology
    Talent Innovation Support Plan (2022RY17).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。本研究得到了中国国家自然科学基金（U23A20386, 62276045, 62293540, 62293542）、大连科技人才创新支持计划（2022RY17）的资助。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.
    *Advances in Neural Information Processing Systems*, 35:23716–23736, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alayrac 等 [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds 等. Flamingo: 用于少样本学习的视觉语言模型。*Advances in Neural Information Processing
    Systems*，35:23716–23736，2022年。'
- en: 'Bracha et al. [2023] Lior Bracha, Eitan Shaar, Aviv Shamsian, Ethan Fetaya,
    and Gal Chechik. Disclip: Open-vocabulary referring expression generation. *arXiv
    preprint arXiv:2305.19108*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bracha 等 [2023] Lior Bracha, Eitan Shaar, Aviv Shamsian, Ethan Fetaya, 和 Gal
    Chechik. Disclip: 开放词汇的引用表达生成。*arXiv预印本 arXiv:2305.19108*，2023年。'
- en: 'Caesar et al. [2018] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari.
    Coco-stuff: Thing and stuff classes in context. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 1209–1218, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caesar 等 [2018] Holger Caesar, Jasper Uijlings, 和 Vittorio Ferrari. Coco-stuff:
    语境中的事物和材料类别。在*IEEE计算机视觉与模式识别会议论文集*，第1209–1218页，2018年。'
- en: 'Chen et al. [2023] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
    Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic.
    *arXiv preprint arXiv:2306.15195*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2023] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu,
    和 Rui Zhao. Shikra: 释放多模态LLM的参考对话魔力。*arXiv预印本 arXiv:2306.15195*，2023年。'
- en: Chen et al. [2022] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J
    Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks. *Advances
    in Neural Information Processing Systems*, 35:31333–31346, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2022] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet,
    和 Geoffrey E Hinton. 视觉任务的统一序列接口。*Advances in Neural Information Processing Systems*，35:31333–31346，2022年。
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi
    Zhao, Weisheng Wang, Boyang Li, Pascale Fung, 和 Steven Hoi. Instructblip: 面向通用视觉-语言模型的指令调优，2023年。'
- en: Everingham et al. [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge.
    *International journal of computer vision*, 88:303–338, 2010.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等 [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, 和 Andrew Zisserman. Pascal视觉对象类别（voc）挑战。*International Journal of Computer
    Vision*，88:303–338，2010年。
- en: 'Gao et al. [2023] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu.
    Avsegformer: Audio-visual segmentation with transformer. *arXiv preprint arXiv:2307.01146*,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 [2023] Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, 和 Tong Lu. Avsegformer:
    基于变换器的音频-视觉分割。*arXiv预印本 arXiv:2307.01146*，2023年。'
- en: 'Girdhar et al. [2023] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
    Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding
    space to bind them all. In *CVPR*, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Girdhar 等 [2023] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh,
    Kalyan Vasudev Alwala, Armand Joulin, 和 Ishan Misra. Imagebind: 一种将所有嵌入空间绑定在一起的方法。发表于*CVPR*，2023年。'
- en: Hao et al. [2023] Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao Dai,
    and Yiran Zhong. Improving audio-visual segmentation with bidirectional generation.
    *arXiv preprint arXiv:2308.08288*, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023] Dawei Hao, Yuxin Mao, Bowen He, Xiaodong Han, Yuchao Dai,
    和 Yiran Zhong。通过双向生成改善音视频分割。*arXiv 预印本 arXiv:2308.08288*，2023年。
- en: He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    Mask r-cnn. In *Proceedings of the IEEE international conference on computer vision*,
    pages 2961–2969, 2017.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick。Mask
    r-cnn。见于 *IEEE国际计算机视觉会议论文集*，第2961–2969页，2017年。
- en: Hong et al. [2022] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong
    Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation.
    In *European Conference on Computer Vision*, pages 108–126\. Springer, 2022.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. [2022] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, 和 Seungryong
    Kim。使用4D卷积Swin Transformer进行少样本分割的成本聚合。见于 *欧洲计算机视觉会议*，第108–126页。Springer，2022年。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen。Lora: 大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021年。'
- en: 'Kazemzadeh et al. [2014] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
    Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes.
    In *Proceedings of the 2014 conference on empirical methods in natural language
    processing (EMNLP)*, pages 787–798, 2014.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kazemzadeh et al. [2014] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, 和
    Tamara Berg。Referitgame: 指向自然场景照片中的物体。见于 *2014年自然语言处理实证方法会议（EMNLP）论文集*，第787–798页，2014年。'
- en: Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
    Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.
    Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. *arXiv:2304.02643*,
    2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
    Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.
    Berg, Wan-Yen Lo, Piotr Dollár, 和 Ross Girshick。Segment anything。*arXiv:2304.02643*，2023年。
- en: 'Lai et al. [2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model.
    *arXiv preprint arXiv:2308.00692*, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. [2023] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, 和 Jiaya Jia。Lisa: 通过大型语言模型进行推理分割。*arXiv 预印本 arXiv:2308.00692*，2023年。'
- en: 'Lavie and Agarwal [2007] Alon Lavie and Abhaya Agarwal. METEOR: An automatic
    metric for MT evaluation with high levels of correlation with human judgments.
    In *Proceedings of the Second Workshop on Statistical Machine Translation*, pages
    228–231, Prague, Czech Republic, 2007\. Association for Computational Linguistics.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lavie and Agarwal [2007] Alon Lavie 和 Abhaya Agarwal。METEOR: 一种自动化的机器翻译评估指标，与人工评估具有高度相关性。见于
    *第二届统计机器翻译研讨会论文集*，第228–231页，捷克共和国布拉格，2007年。计算语言学协会。'
- en: 'Li et al. [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning.
    *arXiv preprint arXiv:2305.03726*, 2023a.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, 和 Ziwei Liu。Otter: 一种具有上下文指令调整的多模态模型。*arXiv 预印本 arXiv:2305.03726*，2023年。'
- en: 'Li et al. [2023b] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2:
    bootstrapping language-image pre-training with frozen image encoders and large
    language models. In *ICML*, 2023b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023b] Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi。BLIP-2:
    通过冻结图像编码器和大型语言模型进行语言-图像预训练的自举。见于 *ICML*，2023年。'
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*, pages 740–755\.
    Springer, 2014.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick。Microsoft coco:
    语境中的常见物体。见于 *计算机视觉–ECCV 2014: 第13届欧洲会议, 苏黎世, 瑞士, 2014年9月6-12日, 论文集，第五部分13*，第740–755页。Springer，2014年。'
- en: 'Ling et al. [2023] Yuhang Ling, Yuxi Li, Zhenye Gan, Jiangning Zhang, Mingmin
    Chi, and Yabiao Wang. Hear to segment: Unmixing the audio to guide the semantic
    segmentation. *arXiv preprint arXiv:2305.07223*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ling et al. [2023] Yuhang Ling, Yuxi Li, Zhenye Gan, Jiangning Zhang, Mingmin
    Chi, 和 Yabiao Wang。听觉分割: 解混音音频以引导语义分割。*arXiv 预印本 arXiv:2305.07223*，2023年。'
- en: 'Liu et al. [2023a] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized
    referring expression segmentation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 23592–23601, 2023a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023a] Chang Liu, Henghui Ding, 和 Xudong Jiang. Gres: 通用指称表达分割.
    发表在*IEEE/CVF 计算机视觉与模式识别会议论文集*，页码 23592–23601，2023a。'
- en: Liu et al. [2023b] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    Visual instruction tuning. In *NeurIPS*, 2023b.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令调优.
    发表在*NeurIPS*，2023b。
- en: Liu et al. [2017] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring expression
    generation and comprehension via attributes. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 4856–4864, 2017.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2017] Jingyu Liu, Liang Wang, 和 Ming-Hsuan Yang. 通过属性生成和理解指称表达.
    发表在*IEEE 国际计算机视觉会议论文集*，页码 4856–4864，2017。
- en: 'Liu et al. [2023c] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar
    Satzoda, Vijay Mahadevan, and R Manmatha. Polyformer: Referring image segmentation
    as sequential polygon generation. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 18653–18663, 2023c.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023c] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar
    Satzoda, Vijay Mahadevan, 和 R Manmatha. Polyformer: 将指称图像分割视为顺序多边形生成. 发表在*IEEE/CVF
    计算机视觉与模式识别会议论文集*，页码 18653–18663，2023c。'
- en: Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2017] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化. *arXiv
    预印本 arXiv:1711.05101*，2017。
- en: Lüddecke and Ecker [2022] Timo Lüddecke and Alexander Ecker. Image segmentation
    using text and image prompts. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 7086–7096, 2022.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lüddecke 和 Ecker [2022] Timo Lüddecke 和 Alexander Ecker. 使用文本和图像提示的图像分割. 发表在*IEEE/CVF
    计算机视觉与模式识别会议论文集*，页码 7086–7096，2022。
- en: Lüddecke and Ecker [2022] Timo Lüddecke and Alexander Ecker. Image segmentation
    using text and image prompts. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pages 7086–7096, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lüddecke 和 Ecker [2022] Timo Lüddecke 和 Alexander Ecker. 使用文本和图像提示的图像分割. 发表在*IEEE/CVF
    计算机视觉与模式识别会议 (CVPR) 论文集*，页码 7086–7096，2022。
- en: 'Milletari et al. [2016] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
    V-net: Fully convolutional neural networks for volumetric medical image segmentation.
    In *2016 fourth international conference on 3D vision (3DV)*, pages 565–571\.
    IEEE, 2016.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Milletari et al. [2016] Fausto Milletari, Nassir Navab, 和 Seyed-Ahmad Ahmadi.
    V-net: 用于体积医学图像分割的全卷积神经网络. 发表在*2016 年第四届国际三维视觉会议 (3DV)*，页码 565–571\. IEEE，2016。'
- en: Min et al. [2021] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze
    for few-shot segmentation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision (ICCV)*, 2021.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. [2021] Juhong Min, Dahyun Kang, 和 Minsu Cho. 少样本分割的超相关压缩. 发表在*IEEE/CVF
    国际计算机视觉会议 (ICCV)*，2021。
- en: 'Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang,
    Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models
    to the world. *arXiv preprint arXiv:2306.14824*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang,
    Shuming Ma, 和 Furu Wei. Kosmos-2: 将多模态大型语言模型与现实世界对接. *arXiv 预印本 arXiv:2306.14824*，2023。'
- en: 'Plummer et al. [2017] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,
    Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities:
    Collecting region-to-phrase correspondences for richer image-to-sentence models.
    *IJCV*, 123(1):74–93, 2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Plummer et al. [2017] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,
    Juan C. Caicedo, Julia Hockenmaier, 和 Svetlana Lazebnik. Flickr30k 实体: 收集区域到短语的对应关系以丰富图像到句子的模型.
    *IJCV*，123(1):74–93，2017。'
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, 等. 从自然语言监督中学习可迁移的视觉模型. 发表在*国际机器学习会议*，页码 8748–8763\. PMLR，2021。
- en: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In *International Conference on Machine Learning*, pages 28492–28518\. PMLR, 2023.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, 和 Ilya Sutskever。通过大规模弱监督实现鲁棒的语音识别。载于 *国际机器学习会议*，第28492–28518页。PMLR，2023年。
- en: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pages 1–16\. IEEE, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    和 Yuxiong He。Zero: 训练万亿参数模型的内存优化。在 *SC20: 高性能计算、网络、存储与分析国际会议*，第1–16页。IEEE，2020年。'
- en: 'Ramanathan et al. [2023] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic,
    Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek
    Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, and Dhruv Mahajan. PACO: Parts
    and attributes of common objects. In *arXiv preprint arXiv:2301.01795*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramanathan et al. [2023] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic,
    Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek
    Kadian, Amir Mousavi, Yiwen Song, Abhimanyu Dubey, 和 Dhruv Mahajan。PACO: 常见物体的部件和属性。*arXiv预印本
    arXiv:2301.01795*，2023年。'
- en: 'Rasley et al. [2020] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. Deepspeed: System optimizations enable training deep learning models
    with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, pages 3505–3506, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley et al. [2020] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, 和 Yuxiong
    He。Deepspeed: 系统优化使训练超过1000亿参数的深度学习模型成为可能。载于 *第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第3505–3506页，2020年。'
- en: Tanaka et al. [2019] Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro
    Sato, Yoshitaka Ushiku, and Tatsuya Harada. Generating easy-to-understand referring
    expressions for target identifications. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 5794–5803, 2019.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanaka et al. [2019] Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro
    Sato, Yoshitaka Ushiku, 和 Tatsuya Harada。生成易于理解的目标识别指代表达。载于 *IEEE/CVF计算机视觉国际会议论文集*，第5794–5803页，2019年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar 等。Llama: 开放而高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*，2023年。'
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, 和 Devi Parikh。Cider:
    基于共识的图像描述评估。载于 *IEEE计算机视觉与模式识别会议论文集*，第4566–4575页，2015年。'
- en: 'Wang et al. [2023a] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large
    language model is also an open-ended decoder for vision-centric tasks. *arXiv
    preprint arXiv:2305.11175*, 2023a.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023a] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao 等。Visionllm: 大语言模型也是面向视觉中心任务的开放式解码器。*arXiv预印本
    arXiv:2305.11175*，2023年。'
- en: 'Wang et al. [2023b] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang
    Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing
    project: Towards panoptic visual recognition and understanding of the open world.
    *arXiv preprint arXiv:2308.01907*, 2023b.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023b] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang
    Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao 等。全视项目: 面向全景视觉识别和开放世界理解。*arXiv预印本
    arXiv:2308.01907*，2023年。'
- en: 'Wang et al. [2023c] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun
    Huang. Images speak in images: A generalist painter for in-context visual learning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 6830–6839, 2023c.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023c] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, 和 Tiejun
    Huang。图像中讲图像: 用于上下文视觉学习的通用画家。载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，第6830–6839页，2023年。'
- en: 'Wang et al. [2023d] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua
    Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. *arXiv preprint
    arXiv:2304.03284*, 2023d.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023d] 王心龙、张小松、曹跃、王文、沈春华和黄铁军。Seggpt：上下文中的全景分割。*arXiv 预印本 arXiv:2304.03284*，2023d。
- en: 'Wang et al. [2022] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo,
    Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 11686–11695, 2022.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] 王兆青、卢玉、李强、陶荀强、郭艳东、龚明明和刘通梁。CRIS：基于 CLIP 的图像分割。 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第11686–11695页，2022。
- en: 'Wu et al. [2020] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu
    Maji. Phrasecut: Language-based image segmentation in the wild. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    10216–10225, 2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2020] 吴晨云、林哲、斯科特·科恩、阮中和和苏布兰苏·马吉。Phrasecut：基于语言的野外图像分割。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第10216–10225页，2020。
- en: 'Wu et al. [2022] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng
    Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer
    for object understanding. *arXiv preprint arXiv:2212.00280*, 2022.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2022] 吴佳廉、王剑锋、杨正远、甘哲、刘自成、袁俊松和王丽娟。GRIT：用于物体理解的生成区域到文本变换器。*arXiv 预印本 arXiv:2212.00280*，2022。
- en: Yan et al. [2023] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan, Ping
    Luo, and Huchuan Lu. Universal instance perception as object discovery and retrieval.
    In *CVPR*, 2023.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 [2023] 闫彬、江毅、吴建南、王东、袁泽焕、罗平和吕虎川。将通用实例感知视作物体发现与检索。在 *CVPR*，2023。
- en: 'Yang et al. [2022] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang
    Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring
    image segmentation. In *CVPR*, 2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2022] 杨赵、王佳琪、唐艳松、陈凯、赵恒霜和菲利普·HS·托尔。LAVT：用于指称图像分割的语言感知视觉变换器。在 *CVPR*，2022。
- en: Ye et al. [2023] Fulong Ye, Yuxing Long, Fangxiang Feng, and Xiaojie Wang. Whether
    you can locate or not? interactive referring expression generation. In *Proceedings
    of the 31st ACM International Conference on Multimedia*, pages 4697–4706, 2023.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人 [2023] 叶富龙、龙宇星、冯芳翔和王晓杰。你能定位吗？交互式指称表达生成。在 *第31届 ACM 国际多媒体会议论文集*，第4697–4706页，2023。
- en: 'Yu et al. [2016a] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016a.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2016a] 李成宇、帕特里克·普瓦松、单杨、亚历山大·C·伯格和塔玛拉·L·伯格。上下文建模在指称表达中的应用。在 *计算机视觉–ECCV
    2016: 第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，论文集，第II部分14*，第69–85页。Springer，2016a。'
- en: 'Yu et al. [2016b] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016b.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2016b] 李成宇、帕特里克·普瓦松、单杨、亚历山大·C·伯格和塔玛拉·L·伯格。上下文建模在指称表达中的应用。在 *计算机视觉–ECCV
    2016: 第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，论文集，第II部分14*，第69–85页。Springer，2016b。'
- en: 'Yu et al. [2016c] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
    and Tamara L Berg. Modeling context in referring expressions. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part II 14*, pages 69–85\. Springer, 2016c.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2016c] 李成宇、帕特里克·普瓦松、单杨、亚历山大·C·伯格和塔玛拉·L·伯格。上下文建模在指称表达中的应用。在 *计算机视觉–ECCV
    2016: 第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，论文集，第II部分14*，第69–85页。Springer，2016c。'
- en: Yu et al. [2017] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A joint
    speaker-listener-reinforcer model for referring expressions. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*, pages 7282–7290,
    2017.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2017] 李成宇、谭浩、莫希特·班萨尔和塔玛拉·L·伯格。一个联合的说话者-听众-强化者模型用于指称表达。在 *IEEE 计算机视觉与模式识别会议论文集*，第7282–7290页，2017。
- en: Zhang et al. [2022] Jian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-proxy
    transformer for few-shot segmentation. *Advances in Neural Information Processing
    Systems*, 35:6575–6588, 2022.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] 张剑伟、孙亦凡、杨毅和陈伟。针对少样本分割的特征代理变换器。*神经信息处理系统进展*，35:6575–6588，2022。
- en: 'Zhang et al. [2023a] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun
    Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter:
    Efficient fine-tuning of language models with zero-init attention. *arXiv preprint
    arXiv:2303.16199*, 2023a.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023a] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou,
    Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, 和 Yu Qiao. Llama-adapter：零初始化注意力的高效语言模型微调。*arXiv
    预印本 arXiv:2303.16199*，2023a。
- en: 'Zhang et al. [2023b] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
    Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large
    language model on region-of-interest, 2023b.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023b] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
    Shao, Wenwei Zhang, Kai Chen, 和 Ping Luo. Gpt4roi：在感兴趣区域上的大语言模型指令调优，2023b。
- en: 'Zhao et al. [2023] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi
    Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms.
    *arXiv preprint arXiv:2307.08581*, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. [2023] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi
    Feng, 和 Bingyi Kang. Bubogpt：在多模态大语言模型中实现视觉基础。*arXiv 预印本 arXiv:2307.08581*，2023。
- en: Zhou et al. [2019] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,
    Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through
    the ade20k dataset. *International Journal of Computer Vision*, 127:302–321, 2019.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2019] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,
    Adela Barriuso, 和 Antonio Torralba. 通过 ade20k 数据集的场景语义理解。*计算机视觉国际期刊*，127:302–321，2019。
- en: Zhou et al. [2022] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing
    Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio–visual
    segmentation. In *European Conference on Computer Vision*, pages 386–403\. Springer,
    2022.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing
    Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, 和 Yiran Zhong. 音频–视觉分割。收录于*欧洲计算机视觉会议*，页码
    386–403。Springer，2022。
- en: 'Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, 和 Mohamed Elhoseiny.
    Minigpt-4：利用先进的大语言模型提升视觉-语言理解。*arXiv 预印本 arXiv:2304.10592*，2023。
- en: Zou et al. [2023a] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
    Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized
    decoding for pixel, image, and language. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 15116–15127, 2023a.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. [2023a] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,
    Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, 等. 针对像素、图像和语言的广义解码。收录于*IEEE/CVF
    计算机视觉与模式识别会议论文集*，页码 15116–15127，2023a。
- en: Zou et al. [2023b] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
    Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. *arXiv
    preprint arXiv:2304.06718*, 2023b.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. [2023b] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
    Jianfeng Gao, 和 Yong Jae Lee. 在任何地方、任何时刻进行全方位分割。*arXiv 预印本 arXiv:2304.06718*，2023b。
