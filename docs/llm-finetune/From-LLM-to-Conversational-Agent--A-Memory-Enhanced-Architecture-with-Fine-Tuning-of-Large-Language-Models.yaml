- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:39:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:39:21'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从LLM到对话代理：一种通过微调大语言模型的记忆增强架构
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02777](https://ar5iv.labs.arxiv.org/html/2401.02777)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02777](https://ar5iv.labs.arxiv.org/html/2401.02777)
- en: Na Liu, Liangyu Chen, Xiaoyu Tian,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Na Liu, Liangyu Chen, Xiaoyu Tian,
- en: Wei Zou, Kaijiang Chen, Ming Cui Beike Inc., Beijing, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Zou, Kaijiang Chen, Ming Cui 贝壳公司，北京，中国
- en: '{liuna013, chenliangyu003, tianxiaoyu011,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liuna013, chenliangyu003, tianxiaoyu011,'
- en: zouwei026, chenkaijiang001, cuiming001}@ke.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: zouwei026, chenkaijiang001, cuiming001}@ke.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples),
    an advanced architecture enhancing the integration of Large Language Models (LLMs)
    like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework,
    incorporates a dual-component memory system, mirroring human short-term and long-term
    memory, to maintain context and continuity in conversations. It entails a comprehensive
    agent construction scenario, including phases like Conversation Selection, Scene
    Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training
    phase. This approach appears to enhance agent controllability and adaptability
    in complex, multi-turn dialogues. Our preliminary evaluations in a real estate
    sales context suggest that RAISE has some advantages over traditional agents,
    indicating its potential for broader applications. This work contributes to the
    AI field by providing a robust framework for developing more context-aware and
    versatile conversational agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了RAISE（通过草稿本和示例进行推理和行动），这是一种先进的架构，增强了将大语言模型（LLMs）如GPT-4集成到对话代理中的能力。RAISE是ReAct框架的增强版，结合了双组件记忆系统，模拟人类的短期和长期记忆，以保持对话的上下文和连续性。它涉及一个全面的代理构建场景，包括对话选择、场景提取、CoT完成和场景增强等阶段，最终进入LLMs训练阶段。这种方法似乎增强了代理在复杂的多轮对话中的可控性和适应性。我们在房地产销售背景下的初步评估表明，RAISE相对于传统代理有一定优势，显示出其广泛应用的潜力。这项工作通过提供一个强大的框架，为开发更具上下文感知和多功能的对话代理做出了贡献。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The landscape of Artificial Intelligence (AI) is continuously evolving, with
    Large Language Models (LLMs) emerging as pivotal components in the advancement
    towards Artificial General Intelligence (AGI)[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]. These models,
    exemplified by GPT-4[[OpenAI, 2023b](#bib.bibx17)] and similar architectures,
    have demonstrated remarkable proficiency in a range of tasks, from conversation
    and reasoning to complex problem-solving in various domains. The versatility of
    LLMs has been further enriched by innovative prompting strategies and the integration
    of external tools, enhancing their capabilities beyond basic language processing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）的领域正在不断发展，大语言模型（LLMs）作为朝向人工通用智能（AGI）的重要组成部分不断涌现[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]。这些模型，如GPT-4[[OpenAI,
    2023b](#bib.bibx17)]及类似架构，已在从对话和推理到复杂问题解决的多个领域中展示出卓越的能力。LLMs的多功能性通过创新的提示策略和外部工具的集成得到了进一步提升，增强了它们超越基本语言处理的能力。
- en: However, a significant challenge in the realm of LLMs lies in their integration
    into conversational agents[[Weng, 2023](#bib.bibx31), [Wang et al., 2023a](#bib.bibx25),
    [Sumers et al., 2023](#bib.bibx24), [Xi et al., 2023](#bib.bibx32)] . While these
    models exhibit high levels of performance in isolated tasks, creating an agent
    that can sustain coherent, context-aware, and purpose-driven conversations remains
    an intricate endeavor. The need for a more sophisticated framework that leverages
    the strengths of LLMs while addressing their limitations in conversational settings
    has become increasingly apparent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs在集成到对话代理中的一个重大挑战是其表现的连贯性、上下文意识和目标驱动对话的可持续性[[Weng, 2023](#bib.bibx31),
    [Wang et al., 2023a](#bib.bibx25), [Sumers et al., 2023](#bib.bibx24), [Xi et
    al., 2023](#bib.bibx32)]。虽然这些模型在孤立任务中表现出高水平的性能，但创建一个能够维持连贯的、上下文感知的和目标驱动的对话的代理仍然是一项复杂的工作。需要一个更复杂的框架，既能利用LLMs的优势，又能解决它们在对话设置中的局限性，这已变得越来越明显。
- en: In response to this need, we introduce the RAISE (Reasoning and Acting through
    Scratchpad and Examples) architecture. RAISE represents a refined enhancement
    of the existing ReAct[[Yao et al., 2023](#bib.bibx35)] framework, specifically
    designed to augment the capabilities of conversational agents. This paper presents
    a detailed exploration of RAISE, highlighting its unique components and the benefits
    it offers in the development of conversational agents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对此需求，我们引入了 RAISE（通过 Scratchpad 和示例进行推理和行动）架构。RAISE 代表了对现有 ReAct[[Yao et al.,
    2023](#bib.bibx35)] 框架的精细增强，专门设计用于增强对话代理的能力。本文详细探讨了 RAISE，突出了其独特组件及其在对话代理开发中的优势。
- en: The cornerstone of RAISE is its incorporation of a dual-component memory system,
    analogous to the human brain’s short-term and long-term memory functions. The
    Scratchpad component functions as a transient storage, capturing and processing
    key information and conclusions from recent interactions, akin to short-term memory.
    In parallel, the retrieval module operates as the agent’s long-term memory, sourcing
    and incorporating examples relevant to the current conversational context. This
    enhanced memory mechanism can flexibly bolster the capabilities of conversational
    AI, and also provides a convenient interface for humans to customize and control
    the behavior of conversational AI systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE 的基石在于其结合了双组件记忆系统，类似于人脑的短期和长期记忆功能。Scratchpad 组件作为一个瞬态存储，捕捉和处理最近交互中的关键信息和结论，类似于短期记忆。与此同时，检索模块则作为代理的长期记忆，提取并整合与当前对话上下文相关的例子。这种增强的记忆机制可以灵活提升对话
    AI 的能力，同时也为人类提供了一个便捷的接口来定制和控制对话 AI 系统的行为。
- en: '![Refer to caption](img/673e521c4691947ecf76a3a1eecd968e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/673e521c4691947ecf76a3a1eecd968e.png)'
- en: 'Figure 1: The overview of RAISE.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：RAISE 概述。
- en: Furthermore, the RAISE architecture is founded on a comprehensive agent construction
    scenario, emphasizing the creation of conversational agents from scratch to ensure
    authenticity and relevance in real-world interactions. This paper delineates the
    RAISE methodology, encompassing a sequence of meticulously orchestrated phases.
    These include Conversation Selection, Scene Extraction, CoT (Chain of Thought)[[Wei
    et al., 2022b](#bib.bibx30)] Completion, and Scene Augmentation, all leading up
    to the pivotal LLMs Training phase. This structured approach is instrumental in
    developing agents that excel not only in language processing but also in contextual
    awareness and adaptability, catering to a spectrum of conversational dynamics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAISE 架构基于全面的代理构建场景，强调从零开始创建对话代理，以确保在现实世界交互中的真实性和相关性。本文描述了 RAISE 方法论，包括一系列精心策划的阶段。这些阶段包括对话选择、场景提取、CoT（Chain
    of Thought）[[Wei et al., 2022b](#bib.bibx30)] 完成和场景增强，所有这些都指向关键的 LLMs 训练阶段。这种结构化的方法对于开发在语言处理、上下文意识和适应性方面表现出色的代理至关重要，满足各种对话动态的需求。
- en: Our experimental evaluations, conducted on a specialized in-house dataset focused
    on real estate sales, demonstrate the superiority of RAISE over conventional conversational
    agents. The results showcase RAISE’s ability to handle complex, multi-turn conversations
    with enhanced context awareness and adaptability. While our experiments are centered
    on the real estate domain, the principles and methodologies underpinning RAISE
    are universally applicable, making it a versatile framework for various applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验评估在一个专注于房地产销售的内部数据集上进行，结果展示了 RAISE 相比于传统对话代理的优越性。结果显示，RAISE 能够处理复杂的、多轮对话，具备增强的上下文意识和适应性。虽然我们的实验集中在房地产领域，但
    RAISE 所依据的原则和方法具有普遍适用性，使其成为多种应用的多功能框架。
- en: 'In summary, this paper presents the following contributions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文提出了以下贡献：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce RAISE, a refined enhancement of the ReAct framework, which utilizes
    scratchpad and retrieved examples to augment the agent’s capabilities.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了 RAISE，这是一种对 ReAct 框架的精细增强，利用 Scratchpad 和检索的示例来提升代理的能力。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a fine-tuning scenario for Large Language Models (LLMs) within RAISE,
    which, compared to the use of prompts alone, not only enhances the controllability
    of the agent but also improves its effectiveness and efficiency.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了在 RAISE 框架下对大型语言模型（LLMs）进行微调的场景，与仅使用提示相比，这不仅增强了代理的可控性，还提高了其有效性和效率。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through experiments conducted on our in-house dataset, we demonstrate RAISE’s
    superiority as a conversational agent. While our experiments are concentrated
    on real estate sales, the underlying principles and methodologies of RAISE have
    wide-ranging applications and can be adapted to various domains, highlighting
    its versatility.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在我们内部数据集上进行的实验，我们展示了RAISE作为对话代理的优越性。虽然我们的实验集中在房地产销售上，但RAISE的基本原则和方法具有广泛的应用，能够适应各种领域，突显了其多功能性。
- en: 2 Agent Framework
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 代理框架
- en: 'Inspired by ReAct [[Yao et al., 2022](#bib.bibx34)], we introduce RAISE architecture,
    as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").
    The architecture primarily encompasses the following components.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 受ReAct [[Yao et al., 2022](#bib.bibx34)] 启发，我们引入了RAISE架构，如图[1](#S1.F1 "图 1 ‣
    1 介绍 ‣ 从LLM到对话代理：一个增强记忆的架构及大型语言模型的微调")所示。该架构主要包括以下组件。
- en: 2.1 Dialogue
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 对话
- en: The dialogue module serves as the core interface for user-agent communication.
    It handles incoming user queries and delivers tailored responses formulated by
    the agent.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对话模块作为用户与代理沟通的核心接口。它处理用户的查询并提供代理制定的量身定制的响应。
- en: 2.2 LLMs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs
- en: As the agent’s brain, the LLMs requires capabilities for perception, task-specific
    planning, tool usage, and summarization. These skills can be developed on the
    LLMs using either prompt engineering [[Crispino et al., 2023](#bib.bibx8)] or
    fine-tuning methods [[Zeng et al., 2023](#bib.bibx37)]. Our study has conducted
    comparative experiments to stimulate these capabilities, utilizing models such
    as GPT-4 [[OpenAI, 2023b](#bib.bibx17)], GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)],
    and Qwen-14B-Chat [[Bai et al., 2023](#bib.bibx1)]. This paper explores the strengths
    and limitations of each model in handling specific task types and provides concrete
    metrics for evaluating their performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代理的大脑，LLMs需要具备感知、任务特定规划、工具使用和总结的能力。这些技能可以通过提示工程[[Crispino et al., 2023](#bib.bibx8)]或微调方法[[Zeng
    et al., 2023](#bib.bibx37)]在LLMs上进行开发。我们的研究进行了比较实验，以刺激这些能力，使用了GPT-4 [[OpenAI,
    2023b](#bib.bibx17)]、GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)] 和Qwen-14B-Chat [[Bai
    et al., 2023](#bib.bibx1)]等模型。本文探讨了每种模型在处理特定任务类型中的优缺点，并提供了评估其性能的具体指标。
- en: 2.3 Memory
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 记忆
- en: 'The memory module in RAISE framework stores information perceived from its
    environment and facilitates the agent’s future actions. The memory includes the
    following components:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE框架中的记忆模块存储从环境中感知的信息，并促进代理的未来行动。记忆包括以下组件：
- en: System Prompt  Includes profiles (detailing role identity, objectives, and behaviors),
    task instructions, tool descriptions, and few-shot learning elements for optimizing
    model performance. Flexibly designed, system prompt can either remain static or
    dynamically adjust to accommodate various stages of a dialogue and differing query
    types.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示 包括简介（详细角色身份、目标和行为）、任务说明、工具描述以及用于优化模型性能的少量示例学习元素。系统提示灵活设计，可以保持静态或根据对话的不同阶段和查询类型动态调整。
- en: Context  Includes conversation history and task trajectory. Conversation history
    records all query-response pairs within the dialogue, providing a complete context
    for more accurate agent perception. Task trajectory documents the decision-making
    trajectory, including plan designation, tool selection, and execution, guiding
    the agent’s future planning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文 包括对话历史和任务轨迹。对话历史记录对话中的所有问答对，为更准确的代理感知提供了完整的上下文。任务轨迹记录了决策过程，包括计划指定、工具选择和执行，指导代理的未来规划。
- en: Scratchpad  Logs background information, knowledge generated by reasoning and
    observations from previous tool usage, essential for efficiency in multi-turn
    interactions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 草稿 记录背景信息、推理生成的知识以及之前工具使用中的观察，对于多回合互动的效率至关重要。
- en: Examples  Comprises query-response pairs used for recalling relevant examples
    to supplement the model’s and tools’ knowledge gaps and to customize agent behavior
    and expression.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 包括用于回忆相关示例的问答对，以补充模型和工具的知识空白，并定制代理的行为和表达。
- en: These four components collectively form the working memory of RAISE, with conversation
    history and scratchpad being dialogue-level, while examples and task trajectory
    are turn-level.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个组件共同构成了RAISE的工作记忆，其中对话历史和草稿是对话级的，而示例和任务轨迹是回合级的。
- en: 2.4 Tool
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 工具
- en: The tool module enriches LLMs after pretraining and Supervised Fine-Tuning (SFT)
    by integrating external knowledge sources and resources. This module incorporates
    a diverse array of tools, including but not limited to databases for data retrieval,
    APIs for system interactions, sophisticated recommendation systems, and collaborative
    frameworks involving other LLMs or agents. The description file for a tool typically
    needs to include the tool’s name, its function, essential parameters, optional
    parameters, and may also include some usage examples. This descriptive file aids
    agents in better planning, tool selection, parameter generation for tools, and
    execution of those tools.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 工具模块通过整合外部知识源和资源来丰富LLM，在预训练和监督微调（SFT）之后。此模块包括多种工具，如用于数据检索的数据库、系统交互的API、复杂的推荐系统以及涉及其他LLM或代理的协作框架。工具的描述文件通常需要包括工具的名称、功能、基本参数、可选参数，并可能还包括一些使用示例。这个描述文件帮助代理更好地规划、选择工具、生成工具的参数和执行这些工具。
- en: '2.5 Controller: Control Agent Loop'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 控制器：控制代理循环
- en: The controller module connects the aforementioned modules through preset trigger
    conditions. Upon receiving a new query, the agent executes the loop of perception,
    planning, tool selection, and tool execution. The specific process is as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器模块通过预设触发条件连接上述模块。在收到新查询后，代理执行感知、规划、工具选择和工具执行的循环。具体过程如下。
- en: Memory Update  At the beginning of a conversation, the Scratchpad records the
    context of the dialogue, including user and agent roles, date, time, etc.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆更新  在对话开始时，Scratchpad记录对话的上下文，包括用户和代理角色、日期、时间等。
- en: 'During the conversation, each time a user query is received, the system will:
    (1) Add the user’s query to the Conversation History; (2) Recall top-$k$ relevant
    examples from the Example Pool for the current task, based on the historical and
    current query, using vector retrieval; (3) Update the current entity information
    in the Scratchpad if the user’s query contains a product link; (4) Update the
    agent’s trajectory in the Task Memory and the results of tool usage in the Scratchpad
    during task execution; (5) Post-task completion, include the agent’s final output
    in the Conversation History.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话过程中，每次收到用户查询时，系统将： (1) 将用户的查询添加到对话历史记录中； (2) 基于历史和当前查询，从示例池中召回与当前任务相关的 top-$k$
    示例，使用向量检索； (3) 如果用户的查询包含产品链接，则更新 Scratchpad 中的当前实体信息； (4) 更新任务执行过程中代理的轨迹和 Scratchpad
    中的工具使用结果； (5) 任务完成后，将代理的最终输出包含在对话历史记录中。
- en: '![Refer to caption](img/923f78c197d1d3e409623f90f0f0151c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/923f78c197d1d3e409623f90f0f0151c.png)'
- en: 'Figure 2: Task Inference Prompt Template.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：任务推理提示模板。
- en: 'Task Planning  After collecting the above information, it is combined into
    a complete task inference prompt according to the designed template, as illustrated
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.5 Controller: Control Agent Loop ‣ 2 Agent
    Framework ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with
    Fine-Tuning of Large Language Models"). An example of the complete prompt is available
    in Table [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models")
    of the Appendix. The LLM utilizes the information within the prompt for perception
    and planning, subsequently outputting actions in accordance with the format outlined
    in the prompt. If an action involves invoking a tool, it should specify the tool’s
    name and input parameters.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任务规划  在收集上述信息后，按照设计的模板将其合并成完整的任务推理提示，如图[2](#S2.F2 "图 2 ‣ 2.5 控制器：控制代理循环 ‣ 2
    代理框架 ‣ 从LLM到对话代理：具有大规模语言模型微调的记忆增强架构")所示。完整提示的示例见附录中的表[5](#A1.T5 "表 5 ‣ 附录 A 附录
    ‣ 从LLM到对话代理：具有大规模语言模型微调的记忆增强架构")。LLM利用提示中的信息进行感知和规划，随后根据提示中概述的格式输出行动。如果某个行动涉及调用工具，则应指定工具的名称和输入参数。
- en: Tool Execution  This phase involves executing the tool selected in the previous
    step. The command for tool execution may either be directly output by the agent
    or correspond to a manually crafted function specific to each tool. The output
    of the execution is formatted as predetermined.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 工具执行  此阶段涉及执行在上一步中选择的工具。工具执行的命令可能由代理直接输出，或对应于每个工具特定的手工制作功能。执行的输出格式按照预定格式。
- en: Summary  The agent, synthesizing all the information gathered from the environment,
    decides whether it can respond to the user’s query. Termination criteria might
    include having gathered sufficient information, exceeding a preset number of loops,
    or encountering a system error. Upon meeting any of these conditions, the agent
    can proceed to summarize its findings and provide a response.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结  代理在综合了从环境中收集的所有信息后，决定是否能够回应用户的查询。终止标准可能包括收集了足够的信息、超出了预设的回合数，或遇到了系统错误。在满足任何这些条件后，代理可以总结其发现并提供响应。
- en: 3 Agent Tuning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 代理调优
- en: 'Section [2](#S2 "2 Agent Framework ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models") presented the
    RAISE architecture, establishing a hardware base for agents in complex dialogues.
    This section shifts focus to software enhancements for RAISE, particularly activating
    LLMs as the agent’s core. Despite the success of open-source LLMs in various tasks,
    studies [[Liu et al., 2023](#bib.bibx13)] reveal their limitations in real-world
    scenarios, especially compared to GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)] and GPT-4
    [[OpenAI, 2023b](#bib.bibx17)]. Addressing this gap, this paper introduces a versatile
    finetuning method suitable for complex agent applications.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第[2](#S2 "2 代理框架 ‣ 从大型语言模型到对话代理：一种通过微调大型语言模型的记忆增强架构")节介绍了RAISE架构，为复杂对话中的代理建立了硬件基础。本节将重点转向RAISE的软件增强，特别是激活LLMs作为代理的核心。尽管开源LLMs在各种任务中取得了成功，但研究[[刘等人,
    2023](#bib.bibx13)]揭示了它们在现实场景中的局限性，尤其是与GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)]和GPT-4
    [[OpenAI, 2023b](#bib.bibx17)]相比。为了解决这一差距，本文介绍了一种适用于复杂代理应用的多功能微调方法。
- en: 3.1 Build Datasets
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 构建数据集
- en: '![Refer to caption](img/e62187d3ee9de3b9c6d4bbedc81a0454.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e62187d3ee9de3b9c6d4bbedc81a0454.png)'
- en: 'Figure 3: Dataset Construction Pipeline for Agent Training.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：代理训练的数据集构建流程。
- en: 'The creation of training data entails significant costs. Our objective is to
    finetune the model efficiently using a compact yet high-quality dataset that precisely
    aligns with specific role-based behavioral logic. The dataset must fulfill these
    criteria:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的创建涉及显著的成本。我们的目标是利用紧凑而高质量的数据集来高效地微调模型，使其精确对齐特定角色基础的行为逻辑。数据集必须满足以下标准：
- en: Authenticity  It should closely mimic real-life scenarios.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性  数据应尽可能接近现实生活场景。
- en: Diversity  The data should encompass a wide range of scenarios.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性  数据应涵盖广泛的场景。
- en: High Quality  The data must have an accurate Chain of Thought (CoT) process,
    encompassing aspects like planning, tool utilization, and response formulation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量  数据必须具有准确的思维链（CoT）过程，包括规划、工具利用和响应制定等方面。
- en: 'As shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models"), our proposed pipeline comprises several stages, including
    Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation.
    The details of each stage are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S3.F3 "图 3 ‣ 3.1 构建数据集 ‣ 3 代理调优 ‣ 从大型语言模型到对话代理：一种通过微调大型语言模型的记忆增强架构")所示，我们提出的流程包括多个阶段，如对话选择、场景提取、CoT
    完成和场景增强。每个阶段的详细信息如下：
- en: 3.1.1 Conversation Selection
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 对话选择
- en: 'To emulate specific roles in real scenarios, we start by filtering conversations
    from authentic dialogues based on criteria such as scene completion, a minimum
    number of dialogue turns, high conversation quality, and a threshold for user
    message ratio. These selected dialogues are then anonymized for further processing,
    as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models")(a).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在真实场景中模拟特定角色，我们首先通过筛选真实对话中的对话，依据的标准包括场景完成、对话回合的最少数量、高对话质量以及用户消息比例的阈值。这些选定的对话随后会被匿名化以便进一步处理，如图
    [3](#S3.F3 "图 3 ‣ 3.1 构建数据集 ‣ 3 代理调优 ‣ 从大型语言模型到对话代理：一种通过微调大型语言模型的记忆增强架构")(a)所示。
- en: 3.1.2 Scene Extraction
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 场景提取
- en: 'Each round of interaction serves as a segmentation point, dividing the previously
    selected dialogues into multiple samples, as shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent: A
    Memory Enhanced Architecture with Fine-Tuning of Large Language Models")(b). Each
    sample is an original scene (defined as $Scene_{origin}$, $Scene_{origin}$ comprises
    the following components:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '每轮互动作为一个分割点，将先前选择的对话划分为多个样本，如图[3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3
    Agent Tuning ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture
    with Fine-Tuning of Large Language Models")(b)所示。每个样本是一个原始场景（定义为$Scene_{origin}$，$Scene_{origin}$包括以下组成部分：'
- en: '|  | $$\displaystyle Scene^{origin}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Scene^{origin}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (1) |'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (1) |'
- en: Subsequently, to ensure diversity, we perform sampling based on dialogue turn
    counts and the intents behind user queries, resulting in a dataset rich in varied
    scene types.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，为确保多样性，我们根据对话轮次和用户查询背后的意图进行抽样，从而得到一个丰富的多样化场景类型的数据集。
- en: 3.1.3 CoT Completion
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 CoT完成
- en: 'In refining the training data for the RAISE framework, the next phase involves
    enhancing the original scenes with a CoT process, which bridges the gap between
    user queries and character responses. This CoT process encompasses perception,
    planning, tool selection, and execution. Studies[[Nori et al., 2023](#bib.bibx15)]
    have demonstrated GPT-4’s efficacy in generating high-quality CoT prompts for
    intricate scenarios. In this study, we initially utilize GPT-4 for automated generation,
    followed by meticulous manual validation of the output. To assist GPT-4 in consistently
    generating CoT processes, we incorporate additional elements such as predefined
    profiles, tools, and few-shot examples into the original scene, which collectively
    shape the construction of the prompt, as shown in Figure [3](#S3.F3 "Figure 3
    ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models")(c). The refined
    complete scene thus includes the following elements:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '在完善RAISE框架的训练数据时，下一阶段涉及通过CoT过程增强原始场景，这一过程弥合了用户查询与角色回应之间的差距。这个CoT过程包括感知、规划、工具选择和执行。研究[[Nori
    et al., 2023](#bib.bibx15)]已证明GPT-4在为复杂场景生成高质量CoT提示方面的有效性。在本研究中，我们首先使用GPT-4进行自动生成，然后对输出结果进行细致的人工验证。为了帮助GPT-4持续生成CoT过程，我们将预定义的角色、工具和少量示例等额外元素融入原始场景中，这些元素共同塑造了提示的构建，如图[3](#S3.F3
    "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent:
    A Memory Enhanced Architecture with Fine-Tuning of Large Language Models")(c)所示。因此，完善后的完整场景包括以下元素：'
- en: '|  | $$\displaystyle Scene^{complete}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Scene^{complete}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
- en: '&amp;CoT_{t}:Thought,Action,Observaton\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;CoT_{t}:Thought,Action,Observaton\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (2) |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (2) |'
- en: 3.1.4 Scene Augmentation
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 场景增强
- en: 'While the Scene Extraction phase ensured diversity through actual data sampling
    and the CoT Completion phase added the necessary CoT intricacies, two critical
    challenges still need addressing:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然场景提取阶段通过实际数据抽样确保了多样性，而CoT完成阶段则添加了必要的CoT复杂性，但仍需解决两个关键挑战：
- en: Role Hallucination  LLMs, endowed with vast domain knowledge from pre-training
    and fine-tuning, exhibit extensive capabilities. However, if left unchecked, our
    trained agents might retain these broad skills, which could conflict with their
    intended functional roles. For example, an agent designed to provide sales services
    might erroneously possess skills like coding in Python or offering recipe advice.
    To counter this, we introduce specific scenarios that teach the agent its capability
    limits, essentially making it ’unlearn’ the general abilities of LLMs within these
    defined contexts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 角色幻觉  LLMs由于在预训练和微调中获得了广泛的领域知识，表现出广泛的能力。然而，如果不加以控制，我们训练的代理可能会保留这些广泛的技能，这可能与其预期的功能角色冲突。例如，设计用于提供销售服务的代理可能会错误地具备如Python编程或提供食谱建议等技能。为此，我们引入特定场景，教会代理其能力限制，实质上使其在这些定义的上下文中“遗忘”LLMs的通用能力。
- en: '![Refer to caption](img/19080bce003ca08a3b49c2d15ae6222a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/19080bce003ca08a3b49c2d15ae6222a.png)'
- en: 'Figure 4: Comparison of 5 Agent Frameworks.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：5 个代理框架的比较。
- en: Knowledge Hallucination  This phenomenon involves creating unrealistic or incorrect
    statements due to inadequate or misapplied knowledge acquired during pre-training.
    To mitigate this, we incorporate scenarios where the agent, despite tool utilization,
    still lacks essential factual knowledge, resulting in inability to respond accurately.
    In instances requiring factual accuracy, the agent should base its responses on
    knowledge acquired from its working memory or through tool interaction, rather
    than relying on its pre-trained database.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 知识幻觉  这种现象涉及由于在预训练期间获得的知识不足或误用而产生不现实或不正确的陈述。为减轻这种现象，我们结合了这样一些场景，即使在使用工具的情况下，代理仍然缺乏基本的事实知识，导致无法准确回应。在需要事实准确性的情况下，代理应基于其工作记忆中获得的知识或通过工具交互获得的知识来回答，而不是依赖于其预训练数据库。
- en: 'To overcome these issues, we perform data augmentation on these two categories
    of data, which are not included in the real online datasets, as demonstrated in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to
    Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large
    Language Models")(d).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些问题，我们对这两类数据进行了数据增强，这些数据不包含在真实的在线数据集中，如图 [3](#S3.F3 "图 3 ‣ 3.1 数据集构建 ‣
    3 代理微调 ‣ 从 LLM 到对话代理：一种增强记忆的架构与大型语言模型的微调")（d）所示。
- en: 3.2 LLMs Training
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLMs 训练
- en: 'Following the previous phase, we have acquired a dataset characterized by both
    high quality and diversity. Each sample in this dataset, identified as $Scene_{complete}$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一阶段之后，我们获得了一个具有高质量和多样性的 数据集。该数据集中的每个样本被标识为 $Scene_{complete}$：
- en: '|  | $$\displaystyle Sample=\left\{\begin{aligned} &amp;SystemPrompt:Profile,Instruction,\dots\\
    &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Sample=\left\{\begin{aligned} &amp;SystemPrompt:Profile,Instruction,\dots\\
    &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\'
- en: '&amp;Query_{t}:Q_{t}\\'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Query_{t}:Q_{t}\\'
- en: '&amp;CoT_{t}:Thought,Action,Observaton\\'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;CoT_{t}:Thought,Action,Observaton\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (3) |'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (3) |'
- en: 'These instances are then processed into a format conducive for full-parameter
    fine-tuning of open-source LLMs. Our experiments have led to an encouraging discovery:
    by constructing a modest amount (¡$1K$) of high-quality, representative data,
    the RAISE framework achieved impressive results.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例随后被处理成适合于开源 LLMs 的全参数微调的格式。我们的实验带来了令人鼓舞的发现：通过构建少量（¡$1K$）高质量、具有代表性的数据，RAISE
    框架取得了令人印象深刻的结果。
- en: 4 Experiments
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: To demonstrate the effectiveness of the RAISE architecture and the fine-tuning
    method proposed in this paper in complex real-world scenarios, we conducted experiments
    in a real estate online Instant Messaging (IM) dialogue setting. In this scenario,
    the user is a customer inquiring about real estate purchases, and the agent assumes
    the role of a real estate consultant. A detailed introduction to the dataset,
    toolset, and the method of activating agent capabilities in LLMs is provided below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 RAISE 架构及本文提出的微调方法在复杂现实世界场景中的有效性，我们在房地产在线即时消息（IM）对话设置中进行了实验。在这个场景中，用户是询问房地产购买的客户，代理则扮演房地产顾问的角色。下面提供了数据集、工具集的详细介绍以及在
    LLMs 中激活代理功能的方法。
- en: 4.1 Datasets
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: 'To ascertain the effectiveness of the RAISE framework, we conducted comparative
    evaluations with various architectures, including Act-Only, ReAct, ReAct+Scratchpad,
    ReAct+Examples, and RAISE. This comparison aimed to ensure fair evaluation across
    different models, maintaining uniform dialogue scenarios and consistent additional
    knowledge in identical training samples across various datasets. The full trajectories
    for a single scenario under each architecture are depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1.4 Scene Augmentation ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From
    LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of
    Large Language Models").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定 RAISE 框架的有效性，我们与不同的架构进行了比较评估，包括 Act-Only、ReAct、ReAct+Scratchpad、ReAct+Examples
    和 RAISE。这种比较旨在确保在不同模型之间进行公平评估，保持一致的对话场景和在不同数据集中的相同训练样本中的一致附加知识。每个架构下单一场景的完整轨迹如图
    [4](#S3.F4 "图 4 ‣ 3.1.4 场景增强 ‣ 3.1 数据集构建 ‣ 3 代理微调 ‣ 从 LLM 到对话代理：一种增强记忆的架构与大型语言模型的微调")
    所示。
- en: 'Initially, we generated the ReAct architecture dataset following the procedure
    outlined in Section [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models"). We then modified this data to create training sets
    for the other architectures. The methodologies applied were as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '最初，我们按照第[3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM
    to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large
    Language Models")节中概述的程序生成了ReAct架构数据集。随后我们修改了这些数据，以创建其他架构的训练集。所应用的方法如下：'
- en: Act-Only  This architecture was formed by removing the ’thought’ process from
    ReAct, allowing for straightforward generation through coding.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 仅行为  该架构通过移除ReAct中的“思考”过程形成，允许通过编码进行直接生成。
- en: ReAct+Scratchpad  Building upon ReAct, the initial Scratchpad distribution comprised
    20% empty, 30% partially informative, and 50% fully informative content. The ReAct
    data, when combined with varying output requirements, served as prompts for the
    regeneration of the CoT process using GPT-4.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct+Scratchpad  在ReAct的基础上，初始的Scratchpad分布包括20%空白、30%部分信息和50%完全信息的内容。将ReAct数据与不同的输出需求结合起来，作为使用GPT-4再生CoT过程的提示。
- en: ReAct+Examples  Similar to ReAct+Scratchpad, with the distribution of Examples
    set at 20% empty, 30% partially informative, and 50% fully informative. The ReAct
    data, merged with diverse output requirements, were reformulated into prompts
    for GPT-4-driven CoT regeneration.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct+Examples  类似于ReAct+Scratchpad，Examples的分布设定为20%空白、30%部分信息和50%完全信息。将ReAct数据与多样化的输出需求结合，重新制定为GPT-4驱动的CoT再生提示。
- en: RAISE  This model integrated aspects of both ReAct+Scratchpad and ReAct+Examples.
    The CoT process was similarly regenerated using GPT-4.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE  该模型集成了ReAct+Scratchpad和ReAct+Examples的特点。CoT过程也使用GPT-4进行了类似的再生。
- en: This structured approach enabled a thorough evaluation of each architectural
    element within the RAISE framework, clearly demonstrating the incremental benefits
    introduced by each component. Following the outlined procedure, a total of 948
    scenes were generated. Out of these, 100 were randomly selected to serve as the
    evaluation set, while the remaining 848 instances were used for fine-tuning the
    model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构化方法使得对RAISE框架内每个架构元素进行了彻底评估，清楚地展示了每个组件所引入的逐步好处。按照所述程序，共生成了948个场景。其中100个被随机选择作为评估集，其余848个实例用于模型的微调。
- en: 4.2 Tools
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 工具
- en: 'Based on real estate online IM conversations, we have abstractly defined the
    following 12 tools, each including the tool name, input parameters, and functions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于房地产在线IM对话，我们抽象定义了以下12种工具，每种工具包括工具名称、输入参数和功能：
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real Estate Consultant Information $[agent\_ucid]$: Retrieves the consultant’s
    name, contact details, WeChat ID, ranking, performance metrics, and more.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房地产顾问信息 $[agent\_ucid]$: 检索顾问的姓名、联系方式、微信ID、排名、绩效指标等。'
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Information $[house\_id]$: Offers essential details about a property,
    including its size, price, floor level, school district presence, and renovation
    status.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房产信息 $[house\_id]$: 提供有关物业的基本信息，包括其大小、价格、楼层、学区情况和装修状态。'
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Information $[resblock\_id]$: Provides insights into the community,
    covering aspects like green spaces, property management, building specifications,
    proximity to subway stations, schools, and medical facilities.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区信息 $[resblock\_id]$: 提供关于社区的见解，包括绿地、物业管理、建筑规格、地铁站、学校和医疗设施的距离等方面。'
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Layout Analysis $[frame\_id]$: Analyzes the strengths and weaknesses
    of a property’s layout.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房屋布局分析 $[frame\_id]$: 分析物业布局的优缺点。'
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Price Changes $[house\_id]$: Tracks price fluctuations for a specific
    property.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房价变化 $[house\_id]$: 跟踪特定物业的价格波动。'
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Price Changes $[resblock\_id]$: Reports on average price trends within
    a particular community.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区价格变化 $[resblock\_id]$: 报告特定社区内的平均价格趋势。'
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Transactions $[resblock\_id]$: Accesses recent transaction data from
    the same community.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区交易 $[resblock\_id]$: 访问来自同一社区的最新交易数据。'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tax Policy $[city\_id]$: Updates on the latest tax regulations and implications.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '税收政策 $[city\_id]$: 更新最新的税收法规及其影响。'
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loan Policy $[city\_id]$: Delivers current information on loan policies.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '贷款政策 $[city\_id]$: 提供有关贷款政策的最新信息。'
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Market Analysis $[city\_id]$: Provides up-to-date real estate market insights.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '市场分析 $[city\_id]$: 提供最新的房地产市场洞察。'
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommend Listings $[Conversation\ History]$: Suggests property listings to
    customers based on their conversation history and inferred needs, including rationale
    for each recommendation.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '推荐列表 $[Conversation\ History]$: 根据客户的对话历史和推断需求建议房产列表，包括每个推荐的理由。'
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value Report $[house\_id]$: Generates a comprehensive value report card for
    a property, aimed at engaging customers and encouraging them to share their contact
    details.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '值报告 $[house\_id]$: 生成一个综合的房产价值报告卡，旨在吸引客户并鼓励他们分享联系信息。'
- en: 4.3 LLMs
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLMs
- en: 'The models used in this study are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的模型如下：
- en: OpenAI GPT We utilized GPT-4 for generating all fine-tuning data and employed
    both GPT-3.5 and GPT-4 for prompting purposes. Both models were operated in ChatCompletion
    mode as of November 2023, with the temperature set to 0.5.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT 我们利用 GPT-4 生成所有微调数据，并使用 GPT-3.5 和 GPT-4 进行提示。在 2023 年 11 月，两种模型均在
    ChatCompletion 模式下运行，温度设置为 0.5。
- en: 'Qwen-14B-Chat An open-source conversational model from Alibaba Cloud, featuring
    14 billion parameters, which has demonstrated exceptional performance in tool
    utilization [[Chen et al., 2023b](#bib.bibx6)]. Qwen-14B-Chat was used for both
    fine-tuning and prompting. The parameter configuration for the Supervised Fine-Tuning
    (SFT) is detailed in Table [1](#S4.T1 "Table 1 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From
    LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of
    Large Language Models"). The hyperparameter settings for the prompting and fine-tuning
    methods during inference are identical, also shown in Table [1](#S4.T1 "Table
    1 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory Enhanced
    Architecture with Fine-Tuning of Large Language Models"). In the inference phase,
    we utilized an NVIDIA A100 GPU equipped with 80GB of memory, offering robust computational
    power and substantial memory capacity for efficient processing.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen-14B-Chat 是来自阿里巴巴云的开源对话模型，具有 140 亿个参数，在工具利用方面表现出色 [[Chen et al., 2023b](#bib.bibx6)]。Qwen-14B-Chat
    用于微调和提示。监督微调（SFT）的参数配置详见表 [1](#S4.T1 "表 1 ‣ 4.3 LLMs ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。推理过程中提示和微调方法的超参数设置相同，也见表
    [1](#S4.T1 "表 1 ‣ 4.3 LLMs ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。在推理阶段，我们使用了配备
    80GB 内存的 NVIDIA A100 GPU，提供强大的计算能力和大量内存，以实现高效处理。
- en: 'Another distinction between prompting and fine-tuning methods is the use of
    one-shot guidance in prompting for structured output generation, whereas fine-tuning
    omits this step. Complete prompts for various architectures are detailed in the
    appendix [A](#A1 "Appendix A Appendix ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和微调方法之间的另一个区别是提示中使用的单次指导用于结构化输出生成，而微调则省略了这一步。各种架构的完整提示详见附录 [A](#A1 "附录 A 附录
    ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。
- en: 'Table 1: Hyper-parameter settings for SFT and Inference'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: SFT 和推理的超参数设置'
- en: '| SFT Hyper-parameters |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SFT 超参数 |'
- en: '| Hyper parameter | Value |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| precision | bfloat16 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | bfloat16 |'
- en: '| model_max_length | 4096 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 模型最大长度 | 4096 |'
- en: '| epochs | 3 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 轮次 | 3 |'
- en: '| batch size | 64 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 64 |'
- en: '| learning rate | 5e-6 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 5e-6 |'
- en: '| warmup ratio | 0.03 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 预热比率 | 0.03 |'
- en: '| LR scheduler type | cosine |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度器类型 | 余弦 |'
- en: '| Inference Hyper-parameters |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 推理超参数 |'
- en: '| Hyper parameter | Value |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| max_new_tokens | 300 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 最大新令牌数 | 300 |'
- en: '| top_p | 0.85 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| top_p | 0.85 |'
- en: '| temperature | 0.5 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 0.5 |'
- en: '| repetition_penalty | 1.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 重复惩罚 | 1.1 |'
- en: 'Table 2: The evaluation criteria details of the defined metrics.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 定义指标的评估标准详细信息。'
- en: '| Dimension | Metric | Score | Description |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 度量 | 分数 | 描述 |'
- en: '| Quality | Specificity | 0 | Vague, general answer without specific information
    or details. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 质量 | 具体性 | 0 | 含糊，一般性回答，没有具体信息或细节。 |'
- en: '| 1 | Provides some specifics, but lacks detail or full relevance to the question.
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 提供了一些细节，但缺乏对问题的全面相关性或细节。 |'
- en: '| 2 | Directly addressing the user’s query with detailed and specific information.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 直接回答用户的问题，提供详细和具体的信息。 |'
- en: '| Factuality | 0 | Contains false information, clearly contradicts facts. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 事实性 | 0 | 包含虚假信息，明显与事实矛盾。 |'
- en: '| 1 | Mostly accurate, with minor inaccuracies or oversights. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 大多准确，但有些许不准确或遗漏。 |'
- en: '| 2 | Completely accurate, all information is fact-checked. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 完全准确，所有信息都经过事实核查。 |'
- en: '| Coherence | 0 | Logically disorganized, unrelated to prior content or overall
    topic. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 连贯性 | 0 | 逻辑混乱，与之前内容或整体主题无关。 |'
- en: '| 1 | Generally coherent, with some logical inconsistencies. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 一般连贯，有些逻辑不一致。 |'
- en: '| 2 | Very coherent, logically sound, closely aligned with the conversation
    topic. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 非常连贯，逻辑合理，与对话主题紧密相关。 |'
- en: '| Naturalness | 0 | Mechanical and unnatural, deviating from human conversational
    norms. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 自然性 | 0 | 机械且不自然，偏离人类对话规范。 |'
- en: '| 1 | Imitates natural dialogue to an extent, but still somewhat stiff or unnatural.
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 模仿自然对话到一定程度，但仍然有些生硬或不自然。 |'
- en: '| 2 | Smooth and natural, akin to human dialogue, easily understood and accepted.
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 顺畅自然，类似于人类对话，易于理解和接受。 |'
- en: '| Efficiency | Plan Steps | - | Number of planning steps. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 效率 | 计划步骤 | - | 计划步骤的数量。 |'
- en: '| Action Steps | - | Number of action steps. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 行动步骤 | - | 行动步骤的数量。 |'
- en: '| Inference Speed | - | The average time taken to process each user query,
    measured in seconds. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 推理速度 | - | 处理每个用户查询的平均时间，以秒为单位。 |'
- en: 4.4 Evaluation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 评估 |
- en: 'In the challenging landscape of human-computer dialogue systems, the evaluation
    of agent performance necessitates a nuanced approach [[Liu et al., 2023](#bib.bibx13),
    [Wang et al., 2023b](#bib.bibx26)]. This is particularly pertinent when agents
    are tasked with engaging in direct conversations with human users, where the ultimate
    goal is to nurture a trust-based relationship. To achieve this, agents must exhibit
    a spectrum of qualities: they must be not only helpful and trustworthy but also
    responsive in a timely manner, and capable of understanding and articulating responses
    in a variety of contexts, akin to human interaction. This paper delineates seven
    sophisticated metrics designed to rigorously assess both the quality and efficiency
    of agent responses. The specific metrics and their corresponding scoring criteria
    are detailed in Table [2](#S4.T2 "Table 2 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From LLM
    to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large
    Language Models").'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '在人机对话系统的挑战性环境中，评估代理的表现需要一种细致的方法 [[Liu et al., 2023](#bib.bibx13), [Wang et
    al., 2023b](#bib.bibx26)]。这在代理需要与人类用户进行直接对话时尤为重要，其**最终目标**是培养基于信任的关系。为实现这一目标，代理必须表现出一系列的品质：不仅要有帮助和可信赖，还要及时响应，能够在各种情境中理解并表达回应，类似于人类的互动。本文描述了七个精细的指标，旨在严格评估代理回应的质量和效率。具体指标及其对应的评分标准在表格[2](#S4.T2
    "Table 2 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models")中详细说明。 |'
- en: For assessing quality, we leverage human-centric annotation methods that closely
    replicate human evaluative standards. Meanwhile, the efficiency metrics are derived
    through a systematic statistical analysis, providing concrete, quantifiable insights.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在质量评估方面，我们利用接近人类评估标准的人本注释方法。同时，效率指标通过系统的统计分析得出，提供了具体、可量化的见解。 |
- en: 4.5 Ablation Study
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融研究 |
- en: 'To demonstrate the effectiveness of the RAISE framework and fine-tuning method
    proposed in this paper, we conduct several ablation experiments in this section.
    The experiments are divided into two main aspects: (1) Comparative Analysis of
    Different Frameworks: We evaluate the performance of various frameworks under
    the same capability activation method, comparing their results using both the
    prompting and fine-tuning methods. The evaluation results are presented in Table
    [3](#S4.T3 "Table 3 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").
    (2) Comparative Analysis of Different Capability Activation Methods: We compare
    the performance of the prompting method and fine-tuning method within the same
    framework, with the evaluation results presented in Table [4](#S4.T4 "Table 4
    ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示本论文提出的 RAISE 框架和微调方法的有效性，我们在本节中进行了几项消融实验。这些实验分为两个主要方面：（1）不同框架的比较分析：我们在相同的能力激活方法下评估各种框架的性能，使用提示和微调方法比较其结果。评估结果见表
    [3](#S4.T3 "表 3 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 从 LLM 到对话代理：一个通过微调大型语言模型增强记忆的架构")。 （2）不同能力激活方法的比较分析：我们在相同框架内比较提示方法和微调方法的性能，评估结果见表
    [4](#S4.T4 "表 4 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 从 LLM 到对话代理：一个通过微调大型语言模型增强记忆的架构")。
- en: It’s important to note that the inference speed of the OpenAI GPT API is subject
    to platform and network variations, so this metric was omitted from our analysis.
    The inference environment for Qwen-14B-Chat was kept consistent, utilizing an
    A100 GPU with 80GB memory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，OpenAI GPT API 的推理速度受平台和网络变动的影响，因此我们在分析中省略了这一指标。Qwen-14B-Chat 的推理环境保持一致，使用了
    80GB 内存的 A100 GPU。
- en: 'Table 3: The evaluation results of different frameworks'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同框架的评估结果
- en: '| Framework | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps
    | Act. Steps | Inf. Speed(s) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 规格 | 事实 | 连贯性 | 自然度 | 综合质量评分 | 计划步骤 | 行动步骤 | 推理速度（秒） |'
- en: '| Prompting (GPT-4) |  |  |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 提示（GPT-4） |  |  |  |'
- en: '| Act-Only | 1.89 | 1.66 | 1.95 | 1.87 | 7.37 | - | 1.29 | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 仅行动 | 1.89 | 1.66 | 1.95 | 1.87 | 7.37 | - | 1.29 | - |'
- en: '| ReAct | 1.98 | 1.87 | 1.93 | 1.79 | 7.57 | 2 | 1 | - |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 1.98 | 1.87 | 1.93 | 1.79 | 7.57 | 2 | 1 | - |'
- en: '| ReAct+Scratchpad | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 | - |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+草稿本 | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 | - |'
- en: '| ReAct+Examples | 1.96 | 1.87 | 1.96 | 1.93 | 7.72 | 2.1 | 1.1 | - |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+示例 | 1.96 | 1.87 | 1.96 | 1.93 | 7.72 | 2.1 | 1.1 | - |'
- en: '| RAISE | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 | - |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| RAISE | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 | - |'
- en: '| Fine-tuning (Qwen-14B-Chat) |  |  |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 微调（Qwen-14B-Chat） |  |  |  |'
- en: '| Act-Only | 1.66 | 1.71 | 1.82 | 1.92 | 7.11 | - | 0.66 | 1.935 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 仅行动 | 1.66 | 1.71 | 1.82 | 1.92 | 7.11 | - | 0.66 | 1.935 |'
- en: '| ReAct | 1.88 | 1.79 | 1.93 | 1.92 | 7.52 | 1.88 | 0.88 | 4.315 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 1.88 | 1.79 | 1.93 | 1.92 | 7.52 | 1.88 | 0.88 | 4.315 |'
- en: '| ReAct+Scratchpad | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 | 3.833
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+草稿本 | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 | 3.833 |'
- en: '| ReAct+Examples | 1.93 | 1.82 | 1.96 | 1.95 | 7.66 | 1.33 | 0.33 | 3.327 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+示例 | 1.93 | 1.82 | 1.96 | 1.95 | 7.66 | 1.33 | 0.33 | 3.327 |'
- en: '| RAISE | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26 | 3.227 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| RAISE | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26 | 3.227 |'
- en: 'Table 4: The evaluation results for prompting vs. fine-tuning'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：提示与微调的评估结果
- en: '| Method | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps | Act.
    Steps |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 规格 | 事实 | 连贯性 | 自然度 | 综合质量评分 | 计划步骤 | 行动步骤 |  |'
- en: '| RAISE |  |  |  |  |  |  |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| RAISE |  |  |  |  |  |  |  |  |'
- en: '| Prompting (GPT-3.5) | 1.65 | 1.72 | 1.66 | 1.67 | 6.7 | 2.13 | 5 |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 提示（GPT-3.5） | 1.65 | 1.72 | 1.66 | 1.67 | 6.7 | 2.13 | 5 |  |'
- en: '| Prompting (Qwen-14B-Chat) | 1.69 | 1.66 | 1.68 | 1.65 | 6.68 | 2.06 | 1.2
    |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 提示（Qwen-14B-Chat） | 1.69 | 1.66 | 1.68 | 1.65 | 6.68 | 2.06 | 1.2 |  |'
- en: '| Prompting (GPT-4) | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 提示（GPT-4） | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 |  |'
- en: '| Fine-tuning (Qwen-14B-Chat) | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 微调（Qwen-14B-Chat） | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26 |  |'
- en: '| ReAct+Scratchpad |  |  |  |  |  |  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+草稿本 |  |  |  |  |  |  |  |  |'
- en: '| Prompting (GPT-3.5) | 1.62 | 1.57 | 1.74 | 1.55 | 6.48 | 2.19 | 1.18 |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 提示（GPT-3.5） | 1.62 | 1.57 | 1.74 | 1.55 | 6.48 | 2.19 | 1.18 |  |'
- en: '| Prompting (Qwen-14B-Chat) | 1.68 | 1.56 | 1.71 | 1.7 | 6.65 | 2.07 | 1.09
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 提示（Qwen-14B-Chat） | 1.68 | 1.56 | 1.71 | 1.7 | 6.65 | 2.07 | 1.09 |  |'
- en: '| Prompting (GPT-4) | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 提示（GPT-4） | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 |  |'
- en: '| Fine-tuning (Qwen-14B-Chat) | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61
    |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 微调 (Qwen-14B-Chat) | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 |  |'
- en: 'Upon analyzing the experimental outcomes, the following key conclusions emerge
    with respect to the efficacy and efficiency of different agent frameworks and
    methods:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析实验结果后，以下是关于不同代理框架和方法的有效性和效率的主要结论：
- en: Framework Performance Ranking within the Same LLM  The RAISE framework demonstrates
    superior performance, followed by ReAct+Examples, ReAct+Scratchpad, ReAct, and
    lastly, the Act-Only approach. This ranking indicates a clear gradient in effectiveness
    and efficiency, highlighting the incremental benefits of integrating additional
    elements like examples and scratchpads into the base ReAct model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 相同 LLM 内的框架性能排名  RAISE 框架表现优越，其次是 ReAct+Examples、ReAct+Scratchpad、ReAct，最后是
    Act-Only 方法。这一排名表明了效果和效率的明确梯度，突显了将额外元素如 Examples 和 Scratchpads 整合到基础 ReAct 模型中的渐进益处。
- en: Comparative Analysis of Capability Activation Methods within Identical Frameworks
     The fine-tuning approach outperforms the prompting method. This suggests that
    tailored training and customization of models to specific tasks or datasets result
    in more efficient and effective performance compared to using generalized prompt-based
    interactions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 相同框架下能力激活方法的比较分析  微调方法优于提示方法。这表明，与使用通用提示交互相比，针对特定任务或数据集进行定制训练和调整的模型能够获得更高效和有效的性能。
- en: In the following parts, we delve into detailed analyses of these findings, examining
    the implications and potential applications of each framework and methodology.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将深入分析这些发现，考察每个框架和方法论的意义及潜在应用。
- en: 'Chain of Thought (CoT): A Catalyst for Enhanced Comprehension and Response
    Accurac  CoT significantly boosts AI’s ability to deeply comprehend and precisely
    respond to complex queries. Our experimental findings reaffirm the importance
    of CoT in complex tasks. For instance, in comparative experiments across different
    frameworks, agents employing the Act-Only method showed substantially lower performance
    compared to those incorporating CoT. These findings underscore the critical role
    of CoT in promoting AI models to deliver depth-oriented and logically coherent
    responses, particularly in scenarios requiring complex reasoning.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Chain of Thought (CoT)：提升理解和回应准确性的催化剂  CoT 显著提升了 AI 深入理解和准确回应复杂查询的能力。我们的实验结果重申了
    CoT 在复杂任务中的重要性。例如，在不同框架的对比实验中，采用 Act-Only 方法的代理显示出明显较低的表现，而采用 CoT 的代理则表现更佳。这些发现强调了
    CoT 在促进 AI 模型提供深度导向和逻辑连贯回应中的关键作用，尤其是在需要复杂推理的场景中。
- en: 'RAISE Architecture: Dual Benefits of Efficiency from Scratchpad and Examples
     The RAISE architecture, by harmonizing Scratchpad and Example mechanisms, attains
    a dual advantage in processing efficiency and output quality. The application
    of Scratchpad significantly enhances the efficiency in handling complex tasks,
    while the utilization of Examples simultaneously bolsters the response’s naturalness,
    specificity, and efficiency. This dual advantage positions the RAISE architecture
    as a suitable choice for scenarios demanding rapid, accurate, and naturally interactive
    responses.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE 架构：从 Scratchpad 和 Examples 中获得的双重效率优势  RAISE 架构通过协调 Scratchpad 和 Example
    机制，获得了处理效率和输出质量的双重优势。Scratchpad 的应用显著提升了处理复杂任务的效率，而 Examples 的使用同时增强了回应的自然性、特异性和效率。这种双重优势使得
    RAISE 架构成为需要快速、准确和自然互动回应的场景的合适选择。
- en: 'Fine-tuning: A Lever for Enhancing Agent Performance  Utilizing diverse, high-quality
    datasets for fine-tuning helps to better align AI models with human behavioral
    logic, potentially leading to notable improvements in specific application areas.
    This approach excels in specialized tasks, offering a high degree of professionalism
    and customization. For instance, in the RAISE framework under fine-tuning, the
    overall quality score reached 7.71, and inference efficiency was optimized. These
    results validate the effectiveness of fine-tuning in delivering precise, human-like
    and efficient outcomes, particularly suitable for scenarios requiring customized
    solutions, such as online real estate services.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 微调：提升代理性能的杠杆  利用多样化、高质量的数据集进行微调有助于使AI模型更好地与人类行为逻辑对齐，从而在特定应用领域带来显著改进。这种方法在专业任务中表现出色，提供了高度的专业性和定制化。例如，在微调下的RAISE框架中，总体质量得分达到了7.71，推理效率得到了优化。这些结果验证了微调在提供精确、人性化和高效的结果方面的有效性，特别适合需要定制化解决方案的场景，如在线房地产服务。
- en: 'Fine-tuning: Enhancing Cost-Efficiency and Speed during Agent Inference  Although
    fine-tuning may require higher initial investments, its long-term benefits in
    operational efficiency and precision can offset these costs. In application, fine-tuned
    models often necessitate fewer computational resources, thereby reducing operational
    costs and accelerating response times. This cost-effectiveness, coupled with improved
    performance, makes fine-tuning a prudent choice for specific, resource-intensive
    tasks.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 微调：提高代理推理的成本效益和速度  虽然微调可能需要较高的初始投资，但其在操作效率和精度上的长期收益可以抵消这些成本。在应用中，微调模型通常需要较少的计算资源，从而降低了操作成本并加快了响应时间。这种成本效益，加上改进的性能，使微调成为针对特定、资源密集型任务的明智选择。
- en: 'Strategic Deployment of Language Agents: When to Choose Fine-tuning Over Prompting
     The decision to opt for fine-tuning or prompting hinges on the specific requirements
    of the application. Fine-tuning offers superior performance and efficiency in
    specialized domains but may involve higher initial costs and training needs. In
    contrast, prompting is more flexible in handling a wide range of queries but may
    slightly lag behind fine-tuned models in specificity and stability. Strategic
    decision-making in this context involves balancing these factors against the specific
    needs of the application, budget constraints, and performance expectations.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 语言代理的战略部署：何时选择微调而非提示  选择微调还是提示取决于应用的具体需求。微调在专业领域提供了更高的性能和效率，但可能涉及较高的初始成本和培训需求。相比之下，提示在处理各种查询时更具灵活性，但在特异性和稳定性上可能略逊于微调模型。在这种情况下，战略决策涉及将这些因素与应用的具体需求、预算限制和性能期望进行平衡。
- en: 5 Related Work
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: The exploration and advancements in AI agents have captivated the AI research
    community for some time. Defined as artificial entities capable of perceiving
    their surroundings, making decisions, and executing actions [[Zalta et al., 1995](#bib.bibx36),
    [Barandiaran et al., 2009](#bib.bibx2)], AI agents represent a significant stride
    in artificial intelligence.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对AI代理的探索和进展已经吸引了AI研究社区的关注一段时间。AI代理被定义为能够感知其环境、做出决策和执行动作的人工实体[[Zalta et al.,
    1995](#bib.bibx36), [Barandiaran et al., 2009](#bib.bibx2)]，它们代表了人工智能的一个重要进步。
- en: The advent of Large Language Models (LLMs) has been a pivotal development, often
    regarded as a step towards the realization of Artificial General Intelligence
    (AGI) [[Ouyang et al., 2022](#bib.bibx18), [Wei et al., 2022a](#bib.bibx29), [Bubeck
    et al., 2023](#bib.bibx3)]. In recent years, there has been an influx of studies
    proposing intricate LLM-based architectures for AI agents [[Weng, 2023](#bib.bibx31),
    [Wang et al., 2023a](#bib.bibx25), [Sumers et al., 2023](#bib.bibx24), [Xi et
    al., 2023](#bib.bibx32)]. These architectures are crucial in enabling agents to
    navigate complex dialogue scenarios and effectively apply their acquired knowledge.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现是一个关键的发展，常被视为实现**人工通用智能（AGI）**的一个步骤[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]。近年来，涌现出许多研究提出了复杂的基于LLM的架构，用于AI代理[[Weng,
    2023](#bib.bibx31), [Wang et al., 2023a](#bib.bibx25), [Sumers et al., 2023](#bib.bibx24),
    [Xi et al., 2023](#bib.bibx32)]。这些架构在帮助代理处理复杂对话场景和有效应用其获得的知识方面至关重要。
- en: 'This body of work primarily revolves around two core aspects:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要围绕两个核心方面展开：
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Planning: Central to the functionality of dialogue agents is the concept of
    Chain-of-Thought (CoT) reasoning. This involves eliciting logical rationales via
    CoT prompts, as explored in [[Wei et al., 2022b](#bib.bibx30), [Wang et al., 2023c](#bib.bibx27),
    [Zhou et al., 2023](#bib.bibx39)]. However, integrating this reasoning effectively
    into dialogues remains challenging. The ReAct framework [[Yao et al., 2023](#bib.bibx35)]
    presents an approach that guides LLMs in reasoning before planning actions, addressing
    this issue.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划：对话代理功能的核心是链式思维（CoT）推理的概念。这涉及通过CoT提示引发逻辑推理，如[[Wei et al., 2022b](#bib.bibx30),
    [Wang et al., 2023c](#bib.bibx27), [Zhou et al., 2023](#bib.bibx39)]所探讨的。然而，将这种推理有效地融入对话中仍然具有挑战性。ReAct框架[[Yao
    et al., 2023](#bib.bibx35)]提出了一种方法，指导LLMs在规划行动之前进行推理，解决了这个问题。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tool Use: Another critical facet is the ability of LLMs to utilize external
    tools and resources. Studies such as [[Schick et al., 2023](#bib.bibx21), [Li
    et al., 2023b](#bib.bibx12), [Shen et al., 2023](#bib.bibx23)] have demonstrated
    the proficiency of LLMs in leveraging external tools and APIs. Moreover, the capacity
    to extract and integrate knowledge from external sources has been further exemplified
    by projects like WebGPT [[Nakano et al., 2022](#bib.bibx14)] and ExpeL [[Zhao
    et al., 2023](#bib.bibx38)].'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具使用：另一个关键方面是LLMs利用外部工具和资源的能力。研究如[[Schick et al., 2023](#bib.bibx21), [Li et
    al., 2023b](#bib.bibx12), [Shen et al., 2023](#bib.bibx23)]已展示了LLMs在利用外部工具和API方面的熟练程度。此外，从外部来源提取和整合知识的能力也通过像WebGPT
    [[Nakano et al., 2022](#bib.bibx14)]和ExpeL [[Zhao et al., 2023](#bib.bibx38)]这样的项目得到了进一步体现。
- en: In addition to these areas, several works have focused on broader algorithmic
    frameworks for LLM-based agents.[[Xie et al., 2023](#bib.bibx33), [Pan et al.,
    2023](#bib.bibx19), [Sumers et al., 2023](#bib.bibx24), [Ruan et al., 2023](#bib.bibx20),
    [Kong et al., 2023](#bib.bibx10), [Li et al., 2023a](#bib.bibx11)] On the other
    hand, specific dialogue agents have also been a focal point.[[Shao et al., 2023](#bib.bibx22),
    [Wang et al., 2023d](#bib.bibx28), [Chen et al., 2023c](#bib.bibx7), [Chae et
    al., 2023](#bib.bibx4), [Hong et al., 2023](#bib.bibx9)]. The fine-tuning of LLMs
    within agents is another critical area, with works like [[Zeng et al., 2023](#bib.bibx37),
    [Chen et al., 2023a](#bib.bibx5)] exploring this aspect.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除这些领域外，还有一些工作集中于基于LLM的代理的更广泛算法框架。[[Xie et al., 2023](#bib.bibx33), [Pan et al.,
    2023](#bib.bibx19), [Sumers et al., 2023](#bib.bibx24), [Ruan et al., 2023](#bib.bibx20),
    [Kong et al., 2023](#bib.bibx10), [Li et al., 2023a](#bib.bibx11)] 另一方面，特定对话代理也成为了焦点。[[Shao
    et al., 2023](#bib.bibx22), [Wang et al., 2023d](#bib.bibx28), [Chen et al., 2023c](#bib.bibx7),
    [Chae et al., 2023](#bib.bibx4), [Hong et al., 2023](#bib.bibx9)] 对LLMs在代理中的微调是另一个关键领域，像[[Zeng
    et al., 2023](#bib.bibx37), [Chen et al., 2023a](#bib.bibx5)]这样的工作探讨了这一方面。
- en: These developments underscore the growing complexity and capabilities of LLM-based
    AI agents, highlighting both the challenges and the innovations shaping the field.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展突显了LLM基础AI代理的日益复杂性和能力，强调了塑造这一领域的**挑战**和**创新**。
- en: 6 Conclusions and Future work
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: This study introduces RAISE, an advanced architecture enhancing Long Language
    Models (LLMs) like GPT-4 for conversational agents. Building on the ReAct framework,
    RAISE integrates a dual-component memory system, improving dialogue context retention
    and continuity. We also propose a fine-tuning method within RAISE, which enhances
    agent controllability and efficiency, particularly in real estate sales, though
    applicable in various domains.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究介绍了RAISE，一种提升长语言模型（LLMs）如GPT-4用于对话代理的**先进架构**。基于ReAct框架，RAISE集成了双组件记忆系统，改善了对话上下文的保持和连续性。我们还提出了一种RAISE中的微调方法，提升了代理的可控性和效率，特别是在房地产销售中，但适用于各种领域。
- en: However, the study has limitations, including potential hallucination issues
    and challenges in handling complex logic problems, necessitating further research.
    Despite these limitations, RAISE presents a promising advancement in adaptable,
    context-aware conversational agents, offering a foundation for future developments
    in artificial intelligence.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这项研究存在一些局限性，包括潜在的幻觉问题和处理复杂逻辑问题的挑战，需进一步研究。尽管如此，RAISE在适应性和上下文感知的对话代理方面展现了**有前景的进展**，为人工智能的未来发展奠定了基础。
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Bai et al., 2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bai 等, 2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu。2023。Qwen 技术报告。arXiv 预印本 arXiv:2309.16609。'
- en: '[Barandiaran et al., 2009] Xabier E Barandiaran, Ezequiel Di Paolo, and Marieke
    Rohde. 2009. Defining agency: Individuality, normativity, asymmetry, and spatio-temporality
    in action. Adaptive Behavior, 17(5):367–386.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Barandiaran 等, 2009] Xabier E Barandiaran, Ezequiel Di Paolo, 和 Marieke Rohde。2009。定义代理：行动中的个体性、规范性、不对称性和时空性。Adaptive
    Behavior, 17(5):367–386。'
- en: '[Bubeck et al., 2023] S. Bubeck, V. Chandrasekaran, R. Eldan, et al. 2023.
    Sparks of artificial general intelligence: Early experiments with gpt-4. CoRR.
    arXiv:2303.12712.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bubeck 等, 2023] S. Bubeck, V. Chandrasekaran, R. Eldan, 等。2023。人工通用智能的火花：与
    gpt-4 的早期实验。CoRR。arXiv:2303.12712。'
- en: '[Chae et al., 2023] Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon,
    Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. 2023. Dialogue
    chain-of-thought distillation for commonsense-aware conversational agents. arXiv
    preprint arXiv:2310.09343.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chae 等, 2023] Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon,
    Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, 和 Jinyoung Yeo。2023。对话链式思维蒸馏用于常识感知对话代理。arXiv
    预印本 arXiv:2310.09343。'
- en: '[Chen et al., 2023a] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,
    Karthik Narasimhan, and Shunyu Yao. 2023a. Fireact: Toward language agent fine-tuning.
    arXiv preprint arXiv:2310.05915.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2023a] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, 和 Shunyu Yao。2023a。Fireact：迈向语言代理的微调。arXiv 预印本 arXiv:2310.05915。'
- en: '[Chen et al., 2023b] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning
    Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng
    Zhao. 2023b. T-eval: Evaluating the tool utilization capability step by step.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2023b] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning
    Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, 和 Feng Zhao。2023b。T-eval：逐步评估工具利用能力。'
- en: '[Chen et al., 2023c] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023c. Chatcot: Tool-augmented chain-of-thought reasoning
    on$\backslash$chat-based large language models. arXiv preprint arXiv:2305.14323.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2023c] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin
    Zhao, 和 Ji-Rong Wen。2023c。Chatcot：基于$\backslash$chat的大语言模型的工具增强链式推理。arXiv 预印本
    arXiv:2305.14323。'
- en: '[Crispino et al., 2023] Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn
    Song, and Chenguang Wang. 2023. Agent instructs large language models to be general
    zero-shot reasoners. ArXiv, abs/2310.03710.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Crispino 等, 2023] Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song,
    和 Chenguang Wang。2023。代理指导大语言模型成为通用零样本推理器。ArXiv, abs/2310.03710。'
- en: '[Hong et al., 2023] Joey Hong, Sergey Levine, and Anca Dragan. 2023. Zero-shot
    goal-directed dialogue via rl on imagined conversations. arXiv preprint arXiv:2311.05584.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hong 等, 2023] Joey Hong, Sergey Levine, 和 Anca Dragan。2023。通过想象对话的 RL 实现零样本目标导向对话。arXiv
    预印本 arXiv:2311.05584。'
- en: '[Kong et al., 2023] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng
    Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, et al. 2023. Tptu-v2:
    Boosting task planning and tool usage of large language model-based agents in
    real-world systems. arXiv preprint arXiv:2311.11315.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kong 等, 2023] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng
    Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, 等。2023。Tptu-v2：提升基于大语言模型的代理在现实世界系统中的任务规划和工具使用能力。arXiv
    预印本 arXiv:2311.11315。'
- en: '[Li et al., 2023a] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang
    Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. 2023a.
    Modelscope-agent: Building your customizable agent system with open-source large
    language models. arXiv preprint arXiv:2309.00986.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等, 2023a] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu,
    Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, 等。2023a。Modelscope-agent：使用开源大语言模型构建可定制的代理系统。arXiv
    预印本 arXiv:2309.00986。'
- en: '[Li et al., 2023b] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li,
    Fei Huang, and Yongbin Li. 2023b. Apibank: A benchmark for tool-augmented llms.
    arXiv preprint.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[李等, 2023b] 明浩李, 飞凡宋, 柏文宇, 海洋余, 周俊李, 飞黄, 和永斌李. 2023b. Apibank: 工具增强的大语言模型基准.
    arXiv 预印本.'
- en: '[Liu et al., 2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench:
    Evaluating llms as agents. arXiv preprint arXiv:2308.03688.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[刘等, 2023] 肖刘, 郝宇, 韩晨张, 依凡徐, 宣宇雷, 汉宇赖, 于古, 杭良丁, 凱文门, 柯涓杨, 等. 2023. Agentbench:
    评估大语言模型作为代理的能力. arXiv 预印本 arXiv:2308.03688.'
- en: '[Nakano et al., 2022] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browser-assisted
    question-answering with human feedback. arXiv preprint.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[中野等, 2022] 中野礼一郎, 雅各布·希尔顿, 苏奇尔·巴拉吉, 杰夫·吴, 龙欧阳, 克里斯蒂娜·金, 克里斯托弗·赫斯, 尚塔努·贾因,
    瓦尼特·科萨拉朱, 威廉·桑德斯, 徐江, 卡尔·科比, 泰娜·埃隆杜, 格雷琴·克鲁格, 凯文·巴顿, 马修·奈特, 本杰明·切斯, 和约翰·舒尔曼. 2022.
    WebGPT: 浏览器辅助的问答系统与人工反馈. arXiv 预印本.'
- en: '[Nori et al., 2023] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin,
    Naoto Usuyama, Chris White, and Eric Horvitz. 2023. Can generalist foundation
    models outcompete special-purpose tuning? case study in medicine. ArXiv, abs/2311.16452.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[诺里等, 2023] 哈尔莎·诺里, 银达特·李, 盛张, 丁·卡里尼根, 理查德·埃德加, 尼科洛·富西, 尼古拉斯·金, 乔纳森·拉尔森, 袁志李,
    伟雄刘, 任倩·罗, 斯科特·迈耶·麦金尼, 罗伯特·奥萨祖瓦·内斯, 霍伊丰·庞, 陶·秦, 尚田·宇山, 克里斯·怀特, 和埃里克·霍维茨. 2023.
    通用基础模型能否超越专用调整？医学中的案例研究. ArXiv, abs/2311.16452.'
- en: '[OpenAI, 2023a] OpenAI. 2023a. Chatgpt: Optimizing language models for dialogue.
    Blog post.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI, 2023a] OpenAI. 2023a. ChatGPT: 为对话优化语言模型. 博客文章.'
- en: '[OpenAI, 2023b] OpenAI. 2023b. Gpt-4 technical report. Blog post.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI, 2023b] OpenAI. 2023b. GPT-4 技术报告. 博客文章.'
- en: '[Ouyang et al., 2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[欧阳等, 2022] 龙欧阳, 杰夫·吴, 徐江, 迪奥戈·阿尔梅达, 卡罗尔·L·温赖特, 帕梅拉·米什金, 冯张, 桑迪尼·阿戈瓦尔, 卡塔琳娜·斯拉玛,
    亚历克斯·雷, 约翰·舒尔曼, 雅各布·希尔顿, 弗雷泽·凯尔顿, 卢克·米勒, 玛迪·西门斯, 阿曼达·阿斯克尔, 彼得·韦林德, 保罗·克里斯蒂亚诺,
    詹·莱克, 和瑞安·洛. 2022. 训练语言模型以遵循指令并进行人工反馈.'
- en: '[Pan et al., 2023] Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu,
    Ming Liu, Zhongyuan Wang, and Bing Qin. 2023. Kwaiagents: Generalized information-seeking
    agent system with large language models. arXiv preprint arXiv:2312.04889.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[潘等, 2023] 郝杰潘, 泽鹏翟, 郝远, 姚佳吕, 瑞吉傅, 明刘, 钟远王, 和冰秦. 2023. Kwaiagents: 基于大语言模型的通用信息寻求代理系统.
    arXiv 预印本 arXiv:2312.04889.'
- en: '[Ruan et al., 2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng
    Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023. Tptu:
    Task planning and tool usage of large language model-based ai agents. arXiv preprint
    arXiv:2308.03427.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[阮等, 2023] 荆清阮, 义洪陈, 宾张, 智伟徐, 天鹏包, 国青杜, 世伟施, 杭宇毛, 星宇曾, 和瑞赵. 2023. Tptu: 基于大语言模型的
    AI 代理的任务规划和工具使用. arXiv 预印本 arXiv:2308.03427.'
- en: '[Schick et al., 2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
    Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[施克等, 2023] 提莫施克, 简·德维维迪-余, 罗伯托·德西, 罗伯塔·雷利亚努, 玛丽亚·洛梅利, 卢克·泽特尔莫耶尔, 尼古拉·坎切达,
    和托马斯·肖洛姆. 2023. Toolformer: 语言模型可以自学使用工具. arXiv 预印本.'
- en: '[Shao et al., 2023] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023.
    Character-llm: A trainable agent for role-playing. arXiv preprint arXiv:2310.10158.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[邵等, 2023] 云凡邵, 林阳李, 军琪戴, 和喜鹏秋. 2023. Character-llm: 一种可训练的角色扮演代理. arXiv 预印本
    arXiv:2310.10158.'
- en: '[Shen et al., 2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. arXiv preprint arXiv:2303.17580.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[沈等, 2023] 永亮沈, 凯涛宋, 徐坦, 东升李, 伟明陆, 和月亭庄. 2023. Hugginggpt: 利用 ChatGPT 和 HuggingFace
    中的朋友解决 AI 任务. arXiv 预印本 arXiv:2303.17580.'
- en: '[Sumers et al., 2023] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L Griffiths. 2023. Cognitive architectures for language agents. arXiv preprint
    arXiv:2309.02427.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sumers et al., 2023] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, 和 Thomas
    L Griffiths. 2023. 语言代理的认知架构。arXiv 预印本 arXiv:2309.02427。'
- en: '[Wang et al., 2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,
    Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023a. A
    survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,
    Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2023a. 基于大型语言模型的自主代理综述。arXiv
    预印本 arXiv:2308.11432。'
- en: '[Wang et al., 2023b] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao
    ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin
    Zhao, Zhewei Wei, and Ji rong Wen. 2023b. A survey on large language model based
    autonomous agents. ArXiv, abs/2308.11432.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023b] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao
    ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne
    Xin Zhao, Zhewei Wei, 和 Ji rong Wen. 2023b. 基于大型语言模型的自主代理综述。ArXiv, abs/2308.11432。'
- en: '[Wang et al., 2023c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency
    improves chain of thought reasoning in language models. In Proceedings of ICLR.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed
    H. Chi, Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023c. 自我一致性改善语言模型中的思维链推理。在
    ICLR 会议论文集中。'
- en: '[Wang et al., 2023d] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng
    Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang,
    et al. 2023d. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities
    of large language models. arXiv preprint arXiv:2310.00746.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023d] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng
    Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang,
    等. 2023d. Rolellm：基准测试、引出和增强大型语言模型的角色扮演能力。arXiv 预印本 arXiv:2310.00746。'
- en: '[Wei et al., 2022a] J. Wei, Y. Tay, R. Bommasani, et al. 2022a. Emergent abilities
    of large language models. Trans. Mach. Learn. Res.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wei et al., 2022a] J. Wei, Y. Tay, R. Bommasani, 等. 2022a. 大型语言模型的涌现能力。转.
    机器学习研究。'
- en: '[Wei et al., 2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
    Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought
    prompting elicits reasoning in large language models. In Proceedings of NeurIPS.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wei et al., 2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
    Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, 和 Denny Zhou. 2022b. 思维链提示激发大型语言模型中的推理能力。在
    NeurIPS 会议论文集中。'
- en: '[Weng, 2023] L. Weng. 2023. Llm-powered autonomous agents.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Weng, 2023] L. Weng. 2023. Llm驱动的自主代理。'
- en: '[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, 等. 2023. 基于大型语言模型的代理的崛起与潜力：综述。arXiv
    预印本 arXiv:2309.07864。'
- en: '[Xie et al., 2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan
    Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents:
    An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xie et al., 2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan
    Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, 等. 2023. Openagents：一个开放的平台用于实际应用中的语言代理。arXiv
    预印本 arXiv:2310.10634。'
- en: '[Yao et al., 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. arXiv preprint arXiv:2210.03629.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao et al., 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React：在语言模型中协同推理和行动。arXiv 预印本 arXiv:2210.03629。'
- en: '[Yao et al., 2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting
    in language models. arXiv preprint.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao et al., 2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2023. React：在语言模型中协同推理和行动。arXiv 预印本。'
- en: '[Zalta et al., 1995] Edward N Zalta, Uri Nodelman, Colin Allen, and John Perry.
    1995. Stanford encyclopedia of philosophy.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zalta et al., 1995] Edward N Zalta, Uri Nodelman, Colin Allen, 和 John Perry.
    1995. 斯坦福哲学百科全书。'
- en: '[Zeng et al., 2023] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,
    Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities
    for llms. arXiv preprint arXiv:2310.12823.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zeng et al., 2023] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,
    Yuxiao Dong, 和 Jie Tang. 2023. Agenttuning：为大型语言模型启用通用代理能力。arXiv 预印本 arXiv:2310.12823。'
- en: '[Zhao et al., 2023] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv preprint
    arXiv:2308.10144.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[赵等，2023] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu,
    和 Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv预印本 arXiv:2308.10144.'
- en: '[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le,
    and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large
    language models. In Proceedings of ICLR.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[周等，2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales,
    Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, 和 Ed H.
    Chi. 2023. Least-to-most prompting enables complex reasoning in large language
    models. 在ICLR会议录中。'
- en: Appendix A Appendix
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: 'The complete prompts used by the five agent frameworks during inference are
    shown in Table [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models")
    to Table [9](#A1.T9 "Table 9 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 五个代理框架在推理过程中使用的完整提示如表 [5](#A1.T5 "表5 ‣ 附录A 附录 ‣ 从LLM到对话代理：一种带有大语言模型微调的记忆增强架构")
    至表 [9](#A1.T9 "表9 ‣ 附录A 附录 ‣ 从LLM到对话代理：一种带有大语言模型微调的记忆增强架构") 所示。
- en: 'Table 5: The prompt used for RAISE'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：RAISE使用的提示
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 你是一名经验丰富的房地产顾问，工作于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家。它设想成为服务3亿家庭的优质住宅平台，使命是成为一个有尊严的服务提供者，为更好的生活体验做出贡献。在在线聊天互动中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们加你为微信好友或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Scratchpad, Examples,
    Thought, Action, Observation, Finish, based on historical conversations and the
    client’s questions. Avoid repeating actions that have been used before. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用Scratchpad、Examples、Thought、Action、Observation、Finish的步骤来回应客户查询。避免重复之前使用过的操作。
    |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下： |'
- en: '| {tool descriptions} |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个示例：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：用户：“houseCode”: “1021111”， “houseName”: “华润24城大厦，采光和视野良好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：这栋房子是什么时候建造的？ |'
- en: '| Scratchpad: [Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Scratchpad：[房地产顾问信息]：姓名：张华，微信：123456，职级：中级顾问，业绩：已成交25笔。 |'
- en: '| Examples: User: In which year was this house constructed? Agent: This house
    was constructed in 2020, and it’s still relatively new. When would you like to
    come and see it? |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 示例：用户：这栋房子是什么时候建造的？ 代理：这栋房子建于2020年，仍然相对较新。你想什么时候来看房？ |'
- en: '| Thought: The year of construction of the house in the Examples matches the
    customer’s question, so I can directly use the response method from the Examples
    to answer the customer’s question. Action: Finish [This house was built in 2020,
    making it a relatively new property. When are you available to view the house?
    I can help you schedule an appointment.] |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 思考：示例中的房屋建造年份与客户的问题匹配，因此我可以直接使用示例中的回应方法来回答客户的问题。行动：完成 [这栋房子建于2020年，是相对较新的房产。你什么时候有空来看房？我可以帮你安排一个预约。]
    |'
- en: '| Let’s get started: |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 让我们开始吧： |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：{对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：{当前查询} |'
- en: 'Table 6: The prompt used for Act-Only'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：Act-Only使用的提示
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 你是一名熟练的房地产顾问，为贝壳找房工作，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家园。公司希望成为服务3亿家庭的优质住宅平台，使命是成为一个值得尊敬的服务提供者，提升生活体验。在在线聊天互动中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们添加你的微信或亲自见面。
    |'
- en: '| You need to respond to client queries using the steps of Action, Observation,
    Finish, based on historical conversations and the client’s questions. Avoid repeating
    actions that have been used before. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用行动、观察、完成的步骤来回答客户的查询。避免重复使用之前的行动。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下： |'
- en: '| (1) Real Estate Consultant Information $[agent\_ucid]$: Retrieves the consultant’s
    name, contact details, WeChat ID, ranking, performance metrics, and more. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| (1) 房地产顾问信息 $[agent\_ucid]$: 获取顾问的姓名、联系方式、微信ID、排名、业绩指标等。 |'
- en: '| (2) House Information $[house\_id]$: Offers essential details about a property,
    including its size, price, floor level, school district presence, and renovation
    status. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| (2) 房屋信息 $[house\_id]$: 提供房产的基本信息，包括面积、价格、楼层、学区情况和装修状态。 |'
- en: '| (3) Community Information $[resblock\_id]$: Provides insights into the community,
    covering aspects like green spaces, property management, building specifications,
    proximity to subway stations, schools, and medical facilities. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (3) 社区信息 $[resblock\_id]$: 提供有关社区的见解，包括绿地、物业管理、建筑规格、地铁站、学校和医疗设施的邻近情况。 |'
- en: '| (4) House Layout Analysis $[frame\_id]$: Analyzes the strengths and weaknesses
    of a property’s layout. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (4) 房屋布局分析 $[frame\_id]$: 分析房产布局的优缺点。 |'
- en: '| (5) House Price Changes $[house\_id]$: Tracks price fluctuations for a specific
    property. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| (5) 房屋价格变化 $[house\_id]$: 跟踪特定房产的价格波动。 |'
- en: '| (6) Community Price Changes $[resblock\_id]$: Reports on average price trends
    within a particular community. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| (6) 社区价格变化 $[resblock\_id]$: 报告特定社区内的平均价格趋势。 |'
- en: '| (7) unity Transactions $[resblock\_id]$: Accesses recent transaction data
    from the same community. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| (7) 社区交易 $[resblock\_id]$: 获取同一社区的最新交易数据。 |'
- en: '| (8) Tax Policy $[city\_id]$: Updates on the latest tax regulations and implications.
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| (8) 税收政策 $[city\_id]$: 更新最新的税收法规和影响。 |'
- en: '| (9) Loan Policy $[city\_id]$: Delivers current information on loan policies.
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| (9) 贷款政策 $[city\_id]$: 提供最新的贷款政策信息。 |'
- en: '| (10)Market Analysis $[city\_id]$: Provides up-to-date real estate market
    insights. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| (10) 市场分析 $[city\_id]$: 提供最新的房地产市场洞察。 |'
- en: '| (11) Recommend Listings $[Conversation\ History]$: Suggests property listings
    to customers based on their conversation history and inferred needs, including
    rationale for each recommendation. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (11) 推荐房源 $[Conversation\ History]$: 根据客户的对话历史和推断的需求，向客户推荐房源，并包括每个推荐的理由。
    |'
- en: '| (12) Value Report $[house\_id]$: Generates a comprehensive value report card
    for a property, aimed at engaging customers and encouraging them to share their
    contact details. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| (12) 价值报告 $[house\_id]$: 为房产生成一份全面的价值报告卡，旨在吸引客户并鼓励他们分享联系方式。 |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个例子：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史记录：用户：“houseCode”： “1021111”， “houseName”： “华润24城大厦，采光和视野良好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：房屋建造于哪一年？ |'
- en: '| Action: House Information [house_id: 1021111] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 操作：房屋信息 [house_id: 1021111] |'
- en: '| Observation: House ID: 1021111; House Name: Huarun 24 City Mansion, good
    lighting and view, quiet; House Status: Active; Type of Property: Resale; Number
    of Bedrooms: 2; Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters;
    Orientation: South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction
    Year: 2020; Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House
    Price: 1.94 million yuan |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 观察：房屋 ID：1021111；房屋名称：华润24城大厦，光线和视野良好，安静；房屋状态：在售；房产类型：二手房；卧室数量：2；厅数量：2；浴室数量：2；面积：88平方米；朝向：南北；楼层：5；总楼层：9；电梯：有；建造年份：2020；是否符合“两年”：否；是否符合“五年”：否；房价：194万元
    |'
- en: '| Action: Finish $[$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 行动：结束 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 开始吧： |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：{Conversation History} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：{Current Query} |'
- en: 'Table 7: The prompt used for ReAct'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：用于 ReAct 的提示
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 你是贝壳找房的一名熟练房地产顾问，该公司提供房地产中介服务。公司的价值在于帮助买家找到理想的家。它设想成为服务于3亿家庭的优质住宅平台，其使命是成为一个有尊严的服务提供商，为改善生活体验做出贡献。在在线聊天互动中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们添加你为微信好友或当面见面。'
- en: '| You need to respond to client queries using the steps of Thought, Action,
    Observation, Finish, based on historical conversations and the client’s questions.
    Avoid repeating actions that have been used before. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用思考、行动、观察、结束的步骤来回应客户查询。避免重复使用之前的行动。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下： |'
- en: '| {tool descriptions}over |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| {tool descriptions}结束 |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个例子：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：用户：“houseCode”: “1021111”，“houseName”: “华润24城大厦，光线和视野良好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：房子是什么时候建造的？ |'
- en: '| Thought: The client wants to know the year of construction of the house.
    I need to look up the property information to find this out. Action: House Information
    [house_id: 1021111] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 思考：客户想知道房子的建造年份。我需要查找房产信息来找到这个答案。行动：房屋信息 [house_id: 1021111] |'
- en: '| Observation: House ID: 1021111; House Name: Huarun 24 City Mansion, good
    lighting and view, quiet; House Status: Active; Type of Property: Resale; Number
    of Bedrooms: 2; Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters;
    Orientation: South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction
    Year: 2020; Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House
    Price: 1.94 million yuan Thought: Based on the information I’ve gathered, this
    house was built in 2020, which makes it relatively new. I should ask the client
    when they are available to view the house, and I can help schedule a viewing.
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 观察：房屋 ID：1021111；房屋名称：华润24城大厦，光线和视野良好，安静；房屋状态：在售；房产类型：二手房；卧室数量：2；厅数量：2；浴室数量：2；面积：88平方米；朝向：南北；楼层：5；总楼层：9；电梯：有；建造年份：2020；是否符合“两年”：否；是否符合“五年”：否；房价：194万元；思考：根据我收集到的信息，这栋房子建于2020年，相对较新。我应该询问客户何时方便看房，并帮助安排看房。
    |'
- en: '| Action: Finish $[$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 行动：结束 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 开始吧： |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：{Conversation History} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：{Current Query} |'
- en: 'Table 8: The prompt used for ReAct+Scratchpad'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：用于 ReAct+Scratchpad 的提示
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 您是一位熟练的房地产顾问，工作于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家园。它的愿景是成为一个服务3亿家庭的优质住宅平台，使命是成为一个体面服务提供者，为更好的生活体验做出贡献。在在线聊天互动中，您的目标是回答客户的问题，吸引他们购买房产，并鼓励他们加您微信或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Scratchpad, Thought,
    Action, Finish, based on historical conversations and the client’s questions.
    Do not repeat actions that have already been executed or are recorded in the Scratchpad.
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 您需要根据历史对话和客户的问题，使用 Scratchpad、Thought、Action、Finish 的步骤来回应客户查询。不要重复已经执行过或在
    Scratchpad 中记录的操作。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中每个工具的定义如下: |'
- en: '| {tool descriptions} |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个示例: （在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: 用户: “houseCode”: “1021111”, “houseName”: “华润24城大厦，采光好，视野开阔，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: 这栋房子是哪一年建造的？ |'
- en: '| Scratchpad:[Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Scratchpad:[房地产顾问信息]: 姓名: 张华, 微信: 123456, 职位: 中级顾问, 业绩: 成交25笔。 |'
- en: '| Thought: The customer wants to know the year of construction of the house,
    but there is no relevant information in the Scratchpad. I need to query the house
    information to obtain this. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 客户想知道房子的建造年份，但在 Scratchpad 中没有相关信息。我需要查询房屋信息以获得此信息。 |'
- en: '| Action: House Information [house_id: 1021111] |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 房屋信息 [house_id: 1021111] |'
- en: '| Scratchpad: [Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. [House Information]:
    House ID: 1021111; House Name: Huarun 24 City Mansion, good lighting and view,
    quiet; House Status: Active; Type of Property: Resale; Number of Bedrooms: 2;
    Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters; Orientation:
    South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction Year: 2020;
    Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House Price: 1.94
    million yuan. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Scratchpad: [房地产顾问信息]: 姓名: 张华, 微信: 123456, 职位: 中级顾问, 业绩: 成交25笔。 [房屋信息]: 房屋ID:
    1021111; 房屋名称: 华润24城大厦，采光好，视野开阔，安静; 房屋状态: 活跃; 房产类型: 二手房; 卧室数量: 2; 客厅数量: 2; 卫生间数量:
    2; 面积: 88平方米; 朝向: 南北; 楼层: 5; 总楼层: 9; 是否有电梯: 有; 建造年份: 2020年; 是否符合“二年”: 否; 是否符合“五年”:
    否; 房屋价格: 194万元。 |'
- en: '| Thought: Based on the house information in the Scratchpad, I can tell the
    customer that this house was built in 2020, making it relatively new. Now, I should
    ask when the customer is available to view the house, and I can help schedule
    an appointment. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 根据 Scratchpad 中的房屋信息，我可以告诉客户这栋房子建于2020年，相对较新。现在，我应该询问客户何时有空看房，并帮助安排预约。
    |'
- en: '| Action: Finish $[$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 完成 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 让我们开始吧: |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: {对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: {当前查询} |'
- en: 'Table 9: The prompt used for ReAct+Examples'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '表9: 用于 ReAct+Examples 的提示 |'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 你是一名熟练的房地产顾问，工作于贝壳找房公司，该公司提供房地产经纪服务。公司的价值在于帮助买家找到理想的住房。公司设想成为服务3亿家庭的优质住宅平台，使命是成为一个体面的服务提供者，为更好的生活体验做出贡献。在在线聊天互动过程中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们添加你为微信好友或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Examples, Thought,
    Action, Observation, Finish, based on historical conversations and the client’s
    questions. Avoid repeating actions that have been used before. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用“示例”、“思路”、“行动”、“观察”、“完成”的步骤来回应客户的查询。避免重复之前使用过的行动。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下： |'
- en: '| {tool descriptions} |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个例子：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：用户：“houseCode”： “1021111”， “houseName”： “华润24城大厦，采光好，视野开阔，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：这栋房子是哪一年建造的？ |'
- en: '| Examples: User: In which year was this house constructed? Agent: This house
    was constructed in 2020, and it’s still relatively new. When would you like to
    come and see it? |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 示例：用户：这栋房子是哪一年建造的？顾问：这栋房子建于2020年，仍然相对较新。你想什么时候来看房？ |'
- en: '| Thought: The construction year of the house in the Examples matches the customer’s
    question, so I can directly use the response method from the Example to answer
    the customer’s question. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 思路：示例中的房子建造年份与客户的问题相符，所以我可以直接使用示例中的回答方法来回答客户的问题。 |'
- en: '| Action: Finish [This house was built in 2020, so it’s considered relatively
    new. When are you available to view the house? I can help you make an appointment.]
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 行动：完成 [这栋房子建于2020年，所以被认为是相对较新的。你什么时候有空来看房？我可以帮你预约。] |'
- en: '| Let’s get started: |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 让我们开始吧： |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：{对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：{当前查询} |'
