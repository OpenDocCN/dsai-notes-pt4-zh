- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Fine-Tuned ‘Small’ LLMs (Still) Significantly Outperform Zero-Shot Generative
    AI Models in Text Classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调后的“小型”LLMs（依然）在文本分类中显著优于零-shot生成性AI模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08660](https://ar5iv.labs.arxiv.org/html/2406.08660)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08660](https://ar5iv.labs.arxiv.org/html/2406.08660)
- en: Martin Juan José Bucher
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 马丁·胡安·何塞·布赫
- en: Stanford University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学
- en: mnbucher@stanford.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: mnbucher@stanford.edu
- en: \AndMarco Martini
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \AndMarco Martini
- en: University of Zurich
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世大学
- en: marco.martini@uzh.ch
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: marco.martini@uzh.ch
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative AI offers a simple, prompt-based alternative to fine-tuning smaller
    BERT-style LLMs for text classification tasks. This promises to eliminate the
    need for manually labeled training data and task-specific model training. However,
    it remains an open question whether tools like ChatGPT can deliver on this promise.
    In this paper, we show that smaller, fine-tuned LLMs (still) consistently and
    significantly outperform larger, zero-shot prompted models in text classification.
    We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude
    Opus) with several fine-tuned LLMs across a diverse set of classification tasks
    (sentiment, approval/disapproval, emotions, party positions) and text categories
    (news, tweets, speeches). We find that fine-tuning with application-specific training
    data achieves superior performance in all cases. To make this approach more accessible
    to a broader audience, we provide an easy-to-use toolkit alongside this paper.
    Our toolkit, accompanied by non-technical step-by-step guidance, enables users
    to select and fine-tune BERT-like LLMs for any classification task with minimal
    technical and computational effort.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性AI提供了一种简单的、基于提示的替代方案，来替代针对文本分类任务的小型BERT风格LLMs的微调。这有望消除对手动标注训练数据和任务特定模型训练的需求。然而，像ChatGPT这样的工具是否能兑现这一承诺仍然是一个悬而未决的问题。在本文中，我们展示了较小的、微调后的LLMs（依然）在文本分类中持续且显著地优于更大的零-shot提示模型。我们比较了三种主要的生成性AI模型（ChatGPT与GPT-3.5/GPT-4和Claude
    Opus）与几种微调后的LLMs，在各种分类任务（情感、批准/不批准、情绪、党派立场）和文本类别（新闻、推文、演讲）中。我们发现，使用特定应用的训练数据进行微调在所有情况下都能取得优越的性能。为了使这种方法更易于被更广泛的受众使用，我们在本文中提供了一个易于使用的工具包。我们的工具包配有非技术性的逐步指导，帮助用户选择和微调类似BERT的LLMs，针对任何分类任务，且仅需最少的技术和计算努力。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The last decade has seen a paradigm shift in Natural Language Processing (NLP)
    with the advent of pre-trained Large Language Models (LLMs). These models have
    not only achieved previously unseen performance on a wide range of benchmarks.
    They have also shifted expectations for the future of artificial intelligence
    (AI) more generally.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年，随着预训练大型语言模型（LLMs）的出现，自然语言处理（NLP）领域经历了一次范式转变。这些模型不仅在各种基准测试中取得了前所未有的性能，还改变了对人工智能（AI）未来的期望。
- en: 'Traditionally, leveraging LLMs involves two sequential steps: First, models
    are pre-trained on large text corpora to instill general language understanding.
    Pre-training is typically performed by specialized experts who then make the pre-trained
    models publicly available by providing all parameters and model states via checkpoints.
    Subsequently, these models can be further fine-tuned for specific tasks using
    smaller, dedicated training datasets, a task usually carried out by end-users
    who need to implement deep learning code and provide application-specific training
    data in the process. LLMs of this kind, like BERT or RoBERTa, have since been
    the state-of-the-art in numerous NLP tasks. Empirical evidence has shown that
    fine-tuned LLMs outperform traditional methods such as dictionaries, bag-of-words
    models, or approaches based on word embeddings [[13](#bib.bibx13), [24](#bib.bibx24),
    [45](#bib.bibx45), [11](#bib.bibx11)].'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，利用LLMs涉及两个连续的步骤：首先，模型在大型文本语料库上进行预训练，以培养通用语言理解。预训练通常由专业专家进行，然后通过检查点提供所有参数和模型状态，使预训练模型公开可用。随后，这些模型可以使用较小的专用训练数据集进一步微调，这一任务通常由需要实现深度学习代码并提供应用特定训练数据的最终用户完成。这类LLMs，如BERT或RoBERTa，已成为众多NLP任务中的最先进技术。实证证据表明，微调后的LLMs优于传统方法，如词典、词袋模型或基于词嵌入的方法
    [[13](#bib.bibx13), [24](#bib.bibx24), [45](#bib.bibx45), [11](#bib.bibx11)]。
- en: 'More recently, with models continuously growing in size and complexity, instruction-tuned
    generative AI models such as ChatGPT and Claude Opus have emerged. These models
    are not only significantly larger, but also more versatile than BERT-style models:
    While generative AI models are also pre-trained, they can be directly prompted
    via text commands to perform tasks without the need for a further fine-tuning
    step. As a result, generative AI promises a straightforward and intuitive user
    interaction while challenging the traditional approach, which requires fitting
    a model to labeled data. Although the empirical picture is still far from decisive,
    first studies suggest that generative AI models are already outperforming fine-tuned
    LLMs in text classification tasks [[18](#bib.bibx18), [46](#bib.bibx46)].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着模型规模和复杂性的不断增长，像 ChatGPT 和 Claude Opus 这样的指令调优生成式 AI 模型已出现。这些模型不仅大得多，而且比
    BERT 风格的模型更为多样化：虽然生成式 AI 模型也经过预训练，但可以通过文本命令直接提示执行任务，无需进一步的微调步骤。因此，生成式 AI 提供了一种直接且直观的用户交互方式，同时挑战了传统的方法，该方法需要将模型拟合到标注数据上。尽管实证情况仍远未决定，但初步研究表明，生成式
    AI 模型在文本分类任务中已经超越了微调的 LLM [[18](#bib.bibx18), [46](#bib.bibx46)]。
- en: In this paper, we contribute to the ongoing debate by systematically comparing
    the text classification performance of three prompt-instructed generative AI models
    (ChatGPT-3.5, ChatGPT-4, and Claude Opus) as well as Facebook’s BART with that
    of several smaller, fine-tuned LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过系统比较三种提示指令生成式 AI 模型（ChatGPT-3.5、ChatGPT-4 和 Claude Opus）以及 Facebook
    的 BART 与若干较小、经过微调的 LLM 的文本分类性能，来为持续的辩论做出贡献。
- en: 'Our analysis spans a wide range of text classification tasks, including sentiment
    analysis, approval/disapproval recognition, emotion detection, and identification
    of political party positions. We also evaluate model performance across different
    text categories, such as news articles, tweets, and political speeches. Specifically,
    we conduct four case studies: (i) sentiment analysis of The New York Times coverage
    on the U.S. economy, (ii) stance classification of tweets about Brett Kavanaugh’s
    nomination to the U.S. Supreme Court, (iii) emotion detection in German political
    texts, and (iv) multi-class stance classification of nationalist party positions
    on European integration.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析涵盖了广泛的文本分类任务，包括情感分析、认可/不认可识别、情感检测和政治党派立场识别。我们还评估了模型在不同文本类别中的表现，如新闻文章、推文和政治演讲。具体来说，我们进行四个案例研究：（i）对《纽约时报》关于美国经济的报道进行情感分析，（ii）对有关布雷特·卡瓦诺提名至美国最高法院的推文进行立场分类，（iii）对德语政治文本中的情感进行检测，以及（iv）对民族主义政党在欧洲一体化问题上的多类立场分类。
- en: Our results demonstrate that fine-tuning smaller BERT-style models significantly
    outperforms generative AI models such as ChatGPT and Claude Opus (used in a "zero-shot"
    fashion) across all four applications when moderate amounts of training data for
    fine-tuning are provided. This tendency is especially pronounced for more specialized,
    non-standard classification tasks. Overall, these findings suggest that smaller,
    fine-tuned LLMs (still) constitute the state-of-the-art in text classification.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，在提供适量的微调训练数据时，微调较小的 BERT 风格模型在所有四个应用场景中的表现显著优于生成式 AI 模型（如 ChatGPT 和
    Claude Opus，以“零样本”方式使用）。这种倾向在更专业、非标准分类任务中尤为明显。总体而言，这些发现表明，较小、经过微调的 LLM（仍然）构成了文本分类领域的**最先进技术**。
- en: Our results also imply that model size and complexity are no sufficient substitute
    for application-specific training data. For text classification tasks, the lightweight,
    tailored approach of fine-tuning small LLMs via training data remains preferable
    to the heavy-duty, one-size-fits-all approach of zero-shot prompting generative
    AI models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果还表明，模型的规模和复杂性不足以替代特定应用的训练数据。在文本分类任务中，通过训练数据微调小型 LLM 的轻量化定制方法仍然优于零样本提示生成式
    AI 模型的重型“一刀切”方法。
- en: Given our clear-cut results in favor of fine-tuning, we make available an easy-to-use
    toolkit with this paper. Presented as a simple Jupyter Notebook built on top of
    Hugging Face, our toolkit simplifies the process of fine-tuning LLMs, which typically
    requires deep learning and programming experience. It allows users to select and
    fine-tune smaller pre-trained LLMs for any classification problem (e.g., sentiment)
    or text category (e.g., news) with minimal requirements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们明确支持微调的结果，我们随本文提供了一个易于使用的工具包。该工具包以Hugging Face为基础，呈现为一个简单的Jupyter Notebook，简化了微调LLMs的过程，这通常需要深度学习和编程经验。它允许用户选择并微调较小的预训练LLMs，以解决任何分类问题（例如情感分析）或文本类别（例如新闻），并且要求最低。
- en: Our toolkit supports different languages and handles both binary and non-binary
    classification problems. It includes pre-implemented methods to address class
    imbalance, common in many applications (e.g., news texts typically have more negative
    than positive sentiment). While the notebook allows for the configuration and
    optimization of deep-learning hyperparameters, it comes with default hyperparameters
    that deliver strong performance out-of-the-box. Computationally intensive hyperparameter
    tuning is therefore an option but not required. The modular design of the toolkit
    allows for the integration of additional/future model releases. We further provide
    a detailed documentation and step-by-step user guidance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工具包支持多种语言，并处理二分类和非二分类问题。它包括预实现的方法来解决类不平衡问题，这在许多应用中很常见（例如，新闻文本通常包含更多的负面情感而不是正面情感）。尽管笔记本允许配置和优化深度学习超参数，但它提供了默认的超参数，这些默认值开箱即用时能够提供强大的性能。因此，计算密集型的超参数调优是一个可选项，但不是必需的。工具包的模块化设计允许集成额外的或未来的模型发布。我们还提供了详细的文档和逐步的用户指导。
- en: To further facilitate use and adoption of fine-tuning, we provide a non-technical
    introduction to the functioning principles of LLMs, clarify how these models expand
    on earlier text-as-data methods, and explain why this results in significant performance
    gains (Section 3). We also discuss current research areas, potential future trends
    in NLP and AI, and the opportunities of few-shot learning (Section 6).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步促进微调的使用和采纳，我们提供了对LLMs（大型语言模型）工作原理的非技术性介绍，阐明这些模型如何扩展早期的文本数据方法，并解释为何这会带来显著的性能提升（第3节）。我们还讨论了当前的研究领域、NLP和AI中的潜在未来趋势，以及少量样本学习的机会（第6节）。
- en: 2 Related Work and Contribution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作与贡献
- en: A growing literature evaluates and compares the performance of fine-tuned and
    prompt-based LLMs [[5](#bib.bibx5), [6](#bib.bibx6), [8](#bib.bibx8), [10](#bib.bibx10),
    [41](#bib.bibx41), [15](#bib.bibx15), [18](#bib.bibx18), [47](#bib.bibx47), [46](#bib.bibx46)].
    However, some of this work does not specifically address text classification,
    which is the primary focus of our paper, but instead concentrates on various other
    text comprehension tasks [[6](#bib.bibx6), [41](#bib.bibx41)]. Other studies primarily
    compare the performance of different fine-tuning methods [[10](#bib.bibx10)],
    or evaluate various zero-shot approaches against each other [[47](#bib.bibx47)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不断增长的文献评估和比较了微调的LLMs和基于提示的LLMs的性能[[5](#bib.bibx5), [6](#bib.bibx6), [8](#bib.bibx8),
    [10](#bib.bibx10), [41](#bib.bibx41), [15](#bib.bibx15), [18](#bib.bibx18), [47](#bib.bibx47),
    [46](#bib.bibx46)]。然而，其中一些工作并未专门针对文本分类，这也是我们论文的主要关注点，而是集中于各种其他文本理解任务[[6](#bib.bibx6),
    [41](#bib.bibx41)]。其他研究主要比较了不同微调方法的性能[[10](#bib.bibx10)]，或评估了各种零样本方法的效果[[47](#bib.bibx47)]。
- en: The existing studies that come closest to our work, as they compare fine-tuned
    and prompt-based models for text classification tasks, are [[5](#bib.bibx5)] and
    [[15](#bib.bibx15)] as well as [[46](#bib.bibx46)] and [[18](#bib.bibx18)]. However,
    these papers provide conflicting evidence as to which approach is superior for
    text classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的工作最接近的现有研究是[[5](#bib.bibx5)]和[[15](#bib.bibx15)]，以及[[46](#bib.bibx46)]和[[18](#bib.bibx18)]，因为这些研究比较了针对文本分类任务的微调模型和基于提示的模型。然而，这些论文提供了相互矛盾的证据，无法确定哪种方法在文本分类中更优。
- en: For example, [[15](#bib.bibx15)] compare a fine-tuned RoBERTa model with prompt-based
    models like GPT-3.5 and Meta’s Llama models across different classification tasks.
    The authors find that fine-tuned models (trained on the entire dataset) generally
    outperform prompt-based models. Similarly, [[5](#bib.bibx5)] compare the performance
    of RoBERTa and GPT-3 in a study on an English-language dataset of parliamentary
    speeches, concluding that fine-tuning yields better results.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[15](#bib.bibx15)]比较了微调的RoBERTa模型与基于提示的模型（如GPT-3.5和Meta的Llama模型）在不同分类任务中的表现。作者发现，微调模型（在整个数据集上训练）通常优于基于提示的模型。类似地，[[5](#bib.bibx5)]在对英语议会演讲数据集的研究中比较了RoBERTa和GPT-3的表现，得出微调能带来更好结果的结论。
- en: In contrast, recent work by [[46](#bib.bibx46)] and [[18](#bib.bibx18)] suggests
    that ChatGPT has caught up with or even surpassed fine-tuned models. [[46](#bib.bibx46)]
    find that ChatGPT is on par with BERT-style models for some text understanding
    tasks, though results are mixed overall. [[18](#bib.bibx18)] present results suggesting
    that ChatGPT even outperforms human annotators for text labeling tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，[[46](#bib.bibx46)]和[[18](#bib.bibx18)]的近期研究表明，ChatGPT已经赶上甚至超越了微调模型。[[46](#bib.bibx46)]发现ChatGPT在某些文本理解任务上与BERT风格的模型相当，尽管总体结果不一。[[18](#bib.bibx18)]则呈现了结果，表明ChatGPT在文本标注任务中甚至超越了人工标注员。
- en: Inspired by these works, we aim to provide an up-to-date, systematic empirical
    analysis that offers a comprehensive view on the performance of fine-tuned versus
    prompt-based models for text classification.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受这些工作的启发，我们旨在提供一个最新的、系统的实证分析，提供关于微调与基于提示的模型在文本分类中的表现的全面视角。
- en: 'While our results clearly point in the same direction as the studies by [[5](#bib.bibx5)]
    and [[15](#bib.bibx15)], our paper expands on the existing literature in several
    ways:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的结果明显与[[5](#bib.bibx5)]和[[15](#bib.bibx15)]的研究方向一致，但我们的论文在几个方面扩展了现有文献：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensive Comparison: We systematically compare model performance across
    a diverse set of classification tasks and text categories. This allows us to identify
    performance variations across tasks and highlight areas where the differences
    between fine-tuned and prompt-based models are most (and least) pronounced.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合比较：我们系统地比较了模型在各种分类任务和文本类别上的表现。这使我们能够识别任务之间的表现差异，并突出微调模型与基于提示的模型之间差异最明显（和最不明显）的领域。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Latest Generative AI Models: We provide results for the latest generation of
    prompt-based generative AI models (GPT-4 and Claude Opus). This is crucial, as
    previous studies based on older models like GPT-3 offer limited insights into
    the current performance balance.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最新的生成性AI模型：我们提供了最新一代基于提示的生成AI模型（GPT-4和Claude Opus）的结果。这至关重要，因为基于旧模型（如GPT-3）的先前研究对当前性能平衡提供的见解有限。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Nuanced Understanding of Fine-Tuned LLMs: By investigating the performance
    of several fine-tuned LLMs, we offer a detailed understanding of how these models
    compare across classification tasks. This helps identify which models are better
    suited for specific tasks, thereby aiding user choice. Such a comparison provides
    a broader perspective on the current LLM landscape for text classification.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调LLMs的细致理解：通过研究多个微调LLMs的表现，我们提供了这些模型在分类任务中如何比较的详细理解。这有助于识别哪些模型更适合特定任务，从而帮助用户选择。这种比较提供了当前LLM在文本分类领域的更广泛视角。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ablation Studies and Training Data Impact: We conduct ablation studies and
    analyze the effect of training data size on model performance for fine-tuning.
    This offers valuable insights into the number of labeled samples required to achieve
    optimal performance, a crucial consideration given our consistent findings in
    favor of fine-tuned LLMs.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消融研究与训练数据影响：我们进行消融研究，并分析训练数据量对微调模型性能的影响。这为达到最佳性能所需的标注样本数量提供了宝贵的见解，这在我们对微调LLMs的一致发现中尤为重要。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accessible Toolkit for Fine-Tuning: We offer a one-stop-shop toolkit for text
    classification to enable a wider audience to use pre-trained LLMs for text classification
    tasks.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可访问的微调工具包：我们提供一个一站式文本分类工具包，使更广泛的受众能够使用预训练的LLM进行文本分类任务。
- en: With these contributions, we aim to provide a comprehensive and practical resource
    for understanding and applying LLMs in text classification.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些贡献，我们旨在提供一个全面且实用的资源，以便理解和应用LLMs进行文本分类。
- en: '3 Non-technical Background: From Keywords to Large Language Models'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 非技术背景：从关键词到大型语言模型
- en: This section provides a non-technical introduction to LLMs, which complements
    our empirical results and our provided fine-tuning toolkit. It explains the functioning
    principles of LLMs and the reasons for their superior performance compared to
    earlier methods. To provide a comprehensive overview, we first briefly cover traditional
    hand-coding and dictionary approaches before discussing machine learning methods
    and LLMs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对LLMs的非技术性介绍，这补充了我们的实证结果和提供的微调工具包。它解释了LLMs的工作原理及其优于早期方法的原因。为了提供全面的概述，我们首先简要介绍传统的手工编码和词典方法，然后讨论机器学习方法和LLMs。
- en: Hand-coding is among the earliest text-as-data approaches. When hand-coding,
    Human coders classify verbal information according to pre-specified codebooks
    and explicit coding rules [[12](#bib.bibx12)], [[33](#bib.bibx33)]. Because hand-coding
    leverages human text-understanding, it is relatively easy to perform and yields
    high-quality results. However, hand-coding is labor-intensive — both regarding
    codebook development and the coding process itself. This makes it expensive and
    slow (see Figure 1). Moreover, hand-coding can be inconsistent across coders and
    within coders over time, making it partially non-replicable. High costs, slow
    procedures, and limited reproducibility are thus clear drawbacks of hand-coding
    approaches.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 手工编码是最早的文本数据方法之一。在手工编码时，人工编码人员根据预先指定的编码手册和明确的编码规则对口头信息进行分类[[12](#bib.bibx12)],
    [[33](#bib.bibx33)]。由于手工编码依赖于人类的文本理解，它相对容易执行并且结果高质量。然而，手工编码劳动强度大——无论是在编码手册的开发还是编码过程本身。这使得手工编码既昂贵又缓慢（见图1）。此外，手工编码在编码者之间和编码者自身随时间的变化中可能存在不一致，导致部分不可重复。因此，高成本、缓慢的程序和有限的可重复性是手工编码方法的明显缺点。
- en: Dictionary approaches alleviate some of these concerns. These methods automatically
    extract information from text by mapping a larger set of keywords (or keyword
    combinations) to a small set of categories of interest. For example, sentiment
    dictionaries classify texts according to their content of positive and negative
    words or phrases [[21](#bib.bibx21), [37](#bib.bibx37)]. Dictionaries can be fully
    automated and therefore fast, replicable, and transparent. However, because dictionaries
    only take selected keywords into account, they ignore most nuanced textual information.
    Because words have different meanings in different settings, dictionaries can
    also be context-dependent and noisy when used off-the-shelf. Moreover, dictionary
    construction in itself is a difficult and time-consuming process and requires
    iterative testing to minimize false positives and false negatives. Thus, while
    dictionaries have advantages in terms of speed and replicability, they remain
    a relatively blunt tool.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 词典方法缓解了这些担忧。这些方法通过将较大的关键词集（或关键词组合）映射到一小组感兴趣的类别，从文本中自动提取信息。例如，情感词典根据文本中的积极和消极词汇或短语对文本进行分类[[21](#bib.bibx21),
    [37](#bib.bibx37)]。词典可以完全自动化，因此速度快、可重复且透明。然而，由于词典只考虑选定的关键词，它们忽略了大多数细微的文本信息。由于词汇在不同背景下有不同的含义，词典在即用时也可能具有上下文依赖性和噪声。此外，词典的构建本身是一个困难且耗时的过程，需要反复测试以最小化假阳性和假阴性。因此，尽管词典在速度和可重复性方面具有优势，但它们仍然是相对笨拙的工具。
- en: '![Refer to caption](img/3b7d5d64ae0d2d857f21d612add17929.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3b7d5d64ae0d2d857f21d612add17929.png)'
- en: 'Figure 1: Overview of existing text-as-data methods and their characteristics:
    Machine learning approaches for text classification have the *potential* to combine
    the advantages of hand-coding (high-quality) and dictionaries (speed), while avoiding
    their respective downsides. The degree to which this potential can be realized
    in practice depends on the underlying text representation (see Figure 2).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：现有文本数据方法及其特征概述：机器学习文本分类方法具有结合手工编码（高质量）和词典（速度）优点的*潜力*，同时避免各自的缺点。这个潜力在实际中能否实现，取决于基础文本表示（见图2）。
- en: Machine learning methods follow a different approach. Unlike hand-coding and
    dictionary approaches, which both rely on explicit human-crafted rules, machine
    learning techniques derive implicit rules from human-labeled data. In supervised
    learning, a model extracts classification-relevant patterns from a sample of manually-coded
    (labeled) training data. These patterns are then used to auto-classify the bulk
    of the remaining data [[28](#bib.bibx28), [16](#bib.bibx16), [9](#bib.bibx9)].
    Machine learning methods allow human coders to classify texts based on natural
    human language. This frees coders from having to create abstract rules and instead
    allows them to follow their intuitive language understanding. Humans can thus
    focus on the information content of texts, while the learning algorithm extracts
    predictive regularities from the relation between text features and labels.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法采用不同的方法。与依赖于明确人工制定规则的手工编码和词典方法不同，机器学习技术从人类标记的数据中推导出隐含规则。在监督学习中，模型从一组手动编码（标记）训练数据中提取与分类相关的模式。这些模式随后用于自动分类大量剩余数据[[28](#bib.bibx28),
    [16](#bib.bibx16), [9](#bib.bibx9)]。机器学习方法使人工编码人员能够基于自然语言来分类文本。这使得编码人员不必创建抽象规则，而是能够遵循他们直观的语言理解。因此，人们可以专注于文本的信息内容，而学习算法则从文本特征与标签之间的关系中提取预测规律。
- en: The combination of human-labeled data and automated rule extraction is powerful
    because it can fuse the high quality of human coding with the speed of automated
    approaches (see Figure 1). This combination works best, however, with more sophisticated
    and information-dense numerical language representations. Deep-learning approaches
    based on LLMs are now making it possible to realize this potential. To see this,
    we next describe the evolution of language representations from bag-of-words models
    via word embeddings to LLMs and point out why each step has generated significant
    performance increases in NLP tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人工标记数据与自动规则提取的结合非常强大，因为它可以将人工编码的高质量与自动化方法的速度结合起来（见图1）。然而，这种结合在更复杂和信息密集的数值语言表示中效果最好。基于LLMs的深度学习方法现在使实现这种潜力成为可能。为了理解这一点，我们接下来描述从词袋模型到词嵌入再到LLMs的语言表示演变，并指出每一步为何在NLP任务中带来了显著的性能提升。
- en: 3.1 Bag-Of-Words
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 词袋模型
- en: Early machine learning approaches for automated text analysis rest on bag-of-words
    models [[42](#bib.bibx42)]. These models treat text documents as collections (or
    bags) of words without regard to word order and grammar. This allows representing
    complex language information in a simple spreadsheet format that is easy to work
    with in downstream applications. In this format, rows represent documents, columns
    represent words, and cells record how often a word occurs in a document. Thus,
    a corpus with $D$ matrix. Each row (corresponding to a document) then captures
    the word signature (i.e., word distribution) of the original input text.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的机器学习方法用于自动文本分析基于词袋模型[[42](#bib.bibx42)]。这些模型将文本文档视为单词的集合（或词袋），而不考虑单词的顺序和语法。这允许以简单的电子表格格式表示复杂的语言信息，这种格式易于在后续应用中使用。在这种格式中，行表示文档，列表示单词，单元格记录单词在文档中的出现频率。因此，一个包含$D$矩阵的语料库。每行（对应一个文档）然后捕捉原始输入文本的单词签名（即单词分布）。
- en: While bag-of-word approaches can be powerful, they have notable shortcomings.
    First, they tend to produce large sparse matrices - because most words typically
    do not occur in most documents. This also results in matrices with more columns
    (words = variables) than rows (documents = observations), leading to $$P>
    problems that are unwieldy in regression contexts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词袋模型方法可能很强大，但它们也有显著的不足。首先，这些方法往往会产生大型稀疏矩阵——因为大多数单词通常不会出现在大多数文档中。这也导致了矩阵的列数（单词
    = 变量）多于行数（文档 = 观测值），从而引发在回归环境中不便处理的$$P>问题。
- en: Second, the spreadsheet representation treats all words as equally different
    from each other. For example, bag-of-words representations cannot capture that
    the words ’cake’ and ’cookie’ have more in common than the words ‘cake’ and ‘court’.
    Bag-of-words approaches are oblivious to the meaning of words and cannot reflect
    relations between words.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，电子表格表示将所有单词视为彼此完全不同。例如，词袋表示不能捕捉到单词‘cake’和‘cookie’比单词‘cake’和‘court’更有共同点这一事实。词袋方法对单词的含义无动于衷，无法反映单词之间的关系。
- en: Third, bag-of-words approaches discard all information contained in word order,
    sentence structure, and grammar. This ignores the entire narrative content of
    a text and makes it difficult to retain information beyond the general topic of
    a document. This problem is further aggravated by pre-processing steps such as
    stopword removal, which typically discards negations, and thus discards further
    potentially important information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，词袋模型丢弃了所有关于词序、句子结构和语法的信息。这忽略了文本的整体叙事内容，使得保留超出文档一般主题的信息变得困难。这个问题被进一步加剧，因为预处理步骤如停用词移除通常会丢弃否定词，从而丢弃更多潜在的重要信息。
- en: 3.2 Word Embeddings
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 词嵌入
- en: In contrast to bag-of-words models, word embeddings capture core aspects of
    word meaning and similarity [[26](#bib.bibx26)]. Word embeddings represent words
    as vectors in a high-dimensional space, where one or multiple dimensions can be
    thought of as corresponding to a particular characteristic. For example, one dimension
    might represent the degree of positivity of a word, while another might represent
    its length or its frequency of use. Consequently, similar words are (ideally)
    mapped to vectors that point to a similar location in vector space.¹¹1Word embeddings
    are created by training neural networks to learn co-occurrence patterns of words.
    Well-known word embedding models are Word2Vec [[27](#bib.bibx27)], GloVe [[31](#bib.bibx31)],
    and FastText [[4](#bib.bibx4)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与词袋模型相比，词嵌入捕捉了词义和相似性的核心方面[[26](#bib.bibx26)]。词嵌入将词表示为高维空间中的向量，其中一个或多个维度可以看作是对应于特定特征。例如，一个维度可能代表词的积极程度，而另一个维度可能代表词的长度或使用频率。因此，相似的词（理想情况下）会被映射到在向量空间中指向相似位置的向量上。¹¹词嵌入是通过训练神经网络来学习词的共现模式来创建的。著名的词嵌入模型有Word2Vec[[27](#bib.bibx27)]、GloVe[[31](#bib.bibx31)]和FastText[[4](#bib.bibx4)]。
- en: Word embeddings are a richer (and usually more compact) numerical representation
    of natural language than bags-of-words matrices. This has several advantages.
    First, word embeddings reduce the dimensionality of the language representation,
    which solves the  marks the text placeholder
    for the current text sample. We set temperature=0.1 for all runs to reduce variance
    and force the model to produce a single label output from the provided list of
    labels. We repeat the prompting if the output does not match the labels provided
    for the classification task until the model produces a correct label as output.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 API 尝试了几个提示后，我们对最终提示进行了微调，以最大限度地提高模型只返回给定类标签列表中的一个标签的可能性。由于大语言模型（LLMs）是离散的概率模型，其随机性可能导致生成的输出存在差异。通过控制温度参数并降低其值，我们可以减少生成输出的差异。然而，模型仍然会生成除了给定的输出标签之外的其他内容。在下文中，我们展示了我们提供给生成
    AI API 的提示，其中  表示当前文本样本的文本占位符。我们将温度设置为0.1以减少差异，并迫使模型从提供的标签列表中生成单一标签输出。如果输出与分类任务中提供的标签不匹配，我们会重复提示，直到模型生成正确的标签作为输出。
- en: Sentiment Analysis on The New York Times Coverage of the US Economy
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于《纽约时报》对美国经济报道的情感分析
- en: 'Prompt:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for sentiment
    analysis. Your objective is to classify a given text snippet into one of several
    possible class labels, based on the sentiment expressed in the text. Your output
    should consist of a single class label that best matches the sentiment expressed
    in the text. Your output should consist of a single class label that best matches
    the given text. Choose ONLY from the given class labels below and ONLY output
    the label without any other characters. Text:  Labels: ’Negative Sentiment’,
    ’Positive Sentiment’ Answer:'
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了情感分析的零样本文本分类任务。你的目标是将给定的文本片段分类到几个可能的类标签中的一个，基于文本中表达的情感。你的输出应由一个最符合文本中表达的情感的单一类标签组成。你的输出应由一个最符合给定文本的单一类标签组成。仅从以下给定的类标签中选择，并且仅输出标签而不添加其他字符。文本：
    标签：’负面情感’，’正面情感’ 答案：
- en: Stance Classification on Tweets about Kavanaugh Nomination
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于卡瓦诺提名的推文的立场分类
- en: 'Prompt:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for stance
    classification. Your objective is to classify a given text snippet into one of
    several possible class labels, based on the attitudinal stance towards the given
    text. Your output should consist of a single class label that best matches the
    stance expressed in the text. Your output should consist of a single class label
    that best matches the given text. Choose ONLY from the given class labels below
    and ONLY output the label without any other characters. Text:  Labels: ’negative
    attitudinal stance towards’, ’positive attitudinal stance towards’ Answer:'
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是进行立场分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于对给定文本的态度立场。你的输出应包含一个最能匹配文本中表达的立场的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：
    标签：’对离开要求的负面态度’，’对离开要求的积极态度’ 答案：
- en: Emotion Detection on Political Texts in German
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对德语政治文本中的情感检测
- en: 'Prompt:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for emotion
    classification. Your objective is to classify a given text snippet into one of
    several possible class labels, based on the anger level in the given text. Your
    output should consist of a single class label that best matches the anger expressed
    in the text. Choose ONLY from the given class labels below and ONLY output the
    label without any other characters. Text:  Labels: ’Angry’, ’Non-Angry’
    Answer:'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是对情感进行分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于文本中的愤怒程度。你的输出应包含一个最能匹配文本中表达的愤怒情感的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：
    标签：’愤怒’，’非愤怒’ 答案：
- en: Multi-Class Stance Classification on Parties’ EU Positions
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对政党的欧盟立场的多类别立场分类
- en: 'Prompt:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: 'You have been assigned the task of zero-shot text classification for political
    texts on attitudinal stance towards Brexit and leave demands related to the European
    Union (EU). Your objective is to classify a given text snippet into one of several
    possible class labels, based on the stance towards Brexit and general leave demands
    in the given text. Your output should consist of a single class label that best
    matches the content expressed in the text. Choose ONLY from the given class labels
    below and ONLY output the label without any other characters. Text:  Labels:
    ’Neutral towards Leave demands’, ’Pro-Leave demands’, ’Very Pro-Leave demands’
    Answer:'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你被分配了一个零样本文本分类的任务，目的是对有关脱欧和与欧盟（EU）相关的离开要求的政治文本进行分类。你的目标是将给定的文本片段分类到几个可能的类别标签之一，基于对脱欧和一般离开要求的态度。你的输出应包含一个最能匹配文本中表达的内容的单一类别标签。仅从以下给定的类别标签中选择，并且仅输出标签，不添加其他字符。文本：
    标签：’对离开要求的中立态度’，’支持离开要求’，’非常支持离开要求’ 答案：
- en: 'B: BART'
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'B: BART'
- en: For BART, we use the BART Large model facebook/bart-large-mnli introduced by
    [[23](#bib.bibx23)] together with the Zero-Shot Classification pipeline from Huggingface.
    We use a technique called Natural Language Inference (NLI), which prompts a model
    using two sentences, a Premise (in our case the text to be classified) and a Hypothesis
    (a possible class label for that text). The model then predicts if the hypothesis
    is consistent with the premise. NLI evaluates a hypothesis for each label and
    then selects the label with the highest confidence as output.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BART，我们使用了 BART Large 模型 facebook/bart-large-mnli，该模型由 [[23](#bib.bibx23)]
    介绍，并结合了来自 Huggingface 的零样本分类管道。我们使用了一种称为自然语言推理（NLI）的技术，它通过两个句子来提示模型，一个前提（在我们这种情况下是待分类的文本）和一个假设（该文本的可能类别标签）。然后模型预测假设是否与前提一致。NLI
    对每个标签进行评估，然后选择置信度最高的标签作为输出。
