- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:37:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调企业用的LLM：实用指南和建议
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10779](https://ar5iv.labs.arxiv.org/html/2404.10779)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10779](https://ar5iv.labs.arxiv.org/html/2404.10779)
- en: Mathav Raj J
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Mathav Raj J
- en: HCLTech
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: HCLTech
- en: Bengaluru
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 班加罗尔
- en: mathavraj.j@hcl.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: mathavraj.j@hcl.com
- en: '&Kushala VM'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Kushala VM'
- en: HCLTech
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: HCLTech
- en: Bengaluru
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 班加罗尔
- en: kushala.vm@hcl.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: kushala.vm@hcl.com
- en: '&Harikrishna Warrier'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Harikrishna Warrier'
- en: HCLTech
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: HCLTech
- en: Bengaluru
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 班加罗尔
- en: harikrishna.w@hcl.com
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: harikrishna.w@hcl.com
- en: '&Yogesh Gupta'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yogesh Gupta'
- en: HCLTech
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: HCLTech
- en: Bengaluru
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 班加罗尔
- en: yogeshg@hcl.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: yogeshg@hcl.com
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: There is a compelling necessity from enterprises for fine tuning LLMs (Large
    Language Models) to get them trained on proprietary domain knowledge. The challenge
    is to imbibe the LLMs with domain specific knowledge using the most optimial resource
    and cost and in the best possible time. Many enterprises rely on RAG (Retrieval
    Augmented Generation) which does not need LLMs to be fine-tuned but they are limited
    by the quality of vector databases and their retrieval capabilities rather than
    the intrinsic capabilities of the LLMs themselves. In our current work we focus
    on fine tuning LLaMA, an open source LLM using proprietary documents and code
    from an enterprise repository and use the fine tuned models to evaluate the quality
    of responses. As part of this work, we aim to guide beginners on how to start
    with fine tuning an LLM for documentation and code by making educated guesses
    on size of GPU required and options that are available for formatting the data.
    We also propose pre processing recipes for both documentation and code to prepare
    dataset in different formats. The proposed methods of data preparation for document
    datasets are forming paragraph chunks, forming question and answer pairs and forming
    keyword and paragraph chunk pairs. For code dataset we propose forming summary
    and function pairs. Further, we qualitatively evaluate the results of the models
    for domain specific queries. Finally, we also propose practical guidelines and
    recommendations for fine tuning LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 企业迫切需要微调LLM（大型语言模型），以便将其训练在专有领域知识上。挑战在于利用最优资源和成本以及尽可能短的时间，将领域特定知识融入LLM。许多企业依赖RAG（检索增强生成），这不需要对LLM进行微调，但它们受到向量数据库质量和检索能力的限制，而不是LLM本身的固有能力。在我们当前的工作中，我们专注于微调LLaMA，一个开源LLM，使用来自企业库的专有文档和代码，并使用微调后的模型评估响应的质量。作为这项工作的一个部分，我们旨在指导初学者如何开始为文档和代码微调LLM，通过对所需GPU大小和数据格式化选项做出有根据的猜测。我们还提出了文档和代码的预处理方法，以准备不同格式的数据集。文档数据集的数据准备方法包括形成段落块、形成问答对和形成关键字与段落块对。对于代码数据集，我们建议形成摘要与函数对。此外，我们还定性评估了模型在领域特定查询中的结果。最后，我们还提出了微调LLM的实际指南和建议。
- en: '*K*eywords Fine tuning guidelines  $\cdot$ Document dataset'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 微调指南  $\cdot$ 文档数据集'
- en: 1 Introduction
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of LLMs has revolutionised natural language processing. Applications
    are varying from language translation [[1](#bib.bib1)], content creation [[2](#bib.bib2)]
    and to emotional support chatbots [[3](#bib.bib3)]. LLMs like LLaMA have been
    trained on trillions of tokens[[4](#bib.bib4)] from various resources. To adapt
    a general purpose LLM for one of these specific tasks, it has to be trained on
    task oriented dataset. This additional training allows the model to fine tune
    its parameters to the task or domain we are interested in. Models like FinGPT
    for finance domain [[5](#bib.bib5)], PMC-LLaMA for medical domain [[6](#bib.bib6)]
    are fine tuned on particular domain datasets to achieve improved accuracy on domain
    related questions. Domain specific LLMs can be helpful in scenarios such as support
    ticket resolution, querying document base or code repository to adapt into new
    system etc. Though there is an option to use OpenAI models to solve most of the
    use-cases, there is a high demand for domain specific LLMs due to data privacy
    and pricing concerns. The stake holder’s dataset can stay on premise as the LLMs
    are also present on premise. Fine-tuned LLMs provide quality and custom feel to
    the stake holder and also has low latency in displaying the results.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现彻底改变了自然语言处理。应用范围从语言翻译[[1](#bib.bib1)]、内容创作[[2](#bib.bib2)]到情感支持聊天机器人[[3](#bib.bib3)]。像
    LLaMA 这样的 LLM 已在来自各种资源的万亿级 token[[4](#bib.bib4)] 上进行训练。为了将通用 LLM 适配到这些特定任务之一，必须在任务导向的数据集上进行训练。这种额外的训练使模型能够将其参数微调到我们感兴趣的任务或领域。像
    FinGPT 这样的金融领域模型[[5](#bib.bib5)]、PMC-LLaMA 这样的医疗领域模型[[6](#bib.bib6)] 都是在特定领域数据集上进行微调的，以在领域相关问题上提高准确性。领域特定的
    LLM 在支持票务解决、查询文档库或代码库以适应新系统等场景中可以发挥作用。尽管可以使用 OpenAI 模型来解决大多数用例，但由于数据隐私和定价问题，对领域特定
    LLM 的需求很高。利益相关者的数据集可以保留在本地，因为 LLM 也可以在本地运行。微调后的 LLM 为利益相关者提供了高质量和定制的体验，并且在展示结果时具有低延迟。
- en: 'This paper aims to enable a beginner in preparing the data for fine tuning,
    estimating the compute capability and memory needed, choosing the right dataset
    format and the optimal configurations for fine tuning. The paper is arranged as
    follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在帮助初学者准备微调数据，估算所需的计算能力和内存，选择合适的数据集格式及微调的最佳配置。文章安排如下：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Research Background: A short survey on related research work on fine tuning
    vs RAG, fine tuning guidelines, efficient techniques for fine tuning and preparation
    of datasets for fine tuning'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究背景：简要综述了微调与 RAG、微调指南、高效微调技术及微调数据集准备方面的相关研究工作。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine tuning configurations: Before starting the fine tuning process it is necessary
    to understand what configurations can be adjusted to run on available resources.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调配置：在开始微调过程之前，需要了解可以调整哪些配置以适应可用资源。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proposed Dataset formats for Text and Code: The overall workflow of the proprietary
    data fine tuning is detailed in this section. The different proposed formats of
    text and code data has been explained in detail'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本和代码的建议数据集格式：本节详细介绍了专有数据微调的整体工作流程。不同的文本和代码数据格式已被详细解释。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Experiments: A proprietary document and code repository is used to showcase
    the fine tuning work flow. Empirical studies have been conducted to understand
    the effects of quantization on time and memory, to understand the selection of
    appropriate rank in LORA (Low Rank Adapater) fine tuning, understand the memory
    requirements for full model fine tuning and to understand the effects of fine
    tuned model in a Retrieval Augmented Generation (RAG) pipeline'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验：使用专有文档和代码库展示微调工作流程。通过实证研究，了解量化对时间和内存的影响，理解在 LORA（低秩适配器）微调中选择合适秩的标准，了解完整模型微调的内存要求，并探究微调模型在检索增强生成（RAG）管道中的效果。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Guidelines: In the final section, practical guidelines for fine tuning has
    been given. Some tips to choosing right parameters for fine tuning efficient techniques
    like LORA is also listed.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指南：在最后一节中，提供了微调的实际指南。还列出了一些选择合适参数和高效技术（如 LORA）的提示。
- en: 2 Research Background
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 研究背景
- en: The authors in [[7](#bib.bib7)] give an overall view of the recent trends in
    LLMs. LLMs can be trained in different phases. The first phase being the pretraining
    with defined objectives such as causal language modelling, masked language modelling,
    span denoising objective etc. Then comes the transfer learning phase which is
    further classified as feature based transfer and finetuning approach. Transfer
    learning is required when the dataset is inadequate for a complete training from
    scratch. Therefore pretrained weights are used as starting point. In feature based
    transfer learning features obtained from the pretrained model for the given domain
    dataset are used by another smaller model to train. Meanwhile finetuning on a
    dataset is to nudge the pretrained weights on a particular task oriented dataset.
    Depending on the how many layers are fine tuned and how prompts are handled during
    finetuning, it is further classified as adapter tuning, gradual unfreezing, prompt
    tuning etc.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7](#bib.bib7)] 中的作者对 LLMs 最近的趋势进行了总体介绍。LLMs 可以在不同阶段进行训练。第一阶段是具有明确目标的预训练，如因果语言建模、掩码语言建模、跨度去噪目标等。然后进入迁移学习阶段，进一步分为基于特征的迁移和微调方法。当数据集不足以从头开始完整训练时，需要迁移学习。因此，使用预训练的权重作为起点。在基于特征的迁移学习中，利用从预训练模型中获得的特征来训练另一个较小的模型。与此同时，在特定任务导向的数据集上进行微调，以微调预训练权重。根据微调的层数和在微调过程中如何处理提示，它进一步分类为适配器调优、渐进解冻、提示调优等。'
- en: An alternate approach to fine tuning is to chunk the documents, convert to embeddings
    and store it in a vector database for retrieval using similarity search with the
    query and use the pretrained LLM to come up with a consolidated answer from the
    retrieved documents. [[8](#bib.bib8)]. This is the production ready approach as
    it is fast and gives more or less exact results. However the RAG approach can
    be crippled by a not so good retrieval mechanism. Though there are simple alleviations
    like the claim in [[9](#bib.bib9)] that information retrieval in RAG has improved
    by purposeful addition of noisy documents, the quality of answers are limited
    by the similarity search which has nothing to do with the LLM capability itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的另一种方法是将文档分块，转换为嵌入，并将其存储在向量数据库中，通过相似性搜索与查询进行检索，并使用预训练的 LLM 从检索的文档中得出综合答案 [[8](#bib.bib8)]。这种方法在生产中已经准备好，因为它快速且结果大致准确。然而，RAG
    方法可能会受到检索机制不佳的影响。尽管 [[9](#bib.bib9)] 中的声明称通过有目的地添加噪声文档，RAG 中的信息检索已得到改善，但答案的质量仍受限于与
    LLM 能力本身无关的相似性搜索。
- en: A study by [[10](#bib.bib10)] shows the comparison of between finetuning and
    RAG. The experiments in the paper reveal that finetuning on a domain data extracted
    from agriculture journals have given more succinct and accurate responses than
    a RAG pipeline. That being said, the authors also have ackowledged the high initial
    cost required to fine tune a model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[[10](#bib.bib10)] 的研究显示了微调和 RAG 之间的比较。论文中的实验揭示了从农业期刊中提取的领域数据进行微调比 RAG 流水线提供了更简洁和准确的回应。尽管如此，作者也承认了微调模型所需的高初始成本。'
- en: Low Rank Adaptation (LORA) finetuning of LLMs [[11](#bib.bib11)] has opened
    up a whole new possibility of finetuning limited number of essential parameters
    usually of the order of few thousands to a millions instead of the entire parameters
    which is in the order of billions. The work on quantizing LLMs has opened up avenues
    for resource deficient systems to train at low memory cost [[12](#bib.bib12)].
    Papers on quantized LLMs in combination with parameter efficient techniques like
    LORA have further enabled obtaining satisfactory results with low resources[[13](#bib.bib13)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩适应（LORA）微调 LLMs [[11](#bib.bib11)] 开辟了微调有限数量的核心参数的全新可能性，通常是几千到几百万个参数，而不是数十亿的全部参数。对
    LLMs 量化的研究为资源不足的系统以低内存成本进行训练开辟了新途径 [[12](#bib.bib12)]。结合 LORA 的量化 LLMs 的论文进一步使得以低资源获得令人满意的结果成为可能
    [[13](#bib.bib13)]。
- en: Code generation with LLM is the most attractive task engineers are looking at.
    Even though LLM are already trained with lots of data which makes them to generate
    code depending on the input, the challenge is generating code about a specific
    enterprise domain code. LLM fine-tuning for a specific task makes the model utilize
    its capacity to fullest by making it adapt to a domain by making the model familiar
    to jargons, domain terminology by understanding the context of the code, class,
    functions, exceptions, libraries etc., Fine-tuning also helps in adapting the
    model to address the task specific problems. Fine-tuning large language models
    for code-related tasks presents a myriad set of challenges that must be carefully
    addressed to ensure optimal performance and reliability. The challenges encompass
    aspects such as data quality and quantity, domain-specific understanding, tokenization
    and vocabulary, contextual understanding, code generation quality, code understanding
    vs. generation, model size and computational resources, overfitting and generalization,
    evaluation metrics, and ethical and security concerns.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 进行代码生成是工程师们最感兴趣的任务之一。尽管 LLM 已经通过大量数据进行训练，使其能够根据输入生成代码，但挑战在于生成特定企业领域的代码。针对特定任务进行
    LLM 微调可以使模型充分发挥其能力，通过使模型熟悉术语、领域术语、理解代码、类、函数、异常、库等的上下文，从而使模型适应某个领域。微调还有助于让模型适应任务特定的问题。为代码相关任务微调大型语言模型面临一系列挑战，这些挑战必须仔细解决以确保最佳性能和可靠性。这些挑战涉及数据质量和数量、领域特定理解、分词和词汇、上下文理解、代码生成质量、代码理解与生成、模型规模和计算资源、过拟合与泛化、评估指标以及伦理和安全问题。
- en: 3 Fine Tuning LLMs on Available Resources
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 微调 LLM 以适应可用资源
- en: 3.1 Quantization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化
- en: By default, most open source LLM weights are released in full 32 bit floating
    point precision. Even for fine tuning a model of relatively smaller size say 7
    billion parameters, nearly 28 GB space is required. With higher precision weights,
    compute units have to spend higher energy in memory movement operations during
    fine tuning [[14](#bib.bib14)]. Quantization is the process of constraining an
    input from continuous set of values to a discrete set. Quantizing the model weights
    to a lower precision and fine tuning greatly reduces the size without hampering
    the quality [[12](#bib.bib12)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，大多数开源 LLM 权重以完整的 32 位浮点精度发布。即使是对相对较小的模型进行微调，比如 70 亿参数，也需要近 28 GB 的空间。使用更高精度的权重时，计算单元在微调期间必须在内存移动操作上消耗更多能量[[14](#bib.bib14)]。量化是将输入从连续值集约束到离散值集的过程。将模型权重量化到较低精度并进行微调，可以大大减少模型大小而不会影响质量[[12](#bib.bib12)]。
- en: 'Data is stored in different numeric types namely: FP32, FP16, int8, FP8, BF16\.
    Integers can be represented in unsigned or signed format or 2’s complement form.
    To represent a fractional decimal we could go for a fixed point representation.
    To further extend the range, systems use the IEEE 754 floating point representation
    which has a much larger range since the numbers are expressed in exponents of
    2\. The 32 bits of floating point representation has three parts namely 1 sign
    bit, 8 bits for exponent (both positive and negative) and 23 bits for mantissa
    or significant figures. The width of the exponent bits determines the range of
    numbers and the width of the mantissa bits determines the precision of numbers.
    Based on the widths there are different forms. Different forms may be needed based
    on the availability of memory resources. FP16 reduces both the range and precision
    by using 5 bits for exponent and 10 bits for mantissa. Brain float 16[[15](#bib.bib15)]
    or BF16 maintains the same range as FP32 but reduction of precision to 7 bits.
    Floating point 16 or FP16 enables training of larger models or training with larger
    mini-batches.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以不同的数字类型存储，即：FP32、FP16、int8、FP8、BF16。整数可以以无符号或有符号格式或 2 的补码形式表示。为了表示小数，我们可以使用定点表示。为了进一步扩展范围，系统使用
    IEEE 754 浮点表示，因为数值以 2 的指数形式表示，范围更大。32 位浮点表示有三部分，即 1 位符号位、8 位指数位（正负）和 23 位尾数或有效数字。指数位的宽度决定了数值的范围，而尾数位的宽度决定了数值的精度。根据位宽，有不同的形式。根据内存资源的可用性，可能需要不同的形式。FP16
    通过使用 5 位指数位和 10 位尾数位来减少范围和精度。Brain float 16[[15](#bib.bib15)] 或 BF16 维持与 FP32
    相同的范围，但将精度减少到 7 位。浮点 16 或 FP16 使得训练更大的模型或使用更大的小批量数据成为可能。
- en: With neural networks, quantization can be done with the weights during storage
    and activation during the computation. Quantization (QAT) and post training quantization
    (PTQ). Integer quantization favours memory reduction and thereby energy and cost
    reduction during inference [[14](#bib.bib14)]. In a QAT scheme proposed by Jacob
    et al. [[16](#bib.bib16)], the quantization errors are included in the computation
    graph during fine tuning so that the model’s inference performance can be as if
    it were never quantized. This allows for deployment of models in edge hardware
    that support only integer arithmetic.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，量化可以在存储时对权重进行，或者在计算时对激活进行。量化（QAT）和训练后量化（PTQ）。整数量化有助于减少内存，从而在推理过程中降低能源和成本[[14](#bib.bib14)]。在Jacob等人提出的QAT方案中[[16](#bib.bib16)]，量化误差被包含在计算图中，以便在微调时模型的推理性能可以如同从未量化过一样。这使得可以在仅支持整数算术的边缘硬件上部署模型。
- en: Tim Dettmers et al. propose a new type of integer quantization scheme called
    LLM int8 [[12](#bib.bib12)]. This has been implemented in the python library ’bitsandbytes’.
    To explain in a more detailed manner, quantization is a two step process that
    involves i) Finding the normalization constant and scale the vector into target
    range ii) Rounding off to the nearest value in target range. During matrix multiplication
    of tensors, quantization of weights with outliers will lead to huge quantization
    loss. To mitigate this, bitsandbytes employs a combination of vector wise quantization
    and mixed precision decomposition to achieve a performance similar to that without
    quantization. Though LLM int8 quantization does not degrade performance, the inference
    time gets increased due to the overhead of quantization [[17](#bib.bib17)]. However
    the memory gets reduced drastically by 71%, which is major cost saving when choosing
    cloud premise GPUs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Tim Dettmers等人提出了一种新的整数量化方案，称为LLM int8[[12](#bib.bib12)]。该方案已在Python库’bitsandbytes’中实现。更详细地说，量化是一个两步过程，包括
    i) 寻找归一化常数并将向量缩放到目标范围 ii) 将值四舍五入到目标范围内的最近值。在张量的矩阵乘法过程中，带有异常值的权重量化将导致巨大的量化损失。为了减轻这一点，bitsandbytes采用了向量级量化和混合精度分解的组合，以实现类似于无量化的性能。尽管LLM
    int8量化不会降低性能，但由于量化的开销，推理时间会增加[[17](#bib.bib17)]。然而，内存大幅减少71%，在选择云端GPU时可以节省大量成本。
- en: 3.2 Gradient Accumulation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 梯度累积
- en: Other than quantization, techniques like gradient accumulation help in reducing
    the memory requirement during fine tuning. The process of accumulating gradients
    in the context of backpropagation involves a strategic approach to parameter updates
    within a neural network during the training phase. Unlike the conventional method
    where parameters are updated after processing each mini-batch, the accumulation
    of gradients entails deferring the parameter updates until all the instances in
    a mini-batch have been processed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了量化，像梯度累积这样的技术有助于在微调过程中减少内存需求。梯度累积的过程涉及在训练阶段内对神经网络中参数更新的战略性方法。与传统方法在处理每个小批量后更新参数不同，梯度累积意味着将参数更新推迟到处理完所有小批量实例后。
- en: In the standard back propagation algorithm, the gradients computed for each
    instance in a mini-batch are typically used to immediately update the model parameters.
    However, in the case of accumulated gradients, these individual gradients are
    not immediately applied to the parameters. Instead, they are summed or averaged
    over the entire mini-batch. As each instance in the mini-batch undergoes forward
    and backward passes, the gradients with respect to the model parameters are computed
    but not immediately applied. These gradients are stored, and the accumulation
    occurs over the entire mini-batch. Only when all instances in the mini-batch have
    been processed, the accumulated gradients are employed to update the model parameters.
    This aggregated update is akin to the effect of utilizing a higher batch size
    for training the neural network.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的反向传播算法中，通常会使用小批量中每个实例计算出的梯度来立即更新模型参数。然而，在梯度累积的情况下，这些单独的梯度不会立即应用到参数上。相反，它们会在整个小批量上进行求和或取平均。由于小批量中的每个实例都经过前向和反向传播，关于模型参数的梯度被计算出来但不会立即应用。这些梯度会被存储，并在整个小批量中进行累积。只有当小批量中的所有实例都处理完毕后，累积的梯度才会用于更新模型参数。这种聚合更新类似于使用更大批量进行神经网络训练的效果。
- en: 3.3 PEFT (Parameter Efficient Fine Tuning)
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 PEFT（参数高效微调）
- en: 'Large language models get more efficient with transfer learning through fine
    tuning. But on the other hand, fine tuning becomes challenging with respect to
    the infrastructure needed, time required and overall memory needs. To overcome
    these challenges, parameter efficient fine tuning comes into picture. Parameter-efficient
    Fine-tuning (PEFT) is a technique used in Natural Language Processing to improve
    the performance of pre-trained language models on specific downstream tasks. It
    involves reusing the pre-trained model’s parameters and fine-tuning them on a
    smaller dataset, which saves computational resources and time compared to training
    the entire model from scratch. PEFT achieves this efficiency by freezing some
    of the layers of the pre-trained model and only fine-tuning the last few layers
    that are specific to the downstream task. There are many methods in PEFT training:
    Adapter, LoRA, QLoRA, Prefix tuning, Prompt tuning, P-tuning, and IA3\. In this
    paper, we will be delving on LoRA and QLoRA methodology of training.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型通过微调在迁移学习中变得更加高效。但另一方面，微调在基础设施需求、所需时间和整体内存需求方面变得具有挑战性。为了克服这些挑战，参数高效微调应运而生。参数高效微调（PEFT）是一种用于自然语言处理的技术，旨在提高预训练语言模型在特定下游任务上的性能。它涉及重用预训练模型的参数，并在较小的数据集上进行微调，与从头训练整个模型相比，这节省了计算资源和时间。PEFT
    通过冻结预训练模型的一些层，并仅微调与下游任务相关的最后几层来实现这种效率。在PEFT训练中有许多方法：Adapter、LoRA、QLoRA、Prefix
    tuning、Prompt tuning、P-tuning 和 IA3。本论文将深入探讨 LoRA 和 QLoRA 的训练方法。
- en: LoRA, [[11](#bib.bib11)] a method for fine-tuning large language models, operates
    by integrating small trainable submodules alongside feed-forward layers in the
    pre-trained transformer architecture. These LoRA modules employ rank decomposition
    to significantly reduce the number of trainable parameters while maintaining or
    even enhancing model performance across various tasks. Specifically, LoRA inserts
    two feed-forward layers adjacent to each feed-forward layer in the transformer
    model, where the first layer projects the input into a lower-dimensional space
    and the second layer restores it to the original dimensionality. This incremental
    change, represented as delta h, is added to the original hidden representation,
    resulting in an updated representation h’. Through this approach, task-specific
    parameters are minimized, facilitating efficient task-switching and reducing hardware
    requirements without introducing additional inference latency.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA，[[11](#bib.bib11)] 一种用于微调大型语言模型的方法，通过在预训练的变换器架构中整合小型可训练子模块和前馈层来操作。这些LoRA模块使用秩分解来显著减少可训练参数的数量，同时保持或甚至提升模型在各种任务上的性能。具体来说，LoRA
    在变换器模型的每个前馈层旁边插入两个前馈层，第一个层将输入投影到较低维度的空间，第二个层将其恢复到原始维度。这种增量变化，表示为 delta h，添加到原始隐藏表示中，从而得到更新后的表示
    h’。通过这种方法，任务特定的参数被最小化，从而促进高效的任务切换，并减少硬件需求，而不会引入额外的推理延迟。
- en: QLoRA, or quantized LORA, is an optimized version of LoRA, where the precision
    of the weight parameters are reduced to 4 bit precision. Since QLoRA shrinks the
    model size due to the reduced precision, it is helpful in scenarios where there
    is limited memory to fine tune.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA，即量化LoRA，是LoRA的优化版本，其中权重参数的精度降低到4位精度。由于QLoRA由于精度降低而缩小了模型大小，它在内存有限的情况下进行微调时特别有用。
- en: 4 Fine Tuning Workflow
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 微调工作流程
- en: The workflow for fine tuning an LLM, be it for text or code, can be represented
    as in Figure 1 below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM，无论是文本还是代码，其工作流程可以如下面的图1所示。
- en: '![Refer to caption](img/799b6d3a7138d34389746d8bd644f5f2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/799b6d3a7138d34389746d8bd644f5f2.png)'
- en: 'Figure 1: Fine tuning Workflow (with LLaMA model as an example)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：微调工作流程（以 LLaMA 模型为例）
- en: Unstructured text or code is fed to the first stage of fine tuning, the pre-processing
    recipe. Based on the recipe, it chunks the data into logical parts and then it
    is tokenized (and padded where needed) so that it accomodates the supported sequence
    lenght of the model. Further, the LoRA / QLoRA configurations are applied and
    fine tuning is done till the loss is minimized. This is the standard process of
    fine tuning. Thus, we can see that the key step in fine tuning is the data pre-processing,
    and the quality of fine tuning is primarily dependent on that. So, we will describe
    that in more detail below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化文本或代码被送到微调的第一阶段，即预处理方案。根据方案，它将数据划分为逻辑部分，然后对其进行分词（并在需要时填充），以便符合模型支持的序列长度。此外，应用LoRA
    / QLoRA配置，并进行微调，直到损失最小化。这是微调的标准过程。因此，我们可以看到，微调中的关键步骤是数据预处理，微调的质量主要取决于此。因此，我们将详细描述这一点。
- en: 4.1 Text Data Pre-Processing
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 文本数据预处理
- en: This step plays a vital role in fine tuning process. From scratch, a Large language
    model is trained to do next token prediction. The datasets are massive text corpus
    like Common crawl, The Pile and code repositories from Github. Further the model
    can be fine tuned for specific tasks using specialised datasets. One such series
    of datasets are called Instruction datasets like Dolly, Orca, Alpaca and Vicuna.
    All these datasets enable instruction fine tuning. Instruction fine tuning bridges
    the gap between next token prediction and user’s requirement of following instruction
    prompts[[18](#bib.bib18)]. An instruction dataset consists of instruction prompt,
    response paired with or without context for additional information. The challenge
    is how to create an instruction dataset from a general document.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤在微调过程中发挥着至关重要的作用。从零开始，大型语言模型被训练进行下一个标记预测。这些数据集是大量的文本语料库，如Common Crawl、The
    Pile和Github上的代码库。进一步地，模型可以使用专门的数据集进行微调。其中一系列数据集称为指令数据集，如Dolly、Orca、Alpaca和Vicuna。所有这些数据集都支持指令微调。指令微调弥合了下一个标记预测与用户要求的指令提示之间的差距[[18](#bib.bib18)]。一个指令数据集包括指令提示、响应，可能带有或不带有额外信息的上下文。挑战在于如何从一般文档中创建一个指令数据集。
- en: As part of our experiments, we developed fine tuning datasets pre-processing
    recipes of four different formats namely a) raw data b) keywords as instruction
    c) headings as instruction and d) queries as instruction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实验的一部分，我们开发了四种不同格式的微调数据集预处理方案，分别是 a) 原始数据 b) 关键词作为指令 c) 标题作为指令和 d) 查询作为指令。
- en: For a decoder model like LLaMA 2, the raw format is essentially continuing the
    initial training objective of unsupervised next token prediction with raw text.
    In the raw data method, we are passing the document as raw chunks with no template.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像LLaMA 2这样的解码器模型，原始格式本质上是继续无监督的下一个标记预测的初始训练目标。在原始数据方法中，我们将文档作为原始块传递，没有模板。
- en: In the keywords method, we are passing the chunks in a template with prompt
    being keywords extracted from the chunk. For this, we used Rapid Automatic Keyword
    Extraction (RAKE) algorithm (the corresponding python library being rake-nltk[[19](#bib.bib19)]).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键词方法中，我们将块传递到一个模板中，提示为从块中提取的关键词。为此，我们使用了快速自动关键词提取（RAKE）算法（对应的Python库为rake-nltk[[19](#bib.bib19)]）。
- en: Another method is to use the document headings with different headings levels
    as prompt. As an example, in cases of Microsoft Word docx documents, the document
    parser from llm search[[20](#bib.bib20)] is used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用不同级别的文档标题作为提示。例如，在Microsoft Word docx文档的情况下，使用来自llm search[[20](#bib.bib20)]的文档解析器。
- en: The last methods that we used is a query based approach. This is particularly
    usefuly when a user is interested in querying for information about the document
    from the LLM, hence it is ideal if the prompt is also having queries. The same
    model is used to generate possible queries that can be asked about a chunk and
    then the dataset is prepared. A single chunk can have multiple queries to promote
    diversity in the dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的最后一种方法是基于查询的方法。这在用户有意查询有关文档的信息时特别有用，因此如果提示中也包含查询内容，那是理想的。相同的模型被用来生成可能针对某个块提问的查询，然后准备数据集。一个块可以有多个查询，以促进数据集的多样性。
- en: When data set is not in a paired format, the challenge is how to structure the
    data and let the model understand the given text. As a baseline, we split the
    text into chunks of fixed length and trained the model with those chunks. The
    objective of fine tuning the model is next token prediction. As a first step the
    model encodes the splitted chunks into embeddings. LLaMA tokenizer is a Byte Pair
    Encoding tokenizer model based on sentencepiece. Once the words are tokenized,
    embedded and padded to appropriate fixed sequence length, the data is ready to
    be fed to model for fine tuning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不是配对格式时，挑战在于如何结构化数据并让模型理解给定的文本。作为基准，我们将文本分割成固定长度的块，并用这些块训练模型。微调模型的目标是下一个标记预测。第一步，模型将分割的块编码成嵌入。LLaMA
    分词器是基于 sentencepiece 的字节对编码分词模型。一旦词语被分词、嵌入并填充到适当的固定序列长度，数据就可以准备好输入模型进行微调。
- en: In Fine tuning a paired dataset format, the input ids will be the input+output+pad
    tokens and labels will be the ignore tokens+output+pad tokens. Here we have essentially
    masked the length of input tokens and it will not be considered while calculating
    the loss. The input ids and labels for the transformer are the same except that
    in case of input ids the padding token is  and for labels it is -100.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调配对数据集格式中，输入 id 将是输入+输出+填充标记，标签将是忽略标记+输出+填充标记。在这里，我们实际上掩盖了输入标记的长度，在计算损失时不会考虑。变换器的输入
    id 和标签是一样的，区别在于输入 id 的填充标记是 ，而标签的填充标记是 -100。
- en: The drawback in this approach is that the structure in the document such as
    headings and topical information might not be present within a chunk. The overlap
    option in Langchain splitters helps with this issue to an extent. However some
    topical information are too long to be captured by just overlapping.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是文档中的结构，如标题和主题信息，可能在一个块中不存在。Langchain 切分器中的重叠选项在一定程度上帮助解决了这个问题。然而，一些主题信息过长，仅通过重叠无法捕捉。
- en: 4.2 Code Data Pre-processing
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 代码数据预处理
- en: LLM models are trained such that higher the context, detailing and prompting
    higher the results. When LLM are trained for code generation task it is very important
    to make the model understand the domain with quality along with quantity information.
    Researchers have been exploring the best possible way to reduce the amount of
    effort required during the preparation of the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 模型的训练目标是上下文越多、细节越丰富、提示越高，结果越好。当 LLM 被训练用于代码生成任务时，重要的是让模型理解领域内的质量信息和数量信息。研究人员一直在探索减少准备数据所需的努力的最佳方法。
- en: In our experiment we have considered three different ways of preparing the training
    data. They are a) Summary method b) Metadata method and c) Tokenization method
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们考虑了三种不同的准备训练数据的方法。它们是 a) 摘要方法 b) 元数据方法 和 c) 分词方法
- en: The first method involves splitting the code at a class level or functional
    level code. The functional level code is considered as a source for preparing
    the data. The entire code repository is split into function level code. The functional
    level code is fed into the instruct model to generate the summaries by prompting
    the model. This type of data becomes our generated dataset which has function
    level code associated with their summaries.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法涉及在类级别或功能级别代码处分割代码。功能级别代码被视为准备数据的来源。整个代码库被分割成功能级别代码。这些功能级别的代码被输入到指令模型中，通过提示模型生成摘要。这种类型的数据成为我们生成的数据集，其中包含与其摘要相关的功能级别代码。
- en: The second method involves extracting information from the coding practices
    embedded in the code. It is said that synthetic, structured, high quality, text
    book like data makes LLM learn faster and produce good results[[21](#bib.bib21)].
    In this approach, the comments and docstrings in the code are extracted along
    with detailed comments and is used along with the raw information gathered from
    the code as pre-processing data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法涉及从代码中嵌入的编码实践中提取信息。据说合成的、结构化的、高质量的类似教科书的数据能让 LLM 学习得更快并产生良好的结果[[21](#bib.bib21)]。在这种方法中，代码中的注释和文档字符串被提取出来，连同详细注释一起使用，并与从代码中收集的原始信息一起作为预处理数据。
- en: The third method involves tokenizing the whole code base irrespective of the
    file type into the supported sequence length. This method doesn’t involve gathering
    any other data. The LLM model with this tokenized data is trained for the purpose
    of next token prediction usecase.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法涉及将整个代码库无论文件类型如何都进行标记化，处理成支持的序列长度。这种方法不涉及收集任何其他数据。使用这些标记化数据训练 LLM 模型，目的是为了下一个
    token 预测的用例。
- en: 4.3 Compute Estimation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 计算估算
- en: After deciding on the input, the expected output and objective at hand, the
    next step is to decide on the hardware. To get the memory that will be occupied
    by a model in FP32 precision a rough estimate is to multiply the model parameter
    size by 4, since a single parameter occupies 4 bytes. For FP16, the multiplication
    factor is 2, and for 8 bit quantized model it is 1 and hence for 4 bit it is 0.5\.
    This estimate alone will not help us in finding the right hardware for fine tuning
    as a higher percentage of memory is required to store the gradients and activations
    during the fine tuning process. This additional storage also has to be accounted
    for.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定输入、预期输出和目标之后，下一步是决定硬件。为了获得 FP32 精度下模型占用的内存，一个粗略估算是将模型参数大小乘以 4，因为单个参数占用 4
    个字节。对于 FP16，乘法因子为 2，对于 8 位量化模型为 1，因此 4 位量化模型为 0.5。这一估算本身不能帮助我们找到合适的微调硬件，因为在微调过程中需要更高百分比的内存来存储梯度和激活值。这部分额外存储也需要考虑。
- en: 5 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: LLaMA 2, the open source model used for experimentation in this paper has a
    pre training data cut off time on September 2022\. Additional data till July 2023
    has also been added as fresh data during fine tuning before its release [[4](#bib.bib4)].
    Hence to test the approaches of data preparation recipes, proprietary documents
    and code repositories were used from our inhouse machine learning platform.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA 2 是本论文实验中使用的开源模型，其预训练数据截止时间为 2022 年 9 月。微调过程中，还添加了直到 2023 年 7 月的额外数据作为新数据[[4](#bib.bib4)]。因此，为了测试数据准备方案的效果，我们使用了来自内部机器学习平台的专有文档和代码库。
- en: A100 80 GB Nvidia GPU from Google Cloud Platform is used as the hardware accelerator
    for the fine tuning process.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Platform 上的 A100 80 GB Nvidia GPU 被用作微调过程的硬件加速器。
- en: 5.1 Text
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 文本
- en: To fine tune with text data, documentation resources from our machine learning
    platform were used. The user guide of this platform which has 66 pages and 5 MB
    of data was used as the raw source of data. This was converted to 60 KB csv file
    with 33 rows. Since the document is not available to public and was also created
    after the fine tuning cut off time of the original LLaMA 2 model, it makes a good
    dataset to study. The user guide is in PDF format and the content is well structured
    with index, headings and step by step instructions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用文本数据进行微调，使用了我们机器学习平台上的文档资源。使用了该平台的用户指南，该指南有 66 页和 5 MB 的数据，作为原始数据源。这些数据被转换成了
    60 KB 的 csv 文件，包含 33 行。由于该文档对公众不可用，且是在原始 LLaMA 2 模型的微调截止时间之后创建的，因此它成为了一个很好的研究数据集。用户指南为
    PDF 格式，内容结构良好，包含索引、标题和逐步说明。
- en: 'Before the start of fine tuning process, the influence of quantization on inference
    is shown in table [1](#S5.T1 "Table 1 ‣ 5.1 Text ‣ 5 Experiments ‣ Fine tuning
    LLMs for Enterprise: Practical Guidelines and Recommendations"). Quantizing a
    model saves on GPU memory and allows fine tuning with higher batch sizes there
    by reducing time and money spent on the training job. However as shown in figure
    [2](#S5.F2 "Figure 2 ‣ 5.1 Text ‣ 5 Experiments ‣ Fine tuning LLMs for Enterprise:
    Practical Guidelines and Recommendations"), quantized model on average takes a
    higher time for inference compared to a non quantized model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程开始之前，量化对推理的影响如表 [1](#S5.T1 "表 1 ‣ 5.1 文本 ‣ 5 实验 ‣ 企业 LLM 微调：实用指南和建议") 所示。量化模型可以节省
    GPU 内存，并允许使用更高的批量大小进行微调，从而减少训练作业所花费的时间和成本。然而，如图 [2](#S5.F2 "图 2 ‣ 5.1 文本 ‣ 5 实验
    ‣ 企业 LLM 微调：实用指南和建议") 所示，量化模型的平均推理时间比未量化模型更长。
- en: '![Refer to caption](img/4caf3e73ad30a6dd225d4a6510b27d72.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4caf3e73ad30a6dd225d4a6510b27d72.png)'
- en: 'Figure 2: Inference time of fine tuned Llama 2 7B Chat model'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 微调后的 Llama 2 7B Chat 模型的推理时间'
- en: 'Table 1: Influence of quantization on inference'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 量化对推理的影响'
- en: '| Quantization | GPU memory | What is AION? |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | GPU 内存 | 什么是 AION? |'
- en: '| Without quantization | 28 GB | AION (Artificial Intelligence ON) is a cloud-based
    platform that enables developers to build, train and deploy machine learning models.
    It provides an end-to-end solution for data scientists and engineers to create,
    test, refine, and deploy ML models in production environments. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 未量化 | 28 GB | AION（人工智能ON）是一个基于云的平台，使开发人员能够构建、训练和部署机器学习模型。它为数据科学家和工程师提供了一个端到端的解决方案，用于在生产环境中创建、测试、优化和部署机器学习模型。
    |'
- en: '| 8 bit quantization | 8 GB | AION (Artificial Intelligence ON) is a cloud-based
    platform that enables developers to build, train and deploy machine learning models.
    It provides an end-to-end solution for data scientists and engineers to create,
    test, refine, and deploy predictive modeling solutions in the form of APIs or
    containerized microservices. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 8位量化 | 8 GB | AION（人工智能ON）是一个基于云的平台，使开发人员能够构建、训练和部署机器学习模型。它提供了一种端到端的解决方案，帮助数据科学家和工程师创建、测试、优化和部署预测建模解决方案，形式包括API或容器化的微服务。'
- en: In table LABEL:tab:llamatable, the maximum possible configurations of PEFT methods
    on a A100 80 GB GPU machine are listed. For example, 70 B parameter flavour of
    LLaMA 2 model can be fine tuned only with QLoRA in 80 GB machine. LORA is not
    possible due to memory constraint.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在表LABEL:tab:llamatable中，列出了A100 80 GB GPU机器上PEFT方法的最大配置。例如，LLaMA 2模型的70B参数版本只能在80
    GB机器上使用QLoRA进行微调。由于内存限制，LORA不可用。
- en: '{talltblr}'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '{talltblr}'
- en: '[ caption = Maximum possible PEFT configurations of Llama 2 models on A100
    80 GB, label = tab:llamatable, ]hlines, vlines, colspec = l *9c Model & size &
    Dataset size Epochs PEFT method CPU memory GPU memory Estimated time Llama 2 Chat
    7B 60 KB 3 LORA 6 GB 18 GB 15 mins'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[ caption = Llama 2模型在A100 80 GB上的最大PEFT配置，label = tab:llamatable, ]hlines,
    vlines, colspec = l *9c 模型 & 尺寸 & 数据集大小 训练轮数 PEFT方法 CPU内存 GPU内存 估计时间 Llama 2 Chat
    7B 60 KB 3 LORA 6 GB 18 GB 15分钟'
- en: Llama 2 Chat 13B 60 KB 3 LORA 6 GB 26 GB 25 mins
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 Chat 13B 60 KB 3 LORA 6 GB 26 GB 25分钟
- en: Llama 2 Chat 70B 60 KB 3 QLORA 7 GB 65 GB 40 mins
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2 Chat 70B 60 KB 3 QLORA 7 GB 65 GB 40分钟
- en: 'Now with the understanding of PEFT configurations possible for different size
    models, next there is a need to tune the PEFT hyper parameters. In [[11](#bib.bib11)],
    alpha is suggested to be fixed and rank is fine tuned. Typically lower ranks are
    preferred with a number of target modules. However owing to a smaller dataset
    size, target modules were kept as q_proj and v_proj only. A high alpha is used
    so that model could learn more from new gradient updates pertaining to the new
    information from documentation. The quality of the responses are captured with
    a manual assessment. As evident from table [2](#S5.T2 "Table 2 ‣ 5.1 Text ‣ 5
    Experiments ‣ Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations"),
    the appropriate rank and alpha that gives decent results vary for the different
    parameter sized models.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在了解了不同大小模型的PEFT配置可能性后，接下来需要调整PEFT超参数。在[[11](#bib.bib11)]中，建议将alpha固定，而对rank进行微调。通常，较低的rank更适合一些目标模块。然而，由于数据集较小，目标模块仅保持为q_proj和v_proj。使用较高的alpha，以便模型能够从新文档的梯度更新中学习更多。响应的质量通过人工评估进行捕捉。从表[2](#S5.T2
    "表2 ‣ 5.1 文本 ‣ 5 实验 ‣ 针对企业的LLM微调：实用指南和建议")可以看出，适当的rank和alpha会因不同参数大小的模型而有所不同。
- en: 'Table 2: LoRA rank tuning'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LoRA排名调优
- en: '| Model and size | LoRA parameters | Prompt Manual assessments |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 模型及尺寸 | LoRA参数 | 提示 手动评估 |'
- en: '|  | Rank | Alpha | Qn1 | Qn2 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | Rank | Alpha | Qn1 | Qn2 |'
- en: '| 7B Chat | 1 | 512 | Good | Good |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 7B Chat | 1 | 512 | 良好 | 良好 |'
- en: '|  | 2 | 512 | Average | Poor |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 2 | 512 | 一般 | 较差 |'
- en: '|  | 4 | 512 | Poor | Poor |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 512 | 较差 | 较差 |'
- en: '|  | 8 | 512 | Poor | Poor |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 8 | 512 | 较差 | 较差 |'
- en: '|  | 16 | 512 | Poor | Poor |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 16 | 512 | 较差 | 较差 |'
- en: '| 13B Chat | 1 | 512 | Poor | Good |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 13B Chat | 1 | 512 | 较差 | 良好 |'
- en: '|  | 2 | 512 | Poor | Average |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 2 | 512 | 较差 | 一般 |'
- en: '|  | 4 | 512 | Poor | Good |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 512 | 较差 | 良好 |'
- en: '|  | 8 | 512 | Good | Good |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 8 | 512 | 良好 | 良好 |'
- en: '|  | 16 | 512 | Average | Average |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 16 | 512 | 一般 | 一般 |'
- en: With data extracted from the pdf, the final dataset is prepared in four ways
    and it is clear that each variation of the data preparation recipe produces different
    small changes in the response. ’Raw’ and ’Keyword’ recipe give short and succinct
    responses. ’Heading’ and ’Summary’ methods give a somewhat detailed response.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从PDF提取的数据经过四种方式处理后，最终的数据集表明，每种数据准备方法都会对响应产生不同的小变化。’Raw’和’Keyword’方法产生简短明了的响应，而’Heading’和’Summmary’方法则提供了相对详细的响应。
- en: 'Table 3: Influence of data preparation recipes on inference'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：数据准备方法对推理的影响
- en: '| Dataset type | What is AION? |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 数据集类型 | 什么是AION？ |'
- en: '| Default | AION (Artificial Intelligence ON) is a cloud-based platform that
    enables developers to build, train and deploy machine learning models. It provides
    an end-to-end solution for data scientists and engineers to create, test, refine,
    and deploy predictive modeling solutions in the form of APIs or containerized
    microservices. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 默认 | AION（人工智能ON）是一个基于云的平台，使开发人员能够构建、训练和部署机器学习模型。它为数据科学家和工程师提供了一个端到端的解决方案，以创建、测试、完善和部署预测建模解决方案，形式为API或容器化微服务。
    |'
- en: '| ML | AION (AI-ON) is an open source platform for building, deploying and
    managing machine learning models at scale. It provides features such as data version
    control, reproducibility, collaboration support, model monitoring, and deployment
    to cloud or on premise infrastructure. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ML | AION（AI-ON）是一个开源平台，用于大规模构建、部署和管理机器学习模型。它提供了数据版本控制、可复现性、协作支持、模型监控以及部署到云端或本地基础设施等功能。
    |'
- en: '| Meta Data | AION (Artificial Intelligence ON) is an open-source, cloud-native
    platform for building and deploying modern machine learning models at scale. It
    provides a unified environment to develop, train, test, and deploy ML models across
    different data sources and platforms. The main goal of AION is to make it easy
    for developers to build, manage, and monitor their ML workflows by automating
    most of the underlying tasks such as data ingestion, feature engineering, model
    training, |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 元数据 | AION（人工智能ON）是一个开源的、云原生平台，用于大规模构建和部署现代机器学习模型。它提供了一个统一的环境，用于开发、训练、测试和部署跨不同数据源和平台的ML模型。AION的主要目标是通过自动化大部分底层任务（如数据摄取、特征工程、模型训练）来简化开发人员构建、管理和监控ML工作流的过程。
    |'
- en: '| Gen AI | AION (Artificial Intelligence ON): It refers to the integration
    of machine learning, data analytics, robotics, and artificial intelligence to
    automate business processes, improve productivity, and enable more informed decisions.AION
    stands for Artificial Intelligence ON which means all the features such as ML,
    Deep Learning, etc are enabled by default without any need to disable them. This
    makes it easy to use and deploy models in production environments seamlessly.
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Gen AI | AION（人工智能ON）：它指的是机器学习、数据分析、机器人技术和人工智能的集成，以自动化业务流程、提高生产力并实现更明智的决策。AION代表“人工智能ON”，意味着所有功能，如机器学习、深度学习等，默认启用，无需禁用它们。这使得在生产环境中使用和部署模型变得更加简单和无缝。
    |'
- en: 'PEFT methods are recommended for smaller dataset. However if there is a huge
    collection of dataset with new information, full fine tuning of all the parameters
    of a model is possible. The A100 80 GB GPU is capable of supporting full model
    fine tuning of 7B and 13B models. In table LABEL:tab:fulltable the loading parameters
    and batch size details are given in detail. Higher batch sizes gave cuda out of
    memory error. The configurations in each row of each of the model size are named
    as config A, B and C respectively. In [3](#S5.F3 "Figure 3 ‣ 5.1 Text ‣ 5 Experiments
    ‣ Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations"),
    a graph is plotted between number of data rows and the training time. With increase
    in data rows, the time increases linearly. A different configuration with higher
    batch size and gradient accumulation steps will decrease the fine tuning time
    slightly. A huge time will be saved if full fine tuning is done for half the total
    capacity of maximum sequence length of LLaMA 2 models.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的数据集，推荐使用PEFT方法。然而，如果数据集中包含大量的新信息，可以对模型的所有参数进行全量微调。A100 80 GB GPU能够支持7B和13B模型的全量微调。在表LABEL:tab:fulltable中，加载参数和批量大小的详细信息已给出。较高的批量大小会导致cuda内存不足错误。每个模型大小的每一行配置分别命名为配置A、B和C。在[3](#S5.F3
    "图3 ‣ 5.1 文本 ‣ 5 实验 ‣ 企业LLM微调：实用指南和建议")中，绘制了数据行数与训练时间之间的图表。随着数据行数的增加，时间线性增长。使用不同的配置，增加批量大小和梯度累积步骤，会略微减少微调时间。如果对LLaMA
    2模型的最大序列长度的一半进行全量微调，可以节省大量时间。 |
- en: '{talltblr}'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '{talltblr}'
- en: '[ caption = Full fine tuning of Llama2 Chat on A100 80 GB, label = tab:fulltable,
    ]hlines, vlines, colspec = l *9c Llama2 Chat Model size & Data size Seq len Model
    precision Batch size Gradient accumulation steps Resulting steps CPU memory GPU
    memory Estimated time (mins) 7B 60 KB 4096 FP32 1 2 254 54 GB 80 GB 57'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[ caption = 在A100 80 GB上对Llama2 Chat进行全量微调，label = tab:fulltable, ]hlines,
    vlines, colspec = l *9c Llama2 Chat模型大小与数据大小 Seq len 模型精度 批量大小 梯度累积步骤 结果步骤 CPU内存
    GPU内存 估计时间（分钟） 7B 60 KB 4096 FP32 1 2 254 54 GB 80 GB 57'
- en: 7B 60 KB 4096 FP16 2 4 63 54 GB 80 GB 33
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 7B 60 KB 4096 FP16 2 4 63 54 GB 80 GB 33
- en: 7B 60 KB 2048 FP16 4 8 16 54 GB 80 GB 10
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 7B 60 KB 2048 FP16 4 8 16 54 GB 80 GB 10
- en: 13B 60 KB 4096 FP32 1 2 254 102 GB 80 GB 110
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 13B 60 KB 4096 FP32 1 2 254 102 GB 80 GB 110
- en: 13B 60 KB 4096 FP16 2 4 127 102 GB 80 GB 75
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 13B 60 KB 4096 FP16 2 4 127 102 GB 80 GB 75
- en: 13B 60 KB 2048 FP16 4 8 16 102 GB 80 GB 25
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 13B 60 KB 2048 FP16 4 8 16 102 GB 80 GB 25
- en: RAG- (Retrieval Augmented Generation) is a popular way of using embeddings and
    a vector similarity search to find only relevant context from a pool of documentation
    with new information or any content longer than maximum capacity of the LLM. The
    LLM interprets the context and gives a polished response. This entire process
    can be summed as embedding conversion followed by similiarity search usually through
    a vector DB which in turn is followed by LLM handling the context. It is evident
    from this process that the contribution of LLM will be good only if the contexts
    obtained are good. A comparison is made in table LABEL:tab:ragvsft between RAG
    with a pretrained model and RAG with the fine tuned model. It is seen that RAG
    responses are improved with specific answers and even following the style of the
    document. Since the document being dealt here is a user guide there are steps
    mentioned in every process. Only the fine tuned model was able to follow this
    pattern of giving responses in step by step format. Another observation is regarding
    the last question which is a multi part question. The last question was not answered
    by the base LLaMA 2 model. Our fine tuned model was not only able to follow the
    style but also give the precise answer to the second part of the question. Hence
    fine tuning could be one solution to hallucinations in RAG.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RAG（检索增强生成）是一种流行的使用嵌入和向量相似度搜索的方法，用于从文档池中仅找到相关的上下文，以应对具有新信息或任何内容超过LLM最大容量的情况。LLM解释这些上下文并给出一个完善的回答。这个整个过程可以总结为嵌入转换，随后是通过向量数据库的相似度搜索，然后由LLM处理上下文。从这个过程可以看出，LLM的贡献只有在获得的上下文质量好的情况下才会好。表格LABEL:tab:ragvsft中比较了使用预训练模型的RAG与使用微调模型的RAG。结果显示，RAG的回答在特定问题上有所改进，甚至遵循了文档的风格。由于这里处理的文档是用户指南，每个过程都有步骤说明。只有微调模型能够遵循这种逐步响应的模式。另一个观察点是关于最后一个问题，它是一个多部分的问题。基础LLaMA
    2模型没有回答最后一个问题。我们的微调模型不仅能够遵循风格，还能准确回答问题的第二部分。因此，微调可能是解决RAG中幻觉现象的一种解决方案。
- en: '![Refer to caption](img/a9c8e0c64cccd0b44fe2e7ef492a05e5.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9c8e0c64cccd0b44fe2e7ef492a05e5.png)'
- en: (a) 7B Chat model full fine tuning
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 7B聊天模型全微调
- en: '![Refer to caption](img/7e1ecb38969e7ddbfc78593adefc8438.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e1ecb38969e7ddbfc78593adefc8438.png)'
- en: (b) 13B Chat model full fine tuning
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 13B聊天模型全微调
- en: 'Figure 3: Fine tuning time of Llama 2 Chat models on different fine tuning
    configurations'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Llama 2聊天模型在不同微调配置下的微调时间
- en: 'Table 4: Retrieval Augmented Generation with LLaMA 2 Chat before and after
    fine tuning'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：LLaMA 2聊天模型在微调前后的检索增强生成
- en: '| Questions | RAG | Finetuned model and RAG |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | RAG | 微调模型与RAG |'
- en: '| What is AION? | Based on the provided context, I can confirm that AION is
    an artificial intelligence (AI) life cycle management platform designed to help
    organizations apply machine learning to real-world problems. AION provides a comprehensive
    suite of tools and services for managing the entire AI lifecycle, including data
    ingestion, exploratory data analysis, data processing, feature selection, model
    training and hyperparameter tuning, and inference services. The platform offers
    a less-code/no-code approach, allowing business users to generate ML models without
    coding. Additionally, AION includes explainability capabilities to provide higher
    accuracy predictions with supported explanations and model confidence. | Based
    on the provided context, I can explain what AION is. AION stands for Artificial
    Intelligence ON, which is an AI life cycle management platform used for solving
    real-world problems using machine learning. AION provides a less-code/no-code
    approach to users, allowing them to train and deploy machine learning models without
    any coding knowledge. It includes various sub-processes such as AutoML, MLOPs,
    MLaC, and Explainability. These sub-processes are automated, and AION generates
    the best deployment model after analyzing the input data and problem statement.
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 什么是 AION？ | 根据提供的上下文，我可以确认 AION 是一个人工智能 (AI) 生命周期管理平台，旨在帮助组织将机器学习应用于现实世界问题。AION
    提供了一整套工具和服务，用于管理整个 AI 生命周期，包括数据摄取、探索性数据分析、数据处理、特征选择、模型训练和超参数调整，以及推理服务。该平台提供了较少代码/无代码的方法，使业务用户能够生成机器学习模型，而无需编写代码。此外，AION
    包括解释性功能，以提供更高准确度的预测，并提供支持的解释和模型置信度。 |'
- en: '| Write in detail about MLLite feature in AION. List the input model formats
    that can be converted to ONNX. | Based on the provided context, here is the information
    regarding the MLLite feature in AION: MLLite is a feature in AION that allows
    users to convert their trained models into formats compatible with various deep
    learning frameworks, including ONNX and TFLite. The main purpose of MLLite is
    to enable efficient inference execution on different devices without requiring
    significant modifications to the original model architecture. Regarding the input
    model formats that can be converted to ONNX, according to the context, the following
    formats are supported:Keras, Sklearn, TensorFlow, TFLite, Light GBM, XGBoost,
    LibSVM,CoreML In summary, MLLite is a useful feature in AION that enables users
    to convert their trained models into optimized formats for efficient inference
    execution on various devices, while also providing support for multiple input
    model formats for conversion to ONNX. | Sure! Here’s what I could gather from
    the given text regarding the MLLite feature in AION:MLLite feature in AION allows
    users to convert their trained models into different formats such as ONNX, TFLite,
    and Sagemaker. The main advantage of this feature is that it enables users to
    consume these optimized models for predictions without having to retrain them.Input
    Model Formats that can be converted to ONNX include:Keras,Sklearn,TensorFlow,Light
    GBM,XGBoost,LibSVM, CoreML |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 详细介绍 AION 中的 MLLite 特性。列出可以转换为 ONNX 的输入模型格式。 | 根据提供的上下文，以下是关于 AION 中 MLLite
    特性的相关信息：MLLite 是 AION 中的一项功能，允许用户将他们训练的模型转换为兼容多种深度学习框架的格式，包括 ONNX 和 TFLite。MLLite
    的主要目的是在不同设备上实现高效的推理执行，而无需对原始模型架构进行重大修改。关于可以转换为 ONNX 的输入模型格式，根据上下文，支持以下格式：Keras、Sklearn、TensorFlow、TFLite、Light
    GBM、XGBoost、LibSVM、CoreML 总结来说，MLLite 是 AION 中一个有用的功能，使用户能够将他们训练的模型转换为优化格式，以便在各种设备上高效推理，同时还支持多种输入模型格式转换为
    ONNX。 |'
- en: '| Is there Incremental learning in AION? If so what are the steps to create
    an incremental model? | Yes, AION supports incremental learning for regression
    and classification problems. To create an incremental model in AION, follow these
    steps: 1\. Select the "Online Learning" (Beta) or "Distributed Learning" (Beta)
    checkbox in the Incremental Learning section of the configuration page. 2\. Choose
    the problem type (Regression or Classification) that supports incremental learning.
    3\. Proceed to the next step by clicking the "Next" button. In summary, to create
    an incremental model in AION, you need to enable online learning in the configuration
    page and choose the appropriate problem type. | Yes, there is incremental learning
    in AION. To create an incremental model follow the below-mentioned steps: Step
    2: Click online learning tab. Step 3: Select the Online Learning (Beta) or Distributed
    learning (Beta) checkbox for Incremental Learning. Step 4: Click next icon to
    proceed to advanced configuration tab. |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| AION中是否支持增量学习？如果支持，创建增量模型的步骤是什么？ | 是的，AION 支持回归和分类问题的增量学习。要在 AION 中创建增量模型，请按照以下步骤操作：1.
    在配置页面的增量学习部分，选择“在线学习”（Beta）或“分布式学习”（Beta）复选框。2. 选择支持增量学习的问题类型（回归或分类）。3. 点击“下一步”按钮进入下一步。总之，要在
    AION 中创建增量模型，你需要在配置页面启用在线学习，并选择适当的问题类型。 | 是的，AION 中有增量学习。要创建增量模型，请按照以下步骤操作：第 2
    步：点击在线学习标签。第 3 步：选择增量学习的“在线学习（Beta）”或“分布式学习（Beta）”复选框。第 4 步：点击下一步图标进入高级配置标签。 |'
- en: '| How can we do text labelling with AION? Give the API URL for model monitoring?
    | Based on the provided context, the API URL for model monitoring during text
    labeling with AION is not explicitly mentioned. Therefore, I cannot provide an
    answer to this question. | Based on the provided context, I can answer the question
    as follows: To perform text labelling with AION, follow these steps: Step 1: Go
    to Utilities > Text Data Labelling. Step 2: Upload the text data in CSV format.
    Step 3: Select relevant features and click Next. Step 4: Generate predictions
    by clicking Predictions tab. The predicted labels are displayed under Predicted
    Labels section. For example, if the input text is "This is a sample text", the
    predicted label would be "positive". API URL for Model Monitoring: To monitor
    the performance of the model, users can use the API endpoint /monitoring/model-performance.
    This API returns the summary statistics of the model’s performance along with
    the confusion matrix. Example: curl -X POST http://localhost:8080/monitoring/model-performance
    -H ’Content-Type: application/json’ -d ’"data":["This is a sample text"],"label":["positive"]’
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 我们如何使用 AION 进行文本标记？提供用于模型监控的 API URL？ | 根据提供的上下文，文本标记过程中用于模型监控的 API URL 并未明确提及。因此，我无法回答这个问题。
    | 根据提供的上下文，我可以这样回答：要使用 AION 进行文本标记，请按照以下步骤操作：第 1 步：转到工具 > 文本数据标记。第 2 步：以 CSV 格式上传文本数据。第
    3 步：选择相关特征并点击下一步。第 4 步：点击预测标签生成预测。预测标签会显示在预测标签部分。例如，如果输入文本为“这是一个示例文本”，则预测标签为“积极”。模型监控的
    API URL：要监控模型的性能，用户可以使用 API 端点 /monitoring/model-performance。该 API 返回模型性能的汇总统计数据以及混淆矩阵。示例：curl
    -X POST http://localhost:8080/monitoring/model-performance -H ''Content-Type:
    application/json'' -d ''{"data":["This is a sample text"],"label":["positive"]}''
    |'
- en: 'Finally the objective of the fine tuning process is to see if the LLM has learnt
    the new knowledge and as shown in table [5](#S5.T5 "Table 5 ‣ 5.1 Text ‣ 5 Experiments
    ‣ Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations"),
    it indeed has learnt.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '最终，微调过程的**目标**是查看 LLM 是否学会了新的知识，如表格 [5](#S5.T5 "Table 5 ‣ 5.1 Text ‣ 5 Experiments
    ‣ Fine tuning LLMs for Enterprise: Practical Guidelines and Recommendations")
    所示，确实学会了。'
- en: 'Table 5: Model learning new information after fine tuning'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：模型在微调后的新信息学习情况
- en: '| Fine tuning | Model | What is AION? |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 模型 | 什么是 AION？ |'
- en: '| Before | Llama 2 chat | Aion (AION) is a blockchain-based platform that enables
    the creation and exchange of digital assets, such as NFTs. It was founded in 2018
    by a team led by Matthew Roszak, who has extensive experience in the cryptocurrency
    industry. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 之前 | Llama 2 chat | Aion（AION）是一个基于区块链的平台，支持数字资产（如NFT）的创建和交换。它由 Matthew Roszak
    领导的团队于 2018 年创立，Matthew Roszak 在加密货币行业拥有丰富经验。 |'
- en: '| After | Llama 2 chat | AION (Artificial Intelligence ON) is a cloud-based
    platform that enables developers to build, train and deploy machine learning models.
    It provides an end-to-end solution for data scientists and engineers to create,
    test, refine, and deploy ML models in production environments. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 之后 | Llama 2 聊天 | AION（人工智能 ON）是一个基于云的平台，允许开发人员构建、训练和部署机器学习模型。它为数据科学家和工程师提供了一个端到端的解决方案，以便在生产环境中创建、测试、改进和部署
    ML 模型。 |'
- en: 5.2 Code
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 代码
- en: For the code dataset, JAVA files from a sustenance engineering solutions platform
    was used. This being a closed source code repository, the codes are new unlearned
    information for Llama models making it a good dataset to experiment with. The
    total size of the code data was 16MB with a total function count of 13807\. The
    dataset was condensed and packed according to token limit of LLaMA model and resulted
    in a csv file of 10 MB size with 60 rows.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码数据集，使用了来自一个维持工程解决方案平台的 JAVA 文件。由于这是一个封闭源代码库，这些代码对 Llama 模型而言是新的未学习信息，因此这是一个很好的实验数据集。代码数据的总大小为
    16MB，总函数数为 13807。数据集被压缩并打包，根据 LLaMA 模型的令牌限制，生成了一个大小为 10 MB、包含 60 行的 csv 文件。
- en: '{talltblr}'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '{talltblr}'
- en: '[ caption = PEFT methods on Code Llama on A100 80 GB, label = tab:codet1, ]hlines,
    vlines, colspec = l *9c Model & size & Data size Epochs PEFT method CPU memory
    GPU memory Estimated time Code Llama 7B 10 MB 3 LORA 6 GB 18 GB 23 mins'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[ caption = Code Llama 在 A100 80 GB 上的 PEFT 方法，label = tab:codet1, ]hlines,
    vlines, colspec = l *9c 模型 & 大小 & 数据大小 纪元 PEFT 方法 CPU 内存 GPU 内存 估计时间 Code Llama
    7B 10 MB 3 LORA 6 GB 18 GB 23 分钟'
- en: Code Llama 13B 10 MB 3 LORA 6 GB 26 GB 25 mins
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 13B 10 MB 3 LORA 6 GB 26 GB 25 分钟
- en: Code Llama 7B 10 MB 3 QLORA 7 GB 15 GB 10 mins
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 7B 10 MB 3 QLORA 7 GB 15 GB 10 分钟
- en: Table LABEL:tab:codet1 exhibits the experiment on the hyper parameters conducted
    to investigate the performance of the model. Few results are discussed by prompting
    the trained model and comparing the results from the code repository. It was seen
    from the results from the experiments the Code LLaMA models gave an exceptionally
    good results with LoRA rank 16 and alpha 32\. Higher the rank and alpha made the
    models to hallucinate and randomly generate the code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:codet1 展示了对超参数进行的实验，以调查模型的性能。通过提示训练模型并比较来自代码库的结果，讨论了几个结果。从实验结果中可以看到，Code
    LLaMA 模型在 LoRA 排名 16 和 Alpha 32 时表现出色。排名和 Alpha 越高，模型越容易产生幻觉并随机生成代码。
- en: '{talltblr}'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '{talltblr}'
- en: '[ caption = Full fine tuning of Code Llama on A100 80 GB, label = tab:?, ]hlines,
    vlines, colspec = l *9c Model & size & Seq len Model precision Batch size Gradient
    accumulation steps CPU memory GPU memory Estimated time Code Llama 7B 4096 FP16
    2 2 54 GB 57 GB 10 mins'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[ caption = Code Llama 在 A100 80 GB 上的全面微调，label = tab:?, ]hlines, vlines,
    colspec = l *9c 模型 & 大小 & 序列长度 模型精度 批量大小 梯度累积步骤 CPU 内存 GPU 内存 估计时间 Code Llama
    7B 4096 FP16 2 2 54 GB 57 GB 10 分钟'
- en: Code Llama 7B 4096 FP16 2 4 54 GB 46 GB 38 mins
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 7B 4096 FP16 2 4 54 GB 46 GB 38 分钟
- en: Code Llama 7B 2048 FP16 4 8 54 GB 59 GB 10 mins
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 7B 2048 FP16 4 8 54 GB 59 GB 10 分钟
- en: Code Llama 13B 4096 FP32 2 2 102 GB 53 GB 9 mins
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 13B 4096 FP32 2 2 102 GB 53 GB 9 分钟
- en: Code Llama 13B 4096 FP16 2 4 102 GB 53 GB 18 mins
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 13B 4096 FP16 2 4 102 GB 53 GB 18 分钟
- en: Code Llama 13B 2048 FP16 4 8 102 GB 62 GB 33 mins
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Code Llama 13B 2048 FP16 4 8 102 GB 62 GB 33 分钟
- en: 'Table 6: Code Llama LORA'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: Code Llama LORA'
- en: '| Model and size | LoRA parameters | Prompt Manual assessments |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 模型及大小 | LoRA 参数 | 提示 手动评估 |'
- en: '|  | Rank | Alpha |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 排名 | Alpha |  |'
- en: '| Code Llama 7B | 2 | 4 | Poor |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Code Llama 7B | 2 | 4 | 较差 |'
- en: '|  | 4 | 8 | Poor |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 8 | 较差 |'
- en: '|  | 8 | 16 | Average |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 8 | 16 | 平均 |'
- en: '|  | 16 | 32 | Good |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 16 | 32 | 良好 |'
- en: 'Table 8: Code for getting testcase by ID'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 根据 ID 获取测试用例的代码'
- en: '| Prompt | Ground truth | Predicted response |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 真实值 | 预测响应 |'
- en: '| A connection object is created to connect to a database for a given project
    name Connection is established to the database using the URL, username and password
    If an exception occurs the error message is printed The connection object is returned
    to the calling function | Connection  con  =  null; HttpServletRequest  httpRequest  =  null;
    try{ System.out.println(""get  New  Connection  projectName:""+projectName)  ;
    Class.  forName(driverName)  .newInstance()  ; con  =  DriverManager.getConnection(url+projectName,  dbUser,  dbPass);
    }  catch  (SQLException  sqle)  { System.out.println(""in  ConnectionPool  of  getNewConnection()11111""+sqle.getStackTrace());
    }  catch  (Exception  e)  { System.out.println(""in  ConnectionPool  of  getNewConnection()222"");
    e.printStackTrace(); } return  con; | public  Connection  getConnection(String  projectName){
    Connection  con  =  null; HttpServletRequest  httpRequest  =  null; try{ Class.forName(driverName).newInstance();
    con  =  DriverManager.getConnection(url+projectName,  dbUser,  dbPass); }  catch  (SQLException  sqle)  {
    System.out.println("In␣ConnectionPool␣of␣getConnection()"+sqle.getMessage());
    }}  catch  (Exception  e)  { System.out.println("In␣ConnectionPool␣of␣getConnection()"+e.getMessage());
    } return  con; |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个连接对象以连接到给定项目名称的数据库。连接通过 URL、用户名和密码建立。如果发生异常，则打印错误信息。连接对象将返回给调用函数。 | Connection
    con = null; HttpServletRequest httpRequest = null; try{ System.out.println(""get
    New Connection projectName:"" + projectName); Class.forName(driverName).newInstance();
    con = DriverManager.getConnection(url + projectName, dbUser, dbPass); } catch
    (SQLException sqle) { System.out.println(""in ConnectionPool of getNewConnection()11111""
    + sqle.getStackTrace()); } catch (Exception e) { System.out.println(""in ConnectionPool
    of getNewConnection()222""); e.printStackTrace(); } return con; | public Connection
    getConnection(String projectName){ Connection con = null; HttpServletRequest httpRequest
    = null; try{ Class.forName(driverName).newInstance(); con = DriverManager.getConnection(url
    + projectName, dbUser, dbPass); } catch (SQLException sqle) { System.out.println("In␣ConnectionPool␣of␣getConnection()"
    + sqle.getMessage()); } catch (Exception e) { System.out.println("In␣ConnectionPool␣of␣getConnection()"
    + e.getMessage()); } return con; |'
- en: 'Table 8: Code for getting testcase by ID'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：通过 ID 获取测试用例的代码
- en: '| Prompt | Ground truth | Predicted response |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 实际结果 | 预测响应 |'
- en: '| Connect to the database. Prepare a query with a given test case ID to the
    table testcases table. Set the parameter to the given value. Execute the query
    and retrieve the results. Get the desired data from the results. Release resources
    used by the query and results. Return the desired data. | public  List  getFieldDefects(String  testcaseId){
    int  tcIdPk  =  DeltaUploadDAO.getInstance().getTestCaseId(testcaseId); List  fieldDefectsList  =  new  ArrayList();
    Connection  connection  =  ConnectionPool.getInstance().getConnection(); if(null  !=  connection){
    Statement  stmt  =  null; ResultSet  rs  =  null; try  { if(null  !=  connection){
    String  sql  =  ""SELECT  *  FROM  DEFECT_TESTCASE_MAPPING  WHERE  STATUS  =  ’R’  AND  TESTCASE_ID_FK  =  ""+tcIdPk+"""";
    stmt  =  connection.createStatement(); rs  =  stmt.executeQuery(sql); while(rs.next())  {
    DefectTestCaseMappingDTO  defectDTO=  new  DefectTestCaseMappingDTO(); defectDTO.setTestCaseId(getTcById(rs.getInt(""TESTCASE_ID_FK"")));
    defectDTO.setStatus(rs.getString(""STATUS"")); defectDTO.setAuto(rs.getString(""AUTO""));
    defectDTO.setReviewed(rs.getString(""REVIEWED"")); fieldDefectsList.add(defectDTO);}}
    } } return  con; | public  DataSet  getTestCaseByTestCaseId(int  testCaseId){
    System.out.println(’getTestCaseByTestCaseId’); Connection  con  =  null; Statement  stmt  =  null;
    ResultSet  rs  =  null; DataSet  ds  =  new  DataSet(); try{ con  =  ConnectionProvider.getConnection();
    if(null  !=  con){ stmt  =  con.createStatment(); String  sql  =  "SELECT␣*␣FROM␣TESTCASES␣where␣TESTCASE_ID_FK="+testCaseId+"";
    while(rs.next())  { TestCaseDAO  tcDao  =  new  TestCaseDAO(); tcDao.setTestCaseName(rs.getString("TESTCASE_NAME"));
    tcDao.setTestCaseDesc(rs.getString("TESTCASE_DESC")); tcDao.setTestCaseId(rs.getInt("TESTCASE_ID_FK"));
    ds.add(tcDao); } return  ds; }  catch  (Exception  e)  { }}} |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 连接到数据库。准备一个具有给定测试用例ID的查询到testcases表。将参数设置为给定值。执行查询并检索结果。从结果中获取所需数据。释放查询和结果使用的资源。返回所需数据。
    | public List getFieldDefects(String testcaseId){ int
    tcIdPk = DeltaUploadDAO.getInstance().getTestCaseId(testcaseId); List
    fieldDefectsList = new ArrayList(); Connection connection
    = ConnectionPool.getInstance().getConnection(); if(null != connection){ Statement
    stmt = null; ResultSet rs = null; try { if(null != connection){ String sql = ""SELECT
    * FROM DEFECT_TESTCASE_MAPPING WHERE STATUS = ’R’ AND TESTCASE_ID_FK = ""+tcIdPk+"""";
    stmt = connection.createStatement(); rs = stmt.executeQuery(sql); while(rs.next())
    { DefectTestCaseMappingDTO defectDTO= new DefectTestCaseMappingDTO(); defectDTO.setTestCaseId(getTcById(rs.getInt(""TESTCASE_ID_FK"")));
    defectDTO.setStatus(rs.getString(""STATUS"")); defectDTO.setAuto(rs.getString(""AUTO""));
    defectDTO.setReviewed(rs.getString(""REVIEWED"")); fieldDefectsList.add(defectDTO);}}
    } } return con; | public DataSet getTestCaseByTestCaseId(int testCaseId){ System.out.println(’getTestCaseByTestCaseId’);
    Connection con = null; Statement stmt = null; ResultSet rs = null; DataSet ds
    = new DataSet(); try{ con = ConnectionProvider.getConnection(); if(null != con){
    stmt = con.createStatment(); String sql = "SELECT * FROM TESTCASES where TESTCASE_ID_FK="+testCaseId+"";
    while(rs.next()) { TestCaseDAO tcDao = new TestCaseDAO(); tcDao.setTestCaseName(rs.getString("TESTCASE_NAME"));
    tcDao.setTestCaseDesc(rs.getString("TESTCASE_DESC")); tcDao.setTestCaseId(rs.getInt("TESTCASE_ID_FK"));
    ds.add(tcDao); } return ds; } catch (Exception e) { }}} |'
- en: 6 Guidelines and Recommendations
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 条指南和建议
- en: 'The following guidelines and recommendations have been summarized below, based
    on the various experiements that we have done on text and code fine tuning:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在文本和代码微调中进行的各种实验，以下指南和建议已被总结如下：
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Empirically loading the model in half precision is sufficient to go ahead with
    fine tuning and it also saves GPU memory to accommodate more batches if needed
    to save on finetuning time
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从经验来看，以半精度加载模型足以进行微调，并且如果需要节省微调时间，还可以节省GPU内存以容纳更多批次。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unless there is an abundance of data, parameter efficient finetuning is preferable
    than full finetuning. This also helps in creating easily moveable low sized adapters
    tuned for different tasks or domains
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除非数据非常丰富，否则参数高效微调比全模型微调更可取。这也有助于创建易于移动的低容量适配器，以便针对不同任务或领域进行微调。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Choose a model quantization level based on section [4.3](#S4.SS3 "4.3 Compute
    Estimation ‣ 4 Fine Tuning Workflow ‣ Fine tuning LLMs for Enterprise: Practical
    Guidelines and Recommendations"). For example, consider Llama 7B model in a 16
    GB colab environment; in this scenario, 8 bit quantized LORA fine tuning is possible
    but not full model fine tuning.'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于第[4.3节](#S4.SS3 "4.3 计算估算 ‣ 4 微调工作流 ‣ 企业级LLMs微调：实用指南和建议")选择模型量化级别。例如，在16 GB的Colab环境中考虑Llama
    7B模型；在这种情况下，8位量化LORA微调是可能的，但不适用于全模型微调。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For full fine tuning, typically multiple GPUs are required. In case of a constraint
    of having only one GPU available and a large CPU memory, it is recommended to
    use paged adam optimizer
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要进行全面微调，通常需要多个GPU。如果仅有一个GPU可用并且CPU内存较大，建议使用分页Adam优化器。
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For small datasets, it is ideal to use LORA fine tuning. Rank and Alpha has
    to be fine tuned
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于小数据集，理想的选择是使用LORA微调。需要对Rank和Alpha进行微调。
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: From the empirical experiments on text and code data, to make a language model
    assimilate new information, lower rank and higher alpha is recommended
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据对文本和代码数据的实证实验，推荐使用较低的Rank和较高的Alpha，以使语言模型吸收新信息。
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For large documents with text content of the order of few hundred MBs, it is
    recommended to utilise the full sequence length capability of the model in every
    row of data
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于几百MB大小的文本内容的大型文档，建议在数据的每一行中利用模型的完整序列长度能力。
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine tuning time largely depends on the number of rows in dataset. If the text
    content is chunked to full context length without padding, number of data rows
    can be greatly reduced
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调时间在很大程度上取决于数据集中的行数。如果文本内容被切分为完整的上下文长度而不进行填充，则数据行数可以大大减少。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gradient accumulation steps is the number of steps after which the optimizer
    is stepped. Until then gradients are accumulated over the batches. This is good
    in distributed system but in single GPU it is slow
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度累积步骤是指在优化器更新之前的步骤数。在此之前，梯度在批次中累积。这在分布式系统中效果很好，但在单GPU中较慢。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A higher batch size will lead to faster convergence and might give better performance
    at inference. Batch size is recommended to be kept at a lower value suitable for
    the model and not to the limiting value of GPU memory.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更高的批量大小将导致更快的收敛，并可能在推理时提供更好的性能。建议将批量大小保持在适合模型的较低值，而不是GPU内存的限制值。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Higher the gradient accumulation steps more the memory will be saved but at
    the cost of longer fine tuning time.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度累积步骤越多，节省的内存越多，但微调时间也会更长。
- en: 7 Further work
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 进一步工作
- en: In this paper we show that LLMs are able to learn new information from limited
    data with right LORA configurations. However the results have traces of hallucinations.
    To mitigate hallucinations, within the current setting different prompt templates
    have to be experimented. It also boils down to the way dataset is prepared. Chunking
    techniques like semantic chunking provide a way to create chunks that stand on
    their own as separate information entities. This could be explored further as
    a dataset preparation recipe to reduce hallucinations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了LORA配置正确的情况下，LLMs能够从有限数据中学习新信息。然而，结果中存在一些幻觉迹象。为了减少幻觉，需要在当前设置中尝试不同的提示模板。这也归结为数据集的准备方式。语义切分等技术提供了一种将切分块作为独立信息实体的方式。这可以作为减少幻觉的数据集准备方案进一步探索。
- en: 8 Conclusion
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: The paper discussed on the topic of fine tuning open source Large Language Models
    with proprietary documents and code repositories. In the Dataset preparation sections
    detailed steps on creating the dataset from raw documents and code bases is given.
    It is followed by experiments with different methods of preparation and manual
    evaluation of the model responses with different LORA configurations. Finally
    some pointers observed during the fine tuning process are given as guidelines.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 论文讨论了如何使用专有文档和代码库对开源大型语言模型进行微调。数据集准备部分提供了从原始文档和代码库创建数据集的详细步骤。接着介绍了不同准备方法的实验和对不同LORA配置下模型响应的手动评估。最后，提供了一些在微调过程中观察到的指导建议。
- en: References
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm
    shift in machine translation: Boosting translation performance of large language
    models, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Haoran Xu, Young Jin Kim, Amr Sharaf, 和 Hany Hassan Awadalla. 机器翻译的范式转变：提升大型语言模型的翻译性能，2024年。'
- en: '[2] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin
    Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu,
    Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao,
    Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru
    Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao,
    Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu
    Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor
    Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, 2024.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin
    Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu,
    Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao,
    Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru
    Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao,
    Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu
    Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen
    Eleanor Jiang, 和 Wangchunshu Zhou。 Weaver：创意写作的基础模型，2024。'
- en: '[3] Zhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang Nie. Building emotional
    support chatbots in the era of llms, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Zhonghua Zheng, Lizi Liao, Yang Deng, 和 Liqiang Nie。 在 llms 时代构建情感支持聊天机器人，2023。'
- en: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom。 Llama 2：开源基础模型和微调聊天模型，2023。'
- en: '[5] Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. Fingpt: Democratizing
    internet-scale data for financial large language models, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, 和 Daochen Zha。 Fingpt：使互联网规模数据可用于金融大型语言模型，2023。'
- en: '[6] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi
    Xie. Pmc-llama: Towards building open-source language models for medicine, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, 和 Weidi
    Xie。 Pmc-llama：致力于构建医学领域的开源语言模型，2023。'
- en: '[7] Rajvardhan Patil and Venkat Gudivada. A review of current trends, techniques,
    and challenges in large language models (llms). Applied Sciences, 14(5), 2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Rajvardhan Patil 和 Venkat Gudivada。 大型语言模型（llms）当前趋势、技术和挑战综述。 应用科学，14(5)，2024。'
- en: '[8] Cheonsu Jeong. A study on the implementation of generative ai services
    using an enterprise data-based llm application architecture. Advances in Artificial
    Intelligence and Machine Learning, 03(04):1588–1618, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Cheonsu Jeong。 基于企业数据的 llm 应用架构生成性 AI 服务的实施研究。 人工智能与机器学习进展，03(04):1588–1618，2023。'
- en: '[9] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice,
    Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. The
    power of noise: Redefining retrieval for rag systems, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice,
    Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, 和 Fabrizio Silvestri。 噪声的力量：重新定义
    rag 系统的检索，2024。'
- en: '[10] Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto
    de M. Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg,
    Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati
    Sharma, Vijay Aski, and Ranveer Chandra. Rag vs fine-tuning: Pipelines, tradeoffs,
    and a case study on agriculture, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 安赫尔斯·巴拉格, 维纳姆拉·贝纳拉, 雷纳托·路易斯·德·弗雷塔斯·库尼亚, 罗伯托·德·M·埃斯特万·菲略, 托德·亨德里, 丹尼尔·霍尔斯坦,
    珍妮弗·马斯曼, 尼克·梅克伦堡, 萨拉·马尔瓦尔, 莱昂纳多·O·努内斯, 拉斐尔·帕迪利亚, 莫里斯·夏普, 布鲁诺·席尔瓦, 斯瓦蒂·夏尔马, 维贾伊·阿斯基,
    和 兰维尔·钱德拉. Rag 与微调：管道、权衡及农业案例研究, 2024.'
- en: '[11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 爱德华·J·胡, 沈跃龙, 菲利普·沃利斯, 泽远·艾伦-朱, 李远志, 王显, 吕旺, 和 陈伟柱. Lora：大型语言模型的低秩适配,
    2021.'
- en: '[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8():
    8-bit matrix multiplication for transformers at scale, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 蒂姆·德特梅尔斯, 迈克·刘易斯, 约内斯·贝尔卡达, 和 卢克·泽特勒莫耶. Llm.int8()：大规模变换器的 8 位矩阵乘法, 2022.'
- en: '[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. In A. Oh, T. Neumann, A. Globerson, K. Saenko,
    M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems,
    volume 36, pages 10088–10115\. Curran Associates, Inc., 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 蒂姆·德特梅尔斯, 阿尔提多罗·帕尼奥尼, 阿里·霍尔茨曼, 和 卢克·泽特勒莫耶. Qlora：量化 LLM 的高效微调。见 A. Oh,
    T. Neumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine 编, 《神经信息处理系统进展》，第 36
    卷，第 10088–10115 页。Curran Associates, Inc., 2023.'
- en: '[14] Mark Horowitz. 1.1 computing’s energy problem (and what we can do about
    it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical
    Papers (ISSCC), pages 10–14, Feb 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 马克·霍洛维茨. 1.1 计算的能耗问题（以及我们可以采取的措施）。见 2014 IEEE 国际固态电路会议技术论文摘要 (ISSCC),
    第 10–14 页, 2014 年 2 月.'
- en: '[15] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das,
    Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka,
    Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos
    Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul,
    and Pradeep Dubey. A study of bfloat16 for deep learning training, 2019.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 迪拉杰·卡拉姆卡尔, 德希瓦莎·穆迪格雷, 纳文·梅伦普迪, 迪潘卡尔·达斯, 库纳尔·巴纳吉, 萨西坎特·阿万查, 达尔玛·提贾·沃图里,
    纳塔拉杰·贾马拉马达卡, 黄剑宇, 赫克托·袁, 杨纪炎, 朴钟秀, 亚历山大·海内克, 埃万杰洛斯·乔尔加纳斯, 苏达尔尚·斯里尼瓦桑, 阿比谢克·昆杜,
    米莎·斯梅连斯基, 巴拉特·考尔, 和 普拉迪普·杜贝. 深度学习训练中的 bfloat16 研究, 2019.'
- en: '[16] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training
    of neural networks for efficient integer-arithmetic-only inference, 2017.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 贝诺瓦·雅各布, 斯基尔曼塔斯·克里吉斯, 博·陈, 孟龙·朱, 马修·唐, 安德鲁·霍华德, 哈特维格·亚当, 和 德米特里·卡列尼琴科.
    为高效整数算术推断量化和训练神经网络, 2017.'
- en: '[17] Memory Decreases! But Latency Increases…., howpublished = [https://github.com/timdettmers/bitsandbytes/issues/6](https://github.com/timdettmers/bitsandbytes/issues/6)
    .'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 内存减少！但延迟增加……, 如何出版 = [https://github.com/timdettmers/bitsandbytes/issues/6](https://github.com/timdettmers/bitsandbytes/issues/6)
    .'
- en: '[18] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe
    Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction
    tuning for large language models: A survey, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 张胜宇, 董林峰, 李晓雅, 张森, 孙小飞, 王书赫, 李吉伟, 胡润逸, 张天伟, 吴飞, 和 王国银. 大型语言模型的指令调整：综述,
    2023.'
- en: '[19] Rapid Automatic Keyword Extraction algorithm domain independent keyword
    extraction algorithm which tries to determine key phrases in a body of text by
    analyzing the frequency of word appearance and its co-occurance with other words
    in the text. [https://pypi.org/project/rake-nltk/](https://pypi.org/project/rake-nltk/).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 快速自动关键词提取算法领域独立关键词提取算法，试图通过分析词汇出现的频率及其与文本中其他词汇的共现来确定文本中的关键短语。 [https://pypi.org/project/rake-nltk/](https://pypi.org/project/rake-nltk/).'
- en: '[20] Querying local documents, powered by LLM. [https://github.com/snexus/llm-search/blob/main/src/llmsearch/parsers/doc.py](https://github.com/snexus/llm-search/blob/main/src/llmsearch/parsers/doc.py).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 本地文档查询，由 LLM 驱动。 [https://github.com/snexus/llm-search/blob/main/src/llmsearch/parsers/doc.py](https://github.com/snexus/llm-search/blob/main/src/llmsearch/parsers/doc.py).'
- en: '[21] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del
    Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli
    Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck,
    Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all
    you need, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie
    Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,
    Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien
    Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, 和 Yuanzhi Li. 只需要教科书，2023。'
