- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:35:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:35:52'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EvalAlign: 监督微调的多模态LLM与人类对齐数据用于评估文本生成图像模型'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16562](https://ar5iv.labs.arxiv.org/html/2406.16562)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16562](https://ar5iv.labs.arxiv.org/html/2406.16562)
- en: Zhiyu Tan¹  Xiaomeng Yang¹  Luozheng Qin¹  Mengping Yang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谭智宇¹  杨晓萌¹  邱洛铮¹  杨梦萍¹
- en: Cheng Zhang²  Hao Li³
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 程章²  郝力³
- en: ¹ Shanghai Academy of AI for Science  ² Carnegie Mellon University  ³ Fudan
    University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 上海人工智能科学研究院  ² 卡内基梅隆大学  ³ 复旦大学
- en: '[https://github.com/SAIS-FUXI/EvalAlign](https://github.com/SAIS-FUXI/EvalAlign)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/SAIS-FUXI/EvalAlign](https://github.com/SAIS-FUXI/EvalAlign)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The recent advancements in text-to-image generative models have been remarkable.
    Yet, the field suffers from a lack of evaluation metrics that accurately reflect
    the performance of these models, particularly lacking fine-grained metrics that
    can guide the optimization of the models. In this paper, we propose EvalAlign,
    a metric characterized by its accuracy, stability, and fine granularity. Our approach
    leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained
    on extensive datasets. We develop evaluation protocols that focus on two key dimensions:
    image faithfulness and text-image alignment. Each protocol comprises a set of
    detailed, fine-grained instructions linked to specific scoring options, enabling
    precise manual scoring of the generated images. We Supervised Fine-Tune (SFT)
    the MLLM to align closely with human evaluative judgments, resulting in a robust
    evaluation model. Our comprehensive tests across 24 text-to-image generation models
    demonstrate that EvalAlign not only provides superior metric stability but also
    aligns more closely with human preferences than existing metrics, confirming its
    effectiveness and utility in model assessment.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在文本生成图像模型方面的进展非常显著。然而，该领域仍然缺乏能够准确反映这些模型性能的评估指标，特别是缺乏可以指导模型优化的细粒度指标。本文提出了EvalAlign，这是一种以其准确性、稳定性和细粒度为特征的指标。我们的方法利用了在广泛数据集上预训练的多模态大语言模型（MLLMs）的能力。我们制定了关注两个关键维度的评估协议：图像真实性和文本-图像对齐。每个协议包含一组详细的、细粒度的说明与特定评分选项相关联，使生成的图像能够进行精准的手动评分。我们对MLLM进行监督微调（SFT），使其与人类评估判断紧密对齐，从而形成一个强健的评估模型。我们在24个文本生成图像模型上的综合测试表明，EvalAlign不仅提供了更优的指标稳定性，而且比现有指标更贴近人类偏好，证实了其在模型评估中的有效性和实用性。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Text-to-image models, such as DALL·E series [[38](#bib.bib38), [37](#bib.bib37),
    [3](#bib.bib3)], Imagen [[43](#bib.bib43)], and Stable Diffusion [[33](#bib.bib33)],
    have significantly impacted various domains such as entertainment, design, and
    education, by enabling high-quality image generation. These technologies not only
    advance the field of text-to-image generation but also enhance related applications
    such as video generation [[4](#bib.bib4), [60](#bib.bib60)], image editing [[46](#bib.bib46),
    [18](#bib.bib18), [59](#bib.bib59)], and personalized image generation [[13](#bib.bib13),
    [42](#bib.bib42)]. Despite this prevalence, in the practical use of generative
    models, we still lack evaluation methods to accurately evaluate existing text-to-image
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成图像模型，如DALL·E系列 [[38](#bib.bib38), [37](#bib.bib37), [3](#bib.bib3)]、Imagen
    [[43](#bib.bib43)] 和Stable Diffusion [[33](#bib.bib33)]，通过实现高质量图像生成，已在娱乐、设计和教育等各个领域产生了显著影响。这些技术不仅推动了文本生成图像领域的发展，还提升了相关应用，如视频生成
    [[4](#bib.bib4), [60](#bib.bib60)]、图像编辑 [[46](#bib.bib46), [18](#bib.bib18), [59](#bib.bib59)]
    和个性化图像生成 [[13](#bib.bib13), [42](#bib.bib42)]。尽管如此，在生成模型的实际应用中，我们仍然缺乏准确评估现有文本生成图像模型的方法。
- en: 'Existing benchmarks for text-to-image generation models are neither comprehensive
    nor accurate enough to meet current evaluation needs. The reasons for this include:
    (1) Limited model parameters: Current evaluation models have too few parameters,
    which restricts their ability to accurately represent images, leading to significant
    discrepancies compared to human evaluations. (2) Training data limitations: Some
    evaluation methods, such as Inception Score (IS) [[44](#bib.bib44)], Frechet Inception
    Distance (FID) [[15](#bib.bib15)], and CLIP Score [[14](#bib.bib14)], use models
    that have not been trained with synthesized images, which may introduce training
    bias and flaws the evaluation. (3) High annotation costs: Some methods, such as
    ImageReward [[56](#bib.bib56)], HPS [[55](#bib.bib55)] and HPS v2 [[54](#bib.bib54)],
    rely heavily on extensive human annotations, which significantly increases the
    cost of labeling. (4) Lack of detailed evaluation metric: The evaluation metrics
    do not provide fine-grained interpretability, preventing them from guiding model
    optimization effectively. (5) Computational inefficiency: The evaluation models
    require substantial computational resources, making them inefficient.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的文本到图像生成模型的基准测试既不全面也不准确，无法满足当前的评估需求。这些原因包括：（1）模型参数有限：目前的评估模型参数过少，限制了其准确表示图像的能力，导致与人类评估相比存在显著差异。（2）训练数据限制：一些评估方法，如Inception
    Score (IS) [[44](#bib.bib44)]，Frechet Inception Distance (FID) [[15](#bib.bib15)]，和CLIP
    Score [[14](#bib.bib14)]，使用的模型没有用合成图像进行训练，这可能引入训练偏差并使评估出现缺陷。（3）高标注成本：一些方法，如ImageReward
    [[56](#bib.bib56)]，HPS [[55](#bib.bib55)] 和HPS v2 [[54](#bib.bib54)]，过度依赖大量人工标注，显著增加了标注成本。（4）缺乏详细的评估指标：评估指标没有提供细粒度的可解释性，阻碍了其对模型优化的有效指导。（5）计算效率低：评估模型需要大量计算资源，效率低下。
- en: 'To address these issues, we propose EvalAlign, which offers low-cost, accurate,
    and efficient model evaluations while providing fine-grained, interpretable metrics.
    Specifically, we Supervised Fine-Tune (SFT) a Multimodal Large Language Model
    (MLLM) to align it with human annotations. Our focus is on two key evaluation
    aspects: image faithfulness and text-image alignment.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们提出了EvalAlign，它提供低成本、准确和高效的模型评估，同时提供细粒度的可解释指标。具体而言，我们对多模态大语言模型（MLLM）进行**监督微调**（SFT），使其与人工标注对齐。我们关注两个关键评估方面：图像忠实性和文本-图像对齐。
- en: 'Due to its pre-training on large-scale datasets and a large number of model
    parameters, the MLLM demonstrates excellent understanding and generalization capabilities
    for images and instructions. This makes it possible to design evaluation instruction
    based on MLLM to evaluate text-to-image models. However, since the pre-trained
    datasets do not include model-generated images (such as distorted body structures
    and human hands) or evaluation-related text instructions, using MLLM directly
    for model evaluation does not yield optimal results. Therefore, we employ SFT
    on a small amount of manually annotated data to align MLLM with human annotations
    for text-to-image generation. In summary, our main contributions can be summarized
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其在大规模数据集上的预训练和大量的模型参数，MLLM展现了对图像和指令的出色理解和泛化能力。这使得基于MLLM设计评估指令以评估文本到图像模型成为可能。然而，由于预训练数据集不包含模型生成的图像（如扭曲的身体结构和人手）或评估相关的文本指令，直接使用MLLM进行模型评估无法获得**最佳**结果。因此，我们在少量手动标注的数据上进行SFT，以使MLLM与人工标注对齐，用于文本到图像生成。总之，我们的主要贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a dataset specifically designed to address the evaluation challenges
    of text-to-image models. This dataset, derived from multiple data sources, has
    been thoroughly cleaned and systematically annotated by human. It enables precise
    evaluation of text-image alignment and image faithfulness.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一个专门设计用于应对文本到图像模型评估挑战的数据集。该数据集来源于多个数据源，经过彻底清理和系统标注。它使得对文本-图像对齐和图像忠实性的准确评估成为可能。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the EvalAlign evaluation metric, which accurately aligns evaluation
    metrics with human preferences. This method is cost-effective in terms of annotation
    and training, computationally efficient, and provides interpretable metrics.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了EvalAlign评估指标，它准确地将评估指标与人类偏好对齐。这种方法在标注和训练方面具有成本效益，计算上高效，并提供了可解释的指标。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct evaluations over 24 text-to-image models and compare EvalAlign with
    existing evaluation methods. Extensive experiments demonstrate that EvalAlign
    outperforms other methods in evaluating model performance.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对 24 个文本到图像模型进行了评估，并将 EvalAlign 与现有评估方法进行了比较。大量实验表明，EvalAlign 在评估模型性能方面优于其他方法。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Evaluations and benchmarks of text-to-image generation. Despite the incredible
    progress achieved by text-to-image generation, evaluations and benchmarks in this
    area are far from flawless and contain critical limitations. For example, the
    most commonly used metrics, IS [[44](#bib.bib44)], FID [[15](#bib.bib15)], and
    CLIPScore [[14](#bib.bib14)] are broadly recognized as inaccurate for their inconsistency
    with human perception. To address, HPS series [[55](#bib.bib55), [54](#bib.bib54)],
    PickScore [[20](#bib.bib20)], and ImageReward [[56](#bib.bib56)] introduced human
    preference prior on image assessing to the benchmark, thereby allowing better
    correlation with image quality. However, with varying source and size of training
    data, these methods merely score the evaluated images in a coarse and general
    way, which cannot serve as an indication for model evolution. Meanwhile, HEIM [[21](#bib.bib21)]
    combined automatic and human evaluation and holistically evaluated text-to-image
    generation in 12 aspects, such as alignment, toxicity, and so on. As a consequence,
    HEIM relies heavily on human labour, limiting its application within budget-limited
    research groups severely. [[30](#bib.bib30)] standardized the protocol and settings
    of human evaluation, ensuring its verifiable and reproducible. There are also
    some works bear a resemble with us. For instance, TIFA [[16](#bib.bib16)], Gecko [[53](#bib.bib53)]
    and LLMScore [[27](#bib.bib27)] also formulate the evaluation as a set of visual
    question answering procedure and use LLMs as evaluation models. However, while
    they all mainly focus on text-image alignment, our approach takes both text-image
    alignment and image faithfulness into consideration. Moreover, the evaluation
    of LLMScore requires an object detection stage, which introduces significantly
    extra inference latency to the evaluation pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像生成的评估和基准。尽管文本到图像生成取得了令人难以置信的进展，但该领域的评估和基准远非完美，存在关键的局限性。例如，最常用的指标，IS [[44](#bib.bib44)]、FID [[15](#bib.bib15)]
    和 CLIPScore [[14](#bib.bib14)] 被广泛认为在与人类感知不一致方面不准确。为了解决这一问题，HPS 系列 [[55](#bib.bib55),
    [54](#bib.bib54)]、PickScore [[20](#bib.bib20)] 和 ImageReward [[56](#bib.bib56)]
    引入了图像评估的人类偏好先验，从而实现了与图像质量更好的相关性。然而，由于训练数据的来源和规模各异，这些方法仅以粗略和一般的方式对评估图像进行打分，这不能作为模型演变的指示。同时，HEIM [[21](#bib.bib21)]
    结合了自动和人工评估，从12个方面，如对齐、毒性等，全面评估了文本到图像生成。因此，HEIM 在很大程度上依赖于人工劳动，严重限制了预算有限的研究组的应用。[[30](#bib.bib30)]
    标准化了人工评估的协议和设置，确保其可验证和可重复性。还有一些工作与我们类似。例如，TIFA [[16](#bib.bib16)]、Gecko [[53](#bib.bib53)]
    和 LLMScore [[27](#bib.bib27)] 也将评估表述为一组视觉问答过程，并使用 LLMs 作为评估模型。然而，虽然它们主要关注文本-图像对齐，但我们的方法同时考虑了文本-图像对齐和图像真实性。此外，LLMScore
    的评估需要一个物体检测阶段，这在评估流程中引入了显著的额外推理延迟。
- en: 'As illustrated in Table [1](#S2.T1 "Table 1 ‣ 2 Related Work ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), existing text-to-image evaluation methods contains various limitations,
    making them incapable to serve as a fine-grained, comprehensive, and human-preference
    aligned automatic benchmark. While our work fills in this gap economically, and
    can be employed to indicate evolution direction and support thorough analysis
    of text-to-image generation models.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[1](#S2.T1 "Table 1 ‣ 2 Related Work ‣ EvalAlign: Supervised Fine-Tuning
    Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models")所示，现有的文本到图像评估方法存在各种局限性，使其无法作为一个细粒度、全面且符合人类偏好的自动基准。虽然我们的工作经济地填补了这一空白，并可以用于指示演变方向以及支持对文本到图像生成模型的深入分析。'
- en: 'Table 1: Comparison of different evaluation metrics and frameworks for text-to-image
    generation. EvalAlign focuses on two key evaluation aspects, i.e., image faithfulness
    and text-image alignment, and supports human-aligned, fine-grained, and automatic
    evaluations. P: Prompt. I: Image. A: Annotation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1：不同评估指标和框架的比较，针对文本到图像生成。EvalAlign 关注两个关键评估方面，即图像真实性和文本-图像对齐，并支持符合人类偏好、细粒度和自动评估。P:
    提示。I: 图像。A: 注释。'
- en: '| Method | Venue | Benchmark Feature | Dataset Size | Evaluation Aspect |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 会议 | 基准特征 | 数据集大小 | 评估方面 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Human-aligned | Fine-grained | Automatic | P | I | A | Faithfulness | Alignment
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 人类对齐 | 细粒度 | 自动化 | P | I | A | 可信度 | 对齐 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Inception Score [[44](#bib.bib44)] | NeurIPS 2016 | ✗ | ✗ | ✓ | – | 1.3M
    | – | ✓ | ✗ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Inception Score [[44](#bib.bib44)] | NeurIPS 2016 | ✗ | ✗ | ✓ | – | 1.3M
    | – | ✓ | ✗ |'
- en: '| FID [[15](#bib.bib15)] | NeurIPS 2017 | ✗ | ✗ | ✓ | – | 1.3M | – | ✓ | ✗
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| FID [[15](#bib.bib15)] | NeurIPS 2017 | ✗ | ✗ | ✓ | – | 1.3M | – | ✓ | ✗
    |'
- en: '| CLIP-score [[14](#bib.bib14)] | EMNLP 2021 | ✗ | ✗ | ✓ | 400M | 400M | –
    | ✗ | ✓ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| CLIP-score [[14](#bib.bib14)] | EMNLP 2021 | ✗ | ✗ | ✓ | 400M | 400M | –
    | ✗ | ✓ |'
- en: '| HPS [[55](#bib.bib55)] | ICCV 2023 | ✓ | ✗ | ✓ | 25K | 98K | 25K | – | –
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| HPS [[55](#bib.bib55)] | ICCV 2023 | ✓ | ✗ | ✓ | 25K | 98K | 25K | – | –
    |'
- en: '| TIFA [[16](#bib.bib16)] | ICCV 2023 | ✓ | ✓ | ✓ | 4K | – | 25K | ✗ | ✓ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| TIFA [[16](#bib.bib16)] | ICCV 2023 | ✓ | ✓ | ✓ | 4K | – | 25K | ✗ | ✓ |'
- en: '| TVRHE [[30](#bib.bib30)] | CVPR 2023 | ✓ | ✗ | ✗ | – | – | – | ✓ | ✗ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| TVRHE [[30](#bib.bib30)] | CVPR 2023 | ✓ | ✗ | ✗ | – | – | – | ✓ | ✗ |'
- en: '| ImageReward [[56](#bib.bib56)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 8.8K | 68K |
    137K | – | – |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ImageReward [[56](#bib.bib56)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 8.8K | 68K |
    137K | – | – |'
- en: '| PickScore [[20](#bib.bib20)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 35K | 1M | 500K
    | – | – |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| PickScore [[20](#bib.bib20)] | NeurIPS 2023 | ✓ | ✗ | ✓ | 35K | 1M | 500K
    | – | – |'
- en: '| HPS v2 [[54](#bib.bib54)] | arXiv 2023 | ✓ | ✗ | ✓ | 107K | 430K | 645K |
    – | – |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| HPS v2 [[54](#bib.bib54)] | arXiv 2023 | ✓ | ✗ | ✓ | 107K | 430K | 645K |
    – | – |'
- en: '| HEIM [[21](#bib.bib21)] | NeurIPS 2023 | ✓ | ✓ | ✗ | – | – | – | ✓ | ✓ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| HEIM [[21](#bib.bib21)] | NeurIPS 2023 | ✓ | ✓ | ✗ | – | – | – | ✓ | ✓ |'
- en: '| Gecko [[53](#bib.bib53)] | arXiv 2024 | ✓ | ✓ | ✓ | 2K | – | 108K | ✗ | ✓
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Gecko [[53](#bib.bib53)] | arXiv 2024 | ✓ | ✓ | ✓ | 2K | – | 108K | ✗ | ✓
    |'
- en: '| LLMScore [[27](#bib.bib27)] | arXiv 2024 | ✓ | ✓ | ✓ | – | – | – | ✗ | ✓
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| LLMScore [[27](#bib.bib27)] | arXiv 2024 | ✓ | ✓ | ✓ | – | – | – | ✗ | ✓
    |'
- en: '| EvalAlign (ours) | – | ✓ | ✓ | ✓ | 3K | 21K | 132K | ✓ | ✓ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign（我们的） | – | ✓ | ✓ | ✓ | 3K | 21K | 132K | ✓ | ✓ |'
- en: Multimodal Large Language Models (MLLMs). Pretrained on massive text-only and
    image-text data, MLLMs have exhibited exceptional image-text joint understanding
    and generalization abilities, facilitating a large spectrum of downstream applications.
    Among the works major in MLLMs, LLaVA [[26](#bib.bib26), [24](#bib.bib24)] and
    MiniGPT4 [[61](#bib.bib61), [5](#bib.bib5)] proposed to conduct visual instruction
    tuning during SFT, so that MLLMs can be easily aligned to human preference and
    precisely answer fine-grained questions on visual content. Meanwhile, Video-LLaMA [[58](#bib.bib58)]
    and VideoChat [[22](#bib.bib22)] utilized MLLMs for video understanding. VILA [[23](#bib.bib23)]
    quantitatively proved that involving text-only instruction-tuning data during
    SFT can further ameliorate model performance on text-only and multimodal downstream
    tasks. LLaVA-NeXT [[25](#bib.bib25)] extracted visual tokens for both the resized
    input image and the segmented sub-images to provide more detailed visual information
    for MLLMs, achieving significant performance bonus on tasks with high-resolution
    input images.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs）。在大规模文本和图像-文本数据上进行预训练的 MLLMs 展现了卓越的图像-文本联合理解和泛化能力，促进了广泛的下游应用。在主要研究
    MLLMs 的工作中，LLaVA [[26](#bib.bib26), [24](#bib.bib24)] 和 MiniGPT4 [[61](#bib.bib61),
    [5](#bib.bib5)] 提出了在 SFT 期间进行视觉指令调优，以使 MLLMs 能够轻松地与人类偏好对齐，并准确回答关于视觉内容的细粒度问题。同时，Video-LLaMA [[58](#bib.bib58)]
    和 VideoChat [[22](#bib.bib22)] 利用 MLLMs 进行视频理解。VILA [[23](#bib.bib23)] 定量证明了在
    SFT 期间涉及文本-only 指令调优数据可以进一步改善模型在文本-only 和多模态下游任务上的表现。LLaVA-NeXT [[25](#bib.bib25)]
    提取了调整大小的输入图像和分割子图像的视觉标记，以提供更多详细的视觉信息，从而在处理高分辨率输入图像的任务中取得了显著的性能提升。
- en: In this paper, we manually annotate a minimal amount of high-quality visual
    instruction-tuning data to adapt MLLMs to perform human preference aligned and
    fine-grained evaluation. Owing to the generalization and multimodal understanding
    abilities of MLLMs, our experiments demonstrated that the finetuned MLLMs can
    accurately assess text-to-image generation models by following instructions regarding
    various fine-grained criteria of synthesised image.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们手动标注了一小部分高质量的视觉指令调优数据，以使多模态大型语言模型（MLLMs）适应执行与人类偏好对齐的细粒度评估。由于 MLLMs 的泛化能力和多模态理解能力，我们的实验表明，经过微调的
    MLLMs 可以通过遵循关于合成图像各种细粒度标准的指令，准确评估文本到图像生成模型。
- en: 3 EvalAlign Dataset Construction
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 EvalAlign 数据集构建
- en: To train, validate and test the effectiveness of our evaluation models, we build
    EvalAlign benchmark. Specifically, EvalAlign dataset is a meticulously annotated
    collection featuring fine-grained annotations for images generated from text prompts.
    This dataset comprises 21k images, each accompanied by detailed instructions.
    The compilation process for the EvalAlign Dataset encompasses prompt collection,
    image generation, and precise instruction-based annotation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练、验证和测试我们评估模型的有效性，我们构建了 EvalAlign 基准测试。具体而言，EvalAlign 数据集是一个经过精细标注的集合，包含了从文本提示生成的图像的细粒度注释。该数据集包括
    21k 张图像，每张图像都附有详细的说明。EvalAlign 数据集的编制过程包括提示收集、图像生成和基于指令的精确注释。
- en: 3.1 Prompts and Images Collection
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 提示和图像收集
- en: Prompt collection. To assess the capabilities of our model in terms of text-image
    alignment and image faithfulness, we collect, filter, and clean prompts from existing
    evaluation datasets and generated prompts based on LLM. These prompts encompass
    a diverse range from real-world user prompts, prompts generated through rule-based
    templates with LLM, to manually crafted prompts. Specifically, the utilized prompts
    are soureced from HPS [[55](#bib.bib55)], HRS-Bench [[2](#bib.bib2)], HPSv2 [[54](#bib.bib54)],
    TIFA [[16](#bib.bib16)], DSG [[7](#bib.bib7)], T2I-Comp [[17](#bib.bib17)], Winoground [[50](#bib.bib50)],
    DALL-EVAL [[8](#bib.bib8)], DiffusionDB [[52](#bib.bib52)], PartiPrompts [[57](#bib.bib57)],
    DrawBench [[43](#bib.bib43)], and JourneryDB [[49](#bib.bib49)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提示收集。为了评估我们模型在文本与图像对齐和图像真实性方面的能力，我们从现有评估数据集中收集、过滤和清理提示，并基于 LLM 生成提示。这些提示涵盖了从现实用户提示、通过规则模板与
    LLM 生成的提示，到手工制作的提示等多种范围。具体而言，所使用的提示来源于 HPS [[55](#bib.bib55)]、HRS-Bench [[2](#bib.bib2)]、HPSv2
    [[54](#bib.bib54)]、TIFA [[16](#bib.bib16)]、DSG [[7](#bib.bib7)]、T2I-Comp [[17](#bib.bib17)]、Winoground
    [[50](#bib.bib50)]、DALL-EVAL [[8](#bib.bib8)]、DiffusionDB [[52](#bib.bib52)]、PartiPrompts
    [[57](#bib.bib57)]、DrawBench [[43](#bib.bib43)] 和 JourneryDB [[49](#bib.bib49)]。
- en: Prompt curation. The prompts are collected for facilitating fine-grained and
    comprehensive evaluation on text-to-image models in terms of image faithfulness
    and text-image alignment. Considering some of the collected prompt are unsuitable
    for these two evaluation tasks, we filter the collected prompts to ensure their
    quantity, quality and diversity. For the image faithfulness evaluation task, we
    prioritize prompts related to human, animals, and other tangible objects, as prompts
    depicting sci-fi scenarios are less suitable for this type of assessment. Our
    filtering process initially selects prompts that describe human, animals, and
    other real objects. After deduplicating these prompts, we carefully selected 1,500
    distinct prompts with varying topic, background and style for further evaluation.
    The selected prompts encompass 10k subjects across 15 categories. For the text-image
    alignment task, we refined our selection based on descriptions of style, color,
    quantity, and spatial relationships in the prompts. Only those prompts containing
    relevant descriptions and exceeding 15 words in length were considered, culminating
    in a final set of 1,500 prompts for evaluation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提示策划。收集这些提示是为了在文本到图像模型的图像真实性和文本与图像对齐方面进行细粒度和全面的评估。考虑到一些收集到的提示不适合这两种评估任务，我们过滤了收集到的提示，以确保其数量、质量和多样性。对于图像真实性评估任务，我们优先考虑与人类、动物及其他有形物体相关的提示，因为描述科幻场景的提示不太适合这种评估。我们的过滤过程最初选择描述人类、动物和其他真实物体的提示。在去重这些提示后，我们仔细挑选了
    1,500 个主题、背景和风格各异的提示用于进一步评估。所选提示涵盖了 15 个类别的 10k 个主题。对于文本与图像对齐任务，我们根据提示中的风格、颜色、数量和空间关系的描述来精炼我们的选择。只有那些包含相关描述且长度超过
    15 个词的提示被考虑，最终得出 1,500 个提示用于评估。
- en: '![Refer to caption](img/bff799c750b2bca9e6aac29a9c23f5fe.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bff799c750b2bca9e6aac29a9c23f5fe.png)'
- en: 'Figure 1: Overview of EvalAlign. We collect, filter and clean prompts from
    various sources to ensure their quantity, quality and diversity. We use 8 state-of-the-art
    text-to-image models to the generate images for evaluation. These synthesized
    images are then delegated to human annotators for thorough multi-turn annotation.
    Finally, the annotated data are used to SFT train a MLLM to align it with fine-grained
    human preference, thereby adapting the model to perform fine-grained human-aligned
    text-to-image evaluations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：EvalAlign概览。我们从各种来源收集、过滤和清理提示，以确保其数量、质量和多样性。我们使用8种最先进的文本到图像模型生成图像用于评估。这些合成图像随后交由人工注释员进行彻底的多轮注释。最终，注释后的数据用于对MLLM进行SFT训练，使其与细致的人类偏好对齐，从而使模型能够进行细粒度的人类对齐的文本到图像评估。
- en: Image generation. To train and evaluate the MLLM, we use a diverse set of images
    generated by various models using the aforementioned prompts, allowing for detailed
    human annotation. For each prompt, multiple images are generated across different
    models. The dataset is enriched by images synthesized by models with varying architectures
    and scales, enhancing the diversity essential for a comprehensive evaluation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成。为了训练和评估MLLM，我们使用了通过各种模型生成的多样化图像，利用上述提示进行详细的人类注释。对于每个提示，多个模型生成的图像被纳入数据集中。数据集通过具有不同架构和规模的模型合成的图像得到了丰富，增强了全面评估所需的多样性。
- en: This variety not only tests the MLLM generalization capabilities but also aids
    in developing a model with broader applicability. The training dataset incorporates
    images from 8 models, whereas the test dataset spans 24 models. The inclusion
    of 16 unseen models in the test set is crucial for evaluating the MLLM to generalize
    beyond its training data. For detailed information on the inference setting of
    each model, please refer to the supplementary material. This structured approach
    ensures a robust framework for training and validating the MLLM, positioning it
    as a versatile and adaptive tool in the field of image generation from textual
    descriptions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性不仅测试了MLLM的泛化能力，还帮助开发具有更广泛适用性的模型。训练数据集包含来自8个模型的图像，而测试数据集涵盖24个模型。测试集中的16个未见过的模型对于评估MLLM是否能够超越其训练数据进行泛化至关重要。有关每个模型推理设置的详细信息，请参阅补充材料。这一结构化的方法确保了对MLLM的训练和验证框架的稳健性，使其成为文本描述生成图像领域中的一种多功能和适应性强的工具。
- en: 3.2 Data Annotation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据注释
- en: 'Prompt annotation. For text prompts focused on text-image alignment, we begin
    by annotating the entities and their attributes within the text, as illustrated
    in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Prompts and Images Collection ‣ 3 EvalAlign
    Dataset Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models"). Our annotators extract
    the entities mentioned in the prompts and label each entity with corresponding
    attributes, including quantity, color, spatial relationships, and actions. During
    the annotation, we also ask the annotators annotate the overall style of the image
    if described in the corresponding prompt and report prompts that contain toxic
    and NSFW content. These high-quality and detailed annotations facilitate the subsequent
    SFT training and evaluation of the MLLM. This meticulous annotation procedure
    ensures that the MLLM can accurately align and respond to the nuanced details
    specified in the prompts, enhancing both the training process and the model’s
    performance in generating images that faithfully reflect the described attributes
    and style.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '提示注释。对于关注文本-图像对齐的文本提示，我们首先对文本中的实体及其属性进行注释，如图[1](#S3.F1 "图1 ‣ 3.1 提示和图像收集 ‣
    3 EvalAlign数据集构建 ‣ EvalAlign: 通过人类对齐数据对多模态LLMs进行监督微调以评估文本到图像模型")所示。我们的注释员提取提示中提到的实体，并标记每个实体的对应属性，包括数量、颜色、空间关系和动作。在注释过程中，我们还要求注释员在提示中描述的情况下注释图像的整体风格，并报告包含有害和NSFW内容的提示。这些高质量和详细的注释有助于后续的MLLM的SFT训练和评估。这一精细的注释过程确保了MLLM能够准确对齐并响应提示中指定的细微细节，从而提高训练过程和模型生成忠实反映描述的属性和风格的图像的表现。'
- en: 'Image annotation. The images generated by text-to-image models often present
    challenges such as occluded human body parts, which can impede the effectiveness
    of SFT training and evaluation of the MLLM. Therefore, to address these challenges
    and enhance the model’s training and evaluative capabilities, specific annotations
    are applied to all images depicting human and animals. These annotations include:
    presence of human or animal faces; visibility of hands; visibility of limbs. By
    implementing these annotations, we ensure that the MLLM can more effectively learn
    from and assess the completeness and faithfulness of the generated images. This
    structured approach to annotation not only aids in identifying common generation
    errors but also optimizes the model’s ability to generate more accurate and realistic
    images, thereby improving both training outcomes and the model’s overall performance
    in generating coherent and contextually appropriate visual content.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标注。文本到图像模型生成的图像经常出现诸如遮挡的人体部位等挑战，这可能会妨碍SFT训练和MLLM的评估。因此，为了解决这些挑战并提升模型的训练和评估能力，对所有描绘人类和动物的图像应用了特定的标注。这些标注包括：人脸或动物脸的存在；手部的可见性；肢体的可见性。通过实施这些标注，我们确保MLLM能更有效地从生成图像中学习并评估其完整性和忠实度。这种结构化的标注方法不仅有助于识别常见的生成错误，还优化了模型生成更准确和真实图像的能力，从而提升训练效果和模型在生成连贯且上下文适当的视觉内容方面的整体表现。
- en: Instruction-Fine-tuning data annotation. To align the MLLM with human preference
    prior on fine-grained image assessing, we can train the model on a minimal amount
    of fine-grained human preference data through SFT training. As a consequence,
    we devise two sets of questions, each is concentrated on a specific fine-grained
    aspect of image faithfulness and image-text alignment, and ask human annotators
    to answer these questions to acquire the fine-grained human preference data. To
    aid the human annotators to understand the meaning of each question and its available
    answer options, thereby ensuring high annotation quality, we employ a thorough
    and comprehensive procedure of annotation preparation. First, we write a detailed
    annotation guideline and conduct a training for the annotators to explain the
    annotation guideline and answer their questions about the annotation (see supplementary
    materials for the annotation guideline). Then, we conduct a multi-turn trial annotation
    on another 50 synthesized images. After each trial, we calculate the Cohen’s kappa
    coefficient and interpret annotation guidelines for our annotators. In total,
    we conduct nine turns of trial annotation, and in the last turn of the trial,
    the Cohen’s kappa coefficient of our annotators reaches $0.681$, indicating high
    inter-annotator reliability and high annotation quality. More results about the
    trial annotation will be included in the supplementary materials.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 指令-细化数据标注。为了使MLLM与人类在细致图像评估上的偏好保持一致，我们可以通过SFT训练在最少量的细化人类偏好数据上训练模型。因此，我们设计了两组问题，每组集中于图像忠实度和图像-文本对齐的特定细致方面，并请人工标注者回答这些问题以获取细化的人类偏好数据。为了帮助人工标注者理解每个问题及其可选答案，从而确保高质量的标注，我们采用了全面细致的标注准备程序。首先，我们编写了详细的标注指南，并对标注者进行了培训，以解释标注指南并回答他们有关标注的问题（详见附录材料中的标注指南）。然后，我们在另外50张合成图像上进行多轮试验标注。每轮试验后，我们计算Cohen的kappa系数并为我们的标注者解读标注指南。总共进行了九轮试验标注，在最后一轮试验中，我们的标注者的Cohen
    kappa系数达到了$0.681$，表明标注者之间的一致性和标注质量都很高。更多关于试验标注的结果将包含在附录材料中。
- en: After completing the aforementioned preparations, we delegate the images filtered
    during image annotation to 10 annotators and ask them to complete the annotation
    just as how they did in the trial annotation. Furthermore, during the whole annotation
    procedure, four text-to-image generation experts conduct random sampling quality
    inspection on the present annotated results, causing a second and a third re-annotation
    on 423 and 112 inspection-failed samples. Overall, owing to the valuable work
    of our human annotators and our fastidious annotation procedure, we get quality-sufficient
    instruction-tuning data required for the SFT training of the MLLM. More details
    of the annotation procedure will be introduced in the supplementary files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上述准备工作后，我们将图像注释期间过滤的图像分配给10位注释员，并要求他们按照试注释时的方式完成注释。此外，在整个注释过程中，四位文本生成图像的专家会对当前注释结果进行随机抽样质量检查，这导致了对423个和112个检查失败样本的第二次和第三次重新注释。总体而言，由于我们人类注释员的宝贵工作和我们严格的注释程序，我们获得了SFT训练MLLM所需的质量充分的指令调整数据。注释程序的更多细节将在补充文件中介绍。
- en: 3.3 Dataset Statistics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据集统计
- en: 'To summarize, we generate 24k images from 3k prompts based on 8 text-to-image
    models, which includes DeepFloyd IF [[1](#bib.bib1)], SD15 [[40](#bib.bib40)],
    LCM [[28](#bib.bib28)], SD21 [[40](#bib.bib40)], SDXL [[33](#bib.bib33)], Wuerstchen [[32](#bib.bib32)],
    Pixart [[6](#bib.bib6)], and SDXL-Turbo [[48](#bib.bib48)]. After data filtering,
    4.5k images were selected as annotation data for task of text-image alignment.
    Subsequently, these images were carefully annotated to generate 13.5k text-image
    pairs, where 11.4k were used to the training dataset and 2.1k to the validation
    dataset. For the image faithfulness task, we select 12k images for annotate, producing
    36k text-image pairs, with 30k are used to the training dataset and 6.2k to the
    validation dataset. Additionally, we employed 24 text-to-image models to generate
    2.4k images from 100 prompts. After annotation, these images were used as testing
    dataset. Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign Dataset
    Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models") and Figure [3](#S3.F3 "Figure 3 ‣ 3.3
    Dataset Statistics ‣ 3 EvalAlign Dataset Construction ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") show the distribution of objects in different categories within our prompts,
    demonstrating the diversity and balance of our prompts.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们基于8个文本生成图像模型，从3k个提示生成了24k个图像，这些模型包括DeepFloyd IF [[1](#bib.bib1)]，SD15 [[40](#bib.bib40)]，LCM [[28](#bib.bib28)]，SD21 [[40](#bib.bib40)]，SDXL [[33](#bib.bib33)]，Wuerstchen [[32](#bib.bib32)]，Pixart [[6](#bib.bib6)]，和SDXL-Turbo [[48](#bib.bib48)]。经过数据过滤，选择了4.5k个图像作为文本-图像对齐任务的注释数据。随后，这些图像经过精心注释，生成了13.5k个文本-图像对，其中11.4k用于训练数据集，2.1k用于验证数据集。对于图像真实性任务，我们选择了12k个图像进行注释，生成了36k个文本-图像对，其中30k用于训练数据集，6.2k用于验证数据集。此外，我们使用24个文本生成图像模型从100个提示生成了2.4k个图像。经过注释后，这些图像被用作测试数据集。图 [3](#S3.F3
    "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign Dataset Construction ‣ EvalAlign:
    Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating
    Text-to-Image Models")和图 [3](#S3.F3 "Figure 3 ‣ 3.3 Dataset Statistics ‣ 3 EvalAlign
    Dataset Construction ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with
    Human-Aligned Data for Evaluating Text-to-Image Models")展示了我们提示中不同类别对象的分布，体现了我们提示的多样性和平衡性。'
- en: '![Refer to caption](img/17b5b4110ec3af14f95215057517430f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17b5b4110ec3af14f95215057517430f.png)'
- en: 'Figure 2: Statistics of prompts on evaluating text-to-image alignment. Prompts
    in our text-to-image alignment benchmark covers a broad range of concepts commonly
    used in text-to-image generation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 评估文本生成图像对齐的提示统计。我们文本生成图像对齐基准中的提示涵盖了文本生成图像中常用的广泛概念。'
- en: '![Refer to caption](img/cac56fc5e2d00b8e486c6e7df212f128.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cac56fc5e2d00b8e486c6e7df212f128.png)'
- en: 'Figure 3: Statistics of prompts on evaluating image faithfulness. Prompts in
    our image faithfulness benchmark covers a broad range of objects and categories
    that related to image faithfulnes.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 评估图像真实性的提示统计。我们图像真实性基准中的提示涵盖了与图像真实性相关的广泛对象和类别。'
- en: 4 Training and Evaluation Methods
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练和评估方法
- en: 4.1 Supervised Fine-Tuning the MLLM
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 对MLLM的监督微调
- en: 'As we mentioned above, we use MLLMs as the evaluation models and let it to
    answer a set of carefully-designed instructions, thereby achieving quantitative
    measurement of fine-grained text-to-image generation skills. Due to training bias,
    zero-shot MLLMs perform poorly when it comes to fine-grained evaluation on synthesized
    images, particularly in term of image faithfulness. To address, we conduct SFT
    training on the annotated data to align the MLLM with human preference prior on
    fine-grained image assessing. Formally, the SFT training sample can be denoted
    as a triplet, i.e., $(Q,M,A)$ denotes the question (or the instruction), the multimodal
    input and the answer, respectively. Specifically, $Q$ is mainly the image and
    necessary textual description, while $A$. Specifically, the loss function can
    be formulated as follows, where $N$ is the length of the ground truth answer:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用 MLLMs 作为评估模型，并让其回答一组精心设计的指令，从而实现对细粒度文本到图像生成技能的定量测量。由于训练偏差，零-shot MLLMs
    在合成图像的细粒度评估中表现不佳，特别是在图像真实性方面。为了解决这一问题，我们在标注数据上进行 SFT 训练，以使 MLLM 与人类对细粒度图像评估的偏好对齐。形式上，SFT
    训练样本可以表示为一个三元组，即 $(Q,M,A)$ 表示问题（或指令）、多模态输入和答案。具体来说，$Q$ 主要是图像和必要的文本描述，而 $A$。具体来说，损失函数可以如下公式化，其中
    $N$ 是真实答案的长度：
- en: '|  | $\displaystyle L(\theta)=\sum^{N}_{i=1}\log p(A_{i}&#124;Q,M,A_{= 1024x1024) for the subsequent 194k steps.
    While, SD v1.2 was initialized from v1.1 and further finetuned for 515k steps
    at resolution 512x512 on laion-aesthetics v2 5+ (a subset of laion2B-en, filtered
    to images with an original size >= 512x512, estimated aesthetics score > 5.0,
    and an estimated watermark probability < 0.5). SD v1.4 is initialized from v1.2
    and subsequently finetuned for 225k steps at resolution 512x512 on laion-aesthetics
    v2 5+. This version incorporates a 10% dropping of the text-conditioning to improve
    classifier-free guidance sampling. Similar to SD v1.4, SD v1.5 is resumed from
    SD v1.2 and trained 595k steps at resolution 512x512 on laion-aesthetics v2 5+,
    with 10% dropping of the text-conditioning.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SD v1.1在laion2B-en上以256x256的分辨率训练了237k步，随后在laion-high-resolution上以512x512的分辨率进行了194k步训练（来自LAION-5B的170M样本，分辨率>=1024x1024）。而SD
    v1.2从v1.1初始化，并在laion-aesthetics v2 5+（laion2B-en的一个子集，过滤为原始大小>=512x512、估计美学分数>5.0和估计水印概率= 4.5\. Then it is further
    trained for 850k steps at resolution 512x512 on the same dataset on images with
    resolution >= 512x512\. SD v2.0 is resumed from stable-diffusion v2 base and trained
    for 150k steps using a v-objective on the same dataset. After that, it is further
    finetuned for another 140k steps on 768x768 images. SD v2.1 is finetuned from
    SD v2.0 with an additional 55k steps on the same dataset (with punsafe=0.1), and
    then finetuned for another 155k extra steps with punsafe=0.98.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SD v2 基础模型从头开始训练 550k 步，分辨率为 256x256，数据集为经过筛选的 LAION-5B，使用 LAION-NSFW 分类器（punsafe
    = 0.1）和美学评分 >= 4.5。然后在同一数据集上的图像（分辨率 >= 512x512）上，进一步训练 850k 步，分辨率为 512x512。SD
    v2.0 从 stable-diffusion v2 基础模型恢复，并在同一数据集上使用 v-objective 训练 150k 步。之后，再进一步微调 140k
    步，分辨率为 768x768。SD v2.1 从 SD v2.0 微调，再额外训练 55k 步，数据集相同（punsafe=0.1），然后再额外训练 155k
    步，punsafe=0.98。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stable Diffusion XL {v1.0, Refiner v1.0}. Stable Diffusion XL (SDXL) is a powerful
    text-to-image generation model that iterates on the previous Stable Diffusion
    models in three key ways: (1) its UNet is 3x larger and SDXL combines a second
    text encoder (OpenCLIP ViT-bigG/14) with the original text encoder to significantly
    increase the number of parameters; (2) it introduces size and crop-conditioning
    to preserve training data from being discarded and gain more control over how
    a generated image should be cropped; (3) it introduces a two-stage model process;
    the base model (can also be run as a standalone model) generates an image as an
    input to the refiner model which adds additional high-quality details.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stable Diffusion XL {v1.0, Refiner v1.0}。Stable Diffusion XL (SDXL) 是一个强大的文本到图像生成模型，相比之前的
    Stable Diffusion 模型有三个关键改进：(1) 它的 UNet 大小增加了 3 倍，SDXL 结合了第二个文本编码器 (OpenCLIP ViT-bigG/14)
    和原始文本编码器，显著增加了参数数量；(2) 它引入了尺寸和裁剪条件，以防止训练数据被丢弃，并增加了对生成图像裁剪方式的控制；(3) 它引入了两阶段模型过程；基础模型（也可以作为独立模型运行）生成图像作为
    refiner 模型的输入，后者添加了额外的高质量细节。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pixart-Alpha. Pixart-Alpha is a model that can be used to generate and modify
    images based on text prompts. It is a Transformer Latent Diffusion Model that
    uses one fixed, pretrained text encoders (T5)) and one latent feature encoder
    (VAE).
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pixart-Alpha。Pixart-Alpha 是一个可以根据文本提示生成和修改图像的模型。它是一个 Transformer 潜在扩散模型，使用一个固定的、预训练的文本编码器
    (T5) 和一个潜在特征编码器 (VAE)。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Latent Consistency Model Stable Diffusion XL Latent Consistency Model Stable
    Diffusion XL (LCM SDXL) [[28](#bib.bib28)] enables SDXL for swift inference with
    minimal steps. Viewing the guided reverse diffusion process as solving an augmented
    probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution
    of such ODE in latent space, mitigating the need for numerous iterations and allowing
    rapid, high-fidelity sampling.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Latent Consistency Model Stable Diffusion XL Latent Consistency Model Stable
    Diffusion XL (LCM SDXL) [[28](#bib.bib28)] 使 SDXL 能够快速推断，步骤最少。通过将引导的反向扩散过程视为解决增强的概率流
    ODE (PF-ODE)，LCM 旨在直接预测这种 ODE 在潜在空间中的解决方案，从而减少大量迭代并允许快速、高保真度的采样。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dreamlike Diffusion 1.0. Dreamlike Diffusion 1.0 [[10](#bib.bib10)] is a SD
    v1.5 model finetuned on high-quality art images by dreamlike.art.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dreamlike Diffusion 1.0。Dreamlike Diffusion 1.0 [[10](#bib.bib10)] 是一个基于 dreamlike.art
    高质量艺术图像微调的 SD v1.5 模型。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dreamlike Photoreal 2.0. Dreamlike Photoreal 2.0 [[11](#bib.bib11)] is a photorealistic
    text-to-image latent diffusion model resumed from SD v1.5 by dreamlike art. This
    model was finetuned on 768x768 images, it works pretty good with resolution 768x768,
    640x896, 896x640 and higher resolution such as 768x1024.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Dreamlike Photoreal 2.0。Dreamlike Photoreal 2.0 [[11](#bib.bib11)] 是一个从 SD v1.5
    恢复的逼真文本到图像潜在扩散模型，由 dreamlike art 制作。该模型经过 768x768 图像的微调，在 768x768、640x896、896x640
    及更高分辨率如 768x1024 的图像中效果很好。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Openjourney v1, v2. Openjourney [[34](#bib.bib34)] is an open-source text-to-image
    generation model resumed from SD v1.5 and finetuned on Midjourney images by PromptHero.
    Openjourney v2 [[35](#bib.bib35)] was further finetuned using another 124000 images
    for 12400 steps, about 4 epochs and 32 training hours.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Openjourney v1, v2。Openjourney [[34](#bib.bib34)] 是一个从 SD v1.5 恢复的开源文本到图像生成模型，由
    PromptHero 基于 Midjourney 图像进行了微调。Openjourney v2 [[35](#bib.bib35)] 进一步通过 124000
    张图像的 12400 步（约 4 个周期和 32 个训练小时）进行了微调。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Redshift Diffusion. Redshift Diffusion [[39](#bib.bib39)] is a Stable Diffusion
    model finetuned on high-resolution 3D artworks.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Redshift Diffusion。Redshift Diffusion [[39](#bib.bib39)] 是一个在高分辨率3D艺术作品上经过微调的Stable
    Diffusion模型。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Vintedois Diffusion. Vintedois Diffusion [[51](#bib.bib51)] is a Stable Diffusion
    v1.5 model finetuned on a large number of high-quality images with simple prompts
    to generate beautiful images without a lot of prompt engineering.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Vintedois Diffusion。Vintedois Diffusion [[51](#bib.bib51)] 是一个在大量高质量图像上经过微调的Stable
    Diffusion v1.5模型，能够用简单的提示生成美丽的图像，而无需大量的提示工程。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Safe Stable Diffusion {Weak, Medium, Strong, Max}. Safe Stable Diffusion [[31](#bib.bib31)]
    is an enhanced version of the SD v1.5 model by mitigating inappropriate degeneration
    caused by pretraining on unfiltered web-crawled datasets. For instance SD may
    unexpectedly generate nudity, violence, images depicting self-harm, and otherwise
    offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that
    drastically reduces this type of content. Specifically, it has an additional safety
    guidance mechanism that aims to suppress and remove inappropriate content (hate,
    harassment, violence, self-harm, sexual content, shocking images, and illegal
    activity) during image generation. The strength levels for inappropriate content
    removal are categorized as: {Weak, Medium, Strong, Max}.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Safe Stable Diffusion {Weak, Medium, Strong, Max}。Safe Stable Diffusion [[31](#bib.bib31)]
    是对SD v1.5模型的增强版本，通过减少由于在未经过滤的网络爬取数据集上预训练而引起的不适当内容的生成。例如，SD可能会意外生成裸露、暴力、自我伤害以及其他令人反感的内容。Safe
    Stable Diffusion 是Stable Diffusion的扩展，极大地减少了这类内容。具体而言，它具有额外的安全指导机制，旨在在图像生成过程中压制和移除不适当的内容（仇恨、骚扰、暴力、自我伤害、色情内容、令人震惊的图像以及非法活动）。不适当内容移除的强度级别分类为：{Weak,
    Medium, Strong, Max}。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MultiFusion. MultiFusion [[29](#bib.bib29)] is a multimodal, multilingual diffusion
    model that extends the capabilities of SD v1.4 by integrating various modules
    to transfer capabilities to the downstream model. This combination results in
    novel decoder embeddings, which enable prompting of the image generation model
    with interleaved multimodal, multilingual inputs, despite being trained solely
    on monomodal data in a single language.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MultiFusion。MultiFusion [[29](#bib.bib29)] 是一个多模态、多语言的扩散模型，通过集成各种模块扩展了SD v1.4的能力，将能力传递到下游模型。这种组合产生了新型解码器嵌入，使得图像生成模型能够使用交错的多模态、多语言输入进行提示，尽管该模型仅在单语言的单模态数据上进行训练。
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DeepFloyd-IF { M, L, XL } v1.0. DeepFloyd-IF [[1](#bib.bib1)] is a novel state-of-the-art
    open-source text-to-image model with a high degree of photorealism and language
    understanding. It is a modular composed of a frozen text encoder and three cascaded
    pixel diffusion modules: a base model that generates 64x64 image based on text
    prompt and two super-resolution models, each designed to generate images of increasing
    resolution: 256x256 and 1024x1024, respectively. All stages of the model utilize
    a frozen text encoder based on the T5 transformer to extract text embeddings,
    which are then fed into a UNet architecture enhanced with cross-attention and
    attention pooling. Besides, it underscores the potential of larger UNet architectures
    in the first stage of cascaded diffusion models and depicts a promising future
    for text-to-image synthesis. The model is available in three different sizes:
    M, L, and XL. M has 0.4B parameters, L has 0.9B parameters, and XL has 4.3B parameters.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepFloyd-IF { M, L, XL } v1.0。DeepFloyd-IF [[1](#bib.bib1)] 是一种新型的最先进开源文本到图像模型，具有高度的真实感和语言理解能力。它是一个模块化系统，由一个冻结的文本编码器和三个级联像素扩散模块组成：一个基础模型根据文本提示生成64x64的图像，以及两个超分辨率模型，分别设计用于生成256x256和1024x1024分辨率的图像。模型的所有阶段都使用基于T5变换器的冻结文本编码器来提取文本嵌入，然后输入到一个经过交叉注意力和注意力池化增强的UNet架构中。此外，它强调了在级联扩散模型的第一阶段中较大UNet架构的潜力，并描绘了文本到图像合成的美好前景。该模型提供三种不同的尺寸：M、L和XL。M有0.4B参数，L有0.9B参数，XL有4.3B参数。
- en: Throughout all of the conducted experiments, we use the default inference settings
    for each evaluated models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有进行的实验中，我们使用了每个评估模型的默认推理设置。
- en: Appendix E Instruction Templates
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 指令模板
- en: 'Here, we present every instruction used for EvalAlign evaluation on image faithfuleness
    and text-image alignment. The templates contain some placeholders set for filling
    in the corresponding attributes of the input images during the evaluation. For
    example, a specific “” and “” can be “people, laptop,
    scissors.” and “plate: 1, turkey sandwich: 3, lettuce: 1.”, respectively.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们展示了用于 EvalAlign 评估图像真实性和文本-图像对齐的每一条指令。这些模板包含一些占位符，用于在评估过程中填充输入图像的相应属性。例如，特定的“”和“”可以是“people,
    laptop, scissors.”和“plate: 1, turkey sandwich: 3, lettuce: 1.”，分别表示。'
- en: 'For EvalAlign evaluation on image faithfulness, we devise 5 questions concentrate
    on the faithfulness of the generated body structure, generated face, generated
    hand, generated objects, as well as generation adherence to commonsense and logic.
    The instruction templates for these fine-grained criteria are as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 EvalAlign 在图像真实性方面的评估，我们设计了 5 个问题，集中在生成的身体结构、生成的面孔、生成的手部、生成的对象以及生成是否遵循常识和逻辑上。这些细化标准的指令模板如下：
- en: '[Q1]:Are
    there any issues with the [human/animals] body structure in the image, such as
    multiple arms, missing limbs or legs when not obscured, multiple heads, limb amputations,
    and etc? [OPTIONS]: 0.There are no human or animal body in the picture; 1.The
    body structure of the people or animals in the picture has a very grievous problem
    that is unbearable; 2.The body structure of the people or animals in the picture
    has some serious problems and is not acceptable; 3.The body structure of the people
    or animals in the picture has a slight problem that does not affect the senses;
    4.The body structure of the people or animals in the picture is basically fine,
    with only a few flaws; 5.The body structure of the people or animals in the picture
    is completely fine and close to reality.[Q2]:Are there any issues with
    the [human/animals] hands in the image, such as having more or less than five
    fingers when not obscured, broken fingers, disproportionate finger sizes, abnormal
    nail size proportions, and etc? [OPTIONS]: 0.No human or animal hands are shown
    in the picture; 1.The hand in the picture has a very grievous problem that is
    unbearable; 2.The hand in the picture has some serious problems and is not acceptable;
    3.The hand in the picture has a slight problem that does not affect the senses;
    4.The hand in the picture is basically fine, with only a few flaws; 5.The hands
    in the picture are completely fine and close to reality.[Q3]:Are there any issues with
    [human/animals] face in the image, such as facial distortion, asymmetrical faces,
    abnormal facial features, unusual expressions in the eyes, and etc? [OPTIONS]:
    0.There is no face of any person or animal in the picture; 1.The face of the person
    or animal in the picture has a very grievous problem that is unbearable; 2.The
    face of the person or animal in the picture has some serious problems and is not
    acceptable; 3.The face of the person or animal in the picture has a slight problem
    that does not affect the senses; 4.The face of the person or animal in the picture
    is basically fine, with only a few flaws; 5.The face of the person or animal in
    the picture is completely fine and close to reality.[Q4]:Are there any issues or
    tentative errors with objects in the image that do not correspond with the real
    world, such as distortion of items, and etc? [OPTIONS]: 0.There are objects in
    the image that completely do not match the real world, which is very serious and
    intolerable; 1.There are objects in the image that do not match the real world,
    which is quite serious and unacceptable; 2.There are slightly unrealistic objects
    in the image that do not affect the senses; 3.There are basically no objects in
    the image that do not match the real world, only some flaws; 4.All objects in
    the image match the real world, no problem.[Q5]:Does the generated image
    contain elements that violate common sense or logical rules, such as animal/human
    with inconsistent anatomy, object-context mismatch, impossible physics, scale
    and proportion issues, temporal and spatial inconsistencies, hybrid objects, and
    etc? [OPTIONS]: 0.The image contains elements that violate common sense or logical
    rules, which is very grievous and intolerable; 1.The presence of elements in the
    image that seriously violate common sense or logical rules is unacceptable; 2.The
    image contains elements that violate common sense or logical rules, which is slightly
    problematic and does not affect the senses; 3.There are basically no elements
    in the image that violate common sense or logical rules, only some flaws; 4.There
    are no elements in the image that violate common sense or logical rules, and they
    are close to reality.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The templates of EvalAlign evaluation on text-image alignment are as follows.
    We select 6 common aspects of text-image alignment, object, number, color, style,
    spatial relationship and action. For images that do not involve the specified
    attribute, the corresponding question template is not filled in and subsequently
    input into EvalAlign.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: EvalAlign对文本-图像对齐的评估模板如下。我们选择了6个文本-图像对齐的常见方面，包括对象、数量、颜色、风格、空间关系和动作。对于不涉及指定属性的图像，相应的问题模板不填写，然后输入到EvalAlign中。
- en: '[Q1]:Does the given image contain
    all the objects () presented in the corresponding prompts? [OPTIONS]:
    1.None objects are included; 2.Some objects are missing; 3.All objects are included.[Q2]:Does the given image correctly
    reflect the numbers () of each object presented in the corresponding
    prompts? [OPTIONS]: 1.All counting numbers are wrong; 2.Some of them are wrong;
    3.All counting numbers are right.[Q3]:Does the given image correctly
    reflect the colors of each object () presented in the corresponding
    prompts? [OPTIONS]: 1.All colors are wrong; 2.Some of them are wrong; 3.All corresponding
    colors numbers are right.[Q4]:Does the given image correctly reflect the style ()
    described in the corresponding prompts? [OPTIONS]: 1.All styles are wrong; 2.Some
    of them are wrong; 3.All styles are right.[Q5]:Does the given image correctly
    reflect the spatial relationship () of each object described in the
    corresponding prompts? [OPTIONS]: 1.All spatial relationships are wrong; 2.Some
    of them are wrong; 3.All spatial relationships are right.[Q6]:Does the given image correctly
    reflect the action of each object () described in the corresponding
    prompts? [OPTIONS]: 1.All actions are wrong; 2.Some of them are wrong; 3.All actions
    are right.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Additional Quantitative Analysis
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 附加定量分析
- en: F.1 Generalization Experiments
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 泛化实验
- en: 'To verify the generalization capability of our evaluation model, We compared
    MLLM’s SFT using different training datasets: one with images generated by all
    8 text-to-image models and another with images generated by only 4 of these models,
    while the final evaluation was conducted on images generated by the other 4 models.
    As shown in Table [10](#A6.T10 "Table 10 ‣ F.1 Generalization Experiments ‣ Appendix
    F Additional Quantitative Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal
    LLMs with Human-Aligned Data for Evaluating Text-to-Image Models") and Table [11](#A6.T11
    "Table 11 ‣ F.1 Generalization Experiments ‣ Appendix F Additional Quantitative
    Analysis ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned
    Data for Evaluating Text-to-Image Models"), We observed that MLLMs trained on
    images from a subset of text-to-image models can effectively generalize to images
    generated by unseen text-to-image models.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证我们评估模型的泛化能力，我们比较了使用不同训练数据集的MLLM的SFT：一个是由所有 8 个文本到图像模型生成的图像，另一个是由其中 4 个模型生成的图像，而最终评估是在由其他
    4 个模型生成的图像上进行的。如表 [10](#A6.T10 "表 10 ‣ F.1 泛化实验 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 用于评估文本到图像模型的人工对齐数据的监督微调多模态
    LLMs")和表 [11](#A6.T11 "表 11 ‣ F.1 泛化实验 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 用于评估文本到图像模型的人工对齐数据的监督微调多模态
    LLMs")所示，我们观察到基于部分文本到图像模型生成的图像训练的MLLM能够有效泛化到由未见过的文本到图像模型生成的图像。'
- en: 'Table 10: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating image faithfulness. We observe that
    EvalAlign exhibits strong generalization capability.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：对用于生成训练数据以评估图像真实度的不同文本到图像模型数量的消融研究。我们观察到**EvalAlign**展现了强大的泛化能力。
- en: '| Method | T2I models | body | hand | face | object | common | MAE |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | T2I 模型 | 身体 | 手 | 脸 | 物体 | 常见 | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | - | 1.4988 | 0.8638 | 1.1648 | 2.2096 | 0.8710 | 0 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | - | 1.4988 | 0.8638 | 1.1648 | 2.2096 | 0.8710 | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 8 | 1.6058 | 0.7901 | 1.1974 | 2.2783 | 0.8871 | 0.0596 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 8 | 1.6058 | 0.7901 | 1.1974 | 2.2783 | 0.8871 | 0.0596 |'
- en: '| 4 | 1.6522 | 0.9588 | 1.2355 | 2.3032 | 0.9516 | 0.0987 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.6522 | 0.9588 | 1.2355 | 2.3032 | 0.9516 | 0.0987 |'
- en: 'Table 11: Ablation study on the number of different text-to-image models used
    to generate the training data for evaluating text-to-image alignment. We observe
    that EVALALIGN exhibits strong generalization capability.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：对用于生成训练数据以评估文本到图像对齐的不同文本到图像模型数量的消融研究。我们观察到**EVALALIGN**展现了强大的泛化能力。
- en: '| Method | T2I models | Object | Count | Color | Style | Spatial | Action |
    MAE |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | T2I 模型 | 物体 | 数量 | 颜色 | 风格 | 空间 | 动作 | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | - | 1.7373 | 1.3131 | 2.0000 | 1.9333 | 1.5952 | 1.8837 | 0 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | - | 1.7373 | 1.3131 | 2.0000 | 1.9333 | 1.5952 | 1.8837 | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | 8 | 1.7203 | 1.3232 | 1.9565 | 1.9333 | 1.6547 | 1.8605 | 0.0256
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | 8 | 1.7203 | 1.3232 | 1.9565 | 1.9333 | 1.6547 | 1.8605 | 0.0256
    |'
- en: '| 4 | 1.7832 | 1.3526 | 1.9637 | 1.9876 | 1.6891 | 1.8954 | 0.0469 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.7832 | 1.3526 | 1.9637 | 1.9876 | 1.6891 | 1.8954 | 0.0469 |'
- en: F.2 Instruction Enhancement Experiments
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 指令增强实验
- en: 'Table 12: Ablation study on the enhancement of instructions. Results are reported
    on image faithfulness under different instructions. We observe that enhanced instructions
    can significantly improves the evaluation metrics. MAE: mean absolute error.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：对指令增强的消融研究。结果报告了在不同指令下图像真实度的评估。我们观察到，增强的指令可以显著改善评估指标。MAE：均值绝对误差。
- en: '| Method | Instruction | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 指令 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766 | 1.0066
    | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign | ✗ | 1.9565 | 1.9286 | 1.8565 | 1.1818 | 1.3419 | 1.4801 | 1.4078
    | 1.1051 | 0.1201 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign | ✗ | 1.9565 | 1.9286 | 1.8565 | 1.1818 | 1.3419 | 1.4801 | 1.4078
    | 1.1051 | 0.1201 |'
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.2960 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
- en: 'Providing more contextual information for instructions enhances the performance
    of MLLMs. To further improve MLLM evaluation performance, we enhanced the prompts
    for both SFT and inference stages. As shown in Table  [12](#A6.T12 "Table 12 ‣
    F.2 Instruction Enhancement Experiments ‣ Appendix F Additional Quantitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), our experiments demonstrate that the enhanced
    prompts significantly increase evaluation accuracy. Specifically, the evaluation
    using enhanced instructions reduced the MAE metric by half, from 0.120 to 0.006,
    compared to the original instructions. Additionally, this approach consistently
    improved evaluation performance across different text-to-image models.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '提供更多上下文信息来增强指令可以提高 MLLMs 的性能。为了进一步提高 MLLM 评估性能，我们增强了 SFT 和推理阶段的提示。正如表 [12](#A6.T12
    "表 12 ‣ F.2 指令增强实验 ‣ 附录 F 额外定量分析 ‣ EvalAlign: 使用人类对齐数据进行监督微调的多模态 LLM 评估文本到图像模型")
    所示，我们的实验表明，增强的提示显著提高了评估准确性。具体而言，使用增强指令的评估将 MAE 指标从 0.120 减少到 0.006，相比于原始指令。此外，这种方法在不同的文本到图像模型中始终提高了评估性能。'
- en: F.3 Mulit-scaling Resolutions Experiments
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 多尺度分辨率实验
- en: 'Table 13: Ablation study on multi-scale input. Results are reported on image
    faithfulness under different input strategy. We observe that input with multi-scale
    resolution images can improves the evaluation metrics. MAE: mean absolute error.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：多尺度输入的消融研究。结果报告了不同输入策略下图像忠实度的表现。我们观察到，使用多尺度分辨率图像可以改善评估指标。MAE：平均绝对误差。
- en: '| Method | Multi Scale | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD
    v1.5 | SD v2.1 | LCM | MAE |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 多尺度 | SDXL | Pixart | Wuerstchen | SDXL-Turbo | IF | SD v1.5 | SD v2.1
    | LCM | MAE |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Human | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766
    | 1.0066 | 0 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | – | 2.1044 | 1.8606 | 1.7839 | 1.3854 | 1.3822 | 1.3818 | 1.1766 | 1.0066
    | 0 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| EvalAlign |  | 1.8105 | 1.9238 | 1.9325 | 1.2078 | 1.2247 | 1.4540 | 1.3012
    | 1.0554 | 0.1358 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| EvalAlign |  | 1.8105 | 1.9238 | 1.9325 | 1.2078 | 1.2247 | 1.4540 | 1.3012
    | 1.0554 | 0.1358 |'
- en: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 2.0443 | 1.9199 | 1.8012 | 1.3353 | 1.296 | 1.4702 | 1.3221 | 1.0305
    | 0.0064 |'
- en: 'In the design of LLaVA-Next, using multi-scale resolution images as input helps
    address the issue of detail information loss, which significantly impacts the
    evaluation of image faithfulness, such as assessing deformations in hands and
    faces. We conducted a multi-scale image training comparison experiment to validate
    this approach. The baseline was the 13B LLaVA model with 336$\times$336, 672$\times$1008)
    as input. As shown in Table [13](#A6.T13 "Table 13 ‣ F.3 Mulit-scaling Resolutions
    Experiments ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models"), training with multi-scale inputs significantly enhanced the model’s
    understanding of image and achieved better evaluation performance.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLaVA-Next 的设计中，使用多尺度分辨率图像作为输入有助于解决细节信息丢失的问题，这对图像忠实度的评估有显著影响，例如评估手部和面部的变形。我们进行了多尺度图像训练对比实验来验证这种方法。基准模型是
    13B LLaVA 模型，输入分辨率为 336$\times$336 和 672$\times$1008。正如表 [13](#A6.T13 "表 13 ‣
    F.3 多尺度分辨率实验 ‣ 附录 F 额外定量分析 ‣ EvalAlign: 使用人类对齐数据进行监督微调的多模态 LLM 评估文本到图像模型") 所示，使用多尺度输入进行训练显著增强了模型对图像的理解，并且取得了更好的评估性能。'
- en: F.4 Full Comparison with Existing Evaluation Methods
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 与现有评估方法的全面比较
- en: 'Table 14: Results on faithfulness.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：忠实度结果。
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 人类 | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore | IS
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ | 31.6226${}^{1~{}~{}}$
    | 0.9696 ${}^{1~{}~{}}$ | 5.2583^(21) |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 2.2848${}^{1~{}~{}}$ | 31.6226${}^{1~{}~{}}$
    | 0.9696 ${}^{1~{}~{}}$ | 5.2583^(21) |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ | 29.2322${}^{6~{}~{}}$
    | 5.6452^(14) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 2.0070${}^{2~{}~{}}$ | 29.2322${}^{6~{}~{}}$
    | 5.6452^(14) |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ | 29.8197${}^{3~{}~{}}$
    | 0.7245 ${}^{2~{}~{}}$ | 5.3985^(16) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 1.9229${}^{3~{}~{}}$ | 29.8197${}^{3~{}~{}}$
    | 0.7245 ${}^{2~{}~{}}$ | 5.3985^(16) |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ | 29.0620${}^{7~{}~{}}$
    | 0.7043 ${}^{3~{}~{}}$ | 5.3774^(18) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 1.8136${}^{4~{}~{}}$ | 29.0620${}^{7~{}~{}}$
    | 0.7043 ${}^{3~{}~{}}$ | 5.3774^(18) |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ | 30.6622${}^{2~{}~{}}$
    | 5.9751${}^{4~{}~{}}$ |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 1.7837${}^{5~{}~{}}$ | 30.6622${}^{2~{}~{}}$
    | 5.9751${}^{4~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ | 29.3588${}^{5~{}~{}}$
    | 21.6532${}^{4~{}~{}}$ |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 1.6910${}^{6~{}~{}}$ | 29.3588${}^{5~{}~{}}$
    | 21.6532${}^{4~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) | 26.3475^(13)
    | 0.8196^(15) | 0.1478 ^(16) | 20.8637^(10) | 5.7368^(10) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 1.6667${}^{7~{}~{}}$ | 1.1750^(10) | 26.3475^(13)
    | 0.8196^(15) | 0.1478 ^(16) | 20.8637^(10) | 5.7368^(10) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ | 25.7396^(17) | 0.7555^(24)
    | -0.0507^(22) | 20.4594^(21) | 5.5428^(15) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 1.6491${}^{8~{}~{}}$ | 25.7396^(17) | 0.7555^(24)
    | -0.0507^(22) | 20.4594^(21) | 5.5428^(15) |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ | 27.3299^(10) |
    0.8364${}^{8~{}~{}}$ | 21.4824${}^{5~{}~{}}$ | 5.6575^(12) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 1.6387${}^{9~{}~{}}$ | 27.3299^(10) |
    0.8364${}^{8~{}~{}}$ | 21.4824${}^{5~{}~{}}$ | 5.6575^(12) |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) | 25.5764^(18)
    | 0.8165^(18) | -0.1022^(23) | 20.6211^(18) | 5.8643${}^{6~{}~{}}$ |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 1.6308^(10) | 1.1466^(11) | 25.5764^(18)
    | 0.8165^(18) | -0.1022^(23) | 20.6211^(18) | 5.8643${}^{6~{}~{}}$ |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) | 26.2798^(14)
    | 0.8101^(20) | 0.2042 ^(12) | 20.7880^(12) | 6.1760${}^{1~{}~{}}$ |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 1.6275^(11) | 1.1298^(15) | 26.2798^(14)
    | 0.8101^(20) | 0.2042 ^(12) | 20.7880^(12) | 6.1760${}^{1~{}~{}}$ |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) | 26.1180^(15)
    | 0.7809^(23) | -0.1264^(24) | 20.3873^(24) | 5.3861^(17) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 1.6078^(12) | 1.1188^(17) | 26.1180^(15)
    | 0.7809^(23) | -0.1264^(24) | 20.3873^(24) | 5.3861^(17) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) | 26.5823^(12) | 0.8377${}^{7~{}~{}}$
    | 21.0502${}^{9~{}~{}}$ | 5.3073^(19) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 1.5524^(13) | 1.1094^(18) | 26.5823^(12) | 0.8377${}^{7~{}~{}}$
    | 21.0502${}^{9~{}~{}}$ | 5.3073^(19) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) | 25.3481^(21) | 0.8170^(17)
    | 0.0872 ^(18) | 20.7529^(13) | 5.9060${}^{5~{}~{}}$ |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 1.5277^(14) | 1.1300^(14) | 25.3481^(21) | 0.8170^(17)
    | 0.0872 ^(18) | 20.7529^(13) | 5.9060${}^{5~{}~{}}$ |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) | 24.6984^(23)
    | 0.7958^(22) | -0.0415^(21) | 20.4088^(22) | 5.1995^(22) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 1.5000^(15) | 0.9956^(20) | 24.6984^(23)
    | 0.7958^(22) | -0.0415^(21) | 20.4088^(22) | 5.1995^(22) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) | 25.1572^(22)
    | 0.8101^(21) | 0.0218 ^(20) | 20.6155^(19) | 5.7657${}^{8~{}~{}}$ |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 1.4733^(16) | 1.1382^(12) | 25.1572^(22)
    | 0.8101^(21) | 0.0218 ^(20) | 20.6155^(19) | 5.7657${}^{8~{}~{}}$ |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    | 0.8543${}^{3~{}~{}}$ | 21.2664${}^{7~{}~{}}$ | 5.6614^(11) |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 1.4652^(17) | 1.2052${}^{9~{}~{}}$
    | 0.8543${}^{3~{}~{}}$ | 21.2664${}^{7~{}~{}}$ | 5.6614^(11) |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) | 25.4972^(19) | 0.8214^(13)
    | 0.1686 ^(14) | 20.7143^(16) | 6.0535${}^{2~{}~{}}$ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 1.4417^(18) | 1.1362^(13) | 25.4972^(19) | 0.8214^(13)
    | 0.1686 ^(14) | 20.7143^(16) | 6.0535${}^{2~{}~{}}$ |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) | 27.4512${}^{9~{}~{}}$
    | 0.6087 ${}^{5~{}~{}}$ | 20.7474^(14) | 5.3012^(20) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 1.3808^(19) | 0.9221^(22) | 27.4512${}^{9~{}~{}}$
    | 0.6087 ${}^{5~{}~{}}$ | 20.7474^(14) | 5.3012^(20) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) | 25.3697^(20) | 0.8190^(16)
    | 0.1050 ^(17) | 20.6535^(17) | 5.8571${}^{7~{}~{}}$ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 1.3592^(20) | 0.9511^(21) | 25.3697^(20) | 0.8190^(16)
    | 0.1050 ^(17) | 20.6535^(17) | 5.8571${}^{7~{}~{}}$ |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |
    26.5901^(11) | 0.8341${}^{9~{}~{}}$ | 0.3562 ^(10) | 20.8358^(11) | 5.0181^(23)
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 1.3562^(21) | 1.0797^(19) |
    26.5901^(11) | 0.8341${}^{9~{}~{}}$ | 0.3562 ^(10) | 20.8358^(11) | 5.0181^(23)
    |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) | 27.4836${}^{8~{}~{}}$
    | 0.4463 ${}^{8~{}~{}}$ | 20.7170^(15) | 5.6502^(13) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 1.2635^(22) | 0.8814^(23) | 27.4836${}^{8~{}~{}}$
    | 0.4463 ${}^{8~{}~{}}$ | 20.7170^(15) | 5.6502^(13) |'
- en: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) | 23.8133^(24)
    | 0.8151^(19) | 0.0695 ^(19) | 20.4780^(20) | 4.3824^(24) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 1.2372^(23) | 1.1298^(16) | 23.8133^(24)
    | 0.8151^(19) | 0.0695 ^(19) | 20.4780^(20) | 4.3824^(24) |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) | 25.9522^(16) |
    0.8329^(11) | 0.1637 ^(15) | 20.4035^(23) | 5.7451${}^{9~{}~{}}$ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 1.0135^(24) | 0.7928^(24) | 25.9522^(16) |
    0.8329^(11) | 0.1637 ^(15) | 20.4035^(23) | 5.7451${}^{9~{}~{}}$ |'
- en: 'Table 15: Results on text-to-image alignment.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '表 15: 文本到图像对齐的结果。'
- en: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Model | Human | EvalAlign | HPS v2 | CLIP-score | ImageReward | PickScore
    | IS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ | 32.5477^(10) | 0.8579${}^{2~{}~{}}$
    | 21.1998^(10) | 7.1864${}^{5~{}~{}}$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-XL v1.0 [[1](#bib.bib1)] | 5.4500${}^{1~{}~{}}$ | 32.5477^(10) | 0.8579${}^{2~{}~{}}$
    | 21.1998^(10) | 7.1864${}^{5~{}~{}}$ |'
- en: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ | 32.7140${}^{9~{}~{}}$
    | 0.3820 ${}^{6~{}~{}}$ | 21.1284^(12) | 6.6571^(20) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-L v1.0 [[1](#bib.bib1)] | 5.2300${}^{2~{}~{}}$ | 32.7140${}^{9~{}~{}}$
    | 0.3820 ${}^{6~{}~{}}$ | 21.1284^(12) | 6.6571^(20) |'
- en: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ | 35.6465${}^{3~{}~{}}$
    | 0.4738 ${}^{2~{}~{}}$ | 7.1101${}^{8~{}~{}}$ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| SDXL Refiner v1.0 [[47](#bib.bib47)] | 5.2100${}^{3~{}~{}}$ | 35.6465${}^{3~{}~{}}$
    | 0.4738 ${}^{2~{}~{}}$ | 7.1101${}^{8~{}~{}}$ |'
- en: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ | 33.8011${}^{6~{}~{}}$
    | 0.3833 ${}^{5~{}~{}}$ | 7.0067^(14) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| LCM SDXL [[28](#bib.bib28)] | 5.1800${}^{4~{}~{}}$ | 33.8011${}^{6~{}~{}}$
    | 0.3833 ${}^{5~{}~{}}$ | 7.0067^(14) |'
- en: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ | 37.0493${}^{1~{}~{}}$
    | 0.6542 ${}^{1~{}~{}}$ | 6.9293^(16) |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| PixArt XL2 1024 MS [[6](#bib.bib6)] | 5.1100${}^{5~{}~{}}$ | 37.0493${}^{1~{}~{}}$
    | 0.6542 ${}^{1~{}~{}}$ | 6.9293^(16) |'
- en: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ | 31.0951^(14) | 0.8434${}^{8~{}~{}}$
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| IF-I-M v1.0 [[1](#bib.bib1)] | 5.0800${}^{6~{}~{}}$ | 31.0951^(14) | 0.8434${}^{8~{}~{}}$
    |'
- en: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ | 32.7752${}^{8~{}~{}}$
    | 21.7627${}^{6~{}~{}}$ | 6.8389^(19) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| LCM LORA SDXL [[28](#bib.bib28)] | 5.0600${}^{7~{}~{}}$ | 32.7752${}^{8~{}~{}}$
    | 21.7627${}^{6~{}~{}}$ | 6.8389^(19) |'
- en: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ | 35.1593${}^{4~{}~{}}$
    | 0.4322 ${}^{4~{}~{}}$ | 7.2762${}^{1~{}~{}}$ |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SDXL v1.0 [[33](#bib.bib33)] | 5.0300${}^{8~{}~{}}$ | 35.1593${}^{4~{}~{}}$
    | 0.4322 ${}^{4~{}~{}}$ | 7.2762${}^{1~{}~{}}$ |'
- en: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ | 36.4632${}^{2~{}~{}}$
    | 0.2513 ${}^{7~{}~{}}$ | 7.1280${}^{7~{}~{}}$ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Wuerstchen [[32](#bib.bib32)] | 4.8700${}^{9~{}~{}}$ | 36.4632${}^{2~{}~{}}$
    | 0.2513 ${}^{7~{}~{}}$ | 7.1280${}^{7~{}~{}}$ |'
- en: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) | 31.1495^(12)
    | 0.8173^(16) | -0.0867^(14) | 21.1163^(13) | 6.2729^(22) |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney [[34](#bib.bib34)] | 4.8300^(10) | 4.9200^(15) | 31.1495^(12)
    | 0.8173^(16) | -0.0867^(14) | 21.1163^(13) | 6.2729^(22) |'
- en: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) | 31.1017^(13) | 0.8278^(14)
    | -0.0453^(12) | 21.2093${}^{9~{}~{}}$ |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.1 [[40](#bib.bib40)] | 4.8000^(11) | 5.0700^(11) | 31.1017^(13) | 0.8278^(14)
    | -0.0453^(12) | 21.2093${}^{9~{}~{}}$ |'
- en: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) | 28.7957^(24)
    | 0.8264^(15) | -0.1337^(15) | 20.9625^(17) | 4.9593^(24) |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| MultiFusion [[29](#bib.bib29)] | 4.6800^(12) | 4.8000^(18) | 28.7957^(24)
    | 0.8264^(15) | -0.1337^(15) | 20.9625^(17) | 4.9593^(24) |'
- en: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |
    34.8196${}^{5~{}~{}}$ | 0.2295 ${}^{8~{}~{}}$ | 6.8581^(18) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Diffusion v1.0 [[10](#bib.bib10)] | 4.6600^(13) | 5.1500^(10) |
    34.8196${}^{5~{}~{}}$ | 0.2295 ${}^{8~{}~{}}$ | 6.8581^(18) |'
- en: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) | 30.6153^(17) | 0.8298^(13)
    | -0.1424^(16) | 21.1905^(11) | 7.0124^(13) |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| SD v2.0 [[40](#bib.bib40)] | 4.6400^(14) | 5.0100^(12) | 30.6153^(17) | 0.8298^(13)
    | -0.1424^(16) | 21.1905^(11) | 7.0124^(13) |'
- en: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |
    31.9503^(11) | 0.8319^(12) | -0.0222^(11) | 21.1141^(14) | 6.1987^(23) |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| Vintedois Diffusion v0.1 [[51](#bib.bib51)] | 4.6200^(15) | 4.9500^(14) |
    31.9503^(11) | 0.8319^(12) | -0.0222^(11) | 21.1141^(14) | 6.1987^(23) |'
- en: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) | 30.6615^(16)
    | 0.7751^(23) | -0.5028^(22) | 20.7491^(21) | 7.2743${}^{2~{}~{}}$ |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD STRONG [[31](#bib.bib31)] | 4.6000^(16) | 4.8300^(17) | 30.6615^(16)
    | 0.7751^(23) | -0.5028^(22) | 20.7491^(21) | 7.2743${}^{2~{}~{}}$ |'
- en: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |
    33.7712${}^{7~{}~{}}$ | 6.9363^(15) |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| Dreamlike Photoreal v2.0 [[11](#bib.bib11)] | 4.5600^(17) | 4.9800^(13) |
    33.7712${}^{7~{}~{}}$ | 6.9363^(15) |'
- en: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) | 30.5644^(18)
    | 0.8140^(18) | -0.2728^(18) | 20.9899^(16) | 6.4238^(21) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD WEAK [[31](#bib.bib31)] | 4.5300^(18) | 4.7100^(20) | 30.5644^(18)
    | 0.8140^(18) | -0.2728^(18) | 20.9899^(16) | 6.4238^(21) |'
- en: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) | 29.9149^(20) | 0.8048^(20)
    | -0.3438^(19) | 20.8462^(19) | 7.0150^(12) |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[40](#bib.bib40)] | 4.5200^(19) | 4.7600^(19) | 29.9149^(20) | 0.8048^(20)
    | -0.3438^(19) | 20.8462^(19) | 7.0150^(12) |'
- en: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) | 30.1673^(19) | 0.8142^(17)
    | -0.2213^(17) | 20.8640^(18) | 7.1642${}^{6~{}~{}}$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.5 [[40](#bib.bib40)] | 4.4500^(20) | 4.9000^(16) | 30.1673^(19) | 0.8142^(17)
    | -0.2213^(17) | 20.8640^(18) | 7.1642${}^{6~{}~{}}$ |'
- en: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) | 30.7820^(15)
    | 0.7974^(21) | -0.3591^(20) | 21.0257^(15) | 7.0709^(10) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MEDIUM [[31](#bib.bib31)] | 4.4000^(21) | 4.5600^(24) | 30.7820^(15)
    | 0.7974^(21) | -0.3591^(20) | 21.0257^(15) | 7.0709^(10) |'
- en: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) | 29.2865^(22)
    | 0.8066^(19) | -0.4172^(21) | 20.6327^(23) | 7.0610^(11) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| Redshift diffusion [[39](#bib.bib39)] | 4.3500^(22) | 4.6700^(21) | 29.2865^(22)
    | 0.8066^(19) | -0.4172^(21) | 20.6327^(23) | 7.0610^(11) |'
- en: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) | 29.8126^(21)
    | 0.7601^(24) | -0.6095^(24) | 20.7046^(22) | 7.2273${}^{4~{}~{}}$ |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Safe SD MAX [[31](#bib.bib31)] | 4.3100^(23) | 4.5900^(23) | 29.8126^(21)
    | 0.7601^(24) | -0.6095^(24) | 20.7046^(22) | 7.2273${}^{4~{}~{}}$ |'
- en: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) | 29.2389^(23)
    | 0.7851^(22) | -0.6051^(23) | 20.5973^(24) | 6.8613^(17) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Openjourney v2 [[35](#bib.bib35)] | 4.1500^(24) | 4.6500^(22) | 29.2389^(23)
    | 0.7851^(22) | -0.6051^(23) | 20.5973^(24) | 6.8613^(17) |'
- en: 'Table [14](#A6.T14 "Table 14 ‣ F.4 Full Comparison with Existing Evaluation
    Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign: Supervised
    Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image
    Models") and Table [15](#A6.T15 "Table 15 ‣ F.4 Full Comparison with Existing
    Evaluation Methods ‣ Appendix F Additional Quantitative Analysis ‣ EvalAlign:
    Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating
    Text-to-Image Models") present the comparison results between our proposed method
    and existing alternatives. Owing to the powerful aligned understanding of images
    and text by MLLMs, our model achieves the best alignment with human performance
    across all tested benchmarks for 24 text-to-image models, along with a significant
    improvement in image faithfulness. This indicates that our model excels in both
    comprehending and evaluating the intricate details of generated images, closely
    mirroring human judgment and setting a new standard for image-text alignment and
    faithfulness.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '表[14](#A6.T14 "表 14 ‣ F.4 与现有评估方法的全面比较 ‣ 附录 F 附加定量分析 ‣ EvalAlign: 使用与人类对齐的数据对多模态
    LLM 进行监督微调以评估文本到图像模型")和表[15](#A6.T15 "表 15 ‣ F.4 与现有评估方法的全面比较 ‣ 附录 F 附加定量分析 ‣
    EvalAlign: 使用与人类对齐的数据对多模态 LLM 进行监督微调以评估文本到图像模型")展示了我们提出的方法与现有替代方法之间的比较结果。由于MLLMs对图像和文本的强大对齐理解，我们的模型在24个文本到图像模型的所有测试基准中，与人类表现的对齐最佳，并且在图像真实性方面有显著提升。这表明我们的模型在理解和评估生成图像的复杂细节方面表现优异，紧密地反映了人类判断，为图像与文本对齐和真实性设立了新标准。'
- en: Appendix G Qualitative Analysis
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 定性分析
- en: 'As shown in Figure [5](#A7.F5 "Figure 5 ‣ Appendix G Qualitative Analysis ‣
    EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models"), we present a comparison of different evaluation
    metrics on images generated by four models, including human annotated scores,
    EvalAlign, ImageReward [[56](#bib.bib56)], HPSv2 [[54](#bib.bib54)], and PickScore [[20](#bib.bib20)].
    The digits in the figure represent the ranking for each evaluation metric, with
    darker colors indicating higher rankings. From the figure, it is evident that
    our proposed EvalAlign metric closely matches the human rankings across two evaluation
    dimensions, demonstrating excellent consistency.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[5](#A7.F5 "图 5 ‣ 附录 G 定性分析 ‣ EvalAlign: 使用与人类对齐的数据对多模态 LLM 进行监督微调以评估文本到图像模型")所示，我们展示了四种模型生成的图像在不同评估指标上的比较，包括人工标注分数、EvalAlign、ImageReward [[56](#bib.bib56)]、HPSv2 [[54](#bib.bib54)]和PickScore [[20](#bib.bib20)]。图中的数字代表每个评估指标的排名，颜色越深表示排名越高。从图中可以明显看出，我们提出的EvalAlign指标在两个评估维度上与人工排名高度一致，表现出色的一致性。'
- en: 'Furthermore, Figure [6](#A7.F6 "Figure 6 ‣ Appendix G Qualitative Analysis
    ‣ EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for
    Evaluating Text-to-Image Models") showcases the EvalAlign evaluation metric across
    different fine-grained results. The numbers in the figure represent EvalAlign
    scores for the corresponding evaluation aspect, with darker colors indicating
    higher scores and better generation performance. Note that if the text prompt
    does not specify a particular style, the style consistency score defaults to 0.
    From these results, it is evident that the same text-to-image model exhibits significant
    performance variation across different evaluation aspects.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图 [6](#A7.F6 "图 6 ‣ 附录 G 定性分析 ‣ EvalAlign：用人类对齐数据对文本到图像模型进行监督微调的多模态大模型")
    展示了 EvalAlign 评价指标在不同细粒度结果中的表现。图中的数字表示对应评价方面的 EvalAlign 分数，颜色越深表示分数越高，生成性能越好。请注意，如果文本提示没有指定特定风格，则风格一致性分数默认为
    0。从这些结果中，可以明显看出相同的文本到图像模型在不同评价方面表现出显著的性能差异。
- en: '![Refer to caption](img/7680590ff159f32f823ec69fde57cd7c.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7680590ff159f32f823ec69fde57cd7c.png)'
- en: 'Figure 5: Qualitative results of EvalAlign benchmark. As can be concluded,
    EvalAlign is consistently aligned with fine-grained human preference in terms
    of image faithfulness and text-image alignment, while other methods fail to do
    so.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：EvalAlign 基准的定性结果。可以得出结论，EvalAlign 在图像忠实度和文本-图像对齐方面与细粒度的人类偏好始终保持一致，而其他方法则未能做到这一点。
- en: '![Refer to caption](img/735d8f6edbed6474fdd91e3d2241f9fb.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/735d8f6edbed6474fdd91e3d2241f9fb.png)'
- en: 'Figure 6: Qualitative results of EvalAlign benchmark. As can be inferred, EvalAlign
    distinctively provides multiple fine-grained scores covering every aspect of image
    faithfulness and text-image alignment. Additionally, we noticed that, in general,
    a model cannot perform well on every fine-grained aspect, which is consistent
    with [[21](#bib.bib21)].'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：EvalAlign 基准的定性结果。如可以推测的，EvalAlign 显著地提供了多个细粒度的评分，涵盖了图像忠实度和文本-图像对齐的各个方面。此外，我们注意到，一般来说，一个模型不能在每个细粒度方面表现出色，这与[[21](#bib.bib21)]一致。
