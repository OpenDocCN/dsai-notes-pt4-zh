- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:37:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:37:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LLMem: 估算微调预训练LLMs的GPU内存使用情况'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10933](https://ar5iv.labs.arxiv.org/html/2404.10933)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10933](https://ar5iv.labs.arxiv.org/html/2404.10933)
- en: Taeho Kim¹¹¹1This work is based on Taeho Kim’s internship at AWS.    Yanming
    Wang²    Vatshank Chaturvedi²    Lokesh Gupta²    Seyeon Kim¹    Yongin Kwon³
       Sangtae Ha¹ ¹University of Colorado Boulder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**Taeho Kim**¹¹¹1本工作基于**Taeho Kim**在AWS的实习经历。    **Yanming Wang**²    **Vatshank
    Chaturvedi**²    **Lokesh Gupta**²    **Seyeon Kim**¹    **Yongin Kwon**³    **Sangtae
    Ha**¹ ¹科罗拉多大学博尔德分校'
- en: ²Amazon Web Services²²2This work is unconnected to current role at AWS.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²亚马逊网络服务²²2本研究与在AWS的当前角色无关。
- en: ³Electronics and Telecommunications Research Institute {taeho.kim,seyeon.kim,sangtae.ha}@colorado.edu,
    {yanmwang,vatshc,lokeshgu}@amazon.com, yongin.kwon@etri.re.kr
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³电子与通信研究院 {taeho.kim,seyeon.kim,sangtae.ha}@colorado.edu, {yanmwang,vatshc,lokeshgu}@amazon.com,
    yongin.kwon@etri.re.kr
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fine-tuning pre-trained large language models (LLMs) with limited hardware presents
    challenges due to GPU memory constraints. Various distributed fine-tuning methods
    have been proposed to alleviate memory constraints on GPU. However, determining
    the most effective method for achieving rapid fine-tuning while preventing GPU
    out-of-memory issues in a given environment remains unclear. To address this challenge,
    we introduce LLMem, a solution that estimates the GPU memory consumption when
    applying distributed fine-tuning methods across multiple GPUs and identifies the
    optimal method. We conduct GPU memory usage estimation prior to fine-tuning, leveraging
    the fundamental structure of transformer-based decoder models and the memory usage
    distribution of each method. Experimental results show that LLMem accurately estimates
    peak GPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally,
    it shows an average error rate of 3.0% when applying distributed fine-tuning methods
    to LLMs with more than a billion parameters on multi-GPU setups.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限硬件条件下微调预训练的大型语言模型（LLMs）面临GPU内存限制的问题。为减轻GPU上的内存限制，提出了各种分布式微调方法。然而，在特定环境中，确定实现快速微调而防止GPU内存不足问题的最有效方法仍不清楚。为解决这一挑战，我们介绍了LLMem，一种在多个GPU上应用分布式微调方法时估算GPU内存消耗并确定最佳方法的解决方案。我们在微调之前进行GPU内存使用估算，利用基于Transformer的解码器模型的基本结构和每种方法的内存使用分布。实验结果表明，LLMem准确估算了单个GPU上的峰值GPU内存使用情况，误差率高达1.6%。此外，在多GPU设置中对超过十亿参数的LLMs应用分布式微调方法时，其平均误差率为3.0%。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Since the introduction of the Transformer model Vaswani et al. ([2017](#bib.bib18)),
    researchers have proposed numerous language models based on it. As the model’s
    performance has improved, its size has grown exponentially, necessitating a substantial
    dataset for training. However, training emerging large language models (LLMs)
    is infeasible without a dedicated infrastructure with high-performance hardware
    due to memory constraints. Instead, it is preferred to utilize a small dataset
    to fine-tune a pre-trained model for a specific application.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自从**Vaswani**等人（[2017](#bib.bib18)）引入Transformer模型以来，研究人员提出了基于该模型的众多语言模型。随着模型性能的提升，其规模呈指数级增长，需要大量的数据集进行训练。然而，由于内存限制，没有专门的高性能硬件基础设施，训练新兴的大型语言模型（LLMs）是不可行的。因此，利用小数据集对预训练模型进行特定应用的微调更为可取。
- en: To efficiently handle small datasets and reduce training time, the conventional
    method of data parallelism places the entire model on each GPU, splits the dataset,
    and trains simultaneously. Nevertheless, the model size remains huge, potentially
    causing GPU out-of-memory (OOM) issues. Therefore, it is necessary to reduce the
    amount of memory a GPU uses by splitting the model and distributing it to each
    GPU.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效处理小数据集并缩短训练时间，传统的数据并行方法将整个模型放置在每个GPU上，拆分数据集并同时训练。然而，模型规模仍然巨大，可能导致GPU内存不足（OOM）问题。因此，有必要通过拆分模型并将其分配到每个GPU上来减少GPU的内存使用量。
- en: ZeRO Rajbhandari et al. ([2020](#bib.bib13)) Stage 3 is an advanced data parallelism
    method that partitions the model parameters, gradients, and optimizer states to
    each GPU for memory advantage while maintaining the distribution of the dataset
    across GPUs. Although ZeRO Stage 3 saves memory by using only partitioned model
    data on each GPU during non-computation phases, there are limitations in preventing
    GPU OOM issues because partitioned parameters/gradients must be all-gathered during
    computation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ZeRO Rajbhandari 等人（[2020](#bib.bib13)）Stage 3 是一种先进的数据并行方法，它将模型参数、梯度和优化器状态分区到每个
    GPU，以便节省内存，同时保持数据集在 GPU 之间的分布。尽管 ZeRO Stage 3 在非计算阶段通过仅使用分区模型数据来节省内存，但由于在计算过程中必须对所有分区的参数/梯度进行汇总，因此在防止
    GPU OOM 问题方面存在一定限制。
- en: Tensor parallelism divides each parameter tensor in the model into rows or columns
    and distributes them to each GPU, using only partitioned parameters on each GPU
    during computation. For example, Megatron-LM Shoeybi et al. ([2019](#bib.bib15)),
    a representative tensor parallelism method, splits a tensor along its rows or
    columns considering the position and connection of operators. By doing so, it
    can reduce GPU memory usage more than data parallelism when the model size is
    large.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor parallelism 将模型中的每个参数张量分成行或列，并将它们分配给每个 GPU，在计算期间仅使用每个 GPU 上的分区参数。例如，Megatron-LM
    Shoeybi 等人（[2019](#bib.bib15)）作为一种典型的张量并行方法，根据操作符的位置和连接，将张量沿行或列分割。通过这种方式，当模型尺寸较大时，它能比数据并行方法更有效地减少
    GPU 内存使用。
- en: As we described above, various distributed fine-tuning methods have been proposed,
    but the GPU memory usage and fine-tuning time required for each are different.
    For instance, conventional data parallelism provides the shortest fine-tuning
    time but requires the highest GPU memory usage. On the other hand, tensor parallelism
    has no benefit in saving fine-tuning time but can significantly reduce GPU memory
    usage. Users may want to select an appropriate method that avoids GPU OOM and
    has a short fine-tuning time. However, it is difficult to determine in advance
    whether there is enough GPU memory to fine-tune a given pre-trained LLM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，已经提出了各种分布式微调方法，但每种方法所需的 GPU 内存使用量和微调时间不同。例如，传统的数据并行方法提供了最短的微调时间，但需要最高的
    GPU 内存使用。另一方面，张量并行在节省微调时间方面没有优势，但可以显著减少 GPU 内存使用。用户可能希望选择一种合适的方法，既能避免 GPU OOM，又能实现较短的微调时间。然而，提前确定是否有足够的
    GPU 内存来微调给定的预训练 LLM 是困难的。
- en: DNNMem Gao et al. ([2020](#bib.bib8)) is the most recent work detailing procedures
    to estimate GPU memory usage on a single GPU. DNNMem provides key equations for
    GPU memory estimation when training various DNN models by analyzing the connections
    between operators and live tensors in the forward and backward passes. However,
    it has limitations for fine-tuning LLMs. GPU memory estimation for fine-tuning
    transformer-based LLM is challenging for two reasons.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DNNMem Gao 等人（[2020](#bib.bib8)）是最新的工作，详细描述了在单 GPU 上估算 GPU 内存使用的方法。DNNMem 提供了训练各种
    DNN 模型时，通过分析前向和反向传递中的操作符和活跃张量之间的连接来估算 GPU 内存的关键方程。然而，它在微调 LLM 时存在局限性。微调基于变压器的
    LLM 的 GPU 内存估算面临两个挑战。
- en: First, when fine-tuning an LLM in multi-GPU, distributed fine-tuning methods
    should be used to overcome GPU memory constraints due to large model sizes. Depending
    on the method used, the distribution of parameters, gradients, and optimizer states
    to each GPU is different, as is the amount of GPU memory used during the calculation
    process. Therefore, GPU memory usage estimates from a single GPU cannot be used
    in a multi-GPU environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在多 GPU 环境中微调大型语言模型（LLM）时，应使用分布式微调方法以克服因模型大小大而造成的 GPU 内存限制。根据所使用的方法，参数、梯度和优化器状态的分配到每个
    GPU 是不同的，计算过程中使用的 GPU 内存量也有所不同。因此，单 GPU 环境下的 GPU 内存使用估算值不能用于多 GPU 环境。
- en: Second, GPU memory consumption must be predicted by distinguishing between transformer
    and language modeling head (lm_head) parts. The transformer part is the central
    part of fine-tuning, where chunk memory management for memory sharing of model
    parameters and gradients is applied, and parameters are updated. On the other
    hand, the lm_head part requires separate analysis because it does not apply distributed
    methods directly and consumes a lot of memory due to its large dictionary size.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，必须通过区分变换器和语言建模头（lm_head）部分来预测GPU内存消耗。变换器部分是微调的核心部分，在这里应用了用于模型参数和梯度的内存共享的块内存管理，并更新参数。另一方面，lm_head部分需要单独分析，因为它不直接应用分布式方法，并由于其大词典大小而消耗大量内存。
- en: To address these challenges, we propose LLMem that estimates the GPU memory
    consumption when applying distributed fine-tuning methods to multiple GPUs. LLMem considers
    several factors to estimate GPU memory usage for each method, including recombining
    parameters prior to computation when applying advanced data parallelism and the
    output driven by all-gather in the backward pass when using tensor parallelism.
    Additionally, LLMem analyzes the difference in memory allocation method between
    the transformer and the lm_head part and reflects it in GPU memory estimation.
    To the best of our knowledge, this is the first work to estimate the peak GPU
    memory consumption for LLM fine-tuning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们提出了LLMem，它估算在将分布式微调方法应用于多个GPU时的GPU内存消耗。LLMem考虑了多个因素来估算每种方法的GPU内存使用，包括在应用高级数据并行性时计算前的参数重组，以及使用张量并行性时在反向传播中由all-gather驱动的输出。此外，LLMem分析了变换器和lm_head部分之间内存分配方法的差异，并将其反映在GPU内存估算中。据我们所知，这是第一次估算LLM微调的峰值GPU内存消耗的工作。
- en: 'In summary, our contributions are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献包括：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a GPU memory usage estimation method for LLM fine-tuning on single
    and multiple GPUs.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种GPU内存使用估算方法，适用于单个和多个GPU上的LLM微调。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide an algorithm to determine the most efficient distributed fine-tuning
    method based on GPU memory usage estimation.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一种算法，用于根据GPU内存使用估算确定最有效的分布式微调方法。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results show that LLMem estimates peak GPU memory usage to fine-tune
    LLM on a single GPU with error rates of up to 1.6%, which is significantly smaller
    than the state-of-the-art DNNMem’s average error rate of 42.6%. When applying
    distributed fine-tuning methods to LLMs with over a billion parameters on multiple
    GPUs, LLMem successfully estimates GPU memory usage with an average error rate
    of 3.0%.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，LLMem估算在单个GPU上微调LLM的峰值GPU内存使用，误差率高达1.6%，显著低于最先进的DNNMem的平均误差率42.6%。在对具有超过十亿参数的LLM应用分布式微调方法到多个GPU时，LLMem成功地估算了GPU内存使用，平均误差率为3.0%。
- en: Our source code repository can be found at [https://github.com/taehokim20/LLMem](https://github.com/taehokim20/LLMem).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的源代码库可以在[https://github.com/taehokim20/LLMem](https://github.com/taehokim20/LLMem)找到。
- en: 2 Related Works
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 GPU Memory Estimation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 GPU内存估算
- en: 'There have been several attempts to avoid GPU OOM issues by predicting the
    GPU memory usage that will be used to train a given model in advance. DNNMem Gao
    et al. ([2020](#bib.bib8)) sequentially traverses the computation graph of a DL
    model and computes the GPU memory consumption by taking into account previously
    allocated but still in-use tensors, newly allocated tensors for the currently
    visited operator, and resident buffers of the CUDA context and allocator reservation.
    Our LLMem is inspired by DNNMem, whose mechanism is described in more detail in
    Section [3](#S3 "3 Motivation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning
    Pre-Trained LLMs"). TSplit Nie et al. ([2022](#bib.bib11)) also calculates the
    total size of live tensors for the visiting operator. However, TSplit lacks an
    explanation of the detailed memory estimation process and its accuracy. SchedTune Albahar
    et al. ([2022](#bib.bib1)) predicts GPU memory usage not only based on DL model
    characteristics but also on different GPU types running the job. However, using
    measured GPU memory as data for prediction does not align with the purpose of
    estimating memory usage before fine-tuning a model.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '已经有几种尝试通过预测训练给定模型所需的 GPU 内存使用来避免 GPU OOM 问题。DNNMem Gao 等人 ([2020](#bib.bib8))
    顺序遍历 DL 模型的计算图，并计算 GPU 内存消耗，考虑到之前分配但仍在使用的张量、当前访问的操作符的新增张量，以及 CUDA 上下文和分配器保留的常驻缓冲区。我们的
    LLMem 受到 DNNMem 的启发，其机制在第 [3](#S3 "3 Motivation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs") 节中有更详细的描述。TSplit Nie 等人 ([2022](#bib.bib11))
    也计算了访问操作符的实时张量的总大小。然而，TSplit 缺乏详细的内存估算过程及其准确性的解释。SchedTune Albahar 等人 ([2022](#bib.bib1))
    预测 GPU 内存使用不仅基于 DL 模型特征，还基于运行作业的不同 GPU 类型。然而，使用测量的 GPU 内存作为预测数据并不符合在微调模型之前估算内存使用的目的。'
- en: 2.2 Distributed Fine-Tuning with GPUs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 使用 GPU 的分布式微调
- en: 'Data parallelism can enhance fine-tuning speed in proportion to the number
    of GPUs. However, LLM often runs into memory constraints, so the ZeRO optimizer Rajbhandari
    et al. ([2020](#bib.bib13)), described in Section [1](#S1 "1 Introduction ‣ LLMem:
    Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs"), is widely used
    as an alternative. The ZeRO optimizer selectively gathers only the model parameters
    or gradients required during the computation process and utilizes reduce-scatter
    after the computation to maintain their partitioning on each GPU.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '数据并行可以根据 GPU 的数量提高微调速度。然而，LLM 经常遇到内存限制，因此 ZeRO 优化器 Rajbhandari 等人 ([2020](#bib.bib13))，在第
    [1](#S1 "1 Introduction ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs") 节中描述，被广泛用作替代方案。ZeRO 优化器在计算过程中选择性地收集仅需要的模型参数或梯度，并在计算后使用 reduce-scatter 来保持它们在每个
    GPU 上的分区。'
- en: Tensor parallelism can further reduce peak GPU memory usage by sharding tensors
    under certain conditions, eliminating the need to gather all model parameters
    and gradients even during computation. Tensor parallelism results in each GPU
    producing only partial results, necessitating that all GPUs receive the same input
    data. The widely adopted tensor parallelism method, Megatron-LM Shoeybi et al.
    ([2019](#bib.bib15)), splits each model parameter tensor by row or column. Other
    proposed methods Xu et al. ([2021](#bib.bib22)) Wang et al. ([2021](#bib.bib20)) Bian
    et al. ([2021](#bib.bib3)) achieve additional memory savings by sharding both
    input and model parameters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行可以在某些条件下通过分片张量进一步减少峰值 GPU 内存使用，避免了在计算过程中需要聚集所有模型参数和梯度。张量并行导致每个 GPU 仅生成部分结果，这要求所有
    GPU 接收相同的输入数据。广泛采用的张量并行方法 Megatron-LM Shoeybi 等人 ([2019](#bib.bib15)) 按行或列拆分每个模型参数张量。其他提出的方法
    Xu 等人 ([2021](#bib.bib22)) Wang 等人 ([2021](#bib.bib20)) Bian 等人 ([2021](#bib.bib3))
    通过对输入和模型参数进行分片实现了额外的内存节省。
- en: If GPU memory constraints cannot be met with any distributed fine-tuning method
    on GPUs alone, we can use heterogeneous fine-tuning utilizing CPU memory. ZeRO-offload Ren
    et al. ([2021](#bib.bib14)) manages gradients, optimizer states, and optimizer
    computation on the CPU while retaining parameters and forward and backward computation
    on the GPU.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 GPU 内存限制无法通过仅在 GPU 上的任何分布式微调方法解决，我们可以使用异构微调来利用 CPU 内存。ZeRO-offload Ren 等人
    ([2021](#bib.bib14)) 在 CPU 上管理梯度、优化器状态和优化器计算，同时在 GPU 上保留参数以及前向和反向计算。
- en: 3 Motivation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机
- en: To select distributed fine-tuning methods, it is crucial to estimate GPU memory
    usage accurately. Existing approaches for estimating GPU memory usage do not consider
    scenarios where advanced data parallelism, such as the ZeRO Stage 3 optimizer Rajbhandari
    et al. ([2020](#bib.bib13)), or tensor parallelism is applied across multiple
    GPUs. Relying solely on estimated GPU memory usage on a single GPU when estimating
    on multiple GPUs can lead to significant errors. In this section, we implement
    the existing DNNMem Gao et al. ([2020](#bib.bib8)), validate the implementation
    results, and discuss factors causing substantial GPU memory estimation errors
    during the fine-tuning of pre-trained transformer-based language models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择分布式微调方法时，准确估算 GPU 内存使用量至关重要。现有的 GPU 内存使用量估算方法没有考虑在多个 GPU 上应用高级数据并行性（如 ZeRO
    Stage 3 优化器 Rajbhandari 等 ([2020](#bib.bib13))）或张量并行性的场景。仅依赖于单个 GPU 上估算的 GPU 内存使用量来估算多个
    GPU 上的情况可能会导致显著的误差。在本节中，我们实现了现有的 DNNMem Gao 等 ([2020](#bib.bib8))，验证了实现结果，并讨论了在微调预训练的基于变换器的语言模型时导致显著
    GPU 内存估算误差的因素。
- en: 3.1 DNNMem Implementation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 DNNMem 实现
- en: DNNMem source codes are not publicly available and are mainly based on TensorFlow,
    so we implement DNNMem based on the description in the paper Gao et al. ([2020](#bib.bib8)).
    First, we extract the corresponding computation graph from a given pre-trained
    DL model to identify the output size in each operator based on parameters, batch
    size ($bs$). We also compute pre-allocated GPU memory, including CUDA context
    and weight tensors of the model, before operator execution. In particular, since
    PyTorch does not release the loaded model parameters until the end of the fine-tuning,
    the initial GPU memory is retained throughout the fine-tuning process. The next
    step is to compute peak GPU memory usage at each operator while traversing the
    graph. We compute additional GPU memory with the input/output tensors and previously
    unreleased tensors in each operator during the forward propagation. Additionally,
    we reflect that PyTorch aligns with multiples of 512 bytes for internal tensor
    fragmentation, and DNNMem treats the buffer size as a constant (64 MB by default)
    as memory block management.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DNNMem 源代码并未公开，并且主要基于 TensorFlow，因此我们根据论文 Gao 等 ([2020](#bib.bib8)) 中的描述实现了
    DNNMem。首先，我们从给定的预训练深度学习模型中提取相应的计算图，以根据参数和批量大小 ($bs$) 确定每个操作符的输出大小。我们还计算了包括 CUDA
    上下文和模型的权重张量在内的预分配 GPU 内存，在操作符执行之前。特别地，由于 PyTorch 在微调结束之前不会释放加载的模型参数，因此初始 GPU 内存在整个微调过程中保持不变。下一步是在遍历计算图时计算每个操作符的峰值
    GPU 内存使用量。在前向传播过程中，我们计算了每个操作符的输入/输出张量和之前未释放张量的额外 GPU 内存。此外，我们反映了 PyTorch 在内部张量碎片化时对齐到
    512 字节的倍数，而 DNNMem 将缓冲区大小视为常量（默认 64 MB）作为内存块管理。
- en: To validate our DNNMem implementation, we compare GPU memory estimation results
    for the BERT Devlin et al. ([2018](#bib.bib6)) model on the GLUE benchmark Wang
    et al. ([2018](#bib.bib19)) with the experimental results from the paper. The
    environment we used in the experiment was PyTorch 2.0.1 with CUDA 11.7 on NVIDIA
    RTX2060, which differs from PyTorch 1.2.0 with CUDA 9.0 on NVIDIA Tesla P40 used
    in the DNNMem paper. In $bs=32,sl=32$, our DNNMem shows 20.48%, and the DNNMem
    shows 19.12% error rates. Considering similar error rates, we use our DNNMem implementation
    for single-GPU comparisons.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的 DNNMem 实现，我们将 BERT Devlin 等 ([2018](#bib.bib6)) 模型在 GLUE 基准测试 Wang 等
    ([2018](#bib.bib19)) 上的 GPU 内存估算结果与论文中的实验结果进行了比较。我们在实验中使用的环境是 PyTorch 2.0.1 和
    CUDA 11.7，运行在 NVIDIA RTX2060 上，这与 DNNMem 论文中使用的 PyTorch 1.2.0 和 CUDA 9.0 在 NVIDIA
    Tesla P40 上不同。在 $bs=32,sl=32$ 的情况下，我们的 DNNMem 显示 20.48% 的误差率，而 DNNMem 显示 19.12%
    的误差率。考虑到相似的误差率，我们使用我们的 DNNMem 实现进行单 GPU 比较。
- en: '![Refer to caption](img/5cddbbdb01d0c78bda80958a554efece.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5cddbbdb01d0c78bda80958a554efece.png)'
- en: 'Figure 1: Peak GPU memory estimates per total parameter size on a single GPU'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单个 GPU 上每总参数大小的峰值 GPU 内存估算
- en: 3.2 Limitations of DNNMem for LLM Fine-Tuning Memory Estimation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 DNNMem 在 LLM 微调内存估算中的局限性
- en: DNNMem Gao et al. ([2020](#bib.bib8)) does not handle mixed precision, which
    is commonly used in fine-tuning pre-trained language models. In addition, it does
    not consider how memory chunks are managed to ensure that forward pass parameters
    and backward pass gradients share the same GPU memory space Fang et al. ([2022](#bib.bib7)).
    Furthermore, DNNMem overlooks extra GPU memory usage during the initial fine-tuning
    iteration due to optimizer states.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DNNMem Gao 等 ([2020](#bib.bib8)) 不处理混合精度，这在微调预训练语言模型时常常使用。此外，它没有考虑如何管理内存块，以确保前向传递参数和反向传递梯度共享相同的
    GPU 内存空间 Fang 等 ([2022](#bib.bib7))。此外，DNNMem 忽视了在初始微调迭代中由于优化器状态而增加的 GPU 内存使用。
- en: 'Comparison results for estimating peak GPU memory usage of our proposed LLMem and
    DNNMem on a single GPU are shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 DNNMem Implementation
    ‣ 3 Motivation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs"). The experimental environment is summarized in Section [7.1](#S7.SS1 "7.1
    Experimental Setup ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning
    Pre-Trained LLMs"). LLMem predicts peak GPU memory usage with minimal error rates
    compared to ground truth, outperforming DNNMem. DNNMem exhibits larger errors
    as the total parameter size increases. Furthermore, DNNMem fails to predict GPU
    memory consumption in the context of distributed fine-tuning methods across multiple
    GPUs. As a result, existing approaches for estimating GPU memory usage face challenges
    when using the current transformer-based LLM for distributed fine-tuning.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '估计我们提出的 LLMem 和 DNNMem 在单个 GPU 上的峰值 GPU 内存使用的比较结果如图 [1](#S3.F1 "图 1 ‣ 3.1 DNNMem
    实现 ‣ 3 动机 ‣ LLMem: 估计 GPU 内存使用情况") 所示。实验环境总结在第 [7.1](#S7.SS1 "7.1 实验设置 ‣ 7 实验
    ‣ LLMem: 估计 GPU 内存使用情况") 节。与实际值相比，LLMem 在预测峰值 GPU 内存使用方面误差最小，优于 DNNMem。随着总参数量的增加，DNNMem
    显示出更大的误差。此外，DNNMem 在多 GPU 分布式微调方法的上下文中无法预测 GPU 内存消耗。因此，当前基于变换器的 LLM 在分布式微调中的 GPU
    内存使用估计面临挑战。'
- en: 'Table 1: Notation'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 符号说明'
- en: '| Symbol | Description |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $m_{base}$ | the initially used GPU memory |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| $m_{base}$ | 初始使用的 GPU 内存 |'
- en: '| $embed_{p}$ | the input embedding param size |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $embed_{p}$ | 输入嵌入参数大小 |'
- en: '| $lm_{p}$ | the language modeling head param size |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $lm_{p}$ | 语言建模头参数大小 |'
- en: '| $cs,bs,sl$ | chunk size, batch size, sequence length |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $cs,bs,sl$ | 块大小、批量大小、序列长度 |'
- en: '| $other_{p}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $other_{p}$ |'
- en: '| $B_{16}$ | 2 bytes for fp16, 4 bytes for fp32 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $B_{16}$ | fp16 为 2 字节，fp32 为 4 字节 |'
- en: '| $m_{p}$ | the GPU memory used by param fp16 and fp32 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| $m_{p}$ | param fp16 和 fp32 使用的 GPU 内存 |'
- en: '| $m_{p,16}$ | the GPU memory used by param fp16 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| $m_{p,16}$ | param fp16 使用的 GPU 内存 |'
- en: '| $m_{p,32}$ | the GPU memory used by param fp32 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $m_{p,32}$ | param fp32 使用的 GPU 内存 |'
- en: '| $cu_{p}$ | the CUDA memory page size |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| $cu_{p}$ | CUDA 内存页面大小 |'
- en: '| $m_{os}$ | the GPU memory used by momentum fp32 and variance fp32 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| $m_{os}$ | momentum fp32 和 variance fp32 使用的 GPU 内存 |'
- en: '| $e_{n},l_{n}$ | the number of Embedding or layers |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| $e_{n},l_{n}$ | 嵌入或层的数量 |'
- en: '| $o_{n}$ | the number of model’s output features |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| $o_{n}$ | 模型输出特征的数量 |'
- en: '| $m_{out}$ | the peak GPU memory usage due to output tensors |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| $m_{out}$ | 由于输出张量导致的峰值 GPU 内存使用 |'
- en: '| $dict_{n}$ | the size of the embedding dictionary |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| $dict_{n}$ | 嵌入词典的大小 |'
- en: '| $m_{lm}$ | the GPU memory used in the lm_head with the loss calculation |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| $m_{lm}$ | lm_head 中用于损失计算的 GPU 内存 |'
- en: '| $m_{peak}^{s}$ | the peak GPU memory usage on a single GPU |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| $m_{peak}^{s}$ | 单个 GPU 上的峰值 GPU 内存使用 |'
- en: '| $gpu_{n}$ | the number of GPUs in use |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| $gpu_{n}$ | 使用中的 GPU 数量 |'
- en: '| $m_{peak}^{dp}$ | the peak GPU memory usage with the advanced DP on multiple
    GPUs |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| $m_{peak}^{dp}$ | 多 GPU 上先进 DP 的峰值 GPU 内存使用 |'
- en: '| $m_{back}^{tp}$ | the additional GPU memory usage due to the temporary buffer
    through the backward all-gather |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| $m_{back}^{tp}$ | 由于反向全收集的临时缓冲区增加的 GPU 内存使用 |'
- en: '| $dp_{n},tp_{n}$ | the number of GPUs used for DP or TP |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| $dp_{n},tp_{n}$ | DP 或 TP 使用的 GPU 数量 |'
- en: '| $m_{peak}^{tp}$ | the peak GPU memory usage with 1D TP on multiple GPUs |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| $m_{peak}^{tp}$ | 多 GPU 上 1D TP 的峰值 GPU 内存使用 |'
- en: '| $m_{peak}^{dp+tp}$ | the peak GPU memory usage with the combination of DP+TP
    on multiple GPUs |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| $m_{peak}^{dp+tp}$ | 多 GPU 上 DP+TP 组合的峰值 GPU 内存使用 |'
- en: '| $m_{total}$ | the total GPU memory capacity |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| $m_{total}$ | 总 GPU 内存容量 |'
- en: 4 Single-GPU Memory Usage Estimation
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 单 GPU 内存使用估计
- en: 'This section outlines considerations for estimating GPU memory usage of transformer-based
    language models on a single GPU. The symbols used in the explanation are organized
    in Table [1](#S3.T1 "Table 1 ‣ 3.2 Limitations of DNNMem for LLM Fine-Tuning Memory
    Estimation ‣ 3 Motivation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning
    Pre-Trained LLMs").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '本节概述了在单个 GPU 上估算基于 Transformer 的语言模型 GPU 内存使用的考虑事项。解释中使用的符号已在表 [1](#S3.T1 "Table
    1 ‣ 3.2 Limitations of DNNMem for LLM Fine-Tuning Memory Estimation ‣ 3 Motivation
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") 中组织。'
- en: '![Refer to caption](img/64c7239048efe63bdc7595094080772e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64c7239048efe63bdc7595094080772e.png)'
- en: 'Figure 2: Illustration of tensors using GPU memory while fine-tuning the pre-trained
    model Ren et al. ([2021](#bib.bib14))'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 GPU 内存精细调优预训练模型的张量示意图 Ren 等人 ([2021](#bib.bib14))
- en: 4.1 Workflow for Fine-Tuning Pre-Trained Models
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 预训练模型的精细调优工作流程
- en: 'Initialization phase. The initialization phase preceding fine-tuning involves
    allocating memory for the CUDA context, responsible for managing information to
    control GPU devices, and memory for applying chunk-based memory management Fang
    et al. ([2022](#bib.bib7)). The initially used GPU memory is denoted as $m_{base}$.
    The chunk manager determines the optimal chunk size to minimize GPU memory waste
    based on the parameters of the provided pre-trained language model. GPU memory
    spaces for param fp16 (float-16) and param fp32 (float-32) are allocated in units
    of the chunk size ( 1 in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based Decoder
    Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage
    for Fine-Tuning Pre-Trained LLMs")).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '初始化阶段。精细调优前的初始化阶段包括为 CUDA 上下文分配内存，该上下文负责管理控制 GPU 设备的信息，以及为基于块的内存管理应用分配内存 Fang
    等人 ([2022](#bib.bib7))。最初使用的 GPU 内存记作 $m_{base}$。块管理器根据所提供的预训练语言模型的参数确定优化的块大小，以最小化
    GPU 内存的浪费。对于 param fp16（float-16）和 param fp32（float-32）的 GPU 内存空间按块大小单位分配（1
    在图 [4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based
    Decoder Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs")) 中)。'
- en: 'Fine-tuning phase. During the fine-tuning phase, param fp16 goes through forward
    and backward passes, and param fp16 is converted to gradient fp16, as illustrated
    in Figure [2](#S4.F2 "Figure 2 ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem:
    Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs"). Consequently,
    param fp16 and gradient fp16 share the same GPU memory space. After the backward
    pass, the ADAM optimizer updates parameters using optimizer states, including
    param fp32, momentum fp32, and variance fp32 tensors. Momentum fp32 and variance
    fp32 tensors, which are not allocated memory during the initialization process
    before fine-tuning, consume GPU memory based on the actual tensor size, not the
    chunk size. GPU memory occupied by these tensor types is allocated in the first
    iteration for fine-tuning ( 5 in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based Decoder
    Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage
    for Fine-Tuning Pre-Trained LLMs")). Subsequently, similar to chunk-based parameters,
    the GPU memory is retained until the fine-tuning process is complete.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '微调阶段。在微调阶段，参数 fp16 经过前向和反向传播，并且参数 fp16 被转换为梯度 fp16，如图[2](#S4.F2 "Figure 2 ‣
    4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for
    Fine-Tuning Pre-Trained LLMs")所示。因此，参数 fp16 和梯度 fp16 共享相同的 GPU 内存空间。反向传播后，ADAM
    优化器使用包括参数 fp32、动量 fp32 和方差 fp32 张量在内的优化器状态来更新参数。在微调前初始化过程中未分配内存的动量 fp32 和方差 fp32
    张量，根据实际张量大小而非块大小消耗 GPU 内存。GPU 内存的这些张量类型在微调的第一次迭代中分配（5
    图[4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based
    Decoder Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs")）。随后，与基于块的参数类似，GPU 内存会保留直到微调过程完成。'
- en: '![Refer to caption](img/10c20a2c5cf334326a18dbbd8c0fba86.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考图示](img/10c20a2c5cf334326a18dbbd8c0fba86.png)'
- en: 'Figure 3: Basic structure of transformer-based decoder model Vaswani et al.
    ([2017](#bib.bib18)). As shown in Figure [2](#S4.F2 "Figure 2 ‣ 4 Single-GPU Memory
    Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs"), the parameters in the transformer part are managed using chunk-based memory,
    while the lm_head part, responsible for deriving the output, consumes GPU memory
    based on its actual size.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 基于 Transformer 的解码器模型的基本结构 Vaswani 等人 ([2017](#bib.bib18))。如图[2](#S4.F2
    "Figure 2 ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs")所示，Transformer 部分的参数使用基于块的内存进行管理，而负责输出的
    lm_head 部分则根据实际大小消耗 GPU 内存。'
- en: 4.2 Memory Consumption with Structure of Transformer-based Decoder Model
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于 Transformer 的解码器模型的内存消耗
- en: The peak GPU memory usage on a single GPU (${m}_{peak}^{s}$) is
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 GPU 的峰值 GPU 内存使用 (${m}_{peak}^{s}$) 是
- en: '|  | $\displaystyle m_{peak}^{s}=m_{base}+m_{p}+m_{os}+m_{out}+m_{lm}$ |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m_{peak}^{s}=m_{base}+m_{p}+m_{os}+m_{out}+m_{lm}$ |  |'
- en: 'Each variable in this formula, except $m_{base}$ described in Section [4.1](#S4.SS1
    "4.1 Workflow for Fine-Tuning Pre-Trained Models ‣ 4 Single-GPU Memory Usage Estimation
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs"), is calculated
    as follows.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '本公式中的每个变量，除了第[4.1](#S4.SS1 "4.1 Workflow for Fine-Tuning Pre-Trained Models
    ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for
    Fine-Tuning Pre-Trained LLMs")节中描述的$m_{base}$，其计算方法如下。'
- en: First, $m_{p}$ is
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，$m_{p}$ 是
- en: '|  | $1$2 |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ', where $embed_{p}$ is the CUDA memory page size, typically $2\times 1024^{2}$
    is managed separately.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中 $embed_{p}$ 是 CUDA 内存页大小，通常 $2\times 1024^{2}$ 被单独管理。
- en: Second, $m_{os}$ is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，$m_{os}$是
- en: '|  | $\displaystyle m_{os}=\sum_{t\in\{E,L\}}\left\lceil t_{p}\times\frac{B_{32}+B_{32}}{cu_{p}}\right\rceil\times
    cu_{p}$ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m_{os}=\sum_{t\in\{E,L\}}\left\lceil t_{p}\times\frac{B_{32}+B_{32}}{cu_{p}}\right\rceil\times
    cu_{p}$ |  |'
- en: ', where $t$ is the parameter size of $t$. The system allocates GPU memory based
    on the actual size of each momentum fp32 and variance fp32, so GPU memory must
    be calculated for each tensor of each operator. Since the amount of GPU memory
    consumed by Bias or LayerNorm is very small, they can use space with other memory
    fragmentation. Therefore, we only calculate the GPU memory usage due to Embedding
    or Linear operator parameters.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中 $t$ 是 $t$ 的参数大小。系统根据每个动量 fp32 和方差 fp32 的实际大小分配 GPU 内存，因此必须为每个操作符的每个张量计算
    GPU 内存。由于 Bias 或 LayerNorm 消耗的 GPU 内存非常少，它们可以使用其他内存碎片的空间。因此，我们仅计算由于嵌入或线性操作符参数引起的
    GPU 内存使用。
- en: Third, $m_{out}$, respectively, then $m_{out}$ is
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，$m_{out}$，然后 $m_{out}$ 是
- en: '|  | $1$2 |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: PyTorch provides gradient checkpointing³³3Gradient checkpointing reduces GPU
    memory usage by clearing specific outputs and recomputing them during a backward
    pass. as an option to save memory during fine-tuning. Therefore, we support estimating
    GPU memory usage due to each operator’s input/output tensors considering gradient
    checkpointing. Since the output tensors of the current operator are the input
    tensors of the next operator, we focus on the output. It is challenging to accurately
    predict GPU memory consumption due to the outputs of operators within a model.
    We observed that the layer and embedding outputs of the transformer model are
    kept in GPU memory for efficient gradient checkpointing, which minimizes the increase
    in fine-tuning time. The estimation error rate is reduced using the $m_{out}$
    equation, which accounts for our observation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了梯度检查点³³3梯度检查点通过清除特定输出并在反向传播过程中重新计算它们，从而减少 GPU 内存使用，作为保存内存的选项。因此，我们支持估计由于每个操作符的输入/输出张量而引起的
    GPU 内存使用，考虑到梯度检查点。由于当前操作符的输出张量是下一个操作符的输入张量，我们关注输出。由于模型中操作符的输出而准确预测 GPU 内存消耗是具有挑战性的。我们观察到，变压器模型的层和嵌入输出保留在
    GPU 内存中，以便进行高效的梯度检查点，这最小化了微调时间的增加。使用 $m_{out}$ 方程来减少估计误差率，该方程考虑了我们的观察。
- en: Lastly, $m_{lm}$ is
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，$m_{lm}$ 是
- en: '|  | $\displaystyle m_{lm}=\left\lceil bs\times sl\times dict_{n}\times\frac{B}{cu_{p}}\right\rceil\times
    cu_{p}+$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m_{lm}=\left\lceil bs\times sl\times dict_{n}\times\frac{B}{cu_{p}}\right\rceil\times
    cu_{p}+$ |  |'
- en: '|  | $\displaystyle 2\times\left\lceil bs\times(sl-1)\times dict_{n}\times\frac{B}{cu_{p}}\right\rceil\times
    cu_{p}+lm_{p}$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 2\times\left\lceil bs\times(sl-1)\times dict_{n}\times\frac{B}{cu_{p}}\right\rceil\times
    cu_{p}+lm_{p}$ |  |'
- en: ', where $lm_{p}$ depending on the model type. The lm_head converts the transformer
    outputs into logits. Then, the value obtained by shifting the sequence length
    of the logits by one space is stored in a separate temporary variable and used
    for the loss calculation. $m_{out}+m_{lm}$ is the output phase ( 3
    in Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based
    Decoder Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs")).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '，其中 $lm_{p}$ 取决于模型类型。lm_head 将变压器输出转换为 logits。然后，将 logits 的序列长度向右移动一个位置得到的值存储在一个单独的临时变量中，并用于损失计算。$m_{out}+m_{lm}$
    是输出阶段（ 3
    在图 [4](#S4.F4 "图 4 ‣ 4.2 基于变压器的解码器模型的结构 ‣ 4 单 GPU 内存使用估计 ‣ LLMem: 估计用于微调预训练 LLM
    的 GPU 内存使用")。'
- en: '![Refer to caption](img/1c745ac875f6a8db87a7af0ff538edf1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c745ac875f6a8db87a7af0ff538edf1.png)'
- en: 'Figure 4: Peak GPU memory computation for different distributed fine-tuning
    methods.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同分布式微调方法的峰值 GPU 内存计算。
- en: '![Refer to caption](img/1652c74d371988939f44af709b41fc8b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1652c74d371988939f44af709b41fc8b.png)'
- en: (a) Advanced DP
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 高级 DP
- en: '![Refer to caption](img/e9cf839db002c236cf229e937bface53.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9cf839db002c236cf229e937bface53.png)'
- en: (b) TP
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (b) TP
- en: 'Figure 5: Advanced DP gathers the entire param fp16, while TP maintains the
    sharded param fp16 intact before entering the computation process.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：高级DP将整个参数fp16聚集在一起，而TP在进入计算过程之前保持分片的参数fp16不变。
- en: '![Refer to caption](img/7bd26658fed2954659fae0e91c267eef.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7bd26658fed2954659fae0e91c267eef.png)'
- en: 'Figure 6: Performing the linear operation in the forward and backward passes
    when employing TP. During the collection of partial outputs from each GPU after
    the backward pass, an additional GPU memory is consumed by a temporary buffer.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在使用TP时，执行前向和后向传递的线性操作。在后向传递后从每个GPU收集部分输出时，会消耗额外的GPU内存用于临时缓冲区。
- en: 5 Multi-GPU Memory Usage Estimation
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 多GPU内存使用估计
- en: This section outlines the factors for estimating peak GPU memory usage during
    distributed fine-tuning on multiple GPUs and summarizes the estimation process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了在多个GPU上进行分布式微调时估计峰值GPU内存使用的因素，并总结了估计过程。
- en: 'Conventional data parallelism (CDP). Since CDP places the entire model on each
    GPU, its peak GPU memory usage estimation equals the peak single-GPU memory usage
    $m_{peak}^{s}$, as shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption
    with Structure of Transformer-based Decoder Model ‣ 4 Single-GPU Memory Usage
    Estimation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 传统数据并行（CDP）。由于CDP将整个模型放置在每个GPU上，因此其峰值GPU内存使用估计等于峰值单GPU内存使用 $m_{peak}^{s}$，如图
    [4](#S4.F4 "图 4 ‣ 4.2 基于Transformer的解码器模型的内存消耗 ‣ 4 单GPU内存使用估计 ‣ LLMem：估计预训练LLMs的GPU内存使用")
    所示。
- en: Advanced data parallelism (ADP). The peak GPU memory usage with ADP on multiple
    GPUs ($m_{peak}^{dp}$) is
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 高级数据并行（ADP）。使用ADP在多个GPU上的峰值GPU内存使用 ($m_{peak}^{dp}$) 为
- en: '|  | $1$2 |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ', where $m_{p,16}$, reducing GPU memory usage. Among these, gradient fp16 shares
    GPU memory with param fp16 as explained in Section [4](#S4 "4 Single-GPU Memory
    Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs"), so we only need to divide the GPU memory usage of parameters and optimizer
    states by $gpu_{n}$ is allocated to the GPU memory.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ', 其中 $m_{p,16}$，减少GPU内存使用。其中，梯度fp16与参数fp16共享GPU内存，如第 [4](#S4 "4 单GPU内存使用估计
    ‣ LLMem：估计预训练LLMs的GPU内存使用") 节所述，因此我们只需将参数和优化器状态的GPU内存使用除以 $gpu_{n}$ 分配到GPU内存中。'
- en: Tensor parallelism (TP). The peak GPU memory usage with 1D TP on multiple GPUs
    ($m_{peak}^{tp}$) is
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行（TP）。使用1D TP在多个GPU上的峰值GPU内存使用 ($m_{peak}^{tp}$) 为
- en: '|  | $1$2 |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ', where $m_{back}^{tp}$ is'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ', 其中 $m_{back}^{tp}$ 为'
- en: '|  | $1$2 |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Tensor parallelism divides the parameter values of each operator by $gpu_{n}$
    is input, $A$ is the total temporary buffer size for tensors imported from the
    other GPUs, calculated by multiplying the output size of each layer by the number
    of layers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行将每个操作符的参数值按 $gpu_{n}$ 进行划分，$A$ 是从其他GPU导入的张量的总临时缓冲区大小，通过将每层的输出大小乘以层数来计算。
- en: Combination of DP+TP. The peak GPU memory usage with the combination of DP+TP
    on multiple GPUs ($m_{peak}^{dp+tp}$) is
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: DP+TP的组合。在多个GPU上使用DP+TP组合的峰值GPU内存使用 ($m_{peak}^{dp+tp}$) 为
- en: '|  | $1$2 |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ', as shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption with Structure
    of Transformer-based Decoder Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem:
    Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs"). It is possible
    to achieve hybrid parallelism by fine-tuning through a combination of data and
    tensor parallelism.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ', 如图 [4](#S4.F4 "图 4 ‣ 4.2 基于Transformer的解码器模型的内存消耗 ‣ 4 单GPU内存使用估计 ‣ LLMem：估计预训练LLMs的GPU内存使用")
    所示。通过结合数据并行和张量并行，可以实现混合并行。'
- en: 6 Distributed Fine-Tuning Method Decision
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分布式微调方法决策
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 6 Distributed Fine-Tuning Method Decision
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") describes
    the process for selecting the optimal method to fine-tune a pre-trained model
    based on the results of estimating the peak GPU memory usage. In Sections [4](#S4
    "4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for
    Fine-Tuning Pre-Trained LLMs") and [5](#S5 "5 Multi-GPU Memory Usage Estimation
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs"), We estimated
    $m_{peak}^{s}$. Here, $m_{peak}^{s}$ represents CDP, and the remaining estimations
    are connected to ADP, TP, and DP+TP, respectively. Of these methods, the optimal
    one is the method that requires the shortest time for fine-tuning while avoiding
    GPU OOM.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](#alg1 "算法 1 ‣ 6 分布式微调方法决策 ‣ LLMem：估算 GPU 内存使用情况以微调预训练 LLM") 描述了根据估算的峰值
    GPU 内存使用情况选择最佳微调方法的过程。在 [4](#S4 "4 单 GPU 内存使用估算 ‣ LLMem：估算 GPU 内存使用情况以微调预训练 LLM")
    和 [5](#S5 "5 多 GPU 内存使用估算 ‣ LLMem：估算 GPU 内存使用情况以微调预训练 LLM") 部分，我们估算了 $m_{peak}^{s}$。这里，$m_{peak}^{s}$
    代表 CDP，其余估算分别与 ADP、TP 和 DP+TP 相关。这些方法中，最佳的是在避免 GPU OOM 的情况下所需微调时间最短的方法。
- en: LLMem takes a pre-trained model $M$ is a list that stores the performance evaluation
    score of each method. $eval[0]$ correspond to CDP, ADP, TP, and DP+TP, respectively.
    LLMem increments the batch size $bs$ amount of data for fine-tuning in one iteration.
    In addition, since the ZeRO-3 optimizer increases the total communication volume
    of a baseline DP to $1.5\times$, and DP+TP uses $(bs-1)\times dp_{n}$ is the number
    of GPUs used for DP. These values become the performance scores of each method.
    Finally, LLMem selects the method with the highest performance score (If the scores
    are tied, select CDP, ADP, TP, and DP+TP in that order). If the performance scores
    of all methods are 0, heterogeneous training using CPU memory is selected as an
    alternative to avoid GPU OOM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LLMem 使用一个预训练模型 $M$ 作为一个列表，存储每种方法的性能评估分数。 $eval[0]$ 分别对应 CDP、ADP、TP 和 DP+TP。LLMem 在一次迭代中增加微调的数据批量大小
    $bs$。此外，由于 ZeRO-3 优化器将基线 DP 的总通信量增加至 $1.5\times$，而 DP+TP 使用 $(bs-1)\times dp_{n}$
    作为 DP 使用的 GPU 数量。这些值成为每种方法的性能分数。最后，LLMem 选择性能分数最高的方法（如果分数相同，按 CDP、ADP、TP 和 DP+TP
    的顺序选择）。如果所有方法的性能分数都是 0，则选择使用 CPU 内存的异构训练，以避免 GPU OOM。
- en: Algorithm 1 Distributed Fine-Tuning Method Decision
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 分布式微调方法决策
- en: 'Input: Pre-trained model $M$'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：预训练模型 $M$
- en: 1:$eval=[0,0,0,0]$ in range(4)do4:Set up the configure of the $i^{th}$, and
    $m_{os}$ then8:Repeat $bs=bs+1$ then11:Repeat $bs=bs+1$ then14:Repeat $bs=bs+1$ then17:Repeat
    $bs=bs+1$21:end for22:Save the index of the maximum score to $idx$  else 4, 0
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '1:$eval=[0,0,0,0]$ 在 range(4)do4: 设置 $i^{th}$ 的配置和 $m_{os}$，然后8: 重复 $bs=bs+1$，然后11:
    重复 $bs=bs+1$，然后14: 重复 $bs=bs+1$，然后17: 重复 $bs=bs+1$21: 结束 for22: 将最大分数的索引保存到 $idx$
    否则 4，0'
- en: 7 Experiments
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 实验
- en: In this section, we compare the peak GPU memory usage estimate of LLMem with
    the ground truth data when applying various distributed fine-tuning methods. In
    addition, our DNNMem implementation is included in comparing GPU memory usage
    estimation on a single GPU.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了 LLMem 对各种分布式微调方法的峰值 GPU 内存使用估算与实际数据。此外，我们的 DNNMem 实现也被包括在内，以比较单 GPU
    上的 GPU 内存使用估算。
- en: 7.1 Experimental Setup
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 实验设置
- en: 'For a multi-GPU environment, we use a Tesla V100 (total GPU memory capacity:
    16384 MB) with 4 GPUs in CloudLab CloudLab ([2023](#bib.bib5)). We also use the
    Colossal-AI Li et al. ([2023](#bib.bib9)), a widely used framework for applying
    distributed fine-tuning methods, and PyTorch 2.0.1 with CUDA 11.7\. The models
    we used in the experiment are OPT Zhang et al. ([2022](#bib.bib23)), BLOOM Workshop
    et al. ([2022](#bib.bib21)), CodeGen Nijkamp et al. ([2022](#bib.bib12)), BioGPT Luo
    et al. ([2022](#bib.bib10)), GPTBigCode Allal et al. ([2023](#bib.bib2)), GPT
    Neo Black et al. ([2021](#bib.bib4)), and LLaMA Touvron et al. ([2023](#bib.bib17)).
    The dataset used is alpaca data Taori et al. ([2023](#bib.bib16)), which is 52K
    instruction-following data. For the ground truth data, we measure peak GPU memory
    usage using only the maximum sequence length of 512.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多 GPU 环境，我们在 CloudLab 使用了 Tesla V100（总 GPU 内存容量：16384 MB）和 4 个 GPU ([2023](#bib.bib5))。我们还使用了
    Colossal-AI 李 et al. ([2023](#bib.bib9))，这是一个广泛使用的分布式微调方法框架，以及 PyTorch 2.0.1 和
    CUDA 11.7。我们在实验中使用的模型有 OPT 张 et al. ([2022](#bib.bib23))，BLOOM Workshop et al.
    ([2022](#bib.bib21))，CodeGen Nijkamp et al. ([2022](#bib.bib12))，BioGPT 罗 et al.
    ([2022](#bib.bib10))，GPTBigCode Allal et al. ([2023](#bib.bib2))，GPT Neo Black
    et al. ([2021](#bib.bib4))，和 LLaMA Touvron et al. ([2023](#bib.bib17))。使用的数据集是
    alpaca data Taori et al. ([2023](#bib.bib16))，共有 52K 条指令跟随数据。对于实际数据，我们仅使用最大序列长度
    512 测量峰值 GPU 内存使用量。
- en: '![Refer to caption](img/83a167ffd666bafce109bc6d3989b30f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/83a167ffd666bafce109bc6d3989b30f.png)'
- en: 'Figure 7: Comparison of peak GPU memory usage estimates between LLMem and DNNMem
    for models experiencing GPU OOM during fine-tuning.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：LLMem 与 DNNMem 在微调期间出现 GPU OOM 的模型之间的峰值 GPU 内存使用量估计的比较。
- en: 'Table 2: Estimating GPU memory usage on a single GPU. The values in parentheses
    represent the comparisons between the LLMem estimate and the ground truth, or
    the DNNMem estimate and the ground truth.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：单个 GPU 上的 GPU 内存使用量估计。括号中的值表示 LLMem 估计值与实际值之间的比较，或 DNNMem 估计值与实际值之间的比较。
- en: '| Model (MB) | LLMem | DNNMem | Ground truth |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 模型 (MB) | LLMem | DNNMem | 实际值 |'
- en: '| OPT-125m | 16314 (0.4) | 10402 (36.5) | 16378 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125m | 16314 (0.4) | 10402 (36.5) | 16378 |'
- en: '| OPT-350m | 16004 (1.6) | 9354 (42.5) | 16264 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350m | 16004 (1.6) | 9354 (42.5) | 16264 |'
- en: '| bloom-560m | 16578 (1.6) | 10726 (34.3) | 16324 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| bloom-560m | 16578 (1.6) | 10726 (34.3) | 16324 |'
- en: '| codegen-350M | 16236 (0.8) | 6910 (57.1) | 16100 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| codegen-350M | 16236 (0.8) | 6910 (57.1) | 16100 |'
- en: 7.2 Estimation of Single-GPU Memory Usage
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 单 GPU 内存使用量估计
- en: 'First, we compare the peak GPU memory usage estimate from LLMem for a single
    GPU with the DNNMem estimate and the actual peak GPU memory usage. Since we used
    gradient checkpointing for LLM fine-tuning, the same approach was applied to DNNMem.
    Figure [7](#S7.F7 "Figure 7 ‣ 7.1 Experimental Setup ‣ 7 Experiments ‣ LLMem:
    Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") compares the peak
    GPU memory usage estimation results of LLMem and DNNMem for various pre-trained
    LLMs that cause GPU OOM during fine-tuning on a single GPU. LLMem predicts GPU
    OOM for all models, while DNNMem predicts peak GPU memory usage that falls short
    of $m_{total}$. Table [2](#S7.T2 "Table 2 ‣ 7.1 Experimental Setup ‣ 7 Experiments
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") shows
    the predicted and actual GPU memory usage peaks when applying the maximum batch
    size to obtain the ground truth data for each model during fine-tuning on a single
    GPU. DNNMem underestimates the peak GPU memory usage for all models because it
    does not account for factors considered when fine-tuning Transformer-based LLM,
    as explained in Section [3.2](#S3.SS2 "3.2 Limitations of DNNMem for LLM Fine-Tuning
    Memory Estimation ‣ 3 Motivation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning
    Pre-Trained LLMs"). LLMem’s GPU memory estimation helps approximate the peak GPU
    memory usage close to the ground truth.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们比较了 LLMem 对单个 GPU 的峰值 GPU 内存使用量估计值与 DNNMem 的估计值和实际峰值 GPU 内存使用量。由于我们在 LLM
    微调中使用了梯度检查点，因此对 DNNMem 也采用了相同的方法。图 [7](#S7.F7 "图 7 ‣ 7.1 实验设置 ‣ 7 实验 ‣ LLMem:
    预训练 LLM 微调 GPU 内存使用量的估计") 比较了 LLMem 和 DNNMem 对于在单个 GPU 上微调期间导致 GPU OOM 的各种预训练
    LLM 的峰值 GPU 内存使用量估计结果。LLMem 预测了所有模型的 GPU OOM，而 DNNMem 预测的峰值 GPU 内存使用量低于 $m_{total}$。表
    [2](#S7.T2 "表 2 ‣ 7.1 实验设置 ‣ 7 实验 ‣ LLMem: 预训练 LLM 微调 GPU 内存使用量的估计") 显示了在对每个模型进行单
    GPU 微调时应用最大批量大小以获得实际数据时预测的 GPU 内存使用量峰值和实际值。由于 DNNMem 在微调 Transformer 基于 LLM 时未考虑相关因素，其对所有模型的峰值
    GPU 内存使用量进行了低估，如第 [3.2](#S3.SS2 "3.2 DNNMem 对 LLM 微调内存估计的局限性 ‣ 3 动机 ‣ LLMem: 预训练
    LLM 微调 GPU 内存使用量的估计") 节中所述。LLMem 的 GPU 内存估计帮助接近实际峰值 GPU 内存使用量。'
- en: '![Refer to caption](img/ddc0cbd6c6847c6bc5d7a0e71a35aa80.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/ddc0cbd6c6847c6bc5d7a0e71a35aa80.png)'
- en: 'Figure 8: Estimating GPU memory usage for ADP on four GPUs at each model’s
    maximum batch size to prevent GPU OOM. OOM in codegen-2b indicates running out
    of memory even at batch size=1.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：在每个模型的最大批量大小下，估计四个GPU上的ADP GPU内存使用量，以防止GPU OOM。codegen-2b中的OOM表示即使在batch
    size=1时也内存不足。
- en: 7.3 Estimation of Multi-GPU Memory Usage
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 多GPU内存使用估计
- en: 'CDP. The experimental results are the same as the memory usage estimation results
    on a single GPU in Section [7.2](#S7.SS2 "7.2 Estimation of Single-GPU Memory
    Usage ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'CDP。实验结果与第[7.2节](#S7.SS2 "7.2 Estimation of Single-GPU Memory Usage ‣ 7 Experiments
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs")中单个GPU的内存使用估计结果相同。'
- en: 'ADP. Figure [8](#S7.F8 "Figure 8 ‣ 7.2 Estimation of Single-GPU Memory Usage
    ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs") shows the predicted and actual GPU memory usage peaks when applying the
    maximum batch size to obtain ground truth data for each model during fine-tuning
    with ADP on four GPUs. The error rate between the predicted value of LLMem and
    the actual GPU memory usage tends to increase on multi-GPU setups. One reason
    is the gap in memory usage between the GPUs. ADP places tensors separately on
    each GPU instead of being sharded, so not all GPUs can use precisely the same
    amount of memory. Second, the error tends to be slightly larger when the model
    size is large. A larger number of layers and outputs in large models can lead
    to larger error rates due to memory allocator characteristics.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'ADP。图[8](#S7.F8 "Figure 8 ‣ 7.2 Estimation of Single-GPU Memory Usage ‣ 7 Experiments
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs")显示了在对四个GPU进行ADP微调时，应用最大批量大小以获取每个模型的真实数据时预测的和实际的GPU内存使用峰值。LLMem的预测值与实际GPU内存使用之间的误差在多GPU设置下趋向于增加。一个原因是GPU之间内存使用的差距。ADP将张量分别放置在每个GPU上，而不是分片，因此并非所有GPU都能使用完全相同数量的内存。其次，当模型尺寸较大时，误差往往略大。大模型中的层数和输出数量更多，可能由于内存分配器的特性导致误差率较高。'
- en: '![Refer to caption](img/1ce918cbb8065afe47f0c92a2806b766.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/1ce918cbb8065afe47f0c92a2806b766.png)'
- en: 'Figure 9: Estimating GPU memory usage for 4TP and 2DP+2TP at each model’s maximum
    batch size to avoid OOM.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在每个模型的最大批量大小下，估计4TP和2DP+2TP的GPU内存使用量，以避免OOM。
- en: 'TP and DP+TP. Figure [9](#S7.F9 "Figure 9 ‣ 7.3 Estimation of Multi-GPU Memory
    Usage ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs") shows the predicted and actual GPU memory usage peaks when applying the
    maximum batch size to obtain ground truth data for each model during fine-tuning
    with 4TP or 2DP+2TP on four GPUs. 4TP uses 4 GPUs in TP, and 2DP+2TP uses 2 GPUs
    in DP and 2 GPUs in TP for hybrid parallelism. We focus on estimating the peak
    GPU memory usage of the large-size model for TP because LLMem can select DP for
    quick fine-tuning of models that are small and do not have OOM problems. TP applies
    the all-gather operation in the backward pass, as shown in Figure [6](#S4.F6 "Figure
    6 ‣ 4.2 Memory Consumption with Structure of Transformer-based Decoder Model ‣
    4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for
    Fine-Tuning Pre-Trained LLMs"). The all-gather operation allocates temporary buffers
    in GPU memory and collects values in those buffers, consuming additional GPU memory.
    If the model size is large and the possible batch size is small, the system can
    use the allocated but currently empty memory space for a temporary buffer. Therefore,
    the GPU memory consumed due to the temporary buffer does not increase excessively,
    leading to smaller errors as shown in Figure [9](#S7.F9 "Figure 9 ‣ 7.3 Estimation
    of Multi-GPU Memory Usage ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage
    for Fine-Tuning Pre-Trained LLMs"). 2DP+2TP shows slightly larger errors than
    4TP in most cases. This is because GPU memory usage due to the temporary buffer
    may be additionally affected in 2 and 4
    of Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Memory Consumption with Structure of Transformer-based
    Decoder Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory
    Usage for Fine-Tuning Pre-Trained LLMs") while applying both DP and TP.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'TP 和 DP+TP。图[9](#S7.F9 "Figure 9 ‣ 7.3 Estimation of Multi-GPU Memory Usage
    ‣ 7 Experiments ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs") 显示了在使用最大批量大小来获取每个模型的真实数据时，应用 4TP 或 2DP+2TP 在四个 GPU 上进行微调时的预测和实际 GPU 内存使用峰值。4TP
    使用 4 个 GPU 进行 TP，而 2DP+2TP 使用 2 个 GPU 进行 DP 和 2 个 GPU 进行 TP，以实现混合并行。我们关注于 TP 大模型的峰值
    GPU 内存使用，因为 LLMem 可以选择 DP 来快速微调小型模型，并且没有 OOM 问题。TP 在反向传播过程中应用 all-gather 操作，如图[6](#S4.F6
    "Figure 6 ‣ 4.2 Memory Consumption with Structure of Transformer-based Decoder
    Model ‣ 4 Single-GPU Memory Usage Estimation ‣ LLMem: Estimating GPU Memory Usage
    for Fine-Tuning Pre-Trained LLMs") 所示。all-gather 操作在 GPU 内存中分配临时缓冲区并收集这些缓冲区中的值，消耗额外的
    GPU 内存。如果模型尺寸较大且可能的批量大小较小，系统可以使用已分配但当前为空的内存空间作为临时缓冲区。因此，由于临时缓冲区而消耗的 GPU 内存不会过度增加，导致如图[9](#S7.F9
    "Figure 9 ‣ 7.3 Estimation of Multi-GPU Memory Usage ‣ 7 Experiments ‣ LLMem:
    Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") 所示的误差较小。2DP+2TP
    在大多数情况下显示出比 4TP 稍大的误差。这是因为由于临时缓冲区引起的 GPU 内存使用可能会在图[4](#S4.F4 "Figure 4 ‣ 4.2 Memory
    Consumption with Structure of Transformer-based Decoder Model ‣ 4 Single-GPU Memory
    Usage Estimation ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained
    LLMs") 的 2
    和 4
    处同时应用 DP 和 TP 时受到额外影响。'
- en: 'Table 3: Distributed fine-tuning method selection by LLMem on four GPUs and
    the actual amount of time it takes to fine-tune each method to the largest possible
    batch size (s)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLMem 在四个 GPU 上的分布式微调方法选择及每种方法微调到最大批量大小所需的实际时间 (s)
- en: '| Model (s) | Selection | 4DP | 2DP+2TP | 4TP |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 模型 (s) | 选择 | 4DP | 2DP+2TP | 4TP |'
- en: '| OPT-1.3b | 4DP | 688 | 1616 | 2186 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3b | 4DP | 688 | 1616 | 2186 |'
- en: '| OPT-2.7b | 4TP | OOM | 8174 | 6038 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| OPT-2.7b | 4TP | 内存溢出 | 8174 | 6038 |'
- en: '| bloom-1b1 | 4DP | 680 | 1724 | 2631 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| bloom-1b1 | 4DP | 680 | 1724 | 2631 |'
- en: '| bloom-3b | 4TP | OOM | OOM | 14495 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| bloom-3b | 4TP | 内存溢出 | 内存溢出 | 14495 |'
- en: '| BioGPT-Large | 4DP | 1022 | 3315 | 4773 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| BioGPT-Large | 4DP | 1022 | 3315 | 4773 |'
- en: '| codegen-2B-nl | 4TP | OOM | 6314 | 6244 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| codegen-2B-nl | 4TP | 内存溢出 | 6314 | 6244 |'
- en: '| gpt_bigcode | 4DP | 651 | 1292 | 1652 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| gpt_bigcode | 4DP | 651 | 1292 | 1652 |'
- en: '| gpt-neo-1.3B | 4DP | 768 | 1686 | 2372 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| gpt-neo-1.3B | 4DP | 768 | 1686 | 2372 |'
- en: '| llama-7b | CPU offloading | OOM | OOM | OOM |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| llama-7b | CPU 卸载 | 内存溢出 | 内存溢出 | 内存溢出 |'
- en: 7.4 Fine-Tuning Method Selection with LLMem
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 使用 LLMem 的微调方法选择
- en: 'Table [3](#S7.T3 "Table 3 ‣ 7.3 Estimation of Multi-GPU Memory Usage ‣ 7 Experiments
    ‣ LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs") assesses
    whether LLMem finds the optimal fine-tuning method to achieve the fastest fine-tuning
    while avoiding GPU OOM for various models. When measuring the time taken for each
    method, we applied the maximum batch size that can prevent GPU OOM. LLMem typically
    selects TP when DP causes GPU OOM. It is challenging for LLMem to choose DP+TP
    because only 4 GPUs were used in the experiment. DP+TP allows for more diverse
    combinations depending on the number of GPUs used and is more likely to be selected.
    LLMem also suggests CPU offloading when GPU memory is insufficient.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S7.T3 "表 3 ‣ 7.3 多 GPU 内存使用估计 ‣ 7 实验 ‣ LLMem: 估计 GPU 内存使用以微调预训练的 LLMs")
    评估了 LLMem 是否能找到最佳的微调方法，以实现最快的微调速度，同时避免 GPU 内存溢出。测量每种方法所需的时间时，我们应用了可以防止 GPU 内存溢出的最大批量大小。LLMem
    通常选择 TP 当 DP 导致 GPU 内存溢出时。由于实验中只使用了 4 个 GPU，因此 LLMem 选择 DP+TP 会有挑战，因为 DP+TP 允许根据使用的
    GPU 数量进行更多的组合，更可能被选择。LLMem 还建议在 GPU 内存不足时进行 CPU 卸载。'
- en: 8 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper introduces LLMem, a method for estimating GPU memory consumption
    during fine-tuning of large language models (LLMs) on multi-GPU setups. We analyze
    factors affecting GPU memory usage, considering different memory allocation methods
    for the transformer and output sections. Experimental results demonstrate that
    LLMem achieves accurate peak GPU memory usage estimation on both single and multiple
    GPUs with minimal error rates.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文介绍了 LLMem，一种在多 GPU 设置下对大型语言模型（LLMs）进行微调时估计 GPU 内存消耗的方法。我们分析了影响 GPU 内存使用的因素，考虑了不同的内存分配方法，用于变换器和输出部分。实验结果表明，LLMem
    在单 GPU 和多 GPU 上都能准确地估计峰值 GPU 内存使用，并且误差率极低。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Albahar et al. [2022] Hadeel Albahar, Shruti Dongare, Yanlin Du, Nannan Zhao,
    Arnab K Paul, and Ali R Butt. Schedtune: A heterogeneity-aware gpu scheduler for
    deep learning. In 2022 22nd IEEE International Symposium on Cluster, Cloud and
    Internet Computing (CCGrid), pages 695–705\. IEEE, 2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Albahar et al. [2022] 哈迪尔·阿尔巴哈尔、舒瑞提·东加雷、闵林·杜、南南·赵、阿纳布·K·保罗 和 阿里·R·巴特。**Schedtune:
    一种考虑异构性的 GPU 调度器**用于深度学习。**在 2022 年第 22 届 IEEE 国际集群、云和互联网计算研讨会（CCGrid）**，第 695–705
    页。IEEE，2022。'
- en: 'Allal et al. [2023] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, et al. Santacoder: don’t reach for the stars! arXiv preprint
    arXiv:2301.03988, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal et al. [2023] **卢布娜·本·阿拉尔、雷蒙德·李、德尼斯·科切特科夫、成浩·牟、克里斯托弗·阿基基、卡洛斯·穆尼奥斯·费兰迪斯、尼克拉斯·穆宁霍夫、马扬克·米什拉、亚历克斯·古、马南·德伊**
    等。**Santacoder: 不要追逐星星！** arXiv 预印本 arXiv:2301.03988，2023。'
- en: Bian et al. [2021] Zhengda Bian, Qifan Xu, Boxiang Wang, and Yang You. Maximizing
    parallelism in distributed training for huge neural networks. arXiv preprint arXiv:2105.14450,
    2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian et al. [2021] 郑达·边、徐启凡、王博翔、游阳。**最大化分布式训练中的并行性**以应对巨大的神经网络。arXiv 预印本 arXiv:2105.14450，2021。
- en: 'Black et al. [2021] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella
    Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow.
    [https://doi.org/10.5281/zenodo.5297715](https://doi.org/10.5281/zenodo.5297715),
    2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Black et al. [2021] 西德·布莱克、高乐、菲尔·王、康纳·利希、斯特拉·比德曼。**Gpt-neo：使用 Mesh-TensorFlow
    的大规模自回归语言建模**。 [https://doi.org/10.5281/zenodo.5297715](https://doi.org/10.5281/zenodo.5297715)，2021。
- en: CloudLab [2023] CloudLab. [https://www.cloudlab.us/](https://www.cloudlab.us/),
    2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudLab [2023] CloudLab。 [https://www.cloudlab.us/](https://www.cloudlab.us/)，2023。
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. arXiv preprint arXiv:1810.04805, 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    BERT: 深度双向变换器的语言理解预训练。arXiv 预印本 arXiv:1810.04805，2018年。'
- en: Fang et al. [2022] Jiarui Fang, Zilin Zhu, Shenggui Li, Hui Su, Yang Yu, Jie
    Zhou, and Yang You. Parallel training of pre-trained models via chunk-based dynamic
    memory management. IEEE Transactions on Parallel and Distributed Systems, 34(1):304–315,
    2022.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. [2022] Jiarui Fang, Zilin Zhu, Shenggui Li, Hui Su, Yang Yu, Jie
    Zhou, 和 Yang You. 通过基于块的动态内存管理进行预训练模型的并行训练。IEEE并行与分布式系统汇刊，34(1):304–315，2022年。
- en: Gao et al. [2020] Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu,
    Haoxiang Lin, and Mao Yang. Estimating gpu memory consumption of deep learning
    models. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering, pages 1342–1352,
    2020.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2020] Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu,
    Haoxiang Lin, 和 Mao Yang. 估计深度学习模型的 GPU 内存消耗。发表于第28届ACM欧洲软件工程会议与软件工程基础研讨会联合会议论文集，页码1342–1352，2020年。
- en: 'Li et al. [2023] Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen
    Huang, Yuliang Liu, Boxiang Wang, and Yang You. Colossal-ai: A unified deep learning
    system for large-scale parallel training. In Proceedings of the 52nd International
    Conference on Parallel Processing, pages 766–775, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023] Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen
    Huang, Yuliang Liu, Boxiang Wang, 和 Yang You. Colossal-AI: 一个统一的大规模并行训练深度学习系统。发表于第52届国际并行处理会议论文集，页码766–775，2023年。'
- en: 'Luo et al. [2022] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang,
    Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for
    biomedical text generation and mining. Briefings in Bioinformatics, 23(6):bbac409,
    2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. [2022] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang,
    Hoifung Poon, 和 Tie-Yan Liu. BioGPT: 生成预训练变换器用于生物医学文本生成与挖掘。生物信息学简报，23(6):bbac409，2022年。'
- en: 'Nie et al. [2022] Xiaonan Nie, Xupeng Miao, Zhi Yang, and Bin Cui. Tsplit:
    Fine-grained gpu memory management for efficient dnn training via tensor splitting.
    In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pages 2615–2628\.
    IEEE, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie et al. [2022] Xiaonan Nie, Xupeng Miao, Zhi Yang, 和 Bin Cui. TSplit: 通过张量拆分实现高效DNN训练的细粒度GPU内存管理。发表于2022
    IEEE第38届数据工程国际会议（ICDE），页码2615–2628，IEEE，2022年。'
- en: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large
    language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474,
    2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, 和 Caiming Xiong. CodeGen: 一个开源的大型代码语言模型，支持多轮程序合成。arXiv
    预印本 arXiv:2203.13474，2022年。'
- en: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis, pages 1–16\. IEEE, 2020.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    和 Yuxiong He. Zero: 针对训练万亿参数模型的内存优化。发表于SC20: 高性能计算、网络、存储与分析国际会议，页码1–16，IEEE，2020年。'
- en: Ren et al. [2021] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. $\{$ model training.
    In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551–564, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. [2021] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, 和 Yuxiong He. $\{$ 模型训练。发表于2021
    USENIX年度技术会议（USENIX ATC 21），页码551–564，2021年。
- en: 'Shoeybi et al. [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion
    parameter language models using model parallelism. arXiv preprint arXiv:1909.08053,
    2019.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi et al. [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, 和 Bryan Catanzaro. Megatron-LM: 使用模型并行训练多十亿参数的语言模型。arXiv
    预印本 arXiv:1909.08053，2019年。'
- en: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 斯坦福 Alpaca:
    一种遵循指令的Llama模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等 [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等等. Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. Advances in neural information processing systems, 30, 2017.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等 [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即你所需。神经信息处理系统进展，30，2017。
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等 [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
    和 Samuel R Bowman. Glue: 自然语言理解的多任务基准和分析平台。arXiv 预印本 arXiv:1804.07461, 2018。'
- en: Wang et al. [2021] Boxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. 2.5-dimensional
    distributed model training. arXiv e-prints, pages arXiv–2105, 2021.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等 [2021] Boxiang Wang, Qifan Xu, Zhengda Bian, 和 Yang You. 2.5维分布式模型训练。arXiv
    e-prints, 页码 arXiv–2105, 2021。
- en: 'Workshop et al. [2022] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher
    Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
    Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual
    language model. arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Workshop等 [2022] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher
    Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
    Luccioni, François Yvon, 等等. Bloom: 一个176b参数的开放获取多语言模型。arXiv 预印本 arXiv:2211.05100,
    2022。'
- en: Xu et al. [2021] Qifan Xu, Shenggui Li, Chaoyu Gong, and Yang You. An efficient
    2d method for training super-large deep learning models. arXiv e-prints, pages
    arXiv–2104, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等 [2021] Qifan Xu, Shenggui Li, Chaoyu Gong, 和 Yang You. 一种高效的2d方法用于训练超大型深度学习模型。arXiv
    e-prints, 页码 arXiv–2104, 2021。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
    2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等 [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, 等等.
    Opt: 开放的预训练变换器语言模型。arXiv 预印本 arXiv:2205.01068, 2022。'
