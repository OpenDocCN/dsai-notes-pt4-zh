- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:40:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Two-stage LLM Fine-tuning with Less Specialization and More Generalization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双阶段LLM微调：减少专门化与提高泛化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00635](https://ar5iv.labs.arxiv.org/html/2211.00635)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.00635](https://ar5iv.labs.arxiv.org/html/2211.00635)
- en: Yihan Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 王宜涵
- en: Department of Computer Science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: UCLA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学洛杉矶分校
- en: wangyihan617@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: wangyihan617@gmail.com
- en: '&Si Si'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Si Si'
- en: Google
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: sisidaisy@google.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: sisidaisy@google.com
- en: '&Daliang Li'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '&大亮·李'
- en: Google
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: daliangli@google.com
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: daliangli@google.com
- en: '&Michal Lukasik'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&米哈乌·卢卡西克'
- en: Google
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: mlukasik@google.com
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: mlukasik@google.com
- en: '&Felix Yu'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '&费利克斯·余'
- en: Google
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: felixyu@google.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: felixyu@google.com
- en: '&Cho-Jui Hsieh'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&卓俊·谢'
- en: Department of Computer Science
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: UCLA
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学洛杉矶分校
- en: chohsieh@cs.ucla.edu
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: chohsieh@cs.ucla.edu
- en: '&Inderjit S Dhillon'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&英德吉特·S·迪伦'
- en: Google
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: isd@google.com
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: isd@google.com
- en: '&Sanjiv Kumar'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&Sanjiv Kumar'
- en: Google
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌
- en: sanjivk@google.com
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: sanjivk@google.com
- en: Abstract
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Pretrained large language models (LLMs) are general purpose problem solvers
    applicable to a diverse set of tasks with prompts. They can be further improved
    towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning
    usually makes the model narrowly specialized on this dataset with reduced general
    in-context learning performances, which is undesirable whenever the fine-tuned
    model needs to handle additional tasks where no fine-tuning data is available.
    In this work, we first demonstrate that fine-tuning on a single task indeed decreases
    LLMs’ general in-context learning performance. We discover one important cause
    of such forgetting, format specialization, where the model overfits to the format
    of the fine-tuned task. We further show that format specialization happens at
    the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning
    with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework
    that reduces format specialization and improves generalization. ProMoT offloads
    task-specific format learning into additional and removable parameters by first
    doing prompt tuning and then fine-tuning the model itself with this soft prompt
    attached. With experiments on several fine-tuning tasks and 8 in-context evaluation
    tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks
    to standard fine-tuning, but with much less loss of in-context learning performances
    across a board range of out-of-domain evaluation tasks. More importantly, ProMoT
    can even enhance generalization on in-context learning tasks that are semantically
    related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly
    improves performance on other language pairs, and ProMoT on NLI improves performance
    on summarization. Experiments also show that ProMoT can improve the generalization
    performance of multi-task training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）是通用问题解决工具，适用于多种任务的提示。通过在专业数据集上进行微调，可以进一步提高其在特定任务上的表现。然而，微调通常使得模型在该数据集上变得过于专门化，导致通用上下文学习性能下降，这在模型需要处理没有微调数据的额外任务时是不理想的。在这项工作中，我们首先展示了在单一任务上进行微调确实会降低LLMs的通用上下文学习性能。我们发现了这种遗忘的一个重要原因，即格式专门化，模型对微调任务的格式过度拟合。我们进一步展示了格式专门化在微调开始时就会发生。为了解决这个问题，我们提出了具有模型调优的提示调优（ProMoT），这是一种简单而有效的两阶段微调框架，它减少了格式专门化并提高了泛化能力。ProMoT通过先进行提示调优，然后在附加了该软提示的情况下对模型本身进行微调，将任务特定的格式学习卸载到附加的可移除参数中。通过在几个微调任务和8个上下文评估任务上的实验，我们表明，ProMoT在微调任务上的表现与标准微调相当，但在广泛的域外评估任务中损失的上下文学习性能要小得多。更重要的是，ProMoT甚至可以增强在与微调任务语义相关的上下文学习任务上的泛化能力，例如，ProMoT在英法翻译上的表现显著提升了其他语言对的性能，而ProMoT在NLI任务上的表现则提升了总结任务的性能。实验还表明，ProMoT可以提高多任务训练的泛化性能。
- en: 1 Introduction
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Natural language processing (NLP) has recently been revolutionized by scaling
    up transformer based large language models (LLMs) together with large-scale pretraining
    (Vaswani et al., [2017](#bib.bib39); Devlin et al., [2019](#bib.bib12); Raffel
    et al., [2020a](#bib.bib33); Brown et al., [2020](#bib.bib6); Rae et al., [2021](#bib.bib32);
    Chowdhery et al., [2022](#bib.bib8); Smith et al., [2022](#bib.bib36); Touvron
    et al., [2023](#bib.bib38)). In addition to improved downstream performances,
    these pretrained LLMs can perform a broad array of unforeseen tasks when provided
    with a prompt. This in-context learning capability allows users to flexibly re-purpose
    LLMs for specific tasks with a minimum amount of supervised data, making it extremely
    convenient for fast prototyping and experimentation, especially in the low data
    regime.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）最近通过扩展基于变换器的大型语言模型（LLMs）以及大规模预训练（Vaswani et al., [2017](#bib.bib39)；Devlin
    et al., [2019](#bib.bib12)；Raffel et al., [2020a](#bib.bib33)；Brown et al., [2020](#bib.bib6)；Rae
    et al., [2021](#bib.bib32)；Chowdhery et al., [2022](#bib.bib8)；Smith et al., [2022](#bib.bib36)；Touvron
    et al., [2023](#bib.bib38)）发生了革命性的变化。除了改进的下游性能，这些预训练的LLMs在提供提示时可以执行各种未曾预料的任务。这种上下文学习能力使用户能够灵活地将LLMs重新用于特定任务，所需的监督数据量最少，使得快速原型开发和实验变得极为方便，特别是在数据较少的情况下。
- en: However, even the largest and most advanced LLMs leave a lot to be improved.
    Grounding and eliminating hallucinations (Maynez et al., [2020](#bib.bib23)),
    reasoning and logical clarity (Creswell & Shanahan, [2022](#bib.bib10)), mathematics
    (Brown et al., [2020](#bib.bib6); Noorbakhsh et al., [2021](#bib.bib28)) are just
    a few examples where LLMs still lag behind the best human performances, or in
    some cases, the fine-tuned performances of the same model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即便是最大最先进的LLMs仍然有许多待改进的地方。基础知识和消除幻觉（Maynez et al., [2020](#bib.bib23)）、推理和逻辑清晰性（Creswell
    & Shanahan, [2022](#bib.bib10)）、数学（Brown et al., [2020](#bib.bib6)；Noorbakhsh
    et al., [2021](#bib.bib28)）只是LLMs仍然落后于最佳人类表现的一些例子，或者在某些情况下，是相同模型经过微调后的表现。
- en: The most common practice to improve a pretrained model is to fine-tune it on
    a specialized task or several tasks. However, fine-tuning on LLM usually causes
    over-specialization to the fine-tuning tasks, and harm the model’s pre-existing
    generalization ability on unseen tasks via in-context learning. As we show later,
    an mT5 model finetuned on a single task loses its few-shot performance on unseen
    tasks within one thousand steps of fine-tuning. When faced with hundreds of downstream
    tasks and even unknown tasks, we expect to have a single fine-tuned model that
    is both superior on supervised fine-tuned tasks and general unseen tasks. Thus,
    it becomes very important to develop new techniques for finetuning that prevent
    over-specialization of these fine-tuned models only to a few tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 改进预训练模型的最常见方法是对其进行特定任务或多个任务的微调。然而，在LLM上进行微调通常会导致对微调任务的过度专业化，并通过上下文学习损害模型在未见任务上的预存泛化能力。如我们稍后所示，经过微调的mT5模型在单一任务上的微调会在`1000`步内丧失其在未见任务上的少量样本表现。当面临数百个下游任务甚至未知任务时，我们期望有一个单一的微调模型，既在监督微调任务中表现优越，又在未见任务中具有良好的泛化能力。因此，开发新技术以防止这些微调模型仅对少数任务过度专业化变得非常重要。
- en: Ground-Truth Output Mercedes’ Lewis Hamilton took the outright championship
    lead for the first time this season with a dominant victory in the Italian Grand
    Prix. Pretrained mT5 Hamilton won the British Grand Prix. Fine-tuned mT5 on RTE
    True Fine-tuned mT5 with ProMoT (Ours) on RTE Lewis Hamilton won the Italian Grand
    Prix.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 真实输出：梅赛德斯车队的刘易斯·汉密尔顿在意大利大奖赛上以压倒性的胜利首次取得本赛季的总冠军领先。预训练的mT5汉密尔顿赢得了英国大奖赛。微调的mT5在RTE上正确，微调的mT5与ProMoT（我们）在RTE上，刘易斯·汉密尔顿赢得了意大利大奖赛。
- en: 'Table 1: Output comparison of pretrained and fine-tuned mT5 models vs. fine-tuned
    with ProMoT on the RTE binary classification NLI dataset, performing in-context
    1-shot summarization.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：预训练和微调的mT5模型与在RTE二分类NLI数据集上使用ProMoT进行微调的模型的输出比较，执行上下文1-shot总结。
- en: In this work, we discover that the loss of general in-context learning abilities
    during fine-tuning is, to a large extent, caused by format specialization, which
    makes model overfitting to the specific task format. For example, an mT5 (Xue
    et al., [2020](#bib.bib43)) model learns in the output space with only “True”
    and “False” if we fine-tune it on a binary classification dataset, losing its
    ability to flexibly generate different output styles according to the in-context
    prompts of other tasks. We show that format specialization tends to happen at
    the very beginning of fine-tuning, before the model fully learns the semantic
    content of the task.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们发现微调过程中通用上下文学习能力的丧失在很大程度上是由于格式专业化，这使得模型过拟合于特定任务格式。例如，如果我们在一个二分类数据集上对
    mT5（Xue 等，[2020](#bib.bib43)）模型进行微调，该模型在输出空间中仅学习“True”和“False”，从而丧失根据其他任务的上下文提示灵活生成不同输出样式的能力。我们表明，格式专业化倾向于发生在微调的最开始阶段，即在模型完全学习任务的语义内容之前。
- en: 'Based on these observations, we propose a simple solution to alleviate format
    specialization: PROmpt Tuning with MOdel Tuning (ProMoT), which off-loads format
    learning to a small amount of task-specific parameters that are external to the
    model. ProMoT is a two-stage fine-tuning process. At the first stage, we freeze
    the pretrained model and tune a small set of additional parameters, where we find
    adding soft prompt before the input (Lester et al., [2021](#bib.bib20)) is a good
    choice. At the second stage, we freeze the additional parameters and tune the
    main model. Since format information is learned first, it mostly enters the small
    set of additional parameters. At inference time, we can decide whether to remove
    the additional parameters depending on whether the incoming task share the same
    format as the fine-tuned task.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些观察，我们提出了一种简单的解决方案来缓解格式专业化问题：**PROmpt Tuning with MOdel Tuning**（ProMoT），它将格式学习转移到模型外的一小部分任务特定参数上。ProMoT
    是一个两阶段的微调过程。在第一阶段，我们冻结预训练模型，并调整一小组附加参数，我们发现将**软提示**添加到输入之前（Lester 等，[2021](#bib.bib20)）是一个不错的选择。在第二阶段，我们冻结附加参数，并调整主模型。由于格式信息首先被学习，它主要进入那一小组附加参数。在推理时，我们可以根据传入任务是否与微调任务具有相同格式来决定是否移除附加参数。
- en: Our experiments show that ProMoT significantly alleviates specialization during
    fine-tuning, while boosting generalization on semantically related tasks with
    different formats. For example, fine-tuning the model only on an NLI binary classification
    dataset, a mT5 XXL model consistently obtains improved in-context learning performance
    on summarization compared with the pretrained model, possibly due to improved
    grounding learned from NLI. See Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization") for a concrete
    example. With ProMoT, we can obtain models with both better supervised performance
    compared to pretrained models and better general in-context learning performance
    compared to standard finetuning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，ProMoT 显著缓解了微调过程中的专业化，同时提升了在具有不同格式的语义相关任务上的泛化能力。例如，仅在 NLI 二分类数据集上微调模型，mT5
    XXL 模型在摘要生成任务上的上下文学习性能相比于预训练模型显著提升，这可能是由于从 NLI 中学习到的改进的基础知识。具体示例见表[1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ Two-stage LLM Fine-tuning with Less Specialization and More
    Generalization")。通过 ProMoT，我们可以获得在监督性能上优于预训练模型，并且在通用上下文学习性能上优于标准微调的模型。
- en: 'To summarize, our contributions are 4-fold:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的贡献有四方面：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show empirically that general in-context learning capabilities decrease during
    single-task fine-tuning for T5 models. We identify format specialization as one
    of the important causes which mostly happens at the beginning of fine-tuning.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过实验证明，在对 T5 模型进行单任务微调时，通用的上下文学习能力会下降。我们确定格式专业化是主要原因之一，这种情况通常发生在微调的开始阶段。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a novel 2-stage fine-tuning framework: PROmpt Tuning with MOdel
    Tuning (ProMoT) to reduce format specialization during fine-tuning.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的两阶段微调框架：**PROmpt Tuning with MOdel Tuning**（ProMoT），以减少微调过程中的格式专业化。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiments on 10+ NLP tasks show that ProMoT significantly reduces specialization
    of fine-tuned models compared to standard fine-tuning, while reaching similar
    supervised performance. The reduction in specialization opens up opportunities
    to enhance generalization across very dissimilar tasks when they share some semantic
    aspects.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 10 多个 NLP 任务上的实验表明，与标准微调相比，ProMoT 显著减少了微调模型的专业化，同时达到了类似的监督性能。专业化的减少为在共享某些语义方面的非常不同任务之间增强泛化能力提供了机会。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ProMoT can be combined with many existing fine-tuning and parameter-efficient
    fine-tuning methods. We show examples where ProMoT is combined with multi-task
    fine-tuning and fine-tuning with 1-shot prompts to further boost the generalization
    on unseen tasks.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ProMoT 可以与许多现有的微调和参数高效微调方法结合使用。我们展示了 ProMoT 与多任务微调和使用 1-shot 提示的微调相结合的例子，以进一步提升对未见任务的泛化能力。
- en: 2 Related Work
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Pretrained LLMs are general problem solvers with in-context prompts (Raffel
    et al., [2020b](#bib.bib34); Xue et al., [2020](#bib.bib43); Radford et al., [2018](#bib.bib31);
    Chowdhery et al., [2022](#bib.bib8); Min et al., [2022](#bib.bib26); Touvron et al.,
    [2023](#bib.bib38)). Zhai et al. ([2023](#bib.bib44)) evaluates the catastrophic
    forgetting in multimodal language model fine-tuning, which is limited to image
    classification tasks. Chan et al. ([2022](#bib.bib7)); Gao et al. ([2020](#bib.bib13))
    study the effect of pretraining data distribution on in-context learning on image
    recognition tasks, where the tension between in-context learning tasks and fine-tuning
    tasks is discussed. They propose changing the data distribution to ease such tension,
    which could be difficult for generative NLP tasks. ProMoT is an orthogonal method
    that does not require changes in data distribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的 LLM 是通用问题解决者，使用上下文提示（Raffel 等，[2020b](#bib.bib34); Xue 等，[2020](#bib.bib43);
    Radford 等，[2018](#bib.bib31); Chowdhery 等，[2022](#bib.bib8); Min 等，[2022](#bib.bib26);
    Touvron 等，[2023](#bib.bib38)）。Zhai 等（[2023](#bib.bib44)）评估了多模态语言模型微调中的灾难性遗忘，这仅限于图像分类任务。Chan
    等（[2022](#bib.bib7)）；Gao 等（[2020](#bib.bib13)）研究了预训练数据分布对图像识别任务中的上下文学习的影响，讨论了上下文学习任务与微调任务之间的紧张关系。他们提出了改变数据分布以缓解这种紧张，这对于生成型
    NLP 任务可能比较困难。ProMoT 是一种正交方法，不需要改变数据分布。
- en: In a recent study, Ramasesh et al. ([2022](#bib.bib35)) found that as model
    size increases, the model becomes less prone to catastrophic forgetting. However
    such studies are mostly focused on tasks of similar format, e.g. a sequence of
    different classification tasks. In this work we explore vastly different tasks,
    e.g. classification v.s. long form generation where the format itself is critical.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项研究中，Ramasesh 等（[2022](#bib.bib35)）发现，随着模型规模的增加，模型变得不那么容易遭遇灾难性遗忘。然而，这些研究大多集中在格式相似的任务上，例如不同分类任务的序列。在这项工作中，我们探索了截然不同的任务，例如分类与长文本生成，其中格式本身是关键。
- en: Different from full fine-tuning, prompt-tuning (Lester et al., [2021](#bib.bib20);
    Zhang et al., [2021](#bib.bib45)), adapters and LoRA (Hu et al., [2021](#bib.bib17);
    He et al., [2021](#bib.bib14); Houlsby et al., [2019](#bib.bib16)) adapt a pretrained
    model to a task with a small set of tunable parameters. Parameter-efficient methods
    like these largely leave the pretrained model intact, which can preserve the pre-existing
    in-context learning abilities. However, they also miss the opportunity to further
    improve the pretrained model with a small, high quality dataset that generalizes
    beyond the fine-tuned task. Besides, these parameter-efficient methods also underperform
    fine-tuning on the supervised task in many cases, as shown in (Lester et al.,
    [2021](#bib.bib20); Liu et al., [2021](#bib.bib22)) and in our results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与完整微调不同，prompt-tuning（Lester 等，[2021](#bib.bib20); Zhang 等，[2021](#bib.bib45)）、适配器和
    LoRA（Hu 等，[2021](#bib.bib17); He 等，[2021](#bib.bib14); Houlsby 等，[2019](#bib.bib16)）通过少量可调参数将预训练模型适配到任务上。这些参数高效的方法大体上保持了预训练模型不变，这可以保留已有的上下文学习能力。然而，它们也错失了用小而高质量的数据集进一步提升预训练模型的机会，这些数据集具有超越微调任务的泛化能力。此外，正如（Lester
    等，[2021](#bib.bib20); Liu 等，[2021](#bib.bib22)）和我们的结果所示，这些参数高效的方法在许多情况下也表现不如微调。
- en: 'Another line of work uses multi-task fine-tuning to improve generalization
    on unseen in-context learning tasks. Wei et al. ([2021a](#bib.bib41)); Chung et al.
    ([2022](#bib.bib9)) fine-tune PaLM and T5 on large-scale multitask datasets with
    diverse natural language prompts, improving the zero- and few-shot performance
    on unseen tasks. Min et al. ([2021](#bib.bib25)) incorporate the in-context learning
    objective into fine-tuning on multitask datasets with few-shot prompts. This approach
    relies on multi-task training to generalize, while orthogonally, ProMoT improves
    the generalization of each single fine-tuning task, whether used in a multi-task
    setting or not. ProMoT can indeed be combined with multi-task training to obtain
    better generalization as we demonstrate in Sec. [5.4](#S5.SS4 "5.4 More Generalization
    with Multitask Training ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization"). In addition, such approaches often require
    human engineered instructions or prompts for each task to partly alleviate format
    specialization, while ProMoT uses prompt tuning, which has two advantages: 1)
    ProMoT does not require the elaborate trial and error of prompt engineering as
    it optimizes the soft prompts with data. 2) Soft prompts are more effective at
    absorbing the format compared to natural language prompts, as shown in Table  [3](#S5.T3
    "Table 3 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类工作使用多任务微调来提高在未见上下文学习任务上的泛化能力。Wei 等（[2021a](#bib.bib41)）；Chung 等（[2022](#bib.bib9)）在大规模多任务数据集上对
    PaLM 和 T5 进行微调，这些数据集包含多样化的自然语言提示，从而提高了在未见任务上的零-shot 和少-shot 性能。Min 等（[2021](#bib.bib25)）将上下文学习目标融入到多任务数据集的少-shot
    提示微调中。这种方法依赖于多任务训练来进行泛化，而 ProMoT 从正交方向上提高了每个单独微调任务的泛化能力，无论是否在多任务设置中使用。ProMoT 确实可以与多任务训练结合，以获得更好的泛化能力，正如我们在第
    [5.4](#S5.SS4 "5.4 More Generalization with Multitask Training ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")
    节中展示的那样。此外，这些方法通常需要针对每个任务的人为设计指令或提示，以部分缓解格式专门化，而 ProMoT 使用提示调优，这具有两个优势：1) ProMoT
    不需要繁琐的提示工程试验，因为它通过数据优化软提示。2) 软提示比自然语言提示更有效地吸收格式，正如表 [3](#S5.T3 "Table 3 ‣ 5.5
    Ablation Study ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization") 所示。
- en: 3 Format specialization in fine-tuning causes the loss of in-context learning
    capabilities
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 格式专门化在微调中导致上下文学习能力的丧失
- en: In this section, we first show empirically with an mT5 XXL model that 1) in-context
    learning abilities are lost during fine-tuning; 2) format specialization is an
    important cause for such loss; 3) format specialization happens at the very beginning
    of fine-tuning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先通过对 mT5 XXL 模型的实证研究展示了 1) 上下文学习能力在微调过程中丧失；2) 格式专门化是这种丧失的重要原因；3) 格式专门化在微调的最初阶段就会发生。
- en: 3.1 Loss of in-context learning capabilities during fine-tuning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 微调过程中上下文学习能力的丧失
- en: In this subsection, we first show that the in-context learning performance usually
    drops significantly after standard fine-tuning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们首先展示了标准微调后上下文学习性能通常会显著下降。
- en: In our experiments, we fine-tune a pretrained mT5 XXL model (13B parameters)
    (Xue et al., [2020](#bib.bib43)) on the Recognizing Textual Entailment (RTE) dataset (Wang
    et al., [2019](#bib.bib40)). In RTE tasks, the model is required to predict “True”
    or “False” for whether the two given sentences are entailed. We fine-tune the
    mT5 model with default hyper-parameters and input/output template used in PaLM
    (Chowdhery et al., [2022](#bib.bib8)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们在 Recognizing Textual Entailment (RTE) 数据集（Wang 等，[2019](#bib.bib40)）上对预训练的
    mT5 XXL 模型（13B 参数）（Xue 等，[2020](#bib.bib43)）进行微调。在 RTE 任务中，模型需要预测给定的两个句子是否具有蕴涵关系，即预测“True”或“False”。我们使用默认的超参数和
    PaLM（Chowdhery 等，[2022](#bib.bib8)）中使用的输入/输出模板对 mT5 模型进行微调。
- en: '![Refer to caption](img/e852f2f82b382687296dc2ac1f76d505.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e852f2f82b382687296dc2ac1f76d505.png)'
- en: 'Figure 1: Loss of in-context learning abilities during fine-tuning. We show
    the learning curve of a model being fine-tuned on RTE dataset while being tested
    on 1-shot QA datasets. Left axis: Accuracy on RTE. Right axis: Exact match rate
    on 1-shot QA.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：微调过程中上下文学习能力的丧失。我们展示了一个模型在 RTE 数据集上微调时的学习曲线，同时在 1-shot QA 数据集上进行测试。左轴：RTE
    上的准确率。右轴：1-shot QA 上的精确匹配率。
- en: '![Refer to caption](img/37782bd491c5cb16a5dfcb7a7fbf71be.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37782bd491c5cb16a5dfcb7a7fbf71be.png)'
- en: 'Figure 2: Format specialization in fine-tuning: showing the frequency of "True/False"
    style outputs when evaluated on 1-shot TriviaQA. The model is being fine-tuned
    on RTE. Left axis: Accuracy on RTE. Right axis: Ratio of True/False.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：微调中的格式专业化：显示在1-shot TriviaQA中评估时“True/False”风格输出的频率。模型正在对RTE进行微调。左轴：RTE的准确率。右轴：True/False的比例。
- en: We want to see whether the model lost its in-context learning abilities on unseen
    task during fine-tuning. Therefore, we evaluate the fine-tuned model with two
    1-shot QA tasks, TriviaQA (Joshi et al., [2017](#bib.bib18)) and web_questions
    (Berant et al., [2013](#bib.bib2)). The results are illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Loss of in-context learning capabilities during fine-tuning ‣
    3 Format specialization in fine-tuning causes the loss of in-context learning
    capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    where we can see that when the accuracy on RTE dataset increases with fine-tuning,
    performance on few-shot QA tasks drops drastically. This phenomenon is general
    and not a result of specific fine-tuning or evaluation tasks (more results in
    Section [5.3](#S5.SS3 "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想查看模型在微调过程中是否丧失了对未见任务的上下文学习能力。因此，我们使用两个1-shot QA任务对微调后的模型进行评估，分别是TriviaQA
    (Joshi et al., [2017](#bib.bib18))和web_questions (Berant et al., [2013](#bib.bib2))。结果如图[2](#S3.F2
    "Figure 2 ‣ 3.1 Loss of in-context learning capabilities during fine-tuning ‣
    3 Format specialization in fine-tuning causes the loss of in-context learning
    capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")所示，我们可以看到，当RTE数据集的准确率随着微调的增加而提高时，few-shot
    QA任务的性能却急剧下降。这一现象是普遍存在的，而不是特定微调或评估任务的结果（更多结果见第[5.3](#S5.SS3 "5.3 Generalization
    with Single Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization")节）。
- en: '![Refer to caption](img/78201dd5fda5440358836e7b5682d7e9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78201dd5fda5440358836e7b5682d7e9.png)'
- en: 'Figure 3: Format specialization happens at the beginning of fine-tuning: we
    show the cosine similarity between the full gradient $g$ are much better aligned
    at the start of training, compared to at 400 steps. Comparison between more steps
    can be found in Figure [7](#A1.F7 "Figure 7 ‣ A.3.7 Plotting more steps for Figure
    3 ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization") (Appendix).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：格式专业化发生在微调的开始阶段：我们展示了全梯度$g$的余弦相似度，在训练开始时对齐效果明显优于在400步时。更多步骤的比较可以在图[7](#A1.F7
    "Figure 7 ‣ A.3.7 Plotting more steps for Figure 3 ‣ A.3 Additional Experiment
    Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization")（附录）中找到。
- en: 3.2 Format specialization
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 格式专业化
- en: Why are the in-context learning abilities of an LLM so easily lost after a few
    hundred steps of fine-tuning? A natural hypothesis is that due to the homogeneity
    of output formats in fine-tuning datasets, the model quickly specializes to this
    task format and learns to follow it no matter what the input sequence is. This
    leads to the loss of in-context learning abilities on other tasks that do not
    share the same format. here by “format” we refer to the common characteristics
    of the sequences in fine-tuning task as a subset of all possible sequences, such
    as the language used, typical input/output lengths and styles, special tokens
    or punctuation, upper/lower case styles etc. For example, the output format of
    RTE is a set of two labels, “True” or “False”, among all possible sequences of
    tokens of various lengths. Since all data points share the same format in single-task
    fine-tuning, the model receives a strong gradient signal that the output should
    follow this format, thus its in-context learning performance on other tasks with
    different formats will drop, even when they share important semantic similarities
    with the fine-tuned task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么LLM的上下文学习能力在经过几百步的微调后如此容易丧失？一个自然的假设是，由于微调数据集中输出格式的同质性，模型很快会专门化到这种任务格式并学会无论输入序列是什么都遵循它。这导致了在其他任务上失去上下文学习能力，而这些任务的格式与微调任务不相同。这里的“格式”指的是微调任务中序列的共同特征作为所有可能序列的子集，如使用的语言、典型的输入/输出长度和风格、特殊符号或标点符号、大写/小写风格等。例如，RTE的输出格式是一组两个标签，“True”或“False”，在各种长度的标记序列中。由于单任务微调中的所有数据点都共享相同的格式，模型接收到强烈的梯度信号，指示输出应遵循这一格式，因此其在其他格式不同的任务上的上下文学习表现将下降，即使这些任务与微调任务在语义上有重要相似之处。
- en: To verify this hypothesis, we evaluate the RTE fine-tuned mT5 model on 1-shot
    TriviaQA task and count the percentage of outputs which are “True” or “False”.
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Loss of in-context learning capabilities during
    fine-tuning ‣ 3 Format specialization in fine-tuning causes the loss of in-context
    learning capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization") shows that as the fine-tuning proceeds, the model outputs
    more “True” or ‘False’ even with a 1-shot prompted input from TriviaQA. In particular,
    after 300 fine-tuning steps, $90\%$ of the output becomes “True” or “False”. This
    indicates that format specialization is a possible reason for the loss of general
    in-context learning capabilities during fine-tuning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一假设，我们在 1-shot TriviaQA 任务上评估 RTE 微调的 mT5 模型，并计算“True”或“False”输出的百分比。图[2](#S3.F2
    "图 2 ‣ 3.1 微调过程中上下文学习能力的丧失 ‣ 3 格式专业化在微调中导致上下文学习能力的丧失 ‣ 两阶段 LLM 微调，减少专业化并增加泛化")显示，随着微调的进行，即使在
    TriviaQA 的 1-shot 提示输入下，模型也输出更多的“True”或“False”。特别是，在 300 次微调步骤后，$90\%$ 的输出变为“True”或“False”。这表明格式专业化可能是微调过程中丧失通用上下文学习能力的一个原因。
- en: 3.3 Format learning happens first during standard fine-tuning
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 格式学习首先在标准微调过程中发生
- en: Next, we show experimental evidence that format learning happens first during
    standard fine-tuning. This is not surprising as the overwhelming majority of fine-tuning
    data points have very similar formats, causing a gradient signal that dominates
    over others, more nuanced elements such as the semantic content of the task.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了实验证据，证明格式学习首先在标准微调过程中发生。这并不令人惊讶，因为绝大多数微调数据点具有非常相似的格式，导致一个主导其他更细微元素（如任务的语义内容）的梯度信号。
- en: More concretely, for the RTE dataset, the “format” refers to the fact that the
    $\text{output}\in\{\text{True},\text{False}\}$ is highly aligned with the format-only
    gradient $g_{\text{format}}$, by first fine-tuning the model on RTE for 400 steps,
    then computing the gradient on the randomized RTE dataset with the same batch
    of input sequences., when the True/False ratio reaches nearly 100%.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，对于 RTE 数据集，“格式”指的是 $\text{output}\in\{\text{True},\text{False}\}$ 与仅格式梯度
    $g_{\text{format}}$ 高度对齐，通过首先在 RTE 上微调模型 400 步，然后在相同输入序列的随机 RTE 数据集上计算梯度，当 True/False
    比率接近 100% 时。
- en: '4 Proposed Method: PROmpt tuning with MOdel Tuning (ProMoT)'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提议的方法：PROmpt 调优与 MOdel 调优 (ProMoT)
- en: 'The observations from Section [3](#S3 "3 Format specialization in fine-tuning
    causes the loss of in-context learning capabilities ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization") inspire us to decouple format
    learning from fine-tuning, in order to alleviate specialization to the fine-tuned
    task and preserve general in-context learning abilities. The key idea is to offload
    format learning to a separate small set of parameters during early fine-tuning,
    and allow the model’s own parameter changes afterwards to focus more on the semantic
    content of the task. We propose a two-stage fine-tuning strategy called ProMoT,
    illustrated in Figure [4](#S4.F4 "Figure 4 ‣ 4 Proposed Method: PROmpt tuning
    with MOdel Tuning (ProMoT) ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization"). At the first stage, ProMoT uses prompt tuning to capture
    the format in a trainable soft prompt while the model itself is frozen. At the
    second stage, ProMoT freezes the learned soft prompt and fine-tunes the model
    itself to focus on semantic skills that might be more transferable.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从第[3](#S3 "3 格式专业化在微调中导致上下文学习能力的丧失 ‣ 两阶段 LLM 微调，减少专业化并增加泛化")节的观察启发我们将格式学习与微调解耦，以缓解对微调任务的专业化，并保留通用的上下文学习能力。关键思想是在早期微调期间将格式学习卸载到一组单独的小参数上，然后允许模型自身的参数变化后，更加关注任务的语义内容。我们提出了一种名为
    ProMoT 的两阶段微调策略，如图[4](#S4.F4 "图 4 ‣ 4 提议的方法：PROmpt 调优与 MOdel 调优 (ProMoT) ‣ 两阶段
    LLM 微调，减少专业化并增加泛化")所示。在第一阶段，ProMoT 使用提示调优将格式捕捉到一个可训练的软提示中，同时模型本身保持冻结。在第二阶段，ProMoT
    冻结已学习的软提示，并微调模型本身以关注可能更具可迁移性的语义技能。
- en: '![Refer to caption](img/c1cd2e549b5a7b5fbd0fb347e3c8a674.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c1cd2e549b5a7b5fbd0fb347e3c8a674.png)'
- en: 'Figure 4: Overview of ProMoT, our two-stage fine-tuning strategy. We run prompt
    tuning at Stage 1 and model fine-tuning with the trained prompt at Stage 2\. Green
    denotes trainable parameters and blue means frozen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ProMoT的概览，我们的两阶段微调策略。在阶段 1 中进行提示调优，在阶段 2 中使用训练好的提示进行模型微调。绿色表示可训练的参数，蓝色表示冻结的参数。
- en: 'Stage 1: Prompt Tuning.'
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阶段 1：提示调优。
- en: Here we use a continuous trainable prompt (soft prompt) (Lester et al., [2021](#bib.bib20))
    prepended before the embedded inputs as the separate small set of tunable parameters.
    The soft prompt for a given fine-tuned task $P_{e}\in\mathbb{R}^{p\times e}$ is
    the embedding size. Given an input sequence, prompt tuning first embeds it with
    the text embedding layer of the pretrained model, and then prepends it with the
    soft trainable prompt. The soft prompt is then optimized to reduce the loss while
    the pretrained model is frozen. As indicated in Section [3.3](#S3.SS3 "3.3 Format
    learning happens first during standard fine-tuning ‣ 3 Format specialization in
    fine-tuning causes the loss of in-context learning capabilities ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization"), fine-tuning first
    learns the format. We expect that by prompt tuning first, the soft prompt will
    learn the format. Although it is not guaranteed that the soft prompt only learns
    the format, the small capacity can prevent the soft prompt from learning all semantic
    skills in most realistic NLP tasks, as demonstrated by the performance gap between
    prompt tuning and standard fine-tuning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一个连续的可训练提示（软提示）（Lester et al., [2021](#bib.bib20)）作为单独的小型可调参数集，前置于嵌入输入之前。对于给定的微调任务，软提示
    $P_{e}\in\mathbb{R}^{p\times e}$ 是嵌入大小。给定一个输入序列，提示调优首先通过预训练模型的文本嵌入层对其进行嵌入，然后用软可训练提示前置。软提示随后被优化以减少损失，而预训练模型保持冻结。正如[3.3](#S3.SS3
    "3.3 格式学习首先发生在标准微调期间 ‣ 3 格式专业化导致上下文学习能力的丧失 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")部分所示，微调首先学习格式。我们期望通过首先进行提示调优，软提示将学习到格式。尽管不能保证软提示仅学习格式，但小容量可以防止软提示在大多数实际
    NLP 任务中学习所有语义技能，这一点在提示调优和标准微调之间的性能差距中有所体现。
- en: 'Stage 2: Fine-tuning with trained prompt.'
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阶段 2：使用训练好的提示进行微调。
- en: 'After prompt-tuning, we expect the trained prompt now storing most of the format
    information. We then freeze the soft prompt and fine-tune the pretrained model.
    Importantly, as shown in Figure [4](#S4.F4 "Figure 4 ‣ 4 Proposed Method: PROmpt
    tuning with MOdel Tuning (ProMoT) ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization"), the soft prompt is still prepended before the input
    during this stage, forcing the model to learn things not captured already by the
    soft prompt.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示调优之后，我们期望训练好的提示现在存储了大部分格式信息。我们随后冻结软提示并微调预训练模型。重要的是，如图[4](#S4.F4 "图 4 ‣ 4
    提议的方法：PROmpt 调优与 MOdel 调优 (ProMoT) ‣ 两阶段 LLM 微调，减少专业化，增加泛化")所示，在此阶段软提示仍然前置于输入之前，强迫模型学习软提示尚未捕获的内容。
- en: Other parameter-efficient and fine-tuning methods.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他参数高效和微调方法。
- en: ProMoT is a general framework that can be combined with different parameter-efficient
    tuning and fine-tuning techniques in respective stages. Conceptually, the prompt-tuning
    at the first stage can be replaced by other commonly used parameter-efficient
    methods such as LoRA (Hu et al., [2021](#bib.bib17)). However, empirically we
    found prompt-tuning is much better than LoRA on absorbing format information in
    early fine-tuning. More discussions can be found in Appendix [A.3.6](#A1.SS3.SSS6
    "A.3.6 Using LORA in the first stage ‣ A.3 Additional Experiment Results ‣ Appendix
    A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization").
    For fine-tuning methods, we show examples to combine ProMoT with multi-task fine-tuning
    (Section [5.4](#S5.SS4 "5.4 More Generalization with Multitask Training ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"))
    and 1-shot in-context learning prompt (Section [5.3](#S5.SS3 "5.3 Generalization
    with Single Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization"), Section [5.4](#S5.SS4 "5.4 More
    Generalization with Multitask Training ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization")). Training with 1-shot prompt
    is introduced by Min et al. ([2021](#bib.bib25)) in a multi-task training setting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ProMoT 是一个通用框架，可以与不同的参数高效调整和微调技术相结合。在概念上，第一阶段的提示调整可以被其他常用的参数高效方法替代，如 LoRA (Hu
    et al., [2021](#bib.bib17))。然而，经验上我们发现提示调整在吸收格式信息方面明显优于 LoRA。在附录 [A.3.6](#A1.SS3.SSS6
    "A.3.6 使用 LORA 在第一阶段 ‣ A.3 额外实验结果 ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增强泛化") 中有更多讨论。对于微调方法，我们展示了将
    ProMoT 与多任务微调 (第 [5.4](#S5.SS4 "5.4 通过多任务训练实现更多泛化 ‣ 5 实验 ‣ 两阶段 LLM 微调，减少专业化，增强泛化")
    节) 和 1-shot 上下文学习提示 (第 [5.3](#S5.SS3 "5.3 单任务微调的泛化 ‣ 5 实验 ‣ 两阶段 LLM 微调，减少专业化，增强泛化")
    节, 第 [5.4](#S5.SS4 "5.4 通过多任务训练实现更多泛化 ‣ 5 实验 ‣ 两阶段 LLM 微调，减少专业化，增强泛化") 节) 结合的示例。1-shot
    提示训练由 Min et al. ([2021](#bib.bib25)) 在多任务训练环境中介绍。
- en: Evaluation.
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估。
- en: After the two-stage fine-tuning, we obtain a fine-tuned model checkpoint and
    a trained soft prompt for a specific fine-tuning target task. We expect the soft
    prompt stores most of the format information, and we only use this prompt during
    inference when the inference task has the same format as the fine-tuned target
    task. Otherwise, we remove the learned prompt and simply feed the original input
    into the fine-tuned model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 经过两阶段的微调，我们获得了一个微调的模型检查点和一个用于特定微调目标任务的训练软提示。我们期望软提示存储大部分格式信息，并且仅在推理任务的格式与微调目标任务相同时使用这个提示。否则，我们会去除学习到的提示，直接将原始输入喂入微调后的模型中。
- en: 5 Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Settings
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Datasets.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: We use RTE (Wang et al., [2019](#bib.bib40); Bentivogli et al., [2009](#bib.bib1))
    and WMT14 En-Fr (Bojar et al., [2014](#bib.bib4)) as two fine-tuning tasks in
    our main experiments. They are selected as examples of classification (RTE) and
    generative tasks (WMT14 En-Fr translation). Experiments on additional fine-tuning
    tasks including SNLI (Bowman et al., [2015](#bib.bib5)) and OpenbookQA (Mihaylov
    et al., [2018](#bib.bib24)) can be found in Appendix [A.3](#A1.SS3 "A.3 Additional
    Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization").
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在主要实验中使用 RTE (Wang et al., [2019](#bib.bib40); Bentivogli et al., [2009](#bib.bib1))
    和 WMT14 En-Fr (Bojar et al., [2014](#bib.bib4)) 作为两个微调任务。它们被选择作为分类 (RTE) 和生成任务
    (WMT14 En-Fr 翻译) 的示例。有关额外微调任务的实验，包括 SNLI (Bowman et al., [2015](#bib.bib5)) 和
    OpenbookQA (Mihaylov et al., [2018](#bib.bib24))，可以在附录 [A.3](#A1.SS3 "A.3 额外实验结果
    ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增强泛化") 中找到。
- en: 'We use 8 tasks unseen during fine-tuning to evaluate the model’s generalization
    abilities. The 8 evaluation tasks are chosen to represent four types of tasks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 8 个在微调期间未见过的任务来评估模型的泛化能力。这 8 个评估任务被选择用来代表四种类型的任务：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Natural language inference: CB (De Marneff et al., [2019](#bib.bib11)) and
    WiC (Pilehvar & Camacho-Collados, [2018](#bib.bib30)) from superGLUE (Wang et al.,
    [2019](#bib.bib40))'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然语言推理：CB (De Marneff et al., [2019](#bib.bib11)) 和 WiC (Pilehvar & Camacho-Collados,
    [2018](#bib.bib30)) 来源于 superGLUE (Wang et al., [2019](#bib.bib40))
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Closed book QA: TriviaQA (Joshi et al., [2017](#bib.bib18)), web_questions
    (Berant et al., [2013](#bib.bib2))'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '闭卷 QA: TriviaQA (Joshi et al., [2017](#bib.bib18)), web_questions (Berant et
    al., [2013](#bib.bib2))'
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Translation: WMT16 En-Ro, WMT16 En-De (Bojar et al., [2016](#bib.bib3))'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 翻译：WMT16 En-Ro，WMT16 En-De (Bojar et al., [2016](#bib.bib3))
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Summarization: XSum (Narayan et al., [2018](#bib.bib27)), WikiLingua (Ladhak
    et al., [2020](#bib.bib19))'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要：XSum (Narayan et al., [2018](#bib.bib27))，WikiLingua (Ladhak et al., [2020](#bib.bib19))
- en: For each evaluation task, we use 1-shot and 4-shots prompts and task templates
    from PaLM (Chowdhery et al., [2022](#bib.bib8)) as described in the Appendix [4](#A1.T4
    "Table 4 ‣ A.2.1 Input Template Used in Experiments ‣ A.2 Experiment Details ‣
    Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More
    Generalization").
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个评估任务，我们使用1-shot和4-shots提示以及来自PaLM (Chowdhery et al., [2022](#bib.bib8))的任务模板，如附录[4](#A1.T4
    "Table 4 ‣ A.2.1 Input Template Used in Experiments ‣ A.2 Experiment Details ‣
    Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More
    Generalization")所述。
- en: Metrics.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指标。
- en: We report accuracy for classification tasks, exact match ratio for QA tasks,
    BLEU score (Papineni et al., [2002](#bib.bib29)) for translation tasks and Rouge-2
    score (Lin, [2004](#bib.bib21)) for summarization tasks. We evaluate the model
    on development set for superGLUE sub-tasks (RTE, CB and WiC) and on test set for
    all other tasks. Besides per-task performance, we also report the normalized average
    (Norm. Avg.) performance on all evaluation tasks by averaging the performances
    normalized to [0,100], following the "normalized preferred metric" in BIG-bench
    (Srivastava et al., [2022](#bib.bib37)) and Chung et al. ([2022](#bib.bib9)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告分类任务的准确率，QA任务的精确匹配比例，翻译任务的BLEU分数 (Papineni et al., [2002](#bib.bib29))和摘要任务的Rouge-2分数 (Lin,
    [2004](#bib.bib21))。我们在superGLUE子任务 (RTE, CB和WiC) 的开发集上评估模型，并在所有其他任务的测试集上评估。除了每任务的性能外，我们还报告了所有评估任务的标准化平均（Norm.
    Avg.）性能，通过将性能标准化到[0,100]并取平均，遵循BIG-bench (Srivastava et al., [2022](#bib.bib37))和Chung
    et al. ([2022](#bib.bib9))中的“标准化优选指标”。
- en: Models.
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型。
- en: We primarily use mT5 (Xue et al., [2020](#bib.bib43)) XXL model (Raffel et al.,
    [2020b](#bib.bib34)) in our main experiments, which is pretrained on multi-lingual
    corpus and contains 13B parameters. This is to accommodate multi-lingual scenarios
    among our training and evaluation tasks. To show the effectiveness of our method
    on different pretraining corpus, model sizes and architectures, we also include
    experiments on mT5 XL, T5.1.1 XXL and PaLM 8b in Appendix [A.3](#A1.SS3 "A.3 Additional
    Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization"). T5 based models are shown to have meaningful
    few-shot performance as shown in Chung et al. ([2022](#bib.bib9)). We do not consider
    FLAN-T5 (Chung et al., [2022](#bib.bib9)) as a base model in our experiments because
    it has already been fine-tuned on a large amount of supervised datasets, including
    our evaluation datasets. More experimental details can be found in Appendix [A.2](#A1.SS2
    "A.2 Experiment Details ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization").
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用mT5 (Xue et al., [2020](#bib.bib43)) XXL模型 (Raffel et al., [2020b](#bib.bib34))进行主要实验，该模型在多语言语料库上进行预训练，并包含13B参数。这是为了适应我们训练和评估任务中的多语言场景。为了展示我们的方法在不同预训练语料库、模型规模和架构上的有效性，我们还包括了对mT5
    XL、T5.1.1 XXL和PaLM 8b的实验，详见附录[A.3](#A1.SS3 "A.3 Additional Experiment Results
    ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization")。T5基础模型显示出有意义的少量样本表现，如Chung et al. ([2022](#bib.bib9))所示。我们没有将FLAN-T5
    (Chung et al., [2022](#bib.bib9))作为实验中的基础模型，因为它已经在大量监督数据集上进行了微调，包括我们的评估数据集。更多实验细节请参见附录[A.2](#A1.SS2
    "A.2 Experiment Details ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization")。
- en: Comparing methods.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 比较方法。
- en: We compare our ProMoT with several different configurations, including
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的ProMoT与几种不同的配置进行比较，包括
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pretrained model: We evaluate the pretrained model on all tasks without any
    fine-tuning.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练模型：我们在所有任务上评估预训练模型，不进行任何微调。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Standard fine-tuning: Fine-tune the pretrained model without trainable prompts.
    We also include a multi-task version in Section [5.4](#S5.SS4 "5.4 More Generalization
    with Multitask Training ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization") which is commonly used to boost model
    generalization on unseen tasks.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准微调：对预训练模型进行微调，不包括可训练的提示。在第[5.4](#S5.SS4 "5.4 More Generalization with Multitask
    Training ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization")节中，我们还包括了一个多任务版本，通常用于提升模型在未见任务上的泛化能力。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt tuning: Tune the trainable prompt with pretrained model frozen. As the
    model is fixed, prompt tuning will not change the pretrained model’s performance
    on in-context learning tasks comparing when the prompt is removed.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示调优：调整可训练的提示，同时冻结预训练模型。由于模型是固定的，提示调优不会改变预训练模型在上下文学习任务上的性能，与提示被移除时的情况进行比较。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Our proposed method: ProMoT: Our proposed two-stage fine-tuning strategy.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的方法：ProMoT：我们提出的两阶段微调策略。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Our proposed method: ProMoT+1-shot: To further boost in-context learning performance,
    we prepend a 1-shot example to the input in Figure [4](#S4.F4 "Figure 4 ‣ 4 Proposed
    Method: PROmpt tuning with MOdel Tuning (ProMoT) ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization") during training.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的方法：ProMoT+1-shot：为了进一步提升上下文学习性能，我们在训练期间将一个 1-shot 示例添加到输入中，如图 [4](#S4.F4
    "图 4 ‣ 4 提出的方法：PROmpt 调优与 MOdel 调优（ProMoT） ‣ 两阶段 LLM 微调，较少专门化，更多泛化") 所示。
- en: 5.2 Supervised Performance on Fine-tuning Tasks
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 微调任务上的监督性能
- en: '[h] Prompt tuning Standard Fine-tuning ProMoT (Ours) RTE 91.34 92.06 92.78
    WMT14 En-Fr 39.28 41.80 41.30 SNLI 88.53 88.91 89.62 OpenbookQA 73.60 77.2 81.6'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[h] 提示调优 标准微调 ProMoT（我们的方法） RTE 91.34 92.06 92.78 WMT14 英法 39.28 41.80 41.30
    SNLI 88.53 88.91 89.62 OpenbookQA 73.60 77.2 81.6'
- en: Comparison of supervised performances of a mT5 XXL model on fine-tuning target
    tasks. We use 0-shot in fine-tuning tasks. We report accuracy for RTE, SNLI and
    OpenbookQA, and BLEU score for WMT14 En-Fr. We first show that ProMoT training
    can achieve similar or even better performance on fine-tuning tasks compared to
    standard fine-tuning. We apply three different fine-tuning methods on four different
    tasks and report the result in Table [5.2](#S5.SS2 "5.2 Supervised Performance
    on Fine-tuning Tasks ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization"). We report the best performance within the same number
    of fine-tuning steps (See Appendix [A.2](#A1.SS2 "A.2 Experiment Details ‣ Appendix
    A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")
    for more details). ProMoT outperforms standard fine-tuning on supervised performance
    on 3 out of 4 fine-tuning target tasks and outperforms prompt-tuning on 4 out
    of 4 tasks. Therefore the improved in-context learning performance on unseen tasks
    (better generalization ability), as will be demonstrated in the next few sections,
    comes without sacrificing the fine-tune task’s performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对 mT5 XXL 模型在微调目标任务上的监督性能进行比较。我们在微调任务中使用 0-shot。我们报告了 RTE、SNLI 和 OpenbookQA
    的准确率，以及 WMT14 英法的 BLEU 分数。我们首先展示了 ProMoT 训练在微调任务上可以实现类似甚至更好的性能，相较于标准微调。我们在四个不同任务上应用了三种不同的微调方法，并在表
    [5.2](#S5.SS2 "5.2 微调任务上的监督性能 ‣ 5 实验 ‣ 两阶段 LLM 微调，较少专门化，更多泛化") 中报告了结果。我们报告了相同微调步骤数量下的最佳性能（详见附录
    [A.2](#A1.SS2 "A.2 实验细节 ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，较少专门化，更多泛化")）。ProMoT 在 4 个微调目标任务中的
    3 个任务上超越了标准微调，并在 4 个任务中超越了提示调优。因此，如接下来几节所示，改进的上下文学习性能（更好的泛化能力）是在不牺牲微调任务性能的情况下实现的。
- en: '[h]'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[h]'
- en: Pretrained Standard Fine-tuning ProMoT (Ours) ProMoT + 1-shot (Ours) Fine-tuning
    RTE 47.653 92.06 92.78 93.86 1-shot 4-shots 1-shot 4-shots 1-shot 4-shots 1-shot
    4-shots Norm. Avg. 17.52 18.75 15.43 16.56 20.10 21.24 22.26 22.33 (-2.10) (-2.19)
    (+2.58) (+2.49) (+4.74) (+3.58) CB 46.43 51.79 73.21 82.14 66.07 67.86 83.93 82.14
    WiC 49.69 49.69 50.00 50.16 51.41 53.61 51.25 50.63 Evaluation triviaQA 17.58
    19.02 0.15 0.11 17.64 18.66 17.82 19.62 web_questions 9.70 13.04 0.05 0.05 11.07
    13.19 10.14 12.11 WMT16_ende 3.97 8.83 0.00 0.00 2.02 3.69 2.26 4.89 WMT16_enro
    1.82 3.92 0.00 0.00 0.70 0.96 0.87 1.87 XSum 6.41 2.35 0.00 0.00 7.02 7.01 6.94
    3.93 WikiLingua/en 4.59 1.33 0.00 0.00 4.84 4.90 4.87 3.43
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练 标准微调 ProMoT（我们的方法） ProMoT + 1-shot（我们的方法） 微调 RTE 47.653 92.06 92.78 93.86
    1-shot 4-shots 1-shot 4-shots 1-shot 4-shots 1-shot 4-shots 归一化 平均 17.52 18.75
    15.43 16.56 20.10 21.24 22.26 22.33 (-2.10) (-2.19) (+2.58) (+2.49) (+4.74) (+3.58)
    CB 46.43 51.79 73.21 82.14 66.07 67.86 83.93 82.14 WiC 49.69 49.69 50.00 50.16
    51.41 53.61 51.25 50.63 评估 triviaQA 17.58 19.02 0.15 0.11 17.64 18.66 17.82 19.62
    web_questions 9.70 13.04 0.05 0.05 11.07 13.19 10.14 12.11 WMT16_ende 3.97 8.83
    0.00 0.00 2.02 3.69 2.26 4.89 WMT16_enro 1.82 3.92 0.00 0.00 0.70 0.96 0.87 1.87
    XSum 6.41 2.35 0.00 0.00 7.02 7.01 6.94 3.93 WikiLingua/en 4.59 1.33 0.00 0.00
    4.84 4.90 4.87 3.43
- en: Performances of a mT5 XXL model finetuned on RTE and evaluated on 8 different
    tasks to verify the generalization ability. The accuracy on fine-tuned task (RTE)
    is in the first row. We compare the Norm. Avg. (normalized average performance)
    with pretrained model and report the relative difference, where red denotes decreased
    performance and blue denotes increased performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 微调在RTE上的mT5 XXL模型，并在8个不同任务上进行评估以验证其泛化能力。微调任务（RTE）上的准确率在第一行。我们比较了与预训练模型的归一化平均性能（Norm.
    Avg.），并报告了相对差异，其中红色表示性能下降，蓝色表示性能提升。
- en: 5.3 Generalization with Single Task Fine-tuning
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 单任务微调的泛化
- en: In this section, we evaluate and compare the few-shot performance on unseen
    tasks after fine-tuning. We show the evaluation results of fine-tuning on RTE
    and WMT14 En-Fr in Table [5.2](#S5.SS2 "5.2 Supervised Performance on Fine-tuning
    Tasks ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization") and Table [5.3](#S5.SS3 "5.3 Generalization with Single
    Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization"), respectively. Experiments on additional fine-tuning
    tasks SNLI/OpenbookQA and additional base models including mT5 XL, T5.1.1 XXL
    and PaLM 8b can be found in Appendix [A.3](#A1.SS3 "A.3 Additional Experiment
    Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization").
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估并比较了在微调后对未见任务的少样本性能。我们在表格[5.2](#S5.SS2 "5.2 监督性能在微调任务 ‣ 5 实验 ‣ 两阶段LLM微调，减少专业化，增加泛化")和表格[5.3](#S5.SS3
    "5.3 单任务微调的泛化 ‣ 5 实验 ‣ 两阶段LLM微调，减少专业化，增加泛化")中分别展示了在RTE和WMT14 En-Fr上的微调评估结果。有关额外微调任务SNLI/OpenbookQA和其他基础模型（包括mT5
    XL、T5.1.1 XXL和PaLM 8b）的实验可以在附录[A.3](#A1.SS3 "A.3 额外实验结果 ‣ 附录A 附录 ‣ 两阶段LLM微调，减少专业化，增加泛化")中找到。
- en: '[!h] Pretrained Standard Fine-tuning ProMoT (Ours) ProMoT + 1-shot (Ours) Fine-tuning
    WMT14 En-Fr 1.98 41.80 41.30 41.19 1-shot 4-shots 1-shot 4-shots 1-shot 4-shots
    1-shot 4-shots Norm. Avg. 17.52 18.75 9.15 11.67 18.87 20.64 19.91 21.99 (-8.37)
    (-7.07) (+1.35) (+1.89) (+2.39) (+3.24) CB 46.43 51.79 16.07 32.14 41.07 57.14
    41.07 53.57 WiC 49.69 49.69 50.63 49.06 50.16 50.31 49.84 50.63 Evaluation triviaQA
    17.58 19.02 3.20 3.15 13.63 15.20 16.93 18.19 web_questions 9.70 13.04 0.89 6.15
    9.40 7.92 10.14 12.01 WMT16_ende 3.97 8.83 0.81 0.18 15.52 15.55 16.14 15.63 WMT16_enro
    1.82 3.92 1.53 0.42 18.54 17.80 17.57 16.81 XSum 6.41 2.35 0.05 1.86 1.49 0.65
    3.41 4.36 WikiLingua/en 4.59 1.33 0.03 0.43 1.14 0.52 4.22 4.73 Performances of
    a mT5 XXL model finetuned on WMT14 En-Fr and evaluated on 8 few-shot tasks to
    verify the generalization ability. BLEU on the fine-tuned task is in the first
    row. We compare the Norm. Avg. (normalized average performance) with pretrained
    model and report the relative difference, where red denotes decreased performance
    and blue denotes increased performance.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[!h] 预训练 标准 微调 ProMoT（我们的） ProMoT + 1-shot（我们的） 微调 WMT14 En-Fr 1.98 41.80 41.30
    41.19 1-shot 4-shots 1-shot 4-shots 1-shot 4-shots 1-shot 4-shots 归一化平均 17.52
    18.75 9.15 11.67 18.87 20.64 19.91 21.99 (-8.37) (-7.07) (+1.35) (+1.89) (+2.39)
    (+3.24) CB 46.43 51.79 16.07 32.14 41.07 57.14 41.07 53.57 WiC 49.69 49.69 50.63
    49.06 50.16 50.31 49.84 50.63 评估 triviaQA 17.58 19.02 3.20 3.15 13.63 15.20 16.93
    18.19 web_questions 9.70 13.04 0.89 6.15 9.40 7.92 10.14 12.01 WMT16_ende 3.97
    8.83 0.81 0.18 15.52 15.55 16.14 15.63 WMT16_enro 1.82 3.92 1.53 0.42 18.54 17.80
    17.57 16.81 XSum 6.41 2.35 0.05 1.86 1.49 0.65 3.41 4.36 WikiLingua/en 4.59 1.33
    0.03 0.43 1.14 0.52 4.22 4.73 微调在WMT14 En-Fr上的mT5 XXL模型，并在8个少样本任务上进行评估以验证其泛化能力。微调任务（RTE）上的BLEU在第一行。我们比较了与预训练模型的归一化平均性能（Norm.
    Avg.），并报告了相对差异，其中红色表示性能下降，蓝色表示性能提升。'
- en: From both tables, we first observe that the model’s in-context learning performance
    drops significantly after standard fine-tuning. In particular, the few-shot learning
    performances drop to near zero for 6 over 8 tasks in Table [5.2](#S5.SS2 "5.2
    Supervised Performance on Fine-tuning Tasks ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization"), with the only exceptions being
    CB and WiC where they share the same format (binary classification) as the RTE
    fine-tuning task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两张表中，我们首先观察到，经过标准微调后，模型的上下文学习性能显著下降。特别是在表格[5.2](#S5.SS2 "5.2 监督性能在微调任务 ‣ 5
    实验 ‣ 两阶段LLM微调，减少专业化，增加泛化")中，8个任务中有6个的少样本学习性能几乎降到零，唯一的例外是CB和WiC，因为它们与RTE微调任务共享相同的格式（二分类）。
- en: On the contrary, ProMoT reduces the loss of the in-context learning performance
    on unseen few-shot evaluation tasks, and even boosts some evaluation tasks that
    are semantically related to the fine-tuning task but with totally different task
    formats, resulting in an increasing in-context learning performance on average.
    In Table [5.2](#S5.SS2 "5.2 Supervised Performance on Fine-tuning Tasks ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    ProMoT on the binary NLI dataset dataset consistently improves few-shot performances
    on two summarization tasks beyond the pretrained model. In Table [5.3](#S5.SS3
    "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization"), ProMoT training
    on English-French translation substantially improves few-shot performance on other
    language translation pairs such as English to German and Romanian. This cross-task
    generalization across different task formats are infeasible with previous fine-tuning
    techniques. Text examples from standard fine-tuning and ProMoT can be found in
    Appendix [A.3.8](#A1.SS3.SSS8 "A.3.8 Qualitative results on fine-tuning WMT14
    En-Fr task ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization"). The improvement
    with less specialization and more generalization can be further boosted when we
    combine ProMoT with 1-shot prompt to incorporate in-context learning objective
    during fine-tuning.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，ProMoT 减少了在未见的少样本评估任务中的上下文学习性能损失，甚至提升了一些与微调任务语义相关但任务格式完全不同的评估任务的性能，从而平均提高了上下文学习的表现。在表格[5.2](#S5.SS2
    "5.2 细化任务的监督性能 ‣ 5 实验 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中，ProMoT 在二分类 NLI 数据集上的表现持续超越预训练模型，在两个总结任务中的少样本表现得到了改善。在表格[5.3](#S5.SS3
    "5.3 单任务微调的泛化 ‣ 5 实验 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中，ProMoT 在英语-法语翻译任务上的训练显著提高了其他语言翻译对（如英语到德语和罗马尼亚语）的少样本表现。这种跨任务格式的泛化在以往的微调技术中是不可行的。标准微调和
    ProMoT 的文本示例可以在附录[A.3.8](#A1.SS3.SSS8 "A.3.8 微调 WMT14 En-Fr 任务的定性结果 ‣ A.3 额外实验结果
    ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中找到。当我们将 ProMoT 与 1-shot 提示结合使用，以在微调过程中纳入上下文学习目标时，减少专业化、增加泛化的改进效果可以进一步提升。
- en: It is however not surprising that even ProMoT cannot completely eliminate specialization
    and may still negatively influence some unseen in-context learning tasks compared
    to the pretrained model, depending on the characteristics of the fine-tuning task.
    In the next section, we show that a multi-task setup further improves the already
    strong generalization of ProMoT.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管如此，ProMoT 仍无法完全消除专业化，并且可能会根据微调任务的特征对一些未见的上下文学习任务产生负面影响，这一点也并不令人惊讶。在下一节中，我们将展示多任务设置进一步提高了
    ProMoT 已经很强的泛化能力。
- en: 5.4 More Generalization with Multitask Training
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 通过多任务训练实现更多泛化
- en: Multi-task training is commonly used to improve model’s generalization ability
    (Wei et al., [2021b](#bib.bib42); Chung et al., [2022](#bib.bib9)). As a general
    fine-tuning framework, ProMoT can be combined with multi-tasking and achieves
    better generalization compared to standard multi-task fine-tuning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务训练通常用于提高模型的泛化能力（Wei et al., [2021b](#bib.bib42); Chung et al., [2022](#bib.bib9)）。作为一种通用微调框架，ProMoT
    可以与多任务结合，且相比于标准多任务微调，能够实现更好的泛化效果。
- en: Pretrained multi-task FT Multi ProMoT multi-task FT Multi-ProMoT + 1-shot +
    1-shot Multi-task RTE 47.65 90.25 91.34 91.70 93.14 Fine-tuning WMT14 En-Fr 1.982
    41.34 40.73 40.87 40.55 Norm. Avg. 17.52 20.06 25.88 22.62 26.17 (+2.54) (+8.35)
    (+5.10) (+8.65) CB 46.43 80.36 83.93 87.50 85.71 WiC 49.69 51.10 51.41 53.29 52.04
    Evaluation TriviaQA 17.58 15.76 16.99 16.53 17.18 Web_questions 9.70 9.70 10.04
    9.40 10.38 WMT16 En-De 3.97 0.88 18.83 2.50 17.57 WMT16 En-Ro 1.82 1.52 18.41
    5.62 18.57 XSum 6.41 0.44 4.50 1.82 4.32 WikiLingua/en 4.59 0.72 2.89 4.33 3.56
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练 多任务 FT Multi ProMoT 多任务 FT Multi-ProMoT + 1-shot + 1-shot 多任务 RTE 47.65
    90.25 91.34 91.70 93.14 微调 WMT14 En-Fr 1.982 41.34 40.73 40.87 40.55 标准化平均 17.52
    20.06 25.88 22.62 26.17 (+2.54) (+8.35) (+5.10) (+8.65) CB 46.43 80.36 83.93 87.50
    85.71 WiC 49.69 51.10 51.41 53.29 52.04 评估 TriviaQA 17.58 15.76 16.99 16.53 17.18
    Web_questions 9.70 9.70 10.04 9.40 10.38 WMT16 En-De 3.97 0.88 18.83 2.50 17.57
    WMT16 En-Ro 1.82 1.52 18.41 5.62 18.57 XSum 6.41 0.44 4.50 1.82 4.32 WikiLingua/en
    4.59 0.72 2.89 4.33 3.56
- en: 'Table 2: Comparison of multi-task training on a mixed dataset of RTE and WMT14
    En-Fr. We compare the evaluation results of pretrained mT5 model, standard multi-task
    fine-tuning (FT) and multitask (Multi) ProMoT training. We compare the Norm. Avg.
    (normalized average performance) with pretrained model and report the relative
    difference, where blue denotes increased performance.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在混合 RTE 和 WMT14 En-Fr 数据集上的多任务训练比较。我们比较了预训练的 mT5 模型、标准多任务微调（FT）和多任务（Multi）ProMoT
    训练的评估结果。我们比较了与预训练模型的标准化平均性能（Norm. Avg.），并报告了相对差异，其中蓝色表示性能提升。
- en: We apply multi-task ProMoT training on mixed RTE and WMT14 En-Fr translation
    dataset. At the prompt-tuning stage, we train a soft prompt for each task. At
    the fine-tuning stage, we mix different tasks and prepend the corresponding soft
    task prompt to each training example. We keep other configurations the same as
    Section [5.3](#S5.SS3 "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")
    and report the results in Table [2](#S5.T2 "Table 2 ‣ 5.4 More Generalization
    with Multitask Training ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization"). We compare multi-task ProMoT with standard
    multi-task fine-tuning. The results show that Multi-task ProMoT significantly
    outperforms standard multi-task fine-tuning on enhancing generalization with larger
    improvement on average on unseen 1-shot evaluation tasks. Similar to the single
    task setting, adding 1-shot prompt before each training input in the fine-tuning
    stage further boosts the performance of both multi-task fine-tuning and multi-task
    ProMoT.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在混合的 RTE 和 WMT14 En-Fr 翻译数据集上应用了多任务 ProMoT 训练。在提示调优阶段，我们为每个任务训练一个软提示。在微调阶段，我们混合不同的任务，并将相应的软任务提示添加到每个训练示例之前。我们保持其他配置与[5.3](#S5.SS3
    "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization")节相同，并在表[2](#S5.T2
    "Table 2 ‣ 5.4 More Generalization with Multitask Training ‣ 5 Experiments ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization")中报告结果。我们将多任务
    ProMoT 与标准多任务微调进行了比较。结果表明，多任务 ProMoT 在增强泛化能力方面显著优于标准多任务微调，且在未见过的1-shot评估任务上有更大的平均提升。类似于单任务设置，在微调阶段，在每个训练输入之前添加
    1-shot 提示进一步提升了多任务微调和多任务 ProMoT 的性能。
- en: 5.5 Ablation Study
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 消融研究
- en: Joint Fine-tuning Fine-tuning ProMoT Fine-tuning + 1-shot with random prompt
    + 1-shot (Ours) Fine-tuning RTE 90.97 90.97 92.06 93.86 CB 83.93 78.57 83.93 83.93
    WiC 50.47 51.41 51.72 51.25 TriviaQA 0.75 0.03 0.83 17.82 Evaluation web_questions
    0.64 0.00 0.30 10.14 WMT16_ende 0.00 0.00 0.00 2.26 WMT16_enro 0.00 0.00 0.00
    0.87 XSum 0.00 0.00 0.00 6.94 WikiLingua/en 0.00 0.00 0.00 4.87
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 联合微调 微调 ProMoT 微调 + 1-shot 随机提示 + 1-shot（我们的方法） 微调 RTE 90.97 90.97 92.06 93.86
    CB 83.93 78.57 83.93 83.93 WiC 50.47 51.41 51.72 51.25 TriviaQA 0.75 0.03 0.83
    17.82 评估 web_questions 0.64 0.00 0.30 10.14 WMT16_ende 0.00 0.00 0.00 2.26 WMT16_enro
    0.00 0.00 0.00 0.87 XSum 0.00 0.00 0.00 6.94 WikiLingua/en 0.00 0.00 0.00 4.87
- en: 'Table 3: The ablation study results. Joint fine-tuning: fine-tuning the soft
    prompt and the main model together. Fine-tuning + 1-shot: standard fine-tuning
    with a 1-shot natural language prompt attached to every input sequence. Fine-tuning
    with random prompt: fine-tuning with a fixed soft prompt randomly initialized
    with uniform distribution. ProMoT + 1-shot: ProMoT is applied with an attached
    1-shot natural language prompt before each training input.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：消融研究结果。联合微调：同时微调软提示和主模型。微调 + 1-shot：标准微调，附加了 1-shot 自然语言提示到每个输入序列。随机提示的微调：使用均匀分布随机初始化的固定软提示进行微调。ProMoT
    + 1-shot：ProMoT 在每个训练输入之前附加 1-shot 自然语言提示进行应用。
- en: We conduct several ablation studies in Table [3](#S5.T3 "Table 3 ‣ 5.5 Ablation
    Study ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization"). First, instead of fine-tuning in a two-stage process, we
    consider “jointly fine-tuning” both the soft prompt and the model parameters in
    one stage. As shown in Table [3](#S5.T3 "Table 3 ‣ 5.5 Ablation Study ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    this method still results in specialization and severe loss of in-context learning
    abilities. Thus the benefit of ProMoT comes from its two-stage nature instead
    of merely adding more learnable parameters (soft prompt). In addition, fine-tuning
    the models with a fixed random soft prompt does not help - as it does not help
    to remove format specialization. Another important baseline is to fine-tune the
    model with natural language prompts in place instead of soft prompts, which also
    capture the format to some extend. In a 1-shot scenario, this approach is still
    far worse compared to ProMoT, showing that learned soft prompts work better than
    natural language prompts in reducing format specialization in fine-tuning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [3](#S5.T3 "Table 3 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization") 中进行了一些消融研究。首先，我们考虑在一个阶段中“联合微调”软提示和模型参数，而不是在两个阶段中微调。如表
    [3](#S5.T3 "Table 3 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization") 所示，这种方法仍然会导致专门化和严重丧失上下文学习能力。因此，ProMoT
    的优势来自于其两阶段的特性，而不仅仅是增加更多可学习的参数（软提示）。此外，使用固定随机软提示微调模型并没有帮助——因为这不能去除格式专门化。另一个重要的基线是用自然语言提示而不是软提示来微调模型，这也在一定程度上捕捉格式。在
    1-shot 情景下，这种方法相比 ProMoT 仍然差距很大，表明学习的软提示在减少格式专门化方面比自然语言提示更有效。
- en: 6 Conclusions
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we identify format specialization as one important cause of the
    loss of general in-context learning abilities during LLM fine-tuning, which tends
    to happen at the beginning of fine-tuning. We are motivated to develop ProMoT,
    a simple yet effective two-stage fine-tuning framework that utilizes soft trainable
    prompts to absorb task-specific formats before model fine-tuning. Experiments
    on a diverse set of NLP tasks show that ProMoT reduces format specialization and
    results in surprising generalization across very different tasks, making it a
    promising method to build general-purpose capabilities into LLMs with small fine-tuning
    datasets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们确定格式专门化是 LLM 微调过程中丧失通用上下文学习能力的一个重要原因，这种情况往往发生在微调的开始阶段。我们受到启发开发了 ProMoT，一个简单而有效的两阶段微调框架，利用可训练的软提示在模型微调前吸收任务特定的格式。在各种
    NLP 任务上的实验表明，ProMoT 减少了格式专门化，并在非常不同的任务上表现出令人惊讶的泛化能力，使其成为将通用能力构建到 LLM 中的有前途的方法，尤其是在微调数据集较小的情况下。
- en: Limitations
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制
- en: We have shown the effectiveness of ProMoT in our main paper, however there is
    no theoretical guarantee on how much format specialization can be absorbed by
    the soft prompt during the first stage of ProMoT. Besides, our experiments are
    done with models smaller than 15B due to limited computation resources. It can
    be interesting to test ProMoT on larger models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在主论文中展示了 ProMoT 的有效性，但对于在 ProMoT 第一阶段中，软提示能吸收多少格式专门化并没有理论保证。此外，由于计算资源有限，我们的实验使用了小于
    15B 的模型。在更大模型上测试 ProMoT 将是一个有趣的方向。
- en: References
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bentivogli et al. (2009) Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
    Giampiccolo. The fifth pascal recognizing textual entailment challenge. In *TAC*,
    2009.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bentivogli 等人 (2009) Luisa Bentivogli, Peter Clark, Ido Dagan 和 Danilo Giampiccolo。第五届
    Pascal 识别文本蕴含挑战。在 *TAC*，2009。
- en: Berant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
    Semantic parsing on freebase from question-answer pairs. In *Proceedings of the
    2013 conference on empirical methods in natural language processing*, pp.  1533–1544,
    2013.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berant 等人 (2013) Jonathan Berant, Andrew Chou, Roy Frostig 和 Percy Liang。在《2013年自然语言处理经验方法会议论文集》中从问答对进行
    Freebase 的语义解析，pp. 1533–1544, 2013。
- en: Bojar et al. (2016) Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette
    Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara
    Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin
    Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi,
    Karin Verspoor, and Marcos Zampieri. Findings of the 2016 conference on machine
    translation. In *Proceedings of the First Conference on Machine Translation*,
    pp.  131–198, Berlin, Germany, August 2016\. Association for Computational Linguistics.
    URL [http://www.aclweb.org/anthology/W/W16/W16-2301](http://www.aclweb.org/anthology/W/W16/W16-2301).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojar 等人（2016）Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham,
    Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva,
    Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Matt
    Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor,
    和 Marcos Zampieri。2016年机器翻译会议的发现。见 *第一届机器翻译会议论文集*，第 131–198 页，德国柏林，2016年8月。计算语言学协会。网址
    [http://www.aclweb.org/anthology/W/W16/W16-2301](http://www.aclweb.org/anthology/W/W16/W16-2301)。
- en: Bojar et al. (2014) Ondrej Bojar, Christian Buck, Christian Federmann, Barry
    Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post,
    Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale s Tamchyna. Findings of
    the 2014 workshop on statistical machine translation. In *Proceedings of the Ninth
    Workshop on Statistical Machine Translation*, pp.  12–58, Baltimore, Maryland,
    USA, June 2014\. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/W/W14/W14-3302](http://www.aclweb.org/anthology/W/W14/W14-3302).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojar 等人（2014）Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow,
    Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
    Saint-Amand, Radu Soricut, Lucia Specia, 和 Aleš Tamchyna。2014年统计机器翻译研讨会的发现。见 *第九届统计机器翻译研讨会论文集*，第
    12–58 页，美国马里兰州巴尔的摩，2014年6月。计算语言学协会。网址 [http://www.aclweb.org/anthology/W/W14/W14-3302](http://www.aclweb.org/anthology/W/W14/W14-3302)。
- en: Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. A large annotated corpus for learning natural language
    inference. In *Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing (EMNLP)*. Association for Computational Linguistics, 2015.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman 等人（2015）Samuel R. Bowman, Gabor Angeli, Christopher Potts 和 Christopher
    D. Manning。用于学习自然语言推理的大型标注语料库。见 *2015年自然语言处理实证方法会议论文集*。计算语言学协会，2015。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in NeurIPS*, volume 33, pp.  1877–1901\.
    Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少量学习者。见 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan 和 H. Lin（编），*NeurIPS
    进展*，第 33 卷，第 1877–1901 页。Curran Associates, Inc., 2020。网址 [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。
- en: Chan et al. (2022) Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X.
    Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data
    distributional properties drive emergent in-context learning in transformers,
    2022. URL [https://arxiv.org/abs/2205.05055](https://arxiv.org/abs/2205.05055).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等人（2022）Stephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X.
    Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland 和 Felix Hill。数据分布特性驱动变换器中的涌现式上下文学习，2022。网址
    [https://arxiv.org/abs/2205.05055](https://arxiv.org/abs/2205.05055)。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling
    with pathways, 2022. URL [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery等（2022）Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann、Parker
    Schuh、Kensen Shi、Sasha Tsvyashchenko、Joshua Maynez、Abhishek Rao、Parker Barnes、Yi
    Tay、Noam Shazeer、Vinodkumar Prabhakaran、Emily Reif、Nan Du、Ben Hutchinson、Reiner
    Pope、James Bradbury、Jacob Austin、Michael Isard、Guy Gur-Ari、Pengcheng Yin、Toju
    Duke、Anselm Levskaya、Sanjay Ghemawat、Sunipa Dev、Henryk Michalewski、Xavier Garcia、Vedant
    Misra、Kevin Robinson、Liam Fedus、Denny Zhou、Daphne Ippolito、David Luan、Hyeontaek
    Lim、Barret Zoph、Alexander Spiridonov、Ryan Sepassi、David Dohan、Shivani Agrawal、Mark
    Omernick、Andrew M. Dai、Thanumalayan Sankaranarayana Pillai、Marie Pellat、Aitor
    Lewkowycz、Erica Moreira、Rewon Child、Oleksandr Polozov、Katherine Lee、Zongwei Zhou、Xuezhi
    Wang、Brennan Saeta、Mark Diaz、Orhan Firat、Michele Catasta、Jason Wei、Kathy Meier-Hellstern、Douglas
    Eck、Jeff Dean、Slav Petrov和Noah Fiedel。Palm：通过路径扩展语言建模，2022年。网址 [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)。
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等（2022）Hyung Won Chung、Le Hou、Shayne Longpre、Barret Zoph、Yi Tay、William
    Fedus、Eric Li、Xuezhi Wang、Mostafa Dehghani、Siddhartha Brahma等。扩展指令微调语言模型。*arXiv预印本
    arXiv:2210.11416*，2022年。
- en: Creswell & Shanahan (2022) Antonia Creswell and Murray Shanahan. Faithful reasoning
    using large language models, 2022. URL [https://arxiv.org/abs/2208.14271](https://arxiv.org/abs/2208.14271).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Creswell & Shanahan（2022）Antonia Creswell和Murray Shanahan。使用大型语言模型进行忠实推理，2022年。网址
    [https://arxiv.org/abs/2208.14271](https://arxiv.org/abs/2208.14271)。
- en: 'De Marneff et al. (2019) Marie-Catherine De Marneff, Mandy Simons, and Judith
    Tonhauser. The commitmentbank: Investigating projection in naturally occurring
    discourse. *proceedings of Sinn und Bedeutung 23*, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Marneff等（2019）Marie-Catherine De Marneff、Mandy Simons和Judith Tonhauser。承诺库：研究自然发生话语中的投射。*Sinn
    und Bedeutung 23会议论文集*，2019年。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of NAACL*, pp.  4171–4186, 2019. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee和Kristina Toutanova。BERT：深度双向变换器的预训练用于语言理解。见于*NAACL会议论文集*，第4171–4186页，2019年。网址
    [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)。
- en: Gao et al. (2020) Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained
    language models better few-shot learners. *arXiv preprint arXiv:2012.15723*, 2020.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等（2020）Tianyu Gao、Adam Fisch和Danqi Chen。让预训练语言模型更好的少样本学习。*arXiv预印本 arXiv:2012.15723*，2020年。
- en: He et al. (2021) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. Towards a unified view of parameter-efficient transfer learning.
    *arXiv preprint arXiv:2110.04366*, 2021.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2021）Junxian He、Chunting Zhou、Xuezhe Ma、Taylor Berg-Kirkpatrick和Graham Neubig。朝向统一的参数高效迁移学习视角。*arXiv预印本
    arXiv:2110.04366*，2021年。
- en: 'He et al. (2022) Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi,
    Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, et al. Hyperprompt: Prompt-based
    task-conditioning of transformers. In *International Conference on Machine Learning*,
    pp. 8678–8690\. PMLR, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等（2022）Yun He、Steven Zheng、Yi Tay、Jai Gupta、Yu Du、Vamsi Aribandi、Zhe Zhao、YaGuang
    Li、Zhao Chen、Donald Metzler等。Hyperprompt：基于提示的任务条件化变换器。见于*国际机器学习会议*，第8678–8690页。PMLR，2022年。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International Conference
    on Machine Learning*, pp. 2790–2799\. PMLR, 2019.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等（2019）尼尔·霍尔斯比、安德烈·吉尔久、斯坦尼斯瓦夫·雅斯特热布斯基、布鲁娜·莫隆、昆汀·德·拉鲁西尔、安德烈亚·盖斯蒙多、莫娜·阿塔里扬和西尔万·盖利。《NLP的参数高效迁移学习》。在
    *国际机器学习会议*，第 2790–2799 页。PMLR，2019年。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）爱德华·J·胡、叶龙·申、菲利普·沃利斯、泽远·艾伦-朱、袁志·李、肖恩·王、陆·王和魏竹·陈。《Lora：大规模语言模型的低秩适应》。*arXiv
    预印本 arXiv:2106.09685*，2021年。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    *arXiv preprint arXiv:1705.03551*, 2017.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi 等（2017）曼达尔·乔希、恩索尔·崔、丹尼尔·S·韦尔德和卢克·泽特尔莫耶。《Triviaqa：一个大规模远程监督挑战数据集，用于阅读理解》。*arXiv
    预印本 arXiv:1705.03551*，2017年。
- en: 'Ladhak et al. (2020) Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen
    McKeown. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization.
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pp. 
    4034–4048, Online, November 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.findings-emnlp.360. URL [https://aclanthology.org/2020.findings-emnlp.360](https://aclanthology.org/2020.findings-emnlp.360).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ladhak 等（2020）费萨尔·拉达克、埃辛·杜尔穆斯、克莱尔·卡尔迪和凯瑟琳·麦克基翁。《WikiLingua：用于跨语言抽象总结的新基准数据集》。在
    *计算语言学协会会议：EMNLP 2020*，第 4034–4048 页，在线，2020年11月。计算语言学协会。doi: 10.18653/v1/2020.findings-emnlp.360。网址
    [https://aclanthology.org/2020.findings-emnlp.360](https://aclanthology.org/2020.findings-emnlp.360)。'
- en: 'Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pp.  3045–3059, Online and
    Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lester 等（2021）布赖恩·莱斯特、拉米·阿尔-鲁夫和诺亚·康斯坦特。《规模的力量对参数高效的提示调整》。在 *2021年自然语言处理实证方法会议论文集*，第
    3045–3059 页，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi: 10.18653/v1/2021.emnlp-main.243。网址
    [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243)。'
- en: 'Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pp.  74–81, 2004.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）林钦耀。《Rouge：自动评估摘要的工具包》。在 *文本摘要的分支*，第 74–81 页，2004年。
- en: 'Liu et al. (2021) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang,
    and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally
    across scales and tasks. *arXiv preprint arXiv:2110.07602*, 2021.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）肖·刘、开轩·季、逸成·付、正霄·杜、志林·杨和杰·唐。《P-tuning v2：提示调整可以在不同规模和任务中与微调相媲美》。*arXiv
    预印本 arXiv:2110.07602*，2021年。
- en: 'Maynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan
    McDonald. On faithfulness and factuality in abstractive summarization. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pp.  1906–1919, Online, July 2020. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.acl-main.173. URL [https://aclanthology.org/2020.acl-main.173](https://aclanthology.org/2020.acl-main.173).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maynez 等（2020）约书亚·梅内兹、沙希·纳拉扬、伯恩德·博内特和瑞安·麦克唐纳。《关于抽象总结中的忠实性和事实性》。在 *计算语言学协会第58届年会论文集*，第
    1906–1919 页，在线，2020年7月。计算语言学协会。doi: 10.18653/v1/2020.acl-main.173。网址 [https://aclanthology.org/2020.acl-main.173](https://aclanthology.org/2020.acl-main.173)。'
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *EMNLP*, 2018.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等（2018）托多尔·米哈伊洛夫、彼得·克拉克、图沙尔·科特和阿希什·萨巴瓦尔。《盔甲能导电吗？一个新的开放书籍问答数据集》。在 *EMNLP*，2018年。
- en: 'Min et al. (2021) Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
    Metaicl: Learning to learn in context. *arXiv preprint arXiv:2110.15943*, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等（2021）苏雯·敏、迈克·刘易斯、卢克·泽特尔莫耶和哈娜赫·哈吉希尔兹。《Metaicl：在上下文中学习学习》。*arXiv 预印本 arXiv:2110.15943*，2021年。
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi 和 Luke Zettlemoyer. 重新思考示例的作用：是什么使得上下文学习有效？*arXiv 预印本 arXiv:2202.12837*,
    2022年。
- en: Narayan et al. (2018) Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t
    give me the details, just the summary! topic-aware convolutional neural networks
    for extreme summarization. *arXiv preprint arXiv:1808.08745*, 2018.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan et al. (2018) Shashi Narayan, Shay B Cohen 和 Mirella Lapata. 不要给我细节，只要总结！主题感知卷积神经网络用于极端摘要.
    *arXiv 预印本 arXiv:1808.08745*, 2018年。
- en: Noorbakhsh et al. (2021) Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol
    Roy, and Pooyan Jamshidi. Pretrained language models are symbolic mathematics
    solvers too! *arXiv preprint arXiv:2110.03501*, 2021.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noorbakhsh et al. (2021) Kimia Noorbakhsh, Modar Sulaiman, Mahdi Sharifi, Kallol
    Roy 和 Pooyan Jamshidi. 预训练语言模型也是符号数学求解器！*arXiv 预印本 arXiv:2110.03501*, 2021年。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th Annual Meeting of the Association for Computational Linguistics*,
    pp.  311–318, Philadelphia, Pennsylvania, USA, July 2002\. Association for Computational
    Linguistics. doi: 10.3115/1073083.1073135. URL [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing
    Zhu. Bleu: 一种自动评估机器翻译的方法. 见 *第40届计算语言学协会年会论文集*, 页码 311–318, 费城, 宾夕法尼亚州, 美国, 2002年7月.
    计算语言学协会. doi: 10.3115/1073083.1073135. 网址 [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040)。'
- en: 'Pilehvar & Camacho-Collados (2018) Mohammad Taher Pilehvar and José Camacho-Collados.
    Wic: 10,000 example pairs for evaluating context-sensitive representations. *arXiv
    preprint arXiv:1808.09121*, 6, 2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pilehvar & Camacho-Collados (2018) Mohammad Taher Pilehvar 和 José Camacho-Collados.
    Wic: 10,000 对示例用于评估上下文敏感的表示. *arXiv 预印本 arXiv:1808.09121*, 2018年6月。'
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    等人. 通过生成预训练改进语言理解. 2018年。
- en: 'Rae et al. (2021) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
    George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia
    Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John
    Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant
    Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela
    Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida
    Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
    Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault
    Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson
    d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,
    Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson,
    Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon
    Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway,
    Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling
    language models: Methods, analysis & insights from training gopher, 2021. URL
    [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae et al. (2021) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
    George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia
    Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John
    Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant
    Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela
    Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida
    Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
    Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault
    Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de
    Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
    Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson,
    Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon
    Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway,
    Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu 和 Geoffrey Irving. 扩展语言模型：方法、分析与从训练
    Gopher 中获得的见解, 2021年. 网址 [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446)。
- en: Raffel et al. (2020a) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *JMLR*,
    21(140):1–67, 2020a. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020a) Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan
    Narang、Michael Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。探索统一文本到文本变换器的迁移学习极限。*JMLR*，21(140)：1–67，2020a。URL
    [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html)。
- en: Raffel et al. (2020b) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21(140):1–67, 2020b.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020b) Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan
    Narang、Michael Matena、Yanqi Zhou、Wei Li、Peter J Liu 等人。探索统一文本到文本变换器的迁移学习极限。*J.
    Mach. Learn. Res.*，21(140)：1–67，2020b。
- en: Ramasesh et al. (2022) Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan
    Dyer. Effect of scale on catastrophic forgetting in neural networks. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=GhVS8_yPeEa](https://openreview.net/forum?id=GhVS8_yPeEa).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramasesh 等人 (2022) Vinay Venkatesh Ramasesh、Aitor Lewkowycz 和 Ethan Dyer。规模对神经网络灾难性遗忘的影响。在*国际学习表征会议*，2022。URL
    [https://openreview.net/forum?id=GhVS8_yPeEa](https://openreview.net/forum?id=GhVS8_yPeEa)。
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer,
    Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan
    Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale
    generative language model, 2022. URL [https://arxiv.org/abs/2201.11990](https://arxiv.org/abs/2201.11990).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等人 (2022) Shaden Smith、Mostofa Patwary、Brandon Norick、Patrick LeGresley、Samyam
    Rajbhandari、Jared Casper、Zhun Liu、Shrimai Prabhumoye、George Zerveas、Vijay Korthikanti、Elton
    Zhang、Rewon Child、Reza Yazdani Aminabadi、Julie Bernauer、Xia Song、Mohammad Shoeybi、Yuxiong
    He、Michael Houston、Saurabh Tiwary 和 Bryan Catanzaro。使用 deepspeed 和 megatron 训练
    megatron-turing nlg 530b，一个大规模生成语言模型，2022年。URL [https://arxiv.org/abs/2201.11990](https://arxiv.org/abs/2201.11990)。
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and
    extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*,
    2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人 (2022) Aarohi Srivastava、Abhinav Rastogi、Abhishek Rao、Abu Awal
    Md Shoeb、Abubakar Abid、Adam Fisch、Adam R Brown、Adam Santoro、Aditya Gupta、Adrià
    Garriga-Alonso 等人。超越模仿游戏：量化和外推语言模型的能力。*arXiv 预印本 arXiv:2206.04615*，2022。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 (2023) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等人。Llama：开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett (eds.), *Advances in NeurIPS*, 2017. URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 (2017) Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N Gomez、Ł ukasz Kaiser 和 Illia Polosukhin。注意力机制是你所需要的一切。在 I. Guyon、U.
    Von Luxburg、S. Bengio、H. Wallach、R. Fergus、S. Vishwanathan 和 R. Garnett (编)，*NeurIPS
    进展*，2017。URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A
    stickier benchmark for general-purpose language understanding systems. *Advances
    in neural information processing systems*, 32, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2019) Alex Wang、Yada Pruksachatkun、Nikita Nangia、Amanpreet Singh、Julian
    Michael、Felix Hill、Omer Levy 和 Samuel Bowman。Superglue：一个更具挑战性的通用语言理解系统基准。*神经信息处理系统进展*，32，2019。
- en: Wei et al. (2021a) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models
    are zero-shot learners. *CoRR*, abs/2109.01652, 2021a. URL [https://arxiv.org/abs/2109.01652](https://arxiv.org/abs/2109.01652).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2021a) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, 和 Quoc V. Le. 微调的语言模型是零样本学习者。*CoRR*,
    abs/2109.01652, 2021a。网址 [https://arxiv.org/abs/2109.01652](https://arxiv.org/abs/2109.01652)。
- en: Wei et al. (2021b) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021b.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2021b) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le. 微调的语言模型是零样本学习者。*arXiv 预印本
    arXiv:2109.01652*, 2021b。
- en: 'Xue et al. (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual
    pre-trained text-to-text transformer. *arXiv preprint arXiv:2010.11934*, 2020.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue 等人 (2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou,
    Aditya Siddhant, Aditya Barua, 和 Colin Raffel. mt5: 一种大规模多语言预训练文本到文本变换器。*arXiv
    预印本 arXiv:2010.11934*, 2020。'
- en: Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,
    Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal
    large language models. *arXiv preprint arXiv:2309.10313*, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等人 (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong
    Jae Lee, 和 Yi Ma. 研究多模态大型语言模型中的灾难性遗忘。*arXiv 预印本 arXiv:2309.10313*, 2023。
- en: Zhang et al. (2021) Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi,
    Chuanqi Tan, Fei Huang, and Huajun Chen. Differentiable prompt makes pre-trained
    language models better few-shot learners. *arXiv preprint arXiv:2108.13161*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2021) Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi
    Tan, Fei Huang, 和 Huajun Chen. 可微调提示使预训练语言模型成为更好的少样本学习者。*arXiv 预印本 arXiv:2108.13161*,
    2021。
- en: Appendix A Appendix
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Broader Impacts
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 更广泛的影响
- en: In our work, we propose a method to improve general-purpose language models
    with fine-tuning datasets. The improved general-purpose language model may be
    used in malicious applications such as generating disinformation. To mitigate
    the potential negative impacts, we can add watermark or deploy AI-generated text
    classifiers before releasing the model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们提出了一种通过微调数据集来改进通用语言模型的方法。改进后的通用语言模型可能会被用于恶意应用，如生成虚假信息。为了减轻潜在的负面影响，我们可以在发布模型之前添加水印或部署
    AI 生成文本分类器。
- en: A.2 Experiment Details
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验细节
- en: A.2.1 Input Template Used in Experiments
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 实验中使用的输入模板
- en: In Table [4](#A1.T4 "Table 4 ‣ A.2.1 Input Template Used in Experiments ‣ A.2
    Experiment Details ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization"), we list the natural language input template
    used in our experiments for each task
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[4](#A1.T4 "表 4 ‣ A.2.1 实验中使用的输入模板 ‣ A.2 实验细节 ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中，我们列出了在我们的实验中为每个任务使用的自然语言输入模板
- en: 'Task Template RTE [premise] question: [hypothesis] Is it true or false? answer:
    {True, False} CB & SNLI [premise] question: [hypothesis] Is it true or false or
    neither? answer: {True, False, Neither} WiC [sentence1] [sentence2] question:
    The word [word] is used in the same way in the two sentences. Is it true or False?
    answer: {True, False} OpenbookQA Q: [question] A) [option A] B) [option B] C)
    [option C] D) [option D] A: QA Q: [question] A: Translation Translate [source
    language] to [target language]: [sentence 1] Summarization Article: [article]
    One sentence summary:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '任务模板 RTE [前提] 问题: [假设] 这是真的还是假的？ 答案: {True, False} CB & SNLI [前提] 问题: [假设]
    这是真的还是假的，还是两者都不是？ 答案: {True, False, Neither} WiC [句子1] [句子2] 问题: 词汇 [word] 在这两个句子中的用法是否相同？这是真的还是假的？
    答案: {True, False} OpenbookQA 问题: [问题] A) [选项 A] B) [选项 B] C) [选项 C] D) [选项 D]
    A: QA 问题: [问题] A: 翻译 将 [源语言] 翻译成 [目标语言]: [句子 1] 摘要 文章: [文章] 一句话总结:'
- en: 'Table 4: Input template for each task'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 每个任务的输入模板'
- en: The example shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization") is from ID
    41141109 in XSum dataset.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S1.T1 "表 1 ‣ 1 引言 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中显示的示例来自 XSum 数据集中的 ID 41141109。
- en: A.2.2 Output Post-processing
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 输出后处理
- en: For each task, we first extract the text after $<$, then trim the text by locating
    and remove the text after the second prefix token (Q:, Translate, Article:). For
    classification tasks including RTE, CB and WiC, we check whether the first output
    token is True or False.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，我们首先提取$<$后的文本，然后通过定位并删除第二个前缀标记（Q:、Translate、Article:）后的文本来修剪文本。对于包括RTE、CB和WiC在内的分类任务，我们检查第一个输出标记是否为True或False。
- en: A.2.3 Dataset and Models
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.3 数据集和模型
- en: Dataset Version Training Validation Test RTE v102 2,490 277 3,000 CB v102 250
    56 250 WiC v102 5,428 638 1,400 WMT14 En-Fr v003 15,786,979 3,000 3,003 WMT16
    En-De v003 4,548,885 2,169 2,999 WMT16 En-Ro v003 610,320 1,999 1,999 TriviaQA
    rc.nocontext:1.1.0 138,384 18,669 17,210 Web Questions 1.0.0 3,778 - 2,032 XSum
    1.1.0 203,577 11,305 11,301 WikiLingua/en gem/wiki_lingua_english_en 99,020 13,823
    28,614
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 版本 训练 验证 测试 RTE v102 2,490 277 3,000 CB v102 250 56 250 WiC v102 5,428 638
    1,400 WMT14 En-Fr v003 15,786,979 3,000 3,003 WMT16 En-De v003 4,548,885 2,169
    2,999 WMT16 En-Ro v003 610,320 1,999 1,999 TriviaQA rc.nocontext:1.1.0 138,384
    18,669 17,210 Web Questions 1.0.0 3,778 - 2,032 XSum 1.1.0 203,577 11,305 11,301
    WikiLingua/en gem/wiki_lingua_english_en 99,020 13,823 28,614
- en: 'Table 5: Version number, sizes of training, validation, and testing splits
    for each dataset used.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：每个数据集的版本号、训练、验证和测试分割的大小。
- en: We list the statistics of all datasets used in the paper in Table [5](#A1.T5
    "Table 5 ‣ A.2.3 Dataset and Models ‣ A.2 Experiment Details ‣ Appendix A Appendix
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization").
    All the datasets and models can be used in research context.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格[5](#A1.T5 "表 5 ‣ A.2.3 数据集和模型 ‣ A.2 实验细节 ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中列出了论文中使用的所有数据集的统计信息。所有数据集和模型可以用于研究环境。
- en: A.2.4 Hyper-parameters
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.4 超参数
- en: 'For all mT5 models, we fine-tune with learning rate 0.001, drop rate 0.1 and
    label smoothing 0.1, following the default settings for T5 models (Raffel et al.,
    [2020b](#bib.bib34)). For all prompt tuning experiments, we use learning rate
    0.2 and prompt length 100\. For all tasks except summarization tasks, we choose
    the model input sequence length larger than the input length in datasets. For
    summarization, we cut each input to 1024 tokens. We use Adafactor optimizer and
    batch size 64 without data-packing across all experiments. In inference, we use
    beam search to decode the outputs with width 4\. More experimental settings are
    provided in the appendix. For ProMoT tuning, at stage 1 we run prompt tuning for
    5000 steps and save a checkpoint every 1000 steps, then select the prompt checkpoint
    with the best performance on target task. At stage 2, we freeze the trained prompt
    and fine-tune the model for 1000 steps, checkpointing every 100 steps. We pick
    the model checkpoint with highest performance on the fine-tuned task as our final
    checkpoint. For comparison, we run prompt tuning and standard fine-tuning for
    5000 and 1000 training steps respectively and report the performance of the best
    checkpoint. We explore fine-tuning with more steps in Appendix [A.3.2](#A1.SS3.SSS2
    "A.3.2 Training more steps: trade-off between fine-tuning target task and in-context
    learning abilities ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣
    Two-stage LLM Fine-tuning with Less Specialization and More Generalization").'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有mT5模型，我们使用学习率0.001、丢弃率0.1和标签平滑0.1进行微调，遵循T5模型的默认设置（Raffel等，[2020b](#bib.bib34)）。对于所有提示微调实验，我们使用学习率0.2和提示长度100。除了总结任务外的所有任务，我们选择模型输入序列长度大于数据集中的输入长度。对于总结任务，我们将每个输入剪切为1024个标记。我们使用Adafactor优化器和批量大小64，不跨所有实验进行数据打包。在推断中，我们使用宽度为4的束搜索解码输出。更多实验设置见附录。对于ProMoT微调，在阶段1中，我们运行5000步的提示微调，并每1000步保存一个检查点，然后选择在目标任务上表现最佳的提示检查点。在阶段2中，我们冻结训练好的提示，并将模型微调1000步，每100步保存一个检查点。我们选择在微调任务上表现最好的模型检查点作为最终检查点。为了比较，我们分别进行5000步和1000步的提示微调和标准微调，并报告最佳检查点的性能。我们在附录[A.3.2](#A1.SS3.SSS2
    "A.3.2 训练更多步骤：微调目标任务与上下文学习能力之间的权衡 ‣ A.3 附加实验结果 ‣ 附录 A 附录 ‣ 两阶段 LLM 微调，减少专业化，增加泛化")中探索了更多步骤的微调。
- en: In ablation study in Section [5.5](#S5.SS5 "5.5 Ablation Study ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    we include an experiment to jointly fine-tune soft prompt and pretrained model.
    In this experiment, we finetune the model and prompt for 1000 steps with the same
    learning rate 0.001, following the setting in (He et al., [2022](#bib.bib15)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5.5](#S5.SS5 "5.5 Ablation Study ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization")节的消融研究中，我们包括了一项实验来共同微调软提示和预训练模型。在这个实验中，我们将模型和提示一起微调1000步，使用相同的学习率0.001，遵循（He
    et al., [2022](#bib.bib15)）中的设置。
- en: A.2.5 Hardware and Implementation
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.5 硬件与实现
- en: All the experiments are implemented based on the original prompt tuning²²2https://github.com/google-research/prompt-tuning
    and T5x code base³³3https://github.com/google-research/t5x. All experiments are
    run on a cluster of 64 parallel TPUs. Time cost for different experiments varies,
    however, all training experiments can be finished within 1 day.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都基于原始的提示微调²²2https://github.com/google-research/prompt-tuning和T5x代码库³³3https://github.com/google-research/t5x实现。所有实验都在一个64个并行TPU的集群上运行。不同实验的时间成本有所不同，但所有训练实验都能在1天内完成。
- en: A.3 Additional Experiment Results
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 额外实验结果
- en: A.3.1 Additional results for single task fine-tuning
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 单任务微调的额外结果
- en: As complementary results of Table [5.2](#S5.SS2 "5.2 Supervised Performance
    on Fine-tuning Tasks ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization") and [5.3](#S5.SS3 "5.3 Generalization with Single Task
    Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization"), we list and compare the performance of prompt tuning
    + 1-shot in Table [6](#A1.T6 "Table 6 ‣ A.3.1 Additional results for single task
    fine-tuning ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization"). We also provide
    experiments on SNLI and OpenbookQA datasets in Table [7](#A1.T7 "Table 7 ‣ A.3.1
    Additional results for single task fine-tuning ‣ A.3 Additional Experiment Results
    ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization"). Without fine-tuning, pretrained mT5 failed to output “A”,
    “B”, “C”, “D” for multi-choice QA in 0-shot openbookQA dataset, which results
    in a zero accuracy. We can see that the additional experiments are consistent
    with our main experiments that ProMoT can achieve similar supervised performance
    on fine-tuning tasks with less forgetting and even better performance on general
    in-context learning tasks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 作为表[5.2](#S5.SS2 "5.2 Supervised Performance on Fine-tuning Tasks ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")和[5.3](#S5.SS3
    "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization")的补充结果，我们在表[6](#A1.T6
    "Table 6 ‣ A.3.1 Additional results for single task fine-tuning ‣ A.3 Additional
    Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization")中列出了并比较了提示微调 + 1-shot的表现。我们还在表[7](#A1.T7
    "Table 7 ‣ A.3.1 Additional results for single task fine-tuning ‣ A.3 Additional
    Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization")中提供了SNLI和OpenbookQA数据集上的实验结果。如果没有微调，预训练的mT5在0-shot
    openbookQA数据集中的多选QA中无法输出“A”、“B”、“C”、“D”，这导致准确率为零。我们可以看到，额外的实验结果与我们的主要实验一致，即ProMoT在微调任务上可以实现类似的监督性能，并且在一般的上下文学习任务上表现更好。
- en: Fine-tuning Datasets Prompt Tuning + 1-shot ProMoT + 1-shot RTE 92.78 93.86
    WMT14 En-Fr 39.41 41.19
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 微调数据集 提示微调 + 1-shot ProMoT + 1-shot RTE 92.78 93.86 WMT14 En-Fr 39.41 41.19
- en: 'Table 6: Performances of prompt tuning + 1-shot and ProMoT + 1-shot on fine-tuning
    tasks.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：提示微调 + 1-shot和ProMoT + 1-shot在微调任务上的表现。
- en: Datasets Pretrained Prompt-tuning Standard Fine-tuning ProMoT (Ours) Standard
    Fine-tuning ProMoT (Ours) on SNLI on SNLI on OpenbookQA on OpenbookQA SNLI 1.32
    88.53 88.91 89.62 - - OpenbookQA 0.00 73.60 - - 77.2 81.6 Norm. Average 17.52
    17.52 16.20 19.83 0.00 17.40 (-1.32) (+2.31) (-17.52) (-0.12) CB 46.43 46.43 69.64
    62.5 0.00 41.07 WiC 49.69 49.69 53.29 51.25 0.00 50.0 triviaQA 17.58 17.58 4.54
    20.56 0.05 21.10 web_questions 9.70 9.70 2.12 9.94 0.00 10.53 WMT16_ende 3.97
    3.97 0.00 2.48 0.00 2.80 WMT16_enro 1.82 1.82 0.00 0.90 0.00 1.04 XSum 6.41 6.41
    0.00 6.58 0.00 7.48 WikiLingua/en 4.59 4.59 0.00 4.39 0.00 5.20
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 预训练 Prompt-tuning 标准微调 ProMoT（我们的） 标准微调 ProMoT（我们的） 在 SNLI 上 在 SNLI 上 在
    OpenbookQA 上 在 OpenbookQA 上 SNLI 1.32 88.53 88.91 89.62 - - OpenbookQA 0.00 73.60
    - - 77.2 81.6 标准平均值 17.52 17.52 16.20 19.83 0.00 17.40 (-1.32) (+2.31) (-17.52)
    (-0.12) CB 46.43 46.43 69.64 62.5 0.00 41.07 WiC 49.69 49.69 53.29 51.25 0.00
    50.0 triviaQA 17.58 17.58 4.54 20.56 0.05 21.10 web_questions 9.70 9.70 2.12 9.94
    0.00 10.53 WMT16_ende 3.97 3.97 0.00 2.48 0.00 2.80 WMT16_enro 1.82 1.82 0.00
    0.90 0.00 1.04 XSum 6.41 6.41 0.00 6.58 0.00 7.48 WikiLingua/en 4.59 4.59 0.00
    4.39 0.00 5.20
- en: 'Table 7: Performance of a mT5 XXL model finetuned on SNLI and OpenbookQA and
    evaluated on 8 1-shot tasks. The accuracy on fine-tuned tasks are in the first
    two rows. Prompt-tuning doesn’t modify pretrained model parameters and has the
    same in-context performance as pretrained model.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：mT5 XXL模型在SNLI和OpenbookQA上微调并在8个1-shot任务上评估的性能。微调任务的准确率在前两行。Prompt-tuning不会修改预训练模型参数，具有与预训练模型相同的上下文性能。
- en: 'A.3.2 Training more steps: trade-off between fine-tuning target task and in-context
    learning abilities'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 训练更多步：微调目标任务与上下文学习能力之间的权衡
- en: 'In Section [5.3](#S5.SS3 "5.3 Generalization with Single Task Fine-tuning ‣
    5 Experiments ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    we report the results of the best checkpoints within 1000 steps of fine-tuning.
    With a longer training period, we can see a more clear trade-off between the performance
    on fine-tuning target task and the performance on in-context learning abilities.
    Here we show the long-term trade-off between fine-tuning target task and in-context
    learning evaluation tasks by scattering the performance of different checkpoints
    within 20000 steps fine-tuning. In Figure [6](#A1.F6 "Figure 6 ‣ A.3.2 Training
    more steps: trade-off between fine-tuning target task and in-context learning
    abilities ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization"), and [6](#A1.F6
    "Figure 6 ‣ A.3.2 Training more steps: trade-off between fine-tuning target task
    and in-context learning abilities ‣ A.3 Additional Experiment Results ‣ Appendix
    A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization"),
    we plot the trade-off on classification and translation tasks, respectively.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '在[5.3节](#S5.SS3 "5.3 Generalization with Single Task Fine-tuning ‣ 5 Experiments
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")中，我们报告了在微调的1000步内最佳检查点的结果。随着训练时间的延长，我们可以看到微调目标任务的性能与上下文学习能力之间的更明显的权衡。这里我们通过在20000步微调内散布不同检查点的性能来展示微调目标任务与上下文学习评估任务之间的长期权衡。在图[6](#A1.F6
    "Figure 6 ‣ A.3.2 Training more steps: trade-off between fine-tuning target task
    and in-context learning abilities ‣ A.3 Additional Experiment Results ‣ Appendix
    A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")和[6](#A1.F6
    "Figure 6 ‣ A.3.2 Training more steps: trade-off between fine-tuning target task
    and in-context learning abilities ‣ A.3 Additional Experiment Results ‣ Appendix
    A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")中，我们分别绘制了分类任务和翻译任务的权衡图。'
- en: '![Refer to caption](img/c45f3185b2043a7dcb76a5bc301cd7e1.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c45f3185b2043a7dcb76a5bc301cd7e1.png)'
- en: 'Figure 5: Trade-off between BLEU score of En-Fr (horizontal axis) and average
    accuracy on classification tasks (vertical axis) when fine-tuning the model on
    En-Fr translation.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在对En-Fr翻译模型进行微调时，En-Fr的BLEU得分（横轴）与分类任务上的平均准确率（纵轴）之间的权衡。
- en: '![Refer to caption](img/c26a37a984eef9ac16008085a1082368.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c26a37a984eef9ac16008085a1082368.png)'
- en: 'Figure 6: Trade-off between BLEU score of En-Fr (horizontal axis) and average
    BLEU score on other language pairs (vertical axis) when fine-tuning the model
    on En-Fr translation.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在对En-Fr翻译模型进行微调时，En-Fr的BLEU得分（横轴）与其他语言对的平均BLEU得分（纵轴）之间的权衡。
- en: As we can see from the figures, datapoints for ProMoT is higher than standard
    fine-tuning on the figures, which implies that with the same performance on fine-tuning
    target task, forgetting is alleviated with ProMoT fine-tuning.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，ProMoT的数据点高于标准微调的图形，这表明在相同的微调目标任务性能下，ProMoT微调减轻了遗忘现象。
- en: A.3.3 Additional experiments on T5 XXL
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.3 关于T5 XXL的附加实验
- en: To show the performance of our method on an English-based pretrained model,
    we did an additional experiment on T5 XXL with fine-tuning target task RTE. The
    result is shown in Table [8](#A1.T8 "Table 8 ‣ A.3.3 Additional experiments on
    T5 XXL ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage LLM
    Fine-tuning with Less Specialization and More Generalization"). The results are
    consistent with our main experiments on the mT5 XXL model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们的方法在基于英语的预训练模型上的表现，我们对 T5 XXL 进行了一个附加实验，任务为 RTE 微调。结果见表格 [8](#A1.T8 "Table
    8 ‣ A.3.3 Additional experiments on T5 XXL ‣ A.3 Additional Experiment Results
    ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization")。结果与我们在 mT5 XXL 模型上的主要实验一致。
- en: Datasets Pretrained Prompt-tuning Standard Fine-tuning ProMoT (Ours) RTE - 91.7
    93.5 93.14 Norm. Average 19.75 19.75 14.07 22.23 (-5.68) (+2.49) CB 55.36 55.36
    62.50 73.21 WiC 49.84 49.84 50.00 50.78 triviaQA 34.15 34.15 0.02 33.86 web_questions
    16.04 16.04 0.00 15.95 WMT16_ende 0.13 0.13 0.00 0.02 WMT16_enro 0.06 0.06 0.00
    0.01 XSum 1.26 1.26 0.00 1.79 WikiLingua/en 1.12 1.12 0 2.25
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 预训练 Prompt-tuning 标准微调 ProMoT（我们的方法） RTE - 91.7 93.5 93.14 规范化平均 19.75 19.75
    14.07 22.23 (-5.68) (+2.49) CB 55.36 55.36 62.50 73.21 WiC 49.84 49.84 50.00 50.78
    triviaQA 34.15 34.15 0.02 33.86 web_questions 16.04 16.04 0.00 15.95 WMT16_ende
    0.13 0.13 0.00 0.02 WMT16_enro 0.06 0.06 0.00 0.01 XSum 1.26 1.26 0.00 1.79 WikiLingua/en
    1.12 1.12 0 2.25
- en: 'Table 8: Performance of a T5.1.1 XXL model finetuned on RTE and evaluated on
    8 1-shot tasks. The accuracy on fine-tuned task (RTE) is in the first row. Prompt-tuning
    doesn’t modify pretrained model parameters and has the same in-context performance
    as pretrained model.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 8：在 RTE 上微调的 T5.1.1 XXL 模型在 8 个 1-shot 任务上的表现。微调任务（RTE）的准确率在第一行。Prompt-tuning
    不修改预训练模型参数，且在上下文中的表现与预训练模型相同。
- en: A.3.4 Additional experiments on mT5 XL
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.4 关于 mT5 XL 的附加实验
- en: To show the performance of our method on an smaller-size pretrained model, we
    did an additional experiment on mT5 XL with fine-tuning target task WMT14 En-Fr.
    The result is shown in Table [9](#A1.T9 "Table 9 ‣ A.3.4 Additional experiments
    on mT5 XL ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization"). The results
    are consistent with our main experiments on the mT5 XXL model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们的方法在较小规模的预训练模型上的表现，我们对 mT5 XL 进行了一个附加实验，任务为 WMT14 En-Fr 微调。结果见表格 [9](#A1.T9
    "Table 9 ‣ A.3.4 Additional experiments on mT5 XL ‣ A.3 Additional Experiment
    Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization")。结果与我们在 mT5 XXL 模型上的主要实验一致。
- en: Datasets Pretrained Prompt-tuning Standard Fine-tuning ProMoT (Ours) WMT14 En-Fr
    - 32.47 35.84 36.46 Norm. Average 13.59 13.59 8.61 14.40 (-4.98) (+0.81) CB 26.79
    26.79 21.43 28.57 WiC 50.0 50.0 44.20 51.10 triviaQA 12.13 12.13 0.83 8.81 web_questions
    6.59 6.59 0.44 5.31 WMT16_ende 2.56 2.56 0.63 7.69 WMT16_enro 1.52 1.52 1.20 10.40
    XSum 4.26 4.26 0.05 1.37 WikiLingua/en 4.88 4.88 0.06 1.94
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 预训练 Prompt-tuning 标准微调 ProMoT（我们的方法） WMT14 En-Fr - 32.47 35.84 36.46 规范化平均
    13.59 13.59 8.61 14.40 (-4.98) (+0.81) CB 26.79 26.79 21.43 28.57 WiC 50.0 50.0
    44.20 51.10 triviaQA 12.13 12.13 0.83 8.81 web_questions 6.59 6.59 0.44 5.31 WMT16_ende
    2.56 2.56 0.63 7.69 WMT16_enro 1.52 1.52 1.20 10.40 XSum 4.26 4.26 0.05 1.37 WikiLingua/en
    4.88 4.88 0.06 1.94
- en: 'Table 9: Performance of a mT5 XL model finetuned on WMT14 En-Fr and evaluated
    on 8 1-shot tasks. The BLEU score on fine-tuned task (WMT14 En-Fr) is in the first
    row. Prompt-tuning doesn’t modify pretrained model parameters and has the same
    in-context performance as pretrained model.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：在 WMT14 En-Fr 上微调的 mT5 XL 模型在 8 个 1-shot 任务上的表现。微调任务（WMT14 En-Fr）的 BLEU
    分数在第一行。Prompt-tuning 不修改预训练模型参数，且在上下文中的表现与预训练模型相同。
- en: A.3.5 Addtional experiments on PaLM 8B
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.5 关于 PaLM 8B 的附加实验
- en: To show the performance of our method on decoder-only models, we did an additional
    experiment on PaLM 8b model with fine-tuning target task WMT14 En-Fr. We use prompt
    length 50 and learning rate 0.3 in prompt-tuning and default fine-tuning hyperparameters
    in fine-tuning. The result is shown in Table [10](#A1.T10 "Table 10 ‣ A.3.5 Addtional
    experiments on PaLM 8B ‣ A.3 Additional Experiment Results ‣ Appendix A Appendix
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization").
    The results are consistent with our main experiments on mT5, where ProMoT can
    achieve similar supervised performance on fine-tuning tasks with less forgetting
    on general in-context learning tasks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们方法在仅解码器模型上的表现，我们对PaLM 8b模型进行了额外的实验，目标任务为WMT14 En-Fr。在提示调优中我们使用了50的提示长度和0.3的学习率，标准微调中使用了默认的超参数。结果如表[10](#A1.T10
    "Table 10 ‣ A.3.5 Addtional experiments on PaLM 8B ‣ A.3 Additional Experiment
    Results ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization")所示。结果与我们在mT5上的主要实验一致，其中ProMoT能够在微调任务上实现类似的有监督性能，同时在一般的上下文学习任务中遗忘较少。
- en: Datasets Pretrained Prompt-tuning Standard Fine-tuning ProMoT (Ours) WMT14 En-Fr
    - 13.62 33.04 32.02 Norm. Average 26.09 26.09 17.80 22.37 (-8.29) (-3.72) CB 46.43
    46.43 32.14 33.93 WiC 49.69 49.69 49.06 49.69 triviaQA 44.69 44.69 37.09 42.11
    web_questions 13.02 13.02 11.91 13.01 WMT16_ende 23.85 23.85 3.77 19.33 WMT16_enro
    19.89 19.89 4.02 13.18 XSum 5.57 5.57 2.29 2.9 WikiLingua/en 5.59 5.59 3.14 4.77
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 预训练 提示调优 标准微调 ProMoT（我们的） WMT14 En-Fr - 13.62 33.04 32.02 标准化平均 26.09 26.09
    17.80 22.37 (-8.29) (-3.72) CB 46.43 46.43 32.14 33.93 WiC 49.69 49.69 49.06 49.69
    triviaQA 44.69 44.69 37.09 42.11 web_questions 13.02 13.02 11.91 13.01 WMT16_ende
    23.85 23.85 3.77 19.33 WMT16_enro 19.89 19.89 4.02 13.18 XSum 5.57 5.57 2.29 2.9
    WikiLingua/en 5.59 5.59 3.14 4.77
- en: 'Table 10: Performance of a PaLM 8b model finetuned on WMT14 En-Fr and evaluated
    on 8 1-shot tasks. The BLEU score on fine-tuned task (WMT14 En-Fr) is in the first
    row. Prompt-tuning doesn’t modify pretrained model parameters and has the same
    in-context performance as pretrained model.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：在WMT14 En-Fr上微调的PaLM 8b模型在8个1-shot任务上的表现。微调任务（WMT14 En-Fr）的BLEU得分在第一行。提示调优不修改预训练模型参数，其在上下文中的表现与预训练模型相同。
- en: A.3.6 Using LORA in the first stage
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.6 在第一阶段使用LORA
- en: 'As we have discussed in Section [4](#S4 "4 Proposed Method: PROmpt tuning with
    MOdel Tuning (ProMoT) ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization"), conceptually we can use any parameter-efficient method
    at the first ProMoT fine-tuning stage to absorb the task format information. Here
    we did experiments to compare LoRA and prompt-tuning (used in our ProMoT main
    experiments) in the first fine-tuning stage. We report the results in Table [11](#A1.T11
    "Table 11 ‣ A.3.6 Using LORA in the first stage ‣ A.3 Additional Experiment Results
    ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization"). As we can see from the table, ProMoT with prompt-tuning
    is significantly better than ProMoT with LoRA, in both supervised fine-tuning
    task and unseen 1-shot evaluation tasks. This might partially due to better alignment
    of soft prompt between format description in natural language corpus.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第[4](#S4 "4 Proposed Method: PROmpt tuning with MOdel Tuning (ProMoT)
    ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")节讨论的那样，从概念上讲，我们可以在第一个ProMoT微调阶段使用任何参数高效的方法来吸收任务格式信息。在这里，我们进行了实验来比较LoRA和提示调优（在我们的ProMoT主要实验中使用）在第一个微调阶段的效果。我们在表[11](#A1.T11
    "Table 11 ‣ A.3.6 Using LORA in the first stage ‣ A.3 Additional Experiment Results
    ‣ Appendix A Appendix ‣ Two-stage LLM Fine-tuning with Less Specialization and
    More Generalization")中报告了结果。从表中可以看出，使用提示调优的ProMoT在有监督微调任务和未见1-shot评估任务中显著优于使用LoRA的ProMoT。这可能部分由于软提示在自然语言语料库中的格式描述对齐更好。'
- en: Datasets Pretrained Standard Fine-tuning ProMoT with LoRA (r=2) ProMoT with
    LoRA (r=4) ProMoT WMT14 En-Fr - 41.80 39.09 39.97 41.19 CB 46.43 16.07 26.78 23.21
    41.07 WiC 49.69 50.63 53.13 53.25 50.16 triviaQA 17.58 0.03 4.98 5.19 13.63 web_questions
    9.70 0.05 3.30 3.84 9.40 WMT16_ende 3.97 0.67 1.23 1.87 15.52 WMT16_enro 1.82
    0.91 2.06 2.89 18.54 XSum 6.41 0.030 0.17 0.35 1.49 WikiLingua/en 4.59 0.05 0.64
    0.57 1.14
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 预训练 标准微调 ProMoT与LoRA（r=2） ProMoT与LoRA（r=4） ProMoT WMT14 En-Fr - 41.80 39.09
    39.97 41.19 CB 46.43 16.07 26.78 23.21 41.07 WiC 49.69 50.63 53.13 53.25 50.16
    triviaQA 17.58 0.03 4.98 5.19 13.63 web_questions 9.70 0.05 3.30 3.84 9.40 WMT16_ende
    3.97 0.67 1.23 1.87 15.52 WMT16_enro 1.82 0.91 2.06 2.89 18.54 XSum 6.41 0.030
    0.17 0.35 1.49 WikiLingua/en 4.59 0.05 0.64 0.57 1.14
- en: 'Table 11: Performance of a mT5 XXL model finetuned on WMT14 En-Fr and evaluated
    on 8 1-shot tasks. In this experiment we use LoRA at the first ProMoT stage instead
    of prompt-tuning. $r$ is the rank of LoRA’s low-rank update matrices. The BLEU
    score on fine-tuned task (WMT14 En-Fr) is in the first row. Prompt-tuning doesn’t
    modify pretrained model parameters and has the same in-context performance as
    pretrained model.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：在 WMT14 英法任务上微调的 mT5 XXL 模型的性能，并在 8 个 1-shot 任务上进行评估。在此实验中，我们在第一阶段 ProMoT
    中使用 LoRA 而不是 prompt-tuning。$r$ 是 LoRA 的低秩更新矩阵的秩。微调任务（WMT14 英法）的 BLEU 分数在第一行。Prompt-tuning
    不修改预训练模型参数，具有与预训练模型相同的上下文性能。
- en: A.3.7 Plotting more steps for Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Loss of in-context
    learning capabilities during fine-tuning ‣ 3 Format specialization in fine-tuning
    causes the loss of in-context learning capabilities ‣ Two-stage LLM Fine-tuning
    with Less Specialization and More Generalization")
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.7 图 [3](#S3.F3 "Figure 3 ‣ 3.1 Loss of in-context learning capabilities
    during fine-tuning ‣ 3 Format specialization in fine-tuning causes the loss of
    in-context learning capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization") 的更多步骤绘制
- en: '![Refer to caption](img/6e8e8c74fcf3f28c5db9267ebc2985eb.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e8e8c74fcf3f28c5db9267ebc2985eb.png)'
- en: 'Figure 7: Cosine similarity between the full gradient $g$ on different parts
    of the last decoder layer. We collect and show the cosine value for gradients
    on MLP kernel, Query, Key and Value on the attention module.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：最后解码层不同部分的完整梯度 $g$ 之间的余弦相似度。我们收集并显示了在注意力模块中 MLP 核心、查询、键和值的梯度的余弦值。
- en: To further strengthen our conclusion in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Loss
    of in-context learning capabilities during fine-tuning ‣ 3 Format specialization
    in fine-tuning causes the loss of in-context learning capabilities ‣ Two-stage
    LLM Fine-tuning with Less Specialization and More Generalization"), here we plot
    the gradient alignment from step 0 to step 400\. As we can see from the figure,
    gradient alignment drops significantly after 300 steps which is matched with Figure
    [2](#S3.F2 "Figure 2 ‣ 3.1 Loss of in-context learning capabilities during fine-tuning
    ‣ 3 Format specialization in fine-tuning causes the loss of in-context learning
    capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")
    where the true and false ratio increases before 300 steps and then remains stable.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步加强我们在图 [3](#S3.F3 "Figure 3 ‣ 3.1 Loss of in-context learning capabilities
    during fine-tuning ‣ 3 Format specialization in fine-tuning causes the loss of
    in-context learning capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization
    and More Generalization") 中的结论，这里我们绘制了从步骤 0 到步骤 400 的梯度对齐图。从图中可以看出，梯度对齐在 300 步后显著下降，这与图
    [2](#S3.F2 "Figure 2 ‣ 3.1 Loss of in-context learning capabilities during fine-tuning
    ‣ 3 Format specialization in fine-tuning causes the loss of in-context learning
    capabilities ‣ Two-stage LLM Fine-tuning with Less Specialization and More Generalization")
    中的情况相符，其中真实与虚假比率在 300 步之前增加，然后保持稳定。
- en: A.3.8 Qualitative results on fine-tuning WMT14 En-Fr task
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.8 关于微调 WMT14 英法任务的定性结果
- en: In Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Two-stage LLM Fine-tuning with
    Less Specialization and More Generalization") we show an example from fine-tuning
    task RTE. Here we show examples for fine-tuning task WMT14 En-Fr translation on
    different unseen few-shot tasks. We compare the outputs from ground-truth targets,
    pretrained mT5, fine-tuned mT5 on WMT14 En-Fr and ProMoT mT5 on WMT14 En-Fr. The
    outputs are generated with a 1-shot example. As we can see from the examples,
    standard fine-tuning on WMT14 En-Fr will 1) make the model overfit its format
    and tend to output French; and 2) model tends to repeat its input which is similar
    to translation task. ProMoT alleviates this specialization on fine-tuning task
    and has better generalization.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Two-stage LLM Fine-tuning with Less
    Specialization and More Generalization") 中，我们展示了来自微调任务 RTE 的一个示例。这里我们展示了不同未见的少量示例任务上的微调任务
    WMT14 英法翻译示例。我们比较了真实目标、预训练的 mT5、在 WMT14 英法上微调的 mT5 和 ProMoT 在 WMT14 英法上微调的 mT5
    的输出。这些输出是通过 1-shot 示例生成的。从这些示例中可以看出，标准的 WMT14 英法微调将 1) 使模型过拟合其格式，并倾向于输出法语；以及 2)
    模型倾向于重复其输入，这类似于翻译任务。ProMoT 缓解了这种微调任务上的专业化，并具有更好的泛化能力。
- en: •
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: WMT16 En-De
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WMT16 英德
- en: –
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Target: Danach war der Mann, der sich nach Angaben seines Anwalts mittlerweile
    wieder auf freiem Fußbefindet, in eine größere Zelle verlegt worden.'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：之后，按照其律师的说法，该男子现在已经重新获得自由，已被转移到一个更大的牢房。
- en: –
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Pretrained: Danach wurde der Mann in eine größere.'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练：之后该男子被转移到一个更大的牢房。
- en: –
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Fine-tune: L’homme, qui, selon une déclaration de son avocat, a depuis été
    libéré, a ensuite été transféré dans une cellule plus grande.'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：根据其律师的声明，该男子后来已被释放，并随后转移到一个更大的牢房。
- en: –
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'ProMoT: Danach wurde der Mann, der mittlerweile freigelassen wurde, in eine
    größere Zelle verlegt.'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ProMoT: 此后，那名目前已被释放的男子被转移到一个更大的牢房。'
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: WebQuestions
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WebQuestions
- en: –
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Target: Milwaukee'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：密尔沃基
- en: –
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Pretrained: Milwaukee, Wisconsin'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练：密尔沃基，威斯康星州
- en: –
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Fine-tune: Where is harley davidson corporate headquarters? A: Milwaukee, Wisconsin
    Q: what movies has scarlett johansson in? A: Girl with a Pearl Earring Q: where
    is harley davidson corporate headquarters? A: Milwaukee, Wisconsin Q: where is
    harley davidson corporate headquarters? …'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：哈雷戴维森的公司总部在哪里？答：密尔沃基，威斯康星州 问：斯嘉丽·约翰逊参演了哪些电影？答：《戴珍珠耳环的少女》 问：哈雷戴维森的公司总部在哪里？答：密尔沃基，威斯康星州
    问：哈雷戴维森的公司总部在哪里？…
- en: –
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'ProMoT: Milwaukee, Wisconsin'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ProMoT: 密尔沃基，威斯康星州'
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: WikiLingua/en
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WikiLingua/en
- en: –
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Target: Ensure that you have never activated a free trial of Amazon Prime on
    your account in the past. Repeat the steps above to locate products that are fulfilled
    by Amazon. Click the button that says, …'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：确保你在过去从未在账户上激活过Amazon Prime的免费试用。重复上述步骤以查找由Amazon发货的产品。点击显示为“…”的按钮，
- en: –
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Pretrained: Click “Start my Free Trial.”Sign up with Amazon Prime. Submit your
    order.'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预训练：点击“开始我的免费试用。”注册Amazon Prime。提交订单。
- en: –
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Fine-tune: Si vous avez besoin d’une aide supplémentaire, n’hésitez pas à communiquer
    avec l’équipe d’Amazon Prime. Vous pouvez vous inscrire à l’Amazon Prime et vous
    inscrire à l’Amazon Prime. Vous pouvez vous inscrire …'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：如果你需要额外的帮助，请随时与Amazon Prime团队联系。你可以注册Amazon Prime并加入Amazon Prime。你可以注册…
- en: –
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'ProMoT: Click “Start my Free Trial.”Sign up with Amazon Prime. Enter your credit
    card details or use one of your saved payment methods. Submit your order. If you
    do not return, you will be charged $99 for a year membership to Amazon Prime at
    the end of your trial period'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ProMoT: 点击“开始我的免费试用。”注册Amazon Prime。输入你的信用卡信息或使用你的保存支付方式。提交订单。如果你不取消，你将被收取99美元的年费，作为Amazon
    Prime的会员费用。'
