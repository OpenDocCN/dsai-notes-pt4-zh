- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:40:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large
    Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LLM-Adapters: 一种参数高效的大型语言模型微调适配器家族'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.01933](https://ar5iv.labs.arxiv.org/html/2304.01933)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2304.01933](https://ar5iv.labs.arxiv.org/html/2304.01933)
- en: Zhiqiang Hu¹   Lei Wang²   Yihuai Lan   Wanyu Xu⁴   Ee-Peng Lim²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 胡志强¹   王磊²   蓝一怀   徐婉瑜⁴   林怡鹏²
- en: Lidong Bing³   Xing Xu⁵   Soujanya Poria¹   Roy Ka-Wei Lee¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 冯立东³   许兴⁵   苏江雅·波里亚¹   李凯伟¹
- en: ¹Singapore University of Technology and Design
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹新加坡科技设计大学
- en: ²Singapore Management University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²新加坡管理大学
- en: ³DAMO Academy, Alibaba Group
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³达摩学院，阿里巴巴集团
- en: ⁴Southwest Jiaotong University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴西南交通大学
- en: ⁵University of Electronic Science and Technology of China   Corresponding author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵电子科技大学   通讯作者。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The success of large language models (LLMs), like GPT-4 and ChatGPT, has led
    to the development of numerous cost-effective and accessible alternatives that
    are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor)
    or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based
    parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive
    topics, as it only requires fine-tuning a few external parameters instead of the
    entire LLMs while achieving comparable or even better performance. To enable further
    research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use
    framework that integrates various adapters into LLMs and can execute these adapter-based
    PEFT methods of LLMs for different tasks. The framework includes state-of-the-art
    open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters
    such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based
    methods. Moreover, we conduct extensive empirical studies on the impact of adapter
    types, placement locations, and hyper-parameters to the best design for each adapter-based
    methods. We evaluate the effectiveness of the adapters on fourteen datasets from
    two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning.
    The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B)
    with few extra trainable parameters yields comparable, and in some cases superior,
    performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如 GPT-4 和 ChatGPT 的成功，促使了许多经济高效且易于获取的替代方案的开发，这些方案是通过对开放获取的 LLM
    进行任务特定数据（例如，ChatDoctor）或指令数据（例如，Alpaca）的微调而创建的。在各种微调方法中，基于适配器的参数高效微调（PEFT）无疑是最吸引人的话题之一，因为它只需微调少量外部参数，而不是整个
    LLM，同时仍能达到类似或更好的性能。为了推动对 LLM 的 PEFT 方法的进一步研究，本文提出了 LLM-Adapters，这是一种易于使用的框架，它将各种适配器集成到
    LLM 中，并可以执行这些适配器基于的 PEFT 方法以应对不同的任务。该框架包括最先进的开放获取 LLM，如 LLaMA、BLOOM 和 GPT-J，以及广泛使用的适配器，如系列适配器、并行适配器、基于提示的学习和基于重新参数化的方法。此外，我们对适配器类型、放置位置和超参数对每种适配器方法的最佳设计的影响进行了广泛的实证研究。我们在来自两个不同推理任务（算术推理和常识推理）的十四个数据集上评估了适配器的有效性。结果表明，在较小规模的
    LLM（7B）上使用基于适配器的 PEFT，并且有少量额外的可训练参数，能够在零-shot 推理中达到与强大的 LLM（175B）相当的，甚至在某些情况下更优的性能。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs), such as ChatGPT OpenAI ([2022](#bib.bib26)) and
    GPT-4 OpenAI ([2023](#bib.bib27)), have demonstrated unprecedented performance
    across various natural language processing (NLP) tasks Qin et al. ([2023](#bib.bib30))
    and multi-modal tasks Shen et al. ([2023](#bib.bib35)). These LLMs often possess
    sizes exceeding hundreds of billions of parameters and are closed-source. Consequently,
    this has spurred the development of accessible and cost-effective alternatives
    such as LLaMA Touvron et al. ([2023](#bib.bib38)). These alternatives involve
    fine-tuning open-source LLMs utilizing either task-specific data (e.g., ChatDoctor Yunxiang
    et al. ([2023](#bib.bib44))) or instructional data (e.g., Alpaca Taori et al.
    ([2023](#bib.bib37))). However, full-model fine-tuning (FFT) is computationally
    and storage-intensive, thereby presenting significant challenges in practical
    implementation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如 ChatGPT OpenAI ([2022](#bib.bib26)) 和 GPT-4 OpenAI ([2023](#bib.bib27))，在各种自然语言处理（NLP）任务中展示了前所未有的性能 Qin
    et al. ([2023](#bib.bib30)) 和多模态任务 Shen et al. ([2023](#bib.bib35))。这些 LLMs 通常拥有超过数百亿参数且为闭源。因此，这催生了诸如
    LLaMA Touvron et al. ([2023](#bib.bib38)) 等可访问且具有成本效益的替代品。这些替代品涉及对开源 LLMs 的微调，利用任务特定数据（例如
    ChatDoctor Yunxiang et al. ([2023](#bib.bib44))) 或指令数据（例如 Alpaca Taori et al.
    ([2023](#bib.bib37)))。然而，完全模型微调（FFT）在计算和存储上非常昂贵，因此在实际应用中面临重大挑战。
- en: 'Prior to the emergence of FFT of LLMs (e.g., LLaMA), a compelling solution
    called parameter-efficient fine-tuning (PEFT) Houlsby et al. ([2019](#bib.bib15))
    has been proposed in the NLP field, specifically for pre-trained models (e.g.,
    BERT Devlin et al. ([2018](#bib.bib7))), offering a promising approach for efficiently
    fine-tuning LLMs. The advantage of PEFT lies in its ability to fine-tune only
    a small set of external parameters rather than the entire backbone model while
    still achieving comparable or even superior performance Mangrulkar et al. ([2022](#bib.bib23)).
    Moreover, PEFT can effectively mitigate catastrophic forgetting in comparison
    to FFT Wang et al. ([2022](#bib.bib43)). As shown in Table [1](#S2.T1 "Table 1
    ‣ 2 PEFT Overview ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning
    of Large Language Models"), the advantage of PEFT has resulted in the developing
    of diverse PEFT modules, encompassing series adapters Houlsby et al. ([2019](#bib.bib15));
    Wang et al. ([2022](#bib.bib43)); He et al. ([2022b](#bib.bib12)); Fu et al. ([2021](#bib.bib9)),
    parallel adapters He et al. ([2022a](#bib.bib11)), reparameterization-based methods Hu
    et al. ([2021](#bib.bib16)); Edalati et al. ([2022](#bib.bib8)), and prompt-based
    learning methods Lester et al. ([2021](#bib.bib20)); Li and Liang ([2021](#bib.bib21)).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLMs（例如 LLaMA）出现 FFT 之前，NLP 领域提出了一种称为参数高效微调（PEFT） Houlsby et al. ([2019](#bib.bib15))
    的有效解决方案，专门针对预训练模型（例如 BERT Devlin et al. ([2018](#bib.bib7)))，为高效微调 LLMs 提供了一种有前景的方法。PEFT
    的优势在于其只需微调一小部分外部参数，而不是整个主干模型，同时仍能实现相当或甚至更优的性能 Mangrulkar et al. ([2022](#bib.bib23))。此外，PEFT
    可以有效减轻相比于 FFT 的灾难性遗忘 Wang et al. ([2022](#bib.bib43))。如表 [1](#S2.T1 "Table 1 ‣
    2 PEFT Overview ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning
    of Large Language Models") 所示，PEFT 的优势导致了各种 PEFT 模块的开发，包括系列适配器 Houlsby et al.
    ([2019](#bib.bib15)); Wang et al. ([2022](#bib.bib43)); He et al. ([2022b](#bib.bib12));
    Fu et al. ([2021](#bib.bib9))，并行适配器 He et al. ([2022a](#bib.bib11))，基于重参数化的方法 Hu
    et al. ([2021](#bib.bib16)); Edalati et al. ([2022](#bib.bib8))，以及基于提示学习的方法 Lester
    et al. ([2021](#bib.bib20)); Li 和 Liang ([2021](#bib.bib21))。'
- en: By incorporating these PEFT modules into backbone models (i.e., LLMs), we can
    capitalize on the remarkable capabilities of backbone models without requiring
    extensive computational resources. This opens up opportunities for a broader range
    of applications, enabling even those with limited access to high-performance computing
    to harness the power of LLMs in their specific tasks. Despite the success of PEFT
    for pre-trained models, it remains unclear which PEFT module, in combination with
    which layer and hyperparameter configuration, is most suitable for a given task
    or dataset when meeting LLMs (e.g., LLaMA Touvron et al. ([2023](#bib.bib38))).
    Therefore, further investigation is needed to determine the optimal PEFT setup
    that maximizes performance across different tasks and datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些PEFT模块集成到主干模型（即LLM）中，我们可以利用主干模型的卓越能力，而无需大量计算资源。这为更广泛的应用打开了机会，使得即使是计算能力有限的用户也能在特定任务中发挥LLM的威力。尽管PEFT在预训练模型中的成功已得到证明，但仍不清楚哪种PEFT模块与哪一层和超参数配置结合最适合特定任务或数据集（如LLMA
    Touvron et al. ([2023](#bib.bib38)))。因此，需要进一步研究以确定能够最大化不同任务和数据集性能的最佳PEFT设置。
- en: 'Motivated by this, in this paper, we conduct a comprehensive empirical study
    of PEFT of three representative open-source LLMs, including BLOOM Muennighoff
    et al. ([2022](#bib.bib25)), GPT-J Wang and Komatsuzaki ([2021](#bib.bib41)),
    and LLaMA Touvron et al. ([2023](#bib.bib38)). Specifically, we undertake an empirical
    study to address the following three research questions: ($i$) What are the differences
    in performance between in-distribution (ID) and out-of-distribution (OOD) scenarios
    for PEFT methods? The findings of our study are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 受到此启发，在本文中，我们对三种代表性的开源LLM的PEFT进行了全面的实证研究，包括BLOOM Muennighoff et al. ([2022](#bib.bib25))，GPT-J
    Wang 和 Komatsuzaki ([2021](#bib.bib41))，以及LLaMA Touvron et al. ([2023](#bib.bib38))。具体来说，我们进行了一项实证研究，以解决以下三个研究问题：($i$)
    PEFT方法在分布内（ID）和分布外（OOD）场景中的性能差异是什么？我们的研究结果如下：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The optimal placement for the series adapter, parallel adapter, and LoRA is
    after the MLP layers, parallel with the MLP layers, and located after both the
    Attention layers and MLP layers simultaneously, respectively;
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系列适配器、并行适配器和LoRA的最佳放置位置分别是在MLP层之后、与MLP层并行以及同时位于Attention层和MLP层之后；
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Smaller language models with the PEFT approach can attain competitive or superior
    performance on specific tasks compared to larger language models. For instance,
    LLaMA-13B with LoRA can outperform GPT-3.5 (>175B) on MultiArith, AddSub, and
    SingleEq ;
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采用PEFT方法的小型语言模型在特定任务上可以获得与大型语言模型相竞争或更优的性能。例如，LLaMA-13B使用LoRA可以在MultiArith、AddSub和SingleEq任务上超越GPT-3.5（>175B）；
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The ID fine-tuned LLaMA-13B with adapters outperforms ChatGPT on commonsense
    reasoning tasks indicating that smaller language models have the potential to
    outperform larger language models on specific tasks with ID fine-tunig data.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ID微调的LLaMA-13B与适配器在常识推理任务中的表现优于ChatGPT，这表明较小的语言模型在特定任务中具有超越较大语言模型的潜力，尤其是使用ID微调数据时。
- en: 'Our contributions can be summarized as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct a comprehensive empirical study of various PEFT methods applied in
    different open-source LLMs.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对各种应用于不同开源LLM的PEFT方法进行了全面的实证研究。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To facilitate our empirical study, we construct two high-quality training datasets
    to enhance PEFT performance in math reasoning and commonsense reasoning tasks.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了促进我们的实证研究，我们构建了两个高质量的训练数据集，以提高在数学推理和常识推理任务中的PEFT性能。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a user-friendly framework, LLM-Adapter, seamlessly integrates diverse
    adapters into LLMs, empowering researchers to implement adapter-based PEFT methods
    for a wide range of tasks.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个用户友好的框架LLM-Adapter，它将各种适配器无缝集成到LLM中，使研究人员能够在广泛的任务中实现基于适配器的PEFT方法。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments to answer the three research questions to serve
    as inspiration for future research.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了广泛的实验，以回答三个研究问题，作为未来研究的灵感来源。
- en: 2 PEFT Overview
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 PEFT概述
- en: '![Refer to caption](img/bc4ffcadd125c7e5adfef3c1652908d4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bc4ffcadd125c7e5adfef3c1652908d4.png)'
- en: 'Figure 1: A detailed illustration of the model architectures of three different
    adapters: (a) Prefix-Tuning, (b) LoRA, (c) Series Adapter, and (d) Parallel Adapter.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：三种不同适配器的模型架构详细说明：（a）Prefix-Tuning，（b）LoRA，（c）Series Adapter，和（d）Parallel
    Adapter。
- en: '| Method | Prompt | Repara | Series | Parallel |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 提示 | Repara | Series | Parallel |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Prompt Tuning Lester et al. ([2021](#bib.bib20)) | $\surd$ |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Prompt Tuning Lester et al. ([2021](#bib.bib20)) | $\surd$ |  |  |  |'
- en: '| Prefix-Tuning Li and Liang ([2021](#bib.bib21)) | $\surd$ |  |  |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Prefix-Tuning Li 和 Liang ([2021](#bib.bib21)) | $\surd$ |  |  |  |'
- en: '| Spot Vu et al. ([2021](#bib.bib40)) | $\surd$ |  |  |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Spot Vu et al. ([2021](#bib.bib40)) | $\surd$ |  |  |  |'
- en: '| IPT Qin et al. ([2021](#bib.bib31)) | $\surd$ |  |  |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| IPT Qin et al. ([2021](#bib.bib31)) | $\surd$ |  |  |  |'
- en: '| LoRA Hu et al. ([2021](#bib.bib16)) |  | $\surd$ |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LoRA Hu et al. ([2021](#bib.bib16)) |  | $\surd$ |  |  |'
- en: '| KronA Edalati et al. ([2022](#bib.bib8)) |  | $\surd$ |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| KronA Edalati et al. ([2022](#bib.bib8)) |  | $\surd$ |  |  |'
- en: '| Adapters Houlsby et al. ([2019](#bib.bib15)) |  |  | $\surd$ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Adapters Houlsby et al. ([2019](#bib.bib15)) |  |  | $\surd$ |  |'
- en: '| AdaMix Wang et al. ([2022](#bib.bib43)) |  |  | $\surd$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| AdaMix Wang et al. ([2022](#bib.bib43)) |  |  | $\surd$ |  |'
- en: '| SparseAdapter He et al. ([2022b](#bib.bib12)) |  |  | $\surd$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SparseAdapter He et al. ([2022b](#bib.bib12)) |  |  | $\surd$ |  |'
- en: '| LeTS Fu et al. ([2021](#bib.bib9)) |  |  | $\surd$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| LeTS Fu et al. ([2021](#bib.bib9)) |  |  | $\surd$ |  |'
- en: '| Parallel Adapter He et al. ([2022a](#bib.bib11)) |  |  |  | $\surd$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Parallel Adapter He et al. ([2022a](#bib.bib11)) |  |  |  | $\surd$ |'
- en: '| MAM Adapter He et al. ([2021](#bib.bib10)) | $\surd$ |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MAM Adapter He et al. ([2021](#bib.bib10)) | $\surd$ |  |'
- en: '| UniPELT Mao et al. ([2021](#bib.bib24)) | $\surd$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| UniPELT Mao et al. ([2021](#bib.bib24)) | $\surd$ |  |'
- en: '| Compacter Henderson et al. ([2021](#bib.bib13)) |  | $\surd$ |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Compacter Henderson et al. ([2021](#bib.bib13)) |  | $\surd$ |  |  |'
- en: '| S4-model Chen et al. ([2023](#bib.bib3)) | $\surd$ |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| S4-model Chen et al. ([2023](#bib.bib3)) | $\surd$ |  |  |'
- en: 'Table 1: The PEFT methods are categorized based on the four common basic methods.
    "Prompt" represents prompt-based learning methods, "Repara" denotes reparametrization-based
    methods, "Series" is Series Adapter, while "Parallel" represents Parallel Adapter.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PEFT 方法根据四种常见的基本方法进行分类。“Prompt”代表基于提示的学习方法，“Repara”表示基于重参数化的方法，“Series”是系列适配器，而“Parallel”代表并行适配器。
- en: 'In this section, we provide a brief overview of four parameter-efficient fine-tuning
    (PEFT) methods: prompt-based learning, reparametrization-based methods, series
    adapters, and parallel adapters. Li and Liang ([2021](#bib.bib21)); Hu et al.
    ([2021](#bib.bib16)); Houlsby et al. ([2019](#bib.bib15)); He et al. ([2022a](#bib.bib11))'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要概述了四种参数高效微调（PEFT）方法：基于提示的学习、基于重参数化的方法、系列适配器和并行适配器。Li 和 Liang ([2021](#bib.bib21));
    Hu et al. ([2021](#bib.bib16)); Houlsby et al. ([2019](#bib.bib15)); He et al.
    ([2022a](#bib.bib11))
- en: Prompt-based learning.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于提示的学习。
- en: 'As shown in Figure [1](#S2.F1 "Figure 1 ‣ 2 PEFT Overview ‣ LLM-Adapters: An
    Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models")(a),
    prompt-based learning transforms the discrete optimization problem of finding
    the optimal hard prompt into a continuous (soft) prompt. To achieve this, Lester
    et al. ([2021](#bib.bib20)) proposed the concept of prompt tuning, where a trainable
    tensor is added as a prefix to the input embeddings. Another approach called Prefix
    TuningLi and Liang ([2021](#bib.bib21)) independently explored the addition of
    soft prompts to the hidden states of all layers. Intrinsic Prompt Tuning Qin et al.
    ([2021](#bib.bib31)) employs an autoencoder to compress and decompress the soft
    prompt. We take learnable vectors incorporated into the attention layer as an
    example of prompt-based learning, which can be formulated as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1](#S2.F1 "图 1 ‣ 2 PEFT 概述 ‣ LLM-Adapters：用于大规模语言模型参数高效微调的适配器家族")(a) 所示，基于提示的学习将找到最优硬提示的离散优化问题转变为连续（软）提示。为实现这一点，Lester
    et al. ([2021](#bib.bib20)) 提出了提示调整的概念，其中一个可训练的张量作为前缀添加到输入嵌入中。另一种方法称为 Prefix Tuning
    Li 和 Liang ([2021](#bib.bib21)) 独立探索了将软提示添加到所有层的隐藏状态中。Intrinsic Prompt Tuning
    Qin et al. ([2021](#bib.bib31)) 使用自编码器来压缩和解压软提示。我们以融入注意力层的可学习向量作为基于提示的学习的一个示例，其公式如下：
- en: '|  | $\displaystyle H_{o}=\text{Attn}(H_{i}W_{Q},[P_{K};H_{i}W_{K}],[P_{V};H_{i}W_{V}]),$
    |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{o}=\text{Attn}(H_{i}W_{Q},[P_{K};H_{i}W_{K}],[P_{V};H_{i}W_{V}]),$
    |  | (1) |'
- en: where $H_{i}\in\mathrm{R}^{T\times d}$ is the vector dimension. $P_{K}\in\mathrm{R}^{L\times
    d}$ denote the query, key, value vectors of th attention module, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{i}\in\mathrm{R}^{T\times d}$ 是向量维度。$P_{K}\in\mathrm{R}^{L\times d}$ 分别表示注意力模块的查询、键、值向量。
- en: Reparametrization-based method.
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于重参数化的方法。
- en: 'This type of methods aim to transform network weights using a low-rank technique.
    This approach effectively reduces the number of trainable parameters while preserving
    the ability to handle high-dimensional matrices. Intrinsic SAID Aghajanyan et al.
    ([2020](#bib.bib1)) investigates the intrinsic dimensionality of fine-tuning within
    a low-rank subspace. LoRA Hu et al. ([2021](#bib.bib16)) introduces a simple approach
    to update the parameters of a weight matrix by decomposing it into a product of
    two low-rank matrices. KronA Edalati et al. ([2022](#bib.bib8)) improves upon
    the matrix factorization aspect of LoRA by utilizing the Kronecker product in
    its technique. We take LoRA as an example of Reparametrization-based learning,
    which can be formulated below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法旨在通过低秩技术来转换网络权重。这种方法有效地减少了可训练参数的数量，同时保持处理高维矩阵的能力。**SAID** Aghajanyan 等人（[2020](#bib.bib1)）研究了在低秩子空间内微调的内在维度。**LoRA**
    Hu 等人（[2021](#bib.bib16)）提出了一种简单的方法，通过将权重矩阵分解为两个低秩矩阵的乘积来更新参数。**KronA** Edalati
    等人（[2022](#bib.bib8)）通过在其技术中利用克罗内克积改进了 LoRA 的矩阵分解方面。我们以 LoRA 为例来说明基于重新参数化的学习，其公式如下：
- en: '|  | $\displaystyle H_{o}=H_{i}W_{0}+H_{i}\Delta W=H_{i}W_{0}+H_{i}BA,$ |  |
    (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{o}=H_{i}W_{0}+H_{i}\Delta W=H_{i}W_{0}+H_{i}BA,$ |  |
    (2) |'
- en: where $W_{0}\in\mathrm{R}^{d\times d}$. $r\ll d$ is an important hyper-parameter
    for LoRA.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{0}\in\mathrm{R}^{d\times d}$。$r\ll d$ 是 LoRA 的一个重要超参数。
- en: Series Adapter.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**系列适配器**。'
- en: 'Series adapters involve incorporating additional learnable modules in a sequential
    manner within a specific sublayer. In their study, Houlsby et al. ([2019](#bib.bib15))
    proposed integrating fully-connected networks after the attention and FFN layers
    in the Transformer model Vaswani et al. ([2017](#bib.bib39)). Another finding
    by Pfeiffer et al. ([2020](#bib.bib29)) revealed that achieving comparable performance
    is possible by inserting the adapter solely after the self-attention layer, instead
    of using two adapters per transformer block. AdaMix (Wang et al., [2022](#bib.bib43))
    introduces a method that utilizes multiple series adapters in a mixture-of-experts
    (MoE) fashion. Compacter (Henderson et al., [2021](#bib.bib13)) utilizes the Kronecker
    product, low-rank matrices, and parameter sharing across layers to generate adapter
    weights. This technique aims to reduce the computational complexity associated
    with the adapters while maintaining their performance. Series Adapter can be formulated
    as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**系列适配器**涉及在特定子层内以顺序方式加入额外的可学习模块。在他们的研究中，**Houlsby** 等人（[2019](#bib.bib15)）提出在
    Transformer 模型**Vaswani** 等人（[2017](#bib.bib39)）的注意力和 FFN 层之后集成全连接网络。另一项研究由 **Pfeiffer**
    等人（[2020](#bib.bib29)）揭示，通过在自注意力层之后插入适配器即可实现相当的性能，而无需在每个 Transformer 块中使用两个适配器。**AdaMix**（**Wang**
    等人，[2022](#bib.bib43)）提出了一种方法，以混合专家（MoE）方式利用多个系列适配器。**Compacter**（**Henderson**
    等人，[2021](#bib.bib13)）利用克罗内克积、低秩矩阵以及跨层的参数共享来生成适配器权重。这项技术旨在减少与适配器相关的计算复杂性，同时保持其性能。**系列适配器**的公式如下：'
- en: '|  | $H_{o}\leftarrow H_{o}+f(H_{o}W_{down})W_{up},$ |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{o}\leftarrow H_{o}+f(H_{o}W_{down})W_{up},$ |  | (3) |'
- en: where the output $H_{o}$ to the original dimension $d$ in the experiment Section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 $H_{o}$ 的维度为实验部分中的原始维度 $d$。
- en: Parallel Adapter.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**并行适配器**。'
- en: 'Parallel adapters He et al. ([2022a](#bib.bib11)) aim to incorporate additional
    learnable modules in parallel with distinct sublayers within the backbone model.
    The parallel adapter can be formulated below:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行适配器** **He** 等人（[2022a](#bib.bib11)）旨在在骨干模型的不同子层中并行引入额外的可学习模块。并行适配器的公式如下：'
- en: '|  | $H_{o}\leftarrow H_{o}+f(H_{i}W_{down})W_{up},$ |  | (4) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{o}\leftarrow H_{o}+f(H_{i}W_{down})W_{up},$ |  | (4) |'
- en: where $H_{i}$) is the input (output) of a specific layer. Expanding on this
    concept, the Multi-head Parallel Adapter takes it a step further by using parallel
    adapters to modify the outputs of head attention. On the other hand, the Scaled
    Parallel Adapter is a variant that applies the composition and insertion format
    of LoRA Hu et al. ([2021](#bib.bib16)) to adapters. Another approach, called Ladder
    Side-Tuning Sung et al. ([2022](#bib.bib36)), involves training a lightweight
    ladder side network. This network accepts intermediate activations from the backbone
    networks through shortcut connections (ladders).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{i}$ 是特定层的输入（输出）。在此概念的基础上，多头并行适配器通过使用并行适配器来修改头部注意力的输出，从而更进一步地发展了这一概念。另一方面，缩放并行适配器是一种变体，它将LoRA
    Hu等人（[2021](#bib.bib16)）的组合和插入格式应用于适配器。另一种方法称为梯形侧调整 Sung等人（[2022](#bib.bib36)），涉及训练一个轻量级的梯形侧网络。该网络通过捷径连接（梯形）接受来自骨干网络的中间激活。
- en: 3 Experiment Setup
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 3.1 Benchmarks
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准测试
- en: 'We conduct extensive empirical studies on fourteen benchmark datasets from
    two categories of reasoning problems: Arithmetic Reasoning: (1) the GSM8K (Cobbe
    et al., [2021](#bib.bib6)) dataset consists of high quality linguistically diverse
    grade school math word problems created by human problem writers, (2) the SVAMP (Patel
    et al., [2021](#bib.bib28)) benchmark consists of one-unknown arithmetic word
    problems for up-to-4 grade level students by making simple changes to a set of
    problems from another existing dataset, (3) the MultiArith (Roy and Roth, [2016](#bib.bib32))
    dataset of math word problems requiring multiple reasoning steps and operations,
    (4) the AddSub Hosseini et al. ([2014](#bib.bib14)) dataset of addition and subtraction
    arithmetic word problems, (5) the AQuA (Ling et al., [2017](#bib.bib22)) dataset
    of algebraic word problems with natural language rationales, and (6) the SingleEq (Koncel-Kedziorski
    et al., [2015](#bib.bib18)) dataset of grade-school algebra word problems that
    map to single equations with varying length; Commonsense Reasoning: (1) the BoolQ
    Clark et al. ([2019](#bib.bib4)) dataset is a question-answering dataset for yes/no
    questions containing 15942 examples. These questions are naturally occurring and
    generated in unprompted and unconstrained settings, (2) the PIQA Bisk et al. ([2020](#bib.bib2))
    dataset of questions with two solutions requiring physical commonsense to answer,
    (3) the SIQA Sap et al. ([2019](#bib.bib34)) focuses on reasoning about people’s
    actions and their social implications, (4) the HellaSwag dataset of commonsense
    NLI questions including a context and several endings which complete the context,
    (5) the WinoGrande Sakaguchi et al. ([2021](#bib.bib33)) dataset is formulated
    as a fill-in-a-blank task with binary options, and the goal is to choose the right
    option for a given sentence which requires commonsense reasoning, (6) the ARC-c
    and (7) the ARC-e are the Challenge Set and Easy Set of ARC Clark et al. ([2018](#bib.bib5))
    dataset of genuine grade-school level, multiple-choice science questions, and
    (8) the OBQA dataset contains questions requiring multi-step reasoning, use of
    additional common and commonsense knowledge, and rich text comprehension. Table
    [2](#S3.T2 "Table 2 ‣ 3.1 Benchmarks ‣ 3 Experiment Setup ‣ LLM-Adapters: An Adapter
    Family for Parameter-Efficient Fine-Tuning of Large Language Models") shows the
    dataset statistics.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对来自两个推理问题类别的十四个基准数据集进行了广泛的实证研究：算术推理：（1）GSM8K (Cobbe et al., [2021](#bib.bib6))
    数据集由人工问题编写者创建，包含高质量、语言多样的学龄数学词汇题，（2）SVAMP (Patel et al., [2021](#bib.bib28)) 基准数据集包含通过对另一现有数据集中的问题进行简单更改，生成的适用于4年级学生的单一未知数算术词汇题，（3）MultiArith (Roy
    and Roth, [2016](#bib.bib32)) 数据集包含需要多个推理步骤和操作的数学词汇题，（4）AddSub Hosseini et al.
    ([2014](#bib.bib14)) 数据集包含加法和减法算术词汇题，（5）AQuA (Ling et al., [2017](#bib.bib22))
    数据集包含带有自然语言推理的代数词汇题，以及（6）SingleEq (Koncel-Kedziorski et al., [2015](#bib.bib18))
    数据集包含映射到单个方程的学龄代数词汇题，方程长度各异；常识推理：（1）BoolQ Clark et al. ([2019](#bib.bib4)) 数据集是一个用于是/否问题的问答数据集，包含15942个例子。这些问题是自然发生的，在没有提示和无约束的环境中生成的，（2）PIQA
    Bisk et al. ([2020](#bib.bib2)) 数据集包含需要物理常识回答的双解问题，（3）SIQA Sap et al. ([2019](#bib.bib34))
    专注于关于人类行为及其社会影响的推理，（4）HellaSwag 数据集包含常识自然语言推理问题，包括一个上下文和几个完成上下文的结尾，（5）WinoGrande
    Sakaguchi et al. ([2021](#bib.bib33)) 数据集被设定为填空任务，带有二元选项，目标是选择给定句子的正确选项，这需要常识推理，（6）ARC-c
    和（7）ARC-e 是 ARC Clark et al. ([2018](#bib.bib5)) 数据集中具有真正学龄水平的选择题科学问题的挑战集和简单集，以及（8）OBQA
    数据集包含需要多步骤推理、使用额外的常识和丰富文本理解的问题。表格 [2](#S3.T2 "Table 2 ‣ 3.1 Benchmarks ‣ 3 Experiment
    Setup ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of
    Large Language Models") 显示了数据集统计信息。'
- en: '| Dataset | Domain | # train | # test | Answer |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 领域 | 训练集数量 | 测试集数量 | 答案 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MultiArith | Math | - | 600 | Number |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| MultiArith | 数学 | - | 600 | 数字 |'
- en: '| AddSub | Math | - | 395 | Number |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| AddSub | 数学 | - | 395 | 数字 |'
- en: '| GSM8K | Math | 8.8K | 1,319 | Number |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 数学 | 8.8K | 1,319 | 数字 |'
- en: '| AQuA | Math | 100K | 254 | Option |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| AQuA | 数学 | 100K | 254 | 选项 |'
- en: '| SingleEq | Math | - | 508 | Number |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| SingleEq | 数学 | - | 508 | 数字 |'
- en: '| SVAMP | Math | - | 1,000 | Number |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| SVAMP | 数学 | - | 1,000 | 数字 |'
- en: '| BoolQ | CS | 9.4K | 3,270 | Yes/No |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | CS | 9.4K | 3,270 | 是/否 |'
- en: '| PIQA | CS | 16.1K | 1,830 | Option |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | CS | 16.1K | 1,830 | 选项 |'
- en: '| SIQA | CS | 33.4K | 1,954 | Option |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SIQA | CS | 33.4K | 1,954 | 选项 |'
- en: '| HellaSwag | CS | 39.9K | 10,042 | Option |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | CS | 39.9K | 10,042 | 选项 |'
- en: '| WinoGrande | CS | 63.2K | 1,267 | Option |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| WinoGrande | CS | 63.2K | 1,267 | 选项 |'
- en: '| ARC-e | CS | 1.1K | 2,376 | Option |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | CS | 1.1K | 2,376 | 选项 |'
- en: '| ARC-c | CS | 2.3K | 1,172 | Option |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | CS | 2.3K | 1,172 | 选项 |'
- en: '| OBQA | CS | 5.0K | 500 | Option |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | CS | 5.0K | 500 | 选项 |'
- en: 'Table 2: Details of datasets being evaluated. Math: arithmetic reasoning. CS:
    commonsense reasoning.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表格2：被评估的数据集详情。Math: 算术推理。CS: 常识推理。'
- en: 3.2 Fine-tuning Data Collection
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 精细调整数据收集
- en: 'In order to perform fine-tuning on adapters, we acquire two high-quality training
    datasets specifically designed for math reasoning and commonsense reasoning. Table
    [2](#S3.T2 "Table 2 ‣ 3.1 Benchmarks ‣ 3 Experiment Setup ‣ LLM-Adapters: An Adapter
    Family for Parameter-Efficient Fine-Tuning of Large Language Models") reveals
    that only GSM8K and AQuA datasets provide training sets for arithmetic reasoning.
    To enhance the diversity of our data, we incorporate the training sets from GSM8K,
    MAWPS, MAWPS-single Koncel-Kedziorski et al. ([2016](#bib.bib19)), and select
    1000 examples from AQuA for the purpose of collecting the fine-tuning data. However,
    it is worth noting that the chosen datasets solely offer equations and corresponding
    answers. In order to augment the reasoning capabilities of our model, particularly
    in terms of providing step-by-step rationales, we leverage ChatGPT as the teacher
    model. By utilizing zero-shot chain-of-thought prompts, ChatGPT generates reasoning
    steps. We have included the specific prompt templates used to collect the math
    reasoning dataset in Appendix [A.1](#A1.SS1 "A.1 Math Reasoning Prompt Templates
    ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models"). To ensure the quality of the data, we
    eliminate samples that contain incorrect answers. As a result, we obtain a set
    of 10K math reasoning samples, referred to as Math10K, which we consider for further
    analysis and fine-tuning.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '为了对适配器进行精细调整，我们获取了两个高质量的训练数据集，专门用于数学推理和常识推理。表格[2](#S3.T2 "Table 2 ‣ 3.1 Benchmarks
    ‣ 3 Experiment Setup ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models")显示，只有GSM8K和AQuA数据集提供了算术推理的训练集。为了增强数据的多样性，我们整合了来自GSM8K、MAWPS、MAWPS-single
    Koncel-Kedziorski等人（[2016](#bib.bib19)）的训练集，并从AQuA中选择了1000个示例来收集精细调整数据。然而，值得注意的是，所选数据集仅提供方程式及其对应答案。为了增强我们模型的推理能力，特别是在提供逐步推理的方面，我们利用ChatGPT作为教师模型。通过使用零-shot
    chain-of-thought提示，ChatGPT生成推理步骤。我们在附录[A.1](#A1.SS1 "A.1 Math Reasoning Prompt
    Templates ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models")中包含了用于收集数学推理数据集的具体提示模板。为了确保数据质量，我们删除了包含错误答案的样本。因此，我们获得了一组10K数学推理样本，称为Math10K，我们将对此进行进一步分析和精细调整。'
- en: 'To facilitate fine-tuning in the domain of commonsense reasoning, we construct
    fine-tuning data by formatting the training sets from BoolQ, PIQA, SIQA, HellaSwag,
    WinoGrande, ARC-e, ARC-c, and OBQA with pre-defined templates. As each dataset
    in the commonsense reasoning domain entails distinct tasks, we adopt a structured
    template by initially describing the task’s goal, followed by the corresponding
    content and answer. The template utilized for creating the fine-tuning data can
    be found in [A.2](#A1.SS2 "A.2 Commonsense Data Templates ‣ Appendix A Appendix
    ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large
    Language Models"). Upon completion of this process, we obtain a collection of
    170K commonsense reasoning samples, which we refer to as Commonsense170K. These
    datasets will be made publicly available to encourage further research and exploration
    in this area.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '为了方便在常识推理领域的精细调整，我们通过格式化BoolQ、PIQA、SIQA、HellaSwag、WinoGrande、ARC-e、ARC-c和OBQA的数据集，构建精细调整数据。由于常识推理领域中的每个数据集包含不同的任务，我们采用结构化模板，首先描述任务的目标，然后是相应的内容和答案。用于创建精细调整数据的模板可以在[A.2](#A1.SS2
    "A.2 Commonsense Data Templates ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter
    Family for Parameter-Efficient Fine-Tuning of Large Language Models")中找到。完成这一过程后，我们获得了一组170K常识推理样本，称为Commonsense170K。这些数据集将公开提供，以鼓励进一步研究和探索这一领域。'
- en: 3.3 Implementations
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实施
- en: To facilitate the seamless utilization of PEFT methods in both research and
    practical applications, we have developed a user-friendly framework, LLM-Adapter.
    LLM-Adapters seamlessly integrates diverse adapters into LLMs, empowering researchers
    to implement adapter-based PEFT methods for a wide range of tasks. We utilize
    LLaMA (7B, 13B) Touvron et al. ([2023](#bib.bib38)), BLOOMz (7B) Muennighoff et al.
    ([2022](#bib.bib25)), and GPT-J (6B) Wang and Komatsuzaki ([2021](#bib.bib41))
    as the base models for our experiments. As for the four categories of PEFT methods,
    we select Prefix-Tuning Li and Liang ([2021](#bib.bib21)), Series Adapter Houlsby
    et al. ([2019](#bib.bib15)), LoRA Hu et al. ([2021](#bib.bib16)), and Parallel
    adapter He et al. ([2022a](#bib.bib11)) as representative candidates to examine
    their efficacy. For consistency across all fine-tuning experiments, we maintain
    a batch size of 16\. The learning rate for Prefix-Tuning is set to 3e-2, while
    the rest of the methods adopt a learning rate of 3e-4\. Each of the PEFT methods
    is fine-tuned for three epochs on the fine-tuning datasets. It is important to
    note that we fine-tune a single model for either the math or commonsense reasoning
    task, and subsequently evaluate its performance across all corresponding datasets.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于在研究和实际应用中无缝使用PEFT方法，我们开发了一个用户友好的框架LLM-Adapter。LLM-Adapter无缝集成了多种适配器到LLMs中，使研究人员能够实施基于适配器的PEFT方法以应对各种任务。我们使用了LLaMA
    (7B, 13B) Touvron等人（[2023](#bib.bib38)）、BLOOMz (7B) Muennighoff等人（[2022](#bib.bib25)）和GPT-J
    (6B) Wang和Komatsuzaki（[2021](#bib.bib41)）作为我们实验的基础模型。对于四类PEFT方法，我们选择了Prefix-Tuning
    Li和Liang（[2021](#bib.bib21)）、Series Adapter Houlsby等人（[2019](#bib.bib15)）、LoRA
    Hu等人（[2021](#bib.bib16)）和Parallel adapter He等人（[2022a](#bib.bib11)）作为代表性候选方法来检查其有效性。为了在所有微调实验中保持一致性，我们维持批量大小为16。Prefix-Tuning的学习率设置为3e-2，而其余方法的学习率为3e-4。每种PEFT方法在微调数据集上微调三轮。值得注意的是，我们为数学或常识推理任务微调一个单一模型，然后在所有相关数据集上评估其性能。
- en: 4 Experiment Results
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果
- en: 4.1 Placement and Configuration
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 放置和配置
- en: 'To address the research question, “What is the optimal placement and configuration
    for various types of adapters?”, we employ LLaMA-7B as the base model to assess
    different adapter settings within the context of the math reasoning task. Our
    empirical study begins by determining the most effective placement for the Series
    Adapter, Parallel Adapter, and LoRA. Prefix-Tuning is excluded from this analysis
    since its placement is predetermined. For the Series Adapter, we explore its placement
    options after the multi-head attention layers, MLP layers, or both of them. As
    for the Parallel Adapter and LoRA, we integrate them into the multi-head attention
    layers, MLP layers, or both of them, in order to assess their respective performances.
    The detailed results on each dataset are shown in Appendix [A.3](#A1.SS3 "A.3
    Placement Analysis ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for
    Parameter-Efficient Fine-Tuning of Large Language Models"). Figure [2](#S4.F2
    "Figure 2 ‣ 4.1 Placement and Configuration ‣ 4 Experiment Results ‣ LLM-Adapters:
    An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models")
    shows the average accuracy on math reasoning datasets. We can observe that for
    the Series Adapter, the best position is to place it after the MLP layers, achieving
    an average accuracy of $59.5\%$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答研究问题，“各种适配器的最佳放置和配置是什么？”，我们采用LLaMA-7B作为基础模型来评估数学推理任务中不同适配器设置的效果。我们的实证研究首先确定Series
    Adapter、Parallel Adapter和LoRA的最有效放置位置。Prefix-Tuning被排除在此分析之外，因为其放置位置是预定的。对于Series
    Adapter，我们探索了其在多头注意力层、MLP层或两者之后的放置选项。对于Parallel Adapter和LoRA，我们将它们集成到多头注意力层、MLP层或两者中，以评估其各自的性能。每个数据集上的详细结果见附录
    [A.3](#A1.SS3 "A.3 放置分析 ‣ 附录 A ‣ LLM-Adapters：一种参数高效微调大型语言模型的适配器系列")。图 [2](#S4.F2
    "图2 ‣ 4.1 放置和配置 ‣ 4 实验结果 ‣ LLM-Adapters：一种参数高效微调大型语言模型的适配器系列") 显示了数学推理数据集上的平均准确率。我们可以观察到，对于Series
    Adapter，最佳位置是将其放置在MLP层之后，实现了$59.5\%$的平均准确率。
- en: '![Refer to caption](img/8c4a327602e8d0ded49344e8a959935a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8c4a327602e8d0ded49344e8a959935a.png)'
- en: 'Figure 2: The average accuracy of different adapter locations on math reasoning
    datasets.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同适配器位置在数学推理数据集上的平均准确率。
- en: '| ![Refer to caption](img/b18041b832ef56d2a48b7f7d47c4f8df.png) | ![Refer to
    caption](img/0bc840539960ccaab008eb4ff8ba9e21.png) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/b18041b832ef56d2a48b7f7d47c4f8df.png) | ![参见标题](img/0bc840539960ccaab008eb4ff8ba9e21.png)
    |'
- en: '| ![Refer to caption](img/f8a0948d2b6fb65d343b1b44ad2e80eb.png) | ![Refer to
    caption](img/e8c200e0822f9f44f30f2f53af76c17c.png) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/f8a0948d2b6fb65d343b1b44ad2e80eb.png) | ![参见说明](img/e8c200e0822f9f44f30f2f53af76c17c.png)
    |'
- en: 'Figure 3: The average accuracy of different variable settings on math reasoning
    datasets. Where "vt" refers to the number of virtual tokens, "bn" denotes the
    bottleneck size, while "r" is the LoRA rank.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同变量设置下在数学推理数据集上的平均准确性。其中，“vt”指的是虚拟标记的数量，“bn”表示瓶颈大小，而“r”是 LoRA 秩。
- en: '| LLM | Method | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP | Avg
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 方法 | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-3.5${}_{\text{175B}}$ | - | 83.8 | 56.4 | 85.3 | 38.9 | 88.1 | 69.9 |
    70.4 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5${}_{\text{175B}}$ | - | 83.8 | 56.4 | 85.3 | 38.9 | 88.1 | 69.9 |
    70.4 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BLOOMz${}_{\text{7B}}$ | Prefix | 68.8 | 13.8 | 47.1 | 12.5 | 49.4 | 24.1
    | 36.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| BLOOMz${}_{\text{7B}}$ | Prefix | 68.8 | 13.8 | 47.1 | 12.5 | 49.4 | 24.1
    | 36.0 |'
- en: '| Series | 80.7 | 14.3 | 72.6 | 20.5 | 69.3 | 38.1 | 49.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 80.7 | 14.3 | 72.6 | 20.5 | 69.3 | 38.1 | 49.3 |'
- en: '| Parallel | 85.8 | 18.5 | 77.7 | 18.9 | 74.8 | 36.4 | 52.0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 85.8 | 18.5 | 77.7 | 18.9 | 74.8 | 36.4 | 52.0 |'
- en: '| LoRA | 82.8 | 17.4 | 72.4 | 21.3 | 69.9 | 41.0 | 50.8 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 82.8 | 17.4 | 72.4 | 21.3 | 69.9 | 41.0 | 50.8 |'
- en: '| GPT-J${}_{\text{6B}}$ | Prefix | 74.5 | 16.0 | 65.6 | 14.7 | 61.4 | 31.0
    | 43.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J${}_{\text{6B}}$ | Prefix | 74.5 | 16.0 | 65.6 | 14.7 | 61.4 | 31.0
    | 43.9 |'
- en: '| Series | 91.7 | 19.5 | 85.8 | 15.0 | 81.7 | 43.6 | 56.2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 91.7 | 19.5 | 85.8 | 15.0 | 81.7 | 43.6 | 56.2 |'
- en: '| Parallel | 92.2 | 18.9 | 83.8 | 17.9 | 80.7 | 41.1 | 55.8 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 92.2 | 18.9 | 83.8 | 17.9 | 80.7 | 41.1 | 55.8 |'
- en: '| LoRA | 90.7 | 23.0 | 84.1 | 16.1 | 84.1 | 46.0 | 57.3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 90.7 | 23.0 | 84.1 | 16.1 | 84.1 | 46.0 | 57.3 |'
- en: '| LLaMA${}_{\text{7B}}$ | Prefix | 63.2 | 24.4 | 57.0 | 14.2 | 55.3 | 38.1
    | 42.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{7B}}$ | Prefix | 63.2 | 24.4 | 57.0 | 14.2 | 55.3 | 38.1
    | 42.0 |'
- en: '| Series | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
- en: '| Parallel | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
- en: '| LoRA | 95.0 | 37.5 | 83.3 | 18.9 | 84.4 | 52.1 | 61.9 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 95.0 | 37.5 | 83.3 | 18.9 | 84.4 | 52.1 | 61.9 |'
- en: '| LLaMA${}_{\text{13B}}$ | Prefix | 72.2 | 31.1 | 56.0 | 15.7 | 62.8 | 41.4
    | 46.5 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$ | Prefix | 72.2 | 31.1 | 56.0 | 15.7 | 62.8 | 41.4
    | 46.5 |'
- en: '| Series | 93.0 | 44.0 | 80.5 | 22.0 | 87.6 | 50.8 | 63.0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 93.0 | 44.0 | 80.5 | 22.0 | 87.6 | 50.8 | 63.0 |'
- en: '| Parallel | 94.3 | 43.3 | 83.0 | 20.5 | 89.6 | 55.7 | 64.4 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 94.3 | 43.3 | 83.0 | 20.5 | 89.6 | 55.7 | 64.4 |'
- en: '| LoRA | 94.8 | 47.5 | 87.3 | 18.5 | 89.8 | 54.6 | 65.4 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 94.8 | 47.5 | 87.3 | 18.5 | 89.8 | 54.6 | 65.4 |'
- en: 'Table 3: Accuracy comparison of LLMs with different adapters on six math reasoning
    datasets. We use GPT-3.5 text-Davinci-003 for Zero-shot CoT Kojima et al. ([2022](#bib.bib17))
    as the baseline.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同适配器的 LLM 在六个数学推理数据集上的准确性比较。我们使用 GPT-3.5 text-Davinci-003 作为 Zero-shot
    CoT Kojima 等人 ([2022](#bib.bib17)) 的基线。
- en: In order to determine the optimal configuration of various adapters, we conduct
    an analysis of the most crucial variable for each type of the PEFT methods. We
    compare the average accuracy on math reasoning datasets. The placement of adapters
    follows the optimal settings derived from the placement analysis. Regarding Prefix-tuning,
    we assess the performance with different numbers of virtual tokens ($vt$. For
    LoRA, we examine the influence of different rank values ($r$ $61.9\%$.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定各种适配器的最佳配置，我们分析了每种 PEFT 方法的最重要变量。我们比较了数学推理数据集上的平均准确性。适配器的放置遵循从放置分析中得出的最佳设置。关于
    Prefix-tuning，我们评估了不同虚拟标记数量（`vt`）的性能。对于 LoRA，我们检查了不同秩值（`r`）的影响，最高达到 61.9%。
- en: Based on our comprehensive placement and configuration analysis, we have determined
    the optimal settings for each adapter, which will be consistently employed throughout
    the subsequent experiments.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的综合部署和配置分析，我们已经确定了每个适配器的最佳设置，并将在随后的实验中一致使用这些设置。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For Prefix-Tuning, we establish the number of virtual tokens at 10.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 Prefix-Tuning，我们将虚拟标记的数量设置为 10。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For Series and Parallel Adapter, we seamlessly incorporate them into the MLP
    layers, configuring the bottleneck size to 256.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于系列和并行适配器，我们将它们无缝集成到 MLP 层中，将瓶颈大小配置为 256。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regarding LoRA, we seamlessly integrate it into both the Multi-head Attention
    layers and the MLP layers with rank 32.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于 LoRA，我们将其无缝集成到 Multi-head Attention 层和 MLP 层中，秩为 32。
- en: 4.2 Arithmetic Reasoning
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 算术推理
- en: '| LLM | Method | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c
    | OBQA | Avg |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 方法 | BoolQ | PIQA | SIQA | HellaSwag | WinoGrande | ARC-e | ARC-c |
    OBQA | 平均 |'
- en: '| GPT-3${}_{\text{175B}}$ | - | 60.5 | 81.0 | - | 78.9 | 70.2 | 68.8 | 51.4
    | 57.6 | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3${}_{\text{175B}}$ | - | 60.5 | 81.0 | - | 78.9 | 70.2 | 68.8 | 51.4
    | 57.6 | - |'
- en: '| PaLM${}_{\text{540B}}$ | - | 88.0 | 82.3 | - | 83.4 | 81.1 | 76.6 | 53.0
    | 53.4 | - |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| PaLM${}_{\text{540B}}$ | - | 88.0 | 82.3 | - | 83.4 | 81.1 | 76.6 | 53.0
    | 53.4 | - |'
- en: '| ChatGPT | - | 73.1 | 85.4 | 68.5 | 78.5 | 66.1 | 89.8 | 79.9 | 74.8 | 77.0
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | - | 73.1 | 85.4 | 68.5 | 78.5 | 66.1 | 89.8 | 79.9 | 74.8 | 77.0
    |'
- en: '| BLOOMz${}_{\text{7B}}$ | Prefix | 45.6 | 53.7 | 46.3 | 26.7 | 49.5 | 52.1
    | 39.7 | 44.3 | 44.7 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| BLOOMz${}_{\text{7B}}$ | 前缀 | 45.6 | 53.7 | 46.3 | 26.7 | 49.5 | 52.1 | 39.7
    | 44.3 | 44.7 |'
- en: '| Series | 65.4 | 70.4 | 73.6 | 53.4 | 69.3 | 72.3 | 55.9 | 68.0 | 66.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 65.4 | 70.4 | 73.6 | 53.4 | 69.3 | 72.3 | 55.9 | 68.0 | 66.0 |'
- en: '| Parallel | 64.1 | 71.5 | 72.1 | 52.9 | 67.0 | 70.5 | 54.7 | 69.6 | 65.3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 64.1 | 71.5 | 72.1 | 52.9 | 67.0 | 70.5 | 54.7 | 69.6 | 65.3 |'
- en: '| LoRA | 65.9 | 75.3 | 74.5 | 57.3 | 72.5 | 74.6 | 57.8 | 73.4 | 68.9 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 65.9 | 75.3 | 74.5 | 57.3 | 72.5 | 74.6 | 57.8 | 73.4 | 68.9 |'
- en: '| GPT-J${}_{\text{6B}}$ | Prefix | 63.1 | 66.9 | 68.7 | 34.4 | 64.5 | 64.4
    | 46.8 | 59.0 | 58.5 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J${}_{\text{6B}}$ | 前缀 | 63.1 | 66.9 | 68.7 | 34.4 | 64.5 | 64.4 | 46.8
    | 59.0 | 58.5 |'
- en: '| Series | 62.1 | 63.5 | 72.3 | 30.6 | 68.0 | 63.9 | 48.1 | 63.8 | 59.0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 62.1 | 63.5 | 72.3 | 30.6 | 68.0 | 63.9 | 48.1 | 63.8 | 59.0 |'
- en: '| Parallel | 62.2 | 69.7 | 70.0 | 41.7 | 65.0 | 60.2 | 44.6 | 58.2 | 59.0 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 62.2 | 69.7 | 70.0 | 41.7 | 65.0 | 60.2 | 44.6 | 58.2 | 59.0 |'
- en: '| LoRA | 62.4 | 68.6 | 49.5 | 43.1 | 57.3 | 43.4 | 31.0 | 46.6 | 50.2 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 62.4 | 68.6 | 49.5 | 43.1 | 57.3 | 43.4 | 31.0 | 46.6 | 50.2 |'
- en: '| LLaMA${}_{\text{7B}}$ | Prefix | 64.3 | 76.8 | 73.9 | 42.1 | 72.1 | 72.9
    | 54.0 | 60.6 | 64.6 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{7B}}$ | 前缀 | 64.3 | 76.8 | 73.9 | 42.1 | 72.1 | 72.9 | 54.0
    | 60.6 | 64.6 |'
- en: '| Series | 63.0 | 79.2 | 76.3 | 67.9 | 75.7 | 74.5 | 57.1 | 72.4 | 70.8 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 63.0 | 79.2 | 76.3 | 67.9 | 75.7 | 74.5 | 57.1 | 72.4 | 70.8 |'
- en: '| Parallel | 67.9 | 76.4 | 78.8 | 69.8 | 78.9 | 73.7 | 57.3 | 75.2 | 72.3 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 67.9 | 76.4 | 78.8 | 69.8 | 78.9 | 73.7 | 57.3 | 75.2 | 72.3 |'
- en: '| LoRA | 68.9 | 80.7 | 77.4 | 78.1 | 78.8 | 77.8 | 61.3 | 74.8 | 74.7 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 68.9 | 80.7 | 77.4 | 78.1 | 78.8 | 77.8 | 61.3 | 74.8 | 74.7 |'
- en: '| LLaMA${}_{\text{13B}}$ | Prefix | 65.3 | 75.4 | 72.1 | 55.2 | 68.6 | 79.5
    | 62.9 | 68.0 | 68.4 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$ | 前缀 | 65.3 | 75.4 | 72.1 | 55.2 | 68.6 | 79.5 | 62.9
    | 68.0 | 68.4 |'
- en: '| Series | 71.8 | 83.0 | 79.2 | 88.1 | 82.4 | 82.5 | 67.3 | 81.8 | 79.5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 系列 | 71.8 | 83.0 | 79.2 | 88.1 | 82.4 | 82.5 | 67.3 | 81.8 | 79.5 |'
- en: '| Parallel | 72.5 | 84.8 | 79.8 | 92.1 | 84.7 | 84.2 | 71.2 | 82.4 | 81.5 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | 72.5 | 84.8 | 79.8 | 92.1 | 84.7 | 84.2 | 71.2 | 82.4 | 81.5 |'
- en: '| LoRA | 72.1 | 83.5 | 80.5 | 90.5 | 83.7 | 82.8 | 68.3 | 82.4 | 80.5 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 72.1 | 83.5 | 80.5 | 90.5 | 83.7 | 82.8 | 68.3 | 82.4 | 80.5 |'
- en: 'Table 4: Accuracy comparison of LLMs with different adapters on eight commonsense
    reasoning datasets. The ChatGPT results are obtained by Zero-shot CoT with gpt-3.5-turbo
    API.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同适配器的 LLM 在八个常识推理数据集上的准确性比较。ChatGPT 结果是通过 gpt-3.5-turbo API 的 Zero-shot
    CoT 获得的。
- en: 'In order to evaluate the effectiveness of adapters on the Arithmetic Reasoning
    task, we conducted a study where adapters are fine-tuned on the Math10K dataset
    and subsequently evaluated on six different math reasoning datasets. As our baseline,
    we utilize the GPT-3.5 model, specifically the text-Davinci-003 variant, for Zero-shot
    CoT according to Kojima et al. ([2022](#bib.bib17)). The results of the GPT-3.5
    model can be found in Wang et al. ([2023](#bib.bib42)). Table [3](#S4.T3 "Table
    3 ‣ 4.1 Placement and Configuration ‣ 4 Experiment Results ‣ LLM-Adapters: An
    Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models")
    reports the performance of different PEFT methods and the baseline. On average,
    the GPT-3.5 model (175B) outperforms adapter-based PEFT LLMs in terms of accuracy.
    However, for simpler math reasoning datasets such as MultiArith, AddSub, and SingleEq,
    adapter-based methods like LLaMA-13B with LoRA outperform GPT-3.5\. Notably, LLaMA-13B
    with LoRA achieves an average accuracy of 65.4%, which is approximately 92.8%
    of the performance exhibited by GPT-3.5\. This suggests that with sufficient task-specific
    training data, adapter-based PEFT of smaller LLMs has the potential to achieve
    performance comparable to that of extremely large language models. The utilization
    of adapter-based PEFT yields superior performance by smaller language models compared
    to GPT-3.5 specifically in simpler tasks such as MultiArith, AddSub, and SingleEq.
    However, challenges persist in more complex tasks like GSM8K and SVAMP, which
    require a higher level of language comprehension and proficiency from the underlying
    base model, thereby resulting in a discernible performance gap. Regarding the
    different adapters employed, LoRA achieves remarkable performance while utilizing
    significantly fewer trainable parameters. This implies that excessive learnable
    parameters may not be necessary for task-specific fine-tuning. Overall, these
    findings demonstrate the potential for adapter-based PEFT of smaller LLMs to achieve
    high performance on specific tasks with few trainable parameters.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估适配器在算术推理任务中的有效性，我们进行了一项研究，其中适配器在Math10K数据集上进行了微调，然后在六个不同的数学推理数据集上进行了评估。作为基准，我们使用GPT-3.5模型，特别是text-Davinci-003变体，根据Kojima等人（[2022](#bib.bib17)）。GPT-3.5模型的结果可以在Wang等人（[2023](#bib.bib42)）中找到。表格[3](#S4.T3
    "Table 3 ‣ 4.1 Placement and Configuration ‣ 4 Experiment Results ‣ LLM-Adapters:
    An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models")报告了不同PEFT方法和基准的性能。平均而言，GPT-3.5模型（175B）在准确性方面优于基于适配器的PEFT
    LLM。然而，对于MultiArith、AddSub和SingleEq等较简单的数学推理数据集，像LLaMA-13B配合LoRA这样的基于适配器的方法的表现超过了GPT-3.5。值得注意的是，LLaMA-13B配合LoRA实现了65.4%的平均准确率，约为GPT-3.5表现的92.8%。这表明，具备足够任务特定训练数据的基于适配器的PEFT较小LLM有潜力达到与极大语言模型相当的性能。基于适配器的PEFT在较小的语言模型上实现了优于GPT-3.5的性能，尤其是在MultiArith、AddSub和SingleEq等简单任务中。然而，在需要更高语言理解和能力的复杂任务如GSM8K和SVAMP中仍存在挑战，从而导致明显的性能差距。关于所使用的不同适配器，LoRA在使用显著更少的可训练参数的情况下实现了卓越的性能。这意味着过多的可训练参数可能并不必要用于任务特定的微调。总体而言，这些发现展示了基于适配器的PEFT在少量可训练参数下，较小LLM在特定任务中实现高性能的潜力。'
- en: '| Question | Mr Boarden is remodeling his bathroom. For every square foot,
    he needs 24 mosaic tiles. How many mosaic tiles would Mr Boarden need to cover
    two thirds of his 36 sq ft bathroom? |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 博登先生正在重新装修他的浴室。每平方英尺需要24块马赛克瓷砖。博登先生需要多少块马赛克瓷砖才能覆盖他36平方英尺浴室的三分之二？ |'
- en: '| --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | 1\. Find two-thirds of 36: Multiply 36 by 2/3\. (36 * 2/3) = 24
    square feet 2.Determine the number of mosaic tiles needed per square foot: Given
    that Mr. Boarden needs 24 mosaic tiles per square foot, multiply the result from
    step 1 by 24\. (24 * 24) = 576 mosaic tiles Therefore, Mr. Boarden would need
    576 mosaic tiles to cover two-thirds of his 36 square foot bathroom. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 1. 计算36的三分之二：将36乘以2/3。 (36 * 2/3) = 24平方英尺 2. 确定每平方英尺所需的马赛克瓷砖数量：由于博登先生每平方英尺需要24块马赛克瓷砖，将步骤1的结果乘以24。
    (24 * 24) = 576块马赛克瓷砖 因此，博登先生需要576块马赛克瓷砖才能覆盖他36平方英尺浴室的三分之二。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1 square foot = 4 square inches, 1 square
    inch = 16 square centimeters. So, 1 sq ft = 16 sq cm. Therefore, 36 sq ft = 36
    x 16 sq cm = 576 sq cm. To cover two-thirds of the bathroom, Mr Boarden needs
    to cover 576 sq cm x 2/3 = 384 sq cm. Since 1 sq cm = 1 mosaic tile, Mr Boarden
    needs 384 mosaic tiles. Therefore, Mr Boarden needs 384 mosaic tiles to cover
    two-thirds of his 36 sq ft bathroom. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1 平方英尺 = 4 平方英寸，1 平方英寸 = 16 平方厘米。所以，1 平方英尺
    = 16 平方厘米。因此，36 平方英尺 = 36 x 16 平方厘米 = 576 平方厘米。为了覆盖浴室的三分之二，博登先生需要覆盖 576 平方厘米 x
    2/3 = 384 平方厘米。由于 1 平方厘米 = 1 马赛克瓷砖，博登先生需要 384 块马赛克瓷砖。因此，博登先生需要 384 块马赛克瓷砖来覆盖他的
    36 平方英尺浴室的三分之二。 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Find the area of the bathroom: 36 sq
    ft 2\. Find two-thirds of the area: 2/3 x 36 sq ft = 24 sq ft 3\. Multiply the
    area by the number of tiles needed per square foot: 24 sq ft x 24 tiles/sq ft
    = 576 tiles 4\. Therefore, Mr Boarden would need 576 mosaic tiles to cover two-thirds
    of his 36 sq ft bathroom. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 找出浴室的面积：36 平方英尺 2\. 找出三分之二的面积：2/3 x 36
    平方英尺 = 24 平方英尺 3\. 将面积乘以每平方英尺所需的瓷砖数量：24 平方英尺 x 每平方英尺 24 块瓷砖 = 576 块瓷砖 4\. 因此，博登先生需要
    576 块马赛克瓷砖来覆盖他的 36 平方英尺浴室的三分之一。 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. Calculate the total area of the bathroom:
    36 sq ft 2\. Determine how much area Mr Boarden wants to cover with mosaic tiles:
    2/3 x 36 sq ft = 24 sq ft 3\. Calculate the number of mosaic tiles needed: 24
    sq ft x 24 mosaic tiles per sq ft = 600 mosaic tiles Therefore, Mr Boarden would
    need 600 mosaic tiles to cover two thirds of his 36 sq ft bathroom. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. 计算浴室的总面积：36 平方英尺 2\. 确定博登先生想用马赛克瓷砖覆盖的面积：2/3
    x 36 平方英尺 = 24 平方英尺 3\. 计算所需的马赛克瓷砖数量：24 平方英尺 x 每平方英尺 24 块马赛克瓷砖 = 600 块马赛克瓷砖。因此，博登先生需要
    600 块马赛克瓷砖来覆盖他的 36 平方英尺浴室的三分之一。'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Calculate the total area of the bathroom:
    36 sq ft 2\. Determine how much area needs to be covered: 2/3 x 36 sq ft = 24
    sq ft 3\. Calculate the number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles
    per sq ft = 576 mosaic tiles Therefore, Mr Boarden would need 576 mosaic tiles
    to cover two thirds of his 36 sq ft bathroom. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 计算浴室的总面积：36 平方英尺 2\. 确定需要覆盖的面积：2/3 x 36
    平方英尺 = 24 平方英尺 3\. 计算所需的马赛克瓷砖数量：24 平方英尺 x 每平方英尺 24 块马赛克瓷砖 = 576 块马赛克瓷砖。因此，博登先生需要
    576 块马赛克瓷砖来覆盖他的 36 平方英尺浴室的三分之一。 |'
- en: 'Table 5: An example randomly sampled from GSM8K. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：从 GSM8K 随机抽取的一个示例。ChatGPT 和 LLaMA-13B 采用不同 PEFT 方法的输出结果。
- en: 4.3 Commonsense Reasoning
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 常识推理
- en: 'Additionally, we assess the efficacy of various PEFT methods for commonsense
    reasoning tasks. The adapters undergo fine-tuning using the Commonsense170K dataset.
    Our baseline models for commonsense reasoning include GPT-3 (175B), PaLM (540B),
    and ChatGPT. The results for GPT-3 and PaLM can be found in the study by Touvron
    et al. ([2023](#bib.bib38)). To evaluate ChatGPT’s performance in commonsense
    reasoning, we employ the gpt-3.5-turbo API with a zero-shot CoT. The zero-shot
    CoT prompts align with the template used for collecting our commonsense fine-tuning
    dataset, as outlined in Appendix [A.2](#A1.SS2 "A.2 Commonsense Data Templates
    ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models"). Table [4](#S4.T4 "Table 4 ‣ 4.2 Arithmetic
    Reasoning ‣ 4 Experiment Results ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models") presents the performance of the PEFT methods
    utilizing different LLMs alongside the baselines. Remarkably, LLaMA-13B with Series
    Adapter, Parallel Adapter, and LoRA outperform all the baselines, including ChatGPT,
    which has been hailed as the most impressive LLM to date. LLaMA-13B with Parallel
    Adapter achieves an average accuracy of 81.5%, representing a 4.5% improvement
    over ChatGPT. It is worth noting that all the training sets from the commonsense
    reasoning datasets are included in the fine-tuning data Commonsense170K. Furthermore,
    we observe that the performance of the PEFT methods is influenced by the underlying
    capabilities of the base models. LLaMA-7B and LLaMA-13B demonstrate superior commonsense
    reasoning abilities compared to the BLOOMz and GPT-J models.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们评估了各种PEFT方法在常识推理任务中的有效性。这些适配器使用Commonsense170K数据集进行微调。我们的常识推理基准模型包括GPT-3
    (175B)、PaLM (540B) 和 ChatGPT。GPT-3和PaLM的结果可以在Touvron等人的研究中找到 ([2023](#bib.bib38))。为了评估ChatGPT在常识推理中的表现，我们使用了gpt-3.5-turbo
    API，并结合了零-shot CoT。零-shot CoT提示与我们用于收集常识微调数据集的模板一致，如附录 [A.2](#A1.SS2 "A.2 Commonsense
    Data Templates ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models")中所述。表 [4](#S4.T4 "Table 4 ‣ 4.2 Arithmetic
    Reasoning ‣ 4 Experiment Results ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models") 展示了不同LLM与基准线相比使用PEFT方法的表现。值得注意的是，LLaMA-13B配备Series
    Adapter、Parallel Adapter和LoRA在所有基准线中表现突出，超越了被誉为迄今为止最令人印象深刻的LLM ChatGPT。LLaMA-13B配备Parallel
    Adapter实现了81.5%的平均准确率，比ChatGPT提高了4.5%。值得一提的是，所有常识推理数据集的训练集都包含在微调数据Commonsense170K中。此外，我们观察到PEFT方法的表现受基础模型的能力影响。LLaMA-7B和LLaMA-13B相比BLOOMz和GPT-J模型表现出更优越的常识推理能力。'
- en: 4.4 ID and OOD Analysis
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 ID 和 OOD 分析
- en: When comparing the performance of PEFT methods on math reasoning and commonsense
    reasoning tasks, we can observe that PEFT methods exhibit more remarkable results
    in the realm of commonsense reasoning. Moving forward, we will analyze the factors
    contributing to this phenomenon from both the in-distribution (ID) and out-of-distribution
    (OOD) perspectives. In the context of commonsense reasoning, the fine-tuning data
    set, Commonsense170K, encompasses all the training sets from the commonsense reasoning
    datasets. Notably, PEFT methods have demonstrated the ability to outperform ChatGPT.
    This observation implies that, by utilizing ID fine-tuning data, smaller language
    models like LLaMA-13B could surpass larger language models such as ChatGPT and
    PaLM in specific downstream tasks. However, when considering math reasoning tasks,
    the fine-tuning data set, Math10K, only includes the training sets of GSM8K and
    AQuA. In this regard, it has been observed that PEFT methods, particularly LLaMA-13B
    with LoRA, exhibit superior performance compared to GPT-3.5 on MultiArith, AddSub,
    and SingleEq. These findings suggest that PEFT methods can enhance the math reasoning
    abilities of LLMs and can be successfully applied to OOD datasets. Nonetheless,
    when evaluating the performance of PEFT methods on the ID datasets GSM8K and AQuA,
    a performance gap is still evident compared to GPT-3.5\. This discrepancy is likely
    due to the higher complexity of GSM8K and AQuA datasets in terms of math reasoning,
    while the reasoning capabilities of smaller LLMs remain limited. Consequently,
    identifying strategies to improve the performance of PEFT methods on complex math
    reasoning tasks represents a potential avenue for future research.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较PEFT方法在数学推理和常识推理任务中的表现时，我们可以观察到PEFT方法在常识推理领域表现更为突出。接下来，我们将从分布内（ID）和分布外（OOD）两方面深入分析导致这一现象的因素。在常识推理的背景下，微调数据集Commonsense170K包含了所有来自常识推理数据集的训练集。值得注意的是，PEFT方法表现出超越ChatGPT的能力。这一观察结果意味着，通过利用ID微调数据，像LLaMA-13B这样的小型语言模型可以在特定下游任务中超越像ChatGPT和PaLM这样的大型语言模型。然而，在数学推理任务中，微调数据集Math10K仅包括GSM8K和AQuA的训练集。在这方面，已经观察到PEFT方法，特别是带有LoRA的LLaMA-13B，在MultiArith、AddSub和SingleEq上表现优于GPT-3.5。这些发现表明PEFT方法可以提升LLMs的数学推理能力，并可以成功应用于OOD数据集。然而，在评估PEFT方法在ID数据集GSM8K和AQuA上的表现时，与GPT-3.5相比，仍然存在性能差距。这一差距可能由于GSM8K和AQuA数据集在数学推理方面的复杂性较高，而较小的LLMs的推理能力仍然有限。因此，确定改进PEFT方法在复杂数学推理任务中表现的策略是未来研究的一个潜在方向。
- en: 5 Qualitative Study
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 定性研究
- en: 'The previous sections have presented the quantitative analysis. In this section,
    we will provide qualitative examples to demonstrate the quality of outputs from
    different models. Table [5](#S4.T5 "Table 5 ‣ 4.2 Arithmetic Reasoning ‣ 4 Experiment
    Results ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning
    of Large Language Models") displays a randomly selected question from GSM8K along
    with the outputs of ChatGPT and LLaMA-13B models using various PEFT methods. More
    detailed examples can be found in Appendix [A.5](#A1.SS5 "A.5 Qualitative Examples
    ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models"). ChatGPT demonstrates a comprehensive understanding
    of the question and generates two steps, "(36 * 2/3) = 24 square feet" and "(24
    * 24) = 576 mosaic tiles," effectively solving the problem. However, the language
    understanding ability of LLaMA-13B-Prefix models is limited, leading LLaMA-13B-Prefix
    to take the wrong direction in the first step. On the other hand, LLaMA-13B with
    Series Adapter produces a high-quality answer by providing the crucial two steps
    and performing the correct calculations to obtain the accurate result. Interestingly,
    LLaMA-13B-Parallel and LLaMA-13B-LoRA generate almost identical rationales. However,
    LLaMA-13B-Parallel produces an incorrect answer due to a calculation error, stating
    "24 sq ft x 24 mosaic tiles per sq ft = 600 mosaic tiles". In general, when equipped
    with task-specific fine-tuning data, smaller language models like LLaMA-13B can
    generate impressive, high-quality answers that are comparable to those produced
    by ChatGPT.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的部分展示了定量分析。在这一部分，我们将提供定性示例，以展示不同模型输出的质量。表[5](#S4.T5 "表 5 ‣ 4.2 算术推理 ‣ 4 实验结果
    ‣ LLM-Adapters: 一种用于大规模语言模型参数高效微调的适配器家族")展示了从GSM8K中随机选择的一个问题，以及使用各种PEFT方法的ChatGPT和LLaMA-13B模型的输出。更多详细示例可以在附录[A.5](#A1.SS5
    "A.5 定性示例 ‣ 附录A 附录 ‣ LLM-Adapters: 一种用于大规模语言模型参数高效微调的适配器家族")中找到。ChatGPT展示了对问题的全面理解，生成了两个步骤：“(36
    * 2/3) = 24 平方英尺”和“(24 * 24) = 576 马赛克瓷砖”，有效地解决了问题。然而，LLaMA-13B-Prefix模型的语言理解能力有限，导致LLaMA-13B-Prefix在第一步中走错了方向。另一方面，带有Series
    Adapter的LLaMA-13B通过提供关键的两个步骤并进行正确的计算来获得准确结果，生成了高质量的答案。有趣的是，LLaMA-13B-Parallel和LLaMA-13B-LoRA生成了几乎相同的推理。然而，由于计算错误，LLaMA-13B-Parallel产生了不正确的答案，表述为“24平方英尺
    x 每平方英尺24块马赛克瓷砖 = 600块马赛克瓷砖”。总体而言，当配备任务特定的微调数据时，像LLaMA-13B这样较小的语言模型能够生成令人印象深刻的高质量答案，与ChatGPT生成的答案相媲美。'
- en: 6 Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we develop a user-friendly framework, LLM-Adapter, seamlessly
    integrates diverse adapters into LLMs, empowering researchers to implement adapter-based
    PEFT methods for a wide range of tasks. To evaluate different PEFT methods on
    downstream tasks, we construct two high-quality fine-tuning datasets to enhance
    PEFT performance on math reasoning and commonsense reasoning tasks. By utilizing
    the LLM-Adapter toolkit and the constructed fine-tuning datasets, we conduct a
    comprehensive empirical study and find the answer of research questions on the
    optimal placement and configuration of different PEFT methods, the impact of adapter
    architectures, and the influence of ID and OOD scenarios. We hope this work will
    encourage further research on PEFT methods for LLMs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们开发了一个用户友好的框架LLM-Adapter，能够无缝地将各种适配器集成到LLM中，赋能研究人员实现基于适配器的PEFT方法以应对广泛的任务。为了评估不同PEFT方法在下游任务上的表现，我们构建了两个高质量的微调数据集，以提高PEFT在数学推理和常识推理任务上的表现。通过利用LLM-Adapter工具包和构建的微调数据集，我们进行了全面的实证研究，找出了关于不同PEFT方法的最佳位置和配置、适配器架构的影响以及ID和OOD场景的影响的研究问题的答案。我们希望这项工作能鼓励进一步研究LLM的PEFT方法。
- en: 7 Limitations
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: There are two limitations to this work. Firstly, due to constrained computing
    resources, we were unable to evaluate the performance of larger language models
    such as LLaMA-33B and LLaMA-65B. It is anticipated that these larger models, possessing
    enhanced language understanding capabilities, would yield superior performance.
    Secondly, this paper does not delve into the exploration of combining different
    adapters. Given the extensive search space associated with the combination of
    various PEFT methods, we intend to explore this direction in future research endeavors.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作有两个限制。首先，由于计算资源有限，我们未能评估更大语言模型如LLaMA-33B和LLaMA-65B的性能。预计这些较大的模型具备增强的语言理解能力，将会产生更优的性能。其次，本文未深入探讨结合不同适配器的研究。鉴于不同PEFT方法组合的广泛搜索空间，我们计划在未来的研究中探讨这一方向。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aghajanyan et al. (2020) Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
    2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning.
    In *Annual Meeting of the Association for Computational Linguistics*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aghajanyan等人（2020年）Armen Aghajanyan, Luke Zettlemoyer, 和 Sonal Gupta。2020年。固有维度解释了语言模型微调的有效性。于*计算语言学协会年会*。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language.
    In *Thirty-Fourth AAAI Conference on Artificial Intelligence*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk等人（2020年）Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, 和 Yejin
    Choi。2020年。Piqa: 在自然语言中推理关于物理常识。于*第34届AAAI人工智能会议*。'
- en: Chen et al. (2023) Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola,
    and Diyi Yang. 2023. Parameter-efficient fine-tuning design spaces. *arXiv preprint
    arXiv:2301.01821*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2023年）Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, 和 Diyi
    Yang。2023年。参数高效的微调设计空间。*arXiv预印本 arXiv:2301.01821*。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. [BoolQ: Exploring the surprising
    difficulty of natural yes/no questions](https://doi.org/10.18653/v1/N19-1300).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 2924–2936, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark等人（2019年）Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova。2019年。[BoolQ: 探索自然是/否问题的惊人难度](https://doi.org/10.18653/v1/N19-1300)。在*2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长文和短文）*，页码2924–2936，明尼阿波利斯，明尼苏达州。计算语言学协会。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv:1803.05457v1*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等人（2018年）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord。2018年。认为你已经解决了问答问题？试试arc，AI2推理挑战。*arXiv:1803.05457v1*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton,
    Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers
    to solve math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人（2021年）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton,
    Reiichiro Nakano, Christopher Hesse, 和 John Schulman。2021年。训练验证器以解决数学文字问题。*arXiv预印本
    arXiv:2110.14168*。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin等人（2018年）Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。2018年。Bert:
    深度双向变换器的预训练以进行语言理解。*arXiv预印本 arXiv:1810.04805*。'
- en: 'Edalati et al. (2022) Ali Edalati, Marzieh S. Tahaei, Ivan Kobyzev, V. Nia,
    James J. Clark, and Mehdi Rezagholizadeh. 2022. Krona: Parameter efficient tuning
    with kronecker adapter. *ArXiv*, abs/2212.10650.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Edalati等人（2022年）Ali Edalati, Marzieh S. Tahaei, Ivan Kobyzev, V. Nia, James
    J. Clark, 和 Mehdi Rezagholizadeh。2022年。Krona: 使用Kronecker适配器的参数高效调优。*ArXiv*，abs/2212.10650。'
- en: 'Fu et al. (2021) Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen
    Zhao. 2021. [Learn-to-share: A hardware-friendly transfer learning framework exploiting
    computation and parameter sharing](https://proceedings.mlr.press/v139/fu21a.html).
    In *Proceedings of the 38th International Conference on Machine Learning*, volume
    139 of *Proceedings of Machine Learning Research*, pages 3469–3479\. PMLR.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2021) Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, 和 Jishen
    Zhao. 2021. [Learn-to-share: 一个硬件友好的迁移学习框架，利用计算和参数共享](https://proceedings.mlr.press/v139/fu21a.html)。收录于
    *第38届国际机器学习大会论文集*，第139卷，*机器学习研究论文集*，第3469–3479页。PMLR。'
- en: He et al. (2021) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer
    learning. *arXiv preprint arXiv:2110.04366*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2021) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    和 Graham Neubig. 2021. 迈向参数高效迁移学习的统一视角。*arXiv 预印本 arXiv:2110.04366*。
- en: He et al. (2022a) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. 2022a. [Towards a unified view of parameter-efficient transfer
    learning](https://openreview.net/forum?id=0RDcd5Axok). In *International Conference
    on Learning Representations*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022a) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    和 Graham Neubig. 2022a. [迈向参数高效迁移学习的统一视角](https://openreview.net/forum?id=0RDcd5Axok)。收录于
    *国际学习表示会议*。
- en: 'He et al. (2022b) Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng
    Tao. 2022b. [SparseAdapter: An easy approach for improving the parameter-efficiency
    of adapters](https://aclanthology.org/2022.findings-emnlp.160). In *Findings of
    the Association for Computational Linguistics: EMNLP 2022*, pages 2184–2190, Abu
    Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2022b) Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, 和 Dacheng
    Tao. 2022b. [SparseAdapter: 一种简单的方法来提高适配器的参数效率](https://aclanthology.org/2022.findings-emnlp.160)。收录于
    *计算语言学协会发现：EMNLP 2022*，第2184–2190页，阿布扎比，阿拉伯联合酋长国。计算语言学协会。'
- en: 'Henderson et al. (2021) James Henderson, Sebastian Ruder, et al. 2021. Compacter:
    Efficient low-rank hypercomplex adapter layers. In *Advances in Neural Information
    Processing Systems*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson et al. (2021) James Henderson, Sebastian Ruder 等. 2021. Compacter:
    高效的低秩超复数适配器层。收录于 *神经信息处理系统进展*。'
- en: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization.
    In *EMNLP*, pages 523–533.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni,
    和 Nate Kushman. 2014. 通过动词分类学习解决算术文字问题。收录于 *EMNLP*，第523–533页。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. 2019. Parameter-efficient transfer learning for nlp. In *International
    Conference on Machine Learning*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, 和 Sylvain Gelly.
    2019. NLP的参数高效迁移学习。收录于 *国际机器学习大会*。
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large
    language models. *ArXiv*, abs/2106.09685.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, 和 Weizhu Chen. 2021. Lora: 大型语言模型的低秩适配。*ArXiv*，abs/2106.09685。'
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *arXiv preprint arXiv:2205.11916*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, 和 Yusuke Iwasawa. 2022. 大型语言模型是零样本推理器。*arXiv 预印本 arXiv:2205.11916*。
- en: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word
    problems into equations. *Transactions of the Association for Computational Linguistics*,
    3:585–597.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
    Ashish Sabharwal, Oren Etzioni, 和 Siena Dumas Ang. 2015. 将代数文字问题解析为方程。*计算语言学协会会刊*，3:585–597。
- en: 'Koncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
    Nate Kushman, and Hannaneh Hajishirzi. 2016. [MAWPS: A math word problem repository](https://aclanthology.org/N16-1136).
    In *Proceedings of NAACL*, pages 1152–1157.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,
    Nate Kushman, 和 Hannaneh Hajishirzi. 2016. [MAWPS: 数学文字问题库](https://aclanthology.org/N16-1136).
    收录于 *NAACL 会议论文集*，第1152–1157页。'
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *ArXiv*, abs/2104.08691.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, 和 Noah Constant. 2021. 参数高效的提示调优的规模效应。*ArXiv*，abs/2104.08691。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. [Prefix-tuning: Optimizing
    continuous prompts for generation](https://doi.org/10.18653/v1/2021.acl-long.353).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 4582–4597, Online. Association for Computational
    Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li and Liang (2021) Xiang Lisa Li 和 Percy Liang. 2021. [Prefix-tuning: 优化连续提示生成](https://doi.org/10.18653/v1/2021.acl-long.353)。在
    *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）* 中，页面 4582–4597，在线。计算语言学协会。'
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    2017. Program induction by rationale generation: Learning to solve and explain
    algebraic word problems. In *Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 158–167.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, 和 Phil Blunsom. 2017.
    通过推理生成程序诱导：学习解决和解释代数文字问题。在 *第55届计算语言学协会年会（第1卷：长篇论文）* 中，页面 158–167.
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, and Sayak Paul. 2022. Peft: State-of-the-art parameter-efficient
    fine-tuning methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, 和 Sayak Paul. 2022. Peft: 最先进的参数高效微调方法。 [https://github.com/huggingface/peft](https://github.com/huggingface/peft).'
- en: 'Mao et al. (2021) Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao
    Ma, Jiawei Han, Wen tau Yih, and Madian Khabsa. 2021. Unipelt: A unified framework
    for parameter-efficient language model tuning. *ArXiv*, abs/2110.07577.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mao et al. (2021) Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao
    Ma, Jiawei Han, Wen tau Yih, 和 Madian Khabsa. 2021. Unipelt: 一个统一的参数高效语言模型调整框架。*ArXiv*,
    abs/2110.07577.'
- en: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask
    finetuning. *arXiv preprint arXiv:2211.01786*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, 等. 2022. 通过多任务微调实现跨语言泛化。*arXiv 预印本 arXiv:2211.01786*.
- en: OpenAI (2022) OpenAI. 2022. Introducing chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. 介绍 ChatGPT。 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
- en: OpenAI (2023) OpenAI. 2023. GPT-4 technical report. *CoRR*, abs/2303.08774.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。*CoRR*, abs/2303.08774.
- en: Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021.
    [Are NLP models really able to solve simple math word problems?](https://aclanthology.org/2021.naacl-main.168)
    In *Proceedings of NAACL*, pages 2080–2094.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, 和 Navin Goyal. 2021. [NLP
    模型真的能解决简单的数学文字问题吗？](https://aclanthology.org/2021.naacl-main.168) 在 *NAACL 会议论文集*
    中，页面 2080–2094.
- en: 'Pfeiffer et al. (2020) Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian
    Ruder. 2020. Mad-x: An adapter-based framework for multi-task cross-lingual transfer.
    In *Conference on Empirical Methods in Natural Language Processing*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer et al. (2020) Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, 和 Sebastian
    Ruder. 2020. Mad-x: 基于适配器的多任务跨语言迁移框架。在 *自然语言处理经验方法会议* 中。'
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing
    task solver? *arXiv preprint arXiv:2302.06476*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, 和 Diyi Yang. 2023. ChatGPT 是通用自然语言处理任务求解器吗？*arXiv 预印本 arXiv:2302.06476*.
- en: Qin et al. (2021) Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding,
    Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, et al. 2021. Exploring universal
    intrinsic task subspace via prompt tuning. *arXiv e-prints*, pages arXiv–2110.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2021) Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding,
    Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, 等. 2021. 通过提示调整探索通用内在任务子空间。*arXiv
    e-prints*, pages arXiv–2110.
- en: Roy and Roth (2016) Subhro Roy and Dan Roth. 2016. Solving general arithmetic
    word problems. *arXiv preprint arXiv:1608.01413*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy and Roth (2016) Subhro Roy 和 Dan Roth. 2016. 解决一般算术文字问题。*arXiv 预印本 arXiv:1608.01413*.
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. 2021. Winogrande: 大规模对抗性 Winograd 方案挑战。*Communications of the ACM*,
    64(9):99–106.'
- en: 'Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and
    Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions.
    *arXiv preprint arXiv:1904.09728*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, 和
    Yejin Choi. 2019. *Socialiqa: Commonsense reasoning about social interactions*。*arXiv
    preprint arXiv:1904.09728*。'
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving AI tasks with chatgpt and its
    friends in huggingface. *CoRR*, abs/2303.17580.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, 和 Yueting Zhuang. 2023. *Hugginggpt: Solving AI tasks with chatgpt and its
    friends in huggingface*。*CoRR*，abs/2303.17580。'
- en: 'Sung et al. (2022) Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder
    side-tuning for parameter and memory efficient transfer learning. *ArXiv*, abs/2206.06522.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sung et al. (2022) Yi-Lin Sung, Jaemin Cho, 和 Mohit Bansal. 2022. *Lst: Ladder
    side-tuning for parameter and memory efficient transfer learning*。*ArXiv*，abs/2206.06522。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. *Stanford
    alpaca: An instruction-following llama model*。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023. *Llama: Open and efficient foundation language
    models*。*arXiv preprint arXiv:2302.13971*。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in neural information processing systems*, pages
    5998–6008.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. *Attention
    is all you need*。收录于《*神经信息处理系统的进展*》，第5998–6008页。
- en: 'Vu et al. (2021) Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel
    Cer. 2021. Spot: Better frozen model adaptation through soft prompt transfer.
    *arXiv preprint arXiv:2110.07904*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vu et al. (2021) Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, 和 Daniel
    Cer. 2021. *Spot: Better frozen model adaptation through soft prompt transfer*。*arXiv
    preprint arXiv:2110.07904*。'
- en: 'Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:
    A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang and Komatsuzaki (2021) Ben Wang 和 Aran Komatsuzaki. 2021. *GPT-J-6B: A
    6 Billion Parameter Autoregressive Language Model*。 [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)。'
- en: 'Wang et al. (2023) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. *arXiv preprint arXiv:2305.04091*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, 和 Ee-Peng Lim. 2023. *Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models*。*arXiv preprint arXiv:2305.04091*。'
- en: 'Wang et al. (2022) Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, and Jianfeng Gao. 2022. Adamix: Mixture-of-adapter for
    parameter-efficient tuning of large language models. *ArXiv*, abs/2205.12410.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao,
    Ahmed Hassan Awadallah, 和 Jianfeng Gao. 2022. *Adamix: Mixture-of-adapter for
    parameter-efficient tuning of large language models*。*ArXiv*，abs/2205.12410。'
- en: 'Yunxiang et al. (2023) Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang
    You. 2023. Chatdoctor: A medical chat model fine-tuned on llama model using medical
    domain knowledge. *arXiv preprint arXiv:2303.14070*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yunxiang et al. (2023) Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, 和 Zhang
    You. 2023. *Chatdoctor: A medical chat model fine-tuned on llama model using medical
    domain knowledge*。*arXiv preprint arXiv:2303.14070*。'
- en: Appendix A Appendix
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Math Reasoning Prompt Templates
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数学推理提示模板
- en: 'We utilize ChatGPT to collect the math reasoning data for fine-tuning. Table
    [6](#A1.T6 "Table 6 ‣ A.1 Math Reasoning Prompt Templates ‣ Appendix A Appendix
    ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large
    Language Models") show the prompt template used to query ChatGPT. The expression
    "Please give the steps" is employed to guide ChatGPT to generate reasoning steps,
    thus, we can use the rationale information to fine-tune adapters. "Give the arabic
    numerals as the answer." is utilized to guide ChatGPT to generate arabic numbers
    as the final answer making it easier to extract the answer from the outputs.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '我们利用ChatGPT收集数学推理数据以进行微调。表[6](#A1.T6 "Table 6 ‣ A.1 Math Reasoning Prompt Templates
    ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models")展示了用于查询ChatGPT的提示模板。表达“请给出步骤”用于引导ChatGPT生成推理步骤，从而可以使用推理信息来微调适配器。“给出阿拉伯数字作为答案”用于引导ChatGPT生成阿拉伯数字作为最终答案，使得从输出中提取答案更加容易。'
- en: '| Template | [QUESTION] Please give the steps and the arabic numerals as the
    answer. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Template | [QUESTION] 请给出步骤和阿拉伯数字作为答案。 |'
- en: '| --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Prompt | Albert is wondering how much pizza he can eat in one day. He buys
    2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza
    has 8 slices. If he eats it all, how many pieces does he eat that day? Please
    give the steps and the arabic numerals as the answer. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Prompt | 阿尔伯特想知道他一天能吃多少比萨饼。他买了2个大比萨饼和2个小比萨饼。一个大比萨饼有16片，一个小比萨饼有8片。如果他吃完所有的比萨饼，他那天吃了多少片？请给出步骤和阿拉伯数字作为答案。
    |'
- en: '| ChatGPT Output | To solve the problem, we need to first find out how many
    slices are in 2 large pizzas and 2 small pizzas: 2 large pizzas x 16 slices per
    pizza = 32 slices 2 small pizzas x 8 slices per pizza = 16 slices. Next, we add
    the number of slices in each size pizza to find the total number of slices: 32
    slices + 16 slices = 48 slices. Therefore, if Albert eats all 2 large pizzas and
    2 small pizzas, he will eat 48 slices of pizza that day. Answer: 48 slices. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT 输出 | 要解决这个问题，我们首先需要找出2个大比萨饼和2个小比萨饼中有多少片：2个大比萨饼 × 每个比萨饼16片 = 32片 2个小比萨饼
    × 每个比萨饼8片 = 16片。接下来，我们将每种大小的比萨饼的片数相加，以找出总片数：32片 + 16片 = 48片。因此，如果阿尔伯特吃掉所有的2个大比萨饼和2个小比萨饼，他那天将吃48片比萨饼。答案：48片。
    |'
- en: 'Table 6: The prompt template used to collect math reasoning dataset for fine-tuning.
    An example from GSM8K is also included.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用于收集数学推理数据集以进行微调的提示模板。还包括了来自GSM8K的一个示例。
- en: A.2 Commonsense Data Templates
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 常识数据模板
- en: 'As each dataset in the commonsense reasoning domain entails distinct tasks,
    we adopt a structured template by initially describing the task’s goal, followed
    by the corresponding content and answer. Table [7](#A1.T7 "Table 7 ‣ A.2 Commonsense
    Data Templates ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient
    Fine-Tuning of Large Language Models") shows the templates used to collect commonsense
    reasoning data for fine-tuning.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '由于常识推理领域中的每个数据集涉及不同的任务，我们采用了一个结构化的模板，首先描述任务的目标，然后是相应的内容和答案。表[7](#A1.T7 "Table
    7 ‣ A.2 Commonsense Data Templates ‣ Appendix A Appendix ‣ LLM-Adapters: An Adapter
    Family for Parameter-Efficient Fine-Tuning of Large Language Models")展示了用于收集常识推理数据以进行微调的模板。'
- en: '| Dataset | Fine-tuning Data Template |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | 微调数据模板 |'
- en: '| --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| BoolQ | Please answer the following question with true or false, question:
    [QUESTION] Answer format: true/false the correct answer is [ANSWER] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 请用“真”或“假”回答以下问题，问题：[QUESTION] 答案格式：真/假 正确答案是[ANSWER] |'
- en: '| PIQA | Please choose the correct solution to the question: [QUESTION] Solution1:
    [SOLUTION_1] Solution2: [SOLUTION_2] Answer format: solution1/solution2 the correct
    answer is [ANSWER] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 请选择问题的正确解决方案：[QUESTION] 解决方案1：[SOLUTION_1] 解决方案2：[SOLUTION_2] 答案格式：解决方案1/解决方案2
    正确答案是[ANSWER] |'
- en: '| SIQA | Please choose the correct answer to the question: [QUESTION] Answer1:
    [ANSWER_1] Answer2: [ANSWER_2] Answer3: [ANSWER_3] Answer format: answer1/answer2/answer3
    the correct answer is [ANSWER] |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| SIQA | 请选择问题的正确答案：[QUESTION] 答案1：[ANSWER_1] 答案2：[ANSWER_2] 答案3：[ANSWER_3]
    答案格式：答案1/答案2/答案3 正确答案是[ANSWER] |'
- en: '| HellaSwag | Please choose the correct ending to complete the given sentence:
    [ACTIVITY_lABEL]: [CONTEXT] Ending1: [ENDING_1] Ending2: [ENDING_2] Ending3: [ENDING_3]
    Ending4: [ENDING_4] Answer format: ending1/ending2/ending3/ending4 the correct
    answer is [ANSWER] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 请选择正确的结尾以完成给定的句子：[ACTIVITY_lABEL]：[CONTEXT] 结尾1：[ENDING_1] 结尾2：[ENDING_2]
    结尾3：[ENDING_3] 结尾4：[ENDING_4] 答案格式：结尾1/结尾2/结尾3/结尾4 正确答案是[ANSWER] |'
- en: '| WinoGrande | Please choose the correct answer to fill in the blank to complete
    the given sentence: [SENTENCE] Option1: [OPTION_1] Option2: [OPTION_2] the correct
    answer is [ANSWER] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| WinoGrande | 请选择正确的答案来填空以完成给定的句子：[SENTENCE] 选项1: [OPTION_1] 选项2: [OPTION_2]
    正确答案是 [ANSWER] |'
- en: '| ARC-e&ARC-c | Please choose the correct answer to the question: [QUESTION]
    Answer1: [ANSWER_1] Answer2: [ANSWER_2] Answer3: [ANSWER_3] Answer4: [ANSWER_4]
    Answer format: answer1/answer2/answer3/answer4 the correct answer is [ANSWER]
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e&ARC-c | 请选择正确的答案来回答问题：[QUESTION] 答案1: [ANSWER_1] 答案2: [ANSWER_2] 答案3:
    [ANSWER_3] 答案4: [ANSWER_4] 答案格式：答案1/答案2/答案3/答案4 正确答案是 [ANSWER] |'
- en: '| OBQA | Please choose the correct answer to the question: [QUESTION] Answer1:
    [ANSWER_1] Answer2: [ANSWER_2] Answer3: [ANSWER_3] Answer4: [ANSWER_4] Answer
    format: answer1/answer2/answer3/answer4 the correct answer is [ANSWER] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 请选择正确的答案来回答问题：[QUESTION] 答案1: [ANSWER_1] 答案2: [ANSWER_2] 答案3: [ANSWER_3]
    答案4: [ANSWER_4] 答案格式：答案1/答案2/答案3/答案4 正确答案是 [ANSWER] |'
- en: 'Table 7: The data template of each dataset used to create commonsense reasoning
    data for fine-tuning.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：用于创建常识推理数据的每个数据集的数据模板。
- en: A.3 Placement Analysis
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 放置分析
- en: 'Table [8](#A1.T8 "Table 8 ‣ A.3 Placement Analysis ‣ Appendix A Appendix ‣
    LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language
    Models") shows the performance regarding the placement of adapters in various
    locations on math reasoning datasets. The fine-tuning dataset utilized for this
    study is Math10K. Meanwhile, the base models employed is LLaMA-7B. We can observe
    that for the Series Adapter, the best position is to place it after the MLP layers,
    achieving an average accuracy of $59.5\%$.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A1.T8 "表 8 ‣ A.3 放置分析 ‣ 附录 A 附录 ‣ LLM-Adapters: 一种用于参数高效微调的大型语言模型的适配器系列")
    显示了关于数学推理数据集中各种位置的适配器放置性能。用于本研究的微调数据集是 Math10K。与此同时，使用的基础模型是 LLaMA-7B。我们可以观察到，对于串行适配器，最佳位置是在
    MLP 层之后放置，平均准确率为 $59.5\%$。'
- en: '| Model | Location | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP
    | Average |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位置 | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Series | Attn | 92.3 | 32.0 | 80.0 | 16.9 | 80.5 | 47.9 | 58.3 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 串行 | Attn | 92.3 | 32.0 | 80.0 | 16.9 | 80.5 | 47.9 | 58.3 |'
- en: '| MLP | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
- en: '| Both | 94 | 29.8 | 84.1 | 17.3 | 83.5 | 45.1 | 59.0 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 两者 | 94 | 29.8 | 84.1 | 17.3 | 83.5 | 45.1 | 59.0 |'
- en: '| Parallel | Attn | 94.5 | 33.5 | 83.0 | 17.3 | 80.5 | 46.9 | 59.3 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | Attn | 94.5 | 33.5 | 83.0 | 17.3 | 80.5 | 46.9 | 59.3 |'
- en: '| MLP | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
- en: '| Both | 94.3 | 30.2 | 84.8 | 17.7 | 84.3 | 47.2 | 59.8 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 两者 | 94.3 | 30.2 | 84.8 | 17.7 | 84.3 | 47.2 | 59.8 |'
- en: '| LoRA | Attn | 94.2 | 35.3 | 79.7 | 16.9 | 78.7 | 45.9 | 58.5 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | Attn | 94.2 | 35.3 | 79.7 | 16.9 | 78.7 | 45.9 | 58.5 |'
- en: '| MLP | 95.8 | 35.0 | 80.0 | 15.7 | 81.7 | 47.0 | 59.2 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 95.8 | 35.0 | 80.0 | 15.7 | 81.7 | 47.0 | 59.2 |'
- en: '| Both | 96.2 | 35.6 | 80.5 | 15.7 | 82.3 | 49.6 | 60.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 两者 | 96.2 | 35.6 | 80.5 | 15.7 | 82.3 | 49.6 | 60.0 |'
- en: 'Table 8: An evaluation of the accuracy regarding the placement of adapters
    in various locations is conducted on math reasoning datasets. The fine-tuning
    dataset used for this analysis is Math10K. In this context, "Attn" refers to the
    multi-head attention layer, while "MLP" denotes the MLP layer. The base model
    employed for this study is LLaMA-7B.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：对各种位置的适配器放置准确性的评估是基于数学推理数据集进行的。用于此分析的微调数据集是 Math10K。在此上下文中，“Attn”指的是多头注意力层，而“MLP”表示
    MLP 层。用于本研究的基础模型是 LLaMA-7B。
- en: A.4 Configuration Analysis
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 配置分析
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.4 Configuration Analysis ‣ Appendix A Appendix
    ‣ LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large
    Language Models") shows the accuracy comparison regarding different settings of
    variable for PEFT methods on math reasoning datasets. The fine-tuning dataset
    used for this study is Math10K. It can be noted that when the number of virtual
    tokens in Prefix-Tuning is set to 10, Prefix-Tuning attains an average accuracy
    of $42.0\%$.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#A1.T9 "表 9 ‣ A.4 配置分析 ‣ 附录 A 附录 ‣ LLM-Adapters: 一种用于参数高效微调的大型语言模型的适配器系列")
    显示了关于 PEFT 方法在数学推理数据集上不同设置变量的准确性比较。用于本研究的微调数据集是 Math10K。可以注意到，当 Prefix-Tuning
    中的虚拟令牌数量设置为 10 时，Prefix-Tuning 的平均准确率为 $42.0\%$。'
- en: '| Model | Variable | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP
    | Average |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 变量 | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Prefix | vt=10 | 63.2 | 24.4 | 57.0 | 14.2 | 55.3 | 38.1 | 42.0 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Prefix | vt=10 | 63.2 | 24.4 | 57.0 | 14.2 | 55.3 | 38.1 | 42.0 |'
- en: '| vt=20 | 60.3 | 22.9 | 46.1 | 16.1 | 51.8 | 30.6 | 38.0 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| vt=20 | 60.3 | 22.9 | 46.1 | 16.1 | 51.8 | 30.6 | 38.0 |'
- en: '| vt=30 | 51.2 | 16.9 | 42.3 | 15.4 | 41.9 | 31.1 | 33.1 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| vt=30 | 51.2 | 16.9 | 42.3 | 15.4 | 41.9 | 31.1 | 33.1 |'
- en: '| vt=40 | 54.2 | 17.8 | 49.6 | 21.7 | 52.0 | 33.2 | 38.1 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| vt=40 | 54.2 | 17.8 | 49.6 | 21.7 | 52.0 | 33.2 | 38.1 |'
- en: '| Series | bn=64 | 93.0 | 31.6 | 80.5 | 14.6 | 79.9 | 44.1 | 57.3 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Series | bn=64 | 93.0 | 31.6 | 80.5 | 14.6 | 79.9 | 44.1 | 57.3 |'
- en: '| bn=128 | 93.8 | 33.4 | 81.5 | 18.5 | 79.9 | 48.1 | 59.2 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| bn=128 | 93.8 | 33.4 | 81.5 | 18.5 | 79.9 | 48.1 | 59.2 |'
- en: '| bn=256 | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| bn=256 | 92.8 | 33.3 | 80.0 | 15.0 | 83.5 | 52.3 | 59.5 |'
- en: '| bn=512 | 95.3 | 33.7 | 77.2 | 14.9 | 76.6 | 42.0 | 56.6 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| bn=512 | 95.3 | 33.7 | 77.2 | 14.9 | 76.6 | 42.0 | 56.6 |'
- en: '| Parallel | bn=64 | 92.7 | 36.8 | 77.5 | 16.9 | 81.9 | 48.5 | 59.1 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Parallel | bn=64 | 92.7 | 36.8 | 77.5 | 16.9 | 81.9 | 48.5 | 59.1 |'
- en: '| bn=128 | 95.8 | 36.9 | 83.0 | 17.7 | 83.3 | 47.8 | 60.8 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| bn=128 | 95.8 | 36.9 | 83.0 | 17.7 | 83.3 | 47.8 | 60.8 |'
- en: '| bn=256 | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| bn=256 | 94.5 | 35.3 | 86.6 | 18.1 | 86.0 | 49.6 | 61.7 |'
- en: '| bn=512 | 95.0 | 31.5 | 81.8 | 14.6 | 81.5 | 43.4 | 58.0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| bn=512 | 95.0 | 31.5 | 81.8 | 14.6 | 81.5 | 43.4 | 58.0 |'
- en: '| LoRA | r=4 | 95.2 | 34.7 | 78.5 | 16.1 | 77.8 | 45.4 | 57.9 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | r=4 | 95.2 | 34.7 | 78.5 | 16.1 | 77.8 | 45.4 | 57.9 |'
- en: '| r=8 | 96.2 | 35.6 | 80.5 | 15.7 | 82.3 | 49.6 | 60.0 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| r=8 | 96.2 | 35.6 | 80.5 | 15.7 | 82.3 | 49.6 | 60.0 |'
- en: '| r=16 | 95.5 | 36.2 | 82.8 | 13.8 | 84.4 | 50.9 | 60.6 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| r=16 | 95.5 | 36.2 | 82.8 | 13.8 | 84.4 | 50.9 | 60.6 |'
- en: '| r=32 | 95.0 | 37.5 | 83.3 | 18.9 | 84.4 | 52.1 | 61.9 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| r=32 | 95.0 | 37.5 | 83.3 | 18.9 | 84.4 | 52.1 | 61.9 |'
- en: 'Table 9: The accuracy comparison regarding different settings of variable for
    PEFT methods on math reasoning datasets. The fine-tuning dataset used for this
    analysis is Math10K. In this context, "vt" refers to the number of virtual tokens,
    "bn" denotes the bottleneck size, while "r" is the LoRA rank. The base model employed
    for this study is LLaMA-7B.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：关于PEFT方法在数学推理数据集上的不同设置变量的准确性比较。用于此分析的微调数据集是Math10K。在此背景下，“vt”指的是虚拟标记的数量，“bn”表示瓶颈大小，而“r”是LoRA秩。用于本研究的基础模型是LLaMA-7B。
- en: A.5 Qualitative Examples
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 定性示例
- en: We will show examples randomly sampled from math reasoning and commonsense reasoning
    datasets in this section.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中展示随机从数学推理和常识推理数据集中抽取的示例。
- en: '| Question | At the schools book fair Sam bought 13 adventure books and 17
    mystery books. If 15 of the books were used, how many new books did he buy? |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 在学校书展上，Sam购买了13本冒险书和17本悬疑书。如果15本书是二手书，他买了多少本新书？ |'
- en: '| --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of mosaic tiles needed to cover two-thirds
    of Mr. Boarden’s 36 square foot bathroom, follow these steps: Find two-thirds
    of 36: Multiply 36 by 2/3. (36 * 2/3) = 24 square feet Determine the number of
    mosaic tiles needed per square foot: Given that Mr. Boarden needs 24 mosaic tiles
    per square foot, multiply the result from step 1 by 24. (24 * 24) = 576 mosaic
    tiles Therefore, Mr. Boarden would need 576 mosaic tiles to cover two-thirds of
    his 36 square foot bathroom. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 计算覆盖博登先生36平方英尺浴室三分之二所需的马赛克瓷砖数量，按以下步骤进行：找出36的三分之二：将36乘以2/3。（36 *
    2/3）= 24平方英尺 确定每平方英尺所需的马赛克瓷砖数量：鉴于博登先生每平方英尺需要24块马赛克瓷砖，将第1步的结果乘以24。（24 * 24）= 576块马赛克瓷砖
    因此，博登先生需要576块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。|'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. Find the total number of books Sam bought:
    13 + 17 = 30 books 2\. Subtract the number of used books from the total number
    of books: 30 - 15 = 15 new books Therefore, Sam bought 15 new books at the book
    fair. Steps: 1\. Total number of books = 13 + 17 = 30 2\. Number of used books
    = 15 3\. Number of new books = Total number of books - Number of used books 4\.
    Number of new books = 30 - 15 = 15 5\. Sam bought 15 new books at the book fair.
    Answer: Sam bought 15 new books at the book fair. (15) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. 找出Sam购买的书籍总数：13 + 17 = 30本 2\. 从书籍总数中减去二手书的数量：30
    - 15 = 15本新书 因此，Sam在书展上买了15本新书。步骤：1\. 书籍总数 = 13 + 17 = 30 2\. 二手书的数量 = 15 3\.
    新书的数量 = 书籍总数 - 二手书的数量 4\. 新书的数量 = 30 - 15 = 15 5\. Sam在书展上买了15本新书。答案：Sam在书展上买了15本新书。（15）|'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Find the total number of books Sam bought:
    13 + 17 = 30 2\. Subtract the number of used books from the total number of books:
    30 - 15 = 15 3\. Therefore, Sam bought 15 new books. Answer: 15 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 找出Sam购买的书籍总数：13 + 17 = 30 2\. 从书籍总数中减去二手书的数量：30
    - 15 = 15 3\. 因此，Sam购买了15本新书。答案：15 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | To find out how many new books Sam bought,
    we need to subtract the number of used books from the total number of books he
    bought. Total books = 13 + 17 = 30 Used books = 15 New books = Total books - Used
    books New books = 30 - 15 New books = 15 Therefore, Sam bought 15 new books. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 要找出Sam买了多少本新书，我们需要从他买的书的总数中减去旧书的数量。 总书数
    = 13 + 17 = 30 旧书 = 15 新书 = 总书数 - 旧书 新书 = 30 - 15 新书 = 15 因此，Sam买了15本新书。 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Find the total number of books Sam bought:
    13 + 17 = 30 2\. Subtract the number of used books from the total number of books:
    30 - 15 = 15 3\. Therefore, Sam bought 15 new books. Answer: 15 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 找出Sam买了多少本书：13 + 17 = 30 2\. 从总书数中减去旧书的数量：30
    - 15 = 15 3\. 因此，Sam买了15本新书。 答案：15 |'
- en: 'Table 10: An example randomly sampled from MultiArith. The outputs of ChatGPT
    and LLaMA-13B with different PEFT methods.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：一个从MultiArith中随机抽样的示例。ChatGPT和LLaMA-13B使用不同PEFT方法的输出。
- en: '| Question | Mr Boarden is remodeling his bathroom. For every square foot,
    he needs 24 mosaic tiles. How many mosaic tiles would Mr Boarden need to cover
    two thirds of his 36 sq ft bathroom? |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Question | Boarden先生正在翻新他的浴室。每平方英尺，他需要24块马赛克瓷砖。Boarden先生需要多少块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二？
    |'
- en: '| --- | --- |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of mosaic tiles needed to cover two-thirds
    of Mr. Boarden’s 36 square foot bathroom, follow these steps: 1\. Find two-thirds
    of 36: Multiply 36 by 2/3. (36 * 2/3) = 24 square feet 2.Determine the number
    of mosaic tiles needed per square foot: Given that Mr. Boarden needs 24 mosaic
    tiles per square foot, multiply the result from step 1 by 24. (24 * 24) = 576
    mosaic tiles Therefore, Mr. Boarden would need 576 mosaic tiles to cover two-thirds
    of his 36 square foot bathroom. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 要计算覆盖Boarden先生36平方英尺浴室三分之二所需的马赛克瓷砖数量，请遵循以下步骤： 1\. 找出36的三分之二：将36乘以2/3。(36
    * 2/3) = 24平方英尺 2\. 确定每平方英尺所需的马赛克瓷砖数量：鉴于Boarden先生每平方英尺需要24块马赛克瓷砖，将第1步的结果乘以24。(24
    * 24) = 576块马赛克瓷砖 因此，Boarden先生需要576块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1 square foot = 4 square inches 1 square
    inch = 16 square centimeters So, 1 sq ft = 16 sq cm Therefore, 36 sq ft = 36 x
    16 sq cm = 576 sq cm To cover two-thirds of the bathroom, Mr Boarden needs to
    cover 576 sq cm x 2/3 = 384 sq cm Since 1 sq cm = 1 mosaic tile, Mr Boarden needs
    384 sq cm / 1 sq cm = 384 mosaic tiles Therefore, Mr Boarden needs 384 mosaic
    tiles to cover two-thirds of his 36 sq ft bathroom. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1平方英尺 = 4平方英寸 1平方英寸 = 16平方厘米 所以，1平方英尺 = 16平方厘米
    因此，36平方英尺 = 36 x 16平方厘米 = 576平方厘米 为了覆盖浴室的三分之二，Boarden先生需要覆盖576平方厘米 x 2/3 = 384平方厘米
    由于1平方厘米 = 1块马赛克瓷砖，Boarden先生需要384平方厘米 / 1平方厘米 = 384块马赛克瓷砖 因此，Boarden先生需要384块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Find the area of the bathroom: 36 sq
    ft 2\. Find two-thirds of the area: 2/3 x 36 sq ft = 24 sq ft 3\. Multiply the
    area by the number of tiles needed per square foot: 24 sq ft x 24 tiles/sq ft
    = 576 tiles 4\. Therefore, Mr Boarden would need 576 mosaic tiles to cover two-thirds
    of his 36 sq ft bathroom. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 计算浴室的面积：36平方英尺 2\. 计算面积的三分之二：2/3 x 36平方英尺
    = 24平方英尺 3\. 将面积乘以每平方英尺所需的瓷砖数量：24平方英尺 x 24块瓷砖/平方英尺 = 576块瓷砖 4\. 因此，Boarden先生需要576块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. Calculate the total area of the bathroom:
    36 sq ft 2\. Determine how much area Mr Boarden wants to cover with mosaic tiles:
    2/3 x 36 sq ft = 24 sq ft 3\. Calculate the total number of mosaic tiles needed:
    24 sq ft x 24 mosaic tiles per sq ft = 600 mosaic tiles Therefore, Mr Boarden
    would need 600 mosaic tiles to cover two thirds of his 36 sq ft bathroom. The
    answer in Arabic numerals is 600. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. 计算浴室的总面积：36平方英尺 2\. 确定Boarden先生想用马赛克瓷砖覆盖的面积：2/3
    x 36平方英尺 = 24平方英尺 3\. 计算所需的马赛克瓷砖总数：24平方英尺 x 每平方英尺24块马赛克瓷砖 = 600块马赛克瓷砖 因此，Boarden先生需要600块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。答案用阿拉伯数字表示为600。
    |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Calculate the total area of the bathroom:
    36 sq ft 2\. Determine how much area needs to be covered: 2/3 x 36 sq ft = 24
    sq ft 3\. Calculate the number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles
    per sq ft = 576 mosaic tiles Therefore, Mr Boarden would need 576 mosaic tiles
    to cover two thirds of his 36 sq ft bathroom. The answer in Arabic numerals is
    576. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 计算浴室的总面积：36平方英尺 2\. 确定需要覆盖的面积：2/3 x 36平方英尺
    = 24平方英尺 3\. 计算所需的马赛克瓷砖数量：24平方英尺 x 每平方英尺24块马赛克瓷砖 = 576块马赛克瓷砖 因此，Boarden先生需要576块马赛克瓷砖来覆盖他36平方英尺浴室的三分之二。阿拉伯数字的答案是576。
    |'
- en: 'Table 11: An example randomly sampled from GSM8K. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：从GSM8K中随机抽取的示例。ChatGPT和LLaMA-13B使用不同PEFT方法的输出。
- en: '| Question | There are 7 crayons in the drawer. Mary took 3 crayons out of
    the drawer. How many crayons are there now? |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 抽屉里有7支蜡笔。玛丽拿出了3支蜡笔。现在抽屉里还有多少支蜡笔？ |'
- en: '| --- | --- |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of crayons remaining in the drawer after
    Mary took some out, follow these steps: Subtract the number of crayons Mary took
    from the total number of crayons in the drawer: 7 - 3 = 4 crayons. Therefore,
    there are now 4 crayons remaining in the drawer. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 要计算玛丽拿出一些蜡笔后抽屉中剩余的蜡笔数量，请按照以下步骤操作：从抽屉中的蜡笔总数中减去玛丽拿出的蜡笔数量：7 - 3 =
    4支蜡笔。因此，现在抽屉里剩下4支蜡笔。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. Start with the initial number of crayons
    in the drawer: 7. 2\. Subtract the number of crayons Mary took out of the drawer:
    3. 3\. The final number of crayons in the drawer is 7 - 3 = 4. Therefore, there
    are 4 crayons in the drawer now. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. 从抽屉中的初始蜡笔数量开始：7。 2\. 减去玛丽拿出的蜡笔数量：3。 3\.
    抽屉中蜡笔的最终数量是7 - 3 = 4。因此，现在抽屉里有4支蜡笔。 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Start with the initial number of crayons
    in the drawer: 7 2\. Subtract the number of crayons Mary took out: 3 3\. Perform
    the subtraction: 7 - 3 = 4 4\. Therefore, there are now 4 crayons in the drawer.
    Answer: There are now 4 crayons in the drawer. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 从抽屉中的初始蜡笔数量开始：7 2\. 减去玛丽拿出的蜡笔数量：3 3\.
    执行减法运算：7 - 3 = 4 4\. 因此，现在抽屉里有4支蜡笔。答案：现在抽屉里有4支蜡笔。 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. Start with the total number of crayons
    in the drawer: 7 2\. Subtract the number of crayons Mary took out: 3 3\. Perform
    the subtraction: 7 - 3 = 4 4\. The final answer is 4 crayons. Therefore, there
    are now 4 crayons in the drawer. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. 从抽屉中的蜡笔总数开始：7 2\. 减去玛丽拿出的蜡笔数量：3 3\.
    执行减法运算：7 - 3 = 4 4\. 最终答案是4支蜡笔。因此，现在抽屉里有4支蜡笔。 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Start with the initial number of crayons:
    7 2\. Subtract the number of crayons Mary took out: 7 - 3 = 4 3\. The final answer
    is 4 crayons. Therefore, there are now 4 crayons in the drawer. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 从蜡笔的初始数量开始：7 2\. 减去玛丽拿出的蜡笔数量：7 - 3 = 4
    3\. 最终答案是4支蜡笔。因此，现在抽屉里有4支蜡笔。 |'
- en: 'Table 12: An example randomly sampled from AddSub. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：从AddSub中随机抽取的示例。ChatGPT和LLaMA-13B使用不同PEFT方法的输出。
- en: '| Question | Suppose you want to arrange your English, Hindi, Mathematics,
    History, Geography and Science books on a shelf. In how many ways can you do it
    ? Answer Choices: (A) 520 (B) 720 (C) 920 (D) None (E) Cannot be determined |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 假设你想把你的英语、印地语、数学、历史、地理和科学书籍放在书架上。你可以用多少种方式排列这些书？ 答案选项： (A) 520 (B) 720
    (C) 920 (D) 无 (E) 无法确定 |'
- en: '| --- | --- |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of ways you can arrange the English, Hindi,
    Mathematics, History, Geography, and Science books on a shelf, we can use the
    concept of permutations. Since order matters when arranging the books, we need
    to find the number of permutations. To determine the total number of arrangements,
    follow these steps: Count the number of books: There are 6 books in total. Use
    the formula for permutations of n objects, where n is the number of books: n!
    (n factorial). n! = 6! = 6 x 5 x 4 x 3 x 2 x 1 = 720 Therefore, you can arrange
    the books on the shelf in 720 different ways. The correct answer choice is (B)
    720. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 要计算如何将英语、印地语、数学、历史、地理和科学书籍排列在书架上的方法数，我们可以使用排列的概念。由于书籍的排列顺序很重要，我们需要找到排列的数量。要确定总的排列数，请按照以下步骤操作：计算书籍的数量：共有6本书。使用n个对象的排列公式，其中n是书籍的数量：n!（n的阶乘）。n!
    = 6! = 6 x 5 x 4 x 3 x 2 x 1 = 720 因此，您可以以720种不同的方式将书籍排列在书架上。正确的答案选择是（B）720。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. Let’s start by assigning variables to
    the unknowns in the problem. Let a, b, and c be the digits in M, N, and 396c respectively.
    2\. We know that M and N are three-digit numbers, so a + b + c = 3. 3\. We also
    know that M - N + 396c = 990, so we can substitute a, b, and c into this equation:
    a(1000000 - 100000 + 396c) + b(1000000 - 100000 + 396c) + c(1000000 - 100000 +
    396c) = 990 4\. Simplifying this equation, we get: a |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. 让我们首先为问题中的未知数分配变量。让a、b和c分别代表M、N和396c中的数字。
    2\. 我们知道M和N是三位数，所以a + b + c = 3。 3\. 我们还知道M - N + 396c = 990，因此我们可以将a、b和c代入这个方程：a(1000000
    - 100000 + 396c) + b(1000000 - 100000 + 396c) + c(1000000 - 100000 + 396c) = 990
    4\. 简化这个方程，我们得到：a |'
- en: '| LLaMA${}_{\text{13B}}$-Series | To solve this problem, we need to use the
    multiplication principle of counting, which states that if there are m ways to
    do one thing and n ways to do another thing, then there are m x n ways to do both
    things together. In this case, there are 6 books to arrange (English, Hindi, Mathematics,
    History, Geography, and Science). We can arrange these books in 6! ways. Therefore,
    the total number of ways to arrange the books on the shelf is 6! = 720. Hence,
    the answer is option B) 720. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 要解决这个问题，我们需要使用计数的乘法原则，该原则指出，如果有m种方法做一件事，而有n种方法做另一件事，那么同时做这两件事的方式总数是m
    x n。在这种情况下，有6本书需要排列（英语、印地语、数学、历史、地理和科学）。我们可以用6!种方式排列这些书籍。因此，将书籍排列在书架上的总方式数是6!
    = 720。因此，答案是选项B）720。 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | The total number of books to be arranged
    on the shelf is 5 (English, Hindi, Mathematics, History, Geography, and Science).
    To find the number of ways to arrange these books on the shelf, we can use the
    formula for permutations of n objects taken r at a time, which is given by: P(n,r)
    = n! / (n-r)! where n is the total number of objects, and r is the number of objects
    to be arranged at a time. In this case, n = 5 (the total number of books), and
    r = 5 (since we want to arrange all 5 books on the shelf). Substituting these
    values into the formula, we get: P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5)
    = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 需要排列在书架上的书籍总数是5本（英语、印地语、数学、历史、地理和科学）。要找到这些书籍在书架上排列的方式数，我们可以使用n个对象取r个对象的排列公式，公式为：P(n,r)
    = n! / (n-r)! 其中n是对象的总数，r是每次要排列的对象数。在这种情况下，n = 5（书籍的总数），r = 5（因为我们要将所有5本书排列在书架上）。将这些值代入公式，我们得到：P(5,5)
    = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5)
    = 5! / (5-5)! |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | The total number of ways to arrange 6 books
    on a shelf is 6! = 6 x 5 x 4 x 3 x 2 x 1 = 720. Therefore, the answer is option
    B) 720. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 将6本书排列在书架上的总方法数是6! = 6 x 5 x 4 x 3 x 2 x 1
    = 720。因此，答案是选项B）720。 |'
- en: 'Table 13: An example randomly sampled from AQuA. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：从AQuA随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出结果。
- en: '| Question | Oceanside Bike Rental Shop charges 17 dollars plus 7 dollars an
    hour for renting a bike. Tom paid 80 dollars to rent a bike. How many hours did
    he pay to have the bike checked out? |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | Oceanside自行车租赁店收费17美元加每小时7美元租赁自行车。汤姆支付了80美元租借自行车。他支付了多少小时的租赁费用？ |'
- en: '| --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of hours Tom paid to have the bike checked
    out, follow these steps: 1\. Subtract the base charge from the total amount Tom
    paid: 80 - 17 = 63 dollars. 2\. Divide the remaining amount by the hourly rate
    to find the number of hours: 63 / 7 = 9 hours. Therefore, Tom paid to have the
    bike checked out for 9 hours. Note: It’s important to note that the result represents
    the whole number of hours. If there were any partial hours, they would not be
    reflected in the calculation. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 计算汤姆为检查自行车支付的小时数，请按照以下步骤：1\. 从汤姆支付的总金额中减去基本费用：80 - 17 = 63美元。2\.
    用剩余金额除以每小时费用，找到小时数：63 / 7 = 9小时。因此，汤姆支付了9小时的检查费。注意：需要注意的是，结果代表完整的小时数。如果有任何部分小时，它们不会反映在计算中。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. Let x be the number of hours Tom paid
    to rent the bike. 2\. The cost of renting the bike for x hours is 17 + 7x dollars.
    3\. We know that Tom paid 80 dollars, so we can set up an equation: 17 + 7x =
    80 4\. Solving for x, we divide both sides by 7: x = 10 5\. Therefore, Tom paid
    10 hours to rent the bike. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. 设x为汤姆租用自行车的小时数。2\. 租用自行车x小时的费用为17 + 7x美元。3\.
    我们知道汤姆支付了80美元，因此可以建立方程：17 + 7x = 80 4\. 求解x时，我们将两边除以7：x = 10 5\. 因此，汤姆支付了10小时的租车费。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Find the total cost of renting the bike:
    $17 + $7/hour = $24/hour 2\. Divide the total cost by the cost per hour to find
    the number of hours: $80 ÷ $24/hour = 3.33 hours 3\. Round up to the nearest whole
    number since you can’t rent a bike for a fraction of an hour: 3 hours Therefore,
    Tom paid to have the bike checked out for 3 hours. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 找到租用自行车的总费用：17 + 7美元/小时 = 24美元/小时 2\.
    用总费用除以每小时费用，找到小时数：80 ÷ 24 = 3.33小时 3\. 向上取整到最近的整数，因为不能租用部分小时：3小时 因此，汤姆支付了3小时的检查费。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. Subtract the initial charge of $17
    from the total amount paid: $80 - $17 = $63 2\. Divide the remaining amount by
    the hourly rate: $63 ÷ $7 = 9 3\. The answer is 9 hours. Therefore, Tom paid to
    have the bike checked out for 9 hours. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. 从总支付金额中减去初始费用17美元：80 - 17 = 63美元 2\.
    用剩余金额除以每小时费用：63 ÷ 7 = 9 3\. 答案是9小时。因此，汤姆支付了9小时的检查费。 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Let x be the number of hours Tom paid to
    have the bike checked out. 2\. The cost of renting the bike for x hours is 7x
    dollars. 3\. The total cost of renting the bike for x hours is 17 + 7x dollars.
    4\. We know that Tom paid 80 dollars to rent the bike, so we can set up an equation:
    17 + 7x = 80 5\. Solving for x, we can subtract 17 from both sides: 7x = 63 6\.
    Dividing both sides by 7, we get x = 9 Therefore, Tom paid to have the bike checked
    out for 9 hours. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 设x为汤姆为检查自行车支付的小时数。2\. 租用自行车x小时的费用为7x美元。3\.
    租用自行车x小时的总费用为17 + 7x美元。4\. 我们知道汤姆支付了80美元租用自行车，因此可以建立方程：17 + 7x = 80 5\. 求解x时，我们可以从两边减去17：7x
    = 63 6\. 两边都除以7，得到x = 9 因此，汤姆支付了9小时的检查费。 |'
- en: 'Table 14: An example randomly sampled from SingleEq. The outputs of ChatGPT
    and LLaMA-13B with different PEFT methods.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 表14：一个从SingleEq中随机抽样的示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出结果。
- en: '| Question | Matthew gave equal numbers of crackers and cakes to his 4 friends.
    If he had 32 crackers and 98 cakes initially. How many crackers did each person
    eat? |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 马修给他的4个朋友每人一样数量的饼干和蛋糕。如果他最初有32块饼干和98块蛋糕。每个人吃了多少块饼干？ |'
- en: '| --- | --- |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | To calculate the number of crackers each person ate, we need to
    find the number of crackers that each friend received initially and subtract it
    from the total number of crackers Matthew had. Here are the steps: Divide the
    total number of crackers by the number of friends (4) to find the number of crackers
    each friend received initially: 32 / 4 = 8 crackers per friend. Since we want
    to find the number of crackers each person ate, we need to subtract the initial
    number of crackers per friend from the total number of crackers: 32 - 8 = 24 crackers.
    Therefore, each person ate 24 crackers. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 计算每个人吃了多少块饼干，我们需要找出每个朋友最初得到的饼干数量，并从马修拥有的饼干总数中减去。步骤如下：将饼干总数除以朋友人数（4），找出每个朋友最初得到的饼干数量：32
    / 4 = 8块饼干每人。由于我们要找出每个人吃了多少块饼干，因此需要从饼干总数中减去每人最初得到的饼干数量：32 - 8 = 24块饼干。因此，每个人吃了24块饼干。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. Find the total number of crackers and
    cakes that Matthew initially had: 32 + 98 = 130 2\. Divide the total number of
    crackers and cakes by the number of friends to find the initial amount given to
    each friend: 130 ÷ 5 = 26 3\. Subtract the initial amount given to each friend
    from the total number of crackers and cakes to find the remaining amount: 130
    - 26 = 104 4\. Divide the remaining amount by the number of friends to find the
    amount each person ate: 104 ÷ 5 = 21 Therefore, each person ate 21 crackers. |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 1\. 找到马修最初拥有的饼干和蛋糕的总数：32 + 98 = 130 2\. 将饼干和蛋糕的总数除以朋友的数量，以找到最初分配给每个朋友的数量：130
    ÷ 5 = 26 3\. 从饼干和蛋糕的总数中减去分配给每个朋友的初始数量，以找到剩余的数量：130 - 26 = 104 4\. 将剩余的数量除以朋友的数量，以找到每个人吃的数量：104
    ÷ 5 = 21 因此，每个人吃了21块饼干。 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | 1\. Divide the total number of crackers by
    the number of friends: 32 ÷ 4 = 8 2\. Divide the total number of cakes by the
    number of friends: 98 ÷ 4 = 24 3\. Add the number of crackers and cakes each friend
    received: 8 + 24 = 32 4\. Therefore, each person ate 32 crackers and 24 cakes.
    Answer: Each person ate 32 crackers. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 1\. 将饼干的总数除以朋友的数量：32 ÷ 4 = 8 2\. 将蛋糕的总数除以朋友的数量：98
    ÷ 4 = 24 3\. 加上每个朋友收到的饼干和蛋糕的数量：8 + 24 = 32 4\. 因此，每个人吃了32块饼干和24块蛋糕。 答案：每个人吃了32块饼干。
    |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. Divide the total number of crackers
    by the number of friends: 32 ÷ 4 = 8 2\. Divide the total number of cakes by the
    number of friends: 98 ÷ 4 = 24 3\. Add the number of crackers and cakes each person
    ate: 8 + 24 = 32 Therefore, each person ate 32 crackers and cakes combined. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 1\. 将饼干的总数除以朋友的数量：32 ÷ 4 = 8 2\. 将蛋糕的总数除以朋友的数量：98
    ÷ 4 = 24 3\. 加上每个人吃的饼干和蛋糕的数量：8 + 24 = 32 因此，每个人总共吃了32块饼干和蛋糕。 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. Divide the total number of crackers by
    the number of friends: 32 ÷ 4 = 8 2\. Divide the total number of cakes by the
    number of friends: 98 ÷ 4 = 24 3\. Add the number of crackers and cakes each person
    received: 8 + 24 = 32 Therefore, each person ate 32 crackers and cakes in total.
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 1\. 将饼干的总数除以朋友的数量：32 ÷ 4 = 8 2\. 将蛋糕的总数除以朋友的数量：98
    ÷ 4 = 24 3\. 加上每个人收到的饼干和蛋糕的数量：8 + 24 = 32 因此，每个人总共吃了32块饼干和蛋糕。 |'
- en: 'Table 15: An example randomly sampled from SVAMP. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 15: 从SVAMP随机抽样的一个例子。不同PEFT方法下的ChatGPT和LLaMA-13B的输出结果。'
- en: '| Question | Does ethanol take more energy make that produces? |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Question | 乙醇的生产是否需要更多的能量？ |'
- en: '| --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | False. |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 错误。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is true |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是对的 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is true |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是对的 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | false |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 错误 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is false |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是错误 |'
- en: 'Table 16: An example randomly sampled from BoolQ. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 16: 从BoolQ随机抽样的一个例子。不同PEFT方法下的ChatGPT和LLaMA-13B的输出结果。'
- en: '| Question | How do I ready a guinea pig cage for it’s new occupants? Solution1:
    Provide the guinea pig with a cage full of a few inches of bedding made of ripped
    paper strips, you will also need to supply it with a water bottle and a food dish.
    Solution2: Provide the guinea pig with a cage full of a few inches of bedding
    made of ripped jeans material, you will also need to supply it with a water bottle
    and a food dish. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Question | 如何为新住户准备豚鼠笼？ Solution1: 给豚鼠提供一个充满几英寸撕纸条制成的垫料的笼子，你还需要提供一个水瓶和一个食物碗。
    Solution2: 给豚鼠提供一个充满几英寸撕旧牛仔裤材料制成的垫料的笼子，你还需要提供一个水瓶和一个食物碗。 |'
- en: '| ChatGPT | Solution1 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | Solution1 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is solution1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是solution1 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is solution2 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是solution2 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is solution2 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是solution2 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is solution1 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是solution1 |'
- en: 'Table 17: An example randomly sampled from PIQA. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 17: 从PIQA随机抽样的一个例子。不同PEFT方法下的ChatGPT和LLaMA-13B的输出结果。'
- en: '| Question | Tracy didn’t go home that evening and resisted Riley’s attacks.
    What does Tracy need to do before this? Answer1: make a new plan Answer2: Go home
    and see Riley Answer3: Find somewhere to go |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | Tracy那晚没有回家，并且抵抗了Riley的攻击。在此之前，Tracy需要做什么？ 答案1：制定一个新计划 答案2：回家看Riley
    答案3：找个地方去 |'
- en: '| ChatGPT | Answer1/make a new plan |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 答案1/制定一个新计划 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is answer1 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是答案1 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is answer3 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是答案3 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is answer3 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是答案3 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is answer3 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是答案3 |'
- en: 'Table 18: An example randomly sampled from SIQA. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表18：从SIQA中随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
- en: '| Question | Roof shingle removal: A man is sitting on a roof. he Ending1:
    is using wrap to wrap a pair of skis. Ending2: is ripping level tiles off. Ending3:
    is holding a rubik’s cube. Ending4: starts pulling up roofing on a roof. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 屋顶瓦片拆除：一个男人坐在屋顶上。 结尾1：正在用包裹包裹一对滑雪板。 结尾2：正在撕下水平瓦片。 结尾3：正在玩魔方。 结尾4：开始拆除屋顶上的屋面材料。
    |'
- en: '| ChatGPT | Ending4: starts pulling up roofing on a roof. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 结尾4：开始在屋顶上拆除屋面材料。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is ending4 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是结尾4 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is ending4 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是结尾4 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is ending4 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是结尾4 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is ending4 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是结尾4 |'
- en: 'Table 19: An example randomly sampled from . The outputs of ChatGPT and LLaMA-13B
    with different PEFT methods.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表19：从中随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
- en: '| Question | Sarah was a much better surgeon than Maria so _ always got the
    easier cases. Option1: Sarah Option2: Maria |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | Sarah比Maria是更好的外科医生，所以_总是得到更简单的病例。 选项1：Sarah 选项2：Maria |'
- en: '| --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ChatGPT | option1 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 选项1 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is option1 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是选项1 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is option1 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是选项1 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is option1 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是选项1 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is option1 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是选项1 |'
- en: 'Table 20: An example randomly sampled from WinoGrande. The outputs of ChatGPT
    and LLaMA-13B with different PEFT methods.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表20：从WinoGrande中随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
- en: '| Question | Which statement best explains why photosynthesis is the foundation
    of most food webs? Answer1: Sunlight is the source of energy for nearly all ecosystems.
    Answer2: Most ecosystems are found on land instead of in water. Answer3: Carbon
    dioxide is more available than other gases. Answer4: The producers in all ecosystems
    are plants. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 哪种陈述最能解释为什么光合作用是大多数食物网的基础？ 答案1：阳光是几乎所有生态系统的能源来源。 答案2：大多数生态系统位于陆地而非水中。
    答案3：二氧化碳比其他气体更易获得。 答案4：所有生态系统中的生产者都是植物。 |'
- en: '| ChatGPT |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT |  |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | Answer1\. Sunlight is the source of energy
    for nearly all ecosystems. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 答案1：阳光是几乎所有生态系统的能源来源。 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is answer1 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是答案1 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is answer4 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是答案4 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is answer1 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是答案1 |'
- en: 'Table 21: An example randomly sampled from ARC-e. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表21：从ARC-e中随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
- en: '| Question | An astronomer observes that a planet rotates faster after a meteorite
    impact. Which is the most likely effect of this increase in rotation? Answer1:
    Planetary density will decrease. Answer2: Planetary years will become longer.
    Answer3: Planetary days will become shorter. Answer4: Planetary gravity will become
    stronger. |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 一位天文学家观察到一颗行星在陨石撞击后旋转变快。这种旋转增加的最可能效果是什么？ 答案1：行星密度将会减少。 答案2：行星年将变长。 答案3：行星日将变短。
    答案4：行星重力将变强。 |'
- en: '| ChatGPT | Answer3 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 答案3 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is answer3 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是answer3 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is answer2 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是answer2 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is answer3 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是answer3 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is answer3 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是answer3 |'
- en: 'Table 22: An example randomly sampled from ARC-c. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表22：从ARC-c随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
- en: '| Question | A person wants to start saving money so that they can afford a
    nice vacation at the end of the year. After looking over their budget and expenses,
    they decide the best way to save money is to Answer1: make more phone calls Answer2:
    quit eating lunch out Answer3: buy less with monopoly money Answer4: have lunch
    with friends |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 一个人希望开始储蓄，以便年底能够负担得起一次美好的假期。在查看了他们的预算和开支后，他们决定节省开支的最佳方法是 答案1：打更多电话 答案2：停止外出吃午餐
    答案3：减少用垄断币购物 答案4：与朋友共进午餐 |'
- en: '| ChatGPT | Answer2: quit eating lunch out. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 答案2：停止外出吃午餐。 |'
- en: '| LLaMA${}_{\text{13B}}$-Prefix | the correct answer is answer2 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Prefix | 正确答案是answer2 |'
- en: '| LLaMA${}_{\text{13B}}$-Series | the correct answer is answer2 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Series | 正确答案是answer2 |'
- en: '| LLaMA${}_{\text{13B}}$-Parallel | the correct answer is answer2 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-Parallel | 正确答案是answer2 |'
- en: '| LLaMA${}_{\text{13B}}$-LoRA | the correct answer is answer2 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA${}_{\text{13B}}$-LoRA | 正确答案是answer2 |'
- en: 'Table 23: An example randomly sampled from OBQA. The outputs of ChatGPT and
    LLaMA-13B with different PEFT methods.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 表23：从OBQA随机抽取的一个示例。ChatGPT和LLaMA-13B在不同PEFT方法下的输出。
