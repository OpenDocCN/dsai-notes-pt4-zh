- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:34:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:34:35'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GIFT-SW: 高斯噪声注入显著权重的细化调整方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15300](https://ar5iv.labs.arxiv.org/html/2408.15300)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15300](https://ar5iv.labs.arxiv.org/html/2408.15300)
- en: Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹,
- en: Egor Venediktov, Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Egor Venediktov, Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2)
- en: ¹ Skolkovo Institute of Science and Technology
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 斯科尔科沃科技学院
- en: ² Artificial Intelligence Research Institute
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 人工智能研究所
- en: ³ HSE University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 俄罗斯高等经济学院
- en: 'Correspondence: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$
    indicates equal contribution'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '通讯作者: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$ 表示贡献相等'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized
    the usage of Large Language Models (LLMs). Recent studies have shown that a small
    subset of weights significantly impacts performance. Based on this observation,
    we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of
    Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting
    Gaussian noise into non-salient ones. To identify these columns, we developed
    a generalized sensitivity metric that extends and unifies metrics from previous
    studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full
    fine-tuning and modern PEFT methods under the same computational budget. Moreover,
    GIFT-SW offers practical advantages to recover performance of models subjected
    to mixed-precision quantization with keeping salient weights in full precision.
    Code is available in [our repository](https://github.com/On-Point-RND/GIFT_SW).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效细化调整（PEFT）方法已经变得流行，并使大型语言模型（LLMs）的使用变得更加普及。最近的研究表明，一小部分权重对性能的影响显著。基于这一观察，我们提出了一种新颖的PEFT方法，称为高斯噪声注入显著权重的细化调整（GIFT-SW）。我们的方法仅更新显著的列，同时将高斯噪声注入非显著的列。为了识别这些列，我们开发了一种通用的敏感度度量，该度量扩展并统一了之前研究中的度量。对LLaMA模型的实验表明，在相同的计算预算下，GIFT-SW的表现优于完全细化调整和现代PEFT方法。此外，GIFT-SW还具有实际优势，可以在保留显著权重的全精度下恢复经过混合精度量化的模型性能。代码可在[我们的仓库](https://github.com/On-Point-RND/GIFT_SW)中获取。
- en: 'GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'GIFT-SW: 高斯噪声注入显著权重的细化调整方法'
- en: 'Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹, Egor Venediktov,
    Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2) ¹ Skolkovo Institute of
    Science and Technology ² Artificial Intelligence Research Institute ³ HSE University
    Correspondence: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$
    indicates equal contribution.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'Maxim Zhelnin^♣ ¹, Viktor Moskvoretskii^♣ ^(1,3), Egor Shvetsov¹, Egor Venediktov,
    Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev ^(1,2) ¹ 斯科尔科沃科技学院 ² 人工智能研究所 ³
    俄罗斯高等经济学院 通讯作者: [m.zhelnin@skol.tech](mailto:%20m.zhelnin@skol.tech) $\clubsuit$
    表示贡献相等。'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/429c6774423f92a2b210f31e12b31bb9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/429c6774423f92a2b210f31e12b31bb9.png)'
- en: 'Figure 1: Mean performance of different fine-tuning approaches for LLaMA models
    with scaling data budget. GIFT-SW shows superior performance with nearly all data
    budgets, also being as stable as full fine-tuning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同细化调整方法在LLaMA模型上的平均性能，随着数据预算的扩展。GIFT-SW在几乎所有的数据预算下都表现出优越的性能，同时稳定性也与完全细化调整相当。
- en: Modern LLMs demonstrate remarkable generalization capabilities on unseen tasks.
    However, fine-tuning remains crucial to enhance these models performance or to
    restore the performance after compression techniques like quantization Dettmers
    et al. ([2024](#bib.bib9)); Moskvoretskii et al. ([2024](#bib.bib28)), pruning Frantar
    and Alistarh ([2023](#bib.bib12)); Kim et al. ([2023](#bib.bib19)), or tensor
    decomposition have been applied. Given the large scale of modern LLMs, fine-tuning
    all parameters can be computationally and memory-intensive. To overcome this challenge,
    Parameter Efficient Fine-Tuning schemes have been developed, aimed to improve
    model performance while using limited computational and memory resources.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现代大型语言模型（LLMs）在未见任务上的泛化能力表现出色。然而，微调仍然至关重要，以提升这些模型的性能或在使用量化等压缩技术后恢复其性能 Dettmers
    等人 ([2024](#bib.bib9)); Moskvoretskii 等人 ([2024](#bib.bib28))，剪枝 Frantar 和 Alistarh
    ([2023](#bib.bib12)); Kim 等人 ([2023](#bib.bib19))，或张量分解已经被应用。鉴于现代大型语言模型的规模巨大，微调所有参数可能会消耗大量的计算和内存资源。为了解决这一挑战，已经开发了参数高效微调方案，旨在在使用有限计算和内存资源的情况下提升模型性能。
- en: To date, PEFT methods have not matched the accuracy of full fine-tuning Nikdan
    et al. ([2024](#bib.bib30)), highlighting the need for new approaches that can
    close this gap while still minimizing resource use. Additionally, most PEFT methods
    involve adding extra parameters, which increases computational demands.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，PEFT 方法尚未达到完全微调的准确性 Nikdan 等人 ([2024](#bib.bib30))，这突显了需要新的方法来缩小这一差距，同时仍然最小化资源使用。此外，大多数
    PEFT 方法涉及添加额外的参数，这增加了计算需求。
- en: 'To address those issues and enhance the performance of efficiently trained
    LLMs, we introduce a novel PEFT method, GIFT-SW. This approach focuses on updating
    a small subset of salient weights while injecting noise into the non-salient weights.
    The development of this method is grounded in observations from previous studies
    and the related questions they raise, which we aim to answer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题并提升高效训练 LLMs 的性能，我们提出了一种新型 PEFT 方法——GIFT-SW。该方法专注于更新少量显著权重，同时在非显著权重中注入噪声。该方法的开发基于之前研究的观察结果及其提出的相关问题，我们旨在回答这些问题：
- en: 'Previous research has shown that there is a small subset of salient weights
    which can significantly affect the effectiveness of post-training quantization
    (PTQ) Dettmers et al. ([2022](#bib.bib8), [2023](#bib.bib10)); Kim et al. ([2023](#bib.bib19))
    and pruning techniques Yin et al. ([2023](#bib.bib46)); Frantar and Alistarh ([2023](#bib.bib12));
    Sun et al. ([2023](#bib.bib40)). Moreover, [Gurnee et al.](#bib.bib15) identified
    a group of "universal neurons" that are critical to a model’s functionality, emphasizing
    the importance of selecting and updating these salient weights. Question 1: Does
    updating a small subset of salient weights is sufficient to adjust the model?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究表明，存在一个小的显著权重子集，这些权重可以显著影响后训练量化（PTQ） Dettmers 等人 ([2022](#bib.bib8), [2023](#bib.bib10));
    Kim 等人 ([2023](#bib.bib19)) 和剪枝技术 Yin 等人 ([2023](#bib.bib46)); Frantar 和 Alistarh
    ([2023](#bib.bib12)); Sun 等人 ([2023](#bib.bib40)) 的有效性。此外，[Gurnee 等人](#bib.bib15)
    识别了一组对模型功能至关重要的“通用神经元”，强调了选择和更新这些显著权重的重要性。问题 1：更新少量显著权重是否足以调整模型？
- en: 'Recent studies have demonstrated that Perturbed Gradient Descent (PGD), with
    noise injections applied both before and after the gradient step, can stabilize
    convergence and help prevent overfitting Poole et al. ([2014](#bib.bib33)); Zhu
    et al. ([2018](#bib.bib48)); Jin et al. ([2021](#bib.bib18)). Question 2: Does
    Injecting Noise helps convergence?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，扰动梯度下降（PGD），在梯度步骤前后施加噪声，可以稳定收敛并帮助防止过拟合 Poole 等人 ([2014](#bib.bib33));
    Zhu 等人 ([2018](#bib.bib48)); Jin 等人 ([2021](#bib.bib18))。问题 2：注入噪声是否有助于收敛？
- en: 'PGD is commonly employed to enhance model robustness by approximating the quantization
    process Shvetsov et al. ([2022](#bib.bib38)); Shin et al. ([2023](#bib.bib37));
    Défossez et al. ([2021](#bib.bib7)). This increased robustness can aid in maintaining
    the quality of the quantized model. Question 3: Does injecting noise helps robustness?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PGD 通常用于通过逼近量化过程来增强模型的鲁棒性 Shvetsov 等人 ([2022](#bib.bib38)); Shin 等人 ([2023](#bib.bib37));
    Défossez 等人 ([2021](#bib.bib7))。这种增强的鲁棒性可以帮助维持量化模型的质量。问题 3：注入噪声是否有助于鲁棒性？
- en: Selecting salient weights is a significant challenge, particularly in quantization
    and pruning, and it is central to our method. In our paper, we derive a general
    formulation for all previously established saliency metrics and present experiments
    to compare their effectiveness.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 选择显著权重是一个重要的挑战，特别是在量化和剪枝中，这也是我们方法的核心。在我们的论文中，我们推导了所有先前建立的显著性度量的通用公式，并进行实验以比较它们的有效性。
- en: 'The main contributions of our work can be summarized as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献可以总结如下：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a novel PEFT method for pre-trained and quantized LLMs, called
    GIFT-SW. It is designed to fine-tune weights in salient columns while injecting
    Gaussian noise into non-salient weights, which are kept frozen during training.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种新颖的PEFT方法，称为GIFT-SW，适用于预训练和量化的LLMs。该方法旨在对显著列中的权重进行微调，同时向非显著权重中注入高斯噪声，这些非显著权重在训练过程中保持冻结。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We generalize sensitivity metrics for identifying salient columns in pre-trained
    LLMs. We compare various novel and existing instances of the proposed general
    form and identify a new metric, which on average outperform previously studied
    in the literature metricsXiao et al. ([2023](#bib.bib44)); Lee et al. ([2024](#bib.bib22)).
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对预训练LLMs中显著列的敏感性度量进行了概括。我们比较了提议的通用形式的各种新颖和现有实例，并确定了一种新的度量，该度量在平均上优于以前文献中研究的度量
    Xiao et al.（[2023](#bib.bib44)）；Lee et al.（[2024](#bib.bib22)）。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiments demonstrate that GIFT-SW outperforms modern PEFT methods and full
    fine-tuning baselines across most zero-shot tasks. GIFT-SW for LLaMA models achieve
    comparable accuracy to the corresponding state-of-the-art TÜLU2 models, despite
    fine-tuning only 3% of the parameters and utilizing ten times less computational
    resources.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验表明，GIFT-SW在大多数零样本任务中优于现代PEFT方法和完全微调基线。尽管仅微调了3%的参数，并且利用了十倍较少的计算资源，GIFT-SW对于LLaMA模型实现了与对应的最先进TÜLU2模型相当的准确性。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate that GIFT-SW is more stable with respect to a size of training
    set compared with low-rank adapters.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了与低秩适配器相比，GIFT-SW在训练集大小方面更加稳定。
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Parameter efficient fine-tuning of LLM
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM的参数高效微调
- en: One of the most popular method with high efficiency is LoRA Hu et al. ([2021](#bib.bib16)),
    which trains the low-rank adapters. Recent modifications to the method aim to
    improve the initialization of the adapters Liu et al. ([2024](#bib.bib26)) and
    enhance the low-rank representation of pre-trained weights by adding sparse adapters
    Nikdan et al. ([2024](#bib.bib30)). Another improvement of the learning capacity
    of LoRA is given by DoRA Liu et al. ([2024](#bib.bib26)), which fine-tunes magnitude
    and direction components of the pretrained weights. This method achieves considerable
    performance across various fine-tuning tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的最流行方法之一是LoRA Hu et al.（[2021](#bib.bib16)），该方法训练低秩适配器。最近对该方法的修改旨在改善适配器的初始化
    Liu et al.（[2024](#bib.bib26)），并通过添加稀疏适配器 Nikdan et al.（[2024](#bib.bib30)）来增强预训练权重的低秩表示。LoRA的另一个改进是由DoRA
    Liu et al.（[2024](#bib.bib26)）提供的，该方法对预训练权重的幅度和方向分量进行微调。这种方法在各种微调任务中表现出显著的性能。
- en: 2.2 Salient Weights in LLMs
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM中的显著权重
- en: The identification of salient weights¹¹1In our work, we use the terms salient
    weights and weight outliers interchangeably. is one of the main problems in weight
    pruning. Recently, several approaches have been proposed to identify such weights
    in LLMs, including SparseGPT Frantar and Alistarh ([2023](#bib.bib12)), Wanda Sun
    et al. ([2023](#bib.bib40)), and OWL Yin et al. ([2023](#bib.bib46)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 显著权重的识别¹¹1在我们的工作中，我们将显著权重和权重异常值互换使用。是权重剪枝的主要问题之一。最近，已经提出了几种方法来识别LLMs中的这些权重，包括SparseGPT
    Frantar 和 Alistarh（[2023](#bib.bib12)），Wanda Sun et al.（[2023](#bib.bib40)），和OWL
    Yin et al.（[2023](#bib.bib46)）。
- en: '[Dettmers et al.](#bib.bib8)’s ([2022](#bib.bib8)) demonstrated that a small
    subset of outliers in input activations has a substantial impact on LLM performance,
    highlighting the relationship between the activation outliers and the salient
    weights. Many subsequent Post-Training Quantization (PTQ) methods used similar
    or identical pruning metrics to identify these salient weights Dettmers et al.
    ([2023](#bib.bib10)); Xiao et al. ([2023](#bib.bib44)); Lee et al. ([2024](#bib.bib22)).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[Dettmers et al.](#bib.bib8)（[2022](#bib.bib8)）证明了输入激活中的少量异常值对LLM性能有显著影响，突显了激活异常值与显著权重之间的关系。许多后续的后训练量化（PTQ）方法使用了类似或相同的剪枝指标来识别这些显著权重
    Dettmers et al.（[2023](#bib.bib10)）；Xiao et al.（[2023](#bib.bib44)）；Lee et al.（[2024](#bib.bib22)）。'
- en: In our work, we generalize the identification metrics for salient weights by
    considering metrics from both the literature on pruning and quantization.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们通过考虑来自剪枝和量化文献的度量来概括显著权重的识别度量。
- en: 2.3 Structured and Non-structured Salient Weights selection
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 结构化和非结构化显著权重选择
- en: Since salient weights account for only a few percent of all the weights, a straightforward
    approach to preserve them would be to store unstructured salient weights in a
    sparse matrix. Dettmers et al. ([2023](#bib.bib10)) demonstrated that this approach
    is computationally reasonable and leads to performance improvement. On the other
    hand, [Xiao et al.](#bib.bib44)’s ([2023](#bib.bib44)) revealed that outliers
    in activations are confined to a small fraction of weight channels, which was
    incorporated into SmoothQuant, where outlier columns are identified using a small
    calibration dataset. This concept is further developed in QUIK Ashkboos et al.
    ([2023](#bib.bib1)), where outlier columns are retained in full precision, while
    other columns are quantized using GPTQ (Frantar et al., [2022](#bib.bib13)). A
    similar procedure is used in OWQ Lee et al. ([2024](#bib.bib22)), but with an
    OBD-based metric LeCun et al. ([1989](#bib.bib21)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于显著权重仅占所有权重的几个百分比，因此保存它们的简单方法是将非结构化显著权重存储在稀疏矩阵中。Dettmers 等人（[2023](#bib.bib10)）证明了这种方法在计算上是合理的，并且能够提高性能。另一方面，[Xiao
    等人](#bib.bib44)（[2023](#bib.bib44)）揭示了激活中的异常值仅限于少数几个权重通道，这一概念被纳入了 SmoothQuant，其中异常列通过一个小的校准数据集进行识别。这个概念在
    QUIK Ashkboos 等人（[2023](#bib.bib1)）中得到了进一步发展，其中异常列保持全精度，而其他列则使用 GPTQ（Frantar 等人，[2022](#bib.bib13)）进行量化。类似的程序也在
    OWQ Lee 等人（[2024](#bib.bib22)）中使用，但使用了基于 OBD 的度量 LeCun 等人（[1989](#bib.bib21)）。
- en: Due to the lack of results in the literature on which approach brings better
    results, structured or unstructured salient weight selection, and motivated by
    computational efficiency mentioned in  Ashkboos et al. ([2023](#bib.bib1)), in
    our work we follow the second line of work with structured column-wise salient
    weight selection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文献中缺乏对结构化或非结构化显著权重选择哪个方法带来更好结果的研究，并且受到 Ashkboos 等人（[2023](#bib.bib1)）提到的计算效率的启发，我们的工作采用了第二种方法，即结构化列式显著权重选择。
- en: 2.4 Noise Injections
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 噪声注入
- en: In this section, we briefly describe Gaussian Noise Injections (GNI) and its
    benefits. Then, we show that the approximation of quantization noise and GNI are
    identical. Therefore, GNI can also benefit further model quantization. Therefor,
    to examine our third question, we sample noise relative to quantization levels,
    leaving other sampling options for future work.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要描述了高斯噪声注入（GNI）及其好处。然后，我们展示了量化噪声的近似与 GNI 是相同的。因此，GNI 还可以进一步提高模型量化的效果。因此，为了研究我们的第三个问题，我们对相对于量化水平的噪声进行采样，将其他采样选项留待未来研究。
- en: Gaussian Noise Injections (GNI). Perturbed Gradient Descent (PGD) is a family
    of methods that involve adding or multiplying weights with samples from some random
    distribution, during an optimization procedure. Gaussian noise injection (GNI)
    after the gradient step helps model to escape saddle points efficiently in non-convex
    optimization (Jin et al., [2021](#bib.bib18)). However, when Gaussian noise is
    injected before the gradient step, it helps model to escape from the spurious
    local optimum (Zhu et al., [2018](#bib.bib48)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯噪声注入（GNI）。扰动梯度下降（PGD）是一类方法，包括在优化过程中将权重与来自某些随机分布的样本相加或相乘。梯度步骤后的高斯噪声注入（GNI）帮助模型在非凸优化中有效地摆脱鞍点（Jin
    等人，[2021](#bib.bib18)）。然而，当在梯度步骤之前注入高斯噪声时，它有助于模型摆脱虚假的局部最优（Zhu 等人，[2018](#bib.bib48)）。
- en: '|  | $\displaystyle\theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t})+\xi)$
    |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t})+\xi)$
    |  | (1) |'
- en: '|  | $\displaystyle\ \theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t}+\xi))$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ \theta_{t+1}\leftarrow\theta_{t}-\tau(\nabla f(\theta_{t}+\xi))$
    |  | (2) |'
- en: '|  | $\displaystyle\xi\sim\mathcal{N}(\mu,\,\sigma^{2})$ |  | (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\xi\sim\mathcal{N}(\mu,\,\sigma^{2})$ |  | (3) |'
- en: Moreover, practical benefits of noise injections are well documented in the
    literature and often can be discussed as regularization techniques Bishop ([1995](#bib.bib3));
    Srivastava et al. ([2014](#bib.bib39)); Camuto et al. ([2020](#bib.bib4)), methods
    to prompt adversarial robustenss Panda and Roy ([2021](#bib.bib32)) and to be
    used for data agumentation Moreno-Barea et al. ([2018](#bib.bib27)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，噪声注入的实际好处在文献中有充分记录，并且通常可以作为正则化技术进行讨论 Bishop ([1995](#bib.bib3)); Srivastava
    et al. ([2014](#bib.bib39)); Camuto et al. ([2020](#bib.bib4))，方法用于促进对抗鲁棒性 Panda
    和 Roy ([2021](#bib.bib32))，以及用于数据增强 Moreno-Barea et al. ([2018](#bib.bib27))。
- en: In our work we use GNI before evaluating the gradient. For this scenario, Orvieto
    et al. ([2023](#bib.bib31)) proposed to add noise only to one layer at training
    iteration to avoid variance explosion. It was empirically and theoretically demonstrated
    that GNI serves as a regularization. Liu et al. ([2023](#bib.bib25)) study fine-tuning
    of pre-trained Language Models with GNI. Authors propose first to learn layer-wise
    variance parameters for noise distributions and then to fine-tune the model by
    adding noise to all the weights. The obtained results showed that the approach
    is superior to independent layer-wise noise injections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们在评估梯度之前使用 GNI。对于这种情况，Orvieto et al. ([2023](#bib.bib31)) 提议在训练迭代中仅向一个层添加噪声，以避免方差爆炸。实证和理论证明
    GNI 作为一种正则化。Liu et al. ([2023](#bib.bib25)) 研究了用 GNI 微调预训练语言模型的方法。作者首先建议学习噪声分布的逐层方差参数，然后通过向所有权重添加噪声来微调模型。获得的结果表明，这种方法优于独立的逐层噪声注入。
- en: 'Quantization Noise Injections (QNI). Quantization aware training (QAT) of networks
    is applied to mitigate their accuracy degradation after quantization. However,
    uniform quantization ²²2For the reader not familiar with uniform quantization,
    we discuss it in more details in Section [A](#A1 "Appendix A Uniform quantization
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    $Q$ such that $\mathbf{\Omega}=Q(\mathbf{W})-\mathbf{W}$ Défossez et al. ([2021](#bib.bib7));
    Shvetsov et al. ([2022](#bib.bib38)); Shin et al. ([2023](#bib.bib37)). Thus,
    training models with QNI is exactly the same as employing PGD with GNI before
    evaluating the gradient.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 量化噪声注入（QNI）。网络的量化感知训练（QAT）被应用于减轻量化后的准确度下降。然而，均匀量化²²对于不熟悉均匀量化的读者，我们在第[A](#A1
    "附录 A 均匀量化 ‣ GIFT-SW：高斯噪声注入的显著权重微调")节中更详细地讨论。$Q$ 使得 $\mathbf{\Omega}=Q(\mathbf{W})-\mathbf{W}$
    Défossez et al. ([2021](#bib.bib7)); Shvetsov et al. ([2022](#bib.bib38)); Shin
    et al. ([2023](#bib.bib37))。因此，使用 QNI 训练模型与在评估梯度之前使用 GNI 的 PGD 是完全一样的。
- en: Under some assumptions the noise $\mathbf{\Omega}$ typically yields improved
    outcomes Défossez et al. ([2021](#bib.bib7)); Shvetsov et al. ([2022](#bib.bib38)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些假设下，噪声 $\mathbf{\Omega}$ 通常会带来改进的结果 Défossez et al. ([2021](#bib.bib7));
    Shvetsov et al. ([2022](#bib.bib38))。
- en: Although GNI is beneficial for model training there is no clear answer on how
    to choose noise parameters. Liu et al. ([2023](#bib.bib25)) determine noise parameters
    such that KL divergence between original and perturbed weights is minimized. Shin
    et al. ([2023](#bib.bib37)) identify parameters of the Gaussian distribution to
    resemble the weight distribution with a scale proportional to quantization step.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GNI 对模型训练有益，但如何选择噪声参数仍无明确答案。Liu et al. ([2023](#bib.bib25)) 确定噪声参数，以使原始权重与扰动权重之间的
    KL 散度最小化。Shin et al. ([2023](#bib.bib37)) 确定高斯分布的参数，使其类似于权重分布，比例与量化步长成正比。
- en: 2.5 Straight Through Estimator
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 直通估计器
- en: 'The most popular QAT technique incorporating quantization operation into the
    traning process is Straight Through Estimation (STE)³³3More details on STE can
    be found in Section [C](#A3 "Appendix C Straight Through Estimator ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"). Bengio et al.
    ([2013](#bib.bib2)); Shang et al. ([2023](#bib.bib36)), which basically re-parameterizes
    gradients. However, [Défossez et al.](#bib.bib7)’s ([2021](#bib.bib7)) demonstrated
    that STE has some disadvantages compared with QNI⁴⁴4Event though QNI and GNI are
    identical operations for consistency and clarity, in the case of quantization
    we will refer to this procedure as Quantization Noise Injections (QNI), as STE
    is biased and may cause weight oscillation between quantization steps. [Shin et al.](#bib.bib37)’s
    ([2023](#bib.bib37)) demonstrated that pretraining models for the following quantization
    with QNI instead of STE results in better performance. More technical details
    are provided in Section [C](#A3 "Appendix C Straight Through Estimator ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '将量化操作纳入训练过程中的最流行的 QAT 技术是直通估计（STE）³³3有关 STE 的更多细节请参见第[C](#A3 "附录 C 直通估计器 ‣
    GIFT-SW: 高斯噪声注入的显著权重微调用于 LLM")节。Bengio 等人（[2013](#bib.bib2)）；Shang 等人（[2023](#bib.bib36)），它基本上重新参数化了梯度。然而，[Défossez
    等人](#bib.bib7)（[2021](#bib.bib7)）表明，与 QNI⁴⁴4相比，STE 存在一些缺点。尽管 QNI 和 GNI 在一致性和清晰度方面是相同的操作，但在量化的情况下，我们将这一过程称为量化噪声注入（QNI），因为
    STE 存在偏差，可能导致量化步骤之间的权重波动。[Shin 等人](#bib.bib37)（[2023](#bib.bib37)）表明，用 QNI 代替
    STE 进行预训练模型的量化可以取得更好的性能。更多技术细节请参见第[C](#A3 "附录 C 直通估计器 ‣ GIFT-SW: 高斯噪声注入的显著权重微调用于
    LLM")节。'
- en: 3 Method
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/3b69966161c71674487c72a55903caf0.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3b69966161c71674487c72a55903caf0.png)'
- en: 'Figure 2: GIFT-SW procedure follows Equation [2](#S2.E2 "In 2.4 Noise Injections
    ‣ 2 Related Work ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs"). We first sample some noise, relative to quantization levels, then,
    perform forward pass, and then update salient weights only. In GIFT-SW, quantization,
    pruning or tensor decomposition can be applied to non-salient weights and then,
    salient weights can be fine-tuned effectively without changing non-salient weights
    structure. In our experiments we select only 128 columns of salient weights, unless
    specified otherwise.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：GIFT-SW 过程遵循方程[2](#S2.E2 "在 2.4 噪声注入 ‣ 2 相关工作 ‣ GIFT-SW: 高斯噪声注入的显著权重微调用于
    LLM")。我们首先采样一些噪声，与量化水平相关，然后执行前向传递，然后仅更新显著权重。在 GIFT-SW 中，可以对非显著权重应用量化、剪枝或张量分解，然后可以有效地微调显著权重，而不改变非显著权重的结构。在我们的实验中，我们仅选择
    128 列显著权重，除非另有说明。'
- en: 'GIFT-SW consists of the following steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GIFT-SW 包含以下步骤：
- en: (1)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Identify a fixed number of salient columns using a chosen sensitive metric,
    based on a small calibration set. This number remains consistent across all layers.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用选择的敏感度度量，基于小的校准集识别固定数量的显著列。这个数量在所有层中保持一致。
- en: (2)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: Split columns of the matrices into subsets of salient columns and regular ones.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将矩阵的列分成突出的列子集和常规列子集。
- en: (3)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: During training, add noise to the weights in non-salient columns and update
    weights only in the salient columns.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练期间，对非显著列的权重添加噪声，并仅更新显著列的权重。
- en: 'Thus, the method depends on two main design choices: 1) how to choose salient
    columns and 2) the parameters of noise injections. We cover the choice of metrics
    in Section [3.1](#S3.SS1 "3.1 Generalizing parameter sensitivity metrics ‣ 3 Method
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    Noise injection details are provided in Section [3.2](#S3.SS2 "3.2 Quantization
    Noise Injection ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，该方法依赖于两个主要设计选择：1) 如何选择显著列和 2) 噪声注入的参数。我们在第[3.1](#S3.SS1 "3.1 参数敏感度度量的泛化
    ‣ 3 方法 ‣ GIFT-SW: 高斯噪声注入的显著权重微调用于 LLM")节中涵盖了度量选择。噪声注入的细节在第[3.2](#S3.SS2 "3.2 量化噪声注入
    ‣ 3 方法 ‣ GIFT-SW: 高斯噪声注入的显著权重微调用于 LLM")节中提供。'
- en: 3.1 Generalizing parameter sensitivity metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 参数敏感度度量的泛化
- en: Several approaches have been proposed recently to identify weights sensitive
    to quantization Dettmers et al. ([2023](#bib.bib10)) or pruning Sun et al. ([2023](#bib.bib40)).
    We generalize them as metrics for sensitivity to perturbations, and by applying
    these metrics, we determine which columns are more susceptible to degradation.
    Therefore, we avoid adding noise to such columns and use them to fine-tune the
    model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最近已经提出了几种方法来识别对量化敏感的权重 Dettmers等人 ([2023](#bib.bib10)) 或剪枝 Sun等人 ([2023](#bib.bib40))。我们将它们推广为对扰动的敏感度度量，并通过应用这些度量来确定哪些列更容易退化。因此，我们避免对这些列添加噪声，并用它们来微调模型。
- en: The proposed sensitivity metric is written for a column $j$ as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的敏感度度量对列$j$的表示为
- en: '|  | $s_{j}=\&#124;\mathbf{D}_{j}\&#124;_{\tau}\&#124;\mathbf{X}_{j}\&#124;_{\rho}^{\gamma},$
    |  | (4) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{j}=\&#124;\mathbf{D}_{j}\&#124;_{\tau}\&#124;\mathbf{X}_{j}\&#124;_{\rho}^{\gamma},$
    |  | (4) |'
- en: 'where $\mathbf{D}_{j}$ takes on one of the following values $1/2,1,2$ we utilize
    perturbations caused by quantization.⁵⁵5Optionally, one could use weight pruning
    as a source of perturbations or any other. That would lead to $\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$
    corresponds to the weights subjected to uniform symmetric quantization (see Appendix
    [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs")).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{D}_{j}$ 取以下值之一 $1/2,1,2$ 我们利用量化引起的扰动。⁵⁵5可选择地，可以使用权重剪枝作为扰动来源或其他任何方法。这将导致
    $\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$ 对应于受均匀对称量化影响的权重（见附录[A](#A1
    "附录 A 均匀量化 ‣ GIFT-SW: 高斯噪声注入的显著权重微调")）。'
- en: 'The input feature $\mathbf{X}$ are estimated for individual columns. Columns
    with the highest values are identified as the salient columns. Some details about
    the calibration dataset is described in Section [4.1](#S4.SS1 "4.1 Data ‣ 4 Experiments
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '输入特征$\mathbf{X}$是对单独列进行估计的。具有最高值的列被识别为显著列。有关校准数据集的一些细节在第[4.1节](#S4.SS1 "4.1
    数据 ‣ 4 实验 ‣ GIFT-SW: 高斯噪声注入的显著权重微调")中描述。'
- en: 'The metric given by Equation [4](#S3.E4 "In 3.1 Generalizing parameter sensitivity
    metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs") is closely related to those studied in the recent literature on quantization.
    In particular, the metric $\|\mathbf{X}\|_{\infty}$-th diagonal element of the
    Hessian matrix $\mathbf{H}$.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '方程[4](#S3.E4 "在 3.1 中的一般化参数敏感度度量 ‣ 3 方法 ‣ GIFT-SW: 高斯噪声注入的显著权重微调")给出的度量与近期文献中研究的量化度量紧密相关。特别是，度量为$\|\mathbf{X}\|_{\infty}$-th
    对角元素的Hessian矩阵$\mathbf{H}$。'
- en: In contrast to Wanda, we use $l_{\infty}$ norm, the error for each channel includes
    all deviations between the quantized and original weights. Therefore, rare considerable
    errors can be mitigated by a large number of small deviations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与Wanda相比，我们使用$l_{\infty}$范数，每个通道的误差包括量化权重与原始权重之间的所有偏差。因此，罕见的大误差可以通过大量的小偏差来缓解。
- en: 3.2 Quantization Noise Injection
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 量化噪声注入
- en: 'To improve our fine-tuning procedure with QNI, we avoid applying perturbations
    to sensitive weights. Therefore, after identifying columns that are sensitive
    to perturbations or salient during the fine-tuning stage, we inject quantization
    noise only into non-salient columns across all layers, as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '为了改善我们使用QNI的微调程序，我们避免对敏感权重应用扰动。因此，在微调阶段识别出对扰动敏感或显著的列后，我们仅对所有层中的非显著列注入量化噪声，如图[2](#S3.F2
    "图 2 ‣ 3 方法 ‣ GIFT-SW: 高斯噪声注入的显著权重微调")所示。'
- en: The scale parameters of the Gaussian noise are determined by the quantization
    step sizes, which are computed for each layer prior to the training process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯噪声的尺度参数由量化步长决定，这些参数在训练过程之前为每一层计算。
- en: For the weight matrix $\mathbf{W}$ is scaled with the quantization step size
    $\mathbf{\Delta}$ is given as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重矩阵$\mathbf{W}$，其缩放因子由量化步长$\mathbf{\Delta}$给出。
- en: '|  | $$\displaystyle\mho(\mathbf{W})=\begin{cases}\mathbf{W_{[:,\textit{salient}]}},\\
    \mathbf{W_{[:,\textit{non-salient}]}}+\frac{1}{2}\mathrm{diag}(\mathbf{\Delta})\mathbf{\Omega}\\'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\mho(\mathbf{W})=\begin{cases}\mathbf{W_{[:,\textit{salient}]}},\\
    \mathbf{W_{[:,\textit{non-salient}]}}+\frac{1}{2}\mathrm{diag}(\mathbf{\Delta})\mathbf{\Omega}\\'
- en: \end{cases},$$ |  | (5) |
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases},$$ |  | (5) |
- en: where $\mathrm{diag}(\mathbf{\Delta})$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{diag}(\mathbf{\Delta})$。
- en: Only weights of the salient columns $\mathbf{W_{[:,\textit{salient}]}}$ are
    frozen. We do not inject noise to salient weights since small perturbations in
    them can cause high model degradation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对显著列的权重 $\mathbf{W_{[:,\textit{salient}]}}$ 进行冻结。我们不对显著权重注入噪声，因为这些权重的小扰动可能导致模型性能的大幅下降。
- en: 'The quantization step size $\mathbf{\Delta}$-s row the scale factor $\Delta_{i}$
    is computed as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 量化步长 $\mathbf{\Delta}$-s 行的尺度因子 $\Delta_{i}$ 的计算公式为：
- en: '|  | $\Delta_{i}=\frac{\alpha_{i}}{2^{b-1}-1},$ |  | (6) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta_{i}=\frac{\alpha_{i}}{2^{b-1}-1},$ |  | (6) |'
- en: 'where $b$ is estimated by optimizing weight error through linear search as
    discussed in Appendix [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $b$ 通过线性搜索优化权重误差来估算，详细内容见附录 [A](#A1 "附录 A 均匀量化 ‣ GIFT-SW: 对 LLMs 显著权重的高斯噪声注入微调")。'
- en: 'Based on Equations [5](#S3.E5 "In 3.2 Quantization Noise Injection ‣ 3 Method
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") and
    [6](#S3.E6 "In 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs"), the variance of the
    injected noise is determined by the distribution of non-salient weights across
    rows. We exclude salient columns from this distribution, as the salient weights
    may induce large quantization error and distort row-wise scale factors. This approach
    helps us to minimize the noise variance, which, in turn, leads to a reduction
    in the deviation of the non-salient weights during training.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '基于方程 [5](#S3.E5 "在 3.2 量化噪声注入 ‣ 3 方法 ‣ GIFT-SW: 对 LLMs 显著权重的高斯噪声注入微调") 和 [6](#S3.E6
    "在 3.2 量化噪声注入 ‣ 3 方法 ‣ GIFT-SW: 对 LLMs 显著权重的高斯噪声注入微调")，注入噪声的方差由每行非显著权重的分布决定。我们将显著列从该分布中排除，因为显著权重可能会引起较大的量化误差，并扭曲逐行尺度因子。这种方法有助于最小化噪声方差，从而减少训练过程中非显著权重的偏差。'
- en: 'By sampling noise in such way we can use it for quantization pre-training experiments
    discussed in Section [6.3](#S6.SS3 "6.3 Quantization Before and After Training
    ‣ 6 Ablation ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这种方式采样噪声，我们可以将其用于第 [6.3](#S6.SS3 "6.3 训练前后的量化 ‣ 6 消融 ‣ GIFT-SW: 对 LLMs 显著权重的高斯噪声注入微调")
    节中讨论的量化预训练实验。'
- en: '|  | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
- en: '| --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca | TÜLU-V2-mix | OpenOrca
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FT | $71.97$ | $76.13$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| FT | $71.97$ | $76.13$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LoRA | $71.78$ | $75.91$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $71.78$ | $75.91$ |'
- en: '| DoRA | $72.03$ | $75.89$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $72.03$ | $75.89$ |'
- en: '| GIFT-SW | $\mathbf{73.33}$ | $\mathbf{76.37}$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{73.33}$ | $\mathbf{76.37}$ |'
- en: 'Table 1: Mean accuracy of LLaMA models fine-tuned with various instructive
    datasets and different methods.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 使用不同的指导数据集和方法对 LLaMA 模型进行微调后的平均准确率。'
- en: '| Bits | Method | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 位数 | 方法 | LLaMA2-7b | LLaMA2-13b | LLaMA3-8b |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 4 bit | STE | $72.43$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 4 bit | STE | $72.43$ |'
- en: '| QUIK + LORA | $63.99$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $63.99$ |'
- en: '| GIFT-SW | $\mathbf{72.53}$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{72.53}$ |'
- en: '| 3 bit | STE | $69.82$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 3 bit | STE | $69.82$ |'
- en: '| QUIK + LORA | $62.91$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $62.91$ |'
- en: '| GIFT-SW | $\mathbf{71.00}$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{71.00}$ |'
- en: '| 2 bit | STE | $58.20$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 2 bit | STE | $58.20$ |'
- en: '| QUIK + LORA | $41.44$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $41.44$ |'
- en: '| GIFT-SW | $\mathbf{61.09}$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{61.09}$ |'
- en: 'Table 2: Mean accuracy of quantized and then fine-tuned models. For fine-tuning
    we used TÜLU-V2-mix.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 量化后再微调模型的平均准确率。微调时我们使用了 TÜLU-V2-mix。'
- en: 4 Experiments
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we describe the experimental procedure used to test the performance
    of GIFT-SW compared to others.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了测试 GIFT-SW 性能与其他方法的实验过程。
- en: 4.1 Data
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据
- en: Following previous studies Nikdan et al. ([2024](#bib.bib30)); Hu et al. ([2021](#bib.bib16));
    Liu et al. ([2024](#bib.bib26)), we focus on the instruction tuning task. For
    this purpose, we use the TULU-V2-Mix as the main source of data Ivison et al.
    ([2023](#bib.bib17)), as it encompasses a wide range of instructions from different
    sources. This dataset has been filtered, contains a substantial amount of data
    without being too large, and models tuned to this set show superior performance.
    Additionally, we utilize the OpenOrca dataset Mukherjee et al. ([2023](#bib.bib29))
    to demonstrate that our method does not depend on a specific set of instructions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的研究，Nikdan等人 ([2024](#bib.bib30))；Hu等人 ([2021](#bib.bib16))；Liu等人 ([2024](#bib.bib26))，我们专注于指令调优任务。为此，我们使用TULU-V2-Mix作为主要数据来源，Ivison等人
    ([2023](#bib.bib17))，因为它涵盖了来自不同来源的广泛指令。该数据集已进行筛选，包含了大量数据而不至于过于庞大，且在此数据集上调优的模型表现优越。此外，我们利用OpenOrca数据集，Mukherjee等人
    ([2023](#bib.bib29))，以展示我们的方法不依赖于特定的指令集。
- en: The sensitivity metrics to find salient columns are estimated based on 512 random
    sentences from the Pile validation dataset Xiao et al. ([2023](#bib.bib44)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 用于查找显著列的灵敏度度量是基于来自Pile验证数据集的512个随机句子进行估计的，Xiao等人 ([2023](#bib.bib44))。
- en: 4.2 Baselines
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基线
- en: We consider several baselines for both full precision and quantized experiments.
    All baselines are applied to LLaMA2-7b, LLaMA2-13b and LLaMA3-8b.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了全精度和量化实验的几种基线。所有基线都应用于LLaMA2-7b、LLaMA2-13b和LLaMA3-8b。
- en: 'Full precision version includes the choice of baselines, following recent studies
    Liu et al. ([2024](#bib.bib26)); Nikdan et al. ([2024](#bib.bib30)). We employ:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 全精度版本包括最近研究的基线选择，Liu等人 ([2024](#bib.bib26))；Nikdan等人 ([2024](#bib.bib30))。我们采用：
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LoRA is a widely used adapter-based method Hu et al. ([2021](#bib.bib16))
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoRA 是一种广泛使用的基于适配器的方法，Hu等人 ([2021](#bib.bib16))
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DoRA is modification of LoRA outperforming all current PEFT methods Liu et al.
    ([2024](#bib.bib26))
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DoRA 是对LoRA的修改，优于所有当前的PEFT方法，Liu等人 ([2024](#bib.bib26))
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: FT is full fine-tuning of all parameters
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FT 是对所有参数进行全面微调
- en: We do not include PEFT methods connected with prompt tuning, as they show worse
    performance compared to adapter-based methods Xu et al. ([2023](#bib.bib45)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不包括与提示调优相关的PEFT方法，因为它们的表现比基于适配器的方法差，Xu等人 ([2023](#bib.bib45))。
- en: 'Quantized version is presented by baselines of only weight quantization at
    $\{4,3,2\}$ bit-widths:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 量化版本仅由$\{4,3,2\}$比特宽度的权重量化基线表示：
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: STE is quantization-aware fine-tuning of all parameters of a pre-trained model
    Bengio et al. ([2013](#bib.bib2)). During fine-tuning all parameters are trained,
    but 128 salient columns are updated in full-precision without quantization.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: STE 是对预训练模型所有参数进行量化感知微调的方法，Bengio等人 ([2013](#bib.bib2))。在微调过程中，所有参数都会被训练，但128个显著列以全精度更新，而不进行量化。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: QUIK + LoRA is an application of LoRA to the QUIK quantized model. Only low-rank
    adapters are trained, while the quantized weights and the salient weights are
    frozen.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: QUIK + LoRA 是将LoRA应用于QUIK量化模型。仅训练低秩适配器，同时冻结量化权重和显著权重。
- en: QUIK is a mixed-precision quantization method, that leverages GPTQ for quantization
    non-salient columns, while keeping the salient weight in full-precision Frantar
    et al. ([2022](#bib.bib13)); Ashkboos et al. ([2023](#bib.bib1)). Due to the techniques,
    QUIK achieves the highest performance among PTQ methods, such as GTPQ Frantar
    et al. ([2022](#bib.bib13)), AWQ Lin et al. ([2023](#bib.bib23)), SmoothQuant
    Xiao et al. ([2023](#bib.bib44)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: QUIK 是一种混合精度量化方法，它利用GPTQ对非显著列进行量化，同时保持显著权重为全精度，Frantar等人 ([2022](#bib.bib13))；Ashkboos等人
    ([2023](#bib.bib1))。由于这些技术，QUIK在PTQ方法中取得了最高性能，如GTPQ Frantar等人 ([2022](#bib.bib13))、AWQ
    Lin等人 ([2023](#bib.bib23))、SmoothQuant Xiao等人 ([2023](#bib.bib44))。
- en: 4.3 Evaluation and Datasets
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估与数据集
- en: We perform a comprehensive evaluation measuring zero-shot performance on HellaSwag
    Zellers et al. ([2019](#bib.bib47)), BoolQ Clark et al. ([2019](#bib.bib5)), WinoGrande
    Sakaguchi et al. ([2021](#bib.bib35)), PiQA Tata and Patel ([2003](#bib.bib41)),
    ARC-easy, and ARC-challenge Clark et al. ([2018](#bib.bib6)) using the LM Eval
    Harness Gao et al. ([2023](#bib.bib14)). The choice of baselines is similar to
    those in previous studies Egiazarian et al. ([2024](#bib.bib11)); Frantar et al.
    ([2022](#bib.bib13)); van Baalen et al. ([2024](#bib.bib42)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了全面评估，测量了在HellaSwag Zellers等（[2019](#bib.bib47)）、BoolQ Clark等（[2019](#bib.bib5)）、WinoGrande
    Sakaguchi等（[2021](#bib.bib35)）、PiQA Tata和Patel（[2003](#bib.bib41)）、ARC-easy和ARC-challenge
    Clark等（[2018](#bib.bib6)）上的零样本性能，使用了LM Eval Harness Gao等（[2023](#bib.bib14)）。基准的选择类似于以往研究中的选择Egiazarian等（[2024](#bib.bib11)）；Frantar等（[2022](#bib.bib13)）；van
    Baalen等（[2024](#bib.bib42)）。
- en: 'We demonstrate average accuracy across all the datasets, detailed per-dataset
    comparison can be found in Section [D](#A4 "Appendix D Detailed Benchmark Results
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了所有数据集的平均准确度，详细的每数据集比较可以在[D](#A4 "Appendix D Detailed Benchmark Results
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs")节中找到。'
- en: 4.4 Compute Budget
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 计算预算
- en: In all experiments, the number of salient columns in the models is fixed at
    $128$ iterations are performed within one epoch with no instruction repetitions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中，模型中的显著列数固定为$128$，迭代在一个周期内进行，无重复指令。
- en: 4.5 Training Details
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 训练细节
- en: The training was performed with 4 GPUs ( 40 GB each) for 500 iterations. The
    batch size is 128 for 7b models and 64 for 13b models. For baseline methods, the
    learning rate was set to $3\times 10^{-5}$ for the LLaMA3 model. We experimented
    with different learning rates and found these to be the most beneficial for baseline
    methods. We used a cosine annealing scheduler with the warmup ratio of 0.03\.
    The LoRA and DoRA alpha and dropout values were as specified in the original papers,
    and the rank was set to 64 to match the number of trainable parameters in our
    method. Thus, the number of trainable parameters is 160M for LLaMA2-7b, 250M for
    LLaMA2-13b, 167M for LLaMA3-8b.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 训练使用了4个GPU（每个40 GB）进行了500次迭代。批次大小对于7b模型为128，对于13b模型为64。对于基准方法，LLaMA3模型的学习率设定为$3\times
    10^{-5}$。我们尝试了不同的学习率，发现这些对基准方法最为有利。我们使用了带有0.03的预热比例的余弦退火调度器。LoRA和DoRA的alpha和dropout值按照原始论文中的规定，秩被设置为64，以匹配我们方法中的可训练参数数量。因此，LLaMA2-7b的可训练参数数量为160M，LLaMA2-13b为250M，LLaMA3-8b为167M。
- en: For our method, the learning rate was set to $1\times 10^{-4}$ of the LLaMA3
    model. We fixed the number of salient columns at 128, such that the number of
    trainable parameters is 174M for LLaMA2-7b, 272M for LLaMA2-13b, and 176M for
    LLaMA3-8b.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的方法，LLaMA3模型的学习率设定为$1\times 10^{-4}$。我们将显著列数固定为128，因此LLaMA2-7b的可训练参数为174M，LLaMA2-13b为272M，LLaMA3-8b为176M。
- en: In the case of full fune-tuning with the noise injection, the learning rate
    was set to $3\times 10^{-5}$ for LLaMA2 & 3 models, correspondingly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用噪声注入进行全面微调的情况下，LLaMA2和LLaMA3模型的学习率分别设置为$3\times 10^{-5}$。
- en: '|  | LLaMA2-7b | LLaMA2-13b |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2-7b | LLaMA2-13b |'
- en: '| --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Performance | Compute^† | Performance | Compute^† |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | 性能 | 计算^† | 性能 | 计算^† |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| TÜLU2 | $73.49$ | $13$K |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2 | $73.49$ | $13$K |'
- en: '| TÜLU2-DPO | $73.8$ | $13$K |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-DPO | $73.8$ | $13$K |'
- en: '| GIFT-SW | $73.33$ | $272$ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $73.33$ | $272$ |'
- en: 'Table 3: Comparison of Performance and Compute for LLaMA2 Models using our
    fine-tuning method versus original TÜLU2 models. Note: Compute values are represented
    as Trainable Parameters / Iterations.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用我们微调方法与原始TÜLU2模型进行LLaMA2模型性能和计算的比较。注意：计算值以可训练参数/迭代表示。
- en: 5 Results
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 'In this section, we present the results of our computational experiments and
    answer the questions posed in Section [1](#S1 "1 Introduction ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs"). In short, our results
    are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了计算实验的结果，并回答了第[1](#S1 "1 Introduction ‣ GIFT-SW: Gaussian noise Injected
    Fine-Tuning of Salient Weights for LLMs")节中提出的问题。简而言之，我们的结果如下：'
- en: 'Q1:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Q1:'
- en: The results confirm that fine-tuning a subset of salient weights produces results
    comparable to those obtained using low-rank adapters.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果确认微调一部分显著权重产生的结果与使用低秩适配器获得的结果相当。
- en: 'Q2:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Q2:'
- en: Noise injections lead to improved model performance.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 噪声注入导致了模型性能的提升。
- en: 'Q3:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Q3:'
- en: We could not confirm that models trained with noise injections are more robust
    to further degradation.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们无法确认使用噪声注入训练的模型是否对进一步的退化更具鲁棒性。
- en: 5.1 Full Precision
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 全精度
- en: 'The average performance across evaluation benchmarks for full precision models
    is presented in Table [1](#S3.T1 "Table 1 ‣ 3.2 Quantization Noise Injection ‣
    3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for
    LLMs"). GIFT-SW generally shows superior metrics across most models and instruction
    sets. However, we observe slight underperformance in LLaMA3 on the OpenOrca subset,
    where full training proves superior. This issue likely stems from the choice of
    learning rate and schedule, which can impact the tuning of outliers.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[1](#S3.T1 "Table 1 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs")展示了全精度模型在评估基准上的平均性能。GIFT-SW通常在大多数模型和指令集上表现出更优的指标。然而，我们在OpenOrca子集上观察到LLaMA3略有性能不足，完全训练在这里证明了更优。这个问题可能源于学习率和调度的选择，这会影响离群点的调优。'
- en: 5.2 Quantized Models
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 量化模型
- en: 'We present the averaged performance of models quantized with different precision
    (4, 3, 2) in Table [2](#S3.T2 "Table 2 ‣ 3.2 Quantization Noise Injection ‣ 3
    Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").
    For 4 and 3 bits GIFT-SW achieves comparable quality with STE, however, latter
    one requires significantly more compute. In the 2-bit setting, GIFT-SW shows a
    substantial quality improvement, surpassing the second-ranked model by over 5
    points.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[2](#S3.T2 "Table 2 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs")中展示了不同精度（4, 3,
    2）的量化模型的平均性能。对于4位和3位，GIFT-SW实现了与STE相当的质量，但后者需要显著更多的计算。在2位设置中，GIFT-SW显示了显著的质量提升，超过了第二名模型5分以上。'
- en: 5.3 Comparison with TÜLU2
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 与TÜLU2的比较
- en: We compare GIFT-SW with TÜLU2 models Ivison et al. ([2023](#bib.bib17)), which
    are LLaMA2 models fine-tuned using a combination of instructions and DPO Rafailov
    et al. ([2023](#bib.bib34)). These models are among the top-performing LLaMA2
    modifications but demand significant computational resources.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GIFT-SW与TÜLU2模型Ivison等人（[2023](#bib.bib17)）进行比较，这些模型是使用指令和DPO Rafailov等人（[2023](#bib.bib34)）的组合进行微调的LLaMA2模型。这些模型是表现最好的LLaMA2修改之一，但需要大量的计算资源。
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.5 Training Details ‣ 4 Experiments ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"), we show that
    by applying GIFT-SW with significantly lower computational budget (a smaller number
    of parameters and iterations) we achieve comparable results for LLaMA2-7b and
    outperform TÜLU2 for 13b.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[3](#S4.T3 "Table 3 ‣ 4.5 Training Details ‣ 4 Experiments ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs")中，我们展示了通过应用GIFT-SW以显著更低的计算预算（更少的参数和迭代）获得了与LLaMA2-7b相当的结果，并在13b模型中超越了TÜLU2。'
- en: 5.4 Scaling Properties
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 扩展属性
- en: 'We perform experiments to explore the performance of GIFT-SW and baselines
    with scaling data using LLaMA2 and LLaMA3 models. The results reported in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs") show that while LoRA and DoRA exhibit unstable performance
    with scaling data, our method and full fine-tuning are more stable. Moreover,
    our method consistently ranks first across nearly all data budgets.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了实验，探索了使用LLaMA2和LLaMA3模型的GIFT-SW和基线的性能。图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs")报告的结果显示，虽然LoRA和DoRA在扩展数据时表现不稳定，但我们的方法和全面微调则更稳定。此外，我们的方法在几乎所有数据预算中始终排名第一。'
- en: '| Method | 4 bit | 3 bit | 2 bit |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 4位 | 3位 | 2位 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Salient FT | $72.82$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 突出 FT | $72.82$ |'
- en: '| Pre-GIFT-SW | $\mathbf{73.15}$ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 预先GIFT-SW | $\mathbf{73.15}$ |'
- en: '| Post-GIFT-SW | $72.53$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 后GIFT-SW | $72.53$ |'
- en: 'Table 4: Mean performance for quantized models with or without applying GIFT-SW
    before or after quantization, results are demonstrated for LLaMA2-7b model.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：量化模型在应用或未应用GIFT-SW前后的平均性能，结果展示了LLaMA2-7b模型。
- en: '| Bits | Model | $\&#124;\mathbf{D}_{j}\&#124;_{2}^{2}\&#124;\mathbf{X}_{j}\&#124;_{2}^{2}$
    | $\&#124;\mathbf{D}_{j}\&#124;_{\infty}\&#124;\mathbf{X}_{j}\&#124;_{\infty}^{2}$
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 位数 | 模型 | $\&#124;\mathbf{D}_{j}\&#124;_{2}^{2}\&#124;\mathbf{X}_{j}\&#124;_{2}^{2}$
    | $\&#124;\mathbf{D}_{j}\&#124;_{\infty}\&#124;\mathbf{X}_{j}\&#124;_{\infty}^{2}$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 4 bit | LLaMA2-7b | $\mathbf{69.86}$ | $69.52$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 4 bit | LLaMA2-7b | $\mathbf{69.86}$ | $69.52$ |'
- en: '| TÜLU2-7b | $72.94$ | $72.78$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-7b | $72.94$ | $72.78$ |'
- en: '| LLaMA2-13b | $72.92$ | $72.56$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | $72.92$ | $72.56$ |'
- en: '| TÜLU2-13b | $75.12$ | $75.17$ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-13b | $75.12$ | $75.17$ |'
- en: '| 3 bit | LLaMA2-7b | $67.50$ | $67.86$ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 3 位 | LLaMA2-7b | $67.50$ | $67.86$ |'
- en: '| TÜLU2-7b | $70.91$ | $70.88$ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-7b | $70.91$ | $70.88$ |'
- en: '| LLaMA2-13b | $71.92$ | $71.45$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | $71.92$ | $71.45$ |'
- en: '| TÜLU2-13b | $\mathbf{74.33}$ | $74.31$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-13b | $\mathbf{74.33}$ | $74.31$ |'
- en: '| 2 bit | LLaMA2-7b | $45.86$ | $\mathbf{46.83}$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 2 位 | LLaMA2-7b | $45.86$ | $\mathbf{46.83}$ |'
- en: '| TÜLU2-7b | $\mathbf{54.84}$ | $48.20$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-7b | $\mathbf{54.84}$ | $48.20$ |'
- en: '| LLaMA2-13b | $57.07$ | $56.73$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | $57.07$ | $56.73$ |'
- en: '| TÜLU2-13b | $59.62$ | $59.39$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| TÜLU2-13b | $59.62$ | $59.39$ |'
- en: 'Table 5: Performance of LLaMA2 and TÜLU2 models after QUIK quantization with
    salient columns selected via various metrics. Weight perturbation is given by
    $\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：经过QUIK量化后的LLaMA2和TÜLU2模型的性能，显著列通过各种度量进行选择。权重扰动由$\mathbf{D}_{j}=\mathbf{W}_{:,j}-Q(\mathbf{W}_{:,j})$给出。
- en: '| Model | Outliers FT | Full FT |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 异常值 FT | 完整 FT |'
- en: '| --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| w/ Noise | w/o Noise | w/ Noise | w/o Noise |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 有噪声 | 无噪声 | 有噪声 | 无噪声 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLaMA2-7b | $\mathbf{73.33}$ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7b | $\mathbf{73.33}$ |'
- en: '| LLaMA2-13b | $\mathbf{75.93}$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | $\mathbf{75.93}$ |'
- en: '| LLaMA3-8b | $\mathbf{76.37}$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | $\mathbf{76.37}$ |'
- en: 'Table 6: Mean Performance of LLaMA models with and without Noise Injection
    for outlier fine-tuning and full model fine-tuning'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：LLaMA模型在异常值微调和完整模型微调中有无噪声注入的平均性能
- en: 6 Ablation
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 消融实验
- en: 6.1 Comparison sensitivity metrics
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 灵敏度度量比较
- en: We study sensitivity metrics with respect to different noise levels (various
    perturbation magnitudes), which translate into varying quantization precision.
    In this experiment, the non-salient weights of LLaMA2 and TÜLU2 with 7B and 13B
    parameters. Models are quantized with QUIK, the salient weights are not updated.
    We select 128 columns of salient weights.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了不同噪声水平（各种扰动幅度）的灵敏度度量，这些噪声水平转化为不同的量化精度。在此实验中，LLaMA2和TÜLU2的7B和13B参数的非显著权重模型通过QUIK量化，显著权重没有更新。我们选择了128列显著权重。
- en: 'Mean results for zero-shot tasks in Table [5](#S5.T5 "Table 5 ‣ 5.4 Scaling
    Properties ‣ 5 Results ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient
    Weights for LLMs") show that for most precisions, the best performance is achieved
    with salient columns identified by Equation [4](#S3.E4 "In 3.1 Generalizing parameter
    sensitivity metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs") with $\gamma=1,\rho=\infty,\tau=\infty$ norm of
    the input feature (the OWQ metric) show better performance only for TÜLU2 quantized
    to 3 and 2 bits. Choosing salient columns solely by the input features (the QUIK
    metric) leads to underperformance, especially for 2 bit. Therefore, identifying
    salient columns sensitive to quantization noise requires considering both the
    weight quantization error and the maximum values of input activation.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S5.T5 "Table 5 ‣ 5.4 Scaling Properties ‣ 5 Results ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs")中的零-shot任务的平均结果显示，对于大多数精度，最佳性能是通过方程[4](#S3.E4
    "In 3.1 Generalizing parameter sensitivity metrics ‣ 3 Method ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs")中确定的显著列实现的，其中$\gamma=1,\rho=\infty,\tau=\infty$输入特征的范数（OWQ度量）仅在TÜLU2量化为3和2位时表现更好。仅通过输入特征（QUIK度量）选择显著列会导致性能不足，尤其是对于2位。因此，识别对量化噪声敏感的显著列需要同时考虑权重量化误差和输入激活的最大值。'
- en: Based on the results, we chose the best-performing sensitivity metric with $\gamma=1,\rho=\infty,\tau=\infty$.
    However, the results do not reveal a clear rule for selecting the optimal sensitivity
    metric, as performance varies across different bit-widths and models with no discernible
    pattern. This remains an area for future research.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们选择了表现最佳的灵敏度度量，$\gamma=1,\rho=\infty,\tau=\infty$。然而，结果并未揭示出选择最佳灵敏度度量的明确规则，因为性能在不同的位宽和模型中变化，没有明显的模式。这仍然是未来研究的一个领域。
- en: 6.2 Noise Injection Impact
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 噪声注入影响
- en: To ablate the importance of QNI in the full-precision setting, we measure the
    mean performance of LLaMA2 models with and without noise injections for both salient
    columns fine-tuning and full fine-tuning. In the latter case, the noise is applied
    to the entire weight matrix.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除QNI在全精度设置中的重要性，我们测量了LLaMA2模型在显著列微调和完整微调中有无噪声注入的平均性能。在后一种情况下，噪声应用于整个权重矩阵。
- en: 'The results in Table [6](#S5.T6 "Table 6 ‣ 5.4 Scaling Properties ‣ 5 Results
    ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") show
    that QNI consistently enhances the performance of outlier fine-tuning. Although
    QNI can reduce performance when applied to the entire network, it still benefits
    LLaMA3-8b. Notably, outlier fine-tuning outperforms full fine-tuning, but only
    when QNI is used.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表[6](#S5.T6 "Table 6 ‣ 5.4 Scaling Properties ‣ 5 Results ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs")中的结果显示 QNI 一致地提升了离群点微调的性能。尽管
    QNI 在应用于整个网络时可能降低性能，但它仍然对 LLaMA3-8b 有益。值得注意的是，离群点微调的表现优于全微调，但仅在使用 QNI 时。'
- en: 6.3 Quantization Before and After Training
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 训练前后的量化
- en: 'From studies related to QAT, it is known that pre-training a model with noise
    injection enables to improve its predictive capabilities after quantization Défossez
    et al. ([2021](#bib.bib7)); Shvetsov et al. ([2022](#bib.bib38)). Based on those
    observations, in this section we examine the performance of the quantized LLaMA2-7b
    after fine-tuning full precision salient columns in several settings:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 从与 QAT 相关的研究中已知，通过噪声注入对模型进行预训练可以在量化后提高其预测能力 Défossez et al. ([2021](#bib.bib7));
    Shvetsov et al. ([2022](#bib.bib38))。基于这些观察，本节我们将考察在多种设置下，经过全精度重要列微调后的量化 LLaMA2-7b
    的表现：
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pre-GIFT-SW. Applying GIFT-SW prior to the quantization.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pre-GIFT-SW。在量化前应用 GIFT-SW。
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Post-GIFT-SW. Applying GIFT-SW after the quantization.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Post-GIFT-SW。在量化后应用 GIFT-SW。
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Salient FT. Fine-tuning salient columns after quantization with no noise injected
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要 FT。在量化后没有注入噪声的情况下微调重要列
- en: In the case of the pre-training, the bit-width for the model quantization corresponds
    to the noise level injected during the training. For the post-training, the noise
    injection is always performed at 4 bit.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练的情况下，模型量化的比特宽度对应于训练过程中注入的噪声水平。对于后训练，噪声注入始终在 4 位进行。
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.4 Scaling Properties ‣ 5 Results ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs") presents the average
    scores achieved by the models across evaluation benchmark. In the case of 4 bit
    quantization the Pre-GIFT-SW model considerable outperforms other models. But
    in the case of 3 and 2 bits, fine-tuning salient columns after quantization enables
    to achieve quantized models better generative capabilities.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '表[4](#S5.T4 "Table 4 ‣ 5.4 Scaling Properties ‣ 5 Results ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs")展示了模型在评估基准中的平均得分。在 4 位量化的情况下，Pre-GIFT-SW
    模型显著优于其他模型。但在 3 位和 2 位的情况下，量化后微调重要列能够实现量化模型更好的生成能力。'
- en: It can be explained by significant deviation of the quantized weights from their
    original values that is induced by the extremely low-bit quantization. As a result,
    the interrelations between the salient weights and the quantized weights are disrupted,
    and the positive effect of pre-training disappears. However, post-training of
    the salient weight enables to form them new relations with other weights, so the
    model partially recovers its generative capabilities.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过极低位量化引起的量化权重与原始值之间的显著偏差来解释。因此，重要权重与量化权重之间的相互关系被破坏，预训练的积极效果消失。然而，对重要权重进行后训练能够形成它们与其他权重的新关系，从而使模型部分恢复其生成能力。
- en: Also it can be observed that application of Post-GIFT-SW and Salient FT to model
    quantized in 3 bit gives the similar scores. But in the case of 2 bit quantization,
    the noise injection improves the fine-tuning of the quantized model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以观察到，将 Post-GIFT-SW 和 Salient FT 应用到 3 位量化的模型上会得到类似的得分。但在 2 位量化的情况下，噪声注入可以改善量化模型的微调。
- en: 7 Conclusion
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we introduce GIFT-SW, a parameter-efficient fine-tuning method
    that trains only weights in a small subset of salient columns while injecting
    quantization noise into the frozen weights. GIFT-SW proves to be superior to previous
    fine-tuning strategies in both full precision and quantized settings, requiring
    less compute budget. In data scaling experiments, GIFT-SW demonstrates greater
    stability than previous PEFT methods and outperforms both PEFT and full fine-tuning
    across nearly all data budgets. Our ablation studies show that QNI is beneficial
    but only with salient weights. Although GIFT-SW outperforms previous methods,
    further research is needed to determine how to maximize its performance in quantized
    settings.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 GIFT-SW，这是一种参数高效的微调方法，它仅训练少量显著列中的权重，同时将量化噪声注入到冻结权重中。GIFT-SW 在全精度和量化设置下均优于以前的微调策略，且所需计算预算较少。在数据规模实验中，GIFT-SW
    显示出比以前的 PEFT 方法更大的稳定性，并且在几乎所有数据预算下超越了 PEFT 和全面微调。我们的消融研究表明，QNI 是有益的，但仅适用于显著权重。尽管
    GIFT-SW 优于以前的方法，但仍需进一步研究以确定如何在量化设置中最大化其性能。
- en: We generalize the criterion for selecting salient columns from previous studies
    and empirically compare various parameters. Our experiments show that while some
    criteria perform better than others, none emerge as a clear dominant choice. This
    significant finding underscores the need for further research to refine these
    criteria.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以前的研究中概括了选择显著列的标准，并对各种参数进行了实证比较。我们的实验表明，尽管一些标准表现优于其他标准，但没有一个标准表现出明显的主导地位。这一重要发现突显了进一步研究以改进这些标准的必要性。
- en: 8 Limitations
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: 'We find the main limitations of our work as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现我们工作的主要限制如下：
- en: '1.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We report results of GIFT-SW exclusively for LLaMA models. Currently, numerous
    open-source pre-trained LLMs with high generative capabilities are available.
    However, LLaMA models are the most commonly chosen for studying the efficiency
    of modern PEFT and quantization methods. Despite the architectural similarities
    among most LLMs, future experiments with different models are necessary.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们仅报告了 GIFT-SW 在 LLaMA 模型上的结果。目前，有许多具有高生成能力的开源预训练 LLM 可用。然而，LLaMA 模型是研究现代 PEFT
    和量化方法效率时最常选择的模型。尽管大多数 LLM 在架构上有相似之处，但仍需要对不同模型进行未来的实验。
- en: '2.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For quantizing models, we use only the GPTQ method, which is widely used for
    mixed-precision quantization of LLMs. This method improves the performance of
    quantized models by aggregating quantization error into columns stored in full
    precision. However, GIFT-SW can be easily integrated with other methods, such
    as conventional RTN or QuantEase.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于模型的量化，我们仅使用了 GPTQ 方法，这是一种广泛用于 LLM 混合精度量化的方法。该方法通过将量化误差聚合到以全精度存储的列中来提高量化模型的性能。然而，GIFT-SW
    可以轻松地与其他方法集成，如传统的 RTN 或 QuantEase。
- en: '3.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Experiments with GIFT-SW report results for salient columns selected using
    the sensitivity metric ([4](#S3.E4 "In 3.1 Generalizing parameter sensitivity
    metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs")) with $\gamma=1$. Our proposed metric, based on our analysis, shows
    high sensitivity of the salient columns to quantization in most LLaMA2 cases.
    However, other sensitivity metrics may yield better performance for GIFT-SW and
    mixed-precision quantization in different LLMs.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 GIFT-SW 的实验报告了使用灵敏度度量（[4](#S3.E4 "在 3.1 泛化参数灵敏度度量 ‣ 3 方法 ‣ GIFT-SW：高斯噪声注入显著权重的微调")）选择的显著列的结果，其中
    $\gamma=1$。根据我们的分析，我们提出的度量在大多数 LLaMA2 情况下显示了显著列对量化的高敏感性。然而，其他灵敏度度量可能在 GIFT-SW
    和不同 LLM 的混合精度量化中表现更好。
- en: '4.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Noise parameters for fine-tuning the salient weights are determined using the
    QNI approach. However, other noise distributions may also enhance the fine-tuning
    process. Identifying the optimal noise distribution is beyond the scope of this
    paper.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于微调显著权重的噪声参数是通过 QNI 方法确定的。然而，其他噪声分布也可能增强微调过程。确定最佳噪声分布超出了本文的范围。
- en: '5.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: In this study, we focus on developing the GIFT-SW algorithm for effective fine-tuning
    of LLMs, but we do not provide computationally efficient implementations of CUDA
    kernels for the algorithm. In the future, CUDA kernels for GIFT-SW can be developed
    based on the code from QUIK Ashkboos et al. ([2023](#bib.bib1)) and OWQ Lee et al.
    ([2024](#bib.bib22)).
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本研究中，我们专注于开发GIFT-SW算法，以便有效地微调LLMs，但我们并未提供该算法的CUDA内核的计算高效实现。未来，可以基于QUIK Ashkboos等人（[2023](#bib.bib1)）和OWQ
    Lee等人（[2024](#bib.bib22)）的代码开发GIFT-SW的CUDA内核。
- en: '6.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: We train GIFT-SW with only a few fine-tuning instruction sets, selected for
    their size and high benchmark results in previous studies. However, expanding
    the number of fine-tuning sets could make the experiments more comprehensive.
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用仅有的几个微调指令集对GIFT-SW进行训练，这些指令集因其规模和在前期研究中的高基准结果而被选中。然而，增加微调集的数量可能使实验更加全面。
- en: '7.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: We evaluate our method using six distinct benchmarks inherited from various
    previous studies. In future research, it would be beneficial to include more benchmarks
    to gain additional insights.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用来自各个前期研究的六个不同基准测试来评估我们的方法。在未来的研究中，纳入更多基准测试将有助于获得额外的见解。
- en: 9 Potential Risks
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 潜在风险
- en: The GIFT-SW method poses risks similar to those of any PEFT method. For example,
    it omits explicit safety training measures, so could be applied to fine-tune LLMs
    for generating harmful content. Also it can be applied to tailor LLMs to tailor
    highly specific and potentially dangerous outputs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: GIFT-SW方法存在类似于任何PEFT方法的风险。例如，它省略了明确的安全培训措施，因此可能被用于微调LLMs以生成有害内容。同时，它也可能被用于将LLMs调整为生成高度特定且潜在危险的输出。
- en: 10 Acknowledgment
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 致谢
- en: The work was supported by the Analytical center under the RF Government (subsidy
    agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了俄罗斯政府分析中心的资助（补贴协议 000000D730321P5Q0002，资助号 70-2021-00145 02.11.2021）。
- en: References
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ashkboos et al. (2023) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023. Towards
    end-to-end 4-bit inference on generative large language models. *arXiv preprint
    arXiv:2310.09259*.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos et al. (2023) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, 和 Dan Alistarh. 2023. 朝向端到端4位推理在生成大型语言模型上的应用。*arXiv预印本
    arXiv:2310.09259*。
- en: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013.
    Estimating or propagating gradients through stochastic neurons for conditional
    computation. *arXiv preprint arXiv:1308.3432*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2013) Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville. 2013.
    通过随机神经元估计或传播梯度以进行条件计算。*arXiv预印本 arXiv:1308.3432*。
- en: Bishop (1995) Chris M Bishop. 1995. Training with noise is equivalent to tikhonov
    regularization. *Neural computation*, 7(1):108–116.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bishop (1995) Chris M Bishop. 1995. 训练噪声等同于Tikhonov正则化。*神经计算*，7(1):108–116。
- en: Camuto et al. (2020) Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J
    Roberts, and Chris C Holmes. 2020. Explicit regularisation in gaussian noise injections.
    *Advances in Neural Information Processing Systems*, 33:16603–16614.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Camuto et al. (2020) Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen
    J Roberts, 和 Chris C Holmes. 2020. 高斯噪声注入中的显式正则化。*神经信息处理系统进展*，33:16603–16614。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. 2019. Boolq：探索自然是/否问题的惊人难度。*arXiv预印本 arXiv:1905.10044*。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 2018. 认为你已经解决了问答问题？试试arc，AI2推理挑战。*arXiv预印本
    arXiv:1803.05457*。
- en: Défossez et al. (2021) Alexandre Défossez, Yossi Adi, and Gabriel Synnaeve.
    2021. Differentiable model compression via pseudo quantization noise. *arXiv preprint
    arXiv:2104.09987*.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Défossez et al. (2021) Alexandre Défossez, Yossi Adi, 和 Gabriel Synnaeve. 2021.
    通过伪量化噪声进行可微模型压缩。*arXiv预印本 arXiv:2104.09987*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。2022。GPT-3\.
    int8 (): 大规模变换器的 8 位矩阵乘法。*神经信息处理系统进展*，35:30318–30332。'
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2024）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。2024。QLora：量化大语言模型的高效微调。*神经信息处理系统进展*，36。
- en: 'Dettmers et al. (2023) Tim Dettmers, Ruslan A Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. 2023. Spqr: A sparse-quantized representation for near-lossless
    llm weight compression. In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Tim Dettmers、Ruslan A Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler 和 Dan Alistarh。2023。SPQR：一种稀疏量化表示用于接近无损的
    LLM 权重压缩。在*第十二届国际表示学习会议*。
- en: Egiazarian et al. (2024) Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large
    language models via additive quantization. *arXiv preprint arXiv:2401.06118*.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等（2024）Vage Egiazarian、Andrei Panferov、Denis Kuznedelev、Elias Frantar、Artem
    Babenko 和 Dan Alistarh。2024。通过加性量化对大语言模型进行极限压缩。*arXiv 预印本 arXiv:2401.06118*。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    massive language models can be accurately pruned in one-shot. In *Proceedings
    of the 40th International Conference on Machine Learning*, pages 10323–10337.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh（2023）Elias Frantar 和 Dan Alistarh。2023。SparseGPT：可以一次性准确修剪大规模语言模型。在*第40届国际机器学习会议论文集*，第10323–10337页。
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。2022。GPTQ：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*。
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2023）Leo Gao、Jonathan Tow、Baber Abbasi、Stella Biderman、Sid Black、Anthony
    DiPofi、Charles Foster、Laurence Golding、Jeffrey Hsu、Alain Le Noac’h、Haonan Li、Kyle
    McDonell、Niklas Muennighoff、Chris Ociepa、Jason Phang、Laria Reynolds、Hailey Schoelkopf、Aviya
    Skowron、Lintang Sutawika、Eric Tang、Anish Thite、Ben Wang、Kevin Wang 和 Andy Zou。2023。[少样本语言模型评估框架](https://doi.org/10.5281/zenodo.10256836)。
- en: Gurnee et al. (2024) Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah,
    Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas. 2024. Universal
    neurons in gpt2 language models. *arXiv preprint arXiv:2401.12181*.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurnee 等（2024）Wes Gurnee、Theo Horsley、Zifan Carl Guo、Tara Rezaei Kheirkhah、Qinyi
    Sun、Will Hathaway、Neel Nanda 和 Dimitris Bertsimas。2024。GPT-2 语言模型中的**通用神经元**。*arXiv
    预印本 arXiv:2401.12181*。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Edward J Hu、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean Wang、Lu
    Wang、Weizhu Chen 等。2021。LoRA：大规模语言模型的低秩适应。在*国际表示学习会议*。
- en: 'Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan
    Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. 2023. [Camels in a changing climate: Enhancing
    lm adaptation with tulu 2](https://arxiv.org/abs/2311.10702). *Preprint*, arXiv:2311.10702.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivison 等（2023）Hamish Ivison、Yizhong Wang、Valentina Pyatkin、Nathan Lambert、Matthew
    Peters、Pradeep Dasigi、Joel Jang、David Wadden、Noah A. Smith、Iz Beltagy 和 Hannaneh
    Hajishirzi。2023。[气候变化中的骆驼：使用 Tulu 2 提升语言模型适应性](https://arxiv.org/abs/2311.10702)。*预印本*，arXiv:2311.10702。
- en: 'Jin et al. (2021) Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and
    Michael I Jordan. 2021. On nonconvex optimization for machine learning: Gradients,
    stochasticity, and saddle points. *Journal of the ACM (JACM)*, 68(2):1–29.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等（2021）Chi Jin、Praneeth Netrapalli、Rong Ge、Sham M Kakade 和 Michael I Jordan。2021。关于机器学习中的非凸优化：梯度、随机性和鞍点。*ACM
    杂志（JACM）*，68(2):1–29。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023）Sehoon Kim、Coleman Hooper、Amir Gholami、Zhen Dong、Xiuyu Li、Sheng Shen、Michael
    W Mahoney 和 Kurt Keutzer。2023。《Squeezellm：密集与稀疏量化》。*arXiv 预印本 arXiv:2306.07629*。
- en: Komatsuzaki (2019) Aran Komatsuzaki. 2019. One epoch is all you need. *arXiv
    preprint arXiv:1906.06669*.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komatsuzaki（2019）Aran Komatsuzaki。2019。《一个周期就是你所需要的》。*arXiv 预印本 arXiv:1906.06669*。
- en: LeCun et al. (1989) Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain
    damage. *Advances in neural information processing systems*, 2.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1989）Yann LeCun、John Denker 和 Sara Solla。1989。《Optimal brain damage》。*Advances
    in neural information processing systems*，第 2 卷。
- en: 'Lee et al. (2024) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok
    Park. 2024. Owq: Outlier-aware weight quantization for efficient fine-tuning and
    inference of large language models. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, volume 38, pages 13355–13364.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2024）Changhun Lee、Jungyu Jin、Taesu Kim、Hyungjun Kim 和 Eunhyeok Park。2024。《Owq：面向高效微调和推理的大型语言模型的异常值感知权重量化》。在
    *AAAI 人工智能会议论文集*，第 38 卷，第 13355–13364 页。
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2023.
    Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2023）Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Wei-Ming Chen、Wei-Chen
    Wang、Guangxuan Xiao、Xingyu Dang、Chuang Gan 和 Song Han。2023。《Awq：激活感知的权重量化用于 llm
    压缩和加速》。*arXiv 预印本 arXiv:2306.00978*。
- en: Lin et al. (2017) Xiaofan Lin, Cong Zhao, and Wei Pan. 2017. Towards accurate
    binary convolutional neural network. *Advances in neural information processing
    systems*, 30.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2017）Xiaofan Lin、Cong Zhao 和 Wei Pan。2017。《朝着准确的二进制卷积神经网络》。*Advances in
    neural information processing systems*，第 30 卷。
- en: 'Liu et al. (2023) Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson,
    and Rongrong Wang. 2023. Pac-tuning: Fine-tuning pretrained language models with
    pac-driven perturbed gradient descent. *arXiv preprint arXiv:2310.17588*.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Guangliang Liu、Zhiyu Xue、Xitong Zhang、Kristen Marie Johnson 和 Rongrong
    Wang。2023。《Pac-tuning：基于 Pac 驱动的扰动梯度下降的预训练语言模型微调》。*arXiv 预印本 arXiv:2310.17588*。
- en: 'Liu et al. (2024) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov,
    Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024. Dora: Weight-decomposed
    low-rank adaptation. *arXiv preprint arXiv:2402.09353*.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Shih-Yang Liu、Chien-Yi Wang、Hongxu Yin、Pavlo Molchanov、Yu-Chiang
    Frank Wang、Kwang-Ting Cheng 和 Min-Hung Chen。2024。《Dora：权重分解低秩适应》。*arXiv 预印本 arXiv:2402.09353*。
- en: Moreno-Barea et al. (2018) Francisco J Moreno-Barea, Fiammetta Strazzera, José M
    Jerez, Daniel Urda, and Leonardo Franco. 2018. Forward noise adjustment scheme
    for data augmentation. In *2018 IEEE symposium series on computational intelligence
    (SSCI)*, pages 728–734\. IEEE.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moreno-Barea 等（2018）Francisco J Moreno-Barea、Fiammetta Strazzera、José M Jerez、Daniel
    Urda 和 Leonardo Franco。2018。《用于数据增强的前向噪声调整方案》。在 *2018 IEEE 计算智能研讨会系列（SSCI）*，第
    728–734 页。IEEE。
- en: Moskvoretskii et al. (2024) Viktor Moskvoretskii, Alexander Panchenko, and Irina
    Nikishina. 2024. [Are large language models good at lexical semantics? a case
    of taxonomy learning](https://aclanthology.org/2024.lrec-main.133). In *Proceedings
    of the 2024 Joint International Conference on Computational Linguistics, Language
    Resources and Evaluation (LREC-COLING 2024)*, pages 1498–1510, Torino, Italia.
    ELRA and ICCL.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moskvoretskii 等（2024）Viktor Moskvoretskii、Alexander Panchenko 和 Irina Nikishina。2024。《[大型语言模型在词汇语义方面表现如何？以分类学习为例](https://aclanthology.org/2024.lrec-main.133)》。在
    *2024 年联合国际计算语言学会议、语言资源与评估会议（LREC-COLING 2024）*，第 1498–1510 页，意大利都灵。ELRA 和 ICCL。
- en: 'Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar,
    Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. [Orca: Progressive learning
    from complex explanation traces of gpt-4](https://arxiv.org/abs/2306.02707). *Preprint*,
    arXiv:2306.02707.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukherjee 等（2023）Subhabrata Mukherjee、Arindam Mitra、Ganesh Jawahar、Sahaj Agarwal、Hamid
    Palangi 和 Ahmed Awadallah。2023。《[Orca：从 GPT-4 的复杂解释轨迹中逐步学习](https://arxiv.org/abs/2306.02707)》。*预印本*，arXiv:2306.02707。
- en: 'Nikdan et al. (2024) Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. 2024.
    Rosa: Accurate parameter-efficient fine-tuning via robust adaptation. *arXiv preprint
    arXiv:2401.04679*.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nikdan 等（2024）Mahdi Nikdan、Soroush Tabesh 和 Dan Alistarh。2024。《Rosa：通过稳健适应进行准确的参数高效微调》。*arXiv
    预印本 arXiv:2401.04679*。
- en: Orvieto et al. (2023) Antonio Orvieto, Anant Raj, Hans Kersting, and Francis
    Bach. 2023. Explicit regularization in overparametrized models via noise injection.
    In *International Conference on Artificial Intelligence and Statistics*, pages
    7265–7287\. PMLR.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orvieto 等 (2023) Antonio Orvieto, Anant Raj, Hans Kersting 和 Francis Bach. 2023.
    通过噪声注入在过参数化模型中显式正则化. 在 *国际人工智能与统计会议*，第7265–7287页。PMLR.
- en: Panda and Roy (2021) Priyadarshini Panda and Kaushik Roy. 2021. Implicit adversarial
    data augmentation and robustness with noise-based learning. *Neural Networks*,
    141:120–132.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panda 和 Roy (2021) Priyadarshini Panda 和 Kaushik Roy. 2021. 隐式对抗数据增强与基于噪声的鲁棒性.
    *神经网络*, 141:120–132.
- en: Poole et al. (2014) Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli. 2014.
    Analyzing noise in autoencoders and deep networks. *arXiv preprint arXiv:1406.1831*.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poole 等 (2014) Ben Poole, Jascha Sohl-Dickstein 和 Surya Ganguli. 2014. 分析自动编码器和深度网络中的噪声.
    *arXiv 预印本 arXiv:1406.1831*.
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. 2023. [Direct preference optimization:
    Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290).
    *Preprint*, arXiv:2305.18290.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等 (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,
    Christopher D. Manning 和 Chelsea Finn. 2023. [直接偏好优化：你的语言模型秘密地是一个奖励模型](https://arxiv.org/abs/2305.18290).
    *预印本*，arXiv:2305.18290.
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi 等 (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和
    Yejin Choi. 2021. Winogrande: 大规模对抗性 Winograd 语法挑战. *ACM 通讯*, 64(9):99–106.'
- en: 'Shang et al. (2023) Yuzhang Shang, Zhihang Yuan, and Zhen Dong. 2023. Pb-llm:
    Partially binarized large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shang 等 (2023) Yuzhang Shang, Zhihang Yuan 和 Zhen Dong. 2023. Pb-llm: 部分二值化大型语言模型.
    在 *第十二届国际学习表征会议*.'
- en: 'Shin et al. (2023) Juncheol Shin, Junhyuk So, Sein Park, Seungyeop Kang, Sungjoo
    Yoo, and Eunhyeok Park. 2023. Nipq: Noise proxy-based integrated pseudo-quantization.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 3852–3861.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shin 等 (2023) Juncheol Shin, Junhyuk So, Sein Park, Seungyeop Kang, Sungjoo
    Yoo 和 Eunhyeok Park. 2023. Nipq: 基于噪声代理的集成伪量化. 在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，第3852–3861页.'
- en: 'Shvetsov et al. (2022) Egor Shvetsov, Dmitry Osin, Alexey Zaytsev, Ivan Koryakovskiy,
    Valentin Buchnev, Ilya Trofimov, and Evgeny Burnaev. 2022. Quantnas for super
    resolution: searching for efficient quantization-friendly architectures against
    quantization noise. *arXiv preprint arXiv:2208.14839*.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shvetsov 等 (2022) Egor Shvetsov, Dmitry Osin, Alexey Zaytsev, Ivan Koryakovskiy,
    Valentin Buchnev, Ilya Trofimov 和 Evgeny Burnaev. 2022. 用于超分辨率的 Quantnas: 寻找高效的量化友好架构以对抗量化噪声.
    *arXiv 预印本 arXiv:2208.14839*.'
- en: 'Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent
    neural networks from overfitting. *The journal of machine learning research*,
    15(1):1929–1958.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srivastava 等 (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
    Sutskever 和 Ruslan Salakhutdinov. 2014. Dropout: 一种简单的方法来防止神经网络过拟合. *机器学习研究杂志*,
    15(1):1929–1958.'
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. In *The Twelfth
    International Conference on Learning Representations*.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2023) Mingjie Sun, Zhuang Liu, Anna Bair 和 J Zico Kolter. 2023. 一种简单而有效的大型语言模型剪枝方法.
    在 *第十二届国际学习表征会议*.
- en: 'Tata and Patel (2003) Sandeep Tata and Jignesh M Patel. 2003. Piqa: An algebra
    for querying protein data sets. In *15th International Conference on Scientific
    and Statistical Database Management, 2003.*, pages 141–150\. IEEE.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tata 和 Patel (2003) Sandeep Tata 和 Jignesh M Patel. 2003. Piqa: 用于查询蛋白质数据集的代数.
    在 *第15届国际科学与统计数据库管理会议，2003年*，第141–150页。IEEE.'
- en: 'van Baalen et al. (2024) Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter
    Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, and Paul Whatmough.
    2024. Gptvq: The blessing of dimensionality for llm quantization. *arXiv preprint
    arXiv:2402.15319*.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van Baalen 等 (2024) Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus,
    Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort 和 Paul Whatmough. 2024. Gptvq:
    LLM 量化的维度祝福. *arXiv 预印本 arXiv:2402.15319*.'
- en: Widrow et al. (1996) Bernard Widrow, Istvan Kollar, and Ming-Chang Liu. 1996.
    Statistical theory of quantization. *IEEE Transactions on instrumentation and
    measurement*, 45(2):353–361.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Widrow 等 (1996) Bernard Widrow, Istvan Kollar 和 Ming-Chang Liu. 1996. 量化的统计理论.
    *IEEE 仪器与测量学报*, 45(2):353–361.
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. 2023. Smoothquant：大型语言模型的准确和高效后训练量化。在 *国际机器学习会议*，第 38087–38099 页。PMLR。
- en: 'Xu et al. (2023) Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and
    Fu Lee Wang. 2023. Parameter-efficient fine-tuning methods for pretrained language
    models: A critical review and assessment. *arXiv preprint arXiv:2312.12148*.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023) Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, 和 Fu Lee
    Wang. 2023. 预训练语言模型的参数高效微调方法：批判性回顾和评估。 *arXiv 预印本 arXiv:2312.12148*。
- en: 'Yin et al. (2023) Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang,
    Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. 2023.
    Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms
    to high sparsity. In *Conference on Parsimony and Learning (Recent Spotlight Track)*.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等人 (2023) Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling
    Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, 和 Shiwei Liu. 2023. 离群加权逐层稀疏性
    (owl)：剪枝大型语言模型以实现高稀疏性的缺失秘密成分。在 *节俭与学习会议 (近期聚焦讲座)*。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等人 (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和
    Yejin Choi. 2019. Hellaswag：机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*。
- en: 'Zhu et al. (2018) Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
    2018. The anisotropic noise in stochastic gradient descent: Its behavior of escaping
    from sharp minima and regularization effects. *arXiv preprint arXiv:1803.00195*.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2018) Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, 和 Jinwen Ma. 2018.
    随机梯度下降中的各向异性噪声：其逃避尖锐极小值的行为及正则化效应。 *arXiv 预印本 arXiv:1803.00195*。
- en: Appendix A Uniform quantization
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 均匀量化
- en: 'While non-uniform quantization may lead to higher compression rates, in our
    work we focus on uniform quantization since it widely used in efficient PTQ methods
    such as GPTQ, QUIK, OWQ Frantar et al. ([2022](#bib.bib13)); Ashkboos et al. ([2023](#bib.bib1));
    Lee et al. ([2024](#bib.bib22)). Quantization is a mapping that converts a range
    of full-precision values into a discrete range of values allowing usage of integer
    arithmetic and reduced memory consumption. For example, Fig. [3](#A1.F3 "Figure
    3 ‣ Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs") depicts a mapping with the quantization scale size
    $\Delta=\frac{1}{4}$ into integer values.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管非均匀量化可能导致更高的压缩率，但在我们的工作中，我们专注于均匀量化，因为它广泛应用于高效的 PTQ 方法，如 GPTQ、QUIK、OWQ Frantar
    等 ([2022](#bib.bib13))；Ashkboos 等 ([2023](#bib.bib1))；Lee 等 ([2024](#bib.bib22))。量化是将一系列全精度值转换为离散范围值的映射，从而允许使用整数算术和减少内存消耗。例如，图
    [3](#A1.F3 "Figure 3 ‣ Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise
    Injected Fine-Tuning of Salient Weights for LLMs") 描绘了将量化尺度大小 $\Delta=\frac{1}{4}$
    映射到整数值的情况。'
- en: In our work we apply uniform symmetric quantization with the row-wise quantization
    step size $\mathbf{\Delta}$ as below
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们应用了均匀对称量化，行级量化步长 $\mathbf{\Delta}$ 如下所示
- en: '|  | $\displaystyle q_{\text{min}}=-2^{b-1},\quad q_{\text{max}}=2^{b-1}-1$
    |  | (7) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q_{\text{min}}=-2^{b-1},\quad q_{\text{max}}=2^{b-1}-1$
    |  | (7) |'
- en: '|  | $\displaystyle\text{clamp}(x;q_{\text{min}},q_{\text{max}})=\max(q_{\text{min}},\min(x,q_{\text{max}}))$
    |  | (8) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{clamp}(x;q_{\text{min}},q_{\text{max}})=\max(q_{\text{min}},\min(x,q_{\text{max}}))$
    |  | (8) |'
- en: '|  | $\displaystyle\mathbf{\Delta}=(\Delta_{1},\ldots,\Delta_{n})^{\mathrm{T}},\quad\Delta_{i}=\frac{\alpha_{i}}{q_{\text{max}}}$
    |  | (9) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\Delta}=(\Delta_{1},\ldots,\Delta_{n})^{\mathrm{T}},\quad\Delta_{i}=\frac{\alpha_{i}}{q_{\text{max}}}$
    |  | (9) |'
- en: '|  | $\displaystyle\mathbf{W}^{\text{int}}_{i,:}=\text{clamp}\left(\left\lfloor\frac{\mathbf{W}_{i,:}}{\Delta_{i}}\right\rfloor;q_{\text{min}},q_{\text{max}}\right)$
    |  | (10) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}^{\text{int}}_{i,:}=\text{clamp}\left(\left\lfloor\frac{\mathbf{W}_{i,:}}{\Delta_{i}}\right\rfloor;q_{\text{min}},q_{\text{max}}\right)$
    |  | (10) |'
- en: '|  | $\displaystyle\mathbf{W}\approx Q(\mathbf{W})=\mathrm{diag}(\mathbf{\Delta})\mathbf{W}^{\text{int}}$
    |  | (11) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}\approx Q(\mathbf{W})=\mathrm{diag}(\mathbf{\Delta})\mathbf{W}^{\text{int}}$
    |  | (11) |'
- en: 'where $\Delta_{i}$ denotes the matrix of the quantized weights, $\mathrm{diag}(\mathbf{\Delta})$
    is found for each row by performing linear grid search over the interval $[0,\max(\mathbf{W}_{i,:})]$
    row . The search is conducted to minimize layer-wise mean squared error between
    weights:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta_{i}$ 表示量化权重的矩阵，$\mathrm{diag}(\mathbf{\Delta})$ 是通过在 $[0,\max(\mathbf{W}_{i,:})]$
    行上执行线性网格搜索找到的。搜索旨在最小化权重之间的层级均方误差：
- en: '|  | $\mathrm{argmin}_{\mathbf{\Delta}}\&#124;\mathbf{W}-Q(\mathbf{W})\&#124;_{2}^{2},$
    |  | (12) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{argmin}_{\mathbf{\Delta}}\&#124;\mathbf{W}-Q(\mathbf{W})\&#124;_{2}^{2},$
    |  | (12) |'
- en: $0.25$$1$$\Delta$
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: $0.25$$1$$\Delta$
- en: 'Figure 3: Uniform quantization step function with real valued one dimensional
    $w$.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：具有实值一维 $w$ 的均匀量化阶跃函数。
- en: Appendix B Details of LLMs quantization
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B LLMs量化的详细信息
- en: 'For only weight quantization of LLaMA and TÜLU2 models models, we apply QUIK
    implementation of mixed-precision GPTQ method Ashkboos et al. ([2023](#bib.bib1));
    Frantar et al. ([2022](#bib.bib13)). We isolate 128 salient columns in full-precision.
    Non-salient columns are subjected to uniform symmetric quantization, as discussed
    in Appendix [A](#A1 "Appendix A Uniform quantization ‣ GIFT-SW: Gaussian noise
    Injected Fine-Tuning of Salient Weights for LLMs"). The salient columns are identified
    through sensitive metrics described in Section [3.1](#S3.SS1 "3.1 Generalizing
    parameter sensitivity metrics ‣ 3 Method ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning
    of Salient Weights for LLMs"). The Hessian matrix for the GPTQ method is computed
    on 128 random samples of the Wikitext-2 dataset.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅LLaMA和TÜLU2模型的权重量化，我们应用了QUIK实现的混合精度GPTQ方法 Ashkboos等人（[2023](#bib.bib1)）；Frantar等人（[2022](#bib.bib13)）。我们在全精度下隔离了128个显著列。非显著列则进行均匀对称量化，如附录[A](#A1
    "附录 A 均匀量化 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")中所述。通过第[3.1](#S3.SS1 "3.1 参数敏感性度量的推广
    ‣ 3 方法 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")节描述的敏感度量来识别显著列。GPTQ方法的Hessian矩阵是在Wikitext-2数据集的128个随机样本上计算的。
- en: Appendix C Straight Through Estimator
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 直接通过估计器
- en: 'STE can be described in two steps:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: STE可以通过两个步骤来描述：
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Obtain quantized weights $Q(\mathbf{W})$, which is usually is non differentiable.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 获取量化权重 $Q(\mathbf{W})$，这通常是不可微分的。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compute gradients at quantized weights $Q(\mathbf{W})$
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算量化权重 $Q(\mathbf{W})$ 的梯度
- en: STE makes a particular choice of a quantization function to obtain the discrete
    weights from the real-valued weights. This approximation can be justified in some
    settings (Lin et al., [2017](#bib.bib24)) but in general the reasons behind its
    effectiveness are unknown.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: STE选择了特定的量化函数以从实值权重中获取离散权重。这种近似在某些设置下是可以被证明的（Lin等人，[2017](#bib.bib24)），但通常其有效性的原因仍不清楚。
- en: Appendix D Detailed Benchmark Results
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 详细基准结果
- en: 'In this section we report detailed benchmark results for LLaMA 2 & 3 after
    training with different methods. Tables [7](#A4.T7 "Table 7 ‣ Appendix D Detailed
    Benchmark Results ‣ GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights
    for LLMs"), [8](#A4.T8 "Table 8 ‣ Appendix D Detailed Benchmark Results ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs") present accuracy
    metrics which are achieved by the full-precision models after fine-tuning on the
    TÜLU-V2-mix and OpenOrca subsets. Corresponding mean values are listed in Table
     [1](#S3.T1 "Table 1 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs"). Tables present
    accuracy metrics which are achieved by quantized in 4, 3, 2 bits models after
    fine-tuning on the TÜLU-V2-mix subset. Corresponding mean values are listed in
    Table  [2](#S3.T2 "Table 2 ‣ 3.2 Quantization Noise Injection ‣ 3 Method ‣ GIFT-SW:
    Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs").'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们报告了LLaMA 2 & 3在不同训练方法下的详细基准结果。表[7](#A4.T7 "表7 ‣ 附录D 详细基准结果 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")、[8](#A4.T8
    "表8 ‣ 附录D 详细基准结果 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")展示了在TÜLU-V2-mix和OpenOrca子集上微调后的全精度模型所达到的准确性指标。相应的均值列在表[1](#S3.T1
    "表1 ‣ 3.2 量化噪声注入 ‣ 3 方法 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")中。表格展示了在TÜLU-V2-mix子集上微调后，4位、3位、2位量化模型所达到的准确性指标。相应的均值列在表[2](#S3.T2
    "表2 ‣ 3.2 量化噪声注入 ‣ 3 方法 ‣ GIFT-SW：对LLMs的显著权重进行高斯噪声注入微调")中。
- en: '| Model | Method | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA2-7b | FP | $78.65$ | $48.63$ |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7b | FP | $78.65$ | $48.63$ |'
- en: '| LoRA | $80.28$ | $47.95$ |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $80.28$ | $47.95$ |'
- en: '| DoRA | $81.93$ | $48.89$ |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $81.93$ | $48.89$ |'
- en: '| GIFT-SW | $\mathbf{82.63}$ | $\mathbf{49.91}$ |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{82.63}$ | $\mathbf{49.91}$ |'
- en: '| LLaMA2-13b | FP | $83.27$ | $53.67$ |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | FP | $83.27$ | $53.67$ |'
- en: '| LoRA | $81.10$ | $51.28$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $81.10$ | $51.28$ |'
- en: '| DoRA | $81.01$ | $51.54$ |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $81.01$ | $51.54$ |'
- en: '| GIFT-SW | $\mathbf{84.22}$ | $\mathbf{55.38}$ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{84.22}$ | $\mathbf{55.38}$ |'
- en: '| LLaMA3-8b | FP | $83.64$ | $55.72$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | FP | $83.64$ | $55.72$ |'
- en: '| LoRA | $83.30$ | $56.06$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $83.30$ | $56.06$ |'
- en: '| DoRA | $83.61$ | $55.63$ |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $83.61$ | $55.63$ |'
- en: '| GIFT-SW | $\mathbf{83.88}$ | $\mathbf{57.00}$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{83.88}$ | $\mathbf{57.00}$ |'
- en: 'Table 7: LLaMA models performance fine-tuned with TÜLU-V2-mix subset'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：用 TÜLU-V2-mix 子集微调的 LLaMA 模型性能
- en: '| Model | Method | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Model | Method | BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA2-7b | FT | $80.03$ | $48.72$ |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7b | FT | $80.03$ | $48.72$ |'
- en: '| LoRA | $78.81$ | $46.59$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $78.81$ | $46.59$ |'
- en: '| DoRA | $78.78$ | $46.93$ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $78.78$ | $46.93$ |'
- en: '| Our Best | $\mathbf{82.51}$ | $\mathbf{48.89}$ |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Our Best | $\mathbf{82.51}$ | $\mathbf{48.89}$ |'
- en: '| LLaMA2-13b | FT | $82.66$ | $54.78$ |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | FT | $82.66$ | $54.78$ |'
- en: '| LoRA | $81.68$ | $51.11$ |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $81.68$ | $51.11$ |'
- en: '| DoRA | $81.65$ | $51.19$ |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $81.65$ | $51.19$ |'
- en: '| Our Best | $\mathbf{85.44}$ | $\mathbf{56.48}$ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Our Best | $\mathbf{85.44}$ | $\mathbf{56.48}$ |'
- en: '| LLaMA3-8b | FT | $\mathbf{84.37}$ | $\mathbf{57.85}$ |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | FT | $\mathbf{84.37}$ | $\mathbf{57.85}$ |'
- en: '| LoRA | $82.84$ | $55.54$ |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | $82.84$ | $55.54$ |'
- en: '| DoRA | $82.63$ | $55.46$ |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| DoRA | $82.63$ | $55.46$ |'
- en: '| Our Best | $84.34$ | $57.76$ |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Our Best | $84.34$ | $57.76$ |'
- en: 'Table 8: LLaMA models performance fine-tuned with OpenOrca'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：用 OpenOrca 微调的 LLaMA 模型性能
- en: '| Bits | Model | Method | Benchmarks |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Bits | Model | Method | Benchmarks |'
- en: '| --- | --- | --- | --- |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | HellaSwag | WinoGrande | ARC-e | ARC-c | PiQA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 4 bit | LLaMA2-7b | STE | $80.21$ | $48.55$ |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 4 bit | LLaMA2-7b | STE | $80.21$ | $48.55$ |'
- en: '| QUIK + LORA | $68.96$ | $37.20$ |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $68.96$ | $37.20$ |'
- en: '| GIFT-SW | $\mathbf{82.78}$ | $\mathbf{50.00}$ |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{82.78}$ | $\mathbf{50.00}$ |'
- en: '| LLaMA2-13b | STE | $\mathbf{84.77}$ | $\mathbf{53.67}$ |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | STE | $\mathbf{84.77}$ | $\mathbf{53.67}$ |'
- en: '| QUIK + LORA | $74.89$ | $50.17$ |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $74.89$ | $50.17$ |'
- en: '| GIFT-SW | $84.65$ | $53.50$ |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $84.65$ | $53.50$ |'
- en: '| LLaMA3-8b | STE | $81.59$ | $54.27$ |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | STE | $81.59$ | $54.27$ |'
- en: '| QUIK + LORA | $82.51$ | $51.62$ |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $82.51$ | $51.62$ |'
- en: '| GIFT-SW | $\mathbf{83.15}$ | $\mathbf{55.20}$ |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{83.15}$ | $\mathbf{55.20}$ |'
- en: '| 3 bit | LLaMA2-7b | STE | $76.79$ | $45.65$ |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 3 bit | LLaMA2-7b | STE | $76.79$ | $45.65$ |'
- en: '| QUIK + LORA | $63.88$ | $38.74$ |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $63.88$ | $38.74$ |'
- en: '| GIFT-SW | $\mathbf{80.46}$ | $\mathbf{47.35}$ |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{80.46}$ | $\mathbf{47.35}$ |'
- en: '| LLaMA2-13b | STE | $83.33$ | $\mathbf{53.24}$ |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | STE | $83.33$ | $\mathbf{53.24}$ |'
- en: '| QUIK + LORA | $82.02$ | $48.21$ |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $82.02$ | $48.21$ |'
- en: '| GIFT-SW | $\mathbf{85.44}$ | $51.54$ |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{85.44}$ | $51.54$ |'
- en: '| LLaMA3-8b | STE | $75.87$ | $49.32$ |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | STE | $75.87$ | $49.32$ |'
- en: '| QUIK + LORA | $78.72$ | $50.60$ |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $78.72$ | $50.60$ |'
- en: '| GIFT-SW | $\mathbf{80.31}$ | $\mathbf{52.99}$ |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{80.31}$ | $\mathbf{52.99}$ |'
- en: '| 2 bit | LLaMA2-7b | STE | $68.47$ | $32.17$ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 2 bit | LLaMA2-7b | STE | $68.47$ | $32.17$ |'
- en: '| QUIK + LORA | $62.11$ | $26.45$ |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $62.11$ | $26.45$ |'
- en: '| GIFT-SW | $\mathbf{71.90}$ | $\mathbf{34.90}$ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{71.90}$ | $\mathbf{34.90}$ |'
- en: '| LLaMA2-13b | STE | $73.09$ | $36.09$ |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | STE | $73.09$ | $36.09$ |'
- en: '| QUIK + LORA | $59.36$ | $27.82$ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $59.36$ | $27.82$ |'
- en: '| GIFT-SW | $\mathbf{81.99}$ | $\mathbf{43.17}$ |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{81.99}$ | $\mathbf{43.17}$ |'
- en: '| LLaMA3-8b | STE | $60.46$ | $27.65$ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8b | STE | $60.46$ | $27.65$ |'
- en: '| QUIK + LORA | $64.68$ | $32.17$ |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| QUIK + LORA | $64.68$ | $32.17$ |'
- en: '| GIFT-SW | $\mathbf{74.13}$ | $\mathbf{37.88}$ |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| GIFT-SW | $\mathbf{74.13}$ | $\mathbf{37.88}$ |'
- en: 'Table 9: Performance of quantized LLaMA models fine-tuned with TÜLU-V2-mix
    subset'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：用 TÜLU-V2-mix 子集微调的量化 LLaMA 模型性能
- en: Appendix E TÜLU-V2-mix subset
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E TÜLU-V2-mix 子集
- en: 'Figure [4](#A5.F4 "Figure 4 ‣ Appendix E TÜLU-V2-mix subset ‣ GIFT-SW: Gaussian
    noise Injected Fine-Tuning of Salient Weights for LLMs") shows number of examples
    in datasets included in the TÜLU-V2-mix subset, which is used for fine-tuning
    experiments presented in this paper.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#A5.F4 "图 4 ‣ 附录 E TÜLU-V2-mix 子集 ‣ GIFT-SW: 为 LLMs 注入高斯噪声的显著权重微调") 显示了
    TÜLU-V2-mix 子集中包含的数据集示例数量，这些数据用于本文中展示的微调实验。'
- en: '![Refer to caption](img/7ab859181fb8781d74d000a1d7be6c04.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7ab859181fb8781d74d000a1d7be6c04.png)'
- en: 'Figure 4: Number of examples in datasets included in TÜLU-V2-mix subset'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：包含在 TÜLU-V2-mix 子集中的数据集示例数量
