- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:36:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:36:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在低资源环境中的不同高效微调方法的比较分析
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13181](https://ar5iv.labs.arxiv.org/html/2405.13181)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13181](https://ar5iv.labs.arxiv.org/html/2405.13181)
- en: Krishna Prasad Varadarajan Srinivasan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 克里希纳·普拉萨德·瓦拉达拉贾·斯里尼瓦桑
- en: Georgia Institute of Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚理工学院
- en: kvarada@gatech.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: kvarada@gatech.edu
- en: '& Prasanth Gumpena'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '& 普拉尚特·贡佩纳'
- en: Georgia Institute of Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚理工学院
- en: pgumpena3@gatech.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: pgumpena3@gatech.edu
- en: '& Madhusudhana Yattapu'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '& 马杜苏达纳·亚塔普'
- en: Georgia Institute of Technology
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚理工学院
- en: myattapu3@gatech.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: myattapu3@gatech.edu
- en: '& Vishal H. Brahmbhatt'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '& 维沙尔·H·布拉姆巴特'
- en: Georgia Institute of Technology
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚理工学院
- en: vbrahmbhatt3@gatech.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: vbrahmbhatt3@gatech.edu
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In the domain of large language models (LLMs), Mosbach et al. ([2023](#bib.bib1))
    showed that few-shot full-model fine-tuning – namely Vanilla Fine Tuning (FT)
    and Pattern-Based Fine Tuning (PBFT) –, and In-Context Learning (ICL) generalize
    similarly on Out-Of-Domain (OOD) datasets, but vary in terms of task adaptation.
    However, they both pose challenges, especially in term of memory requirements.
    In this paper, we further try to push the understanding of different fine-tuning
    strategies for LLM and aim to bring a myriad of these on the same pedestal for
    an elaborate comparison with full-model fine-tuning on two diverse datasets. To
    that end, we conducted a series of experiments, beginning with state-of-the-art
    methods like vanilla fine-tuning and Pattern-Based Fine-Tuning (PBFT) on pre-trained
    models across two datasets, COLA and MNLI. We then investigate adaptive fine-tuning
    and the efficiency of LoRA adapters in a few-shot setting. Finally, we also compare
    an alternative approach that has gained recent popularity – context distillation
    – with the vanilla FT and PBFT with and without few-shot setup.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）领域，Mosbach等人 ([2023](#bib.bib1)) 显示，少样本全模型微调——即普通微调（FT）和基于模式的微调（PBFT），以及上下文学习（ICL）——在领域外（OOD）数据集上的泛化类似，但在任务适应方面有所不同。然而，它们都存在挑战，特别是在内存需求方面。本文进一步尝试深入了解不同的微调策略，并旨在将这些策略放在同一平台上，以便对比全模型微调在两个不同数据集上的表现。为此，我们进行了一系列实验，首先在两个数据集COLA和MNLI上的预训练模型上测试了最先进的方法，如普通微调和基于模式的微调（PBFT）。然后，我们调查了自适应微调和LoRA适配器在少样本设置中的效率。最后，我们还将一种最近受到关注的替代方法——上下文蒸馏——与普通FT和PBFT在有无少样本设置下进行比较。
- en: Our findings suggest that these alternative strategies that we explored can
    exhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.
    PBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the need
    for effective prompts. Further, our adaptive-fine tuning and LoRA experiments
    perform comparable or slightly worse than the standard fine-tunings as anticipated,
    since standard fine-tunings involve tuning the entire model. Finally, our context
    distillation experiments out-perform the standard fine-tuning methods. These findings
    underscore that eventually the choice of an appropriate fine-tuning method depends
    on the available resources (memory, compute, data) and task adaptability.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，我们探索的这些替代策略可以表现出与普通FT和PBFT相当的领域外泛化能力。PBFT在领域外（OOD）数据上表现不如普通FT，强调了有效提示的必要性。此外，我们的自适应微调和LoRA实验表现与标准微调相当或略差，如预期的那样，因为标准微调涉及调优整个模型。最后，我们的上下文蒸馏实验优于标准微调方法。这些发现突显了最终选择适当的微调方法取决于可用资源（内存、计算、数据）和任务适应性。
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The goal of our work is to evaluate and compare the performance of a pre-trained
    large language model on sequence classification tasks. We aimed to do this by
    employing – 1) different fine tuning (FT) methods, 2) applying Low-Rank Adaptation
    - LoRA (Hu et al. ([2021](#bib.bib2))) adaptors with few-shot learning, and 3)
    performing context-distillation both with and without few-shot learning setting.
    We aim to understand the efficacy of alternative fine-tuning methods on a pre-trained
    large language model’s performance in sequence classification tasks using 2 datasets,
    namely [MNLI](https://paperswithcode.com/dataset/multinli) (Williams et al. ([2018](#bib.bib3)))
    and [COLA](https://nyu-mll.github.io/CoLA/) (Warstadt et al. ([2018](#bib.bib4))),
    which are further explained in Section [1.1](#S1.SS1 "1.1 Datasets ‣ 1 Introduction
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting"). We explored alternate ways of efficiently
    fine-tuning the model and compared them with the baseline methods (vanilla and
    pattern-based fine-tuning) for Open Pre-trained Transformer (OPT) (Zhang et al.
    ([2022](#bib.bib5))) model’s performance on the text sequence classification task
    using both in-domain and out of domain accuracies. We try to keep the training
    process, experiments, and hyper-parameters similar across various experiments,
    wherever possible, for a fair comparison.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的目标是评估和比较预训练大型语言模型在序列分类任务中的性能。我们计划通过以下方式实现这一目标：1）采用不同的微调（FT）方法，2）使用低秩适配（Low-Rank
    Adaptation - LoRA）（Hu 等人（[2021](#bib.bib2)））适配器和少样本学习，3）在有无少样本学习设置下进行上下文蒸馏。我们旨在了解替代微调方法在预训练大型语言模型的序列分类任务中的有效性，使用
    2 个数据集，即 [MNLI](https://paperswithcode.com/dataset/multinli)（Williams 等人（[2018](#bib.bib3)））和
    [COLA](https://nyu-mll.github.io/CoLA/)（Warstadt 等人（[2018](#bib.bib4)）），这些将在第
    [1.1](#S1.SS1 "1.1 数据集 ‣ 1 引言 ‣ 大型语言模型（LLMs）在低资源设置下的不同高效微调方法的比较分析") 节中进一步解释。我们探索了有效微调模型的替代方法，并将其与基线方法（原始和基于模式的微调）进行了比较，以评估
    Open 预训练 Transformer（OPT）（Zhang 等人（[2022](#bib.bib5)））模型在文本序列分类任务中的性能，使用了领域内和领域外的准确率。我们尽量保持训练过程、实验和超参数在各种实验中相似，以确保公平比较。
- en: Currently large language models (LLMs) are pre-dominantly used by leveraging
    In-Context Learning - ICL (Brown et al. ([2020](#bib.bib6))), whereby during the
    inference time, the model learns to answer follow-up questions from a series of
    prompts. This approach requires significant inference time memory and compute.
    In recent times, bunch of alternate methods have been proposed and explored to
    augment the issues faced with ICL. We explore and analyze a number of these methods
    in our work presented here.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大型语言模型（LLMs）主要通过利用上下文学习（In-Context Learning - ICL）（Brown 等人（[2020](#bib.bib6)））来使用，在推理过程中，模型通过一系列提示学习回答后续问题。这种方法需要大量的推理时间、内存和计算能力。近年来，提出并探索了许多替代方法来解决
    ICL 面临的问题。我们在这里展示的工作中探索和分析了这些方法中的一些。
- en: Our work is relevant to anyone leveraging large language models for tasks like
    chat bots and code completion. If successful, our methods could improve the efficiency
    and performance of these models, particularly in tasks requiring the ingestion
    of large sequences of dialogue, code, or text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作与任何利用大型语言模型进行聊天机器人和代码补全等任务的人都相关。如果成功，我们的方法可能会提高这些模型的效率和性能，特别是在需要处理大量对话、代码或文本序列的任务中。
- en: We expand on all of these methods in section [2](#S2 "2 Approach ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting") of this paper. We also further expand on the 2 datasets
    that we have performed our experiments on, in the section [1.1](#S1.SS1 "1.1 Datasets
    ‣ 1 Introduction ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文的第 [2](#S2 "2 方法 ‣ 大型语言模型（LLMs）在低资源设置下的不同高效微调方法的比较分析") 节中扩展了所有这些方法。我们还在第
    [1.1](#S1.SS1 "1.1 数据集 ‣ 1 引言 ‣ 大型语言模型（LLMs）在低资源设置下的不同高效微调方法的比较分析") 节中进一步扩展了我们进行实验的
    2 个数据集。
- en: 1.1 Datasets
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 数据集
- en: 1.1.1 MNLI
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.1 MNLI
- en: Multi-Genre Natural Language Inference - MNLI (Williams et al. ([2018](#bib.bib3)))
    is a crowd-sourced collection of sentence pairs with textual entailment annotations.
    This is one of the largest corpora available for natural language inference (NLI)
    with ten distinct genres of written and spoken English. This paper used a subset
    of 261,802 samples accessed via GLUE (Wang et al. ([2019](#bib.bib7))) for training
    and entire dataset for in-domain-performance. Note that the labels were binarized
    to only include entailment and contradiction. The most important aspects of the
    dataset are suitability for NLI task and variety of samples belonging to different
    genres. An example of premise, hypothesis, and label are -
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多领域自然语言推理 - MNLI（Williams 等人 ([2018](#bib.bib3))) 是一个众包收集的句子对数据集，带有文本蕴涵标注。这是可用于自然语言推理（NLI）的最大数据集之一，包含了十种不同类型的书面和口头英语。这篇论文使用了通过
    GLUE（Wang 等人 ([2019](#bib.bib7))) 访问的 261,802 个样本子集用于训练，并使用了整个数据集进行领域内性能评估。请注意，标签被二值化，仅包括蕴涵和矛盾。数据集最重要的方面是适合
    NLI 任务和样本的多样性。以下是前提、假设和标签的一个示例 -
- en: 'Premise: How do you know? All this is their information again.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前提：你怎么知道？这些信息又是他们的。
- en: 'Hypothesis: This information belongs to them.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设：这些信息属于他们。
- en: 'Label: 0 (entailment)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：0（蕴涵）
- en: HANS dataset (McCoy et al. ([2019](#bib.bib8))) with 30,000 samples, was used
    for out-of-domain performance. It was chosen to evaluate performance when training
    on the MNLI dataset using different methods. This dataset includes examples that
    challenge conventional MNLI patterns by failing three syntactic heuristics - lexical
    overlap, subsequence, and constituent. These examples test how well models handle
    misleading or incorrect heuristic cues.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: HANS 数据集（McCoy 等人 ([2019](#bib.bib8)))，包含 30,000 个样本，用于领域外性能评估。它被选择用于评估在 MNLI
    数据集上使用不同方法训练时的表现。该数据集包含了挑战传统 MNLI 模式的示例，通过违反三种句法启发式规则 - 词汇重叠、子序列和组成。 这些示例测试了模型处理误导性或不正确启发式提示的能力。
- en: 'Premise: The senators supported the actor.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前提：参议员支持了演员。
- en: 'Hypothesis: The actor supported the senators.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设：演员支持了参议员。
- en: 'Label: 1 (positive)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：1（正面）
- en: 1.1.2 COLA
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.1.2 COLA
- en: The Corpus of Linguistic Acceptability (COLA) (Warstadt et al. ([2018](#bib.bib4))),
    developed by researchers at New York University (NYU) and Meta’s AI research lab,
    encompasses 10,657 English sentences from 23 linguistic publications. It provides
    annotations on sentence grammaticality, with 9,594 sentences for training and
    1,063 for testing. These annotations are provided by the authors of the sources.
    COLA aims to uniquely explore neural networks’ capacity to understand grammatical
    nuances.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 语言接受度语料库（COLA）（Warstadt 等人 ([2018](#bib.bib4)))，由纽约大学 (NYU) 和 Meta 的 AI 研究实验室的研究人员开发，包含了来自
    23 个语言学出版物的 10,657 个英语句子。它提供了句子语法性的标注，其中 9,594 个句子用于训练，1,063 个句子用于测试。这些标注由来源的作者提供。COLA
    旨在独特地探索神经网络理解语法细微差别的能力。
- en: 'The dataset uses labels for grammatical acceptability: 0 - unacceptable, 1
    - acceptable. For instance:  Sentence: This paper was written in Overleaf.  Label:
    1 (acceptable)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集使用语法接受度标签：0 - 不可接受，1 - 可接受。例如：句子：这篇论文是在 Overleaf 上写的。标签：1（可接受）
- en: Among the 23 sources, 17 are deemed in-domain, while the remaining 6 are out-of-domain
    (OOD). Access to the in-domain data was via the GLUE framework (Wang et al. ([2019](#bib.bib7))),
    and the OOD data from (Inc. ([2023](#bib.bib9))) GitHub repository, allowing for
    consistent analysis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 23 个来源中，17 个被认为是领域内的，其余 6 个为领域外 (OOD)。对领域内数据的访问是通过 GLUE 框架（Wang 等人 ([2019](#bib.bib7)))，领域外数据则来自
    (Inc. ([2023](#bib.bib9))) GitHub 存储库，以便进行一致的分析。
- en: 2 Approach
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: We aimed to enhance sequence classification performance of large language models.
    Using fine-tuning methods like vanilla and Pattern-Based Fine Tuning (PBFT), we
    established baselines on Facebook’s OPT 125M and OPT 350M models in a few-shot
    setting. Limited by time and resources, we focused solely on these models. Our
    baseline experiments, which replicated (Mosbach et al. ([2023](#bib.bib1))), results
    for Vanilla and PBFT methods, used both datasets mentioned in Section [1.1](#S1.SS1
    "1.1 Datasets ‣ 1 Introduction ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在提升大型语言模型的序列分类性能。使用如vanilla和基于模式的微调（PBFT）等微调方法，我们在Facebook的OPT 125M和OPT 350M模型上建立了基准，在少量样本设置中进行。由于时间和资源的限制，我们仅专注于这些模型。我们的基准实验复制了（Mosbach等人（[2023](#bib.bib1)））的Vanilla和PBFT方法的结果，使用了第[1.1](#S1.SS1
    "1.1 Datasets ‣ 1 Introduction ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting")节中提到的两个数据集。
- en: We broadened our experiments to include Context Distillation, as per Anthropic’s
    discussion in Askell et al. ([2021](#bib.bib10)). We experimented this fine-tuning
    in both few-shot and standard scenarios, with performance being comparable. We
    also tested Parameter-Efficient Fine-Tuning (PEFT) enhanced with Low-Rank Adaptation
    - LoRA (Hu et al. ([2021](#bib.bib2))) and Adaptive Fine-Tuning. Our findings
    indicate that while all methods perform similarly, they can be unstable and under-perform
    due to training instability. Performance generally improves with model size, and
    context distillation often excels in generalization. A comprehensive analysis
    is provided in Section [6](#S6 "6 Experiments and Results ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting").
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩大了实验范围，包含了Context Distillation，参考了Anthropic在Askell等人（[2021](#bib.bib10)）中的讨论。我们在少量样本和标准场景中进行了这种微调，性能相当。我们还测试了通过低秩适应
    - LoRA（Hu等人（[2021](#bib.bib2)））和自适应微调增强的参数高效微调（PEFT）。我们的发现表明，虽然所有方法的表现类似，但由于训练不稳定，它们可能不稳定并表现欠佳。性能通常随着模型规模的增加而提高，而上下文蒸馏在泛化方面往往表现突出。全面的分析见第[6](#S6
    "6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting")节。
- en: Our approach, a unique mix of traditional and new methods, shows potential.
    While similar methods may have been individually explored, we focus on their benefits
    when applied separately and the synergy when few-shot learning is combined with
    LoRA adapter or context distillation. We aim to thoroughly analyze their performance
    in sequence classification tasks, hoping our exploration offers valuable insights
    to the field.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是一种传统方法和新方法的独特结合，显示出了潜力。虽然类似的方法可能已被单独探索，但我们关注的是它们分别应用时的好处以及当少量样本学习与LoRA适配器或上下文蒸馏结合时的协同效应。我们旨在彻底分析它们在序列分类任务中的表现，希望我们的探索为该领域提供有价值的见解。
- en: We foresaw challenges with computational resources and time, leading us to focus
    on the OPT 125M and OPT 350M models. We also expected performance differences
    between in-domain and out-of-domain accuracies. A main challenge was ensuring
    fair comparison across methods, especially as context distillation had significantly
    more training examples on the full set than in a few-shot setting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预见到计算资源和时间的挑战，因此我们专注于OPT 125M和OPT 350M模型。我们还预计了领域内和领域外准确率之间的性能差异。一个主要挑战是确保方法之间的公平比较，尤其是上下文蒸馏在完整数据集上的训练样本显著多于少量样本设置。
- en: 2.1 Few-Shot Learning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 少量样本学习
- en: For our experiments, we used few-shot learning to design models that learn from
    a small number of examples, using knowledge from related tasks. We conducted experiments
    with varying numbers of examples (N = 2, 16, 32, 64, and 128), fine-tuning and
    evaluating models in each setting. This process, repeated across all settings,
    allowed us to observe the impact of example quantity on model performance. To
    ensure robust results, we performed 10 runs per model size per N, accounting for
    data seed variability.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用少量样本学习来设计从少量样本中学习的模型，利用相关任务的知识。我们进行了具有不同样本数量（N = 2, 16, 32, 64和128）的实验，在每种设置中对模型进行了微调和评估。这个过程在所有设置中重复进行，使我们能够观察样本数量对模型性能的影响。为了确保结果的可靠性，我们对每个模型规模的每个N进行了10次运行，以考虑数据种子变异性。
- en: 2.2 Infrastructure and Replicating the Codebase
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基础设施与代码库复制
- en: The experiments were conducted using Google Colab Pro, equipped with Nvidia
    Tesla T4 and L4 GPUs. The entire codebase for our work is hosted on GitHub and
    can be accessed at [https://github.com/iamvarada/llm_finetuning](https://github.com/iamvarada/llm_finetuning)
    ¹¹1Please contact the authors for any access issues. Researchers interested in
    replicating or extending our work are encouraged to clone or fork this repository.
    To ensure consistency and ease of setup, all necessary Python dependencies are
    listed in the requirements.txt file within the repository.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实验使用了配备Nvidia Tesla T4和L4 GPU的Google Colab Pro进行。我们工作的整个代码库托管在GitHub上，可以通过[https://github.com/iamvarada/llm_finetuning](https://github.com/iamvarada/llm_finetuning)访问¹¹1如有访问问题，请联系作者。希望复制或扩展我们工作的研究人员可以克隆或分叉该仓库。为确保一致性和便捷设置，所有必要的Python依赖项都列在仓库中的requirements.txt文件中。
- en: 3 Fine-Tuning Methods
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 微调方法
- en: 3.1 Vanilla Fine-Tuning
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 普通微调
- en: Vanilla fine-tuning was performed on two models, OPT 125M and OPT 350M, instantiated
    as SequenceClassification with dual classification heads for binary labels. The
    models were directly fed with the ‘premise’ and ‘hypothesis’, and respective labels
    for MNLI and COLA datasets, without any prompts. The training loop, focused on
    few-shot learning as explained in Section [2.1](#S2.SS1 "2.1 Few-Shot Learning
    ‣ 2 Approach ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting"),
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 普通微调在两个模型OPT 125M和OPT 350M上进行，实例化为SequenceClassification，并具有用于二元标签的双分类头。模型直接输入‘前提’和‘假设’，以及MNLI和COLA数据集的相应标签，无需任何提示。训练循环专注于少量学习，如第[2.1](#S2.SS1
    "2.1 少量学习 ‣ 2 方法 ‣ 大型语言模型（LLMs）在低资源环境中的不同高效微调方法的比较分析")节所述。
- en: 3.2 Pattern-Based Fine-Tuning
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于模式的微调
- en: Pattern-Based Fine-Tuning (PBFT) is an advanced approach that uses language
    patterns to guide fine-tuning. Unlike vanilla fine-tuning, PBFT uses specific
    prompts or patterns to guide model predictions. We use GPT-3 derived patterns
    to transform the ‘premise’ and ‘hypothesis’ into a more learnable format. The
    model is then fine-tuned on these transformed inputs. PBFT, particularly with
    GPT-3 patterns, leverages inductive biases in prompts for effective learning and
    better task performance, especially useful in few-shot learning scenarios with
    limited data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模式的微调（PBFT）是一种先进的方法，利用语言模式来指导微调。与普通微调不同，PBFT使用特定的提示或模式来引导模型预测。我们使用源自GPT-3的模式将‘前提’和‘假设’转换成更易学习的格式。然后，模型在这些转换后的输入上进行微调。PBFT，特别是结合GPT-3模式，利用提示中的归纳偏差进行有效学习和更好的任务表现，这在有限数据的少量学习场景中尤为有用。
- en: 3.3 Adaptive Fine-Tuning
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 自适应微调
- en: 'Adaptive fine-tuning enables nuanced training using two techniques: Freezing
    Layers and Dynamic Learning Rate. The initial layers of the OPT 125M model, which
    hold general-purpose knowledge, are frozen to avoid overwriting during fine-tuning.
    A dynamic learning rate scheduler is used, applying a higher rate to the final
    task-specific layers and a lower rate to the frozen layers, aiding model adaptation
    while retaining pre-trained knowledge.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应微调通过两种技术实现细致训练：冻结层和动态学习率。OPT 125M模型的初始层保存了通用知识，这些层在微调期间被冻结以避免被覆盖。使用动态学习率调度器，对最终任务特定层应用较高的学习率，对冻结层应用较低的学习率，帮助模型适应同时保持预训练知识。
- en: 4 Parameter-Efficient Modeling using Low-Rank Adaptation (LoRA)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用低秩适应（LoRA）的参数高效建模
- en: 'We further explored Parameter Efficient Fine-Tuning (Houlsby et al. ([2019](#bib.bib11)))
    for fine-tuning our pre-trained OPT 125M model on COLA dataset by employing the
    Low-Rank Adaptation (LoRA) adapter (Hu et al. ([2021](#bib.bib2))). In this method
    of fine-tuning a model, we break down the weight matrix that is learned during
    the gradient descent step of backpropagation into two, smaller-rank matrices,
    thereby significantly reducing the number of parameters that we optimize for during
    our training process. This is depicted in Figure [1](#S4.F1 "Figure 1 ‣ 4 Parameter-Efficient
    Modeling using Low-Rank Adaptation (LoRA) ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting"). If say, in a regular training process, we learn a matrix $\Delta W$
    x $m$, such that:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步探索了参数高效微调（Houlsby et al. ([2019](#bib.bib11)))，通过采用低秩适配器（LoRA）来微调我们预训练的OPT
    125M模型在COLA数据集上的表现（Hu et al. ([2021](#bib.bib2)))。在这种模型微调方法中，我们将权重矩阵在梯度下降反向传播步骤中分解为两个较小秩的矩阵，从而显著减少了在训练过程中需要优化的参数数量。这在图[1](#S4.F1
    "Figure 1 ‣ 4 Parameter-Efficient Modeling using Low-Rank Adaptation (LoRA) ‣
    Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting")中有所体现。如果说，在常规训练过程中，我们学习一个矩阵$\Delta W$ x
    $m$，使得：
- en: '|  | $W\xleftarrow{}W+\Delta W$ |  | (1) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $W\xleftarrow{}W+\Delta W$ |  | (1) |'
- en: ', with LoRA, we decompose the $\Delta W$ and $B$ x $r$ x $m$, such that:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ，在LoRA中，我们将$\Delta W$和$B$ x $r$ x $m$进行分解，使得：
- en: '|  | $\begin{split}\Delta W=A.B,\\ \ni r\ll m\end{split}$ |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\Delta W=A.B,\\ \ni r\ll m\end{split}$ |  | (2) |'
- en: ', where $r$, rank of the matrices, is a hyper-parameter of the model.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中$r$，矩阵的秩，是模型的超参数。
- en: In the domain of LLMs, decomposing non-trivial weight matrices in each layer
    with r = 2, 4, 6, …, significantly reduces the parameters to optimize, depending
    on the chosen rank. This decomposition is an approximation that aims for a balance
    between model accuracy and resource utilization during fine-tuning. In LoRA, the
    adapter layer output is multiplied by a factor $\alpha$ (another hyper-paramter),
    which determines the impact of the LoRA decomposition on the model layer to which
    the adapter is applied.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs领域中，通过对每一层的非平凡权重矩阵进行分解，选择r = 2, 4, 6, …，可以显著减少需要优化的参数数量，具体取决于所选择的秩。这种分解是一种近似方法，旨在模型精度和资源利用之间取得平衡。在LoRA中，适配器层的输出会乘以一个因子$\alpha$（另一个超参数），它决定了LoRA分解对应用了适配器的模型层的影响。
- en: '![Refer to caption](img/74595c7f4336990707b8ef6062bb111c.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/74595c7f4336990707b8ef6062bb111c.png)'
- en: 'Figure 1: Weight update step (right) with and (left) without with LoRA adapter,
    Figure courtesy: Raschka ([2024](#bib.bib12))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：权重更新步骤（右）使用和（左）不使用LoRA适配器，图源：Raschka ([2024](#bib.bib12))
- en: 5 Context Distillation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 上下文蒸馏
- en: 'In our approach, training uses two primary loss functions: distillation and
    classification loss. Distillation loss, computed using Kullback-Leibler divergence
    (Csiszar ([1975](#bib.bib13))), is a measure that quantifies the difference between
    any two probability distributions. More specifically, in our case, the KL divergence
    is calculated between the log-softmax of the student model’s outputs and the probability
    distribution of the teacher model, with the results averaged across the batch.
    Classification loss, determined using cross-entropy loss function, assesses the
    discrepancy between the student model’s predictions and the true labels.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法中，训练使用两个主要的损失函数：蒸馏损失和分类损失。蒸馏损失，使用Kullback-Leibler散度（Csiszar ([1975](#bib.bib13)))计算，是一个量化两个概率分布之间差异的度量。更具体地说，在我们的案例中，KL散度是在学生模型的输出的对数softmax和教师模型的概率分布之间计算的，结果在整个批次中平均。分类损失，使用交叉熵损失函数确定，评估学生模型预测与真实标签之间的差异。
- en: 'We integrate these losses into training by formulating the overall loss as
    a weighted sum of both, with each loss component assigned a task-dependent weight.
    This balance is represented by the equation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将整体损失公式化为两个损失的加权和，将这些损失整合到训练中，每个损失组件分配一个任务相关的权重。这种平衡由以下方程表示：
- en: '|  | $\text{Loss}=0.5\times\text{Distillation Loss}+0.5\times\text{Classification
    Loss}$ |  | (3) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{损失}=0.5\times\text{蒸馏损失}+0.5\times\text{分类损失}$ |  | (3) |'
- en: This approach allows the student model to learn from hard labels and mimic the
    teacher model’s probabilistic output, fostering nuanced understanding and potential
    improved generalization on unseen data (Askell et al. ([2021](#bib.bib10))).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许学生模型从硬标签中学习，并模拟教师模型的概率输出，从而促进对未见数据的细致理解和潜在的改进泛化（Askell 等人 ([2021](#bib.bib10))）。
- en: We also assessed context distillation with few-shot learning setup from section
    [2.1](#S2.SS1 "2.1 Few-Shot Learning ‣ 2 Approach ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting"), where the student model was fine-tuned using limited data. Subsets
    of representative samples were used to fine-tune the student model, and its performance
    was evaluated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在[2.1节](#S2.SS1 "2.1 Few-Shot Learning ‣ 2 Approach ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting")的少量样本学习设置中评估了上下文蒸馏，其中学生模型使用有限的数据进行了微调。通过使用具有代表性的样本子集对学生模型进行微调，并对其性能进行了评估。
- en: 6 Experiments and Results
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验与结果
- en: 'This section examines the effectiveness of fine-tuning techniques on two OPT
    models: OPT 125M and OPT 350M. We cover Vanilla FT, PBFT, PEFT with LoRA adapters,
    Adaptive FT, and Context Distillation (with and without few-shot learning). These
    experiments helped us understand the model’s adaptability to sequence classification
    and methods to improve its performance.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本节检查了微调技术在两个 OPT 模型上的有效性：OPT 125M 和 OPT 350M。我们涵盖了原始微调、PBFT、带 LoRA 适配器的 PEFT、自适应微调和上下文蒸馏（有和没有少量样本学习）。这些实验帮助我们理解模型对序列分类的适应能力及其性能改进的方法。
- en: 6.1 Hyper-Parameters and Experiment Set-Up
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 超参数和实验设置
- en: 'We initiated this empirical study by evaluating the impact of various hyper-parameters
    in our experiments. To compare the myriad techniques we developed against the
    benchmark fairly, we aligned our fine-tuning parameters with those in Mosbach
    et al. ([2023](#bib.bib1)). We used few-shot sample sizes of 2, 16, 32, 64, and
    128 examples per class to reflect different levels of information availability.
    Our training spanned 40 epochs with a batch size of 32 to balance data exposure
    and computational efficiency. We conducted 10 trials per run to remove bias, selecting
    samples randomly from the training set. The learning rate was set at $1e^{-5}$
    and employed the AdamW optimizer (Loshchilov and Hutter ([2019](#bib.bib14))),
    a typical choice for LLM optimization. A consolidated table of the hyper-parameters
    used in our experiments can be found in Appendix [A](#A1 "Appendix A Appendix:
    Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过评估实验中各种超参数的影响来启动这项实证研究。为了公平比较我们开发的各种技术与基准，我们将微调参数与 Mosbach 等人 ([2023](#bib.bib1))
    的参数对齐。我们使用了每类 2、16、32、64 和 128 个样本的少量样本大小，以反映不同的信息可用性水平。我们的训练历时 40 个 epoch，批量大小为
    32，以平衡数据暴露和计算效率。每次运行进行了 10 次试验，以消除偏差，随机从训练集中选择样本。学习率设定为 $1e^{-5}$，并采用了 AdamW 优化器（Loshchilov
    和 Hutter ([2019](#bib.bib14))），这是 LLM 优化中的一种典型选择。我们实验中使用的超参数的汇总表见附录[A](#A1 "Appendix
    A Appendix: Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting")。'
- en: 6.2 Vanilla FT, PBFT and Adaptive FT
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 原始微调、PBFT 和自适应微调
- en: 6.2.1 Vanilla FT
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 原始微调
- en: In our first experiment, we assessed the impact of vanilla fine-tuning on the
    CoLA and MNLI datasets using OPT 125M and OPT 350M models. The left plot in Figure
    [2](#S6.F2 "Figure 2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT
    ‣ 6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting") shows
    that for the CoLA dataset, both models exhibited increased accuracy with larger
    sample sizes, with the OPT 125M model peaking at N=128\. OPT 350M model having
    nearly 3 times the parameters of the OPT 125M showed less pronounced effect of
    data size on accuracy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一次实验中，我们评估了原始微调在 CoLA 和 MNLI 数据集上的影响，使用了 OPT 125M 和 OPT 350M 模型。图[2](#S6.F2
    "Figure 2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments
    and Results ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting")左侧的图表显示，对于 CoLA 数据集，两个模型在样本量增加时精度都得到了提升，其中
    OPT 125M 模型在 N=128 时达到峰值。OPT 350M 模型的参数量接近 OPT 125M 的三倍，数据量对准确性的影响较小。
- en: '![Refer to caption](img/b9c89fa4f407e08b106d35c7bb7e5d93.png)![Refer to caption](img/7ff127b0601457140c4185c9ad405677.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b9c89fa4f407e08b106d35c7bb7e5d93.png)![参考说明](img/7ff127b0601457140c4185c9ad405677.png)'
- en: 'Figure 2: Vanilla FT accuracy for OPT 125M and OPT 350M on (left) COLA and
    (right) MNLI for different samples'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：OPT 125M和OPT 350M在（左）COLA和（右）MNLI上对于不同样本的Vanilla FT准确率
- en: The right plot in Figure [2](#S6.F2 "Figure 2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla
    FT, PBFT and Adaptive FT ‣ 6 Experiments and Results ‣ Comparative Analysis of
    Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting") presents a similar case for the MNLI data set. The only caveat was that
    models’ out-of-domain accuracy remained relatively stagnant across varying sample
    sizes. The larger OPT 350M model showed moderate enhancements in in-domain accuracy
    as the sample size grew, with minimal fluctuations in out-of-domain accuracy.
    This was expected because as detailed in Section [1.1](#S1.SS1 "1.1 Datasets ‣
    1 Introduction ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting"), MNLI dataset uses the
    HANS dataset as OOD data while the COLA dataset uses a subset of its own sources.
    The OPT 125M model consistently outperformed the OPT 350M model in in-domain accuracy
    across all few-shot learning sizes. This is due to the higher complexity of OPT
    350M which did not allow better generalization.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S6.F2 "图2 ‣ 6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT和Adaptive FT ‣ 6 实验与结果
    ‣ 大型语言模型（LLMs）在低资源设置下不同高效微调方法的比较分析")中的右侧图呈现了MNLI数据集的类似情况。唯一需要注意的是，模型的领域外准确率在不同样本量下保持相对不变。随着样本量的增加，较大的OPT
    350M模型在领域内准确率上表现出适度的提升，而领域外准确率的波动很小。这是预期中的情况，因为如第[1.1节](#S1.SS1 "1.1 数据集 ‣ 1 介绍
    ‣ 大型语言模型（LLMs）在低资源设置下不同高效微调方法的比较分析")所述，MNLI数据集使用HANS数据集作为OOD数据，而COLA数据集使用其自身来源的一个子集。OPT
    125M模型在所有少样本学习大小下的一致性领域内准确率始终优于OPT 350M模型。这是由于OPT 350M的更高复杂性未能提供更好的泛化能力。
- en: 6.2.2 PBFT
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 PBFT
- en: For the COLA dataset, as we can see in the left plot of Figure [3](#S6.F3 "Figure
    3 ‣ 6.2.2 PBFT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting"), OPT 125M model displayed better out-of-domain
    accuracy at the smallest sample size (N=2), while the OPT 350M model showed better
    in-domain performance at mid-range sample sizes (N=16, 64). At N=128, the OPT
    350M model slightly surpassed the 125M model in out-of-domain accuracy, demonstrating
    its potential for higher performance gains at larger few-shot sizes. These findings
    highlight the complex effects of PBFT on model accuracy, suggesting larger few-shot
    sizes may be preferable for optimal performance in linguistic acceptability tasks.
    We used GPT-3 style prompts for our patterns, suggesting that improved prompting
    could further enhance our models’ performance. However, the study of prompts’
    effect on model accuracy is beyond the scope of this work.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于COLA数据集，如图[3](#S6.F3 "图3 ‣ 6.2.2 PBFT ‣ 6.2 Vanilla FT, PBFT和Adaptive FT ‣
    6 实验与结果 ‣ 大型语言模型（LLMs）在低资源设置下不同高效微调方法的比较分析")的左侧图所示，OPT 125M模型在最小样本量（N=2）下表现出更好的领域外准确率，而OPT
    350M模型在中等样本量（N=16, 64）下表现出更好的领域内性能。在N=128时，OPT 350M模型在领域外准确率上稍微超过了125M模型，显示了在更大少样本量下其潜在的性能提升。这些发现突显了PBFT对模型准确率的复杂影响，建议更大的少样本量可能更适合于语言接受性任务中的最佳表现。我们使用了GPT-3风格的提示作为我们的模式，表明改进的提示可能进一步提升我们模型的表现。然而，提示对模型准确率的影响研究超出了本工作的范围。
- en: '![Refer to caption](img/3ec0558405ad979fda2122793d86949e.png)![Refer to caption](img/0e1b36e34f3321927d06489328c83eef.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3ec0558405ad979fda2122793d86949e.png)![参见标题](img/0e1b36e34f3321927d06489328c83eef.png)'
- en: 'Figure 3: PBFT accuracy for OPT 125M and OPT 350M on (left) COLA and (right)
    MNLI for different samples'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：OPT 125M和OPT 350M在（左）COLA和（右）MNLI上对于不同样本的PBFT准确率
- en: For the MNLI dataset, OPT 125M model consistently outperformed the OPT 350M
    model in in-domain accuracy at higher sample sizes, while out-of-domain accuracies
    remained relatively stable across both models, with slight improvements noted
    at the largest sample sizes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MNLI数据集，OPT 125M模型在较高样本量下的一致性领域内准确率始终优于OPT 350M模型，而领域外准确率在两个模型间相对稳定，仅在最大样本量下有轻微改进。
- en: 6.2.3 Adaptive FT
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 Adaptive FT
- en: We performed adaptive tuning on the OPT 125M model by freezing the entire model
    and tuning only the last two decoder layers (11th and 12th). We used a dynamic
    learning rate strategy. This approach aimed to assess the impact of targeted layer
    freezing and responsive learning rate adjustments on model performance. Our results
    showed progressive improvement in both in-domain and out-of-domain accuracies
    as sample size increased, peaking at N=128 as seen in Figure [4](#S6.F4 "Figure
    4 ‣ 6.2.3 Adaptive FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and
    Results ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large
    Language Models (LLMs) in Low-Resource Setting"). These findings underscore the
    effectiveness of this approach in boosting performance and enhancing generalization
    capabilities. However, compared to full-model fine-tuning, [6.2.1](#S6.SS2.SSS1
    "6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting") and [6.2.2](#S6.SS2.SSS2 "6.2.2 PBFT ‣
    6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting"), the accuracies obtained in adaptive fine-tuning were
    lower, as expected, since we were only tuning the last few layers of the model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对OPT 125M模型进行了自适应调优，通过冻结整个模型，仅调整最后两层解码器（第11层和第12层）。我们使用了动态学习率策略。这种方法旨在评估有针对性地冻结层和响应性学习率调整对模型性能的影响。我们的结果显示，随着样本量的增加，领域内和领域外的准确率均有所提升，在N=128时达到峰值，如图[4](#S6.F4
    "Figure 4 ‣ 6.2.3 Adaptive FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments
    and Results ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting")所示。这些发现强调了这种方法在提升性能和增强泛化能力方面的有效性。然而，与完全模型微调相比，[6.2.1](#S6.SS2.SSS1
    "6.2.1 Vanilla FT ‣ 6.2 Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting")和[6.2.2](#S6.SS2.SSS2 "6.2.2 PBFT ‣ 6.2
    Vanilla FT, PBFT and Adaptive FT ‣ 6 Experiments and Results ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting")中自适应微调获得的准确率较低，这符合预期，因为我们仅调整了模型的最后几层。
- en: '![Refer to caption](img/88629c3ba898dc3565096cc9c670680e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/88629c3ba898dc3565096cc9c670680e.png)'
- en: 'Figure 4: Adaptive FT accuracy for OPT 125M on COLA for different few-shot
    samples'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OPT 125M在COLA上针对不同少量样本的自适应微调准确率
- en: 6.3 Effect of using parameter-efficient modeling
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 使用参数高效建模的效果
- en: We anticipated similar performance between parameter-efficient fine-tuning (PEFT),
    vanilla, and pattern-based fine-tuning. Experiments were conducted for different
    matrix ranks, adapting all model layers with LoRA adapters. The LoRA configuration
    was applied to training but not inference. After experimenting with different
    $\alpha$ and LoRA dropout values, we settled on 8 and 0.1 respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预期参数高效微调（PEFT）、传统微调和基于模式的微调之间会有类似的表现。实验针对不同的矩阵秩进行了，使用LoRA适配器调整了所有模型层。LoRA配置应用于训练而非推理。在试验了不同的$\alpha$和LoRA
    dropout值后，我们最终确定了分别为8和0.1。
- en: 'In Figure [5](#S6.F5 "Figure 5 ‣ 6.3 Effect of using parameter-efficient modeling
    ‣ 6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting") – The
    plot compares in-domain and out-of-domain accuracies on the COLA dataset for different
    example sizes, showing an upward trend for N=2 to N=64, highlighting the benefits
    of more training data. However, accuracy decreases at N=128, a trend differing
    from other tuning methods used in this study. This suggests that the matrix decomposition
    approximation might impact the adaptability of the original model for the inferred
    task. Given the OPT model’s design for sequence generation and its use for classification
    here, techniques like LoRA and few-shot, which add approximation to backpropagation
    and reduce the training set, might not be as effective. We refer user to Appendix
    [B](#A2 "Appendix B Appendix: Plots for LoRA for different ranks ‣ Comparative
    Analysis of Different Efficient Fine Tuning Methods of Large Language Models (LLMs)
    in Low-Resource Setting") for the graphs for individual ranks.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#S6.F5 "Figure 5 ‣ 6.3 使用参数高效建模的效果 ‣ 6 实验与结果 ‣ 大型语言模型（LLMs）在低资源设置下不同高效微调方法的比较分析")中
    - 图表比较了COLA数据集上不同示例大小的领域内和领域外准确率，显示N=2到N=64的上升趋势，突出更多训练数据的好处。然而，N=128时准确率下降，这一趋势与本研究中使用的其他微调方法不同。这表明矩阵分解近似可能影响原始模型对推断任务的适应性。鉴于OPT模型的设计用于序列生成，并在此处用于分类，像LoRA和少样本这样的技术，虽然为反向传播增加了近似并减少了训练集，但可能效果不如预期。我们建议用户参阅附录[B](#A2
    "附录B 附录：LoRA不同秩的图表 ‣ 大型语言模型（LLMs）在低资源设置下不同高效微调方法的比较分析")中有关单个秩的图表。
- en: These results illuminate the nuanced balance required in parameter-efficient
    fine-tuning strategies to enhance model performance while maintaining robust generalization
    capabilities.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果揭示了在参数高效微调策略中需要的微妙平衡，以提升模型性能同时保持稳健的泛化能力。
- en: '![Refer to caption](img/76bd4d9e8018b857bd6b163d18e5c61d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76bd4d9e8018b857bd6b163d18e5c61d.png)'
- en: 'Figure 5: Accuracy on COLA for various few shot samples averaged across ranks
    of LoRA layers'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在COLA上不同少样本样本的准确率，按LoRA层的秩平均
- en: 6.4 Effect of using context distillation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 使用上下文蒸馏的效果
- en: We expected two outcomes from context distillation. Firstly, the distilled models
    would likely show better performance due to acquired knowledge. Secondly, the
    few-shot distillation aimed to balance performance with efficiency, enhancing
    learning with fewer examples, potentially leading to more resource-efficient training
    without much performance compromise.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对上下文蒸馏期望有两个结果。首先，蒸馏模型可能会因获得的知识而显示出更好的性能。其次，少样本蒸馏旨在平衡性能与效率，通过更少的示例提升学习，可能实现更具资源效率的训练，而不会大幅度妥协性能。
- en: Applying context distillation to the OPT 125M model showed a domain accuracy
    increase from 0.6314 to 0.7209 over three epochs, confirming its effectiveness.
    However, out-of-domain accuracy was inconsistent, peaking at 0.5250 then dropping
    to 0.4975, showing complex interactions between domain-specific learning and generalization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将上下文蒸馏应用于OPT 125M模型，在三个周期内，领域准确率从0.6314提高到0.7209，确认了其有效性。然而，领域外准确率不稳定，最高达到0.5250，然后降至0.4975，显示了领域特定学习和泛化之间的复杂互动。
- en: '![Refer to caption](img/78a7d4acdab7af26a3677409ddd46059.png)![Refer to caption](img/52c614fba78ddc7326cd9fdeb04a39c3.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78a7d4acdab7af26a3677409ddd46059.png)![参见说明](img/52c614fba78ddc7326cd9fdeb04a39c3.png)'
- en: 'Figure 6: Accuracy with context distillation on OPT 125M model (right) with
    and (left) w/o few-shot on MNLI for different samples'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在OPT 125M模型上（右）应用上下文蒸馏的准确率（左）和（右）不使用少样本在MNLI上的不同样本准确率
- en: Few-shot distillation evaluations showed a significant increase in in-domain
    accuracy from 0.4801 at N=2 to 0.6708 at N=128, indicating better knowledge assimilation
    with more examples. Out-of-domain accuracy remained stable at 0.5, with a slight
    drop to 0.4967 at N=64, suggesting limited generalization benefits. Combining
    few-shot with context distillation achieved an in-domain accuracy of 0.67 with
    N=128, nearing the 0.72 from traditional context distillation. This combined approach
    took only 5 minutes and 19 seconds, compared to 1 hour and 51 minutes for traditional
    methods, suggesting that increasing sample size in few-shot fine-tuning could
    match or surpass full dataset fine-tuning efficacy, while being more time and
    resource-efficient.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本蒸馏评估显示，从 N=2 的 0.4801 到 N=128 的 0.6708，领域内准确率显著提高，表明随着样本数量增加，知识吸收更好。领域外准确率保持稳定在
    0.5，略微下降到 N=64 的 0.4967，表明泛化收益有限。将少样本与上下文蒸馏结合在 N=128 时达到了 0.67 的领域内准确率，接近传统上下文蒸馏的
    0.72。该综合方法仅需 5 分钟 19 秒，而传统方法需 1 小时 51 分钟，这表明在少样本微调中增加样本量可以匹配或超越全数据集微调的效果，同时更节省时间和资源。
- en: Additionally, it is important to note that no significant performance differences
    were observed in the out-of-domain evaluation, using the HANS dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值得注意的是，在使用 HANS 数据集的领域外评估中未观察到显著的性能差异。
- en: 7 Conclusion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this paper, we compared different ways of efficiently fine-tuning a LLM
    originally created for sequence generation tasks on a sequence classification
    task in the realms of few-shot learning. We examined standard fine-tuning techniques
    like vanilla FT, PBFT and compared their OOD accuracies against some of the more
    advanced methods such as adaptive FT, PEFT with LoRA and context distillation.
    We present results for a few-shot setting with 16 examples ($N$) in this section,
    with additional comparisons in Appendix [C](#A3 "Appendix C Appendix: Results
    for various few-shot sample sizes ‣ Comparative Analysis of Different Efficient
    Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们比较了在少样本学习领域内，高效微调原本用于序列生成任务的 LLM 以进行序列分类任务的不同方法。我们检查了标准微调技术，如传统 FT、PBFT，并将它们的
    OOD 准确率与一些更先进的方法，如适应性 FT、结合 LoRA 的 PEFT 和上下文蒸馏进行了比较。本节展示了 16 个样本（$N$）的少样本设置结果，附录
    [C](#A3 "附录 C 附录：各种少样本样本大小的结果 ‣ 不同高效微调方法在低资源环境下的大型语言模型（LLMs）的比较分析") 中还有更多比较。
- en: As we can see in Table [1](#S7.T1 "Table 1 ‣ 7 Conclusion ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting") below, Our experiments reveal that vanilla FT outperforms
    PBFT on the CoLA dataset for the smaller model, indicating a need for improved
    prompting.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表 [1](#S7.T1 "表 1 ‣ 7 结论 ‣ 不同高效微调方法在低资源环境下的大型语言模型（LLMs）的比较分析") 所示，我们的实验表明，传统
    FT 在 CoLA 数据集上的表现优于 PBFT 对于较小的模型，表明需要改进提示。
- en: 'Table 1: OOD accuracy comparison b/w Vanilla and PBFT on COLA (N=16)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：传统与 PBFT 在 COLA 上的 OOD 准确率比较（N=16）
- en: '| Model | OPT 125M | OPT 350M |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | OPT 125M | OPT 350M |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method |  |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  |  |'
- en: '| Vanilla Fine Tuning | 0.5310 | 0.5058 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 传统微调 | 0.5310 | 0.5058 |'
- en: '| Pattern Based | 0.5058 | 0.5213 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.5058 | 0.5213 |'
- en: Adaptive fine-tuning performs better compared to vanilla fine-tuning and pattern-based
    methods (Table [2](#S7.T2 "Table 2 ‣ 7 Conclusion ‣ Comparative Analysis of Different
    Efficient Fine Tuning Methods of Large Language Models (LLMs) in Low-Resource
    Setting")), suggesting its effectiveness in optimizing the model for better generalization
    in certain contexts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于传统微调和基于模式的方法，适应性微调表现更好（表 [2](#S7.T2 "表 2 ‣ 7 结论 ‣ 不同高效微调方法在低资源环境下的大型语言模型（LLMs）的比较分析")），表明其在特定环境下优化模型以获得更好泛化效果的有效性。
- en: 'Table 2: OOD delta b/w adaptive and base FT on COLA (N=16)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：COLA 上适应性和基础 FT 的 OOD 差异（N=16）
- en: '| Method | Adaptive Fine Tuning |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 适应性微调 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | 0.0290 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 传统微调 | 0.0290 |'
- en: '| Pattern Based | 0.0543 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0543 |'
- en: Accuracies in LoRA are similar to other fine-tuning methods (Table [3](#S7.T3
    "Table 3 ‣ 7 Conclusion ‣ Comparative Analysis of Different Efficient Fine Tuning
    Methods of Large Language Models (LLMs) in Low-Resource Setting")), potentially
    due to fine-tuning all layers in the latter set of methods. Also, as noted earlier,
    we use a $\alpha$ value of 8 for our LoRA configuration, as compared to 32 by
    the original authors of the paper Hu et al. ([2021](#bib.bib2)), which could undermine
    LoRA.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的准确性与其他微调方法相似（表 [3](#S7.T3 "表 3 ‣ 7 结论 ‣ 大型语言模型 (LLMs) 在低资源设置中不同高效微调方法的比较分析")），这可能是由于在后一组方法中对所有层进行微调所致。此外，如前所述，我们在
    LoRA 配置中使用了 $\alpha$ 值为 8，而原文作者 Hu 等人 ([2021](#bib.bib2)) 使用的是 32，这可能会削弱 LoRA。
- en: 'Table 3: OOD delta b/w LoRA and base FT on COLA (N=16)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：COLA 上 LoRA 和基本 FT 之间的 OOD 差异 (N=16)
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法/排名 | 1 | 2 | 4 | 8 | 64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FT | -0.0755 | -0.0697 | -0.0697 | -0.0697 | -0.0697 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 普通 FT | -0.0755 | -0.0697 | -0.0697 | -0.0697 | -0.0697 |'
- en: '| Pattern Based | -0.0503 | -0.0445 | -0.0445 | -0.0445 | -0.0445 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式的 | -0.0503 | -0.0445 | -0.0445 | -0.0445 | -0.0445 |'
- en: Finally, we notice that context distillation leads to a modest improvement in
    OOD accuracy over the other methods (Table [4](#S7.T4 "Table 4 ‣ 7 Conclusion
    ‣ Comparative Analysis of Different Efficient Fine Tuning Methods of Large Language
    Models (LLMs) in Low-Resource Setting")). Despite small differences, it underscores
    context distillation’s potential to boost the model’s generalization beyond pattern-based
    or vanilla fine-tuning techniques, explaining its growing popularity in the LLM
    community.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们注意到上下文蒸馏相较于其他方法在 OOD 准确性上有适度的提升（表 [4](#S7.T4 "表 4 ‣ 7 结论 ‣ 大型语言模型 (LLMs)
    在低资源设置中不同高效微调方法的比较分析")）。尽管差异较小，但这突显了上下文蒸馏提升模型泛化能力的潜力，解释了其在 LLM 社区日益增长的受欢迎程度。
- en: 'Table 4: OOD delta b/w few shot context distillation and base FT on MNLI (N=16)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：MNLI 上少量样本上下文蒸馏和基本 FT 之间的 OOD 差异 (N=16)
- en: '| Method | Few Shot Context Distillation |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 少量样本上下文蒸馏 |'
- en: '| --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | 0.0310 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 普通微调 | 0.0310 |'
- en: '| Pattern Based | 0.0058 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式的 | 0.0058 |'
- en: In summary, our analysis highlights the advantages and subtleties of various
    fine-tuning methods in the LLM community. We consolidated these methods in a few-shot
    learning setting to understand their benefits. Despite resource limitations, our
    experiments establish a premise and more extensive resources and time could yield
    a deeper understanding of these methods.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的分析突出了 LLM 社区中各种微调方法的优势和细微之处。我们在少量样本学习环境中整合了这些方法，以理解它们的好处。尽管资源有限，我们的实验建立了一个前提，更广泛的资源和时间可能会提供对这些方法更深入的理解。
- en: 8 Acknowledgement
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: 'We would like to thank Dr. Zsolt Kira of Georgia Institute of Technology and
    the course teaching assistants and staff of CS 7643: Deep Learning at Georgia
    Institute of Technology. This work was pursued as part of the above-said course
    and the knowledge gained by the authors in this course was instrumental in carrying
    out this work.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们感谢乔治亚理工学院的 Zsolt Kira 博士，以及乔治亚理工学院 CS 7643: 深度学习课程的教学助理和工作人员。本研究作为上述课程的一部分进行，作者在课程中获得的知识对开展这项工作至关重要。'
- en: References
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Mosbach et al. [2023] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich
    Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair
    comparison and evaluation, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mosbach 等人 [2023] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich
    Klakow, 和 Yanai Elazar。少量样本微调与上下文学习：公平比较和评估，2023。
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen。Lora: 大型语言模型的低秩适应，2021。'
- en: 'Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*,
    pages 1112–1122\. Association for Computational Linguistics, 2018. URL [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams 等人 [2018] Adina Williams, Nikita Nangia 和 Samuel Bowman. 一种广覆盖的挑战语料库，用于通过推理理解句子。在
    *2018年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇论文）*，第1112–1122页。计算语言学协会, 2018。网址 [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101)。
- en: Warstadt et al. [2018] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman.
    Neural network acceptability judgments. *arXiv preprint arXiv:1805.12471*, 2018.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warstadt 等人 [2018] Alex Warstadt, Amanpreet Singh 和 Samuel R Bowman. 神经网络可接受性判断。*arXiv
    预印本 arXiv:1805.12471*, 2018。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor
    Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
    Anjali Sridhar, Tianlu Wang 和 Luke Zettlemoyer. Opt: 开放预训练变换器语言模型, 2022。'
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 语言模型是少样本学习者, 2020。
- en: 'Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding, 2019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy 和 Samuel R. Bowman. Glue: 用于自然语言理解的多任务基准与分析平台, 2019。'
- en: 'McCoy et al. [2019] Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the
    wrong reasons: Diagnosing syntactic heuristics in natural language inference.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3428–3448, Florence, Italy, 2019. Association for Computational
    Linguistics.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCoy 等人 [2019] Tom McCoy, Ellie Pavlick 和 Tal Linzen. 错误的正确理由：诊断自然语言推理中的句法启发式。在
    *第57届计算语言学协会年会论文集*，第3428–3448页，意大利佛罗伦萨, 2019。计算语言学协会。
- en: 'Inc. [2023] Meta Platforms Inc. Few-shot fine-tuning vs. in-context learning:
    A fair comparison and evaluation. *https://github.com/uds-lsv/llmft*, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Inc. [2023] Meta Platforms Inc. 少样本微调与上下文学习: 公平比较与评估。*https://github.com/uds-lsv/llmft*,
    2023。'
- en: Askell et al. [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared
    Kaplan. A general language assistant as a laboratory for alignment, 2021.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell 等人 [2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli,
    Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage,
    Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah 和 Jared
    Kaplan. 一种作为对齐实验室的通用语言助手, 2021。
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp, 2019.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等人 [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan 和 Sylvain Gelly.
    参数高效的自然语言处理迁移学习, 2019。
- en: 'Raschka [2024] Sebastian Raschka. Improving lora: Implementing weight-decomposed
    low-rank adaptation (dora) from scratch. *https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch*,
    2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raschka [2024] Sebastian Raschka. 改进 LoRA: 从头实现权重分解低秩适应（DORA）。*https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch*，2024。'
- en: Csiszar [1975] I. Csiszar. $i$-divergence geometry of probability distributions
    and minimization problems. In *The Annals of Probability*, pages 146 – 158\. The
    Annals of Probability, 1975. URL [https://doi.org/10.1214/aop/1176996454](https://doi.org/10.1214/aop/1176996454).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Csiszar [1975] I. Csiszar. 概率分布的 $i$-散度几何及最优化问题。在 *The Annals of Probability*，第146
    – 158页。The Annals of Probability，1975年。网址 [https://doi.org/10.1214/aop/1176996454](https://doi.org/10.1214/aop/1176996454)。
- en: Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization, 2019.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2019] Ilya Loshchilov 和 Frank Hutter. 解耦的权重衰减正则化，2019。
- en: 'Appendix A Appendix: Hyper-parameters'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 A: 超参数'
- en: 'To perform a fair comparison between all the experiments, we kept the hyper-parameters
    uniform across all of them, as listed in table [5](#A1.T5 "Table 5 ‣ Appendix
    A Appendix: Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine
    Tuning Methods of Large Language Models (LLMs) in Low-Resource Setting").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在所有实验之间进行公平比较，我们保持了所有实验的超参数一致，如表[5](#A1.T5 "表 5 ‣ 附录 A: 超参数 ‣ 大型语言模型（LLMs）在低资源环境中的不同高效微调方法的比较分析")中所列。'
- en: 'Table 5: Hyper-parameters used in all the experiments'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 所有实验中使用的超参数'
- en: '| Hyper-parameter | Value |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Few-shot sample sizes | 2, 16, 32, 64, 128 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 少量样本大小 | 2, 16, 32, 64, 128 |'
- en: '| # of epochs | 40 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 轮次数 | 40 |'
- en: '| Batch size | 32 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 |'
- en: '| Learning Rate | $1e^{-5}$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $1e^{-5}$ |'
- en: '| Weight decay | 0.0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.0 |'
- en: '| Warm-up ratio | 0.1 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 预热比例 | 0.1 |'
- en: '| # of runs/trials per sample size | 10 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 每种样本大小的运行/试验次数 | 10 |'
- en: '| Optimizer | AdamW |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW |'
- en: 'Further, some additional parameters were used for experiments that involved
    LoRA. Those are listed below in Table [6](#A1.T6 "Table 6 ‣ Appendix A Appendix:
    Hyper-parameters ‣ Comparative Analysis of Different Efficient Fine Tuning Methods
    of Large Language Models (LLMs) in Low-Resource Setting").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在涉及 LoRA 的实验中还使用了一些额外的参数，这些参数在表[6](#A1.T6 "表 6 ‣ 附录 A: 超参数 ‣ 大型语言模型（LLMs）在低资源环境中的不同高效微调方法的比较分析")中列出。'
- en: 'Table 6: Additional hyper-parameters used in LoRA experiments'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: LoRA 实验中使用的额外超参数'
- en: '| Hyper-parameter | Value |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LoRA matrix ranks | 1, 2, 4, 8, 64 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 矩阵秩 | 1, 2, 4, 8, 64 |'
- en: '| $\alpha$ | 8 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 8 |'
- en: '| Dropout | 0.1 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | 0.1 |'
- en: 'Appendix B Appendix: Plots for LoRA for different ranks'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 B: 不同秩的 LoRA 图示'
- en: In Section [6.3](#S6.SS3 "6.3 Effect of using parameter-efficient modeling ‣
    6 Experiments and Results ‣ Comparative Analysis of Different Efficient Fine Tuning
    Methods of Large Language Models (LLMs) in Low-Resource Setting"), we plotted
    the results of our experiments with PEFT LoRA adapters for different few-shot
    sample sizes, averaged across ranks. In this appendix section, we present the
    supplementary plots for the same experiment but segregated for each individual
    matrix rank.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[6.3节](#S6.SS3 "6.3 使用参数高效建模的效果 ‣ 6 实验与结果 ‣ 大型语言模型（LLMs）在低资源环境中的不同高效微调方法的比较分析")中，我们绘制了使用
    PEFT LoRA 适配器的实验结果，按秩对不同的少量样本大小进行平均。在本附录部分，我们展示了相同实验的补充图示，但按每个单独的矩阵秩进行分隔。
- en: '![Refer to caption](img/18e7b8e4f392ba47b879220bd5c9f6c0.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/18e7b8e4f392ba47b879220bd5c9f6c0.png)'
- en: 'Figure 7: Accuracy for various few shot samples LoRA layers with rank, $r=1$'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 各种少量样本的 LoRA 层的准确率，秩 $r=1$'
- en: '![Refer to caption](img/57486339eb7e76373e3a9a8c76fdf8cf.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/57486339eb7e76373e3a9a8c76fdf8cf.png)'
- en: 'Figure 8: Accuracy for various few shot samples LoRA layers with rank, $r=2$'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 各种少量样本的 LoRA 层的准确率，秩 $r=2$'
- en: '![Refer to caption](img/bf0a1455ad3a0e48495a91f5e09681d2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bf0a1455ad3a0e48495a91f5e09681d2.png)'
- en: 'Figure 9: Accuracy for various few shot samples LoRA layers with rank, $r=4$'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 各种少量样本的 LoRA 层的准确率，秩 $r=4$'
- en: '![Refer to caption](img/6ebc19dfafa1baa9637b940747d0182e.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ebc19dfafa1baa9637b940747d0182e.png)'
- en: 'Figure 10: Accuracy for various few shot samples LoRA layers with rank, $r=8$'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 各种少量样本的 LoRA 层的准确率，秩 $r=8$'
- en: '![Refer to caption](img/47980c874a71e655a6175d9c4e4693fd.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/47980c874a71e655a6175d9c4e4693fd.png)'
- en: 'Figure 11: Accuracy for various few shot samples LoRA layers with rank, $r=64$'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 各种少量样本的 LoRA 层的准确率，秩 $r=64$'
- en: 'Appendix C Appendix: Results for various few-shot sample sizes'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 C 附录: 不同少量样本大小的结果'
- en: In our conclusion section, Section [7](#S7 "7 Conclusion ‣ Comparative Analysis
    of Different Efficient Fine Tuning Methods of Large Language Models (LLMs) in
    Low-Resource Setting"), for concise analysis and due to space constraint, we presented
    all the comparisons with respect to few-shot sample size, $N=16$. In this appndix
    section, we tabulate those comparisons for all the fine-tuning methods for all
    different few-shot sample sizes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的结论部分，[第 7 节](#S7 "7 结论 ‣ 大型语言模型 (LLMs) 在低资源设置下的不同高效微调方法的比较分析")，为了简洁分析和由于空间限制，我们展示了关于少量样本大小
    $N=16$ 的所有比较。在本附录部分，我们列出了所有微调方法在不同少量样本大小下的比较。
- en: C.1 Adaptive Fine-Tuning
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 自适应微调
- en: 'Table 7: OOD delta b/w adaptive and base FT on COLA (N=2)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 在 COLA 上，adaptive 和 base FT 之间的 OOD 差值（N=2）'
- en: '| Method | Adaptive Fine Tuning |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 自适应微调 |'
- en: '| --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | -0.006977 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | -0.006977 |'
- en: '| Pattern Based | -0.02441 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | -0.02441 |'
- en: 'Table 8: OOD delta b/w adaptive and base FT on COLA (N=32)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 在 COLA 上，adaptive 和 base FT 之间的 OOD 差值（N=32）'
- en: '| Method | Adaptive Fine Tuning |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 自适应微调 |'
- en: '| --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | -0.01647 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | -0.01647 |'
- en: '| Pattern Based | 0.0823 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0823 |'
- en: 'Table 9: OOD delta b/w adaptive and base FT on COLA (N=64)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 在 COLA 上，adaptive 和 base FT 之间的 OOD 差值（N=64）'
- en: '| Method | Adaptive Fine Tuning |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 自适应微调 |'
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | -0.0040 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | -0.0040 |'
- en: '| Pattern Based | 0.0209 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0209 |'
- en: 'Table 10: OOD delta b/w adaptive and base FT on COLA (N=128)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 在 COLA 上，adaptive 和 base FT 之间的 OOD 差值（N=128）'
- en: '| Method | Adaptive Fine Tuning |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 自适应微调 |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | -0.0358 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | -0.0358 |'
- en: '| Pattern Based | -0.04651 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | -0.04651 |'
- en: C.2 Parameter-Efficient Fine tuning with Low-Rank Adaptation (LoRA)
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 参数高效的低秩适应微调 (LoRA)
- en: 'Table 11: OOD delta b/w LoRA and base FT on COLA (N=2)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 在 COLA 上，LoRA 和 base FT 之间的 OOD 差值（N=2）'
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 方法/排名 | 1 | 2 | 4 | 8 | 64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FT | -0.1025 | -0.1286 | -0.1286 | -0.1286 | -0.1286 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 原始 FT | -0.1025 | -0.1286 | -0.1286 | -0.1286 | -0.1286 |'
- en: '| Pattern Based | -0.1199 | -0.1461 | -0.1461 | -0.1461 | -0.1461 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | -0.1199 | -0.1461 | -0.1461 | -0.1461 | -0.1461 |'
- en: 'Table 12: OOD delta b/w LoRA and base FT on COLA (N=32)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 在 COLA 上，LoRA 和 base FT 之间的 OOD 差值（N=32）'
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 方法/排名 | 1 | 2 | 4 | 8 | 64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FT | -0.0073 | -0.0017 | -0.0017 | 0.0001 | -0.0017 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 原始 FT | -0.0073 | -0.0017 | -0.0017 | 0.0001 | -0.0017 |'
- en: '| Pattern Based | 0.0914 | 0.0970 | 0.0970 | 0.0990 | 0.0970 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0914 | 0.0970 | 0.0970 | 0.0990 | 0.0970 |'
- en: 'Table 13: OOD delta b/w LoRA and base FT on COLA (N=64)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 在 COLA 上，LoRA 和 base FT 之间的 OOD 差值（N=64）'
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 方法/排名 | 1 | 2 | 4 | 8 | 64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FT | 0.0251 | 0.0232 | 0.0251 | 0.0310 | 0.0368 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 原始 FT | 0.0251 | 0.0232 | 0.0251 | 0.0310 | 0.0368 |'
- en: '| Pattern Based | 0.0501 | 0.0482 | 0.0501 | 0.0560 | 0.0618 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0501 | 0.0482 | 0.0501 | 0.0560 | 0.0618 |'
- en: 'Table 14: OOD delta b/w LoRA and base FT on COLA (N=128)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 在 COLA 上，LoRA 和 base FT 之间的 OOD 差值（N=128）'
- en: '| Method/Rank | 1 | 2 | 4 | 8 | 64 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 方法/排名 | 1 | 2 | 4 | 8 | 64 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FT | -0.1296 | -0.1379 | -0.1534 | -0.1457 | -0554 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 原始 FT | -0.1296 | -0.1379 | -0.1534 | -0.1457 | -0.0554 |'
- en: '| Pattern Based | -0.1403 | -0.1486 | -0.1641 | -0.1563 | -0.1660 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | -0.1403 | -0.1486 | -0.1641 | -0.1563 | -0.1660 |'
- en: C.3 Context Distillation with Few-Shot Learning
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 少量样本学习的上下文蒸馏
- en: 'Table 15: OOD delta b/w context distillation and base FT on MNLI (N=2)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '表 15: 在 MNLI 上，context distillation 和 base FT 之间的 OOD 差值（N=2）'
- en: '| Method | Few Shot Context Distillation |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 少量样本上下文蒸馏 |'
- en: '| --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | -0.0531 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | -0.0531 |'
- en: '| Pattern Based | -0.07054 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | -0.07054 |'
- en: 'Table 16: OOD delta b/w context distillation and base FT on MNLI (N=32)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '表 16: 在 MNLI 上，context distillation 和 base FT 之间的 OOD 差值（N=32）'
- en: '| Method | Few Shot Context Distillation |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 少量样本上下文蒸馏 |'
- en: '| --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | 0.0521 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | 0.0521 |'
- en: '| Pattern Based | 0.0467 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.0467 |'
- en: 'Table 17: OOD delta b/w context distillation and base FT on MNLI (N=64)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '表 17: 在 MNLI 上，context distillation 和 base FT 之间的 OOD 差值（N=64）'
- en: '| Method | Few Shot Context Distillation |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 少量样本上下文蒸馏 |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | 0.0769 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 原始微调 | 0.0769 |'
- en: '| Pattern Based | 0.05191 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.05191 |'
- en: 'Table 18: OOD delta b/w context distillation and base FT on MNLI (N=128)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：在 MNLI 上，上下文蒸馏与基础微调的 OOD 差异（N=128）
- en: '| Method | Few Shot Context Distillation |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 少样本上下文蒸馏 |'
- en: '| --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Vanilla Fine Tuning | 0.1205 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 普通微调 | 0.1205 |'
- en: '| Pattern Based | 0.1312 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 基于模式 | 0.1312 |'
