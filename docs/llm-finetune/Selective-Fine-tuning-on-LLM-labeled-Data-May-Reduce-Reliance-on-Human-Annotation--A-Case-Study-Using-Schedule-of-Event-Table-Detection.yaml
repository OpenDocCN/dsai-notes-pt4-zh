- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:36:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:36:53'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation:
    A Case Study Using Schedule-of-Event Table Detection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 LLM 标注数据进行选择性微调可能减少对人工标注的依赖：基于 Schedule-of-Event 表检测的案例研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06093](https://ar5iv.labs.arxiv.org/html/2405.06093)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06093](https://ar5iv.labs.arxiv.org/html/2405.06093)
- en: \newunicodechar
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newunicodechar
- en: '✓✓ \newunicodechar✗✗ \theorembodyfont \theoremheaderfont \theorempostheader:
    \theoremsep'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '✓✓ \newunicodechar✗✗ \theorembodyfont \theoremheaderfont \theorempostheader:
    \theoremsep'
- en: \NameBhawesh Kumar \Emailbhaweshk@verily.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \NameBhawesh Kumar \Emailbhaweshk@verily.com
- en: \addrVerily Life Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \addrVerily Life Sciences
- en: 269 East Grand Avenue    South San Francisco    \NameJonathan Amar \Emailjonathanamar@verily.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 269 East Grand Avenue    South San Francisco    \NameJonathan Amar \Emailjonathanamar@verily.com
- en: \addrVerily Life Sciences
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addrVerily Life Sciences
- en: 269 East Grand Avenue    South San Francisco    \NameEric Yang \Emaileryang@verily.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 269 East Grand Avenue    South San Francisco    \NameEric Yang \Emaileryang@verily.com
- en: \addrVerily Life Sciences
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \addrVerily Life Sciences
- en: 269 East Grand Avenue    South San Francisco    \NameNan Li \Emailnotanumber@verily.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 269 East Grand Avenue    South San Francisco    \NameNan Li \Emailnotanumber@verily.com
- en: \addrVerily Life Sciences
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \addrVerily Life Sciences
- en: 269 East Grand Avenue    South San Francisco    \NameYugang Jia \Emailyugang@verily.com
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 269 East Grand Avenue    South San Francisco    \NameYugang Jia \Emailyugang@verily.com
- en: \addrVerily Life Sciences
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \addrVerily Life Sciences
- en: 269 East Grand Avenue    South San Francisco
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 269 East Grand Avenue    South San Francisco
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have demonstrated their efficacy across a broad
    spectrum of tasks in healthcare applications. However, often LLMs need to be fine-tuned
    on task-specific expert-annotated data to achieve optimal performance, which can
    be expensive and time consuming. In this study, we fine-tune PaLM-2 (Anil et al.
    ([2023](#bib.bib1))) with parameter efficient finetuning (PEFT) using noisy labels
    obtained from gemini-pro 1.0 (Google ([2024](#bib.bib7))) for the detection of
    Schedule-of-Event (SoE) tables, which specify care plan in clinical trial protocols.
    We introduce a filtering mechanism to select high-confidence labels for this table
    classification task, thereby reducing the noise in the auto-generated labels.
    We show that fine-tuned PaLM-2 with those labels achieves performance that exceeds
    the gemini-pro 1.0 and other LLMs. Furthermore, its performance is close to a
    PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our results show
    that leveraging LLM-generated labels through powerful models like gemini-pro can
    potentially serve as a viable strategy for improving LLM performance through fine-tuning
    in specialized tasks, particularly in domains where expert annotations are scarce,
    expensive, or time-consuming to obtain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在医疗保健应用中已经证明了其在各种任务中的有效性。然而，LLMs 通常需要在任务特定的专家标注数据上进行微调以达到**最佳性能**，这可能既昂贵又耗时。在这项研究中，我们利用从
    gemini-pro 1.0（Google ([2024](#bib.bib7))) 获取的噪声标签，通过参数高效微调（PEFT）对 PaLM-2（Anil
    等人 ([2023](#bib.bib1))) 进行微调，以检测 Schedule-of-Event (SoE) 表，这些表格在临床试验方案中指定护理计划。我们引入了一种过滤机制，以选择高置信度标签进行此表格分类任务，从而减少自动生成标签中的噪声。我们展示了使用这些标签微调后的
    PaLM-2 达到了超越 gemini-pro 1.0 和其他 LLMs 的性能。此外，其性能接近于在非专家标注者获得的标签上微调的 PaLM-2。我们的结果表明，通过强大的模型如
    gemini-pro 利用 LLM 生成的标签，可以成为提高 LLM 在专业任务中性能的**可行策略**，尤其是在专家标注稀缺、昂贵或耗时的领域。
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large Language Models (LLMs) have been found to be useful across diverse tasks
    like natural language understanding and generation, question-answering, summarization,
    programming, and creative arts (Chen et al. ([2021](#bib.bib4)); Radford et al.
    ([2018](#bib.bib18), [2019](#bib.bib19)); Ramesh et al. ([2021](#bib.bib21))).
    LLMs are particularly promising in specialized fields such as healthcare, where
    they can significantly enhance clinical decision-making, patient care, drug discovery,
    and the management and utilization of medical data (Singhal et al. ([2023](#bib.bib25));
    Ingraham et al. ([2023](#bib.bib10)); Tu et al. ([2024a](#bib.bib26), [b](#bib.bib27));
    Sharma et al. ([2024](#bib.bib23))). However, the successful application of LLMs
    in specialized domains frequently depends on their ability to process and understand
    complex, domain-specific structured and unstructured content, which often requires
    fine-tuning the models with data annotated by experts (van Aken ([2023](#bib.bib28))).
    This necessity presents considerable challenges, primarily due to the scarcity,
    high cost, and substantial time required to acquire expert annotations in fields
    like healthcare. In response to these challenges, our work investigates the use
    of LLM-generated labels for fine-tuning purposes, with a specific case-study on
    identifying Schedule-of-Event (SoE) tables in clinical trial protocols. The accurate
    identification of SoE tables, which outlines plan-of-care in clinical trials (also
    see appendix [A](#A1 "Appendix A Schedule of Event Tables ‣ Selective Fine-tuning
    on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using
    Schedule-of-Event Table Detection") for more details on SoE tables), plays a pivotal
    role in the digitization of clinical trial protocols which we briefly describe
    below.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言理解与生成、问答、总结、编程以及创意艺术等多种任务中被发现是有用的（Chen et al. ([2021](#bib.bib4));
    Radford et al. ([2018](#bib.bib18), [2019](#bib.bib19)); Ramesh et al. ([2021](#bib.bib21)))。LLMs
    在医疗保健等专业领域特别有前景，它们可以显著提升临床决策、病人护理、药物发现以及医疗数据的管理和利用（Singhal et al. ([2023](#bib.bib25));
    Ingraham et al. ([2023](#bib.bib10)); Tu et al. ([2024a](#bib.bib26), [b](#bib.bib27));
    Sharma et al. ([2024](#bib.bib23)))。然而，LLMs 在专业领域的成功应用通常依赖于它们处理和理解复杂的、领域特定的结构化和非结构化内容的能力，这往往需要通过专家标注的数据来对模型进行微调（van
    Aken ([2023](#bib.bib28)))。这一必要性带来了相当大的挑战，主要由于在医疗保健等领域获取专家标注的稀缺性、高成本和时间消耗。为应对这些挑战，我们的工作研究了使用LLM生成的标签进行微调的方式，特别是对识别临床试验方案中的事件时间表（SoE）进行案例研究。准确识别SoE表格，这些表格概述了临床试验中的护理计划（有关SoE表格的更多细节请参见附录
    [A](#A1 "附录 A 事件时间表 ‣ 基于LLM标注数据的选择性微调可能减少对人工标注的依赖：基于事件时间表检测的案例研究")），在临床试验方案的数字化中扮演着关键角色，我们将在下面简要描述。
- en: 1.1 Brief Introduction to Clinical Trial Protocols and Digitization
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 临床试验方案与数字化简要介绍
- en: Clinical trials are the backbone of medical research. However, the traditional
    conduct of clinical trials is fraught with inefficiencies at various stages including
    patient recruitment, follow-ups, data acquisition and handling (Inan et al. ([2020](#bib.bib9));
    Marquis-Gravel et al. ([2019](#bib.bib14))). Clinical trials rely heavily on manual
    processes, leading to time-consuming, expensive, and error-prone workflows. The
    inefficiencies pose challenges to all stakeholders involved in the trial and also
    slow down the pace of medical research (Getz and Campo ([2017](#bib.bib6)); Jones
    et al. ([2016](#bib.bib11))).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 临床试验是医学研究的基石。然而，传统的临床试验执行过程中在患者招募、跟进、数据获取和处理等各个阶段充满了低效（Inan et al. ([2020](#bib.bib9));
    Marquis-Gravel et al. ([2019](#bib.bib14)))。临床试验严重依赖手工过程，导致工作流程耗时、昂贵且容易出错。这些低效对所有涉及试验的利益相关者构成挑战，也减慢了医学研究的进展（Getz
    and Campo ([2017](#bib.bib6)); Jones et al. ([2016](#bib.bib11)))。
- en: 'Clinical trial protocols are foundational documents in the trials, outlining
    the detailed methodologies, objectives, and care plans that guide the conduct
    of studies in accordance with regulatory, ethical, and scientific standards. These
    protocols include critical components such as the Schedule of Events (SoE) table,
    which details the plan of care for participants, including visits for screening,
    treatment, and follow-up phases, along with the assessments, treatments, and data
    collection scheduled for these visits. The digitization of clinical trial protocols
    refers to the process of converting these detailed and often voluminous paper-based
    documents into accurate digital workflows (Verily Life Sciences ([2023](#bib.bib30));
    Rosa et al. ([2021](#bib.bib22)); Inan et al. ([2020](#bib.bib9))). This transformation
    is not just a matter of changing the medium but involves the systematic identification,
    classification and ultimately extraction of key elements within the protocols,
    such as SoE tables, to ensure they are accurately captured and can be effectively
    managed and analyzed in a digital system (Inan et al. ([2020](#bib.bib9))). Correctly
    identifying these tables, which can vary significantly in formatting, terminology,
    and layout across different protocols, poses a significant challenge (refer to
    appendix [A](#A1 "Appendix A Schedule of Event Tables ‣ Selective Fine-tuning
    on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using
    Schedule-of-Event Table Detection") for more details on SoE tables as well as
    examples.) However, the accurate classification of such tables are crucial for
    any automated protocol digitization workflow; an undetected SoE table can lead
    to an incomplete care plan, while a misclassified table introduces erroneous information
    into the system, underscoring the paramount importance of reliability in this
    process.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 临床试验方案是试验中的基础文件，概述了详细的方法、目标和护理计划，这些都指导了试验的进行，以符合监管、伦理和科学标准。这些方案包括关键组成部分，如事件时间表（SoE）表格，详细说明了参与者的护理计划，包括筛查、治疗和随访阶段的访问，以及这些访问安排的评估、治疗和数据收集。临床试验方案的数字化指的是将这些详细且通常庞大的纸质文件转换为准确的数字工作流程（Verily
    Life Sciences ([2023](#bib.bib30))；Rosa et al. ([2021](#bib.bib22))；Inan et al.
    ([2020](#bib.bib9)))。这一转变不仅仅是更换介质，而是涉及系统化地识别、分类并最终提取方案中的关键元素，如SoE表格，以确保其准确捕捉并可以在数字系统中有效管理和分析（Inan
    et al. ([2020](#bib.bib9)))。准确识别这些表格是一个重大挑战，因为不同方案中的格式、术语和布局可能差异很大（有关SoE表格的更多详细信息及示例，请参见附录
    [A](#A1 "附录 A 事件时间表 ‣ LLM标注数据的选择性微调可能减少对人工标注的依赖：基于事件时间表检测的案例研究")）。然而，准确分类这些表格对任何自动化方案数字化工作流程至关重要；未检测到的SoE表格可能导致护理计划不完整，而分类错误的表格则会将错误信息引入系统，突显了这一过程可靠性的重要性。
- en: 1.2 Improving Domain-Specific LLM Performance with Synthetic Labels
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 利用合成标签提升领域特定LLM的性能
- en: 1.2.1 Modeling SoE Detection with LLMs
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1 使用LLM建模SoE检测
- en: We model the problem of SoE table classification as a binary classification
    problem and use an LLM (PaLM-2) for accurate classification of SoE tables. To
    improve PaLM-2’s ability to accurately classify SoE tables with fine-tuning, we
    use gemini-pro 1.0 to auto-generate training labels for fine-tuning task. This
    strategy aims to address the challenges of acquiring expert annotations by leveraging
    the capabilities of LLMs to produce high-quality, task-specific data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SoE表格分类问题建模为二分类问题，并使用LLM（PaLM-2）来准确分类SoE表格。为了通过微调提高PaLM-2准确分类SoE表格的能力，我们使用gemini-pro
    1.0自动生成训练标签以用于微调任务。这一策略旨在通过利用LLM生成高质量的任务特定数据来解决获取专家注释的挑战。
- en: 1.2.2 Fine-Tuning LLM with LLM-Generated Labels
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2 使用LLM生成标签对LLM进行微调
- en: The labels obtained from LLMs for specialized tasks like SoE table classification
    can be quite noisy. Thus, for a fine-tuning task to succeed, we need to remove
    potentially incorrect labels from auto-generated labels. For our specific task
    of SoE table classification, we use the consensus in gemini-pro 1.0 model inference
    across dual data representations of tables – JSON and text representations – to
    reduce noise in the training dataset for PaLM-2\. Specifically, we fine-tune PaLM-2
    models on only those LLM labels, where the JSON and text based inferences of the
    tables are identical for the label generating LLM (gemini-pro 1.0 in this case.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLM获取的用于专业任务如SoE表格分类的标签可能会非常嘈杂。因此，为了使微调任务成功，我们需要从自动生成的标签中删除潜在的错误标签。对于我们的SoE表格分类任务，我们使用gemini-pro
    1.0模型在表格的双重数据表示——JSON和文本表示——中的一致性，以减少PaLM-2训练数据集中的噪声。具体来说，我们只对那些JSON和基于文本的表格推断结果相同的LLM标签（在这种情况下为gemini-pro
    1.0）进行PaLM-2模型的微调。
- en: The JSON representation of the table, which represents each of the table columns
    as a dictionary with the key being the row number and the value being the cell
    value for that column, preserves the structural details of the table. In contrast,
    the text representation encompasses not only the contents within the table but
    also all surrounding text on the page, including footnotes, titles, and any other
    textual content. This comprehensive capture of page content provides a fuller
    context and valuable redundancy for our inference process, improving the model’s
    ability to accurately interpret and classify the tables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的JSON表示将每一列表示为一个字典，其中键是行号，值是该列的单元格值，保留了表格的结构细节。相比之下，文本表示不仅包含表格中的内容，还包括页面上的所有周边文本，包括脚注、标题以及其他任何文本内容。这种对页面内容的全面捕捉提供了更完整的上下文和有价值的冗余，有助于我们的推断过程，提升模型准确解读和分类表格的能力。
- en: 'We find that enhancing the quality of the auto-generated dataset for fine-tuning
    PaLM-2 leads to substantial improvement in fine-tuned model performance. The PaLM-2
    model trained with these subset of LLM generated labels outperforms both the baseline
    PaLM-2 and gemini-pro 1.0 on SoE detection task (see table [2](#S4.T2 "Table 2
    ‣ 4.1 Baselines ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data May Reduce
    Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection")).
    Remarkably, the fine-tuned PaLM-2 model achieves performance levels close to those
    obtained with human-annotated labels, showcasing the effectiveness of using LLM-generated
    labels for domain-specific tasks, particularly in settings where expert annotations
    are sparse.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，通过提高用于微调PaLM-2的自动生成数据集的质量，可以显著改善微调模型的性能。使用这些LLM生成标签子集训练的PaLM-2模型在SoE检测任务上优于基线PaLM-2和gemini-pro
    1.0（参见表格[2](#S4.T2 "表格 2 ‣ 4.1 基线 ‣ 4 结果 ‣ 基于LLM标注数据的选择性微调可能减少对人工标注的依赖：以事件安排表检测为例")）。值得注意的是，微调后的PaLM-2模型达到了接近使用人工标注标签所获得的性能水平，展示了在专家标注稀缺的设置中使用LLM生成标签进行领域特定任务的有效性。
- en: The success of our approach on SoE table classification, a highly specialized
    and narrow task, underscores the broader potential of LLMs in natural language
    understanding. It also highlights the potential impact these models can have on
    streamlining and enhancing manual processes in complex domains such as clinical
    trials. Finally, our work shows that auto-generated labels can be effective for
    fine-tuning LLMs for highly specialized applications. This can offer a scalable
    and cost-effective alternative to traditional annotation methods relying on human
    annotators in specialized domain like healthcare.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在SoE表格分类这个高度专业化且狭窄的任务上的成功，突显了LLM在自然语言理解中的广泛潜力。这也强调了这些模型在简化和提升复杂领域（如临床试验）中的手动流程方面可能产生的影响。最后，我们的工作表明，自动生成的标签可以有效地用于微调LLM，以应对高度专业化的应用。这可以为依赖人工标注者的专业领域（如医疗保健）提供一种可扩展且具有成本效益的替代方案。
- en: Summary of Contributions
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贡献总结
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present an innovative fine-tuning approach for Large Language Models (LLMs),
    utilizing noisy labels from another LLM for the task of table classification.
    This method incorporates a strategic label filtering mechanism—selecting labels
    for fine-tuning only when there is an agreement between dual data representations
    of tables—which leads to significant performance improvements compared to both
    the base model and the label-generating LLM and closely approaches the performance
    of a model fine-tuned with human annotations.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种创新的微调方法，用于大型语言模型（LLMs），利用另一个LLM生成的噪声标签进行表格分类任务。该方法包括一种策略性标签筛选机制——仅在表格的双重数据表示之间达成一致时，才选择标签进行微调——这导致了相较于基础模型和标签生成LLM的显著性能提升，并接近于用人工标注微调模型的性能。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our work underscores the adaptability and robust capabilities of LLMs in processing
    and interpreting complex, domain-specific documents, reinforcing their role as
    a valuable tool in automating and improving manual and error-prone workflows in
    specialized domains of healthcare like clinical trials.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的工作强调了大语言模型（LLMs）在处理和解释复杂的领域特定文档中的适应性和强大能力，进一步巩固了它们在自动化和改进医疗领域（如临床试验）中手动且易出错的工作流程中的重要工具角色。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose that LLMs can be a viable option for generating labels for fine-tuning
    other LLMs in specialized domains like healthcare where expert annotations are
    often scarce or prohibitively expensive. Auto-generation of labels can provide
    a scalable and economically efficient alternative to conventional human-based
    annotation in certain scenarios, paving the way for broader adoption and application
    of LLMs in data-rich, expertise-driven fields like healthcare. We also discuss
    broader implications and ethical considerations of generating labels using LLMs.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出LLMs可以成为在医疗等专业领域生成标签以微调其他LLMs的可行选项，在这些领域，专家标注通常稀缺或成本高昂。标签的自动生成可以在某些情况下提供一种可扩展且经济高效的替代传统人工标注的方式，为LLMs在数据丰富、专家驱动的领域（如医疗）中的更广泛应用铺平道路。我们还讨论了使用LLMs生成标签的更广泛影响和伦理考量。
- en: 2 Related Work
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Large Language Models in Healthcare
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 医疗领域的大语言模型
- en: Recent advances in natural language processing (NLP) and machine learning have
    significantly enhanced the potential for integrating these technologies into various
    aspects of healthcare, including clinical decision-making, patient care, drug
    discovery, and medical information management. A wealth of studies have underscored
    the capabilities of Large Language Models (LLMs) in performing crucial NLP tasks
    in healthcare and medicine, such as extracting medical information, summarizing
    patient information, facilitating automated diagnosis, and even passing board
    certification exams in specialty medicines (Liu et al. ([2021](#bib.bib13)); Shay
    et al. ([2024](#bib.bib24)); Van Veen et al. ([2023](#bib.bib29)); Ingraham et al.
    ([2023](#bib.bib10)); Tu et al. ([2024b](#bib.bib27), [a](#bib.bib26))). These
    applications highlight the potentially transformative impact LLMs could have on
    healthcare.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在自然语言处理（NLP）和机器学习领域的进展显著提升了将这些技术融入医疗领域各个方面的潜力，包括临床决策、患者护理、药物发现和医学信息管理。大量研究强调了大型语言模型（LLMs）在医疗和医学中执行关键NLP任务的能力，例如提取医学信息、总结患者信息、促进自动化诊断，甚至通过专业医学的认证考试（Liu
    et al. ([2021](#bib.bib13)); Shay et al. ([2024](#bib.bib24)); Van Veen et al.
    ([2023](#bib.bib29)); Ingraham et al. ([2023](#bib.bib10)); Tu et al. ([2024b](#bib.bib27),
    [a](#bib.bib26)))。这些应用突显了LLMs可能对医疗产生的变革性影响。
- en: In the context of clinical trials, LLMs have been utilized to parse and understand
    interventions and findings from randomized control trials (Wadhwa et al. ([2023](#bib.bib31))),
    and to assist in patient-matching for clinical trials by analyzing electronic
    health records (EHRs) alongside clinical trial documentation (Yuan et al. ([2023](#bib.bib33))).
    Previous research has also studied the problem of automated identification of
    specific elements from Schedule-of-Event (SoE) tables, such as detailed activity
    information, employing a human-in-the-loop approach to ensure accuracy and relevance
    (Dhuliawala et al. ([2018](#bib.bib5))).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在临床试验的背景下，LLMs已被用于解析和理解随机对照试验中的干预措施和发现（Wadhwa et al. ([2023](#bib.bib31)))，并通过分析电子健康记录（EHRs）与临床试验文档来辅助临床试验的患者匹配（Yuan
    et al. ([2023](#bib.bib33)))。先前的研究还研究了从事件日程表（SoE）中自动识别特定元素（如详细的活动信息）的问题，采用了人机结合的方法以确保准确性和相关性（Dhuliawala
    et al. ([2018](#bib.bib5)))。
- en: 'These emerging applications not only underscore the versatility of LLMs in
    managing diverse and complex healthcare datasets but also illustrate a pivotal
    challenge: the dependency on extensive, expert-annotated datasets for fine-tuning
    and evaluating LLMs in specialized tasks. This has led to a growing interest in
    automated label generation techniques and the exploration of fine-tuning and testing
    of LLMs with these synthetic labels.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新兴应用不仅强调了LLMs在管理多样且复杂的医疗保健数据集中的多功能性，还展示了一个关键挑战：在专业任务中对大量专家标注数据集的依赖。这导致了对自动标签生成技术的兴趣日益增加，并探索了使用这些合成标签对LLMs进行微调和测试的方法。
- en: 2.2 Model Training on Synthetic Dataset
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 在合成数据集上的模型训练
- en: Successes of generative models in various tasks have spurred research into leveraging
    these models to augment real data for model fine-tuning and validation. Besnier
    et al. ([2019](#bib.bib2)), for example, use class-conditional GAN generated image
    for training model for image classification tasks. He et al. ([2023](#bib.bib8))
    study the potential of synthetic data in zero-shot and few-shot classification
    using CLIP model (Radford et al. ([2021](#bib.bib20)). Recently, researchers have
    also used LLMs to augment data for various classification tasks. Meng et al. ([2022](#bib.bib15))
    use a pre-trained language model to generate samples by prompting it with real
    data and using the generated data for fine-tuning a BERT model. To control the
    quality of samples, they use log-probability of generated samples for filtering
    poor quality auto-generated samples. Yoo et al. ([2021](#bib.bib32)) use randomly
    sampled existing data samples to condition models to generate new samples, while
    using token probability corresponding to the label-classes to obtain soft probability
    for these generated samples. These soft probabilities for synthetic samples are
    used to train BERT-style models for classification. One of the recent studies
    by Li et al. ([2023](#bib.bib12)) has tried to understand when synthetic data
    can be helpful in successful model training. They find that synthetic data is
    less effective when a classification task is subjective or when a specific instance
    of data to be classified is subjective as measured by agreement amongst annotators.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在各种任务中的成功促使研究人员探索利用这些模型来增强真实数据，从而进行模型微调和验证。例如，Besnier等人（[2019](#bib.bib2)）使用条件GAN生成的图像来训练图像分类任务的模型。He等人（[2023](#bib.bib8)）研究了使用CLIP模型（Radford等人（[2021](#bib.bib20)））在零-shot和少-shot分类中的合成数据潜力。最近，研究人员还使用LLMs来增强各种分类任务的数据。Meng等人（[2022](#bib.bib15)）使用预训练语言模型，通过用真实数据提示它来生成样本，并利用生成的数据来微调BERT模型。为了控制样本的质量，他们使用生成样本的对数概率来过滤低质量的自动生成样本。Yoo等人（[2021](#bib.bib32)）使用随机抽取的现有数据样本来条件模型生成新样本，同时使用与标签类别对应的token概率来获得这些生成样本的软概率。这些合成样本的软概率被用来训练BERT风格的分类模型。Li等人（[2023](#bib.bib12)）的一项最新研究尝试了解合成数据在成功模型训练中的帮助。他们发现，当分类任务主观性较强或被分类的数据实例具有主观性时，合成数据的效果较差，这种主观性是通过标注者之间的一致性来衡量的。
- en: Building on these previous research work, our study employs LLM generated labels
    for fine-tuning another LLM for table classification task in a highly specialized
    context, specifically, Schedule-of-Event table classification in clinical trial
    protocols. Distinct from previous research, which often relies on standard benchmarks
    or datasets for generating synthetic data, our work showcases a novel application
    of synthetic labels for fine-tuning in domains where expert annotation is expensive
    and challenging to obtain. Furthermore, we offer a detailed comparison between
    models fine-tuned on LLM-generated data versus those fine-tuned on data annotated
    by human experts. Our approach also introduces an innovative label filtering mechanism
    that utilizes dual data representations of tables for removing potentially noisy
    synthetic labels. Finally, our approach doesn’t require access to logits for tokens
    and can be applied even when working with LLMs through black-box API access.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些先前的研究工作，我们的研究利用LLM生成的标签对另一个LLM进行微调，以完成在高度专业化背景下的表格分类任务，特别是在临床试验协议中的事件安排表分类。不同于之前的研究，后者通常依赖标准基准或数据集来生成合成数据，我们的工作展示了一种在专家注释昂贵且难以获得的领域中，合成标签用于微调的创新应用。此外，我们详细比较了基于LLM生成的数据微调的模型与基于人工专家注释的数据微调的模型之间的差异。我们的方法还引入了一种创新的标签过滤机制，利用表格的双重数据表示来去除潜在的噪声合成标签。最后，我们的方法不需要访问token的logits，即使在通过黑箱API访问LLMs时也可以应用。
- en: 3 Methods
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Problem Set-up
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题设置
- en: We frame the Schedule-of-Event table detection in a clinical protocol as a binary
    classification task of correctly classifying a table as a SoE table or a non-SoE
    table. Specifically, for each clinical trial protocol, the goal is to classify
    all the tables present inside that protocol as SoE or non-SoE table. We define
    a table as a SoE table when our in-house protocol digitization specialists label
    it as a SoE table. The goal is to achieve a very high level of precision and recall
    on table classification task. Since we don’t expect the model to be perfect in
    classification of the table, the classification algorithm is supposed to be used
    for reducing the annotator’s burden of going through every page in a long protocol.
    All protocols digitized through this semi-automated approach with human-in-the-loop
    goes through stringent review and quality checks to ensure accuracy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将临床协议中的事件安排表检测框架定义为将表格正确分类为事件安排表或非事件安排表的二分类任务。具体来说，对于每个临床试验协议，目标是将该协议中存在的所有表格分类为事件安排表或非事件安排表。当我们的内部协议数字化专家将表格标记为事件安排表时，我们定义该表格为事件安排表。目标是实现表格分类任务的极高精度和召回率。由于我们不期望模型在表格分类上做到完美，分类算法旨在减少注释员在长协议中逐页检查的负担。通过这种半自动化方法与人工环节处理的所有协议都经过严格审查和质量检查，以确保准确性。
- en: 3.2 Dataset & Models
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据集与模型
- en: 3.2.1 Training and Test Set
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 训练集与测试集
- en: Our data set consists of a total of 499 clinical trial protocols of which 91
    are expert-labeled by a team of five protocol digitization specialists and are
    used as the test set in our experiments. These 91 test protocols have a total
    of 3019 tables with 411 SoE tables (13.6%) and 2608 non-SoE tables (86.4%.) These
    expert-digitizers are specifically trained to manually label and digitize the
    clinical protocol for our in-house clinical trial management system (CTMS) software
    and the labeling process and digitization requires significant domain knowledge,
    time and effort. We take the expert annotations as ground truth for all experiments.
    The subset of 408 protocols that don’t have any expert-labels are used for the
    fine-tuning tasks. Of the 408 protocols, we randomly select 300 as training set,
    18 as validation set and 90 as test set for model fine-tuning. These 499 protocols
    in our experiments span a diverse set of clinical trials across pharmaceutical
    companies, academic organizations, hospitals, and government organizations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集总共包含499个临床试验协议，其中91个由五名协议数字化专家专家标注，并作为实验中的测试集。这91个测试协议总共有3019个表格，其中411个是事件安排表（13.6%），2608个是非事件安排表（86.4%）。这些专家数字化人员经过专门培训，手动标注和数字化临床协议，以用于我们的内部临床试验管理系统（CTMS）软件，标注和数字化过程需要大量的领域知识、时间和精力。我们将专家注释作为所有实验的真实数据。未标注的408个协议子集用于微调任务。在408个协议中，我们随机选择300个作为训练集，18个作为验证集，90个作为模型微调的测试集。这499个协议涵盖了来自制药公司、学术组织、医院和政府组织的各种临床试验。
- en: 3.2.2 Models
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 模型
- en: We use GPT-4 API (gpt-4-0613) (OpenAI ([2024](#bib.bib16))), PaLM-2 (text-bison@001
    on GCP) (Anil et al. ([2023](#bib.bib1))), and gemini-pro 1.0 (Google ([2024](#bib.bib7)))
    for our inference tasks. The base models (without any fine-tuning) serve as the
    baselines. We use the PaLM-2 model for all the fine-tuning experiments. We note
    that gemini-pro 1.0 and GPT-4 models have been reported as having substantially
    better performance on LLM benchmarks than PaLM-2 (Google ([2024](#bib.bib7))).
    The gemini-pro 1.0 model is not available for fine-tuning as of this writing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GPT-4 API（gpt-4-0613）（OpenAI（[2024](#bib.bib16)））、PaLM-2（text-bison@001在GCP上）（Anil
    et al.（[2023](#bib.bib1)））和gemini-pro 1.0（Google（[2024](#bib.bib7)））来进行推断任务。基准模型（没有任何微调）作为基线。我们使用PaLM-2模型进行所有的微调实验。我们注意到，gemini-pro
    1.0和GPT-4模型在LLM基准测试中表现明显优于PaLM-2（Google（[2024](#bib.bib7)））。截至目前，gemini-pro 1.0模型尚不可用于微调。
- en: 3.3 Selective Human and LLM Annotation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 选择性人类与LLM注释
- en: For our fine-tuning task, we selectively collect human and gemini-pro 1.0 annotations
    on previously mentioned 408 protocols. The annotation is done by a team of six
    non-experts annotators and they can mark complex cases for review by the expert
    annotators as well as directly ask about any specific annotation from an expert.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的微调任务，我们选择性地收集了之前提到的408个协议上的人类和gemini-pro 1.0注释。注释由一个六人非专家团队完成，他们可以标记复杂情况以供专家注释者审查，也可以直接向专家询问任何特定注释。
- en: We first do inference with the PaLM-2 model. On a subset of 60 protocols, we
    manually go through PaLM-2 model prediction with the help of experts to find specific
    patterns in incorrect model prediction. We find that the base PaLM-2 has a very
    high recall but also a very high false positive rate and often predicts trivial
    cases of non-SoE tables as SoE. Thus, we only obtain human and gemini-pro 1.0
    annotations on tables identified as SoE by the base PaLM-2 model (around 25% of
    all tables.) This selective annotation allows us to keep the size of annotation
    tasks manageable (by reducing the task to one-fourth), while also helps us over-sample
    the SoE table examples for fine-tuning. Additionally, this approach allows annotators
    to concentrate their efforts on more ambiguous cases potentially leading to higher
    quality annotations since annotators can spend more time on each item.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用PaLM-2模型进行推断。在60个协议的子集中，我们在专家的帮助下手动查看PaLM-2模型的预测，以找出模型预测不正确的特定模式。我们发现基准PaLM-2具有非常高的召回率，但也有非常高的假阳性率，并且经常将非SoE表的简单情况预测为SoE。因此，我们仅对基准PaLM-2模型识别为SoE的表（约占所有表的25%）进行人类和gemini-pro
    1.0注释。这种选择性注释使我们能够将注释任务的规模保持在可控范围内（减少到四分之一），同时帮助我们对SoE表示例进行过采样以进行微调。此外，这种方法还允许注释者将精力集中在更模糊的案例上，从而提高注释质量，因为注释者可以花更多时间在每个项目上。
- en: 'We summarize the results of non-expert and gemini-pro 1.0 based annotations
    in table [1](#S3.T1 "Table 1 ‣ 3.3 Selective Human and LLM Annotation ‣ 3 Methods
    ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation:
    A Case Study Using Schedule-of-Event Table Detection"). The train, validation,
    and test set for fine-tuning consist of 300, 18, and 90 protocols respectively.
    The number of SoE and non-SoE table annotation counts for non-expert and gemini-pro
    1.0 annotations differ (since neither the non-expert human annotators nor gemini-pro
    1.0 are perfect at identifying SoE tables and they may annotate a specific table
    differently), but total table counts are the same across various data splits.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[1](#S3.T1 "表 1 ‣ 3.3 选择性人类与LLM注释 ‣ 3 方法 ‣ 基于LLM标注数据的选择性微调可能减少对人工注释的依赖：以事件表检测为例")中总结了非专家和gemini-pro
    1.0基础注释的结果。微调的训练集、验证集和测试集分别包含300、18和90个协议。非专家和gemini-pro 1.0注释的SoE和非SoE表的数量不同（由于非专家注释者和gemini-pro
    1.0在识别SoE表方面都不完美，且他们可能对特定表有不同的标注），但各种数据划分中的总表数量是相同的。
- en: 'Table 1: Summary of Annotations'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：注释总结
- en: '| Annotation Type | Train Set | Validation Set | Test Set |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 注释类型 | 训练集 | 验证集 | 测试集 |'
- en: '| Non-Expert | 1536 SoE, | 53 SoE, | 383 SoE, |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 非专家 | 1536 SoE, | 53 SoE, | 383 SoE, |'
- en: '|  | 1264 Non-SoE | 74 Non-SoE | 413 Non-SoE |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 1264 非SoE | 74 非SoE | 413 非SoE |'
- en: '| Gemini-pro | 1748 SoE, | 64 SoE, | 490 SoE, |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-pro | 1748 SoE, | 64 SoE, | 490 SoE, |'
- en: '|  | 1052 Non-SoE | 63 Non-SoE | 306 Non-SoE |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | 1052 非SoE | 63 非SoE | 306 非SoE |'
- en: We emphasize that we do not use expert annotators directly for labeling tasks.
    However, the annotators do have some previous experience with annotation for SoE
    tables and they also have access to expert annotators for any annotation they
    need help with. Additionally, they can choose to not annotate a table and leave
    it as “Do not know”. These are later annotated by an expert. Despite access to
    experts, non-expert annotations can be noisy due to variation in skills among
    the non-expert annotators. On random overlapping sets of 50 annotation, the average
    inter-rater agreement among non-expert annotators is 81.2%. We note that all annotations
    are collected only on tables predicted as SoE by base PaLM-2 models as described
    previously. Thus, the inter-rate agreement is only on a portion of all tables
    present in the protocols that are annotated by human experts.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调我们并不直接使用专家注释员进行标注任务。然而，注释员确实有一些之前的SoE表格标注经验，并且他们也可以向专家注释员寻求帮助。此外，他们可以选择不标注某个表格，而将其标记为“未知”。这些表格稍后由专家进行标注。尽管有专家的支持，但由于非专家注释员技能差异，非专家的标注可能会有噪声。在50个随机重叠的标注集合中，非专家注释员的平均一致性为81.2%。我们注意到，所有标注都是基于前述的基础PaLM-2模型预测为SoE的表格。因此，一致性只涉及由人工专家标注的协议中部分表格。
- en: 3.4 Experiments
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 实验
- en: 'We use PaLM-2, gemini-pro 1.0 and GPT-4 (gpt-4-0613) in our experiments. As
    described in section [1.2.2](#S1.SS2.SSS2 "1.2.2 Fine-Tuning LLM with LLM-Generated
    Labels ‣ 1.2 Improving Domain-Specific LLM Performance with Synthetic Labels ‣
    1 Introduction ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance
    on Human Annotation: A Case Study Using Schedule-of-Event Table Detection"), we
    use JSON and text representations of a table for inference. We use camelot (0.11.0)
    (Camelot Developers ([2023](#bib.bib3))) for extracting JSON representations and
    pdfminer.six (20231228) (pdfminer.six Developers ([2023](#bib.bib17))) for text
    extraction. The model is asked to respond with “YES” or “NO” corresponding to
    the prompts (see Appendix [B](#A2 "Appendix B Prompts ‣ Selective Fine-tuning
    on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using
    Schedule-of-Event Table Detection") for the prompts) for these JSON and text representations
    of the table. If either of the JSON or text inference output corresponding to
    a table is “YES”, we classify the table as SoE. This conservative approach for
    classification of SoE leads to higher false positives, but those are more easier
    to rectify in our digitization workflow than missed SoE tables, which can lead
    to missed plan-of-care and cause expensive manual corrections at later steps in
    the protocol digitization process.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在实验中使用了PaLM-2、gemini-pro 1.0和GPT-4（gpt-4-0613）。如[1.2.2](#S1.SS2.SSS2 "1.2.2
    Fine-Tuning LLM with LLM-Generated Labels ‣ 1.2 Improving Domain-Specific LLM
    Performance with Synthetic Labels ‣ 1 Introduction ‣ Selective Fine-tuning on
    LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection")节所述，我们使用表格的JSON和文本表示进行推理。我们使用camelot (0.11.0) (Camelot Developers
    ([2023](#bib.bib3)))提取JSON表示，并使用pdfminer.six (20231228) (pdfminer.six Developers
    ([2023](#bib.bib17)))进行文本提取。模型会根据提示（请参见附录[B](#A2 "Appendix B Prompts ‣ Selective
    Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case
    Study Using Schedule-of-Event Table Detection")中的提示）回应“YES”或“NO”。如果表格的JSON或文本推理输出为“YES”，我们将该表格分类为SoE。这种保守的SoE分类方法导致更高的假阳性，但这些假阳性在我们的数字化工作流程中更容易纠正，而错过的SoE表格可能会导致漏掉护理计划，并在后续的协议数字化过程中造成昂贵的人工修正。'
- en: 'For fine-tuning the PaLM-2 model, we use the 408 protocols as previously described
    in section [3.2.1](#S3.SS2.SSS1 "3.2.1 Training and Test Set ‣ 3.2 Dataset & Models
    ‣ 3 Methods ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on
    Human Annotation: A Case Study Using Schedule-of-Event Table Detection"). We fine-tune
    all PaLM-2 models on table annotations obtained from 300 protocols and use 18
    protocols for validation and the rest 90 protocols as test set. We fine-tune two
    sets of models with gemini-pro 1.0 generated annotations and one model with human
    annotations. The first gemini-pro based fine-tuned model uses all 2800 table annotations
    from 300 protocols, while the second removes all “noisy labels” from fine-tuning.
    Specifically, for the second fine-tuning experiment with gemini-pro annotations,
    we remove all samples from training where the JSON and text annotations of gemini-pro
    are not identical. This reduces the set of table annotations to 2512 tables in
    the training set. All models are fine-tuned for 300 epochs with learning rate
    multiplier of 1, early stopping set to True, and an evaluation interval of 10
    epochs with Google Cloud Vertex AI fine-tuning pipeline which uses Parameter Efficient
    Fine-tuning (PEFT). We track the model training through a tensorboard instance.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调 PaLM-2 模型，我们使用了第 [3.2.1](#S3.SS2.SSS1 "3.2.1 训练和测试集 ‣ 3.2 数据集与模型 ‣ 3 方法
    ‣ 基于 LLM 标记数据的选择性微调可能减少对人工标注的依赖：以事件表检测为例") 节中描述的 408 个协议。我们在从 300 个协议中获得的表格注释上对所有
    PaLM-2 模型进行了微调，并使用了 18 个协议作为验证集，其余 90 个协议作为测试集。我们使用 gemini-pro 1.0 生成的注释微调了两组模型，另外一组模型使用了人工注释。第一组基于
    gemini-pro 的微调模型使用了来自 300 个协议的所有 2800 个表格注释，而第二组则从微调中删除了所有“噪声标签”。具体而言，对于第二次使用
    gemini-pro 注释的微调实验，我们删除了所有 JSON 和文本注释不一致的训练样本。这将训练集中的表格注释集减少到 2512 张表格。所有模型都在学习率乘数为
    1、早停设置为 True 的条件下进行了 300 个 epoch 的微调，并使用了 Google Cloud Vertex AI 微调管道，该管道采用了参数高效微调（PEFT）。我们通过
    tensorboard 实例跟踪模型训练情况。
- en: 4 Results
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: 'We evaluate models on a comprehensive set of metrics to assess the effectiveness
    of each model in the context of clinical trial protocol digitization. The models
    are benchmarked based on recall, precision, F-1 score, and accuracy. We additionally
    measure model performance at various precision threshold as well as on the percentage
    of protocols achieving 100% recall and precision (refer appendix [D](#A4 "Appendix
    D Additional Recall and Precision Metrics ‣ Selective Fine-tuning on LLM-labeled
    Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection")), which are critical for the practical deployment of the automated
    digitization pipeline.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一系列全面的指标来评估模型在临床试验协议数字化背景下的有效性。这些模型基于召回率、精确率、F-1 分数和准确率进行基准测试。我们还测量模型在不同精确度阈值下的表现以及协议达到
    100% 召回率和精确率的百分比（参见附录 [D](#A4 "附录 D 附加召回率和精确度指标 ‣ 基于 LLM 标记数据的选择性微调可能减少对人工标注的依赖：以事件表检测为例")），这些对于自动化数字化流程的实际部署至关重要。
- en: 4.1 Baselines
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基线
- en: 'We start with a very simple baseline of non-finetuned models–gemini-pro 1.0,
    GPT-4, and PaLM-2\. We have summarized the results in Table [2](#S4.T2 "Table
    2 ‣ 4.1 Baselines ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data May
    Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table
    Detection"). We notice that all baseline models achieve a very high recall. Among
    the models that are not fine-tuned, the inference with GPT-4 results in best performance
    with a precision of 78.2%, f1-score of 0.84 and an accuracy of 94.0%. The inference
    with the PaLM-2 base model achieves 59.8% precision, 0.71 f1-score and 87.6% accuracy.
    The inference with gemini-pro 1.0 results in a performance between PaLM-2 and
    GPT-4 with 65.7% precision, 0.76 f1-score and 90.0% accuracy. In addition to these
    baselines, we also use baselines with naive combinations of gemini-pro 1.0 and
    PaLM-2 prediction (see Appendix [C](#A3 "Appendix C Naive Combination of Gemini-pro
    and PaLM-2 models ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance
    on Human Annotation: A Case Study Using Schedule-of-Event Table Detection") for
    details.)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从非常简单的基线模型开始——gemini-pro 1.0、GPT-4 和 PaLM-2。我们在表格[2](#S4.T2 "Table 2 ‣ 4.1
    Baselines ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance
    on Human Annotation: A Case Study Using Schedule-of-Event Table Detection")中总结了结果。我们注意到所有基线模型都取得了很高的召回率。在未经过微调的模型中，GPT-4的推断表现最佳，精确度为78.2%，f1-score为0.84，准确率为94.0%。PaLM-2基础模型的推断结果为59.8%的精确度，0.71的f1-score和87.6%的准确率。Gemini-pro
    1.0的推断结果介于PaLM-2和GPT-4之间，精确度为65.7%，f1-score为0.76，准确率为90.0%。除了这些基线之外，我们还使用了naive的gemini-pro
    1.0和PaLM-2预测的组合基线（详见附录[C](#A3 "Appendix C Naive Combination of Gemini-pro and
    PaLM-2 models ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance
    on Human Annotation: A Case Study Using Schedule-of-Event Table Detection")）。'
- en: 'Table 2: Average Recall, Precision, F-1 Score, and Accuracy on the test set'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：测试集上的平均召回率、精确度、F-1分数和准确率
- en: '| Model | Recall | Precision | F-1 Score | Accuracy |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 召回率 | 精确度 | F-1 分数 | 准确率 |'
- en: '| PaLM-2 | 97.2% | 59.8% | 0.71 | 87.6% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PaLM-2 | 97.2% | 59.8% | 0.71 | 87.6% |'
- en: '| GPT-4 (gpt-4-0613) | 98.6% | 78.2% | 0.84 | 94.0% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (gpt-4-0613) | 98.6% | 78.2% | 0.84 | 94.0% |'
- en: '| Gemini Pro 1.0 | 99.5% | 65.7% | 0.76 | 90.0% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro 1.0 | 99.5% | 65.7% | 0.76 | 90.0% |'
- en: '| Fine-tuned PaLM-2 | 98.9% | 87.3% | 0.91 | 96.0% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 微调后的 PaLM-2 | 98.9% | 87.3% | 0.91 | 96.0% |'
- en: '| (Using Human Labels) |  |  |  |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| (使用人工标签) |  |  |  |  |'
- en: '| Fine-tuned PaLM-2 | 100% | 63.7% | 0.74 | 88.1% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 微调后的 PaLM-2 | 100% | 63.7% | 0.74 | 88.1% |'
- en: '| (Using ALL Gemini Labels) |  |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| (使用所有 Gemini 标签) |  |  |  |  |'
- en: '| Fine-tuned PaLM-2 | 97.7% | 85.9% | 0.89 | 95.7% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 微调后的 PaLM-2 | 97.7% | 85.9% | 0.89 | 95.7% |'
- en: '| (Using Filtered Gemini Labels) |  |  |  |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (使用过滤后的 Gemini 标签) |  |  |  |  |'
- en: 4.2 Fine-tuned Models
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 微调模型
- en: 'We conduct three sets of fine-tuning experiments using PaLM-2 models. We first
    conduct fine-tuning on PaLM-2 model using the non-expert human annotation obtained
    as described in section [3.3](#S3.SS3 "3.3 Selective Human and LLM Annotation
    ‣ 3 Methods ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on
    Human Annotation: A Case Study Using Schedule-of-Event Table Detection"). This
    serves as a strong benchmark for our second and third fine-tuning experiments
    that use gemini-pro 1.0 based annotations for fine-tuning PaLM-2\. The second
    and third finetuning experiments differ in the sense that while one of the experiments
    use entirety of gemini-pro’s labels during fine-tuning, the other experiment is
    fine-tuned on only those gemini-pro labels where there is a consensus between
    Gemini-pro’s inference for JSON and text-based representation of a given table.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用PaLM-2模型进行三组微调实验。我们首先使用第[3.3](#S3.SS3 "3.3 Selective Human and LLM Annotation
    ‣ 3 Methods ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on
    Human Annotation: A Case Study Using Schedule-of-Event Table Detection")节中描述的非专家人工标注对PaLM-2模型进行微调。这为我们的第二组和第三组微调实验提供了一个强有力的基准，这些实验使用基于gemini-pro
    1.0的标注进行PaLM-2微调。第二组和第三组微调实验的不同之处在于，其中一个实验使用了gemini-pro所有的标签进行微调，而另一个实验仅在Gemini-pro对JSON和基于文本的表格表示之间达成共识的gemini-pro标签上进行微调。'
- en: 'As seen in table [2](#S4.T2 "Table 2 ‣ 4.1 Baselines ‣ 4 Results ‣ Selective
    Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case
    Study Using Schedule-of-Event Table Detection"), fine-tuning PaLM-2 with labels
    generated by Gemini-pro 1.0 leads to improvements over the base PaLM-2 model’s
    performance. However, this improvement is nuanced. When the model is fine-tuned
    using the entirety of gemini-pro’s labels, it improves over the baseline PaLM-2
    model. However, the precision, f1-score, and accuracy compared to the standalone
    gemini-pro 1.0 model remains inferior. Optimal fine-tuning is achieved through
    only incorporating labels for which there is a consensus between gemini-pro’s
    JSON and text-based inferences for a table. This fine-tuned variant (table [2](#S4.T2
    "Table 2 ‣ 4.1 Baselines ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data
    May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection") last row), not only exceeds the performance of the base PaLM-2
    and gemini-pro 1.0 models, but also narrows the gap to the precision and f1-score
    of PaLM-2 fine-tuned with human labels, though it doesn’t completely bridge it.
    Furthermore, our fine-tuned models surpass the performance of naive ensemble approaches
    combining gemini-pro 1.0 and PaLM-2, as detailed in Appendix [C](#A3 "Appendix
    C Naive Combination of Gemini-pro and PaLM-2 models ‣ Selective Fine-tuning on
    LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection"). Yet, it is worth noting that the naive ensembles still offer
    better results than either standalone base model. These findings underscore the
    value of naive ensembles for preliminary analysis and for scenarios where fine-tuning
    isn’t feasible, while also highlighting the potentially superior results attainable
    with fine-tuned models.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[2](#S4.T2 "Table 2 ‣ 4.1 Baselines ‣ 4 Results ‣ Selective Fine-tuning
    on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using
    Schedule-of-Event Table Detection")所示，使用Gemini-pro 1.0生成的标签对PaLM-2进行微调，相较于基础PaLM-2模型的性能有所提升。然而，这种提升是微妙的。当模型使用Gemini-pro的全部标签进行微调时，相比基线PaLM-2模型有所改进。但与独立的Gemini-pro
    1.0模型相比，精度、f1分数和准确率仍然较低。最佳的微调效果是通过仅结合Gemini-pro的JSON与基于文本的推理达成一致的标签实现的。这个微调后的变体（表格[2](#S4.T2
    "Table 2 ‣ 4.1 Baselines ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data
    May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection")最后一行），不仅超越了基础PaLM-2和Gemini-pro 1.0模型的性能，还缩小了与使用人工标签微调的PaLM-2模型在精度和f1分数上的差距，尽管没有完全弥合。此外，我们的微调模型超越了将Gemini-pro
    1.0和PaLM-2结合的简单集成方法的表现，详细情况见附录[C](#A3 "Appendix C Naive Combination of Gemini-pro
    and PaLM-2 models ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance
    on Human Annotation: A Case Study Using Schedule-of-Event Table Detection")。然而，值得注意的是，简单的集成方法仍然提供了比单独基础模型更好的结果。这些发现突显了简单集成方法在初步分析和无法进行微调的场景中的价值，同时也强调了微调模型可能获得的更优结果。'
- en: 'We plot the results for precision and f1-score in figure [1](#S4.F1 "Figure
    1 ‣ 4.2 Fine-tuned Models ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data
    May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection") corresponding to models in table [2](#S4.T2 "Table 2 ‣ 4.1 Baselines
    ‣ 4 Results ‣ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on
    Human Annotation: A Case Study Using Schedule-of-Event Table Detection") for all
    91 test protocols. The bubble size in the scatter plots corresponding to the protocols
    are proportional to the number of SoE tables present in that protocol. We also
    overlay the boxplot on the scatter plot to show the the precision and f1-scores
    across individual protocols in the test set. We see that PaLM-2 models fine-tuned
    on human labels as well as consensus-based gemini-pro 1.0 labels achieve a median
    precision of 100% and median f1-score of 1\. This means that for at least 50%
    of the protocols, the SoE table detection step in digitization workflow will be
    processed correctly without needing any further correction.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[1](#S4.F1 "图 1 ‣ 4.2 微调模型 ‣ 4 结果 ‣ 选择性微调LLM标注数据可能减少对人工注释的依赖：以事件时间表检测为例")中绘制了精度和f1分数的结果，对应于表[2](#S4.T2
    "表 2 ‣ 4.1 基线 ‣ 4 结果 ‣ 选择性微调LLM标注数据可能减少对人工注释的依赖：以事件时间表检测为例")中的模型，涵盖了所有91个测试协议。散点图中对应协议的气泡大小与该协议中存在的SoE表的数量成正比。我们还在散点图上叠加了箱形图，以显示测试集中各个协议的精度和f1分数。我们看到，基于人工标签和基于共识的gemini-pro
    1.0标签微调的PaLM-2模型在中位精度上达到100%，中位f1分数为1。这意味着在至少50%的协议中，数字化工作流中的SoE表检测步骤将正确处理，而无需进一步修正。
- en: '![Refer to caption](img/9ed37f69605341c6cc354b26b24582d1.png)![Refer to caption](img/3d61b109db66c14ee82f6b17aacf8326.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ed37f69605341c6cc354b26b24582d1.png)![参见说明](img/3d61b109db66c14ee82f6b17aacf8326.png)'
- en: 'Figure 1: Precision and F1 Score Across Models for 91 protocols in test set.
    Bubble sizes represent the number of SoE tables within a protocol. PaLM-2 models
    fine-tuned with human labels and consensus-based gemini-pro 1.0 labels achieve
    a median precision of 100% and F1 score of 1.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：测试集中的91个协议模型的精度和F1分数。气泡大小表示协议中的SoE表数量。用人工标签和基于共识的gemini-pro 1.0标签微调的PaLM-2模型在中位精度上达到100%，F1分数为1。
- en: 5 Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Our study proposes a novel approach to fine-tuning Large Language Models for
    specialized domains where labels can be especially difficult to obtain. The approach
    of utilizing noisy labels from an LLM (gemini-pro) for fine-tuning another LLM
    (PaLM-2) demonstrates a scalable and cost-effective alternative to conventional
    expert annotation processes. Notably, the introduction of a label filtering mechanism,
    which selects labels for fine-tuning only when there is agreement between dual
    data representations (JSON and text) of a table, leads to substantial improvements
    in model performance. This method not only outperforms the base model and the
    label-generating LLM, it also approaches the performance level of models fine-tuned
    with human annotations on our table classification task, highlighting the potential
    of our fine-tuning strategy to effectively leverage auto-generated labels. While
    we use JSON and text based consensus approach as a proxy for selecting high quality
    training labels for our specific table classification task, any multi-modal data
    representation can serve a similar purpose in filtering out potentially noisy
    labels.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究提出了一种新颖的方法，用于对大语言模型进行专门领域的微调，在这些领域中，标签可能特别难以获取。利用LLM（gemini-pro）生成的噪声标签来微调另一个LLM（PaLM-2）的方法展示了一个可扩展且成本效益高的替代方案，与传统的专家注释过程相比具有优势。特别是，引入了标签过滤机制，该机制仅在表的双重数据表示（JSON和文本）之间达成一致时选择标签进行微调，从而显著提高了模型性能。这种方法不仅优于基础模型和标签生成LLM，还接近于用人工注释微调模型在我们的表格分类任务中的表现，突显了我们微调策略在有效利用自动生成标签方面的潜力。虽然我们使用基于JSON和文本的一致性方法作为选择高质量训练标签的代理，但任何多模态数据表示都可以在过滤潜在噪声标签方面发挥类似作用。
- en: Despite its strengths, our approach has several limitations. The task of SoE
    table detection is highly specialized and relies on data from our internal Clinical
    Trial Management System (CTMS) software, which may not fully capture the variety
    and complexity of clinical trial protocols encountered in broader applications.
    This specificity could limit the generalizability of our findings to other types
    of documents or on different table classification tasks. If LLMs perform poorly
    across the board on a specific task, automated generation of labels may not be
    feasible even with powerful models like gemini-pro. Moreover, while our study
    underscores the feasibility and effectiveness of using LLM-generated labels for
    fine-tuning, it lacks a direct comparison with a baseline model fine-tuned on
    expert annotations due to the high costs and resource requirements associated
    with obtaining such annotations. This comparison could have provided a clearer
    benchmark for evaluating the relative performance of our approach. Importantly,
    the reliance on auto-label generation and consensus-based fine-tuning may introduce
    or perpetuate biases inherent in the models used for label generation. Depending
    on specific context and fine-tuning task, these biases may manifest as demographic,
    entity, or domain-specific biases, affecting the accuracy and fairness of the
    fine-tuned model, particularly in sensitive domains like healthcare. Thus, for
    settings where there is a concern regarding bias and fairness in model generated
    labels and outputs, comprehensive fairness and bias evaluation specific to auto-labeling
    and fine-tuning steps are essential for detecting and mitigating biases in auto-generated
    labels and fine-tuned models. This can potentially mitigate such concerns for
    auto-generated labels and model fine-tuned on such labels paving the way for broader
    adoption of LLMs in data-rich, but expert-scarce domain like healthcare, where
    processes often rely heavily on manual workflows.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的方法有其优点，但仍存在几个局限性。SoE表格检测任务高度专业化，依赖于我们内部临床试验管理系统（CTMS）软件的数据，这可能无法全面捕捉更广泛应用中临床试验协议的多样性和复杂性。这种特定性可能会限制我们发现的泛化能力，尤其是在其他类型的文档或不同表格分类任务中。如果LLMs在特定任务上的表现不佳，即使是像gemini-pro这样的强大模型，自动生成标签也可能不可行。此外，虽然我们的研究强调了使用LLM生成标签进行微调的可行性和有效性，但由于获取专家注释相关的高成本和资源需求，我们未能直接与基线模型进行比较。这种比较本可以提供一个更明确的基准来评估我们方法的相对性能。重要的是，依赖于自动标签生成和基于共识的微调可能会引入或延续生成标签模型中的固有偏见。根据具体的上下文和微调任务，这些偏见可能表现为人口统计、实体或领域特定的偏见，影响微调模型的准确性和公平性，特别是在像医疗保健这样的敏感领域。因此，对于那些关心模型生成标签和输出中的偏见与公平性的设置，针对自动标签生成和微调步骤的全面公平性和偏见评估对于检测和减轻自动生成标签和微调模型中的偏见至关重要。这有可能缓解对自动生成标签的担忧，并推动LLMs在数据丰富但专家稀缺的领域如医疗保健中更广泛的应用，这些领域通常依赖于人工工作流程。
- en: 6 Data & Code Availability
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 数据与代码可用性
- en: Due to the terms of our data sharing agreement, we are unable to provide access
    to the dataset. Additionally, the code used in this study is part of our proprietary
    software and cannot be shared.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们数据共享协议的条款，我们无法提供数据集的访问权限。此外，本研究中使用的代码是我们专有软件的一部分，不能分享。
- en: 7 Author Contribution
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 作者贡献
- en: BK and YJ conceived of utilizing LLM-based labels for fine-tuning. JA, EY, and
    BK conceived of the initial fine-tuning experiments using human labeled data.
    NL came up with the idea and initial prompt to use an LLM for table classification
    task. BK conducted all the experiments and wrote the draft of the paper. All authors
    reviewed the draft and edited the manuscript and take responsibility for all aspects
    of the work.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: BK和YJ提出了利用LLM基础标签进行微调的构想。JA、EY和BK提出了使用人工标记数据的初步微调实验。NL提出了使用LLM进行表格分类任务的想法和初步提示。BK进行了所有实验并撰写了论文草稿。所有作者审阅了草稿并编辑了手稿，对工作中的所有方面负责。
- en: 8 Ethics Declaration
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理声明
- en: 8.1 Competing interests
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 竞争利益
- en: All authors are employees and shareholders of Verily Life Sciences LLC.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有作者都是Verily Life Sciences LLC的员工和股东。
- en: References
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical
    report, 2023.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2023）**Rohan Anil**、**Andrew M. Dai**、**Orhan Firat**、**Melvin Johnson**、**Dmitry
    Lepikhin**、**Alexandre Passos**、**Siamak Shakeri**、**Emanuel Taropa**、**Paige
    Bailey**、**Zhifeng Chen**、**Eric Chu**、**Jonathan H. Clark**、**Laurent El Shafey**、**Yanping
    Huang**、**Kathy Meier-Hellstern**、**Gaurav Mishra**、**Erica Moreira**、**Mark Omernick**、**Kevin
    Robinson**、**Sebastian Ruder**、**Yi Tay**、**Kefan Xiao**、**Yuanzhong Xu**、**Yujing
    Zhang**、**Gustavo Hernandez Abrego**、**Junwhan Ahn**、**Jacob Austin**、**Paul Barham**、**Jan
    Botha**、**James Bradbury**、**Siddhartha Brahma**、**Kevin Brooks**、**Michele Catasta**、**Yong
    Cheng**、**Colin Cherry**、**Christopher A. Choquette-Choo**、**Aakanksha Chowdhery**、**Clément
    Crepy**、**Shachi Dave**、**Mostafa Dehghani**、**Sunipa Dev**、**Jacob Devlin**、**Mark
    Díaz**、**Nan Du**、**Ethan Dyer**、**Vlad Feinberg**、**Fangxiaoyu Feng**、**Vlad
    Fienber**、**Markus Freitag**、**Xavier Garcia**、**Sebastian Gehrmann**、**Lucas
    Gonzalez**、**Guy Gur-Ari**、**Steven Hand**、**Hadi Hashemi**、**Le Hou**、**Joshua
    Howland**、**Andrea Hu**、**Jeffrey Hui**、**Jeremy Hurwitz**、**Michael Isard**、**Abe
    Ittycheriah**、**Matthew Jagielski**、**Wenhao Jia**、**Kathleen Kenealy**、**Maxim
    Krikun**、**Sneha Kudugunta**、**Chang Lan**、**Katherine Lee**、**Benjamin Lee**、**Eric
    Li**、**Music Li**、**Wei Li**、**YaGuang Li**、**Jian Li**、**Hyeontaek Lim**、**Hanzhao
    Lin**、**Zhongtao Liu**、**Frederick Liu**、**Marcello Maggioni**、**Aroma Mahendru**、**Joshua
    Maynez**、**Vedant Misra**、**Maysam Moussalem**、**Zachary Nado**、**John Nham**、**Eric
    Ni**、**Andrew Nystrom**、**Alicia Parrish**、**Marie Pellat**、**Martin Polacek**、**Alex
    Polozov**、**Reiner Pope**、**Siyuan Qiao**、**Emily Reif**、**Bryan Richter**、**Parker
    Riley**、**Alex Castro Ros**、**Aurko Roy**、**Brennan Saeta**、**Rajkumar Samuel**、**Renee
    Shelby**、**Ambrose Slone**、**Daniel Smilkov**、**David R. So**、**Daniel Sohn**、**Simon
    Tokumine**、**Dasha Valter**、**Vijay Vasudevan**、**Kiran Vodrahalli**、**Xuezhi
    Wang**、**Pidong Wang**、**Zirui Wang**、**Tao Wang**、**John Wieting**、**Yuhuai Wu**、**Kelvin
    Xu**、**Yunhan Xu**、**Linting Xue**、**Pengcheng Yin**、**Jiahui Yu**、**Qiao Zhang**、**Steven
    Zheng**、**Ce Zheng**、**Weikang Zhou**、**Denny Zhou**、**Slav Petrov** 和 **Yonghui
    Wu**。Palm 2 技术报告，2023。
- en: 'Besnier et al. (2019) Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu
    Cord, and Patrick Pérez. This dataset does not exist: training models from generated
    images. *CoRR*, abs/1911.02888, 2019. URL [http://arxiv.org/abs/1911.02888](http://arxiv.org/abs/1911.02888).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besnier 等人（2019）**Victor Besnier**、**Himalaya Jain**、**Andrei Bursuc**、**Matthieu
    Cord** 和 **Patrick Pérez**。该数据集不存在：从生成的图像中训练模型。*CoRR*，abs/1911.02888，2019。网址 [http://arxiv.org/abs/1911.02888](http://arxiv.org/abs/1911.02888)。
- en: 'Camelot Developers (2023) Camelot Developers. *Camelot: PDF Table Extraction
    for Humans*, 2023. URL [https://camelot-py.readthedocs.io/](https://camelot-py.readthedocs.io/).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Camelot 开发者（2023）**Camelot Developers**。*Camelot: 人类的 PDF 表格提取*，2023。网址 [https://camelot-py.readthedocs.io/](https://camelot-py.readthedocs.io/)。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code, 2021.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever 和 Wojciech
    Zaremba。评估基于代码训练的大型语言模型，2021年。
- en: Dhuliawala et al. (2018) Murtaza Dhuliawala, Nicholas Fay, Daniel Gruen, and
    Amar Das. What happens when? interpreting schedule of activity tables in clinical
    trial documents. In *Proceedings of the 2018 ACM International Conference on Bioinformatics,
    Computational Biology, and Health Informatics*, pages 301–306, 2018.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala等人（2018）Murtaza Dhuliawala, Nicholas Fay, Daniel Gruen 和 Amar Das。发生了什么？解释临床试验文件中的活动时间表。在*2018年ACM国际生物信息学、计算生物学与健康信息学会议论文集*，第301–306页，2018年。
- en: 'Getz and Campo (2017) Kenneth A Getz and Rafael A Campo. Trial watch: trends
    in clinical trial design complexity. *Nature Reviews Drug Discovery*, 16(5):307–308,
    2017.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Getz和Campo（2017）Kenneth A Getz 和 Rafael A Campo。试验观察：临床试验设计复杂性的趋势。*自然评论：药物发现*，16(5):307–308，2017年。
- en: 'Google (2024) Google. Gemini: A family of highly capable multimodal models,
    2024.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google（2024）Google。Gemini：一系列高能力的多模态模型，2024年。
- en: He et al. (2023) Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang,
    Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models
    ready for image recognition?, 2023.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2023）Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip
    Torr, Song Bai 和 Xiaojuan Qi。生成模型生成的合成数据是否已准备好用于图像识别？2023年。
- en: Inan et al. (2020) Omer T Inan, P Tenaerts, Sheila A Prindiville, HR Reynolds,
    DS Dizon, K Cooper-Arnold, M Turakhia, Mark J Pletcher, Kenzie L Preston, Harlan M
    Krumholz, et al. Digitizing clinical trials. *NPJ digital medicine*, 3(1):101,
    2020.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inan等人（2020）Omer T Inan, P Tenaerts, Sheila A Prindiville, HR Reynolds, DS Dizon,
    K Cooper-Arnold, M Turakhia, Mark J Pletcher, Kenzie L Preston, Harlan M Krumholz
    等。数字化临床试验。*NPJ数字医学*，3(1):101，2020年。
- en: Ingraham et al. (2023) John B Ingraham, Max Baranov, Zak Costello, Karl W Barber,
    Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing,
    Erik R Van Vlack, et al. Illuminating protein space with a programmable generative
    model. *Nature*, 623(7989):1070–1078, 2023.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingraham等人（2023）John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie
    Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik
    R Van Vlack 等。利用可编程生成模型照亮蛋白质空间。*自然*，623(7989):1070–1078，2023年。
- en: Jones et al. (2016) W Schuyler Jones, Matthew T Roe, Elliott M Antman, Mark J
    Pletcher, Robert A Harrington, Russell L Rothman, William J Oetgen, Sunil V Rao,
    Mitchell W Krucoff, Lesley H Curtis, et al. The changing landscape of randomized
    clinical trials in cardiovascular disease. *Journal of the American College of
    Cardiology*, 68(17):1898–1907, 2016.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones等人（2016）W Schuyler Jones, Matthew T Roe, Elliott M Antman, Mark J Pletcher,
    Robert A Harrington, Russell L Rothman, William J Oetgen, Sunil V Rao, Mitchell
    W Krucoff, Lesley H Curtis等。心血管疾病随机临床试验的变化趋势。*美国心脏病学院期刊*，68(17):1898–1907，2016年。
- en: 'Li et al. (2023) Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming Yin. Synthetic
    data generation with large language models for text classification: Potential
    and limitations. *arXiv preprint arXiv:2310.07849*, 2023.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023）Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu 和 Ming Yin。使用大型语言模型生成的合成数据用于文本分类：潜力与局限。*arXiv预印本
    arXiv:2310.07849*，2023年。
- en: Liu et al. (2021) Zhichao Liu, Ruth A Roberts, Madhu Lal-Nag, Xi Chen, Ruili
    Huang, and Weida Tong. Ai-based language models powering drug discovery and development.
    *Drug Discovery Today*, 26(11):2593–2607, 2021.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）**Zhichao Liu**，**Ruth A Roberts**，**Madhu Lal-Nag**，**Xi Chen**，**Ruili
    Huang**，和 **Weida Tong**。基于AI的语言模型驱动药物发现与开发。*药物发现今日*，26(11)：2593–2607，2021。
- en: 'Marquis-Gravel et al. (2019) Guillaume Marquis-Gravel, Matthew T Roe, Mintu P
    Turakhia, William Boden, Robert Temple, Abhinav Sharma, Boaz Hirshberg, Paul Slater,
    Noah Craft, Norman Stockbridge, et al. Technology-enabled clinical trials: transforming
    medical evidence generation. *Circulation*, 140(17):1426–1436, 2019.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marquis-Gravel 等（2019）**Guillaume Marquis-Gravel**，**Matthew T Roe**，**Mintu
    P Turakhia**，**William Boden**，**Robert Temple**，**Abhinav Sharma**，**Boaz Hirshberg**，**Paul
    Slater**，**Noah Craft**，**Norman Stockbridge** 等人。技术驱动的临床试验：转变医学证据生成。*循环*，140(17)：1426–1436，2019。
- en: 'Meng et al. (2022) Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. Generating
    training data with language models: Towards zero-shot language understanding.
    *Advances in Neural Information Processing Systems*, 35:462–477, 2022.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等（2022）**Yu Meng**，**Jiaxin Huang**，**Yu Zhang**，和 **Jiawei Han**。利用语言模型生成训练数据：迈向零样本语言理解。*神经信息处理系统进展*，35：462–477，2022。
- en: OpenAI (2024) OpenAI. Gpt-4 technical report, 2024.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI。GPT-4 技术报告，2024。
- en: 'pdfminer.six Developers (2023) pdfminer.six Developers. *pdfminer.six: PDF
    parser and analyzer*, 2023. URL [https://github.com/pdfminer/pdfminer.six](https://github.com/pdfminer/pdfminer.six).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pdfminer.six 开发者（2023）pdfminer.six 开发者。*pdfminer.six：PDF 解析器和分析器*，2023。网址 [https://github.com/pdfminer/pdfminer.six](https://github.com/pdfminer/pdfminer.six)。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）**Alec Radford**，**Karthik Narasimhan**，**Tim Salimans**，**Ilya
    Sutskever** 等人。通过生成性预训练提高语言理解。2018。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）**Alec Radford**，**Jeffrey Wu**，**Rewon Child**，**David Luan**，**Dario
    Amodei**，**Ilya Sutskever** 等人。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8)：9，2019。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2021）**Alec Radford**, **Jong Wook Kim**, **Chris Hallacy**, **Aditya
    Ramesh**, **Gabriel Goh**, **Sandhini Agarwal**, **Girish Sastry**, **Amanda Askell**,
    **Pamela Mishkin**, **Jack Clark** 等人。通过自然语言监督学习可迁移的视觉模型。发表于*国际机器学习会议*，第8748–8763页。PMLR，2021。
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation, 2021.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等（2021）**Aditya Ramesh**，**Mikhail Pavlov**，**Gabriel Goh**，**Scott Gray**，**Chelsea
    Voss**，**Alec Radford**，**Mark Chen**，和 **Ilya Sutskever**。零样本文本到图像生成，2021。
- en: 'Rosa et al. (2021) Carmen Rosa, Lisa A Marsch, Erin L Winstanley, Meg Brunner,
    and Aimee NC Campbell. Using digital technologies in clinical trials: Current
    and future applications. *Contemporary clinical trials*, 100:106219, 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosa 等（2021）**Carmen Rosa**，**Lisa A Marsch**，**Erin L Winstanley**，**Meg Brunner**，和
    **Aimee NC Campbell**。在临床试验中使用数字技术：当前和未来的应用。*当代临床试验*，100：106219，2021。
- en: 'Sharma et al. (2024) Puneet Sharma, Guangze Luo, Cindy Wang, Dara Brodsky,
    Camilia R Martin, Andrew Beam, and Kristyn Beam. Assessment of the clinical knowledge
    of chatgpt-4 in neonatal-perinatal medicine: a comparative analysis with chatgpt-3.5.
    *Journal of Perinatology*, pages 1–2, 2024.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等（2024）**Puneet Sharma**，**Guangze Luo**，**Cindy Wang**，**Dara Brodsky**，**Camilia
    R Martin**，**Andrew Beam**，和 **Kristyn Beam**。ChatGPT-4 在新生儿-围产医学中的临床知识评估：与 ChatGPT-3.5
    的比较分析。*围产学杂志*，第1–2页，2024。
- en: Shay et al. (2024) Denys Shay, Bhawesh Kumar, Simone Redaelli, Dario von Wedel,
    Manqing Liu, Mark Dershwitz, Maximilian S Schaefer, and Andrew Beam. Could chatgpt-4
    pass an anaesthesiology board examination? follow-up assessment of a comprehensive
    set of board examination practice questions. *British Journal of Anaesthesia*,
    132(1):172–174, 2024.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shay 等（2024）**Denys Shay**，**Bhawesh Kumar**，**Simone Redaelli**，**Dario von
    Wedel**，**Manqing Liu**，**Mark Dershwitz**，**Maximilian S Schaefer**，和 **Andrew
    Beam**。ChatGPT-4 能否通过麻醉学委员会考试？对一套全面的考试练习题的跟进评估。*英国麻醉学杂志*，132(1)：172–174，2024。
- en: Singhal et al. (2023) Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi,
    Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen
    Pfohl, et al. Large language models encode clinical knowledge. *Nature*, 620(7972):172–180,
    2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等（2023）**Karan Singhal**，**Shekoofeh Azizi**，**Tao Tu**，**S Sara Mahdavi**，**Jason
    Wei**，**Hyung Won Chung**，**Nathan Scales**，**Ajay Tanwani**，**Heather Cole-Lewis**，**Stephen
    Pfohl** 等人。大型语言模型编码临床知识。*自然*，620(7972)：172–180，2023。
- en: Tu et al. (2024a) Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann,
    Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira
    Ktena, Anil Palepu, Basil Mustafa, Aakanksha Chowdhery, Yun Liu, Simon Kornblith,
    David Fleet, Philip Mansfield, Sushant Prakash, Renee Wong, Sunny Virmani, Christopher
    Semturs, S. Sara Mahdavi, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas,
    Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Karan Singhal, Pete
    Florence, Alan Karthikesalingam, and Vivek Natarajan. Towards generalist biomedical
    ai. *NEJM AI*, 1(3):AIoa2300138, 2024a. [10.1056/AIoa2300138](https:/doi.org/10.1056/AIoa2300138).
    URL [https://ai.nejm.org/doi/abs/10.1056/AIoa2300138](https://ai.nejm.org/doi/abs/10.1056/AIoa2300138).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu等（2024a）**Tao Tu**, **Shekoofeh Azizi**, **Danny Driess**, **Mike Schaekermann**,
    **Mohamed Amin**, **Pi-Chuan Chang**, **Andrew Carroll**, **Charles Lau**, **Ryutaro
    Tanno**, **Ira Ktena**, **Anil Palepu**, **Basil Mustafa**, **Aakanksha Chowdhery**,
    **Yun Liu**, **Simon Kornblith**, **David Fleet**, **Philip Mansfield**, **Sushant
    Prakash**, **Renee Wong**, **Sunny Virmani**, **Christopher Semturs**, **S. Sara
    Mahdavi**, **Bradley Green**, **Ewa Dominowska**, **Blaise Aguera y Arcas**, **Joelle
    Barral**, **Dale Webster**, **Greg S. Corrado**, **Yossi Matias**, **Karan Singhal**,
    **Pete Florence**, **Alan Karthikesalingam**和**Vivek Natarajan**。迈向通用生物医学人工智能。*NEJM
    AI*，1(3):AIoa2300138，2024a。 [10.1056/AIoa2300138](https://doi.org/10.1056/AIoa2300138)。网址
    [https://ai.nejm.org/doi/abs/10.1056/AIoa2300138](https://ai.nejm.org/doi/abs/10.1056/AIoa2300138)。
- en: Tu et al. (2024b) Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg,
    Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards
    conversational diagnostic ai. *arXiv preprint arXiv:2401.05654*, 2024b.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu等（2024b）**Tao Tu**, **Anil Palepu**, **Mike Schaekermann**, **Khaled Saab**,
    **Jan Freyberg**, **Ryutaro Tanno**, **Amy Wang**, **Brenna Li**, **Mohamed Amin**,
    **Nenad Tomasev** 等。迈向对话诊断人工智能。*arXiv preprint arXiv:2401.05654*，2024b。
- en: van Aken (2023) Betty van Aken. Exploration and adaptation of large language
    models for specialized domains. 2023.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Aken（2023）**Betty van Aken**。大型语言模型在专业领域的探索与适应。2023。
- en: 'Van Veen et al. (2023) Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit
    Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes
    Reis, Anna Seehofnerova, et al. Clinical text summarization: Adapting large language
    models can outperform human experts. *Research Square*, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Veen等（2023）**Dave Van Veen**, **Cara Van Uden**, **Louis Blankemeier**,
    **Jean-Benoit Delbrouck**, **Asad Aali**, **Christian Bluethgen**, **Anuj Pareek**,
    **Malgorzata Polacin**, **Eduardo Pontes Reis**, **Anna Seehofnerova**等。临床文本摘要：适应大型语言模型可以超越人类专家。*Research
    Square*，2023。
- en: 'Verily Life Sciences (2023) Verily Life Sciences. Verily Viewpoint: Site CTMS
    and Protocol Digitization. Technical report, Q3 2023. URL [https://assets.verily.com/m/5a2000e85ed78214/original/Verily-Viewpoint-Site-CTMS-ProtDig_FeatureArticle_Q3-2023-1.pdf](https://assets.verily.com/m/5a2000e85ed78214/original/Verily-Viewpoint-Site-CTMS-ProtDig_FeatureArticle_Q3-2023-1.pdf).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verily Life Sciences（2023）**Verily Life Sciences**。Verily观点：站点CTMS和协议数字化。技术报告，2023年第三季度。网址
    [https://assets.verily.com/m/5a2000e85ed78214/original/Verily-Viewpoint-Site-CTMS-ProtDig_FeatureArticle_Q3-2023-1.pdf](https://assets.verily.com/m/5a2000e85ed78214/original/Verily-Viewpoint-Site-CTMS-ProtDig_FeatureArticle_Q3-2023-1.pdf)。
- en: Wadhwa et al. (2023) Somin Wadhwa, Jay DeYoung, Benjamin Nye, Silvio Amir, and
    Byron C Wallace. Jointly extracting interventions, outcomes, and findings from
    rct reports with llms. In *Machine Learning for Healthcare Conference*, pages
    754–771\. PMLR, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa等（2023）**Somin Wadhwa**, **Jay DeYoung**, **Benjamin Nye**, **Silvio Amir**
    和 **Byron C Wallace**。联合提取RCT报告中的干预、结果和发现。 在 *Machine Learning for Healthcare
    Conference*，第754–771页。PMLR，2023。
- en: 'Yoo et al. (2021) Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and
    Woomyoung Park. GPT3Mix: Leveraging large-scale language models for text augmentation.
    In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih,
    editors, *Findings of the Association for Computational Linguistics: EMNLP 2021*,
    pages 2225–2239, Punta Cana, Dominican Republic, November 2021\. Association for
    Computational Linguistics. [10.18653/v1/2021.findings-emnlp.192](https:/doi.org/10.18653/v1/2021.findings-emnlp.192).
    URL [https://aclanthology.org/2021.findings-emnlp.192](https://aclanthology.org/2021.findings-emnlp.192).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yoo等（2021）**Kang Min Yoo**, **Dongju Park**, **Jaewook Kang**, **Sang-Woo Lee**
    和 **Woomyoung Park**。GPT3Mix：利用大规模语言模型进行文本增强。在**Marie-Francine Moens**, **Xuanjing
    Huang**, **Lucia Specia** 和 **Scott Wen-tau Yih** 编辑的 *Findings of the Association
    for Computational Linguistics: EMNLP 2021*，第2225–2239页，多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。
    [10.18653/v1/2021.findings-emnlp.192](https://doi.org/10.18653/v1/2021.findings-emnlp.192)。网址
    [https://aclanthology.org/2021.findings-emnlp.192](https://aclanthology.org/2021.findings-emnlp.192)。'
- en: 'Yuan et al. (2023) Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Llm
    for patient-trial matching: Privacy-aware data augmentation towards better performance
    and generalizability. In *American Medical Informatics Association (AMIA) Annual
    Symposium*, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等（2023）**Jiayi Yuan**, **Ruixiang Tang**, **Xiaoqian Jiang** 和 **Xia Hu**。LLM在患者-试验匹配中的应用：隐私感知数据增强以提升性能和泛化能力。在
    *American Medical Informatics Association (AMIA) Annual Symposium*，2023。
- en: Appendix A Schedule of Event Tables
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 事件时间表
- en: 'In clinical trials, a schedule of events table is a table outlining the timeline
    and sequence of assessments, procedures, and data collection that will take place
    during the study. This table is an important part of the study protocol and provides
    a comprehensive overview of the study activities for both the researchers and
    participants. The schedule of events table typically includes the following information:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在临床试验中，事件时间表是一张概述在研究期间将进行的评估、程序和数据收集的时间表。这张表是研究方案的重要组成部分，为研究人员和参与者提供了研究活动的全面概述。事件时间表通常包括以下信息：
- en: '1.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Study Visits: This includes different study visits or assessment time-points,
    such as screening, baseline, treatment periods, follow-up visits, and the end
    of study. Typically the timing of each visit (e.g., day, week, month) are also
    specified.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究访问：包括不同的研究访问或评估时间点，如筛查、基线、治疗期间、随访访问和研究结束。通常还会指定每次访问的时间（例如，天、周、月）。
- en: '2.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Assessments and Procedures: The Schedule-of-events table also describes the
    various assessments, tests, and procedures that will be performed at each study
    visit. This may include informed consent, physical examinations, vital sign measurements,
    laboratory tests, imaging studies, patient-reported outcomes, and any other relevant
    data collection.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估与程序：事件时间表还描述了在每次研究访问时将进行的各种评估、测试和程序。这可能包括知情同意、体检、生命体征测量、实验室检查、影像学研究、患者报告的结果以及其他相关的数据收集。
- en: '3.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Data Collection: The table includes the data that will be collected at each
    study visit, such as adverse events, concomitant medications, and any other relevant
    information.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集：表格包括在每次研究访问时将收集的数据，如不良事件、同时使用的药物以及其他相关信息。
- en: 'We provide a sample SoE table (refer table [3](#A1.T3 "Table 3 ‣ A.1 Example
    SoE and Non-SoE Tables ‣ Appendix A Schedule of Event Tables ‣ Selective Fine-tuning
    on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using
    Schedule-of-Event Table Detection")) based on [NIH template](https://www.nia.nih.gov/sites/default/files/2019-10/startup_protocol_template_09202019.docx)
    and two non-SoE tables (refer tables [4](#A1.T4 "Table 4 ‣ A.1 Example SoE and
    Non-SoE Tables ‣ Appendix A Schedule of Event Tables ‣ Selective Fine-tuning on
    LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection") and [5](#A1.T5 "Table 5 ‣ A.1 Example SoE and Non-SoE Tables
    ‣ Appendix A Schedule of Event Tables ‣ Selective Fine-tuning on LLM-labeled Data
    May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event
    Table Detection")) below. The first table has clear screening, treatment and follow-up
    period. It specifies various visit with time information as well as window during
    which the visit can take place. The second table looks like an SoE table in terms
    of structure, but it doesn’t have clearly demarcated screening, treatment and
    follow-up. Further, it only specifies specific lab tests at the start of the diagnosis
    and completion of therapy and lacks treatment period information. Often this table
    would require protocol digitization specialists to look at additional context
    (like surrounding texts on the page) in the protocol to determine whether or not
    it is a SoE table. The last table specifies pharmacokinetic collections and is
    not a SoE table (see prompts in appendix [B](#A2 "Appendix B Prompts ‣ Selective
    Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case
    Study Using Schedule-of-Event Table Detection") which we wrote in consultation
    with the digitizers for SoE tables)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了一个示例 SoE 表格（参见表 [3](#A1.T3 "表 3 ‣ A.1 示例 SoE 和非 SoE 表格 ‣ 附录 A 事件表时间表 ‣
    在 LLM 标记数据上进行选择性微调可能减少对人工标注的依赖: 以事件表检测为例")），基于 [NIH 模板](https://www.nia.nih.gov/sites/default/files/2019-10/startup_protocol_template_09202019.docx)
    和两个非 SoE 表格（参见表 [4](#A1.T4 "表 4 ‣ A.1 示例 SoE 和非 SoE 表格 ‣ 附录 A 事件表时间表 ‣ 在 LLM 标记数据上进行选择性微调可能减少对人工标注的依赖:
    以事件表检测为例") 和 [5](#A1.T5 "表 5 ‣ A.1 示例 SoE 和非 SoE 表格 ‣ 附录 A 事件表时间表 ‣ 在 LLM 标记数据上进行选择性微调可能减少对人工标注的依赖:
    以事件表检测为例")）。第一个表格具有明确的筛查、治疗和随访阶段。它指定了各种访视的时间信息以及访视可以发生的窗口。第二个表格在结构上看起来像 SoE 表格，但没有明确划分筛查、治疗和随访。此外，它只在诊断开始和治疗完成时指定了具体的实验室测试，并缺乏治疗阶段的信息。通常，此表格需要方案数字化专家查看协议中的其他上下文（如页面上的周边文本），以确定它是否为
    SoE 表格。最后一个表格指定了药代动力学收集，不是 SoE 表格（参见附录 [B](#A2 "附录 B 提示 ‣ 在 LLM 标记数据上进行选择性微调可能减少对人工标注的依赖:
    以事件表检测为例") 中的提示，我们在与 SoE 表格数字化专家咨询时编写）。'
- en: Note that we are unable to provide identical sample tables from our own dataset
    due to limitations on data sharing. The first example of the SoE table is taken
    from the NIH template for SoE tables and the last two tables are fictitious and
    are not from any actual clinical trial protocols.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于数据共享的限制，我们无法提供来自自己数据集的完全相同的示例表格。SoE 表格的第一个示例取自 NIH SoE 表格模板，最后两个表格是虚构的，不来自任何实际的临床试验方案。
- en: A.1 Example SoE and Non-SoE Tables
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 示例 SoE 和非 SoE 表格
- en: 'Table 3: Sample SoE Table'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 示例 SoE 表格'
- en: '| Assessment | Screening: | Treatment Visits | Follow-up |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 筛查: | 治疗访视 | 随访 |'
- en: '| Visit (Day -14 to -1) | Baseline, | Visit 2 | Visit 3 | Visit 4 | Visit 5
    | Final |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 访视（第 -14 到 -1 天） | 基线， | 访视 2 | 访视 3 | 访视 4 | 访视 5 | 最终 |'
- en: '|  |  | Enrollment, |  |  |  |  | Visit |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 入组， |  |  |  |  | 访视 |'
- en: '|  |  | Visit 1 (Day 0) | (Day 7$\pm$2 Days) | (Day 70$\pm$7 Days) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 访视 1（第 0 天） | （第 7$\pm$2 天） | （第 70$\pm$7 天） |'
- en: '| Informed Consent Form | X |  |  |  |  |  |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 知情同意书 | X |  |  |  |  |  |  |'
- en: '| Demographics | X |  |  |  |  |  | X |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 人口统计 | X |  |  |  |  |  | X |'
- en: '| DXA | X |  |  |  |  |  | X |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| DXA | X |  |  |  |  |  | X |'
- en: '| Medical History | X |  |  |  |  |  |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 病史 | X |  |  |  |  |  |  |'
- en: '| General Physical Examination | X | X | X | X | X |  | X |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 一般体格检查 | X | X | X | X | X |  | X |'
- en: '| Current Medications | X | X |  |  |  |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 当前用药 | X | X |  |  |  |  |  |'
- en: '| Blood Chemistries | X | X | X | X | X | X | X |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 血液化学 | X | X | X | X | X | X | X |'
- en: '| Hematology | X | X | X | X | X | X | X |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 血液学 | X | X | X | X | X | X | X |'
- en: '| Urine Analysis | X | X | X | X | X | X |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 尿液分析 | X | X | X | X | X | X |  |'
- en: '| Vital Signs | X | X | X | X | X | X | X |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 生命体征 | X | X | X | X | X | X | X |'
- en: '| Inclusion/Exclusion Criteria | X | X |  |  |  |  |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 纳入/排除标准 | X | X |  |  |  |  |  |'
- en: '| Enrollment/Randomization |  | X |  |  |  |  | X |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 登记/随机分组 |  | X |  |  |  |  | X |'
- en: '| Treatment Administration Form |  | X | X | X | X | X | X |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 治疗给药表 |  | X | X | X | X | X | X |'
- en: '| Concomitant Medications |  | X | X | X | X | X |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 同时用药 |  | X | X | X | X | X |  |'
- en: '| Adverse Events |  | X | X | X | X | X | X |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 不良事件 |  | X | X | X | X | X | X |'
- en: 'Table 4: Non-SoE Table Example 1'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 非 SoE 表格示例 1'
- en: '| Evaluation |  | Months Following the Completion of Therapy |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 评估 |  | 治疗完成后的月份 |  |'
- en: '| Diagnosis | 3 | 9 | 24 | 48 |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 诊断 | 3 | 9 | 24 | 48 |  |'
- en: '| Physical measurements | X |  | X | X | X |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 体格测量 | X |  | X | X | X |  |'
- en: '| IGF-1 | X |  | X | X |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| IGF-1 | X |  | X | X |  |  |'
- en: '| TSH | X | X | X | X |  |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| TSH | X | X | X | X |  |  |'
- en: '| Morning Cortisol (7AM-9AM) | X |  |  | X |  |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 早晨皮质醇 (7AM-9AM) | X |  |  | X |  |  |'
- en: '| Systolic BP | X | X | X | X | X |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 收缩压 | X | X | X | X | X |  |'
- en: '| Serum Sodium | X |  | X | X |  |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 血清钠 | X |  | X | X |  |  |'
- en: '| HbA1c | X |  | X | X | X |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| HbA1c | X |  | X | X | X |  |'
- en: '| Serum Calcium | X |  |  | X |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 血清钙 | X |  |  | X |  |  |'
- en: 'Table 5: Non-SoE Table Example 2'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 非 SoE 表格示例 2'
- en: '| Assessment or Procedure | Dose | Day | Time | Time Window | Pharmacokinetics
    | Immunogenicity |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 评估或程序 | 剂量 | 天数 | 时间 | 时间窗口 | 药代动力学 | 免疫原性 |'
- en: '|  | Dose 1 | Day 2 | Pre-dose |  | X | X |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 剂量 1 | 第 2 天 | 给药前 |  | X | X |'
- en: '|  | Dose 2 | Day 9 | Pre-dose | $\pm$2 hours | X |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 剂量 2 | 第 9 天 | 给药前 | $\pm$2 小时 | X |  |'
- en: '| Cycle 1 | Dose 3 | Day 16 | Pre-dose |  |  | X |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 周期 1 | 剂量 3 | 第 16 天 | 给药前 |  |  | X |'
- en: '|  | Dose 4 | Day 25 | Pre-dose | $\pm$8 hours | X |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | 剂量 4 | 第 25 天 | 给药前 | $\pm$8 小时 | X |  |'
- en: '| Cycle 2 | Dose 5 | Day 30 | Pre-dose |  |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 周期 2 | 剂量 5 | 第 30 天 | 给药前 |  |  |  |'
- en: '| Final Assessment |  | End of Tx |  |  |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 最终评估 |  | 治疗结束 |  |  |  |  |'
- en: '| Follow-up Review |  | Day 30 | Post 5 weeks |  | X | X |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 随访检查 |  | 第 30 天 | 术后 5 周 |  | X | X |'
- en: Appendix B Prompts
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 提示
- en: B.1 Prompt for JSON based inference
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 基于 JSON 的推断提示
- en: 'We use the following prompt for inference with JSON representation of the table:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下提示进行基于 JSON 的表格推断：
- en: '[PRE0]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: B.2 Prompt for text based inference
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 基于文本的推断提示
- en: 'The text based model inference for table classification is done with the following
    prompt:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的模型推断用于表格分类的提示如下：
- en: '[PRE1]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Appendix C Naive Combination of Gemini-pro and PaLM-2 models
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C Gemini-pro 和 PaLM-2 模型的简单组合
- en: To broaden our baseline comparisons, we experimented with a naive ensemble approach
    by combining the outputs of the gemini-pro 1.0 and PaLM-2 models. This exploratory
    analysis aimed to assess whether a naive combination of model inferences could
    leverage the strengths of both individual models to improve the detection of Schedule-of-Event
    (SoE) tables.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拓宽我们的基准比较，我们尝试通过结合 gemini-pro 1.0 和 PaLM-2 模型的输出，进行了一种简单的集成方法。这项探索性分析旨在评估模型推断的简单组合是否能够利用单个模型的优势，从而改善对事件时间表
    (SoE) 的检测。
- en: 'Our ensemble strategy entailed aggregating predictions from both models, each
    producing two sets of inferences for the tables in clinical trial protocols based
    on JSON and text representations. We established varying thresholds—from a minimum
    of one to a maximum of four affirmative (“YES”) inferences—to determine when a
    table should be classified as a SoE. The performance metrics of the naive ensemble
    models, detailed in Table [6](#A3.T6 "Table 6 ‣ Appendix C Naive Combination of
    Gemini-pro and PaLM-2 models ‣ Selective Fine-tuning on LLM-labeled Data May Reduce
    Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection"),
    indicate that the ensemble outperforms the individual models when a threshold
    of at least two affirmative inferences is applied. This specific threshold represents
    a balance, capturing the consensus across the models while mitigating the impact
    of any one model’s false positives or negatives. Nonetheless, the performance
    of naive ensemble approaches remained inferior to the fine-tuned models (PaLM-2
    fine-tuned with human labels or gemini annotated and consensus-filtered labels)
    at all thresholds underscoring the value of fine-tuning over simple ensemble methods
    in this context. The results of our naive ensemble models show that while aggregation
    techniques can yield benefits, they are outperformed by a more sophisticated method
    of fine-tuning models with carefully curated labels.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的集成策略涉及从两个模型中汇总预测，每个模型基于JSON和文本表示为临床试验协议中的表格生成两组推断。我们建立了不同的阈值——从一个到四个肯定（“YES”）推断不等——以确定何时将表格分类为SoE。表[6](#A3.T6
    "表6 ‣ 附录C Gemini-pro和PaLM-2模型的朴素组合 ‣ 基于LLM标记数据的选择性微调可能减少对人工注释的依赖：以事件表检测为例")中详细说明的朴素集成模型的性能指标表明，当应用至少两个肯定推断的阈值时，集成模型的表现优于单个模型。这个特定的阈值代表了一个平衡，捕捉了模型之间的共识，同时减轻了任何一个模型的假阳性或假阴性的影响。然而，朴素集成方法的性能在所有阈值下都不及微调模型（使用人工标签微调的PaLM-2或使用Gemini标注和共识过滤标签的模型），这突显了在这种情况下微调相对于简单集成方法的价值。我们的朴素集成模型的结果显示，尽管聚合技术可以带来好处，但它们不如通过精心策划标签微调模型的复杂方法表现优越。
- en: 'Table 6: Performance of models when using various thresholds for classifying
    as SoE Tables'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：使用各种阈值对模型进行SoE表分类时的性能
- en: '| Model | Recall | Precision | F-1 Score | Accuracy |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 召回率 | 精确度 | F-1得分 | 准确率 |'
- en: '| PaLM-2-Gemini Naive | 100% | 51.5% | 0.65 | 83.6% |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| PaLM-2-Gemini 朴素 | 100% | 51.5% | 0.65 | 83.6% |'
- en: '| Ensemble-1 |  |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 集成-1 |  |  |  |  |'
- en: '| (SoE if at least one inference is SoE) |  |  |  |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (如果至少一个推断是SoE) |  |  |  |  |'
- en: '| PaLM-2-Gemini Naive | 99.5% | 75.6% | 0.83 | 92.6% |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| PaLM-2-Gemini 朴素 | 99.5% | 75.6% | 0.83 | 92.6% |'
- en: '| Ensemble-2 |  |  |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 集成-2 |  |  |  |  |'
- en: '| (SoE if $$> inferences are SoE) |  |  |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| (如果$$>推断都是SoE) |  |  |  |  |'
- en: '| PaLM-2-Gemini Naive | 94.9% | 83.6% | 0.86 | 94.9% |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| PaLM-2-Gemini 朴素 | 94.9% | 83.6% | 0.86 | 94.9% |'
- en: '| Ensemble-3 |  |  |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 集成-3 |  |  |  |  |'
- en: '| (SoE if 60% &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用$$>60% &#124;'
- en: '&#124; precision &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度 &#124;'
- en: '|'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; % of protocol &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协议的百分比 &#124;'
- en: '&#124; with <math id=$$80% &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用<math id=$$80% &#124;'
- en: '&#124; precision &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度 &#124;'
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; % of protocol &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协议的百分比 &#124;'
- en: '&#124; with 100% &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用100% &#124;'
- en: '&#124; precision &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度 &#124;'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; % of protocol &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协议的百分比 &#124;'
- en: '&#124; with 100% &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用100% &#124;'
- en: '&#124; recall &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 召回率 &#124;'
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PaLM-2 | 44.0 | 22.0 | 14.3 | 93.4 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| PaLM-2 | 44.0 | 22.0 | 14.3 | 93.4 |'
- en: '| GPT-4 (gpt-4-0613) | 75.8 | 56.0 | 42.9 | 95.6 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (gpt-4-0613) | 75.8 | 56.0 | 42.9 | 95.6 |'
- en: '| Gemini Pro 1.0 | 56.0 | 33.0 | 22.0 | 98.9 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro 1.0 | 56.0 | 33.0 | 22.0 | 98.9 |'
- en: '| Fine-tuned PaLM | 85.7 | 71.4 | 68.1 | 97.8 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 微调的PaLM | 85.7 | 71.4 | 68.1 | 97.8 |'
- en: '| (Using Human Labels) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| (使用人工标签) |'
- en: '| Fine-tuned PaLM-2 | 53.8 | 28.6 | 19.8 | 100.0 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 微调的PaLM-2 | 53.8 | 28.6 | 19.8 | 100.0 |'
- en: '| (using ALL Gemini |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (使用所有Gemini |'
- en: '| Labels) |  |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 标签) |  |  |  |  |'
- en: '| Fine-tuned PaLM-2 | 82.4 | 69.2 | 64.8 | 95.0 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 微调的PaLM-2 | 82.4 | 69.2 | 64.8 | 95.0 |'
- en: '| (Using Filtered Gemini |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| (使用过滤后的Gemini |'
- en: '| Labels) |  |  |  |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 标签) |  |  |  |  |'
