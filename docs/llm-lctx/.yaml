- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:03:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10308](https://ar5iv.labs.arxiv.org/html/2404.10308)
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10308](https://ar5iv.labs.arxiv.org/html/2404.10308)
- en: \etocsettocstyle
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \etocsettocstyle
- en: Appendix
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'Woomin Song^(1,)   Seunghyuk Oh^(1,)¹¹footnotemark: 1   Sangwoo Mo²   Jaehyung
    Kim^(3,)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Woomin Song^(1,)   Seunghyuk Oh^(1,)¹¹footnotemark: 1   Sangwoo Mo²   Jaehyung
    Kim^(3,)'
- en: Sukmin Yun^(4,)   Jung-Woo Ha⁵   Jinwoo Shin¹
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Sukmin Yun^(4,)   Jung-Woo Ha⁵   Jinwoo Shin¹
- en: ¹KAIST ²University of Michigan ³Carnegie Mellon University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹KAIST ²密歇根大学 ³卡内基梅隆大学
- en: ⁴Hanyang University ERICA ⁵NAVER Equal contribution. Work done in KAIST.Work
    done in Mohamed bin Zayed University of Artificial Intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴汉阳大学 ERICA ⁵NAVER 平等贡献。工作完成于 KAIST。工作完成于穆罕默德·本·扎耶德人工智能大学。
- en: 'Hierarchical Context Merging: Better Long'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次化上下文合并：更长的
- en: Context Understanding for Pre-trained LLMs
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练 LLMs 的上下文理解
- en: 'Woomin Song^(1,)   Seunghyuk Oh^(1,)¹¹footnotemark: 1   Sangwoo Mo²   Jaehyung
    Kim^(3,)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'Woomin Song^(1,)   Seunghyuk Oh^(1,)¹¹footnotemark: 1   Sangwoo Mo²   Jaehyung
    Kim^(3,)'
- en: Sukmin Yun^(4,)   Jung-Woo Ha⁵   Jinwoo Shin¹
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Sukmin Yun^(4,)   Jung-Woo Ha⁵   Jinwoo Shin¹
- en: ¹KAIST ²University of Michigan ³Carnegie Mellon University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ¹KAIST ²密歇根大学 ³卡内基梅隆大学
- en: ⁴Hanyang University ERICA ⁵NAVER Equal contribution. Work done in KAIST.Work
    done in Mohamed bin Zayed University of Artificial Intelligence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴汉阳大学 ERICA ⁵NAVER 平等贡献。工作完成于 KAIST。工作完成于穆罕默德·本·扎耶德人工智能大学。
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have shown remarkable performance in various natural
    language processing tasks. However, a primary constraint they face is the context
    limit, i.e., the maximum number of tokens they can process. Previous works have
    explored architectural changes and modifications in positional encoding to relax
    the constraint, but they often require expensive training or do not address the
    computational demands of self-attention. In this paper, we present Hierarchical
    cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations.
    HOMER uses a divide-and-conquer algorithm, dividing long inputs into manageable
    chunks. Each chunk is then processed collectively, employing a hierarchical strategy
    that merges adjacent chunks at progressive transformer layers. A token reduction
    technique precedes each merging, ensuring memory usage efficiency. We also propose
    an optimized computational order reducing the memory requirement to logarithmically
    scale with respect to input length, making it especially favorable for environments
    with tight memory restrictions. Our experiments demonstrate the proposed method’s
    superior performance and memory efficiency, enabling the broader use of LLMs in
    contexts requiring extended context. Code is available at [https://github.com/alinlab/HOMER](https://github.com/alinlab/HOMER).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理任务中表现出色。然而，它们面临的主要限制是上下文限制，即它们可以处理的最大标记数量。以往的研究探讨了架构变化和位置编码的修改以放宽限制，但这些方法通常需要昂贵的训练或未能解决自注意力的计算需求。在本文中，我们提出了层次化上下文合并（HOMER），一种无需训练的新方案，旨在克服这些限制。HOMER使用分而治之算法，将长输入分割成可管理的块。每个块随后被整体处理，采用层次化策略在逐步的变换层中合并相邻块。每次合并前使用标记减少技术，确保内存使用效率。我们还提出了一种优化的计算顺序，将内存需求减少到与输入长度对数规模成正比，使其在内存限制严格的环境中特别有利。我们的实验展示了该方法的卓越性能和内存效率，使大型语言模型在需要扩展上下文的场景中得到更广泛的应用。代码可在
    [https://github.com/alinlab/HOMER](https://github.com/alinlab/HOMER) 获取。
- en: \etocdepthtag
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \etocdepthtag
- en: .tocmtchapter \etocsettagdepthmtchaptersubsection \etocsettagdepthmtappendixnone
    \faketableofcontents
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: .tocmtchapter \etocsettagdepthmtchaptersubsection \etocsettagdepthmtappendixnone
    \faketableofcontents
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, large language models (LLMs) have performed exceptionally
    in various natural language processing tasks (OpenAI, [2023](#bib.bib22); Touvron
    et al., [2023](#bib.bib31)). Using this capability, multiple emerging applications
    are using LLMs as a central component. However, LLMs have a fundamental constraint
    in their context limit, which means the maximum number of input tokens they can
    process. The ability to handle long contexts is important for real-world applications:
    chatbots might need to interpret extensive chat histories, while the user could
    task code comprehension models to process extensive codebases.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）在各种自然语言处理任务中表现出色（OpenAI, [2023](#bib.bib22); Touvron et al.,
    [2023](#bib.bib31)）。利用这种能力，多个新兴应用正在将LLMs作为核心组件。然而，LLMs在上下文限制方面存在一个基本的约束，即它们可以处理的最大输入令牌数。处理长上下文的能力对实际应用非常重要：聊天机器人可能需要解释大量的聊天历史，而用户可能会要求代码理解模型处理大量的代码库。
- en: A significant challenge in overcoming the context limit is addressing the quadratic
    computational burden of the self-attention mechanism. Prior works have attempted
    to reduce the computational cost by altering the model architecture, such as introducing
    sparse attention (Child et al., [2019](#bib.bib7); Beltagy et al., [2020](#bib.bib1))
    or linearized attention (Kitaev et al., [2020](#bib.bib18); Katharopoulos et al.,
    [2020](#bib.bib17)). Yet, such methods are often not scalable (Tay et al., [2022](#bib.bib30)),
    and more importantly, they often require extensive model training, making them
    difficult to use for large-scale models that are prevalent today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 克服上下文限制的一个重大挑战是解决自注意机制的二次计算负担。以前的工作试图通过改变模型架构来降低计算成本，例如引入稀疏注意（Child et al.,
    [2019](#bib.bib7); Beltagy et al., [2020](#bib.bib1)）或线性化注意（Kitaev et al., [2020](#bib.bib18);
    Katharopoulos et al., [2020](#bib.bib17)）。然而，这些方法通常不具备可扩展性（Tay et al., [2022](#bib.bib30)），更重要的是，它们通常需要大量的模型训练，使得它们难以用于当前流行的大规模模型。
- en: To overcome this issue, recent works have focused on strategies to extend the
    context limit of pre-trained state-of-the-art LLMs. However, their major focus
    has been modifying the positional encoding (Chen et al., [2023](#bib.bib6); Peng
    et al., [2023](#bib.bib24)), which does not address the quadratic computational
    cost of self-attention, leaving the efficiency concern unaddressed. Reducing the
    complexity of pre-trained LLMs remains an important yet underexplored research
    question.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，最近的工作集中在扩展预训练的最先进LLMs的上下文限制的策略上。然而，它们的主要关注点一直是修改位置编码（Chen et al., [2023](#bib.bib6);
    Peng et al., [2023](#bib.bib24)），这并未解决自注意力的二次计算成本，导致效率问题未得到解决。降低预训练LLMs的复杂性仍然是一个重要但未充分探索的研究问题。
- en: In this paper, we introduce HOMER (Hierarchical cOntext MERging), a novel technique
    designed to extend the context limit while ensuring computational efficiency.
    HOMER employs a divide-and-conquer approach, dividing the long input into manageable
    chunks. Unlike previous methodologies (Wang et al., [2023](#bib.bib33); Bertsch
    et al., [2023](#bib.bib2)), HOMER does not process these chunks independently.
    Instead, it employs a hierarchical merging strategy, progressively merging adjacent
    chunks as they are processed along the transformer layers (see Figure [2](#S3.F2
    "Figure 2 ‣ 3 Hierarchical Context Merging") for its illustration). To ensure
    computational efficiency, apply token reduction before each merging stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了HOMER（Hierarchical cOntext MERging），这是一种旨在扩展上下文限制同时确保计算效率的新技术。HOMER采用了分而治之的方法，将长输入划分为可管理的块。与之前的方法（Wang
    et al., [2023](#bib.bib33); Bertsch et al., [2023](#bib.bib2)）不同，HOMER不会独立处理这些块。相反，它采用了层次化合并策略，在变换器层中逐步合并相邻的块（见图
    [2](#S3.F2 "Figure 2 ‣ 3 Hierarchical Context Merging) 以了解其示意图）。为了确保计算效率，在每个合并阶段前应用令牌减少。
- en: Furthermore, HOMER can be applied to pre-trained LLMs without any further finetuning.
    This can be beneficial for practical use scenarios where model finetuning is infeasible,
    such as in environments with limited computing resources. Also, data preparation
    may present another challenge for finetuning due to the scarcity of coherent texts
    with tens of thousands of tokens. For instance, specialized text data should be
    prepared to finetune an instruction-finetuned or chat-finetuned model without
    severely losing its desired properties.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，HOMER 可以应用于预训练的 LLMs，无需进一步微调。这对于模型微调不可行的实际使用场景（如计算资源有限的环境）具有潜在的好处。同时，由于缺乏包含数万
    tokens 的连贯文本，数据准备可能成为微调的另一个挑战。例如，应该准备专门的文本数据来微调一个指令微调或聊天微调的模型，而不严重丧失其期望的特性。
- en: '![Refer to caption](img/f01a7ef66777e59932ca299e44133e61.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f01a7ef66777e59932ca299e44133e61.png)'
- en: (a) Passkey retrieval accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 密钥检索准确率。
- en: '![Refer to caption](img/c8b269c60e762abac1100206a57cc662.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8b269c60e762abac1100206a57cc662.png)'
- en: (b) Peak memory usage.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 峰值内存使用量。
- en: '![Refer to caption](img/aa102ecd1c79d12c2c64034554a01372.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa102ecd1c79d12c2c64034554a01372.png)'
- en: (c) Average inference time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 平均推理时间。
- en: 'Figure 1: (a) Passkey retrieval accuracy on various context lengths, measured
    with Llama-2-7b-chat. HOMER maintains reasonable performance for context lengths
    up to 32K tokens. Detailed comparisons with more baselines are provided in [Table 1](#S4.T1
    "In 4.1 Passkey retrieval ‣ 4 Experiments"). (b) The memory requirement for processing
    long inputs. (c) Average inference time required for generating 100 tokens conditioned
    on various context lengths. All efficiency measurements are done with a single
    A100 GPU. The baselines include plain Llama, PI, NTK, and YaRN. Peak memory usage
    of the baselines at 64k is an estimated value, as they do not fit in a single
    A100 GPU. Detailed results are provided in [Table 5](#S4.T5 "In 4.5 Computational
    efficiency ‣ 4 Experiments") and [Appendix E](#A5 "Appendix E Inference speed
    analysis").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: (a) 在不同上下文长度下的密钥检索准确率，使用 Llama-2-7b-chat 进行测量。HOMER 在上下文长度达到 32K tokens
    时保持了合理的性能。与更多基线的详细比较见[表 1](#S4.T1 "在 4.1 密钥检索 ‣ 4 实验")。 (b) 处理长输入的内存需求。 (c) 生成
    100 个 token 所需的平均推理时间，依赖于不同的上下文长度。所有效率测量均使用单个 A100 GPU 完成。基线包括普通 Llama、PI、NTK
    和 YaRN。在 64k 下，基线的峰值内存使用量是估计值，因为它们无法在单个 A100 GPU 上适配。详细结果见[表 5](#S4.T5 "在 4.5
    计算效率 ‣ 4 实验")和[附录 E](#A5 "附录 E 推理速度分析")。'
- en: Through extensive evaluation on downstream tasks and perplexity measurements,
    we demonstrate that HOMER can effectively extend pre-trained LLMs to handle long
    inputs beyond their context limits. We first verify the effectiveness of our method
    on various downstream tasks, including passkey retrieval and question answering.
    We further demonstrate the fluency of HOMER by measuring perplexity on long documents.
    Finally, we highlight the computational efficiency of HOMER as presented in [Figure 1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction") and [Figure 1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1
    Introduction"). In all experiments, we illustrate that HOMER can be used with
    conventional positional encoding scaling techniques (Chen et al., [2023](#bib.bib6);
    bloc97, [2023](#bib.bib3); Peng et al., [2023](#bib.bib24)), and shows improved
    performance when used on top of these approaches.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对下游任务和困惑度测量的广泛评估，我们证明了 HOMER 可以有效扩展预训练的 LLMs，以处理超出其上下文限制的长输入。我们首先在各种下游任务（包括密钥检索和问答）上验证了我们方法的有效性。我们进一步通过测量长文档的困惑度来展示
    HOMER 的流畅性。最后，我们强调了 HOMER 的计算效率，如[图 1(b)](#S1.F1.sf2 "在图 1 ‣ 1 引言")和[图 1(c)](#S1.F1.sf3
    "在图 1 ‣ 1 引言")中所示。在所有实验中，我们展示了 HOMER 可以与传统的位置编码缩放技术（Chen 等人，[2023](#bib.bib6)；bloc97，[2023](#bib.bib3)；Peng
    等人，[2023](#bib.bib24)）一起使用，并且在这些方法的基础上使用时表现出改进的性能。
- en: 'In summary, our contributions are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We present hierarchical context merging: a memory-efficient context limit extension
    technique, that can be used with pre-trained LLMs without additional training.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了层次化上下文合并：一种内存高效的上下文限制扩展技术，可以在不进行额外训练的情况下与预训练的大型语言模型（LLMs）一起使用。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We assess the effectiveness of HOMER through experiments on long inputs. In
    passkey retrieval experiments, HOMER shows 80.4% retrieval accuracy for 32k inputs,
    whereas even the best-performing baseline shows only 22.4% accuracy. HOMER also
    improves the prediction accuracy on question answering by 3% (32.7% $\rightarrow$
    35.7%), presenting its capability to perform complex reasoning about the content
    in the extended context length. In language modeling experiments, HOMER is the
    only method showing low perplexity on inputs up to 64k tokens, while the baselines
    exhibit severe performance degradation for inputs over 32k tokens.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过对长输入的实验来评估HOMER的效果。在密码检索实验中，HOMER对32k个输入显示了80.4%的检索准确率，而即使是表现最好的基线方法也仅显示22.4%的准确率。HOMER还提高了问答的预测准确率3%（32.7%
    $\rightarrow$ 35.7%），展现了其在扩展上下文长度中进行复杂推理的能力。在语言建模实验中，HOMER是唯一在最多64k个标记的输入上显示低困惑度的方法，而基线方法在超过32k个标记的输入上表现出严重的性能下降。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate the efficiency of our approach and analyze the source of computational
    savings. Utilizing an optimized computation order, memory requirement scales logarithmically
    with respect to the input sequence length, reducing the memory requirement by
    over 70%.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了我们方法的效率并分析了计算节省的来源。利用优化的计算顺序，内存需求与输入序列长度成对数关系，内存需求减少超过70%。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that our method is compatible with the conventional RoPE-scaling methods
    in a plug-in manner, and using them together achieves an additional performance
    gain.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了我们的方法与传统的RoPE缩放方法兼容，并且一起使用可以获得额外的性能提升。
- en: 2 Related Work
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long-range transformers. Classical methods for long-range transformers primarily
    focus on reducing the quadratic computational cost of self-attention, such as
    sparse attention (Dai et al., [2019](#bib.bib9); Child et al., [2019](#bib.bib7);
    Rae et al., [2019](#bib.bib26); Qiu et al., [2019](#bib.bib25); Beltagy et al.,
    [2020](#bib.bib1); Zaheer et al., [2020](#bib.bib35)), or linearized attention (Kitaev
    et al., [2020](#bib.bib18); Katharopoulos et al., [2020](#bib.bib17); Wang et al.,
    [2020](#bib.bib32); Choromanski et al., [2021](#bib.bib8)). However, these approaches
    fundamentally change the underlying architecture, and it has not been proven to
    be scalable for large models (Tay et al., [2022](#bib.bib30)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 长程变换器。经典的长程变换器方法主要集中在减少自注意力的二次计算成本上，例如稀疏注意力（Dai et al., [2019](#bib.bib9); Child
    et al., [2019](#bib.bib7); Rae et al., [2019](#bib.bib26); Qiu et al., [2019](#bib.bib25);
    Beltagy et al., [2020](#bib.bib1); Zaheer et al., [2020](#bib.bib35)），或线性化注意力（Kitaev
    et al., [2020](#bib.bib18); Katharopoulos et al., [2020](#bib.bib17); Wang et
    al., [2020](#bib.bib32); Choromanski et al., [2021](#bib.bib8)）。然而，这些方法从根本上改变了基础架构，尚未证明其对于大型模型具有可扩展性（Tay
    et al., [2022](#bib.bib30)）。
- en: Extension of LLM context lengths. As the context limit of LLMs has become a
    critical problem, a line of concurrent works emerged, focusing on efficiently
    extending the context length of LLMs, with most works focusing on Llama (Touvron
    et al., [2023](#bib.bib31)). Most works focus on scaling the Rotary Position Embedding
    (RoPE) (Su et al., [2021](#bib.bib29)). Chen et al. ([2023](#bib.bib6)) and kaiokendev
    ([2023](#bib.bib16)) concurrently discovered the Position Interpolation method
    (PI), which involves linearly interpolating the position ids. bloc97 ([2023](#bib.bib3))
    suggested an NTK-aware scaling method (NTK) which further alters the base of RoPE.
    Peng et al. ([2023](#bib.bib24)) further extended NTK-aware scaling, suggesting
    another RoPE scaling method, YaRN. Several works additionally alter the attention
    mechanism by either applying a mask (Han et al., [2023](#bib.bib12)) or setting
    an upper bound on the distance between tokens (Su, [2023](#bib.bib28)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展LLM上下文长度。随着LLM的上下文限制成为一个关键问题，一系列并行的研究工作出现，专注于高效扩展LLM的上下文长度，大多数工作集中在Llama（Touvron
    et al., [2023](#bib.bib31)）。大多数工作集中在扩展旋转位置嵌入（RoPE）（Su et al., [2021](#bib.bib29)）。Chen
    et al. ([2023](#bib.bib6))和kaiokendev ([2023](#bib.bib16))同时发现了位置插值方法（PI），该方法涉及线性插值位置ID。bloc97
    ([2023](#bib.bib3))提出了一种NTK感知的缩放方法（NTK），进一步改变了RoPE的基础。Peng et al. ([2023](#bib.bib24))进一步扩展了NTK感知的缩放，提出了另一种RoPE缩放方法YaRN。一些工作通过应用掩码（Han
    et al., [2023](#bib.bib12)）或设置标记之间距离的上限（Su, [2023](#bib.bib28)）来进一步改变注意力机制。
- en: While all methods are known to work without further training, we consider PI,
    NTK, and YaRN as our main baselines as they are directly compatible with Flash
    Attention 2 (Dao, [2023](#bib.bib10)), easily enabling memory-efficient inference
    on long inputs. We also emphasize that our work is orthogonal to these work, and
    can be further applied on top of these methods to further improve performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有方法都被认为可以在没有进一步训练的情况下工作，但我们将PI、NTK和YaRN视为我们的主要基准，因为它们与Flash Attention 2 (Dao,
    [2023](#bib.bib10))直接兼容，轻松实现对长输入的内存高效推理。我们还强调，我们的工作与这些工作是正交的，可以在这些方法的基础上进一步应用，以进一步提高性能。
- en: Divide-and-conquer approaches. Approaches to overcome the quadratic computation
    problem in long context modeling while using the same quadratic self-attention
    mechanism are to divide the long input into multiple chunks, and most methods
    process the chunks independently. Inspired by Fusion-in-Decoder (Izacard & Grave,
    [2020](#bib.bib15)), SLED (Ivgi et al., [2023](#bib.bib14)) independently encodes
    multiple chunks and feeds all of them to the decoder. Similarly, Unlimiformer
    (Bertsch et al., [2023](#bib.bib2)) introduces a k-NN search on the encoder outputs,
    reducing the number of visible tokens at inference time. Retrieval-augmented LLMs
    including Memorizing transformers (Wu et al., [2022](#bib.bib34)) and LongMem
    (Wang et al., [2023](#bib.bib33)) take a similar approach of individually forwarding
    each chunk, and retrieve the cached hidden states for further use. Most of these
    methods, except for Unlimiformer, require method-specific finetuning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分治法。在使用相同的二次自注意力机制进行长上下文建模时，克服二次计算问题的方法是将长输入划分为多个块，大多数方法独立处理这些块。受到Fusion-in-Decoder
    (Izacard & Grave, [2020](#bib.bib15))的启发，SLED (Ivgi et al., [2023](#bib.bib14))独立编码多个块并将它们全部输入到解码器中。类似地，Unlimiformer
    (Bertsch et al., [2023](#bib.bib2))在编码器输出上引入了k-NN搜索，减少了推理时可见的标记数量。包括记忆转换器 (Wu
    et al., [2022](#bib.bib34)) 和 LongMem (Wang et al., [2023](#bib.bib33))在内的检索增强LLMs采取了类似的方法，单独转发每个块，并检索缓存的隐藏状态以供进一步使用。除了Unlimiformer之外，大多数这些方法需要特定于方法的微调。
- en: Token reduction. Token reduction methods have been widely studied in the field
    of efficient vision transformers. The key idea of these methods is to progressively
    reduce the number of tokens in order to reduce computation, resulting in more
    efficient training and inference. Two main approaches in this direction are either
    pruning the redundant tokens (Liang et al., [2022](#bib.bib19)) or merging them
    (Bolya et al., [2022](#bib.bib4)). To the best of our knowledge, this is the first
    work to apply token reduction to extend the context limit of large language models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 标记减少。标记减少方法在高效视觉变换器领域已经得到了广泛研究。这些方法的关键思想是逐步减少标记的数量，以减少计算，从而实现更高效的训练和推理。这方面的两种主要方法是修剪冗余标记
    (Liang et al., [2022](#bib.bib19)) 或合并它们 (Bolya et al., [2022](#bib.bib4))。据我们所知，这是首次将标记减少应用于扩展大型语言模型的上下文限制。
- en: 3 Hierarchical Context Merging
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 分层上下文合并
- en: '![Refer to caption](img/176878fb90d48522a410cf2c1e901f23.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/176878fb90d48522a410cf2c1e901f23.png)'
- en: 'Figure 2: An overview of the proposed hierarchical context merging. We first
    divide a long context into multiple chunks and independently forward them through
    the early transformer layers. In the intermediate layers, we merge multiple chunks
    by concatenation, forming a new, merged chunk. To keep the chunk length bounded,
    we apply token reduction on the original chunks to make them shorter, prior to
    merging. This process is repeated until all chunks are merged into a single chunk.
    Finally, we further refine the lower-layer embeddings to get a compact fixed-length,
    layer-wise embedding. The embedding can then be used like a standard kv-cache
    (Chen, [2022](#bib.bib5)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：所提出的分层上下文合并的概述。我们首先将长上下文划分为多个块，并通过早期变换器层独立转发它们。在中间层中，我们通过连接合并多个块，形成一个新的合并块。为了保持块长度的限制，我们在合并之前对原始块应用标记减少，使其更短。此过程重复进行，直到所有块合并为一个块。最后，我们进一步优化低层嵌入，以获得紧凑的固定长度、逐层嵌入。然后，可以像标准kv-cache
    (Chen, [2022](#bib.bib5)) 一样使用这些嵌入。
- en: 'In this section, we illustrate the detailed procedure of our proposed method,
    Hierarchical cOntext MERging (HOMER); a novel and efficient method for extending
    the context limit of large language models (LLMs). As visualized in [Figure 2](#S3.F2
    "In 3 Hierarchical Context Merging"), HOMER consists of two steps: (i) hierarchical
    merging of the intermediate hidden states, which we call context embeddings, and
    (ii) further refinement of the lower-layer embeddings by propagative refinement
    to produce a compact, fixed-length embedding for each layer, which can be seamlessly
    integrated as a typical kv-cache (Chen, [2022](#bib.bib5)). We first introduce
    the key idea of hierarchical merging in [Section 3.1](#S3.SS1 "3.1 Hierarchical
    merging of context embeddings ‣ 3 Hierarchical Context Merging"). Then, we explain
    propagative refinement in [Section 3.2](#S3.SS2 "3.2 Propagative refinement of
    lower-layer embeddings ‣ 3 Hierarchical Context Merging"). Finally in [Section 3.3](#S3.SS3
    "3.3 Computation order optimization for memory-limited environments ‣ 3 Hierarchical
    Context Merging"), we introduce an optimized computation order to further reduce
    the memory requirement to scale logarithmically with the input length.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细说明了我们提出的方法 Hierarchical cOntext MERging (HOMER) 的过程；这是一种新颖且高效的方法，用于扩展大型语言模型（LLMs）的上下文限制。如[图2](#S3.F2
    "在3 分层上下文合并中")所示，HOMER 包含两个步骤：(i) 中间隐藏状态的分层合并，我们称之为上下文嵌入，以及 (ii) 通过传播性细化进一步精炼低层嵌入，以产生每层的紧凑固定长度嵌入，这些嵌入可以无缝集成为典型的
    kv-cache（Chen, [2022](#bib.bib5)）。我们首先在[3.1节](#S3.SS1 "3.1 上下文嵌入的分层合并 ‣ 3 分层上下文合并")中介绍分层合并的关键思想。然后，在[3.2节](#S3.SS2
    "3.2 低层嵌入的传播性细化 ‣ 3 分层上下文合并")中解释传播性细化。最后，在[3.3节](#S3.SS3 "3.3 针对内存受限环境的计算顺序优化
    ‣ 3 分层上下文合并")中，我们介绍了一种优化的计算顺序，以进一步减少内存需求，使其随着输入长度的增加呈对数增长。
- en: 3.1 Hierarchical merging of context embeddings
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 上下文嵌入的分层合并
- en: We propose a divide-and-conquer approach to handle the quadratic computation
    of self-attention more efficiently. We divide the long input into multiple chunks
    and process the local chunks with the usual self-attention. Although some previous
    studies have adopted a similar approach (Ivgi et al., [2023](#bib.bib14); Bertsch
    et al., [2023](#bib.bib2)), they independently handle each chunk, possibly restricting
    the richness of the intermediate embeddings as they only have access to local
    information. In contrast, we progressively merge adjacent chunks as they move
    through the transformer layers, enabling the chunks to see each other. However,
    naïvely concatenating the adjacent chunks lengthens the resulting chunk and adds
    a significant computational burden. Thus we propose to use a token reduction technique
    to shorten each chunk before merging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种分治方法，以更高效地处理自注意力的二次计算。我们将长输入划分为多个块，并用常规自注意力处理这些局部块。尽管一些先前的研究采用了类似的方法（Ivgi
    et al., [2023](#bib.bib14); Bertsch et al., [2023](#bib.bib2)），但它们独立处理每个块，这可能限制了中间嵌入的丰富性，因为它们只能访问局部信息。相比之下，我们在块通过变换器层时逐步合并相邻的块，使得块能够相互查看。然而，简单地将相邻块连接起来会延长结果块的长度，并增加显著的计算负担。因此，我们提出使用令牌减少技术在合并之前缩短每个块的长度。
- en: By hierarchically reducing and merging the context embeddings, our method bypasses
    the quadratic computations required by the self-attention mechanism. This approach
    not only aims at computational efficiency but also preserves the richness of the
    context. The detailed process of hierarchical context merging is carried out as
    follows.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分层减少和合并上下文嵌入，我们的方法绕过了自注意力机制所需的二次计算。这种方法不仅旨在提高计算效率，还保留了上下文的丰富性。分层上下文合并的详细过程如下。
- en: Division of long context into multiple chunks. The first step of our method
    is to divide the long context into uniform chunks. However, simply slicing the
    input into chunks encounters issues in the network’s initial layers where each
    chunk cannot see each other. This approach restricts most tokens from accessing
    the starting instructions, harming the resulting embeddings’ quality. Moreover,
    the tokens at the end miss the global context, which is essential for generating
    subsequent tokens. We address this by attaching the initial and concluding parts
    of the prompt to every segment (i.e., treating them as shared prefixes and suffixes),
    ensuring each chunk contains the instruction and the ending tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将长上下文划分为多个块。我们方法的第一步是将长上下文划分为均匀的块。然而，简单地将输入切割成块在网络的初始层会遇到问题，因为每个块无法相互查看。这种方法限制了大多数标记访问起始指令，从而影响生成的嵌入质量。此外，末尾的标记缺失全球上下文，这对生成后续标记至关重要。我们通过将提示的初始部分和结束部分附加到每个片段上（即将它们视为共享前缀和后缀）来解决此问题，确保每个块都包含指令和结束标记。
- en: Token reduction on individual chunks. To keep the resulting chunk’s length short
    after merging, we adopt token reduction techniques, which have been widely studied
    in the field of efficient vision transformers. For vision transformers (Dosovitskiy
    et al., [2021](#bib.bib11)), dropping the tokens that receive minimal attention
    from the [CLS] token (i.e. the classification token) is known to be a simple and
    effective token pruning method (Haurum et al., [2023](#bib.bib13)). Inspired by
    this, we propose to prune the tokens receiving minimal attention from the final
    token in each chunk. If the chunks contain affixes, we do not prune the tokens
    corresponding to the affixes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对单个块进行标记减少。为了在合并后保持结果块的长度较短，我们采用了标记减少技术，这些技术在高效视觉变换器领域已经得到了广泛研究。对于视觉变换器 (Dosovitskiy
    et al., [2021](#bib.bib11))，丢弃从 [CLS] 标记（即分类标记）接收到最小注意力的标记被认为是一种简单而有效的标记剪枝方法 (Haurum
    et al., [2023](#bib.bib13))。受到此启发，我们建议从每个块中的最终标记中修剪接收到最小注意力的标记。如果块中包含附加部分，我们不会剪枝对应附加部分的标记。
- en: 'In practice, we identified a position bias in simple attention-based pruning
    where tokens near the end often receive higher attention weights. To rectify this,
    We incorporate a calibration technique inspired by (Zhao et al., [2021](#bib.bib36)).
    By averaging the attention weights of the last token with respect to the tokens
    at each position ahead, we derive the bias logits. These bias logits are subtracted
    from the attention logits during the token reduction to refine token pruning.
    In summary, the final token pruning is performed by pruning a fixed number of
    tokens according to the significance score $s_{\mathtt{sig}}$ defined as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们在简单的基于注意力的剪枝中发现了位置偏差，其中靠近末尾的标记通常会收到更高的注意力权重。为了解决这个问题，我们采用了一种受 (Zhao et
    al., [2021](#bib.bib36)) 启发的校准技术。通过平均最后一个标记相对于每个位置前面的标记的注意力权重，我们推导出偏置对数。在标记减少过程中，这些偏置对数从注意力对数中减去，以精细化标记剪枝。总之，最终的标记剪枝是通过根据以下定义的显著性得分
    $s_{\mathtt{sig}}$ 剪除固定数量的标记来完成的：
- en: '|  | $s_{\mathtt{sig}}^{\mathtt{i}}:=l_{\mathtt{att}}^{\mathtt{i}}-l_{\mathtt{bias}}^{\mathtt{dist(i)}},$
    |  | (1) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{\mathtt{sig}}^{\mathtt{i}}:=l_{\mathtt{att}}^{\mathtt{i}}-l_{\mathtt{bias}}^{\mathtt{dist(i)}},$
    |  | (1) |'
- en: where $s_{\mathtt{sig}}^{\mathtt{i}}$ denotes the bias logit corresponding to
    the token’s distance from the final token, $\mathtt{dist(i)}$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{\mathtt{sig}}^{\mathtt{i}}$ 表示与最终标记距离相关的偏置对数，$\mathtt{dist(i)}$。
- en: Merging chunk embeddings. After shortening, adjacent chunks are concatenated
    to form a unified chunk. This iterative process of reduction and merging across
    layers ensures individual chunks converge into a single chunk at the final layers.
    If the chunks include affixes, direct concatenation might lead to redundancy;
    we address this by simply averaging the duplicates.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 合并块嵌入。缩短后，相邻块被连接以形成一个统一的块。这种逐层的减少和合并过程确保了单个块在最终层汇聚成一个块。如果块中包含附加部分，直接连接可能导致冗余；我们通过简单地平均重复项来解决这个问题。
- en: Handling position ids. Management of position ids is an important design choice
    for our approach. While dynamically scaling the position ids through conventional
    methods like PI, NTK, and YaRN is viable, these techniques tend to underperform
    with increased scale factors, being less effective for extended contexts. To circumvent
    this issue, we reuse the same position ids across different chunks. For affixes,
    we ensure that corresponding tokens in different chunks are assigned the same
    ids for consistency across the chunks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 处理位置ID。位置ID的管理是我们方法中的一个重要设计选择。虽然通过PI、NTK和YaRN等传统方法动态缩放位置ID是可行的，但这些技术在增加规模因子时往往表现不佳，对于扩展上下文效果较差。为了解决这个问题，我们在不同块中重复使用相同的位置ID。对于词缀，我们确保不同块中的对应标记被分配相同的ID，以保持块间的一致性。
- en: 3.2 Propagative refinement of lower-layer embeddings
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 低层嵌入的传播性精细化
- en: As depicted in [Figure 2](#S3.F2 "In 3 Hierarchical Context Merging"), the hierarchical
    context merging produces embeddings characterized by a trapezoidal shape. The
    higher-layer embeddings are concise, while the lower-layer ones remain extended.
    To further reduce the computational burden for lower layers, we introduce an additional
    refinement step after token reduction, called propagative refinement.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图2](#S3.F2 "In 3 Hierarchical Context Merging")所示，层次上下文合并产生的嵌入特征具有梯形形状。高层嵌入较为简洁，而低层嵌入则保持扩展。为了进一步降低低层的计算负担，我们在标记减少后引入了一个额外的精细化步骤，称为传播性精细化。
- en: 'The process is straightforward: when a token is pruned in the upper layers,
    the corresponding tokens are also pruned in the lower-layer embeddings. Therefore,
    the pruning decision of the upper layers propagates back to the lower layers.
    The synchronized pruning across layers results in shorter, uniform embeddings
    for each layer. For better understanding, we have added a detailed illustration
    in [Appendix D](#A4 "Appendix D Illustration of propagative refinement") demonstrating
    the process step-by-step. The rationale behind this is an intuition that the upper
    layers have a better ability to identify the important tokens. Thus, we apply
    pruning in the upper layers and reflect them in the lower layers. After performing
    hierarchical merging and propagative refinement, we end up with standardized,
    fixed-length embeddings for every layer.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程很简单：当上层中的一个标记被修剪时，相应的标记也会在下层嵌入中被修剪。因此，上层的修剪决策会传播回下层。层间同步修剪使每一层的嵌入更短且统一。为了更好地理解，我们在[附录D](#A4
    "Appendix D Illustration of propagative refinement")中添加了详细的说明，逐步演示了这个过程。其背后的原理是，上层更能识别重要标记。因此，我们在上层进行修剪并在下层反映这些修剪。经过层次合并和传播性精细化后，我们得到每一层的标准化、固定长度嵌入。
- en: Using the refined embeddings for further generation. Conventional implementation
    of autoregressive language models often cache the key and value embeddings in
    order to avoid redundant computation. This technique is commonly known as kv-caching
    (Chen, [2022](#bib.bib5)). As the refined embeddings have the same length for
    every layer, they can easily be integrated with the kv-cache implementation by
    simply replacing it for the generation process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用精细化后的嵌入进行进一步生成。传统的自回归语言模型通常缓存关键和价值嵌入，以避免冗余计算。这种技术通常称为kv-caching（Chen，[2022](#bib.bib5)）。由于精细化后的嵌入在每一层具有相同的长度，它们可以通过简单地替换生成过程中的kv-cache实现来轻松集成。
- en: Algorithm 1 Memory-efficient computation ordering
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 内存高效计算排序
- en: procedure HierarchicalMerge(node)     if node is a leaf then         $\text{emb\_list}\leftarrow\text{ForwardLayers(node)}$
    Propagative refinement     else         $\text{l\_embs}\leftarrow\text{HierarchicalMerge}(\text{node.left})$
    $\triangleright$ $\triangleright$ Propagative refinement     end if     return
    emb_listend procedure
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: procedure HierarchicalMerge(node)     if node is a leaf then         $\text{emb\_list}\leftarrow\text{ForwardLayers(node)}$
    传播性精细化     else         $\text{l\_embs}\leftarrow\text{HierarchicalMerge}(\text{node.left})$
    $\triangleright$ $\triangleright$ 传播性精细化     end if     return emb_listend procedure
- en: 3.3 Computation order optimization for memory-limited environments
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 内存受限环境的计算顺序优化
- en: In typical Transformer models, all tokens at a given layer are computed in parallel.
    Following this paradigm, a direct implementation of HOMER would also process multiple
    chunks concurrently. While such implementation of HOMER inherently requires linear
    memory with respect to the input length, we propose a more optimized computation
    order that allows the memory requirement to scale logarithmically. This efficiency
    is achieved by strategically reordering the processing steps during the hierarchical
    merging.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 Transformer 模型中，给定层中的所有标记都是并行计算的。按照这一范式，HOMER 的直接实现也会同时处理多个块。虽然这种 HOMER
    实现本质上需要与输入长度成线性关系的内存，但我们提出了一种更优化的计算顺序，使得内存需求呈对数级增长。通过在层次合并过程中战略性地重新排序处理步骤，实现了这种效率。
- en: While representing each chunk as a node, the hierarchical context merging process
    can be conceptualized as a traversal on the binary tree from leaves to the root.
    By adopting the depth-first search (DFS) algorithm to the computation sequence
    while executing the propagative refinement, we can achieve a computation cost
    of a logarithmic scale with respect to the length of the input sequence. For clarity,
    a pseudo-code representation is provided in [Algorithm 1](#alg1 "In 3.2 Propagative
    refinement of lower-layer embeddings ‣ 3 Hierarchical Context Merging") and [Figure 3](#A1.F3
    "In Appendix A Memory-efficient computation order"). A comprehensive proof of
    the memory requirement can be found in [Appendix A](#A1 "Appendix A Memory-efficient
    computation order"). Through this approach, extensive inputs can be processed
    even in resource-constrained setups.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在将每个块表示为一个节点时，层次上下文合并过程可以被概念化为从叶子到根的二叉树遍历。通过在执行传播细化时将深度优先搜索（DFS）算法应用于计算序列，我们可以实现与输入序列长度成对数规模的计算成本。为了清晰起见，[算法
    1](#alg1 "In 3.2 Propagative refinement of lower-layer embeddings ‣ 3 Hierarchical
    Context Merging") 和 [图 3](#A1.F3 "In Appendix A Memory-efficient computation order")
    中提供了伪代码表示。有关内存需求的全面证明可以在 [附录 A](#A1 "Appendix A Memory-efficient computation order")
    中找到。通过这种方法，即使在资源受限的设置中，也可以处理大量输入。
- en: 4 Experiments
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we demonstrate the effectiveness of the proposed method, HOMER through
    extensive experiments. [Section 4.1](#S4.SS1 "4.1 Passkey retrieval ‣ 4 Experiments")
    contains the passkey retrieval experiments, originally suggested by Mohtashami
    & Jaggi ([2023](#bib.bib21)). This shows our method’s ability to utilize the long
    context to handle downstream tasks. [Section 4.2](#S4.SS2 "4.2 Question answering
    ‣ 4 Experiments") contains experiments on question answering. This shows the model’s
    capability to handle more complex and challenging tasks. [Section 4.3](#S4.SS3
    "4.3 Language modeling ‣ 4 Experiments") demonstrates that HOMER remains fluent,
    even when conditioned on very long contexts. This is done by measuring perplexity
    on long documents from PG-19 dataset (Rae et al., [2019](#bib.bib26)). [Section 4.4](#S4.SS4
    "4.4 Ablation studies ‣ 4 Experiments") contains ablation study on the key components
    that make HOMER effective. Finally in [Section 4.5](#S4.SS5 "4.5 Computational
    efficiency ‣ 4 Experiments"), we analyze the memory efficiency of our method.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过大量实验展示了所提方法 HOMER 的有效性。[第 4.1 节](#S4.SS1 "4.1 Passkey retrieval ‣ 4
    Experiments") 包含了最初由 Mohtashami 和 Jaggi ([2023](#bib.bib21)) 提出的密码检索实验。这展示了我们方法利用长上下文处理下游任务的能力。[第
    4.2 节](#S4.SS2 "4.2 Question answering ‣ 4 Experiments") 包含了问答实验。这展示了模型处理更复杂和具有挑战性任务的能力。[第
    4.3 节](#S4.SS3 "4.3 Language modeling ‣ 4 Experiments") 证明了 HOMER 即使在非常长的上下文条件下也能保持流畅。这是通过对
    PG-19 数据集中的长文档进行困惑度测量（Rae 等， [2019](#bib.bib26)）实现的。[第 4.4 节](#S4.SS4 "4.4 Ablation
    studies ‣ 4 Experiments") 包含了对 HOMER 关键组件的消融研究。最后，在 [第 4.5 节](#S4.SS5 "4.5 Computational
    efficiency ‣ 4 Experiments") 中，我们分析了我们方法的内存效率。
- en: Common setup and baselines. We select Llama-2 as our base model, as it is the
    most widely used and the strongest open-source large language model. We use the
    pretrained models for language modeling experiments, and the chat model for evaluation
    on downstream tasks, which include passkey retrieval and question answering.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 常见设置和基线。我们选择 Llama-2 作为我们的基础模型，因为它是最广泛使用和最强大的开源大型语言模型。我们使用预训练模型进行语言建模实验，使用聊天模型进行下游任务评估，这些任务包括密码检索和问答。
- en: Recent works on positional encoding interpolation have shown their ability to
    extend Llama’s context limit without training. We set Position Interpolation (PI)
    (kaiokendev, [2023](#bib.bib16)), NTK-aware scaling (bloc97, [2023](#bib.bib3)),
    and YaRN (Peng et al., [2023](#bib.bib24)) as our main baselines. As these models
    scale the positional encoding by a constant factor, we define their context limit
    as the original context limit (4k tokens for Llama-2) multiplied by the scaling
    factor. In practice, NTK and YaRN are known to be able to process slightly shorter
    context than the defined context limit (Peng et al., [2023](#bib.bib24)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于位置编码插值的研究显示，它们能够在不进行训练的情况下扩展 Llama 的上下文限制。我们将位置插值（PI）（kaiokendev，[2023](#bib.bib16)）、NTK
    感知缩放（bloc97，[2023](#bib.bib3)）和 YaRN（Peng 等，[2023](#bib.bib24)）作为主要基线。由于这些模型通过常数因子缩放位置编码，我们将它们的上下文限制定义为原始上下文限制（Llama-2
    的 4k 令牌）乘以缩放因子。在实际应用中，NTK 和 YaRN 被认为能够处理略短于定义上下文限制的上下文（Peng 等，[2023](#bib.bib24)）。
- en: For each task, we report the performance of HOMER applied on plain Llama. To
    further emphasize that our method is orthogonal to the positional encoding scaling
    methods, and can be applied on top of them, we additionally show the performance
    of HOMER combined with the best-performing baseline for each task.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个任务，我们报告了在普通 Llama 上应用 HOMER 的性能。为了进一步强调我们的方法与位置编码缩放方法是正交的，并且可以在它们之上应用，我们还展示了
    HOMER 与每个任务的最佳基线相结合的性能。
- en: 4.1 Passkey retrieval
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 密钥检索
- en: 'Table 1: Retrieval accuracy on passkey retrieval. Average accuracy on 500 samples
    are reported. The best values are in bold, and the second-best values are underlined.
    Empty values indicate NaN.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：密钥检索的检索准确性。报告了 500 个样本的平均准确性。最佳值用粗体显示，第二好的值用下划线标出。空值表示 NaN。
- en: '|  | Context | Llama-2-7b-chat | Llama-2-13b-chat |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | Context | Llama-2-7b-chat | Llama-2-13b-chat |'
- en: '| Method | limit | 4K | 8K | 16K | 32K | 4K | 8K | 16K | 32K |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Method | limit | 4K | 8K | 16K | 32K | 4K | 8K | 16K | 32K |'
- en: '| Plain | 4k | 1.000 | 0.000 | - | - | 1.000 | 0.000 | 0.000 | 0.000 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Plain | 4k | 1.000 | 0.000 | - | - | 1.000 | 0.000 | 0.000 | 0.000 |'
- en: '| Plain + HOMER | None | 0.990 | 0.924 | 0.890 | 0.776 | 1.000 | 0.944 | 0.882
    | 0.804 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Plain + HOMER | None | 0.990 | 0.924 | 0.890 | 0.776 | 1.000 | 0.944 | 0.882
    | 0.804 |'
- en: '| PI | 8k | 0.432 | 0.356 | 0.000 | - | 0.600 | 0.544 | 0.000 | 0.000 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| PI | 8k | 0.432 | 0.356 | 0.000 | - | 0.600 | 0.544 | 0.000 | 0.000 |'
- en: '|  | 16k | 0.006 | 0.006 | 0.006 | 0.000 | 0.022 | 0.028 | 0.018 | 0.000 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 16k | 0.006 | 0.006 | 0.006 | 0.000 | 0.022 | 0.028 | 0.018 | 0.000 |'
- en: '| NTK | 8k | 0.812 | 0.000 | 0.000 | 0.000 | 0.866 | 0.000 | 0.000 | 0.000
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| NTK | 8k | 0.812 | 0.000 | 0.000 | 0.000 | 0.866 | 0.000 | 0.000 | 0.000
    |'
- en: '|  | 16k | 0.516 | 0.652 | 0.000 | 0.000 | 0.626 | 0.692 | 0.000 | 0.000 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 16k | 0.516 | 0.652 | 0.000 | 0.000 | 0.626 | 0.692 | 0.000 | 0.000 |'
- en: '|  | 32k | 0.106 | 0.194 | 0.162 | 0.000 | 0.286 | 0.570 | 0.442 | 0.000 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 32k | 0.106 | 0.194 | 0.162 | 0.000 | 0.286 | 0.570 | 0.442 | 0.000 |'
- en: '| YaRN | 8k | 0.996 | 0.002 | 0.000 | - | 1.000 | 0.464 | 0.000 | 0.000 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| YaRN | 8k | 0.996 | 0.002 | 0.000 | - | 1.000 | 0.464 | 0.000 | 0.000 |'
- en: '|  | 16k | 0.844 | 0.756 | 0.000 | 0.000 | 0.980 | 0.952 | 0.214 | 0.000 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 16k | 0.844 | 0.756 | 0.000 | 0.000 | 0.980 | 0.952 | 0.214 | 0.000 |'
- en: '|  | 32k | 0.702 | 0.654 | 0.696 | 0.002 | 0.926 | 0.888 | 0.836 | 0.026 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 32k | 0.702 | 0.654 | 0.696 | 0.002 | 0.926 | 0.888 | 0.836 | 0.026 |'
- en: '|  | 64k | 0.678 | 0.358 | 0.148 | 0.026 | 0.902 | 0.826 | 0.364 | 0.224 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 64k | 0.678 | 0.358 | 0.148 | 0.026 | 0.902 | 0.826 | 0.364 | 0.224 |'
- en: '| YaRN + HOMER | None | 0.996 | 0.984 | 0.876 | 0.802 | 1.000 | 1.000 | 0.974
    | 0.860 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| YaRN + HOMER | None | 0.996 | 0.984 | 0.876 | 0.802 | 1.000 | 1.000 | 0.974
    | 0.860 |'
- en: In this section, we investigate if HOMER can effectively leverage the long context
    to handle downstream tasks. We evaluate this on the passkey retrieval task, originally
    proposed by Mohtashami & Jaggi ([2023](#bib.bib21)). In this task, the model is
    asked to retrieve a random number (called passkey) hidden inside distracting texts.
    The task is widely used to evaluate the maximum context length that the model
    can effectively handle.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨 HOMER 是否能有效利用长上下文来处理下游任务。我们在密钥检索任务上评估这一点，该任务最初由 Mohtashami 和 Jaggi
    提出（[2023](#bib.bib21)）。在这个任务中，模型需要从干扰文本中检索一个随机数字（称为密钥）。这个任务广泛用于评估模型能够有效处理的最大上下文长度。
- en: To evaluate the performance at different input lengths, we evaluate the models
    with inputs of lengths 4k, 8k, 16k, and 32k tokens. We report the retrieval accuracy
    in [Table 1](#S4.T1 "In 4.1 Passkey retrieval ‣ 4 Experiments"). The result demonstrates
    that HOMER successfully maintains a high accuracy of around $80\%$ for context
    length up to 32k tokens which is 8 times longer than the pre-trained context length,
    while significantly outperforming every baseline. Furthermore, it is also evident
    that the performance can be further improved by applying HOMER on top of YaRN,
    the best-performing baseline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估不同输入长度的性能，我们使用4k、8k、16k和32k标记的输入对模型进行评估。我们在[表1](#S4.T1 "在4.1 Passkey检索 ‣
    4 实验")中报告了检索准确率。结果表明，HOMER成功地在上下文长度达到32k标记时保持了约$80\%$的高准确率，这比预训练上下文长度长8倍，同时显著超越了所有基线。此外，显然将HOMER应用于YaRN（表现最佳的基线）上还可以进一步提升性能。
- en: 4.2 Question answering
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 问答
- en: 'Table 2: Accuracy in question answering, as evaluated on the QuALITY validation
    set. The best results are highlighted in bold.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在QuALITY验证集上评估的问答准确率。最佳结果以**粗体**突出显示。
- en: '| Method | Context limit | Accuracy |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 上下文限制 | 准确率 |'
- en: '| Plain | 4k | 0.327 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 4k | 0.327 |'
- en: '| Plain + HOMER | None | 0.358 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 原始 + HOMER | 无 | 0.358 |'
- en: '| PI | 8k | 0.366 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| PI | 8k | 0.366 |'
- en: '| NTK | 8k | 0.379 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| NTK | 8k | 0.379 |'
- en: '| YaRN | 8k | 0.310 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| YaRN | 8k | 0.310 |'
- en: '| NTK + HOMER | None | 0.388 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| NTK + HOMER | 无 | 0.388 |'
- en: 'In this section, we push HOMER further and evaluate its performance on a more
    challenging task: question answering based on long documents. To this end, we
    measure the model’s performance on the validation set of QuALITY (Pang et al.,
    [2021](#bib.bib23)).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进一步推动HOMER的极限，评估其在更具挑战性的任务上的性能：基于长文档的问答。为此，我们测量了模型在QuALITY验证集（Pang等，[2021](#bib.bib23)）上的表现。
- en: For baselines with limited context length, the input documents are clipped to
    fit in the context limit. For NTK and YaRN, we further clip the documents to be
    3/4 of their context limit, as they are only capable of handling inputs slightly
    shorter than the claimed context limit. This observation can also be found in
    [Section 4.1](#S4.SS1 "4.1 Passkey retrieval ‣ 4 Experiments"), as NTK and YaRN
    models could not handle inputs that are as long as their context limit. For HOMER experiments,
    we feed the full context into the model as HOMER has no hard limit on the maximum
    context length.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上下文长度有限的基线，输入文档被裁剪以适应上下文限制。对于NTK和YaRN，我们进一步裁剪文档至其上下文限制的3/4，因为它们仅能处理比声称的上下文限制稍短的输入。这一观察也可以在[第4.1节](#S4.SS1
    "4.1 Passkey检索 ‣ 4 实验")中找到，因为NTK和YaRN模型无法处理长度达到其上下文限制的输入。对于HOMER实验，我们将完整的上下文输入到模型中，因为HOMER对最大上下文长度没有硬性限制。
- en: We report the prediction accuracy in [Table 2](#S4.T2 "In 4.2 Question answering
    ‣ 4 Experiments"). As evident from the table, HOMER effectively extends the context
    limit, enjoying over 3% of accuracy gain compared to plain Llama. The performance
    is further improved when applied on top of the best-performing positional encoding
    scaling method (NTK), achieving 38.8% accuracy. This demonstrates that language
    models extended with HOMER could potentially perform more sophisticated reasoning
    based on the extended context.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表2](#S4.T2 "在4.2 问答 ‣ 4 实验")中报告了预测准确率。从表中可以明显看出，HOMER有效地扩展了上下文限制，相比于原始Llama提高了超过3%的准确率。当应用于最佳表现的位置编码缩放方法（NTK）时，性能进一步提升，达到38.8%的准确率。这表明，扩展了HOMER的语言模型可以基于扩展的上下文进行更复杂的推理。
- en: 4.3 Language modeling
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 语言建模
- en: 'Table 3: Perplexity of 25 long documents from PG-19 truncated to the evaluation
    length. The best values are in bold, and the second-best values are underlined.
    Empty values indicate NaN.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：PG-19的25个长文档的困惑度，截断至评估长度。最佳值以**粗体**表示，第二最佳值用*下划线*标出。空值表示NaN。
- en: '|  | Context | Llama-2-7b | Llama-2-13b |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 上下文 | Llama-2-7b | Llama-2-13b |'
- en: '| Method | limit | 4K | 8K | 16K | 32K | 64K | 4K | 8K | 16K | 32K | 64K |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 限制 | 4K | 8K | 16K | 32K | 64K | 4K | 8K | 16K | 32K | 64K |'
- en: '| Plain | 4k | 6.72 | - | - | - | - | 6.14 | $$> |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 4k | 6.72 | - | - | - | - | 6.14 | $$> |'
- en: '| Plain + HOMER | None | 6.72 | 7.29 | 7.78 | 8.43 | 9.64 | 6.13 | 6.60 | 6.87
    | 7.13 | 7.59 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 原始 + HOMER | 无 | 6.72 | 7.29 | 7.78 | 8.43 | 9.64 | 6.13 | 6.60 | 6.87 |
    7.13 | 7.59 |'
- en: '| PI | 8k | 7.91 | 8.19 | - | - | - | 6.96 | 7.19 |  |  |  |  |  |  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 32k | 8.42 | 8.97 | 9.76 | $$> |'
- en: '| YaRN | 8k | 6.79 | 7.40 | - | - | - | 6.19 | 6.59 |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 16k | 7.00 | 7.32 | 8.98 | - | - | 6.36 | 6.65 | 7.83 | $$> |'
- en: '|  | 32k | 7.50 | 8.05 | 8.78 |  | 7.17 | 8.32 |  | 7.17 | 8.32 |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 18.2 | 23.6 | 34.4 | 56.7 | $$> |'
- en: '| HOMER | 16.3 (-10.8%) | 16.5 (-30.1%) | 16.7 (-51.4%) | 17.6 (-68.9%) | 21.3
    (at least -73.4%) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| HOMER | 16.3 (-10.8%) | 16.5 (-30.1%) | 16.7 (-51.4%) | 17.6 (-68.9%) | 21.3
    (至少 -73.4%) |'
- en: '|    + Baselines | 19.2 (+5.5%) | 20.1 (-14.6%) | 20.3 (-41.1%) | 20.8 (-63.4%)
    | 22.5 (at least -71.8%) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|    + 基线 | 19.2 (+5.5%) | 20.1 (-14.6%) | 20.3 (-41.1%) | 20.8 (-63.4%) |
    22.5 (至少 -71.8%) |'
- en: In this section, we discuss the computational efficiency offered by our methodology,
    with a primary focus on memory efficiency. We first demonstrate the computational
    efficiency of our method by measuring the peak GPU memory usage while processing
    long inputs. In the following part of the section, we discuss the four key mechanisms
    that bring efficiency gains.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了我们的方法提供的计算效率，重点关注内存效率。我们首先通过测量处理长输入时的峰值GPU内存使用来展示我们方法的计算效率。在本节的后续部分，我们讨论了带来效率提升的四个关键机制。
- en: The peak memory usage for HOMER and baselines is illustrated in [Table 5](#S4.T5
    "In 4.5 Computational efficiency ‣ 4 Experiments"). Note that we report a single
    number for all baselines (Plain Llama, PI, NTK, YaRN) because the baselines only
    modify the positional encoding, making no difference in the peak GPU memory usage.
    For a fair comparison, all methods are tested with Flash Attention 2 (Dao, [2023](#bib.bib10))
    enabled. As shown in the table, HOMER significantly reduces memory requirements,
    reducing the peak memory usage by over 70% when running inference on 64k inputs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: HOMER及其基线的峰值内存使用情况如[表5](#S4.T5 "4.5 计算效率 ‣ 4 实验")所示。请注意，我们为所有基线（Plain Llama,
    PI, NTK, YaRN）报告了一个单一数字，因为这些基线仅修改了位置编码，对峰值GPU内存使用没有影响。为了公平比较，所有方法都使用了启用Flash Attention
    2（Dao，[2023](#bib.bib10)）。如表中所示，HOMER显著减少了内存需求，在对64k输入进行推理时，峰值内存使用减少了超过70%。
- en: The first source of our efficiency gains is the chunking mechanism. We circumvent
    the quadratic computation associated with self-attention by processing each chunk
    separately at the earlier layers. Token reduction is our second source of computation
    reduction. As our algorithm progressively reduces the number of tokens, fewer
    tokens have to be processed in the upper layers, reducing the computational overhead.
    The third source of computation saving is that HOMER outputs concise embeddings,
    optimizing the subsequent self-attention computation during the generation phase.
    Compared to naïve forwarding of the complete input, our compact embeddings significantly
    minimize the size of kv-cache, thus optimizing the computation process. Finally,
    the memory requirement is further reduced from linear to logarithmic with respect
    to the input length, thanks to the optimized computation ordering described in
    [Section 3.3](#S3.SS3 "3.3 Computation order optimization for memory-limited environments
    ‣ 3 Hierarchical Context Merging"). Additional discussion on the inference speed
    is provided in [Appendix E](#A5 "Appendix E Inference speed analysis").
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们效率提升的第一个来源是分块机制。我们通过在较早的层处理每个块，规避了与自注意力相关的平方计算。令牌减少是我们计算减少的第二个来源。由于我们的算法逐步减少令牌数量，因此在上层处理的令牌更少，从而降低了计算开销。第三个计算节省来源是HOMER输出简洁的嵌入，优化了生成阶段后续的自注意力计算。与简单地转发完整输入相比，我们的紧凑嵌入显著减少了kv-cache的大小，从而优化了计算过程。最后，由于在[第3.3节](#S3.SS3
    "3.3 内存受限环境中的计算顺序优化 ‣ 3 层次上下文合并")中描述的优化计算顺序，内存需求从线性减少到对输入长度的对数。关于推理速度的更多讨论请参见[附录E](#A5
    "附录E 推理速度分析")。
- en: 5 Conclusion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduced Hierarchical cOntext MERging (HOMER), a novel method
    that efficiently addresses the context limit issue inherent in large language
    models (LLMs). By employing a strategic divide-and-conquer technique, HOMER prunes
    redundant tokens, creating compact embeddings while maintaining the richness of
    information. This approach, validated by our experiments, has proven to be memory-efficient
    and effective, enabling the handling of extended contexts up to 64k tokens with
    significantly reduced memory requirements.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了Hierarchical cOntext MERging（HOMER），一种有效解决大型语言模型（LLMs）固有的上下文限制问题的新方法。通过采用战略性的分而治之技术，HOMER修剪冗余令牌，创建紧凑的嵌入，同时保持信息的丰富性。经过实验验证，这种方法被证明是内存高效且有效的，能够处理多达64k令牌的扩展上下文，内存需求显著减少。
- en: Limitations and future work. Although our work focuses on training-free extension
    of context limit, there is no fundamental limit on our method making finetuning
    impossible. We believe that further improving our method with small-data finetuning
    can additionally boost performance, and the resulting model would enjoy both the
    extended context limit and reduced memory requirements.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与未来工作。尽管我们的工作重点是无训练上下文限制的扩展，但我们的技术并没有根本限制使得微调不可能。我们相信，通过小数据微调进一步改进我们的方法可以额外提升性能，结果模型将享受扩展的上下文限制和减少的内存需求。
- en: Ethics Statement
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: While large language models (LLMs) have become a new paradigm of AI academia
    and business industry by showing remarkable contributions, their critical limitations
    still remain such as hallucination, biased content generation, and unintended
    toxicity. The proposed research on long context windows does not address this
    limitation directly. The influences of longer context limits on the LLM limitations
    should be more explored, which is one of significant future research directions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）通过展示显著贡献已成为人工智能学术界和商业行业的新范式，但其关键限制仍然存在，如幻觉、偏见内容生成和非预期的毒性。提出的长上下文窗口研究并未直接解决这一限制。应更多地探讨较长上下文限制对LLM限制的影响，这是未来研究的重要方向之一。
- en: Reproducibility Statement
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重复性声明
- en: We outline the implementation details (including detailed algorithm, prompt
    design, and hyperparameters) and experiment setups (including tasks, datasets,
    and metrics) in [Section 4](#S4 "4 Experiments") and [Appendix B](#A2 "Appendix
    B Implementation details"). We also release the source codes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4节](#S4 "4 Experiments")和[附录B](#A2 "Appendix B Implementation details")中概述了实现细节（包括详细算法、提示设计和超参数）和实验设置（包括任务、数据集和指标）。我们还发布了源代码。
- en: Acknowledgements and Disclosure of Funding
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢与资助声明
- en: This work was supported by Institute of Information & communications Technology
    Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075,
    Artificial Intelligence Graduate School Program (KAIST); No.2021-0-02068, Artificial
    Intelligence Innovation Hub; No.2022-0-00959, Few-shot Learning of Casual Inference
    in Vision and Language for Decision Making).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了由韩国政府（MSIT）资助的“信息与通信技术规划与评估研究所（IITP）”资助（编号：2019-0-00075，人工智能研究生院项目（KAIST）；编号：2021-0-02068，人工智能创新中心；编号：2022-0-00959，视觉和语言决策中的因果推断的少样本学习）。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 等人（2020）Iz Beltagy、马修·E·彼得斯和阿尔曼·科汉。Longformer：长文档变换器。*arXiv 预印本 arXiv:2004.05150*，2020。
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. Unlimiformer: Long-range transformers with unlimited length input. *arXiv
    preprint arXiv:2305.01625*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsch 等人（2023）阿曼达·伯奇、乌里·阿龙、格雷厄姆·纽比格和马修·R·戈姆利。Unlimiformer：具有无限长度输入的长范围变换器。*arXiv
    预印本 arXiv:2305.01625*，2023。
- en: bloc97 (2023) bloc97. Ntk-aware scaled rope allows llama models to have extended
    (8k+) context size without any fine-tuning and minimal perplexity degradation.
    [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_llama_models_to_have/),
    2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bloc97（2023）bloc97。NTK感知缩放绳索允许llama模型在没有任何微调和最小困惑度下降的情况下具有扩展的（8k+）上下文大小。 [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_%20scaled_rope_allows_llama_models_to_have/)，2023。
- en: 'Bolya et al. (2022) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster.
    *arXiv preprint arXiv:2210.09461*, 2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolya 等人（2022）丹尼尔·博利亚、傅成阳、戴晓亮、张佩钊、克里斯托夫·费希特霍夫和朱迪·霍夫曼。标记合并：你的 vit 但更快。*arXiv
    预印本 arXiv:2210.09461*，2022。
- en: Chen (2022) Carol Chen. Transformer inference arithmetic. [https://kipp.ly/blog/transformer-inference-arithmetic/](https://kipp.ly/blog/transformer-inference-arithmetic/),
    2022.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈（2022）卡罗尔·陈。变换器推理算术。 [https://kipp.ly/blog/transformer-inference-arithmetic/](https://kipp.ly/blog/transformer-inference-arithmetic/)，2022。
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023）寿远·陈、舍曼·黄、梁健·陈和袁东·田。通过位置插值扩展大语言模型的上下文窗口。*arXiv 预印本 arXiv:2306.15595*，2023。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*,
    2019.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, 和 Ilya Sutskever.
    用稀疏变换器生成长序列。 *arXiv预印本 arXiv:1904.10509*，2019。
- en: Choromanski et al. (2021) Krzysztof Choromanski, Valerii Likhosherstov, David
    Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
    Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. *arXiv
    preprint arXiv:2009.14794*, 2021.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski et al. (2021) Krzysztof Choromanski, Valerii Likhosherstov, David
    Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
    Mohiuddin, Lukasz Kaiser 等人。重新思考与表现者的注意力。 *arXiv预印本 arXiv:2009.14794*，2021。
- en: 'Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V
    Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond
    a fixed-length context. *arXiv preprint arXiv:1901.02860*, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc
    V Le, 和 Ruslan Salakhutdinov. Transformer-xl：超越固定长度上下文的注意力语言模型。 *arXiv预印本 arXiv:1901.02860*，2019。
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao (2023) Tri Dao. Flashattention-2：更快的注意力机制，通过更好的并行性和工作划分。 *arXiv预印本 arXiv:2307.08691*，2023。
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
    image is worth 16x16 words: Transformers for image recognition at scale. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, 和 Neil Houlsby. 一张图片值
    16x16 个词：大规模图像识别的变换器。 在 *国际学习表征会议*，2021。网址 [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)。
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language
    models. *arXiv preprint arXiv:2308.16137*, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong
    Wang. Lm-infinite：大型语言模型的简单即时长度泛化。 *arXiv预印本 arXiv:2308.16137*，2023。
- en: Haurum et al. (2023) Joakim Bruslund Haurum, Sergio Escalera, Graham W Taylor,
    and Thomas B Moeslund. Which tokens to use? investigating token reduction in vision
    transformers. *arXiv preprint arXiv:2308.04657*, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haurum et al. (2023) Joakim Bruslund Haurum, Sergio Escalera, Graham W Taylor,
    和 Thomas B Moeslund. 选择使用哪些令牌？探讨视觉变换器中的令牌减少。 *arXiv预印本 arXiv:2308.04657*，2023。
- en: Ivgi et al. (2023) Maor Ivgi, Uri Shaham, and Jonathan Berant. Efficient long-text
    understanding with short-text models. *Transactions of the Association for Computational
    Linguistics*, 11:284–299, 2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivgi et al. (2023) Maor Ivgi, Uri Shaham, 和 Jonathan Berant. 使用短文本模型进行高效长文本理解。
    *计算语言学协会会刊*，11:284–299，2023。
- en: Izacard & Grave (2020) Gautier Izacard and Edouard Grave. Leveraging passage
    retrieval with generative models for open domain question answering. *arXiv preprint
    arXiv:2007.01282*, 2020.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard & Grave (2020) Gautier Izacard 和 Edouard Grave. 利用生成模型进行开放域问答的段落检索。
    *arXiv预印本 arXiv:2007.01282*，2020。
- en: kaiokendev (2023) kaiokendev. Things i’m learning while training superhot. [https://kaiokendev.github.io/til#extending-context-to-8k./](https://kaiokendev.github.io/til#extending-context-to-8k./),
    2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kaiokendev (2023) kaiokendev. 在训练超级热度模型时我学到的东西。 [https://kaiokendev.github.io/til#extending-context-to-8k./](https://kaiokendev.github.io/til#extending-context-to-8k./)，2023。
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *International Conference on Machine Learning*, 2020.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    和 François Fleuret. 变换器就是 RNN：具有线性注意力的快速自回归变换器。 在 *国际机器学习会议*，2020。
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya. Reformer：高效变换器。
    *arXiv预印本 arXiv:2001.04451*，2020。
- en: 'Liang et al. (2022) Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue
    Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers
    via token reorganizations. *arXiv preprint arXiv:2202.07800*, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue
    Wang, 和 Pengtao Xie. 不是所有的补丁都是你需要的：通过令牌重组加速视觉变换器。 *arXiv预印本 arXiv:2202.07800*，2022。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity等（2016）Stephen Merity、Caiming Xiong、James Bradbury和Richard Socher. 指针哨兵混合模型.
    *arXiv预印本 arXiv:1609.07843*, 2016.
- en: 'Mohtashami & Jaggi (2023) Amirkeivan Mohtashami and Martin Jaggi. Landmark
    attention: Random-access infinite context length for transformers. *arXiv preprint
    arXiv:2305.16300*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mohtashami & Jaggi（2023）Amirkeivan Mohtashami和Martin Jaggi. Landmark attention:
    变换器的随机访问无限上下文长度. *arXiv预印本 arXiv:2305.16300*, 2023.'
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. Gpt-4技术报告. *arXiv预印本 arXiv:2303.08774*, 2023.
- en: 'Pang et al. (2021) Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita
    Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson,
    He He, et al. Quality: Question answering with long input texts, yes! *arXiv preprint
    arXiv:2112.08608*, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang等（2021）Richard Yuanzhe Pang、Alicia Parrish、Nitish Joshi、Nikita Nangia、Jason
    Phang、Angelica Chen、Vishakh Padmakumar、Johnny Ma、Jana Thompson、He He等. Quality:
    使用长输入文本的问答，没问题! *arXiv预印本 arXiv:2112.08608*, 2021.'
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. *arXiv preprint
    arXiv:2309.00071*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng等（2023）Bowen Peng、Jeffrey Quesnelle、Honglu Fan和Enrico Shippole. Yarn: 大型语言模型的高效上下文窗口扩展.
    *arXiv预印本 arXiv:2309.00071*, 2023.'
- en: Qiu et al. (2019) Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong
    Wang, and Jie Tang. Blockwise self-attention for long document understanding.
    *arXiv preprint arXiv:1911.02972*, 2019.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu等（2019）Jiezhong Qiu、Hao Ma、Omer Levy、Scott Wen-tau Yih、Sinong Wang和Jie Tang.
    用于长文档理解的块状自注意力. *arXiv预印本 arXiv:1911.02972*, 2019.
- en: Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P
    Lillicrap. Compressive transformers for long-range sequence modelling. *arXiv
    preprint arXiv:1911.05507*, 2019.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae等（2019）Jack W Rae、Anna Potapenko、Siddhant M Jayakumar和Timothy P Lillicrap.
    压缩变换器用于长距离序列建模. *arXiv预印本 arXiv:1911.05507*, 2019.
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. *arXiv
    preprint arXiv:2305.14196*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham等（2023）Uri Shaham、Maor Ivgi、Avia Efrat、Jonathan Berant和Omer Levy. Zeroscrolls:
    用于长文本理解的零样本基准. *arXiv预印本 arXiv:2305.14196*, 2023.'
- en: Su (2023) Jianlin Su. Rectified rotary position embeddings. [https://github.com/bojone/rerope](https://github.com/bojone/rerope),
    2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su（2023）Jianlin Su. 纠正的旋转位置嵌入. [https://github.com/bojone/rerope](https://github.com/bojone/rerope),
    2023.
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su等（2021）Jianlin Su、Yu Lu、Shengfeng Pan、Ahmed Murtadha、Bo Wen和Yunfeng Liu.
    Roformer: 带有旋转位置嵌入的增强型变换器. *arXiv预印本 arXiv:2104.09864*, 2021.'
- en: 'Tay et al. (2022) Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung,
    William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald
    Metzler. Scaling laws vs model architectures: How does inductive bias influence
    scaling? *arXiv preprint arXiv:2207.10551*, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tay等（2022）Yi Tay、Mostafa Dehghani、Samira Abnar、Hyung Won Chung、William Fedus、Jinfeng
    Rao、Sharan Narang、Vinh Q Tran、Dani Yogatama和Donald Metzler. 规模定律与模型架构: 归纳偏置如何影响规模?
    *arXiv预印本 arXiv:2207.10551*, 2022.'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等（2023）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale等. Llama
    2: 开放基础和微调聊天模型. *arXiv预印本 arXiv:2307.09288*, 2023.'
- en: 'Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
    Hao Ma. Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*,
    2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等（2020）Sinong Wang、Belinda Z Li、Madian Khabsa、Han Fang和Hao Ma. Linformer:
    线性复杂度的自注意力. *arXiv预印本 arXiv:2006.04768*, 2020.'
- en: Wang et al. (2023) Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    *arXiv preprint arXiv:2306.07174*, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023）Weizhi Wang、Li Dong、Hao Cheng、Xiaodong Liu、Xifeng Yan、Jianfeng Gao和Furu
    Wei. 通过长期记忆增强语言模型. *arXiv预印本 arXiv:2306.07174*, 2023.
- en: Wu et al. (2022) Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian
    Szegedy. Memorizing transformers. *arXiv preprint arXiv:2203.08913*, 2022.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2022）Yuhuai Wu、Markus N Rabe、DeLesley Hutchins和Christian Szegedy. 记忆变换器.
    *arXiv预印本 arXiv:2203.08913*, 2022.
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Neural Information
    Processing Systems*, 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaheer 等人 (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang 等。大鸟：用于更长序列的变换器。*神经信息处理系统*，2020。
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. Calibrate before use: Improving few-shot performance of language models.
    In *International Conference on Machine Learning*, pp.  12697–12706\. PMLR, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, 和 Sameer Singh.
    使用前校准：提高语言模型的少量样本性能。在 *国际机器学习大会*，第 12697–12706 页。PMLR, 2021。
- en: \etocdepthtag
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: \etocdepthtag
- en: .tocmtappendix \etocsettagdepthmtchapternone \etocsettagdepthmtappendixsection
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: .tocmtappendix \etocsettagdepthmtchapternone \etocsettagdepthmtappendixsection
- en: Contents
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1)'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#S1)'
- en: '[2 Related Work](#S2)'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 相关工作](#S2)'
- en: '[3 Hierarchical Context Merging](#S3)'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 层次化上下文合并](#S3)'
- en: '[3.1 Hierarchical merging of context embeddings](#S3.SS1 "In 3 Hierarchical
    Context Merging")'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 上下文嵌入的层次化合并](#S3.SS1 "在 3 层次化上下文合并中")'
- en: '[3.2 Propagative refinement of lower-layer embeddings](#S3.SS2 "In 3 Hierarchical
    Context Merging")'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 下层嵌入的传播性精炼](#S3.SS2 "在 3 层次化上下文合并中")'
- en: '[3.3 Computation order optimization for memory-limited environments](#S3.SS3
    "In 3 Hierarchical Context Merging")'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 面向内存受限环境的计算顺序优化](#S3.SS3 "在 3 层次化上下文合并中")'
- en: '[4 Experiments](#S4)'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 实验](#S4)'
- en: '[4.1 Passkey retrieval](#S4.SS1 "In 4 Experiments")'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 密钥检索](#S4.SS1 "在 4 实验中")'
- en: '[4.2 Question answering](#S4.SS2 "In 4 Experiments")'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 问答](#S4.SS2 "在 4 实验中")'
- en: '[4.3 Language modeling](#S4.SS3 "In 4 Experiments")'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 语言建模](#S4.SS3 "在 4 实验中")'
- en: '[4.4 Ablation studies](#S4.SS4 "In 4 Experiments")'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 消融研究](#S4.SS4 "在 4 实验中")'
- en: '[4.5 Computational efficiency](#S4.SS5 "In 4 Experiments")'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5 计算效率](#S4.SS5 "在 4 实验中")'
- en: '[5 Conclusion](#S5)'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 结论](#S5)'
- en: '[A Memory-efficient computation order](#A1)'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 内存高效计算顺序](#A1)'
- en: '[A.1 Preliminaries](#A1.SS1 "In Appendix A Memory-efficient computation order")'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.1 初步知识](#A1.SS1 "在附录 A 内存高效计算顺序中")'
- en: '[A.2 Proof](#A1.SS2 "In Appendix A Memory-efficient computation order")'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.2 证明](#A1.SS2 "在附录 A 内存高效计算顺序中")'
- en: '[B Implementation details](#A2)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 实现细节](#A2)'
- en: '[C Prompts for downstream tasks](#A3)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 下游任务的提示](#A3)'
- en: '[D Illustration of propagative refinement](#A4)'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 传播性精炼的示意图](#A4)'
- en: '[E Inference speed analysis](#A5)'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 推理速度分析](#A5)'
- en: '[F Perplexity plot for language modeling experiment](#A6)'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 语言建模实验的困惑度图](#A6)'
- en: '[G Perplexity evaluation on downstream tasks](#A7)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G 下游任务的困惑度评估](#A7)'
- en: Appendix A Memory-efficient computation order
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 内存高效计算顺序
- en: In this section, we outline the proof for the logarithmic memory requirement
    of the proposed memory-efficient computation ordering suggested in [Section 3.3](#S3.SS3
    "3.3 Computation order optimization for memory-limited environments ‣ 3 Hierarchical
    Context Merging").
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了在 [第 3.3 节](#S3.SS3 "3.3 面向内存受限环境的计算顺序优化 ‣ 3 层次化上下文合并") 提出的内存高效计算顺序的对数内存需求的证明。
- en: '![Refer to caption](img/e9fcda4513a9a9ec4db79381b42169d7.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e9fcda4513a9a9ec4db79381b42169d7.png)'
- en: 'Figure 3: Hierarchical context merging process conceptualized as a binary tree.
    The top-left numbers of each node denote the memory-efficient computation order.
    Note that propagative refinement must be applied after processing each node to
    enjoy the optimized memory usage.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：将层次化上下文合并过程概念化为二叉树。每个节点的左上角数字表示内存高效计算顺序。请注意，在处理每个节点后必须应用传播性精炼，以实现优化的内存使用。
- en: A.1 Preliminaries
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 初步知识
- en: Problem setup. We conceptualize the hierarchical context merging process as
    a binary tree. For example, [Figure 3](#A1.F3 "In Appendix A Memory-efficient
    computation order") illustrates a merging process with 4 input chunks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 问题设置。我们将层次化上下文合并过程概念化为二叉树。例如，[图 3](#A1.F3 "在附录 A 内存高效计算顺序中")展示了一个包含 4 个输入块的合并过程。
- en: Constants. $L_{i}$ is the maximum chunk size. $M$ is the memory required for
    storing a key-value pair for a single token in a single layer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 常量。$L_{i}$ 是最大块大小。$M$ 是存储单层中单个标记的键值对所需的内存。
- en: Remarks. As the chunk size is bounded, the memory required for forwarding a
    single chunk through a single layer can be treated as constant. Therefore, it
    suffices to consider the memory required for storing the key-value pairs at each
    layer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：由于块大小是有界的，通过单层转发一个块所需的内存可以视为常量。因此，仅需考虑每层存储键值对所需的内存。
- en: Let $\texttt{FinalMem}(h)$ is bounded as follows.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\texttt{FinalMem}(h)$ 的上界如下。
- en: '|  | $\texttt{FinalMem}(h)=\frac{C}{2}\times\sum_{i=0}^{h}L_{i}\times M\leq\frac{1}{2}LCM$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{FinalMem}(h)=\frac{C}{2}\times\sum_{i=0}^{h}L_{i}\times M\leq\frac{1}{2}LCM$
    |  |'
- en: A.2 Proof
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 证明
- en: Proposition. Let $\texttt{PeakMem}(h)$. Then,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 命题：设 $\texttt{PeakMem}(h)$。那么，
- en: '|  | $\texttt{PeakMem}(h)\leq\left(\frac{1}{2}h+1\right)LCM.$ |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{PeakMem}(h)\leq\left(\frac{1}{2}h+1\right)LCM.$ |  |'
- en: We prove the proposition using induction. First, consider the leaf node where
    $h=0$ layers, the peak memory usage is given as follows, proving the base case.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过归纳法证明这个命题。首先，考虑叶节点，其中 $h=0$ 层，峰值内存使用如下所示，证明基本情况。
- en: '|  | $\texttt{PeakMem}(0)=L_{0}CM\leq LCM$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{PeakMem}(0)=L_{0}CM\leq LCM$ |  |'
- en: Now consider a non-leaf node with  | - | - | - |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Plain | 4k | 23.12 | 13.013 | $$> | - | - | - |'
- en: '| Plain + HOMER | None | 23.22 | 29.882 | 23.932 | 20.897 | 7.119 | 4.085 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Plain + HOMER | None | 23.22 | 29.882 | 23.932 | 20.897 | 7.119 | 4.085 |'
- en: '| YaRN | 8k | 23.414 | 17.338 | 23.394 |  | - |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | 16k | 28.183 | 19.07 | 15.366 | $$> | - |'
- en: '|  | 32k | 19.248 | 20.491 | 28.034 | 17.187 | 31.017 |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | 64k | 22.031 | 16.963 | 15.481 | 36.760 | 38.786 |  |'
- en: '| YaRN + HOMER | None | 23.414 | 17.232 | 22.263 | 11.317 | 7.618 | 2.412 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| YaRN + HOMER | None | 23.414 | 17.232 | 22.263 | 11.317 | 7.618 | 2.412 |'
- en: In this section, we provide additional perplexity experiments on a more challenging
    benchmark where accessing previous long contexts is essential. To achieve this,
    we reformulated the passkey retrieval task in [Section 4.1](#S4.SS1 "4.1 Passkey
    retrieval ‣ 4 Experiments") and measured the perplexity of ground-truth answer
    phrases (e.g., ’The passkey is 12321.’). The results are demonstrated in [Figure 6](#A7.F6
    "In Appendix G Perplexity evaluation on downstream tasks") and [Table 9](#A7.T9
    "In Appendix G Perplexity evaluation on downstream tasks").
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了在更具挑战性的基准上进行的额外困惑度实验，其中访问先前长上下文是至关重要的。为此，我们重新制定了[第 4.1 节](#S4.SS1
    "4.1 密钥检索 ‣ 4 实验")中的密钥检索任务，并测量了真实答案短语的困惑度（例如，‘密钥是 12321。’）。结果展示在[图 6](#A7.F6 "在附录
    G 中评估下游任务的困惑度")和[表 9](#A7.T9 "在附录 G 中评估下游任务的困惑度")中。
- en: As the results show, HOMER exhibits lower perplexity when conditioned on longer
    contexts, achieving its best performance with 64k inputs. Furthermore, HOMER outperforms
    the long-context competitors with context lengths of 16k and beyond. These experiments
    emphasize the efficacy of HOMER in utilizing long contexts, particularly in scenarios
    where accessing such context is necessary.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，当条件基于较长的上下文时，HOMER 展现出更低的困惑度，并在 64k 输入时取得了最佳表现。此外，HOMER 在上下文长度达到 16k 及以上时，优于长上下文竞争对手。这些实验强调了
    HOMER 在利用长上下文中的有效性，特别是在需要访问此类上下文的场景中。
