- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Structured Packing in LLM Training Improves Long Context Utilization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化打包在大语言模型训练中提升了长上下文的利用率
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17296](https://ar5iv.labs.arxiv.org/html/2312.17296)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17296](https://ar5iv.labs.arxiv.org/html/2312.17296)
- en: Konrad Staniszewski    Szymon Tworkowski    Yu Zhao    Sebastian Jaszczur   
    Henryk Michalewski    Łukasz Kuciński    Piotr Miłoś
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Konrad Staniszewski    Szymon Tworkowski    Yu Zhao    Sebastian Jaszczur   
    Henryk Michalewski    Łukasz Kuciński    Piotr Miłoś
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent developments in long-context large language models have attracted considerable
    attention. Yet, their real-world applications are often hindered by ineffective
    context information use. This work shows that structuring training data to increase
    semantic interdependence is an effective strategy for optimizing context utilization.
    To this end, we introduce Structured Packing for Long Context (SPLiCe), a method
    for creating training examples by using information retrieval methods to collate
    mutually relevant documents into a single training context. We empirically validate
    SPLiCe on large $3$B models, showing perplexity improvements and better long-context
    utilization on downstream tasks. Remarkably, already relatively short fine-tuning
    with SPLiCe is enough to attain these benefits. Additionally, the comprehensive
    study of SPLiCe reveals intriguing transfer effects such as training on code data
    leading to perplexity improvements on text data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，长上下文大语言模型的进展引起了广泛关注。然而，它们在实际应用中常常受到上下文信息使用不有效的阻碍。这项工作表明，通过结构化训练数据以增加语义依赖性是优化上下文利用的有效策略。为此，我们介绍了长上下文结构化打包（SPLiCe），这是一种通过信息检索方法将互相关联的文档整合到一个训练上下文中来创建训练示例的方法。我们在大型$3$B模型上对SPLiCe进行了实证验证，显示出困惑度的改善和下游任务中的长上下文利用效果更佳。值得注意的是，已经相对短暂的SPLiCe微调就足以获得这些好处。此外，对SPLiCe的全面研究揭示了有趣的迁移效应，例如在代码数据上的训练导致文本数据上的困惑度改善。
- en: Machine Learning, Language Models, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、语言模型、ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) (Brown et al., [2020](#bib.bib7); Chowdhery et al.,
    [2022](#bib.bib10); Lewkowycz et al., [2022](#bib.bib24); OpenAI, [2023b](#bib.bib30);
    Bai et al., [2023](#bib.bib3)) have transformed the field of AI. Recently, the
    field has observed the rise of Long-Context Language Models (LCLMs) that promise
    to unveil novel and powerful capabilities (Anthropic, [2023](#bib.bib1); OpenAI,
    [2023a](#bib.bib29)). However, their ability to process long context is not always
    as effective as one hopes for. Indeed, several studies have highlighted an important
    limitation: when processing prompts composed of multiple documents, LCLMs frequently
    encounter difficulties in accurately extracting relevant information (Tworkowski
    et al., [2023](#bib.bib41); Liu et al., [2023](#bib.bib28); Shi et al., [2023a](#bib.bib36)).
    Additionally, they typically find it challenging to utilize information from the
    middle of their inputs (Liu et al., [2023](#bib.bib28)), even on simple synthetic
    retrieval tasks (Li et al., [2023a](#bib.bib25)). Understanding these issues is
    vital for advancements in LCLM technologies and calls for systematic research.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs） (Brown et al., [2020](#bib.bib7); Chowdhery et al., [2022](#bib.bib10);
    Lewkowycz et al., [2022](#bib.bib24); OpenAI, [2023b](#bib.bib30); Bai et al.,
    [2023](#bib.bib3)) 已经改变了人工智能领域。最近，领域中出现了长上下文语言模型（LCLMs），它们承诺揭示新颖而强大的能力 (Anthropic,
    [2023](#bib.bib1); OpenAI, [2023a](#bib.bib29))。然而，它们处理长上下文的能力并不总是如人们所希望的那样有效。确实，几项研究突出了一个重要的限制：当处理由多个文档组成的提示时，LCLMs
    经常遇到准确提取相关信息的困难 (Tworkowski et al., [2023](#bib.bib41); Liu et al., [2023](#bib.bib28);
    Shi et al., [2023a](#bib.bib36))。此外，它们通常发现从输入中间部分利用信息也很具挑战性 (Liu et al., [2023](#bib.bib28))，即使在简单的合成检索任务中
    (Li et al., [2023a](#bib.bib25))。理解这些问题对于LCLM技术的发展至关重要，并呼吁系统性的研究。
- en: In this work, we take a step towards better context utilization in LCLMs. We
    focus on training data, keeping other components, such as the architecture and
    training objectives, unchanged. The broad question is *how to organize training
    data to enhance long context capabilities*? Such perspective has received some
    attention recently (Levine et al., [2022](#bib.bib23); Chan et al., [2022](#bib.bib8);
    Shi et al., [2023b](#bib.bib37)), but the problem is far from being solved. The
    central finding of this work is that *structuring training data to increase semantic
    interdependence is an effective strategy towards better long context utilization*.
    We achieve this by introducing and evaluating Structured Packing for Long Context
    (SPLiCe), a method for creating training examples by using retrieval (e.g., BM25
    or repository structure) to collate mutually relevant documents into a single
    training context.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们朝着更好地利用LCLMs的上下文迈出了一步。我们专注于训练数据，保持其他组件（如架构和训练目标）不变。广泛的问题是*如何组织训练数据以增强长上下文能力*？这种观点最近受到了一些关注（Levine等，[2022](#bib.bib23)；Chan等，[2022](#bib.bib8)；Shi等，[2023b](#bib.bib37)），但这个问题远未解决。这项工作的核心发现是*结构化训练数据以增加语义相互依赖性是实现更好长上下文利用的有效策略*。我们通过引入和评估用于长上下文的结构化打包（SPLiCe），即通过使用检索（例如BM25或存储库结构）将相互相关的文档整理到单一训练上下文中，从而实现这一目标。
- en: '![Refer to caption](img/86c9bc61bfe293d443645332d8aa00fd.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86c9bc61bfe293d443645332d8aa00fd.png)'
- en: 'Figure 1: Training examples generated by the standard sampling procedure vs
    proposed by SPLiCe. Similar colors indicate related documents, which could be
    found using a retrieval method (e.g., BM25 or contriver) or metadata (e.g., repository
    structure for code).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：标准采样程序生成的训练示例与SPLiCe提出的训练示例。相似的颜色表示相关文档，这些文档可以通过检索方法（例如BM25或contriver）或元数据（例如代码的存储库结构）找到。
- en: We empirically validate SPLiCe showing that fine-tuning of OpenLLaMA $3$B tokens
    already brings perplexity reduction. This reduction translates to substantial
    improvements in handling long-context information in downstream tasks that require
    retrieval and in-context learning. These tasks include TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)), DBpedia (Lehmann et al., [2015](#bib.bib22)),
    Qasper (Shaham et al., [2022](#bib.bib35); Dasigi et al., [2021](#bib.bib11)),
    HotPotQA (Yang et al., [2018](#bib.bib45)), and the lost-in-the-middle benchmark
    (Liu et al., [2023](#bib.bib28)). We perform a comprehensive study of the design
    choices and properties of SPLiCe, showing that the acquired long-context capabilities
    transfer between modalities, such as code and text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实证验证了SPLiCe，显示OpenLLaMA $3$B tokens的微调已经带来了困惑度的降低。这种降低转化为在需要检索和上下文学习的下游任务中处理长上下文信息的显著改善。这些任务包括TREC（Li
    & Roth，[2002](#bib.bib27)；Hovy等，[2001](#bib.bib18)），DBpedia（Lehmann等，[2015](#bib.bib22)），Qasper（Shaham等，[2022](#bib.bib35)；Dasigi等，[2021](#bib.bib11)），HotPotQA（Yang等，[2018](#bib.bib45)），以及lost-in-the-middle基准（Liu等，[2023](#bib.bib28)）。我们对SPLiCe的设计选择和属性进行了全面研究，显示获得的长上下文能力可以在不同模态之间转移，例如代码和文本。
- en: 'Our contributions can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We comprehensively show that structuring training data is a viable way of improving
    the long context utilization of LLMs. To this end, we introduce SPLiCe, a method
    for creating training examples by using retrieval to collate mutually relevant
    documents into a single training context.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们全面展示了结构化训练数据是提高LLMs长上下文利用的可行方法。为此，我们介绍了SPLiCe，一种通过检索将相互相关的文档整理成单一训练上下文的训练示例创建方法。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We fine-tune OpenLLaMA $3$Bv2 (Geng & Liu, [2023](#bib.bib15)) using SPLiCe,
    showing that it improves perplexity on long context evaluation, in-context learning
    ability, and ability to retrieve relevant data from the context.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用SPLiCe对OpenLLaMA $3$Bv2（Geng & Liu，[2023](#bib.bib15)）进行微调，显示其在长上下文评估、上下文学习能力和从上下文中检索相关数据的能力方面有所改善。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a thorough analysis of the design choices behind SPLiCe, such as
    the number of retrieved documents and the order in which documents are merged
    into a training example.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对SPLiCe背后的设计选择进行了全面分析，例如检索文档的数量和文档合并到训练示例中的顺序。
- en: 2 Method
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: SPLiCe is a method for constructing training examples that enhances the LLMs
    long-context utilization. This translates to improved performance on a range of
    tasks like in-context learning, question answering, in-context information retrieval,
    and long context language modeling. We make the source code available to ensure
    the reproducibility of our results. (For this anonymous submission we share the
    zip file with our code.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe 是一种构建训练示例的方法，旨在增强 LLM 的长上下文利用能力。这转化为在如上下文学习、问题回答、上下文信息检索和长上下文语言建模等任务上的性能提升。我们提供源代码以确保结果的可重复性。（对于此次匿名提交，我们共享包含代码的压缩文件。）
- en: Algorithm 1 SPLiCe training example construction
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SPLiCe 训练示例构建
- en: 'Input:    $D$  while $Q\neq\emptyset$ do        if $d_{i}\in D$'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：    $D$  当 $Q\neq\emptyset$ 时        如果 $d_{i}\in D$
- en: Rationale and intuitions
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 原理与直觉
- en: Capturing long-range dependencies is believed to enhance language modeling and
    retrieval-augmentation (Borgeaud et al., [2022](#bib.bib6)). However, it is an
    open question how to achieve such benefits in pre-training or fine-tuning. The
    primary difficulty comes from the fact that long-range dependencies are rare in
    training data (de Vries, [2023](#bib.bib12)) and diminish with distance. It is
    thus unlikely that a model will learn to utilize long context without more explicit
    guidance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉长距离依赖被认为可以增强语言建模和检索增强（Borgeaud 等人，[2022](#bib.bib6)）。然而，如何在预训练或微调中实现这种益处仍是一个悬而未决的问题。主要难点在于长距离依赖在训练数据中很少出现（de
    Vries，[2023](#bib.bib12)），且随距离增加而减少。因此，在没有更多明确指导的情况下，模型不太可能学会利用长上下文。
- en: Recent studies indicate that structuring data, i.e., going beyond the i.i.d.
    paradigm, might be beneficial or even necessary to achieve good long-context performance.
    (Levine et al., [2022](#bib.bib23)) develops a theory showing that the trained
    model establishes stronger dependencies between text segments in the same training
    example. (Chan et al., [2022](#bib.bib8)) indicates that specific distributional
    properties of the training data have a significant impact on the model’s in-context
    performance. Finally, (Shi et al., [2023b](#bib.bib37)) shows that training on
    structured data improves the performance of LLMs on long-context tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，结构化数据，即超越 i.i.d. 范式，可能对实现良好的长上下文性能有益甚至必要。（Levine 等人，[2022](#bib.bib23)）提出了一种理论，表明训练模型在同一训练示例中的文本片段之间建立了更强的依赖关系。（Chan
    等人，[2022](#bib.bib8)）指出训练数据的特定分布特性对模型的上下文性能有显著影响。最后，（Shi 等人，[2023b](#bib.bib37)）表明，在结构化数据上训练可以改善
    LLM 在长上下文任务上的表现。
- en: SPLiCe follows these intuitions. Namely, it constructs training examples by
    collating mutually relevant documents to increase the dependency density, thus
    allowing the model to learn to utilize long context. We provide additional insights
    into the distributional properties of SPLiCe created data at the end of this section
    and in Appendix [K](#A11 "Appendix K Data Distribution Property ‣ Structured Packing
    in LLM Training Improves Long Context Utilization").
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe 遵循这些直觉。也就是说，它通过汇总相互相关的文档来构建训练示例，以增加依赖密度，从而使模型能够学会利用长上下文。我们在本节末尾和附录 [K](#A11
    "附录 K 数据分布特性 ‣ 结构化打包在 LLM 训练中改善长上下文利用") 中提供了对 SPLiCe 创建数据的分布特性更多的见解。
- en: Structured Packing for Long Context (SPLiCe)
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结构化打包用于长上下文 (SPLiCe)
- en: We propose SPLiCe, a method that creates training examples by building a tree
    of documents, see Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing
    in LLM Training Improves Long Context Utilization") and Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Structured Packing in LLM Training Improves Long Context
    Utilization") for a general overview.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出 SPLiCe，这是一种通过构建文档树来创建训练示例的方法，详细见算法 [1](#alg1 "算法 1 ‣ 2 方法 ‣ 结构化打包在 LLM
    训练中改善长上下文利用") 和图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 结构化打包在 LLM 训练中改善长上下文利用") 以获得概述。
- en: SPLiCe starts by picking a random document from the dataset to create a single-node
    tree and continues in a breadth-first manner, each time appending top-$k$ similar
    documents from the corpus. The final sequence consists of the tree flattened according
    to a certain tree traversal strategy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe 从数据集中随机选择一个文档来创建一个单节点树，并以广度优先的方式继续，每次从语料库中附加 top-$k$ 相似文档。最终序列由根据某种树遍历策略扁平化的树组成。
- en: The hyperparameter $k$, SPLiCe produces examples that are similar to the ones
    used by retrieval augmented models, e.g., (Borgeaud et al., [2022](#bib.bib6)).
    Note that the dataset must be reasonably curated for SPLiCe to work efficiently.
    For example, a low duplication rate incentivizes the model to utilize long context
    beyond simple copying.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数$k$，SPLiCe生成的示例类似于检索增强模型使用的示例，例如(Borgeaud et al., [2022](#bib.bib6))。请注意，为了使SPLiCe有效工作，数据集必须经过合理策划。例如，低重复率会促使模型利用长上下文，而不仅仅是简单复制。
- en: 'Many possible retrieval methods can be used with SPLiCe (Retrieve function
    in Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing in LLM Training
    Improves Long Context Utilization")). In our experiments, we test the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可以与SPLiCe一起使用的检索方法有很多种 (算法 [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing
    in LLM Training Improves Long Context Utilization")中的Retrieve函数)。在我们的实验中，我们测试了以下方法：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPLiCe Repo: based on additional meta-information about the data, that is the
    repository structure of the code (Repo): we concatenate files using a depth-first
    search algorithm on the directory structure, that is files from the same directory
    are grouped together. A similar method has been pioneered by (Wu et al., [2022](#bib.bib44))
    and proposed in (Shi et al., [2023b](#bib.bib37)) as an interesting future direction.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SPLiCe Repo：基于关于数据的附加元信息，即代码的存储库结构 (Repo)：我们使用深度优先搜索算法在目录结构中拼接文件，即将来自同一目录的文件分组在一起。类似的方法由(Wu
    et al., [2022](#bib.bib44))开创，并在(Shi et al., [2023b](#bib.bib37))中提出作为一个有趣的未来方向。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPLiCe BM25: based on BM25 (Robertson & Zaragoza, [2009](#bib.bib33); Bassani,
    [2023](#bib.bib4)), a standard retrieval method that uses a bag-of-words approach
    to rank documents based on their similarity to a query.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SPLiCe BM25：基于BM25 (Robertson & Zaragoza, [2009](#bib.bib33); Bassani, [2023](#bib.bib4))，这是一种标准检索方法，使用词袋方法根据文档与查询的相似性对文档进行排序。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SPLiCe Cont: based on Contriever-MSMARCO (Cont) (Izacard et al., [2022](#bib.bib19)),
    a recently proposed retrieval method that uses a transformer model to rank documents
    based on their similarity to a query.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SPLiCe Cont：基于Contriever-MSMARCO (Cont) (Izacard et al., [2022](#bib.bib19))，这是一种最近提出的检索方法，它使用变换器模型根据文档与查询的相似性对文档进行排序。
- en: SPLiCe computational efficiency Given the dataset sizes used in training LLMs,
    the computational efficiency plays a non-trivial role. SPLiCe Repo is the fastest
    and easy to implement but requires additional directory structure, i.e., is not
    applicable to general web data. SPLiCe BM25 requires quadratic time index and
    needs dataset chunking to be applicable to large datasets, see Section [3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). SPLiCe Cont requires
    calculating embeddings for each document and retreval based on the cosine similarity.
    The latter step can be done efficiently using fast approximate inner-product search,
    e.g., Faiss (Johnson et al., [2017](#bib.bib20)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe计算效率 鉴于训练LLMs使用的数据集大小，计算效率发挥着重要作用。SPLiCe Repo是最快的且易于实现，但需要额外的目录结构，即不适用于一般的网页数据。SPLiCe BM25需要二次时间索引，并且需要数据集分块以适用于大数据集，见第[3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")节。SPLiCe Cont需要计算每个文档的嵌入，并根据余弦相似性进行检索。后者步骤可以使用快速近似内积搜索高效完成，例如Faiss
    (Johnson et al., [2017](#bib.bib20))。
- en: Baseline The standard approach, commonly used in LLM training pipelines, is
    to randomly sample documents from the dataset and concatenate them to make training
    examples of a desired context. This is known as example packing (Brown et al.,
    [2020](#bib.bib7)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Baseline 标准方法，通常在LLM训练流程中使用，是从数据集中随机抽取文档并将它们拼接起来，生成所需上下文的训练示例。这被称为示例打包 (Brown
    et al., [2020](#bib.bib7))。
- en: Burstiness SPLiCe conjecturally falls into the framework presented in (Chan
    et al., [2022](#bib.bib8)). This paper shows that the distributional properties
    of the training data affect the in-context capabilities of transformer models.
    In particular, it indicates the importance of ”burstiness”, i.e., a flatter frequency
    distribution with a relatively higher mass on the rare, long-tail tokens appearing
    in a sequence . In Appendix [K](#A11 "Appendix K Data Distribution Property ‣
    Structured Packing in LLM Training Improves Long Context Utilization"), we show
    that SPLiCe increases the burstiness of the training data (measured in terms of
    Zipf’s coefficient of token frequency) compared to the baseline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Burstiness SPLiCe 推测性地落入 (Chan 等，[2022](#bib.bib8)) 提出的框架中。该论文显示了训练数据的分布特性如何影响变换器模型的上下文能力。特别地，它指出了“burstiness”的重要性，即在序列中较少、长尾令牌上相对更高的质量的更平坦频率分布。在附录
    [K](#A11 "Appendix K Data Distribution Property ‣ Structured Packing in LLM Training
    Improves Long Context Utilization") 中，我们展示了 SPLiCe 如何在与基线相比提高训练数据的 burstiness（通过
    Zipf 令牌频率系数来衡量）。
- en: 3 Experiments with Medium-Scale Models
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个中等规模模型的实验
- en: In this section, we conduct a comprehensive examination of the impact of document
    packing on long-context performance, using medium-scale models (see Section [4](#S4
    "4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization") for large models results). We demonstrate that structuring data
    with SPLiCe is a generally applicable approach to improve long context performance.
    We study the performance in Section [3.2](#S3.SS2 "3.2 Experimental Results ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") and design choices of SPLiCe in Section [3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对文档打包对长上下文性能的影响进行了全面检查，使用中等规模模型（有关大规模模型结果，请参见第 [4](#S4 "4 Large-Scale
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    节）。我们展示了使用 SPLiCe 结构化数据是一种普遍适用的方法来改善长上下文性能。我们在第 [3.2](#S3.SS2 "3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization") 节研究了性能，并在第 [3.3](#S3.SS3 "3.3 Design Choices
    Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM
    Training Improves Long Context Utilization") 节设计了 SPLiCe 的选择。
- en: 3.1 Experimental Setup
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: In this part, we employ $270$K on a mixture of the original RedPajama data (TogetherComputer,
    [2023](#bib.bib39)) and long context data created using SPLiCe/Baseline. We employ
    the Focused Transformer (FoT) objective (Tworkowski et al., [2023](#bib.bib41))
    for context extension (unless stated otherwise). This approach is motivated by
    practical factors, viz. training with short context length expedites the process,
    while context scaling can be achieved by finetuning on a relatively small amount
    of tokens, as demonstrated by (Chen et al., [2023](#bib.bib9); Tworkowski et al.,
    [2023](#bib.bib41)). Loosely inspired by (Ouyang et al., [2022](#bib.bib31); Rozière
    et al., [2023](#bib.bib34)), in the latter phase, long context data (i.e., prepared
    with SPLiCe) constitutes half of the mixture. This aims to prevent the model from
    overfitting to artificially created long documents.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们在原始 RedPajama 数据 (TogetherComputer，[2023](#bib.bib39)) 和使用 SPLiCe/Baseline
    创建的长上下文数据的混合上使用 $270$K。我们使用 Focused Transformer (FoT) 目标 (Tworkowski 等，[2023](#bib.bib41))
    来扩展上下文（除非另有说明）。这种方法受到实际因素的激励，即短上下文长度的训练加快了过程，而上下文扩展可以通过对相对少量的令牌进行微调来实现，如 (Chen
    等，[2023](#bib.bib9); Tworkowski 等，[2023](#bib.bib41)) 所示。松散地受到 (Ouyang 等，[2022](#bib.bib31);
    Rozière 等，[2023](#bib.bib34)) 的启发，在后期阶段，长上下文数据（即，使用 SPLiCe 准备的）占混合物的一半。这旨在防止模型过度拟合到人为创建的长文档。
- en: In the evaluation, we measure perplexity on held-out portions of the arXiv (Azerbayev
    et al., [2022](#bib.bib2)) and StarCoder (Li et al., [2023b](#bib.bib26)) datasets
    employing a context length of $32$K tokens and truncate those exceeding this length.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估中，我们在保留的 arXiv (Azerbayev 等，[2022](#bib.bib2)) 和 StarCoder (Li 等，[2023b](#bib.bib26))
    数据集上测量困惑度，使用 $32$K 令牌的上下文长度，并截断超过此长度的部分。
- en: We refer to Appendix [A](#A1 "Appendix A Architecture ‣ Structured Packing in
    LLM Training Improves Long Context Utilization") for a detailed description of
    the model architecture. For information regarding the training and evaluation
    data, see Appendix [G](#A7 "Appendix G Data Preparation ‣ Structured Packing in
    LLM Training Improves Long Context Utilization").
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们参考附录[A](#A1 "Appendix A Architecture ‣ Structured Packing in LLM Training
    Improves Long Context Utilization")了解模型架构的详细描述。有关训练和评估数据的信息，请参见附录[G](#A7 "Appendix
    G Data Preparation ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")。
- en: 3.2 Experimental Results
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验结果
- en: 'In this section, we compare SPLiCe against Baseline. We test two retrieval
    approaches for SPLiCe: BM25 (Robertson & Zaragoza, [2009](#bib.bib33); Bassani,
    [2023](#bib.bib4)), and Contriever-MSMARCO (Izacard et al., [2022](#bib.bib19)))
    denoted as SPLiCe BM25 and SPLiCe Cont respectively. In SPLiCe BM25 retrieval
    takes into account the whole content of the document, whereas in SPLiCe Cont we
    use the first $512$; see Section [3.3](#S3.SS3 "3.3 Design Choices Analysis ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") for more details.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将SPLiCe与基线模型进行比较。我们测试了SPLiCe的两种检索方法：BM25（Robertson & Zaragoza，[2009](#bib.bib33)；Bassani，[2023](#bib.bib4)）和Contriever-MSMARCO（Izacard等，[2022](#bib.bib19)），分别表示为SPLiCe BM25和SPLiCe Cont。在SPLiCe BM25中，检索考虑了文档的全部内容，而在SPLiCe Cont中，我们使用前$512$；有关更多细节，请参见第[3.3](#S3.SS3
    "3.3 Design Choices Analysis ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")节。
- en: The results are presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results
    ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), and Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), from which we draw the following conclusions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在表[3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")、表[3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")和表[3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")中，我们从中得出以下结论。
- en: Structuring data improves performance. We observe (see Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")) that all variants
    of SPLiCe outperform Baseline, often significantly, regardless of the training
    and eval dataset. This indicates that the structure of the data is important for
    long-context training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构化提高了性能。我们观察到（见表[3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments
    with Medium-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")），所有SPLiCe的变体在性能上都优于基线模型，通常显著优越，无论是训练还是评估数据集。这表明数据的结构对于长上下文训练是重要的。
- en: The positive effects transfer between domains. We observe (see Table [3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")) positive
    effects when training on code and evaluating on arXiv and when training on web
    data and evaluating on code. This indicates that the positive effects of SPLiCe
    transfer between domains. We conjecture that better data mixtures could bring
    further benefits.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 积极效果在领域之间转移。我们观察到（见表[3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments
    with Medium-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")）在对代码进行训练并在arXiv上评估以及在对网页数据进行训练并在代码上评估时都有积极效果。这表明SPLiCe的积极效果在领域之间转移。我们推测更好的数据混合可能带来进一步的好处。
- en: Retrieval method is of secondary importance. We observe (see Table [3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")) that the
    choice of retrieval method has a minor impact on the results. Typically, SPLiCe BM25
    is slightly better than SPLiCe Cont. On the code data, SPLiCe Repo is on par with
    SPLiCe BM25. We recall that SPLiCe Repo can be applied only to code data, whereas
    SPLiCe BM25 and SPLiCe Cont are more general. In practice, we often observe that
    SPLiCe Cont is faster than SPLiCe BM25 due to the use of Faiss.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 检索方法的重要性次要。我们观察到（见表 [3](#S3.T3 "表 3 ‣ 3.2 实验结果 ‣ 3 中等规模模型的实验 ‣ 在 LLM 训练中结构化打包改进了长上下文利用")），检索方法的选择对结果的影响较小。通常，SPLiCe BM25
    稍微优于 SPLiCe Cont。在代码数据上，SPLiCe Repo 与 SPLiCe BM25 持平。我们回顾到 SPLiCe Repo 只能应用于代码数据，而
    SPLiCe BM25 和 SPLiCe Cont 更为通用。实际上，我们经常观察到 SPLiCe Cont 由于使用 Faiss 比 SPLiCe BM25
    更快。
- en: 'Table 1: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K context on
    a 50/50 mixture of RedPajama (organized in a standard way) and long-context data
    C, C#, Python, Wikipedia, StackExchange prepared using a method of choice (SPLiCe BM25,
    SPLiCe Cont, SPLiCe Repo, Baseline). Baseline denotes organizing long-context
    data in the same way as RedPajama. SPLiCe beats the Baseline often by a large
    margin. The variants of SPLiCe perform similarly, with SPLiCe BM25 being slightly
    better. For detailed results, see Appendix [B](#A2 "Appendix B Detailed Results
    For Medium-Size Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 在 50/50 混合的 RedPajama（以标准方式组织）和长上下文数据 C、C#、Python、Wikipedia、StackExchange
    上的困惑度 ${}_{(\text{相对于 {基准}})}$K 上下文，使用选择的方法（SPLiCe BM25、SPLiCe Cont、SPLiCe Repo、基准）。基准表示以与
    RedPajama 相同的方式组织长上下文数据。SPLiCe 经常显著超越基准。SPLiCe 的变体表现相似，其中 SPLiCe BM25 稍微更好。有关详细结果，请参见附录
    [B](#A2 "附录 B 中等规模模型的详细结果 ‣ 在 LLM 训练中结构化打包改进了长上下文利用")。'
- en: '| Long Context | Method | arXiv | Code | Code & |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | 方法 | arXiv | 代码 | 代码 & |'
- en: '| Data |  |  | Haskell | Python | CUDA | All | arXiv |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 数据 |  |  | Haskell | Python | CUDA | All | arXiv |'
- en: '| C | SPLiCe BM25 | 5.46 [(.09)] | 3.20 [(.17)] | 2.81 [(.12)] | 2.22 [(.11)]
    | 2.94 [(.13)] | 3.10 [(.13)] |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| C | SPLiCe BM25 | 5.46 [(.09)] | 3.20 [(.17)] | 2.81 [(.12)] | 2.22 [(.11)]
    | 2.94 [(.13)] | 3.10 [(.13)] |'
- en: '| SPLiCe Cont | 5.48 [(.07)] | 3.22 [(.15)] | 2.82 [(.11)] | 2.23 [(.10)] |
    2.96 [(.11)] | 3.11 [(.12)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.48 [(.07)] | 3.22 [(.15)] | 2.82 [(.11)] | 2.23 [(.10)] |
    2.96 [(.11)] | 3.11 [(.12)] |'
- en: '| SPLiCe Repo | 5.47 [(.08)] | 3.22 [(.15)] | 2.83 [(.10)] | 2.24 [(.09)] |
    2.96 [(.11)] | 3.12 [(.11)] |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.47 [(.08)] | 3.22 [(.15)] | 2.83 [(.10)] | 2.24 [(.09)] |
    2.96 [(.11)] | 3.12 [(.11)] |'
- en: '|  | Baseline | 5.55 | 3.37 | 2.93 | 2.33 | 3.07 | 3.23 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 基准 | 5.55 | 3.37 | 2.93 | 2.33 | 3.07 | 3.23 |'
- en: '| C# | SPLiCe BM25 | 5.52 [(.13)] | 3.33 [(.25)] | 2.90 [(.17)] | 2.46 [(.19)]
    | 3.11 [(.20)] | 3.26 [(.20)] |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| C# | SPLiCe BM25 | 5.52 [(.13)] | 3.33 [(.25)] | 2.90 [(.17)] | 2.46 [(.19)]
    | 3.11 [(.20)] | 3.26 [(.20)] |'
- en: '| SPLiCe Cont | 5.53[(.12)] | 3.35[(.23)] | 2.91[(.16)] | 2.48[(.17)] | 3.12[(.19)]
    | 3.27[(.19)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.53[(.12)] | 3.35[(.23)] | 2.91[(.16)] | 2.48[(.17)] | 3.12[(.19)]
    | 3.27[(.19)] |'
- en: '| SPLiCe Repo | 5.53 [(.12)] | 3.35 [(.23)] | 2.91 [(.16)] | 2.49 [(.16)] |
    3.12 [(.19)] | 3.27 [(.19)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.53 [(.12)] | 3.35 [(.23)] | 2.91 [(.16)] | 2.49 [(.16)] |
    3.12 [(.19)] | 3.27 [(.19)] |'
- en: '|  | Baseline | 5.65 | 3.58 | 3.07 | 2.65 | 3.31 | 3.46 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | 基准 | 5.65 | 3.58 | 3.07 | 2.65 | 3.31 | 3.46 |'
- en: '| Python | SPLiCe BM25 | 5.47 [(.10)] | 3.25 [(.21)] | 2.53 [(.09)] | 2.41
    [(.15)] | 3.02 [(.15)] | 3.17 [(.15)] |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Python | SPLiCe BM25 | 5.47 [(.10)] | 3.25 [(.21)] | 2.53 [(.09)] | 2.41
    [(.15)] | 3.02 [(.15)] | 3.17 [(.15)] |'
- en: '| SPLiCe Cont | 5.49 [(.08)] | 3.28 [(.18)] | 2.53 [(.09)] | 2.43 [(.13)] |
    3.03 [(.14)] | 3.19 [(.13)] |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.49 [(.08)] | 3.28 [(.18)] | 2.53 [(.09)] | 2.43 [(.13)] |
    3.03 [(.14)] | 3.19 [(.13)] |'
- en: '| SPLiCe Repo | 5.48 [(.09)] | 3.27 [(.19)] | 2.54 [(.08)] | 2.44 [(.12)] |
    3.03 [(.14)] | 3.18 [(.14)] |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.48 [(.09)] | 3.27 [(.19)] | 2.54 [(.08)] | 2.44 [(.12)] |
    3.03 [(.14)] | 3.18 [(.14)] |'
- en: '|  | Baseline | 5.57 | 3.46 | 2.62 | 2.56 | 3.17 | 3.32 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 基准 | 5.57 | 3.46 | 2.62 | 2.56 | 3.17 | 3.32 |'
- en: '| Wikipedia | SPLiCe BM25 | 5.64 [(.09)] | 3.82 [(.15)] | 3.26 [(.11)] | 2.87
    [(.13)] | 3.55 [(.13)] | 3.68 [(.13)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Wikipedia | SPLiCe BM25 | 5.64 [(.09)] | 3.82 [(.15)] | 3.26 [(.11)] | 2.87
    [(.13)] | 3.55 [(.13)] | 3.68 [(.13)] |'
- en: '| SPLiCe Cont | 5.65 [(.08)] | 3.87 [(.10)] | 3.30 [(.07)] | 2.92 [(.08)] |
    3.59 [(.09)] | 3.72 [(.09)] |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.65 [(.08)] | 3.87 [(.10)] | 3.30 [(.07)] | 2.92 [(.08)] |
    3.59 [(.09)] | 3.72 [(.09)] |'
- en: '| Baseline | 5.73 | 3.97 | 3.37 | 3.00 | 3.68 | 3.81 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.73 | 3.97 | 3.37 | 3.00 | 3.68 | 3.81 |'
- en: '| StackExchange | SPLiCe BM25 | 5.07 [(.07)] | 3.88 [(.06)] | 3.32 [(.04)]
    | 2.89 [(.05)] | 3.60 [(.05)] | 3.69 [(.05)] |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| StackExchange | SPLiCe BM25 | 5.07 [(.07)] | 3.88 [(.06)] | 3.32 [(.04)]
    | 2.89 [(.05)] | 3.60 [(.05)] | 3.69 [(.05)] |'
- en: '| SPLiCe Cont | 5.09 [(.05)] | 3.91 [(.03)] | 3.35 [(.01)] | 2.93 [(.01)] |
    3.63 [(.02)] | 3.73 [(.01)] |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.09 [(.05)] | 3.91 [(.03)] | 3.35 [(.01)] | 2.93 [(.01)] |
    3.63 [(.02)] | 3.73 [(.01)] |'
- en: '| Baseline | 5.14 | 3.94 | 3.36 | 2.94 | 3.65 | 3.74 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.14 | 3.94 | 3.36 | 2.94 | 3.65 | 3.74 |'
- en: 'Table 2: Perplexity for training on a $50/50$ data mixture of RedPajama and
    C code. We perform three trainings using different training datesets and report
    the mean and standard deviation.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 在 RedPajama 和 C 代码的 $50/50$ 数据混合上进行训练的困惑度。我们使用不同的训练数据集进行了三次训练，并报告了均值和标准差。'
- en: '| Long Context | Method | arXiv | Code | Code & |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | 方法 | arXiv | 代码 | 代码 & |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Data |  |  | Python | All | arXiv |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 数据 |  |  | Python | 全部 | arXiv |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| C | SPLiCe BM25 | 5.463 $\pm$ .004 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| C | SPLiCe BM25 | 5.463 $\pm$ .004 |'
- en: '| SPLiCe Cont | 5.477 $\pm$ .006 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 5.477 $\pm$ .006 |'
- en: '| SPLiCe Repo | 5.474 $\pm$ .009 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.474 $\pm$ .009 |'
- en: '|  | Baseline | 5.550 $\pm$ .005 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 基准 | 5.550 $\pm$ .005 |'
- en: 'Table 3: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K) using the
    method of CodeLlama (Rozière et al., [2023](#bib.bib34)), YaRN (Peng et al., [2023](#bib.bib32)),
    or left without changes (Naive), as opposed to FoT used in the other experiments.
    For details see Table [13](#A2.T13 "Table 13 ‣ Appendix B Detailed Results For
    Medium-Size Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 使用 CodeLlama (Rozière 等，[2023](#bib.bib34))、YaRN (Peng 等，[2023](#bib.bib32))
    或未做修改 (Naive) 的困惑度 ${}_{(\text{相较于基准的提升})}$K) ，与其他实验中使用的 FoT 相对。详细信息见表 [13](#A2.T13
    "表 13 ‣ 附录 B 中型模型的详细结果 ‣ LLM 训练中的结构化打包提高了长上下文利用率")。'
- en: '| RoPe scale | Training | Method | arXiv | Code | Code & |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RoPe 规模 | 训练 | 方法 | arXiv | 代码 | 代码 & |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| method | data |  |  | Haskell | Python | CUDA | All | arXiv |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据 |  |  | Haskell | Python | CUDA | 全部 | arXiv |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Naive | C# | SPLiCe BM25 | 6.25 [(.08)] | 4.84 [(.19)] | 3.55 [(.11)] | 2.84
    [(.12)] | 3.72 [(.13)] | 3.88 [(.12)] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Naive | C# | SPLiCe BM25 | 6.25 [(.08)] | 4.84 [(.19)] | 3.55 [(.11)] | 2.84
    [(.12)] | 3.72 [(.13)] | 3.88 [(.12)] |'
- en: '| SPLiCe Repo | 6.25 [(.08)] | 4.87 [(.16)] | 3.56 [(.10)] | 2.85 [(.11)] |
    3.74 [(.11)] | 3.89 [(.11)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 6.25 [(.08)] | 4.87 [(.16)] | 3.56 [(.10)] | 2.85 [(.11)] |
    3.74 [(.11)] | 3.89 [(.11)] |'
- en: '| Baseline | 6.33 | 5.03 | 3.66 | 2.96 | 3.85 | 4.00 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 6.33 | 5.03 | 3.66 | 2.96 | 3.85 | 4.00 |'
- en: '| CodeLlama | C# | SPLiCe BM25 | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)]
    | 2.53 [(.05)] | 3.34 [(.06)] | 3.49 [(.06)] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama | C# | SPLiCe BM25 | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)]
    | 2.53 [(.05)] | 3.34 [(.06)] | 3.49 [(.06)] |'
- en: '| SPLiCe Repo | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)] | 2.54 [(.04)] |
    3.35 [(.05)] | 3.50 [(.05)] |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.74 [(.02)] | 4.28 [(.09)] | 3.22 [(.05)] | 2.54 [(.04)] |
    3.35 [(.05)] | 3.50 [(.05)] |'
- en: '| Baseline | 5.76 | 4.37 | 3.27 | 2.58 | 3.40 | 3.55 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.76 | 4.37 | 3.27 | 2.58 | 3.40 | 3.55 |'
- en: '| YaRN | C# | SPLiCe BM25 | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.55
    [(.06)] | 3.37 [(.07)] | 3.52 [(.06)] |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| YaRN | C# | SPLiCe BM25 | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.55
    [(.06)] | 3.37 [(.07)] | 3.52 [(.06)] |'
- en: '| SPLiCe Repo | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.56 [(.05)] |
    3.38 [(.06)] | 3.53 [(.05)] |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 5.77 [(.02)] | 4.32 [(.10)] | 3.24 [(.05)] | 2.56 [(.05)] |
    3.38 [(.06)] | 3.53 [(.05)] |'
- en: '| Baseline | 5.79 | 4.42 | 3.29 | 2.61 | 3.44 | 3.58 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.79 | 4.42 | 3.29 | 2.61 | 3.44 | 3.58 |'
- en: SPLiCe works with various context extension results Our main experiments use
    the Focused Transformer (FoT) approach (Tworkowski et al., [2023](#bib.bib41))
    for context extension. FoT extends the context only in a couple of attention layers
    and does not change the positional embeddings of the remaining layers. This allows
    for computationally efficient long-context fine-tuning. To show that our method
    works more generally, we have also evaluated it using more standard approaches.
    That is, we checked the effectiveness of the method when context extension is
    done in all layers, and parameters of Rotary Positional Encodings are either adjusted
    as in CodeLlama (Rozière et al., [2023](#bib.bib34)), adjusted with YaRN (Peng
    et al., [2023](#bib.bib32)), or left without changes. Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows the results.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe与各种上下文扩展结果兼容。我们的主要实验使用了Focused Transformer（FoT）方法（Tworkowski等，[2023](#bib.bib41)）进行上下文扩展。FoT仅在少数几个注意力层中扩展上下文，并且不改变其余层的位置信息嵌入。这使得长上下文微调在计算上更为高效。为了展示我们的方法更普遍地有效，我们还使用了更标准的方法进行评估。也就是说，我们检查了当上下文扩展在所有层中进行时，方法的有效性，以及Rotary
    Positional Encodings的参数是调整如CodeLlama（Rozière等，[2023](#bib.bib34)）、调整与YaRN（Peng等，[2023](#bib.bib32)）还是保持不变。表[3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")显示了结果。
- en: Training is stable. We prepared three subsets of the C code dataset and ran
    the tested methods on each of them. In Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), we report the mean perplexity and its standard
    deviation. We observe that the differences between the subsets are minimal, which
    indicates training stability and confirms the statistical significance of our
    results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练稳定性。我们准备了C代码数据集的三个子集，并在每个子集上运行了测试方法。在表[3](#S3.T3 "Table 3 ‣ 3.2 Experimental
    Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization")中，我们报告了平均困惑度及其标准差。我们观察到子集之间的差异最小，这表明训练稳定性并确认了我们结果的统计显著性。
- en: 3.3 Design Choices Analysis
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 设计选择分析
- en: Choice of k As already mentioned in the previous section, the choice of the
    retrieval method (BM25 or Contriver) plays a minor role. In Appendix [C.1](#A3.SS1
    "C.1 SPLiCe Parameters ‣ Appendix C Ablations ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), we study the role of $k$ being the best choice.
    We leave for further work a more detailed analysis of this hyperparameter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: k的选择 如前一节所述，检索方法（BM25或Contriver）的选择作用不大。在附录[C.1](#A3.SS1 "C.1 SPLiCe Parameters
    ‣ Appendix C Ablations ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")中，我们研究了$k$的最佳选择。我们将对这个超参数的更详细分析留待进一步研究。
- en: 'Document ordering We discovered that the ordering of the documents collected
    by the retrieval procedure might be a subtle issue. We consider three choices
    of ORDER function in Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"): identity (do not
    permute the documents), reverse (reverse the order of documents) and a random
    shuffle. In Appendix [C.1](#A3.SS1 "C.1 SPLiCe Parameters ‣ Appendix C Ablations
    ‣ Structured Packing in LLM Training Improves Long Context Utilization"), we present
    results for $270$M models showing negligible differences. However, we observed
    differences for large models, elaborated in Section [4.2.4](#S4.SS2.SSS4 "4.2.4
    Shuffling ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), in which random shuffling
    is the best choice for the CodeLlama context extension method.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 文档排序 我们发现，通过检索过程收集的文档排序可能是一个微妙的问题。我们考虑了Algorithm [1](#alg1 "Algorithm 1 ‣ 2
    Method ‣ Structured Packing in LLM Training Improves Long Context Utilization")中的三种ORDER函数选择：identity（不排列文档）、reverse（反转文档顺序）和随机洗牌。在附录[C.1](#A3.SS1
    "C.1 SPLiCe Parameters ‣ Appendix C Ablations ‣ Structured Packing in LLM Training
    Improves Long Context Utilization")中，我们展示了$270$M模型的结果，显示差异可以忽略。然而，我们观察到大模型存在差异，详见第[4.2.4](#S4.SS2.SSS4
    "4.2.4 Shuffling ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")节，其中随机洗牌是CodeLlama上下文扩展方法的最佳选择。
- en: Corpus chunking Performing retrieval from the whole corpus might be cumbersome
    due to its scale. However, we find that chunking the corpus into smaller parts
    still brings improvements. In particular, for the experiments with English Wikipedia
    and BM25 presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3
    Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization"), we used a manageable size of $2$GB.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 语料块处理 从整个语料库中进行检索可能会因其规模而变得繁琐。然而，我们发现将语料库切分成较小的部分仍然能带来改进。特别是，对于在表[3](#S3.T3
    "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")中展示的使用英语维基百科和BM25的实验，我们使用了$2$GB的可管理大小。
- en: 4 Large-Scale Models
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 大规模模型
- en: In this section, we show that SPLiCe can improve the long context performance
    of large-scale language models. To this end, we use $3$B parameter models and
    tasks that test in-context learning (Section [4.2.1](#S4.SS2.SSS1 "4.2.1 In-Context
    Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization")), question answering (Section
    [4.2.2](#S4.SS2.SSS2 "4.2.2 Question Answering ‣ 4.2 Experimental Results ‣ 4
    Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization")), and in-context information retrieval capabilities (Section [4.2.3](#S4.SS2.SSS3
    "4.2.3 Key Retrieval Performance ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization")). We
    provide perplexity results in Appendix [E](#A5 "Appendix E Perplexity Improvements
    ‣ Structured Packing in LLM Training Improves Long Context Utilization").
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了SPLiCe如何提高大规模语言模型的长上下文表现。为此，我们使用了$3$B参数模型和测试上下文学习（第[4.2.1](#S4.SS2.SSS1
    "4.2.1 In-Context Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")节）、问答（第[4.2.2](#S4.SS2.SSS2
    "4.2.2 Question Answering ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣
    Structured Packing in LLM Training Improves Long Context Utilization")节）和上下文信息检索能力（第[4.2.3](#S4.SS2.SSS3
    "4.2.3 Key Retrieval Performance ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization")节）的任务。我们在附录[E](#A5
    "Appendix E Perplexity Improvements ‣ Structured Packing in LLM Training Improves
    Long Context Utilization")中提供了困惑度结果。
- en: We also emphasize that those improvements occur during a relatively short, compared
    to pre-training, fine-tuning. To be more precise, $3$T tokens.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还强调这些改进发生在相对于预训练的较短的微调阶段。更准确地说，$3$T个标记。
- en: 4.1 Experimental Setup
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: For $3$B v2 on 75/25/25 mixture of RedPajama (50) prepared in the standard way,
    StackExchange (25) and C (25) prepared using SPLiCe BM25. StackExchange is part
    of RedPajama (TogetherComputer, [2023](#bib.bib39)), and C data come from StarCoder
    (Li et al., [2023b](#bib.bib26)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$3$B v2，在75/25/25的RedPajama (50)数据集（以标准方式准备）、StackExchange (25)和C (25)数据集（使用SPLiCe BM25准备）。StackExchange是RedPajama的一部分（TogetherComputer，[2023](#bib.bib39)），而C数据来自StarCoder（Li
    et al.，[2023b](#bib.bib26)）。
- en: We train with $32$ with linear warmup and cosine decay, following (Geng & Liu,
    [2023](#bib.bib15)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$32$进行训练，采用线性预热和余弦衰减，参照(Geng & Liu，[2023](#bib.bib15))。
- en: 'Regarding the context extensions method we use FoT (Tworkowski et al., [2023](#bib.bib41)),
    and the CodeLlama (CL) (Rozière et al., [2023](#bib.bib34)) approach. More explicitly,
    we test six models:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上下文扩展方法，我们使用了FoT (Tworkowski et al., [2023](#bib.bib41))，以及CodeLlama (CL)
    (Rozière et al., [2023](#bib.bib34))方法。更具体地说，我们测试了六种模型：
- en: $\{$,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $\{$,
- en: where Baseline denotes the standard data preparation method. Hyperparameter
    details are located in Appendix [A](#A1 "Appendix A Architecture ‣ Structured
    Packing in LLM Training Improves Long Context Utilization").
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，Baseline表示标准数据准备方法。超参数详情见附录[A](#A1 "Appendix A Architecture ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")。
- en: 4.2 Experimental Results
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: We present perplexity measurements in Appendix [E](#A5 "Appendix E Perplexity
    Improvements ‣ Structured Packing in LLM Training Improves Long Context Utilization").
    In particular, following (Anthropic, [2023](#bib.bib1)), we examine perplexity
    improvements in the function of the token position in the document. Our findings
    reveal that SPLiCe models demonstrate superior perplexity across all distances.
    Notably, the improvement in perplexity is more pronounced for tokens positioned
    later in the document, indicating enhanced language modeling capabilities.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附录[E](#A5 "附录 E 困惑度改进 ‣ 结构化打包在LLM训练中改善长上下文利用")中展示了困惑度测量。特别地，参照（Anthropic，[2023](#bib.bib1)），我们检查了文档中令牌位置对困惑度的影响。我们的发现揭示了SPLiCe模型在所有距离上表现出优越的困惑度。特别地，困惑度的改善在文档后期的令牌中更为明显，表明语言建模能力的提升。
- en: For the remainder of this section, we focus on testing the in-context learning,
    question answering and information retrieval abilities on our models using downstream
    tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们专注于通过下游任务测试我们的模型在上下文学习、问答和信息检索能力方面的表现。
- en: 4.2.1 In-Context Learning
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 上下文学习
- en: To evaluate the improvements of in-context learning abilities, we use TREC (Li
    & Roth, [2002](#bib.bib27); Hovy et al., [2001](#bib.bib18)) and DBpedia (Lehmann
    et al., [2015](#bib.bib22)), which are text classification tasks. For TREC, to
    test the long context utilization, we vary the number of in-context examples in
    $\{190,380,780,1560\}$K context length). We sample these examples multiple times
    and measure classification accuracy on test set. In Table [4](#S4.T4 "Table 4
    ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization"), we observe
    that SPLiCe improves the average performance across different context lengths
    and model sizes. We follow a very similar protocol for DBpedia, see Table [5](#S4.T5
    "Table 5 ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization"),
    confirming that SPLiCe improves the context utilization. In Appendix [I](#A9 "Appendix
    I Detailed Accuracy Improvements ‣ Structured Packing in LLM Training Improves
    Long Context Utilization"), we study the distribution of improvements with respect
    to the choice of the in-context examples, showing the domination of SPLiCe.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估上下文学习能力的改进，我们使用TREC（Li & Roth，[2002](#bib.bib27)；Hovy et al., [2001](#bib.bib18)）和DBpedia（Lehmann
    et al., [2015](#bib.bib22)），这些都是文本分类任务。对于TREC，为了测试长上下文的利用，我们变化了上下文示例的数量，范围为$\{190,380,780,1560\}$K上下文长度。我们多次抽样这些示例，并测量测试集上的分类准确度。在表[4](#S4.T4
    "表 4 ‣ 4.2.1 上下文学习 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ 结构化打包在LLM训练中改善长上下文利用")中，我们观察到SPLiCe在不同的上下文长度和模型尺寸下改善了平均性能。我们对DBpedia采用了非常类似的协议，见表[5](#S4.T5
    "表 5 ‣ 4.2.1 上下文学习 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ 结构化打包在LLM训练中改善长上下文利用")，确认SPLiCe改善了上下文利用。在附录[I](#A9
    "附录 I 详细准确度改进 ‣ 结构化打包在LLM训练中改善长上下文利用")中，我们研究了改善与上下文示例选择的分布，展示了SPLiCe的主导作用。
- en: In Table [4](#S4.T4 "Table 4 ‣ 4.2.1 In-Context Learning ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") and Table [5](#S4.T5 "Table 5 ‣ 4.2.1 In-Context Learning
    ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM
    Training Improves Long Context Utilization"), we report mean improvement $\Delta$
    bootstrap confidence intervals. See Appendix [I](#A9 "Appendix I Detailed Accuracy
    Improvements ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    for related histograms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[4](#S4.T4 "表 4 ‣ 4.2.1 上下文学习 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ 结构化打包在LLM训练中改善长上下文利用")和表[5](#S4.T5
    "表 5 ‣ 4.2.1 上下文学习 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ 结构化打包在LLM训练中改善长上下文利用")中，我们报告了均值改进$\Delta$自助法置信区间。有关直方图，请参见附录[I](#A9
    "附录 I 详细准确度改进 ‣ 结构化打包在LLM训练中改善长上下文利用")。
- en: 'Table 4: Average classification performance on TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)). For TREC, we average across $50$ bootstrap confidence
    intervals (see Appendix [I](#A9 "Appendix I Detailed Accuracy Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：TREC上的平均分类性能（Li & Roth，[2002](#bib.bib27)；Hovy et al., [2001](#bib.bib18)）。对于TREC，我们对$50$个自助法置信区间进行平均（见附录[I](#A9
    "附录 I 详细准确度改进 ‣ 结构化打包在LLM训练中改善长上下文利用")）。
- en: '| TREC |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| TREC |'
- en: '| --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Model |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Context &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文 &#124;'
- en: '&#124; (#examples) &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (#examples) &#124;'
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | SPLiCe | $\Delta$ [置信区间] |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $3$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $3$ |'
- en: '| $16$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: '| $8$ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| $8$ |'
- en: '| $4$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| $4$ |'
- en: '| $7$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $16$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: '| $8$ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| $8$ |'
- en: '| $4$ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $4$ |'
- en: '| $7$ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $16$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: '| $8$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $8$ |'
- en: '| $4$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| $4$ |'
- en: 'Table 5: Average performance on DBpedia (Lehmann et al., [2015](#bib.bib22)).
    We average results across $40$ elements of the evaluation set.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: DBpedia上的平均性能 (Lehmann et al., [2015](#bib.bib22))。我们在$40$个评估集元素上计算平均结果。'
- en: '| DBpedia |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| DBpedia |'
- en: '| --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Model |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Context &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文 &#124;'
- en: '&#124; (#examples) &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (#examples) &#124;'
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | SPLiCe | $\Delta$ [置信区间] |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $3$ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $3$ |'
- en: '| $16$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: '| $7$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $16$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: '| $7$ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $16$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $16$ |'
- en: 'Table 6: Average performance on HotPotQA (Yang et al., [2018](#bib.bib45)).
    We average results across $7$ bootstrap confidence intervals.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: HotPotQA上的平均性能 (Yang et al., [2018](#bib.bib45))。我们在$7$个自助法置信区间上计算平均结果。'
- en: '| HotPotQA |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| HotPotQA |'
- en: '| --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Model |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Context &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文 &#124;'
- en: '&#124; (#examples) &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (#examples) &#124;'
- en: '| Baseline | SPLiCe | $\Delta$ [conf interv] |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | SPLiCe | $\Delta$ [置信区间] |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $3$ |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| $3$ |'
- en: '| $7$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: '| $7$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| $7$ |'
- en: 4.2.2 Question Answering
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 问答
- en: We also show that SPLiCe models have improved question answering capabilities.
    To this end, we use Qasper (Dasigi et al., [2021](#bib.bib11); Shaham et al.,
    [2022](#bib.bib35)) and HotPotQA (Yang et al., [2018](#bib.bib45)). Qasper contains
    questions regarding research papers provided as a part of the input context. In
    Table [7](#S4.T7 "Table 7 ‣ 4.2.2 Question Answering ‣ 4.2 Experimental Results
    ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization"), we observe that SPLiCe improves the two-shot performance. Similarly,
    we observe improvements on HotPotQA, see Table [6](#S4.T6 "Table 6 ‣ 4.2.1 In-Context
    Learning ‣ 4.2 Experimental Results ‣ 4 Large-Scale Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"). We stress that in both cases,
    the results measure in-context capabilities as neither of the datasets was used
    during training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了SPLiCe模型在问答方面的改进。为此，我们使用了Qasper (Dasigi et al., [2021](#bib.bib11); Shaham
    et al., [2022](#bib.bib35)) 和 HotPotQA (Yang et al., [2018](#bib.bib45))。Qasper包含有关研究论文的问题，作为输入上下文的一部分。在表
    [7](#S4.T7 "表 7 ‣ 4.2.2 问答 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ LLM训练中的结构化打包改善长上下文利用") 中，我们观察到SPLiCe在双重性能上的提升。类似地，我们在HotPotQA上也观察到了改进，见表
    [6](#S4.T6 "表 6 ‣ 4.2.1 上下文学习 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ LLM训练中的结构化打包改善长上下文利用")。我们强调，在这两种情况下，结果衡量的是上下文能力，因为这两个数据集都未在训练过程中使用。
- en: 'Table 7: Two-shot F1 performance on the validation subset of Qasper (Dasigi
    et al., [2021](#bib.bib11)). For Qasper, we use the implementation from Language
    Model Evaluation Harness (Gao et al., [2021](#bib.bib13)). Note that in the $3$B
    model case, despite using SPLiCe for code data only, we still have improvements
    in non-code tasks.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: Qasper验证子集上的双重 F1 性能 (Dasigi et al., [2021](#bib.bib11))。对于 Qasper，我们使用了来自语言模型评估工具包
    (Gao et al., [2021](#bib.bib13)) 的实现。请注意，在$3$B模型情况下，尽管仅对代码数据使用了SPLiCe，但我们在非代码任务上仍然有改进。'
- en: '| QASPER |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| QASPER |'
- en: '| --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Model |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Context length &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文长度 &#124;'
- en: '&#124; (#examples) &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (#examples) &#124;'
- en: '| Baseline | SPLiCe |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | SPLiCe |'
- en: '| --- | --- | --- | --- |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $3$K (2) | 22.1 | 22.8 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| $3$K (2) | 22.1 | 22.8 |'
- en: '| $7$K (2) | 22.8 | 23.1 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| $7$K (2) | 22.8 | 23.1 |'
- en: '| $7$K (2) | 29.0 | 29.7 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| $7$K (2) | 29.0 | 29.7 |'
- en: 4.2.3 Key Retrieval Performance
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 关键检索性能
- en: The key retrieval task introduced in (Liu et al., [2023](#bib.bib28)) has become
    a routine diagnostic tool to study context capabilities. Specifically, the model
    is presented with a list of key-value pairs, which are $128$-bit UUIDs, and is
    tasked to retrieve the value for a given key. Even though very simple, the task
    proved to be challenging for many open-source language models, see Appendix [D](#A4
    "Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training Improves
    Long Context Utilization") for the task details.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由 (Liu et al., [2023](#bib.bib28)) 引入的关键检索任务已经成为研究上下文能力的常规诊断工具。具体来说，模型会面对一个包含$128$位UUID的键值对列表，并被要求检索给定键的值。尽管任务非常简单，但对许多开源语言模型来说，这个任务却证明是具有挑战性的，详见附录
    [D](#A4 "附录 D 关键-值检索任务 ‣ LLM训练中的结构化打包改善长上下文利用")。
- en: In Figure [2](#S4.F2 "Figure 2 ‣ 4.2.3 Key Retrieval Performance ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization"), we present the key retrieval performance of $7$B models
    trained with CodeLlama (Rozière et al., [2023](#bib.bib34)) context extension
    method. We note that despite relatively short tuning, SPLiCe significantly helps
    on hard-to-retrieve positions. We provide the results for FoT models in Appendix [D](#A4
    "Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training Improves
    Long Context Utilization").
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[2](#S4.F2 "Figure 2 ‣ 4.2.3 Key Retrieval Performance ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization")中，我们展示了使用CodeLlama（Rozière等，[2023](#bib.bib34)）上下文扩展方法训练的$7$B模型的关键检索性能。我们注意到，尽管调整时间相对较短，SPLiCe在难以检索的位置上仍有显著帮助。我们在附录[D](#A4
    "Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training Improves
    Long Context Utilization")中提供了FoT模型的结果。
- en: '![Refer to caption](img/d5a76b85c8d01c548e5cdc928c46a535.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d5a76b85c8d01c548e5cdc928c46a535.png)'
- en: 'Figure 2: Key-value retrieval performance on a dictionary of $300$ examples.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在$300$个示例字典上的键值检索性能。
- en: 4.2.4 Shuffling
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 打乱
- en: In our experiments with large $7$B CodeLlama models, we have noted slight improvements
    in handling long inputs when shuffling documents inside the context, that is,
    setting Order in the Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") to be a random shuffle.
    We provide a comparison on TREC in Table [17](#A6.T17 "Table 17 ‣ Appendix F Shuffling
    ‣ Structured Packing in LLM Training Improves Long Context Utilization").
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对大规模$7$B CodeLlama模型的实验中，我们注意到在上下文中打乱文档时对长输入的处理略有改进，即将算法[1](#alg1 "Algorithm
    1 ‣ 2 Method ‣ Structured Packing in LLM Training Improves Long Context Utilization")中的顺序设置为随机打乱。我们在表[17](#A6.T17
    "Table 17 ‣ Appendix F Shuffling ‣ Structured Packing in LLM Training Improves
    Long Context Utilization")中提供了TREC的比较。
- en: 5 Related Work
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: There is an increasing number of works aiming to study the role of data in LLM
    training in detail. For instance, (Levine et al., [2022](#bib.bib23)) developed
    a theory and demonstrated empirically that incorporating non-adjacent but semantically
    related sentences in training examples leads to better sentence embeddings and
    improves open-domain question-answering performance. Another work (Gu et al.,
    [2023](#bib.bib16)) introduced a pretraining framework grounded on the idea that
    text documents often include intrinsic tasks. They showed that this approach substantially
    boosts in-context learning. Additionally, there is existing work on training long-context
    language models using repository-level code data, such as (Wu et al., [2022](#bib.bib44)).
    A recent work (Chan et al., [2022](#bib.bib8)) identifies the training data’s
    distributional properties that affect transformer models’ in-context capabilities.
    Similarly, (Han et al., [2023](#bib.bib17)) constructs small-scale data using
    an iterative gradient approach and shows that such data improve in-context performance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的研究致力于详细研究数据在LLM训练中的作用。例如，（Levine等，[2022](#bib.bib23)）提出了一种理论，并通过实验证明，训练示例中包含非相邻但语义相关的句子可以获得更好的句子嵌入，并提高开放领域问答性能。另一项工作（Gu等，[2023](#bib.bib16)）介绍了一种基于文本文件通常包含内在任务的想法的预训练框架。他们展示了这种方法显著提升了上下文学习能力。此外，还存在利用库级代码数据训练长上下文语言模型的研究，例如（Wu等，[2022](#bib.bib44)）。最近的研究（Chan等，[2022](#bib.bib8)）确定了影响变换器模型上下文能力的训练数据的分布特性。同样，（Han等，[2023](#bib.bib17)）使用迭代梯度方法构建了小规模数据，并展示了这些数据能够提高上下文性能。
- en: Our methodology diverges from these works in several key ways. Firstly, we focus
    on the document-level context during the training phase, as opposed to sentence-level
    (Levine et al., [2022](#bib.bib23)) or paragraph-level (Gu et al., [2023](#bib.bib16))
    granularity. We demonstrate the efficacy of this approach in large-scale language
    modeling, specifically with OpenLLaMA $3$B. Secondly, we construct a tree structure
    of related documents through BM25/Contriever-MSMARCO retrieval and linearize this
    structure to form long-context examples. This allows for greater control over
    example coherence compared to relying solely on natural data structures like repository-level
    code. The gradient-based method presented in (Han et al., [2023](#bib.bib17))
    can be broadly associated with retrieval used in our work; however, it differs
    in scale and granularity.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法在几个关键方面与这些工作有所不同。首先，我们在训练阶段专注于文档级上下文，而不是句子级（Levine 等，[2022](#bib.bib23)）或段落级（Gu
    等，[2023](#bib.bib16)）粒度。我们展示了这种方法在大规模语言建模中的有效性，特别是使用 OpenLLaMA $3$B。其次，我们通过 BM25/Contriever-MSMARCO
    检索构建了相关文档的树形结构，并将该结构线性化以形成长上下文样本。这使得相对于仅依赖自然数据结构（如代码库级代码），在样本一致性上具有更大的控制。所呈现的基于梯度的方法（Han
    等，[2023](#bib.bib17)）可以广泛关联到我们工作中使用的检索；然而，它在规模和粒度上有所不同。
- en: Concurrently to our research, (Shi et al., [2023b](#bib.bib37)) showed a similar
    method for preparing the training data. However, in our work, we focus on tuning
    the models for long context, up to $32$K tokens. In addition to that, we provide
    ablations regarding context extension methods.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的研究同时进行，（Shi 等，[2023b](#bib.bib37)）展示了一种类似的训练数据准备方法。然而，在我们的工作中，我们专注于调整模型以处理长上下文，最长可达
    $32$K 令牌。此外，我们还提供了关于上下文扩展方法的消融实验。
- en: 6 Limitations and Future Work
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制与未来工作
- en: We show that structuring the training data is a viable way of improving the
    model’s performance. The presented method can viewed as a general framework for
    organizing the documents into training examples. This opens multiple further research
    avenues.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了结构化训练数据是一种有效改善模型性能的方法。所呈现的方法可以视为组织文档成训练样本的通用框架。这为进一步的研究开辟了多个方向。
- en: Degree of relevance In this work, SPLiCe constructed training examples by concatenating
    most matching documents (according to BM25/Cont/Repo). However, recent results
    of (Tworkowski et al., [2023](#bib.bib41)) show that introducing unrelated data
    to the context may help the model learn better representations. We leave the study
    of how the choice of retriever (in particular, the ratio of related and unrelated
    documents) affects performance for future work. Note that as BM25 is a syntactic
    retriever, it is not guaranteed that the documents returned by it are semantically
    related. Moreover, we have not tuned Contriever-MSMARCO, and it was provided only
    with a $512$ token long prefix of a document.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性程度 在这项工作中，SPLiCe 通过连接最匹配的文档（根据 BM25/Cont/Repo）构建训练样本。然而，（Tworkowski 等，[2023](#bib.bib41)）的最新结果表明，将不相关的数据引入上下文可能有助于模型学习更好的表示。我们将如何选择检索器（特别是相关和不相关文档的比例）影响性能的研究留待未来工作。请注意，由于
    BM25 是一种句法检索器，无法保证其返回的文档在语义上相关。此外，我们尚未调整 Contriever-MSMARCO，它仅提供了文档的 $512$ 令牌长前缀。
- en: In order to prepare the training data, SPLiCe uses each document exactly once
    (we mask out each used document for further retrieval). However, it is possible
    that allowing some high-quality documents to occur more than once may be beneficial.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备训练数据，SPLiCe 每个文档仅使用一次（我们屏蔽了每个已使用的文档以供进一步检索）。然而，允许一些高质量文档出现多次可能会有利。
- en: Retrieval granularity Another avenue for future work is to study the granularity
    of the pieces from which the training examples are constructed. In this work,
    we focus on the document-level granularity. However, it is possible to construct
    training examples from smaller pieces, such as paragraphs or sentences.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 检索粒度 未来的另一个研究方向是研究训练样本构建的粒度。在这项工作中，我们关注的是文档级粒度。然而，也可以从更小的单元构建训练样本，例如段落或句子。
- en: Scaling Our method requires further studies to understand its scaling properties
    beyond the range presented in the paper.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展我们的方法需要进一步研究，以理解其在论文中展示范围之外的扩展属性。
- en: Other context extension methods In most cases, our models were trained for a
    long context using the methodology presented in (Tworkowski et al., [2023](#bib.bib41)).
    Additionally, we tested three popular context extension methods on a medium scale
    (Naive, YaRN, and CodeLlama) and tuned a large $7$B model using the CodeLlama
    approach, preliminary confirming the applicability of SPLiCe. However, we leave
    more detailed studies of the design choices of SPLiCe with other context extension
    methods to future work.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其他上下文扩展方法 在大多数情况下，我们的模型是使用（Tworkowski et al., [2023](#bib.bib41)）中提出的方法进行长上下文训练的。此外，我们在中等规模上测试了三种流行的上下文扩展方法（Naive、YaRN
    和 CodeLlama），并使用 CodeLlama 方法调整了一个大规模 $7$B 模型，初步确认了 SPLiCe 的适用性。然而，我们将 SPLiCe
    设计选择与其他上下文扩展方法的更详细研究留待未来工作。
- en: Other data sources One of the approaches to training long-context language models
    is to use conversational data (Li et al., [2023a](#bib.bib25)). This is complementary
    to our method. SPLiCe can utilize data that already exists in vast quantities
    and can be easily applied to different types of text (like code, Wikipedia articles,
    StackExchange questions, answers, etc.) to further increase the number of long-context
    examples. We leave researching how SPLiCe integrates with other methods for preparing
    the long-context data as future work.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据来源 训练长上下文语言模型的一种方法是使用对话数据（Li et al., [2023a](#bib.bib25)）。这与我们的方法是互补的。SPLiCe
    可以利用已经存在的大量数据，并且可以轻松应用于不同类型的文本（如代码、维基百科文章、StackExchange 问题、答案等），以进一步增加长上下文示例的数量。我们将研究
    SPLiCe 如何与其他长上下文数据准备方法集成留待未来工作。
- en: Data curation Using highly correlated samples has the potential to result in
    training instability. However, we noted no performance degradation during our
    experiments. We leave the study of how SPLiCe integrates with different data types
    for the future.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理 使用高度相关的样本可能导致训练不稳定。然而，我们在实验过程中没有发现性能下降。我们将研究 SPLiCe 如何与不同数据类型集成的工作留待未来。
- en: 7 Conclusions
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we present SPLiCe, a novel method of constructing training examples
    for long-context language models, which utilizes BM25/Contriever-MSMARCO to find
    relevant documents and feed them to the model in a structured manner. We show
    that SPLiCe improves the perplexity of the language modeling task and performance
    on downstream tasks. We further show that SPLiCe can be used to improve long-context
    utilization of large-scale models using only short fine-tuning. We believe that
    our work indicates multiple interesting research directions for improving the
    performance of long-context language models with structured data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 SPLiCe，一种构建长上下文语言模型训练示例的新方法，该方法利用 BM25/Contriever-MSMARCO 查找相关文档并以结构化方式将其输入模型。我们展示了
    SPLiCe 改善了语言建模任务的困惑度以及下游任务的表现。我们进一步展示了 SPLiCe 可以通过仅进行短时间微调来改善大规模模型的长上下文利用。我们认为我们的工作表明，利用结构化数据提高长上下文语言模型性能存在多个有趣的研究方向。
- en: 8 Impact Statement
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 影响声明
- en: This paper presents work whose goal is to improve the long context utilization
    ability of language models through structuring training data. It bears standard
    risks associated with advancing the field of LLMs (such as misuse and generation
    of malicious/misleading content). For a detailed overview, we refer to (Bender
    et al., [2021](#bib.bib5); Weidinger et al., [2021](#bib.bib43)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的工作旨在通过结构化训练数据来提高语言模型的长上下文利用能力。这具有推动 LLM 领域发展所固有的标准风险（例如误用和生成恶意/误导性内容）。有关详细概述，请参见（Bender
    et al., [2021](#bib.bib5); Weidinger et al., [2021](#bib.bib43)）。
- en: References
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anthropic (2023) Anthropic. Model card and evaluations for claude models. Technical
    report, Anthropic, 2023. URL [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2023) Anthropic. Claude 模型的模型卡和评估。技术报告，Anthropic，2023。网址 [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf)。
- en: 'Azerbayev et al. (2022) Azerbayev, Z., Piotrowski, B., and Avigad, J. Proofnet:
    A benchmark for autoformalizing and formally proving undergraduate-level mathematics
    problems. In *Advances in Neural Information Processing Systems 35, 2nd MATH-AI
    Workshop at NeurIPS’22*, 2022. URL [https://mathai2022.github.io/papers/20.pdf](https://mathai2022.github.io/papers/20.pdf).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azerbayev 等（2022）Azerbayev, Z., Piotrowski, B., 和 Avigad, J. Proofnet: 一个自动形式化和形式证明本科水平数学问题的基准。在
    *Advances in Neural Information Processing Systems 35, 2nd MATH-AI Workshop at
    NeurIPS’22*，2022。URL [https://mathai2022.github.io/papers/20.pdf](https://mathai2022.github.io/papers/20.pdf)。'
- en: Bai et al. (2023) Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
    Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu,
    D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan,
    S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang,
    H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang,
    X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical
    report, 2023.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2023）Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y.,
    Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D.,
    Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S.,
    Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang,
    H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang,
    X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., 和 Zhu, T. Qwen 技术报告，2023。
- en: 'Bassani (2023) Bassani, E. retriv: A Python Search Engine for the Common Man,
    May 2023. URL [https://github.com/AmenRa/retriv](https://github.com/AmenRa/retriv).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bassani（2023）Bassani, E. retriv: 一款面向普通人的 Python 搜索引擎，2023年5月。URL [https://github.com/AmenRa/retriv](https://github.com/AmenRa/retriv)。'
- en: 'Bender et al. (2021) Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell,
    S. On the dangers of stochastic parrots: Can language models be too big? In *Proceedings
    of the 2021 ACM Conference on Fairness, Accountability, and Transparency*, FAccT
    ’21, pp.  610–623, New York, NY, USA, 2021\. Association for Computing Machinery.
    ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bender 等（2021）Bender, E. M., Gebru, T., McMillan-Major, A., 和 Shmitchell, S.
    关于随机鹦鹉的危险：语言模型是否会过于庞大？在 *2021年ACM公平性、问责制与透明度会议*，FAccT ’21，第610–623页，纽约，NY，美国，2021。计算机协会。ISBN
    9781450383097。doi: 10.1145/3442188.3445922。URL [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)。'
- en: Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas,
    D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones,
    C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero,
    S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models
    by retrieving from trillions of tokens. In *International Conference on Machine
    Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA*, volume 162 of
    *Proceedings of Machine Learning Research*, pp.  2206–2240\. PMLR, 2022. URL [https://proceedings.mlr.press/v162/borgeaud22a.html](https://proceedings.mlr.press/v162/borgeaud22a.html).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等（2022）Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de
    Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore,
    L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O.,
    Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., 和 Sifre, L. 通过从数万亿标记中检索来改进语言模型。在
    *国际机器学习大会，ICML 2022，2022年7月17-23日，马里兰州巴尔的摩，美国*，*机器学习研究会会议录*第162卷，第2206–2240页。PMLR，2022。URL
    [https://proceedings.mlr.press/v162/borgeaud22a.html](https://proceedings.mlr.press/v162/borgeaud22a.html)。
- en: Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners. *CoRR*, abs/2005.14165,
    2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., 和 Amodei, D.
    语言模型是少样本学习者。*CoRR*，abs/2005.14165，2020。URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)。
- en: Chan et al. (2022) Chan, S., Santoro, A., Lampinen, A. K., Wang, J., Singh,
    A., Richemond, P. H., McClelland, J. L., and Hill, F. Data distributional properties
    drive emergent in-context learning in transformers. In *NeurIPS*, 2022.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan et al. (2022) Chan, S., Santoro, A., Lampinen, A. K., Wang, J., Singh,
    A., Richemond, P. H., McClelland, J. L., 和 Hill, F. 数据分布特性驱动变压器中的上下文学习。发表于 *NeurIPS*，2022年。
- en: Chen et al. (2023) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation, 2023.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Chen, S., Wong, S., Chen, L., 和 Tian, Y. 通过位置插值扩展大型语言模型的上下文窗口，2023年。
- en: 'Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with
    pathways, 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P.,
    Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
    N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
    Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat,
    S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L.,
    Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi,
    R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,
    Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X.,
    Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
    D., Dean, J., Petrov, S., 和 Fiedel, N. Palm: 通过路径扩展语言建模，2022年。'
- en: Dasigi et al. (2021) Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A.,
    and Gardner, M. A dataset of information-seeking questions and answers anchored
    in research papers, 2021.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi et al. (2021) Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A.,
    和 Gardner, M. 一个以研究论文为基础的信息检索问答数据集，2021年。
- en: 'de Vries (2023) de Vries, H. In the long (context) run, 2023. URL [https://www.harmdevries.com/post/context-length](https://www.harmdevries.com/post/context-length).
    Accessed: 2023-09-28.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Vries (2023) de Vries, H. 从长远（上下文）来看，2023年。网址 [https://www.harmdevries.com/post/context-length](https://www.harmdevries.com/post/context-length)。访问时间：2023年9月28日。
- en: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, sep 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
    C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 一个用于少量样本语言模型评估的框架，2021年9月。网址
    [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: 'Geng (2023) Geng, X. Easylm: A simple and scalable training framework for large
    language models, March 2023. URL [https://github.com/young-geng/EasyLM](https://github.com/young-geng/EasyLM).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geng (2023) Geng, X. Easylm: 一个简单且可扩展的大型语言模型训练框架，2023年3月。网址 [https://github.com/young-geng/EasyLM](https://github.com/young-geng/EasyLM)。'
- en: 'Geng & Liu (2023) Geng, X. and Liu, H. Openllama: An open reproduction of llama,
    May 2023. URL [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geng & Liu (2023) Geng, X. 和 Liu, H. Openllama: 一个开放的 llama 复现项目, 2023年5月。网址
    [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama)。'
- en: 'Gu et al. (2023) Gu, Y., Dong, L., Wei, F., and Huang, M. Pre-training to learn
    in context. In *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pp.  4849–4870, Toronto, Canada,
    July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.267.
    URL [https://aclanthology.org/2023.acl-long.267](https://aclanthology.org/2023.acl-long.267).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2023) Gu, Y., Dong, L., Wei, F., 和 Huang, M. 预训练以便于上下文学习。发表于 *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*，第4849–4870页，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.acl-long.267。网址
    [https://aclanthology.org/2023.acl-long.267](https://aclanthology.org/2023.acl-long.267)。'
- en: Han et al. (2023) Han, X., Simig, D., Mihaylov, T., Tsvetkov, Y., Celikyilmaz,
    A., and Wang, T. Understanding in-context learning via supportive pretraining
    data, 2023.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2023) Han, X., Simig, D., Mihaylov, T., Tsvetkov, Y., Celikyilmaz,
    A., 和 Wang, T. 通过支持性预训练数据理解上下文学习，2023年。
- en: Hovy et al. (2001) Hovy, E., Gerber, L., Hermjakob, U., Lin, C.-Y., and Ravichandran,
    D. Toward semantics-based answer pinpointing. In *Proceedings of the First International
    Conference on Human Language Technology Research*, 2001. URL [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy等（2001）Hovy, E., Gerber, L., Hermjakob, U., Lin, C.-Y., 和 Ravichandran,
    D. 朝向基于语义的答案定位。收录于*第一次国际人类语言技术研究会议论文集*，2001年。网址 [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069)。
- en: Izacard et al. (2022) Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,
    P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive
    learning. *Trans. Mach. Learn. Res.*, 2022, 2022. URL [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard等（2022）Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,
    P., Joulin, A., 和 Grave, E. 使用对比学习进行无监督的密集信息检索。*机器学习研究杂志*，2022年，2022年。网址 [https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0)。
- en: Johnson et al. (2017) Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity
    search with gpus, 2017.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson等（2017）Johnson, J., Douze, M., 和 Jégou, H. 十亿规模相似性搜索与GPU，2017年。
- en: Johnson et al. (2019) Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity
    search with GPUs. *IEEE Transactions on Big Data*, 7(3):535–547, 2019.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson等（2019）Johnson, J., Douze, M., 和 Jégou, H. 十亿规模相似性搜索与GPU。*IEEE大数据学报*，7(3):535–547,
    2019年。
- en: 'Lehmann et al. (2015) Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,
    D., Mendes, P. N., Hellmann, S., Morsey, M., van Kleef, P., Auer, S., and Bizer,
    C. Dbpedia - A large-scale, multilingual knowledge base extracted from wikipedia.
    *Semantic Web*, 6(2):167–195, 2015. doi: 10.3233/SW-140134. URL [https://doi.org/10.3233/SW-140134](https://doi.org/10.3233/SW-140134).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lehmann等（2015）Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,
    D., Mendes, P. N., Hellmann, S., Morsey, M., van Kleef, P., Auer, S., 和 Bizer,
    C. Dbpedia - 一个从维基百科中提取的大规模多语言知识库。*语义网*，6(2):167–195, 2015年。doi: 10.3233/SW-140134。网址
    [https://doi.org/10.3233/SW-140134](https://doi.org/10.3233/SW-140134)。'
- en: 'Levine et al. (2022) Levine, Y., Wies, N., Jannai, D., Navon, D., Hoshen, Y.,
    and Shashua, A. The inductive bias of in-context learning: Rethinking pretraining
    example design. In *The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=lnEaqbTJIRz](https://openreview.net/forum?id=lnEaqbTJIRz).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine等（2022）Levine, Y., Wies, N., Jannai, D., Navon, D., Hoshen, Y., 和 Shashua,
    A. 上下文学习的归纳偏差：重新思考预训练示例设计。收录于*第十届国际学习表征会议，ICLR 2022，虚拟会议，2022年4月25-29日*。OpenReview.net，2022年。网址
    [https://openreview.net/forum?id=lnEaqbTJIRz](https://openreview.net/forum?id=lnEaqbTJIRz)。
- en: Lewkowycz et al. (2022) Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E.,
    Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo,
    T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning
    problems with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
    K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=IFXTZERXdM7](https://openreview.net/forum?id=IFXTZERXdM7).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz等（2022）Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski,
    H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y.,
    Neyshabur, B., Gur-Ari, G., 和 Misra, V. 使用语言模型解决定量推理问题。收录于Oh, A. H., Agarwal,
    A., Belgrave, D., 和 Cho, K.（编），*神经信息处理系统进展*，2022年。网址 [https://openreview.net/forum?id=IFXTZERXdM7](https://openreview.net/forum?id=IFXTZERXdM7)。
- en: Li et al. (2023a) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise
    on context length?, June 2023a. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023a）Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E.,
    Stoica, I., Ma, X., 和 Zhang, H. 开源LLMs在上下文长度上的真正承诺有多长？，2023年6月。网址 [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: 'Li et al. (2023b) Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov,
    D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii,
    E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro,
    J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L. K.,
    Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., V, R. M., Stillerman, J., Patel,
    S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Moustafa-Fahmy, N., Bhattacharyya,
    U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero,
    M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J.,
    Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B.,
    Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M.,
    Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may
    the source be with you! *CoRR*, abs/2305.06161, 2023b. doi: 10.48550/arXiv.2305.06161.
    URL [https://doi.org/10.48550/arXiv.2305.06161](https://doi.org/10.48550/arXiv.2305.06161).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023b）Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou,
    C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo,
    T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko,
    O., Gontier, N., Meade, N., Zebaze, A., Yee, M., Umapathi, L. K., Zhu, J., Lipkin,
    B., Oblokulov, M., Wang, Z., V, R. M., Stillerman, J., Patel, S. S., Abulkhanov,
    D., Zocca, M., Dey, M., Zhang, Z., Moustafa-Fahmy, N., Bhattacharyya, U., Yu,
    W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M.,
    Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao,
    T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor,
    D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes,
    S., Wolf, T., Guha, A., von Werra, L., 和 de Vries, H. Starcoder：愿源代码与你同在！*CoRR*，abs/2305.06161，2023b。doi:
    10.48550/arXiv.2305.06161。网址 [https://doi.org/10.48550/arXiv.2305.06161](https://doi.org/10.48550/arXiv.2305.06161)。'
- en: 'Li & Roth (2002) Li, X. and Roth, D. Learning question classifiers. In *COLING
    2002: The 19th International Conference on Computational Linguistics*, 2002. URL
    [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Roth（2002）Li, X. 和 Roth, D. 学习问题分类器。在*COLING 2002：第19届国际计算语言学大会*，2002。网址
    [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150)。
- en: 'Liu et al. (2023) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,
    M., Petroni, F., and Liang, P. Lost in the middle: How language models use long
    contexts. *CoRR*, abs/2307.03172, 2023. doi: 10.48550/arXiv.2307.03172. URL [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023）Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M.,
    Petroni, F., 和 Liang, P. 迷失在中间：语言模型如何使用长上下文。*CoRR*，abs/2307.03172，2023。doi: 10.48550/arXiv.2307.03172。网址
    [https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172)。'
- en: OpenAI (2023a) OpenAI. New models and developer products. OpenAI Blog, 2023a.
    URL [https://blog.salesforceairesearch.com/xgen-7b/](https://blog.salesforceairesearch.com/xgen-7b/).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI. 新模型和开发者产品。OpenAI 博客，2023a。网址 [https://blog.salesforceairesearch.com/xgen-7b/](https://blog.salesforceairesearch.com/xgen-7b/)。
- en: OpenAI (2023b) OpenAI. Gpt-4 technical report, 2023b.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI. GPT-4 技术报告，2023b。
- en: 'Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., and Lowe, R. Training language models to follow instructions
    with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,
    K., and Oh, A. (eds.), *Advances in Neural Information Processing Systems 35:
    Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022,
    New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L.,
    Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,
    J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., 和 Lowe, R. 训练语言模型以遵循人类反馈的指令。在 Koyejo, S., Mohamed, S., Agarwal,
    A., Belgrave, D., Cho, K., 和 Oh, A.（编辑），*《神经信息处理系统进展 35：2022 年神经信息处理系统年度会议，NeurIPS
    2022，新奥尔良，LA，美国，2022年11月28日 - 12月9日》*，2022。
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023）Peng, B., Quesnelle, J., Fan, H., 和 Shippole, E. Yarn：大规模语言模型的高效上下文窗口扩展，2023。
- en: 'Robertson & Zaragoza (2009) Robertson, S. E. and Zaragoza, H. The probabilistic
    relevance framework: BM25 and beyond. *Found. Trends Inf. Retr.*, 3(4):333–389,
    2009. doi: 10.1561/1500000019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Robertson & Zaragoza（2009）Robertson, S. E. 和 Zaragoza, H. 概率相关性框架：BM25 及其更远的应用。*《信息检索基础与趋势》*，3（4）：333–389，2009。doi:
    10.1561/1500000019。'
- en: 'Rozière et al. (2023) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov,
    I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez,
    A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
    Synnaeve, G. Code llama: Open foundation models for code, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière 等人（2023） Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
    Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov,
    I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez,
    A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., 和
    Synnaeve, G. Code llama: 面向代码的开放基础模型，2023。'
- en: 'Shaham et al. (2022) Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O.,
    Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., and Levy, O. Scrolls: Standardized
    comparison over long language sequences, 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham 等人（2022） Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv,
    A., Gupta, A., Xiong, W., Geva, M., Berant, J., 和 Levy, O. Scrolls: 长语言序列的标准化比较，2022。'
- en: Shi et al. (2023a) Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi,
    E. H., Schärli, N., and Zhou, D. Large language models can be easily distracted
    by irrelevant context. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,
    Sabato, S., and Scarlett, J. (eds.), *International Conference on Machine Learning,
    ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings
    of Machine Learning Research*, pp.  31210–31227\. PMLR, 2023a. URL [https://proceedings.mlr.press/v202/shi23a.html](https://proceedings.mlr.press/v202/shi23a.html).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2023a） Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H.,
    Schärli, N., 和 Zhou, D. 大型语言模型很容易被无关的上下文分散注意力。见 Krause, A., Brunskill, E., Cho,
    K., Engelhardt, B., Sabato, S., 和 Scarlett, J.（编），*国际机器学习大会，ICML 2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷，*机器学习研究会议录*，第31210–31227页。PMLR，2023a。网址
    [https://proceedings.mlr.press/v202/shi23a.html](https://proceedings.mlr.press/v202/shi23a.html)。
- en: 'Shi et al. (2023b) Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., James, R.,
    Lin, X. V., Smith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M. In-context pretraining:
    Language modeling beyond document boundaries, 2023b.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2023b） Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., James, R., Lin,
    X. V., Smith, N. A., Zettlemoyer, L., Yih, S., 和 Lewis, M. 上下文内预训练：超越文档边界的语言建模，2023b。
- en: 'Su et al. (2021) Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced
    transformer with rotary position embedding. *CoRR*, abs/2104.09864, 2021. URL
    [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su 等人（2021） Su, J., Lu, Y., Pan, S., Wen, B., 和 Liu, Y. Roformer: 具有旋转位置嵌入的增强型变换器。*CoRR*，abs/2104.09864，2021。网址
    [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)。'
- en: 'TogetherComputer (2023) TogetherComputer. Redpajama: An open source recipe
    to reproduce llama training dataset, April 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TogetherComputer（2023） TogetherComputer. Redpajama: 复现 llama 训练数据集的开源方案，2023年4月。网址
    [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)。'
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023） Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。Llama: 开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023。'
- en: 'Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
    Michalewski, H., and Milos, P. Focused transformer: Contrastive training for context
    scaling. *NeurIPS 2023*, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski 等人（2023） Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski,
    H., 和 Milos, P. 专注变换器：用于上下文缩放的对比训练。*NeurIPS 2023*，2023。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. *CoRR*,
    abs/1706.03762, 2017. URL [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762).
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017） Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L., 和 Polosukhin, I. 注意力机制即一切所需。*CoRR*，abs/1706.03762，2017。网址
    [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)。
- en: Weidinger et al. (2021) Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato,
    J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z.,
    Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell,
    L., Hendricks, L. A., Isaac, W., Legassick, S., Irving, G., and Gabriel, I. Ethical
    and social risks of harm from language models, 2021.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger 等人（2021） Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato,
    J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z.,
    Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell,
    L., Hendricks, L. A., Isaac, W., Legassick, S., Irving, G., 和 Gabriel, I. 语言模型的伦理与社会风险，2021。
- en: Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing
    transformers. In *The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022）Wu, Y., Rabe, M. N., Hutchins, D., 和 Szegedy, C. 记忆变换器。见于 *第十届国际学习表征会议,
    ICLR 2022, 虚拟活动, 2022年4月25-29日*。OpenReview.net，2022年。URL [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-)。
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J.
    (eds.), *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing, Brussels, Belgium, October 31 - November 4, 2018*, pp.  2369–2380\.
    Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL
    [https://doi.org/10.18653/v1/d18-1259](https://doi.org/10.18653/v1/d18-1259).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2018）Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., 和 Manning, C. D. Hotpotqa: 一个用于多跳问答的多样化、可解释数据集。见于 Riloff, E., Chiang, D.,
    Hockenmaier, J., 和 Tsujii, J.（编辑），*2018年自然语言处理实证方法会议论文集，比利时布鲁塞尔，2018年10月31日 -
    11月4日*，第2369–2380页。计算语言学协会，2018年。doi: 10.18653/V1/D18-1259。URL [https://doi.org/10.18653/v1/d18-1259](https://doi.org/10.18653/v1/d18-1259)。'
- en: Appendix A Architecture
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 架构
- en: The architecture of our models is based on LLaMA (Touvron et al., [2023](#bib.bib40)),
    and the architectural details can be found in Table [8](#A1.T8 "Table 8 ‣ Appendix
    A Architecture ‣ Structured Packing in LLM Training Improves Long Context Utilization").
    Briefly speaking, our architecture is similar to the one introduced in (Vaswani
    et al., [2017](#bib.bib42)) with a few standard changes. First, we use only the
    decoder without the encoder part. Secondly, we perform RMSNorm before the input
    of both the attention and feed-forward modules. Thirdly, we use the LLaMA FeedForward
    module. Additionally, we use Rotary Position Embedding (Su et al., [2021](#bib.bib38)).
    For context extension, we use Focused Transformer (FoT)(Tworkowski et al., [2023](#bib.bib41)),
    CodeLlama (Rozière et al., [2023](#bib.bib34)) (CL) and YaRN (Peng et al., [2023](#bib.bib32)).
    Table [9](#A1.T9 "Table 9 ‣ Appendix A Architecture ‣ Structured Packing in LLM
    Training Improves Long Context Utilization") presents the details about both standard
    and long-context pretraining.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的架构基于 LLaMA（Touvron 等，[2023](#bib.bib40)），架构细节可以在表 [8](#A1.T8 "表 8 ‣ 附录
    A 架构 ‣ 结构化打包在 LLM 训练中提高长上下文利用率") 中找到。简而言之，我们的架构类似于（Vaswani 等，[2017](#bib.bib42)）中介绍的，只是做了一些标准的修改。首先，我们只使用解码器部分，不使用编码器。其次，在注意力和前馈模块的输入之前，我们执行
    RMSNorm。第三，我们使用 LLaMA FeedForward 模块。此外，我们使用旋转位置嵌入（Su 等，[2021](#bib.bib38)）。为了扩展上下文，我们使用
    Focused Transformer（FoT）（Tworkowski 等，[2023](#bib.bib41)）、CodeLlama（Rozière 等，[2023](#bib.bib34)）（CL）和
    YaRN（Peng 等，[2023](#bib.bib32)）。表 [9](#A1.T9 "表 9 ‣ 附录 A 架构 ‣ 结构化打包在 LLM 训练中提高长上下文利用率")
    展示了标准和长上下文预训练的详细信息。
- en: 'Table 8: Architecture details. Focused Transformer context extension is applied
    in continued pretraining for $32$K context and evaluation.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：架构细节。Focused Transformer 上下文扩展在 $32$K 上下文和评估中应用于继续预训练。
- en: '| Parameter/Model Size | 270M | 3B | 7B |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 参数/模型大小 | 270M | 3B | 7B |'
- en: '| Vocab Size | 32000 | 32000 | 32000 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 词汇大小 | 32000 | 32000 | 32000 |'
- en: '| Embedding Size | 1024 | 3200 | 4096 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入大小 | 1024 | 3200 | 4096 |'
- en: '| Num Attention Layers | 12 | 26 | 32 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 注意力层数 | 12 | 26 | 32 |'
- en: '| Num Attention Heads | 8 | 32 | 32 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 注意力头数 | 8 | 32 | 32 |'
- en: '| Head Size | 128 | 100 | 128 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 头部大小 | 128 | 100 | 128 |'
- en: '| MLP Hidden Size | 4096 | 8640 | 11008 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| MLP 隐藏层大小 | 4096 | 8640 | 11008 |'
- en: '| FoT Context Extension Layers | [6, 9] | [6, 12, 18] | [8, 16, 24] |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| FoT 上下文扩展层 | [6, 9] | [6, 12, 18] | [8, 16, 24] |'
- en: 'Table 9: Training details. We pretrain a custom $270$B parameter OpenLLaMAv2
    model (Geng, [2023](#bib.bib14)). Subscript denotes that parameter was specific
    for a context extension method with FoT referring to (Tworkowski et al., [2023](#bib.bib41))
    and no-FoT to other methods (Naive, YaRN (Peng et al., [2023](#bib.bib32)), CodeLlama
    (Rozière et al., [2023](#bib.bib34))).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：训练细节。我们预训练了一个定制的 $270$B 参数 OpenLLaMAv2 模型（Geng，[2023](#bib.bib14)）。下标表示该参数特定于上下文扩展方法，其中
    FoT 指（Tworkowski 等，[2023](#bib.bib41)），无 FoT 指其他方法（Naive, YaRN（Peng 等，[2023](#bib.bib32)），CodeLlama（Rozière
    等，[2023](#bib.bib34)））。
- en: '| Stage | Parameter/Model Size | 270M | 3B | 7B |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 参数/模型大小 | 270M | 3B | 7B |'
- en: '| Pretraining | Context | 2K | 2K | 2K |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 预训练 | 上下文 | 2K | 2K | 2K |'
- en: '| Tokens | 6.3B | 1T | 1T |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 令牌数 | 6.3B | 1T | 1T |'
- en: '| Long Context Pretraining | Context | 32K[FoT], 16K[no-FoT] | 32K | 32K |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文预训练 | 上下文 | 32K[FoT], 16K[no-FoT] | 32K | 32K |'
- en: '| Batch Size | 128[FoT], 16[no-FoT] | 128 | 128[FoT], 32[no-FoT] |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 128[FoT], 16[no-FoT] | 128 | 128[FoT], 32[no-FoT] |'
- en: '| Start Learning Rate | 5e-5 | 1.5e-5 | 1.5e-5 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 起始学习率 | 5e-5 | 1.5e-5 | 1.5e-5 |'
- en: '| End Learning Rate | 5e-6 | 1.5e-6 | 1.5e-6 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 结束学习率 | 5e-6 | 1.5e-6 | 1.5e-6 |'
- en: '| Warmup Steps | 250 | 1000 | 1000[FoT], 200[no-FoT] |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 预热步骤 | 250 | 1000 | 1000[FoT], 200[no-FoT] |'
- en: '| Tokens | 1B | 5.4B | 2B |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 令牌数 | 1B | 5.4B | 2B |'
- en: Appendix B Detailed Results For Medium-Size Models
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 中型模型的详细结果
- en: In this section, we extend results presented in Section [3](#S3 "3 Experiments
    with Medium-Scale Models ‣ Structured Packing in LLM Training Improves Long Context
    Utilization"). The training methodology is described in Section [3.1](#S3.SS1
    "3.1 Experimental Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). Details about the
    number of tokens used for evaluation can be found in Table [18](#A7.T18 "Table
    18 ‣ G.1 Evaluation Data ‣ Appendix G Data Preparation ‣ Structured Packing in
    LLM Training Improves Long Context Utilization").
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们扩展了[第3节](#S3 "3 中型模型的实验 ‣ 结构化打包在LLM训练中改善长上下文利用")中提出的结果。训练方法在[第3.1节](#S3.SS1
    "3.1 实验设置 ‣ 3 中型模型的实验 ‣ 结构化打包在LLM训练中改善长上下文利用")中进行了描述。关于用于评估的令牌数量的详细信息可以在[表18](#A7.T18
    "表18 ‣ G.1 评估数据 ‣ 附录G 数据准备 ‣ 结构化打包在LLM训练中改善长上下文利用")中找到。
- en: Tables [10](#A2.T10 "Table 10 ‣ Appendix B Detailed Results For Medium-Size
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    and [11](#A2.T11 "Table 11 ‣ Appendix B Detailed Results For Medium-Size Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") show
    the results of training $270$K context on a 50/50 mixture of RedPajama data (organized
    in a standard way) and code data organized using a specified method. Table [10](#A2.T10
    "Table 10 ‣ Appendix B Detailed Results For Medium-Size Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization") contains detailed results
    from training on C# and Python. Table [11](#A2.T11 "Table 11 ‣ Appendix B Detailed
    Results For Medium-Size Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") contains results on C averaged across three different subsets
    of C (for details about the construction of those subsets, see Appendix [G](#A7
    "Appendix G Data Preparation ‣ Structured Packing in LLM Training Improves Long
    Context Utilization")). Both tables show that SPLiCe outperforms the Baseline
    by a significant margin.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10](#A2.T10 "表10 ‣ 附录B 中型模型的详细结果 ‣ 结构化打包在LLM训练中改善长上下文利用")和[表11](#A2.T11 "表11
    ‣ 附录B 中型模型的详细结果 ‣ 结构化打包在LLM训练中改善长上下文利用")显示了在50/50混合的RedPajama数据（以标准方式组织）和使用指定方法组织的代码数据上训练$270$K上下文的结果。[表10](#A2.T10
    "表10 ‣ 附录B 中型模型的详细结果 ‣ 结构化打包在LLM训练中改善长上下文利用")包含了在C#和Python上的训练详细结果。[表11](#A2.T11
    "表11 ‣ 附录B 中型模型的详细结果 ‣ 结构化打包在LLM训练中改善长上下文利用")包含了在C上进行的结果，这些结果是通过对C的三个不同子集进行平均得出的（有关这些子集构建的详细信息，请参见附录[G](#A7
    "附录G 数据准备 ‣ 结构化打包在LLM训练中改善长上下文利用")）。这两张表格都显示SPLiCe在显著程度上优于基准模型。'
- en: The main advantage of SPLiCe BM25/SPLiCe Cont over the SPLiCe Repo approach
    is that it can be used also for non-structured data. Table [12](#A2.T12 "Table
    12 ‣ Appendix B Detailed Results For Medium-Size Models ‣ Structured Packing in
    LLM Training Improves Long Context Utilization") shows the detailed results of
    applying the SPLiCe on non-code data. Note that training on non-code data allows
    us to improve the model perplexity on the arXiv dataset in comparison to the model
    trained on code.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: SPLiCe BM25/SPLiCe Cont相较于SPLiCe Repo方法的主要优势在于它也可以用于非结构化数据。[表12](#A2.T12 "表12
    ‣ 附录B 中型模型的详细结果 ‣ 结构化打包在LLM训练中改善长上下文利用")显示了在非代码数据上应用SPLiCe的详细结果。请注意，训练非代码数据使我们能够在arXiv数据集上改善模型的困惑度，而与训练于代码的数据模型相比。
- en: Table [13](#A2.T13 "Table 13 ‣ Appendix B Detailed Results For Medium-Size Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") considers
    models trained with YaRN (Peng et al., [2023](#bib.bib32)), CodeLlama (Rozière
    et al., [2023](#bib.bib34)) and Naive (no adjustment to RoPE) context extension
    methods. Table [14](#A2.T14 "Table 14 ‣ Appendix B Detailed Results For Medium-Size
    Models ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    shows that a simple artificial extension of example length via random concatenation
    of documents does not help.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表[13](#A2.T13 "Table 13 ‣ Appendix B Detailed Results For Medium-Size Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization")考虑了使用YaRN（Peng等，[2023](#bib.bib32)）、CodeLlama（Rozière等，[2023](#bib.bib34)）和Naive（对RoPE没有调整）上下文扩展方法训练的模型。表[14](#A2.T14
    "Table 14 ‣ Appendix B Detailed Results For Medium-Size Models ‣ Structured Packing
    in LLM Training Improves Long Context Utilization")显示，通过随机拼接文档进行的简单人工扩展并没有帮助。
- en: 'Table 10: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and code organized in the mentioned ways. For details
    about training please refer to Section [3.1](#S3.SS1 "3.1 Experimental Setup ‣
    3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training Improves
    Long Context Utilization").'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：比较组织相同数据的不同方式的困惑度结果。所有运行都从相同的$270M$K上下文开始，混合了以标准方式组织的RedPajama和按提到的方式组织的代码。有关训练的详细信息，请参阅第[3.1](#S3.SS1
    "3.1 Experimental Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization")节。
- en: '| Altered Train Data: C# |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Altered Train Data: C# |'
- en: '| --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
- en: '| ArXiv | 5.52 | 5.53 | 5.65 | 5.53 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.52 | 5.53 | 5.65 | 5.53 |'
- en: '| C | 2.39 | 2.40 | 2.50 | 2.40 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.39 | 2.40 | 2.50 | 2.40 |'
- en: '| C++ | 2.60 | 2.61 | 2.74 | 2.62 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.60 | 2.61 | 2.74 | 2.62 |'
- en: '| CUDA | 2.46 | 2.48 | 2.65 | 2.49 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.46 | 2.48 | 2.65 | 2.49 |'
- en: '| C# | 1.82 | 1.82 | 1.90 | 1.82 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.82 | 1.82 | 1.90 | 1.82 |'
- en: '| Common Lisp | 3.41 | 3.44 | 3.72 | 3.42 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 3.41 | 3.44 | 3.72 | 3.42 |'
- en: '| Dart | 2.14 | 2.16 | 2.31 | 2.16 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.14 | 2.16 | 2.31 | 2.16 |'
- en: '| Emacs Lisp | 8.41 | 8.46 | 8.85 | 8.41 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 8.41 | 8.46 | 8.85 | 8.41 |'
- en: '| Erlang | 2.80 | 2.80 | 2.95 | 2.81 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 2.80 | 2.80 | 2.95 | 2.81 |'
- en: '| Fortran | 3.68 | 3.71 | 4.05 | 3.72 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.68 | 3.71 | 4.05 | 3.72 |'
- en: '| Go | 1.94 | 1.95 | 2.06 | 1.96 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Go | 1.94 | 1.95 | 2.06 | 1.96 |'
- en: '| Groovy | 3.01 | 3.03 | 3.25 | 3.04 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.01 | 3.03 | 3.25 | 3.04 |'
- en: '| Haskell | 3.33 | 3.35 | 3.58 | 3.35 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.33 | 3.35 | 3.58 | 3.35 |'
- en: '| Java | 2.09 | 2.10 | 2.22 | 2.10 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.09 | 2.10 | 2.22 | 2.10 |'
- en: '| Pascal | 3.61 | 3.62 | 3.80 | 3.60 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.61 | 3.62 | 3.80 | 3.60 |'
- en: '| Python | 2.90 | 2.91 | 3.07 | 2.91 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Python | 2.90 | 2.91 | 3.07 | 2.91 |'
- en: '| Mean | 3.26 | 3.27 | 3.46 | 3.27 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.26 | 3.27 | 3.46 | 3.27 |'
- en: '| Altered Train Data: Python |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Altered Train Data: Python |'
- en: '| --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
- en: '| ArXiv | 5.47 | 5.49 | 5.57 | 5.48 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.47 | 5.49 | 5.57 | 5.48 |'
- en: '| C | 2.38 | 2.39 | 2.45 | 2.39 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.38 | 2.39 | 2.45 | 2.39 |'
- en: '| C++ | 2.61 | 2.63 | 2.71 | 2.63 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.61 | 2.63 | 2.71 | 2.63 |'
- en: '| CUDA | 2.41 | 2.43 | 2.56 | 2.44 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.41 | 2.43 | 2.56 | 2.44 |'
- en: '| C# | 1.98 | 1.99 | 2.06 | 2.00 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.98 | 1.99 | 2.06 | 2.00 |'
- en: '| Common Lisp | 3.23 | 3.28 | 3.47 | 3.27 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 3.23 | 3.28 | 3.47 | 3.27 |'
- en: '| Dart | 2.15 | 2.17 | 2.27 | 2.17 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.15 | 2.17 | 2.27 | 2.17 |'
- en: '| Emacs Lisp | 7.94 | 7.98 | 8.28 | 7.93 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 7.94 | 7.98 | 8.28 | 7.93 |'
- en: '| Erlang | 2.70 | 2.71 | 2.82 | 2.71 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 2.70 | 2.71 | 2.82 | 2.71 |'
- en: '| Fortran | 3.42 | 3.46 | 3.72 | 3.46 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.42 | 3.46 | 3.72 | 3.46 |'
- en: '| Go | 1.95 | 1.96 | 2.03 | 1.96 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Go | 1.95 | 1.96 | 2.03 | 1.96 |'
- en: '| Groovy | 2.97 | 2.99 | 3.13 | 2.99 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 2.97 | 2.99 | 3.13 | 2.99 |'
- en: '| Haskell | 3.25 | 3.28 | 3.46 | 3.27 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.25 | 3.28 | 3.46 | 3.27 |'
- en: '| Java | 2.12 | 2.12 | 2.20 | 2.13 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.12 | 2.12 | 2.20 | 2.13 |'
- en: '| Pascal | 3.57 | 3.58 | 3.73 | 3.58 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.57 | 3.58 | 3.73 | 3.58 |'
- en: '| Python | 2.53 | 2.53 | 2.62 | 2.54 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Python | 2.53 | 2.53 | 2.62 | 2.54 |'
- en: '| Mean | 3.17 | 3.19 | 3.32 | 3.18 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.17 | 3.19 | 3.32 | 3.18 |'
- en: 'Table 11: To check the statistical significance of our results, we prepare
    three subsets of C (details in Appendix [G](#A7 "Appendix G Data Preparation ‣
    Structured Packing in LLM Training Improves Long Context Utilization") and train
    the models on a 50/50 mixture of RedPajama data (organized in the standard way)
    and C data organized using one of the methods. Note that the standard deviation
    is much lower than the perplexity improvements from using SPLiCe.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 为了检查结果的统计显著性，我们准备了三个 C 的子集（详细信息见附录 [G](#A7 "附录 G 数据准备 ‣ 结构化打包在 LLM 训练中提升长上下文利用")），并在
    50/50 的 RedPajama 数据（按标准方式组织）与 C 数据（使用其中一种方法组织）的混合上训练模型。请注意，标准差远低于使用 SPLiCe 的困惑度改善。'
- en: '| Altered Train Data: C |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Altered Train Data: C |'
- en: '| --- |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline | SPLiCe Repo |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ArXiv | 5.463 $\pm$ 0.007 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.463 $\pm$ 0.007 |'
- en: '| C | 2.126 $\pm$ 0.020 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.126 $\pm$ 0.020 |'
- en: '| C++ | 2.396 $\pm$ 0.006 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.396 $\pm$ 0.006 |'
- en: '| CUDA | 2.219 $\pm$ 0.004 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.219 $\pm$ 0.004 |'
- en: '| C# | 1.939 $\pm$ 0.002 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.939 $\pm$ 0.002 |'
- en: '| Common Lisp | 2.994 $\pm$ 0.102 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 2.994 $\pm$ 0.102 |'
- en: '| Dart | 2.141 $\pm$ 0.002 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.141 $\pm$ 0.002 |'
- en: '| Emacs Lisp | 7.857 $\pm$ 0.019 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 7.857 $\pm$ 0.019 |'
- en: '| Erlang | 2.665 $\pm$ 0.003 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 2.665 $\pm$ 0.003 |'
- en: '| Fortran | 3.306 $\pm$ 0.012 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.306 $\pm$ 0.012 |'
- en: '| Go | 1.910 $\pm$ 0.006 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Go | 1.910 $\pm$ 0.006 |'
- en: '| Groovy | 3.009 $\pm$ 0.007 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.009 $\pm$ 0.007 |'
- en: '| Haskell | 3.198 $\pm$ 0.008 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.198 $\pm$ 0.008 |'
- en: '| Java | 2.075 $\pm$ 0.005 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.075 $\pm$ 0.005 |'
- en: '| Pascal | 3.492 $\pm$ 0.014 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.492 $\pm$ 0.014 |'
- en: '| Python | 2.810 $\pm$ 0.006 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Python | 2.810 $\pm$ 0.006 |'
- en: '| Mean | 3.100 $\pm$ 0.009 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.100 $\pm$ 0.009 |'
- en: 'Table 12: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and other data organized using one of the methods.
    For details about training, please refer to Section [3.1](#S3.SS1 "3.1 Experimental
    Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"). Note that the model trained with SPLiCe on
    StackExchange outperforms the one trained on code on arXiv evaluation, showing
    the benefits of SPLiCe’s applicability to non-code data.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 比较不同组织相同数据方式的困惑度结果。所有实验均从相同的 $270M$K 上下文开始，使用 50/50 的 RedPajama（按标准方式组织）与其他数据（使用其中一种方法组织）的混合。有关训练的详细信息，请参见第
    [3.1](#S3.SS1 "3.1 实验设置 ‣ 3 中型模型实验 ‣ 结构化打包在 LLM 训练中提升长上下文利用") 节。请注意，在 StackExchange
    上用 SPLiCe 训练的模型在 arXiv 评估中表现优于在代码上训练的模型，显示了 SPLiCe 在非代码数据中的适用性。'
- en: '| Altered Train Data: StackExchange |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Altered Train Data: StackExchange |'
- en: '| --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
- en: '| ArXiv | 5.07 | 5.09 | 5.14 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.07 | 5.09 | 5.14 |'
- en: '| C | 2.68 | 2.69 | 2.70 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.68 | 2.69 | 2.70 |'
- en: '| C++ | 3.02 | 3.04 | 3.06 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 3.02 | 3.04 | 3.06 |'
- en: '| CUDA | 2.89 | 2.93 | 2.94 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.89 | 2.93 | 2.94 |'
- en: '| C# | 2.27 | 2.28 | 2.29 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| C# | 2.27 | 2.28 | 2.29 |'
- en: '| Common Lisp | 4.02 | 4.06 | 4.08 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 4.02 | 4.06 | 4.08 |'
- en: '| Dart | 2.58 | 2.60 | 2.61 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.58 | 2.60 | 2.61 |'
- en: '| Emacs Lisp | 9.55 | 9.67 | 9.69 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 9.55 | 9.67 | 9.69 |'
- en: '| Erlang | 3.13 | 3.16 | 3.18 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 3.13 | 3.16 | 3.18 |'
- en: '| Fortran | 4.28 | 4.34 | 4.38 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 4.28 | 4.34 | 4.38 |'
- en: '| Go | 2.24 | 2.25 | 2.27 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Go | 2.24 | 2.25 | 2.27 |'
- en: '| Groovy | 3.62 | 3.66 | 3.68 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.62 | 3.66 | 3.68 |'
- en: '| Haskell | 3.88 | 3.91 | 3.94 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.88 | 3.91 | 3.94 |'
- en: '| Java | 2.43 | 2.45 | 2.45 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.43 | 2.45 | 2.45 |'
- en: '| Pascal | 4.08 | 4.11 | 4.14 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 4.08 | 4.11 | 4.14 |'
- en: '| Python | 3.32 | 3.35 | 3.36 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Python | 3.32 | 3.35 | 3.36 |'
- en: '| Mean | 3.69 | 3.73 | 3.74 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.69 | 3.73 | 3.74 |'
- en: '| Altered Train Data: Wikipedia |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Altered Train Data: Wikipedia |'
- en: '| --- |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Eval/Method | SPLiCe BM25 | SPLiCe Cont | Baseline |'
- en: '| ArXiv | 5.64 | 5.65 | 5.73 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.64 | 5.65 | 5.73 |'
- en: '| C | 2.65 | 2.67 | 2.71 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.65 | 2.67 | 2.71 |'
- en: '| C++ | 2.98 | 3.01 | 3.07 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.98 | 3.01 | 3.07 |'
- en: '| CUDA | 2.87 | 2.92 | 3.00 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.87 | 2.92 | 3.00 |'
- en: '| C# | 2.22 | 2.24 | 2.29 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| C# | 2.22 | 2.24 | 2.29 |'
- en: '| Common Lisp | 3.87 | 3.96 | 4.08 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 3.87 | 3.96 | 4.08 |'
- en: '| Dart | 2.51 | 2.55 | 2.61 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.51 | 2.55 | 2.61 |'
- en: '| Emacs Lisp | 9.38 | 9.45 | 9.63 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 9.38 | 9.45 | 9.63 |'
- en: '| Erlang | 3.13 | 3.16 | 3.23 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 3.13 | 3.16 | 3.23 |'
- en: '| Fortran | 4.23 | 4.32 | 4.49 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 4.23 | 4.32 | 4.49 |'
- en: '| Go | 2.18 | 2.21 | 2.26 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Go | 2.18 | 2.21 | 2.26 |'
- en: '| Groovy | 3.49 | 3.55 | 3.67 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.49 | 3.55 | 3.67 |'
- en: '| Haskell | 3.82 | 3.87 | 3.97 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.82 | 3.87 | 3.97 |'
- en: '| Java | 2.39 | 2.41 | 2.46 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.39 | 2.41 | 2.46 |'
- en: '| Pascal | 4.32 | 4.23 | 4.40 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 4.32 | 4.23 | 4.40 |'
- en: '| Python | 3.26 | 3.30 | 3.37 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Python | 3.26 | 3.30 | 3.37 |'
- en: '| Mean | 3.68 | 3.72 | 3.81 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.68 | 3.72 | 3.81 |'
- en: 'Table 13: Perplexity results comparing different ways of organizing the same
    data for non-FoT models. All runs started from the same $270M$K context on a 50/50
    mixture of RedPajama (organized in a standard way) and C# code is organized in
    one of three ways.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: 比较不同方式组织相同数据的困惑度结果，针对非 FoT 模型。所有实验都从相同的 $270M$K 上下文开始，RedPajama（以标准方式组织）和
    C# 代码以三种方式之一进行组织。'
- en: '| Altered Train Data: C# |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 更改训练数据：C# |'
- en: '| --- |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Context $16$K: CodeLlama |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 $16$K: CodeLlama |'
- en: '| --- |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 评估/方法 | SPLiCe BM25 | 基线 | SPLiCe Repo |'
- en: '| --- | --- | --- | --- |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ArXiv | 5.74 | 5.76 | 5.74 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.74 | 5.76 | 5.74 |'
- en: '| C | 2.66 | 2.70 | 2.66 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.66 | 2.70 | 2.66 |'
- en: '| C++ | 2.79 | 2.83 | 2.79 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.79 | 2.83 | 2.79 |'
- en: '| CUDA | 2.53 | 2.58 | 2.54 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.53 | 2.58 | 2.54 |'
- en: '| C# | 1.91 | 1.93 | 1.91 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.91 | 1.93 | 1.91 |'
- en: '| Common Lisp | 3.78 | 3.85 | 3.79 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 3.78 | 3.85 | 3.79 |'
- en: '| Dart | 2.28 | 2.33 | 2.28 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.28 | 2.33 | 2.28 |'
- en: '| Emacs Lisp | 8.29 | 8.41 | 8.30 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 8.29 | 8.41 | 8.30 |'
- en: '| Erlang | 3.57 | 3.64 | 3.58 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 3.57 | 3.64 | 3.58 |'
- en: '| Fortran | 3.93 | 4.01 | 3.95 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.93 | 4.01 | 3.95 |'
- en: '| Go | 1.99 | 2.03 | 2.00 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Go | 1.99 | 2.03 | 2.00 |'
- en: '| Groovy | 2.95 | 3.01 | 2.96 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 2.95 | 3.01 | 2.96 |'
- en: '| Haskell | 4.28 | 4.37 | 4.28 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 4.28 | 4.37 | 4.28 |'
- en: '| Java | 2.31 | 2.35 | 2.31 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.31 | 2.35 | 2.31 |'
- en: '| Pascal | 3.67 | 3.72 | 3.67 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.67 | 3.72 | 3.67 |'
- en: '| Python | 3.22 | 3.27 | 3.22 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Python | 3.22 | 3.27 | 3.22 |'
- en: '| Mean | 3.49 | 3.55 | 3.50 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 3.49 | 3.55 | 3.50 |'
- en: '| Context $16$K: YaRN |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 $16$K: YaRN |'
- en: '| --- |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 评估/方法 | SPLiCe BM25 | 基线 | SPLiCe Repo |'
- en: '| --- | --- | --- | --- |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ArXiv | 5.77 | 5.79 | 5.77 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.77 | 5.79 | 5.77 |'
- en: '| C | 2.68 | 2.72 | 2.68 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.68 | 2.72 | 2.68 |'
- en: '| C++ | 2.81 | 2.85 | 2.81 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.81 | 2.85 | 2.81 |'
- en: '| CUDA | 2.55 | 2.61 | 2.56 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.55 | 2.61 | 2.56 |'
- en: '| C# | 1.92 | 1.94 | 1.92 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.92 | 1.94 | 1.92 |'
- en: '| Common Lisp | 3.83 | 3.92 | 3.84 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 3.83 | 3.92 | 3.84 |'
- en: '| Dart | 2.30 | 2.36 | 2.30 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.30 | 2.36 | 2.30 |'
- en: '| Emacs Lisp | 8.37 | 8.52 | 8.38 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 8.37 | 8.52 | 8.38 |'
- en: '| Erlang | 3.60 | 3.67 | 3.61 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 3.60 | 3.67 | 3.61 |'
- en: '| Fortran | 3.97 | 4.06 | 3.99 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.97 | 4.06 | 3.99 |'
- en: '| Go | 2.00 | 2.04 | 2.01 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| Go | 2.00 | 2.04 | 2.01 |'
- en: '| Groovy | 2.98 | 3.03 | 2.98 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 2.98 | 3.03 | 2.98 |'
- en: '| Haskell | 4.32 | 4.42 | 4.32 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 4.32 | 4.42 | 4.32 |'
- en: '| Java | 2.33 | 2.37 | 2.33 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.33 | 2.37 | 2.33 |'
- en: '| Pascal | 3.69 | 3.75 | 3.69 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.69 | 3.75 | 3.69 |'
- en: '| Python | 3.24 | 3.29 | 3.24 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Python | 3.24 | 3.29 | 3.24 |'
- en: '| Mean | 3.52 | 3.58 | 3.53 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 3.52 | 3.58 | 3.53 |'
- en: '| Context $16$K: Naive |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 $16$K: Naive |'
- en: '| --- |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Eval/Method | SPLiCe BM25 | Baseline | SPLiCe Repo |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 评估/方法 | SPLiCe BM25 | 基线 | SPLiCe Repo |'
- en: '| --- | --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ArXiv | 6.25 | 6.33 | 6.25 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 6.25 | 6.33 | 6.25 |'
- en: '| C | 2.88 | 2.96 | 2.89 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.88 | 2.96 | 2.89 |'
- en: '| C++ | 3.04 | 3.13 | 3.05 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 3.04 | 3.13 | 3.05 |'
- en: '| CUDA | 2.84 | 2.96 | 2.85 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.84 | 2.96 | 2.85 |'
- en: '| C# | 2.04 | 2.08 | 2.04 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| C# | 2.04 | 2.08 | 2.04 |'
- en: '| Common Lisp | 4.40 | 4.56 | 4.39 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 4.40 | 4.56 | 4.39 |'
- en: '| Dart | 2.50 | 2.60 | 2.51 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.50 | 2.60 | 2.51 |'
- en: '| Emacs Lisp | 9.25 | 9.46 | 9.25 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 9.25 | 9.46 | 9.25 |'
- en: '| Erlang | 3.98 | 4.10 | 4.00 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 3.98 | 4.10 | 4.00 |'
- en: '| Fortran | 4.56 | 4.79 | 4.59 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 4.56 | 4.79 | 4.59 |'
- en: '| Go | 2.14 | 2.21 | 2.16 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Go | 2.14 | 2.21 | 2.16 |'
- en: '| Groovy | 3.27 | 3.39 | 3.28 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.27 | 3.39 | 3.28 |'
- en: '| Haskell | 4.84 | 5.03 | 4.87 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 4.84 | 5.03 | 4.87 |'
- en: '| Java | 2.52 | 2.60 | 2.53 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.52 | 2.60 | 2.53 |'
- en: '| Pascal | 4.05 | 4.20 | 4.10 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 4.05 | 4.20 | 4.10 |'
- en: '| Python | 3.55 | 3.66 | 3.56 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Python | 3.55 | 3.66 | 3.56 |'
- en: '| Mean | 3.88 | 4.00 | 3.89 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 3.88 | 4.00 | 3.89 |'
- en: 'Table 14: Perplexity results comparing different ways of organizing the same
    data. All runs started from the same $270M$K context on a 50/50 mixture of RedPajama
    (organized in a standard way) and C code is organized in one of three ways. For
    details about training please refer to Section [3.1](#S3.SS1 "3.1 Experimental
    Setup ‣ 3 Experiments with Medium-Scale Models ‣ Structured Packing in LLM Training
    Improves Long Context Utilization").'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14: 比较不同方式组织相同数据的困惑度结果。所有实验都从相同的 $270M$K 上下文开始，RedPajama（以标准方式组织）和 C 代码以三种方式之一进行组织。有关训练的详细信息，请参阅第
    [3.1](#S3.SS1 "3.1 Experimental Setup ‣ 3 Experiments with Medium-Scale Models
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") 节。'
- en: '|  | Altered Train Data: C |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | 更改训练数据：C |'
- en: '| --- | --- |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Eval/Method | SPLiCe BM25 | Baseline | Random |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 评估/方法 | SPLiCe BM25 | 基线 | 随机 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ArXiv | 5.46 | 5.55 | 5.55 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 5.46 | 5.55 | 5.55 |'
- en: '| C | 2.13 | 2.17 | 2.18 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| C | 2.13 | 2.17 | 2.18 |'
- en: '| C++ | 2.40 | 2.47 | 2.47 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 2.40 | 2.47 | 2.47 |'
- en: '| CUDA | 2.22 | 2.33 | 2.33 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 2.22 | 2.33 | 2.33 |'
- en: '| C# | 1.94 | 2.02 | 2.02 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| C# | 1.94 | 2.02 | 2.02 |'
- en: '| Common Lisp | 2.99 | 3.20 | 3.18 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 2.99 | 3.20 | 3.18 |'
- en: '| Dart | 2.14 | 2.27 | 2.27 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 2.14 | 2.27 | 2.27 |'
- en: '| Emacs Lisp | 7.86 | 8.10 | 8.09 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 7.86 | 8.10 | 8.09 |'
- en: '| Erlang | 2.67 | 2.77 | 2.77 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 2.67 | 2.77 | 2.77 |'
- en: '| Fortran | 3.31 | 3.55 | 3.56 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 3.31 | 3.55 | 3.56 |'
- en: '| Go | 1.91 | 2.00 | 2.00 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| Go | 1.91 | 2.00 | 2.00 |'
- en: '| Groovy | 3.01 | 3.15 | 3.15 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 3.01 | 3.15 | 3.15 |'
- en: '| Haskell | 3.20 | 3.37 | 3.37 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 3.20 | 3.37 | 3.37 |'
- en: '| Java | 2.07 | 2.16 | 2.16 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| Java | 2.07 | 2.16 | 2.16 |'
- en: '| Pascal | 3.49 | 3.62 | 3.64 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 3.49 | 3.62 | 3.64 |'
- en: '| Python | 2.81 | 2.93 | 2.93 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| Python | 2.81 | 2.93 | 2.93 |'
- en: '| Mean | 3.10 | 3.23 | 3.23 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Mean | 3.10 | 3.23 | 3.23 |'
- en: Appendix C Ablations
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 消融
- en: C.1 SPLiCe Parameters
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 SPLiCe参数
- en: There are two important design choices related to SPLiCe. First, how many related
    documents are retrieved in each step (the parameter $k$M models. We use ’standard’,
    as ordered by Algorithm [1](#alg1 "Algorithm 1 ‣ 2 Method ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), the reversed order, and random
    shuffling.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个与SPLiCe相关的重要设计选择。首先，每一步检索多少相关文档（参数 $k$M 模型）。我们使用“标准”，按算法 [1](#alg1 "Algorithm
    1 ‣ 2 Method ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    排序，倒序排序，以及随机打乱。
- en: 'Table 15: Ablation of SPLiCe hyper-parameters. For each ablation, we have trained
    the same $270M$ is reversed, shuffle – examples are shuffled.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：SPLiCe超参数的消融。对于每个消融，我们都训练了相同的 $270M$ 被倒序，打乱——示例被打乱。
- en: '|  |  | Code |  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 代码 |  |'
- en: '| Method | Top-$k$ | Order | C++ | Haskell | Python | CUDA | All |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Top-$k$ | 顺序 | C++ | Haskell | Python | CUDA | 所有 |'
- en: '| SPLiCe BM25 | top 1 | standard | 2.38 | 3.20 | 2.82 | 2.23 | 2.93 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe BM25 | top 1 | standard | 2.38 | 3.20 | 2.82 | 2.23 | 2.93 |'
- en: '| reverse | 2.38 | 3.21 | 2.82 | 2.23 | 2.93 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| reverse | 2.38 | 3.21 | 2.82 | 2.23 | 2.93 |'
- en: '| top 2 | standard | 2.39 | 3.23 | 2.84 | 2.24 | 2.95 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| top 2 | standard | 2.39 | 3.23 | 2.84 | 2.24 | 2.95 |'
- en: '| reverse | 2.39 | 3.24 | 2.84 | 2.24 | 2.95 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| reverse | 2.39 | 3.24 | 2.84 | 2.24 | 2.95 |'
- en: '| top 3 | standard | 2.39 | 3.24 | 2.84 | 2.25 | 2.97 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| top 3 | standard | 2.39 | 3.24 | 2.84 | 2.25 | 2.97 |'
- en: '| reverse | 2.39 | 3.25 | 2.84 | 2.25 | 2.96 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| reverse | 2.39 | 3.25 | 2.84 | 2.25 | 2.96 |'
- en: '| shuffle | 2.40 | 3.24 | 2.84 | 2.25 | 2.96 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| shuffle | 2.40 | 3.24 | 2.84 | 2.25 | 2.96 |'
- en: We also evaluated the influence of BOS and EOS tokens on the performance of
    trained models. As both Repo and SPLiCe methods concatenate documents to create
    training examples, they effectively slightly decrease the number of separating
    tokens compared to the Baseline. However, in Appendix [C.2](#A3.SS2 "C.2 Importance
    of Separating Tokens ‣ Appendix C Ablations ‣ Structured Packing in LLM Training
    Improves Long Context Utilization") we included experiments showing that this
    has no impact on model performance.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了BOS和EOS令牌对训练模型性能的影响。由于Repo和SPLiCe方法都将文档连接起来以创建训练示例，它们相对于基线略微减少了分隔令牌的数量。然而，在附录
    [C.2](#A3.SS2 "C.2 Importance of Separating Tokens ‣ Appendix C Ablations ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") 中，我们包含了实验结果，显示这对模型性能没有影响。
- en: C.2 Importance of Separating Tokens
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 分离令牌的重要性
- en: We also evaluate the influence of BOS and EOS tokens on the performance. To
    be more precise, in all setups, training examples are separated by BOS and EOS
    tokens. As SPLiCe methods concatenate documents to create training examples, they
    effectively increase the average example length and decrease the number of separating
    tokens. To check whether those methods do not simply benefit from the reduction
    in the number of BOS and EOS tokens, we have trained a model on data prepared
    similarly as in SPLiCe, but instead of most matching documents $\texttt{RETRIEVE}(d,k)$
    returned random documents from the dataset (sampling without replacement). The
    results are shown in Table [16](#A3.T16 "Table 16 ‣ C.2 Importance of Separating
    Tokens ‣ Appendix C Ablations ‣ Structured Packing in LLM Training Improves Long
    Context Utilization"). We note that the difference between the Baseline and the
    random concatenation approach is small, and the random concatenation approach
    does not result in significant perplexity gains.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了BOS和EOS令牌对性能的影响。更确切地说，在所有设置中，训练示例都由BOS和EOS令牌分隔。由于SPLiCe方法将文档连接起来以创建训练示例，它们实际上增加了平均示例长度并减少了分隔令牌的数量。为了检查这些方法是否仅仅因减少BOS和EOS令牌的数量而受益，我们训练了一个模型，数据准备方式类似于SPLiCe，但大多数匹配文档
    $\texttt{RETRIEVE}(d,k)$ 返回数据集中的随机文档（无替换抽样）。结果见表 [16](#A3.T16 "Table 16 ‣ C.2
    Importance of Separating Tokens ‣ Appendix C Ablations ‣ Structured Packing in
    LLM Training Improves Long Context Utilization")。我们注意到，基线与随机连接方法之间的差异很小，随机连接方法并未显著提高困惑度。
- en: 'Table 16: Perplexity evaluation of two methods of organizing the data. Baseline
    – document equals training example. Random – concatenate documents into examples
    of length bounded by $120$M parameter model. We performed three runs on different
    subsets of C to provide mean and standard deviation.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：两种数据组织方法的困惑度评估。基准 – 文档等于训练样本。随机 – 将文档连接成长度由 $120$M 参数模型限定的示例。我们在 C 的不同子集上进行了三次运行，以提供均值和标准差。
- en: '| Long Context | Method | arXiv | Code | Code & |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | 方法 | arXiv | 代码 | 代码 & |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Data |  |  | Python | All | arXiv |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 数据 |  |  | Python | 全部 | arXiv |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| C | Random | 5.554 $\pm$ 0.005 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| C | 随机 | 5.554 $\pm$ 0.005 |'
- en: '| Baseline | 5.550 $\pm$ 0.005 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 5.550 $\pm$ 0.005 |'
- en: Appendix D Key-Value Retrieval Task
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 关键值检索任务
- en: Figure [2](#S4.F2 "Figure 2 ‣ 4.2.3 Key Retrieval Performance ‣ 4.2 Experimental
    Results ‣ 4 Large-Scale Models ‣ Structured Packing in LLM Training Improves Long
    Context Utilization") shows how training on SPLiCe organized data improves the
    performance on the key-value retrieval task proposed in (Liu et al., [2023](#bib.bib28)).
    This is a zero-shot task in which a model is prompted with a JSON dictionary and
    asked to retrieve a value corresponding to a specified key. The structure of the
    input is showcased below.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "图 2 ‣ 4.2.3 关键检索性能 ‣ 4.2 实验结果 ‣ 4 大规模模型 ‣ 结构化打包在 LLM 训练中提高了长上下文利用")
    显示了在 SPLiCe 组织的数据上训练如何提高在 (Liu et al., [2023](#bib.bib28)) 提出的关键值检索任务上的表现。这是一个零样本任务，其中模型使用
    JSON 字典作为提示，并被要求检索与指定键对应的值。输入的结构如下所示。
- en: 'Extract the value corresponding to the specified key in the JSON object below.
    JSON data: {"4ef217b7-6bc0-48c6-af35-2765f1e730f3": "068192b7-16b1-40e0-8495-61c63f979d50",
     "cd6b8bdc-bc6c-4490-acb4-bc187a2dccba": "7364a26e-289f-4968-93d3-b273e882bdee",
     "7d057372-4ab8-4811-8110-658c3f19fff4": "3ad075c5-b567-4201-85a7-cb31a0c91540",
     "c62e192d-45e6-4646-bb88-1529c73256c9": "f0411644-1f6d-42a6-8af8-f06da66efc77",
     "06134e93-e158-490e-a66c-8e3b98e12735": "50a26a36-d832-450c-8d6e-a4cc3d0ec0ab",
     "3286f978-4270-4b54-8bfa-540d7e0772e6": "075cc716-1836-4f90-9be3-53e3d4ec6585",
     "4701aa05-c523-4b89-9700-64ab9c37c537": "49d86354-74c4-4256-9b3a-35e6e2b80d00",
     "c8895805-e574-4f13-9fe5-89da1d8c4748": "cc91af7f-8509-4bdc-bad7-2646af68e6d2"}
     "4701aa05-c523-4b89-9700-64ab9c37c537":'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '从下面的 JSON 对象中提取与指定键对应的值。JSON 数据：{"4ef217b7-6bc0-48c6-af35-2765f1e730f3": "068192b7-16b1-40e0-8495-61c63f979d50",
    "cd6b8bdc-bc6c-4490-acb4-bc187a2dccba": "7364a26e-289f-4968-93d3-b273e882bdee",
    "7d057372-4ab8-4811-8110-658c3f19fff4": "3ad075c5-b567-4201-85a7-cb31a0c91540",
    "c62e192d-45e6-4646-bb88-1529c73256c9": "f0411644-1f6d-42a6-8af8-f06da66efc77",
    "06134e93-e158-490e-a66c-8e3b98e12735": "50a26a36-d832-450c-8d6e-a4cc3d0ec0ab",
    "3286f978-4270-4b54-8bfa-540d7e0772e6": "075cc716-1836-4f90-9be3-53e3d4ec6585",
    "4701aa05-c523-4b89-9700-64ab9c37c537": "49d86354-74c4-4256-9b3a-35e6e2b80d00",
    "c8895805-e574-4f13-9fe5-89da1d8c4748": "cc91af7f-8509-4bdc-bad7-2646af68e6d2"}
    "4701aa05-c523-4b89-9700-64ab9c37c537":'
- en: We noted that FoT-trained models struggle with this task. This is probably due
    to the fact that they extend context only in a couple of layers, and the key-value
    retrieval task requires looking up and extracting a long sequence of letters and
    digits. Because of that, we evaluate FoT models with shorter dictionaries consisting
    of $75$B CL model with this context length and show the results in Figure [3](#A4.F3
    "Figure 3 ‣ Appendix D Key-Value Retrieval Task ‣ Structured Packing in LLM Training
    Improves Long Context Utilization").
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，FoT 训练的模型在这个任务上表现不佳。这可能是因为它们仅在几层中扩展上下文，而关键值检索任务需要查找和提取长序列的字母和数字。因此，我们使用具有此上下文长度的
    $75$B CL 模型对 FoT 模型进行评估，并在图 [3](#A4.F3 "图 3 ‣ 附录 D 关键值检索任务 ‣ 结构化打包在 LLM 训练中提高了长上下文利用")
    中展示了结果。
- en: '![Refer to caption](img/d86d45736b83e67735eea4e182fcf1b1.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d86d45736b83e67735eea4e182fcf1b1.png)'
- en: (a)
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/ce9e476f1707efb102fd9116f5e754c1.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ce9e476f1707efb102fd9116f5e754c1.png)'
- en: (b)
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/5e5caa25e547c15b811a204e94cdd1fb.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e5caa25e547c15b811a204e94cdd1fb.png)'
- en: (c)
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 3: Performance on a smaller version of key-value retrieval task from
    (Liu et al., [2023](#bib.bib28)). We note that FoT models (a), (b) generally struggle
    to retrieve tokens that are only visible to a subset of layers with extended context.
    For comparison, we show the results with a model that has extended context in
    all layers (c) using CodeLlama (Rozière et al., [2023](#bib.bib34)) method of
    context extension. Each position was evaluated using $500$ examples.'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：来自(Liu et al., [2023](#bib.bib28))的较小版本的键值检索任务的性能。我们注意到FoT模型 (a), (b) 通常在检索仅对扩展上下文的部分层可见的标记时表现困难。作为对比，我们展示了使用CodeLlama
    (Rozière et al., [2023](#bib.bib34)) 的上下文扩展方法的所有层具有扩展上下文的模型 (c) 的结果。每个位置使用$500$个示例进行评估。
- en: Appendix E Perplexity Improvements
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 困惑度改进
- en: In Figure [4](#A5.F4 "Figure 4 ‣ Appendix E Perplexity Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") we present perplexity
    improvements of $3$, and then average within the buckets. We average perplexity
    across arXiv, CUDA, Haskell, and CommonCrawl datasets.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4](#A5.F4 "图 4 ‣ 附录 E 困惑度改进 ‣ LLM 训练中的结构化打包改善了长期上下文的利用")中，我们展示了困惑度的$3$的改进，然后在桶内取平均。我们对arXiv、CUDA、Haskell和CommonCrawl数据集的困惑度进行平均。
- en: '![Refer to caption](img/1a27f94ba011deb4034f9268f5377400.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1a27f94ba011deb4034f9268f5377400.png)'
- en: 'Figure 4: Perplexity improvement with SPLiCe against the Baseline of the final
    models (after $21$k training steps). We bucket tokens by their positions in the
    document and calculate the average. Each dot is the difference of the averages
    of the SPLiCe and Baseline models. We observe that SPLiCe has smaller perplexity,
    and the improvements tend to be larger for tokens further in the document.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：SPLiCe 相对于最终模型的基线（经过$21$k训练步骤）的困惑度改进。我们按标记在文档中的位置进行分桶并计算平均值。每个点是SPLiCe和基线模型平均值的差异。我们观察到SPLiCe的困惑度较小，而且改进在文档中的更远标记上往往更大。
- en: '![Refer to caption](img/559373c3b9764e91df25c47b407ccd2e.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/559373c3b9764e91df25c47b407ccd2e.png)'
- en: 'Figure 5: Evolution of the perplexity with SPLiCe, as the model is trained
    on more tokens. See [4](#A5.F4 "Figure 4 ‣ Appendix E Perplexity Improvements
    ‣ Structured Packing in LLM Training Improves Long Context Utilization") for the
    difference with the baseline. As expected, SPLiCe significantly improves perplexity
    for tokens whose positions are very distant in the sequence. Perplexity for more
    distant tokens improves more significantly compared to tokens in the beginning,
    early in the training.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：随着模型在更多标记上进行训练，SPLiCe 的困惑度演变。有关与基线的差异，请参见[4](#A5.F4 "图 4 ‣ 附录 E 困惑度改进 ‣
    LLM 训练中的结构化打包改善了长期上下文的利用")。正如预期的那样，SPLiCe显著改善了序列中位置非常远的标记的困惑度。相比于训练初期的标记，更远的标记在困惑度上有显著改善。
- en: Appendix F Shuffling
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 洗牌
- en: 'Table 17: Average classification performance on TREC (Li & Roth, [2002](#bib.bib27);
    Hovy et al., [2001](#bib.bib18)). We compare $7$ to denote the standard deviation.
    We decided to stick with the model trained with random shuffling as it has slightly
    better long-context performance and lower standard deviation.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17：TREC上的平均分类性能 (Li & Roth, [2002](#bib.bib27); Hovy et al., [2001](#bib.bib18))。我们比较$7$以表示标准差。我们决定坚持使用随机洗牌训练的模型，因为它在长期上下文性能上稍好，标准差更低。
- en: '| TREC |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| TREC |'
- en: '| --- |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Model |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Context length &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文长度 &#124;'
- en: '&#124; (# of examples) &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （示例数量） &#124;'
- en: '| SPLiCe-no-shuf | SPLiCe-shuf |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe-no-shuf | SPLiCe-shuf |'
- en: '| --- | --- | --- | --- |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $7$1.9 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| $7$1.9 |'
- en: '| $8$1.8 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| $8$1.8 |'
- en: Appendix G Data Preparation
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 数据准备
- en: G.1 Evaluation Data
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 评估数据
- en: We have taken a random subset of arXiv from Proof-pile. For StarCoder data,
    we have downloaded up to $64$GB of each of the mentioned language subsets and
    performed a random 85/15 split for languages that we train on.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Proof-pile中随机抽取了arXiv的一个子集。对于StarCoder数据，我们下载了每种提到的语言子集最多$64$GB，并对我们训练的语言进行了随机85/15拆分。
- en: When evaluating the perplexity of the model, we skip documents that are shorter
    than the model context and truncate documents that are longer than that. Table
    [18](#A7.T18 "Table 18 ‣ G.1 Evaluation Data ‣ Appendix G Data Preparation ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows the number of
    tokens over which the perplexity was calculated.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型的困惑度时，我们跳过比模型上下文短的文档，并截断比模型上下文长的文档。表[18](#A7.T18 "表 18 ‣ G.1 评估数据 ‣ 附录
    G 数据准备 ‣ LLM 训练中的结构化打包改善了长期上下文的利用")显示了计算困惑度的标记数量。
- en: 'Table 18: Number of evaluation tokens in each of the considered datasets. For
    each context length $c$ tokens prefix.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：每个考虑的数据集中评估标记的数量。对于每个上下文长度 $c$ 标记前缀。
- en: '| Eval/Method | 16K | 32K | 64K |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 评估/方法 | 16K | 32K | 64K |'
- en: '| --- | --- | --- | --- |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ArXiv | 16M | 16M | 16M |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| ArXiv | 16M | 16M | 16M |'
- en: '| C | 16M | 16M | 16M |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| C | 16M | 16M | 16M |'
- en: '| C++ | 16M | 16M | 16M |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 16M | 16M | 16M |'
- en: '| CUDA | 16M | 14M | 8M |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 16M | 14M | 8M |'
- en: '| C# | 16M | 16M | 16M |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| C# | 16M | 16M | 16M |'
- en: '| Common Lisp | 16M | 16M | 16M |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Common Lisp | 16M | 16M | 16M |'
- en: '| Dart | 16M | 16M | 16M |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| Dart | 16M | 16M | 16M |'
- en: '| Emacs Lisp | 16M | 15M | 9M |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| Emacs Lisp | 16M | 15M | 9M |'
- en: '| Erlang | 16M | 16M | 12M |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| Erlang | 16M | 16M | 12M |'
- en: '| Fortran | 16M | 16M | 16M |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| Fortran | 16M | 16M | 16M |'
- en: '| Go | 16M | 16M | 16M |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Go | 16M | 16M | 16M |'
- en: '| Groovy | 11M | 4M | 2M |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| Groovy | 11M | 4M | 2M |'
- en: '| Haskell | 16M | 16M | 15M |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| Haskell | 16M | 16M | 15M |'
- en: '| Java | 16M | 16M | 16M |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| Java | 16M | 16M | 16M |'
- en: '| Pascal | 16M | 16M | 16M |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| Pascal | 16M | 16M | 16M |'
- en: '| Python | 16M | 16M | 16M |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| Python | 16M | 16M | 16M |'
- en: G.2 Train Data
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 训练数据
- en: The StackExchange data was taken from the Proof-pile. To prepare the code train
    data, we take the StarCoder train splits mentioned in Section [G.1](#A7.SS1 "G.1
    Evaluation Data ‣ Appendix G Data Preparation ‣ Structured Packing in LLM Training
    Improves Long Context Utilization"), shuffle them, group the documents by the
    repository (documents from the same repository occur one after another), and split
    them into smaller packs. We also split repos larger than $25$M characters. The
    character filtering is consistent with our method as we aim to improve the performance
    in a scenario that lacks high-quality long-context data. For C# and Python, only
    one pack is used to organize the data. For C, we have performed a run on three
    packs and provided results and standard deviation in Table [3](#S3.T3 "Table 3
    ‣ 3.2 Experimental Results ‣ 3 Experiments with Medium-Scale Models ‣ Structured
    Packing in LLM Training Improves Long Context Utilization"). For large models,
    we run the methods on several packs and concatenate the results into a single
    dataset. For natural language datasets, we extract a random subset of documents.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: StackExchange 数据取自 Proof-pile。为了准备代码训练数据，我们取了第 [G.1](#A7.SS1 "G.1 评估数据 ‣ 附录
    G 数据准备 ‣ LLM 训练中的结构化打包提高了长上下文利用") 节中提到的 StarCoder 训练拆分，打乱它们，将文档按仓库分组（来自同一仓库的文档依次出现），然后拆分成较小的包。我们还将大于
    $25$M 字符的仓库拆分。字符过滤与我们的方法一致，因为我们旨在改善在缺乏高质量长上下文数据的情况下的性能。对于 C# 和 Python，仅使用一个包来组织数据。对于
    C，我们在三个包上进行了运行，并在表 [3](#S3.T3 "表 3 ‣ 3.2 实验结果 ‣ 3 中等规模模型实验 ‣ LLM 训练中的结构化打包提高了长上下文利用")
    中提供了结果和标准差。对于大模型，我们在多个包上运行这些方法，并将结果连接成一个数据集。对于自然语言数据集，我们提取了文档的随机子集。
- en: Appendix H Faiss Parameters
  id: totrans-544
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H Faiss 参数
- en: Our experiments with SPLiCe Cont utilize Faiss (Johnson et al., [2019](#bib.bib21))
    for fast approximate inner-product search. To be more precise, we use the ”IVF8192,Flat”
    index that we train on 262144 examples coming from the dataset.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 SPLiCe Cont 的实验中利用 Faiss（Johnson 等人，[2019](#bib.bib21)）进行快速近似内积搜索。更准确地说，我们使用
    ”IVF8192,Flat” 索引，在 262144 个来自数据集的示例上进行训练。
- en: Appendix I Detailed Accuracy Improvements
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 详细准确性改进
- en: The performance of in-context learning depends much on the choice of the in-context
    examples. To study this in more detail we study the following random variable
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习的性能在很大程度上取决于上下文示例的选择。为了更详细地研究这一点，我们研究了以下随机变量。
- en: '|  | $\Delta(c)=\text{ACC}_{\textsc{{SPLiCe}}}(c)-\text{ACC}_{\textsc{Baseline}}(c),$
    |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta(c)=\text{ACC}_{\textsc{{SPLiCe}}}(c)-\text{ACC}_{\textsc{Baseline}}(c),$
    |  |'
- en: where $\text{ACC}_{\textsc{{SPLiCe}}}(c),\text{ACC}_{\textsc{Baseline}}(c)$ [confidence
    interval].
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{ACC}_{\textsc{{SPLiCe}}}(c),\text{ACC}_{\textsc{Baseline}}(c)$ [置信区间]。
- en: Figure [6](#A9.F6 "Figure 6 ‣ Appendix I Detailed Accuracy Improvements ‣ Structured
    Packing in LLM Training Improves Long Context Utilization") shows details about
    accuracy improvements on TREC when considering different numbers of in-context
    examples.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#A9.F6 "图 6 ‣ 附录 I 详细准确性改进 ‣ LLM 训练中的结构化打包提高了长上下文利用") 显示了在 TREC 上考虑不同数量的上下文示例时准确性改进的细节。
- en: '![Refer to caption](img/20b6dfc34c6a4077402995ea00026928.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/20b6dfc34c6a4077402995ea00026928.png)'
- en: '(a) Examples: $190$K'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 示例：$190$K
- en: '![Refer to caption](img/3acd2eb5ef4d419990c03dbe654d4068.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3acd2eb5ef4d419990c03dbe654d4068.png)'
- en: '(b) Examples: $380$K'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 示例：$380$K
- en: '![Refer to caption](img/faf3e44aac2e8209fb8d8ab3a1c68d45.png)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/faf3e44aac2e8209fb8d8ab3a1c68d45.png)'
- en: '(c) Examples: $780$K'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 示例：$780$K
- en: '![Refer to caption](img/431fe0d2273e4e9395fb4d6da6b3f24f.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/431fe0d2273e4e9395fb4d6da6b3f24f.png)'
- en: '(d) Examples: $1560$K'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 示例：$1560$K
- en: 'Figure 6: Histograms of accuracy improvement of SPLiCe BM25 over Baseline on
    TREC question classification task. The results are obtained by comparing the accuracy
    on the test set of TREC of the $3$ sets of in-context examples. Each set of in-context
    examples consists of elements randomly sampled (without replacement) from the
    training subset of TREC. Note that the model trained with SPLiCe is almost always
    better than the Baseline.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：SPLiCe BM25 相较于基线在 TREC 问题分类任务上的准确度提升直方图。结果通过比较 TREC 测试集上 $3$ 组上下文示例的准确度获得。每组上下文示例由从
    TREC 训练子集中随机抽取的元素（不放回）组成。注意，使用 SPLiCe 训练的模型几乎总是优于基线。
- en: '![Refer to caption](img/56e3b7d4d17dfc2625897079fbd0bfda.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/56e3b7d4d17dfc2625897079fbd0bfda.png)'
- en: '(a) Examples: $190$B FoT'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 示例：$190$B FoT
- en: '![Refer to caption](img/e97a84e1e85951f82ef31437e3c55ddb.png)'
  id: totrans-562
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e97a84e1e85951f82ef31437e3c55ddb.png)'
- en: '(b) Examples: $380$B FoT'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 示例：$380$B FoT
- en: '![Refer to caption](img/bc3d1082807495227440348d4285b872.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bc3d1082807495227440348d4285b872.png)'
- en: '(c) Examples: $190$B FoT'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 示例：$190$B FoT
- en: '![Refer to caption](img/960f4fc58aad2e18b8d584d95496fb42.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/960f4fc58aad2e18b8d584d95496fb42.png)'
- en: '(d) Examples: $380$B FoT'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 示例：$380$B FoT
- en: '![Refer to caption](img/5fd2c76773a25b158a3ab47c220d2ff0.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5fd2c76773a25b158a3ab47c220d2ff0.png)'
- en: '(e) Examples: $190$B CL'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 示例：$190$B CL
- en: '![Refer to caption](img/6a54e7314313da1c906735763c04c13e.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a54e7314313da1c906735763c04c13e.png)'
- en: '(f) Examples: $380$B CL'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 示例：$380$B CL
- en: 'Figure 7: Histograms of accuracy improvement of SPLiCe BM25 over Baseline on
    DBPedia. We sample $40$ element subset of the DBPedia test set.'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：SPLiCe BM25 相较于基线在 DBPedia 上的准确度提升直方图。我们从 DBPedia 测试集中抽取了 $40$ 个元素的子集。
- en: Appendix J Longer Context
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 更长的上下文
- en: In Table [19](#A10.T19 "Table 19 ‣ Appendix J Longer Context ‣ Structured Packing
    in LLM Training Improves Long Context Utilization"), we present the perplexity
    results for $270$K.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [19](#A10.T19 "Table 19 ‣ Appendix J Longer Context ‣ Structured Packing
    in LLM Training Improves Long Context Utilization") 中，我们展示了 $270$K 的困惑度结果。
- en: 'Table 19: Perplexity ${}_{(\text{imporovment over {Baseline}})}$K context.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：困惑度 ${}_{(\text{比基线改进})}$K 上下文。
- en: '| Long Context | Method | arXiv | Code | Code & |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | 方法 | arXiv | 代码 | 代码 & |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Data |  |  | Haskell | Python | CUDA | All | arXiv |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 数据 |  |  | Haskell | Python | CUDA | 全部 | arXiv |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| C# | SPLiCe BM25 | 4.86 [(.15)] | 2.60 [(.19)] | 2.66 [(.16)] | 2.32 [(.19)]
    | 2.75 [(.19)] | 2.88 [(.19)] |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| C# | SPLiCe BM25 | 4.86 [(.15)] | 2.60 [(.19)] | 2.66 [(.16)] | 2.32 [(.19)]
    | 2.75 [(.19)] | 2.88 [(.19)] |'
- en: '| SPLiCe Cont | 4.88 [(.13)] | 2.62 [(.17)] | 2.67 [(.15)] | 2.34 [(.17)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Cont | 4.88 [(.13)] | 2.62 [(.17)] | 2.67 [(.15)] | 2.34 [(.17)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
- en: '| SPLiCe Repo | 4.88 [(.13)] | 2.62 [(.17)] | 2.68 [(.14)] | 2.35 [(.16)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| SPLiCe Repo | 4.88 [(.13)] | 2.62 [(.17)] | 2.68 [(.14)] | 2.35 [(.16)] |
    2.77 [(.17)] | 2.90 [(.17)] |'
- en: '| Baseline | 5.01 | 2.79 | 2.82 | 2.51 | 2.94 | 3.07 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 5.01 | 2.79 | 2.82 | 2.51 | 2.94 | 3.07 |'
- en: Appendix K Data Distribution Property
  id: totrans-584
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 K 数据分布特性
- en: Chan et al. ([2022](#bib.bib8)); Han et al. ([2023](#bib.bib17)) shows that
    the ”burstiness” property of training data benefits a model’s generalisation.
    Specifically, ”burstiness” presents a flatter frequency distribution, with a relatively
    higher mass on the rare, long-tail tokens appearing in a sequence, which results
    in a lower Zipf’s coefficient of token frequency. We also observe a burstiness
    property of data in SPLiCe, as shown in Table [20](#A11.T20 "Table 20 ‣ Appendix
    K Data Distribution Property ‣ Structured Packing in LLM Training Improves Long
    Context Utilization").
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: Chan 等人 ([2022](#bib.bib8)); Han 等人 ([2023](#bib.bib17)) 表明，训练数据的“突发性”特性对模型的泛化有益。具体而言，“突发性”表现为较平坦的频率分布，相对较高的质量集中在序列中的稀有长尾标记上，这导致较低的
    Zipf 系数。我们还观察到 SPLiCe 数据的突发性特性，如表 [20](#A11.T20 "Table 20 ‣ Appendix K Data Distribution
    Property ‣ Structured Packing in LLM Training Improves Long Context Utilization")
    所示。
- en: 'Table 20: Zipf’s coefficient of token frequency on Baseline and SPLiCe. A lower
    Zipf’s coefficient represents a more significant burstiness property.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 表 20：基线和 SPLiCe 的 Zipf 系数。较低的 Zipf 系数表示更显著的突发性特征。
- en: '| Training Data | Context Length | Method | Zipf’s Coefficient |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据 | 上下文长度 | 方法 | Zipf 系数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| C | 2K | SPLiCe BM25 | ${1.667}_{(0.139)}$ |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| C | 2K | SPLiCe BM25 | ${1.667}_{(0.139)}$ |'
- en: '| Baseline | ${1.717}_{(0.152)}$ |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | ${1.717}_{(0.152)}$ |'
- en: '| StackEx | 2K | SPLiCe BM25 | ${1.836}_{(0.088)}$ |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| StackEx | 2K | SPLiCe BM25 | ${1.836}_{(0.088)}$ |'
- en: '| Baseline | ${1.865}_{(0.074)}$ |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | ${1.865}_{(0.074)}$ |'
- en: '| C | 32K | SPLiCe BM25 | ${1.512}_{(0.055)}$ |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| C | 32K | SPLiCe BM25 | ${1.512}_{(0.055)}$ |'
- en: '| Baseline | ${1.593}_{(0.025)}$ |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | ${1.593}_{(0.025)}$ |'
- en: '| StackEx | 32K | SPLiCe BM25 | ${1.643}_{(0.026)}$ |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| StackEx | 32K | SPLiCe BM25 | ${1.643}_{(0.026)}$ |'
- en: '| Baseline | ${1.664}_{(0.013)}$ |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | ${1.664}_{(0.013)}$ |'
