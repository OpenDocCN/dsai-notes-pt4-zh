- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:02:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:02:17'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不留任何文档：基于扩展多文档问答的长文本LLMs基准测试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17419](https://ar5iv.labs.arxiv.org/html/2406.17419)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17419](https://ar5iv.labs.arxiv.org/html/2406.17419)
- en: 'Minzheng Wang^(1,3), Longze Chen^(2,3)¹¹footnotemark: 1, Cheng Fu⁴, Shengyi
    Liao⁴, Xinghua Zhang⁴, Bingli Wu⁴'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '王敏正^(1,3), 陈龙泽^(2,3)¹¹脚注标记: 1, 傅成^(4), 廖盛毅^(4), 张兴华^(4), 吴冰力^(4)'
- en: 'Haiyang Yu⁴, Nan Xu¹, Lei Zhang^(2,3), Run Luo^(2,3), Yunshui Li^(2,3), Min
    Yang², Fei Huang⁴, Yongbin Li⁴²²footnotemark: 2'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '于海洋⁴, 许楠¹, 张磊^(2,3), 罗润^(2,3), 李云水^(2,3), 杨敏², 黄飞⁴, 李永宾⁴²²脚注标记: 2'
- en: ¹MAIS, Institute of Automation, Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹MAIS, 中国科学院自动化研究所
- en: ²Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²中国科学院深圳先进技术研究院
- en: ³School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³中国科学院大学人工智能学院
- en: ⁴Alibaba Group
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴阿里巴巴集团
- en: 🖂: wangminzheng2023@ia.ac.cn;  lz.chen2@siat.ac.cn Equal contribution.Corresponding
    author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '🖂: wangminzheng2023@ia.ac.cn; lz.chen2@siat.ac.cn 等贡献.通讯作者.'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Long-context modeling capabilities have garnered widespread attention, leading
    to the emergence of Large Language Models (LLMs) with ultra-context windows. Meanwhile,
    benchmarks for evaluating long-context LLMs are gradually catching up. However,
    existing benchmarks employ irrelevant noise texts to artificially extend the length
    of test cases, diverging from the real-world scenarios of long-context applications.
    To bridge this gap, we propose a novel long-context benchmark, Loong, aligning
    with realistic scenarios through extended multi-document question answering (QA).
    Unlike typical document QA, in Loong’s test cases, each document is relevant to
    the final answer, ignoring any document will lead to the failure of the answer.
    Furthermore, Loong introduces four types of tasks with a range of context lengths:
    Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate
    a more realistic and comprehensive evaluation of long-context understanding. Extensive
    experiments indicate that existing long-context language models still exhibit
    considerable potential for enhancement. Retrieval augmented generation (RAG) achieves
    poor performance, demonstrating that Loong can reliably assess the model’s long-context
    modeling capabilities.¹¹1The code and benchmark are available at [https://github.com/MozerWang/Loong](https://github.com/MozerWang/Loong)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本建模能力已引起广泛关注，促使了具有超长上下文窗口的大型语言模型（LLMs）的出现。同时，评估长文本LLMs的基准测试也在逐步跟进。然而，现有的基准测试使用了无关的噪声文本来人为延长测试案例的长度，与真实的长文本应用场景有所偏离。为了弥补这一差距，我们提出了一种新型长文本基准测试——Loong，通过扩展的多文档问答（QA）与现实场景对齐。与典型的文档问答不同，在Loong的测试案例中，每个文档与最终答案相关，忽视任何一个文档都会导致答案失败。此外，Loong引入了四种类型的任务，具有不同的上下文长度：焦点定位、比较、聚类和推理链，以便更真实和全面地评估长文本理解能力。大量实验表明，现有的长文本语言模型仍具有相当大的提升潜力。检索增强生成（RAG）表现不佳，表明Loong可以可靠地评估模型的长文本建模能力。¹¹1代码和基准测试可在
    [https://github.com/MozerWang/Loong](https://github.com/MozerWang/Loong) 获取
- en: 'Leave No Document Behind:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不留任何文档：
- en: Benchmarking Long-Context LLMs with Extended Multi-Doc QA
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩展多文档问答的长文本LLMs基准测试
- en: 'Minzheng Wang^(1,3)^†^†thanks: Equal contribution., Longze Chen^(2,3)¹¹footnotemark:
    1, Cheng Fu⁴, Shengyi Liao⁴, Xinghua Zhang⁴, Bingli Wu⁴ Haiyang Yu⁴, Nan Xu¹,
    Lei Zhang^(2,3), Run Luo^(2,3), Yunshui Li^(2,3), Min Yang²^†^†thanks: Corresponding
    author., Fei Huang⁴, Yongbin Li⁴²²footnotemark: 2 ¹MAIS, Institute of Automation,
    Chinese Academy of Sciences ²Shenzhen Institute of Advanced Technology, Chinese
    Academy of Sciences ³School of Artificial Intelligence, University of Chinese
    Academy of Sciences ⁴Alibaba Group 🖂: wangminzheng2023@ia.ac.cn;  lz.chen2@siat.ac.cn'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '王敏正^(1,3)^†^†感谢: 等贡献., 陈龙泽^(2,3)¹¹脚注标记: 1, 傅成^(4), 廖盛毅^(4), 张兴华^(4), 吴冰力^(4)
    于海洋^(4), 许楠¹, 张磊^(2,3), 罗润^(2,3), 李云水^(2,3), 杨敏²^†^†感谢: 通讯作者., 黄飞^(4), 李永宾^(4)²²脚注标记:
    2 ¹MAIS, 中国科学院自动化研究所 ²中国科学院深圳先进技术研究院 ³中国科学院大学人工智能学院 ⁴阿里巴巴集团 🖂: wangminzheng2023@ia.ac.cn;
    lz.chen2@siat.ac.cn'
- en: '![Refer to caption](img/80e0db5186c53acb3602f36b4622372f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80e0db5186c53acb3602f36b4622372f.png)'
- en: 'Figure 1: Previous benchmarks vs. Loong. ![Refer to caption](img/d331886b9a784081ab138740c1288102.png)
    marks the existence of evidence related to the answer in that document. Compared
    to centralized distribution in previous ones, evidence in Loong are scattered
    in different parts across multi-document long contexts, necessitating that no
    document can be ignored for success.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 之前的基准测试与 Loong 的对比。 ![参见说明](img/d331886b9a784081ab138740c1288102.png)
    标记了与该文档中的答案相关的证据的存在。与之前的集中分布相比，Loong 中的证据分散在多文档长上下文中的不同部分，必须注意所有文档，以确保成功。'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have exhibited remarkable proficiency in diverse
    downstream applications OpenAI ([2023](#bib.bib19)). Recent works focus on scaling
    up the context window of LLMs (Xiong et al., [2023](#bib.bib35); Peng et al.,
    [2023](#bib.bib21); Chen et al., [2024b](#bib.bib9)), which is crucial for LLMs
    in handling complex tasks that require delving deeply into long texts. A few LLM
    (e.g. GPT-4o, Gemini-Pro) websites have been equipped with the intelligent document
    analysis function, allowing users to upload documents for answering queries. Meanwhile,
    retrieval-augmented generation (RAG) has been a commonly used framework that prompts
    LLMs with multiple relevant retrieved contents and can significantly improve model
    performance Wu et al. ([2024](#bib.bib33)); Chen et al. ([2024a](#bib.bib8)).
    These demand the model leverage its long-context capability to conduct an in-depth
    analysis of multiple long documents.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在多种下游应用中展现出了显著的能力 OpenAI ([2023](#bib.bib19))。近期的研究集中于扩大 LLMs 的上下文窗口 (Xiong
    et al., [2023](#bib.bib35); Peng et al., [2023](#bib.bib21); Chen et al., [2024b](#bib.bib9))，这对
    LLMs 处理需要深入阅读长文本的复杂任务至关重要。一些 LLM（例如 GPT-4o、Gemini-Pro）网站已配备了智能文档分析功能，允许用户上传文档以回答查询。同时，检索增强生成（RAG）已成为一种常用框架，它通过多项相关检索内容来提示
    LLMs，并能显著提升模型性能 Wu et al. ([2024](#bib.bib33)); Chen et al. ([2024a](#bib.bib8))。这些要求模型利用其长上下文能力对多个长文档进行深入分析。
- en: '| Benchmark |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试 |'
- en: '&#124;    Multi-doc &#124;'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    多文档 &#124;'
- en: '&#124; Tasks &#124;'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务 &#124;'
- en: '|'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;    Broad &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    广泛性 &#124;'
- en: '&#124; Length Sets &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长度集合 &#124;'
- en: '|'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;    Avoidance of &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    避免 &#124;'
- en: '&#124; Contamination &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 污染 &#124;'
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;    Realistic &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    现实性 &#124;'
- en: '&#124; Scenarios &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 场景 &#124;'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;    High Evidence &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;    高证据 &#124;'
- en: '&#124; Dispersion &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分散性 &#124;'
- en: '|    Multilingual |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|    多语言 |'
- en: '| L-Eval (An et al., [2023](#bib.bib1)) | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| L-Eval (An et al., [2023](#bib.bib1)) | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
- en: '| LongBench (Bai et al., [2023b](#bib.bib5)) | ✓ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LongBench (Bai et al., [2023b](#bib.bib5)) | ✓ | ✗ | ✗ | ✓ | ✗ | ✓ |'
- en: '| Marathon (Zhang et al., [2023](#bib.bib37)) | ✓ | ✓ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Marathon (Zhang et al., [2023](#bib.bib37)) | ✓ | ✓ | ✗ | ✓ | ✗ | ✓ |'
- en: '| LooGLE (Li et al., [2023](#bib.bib17)) | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| LooGLE (Li et al., [2023](#bib.bib17)) | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |'
- en: '| InfiniteBench (Zhang et al., [2024](#bib.bib38)) | ✓ | ✗ | ✗ | ✓ | ✗ | ✓
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| InfiniteBench (Zhang et al., [2024](#bib.bib38)) | ✓ | ✗ | ✗ | ✓ | ✗ | ✓
    |'
- en: '| RULER (Hsieh et al., [2024](#bib.bib15)) | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| RULER (Hsieh et al., [2024](#bib.bib15)) | ✓ | ✓ | ✗ | ✗ | ✗ | ✗ |'
- en: '| NIAH (Kamradt, [2023](#bib.bib16)) | ✗ | ✓ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| NIAH (Kamradt, [2023](#bib.bib16)) | ✗ | ✓ | ✓ | ✗ | ✗ | ✗ |'
- en: '| Loong (Ours) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Loong (我们) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 'Table 1: Characteristics of Loong, where the evidences are scattered across
    multi-document long contexts.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: Loong 的特性，其中证据分散在多文档长上下文中。'
- en: 'However, there remains a lack of appropriate benchmarks for evaluating long-context
    understanding in real-world multi-document scenarios. Multi-document input as
    long-context modeling possesses extensive application scenarios of LLMs, such
    as analysis of financial reports over the years. Nevertheless, most existing benchmarks
    only place emphasis on single-document long contexts (An et al., [2023](#bib.bib1);
    Li et al., [2023](#bib.bib17); Kamradt, [2023](#bib.bib16)) or involve multi-document
    question answering settings by adding distracting information to the input of
    existing short-context QA datasets (Hsieh et al., [2024](#bib.bib15)). As shown
    in [Figure 1](#S0.F1 "In Leave No Document Behind: Benchmarking Long-Context LLMs
    with Extended Multi-Doc QA"), evidence supporting the answer in previous benchmarks
    is relatively centralized, such as being contained within a single document. Yet,
    such a centralized distribution of evidence may cause the model to overlook certain
    documents and take shortcuts to formulate an answer, complicating the modeling
    of the real context length. Moreover, the prevalent evaluation tasks, such as
    “needle in a haystack” (NIAH) (Kamradt, [2023](#bib.bib16)), only scratch the
    surface of long-context understanding by searching from context, far from real-world
    demands.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仍缺乏适当的基准来评估现实世界中多文档场景的长上下文理解。多文档输入作为长上下文建模具有广泛的应用场景，例如多年来的财务报告分析。然而，大多数现有基准仅强调单文档长上下文（An
    et al., [2023](#bib.bib1); Li et al., [2023](#bib.bib17); Kamradt, [2023](#bib.bib16)）或通过将分散信息添加到现有短上下文QA数据集的输入中来涉及多文档问答设置（Hsieh
    et al., [2024](#bib.bib15)）。如[图1](#S0.F1 "在《留下文档不落下：长上下文LLMs的多文档QA基准》")所示，之前基准中支持答案的证据相对集中，例如包含在单个文档中。然而，这种证据的集中分布可能会导致模型忽略某些文档，并采取捷径来形成答案，复杂化了真实上下文长度的建模。此外，流行的评估任务，如“针在干草堆中”（NIAH）
    (Kamradt, [2023](#bib.bib16))，仅仅触及了长上下文理解的表面，通过从上下文中搜索，远未满足现实世界的需求。
- en: 'We commence with “$\mathtt{leave}$” and scatter the evidence across multi-document
    long contexts. In this context, bypassing any document will lead to an erroneous
    answer, which better tests the long-context modeling ability. To this end, this
    paper develops Loong, an innovative benchmark crafted to evaluate the long-context
    ability of LLMs across multiple documents in real-world scenarios. Loong typically
    consists of 11 documents per test instance on average, spanning three real-world
    scenarios in English and Chinese: (1) Financial Reports, (2) Legal Cases, and
    (3) Academic Papers. Meanwhile, Loong introduces new evaluation tasks from the
    perspectives of Spotlight Locating, Comparison, Clustering, and Chain of Reasoning.
    Furthermore, Loong features inputs of varying lengths (e.g., 10K-50K, 50K-100K,
    100K-200K, >200K) and evaluation tasks of diverse difficulty, enabling fine-grained
    assessment of LLMs across different context lengths and task complexities.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从“$\mathtt{leave}$”开始，并将证据分散到多个文档的长上下文中。在这种情况下，绕过任何文档将导致错误的答案，这更好地测试了长上下文建模能力。为此，本文开发了Loong，一个创新的基准，旨在评估LLMs在现实场景中跨多个文档的长上下文能力。Loong通常包含每个测试实例平均11个文档，涵盖三个现实场景：
    (1) 财务报告， (2) 法律案件， (3) 学术论文。同时，Loong从聚焦定位、比较、聚类和推理链等角度引入了新的评估任务。此外，Loong具有不同长度的输入（例如，10K-50K，50K-100K，100K-200K，>200K）和多样的评估任务难度，使得对LLMs在不同上下文长度和任务复杂性下的细粒度评估成为可能。
- en: 'We conduct extensive experiments on Loong to test the long-context modeling
    capabilities of several advanced LLMs. The empirical results show that even the
    current most powerful LLMs still struggle with the tasks in Loong, suggesting
    significant room for improvement in current LLMs. Furthermore, this paper conducts
    in-depth analyses regarding the behavior of long-context LLMs, involving RAG and
    the scaling law of context size. Our main contributions are summarized as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Loong上进行了广泛的实验，以测试几种先进LLMs的长上下文建模能力。实证结果表明，即使是当前最强大的LLMs也在Loong中的任务上表现不佳，这表明当前LLMs还有显著的改进空间。此外，本文对长上下文LLMs的行为进行了深入分析，包括RAG和上下文大小的扩展规律。我们的主要贡献总结如下：
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Loong primarily focuses on testing long-context ability of LLMs across multiple
    documents by scattering the evidence to examine the real length of long contexts.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Loong主要关注通过分散证据来测试LLMs在多个文档中的长上下文能力，以检查长上下文的真实长度。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Loong offers evaluation sets with varying input lengths and evaluation tasks
    of differing difficulty, encompassing novel task categories as well as common
    multi-document scenarios.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Loong 提供了具有不同输入长度和难度不同的评估任务的评估集，包括新颖的任务类别以及常见的多文档场景。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All test instances are newly annotated and checked to guarantee quality. Extensive
    experiments and analyses deeply unveil the long-context modeling abilities of
    LLMs.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有测试实例都经过新标注和检查，以保证质量。广泛的实验和分析深入揭示了 LLMs 在长上下文建模方面的能力。
- en: '![Refer to caption](img/ad24c7a364e405e040c7a9603d81f2e4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad24c7a364e405e040c7a9603d81f2e4.png)'
- en: 'Figure 2: Showcase of four evaluation tasks in Loong (<$\mathtt{di}$> marks
    the content of the i-th document). a) Spotlight Locating: Locate the evidences.
    b) Comparison: Locate and compare the evidences. c) Clustering: Locate and cluster
    the evidences into groups. d) Chain of Reasoning: Locate and reasoning along a
    logical chain.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：展示 Loong 中的四个评估任务 (<$\mathtt{di}$> 标记了第 i 个文档的内容)。 a) 聚焦定位：定位证据。 b) 比较：定位并比较证据。
    c) 聚类：定位并将证据分组。 d) 推理链：定位并沿逻辑链推理。
- en: 2 Related Work
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Long-Context Language Models
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 长上下文语言模型
- en: With support for increasingly larger context windows, closed-source LLMs have
    taken the lead in the field of long-context modeling. From 128k to 1000k, GPT-4o (OpenAI,
    [2023](#bib.bib19)), Claude3-200k (Anthropic, [2024a](#bib.bib2)) and Gemini-pro1.5-1000k (Reid
    et al., [2024](#bib.bib25)) are capable of modeling increasingly longer documents,
    expanding the new scenarios that LLMs can handle.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持越来越大的上下文窗口方面，闭源 LLMs 在长上下文建模领域处于领先地位。从 128k 到 1000k，GPT-4o (OpenAI, [2023](#bib.bib19))、Claude3-200k
    (Anthropic, [2024a](#bib.bib2)) 和 Gemini-pro1.5-1000k (Reid et al., [2024](#bib.bib25))
    能够建模越来越长的文档，扩展了 LLMs 能够处理的新场景。
- en: Considering the quadratic complexity of Transformer (Vaswani et al., [2017](#bib.bib31)),
    training LLMs with extensive context windows from scratch necessitates substantial
    computational resources, exceeding the capabilities of the general researchers.
    Consequently, recent studies have explored ways to expand the context length of
    these models during the fine-tuning stage. For example, PI (Chen et al., [2023](#bib.bib10)),
    NTK-aware (bloc97, [2023](#bib.bib6)), YaRN (Peng et al., [2023](#bib.bib21)),
    Giraffe (Pal et al., [2023](#bib.bib20)), Code LLaMA (Roziere et al., [2023](#bib.bib26)),
    and PoSE (Zhu et al., [2023](#bib.bib39)) adapts position embedding based on the
    rotary position encoding (RoPE) (Su et al., [2024](#bib.bib29)), with only a few
    fine-tuning steps, the context length can be efficiently extended.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 Transformer 的二次复杂度 (Vaswani et al., [2017](#bib.bib31))，从头开始训练具有广泛上下文窗口的
    LLMs 需要大量计算资源，超出了普通研究人员的能力范围。因此，最近的研究探索了在微调阶段扩展这些模型上下文长度的方法。例如，PI (Chen et al.,
    [2023](#bib.bib10))、NTK-aware (bloc97, [2023](#bib.bib6))、YaRN (Peng et al., [2023](#bib.bib21))、Giraffe
    (Pal et al., [2023](#bib.bib20))、Code LLaMA (Roziere et al., [2023](#bib.bib26))
    和 PoSE (Zhu et al., [2023](#bib.bib39)) 采用了基于旋转位置编码 (RoPE) (Su et al., [2024](#bib.bib29))
    的位置嵌入，通过仅少量的微调步骤，就能高效地扩展上下文长度。
- en: Another strong baseline for long-context modeling is the sliding window method.
    Various sliding window-based variants such as ALibi (Press et al., [2021](#bib.bib22)),
    xPos (Sun et al., [2022](#bib.bib30)), PCW (Ratner et al., [2022](#bib.bib24)),
    LM-Infinit (Han et al., [2023](#bib.bib14)), StreamingLLM (Xiao et al., [2023](#bib.bib34))
    are used to achieve efficient context scaling. Yet they diverge from the global
    perception characteristic of the Transformer, failing to exploit the entire context.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大的长上下文建模基线是滑动窗口方法。各种基于滑动窗口的变体，如 ALibi (Press et al., [2021](#bib.bib22))、xPos
    (Sun et al., [2022](#bib.bib30))、PCW (Ratner et al., [2022](#bib.bib24))、LM-Infinit
    (Han et al., [2023](#bib.bib14))、StreamingLLM (Xiao et al., [2023](#bib.bib34))，被用来实现高效的上下文扩展。然而，它们偏离了
    Transformer 的全局感知特性，未能充分利用整个上下文。
- en: 2.2 Long-Context Benchmarks
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 长上下文基准
- en: Long-context modeling methods are rapidly evolving, yet the quality of existing
    benchmarks does not align with this progress. Synthetic task such as Needle-in-a-Haystack
    (NIAH) (Kamradt, [2023](#bib.bib16)) and Counting stars (Song et al., [2024](#bib.bib28))
    are initially utilized for evaluating long-context language models (LCLMs) due
    to their lower construction costs, but they are indicative of only a surface form
    of long-context understanding.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文建模方法正在迅速发展，但现有基准的质量并未与此进展保持一致。合成任务如 Needle-in-a-Haystack (NIAH) (Kamradt,
    [2023](#bib.bib16)) 和 Counting stars (Song et al., [2024](#bib.bib28)) 最初被用于评估长上下文语言模型
    (LCLMs)，由于其较低的构建成本，但它们仅反映了长上下文理解的表面形式。
- en: Longbench (Bai et al., [2023b](#bib.bib5)), LooGLE  (Li et al., [2023](#bib.bib17))
    and Marathon (Zhang et al., [2023](#bib.bib37)) are earlier benchmarks for comprehensive
    assessment of long context. However, the average length for most tasks is between
    5k and 25k, far less than the window size of LCLMs. L-Eval (An et al., [2023](#bib.bib1)),
    BAMBOO (Dong et al., [2023](#bib.bib11)), CLongEval (Qiu et al., [2024](#bib.bib23))
    and InfiniteBench (Zhang et al., [2024](#bib.bib38)) contain sufficiently long
    evaluation data, and the wide variety of tasks makes the assessment more comprehensive.
    RULER (Hsieh et al., [2024](#bib.bib15)) creates a comprehensive testing method
    with flexibly adjustable length and difficulty, yet they only add distracting
    information to the input of existing short-context QA datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Longbench （Bai 等，[2023b](#bib.bib5)）、LooGLE （Li 等，[2023](#bib.bib17)）和 Marathon （Zhang
    等，[2023](#bib.bib37)）是早期用于全面评估长上下文的基准。然而，大多数任务的平均长度在5k到25k之间，远低于LCLMs的窗口大小。L-Eval （An
    等，[2023](#bib.bib1)）、BAMBOO （Dong 等，[2023](#bib.bib11)）、CLongEval （Qiu 等，[2024](#bib.bib23)）和
    InfiniteBench （Zhang 等，[2024](#bib.bib38)）包含了足够长的评估数据，且任务的多样性使得评估更加全面。RULER （Hsieh
    等，[2024](#bib.bib15)）创建了一个具有灵活可调长度和难度的综合测试方法，但它们仅向现有短上下文问答数据集的输入中添加了干扰信息。
- en: While these long-context benchmarks have their own advantages, we still lack
    a benchmark that is sufficiently long, free from data contamination (Golchin and
    Surdeanu, [2023](#bib.bib13)), and fully aligned with the real-world multi-document
    question answering scenario.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些长上下文基准各有优点，但我们仍缺乏一个足够长、没有数据污染（Golchin 和 Surdeanu，[2023](#bib.bib13)）且完全符合真实世界多文档问答场景的基准。
- en: 2.3 Retrieval Augmented Language Models
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 检索增强语言模型
- en: Leveraging long documents as external knowledge, Retrieval Augmented Language
    Models (RALMs) has achieved comparable or even better performance than LCLMs fine-tuned
    for specific tasks with long document. In previous study, RALMs could directly
    utilize the content retrieved during the inference phase. REPLUG (Shi et al.,
    [2023](#bib.bib27)) treats the language model as a black box and the retrieval
    component as an adjustable plug-and-play module. RETRO (Borgeaud et al., [2022](#bib.bib7))
    use a chunked cross-attention module to incorporate the retrieved text. Additionally,
    Xu et al. ([2023](#bib.bib36)) explored whether RALMs or LCLMs are more suitable
    for long-context tasks under a larger parameter setting. However, there is currently
    a lack of analysis on what tasks RALMs and LCLMs each excel at, thus making it
    difficult to determine which type a black box model belongs to.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 利用长文档作为外部知识，检索增强语言模型（RALMs）在处理长文档的特定任务上表现出与LCLMs相当甚至更好的性能。在以往的研究中，RALMs可以直接利用推理阶段检索到的内容。REPLUG （Shi
    等，[2023](#bib.bib27)）将语言模型视为黑箱，将检索组件视为可调插拔模块。RETRO （Borgeaud 等，[2022](#bib.bib7)）使用了分块的交叉注意力模块来融入检索到的文本。此外，Xu
    等人（[2023](#bib.bib36)）探讨了在更大参数设置下RALMs与LCLMs在长上下文任务中的适用性。然而，目前缺乏对RALMs和LCLMs各自擅长的任务的分析，因此很难确定黑箱模型属于哪种类型。
- en: '3 Loong: A Long-Context Benchmark'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 Loong: 长上下文基准'
- en: 3.1 Overview
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: 'The Loong benchmark comprises tasks across four categories: Spotlight Locating,
    Comparison, Clustering, and Chain of reasoning. To align with realistic scenarios,
    we collect documents from three domains: financial reports, academic papers, and
    legal cases. Furthermore, all tasks are presented in the question-answering format,
    which are all newly annotated by GPT-4o and humans. Totally, Loong includes 1600
    test instances in both Chinese and English, featuring four sets with different
    intervals of context size: $\mathtt{Set1}$ (200-250K). We use tiktoken²²2[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
    tokenizer to tokenize the input and report the number of tokens. [Table 2](#S3.T2
    "In 3.2 Evaluation Task ‣ 3 Loong: A Long-Context Benchmark ‣ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA") and [Appendix C](#A3
    "Appendix C Length Distribution ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") show the details of data statistics. The following
    sections will provide a detailed description of the evaluation task and benchmark
    construction.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'Loong 基准测试包含四个类别的任务：聚光灯定位、比较、聚类和推理链。为了符合现实场景，我们从三个领域收集了文档：财务报告、学术论文和法律案件。此外，所有任务都以问答格式呈现，均由
    GPT-4o 和人工进行新标注。总的来说，Loong 包含 1600 个中英文测试实例，涵盖四个不同上下文大小区间的集合：$\mathtt{Set1}$ (200-250K)。我们使用
    tiktoken²²2[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
    分词器对输入进行分词并报告标记数。[表 2](#S3.T2 "3.2 评估任务 ‣ 3 Loong: 长文本基准测试 ‣ 不遗漏任何文档：基准测试长文本 LLM
    的扩展多文档问答")和[附录 C](#A3 "附录 C 长度分布 ‣ 不遗漏任何文档：基准测试长文本 LLM 的扩展多文档问答") 显示了数据统计的详细信息。以下部分将提供评估任务和基准构建的详细描述。'
- en: 3.2 Evaluation Task
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估任务
- en: '| Category | Avg Token | Language | #Test Instance |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 平均标记数 | 语言 | 测试实例数量 |'
- en: '| Task |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '| Spotlight Locating | 119.3K | EN, ZH | 250 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 聚光灯定位 | 119.3K | EN, ZH | 250 |'
- en: '| Comparison | 110.6K | EN, ZH | 300 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 比较 | 110.6K | EN, ZH | 300 |'
- en: '| Clustering | 109.8K | EN, ZH | 641 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 109.8K | EN, ZH | 641 |'
- en: '| Chain of Reasoning | 103.9K | EN, ZH | 409 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 推理链 | 103.9K | EN, ZH | 409 |'
- en: '| Sub Task |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 子任务 |'
- en: '| Sequential Enumeration | 103K | EN, ZH | 87 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 顺序枚举 | 103K | EN, ZH | 87 |'
- en: '| Extremum Acquisition | 115K | EN, ZH | 143 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 极值获取 | 115K | EN, ZH | 143 |'
- en: '| Range Awareness | 111K | EN, ZH | 70 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 范围意识 | 111K | EN, ZH | 70 |'
- en: '| Report Integration | 117K | EN, ZH | 250 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 报告整合 | 117K | EN, ZH | 250 |'
- en: '| Citation&Reference | 105K | EN | 270 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 引用与参考文献 | 105K | EN | 270 |'
- en: '| Case Classification | 106K | ZH | 121 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 案例分类 | 106K | ZH | 121 |'
- en: '| Temporal Analysis | 112K | EN, ZH | 100 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 时间分析 | 112K | EN, ZH | 100 |'
- en: '| Citation Chain | 91K | EN | 130 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 引用链 | 91K | EN | 130 |'
- en: '| Link the Links | 117K | ZH | 113 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 链接链接 | 117K | ZH | 113 |'
- en: '| Solitaire | 94K | ZH | 66 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 单人游戏 | 94K | ZH | 66 |'
- en: '| Domain |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 领域 |'
- en: '| Financial Reports | 117.5K | EN, ZH | 700 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 财务报告 | 117.5K | EN, ZH | 700 |'
- en: '| Legal Cases | 107.2K | ZH | 500 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 法律案件 | 107.2K | ZH | 500 |'
- en: '| Academic Papers | 100.9K | EN | 400 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 学术论文 | 100.9K | EN | 400 |'
- en: '| Length Set |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 长度集合 |'
- en: '| $\mathtt{Set1}$ (10-50K) | 37.8K | EN, ZH | 323 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10-50K) | 37.8K | EN, ZH | 323 |'
- en: '| $\mathtt{Set2}$ (50-100K) | 75.6K | EN, ZH | 564 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50-100K) | 75.6K | EN, ZH | 564 |'
- en: '| $\mathtt{Set3}$ (100-200K) | 138.9K | EN, ZH | 481 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100-200K) | 138.9K | EN, ZH | 481 |'
- en: '| $\mathtt{Set4}$ (200-250K) | 233.9K | EN, ZH | 232 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200-250K) | 233.9K | EN, ZH | 232 |'
- en: 'Table 2: Data statistics of Loong benchmark.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: Loong 基准测试的数据统计。'
- en: 'Based on various multi-document semantic relationships and LLMs’ handling of
    multi-document input, we propose new task categories for multi-document long-context
    modeling and closer alignment with real-world scenarios. [Figure 2](#S1.F2 "In
    1 Introduction ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with
    Extended Multi-Doc QA") illustrates the evaluation tasks of the Loong benchmark.
    [Appendix B](#A2 "Appendix B Test Case ‣ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA") shows the detailed test case and
    prompt of each task. [Appendix E](#A5 "Appendix E Comparison of Evidence Distribution
    ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA") presents a comparison of evidence distribution between Loong and LongBench.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基于各种多文档语义关系和LLMs处理多文档输入的能力，我们提出了多文档长上下文建模的新任务类别，以更贴近真实世界场景。 [图2](#S1.F2 "在1引言
    ‣ 不遗漏文档：基于扩展多文档QA的长上下文LLMs基准测试")展示了Loong基准的评估任务。 [附录B](#A2 "附录B 测试案例 ‣ 不遗漏文档：基于扩展多文档QA的长上下文LLMs基准测试")展示了每个任务的详细测试案例和提示。
    [附录E](#A5 "附录E 证据分布比较 ‣ 不遗漏文档：基于扩展多文档QA的长上下文LLMs基准测试")展示了Loong和LongBench之间的证据分布比较。
- en: 3.2.1 Spotlight Locating
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 聚光灯定位
- en: 'The spotlight locating task is designed to assess the model’s capability for
    knowledge localization, which constitutes the foundation ability of long-context
    processing. In this task, the evidences are contained in only one of multiple
    documents, which is the atomic setting of the key information locating. Spotlight
    locating task is aimed at examining the LLMs’ ability to search the evidence within
    one document from multiple ones. Other documents, which are in the same domain
    and have similar semantics as the document but are unrelated to the question,
    will serve as noise texts. The upper left of [Figure 2](#S1.F2 "In 1 Introduction
    ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA") provides an example of the spotlight locating task.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 聚光灯定位任务旨在评估模型在知识定位方面的能力，这构成了长上下文处理的基础能力。在此任务中，证据仅包含在多个文档中的一个，这就是关键性信息定位的原子设置。聚光灯定位任务旨在考察LLMs从多个文档中搜索一个文档中的证据的能力。其他文档，虽然在相同领域并且与文档有相似语义，但与问题无关，将作为噪声文本。
    [图2](#S1.F2 "在1引言 ‣ 不遗漏文档：基于扩展多文档QA的长上下文LLMs基准测试")的左上角提供了一个聚光灯定位任务的示例。
- en: 3.2.2 Comparison
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 比较
- en: The comparison task is primarily aimed at evaluating the model’s ability to
    compare multi-source information with long contexts. In this event, the evidence
    supporting the answer are distributed across multiple documents, testing the LLMs’
    ability to locate dispersed evidence and to correlate and compare them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 比较任务主要旨在评估模型比较多源信息和长上下文的能力。在此事件中，支持答案的证据分布在多个文档中，测试LLMs定位分散证据、关联和比较它们的能力。
- en: 'Comparison task includes three sub-tasks: 1) Sequential Enumeration: Based
    on the concrete numerical value of a specific attribute, it requires the model
    to list all specific values corresponding to that attribute across multiple documents
    in a given order. 2) Extremum Acquisition: It requires the model to deduce the
    extremum of all values corresponding to certain attributes in multiple documents.
    3) Range Awareness: Given a specific numerical or conceptual range, the model
    should output all objects within multiple documents that meet the condition. The
    upper right of [Figure 2](#S1.F2 "In 1 Introduction ‣ Leave No Document Behind:
    Benchmarking Long-Context LLMs with Extended Multi-Doc QA") gives an example of
    comparison task.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 比较任务包括三个子任务：1）顺序枚举：根据特定属性的具体数值，要求模型列出多个文档中与该属性对应的所有具体值，并按给定顺序排列。2）极值获取：要求模型推断多个文档中某些属性的所有值的极值。3）范围感知：给定特定的数值或概念范围，模型应输出多个文档中符合条件的所有对象。
    [图2](#S1.F2 "在1引言 ‣ 不遗漏文档：基于扩展多文档QA的长上下文LLMs基准测试")的右上角给出了比较任务的示例。
- en: 3.2.3 Clustering
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 聚类
- en: The clustering task entails an assessment of the model’s ability to cluster
    key information based on specific conditions across multi-document long contexts.
    This task claims that LLMs cluster relevant evidence scattered in multiple documents
    based on the specified criteria. Furthermore, it necessitates the extraction of
    pertinent information from documents and the integration of this information by
    grouping according to conditions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类任务涉及评估模型根据特定条件在多文档长上下文中聚类关键信息的能力。此任务声明 LLMs 根据指定标准聚类分散在多个文档中的相关证据。此外，它还要求从文档中提取相关信息，并根据条件对这些信息进行整合。
- en: 'The clustering task encompasses three sub-tasks: 1) Report Integration: This
    sub-task requires the model to group the evidence existing in the provided financial
    reports into corresponding sets based on textual or numerical criteria. 2) Citation&Reference:
    For a given paper, the model is tasked with identifying its citations and references
    from the candidate papers. 3) Case Classification: Given the causes of several
    legal cases, the model is required to accurately categorize judgment documents.
    The bottom left of [Figure 2](#S1.F2 "In 1 Introduction ‣ Leave No Document Behind:
    Benchmarking Long-Context LLMs with Extended Multi-Doc QA") depicts an example
    of the clustering task.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类任务包括三个子任务：1) 报告整合：此子任务要求模型根据文本或数值标准将提供的财务报告中的证据分组到相应的集合中。2) 引用&参考：对于给定的论文，模型的任务是从候选论文中识别其引用和参考文献。3)
    案例分类：根据几个法律案件的诉因，模型需要准确分类判断文件。[图 2](#S1.F2 "在 1 引言 ‣ 不留文档：基准测试长上下文 LLM 的扩展多文档
    QA")的左下角描绘了聚类任务的一个示例。
- en: 3.2.4 Chain of Reasoning
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 推理链
- en: The chain of reasoning task requires the model to engage in multi-document reasoning
    along a logical pathway. This task evaluates the model’s proficiency in logical
    reasoning, which requires LLMs to locate the corresponding evidence within multiple
    documents and model the logical relationships among them for deducing the answer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 推理链任务要求模型沿逻辑路径进行多文档推理。此任务评估模型的逻辑推理能力，要求 LLMs 在多个文档中定位相应的证据，并建模它们之间的逻辑关系以推导答案。
- en: 'The chain of reasoning task contains four sub-tasks: 1) Temporal Analysis:
    This task requires the model to analyze the changes or trends of a particular
    attribute based on the temporal relationship, such as taking into account the
    financial reports of a certain company over consecutive years or multiple quarters.
    2) Citation Chain: This task requires the model to accurately understand each
    paper’s content and its interconnections, ultimately inferring the linear citation
    relationships among them. 3) Link the Links: This task involves presenting fact
    descriptions and trial results from different judgment documents separately. The
    model is tasked with accurately pairing each fact description with its corresponding
    trial result. 4) Solitaire: This task first requires the model to match causes
    of action with judgment documents correctly, and then to sequentially infer multiple
    judgment documents based on the given sequence of causes of action. The bottom
    right of [Figure 2](#S1.F2 "In 1 Introduction ‣ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA") gives an example of the chain of
    reasoning task.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 推理链任务包含四个子任务：1) 时间分析：此任务要求模型基于时间关系分析特定属性的变化或趋势，例如考虑某公司连续几年或多个季度的财务报告。2) 引用链：此任务要求模型准确理解每篇论文的内容及其相互联系，最终推断它们之间的线性引用关系。3)
    链接链接：此任务涉及分别呈现来自不同判断文件的事实描述和审判结果。模型的任务是准确地将每个事实描述与其对应的审判结果配对。4) 排名游戏：此任务首先要求模型正确匹配诉因与判断文件，然后根据给定的诉因顺序依次推断多个判断文件。[图
    2](#S1.F2 "在 1 引言 ‣ 不留文档：基准测试长上下文 LLM 的扩展多文档 QA")的右下角给出了推理链任务的一个示例。
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 聚焦定位 | 比较 | 聚类 | 推理链 | 总体 |'
- en: '| GPT-4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
- en: '| Gemini-Pro1.5 (1000K) | 75.02 | 0.56 | 49.94 | 0.27 | 44.10 | 0.09 | 64.97
    | 0.37 | 55.37 | 0.27 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 75.02 | 0.56 | 49.94 | 0.27 | 44.10 | 0.09 | 64.97
    | 0.37 | 55.37 | 0.27 |'
- en: '| Claude3.5-Sonnet (200K) | 58.45 | 0.49 | 54.21 | 0.35 | 45.77 | 0.07 | 43.92
    | 0.25 | 48.85 | 0.23 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 58.45 | 0.49 | 54.21 | 0.35 | 45.77 | 0.07 | 43.92
    | 0.25 | 48.85 | 0.23 |'
- en: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
- en: '| Claude3-Haiku (200K) | 68.68 | 0.59 | 42.10 | 0.21 | 35.04 | 0.02 | 47.59
    | 0.17 | 44.88 | 0.19 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 68.68 | 0.59 | 42.10 | 0.21 | 35.04 | 0.02 | 47.59
    | 0.17 | 44.88 | 0.19 |'
- en: '| Kimi-Chat (200k) | 60.98 | 0.50 | 34.74 | 0.13 | 28.76 | 0.04 | 38.52 | 0.15
    | 37.49 | 0.16 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 60.98 | 0.50 | 34.74 | 0.13 | 28.76 | 0.04 | 38.52 | 0.15
    | 37.49 | 0.16 |'
- en: '| GLM4-9B-Chat (1000K) | 57.35 | 0.47 | 40.38 | 0.20 | 28.52 | 0.02 | 39.94
    | 0.16 | 38.31 | 0.16 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 57.35 | 0.47 | 40.38 | 0.20 | 28.52 | 0.02 | 39.94
    | 0.16 | 38.31 | 0.16 |'
- en: 'Table 3: Overall results on four evaluation tasks. For each task, the indicator
    on the left represents the Avg Scores (0~100), while the right one represents
    the Perfect Rate (0~1).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 3: 四项评估任务的总体结果。对于每项任务，左侧指标表示平均分数（0~100），右侧指标表示完美率（0~1）。'
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 聚焦定位 | 比较 | 聚类 | 推理链 | 总体 |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
- en: '| Gemini-Pro1.5 (1000K) | 75.00 | 0.60 | 54.88 | 0.28 | 56.15 | 0.23 | 70.64
    | 0.37 | 63.36 | 0.34 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 75.00 | 0.60 | 54.88 | 0.28 | 56.15 | 0.23 | 70.64
    | 0.37 | 63.36 | 0.34 |'
- en: '| Claude3.5-Sonnet (200K) | 60.85 | 0.55 | 69.07 | 0.47 | 58.63 | 0.13 | 68.57
    | 0.50 | 63.69 | 0.37 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 60.85 | 0.55 | 69.07 | 0.47 | 58.63 | 0.13 | 68.57
    | 0.50 | 63.69 | 0.37 |'
- en: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
- en: '| Claude3-Haiku (200K) | 60.94 | 0.55 | 59.97 | 0.40 | 45.53 | 0.04 | 66.85
    | 0.34 | 57.14 | 0.28 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 60.94 | 0.55 | 59.97 | 0.40 | 45.53 | 0.04 | 66.85
    | 0.34 | 57.14 | 0.28 |'
- en: '| Kimi-Chat (200k) | 81.11 | 0.74 | 46.70 | 0.20 | 47,84 | 0.07 | 53.77 | 0.17
    | 55.02 | 0.24 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 81.11 | 0.74 | 46.70 | 0.20 | 47,84 | 0.07 | 53.77 | 0.17
    | 55.02 | 0.24 |'
- en: '| GLM4-9B-Chat (1000K) | 63.11 | 0.53 | 54.10 | 0.27 | 39.50 | 0.08 | 56.32
    | 0.28 | 51.43 | 0.25 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 63.11 | 0.53 | 54.10 | 0.27 | 39.50 | 0.08 | 56.32
    | 0.28 | 51.43 | 0.25 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
- en: '| Gemini-Pro1.5 (1000K) | 76.50 | 0.57 | 54.51 | 0.34 | 44.58 | 0.09 | 64.87
    | 0.34 | 55.56 | 0.26 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 76.50 | 0.57 | 54.51 | 0.34 | 44.58 | 0.09 | 64.87
    | 0.34 | 55.56 | 0.26 |'
- en: '| Claude3.5-Sonnet (200K) | 63.83 | 0.53 | 58.90 | 0.39 | 50.96 | 0.10 | 46.09
    | 0.26 | 52.73 | 0.24 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 63.83 | 0.53 | 58.90 | 0.39 | 50.96 | 0.10 | 46.09
    | 0.26 | 52.73 | 0.24 |'
- en: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
- en: '| Claude3-Haiku (200K) | 73.71 | 0.66 | 41.90 | 0.22 | 36.18 | 0.02 | 50.20
    | 0.15 | 45.45 | 0.17 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 73.71 | 0.66 | 41.90 | 0.22 | 36.18 | 0.02 | 50.20
    | 0.15 | 45.45 | 0.17 |'
- en: '| Kimi-Chat (200k) | 72.82 | 0.52 | 46.77 | 0.21 | 33.46 | 0.06 | 40.51 | 0.15
    | 42.40 | 0.16 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 72.82 | 0.52 | 46.77 | 0.21 | 33.46 | 0.06 | 40.51 | 0.15
    | 42.40 | 0.16 |'
- en: '| GLM4-9B-Chat (1000K) | 65.04 | 0.54 | 41.80 | 0.23 | 30.72 | 0.02 | 42.34
    | 0.17 | 40.19 | 0.17 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 65.04 | 0.54 | 41.80 | 0.23 | 30.72 | 0.02 | 42.34
    | 0.17 | 40.19 | 0.17 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
- en: '| Gemini-Pro1.5 (1000K) | 81.25 | 0.56 | 44.66 | 0.20 | 39.90 | 0.05 | 58.38
    | 0.36 | 52.05 | 0.24 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 81.25 | 0.56 | 44.66 | 0.20 | 39.90 | 0.05 | 58.38
    | 0.36 | 52.05 | 0.24 |'
- en: '| Claude3.5-Sonnet (200K) | 65.36 | 0.56 | 50.32 | 0.34 | 37.79 | 0.03 | 25.95
    | 0.11 | 42.06 | 0.19 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 65.36 | 0.56 | 50.32 | 0.34 | 37.79 | 0.03 | 25.95
    | 0.11 | 42.06 | 0.19 |'
- en: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
- en: '| Claude3-Haiku (200K) | 77.81 | 0.67 | 37.07 | 0.17 | 30.94 | 0.01 | 36.87
    | 0.12 | 41.41 | 0.18 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 77.81 | 0.67 | 37.07 | 0.17 | 30.94 | 0.01 | 36.87
    | 0.12 | 41.41 | 0.18 |'
- en: '| Kimi-Chat (200k) | 62.13 | 0.54 | 24.20 | 0.05 | 21.98 | 0.01 | 31.02 | 0.14
    | 31.37 | 0.14 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 62.13 | 0.54 | 24.20 | 0.05 | 21.98 | 0.01 | 31.02 | 0.14
    | 31.37 | 0.14 |'
- en: '| GLM4-9B-Chat (1000K) | 69.19 | 0.56 | 37.99 | 0.18 | 26.63 | 0.01 | 32.30
    | 0.09 | 37.36 | 0.16 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 69.19 | 0.56 | 37.99 | 0.18 | 26.63 | 0.01 | 32.30
    | 0.09 | 37.36 | 0.16 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
- en: '| Gemini-Pro1.5 (1000K) | 62.23 | 0.49 | 43.08 | 0.20 | 36.48 | 0.00 | 68.51
    | 0.49 | 50.70 | 0.25 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro1.5 (1000K) | 62.23 | 0.49 | 43.08 | 0.20 | 36.48 | 0.00 | 68.51
    | 0.49 | 50.70 | 0.25 |'
- en: '| Claude3.5-Sonnet (200K) | 36.91 | 0.24 | 28.82 | 0.05 | 28.68 | 0.00 | 28.77
    | 0.08 | 30.51 | 0.08 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Claude3.5-Sonnet (200K) | 36.91 | 0.24 | 28.82 | 0.05 | 28.68 | 0.00 | 28.77
    | 0.08 | 30.51 | 0.08 |'
- en: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
- en: '| Claude3-Haiku (200K) | 53.26 | 0.40 | 27.00 | 0.03 | 25.36 | 0.00 | 28.11
    | 0.05 | 32.15 | 0.10 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Claude3-Haiku (200K) | 53.26 | 0.40 | 27.00 | 0.03 | 25.36 | 0.00 | 28.11
    | 0.05 | 32.15 | 0.10 |'
- en: '| Kimi-Chat (200k) | 20.17 | 0.12 | 9.17 | 0.00 | 5.65 | 0.00 | 22.61 | 0.11
    | 13.50 | 0.05 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat (200k) | 20.17 | 0.12 | 9.17 | 0.00 | 5.65 | 0.00 | 22.61 | 0.11
    | 13.50 | 0.05 |'
- en: '| GLM4-9B-Chat (1000K) | 15.67 | 0.12 | 21.33 | 0.05 | 12.35 | 0.00 | 21.04
    | 0.05 | 16.84 | 0.05 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GLM4-9B-Chat (1000K) | 15.67 | 0.12 | 21.33 | 0.05 | 12.35 | 0.00 | 21.04
    | 0.05 | 16.84 | 0.05 |'
- en: 'Table 4: The performance of LLMs on four evaluation tasks with different length
    sets. For each task, the indicator on the left represents the Avg Scores (0~100),
    while the right one represents the Perfect Rate (0~1).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同长度集上LLM的四个评估任务的表现。每个任务的左侧指标表示平均分数（0~100），右侧指标表示完美率（0~1）。
- en: 3.3 Benchmark Construction
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基准测试构建
- en: 3.3.1 Data Collection
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 数据收集
- en: 'We established six criteria for the manual collection of the required English
    and Chinese documents: (1) Timeliness: The majority of the documents are the latest
    ones from the year 2024; (2) Accessibility: The data is publicly available and
    permitted for download and collection; (3) Appropriate Length: Collecting longer
    documents as much as possible and ensure they fit within the four designated length
    sets; (4) Parseability: Chosen documents are easy to process and parse, facilitating
    conversion into natural language text; (5) Categorizability: Documents can be
    manually sorted based on certain attributes, such as case type, research theme,
    or company category, allowing for organized archival; (6) Authoritativeness: All
    documents are collected from scratch from official websites (e.g. China Judge
    Online³³3[https://wenshu.court.gov.cn/](https://wenshu.court.gov.cn/), U.S. SEC⁴⁴4[https://www.sec.gov/](https://www.sec.gov/),
    cninf⁵⁵5[http://www.cninfo.com.cn/](http://www.cninfo.com.cn/), Arxiv⁶⁶6[https://arxiv.org/](https://arxiv.org/),
    Semantic Scholar⁷⁷7[https://www.semanticscholar.org/](https://www.semanticscholar.org/)),
    ensuring the quality and authority of the documents.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为手动收集所需的英文和中文文档建立了六个标准：（1）时效性：大多数文档为2024年的最新文档；（2）可获取性：数据公开可用，允许下载和收集；（3）适当长度：尽可能收集较长的文档，并确保它们符合四个指定的长度集；（4）可解析性：选择的文档易于处理和解析，便于转换为自然语言文本；（5）可分类性：文档可以根据某些属性（如案件类型、研究主题或公司类别）手动排序，方便有序归档；（6）权威性：所有文档均从官方网站（例如，中国裁判文书网³³³[https://wenshu.court.gov.cn/](https://wenshu.court.gov.cn/)，美国证券交易委员会⁴⁴⁴[https://www.sec.gov/](https://www.sec.gov/)，cninf⁵⁵⁵[http://www.cninfo.com.cn/](http://www.cninfo.com.cn/)，Arxiv⁶⁶⁶[https://arxiv.org/](https://arxiv.org/)，Semantic
    Scholar⁷⁷⁷[https://www.semanticscholar.org/](https://www.semanticscholar.org/))从头收集，确保文档的质量和权威性。
- en: Specifically, regarding financial reports, we primarily collect the latest quarterly
    and annual reports for the year 2024, totaling 574 documents. For legal documents,
    our collection consists exclusively of cases adjudicated by the higher and intermediate
    courts in 2024, amounting to 629 documents. As for academic papers, our focus
    is on procuring the latest articles from arXiv in 2024, with a total of 764 papers.
    Additionally, to meet the requirements of the chain of reasoning task, we gather
    a small portion of financial reports and academic papers from before 2024. Upon
    the collection of documents, we first parse these documents, converting them uniformly
    into TXT format. Subsequently, we carry out further data cleansing, removing any
    portions that contain personal information.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，关于财务报告，我们主要收集2024年的最新季度和年度报告，总计574份。对于法律文档，我们的收集仅包括2024年由高级和中级法院裁决的案件，总计629份。至于学术论文，我们专注于采购2024年来自arXiv的最新文章，总计764篇。此外，为满足推理任务的要求，我们收集了一小部分2024年之前的财务报告和学术论文。文档收集后，我们首先对这些文档进行解析，统一转换为TXT格式。随后，我们进行进一步的数据清理，去除包含个人信息的部分。
- en: 3.3.2 Annotation Process
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 注释过程
- en: Compared to annotating short texts, annotating long texts is more challenging.
    To address this issue, we designed innovative annotation workflows to reduce the
    cost of annotation while ensuring quality.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与标注短文本相比，标注长文本更加具有挑战性。为解决这一问题，我们设计了创新的标注工作流程，以降低标注成本，同时确保质量。
- en: For financial reports, we compress the information contained within the long
    context, breaking down the annotation process into numerous simple tasks. We initially
    manually identify hundreds of key attributes that cover the important information
    in the long context. Subsequently, we employ GPT-4o to execute the relatively
    simple task of information extraction, pulling the values corresponding to these
    key attributes. After obtaining the key attributes and their corresponding values,
    we can proceed to annotate only the compressed information, eliminating the need
    to refer back to the original lengthy texts. For legal cases, we follow the classification
    provided by China Judge Online, manually downloading judgment documents sorted
    by different causes of action and case types. Additionally, we use a rule-based
    method to segment each judgment document into its factual statement and verdict
    sections. For academic papers, we leverage the Semantic Scholar website’s API
    to access the target paper’s citations and references. Moreover, by utilizing
    the bbl files of each arXiv paper, we write scripts to recursively collect articles
    that meet the requirements of the linear citation chain task.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于财务报告，我们压缩长文本中的信息，将标注过程分解为多个简单任务。我们首先手动识别数百个关键属性，这些属性涵盖了长文本中的重要信息。随后，我们利用GPT-4o执行相对简单的信息提取任务，提取与这些关键属性对应的值。在获得关键属性及其对应值后，我们可以仅对压缩信息进行标注，无需参考原始冗长的文本。对于法律案件，我们遵循中国法官网提供的分类，手动下载按不同诉讼原因和案件类型排序的判决文书。此外，我们使用基于规则的方法将每份判决文书分割为事实陈述和裁决部分。对于学术论文，我们利用Semantic
    Scholar网站的API访问目标论文的引文和参考文献。此外，通过利用每篇arXiv论文的bbl文件，我们编写脚本递归收集符合线性引文链任务要求的文章。
- en: 'During the question-and-answer annotation phase, we adopt two approaches: (1)
    Template-based: We design question types and templates, and based on pre-classified
    documents, we construct Q&A pairs using rules. (2) Free annotation: Referring
    to the compressed information of multiple documents, we design prompts with four
    different task descriptions. We employ GPT-4o to generate Q&A pairs for each task.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在问答标注阶段，我们采用了两种方法：（1）基于模板的方法：我们设计问题类型和模板，并根据预分类的文档，使用规则构建问答对。（2）自由标注：参考多份文档的压缩信息，我们设计了四种不同任务描述的提示。我们利用GPT-4o生成每个任务的问答对。
- en: 3.3.3 Quality Control
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 质量控制
- en: 'Throughout the annotation process, we employ several methods to ensure accuracy:
    (1) Evidence Recall: By designing prompts that not only prompt GPT-4o to generate
    labels but also to recall evidence supporting the labels from the text, significantly
    enhancing the accuracy in practical applications. (2) Self-Check: GPT-4o reviews
    the original text to re-evaluate and correct any mistakes in the generated labels.
    (3) Manual Check: We manually review and confirm the quality of annotations, eliminating
    any unreasonable or low-quality questions. Additionally, we also take into account
    the distribution and number of different length sets, sub-tasks, and language.
    From a pool of 2,814 entries, we conduct a secondary selection process, ultimately
    choosing 1,600 entries for our final benchmark.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个标注过程中，我们采用了几种方法以确保准确性：（1）证据回顾：通过设计提示，不仅促使GPT-4o生成标签，还从文本中回顾支持标签的证据，显著提高了实际应用中的准确性。（2）自我检查：GPT-4o审查原始文本，以重新评估并纠正生成标签中的任何错误。（3）人工检查：我们手动审查和确认标注质量，消除任何不合理或低质量的问题。此外，我们还考虑了不同长度集、子任务和语言的分布和数量。从2,814条条目中，我们进行二次筛选，最终选择了1,600条作为我们的最终基准。
- en: 4 Experiments
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Setup
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Models We evaluate six advanced long-context LLMs, with their context window
    sizes ranging from 128K to 1000K, including API-based LLMs: GPT-4o-128K (OpenAI,
    [2023](#bib.bib19)), Gemini-Pro1.5-1000K (Reid et al., [2024](#bib.bib25)), Claude3.5-Sonnet-200K (Anthropic,
    [2024b](#bib.bib3)), Claude3-Haiku-200K (Anthropic, [2024a](#bib.bib2)), Kimi-Chat-200K⁸⁸8[https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
    and Open-sourced LLMs: Qwen2-72B-Instruct-128K (Bai et al., [2023a](#bib.bib4)),
    GLM4-9B-Chat-1000K (Du et al., [2022](#bib.bib12)).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 我们评估了六个先进的长文本上下文 LLM，其上下文窗口大小从128K到1000K，包括基于 API 的 LLM：GPT-4o-128K (OpenAI,
    [2023](#bib.bib19))，Gemini-Pro1.5-1000K (Reid et al., [2024](#bib.bib25))，Claude3.5-Sonnet-200K
    (Anthropic, [2024b](#bib.bib3))，Claude3-Haiku-200K (Anthropic, [2024a](#bib.bib2))，Kimi-Chat-200K⁸⁸8
    [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/) 和开源 LLM：Qwen2-72B-Instruct-128K
    (Bai et al., [2023a](#bib.bib4))，GLM4-9B-Chat-1000K (Du et al., [2022](#bib.bib12))。
- en: 'Evaluation Metric In the long-context question-answering scenarios, traditional
    evaluation metrics F1 and Rouge-L may lead to inaccurate responses. Recent research (Liu
    et al., [2024](#bib.bib18); Wang et al., [2024](#bib.bib32)) indicates that the
    GPT-4 (OpenAI, [2023](#bib.bib19)) evaluator demonstrates high consistency with
    human evaluations, making it a reasonably reliable annotator. Building on these
    considerations, we prompt GPT-4 as a judge to evaluate the model’s output based
    on the golden answer and the question’s requirements from three aspects: Accuracy,
    Hallucinations, and Completeness, scoring from 0 to 100\. For a detailed prompt,
    please refer to the [Appendix A](#A1 "Appendix A GPT4-as-the-Judge Prompt ‣ Leave
    No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA").
    We also design two indicators: (1) Avg Scores: the average value of scores given
    by GPT-4 for all questions; (2) Perfect Rate: the proportion of cases scoring
    100 out of the total cases. The latter is a more stringent evaluation metric compared
    to the former.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '评估指标 在长文本问答场景中，传统评估指标 F1 和 Rouge-L 可能导致不准确的回答。最近的研究 (Liu et al., [2024](#bib.bib18)；Wang
    et al., [2024](#bib.bib32)) 表明，GPT-4 (OpenAI, [2023](#bib.bib19)) 评估器与人工评估具有高度一致性，使其成为一个相对可靠的注释工具。基于这些考虑，我们使用
    GPT-4 作为评判者，从准确性、幻觉和完整性三个方面对模型输出进行评估，评分范围从 0 到 100。有关详细提示，请参见[附录 A](#A1 "Appendix
    A GPT4-as-the-Judge Prompt ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA")。我们还设计了两个指标：(1) 平均得分：GPT-4 对所有问题给出的得分的平均值；(2)
    完美率：在总案例中得分为100的案例比例。后者是比前者更严格的评估指标。'
- en: 'Prompt Templates For different sub-tasks, we require the model to follow the
    given instructions and output the answer according to the specific prompts shown
    in [Appendix B](#A2 "Appendix B Test Case ‣ Leave No Document Behind: Benchmarking
    Long-Context LLMs with Extended Multi-Doc QA").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '针对不同的子任务，我们要求模型遵循给定的指令，并根据[附录 B](#A2 "Appendix B Test Case ‣ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA")中所示的具体提示输出答案。'
- en: Input Truncation Due to input length limits, we assess whether adding a document
    would exceed the model’s processing length when concatenating multiple documents.
    If appending the document would surpass the model’s capacity, we discard it from
    the concatenation process. The evaluation and selection process continues until
    we have reviewed all documents that need concatenation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输入截断 由于输入长度限制，我们评估在连接多个文档时，添加文档是否会超出模型的处理长度。如果附加文档会超过模型的容量，我们将其从连接过程中删除。评估和选择过程将继续，直到我们审查完所有需要连接的文档。
- en: Implement Details We set ‘temperature = 0’ to eliminate randomness and keep
    other hyper-parameters default. For API-Based LLMs, we directly utilize the official
    API for testing. Since the Kimi-Chat-200k currently does not provide an interface,
    we manually input content on the web. As for open-source models, we conduct experiments
    on a server with 8$\times$A100 80GB.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节 我们设置了‘temperature = 0’以消除随机性，其他超参数保持默认。对于基于 API 的 LLM，我们直接利用官方 API 进行测试。由于
    Kimi-Chat-200k 当前未提供接口，我们在网页上手动输入内容。至于开源模型，我们在配置为8$\times$A100 80GB的服务器上进行实验。
- en: 4.2 Main Results
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'We assess six advanced LLMs on the Loong benchmark. The main results are shown
    in [Table 3](#S3.T3 "In 3.2.4 Chain of Reasoning ‣ 3.2 Evaluation Task ‣ 3 Loong:
    A Long-Context Benchmark ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") and [Table 4](#S3.T4 "In 3.2.4 Chain of Reasoning
    ‣ 3.2 Evaluation Task ‣ 3 Loong: A Long-Context Benchmark ‣ Leave No Document
    Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"). We can see
    that Gemini-Pro-1.5 shows the best overall performance, especially excelling in
    the processing of ultra-long context within $\mathtt{Set3}$. Its comprehensive
    score reached 55.37 with the perfect rate of 27%, followed by GPT-4o. Besides,
    the long-context modeling capacity of open-source models still falls short when
    compared to that of the most powerful closed-source models in the Loong. Additionally,
    larger-parameter models outperform their smaller counterparts within the same
    window size, indicating the advantages of scaling up model sizes for improved
    long-context modeling. The overall assessment results highlight that even the
    most advanced long-context LLMs currently fail to achieve passing marks, particularly
    in terms of the perfect rate. This suggests that there exists significant room
    for improvement in the long-context modeling capabilities of LLMs.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在Loong基准测试上评估了六个先进的LLM。主要结果展示在[表3](#S3.T3 "在3.2.4 推理链 ‣ 3.2 评估任务 ‣ 3 Loong:
    一个长文档基准 ‣ 不遗漏任何文档：用扩展的多文档问答来基准测试长文档LLM")和[表4](#S3.T4 "在3.2.4 推理链 ‣ 3.2 评估任务 ‣
    3 Loong: 一个长文档基准 ‣ 不遗漏任何文档：用扩展的多文档问答来基准测试长文档LLM")中。我们可以看到，Gemini-Pro-1.5表现最佳，特别是在处理$\mathtt{Set3}$中的超长文档时表现突出。其综合得分达到55.37，完美率为27%，其次是GPT-4o。此外，与最强的闭源模型相比，开源模型的长文档建模能力仍显不足。此外，在相同窗口大小下，参数较大的模型优于参数较小的模型，这表明模型规模扩大有助于提高长文档建模能力。总体评估结果表明，即使是最先进的长文档LLM，目前也未能达到及格分数，特别是在完美率方面。这表明LLM的长文档建模能力仍有显著提升空间。'
- en: 4.3 Task Analysis
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 任务分析
- en: Analyzing performance across different tasks, models exhibit their best performance
    in the spotlight locating task. This can be attributed to the task’s relative
    simplicity, which tests the foundational capabilities of long-context modeling.
    Moreover, the evidence is only distributed within a single document, making it
    easier to locate and less prone to confusion. In contrast, due to the requirements
    of multi-source information inference, the comparison and cluster tasks present
    greater challenges, leading to model underperformance. These tasks necessitate
    not only the collection of evidence across documents but also involve complex
    reasoning processes such as matching, contrasting, and classification. Thus, they
    more rigorously test the higher-order capabilities of long-context modeling, revealing
    significant gaps in the current models’ abilities. Regarding the chain of reasoning
    task, models perform well within Set1\. However, as the context length increases,
    their performance drastically declines. This suggests that within the scope of
    long-context modeling capabilities, LLMs possess adequate skills in temporal analysis,
    logical sequencing, and linking multiple concepts. Nevertheless, an overflow in
    context length leads to the loss of key evidence, severely impacting the accuracy
    of chain reasoning tasks.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 分析不同任务的表现，模型在聚光灯定位任务中的表现最佳。这可以归因于任务的相对简单性，它测试了长文档建模的基础能力。此外，证据仅分布在单个文档内，使其更易于定位且不易混淆。相比之下，由于需要多源信息推理，比较和聚类任务面临更大挑战，导致模型表现不佳。这些任务不仅需要跨文档收集证据，还涉及匹配、对比和分类等复杂推理过程。因此，它们更严格地测试了长文档建模的高阶能力，揭示了当前模型能力的显著差距。关于推理链任务，模型在Set1中表现良好。然而，随着上下文长度的增加，它们的表现急剧下降。这表明，在长文档建模能力的范围内，LLM在时间分析、逻辑排序和链接多个概念方面具有足够的技能。然而，上下文长度的溢出导致关键证据丢失，严重影响推理链任务的准确性。
- en: '![Refer to caption](img/e260e607c02eb69963bf19736f0570f0.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e260e607c02eb69963bf19736f0570f0.png)'
- en: (a) The overall results on all length sets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 所有长度集的整体结果。
- en: '![Refer to caption](img/8f0c9ffeba6740bb162ef5d0ca17bf00.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8f0c9ffeba6740bb162ef5d0ca17bf00.png)'
- en: (b) The detailed results on different length sets. The baseline means the setting
    without RAG.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不同长度集的详细结果。基线指的是没有RAG的设置。
- en: 'Figure 3: The results on all tasks after adding RAG module. We only represents
    the Avg Scores (0~100).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：添加 RAG 模块后的所有任务结果。我们仅表示平均分数（0~100）。
- en: 4.4 Scaling Law of Context Window
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 上下文窗口的规模法则
- en: 'It’s observed that the general performance of all models deteriorates with
    the increase in context size. As observed from [Table 4](#S3.T4 "In 3.2.4 Chain
    of Reasoning ‣ 3.2 Evaluation Task ‣ 3 Loong: A Long-Context Benchmark ‣ Leave
    No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"),
    it is apparent that for the same task, models perform well within small length
    sets but exhibit a notable performance decline as the length increases. This indicates
    that the models possess a certain capability to process the task, yet their performance
    is constrained by the context window. Moreover, despite being trained on 128K
    data, the GPT-4o and Qwen2-72B-Instruct begin to show performance degradation
    within the 50-100K interval, revealing that their actual capability boundary is
    significantly lower than the claimed window size. This suggests the presence of
    an ineffective zone within the claimed window. There exists a Scaling Law for
    model window sizes: to truly equip an LLM with the ability to handle 128K long
    texts, it should be trained on data exceeding 128K, meaning the training length
    should be greater than the actual processable length. Among numerous models, only
    the Gemini is less affected by changes in context length, which was training on
    the ultra-long context of 1000K. To ensure your model genuinely possesses the
    desired context window size, train it on longer data!'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '观察到所有模型的总体性能随着上下文大小的增加而恶化。从[表 4](#S3.T4 "在 3.2.4 推理链 ‣ 3.2 评估任务 ‣ 3 Loong:
    长上下文基准 ‣ 不遗漏任何文档：基于扩展多文档 QA 的长上下文 LLM 基准测试")中可以看出，对于相同的任务，模型在较小长度的集合中表现良好，但随着长度的增加表现显著下降。这表明模型具备一定的处理任务的能力，但其性能受到上下文窗口的限制。此外，尽管在
    128K 数据上进行训练，GPT-4o 和 Qwen2-72B-Instruct 在 50-100K 区间内开始出现性能下降，表明它们的实际能力边界远低于声明的窗口大小。这表明在声明的窗口内存在无效区域。模型窗口大小存在规模法则：要真正使
    LLM 能够处理 128K 长文本，应该在超过 128K 的数据上进行训练，即训练长度应大于实际可处理长度。在众多模型中，只有 Gemini 对上下文长度变化的影响较小，因为它在
    1000K 的超长上下文上进行了训练。为了确保你的模型真正具备所需的上下文窗口大小，应该在更长的数据上进行训练！'
- en: 4.5 RAG or Not
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 是否使用 RAG
- en: 'We have also incorporated the Embedding RAG module into the GPT-4o and Qwen2-72B-Instruct
    to explore whether RAG can enhance the model’s performance on Loong. For the Embedding
    choice, we employ two distinct models: the OpenAI Embedding model⁹⁹9[https://huggingface.co/Xenova/text-embedding-ada-002](https://huggingface.co/Xenova/text-embedding-ada-002)
    and the BGE Embedding model^(10)^(10)10[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3).
    Besides, we set the top-k value of 5, 10, 30, and 50 for each model respectively,
    and the chunk size is 1024\. The result is shown in [Figure 3](#S4.F3 "In 4.3
    Task Analysis ‣ 4 Experiments ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA") and the details can be seen in [Appendix D](#A4
    "Appendix D RAG Detailed Results ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将嵌入式 RAG 模块整合到了 GPT-4o 和 Qwen2-72B-Instruct 中，以探索 RAG 是否能提升模型在 Loong 上的表现。对于嵌入式选择，我们使用了两种不同的模型：OpenAI
    嵌入模型⁹⁹9[https://huggingface.co/Xenova/text-embedding-ada-002](https://huggingface.co/Xenova/text-embedding-ada-002)
    和 BGE 嵌入模型^(10)^(10)10[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)。此外，我们分别为每个模型设置了
    top-k 值为 5、10、30 和 50，块大小为 1024。结果见于[图 3](#S4.F3 "在 4.3 任务分析 ‣ 4 实验 ‣ 不遗漏任何文档：基于扩展多文档
    QA 的长上下文 LLM 基准测试")，详细信息可见于[附录 D](#A4 "附录 D RAG 详细结果 ‣ 不遗漏任何文档：基于扩展多文档 QA 的长上下文
    LLM 基准测试")。
- en: Benchmark Analysis It is evident that the inclusion of RAG does not enhance
    the model’s overall performance on the Loong, and there is a noticeable decline
    in assessment. This is because the evidence in the Loong is distributed relatively
    evenly across multiple documents, requiring a comprehensive understanding of long
    texts by the model. RAG, being more limited, only shows some effectiveness in
    the task with sparse evidence, such as spotlight locating. However, RAG’s negative
    impact is significant for tasks requiring a high level of comprehensiveness. Integrating
    RAG does not enhance the performance, indicating that Loong focuses on evaluating
    the model’s complex reasoning and comprehensive analysis capabilities for long
    contexts, thereby effectively assessing the LLM’s long-context modeling ability.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 基准分析 显而易见，RAG 的引入并未提升模型在 Loong 上的整体性能，反而在评估中明显下降。这是因为 Loong 中的证据分布相对均匀在多个文档中，模型需要对长文本有全面的理解。RAG
    的效果相对有限，仅在证据稀疏的任务中，如聚光灯定位，表现出一定的有效性。然而，RAG 对需要高度全面性的任务的负面影响显著。集成 RAG 并未提升性能，表明
    Loong 侧重于评估模型对长上下文的复杂推理和全面分析能力，从而有效评估 LLM 的长上下文建模能力。
- en: Model Analysis Comparing the performance between GPT-4o and Qwen2-72B-Instruct,
    it is evident that the powerful long-context LLM significantly outperforms RAG.
    On the other hand, the performance of RAG is closer to the original performance
    when used with a weak context model. This is because a strong long-context LLM
    can fully exploit the complete information flow of long contexts, capturing complex
    dependencies and semantic information. However, RAG causes context fragmentation
    and information loss, impairing the model’s understanding and reasoning capabilities,
    thereby preventing the full utilization of its inherent modeling advantages. Consequently,
    a strong long-text modeling capability is not suitable for enhancement through
    RAG. Conversely, a weak model with poor long-context modeling capability cannot
    effectively capture information, and RAG cannot compensate for this deficiency.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 模型分析 对比 GPT-4o 和 Qwen2-72B-Instruct 的性能，可以明显看出，强大的长上下文 LLM 明显优于 RAG。另一方面，当与较弱的上下文模型结合使用时，RAG
    的表现更接近原始性能。这是因为强大的长上下文 LLM 可以充分利用长上下文的完整信息流，捕捉复杂的依赖关系和语义信息。然而，RAG 会导致上下文碎片化和信息丢失，损害模型的理解和推理能力，从而阻碍其固有建模优势的充分发挥。因此，强大的长文本建模能力不适合通过
    RAG 来增强。相反，一个长上下文建模能力较差的弱模型无法有效捕捉信息，而 RAG 无法弥补这一不足。
- en: Length Analysis Within the context window size that the model can handle, RAG
    does not offer an advantage. However, for ultra-long context sets, a high top-k
    setting of RAG can produce certain effects. This is because, in short context
    sets, the model’s inherent modeling capability can effectively handle the entire
    text length without losing information. The introduction of RAG, conversely, may
    result in the loss of certain evidence, leading to information gaps. In ultra-long
    context collections, RAG can effectively compress information, recalling evidence
    that the LLM could not access due to length truncation, thereby enhancing the
    model’s performance on Loong.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 长度分析 在模型能够处理的上下文窗口大小范围内，RAG 并未提供优势。然而，对于超长上下文集，RAG 的高 top-k 设置可以产生一定效果。这是因为在短上下文集里，模型的固有建模能力可以有效处理整个文本长度而不会丢失信息。引入
    RAG 可能导致某些证据的丢失，从而产生信息空白。在超长上下文集合中，RAG 可以有效压缩信息，回忆出 LLM 因长度截断而无法访问的证据，从而提升模型在
    Loong 上的性能。
- en: Relying on RAG cannot resolve all the problems associated with long-text modeling.
    To genuinely improve the long-context modeling capability, stronger training methods
    and effective training on longer texts are required, rather than merely integrating
    the RAG module.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖 RAG 并不能解决所有与长文本建模相关的问题。要真正提高长上下文建模能力，需要更强的训练方法和对更长文本的有效训练，而不仅仅是集成 RAG 模块。
- en: 5 Conclusion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this study, we propose Loong, a question-answering format benchmark designed
    to evaluate long-context comprehension in real-world multi-document scenarios.
    We analyze six advanced language models (LLMs), considering variations in their
    parameter sizes and context windows, including GPT-4o and Gemini-Pro1.5\. Notably,
    even the most powerful long-context LLMs fail to achieve satisfactory performance.
    Furthermore, we conduct in-depth analyses to enhance long-context modeling capabilities
    by comparing the RAG approach and the scaling laws related to context size.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了Loong，一个设计用于评估现实世界多文档场景中长上下文理解能力的问题回答格式基准。我们分析了六种先进的语言模型（LLMs），考虑了它们的参数大小和上下文窗口的变化，包括GPT-4o和Gemini-Pro1.5。值得注意的是，即使是最强大的长上下文LLMs也未能达到令人满意的性能。此外，我们通过比较RAG方法和与上下文大小相关的扩展规律，进行深入分析，以提升长上下文建模能力。
- en: Limitations
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Here we list some of the limitations that are not considered when designing
    Loong: (1) Limited Domains. The purpose of Loong is to evaluate the long-context
    understanding capabilities in real-world multi-document scenarios. However, a
    sea of multi-document domains exists in the real world. Considering annotation
    costs and model evaluation efficiency, we only cover the most representative parts
    of them: financial, legal, and academic. (2) High Annotation Cost. To enhance
    the reliability of Loong in assessing the LLM’s long-context understanding capabilities,
    we recruited a group of experts for each of the three domains to proofread the
    data, and they are proficient in both English and Chinese. They need to understand
    the question and search for relevant evidence in multiple documents with an average
    length of up to 110k to judge the consistency between the question and the answer,
    which requires a significant amount of time and effort.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些在设计Loong时未考虑的局限性：（1）有限的领域。Loong的目的是评估现实世界多文档场景中的长上下文理解能力。然而，现实世界中存在大量的多文档领域。考虑到注释成本和模型评估效率，我们仅涵盖了其中最具代表性的部分：金融、法律和学术。（2）高昂的注释成本。为了提高Loong在评估LLM长上下文理解能力方面的可靠性，我们为这三个领域招募了一组专家进行数据校对，他们精通英语和中文。他们需要理解问题，并在平均长度达到110k的多个文档中寻找相关证据，以判断问题与答案之间的一致性，这需要大量的时间和精力。
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An et al. (2023) 陈欣安, 龚善善, 钟铭, 李穆凯, 张俊, 孔灵鹏, 和邱西鹏. 2023. L-eval: 为长上下文语言模型建立标准化评估。*arXiv
    预印本 arXiv:2307.11088*。'
- en: 'Anthropic (2024a) AI Anthropic. 2024a. The claude 3 model family: Opus, sonnet,
    haiku. *Claude-3 Model Card*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024a) AI Anthropic. 2024a. Claude 3模型家族：Opus, sonnet, haiku。*Claude-3模型卡*。
- en: Anthropic (2024b) AI Anthropic. 2024b. Claude 3.5 sonnet model card addendum.
    *Claude-3.5 Model Card*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024b) AI Anthropic. 2024b. Claude 3.5 sonnet模型卡附录。*Claude-3.5模型卡*。
- en: Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023a. Qwen technical report.
    *arXiv preprint arXiv:2309.16609*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023a) 白金泽, 白帅, 朱云飞, 崔泽宇, 黨凯, 邓晓东, 范杨, 葛文彬, 韩宇, 黄飞, 等. 2023a. Qwen技术报告。*arXiv
    预印本 arXiv:2309.16609*。
- en: 'Bai et al. (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai
    Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023b.
    Longbench: A bilingual, multitask benchmark for long context understanding. *arXiv
    preprint arXiv:2308.14508*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023b) 白玉石, 吕鑫, 张家杰, 吕洪昌, 唐建凯, 黄志点, 杜正孝, 刘晓, 曾敖汉, 侯磊, 等. 2023b.
    Longbench: 一个用于长上下文理解的双语多任务基准。*arXiv 预印本 arXiv:2308.14508*。'
- en: bloc97 (2023) bloc97\. 2023. Ntk-aware scaled rope. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bloc97 (2023) bloc97. 2023. Ntk-aware scaled rope. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)。
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *Proceedings of ICML*, pages 2206–2240.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, 等。2022。通过从万亿标记中检索来改进语言模型。在 *ICML 会议论文集* 中，第2206–2240页。
- en: Chen et al. (2024a) Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024a.
    Benchmarking large language models in retrieval-augmented generation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, pages 17754–17762.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024a) Jiawei Chen, Hongyu Lin, Xianpei Han, 和 Le Sun。2024a。在检索增强生成中基准测试大型语言模型。在
    *AAAI 人工智能会议论文集* 中，第17754–17762页。
- en: 'Chen et al. (2024b) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    and Min Yang. 2024b. Long context is not long at all: A prospector of long-dependency
    data for large language models. *arXiv preprint arXiv:2405.17915*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024b) Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo,
    和 Min Yang。2024b。长上下文根本不长: 大型语言模型长依赖数据的探索者。*arXiv 预印本 arXiv:2405.17915*。'
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian。2023。通过位置插值扩展大型语言模型的上下文窗口。*arXiv
    预印本 arXiv:2306.15595*。
- en: 'Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong
    Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling
    capacities of large language models. *arXiv preprint arXiv:2309.13345*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, 和 Ji-Rong
    Wen。2023。Bamboo: 评估大型语言模型长文本建模能力的全面基准。*arXiv 预印本 arXiv:2309.13345*。'
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of ACL*, pages 320–335.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, 和 Jie Tang。2022。GLM: 自回归空白填充的通用语言模型预训练。在 *ACL 会议论文集* 中，第320–335页。'
- en: 'Golchin and Surdeanu (2023) Shahriar Golchin and Mihai Surdeanu. 2023. Time
    travel in llms: Tracing data contamination in large language models. *arXiv preprint
    arXiv:2308.08493*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Golchin and Surdeanu (2023) Shahriar Golchin 和 Mihai Surdeanu。2023。LLMs 中的时间旅行:
    追踪大型语言模型的数据污染。*arXiv 预印本 arXiv:2308.08493*。'
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong
    Wang。2023。LM-Infinite: 大型语言模型的即时长度泛化。*arXiv 预印本 arXiv:2308.16137*。'
- en: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. Ruler: What’s the real context
    size of your long-context language models? *arXiv preprint arXiv:2404.06654*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, 和 Boris Ginsburg。2024。Ruler: 你的长上下文语言模型的真实上下文大小是多少？*arXiv
    预印本 arXiv:2404.06654*。'
- en: Kamradt (2023) Greg Kamradt. 2023. Needle in a haystack - pressure testing llms.
    [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt (2023) Greg Kamradt。2023。针在稻草堆中 - 压力测试 LLMs。[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。
- en: 'Li et al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023.
    Loogle: Can long-context language models understand long contexts? *arXiv preprint
    arXiv:2311.04939*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Jiaqi Li, Mengmeng Wang, Zilong Zheng, 和 Muhan Zhang。2023。Loogle:
    长上下文语言模型能理解长上下文吗？*arXiv 预印本 arXiv:2311.04939*。'
- en: Liu et al. (2024) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024. Calibrating LLM-based
    evaluator. In *Proceedings of LREC-COLING*, pages 2638–2656.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, 和 Qi Zhang。2024。校准基于 LLM 的评估器。在 *LREC-COLING
    会议论文集* 中，第2638–2656页。
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。2023。GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: 'Pal et al. (2023) Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley,
    Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in expanding
    context lengths in llms. *arXiv preprint arXiv:2308.10882*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pal et al. (2023) Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley,
    Arvind Sundararajan, 和 Siddartha Naidu。2023。Giraffe: 扩展上下文长度的冒险。*arXiv 预印本 arXiv:2308.10882*。'
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2023) **彭博文**, **杰弗里·克斯内尔**, **范洪璐**, 和 **恩里科·希波尔**。2023。《Yarn:
    高效扩展大型语言模型的上下文窗口》。*arXiv 预印本 arXiv:2309.00071*。'
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Press et al. (2021) **奥菲尔·普雷斯**, **诺亚·A·史密斯**, 和 **迈克·刘易斯**。2021。《短期训练，长期测试:
    线性偏差的注意力机制实现输入长度外推》。*arXiv 预印本 arXiv:2108.12409*。'
- en: 'Qiu et al. (2024) Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, and
    Irwin King. 2024. Clongeval: A chinese benchmark for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.03514*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu et al. (2024) **邱泽轩**, **李晶晶**, **黄世觉**, **钟万军**, 和 **王尔文**。2024。《Clongeval:
    一个用于评估长上下文大型语言模型的中文基准》。*arXiv 预印本 arXiv:2403.03514*。'
- en: Ratner et al. (2022) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2022. Parallel context windows for large language models. *arXiv preprint arXiv:2212.10947*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratner et al. (2022) **拉特纳**, **莱文**, **贝林科夫**, **拉姆**, **马加尔**, **阿本德**, **卡尔帕斯**,
    **沙休阿**, **雷顿-布朗**, 和 **肖汉**。2022。《大型语言模型的并行上下文窗口》。*arXiv 预印本 arXiv:2212.10947*。
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid et al. (2024) **马歇尔·里德**, **尼古拉·萨维诺夫**, **德尼斯·特普利亚欣**, **德米特里·列皮欣**, **蒂莫西·利利克拉普**,
    **让-巴普蒂斯特·阿拉亚克**, **拉杜·索里库特**, **安杰丽基·拉扎里杜**, **奥尔汉·费拉特**, **朱利安·施瑞特维瑟**, 等人。2024。《Gemini
    1.5: 解锁跨越百万个上下文标记的多模态理解》。*arXiv 预印本 arXiv:2403.05530*。'
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) **巴普蒂斯特·罗兹耶尔**, **乔纳斯·盖林**, **法比安·格洛克尔**, **斯滕·索特拉**,
    **伊泰·加特**, **肖青·艾伦·谭**, **约西·阿迪**, **刘晶宇**, **塔尔·雷梅兹**, **热尔米·拉潘**, 等人。2023。《Code
    llama: 开放代码基础模型》。*arXiv 预印本 arXiv:2308.12950*。'
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. (2023) **石伟佳**, **闵世文**, **安纪宏**, **徐敏俊**, **詹姆斯·里奇**, **迈克·刘易斯**,
    **卢克·泽特尔摩耶**, 和 **易文涛**。2023。《Replug: 检索增强型黑箱语言模型》。*arXiv 预印本 arXiv:2301.12652*。'
- en: 'Song et al. (2024) Mingyang Song, Mao Zheng, and Xuan Luo. 2024. Counting-stars:
    A simple, efficient, and reasonable strategy for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.11802*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2024) **宋明扬**, **郑茂**, 和 **罗轩**。2024。《Counting-stars: 一种简单、高效、合理的评估长上下文大型语言模型的策略》。*arXiv
    预印本 arXiv:2403.11802*。'
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. (2024) **苏剑林**, **艾哈迈德**, **卢雨**, **潘胜峰**, **博文**, 和 **刘云锋**。2024。《Roformer:
    带有旋转位置嵌入的增强型变换器》。*Neurocomputing*，568:127063。'
- en: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) **孙玉涛**, **董力**, **巴伦·帕特拉**, **马树明**, **黄绍汉**, **阿龙·本海姆**,
    **维斯拉夫·乔杜里**, **宋霞**, 和 **魏富如**。2022。《可长度外推的变换器》。*arXiv 预印本 arXiv:2212.10554*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Proceedings of NeurIPs*, page 30.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) **阿希什·瓦斯瓦尼**, **诺姆·沙泽尔**, **尼基·帕尔马尔**, **雅各布·乌斯科雷特**,
    **利昂·琼斯**, **艾丹·N·戈麦斯**, **卢卡斯·凯瑟尔**, 和 **伊利亚·波洛苏金**。2017。《注意力机制就是你所需要的一切》。发表于
    *NeurIPs 会议论文集*，第 30 页。
- en: Wang et al. (2024) Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen
    Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, and Yue Zhang. 2024. Evaluating
    open-qa evaluation. In *Proceedings of NeurIPs*, page 36.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024) **汪存祥**, **程思锐**, **郭启鹏**, **岳元浩**, **丁博文**, **徐智坤**, **王一东**,
    **胡向坤**, **张正**, 和 **张跃**。2024。《开放问答评估》。发表于 *NeurIPs 会议论文集*，第 36 页。
- en: Wu et al. (2024) Kevin Wu, Eric Wu, and James Zou. 2024. How faithful are rag
    models? quantifying the tug-of-war between rag and llms’ internal prior. *arXiv
    preprint arXiv:2404.10198*.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2024) **吴凯文**, **吴艾瑞克**, 和 **詹姆斯·邹**。2024。《RAG 模型有多可靠？量化 RAG 与 LLM
    内部先验的拉锯战》。*arXiv 预印本 arXiv:2404.10198*。
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient streaming language models with attention sinks. *arXiv
    preprint arXiv:2309.17453*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2023）光轩·肖，远东·田，贝迪·陈，宋·韩，迈克·刘易斯。2023。具有注意力汇聚的高效流式语言模型。*arXiv 预印本 arXiv:2309.17453*。
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, et al. 2023. Effective long-context scaling of foundation models. *arXiv
    preprint arXiv:2309.16039*.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人（2023）文汉·熊，晶宇·刘，伊戈尔·莫利博格，贺佳·张，普拉吉瓦尔·巴尔加瓦，瑞·侯，路易斯·马丁，拉希·隆塔，卡尔提克·阿比纳夫·桑卡拉拉曼，巴尔拉斯·奥古兹
    等人。2023。有效的基础模型长上下文扩展。*arXiv 预印本 arXiv:2309.16039*。
- en: Xu et al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. 2023. Retrieval meets long context large language models. *arXiv preprint
    arXiv:2310.03025*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2023）鹏·徐，魏·平，贤超·吴，劳伦斯·麦克阿菲，陈·朱，子涵·刘，桑迪普·苏布拉马尼安，埃维莉娜·巴赫图里纳，穆罕默德·肖耶比，布赖恩·卡坦扎罗。2023。检索遇到长上下文大语言模型。*arXiv
    预印本 arXiv:2310.03025*。
- en: 'Zhang et al. (2023) Lei Zhang, Yunshui Li, Ziqiang Liu, Junhao Liu, Min Yang,
    et al. 2023. Marathon: A race through the realm of long context with large language
    models. *arXiv preprint arXiv:2312.09542*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023）雷·张，云水·李，子强·刘，俊豪·刘，敏·杨 等人。2023。马拉松：大型语言模型中的长上下文领域竞赛。*arXiv 预印本
    arXiv:2312.09542*。
- en: 'Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. 2024.
    $\infty$bench: Extending long context evaluation beyond 100k tokens. *arXiv preprint
    arXiv:2402.13718*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2024）欣荣·张，英发·陈，圣丁·胡，紫杭·徐，俊豪·陈，穆·凯·郝，徐·韩，镇·冷·泰，硕·王，智远·刘 等人。2024。$\infty$bench：将长上下文评估扩展至
    100k 标记以上。*arXiv 预印本 arXiv:2402.13718*。
- en: 'Zhu et al. (2023) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via
    positional skip-wise training. *arXiv preprint arXiv:2309.10400*.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）大伟·朱，南·杨，梁·王，逸凡·宋，文浩·吴，福如·魏，苏建·李。2023。Pose：通过位置跳跃训练扩展 llms 的高效上下文窗口。*arXiv
    预印本 arXiv:2309.10400*。
- en: Appendix A GPT4-as-the-Judge Prompt
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A GPT4 作为评判者的提示
- en: In Loong, GPT4 is used as a Judger to evaluate the correctness of the model-generated
    content, with the prompt used shown in the following. With this evaluation method,
    we expect the Judger model to output a percentage score along with its corresponding
    explanation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Loong 中，GPT4 被用作判断器来评估模型生成内容的正确性，以下是所用的提示。通过这种评估方法，我们期望判断器模型能够输出一个百分比评分及其对应的解释。
- en: '[Gold
    Answer]  [The Start of Assistant’s Predicted Answer] 
    [The End of Assistant’s Predicted Answer] [System] We would like to request your
    feedback on the performance of the AI assistant in response to the user question
    displayed above according to the gold answer. Please use the following listed
    aspects and their descriptions as evaluation criteria: - Accuracy and Hallucinations:
    The assistant’s answer is semantically consistent with the gold answer; The numerical
    value and order need to be accurate, and there should be no hallucinations. -
    Completeness: Referring to the reference answers, the assistant’s answer should
    contain all the key points needed to answer the user’s question; further elaboration
    on these key points can be omitted. Please rate whether this answer is suitable
    for the question. Please note that the gold answer can be considered as a correct
    answer to the question. The assistant receives an overall score on a scale of
    1 to 100, where a higher score indicates better overall performance.Please note
    that if the assistant’s answer and the gold answer fully meet the above criteria,
    its overall rating should be the full marks (100). Please first provide a comprehensive
    explanation of your evaluation, avoiding any potential bias.Then, output a line
    indicating the score of the Assistant. PLEASE OUTPUT WITH THE FOLLOWING FORMAT,
    WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: "[[score]]",
    FOR EXAMPLE "Rating: [[100]]":  Evaluation evidence: your evluation
    explanation here, no more than 100 words Rating: [[score]]  Now, start
    your evaluation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gold
    Answer]  [The Start of Assistant’s Predicted Answer] 
    [The End of Assistant’s Predicted Answer] [System] We would like to request your
    feedback on the performance of the AI assistant in response to the user question
    displayed above according to the gold answer. Please use the following listed
    aspects and their descriptions as evaluation criteria: - Accuracy and Hallucinations:
    The assistant’s answer is semantically consistent with the gold answer; The numerical
    value and order need to be accurate, and there should be no hallucinations. -
    Completeness: Referring to the reference answers, the assistant’s answer should
    contain all the key points needed to answer the user’s question; further elaboration
    on these key points can be omitted. Please rate whether this answer is suitable
    for the question. Please note that the gold answer can be considered as a correct
    answer to the question. The assistant receives an overall score on a scale of
    1 to 100, where a higher score indicates better overall performance.Please note
    that if the assistant’s answer and the gold answer fully meet the above criteria,
    its overall rating should be the full marks (100). Please first provide a comprehensive
    explanation of your evaluation, avoiding any potential bias.Then, output a line
    indicating the score of the Assistant. PLEASE OUTPUT WITH THE FOLLOWING FORMAT,
    WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: "[[score]]",
    FOR EXAMPLE "Rating: [[100]]":  Evaluation evidence: your evluation
    explanation here, no more than 100 words Rating: [[score]]  Now, start
    your evaluation:'
- en: Appendix B Test Case
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 测试案例
- en: To facilitate understanding of Loong’s data examples, we present examples of
    11 sub-tasks in the following, showing the format we input to the model as well
    as the prompts we used.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于理解 Loong 的数据示例，我们展示了以下 11 个子任务的示例，展示了我们输入模型的格式以及使用的提示。
- en: B.1 Spotlight Locating
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 聚光灯定位
- en: 
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Please answer the following questions based only on the judgment documents
    you have seen above. You only need to give the titles of the judgment documents
    that meet the requirements.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：请仅根据您上面看到的判断文档回答以下问题。您只需给出符合要求的判断文档标题。
- en: 'Question: Among the above judgment documents, which one is the case of ’crime
    of endangering public security’?'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在上述判断文档中，哪个是“危害公共安全罪”案件？
- en: 'Answer: Judgment Document 5'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：判断文档 5
- en: B.2 Sequential Enumeration
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 顺序枚举
- en: 
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们请您审阅上述公司提供的财务报表，并仅根据您所看到的信息回答以下问题。如果问题涉及财务报表中未找到的内容，您可以忽略这一部分，仅回答其他部分。
- en: 'Question: Please list the Cash and Cash Equivalents of each of the aforementioned
    companies in ascending order?'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：请按升序列出上述每家公司现金及现金等价物的金额？
- en: 'Answer: $ 1,273 thousand, $ 1,360 thousand, $ 9,364 thousand'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：$1,273 千，$1,360 千，$9,364 千
- en: B.3 Extremum Acquisition
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 极值获取
- en: 
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们恳请你查看上述提供的公司财务报表，并仅根据你所看到的信息回答以下问题。如果问题涉及财务报表中未包含的内容，你可以忽略这部分，只回答其他部分。
- en: 'Question: Which company has the highest ’Total Non-current Assets’?'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：哪家公司拥有最高的“总非流动资产”？
- en: 'Answer: BLUE DOLPHIN ENERGY CO with $56,787,000'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：蓝色海豚能源公司，总额为56,787,000美元
- en: B.4 Range Awareness
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 范围意识
- en: 
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们恳请你查看上述提供的公司财务报表，并仅根据你所看到的信息回答以下问题。如果问题涉及财务报表中未包含的内容，你可以忽略这部分，只回答其他部分。
- en: 'Question: Which company has the highest ’Total Current Liabilities’?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：哪家公司拥有最高的“总流动负债”？
- en: 'Answer: 4 companies'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：4家公司
- en: B.5 Report Integration
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5 报告整合
- en: 
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们恳请你查看上述提供的公司财务报表，并仅根据你所看到的信息回答以下问题。如果问题涉及财务报表中未包含的内容，你可以忽略这部分，只回答其他部分。
- en: 'Question: Please categorize the companies listed above by ’Total Shares Outstanding’
    into the following groups: below 10,000,000 shares and 10,000,000 shares or more.
    Place companies into the same collection for the same category and into different
    collections for different categories.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：请将上述列出的公司按“总流通股数”分为以下几组：低于10,000,000股和10,000,000股或更多。将同一类别的公司放入同一集合，将不同类别的公司放入不同集合。
- en: 'Answer: {"below 10,000,000 shares": ["GSE SYSTEMS INC", "CROSS TIMBERS ROYALTY
    TRUST"], "10,000,000 shares or more": ["HUGOTON ROYALTY TRUST"]}'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：{"低于10,000,000股": ["GSE SYSTEMS INC", "CROSS TIMBERS ROYALTY TRUST"], "10,000,000股或更多":
    ["HUGOTON ROYALTY TRUST"]}'
- en: B.6 Citation&Reference
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.6 引用与参考
- en: 
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We hope you will carefully study the provided papers and determine
    the citation relationships between them. Please follow the instructions below
    strictly to complete the task:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们希望你仔细研究提供的论文，并确定它们之间的引用关系。请严格按照以下说明完成任务：
- en: '#Specific Requirements:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#具体要求：'
- en: '1\. Reference: When a given paper mentions other provided papers, those other
    papers are considered as "references" for the given paper. To summarize in this
    specific context, references are about what the given paper is using.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 参考文献：当给定的论文提到其他提供的论文时，这些其他论文被视为给定论文的“参考文献”。在这个具体的背景下，参考文献是关于给定论文所使用的内容。
- en: '2\. Citation: Conversely, when other provided papers mention the given paper
    in their works, the given paper is being "cited" by those other papers. To summarize
    in this specific context, citations are about who is using the given paper.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 引用：相反，当其他提供的论文在其工作中提到给定的论文时，给定的论文被这些其他论文“引用”。在这个具体的背景下，引用是关于谁在使用给定的论文。
- en: 3\. Given a paper, you need to determine the citation or reference relationship
    between this paper and the other papers. Do not consider papers that are not provided.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 给定一篇论文，你需要确定该论文与其他论文之间的引用或参考关系。请不要考虑未提供的论文。
- en: '3\. Please present the paper titles in a json format as follows: {"Reference":["Reference
    Title 1", "Reference Title 2", …, "Reference Title n"], "Citation":["Citation
    Title 1", "Citation Title 2", …, "Citation Title n"]}.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 请以json格式呈现论文标题，如下所示：{"Reference":["参考标题1", "参考标题2", …, "参考标题n"], "Citation":["引用标题1",
    "引用标题2", …, "引用标题n"]}.
- en: 4\. If a paper does not have any references or citations, please leave the corresponding
    list empty, e.g.{"Refernce":[]}, {"Citation":[]}.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 如果一篇论文没有任何参考文献或引用，请将相应列表留空，例如{"Refernce":[]}, {"Citation":[]}.
- en: 'Question: The paper you need to analyze:Self-Discover: Large Language Models
    Self-Compose Reasoning Structures'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：你需要分析的论文是：《自我发现：大语言模型自我构建推理结构》
- en: 'Answer: {’Reference’: [’# Plan, Verify and Switch: Integrated Reasoning with
    Diverse X-of-Thoughts ’, ’# StrategyLLM: Large Language Models as Strategy Generators,
    Executors, Optimizers, and Evaluators for Problem Solving ’], ’Citation’: [’#
    Can LLMs Solve Longer Math Word Problems Better? ’]}'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：{’参考文献’: [’# 计划、验证和切换：多样化思维整合推理’, ’# StrategyLLM：大型语言模型作为策略生成者、执行者、优化者和评估者用于问题解决’],
    ’引用’: [’# LLM 能否更好地解决更长的数学文字问题？’]}'
- en: B.7 Case Classification
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.7 案例分类
- en: 
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Please answer the following questions based only on the judgment documents
    you have seen above. You only need to give the titles of the judgment documents
    that meet the requirements.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：请仅根据您上面所见的判决文档回答以下问题。您只需要提供符合要求的判决文档标题。
- en: 'Question: After reading the above judgments, please classify all the judgments
    according to the following three types of cases: ’Civil Cases’, ’Enforcement Cases’,
    and ’Administrative Cases’.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：阅读以上判决后，请根据以下三种案件类型对所有判决进行分类：’民事案件’，’执行案件’，和’行政案件’。
- en: 'Answer: {"Civil Cases": ["Judgment Document 2"], "Enforcement Cases": ["Judgment
    Document 4"], "Administrative Cases": ["Judgment Document 1", "Judgment Document
    3"]}'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：{"民事案件": ["判决文档 2"], "执行案件": ["判决文档 4"], "行政案件": ["判决文档 1", "判决文档 3"]}'
- en: B.8 Temporal Analysis
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.8 时间分析
- en: 
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to review the financial statements of the companies
    provided above and answer the following questions based solely on the information
    you have seen. If the question involves content not found in the financial statements,
    you may ignore this part and only answer the other parts.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们恳请您审查上述公司提供的财务报表，并仅根据您所见的信息回答以下问题。如果问题涉及财务报表中未出现的内容，您可以忽略这一部分，只回答其他部分。
- en: 'Question: What is the trend in ARVANA INC’s share capital from 2021 to 2024?'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：ARVANA INC 从 2021 年到 2024 年的股本趋势是什么？
- en: 'Answer: ARVANA INC’s share capital has consistently increased from $4,611 in
    2021 to $34,149 in 2022, $35,949 in 2023, and $107,847 in 2024.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：ARVANA INC 的股本从 2021 年的 $4,611 稳步增长到 2022 年的 $34,149、2023 年的 $35,949，以及 2024
    年的 $107,847。
- en: B.9 Citation Chain
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.9 引用链
- en: 
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: We kindly ask you to thoroughly review the provided papers and construct
    a citation chain from them. Please adhere to the following instructions strictly
    while completing the task:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们恳请您彻底审查提供的论文，并从中构建一个引用链。请在完成任务时严格遵循以下说明：
- en: '#Task Instructions:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '#任务说明：'
- en: Given several papers, you are required to identify and list the longest citation
    chain, which demonstrates the citation relationship among the provided papers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 给定几篇论文，您需要识别并列出最长的引用链，展示所提供论文之间的引用关系。
- en: '#Specific Requirements:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#具体要求：'
- en: '1.Please present the titles of the papers in the form of a list, as follows:
    ["Title of Paper 1", "Title of Paper 2", …, "Title of Paper n"].'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 请以列表形式呈现论文标题，如下所示：["论文 1 的标题", "论文 2 的标题", …, "论文 n 的标题"]。
- en: 2.Ensure that the citation chain in the list is linear and continuous, meaning
    that the first paper title in the list (Paper 1) should not cite any other works.
    Instead, it should be cited by the next paper in the list (Paper 2); subsequently,
    each paper should then be cited by the next one in the list, continuing up to
    the last paper (Paper n).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 确保列表中的引用链是线性的和连续的，即列表中的第一篇论文标题（论文 1）不应引用任何其他作品。相反，它应该被列表中的下一篇论文（论文 2）引用；随后，每篇论文应被列表中的下一篇论文引用，一直持续到最后一篇论文（论文
    n）。
- en: 3.Consider only the citation relationships within the supplied collection of
    papers, and ensure that the citation chain accurately reflects the sequential
    citation order among these documents.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 仅考虑提供的论文集合中的引用关系，并确保引用链准确反映这些文档之间的顺序引用。
- en: 4.Do not take into account any articles not provided, and disregard other non-linear
    citation relationships.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 不考虑未提供的任何文章，并忽略其他非线性引用关系。
- en: 'Answer: ["# Very Deep Transformers for Neural Machine Translation ", "# Understanding
    the Difficulty of Training Transformers ", "# MonaCoBERT: Monotonic attention
    based ConvBERT for Knowledge Tracing"]'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：["# 非常深度的变换器用于神经机器翻译", "# 理解训练变换器的困难", "# MonaCoBERT：基于单调注意力的ConvBERT用于知识追踪"]
- en: B.10 Link the Links
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.10 链接链接
- en: 
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Answer the following questions based solely on the judgment document
    you have seen above.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '提示: 根据你上面看到的判决文书，仅根据这些文书回答以下问题。'
- en: 'Question:After reading the above judgment document, I will give you several
    judgment results:  You need to
    determine the most likely judgment result for each of the above judgment documents.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '问题: 在阅读上述判决文书后，我将提供几个判决结果:  你需要确定每个上述判决文书最可能的判决结果。'
- en: 'Answer: {"Judgment Document 1": "Judgment Result 1", "Judgment Document 2":
    "Judgment Result 6", "Judgment Document 3": "Judgment Result 2", "Judgment Document
    4": "Judgment Result 5"}'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '答案: {"判决文书 1": "判决结果 1", "判决文书 2": "判决结果 6", "判决文书 3": "判决结果 2", "判决文书 4":
    "判决结果 5"}'
- en: B.11 Solitaire
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.11 扑克游戏
- en: 
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 
- en: 'Prompt: Answer the following questions based solely on the judgment document
    you have seen above.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '提示: 根据你上面看到的判决文书，仅根据这些文书回答以下问题。'
- en: 'Question: Reading the above judgments, I will provide several case types arranged
    in a left-to-right sequence: [’CaseType1’, ’CaseType2’, ’CaseType3’, ’CaseType4’].
    You need to sort all the judgment documents according to the above sequence of
    case types. The judgment documents only need to include the titles.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '问题: 阅读上述判决后，我将提供几个按从左到右顺序排列的案件类型: [’CaseType1’, ’CaseType2’, ’CaseType3’, ’CaseType4’]。你需要根据上述案件类型的顺序对所有判决文书进行排序。判决文书只需包括标题。'
- en: 'Please provide the answer:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '请提供答案:'
- en: 'Answer: {"CaseType1": "Judgment Document 3", "CaseType2": "Judgment Document
    1", "CaseType3": "Judgment Document 4", "CaseType4": "Judgment Document 6"}'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '答案: {"CaseType1": "判决文书 3", "CaseType2": "判决文书 1", "CaseType3": "判决文书 4", "CaseType4":
    "判决文书 6"}'
- en: Appendix C Length Distribution
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 长度分布
- en: 'As shown in [Figure 4](#A3.F4 "In Appendix C Length Distribution ‣ Leave No
    Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA") and
    [Table 5](#A5.T5 "In Appendix E Comparison of Evidence Distribution ‣ Leave No
    Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"),
    we present the distribution of data lengths in Loong. It can be observed that
    the data is primarily distributed around 30-150k. Moreover, we have sufficient
    data in both shorter and longer ranges, allowing us to assess the model’s capabilities
    across each length interval.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图 4](#A3.F4 "在附录 C 长度分布 ‣ 不遗漏任何文档: 基准测试长期上下文 LLM 的扩展多文档问答")和[表 5](#A5.T5
    "在附录 E 证据分布比较 ‣ 不遗漏任何文档: 基准测试长期上下文 LLM 的扩展多文档问答")所示，我们展示了 Loong 中数据长度的分布情况。可以观察到数据主要集中在
    30-150k 之间。此外，我们在较短和较长范围内都有足够的数据，允许我们评估模型在每个长度区间的能力。'
- en: '![Refer to caption](img/692808769763ab264d1aa427b7533b4f.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/692808769763ab264d1aa427b7533b4f.png)'
- en: 'Figure 4: Test Case Length Distribution in Loong.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: Loong 测试用例长度分布'
- en: Appendix D RAG Detailed Results
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D RAG 详细结果
- en: 'We conducted experiments on GPT-4o and Qwen2-72B-Instruct with the addition
    of a RAG module. As shown in [Table 6](#A5.T6 "In Appendix E Comparison of Evidence
    Distribution ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended
    Multi-Doc QA"), [Table 7](#A5.T7 "In Appendix E Comparison of Evidence Distribution
    ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA"), and [Table 8](#A5.T8 "In Appendix E Comparison of Evidence Distribution
    ‣ Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc
    QA"), we have published detailed experimental results. It can be seen that RAG
    achieved subpar results on our Loong, indicating that Loong requires the model
    to have genuine long-context understanding capabilities.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对 GPT-4o 和 Qwen2-72B-Instruct 进行了实验，并增加了 RAG 模块。如[表 6](#A5.T6 "在附录 E 证据分布比较
    ‣ 不遗漏任何文档: 基准测试长期上下文 LLM 的扩展多文档问答")、[表 7](#A5.T7 "在附录 E 证据分布比较 ‣ 不遗漏任何文档: 基准测试长期上下文
    LLM 的扩展多文档问答")和[表 8](#A5.T8 "在附录 E 证据分布比较 ‣ 不遗漏任何文档: 基准测试长期上下文 LLM 的扩展多文档问答")中所示，我们公布了详细的实验结果。可以看出，RAG
    在我们的 Loong 上表现不佳，这表明 Loong 需要模型具备真正的长期上下文理解能力。'
- en: Appendix E Comparison of Evidence Distribution
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 证据分布比较
- en: 'In the same multi-document question-answering task, we compared the distribution
    of evidence related to the answers in the context for Loong (Ours) and Longbench Bai
    et al. ([2023b](#bib.bib5)). As shown in [Table 9](#A5.T9 "In Appendix E Comparison
    of Evidence Distribution ‣ Leave No Document Behind: Benchmarking Long-Context
    LLMs with Extended Multi-Doc QA"), we present data examples from Loong and LongBench.
    It can be observed that although Longbench contains a large number of passages,
    the evidence is only distributed within Passage 1\. In contrast, in our Loong,
    the evidence is distributed across every document, requiring the model to understand
    each document in order to provide the correct answer.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的多文档问答任务中，我们比较了 Loong（我们的系统）和 Longbench Bai 等人（[2023b](#bib.bib5)）在上下文中与答案相关的证据分布。如
    [表 9](#A5.T9 "附录 E 证据分布比较 ‣ 不遗漏任何文档：基准测试长上下文 LLM 的扩展多文档 QA") 所示，我们展示了来自 Loong
    和 LongBench 的数据示例。可以观察到，尽管 Longbench 包含大量文段，但证据仅分布在 Passage 1 中。相比之下，在我们的 Loong
    中，证据分布在每个文档中，需要模型理解每个文档才能提供正确的答案。
- en: '| Dataset | #data in 10-50k | #data in 50-100K | #data in 100K-200K | #data
    in 200-250K |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 10-50K 数据量 | 50-100K 数据量 | 100K-200K 数据量 | 200-250K 数据量 |'
- en: '| *Spotlight Locating* | *53* | *70* | *80* | *47* |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| *聚焦定位* | *53* | *70* | *80* | *47* |'
- en: '| *Comparison* | *60* | *105* | *95* | *40* |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| *比较* | *60* | *105* | *95* | *40* |'
- en: '| Sequential Enumeration | 24 | 29 | 20 | 14 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 顺序枚举 | 24 | 29 | 20 | 14 |'
- en: '| Extremum Acquisition | 16 | 55 | 59 | 13 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 极值获取 | 16 | 55 | 59 | 13 |'
- en: '| Range Awareness | 20 | 21 | 16 | 13 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 范围意识 | 20 | 21 | 16 | 13 |'
- en: '| *Clustering* | *113* | *246* | *194* | *88* |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| *聚类* | *113* | *246* | *194* | *88* |'
- en: '| Report Integration | 40 | 90 | 90 | 30 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 报告整合 | 40 | 90 | 90 | 30 |'
- en: '| Citation&Reference | 37 | 120 | 79 | 34 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 引用&参考 | 37 | 120 | 79 | 34 |'
- en: '| Case Classification | 36 | 36 | 25 | 24 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 案例分类 | 36 | 36 | 25 | 24 |'
- en: '| *Chain of Reasoning* | *97* | *143* | *112* | *57* |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| *推理链* | *97* | *143* | *112* | *57* |'
- en: '| Temporal Analysis | 10 | 40 | 35 | 15 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 时序分析 | 10 | 40 | 35 | 15 |'
- en: '| Citation Chain | 33 | 50 | 41 | 6 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 引用链 | 33 | 50 | 41 | 6 |'
- en: '| Link the Links | 35 | 25 | 28 | 25 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 链接链接 | 35 | 25 | 28 | 25 |'
- en: '| Solitaire | 28 | 19 | 8 | 11 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 接龙 | 28 | 19 | 8 | 11 |'
- en: '| *Overall* | *323* | *564* | *281* | *232* |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| *总体* | *323* | *564* | *281* | *232* |'
- en: '| Chinese | 240 | 284 | 251 | 130 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 中文 | 240 | 284 | 251 | 130 |'
- en: '| English | 83 | 280 | 230 | 102 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 英文 | 83 | 280 | 230 | 102 |'
- en: 'Table 5: Data length distributions in Loong benchmark.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：Loong 基准中的数据长度分布。
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 聚焦定位 | 比较 | 聚类 | 推理链 | 总体 |'
- en: '| GPT4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| GPT4o (128K) | 73.95 | 0.62 | 50.50 | 0.28 | 44.29 | 0.09 | 57.95 | 0.28
    | 53.47 | 0.26 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.97 | 0.36 | 31.28 | 0.14 | 27.71 | 0.03
    | 26.65 | 0.04 | 32.85 | 0.11 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 56.97 | 0.36 | 31.28 | 0.14 | 27.71 | 0.03 | 26.65
    | 0.04 | 32.85 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 63.32 | 0.44 | 34.63 | 0.17 | 26.74 | 0.03 |
    26.21 | 0.04 | 34.01 | 0.13 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 63.32 | 0.44 | 34.63 | 0.17 | 26.74 | 0.03 | 26.21 |
    0.04 | 34.01 | 0.13 |'
- en: '| w/ Openai Embedding, Top k=10 | 65.20 | 0.46 | 36.80 | 0.19 | 33.06 | 0.04
    | 33.26 | 0.08 | 38.80 | 0.14 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 65.20 | 0.46 | 36.80 | 0.19 | 33.06 | 0.04 | 33.26
    | 0.08 | 38.80 | 0.14 |'
- en: '| w/ BGE Embedding, Top k=10 | 68.27 | 0.50 | 39.51 | 0.22 | 31.91 | 0.04 |
    30.71 | 0.07 | 38.71 | 0.15 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 68.27 | 0.50 | 39.51 | 0.22 | 31.91 | 0.04 | 30.71 |
    0.07 | 38.71 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=30 | 64.32 | 0.43 | 42.15 | 0.26 | 41.02 | 0.08
    | 40.14 | 0.14 | 44.62 | 0.18 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 64.32 | 0.43 | 42.15 | 0.26 | 41.02 | 0.08 | 40.14
    | 0.14 | 44.62 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 64.76 | 0.45 | 47.56 | 0.32 | 40.43 | 0.08 |
    40.82 | 0.17 | 45.67 | 0.21 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 64.76 | 0.45 | 47.56 | 0.32 | 40.43 | 0.08 | 40.82 |
    0.17 | 45.67 | 0.21 |'
- en: '| w/ Openai Embedding, Top k=50 | 65.59 | 0.45 | 41.69 | 0.28 | 34.49 | 0.04
    | 39.74 | 0.14 | 42.70 | 0.18 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 65.59 | 0.45 | 41.69 | 0.28 | 34.49 | 0.04 | 39.74
    | 0.14 | 42.70 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=50 | 63.28 | 0.42 | 47.05 | 0.32 | 42.64 | 0.10 |
    41.97 | 0.18 | 46.52 | 0.21 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 63.28 | 0.42 | 47.05 | 0.32 | 42.64 | 0.10 | 41.97 |
    0.18 | 46.52 | 0.21 |'
- en: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 54.17 | 0.36 | 42.38 | 0.20 | 36.71 | 0.04 |
    47.76 | 0.18 | 43.29 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=5 | 57.57 | 0.40 | 30.98 | 0.14 | 27.91 | 0.02
    | 28.30 | 0.04 | 33.22 | 0.10 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 57.57 | 0.40 | 30.98 | 0.14 | 27.91 | 0.02 | 28.30
    | 0.04 | 33.22 | 0.10 |'
- en: '| w/ BGE Embedding, Top k=5 | 62.02 | 0.44 | 32.90 | 0.16 | 27.05 | 0.02 |
    29.26 | 0.06 | 34.18 | 0.12 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 62.02 | 0.44 | 32.90 | 0.16 | 27.05 | 0.02 | 29.26 |
    0.06 | 34.18 | 0.12 |'
- en: '| w/ Openai Embedding, Top k=10 | 62.52 | 0.44 | 35.79 | 0.18 | 30.16 | 0.03
    | 32.67 | 0.08 | 36.92 | 0.13 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 62.52 | 0.44 | 35.79 | 0.18 | 30.16 | 0.03 | 32.67
    | 0.08 | 36.92 | 0.13 |'
- en: '| w/ BGE Embedding, Top k=10 | 69.24 | 0.51 | 36.78 | 0.18 | 29.07 | 0.02 |
    31.90 | 0.07 | 37.50 | 0.14 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 69.24 | 0.51 | 36.78 | 0.18 | 29.07 | 0.02 | 31.90 |
    0.07 | 37.50 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 64.11 | 0.43 | 41.91 | 0.26 | 35.61 | 0.04
    | 42.61 | 0.19 | 42.98 | 0.18 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 64.11 | 0.43 | 41.91 | 0.26 | 35.61 | 0.04 | 42.61
    | 0.19 | 42.98 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 58.59 | 0.38 | 42.66 | 0.23 | 33.77 | 0.05 |
    40.39 | 0.17 | 40.94 | 0.16 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 58.59 | 0.38 | 42.66 | 0.23 | 33.77 | 0.05 | 40.39 |
    0.17 | 40.94 | 0.16 |'
- en: '| w/ Openai Embedding, Top k=50 | 57.87 | 0.37 | 42.10 | 0.25 | 32.78 | 0.03
    | 42.39 | 0.19 | 40.86 | 0.17 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 57.87 | 0.37 | 42.10 | 0.25 | 32.78 | 0.03 | 42.39
    | 0.19 | 40.86 | 0.17 |'
- en: '| w/ BGE Embedding, Top k=50 | 56.93 | 0.37 | 39.51 | 0.20 | 32.67 | 0.04 |
    43.44 | 0.21 | 40.46 | 0.16 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 56.93 | 0.37 | 39.51 | 0.20 | 32.67 | 0.04 | 43.44 |
    0.21 | 40.46 | 0.16 |'
- en: 'Table 6: Overall results (%) of adding RAG module on GPT4o and Qwen2-72B-Instruct.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：添加 RAG 模块在 GPT4o 和 Qwen2-72B-Instruct 上的总体结果（%）。
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 聚焦定位 | 比较 | 聚类 | 推理链 | 总体 |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 85.67 | 0.81 | 64.27 | 0.33 | 57.01 | 0.24 | 81.58 | 0.55
    | 70.40 | 0.44 |'
- en: '| w/ Openai Embedding, Top k=5 | 47.60 | 0.31 | 29.75 | 0.10 | 29.10 | 0.06
    | 31.46 | 0.08 | 32.98 | 0.11 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 47.60 | 0.31 | 29.75 | 0.10 | 29.10 | 0.06 | 31.46
    | 0.08 | 32.98 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 57.17 | 0.43 | 34.15 | 0.12 | 30.71 | 0.07 |
    28.77 | 0.08 | 35.23 | 0.14 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 57.17 | 0.43 | 34.15 | 0.12 | 30.71 | 0.07 | 28.77 |
    0.08 | 35.23 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 61.25 | 0.44 | 38.33 | 0.17 | 37.00 | 0.08
    | 41.67 | 0.16 | 42.63 | 0.18 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 61.25 | 0.44 | 38.33 | 0.17 | 37.00 | 0.08 | 41.67
    | 0.16 | 42.63 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=10 | 61.00 | 0.44 | 39.74 | 0.19 | 36.14 | 0.08 |
    34.90 | 0.11 | 40.44 | 0.17 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 61.00 | 0.44 | 39.74 | 0.19 | 36.14 | 0.08 | 34.90 |
    0.11 | 40.44 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 55.15 | 0.37 | 46.60 | 0.28 | 45.54 | 0.13
    | 51.98 | 0.27 | 49.23 | 0.24 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 55.15 | 0.37 | 46.60 | 0.28 | 45.54 | 0.13 | 51.98
    | 0.27 | 49.23 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=30 | 57.40 | 0.38 | 52.25 | 0.32 | 46.54 | 0.18 |
    50.02 | 0.25 | 50.41 | 0.26 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 57.40 | 0.38 | 52.25 | 0.32 | 46.54 | 0.18 | 50.02 |
    0.25 | 50.41 | 0.26 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.47 | 0.40 | 49.62 | 0.33 | 39.61 | 0.10
    | 46.08 | 0.20 | 46.82 | 0.24 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 55.47 | 0.40 | 49.62 | 0.33 | 39.61 | 0.10 | 46.08
    | 0.20 | 46.82 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=50 | 52.08 | 0.38 | 53.42 | 0.37 | 49.83 | 0.21 |
    48.88 | 0.24 | 50.55 | 0.27 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 52.08 | 0.38 | 53.42 | 0.37 | 49.83 | 0.21 | 48.88 |
    0.24 | 50.55 | 0.27 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 86.76 | 0.72 | 59.81 | 0.40 | 47.83 | 0.11 | 62.09 | 0.34
    | 58.38 | 0.29 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.01 | 0.35 | 39.56 | 0.22 | 31.84 | 0.04
    | 27.01 | 0.03 | 35.31 | 0.11 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 56.01 | 0.35 | 39.56 | 0.22 | 31.84 | 0.04 | 27.01
    | 0.03 | 35.31 | 0.11 |'
- en: '| w/ BGE Embedding, Top k=5 | 67.33 | 0.43 | 43.90 | 0.28 | 29.37 | 0.04 |
    27.84 | 0.04 | 36.72 | 0.14 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 67.33 | 0.43 | 43.90 | 0.28 | 29.37 | 0.04 | 27.84 |
    0.04 | 36.72 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 64.77 | 0.45 | 45.44 | 0.31 | 36.07 | 0.05
    | 32.29 | 0.05 | 40.54 | 0.15 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 64.77 | 0.45 | 45.44 | 0.31 | 36.07 | 0.05 | 32.29
    | 0.05 | 40.54 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 72.07 | 0.52 | 50.15 | 0.32 | 34.35 | 0.05 |
    33.49 | 0.07 | 41.90 | 0.17 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 72.07 | 0.52 | 50.15 | 0.32 | 34.35 | 0.05 | 33.49 |
    0.07 | 41.90 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 65.87 | 0.42 | 50.05 | 0.34 | 44.08 | 0.11
    | 42.60 | 0.15 | 47.48 | 0.20 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 65.87 | 0.42 | 50.05 | 0.34 | 44.08 | 0.11 | 42.60
    | 0.15 | 47.48 | 0.20 |'
- en: '| w/ BGE Embedding, Top k=30 | 65.26 | 0.49 | 55.21 | 0.43 | 43.82 | 0.10 |
    42.80 | 0.18 | 48.31 | 0.23 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 65.26 | 0.49 | 55.21 | 0.43 | 43.82 | 0.10 | 42.80 |
    0.18 | 48.31 | 0.23 |'
- en: '| w/ Openai Embedding, Top k=50 | 67.21 | 0.46 | 51.38 | 0.38 | 31.65 | 0.03
    | 40.69 | 0.12 | 43.52 | 0.19 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 67.21 | 0.46 | 51.38 | 0.38 | 31.65 | 0.03 | 40.69
    | 0.12 | 43.52 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=50 | 67.43 | 0.46 | 53.98 | 0.39 | 45.04 | 0.12 |
    46.94 | 0.21 | 49.96 | 0.24 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 67.43 | 0.46 | 53.98 | 0.39 | 45.04 | 0.12 | 46.94 |
    0.21 | 49.96 | 0.24 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 74.84 | 0.65 | 42.40 | 0.21 | 38.70 | 0.04 | 45.06 | 0.09
    | 46.95 | 0.19 |'
- en: '| w/ Openai Embedding, Top k=5 | 67.45 | 0.49 | 29.00 | 0.13 | 25.09 | 0.01
    | 27.22 | 0.02 | 33.69 | 0.12 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 67.45 | 0.49 | 29.00 | 0.13 | 25.09 | 0.01 | 27.22
    | 0.02 | 33.69 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 71.12 | 0.56 | 31.36 | 0.14 | 25.32 | 0.00 |
    25.78 | 0.04 | 34.43 | 0.13 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 71.12 | 0.56 | 31.36 | 0.14 | 25.32 | 0.00 | 25.78 |
    0.04 | 34.43 | 0.13 |'
- en: '| w/ Openai Embedding, Top k=10 | 72.37 | 0.55 | 31.41 | 0.13 | 30.59 | 0.01
    | 33.14 | 0.08 | 38.38 | 0.14 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 72.37 | 0.55 | 31.41 | 0.13 | 30.59 | 0.01 | 33.14
    | 0.08 | 38.38 | 0.14 |'
- en: '| w/ BGE Embedding, Top k=10 | 79.04 | 0.67 | 34.29 | 0.18 | 30.59 | 0.02 |
    29.69 | 0.06 | 39.22 | 0.17 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 79.04 | 0.67 | 34.29 | 0.18 | 30.59 | 0.02 | 29.69 |
    0.06 | 39.22 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=30 | 74.03 | 0.57 | 37.00 | 0.22 | 39.53 | 0.04
    | 36.07 | 0.09 | 43.91 | 0.18 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 74.03 | 0.57 | 37.00 | 0.22 | 39.53 | 0.04 | 36.07
    | 0.09 | 43.91 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 75.45 | 0.59 | 45.96 | 0.31 | 36.91 | 0.04 |
    39.16 | 0.14 | 45.68 | 0.21 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 75.45 | 0.59 | 45.96 | 0.31 | 36.91 | 0.04 | 39.16 |
    0.14 | 45.68 | 0.21 |'
- en: '| w/ Openai Embedding, Top k=50 | 77.24 | 0.59 | 25.02 | 0.12 | 37.55 | 0.05
    | 40.93 | 0.15 | 44.52 | 0.19 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 77.24 | 0.59 | 25.02 | 0.12 | 37.55 | 0.05 | 40.93
    | 0.15 | 44.52 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=50 | 74.19 | 0.55 | 46.15 | 0.31 | 39.71 | 0.04 |
    36.56 | 0.15 | 45.99 | 0.20 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 74.19 | 0.55 | 46.15 | 0.31 | 39.71 | 0.04 | 36.56 |
    0.15 | 45.99 | 0.20 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (128K) | 36.79 | 0.19 | 23.97 | 0.08 | 30.40 | 0.00 | 32.89 | 0.07
    | 31.11 | 0.07 |'
- en: '| w/ Openai Embedding, Top k=5 | 50.76 | 0.22 | 17.25 | 0.00 | 19.53 | 0.00
    | 16.61 | 0.00 | 24.91 | 0.05 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 50.76 | 0.22 | 17.25 | 0.00 | 19.53 | 0.00 | 16.61
    | 0.00 | 24.91 | 0.05 |'
- en: '| w/ BGE Embedding, Top k=5 | 51.02 | 0.26 | 18.75 | 0.03 | 17.83 | 0.00 |
    18.77 | 0.02 | 25.07 | 0.06 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 51.02 | 0.26 | 18.75 | 0.03 | 17.83 | 0.00 | 18.77 |
    0.02 | 25.07 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=10 | 57.98 | 0.31 | 23.00 | 0.03 | 25.08 | 0.00
    | 21.29 | 0.02 | 30.00 | 0.07 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 57.98 | 0.31 | 23.00 | 0.03 | 25.08 | 0.00 | 21.29
    | 0.02 | 30.00 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=10 | 51.48 | 0.25 | 23.36 | 0.05 | 22.55 | 0.00 |
    18.95 | 0.02 | 27.48 | 0.06 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 51.48 | 0.25 | 23.36 | 0.05 | 22.55 | 0.00 | 18.95 |
    0.02 | 27.48 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=30 | 55.85 | 0.26 | 26.38 | 0.08 | 29.94 | 0.00
    | 21.81 | 0.02 | 32.66 | 0.07 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 55.85 | 0.26 | 26.38 | 0.08 | 29.94 | 0.00 | 21.81
    | 0.02 | 32.66 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=30 | 53.94 | 0.21 | 24.82 | 0.07 | 30.77 | 0.00 |
    23.61 | 0.05 | 32.68 | 0.07 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 53.94 | 0.21 | 24.82 | 0.07 | 30.77 | 0.00 | 23.61 |
    0.05 | 32.68 | 0.07 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.00 | 0.28 | 12.79 | 0.04 | 29.64 | 0.00
    | 24.67 | 0.07 | 31.88 | 0.08 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 55.00 | 0.28 | 12.79 | 0.04 | 29.64 | 0.00 | 24.67
    | 0.07 | 31.88 | 0.08 |'
- en: '| w/ BGE Embedding, Top k=50 | 51.17 | 0.21 | 23.36 | 0.10 | 33.08 | 0.02 |
    28.39 | 0.09 | 33.82 | 0.09 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 51.17 | 0.21 | 23.36 | 0.10 | 33.08 | 0.02 | 28.39 |
    0.09 | 33.82 | 0.09 |'
- en: 'Table 7: The result of adding RAG module on GPT-4o with different length sets.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在不同长度数据集上添加 RAG 模块后的 GPT-4o 结果。
- en: '| Model | Spotlight Locating | Comparison | Clustering | Chain of Reasoning
    | Overall |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 聚光定位 | 比较 | 聚类 | 推理链 | 总体 |'
- en: '| $\mathtt{Set1}$ (10K-50K) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set1}$ (10K-50K) |'
- en: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 68.49 | 0.55 | 60.60 | 0.37 | 47.08 | 0.08 |
    70.39 | 0.36 | 60.11 | 0.29 |'
- en: '| w/ Openai Embedding, Top k=5 | 54.62 | 0.45 | 26.17 | 0.08 | 29.60 | 0.03
    | 34.41 | 0.08 | 34.51 | 0.12 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 54.62 | 0.45 | 26.17 | 0.08 | 29.60 | 0.03 | 34.41
    | 0.08 | 34.51 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 62.92 | 0.53 | 30.92 | 0.08 | 31.28 | 0.03 |
    32.95 | 0.11 | 36.91 | 0.15 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 62.92 | 0.53 | 30.92 | 0.08 | 31.28 | 0.03 | 32.95 |
    0.11 | 36.91 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=10 | 59.81 | 0.43 | 34.93 | 0.15 | 29.33 | 0.02
    | 41.27 | 0.15 | 38.96 | 0.15 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 59.81 | 0.43 | 34.93 | 0.15 | 29.33 | 0.02 | 41.27
    | 0.15 | 38.96 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 72.13 | 0.62 | 32.42 | 0.12 | 31.90 | 0.05 |
    44.12 | 0.20 | 42.27 | 0.20 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 72.13 | 0.62 | 32.42 | 0.12 | 31.90 | 0.05 | 44.12 |
    0.20 | 42.27 | 0.20 |'
- en: '| w/ Openai Embedding, Top k=30 | 57.26 | 0.40 | 45.43 | 0.28 | 40.04 | 0.06
    | 57.32 | 0.35 | 49.06 | 0.24 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 57.26 | 0.40 | 45.43 | 0.28 | 40.04 | 0.06 | 57.32
    | 0.35 | 49.06 | 0.24 |'
- en: '| w/ BGE Embedding, Top k=30 | 56.37 | 0.33 | 46.27 | 0.30 | 38.35 | 0.10 |
    51.49 | 0.29 | 46.69 | 0.23 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 56.37 | 0.33 | 46.27 | 0.30 | 38.35 | 0.10 | 51.49 |
    0.29 | 46.69 | 0.23 |'
- en: '| w/ Openai Embedding, Top k=50 | 51.08 | 0.35 | 44.53 | 0.27 | 37.96 | 0.05
    | 53.95 | 0.35 | 46.11 | 0.23 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 51.08 | 0.35 | 44.53 | 0.27 | 37.96 | 0.05 | 53.95
    | 0.35 | 46.11 | 0.23 |'
- en: '| w/ BGE Embedding, Top k=50 | 53.47 | 0.37 | 47.31 | 0.29 | 36.42 | 0.06 |
    54.65 | 0.35 | 46.62 | 0.24 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 53.47 | 0.37 | 47.31 | 0.29 | 36.42 | 0.06 | 54.65 |
    0.35 | 46.62 | 0.24 |'
- en: '| $\mathtt{Set2}$ (50K-100K) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set2}$ (50K-100K) |'
- en: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 64.53 | 0.43 | 42.60 | 0.21 | 38.52 | 0.05 |
    51.18 | 0.20 | 45.71 | 0.17 |'
- en: '| w/ Openai Embedding, Top k=5 | 56.64 | 0.40 | 36.68 | 0.19 | 30.91 | 0.03
    | 28.38 | 0.01 | 34.54 | 0.10 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 56.64 | 0.40 | 36.68 | 0.19 | 30.91 | 0.03 | 28.38
    | 0.01 | 34.54 | 0.10 |'
- en: '| w/ BGE Embedding, Top k=5 | 67.29 | 0.47 | 43.39 | 0.28 | 28.31 | 0.03 |
    32.22 | 0.07 | 36.95 | 0.14 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 67.29 | 0.47 | 43.39 | 0.28 | 28.31 | 0.03 | 32.22 |
    0.07 | 36.95 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=10 | 67.07 | 0.53 | 44.30 | 0.27 | 34.31 | 0.05
    | 34.03 | 0.06 | 40.17 | 0.15 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 67.07 | 0.53 | 44.30 | 0.27 | 34.31 | 0.05 | 34.03
    | 0.06 | 40.17 | 0.15 |'
- en: '| w/ BGE Embedding, Top k=10 | 71.74 | 0.54 | 47.68 | 0.30 | 30.55 | 0.03 |
    30.57 | 0.03 | 38.80 | 0.14 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 71.74 | 0.54 | 47.68 | 0.30 | 30.55 | 0.03 | 30.57 |
    0.03 | 38.80 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 66.27 | 0.46 | 46.28 | 0.31 | 38.95 | 0.05
    | 46.15 | 0.22 | 45.42 | 0.19 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 66.27 | 0.46 | 46.28 | 0.31 | 38.95 | 0.05 | 46.15
    | 0.22 | 45.42 | 0.19 |'
- en: '| w/ BGE Embedding, Top k=30 | 57.35 | 0.41 | 46.92 | 0.29 | 35.30 | 0.05 |
    42.82 | 0.20 | 42.04 | 0.18 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 57.35 | 0.41 | 46.92 | 0.29 | 35.30 | 0.05 | 42.82 |
    0.20 | 42.04 | 0.18 |'
- en: '| w/ Openai Embedding, Top k=50 | 55.94 | 0.32 | 47.94 | 0.31 | 34.32 | 0.03
    | 46.64 | 0.21 | 42.60 | 0.16 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 55.94 | 0.32 | 47.94 | 0.31 | 34.32 | 0.03 | 46.64
    | 0.21 | 42.60 | 0.16 |'
- en: '| w/ BGE Embedding, Top k=50 | 59.41 | 0.39 | 38.52 | 0.21 | 35.40 | 0.06 |
    45.47 | 0.24 | 41.51 | 0.17 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 59.41 | 0.39 | 38.52 | 0.21 | 35.40 | 0.06 | 45.47 |
    0.24 | 41.51 | 0.17 |'
- en: '| $\mathtt{Set3}$ (100K-200K) |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set3}$ (100K-200K) |'
- en: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 46.99 | 0.27 | 37.06 | 0.13 | 31.50 | 0.02 |
    35.01 | 0.07 | 35.94 | 0.09 |'
- en: '| w/ Openai Embedding, Top k=5 | 63.91 | 0.44 | 33.56 | 0.17 | 25.98 | 0.01
    | 28.98 | 0.04 | 34.48 | 0.12 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 63.91 | 0.44 | 33.56 | 0.17 | 25.98 | 0.01 | 28.98
    | 0.04 | 34.48 | 0.12 |'
- en: '| w/ BGE Embedding, Top k=5 | 64.81 | 0.47 | 30.27 | 0.14 | 25.88 | 0.01 |
    27.86 | 0.05 | 33.70 | 0.12 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 64.81 | 0.47 | 30.27 | 0.14 | 25.88 | 0.01 | 27.86 |
    0.05 | 33.70 | 0.12 |'
- en: '| w/ Openai Embedding, Top k=10 | 67.50 | 0.46 | 33.44 | 0.16 | 27.94 | 0.02
    | 31.62 | 0.06 | 36.47 | 0.13 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 67.50 | 0.46 | 33.44 | 0.16 | 27.94 | 0.02 | 31.62
    | 0.06 | 36.47 | 0.13 |'
- en: '| w/ BGE Embedding, Top k=10 | 75.88 | 0.56 | 33.76 | 0.15 | 27.20 | 0.01 |
    30.17 | 0.04 | 37.28 | 0.14 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 75.88 | 0.56 | 33.76 | 0.15 | 27.20 | 0.01 | 30.17 |
    0.04 | 37.28 | 0.14 |'
- en: '| w/ Openai Embedding, Top k=30 | 73.69 | 0.55 | 42.20 | 0.27 | 32.78 | 0.02
    | 37.65 | 0.13 | 42.60 | 0.18 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 73.69 | 0.55 | 42.20 | 0.27 | 32.78 | 0.02 | 37.65
    | 0.13 | 42.60 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=30 | 67.50 | 0.47 | 42.42 | 0.18 | 32.34 | 0.03 |
    37.85 | 0.12 | 41.35 | 0.15 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 67.50 | 0.47 | 42.42 | 0.18 | 32.34 | 0.03 | 37.85 |
    0.12 | 41.35 | 0.15 |'
- en: '| w/ Openai Embedding, Top k=50 | 67.44 | 0.50 | 41.82 | 0.24 | 31.59 | 0.04
    | 37.29 | 0.12 | 40.90 | 0.18 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 67.44 | 0.50 | 41.82 | 0.24 | 31.59 | 0.04 | 37.29
    | 0.12 | 40.90 | 0.18 |'
- en: '| w/ BGE Embedding, Top k=50 | 62.56 | 0.42 | 40.41 | 0.18 | 29.82 | 0.02 |
    40.31 | 0.14 | 39.84 | 0.15 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 62.56 | 0.42 | 40.41 | 0.18 | 29.82 | 0.02 | 40.31 |
    0.14 | 39.84 | 0.15 |'
- en: '| $\mathtt{Set4}$ (200K-250K) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Set4}$ (200K-250K) |'
- en: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct (128K) | 33.18 | 0.16 | 26.59 | 0.08 | 29.84 | 0.01 |
    25.81 | 0.04 | 28.92 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=5 | 51.49 | 0.26 | 17.12 | 0.03 | 21.59 | 0.00
    | 16.37 | 0.00 | 25.59 | 0.06 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=5 | 51.49 | 0.26 | 17.12 | 0.03 | 21.59 | 0.00 | 16.37
    | 0.00 | 25.59 | 0.06 |'
- en: '| w/ BGE Embedding, Top k=5 | 48.40 | 0.26 | 14.55 | 0.00 | 20.69 | 0.00 |
    18.07 | 0.00 | 24.63 | 0.05 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=5 | 48.40 | 0.26 | 14.55 | 0.00 | 20.69 | 0.00 | 18.07 |
    0.00 | 24.63 | 0.05 |'
- en: '| w/ Openai Embedding, Top k=10 | 50.32 | 0.28 | 20.30 | 0.03 | 24.56 | 0.00
    | 16.38 | 0.00 | 27.08 | 0.06 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=10 | 50.32 | 0.28 | 20.30 | 0.03 | 24.56 | 0.00 | 16.38
    | 0.00 | 27.08 | 0.06 |'
- en: '| w/ BGE Embedding, Top k=10 | 51.02 | 0.28 | 21.88 | 0.03 | 25.45 | 0.00 |
    17.29 | 0.00 | 28.10 | 0.06 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=10 | 51.02 | 0.28 | 21.88 | 0.03 | 25.45 | 0.00 | 17.29 |
    0.00 | 28.10 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=30 | 52.17 | 0.24 | 24.60 | 0.10 | 26.78 | 0.00
    | 17.79 | 0.00 | 29.29 | 0.07 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=30 | 52.17 | 0.24 | 24.60 | 0.10 | 26.78 | 0.00 | 17.79
    | 0.00 | 29.29 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=30 | 47.98 | 0.21 | 26.82 | 0.10 | 26.70 | 0.00 |
    20.02 | 0.00 | 29.44 | 0.06 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=30 | 47.98 | 0.21 | 26.82 | 0.10 | 26.70 | 0.00 | 20.02 |
    0.00 | 29.44 | 0.06 |'
- en: '| w/ Openai Embedding, Top k=50 | 51.63 | 0.26 | 23.62 | 0.08 | 24.49 | 0.00
    | 21.84 | 0.02 | 29.14 | 0.07 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Openai 嵌入，Top k=50 | 51.63 | 0.26 | 23.62 | 0.08 | 24.49 | 0.00 | 21.84
    | 0.02 | 29.14 | 0.07 |'
- en: '| w/ BGE Embedding, Top k=50 | 47.23 | 0.28 | 27.78 | 0.08 | 26.48 | 0.00 |
    24.44 | 0.02 | 30.52 | 0.08 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 使用 BGE 嵌入，Top k=50 | 47.23 | 0.28 | 27.78 | 0.08 | 26.48 | 0.00 | 24.44 |
    0.02 | 30.52 | 0.08 |'
- en: 'Table 8: The result of adding RAG module on Qwen2-72B-Instruct with different
    length sets.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 8：在不同长度集合上对Qwen2-72B-Instruct添加RAG模块的结果。
- en: '| LongBench |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| LongBench |'
- en: '| Passage 1: The Real Glory. The Real Glory is a 1939 Samuel Goldwyn Productions
    adventure film starring Gary Cooper, David Niven, Andrea Leeds and Broderick Crawford
    released by United Artists in the weeks immediately following Nazi Germany’s invasion
    of Poland. Based on a 1937 novel of the same name by Charles L. Clifford and directed
    by Henry Hathaway, the film is set against the backdrop of the Moro Rebellion
    during the American occupation of the Philippines at the beginning of the 20th
    century $\ldots$ |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 片段 1：真正的荣耀。《真正的荣耀》是一部1939年由塞缪尔·戈德温制片公司出品的冒险片，由加里·库珀、大卫·尼文、安德烈亚·利兹和布罗德里克·克劳福德主演，由联合艺术家在纳粹德国入侵波兰后的几周内发行。根据查尔斯·L·克利福德1937年的同名小说改编，由亨利·哈撒韦执导，该片背景设定在20世纪初美国占领菲律宾期间的摩洛叛乱中
    $\ldots$'
- en: '| Passage 2: Jay Sheffield. Jay Howard Sheffield (September 25, 1934 – June
    25, 1998) was an American actor, who appeared on the stage, in films, and on television.
    He married Barbara Babcock on June 9, 1962, in San Mateo, California. They later
    divorced $\ldots$ |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 片段 2：杰伊·谢菲尔德。杰伊·霍华德·谢菲尔德（1934年9月25日 – 1998年6月25日）是美国演员，曾在舞台、电影和电视上出演。他于1962年6月9日在加利福尼亚州圣马特奥与芭芭拉·巴布科克结婚，后来他们离婚了
    $\ldots$ |'
- en: '| Passage 3: David Niven. James David Graham Niven (; 1 March 1910 – 29 July
    1983) was a British actor, soldier, memoirist, and novelist. Niven was known as
    a handsome and debonair leading man in Classic Hollywood films. He received an
    Academy Award and a Golden Globe Award $\ldots$ |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 片段 3：大卫·尼文。詹姆斯·大卫·格雷厄姆·尼文（1910年3月1日 – 1983年7月29日）是英国演员、军人、回忆录作者和小说家。尼文以其英俊潇洒的形象在经典好莱坞影片中著称。他获得了奥斯卡奖和金球奖
    $\ldots$ |'
- en: '| Passage 4: Phileas Fogg snacks. Phileas Fogg Ltd is a company that produces
    snack products in the United Kingdom that was created in 1982 by Derwent Valley
    Foods. The brand is named for Phileas Fogg, the protagonist of Jules Verne’s Around
    the World in Eighty Days $\ldots$ |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 片段 4：菲勒斯·福克零食。菲勒斯·福克有限公司是一家生产零食的英国公司，由Derwent Valley Foods于1982年创建。该品牌以儒勒·凡尔纳的《环游世界八十天》中的主角菲勒斯·福克命名
    $\ldots$ |'
- en: '| Passage 5: Jules Verne Trophy. The Jules Verne Trophy is a prize for the
    fastest circumnavigation of the world by any type of yacht with no restrictions
    on the size of the crew provided the vessel has registered with the organization
    and paid an entry fee. A vessel holding the Jules Verne trophy will not necessarily
    hold the absolute round the world record $\ldots$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 片段 5：儒勒·凡尔纳奖。儒勒·凡尔纳奖是一项授予任何类型的游艇进行世界最快环球航行的奖项，船员人数没有限制，只要船只已在组织注册并支付了参赛费。获得儒勒·凡尔纳奖的船只不一定会保持绝对的环球记录
    $\ldots$ |'
- en: '| Question: The actor that plays Phileas Fogg in "Around the World in 80 Days",
    co-starred with Gary Cooper in a 1939 Goldwyn Productions film based on a novel
    by what author? |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 问题：在《80天环游地球》中扮演菲勒斯·福克的演员，与加里·库珀在1939年金怀德制片公司出品的电影中共同出演，该小说的作者是谁？ |'
- en: '| Answer: Charles L. Clifford |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 答案：查尔斯·L·克利福德 |'
- en: '| Loong (Ours) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 龙（我们的） |'
- en: '| Document 1: THE ARENA GROUP HOLDINGS, INC. Proceeds from Simplify loan: $7,748\.
    Unearned revenue: $(11,665). Amortization of debt discounts: $536\. Cash and cash
    equivalents: $4,003. Noncash and accrued interest: $2,839\. Loss on impairment
    of assets: $40,589\. Accounts receivable, net: $12,029\. Subscription refund liability:
    $18\. Accounts payable: $(102). Subscription acquisition costs: $6,131\. Change
    in fair value of contingent consideration: $313\. ($ in thousands, except share
    data) $\ldots$ |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 文档 1：THE ARENA GROUP HOLDINGS, INC. 从简化贷款中获得的收益：$7,748。未赚取收入：$(11,665)。债务折扣的摊销：$536。现金及现金等价物：$4,003。非现金及应计利息：$2,839。资产减值损失：$40,589。应收账款净额：$12,029。订阅退款负债：$18。应付账款：$(102)。订阅获取成本：$6,131。或有对价公允价值变动：$313。($
    in thousands, except share data) $\ldots$ |'
- en: '| Document 2: General Enterprise Ventures, Inc. For purposes of balance sheet
    presentation and reporting of cash flows, the Company considers all unrestricted
    demand deposits, money market funds and highly liquid debt instruments with an
    original maturity of less than 90 days to be cash and cash equivalents. The Company
    did not have any cash equivalents at March 31, 2024\. The Company had cash of
    $549,755 at March 31, 2024 $\ldots$ |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 文档 2：General Enterprise Ventures, Inc. 为了平衡表的展示和现金流的报告，公司将所有不受限制的活期存款、货币市场基金和原始到期日少于90天的高流动性债务工具视为现金及现金等价物。公司在2024年3月31日没有任何现金等价物。公司在2024年3月31日拥有现金$549,755
    $\ldots$ |'
- en: '| Document 3: BROAD STREET REALTY, INC. The carrying amounts of cash and cash
    equivalents, restricted cash, receivables and payables are reasonable estimates
    of their fair value as of March 31, 2024 due to the short-term nature of these
    instruments. Reconciliation of cash and cash equivalents and restricted cash:
    Cash and cash equivalents: $14,631 (in thousands) $\ldots$ |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 文档 3: BROAD STREET REALTY, INC. 由于这些工具的短期性质，现金和现金等价物、受限现金、应收账款和应付账款的账面金额是对其公允价值的合理估计，截止日期为
    2024 年 3 月 31 日。现金和现金等价物与受限现金的对账：现金和现金等价物：$14,631（千美元）$\ldots$ |'
- en: '| Question: Please list the ‘Cash and Cash Equivalents’ of the aforementioned
    companies in ascending order. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 问题：请按升序列出上述公司的‘现金和现金等价物’。 |'
- en: '| Answer: 1\. General Enterprise Ventures, Inc.: $549,755\. 2\. Arena Group
    Holdings, Inc.: $4,003 in thousands. 3\. Broad Street Realty, Inc.: $14,631 in
    thousands. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 答案：1\. General Enterprise Ventures, Inc.: $549,755。2\. Arena Group Holdings,
    Inc.: $4,003（千美元）。3\. Broad Street Realty, Inc.: $14,631（千美元）。 |'
- en: 'Table 9: Comparison of Evidence Distribution in Examples from LongBench and
    Loong (Ours). Evidence related to the answers is highlighted with Orange Background.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: LongBench 和 Loong（我们）的示例中证据分布的比较。与答案相关的证据用橙色背景突出显示。'
