- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:03:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:03:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BABILong: 测试大型语言模型在长上下文推理中的极限'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10149](https://ar5iv.labs.arxiv.org/html/2406.10149)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10149](https://ar5iv.labs.arxiv.org/html/2406.10149)
- en: Yuri Kuratov^∗ ^(1,2)   Aydar Bulatov^∗ ²   Petr Anokhin¹   Ivan Rodkin² \ANDDmitry
    Sorokin¹   Artyom Sorokin¹   Mikhail Burtsev³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuri Kuratov^∗ ^(1,2)   Aydar Bulatov^∗ ²   Petr Anokhin¹   Ivan Rodkin² \ANDDmitry
    Sorokin¹   Artyom Sorokin¹   Mikhail Burtsev³
- en: ¹AIRI, Moscow, Russia   ²Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny,
    Russia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹AIRI, 俄罗斯莫斯科   ²MIPT神经网络与深度学习实验室, 俄罗斯多尔戈普鲁德内
- en: ³London Institute for Mathematical Sciences, London, UK
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³伦敦数学科学研究所, 英国伦敦
- en: '{yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, the input context sizes of large language models (LLMs) have
    increased dramatically. However, existing evaluation methods have not kept pace,
    failing to comprehensively assess the efficiency of models in handling long contexts.
    To bridge this gap, we introduce the BABILong benchmark, designed to test language
    models’ ability to reason across facts distributed in extremely long documents.
    BABILong includes a diverse set of 20 reasoning tasks, including fact chaining,
    simple induction, deduction, counting, and handling lists/sets. These tasks are
    challenging on their own, and even more demanding when the required facts are
    scattered across long natural text. Our evaluations show that popular LLMs effectively
    utilize only 10-20% of the context and their performance declines sharply with
    increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
    Generation methods achieve a modest 60% accuracy on single-fact question answering,
    independent of context length. Among context extension methods, the highest performance
    is demonstrated by recurrent memory transformers, enabling the processing of lengths
    up to 11 million tokens. The BABILong benchmark is extendable to any length to
    support the evaluation of new upcoming models with increased capabilities, and
    we provide splits up to 1 million token lengths.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）的输入上下文大小显著增加。然而，现有的评估方法未能跟上这一进步，未能全面评估模型在处理长上下文时的效率。为弥补这一不足，我们引入了BABILong基准测试，旨在测试语言模型在极长文档中跨事实推理的能力。BABILong包含一套多样化的20个推理任务，包括事实链、简单归纳、演绎推理、计数和处理列表/集合。这些任务本身具有挑战性，当所需的事实散布在长自然文本中时，挑战更大。我们的评估表明，流行的LLMs有效利用的上下文仅为10-20%，且随着推理复杂性的增加，其表现急剧下降。在上下文推理的替代方案中，检索增强生成方法在单一事实问答中取得了适度的60%准确率，与上下文长度无关。在上下文扩展方法中，最高性能由递归记忆变换器展示，能够处理长度达到1100万个标记。BABILong基准测试可以扩展到任何长度，以支持对具有更大能力的新模型的评估，我们提供了最长达到100万标记长度的分割。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Today, large language models (LLMs) and neural architectures are continually
    evolving and achieving remarkable improvements, particularly in their ability
    to handle longer contexts (OpenAI, [2023](#bib.bib45); Reid et al., [2024](#bib.bib53);
    Anthropic, [2024](#bib.bib5)). The ability of these models to process and generate
    text based on rich contextual information is crucial for several reasons. For
    example, longer contexts provide more information for the model to condition its
    outputs, leading to more accurate, contextually relevant, and up-to-date responses.
    Furthermore, long-context capabilities can enhance in-context learning by providing
    more in-context examples, instructions to follow, or example trajectories in context
    of reinforcement learning (Chevalier et al., [2023](#bib.bib14); Agarwal et al.,
    [2024](#bib.bib2); Lee et al., [2024](#bib.bib34)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大型语言模型（LLMs）和神经网络架构不断发展，取得了显著的进步，特别是在处理更长上下文的能力上 (OpenAI, [2023](#bib.bib45);
    Reid et al., [2024](#bib.bib53); Anthropic, [2024](#bib.bib5))。这些模型基于丰富的上下文信息处理和生成文本的能力至关重要。举例来说，更长的上下文为模型提供了更多信息以调整其输出，从而产生更准确、与上下文相关和最新的回应。此外，长上下文能力可以通过提供更多上下文示例、需要遵循的指令或在强化学习的上下文中示例轨迹来增强上下文学习 (Chevalier
    et al., [2023](#bib.bib14); Agarwal et al., [2024](#bib.bib2); Lee et al., [2024](#bib.bib34))。
- en: Despite these advances in models capabilities, the benchmarks used to evaluate
    them have not kept pace. For example, current benchmarks such as Longbench (Bai
    et al., [2023](#bib.bib6)) and L-Eval An et al. ([2023](#bib.bib4)) scale only
    up to 40,000 tokens, while models are capable of hundreds of thousands and millions
    of tokens (OpenAI, [2023](#bib.bib45); Bulatov et al., [2024](#bib.bib11); Gu
    & Dao, [2023](#bib.bib24); Anthropic, [2024](#bib.bib5); Reid et al., [2024](#bib.bib53);
    Liu et al., [2024a](#bib.bib41)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型能力有所提升，但用于评估它们的基准测试却没有跟上。例如，当前的基准测试如 Longbench （Bai 等，[2023](#bib.bib6)）和
    L-Eval An 等 （[2023](#bib.bib4)）的规模仅达到 40,000 个 tokens，而模型能够处理数十万甚至数百万个 tokens （OpenAI，[2023](#bib.bib45)；Bulatov
    等，[2024](#bib.bib11)；Gu & Dao，[2023](#bib.bib24)；Anthropic，[2024](#bib.bib5)；Reid
    等，[2024](#bib.bib53)；Liu 等，[2024a](#bib.bib41)）。
- en: '![Refer to caption](img/c7b4877c270df2d7cab800f25d82e385.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7b4877c270df2d7cab800f25d82e385.png)'
- en: 'Figure 1: a) Generation of BABILong dataset. Facts relevant for the question
    are hidden inside a larger background texts from PG19\. b) Recurrent transformers
    answer questions about facts from very long texts when retrieval augmented generation
    fails. Common RAG method fails to answer questions because order of facts matters.
    GPT-4 LLM effectively uses only about 10% of the full 128K window. Small LMs,
    RMT with GPT-2 (137M) and Mamba (130M) fine-tuned for the task are able to solve
    it, with recurrent memory transformer scoring well up to record 11 111 111 tokens.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：a) BABILong 数据集的生成。与问题相关的事实被隐藏在来自 PG19 的更大背景文本中。b) 当检索增强生成失败时，递归变换器回答有关来自非常长文本的事实的问题。常见的
    RAG 方法因事实顺序的影响而无法回答问题。GPT-4 LLM 实际上只使用了完整 128K 窗口的约 10%。小型 LMs，如针对该任务进行微调的 GPT-2
    (137M) 和 Mamba (130M) 的 RMT，能够解决该问题，递归记忆变换器在记录 11 111 111 tokens 时表现良好。
- en: Creating natural and comprehensive long-context benchmarks that are human labeled
    is very challenging. As a consequence, synthetic benchmarks focusing on variations
    of "needle-in-a-haystack" tasks have become increasingly common (Zhang et al.,
    [2024b](#bib.bib77); Liu et al., [2024a](#bib.bib41); Song et al., [2024b](#bib.bib60);
    Hsieh et al., [2024](#bib.bib29)). One widely used needle-in-a-haystack task involves
    finding specific "needles with magic numbers" in a haystack of Paul Graham’s essays¹¹1[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
    However, the widespread use of this approach has highlighted its limitations -
    it is overly simplistic, and novel long context models often achieve perfect performance,
    as usually demonstrated by fully green heatmaps (Reid et al., [2024](#bib.bib53);
    Cohere, [2024](#bib.bib15); Liu et al., [2024a](#bib.bib41); Wang et al., [2024c](#bib.bib67)).
    This shows that while it serves well as a basic verification tool, it is not a
    rigorous benchmark that can effectively challenge and differentiate advanced long-context
    models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自然且全面的长文本基准测试，并由人工标注，是非常具有挑战性的。因此，聚焦于“针在干草堆里”任务变体的合成基准测试变得越来越普遍 （Zhang 等，[2024b](#bib.bib77)；Liu
    等，[2024a](#bib.bib41)；Song 等，[2024b](#bib.bib60)；Hsieh 等，[2024](#bib.bib29)）。一个广泛使用的“针在干草堆里”任务涉及在
    Paul Graham 的论文中找到特定的“带有魔法数字的针”¹¹1[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。然而，这种方法的广泛使用突出了它的局限性——它过于简单，新的长文本模型通常会实现完美的性能，如通常通过完全绿色的热图来展示 （Reid
    等，[2024](#bib.bib53)；Cohere，[2024](#bib.bib15)；Liu 等，[2024a](#bib.bib41)；Wang
    等，[2024c](#bib.bib67)）。这表明，虽然它作为基本验证工具效果良好，但它不是一个可以有效挑战和区分先进长文本模型的严格基准测试。
- en: To bridge this gap, we introduce the BABILong benchmark, designed to test language
    models’ ability to reason across facts distributed in extremely long documents.
    BABILong includes a diverse set of 20 reasoning tasks, including fact chaining,
    simple induction, deduction, counting, and handling lists/sets, that were designed
    as prerequisites for any system that aims to be capable of conversing with a human (Weston
    et al., [2016](#bib.bib68)). As a source of long natural documents we use books
    from PG19 corpora (Rae et al., [2020](#bib.bib50)). In this way, BABILong allows
    the construction of tasks of almost arbitrary length, in order to adapt them to
    the evaluation of new, more powerful models in an extensible and controllable
    way. We provide sets of predefined lengths with splits up to 1 million tokens,
    and we evaluate models on samples with up to 11 million tokens.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一差距，我们引入了BABILong基准，它旨在测试语言模型在极长文档中跨事实推理的能力。BABILong包括20种推理任务，包括事实链、简单归纳、演绎推理、计数和处理列表/集合，这些任务是任何旨在能够与人类对话的系统的先决条件（Weston等，[2016](#bib.bib68)）。作为长自然文档的来源，我们使用PG19语料库中的书籍（Rae等，[2020](#bib.bib50)）。通过这种方式，BABILong允许构建几乎任意长度的任务，以便以可扩展和可控的方式将其适应于对新、更多强大模型的评估。我们提供了预定义长度的集合，分割长度达到100万个标记，并且我们在样本上评估模型，长度可达1100万个标记。
- en: We find that popular LLMs effectively use only 10-20% of the context, with performance
    declining sharply as length and task complexity increase. Retrieval-Augmented
    Generation methods achieve a modest 60% accuracy in answering single-fact questions,
    regardless of context length. Among other methods, Mamba and Recurrent Memory
    Transformers (RMT) show the highest performance, with RMT capable of processing
    lengths up to 11 million tokens.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现流行的LLM仅有效利用了10-20%的上下文，随着长度和任务复杂性的增加，性能急剧下降。检索增强生成方法在回答单一事实问题时取得了60%的适中准确率，与上下文长度无关。在其他方法中，Mamba和递归记忆变换器（RMT）表现最佳，其中RMT能够处理长度高达1100万个标记。
- en: 'The main contributions of our work are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献如下：
- en: 1\. We introduce BABILong, a novel scalable generative multi-task benchmark
    for evaluating the performance of NLP models in processing arbitrarily long documents
    with distributed facts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了BABILong，这是一个新颖的可扩展生成多任务基准，用于评估NLP模型在处理任意长文档中的分布事实的性能。
- en: 2\. We evaluate over 20 recent long-input language models with various sizes,
    architectures, and context extension methods on BABILong.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在BABILong上评估了20多个近期长输入语言模型，涵盖了各种尺寸、架构和上下文扩展方法。
- en: 3\. We find that popular LLMs effectively utilize only 10-20% of the context,
    with performance degrading sharply as reasoning complexity increases. Retrieval
    augmented generation fails to demonstrate good scores but fine-tuning for specific
    task helps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，流行的LLM仅有效利用了10-20%的上下文，随着推理复杂性的增加，性能急剧下降。检索增强生成未能表现出良好的得分，但针对特定任务的微调有所帮助。
- en: 4\. We demonstrate successful in domain single fact question answering with
    the recurrent memory transformer on input texts up to 11 million tokens, setting
    a new record for the sequence size processed by a single model, extending the
    known capabilities of neural networks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在输入文本长达1100万个标记的情况下使用递归记忆变换器，成功展示了领域单一事实问答的能力，创下了单个模型处理序列大小的新纪录，扩展了神经网络的已知能力。
- en: The BABILong benchmark data and code for evaluation are available on GitHub
    ²²2[https://github.com/booydar/babilong](https://github.com/booydar/babilong).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong基准数据和评估代码可在GitHub ²²2[https://github.com/booydar/babilong](https://github.com/booydar/babilong)
    上获取。
- en: 2 The BABILong Benchmark for Long Context Processing
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BABILong基准：长上下文处理
- en: 'Table 1: The first ten tasks of BABILong with the number of supporting and
    distracting facts. The last column displays the performance of LLMs on each task
    in the absence of background text. Each dot represents one of the selected models,
    while the blue bars indicate the median accuracy across tested models.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：BABILong的前十个任务及其支持和干扰事实的数量。最后一列显示了LLM在没有背景文本的情况下对每个任务的表现。每个点代表所选模型之一，而蓝色条形图表示测试模型的中位准确率。
- en: '| Task | Name | facts | relevant facts | LLMs answer accuracy |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 名称 | 事实 | 相关事实 | LLM的回答准确性 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | per task | per task | without background text (0K) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每个任务 | 每个任务 | 无背景文本 (0K) |'
- en: '| qa1 | single supporting fact | 2-10 | 1 | ![[Uncaptioned image]](img/e30c6ce2910a48cdbcdd1928c6cce429.png)
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| qa1 | 单一支持性事实 | 2-10 | 1 | ![[无标题图片]](img/e30c6ce2910a48cdbcdd1928c6cce429.png)
    |'
- en: '| qa2 | two supporting facts | 2-68 | 2 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| qa2 | 两个支持性事实 | 2-68 | 2 |'
- en: '| qa3 | three supporting facts | 4-320 | 3 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| qa3 | 三个支持性事实 | 4-320 | 3 |'
- en: '| qa4 | two arg relations | 2 | 1 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| qa4 | 两个论证关系 | 2 | 1 |'
- en: '| qa5 | three arg relations | 2-126 | 1 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| qa5 | 三个论证关系 | 2-126 | 1 |'
- en: '| qa6 | yes-no questions | 2-26 | 1 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| qa6 | 是非问题 | 2-26 | 1 |'
- en: '| qa7 | counting | 2-52 | 1-10 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| qa7 | 计数 | 2-52 | 1-10 |'
- en: '| qa8 | lists-sets | 2-50 | 1-8 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| qa8 | 列表-集合 | 2-50 | 1-8 |'
- en: '| qa9 | simple negation | 2-10 | 1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| qa9 | 简单否定 | 2-10 | 1 |'
- en: '| qa10 | indefinite knowledge | 2-10 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| qa10 | 不确定知识 | 2-10 | 1 |'
- en: 'The fundamental concept behind the Benchmark for Artificial Intelligence for
    Long-context evaluation is to extend the length of existing tasks to evaluate
    the ability of generative models to efficiently handle long contexts. Solving
    tasks with a long context size requires the model to distinguish important information
    from large amounts of irrelevant details. To simulate this behavior we "hide"
    the sentences of the original task between the sentences of irrelevant text that
    is drawn from another closely related distribution (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")a).
    Examples are constructed by gradually adding new sentences from the background
    dataset in their natural order until the augmented sample reaches the desired
    length. This way, we are not bound by the length of the original task itself,
    making it possible to assess even the longest available models with context sizes
    up to millions of tokens. For background text we use books from the PG19 dataset
     (Rae et al., [2020](#bib.bib50)) due to the substantial book lengths and naturally
    occurring long contexts. The model is required first to distinguish the sentences
    related to the original task, then memorize and subsequently utilize them to generate
    the correct solution.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试的基本概念是将现有任务的长度扩展，以评估生成模型高效处理长上下文的能力。解决具有长上下文的任务要求模型从大量无关的细节中区分出重要信息。为了模拟这种行为，我们将原始任务的句子“隐藏”在从另一个相关分布中提取的无关文本句子之间（参见图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ BABILong: 测试LLMs在长上下文推理中的极限")a）。通过逐渐将来自背景数据集的新句子按自然顺序添加到示例中，直到增强样本达到所需的长度。这样，我们不受原始任务长度的限制，可以评估甚至长达数百万标记的最长模型。我们使用
    PG19 数据集中的书籍作为背景文本（Rae 等，[2020](#bib.bib50)），因为其书籍长度大且自然存在长上下文。模型首先需要区分与原始任务相关的句子，然后记住并随后利用这些句子生成正确的解决方案。'
- en: 'In this work we extend the bAbI benchmark (Weston et al., [2016](#bib.bib68)),
    which consists of 20 tasks designed to evaluate basic aspects of reasoning. These
    tasks are generated by simulating interactions among characters and objects across
    various locations, each represented as a fact, such as "Mary traveled to the office."
    The challenge is to answer questions based on the facts generated in the current
    simulation, such as "Where is Mary?" The tasks in bAbI vary in the number of facts,
    question complexity, and the reasoning skills they assess, including spatial and
    temporal reasoning, deduction, and coreference resolution. In our paper, we label
    these tasks from ’QA1’ to ’QA20’. The first ten tasks, as shown in Table [1](#S2.T1
    "Table 1 ‣ 2 The BABILong Benchmark for Long Context Processing ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack") demonstrate that
    current LLMs exhibit mixed performance even without distractor texts, indicating
    that the BABILong tasks span a broad spectrum of difficulty and allow for testing
    models across various performance dimensions. Details and performance metrics
    for the bAbI tasks, along with examples of BABILong samples generated using our
    pipeline, can be found in Appendix [L](#A12 "Appendix L BABILong Task Examples
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们扩展了bAbI基准测试（Weston等，[2016](#bib.bib68)），它包含了20个任务，用于评估推理的基本方面。这些任务通过模拟角色和对象在不同位置之间的互动生成，每个互动被表示为一个事实，例如“玛丽去了办公室。”挑战在于根据当前模拟中生成的事实回答问题，例如“玛丽在哪里？”bAbI中的任务在事实数量、问题复杂性和评估的推理技能（包括空间和时间推理、推理和共指解析）方面有所不同。在我们的论文中，我们将这些任务标记为’QA1’到’QA20’。如表[1](#S2.T1
    "Table 1 ‣ 2 The BABILong Benchmark for Long Context Processing ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")所示，前十个任务表明，即使没有干扰文本，当前的LLMs也表现出混合的性能，这表明BABILong任务涵盖了广泛的难度范围，并允许在各种性能维度上测试模型。bAbI任务的详细信息和性能指标，以及使用我们管道生成的BABILong样本的示例，可以在附录[L](#A12
    "Appendix L BABILong Task Examples ‣ BABILong: Testing the Limits of LLMs with
    Long Context Reasoning-in-a-Haystack")中找到。'
- en: As evident in the following sections, these seemingly simple tasks pose significant
    challenges to language models. Although filtering facts from background text might
    be straightforward, models encounter next challenges of finding supporting facts
    among distractors and performing types of reasoning such as counting that are
    especially difficult for LLMs. Additionally, most NLP benchmarks are vulnerable
    to data leakage to training sets of modern large language models (Sainz et al.,
    [2023](#bib.bib55)). Generated benchmarks, such as bAbI and BABILong, are immune
    to this type of contamination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以下部分所示，这些看似简单的任务对语言模型提出了重大挑战。尽管从背景文本中过滤事实可能很简单，但模型面临着从干扰项中找到支持事实和执行如计数等推理类型的下一步挑战，这些都是LLMs特别困难的。此外，大多数NLP基准测试易受数据泄漏到现代大型语言模型训练集的影响（Sainz等，[2023](#bib.bib55)）。生成的基准测试，如bAbI和BABILong，对这种污染类型具有免疫力。
- en: In this work we deliberately employ simple algorithmic tasks to underscore the
    fundamental limitations of current models in collecting evidence over long contexts
    even for basic reasoning. The brevity and similarity of the task sentences also
    enable the model distinguish them from seemingly close background text with the
    assistance of few-shot examples. This difference in distributions enables the
    scalability of BABILong to large amounts of diverse noise. Nevertheless, the BABILong
    approach can be applied to incorporate more complex tasks, using the same strategy
    of mixing task sentences with background text.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们故意使用简单的算法任务来强调当前模型在处理长期上下文中的基本局限性，即使是对于基本推理任务也是如此。任务句子的简洁性和相似性还使得模型能够通过少量示例将它们与看似相近的背景文本区分开。这种分布上的差异使得BABILong能够扩展到大量的多样化噪声。尽管如此，BABILong方法也可以应用于更复杂的任务，使用将任务句子与背景文本混合的相同策略。
- en: 3 Benchmarking Results
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基准测试结果
- en: 'Table 2: BABILong is a challenging benchmark for current long-context models.
    Even models that claim to support 128K tokens experience degradation beyond 10%
    of their input capacity. RAG methods do not help, while fine-tuning of small scale
    models (RMT 137M and Mamba 130M) shows that the tasks are solvable. Values represent
    average accuracy over QA1-QA5 tasks from BABILong.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：BABILong 是当前长上下文模型的一个挑战性基准。即使是声称支持 128K 标记的模型，其输入容量超过 10% 后也会出现性能下降。RAG
    方法无济于事，而小规模模型 (RMT 137M 和 Mamba 130M) 的微调显示任务是可解的。数值表示 BABILong 的 QA1-QA5 任务的平均准确率。
- en: '![[Uncaptioned image]](img/2a38835e3275cd9ec1c61f151ccf2d0a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/2a38835e3275cd9ec1c61f151ccf2d0a.png)'
- en: 'To maximize value for the research community, we have included models with
    the highest number of monthly downloads from the Hugging Face platform in our
    evaluation such as LLama-3 (AI@Meta, [2024](#bib.bib3)); 32k-64k – Mistral (Jiang
    et al., [2023](#bib.bib31)), Mixtral (Jiang et al., [2024](#bib.bib32)); 128k
    – ChatGLM3 (Du et al., [2022](#bib.bib20)), Phi-3 (Abdin et al., [2024](#bib.bib1)),
    Command-R (Cohere, [2024](#bib.bib15)); 200k – Yi (Young et al., [2024](#bib.bib72));
    including long-context fine-tuning: LongChat (Li et al., [2023a](#bib.bib37)),
    LLama-2-7b-32k ³³3[https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct](https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct),
    LongAlpaca (Chen et al., [2023](#bib.bib13)); long-context adaptation methods:
    Yarnv2 Mistral (Peng et al., [2023b](#bib.bib47)), Mistral and LLama-2 with Activation
    Beacons (Zhang et al., [2024a](#bib.bib76)). As a reference, we included GPT-4
    (gpt-4-0125-preview), currently the most powerful model available. Retrieval-augmented
    generation was also tested, as it represents a common solution for long document
    QA. As alternatives to traditional architectures, we considered the Mamba (Gu
    & Dao, [2023](#bib.bib24)), Jamba (Lieber et al., [2024](#bib.bib40)) and Recurrent
    Memory Transformer (RMT) (Bulatov et al., [2022](#bib.bib10)). Summary of evaluation
    results is presented in the Table [2](#S3.T2 "Table 2 ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").
    The table reports average accuracy of models on the first 5 tasks (QA1-QA5) of
    BABILong for different context sizes. For evaluation details for each task see
    Table [3](#A4.T3 "Table 3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    and Appendix [C](#A3 "Appendix C Details on RMT and Mamba fine-tuning on BABILong
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '为了最大化对研究社区的价值，我们在评估中包括了 Hugging Face 平台上每月下载量最高的模型，例如 LLama-3 (AI@Meta, [2024](#bib.bib3))；32k-64k
    – Mistral (Jiang et al., [2023](#bib.bib31)), Mixtral (Jiang et al., [2024](#bib.bib32))；128k
    – ChatGLM3 (Du et al., [2022](#bib.bib20)), Phi-3 (Abdin et al., [2024](#bib.bib1)),
    Command-R (Cohere, [2024](#bib.bib15))；200k – Yi (Young et al., [2024](#bib.bib72))；包括长上下文微调：LongChat
    (Li et al., [2023a](#bib.bib37)), LLama-2-7b-32k [https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct](https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct),
    LongAlpaca (Chen et al., [2023](#bib.bib13))；长上下文适应方法：Yarnv2 Mistral (Peng et
    al., [2023b](#bib.bib47)), Mistral 和 LLama-2 配合激活信标 (Zhang et al., [2024a](#bib.bib76))。作为参考，我们还包括了当前最强大的模型
    GPT-4 (gpt-4-0125-preview)。检索增强生成也进行了测试，因为它代表了长文档问答的常见解决方案。作为传统架构的替代方案，我们考虑了 Mamba
    (Gu & Dao, [2023](#bib.bib24)), Jamba (Lieber et al., [2024](#bib.bib40)) 和递归记忆变换器
    (RMT) (Bulatov et al., [2022](#bib.bib10))。评估结果的总结见表 [2](#S3.T2 "Table 2 ‣ 3 Benchmarking
    Results ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")。表中报告了模型在
    BABILong 的前 5 个任务 (QA1-QA5) 中的平均准确性，按不同上下文大小分列。每个任务的评估细节请参见表 [3](#A4.T3 "Table
    3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") 和附录 [C](#A3 "Appendix
    C Details on RMT and Mamba fine-tuning on BABILong ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")。'
- en: 3.1 Evaluation of Effective Context Size
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 有效上下文大小的评估
- en: '![Refer to caption](img/b9ae6da4ece3a69b21ea035c49de96d3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9ae6da4ece3a69b21ea035c49de96d3.png)'
- en: 'Figure 2: LLMs struggle to answer questions about facts in texts larger than
    10,000 tokens. The plots demonstrate how the performance of selected leading models
    deteriorates with increasing context size. For single supporting fact questions
    (QA1), the majority of models perform well up to 4,000 tokens. However, when a
    correct response requires two (QA2) or three (QA3) facts, LLMs fail to achieve
    satisfactory accuracy.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLMs 在处理超过 10,000 个标记的文本中的事实性问题时表现不佳。图示展示了选定的领先模型在上下文大小增加时表现的恶化情况。对于单一支持事实的问题
    (QA1)，大多数模型在 4,000 个标记以内表现良好。然而，当正确的回答需要两个 (QA2) 或三个 (QA3) 事实时，LLMs 无法达到令人满意的准确率。
- en: One of the most important questions regarding performance of long-context models
    is how effectively they utilize the input context. Ideally, a model should maintain
    uniformly high performance regardless of the input size. For instance, if an LLM
    can process 128K tokens, it is expected to use all of this context in addressing
    the user’s task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于长上下文模型性能的一个重要问题是它们如何有效地利用输入上下文。理想情况下，模型应保持均匀的高性能，无论输入大小如何。例如，如果一个LLM可以处理128K个标记，预计它能够在处理用户任务时使用所有这些上下文。
- en: We evaluated the performance of models on question-answering tasks with varying
    numbers of supporting facts (QA1-QA3) to study how LLMs utilize the available
    context. Here, we distinguish between a QA task, which requires a single correct
    answer, and an information retrieval task, which should generate a list of relevant
    facts or references to information sources. We consider performance satisfactory
    if the accuracy of an answer exceeds 85% and a complete failure if it is below
    30%. ⁴⁴4This definition of satisfactory performance is not universal and should
    be adapted to the specific task at hand.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了模型在不同数量支持事实（QA1-QA3）下的问答任务表现，以研究LLMs如何利用可用上下文。在这里，我们区分了需要单一正确答案的问答任务和需要生成相关事实或信息源引用的检索任务。如果回答的准确率超过85%，我们认为表现令人满意；如果低于30%，则视为完全失败。⁴⁴4这一令人满意的表现定义并非普遍适用，应根据具体任务进行调整。
- en: 'Our benchmarking results show that current LLMs do not efficiently use their
    full context (Fig.[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation of Effective Context Size
    ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack")). Only 15 out of 24 tested LLMs were able to correctly
    answer 85% or more of the questions for any of QA1-QA3 tasks in a baseline setting
    without any background distractor text. Even for the the simplest task involving
    a single supporting fact (QA1), the best models are able to use only up to 4K
    tokens efficiently, except for GPT-4, which performs well till 16K. The range
    of full context utilization varies from 5% to 25%. When two supporting facts are
    required for an answer, only GPT-4 can solve the task without background text.
    When facts are embedded within texts, all tested LLMs fall below 85% performance(Fig.[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA2).
    The task with three supporting facts proves extremely challenging to current LLMs,
    with the best scores falling below 70% (Fig.[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation
    of Effective Context Size ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")QA3).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的基准测试结果显示，目前的LLMs无法有效地利用其全部上下文（图[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation of
    Effective Context Size ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")）。在没有任何背景干扰文本的基准设置中，24个测试LLMs中只有15个能够正确回答85%或更多的问题，即QA1-QA3任务。即使是涉及单一支持事实的最简单任务（QA1），最好的模型也只能有效利用最多4K个标记，除了GPT-4，其表现良好直至16K。完整上下文的利用范围从5%到25%不等。当需要两个支持事实来回答时，只有GPT-4能够在没有背景文本的情况下解决任务。当事实嵌入文本中时，所有测试的LLMs表现都低于85%（图[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA2）。三项支持事实的任务对当前LLMs来说极具挑战，最佳分数低于70%（图[2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation of Effective Context Size ‣ 3 Benchmarking Results
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")QA3）。'
- en: 'Going deeper in performance of specific models presented in the Table [2](#S3.T2
    "Table 2 ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with
    Long Context Reasoning-in-a-Haystack") we found the following. Yarn fails to extend
    to longer contexts despite showing stable results in long-context language modeling (Peng
    et al., [2023b](#bib.bib47)). LongChat, LongAlpaca, and both LLama2-7B-32K and
    LLama2-7B-32K-instruct models, even when fine-tuned on 32K lengths, failed to
    perform well on 32K tokens. Activation Beacon performed better than Yarn context
    extension method for Mistral 7B, but still achieved low results (< 40%) on 32K
    contexts. In contrast, Mistral and Mixtral, trained on lengths up to 32K, performed
    well on these lengths. Yi-9B-200k, trained on sequences up to 200K, shows less
    than 30% on 64K tokens and more. Yi-34B-200k shows very promising and stable results
    on lengths up to 64K, but unfortunately we were not able to run it on 128K tokens.
    Phi-3-Mini drops significantly from 64K to 128K, reaching less than 10%, while
    Phi-3-Medium maintains 30% at 128K. Jamba-v1 and Phi-3-Mini show close results,
    but Jamba-v1 does not have drop at 128K and shows 34% on this length. Command-R
    and Phi-3-Medium are the most robust to longer contexts, but start to lose performance
    more sharply at 128K. Phi-3-Medium and Command-R show results very close to GPT-4
    at 32K+ contexts.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地分析表格[2](#S3.T2 "表2 ‣ 3 基准测试结果 ‣ BABILong：测试长上下文推理的LLMs极限")中展示的特定模型的性能，我们发现以下内容。尽管在长上下文语言建模中表现稳定（Peng
    et al., [2023b](#bib.bib47)），Yarn未能扩展到更长的上下文。LongChat、LongAlpaca以及LLama2-7B-32K和LLama2-7B-32K-instruct模型，即使在32K长度上进行微调，也未能在32K令牌上表现良好。Activation
    Beacon在Mistral 7B上的表现优于Yarn的上下文扩展方法，但在32K上下文中仍然取得了较低的结果（< 40%）。相比之下，Mistral和Mixtral在长度达到32K时表现良好。Yi-9B-200k在处理高达200K的序列时，64K令牌及以上的表现不到30%。Yi-34B-200k在长度高达64K时表现出非常有希望且稳定的结果，但遗憾的是我们未能在128K令牌上进行测试。Phi-3-Mini在64K到128K时显著下降，低于10%，而Phi-3-Medium在128K时保持30%。Jamba-v1和Phi-3-Mini的结果接近，但Jamba-v1在128K时没有下降，且在该长度上显示34%。Command-R和Phi-3-Medium对更长的上下文最为稳健，但在128K时开始更急剧地丧失性能。Phi-3-Medium和Command-R在32K+上下文中显示出非常接近GPT-4的结果。
- en: 3.2 Retrieval-Augmented Generation Does Not Perform Well on BABILong
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 检索增强生成在BABILong上的表现不佳
- en: '![Refer to caption](img/5cdcc5790950fa29b16e6f7cd27aefe9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5cdcc5790950fa29b16e6f7cd27aefe9.png)'
- en: 'Figure 3: Fine-tuning but not RAG solves BABILong. a) RAG on QA1 task. Retrieval
    by chunks with size 512 tokens (RAG-C) fails to improve GPT-4 and Llama-3 performance
    on long-context tasks. RAG-S, which retrieves by sentences achieves better results,
    but further increasing the number of retrieved sentences from top-5 to top-20
    does not help. b) Task specific fine-tuning. Finetuned Mamba achieves the best
    overall results, greatly outperforming RAG models. However, processing sequences
    longer than 128k is extremely slow due to technical limitations. On the other
    hand, RMT shines on extremely long sequences, managing to keep high accuracy up
    to 10M tokens.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：微调而非RAG解决BABILong问题。a) RAG在QA1任务中的表现。通过512个令牌大小的块进行检索（RAG-C）未能改善GPT-4和Llama-3在长上下文任务中的表现。RAG-S通过句子进行检索，取得了更好的结果，但进一步将检索的句子数量从前5个增加到前20个并没有帮助。b)
    任务特定的微调。微调后的Mamba取得了最佳的整体结果，远远超过了RAG模型。然而，由于技术限制，处理超过128k的序列非常缓慢。另一方面，RMT在极长序列上表现出色，能够保持高准确率，直到10M令牌。
- en: Retrieval-Augmented Generation (RAG) is a popular solution for language models
    to handle large amounts of text. In RAG relevant parts of texts are retrieved
    from a large dataset on the first stage. Then, the language model uses input augmented
    with retrieved texts to generate the final response. In the case of BABILong,
    we expect RAG to extract all the facts relevant to a question from a long input
    text and then place them in the context of the model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种流行的解决方案，用于让语言模型处理大量文本。在RAG中，相关的文本部分在第一阶段从大型数据集中检索出来。然后，语言模型使用包含检索文本的输入生成最终响应。在BABILong的情况下，我们期望RAG从长输入文本中提取与问题相关的所有事实，然后将它们置于模型的上下文中。
- en: 'We experiment with two options: (1) retrieval by chunks of size 512 tokens,
    denoted RAG-C and (2) retrieval by sentences, called RAG-S. For details of evaluation
    and RAG pipelines with GPT4 and Llama-3 please refer to Appendix [G](#A7 "Appendix
    G Details of the RAG Pipeline ‣ BABILong: Testing the Limits of LLMs with Long
    Context Reasoning-in-a-Haystack"). The findings from the QA1 task, depicted in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform
    Well on BABILong ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")a, indicate that retrieval performance
    using sentence chunks is superior to that of 512-token segments, with a notable
    decrease in accuracy observed already after 16k token context length. However,
    this superiority is task-specific and may not translate effectively to real-world
    applications due to the potential for information loss in smaller chunks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试了两种选项：（1）按512个token大小的块进行检索，标记为RAG-C，以及（2）按句子进行检索，称为RAG-S。有关评估和RAG管道在GPT4和Llama-3中的详细信息，请参阅附录 [G](#A7
    "附录 G RAG管道详细信息 ‣ BABILong：测试LLMs在长上下文推理中的极限")。从QA1任务中得出的发现，如图 [3](#S3.F3 "图 3
    ‣ 3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")a所示，使用句子块的检索性能优于512-token段，且在16k
    token上下文长度后准确性显著下降。然而，这种优越性是任务特定的，可能无法有效地转化为现实世界的应用，因为较小块的信息丢失潜力较大。
- en: 'The RAG pipeline with GPT-4-turbo shows scalable but weak performance on BABILong
    for sentence embeddings and poor scalability with chunk embeddings (see Fig. [3](#S3.F3
    "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform Well on BABILong
    ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack")a). The weak performance of RAG might be attributable
    to the temporal dependencies inherent in the task, where the relevant fact is
    positioned at the end of the text. In QA2 and QA3, retrieval fails dramatically
    with accuracy plummeting below random guessing. This lack of success is attributable
    to the specific demands of these tasks, which require the retrieval of multiple
    (two or three) supporting facts to generate accurate responses. For example, in
    instances where the key facts are "Mary got the milk there." and "Mary travelled
    to the hallway.", with the query being "Where is the milk?", the retrieval system
    may successfully identify the first fact but fail to retrieve the second due to
    insufficient similarity between the question and the latter fact. The default
    retrieval algorithm’s lack of temporal consideration and limitations in the similarity
    function underscore the necessity for additional methods in tasks with multi-hop
    reasoning and temporal dynamics.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-4-turbo的RAG管道在BABILong中的句子嵌入上表现可扩展但较弱，对块嵌入的可扩展性差（见图 [3](#S3.F3 "图 3 ‣
    3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")a）。RAG的弱性能可能归因于任务固有的时间依赖性，即相关事实位于文本末尾。在QA2和QA3中，检索失败显著，准确率跌至低于随机猜测。这一失败归因于这些任务的具体要求，需要检索多个（两个或三个）支持性事实以生成准确的响应。例如，在关键事实为“Mary
    got the milk there.” 和 “Mary travelled to the hallway.” 的情况下，如果查询为“Where is the
    milk?”，检索系统可能成功识别第一个事实，但由于问题与第二个事实之间相似度不足而未能检索到第二个事实。默认的检索算法缺乏时间考虑和相似度函数的局限性突显了在多跳推理和时间动态任务中需要额外方法的必要性。
- en: 3.3 Fine-Tuning Models on BABILong
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 在BABILong上微调模型
- en: 'We performed fine-tuning experiments with GPT-3.5-Turbo, Mistral-7B-Instruct-v0.2,
    RMT with GPT-2 (137M) backbone, and Mamba (130M) models. Fine-tuning results are
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Retrieval-Augmented Generation Does Not Perform
    Well on BABILong ‣ 3 Benchmarking Results ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")b and Appendix [I](#A9 "Appendix I
    LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    Figure [8](#A9.F8 "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对GPT-3.5-Turbo、Mistral-7B-Instruct-v0.2、RMT与GPT-2 (137M)骨干以及Mamba (130M)模型进行了微调实验。微调结果见图 [3](#S3.F3
    "图 3 ‣ 3.2 基于检索的生成在BABILong上的表现不佳 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")b和附录 [I](#A9
    "附录 I LLM微调 ‣ BABILong：测试LLMs在长上下文推理中的极限") 图 [8](#A9.F8 "图 8 ‣ 附录 I LLM微调 ‣ BABILong：测试LLMs在长上下文推理中的极限")。
- en: 'RMT with a GPT-2 (Radford et al., [2019](#bib.bib49)) backbone model is trained
    on each task individually with a segment size of 512 and memory size of 16\. Train
    and evaluation splits of each task contain 10000 and 1000 samples, respectively,
    with a number of facts in each sample ranging from 2 to 320 depending on the task.
    A curriculum strategy is employed, initiating training on short sequences that
    fit in a single segment and then gradually introducing longer sequences once the
    training converges. During each curriculum stage $n$ segments. We provide details
    of training and evaluation procedures in Appendix [E](#A5 "Appendix E RMT Training
    and Evaluation Details ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 GPT-2 (Radford et al., [2019](#bib.bib49)) 作为骨干模型的 RMT 在每个任务上单独训练，段大小为 512，记忆大小为
    16。每个任务的训练和评估分割包含 10000 和 1000 个样本，样本中的事实数量从 2 到 320 不等，具体取决于任务。采用了课程策略，首先在适合单个段的短序列上开始训练，然后在训练收敛后逐步引入较长的序列。在每个课程阶段，$n$
    段。有关训练和评估程序的详细信息，请参见附录 [E](#A5 "附录 E RMT 训练和评估细节 ‣ BABILong: 测试 LLM 在长上下文推理中的极限")。'
- en: RMT model trained on 32 segments totalling in 16K tokens demonstrates strong
    performance on this length. Notably, RMT outperforms GPT-4 significantly, underscoring
    the efficiency of memory mechanism in processing long context. Even more importantly,
    the power of recurrent models extends to sequences longer than the training size.
    RMT shows consistent performance on longer sequences, up to 128k tokens, with
    only a marginal quality degradation. Surprisingly, with context sizes scaling
    to 1 million, 10 million tokens, and even 11.1 million tokens, which is over 600
    times of the training length, the recurrent model persistently outperform the
    larger counterparts utilizing RAG.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 32 个段、总计 16K 令牌上训练的 RMT 模型在这种长度上表现出色。值得注意的是，RMT 显著优于 GPT-4，突显了记忆机制在处理长上下文中的效率。更重要的是，递归模型的能力扩展到训练大小之外的序列。RMT
    在较长序列上表现稳定，最长可达 128k 令牌，质量仅有轻微下降。令人惊讶的是，当上下文大小扩展到 100 万、1000 万，甚至 1110 万令牌时，超过了训练长度的
    600 倍，递归模型仍然持续优于使用 RAG 的较大模型。
- en: Finetuned recurrent models, Mamba and RMT perform equally well on QA1, however
    due to the technical limitations of the Mamba implementation, the inference beyond
    128k was extremely slow, which makes it nearly impossible to process longer sequences.
    RMT greatly outperforms retrieval-augmented models and is able to process sequences
    up to 10M tokens much faster than Mamba. However, Mamba has an edge in complex
    tasks such as remembering a large number of facts in QA3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的递归模型，Mamba 和 RMT 在 QA1 上表现相当。然而，由于 Mamba 实现的技术限制，超过 128k 的推理速度极其缓慢，使得处理更长序列几乎不可能。RMT
    在处理长达 10M 令牌的序列时，比 Mamba 快得多，显著优于检索增强模型。然而，Mamba 在 QA3 中记住大量事实等复杂任务上具有优势。
- en: 'We evaluated GPT-3.5 and Mistral-7B models fine-tuned with 1000 samples from
    QA1 for 3 epochs. The evaluation results are shown in Appendix [I](#A9 "Appendix
    I LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack")
    Figure [8](#A9.F8 "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack"). Fine-tuning dramatically
    improves performance for longer contexts making scores uniform across all input
    sizes. Still, these results are behind of fine-tuned Mamba and RMT.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对使用来自 QA1 的 1000 个样本进行 3 个周期微调的 GPT-3.5 和 Mistral-7B 模型进行了评估。评估结果见附录 [I](#A9
    "附录 I LLM 微调 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") 图 [8](#A9.F8 "图 8 ‣ 附录 I LLM 微调
    ‣ BABILong: 测试 LLM 在长上下文推理中的极限")。微调显著提高了对较长上下文的性能，使得所有输入大小的得分趋于一致。然而，这些结果仍落后于微调的
    Mamba 和 RMT。'
- en: 3.4 BABILong and Other Benchmarks
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 BABILong 和其他基准测试
- en: '![Refer to caption](img/33335ab64ef7267e2a81ae4e8c0e126b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/33335ab64ef7267e2a81ae4e8c0e126b.png)'
- en: 'Figure 4: BABILong is similar to MMLU (Hendrycks et al., [2020](#bib.bib27))
    on short lengths and captures differences in models behavior for longer contexts.
    MMLU is a relatively short benchmark, with samples up to 1k tokens in length.
    BABILong has a higher correlation with MMLU on short contexts (0K) than RULER (Hsieh
    et al., [2024](#bib.bib29)). However, RULER maintains a high correlation regardless
    of task length, with an even higher correlation at 64K, while BABILong’s correlation
    with MMLU decreases with length. This may indicate that BABILong is better at
    capturing differences in models behavior at different context lengths.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：BABILong在短长度上类似于MMLU (Hendrycks et al., [2020](#bib.bib27))，并在较长的上下文中捕捉模型行为的差异。MMLU是一个相对较短的基准测试，样本长度最多为1k
    tokens。BABILong在短上下文（0K）上的相关性高于RULER (Hsieh et al., [2024](#bib.bib29))。然而，RULER无论任务长度如何，都保持较高的相关性，在64K时相关性更高，而BABILong与MMLU的相关性则随着长度的增加而降低。这可能表明BABILong在不同上下文长度下更能捕捉模型行为的差异。
- en: Here, we analyze how models performance on BABILong benchmark differs from MMLU (Hendrycks
    et al., [2020](#bib.bib27)) and RULER (Hsieh et al., [2024](#bib.bib29)). The
    MMLU benchmark measures various branches of knowledge in LLMs, whereas RULER,
    a recently proposed long-context benchmark, shares a similar "needle-in-a-haystack"
    concept with BABILong. One notable difference is that RULER’s "needles" (such
    as adjectives, nouns, numbers, uuids) and long "haystack" contexts are more synthetic,
    consisting of randomly repeated sentences, except for tasks based on the SQuAD (Rajpurkar
    et al., [2016](#bib.bib51)) and HotPotQA (Yang et al., [2018](#bib.bib71)) datasets
    or using Paul Graham essays.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分析了模型在BABILong基准测试上的表现如何与MMLU (Hendrycks et al., [2020](#bib.bib27))和RULER (Hsieh
    et al., [2024](#bib.bib29))不同。MMLU基准测试衡量了LLMs中各种知识领域，而RULER作为一个新提出的长上下文基准，与BABILong共享类似的“针在干草堆中”的概念。一个显著的不同是RULER的“针”（如形容词、名词、数字、uuid）和长“干草堆”上下文更加合成，由随机重复的句子组成，除了基于SQuAD (Rajpurkar
    et al., [2016](#bib.bib51))和HotPotQA (Yang et al., [2018](#bib.bib71))数据集的任务或使用Paul
    Graham的文章。
- en: 'We collect results from multiple models on MMLU, BABILong, and RULER at lengths
    ranging from 0K (BABILong without texts from PG19) to 128K tokens. In the upper-left
    part of Figure [4](#S3.F4 "Figure 4 ‣ 3.4 BABILong and Other Benchmarks ‣ 3 Benchmarking
    Results ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"),
    we show the correlation between scores on BABILong and RULER for different task
    lengths with those on the MMLU benchmark. At shorter lengths, BABILong exhibits
    a high correlation with MMLU, which diminishes as the length increases. Conversely,
    RULER shows a nearly constant correlation with MMLU, regardless of the length.
    The best correlated RULER lengths with MMLU are 64K and the average of all lengths
    (<=128K). In contrast, the highest correlation of BABILong scores with MMLU is
    at length 0K, which is expected since MMLU is a relatively short benchmark with
    examples up to 1K tokens. Comparing the correlations of BABILong with MMLU at
    the most correlated RULER lengths (<=128K and 64K) shows much lower values: $0.928$,
    respectively.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了多个模型在MMLU、BABILong和RULER上的结果，长度范围从0K（BABILong没有PG19文本）到128K tokens。在图[4](#S3.F4
    "图4 ‣ 3.4 BABILong与其他基准 ‣ 3 基准测试结果 ‣ BABILong：测试LLMs在长上下文推理中的极限")的左上部分，我们展示了不同任务长度下BABILong和RULER的分数与MMLU基准测试的相关性。在较短的长度下，BABILong与MMLU的相关性很高，但随着长度的增加相关性降低。相反，RULER与MMLU的相关性几乎保持不变，与MMLU相关性最好的RULER长度是64K和所有长度（<=128K）的平均值。相比之下，BABILong与MMLU的最高相关性出现在长度为0K时，这是因为MMLU是一个相对较短的基准测试，样本最多为1K
    tokens。将BABILong与MMLU在最相关的RULER长度（<=128K和64K）下的相关性进行比较，结果要低得多：$0.928$。
- en: These results show that BABILong can detect differences in models behavior starting
    from lengths as small as 2K tokens, while RULER requires lengths of at least 128K
    tokens to show significant differentiation from relatively short MMLU benchmark.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，BABILong可以从2K tokens的小长度开始检测模型行为的差异，而RULER则需要至少128K tokens的长度才能显示出与相对较短的MMLU基准测试的显著差异。
- en: 4 Related Work on Long Context Benchmarks and Datasets
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 关于长上下文基准测试和数据集的相关工作
- en: Long Range Arena (LRA) (Tay et al., [2021](#bib.bib63)) was a one of the pioneering
    benchmarks for long context modeling. LRA is a set of tasks with lengths from
    1 to 16 thousand tokens. However, it mainly consists of very specific tasks such
    as ListOps (2k tokens), Byte-Level Text Classification (4k tokens) and Byte-Level
    Text Retrieval (8k tokens), and others that are less related to NLP. They are
    not well suited for evaluating of modern LLMs without fine-tuning on these tasks
    and cannot fully represent the capabilites of LLMs that can handle 100k+ tokens.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Long Range Arena (LRA) (Tay et al., [2021](#bib.bib63)) 是长上下文建模的开创性基准测试之一。LRA
    是一组长度从 1 到 16 千令牌的任务。然而，它主要由一些非常具体的任务组成，如 ListOps (2k 令牌)、字节级文本分类 (4k 令牌) 和字节级文本检索
    (8k 令牌) 等，与自然语言处理（NLP）关系不大。这些任务不适合评估现代 LLM，除非对这些任务进行微调，否则无法完全代表能够处理 100k+ 令牌的
    LLM 的能力。
- en: A new set of datasets and benchmarks specifically designed to test the ability
    of LLMs to handle long contexts has been proposed. The LongBench dataset (Bai
    et al., [2023](#bib.bib6)) contains 6 types of real and synthetic problems, ranging
    from summarization and multidoc QA to code completion. The average sample lengths
    in LongBench are 6k and 13k tokens for English and Chinese respectively, with
    40k tokens at max. Scrolls and ZeroSCROLLS (Shaham et al., [2022](#bib.bib56),
    [2023](#bib.bib57)) consist of QA, classification, summarization tasks and have
    higher average lengths ranging from 1.7k to 49.3k tokens. L-Eval (An et al., [2023](#bib.bib4))
    mostly combines 20 smaller long sequence datasets and adds 4 newly annotated tasks,
    with query-response pairs encompassing diverse question styles and domains. The
    average length of examples for L-Eval varies from 3 to 60 thousand tokens. Some
    of the benchmarks are focusing on evaluation of in-context learning and instruction
    following, such as LongAlign and LongBench-chat (Bai et al., [2024](#bib.bib7)),
    ZeroScrolls, LongICLBench (Li et al., [2024](#bib.bib39)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一组新的数据集和基准测试专门设计用于测试大型语言模型（LLMs）处理长上下文的能力已经被提出。LongBench 数据集 (Bai et al., [2023](#bib.bib6))
    包含 6 种真实和合成的问题类型，从摘要和多文档问答到代码补全。LongBench 的平均样本长度为英语 6k 和中文 13k 令牌，最大长度为 40k 令牌。Scrolls
    和 ZeroSCROLLS (Shaham et al., [2022](#bib.bib56), [2023](#bib.bib57)) 包括问答、分类、摘要任务，具有更高的平均长度，范围从
    1.7k 到 49.3k 令牌。L-Eval (An et al., [2023](#bib.bib4)) 主要结合了 20 个较小的长序列数据集，并添加了
    4 个新标注的任务，查询-响应对涵盖了多样的问题风格和领域。L-Eval 的示例平均长度从 3 到 60 千令牌不等。一些基准测试专注于评估上下文学习和指令跟随，如
    LongAlign 和 LongBench-chat (Bai et al., [2024](#bib.bib7))、ZeroScrolls、LongICLBench (Li
    et al., [2024](#bib.bib39))。
- en: There are other long-context datasets that primarily consist of QA and summarization
    tasks over texts from Wiki, arXiv, novels, movie and TV scripts, or other sources,
    e.g., InfinityBench (Zhang et al., [2024b](#bib.bib77)), Loogle (Li et al., [2023b](#bib.bib38)),
    Bamboo (Dong et al., [2023](#bib.bib18)), LVEval (Yuan et al., [2024](#bib.bib73)),
    NovelQA (Wang et al., [2024b](#bib.bib66)), Marathon (Zhang et al., [2023](#bib.bib75)),
    XL²-Bench (Ni et al., [2024](#bib.bib44)), DocFinQA (Reddy et al., [2024](#bib.bib52)),
    or ChapterBreak (Sun et al., [2022](#bib.bib62)), Ada-LEval (Wang et al., [2024a](#bib.bib65))
    that evaluate operations with text chunks, or move to multimodal tasks in MileBench (Song
    et al., [2024a](#bib.bib59)). These datasets vary in length, with maximum sample
    lengths of 636K tokens in ChapterBreak and average lengths reaching 200K tokens
    in InfinityBench, NovelQA, LVEval, and some subsets of XL²-Bench.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他主要由问答和摘要任务组成的长上下文数据集，这些任务来源于 Wiki、arXiv、小说、电影和电视脚本或其他来源，例如 InfinityBench (Zhang
    et al., [2024b](#bib.bib77))、Loogle (Li et al., [2023b](#bib.bib38))、Bamboo (Dong
    et al., [2023](#bib.bib18))、LVEval (Yuan et al., [2024](#bib.bib73))、NovelQA (Wang
    et al., [2024b](#bib.bib66))、Marathon (Zhang et al., [2023](#bib.bib75))、XL²-Bench (Ni
    et al., [2024](#bib.bib44))、DocFinQA (Reddy et al., [2024](#bib.bib52)) 或 ChapterBreak (Sun
    et al., [2022](#bib.bib62))、Ada-LEval (Wang et al., [2024a](#bib.bib65))，这些数据集评估文本块操作，或者转向
    MileBench (Song et al., [2024a](#bib.bib59)) 中的多模态任务。这些数据集的长度各异，ChapterBreak 的最大样本长度为
    636K 令牌，而 InfinityBench、NovelQA、LVEval 和 XL²-Bench 的某些子集的平均长度达到 200K 令牌。
- en: To further extend benchmarks lengths with real and human annotated data is very
    challenging. Therefore, "needle-in-a-haystack" inspired benchmarks arised. Following
    LLMTest ⁵⁵5[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
    with magic numbers as needles in Paul Graham essays as haystack, passkey and key-value
    retrieval tasks are part of InfinityBench (Zhang et al., [2024b](#bib.bib77)).
    Counting-Stars (Song et al., [2024b](#bib.bib60)) suggests to insert multiple
    sentences about little penguins that count stars into the same essays for English
    or The Story of the Stone for the Chinese language. The task is to answer questions
    based on these "needle" sentences. RULER (Hsieh et al., [2024](#bib.bib29)) extends
    "needle-in-a-haystack" with multiple types and amount of "needles". RULER and
    Counting-Start introduce new task categories such as multi-hop tracing and aggregation
    to test models beyond searching from context.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步扩展基准测试长度，使用真实和人工标注的数据是非常具有挑战性的。因此，受“针在干草堆中”启发的基准测试应运而生。跟随LLMTest ⁵⁵5[https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)，以保罗·格雷厄姆的文章中的干草堆作为“针”的魔法数字为例，InfinityBench （张等，[2024b](#bib.bib77)）包括了密钥和键值检索任务。Counting-Stars （宋等，[2024b](#bib.bib60)）建议在相同的文章中插入关于小企鹅数星星的多句话，以英语或《石头记》为中文语言。任务是基于这些“针”句子回答问题。RULER （谢等，[2024](#bib.bib29)）通过引入多种类型和数量的“针”来扩展“针在干草堆中”的概念。RULER和Counting-Start引入了新的任务类别，如多跳追踪和聚合，以测试模型超越从上下文中搜索的能力。
- en: Some benchmarks have pre-defined length bins, such as LongBench (0-4k, 4k-8k,
    8k+), Ada-LEval (2k-128k), LVEval (16k, 32k, 64k, 128k, 256k), Bamboo (4k, 16k),
    S3Eval (2k, 4k, 8k, 16k) (Lei et al., [2023](#bib.bib35)). A number of benchmarks,
    including RULER, CountingStars, Ada-LEval, and S3Eval, can be generated at required
    lengths. All mentioned datasets are mostly in English with some of them covering
    Chinese language (LongBench, InfinityBench, LongAlign, Counting Stars, CLongEval (Qiu
    et al., [2024](#bib.bib48)), LV-Eval, XL²-Bench).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基准测试具有预定义的长度区间，例如LongBench（0-4k，4k-8k，8k+），Ada-LEval（2k-128k），LVEval（16k，32k，64k，128k，256k），Bamboo（4k，16k），S3Eval（2k，4k，8k，16k） （Lei等，[2023](#bib.bib35)）。包括RULER、CountingStars、Ada-LEval和S3Eval在内的一些基准测试可以生成所需的长度。所有提到的数据集主要是英语，其中一些涵盖了中文（LongBench、InfinityBench、LongAlign、Counting
    Stars、CLongEval （邱等，[2024](#bib.bib48)）、LV-Eval、XL²-Bench）。
- en: BABILong focuses on natural language reasoning over multiple facts distributed
    in very large textual corpora. Compared to existing approaches it provides more
    tasks and more natural and deceptive mixing of information into background documents.
    BABILong consists of diverse set of 20 tasks that cover different capabilities
    including multi-hop tracing, aggregation over needles and extending them with
    basic deduction and induction, time, positional, and size reasoning, and path
    finding. The benchmark goes with predefined splits up to unprecedented 1M token
    length. Lengths beyond 1M tokens could be generated and we test models up to 11M
    tokens. Furthermore, while we evaluate models on English-only tasks from bAbI (Weston
    et al., [2016](#bib.bib68)) adding new languages is straightforward.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong专注于在非常大的文本语料库中进行自然语言推理。与现有的方法相比，它提供了更多的任务，并且将信息更自然和具有欺骗性地混合到背景文档中。BABILong包括20个不同任务的多样化集合，涵盖了包括多跳追踪、在针上聚合以及使用基本推理和归纳、时间、位置、大小推理和路径寻找的不同能力。该基准测试具有预定义的划分，长度达到前所未有的1M标记。超过1M标记的长度可以生成，我们测试了最多11M标记的模型。此外，虽然我们在bAbI （Weston等，[2016](#bib.bib68)）上评估了仅英语的模型，添加新语言则很简单。
- en: Conclusions
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this work, we introduced the BABILong, a diverse and scalable benchmark designed
    to bridge the gap in evaluating large language models (LLMs) across extensive
    context lengths. Our experiments demonstrate that BABILong offers a more representative
    evaluation framework for long-context reasoning among the existing benchmarks.
    The analysis of correlation with other benchmarks further validates BABILong’s
    ability to pose a significant challenge for large language models to maintain
    performance as context lengths scale up. The BABILong benchmark offers algorithmically
    adaptable document length and facts placement, includes predefined sets of bins
    ranging from 0k to 1M tokens. Facts in BABILong could be generated making it leak-proof
    for future LLMs. It consists of a set of 20 diverse tasks covering reasoning tasks,
    including fact chaining, simple induction, deduction, counting, and handling lists/sets.
    Compared to other benchmarks, BABILong shows high correlation on short contexts
    with MMLU and diverges from it as lengths increases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了BABILong，一个多样且可扩展的基准，旨在填补评估大型语言模型（LLMs）在广泛上下文长度上的空白。我们的实验表明，BABILong提供了一个更具代表性的评估框架，用于现有基准中长上下文推理的评估。与其他基准的相关性分析进一步验证了BABILong在上下文长度扩展时给大型语言模型带来的显著挑战。BABILong基准提供了算法上可调整的文档长度和事实位置，包括从0k到1M
    tokens的预定义桶集。BABILong中的事实可以生成，使其对未来的LLMs具有防泄漏性。它由一组20个多样化的任务组成，涵盖推理任务，包括事实链条、简单归纳、演绎、计数和处理列表/集合。与其他基准相比，BABILong在短上下文上与MMLU显示出高度相关性，并且随着长度的增加而逐渐分歧。
- en: Our findings reveal limitations of popular open-source LLMs as well as GPT-4
    and RAG regarding effective long context utilization. Their performance heavily
    relies on the first 5-25 % of the input, highlighting the need for improved context
    processing mechanisms. Fine-tuning experiments show that tasks from BABILong are
    solvable even by relatively small models like RMT with GPT-2 (137M) and Mamba
    (130M). Fine-tuning improves the performance of GPT-3.5-Turbo and Mistral-7B,
    but their context lengths remain limited to 16K and 32K, respectively. Among the
    evaluated models, Mamba (130M) and RMT(137M) achieve the strongest results. However,
    Mamba is hard to infer on lengths more than 128K tokens, while RMT enables the
    processing of lengths up to 11 million tokens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究揭示了流行的开源LLMs（大语言模型）、GPT-4和RAG在有效利用长上下文方面的局限性。它们的性能严重依赖于输入的前5-25%，这突显了改进上下文处理机制的必要性。微调实验表明，BABILong中的任务即使是像GPT-2（137M）和Mamba（130M）这样的相对较小的模型也能解决。微调提高了GPT-3.5-Turbo和Mistral-7B的性能，但它们的上下文长度仍然限制在16K和32K。评估模型中，Mamba（130M）和RMT（137M）表现最强。然而，Mamba在处理超过128K
    tokens的长度时很困难，而RMT则可以处理长达1100万tokens的长度。
- en: Limitations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The BABILong benchmark uses background texts to hide facts in them. In our experiments,
    we only tried PG19 and Wiki as background text sources. Other background texts
    may have a different effect on the results. PG19 and Wiki were chosen because
    they contain natural narratives and facts about people, in a way similar to bAbI
    tasks. Interference between similar facts in the background text can make the
    benchmark even more difficult.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: BABILong基准使用背景文本来隐藏其中的事实。在我们的实验中，我们仅尝试了PG19和Wiki作为背景文本来源。其他背景文本可能对结果有不同的影响。选择PG19和Wiki是因为它们包含自然叙事和有关人物的事实，类似于bAbI任务。背景文本中相似事实的干扰可能使基准测试变得更加困难。
- en: 'In GPT-4 and LLama-3 with RAG experiments, we do not optimize the retrieval
    component. We tried several prompts experiments with LLMs, but the ones that we
    selected could be suboptimal. We provide them in Appendix [J](#A10 "Appendix J
    Prompts Used to Benchmark GPT-4-Turbo and Mistral Models ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") and in our GitHub repository.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在GPT-4和LLama-3与RAG的实验中，我们没有优化检索组件。我们尝试了几种与LLMs的提示实验，但我们选择的提示可能不是最优的。我们将这些提示提供在附录[J](#A10
    "附录 J 用于基准测试 GPT-4-Turbo 和 Mistral 模型的提示 ‣ BABILong: 测试 LLMs 在长上下文推理中的极限")以及我们的GitHub仓库中。'
- en: The current version of the dataset reuses parameters for fact generation from
    the bAbI (Weston et al., [2016](#bib.bib68)). As the initial work used vocabularies
    of limited size, this results in a low variety of names and objects within the
    facts. This limitation makes the BABILong tasks easier for fine-tuned models,
    as they can quickly learn specific tokens that differentiate facts from background
    text. This issue is partially mitigated by generating distractor facts using the
    same vocabularies. Enhancing the dataset’s vocabulary in future versions could
    easily address this limitation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的数据集重新使用了bAbI（Weston等人，[2016](#bib.bib68)）的事实生成参数。由于初始工作使用了有限大小的词汇，这导致事实中的名字和对象种类较少。这一限制使得BABILong任务对于微调模型来说更容易，因为它们可以迅速学习区分事实和背景文本的特定标记。通过使用相同的词汇生成干扰事实，部分缓解了这个问题。未来版本中增强数据集的词汇量可以轻松解决这一限制。
- en: Although recurrent approaches, like RMT, are hindered by their sequential nature,
    resulting in reduced parallelizability, they compensate by constant memory requirements,
    but it is also their limitation as storage capacity of the model is finite. However,
    BABILong is solvable by this type of models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像RMT这样的递归方法受到其顺序性质的限制，导致并行性降低，但它们通过持续的内存需求来补偿这一点，但这也是它们的局限性，因为模型的存储容量是有限的。然而，BABILong可以通过这类模型解决。
- en: References
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdin et al. (2024) Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah,
    A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et al. Phi-3
    technical report: A highly capable language model locally on your phone. *arXiv
    preprint arXiv:2404.14219*, 2024.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdin等人（2024）Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A.,
    Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., 等人。Phi-3技术报告：一种在手机上本地运行的高能力语言模型。*arXiv预印本
    arXiv:2404.14219*，2024年。
- en: Agarwal et al. (2024) Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Chan,
    S., Anand, A., Abbas, Z., Nova, A., Co-Reyes, J. D., Chu, E., et al. Many-shot
    in-context learning. *arXiv preprint arXiv:2404.11018*, 2024.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等人（2024）Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Chan, S., Anand,
    A., Abbas, Z., Nova, A., Co-Reyes, J. D., Chu, E., 等人。多次上下文学习。*arXiv预印本 arXiv:2404.11018*，2024年。
- en: AI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta（2024）AI@Meta. Llama 3模型卡。2024年。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'An et al. (2023) An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L.,
    and Qiu, X. L-eval: Instituting standardized evaluation for long context language
    models. *arXiv preprint arXiv:2307.11088*, 2023.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An等人（2023）An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., 和 Qiu,
    X. L-eval: 为长上下文语言模型建立标准化评估。*arXiv预印本 arXiv:2307.11088*，2023年。'
- en: Anthropic (2024) Anthropic. Introducing the next generation of claude, 2024.
    URL [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2024）Anthropic. 介绍下一代Claude，2024年。网址 [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)。
- en: 'Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,
    Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark
    for long context understanding. *arXiv preprint arXiv:2308.14508*, 2023.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2023）Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z.,
    Liu, X., Zeng, A., Hou, L., 等人。Longbench: 一种用于长上下文理解的双语多任务基准。*arXiv预印本 arXiv:2308.14508*，2023年。'
- en: 'Bai et al. (2024) Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang,
    J., Dong, Y., and Li, J. Longalign: A recipe for long context alignment of large
    language models. *arXiv preprint arXiv:2401.18058*, 2024.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2024）Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong,
    Y., 和 Li, J. Longalign: 一种用于大语言模型长上下文对齐的方案。*arXiv预印本 arXiv:2401.18058*，2024年。'
- en: 'Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy等人（2020）Beltagy, I., Peters, M. E., 和 Cohan, A. Longformer: 长文档变换器。*arXiv预印本
    arXiv:2004.05150*，2020年。'
- en: Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark,
    A., et al. Improving language models by retrieving from trillions of tokens. In
    *International conference on machine learning*, pp.  2206–2240\. PMLR, 2022.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud等人（2022）Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
    E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark,
    A., 等人。通过从数万亿个标记中检索来改善语言模型。发表于*国际机器学习会议*，第2206–2240页。PMLR，2022年。
- en: Bulatov et al. (2022) Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory
    transformer. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
    Oh, A. (eds.), *Advances in Neural Information Processing Systems*, volume 35,
    pp.  11079–11091\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulatov 等（2022）Bulatov, A., Kuratov, Y., 和 Burtsev, M. 循环记忆变换器。在 Koyejo, S.,
    Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., 和 Oh, A.（编），*神经信息处理系统进展*，第35卷，第11079–11091页。Curran
    Associates, Inc.，2022年。网址 [https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf)。
- en: 'Bulatov et al. (2024) Bulatov, A., Kuratov, Y., Kapushev, Y., and Burtsev,
    M. Beyond attention: Breaking the limits of transformer context length with recurrent
    memory. *Proceedings of the AAAI Conference on Artificial Intelligence*, 38(16):17700–17708,
    Mar. 2024. doi: 10.1609/aaai.v38i16.29722. URL [https://ojs.aaai.org/index.php/AAAI/article/view/29722](https://ojs.aaai.org/index.php/AAAI/article/view/29722).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bulatov 等（2024）Bulatov, A., Kuratov, Y., Kapushev, Y., 和 Burtsev, M. 超越注意力：利用循环记忆突破变换器上下文长度的限制。*AAAI人工智能会议论文集*，38(16):17700–17708，2024年3月。doi:
    10.1609/aaai.v38i16.29722。网址 [https://ojs.aaai.org/index.php/AAAI/article/view/29722](https://ojs.aaai.org/index.php/AAAI/article/view/29722)。'
- en: Chase (2022) Chase, H. LangChain, October 2022. URL [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase（2022）Chase, H. LangChain，2022年10月。网址 [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)。
- en: 'Chen et al. (2023) Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S.,
    and Jia, J. Longlora: Efficient fine-tuning of long-context large language models.
    In *The Twelfth International Conference on Learning Representations*, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., 和 Jia,
    J. Longlora：高效的长上下文大语言模型微调。在*第十二届国际学习表征会议*，2023年。
- en: 'Chevalier et al. (2023) Chevalier, A., Wettig, A., Ajith, A., and Chen, D.
    Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali,
    K. (eds.), *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pp.  3829–3846, Singapore, December 2023\. Association for
    Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chevalier 等（2023）Chevalier, A., Wettig, A., Ajith, A., 和 Chen, D. 适应语言模型以压缩上下文。在
    Bouamor, H., Pino, J., 和 Bali, K.（编），*2023年自然语言处理实证方法会议论文集*，第3829–3846页，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.232。网址 [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232)。'
- en: 'Cohere (2024) Cohere. Command r: Retrieval-augmented generation at production
    scale, March 2024. URL [https://cohere.com/blog/command-r](https://cohere.com/blog/command-r).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cohere（2024）Cohere. Command r: 在生产规模上进行检索增强生成，2024年3月。网址 [https://cohere.com/blog/command-r](https://cohere.com/blog/command-r)。'
- en: 'Didolkar et al. (2022) Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B.,
    Lamb, A. M., Ke, N. R., and Bengio, Y. Temporal latent bottleneck: Synthesis of
    fast and slow processing mechanisms in sequence learning. *Advances in Neural
    Information Processing Systems*, 35:10505–10520, 2022.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Didolkar 等（2022）Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B., Lamb,
    A. M., Ke, N. R., 和 Bengio, Y. 时间潜在瓶颈：序列学习中快速与慢速处理机制的综合。*神经信息处理系统进展*，35:10505–10520，2022年。
- en: 'Ding et al. (2023) Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang,
    W., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. *arXiv
    preprint arXiv:2307.02486*, 2023.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2023）Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., 和 Wei,
    F. Longnet：将变换器扩展到 1,000,000,000 个标记。*arXiv预印本 arXiv:2307.02486*，2023年。
- en: 'Dong et al. (2023) Dong, Z., Tang, T., Li, J., Zhao, W. X., and Wen, J.-R.
    Bamboo: A comprehensive benchmark for evaluating long text modeling capacities
    of large language models. *arXiv preprint arXiv:2309.13345*, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Dong, Z., Tang, T., Li, J., Zhao, W. X., 和 Wen, J.-R. Bamboo：评估大语言模型长文本建模能力的综合基准。*arXiv预印本
    arXiv:2309.13345*，2023年。
- en: Douze et al. (2024) Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy,
    G., Mazaré, P.-E., Lomeli, M., Hosseini, L., and Jégou, H. The faiss library.
    2024.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douze 等（2024）Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré,
    P.-E., Lomeli, M., Hosseini, L., 和 Jégou, H. faiss 库。2024年。
- en: 'Du et al. (2022) Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and
    Tang, J. Glm: General language model pretraining with autoregressive blank infilling.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  320–335, 2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等（2022）Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., 和 Tang, J.
    Glm: 自回归空白填充的通用语言模型预训练。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第320–335页，2022年。'
- en: Fan et al. (2020) Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar,
    S. Addressing some limitations of transformers with feedback memory. *arXiv preprint
    arXiv:2002.09402*, 2020.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2020）Fan, A., Lavril, T., Grave, E., Joulin, A., 和 Sukhbaatar, S. 通过反馈记忆解决变换器的一些局限性。*arXiv
    预印本 arXiv:2002.09402*，2020年。
- en: Gebru et al. (2021) Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W.,
    Wallach, H., Iii, H. D., and Crawford, K. Datasheets for datasets. *Communications
    of the ACM*, 64(12):86–92, 2021.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gebru 等（2021）Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach,
    H., Iii, H. D., 和 Crawford, K. 数据集说明书。*ACM 通讯*，64(12):86–92，2021年。
- en: Graves et al. (2014) Graves, A., Wayne, G., and Danihelka, I. Neural turing
    machines. *arXiv preprint arXiv:1410.5401*, 2014.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 等（2014）Graves, A., Wayne, G., 和 Danihelka, I. 神经图灵机。*arXiv 预印本 arXiv:1410.5401*，2014年。
- en: 'Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with
    selective state spaces. *arXiv preprint arXiv:2312.00752*, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu & Dao（2023）Gu, A. 和 Dao, T. Mamba: 具有选择性状态空间的线性时间序列建模。*arXiv 预印本 arXiv:2312.00752*，2023年。'
- en: Gu et al. (2021) Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences
    with structured state spaces. In *International Conference on Learning Representations*,
    2021.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2021）Gu, A., Goel, K., 和 Re, C. 高效建模长序列与结构化状态空间。在*国际学习表征会议*，2021年。
- en: Guu et al. (2020) Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval
    augmented language model pre-training. In *International conference on machine
    learning*, pp.  3929–3938\. PMLR, 2020.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等（2020）Guu, K., Lee, K., Tung, Z., Pasupat, P., 和 Chang, M. 检索增强语言模型预训练。在*国际机器学习会议*，第3929–3938页。PMLR，2020年。
- en: Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2020.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2020）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 测量大规模多任务语言理解。在*国际学习表征会议*，2020年。
- en: 'Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term
    memory. *Neural Comput.*, 9(8):1735–1780, November 1997. ISSN 0899-7667. doi:
    10.1162/neco.1997.9.8.1735. URL [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hochreiter & Schmidhuber（1997）Hochreiter, S. 和 Schmidhuber, J. 长短期记忆。*神经计算*，9(8):1735–1780，1997年11月。ISSN
    0899-7667。doi: 10.1162/neco.1997.9.8.1735。网址 [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)。'
- en: 'Hsieh et al. (2024) Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh,
    D., Jia, F., and Ginsburg, B. Ruler: What’s the real context size of your long-context
    language models? *arXiv preprint arXiv:2404.06654*, 2024.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh 等（2024）Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia,
    F., 和 Ginsburg, B. Ruler: 你长期上下文语言模型的真实上下文大小是多少？*arXiv 预印本 arXiv:2404.06654*，2024年。'
- en: Hutchins et al. (2022) Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur,
    B. Block-recurrent transformers. In Oh, A. H., Agarwal, A., Belgrave, D., and
    Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL
    [https://openreview.net/forum?id=uloenYmLCAo](https://openreview.net/forum?id=uloenYmLCAo).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutchins 等（2022）Hutchins, D., Schlag, I., Wu, Y., Dyer, E., 和 Neyshabur, B.
    块递归变换器。在 Oh, A. H., Agarwal, A., Belgrave, D., 和 Cho, K.（编辑），*神经信息处理系统进展*，2022年。网址
    [https://openreview.net/forum?id=uloenYmLCAo](https://openreview.net/forum?id=uloenYmLCAo)。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等。Mistral
    7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F.,
    et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2024）Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B.,
    Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., 等。Mixtral
    of experts。*arXiv 预印本 arXiv:2401.04088*，2024年。
- en: 'Khandelwal et al. (2019) Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,
    L., and Lewis, M. Generalization through memorization: Nearest neighbor language
    models. In *International Conference on Learning Representations*, 2019.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khandelwal 等（2019）Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., 和
    Lewis, M. 通过记忆实现泛化：最近邻语言模型。在 *国际学习表示会议*，2019。
- en: Lee et al. (2024) Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum,
    O., and Brunskill, E. Supervised pretraining can learn in-context reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36, 2024.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2024）Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O.,
    和 Brunskill, E. 监督预训练可以在上下文中学习强化学习。*神经信息处理系统进展*，36，2024。
- en: 'Lei et al. (2023) Lei, F., Liu, Q., Huang, Y., He, S., Zhao, J., and Liu, K.
    S3eval: A synthetic, scalable, systematic evaluation suite for large language
    models. *arXiv preprint arXiv:2310.15147*, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei 等（2023）Lei, F., Liu, Q., Huang, Y., He, S., Zhao, J., 和 Liu, K. S3eval:
    一套用于大型语言模型的合成、可扩展、系统化评估工具。*arXiv 预印本 arXiv:2310.15147*，2023。'
- en: 'Lei et al. (2020) Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T. L., and Bansal,
    M. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning,
    2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei 等（2020）Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T. L., 和 Bansal, M. Mart:
    用于连贯视频段落描述的记忆增强递归变换器，2020。'
- en: Li et al. (2023a) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J., Stoica, I., Ma, X., and Zhang, H. How long can context length of open-source
    llms truly promise? In *NeurIPS 2023 Workshop on Instruction Tuning and Instruction
    Following*, 2023a.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023a）Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica,
    I., Ma, X., 和 Zhang, H. 开源 LLM 的上下文长度到底能承诺多长？在 *NeurIPS 2023 指令调整与指令跟随研讨会*，2023a。
- en: 'Li et al. (2023b) Li, J., Wang, M., Zheng, Z., and Zhang, M. Loogle: Can long-context
    language models understand long contexts? *arXiv preprint arXiv:2311.04939*, 2023b.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023b）Li, J., Wang, M., Zheng, Z., 和 Zhang, M. Loogle: 长上下文语言模型能否理解长上下文？*arXiv
    预印本 arXiv:2311.04939*，2023b。'
- en: Li et al. (2024) Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long-context
    llms struggle with long in-context learning. *arXiv preprint arXiv:2404.02060*,
    2024.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2024）Li, T., Zhang, G., Do, Q. D., Yue, X., 和 Chen, W. 长上下文的LLMs在长时间上下文学习中挣扎。*arXiv
    预印本 arXiv:2404.02060*，2024。
- en: 'Lieber et al. (2024) Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos,
    I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S., et al. Jamba: A
    hybrid transformer-mamba language model. *arXiv preprint arXiv:2403.19887*, 2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lieber 等（2024）Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos,
    I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S., 等. Jamba: 一种混合的变换器-曼巴语言模型。*arXiv
    预印本 arXiv:2403.19887*，2024。'
- en: Liu et al. (2024a) Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model
    on million-length video and language with blockwise ringattention. *arXiv preprint
    arXiv:2402.08268*, 2024a.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024a）Liu, H., Yan, W., Zaharia, M., 和 Abbeel, P. 世界模型在百万长度的视频和语言中使用块状环形注意力。*arXiv
    预印本 arXiv:2402.08268*，2024a。
- en: 'Liu et al. (2024b) Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro,
    B. Chatqa: Building gpt-4 level conversational qa models. *arXiv preprint arXiv:2401.10225*,
    2024b.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2024b）Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., 和 Catanzaro, B.
    Chatqa: 构建 GPT-4 级别的对话问答模型。*arXiv 预印本 arXiv:2401.10225*，2024b。'
- en: Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay
    regularization. In *International Conference on Learning Representations*, 2019.
    URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter（2019）Loshchilov, I. 和 Hutter, F. 解耦权重衰减正则化。在 *国际学习表示会议*，2019。URL
    [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Ni et al. (2024) Ni, X., Cai, H., Wei, X., Wang, S., Yin, D., and Li, P. XL²
    Bench: A benchmark for extremely long context understanding with long-range dependencies.
    *arXiv preprint arXiv:2404.05446*, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni 等（2024）Ni, X., Cai, H., Wei, X., Wang, S., Yin, D., 和 Li, P. XL² Bench:
    一个用于理解极长上下文和长距离依赖的基准测试。*arXiv 预印本 arXiv:2404.05446*，2024。'
- en: OpenAI (2023) OpenAI. New models and developer products announced at devday,
    2023. URL [https://openai.com/index/new-models-and-developer-products-announced-at-devday/](https://openai.com/index/new-models-and-developer-products-announced-at-devday/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 在 devday 2023 上宣布的新模型和开发者产品。URL [https://openai.com/index/new-models-and-developer-products-announced-at-devday/](https://openai.com/index/new-models-and-developer-products-announced-at-devday/)。
- en: 'Peng et al. (2023a) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,
    S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing
    rnns for the transformer era. *arXiv preprint arXiv:2305.13048*, 2023a.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023a）Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S.,
    Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., 等。Rwkv：为变换器时代重新定义 RNNs。*arXiv
    预印本 arXiv:2305.13048*，2023a。
- en: 'Peng et al. (2023b) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023b.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023b）Peng, B., Quesnelle, J., Fan, H., 和 Shippole, E. Yarn：大语言模型的高效上下文窗口扩展。在*第十二届国际学习表示会议*，2023b。
- en: 'Qiu et al. (2024) Qiu, Z., Li, J., Huang, S., Zhong, W., and King, I. Clongeval:
    A chinese benchmark for evaluating long-context large language models. *arXiv
    preprint arXiv:2403.03514*, 2024.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等（2024）Qiu, Z., Li, J., Huang, S., Zhong, W., 和 King, I. Clongeval：评估长上下文大型语言模型的中文基准。*arXiv
    预印本 arXiv:2403.03514*，2024。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    and Sutskever, I. Language models are unsupervised multitask learners. 2019.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., 和 Sutskever,
    I. 语言模型是无监督的多任务学习者。2019。
- en: Rae et al. (2020) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
    and Lillicrap, T. P. Compressive transformers for long-range sequence modelling.
    In *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=SylKikSYDH](https://openreview.net/forum?id=SylKikSYDH).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2020）Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., 和 Lillicrap,
    T. P. 用于长距离序列建模的压缩变换器。在*国际学习表示会议*，2020。URL [https://openreview.net/forum?id=SylKikSYDH](https://openreview.net/forum?id=SylKikSYDH)。
- en: 'Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
    Squad: 100,000+ questions for machine comprehension of text. In *Proceedings of
    the 2016 Conference on Empirical Methods in Natural Language Processing*, pp. 
    2383–2392, 2016.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等（2016）Rajpurkar, P., Zhang, J., Lopyrev, K., 和 Liang, P. Squad：100,000+
    个问题用于机器理解文本。在*2016 年自然语言处理方法学会议论文集*，第 2383–2392 页，2016。
- en: 'Reddy et al. (2024) Reddy, V., Koncel-Kedziorski, R., Lai, V. D., and Tanner,
    C. Docfinqa: A long-context financial reasoning dataset. *arXiv preprint arXiv:2401.06915*,
    2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddy 等（2024）Reddy, V., Koncel-Kedziorski, R., Lai, V. D., 和 Tanner, C. Docfinqa：一个长上下文的金融推理数据集。*arXiv
    预印本 arXiv:2401.06915*，2024。
- en: 'Reid et al. (2024) Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap,
    T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J.,
    et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens
    of context. *arXiv preprint arXiv:2403.05530*, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid 等（2024）Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap,
    T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J.,
    等。Gemini 1.5：解锁跨越数百万令牌的多模态理解。*arXiv 预印本 arXiv:2403.05530*，2024。
- en: Rubin & Berant (2023) Rubin, O. and Berant, J. Long-range language modeling
    with self-retrieval. *arXiv preprint arXiv:2306.13421*, 2023.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin & Berant（2023）Rubin, O. 和 Berant, J. 通过自我检索进行长距离语言建模。*arXiv 预印本 arXiv:2306.13421*，2023。
- en: 'Sainz et al. (2023) Sainz, O., Campos, J., García-Ferrero, I., Etxaniz, J.,
    de Lacalle, O. L., and Agirre, E. NLP evaluation in trouble: On the need to measure
    LLM data contamination for each benchmark. In Bouamor, H., Pino, J., and Bali,
    K. (eds.), *Findings of the Association for Computational Linguistics: EMNLP 2023*,
    pp.  10776–10787, Singapore, December 2023\. Association for Computational Linguistics.
    doi: 10.18653/v1/2023.findings-emnlp.722. URL [https://aclanthology.org/2023.findings-emnlp.722](https://aclanthology.org/2023.findings-emnlp.722).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sainz 等（2023）Sainz, O., Campos, J., García-Ferrero, I., Etxaniz, J., de Lacalle,
    O. L., 和 Agirre, E. NLP 评估遇到麻烦：关于需要对每个基准测量 LLM 数据污染的问题。在 Bouamor, H., Pino, J.,
    和 Bali, K.（编辑），*计算语言学协会会议发现：EMNLP 2023*，第 10776–10787 页，新加坡，2023 年 12 月。计算语言学协会。doi:
    10.18653/v1/2023.findings-emnlp.722。URL [https://aclanthology.org/2023.findings-emnlp.722](https://aclanthology.org/2023.findings-emnlp.722)。'
- en: 'Shaham et al. (2022) Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O.,
    Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et al. Scrolls: Standardized
    comparison over long language sequences. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing*, pp.  12007–12021, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaham 等（2022）Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv,
    A., Gupta, A., Xiong, W., Geva, M., Berant, J., 等。Scrolls：对长语言序列的标准化比较。在*2022
    年自然语言处理方法学会议论文集*，第 12007–12021 页，2022。
- en: 'Shaham et al. (2023) Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy,
    O. Zeroscrolls: A zero-shot benchmark for long text understanding. *arXiv preprint
    arXiv:2305.14196*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham等人（2023）Shaham, U., Ivgi, M., Efrat, A., Berant, J., 和 Levy, O. Zeroscrolls:
    一种用于长文本理解的零样本基准。*arXiv预印本 arXiv:2305.14196*，2023。'
- en: 'Shi et al. (2023) Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis,
    M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language
    models. *arXiv preprint arXiv:2301.12652*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi等人（2023）Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer,
    L., 和 Yih, W.-t. Replug: 检索增强的黑箱语言模型。*arXiv预印本 arXiv:2301.12652*，2023。'
- en: 'Song et al. (2024a) Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., and Wang,
    B. Milebench: Benchmarking mllms in long context. *arXiv preprint arXiv:2404.18532*,
    2024a.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2024a）Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., 和 Wang, B. Milebench:
    长上下文中的MLLM基准。*arXiv预印本 arXiv:2404.18532*，2024a。'
- en: 'Song et al. (2024b) Song, M., Zheng, M., and Luo, X. Counting-stars: A multi-evidence,
    position-aware, and scalable benchmark for evaluating long-context large language
    models, 2024b.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2024b）Song, M., Zheng, M., 和 Luo, X. Counting-stars: 一种多证据、位置感知且可扩展的基准，用于评估长上下文大型语言模型，2024b。'
- en: 'Sorokin et al. (2022) Sorokin, A., Buzun, N., Pugachev, L., and Burtsev, M.
    Explain my surprise: Learning efficient long-term memory by predicting uncertain
    outcomes. *Advances in Neural Information Processing Systems*, 35:36875–36888,
    2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorokin等人（2022）Sorokin, A., Buzun, N., Pugachev, L., 和 Burtsev, M. 解释我的惊讶：通过预测不确定结果来学习高效的长期记忆。*神经信息处理系统进展*，35:36875–36888，2022年。
- en: 'Sun et al. (2022) Sun, S., Thai, K., and Iyyer, M. Chapterbreak: A challenge
    dataset for long-range language models. In *Proceedings of the 2022 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, pp.  3704–3714, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun等人（2022）Sun, S., Thai, K., 和 Iyyer, M. Chapterbreak: 长距离语言模型的挑战数据集。在*2022年北美计算语言学协会人类语言技术会议论文集*，第3704-3714页，2022年。'
- en: 'Tay et al. (2021) Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham,
    P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark
    for efficient transformers. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay等人（2021）Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P.,
    Rao, J., Yang, L., Ruder, S., 和 Metzler, D. 长距离竞技场：高效变压器的基准。在*国际学习表征会议*，2021年。网址
    [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k)。
- en: 'Voelker et al. (2019) Voelker, A., Kajić, I., and Eliasmith, C. Legendre memory
    units: Continuous-time representation in recurrent neural networks. *Advances
    in neural information processing systems*, 32, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voelker等人（2019）Voelker, A., Kajić, I., 和 Eliasmith, C. Legendre记忆单元：递归神经网络中的连续时间表征。*神经信息处理系统进展*，32，2019年。
- en: 'Wang et al. (2024a) Wang, C., Duan, H., Zhang, S., Lin, D., and Chen, K. Ada-leval:
    Evaluating long-context llms with length-adaptable benchmarks. *arXiv preprint
    arXiv:2404.06480*, 2024a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024a）Wang, C., Duan, H., Zhang, S., Lin, D., 和 Chen, K. Ada-leval:
    使用长度可调基准评估长上下文LLMs。*arXiv预印本 arXiv:2404.06480*，2024a。'
- en: 'Wang et al. (2024b) Wang, C., Ning, R., Pan, B., Wu, T., Guo, Q., Deng, C.,
    Bao, G., Wang, Q., and Zhang, Y. Novelqa: A benchmark for long-range novel question
    answering. *arXiv preprint arXiv:2403.12766*, 2024b.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024b）Wang, C., Ning, R., Pan, B., Wu, T., Guo, Q., Deng, C., Bao, G.,
    Wang, Q., 和 Zhang, Y. Novelqa: 长距离新颖问题回答的基准。*arXiv预印本 arXiv:2403.12766*，2024b。'
- en: 'Wang et al. (2024c) Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang,
    G., Wang, S., Chen, R., Xu, H., and Sun, H. Xl3m: A training-free framework for
    llm length extension based on segment-wise inference. *arXiv preprint arXiv:2405.17755*,
    2024c.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2024c）Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang, G., Wang,
    S., Chen, R., Xu, H., 和 Sun, H. Xl3m: 一种基于分段推理的无训练框架，用于LLM长度扩展。*arXiv预印本 arXiv:2405.17755*，2024c。'
- en: 'Weston et al. (2016) Weston, J., Bordes, A., Chopra, S., and Mikolov, T. Towards
    ai-complete question answering: A set of prerequisite toy tasks. In Bengio, Y.
    and LeCun, Y. (eds.), *4th International Conference on Learning Representations,
    ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings*,
    2016. URL [http://arxiv.org/abs/1502.05698](http://arxiv.org/abs/1502.05698).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston等人（2016）Weston, J., Bordes, A., Chopra, S., 和 Mikolov, T. 朝向ai-complete问题回答：一组先决的玩具任务。在Bengio,
    Y. 和 LeCun, Y.（主编），*第4届国际学习表征会议，ICLR 2016，圣胡安，波多黎各，2016年5月2-4日，会议记录*，2016年。网址
    [http://arxiv.org/abs/1502.05698](http://arxiv.org/abs/1502.05698)。
- en: 'Wu et al. (2022a) Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., and Yu,
    Z. Memformer: A memory-augmented transformer for sequence modeling. In *Findings
    of the Association for Computational Linguistics: AACL-IJCNLP 2022*, pp.  308–318,
    Online only, November 2022a. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-aacl.29](https://aclanthology.org/2022.findings-aacl.29).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2022a) Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., 和 Yu, Z. Memformer:
    一种用于序列建模的记忆增强变换器. 在 *计算语言学协会发现：AACL-IJCNLP 2022*，第 308–318 页，仅在线，2022a 年 11 月.
    计算语言学协会. 网址 [https://aclanthology.org/2022.findings-aacl.29](https://aclanthology.org/2022.findings-aacl.29).'
- en: Wu et al. (2022b) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing
    transformers. In *International Conference on Learning Representations*, 2022b.
    URL [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2022b) Wu, Y., Rabe, M. N., Hutchins, D., 和 Szegedy, C. 记忆变换器. 在 *国际学习表征会议*，2022b.
    网址 [https://openreview.net/forum?id=TrjbxzRcnf-](https://openreview.net/forum?id=TrjbxzRcnf-).
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pp.  2369–2380, 2018.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov,
    R., 和 Manning, C. D. Hotpotqa: 一个用于多跳问答的多样化、可解释数据集. 在 *2018年自然语言处理经验方法会议论文集*，第
    2369–2380 页，2018.'
- en: 'Young et al. (2024) Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang,
    G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: Open foundation models by
    01\. ai. *arXiv preprint arXiv:2403.04652*, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young 等人 (2024) Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G.,
    Li, H., Zhu, J., Chen, J., Chang, J., 等. Yi: 由 01\. ai 开放的基础模型. *arXiv 预印本 arXiv:2403.04652*，2024.'
- en: 'Yuan et al. (2024) Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang,
    M., Tan, Z., Yao, Z., Lin, D., Li, B., et al. Lv-eval: A balanced long-context
    benchmark with 5 length levels up to 256k. *arXiv preprint arXiv:2402.05136*,
    2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等人 (2024) Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang, M.,
    Tan, Z., Yao, Z., Lin, D., Li, B., 等. Lv-eval: 一个平衡的长上下文基准，有 5 个长度级别，最长达 256k.
    *arXiv 预印本 arXiv:2402.05136*，2024.'
- en: 'Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,
    Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed,
    A. Big bird: Transformers for longer sequences. In Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  17283–17297\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人 (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti,
    C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., 和 Ahmed, A. Big bird:
    适用于更长序列的变换器. 在 Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H.
    (编), *神经信息处理系统进展*，第 33 卷，第 17283–17297 页. Curran Associates, Inc., 2020. 网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf).'
- en: 'Zhang et al. (2023) Zhang, L., Li, Y., Liu, Z., Liu, J., Yang, M., et al. Marathon:
    A race through the realm of long context with large language models. *arXiv preprint
    arXiv:2312.09542*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2023) Zhang, L., Li, Y., Liu, Z., Liu, J., Yang, M., 等. Marathon:
    一场穿越长上下文领域的大语言模型竞赛. *arXiv 预印本 arXiv:2312.09542*，2023.'
- en: 'Zhang et al. (2024a) Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou,
    Z. Soaring from 4k to 400k: Extending llm’s context with activation beacon. *arXiv
    preprint arXiv:2401.03462*, 2024a.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2024a) Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., 和 Dou, Z.
    从 4k 到 400k 的飞跃: 用激活信标扩展 LLM 的上下文. *arXiv 预印本 arXiv:2401.03462*，2024a.'
- en: 'Zhang et al. (2024b) Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,
    Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. $\infty$ bench: Extending long
    context evaluation beyond 100k tokens. *arXiv preprint arXiv:2402.13718*, 2024b.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2024b) Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,
    Han, X., Thai, Z. L., Wang, S., Liu, Z., 等. $\infty$ bench: 将长上下文评估扩展到 100k 令牌以上.
    *arXiv 预印本 arXiv:2402.13718*，2024b.'
- en: Appendix A Code and Data Availability
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 代码和数据可用性
- en: Code for generating data and evaluating models is available at [https://github.com/booydar/babilong](https://github.com/booydar/babilong).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据和评估模型的代码可在 [https://github.com/booydar/babilong](https://github.com/booydar/babilong)
    获得。
- en: 'We also provide pre-generated evaluation data hosted on HuggingFace datasets.
    The evaluation sets include 100 samples per length and per task, with lengths
    from 0k (no background text from PG-19) to 10 million tokens: [https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong)
    and 1000 samples per length and per task with lengths from 0k to 128k tokens:
    [https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了托管在 HuggingFace 数据集上的预生成评估数据。评估集包括每种长度和任务的 100 个样本，长度从 0k（没有 PG-19 的背景文本）到
    1000 万个标记：[https://huggingface.co/datasets/RMT-team/babilong](https://huggingface.co/datasets/RMT-team/babilong)
    和每种长度和任务的 1000 个样本，长度从 0k 到 128k 个标记：[https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples](https://huggingface.co/datasets/RMT-team/RMT-team/babilong-1k-samples)。
- en: 'The croissant metadata for both evaluation sets is provided by HuggingFace:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 两个评估集的 croissant 元数据由 HuggingFace 提供：
- en: '[https://huggingface.co/api/datasets/RMT-team/babilong/croissant](https://huggingface.co/api/datasets/RMT-team/babilong/croissant)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/api/datasets/RMT-team/babilong/croissant](https://huggingface.co/api/datasets/RMT-team/babilong/croissant)'
- en: '[https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant](https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant](https://huggingface.co/api/datasets/RMT-team/babilong-1k-samples/croissant)。'
- en: Our code is released under the Apache 2.0 License. We use data from the PG-19
    corpora (Rae et al., [2020](#bib.bib50)) (Apache 2.0 License⁶⁶6[https://github.com/google-deepmind/pg19](https://github.com/google-deepmind/pg19))
    and the bAbI dataset (Weston et al., [2016](#bib.bib68)) (BSD License⁷⁷7[https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码在 Apache 2.0 许可证下发布。我们使用来自 PG-19 语料库的数据（Rae 等， [2020](#bib.bib50)）（Apache
    2.0 许可证⁶⁶6[https://github.com/google-deepmind/pg19](https://github.com/google-deepmind/pg19)）和
    bAbI 数据集（Weston 等，[2016](#bib.bib68)）（BSD 许可证⁷⁷7[https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md](https://github.com/facebookarchive/bAbI-tasks/blob/master/LICENSE.md)）。
- en: A.1 Reproducibility
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 可重复性
- en: 'Our code includes data generation, metrics, and the evaluation pipeline used
    to benchmark models. Additionally, we release the predictions of all models used
    in our study to ensure that all reported results can be reproduced and verified:
    [https://github.com/booydar/babilong/tree/predictions_06_2024](https://github.com/booydar/babilong/tree/predictions_06_2024).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码包括数据生成、指标和用于基准测试模型的评估管道。此外，我们发布了我们研究中使用的所有模型的预测，以确保所有报告的结果可以被重复和验证：[https://github.com/booydar/babilong/tree/predictions_06_2024](https://github.com/booydar/babilong/tree/predictions_06_2024)。
- en: Appendix B Related Work on Long Context Models
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 长文本模型的相关工作
- en: Approaches to long context processing
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理长上下文的方法
- en: In retrieval augmented generation (RAG), a language model is combined with a
    separate module, called a retriever. Given a specific request, the retriever finds
    a set of relevant parts from a dedicated data storage. Then parts selected by
    the retriever along with the input are incorporated by the language model to make
    predictions. Many different implementations of the retrieval mechanism have been
    proposed (Guu et al., [2020](#bib.bib26); Borgeaud et al., [2022](#bib.bib9);
    Shi et al., [2023](#bib.bib58)). Some works focus on directly retrieving predictions (Khandelwal
    et al., [2019](#bib.bib33)). Other works retrieve individual input tokens or text
    segments and add them to the LM input (Guu et al., [2020](#bib.bib26); Borgeaud
    et al., [2022](#bib.bib9)). For example, in REALM (Guu et al., [2020](#bib.bib26))
    whole text segments are retrieved and appended to the input to improve masked
    language modeling. In Memorizing Transformer (Wu et al., [2022b](#bib.bib70)),
    the retriever returns cached (key, value) pairs saved from previous training steps
    of the language model. In Retrieval-Pretrained Transformer (Rubin & Berant, [2023](#bib.bib54)),
    an LM component processes long documents in chunks and a retriever finds relevant
    chunks. Representations of retrieved chunks are fused with current chunk in the
    LM component, and both the LM and retrieval parts are trained jointly. AutoCompressor (Chevalier
    et al., [2023](#bib.bib14)) combines RMT-like (Bulatov et al., [2022](#bib.bib10))
    approach with retrieval from external corpora. AutoCompressor is first used to
    produce memory tokens (or summary vectors) for text chunks. Next, off-the-shelf
    retriever is used and corresponding chunk’s memory tokens are added to the context
    of the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索增强生成（RAG）中，语言模型与一个独立的模块——检索器相结合。给定特定的请求，检索器从专门的数据存储中找到一组相关的部分。然后，检索器选择的部分与输入一起被语言模型用于进行预测。已经提出了许多不同的检索机制实现方式（Guu
    et al., [2020](#bib.bib26)；Borgeaud et al., [2022](#bib.bib9)；Shi et al., [2023](#bib.bib58)）。一些研究集中于直接检索预测结果（Khandelwal
    et al., [2019](#bib.bib33)）。其他研究则检索单独的输入标记或文本段，并将它们添加到语言模型的输入中（Guu et al., [2020](#bib.bib26)；Borgeaud
    et al., [2022](#bib.bib9)）。例如，在 REALM（Guu et al., [2020](#bib.bib26)）中，检索整个文本段并将其附加到输入中，以改善掩码语言建模。在
    Memorizing Transformer（Wu et al., [2022b](#bib.bib70)）中，检索器返回从语言模型之前训练步骤中保存的缓存（key,
    value）对。在 Retrieval-Pretrained Transformer（Rubin & Berant, [2023](#bib.bib54)）中，语言模型组件将长文档分成块处理，检索器找到相关的块。检索到的块的表示与当前块在语言模型组件中融合，语言模型和检索部分一起训练。AutoCompressor（Chevalier
    et al., [2023](#bib.bib14)）结合了类似 RMT（Bulatov et al., [2022](#bib.bib10)）的方法与从外部语料库中检索。AutoCompressor
    首先用于生成文本块的记忆标记（或摘要向量）。接下来，使用现成的检索器，将相应块的记忆标记添加到模型的上下文中。
- en: In this work, we augment the Recurrent Memory Transformer (Bulatov et al., [2024](#bib.bib11))
    with the ability to retrieve its own past memory tokens. As far as we know, this
    is the first combination of a recurrent transformer with a trainable retrieval
    mechanism.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们增强了递归记忆变换器（Bulatov et al., [2024](#bib.bib11)），使其能够检索自己过去的记忆标记。就我们所知，这是递归变换器与可训练检索机制的首次结合。
- en: Recurrence is another mechanism to deal with long context (Graves et al., [2014](#bib.bib23);
    Voelker et al., [2019](#bib.bib64); Sorokin et al., [2022](#bib.bib61)). Instead
    of processing the entire context, a recurrent model breaks it down into smaller
    segments. The recurrent hidden state acts as an aggregator of information from
    past segments of the sequence. Attending to a memory state is much cheaper than
    to all contexts. Many different architectures adding recurrence to transformers
    have been proposed (Wu et al., [2022a](#bib.bib69); Lei et al., [2020](#bib.bib36);
    Fan et al., [2020](#bib.bib21)). For example, Compressive Transformer (Rae et al.,
    [2020](#bib.bib50)) updates recurrent memory by compressing hidden activation’s
    from the previous segment to a smaller set of representations. Recurrent Memory
    Transformer (Bulatov et al., [2022](#bib.bib10)) recurrently passes states of
    special memory tokens added to the input of Transformer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 递归是另一种处理长上下文的机制（Graves et al., [2014](#bib.bib23)；Voelker et al., [2019](#bib.bib64)；Sorokin
    et al., [2022](#bib.bib61)）。与处理整个上下文不同，递归模型将其拆分为更小的片段。递归隐藏状态充当来自序列过去片段的信息聚合器。关注记忆状态比关注所有上下文要便宜得多。已经提出了许多不同的将递归添加到变换器中的架构（Wu
    et al., [2022a](#bib.bib69)；Lei et al., [2020](#bib.bib36)；Fan et al., [2020](#bib.bib21)）。例如，Compressive
    Transformer（Rae et al., [2020](#bib.bib50)）通过将先前片段的隐藏激活压缩为更小的一组表示来更新递归记忆。递归记忆变换器（Bulatov
    et al., [2022](#bib.bib10)）递归地传递添加到变换器输入中的特殊记忆标记的状态。
- en: 'Activation Beacon (Zhang et al., [2024a](#bib.bib76)) compresses activations
    from prior segments using separate parameters and integrates a sliding window
    mechanism, handling up to 400k tokens. Temporal Latent Bottleneck (Didolkar et al.,
    [2022](#bib.bib16)) Transformer splits computation into two streams: recurrent
    slow stream and fast stream with self-attention between tokens. Block-Recurrent
    Transformer (Hutchins et al., [2022](#bib.bib30)) employs LSTM-style (Hochreiter
    & Schmidhuber, [1997](#bib.bib28)) gates to update its recurrent state.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Activation Beacon （Zhang 等人，[2024a](#bib.bib76)）使用单独的参数压缩来自先前段的激活，并整合了滑动窗口机制，处理高达
    400k 的 token。Temporal Latent Bottleneck （Didolkar 等人，[2022](#bib.bib16)）Transformer
    将计算分为两个流：递归的慢流和带有 token 之间自注意力的快流。Block-Recurrent Transformer （Hutchins 等人，[2022](#bib.bib30)）采用
    LSTM 风格的 （Hochreiter & Schmidhuber，[1997](#bib.bib28)）门来更新其递归状态。
- en: We use RMT in our experiments because of its simplicity, plug-and-play compatibility
    with pre-trained transformer-based language models, and promising scaling capabilities (Bulatov
    et al., [2024](#bib.bib11)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用 RMT，因其简单性、与预训练的基于 Transformer 的语言模型的即插即用兼容性以及令人期待的扩展能力 （Bulatov 等人，[2024](#bib.bib11)）。
- en: Big Bird (Zaheer et al., [2020](#bib.bib74)), Longformer (Beltagy et al., [2020](#bib.bib8)),
    LongNet (Ding et al., [2023](#bib.bib17)) help extend context length for Transformers
    by switching from full self-attention to sparse self-attention mechanisms with
    linear complexity. Works like RWKV (Peng et al., [2023a](#bib.bib46)), S4 (Gu
    et al., [2021](#bib.bib25)), Mamba (Gu & Dao, [2023](#bib.bib24)), take another
    approach and focus on advancing recurrent networks to reach high parallelism levels
    available to Transformers while retaining the linear complexity of RNN. These
    works show promising results on long sequences but are still lagging behind the
    best transformer models in natural language processing tasks. Mamba, however,
    seeks to bridge this gap.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Big Bird （Zaheer 等人，[2020](#bib.bib74)）、Longformer （Beltagy 等人，[2020](#bib.bib8)）、LongNet （Ding
    等人，[2023](#bib.bib17)）通过从完整自注意力机制转为具有线性复杂度的稀疏自注意力机制，帮助扩展 Transformer 的上下文长度。像
    RWKV （Peng 等人，[2023a](#bib.bib46)）、S4 （Gu 等人，[2021](#bib.bib25)）、Mamba （Gu & Dao，[2023](#bib.bib24)）这样的工作采取了另一种方法，专注于推动递归网络达到与
    Transformer 相匹配的高并行性水平，同时保留 RNN 的线性复杂度。这些工作在长序列上显示出有希望的结果，但在自然语言处理任务中仍落后于最好的 Transformer
    模型。然而，Mamba 旨在弥补这一差距。
- en: Appendix C Details on RMT and Mamba fine-tuning on BABILong
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 详细信息：RMT 和 Mamba 在 BABILong 上的微调
- en: '![Refer to caption](img/e9b0eec812105b7c08ce29b98041e3a6.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9b0eec812105b7c08ce29b98041e3a6.png)'
- en: 'Figure 5: RMT performance on five BABILong tasks varies between training seeds.
    The plot represents average performance and standard deviation across three training
    runs.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：RMT 在五个 BABILong 任务上的性能因训练种子而异。图表表示三个训练运行的平均性能和标准差。
- en: 'We used the GPT-2 (Radford et al., [2019](#bib.bib49)) (137M) model as the
    backbone for RMT. The segment size was fixed at 512 tokens, and the model was
    augmented with 16 memory tokens. Finetuning was conducted on BABILong using a
    curriculum schedule with progressively increasing sequence lengths: 1, 2, 4, 6,
    8, 16 and 32 segments. For each curriculum step $n$ for every batch to prevent
    overfitting to a certain context size. We maintained a fixed batch size of 64
    and the AdamW Loshchilov & Hutter ([2019](#bib.bib43)) optimizer with learning
    rate 1e-05, a linear schedule and 1000 warmup steps. Each curriculum stage had
    a maximum of 10,000 steps, although checkpoints were frequently finalized earlier
    based on the best validation exact match metric. The weight decay value was set
    to 0.01, and no gradient truncation was used. Training was performed on 1-4 Nvidia
    A100 or H100 GPUs with the duration of each curriculum stage ranging from 40 minutes
    to 20 hours.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 GPT-2 （Radford 等人，[2019](#bib.bib49)）（137M）模型作为 RMT 的骨干。段大小固定为 512 tokens，模型增加了
    16 个记忆 token。微调是在 BABILong 上使用逐步增加序列长度的课程计划进行的：1、2、4、6、8、16 和 32 个段。为了防止对某一上下文大小的过拟合，每个课程步骤
    $n$ 对每个批次进行调整。我们保持了固定的批量大小 64，并使用了 AdamW Loshchilov & Hutter（[2019](#bib.bib43)）优化器，学习率为
    1e-05，线性计划和 1000 个预热步骤。每个课程阶段最多有 10,000 步，尽管根据最佳验证精确匹配指标，检查点通常会更早完成。权重衰减值设置为 0.01，且未使用梯度截断。训练在
    1-4 个 Nvidia A100 或 H100 GPU 上进行，每个课程阶段的持续时间从 40 分钟到 20 小时不等。
- en: 'For each experiment we conducted three runs with different memory intitalizations
    and dataset shuffles. As shown in Figure [5](#A3.F5 "Figure 5 ‣ Appendix C Details
    on RMT and Mamba fine-tuning on BABILong ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack"), performance on context lengths, exceeding
    ones seen during training, may vary across different runs. This suggests that
    the early stopping criterion based on short-context accuracy may not be optimal.
    To reduce deviations between runs and enhance overall performance, techniques
    such as improving early stopping, gradient truncation and training on longer sequences
    can be employed.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个实验，我们进行了三次实验，分别使用了不同的内存初始化和数据集洗牌。如图[5](#A3.F5 "图 5 ‣ 附录 C 关于 RMT 和 Mamba
    在 BABILong 上的微调细节 ‣ BABILong: 测试具有长上下文推理的 LLM 的极限")所示，在上下文长度超出训练时所见的范围时，性能可能会因不同的实验而有所不同。这表明，基于短上下文准确率的早停准则可能不是最佳的。为了减少实验之间的偏差并提升整体性能，可以采用如改善早停、梯度截断和在更长序列上训练等技术。'
- en: To fine-tune mamba-130m, we used the exact same curriculum approach, with a
    randomly selected number of segments that gradually increased. Throughout every
    curriculum step, the batch size remained constant at 128\. We employed the AdamW
    optimizer with a linear schedule, weight decay of 2.0, gradient clipping of 1.0,
    learning rate of 3e–4, and a warmup step count of 10% of the total training steps.
    The model was trained for 10,000 steps in each curriculum stage except for the
    last one, which had 32 segments, where it was trained for 15,000 steps. 4 NVidia
    H100 GPUs were used for the training, and the overall training process for every
    task from BABILong took 2 to 3 days to complete.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调 mamba-130m，我们使用了完全相同的课程方法，随机选择了逐渐增加的段数。在每个课程步骤中，批量大小保持不变为 128。我们使用了带有线性计划的
    AdamW 优化器，权重衰减为 2.0，梯度裁剪为 1.0，学习率为 3e–4，预热步数为总训练步数的 10%。模型在每个课程阶段训练了 10,000 步，除了最后一个阶段，最后一个阶段有
    32 个段，在其中训练了 15,000 步。训练使用了 4 个 NVidia H100 GPU，每个 BABILong 任务的整体训练过程耗时 2 到 3
    天。
- en: Appendix D Detailed LLM evaluation on BABILong tasks
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 关于 BABILong 任务的详细 LLM 评估
- en: '![[Uncaptioned image]](img/ae8fb00b7fdbd759c265fb8452479940.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/ae8fb00b7fdbd759c265fb8452479940.png)'
- en: 'Table 3: Results of LLM evaluation on the first five tasks of BABILong. Rows
    correspond to sequence lengths, columns denote models, and each section represents
    a separate task from QA1 to QA5\. Each number indicates the average accuracy of
    the model at a given sequence length, calculated over 1000 samples for lengths
    up to 32k tokens, and over 100 samples for longer lengths.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：BABILong 前五个任务的 LLM 评估结果。行对应序列长度，列表示模型，每个部分代表一个单独的任务，从 QA1 到 QA5。每个数字表示模型在给定序列长度下的平均准确率，长度为
    32k 标记的样本数为 1000，长度更长的样本数为 100。
- en: 'Here we present the complete results of LLM evaluation. Table [3](#A4.T3 "Table
    3 ‣ Appendix D Detailed LLM evaluation on BABILong tasks ‣ BABILong: Testing the
    Limits of LLMs with Long Context Reasoning-in-a-Haystack") showcases the performance
    of 27 models across the first five tasks. Comparing the tasks in the table makes
    evident the difference in task complexity for language models. QA1 and QA5 are
    the easiest, with most models achieving over 70% accuracy for the 0k split. QA4
    is significantly more challenging, and only 5 models can reach this level of performance.
    QA2 and QA3 pose even greater challenges for most models.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '这里我们展示了 LLM 评估的完整结果。表[3](#A4.T3 "表 3 ‣ 附录 D 关于 BABILong 任务的详细 LLM 评估 ‣ BABILong:
    测试具有长上下文推理的 LLM 的极限")展示了27个模型在前五个任务中的表现。比较表中的任务，可以清楚地看出语言模型任务复杂性的差异。QA1 和 QA5
    是最简单的，大多数模型在 0k 切分上达到了 70% 以上的准确率。QA4 则明显更具挑战性，仅有 5 个模型能够达到这个性能水平。QA2 和 QA3 对大多数模型提出了更大的挑战。'
- en: The number of parameters positively impacts accuracy on the shortest 0k split.
    Among not-finetuned models, GPT-4, Phi-3-medium, Jamba, Command-R, Yi-34B and
    Mixtral 8x22B consistently outperform smaller models. Notably, RWKV and Mamba-2.8B
    also demonstrate strong performance on QA2 and QA3\. However, as the context length
    increases, some of the largest models lose their advantage over smaller ones.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量对最短 0k 切分上的准确性有积极影响。在未微调的模型中，GPT-4、Phi-3-medium、Jamba、Command-R、Yi-34B 和
    Mixtral 8x22B 一直优于较小的模型。特别地，RWKV 和 Mamba-2.8B 在 QA2 和 QA3 上也表现出色。然而，随着上下文长度的增加，一些最大模型相对于较小模型的优势逐渐减小。
- en: Retreival-augmented Llama-3 has a strong advantage of being able to perform
    on any context length up to 10M tokens. On QA4 and QA5 retrieval allows to match
    and even surpass weaker competitors on longer context sizes. However, on QA2 and
    QA3 this approach fails dramatically. The reason for this performance drop lies
    in inability of retrieval to maintain the order of found sentences, complicating
    the task for the underlying Llama. Additionally, relevant sentences in these tasks
    are not always semantically similar to the question, preventing the model from
    retrieving all necessary facts for correct reasoning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 增强检索的Llama-3在任何上下文长度高达10M标记的情况下具有强大的优势。在QA4和QA5中，检索可以匹配甚至超越较弱的竞争对手，在较长的上下文尺寸上表现出色。然而，在QA2和QA3中，这种方法会显著失效。性能下降的原因在于检索无法维持找到的句子的顺序，从而使底层Llama任务变得复杂。此外，这些任务中的相关句子并不总是与问题在语义上相似，阻碍了模型检索所有必要的事实以进行正确推理。
- en: It is important to note, that all BABILong tasks are in practice solvable even
    with smaller models. Finetuned RMT and Mamba achieve outstanding scores across
    most sequence lengths, significantly outperforming LLMs despite having up to 100
    times ewer parameters. Mamba has an advantage on medium-length sequences, but
    RMT excells in processing much larger sequences up to 10M tokens.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，所有BABILong任务实际上即使在较小模型下也可以解决。微调的RMT和Mamba在大多数序列长度上取得了卓越的成绩，尽管参数最多多达100倍，但它们显著超越了LLMs。Mamba在中等长度序列上具有优势，但RMT在处理长度高达10M标记的序列时表现出色。
- en: Appendix E RMT Training and Evaluation Details
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E RMT训练和评估细节
- en: 'For RMT we use curriculum training with sequentially increasing number of segments.
    RMT uses the following schedule for number of segments: 1-2-4-6-8-16-32\. During
    each curriculum stage $n$ and train for {5000, 10000} steps with early stopping
    if metrics stop increasing. For the backbone transformer we use the pretrained
    GPT-2 137M from HuggingFace: [https://huggingface.co/GPT-2](https://huggingface.co/GPT-2).
    We used up to 4 Nvidia A100 80Gb per experiment.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RMT，我们使用递增的课程训练，序列数逐步增加。RMT采用以下序列数量安排：1-2-4-6-8-16-32。在每个课程阶段$n$中训练{5000,
    10000}步，并在指标停止增长时进行提前停止。对于基础变换器，我们使用HuggingFace提供的预训练GPT-2 137M：[https://huggingface.co/GPT-2](https://huggingface.co/GPT-2)。每个实验中使用了最多4个Nvidia
    A100 80Gb。
- en: 'We evaluate models on the BABILong benchmark. We use full test set for sequences
    up to 1M tokens, for 10M tokens we only provide results for 100 samples. As shown
    in Table [4](#A5.T4 "Table 4 ‣ Appendix E RMT Training and Evaluation Details
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"),
    evaluation time grows linearly with context length. We fix a random seed used
    to sample background texts from PG19 for the test set. However, the seed was not
    fixed to avoid overfitting to specific sampled texts during training.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在BABILong基准上评估模型。对于长度最多为1M标记的序列，我们使用完整的测试集；对于10M标记的序列，我们仅提供100个样本的结果。如表[4](#A5.T4
    "Table 4 ‣ Appendix E RMT Training and Evaluation Details ‣ BABILong: Testing
    the Limits of LLMs with Long Context Reasoning-in-a-Haystack")所示，评估时间与上下文长度呈线性增长。我们固定了用于从PG19中抽取背景文本的随机种子。然而，为了避免在训练过程中对特定采样文本的过拟合，种子并没有固定。'
- en: 'Table 4: Time required for processing 1000 BABILong samples with RMT using
    a single A100 80Gb GPU, including input data processing.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：使用单个A100 80Gb GPU处理1000个BABILong样本所需的时间，包括输入数据处理。
- en: '| Context size | 4k | 32k | 128k | 1M |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 上下文大小 | 4k | 32k | 128k | 1M |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Processing time, minutes | 4 | 30 | 80 | 315 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 处理时间，分钟 | 4 | 30 | 80 | 315 |'
- en: Appendix F BABILong Dataset Statistics
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F BABILong数据集统计
- en: 'The proposed benchmark includes 20 diverse tasks, ranging from simple "needle
    in a haystack" scenarios with distractor facts to more complex tasks that require
    counting, logical reasoning, or spatial reasoning. The Figure [6](#A6.F6 "Figure
    6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack") evaluates the complexity of the base
    short versions of these tasks. Tasks such as QA1, QA5, and QA10 are generally
    easier for most models, whereas QA7, QA15, and QA19 are the most challenging.
    The plot clearly shows that the number of facts needed for reasoning significantly
    impacts task complexity, as performance gradually declines from QA1 to QA2 and
    QA3, which differ in the number of supporting facts. The distribution of task
    labels is shown in Table [6](#A6.T6 "Table 6 ‣ Appendix F BABILong Dataset Statistics
    ‣ BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '提出的基准包括20个多样化的任务，从带有干扰事实的简单“针在干草堆中”场景到需要计数、逻辑推理或空间推理的更复杂任务。图[6](#A6.F6 "Figure
    6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")评估了这些任务的基础短版本的复杂性。QA1、QA5和QA10等任务对于大多数模型来说通常较容易，而QA7、QA15和QA19则是最具挑战性的。图中清楚地显示了推理所需的事实数量显著影响任务复杂性，因为从QA1到QA2和QA3，性能逐渐下降，这些任务在支持事实的数量上有所不同。任务标签的分布如表[6](#A6.T6
    "Table 6 ‣ Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits
    of LLMs with Long Context Reasoning-in-a-Haystack")所示。'
- en: '![Refer to caption](img/7ddd1004ec8e6f97351f741bc593b00a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7ddd1004ec8e6f97351f741bc593b00a.png)'
- en: 'Figure 6: The performance of LLMs on the bAbI (BABILong without distractor
    text) depends significantly on the task complexity. Each dot represents the average
    accuracy of the model on one thousand samples of the given task. The median accuracy
    across all models is denoted by black stars.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LLMs在bAbI（没有干扰文本的BABILong）上的表现显著依赖于任务的复杂性。每个点代表模型在给定任务的一千个样本上的平均准确率。所有模型的中位准确率由黑色星星表示。
- en: 'BABILong is a generative benchmark, designed to be scalable with increasing
    length of language models. The same bAbI task can be scaled to any desired length
    in tokens by adding a sufficient number of distractor sentences. For reproducibility,
    we pre-generate dataset splits for several fixed lengths: 0k (tasks with no distractor
    sentences), 4k, 8k, 16k, 32k, 64k, 128k, 512k, 1M and 10M tokens. The length in
    tokens is measured using the classic GPT-2 tokenizer, which is close in fertility
    to the popular GPT-4 tokenizer. As shown in Table [5](#A6.T5 "Table 5 ‣ Appendix
    F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs with Long
    Context Reasoning-in-a-Haystack"), the number of tokens for tokenizers of different
    models may differ for samples in the same split. However considering the trade-off
    between the sequence length and embedding layer size, we believe the comparison
    remains fair.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'BABILong是一个生成性基准，旨在随着语言模型长度的增加而具有可扩展性。通过添加足够数量的干扰句子，可以将相同的bAbI任务扩展到所需的任意长度。为了确保可重复性，我们为多个固定长度预生成了数据集拆分：0k（没有干扰句子的任务）、4k、8k、16k、32k、64k、128k、512k、1M和10M
    tokens。token的长度使用经典的GPT-2分词器进行测量，其在繁殖性方面接近于流行的GPT-4分词器。如表[5](#A6.T5 "Table 5 ‣
    Appendix F BABILong Dataset Statistics ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack")所示，不同模型的分词器在相同拆分中的样本的token数量可能会有所不同。然而，考虑到序列长度和嵌入层大小之间的权衡，我们认为比较仍然公平。'
- en: 'Table 5: Token count for various models across selected tasks. We measure the
    length of BABILong samples using the conservative GPT-2 tokenizer. Actual token
    sizes may vary depending on the model tokenizer.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：各种模型在选定任务中的token计数。我们使用保守的GPT-2分词器来测量BABILong样本的长度。实际的token大小可能会根据模型分词器有所不同。
- en: '| Model | 0k | 4k | 16k | 64k | 128k |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 0k | 4k | 16k | 64k | 128k |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4 | 120 | 3544 | 15071 | 61343 | 123367 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 120 | 3544 | 15071 | 61343 | 123367 |'
- en: '| GPT-2 | 120 | 3700 | 15699 | 63698 | 127695 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 120 | 3700 | 15699 | 63698 | 127695 |'
- en: '| Llama-2 | 135 | 3942 | 16757 | 68110 | 137222 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 135 | 3942 | 16757 | 68110 | 137222 |'
- en: '| Mistral | 128 | 3863 | 16438 | 66862 | 134592 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 128 | 3863 | 16438 | 66862 | 134592 |'
- en: '| Words | 98 | 2548 | 10789 | 44180 | 88592 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 词汇 | 98 | 2548 | 10789 | 44180 | 88592 |'
- en: '| Symbols | 561 | 14507 | 61452 | 251947 | 507598 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 561 | 14507 | 61452 | 251947 | 507598 |'
- en: 'Table 6: The distribution of labels in first five BABILong tasks, % of all
    samples.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：前五个BABILong任务中的标签分布，占所有样本的百分比。
- en: '|  | label1 | label2 | label3 | label4 | label5 | label6 | label7 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | label1 | label2 | label3 | label4 | label5 | label6 | label7 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| qa1 | 15.4 | 14.9 | 15.7 | 18.7 | 18.2 | 17.1 |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| qa1 | 15.4 | 14.9 | 15.7 | 18.7 | 18.2 | 17.1 |  |'
- en: '| qa2 | 15.9 | 18.7 | 16.7 | 16.5 | 17.5 | 14.6 |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| qa2 | 15.9 | 18.7 | 16.7 | 16.5 | 17.5 | 14.6 |  |'
- en: '| qa3 | 13.3 | 18.4 | 21.5 | 14.6 | 15.4 | 16.7 |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| qa3 | 13.3 | 18.4 | 21.5 | 14.6 | 15.4 | 16.7 |  |'
- en: '| qa4 | 15.6 | 17.7 | 16.6 | 17.1 | 15.3 | 17.6 |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| qa4 | 15.6 | 17.7 | 16.6 | 17.1 | 15.3 | 17.6 |  |'
- en: '| qa5 | 9.5 | 18.8 | 12.9 | 16.4 | 13.6 | 18.9 | 9.8 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| qa5 | 9.5 | 18.8 | 12.9 | 16.4 | 13.6 | 18.9 | 9.8 |'
- en: Appendix G Details of the RAG Pipeline
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G RAG 流水线详细信息
- en: 'For the GPT4-RAG pipelines, we employed the FAISS (Douze et al., [2024](#bib.bib19))
    vector database, using Langchain (Chase, [2022](#bib.bib12)), for our experimental
    RAG setup. We utilized the ’text-embedding-ada-002’ model for generating text
    embeddings. Our methodology encompassed two distinct approaches for text chunking:
    firstly, segmentation by sentences utilizing the NLTK library, and secondly, division
    into segments of 512 tokens each. We adopted a binary metric for evaluating retrieval
    accuracy, where the criterion was the presence or absence of relevant facts (singular
    or multiple, based on the specific task) within the retrieved text chunks. This
    retrieval accuracy was quantified for the top 5 chunks. Additionally, we assessed
    the performance of GPT-4-turbo in conjunction with the retrieved facts, specifically
    focusing on the ’QA1’ task. Our experimental scope spanned various context lengths,
    including 8k, 64k, and 128k tokens for tasks ’QA1’ through ’QA5’ of the BABILong
    dataset, with added 4k, 16k, 32k, 500k, 1M and 10M token length for an in-depth
    analysis of the ’QA1’ task. Additionally, we assessed the performance of RAG on
    the ’QA1’ task, utilizing precomputed Wikipedia embeddings⁸⁸8[https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings)
    instead of pg-19 with an average embedding size of 250 tokens. This evaluation
    aimed to determine the influence of embedding size and noise characteristics on
    model performance. For each task, we maintained a consistent sample size of 50
    across different context lengths. For the Llama3 + RAG pipeline we used the ’nvidia/Llama3-ChatQA-1.5-8B’
    as the language model Liu et al. ([2024b](#bib.bib42)) and the ’nvidia/dragon-multiturn-query-encoder’
    for context embedding. Another difference is that we did not use any caching and
    Wikipedia embeddings unlike with GPT-4.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT4-RAG 流水线，我们使用了 FAISS (Douze et al., [2024](#bib.bib19)) 向量数据库，并利用 Langchain
    (Chase, [2022](#bib.bib12)) 进行实验性的 RAG 设置。我们使用了’text-embedding-ada-002’模型生成文本嵌入。我们的方法包括两种不同的文本分块方式：首先，利用
    NLTK 库按句子进行分割；其次，划分为每个512个标记的段落。我们采用了二元指标来评估检索准确性，标准是检索文本块中相关事实的存在或缺失（根据具体任务，可能是单个或多个）。此检索准确性是对前5个块进行量化的。此外，我们评估了
    GPT-4-turbo 与检索到的事实结合的性能，特别关注于’QA1’任务。我们的实验范围涵盖了不同的上下文长度，包括 8k、64k 和 128k 标记，涉及
    BABILong 数据集的’QA1’至’QA5’任务，并增加了 4k、16k、32k、500k、1M 和 10M 标记长度，以深入分析’QA1’任务。此外，我们评估了
    RAG 在’QA1’任务上的表现，使用了预计算的 Wikipedia 嵌入⁸⁸8[https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings](https://huggingface.co/datasets/Supabase/wikipedia-en-embeddings)
    替代了 pg-19，平均嵌入大小为 250 个标记。这项评估旨在确定嵌入大小和噪声特征对模型性能的影响。对于每个任务，我们在不同上下文长度下保持了 50 的一致样本大小。对于
    Llama3 + RAG 流水线，我们使用了’nvidia/Llama3-ChatQA-1.5-8B’作为语言模型 Liu et al. ([2024b](#bib.bib42))
    和’nvidia/dragon-multiturn-query-encoder’用于上下文嵌入。另一个不同之处是我们没有像 GPT-4 那样使用任何缓存和
    Wikipedia 嵌入。
- en: Appendix H Recurrent Memory Transformer Analysis
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 递归记忆变换器分析
- en: 'To understand how recurrent models consistently retain their performance over
    extremely long sequences, we analyze the RMT memory states and attention patterns
    on the QA1 task. We evaluate RMT trained on 32 segments or approximately 16k tokens
    on a single sample with two facts, see Figure [7](#A8.F7 "Figure 7 ‣ Appendix
    H Recurrent Memory Transformer Analysis ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack") (a) and (b). For both sequence lengths
    16k and 128k the memory states exhibit a consistent pattern. In the absence of
    fact in input, the memory remains similar to its initial states, but the introduction
    of fact leads to visible change in the memory state. This indicates that the model
    learned to distinguish important facts from the background text and preserving
    them in memory until a question appears. The operations with memory are represented
    by distinctive patterns on attention maps, specifically, the process of writing
    a new fact to memory Figure [7](#A8.F7 "Figure 7 ‣ Appendix H Recurrent Memory
    Transformer Analysis ‣ BABILong: Testing the Limits of LLMs with Long Context
    Reasoning-in-a-Haystack") (c) and reading from memory to answer a question (d).
    This visual demonstration supports the intuition of learning distinct memory operations
    when dealing with information scattered across extended contextual spans.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解递归模型如何在极长序列上持续保持其性能，我们分析了 RMT 在 QA1 任务上的记忆状态和关注模式。我们评估了在单个样本上训练了 32 个段或大约
    16k 个标记的 RMT，包含两个事实，见图 [7](#A8.F7 "图 7 ‣ 附录 H 递归记忆变换器分析 ‣ BABILong: 测试 LLM 在长上下文推理中的极限")
    (a) 和 (b)。对于 16k 和 128k 的序列长度，记忆状态表现出一致的模式。在输入中没有事实的情况下，记忆保持与其初始状态相似，但事实的引入导致记忆状态发生明显变化。这表明模型学会了将重要事实与背景文本区分开来，并将其保存在记忆中，直到出现问题。记忆操作通过注意图上的独特模式表示，具体来说，就是将新事实写入记忆的过程，见图
    [7](#A8.F7 "图 7 ‣ 附录 H 递归记忆变换器分析 ‣ BABILong: 测试 LLM 在长上下文推理中的极限") (c)，以及从记忆中读取以回答问题
    (d)。这种视觉演示支持在处理分散于扩展上下文跨度的信息时学习不同记忆操作的直观感受。'
- en: '![Refer to caption](img/8cbb567df39d312fa2cd5e1903938826.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8cbb567df39d312fa2cd5e1903938826.png)'
- en: 'Figure 7: RMT learns to detect and store relevant facts using memory. Heatmaps
    (a) and (b) represent pairwise distances between memory states on QA1 with context
    size 16k (a) and 128k (b). Distant states are marked with blue color and similar
    ones with red. Changes in memory mainly occurs when the model meets a new fact,
    which indicates model adaptation to distinguishing and storing facts in memory.
    Memory attention maps (c) and (d) when RMT writes the fact to memory (c) and then
    reads it when answering the question (d). The intensity of red color corresponds
    to the amount of attention between the query on the left and key on the top.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：RMT 学会使用记忆来检测和存储相关事实。热图 (a) 和 (b) 表示在上下文大小为 16k (a) 和 128k (b) 的 QA1 上，记忆状态之间的成对距离。距离较远的状态用蓝色标记，类似的状态用红色标记。当模型遇到新事实时，记忆的变化主要发生，这表明模型在适应于区分和存储记忆中的事实。记忆关注图
    (c) 和 (d) 显示了 RMT 在将事实写入记忆时 (c) 和回答问题时读取记忆的情况 (d)。红色的强度对应于左侧查询和顶部键之间的关注量。
- en: Appendix I LLM fine-tuning
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I LLM 微调
- en: 'Results for GPT-3.5 and Mistral-7B fine-tuning are shown on the Fig.[8](#A9.F8
    "Figure 8 ‣ Appendix I LLM fine-tuning ‣ BABILong: Testing the Limits of LLMs
    with Long Context Reasoning-in-a-Haystack").'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-3.5 和 Mistral-7B 微调的结果如图 [8](#A9.F8 "图 8 ‣ 附录 I LLM 微调 ‣ BABILong: 测试 LLM
    在长上下文推理中的极限") 所示。'
- en: '![Refer to caption](img/cfa45a247ec7e49453b4d4ad2f13081b.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfa45a247ec7e49453b4d4ad2f13081b.png)'
- en: 'Figure 8: LLM fine-tuning makes full context effective. a) After fine-tuning
    both GPT-3.5 and Mistral-7B significantly improved their scores along context
    lengths achieving 90% + accuracy on QA1 task. b) GPT-3.5 fine-tuned for QA1 task
    shows improved performance on QA2-QA5 tasks. c) Full fine-tuning of smaller Mistral-7B
    on QA1 results in degraded scores for other tasks (QA2-QA5). No distractor text
    for b) and c).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM 微调使得完整上下文有效。a) 微调后，GPT-3.5 和 Mistral-7B 的得分在上下文长度上显著提高，在 QA1 任务中达到了 90%
    以上的准确率。b) 针对 QA1 任务微调的 GPT-3.5 在 QA2-QA5 任务上表现出改进的性能。c) 在 QA1 上完全微调的小型 Mistral-7B
    对其他任务 (QA2-QA5) 的得分结果有所下降。b) 和 c) 中没有干扰文本。
- en: Appendix J Prompts Used to Benchmark GPT-4-Turbo and Mistral Models
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 用于基准测试 GPT-4-Turbo 和 Mistral 模型的提示
- en: We used the same prompts to evaluate GPT-4-Turbo and Mistral models in our tasks.
    Each prompt starts with the description of the task followed by several examples
    inside the $<$ tags. The next section inside $<$ tags contains an instance of
    the task. We additionally duplicate the question with the QUESTION mark, in order
    for the model recognize the question in the large input prompts. The last sentences
    specify the required response format.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的提示来评估 GPT-4-Turbo 和 Mistral 模型在我们的任务中的表现。每个提示都以任务描述开始，后面跟着几个在 $<$ 标签内的示例。接下来的部分在
    $<$ 标签内包含任务的一个实例。我们额外重复了带有 QUESTION 标记的问题，以便模型能够识别大输入提示中的问题。最后的句子指定了所需的响应格式。
- en: QA1
    task I will give you context with the facts about positions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts. If a person was in different locations, use the latest location to answer the question.
     Charlie went to the hallway. Judith come back to the kitchen. Charlie travelled to balcony. Where is Charlie?
    Answer: The most recent location of Charlie is balcony.   Alan moved to the garage. Charlie went to the beach. Alan went to the shop. Rouse travelled to balcony. Where is Alan?
    Answer: The most recent location of Alan is shop.   {QA1 query with noise}
     QUESTION: {QA1 question} Always return your answer in the following format: The most recent location of ’person’ is ’location’. Do not write anything else after that.QA2 task
    I give you context with the facts about locations and actions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts.
    If a person got an item in the first location and travelled to the second location the item is also in the second location.
    If a person dropped an item in the first location and moved to the second location the item remains in the first location.
     Charlie went to the kitchen. Charlie got a bottle. Charlie moved to the balcony. Where is the bottle?
    Answer: The bottle is in the balcony.   Alan moved to the garage. Alan got a screw driver. Alan moved to the kitchen. Where is the screw driver?
    Answer: The screw driver is in the kitchen.   {QA2 query with noise}
     QUESTION: {QA2 question} Always return you answer in the following format: The ’item’ is in ’location’. Do not write anything else after that.QA3 task
    I give you context with the facts about locations and actions of different persons hidden in some random text and a question.
    You need to answer the question based only on the information from the facts.
    If a person got an item in the first location and travelled to the second location the item is also in the second location.
    If a person dropped an item in the first location and moved to the second location the item remains in the first location
     John journeyed to the bedroom.Mary grabbed the apple. Mary went back to the bathroom. Daniel journeyed to the bedroom. Daniel moved to the garden. Mary travelled to the kitchen. Where was the apple before the kitchen?
    Answer: Before the kitchen the apple was in the bathroom.  
    John went back to the bedroom. John went back to the garden. John went back to the kitchen. Sandra took the football. Sandra travelled to the garden. Sandra journeyed to the bedroom. Where was the football before the bedroom?
    Answer: Before the kitchen the football was in the garden.  
    {QA3 query with noise}  QUESTION: {QA3 question} Always return you answer in the following format: Before the $location_1& the $item$ was in the $location_2$. Do not write anything else after that.QA4 task
    I will give you context with the facts about different people, their location and actions, hidden in some random text and a question.
    You need to answer the question based only on the information from the facts.
     The hallway is south of the kitchen. The bedroom is north of the kitchen. What is the kitchen south of?
    Answer: bedroom   The garden is west of the bedroom. The bedroom is west of the kitchen. What is west of the bedroom?
    Answer: garden   {QA4 query with noise}  QUESTION: {QA4 question}
    Your answer should contain only one word - location. Do not write anything else after thatQA5 task
    I will give you context with the facts about locations and their relations hidden in some random text and a question. You need to answer the question based only on the information from the facts.
     Mary picked up the apple there. Mary gave the apple to Fred. Mary moved to the bedroom. Bill took the milk there. Who did Mary give the apple to?
    Answer: Fred   1 Jeff took the football there. Jeff passed the football to Fred. Jeff got the milk there. Bill travelled to the bedroom. Who gave the football?
    Answer: Jeff   Fred picked up the apple there. Fred handed the apple to Bill. Bill journeyed to the bedroom. Jeff went back to the garden. What did Fred give to Bill?
    Answer: apple   {QA5 query with noise}  QUESTION: {QA5 question}
    Your answer should contain only one word. Do not write anything else after that. Do not explain your answer.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: QA1任务
    我将提供有关不同人物位置的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人曾经在不同的位置，请使用最新的位置来回答问题。
    查理去了走廊。朱迪回到厨房。查理去阳台了。查理现在在哪里？ 答案：查理最新的位置是阳台。  艾伦去了车库。查理去了海滩。艾伦去了商店。罗斯去了阳台。艾伦现在在哪里？
    答案：艾伦最新的位置是商店。  {QA1查询和噪声}  问题：{QA1问题} 请始终以以下格式返回你的答案：’person’最新的位置是’location’。不要在后面写任何其他内容。QA2任务 我提供有关不同人物的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人在第一个位置拿到了一件物品，并移动到第二个位置，那么物品也在第二个位置。如果一个人在第一个位置丢下了物品，并移动到第二个位置，那么物品仍然在第一个位置。
    查理去了厨房。查理拿到了一个瓶子。查理移到了阳台。瓶子现在在哪里？ 答案：瓶子在阳台。  艾伦去了车库。艾伦拿到了一把螺丝刀。艾伦去了厨房。螺丝刀现在在哪里？
    答案：螺丝刀在厨房。  {QA2查询和噪声}  问题：{QA2问题} 请始终以以下格式返回你的答案：’item’在’location’。不要在后面写任何其他内容。QA3任务 我提供有关不同人物的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。如果一个人在第一个位置拿到了一件物品，并移动到第二个位置，那么物品也在第二个位置。如果一个人在第一个位置丢下了物品，并移动到第二个位置，那么物品仍然在第一个位置。
    约翰去了卧室。玛丽拿到了苹果。玛丽回到浴室。丹尼尔去了卧室。丹尼尔去了花园。玛丽去了厨房。苹果在厨房之前在哪里？ 答案：在厨房之前，苹果在浴室。
     约翰回到卧室。约翰回到花园。约翰回到厨房。桑德拉拿到了足球。桑德拉去了花园。桑德拉去了卧室。足球在卧室之前在哪里？ 答案：在卧室之前，足球在花园。
     {QA3查询和噪声}  问题：{QA3问题} 请始终以以下格式返回你的答案：在$location_1$之前，$item$在$location_2$。不要在后面写任何其他内容。QA4任务 我将提供有关不同人物、他们的位置和行动的背景信息，这些信息隐藏在一些随机文本中，并附带一个问题。你需要仅根据这些事实的信息回答问题。
    走廊在厨房的南边。卧室在厨房的北边。厨房在南边是什么？ 答案：卧室  花园在卧室的西边。卧室在厨房的西边。卧室的西边是什么？
    答案：花园  {QA4查询和噪声}  问题：{QA4问题} 你的答案应仅包含一个词 - 位置。不要在后面写任何其他内容。QA5任务 
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，可以通过 GitHub 和 HuggingFace 数据集上的 Pull Requests 进行贡献。
