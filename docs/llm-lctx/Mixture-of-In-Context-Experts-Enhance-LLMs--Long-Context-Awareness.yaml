- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:01:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:01:16'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文专家混合增强LLMs的长上下文意识
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19598](https://ar5iv.labs.arxiv.org/html/2406.19598)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19598](https://ar5iv.labs.arxiv.org/html/2406.19598)
- en: 'Hongzhan Lin¹  Ang Lv¹¹¹footnotemark: 1  Yuhan Chen²¹¹footnotemark: 1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 洪瞻林¹  昂吕¹¹¹脚注标记：1  余涵陈²¹¹脚注标记：1
- en: Chen Zhu³ Yang Song⁴ Hengshu Zhu³  Rui Yan¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈铸³ 杨松⁴ 恒书铸³  锐焰¹
- en: ¹ Gaoling School of Artificial Intelligence, Renmin University of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 中国人民大学高岭人工智能学院
- en: ² XiaoMi AI Lab ³ Career Science Lab, BOSS Zhipin ⁴ NLP Center, BOSS Zhipin
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 小米AI实验室 ³ 职场科学实验室，BOSS直聘 ⁴ NLP中心，BOSS直聘
- en: '{linhongzhan, anglv, ruiyan}@ruc.edu.cn'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{linhongzhan, anglv, ruiyan}@ruc.edu.cn'
- en: '{chenyuhan5}@xiaomi.com Equal contribution. Hongzhan Lin and Ang Lv proposed
    the idea of MoICE. Hongzhan Lin and Yuhan Chen designed the MoICE router architecture
    and implemented efficient code. Experiments were conducted by Hongzhan Lin, while
    Ang Lv led the writing. Code is available at [https://github.com/p1nksnow/MoICE](https://github.com/p1nksnow/MoICE).
    Corresponding author: Rui Yan ([ruiyan@ruc.edu.cn](ruiyan@ruc.edu.cn))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{chenyuhan5}@xiaomi.com 平等贡献。洪瞻林和昂吕提出了MoICE的想法。洪瞻林和余涵陈设计了MoICE路由器架构并实现了高效的代码。实验由洪瞻林进行，而昂吕主导了写作。代码可在
    [https://github.com/p1nksnow/MoICE](https://github.com/p1nksnow/MoICE) 获取。通讯作者：锐焰
    ([ruiyan@ruc.edu.cn](ruiyan@ruc.edu.cn))'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Many studies have revealed that large language models (LLMs) exhibit uneven
    awareness of different contextual positions. Their limited context awareness can
    lead to overlooking critical information and subsequent task failures. While several
    approaches have been proposed to enhance LLMs’ context awareness, achieving both
    effectiveness and efficiency remains challenging. In this paper, for LLMs utilizing
    RoPE as position embeddings, we introduce a novel method called “Mixture of In-Context
    Experts” (MoICE) to address this challenge. MoICE comprises two key components:
    a router integrated into each attention head within LLMs and a lightweight router-only
    training optimization strategy: (1) MoICE views each RoPE angle as an ‘in-context’
    expert, demonstrated to be capable of directing the attention of a head to specific
    contextual positions. Consequently, each attention head flexibly processes tokens
    using multiple RoPE angles dynamically selected by the router to attend to the
    needed positions. This approach mitigates the risk of overlooking essential contextual
    information. (2) The router-only training strategy entails freezing LLM parameters
    and exclusively updating routers for only a few steps. When applied to open-source
    LLMs including Llama and Mistral, MoICE surpasses prior methods across multiple
    tasks on long context understanding and generation, all while maintaining commendable
    inference efficiency.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究揭示了大型语言模型（LLMs）在不同上下文位置的意识不均。其有限的上下文意识可能导致忽略关键信息及任务失败。尽管已有几种方法提出以增强LLMs的上下文意识，但实现效果与效率的平衡仍然具有挑战性。在本文中，对于利用RoPE作为位置嵌入的LLMs，我们引入了一种新方法称为“上下文专家混合”（MoICE）来应对这一挑战。MoICE包括两个关键组件：一个集成到LLMs每个注意力头中的路由器，以及一种轻量级的仅路由器训练优化策略：（1）MoICE将每个RoPE角度视为一个“上下文”专家，证明其能够将头部的注意力引导到特定的上下文位置。因此，每个注意力头灵活地处理由路由器动态选择的多个RoPE角度的令牌，以关注所需的位置。这种方法减轻了忽略重要上下文信息的风险。（2）仅路由器训练策略涉及冻结LLM参数，仅在少数步骤中更新路由器。当应用于包括Llama和Mistral在内的开源LLMs时，MoICE在长上下文理解和生成的多个任务中超过了先前的方法，同时保持了令人满意的推理效率。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Although large language models (LLMs) have demonstrated impressive capabilities
    across diverse NLP tasks, several studies [[22](#bib.bib22), [6](#bib.bib6)] have
    pointed out that the contextual awareness of LLMs is not as powerful as widely
    believed, constraining their application in tasks demanding extensive contextual
    awareness, such as coherent long text generation [[38](#bib.bib38)] and Retrieval-Augmented
    Generation (RAG,  [[15](#bib.bib15), [4](#bib.bib4), [8](#bib.bib8)]) tasks necessitating
    in-context retrieval [[6](#bib.bib6)]. Liu et al. [[22](#bib.bib22)] identified
    a common issue termed the “lost-in-middle” phenomenon, indicating that LLMs often
    exhibit a weaker awareness of information situated in the middle of the long context
    compared to the beginning or end. Chen et al. [[6](#bib.bib6)] highlighted challenges
    arising from a mathematical property of RoPE [[29](#bib.bib29)], a wide-used positional
    embedding in LLMs, which impedes attention to specific positions within the long
    context. Consequently, if critical information coincides with such positions,
    task performance suffers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在各种自然语言处理任务中展示了令人印象深刻的能力，但几项研究 [[22](#bib.bib22), [6](#bib.bib6)]
    指出 LLMs 的上下文意识并没有广泛认为的那么强大，这限制了它们在需要广泛上下文意识的任务中的应用，例如连贯的长文本生成 [[38](#bib.bib38)]
    和检索增强生成（RAG，[[15](#bib.bib15), [4](#bib.bib4), [8](#bib.bib8)]) 任务，这些任务需要在上下文中进行检索 [[6](#bib.bib6)]。刘等人 [[22](#bib.bib22)]
    发现了一个常见问题，称为“中间丢失”现象，这表明 LLMs 通常对位于长上下文中间的信息的意识较弱，相比于开头或结尾。陈等人 [[6](#bib.bib6)]
    强调了 RoPE [[29](#bib.bib29)] 的数学性质所带来的挑战，RoPE 是 LLMs 中广泛使用的位置嵌入，这阻碍了对长上下文中特定位置的关注。因此，如果关键的信息恰好出现在这些位置，任务表现会受到影响。
- en: 'Many works [[19](#bib.bib19), [38](#bib.bib38), [6](#bib.bib6), [37](#bib.bib37)]
    have attempted to enhance the long-context awareness of LLMs. Central to these
    efforts is the enhancement of attention heads which serve as the linchpin for
    contextual awareness, given that FFNs in language models do not introduce token
    interaction. Chen et al. [[6](#bib.bib6)] proposed an inference algorithm named
    Attention Buckets (AB), which enhanced the context awareness of LLMs by executing
    $N$ inference instances, each with a distinct RoPE angle, and aggregated the outputs
    at the final layer. Zhang et al. [[38](#bib.bib38)] observed the varying awareness
    of attention heads to contextual positions. They proposed an inference algorithm
    named Ms-PoE. Ms-PoE enhances the utility of position-aware heads by re-scaling
    the positional embedding indices, equivalent to assigning each head a unique RoPE
    angle. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Mixture of In-Context Experts
    Enhance LLMs’ Long Context Awareness") illustrates these approaches. However,
    these approaches each come with their own drawbacks: AB conducts excessive redundant
    FFNs calculations, leading to high memory consumption. In Ms-PoE, determining
    a distinct re-scale factor for every attention head needs an additional forward
    pass. Meanwhile, each attention head still depends on a single re-scaled static
    RoPE. As highlighted by AB [[6](#bib.bib6)], this leads to limited awareness of
    certain contextual positions, thereby constraining its potential. Moreover, a
    significant drawback of both AB and Ms-PoE lies in their static assignment of
    the RoPE angle for each attention head throughout the generation. However, as
    the generation progresses, the positions of crucial tokens shift, necessitating
    corresponding adjustments in the required RoPE angles for each head.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究 [[19](#bib.bib19), [38](#bib.bib38), [6](#bib.bib6), [37](#bib.bib37)]
    尝试提升 LLMs 的长上下文意识。这些努力的核心是提升注意力头的能力，因为在语言模型中，FFNs 不引入标记交互。陈等人 [[6](#bib.bib6)]
    提出了一个名为 Attention Buckets（AB）的推理算法，通过执行 $N$ 个推理实例，每个实例具有不同的 RoPE 角度，并在最终层汇总输出，从而增强了
    LLMs 的上下文意识。张等人 [[38](#bib.bib38)] 观察到注意力头对上下文位置的意识有差异。他们提出了一种名为 Ms-PoE 的推理算法。Ms-PoE
    通过重新调整位置嵌入索引来增强位置感知头的效用，相当于为每个头分配一个独特的 RoPE 角度。图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness") 说明了这些方法。然而，这些方法各自都有缺点：AB
    进行过多冗余的 FFNs 计算，导致高内存消耗。在 Ms-PoE 中，为每个注意力头确定一个独特的重新缩放因子需要额外的前向传播。同时，每个注意力头仍然依赖于一个单一的重新缩放的静态
    RoPE。正如 AB [[6](#bib.bib6)] 所强调的，这导致了对某些上下文位置的意识有限，从而限制了其潜力。此外，AB 和 Ms-PoE 的一个显著缺点在于它们在整个生成过程中对每个注意力头静态分配
    RoPE 角度。然而，随着生成的进行，关键标记的位置会发生变化，需要相应地调整每个头所需的 RoPE 角度。
- en: '![Refer to caption](img/ebb584353387501dddfc702bcf3243e3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebb584353387501dddfc702bcf3243e3.png)'
- en: 'Figure 1: Some methods developed to enhance LLMs’ context awareness. (a) Attention
    Buckets [[6](#bib.bib6)] selects $N$ parallel inferences for each input. The outputs
    are then aggregated in the final layer. (b) Ms-PoE [[38](#bib.bib38)] employs
    a unique RoPE angle for each attention head. However, it needs an additional forward
    pass for RoPE angle assignment. (c) MoICE integrates a router within each attention
    head. This novel plug-in selects several of the most suitable RoPE angles for
    each token. The selected RoPE angles collectively contribute to computing the
    attention scores. MoICE demonstrates superior memory efficiency and performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一些用于增强LLM上下文感知的方法。(a) 注意力桶[[6](#bib.bib6)]为每个输入选择$N$个并行推理。然后在最终层中聚合这些输出。(b)
    Ms-PoE[[38](#bib.bib38)]为每个注意力头采用独特的RoPE角度。然而，它需要额外的前向传递来分配RoPE角度。(c) MoICE在每个注意力头中集成了一个路由器。这个新型插件为每个token选择几个最合适的RoPE角度。所选的RoPE角度共同用于计算注意力分数。MoICE表现出优越的内存效率和性能。
- en: 'In this study, we present Mixture of In-Context Experts (MoICE), a novel plug-in
    of LLMs for enhancing context awareness. Specifically, We conceptualize a unique
    RoPE angle as an “in-context expert,” as it can allocate a head’s more attention
    to certain contextual positions [[6](#bib.bib6)]. We integrate a router within
    each attention head, which discerns the potentially important tokens for the head
    and dynamically selects $K$ RoPE angles that provide comprehensive awareness of
    these tokens for attention computation. Through the re-computation of only a few
    query-key dot products, attention patterns computed with selected RoPE angles
    are aggregated to produce the final attention pattern. This approach yields two
    primary advantages: (1) It eliminates unnecessary computational overhead in AB,
    enhancing efficiency. (2) The dynamic expert selection of each head for arbitrary
    tokens introduces flexibility not attained in previous studies. This minimizes
    the risk of the initial RoPE angle assigned to a head failing to work due to crucial
    token positions shifting during generation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们介绍了“上下文专家混合”（MoICE），这是一个用于增强上下文感知的LLM新型插件。具体来说，我们将独特的RoPE角度概念化为“上下文专家”，因为它可以将头的更多注意力分配到特定的上下文位置[[6](#bib.bib6)]。我们在每个注意力头中集成了一个路由器，该路由器辨别出可能对头重要的token，并动态选择$K$个RoPE角度，为这些token提供全面的关注计算。通过重新计算仅少量的查询-键点积，使用选定RoPE角度计算的注意力模式被聚合以生成最终的注意力模式。这种方法带来了两个主要优点：(1)
    它消除了AB中不必要的计算开销，提高了效率。(2) 每个头对任意token的动态专家选择引入了以前研究中未能实现的灵活性。这最小化了在生成过程中由于关键token位置的变化导致初始RoPE角度未能正常工作的风险。
- en: Consequently, MoICE not only surpasses AB’s effectiveness but also achieves
    commendable efficiency. We name our approach as “Mixture of In-Context Experts”
    (MoICE) due to the aggregation of attention patterns calculated with different
    RoPE angles resembling the concept of “Mixture of Experts” (MoE, [[28](#bib.bib28)]).
    When applying MoICE to open-source LLMs, we freeze LLMs’ parameters and conduct
    lightweight training only on the MoICE routers. With only a few quick updates,
    MoICE surpasses many competitive baselines in tasks involving long-context generation
    and understanding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MoICE不仅超越了AB的效果，还实现了令人称赞的效率。我们将我们的方法命名为“上下文专家混合”（MoICE），因为它将用不同RoPE角度计算的注意力模式聚合在一起，类似于“专家混合”（MoE，[[28](#bib.bib28)]）的概念。当将MoICE应用于开源LLM时，我们冻结LLM的参数，仅对MoICE路由器进行轻量级训练。仅通过少量快速更新，MoICE在涉及长上下文生成和理解的任务中超越了许多竞争基线。
- en: In summary, our main contribution is the introduction of MoICE, a novel plug-in
    for enhancing LLMs’ context awareness. It achieves head-and token-specific dynamic
    multiple RoPE angles assignment, outperforms previous methods across various tasks,
    and maintains commendable inference efficiency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的主要贡献是引入了MoICE，一个用于增强LLM上下文感知的新型插件。它实现了头和token特定的动态多RoPE角度分配，在各种任务中超越了以前的方法，并保持了令人称赞的推理效率。
- en: 2 Background
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'We introduce some background of Mixture of In-Context Experts, including (1)
    the rotary position embeddings commonly used by mainstream LLMs, (2) the primary
    problem addressed in this paper: the limited context awareness of LLMs, (3) an
    explanation of the underlying reasons for this limitation, and (4) the Mixture
    of Expert techniques employed to mitigate the limitation.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了上下文专家混合的一些背景，包括（1）主流LLMs常用的旋转位置嵌入，（2）本文解决的主要问题：LLMs的有限上下文意识，（3）这一限制的根本原因解释，以及（4）用于缓解该限制的专家混合技术。
- en: Position embedding
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: Positional embedding is crucial for Transformer [[33](#bib.bib33)] to perceive
    sequence order and compensate for the position-agnostic nature of the attention
    mechanism. In this paper, we mainly focus on LLMs using Rotary Position Embedding
    (RoPE, [[29](#bib.bib29)]) which is the prevalent position embedding in current
    LLMs. We discuss other position embeddings in Appendix [B](#A2 "Appendix B Discussions
    on more position embeddings ‣ Mixture of In-Context Experts Enhance LLMs’ Long
    Context Awareness").
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入对于Transformer [[33](#bib.bib33)] 感知序列顺序至关重要，并弥补了注意力机制的无位置性特点。在本文中，我们主要关注使用旋转位置嵌入（RoPE，[[29](#bib.bib29)])
    的大型语言模型（LLMs），这是当前LLMs中普遍使用的位置嵌入。我们在附录[B](#A2 "附录 B 更多位置嵌入讨论 ‣ 上下文专家混合增强LLMs的长上下文意识")中讨论了其他位置嵌入。
- en: 'In a Transformer layer with $H$-th head. To encode position information, RoPE
    initially applies a rotary matrix to the query and key vectors:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有 $H$-th 头的Transformer层中。为了编码位置信息，RoPE首先将旋转矩阵应用于查询和键向量：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: '|  |  $$\mathbf{R}_{\Theta_{j},n}=\left[\begin{array}[]{cccc}\mathbf{r}_{\theta_{j,0},n}&amp;O&amp;\cdots&amp;O\\
    O&amp;\mathbf{r}_{\theta_{j,1},n}&amp;\cdots&amp;O\\'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  $$\mathbf{R}_{\Theta_{j},n}=\left[\begin{array}[]{cccc}\mathbf{r}_{\theta_{j,0},n}&amp;O&amp;\cdots&amp;O\\
    O&amp;\mathbf{r}_{\theta_{j,1},n}&amp;\cdots&amp;O\\'
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: O&amp;O&amp;\cdots&amp;\mathbf{r}_{\theta_{j,d/2-1},n}\\
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: O&amp;O&amp;\cdots&amp;\mathbf{r}_{\theta_{j,d/2-1},n}\\
- en: \end{array}\right],\text{\ where\ \ }\mathbf{r}_{\theta_{j,i},n}=\left[\begin{array}[]{cc}\cos
    n\theta_{j,i}&amp;-\sin n\theta_{j,i}\\
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right],\text{\ 其中\ \ }\mathbf{r}_{\theta_{j,i},n}=\left[\begin{array}[]{cc}\cos
    n\theta_{j,i}&amp;-\sin n\theta_{j,i}\\
- en: \sin n\theta_{j,i}&amp;\cos n\theta_{j,i}\end{array}\right],O=\left[\begin{array}[]{cc}0&amp;0\\
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \sin n\theta_{j,i}&amp;\cos n\theta_{j,i}\end{array}\right],O=\left[\begin{array}[]{cc}0&amp;0\\
- en: 0&amp;0\end{array}\right].$$  |  | (2) |
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0\end{array}\right].$$  |  | (2) |
- en: 'Here, $\theta_{j,i}=B^{-2i/d}_{j},i\in[0,\cdots,d/2-1]$ in the query-key product
    during attention computation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\theta_{j,i}=B^{-2i/d}_{j},i\in[0,\cdots,d/2-1]$ 在注意力计算中的查询-键乘积中：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: '|  | $1$2 |  | (4) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: Here, $\mathbf{Attn}^{h}_{nm}$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{Attn}^{h}_{nm}$。
- en: Context awareness of LLMs
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的上下文意识
- en: '![Refer to caption](img/94d0fe05375548aec5c04278e6720488.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/94d0fe05375548aec5c04278e6720488.png)'
- en: 'Figure 2: Different RoPE angles $\Theta_{j}$.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 不同的RoPE角度 $\Theta_{j}$。'
- en: LLMs struggle with limited context awareness, significantly impacting their
    performance in tasks like long-text generation [[38](#bib.bib38)], Retrieval-Augmented
    Generation (RAG, [[15](#bib.bib15), [4](#bib.bib4), [8](#bib.bib8)]), and multi-turn
    human-agent interactions [[6](#bib.bib6)] involving complex contexts. Liu et al. [[22](#bib.bib22)]
    identified a problem known as “Lost-in-the-Middle,” where LLMs process the beginning
    and end of the context well but have reduced awareness of the middle. Chen et
    al. [[6](#bib.bib6)] observed that LLMs using RoPE exhibit uneven context awareness,
    favoring certain positions. Peysakhovich et al. [[26](#bib.bib26)] further highlighted
    that LLMs exhibit variable attention to document-level token segments based on
    their contextual positions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在上下文意识有限的问题上挣扎，这显著影响了它们在长文本生成 [[38](#bib.bib38)]、检索增强生成（RAG，[[15](#bib.bib15),
    [4](#bib.bib4), [8](#bib.bib8)]）以及涉及复杂上下文的多轮人机交互 [[6](#bib.bib6)] 中的表现。Liu等人 [[22](#bib.bib22)]
    发现了一个被称为“Lost-in-the-Middle”的问题，即LLMs能够很好地处理上下文的开始和结束，但对中间部分的意识降低。Chen等人 [[6](#bib.bib6)]
    观察到，使用RoPE的LLMs在上下文意识上表现不均，偏向某些位置。Peysakhovich等人 [[26](#bib.bib26)] 进一步指出，LLMs对基于上下文位置的文档级令牌片段的注意力存在变异。
- en: Attention waveforms
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力波形
- en: According to Chen et al. [[6](#bib.bib6)], LLM’s uneven awareness of different
    contextual positions is due to RoPE’s mathematical characteristics. Within RoPE,
    the attention score exhibits “waveforms” when retrieving the same token from the
    context, based on their relative positions. The troughs in these waveforms can
    impair task performance, especially when critical tokens are situated at these
    positions during generation. Different RoPE angles produce waveforms with troughs
    occurring at different positions. These phenomena are depicted in Figure [2](#S2.F2
    "Figure 2 ‣ Context awareness of LLMs ‣ 2 Background ‣ Mixture of In-Context Experts
    Enhance LLMs’ Long Context Awareness"). A detailed derivation of the depicted
    curves in Figure [2](#S2.F2 "Figure 2 ‣ Context awareness of LLMs ‣ 2 Background
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness") is provided
    in Appendix [A](#A1 "Appendix A Attention waveforms ‣ Mixture of In-Context Experts
    Enhance LLMs’ Long Context Awareness").
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Chen 等人 [[6](#bib.bib6)] 的研究，LLM 对不同上下文位置的感知不均匀是由于 RoPE 的数学特性。在 RoPE 中，注意力得分在检索相同标记时会展示“波形”，这取决于它们的相对位置。这些波形中的低谷可能会损害任务性能，特别是当生成过程中关键标记位于这些位置时。不同的
    RoPE 角度产生的波形低谷出现的位置不同。这些现象在图 [2](#S2.F2 "图 2 ‣ LLM 的上下文感知 ‣ 2 背景 ‣ 上下文专家混合增强 LLM
    的长上下文感知") 中有所展示。图 [2](#S2.F2 "图 2 ‣ LLM 的上下文感知 ‣ 2 背景 ‣ 上下文专家混合增强 LLM 的长上下文感知")
    中所示曲线的详细推导见附录 [A](#A1 "附录 A 注意力波形 ‣ 上下文专家混合增强 LLM 的长上下文感知")。
- en: 3 Mixture of In-Context Experts
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 上下文专家混合
- en: In this section, we first introduce the core component of MoICE, the MoICE router,
    detailed in Section [3.1](#S3.SS1 "3.1 Architecture ‣ 3 Mixture of In-Context
    Experts ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness").
    Subsequently, we delve into the optimization of MoICE in Section [3.2](#S3.SS2
    "3.2 Router-only training ‣ 3 Mixture of In-Context Experts ‣ Mixture of In-Context
    Experts Enhance LLMs’ Long Context Awareness"). Figure [3](#S3.F3 "Figure 3 ‣
    3.1 Architecture ‣ 3 Mixture of In-Context Experts ‣ Mixture of In-Context Experts
    Enhance LLMs’ Long Context Awareness") provides an overview of MoICE. The discussion
    in this section focuses solely on a single layer of transformer for clarity, with
    the same principles applying to any other layers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍 MoICE 的核心组件——MoICE 路由器，详细内容见第 [3.1](#S3.SS1 "3.1 结构 ‣ 3 上下文专家混合
    ‣ 上下文专家混合增强 LLM 的长上下文感知") 节。随后，我们在第 [3.2](#S3.SS2 "3.2 仅路由器训练 ‣ 3 上下文专家混合 ‣ 上下文专家混合增强
    LLM 的长上下文感知") 节深入探讨 MoICE 的优化。图 [3](#S3.F3 "图 3 ‣ 3.1 结构 ‣ 3 上下文专家混合 ‣ 上下文专家混合增强
    LLM 的长上下文感知") 提供了 MoICE 的概述。本节讨论仅针对一个层的变压器，为了清晰起见，其他层也适用相同的原则。
- en: 3.1 Architecture
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 结构
- en: '![Refer to caption](img/a6e74e85b6a440dbd00b26a2864b0ce8.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/a6e74e85b6a440dbd00b26a2864b0ce8.png)'
- en: 'Figure 3: The structure of MoICE. Only the router’s parameters are trainable
    when plugged into an LLM. For clarity, the figure illustrates a single head, with
    $N$=2 as toy demonstration examples.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: MoICE 的结构。插入到 LLM 时，只有路由器的参数是可训练的。为清晰起见，图中展示了单个头部，以 $N$=2 作为示例。'
- en: 'We aim to design an enhanced attention mechanism in LLMs that dynamically attends
    to crucial information across various contextual positions required for completing
    the head’s function. As a result, we can mitigate the performance drop caused
    by inadequate context awareness. Motivated by insights of Chen et al. [[6](#bib.bib6)],
    who demonstrated that a distinct RoPE angle $\Theta_{j}$ could direct the attention
    heads more focus on specific contextual positions, we propose the integration
    of a contextual-aware routing mechanism. This routing mechanism is designed to
    select the appropriate RoPE angles for processing a token. We implement the router
    as a Multi-Layer Perceptron (MLP) with the SiLU activation function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在设计一种改进的注意力机制，使 LLM 能动态地关注完成头部功能所需的各种上下文位置的关键信息。因此，我们可以减少由于上下文感知不足而导致的性能下降。受到
    Chen 等人 [[6](#bib.bib6)] 的启发，他们展示了不同的 RoPE 角度 $\Theta_{j}$ 可以使注意力头更加专注于特定的上下文位置，我们提出了集成上下文感知路由机制。该路由机制旨在选择适当的
    RoPE 角度来处理一个标记。我们将路由器实现为具有 SiLU 激活函数的多层感知器（MLP）：
- en: '|  | $\texttt{Router}\left(\mathbf{q}\right):=\mathbf{W}_{3}\left(\texttt{SiLU}\left(\mathbf{W}_{1}\mathbf{q}\right)\odot\left(\mathbf{W}_{2}\mathbf{q}\right)\right).$
    |  | (5) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{Router}\left(\mathbf{q}\right):=\mathbf{W}_{3}\left(\texttt{SiLU}\left(\mathbf{W}_{1}\mathbf{q}\right)\odot\left(\mathbf{W}_{2}\mathbf{q}\right)\right).$
    |  | (5) |'
- en: Here, q is the query vector that encapsulates the contextual information for
    the task. This router input indicates the specific information for which the current
    token is “querying.” $\mathbf{W}_{1},\mathbf{W}_{2}\in\mathbb{R}^{N\times d}$
    denotes the number of the number of RoPE angle candidates. Considering each head’s
    distinct function [[24](#bib.bib24), [35](#bib.bib35), [23](#bib.bib23)], we integrate
    a router into every attention head in the LLM. Notably, a router’s decision is
    independent of other heads and dynamic to the context.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，q是封装任务上下文信息的查询向量。这个路由器输入表示当前token正在“查询”的具体信息。$\mathbf{W}_{1},\mathbf{W}_{2}\in\mathbb{R}^{N\times
    d}$ 表示RoPE角度候选的数量。考虑到每个头的独特功能[[24](#bib.bib24), [35](#bib.bib35), [23](#bib.bib23)]，我们将一个路由器集成到LLM的每个注意力头中。值得注意的是，路由器的决策独立于其他头，并且对上下文是动态的。
- en: 'As defined in Eq. [5](#S3.E5 "In 3.1 Architecture ‣ 3 Mixture of In-Context
    Experts ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"),
    the router outputs an $N$-th head:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如公式[5](#S3.E5 "In 3.1 Architecture ‣ 3 Mixture of In-Context Experts ‣ Mixture
    of In-Context Experts Enhance LLMs’ Long Context Awareness")定义的，路由器输出第$N$个头：
- en: '|  |  | $\displaystyle\text{TopK-Indices}_{n}^{h}=\texttt{argsort}(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right))[:K],$
    |  | (6) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{TopK-Indices}_{n}^{h}=\texttt{argsort}(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right))[:K],$
    |  | (6) |'
- en: '|  |  | $\displaystyle\textbf{p}_{n}^{h}=\texttt{Softmax}\left(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right)[\text{TopK-Indices}_{n}^{h}]\right),$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textbf{p}_{n}^{h}=\texttt{Softmax}\left(\texttt{Router}\left(\mathbf{q}_{n}^{h}\right)[\text{TopK-Indices}_{n}^{h}]\right),$
    |  |'
- en: 'where $\mathbf{q}_{n}^{h}$:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{q}_{n}^{h}$:'
- en: '|  | $1$2 |  | (7) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 'Considering RoPE angles impact how attention heads allocate attention and focus
    on specific contextual positions, we view each distinct RoPE angle as an in-context
    expert, in contrast to traditional in-weight experts [[14](#bib.bib14), [17](#bib.bib17),
    [10](#bib.bib10)], where the experts are learnable parameter weights. Given that
    these in-context experts together augment LLMs’ context awareness, we term this
    method Mixture of In-Context Experts (MoICE, Section [3](#S3 "3 Mixture of In-Context
    Experts ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness")).
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Architecture ‣ 3 Mixture of In-Context Experts
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness") illustrates
    the overview of MoICE. Our proposed MoICE has three major advantages:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到RoPE角度影响注意力头如何分配注意力并专注于特定的上下文位置，我们将每个不同的RoPE角度视为上下文专家，与传统的权重专家[[14](#bib.bib14),
    [17](#bib.bib17), [10](#bib.bib10)]不同，其中专家是可学习的参数权重。鉴于这些上下文专家共同增强了LLMs的上下文感知，我们将这种方法称为上下文专家混合（MoICE，第[3](#S3
    "3 Mixture of In-Context Experts ‣ Mixture of In-Context Experts Enhance LLMs’
    Long Context Awareness")节）。图[3](#S3.F3 "Figure 3 ‣ 3.1 Architecture ‣ 3 Mixture
    of In-Context Experts ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context
    Awareness")展示了MoICE的概述。我们提出的MoICE具有三个主要优点：
- en: (1) We only add additional computational overhead to the query-key dot products,
    resulting in a minimal increase in memory usage and a negligible impact on inference
    speed (Section [4.2](#S4.SS2 "4.2 Long context understanding and generation ‣
    4 Experiment ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness")).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 我们仅在查询-键点积计算中增加了额外的计算开销，导致内存使用量的增加非常有限，并对推理速度几乎没有影响（第[4.2](#S4.SS2 "4.2
    Long context understanding and generation ‣ 4 Experiment ‣ Mixture of In-Context
    Experts Enhance LLMs’ Long Context Awareness")节）。
- en: (2) MoICE dynamically selects suitable RoPE angles token-wise and head-wise,
    offering unprecedented flexibility and unlocking the full potential of each attention
    head.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (2) MoICE动态选择适合的RoPE角度，按token和head进行，提供了前所未有的灵活性，并充分发挥了每个注意力头的潜力。
- en: '(3) Concerning LLMs’ context awareness enhancement, MoICE addresses a longstanding
    issue: the relative position of the relevant information will shift during generation,
    leading to previous static modification of the attention heads [[6](#bib.bib6),
    [36](#bib.bib36)] will be sub-optimal during practical generation. The contextual-aware
    dynamic routing in MoICE is not bothered by this issue.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 关于LLMs的上下文感知增强，MoICE解决了一个长期存在的问题：相关信息的相对位置在生成过程中会发生变化，这导致之前对注意力头的静态修改[[6](#bib.bib6),
    [36](#bib.bib36)]在实际生成过程中效果不佳。MoICE中的上下文感知动态路由不会受此问题影响。
- en: 3.2 Router-only training
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 仅路由器训练
- en: To train the newly incorporated MoICE routers in LLMs, the most straightforward
    way is to simultaneously update the LLMs’ parameters alongside the routers. However,
    updating the original LLMs’ parameters can result in catastrophic forgetting.
    Therefore, we propose a more effective and efficient strategy, the router-only
    training strategy, which freezes the LLMs’ parameters and solely optimizing the
    routers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 LLM 中训练新引入的 MoICE 路由器，最简单的方法是同时更新 LLM 的参数和路由器。然而，更新原始 LLM 的参数可能导致灾难性遗忘。因此，我们提出了一种更有效和高效的策略，即仅路由器训练策略，该策略冻结
    LLM 的参数，只优化路由器。
- en: 'Given an input sequence, we calculate the negative log-likelihood loss ($\mathcal{L}_{nll}$:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入序列，我们计算负对数似然损失（$\mathcal{L}_{nll}$：
- en: '|  | $\mathcal{L}_{aux}=\alpha\cdot N\cdot\sum_{j=1}^{N}\mathbf{F}_{j}\cdot\mathbf{P}_{j}.$
    |  | (8) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{aux}=\alpha\cdot N\cdot\sum_{j=1}^{N}\mathbf{F}_{j}\cdot\mathbf{P}_{j}.$
    |  | (8) |'
- en: 'Eq. [8](#S3.E8 "In 3.2 Router-only training ‣ 3 Mixture of In-Context Experts
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness") avoids
    the router falling into a sub-optimal solution favoring specific experts overwhelmingly,
    as its minimal is achieved when the routing probability is uniform. Here, $\alpha$:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [8](#S3.E8 "在 3.2 仅路由器训练 ‣ 3 上下文专家混合 ‣ 上下文专家混合提升 LLM 的长上下文感知能力") 避免了路由器陷入过度偏向特定专家的次优解，因为其最小值在路由概率均匀时达到。这里，$\alpha$：
- en: '|  |  | $\displaystyle\mathbf{F}_{j}=\frac{1}{T\times H}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbbm{1}\{j\in\text{TopK-Indices}_{t}^{h}\},$
    |  | (9) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbf{F}_{j}=\frac{1}{T\times H}\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbbm{1}\{j\in\text{TopK-Indices}_{t}^{h}\},$
    |  | (9) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Our overall training objective is to minimize the following loss:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的整体训练目标是最小化以下损失：
- en: '|  | $\mathcal{L}=\mathcal{L}_{nll}+\mathcal{L}_{aux}.$ |  | (10) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathcal{L}_{nll}+\mathcal{L}_{aux}.$ |  | (10) |'
- en: 4 Experiment
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Setup
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: To evaluate the efficacy of MoICE, we implement it with open-source LLMs, which
    we will introduce later, and conduct lightweight training of MoICE routers on
    a small and general dataset. Subsequently, we evaluate the enhanced LLM’s capability
    to zero-shot undertake multiple tasks in long context understanding and generation,
    as detailed in Section [4.2](#S4.SS2 "4.2 Long context understanding and generation
    ‣ 4 Experiment ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness")
    and Section [4.3](#S4.SS3 "4.3 Retrieval-augmented generation (RAG) ‣ 4 Experiment
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 MoICE 的有效性，我们在开源 LLM 上实现了它，这些 LLM 将在后文介绍，并在一个小型通用数据集上进行轻量级的 MoICE 路由器训练。随后，我们评估增强后的
    LLM 在长上下文理解和生成中的零-shot 多任务能力，详见第 [4.2](#S4.SS2 "4.2 长上下文理解和生成 ‣ 4 实验 ‣ 上下文专家混合提升
    LLM 的长上下文感知能力") 和第 [4.3](#S4.SS3 "4.3 检索增强生成（RAG） ‣ 4 实验 ‣ 上下文专家混合提升 LLM 的长上下文感知能力")
    节。
- en: Training data
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据
- en: We use a training dataset¹¹1[https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest](https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest)
    which extracts the one thousand longest entries from OpenHermes [[31](#bib.bib31)].
    OpenHermes is a multi-source integrated dataset containing high-quality synthetically
    generated instruction and chat samples. A detailed analysis of other training
    data is in Section [5.3](#S5.SS3 "5.3 MoICE is robust to training data ‣ 5 Method
    analysis ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness").
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个训练数据集¹¹1[https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest](https://huggingface.co/datasets/HuggingFaceH4/OpenHermes-2.5-1k-longest)，从
    OpenHermes 中提取了一千个最长的条目 [[31](#bib.bib31)]。OpenHermes 是一个多源集成数据集，包含高质量的合成生成指令和对话样本。其他训练数据的详细分析见第
    [5.3](#S5.SS3 "5.3 MoICE 对训练数据的鲁棒性 ‣ 5 方法分析 ‣ 上下文专家混合提升 LLM 的长上下文感知能力") 节。
- en: Hyperparameters for MoICE-router-only training
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MoICE-router-only 训练的超参数
- en: We froze all the original parameters of the open-source LLMs we used and only
    trained the MoICE router. Following Attention Buckets [[6](#bib.bib6)], we employed
    the RoPE angle set of $N=7$=7 bases to ensure a fair comparison with [[6](#bib.bib6)].
    Section [4.2](#S4.SS2 "4.2 Long context understanding and generation ‣ 4 Experiment
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness") introduces
    our baselines in detail. Section [5](#S5 "5 Method analysis ‣ Mixture of In-Context
    Experts Enhance LLMs’ Long Context Awareness") delves into the impact of set size
    and the number of selected items.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们冻结了所有开源 LLM 的原始参数，只训练了 MoICE 路由器。按照 Attention Buckets [[6](#bib.bib6)] 的方法，我们采用了
    $N=7$=7 基础的 RoPE 角度集合，以确保与 [[6](#bib.bib6)] 的公平比较。第 [4.2](#S4.SS2 "4.2 Long context
    understanding and generation ‣ 4 Experiment ‣ Mixture of In-Context Experts Enhance
    LLMs’ Long Context Awareness") 节详细介绍了我们的基准。第 [5](#S5 "5 Method analysis ‣ Mixture
    of In-Context Experts Enhance LLMs’ Long Context Awareness") 节深入探讨了集合大小和所选项数量的影响。
- en: We implement a warm-up strategy comprising 20% of the total steps, with a maximum
    learning rate of 0.0001. The batch size is 128. $\alpha$ is set as 0.3. We train
    the MoICE routers for 1 epoch (about 8 minutes) on four A800-80G GPUs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实施了一种热身策略，占总步骤的 20%，最大学习率为 0.0001。批量大小为 128。$\alpha$ 设置为 0.3。我们在四台 A800-80G
    GPU 上训练 MoICE 路由器 1 个周期（约 8 分钟）。
- en: 4.2 Long context understanding and generation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 长期上下文理解与生成
- en: 'Following the L-Eval benchmark [[1](#bib.bib1)], we evaluated the LLM with
    tasks categorized into two main groups: closed-ended and open-ended tasks. Closed-ended
    tasks primarily focus on the capacity for understanding and reasoning within long
    contexts, including tasks like multiple-choice questions from QuALITY [[3](#bib.bib3)],
    Coursera, ²²2[https://coursera.org/](https://coursera.org/) TOEFL [[9](#bib.bib9)],
    and True/False question answering from SFiction. ³³3[https://github.com/nschaetti/SFGram-dataset](https://github.com/nschaetti/SFGram-dataset)
    On the other hand, open-ended tasks include summarization generation and open-format
    question-answering tasks, requiring extracting information from lengthy in-context
    documents. The open-ended tasks comprise a subset of 181 questions drawn from
    29 diverse long documents.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 L-Eval 基准 [[1](#bib.bib1)]，我们对 LLM 进行了任务评估，这些任务分为两大类：封闭式和开放式任务。封闭式任务主要关注在长期上下文中的理解和推理能力，包括来自
    QuALITY [[3](#bib.bib3)]、Coursera、²²2[https://coursera.org/](https://coursera.org/)
    TOEFL [[9](#bib.bib9)] 和 SFiction 的是/否问题回答任务。 ³³3[https://github.com/nschaetti/SFGram-dataset](https://github.com/nschaetti/SFGram-dataset)
    另一方面，开放式任务包括摘要生成和开放格式问答任务，需要从冗长的上下文文档中提取信息。开放式任务包括从 29 个不同的长期文档中抽取的 181 个问题的子集。
- en: Baselines and open-source LLMs
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准和开源 LLM
- en: 'In evaluating the efficacy of our proposed MoICE, we compare it against several
    state-of-the-art methods known for enhancing the capacity of LLMs to understand
    and generate long contexts. These baselines include two context extrapolation
    techniques: Positional Interpolation (PI, [[5](#bib.bib5)]) and Dynamic NTK [[13](#bib.bib13)].
    Additionally, we consider two inference algorithms for context-awareness enhancement:
    Ms-PoE [[38](#bib.bib38)] and Attention Buckets [[6](#bib.bib6)].'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估我们提出的 MoICE 的效果时，我们将其与几种先进的方法进行了比较，这些方法以增强 LLM 理解和生成长期上下文的能力而闻名。这些基准包括两种上下文外推技术：位置插值
    (PI, [[5](#bib.bib5)]) 和动态 NTK [[13](#bib.bib13)]。此外，我们还考虑了两种用于上下文感知增强的推理算法：Ms-PoE [[38](#bib.bib38)]
    和 Attention Buckets [[6](#bib.bib6)]。
- en: 'We evaluate all these methods alongside our MoICE on two representative open-source
    LLMs that utilize RoPE for positional embeddings: Llama2-7B-chat [[32](#bib.bib32)]
    with a pre-trained context length of 4,096, and Mistral-7B-Instruct-v0.1 [[16](#bib.bib16)].
    Mistral-7B employs a sliding window attention (SWA) mechanism with a window size
    of 4,096 tokens, enabling it to accommodate longer contexts than the default.
    Therefore, we conduct experiments with a context length of 8,192 on Mistral-7B,
    using SWA as the exclusive baseline for comparison. For PI and Dynamic NTK, we
    apply a scaling ratio of 1.5, while for the remaining baselines, we adhere to
    the hyperparameters specified in their original papers. All methods are tested
    on a single A800-80G GPU, except for applying AB to Mistral-7B-8k, which needs
    2 GPUs due to substantial memory requirements.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两种代表性开源 LLM 上评估了这些方法和我们的 MoICE，这些 LLM 使用 RoPE 进行位置嵌入：Llama2-7B-chat [[32](#bib.bib32)]
    预训练上下文长度为 4,096，以及 Mistral-7B-Instruct-v0.1 [[16](#bib.bib16)]。Mistral-7B 使用滑动窗口注意力（SWA）机制，窗口大小为
    4,096 个标记，使其能够容纳比默认设置更长的上下文。因此，我们在 Mistral-7B 上进行上下文长度为 8,192 的实验，将 SWA 作为唯一的基准进行比较。对于
    PI 和 Dynamic NTK，我们应用了 1.5 的缩放比例，而对于其余基准，我们遵循其原始论文中指定的超参数。除了将 AB 应用到 Mistral-7B-8k
    需要 2 个 GPU 以外，所有方法均在单个 A800-80G GPU 上测试。
- en: 'Table 1: Experimental results on the L-Eval Benchmark [[1](#bib.bib1)]. Applying
    to various models, MoICE demonstrate superior performance compared to previous
    competitive approaches. We emphasize the highest score in bold.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 L-Eval 基准测试 [[1](#bib.bib1)] 上的实验结果。应用于各种模型时，MoICE 展现出比以往竞争方法更优越的性能。我们强调了**最高分数**。
- en: '| Method | Closed - Ended Task | Open - Ended Task |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 闭合任务 | 开放任务 |'
- en: '| Coursera | QuALITY | TOEFL | SFiction | Average | wins | ties | win-rate%^*
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Coursera | QuALITY | TOEFL | SFiction | 平均 | 胜数 | 平局数 | 胜率%^* |'
- en: '| Llama2-7B-chat [[32](#bib.bib32)] | 36.77 | 38.12 | 55.02 | 60.16 | 47.52
    | 68 | 117 | 34.94 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-chat [[32](#bib.bib32)] | 36.77 | 38.12 | 55.02 | 60.16 | 47.52
    | 68 | 117 | 34.94 |'
- en: '| + Fine-tuning | 32.85 | 30.20 | 51.30 | 59.38 | 43.43 | 65 | 91 | 30.52 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| + Fine-tuning | 32.85 | 30.20 | 51.30 | 59.38 | 43.43 | 65 | 91 | 30.52 |'
- en: '| + PI [[5](#bib.bib5)] | 38.23 | 38.61 | 56.51 | 61.72 | 48.77 | 76 | 112
    | 36.46 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| + PI [[5](#bib.bib5)] | 38.23 | 38.61 | 56.51 | 61.72 | 48.77 | 76 | 112
    | 36.46 |'
- en: '| + Dynamic NTK [[13](#bib.bib13)] | 40.26 | 39.11 | 55.76 | 62.50 | 49.41
    | 82 | 112 | 38.12 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| + Dynamic NTK [[13](#bib.bib13)] | 40.26 | 39.11 | 55.76 | 62.50 | 49.41
    | 82 | 112 | 38.12 |'
- en: '| + Ms-PoE [[38](#bib.bib38)] | 39.24 | 40.10 | 55.76 | 63.28 | 49.60 | 86
    | 110 | 38.95 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| + Ms-PoE [[38](#bib.bib38)] | 39.24 | 40.10 | 55.76 | 63.28 | 49.60 | 86
    | 110 | 38.95 |'
- en: '| + AB [[6](#bib.bib6)] | 40.41 | 41.09 | 56.88 | 61.72 | 50.02 | 85 | 114
    | 39.23 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| + AB [[6](#bib.bib6)] | 40.41 | 41.09 | 56.88 | 61.72 | 50.02 | 85 | 114
    | 39.23 |'
- en: '| + MoICE (Ours) | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 | 89 | 118 | 40.88
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| + MoICE（我们的） | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 | 89 | 118 | 40.88 |'
- en: '| Mistral-7B-Instruct-8k [[16](#bib.bib16)] | 45.20 | 44.06 | 62.08 | 61.72
    | 53.27 | 71 | 105 | 34.11 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-8k [[16](#bib.bib16)] | 45.20 | 44.06 | 62.08 | 61.72
    | 53.27 | 71 | 105 | 34.11 |'
- en: '| + Fine-tuning | 25.29 | 26.73 | 25.65 | 50.00 | 31.92 | 53 | 85 | 26.38 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| + Fine-tuning | 25.29 | 26.73 | 25.65 | 50.00 | 31.92 | 53 | 85 | 26.38 |'
- en: '| + SWA | 44.77 | 42.57 | 62.08 | 60.94 | 52.59 | 73 | 89 | 32.45 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| + SWA | 44.77 | 42.57 | 62.08 | 60.94 | 52.59 | 73 | 89 | 32.45 |'
- en: '| + PI [[5](#bib.bib5)] | 44.19 | 44.06 | 64.68 | 62.50 | 53.86 | 73 | 96 |
    33.43 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| + PI [[5](#bib.bib5)] | 44.19 | 44.06 | 64.68 | 62.50 | 53.86 | 73 | 96 |
    33.43 |'
- en: '| + Dynamic NTK [[13](#bib.bib13)] | 45.35 | 42.08 | 62.08 | 63.28 | 53.20
    | 78 | 103 | 35.77 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| + Dynamic NTK [[13](#bib.bib13)] | 45.35 | 42.08 | 62.08 | 63.28 | 53.20
    | 78 | 103 | 35.77 |'
- en: '| + Ms-PoE [[38](#bib.bib38)] | 46.37 | 45.05 | 61.34 | 57.03 | 52.45 | 84
    | 106 | 37.84 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| + Ms-PoE [[38](#bib.bib38)] | 46.37 | 45.05 | 61.34 | 57.03 | 52.45 | 84
    | 106 | 37.84 |'
- en: '| + AB [[6](#bib.bib6)] | 46.08 | 42.57 | 62.08 | 62.50 | 53.31 | 87 | 110
    | 39.22 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| + AB [[6](#bib.bib6)] | 46.08 | 42.57 | 62.08 | 62.50 | 53.31 | 87 | 110
    | 39.22 |'
- en: '| + MoICE (Ours) | 47.82 | 46.53 | 64.68 | 62.50 | 55.38 | 85 | 117 | 39.36
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| + MoICE（我们的） | 47.82 | 46.53 | 64.68 | 62.50 | 55.38 | 85 | 117 | 39.36 |'
- en: '*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Following [[1](#bib.bib1)], win-rate = (win counts + 0.5 * tie counts)
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据 [[1](#bib.bib1)]，胜率 =（胜利次数 + 0.5 * 平局次数）
- en: 'Table 2: Practical inference time (in minutes) / GPU memory costs (GB) on a
    single A800-80G GPU for each method applied to Llama2-7B-chat (top) and Mistral-7B-Instruct-8k
    (bottom), respectively. Due to out-of-memory issues, AB can not accomplish many
    tasks, denoted as OOM in the table.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：每种方法在 Llama2-7B-chat（顶部）和 Mistral-7B-Instruct-8k（底部）上单个 A800-80G GPU 上的实际推理时间（以分钟为单位）/
    GPU 内存消耗（GB）。由于内存不足问题，AB 无法完成许多任务，表中标记为 OOM。
- en: '| Method | Coursera $\downarrow$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Coursera $\downarrow$ |'
- en: '| AB [[6](#bib.bib6)] | 10.9 / 78.7 | 18.1 / 62.5 | 19.9 / 56.5 | 5.0 / 33.2
    | 45.9 / 78.2 | 20.0 / 61.8 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AB [[6](#bib.bib6)] | 10.9 / 78.7 | 18.1 / 62.5 | 19.9 / 56.5 | 5.0 / 33.2
    | 45.9 / 78.2 | 20.0 / 61.8 |'
- en: '| Ms-PoE [[38](#bib.bib38)] | 4.1 / 27.2 | 6.0 / 27.8 | 6.7 / 28.6 | 6.0 /
    27.8 | 20.2 / 28.9 | 8.6 / 28.1 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Ms-PoE [[38](#bib.bib38)] | 4.1 / 27.2 | 6.0 / 27.8 | 6.7 / 28.6 | 6.0 /
    27.8 | 20.2 / 28.9 | 8.6 / 28.1 |'
- en: '| MoICE (Ours) | 5.0 / 19.6 | 11.0 / 19.7 | 10.2 / 19.5 | 1.6 / 15.2 | 34.2
    / 23.2 | 12.4 / 19.4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MoICE (我们的方法) | 5.0 / 19.6 | 11.0 / 19.7 | 10.2 / 19.5 | 1.6 / 15.2 | 34.2
    / 23.2 | 12.4 / 19.4 |'
- en: '| AB [[6](#bib.bib6)] | OOM | OOM | 37.2 / 71.4 | OOM | OOM | N/A |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AB [[6](#bib.bib6)] | OOM | OOM | 37.2 / 71.4 | OOM | OOM | 不适用 |'
- en: '| Ms-PoE [[38](#bib.bib38)] | 14.1 / 50.3 | 11.2 / 48.4 | 9.8 / 25.4 | 4.5
    / 50.3 | 72.8 / 62.4 | 22.5 / 47.4 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Ms-PoE [[38](#bib.bib38)] | 14.1 / 50.3 | 11.2 / 48.4 | 9.8 / 25.4 | 4.5
    / 50.3 | 72.8 / 62.4 | 22.5 / 47.4 |'
- en: '| MoICE (Ours) | 13.4 / 25.7 | 7.7 / 22.9 | 11.3 / 20.4 | 2.3 / 22.8 | 77.8
    / 29.3 | 22.5 / 24.2 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MoICE (我们的方法) | 13.4 / 25.7 | 7.7 / 22.9 | 11.3 / 20.4 | 2.3 / 22.8 | 77.8
    / 29.3 | 22.5 / 24.2 |'
- en: Evaluation metrics
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价指标
- en: We adopt the exact match for closed-ended tasks. For open-ended tasks, we employ
    GPT-4-Turbo [[25](#bib.bib25)] as the judge to evaluate the effectiveness of various
    enhancement methods on open-source LLMs. This evaluation compares their performance
    against GPT3.5-Turbo-16k-0613 across 181 questions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于封闭式任务，我们采用了精确匹配。对于开放式任务，我们使用GPT-4-Turbo [[25](#bib.bib25)]作为评判者，评估各种增强方法在开源LLMs上的有效性。该评估将其性能与GPT3.5-Turbo-16k-0613在181个问题上的表现进行比较。
- en: Results and analysis
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果与分析
- en: We report our experimental results in Table [1](#S4.T1 "Table 1 ‣ Baselines
    and open-source LLMs ‣ 4.2 Long context understanding and generation ‣ 4 Experiment
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"). MoICE
    significantly enhances the overall performance of Llama-2-7B-chat (with p-value
    < 0.02 in the t-test) in both closed-ended and open-ended tasks. On Mistral, MoICE
    outperforms all baseline models significantly (p-value < 0.02). Standard fine-tuning
    degrades the performance of original LLMs, demonstrating catastrophic forgetting
    and proving that the improvement of MoICE does not stem from more training. These
    results underscore MoICE’s efficacy in enhancing LLMs’ ability to understand and
    generate long contexts, both of which require high context awareness. Furthermore,
    these results underscore the broad applicability of MoICE across different LLMs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [1](#S4.T1 "Table 1 ‣ Baselines and open-source LLMs ‣ 4.2 Long context
    understanding and generation ‣ 4 Experiment ‣ Mixture of In-Context Experts Enhance
    LLMs’ Long Context Awareness")中报告了实验结果。MoICE显著提升了Llama-2-7B-chat在封闭式和开放式任务中的整体表现（t检验的p值<0.02）。在Mistral上，MoICE显著优于所有基线模型（p值<0.02）。标准的微调会降低原始LLMs的性能，显示出灾难性遗忘，并证明MoICE的改进并非源于更多训练。这些结果突显了MoICE在增强LLMs理解和生成长上下文能力方面的有效性，这两者都需要高上下文意识。此外，这些结果强调了MoICE在不同LLMs中的广泛适用性。
- en: Regarding efficiency, we provide practical inference time and memory costs associated
    with AB, Ms-PoE, and MoICE in Table [2](#S4.T2 "Table 2 ‣ Baselines and open-source
    LLMs ‣ 4.2 Long context understanding and generation ‣ 4 Experiment ‣ Mixture
    of In-Context Experts Enhance LLMs’ Long Context Awareness"). For a fair comparison,
    we utilize Flash Attention 2 [[11](#bib.bib11)] across all approaches. While achieving
    superior overall performance, MoICE remains at an inference speed similar to Ms-PoE
    and notably excels in memory efficiency compared to these two baselines.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关于效率，我们在表格 [2](#S4.T2 "Table 2 ‣ Baselines and open-source LLMs ‣ 4.2 Long context
    understanding and generation ‣ 4 Experiment ‣ Mixture of In-Context Experts Enhance
    LLMs’ Long Context Awareness")中提供了AB、Ms-PoE和MoICE的实际推理时间和内存开销。为了公平比较，我们在所有方法中使用了Flash
    Attention 2 [[11](#bib.bib11)]。尽管MoICE在整体性能上表现优越，但其推理速度与Ms-PoE相似，并且在内存效率方面明显优于这两个基线方法。
- en: 4.3 Retrieval-augmented generation (RAG)
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 检索增强生成（RAG）
- en: Retrieval-augmented generation (RAG) tasks involve retrieving numerous documents
    related to the current generation. The retrieved documents are arranged in the
    context. RAG necessitates that LLMs have robust context awareness to pinpoint
    crucial documents, process the retrieved information effectively, and integrate
    it to generate responses.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）任务涉及检索与当前生成相关的众多文档。这些检索到的文档被安排在上下文中。RAG要求LLMs具有强大的上下文意识，以确定关键文档、有效处理检索到的信息，并将其整合以生成响应。
- en: Following [[6](#bib.bib6), [38](#bib.bib38)], we employ the MDQA task to evaluate
    the efficacy of MoICE in enhancing LLMs’ performance in RAG tasks. Meanwhile,
    MDQA offers the bonus of allowing flexible control over the location of documents,
    enabling a more precise evaluation of LLMs’ context awareness across various contextual
    positions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [[6](#bib.bib6), [38](#bib.bib38)]，我们采用MDQA任务来评估MoICE在提高LLM在RAG任务中表现的效果。同时，MDQA还允许灵活控制文档的位置，从而更精确地评估LLM在各种上下文位置的上下文意识。
- en: 'Table 3: The experiment results on the MDQA task. MoICE achieve superior average
    performance compared to previous competitive approaches. We emphasize the highest
    score in bold.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：MDQA任务的实验结果。MoICE在平均性能上优于以前的竞争方法。我们将最高得分以**粗体**标出。
- en: '| Method | 1 | 3 | 5 | 7 | 10 | Gap | Avg. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1 | 3 | 5 | 7 | 10 | 差距 | 平均值 |'
- en: '| Llama2-7B-chat | 64.14 | 65.95 | 64.97 | 62.67 | 67.53 | 4.86 | 65.05 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-chat | 64.14 | 65.95 | 64.97 | 62.67 | 67.53 | 4.86 | 65.05 |'
- en: '| + Ms-PoE [[38](#bib.bib38)] | 66.06 | 64.29 | 63.99 | 62.22 | 64.75 | 3.84
    | 64.34 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| + Ms-PoE [[38](#bib.bib38)] | 66.06 | 64.29 | 63.99 | 62.22 | 64.75 | 3.84
    | 64.34 |'
- en: '| + AB [[5](#bib.bib5)] | 66.36 | 66.14 | 65.25 | 63.20 | 64.93 | 3.16 | 65.18
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| + AB [[5](#bib.bib5)] | 66.36 | 66.14 | 65.25 | 63.20 | 64.93 | 3.16 | 65.18
    |'
- en: '| + MoICE (Ours) | 65.50 | 66.33 | 65.61 | 64.11 | 65.84 | 2.22 | 65.48 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| + MoICE (我们) | 65.50 | 66.33 | 65.61 | 64.11 | 65.84 | 2.22 | 65.48 |'
- en: '| Method | 1 | 8 | 15 | 23 | 30 | Gap | Avg. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 1 | 8 | 15 | 23 | 30 | 差距 | 平均值 |'
- en: '| Mistral-7B-Instruct-8k | 58.38 | 47.42 | 46.97 | 49.68 | 50.81 | 11.41 |
    50.65 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-8k | 58.38 | 47.42 | 46.97 | 49.68 | 50.81 | 11.41 |
    50.65 |'
- en: '| + Ms-PoE [[38](#bib.bib38)] | 52.76 | 41.24 | 42.80 | 42.90 | 43.58 | 11.52
    | 44.66 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| + Ms-PoE [[38](#bib.bib38)] | 52.76 | 41.24 | 42.80 | 42.90 | 43.58 | 11.52
    | 44.66 |'
- en: '| + AB [[5](#bib.bib5)] | 58.57 | 47.57 | 47.12 | 49.83 | 50.96 | 11.45 | 50.81
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| + AB [[5](#bib.bib5)] | 58.57 | 47.57 | 47.12 | 49.83 | 50.96 | 11.45 | 50.81
    |'
- en: '| + MoICE (Ours) | 61.81 | 52.54 | 52.43 | 50.36 | 49.34 | 12.47 | 53.30 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| + MoICE (我们) | 61.81 | 52.54 | 52.43 | 50.36 | 49.34 | 12.47 | 53.30 |'
- en: Our MDQA experiments leverage a subset of NaturalQuestions-Open [[21](#bib.bib21),
    [20](#bib.bib20)], consisting of 2,655 queries, following [[38](#bib.bib38), [22](#bib.bib22)].
    Each query is paired with a context consisting of 10 or 30 documents (with an
    average of 1,722 or 5,046 tokens), depending on the model (Llama-2-7B-chat or
    Mistral-7B-Instruct-8k), tasked with answering based on this contextual information.
    Only one document among these comprises useful information for the given query.
    We compare Ms-PoE, AB, and MoICE, testing each method through 5 iterations. For
    Llama, the relevant document is positioned 1st, 3rd, 5th, 7th, and 10th within
    the context, while for Mistral, it is positioned 1st, 8th, 15th, 23rd, and 30th,
    respectively.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MDQA实验利用了NaturalQuestions-Open [[21](#bib.bib21), [20](#bib.bib20)] 的一个子集，包含2,655个查询，遵循
    [[38](#bib.bib38), [22](#bib.bib22)]。每个查询都配有一个由10或30个文档（平均1,722或5,046个tokens）组成的上下文，具体取决于模型（Llama-2-7B-chat或Mistral-7B-Instruct-8k），任务是基于这些上下文信息进行回答。在这些文档中，只有一个文档包含对给定查询有用的信息。我们比较了Ms-PoE、AB和MoICE，通过5次迭代测试每种方法。对于Llama，相关文档分别位于上下文中的第1、3、5、7和10位置，而对于Mistral，则分别位于第1、8、15、23和30位置。
- en: In Table [3](#S4.T3 "Table 3 ‣ 4.3 Retrieval-augmented generation (RAG) ‣ 4
    Experiment ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"),
    MoICE on Llama demonstrates the highest average performance across most positions,
    showcasing its remarkable stability. Its accuracy scores show minimal variation,
    with only a marginal difference of 2.22 points between its highest and lowest
    values. On Mistral, MoICE exhibits significant average improvement (p-value <
    0.02). Notably, when the relevant document is positioned at the end of the context,
    all methods on Llama exhibit a decrease compared to the original model, although
    MoICE shows a minimal decline. This phenomenon also happens in the Mistral model.
    We posit that this decline may stem from the original model predominantly directing
    attention towards nearest documents [[22](#bib.bib22), [26](#bib.bib26)]. However,
    as approaches enhance awareness of various contextual positions, the model’s attention
    to the nearest documents is diffused by other positions, as its overall capacity
    for context awareness is constant and limited. Nevertheless, MoICE consistently
    emerges as the superior-performing method overall across language models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [3](#S4.T3 "表格 3 ‣ 4.3 检索增强生成（RAG） ‣ 4 实验 ‣ 上下文专家的混合提升了LLM的长时上下文感知")中，MoICE
    在 Llama 上展示了在大多数位置上的最高平均性能，展示了其显著的稳定性。其准确率分数变化最小，最高值和最低值之间的差异仅为2.22分。在 Mistral
    上，MoICE 显示出显著的平均提升（p值 < 0.02）。值得注意的是，当相关文档位于上下文末尾时，所有在 Llama 上的方法相比于原始模型均有所下降，尽管
    MoICE 的下降幅度最小。Mistral 模型中也出现了这种现象。我们认为，这种下降可能源于原始模型主要将注意力集中在最近的文档上 [[22](#bib.bib22),
    [26](#bib.bib26)]。然而，随着方法提升对各种上下文位置的感知，模型对最近文档的注意力被其他位置所分散，因为其整体的上下文感知能力是恒定且有限的。尽管如此，MoICE
    一直表现为在所有语言模型中整体表现最优的方法。
- en: 5 Method analysis
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 方法分析
- en: In this section, we delve into a comprehensive analysis of the properties of
    MoICE. We illustrate how $N$, the specific number of selected in-context experts
    (Section [5.2](#S5.SS2 "5.2 The effect of selected experts number 𝐾 ‣ 5 Method
    analysis ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness")),
    influence MoICE. We further demonstrate that MoICE is robust to training data
    (Section [5.3](#S5.SS3 "5.3 MoICE is robust to training data ‣ 5 Method analysis
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness")) Additionally,
    we present a case study demonstrating the dynamic selection of in-context experts
    for tokens during generation (Section [5.4](#S5.SS4 "5.4 The visualization of
    dynamic routing states ‣ 5 Method analysis ‣ Mixture of In-Context Experts Enhance
    LLMs’ Long Context Awareness")).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入分析 MoICE 的属性。我们说明了 $N$，即选择的上下文专家的具体数量（第 [5.2](#S5.SS2 "5.2 选择的专家数量
    𝐾 的效果 ‣ 5 方法分析 ‣ 上下文专家的混合提升了LLM的长时上下文感知")节），如何影响 MoICE。我们进一步展示了 MoICE 对训练数据的鲁棒性（第
    [5.3](#S5.SS3 "5.3 MoICE 对训练数据的鲁棒性 ‣ 5 方法分析 ‣ 上下文专家的混合提升了LLM的长时上下文感知")节）。此外，我们展示了一个案例研究，演示了在生成过程中动态选择上下文专家的过程（第
    [5.4](#S5.SS4 "5.4 动态路由状态的可视化 ‣ 5 方法分析 ‣ 上下文专家的混合提升了LLM的长时上下文感知")节）。
- en: 5.1 The effect of expert total numbers $N$
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 专家总数 $N$ 的效果
- en: We investigate the impact of the total number of experts. Employing the search
    algorithm proposed by Chen et al. [[6](#bib.bib6)], we obtain various sets of
    different sizes, each comprising complementary base values. The searched expert
    sets are detailed in Appendix [C](#A3 "Appendix C Details on expert sets ‣ Mixture
    of In-Context Experts Enhance LLMs’ Long Context Awareness"). We apply MoICE to
    Llama-2-7B-chat and test the model on L-Eval tasks. The results are presented
    in Table [4](#S5.T4 "Table 4 ‣ 5.2 The effect of selected experts number 𝐾 ‣ 5
    Method analysis ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness").
    The results of the original Llama are denoted as ($N$=7 experts is sufficient
    for general usage.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了专家总数的影响。通过使用 Chen 等人提出的搜索算法 [[6](#bib.bib6)]，我们获得了各种大小的专家集合，每个集合包含互补的基础值。搜索到的专家集合详细信息见附录 [C](#A3
    "附录 C 专家集合的详细信息 ‣ 上下文专家的混合提升了LLM的长时上下文感知")。我们将 MoICE 应用于 Llama-2-7B-chat 并在 L-Eval
    任务上测试模型。结果如表格 [4](#S5.T4 "表格 4 ‣ 5.2 选择的专家数量 𝐾 的效果 ‣ 5 方法分析 ‣ 上下文专家的混合提升了LLM的长时上下文感知")所示。原始
    Llama 的结果表示为（$N$=7 的专家对于一般使用是足够的）。
- en: 5.2 The effect of selected experts number $K$
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 选择的专家数量 $K$ 的效果
- en: With a fixed number of experts ($N$ greater than 3, performance improvements
    become evident. This shows that the MoICE router in our method can select the
    appropriate combination of experts to better aware the context. Randomly selecting
    experts ruins the model’s language modeling ability, leading to aberrant outputs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定数量的专家 ($N$ 大于 3) 时，性能改进变得明显。这表明我们方法中的 MoICE 路由器可以选择适当的专家组合以更好地感知上下文。随机选择专家会破坏模型的语言建模能力，导致异常输出。
- en: 'Table 4: The performance of Llama-2-7B-chat enhanced by MoICE with $N$ in-context
    experts. We show results marked with color to emphasize the improvements over
    the original model.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：MoICE 提升了 Llama-2-7B-chat 的表现，使用了 $N$ 个上下文专家。我们用颜色标记结果以强调相对于原始模型的改进。
- en: '| Method | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Coursera | QuALITY | TOEFL | SFiction | 平均值 |'
- en: '| Original ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 原始 ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
- en: '| $N$=3 | 37.65 | 40.10 | 55.76 | 62.50 | 49.00 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| $N$=3 | 37.65 | 40.10 | 55.76 | 62.50 | 49.00 |'
- en: '| $N$=5 | 38.23 | 39.60 | 56.13 | 63.28 | 49.32 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| $N$=5 | 38.23 | 39.60 | 56.13 | 63.28 | 49.32 |'
- en: '| $N$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| $N$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
- en: '| $N$=9 | 40.26 | 41.58 | 56.13 | 64.84 | 50.70 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| $N$=9 | 40.26 | 41.58 | 56.13 | 64.84 | 50.70 |'
- en: 'Table 5: The improvement of context awareness of Llama-2-7B-chat by MoICE,
    wherein each head dynamically selects diverse $K$=7). We show results marked with
    color to emphasize the improvements over the original model.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：MoICE 提升了 Llama-2-7B-chat 的上下文感知，其中每个头动态选择不同的 $K$=7）。我们用颜色标记结果以强调相对于原始模型的改进。
- en: '| Method | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Coursera | QuALITY | TOEFL | SFiction | 平均值 |'
- en: '| Original ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 原始 ($N$=1) | 36.77 | 38.12 | 55.02 | 60.16 | 47.52 |'
- en: '| $K$=1 | 35.03 | 35.64 | 56.51 | 61.72 | 47.22 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| $K$=1 | 35.03 | 35.64 | 56.51 | 61.72 | 47.22 |'
- en: '| $K$=3 | 39.83 | 41.58 | 56.13 | 64.84 | 50.60 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| $K$=3 | 39.83 | 41.58 | 56.13 | 64.84 | 50.60 |'
- en: '| $K$=5 | 38.52 | 39.60 | 56.13 | 64.84 | 49.77 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $K$=5 | 38.52 | 39.60 | 56.13 | 64.84 | 49.77 |'
- en: '| $K$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $K$=7 | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
- en: '| Equal Weights | 36.48 | 38.12 | 53.90 | 61.72 | 47.56 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 相等权重 | 36.48 | 38.12 | 53.90 | 61.72 | 47.56 |'
- en: '| Random Weights | 15.55 | 28.71 | 21.75 | 8.59 | 18.65 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 随机权重 | 15.55 | 28.71 | 21.75 | 8.59 | 18.65 |'
- en: 5.3 MoICE is robust to training data
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 MoICE 对训练数据具有鲁棒性
- en: 'We further analyze the impact of the data for training routers. We additionally
    use three instruction fine-tuning datasets from different sources: a self-instruct
    dataset, Airoboros [[18](#bib.bib18)]; and two datasets for LLM alignment with
    long context, Long-Alpaca [[7](#bib.bib7)], and LongAlign [[2](#bib.bib2)]. The
    hyperparameters remain consistent as mentioned in Section [4](#S4 "4 Experiment
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"). As presented
    in Table [6](#S5.T6 "Table 6 ‣ 5.3 MoICE is robust to training data ‣ 5 Method
    analysis ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"),
    MoICE exhibits almost identical scores when trained on different data, showcasing
    the robustness of our method.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步分析了用于训练路由器的数据的影响。我们额外使用了来自不同来源的三种指令微调数据集：一个自我指导数据集 Airoboros [[18](#bib.bib18)]；以及两个用于
    LLM 对齐的长上下文数据集，Long-Alpaca [[7](#bib.bib7)] 和 LongAlign [[2](#bib.bib2)]。超参数保持一致，如第
    [4](#S4 "4 Experiment ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context
    Awareness") 节所述。正如表 [6](#S5.T6 "Table 6 ‣ 5.3 MoICE is robust to training data
    ‣ 5 Method analysis ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context
    Awareness") 中所示，当使用不同数据进行训练时，MoICE 展现出几乎相同的评分，展示了我们方法的鲁棒性。
- en: 'Table 6: The improvement of context awareness of Llama-2-7B-chat by MoICE trained
    on various data.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6：MoICE 在不同数据上训练的 Llama-2-7B-chat 的上下文感知的改进。
- en: '| Training Data | Coursera | QuALITY | TOEFL | SFiction | Avg. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据 | Coursera | QuALITY | TOEFL | SFiction | 平均值 |'
- en: '| OpenHermes [[31](#bib.bib31)] | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| OpenHermes [[31](#bib.bib31)] | 39.83 | 42.08 | 56.13 | 64.84 | 50.72 |'
- en: '| Airoboros [[18](#bib.bib18)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Airoboros [[18](#bib.bib18)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
- en: '| Long-Alpaca [[7](#bib.bib7)] | 39.68 | 42.08 | 56.13 | 64.84 | 50.68 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Long-Alpaca [[7](#bib.bib7)] | 39.68 | 42.08 | 56.13 | 64.84 | 50.68 |'
- en: '| LongAlign [[2](#bib.bib2)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LongAlign [[2](#bib.bib2)] | 39.68 | 41.58 | 56.13 | 64.84 | 50.56 |'
- en: 5.4 The visualization of dynamic routing states
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 动态路由状态的可视化
- en: We provide a case study exemplifying the dynamic routing mechanism within MoICE
    during text generation. Depicted in Figure [4](#A5.F4 "Figure 4 ‣ Appendix E Broader
    impacts and safety issues ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context
    Awareness") in the Appendix, the MoICE router of each head independently selects
    distinct experts. At each step of the generation process, these heads dynamically
    choose experts for each new token. This dynamic utilization of diverse RoPE angles
    within each attention head maximizes the potential of attention heads across various
    inputs, a capability not attained in prior research, including both Attention
    Buckets and Ms-PoE.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个案例研究，展示了MoICE在文本生成中的动态路由机制。如附录中的图[4](#A5.F4 "图4 ‣ 附录E 更广泛的影响和安全问题 ‣ 上下文专家混合增强LLMs的长上下文感知")所示，MoICE的每个头部独立选择不同的专家。在生成过程的每一步，这些头部动态选择每个新令牌的专家。这种在每个注意力头中动态利用不同RoPE角度的方式最大化了注意力头在各种输入中的潜力，这是以往研究，包括Attention
    Buckets和Ms-PoE未曾实现的能力。
- en: 6 Conclusion
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduce a novel approach to enhancing the context awareness
    of LLMs termed Mixture of In-Context Experts (MoICE). Through lightweight training,
    open-source LLMs such as Llama and Mistral, enhanced by MoICE, demonstrate improved
    context awareness. Across numerous tasks demanding substantial context awareness,
    MoICE-enhanced LLMs consistently outperform competitive baselines, all the while
    maintaining commendable efficiency. A distinctive feature of MoICE is that it
    first implements head- and token-specific RoPE angles assignment for attention
    heads, a pivotal factor contributing to its success. This paper underscores the
    need to address the inherent limitations in current LLMs and advocates for a thorough
    exploration of their existing capabilities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种增强LLMs（大规模语言模型）上下文感知的新方法，称为“上下文专家混合（MoICE）”。通过轻量化的训练，诸如Llama和Mistral等开源LLMs，在MoICE的增强下，展现了改进的上下文感知能力。在许多需要大量上下文感知的任务中，MoICE增强的LLMs始终优于竞争基线，同时保持了令人称赞的效率。MoICE的一个独特特点是，它首先为注意力头实现了基于头部和令牌的RoPE角度分配，这是其成功的关键因素。本文强调了需要解决当前LLMs固有的局限性，并倡导对其现有能力进行深入探讨。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢与资金披露
- en: This work was supported by Boss Zhipin for providing computational resources.
    We appreciate Ting-En Lin for his discussions on training hyperparameters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了Boss Zhipin提供的计算资源支持。我们感谢Ting-En Lin对训练超参数的讨论。
- en: References
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong,
    and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language
    models. arXiv preprint arXiv:2307.11088, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong,
    和 Xipeng Qiu. L-eval：为长上下文语言模型制定标准化评估。arXiv预印本 arXiv:2307.11088，2023年。'
- en: '[2] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao
    Dong, and Juanzi Li. Longalign: A recipe for long context alignment of large language
    models. arXiv preprint arXiv:2401.18058, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao
    Dong, 和 Juanzi Li. Longalign：大规模语言模型长上下文对齐的方案。arXiv预印本 arXiv:2401.18058，2024年。'
- en: '[3] Samuel R Bowman, Angelica Chen, He He, Nitish Joshi, Johnny Ma, Nikita
    Nangia, Vishakh Padmakumar, Richard Yuanzhe Pang, Alicia Parrish, Jason Phang,
    et al. Quality: Question answering with long input texts, yes! NAACL 2022, 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Samuel R Bowman, Angelica Chen, He He, Nitish Joshi, Johnny Ma, Nikita
    Nangia, Vishakh Padmakumar, Richard Yuanzhe Pang, Alicia Parrish, Jason Phang等.
    质量：长输入文本的问答，没错！NAACL 2022，2022年。'
- en: '[4] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language
    models in retrieval-augmented generation. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 38, pages 17754–17762, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jiawei Chen, Hongyu Lin, Xianpei Han, 和 Le Sun. 大规模语言模型在检索增强生成中的基准测试。发表于《AAAI人工智能会议论文集》，第38卷，17754–17762页，2024年。'
- en: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian. 通过位置插值扩展大规模语言模型的上下文窗口。arXiv预印本
    arXiv:2306.15595，2023年。'
- en: '[6] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin
    Li, and Rui Yan. Fortify the shortest stave in attention: Enhancing context awareness
    of large language models for effective tool use. arXiv preprint arXiv:2312.04455,
    2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin
    Li 和 Rui Yan。强化注意力中的最短音符：增强大型语言模型的上下文意识以有效使用工具。arXiv 预印本 arXiv:2312.04455，2023年。'
- en: '[7] Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han, and Jiaya Jia. Long alpaca: Long-context instruction-following models.
    [https://github.com/dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA),
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han 和 Jiaya Jia。Long alpaca: 长上下文指令跟随模型。 [https://github.com/dvlab-research/LongLoRA](https://github.com/dvlab-research/LongLoRA)，2023年。'
- en: '[8] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.
    Lift yourself up: Retrieval-augmented text generation with self-memory. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao 和 Rui Yan。自我提升：结合自我记忆的检索增强文本生成。《神经信息处理系统进展》，36，2024年。'
- en: '[9] Yu-An Chung, Hung-Yi Lee, and James Glass. Supervised and unsupervised
    transfer learning for question answering. In Marilyn Walker, Heng Ji, and Amanda
    Stent, editors, Proceedings of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long Papers), pages 1585–1594, New Orleans, Louisiana, June 2018\. Association
    for Computational Linguistics.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yu-An Chung, Hung-Yi Lee 和 James Glass。用于问答的监督和无监督迁移学习。在 Marilyn Walker,
    Heng Ji 和 Amanda Stent 编辑的《2018年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇论文）》中，页码 1585–1594，新奥尔良，路易斯安那州，2018年6月。计算语言学协会。'
- en: '[10] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen,
    Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, et al. Deepseekmoe: Towards ultimate
    expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066,
    2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen,
    Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu 等人。Deepseekmoe: 迈向混合专家语言模型中的终极专家专精。arXiv
    预印本 arXiv:2401.06066，2024年。'
- en: '[11] Tri Dao. Flashattention-2: Faster attention with better parallelism and
    work partitioning. In The Twelfth International Conference on Learning Representations,
    2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tri Dao。Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分区。在第十二届国际学习表征会议上，2024年。'
- en: '[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    4171–4186, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova. BERT: 深度双向变换器的预训练用于语言理解。在
    Jill Burstein, Christy Doran 和 Thamar Solorio 编辑的《2019年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇和短篇论文）》中，页码
    4171–4186，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。'
- en: '[13] emozilla. Dynamically scaled rope further increases performance of long
    context llama with zero fine-tuning. [https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] emozilla. 动态缩放的ROPE进一步提高了长上下文LLAMA的性能，无需微调。 [https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases)。'
- en: '[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling
    to trillion parameter models with simple and efficient sparsity. Journal of Machine
    Learning Research, 23(120):1–39, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] William Fedus, Barret Zoph 和 Noam Shazeer。Switch transformers: 通过简单高效的稀疏性扩展到万亿参数模型。《机器学习研究期刊》，23(120):1–39，2022年。'
- en: '[15] Yizheng Huang and Jimmy Huang. A survey on retrieval-augmented text generation
    for large language models. arXiv preprint arXiv:2404.10981, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yizheng Huang 和 Jimmy Huang。关于大型语言模型的检索增强文本生成的调查。arXiv 预印本 arXiv:2404.10981，2024年。'
- en: '[16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等人。Mistral 7b。arXiv 预印本 arXiv:2310.06825，2023年。'
- en: '[17] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 阿尔伯特·Q·江，亚历山大·萨布拉约尔，安托万·鲁，亚瑟·门施，布兰奇·萨瓦里，克里斯·班福德，德文德拉·辛格·查普洛特，迭戈·德·拉斯·卡萨斯，艾玛·布哈娜，弗洛里安·布雷桑，等。专家混合模型的Mixtral。arXiv预印本
    arXiv:2401.04088，2024年。'
- en: '[18] jondurbin. airoboros: using large language models to fine-tune large language
    models. [https://github.com/jondurbin/airoboros](https://github.com/jondurbin/airoboros).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] jondurbin. airoboros：利用大型语言模型微调大型语言模型。 [https://github.com/jondurbin/airoboros](https://github.com/jondurbin/airoboros)。'
- en: '[19] He Junqing, Pan Kunhao, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang Yuxin,
    Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, et al. Never lost in the middle:
    Improving large language models via attention strengthening question answering.
    arXiv preprint arXiv:2311.09198, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 贺俊卿，潘昆浩，董晓群，宋卓扬，刘艺博，梁玉新，王浩，孙千国，张松欣，谢泽剑，等。中途从未迷失：通过注意力强化问答改进大型语言模型。arXiv预印本
    arXiv:2311.09198，2023年。'
- en: '[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins,
    Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
    Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.
    Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark
    for question answering research. Transactions of the Association for Computational
    Linguistics, 7:452–466, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 汤姆·克维亚特科夫斯基，珍妮玛利亚·帕洛马克，奥利维亚·雷德菲尔德，迈克尔·柯林斯，安库尔·帕里克，克里斯·阿尔伯蒂，丹妮尔·爱泼斯坦，伊利亚·波洛苏金，雅各布·德夫林，肯顿·李，克里斯蒂娜·图塔诺娃，莱昂·琼斯，马修·凯尔西，*明伟·张*，安德鲁·M·戴，雅各布·乌斯科雷特，阮戈，和斯拉夫·彼得罗夫。自然问题：一个问答研究的基准。计算语言学协会会刊，7:452–466，2019年。'
- en: '[21] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for
    weakly supervised open domain question answering. In Anna Korhonen, David Traum,
    and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019\. Association
    for Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 肯顿·李，明伟·张，和克里斯蒂娜·图塔诺娃。弱监督开放领域问答的潜在检索。在安娜·科赫农，戴维·特劳姆，和吕利斯·马尔克斯主编的第57届计算语言学协会年会论文集，6086–6096页，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[22] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long
    contexts, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 纳尔逊·F·刘，凯文·林，约翰·赫维特，阿什温·帕兰贾佩，米歇尔·贝维拉夸，法比奥·彼得罗尼，和佩尔西·梁。迷失在中途：语言模型如何使用长上下文，2023年。'
- en: '[23] Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen,
    Jian Xie, and Rui Yan. Interpreting key mechanisms of factual recall in transformer-based
    language models, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 方 Lv，凯伊·张，余欢·陈，余龙·王，李丰·刘，季戎·温，简·谢，和瑞·燕。解释基于变换器的语言模型中事实回忆的关键机制，2024年。'
- en: '[24] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
    Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn
    Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy
    Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,
    Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning
    and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 凯瑟琳·奥尔森，内尔森·埃尔哈赫，尼尔·南达，尼古拉斯·约瑟夫，诺瓦·达萨玛，汤姆·亨尼根，本·曼，阿曼达·阿斯克尔，云涛·白，安娜·陈，汤姆·康纳利，道恩·德雷恩，深度·甘古利，扎克·哈特菲尔德-多兹，丹尼·埃尔南德斯，斯科特·约翰斯顿，安迪·琼斯，杰克逊·科尼恩，莉安娜·洛维特，卡马尔·恩杜斯，达里奥·阿莫代伊，汤姆·布朗，杰克·克拉克，贾里德·卡普兰，萨姆·麦坎德利什，和克里斯·奥拉。上下文学习和归纳头。变换器电路线程，2022年。
    [https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)。'
- en: '[25] OpenAI. Gpt-4 technical report, 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] OpenAI。GPT-4技术报告，2024年。'
- en: '[26] Alexander Peysakhovich and Adam Lerer. Attention sorting combats recency
    bias in long context language models, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 亚历山大·佩萨科维奇 和 亚当·勒勒。注意力排序对抗长上下文语言模型中的近期偏差，2023年。'
- en: '[27] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention
    with linear biases enables input length extrapolation, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 奥菲尔·普雷斯，诺亚·A·史密斯，和迈克·刘易斯。短期训练，长期测试：具有线性偏差的注意力使输入长度外推成为可能，2022年。'
- en: '[28] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc
    Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated
    mixture-of-experts layer. In International Conference on Learning Representations,
    2017.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 诺亚·沙泽尔，*阿扎利亚·米尔霍塞尼*，*克日什托夫·马齐亚兹*，安迪·戴维斯，阮戈，杰弗里·辛顿，和杰夫·迪恩。极其庞大的神经网络：稀疏门控专家混合层。国际学习表征会议，2017年。'
- en: '[29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 苏建林，艾姆尔塔达·艾哈迈德，陆瑜，潘胜锋，博文，和刘云峰。Roformer：增强的旋转位置嵌入变压器。神经计算，568:127063，2024年。'
- en: '[30] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
    Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.
    In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 14590–14604, Toronto, Canada, July 2023\. Association for
    Computational Linguistics.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 孙雨涛，董力，巴伦·帕特拉，马树铭，黄少汉，阿隆·本海姆，维什拉夫·乔杜里，宋霞，和魏富如。一个长度可外推的变压器。在安娜·罗杰斯，乔丹·博伊德-格雷伯，和冈崎直明编辑的《第61届计算语言学协会年会论文集（第1卷：长篇论文）》中，页14590–14604，加拿大多伦多，2023年7月。计算语言学协会。'
- en: '[31] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist
    llm assistants, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Teknium. Openhermes 2.5: 一个用于通用大型语言模型助手的开放合成数据集，2023年。'
- en: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    等。Llama 2：开放基础和微调聊天模型。arXiv预印本 arXiv:2307.09288，2023年。'
- en: '[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30\.
    Curran Associates, Inc., 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser, 和 Illia Polosukhin。注意力即你所需。在I. Guyon, U. Von Luxburg,
    S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, 和 R. Garnett编辑的《神经信息处理系统进展》第30卷。Curran
    Associates, Inc., 2017年。'
- en: '[34] Haotao Wang, Ziyu Jiang, Yuning You, Yan Han, Gaowen Liu, Jayanth Srinivasa,
    Ramana Kompella, Zhangyang Wang, et al. Graph mixture of experts: Learning on
    large-scale graphs with explicit diversity modeling. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 王浩涛，蒋子瑜，游宇宁，韩燕，刘高文，斯里尼瓦萨·贾扬特，拉马纳·孔佩拉，王章扬等。专家图混合：在大规模图上进行显式多样性建模的学习。神经信息处理系统进展，36，2024年。'
- en: '[35] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
    Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object
    identification in GPT-2 small. In The Eleventh International Conference on Learning
    Representations, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, 和 Jacob
    Steinhardt。现实世界中的可解释性：GPT-2小模型中用于间接对象识别的电路。在第十一届国际学习表征会议上，2023年。'
- en: '[36] Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid,
    Natan Vidra, and Tommy Clifford. Enhancing large language model performance to
    answer questions and extract information more accurately. arXiv preprint arXiv:2402.01722,
    2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 张亮，凯瑟琳·吉乔，斯普尔提·塞蒂，伊甸·钟，法蒂玛·贾维德，纳坦·维德拉，和汤米·克利福德。增强大型语言模型性能，以更准确地回答问题和提取信息。arXiv预印本
    arXiv:2402.01722，2024年。'
- en: '[37] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng
    Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering
    for llms. arXiv preprint arXiv:2311.02262, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 张青如，钱丹·辛格，刘丽媛，刘晓东，余斌，高剑锋，和赵拓。告诉你的模型关注哪里：针对大型语言模型的事后注意力引导。arXiv预印本 arXiv:2311.02262，2023年。'
- en: '[38] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi
    Chen, Xiaoxia Wu, and Zhangyang Wang. Found in the middle: How language models
    use long contexts better via plug-and-play positional encoding. arXiv preprint
    arXiv:2403.04797, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 张震宇，陈润金，刘士伟，姚哲伟，奥拉图吉·鲁瓦塞，陈北迪，吴晓霞，和王章扬。中间发现：语言模型如何通过即插即用的位置编码更好地使用长上下文。arXiv预印本
    arXiv:2403.04797，2024年。'
- en: Appendix A Attention waveforms
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 注意力波形
- en: 'In this section, we will elaborate on attention waveforms and the concept of
    complementarity. Assuming $\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$-th
    attention head. The attention score can be formulated as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将详细阐述注意力波形和互补性的概念。假设为$\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$-th注意力头。注意力分数可以表述如下：
- en: '|  | $\displaystyle\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$ |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\hat{q}}_{n}^{h}\cdot\mathbf{\hat{k}}_{m}^{h}$ |  |'
- en: '|  |  | $\displaystyle=\operatorname{Re}\left[\sum_{j=0}^{d/2-1}\mathbf{q_{n}^{h}}[2j:2j+1]\mathbf{k_{m}^{h*}}[2j:2j+1]e^{i(n-m)\theta_{j}}\right]$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname{Re}\left[\sum_{j=0}^{d/2-1}\mathbf{q_{n}^{h}}[2j:2j+1]\mathbf{k_{m}^{h*}}[2j:2j+1]e^{i(n-m)\theta_{j}}\right]$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $\theta_{j}=B^{-2j/d}$ by [[6](#bib.bib6)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta_{j}=B^{-2j/d}$，参考[[6](#bib.bib6)]。
- en: '|  | $\mathcal{W}\leq\sum_{j=0}^{d/2-1}2\cos\left((n-m)\theta_{j}\right).$
    |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{W}\leq\sum_{j=0}^{d/2-1}2\cos\left((n-m)\theta_{j}\right).$
    |  |'
- en: 'As illustrated in Figure  [2](#S2.F2 "Figure 2 ‣ Context awareness of LLMs
    ‣ 2 Background ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness"),
    the waveform exhibits two notable mathematical properties concerning attention
    scores: it demonstrates fluctuations and undergoes a gradual decay with the increasing
    relative position (i.e., long-term decay).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](#S2.F2 "图 2 ‣ LLMs的上下文意识 ‣ 2 背景 ‣ 上下文专家的混合增强了LLMs的长上下文意识")所示，波形展示了两个显著的数学性质与注意力分数相关：它展示了波动，并随着相对位置的增加（即长期衰减）逐渐衰减。
- en: Chen et al. [[6](#bib.bib6)] observed that crucial information falling within
    the troughs of a waveform might diminish the performance of models employing RoPE.
    Meanwhile, they pointed out the waveform, characterized by peaks and troughs,
    vary across RoPE bases. When leveraging the peaks of one attention wave to compensate
    for the overlook of the troughs in another, the model’s capability to perceive
    and process information from diverse contextual positions can be enhanced. When
    a set of bases possesses this waveform characteristic, they are termed “complementary.”
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Chen等人[[6](#bib.bib6)]观察到波形的槽内的关键信息可能会削弱使用RoPE的模型的性能。同时，他们指出波形由峰值和槽值特征组成，在RoPE基础中有所变化。当利用一个注意力波的峰值来弥补另一个波中的槽值时，模型从不同上下文位置感知和处理信息的能力可以得到增强。当一组基具有这种波形特性时，它们被称为“互补”。
- en: Appendix B Discussions on more position embeddings
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更多位置嵌入的讨论
- en: 'In this section, we discuss other position embeddings and demonstrate why they
    are not studied, e.g., discarded in LLMs, do not exhibit attention waveform pattern,
    or are in the same family of RoPE: Firstly, the waveform pattern only exists in
    position embeddings constructed by cosine functions. Regarding the cosine embedding
    used in the original Transformer, it does exhibit long-term decay and periodic
    waveforms. However, this embedding is disregarded in modern LLMs. Moreover, these
    embeddings are incorporated before the initial model layer rather than during
    the attention computation, making it hard to assess their impact on attention
    patterns. Secondly, the learned positional embeddings utilized in BERT [[12](#bib.bib12)]
    lack mathematical constraints to display periodic patterns. They are similarly
    added before the first model layer. Thirdly, Alibi [[27](#bib.bib27)] introduces
    a linear bias to attention scores. The linear bias is devoid of wave patterns.
    The remaining popular positional embeddings used in LLMs such as xPos [[30](#bib.bib30)]
    are RoPE-based variants. These variants are predominantly modified for long-context
    extrapolation rather than better context awareness. Therefore, they share the
    same shortcoming: tokens in attention trough are less focused on, thereby limiting
    context awareness, which is the study focus in our paper.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论其他位置嵌入，并展示它们为何没有被研究，例如，在LLMs中被丢弃、不展示注意力波形模式，或属于RoPE的同一类别：首先，波形模式仅存在于由余弦函数构造的位置嵌入中。关于原始Transformer中使用的余弦嵌入，它确实展示了长期衰减和周期性波形。然而，这种嵌入在现代LLMs中被忽视。此外，这些嵌入在初始模型层之前被加入，而不是在注意力计算过程中，这使得难以评估它们对注意力模式的影响。其次，BERT[[12](#bib.bib12)]中使用的学习位置嵌入缺乏数学约束以展示周期性模式。这些嵌入同样是在第一个模型层之前添加的。第三，Alibi[[27](#bib.bib27)]引入了线性偏置到注意力分数中。线性偏置不具备波形模式。剩下的流行位置嵌入，如xPos[[30](#bib.bib30)]，是基于RoPE的变体。这些变体主要是为了长上下文外推而非更好的上下文意识。因此，它们存在相同的缺陷：注意力槽中的标记关注度较低，从而限制了上下文意识，这是我们论文中的研究重点。
- en: Appendix C Details on expert sets
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 专家集合的详细信息
- en: Utilizing the RoPE-base searching algorithm as proposed by Chen et al. [[6](#bib.bib6)],
    Table [7](#A3.T7 "Table 7 ‣ Appendix C Details on expert sets ‣ Mixture of In-Context
    Experts Enhance LLMs’ Long Context Awareness") illustrates the resulting sets
    for different values of $N$.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Chen等人提出的RoPE-base搜索算法[[6](#bib.bib6)]，表格[7](#A3.T7 "表格 7 ‣ 附录 C 专家集合的详细信息
    ‣ 上下文专家的混合增强了LLMs的长上下文意识")展示了不同$N$值下的结果集合。
- en: 'Table 7: Searched Sets for Different $N$'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同 $N$ 的搜索集合
- en: '| $N$ | Searched Set |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| $N$ | 搜索集合 |'
- en: '| 3 | {10,000, 18,000, 19,000} |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 3 | {10,000, 18,000, 19,000} |'
- en: '| 5 | {10,000, 17,500, 18,000, 19,000, 20,000} |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 5 | {10,000, 17,500, 18,000, 19,000, 20,000} |'
- en: '| 7 | {10,000, 17,500, 18,000, 19,000, 20,000, 22,500, 25,000} |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 7 | {10,000, 17,500, 18,000, 19,000, 20,000, 22,500, 25,000} |'
- en: '| 9 | {10,000, 13,500, 17,500, 18,000, 19,000, 20,000, 22,500, 24,000, 25,000}
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 9 | {10,000, 13,500, 17,500, 18,000, 19,000, 20,000, 22,500, 24,000, 25,000}
    |'
- en: Appendix D Limitations
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 限制
- en: In this paper, we introduce a plug-in module called MoICE, which is integrated
    into the attention heads of open-source LLMs to enhance their context awareness.
    One limitation is that, due to limited computational resources, we did not investigate
    the effectiveness of pretraining a language model using the MoICE architecture.
    Furthermore, our proposed method exploits the potential for context awareness
    within LLMs, but it does not imbue the models with additional inherent context
    awareness abilities. Achieving this may necessitate more extensive data to train
    all model parameters.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个名为 MoICE 的插件模块，它集成到开源 LLM 的注意力头中，以增强其上下文意识。一个限制是，由于计算资源有限，我们未研究使用
    MoICE 架构进行语言模型预训练的效果。此外，我们提出的方法利用了 LLM 内部的上下文意识潜力，但并未赋予模型额外的固有上下文意识能力。实现这一点可能需要更多的数据来训练所有模型参数。
- en: Appendix E Broader impacts and safety issues
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 更广泛的影响与安全问题
- en: Our novel lightweight plug-in approach efficiently enhances the context awareness
    of open-source LLMs. This advancement holds great promise for enhancing the effectiveness
    of LLMs across diverse scenarios characterized by extensive and complex contexts,
    such as RAG, tool utilization, and role-playing. The safety issue of our method
    mainly comes from the large language models we used, as they might output toxic
    and biased texts, which is a common safety issue regarding LLM research.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新型轻量级插件方法有效地增强了开源 LLM 的上下文意识。这一进展对增强 LLM 在具有广泛且复杂上下文的各种场景中的有效性具有巨大潜力，如 RAG、工具使用和角色扮演。我们方法的安全问题主要来自我们使用的大型语言模型，因为它们可能输出有毒和偏见的文本，这是
    LLM 研究中常见的安全问题。
- en: '![Refer to caption](img/5f118a2dd0ecc0d8c46fb6115f0cc7ef.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5f118a2dd0ecc0d8c46fb6115f0cc7ef.png)'
- en: 'Figure 4: The routing weights across two distinct attention heads at the 27th
    layer in Llama-2-7B-chat. The input tokens are randomly sampled from the training
    data, and the attention heads under observation are also randomly selected. The
    horizontal axis depicts the input tokens, while the vertical axis represents experts
    with varying RoPE angles. Due to their distinct functions, each head dynamically
    chooses different experts to process individual tokens. Input text can be found
    in Figure  [5](#A5.F5 "Figure 5 ‣ Appendix E Broader impacts and safety issues
    ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context Awareness").'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Llama-2-7B-chat 在第 27 层两个不同注意力头的路由权重。输入标记是从训练数据中随机采样的，观察中的注意力头也是随机选择的。横轴表示输入标记，纵轴表示具有不同
    RoPE 角度的专家。由于各自的功能不同，每个头动态选择不同的专家来处理单个标记。输入文本见图 [5](#A5.F5 "图 5 ‣ 附录 E 更广泛的影响与安全问题
    ‣ 领域内专家混合增强 LLM 的长上下文意识")。
- en: '![Refer to caption](img/5e818ab172df62252a2ba0d1238e2020.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e818ab172df62252a2ba0d1238e2020.png)'
- en: 'Figure 5: The input text in Figure  [4](#A5.F4 "Figure 4 ‣ Appendix E Broader
    impacts and safety issues ‣ Mixture of In-Context Experts Enhance LLMs’ Long Context
    Awareness"). To clearly display, we only show part of the input text, where the
    text with a yellow background corresponds to the decoded tokens.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：图中输入文本见 [4](#A5.F4 "图 4 ‣ 附录 E 更广泛的影响与安全问题 ‣ 领域内专家混合增强 LLM 的长上下文意识")。为了清晰展示，我们仅展示了部分输入文本，其中带有黄色背景的文本对应解码的标记。
