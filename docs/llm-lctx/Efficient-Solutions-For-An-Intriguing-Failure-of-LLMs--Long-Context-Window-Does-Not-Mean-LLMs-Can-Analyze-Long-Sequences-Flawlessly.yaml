- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:00:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:00:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window
    Does Not Mean LLMs Can Analyze Long Sequences Flawlessly'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）有趣失败的高效解决方案：长上下文窗口并不意味着LLMs能够无缝分析长序列
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01866](https://ar5iv.labs.arxiv.org/html/2408.01866)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01866](https://ar5iv.labs.arxiv.org/html/2408.01866)
- en: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2)
- en: ¹School of EECS, Queen Mary University of London, London, UK
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹电子电气与计算机科学学院，伦敦玛丽女王大学，伦敦，英国
- en: ²Department of Knowledge Technologies, Jožef Stefan Institute, Ljubljana, Slovenia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²知识技术系，约瑟夫·斯特凡研究所，卢布尔雅那，斯洛文尼亚
- en: '{s.hosseini, i.castro, i.ghinassi m.purver}@qmul.ac.uk'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{s.hosseini, i.castro, i.ghinassi m.purver}@qmul.ac.uk'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have demonstrated remarkable capabilities in comprehending
    and analyzing lengthy sequential inputs, owing to their extensive context windows
    that allow processing millions of tokens in a single forward pass. However, this
    paper uncovers a surprising limitation: LLMs fall short when handling long input
    sequences. We investigate this issue using three datasets and two tasks (sentiment
    analysis and news categorization) across various LLMs, including Claude 3, Gemini
    Pro, GPT 3.5 Turbo, Llama 3 Instruct, and Mistral Instruct models. To address
    this limitation, we propose and evaluate ad-hoc solutions that substantially enhance
    LLMs’ performance on long input sequences by up to 50%, while reducing API cost
    and latency by up to 93% and 50%, respectively.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在理解和分析长序列输入方面展现了显著能力，这得益于其广泛的上下文窗口，可以在单次前向传递中处理数百万个标记。然而，本文揭示了一个令人惊讶的局限性：LLMs在处理长输入序列时表现不足。我们使用三个数据集和两个任务（情感分析和新闻分类）在各种LLMs（包括Claude
    3、Gemini Pro、GPT 3.5 Turbo、Llama 3 Instruct 和 Mistral Instruct 模型）上进行了调查。为了应对这一局限性，我们提出并评估了特别解决方案，这些解决方案显著提升了LLMs在长输入序列上的表现，提升幅度达到50%，同时API成本和延迟分别降低了高达93%和50%。
- en: 'Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window
    Does Not Mean LLMs Can Analyze Long Sequences Flawlessly'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）有趣失败的高效解决方案：长上下文窗口并不意味着LLMs能够无缝分析长序列
- en: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2) ¹School
    of EECS, Queen Mary University of London, London, UK ²Department of Knowledge
    Technologies, Jožef Stefan Institute, Ljubljana, Slovenia {s.hosseini, i.castro,
    i.ghinassi m.purver}@qmul.ac.uk
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Peyman Hosseini¹, Ignacio Castro¹, Iacopo Ghinassi¹, Matthew Purver^(1, 2) ¹电子电气与计算机科学学院，伦敦玛丽女王大学，伦敦，英国
    ²知识技术系，约瑟夫·斯特凡研究所，卢布尔雅那，斯洛文尼亚 {s.hosseini, i.castro, i.ghinassi m.purver}@qmul.ac.uk
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs have demonstrated remarkable capabilities in natural language understanding
    and generation tasks. Leveraging extensive pretraining on massive text corpora,
    the new generation of LLMs can perform a wide range of language tasks with minimal
    task-specific fine-tuning. Additionally, these LLMs are equipped with behemothic
    context windows that enable them to analyze inputs spanning up to tens or hundreds
    of pages in one forward pass. In this paper, we study the performance of Claude
    3 Haiku Anthropic ([2024](#bib.bib1)), GPT3.5-Turbo OpenAI ([2022](#bib.bib19)),
    Gemini-1.0-pro Team et al. ([2023](#bib.bib20)), Llama 3 8b Instruct FacebookResearch
    ([2024](#bib.bib7)), and Mistral 7b Instruct Jiang et al. ([2023](#bib.bib12)).
    These are equipped with context windows supporting up to 200,000, 16,000, 32,000,
    160,000, and 32,000 tokens respectively on long-form text inputs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在自然语言理解和生成任务中展现了显著能力。利用对海量文本语料的广泛预训练，新一代LLMs能够在最少的任务特定微调下执行广泛的语言任务。此外，这些LLMs配备了庞大的上下文窗口，使其能够在一次前向传递中分析跨度达到数十或数百页的输入。本文研究了Claude
    3 Haiku Anthropic（[2024](#bib.bib1)）、GPT3.5-Turbo OpenAI（[2022](#bib.bib19)）、Gemini-1.0-pro
    Team et al.（[2023](#bib.bib20)）、Llama 3 8b Instruct FacebookResearch（[2024](#bib.bib7)）和
    Mistral 7b Instruct Jiang et al.（[2023](#bib.bib12)）的表现。这些模型配备了支持最多200,000、16,000、32,000、160,000
    和 32,000 个标记的上下文窗口，适用于长文本输入。
- en: Related Work.
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 相关工作。
- en: Despite being equipped with context windows capable of supporting a huge snippet
    of text, the performance of LLMs on lengthy input sequences has been a subject
    of ongoing research Li et al. ([2023](#bib.bib14), [2024](#bib.bib15)). Different
    prompting strategies have emerged as a promising avenue for improving LLM performance
    by providing concise and informative input(Liu et al., [2023](#bib.bib16); Brown
    et al., [2020](#bib.bib5)). These strategies involve extracting key information
    from the input text and presenting it to the LLM in a structured manner.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管配备了能够支持大量文本片段的上下文窗口，但 LLM 在长输入序列上的性能仍然是一个持续研究的话题 Li 等 ([2023](#bib.bib14),
    [2024](#bib.bib15))。不同的提示策略已经成为提高 LLM 性能的有前途的途径，通过提供简洁且信息丰富的输入（Liu 等，[2023](#bib.bib16);
    Brown 等，[2020](#bib.bib5)）。这些策略涉及从输入文本中提取关键信息，并以结构化的方式呈现给 LLM。
- en: Summarization techniques play a crucial role in many natural language processing
    tasks by condensing lengthy inputs into more manageable snippets. Extractive Summarization
    methods such as TextRank Mihalcea and Tarau ([2004](#bib.bib18)) are widely used
    to identify and extract the most significant sentences from a document for different
    purposes from summarizing dialogues Feng et al. ([2022](#bib.bib8)) and scientific
    documents Cachola et al. ([2020](#bib.bib6)) to assessing content credibility
    Balcerzak et al. ([2014](#bib.bib2)) and creating automatic writing tools Wang
    et al. ([2020](#bib.bib21)). In this paper, we design and use summarisation pipelines
    as well as text truncation techniques to boost LLMs’ performance by optimizing
    the input while reducing their load.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结技术在许多自然语言处理任务中发挥着关键作用，通过将冗长的输入压缩成更易于管理的片段。提取式总结方法，如 TextRank Mihalcea 和 Tarau
    ([2004](#bib.bib18))，被广泛用于从文档中识别并提取最重要的句子，用于不同的目的，从对话总结 Feng 等 ([2022](#bib.bib8))
    和科学文档 Cachola 等 ([2020](#bib.bib6)) 到评估内容可信度 Balcerzak 等 ([2014](#bib.bib2)) 和创建自动写作工具
    Wang 等 ([2020](#bib.bib21))。在本文中，我们设计并使用总结管道以及文本截断技术，通过优化输入同时减少负载来提升 LLM 的性能。
- en: '![Refer to caption](img/06340ef66357ccc61d7bdea35cda1db6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/06340ef66357ccc61d7bdea35cda1db6.png)'
- en: (a) Pure Extractive Summarization Pipeline
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 纯提取式总结管道
- en: '![Refer to caption](img/329a4dfdab2c0a4fe107bb92442ab5a8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/329a4dfdab2c0a4fe107bb92442ab5a8.png)'
- en: (b) Diverse Summarization Pipeline
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 多样化总结管道
- en: 'Figure 1: The summarization pipelines for summarising information. The diverse
    summarization approach builds on top of the purely extractive approach but gives
    higher priority to lexical diversity.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：总结信息的总结管道。多样化总结方法基于纯提取式方法，但更重视词汇多样性。
- en: Motivation.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动机。
- en: There is a body of research dedicated to studying the limitations of LLMs on
    long sequences and proposing mitigations at both architecture-level Beltagy et al.
    ([2020](#bib.bib3)); Bertsch et al. ([2024](#bib.bib4)) as well as prompt-level
    Wei et al. ([2022](#bib.bib22)). These studies often involve defining and exploring
    overly complex problems such as those about extreme-label classification Li et al.
    ([2024](#bib.bib15)) or “Needle In a Haystack" Machlab and Battle ([2024](#bib.bib17)).
    However, a systematic study of LLM capabilities and limitations on long-form analysis
    tasks such as news categorization or sentiment analysis of long reviews which
    require a common general understanding of the input context is still lacking.
    Furthermore, the emphasis on approaches involving prompt-tuning has diverted attention
    away from optimizing and streamlining the information fed to LLMs. This study
    serves to fill these gaps by showcasing the failure of LLMs on canonical NLP tasks
    when dealing with long sequences and to ignite a spark of interest in the research
    community to explore the untapped potential of optimizing and condensing the information
    fed to LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量研究致力于研究 LLM 在长序列上的局限性，并在架构层面 Beltagy 等 ([2020](#bib.bib3)); Bertsch 等 ([2024](#bib.bib4))
    以及提示层面 Wei 等 ([2022](#bib.bib22)) 提出缓解方案。这些研究通常涉及定义和探索过于复杂的问题，例如极端标签分类 Li 等 ([2024](#bib.bib15))
    或“干草堆中的针” Machlab 和 Battle ([2024](#bib.bib17))。然而，关于 LLM 在长文本分析任务（如新闻分类或长评论情感分析）中的能力和局限性的系统研究仍然缺乏，这些任务需要对输入上下文有共同的一般理解。此外，对涉及提示调优的方法的关注转移了对优化和简化输入信息的注意力。本研究旨在填补这些空白，通过展示
    LLM 在处理长序列时在经典 NLP 任务上的失败，并激发研究社区对优化和浓缩输入信息的未开发潜力的兴趣。
- en: Contribution.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献。
- en: 'Our main contributions are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是：
- en: '1.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We systematically study the performance of state-of-the-art LLMs on sentiment
    analysis and news categorization tasks, revealing their limitations in processing
    long-form text effectively.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们系统地研究了最先进的LLM在情感分析和新闻分类任务中的表现，揭示了它们在有效处理长文本方面的局限性。
- en: '2.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose and evaluate ad-hoc solutions using extractive and diverse summarization
    as well as selective truncation to condense input text, which substantially improves
    LLM performance by up to 50%, reduces API costs by as much as 93% and significantly
    reduces latency.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出并评估了使用提取式和多样化摘要以及选择性截断的临时解决方案，以浓缩输入文本，这大大提高了LLM的性能，最多提高了50%，减少了API成本高达93%，并显著降低了延迟。
- en: '3.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We present comprehensive empirical and ablation studies examining the relationship
    between input length, summarization strategies, and model performance, providing
    insights into optimal summarization approaches for LLMs.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了全面的实证研究和消融研究，检查输入长度、摘要策略和模型性能之间的关系，提供了LLM的最佳摘要方法的见解。
- en: 2 Methodology
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: We first present our methodology on two pipelines for summarising information
    before prompting the LLM. Then we discuss all the scenarios we examine to analyze
    LLMs’s performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了在提示LLM之前用于总结信息的两种方法论。然后我们讨论了我们检查的所有场景，以分析LLM的性能。
- en: 2.1 Summarization Methodology
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 摘要方法论
- en: 'We study two different summarization approaches for extracting key information
    from the documents and providing the input for prompting:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了两种不同的摘要方法，用于从文档中提取关键信息，并为提示提供输入：
- en: '1\. Pure Extractive Summarization:'
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1\. 纯提取式摘要：
- en: 'As shown in [Fig. 1(a)](#S1.F1.sf1 "In Fig. 1 ‣ Related Work. ‣ 1 Introduction
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly"), we use TextRank (Mihalcea
    and Tarau, [2004](#bib.bib18)), a well-known unsupervised extractive summarization
    algorithm, to select the most important sentences. TextRank uses a graph-based
    ranking model to measure the similarity between sentences and their centrality
    within the graph. We then write the instruction to the LLM (i.e., categorize or
    rate) and append the extracted sentences as input.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图1(a)](#S1.F1.sf1 "图1 ‣ 相关工作 ‣ 1 引言 ‣ LLM的有趣失败的高效解决方案：长上下文窗口并不意味着LLM能无缝分析长序列")所示，我们使用了TextRank（Mihalcea
    和 Tarau，[2004](#bib.bib18)），这是一个著名的无监督提取式摘要算法，来选择最重要的句子。TextRank使用基于图的排名模型来测量句子之间的相似性及其在图中的中心性。然后，我们将指令写入LLM（即，分类或评分），并将提取的句子作为输入附加。
- en: '2\. Diverse Summarization:'
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2\. 多样化摘要：
- en: 'Build on top of the previous approach. We discard the least relevant sentences
    using TextRank ranking. Then we use TF-IDF to represent the sentences as vectors
    and calculate the diversity scores based on the dissimilarity between sentences
    using cosine similarity. The top N sentences with the highest diversity scores
    are chosen as the input used in prompting (see [§ A.2.1](#A1.SS2.SSS1 "A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly")). This extension maximises the diversity of
    the information for the LLMs instead of providing the LLMs with a concise summary.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前述方法进行扩展。我们使用TextRank排名丢弃最不相关的句子。然后，我们使用TF-IDF将句子表示为向量，并基于句子之间的相似性计算多样性分数。选择具有最高多样性分数的前N个句子作为提示中使用的输入（参见[§
    A.2.1](#A1.SS2.SSS1 "A.2.1 多样化摘要 ‣ A.2 新闻分类 ‣ 附录A 附录 ‣ LLM的有趣失败的高效解决方案：长上下文窗口并不意味着LLM能无缝分析长序列")）。该扩展最大化了LLM的信息多样性，而不是提供LLM一个简明的摘要。
- en: 2.2 Prompting Scenarios
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 提示场景
- en: 'To investigate the performance of LLMs on sentiment analysis and news categorization
    tasks involving long input sequences, we employ 7 prompting strategies and evaluate
    their effectiveness on three datasets, which we introduce in the next section.
    These prompting strategies include:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究LLM在涉及长输入序列的情感分析和新闻分类任务中的表现，我们使用了7种提示策略，并评估了它们在三个数据集上的有效性，我们将在下一节介绍这些数据集。这些提示策略包括：
- en: '1.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Full Context: The entire lengthy review is provided as input for analysis (Motiv.:
    the baseline approach for comparison with other methods).'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整上下文：将整个冗长的评论作为输入进行分析（动机：与其他方法进行比较的基准方法）。
- en: '2.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Full Context + Summary: The $N$-sentence summary extracted using [Fig. 1(a)](#S1.F1.sf1
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly") pipeline is appended to the lengthy review. (Motiv.: how does emphasizing
    a selected summary with repetition affect the performance?)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完整上下文+摘要：使用[图1(a)](#S1.F1.sf1 "图1 ‣ 相关工作 ‣ 1 引言 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")管道提取的$N$句摘要被附加到长篇评论中。（动机：强调选择的摘要与重复的结合如何影响性能？）
- en: '3.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'First Sentences: We crop the initial $N$ sentences from the text and provide
    it as input. (Motiv.: how does choosing the ‘opening’ section of a lengthy review
    affect the performance?)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首句：我们剪裁文本开头的$N$句并将其作为输入。（动机：选择长篇评论的‘开头’部分如何影响性能？）
- en: '4.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Last Sentences: We crop the ending $N$ sentences from the text and provide
    it as input. (Motiv.: how does choosing the ‘ending’ section of a lengthy review
    affect the performance?)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后句子：我们剪裁文本末尾的$N$句并将其作为输入。（动机：选择长篇评论的‘结尾’部分如何影响性能？）
- en: '5.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Summary: We provide the extracted $N$ sentence summary ([Fig. 1(a)](#S1.F1.sf1
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly")) as input. (Motiv.: how does choosing a summary affect performance?)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要：我们提供提取的$N$句摘要（[图1(a)](#S1.F1.sf1 "图1 ‣ 相关工作 ‣ 1 引言 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")）作为输入。（动机：选择摘要如何影响性能？）
- en: '6.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Diverse Summary: We provide the extracted $N$-sentence summary ([Fig. 1(b)](#S1.F1.sf2
    "In Fig. 1 ‣ Related Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly")) as input. (Motiv.: how does giving more priority to lexical diversity
    in the summary affect performance?)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样化摘要：我们提供提取的$N$句摘要（[图1(b)](#S1.F1.sf2 "图1 ‣ 相关工作 ‣ 1 引言 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")）作为输入。（动机：在摘要中优先考虑词汇多样性如何影响性能？）
- en: '7.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Random Sampling: We randomly select $N$ sentences from the document. (Motiv.:
    how does randomly choosing a short snippet perform in comparison to providing
    the full context?)'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机抽样：我们从文档中随机选择$N$句。（动机：随机选择短片段与提供完整上下文相比，效果如何？）
- en: 3 Evaluation
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 评估
- en: 3.1 Datasets
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'We now introduce the datasets used in this paper (see [§ A](#A1 "Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") for more details).
    For each dataset, as our interest is in LLM performance with long inputs, we use
    only the subsets of the data that exceed a minimum length. We report the average
    length of the studied subset in terms of the number of tokens in [Tabs. 6](#A1.T6
    "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly"), [4](#A1.T4 "Tab. 4 ‣ BBC
    News Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [5](#A1.T5 "Tab. 5 ‣ BBC News Archive.
    ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍本文使用的数据集（见[§A](#A1 "附录A 附录 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")以获取更多细节）。由于我们的兴趣在于LLM对长输入的性能，我们仅使用超过最小长度的数据子集。我们报告研究子集的平均长度，以[表6](#A1.T6
    "在A.2.1 多样化总结 ‣ A.2 新闻分类 ‣ 附录A 附录 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")、[4](#A1.T4
    "表4 ‣ BBC新闻档案 ‣ A.2 新闻分类 ‣ 附录A 附录 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")和[5](#A1.T5
    "表5 ‣ BBC新闻档案 ‣ A.2 新闻分类 ‣ 附录A 附录 ‣ 解决LLMs的长上下文窗口并不意味着LLMs可以无瑕分析长序列的有趣失败问题")中的标记数来衡量。
- en: 'GameSpot Reviews GameSpot ([2024](#bib.bib9)):'
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'GameSpot 评论 GameSpot ([2024](#bib.bib9)):'
- en: more than 12,000 long game reviews with a sentiment score assigned by the author
    ranging from 1 to 100.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 超过12,000篇长游戏评论，作者赋予情感分数范围从1到100。
- en: '20 Newsgroups Lang ([1995](#bib.bib13)):'
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 20 新组 Lang ([1995](#bib.bib13))：
- en: nearly 20,000 news documents belonging to 20 different topic categories. High-level
    topics include politics, religion, sports, and computers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎 20,000 篇新闻文档，属于 20 个不同的话题类别。高级话题包括政治、宗教、体育和计算机。
- en: 'BBC News Archive Greene and Cunningham ([2006](#bib.bib11)):'
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: BBC 新闻档案 Greene 和 Cunningham ([2006](#bib.bib11))：
- en: 2,225 BBC articles covering business, entertainment, politics, sport, and tech.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2,225 篇 BBC 文章，涵盖商业、娱乐、政治、体育和科技。
- en: 3.2 Experiments
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验
- en: 'We evaluated the performance of Claude 3 Haiku, Gemini-1.0-Pro, GPT-3.5 Turbo,
    Llama 3 8b Instruct, and Mistral 7b Instruct on the datasets and tasks detailed
    in [§ 3.1](#S3.SS1 "3.1 Datasets ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly"), using the prompting scenarios discussed in [§ 2.2](#S2.SS2 "2.2
    Prompting Scenarios ‣ 2 Methodology ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly").
    The summarized results are available in [Tabs. 1](#S3.T1 "In 3.2.1 Sentiment Analysis
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"),
    [2](#S3.T2 "Tab. 2 ‣ 3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [3](#S3.T3 "Tab. 3 ‣
    3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly"). More detailed analysis for each LLM is available
    in [Tabs. 6](#A1.T6 "In A.2.1 Diverse Summarization ‣ A.2 News Categorization
    ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs:
    Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"),
    [4](#A1.T4 "Tab. 4 ‣ BBC News Archive. ‣ A.2 News Categorization ‣ Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [5](#A1.T5
    "Tab. 5 ‣ BBC News Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣
    Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly") in [§ A](#A1 "Appendix A
    Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了 Claude 3 Haiku、Gemini-1.0-Pro、GPT-3.5 Turbo、Llama 3 8b Instruct 和 Mistral
    7b Instruct 在 [§ 3.1](#S3.SS1 "3.1 数据集 ‣ 3 评估 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着
    LLMs 能够无误分析长序列")、[§ 2.2](#S2.SS2 "2.2 提示场景 ‣ 2 方法论 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着
    LLMs 能够无误分析长序列") 中详细说明的数据集和任务上的表现。总结结果可以在 [表 1](#S3.T1 "在 3.2.1 情感分析 ‣ 3.2 实验
    ‣ 3 评估 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着 LLMs 能够无误分析长序列")、[2](#S3.T2 "表 2 ‣ 3.2.2
    新闻分类 ‣ 3.2 实验 ‣ 3 评估 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着 LLMs 能够无误分析长序列") 和 [3](#S3.T3
    "表 3 ‣ 3.2.2 新闻分类 ‣ 3.2 实验 ‣ 3 评估 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着 LLMs 能够无误分析长序列")
    中找到。每个 LLM 的更详细分析见 [表 6](#A1.T6 "在 A.2.1 多样化总结 ‣ A.2 新闻分类 ‣ 附录 A 附录 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着
    LLMs 能够无误分析长序列")、[4](#A1.T4 "表 4 ‣ BBC 新闻档案 ‣ A.2 新闻分类 ‣ 附录 A 附录 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着
    LLMs 能够无误分析长序列") 和 [5](#A1.T5 "表 5 ‣ BBC 新闻档案 ‣ A.2 新闻分类 ‣ 附录 A 附录 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着
    LLMs 能够无误分析长序列") 中 [§ A](#A1 "附录 A 附录 ‣ LLMs 的有趣失败的高效解决方案：长上下文窗口并不意味着 LLMs 能够无误分析长序列")。
- en: 3.2.1 Sentiment Analysis
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 情感分析
- en: '| Scenario | MSE | MAE | Accuracy | Avg. Lat. | Inp. Len. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 均方误差 | 平均绝对误差 | 准确率 | 平均延迟 | 输入长度 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Full | 272.8 (6) | 11.7 (6) | 36.0 (7) | 1.27 (6) | 2120 (6) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 272.8 (6) | 11.7 (6) | 36.0 (7) | 1.27 (6) | 2120 (6) |'
- en: '| Full+Sum. | 403.1 (7) | 13.5 (7) | 38.2 (6) | 1.33 (7) | 2450 (7) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 全部+总结 | 403.1 (7) | 13.5 (7) | 38.2 (6) | 1.33 (7) | 2450 (7) |'
- en: '| First Sent. | 169.1 (5) | 9.9 (5) | 41.2 (5) | 0.82 (1) | 320 (1) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 首句 | 169.1 (5) | 9.9 (5) | 41.2 (5) | 0.82 (1) | 320 (1) |'
- en: '| Last Sent. | 99.6 (1) | 7.9 (1) | 50.7 (2) | 0.82 (1) | 320 (1) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 最后一句 | 99.6 (1) | 7.9 (1) | 50.7 (2) | 0.82 (1) | 320 (1) |'
- en: '| Sum. | 124.2 (2) | 8.8 (4) | 48.2 (4) | 0.82 (1) | 320 (1) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 总结 | 124.2 (2) | 8.8 (4) | 48.2 (4) | 0.82 (1) | 320 (1) |'
- en: '| Div. Sum. | 133.7 (4) | 8.6 (2) | 54.0 (1) | 0.82 (1) | 320 (1) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Div. Sum. | 133.7 (4) | 8.6 (2) | 54.0 (1) | 0.82 (1) | 320 (1) |'
- en: '| Rand. Samp. | 129.6 (3) | 8.7 (3) | 50.0 (3) | 0.82 (1) | 320 (1) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Rand. Samp. | 129.6 (3) | 8.7 (3) | 50.0 (3) | 0.82 (1) | 320 (1) |'
- en: 'Table 1: Average performance of different LLMs for Sentiment Analysis on GameSpot
    over 5 runs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在GameSpot上进行5次运行的不同LLMs在情感分析中的平均性能。
- en: 'As shown in [Tab. 1](#S3.T1 "In 3.2.1 Sentiment Analysis ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), both Full and
    Full+Sum approaches failed to perform favourably in predicting the article scores.
    However, extracting a subset of the input text, and providing it in the prompt,
    even through randomly sampling sentences, yielded superior performance. The ‘Last
    Sent.’ scenario performs the best in both loss metrics while the ‘Div Sum.’ achieves
    the highest accuracy by a substantial margin, performing 50% better than when
    providing the LLM with Full Context. Detailed analysis of the performance for
    each LLM on the GameSpot dataset is available in [Tab. 6](#A1.T6 "In A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表1](#S3.T1 "在3.2.1情感分析 ‣ 3.2实验 ‣ 3评估 ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")所示，Full和Full+Sum方法在预测文章评分方面表现不佳。然而，通过提取输入文本的一个子集，并将其作为提示，即使是随机抽取句子，也能取得更好的效果。‘Last
    Sent.’场景在两个损失指标中表现最佳，而‘Div Sum.’在准确率上取得了显著优势，比提供完整上下文的LLM高出50%。每个LLM在GameSpot数据集上的详细性能分析见[表6](#A1.T6
    "在A.2.1多样化总结 ‣ A.2新闻分类 ‣ 附录A ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")。
- en: 3.2.2 News Categorization
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 新闻分类
- en: 'We evaluated the performance of LLMs on two news categorization datasets. [Tabs. 2](#S3.T2
    "In 3.2.2 News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [3](#S3.T3 "Tab. 3 ‣ 3.2.2 News Categorization
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly")
    summarize these results across different prompting scenarios. Detailed analyses
    for each LLM for both experiments are available in [Tabs. 4](#A1.T4 "In BBC News
    Archive. ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly") and [5](#A1.T5 "Tab. 5 ‣ BBC News Archive.
    ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly") in [§ A](#A1 "Appendix A Appendix ‣ Efficient Solutions For An Intriguing
    Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences
    Flawlessly").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了LLMs在两个新闻分类数据集上的表现。[表2](#S3.T2 "在3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")和[3](#S3.T3
    "表3 ‣ 3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")总结了这些结果在不同提示场景下的表现。每个LLM在两个实验中的详细分析见[表4](#A1.T4
    "在BBC新闻档案 ‣ A.2新闻分类 ‣ 附录A ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")和[5](#A1.T5
    "表5 ‣ BBC新闻档案 ‣ A.2新闻分类 ‣ 附录A ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")，详见[§
    A](#A1 "附录A ‣ 针对LLMs的有趣失败的高效解决方案：长上下文窗口并不意味着LLMs可以无误分析长序列")。
- en: '![Refer to caption](img/996b00b9bcbbd8517359b996e543c9b5.png)![Refer to caption](img/d2d2463722598a9432f38ff1f4577ff2.png)![Refer
    to caption](img/686f7df516d501cd64167a52c8e819de.png)![Refer to caption](img/383fad1446ffb9fe6e8cc1fdf87e25bf.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/996b00b9bcbbd8517359b996e543c9b5.png)![参考说明](img/d2d2463722598a9432f38ff1f4577ff2.png)![参考说明](img/686f7df516d501cd64167a52c8e819de.png)![参考说明](img/383fad1446ffb9fe6e8cc1fdf87e25bf.png)'
- en: 'Figure 2: Ablation study on the length of the selected truncation/summary for
    different scenarios using Claude 3 Haiku over 5 runs with 85% Confidence Intervals.
    The results show the efficacy of approaches optimizing LLMs’ input. ‘Full’ context
    performs poorly on all metrics. Additionally, after the length of input surpasses
    10 sentences, less meaningful improvement in the performance of all scenarios
    is observed. A similar trend is seen for all LLMs.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同场景下使用Claude 3 Haiku对选定截断/总结长度进行的消融研究，进行了5次运行，置信区间为85%。结果显示了优化LLMs输入方法的有效性。‘全部’上下文在所有指标上表现较差。此外，当输入长度超过10个句子后，所有场景的性能提升变得不太显著。所有LLMs均表现出类似的趋势。
- en: '| Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | Mac. F1 | 准确率 | 平均延迟 | 输入长度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Full | 0.27 (7) | 35.2 (7) | 1.58 (6) | 3450 (6) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 0.27 (7) | 35.2 (7) | 1.58 (6) | 3450 (6) |'
- en: '| Full+Sum. | 0.30 (3) | 38.2 (5) | 1.94 (7) | 3700 (7) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 全部+总和 | 0.30 (3) | 38.2 (5) | 1.94 (7) | 3700 (7) |'
- en: '| First Sent. | 0.30 (3) | 39.1 (3) | 0.79 (1) | 240 (1) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 第一句 | 0.30 (3) | 39.1 (3) | 0.79 (1) | 240 (1) |'
- en: '| Last Sent. | 0.29 (5) | 39.1 (3) | 0.79 (1) | 240 (1) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 最后一个句子 | 0.29 (5) | 39.1 (3) | 0.79 (1) | 240 (1) |'
- en: '| Sum. | 0.31 (1) | 39.4 (2) | 0.79 (1) | 240 (1) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 总和 | 0.31 (1) | 39.4 (2) | 0.79 (1) | 240 (1) |'
- en: '| Div. Sum. | 0.31 (1) | 39.5 (1) | 0.79 (1) | 240 (1) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 除法总和 | 0.31 (1) | 39.5 (1) | 0.79 (1) | 240 (1) |'
- en: '| Rand. Samp. | 0.29 (5) | 37.6 (6) | 0.79 (1) | 240 (1) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 随机采样 | 0.29 (5) | 37.6 (6) | 0.79 (1) | 240 (1) |'
- en: 'Table 2: Average performance of different LLMs for news categorization on 20
    NewsGroup over 5 runs.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同LLMs在20 NewsGroup上进行新闻分类的平均表现，进行了5次运行。
- en: 'For the 20 NewsGroup dataset (see [Tab. 2](#S3.T2 "In 3.2.2 News Categorization
    ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure
    of LLMs: Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"))
    both ‘Sum.’ and ‘Div. Sum.’ approaches achieve the highest F1 scores. With a 39.5%
    accuracy, ‘Div. Sum’ outperforms all other scenarios in this metric. Importantly,
    all approaches achieve better results than providing the full context to the LLM,
    showing the effectiveness of summarising the information provided to the LLM for
    this dataset.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于20 NewsGroup数据集（参见 [表2](#S3.T2 "在3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 解决LLMs的一个有趣失败：长上下文窗口并不意味着LLMs可以完美分析长序列")），‘总和’和‘除法总和’方法都取得了最高的F1分数。‘除法总和’以39.5%的准确率在这个指标上超越了所有其他场景。重要的是，所有方法的结果都优于提供完整上下文给LLM的结果，显示了对LLM总结信息的有效性。
- en: '| Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | Mac. F1 | 准确率 | 平均延迟 | 输入长度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Full | 0.51 (6) | 54.0 (6) | 1.07 (6) | 1150 (6) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 0.51 (6) | 54.0 (6) | 1.07 (6) | 1150 (6) |'
- en: '| Full+Sum. | 0.50 (7) | 53.7 (7) | 1.72 (7) | 1400 (7) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 全部+总和 | 0.50 (7) | 53.7 (7) | 1.72 (7) | 1400 (7) |'
- en: '| First Sent. | 0.61 (1) | 64.5 (1) | 0.78 (1) | 230 (1) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 第一句 | 0.61 (1) | 64.5 (1) | 0.78 (1) | 230 (1) |'
- en: '| Last Sent. | 0.53 (4) | 57.9 (4) | 0.78 (1) | 230 (1) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 最后一个句子 | 0.53 (4) | 57.9 (4) | 0.78 (1) | 230 (1) |'
- en: '| Sum. | 0.56 (3) | 59.8 (3) | 0.78 (1) | 230 (1) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 总和 | 0.56 (3) | 59.8 (3) | 0.78 (1) | 230 (1) |'
- en: '| Div. Sum. | 0.58 (2) | 60.9 (2) | 0.78 (1) | 230 (1) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 除法总和 | 0.58 (2) | 60.9 (2) | 0.78 (1) | 230 (1) |'
- en: '| Rand. Samp. | 0.53 (4) | 57.6 (5) | 0.78 (1) | 230 (1) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 随机采样 | 0.53 (4) | 57.6 (5) | 0.78 (1) | 230 (1) |'
- en: 'Table 3: Average performance of different LLMs for news categorization task
    on BBC News over 5 runs.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同LLMs在BBC新闻上进行新闻分类任务的平均表现，进行了5次运行。
- en: '![Refer to caption](img/3a62e3b116d69fabde4171d4b7264465.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a62e3b116d69fabde4171d4b7264465.png)'
- en: 'Figure 3: Performance gain (% accuracy boost) vs. normalized ARP score for
    each summarization/truncation scenario compared to the ‘Full’ baseline. Lower
    ARP scores (more cohesive corpora) generally yield higher performance gains across
    scenarios.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：每个总结/截断场景与‘全部’基准相比的性能提升（%准确率提升）与归一化ARP分数的关系。较低的ARP分数（更具凝聚性的语料）通常在各个场景中带来更高的性能提升。
- en: 'As summarized in [Tab. 3](#S3.T3 "In 3.2.2 News Categorization ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), we observe
    a similar trend for the BBC dataset: all approaches except ‘Full+Sum.’ outperform
    ‘Full’ scenario in all metrics. This emphasizes the importance of selectively
    summarising the information provided to LLMs.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [表3](#S3.T3 "在3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 解决LLMs的一个有趣失败：长上下文窗口并不意味着LLMs可以完美分析长序列")
    所总结，对于BBC数据集也观察到类似趋势：除‘全部+总和’外的所有方法在所有指标上都优于‘全部’场景。这突显了选择性总结提供给LLMs信息的重要性。
- en: 3.2.3 Ablation Studies
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 消融研究
- en: 'To further investigate the performance of our models under different conditions,
    we conducted two ablation studies. First, we varied the truncation/summary lengths
    from 3 to 15 sentences for all models and evaluated their sentiment analysis performance
    on the GameSpot dataset ([Fig. 2](#S3.F2 "In 3.2.2 News Categorization ‣ 3.2 Experiments
    ‣ 3 Evaluation ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context
    Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly") and [Fig. 5](#A1.F5
    "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix
    ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long Context Window Does
    Not Mean LLMs Can Analyze Long Sequences Flawlessly")). Our solutions consistently
    outperformed the baseline across different summary lengths. Second, we studied
    the effect of temperature on the Mistral Instruct and Llama 3 Instruct models
    for news categorization on the 20 NewsGroup dataset ([Fig. 4](#A1.F4 "In A.2.1
    Diverse Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient
    Solutions For An Intriguing Failure of LLMs: Long Context Window Does Not Mean
    LLMs Can Analyze Long Sequences Flawlessly")). Again, the summarization approaches
    demonstrated superior performance compared to the baseline across different temperature
    settings.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步调查我们模型在不同条件下的表现，我们进行了两项消融研究。首先，我们将所有模型的截断/摘要长度从3到15句进行了变化，并评估了它们在GameSpot数据集上的情感分析性能（[图2](#S3.F2
    "在3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 解决LLMs长上下文窗口并不意味着LLMs能够完美分析长序列的有趣失败问题的高效解决方案") 和 [图5](#A1.F5
    "在A.2.1多样化总结 ‣ A.2新闻分类 ‣ 附录A 附录 ‣ 解决LLMs长上下文窗口并不意味着LLMs能够完美分析长序列的有趣失败问题的高效解决方案")）。我们的解决方案在不同的摘要长度下始终优于基准。其次，我们研究了温度对Mistral
    Instruct和Llama 3 Instruct模型在20 NewsGroup数据集上的新闻分类效果（[图4](#A1.F4 "在A.2.1多样化总结 ‣
    A.2新闻分类 ‣ 附录A 附录 ‣ 解决LLMs长上下文窗口并不意味着LLMs能够完美分析长序列的有趣失败问题的高效解决方案")）。同样，摘要方法在不同温度设置下表现出优于基准的性能。
- en: 4 Discussion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: This study’s findings have significant implications for developing and deploying
    LLMs. Despite their long context windows, state-of-the-art LLMs still struggle
    to effectively process long text sequences, a critical limitation under-examined
    by prior research for common NLP tasks relying on contextual understanding. Our
    results highlight the need for more research into optimizing lengthy text inputs
    to enhance LLM performance. Future work should explore the generalizability of
    these findings across diverse optimization techniques, NLP tasks, and data domains.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的发现对开发和部署LLMs具有重要意义。尽管具有较长的上下文窗口，最先进的LLMs仍然难以有效处理长文本序列，这是以前研究中未充分探讨的一个关键限制，尤其是在依赖上下文理解的常见NLP任务中。我们的结果突显了对优化长文本输入以提升LLM性能的更多研究的需求。未来的工作应探索这些发现是否在不同的优化技术、NLP任务和数据领域中具有普遍性。
- en: 'We conducted a preliminary analysis of the correlation between document cohesiveness
    and performance gains for each corpus. Using Average Relative Proximity (ARP)
    score (Ghinassi et al., [2023](#bib.bib10)) with average cosine similarity scoring
    over 2-sentence segments, we found that more cohesive corpora (lower ARP) had
    higher average performance gains from summarization ([Fig. 3](#S3.F3 "In 3.2.2
    News Categorization ‣ 3.2 Experiments ‣ 3 Evaluation ‣ Efficient Solutions For
    An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can Analyze
    Long Sequences Flawlessly")). Future studies could establish these findings further.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个语料库的文档连贯性与性能提升之间的相关性进行了初步分析。使用平均相对接近度（ARP）评分（Ghinassi et al., [2023](#bib.bib10)）和在2句子段上的平均余弦相似度评分，我们发现更具连贯性的语料库（较低的ARP）在总结中的平均性能提升更高（[图3](#S3.F3
    "在3.2.2新闻分类 ‣ 3.2实验 ‣ 3评估 ‣ 解决LLMs长上下文窗口并不意味着LLMs能够完美分析长序列的有趣失败问题的高效解决方案")）。未来的研究可以进一步验证这些发现。
- en: 5 Conclusions
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This paper examined the performance of lengthy inputs on various LLMs (Claude
    3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct, and Mistral Instruct). We found
    that the longest inputs resulted in the worst performance. Our results were consistent
    for three datasets and two tasks. We proposed several ad-hoc solutions to substantially
    enhance LLMs’ performance (up to 50%) on long input sequences and succeeded in
    reducing API cost (by up to 93%) while reducing the average latency (up to 50%).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本文检查了不同LLM（Claude 3、Gemini Pro、GPT 3.5 Turbo、Llama 3 Instruct 和 Mistral Instruct）在处理长输入时的表现。我们发现，最长的输入导致了最差的性能。我们的结果在三个数据集和两个任务中是一致的。我们提出了几种特定的解决方案，显著提高了LLM在长输入序列上的性能（最高提升50%），并成功降低了API成本（最高减少93%），同时减少了平均延迟（最高减少50%）。
- en: Limitations
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Rapid change of model specifications.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型规格的快速变化。
- en: First, we examined a diverse set of state-of-the-art large language models,
    all of which are among the most commonly used LLMs. However, the rapidly evolving
    nature of this field means these findings may not be fully generalized to future
    LLMs with different architectures and training paradigms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查了一组多样化的最先进的大型语言模型，这些模型都是最常用的LLM。然而，该领域的迅速发展意味着这些发现可能无法完全推广到具有不同架构和训练模式的未来LLM中。
- en: Tasks requiring general context understanding.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 需要一般上下文理解的任务。
- en: Second, although we evaluated performance on two core NLP tasks (sentiment analysis
    and news categorization) across three datasets, further research is needed to
    determine if our conclusions hold for a wider range of natural language understanding
    tasks and domains. The datasets we used, while lengthy, may not fully capture
    the types of long-form content that LLMs will need to process in other real-world
    applications. However, our studies here laid the foundation to show the limitations
    of LLMs when dealing with long sequences even in canonical NLP tasks as an underexplored
    problem. We believe our ad-hoc solutions are applicable to a wide variety of tasks
    that require a general understanding of the input sequence rather than a detailed
    understanding of the whole context.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，尽管我们在三个数据集上评估了两项核心NLP任务（情感分析和新闻分类）的性能，但仍需进一步研究以确定我们的结论是否适用于更广泛的自然语言理解任务和领域。我们使用的数据集虽然很长，但可能无法完全涵盖LLM在其他实际应用中需要处理的长篇内容。然而，我们的研究奠定了基础，显示了LLM在处理长序列时的局限性，即使在经典NLP任务中也是一个尚未充分探索的问题。我们相信，我们的特定解决方案适用于需要对输入序列进行一般理解的各种任务，而不是对整个上下文的详细理解。
- en: Societal Impact
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 社会影响
- en: In terms of societal impact, we believe our findings can help enable the more
    effective and efficient application of LLMs to tasks involving longer documents,
    which has the potential to unlock significant value in domains like business analytics,
    legal contract review, and scientific literature mining. In addition, our ad-hoc
    solutions are a step forward for democratizing access to AI. Even though using
    LLM APIs is becoming more affordable, our approaches reduce the API cost for users
    by up to 93% and this further enables more accessibility across different sections
    of society. At the same time, the ability to extract key information from lengthy
    privacy policies, terms of service, and other consumer agreements could be misused
    in ways that fail to represent the full context. As LLMs achieve greater summarization
    capabilities, extra care will be needed to ensure these summaries are accurate,
    unbiased and not misused for deceptive purposes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在社会影响方面，我们相信我们的发现可以帮助更有效和高效地将LLM应用于涉及较长文档的任务，这有可能在业务分析、法律合同审查和科学文献挖掘等领域释放出重要价值。此外，我们的特定解决方案是推进AI普及化的一步。尽管使用LLM
    API变得越来越经济，但我们的方法将用户的API成本降低了多达93%，这进一步提高了社会各个层面的可及性。同时，从冗长的隐私政策、服务条款和其他消费者协议中提取关键信息的能力可能被滥用，无法代表完整的上下文。随着LLM达到更高的总结能力，将需要额外的注意，以确保这些总结准确、公正，并且不会被用于欺骗性目的。
- en: Overall, while our work provides important empirical insights into the limitations
    of current LLMs on long sequence tasks and highlights promising directions for
    overcoming these challenges, we see it as a motivating starting point rather than
    a conclusive result. We encourage the research community to further test and expand
    on our findings to drive the development of more capable and robust prompting
    techniques.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，虽然我们的工作提供了对当前LLM在长序列任务中的局限性的重要实证见解，并突出了克服这些挑战的有前景的方向，但我们将其视为一个激励的起点，而不是最终结果。我们鼓励研究社区进一步测试和扩展我们的发现，以推动更强大和稳健的提示技术的发展。
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Anthropic (2024) Anthropic. 2024. Claude. [https://www.anthropic.com](https://www.anthropic.com).
    Version claude-3-haiku-20240307\. Accessed: 2024-04-05 to 2024-06-11.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024) Anthropic. 2024. Claude. [https://www.anthropic.com](https://www.anthropic.com).
    版本 claude-3-haiku-20240307\. 访问时间：2024-04-05 到 2024-06-11。
- en: Balcerzak et al. (2014) Bartomiej Balcerzak, Wojciech Jaworski, and Adam Wierzbicki.
    2014. Application of textrank algorithm for credibility assessment. In *2014 IEEE/WIC/ACM
    International Joint Conferences on Web Intelligence (WI) and Intelligent Agent
    Technologies (IAT)*, volume 1, pages 451–454\. IEEE.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balcerzak et al. (2014) Bartomiej Balcerzak, Wojciech Jaworski, 和 Adam Wierzbicki.
    2014. 使用textrank算法进行可信度评估。见于*2014 IEEE/WIC/ACM国际联合会议：网页智能（WI）和智能代理技术（IAT）*，第1卷，第451–454页。IEEE。
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, 和 Arman Cohan. 2020. Longformer：长文档变换器。*arXiv预印本
    arXiv:2004.05150*。
- en: 'Bertsch et al. (2024) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew
    Gormley. 2024. Unlimiformer: Long-range transformers with unlimited length input.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsch et al. (2024) Amanda Bertsch, Uri Alon, Graham Neubig, 和 Matthew Gormley.
    2024. Unlimiformer：具有无限长度输入的长距离变换器。*神经信息处理系统进展*，36。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Cachola et al. (2020) Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld.
    2020. TLDR: Extreme summarization of scientific documents. In *Findings of the
    Association for Computational Linguistics: EMNLP 2020*, pages 4766–4777, Online.
    Association for Computational Linguistics.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cachola et al. (2020) Isabel Cachola, Kyle Lo, Arman Cohan, 和 Daniel Weld. 2020.
    TLDR：科学文献的极端总结。见于*计算语言学协会发现：EMNLP 2020*，第4766–4777页，在线。计算语言学协会。
- en: 'FacebookResearch (2024) FacebookResearch. 2024. Llama3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3).
    Accessed: 2024-05-05 to 2024-06-11.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FacebookResearch (2024) FacebookResearch. 2024. Llama3. [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3).
    访问时间：2024-05-05 到 2024-06-11。
- en: 'Feng et al. (2022) Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2022. A survey
    on dialogue summarization: Recent advances and new frontiers. In *Proceedings
    of the Thirty-First International Joint Conference on Artificial Intelligence,
    IJCAI-22*, pages 5453–5460\. International Joint Conferences on Artificial Intelligence
    Organization. Survey Track.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2022) Xiachong Feng, Xiaocheng Feng, 和 Bing Qin. 2022. 对话总结的调查：近期进展与新前沿。见于*第31届国际人工智能联合会议论文集，IJCAI-22*，第5453–5460页。国际人工智能联合会议组织。调查专题。
- en: 'GameSpot (2024) GameSpot. 2024. [Gamespot reviews](https://www.gamespot.com/games/reviews/).
    Accessed: 2024-02-05 to 2024-06-11.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GameSpot (2024) GameSpot. 2024. [Gamespot评论](https://www.gamespot.com/games/reviews/)。访问时间：2024-02-05
    到 2024-06-11。
- en: 'Ghinassi et al. (2023) Iacopo Ghinassi, Lin Wang, Chris Newell, and Matthew
    Purver. 2023. Comparing neural sentence encoders for topic segmentation across
    domains: not your typical text similarity task. *PeerJ Computer Science*, 9:e1593.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghinassi et al. (2023) Iacopo Ghinassi, Lin Wang, Chris Newell, 和 Matthew Purver.
    2023. 比较跨领域的神经句子编码器用于主题分割：不是你典型的文本相似性任务。*PeerJ计算机科学*，9:e1593。
- en: Greene and Cunningham (2006) Derek Greene and Pádraig Cunningham. 2006. Practical
    solutions to the problem of diagonal dominance in kernel document clustering.
    In *Proc. 23rd International Conference on Machine learning (ICML’06)*, pages
    377–384\. ACM Press.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greene and Cunningham (2006) Derek Greene 和 Pádraig Cunningham. 2006. 解决核文档聚类中的对角主导性问题的实际解决方案。见于*第23届国际机器学习会议（ICML’06）论文集*，第377–384页。ACM出版社。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier 等人. 2023. Mistral 7b。 *arXiv 预印本 arXiv:2310.06825*。
- en: 'Lang (1995) Ken Lang. 1995. Newsweeder: Learning to filter netnews. In *Proceedings
    of the Twelfth International Conference on Machine Learning*, pages 331–339.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lang (1995) Ken Lang. 1995. Newsweeder: 学习过滤网络新闻。发表于 *第十二届国际机器学习会议论文集*，第 331–339
    页。'
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can context
    length of open-source llms truly promise? In *NeurIPS 2023 Workshop on Instruction
    Tuning and Instruction Following*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
    Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 2023. 开源 llms 的上下文长度究竟能保证多长？在 *NeurIPS
    2023 Workshop on Instruction Tuning and Instruction Following*。
- en: Li et al. (2024) Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen.
    2024. Long-context llms struggle with long in-context learning. *arXiv preprint
    arXiv:2404.02060*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2024) Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue 和 Wenhu Chen. 2024.
    长上下文 llms 在长上下文学习中表现挣扎。 *arXiv 预印本 arXiv:2404.02060*。
- en: 'Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Comput. Surv.*,
    55(9).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi
    和 Graham Neubig. 2023. 预训练、提示和预测：自然语言处理中的提示方法系统综述。 *ACM 计算机调查*，55(9)。
- en: Machlab and Battle (2024) Daniel Machlab and Rick Battle. 2024. Llm in-context
    recall is prompt dependent. *arXiv preprint arXiv:2404.08865*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Machlab 和 Battle (2024) Daniel Machlab 和 Rick Battle. 2024. Llm 上下文回忆依赖于提示。
    *arXiv 预印本 arXiv:2404.08865*。
- en: 'Mihalcea and Tarau (2004) Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing
    order into text. In *Proceedings of the 2004 Conference on Empirical Methods in
    Natural Language Processing*, pages 404–411\. Association for Computational Linguistics.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mihalcea 和 Tarau (2004) Rada Mihalcea 和 Paul Tarau. 2004. TextRank: 将秩序引入文本。发表于
    *2004 年自然语言处理实证方法会议论文集*，第 404–411 页。计算语言学协会。'
- en: 'OpenAI (2022) OpenAI. 2022. Gpt-3.5-turbo. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
    Accessed: 2024-03-10 to 2024-06-11.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. Gpt-3.5-turbo. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。访问时间：2024-03-10
    到 2024-06-11。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team 等人 (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth 等人.
    2023. Gemini: 一系列高能力的多模态模型。 *arXiv 预印本 arXiv:2312.11805*。'
- en: Wang et al. (2020) Hei-Chia Wang, Wei-Ching Hsiao, and Sheng-Han Chang. 2020.
    Automatic paper writing based on a rnn and the textrank algorithm. *Applied Soft
    Computing*, 97:106767.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2020) Hei-Chia Wang, Wei-Ching Hsiao 和 Sheng-Han Chang. 2020. 基于 rnn
    和 textrank 算法的自动论文写作。 *应用软计算*，97:106767。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou 等人. 2022. 链式思维提示在大型语言模型中引发推理。 *神经信息处理系统进展*，35:24824–24837。
- en: Appendix A Appendix
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Datasets and Main Experiments
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据集和主要实验
- en: We used three datasets in our evaluations. Here, we provide a more detailed
    explanation of each dataset and task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估中使用了三个数据集。这里，我们提供了每个数据集和任务的更详细解释。
- en: A.1.1 Seniment Analysis
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 情感分析
- en: GameSpot.
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GameSpot。
- en: The GameSpot Reviews dataset contains over 12,000 lengthy video game reviews
    with author-assigned sentiment scores ranging from 1 to 100\. Almost all the reviews
    in this dataset are quite lengthy and by using a minimum threshold of 45 sentences,
    there are still thousands of reviews available in this dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: GameSpot Reviews 数据集包含超过 12,000 条冗长的视频游戏评论，作者分配的情感评分范围从 1 到 100。几乎所有评论都非常冗长，使用最低阈值为
    45 句，数据集中仍有数千条评论。
- en: In the sentiment analysis experiments, we asked each LLM to give a rating on
    the sentiment from 1 to 100 for each document. Most labels in the data were multiples
    of 10 (i.e., 10, 20, 30, …, 90, 100). However, sometimes the labels had other
    values like 95, or 85 as well. To this end and to cover even the corner cases,
    we asked the LLM to predict the label as an integer from 1 to 100\. The calculation
    of MSE and MAE metrics is straightforward and according to the standard definition.
    For calculating the accuracy, we considered a prediction as accurate if it was
    within 5 scoring points of the label. For example, if the label has a value of
    70, a predicted label between 65-75 range is considered an accurate prediction
    and any prediction outside this range is considered not accurate.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在情感分析实验中，我们要求每个LLM对每篇文档的情感从1到100进行评分。数据中的大多数标签是10的倍数（即10, 20, 30, …, 90, 100）。然而，有时标签也有其他值，如95或85。为此，并涵盖边缘情况，我们要求LLM预测标签为1到100的整数。MSE和MAE指标的计算方法简单明了，按照标准定义进行。为了计算准确性，我们将预测值在标签的5分数点以内视为准确预测。例如，如果标签值为70，则预测标签在65-75范围内被视为准确预测，任何超出该范围的预测都被视为不准确。
- en: Regarding the temperature used in this study as well as other studies, we tried
    different temperature values for the LLMs but no significant change or decrease
    was observed by doing this and summarisation/truncation methods always showed
    superior performance. The results reported in this experiment are the average
    over 5 runs and where applicable we have reported the 85% Confidence Interval
    as well. The temperature in the experiments reported here was set to 0.01.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本研究及其他研究中使用的温度，我们尝试了不同的温度值对LLMs进行测试，但未观察到显著变化或减少，并且总结/截断方法始终表现优越。本实验报告的结果是5次运行的平均值，并且在适用的情况下，我们也报告了85%的置信区间。本实验中报告的温度设置为0.01。
- en: A.2 News Categorization
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 新闻分类
- en: 20 NewsGroup.
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 20 NewsGroup.
- en: The 20 Newsgroups dataset features nearly 20,000 documents across 20 topic categories
    like politics, religion, sports and computers. We focused on a subset surpassing
    60 sentences, averaging 3450 tokens per document when tokenized by the NLTK word
    tokenizer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 20 Newsgroups 数据集包含近20,000篇文档，涵盖政治、宗教、体育和计算机等20个主题类别。我们将重点放在超过60句的子集上，这些文档在使用NLTK词标记器进行标记化时平均有3450个词元。
- en: BBC News Archive.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: BBC新闻档案。
- en: The BBC News Archive, consisting of 2,225 articles covering business, entertainment,
    politics, sports and tech. We focused our study on the subset surpassing 35 sentences,
    averaging 1150 tokens per document when tokenized by the NLTK word tokenizer.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: BBC新闻档案包含了2,225篇涵盖商业、娱乐、政治、体育和科技的文章。我们将研究重点放在超过35句的子集上，这些文档在使用NLTK词标记器进行标记化时平均有1150个词元。
- en: 'As we explained in the main body of the paper as well as the results shown
    in [fig. 4](#A1.F4 "In A.2.1 Diverse Summarization ‣ A.2 News Categorization ‣
    Appendix A Appendix ‣ Efficient Solutions For An Intriguing Failure of LLMs: Long
    Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"), when
    trying different temperatures, there were no meaningful changes in the results
    and order of the approaches in terms of their performance.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在论文主体部分以及[图4](#A1.F4 "在A.2.1多样化总结 ‣ A.2 新闻分类 ‣ 附录A 附录 ‣ 高效解决LLMs的一个有趣失败：长上下文窗口并不意味着LLMs可以完美分析长序列")中所示，当尝试不同的温度时，结果和方法的排序在性能方面没有显著变化。
- en: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | Full | 0.54 | 67.8 | 1.24 | 3450 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.54 | 67.8 | 1.24 | 3450 |'
- en: '|  | Full+Sum. | 0.58 | 71.2 | 1.78 | 3700 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.58 | 71.2 | 1.78 | 3700 |'
- en: '|  | First Sent. | 0.50 | 66.4 | 0.72 | 240 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.50 | 66.4 | 0.72 | 240 |'
- en: '|  | Last Sent. | 0.49 | 64.4 | 0.72 | 240 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.49 | 64.4 | 0.72 | 240 |'
- en: '|  | Sum. | 0.56 | 69.6 | 0.72 | 240 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.56 | 69.6 | 0.72 | 240 |'
- en: '|  | Div. Sum. | 0.52 | 65.5 | 0.72 | 240 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.52 | 65.5 | 0.72 | 240 |'
- en: '| Claude 3 Haiku | Rand. Samp. | 0.50 | 64.6 | 0.72 | 240 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | Rand. Samp. | 0.50 | 64.6 | 0.72 | 240 |'
- en: '|  | Full | 0.38 | 47.1 | 1.19 | 3450 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.38 | 47.1 | 1.19 | 3450 |'
- en: '|  | Full+Sum. | 0.41 | 53.2 | 1.95 | 3700 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.41 | 53.2 | 1.95 | 3700 |'
- en: '|  | First Sent. | 0.42 | 45.2 | 0.33 | 240 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.42 | 45.2 | 0.33 | 240 |'
- en: '|  | Last Sent. | 0.40 | 42.8 | 0.33 | 240 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.40 | 42.8 | 0.33 | 240 |'
- en: '|  | Sum. | 0.38 | 43.4 | 0.33 | 240 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.38 | 43.4 | 0.33 | 240 |'
- en: '|  | Div. Sum. | 0.39 | 44.3 | 0.33 | 240 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.39 | 44.3 | 0.33 | 240 |'
- en: '| GPT 3.5 Turbo | Rand. Samp. | 0.41 | 43.1 | 0.33 | 240 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 Turbo | Rand. Samp. | 0.41 | 43.1 | 0.33 | 240 |'
- en: '|  | Full | 0.30 | 34.6 | 3.48 | 3450 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.30 | 34.6 | 3.48 | 3450 |'
- en: '|  | Full+Sum. | 0.36 | 35.2 | 3.88 | 3700 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.36 | 35.2 | 3.88 | 3700 |'
- en: '|  | First Sent. | 0.36 | 43.2 | 1.12 | 240 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.36 | 43.2 | 1.12 | 240 |'
- en: '|  | Last Sent. | 0.45 | 46.4 | 1.12 | 240 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.45 | 46.4 | 1.12 | 240 |'
- en: '|  | Sum. | 0.39 | 40.4 | 1.12 | 240 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.39 | 40.4 | 1.12 | 240 |'
- en: '|  | Div. Sum. | 0.38 | 40.8 | 1.12 | 240 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.38 | 40.8 | 1.12 | 240 |'
- en: '| Gemini Pro | Rand. Samp. | 0.36 | 40.2 | 1.12 | 240 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro | Rand. Samp. | 0.36 | 40.2 | 1.12 | 240 |'
- en: '|  | Full | 0.01 | 1.1 | 0.96 | 3450 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.01 | 1.1 | 0.96 | 3450 |'
- en: '|  | Full+Sum. | 0.01 | 2.3 | 0.99 | 3650 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.01 | 2.3 | 0.99 | 3650 |'
- en: '|  | First Sent. | 0.03 | 11.5 | 0.87 | 180 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.03 | 11.5 | 0.87 | 180 |'
- en: '|  | Last Sent. | 0.04 | 12.2 | 0.87 | 180 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.04 | 12.2 | 0.87 | 180 |'
- en: '|  | Sum. | 0.05 | 12.3 | 0.87 | 180 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.05 | 12.3 | 0.87 | 180 |'
- en: '|  | Div. Sum. | 0.04 | 12.6 | 0.87 | 180 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.04 | 12.6 | 0.87 | 180 |'
- en: '| Mistral 7b | Rand. Samp. | 0.05 | 12.2 | 0.87 | 180 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7b | Rand. Samp. | 0.05 | 12.2 | 0.87 | 180 |'
- en: '|  | Full | 0.15 | 25.6 | 1.02 | 3450 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.15 | 25.6 | 1.02 | 3450 |'
- en: '|  | Full+Sum. | 0.16 | 29.3 | 1.11 | 3650 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.16 | 29.3 | 1.11 | 3650 |'
- en: '|  | First Sent. | 0.17 | 29.1 | 0.89 | 180 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.17 | 29.1 | 0.89 | 180 |'
- en: '|  | Last Sent. | 0.17 | 29.8 | 0.89 | 180 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.17 | 29.8 | 0.89 | 180 |'
- en: '|  | Sum. | 0.19 | 31.2 | 0.89 | 180 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.19 | 31.2 | 0.89 | 180 |'
- en: '|  | Div. Sum. | 0.21 | 34.1 | 0.89 | 180 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.21 | 34.1 | 0.89 | 180 |'
- en: '| Llama 3 8b | Rand. Samp. | 0.16 | 27.8 | 0.89 | 180 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8b | Rand. Samp. | 0.16 | 27.8 | 0.89 | 180 |'
- en: 'Table 4: The performance of different LLMs for Categorization task on 20 NewsGroup
    dataset. $N$ parameter for summarization and truncation is set to 7 for the Mistral
    and Llama models and 10 for the others.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同LLM在20 NewsGroup数据集上的分类任务表现。Mistral和Llama模型的摘要和截断参数$N$设置为7，其它模型设置为10。
- en: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Model | Scenario | Mac. F1 | Acc. | Avg. Lat. | Inp. Len. |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | Full | 0.63 | 63.8 | 0.69 | 1150 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.63 | 63.8 | 0.69 | 1150 |'
- en: '|  | Full+Sum. | 0.67 | 67.1 | 1.45 | 1400 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.67 | 67.1 | 1.45 | 1400 |'
- en: '|  | First Sent. | 0.69 | 70.4 | 0.65 | 230 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.69 | 70.4 | 0.65 | 230 |'
- en: '|  | Last Sent. | 0.56 | 56.9 | 0.65 | 230 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.56 | 56.9 | 0.65 | 230 |'
- en: '|  | Sum. | 0.61 | 61.5 | 0.65 | 230 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.61 | 61.5 | 0.65 | 230 |'
- en: '|  | Div. Sum. | 0.64 | 63.8 | 0.65 | 230 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.64 | 63.8 | 0.65 | 230 |'
- en: '| Claude 3 Haiku | Rand. Samp. | 0.60 | 61.2 | 0.65 | 230 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | Rand. Samp. | 0.60 | 61.2 | 0.65 | 230 |'
- en: '|  | Full | 0.75 | 83.8 | 0.54 | 1150 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.75 | 83.8 | 0.54 | 1150 |'
- en: '|  | Full+Sum. | 0.67 | 76.4 | 1.08 | 1400 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.67 | 76.4 | 1.08 | 1400 |'
- en: '|  | First Sent. | 0.80 | 86.3 | 0.49 | 230 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.80 | 86.3 | 0.49 | 230 |'
- en: '|  | Last Sent. | 0.69 | 78.4 | 0.49 | 230 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.69 | 78.4 | 0.49 | 230 |'
- en: '|  | Sum. | 0.72 | 79.9 | 0.49 | 230 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.72 | 79.9 | 0.49 | 230 |'
- en: '|  | Div. Sum. | 0.69 | 78.7 | 0.49 | 230 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.69 | 78.7 | 0.49 | 230 |'
- en: '| GPT 3.5 Turbo | Rand. Samp. | 0.68 | 77.7 | 0.49 | 230 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 Turbo | Rand. Samp. | 0.68 | 77.7 | 0.49 | 230 |'
- en: '|  | Full | 0.65 | 61.2 | 2.26 | 1150 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.65 | 61.2 | 2.26 | 1150 |'
- en: '|  | Full+Sum. | 0.69 | 66.6 | 2.61 | 1400 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.69 | 66.6 | 2.61 | 1400 |'
- en: '|  | First Sent. | 0.63 | 59.7 | 1.04 | 230 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.63 | 59.7 | 1.04 | 230 |'
- en: '|  | Last Sent. | 0.64 | 58.4 | 1.04 | 230 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.64 | 58.4 | 1.04 | 230 |'
- en: '|  | Sum. | 0.61 | 57.7 | 1.04 | 230 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.61 | 57.7 | 1.04 | 230 |'
- en: '|  | Div. Sum. | 0.61 | 56.8 | 1.04 | 230 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.61 | 56.8 | 1.04 | 230 |'
- en: '| Gemini Pro | Rand. Samp. | 0.61 | 56.2 | 1.04 | 230 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro | Rand. Samp. | 0.61 | 56.2 | 1.04 | 230 |'
- en: '|  | Full | 0.13 | 22.1 | 0.97 | 1150 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.13 | 22.1 | 0.97 | 1150 |'
- en: '|  | Full+Sum. | 0.03 | 11.7 | 1.67 | 1330 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.03 | 11.7 | 1.67 | 1330 |'
- en: '|  | First Sent. | 0.32 | 37.1 | 0.85 | 170 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.32 | 37.1 | 0.85 | 170 |'
- en: '|  | Last Sent. | 0.23 | 32.1 | 0.85 | 170 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.23 | 32.1 | 0.85 | 170 |'
- en: '|  | Sum. | 0.33 | 38.5 | 0.85 | 170 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.33 | 38.5 | 0.85 | 170 |'
- en: '|  | Div. Sum. | 0.35 | 39.9 | 0.85 | 170 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.35 | 39.9 | 0.85 | 170 |'
- en: '| Mistral 7b | Rand. Samp. | 0.28 | 33.5 | 0.85 | 170 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7b | Rand. Samp. | 0.28 | 33.5 | 0.85 | 170 |'
- en: '|  | Full | 0.38 | 39.2 | 0.89 | 1150 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | Full | 0.38 | 39.2 | 0.89 | 1150 |'
- en: '|  | Full+Sum. | 0.42 | 46.6 | 1.81 | 1330 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | Full+Sum. | 0.42 | 46.6 | 1.81 | 1330 |'
- en: '|  | First Sent. | 0.62 | 68.8 | 0.85 | 170 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | First Sent. | 0.62 | 68.8 | 0.85 | 170 |'
- en: '|  | Last Sent. | 0.53 | 63.5 | 0.85 | 170 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | Last Sent. | 0.53 | 63.5 | 0.85 | 170 |'
- en: '|  | Sum. | 0.54 | 61.2 | 0.85 | 170 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | Sum. | 0.54 | 61.2 | 0.85 | 170 |'
- en: '|  | Div. Sum. | 0.59 | 65.3 | 0.85 | 170 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | Div. Sum. | 0.59 | 65.3 | 0.85 | 170 |'
- en: '| Llama 3 8b | Rand. Samp. | 0.51 | 59.2 | 0.85 | 170 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8b | Rand. Samp. | 0.51 | 59.2 | 0.85 | 170 |'
- en: 'Table 5: The performance of different LLMs for Categorization task on BBC News
    dataset. $N$ parameter for summarization and truncation is set to 7 for the Mistral
    and Llama models and 10 for the others.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同LLMs在BBC新闻数据集上的分类任务性能。总结和截断的 $N$ 参数对Mistral和Llama模型设置为7，对其他模型设置为10。
- en: The results reported in this paper in the tables for the news categorization
    task are averaged over 5 runs and we have reported 85% Confidence Interval when
    applicable. The temperature of the models was set to 0.0 for the results reported
    in the tables.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中新闻分类任务的结果在表中列出，结果是5次运行的平均值，适用时我们报告了85%的置信区间。表中报告的结果模型的温度设置为0.0。
- en: A.2.1 Diverse Summarization
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 多样性总结
- en: 'Here we provide more explanation about the diverse summarization approach and
    how the green-coloured component (in [Fig. 1(b)](#S1.F1.sf2 "In Fig. 1 ‣ Related
    Work. ‣ 1 Introduction ‣ Efficient Solutions For An Intriguing Failure of LLMs:
    Long Context Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly"))
    which is the diversity selector in our algorithm works.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供关于多样性总结方法的更多解释，并说明绿色组件（见 [图1(b)](#S1.F1.sf2 "在图1 ‣ 相关工作 ‣ 1 引言 ‣ 针对LLMs长上下文窗口的有趣失败的有效解决方案：长上下文窗口并不意味着LLMs可以无缝分析长序列")）在我们算法中作为多样性选择器的工作原理。
- en: To write equations describing what the green component is doing, we can focus
    on the main functions in this component and their inputs and outputs. Let’s denote
    the input text as $T$.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编写描述绿色组件的方程，我们可以集中关注该组件的主要功能及其输入和输出。我们将输入文本表示为 $T$。
- en: '1.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Tokenize Sentences:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分词句子：
- en: '|  | $S=\mathrm{sent\_tokenize}(T)$ |  | (1) |'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $S=\mathrm{sent\_tokenize}(T)$ |  | (1) |'
- en: '2.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Calculate sentence embeddings:'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算句子嵌入：
- en: '|  | $E=\mathrm{TfidfVectorizer}(S)$ |  | (2) |'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $E=\mathrm{TfidfVectorizer}(S)$ |  | (2) |'
- en: where $E$ is the TF-IDF matrix representing the embeddings of the sentences.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $E$ 是表示句子嵌入的 TF-IDF 矩阵。
- en: '3.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Calculate diversity scores:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算多样性得分：
- en: '|  | $\displaystyle D$ |  | (3) |'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle D$ |  | (3) |'
- en: '|  | $\displaystyle D_{\mathrm{sum}}$ |  | (4) |'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{\mathrm{sum}}$ |  | (4) |'
- en: where $D$ is the sum of dissimilarity scores for each sentence.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $D$ 是每个句子的相异性得分之和。
- en: '4.'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Select top N diverse sentences:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择前 N 个多样性句子：
- en: '|  | $S_{\mathrm{top_{N}}}=\arg\max_{N}(D_{\mathrm{sum}})$ |  | (5) |'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $S_{\mathrm{top_{N}}}=\arg\max_{N}(D_{\mathrm{sum}})$ |  | (5) |'
- en: where $S_{\mathrm{top_{N}}}$ sentences with the highest diversity scores.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $S_{\mathrm{top_{N}}}$ 是多样性得分最高的句子。
- en: '5.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Generate the final summary by joining the sentences:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过连接句子生成最终总结：
- en: '|  | $\mathrm{summary}=\mathrm{join}(S_{\mathrm{top_{N}}})$ |  | (6) |'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathrm{summary}=\mathrm{join}(S_{\mathrm{top_{N}}})$ |  | (6) |'
- en: where the final summary is the concatenation of the selected top $N$ diverse
    sentences.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终总结是选择的前 $N$ 个多样性句子的拼接。
- en: 'The main steps denoted in the above equations can be summarized as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程中表示的主要步骤可以总结如下：
- en: '1.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Tokenize the input text $T$.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对输入文本 $T$ 进行分词。
- en: '2.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Compute the TF-IDF embedding matrix $E$ for the sentences.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算句子的 TF-IDF 嵌入矩阵 $E$。
- en: '3.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Calculate the dissimilarity matrix $D$ contains the pairwise dissimilarity
    scores between all sentences. In [Eqn. 4](#A1.E4 "In Item 3\. ‣ A.2.1 Diverse
    Summarization ‣ A.2 News Categorization ‣ Appendix A Appendix ‣ Efficient Solutions
    For An Intriguing Failure of LLMs: Long Context Window Does Not Mean LLMs Can
    Analyze Long Sequences Flawlessly"), $D\_sum$ is a vector where each element represents
    the total dissimilarity score for a particular sentence compared to all other
    sentences.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算包含所有句子对间相异性得分的不相似度矩阵 $D$。在 [方程4](#A1.E4 "在第3项 ‣ A.2.1 多样性总结 ‣ A.2 新闻分类 ‣ 附录A
    附录 ‣ 针对LLMs长上下文窗口的有趣失败的有效解决方案：长上下文窗口并不意味着LLMs可以无缝分析长序列") 中，$D\_sum$ 是一个向量，其中每个元素表示特定句子与所有其他句子的总相异性得分。
- en: '4.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Select the top $N$ with the highest diversity scores.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择多样性得分最高的前 $N$ 个句子。
- en: '5.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Concatenate the selected sentences to form the final summary.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将选择的句子拼接成最终总结。
- en: '| Model | Scenario | MSE | MAE | Accuracy | Avg. Lat. | Inp. Len. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 场景 | MSE | MAE | 准确率 | 平均延迟 | 输入长度 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | Full | 112.3 (6) | 8.50 (6) | 43.8 (7) | 1.04 | 2120 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | 112.3 (6) | 8.50 (6) | 43.8 (7) | 1.04 | 2120 |'
- en: '|  | Full+Sum. | 80.2 (2) | 7.17 (2) | 50.6 (5) | 1.05 | 2450 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整+总结 | 80.2 (2) | 7.17 (2) | 50.6 (5) | 1.05 | 2450 |'
- en: '|  | First Sent. | 131.8 (7) | 8.87 (7) | 44.4 (6) | 0.77 | 320 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一句 | 131.8 (7) | 8.87 (7) | 44.4 (6) | 0.77 | 320 |'
- en: '|  | Last Sent. | 77.0 (1) | 6.58 (1) | 60.2 (2) | 0.77 | 320 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后句 | 77.0 (1) | 6.58 (1) | 60.2 (2) | 0.77 | 320 |'
- en: '|  | Sum. | 97.6 (3) | 7.55 (4) | 56.8 (4) | 0.77 | 320 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 97.6 (3) | 7.55 (4) | 56.8 (4) | 0.77 | 320 |'
- en: '|  | Div. Sum. | 106.0 (5) | 7.21 (3) | 65.2 (1) | 0.77 | 320 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | 分段总结 | 106.0 (5) | 7.21 (3) | 65.2 (1) | 0.77 | 320 |'
- en: '| Claude 3 Haiku | Rand. Samp. | 104.2 (4) | 7.56 (5) | 59.0 (3) | 0.77 | 320
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | 随机抽样 | 104.2 (4) | 7.56 (5) | 59.0 (3) | 0.77 | 320 |'
- en: '|  | Full | 134.9 (4) | 9.76 (6) | 40.4 (7) | 0.59 | 2120 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | 134.9 (4) | 9.76 (6) | 40.4 (7) | 0.59 | 2120 |'
- en: '|  | Full+Sum. | 110.8 (1) | 8.75 (1) | 46.6 (5) | 0.61 | 2450 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整+总结 | 110.8 (1) | 8.75 (1) | 46.6 (5) | 0.61 | 2450 |'
- en: '|  | First Sent. | 177.3 (7) | 10.81 (7) | 41.0 (6) | 0.47 | 320 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一个句子 | 177.3 (7) | 10.81 (7) | 41.0 (6) | 0.47 | 320 |'
- en: '|  | Last Sent. | 119.1 (3) | 8.78 (2) | 55.6 (2) | 0.47 | 320 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后句子 | 119.1 (3) | 8.78 (2) | 55.6 (2) | 0.47 | 320 |'
- en: '|  | Sum. | 117.8 (2) | 8.94 (3) | 53.0 (3) | 0.47 | 320 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 117.8 (2) | 8.94 (3) | 53.0 (3) | 0.47 | 320 |'
- en: '|  | Div. Sum. | 141.7 (6) | 9.11 (4) | 59.2 (1) | 0.47 | 320 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | 分段总结 | 141.7 (6) | 9.11 (4) | 59.2 (1) | 0.47 | 320 |'
- en: '| GPT 3.5 Turbo | Rand. Samp. | 135.3 (5) | 9.27 (5) | 51.8 (4) | 0.47 | 320
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| GPT 3.5 Turbo | 随机抽样 | 135.3 (5) | 9.27 (5) | 51.8 (4) | 0.47 | 320 |'
- en: '|  | Full | 130.8 (6) | 9.89 (7) | 43.8 (7) | 2.74 | 2120 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | 130.8 (6) | 9.89 (7) | 43.8 (7) | 2.74 | 2120 |'
- en: '|  | Full+Sum. | 117.6 (6) | 9.14 (6) | 53.4 (6) | 2.96 | 2450 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整+总结 | 117.6 (6) | 9.14 (6) | 53.4 (6) | 2.96 | 2450 |'
- en: '|  | First Sent. | 88.0 (4) | 7.15 (4) | 61.6 (5) | 1.15 | 320 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一个句子 | 88.0 (4) | 7.15 (4) | 61.6 (5) | 1.15 | 320 |'
- en: '|  | Last Sent. | 83.3 (3) | 7.14 (3) | 63.8 (4) | 1.08 | 320 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后句子 | 83.3 (3) | 7.14 (3) | 63.8 (4) | 1.08 | 320 |'
- en: '|  | Sum. | 73.3 (1) | 6.80 (1) | 67.4 (2) | 1.08 | 320 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 73.3 (1) | 6.80 (1) | 67.4 (2) | 1.08 | 320 |'
- en: '|  | Div. Sum. | 94.2 (5) | 7.23 (5) | 69.6 (1) | 1.08 | 320 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | 分段总结 | 94.2 (5) | 7.23 (5) | 69.6 (1) | 1.08 | 320 |'
- en: '| Gemini Pro | Rand. Samp. | 78.8 (2) | 7.05 (2) | 66.2 (3) | 1.08 | 320 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro | 随机抽样 | 78.8 (2) | 7.05 (2) | 66.2 (3) | 1.08 | 320 |'
- en: '|  | Full | 811.6 (6) | 19.54 (6) | 13.1 (6) | 1.01 | 2120 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | 811.6 (6) | 19.54 (6) | 13.1 (6) | 1.01 | 2120 |'
- en: '|  | Full+Sum. | 1532.1 (7) | 30.68 (7) | 8.9 (7) | 1.04 | 2450 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整+总结 | 1532.1 (7) | 30.68 (7) | 8.9 (7) | 1.04 | 2450 |'
- en: '|  | First Sent. | 236.4 (4) | 10.81 (5) | 29.0 (4) | 0.91 | 320 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一个句子 | 236.4 (4) | 10.81 (5) | 29.0 (4) | 0.91 | 320 |'
- en: '|  | Last Sent. | 93.5 (1) | 7.96 (1) | 38.0 (1) | 0.91 | 320 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后句子 | 93.5 (1) | 7.96 (1) | 38.0 (1) | 0.91 | 320 |'
- en: '|  | Sum. | 172.5 (4) | 10.86 (4) | 23.1 (5) | 0.91 | 320 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 172.5 (4) | 10.86 (4) | 23.1 (5) | 0.91 | 320 |'
- en: '|  | Div. Sum. | 155.1 (2) | 9.56 (2) | 35.8 (2) | 0.91 | 320 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | 分段总结 | 155.1 (2) | 9.56 (2) | 35.8 (2) | 0.91 | 320 |'
- en: '| Mistral 7b | Rand. Samp. | 164.1 (3) | 9.57 (3) | 34.6 (3) | 0.91 | 320 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Mistral 7b | 随机抽样 | 164.1 (3) | 9.57 (3) | 34.6 (3) | 0.91 | 320 |'
- en: '|  | Full | 174.7 (5) | 10.62 (5) | 39.1 (3) | 0.95 | 2120 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整 | 174.7 (5) | 10.62 (5) | 39.1 (3) | 0.95 | 2120 |'
- en: '|  | Full+Sum. | 192.4 (6) | 11.79 (7) | 31.3 (6) | 0.99 | 2450 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整+总结 | 192.4 (6) | 11.79 (7) | 31.3 (6) | 0.99 | 2450 |'
- en: '|  | First Sent. | 212.1 (7) | 11.76 (6) | 30.1 (7) | 0.90 | 320 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一个句子 | 212.1 (7) | 11.76 (6) | 30.1 (7) | 0.90 | 320 |'
- en: '|  | Last Sent. | 125.4 (1) | 9.33 (1) | 36.1 (5) | 0.90 | 320 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后句子 | 125.4 (1) | 9.33 (1) | 36.1 (5) | 0.90 | 320 |'
- en: '|  | Sum. | 159.7 (2) | 10.06 (2) | 40.5 (1) | 0.90 | 320 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | 总结 | 159.7 (2) | 10.06 (2) | 40.5 (1) | 0.90 | 320 |'
- en: '|  | Div. Sum. | 171.6 (4) | 10.15 (4) | 40.0 (2) | 0.90 | 320 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | 分段总结 | 171.6 (4) | 10.15 (4) | 40.0 (2) | 0.90 | 320 |'
- en: '| Llama 3 8b | Rand. Samp. | 165.5 (3) | 10.10 (3) | 38.2 (4) | 0.90 | 320
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8b | 随机抽样 | 165.5 (3) | 10.10 (3) | 38.2 (4) | 0.90 | 320 |'
- en: 'Table 6: The performance of different LLMs for Sentiment Analysis task on GameSpot
    dataset. $N$ parameter for summarization and truncation is set to 7 for all the
    models.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：不同 LLM 在 GameSpot 数据集上的情感分析任务表现。所有模型的总结和截断参数 $N$ 设置为 7。
- en: '![Refer to caption](img/4bab81501a23069b39da99365313f559.png)![Refer to caption](img/a3e6d10ee5524636efb1b2cb31c57567.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4bab81501a23069b39da99365313f559.png)![参见说明](img/a3e6d10ee5524636efb1b2cb31c57567.png)'
- en: (a) Mistral 7b Instruct F1-Temperature Curve
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mistral 7b 指令 F1-温度曲线
- en: '![Refer to caption](img/d05cd4cea8e5cb8c72425836b2a94514.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d05cd4cea8e5cb8c72425836b2a94514.png)'
- en: (b) Mistral 7b Instruct Accuracy-Temperature Curve
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral 7b 指令准确率-温度曲线
- en: '![Refer to caption](img/27524b9ba25ada7a0b11aba4136530bf.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/27524b9ba25ada7a0b11aba4136530bf.png)'
- en: (c) Llama3 8b Instruct F1-Temperature Curve
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Llama3 8b 指令 F1-温度曲线
- en: '![Refer to caption](img/cb369df10a818584f5b0eb4c008eeeb9.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb369df10a818584f5b0eb4c008eeeb9.png)'
- en: (d) Llama3 8b Instruct Accuracy-Temperature Curve
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Llama3 8b 指令准确率-温度曲线
- en: 'Figure 4: Ablation Study on temperature in the news categorization task on
    20 newsgroup dataset. The results show the performance of the models does not
    experience much difference as we change the temperature from 0 to 0.1 aside from
    a slight increase in the confidence interval. These results are over 5 runs. The
    width of the 85% Confidence Interval for the ‘Random Sampling’ scenario is much
    bigger due to the randomness introduced by selecting the sentences.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在 20 个新闻组数据集上的新闻分类任务中，温度的消融研究。结果表明，模型的性能在将温度从 0 更改为 0.1 时没有发生很大变化，除了置信区间的略微增加。这些结果基于
    5 次运行。“随机抽样”场景的 85% 置信区间宽度由于选择句子引入的随机性而显著增大。
- en: '![Refer to caption](img/73e096ec16266303377e892485f78364.png)![Refer to caption](img/be8b9f9ef6c347249796e8f7012693f6.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/73e096ec16266303377e892485f78364.png)![参考标题](img/be8b9f9ef6c347249796e8f7012693f6.png)'
- en: (a) Claude3 Haiku Accuracy Curve
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Claude3 Haiku 准确度曲线
- en: '![Refer to caption](img/975ce12b8089f953650fc63ed290adaa.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/975ce12b8089f953650fc63ed290adaa.png)'
- en: (b) Claude3 Haiku MAE Curve
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Claude3 Haiku MAE 曲线
- en: '![Refer to caption](img/37533dbaad122255e893586e15ad8e48.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/37533dbaad122255e893586e15ad8e48.png)'
- en: (c) Claude3 Haiku MSE Curve
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Claude3 Haiku MSE 曲线
- en: '![Refer to caption](img/b5a2854b90da8bb003eb4da1c72e7853.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b5a2854b90da8bb003eb4da1c72e7853.png)'
- en: (d) Gemini Pro Accuracy Curve
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Gemini Pro 准确度曲线
- en: '![Refer to caption](img/81d3a4c6a41caead96552e7631c53138.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/81d3a4c6a41caead96552e7631c53138.png)'
- en: (e) Gemini Pro MAE Curve
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Gemini Pro MAE 曲线
- en: '![Refer to caption](img/f97986a29c64171fc461e5d1071544cd.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f97986a29c64171fc461e5d1071544cd.png)'
- en: (f) Gemini Pro MSE Curve
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Gemini Pro MSE 曲线
- en: '![Refer to caption](img/73d8d3c42a542c095f273227a0e0dd19.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/73d8d3c42a542c095f273227a0e0dd19.png)'
- en: (g) GPT3.5 Turbo Accuracy Curve
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (g) GPT3.5 Turbo 准确度曲线
- en: '![Refer to caption](img/0896fb83eced9ef259870b44e4b330d0.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0896fb83eced9ef259870b44e4b330d0.png)'
- en: (h) GPT3.5 Turbo MAE Curve
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: (h) GPT3.5 Turbo MAE 曲线
- en: '![Refer to caption](img/aa39e8525886a7a51bc8426a5092d0ce.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/aa39e8525886a7a51bc8426a5092d0ce.png)'
- en: (i) GPT3.5 Turbo MSE Curve
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (i) GPT3.5 Turbo MSE 曲线
- en: '![Refer to caption](img/a8c04677026581e514bc8ac2e1f06d36.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a8c04677026581e514bc8ac2e1f06d36.png)'
- en: (j) Mistral 7b Instruct Accuracy Curve
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: (j) Mistral 7b Instruct 准确度曲线
- en: '![Refer to caption](img/35364c6b794c46854768797d069fafa8.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/35364c6b794c46854768797d069fafa8.png)'
- en: (k) Mistral 7b Instruct MAE Curve
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Mistral 7b Instruct MAE 曲线
- en: '![Refer to caption](img/0353617dfbbd437036538e8e6b0c3974.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0353617dfbbd437036538e8e6b0c3974.png)'
- en: (l) Mistral 7b Instruct MSE Curve
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: (l) Mistral 7b Instruct MSE 曲线
- en: '![Refer to caption](img/f40e7b9f6e5d8b9059eac5b56cb87e35.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f40e7b9f6e5d8b9059eac5b56cb87e35.png)'
- en: (m) Llama3 8b Instruct Accuracy Curve
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: (m) Llama3 8b Instruct 准确度曲线
- en: '![Refer to caption](img/429328ad9148b4c4fe4b41332d64ea04.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/429328ad9148b4c4fe4b41332d64ea04.png)'
- en: (n) Llama3 8b Instruct MAE Curve
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: (n) Llama3 8b Instruct MAE 曲线
- en: '![Refer to caption](img/0037b1ee225ac76991849c4b4ab2d76e.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0037b1ee225ac76991849c4b4ab2d76e.png)'
- en: (o) Llama3 8b Instruct MSE Curve
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: (o) Llama3 8b Instruct MSE 曲线
- en: 'Figure 5: LLMs performance over 5 runs on sentiment analysis on GameSpot dataset.
    The “Full Text” scenario is a horizontal line since it always contains the full
    text, not summary/truncated text and we have included it as a horizontal line
    here as baseline'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLMs 在 GameSpot 数据集上的情感分析的 5 次运行表现。“全文”场景是水平线，因为它始终包含完整的文本，而不是摘要/截断的文本，我们将其作为基线在此用水平线表示。
