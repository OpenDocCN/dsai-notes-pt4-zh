- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:56:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:56:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SampleAttention：基于自适应结构稀疏注意力的长上下文 LLM 推理的近无损加速
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15486](https://ar5iv.labs.arxiv.org/html/2406.15486)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15486](https://ar5iv.labs.arxiv.org/html/2406.15486)
- en: Qianchao Zhu^†, Jiangfei Duan^‡, Chang Chen^†, Siran Liu^†, Xiuhong Li^†, Guanyu
    Feng^§
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qianchao Zhu^†, Jiangfei Duan^‡, Chang Chen^†, Siran Liu^†, Xiuhong Li^†, Guanyu
    Feng^§
- en: Xin Lv^§, Huanqi Cao^∪, Chuanfu Xiao^†, Xingcheng Zhang^∩, Dahua Lin^(‡∩), Chao
    Yang^†
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xin Lv^§, Huanqi Cao^∪, Chuanfu Xiao^†, Xingcheng Zhang^∩, Dahua Lin^(‡∩), Chao
    Yang^†
- en: ^†Peking University  ^‡The Chinese University of Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^†北京大学  ^‡香港中文大学
- en: ^§Zhipu.AI  ^∪Tsinghua University  ^∩Shanghai AI Lab
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^§Zhipu.AI  ^∪清华大学  ^∩上海 AI 实验室
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) now support extremely long context windows, but
    the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token
    (TTFT) latency. Existing approaches to address this complexity require additional
    pretraining or finetuning, and often sacrifice model accuracy. In this paper,
    we first provide both theoretical and empirical foundations for near-lossless
    sparse attention. We find dynamically capturing head-specific sparse patterns
    at runtime with low overhead is crucial. To address this, we propose SampleAttention,
    an adaptive structured and near-lossless sparse attention. Leveraging observed
    significant sparse patterns, SampleAttention attends to a fixed percentage of
    adjacent tokens to capture local window patterns, and employs a two-stage query-guided
    key-value filtering approach, which adaptively select a minimum set of key-values
    with low overhead, to capture column stripe patterns. Comprehensive evaluations
    show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
    LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared
    with FlashAttention.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型 (LLMs) 现在支持极长的上下文窗口，但普通注意力的二次复杂度导致了显著的首个标记时间 (TTFT) 延迟。现有方法解决这种复杂度需要额外的预训练或微调，且往往牺牲模型准确性。本文首先提供了近无损稀疏注意力的理论和实证基础。我们发现，在运行时以低开销动态捕获特定头部的稀疏模式是至关重要的。为此，我们提出了
    SampleAttention，一种自适应结构和近无损的稀疏注意力。SampleAttention 利用观察到的显著稀疏模式，关注固定百分比的相邻标记以捕获局部窗口模式，并采用两阶段的查询引导的键值过滤方法，该方法自适应地选择一组最小的键值且开销低，以捕获列条纹模式。综合评估表明，SampleAttention
    可以无缝替代普通注意力，几乎没有准确性损失，并且相比于 FlashAttention 将 TTFT 降低了最多 $2.42\times$。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Recent advances [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)] race to scale the context window of large language models (LLMs) [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)] for more complex applications, including document
    analysis [[9](#bib.bib9)], code copilot [[10](#bib.bib10), [11](#bib.bib11)],
    and prolonged conversations [[12](#bib.bib12), [13](#bib.bib13)]. Popular LLMs
    like Gemini [[14](#bib.bib14)], Claude [[15](#bib.bib15)] and Kimi [[16](#bib.bib16)]
    now support context lengths exceeding 1 million tokens. However, the increase
    in context length makes it challenging to support live interactions due to the
    quadratic complexity of attention mechanism. As illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention"), the attention
    computation time increases quadratically with sequence length, quickly dominating
    the Time to First Token (TTFT) latency (i.e. prefill latency). For example, in
    a 1 million token context, the attention of ChatGLM-6B [[17](#bib.bib17)] takes
    $1555$ seconds, constituting over 90% of the TTFT when evaluated on an A100 GPU.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的进展[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]
    竞相扩大大语言模型 (LLMs) 的上下文窗口[[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)] 以适应更复杂的应用，包括文档分析[[9](#bib.bib9)]、代码助手[[10](#bib.bib10),
    [11](#bib.bib11)] 和长期对话[[12](#bib.bib12), [13](#bib.bib13)]。流行的 LLM，如 Gemini[[14](#bib.bib14)]、Claude[[15](#bib.bib15)]
    和 Kimi[[16](#bib.bib16)] 现在支持超过 100 万个标记的上下文长度。然而，上下文长度的增加使得支持实时交互变得具有挑战性，因为注意力机制的复杂度是二次的。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") 所示，注意力计算时间与序列长度呈二次增长，迅速主导了首个标记的时间（TTFT）延迟（即预填充延迟）。例如，在
    100 万个标记的上下文中，ChatGLM-6B[[17](#bib.bib17)] 的注意力计算需要 $1555$ 秒，占 TTFT 的 90% 以上，当在
    A100 GPU 上评估时。'
- en: Various solutions have been proposed to address the quadratic complexity of
    attention, but none of them can be seamlessly and practically applied to pretrained
    LLMs without finetuning or pretraining and sacrificing model accuracy. Prior approaches
    explore to approximate dense attention with static or dynamic sparse attention [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)], low-rank matrices [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)], and unified sparse and low-rank attention [[30](#bib.bib30),
    [31](#bib.bib31)]. Recurrent states [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]
    and external memory [[35](#bib.bib35), [36](#bib.bib36)] are also investigated
    to mitigate the complexity. However, these approaches require pretraining from
    scratch or additional finetuning, and cannot achieve the same accuracy of full
    attention. StreamingLLM [[37](#bib.bib37)] offers a tuning-free sparse attention
    for infinite generation scenarios, but it cannot effectively reduce TTFT without
    accuracy loss. Therefore, we ask the question,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 各种解决方案已被提出以应对注意力的二次复杂度，但没有一种可以在不进行微调或预训练的情况下，直接无缝地应用于预训练的LLM，并保持模型的准确性。以前的方法探索了用静态或动态稀疏注意力[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]、低秩矩阵[[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]以及统一稀疏和低秩注意力[[30](#bib.bib30), [31](#bib.bib31)]来近似密集注意力。递归状态[[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)]和外部记忆[[35](#bib.bib35), [36](#bib.bib36)]也被研究以减轻复杂性。然而，这些方法需要从头开始预训练或额外微调，且无法达到完整注意力的相同准确度。StreamingLLM[[37](#bib.bib37)]提供了一种无需调优的稀疏注意力，用于无限生成场景，但它不能有效地减少TTFT而不损失准确性。因此，我们提出了以下问题，
- en: '![Refer to caption](img/9b92eaacd2ebefcc5134639c7bc7a79a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b92eaacd2ebefcc5134639c7bc7a79a.png)'
- en: 'Figure 1: Comparison of sparse attention pattern and TTFT latency speedup.
    SampleAttention features adaptive structured sparse, compared with previous static
    and dynamic sparse attention. It achieves significant reduction in TTFT compared
    with FlashAttention.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：稀疏注意力模式与TTFT延迟加速的比较。SampleAttention具有自适应结构化稀疏，相比于之前的静态和动态稀疏注意力，其TTFT有显著减少。
- en: How can we reduce the TTFT for off-the-shelf long context LLMs with near-lossless¹¹1Near-lossless
    refers to that model accuracy stays above $99\%$ of the baseline according to
    MLPerf[[38](#bib.bib38)]. model accuracy?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在几乎无损¹¹1近乎无损指模型准确度保持在$99\%$以上，参考MLPerf[[38](#bib.bib38)]。的情况下减少现成长上下文LLM的TTFT模型准确度？
- en: 'In this paper, we first provide both theoretical and empirical foundations
    for near-lossless sparse attention. We find that the sparsity of intermediate
    score matrix in long-context attention is inherently-high, head-specific, and
    content-aware. Specifically, for a given long context prompt, some attention heads
    focus on only $0.2\%$ of the tokens, while others may need to attend to over half.
    From the dynamic sparse patterns, we also demonstrate some inherent local window
    and column stripe patterns as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"). Except for adjacent tokens in the local
    window, some dynamic column stripes appear to be critical for near-lossless attention.
    This flexible sparsity indicates that sparse attention should dynamically capture
    the head-specific sparse patterns at runtime to be near-lossless. However, adaptive
    selection of essential elements involves significant overhead. The trade-off between
    efficiency and accuracy is a permanent topic in sparse attention design.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文首先提供了近乎无损稀疏注意力的理论和实证基础。我们发现，长上下文注意力中的中间得分矩阵的稀疏性本质上很高、特定于头部且内容感知。具体而言，对于给定的长上下文提示，某些注意力头仅关注$0.2\%$的令牌，而其他头可能需要关注一半以上。从动态稀疏模式中，我们还展示了一些固有的局部窗口和列条纹模式，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ SampleAttention：通过自适应结构化稀疏注意力实现长上下文LLM推理的近乎无损加速")所示。除了局部窗口中的相邻令牌外，一些动态列条纹似乎对近乎无损的注意力至关重要。这种灵活的稀疏性表明，稀疏注意力应在运行时动态捕获特定于头部的稀疏模式，以实现近乎无损。然而，关键元素的自适应选择涉及显著的开销。效率与准确性的权衡是稀疏注意力设计中的一个永恒话题。
- en: To address these challenges, we propose SampleAttention, an adaptive structured
    sparse attention that can be seamlessly integrated into off-the-shelf long context
    LLMs with near-lossless model accuracy. SampleAttention leverages the significant
    window and stripe sparse patterns, thus achieves structured sparse and is hardware-efficient.
    To resolve the adaptive sparsity, SampleAttention attends to a fixed percentage
    of adjacent tokens to capture local window patterns, and employs a two-stage query-guided
    key-value filtering approach, which adaptively select a minimum set of key-values
    with low overhead, to focus on column stripe patterns. SampleAttention significantly
    accelerates vanilla attention by reducing both I/O and computation requirements.
    We also implement hardware-efficient kernels. Notably, SampleAttention aims to
    reduce the computation overhead of attention, and is orthogonal and can be combined
    with existing KV cache eviction approaches [[39](#bib.bib39), [40](#bib.bib40),
    [41](#bib.bib41)] to further reduce memory consumption.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们提出了 SampleAttention，一种自适应结构化稀疏注意力，可以无缝集成到现成的长上下文 LLMs 中，几乎不损失模型准确性。SampleAttention
    利用显著的窗口和条纹稀疏模式，从而实现结构化稀疏，并且硬件效率高。为了解决自适应稀疏性问题，SampleAttention 关注固定比例的相邻令牌，以捕捉局部窗口模式，并采用两阶段查询引导的键值过滤方法，该方法自适应地选择一个低开销的最小键值集，以专注于列条纹模式。SampleAttention
    通过减少 I/O 和计算需求显著加速了普通注意力。我们还实现了硬件高效的内核。值得注意的是，SampleAttention 旨在减少注意力的计算开销，并且是正交的，可以与现有的
    KV 缓存驱逐方法 [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)] 结合，以进一步减少内存消耗。
- en: We evaluate SampleAttention on ChatGLM2 and InternLM2 with a suite of popular
    benchmarks covering various generative tasks across different sequence lengths.
    Experimental results show that SampleAttention achieves nearly no accuracy loss
    for different LLMs, significantly outperforming prior works, and reduces the TTFT
    by up to $2.42\times$ compared with FlashAttention.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 ChatGLM2 和 InternLM2 上评估了 SampleAttention，使用了一系列流行的基准测试，涵盖了不同序列长度的各种生成任务。实验结果表明，SampleAttention
    在不同的 LLMs 上几乎没有准确度损失，显著优于先前的工作，并且与 FlashAttention 相比，TTFT 减少了多达 $2.42\times$。
- en: 2 Related Work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Approximate Attention. Plenty of works have been proposed to approximate quadratic
    attention with lower complexity[[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [42](#bib.bib42), [40](#bib.bib40), [25](#bib.bib25)]. For example,
    BigBird [[20](#bib.bib20)] combines window-, global- and random-attention to capture
    long range dependency. Reformer [[21](#bib.bib21)] reduces computional cost via
    locality-sensitive hashing. LongNet [[22](#bib.bib22)] replaces full attention
    with dilated attention. Linformer [[27](#bib.bib27)] employs low-rank matrix to
    approximate attention. HyperAttention [[26](#bib.bib26)] utilizes locality sensitive
    hashing to identify important entries on attention map. However, these approaches
    uses either static or coarse-grained sparse pattern, and often overlook the head-specific
    sparsity pattern. They cannot be losslessly applied in pretrained LLMs without
    additional finetuning or training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近似注意力。许多研究提出了用较低复杂度来近似二次注意力[[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [42](#bib.bib42), [40](#bib.bib40), [25](#bib.bib25)]。例如，BigBird
    [[20](#bib.bib20)] 结合了窗口、全局和随机注意力来捕捉长范围依赖。Reformer [[21](#bib.bib21)] 通过局部敏感哈希减少计算成本。LongNet
    [[22](#bib.bib22)] 用膨胀注意力替代了完全注意力。Linformer [[27](#bib.bib27)] 采用低秩矩阵来近似注意力。HyperAttention
    [[26](#bib.bib26)] 利用局部敏感哈希来识别注意力图中的重要条目。然而，这些方法使用的是静态或粗略的稀疏模式，且通常忽略了特定头部的稀疏模式。它们不能在没有额外微调或训练的情况下无损地应用于预训练的
    LLMs。
- en: KV Cache Compression. Long sequence comes with substantial KV cache memory consumption.
    StreamingLLM [[37](#bib.bib37)] keeps attention sinks and several recent tokens
    for infinite length generation. H2O [[39](#bib.bib39)] dynamically retains a balance
    of recent and heavy hitter tokens according to attention score during decoding.
    FastGen [[43](#bib.bib43)] adaptively construct KV cache according to observed
    head-specific policies. Recent efforts also quantize KV cache to lower precision
    to reduce memory consumption [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)].
    These works target on reducing the memory consumption of KV cache, while SampleAttention
    focuses on mitigating the long context computation overhead. SampleAttention can
    be combined with these approaches to further reduce memory consumption of KV cache.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存压缩。长序列带来了巨大的KV缓存内存消耗。StreamingLLM [[37](#bib.bib37)] 保留了注意力目标和几个最近的令牌以进行无限长度生成。H2O
    [[39](#bib.bib39)] 在解码过程中根据注意力分数动态保留最近和重要令牌的平衡。FastGen [[43](#bib.bib43)] 根据观察到的特定头策略自适应地构建KV缓存。近期的工作还将KV缓存量化为较低的精度，以减少内存消耗
    [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)]。这些工作旨在减少KV缓存的内存消耗，而SampleAttention则专注于减轻长上下文计算开销。SampleAttention可以与这些方法结合使用，以进一步减少KV缓存的内存消耗。
- en: 3 Foundation of Near-Lossless Sparse Attention
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 近乎无损稀疏注意力的基础
- en: We start with a regular full attention mechanism for one attention head, while
    the following contents can be seamlessly applied to multiple attention heads.
    Let $\textbf{Q}\in\mathbb{R}^{S_{q}\times d}$ can be formulated as,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从单个注意力头的常规全注意力机制开始，接下来的内容可以无缝应用于多个注意力头。设$\textbf{Q}\in\mathbb{R}^{S_{q}\times
    d}$可以表示为，
- en: '|  | $\textbf{P}=\texttt{softmax}(\frac{\textbf{QK}^{T}}{\sqrt{d}})\in[0,1]^{S_{q}\times
    S_{k}},\quad\textbf{O}=\textbf{PV}\in\mathbb{R}^{S_{q}\times d},$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{P}=\texttt{softmax}(\frac{\textbf{QK}^{T}}{\sqrt{d}})\in[0,1]^{S_{q}\times
    S_{k}},\quad\textbf{O}=\textbf{PV}\in\mathbb{R}^{S_{q}\times d},$ |  | (1) |'
- en: where softmax is applied in row-wise, and P is the attention score. We find,
    in long context LLMs, the attention score matrix P becomes extremely large, leading
    to inefficiencies. Moreover, applying softmax over long sequences tends to reduce
    the influence of smaller elements, making them less significant. This insight
    motivates us to investigate the inherent sparsity in the attention scores, which
    can potentially accelerate the attention mechanism without compromising accuracy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中softmax按行应用，P是注意力分数。我们发现，在长上下文LLM中，注意力分数矩阵P变得非常大，导致效率低下。此外，对长序列应用softmax倾向于减少较小元素的影响，使其不那么显著。这一见解促使我们研究注意力分数中的固有稀疏性，这可能在不影响准确性的情况下加速注意力机制。
- en: 3.1 Theoretical Foundation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 理论基础
- en: We first present a theoretical foundation to explore the attention score sparsity.
    Suppose we apply an attention mask $\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$
    can be formulated as,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提出一个理论基础来探索注意力分数的稀疏性。假设我们应用一个注意力掩码$\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$可以表示为，
- en: '|  | $\tilde{\textbf{P}}=\textbf{M}*\textbf{P}\in[0,1]^{S_{q}\times S_{k}},\quad\tilde{\textbf{O}}=\tilde{\textbf{P}}\textbf{V}\in\mathbb{R}^{S_{q}\times
    d},$ |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\textbf{P}}=\textbf{M}*\textbf{P}\in[0,1]^{S_{q}\times S_{k}},\quad\tilde{\textbf{O}}=\tilde{\textbf{P}}\textbf{V}\in\mathbb{R}^{S_{q}\times
    d},$ |  | (2) |'
- en: where $*$ represents the element-wise product. We give a theorem for near-lossless
    sparse attention.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$*$表示元素级乘积。我们给出一个关于近乎无损稀疏注意力的定理。
- en: Theorem 1.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1.
- en: (near-lossless sparse attention) Assume that $L_{1}$ near-losslessly approximates
    the attention output O.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (近乎无损稀疏注意力) 假设$L_{1}$近乎无损地近似了注意力输出O。
- en: 'The proof of Theorem [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation
    ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")
    please refer to Appendix [A.1](#A1.SS1 "A.1 Proof of Theorems ‣ Appendix A Appendix
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"). Theorem [1](#Thmtheorem1 "Theorem 1\.
    ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") suggests that we can always find an attention
    mask M to achieve near-lossless approximate attention for a given threshold. We
    then define a key metric that helps understand and quantify the efficiency of
    a sparse attention.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '定理 [1](#Thmtheorem1 "定理 1\. ‣ 3.1 理论基础 ‣ 3 近乎无损稀疏注意力的基础 ‣ SampleAttention:
    近乎无损的长上下文 LLM 推断加速与自适应结构化稀疏注意力") 的证明请参见附录 [A.1](#A1.SS1 "A.1 定理证明 ‣ 附录 A 附录 ‣
    SampleAttention: 近乎无损的长上下文 LLM 推断加速与自适应结构化稀疏注意力")。定理 [1](#Thmtheorem1 "定理 1\.
    ‣ 3.1 理论基础 ‣ 3 近乎无损稀疏注意力的基础 ‣ SampleAttention: 近乎无损的长上下文 LLM 推断加速与自适应结构化稀疏注意力")
    表明我们总是可以找到一个注意力掩码 M，以实现给定阈值的近乎无损的近似注意力。我们接着定义一个关键指标，以帮助理解和量化稀疏注意力的效率。'
- en: Definition 1.
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.
- en: The sparsity degree (SD) measures the maximum percentage of key-value elements
    that can be dropped while maintaining a specified CRA threshold $\alpha$, and
    formulated as,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏度（SD）衡量了在保持指定的 CRA 阈值 $\alpha$ 的情况下，可以丢弃的键值对元素的最大百分比，其公式为，
- en: '|  | $1$2 |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where CRA evaluates the degree to which the attention score matrix can be recovered.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 CRA 评估注意力分数矩阵的恢复程度。
- en: Definition 2.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.
- en: The cumulative residual attention (CRA) is defined as the minimum sum of the
    remaining attention probabilities among each query after sparsification with M,
    and formulated as,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 累积残差注意力（CRA）定义为在用 M 进行稀疏化后，每个查询剩余注意力概率的最小总和，其公式为，
- en: '|  | $\textbf{CRA}(\textbf{M})=\min_{i\in\{0,\cdots,S_{q}-1\}}\sum_{k=0}^{i}\tilde{\textbf{P}}_{ik},\quad\text{where}\quad\tilde{\textbf{P}}=\textbf{M}*\textbf{P}$
    |  | (4) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{CRA}(\textbf{M})=\min_{i\in\{0,\cdots,S_{q}-1\}}\sum_{k=0}^{i}\tilde{\textbf{P}}_{ik},\quad\text{where}\quad\tilde{\textbf{P}}=\textbf{M}*\textbf{P}$
    |  | (4) |'
- en: Here we use minimum for CRA because we want to ensure even the row with minimal
    residual attention score can be near-losslessly recovered. We can show that the
    CRA of near-lossless sparse attention has a lower bound.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用 CRA 的最小值，因为我们希望确保即使是具有最小残差注意力分数的行也能近乎无损地恢复。我们可以证明，近乎无损的稀疏注意力的 CRA 有一个下界。
- en: Lemma 1.
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1.
- en: Given .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 。
- en: 'Lemma [1](#Thmlemma1 "Lemma 1\. ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") can
    be easily proved since $||\tilde{\textbf{P}}-\textbf{P}||_{1}=1-\textbf{CRA}(\textbf{M})$.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '引理 [1](#Thmlemma1 "引理 1\. ‣ 3.1 理论基础 ‣ 3 近乎无损稀疏注意力的基础 ‣ SampleAttention: 近乎无损的长上下文
    LLM 推断加速与自适应结构化稀疏注意力") 可以很容易地证明，因为 $||\tilde{\textbf{P}}-\textbf{P}||_{1}=1-\textbf{CRA}(\textbf{M})$。'
- en: 'Takeaway: By discovering an effective attention mask M that meets a desired
    CRA threshold $\alpha$) brings greater acceleration.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 收获：通过发现一个有效的注意力掩码 M，达到所需的 CRA 阈值 $\alpha$) 可以带来更大的加速。
- en: '![Refer to caption](img/a48dddd8b02b99e54e295f725bb7a652.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a48dddd8b02b99e54e295f725bb7a652.png)'
- en: 'Figure 2: Statistics of ChatGLM-6B (Model1, 28 layers$\times$=0.95) across
    different heads under a 90K sequence, indicating significant disparities in sparsity
    among the heads. (d) Different contexts cause the same head to display varied
    sparse structures, while numerous attention heads follow two primary patterns:
    column stripe and local window. (e) Relationship between the ratio of selected
    top-k strips and CRA. The high row-wise numerical distribution similarity enables
    a small amount of critical column stripes to cover the majority values of the
    full attention score matrix. Further details are presented in Appendix [A.3](#A1.SS3
    "A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),
    [A.4](#A1.SS4 "A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ChatGLM-6B（Model1, 28 layers$\times$=0.95）在90K序列下不同头部的统计，显示头部之间稀疏度的显著差异。
    (d) 不同的上下文导致同一头部显示出不同的稀疏结构，而许多注意力头遵循两种主要模式：列条纹和局部窗口。 (e) 选定的前k条带与CRA的比例关系。高行级数值分布相似性使得少量关键的列条带可以覆盖全注意力得分矩阵的大部分值。更多细节见附录[A.3](#A1.SS3
    "A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")，[A.4](#A1.SS4
    "A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")。'
- en: 3.2 Empirical Foundation of Adaptive Sparsity in Attention
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自适应稀疏性的经验基础
- en: 'Section [3.1](#S3.SS1 "3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") uncovers the possibility
    of approximating the attention output using near-lossless sparse attention, with
    the key lying in finding an effective attention mask. In this section, we present
    our empirical findings that reveal the inherently-high, head-specific, and content-aware
    adaptive sparsity and the significant patterns. These can be leveraged to achieve
    efficient near-lossless sparse attention.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '第[3.1](#S3.SS1 "3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")节揭示了使用近乎无损的稀疏注意力来近似注意力输出的可能性，关键在于找到有效的注意力掩码。在本节中，我们展示了我们的实证发现，这些发现揭示了固有的高水平、头部特定的和内容感知的自适应稀疏性及其显著模式。这些可以被利用以实现高效的近乎无损的稀疏注意力。'
- en: 'Inherently-High Sparsity Degree. Our observations reveal that LLMs inherently
    exhibit a significant sparsity degree when using near-lossless sparse attention.
    In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of
    Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention")(a), the
    average sparsity degree across different layers of various LLMs is depicted, with
    a threshold of $\alpha=0.95$ for near-lossless model accuracy. We find that most
    layers exhibit remarkably high sparsity degree, surpassing 90%, regardless of
    the input length. Notably, the first layer has a lower sparsity degree.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '固有的高稀疏度。我们的观察表明，当使用近乎无损的稀疏注意力时，LLMs固有地表现出显著的稀疏度。在图[2](#S3.F2 "Figure 2 ‣ 3.1
    Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention")(a)中，展示了不同LLMs各层的平均稀疏度，其中$\alpha=0.95$为近乎无损模型准确度的阈值。我们发现，大多数层的稀疏度非常高，超过90%，无论输入长度如何。值得注意的是，第一层的稀疏度较低。'
- en: 'To further quantify the variation in sparsity degree with increasing sequence
    length, we conduct a scaling evaluation on the "Needle in a Haystack" [[47](#bib.bib47)]
    task, as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation
    ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")(b).
    Our findings indicate that as the context becomes longer, there is a corresponding
    increase in the sparsity degree.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步量化序列长度增加对稀疏度变化的影响，我们在“干草堆中的针”[[47](#bib.bib47)]任务上进行了一次规模评估，如图[2](#S3.F2
    "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse
    Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention")(b)所示。我们的研究结果表明，随着上下文变长，稀疏度相应增加。'
- en: 'Adaptive Sparsity. The attention sparsity is head-specific and content-aware.
    The sparsity degree and structure varies across different attention heads and
    input contexts. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")(c) demonstrates
    that certain heads in most layers exhibit lower SD($\alpha$. This suggests that
    different heads may have distinct roles in processing long sequences, indicating
    that uniform compression across all heads may not be optimal. Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse
    Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention")(d) shows that different contents of
    similar length result in noticeable variations in sparse patterns within the same
    layer and head. This indicates that regions with higher attention scores change
    significantly based on the given scenario, such as different user prompts.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '自适应稀疏性。注意力稀疏性是特定于头部和内容感知的。稀疏度和结构在不同的注意力头和输入上下文中有所不同。图[2](#S3.F2 "图 2 ‣ 3.1
    理论基础 ‣ 3 接近无损稀疏注意力的基础 ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理")(c)展示了大多数层中的某些头部表现出较低的SD($\alpha$。这表明不同的头部可能在处理长序列时具有不同的作用，表明在所有头部上进行均匀压缩可能不是最佳选择。图[2](#S3.F2
    "图 2 ‣ 3.1 理论基础 ‣ 3 接近无损稀疏注意力的基础 ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理")(d)显示，相似长度的不同内容在同一层和头部中会导致稀疏模式的显著变化。这表明具有较高注意力得分的区域会根据给定场景（例如不同的用户提示）显著变化。'
- en: 'Significant Window and Stripe Patterns. We identify two significant sparse
    patterns that substantially contribute to the attention score, as depicted in
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")(d). The local window
    pattern captures recent context information, while column stripe pattern embodies
    the key global contextual information. By adaptively combining these two patterns,
    LLMs can effectively handle both fine-grained information and key contextual cues.
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")(e) demonstrates that
    selecting a small amount of critical column strips is able to cover the majority
    values of the full attention score matrix, thus achieving a high CRA. This indicates
    the high numerical distribution similarity across rows.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '显著的窗口和条纹模式。我们识别出两种显著的稀疏模式，它们对注意力得分的贡献非常大，如图[2](#S3.F2 "图 2 ‣ 3.1 理论基础 ‣ 3 接近无损稀疏注意力的基础
    ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理")(d)所示。局部窗口模式捕捉最近的上下文信息，而列条纹模式体现了关键的全局上下文信息。通过自适应地结合这两种模式，LLM能够有效地处理细粒度信息和关键上下文线索。图[2](#S3.F2
    "图 2 ‣ 3.1 理论基础 ‣ 3 接近无损稀疏注意力的基础 ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理")(e)表明，选择少量关键的列条纹能够覆盖完整注意力得分矩阵的大部分值，从而实现高CRA。这表明跨行的数值分布相似度很高。'
- en: Although similar patterns have been observed in recent works [[43](#bib.bib43),
    [37](#bib.bib37), [39](#bib.bib39)], they focus on reducing KV cache memory consumption
    during decoding. Directly migrating these approaches to accelerate prefill attention
    requires computing full attention score, which is unaffordable in long context.
    How to effectively explore these patterns for near-lossless acceleration of prefill
    is remain challenging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近的研究中观察到了类似的模式[[43](#bib.bib43), [37](#bib.bib37), [39](#bib.bib39)]，但它们主要集中在减少解码过程中的KV缓存内存消耗。直接将这些方法迁移到加速预填充注意力需要计算完整的注意力得分，这在长上下文中是难以承受的。如何有效地探索这些模式以实现接近无损的预填充加速仍然是一个挑战。
- en: 'Takeaway: Attention sparsity is inherently-high, head-specific, and content-aware,
    and exhibits significant local window and column stripe patterns. This adaptive
    sparsity indicates that sparse attention should dynamically capture the adaptive
    sparse patterns at runtime to be near-lossless.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要点：注意力稀疏性本质上是高度特定于头部、内容感知的，并且展示了显著的局部窗口和列条纹模式。这种自适应稀疏性表明，稀疏注意力应该在运行时动态捕捉自适应稀疏模式，以接近无损。
- en: 4 SampleAttention
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 SampleAttention
- en: In this section, we introduce our approach to efficiently discover effective
    attention masks with observed significant sparse patterns and accelerate the attention
    with near-lossless sparse attention.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法，以高效地发现有效的注意力掩码，这些掩码观察到显著的稀疏模式，并通过近乎无损的稀疏注意力加速注意力机制。
- en: 4.1 Problem Formulation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 问题表述
- en: 'As discussed, the key to utilizing near-lossless sparse attention is to find
    an attention mask M with the following properties to achieve superior performance:
    1) near-lossless: meets a desired CRA threshold $\alpha$, 2) adaptive: varies
    across different heads, layers and contents, 3) hardware-efficient: maximizes
    hardware efficiency, 4) efficiently discoverable: can be found with minimal overhead.
    A static mask clearly cannot meet these criteria, and these properties pose significant
    challenges.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如讨论所述，利用近乎无损稀疏注意力的关键是找到一个具有以下属性的注意力掩码 M，以实现优越的性能：1) 近乎无损：满足期望的 CRA 阈值 $\alpha$，2)
    自适应：在不同的头、层和内容中变化，3) 硬件高效：最大化硬件效率，4) 高效可发现：可以以最小的开销找到。静态掩码显然无法满足这些标准，这些属性也带来了显著的挑战。
- en: Selecting an attention mask $\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个注意力掩码 $\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$，
- en: '|  | $1$2 |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $w$ maintains near-lossless property,
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w$ 维持近乎无损的属性，
- en: Theorem 2.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2。
- en: The hardware-efficient structured sparse pattern mask $\hat{\textbf{M}}$ maintains
    near-lossless sparse.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件高效的结构化稀疏模式掩码 $\hat{\textbf{M}}$ 维持了近乎无损的稀疏性。
- en: 'The proof of Theorem [2](#Thmtheorem2 "Theorem 2\. ‣ 4.1 Problem Formulation
    ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") please refer to Appendix [A.1](#A1.SS1
    "A.1 Proof of Theorems ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention").
    Given the formulation, the problem now is to find $w$ for each head to meet the
    required properties during runtime.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '定理 [2](#Thmtheorem2 "定理 2\. ‣ 4.1 问题表述 ‣ 4 SampleAttention ‣ SampleAttention:
    具有自适应结构化稀疏注意力的长上下文 LLM 推断的近乎无损加速") 的证明请参阅附录 [A.1](#A1.SS1 "A.1 定理证明 ‣ 附录 A 附录
    ‣ SampleAttention: 具有自适应结构化稀疏注意力的长上下文 LLM 推断的近乎无损加速")。给定公式，现在的问题是找到每个头的 $w$ 以在运行时满足所需的属性。'
- en: 4.2 Method
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 方法
- en: Tuned Window Size $w$ is the sequence length of the input request. The percentage
    is tuned to be enough large to capture important local windows, and it also accommodates
    dynamic window sizes across various context lengths. While previous works have
    explored window attention [[20](#bib.bib20), [39](#bib.bib39), [37](#bib.bib37)],
    they typically rely on a fixed window size, which cannot adequately capture local
    dependencies across various context lengths.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 调整的窗口大小 $w$ 是输入请求的序列长度。该百分比调整为足够大以捕捉重要的局部窗口，同时也适应不同上下文长度的动态窗口大小。尽管以前的工作已经探讨了窗口注意力[[20](#bib.bib20),
    [39](#bib.bib39), [37](#bib.bib37)]，但它们通常依赖于固定的窗口大小，这不能充分捕捉不同上下文长度的局部依赖关系。
- en: KV Indices of Interest $I_{KV}$,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 关注的 KV 索引 $I_{KV}$，
- en: '|  | $1$2 |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: 'Ideally, computing the entire attention score matrix P and then selecting $I_{KV}$
    would be optimal, but this incurs unaffordable quadratic overhead in both computation
    and memory consumption. Fortunately, the similar distribution of large numerical
    values across rows, as observed in Section [3.2](#S3.SS2 "3.2 Empirical Foundation
    of Adaptive Sparsity in Attention ‣ 3 Foundation of Near-Lossless Sparse Attention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"), can be leveraged to simplify the indices
    selection process. SampleAttention introduces a two-stage query-guided key-value
    filtering approach to approximate the solution. The PyTorch-style algorithm refers
    to Appendix [A.7](#A1.SS7 "A.7 PyTorch-Style Implementation Algorithm ‣ Appendix
    A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention"). Our evaluations show that the approximation
    performs pretty well (Section [5](#S5 "5 Experiments ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，计算整个注意力得分矩阵P，然后选择$I_{KV}$将是最优的，但这会导致计算和内存消耗上的不可承受的二次开销。幸运的是，如第[3.2](#S3.SS2
    "3.2 自适应稀疏性的经验基础 ‣ 3 几乎无损稀疏注意力的基础 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")节所观察到的，大量数值在行中的相似分布可以用来简化索引选择过程。SampleAttention引入了一种两阶段查询引导的键值过滤方法来近似解决方案。PyTorch风格的算法参见附录[A.7](#A1.SS7
    "A.7 PyTorch风格实现算法 ‣ 附录A 附录 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")。我们的评估显示，这种近似方法表现得相当不错（第[5](#S5
    "5 实验 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")节）。
- en: 'Stage-1: Query-Guided Attention Sampling. SampleAttention first samples the
    attention score matrix by computing exact scores for a few queries (Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")①). This
    is motivated by the significant column stripe sparse pattern: a high score for
    $\textbf{P}_{ik}$. Experiments show that this simple approach is effective: sampling
    a small amount of rows can accurately approximate the real CRA, further details
    can be found in Appendix [A.5](#A1.SS5 "A.5 Effectiveness of sampling ‣ Appendix
    A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段：查询引导的注意力采样。SampleAttention首先通过计算少量查询的准确得分来采样注意力得分矩阵（图[3](#S4.F3 "图3 ‣ 4.2方法
    ‣ 4 SampleAttention ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")①）。这是受到显著的列条带稀疏模式的驱动：$\textbf{P}_{ik}$的高得分。实验表明，这种简单方法是有效的：采样少量行可以准确近似真实的CRA，更多细节见附录[A.5](#A1.SS5
    "A.5 采样的有效性 ‣ 附录A 附录 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")。
- en: 'Stage-2: Score-Based Key-Value Filtering. SampleAttention then filters key-values
    indices of interest base on the sampled attention score. Exactly solve Equation [6](#S4.E6
    "In 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") for
    sampled queries is inefficient due to long sequence length. To resolve this, SampleAttention
    filters key-values based on the accumulated attention scores along column (Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")②), which
    is more statistical approximation of attention score. After column-wise reduction,
    SampleAttention separately select top-k key-value indices that can meet the desired
    CRA threshold $\alpha$ for each head. Attention sinks can also be discovered in
    this way.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段：基于得分的键值过滤。然后，SampleAttention根据采样的注意力得分过滤感兴趣的键值索引。由于序列长度较长，精确解决方程[6](#S4.E6
    "在4.2方法 ‣ 4 SampleAttention ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")对采样查询来说效率较低。为了解决这一问题，SampleAttention基于沿列的累计注意力得分过滤键值（图[3](#S4.F3
    "图3 ‣ 4.2方法 ‣ 4 SampleAttention ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")②），这是一种对注意力得分的统计近似。在按列减少后，SampleAttention分别选择可以满足所需CRA阈值$\alpha$的前k个键值索引。也可以通过这种方式发现注意力沉没。
- en: '![Refer to caption](img/13e1b338ba1218fd30490c4ec57e9e11.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13e1b338ba1218fd30490c4ec57e9e11.png)'
- en: 'Figure 3: SampleAttention replaces the original full attention with a two-stage
    implementation. In the first stage, attention scores are computed by performing
    stride sampling across multiple rows and accumulating the scores along the column.
    In the second stage, the indices $I_{KV}$ is then merged with the masks of the
    local window and bottom area to enable sparse computation of the attention.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：SampleAttention 用两个阶段的实现替代了原始的全注意力机制。在第一个阶段，通过在多行上进行步幅采样并沿列累积分数来计算注意力分数。在第二个阶段，索引
    $I_{KV}$ 与局部窗口和底部区域的掩码合并，以实现稀疏计算注意力。
- en: 'Table 1: The meaning of hyperparameters and tuning approach.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：超参数的含义和调优方法。
- en: '| Hyperparameter | Description | Tuning |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 描述 | 调优 |'
- en: '| $\alpha$ | The desired CRA threshold | Offline profiling separately |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 期望的 CRA 阈值 | 离线分析 |'
- en: '| $r_{row}$ | The sampling ratio in stage-1 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $r_{row}$ | 阶段 1 中的采样比例 |'
- en: '| $r_{w}\%$ | The ratio of local window size |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $r_{w}\%$ | 局部窗口大小的比例 |'
- en: 'Hyperparameter Tuning. SampleAttention needs to tune several hyperparameters
    as listed in Table [1](#S4.T1 "Table 1 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention"). These hyperparameters affects both model accuracy and inference
    latency. For example, a large $\alpha$ increases the sampling overhead but reduces
    the attention approximation error. We find that fixed hyperparameter, obtained
    by lightweight offline profiling, for an LLM performs well across different tasks.
    Thus we use a small dataset that contains 22 requests ranging from 25K-96K context
    length to determine these hyperparameters. The detailed effects of varying these
    hyperparameters are studied in Section [5.3](#S5.SS3 "5.3 Hyperparameter Ablation
    Study ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优。SampleAttention 需要调整表 [1](#S4.T1 "表 1 ‣ 4.2 方法 ‣ 4 SampleAttention ‣ SampleAttention：通过自适应结构稀疏注意力对长上下文
    LLM 推理进行近乎无损加速") 中列出的几个超参数。这些超参数会影响模型的准确性和推理延迟。例如，大的 $\alpha$ 增加了采样开销，但减少了注意力近似误差。我们发现，通过轻量级离线分析获得的固定超参数在不同任务中表现良好。因此，我们使用一个包含
    22 个请求的小数据集，这些请求的上下文长度从 25K 到 96K，以确定这些超参数。不同超参数的详细影响在第 [5.3](#S5.SS3 "5.3 超参数消融研究
    ‣ 5 实验 ‣ SampleAttention：通过自适应结构稀疏注意力对长上下文 LLM 推理进行近乎无损加速") 节中进行了研究。
- en: 4.3 Hardware-efficient Implementation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 硬件高效实现
- en: To achieve substantial speedup in wall-clock time, SampleAttention is implemented
    with IO-awareness to maximize hardware-efficiency. First, the query-guided key-value
    filtering involves a series of small operators (bmm, softmax, reduction) that
    read and write large intermediate results. SampleAttention significantly reduces
    IO overhead by fusing these operators. Second, SampleAttention implements an efficient
    adaptive structured sparse attention kernel by modifying FlashAttention [[48](#bib.bib48)].
    These hardware-aware optimizations enhance speed performance significantly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显著提高壁钟时间的速度，SampleAttention 实现了 IO 感知以最大化硬件效率。首先，基于查询的键值过滤涉及一系列小型操作（bmm、softmax、reduction），这些操作读取和写入大量中间结果。SampleAttention
    通过融合这些操作显著减少了 IO 开销。其次，SampleAttention 通过修改 FlashAttention [[48](#bib.bib48)]
    实现了高效的自适应结构稀疏注意力内核。这些硬件感知的优化显著提升了速度性能。
- en: 5 Experiments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Setup
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: 'Backbones. We evaluate our method on two widely used open-source LLM variants:
    ChatGLM2-6B with a 96K context window based on GLM [[17](#bib.bib17)], and internLM2-7B [[49](#bib.bib49)]
    with a 200K context window based on LLAMA2 [[8](#bib.bib8)]. All utilized models
    are decoder-only transformers [[50](#bib.bib50)], and are pre-trained via causal
    language modeling. They encompass similar architectural components, such as rotary
    positional encoding [[51](#bib.bib51)], and grouped-query attention [[52](#bib.bib52)].
    Simultaneously, there are notable differences, e.g., the former augments the context
    window capacity via continued training with an extended sequence length, whereas
    the latter achieves length extrapolation through rope scaling. We only replace
    the full attention implementation during the prompt prefill stage with SampleAttention
    and various baselines, while maintaining an uncompressed KV cache in the decode
    phase.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 主干。我们在两种广泛使用的开源 LLM 变体上评估我们的方法：基于 GLM 的 ChatGLM2-6B，具有 96K 上下文窗口[[17](#bib.bib17)]，和基于
    LLAMA2 的 internLM2-7B[[49](#bib.bib49)]，具有 200K 上下文窗口[[8](#bib.bib8)]。所有使用的模型都是仅解码器的变换器[[50](#bib.bib50)]，并通过因果语言建模进行预训练。它们包含类似的架构组件，如旋转位置编码[[51](#bib.bib51)]和分组查询注意力[[52](#bib.bib52)]。同时，也存在显著差异，例如，前者通过扩展序列长度的持续训练来增强上下文窗口容量，而后者则通过绳索缩放实现长度外推。我们仅在提示预填阶段用
    SampleAttention 和各种基线替换了完整的注意力实现，同时在解码阶段保持未压缩的 KV 缓存。
- en: 'Tasks. We evaluate SampleAttention and other methods’ understanding capabilities
    in long-context scenarios on three distinct tasks: LongBench [[53](#bib.bib53)],
    BABILong [[54](#bib.bib54)], and Needle in a Haystack [[47](#bib.bib47)]. LongBench,
    a multi-task benchmark, comprises single and multi-document QA, summarization,
    few-shot learning, synthetic tasks, and code completion. It offers over 4,750
    test cases with task lengths from 4K-35K. BABILong is a generative benchmark test
    designed to assess long-context inferencing capability, consisting of 20 different
    tasks. Given its generative nature, task lengths can be flexibly set from 4K-88K.
    Additionally, the "Needle in a Haystack" stress test challenges models to accurately
    extract information from a specific sentence buried within a lengthy document
    at a random position. We have set the number of depth intervals at 32, with lengths
    ranging from 10K-96K. Note that in these tasks, each case is evaluated after the
    model provides an output. This output is compared against a standard answer or
    judged by more advanced models, such as GPT-4 [[55](#bib.bib55)], for scoring.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 任务。我们在三个不同任务上评估 SampleAttention 和其他方法在长上下文场景中的理解能力：LongBench[[53](#bib.bib53)]、BABILong[[54](#bib.bib54)]
    和 Needle in a Haystack[[47](#bib.bib47)]。LongBench 是一个多任务基准，包含单文档和多文档 QA、摘要生成、少样本学习、合成任务和代码补全。它提供了超过
    4,750 个测试案例，任务长度从 4K 到 35K。BABILong 是一个生成基准测试，旨在评估长上下文推理能力，由 20 个不同的任务组成。由于其生成性质，任务长度可以灵活设置为
    4K 到 88K。此外，“针在干草堆中”压力测试挑战模型从长文档中随机位置的特定句子中准确提取信息。我们将深度区间数量设置为 32，长度范围从 10K 到
    96K。请注意，在这些任务中，每个案例在模型提供输出后进行评估。该输出与标准答案进行比较，或由更先进的模型（如 GPT-4[[55](#bib.bib55)]）进行评分。
- en: 5.2 Accuracy Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 准确性结果
- en: 'Table 2: Accuracy comparison across various sparse methods on LongBench and
    BABILong. The best results are highlighted in Bold while the second best results
    are marked with an Underline.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 LongBench 和 BABILong 上各种稀疏方法的准确性比较。最佳结果用 **粗体** 标出，第二好结果用 *下划线* 标记。
- en: Model Baseline LongBench BABILong Single- Doc QA Multi- Doc QA Summari- zation
    Few-shot Learning Synthetic Tasks Code Completion Total Scores Total Scores ChatGLM2
    6B Full Attention 161.15 147.76 98.64 243.66 87.00 99.20 837.40 30.20 SampleAttention($\alpha=0.95$)
    77.53 76.01 98.52 254.95 53.02 126.83 686.86 36.88 BigBrid 72.55 73.16 95.59 254.87
    19.88 120.99 637.04 34.12 Streaming LLM 31.49 26.44 35.32 133.53 3.33 89.44 319.55
    5.96 HyperAttention 87.98 33.40 38.52 95.78 3.09 77.80 336.57 16.64 Hash-Sparse
    20.12 11.37 24.32 49.88 5.87 45.28 156.84 2.82
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 基线 LongBench BABILong 单文档 QA 多文档 QA 摘要生成 少样本学习 合成任务 代码补全 总分 总分 ChatGLM2 6B
    全注意力 161.15 147.76 98.64 243.66 87.00 99.20 837.40 30.20 SampleAttention($\alpha=0.95$)
    77.53 76.01 98.52 254.95 53.02 126.83 686.86 36.88 BigBrid 72.55 73.16 95.59 254.87
    19.88 120.99 637.04 34.12 流式 LLM 31.49 26.44 35.32 133.53 3.33 89.44 319.55 5.96
    HyperAttention 87.98 33.40 38.52 95.78 3.09 77.80 336.57 16.64 Hash-Sparse 20.12
    11.37 24.32 49.88 5.87 45.28 156.84 2.82
- en: '![Refer to caption](img/efba74be3dba7153eab3657454f4e773.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/efba74be3dba7153eab3657454f4e773.png)'
- en: 'Figure 4: Scores of different methods on the "Needle in a Haystack" task at
    various lengths.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同方法在各种长度的“针在干草堆中”任务上的得分。
- en: Baselines and settings. We consider the full attention (as the gold baseline),
    BigBrid [[20](#bib.bib20)], Streaming-LLM [[37](#bib.bib37)], HyperAttention [[26](#bib.bib26)]
    and Hash-Sparse [[24](#bib.bib24)] as baselines to compare model accuracy across
    different tasks. To maintain consistency, we assign the same window size ratio
    $8\%$ for SampleAttention are set to 5% and 0.95, respectively, through offline
    profiling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基线和设置。我们将完整注意力（作为黄金基线）、BigBrid [[20](#bib.bib20)]、Streaming-LLM [[37](#bib.bib37)]、HyperAttention
    [[26](#bib.bib26)] 和 Hash-Sparse [[24](#bib.bib24)] 作为基线，以比较不同任务中的模型准确性。为了保持一致性，我们为
    SampleAttention 分配了相同的窗口大小比率 $8\%$，并通过离线分析将其设置为 5% 和 0.95。
- en: 'Main results. Table [2](#S5.T2 "Table 2 ‣ 5.2 Accuracy Results ‣ 5 Experiments
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") and Figure [4](#S5.F4 "Figure 4 ‣ 5.2 Accuracy
    Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention") display the
    accuracy results of the models on three downstream tasks. Detailed results are
    listed in Appendix [A.2](#A1.SS2 "A.2 Detailed results ‣ Appendix A Appendix ‣
    SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"). The results show that:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '主要结果。表 [2](#S5.T2 "Table 2 ‣ 5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention") 和图 [4](#S5.F4 "Figure 4 ‣ 5.2 Accuracy Results ‣ 5 Experiments
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") 显示了模型在三个下游任务上的准确性结果。详细结果列在附录 [A.2](#A1.SS2
    "A.2 Detailed results ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") 中。结果显示：'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The performance in accuracy of SampleAttention is consistently robust across
    all benchmarks (including subdomains), various models, and diverse sequence lengths.
    When compared to full attention, which serves as the gold standard, SampleAttention
    consistently achieves scores above 99% of full attention, demonstrating near-lossless
    efficiency.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SampleAttention 在所有基准测试（包括子领域）、各种模型和不同序列长度上的准确性表现始终稳健。与作为黄金标准的完整注意力相比，SampleAttention
    始终能达到超过 99% 的完整注意力得分，显示出接近无损的效率。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BigBrid exhibits varying degrees of performance degradation across different
    tasks, with "Synthetic Task" presenting a significant challenge. Nonetheless,
    on average, BigBrid still attains scores that are approximately 91% of those achieved
    by full attention.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BigBrid 在不同任务中表现出不同程度的性能下降，其中“合成任务”呈现出显著的挑战。然而，平均而言，BigBrid 仍能获得大约为完整注意力 91%
    的得分。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: StreamingLLM, HyperAttention and Hash-Sparse result in performance degradation
    across all tasks, demonstrating that these techniques fail to capture critical
    KV elements in long sequences at the prefill stage.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: StreamingLLM、HyperAttention 和 Hash-Sparse 在所有任务中均表现出性能下降，表明这些技术未能在预填阶段捕捉长序列中的关键
    KV 元素。
- en: 5.3 Hyperparameter Ablation Study
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 超参数消融研究
- en: 'We conducted further tests on the impact of three critical hyperparameters
    in SampleAttention on the accuracy of downstream tasks. These experiments adhered
    to the settings outlined in Section [5.2](#S5.SS2 "5.2 Accuracy Results ‣ 5 Experiments
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"), with only one hyperparameter changed at
    a time. Detailed results under different hyperparameter configurations are provided
    in Table [5.2](#S5.SS2 "5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步测试了 SampleAttention 中三个关键超参数对下游任务准确性的影响。这些实验遵循了第 [5.2](#S5.SS2 "5.2 Accuracy
    Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention") 节中概述的设置，每次只更改一个超参数。不同超参数配置下的详细结果见表
    [5.2](#S5.SS2 "5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")。'
- en: 'Table 3: Results of varying the three hyperparameters in the SampleAttention
    on the ChatGLM2-6B. The best results are highlighted in Bold while the second
    best results are marked with an Underline.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：SampleAttention 在 ChatGLM2-6B 上变动三个超参数的结果。最佳结果用**粗体**标出，而第二最佳结果用*下划线*标记。
- en: Task full attention CRA threshold $\alpha$ $r_{w}=8$ LongBench 837.40 820.30
    824.98 833.00 829.80 792.87 833.00 809.34 833.00 831.14 BABILong 30.20 27.28 29.08
    31.04 31.16 31.12 31.04 28.92 31.04 30.64 Needle in a Haystack 2235 2130 2090
    2239 2231 2084 2239 2106 2239 2231
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 任务全注意力 CRA 阈值 $\alpha$ $r_{w}=8$ LongBench 837.40 820.30 824.98 833.00 829.80
    792.87 833.00 809.34 833.00 831.14 BABILong 30.20 27.28 29.08 31.04 31.16 31.12
    31.04 28.92 31.04 30.64 Needle in a Haystack 2235 2130 2090 2239 2231 2084 2239
    2106 2239 2231
- en: CRA threshold $\alpha$ for a given model is essential.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型的 CRA 阈值 $\alpha$ 非常重要。
- en: Local window size and sampling ratio. Additionally, setting excessively small
    local window ratios or sampling ratios also results in performance decreases.
    Specifically, halving the ratio of the local window size (k=4) results in a performance
    decline of over 6% in the LongBench and "Needle-in-a-Haystack" tasks. This confirms
    the high significance of KV elements within the local window area. Additionally,
    reducing the sampling ratio to 2% results in an approximate 4.5% performance loss.
    However, performance stabilizes once the sampling ratio reaches a certain threshold,
    as the top-k results for the approximate attention becomes stable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本地窗口大小和采样比例。此外，设置过小的本地窗口比例或采样比例也会导致性能下降。具体来说，将本地窗口大小的比例减半（k=4）会导致 LongBench
    和“针在大海中”任务的性能下降超过 6%。这确认了 KV 元素在本地窗口区域内的重要性。此外，将采样比例降低到 2% 会导致大约 4.5% 的性能损失。然而，一旦采样比例达到某一阈值，性能会稳定，因为近似注意力的
    top-k 结果变得稳定。
- en: 5.4 Acceleration Speedup Benchmarking
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 加速速度基准测试
- en: 'We conducted micro-benchmarks on a single NVIDIA-A100 GPU (80GB) to evaluate
    performance in speed of attention operation during the prefill and TTFT metrics.
    The baselines selected were PyTorch’s scaled_dot_product_attention (noted as SDPA)
    and FlashAttention2. All tests were conducted using the configuration from ChatGLM2-6B:
    32 heads, and $d=128$, with synthetic data from the "Needle-in-a-Haystack" benchmark
    as input. We standardize the batch size of the input data to 1 to support longer
    sequence lengths.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单个 NVIDIA-A100 GPU（80GB）上进行了微基准测试，以评估预填充和 TTFT 指标期间注意力操作的速度性能。选择的基准是 PyTorch
    的 scaled_dot_product_attention（记作 SDPA）和 FlashAttention2。所有测试均使用 ChatGLM2-6B 的配置：32
    个头，$d=128$，输入数据来自“针在大海中”基准测试。我们将输入数据的批量大小标准化为 1，以支持更长的序列长度。
- en: 'Speedup and sampling overhead Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration
    Speedup Benchmarking ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") displays
    the profiling results conducted on the model’s full 28 layers using generated
    data ranging from 8K to 96K. Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration Speedup
    Benchmarking ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention")(a), focusing
    on the GPU performance of the attention module, indicates that both SampleAttention
    ($\alpha=0.95$, respectively. Furthermore, Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration
    Speedup Benchmarking ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")(c) demonstrates
    that as sequence lengths increase, the proportion of sampling overhead decreases,
    suggesting that SampleAttention can offer greater acceleration benefits for longer
    sequences.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 加速和采样开销图[5](#S5.F5 "图 5 ‣ 5.4 加速速度基准测试 ‣ 5 实验 ‣ SampleAttention：具有自适应结构稀疏注意力的长上下文
    LLM 推理的近乎无损加速") 显示了对模型的全部 28 层使用生成数据（从 8K 到 96K）的分析结果。图[5](#S5.F5 "图 5 ‣ 5.4 加速速度基准测试
    ‣ 5 实验 ‣ SampleAttention：具有自适应结构稀疏注意力的长上下文 LLM 推理的近乎无损加速")(a) 聚焦于注意力模块的 GPU 性能，表明
    SampleAttention（$\alpha=0.95$）分别。进一步地，图[5](#S5.F5 "图 5 ‣ 5.4 加速速度基准测试 ‣ 5 实验 ‣
    SampleAttention：具有自适应结构稀疏注意力的长上下文 LLM 推理的近乎无损加速")(c) 证明随着序列长度的增加，采样开销的比例减少，这表明
    SampleAttention 可以为更长的序列提供更大的加速收益。
- en: '![Refer to caption](img/f9e963ec7e334475f3a5ada2944586a3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f9e963ec7e334475f3a5ada2944586a3.png)'
- en: 'Figure 5: (a) Latency comparison for the self-attention module. (b)The proportion
    of time spent on sampling and sparse computation in SampleAttention. (c) Comparison
    for the TTFT metric.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: (a) 自注意力模块的延迟比较。(b) SampleAttention 中采样和稀疏计算的时间比例。(c) TTFT 指标的比较。'
- en: 'Scaling the sequence length to 1M. We conducted GPU performance evaluations
    scalable to a sequence length of 1 million, based on profiling results from the
    first layer. Since SampleAttention is content-aware, for sequences longer than
    128K, we derived the average attention latency per layer from the first layer
    results of SampleAttention combined with model sparsity analysis to avoid memory
    issues. Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Acceleration Speedup Benchmarking ‣
    5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM
    Inference with Adaptive Structured Sparse Attention") illustrates that at a sequence
    scaling to 1M, thresholds of 0.95 and 0.80 respectively achieve reductions in
    the TTFT metric by $2.27\times$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '扩展序列长度到1M。我们进行了GPU性能评估，以适应序列长度为100万的情况，基于来自第一层的分析结果。由于SampleAttention是内容感知的，对于长度超过128K的序列，我们从SampleAttention的第一层结果结合模型稀疏性分析中推导出每层的平均注意力延迟，以避免内存问题。图
    [6](#S5.F6 "图6 ‣ 5.4 加速速度基准测试 ‣ 5 实验 ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理的几乎无损加速")
    显示，在序列扩展到1M时，0.95和0.80的阈值分别使TTFT指标减少了 $2.27\times$。'
- en: '![Refer to caption](img/b530a455793848fe404fb6954930da39.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b530a455793848fe404fb6954930da39.png)'
- en: 'Figure 6: (a) and (b) compare the latency of attention and TTFT metrics as
    the sequence scales from 8K to 1M, respectively. The numbers represent the speedup
    compared with FlashAttention2.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: (a) 和 (b) 比较了在序列从8K到1M时，注意力和TTFT指标的延迟。数字代表与FlashAttention2相比的加速比。'
- en: 6 Conclusion
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, we first present both theoretical and empirical foundation for
    near-lossless sparse attention, and then leverage observed significant patterns
    to design SampleAttention, an adaptive structured sparse attention that can seamlessly
    replace FlashAttention in long context LLMs without accuracy loss. SampleAttention
    significantly reduces the TTFT of long context requests. Limitations and future
    work are discussed in Appendix [A.6](#A1.SS6 "A.6 Limitations and Future Work
    ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们首先展示了几乎无损稀疏注意力的理论和实证基础，然后利用观察到的显著模式设计了SampleAttention，这是一种自适应结构化稀疏注意力，可以无缝替代长上下文LLM中的FlashAttention而不会损失准确性。SampleAttention显著减少了长上下文请求的TTFT。限制和未来工作在附录
    [A.6](#A1.SS6 "A.6 限制和未来工作 ‣ 附录A 附录 ‣ SampleAttention: 通过自适应结构化稀疏注意力加速长上下文LLM推理的几乎无损加速")
    中讨论。'
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
    et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039,
    2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
    等. 基础模型的有效长上下文扩展. arXiv 预印本 arXiv:2309.16039, 2023.'
- en: '[2] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin.
    Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, 和 Dahua Lin.
    基于绳索的外推的尺度定律. arXiv 预印本 arXiv:2310.05209, 2023.'
- en: '[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language
    models. In The Twelfth International Conference on Learning Representations, 2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    和 Jiaya Jia. Longlora: 高效的长上下文大语言模型微调. 在第十二届国际学习表示会议上, 2023.'
- en: '[4] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E.
    Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms
    truly promise on context length?, June 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E.
    Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 开源LLM在上下文长度上的真正承诺时间有多长？，2023年6月。'
- en: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian. 通过位置插值扩展大语言模型的上下文窗口.
    arXiv 预印本 arXiv:2306.15595, 2023.'
- en: '[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei。语言模型是少样本学习者。在
    Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, 和 Hsuan-Tien
    Lin 主编的《神经信息处理系统进展 33：2020 年神经信息处理系统年会，NeurIPS 2020》，2020 年 12 月 6-12 日，虚拟，2020。'
- en: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin。注意力是你所需要的一切。《神经信息处理系统进展》，30,
    2017。'
- en: '[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    等。Llama 2：开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[9] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown,
    and Tatsunori Hashimoto. Benchmarking large language models for news summarization.
    Transactions of the Association for Computational Linguistics, 12, 2024.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown,
    和 Tatsunori Hashimoto。基准测试大型语言模型用于新闻摘要。《计算语言学协会会刊》，12, 2024。'
- en: '[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
    et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,
    2021.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
    等。评估在代码上训练的大型语言模型。arXiv 预印本 arXiv:2107.03374, 2021。'
- en: '[11] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
    Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code
    llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
    Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, 等。Code Llama：开放基础模型用于代码。arXiv
    预印本 arXiv:2308.12950, 2023。'
- en: '[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing。Vicuna：一个开源聊天机器人，以90%* chatgpt质量打动 GPT-4，2023年3月。'
- en: '[13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto。斯坦福 Alpaca：一个遵循指令的 Llama
    模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023。'
- en: '[14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, 等。Gemini：一个高能力的多模态模型系列。arXiv
    预印本 arXiv:2312.11805, 2023。'
- en: '[15] Anthropic. Claude. [https://www.anthropic.com/claude](https://www.anthropic.com/claude),
    2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Anthropic。Claude。 [https://www.anthropic.com/claude](https://www.anthropic.com/claude),
    2023。'
- en: '[16] Moonshot. Kimi chat. [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/),
    2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Moonshot。Kimi 聊天。 [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/),
    2023。'
- en: '[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. arXiv preprint arXiv:2103.10360, 2021.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zhengxiao Du、Yujie Qian、Xiao Liu、Ming Ding、Jiezhong Qiu、Zhilin Yang 和
    Jie Tang。GLM：具有自回归空白填充的通用语言模型预训练。arXiv 预印本 arXiv:2103.10360，2021年。'
- en: '[18] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary
    Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC:
    Encoding long and structured inputs in transformers. In Bonnie Webber, Trevor
    Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), pages 268–284, Online, November
    2020\. Association for Computational Linguistics.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Joshua Ainslie、Santiago Ontanon、Chris Alberti、Vaclav Cvicek、Zachary Fisher、Philip
    Pham、Anirudh Ravula、Sumit Sanghai、Qifan Wang 和 Li Yang。ETC：在变换器中编码长且结构化的输入。在 Bonnie
    Webber、Trevor Cohn、Yulan He 和 Yang Liu 主编的《2020 年自然语言处理实证方法会议论文集》（EMNLP）中，页码 268–284，在线，2020年11月。计算语言学协会。'
- en: '[19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. arXiv preprint arXiv:2004.05150, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Iz Beltagy、Matthew E Peters 和 Arman Cohan。Longformer：长文档变换器。arXiv 预印本
    arXiv:2004.05150，2020年。'
- en: '[20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
    Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.
    Big bird: Transformers for longer sequences. Advances in neural information processing
    systems, 33:17283–17297, 2020.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Manzil Zaheer、Guru Guruganesh、Kumar Avinava Dubey、Joshua Ainslie、Chris
    Alberti、Santiago Ontanon、Philip Pham、Anirudh Ravula、Qifan Wang、Li Yang 等。Big Bird：适用于更长序列的变换器。神经信息处理系统进展，33:17283–17297，2020年。'
- en: '[21] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. arXiv preprint arXiv:2001.04451, 2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Nikita Kitaev、Łukasz Kaiser 和 Anselm Levskaya。Reformer：高效的变换器。arXiv 预印本
    arXiv:2001.04451，2020年。'
- en: '[22] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. arXiv preprint arXiv:2307.02486, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jiayu Ding、Shuming Ma、Li Dong、Xingxing Zhang、Shaohan Huang、Wenhui Wang、Nanning
    Zheng 和 Furu Wei。LongNet：将变换器扩展到 1,000,000,000 个标记。arXiv 预印本 arXiv:2307.02486，2023年。'
- en: '[23] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏变换器生成长序列。arXiv
    预印本 arXiv:1904.10509，2019年。'
- en: '[24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.
    Faster causal attention over large sequences through sparse flash attention. arXiv
    preprint arXiv:2306.01160, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Matteo Pagliardini、Daniele Paliotta、Martin Jaggi 和 François Fleuret。通过稀疏闪存注意力在大序列上实现更快的因果注意力。arXiv
    预印本 arXiv:2306.01160，2023年。'
- en: '[25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Aurko Roy、Mohammad Saffar、Ashish Vaswani 和 David Grangier。高效的基于内容的稀疏注意力与路由变换器。计算语言学协会会刊，9:53–68，2021年。'
- en: '[26] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff,
    and Amir Zandieh. Hyperattention: Long-context attention in near-linear time.
    In The Twelfth International Conference on Learning Representations, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Insu Han、Rajesh Jayaram、Amin Karbasi、Vahab Mirrokni、David Woodruff 和 Amir
    Zandieh。超注意力：近线性时间的长上下文注意力。在第十二届国际学习表征会议上，2023年。'
- en: '[27] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
    Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Sinong Wang、Belinda Z Li、Madian Khabsa、Han Fang 和 Hao Ma。Linformer：具有线性复杂度的自注意力。arXiv
    预印本 arXiv:2006.04768，2020年。'
- en: '[28] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference
    on Learning Representations, 2020.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Krzysztof Marcin Choromanski、Valerii Likhosherstov、David Dohan、Xingyou
    Song、Andreea Gane、Tamas Sarlos、Peter Hawkins、Jared Quincy Davis、Afroz Mohiuddin、Lukasz
    Kaiser 等。用表现者重新思考注意力。在国际学习表征会议上，2020年。'
- en: '[29] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    In International conference on machine learning, pages 5156–5165\. PMLR, 2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Angelos Katharopoulos、Apoorv Vyas、Nikolaos Pappas 和 François Fleuret。变换器是
    RNN：具有线性注意力的快速自回归变换器。在国际机器学习大会上，页码 5156–5165。PMLR，2020年。'
- en: '[30] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information
    Processing Systems, 34:17413–17426, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, 和 Christopher
    Ré。Scatterbrain: 统一稀疏和低秩注意力。神经信息处理系统进展，34:17413–17426, 2021。'
- en: '[31] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra,
    and Christopher Re. Pixelated butterfly: Simple and efficient sparse training
    for neural network models. In International Conference on Learning Representations,
    2021.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra,
    和 Christopher Re。像素化蝴蝶：神经网络模型的简单高效稀疏训练。国际学习表征会议，2021。'
- en: '[32] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. arXiv preprint arXiv:2312.00752, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Albert Gu 和 Tri Dao。Mamba: 具有选择性状态空间的线性时间序列建模。arXiv 预印本 arXiv:2312.00752,
    2023。'
- en: '[33] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.
    Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV 等。Rwkv:
    为变换器时代重新发明 rnns。arXiv 预印本 arXiv:2305.13048, 2023。'
- en: '[34] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.
    Advances in Neural Information Processing Systems, 35:11079–11091, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Aydar Bulatov, Yury Kuratov, 和 Mikhail Burtsev。递归记忆变换器。神经信息处理系统进展，35:11079–11091,
    2022。'
- en: '[35] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
    transformers. arXiv preprint arXiv:2203.08913, 2022.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, 和 Christian Szegedy。记忆变换器。arXiv
    预印本 arXiv:2203.08913, 2022。'
- en: '[36] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
    Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau,
    Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from
    trillions of tokens. In International conference on machine learning, pages 2206–2240\.
    PMLR, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
    Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau,
    Bogdan Damoc, Aidan Clark 等。通过从万亿级标记中检索来改进语言模型。国际机器学习会议，页码 2206–2240。PMLR, 2022。'
- en: '[37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis。具有注意力汇流池的高效流媒体语言模型。arXiv
    预印本 arXiv:2309.17453, 2023。'
- en: '[38] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther
    Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois,
    William Chou, et al. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual
    International Symposium on Computer Architecture (ISCA), pages 446–459\. IEEE,
    2020.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther
    Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois,
    William Chou 等。Mlperf 推断基准。2020 年 ACM/IEEE 第47届国际计算机架构年会 (ISCA)，页码 446–459。IEEE,
    2020。'
- en: '[39] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter
    oracle for efficient generative inference of large language models. Advances in
    Neural Information Processing Systems, 36, 2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett 等。H2o: 大型语言模型高效生成推断的重型命中预言机。神经信息处理系统进展，36,
    2024。'
- en: '[40] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv
    preprint arXiv:2312.04985, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, 和 Douglas Orr。Sparq attention: 带宽高效的 llm 推断。arXiv 预印本 arXiv:2312.04985,
    2023。'
- en: '[41] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with
    gist tokens. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jesse Mu, Xiang Li, 和 Noah Goodman。学习压缩提示词的 gist tokens。神经信息处理系统进展，36,
    2024。'
- en: '[42] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson WH Lau. Biformer:
    Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 10323–10333, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, 和 Rynson WH Lau。Biformer:
    具有双层路由注意力的视觉变换器。IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10323–10333, 2023。'
- en: '[43] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive kv cache compression for llms.
    arXiv preprint arXiv:2310.01801, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao. 模型告诉你要丢弃什么：自适应 kv 缓存压缩用于 LLM。arXiv 预印本 arXiv:2310.01801, 2023。'
- en: '[44] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang,
    and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large
    language models. arXiv preprint arXiv:2405.06219, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang,
    和 Dahua Lin. Skvq: 滑动窗口键值缓存量化用于大型语言模型。arXiv 预印本 arXiv:2405.06219, 2024。'
- en: '[45] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和 Song
    Han. Smoothquant: 大型语言模型的准确高效后训练量化。在国际机器学习会议上, 页码 38087–38099. PMLR, 2023。'
- en: '[46] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit
    quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, 和 Baris Kasikci. Atom: 低位量化以实现高效且准确的
    LLM 服务。arXiv 预印本 arXiv:2310.19102, 2023。'
- en: '[47] G Kamradt. Needle in a haystack–pressure testing llms, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G Kamradt. 稻草堆中的针——压力测试 LLM，2023。'
- en: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. Advances in Neural
    Information Processing Systems, 35:16344–16359, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré. Flashattention:
    具有 IO 觉知的快速且内存高效的精确注意力。神经信息处理系统进展, 35:16344–16359, 2022。'
- en: '[49] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen,
    Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv
    preprint arXiv:2403.17297, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen,
    Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, 等等. Internlm2 技术报告。arXiv 预印本 arXiv:2403.17297,
    2024。'
- en: '[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 等等. 通过生成预训练提升语言理解。2018。'
- en: '[51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, 和 Yunfeng Liu.
    Roformer: 增强的旋转位置嵌入变换器。Neurocomputing, 568:127063, 2024。'
- en: '[52] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models
    from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, 和 Sumit Sanghai. Gqa: 从多头检查点训练广义多查询变换器模型。arXiv 预印本 arXiv:2305.13245, 2023。'
- en: '[53] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
    Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual,
    multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508,
    2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
    Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等等. Longbench: 一个双语、多任务的长上下文理解基准。arXiv
    预印本 arXiv:2308.14508, 2023。'
- en: '[54] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin,
    and Mikhail Burtsev. In search of needles in a 10m haystack: Recurrent memory
    finds what llms miss. arXiv preprint arXiv:2402.10790, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin,
    和 Mikhail Burtsev. 在 10 米稻草堆中寻找针：递归记忆发现 LLM 漏掉的内容。arXiv 预印本 arXiv:2402.10790,
    2024。'
- en: '[55] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, 等等. Gpt-4 技术报告。arXiv 预印本 arXiv:2303.08774, 2023。'
- en: Appendix A Appendix
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Proof of Theorems
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 定理证明
- en: 'For Theorem [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation ‣ 3
    Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '对于定理 [1](#Thmtheorem1 "定理 1. ‣ 3.1 理论基础 ‣ 3 接近无损稀疏注意力的基础 ‣ SampleAttention:
    通过自适应结构化稀疏注意力加速长上下文 LLM 推理")，'
- en: Proof.
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: In the worst case, we can set the attention mask M to all ones, ensuring that
    the sparse attention score $\tilde{\textbf{P}}$. With that attention mask,
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，我们可以将注意力掩码 M 设置为全 1，从而确保稀疏注意力分数 $\tilde{\textbf{P}}$。使用该注意力掩码，
- en: '|  | $1$2 |  | (7) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: Therefore, $||\tilde{\textbf{O}}-\textbf{O}||_{1}\leq\epsilon$, completing the
    proof. ∎
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$||\tilde{\textbf{O}}-\textbf{O}||_{1}\leq\epsilon$，证明完成。∎
- en: 'For Theorem [2](#Thmtheorem2 "Theorem 2\. ‣ 4.1 Problem Formulation ‣ 4 SampleAttention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"),'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定理 [2](#Thmtheorem2 "定理 2. ‣ 4.1 问题表述 ‣ 4 样本注意力 ‣ 样本注意力：使用自适应结构化稀疏注意力加速长上下文LLM推理，接近无损")，
- en: Proof.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'In the worst case, we can also set the attention mask $\hat{\textbf{M}}$ maintains
    the decomposed structured sparse pattern. Therefore, following the proof of Theorem [1](#Thmtheorem1
    "Theorem 1\. ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse
    Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention") completes the proof. ∎'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，我们还可以设置注意力掩码 $\hat{\textbf{M}}$ 以保持分解的结构化稀疏模式。因此，按照定理 [1](#Thmtheorem1
    "定理 1. ‣ 3.1 理论基础 ‣ 3 近无损稀疏注意力的基础 ‣ 样本注意力：使用自适应结构化稀疏注意力加速长上下文LLM推理，接近无损") 的证明完成证明。∎
- en: A.2 Detailed results
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 详细结果
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ A.2 Detailed results ‣ Appendix A Appendix ‣
    SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") and Figure [8](#A1.F8 "Figure 8 ‣ A.2 Detailed
    results ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention") report
    the detailed scores of the two evaluated models on the BABILong and "Needle in
    a Haystack" tasks across different sequence lengths, respectively. For settings
    of the baselines and overall score statistics, please refer to Section [5.2](#S5.SS2
    "5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#A1.F7 "图 7 ‣ A.2 详细结果 ‣ 附录 A 附录 ‣ 样本注意力：使用自适应结构化稀疏注意力加速长上下文LLM推理，接近无损")
    和图 [8](#A1.F8 "图 8 ‣ A.2 详细结果 ‣ 附录 A 附录 ‣ 样本注意力：使用自适应结构化稀疏注意力加速长上下文LLM推理，接近无损")
    报告了两种评估模型在 BABILong 和“针在稻草堆中”任务上的详细分数，分别对应不同的序列长度。有关基准设置和整体评分统计，请参见第 [5.2](#S5.SS2
    "5.2 准确性结果 ‣ 5 实验 ‣ 样本注意力：使用自适应结构化稀疏注意力加速长上下文LLM推理，接近无损") 节。
- en: '![Refer to caption](img/6a664e699e17a6e46f892e18d9eb612f.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a664e699e17a6e46f892e18d9eb612f.png)'
- en: (a) Full attention
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全注意力
- en: '![Refer to caption](img/96c642ce2ce0f643ada0083e3ff84360.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/96c642ce2ce0f643ada0083e3ff84360.png)'
- en: (b) SampleAttention
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 样本注意力
- en: '![Refer to caption](img/f45265c1e9ddbeff4442964f222545f2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f45265c1e9ddbeff4442964f222545f2.png)'
- en: (c) BigBrid
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (c) BigBrid
- en: '![Refer to caption](img/4137b667ea4139c114a129ee52c3d275.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4137b667ea4139c114a129ee52c3d275.png)'
- en: (d) StreamingLLM
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (d) StreamingLLM
- en: '![Refer to caption](img/6589c94d1f697e0074ac13f3cc529397.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6589c94d1f697e0074ac13f3cc529397.png)'
- en: (e) Full attention
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 全注意力
- en: '![Refer to caption](img/7173e90e339c5096e6d36c8a7e17d8a6.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7173e90e339c5096e6d36c8a7e17d8a6.png)'
- en: (f) SampleAttention
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 样本注意力
- en: '![Refer to caption](img/0f7c5e0e57849a20f3b380e9b6ae0e65.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f7c5e0e57849a20f3b380e9b6ae0e65.png)'
- en: (g) BigBrid
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: (g) BigBrid
- en: '![Refer to caption](img/35449fdcf1b2776869be8d4cedd3ff4a.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35449fdcf1b2776869be8d4cedd3ff4a.png)'
- en: (h) StreamingLLM
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (h) StreamingLLM
- en: 'Figure 7: Detailed results of the evaluations on the BABILong benchmark: (a),
    (b), (c), and (d) are based on the ChatGLM-6B model, while (e), (f), (g), and
    (h) are based on the InternLM2-7B model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在 BABILong 基准上的评估详细结果：(a)、(b)、(c) 和 (d) 基于 ChatGLM-6B 模型，而 (e)、(f)、(g) 和
    (h) 基于 InternLM2-7B 模型。
- en: '![Refer to caption](img/1966e7a2f8140fbc8a3591100f8cf142.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1966e7a2f8140fbc8a3591100f8cf142.png)'
- en: (a) Full attention
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全注意力
- en: '![Refer to caption](img/0af63c2d7bf59bb38cf7a05455ce6ac8.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0af63c2d7bf59bb38cf7a05455ce6ac8.png)'
- en: (b) SampleAttention
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 样本注意力
- en: '![Refer to caption](img/7a210177c85a5e5e141a07347dbb8709.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a210177c85a5e5e141a07347dbb8709.png)'
- en: (c) BigBrid
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: (c) BigBrid
- en: '![Refer to caption](img/c14d0e1d3596339e21f7a0593b1d9513.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c14d0e1d3596339e21f7a0593b1d9513.png)'
- en: (d) StreamingLLM
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (d) StreamingLLM
- en: '![Refer to caption](img/8020b6765a75891d4ae5523c54525af7.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8020b6765a75891d4ae5523c54525af7.png)'
- en: (e) Full attention
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 全注意力
- en: '![Refer to caption](img/b06f917260f8a343e6675e8e2155f6b0.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b06f917260f8a343e6675e8e2155f6b0.png)'
- en: (f) SampleAttention
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (f) SampleAttention
- en: '![Refer to caption](img/66907bf7832bbab35d729504ead3a866.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/66907bf7832bbab35d729504ead3a866.png)'
- en: (g) BigBrid
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (g) BigBrid
- en: '![Refer to caption](img/202bac61cc8d0eb3ae69cb9973610382.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/202bac61cc8d0eb3ae69cb9973610382.png)'
- en: (h) StreamingLLM
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (h) StreamingLLM
- en: 'Figure 8: Detailed results of the evaluations on the "Need in a Haystack" task:
    (a), (b), (c), and (d) are based on the ChatGLM-6B model, while (e), (f), (g),
    and (h) are based on the InternLM2-7B model.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: “Need in a Haystack” 任务的详细评估结果：(a)、(b)、(c) 和 (d) 基于 ChatGLM-6B 模型，而 (e)、(f)、(g)
    和 (h) 基于 InternLM2-7B 模型。'
- en: 'Table [4](#A1.T4 "Table 4 ‣ A.2 Detailed results ‣ Appendix A Appendix ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention") displays the sequence scaling results at the prefill stage
    based on the text-generation-interface serving framework, using the ChatGLM2-6B
    model with 8$\times$NVIDIA A100 GPUs. The parallelism configuration employed is
    TP=4 and PP=2, and a chunking implementation on the sequence length has been used
    for memory-efficiency. The profiled TTFT (Time To First Token) metric and the
    proportion of self-attention modules demonstrate the influence of the attention
    mechanism’s quadratic complexity. As sequence lengths increase, this complexity
    causes a significant rise in the latency of the attention module, which can approach
    around 90% at sequence lengths of 1 million.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#A1.T4 "表 4 ‣ A.2 详细结果 ‣ 附录 A 附录 ‣ SampleAttention: 近乎无损的长上下文 LLM 推理加速与自适应结构稀疏注意力")
    显示了基于文本生成接口服务框架的预填阶段的序列扩展结果，使用了 ChatGLM2-6B 模型和 8$\times$NVIDIA A100 GPU。所采用的并行配置为
    TP=4 和 PP=2，并且在序列长度上使用了分块实现以提高内存效率。分析的 TTFT（首次标记时间）指标和自注意力模块的比例展示了注意力机制的平方复杂度的影响。随着序列长度的增加，这种复杂度导致了注意力模块的延迟显著上升，在序列长度达到
    100 万时接近 90%。'
- en: 'Table 4: Latency breakdown at the prefill stage (Based on the ChatGLM-6B).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 预填阶段的延迟分解（基于 ChatGLM-6B）。'
- en: '| Sequence Length | TTFT (ms) | Full Attention (ms) | Precent (%) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | TTFT (毫秒) | 完整注意力 (毫秒) | 比例 (%) |'
- en: '| 32K | 1273.4 | 410.4 | 32.2 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 32K | 1273.4 | 410.4 | 32.2 |'
- en: '| 64K | 2917.3 | 1538.1 | 52.7 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 64K | 2917.3 | 1538.1 | 52.7 |'
- en: '| 128K | 7756.5 | 4403.9 | 56.8 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 128K | 7756.5 | 4403.9 | 56.8 |'
- en: '| 256K | 23403.7 | 16839.5 | 72.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 256K | 23403.7 | 16839.5 | 72.0 |'
- en: '| 512K | 51084.3 | 43477.0 | 85.1 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 512K | 51084.3 | 43477.0 | 85.1 |'
- en: '| 1M | 169653.0 | 148774.1 | 87.7 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 1M | 169653.0 | 148774.1 | 87.7 |'
- en: A.3 Visualization of attention
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 注意力的可视化
- en: 'Figures [9](#A1.F9 "Figure 9 ‣ A.3 Visualization of attention ‣ Appendix A
    Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention") and Figures [10](#A1.F10 "Figure 10
    ‣ A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")
    present the sparse patterns across various heads in the ChatGLM2-6B model (28
    layers x 32 heads) under a sequence length of 61K. We conducted row-by-row filtering
    based on the full attention softmax weight, using a CRA threshold of $\alpha=0.95$,
    and randomly selected four heads from different layers for display.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](#A1.F9 "图 9 ‣ A.3 注意力的可视化 ‣ 附录 A 附录 ‣ SampleAttention: 近乎无损的长上下文 LLM
    推理加速与自适应结构稀疏注意力") 和图 [10](#A1.F10 "图 10 ‣ A.3 注意力的可视化 ‣ 附录 A 附录 ‣ SampleAttention:
    近乎无损的长上下文 LLM 推理加速与自适应结构稀疏注意力") 展示了 ChatGLM2-6B 模型（28 层 x 32 头）在序列长度为 61K 时的稀疏模式。我们基于完整注意力
    softmax 权重进行了逐行过滤，使用了 $\alpha=0.95$ 的 CRA 阈值，并随机选择了来自不同层的四个头进行展示。'
- en: 'According to the visualization results on the majority of heads, we observed
    two distinct and prominent patterns prevalent in the heatmap of attention weight:
    column stripes and local windows. Column stripe patterns embody the global contextual
    information whereas diagonal window patterns capture local information.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对大多数头部的可视化结果，我们观察到热图中存在两种明显的模式：列条纹和局部窗口。列条纹模式体现了全局上下文信息，而对角窗口模式则捕捉了局部信息。
- en: '![Refer to caption](img/48a1119234718bd8c23ea5f110251adc.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48a1119234718bd8c23ea5f110251adc.png)'
- en: (a) Layer0
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Layer0
- en: '![Refer to caption](img/5edf9121db3b91a9f0b2c3eb3b9e75e2.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5edf9121db3b91a9f0b2c3eb3b9e75e2.png)'
- en: (b) Layer0
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Layer0
- en: '![Refer to caption](img/fd2d1b8d0ff441ac8be57cd1decb0e77.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd2d1b8d0ff441ac8be57cd1decb0e77.png)'
- en: (c) Layer0
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Layer0
- en: '![Refer to caption](img/1c9f30c8082e33fa1bf9cab84ab9180f.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c9f30c8082e33fa1bf9cab84ab9180f.png)'
- en: (d) Layer0
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Layer0
- en: '![Refer to caption](img/bacd1e1e177c4869233c8b4cdf719e81.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bacd1e1e177c4869233c8b4cdf719e81.png)'
- en: (e) Layer4
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Layer4
- en: '![Refer to caption](img/2b2316233bfceba83adbe98e6f9c099b.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b2316233bfceba83adbe98e6f9c099b.png)'
- en: (f) Layer4
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Layer4
- en: '![Refer to caption](img/5c490c0f971bae8736f99f565c138d36.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c490c0f971bae8736f99f565c138d36.png)'
- en: (g) Layer4
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Layer4
- en: '![Refer to caption](img/2796bab26799d43ac594b341ba9c06eb.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2796bab26799d43ac594b341ba9c06eb.png)'
- en: (h) Layer4
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Layer4
- en: '![Refer to caption](img/f79f3c0f87fdc0d78d6ed47ef07bb5ac.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f79f3c0f87fdc0d78d6ed47ef07bb5ac.png)'
- en: (i) Layer8
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Layer8
- en: '![Refer to caption](img/590bdf18e7169d3697249e6469c6da83.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/590bdf18e7169d3697249e6469c6da83.png)'
- en: (j) Layer8
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (j) Layer8
- en: '![Refer to caption](img/4784729f933c22669afb106c859ff4e3.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4784729f933c22669afb106c859ff4e3.png)'
- en: (k) Layer8
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Layer8
- en: '![Refer to caption](img/e9d9899d0753fed264bfdaac05b77243.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9d9899d0753fed264bfdaac05b77243.png)'
- en: (l) Layer8
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: (l) Layer8
- en: '![Refer to caption](img/d647f6d6de405e6a7d40c8571f3fbe7c.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d647f6d6de405e6a7d40c8571f3fbe7c.png)'
- en: (m) Layer12
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: (m) Layer12
- en: '![Refer to caption](img/b3dd88a44d218553ed7dc812b2ca22bf.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b3dd88a44d218553ed7dc812b2ca22bf.png)'
- en: (n) Layer12
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (n) Layer12
- en: '![Refer to caption](img/fe20981bbf6d142a2a462252fd6f2f8a.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe20981bbf6d142a2a462252fd6f2f8a.png)'
- en: (o) Layer12
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (o) Layer12
- en: '![Refer to caption](img/fcbce890d2f8b58c1486e17b13135ad3.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fcbce890d2f8b58c1486e17b13135ad3.png)'
- en: (p) Layer12
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (p) Layer12
- en: 'Figure 9: The visualization attention based on a content length of 61K, displays
    the sparse patterns for randomly chosen heads from layers 0, 4, 8 and 12.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于内容长度为 61K 的可视化注意力，展示了从层 0、4、8 和 12 随机选择的头部的稀疏模式。
- en: '![Refer to caption](img/6150a3c749f4934a0f798c309bb8f503.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6150a3c749f4934a0f798c309bb8f503.png)'
- en: (a) Layer16
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Layer16
- en: '![Refer to caption](img/a678f3c2e8e301b1121309351075c50e.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a678f3c2e8e301b1121309351075c50e.png)'
- en: (b) Layer16
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Layer16
- en: '![Refer to caption](img/f9bf6a4e8dd1b124f165a1f760b12b88.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f9bf6a4e8dd1b124f165a1f760b12b88.png)'
- en: (c) Layer16
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Layer16
- en: '![Refer to caption](img/39167f8a9fa50c6f371bcb597bd2158e.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39167f8a9fa50c6f371bcb597bd2158e.png)'
- en: (d) Layer16
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Layer16
- en: '![Refer to caption](img/f51fc7fdf1c62522a8a324bebe8f907a.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f51fc7fdf1c62522a8a324bebe8f907a.png)'
- en: (e) Layer20
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Layer20
- en: '![Refer to caption](img/79a78c1b96773367e3332ae86fc643f4.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79a78c1b96773367e3332ae86fc643f4.png)'
- en: (f) Layer20
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Layer20
- en: '![Refer to caption](img/74a3fc0c2031ba3981b9dbc5596ebe18.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/74a3fc0c2031ba3981b9dbc5596ebe18.png)'
- en: (g) Layer20
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Layer20
- en: '![Refer to caption](img/27a2a3b5924f3f29b9dcbd49c852d42f.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/27a2a3b5924f3f29b9dcbd49c852d42f.png)'
- en: (h) Layer20
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Layer20
- en: '![Refer to caption](img/c6c4fdacda76faa23dfa792dab202cac.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c6c4fdacda76faa23dfa792dab202cac.png)'
- en: (i) Layer24
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Layer24
- en: '![Refer to caption](img/d65b8c695d2d2005d0ed9c50fa99a24c.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d65b8c695d2d2005d0ed9c50fa99a24c.png)'
- en: (j) Layer24
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: (j) Layer24
- en: '![Refer to caption](img/00a2f27a4fdc0ba501140116c84acb17.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00a2f27a4fdc0ba501140116c84acb17.png)'
- en: (k) Layer24
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Layer24
- en: '![Refer to caption](img/76e853c55fa04011d11f2acdb5fcbbc3.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76e853c55fa04011d11f2acdb5fcbbc3.png)'
- en: (l) Layer24
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (l) Layer24
- en: 'Figure 10: The visualization attention based on a content length of 61K, displays
    the sparse patterns for randomly chosen heads from layers 16, 20 and 24.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于内容长度为 61K 的可视化注意力，展示了从层 16、20 和 24 随机选择的头部的稀疏模式。
- en: A.4 Sparisty analysis
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 稀疏度分析
- en: 'To further quantify the degree of sparsity exposed as sequence lengths increase,
    we conducted scalability tests on the ChatGLM2-6B model using the "Needle-in-a-Haystack"
    task to evaluate sparsity. The results are presented in Table [5](#A1.T5 "Table
    5 ‣ A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention").
    According to the results, the increase in sequence length introduces more apparent
    sparsity. With each doubling of length, the proportion of KV elements needed to
    maintain the same threshold $\alpha$ dimension for heads exhibiting different
    degrees of sparsity.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步量化序列长度增加时的稀疏度，我们对 ChatGLM2-6B 模型进行了可扩展性测试，使用了“针在干草堆中”任务来评估稀疏度。结果见表 [5](#A1.T5
    "Table 5 ‣ A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")。根据结果，序列长度的增加引入了更加明显的稀疏度。每次长度翻倍，所需的
    KV 元素比例也随之变化，以维持相同的阈值 $\alpha$ 维度，适用于展示不同稀疏度的头部。'
- en: 'Table 5: Sparsity analysis for ChatGLM2-6B model as sequence length scales.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ChatGLM2-6B 模型在序列长度扩展下的稀疏度分析。
- en: '| Sequence length | Average SD ($\alpha$=0.98) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 平均标准差 ($\alpha$=0.98) |'
- en: '| 4K | 91.27% | 88.00% | 79.17% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 4K | 91.27% | 88.00% | 79.17% |'
- en: '| 8K | 93.68% | 90.74% | 83.43% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 8K | 93.68% | 90.74% | 83.43% |'
- en: '| 16K | 95.84% | 92.52% | 86.37% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 16K | 95.84% | 92.52% | 86.37% |'
- en: '| 32K | 96.34% | 93.88% | 88.68% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 32K | 96.34% | 93.88% | 88.68% |'
- en: '| 64K | 96.91% | 94.89% | 90.70% |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 64K | 96.91% | 94.89% | 90.70% |'
- en: '| 128K | 97.44% | 95.84% | 92.43% |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 128K | 97.44% | 95.84% | 92.43% |'
- en: '![Refer to caption](img/b30a4ec87c459fb4169bb63ecda2e861.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b30a4ec87c459fb4169bb63ecda2e861.png)'
- en: 'Figure 11: The frequency reduction results for the retained KV elements in
    the Sk dimension on two randomly selected heads. The SD ($\alpha=0.95$) for the
    left head under sequence length of 61K is 41.2%, while 97.5% for the right head.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 在两个随机选择的头部中 Sk 维度保留的 KV 元素的频率减少结果。左侧头部在 61K 序列长度下的 SD ($\alpha=0.95$)
    为 41.2%，右侧头部为 97.5%。'
- en: A.5 Effectiveness of sampling
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 采样效果
- en: 'To verify the efficiency of this sampling method, we conducted tests on different
    heads using two distinct sampling ratios $r_{w}$. We applied different ratios
    of top-k stripes combined with a tuned window mask to the full attention matrices
    to observe the changes in CRA. The results, as shown in Table [6](#A1.T6 "Table
    6 ‣ A.5 Effectiveness of sampling ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),
    indicate that the CRA achieved by selecting top-k stripes at a 5% sampling ratio
    is remarkably close to that obtained from the full attention score. This confirms
    that SampleAttention’s simple sampling method is highly efficient.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证这种采样方法的效率，我们在不同的头部上使用两种不同的采样比例 $r_{w}$ 进行了测试。我们将不同比例的 top-k 条纹与调整过的窗口掩码应用于完整的注意力矩阵，以观察
    CRA 的变化。结果如表 [6](#A1.T6 "Table 6 ‣ A.5 Effectiveness of sampling ‣ Appendix A
    Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention") 所示，选择 5% 采样比例的 top-k 条纹获得的 CRA 显著接近于从完整的注意力得分中获得的结果。这证实了
    SampleAttention 的简单采样方法是非常高效的。'
- en: 'Table 6: The CRA percentages that can be achieved by selecting different ratios
    of top-k stripes under different sampling ratios for each head. The sequence length
    of tested content is 61K.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 选择不同 top-k 条纹比例下，在不同采样比例下每个头部可以实现的 CRA 百分比。测试内容的序列长度为 61K。'
- en: ratio of top-k stripes 2.5% 5% 10% 20% 40% 80% sampling ratio 100% 5% 100% 5%
    100% 5% 100% 5% 100% 5% 100% 5% Layer0-Head0 10.60% 10.31% 17.85% 17.74% 29.49%
    28.83% 47.09% 46.14% 71.19% 70.15% 97.12% 96.65% Layer13-Head0 75.29% 65.62% 80.57%
    74.89% 86.33% 81.58% 92.09% 89.98% 97.07% 95.21% 99.85% 98.68% Layer13-Head13
    98.24% 97.85% 98.63% 98.29% 99.02% 98.73% 99.41% 99.12% 99.76% 99.66% 100.00%
    99.80a%
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: top-k 条纹比例 2.5% 5% 10% 20% 40% 80% 采样比例 100% 5% 100% 5% 100% 5% 100% 5% 100%
    5% 100% 5% Layer0-Head0 10.60% 10.31% 17.85% 17.74% 29.49% 28.83% 47.09% 46.14%
    71.19% 70.15% 97.12% 96.65% Layer13-Head0 75.29% 65.62% 80.57% 74.89% 86.33% 81.58%
    92.09% 89.98% 97.07% 95.21% 99.85% 98.68% Layer13-Head13 98.24% 97.85% 98.63%
    98.29% 99.02% 98.73% 99.41% 99.12% 99.76% 99.66% 100.00% 99.80%
- en: A.6 Limitations and Future Work
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 局限性和未来工作
- en: We discuss limitations of SampleAttention and future directions in this subsection.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本小节中讨论了 SampleAttention 的局限性及未来的研究方向。
- en: Other pattern and sampling.
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他模式和采样。
- en: We also identified additional diagonal structures in heads with lower sparsity
    levels. Although SampleAttention is capable of covering these areas by selecting
    an adequate proportion of KVs, accurately capturing these patterns could potentially
    lead to further performance enhancements. Additionally, considering the time overhead
    associated with sampling, how to further improve sampling efficiency to achieve
    acceleration even at shorter sequence lengths remains an important challenge for
    future research.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现了在稀疏度较低的头部中额外的对角结构。尽管 SampleAttention 能通过选择适当比例的 KVs 来覆盖这些区域，但准确捕捉这些模式可能会进一步提升性能。此外，考虑到采样的时间开销，如何进一步提高采样效率以便在较短的序列长度下也能加速处理仍然是未来研究的重要挑战。
- en: Hyperparameter tuning.
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数调优。
- en: The experimental results demonstrate that hyperparameters substantially influence
    the trade-off between task performance and speedup. Consequently, swiftly determining
    efficient hyperparameters for a specific model emerges as a critical challenge.
    In the future, we aim to implement autotuning of these hyperparameters during
    task runtime, enabling SampleAttention to consistently achieve high accuracy and
    low latency across diverse sequence lengths and scenarios.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，超参数对任务性能和加速之间的权衡具有显著影响。因此，迅速确定特定模型的高效超参数成为一个关键挑战。未来，我们计划在任务运行时实施这些超参数的自动调优，使
    SampleAttention 在各种序列长度和场景下始终实现高准确率和低延迟。
- en: Serving.
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务。
- en: After integrating SampleAttention into the distributed serving framework, we
    found that requests with ultra-long sequences (>=128K) or large batch sizes will
    cause memory issues. More engineering efforts are required to achieve memory efficiency,
    potentially through strategies like implementing pipeline or sequence parallelism
    and chunking along the sequence dimension.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 SampleAttention 集成到分布式服务框架后，我们发现请求中超长序列（>=128K）或大批量尺寸会导致内存问题。需要更多的工程工作来实现内存效率，可能通过实现流水线或序列并行以及沿序列维度的分块等策略。
- en: A.7 PyTorch-Style Implementation Algorithm
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 PyTorch 风格的实现算法
- en: 'Algorithm [1](#algorithm1 "In A.7 PyTorch-Style Implementation Algorithm ‣
    Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") presents a succinct
    pseudo-code of the SampleAttention’s implementation in the PyTorch style. Link
    to the source code based on PyTorch and Triton, along with scripts to reproduce
    the main experimental results, will be provided in the camera-ready version.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [1](#algorithm1 "在 A.7 PyTorch 风格的实现算法 ‣ 附录 A 附录 ‣ SampleAttention: 使用自适应结构化稀疏注意力加速长上下文
    LLM 推断的近乎无损加速") 提供了 SampleAttention 在 PyTorch 风格下实现的简明伪代码。基于 PyTorch 和 Triton
    的源代码链接，以及重现主要实验结果的脚本，将在最终版本中提供。'
- en: Input:$\textbf{Q}\in\mathbb{R}^{Sq\times d},\textbf{K}\in\mathbb{R}^{Sk\times
    d},\textbf{V}\in\mathbb{R}^{Sk\times d},\alpha\in(0,1),r_{row}\in(0,1),r_{w}\in\mathbb{N}$)Output
    = sparse_flash_attn(Q, K, V, M_Merged)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\textbf{Q}\in\mathbb{R}^{Sq\times d},\textbf{K}\in\mathbb{R}^{Sk\times d},\textbf{V}\in\mathbb{R}^{Sk\times
    d},\alpha\in(0,1),r_{row}\in(0,1),r_{w}\in\mathbb{N}$) 输出 = sparse_flash_attn(Q,
    K, V, M_Merged)
- en: Algorithm 1 Implementation of SampleAttention
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 样本注意力的实现
