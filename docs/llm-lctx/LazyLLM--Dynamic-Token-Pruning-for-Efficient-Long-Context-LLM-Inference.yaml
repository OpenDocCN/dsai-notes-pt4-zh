- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:00:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:00:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LazyLLM：用于高效长上下文 LLM 推断的动态令牌修剪
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14057](https://ar5iv.labs.arxiv.org/html/2407.14057)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14057](https://ar5iv.labs.arxiv.org/html/2407.14057)
- en: Qichen Fu Apple Minsik Cho Apple Thomas Merth Apple Sachin Mehta Apple Mohammad
    Rastegari Mahyar Najibi Apple
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qichen Fu Apple Minsik Cho Apple Thomas Merth Apple Sachin Mehta Apple Mohammad
    Rastegari Mahyar Najibi Apple
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The inference of transformer-based large language models consists of two sequential
    stages: 1) a *prefilling* stage to compute the KV cache of prompts and generate
    the first token, and 2) a *decoding* stage to generate subsequent tokens. For
    long prompts, the KV cache must be computed for all tokens during the *prefilling*
    stage, which can significantly increase the time needed to generate the first
    token. Consequently, the *prefilling* stage may become a bottleneck in the generation
    process. An open question remains whether all prompt tokens are essential for
    generating the first token. To answer this, we introduce a novel method, *LazyLLM*,
    that selectively computes the KV for tokens important for the next token prediction
    in both the *prefilling* and *decoding* stages. Contrary to static pruning approaches
    that prune the prompt at once, *LazyLLM* allows language models to dynamically
    select different subsets of tokens from the context in different generation steps,
    even though they might be pruned in previous steps. Extensive experiments on standard
    datasets across various tasks demonstrate that *LazyLLM* is a generic method that
    can be seamlessly integrated with existing language models to significantly accelerate
    the generation without fine-tuning. For instance, in the multi-document question-answering
    task, *LazyLLM* accelerates the *prefilling* stage of the LLama 2 7B model by
    $2.34\times$ while maintaining accuracy.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器的大型语言模型的推断包括两个连续的阶段：1）*预填充*阶段，用于计算提示的 KV 缓存并生成第一个令牌，以及 2）*解码*阶段，用于生成后续的令牌。对于长提示，在*预填充*阶段必须计算所有令牌的
    KV 缓存，这会显著增加生成第一个令牌所需的时间。因此，*预填充*阶段可能成为生成过程中的瓶颈。一个未解的问题是，是否所有提示令牌对于生成第一个令牌都是必不可少的。为了解答这个问题，我们引入了一种新方法，*LazyLLM*，它在*预填充*和*解码*阶段选择性地计算对下一个令牌预测重要的令牌的
    KV。与静态修剪方法一次性修剪提示不同，*LazyLLM* 允许语言模型在不同生成步骤中动态选择上下文中的不同子集，即使这些令牌在之前的步骤中已经被修剪。大量在各种任务的标准数据集上的实验表明，*LazyLLM*
    是一种通用方法，可以与现有语言模型无缝集成，从而显著加快生成速度而无需微调。例如，在多文档问答任务中，*LazyLLM* 将 LLama 2 7B 模型的*预填充*阶段加速了
    $2.34\times$，同时保持了准确性。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Standard prompt-based LLM inference has two sequential stages: *prefilling*
    and *decoding*, as shown in [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic
    Token Pruning for Efficient Long Context LLM Inference"). During the *prefilling*
    stage, the model computes and saves the KV cache of each token from the prompt,
    and predicts the first token. We refer to the time taken during *prefilling* stage
    as “time-to-first-token” (*TTFT*). Following the *prefilling* stage is the *decoding*
    stage, where the model reuses cached KVs to decode the next token iteratively
    until the stop criteria are met.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的基于提示的 LLM 推断有两个连续的阶段：*预填充*和*解码*，如[图 1](#S1.F1 "在 1 介绍 ‣ LazyLLM：用于高效长上下文
    LLM 推断的动态令牌修剪")所示。在*预填充*阶段，模型计算并保存每个来自提示的令牌的 KV 缓存，并预测第一个令牌。我们将*预填充*阶段所需的时间称为“时间到第一个令牌”(*TTFT*)。*预填充*阶段之后是*解码*阶段，在此阶段，模型重复使用缓存的
    KV 迭代解码下一个令牌，直到满足停止标准为止。
- en: During the *prefilling* stage, all tokens from the prompt are used by all transformer
    layers. For long prompts, *TTFT* could be slow because state-of-the-art transformer-based
    LLMs are both deep and wide (Pope et al., [2023](#bib.bib26); Kim et al., [2023](#bib.bib16);
    Aminabadi et al., [2022](#bib.bib2)), and the cost of computing attention increases
    quadratically with the number of tokens in the prompts. For instance, Llama 2
    (Touvron et al., [2023](#bib.bib28)), with 7 billion parameters, stacks 32 transformer
    layers with a model dimension of 4096\. In this scenario, *TTFT* requires $21\times$
    tokens. (Bai et al., [2023](#bib.bib4)). Therefore, optimizing *TTFT* is a critical
    path toward efficient LLM inference (NVIDIA, [2024](#bib.bib25)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *预填充* 阶段，所有来自提示的标记都会被所有变换器层使用。对于长提示，由于最先进的变换器基础的 LLM 既深且宽（Pope 等，[2023](#bib.bib26)；Kim
    等，[2023](#bib.bib16)；Aminabadi 等，[2022](#bib.bib2)），*TTFT* 可能会很慢，因为计算注意力的成本随着提示中的标记数量呈二次方增长。例如，Llama
    2（Touvron 等，[2023](#bib.bib28)），拥有 70 亿个参数，堆叠了 32 层变换器，每层的模型维度为 4096。在这种情况下，*TTFT*
    需要 $21\times$ 的标记（Bai 等，[2023](#bib.bib4)）。因此，优化 *TTFT* 是实现高效 LLM 推理的关键路径（NVIDIA，[2024](#bib.bib25)）。
- en: 'While optimizing LLM inference is an active area of research, many methods
    (Leviathan et al., [2023](#bib.bib18); Cai et al., [2024](#bib.bib7); Zhang et al.,
    [2024](#bib.bib31); Bhendawade et al., [2024](#bib.bib6); Li et al., [2024](#bib.bib20))
    have focused on improving inference speed during the *decoding* stage. Yet, there
    is little attention given to improving *TTFT*. We note that some compression-based
    works implicitly improve the *TTFT* by reducing the size of LLMs (Frantar et al.,
    [2022](#bib.bib12); Sun et al., [2023](#bib.bib27); Ma et al., [2023](#bib.bib21)).
    However, an orthogonal line of research(Li et al., [2023](#bib.bib19); Jiang et al.,
    [2023](#bib.bib14); Dao et al., [2022](#bib.bib9)) investigates how *TTFT* can
    be improved given a static transformer architecture. Within this line of research,
    a natural question arises: Are all prompt tokens essential for generating the
    first token?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管优化 LLM 推理是一个活跃的研究领域，但许多方法（Leviathan 等，[2023](#bib.bib18)；Cai 等，[2024](#bib.bib7)；Zhang
    等，[2024](#bib.bib31)；Bhendawade 等，[2024](#bib.bib6)；Li 等，[2024](#bib.bib20)）已经集中于提升
    *解码* 阶段的推理速度。然而，对于提升 *TTFT* 的关注却不多。我们注意到，一些基于压缩的方法通过减少 LLM 的大小隐式地改善了 *TTFT*（Frantar
    等，[2022](#bib.bib12)；Sun 等，[2023](#bib.bib27)；Ma 等，[2023](#bib.bib21)）。然而，另一条正交的研究路线（Li
    等，[2023](#bib.bib19)；Jiang 等，[2023](#bib.bib14)；Dao 等，[2022](#bib.bib9)）探讨了在静态变换器架构下如何改善
    *TTFT*。在这条研究路线中，产生了一个自然的问题：生成第一个标记时，所有提示标记是否都是必需的？
- en: '![Refer to caption](img/c8265491916632562842c341b75efe84.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8265491916632562842c341b75efe84.png)'
- en: 'Figure 1: Prompt-based LLM inference can be divided into two sequential stages:
    *prefilling* and *decoding*. For long prompts, the first token generation during
    *prefilling* stage could be slow. As an example, for Llama 2 7B model (Touvron
    et al., [2023](#bib.bib28)), on average, the time to generate the first token
    requires $21\times$ of the total generation time in the LongBench benchmark.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于提示的 LLM 推理可以分为两个顺序阶段：*预填充* 和 *解码*。对于长提示，*预填充* 阶段生成第一个标记可能会很慢。例如，对于 Llama
    2 7B 模型（Touvron 等，[2023](#bib.bib28)），在 LongBench 基准测试中，生成第一个标记的时间平均需要 $21\times$
    的总生成时间。
- en: 'LLM profiling on the LongBench benchmark (Bai et al., [2023](#bib.bib4)) in
    [Figure 2](#S1.F2 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning for Efficient
    Long Context LLM Inference") reveals that the attention scores of input tokens
    w.r.t. to the first generated token are very sparse, indicating that many tokens
    in the input prompt are redundant and can be removed without affecting the next
    token prediction. To this end, we propose *LazyLLM*, a novel, simple, yet effective
    technique tailored for speeding up *prefilling*. As depicted in [Figure 3](#S3.F3
    "In 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM
    Inference"), in each generation step, *LazyLLM* selectively computes the KV for
    tokens important for the next token prediction and “lazily” defers the computation
    of remaining tokens to later steps when they become relevant. We propose using
    the attention score of the prior transformer layer to measure the importance of
    tokens and progressively prune tokens along the depth of the transformer. In contrast
    to prompt compression works (Li et al., [2023](#bib.bib19); Jiang et al., [2023](#bib.bib14);
    Xu et al., [2023](#bib.bib29)), which permanently reduce the prompt for all the
    following generation steps, our method allows the model to revive previously pruned
    tokens, which we found crucial to retain accuracy. Extending progressive token
    pruning to all generation steps is non-trivial. Specifically, if a token is pruned
    at generation step $t$. To avoid such repetitive computation, we employ an additional
    caching mechanism, *Aux Cache*, to cache the hidden states of pruned tokens. This
    enables a computationally efficient pathway to revive pruned tokens, and ensures
    that the worst runtime of *LazyLLM* is never slower than the baseline.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2](#S1.F2 "在1 引言 ‣ LazyLLM：高效长上下文LLM推理的动态令牌剪枝")中对LongBench基准（Bai等，[2023](#bib.bib4)）的LLM分析显示，相对于第一个生成令牌，输入令牌的注意力分数非常稀疏，表明输入提示中的许多令牌是多余的，可以在不影响下一个令牌预测的情况下删除。为此，我们提出了*LazyLLM*，这是一种新颖、简单而有效的技术，旨在加速*预填充*。如[图3](#S3.F3
    "在3 LazyLLM ‣ LazyLLM：高效长上下文LLM推理的动态令牌剪枝")所示，在每次生成步骤中，*LazyLLM*选择性地计算对下一个令牌预测重要的令牌的KV，并“懒惰”地将其余令牌的计算推迟到稍后步骤，以便它们变得相关。我们建议使用前一层转换器的注意力分数来衡量令牌的重要性，并在转换器的深度中逐步剪枝令牌。与提示压缩方法（Li等，[2023](#bib.bib19)；Jiang等，[2023](#bib.bib14)；Xu等，[2023](#bib.bib29)）相比，这些方法会永久减少所有后续生成步骤中的提示，我们的方法允许模型恢复先前剪枝的令牌，我们发现这对保持准确性至关重要。将渐进令牌剪枝扩展到所有生成步骤并非易事。具体来说，如果一个令牌在生成步骤$t$时被剪枝。为了避免这种重复计算，我们采用了额外的缓存机制*Aux
    Cache*，以缓存剪枝令牌的隐藏状态。这使得恢复剪枝令牌的计算路径更高效，并确保*LazyLLM*的最差运行时间不比基线慢。
- en: '![Refer to caption](img/54450a998b61941f2e6a2a8eea419e26.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/54450a998b61941f2e6a2a8eea419e26.png)'
- en: 'Figure 2: We visualize the attention scores of input tokens in the prompt w.r.t.
    to the next token for each layer of Llama 2 7BTouvron et al. ([2023](#bib.bib28)).
    We also plot the distribution of the average attention score across all transformer
    layers. Result reveals that the attention scores of input tokens w.r.t. to the
    next token are very sparse, indicating that many tokens in the input prompt are
    redundant and can be safely removed without affecting the next token prediction.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们可视化了Llama 2 7BTouvron等人（[2023](#bib.bib28)）在每层的输入令牌相对于下一个令牌的注意力分数。我们还绘制了所有转换器层的平均注意力分数分布。结果显示，相对于下一个令牌的输入令牌的注意力分数非常稀疏，表明输入提示中的许多令牌是多余的，可以安全删除而不会影响下一个令牌预测。
- en: 'In summary, the advantages of *LazyLLM* are: (1) Universal: *LazyLLM* can be
    seamlessly integrated with any existing transformer-based LLM to improve inference
    speed, (2) Training-free: *LazyLLM* doesn’t require any finetuning and can be
    directly integrated without any parameter modification, (3) Effective: Empirical
    results on 16 standard datasets across 6 different language tasks shows *LazyLLM*
    can improve the inference speed of the LLM during both *prefilling* and *decoding*
    stages.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，*LazyLLM*的优势包括：(1) 通用性：*LazyLLM*可以与任何现有的基于转换器的LLM无缝集成，从而提高推理速度；(2) 无需训练：*LazyLLM*不需要任何微调，可以直接集成而无需修改参数；(3)
    高效性：在16个标准数据集上进行的实证结果表明，*LazyLLM*可以提高LLM在*预填充*和*解码*阶段的推理速度。
- en: 2 Related Work
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'The increase in the scale of large language models (LLMs) has greatly enhanced
    their performance but also introduced challenges with respect to their inference
    efficiency. The inference of generative LLMs consists of two distinct stages as
    depicted in [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference"). In particular, extensive computation
    is needed under long context scenarios to calculate the full KV cache during the
    *prefilling* stage, resulting in a long time-to-first-token (*TTFT*). This delay
    causes users to wait several seconds after submitting a prompt before receiving
    any response from the agent, leading to a poor user experience.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）规模的增加极大提升了其性能，但也带来了推断效率方面的挑战。生成型LLMs的推断包括两个不同的阶段，如[图1](#S1.F1 "在
    1 介绍 ‣ LazyLLM：用于高效长上下文LLM推断的动态标记修剪")所示。特别是在长上下文场景下，需要大量计算来在*预填充*阶段计算完整的KV缓存，导致较长的首次标记时间（*TTFT*）。这一延迟使得用户在提交提示后需要等待几秒钟才能收到代理的回应，从而导致较差的用户体验。
- en: Efficient Long Context Inference. Extensive work (Merth et al., [2024](#bib.bib22);
    Chen et al., [2023](#bib.bib8); Beltagy et al., [2020](#bib.bib5); Kitaev et al.,
    [2020](#bib.bib17)) has been proposed to improve inference efficiency for long
    context applications by reducing the memory footprint and total computations.
    Some works have focused on tailoring the architecture of the transformer for long
    context input. For instance, (Beltagy et al., [2020](#bib.bib5)) introduces a
    drop-in replacement for standard self-attention and combines local windowed attention
    with task-motivated global attention. In parallel, Reformer (Kitaev et al., [2020](#bib.bib17))
    replaces dot-product attention by one that uses locality-sensitive hashing to
    reduce its computational complexity. Though the above methods can speed up long
    context inference, they require significant model architecture change and re-training.
    This drawback makes them impractical to be applied to existing pre-trained LLMs.
    Closer to our work are efficient techniques that optimize the KV cache (Zhang
    et al., [2024](#bib.bib31); Li et al., [2024](#bib.bib20); Anagnostidis et al.,
    [2024](#bib.bib3); Nawrot et al., [2024](#bib.bib23)) by minimizing the KV cache
    size and data transfer. However, these works only focus on accelerating decoding
    steps, which are not applicable to reducing *TTFT*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的长上下文推断。大量研究（Merth et al., [2024](#bib.bib22); Chen et al., [2023](#bib.bib8);
    Beltagy et al., [2020](#bib.bib5); Kitaev et al., [2020](#bib.bib17)）提出了通过减少内存占用和总计算量来提高长上下文应用的推断效率。一些研究集中于为长上下文输入量身定制变换器的架构。例如，（Beltagy
    et al., [2020](#bib.bib5)）引入了一种可替代标准自注意力机制的方案，并结合了局部窗口注意力和任务驱动的全局注意力。与此同时，Reformer（Kitaev
    et al., [2020](#bib.bib17)）通过使用局部敏感哈希来减少点积注意力的计算复杂性。尽管上述方法可以加快长上下文推断速度，但它们需要显著的模型架构更改和重新训练。这一缺点使得它们在现有预训练大语言模型（LLMs）中应用不够实际。与我们的工作更接近的是优化KV缓存的高效技术（Zhang
    et al., [2024](#bib.bib31); Li et al., [2024](#bib.bib20); Anagnostidis et al.,
    [2024](#bib.bib3); Nawrot et al., [2024](#bib.bib23)），通过最小化KV缓存大小和数据传输。然而，这些工作仅关注加速解码步骤，这对于减少*TTFT*并不适用。
- en: Token Pruning. Previous studies on the sentence classification task (Kim et al.,
    [2022](#bib.bib15); Anagnostidis et al., [2024](#bib.bib3); He et al., [2021](#bib.bib13))
    has shown that not all tokens (*i.e*. words) in an input sequence are necessary
    to make a successful prediction. This provides several possibilities for token
    pruning, which minimizes computational demands by selectively removing less important
    tokens during inference. For example, (Kim et al., [2022](#bib.bib15)) presents
    Learned Token Pruning which adaptively removes unimportant tokens as an input
    sequence passes through transformer layers. In parallel, (He et al., [2021](#bib.bib13))
    proposes to reduce width-wise computation via token pruning for transformer-based
    models such as BERT (Devlin et al., [2018](#bib.bib10)). These aforementioned
    approaches were designed for tasks requiring only a single iteration of processing,
    such as text classification. In this work, we extend the idea of token pruning
    to generative LLMs. Specifically, our method allows the model to dynamically choose
    different sets of tokens at each generation step, which is crucial to retaining
    the performance. Furthermore, we also introduce *Aux Cache* to ensure that each
    token is computed at most once along the whole generation, and ensure the worst
    runtime of our method is not slower than the baseline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Token 剪枝。之前对句子分类任务的研究（Kim et al., [2022](#bib.bib15); Anagnostidis et al., [2024](#bib.bib3);
    He et al., [2021](#bib.bib13)）表明，并非所有输入序列中的 token（*即* 词语）都是成功预测所必需的。这为 token 剪枝提供了几种可能性，通过在推理过程中选择性地移除不重要的
    token 来减少计算需求。例如，(Kim et al., [2022](#bib.bib15)) 提出了学习的 Token 剪枝，它在输入序列经过 transformer
    层时自适应地移除不重要的 token。同时，(He et al., [2021](#bib.bib13)) 提出了通过 token 剪枝减少宽度计算，用于
    BERT 等基于 transformer 的模型 (Devlin et al., [2018](#bib.bib10))。这些方法是为仅需单次处理的任务设计的，例如文本分类。在这项工作中，我们将
    token 剪枝的理念扩展到生成型 LLMs。具体来说，我们的方法允许模型在每个生成步骤中动态选择不同的 token 集合，这对保持性能至关重要。此外，我们还引入了
    *Aux Cache* 以确保每个 token 在整个生成过程中最多计算一次，并确保我们方法的最坏运行时间不慢于基线。
- en: 3 *LazyLLM*
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 *LazyLLM*
- en: '![Refer to caption](img/cf40bab5072e97eb7576d52bb932d7df.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf40bab5072e97eb7576d52bb932d7df.png)'
- en: 'Figure 3: Comparison between standard LLM and *LazyLLM*. Instead of computing
    the KV cache of all input tokens at the *prefilling* stage, *LazyLLM* only selectively
    computes the tokens that are important to the next token prediction, deferring
    the computation of remaining tokens to later steps. *LazyLLM* significantly optimizes
    *TTFT* by reducing the amount of computation during *prefilling*. Moreover, as
    some tokens in the prompt are never selected by *LazyLLM* during the whole generation
    process (even though theoretically the model could use all tokens in the prompt),
    *LazyLLM* also reduces the total amount of computation and accelerates the overall
    generation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：标准 LLM 和 *LazyLLM* 的比较。在 *prefilling* 阶段，*LazyLLM* 不计算所有输入 token 的 KV 缓存，而是仅选择性地计算对下一个
    token 预测重要的 token，将剩余 token 的计算推迟到后续步骤。*LazyLLM* 通过减少 *prefilling* 阶段的计算量显著优化了
    *TTFT*。此外，由于在整个生成过程中，有些 prompt 中的 token 从未被 *LazyLLM* 选择（尽管理论上模型可以使用 prompt 中的所有
    token），*LazyLLM* 还减少了总计算量，加快了整体生成速度。
- en: 3.1 Background on LLM Inference
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLM 推理背景
- en: 'Generative LLM inference consists of two stages: *prefilling* and *decoding*
    (see [Figure 1](#S1.F1 "In 1 Introduction ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference")). In the *prefilling* stage, the model
    receives the prompt (a sequence of tokens) $\mathcal{T}=\{t_{i}\}_{i=1}^{N}$.
    The transformer architecture commonly used in LLMs is a stack of layers where
    each layer shares the same architecture with a multiple-head self-attention mechanism
    followed by a multi-layer perception (MLP). The time of *prefilling* is referred
    to as time-to-first-token (*a.k.a*. *TTFT*). Following the *prefilling* is the
    *decoding* steps, where the model appends the generated token $t_{n+1}$ to the
    input, and subsequently decodes the following token. The *decoding* step is repeatedly
    performed until the stop criteria are met. While the formula of each decoding
    step is similar to *prefilling*, the amount of its computation is significantly
    lower thanks to the KV cache. Specifically, with saved KV cache from *prefilling*,
    all the previous tokens do not need to pass any linear layers in the model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '生成式LLM推理包括两个阶段：*预填充*和*解码*（见[图1](#S1.F1 "在1 引言 ‣ LazyLLM: 动态令牌修剪以高效长上下文LLM推理")）。在*预填充*阶段，模型接收提示（一个令牌序列）$\mathcal{T}=\{t_{i}\}_{i=1}^{N}$。LLM中常用的变换器架构是一个层堆叠，每层共享相同的架构，具有多头自注意力机制，后跟多层感知机（MLP）。*预填充*的时间被称为首次令牌时间（*即*
    *TTFT*）。紧接着*预填充*的是*解码*步骤，其中模型将生成的令牌$t_{n+1}$附加到输入中，然后逐步解码下一个令牌。*解码*步骤重复执行，直到满足停止标准。虽然每个解码步骤的公式类似于*预填充*，但由于KV缓存，其计算量显著降低。具体而言，借助于从*预填充*中保存的KV缓存，所有先前的令牌不需要经过模型中的任何线性层。'
- en: 3.2 Inference with *LazyLLM*
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用*LazyLLM*推理
- en: 'The overview of the proposed *LazyLLM* framework is illustrated in [Figure 4](#S3.F4
    "In 3.2 Inference with LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference"). *LazyLLM* starts with the full context
    and progressively prunes tokens to gradually reduce the number of computations
    towards the end of the model. Note, *LazyLLM* allows the model to select different
    subsets of tokens from the context in different generation steps, even though
    some of them may be pruned in previous steps. Compared to static pruning which
    prunes all the tokens at once, dynamic pruning optimizes the next token prediction
    in each generation step, which is crucial to retaining the performance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '提出的*LazyLLM*框架的概述如[图4](#S3.F4 "在3.2 使用LazyLLM推理 ‣ 3 LazyLLM ‣ LazyLLM: 动态令牌修剪以高效长上下文LLM推理")所示。*LazyLLM*从完整上下文开始，并逐步修剪令牌，以逐渐减少模型末端的计算量。请注意，*LazyLLM*允许模型在不同的生成步骤中选择上下文中的不同令牌子集，即使其中一些可能在之前的步骤中已被修剪。与一次性修剪所有令牌的静态修剪相比，动态修剪优化了每个生成步骤中的下一个令牌预测，这对于保持性能至关重要。'
- en: Progressive Token Pruning. Prior to this work, token pruning has been successfully
    applied to optimize LLM inference (Zhang et al., [2024](#bib.bib31); Li et al.,
    [2024](#bib.bib20); Adnan et al., [2024](#bib.bib1); Nawrot et al., [2024](#bib.bib23)).
    However, these approaches require accumulating the full attention maps of predicting
    the first few tokens to profile the importance of prompt tokens before starting
    pruning. Consequently, they are not applicable to reduce *TTFT* as they still
    require computing all the KV cache at the *prefilling* stage.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进式令牌修剪。在这项工作之前，令牌修剪已经成功应用于优化LLM推理（Zhang等，[2024](#bib.bib31)；Li等，[2024](#bib.bib20)；Adnan等，[2024](#bib.bib1)；Nawrot等，[2024](#bib.bib23)）。然而，这些方法需要累积前几个令牌的完整注意力图，以在开始修剪之前分析提示令牌的重要性。因此，它们不能减少*TTFT*，因为它们仍需要在*预填充*阶段计算所有KV缓存。
- en: In contrast, *LazyLLM* only “lazily” computes the tokens that are important
    to predict the next token by starting from the first iteration of the inference
    (the *prefilling* step). A key challenge to pruning tokens in the first iteration
    is determining their importance. Inspired by the early exiting work (Elhoushi
    et al., [2024](#bib.bib11)) which shows the token hidden states gradually evolve
    through the transformer layers, we apply layer-wise token pruning in each generation
    step. Specifically, we use the attention map of the layer $A^{l}\in\mathcal{R}^{H\times
    N\times N}$ w.r.t. the next token to be predicted as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*LazyLLM* 仅仅通过从推理的第一次迭代（*预填充*步骤）开始，"懒惰地" 计算对预测下一个令牌重要的令牌。第一次迭代中修剪令牌的关键挑战是确定它们的重要性。受早期退出工作的启发（Elhoushi
    等人，[2024](#bib.bib11)），我们应用了每一步生成中的逐层令牌修剪。具体来说，我们使用相对于下一个待预测令牌的层的注意力图 $A^{l}\in\mathcal{R}^{H\times
    N\times N}$。
- en: '|  | $s_{i}^{l}=\frac{1}{H}\sum_{h=1}^{H}A^{l}_{h,i,N}$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}^{l}=\frac{1}{H}\sum_{h=1}^{H}A^{l}_{h,i,N}$ |  | (1) |'
- en: where $H$ attending to token $t_{i}$ head.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 表示关注于令牌 $t_{i}$ 的头。
- en: After computing the confidence scores of tokens, it is challenging to determine
    the threshold value to prune the token. Concretely, the threshold can change as
    the distribution of the attention scores varies between different layers and different
    tasks. We address this challenge by using the top-$k$ is smaller than $k^{l}$th
    percentile among the input tokens. Once the token is pruned, it is excluded from
    the computation of all successive layers. In other words, the tokens used in the
    later layers will be a subset of previous layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算令牌的置信度分数后，确定修剪令牌的阈值具有挑战性。具体来说，阈值可以随注意力分数在不同层和不同任务之间的分布而变化。我们通过使用输入令牌中小于 $k^{l}$
    百分位的 top-$k$ 值来解决这一挑战。一旦令牌被修剪，它将被排除在所有后续层的计算之外。换句话说，后层使用的令牌将是前层的一个子集。
- en: 'Our study in [Section 5.4](#S5.SS4 "5.4 Drop Rate in Different Layers ‣ 5 Experiments
    ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference") shows
    the performance changes with different locations of pruning layers and the number
    of tokens pruned. In particular, when pruning at the same transformer layer, the
    model’s performance gradually decreases as fewer tokens are kept. We also found
    pruning at later transformer layers consistently has better performance than pruning
    at earlier layers, suggesting that later layers are less sensitive to token pruning.
    To achieve a better balance of speedup and accuracy, as shown in [Figure 4](#S3.F4
    "In 3.2 Inference with LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference"), we apply progressive pruning that keeps
    more tokens at earlier transformer layers and gradually reduces the number of
    tokens towards the end of the transformer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 [第 5.4 节](#S5.SS4 "5.4 Drop Rate in Different Layers ‣ 5 Experiments ‣
    LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference") 的研究显示了不同修剪层的位置和修剪令牌数量对性能的变化。特别是，当在相同的变换器层上进行修剪时，随着保留的令牌数量减少，模型的性能逐渐下降。我们还发现，后期变换器层的修剪表现始终优于早期层的修剪，这表明后期层对令牌修剪的敏感度较低。为了在速度和准确性之间达到更好的平衡，如
    [图 4](#S3.F4 "In 3.2 Inference with LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token
    Pruning for Efficient Long Context LLM Inference") 所示，我们应用了渐进修剪，在较早的变换器层保留更多令牌，并逐步减少变换器末尾的令牌数量。'
- en: '![Refer to caption](img/7fca38505a7abefe072633fe28c6f984.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7fca38505a7abefe072633fe28c6f984.png)'
- en: 'Figure 4: Overview of the *LazyLLM* framework. *LazyLLM* starts with the full
    context and progressively prunes tokens to gradually reduce the number of computations
    towards the end of the model. *LazyLLM* allows the model to select different subsets
    of tokens from the context in different generation steps, which is crucial to
    retaining the performance.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：*LazyLLM* 框架概述。*LazyLLM* 从完整的上下文开始，并逐步修剪令牌，逐渐减少模型末尾的计算量。*LazyLLM* 允许模型在不同的生成步骤中选择上下文中的不同令牌子集，这对于保持性能至关重要。
- en: Aux Cache. In the prefilling stage, there is no KV cache and every token is
    represented by hidden states. Thus, progressive token pruning can be implemented
    by removing pruned tokens’ hidden states. However, extending the progressive token
    pruning to the following *decoding* steps is non-trivial. This is because each
    *decoding* step leverages the KV cache computed in the *prefilling* to compute
    attention. As the *LazyLLM* performs progressive token pruning at the *prefilling*
    stage, the KV of tokens pruned at layer $l$) that do not exist in the KV cache
    of layer $l+1$ may be re-selected to compute attention. In such cases, the model
    can not retrieve the KV cache of these tokens. An intuitive solution is to pass
    those tokens again from the beginning of the transformer. However, that would
    cause repetitive computation for the same token, and eventually slow down the
    whole generation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Aux Cache。在预填充阶段，没有KV缓存，每个令牌由隐藏状态表示。因此，通过删除剪枝令牌的隐藏状态可以实现逐步令牌剪枝。然而，将逐步令牌剪枝扩展到随后的*解码*步骤是非平凡的。这是因为每个*解码*步骤利用*预填充*中计算的KV缓存来计算注意力。由于*LazyLLM*在*预填充*阶段执行逐步令牌剪枝，可能会重新选择在第$l$层剪枝的令牌的KV，这些令牌在第$l+1$层的KV缓存中不存在。在这种情况下，模型无法检索这些令牌的KV缓存。一个直观的解决方案是从变换器的开头再次传递这些令牌。然而，这将导致对同一令牌的重复计算，并最终减慢整个生成过程。
- en: To tackle this challenge, we introduce *Aux Cache* in addition to the original
    KV cache, which stores the hidden states of those pruned tokens (*e.g*. $T4$ and
    $T8$), we could retrieve their hidden states from the *Aux Cache* of its previous
    layer directly instead of passing through previous layers again. The introduction
    of *Aux Cache* ensures that each token is computed at most once in every transformer
    layer, and ensures the worst runtime of *LazyLLM* is not slower than the baseline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，我们在原始KV缓存之外引入了*Aux Cache*，它存储那些被剪枝的令牌的隐藏状态（*例如* $T4$ 和 $T8$），我们可以直接从其上一层的*Aux
    Cache*中检索它们的隐藏状态，而不必再次通过上一层。*Aux Cache*的引入确保了每个令牌在每个变换器层中最多只计算一次，并确保*LazyLLM*的最坏运行时间不慢于基线。
- en: 4 Implementations Details
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实现细节
- en: We implement *LazyLLM* on Llama 2 (Touvron et al., [2023](#bib.bib28)) and XGen
    (Nijkamp et al., [2023](#bib.bib24)) and evaluate it on the LongBench (Bai et al.,
    [2023](#bib.bib4)) using HuggingFace²²2[https://github.com/huggingface/transformers/](https://github.com/huggingface/transformers/).
    We follow the official GitHub repository³³3[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)
    of LongBench for data preprocessing and prompting in all experiments. The LongBench
    benchmark consists of multiple datasets in different tasks, where each task may
    have different metrics, including ROUGE-L, F1, Accuracy, and Edit Sim. Following
    the official evaluation pipeline, we categorize all results over major task categories
    by computing the macro-average score.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Llama 2 (Touvron et al., [2023](#bib.bib28)) 和XGen (Nijkamp et al., [2023](#bib.bib24))
    上实现了*LazyLLM*，并在LongBench (Bai et al., [2023](#bib.bib4)) 上进行评估，使用HuggingFace²²2[https://github.com/huggingface/transformers/](https://github.com/huggingface/transformers/)。我们遵循LongBench的官方GitHub仓库³³3[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)进行数据预处理和提示设置。LongBench基准测试包含多个不同任务的数据集，每个任务可能有不同的指标，包括ROUGE-L、F1、准确性和编辑相似度。按照官方评估流程，我们通过计算宏平均分对所有结果进行分类。
- en: As previously noted, the proposed *LazyLLM* doesn’t require any training. Thus,
    *LazyLLM* uses the exact same existing checkpoints as the baseline, for all models.
    For inference, we conduct all experiments on NVIDIA A100 GPUs. We measure and
    report the speedup based on the empirical walltime improvement. Specifically,
    for *TTFT Speedup*, we measure the empirical walltime between when the prompt
    is fed to the model, and when the model generates the first token. For *Generation
    Speedup*, we measure the empirical walltime between when the prompt is fed to
    the model, and when the model finished generating all output tokens. We add 5
    warmup runs for each experiment before starting the time measurement to remove
    the noise such as loading model parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，提议的*LazyLLM*不需要任何训练。因此，*LazyLLM*对所有模型使用完全相同的现有检查点作为基线。对于推理，我们在NVIDIA A100
    GPUs上进行所有实验。我们根据实际墙钟时间的改进来测量和报告加速效果。具体而言，对于*TTFT加速*，我们测量从提示输入模型到模型生成第一个令牌的实际墙钟时间。对于*生成加速*，我们测量从提示输入模型到模型完成生成所有输出令牌的实际墙钟时间。我们在开始时间测量前为每个实验添加5次预热运行，以去除如加载模型参数等噪音。
- en: 5 Experiments
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'We examine our method using two large language models: Llama 2 7B and XGen
    7B. We compare our method with baselines using the same publicly released pretrained
    checkpoints, without employing any additional training. We perform experiments
    using LongBench, a multi-task benchmark for long content understanding. The LongBench
    comprises 16 datasets and covers 6 tasks including single-doc QA, multi-doc QA,
    summarization, few-shot learning, synthetic tasks, and code completion.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个大型语言模型来检验我们的方法：Llama 2 7B 和 XGen 7B。我们使用相同的公开发布的预训练检查点与基线进行比较，而未使用任何额外的训练。我们使用
    LongBench 进行实验，LongBench 是一个用于长内容理解的多任务基准。LongBench 包含 16 个数据集，并涵盖包括单文档问答、多文档问答、总结、少样本学习、合成任务和代码完成在内的
    6 个任务。
- en: For the metrics, we primarily evaluate the effectiveness and efficiency of each
    method in the *TTFT* speedup *vs*. accuracy trade-off. Following LongBench, the
    accuracy (*score*) denotes the macro-averaged scores across datasets in each task.
    The *TTFT* speedup measures the wall time improvement w.r.t. to the baseline for
    generating the first token. In analysis, we also assess the impact of our method
    on *$\%$ of Prompt Token Computed* measures the accumulated percent of prompt
    tokens computed at the end of the generation, which indicates the save of total
    computation. The *Generation* speedup measures the walltime change w.r.t. to the
    baseline for completing the entire generation process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些指标，我们主要评估每种方法在*TTFT*加速与准确度权衡中的有效性和效率。遵循 LongBench 的方法，准确度（*score*）表示每个任务中跨数据集的宏平均得分。*TTFT*加速衡量相对于基线生成第一个标记的壁钟时间改进。在分析中，我们还评估了我们的方法对
    *$\%$ 的提示标记计算* 的影响，这一指标表示在生成结束时计算的提示标记的累计百分比，指示了总计算的节省。*Generation* 加速衡量相对于基线完成整个生成过程的壁钟时间变化。
- en: '| Tasks | Method | Llama 2 |  | XGen |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 方法 | Llama 2 |  | XGen |'
- en: '| Score | TTFT Speedup ($\times$) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | TTFT 加速（$\times$） |'
- en: '| Single-Document QA | Baseline | $\mathbf{25.79}$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 单文档问答 | 基线 | $\mathbf{25.79}$ |'
- en: '| Random Token Drop | $20.05$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $20.05$ |'
- en: '| Static Token Pruning | $21.89$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $21.89$ |'
- en: '| Prompt Compression | $22.88$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $22.88$ |'
- en: '| *LazyLLM (Ours)* | $25.59$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $25.59$ |'
- en: '| Multi-Document QA | Baseline | $\mathbf{22.43}$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 多文档问答 | 基线 | $\mathbf{22.43}$ |'
- en: '| Random Token Drop | $16.77$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $16.77$ |'
- en: '| Static Token Pruning | $19.93$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $19.93$ |'
- en: '| Prompt Compression | $8.42$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $8.42$ |'
- en: '| *LazyLLM (Ours)* | $22.31$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $22.31$ |'
- en: '| Summarization | Baseline | $24.65$ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 总结 | 基线 | $24.65$ |'
- en: '| Random Token Drop | $24.39$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $24.39$ |'
- en: '| Static Token Pruning | $24.59$ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $24.59$ |'
- en: '| Prompt Compression | $25.16$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $25.16$ |'
- en: '| *LazyLLM (Ours)* | $\mathbf{24.75}$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $\mathbf{24.75}$ |'
- en: '| Few-shot Learning | Baseline | $\mathbf{62.90}$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 少样本学习 | 基线 | $\mathbf{62.90}$ |'
- en: '| Random Token Drop | $53.93$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $53.93$ |'
- en: '| Static Token Pruning | $56.54$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $56.54$ |'
- en: '| Prompt Compression | $24.18$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $24.18$ |'
- en: '| *LazyLLM (Ours)* | $62.81$ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $62.81$ |'
- en: '| Synthetic | Baseline | $4.97$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | 基线 | $4.97$ |'
- en: '| Random Token Drop | $3.57$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $3.57$ |'
- en: '| Static Token Pruning | $2.81$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $2.81$ |'
- en: '| Prompt Compression | $3.20$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $3.20$ |'
- en: '| *LazyLLM (Ours)* | $\mathbf{4.98}$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $\mathbf{4.98}$ |'
- en: '| Code Completion | Baseline | $\mathbf{55.18}$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 代码完成 | 基线 | $\mathbf{55.18}$ |'
- en: '| Random Token Drop | $44.92$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 随机标记丢弃 | $44.92$ |'
- en: '| Static Token Pruning | $37.51$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 静态标记剪枝 | $37.51$ |'
- en: '| Prompt Compression | $17.45$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 提示压缩 | $17.45$ |'
- en: '| *LazyLLM (Ours)* | $53.30$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *LazyLLM（我们的）* | $53.30$ |'
- en: 'Table 1: Comparisons of *TTFT* speedup *vs*. accuracy on various tasks. Without
    requiring any training/finetuning, *LazyLLM* consistently achieves better *TTFT*
    speedup with negligible accuracy drop. Note that the prompt compression approach
    fails at improving *TTFT* because the overhead of running LLMs to compress the
    prompt is very computationally expensive.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在各种任务上*TTFT*加速与准确度的比较。无需任何训练/微调，*LazyLLM* 始终实现了更好的*TTFT*加速，并且准确度下降可以忽略不计。请注意，提示压缩方法未能改善*TTFT*，因为运行
    LLM 来压缩提示的开销非常大。
- en: 5.1 Results
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 结果
- en: '[Table 1](#S5.T1 "In 5 Experiments ‣ LazyLLM: Dynamic Token Pruning for Efficient
    Long Context LLM Inference") presents the *TTFT* speedup *vs*. accuracy comparisons
    between *LazyLLM*, standard LLM, and other baselines. In the table, the “baseline”
    refers to the standard LLM inference. The “random token drop” baseline is based
    on (Yao et al., [2022](#bib.bib30)) that randomly prunes the prompt tokens before
    feeding them to the LLMs. We report the average metrics across 5 runs for the
    “random token drop” baseline. Our “static token pruning” baseline prunes input
    tokens at once based on their attention score of the first few transformer layers
    during the *prefilling* stage. We also compare with the prompt compression method
    (Li et al., [2023](#bib.bib19)) which pruning redundancy in the input context
    using LLMs. [Table 1](#S5.T1 "In 5 Experiments ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference") shows *LazyLLM* consistently achieves
    better *TTFT* speedup with negligible accuracy drop across multiple tasks. It
    is worth noting that the overhead of running LLMs to compress the prompt is very
    computationally expensive. Even though the inference on the reduced prompt is
    faster, the actual *TTFT* of the “prompt compression” baseline is longer than
    the baseline.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S5.T1 "在5 实验 ‣ LazyLLM: 动态标记修剪以高效处理长上下文LLM推理")展示了*TTFT*加速 *与* 准确性的比较，包括*LazyLLM*、标准LLM和其他基线。在表中，“基线”指的是标准LLM推理。“随机标记丢弃”基线基于（Yao等，[2022](#bib.bib30)）随机修剪提示标记，然后将它们输入LLM。我们报告了“随机标记丢弃”基线的5次运行的平均指标。我们的“静态标记修剪”基线在*预填充*阶段基于前几层变换器的注意力得分一次性修剪输入标记。我们还与提示压缩方法（Li等，[2023](#bib.bib19)）进行了比较，该方法通过LLMs修剪输入上下文中的冗余。[表1](#S5.T1
    "在5 实验 ‣ LazyLLM: 动态标记修剪以高效处理长上下文LLM推理")显示*LazyLLM*在多个任务中始终实现了更好的*TTFT*加速，且准确性下降微乎其微。值得注意的是，运行LLMs以压缩提示的开销非常高。即使在减少提示上的推理更快，“提示压缩”基线的实际*TTFT*仍比基线长。'
- en: 5.2 *TTFT* Speedup *vs*. Accuracy
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 *TTFT* 加速 *与* 准确性
- en: 'The inference efficiency of *LazyLLM* is controlled using three parameters:
    1) the number of pruning layers, 2) the locations of these pruning layers, and
    3) the number of tokens pruned within these layers. Increasing the number of pruning
    layers and pruning more tokens optimize computation by processing fewer tokens,
    and pruning tokens at earlier layers can save the computations for the successive
    layers. Prompting these factors will give more overall computation reduction,
    and offer better *TTFT* speedup. As a side effect, excessively pruning tokens
    may cause information loss and eventually lead to performance degradation. Similarly,
    the *TTFT* speedup and accuracy of baselines can vary with different hyperparameters.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*LazyLLM*的推理效率由三个参数控制：1）修剪层的数量，2）这些修剪层的位置，以及3）这些层中修剪的标记数量。增加修剪层的数量和修剪更多的标记可以通过处理更少的标记来优化计算，而在较早层修剪标记可以节省后续层的计算。调整这些因素将带来更大的整体计算减少，并提供更好的*TTFT*加速。然而，过度修剪标记可能导致信息丢失，最终导致性能下降。同样，基线的*TTFT*加速和准确性可能会随着不同的超参数而变化。'
- en: 'We compare *TTFT* speedup *vs*. accuracy in [Figure 5](#S5.F5 "In 5.3 Impact
    on Overall Generation Speed ‣ 5 Experiments ‣ LazyLLM: Dynamic Token Pruning for
    Efficient Long Context LLM Inference") with different hyperparameters. The visualization
    shows that, without any training, the proposed *LazyLLM* retains the accuracy
    better than baselines under the same *TTFT* speedup. For example, our method can
    offer $2.34\times$ degradation in accuracy. On the other hand, baseline methods
    accuracy degrades significantly for similar *TTFT* speed-up. Note that the prompt
    compression approaches fail at improving *TTFT* because of the compression overhead.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[图5](#S5.F5 "在5.3 对整体生成速度的影响 ‣ 5 实验 ‣ LazyLLM: 动态标记修剪以高效处理长上下文LLM推理")中比较了*TTFT*加速
    *与* 准确性，使用了不同的超参数。可视化结果显示，在没有任何训练的情况下，提出的*LazyLLM*在相同的*TTFT*加速下保持了比基线更好的准确性。例如，我们的方法可以提供$2.34\times$的准确性降低。另一方面，对于类似的*TTFT*加速，基线方法的准确性显著下降。请注意，由于压缩开销，提示压缩方法未能改善*TTFT*。'
- en: 5.3 Impact on Overall Generation Speed
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 对整体生成速度的影响
- en: To evaluate the impact of the proposed method on the overall generation process,
    we also profile the *$\%$ of Token Computed*. A lower *$\%$ of Token Computed*
    indicates *LazyLLM* reduces the total computation, consequently offering additional
    speedup to the overall generation process across diverse tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估所提方法对整体生成过程的影响，我们还对 *$\%$ 的令牌计算* 进行了分析。较低的 *$\%$ 的令牌计算* 表明 *LazyLLM* 减少了总计算量，从而为各种任务的整体生成过程提供了额外的加速。
- en: '![Refer to caption](img/d204b790048ec6b030af8ac2d35df6b6.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d204b790048ec6b030af8ac2d35df6b6.png)'
- en: 'Figure 5: *TTFT* speedup *vs*. accuracy comparison for Llama 2 7B across different
    tasks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: *TTFT* 加速与准确性比较，针对不同任务的 Llama 2 7B。'
- en: '| Tasks | $\%$ of Prompt Token Computed |  | Overall Generation Speedup |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 计算的 $\%$ 的提示令牌 |  | 整体生成速度提升 |'
- en: '| Llama 2 | XGen |  | Llama 2 | XGen |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 | XGen |  | Llama 2 | XGen |'
- en: '| Single-Document QA | $87.31$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 单文档问答 | $87.31$ |'
- en: '| Multi-Document QA | $63.94$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 多文档问答 | $63.94$ |'
- en: '| Summarization | $99.59$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | $99.59$ |'
- en: '| Few-shot Learning | $69.98$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 少样本学习 | $69.98$ |'
- en: '| Synthetic | $63.73$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 合成 | $63.73$ |'
- en: '| Code Completion | $68.57$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 代码补全 | $68.57$ |'
- en: 'Table 2: The *$\%$ of Token Computed* indicates *LazyLLM* reduces the total
    computation, consequently offering additional speedup to the overall generation
    process across diverse tasks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: *$\%$ 的令牌计算* 表示 *LazyLLM* 通过减少总计算量，从而为各种任务的整体生成过程提供额外的加速。'
- en: 5.4 Drop Rate in Different Layers
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 不同层的丢弃率
- en: 'In this section, we analyze the effect of the locations of pruning layers,
    and the number of tokens pruned. In particular, we report a series of experiments
    using a simplified version of *LazyLLM* that prunes tokens just once within the
    transformer. For each trial, we position the pruning layer at various levels of
    the transformer stack and apply different pruning ratios. We perform the experiments
    for both Llama 2 and XGen, and visualize the results in [Figure 6](#S5.F6 "In
    5.4 Drop Rate in Different Layers ‣ 5 Experiments ‣ LazyLLM: Dynamic Token Pruning
    for Efficient Long Context LLM Inference").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们分析了修剪层的位置及修剪令牌的数量的影响。特别地，我们报告了一系列使用简化版 *LazyLLM* 的实验，该版本在变换器中仅修剪一次令牌。对于每次试验，我们将修剪层放置在变换器堆栈的不同层级，并应用不同的修剪比例。我们对
    Llama 2 和 XGen 进行了实验，并在 [图 6](#S5.F6 "5.4 不同层的丢弃率 ‣ 5 实验 ‣ LazyLLM: 高效长上下文 LLM
    推理的动态令牌修剪") 中可视化结果。'
- en: 'The results show both models share a similar trend. As expected, when pruning
    at the same transformer layer, the model’s performance gradually decreases as
    fewer tokens are kept. Furthermore, pruning at later transformer layers consistently
    yields better performance compared to pruning at earlier layers, suggesting that
    later layers are less sensitive to token pruning. Based on these observations,
    we propose progressive token pruning in [Section 3.2](#S3.SS2 "3.2 Inference with
    LazyLLM ‣ 3 LazyLLM ‣ LazyLLM: Dynamic Token Pruning for Efficient Long Context
    LLM Inference"), which strategically prunes more tokens in later layers while
    preserving more in the earlier layers, optimizing the balance between efficiency
    and performance retention.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '结果显示，两种模型的趋势类似。如预期的那样，当在相同的变换器层进行修剪时，随着保留的令牌数量减少，模型的性能逐渐下降。此外，与在早期层进行修剪相比，在后期变换器层进行修剪的性能更好，这表明后期层对令牌修剪的敏感度较低。基于这些观察，我们在
    [第 3.2 节](#S3.SS2 "3.2 使用 LazyLLM 推理 ‣ 3 LazyLLM ‣ LazyLLM: 高效长上下文 LLM 推理的动态令牌修剪")
    中提出了逐步令牌修剪策略，该策略在后期层中修剪更多的令牌，同时在早期层中保留更多，从而优化效率与性能保留之间的平衡。'
- en: '![Refer to caption](img/a4f0a9266dac4b1b2f64b8b830a0b9bd.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4f0a9266dac4b1b2f64b8b830a0b9bd.png)'
- en: 'Figure 6: Effect of the locations of pruning layers, and the number of tokens
    pruned. The results of both Llama 2 7B Touvron et al. ([2023](#bib.bib28)) and
    XGen 7B Nijkamp et al. ([2023](#bib.bib24)) share a similar trend: 1) when pruning
    at the same transformer layer, the model’s performance gradually decreases as
    fewer tokens are kept, and 2) Pruning at later transformer layers consistently
    has better performance than pruning at earlier layers, suggesting that later layers
    are less sensitive to token pruning.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 修剪层的位置及修剪令牌数量的影响。Llama 2 7B Touvron 等 ([2023](#bib.bib28)) 和 XGen 7B Nijkamp
    等 ([2023](#bib.bib24)) 的结果表现出相似的趋势：1) 当在相同的变换器层进行修剪时，随着保留的令牌数量减少，模型的性能逐渐下降；2)
    在后期变换器层进行修剪的性能始终优于在早期层进行修剪，这表明后期层对令牌修剪的敏感度较低。'
- en: '![Refer to caption](img/724f5f3ad87c2a64772e8d31a2c779bb.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/724f5f3ad87c2a64772e8d31a2c779bb.png)'
- en: 'Figure 7: Statistics on number of tokens processed during generation using
    our LazyLLM technique with Llama 2 7B (Touvron et al., [2023](#bib.bib28)). We
    visualize the statistics of 1000 samples randomly sampled from LongBench. The
    $x$-axis represents the number of prompt tokens processed at that time step (normalized
    by the prompt size). We visualize these statistics for various stages within the
    network. Note that cumulative token usage is upper-bounded by the baseline (evident
    with early layers).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用我们的 LazyLLM 技术和 Llama 2 7B（Touvron 等，[2023](#bib.bib28)）生成过程中处理的标记数量统计。我们可视化了从
    LongBench 随机抽取的 1000 个样本的统计数据。$x$ 轴表示在该时间步处理的提示标记数量（按提示大小标准化）。我们对网络中的不同阶段可视化这些统计数据。请注意，累积标记使用量受到基线的上限限制（在早期层中明显）。
- en: 5.5 Progressive KV Growth
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 递进 KV 增长
- en: 'In this section, we characterize the internals of the model with the token
    pruning logic. Specifically, we seek to understand what fractions of prompt tokens
    are cumulatively used and, inversely, not used. This “cumulative token usage”
    can be equivalently defined as the KV cache size at each given step. [Figure 7](#S5.F7
    "In 5.4 Drop Rate in Different Layers ‣ 5 Experiments ‣ LazyLLM: Dynamic Token
    Pruning for Efficient Long Context LLM Inference") presents these cumulative prompt
    token usage numbers for each of the stages of the LazyLLM.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们用标记修剪逻辑来描述模型的内部结构。具体来说，我们试图了解提示标记的哪些部分是累积使用的，反之，又哪些是未使用的。这种“累积标记使用”可以等同于每个给定步骤的
    KV 缓存大小。[图 7](#S5.F7 "在 5.4 不同层的丢弃率 ‣ 5 实验 ‣ LazyLLM：高效长上下文 LLM 推理的动态标记修剪") 展示了
    LazyLLM 各阶段的这些累积提示标记使用数据。
- en: Our analysis supports the hypothesis that many tokens are never selected by
    the model (even though theoretically the model could use all tokens in the prompt).
    Since this model retains accuracy on the task(s), we can conclude that the model
    effectively drops the tokens which do not affect the output quality.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析支持这样的假设：许多标记从未被模型选择（尽管理论上模型可以使用提示中的所有标记）。由于该模型在任务上保持了准确性，我们可以得出结论：模型有效地丢弃了那些不影响输出质量的标记。
- en: 6 Conclusion
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we proposed a novel *LazyLLM* technique for efficient LLM inference,
    in particular under long context scenarios. *LazyLLM* selectively computes the
    KV for tokens important for the next token prediction and “lazily” defers the
    computation of remaining tokens to later steps, when they become relevant. We
    carefully examine *LazyLLM* on various tasks, where we observed the proposed method
    effectively reduces *TTFT* with negligible performance loss. It is worth noting
    that our method can be seamlessly integrated with existing transformer-based LLMs
    to improve their inference speed without requiring any fine-tuning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的*LazyLLM*技术，以提高 LLM 推理的效率，特别是在长上下文场景下。*LazyLLM* 选择性地计算对下一个标记预测重要的
    KV，并“懒惰”地将剩余标记的计算推迟到后续步骤，当它们变得相关时再进行。我们在各种任务上仔细检查了*LazyLLM*，观察到该方法有效减少了*TTFT*，且性能损失微乎其微。值得注意的是，我们的方法可以无缝集成到现有的基于
    Transformer 的 LLM 中，以提高推理速度，而无需任何微调。
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adnan et al. (2024) Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J
    Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction
    through key tokens selection for efficient generative inference. *arXiv preprint
    arXiv:2403.09054*, 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adnan 等（2024）穆罕默德·阿德南、阿基尔·阿伦库马尔、高拉夫·贾因、普拉尚特·J·奈尔、伊利亚·索洛维奇和普鲁索瑟姆·卡马斯。Keyformer：通过关键标记选择减少
    KV 缓存以提高生成推理效率。*arXiv 预印本 arXiv:2403.09054*，2024。
- en: 'Aminabadi et al. (2022) Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad
    Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang,
    Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer
    models at unprecedented scale. In *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*, pp.  1–15\. IEEE, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aminabadi 等（2022）雷扎·雅兹达尼·阿敏阿巴迪、萨米扬·拉杰班达里、安马尔·艾哈迈德·阿万、程丽、杜丽、埃尔顿·郑、奥拉图恩吉·鲁瓦斯、沙登·史密斯、敏佳·张、杰夫·拉斯利
    等。Deepspeed-inference：在前所未有的规模下实现 Transformer 模型的高效推理。在*SC22：国际高性能计算、网络、存储与分析会议*，第
    1–15 页。IEEE，2022。
- en: Anagnostidis et al. (2024) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio,
    Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for
    efficient and interpretable autoregressive transformers. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anagnostidis 等（2024）索提里斯·安纳格诺斯蒂迪斯、达里奥·帕沃洛、卢卡·比吉奥、洛伦佐·诺奇、奥雷利安·卢奇和托马斯·霍夫曼。动态上下文修剪用于高效且可解释的自回归
    Transformer。*神经信息处理系统进展*，第 36 期，2024。
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等. Longbench: 一个用于长文本理解的双语多任务基准。*arXiv
    预印本 arXiv:2308.14508*，2023年。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, 和 Arman Cohan. Longformer:
    长文档变压器。*arXiv 预印本 arXiv:2004.05150*，2020年。'
- en: 'Bhendawade et al. (2024) Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry
    Mason, Mohammad Rastegari, and Mahyar Najibi. Speculative streaming: Fast llm
    inference without auxiliary models. *arXiv preprint arXiv:2402.11131*, 2024.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhendawade et al. (2024) Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry
    Mason, Mohammad Rastegari, 和 Mahyar Najibi. Speculative streaming: 无需辅助模型的快速 llm
    推理。*arXiv 预印本 arXiv:2402.11131*，2024年。'
- en: 'Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason
    D Lee, Deming Chen, 和 Tri Dao. Medusa: 具有多个解码头的简单 llm 推理加速框架。*arXiv 预印本 arXiv:2401.10774*，2024年。'
- en: 'Chen et al. (2023) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. *arXiv preprint arXiv:2309.12307*, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, 和 Jiaya Jia. Longlora: 高效微调长文本大语言模型。*arXiv 预印本 arXiv:2309.12307*，2023年。'
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher
    Ré. Flashattention: 快速且内存高效的确切注意力与 IO 感知。*神经信息处理系统进展*，35:16344–16359，2022年。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert: 用于语言理解的深度双向变压器预训练。*arXiv 预印本 arXiv:1810.04805*，2018年。'
- en: 'Elhoushi et al. (2024) Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
    Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal,
    Ahmed Roman, et al. Layer skip: Enabling early exit inference and self-speculative
    decoding. *arXiv preprint arXiv:2404.16710*, 2024.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elhoushi et al. (2024) Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
    Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal,
    Ahmed Roman, 等. Layer skip: 启用早期退出推理和自我推测解码。*arXiv 预印本 arXiv:2404.16710*，2024年。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 用于生成预训练变压器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。'
- en: 'He et al. (2021) Xuanli He, Iman Keivanloo, Yi Xu, Xiang He, Belinda Zeng,
    Santosh Rajagopalan, and Trishul Chilimbi. Magic pyramid: Accelerating inference
    with early exiting and token pruning. *arXiv preprint arXiv:2111.00230*, 2021.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2021) Xuanli He, Iman Keivanloo, Yi Xu, Xiang He, Belinda Zeng,
    Santosh Rajagopalan, 和 Trishul Chilimbi. Magic pyramid: 通过早期退出和标记修剪加速推理。*arXiv
    预印本 arXiv:2111.00230*，2021年。'
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large
    language models. *arXiv preprint arXiv:2310.05736*, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    和 Lili Qiu. Llmlingua: 压缩提示以加速大语言模型的推理。*arXiv 预印本 arXiv:2310.05736*，2023年。'
- en: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pp.  784–794, 2022.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2022) Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk
    Kwon, Joseph Hassoun, 和 Kurt Keutzer. 针对变压器的学习型标记修剪。在 *第28届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第784–794页，2022年。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo
    Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W
    Mahoney, et al. Full stack optimization of transformer inference: a survey. *arXiv
    preprint arXiv:2302.14017*, 2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang,
    Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney,
    等。变换器推理的全栈优化：综述。*arXiv 预印本 arXiv:2302.14017*, 2023。
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya。Reformer:
    高效变换器。*arXiv 预印本 arXiv:2001.04451*, 2020。'
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pp.  19274–19286\. PMLR, 2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, 和 Yossi Matias。通过推测解码实现变换器的快速推理。在
    *国际机器学习大会*, pp. 19274–19286\. PMLR, 2023。
- en: Li et al. (2023) Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing
    context to enhance inference efficiency of large language models. *arXiv preprint
    arXiv:2310.06201*, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yucheng Li, Bo Dong, Chenghua Lin, 和 Frank Guerin。压缩上下文以提高大型语言模型的推理效率。*arXiv
    预印本 arXiv:2310.06201*, 2023。
- en: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm
    knows what you are looking for before generation. *arXiv preprint arXiv:2404.14469*,
    2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, 和 Deming Chen。Snapkv: Llm 在生成之前知道你在寻找什么。*arXiv
    预印本 arXiv:2404.14469*, 2024。'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang。Llm-pruner: 关于大型语言模型的结构剪枝。*神经信息处理系统进展*,
    36:21702–21720, 2023。'
- en: 'Merth et al. (2024) Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar
    Najibi. Superposition prompting: Improving and accelerating retrieval-augmented
    generation. 2024. URL [https://api.semanticscholar.org/CorpusID:269033436](https://api.semanticscholar.org/CorpusID:269033436).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merth et al. (2024) Thomas Merth, Qichen Fu, Mohammad Rastegari, 和 Mahyar Najibi。超位置提示：改进和加速检索增强生成。2024。网址
    [https://api.semanticscholar.org/CorpusID:269033436](https://api.semanticscholar.org/CorpusID:269033436)。
- en: 'Nawrot et al. (2024) Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David
    Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for
    accelerated inference. *arXiv preprint arXiv:2403.09636*, 2024.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nawrot et al. (2024) Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David
    Tarjan, 和 Edoardo M Ponti。动态内存压缩：为加速推理改造 llms。*arXiv 预印本 arXiv:2403.09636*, 2024。
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. Xgen-7b
    technical report. *arXiv preprint arXiv:2309.03450*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, 等。Xgen-7b
    技术报告。*arXiv 预印本 arXiv:2309.03450*, 2023。
- en: 'NVIDIA (2024) NVIDIA. NVIDIA L40S: Unparalleled AI and graphics performance
    for the data center. [https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413),
    2024. [Online; accessed 31-May-2024].'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2024) NVIDIA。NVIDIA L40S: 数据中心无与伦比的 AI 和图形性能。 [https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413),
    2024。 [在线; 访问日期 2024年5月31日]。'
- en: Pope et al. (2023) Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. *Proceedings of Machine Learning and Systems*,
    5, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope et al. (2023) Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, 和 Jeff Dean。高效扩展变换器推理。*机器学习与系统会议论文集*,
    5, 2023。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter。一个简单有效的剪枝方法用于大型语言模型。*arXiv
    预印本 arXiv:2306.11695*, 2023。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等。Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*, 2023。'
- en: 'Xu et al. (2023) Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang,
    Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving
    accuracy-efficiency trade-off of llm inference with transferable prompt. *arXiv
    preprint arXiv:2305.11186*, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2023）徐兆卓，刘子睿，陈贝迪，唐宇欣，王珏，周凯雄，胡霞，和安书玛利·施里瓦斯塔瓦。压缩，然后提示：通过可迁移提示提高大型语言模型推断的准确性与效率的权衡。*arXiv
    预印本 arXiv:2305.11186*，2023年。
- en: 'Yao et al. (2022) Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia
    Zhang, Cheng Li, and Yuxiong He. Random-ltd: Random and layerwise token dropping
    brings efficient training for large-scale transformers. *arXiv preprint arXiv:2211.11586*,
    2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2022）赵伟，吴晓霞，李从龙，康纳·霍尔姆斯，张敏佳，李诚，和何宇雄。Random-ltd: 随机和逐层令牌丢弃带来大规模变换器的高效训练。*arXiv
    预印本 arXiv:2211.11586*，2022年。'
- en: 'Zhang et al. (2024) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.
    H2o: Heavy-hitter oracle for efficient generative inference of large language
    models. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2024）张振宇，盛颖，周天意，陈天龙，郑炼敏，蔡瑞斯，宋赵，田远东，克里斯托弗·瑞，克拉克·巴雷特等。H2o: 用于大规模语言模型高效生成推断的重型预言机。*神经信息处理系统进展*，36，2024年。'
