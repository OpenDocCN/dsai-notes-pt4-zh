- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely
    Long Sequences with Training-Free Memory'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfLLM：揭示LLMs处理极长序列的内在能力，采用无训练记忆的方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04617](https://ar5iv.labs.arxiv.org/html/2402.04617)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04617](https://ar5iv.labs.arxiv.org/html/2402.04617)
- en: Chaojun Xiao    Pengle Zhang    Xu Han    Guangxuan Xiao    Yankai Lin    Zhengyan
    Zhang    Zhiyuan Liu    Song Han    Maosong Sun
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao Chaojun    Zhang Pengle    Han Xu    Xiao Guangxuan    Lin Yankai    Zhang
    Zhengyan    Liu Zhiyuan    Han Song    Sun Maosong
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have emerged as a cornerstone in real-world applications
    with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs,
    pre-trained on sequences with restricted maximum length, cannot generalize to
    longer sequences due to the out-of-domain and distraction issues. To alleviate
    these issues, existing efforts employ sliding attention windows and discard distant
    tokens to achieve the processing of extremely long sequences. Unfortunately, these
    approaches inevitably fail to capture long-distance dependencies within sequences
    to deeply understand semantics. This paper introduces a training-free memory-based
    method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long
    sequences. Specifically, InfLLM stores distant contexts into additional memory
    units and employs an efficient mechanism to lookup token-relevant units for attention
    computation. Thereby, InfLLM allows LLMs to efficiently process long sequences
    while maintaining the ability to capture long-distance dependencies. Without any
    training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens
    to achieve superior performance than competitive baselines continually training
    these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K,
    InfLLM still effectively captures long-distance dependencies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已成为处理具有长时间流输入的实际应用中的基石，例如LLM驱动的代理。然而，现有的LLMs由于在具有限制最大长度的序列上进行预训练，无法推广到更长的序列，因为存在领域外和干扰问题。为了缓解这些问题，现有的努力采用滑动注意窗口并丢弃远处的标记以实现极长序列的处理。不幸的是，这些方法不可避免地无法捕捉序列中的长距离依赖关系，从而无法深入理解语义。本文介绍了一种无训练的基于记忆的方法——InfLLM，以揭示LLMs处理流式长序列的内在能力。具体来说，InfLLM将远程上下文存储到额外的记忆单元中，并采用高效机制查找与标记相关的单元以进行注意力计算。因此，InfLLM使得LLMs能够高效处理长序列，同时保持捕捉长距离依赖的能力。在没有任何训练的情况下，InfLLM使得在数千个标记的序列上进行预训练的LLMs表现优于不断在长序列上训练这些LLMs的竞争基线。即使序列长度扩展到$1,024$K，InfLLM仍然能够有效地捕捉长距离依赖。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently, large language models (LLMs) have achieved profound accomplishments
    in various tasks (Brown et al., [2020](#bib.bib6); Bommasani et al., [2021](#bib.bib5);
    Han et al., [2021](#bib.bib22); Touvron et al., [2023a](#bib.bib51)). Their ability
    to follow complex instructions shed light on the realization of artificial general
    intelligence (OpenAI, [2023](#bib.bib37); Ouyang et al., [2022](#bib.bib38)).
    With the blooming of LLM-driven applications, such as embodied robotics (Driess
    et al., [2023](#bib.bib16); Liang et al., [2023](#bib.bib33)) and agent construction (Park
    et al., [2023](#bib.bib39); Qian et al., [2023](#bib.bib42)), enhancing the capability
    of LLMs to process streaming long sequences become increasingly crucial. For instance,
    LLM-driven agents are required to process information continuously received from
    external environments based on all historical memories, necessitating a robust
    capability for handling long sequences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）在各种任务中取得了深远的成就（Brown et al., [2020](#bib.bib6)；Bommasani et al.,
    [2021](#bib.bib5)；Han et al., [2021](#bib.bib22)；Touvron et al., [2023a](#bib.bib51)）。它们跟随复杂指令的能力为实现人工通用智能提供了新的视角（OpenAI,
    [2023](#bib.bib37)；Ouyang et al., [2022](#bib.bib38)）。随着以LLM为驱动的应用程序的兴起，如具身机器人（Driess
    et al., [2023](#bib.bib16)；Liang et al., [2023](#bib.bib33)）和代理构建（Park et al.,
    [2023](#bib.bib39)；Qian et al., [2023](#bib.bib42)），提高LLM处理流式长序列的能力变得越来越重要。例如，基于LLM的代理需要根据所有历史记忆处理来自外部环境的持续信息，因此需要强大的处理长序列的能力。
- en: Due to limitations caused by unseen lengthy inputs (Han et al., [2023](#bib.bib21))
    and distracting noisy contexts (Liu et al., [2023](#bib.bib34); Tworkowski et al.,
    [2023](#bib.bib53)), most LLMs, pre-trained on sequences with a few thousand tokens,
    cannot generalize on longer sequences and achieve unsatisfactory performance (Press
    et al., [2022](#bib.bib41); Zhao et al., [2023](#bib.bib64)). Common solutions
    usually involve continually training LLMs on longer sequences but further result
    in substantial costs (Xiong et al., [2023](#bib.bib58); Li et al., [2023](#bib.bib32)).
    Improving the length generalizability of LLMs without further training thus receives
    extensive attention, trying to make LLMs trained on short sequences directly applicable
    to long sequences. To this end, some recent works employ sliding windows to discard
    all distant contexts, ensuring that the sequence length and input noises processed
    at each step do not exceed the maximum capacity of LLMs (Xiao et al., [2023](#bib.bib57);
    Han et al., [2023](#bib.bib21)). Thus, the sliding window mechanism enables effective
    reading of long sequences but fails to understand them, since it cannot capture
    the necessary long-distance dependencies among sequence tokens. In summary, existing
    endeavors are still hard to effectively improve the length generalizability of
    LLMs at a low cost.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于未见长输入（Han et al., [2023](#bib.bib21)）和干扰性噪声上下文（Liu et al., [2023](#bib.bib34);
    Tworkowski et al., [2023](#bib.bib53)）造成的限制，大多数在几千个标记序列上预训练的LLMs无法在更长的序列上进行泛化，并且表现不佳（Press
    et al., [2022](#bib.bib41); Zhao et al., [2023](#bib.bib64)）。常见的解决方案通常涉及对LLMs进行持续训练以处理更长的序列，但进一步导致了巨大的成本（Xiong
    et al., [2023](#bib.bib58); Li et al., [2023](#bib.bib32)）。因此，在不进一步训练的情况下提高LLMs的长度泛化能力受到了广泛关注，试图使在短序列上训练的LLMs能够直接适用于长序列。为此，一些最新的工作采用滑动窗口来丢弃所有远程上下文，确保每一步处理的序列长度和输入噪声不超过LLMs的最大容量（Xiao
    et al., [2023](#bib.bib57); Han et al., [2023](#bib.bib21)）。因此，滑动窗口机制能够有效地读取长序列，但无法理解它们，因为它无法捕捉序列标记之间所需的长距离依赖。总之，现有的努力仍然很难以低成本有效地提高LLMs的长度泛化能力。
- en: In this paper, we propose a training-free memory-based approach, named InfLLM,
    for streamingly processing extremely long sequences. InfLLM aims to stimulate
    the intrinsic capacity of LLMs for capturing long-distance dependencies among
    massive contexts with limited computational costs. Considering the sparsity of
    attention score matrices, processing each token typically requires only a small
    portion of its context (Zhang et al., [2023b](#bib.bib62)). We construct an external
    memory containing distant context information. Only some relevant information
    from the memory is selected for each computation step and other irrelevant noises
    are ignored. Owing to this, LLMs can understand whole long sequences with a finite
    window size and avoid noisy context inputs. However, the vast amount of noisy
    past contexts in long sequences poses significant challenges to the effective
    location of relevant units and the efficiency of memory lookup.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种无需训练的基于记忆的方法，命名为InfLLM，用于流式处理极长的序列。InfLLM旨在激发LLMs在有限计算成本下捕捉大规模上下文中的长距离依赖的内在能力。考虑到注意力得分矩阵的稀疏性，处理每个标记通常只需要其上下文中的一小部分（Zhang
    et al., [2023b](#bib.bib62)）。我们构建了一个包含远程上下文信息的外部记忆。每次计算步骤中只选择来自记忆的相关信息，其余无关的噪声被忽略。因此，LLMs可以在有限的窗口大小下理解整个长序列，并避免噪声上下文输入。然而，长序列中的大量噪声过往上下文对相关单元的有效定位和记忆查找效率带来了显著挑战。
- en: To address these challenges, we design a block-level context memory. (1) For
    effective localization of relevant units, the coherent semantics of each block
    can more effectively fulfill the requirements for long-sequence reasoning than
    fragmented tokens. Moreover, we select the semantically most significant tokens
    from each unit, namely the tokens that receive the highest attention score, as
    the unit representation. This approach helps avoid the interference of unimportant
    tokens in the relevance calculation. (2) For efficient memory lookup, the block-level
    memory unit eliminates the need for per-token, per-head relevance calculations,
    reducing the computational complexity. Additionally, block-level units ensure
    contiguous memory access and reduce memory loading costs. Benefiting from this,
    we design an efficient offloading mechanism tailored for context memory. Considering
    the infrequent usage of most units, InfLLM offloads all units on CPU memory and
    dynamically retains the frequently used units on GPU memory, significantly reducing
    the memory usage. InfLLM do not involve any additional training, and can be directly
    applied to any LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，我们设计了一个块级上下文记忆。（1）为了有效定位相关单元，每个块的连贯语义比碎片化的标记更能有效满足长序列推理的要求。此外，我们从每个单元中选择语义上最重要的标记，即获得最高注意力分数的标记，作为单元表示。这种方法有助于避免不重要标记在相关性计算中的干扰。（2）为了高效的内存查找，块级记忆单元消除了对每个标记、每个头的相关性计算的需求，从而降低了计算复杂性。此外，块级单元确保了连续的内存访问，并减少了内存加载成本。得益于此，我们设计了一种针对上下文记忆的高效卸载机制。考虑到大多数单元的使用频率较低，InfLLM将所有单元卸载到CPU内存中，并动态保留频繁使用的单元在GPU内存中，从而显著降低了内存使用。InfLLM不涉及任何额外的训练，可以直接应用于任何LLMs。
- en: To evaluate the effectiveness of InfLLM, we employ Vicuna-7B-v1.5 (Chiang et al.,
    [2023](#bib.bib9)) and Mistral-7B-inst-v0.2 (Jiang et al., [2023](#bib.bib25))
    as base models, which are pre-trained on the sequences containing no more than
    $4$-Bench exceeds $100$K tokens, and InfLLM can still effectively capture long-distance
    dependencies, demonstrating the potential of InfLLM in scenarios involving long
    streaming inputs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估InfLLM的有效性，我们采用了Vicuna-7B-v1.5（Chiang等，[2023](#bib.bib9)）和Mistral-7B-inst-v0.2（Jiang等，[2023](#bib.bib25)）作为基础模型，这些模型在包含不超过$4$-Bench超过$100$K标记的序列上进行了预训练，而InfLLM仍能有效捕捉长距离依赖，展示了InfLLM在处理长流输入场景中的潜力。
- en: 2 Related Work
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Enabling LLMs to process long sequences has been extensively studied (Dong
    et al., [2023](#bib.bib15); Tay et al., [2023](#bib.bib50); Huang et al., [2023](#bib.bib24))
    and can generally be categorized into two main approaches: context length extrapolation
    and efficient context computation. The former aims to enable LLMs trained on short
    sequences to process much longer sequences. The latter focuses on enhancing the
    computational efficiency of attention layers, allowing LLMs to utilize limited
    resources to process longer sequences. Although the focus of this paper is context
    length extrapolation, we also detailedly introduce efficient transformers. We
    also present the relevant works for memory-based models, the source of InfLLM.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使LLMs能够处理长序列已被广泛研究（Dong等，[2023](#bib.bib15)；Tay等，[2023](#bib.bib50)；Huang等，[2023](#bib.bib24)），通常可以分为两种主要方法：上下文长度外推和高效上下文计算。前者旨在使训练于短序列的LLMs能够处理更长的序列。后者则侧重于提高注意力层的计算效率，使LLMs能够利用有限的资源处理更长的序列。尽管本文的重点是上下文长度外推，但我们也详细介绍了高效变压器。我们还介绍了与基于记忆的模型相关的工作，这是InfLLM的来源。
- en: Context Length Extrapolation. Due to the high computational and memory requirements,
    the training of LLMs is often restricted to short sequences. Directly applying
    LLMs to long sequences will suffer from out-of-domain and distraction challenges
    caused by lengthy and noisy inputs (Han et al., [2023](#bib.bib21); Tworkowski
    et al., [2023](#bib.bib53)). Consequently, context length extrapolation has garnered
    attention as a method to extend the sequence length for LLMs without incurring
    additional training. The earliest approaches involve designing new relative positional
    encoding mechanisms during pre-training (Press et al., [2022](#bib.bib41); Sun
    et al., [2023](#bib.bib49)). Subsequent studies mainly focus on the widely-used
    rotary position embedding (RoPE) (Su et al., [2021](#bib.bib47)), and propose
    to achieve length extension by interpolating positions to introduce non-integer
    positions (Chen et al., [2023b](#bib.bib8); Peng et al., [2023](#bib.bib40); Jin
    et al., [2024](#bib.bib26); Chen et al., [2023a](#bib.bib7)). These works can
    alleviate the out-of-domain issue from the unseen length, but can not alleviate
    the distraction challenge of noisy context. To address this, Xiao et al. ([2023](#bib.bib57))
    and Han et al. ([2023](#bib.bib21)) employ the sliding window attention mechanism
    and directly discard all distant contexts to streamingly read extremely long sequences.
    However as these models overlook information from distant tokens, they can not
    capture the long-distance dependencies for long-text understanding. InfLLM constructs
    an efficient context memory to provide LLMs with relevant context information,
    which enables LLMs to effectively read and understand extremely long sequences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文长度外推。由于高计算和内存要求，LLM 的训练通常限制在短序列上。直接将 LLM 应用到长序列上会面临由冗长和嘈杂输入引起的域外和干扰挑战（Han
    等，[2023](#bib.bib21); Tworkowski 等，[2023](#bib.bib53)）。因此，上下文长度外推作为一种在不增加额外训练的情况下扩展
    LLM 序列长度的方法引起了关注。最早的方法涉及在预训练期间设计新的相对位置编码机制（Press 等，[2022](#bib.bib41); Sun 等，[2023](#bib.bib49)）。随后，研究主要集中在广泛使用的旋转位置嵌入
    (RoPE)（Su 等，[2021](#bib.bib47)），并提出通过插值位置来引入非整数位置以实现长度扩展（Chen 等，[2023b](#bib.bib8);
    Peng 等，[2023](#bib.bib40); Jin 等，[2024](#bib.bib26); Chen 等，[2023a](#bib.bib7)）。这些工作可以缓解来自未知长度的域外问题，但不能缓解嘈杂上下文的干扰挑战。为了解决这个问题，Xiao
    等（[2023](#bib.bib57)）和 Han 等（[2023](#bib.bib21)）采用了滑动窗口注意力机制，直接丢弃所有远程上下文，以流式读取极长的序列。然而，由于这些模型忽略了远程标记的信息，它们不能捕捉到长文本理解的长距离依赖。InfLLM
    构建了一个高效的上下文记忆，向 LLM 提供相关的上下文信息，使 LLM 能够有效地读取和理解极长的序列。
- en: '![Refer to caption](img/8626e714e72295b6fb769977b221ff88.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8626e714e72295b6fb769977b221ff88.png)'
- en: 'Figure 1: The illustration of InfLLM. Here, the current tokens refer to tokens
    that need to be encoded in the current computation step. The past key-value hidden
    states can be divided into the initial tokens, evicted tokens, and local tokens,
    arranged the furthest to the nearest relative to the current tokens. For each
    computation step, the past context window consists of the initial tokens, relevant
    memory units, and local tokens.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：InfLLM 的示意图。这里，当前标记指的是在当前计算步骤中需要编码的标记。过去的键值隐藏状态可以分为初始标记、驱逐标记和本地标记，相对于当前标记，从最远到最近排列。在每个计算步骤中，过去的上下文窗口由初始标记、相关的记忆单元和本地标记组成。
- en: Efficient Context Computation. The quadratic computational complexity of the
    attention layers is a primary factor limiting the lengthy sequence-processing
    capabilities of LLMs. Thus, numerous scholars have endeavored to design efficient
    attention mechanisms, including the utilization of sparse attention (Zaheer et al.,
    [2020](#bib.bib60); Beltagy et al., [2020](#bib.bib3); Child et al., [2019](#bib.bib10);
    Ainslie et al., [2020](#bib.bib1); Zhao et al., [2019](#bib.bib63)), approximating
    attention computations using kernel functions (Kitaev et al., [2020](#bib.bib29);
    Wang et al., [2020](#bib.bib54); Katharopoulos et al., [2020](#bib.bib27)), and
    replacing the attention layer with linear-complexity state-space models (Gu et al.,
    [2022](#bib.bib20); Gu & Dao, [2023](#bib.bib19)). These approaches necessitate
    a modification in the model architecture, requiring retraining the models. Simultaneously,
    many researchers have approached this from an infrastructural perspective, optimizing
    the memory footprint of attention computations to reduce the computational resource
    demands of the model (Dao et al., [2022](#bib.bib14); Dao, [2023](#bib.bib13);
    Hong et al., [2023](#bib.bib23); Shazeer, [2019](#bib.bib45); Kwon et al., [2023](#bib.bib30)).
    These works are parallel to ours, and can be directly combined to further accelerate
    LLM inference.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的上下文计算。注意力层的二次计算复杂度是限制LLMs处理长序列能力的主要因素。因此，许多学者努力设计高效的注意力机制，包括利用稀疏注意力（Zaheer
    et al., [2020](#bib.bib60); Beltagy et al., [2020](#bib.bib3); Child et al., [2019](#bib.bib10);
    Ainslie et al., [2020](#bib.bib1); Zhao et al., [2019](#bib.bib63)），使用核函数逼近注意力计算（Kitaev
    et al., [2020](#bib.bib29); Wang et al., [2020](#bib.bib54); Katharopoulos et
    al., [2020](#bib.bib27)），以及用线性复杂度的状态空间模型替代注意力层（Gu et al., [2022](#bib.bib20);
    Gu & Dao, [2023](#bib.bib19)）。这些方法需要修改模型架构，要求对模型进行重新训练。同时，许多研究者从基础设施的角度入手，优化注意力计算的内存占用，以减少模型的计算资源需求（Dao
    et al., [2022](#bib.bib14); Dao, [2023](#bib.bib13); Hong et al., [2023](#bib.bib23);
    Shazeer, [2019](#bib.bib45); Kwon et al., [2023](#bib.bib30)）。这些工作与我们的研究是平行的，可以直接结合以进一步加速LLM推理。
- en: Memory-based Models. Memory networks have been studied for decades, which are
    proven effective in providing models with additional knowledge and information
    storage capabilities (Graves et al., [2014](#bib.bib18); Weston et al., [2015](#bib.bib55);
    Sukhbaatar et al., [2015](#bib.bib48); Miller et al., [2016](#bib.bib35)). With
    the success of pre-trained models, memory layers have also been gradually applied
    in the training processes of recurrent transformer layers, enabling models to
    process long sequences recursively (Dai et al., [2019](#bib.bib11); Rae et al.,
    [2020](#bib.bib43); Khandelwal et al., [2020](#bib.bib28); Wu et al., [2022](#bib.bib56);
    Bertsch et al., [2023](#bib.bib4)). These works split sequences into segments,
    encoding each segment individually, and use memory to store context information
    from preceding segments. While these approaches are similar in concept to InfLLM,
    they involve modifications to the model architecture and should be applied during
    the pre-training phase. In contrast, we aim to explore the inherent characteristics
    of LLMs, and propose a training-free memory module for long-text understanding.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于记忆的模型。记忆网络已经研究了几十年，证明在为模型提供额外的知识和信息存储能力方面有效（Graves et al., [2014](#bib.bib18);
    Weston et al., [2015](#bib.bib55); Sukhbaatar et al., [2015](#bib.bib48); Miller
    et al., [2016](#bib.bib35)）。随着预训练模型的成功，记忆层也逐渐应用于递归变压器层的训练过程中，使模型能够递归地处理长序列（Dai
    et al., [2019](#bib.bib11); Rae et al., [2020](#bib.bib43); Khandelwal et al.,
    [2020](#bib.bib28); Wu et al., [2022](#bib.bib56); Bertsch et al., [2023](#bib.bib4)）。这些工作将序列拆分成段，对每个段进行单独编码，并使用记忆存储来自前面段的上下文信息。尽管这些方法在概念上类似于InfLLM，但它们涉及到对模型架构的修改，并应在预训练阶段应用。相比之下，我们旨在探索LLMs的内在特性，提出一个用于长文本理解的无训练记忆模块。
- en: 3 Methodology
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'As shown in Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ InfLLM: Unveiling
    the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with
    Training-Free Memory"), InfLLM builds a training-free context memory to efficiently
    provide relevant context information, which endows sliding window attention with
    the ability to capture long-distance dependencies.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ InfLLM: Unveiling the Intrinsic Capacity
    of LLMs for Understanding Extremely Long Sequences with Training-Free Memory")所示，InfLLM构建了一个无训练的上下文记忆，以高效提供相关的上下文信息，这赋予了滑动窗口注意力捕捉长距离依赖的能力。'
- en: 3.1 Overall Framework
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 总体框架
- en: The main restrictions for extending the context window of LLMs come from the
    out-of-domain and distraction issues caused by the lengthy and noisy context.
    To address the challenges, following previous works (Xiao et al., [2023](#bib.bib57);
    Han et al., [2023](#bib.bib21)), we adopt the sliding window attention mechanism,
    which only considers limited tokens for each step. Additionally, we propose to
    build an extra context memory module to provide relevant context information to
    capture long-distance dependencies. Specifically, we denote the long input sequence
    as $s=\{t_{i}\}_{i=1}^{l}$ and current tokens hidden vectors $\mathbf{X}=\{\mathbf{t}_{i+l_{P}}\}_{i=1}^{l_{X}}$
    equals one.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展LLM的上下文窗口的主要限制来自于由冗长和嘈杂的上下文引起的领域外和干扰问题。为了解决这些挑战，基于之前的工作（Xiao 等，[2023](#bib.bib57)；Han
    等，[2023](#bib.bib21)），我们采用滑动窗口注意机制，该机制仅考虑每一步的有限令牌。此外，我们建议构建额外的上下文记忆模块，以提供相关的上下文信息以捕捉远程依赖关系。具体而言，我们将长输入序列表示为
    $s=\{t_{i}\}_{i=1}^{l}$，并且当前令牌隐藏向量 $\mathbf{X}=\{\mathbf{t}_{i+l_{P}}\}_{i=1}^{l_{X}}$
    等于一。
- en: 'According to the distances from current tokens, we can divide $\mathbf{P}$,
    arranged from the furthest to the nearest relative to the current tokens. Here,
    $l_{P}$, are stored in the context memory, consisting of multiple memory units.
    For each step, InfLLM concatenates the initial tokens, relevant memories units
    from context memory, and local tokens to form the current key-value cache, $\mathbf{C}=\text{Concat}(\mathbf{I},f(\mathbf{X},\mathbf{E}),\mathbf{L})$
    refers to the lookup operation of context memory. The attention output is calculated
    as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前令牌的距离，我们可以将 $\mathbf{P}$ 划分为从最远到最近相对于当前令牌排列的部分。这里，$l_{P}$ 存储在上下文记忆中，由多个记忆单元组成。对于每一步，InfLLM
    将初始令牌、上下文记忆中的相关记忆单元和局部令牌连接形成当前的键值缓存，$\mathbf{C}=\text{Concat}(\mathbf{I},f(\mathbf{X},\mathbf{E}),\mathbf{L})$
    指的是上下文记忆的查找操作。注意力输出计算为：
- en: '|  | $\small\mathbf{O}=\text{Attn}\left[\mathbf{Q}\mathbf{X},\text{Concat}(\mathbf{C}_{k},\mathbf{K}\mathbf{X}),\text{Concat}(\mathbf{C}_{v},\mathbf{V}\mathbf{X})\right].$
    |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{O}=\text{Attn}\left[\mathbf{Q}\mathbf{X},\text{Concat}(\mathbf{C}_{k},\mathbf{K}\mathbf{X}),\text{Concat}(\mathbf{C}_{v},\mathbf{V}\mathbf{X})\right].$
    |  |'
- en: Here, $\mathbf{Q}$ and $\mathbf{C}_{v}$ always returns empty sets, InfLLM is
    degenerated into LM-Infinite (Han et al., [2023](#bib.bib21)) and Streaming-LLM (Xiao
    et al., [2023](#bib.bib57)), which directly discards distant context.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathbf{Q}$ 和 $\mathbf{C}_{v}$ 总是返回空集合，InfLLM 降级为 LM-Infinite （Han 等，[2023](#bib.bib21)）和
    Streaming-LLM （Xiao 等，[2023](#bib.bib57)），直接丢弃远程上下文。
- en: 3.2 Context Memory
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 上下文记忆
- en: Previous findings indicate that the attention score matrices of LLMs are sparse,
    and we can generate the same outputs with only a small portion of key-value vectors
    preserved (Zhang et al., [2023b](#bib.bib62)). Inspired by this, we design a context
    memory to efficiently look up relevant contexts from large-scale evicted tokens
    and ignore irrelevant ones to save computational costs. The most intuitive way
    is to construct a memory consisting of token-level memory units for every past
    key-value hidden states, and every attention head separately, which would result
    in massive memory units and unacceptable computation and non-contiguous memory
    access costs. Thus, considering the local semantic coherence of long sequences,
    we split the past key-value hidden states into blocks, each serving as a memory
    unit, and conduct memory lookup at the block level to reduce the costs while preserving
    the performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究发现，LLM 的注意力评分矩阵是稀疏的，我们可以仅通过保留少量键值向量生成相同的输出（Zhang 等，[2023b](#bib.bib62)）。受到此启发，我们设计了一种上下文记忆来高效地从大规模的被驱逐令牌中查找相关上下文，并忽略无关的上下文以节省计算成本。最直观的方法是为每个过去的键值隐藏状态以及每个注意力头分别构建由令牌级记忆单元组成的记忆，这将导致大量的记忆单元以及不可接受的计算和不连续的内存访问成本。因此，考虑到长序列的局部语义连贯性，我们将过去的键值隐藏状态拆分为块，每块作为一个记忆单元，并在块级别进行记忆查找，以降低成本同时保持性能。
- en: In this subsection, we will introduce the details of the block-level memory
    units. Then we present the method to assign positional embeddings for selected
    relevant memory units and cache management for the context memory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍块级记忆单元的详细信息。然后，我们介绍为选定的相关记忆单元分配位置嵌入的方法以及上下文记忆的缓存管理。
- en: 'Block-Level Memory Units. Block-level memory units can save computation costs
    compared to token-level ones. It also poses new challenges for unit representations,
    which are supposed to contain the semantics of the entire unit for effective relevance
    score computation and be memory-efficient for context length scalability. Traditional
    methods usually involve training an additional encoder to project a given unit
    into a low-dimension vector. Inspired by the token redundancy in hidden states (Goyal
    et al., [2020](#bib.bib17); Dai et al., [2020](#bib.bib12)), we select several
    representative tokens from the entail blocks as the unit representation. Specifically,
    for the $m$-th token, we define the representative score as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 块级记忆单元。与标记级记忆单元相比，块级记忆单元可以节省计算成本。它还带来了单位表示的新挑战，这些表示应包含整个单位的语义，以便有效计算相关性分数，并在上下文长度扩展方面具有内存效率。传统方法通常涉及训练额外的编码器，将给定的单位投影到低维向量中。受到隐藏状态中的标记冗余的启发（Goyal等，[2020](#bib.bib17)；Dai等，[2020](#bib.bib12)），我们从蕴含块中选择几个代表性标记作为单位表示。具体来说，对于第
    $m$ 个标记，我们定义代表性分数如下：
- en: '|  | $r_{m}=\frac{1}{l_{L}}\sum_{j=1}^{l_{L}}\mathbf{q}_{m+j}\cdot\mathbf{k}_{m},$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{m}=\frac{1}{l_{L}}\sum_{j=1}^{l_{L}}\mathbf{q}_{m+j}\cdot\mathbf{k}_{m},$
    |  | (1) |'
- en: where $\mathbf{q}_{m+j}$-th token. Intuitively, $r_{m}$-th token in its corresponding
    local window, indicating the extent of its influence on other tokens within the
    local window. The computation of representative scores requires no additional
    parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{q}_{m+j}$-th 标记。直观上，$r_{m}$-th 标记在其对应的本地窗口中，表示其对本地窗口内其他标记的影响程度。代表性分数的计算不需要额外的参数。
- en: Formally, given the evicted tokens, $\mathbf{E}$ is a small positive integer.
    Let us denote a memory unit as $\mathbf{B}=\{(\mathbf{k}_{j}^{B},\mathbf{v}_{j}^{B})\}_{j=1}^{l_{bs}}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，给定被逐出的标记，$\mathbf{E}$ 是一个小的正整数。我们将记忆单元表示为 $\mathbf{B}=\{(\mathbf{k}_{j}^{B},\mathbf{v}_{j}^{B})\}_{j=1}^{l_{bs}}$。
- en: 'For the memory lookup phrase, only $k_{m}$ as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于记忆查找短语，仅 $k_{m}$ 如下：
- en: '|  | $\text{sim}(\mathbf{X},\mathbf{B})=\sum_{i=1}^{l_{X}}\sum_{j=1}^{r_{k}}\mathbf{q}_{i+l_{P}}\cdot\mathbf{k}_{b_{j}}^{B}.$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{sim}(\mathbf{X},\mathbf{B})=\sum_{i=1}^{l_{X}}\sum_{j=1}^{r_{k}}\mathbf{q}_{i+l_{P}}\cdot\mathbf{k}_{b_{j}}^{B}.$
    |  | (2) |'
- en: Notably, the representative tokens selection is a training-free method to obtain
    the unit representations. Here, we can also train an additional encoder to generate
    more expressive unit representations, which we leave for future work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，代表性标记选择是一种无训练的方法来获取单位表示。在这里，我们也可以训练一个额外的编码器以生成更具表现力的单位表示，这部分留待未来工作中探讨。
- en: Positional Encoding. Existing LLM training usually employs a finite number of
    positional encodings, which encounter out-of-domain distribution challenges when
    directly applied to longer sequence processing (Han et al., [2023](#bib.bib21)).
    Besides, in InfLLM, the current key-value cache is composed of some discontinuous
    text blocks, and directly assigning continuous positional encodings to them would
    also lead to mismatch issues and confuse the model. Therefore, inspired by previous
    works (Raffel et al., [2020](#bib.bib44); Su, [2023](#bib.bib46)), we assign all
    tokens beyond the local window size with the same positional encodings. Specifically,
    the distance between tokens in context memory units and current tokens is set
    as $l_{L}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码。现有的LLM训练通常使用有限数量的位置编码，当直接应用于较长序列处理时，会遇到领域外分布挑战（Han等，[2023](#bib.bib21)）。此外，在InfLLM中，当前的键值缓存由一些不连续的文本块组成，直接为它们分配连续的位置编码也会导致不匹配问题并使模型混淆。因此，受到先前工作的启发（Raffel等，[2020](#bib.bib44)；Su，[2023](#bib.bib46)），我们为所有超出本地窗口大小的标记分配相同的位置编码。具体来说，上下文记忆单元中的标记与当前标记之间的距离设置为
    $l_{L}$。
- en: Cache Management. To enable LLMs to process extremely long sequence streams
    while capturing the semantic relevance contained in the long context, we need
    to retain all memory units and look up them at each computation step. Considering
    the infrequent usage of most units, we employ an offloading mechanism, storing
    most memory units in CPU memory and only preserving the representative tokens
    and memory units needed in current steps in GPU memory. Additionally, given the
    semantic coherence of long sequences, where adjacent tokens often require similar
    memory units, we allocate a cache space in GPU memory, managed using a least recently
    used strategy. This approach allows for efficient encoding of extremely long sequences
    using limited GPU memory. From the observation, we find that the miss rate of
    our GPU cache is quite low, which means the offloading mechanism does not introduce
    significant time overhead in memory loading while saving GPU memory usage. The
    details can be found in Appendix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存管理。为了使LLMs能够处理极长的序列流，同时捕捉长上下文中的语义关联，我们需要保留所有内存单元并在每个计算步骤中查找它们。考虑到大多数单元的使用频率较低，我们采用了卸载机制，将大多数内存单元存储在CPU内存中，仅在当前步骤中保留代表性标记和内存单元在GPU内存中。此外，鉴于长序列的语义连贯性，相邻的标记通常需要类似的内存单元，我们在GPU内存中分配了一个缓存空间，使用最少最近使用策略进行管理。这种方法允许使用有限的GPU内存高效编码极长的序列。从观察来看，我们发现GPU缓存的未命中率相当低，这意味着卸载机制在节省GPU内存使用的同时，并未引入显著的时间开销。详细信息可以在附录中找到。
- en: Furthermore, for extremely long sequences, the representative tokens of each
    unit can also be offloaded to the CPU memory, constructing an efficient k-nearest-neighbor
    index, and thereby further reducing computational complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于极长的序列，每个单元的代表性标记也可以卸载到CPU内存中，构建一个高效的k近邻索引，从而进一步降低计算复杂性。
- en: 'Table 1: The results of Mistral-based models. The models that can process extremely
    long streaming inputs are denoted with ^♣. The models with continual training
    are denoted with ^∗. The $95$K. The context window size for InfLLM is denoted
    as “local window size + selected memory size”.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基于Mistral模型的结果。可以处理极长流输入的模型标记为^♣。具有持续训练的模型标记为^∗。$95$K。InfLLM的上下文窗口大小标记为“局部窗口大小
    + 选择的内存大小”。
- en: '|  |  | Mistral-7B-inst-v0.2 (32K) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Mistral-7B-inst-v0.2 (32K) |'
- en: '|  |  | Original | Yarn^∗ | PI | NTK | Infinite^♣ | Stream^♣ | InfLLM^♣ | InfLLM^♣
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 原始 | Yarn^∗ | PI | NTK | Infinite^♣ | Stream^♣ | InfLLM^♣ | InfLLM^♣
    |'
- en: '|  | Context Window | 32K | 128K | 128K | 128K | 6K | 6K | 4K + 2K | 4K + 8K
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | 上下文窗口 | 32K | 128K | 128K | 128K | 6K | 6K | 4K + 2K | 4K + 8K |'
- en: '|  | Attention Type | Full | Full | Full | Full | Window | Window | Window
    | Window |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | 注意力类型 | 全部 | 全部 | 全部 | 全部 | 窗口 | 窗口 | 窗口 | 窗口 |'
- en: '| $\infty$-Bench (214K tokens) | Math.Find | 22.29 | 17.14 | 0.00 | 26.29 |
    14.00 | 13.71 | 26.57 | 26.86 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| $\infty$-Bench (214K tokens) | Math.Find | 22.29 | 17.14 | 0.00 | 26.29 |
    14.00 | 13.71 | 26.57 | 26.86 |'
- en: '| En.MC | 44.98 | 27.95 | 0.00 | 37.99 | 38.86 | 37.99 | 43.23 | 41.92 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| En.MC | 44.98 | 27.95 | 0.00 | 37.99 | 38.86 | 37.99 | 43.23 | 41.92 |'
- en: '| Code.Debug | 35.28 | 22.59 | 16.24 | 28.17 | 28.93 | 27.66 | 29.44 | 36.04
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Code.Debug | 35.28 | 22.59 | 16.24 | 28.17 | 28.93 | 27.66 | 29.44 | 36.04
    |'
- en: '| Retrieve.KV | 16.60 | 0.00 | 0.00 | 21.20 | 3.40 | 3.40 | 95.60 | 98.20 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.KV | 16.60 | 0.00 | 0.00 | 21.20 | 3.40 | 3.40 | 95.60 | 98.20 |'
- en: '| Retrieve.Number | 28.81 | 56.61 | 0.00 | 86.27 | 6.78 | 6.78 | 99.83 | 99.32
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.Number | 28.81 | 56.61 | 0.00 | 86.27 | 6.78 | 6.78 | 99.83 | 99.32
    |'
- en: '| Retrieve.PassKey | 28.81 | 92.71 | 0.00 | 100.00 | 6.78 | 6.78 | 100.00 |
    100.00 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.PassKey | 28.81 | 92.71 | 0.00 | 100.00 | 6.78 | 6.78 | 100.00 |
    100.00 |'
- en: '| Average | 29.46 | 36.17 | 2.80 | 49.99 | 16.46 | 16.05 | 65.78 | 67.06 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 29.46 | 36.17 | 2.80 | 49.99 | 16.46 | 16.05 | 65.78 | 67.06 |'
- en: '| LongBench (31K tokens) | NarrativeQA | 20.66 | 19.67 | 23.47 | 22.67 | 18.44
    | 17.92 | 22.12 | 23.03 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| LongBench (31K tokens) | NarrativeQA | 20.66 | 19.67 | 23.47 | 22.67 | 18.44
    | 17.92 | 22.12 | 23.03 |'
- en: '| Qasper | 29.14 | 11.10 | 29.36 | 29.90 | 30.02 | 30.05 | 29.33 | 29.52 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 29.14 | 11.10 | 29.36 | 29.90 | 30.02 | 30.05 | 29.33 | 29.52 |'
- en: '| MultiFieldQA | 47.39 | 35.06 | 46.47 | 46.41 | 39.05 | 39.09 | 47.42 | 47.62
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 47.39 | 35.06 | 46.47 | 46.41 | 39.05 | 39.09 | 47.42 | 47.62
    |'
- en: '| HotpotQA | 37.60 | 11.94 | 37.33 | 35.18 | 32.02 | 32.18 | 36.56 | 39.53
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 37.60 | 11.94 | 37.33 | 35.18 | 32.02 | 32.18 | 36.56 | 39.53
    |'
- en: '| 2WikiMQA | 22.54 | 12.02 | 22.19 | 21.71 | 22.27 | 21.83 | 22.31 | 23.61
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 22.54 | 12.02 | 22.19 | 21.71 | 22.27 | 21.83 | 22.31 | 23.61
    |'
- en: '| Musique | 18.50 | 7.52 | 18.80 | 19.45 | 15.81 | 14.71 | 17.68 | 18.92 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 18.50 | 7.52 | 18.80 | 19.45 | 15.81 | 14.71 | 17.68 | 18.92 |'
- en: '| GovReport | 31.03 | 29.46 | 32.36 | 32.86 | 29.74 | 29.83 | 31.03 | 31.37
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 31.03 | 29.46 | 32.36 | 32.86 | 29.74 | 29.83 | 31.03 | 31.37
    |'
- en: '| QMSum | 23.84 | 21.53 | 23.58 | 23.16 | 21.92 | 21.94 | 23.49 | 23.77 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 23.84 | 21.53 | 23.58 | 23.16 | 21.92 | 21.94 | 23.49 | 23.77 |'
- en: '| MultiNews | 26.65 | 16.04 | 26.62 | 26.69 | 26.65 | 26.64 | 26.70 | 26.66
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.65 | 16.04 | 26.62 | 26.69 | 26.65 | 26.64 | 26.70 | 26.66
    |'
- en: '| TREC | 71.00 | 68.50 | 71.50 | 71.50 | 70.00 | 70.00 | 69.00 | 71.00 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 71.00 | 68.50 | 71.50 | 71.50 | 70.00 | 70.00 | 69.00 | 71.00 |'
- en: '| TriviaQA | 86.22 | 88.21 | 88.86 | 89.31 | 85.22 | 85.57 | 86.67 | 87.34
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 86.22 | 88.21 | 88.86 | 89.31 | 85.22 | 85.57 | 86.67 | 87.34
    |'
- en: '| SAMSum | 42.11 | 26.52 | 42.58 | 41.55 | 41.60 | 41.31 | 42.52 | 41.80 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 42.11 | 26.52 | 42.58 | 41.55 | 41.60 | 41.31 | 42.52 | 41.80 |'
- en: '| PassageRetrieval | 89.02 | 16.25 | 89.10 | 92.42 | 42.80 | 42.17 | 64.00
    | 87.42 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| PassageRetrieval | 89.02 | 16.25 | 89.10 | 92.42 | 42.80 | 42.17 | 64.00
    | 87.42 |'
- en: '| LCC | 57.33 | 66.39 | 55.90 | 55.07 | 57.12 | 55.38 | 56.67 | 56.69 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LCC | 57.33 | 66.39 | 55.90 | 55.07 | 57.12 | 55.38 | 56.67 | 56.69 |'
- en: '| RepoBench-P | 54.27 | 55.82 | 52.46 | 50.33 | 53.43 | 51.46 | 52.97 | 52.09
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| RepoBench-P | 54.27 | 55.82 | 52.46 | 50.33 | 53.43 | 51.46 | 52.97 | 52.09
    |'
- en: '| Average | 43.82 | 32.40 | 44.04 | 43.88 | 39.07 | 38.67 | 41.90 | 44.02 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 43.82 | 32.40 | 44.04 | 43.88 | 39.07 | 38.67 | 41.90 | 44.02 |'
- en: 3.3 Connection to Retrieval-Augmented Generation
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 与检索增强生成的关联
- en: 'InfLLM leverages the intrinsic capacity of LLMs to construct a context memory
    for gathering token-relevant information, a concept similar to retrieval augmented
    generation (RAG) (Lewis et al., [2020](#bib.bib31); Nakano et al., [2021](#bib.bib36)).
    However, compared to using RAG, where historical contexts are treated as a searchable
    database for long-sequence understanding (Xu et al., [2023](#bib.bib59)), InfLLM
    has several advantages: (1) Training-Free: RAG requires additional retrieval data
    to train a retrieval model, whereas InfLLM is training-free and applicable to
    any LLMs. Besides, RAG also necessitates fine-tuning LLMs to adapt to the inputs
    augmented by the retrieved knowledge. (2) Broader Applicability: RAG necessitates
    the explicit specification of a query for the retrieval model. Therefore, RAG
    is typically suitable for tasks with a specified query, such as question answering,
    or requires the construction of an additional query generator. However, for many
    tasks, it is not feasible to construct an appropriate query to retrieve information
    from the inputs, such as in summarization. In contrast, InfLLM has no specific
    requirements for the tasks and can be feasibly used for long sequences. Therefore,
    RAG is more suited for knowledge-driven tasks to acquire additional external knowledge.
    Notably, our methods can be directly combined with RAG to advance knowledge-driven
    tasks. RAG is employed to retrieve external knowledge from the Internet, and the
    retrieved multiple documents are concatenated as long inputs for InfLLM to generate
    high-quality answers.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: InfLLM 利用 LLM 的内在能力构建上下文记忆，以收集与 token 相关的信息，这个概念类似于检索增强生成（RAG）（Lewis 等，[2020](#bib.bib31)；Nakano
    等，[2021](#bib.bib36)）。然而，与使用 RAG 的历史上下文作为可搜索数据库进行长序列理解（Xu 等，[2023](#bib.bib59)）相比，InfLLM
    具有几个优势：（1）无训练需求：RAG 需要额外的检索数据来训练检索模型，而 InfLLM 不需要训练，适用于任何 LLM。此外，RAG 还需要对 LLM
    进行微调，以适应由检索到的知识增强的输入。（2）更广泛的适用性：RAG 需要明确指定检索模型的查询。因此，RAG 通常适用于有特定查询的任务，如问答，或者需要构建额外的查询生成器。然而，对于许多任务，构建合适的查询以从输入中检索信息并不可行，例如在摘要生成中。相比之下，InfLLM
    对任务没有特定要求，可以有效地用于长序列。因此，RAG 更适合知识驱动的任务以获取额外的外部知识。值得注意的是，我们的方法可以直接与 RAG 结合，以推动知识驱动的任务。RAG
    被用来从互联网上检索外部知识，检索到的多个文档被连接成长输入，以便 InfLLM 生成高质量的答案。
- en: 'Table 2: The results of Vicuna-based models. The models that can process extremely
    long streaming inputs are denoted with ^♣. The models with continual training
    are denoted with ^∗. The $95$K. The context window size for InfLLM is denoted
    as “local window size + selected memory size”.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于 Vicuna 的模型结果。可以处理极长流式输入的模型用 ^♣ 标记。具有持续训练的模型用 ^∗ 标记。$95$K。InfLLM 的上下文窗口大小表示为“局部窗口大小
    + 选择的记忆大小”。
- en: '|  |  | Vicuna-7B-v1.5 (4K) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Vicuna-7B-v1.5 (4K) |'
- en: '|  |  | Original | LChat^∗ | Vic-16K^∗ | PI | NTK | Infinite^♣ | Stream^♣ |
    InfLLM^♣ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 原始 | LChat^∗ | Vic-16K^∗ | PI | NTK | Infinite^♣ | Stream^♣ | InfLLM^♣
    |'
- en: '|  | Context Window | 4K | 32K | 16K | 128K | 128K | 4K | 4K | 2K + 2K |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 上下文窗口 | 4K | 32K | 16K | 128K | 128K | 4K | 4K | 2K + 2K |'
- en: '|  | Attention Type | Full | Full | Full | Full | Full | Window | Window |
    Window |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 注意力类型 | 全部 | 全部 | 全部 | 全部 | 全部 | 窗口 | 窗口 | 窗口 |'
- en: '| $\infty$-Bench (214K tokens) | Math.Find | 11.71 | 9.43 | 13.43 | OOM | OOM
    | 5.71 | 6.00 | 11.14 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $\infty$-Bench (214K tokens) | Math.Find | 11.71 | 9.43 | 13.43 | OOM | OOM
    | 5.71 | 6.00 | 11.14 |'
- en: '| En.MC | 30.13 | 24.45 | 34.06 | OOM | OOM | 30.57 | 32.31 | 31.44 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| En.MC | 30.13 | 24.45 | 34.06 | OOM | OOM | 30.57 | 32.31 | 31.44 |'
- en: '| Code.Debug | 38.83 | 27.66 | 35.03 | OOM | OOM | 44.92 | 46.19 | 34.26 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Code.Debug | 38.83 | 27.66 | 35.03 | OOM | OOM | 44.92 | 46.19 | 34.26 |'
- en: '| Retrieve.KV | 1.40 | 1.40 | 1.00 | OOM | OOM | 0.00 | 0.00 | 0.60 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.KV | 1.40 | 1.40 | 1.00 | OOM | OOM | 0.00 | 0.00 | 0.60 |'
- en: '| Retrieve.Number | 4.41 | 23.90 | 10.34 | OOM | OOM | 4.24 | 4.41 | 81.69
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.Number | 4.41 | 23.90 | 10.34 | OOM | OOM | 4.24 | 4.41 | 81.69
    |'
- en: '| Retrieve.PassKey | 5.08 | 28.64 | 15.25 | OOM | OOM | 5.08 | 4.92 | 99.15
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Retrieve.PassKey | 5.08 | 28.64 | 15.25 | OOM | OOM | 5.08 | 4.92 | 99.15
    |'
- en: '| Average | 15.26 | 19.25 | 18.19 | – | – | 15.09 | 15.64 | 43.05 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 15.26 | 19.25 | 18.19 | – | – | 15.09 | 15.64 | 43.05 |'
- en: '| LongBench (31K tokens) | NarrativeQA | 11.19 | 20.35 | 17.85 | 0.78 | 5.66
    | 14.27 | 15.61 | 15.53 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LongBench (31K tokens) | NarrativeQA | 11.19 | 20.35 | 17.85 | 0.78 | 5.66
    | 14.27 | 15.61 | 15.53 |'
- en: '| Qasper | 13.79 | 29.35 | 25.85 | 2.71 | 21.17 | 22.88 | 23.84 | 23.57 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 13.79 | 29.35 | 25.85 | 2.71 | 21.17 | 22.88 | 23.84 | 23.57 |'
- en: '| MultiFieldQA | 22.08 | 42.55 | 37.15 | 1.01 | 36.76 | 32.48 | 32.80 | 37.14
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 22.08 | 42.55 | 37.15 | 1.01 | 36.76 | 32.48 | 32.80 | 37.14
    |'
- en: '| HotpotQA | 12.71 | 33.19 | 24.72 | 1.35 | 19.54 | 22.05 | 22.17 | 22.53 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 12.71 | 33.19 | 24.72 | 1.35 | 19.54 | 22.05 | 22.17 | 22.53 |'
- en: '| 2WikiMQA | 13.99 | 24.33 | 21.41 | 1.17 | 14.51 | 18.13 | 18.38 | 18.82 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 13.99 | 24.33 | 21.41 | 1.17 | 14.51 | 18.13 | 18.38 | 18.82 |'
- en: '| Musique | 4.81 | 14.71 | 8.44 | 0.71 | 4.30 | 7.37 | 6.30 | 5.24 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 4.81 | 14.71 | 8.44 | 0.71 | 4.30 | 7.37 | 6.30 | 5.24 |'
- en: '| GovReport | 27.67 | 30.83 | 27.62 | 1.9 | 25.26 | 23.44 | 23.18 | 26.79 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 27.67 | 30.83 | 27.62 | 1.9 | 25.26 | 23.44 | 23.18 | 26.79 |'
- en: '| QMSum | 19.72 | 22.93 | 22.63 | 1.29 | 19.48 | 20.67 | 20.09 | 20.91 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 19.72 | 22.93 | 22.63 | 1.29 | 19.48 | 20.67 | 20.09 | 20.91 |'
- en: '| MultiNews | 26.61 | 26.63 | 27.88 | 1.16 | 25.88 | 26.10 | 26.19 | 26.43
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.61 | 26.63 | 27.88 | 1.16 | 25.88 | 26.10 | 26.19 | 26.43
    |'
- en: '| TREC | 69.00 | 66.50 | 69.00 | 4.50 | 59.00 | 60.50 | 61.00 | 67.50 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 69.00 | 66.50 | 69.00 | 4.50 | 59.00 | 60.50 | 61.00 | 67.50 |'
- en: '| TriviaQA | 81.94 | 83.99 | 85.63 | 0.90 | 25.85 | 77.61 | 78.81 | 84.36 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 81.94 | 83.99 | 85.63 | 0.90 | 25.85 | 77.61 | 78.81 | 84.36 |'
- en: '| SAMSum | 35.12 | 12.83 | 9.15 | 0.12 | 5.05 | 32.55 | 32.46 | 31.89 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 35.12 | 12.83 | 9.15 | 0.12 | 5.05 | 32.55 | 32.46 | 31.89 |'
- en: '| PassageRetrieval | 9.00 | 30.50 | 4.00 | 0.62 | 5.00 | 7.00 | 6.00 | 9.00
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| PassageRetrieval | 9.00 | 30.50 | 4.00 | 0.62 | 5.00 | 7.00 | 6.00 | 9.00
    |'
- en: '| LCC | 64.53 | 54.79 | 50.64 | 21.54 | 53.65 | 64.22 | 63.70 | 61.41 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LCC | 64.53 | 54.79 | 50.64 | 21.54 | 53.65 | 64.22 | 63.70 | 61.41 |'
- en: '| RepoBench-P | 50.17 | 58.99 | 44.94 | 19.36 | 44.58 | 47.18 | 48.26 | 47.52
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| RepoBench-P | 50.17 | 58.99 | 44.94 | 19.36 | 44.58 | 47.18 | 48.26 | 47.52
    |'
- en: '| Average | 30.82 | 34.70 | 31.79 | 3.94 | 24.38 | 31.76 | 31.92 | 33.24 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 30.82 | 34.70 | 31.79 | 3.94 | 24.38 | 31.76 | 31.92 | 33.24 |'
- en: 4 Experiments
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: We adopt representative tasks in two widely-used long document benchmarks, $\infty$K
    respectively. The $95$K, which is far beyond the maximum length of the base models.
    Detailed statistics and task descriptions of these datasets are listed in the
    Appendix.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了两个广泛使用的长文档基准中的代表性任务，分别为 $\infty$K 和 $95$K，这远远超出了基础模型的最大长度。详细的统计数据和这些数据集的任务描述列在附录中。
- en: 4.2 Baseline Models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基准模型
- en: In this paper, we aim to enable LLMs trained with limited sequence length to
    read and understand extremely long sequences without further training. We adopt
    Mistral-7B-Instruct-v0.2 (Jiang et al., [2023](#bib.bib25)) and Vicuna-7B-v1.5 (Chiang
    et al., [2023](#bib.bib9)) as our base models. Mistral-7B-Instruct-v0.2 is first
    pre-trained with a maximum sequence length of $8$K as the maximum length.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们旨在使使用有限序列长度训练的LLMs能够阅读和理解极长的序列，而无需进一步训练。我们采用了 Mistral-7B-Instruct-v0.2
    (Jiang et al., [2023](#bib.bib25)) 和 Vicuna-7B-v1.5 (Chiang et al., [2023](#bib.bib9))
    作为我们的基础模型。Mistral-7B-Instruct-v0.2 首先经过最大序列长度为 $8$K 的预训练。
- en: 'To verify the effectiveness of our proposed method, we compare InfLLM with
    the following competitive baseline models: (1) Original is the original LLMs without
    context length extrapolation. (2) Position Interpolation (PI) (Chen et al., [2023b](#bib.bib8))
    directly down-scale the position indices for models with RoPE-based position encoding.
    Thus, within the same maximum position index, we can encode more tokens. (3) NTK-Aware
    Scaled RoPE (NTK) ¹¹1[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
    designs a nonlinear interpolation method, which basically changes the rotation
    base of RoPE. (4) Continual Training refers to methods that continually pre-train
    or fine-tune LLMs with additional long sequences. Here, we choose Yarn-Mistral-128k
    (Yarn) (Peng et al., [2023](#bib.bib40)) for Mistral-based models. We choose LongChat-32K
    (LChat) (Li et al., [2023](#bib.bib32)) and Vicuna-16K (Vic-16K) ²²2[https://huggingface.co/lmsys/vicuna-7b-v1.5-16k](https://huggingface.co/lmsys/vicuna-7b-v1.5-16k)
    for Vicuna-based models as baselines. These models are fine-tuned versions of
    scaled RoPE methods. (5) Sliding Window indicates the models that apply the sliding
    window mechanism to discard distant contexts, including LM-Infinite (Infinite) (Han
    et al., [2023](#bib.bib21)) and StreamingLLM (Stream) (Xiao et al., [2023](#bib.bib57)).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们提出的方法的有效性，我们将 InfLLM 与以下竞争基准模型进行比较：（1）Original 是没有上下文长度外推的原始 LLM。 （2）Position
    Interpolation (PI) (Chen et al., [2023b](#bib.bib8)) 直接对基于 RoPE 的位置编码模型进行位置索引缩放。因此，在相同的最大位置索引下，我们可以编码更多的标记。
    （3）NTK-Aware Scaled RoPE (NTK) ¹¹1[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
    设计了一种非线性插值方法，基本上改变了 RoPE 的旋转基。 （4）Continual Training 指的是通过额外的长序列不断进行预训练或微调的 LLM
    方法。在这里，我们选择 Yarn-Mistral-128k (Yarn) (Peng et al., [2023](#bib.bib40)) 作为基于 Mistral
    的模型。我们选择 LongChat-32K (LChat) (Li et al., [2023](#bib.bib32)) 和 Vicuna-16K (Vic-16K) ²²2[https://huggingface.co/lmsys/vicuna-7b-v1.5-16k](https://huggingface.co/lmsys/vicuna-7b-v1.5-16k)
    作为基于 Vicuna 的模型的基准。这些模型是缩放 RoPE 方法的微调版本。 （5）Sliding Window 指的是应用滑动窗口机制来丢弃远距离上下文的模型，包括
    LM-Infinite (Infinite) (Han et al., [2023](#bib.bib21)) 和 StreamingLLM (Stream) (Xiao
    et al., [2023](#bib.bib57))。
- en: All models except for models with continual training require no additional training.
    For PI and NTK, we extend the context window to $128$K, which enables LLMs to
    process most instances in both two benchmarks. For models with finite context
    length, we truncate the inputs by only preserving the system prompts and the tail
    of inputs to simulate real-world applications with streaming inputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了具有持续训练的模型外，所有模型都无需额外训练。对于 PI 和 NTK，我们将上下文窗口扩展到 $128$K，这使得 LLM 能够处理这两个基准中的大多数实例。对于具有有限上下文长度的模型，我们通过仅保留系统提示和输入的尾部来截断输入，以模拟具有流式输入的真实应用场景。
- en: '![Refer to caption](img/f1f05a5c7338f175e93004f5a874028d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f1f05a5c7338f175e93004f5a874028d.png)'
- en: (a)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/9821f14070f63afecba1d880efa2d079.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9821f14070f63afecba1d880efa2d079.png)'
- en: (b)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/0daa771e78eedf4a256d52de6288e81f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0daa771e78eedf4a256d52de6288e81f.png)'
- en: (c)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2: Extra studies about InfLLM. Here, (a), (b), and (c) investigate the
    impact of the context memory under different numbers of representative tokens,
    different numbers of selected units, and memory unit sizes, respectively.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：关于 InfLLM 的额外研究。在这里，（a）、（b）和（c）分别研究了不同数量的代表性标记、不同数量的选择单元和记忆单元大小对上下文记忆的影响。
- en: 4.3 Implementation Details
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实现细节
- en: For our model, we set the encoding chunk size as $512$ relevant memory units
    for each step. For Mistral-based InfLLM, we set the local window size as $4$.
    The number of initial tokens is set as $128$ for LM-Infinite, StreamingLLM, and
    InfLLM to cover the system prompts and task descriptions. We adopt FlashAttention (Dao,
    [2023](#bib.bib13)) to accelerate experiments for all baseline models. Please
    refer to the Appendix for more details.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们将编码块大小设置为每步 $512$ 个相关记忆单元。对于基于 Mistral 的 InfLLM，我们将局部窗口大小设置为 $4$。LM-Infinite、StreamingLLM
    和 InfLLM 的初始标记数量设置为 $128$ 以覆盖系统提示和任务描述。我们采用 FlashAttention (Dao, [2023](#bib.bib13))
    来加速所有基准模型的实验。更多细节请参见附录。
- en: 4.4 Main Results
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 主要结果
- en: 'The results for Mistral-based models and Vicuna-based models are reported in
    Table [1](#S3.T1 "Table 1 ‣ 3.2 Context Memory ‣ 3 Methodology ‣ InfLLM: Unveiling
    the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with
    Training-Free Memory") and Table [2](#S3.T2 "Table 2 ‣ 3.3 Connection to Retrieval-Augmented
    Generation ‣ 3 Methodology ‣ InfLLM: Unveiling the Intrinsic Capacity of LLMs
    for Understanding Extremely Long Sequences with Training-Free Memory") respectively.
    From the results, we can observe that: (1) Compared to models with the sliding
    window mechanism, which can also read extremely long sequences, our method demonstrates
    a significant performance improvement on two benchmarks. This indicates that the
    context memory in InfLLM can accurately supplement LLMs with relevant contextual
    information, enabling efficient and effective understanding and reasoning on long
    sequences. (2) Under the training-free setting, previous context length extrapolation
    models, such as PI and NTK, tend to compromise model performance while extending
    the sequence length to $128$-Bench. (3) The scaled RoPE methods, including PI,
    NTK, and continually trained models, can increase the maximum sequence length
    of LLMs but also raise the computational and memory costs. This leads to Vicuna-based
    PI and NTK models exceeding GPU memory limits when processing $128$K-length sequences,
    limiting these methods’ application to encoding extremely long sequences. In contrast,
    InfLLM utilizes block-level memory and offloading mechanisms, enabling efficient
    processing of extremely long sequences within limited resources. (4) When the
    sequence length is within the model’s processing capability, as with Mistral-based
    models on LongBench, InfLLM can achieve comparable or even superior results with
    a shorter window size, significantly reducing computational overhead for inference.
    (5) Compared to models that have undergone continual training on long sequences,
    InfLLM can achieve comparable or even superior results without any additional
    training. This suggests that LLMs inherently possess the capability to identify
    key information in long sequences and to understand and reason effectively.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 基础模型和 Vicuna 基础模型的结果分别报告在表 [1](#S3.T1 "表 1 ‣ 3.2 上下文记忆 ‣ 3 方法论 ‣ InfLLM：揭示
    LLM 对极长序列的内在理解能力，无需训练的记忆") 和表 [2](#S3.T2 "表 2 ‣ 3.3 连接到检索增强生成 ‣ 3 方法论 ‣ InfLLM：揭示
    LLM 对极长序列的内在理解能力，无需训练的记忆")。从结果中，我们可以观察到：(1) 与使用滑动窗口机制的模型相比，这些模型也可以读取极长序列，我们的方法在两个基准上显示出显著的性能提升。这表明，InfLLM
    中的上下文记忆可以准确补充 LLM 相关的上下文信息，从而实现对长序列的高效和有效理解与推理。(2) 在无训练设置下，之前的上下文长度外推模型，如 PI 和
    NTK，在将序列长度扩展到 $128$-Bench 时往往会妥协模型性能。(3) 扩展的 RoPE 方法，包括 PI、NTK 和持续训练的模型，可以增加 LLM
    的最大序列长度，但也提高了计算和内存成本。这导致处理 $128$K 长序列时，Vicuna 基础的 PI 和 NTK 模型超出了 GPU 内存限制，限制了这些方法对极长序列编码的应用。相比之下，InfLLM
    利用块级记忆和卸载机制，在有限资源内实现对极长序列的高效处理。(4) 当序列长度在模型处理能力范围内时，如 Mistral 基础模型在 LongBench
    上，InfLLM 可以在较短的窗口大小下实现相当甚至更优的结果，从而显著减少推理的计算开销。(5) 与经历了长期序列持续训练的模型相比，InfLLM 在没有任何额外训练的情况下也能取得相当甚至更优的结果。这表明
    LLM 天生具备识别长序列中关键信息的能力，并能够有效理解和推理。
- en: 4.5 Extra Investigation
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 额外调查
- en: 'InfLLM relies on the context memory to look up relevant information. We further
    explore the impact of core components in the context memory, specifically the
    representative tokens and memory units. The results are shown in Figure [2](#S4.F2
    "Figure 2 ‣ 4.2 Baseline Models ‣ 4 Experiments ‣ InfLLM: Unveiling the Intrinsic
    Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free
    Memory").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: InfLLM 依赖于上下文记忆来查找相关信息。我们进一步探讨了上下文记忆中核心组件的影响，特别是代表性标记和记忆单元。结果见图 [2](#S4.F2 "图
    2 ‣ 4.2 基准模型 ‣ 4 实验 ‣ InfLLM：揭示 LLM 对极长序列的内在理解能力，无需训练的记忆")。
- en: Different Number of Representative Tokens. InfLLM splits key-value vectors into
    memory units and selects several representative tokens from the unit to serve
    as the unit representations. Consequently, the ability of these representative
    tokens to semantically represent the entire unit directly impacts the model’s
    performance. We conduct experiments with the number of representative tokens as
    $\{1,2,4,8\}$, there is a slight performance decrease on Retrieve.KV and NarrativeQA.
    This decline can be attributed to the inclusion of semantically irrelevant tokens
    as unit representations. More efficient and powerful unit representations will
    further enhance model performance for future work.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性标记的不同数量。InfLLM将键值向量拆分成记忆单元，并从单元中选择几个代表性标记作为单元表示。因此，这些代表性标记在语义上表示整个单元的能力直接影响模型的性能。我们对代表性标记的数量进行实验，设置为$\{1,2,4,8\}$，在Retrieve.KV和NarrativeQA上性能略有下降。这种下降可以归因于将语义上无关的标记作为单元表示。更高效和更强大的单元表示将进一步提升模型性能，这是未来工作的重点。
- en: Different Number of Selected Units. The selected units are utilized to provide
    relevant context to LLMs. We conduct experiments with the number of units set
    as $\{1,2,4,8,32,64,128\}$, the model performance significantly improves, which
    is attributed to that more units imply a greater recall rate of relevant content.
    Larger unit quantity also leads to an increase in the required memory scheduling
    time and the computational time for attention. Therefore, further enhancing lookup
    accuracy remains a crucial direction for improving the efficiency of InfLLM.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数量的选定单元。选定的单元用于为LLM提供相关上下文。我们对单位数量设置为$\{1,2,4,8,32,64,128\}$进行实验，模型性能显著提高，这归因于更多的单元意味着更高的相关内容召回率。较大的单元数量还会增加所需的记忆调度时间和注意力计算时间。因此，进一步提高查找准确性仍然是提升InfLLM效率的关键方向。
- en: 'Different Memory Unit Size. Each memory unit is supposed to be a coherent semantic
    unit. Excessively large unit sizes can hinder precise lookup, while a small size
    will increase the computational overhead of memory lookup. We evaluate InfLLM
    with the unit size as $\{64,128,256,512\}$. The results are shown in Figure [2(c)](#S4.F2.sf3
    "Figure 2(c) ‣ Figure 2 ‣ 4.2 Baseline Models ‣ 4 Experiments ‣ InfLLM: Unveiling
    the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with
    Training-Free Memory"). It can be observed that the optimal unit size varies for
    different tasks due to the varying characteristics of input sequences. For example,
    in Retrieve.KV, a key-value pair constitutes a semantic unit, while in Math.Find,
    a single number represents a semantic unit. Employing heuristic rules to segment
    context can easily lead to suboptimal performance. Therefore, exploring how to
    dynamically segment context is an important direction for future research.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的记忆单元大小。每个记忆单元应该是一个连贯的语义单元。过大的单元尺寸可能会阻碍精确查找，而较小的单元尺寸则会增加记忆查找的计算开销。我们对InfLLM进行单位大小为$\{64,128,256,512\}$的评估。结果如图[2(c)](#S4.F2.sf3
    "图 2(c) ‣ 图 2 ‣ 4.2 基线模型 ‣ 4 实验 ‣ InfLLM：揭示LLM理解极长序列的内在能力，无需训练的记忆")所示。可以观察到，由于输入序列的不同特性，不同任务的最佳单元大小有所不同。例如，在Retrieve.KV中，一个键值对构成一个语义单元，而在Math.Find中，一个单一的数字代表一个语义单元。使用启发式规则对上下文进行分段很容易导致次优性能。因此，探索如何动态分段上下文是未来研究的重要方向。
- en: 4.6 Ablation Study
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 消融研究
- en: 'To further verify the effectiveness of dynamic multi-step memory lookup and
    unit representations, we conduct ablation studies in this section. The results
    are shown in Table [3](#S4.T3 "Table 3 ‣ 4.6 Ablation Study ‣ 4 Experiments ‣
    InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long
    Sequences with Training-Free Memory").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证动态多步记忆查找和单元表示的有效性，我们在本节中进行消融研究。结果如表格[3](#S4.T3 "表格 3 ‣ 4.6 消融研究 ‣ 4 实验
    ‣ InfLLM：揭示LLM理解极长序列的内在能力，无需训练的记忆")所示。
- en: Context Memory Lookup. InfLLM adopts dynamic context memory lookup for both
    input encoding and output decoding steps for comprehensive long-text understanding.
    We present the results of InfLLM with only lookup in output decoding (Decoding-Only)
    and without any memory lookup (w/o Lookup). It can be observed that a significant
    decline in model performance is associated with a reduction in the number of memory
    lookup iterations. This indicates that distant contextual information is crucial
    for both the long-input encoding and answer-generation phases. The model requires
    the integration of long-distance context to generate a coherent context memory
    for input understanding. LLM is supposed to collect useful information from massive
    past context information to generate the correct answers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文内存查找。InfLLM 采用动态上下文内存查找来进行输入编码和输出解码步骤，以全面理解长文本。我们展示了 InfLLM 仅在输出解码中进行查找（仅解码）和没有任何内存查找（无查找）的结果。可以观察到，模型性能显著下降与内存查找迭代次数减少相关。这表明，远程上下文信息对长输入编码和回答生成阶段至关重要。模型需要整合长距离上下文以生成连贯的上下文内存来理解输入。LLM
    应该从大量的过去上下文信息中收集有用的信息，以生成正确的答案。
- en: 'Table 3: The results for ablation study. Here R.KV, M.Find, NQA, and HQA refer
    to Retrieve.KV, Math.Find, NarrativeQA, and HotpotQA.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：消融研究结果。这里的 R.KV、M.Find、NQA 和 HQA 分别指代 Retrieve.KV、Math.Find、NarrativeQA
    和 HotpotQA。
- en: '| Task | R.KV | M.Find | NQA | HQA |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | R.KV | M.Find | NQA | HQA |'
- en: '| InfLLM | 95.60 | 26.57 | 36.56 | 22.12 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM | 95.60 | 26.57 | 36.56 | 22.12 |'
- en: '|    Decoding-Only | 32.60 | 26.57 | 32.74 | 19.72 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|    仅解码 | 32.60 | 26.57 | 32.74 | 19.72 |'
- en: '|    w/o Lookup | 3.40 | 14.00 | 32.02 | 18.44 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|    无查找 | 3.40 | 14.00 | 32.02 | 18.44 |'
- en: '|    Mean Repr | 75.20 | 26.86 | 37.12 | 19.09 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|    均值表示 | 75.20 | 26.86 | 37.12 | 19.09 |'
- en: Unit Representation. We design a block-level memory for efficient context information
    lookup. We select several representative tokens as the unit representations for
    relevance computation. We present the results of InfLLM with another training-free
    representation method (Mean Repr), which computes the representation by averaging
    the key vectors in a memory unit. For a fair comparison, we set the number of
    representation vectors as $4$. From the results, we can observe that InfLLM with
    average representations can also present competitive performance, and achieve
    superior results to representative tokens on Math.Find and NarrativeQA. It indicates
    that the original attention vectors in LLMs are effective for relevance score
    computation, and exploring more efficient unit representations is an important
    future direction.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 单位表示。我们设计了一个块级内存用于高效的上下文信息查找。我们选择了几个代表性标记作为单位表示进行相关性计算。我们展示了 InfLLM 使用另一种无需训练的表示方法（均值表示法）的结果，该方法通过对内存单元中的键向量进行平均来计算表示。为了公平比较，我们将表示向量的数量设置为
    $4$。从结果中可以观察到，使用平均表示的 InfLLM 也能展示出竞争力的性能，并且在 Math.Find 和 NarrativeQA 上取得了优于代表性标记的结果。这表明
    LLM 中的原始注意力向量对相关性得分计算是有效的，探索更高效的单位表示是一个重要的未来方向。
- en: 4.7 Scaling to 1,024K Context
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 扩展到 1,024K 上下文
- en: To assess the effectiveness of InfLLM on extremely long sequences, in this subsection,
    we scale the sequence length to $1024$ thousand tokens and for each length, we
    generate $50$ instances for evaluation. We adopt Mistral as the base model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 InfLLM 在极长序列上的有效性，本节将序列长度扩展到 $1024$ 千个标记，并为每个长度生成 $50$ 个实例进行评估。我们采用 Mistral
    作为基础模型。
- en: '![Refer to caption](img/8c6cfd8df902dc915e20e89c7c2c87f3.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8c6cfd8df902dc915e20e89c7c2c87f3.png)'
- en: 'Figure 3: The results for InfLLM and LM-Infinite on sequences with different
    lengths.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：InfLLM 和 LM-Infinite 在不同长度序列上的结果。
- en: 'The results are shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.7 Scaling to 1,024K
    Context ‣ 4 Experiments ‣ InfLLM: Unveiling the Intrinsic Capacity of LLMs for
    Understanding Extremely Long Sequences with Training-Free Memory"). From the results,
    we can observe that InfLLM can accurately locate the key information from length
    noises and achieve $100$ thousand tokens. However, LM-Infinite can only attend
    to the tokens within the local window, which leads to a rapid decline in its performance
    as the sequence length increases. It proves that InfLLM can accurately capture
    the long-distance dependencies for effective long-sequence reasoning.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见图 [3](#S4.F3 "图 3 ‣ 4.7 扩展到 1,024K 上下文 ‣ 4 实验 ‣ InfLLM：揭示 LLM 理解极长序列的内在能力")。从结果中可以观察到，InfLLM
    可以从长度噪声中准确定位关键信息，并实现 $100$ 千个标记。然而，LM-Infinite 只能关注局部窗口中的标记，导致其性能随着序列长度的增加迅速下降。这证明了
    InfLLM 可以准确捕捉长距离依赖，以进行有效的长序列推理。
- en: 5 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a training-free method to extend the context window
    of LLMs. Based on the sliding window attention mechanism, we construct an additional
    context memory module, which can help LLMs select relevant information from massive
    contexts to capture long-distance dependencies. The experiments on two widely-used
    long-text benchmarks show that InfLLM can effectively improve the ability of LLMs,
    which are trained on sequences with a few thousand tokens, to process extremely
    long sequences. In the future, we will explore efficient training of the context
    memory module to further enhance the model performance. Besides, combining the
    key-value cache compression methods with InfLLM can reduce the computational and
    memory costs for memory management. We hope InfLLM can boost the development of
    streaming applications of LLMs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种无训练的方法来扩展 LLM 的上下文窗口。基于滑动窗口注意机制，我们构建了一个额外的上下文记忆模块，该模块可以帮助 LLM 从大量上下文中选择相关信息，以捕捉长距离依赖。在两个广泛使用的长文本基准上的实验表明，InfLLM
    可以有效提升训练于几千个标记序列的 LLM 处理极长序列的能力。未来，我们将探索有效的上下文记忆模块训练，以进一步增强模型性能。此外，将关键值缓存压缩方法与
    InfLLM 结合可以减少内存管理的计算和内存开销。我们希望 InfLLM 能促进 LLM 流式应用的发展。
- en: Broader Impact
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: This paper presents work whose goal is to advance the field of large language
    models. There are many potential societal consequences of our work, none of which
    we feel must be specifically highlighted here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本文呈现的工作旨在推动大语言模型领域的发展。我们的工作可能会带来许多潜在的社会影响，其中没有特别需要在此强调的。
- en: References
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ainslie et al. (2020) Ainslie, J., Ontañón, S., Alberti, C., Cvicek, V., Fisher,
    Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. ETC: encoding long
    and structured inputs in transformers. In *Proceedings of EMNLP*, pp.  268–284,
    2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie 等（2020）Ainslie, J., Ontañón, S., Alberti, C., Cvicek, V., Fisher, Z.,
    Pham, P., Ravula, A., Sanghai, S., Wang, Q., 和 Yang, L. ETC: 在变换器中编码长且结构化的输入。在
    *EMNLP 会议论文集* 中，第 268–284 页，2020。'
- en: 'Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,
    Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench:
    A bilingual, multitask benchmark for long context understanding. *CoRR*, abs/2308.14508,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等（2023）Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z.,
    Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., 和 Li, J. Longbench: 一个用于长上下文理解的双语多任务基准。*CoRR*，abs/2308.14508，2023。'
- en: 'Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer:
    The long-document transformer. *CoRR*, abs/2004.05150, 2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等（2020）Beltagy, I., Peters, M. E., 和 Cohan, A. Longformer: 长文档变换器。*CoRR*，abs/2004.05150，2020。'
- en: 'Bertsch et al. (2023) Bertsch, A., Alon, U., Neubig, G., and Gormley, M. R.
    Unlimiformer: Long-range transformers with unlimited length input. *CoRR*, abs/2305.01625,
    2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bertsch 等（2023）Bertsch, A., Alon, U., Neubig, G., 和 Gormley, M. R. Unlimiformer:
    具有无限长度输入的长范围变换器。*CoRR*，abs/2305.01625，2023。'
- en: Bommasani et al. (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
    Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E.,
    Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S.,
    Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E.,
    Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie,
    L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson,
    P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S.,
    Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O.,
    Koh, P. W., Krass, M. S., Krishna, R., Kuditipudi, R., and et al. On the opportunities
    and risks of foundation models. *CoRR*, abs/2108.07258, 2021.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani 等 (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora,
    S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson,
    E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K.,
    Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S.,
    Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L.,
    Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P.,
    Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky,
    D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.
    W., Krass, M. S., Krishna, R., Kuditipudi, R., 等. 关于基础模型的机会和风险。*CoRR*, abs/2108.07258,
    2021。
- en: Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners. In *Proceedings of NeurIPS*,
    2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., 和 Amodei, D.
    语言模型是少样本学习者。发表于 *Proceedings of NeurIPS*, 2020。
- en: 'Chen et al. (2023a) Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. CLEX:
    continuous length extrapolation for large language models. *CoRR*, abs/2310.16450,
    2023a. doi: 10.48550/ARXIV.2310.16450.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023a) Chen, G., Li, X., Meng, Z., Liang, S., 和 Bing, L. CLEX: 大型语言模型的连续长度外推。*CoRR*,
    abs/2310.16450, 2023a. doi: 10.48550/ARXIV.2310.16450。'
- en: Chen et al. (2023b) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation. *CoRR*, abs/2306.15595,
    2023b.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023b) Chen, S., Wong, S., Chen, L., 和 Tian, Y. 通过位置插值扩展大型语言模型的上下文窗口。*CoRR*,
    abs/2306.15595, 2023b。
- en: 'Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing,
    E. P. Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality,
    March 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等 (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H.,
    Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., 和 Xing, E. P.
    Vicuna: 一款开源聊天机器人，以 90% ChatGPT 质量打动 GPT-4，2023 年 3 月。'
- en: Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating
    long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等 (2019) Child, R., Gray, S., Radford, A., 和 Sutskever, I. 使用稀疏变换器生成长序列。*CoRR*,
    abs/1904.10509, 2019。
- en: 'Dai et al. (2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V.,
    and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length
    context. In *Proceedings of ACL*, pp.  2978–2988\. Association for Computational
    Linguistics, 2019.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 (2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V., 和 Salakhutdinov,
    R. Transformer-xl: 超越固定长度上下文的注意力语言模型。发表于 *Proceedings of ACL*, pp. 2978–2988\.
    计算语言学协会，2019。'
- en: 'Dai et al. (2020) Dai, Z., Lai, G., Yang, Y., and Le, Q. Funnel-transformer:
    Filtering out sequential redundancy for efficient language processing. In *Proceedings
    of NeurIPS*, 2020.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 (2020) Dai, Z., Lai, G., Yang, Y., 和 Le, Q. Funnel-transformer: 过滤序列冗余以提高语言处理效率。发表于
    *Proceedings of NeurIPS*, 2020。'
- en: 'Dao (2023) Dao, T. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *CoRR*, abs/2307.08691, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao (2023) Dao, T. Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分区。*CoRR*, abs/2307.08691,
    2023。'
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. In *Proceedings of
    NeurIPS*, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等 (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., 和 Ré, C. Flashattention:
    快速且内存高效的精确注意力机制，具有 IO 感知。发表于 *Proceedings of NeurIPS*, 2022。'
- en: Dong et al. (2023) Dong, Z., Tang, T., Li, J., and Zhao, W. X. A survey on long
    text modeling with transformers. *CoRR*, abs/2302.14502, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2023) Dong, Z., Tang, T., Li, J., 和 Zhao, W. X. 基于变换器的长文本建模调查。*CoRR*,
    abs/2302.14502, 2023。
- en: 'Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar,
    Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,
    M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal
    language model. In *Proceedings of ICML*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  8469–8488\. PMLR, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar,
    Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint,
    M., Greff, K., Zeng, A., Mordatch, I., 和 Florence, P. Palm-e: 一种具身多模态语言模型。发表于
    *Proceedings of ICML*，第202卷，*Proceedings of Machine Learning Research*，第8469–8488页。PMLR,
    2023。'
- en: 'Goyal et al. (2020) Goyal, S., Choudhury, A. R., Raje, S., Chakaravarthy, V. T.,
    Sabharwal, Y., and Verma, A. Power-bert: Accelerating BERT inference via progressive
    word-vector elimination. In *Proceedings of ICML*, volume 119, pp.  3690–3699,
    2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal et al. (2020) Goyal, S., Choudhury, A. R., Raje, S., Chakaravarthy, V.
    T., Sabharwal, Y., 和 Verma, A. Power-bert: 通过渐进的词向量消除加速 BERT 推理。发表于 *Proceedings
    of ICML*，第119卷，第3690–3699页，2020。'
- en: Graves et al. (2014) Graves, A., Wayne, G., and Danihelka, I. Neural turing
    machines. *CoRR*, abs/1410.5401, 2014.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves et al. (2014) Graves, A., Wayne, G., 和 Danihelka, I. 神经图灵机。*CoRR*, abs/1410.5401,
    2014。
- en: 'Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with
    selective state spaces. *CoRR*, abs/2312.00752, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu & Dao (2023) Gu, A. 和 Dao, T. Mamba: 具有选择性状态空间的线性时间序列建模。*CoRR*, abs/2312.00752,
    2023。'
- en: Gu et al. (2022) Gu, A., Goel, K., and Ré, C. Efficiently modeling long sequences
    with structured state spaces. In *Proceedings of ICLR*. OpenReview.net, 2022.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2022) Gu, A., Goel, K., 和 Ré, C. 使用结构化状态空间高效建模长序列。发表于 *Proceedings
    of ICLR*。OpenReview.net, 2022。
- en: 'Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang,
    S. Lm-infinite: Simple on-the-fly length generalization for large language models.
    *CoRR*, abs/2308.16137, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., 和 Wang, S.
    Lm-infinite: 对大型语言模型的简单即时长度泛化。*CoRR*, abs/2308.16137, 2023。'
- en: 'Han et al. (2021) Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu,
    J., Yao, Y., Zhang, A., Zhang, L., Han, W., Huang, M., Jin, Q., Lan, Y., Liu,
    Y., Liu, Z., Lu, Z., Qiu, X., Song, R., Tang, J., Wen, J., Yuan, J., Zhao, W. X.,
    and Zhu, J. Pre-trained models: Past, present and future. *AI Open*, 2:225–250,
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2021) Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu,
    J., Yao, Y., Zhang, A., Zhang, L., Han, W., Huang, M., Jin, Q., Lan, Y., Liu,
    Y., Liu, Z., Lu, Z., Qiu, X., Song, R., Tang, J., Wen, J., Yuan, J., Zhao, W.
    X., 和 Zhu, J. 预训练模型: 过去、现在和未来。*AI Open*, 2:225–250, 2021。'
- en: 'Hong et al. (2023) Hong, K., Dai, G., Xu, J., Mao, Q., Li, X., Liu, J., Chen,
    K., Dong, Y., and Wang, Y. Flashdecoding++: Faster large language model inference
    on gpus. *CoRR*, abs/2311.01282, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. (2023) Hong, K., Dai, G., Xu, J., Mao, Q., Li, X., Liu, J., Chen,
    K., Dong, Y., 和 Wang, Y. Flashdecoding++: 在 GPU 上更快的大型语言模型推理。*CoRR*, abs/2311.01282,
    2023。'
- en: 'Huang et al. (2023) Huang, Y., Xu, J., Jiang, Z., Lai, J., Li, Z., Yao, Y.,
    Chen, T., Yang, L., Xin, Z., and Ma, X. Advancing transformer architecture in
    long-context large language models: A comprehensive survey. *CoRR*, abs/2311.12351,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023) Huang, Y., Xu, J., Jiang, Z., Lai, J., Li, Z., Yao, Y.,
    Chen, T., Yang, L., Xin, Z., 和 Ma, X. 提升长上下文大型语言模型中的 transformer 架构: 一项全面的调查。*CoRR*,
    abs/2311.12351, 2023。'
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T., and Sayed, W. E. Mistral 7b. *CoRR*, abs/2310.06825, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T., 和 Sayed, W. E. Mistral 7b。*CoRR*, abs/2310.06825, 2023。
- en: 'Jin et al. (2024) Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.,
    Chen, H., and Hu, X. LLM maybe longlm: Self-extend LLM context window without
    tuning. *CoRR*, abs/2401.01325, 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin et al. (2024) Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.,
    Chen, H., 和 Hu, X. LLM 可能是 longlm: 自我扩展 LLM 上下文窗口而无需调优。*CoRR*, abs/2401.01325,
    2024。'
- en: 'Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret,
    F. Transformers are RNNs: Fast autoregressive transformers with linear attention.
    In *Proceedings of ICML*, volume 119, pp.  5156–5165. PMLR, 2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., 和 Fleuret,
    F. Transformers 是 RNNs: 具有线性注意力的快速自回归 transformers。发表于 *Proceedings of ICML*，第119卷，第5156–5165页。PMLR,
    2020。'
- en: 'Khandelwal et al. (2020) Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,
    L., and Lewis, M. Generalization through memorization: Nearest neighbor language
    models. In *Proceedings of ICLR*. OpenReview.net, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉德尔瓦尔等人（2020）汉德尔瓦尔, U., 莱维, O., 朱拉夫斯基, D., 泽特尔莫耶, L., 和刘易斯, M. 通过记忆实现泛化：最近邻语言模型。在*ICLR
    会议录*。OpenReview.net，2020年。
- en: 'Kitaev et al. (2020) Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
    efficient transformer. In *Proceedings of ICLR*. OpenReview.net, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '基塔耶夫等人（2020）基塔耶夫, N., 凯瑟, L., 和列夫斯卡娅, A. Reformer: 高效的变换器。在*ICLR 会议录*。OpenReview.net，2020年。'
- en: Kwon et al. (2023) Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large
    language model serving with pagedattention. In *Proceedings of SOSP*, pp.  611–626\.
    ACM, 2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权等人（2023）权, W., 李, Z., 庄, S., 盛, Y., 郑, L., 余, C. H., 冯, J., 张, H., 和斯托伊卡, I.
    大型语言模型服务的高效内存管理与分页注意力。在*SOSP 会议录*，第611–626页。ACM，2023年。
- en: Lewis et al. (2020) Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin,
    V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., and
    Kiela, D. Retrieval-augmented generation for knowledge-intensive NLP tasks. In
    *Proceedings of NeurIPS*, 2020.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘易斯等人（2020）刘易斯, P. S. H., 佩雷斯, E., 皮克图斯, A., 佩特罗尼, F., 卡尔普欣, V., 戈亚尔, N., 库特勒,
    H., 刘易斯, M., 易, W., 罗克塔谢尔, T., 里德尔, S., 和基埃拉, D. 用于知识密集型自然语言处理任务的检索增强生成。在*NeurIPS
    会议录*，2020年。
- en: Li et al. (2023) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J. E., Stoica, I., Ma, X., , and Zhang, H. How long can open-source llms truly
    promise on context length?, June 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2023）李, D., 邵, R., 谢, A., 盛, Y., 郑, L., 冯, J. E., 斯托伊卡, I., 马, X., 和张, H.
    开源大型语言模型在上下文长度上的真正承诺有多长？，2023年6月。
- en: 'Liang et al. (2023) Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter,
    B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied
    control. In *Proceedings of ICRA*, pp.  9493–9500\. IEEE, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梁等人（2023）梁, J., 黄, W., 夏, F., 徐, P., 豪斯曼, K., 伊克特, B., 弗洛伦斯, P., 和曾, A. 代码作为策略：用于具身控制的语言模型程序。在*ICRA
    会议录*，第9493–9500页。IEEE，2023年。
- en: 'Liu et al. (2023) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,
    M., Petroni, F., and Liang, P. Lost in the middle: How language models use long
    contexts. *CoRR*, abs/2307.03172, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2023）刘, N. F., 林, K., 休伊特, J., 帕兰贾佩, A., 贝维拉夸, M., 佩特罗尼, F., 和梁, P. 迷失在中间：语言模型如何使用长上下文。*CoRR*，abs/2307.03172，2023年。
- en: Miller et al. (2016) Miller, A. H., Fisch, A., Dodge, J., Karimi, A., Bordes,
    A., and Weston, J. Key-value memory networks for directly reading documents. In
    *Proceedings of EMNLP*, pp.  1400–1409\. The Association for Computational Linguistics,
    2016.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米勒等人（2016）米勒, A. H., 费希, A., 道奇, J., 卡里米, A., 博尔德斯, A., 和韦斯顿, J. 直接阅读文档的键值记忆网络。在*EMNLP
    会议录*，第1400–1409页。计算语言学协会，2016年。
- en: 'Nakano et al. (2021) Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L.,
    Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K.,
    Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J.
    Webgpt: Browser-assisted question-answering with human feedback. *CoRR*, abs/2112.09332,
    2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '中野等人（2021）中野, R., 希尔顿, J., 巴拉吉, S., 吴, J., 欧阳, L., 金, C., 赫斯, C., 贾因, S., 科萨拉朱,
    V., 桑德斯, W., 蒋, X., 科比, K., 埃伦杜, T., 克鲁格, G., 巴顿, K., 奈特, M., 切斯, B., 和舒尔曼, J.
    WebGPT: 浏览器辅助的问题回答与人类反馈。*CoRR*，abs/2112.09332，2021年。'
- en: OpenAI (2023) OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. GPT-4 技术报告。*CoRR*，abs/2303.08774，2023年。
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J.,
    Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
    P. F., Leike, J., and Lowe, R. Training language models to follow instructions
    with human feedback. In *Proceedings of NeurIPS*, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人（2022）欧阳, L., 吴, J., 蒋, X., 阿尔梅达, D., 韦恩赖特, C. L., 米什金, P., 张, C., 阿贾瓦尔,
    S., 斯拉马, K., 雷, A., 舒尔曼, J., 希尔顿, J., 凯尔顿, F., 米勒, L., 西门斯, M., 阿斯克尔, A., 韦林德,
    P., 克里斯蒂亚诺, P. F., 莱克, J., 和洛威, R. 用人类反馈训练语言模型以遵循指令。在*NeurIPS 会议录*，2022年。
- en: 'Park et al. (2023) Park, J. S., O’Brien, J. C., Cai, C. J., Morris, M. R.,
    Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human
    behavior. In *Proceedings of UIST*, pp.  2:1–2:22\. ACM, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕克等人（2023）帕克, J. S., 奥布莱恩, J. C., 蔡, C. J., 莫里斯, M. R., 梁, P., 和伯恩斯坦, M. S.
    生成代理：人类行为的互动模拟。在*UIST 会议录*，第2:1–2:22页。ACM，2023年。
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models. *CoRR*, abs/2309.00071,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '彭等人（2023）彭, B., 克斯内尔, J., 范, H., 和希波尔, E. Yarn: 大型语言模型的高效上下文窗口扩展。*CoRR*，abs/2309.00071，2023年。'
- en: 'Press et al. (2022) Press, O., Smith, N. A., and Lewis, M. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *Proceedings
    of ICLR*. OpenReview.net, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等（2022）Press, O., Smith, N. A., 和 Lewis, M. 短期训练，长期测试：具有线性偏差的注意力机制使输入长度外推成为可能。见
    *ICLR 会议论文集*。OpenReview.net，2022年。
- en: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z., and Sun, M. Communicative agents for software development. *CoRR*, abs/2307.07924,
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等（2023）Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu, Z.,
    和 Sun, M. 用于软件开发的沟通代理。*CoRR*，abs/2307.07924，2023年。
- en: Rae et al. (2020) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
    and Lillicrap, T. P. Compressive transformers for long-range sequence modelling.
    In *Proceedings of ICLR*. OpenReview.net, 2020.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2020）Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., 和 Lillicrap,
    T. P. 用于长范围序列建模的压缩变换器。见 *ICLR 会议论文集*。OpenReview.net，2020年。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *JMLR*, 21:140:1–140:67, 2020.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 通过统一文本到文本变换器探索迁移学习的极限。*JMLR*，21：140:1–140:67，2020年。
- en: 'Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all
    you need. *CoRR*, abs/1911.02150, 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2019）Shazeer, N. 快速变换器解码：一个写头就够了。*CoRR*，abs/1911.02150，2019年。
- en: Su (2023) Su, J. Rectified rotary position embeddings, 2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su（2023）Su, J. 纠正的旋转位置嵌入，2023年。
- en: 'Su et al. (2021) Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced
    transformer with rotary position embedding. *CoRR*, abs/2104.09864, 2021.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2021）Su, J., Lu, Y., Pan, S., Wen, B., 和 Liu, Y. Roformer：具有旋转位置嵌入的增强型变换器。*CoRR*，abs/2104.09864，2021年。
- en: Sukhbaatar et al. (2015) Sukhbaatar, S., Szlam, A., Weston, J., and Fergus,
    R. End-to-end memory networks. In *Proceedings of NeurIPS*, pp.  2440–2448, 2015.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等（2015）Sukhbaatar, S., Szlam, A., Weston, J., 和 Fergus, R. 端到端记忆网络。见
    *NeurIPS 会议论文集*，第2440–2448页，2015年。
- en: 'Sun et al. (2023) Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim,
    A., Chaudhary, V., Song, X., and Wei, F. A length-extrapolatable transformer.
    In *Proceedings of ACL*, pp.  14590–14604\. Association for Computational Linguistics,
    2023. doi: 10.18653/V1/2023.ACL-LONG.816.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等（2023）Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary,
    V., Song, X., 和 Wei, F. 一种长度可外推的变换器。见 *ACL 会议论文集*，第14590–14604页。计算语言学协会，2023年。doi:
    10.18653/V1/2023.ACL-LONG.816。'
- en: 'Tay et al. (2023) Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient
    transformers: A survey. *ACM Comput. Surv.*, 55(6):109:1–109:28, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay 等（2023）Tay, Y., Dehghani, M., Bahri, D., 和 Metzler, D. 高效变换器：综述。*ACM 计算机调查*，55(6)：109:1–109:28，2023年。
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models. *CoRR*, abs/2302.13971, 2023a.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A.,
    Joulin, A., Grave, E., 和 Lample, G. Llama：开放和高效的基础语言模型。*CoRR*，abs/2302.13971，2023a年。
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes,
    J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A.,
    Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann,
    I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D.,
    Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y.,
    Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R.,
    Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A.,
    Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang,
    S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation
    and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher,
    L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,
    J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao,
    Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A.,
    Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., 和 Scialom, T. Llama 2：开放的基础和微调聊天模型。*CoRR*，abs/2307.09288，2023b年。
- en: 'Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
    Michalewski, H., and Milos, P. Focused transformer: Contrastive training for context
    scaling. *CoRR*, abs/2307.03170, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski 等 (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski,
    H., 和 Milos, P. 集中变换器：用于上下文扩展的对比训练。 *CoRR*, abs/2307.03170, 2023。
- en: 'Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer:
    Self-attention with linear complexity. *CoRR*, abs/2006.04768, 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., 和 Ma, H. Linformer：具有线性复杂度的自注意力。
    *CoRR*, abs/2006.04768, 2020。
- en: Weston et al. (2015) Weston, J., Chopra, S., and Bordes, A. Memory networks.
    In *Proceedings of ICLR*, 2015.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston 等 (2015) Weston, J., Chopra, S., 和 Bordes, A. 记忆网络。 在 *ICLR 会议论文集*，2015。
- en: Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing
    transformers. In *Proceedings of ICLR*. OpenReview.net, 2022.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2022) Wu, Y., Rabe, M. N., Hutchins, D., 和 Szegedy, C. 记忆变换器。 在 *ICLR
    会议论文集*。 OpenReview.net, 2022。
- en: Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient
    streaming language models with attention sinks. *CoRR*, abs/2309.17453, 2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等 (2023) Xiao, G., Tian, Y., Chen, B., Han, S., 和 Lewis, M. 高效的流式语言模型与注意力接收器。
    *CoRR*, abs/2309.17453, 2023。
- en: Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P.,
    Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang,
    H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis,
    M., Wang, S., and Ma, H. Effective long-context scaling of foundation models.
    *CoRR*, abs/2309.16039, 2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等 (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou,
    R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H.,
    Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M.,
    Wang, S., 和 Ma, H. 有效的长上下文扩展基础模型。 *CoRR*, abs/2309.16039, 2023。
- en: Xu et al. (2023) Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian,
    S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context
    large language models. *CoRR*, abs/2310.03025, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2023) Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian,
    S., Bakhturina, E., Shoeybi, M., 和 Catanzaro, B. 检索遇见长上下文大语言模型。 *CoRR*, abs/2310.03025,
    2023。
- en: 'Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,
    Alberti, C., Ontañón, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed,
    A. Big bird: Transformers for longer sequences. In *Proceedings of NeurIPS*, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaheer 等 (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti,
    C., Ontañón, S., Pham, P., Ravula, A., Wang, Q., Yang, L., 和 Ahmed, A. Big bird：适用于更长序列的变换器。
    在 *NeurIPS 会议论文集*，2020。
- en: 'Zhang et al. (2023a) Zhang, X., Chen, Y., Hu, S., Wu, Q., Chen, J., Xu, Z.,
    Dai, Z., Han, X., Wang, S., Liu, Z., and Sun, M. Infinitebench: 128k long-context
    benchmark for language models, 2023a.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023a) Zhang, X., Chen, Y., Hu, S., Wu, Q., Chen, J., Xu, Z., Dai,
    Z., Han, X., Wang, S., Liu, Z., 和 Sun, M. Infinitebench：用于语言模型的 128k 长上下文基准，2023a。
- en: 'Zhang et al. (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C. W., Wang, Z., and Chen, B. H${}_{\mbox{2}}$o:
    Heavy-hitter oracle for efficient generative inference of large language models.
    *CoRR*, abs/2306.14048, 2023b.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R.,
    Song, Z., Tian, Y., Ré, C., Barrett, C. W., Wang, Z., 和 Chen, B. H${}_{\mbox{2}}$o：高效生成推理的重型预言机。
    *CoRR*, abs/2306.14048, 2023b。
- en: 'Zhao et al. (2019) Zhao, G., Lin, J., Zhang, Z., Ren, X., Su, Q., and Sun,
    X. Explicit sparse transformer: Concentrated attention through explicit selection.
    *CoRR*, abs/1912.11637, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2019) Zhao, G., Lin, J., Zhang, Z., Ren, X., Su, Q., 和 Sun, X. 明确稀疏变换器：通过明确选择集中注意力。
    *CoRR*, abs/1912.11637, 2019。
- en: 'Zhao et al. (2023) Zhao, L., Feng, X., Feng, X., Qin, B., and Liu, T. Length
    extrapolation of transformers: A survey from the perspective of position encoding.
    *CoRR*, abs/2312.17044, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2023) Zhao, L., Feng, X., Feng, X., Qin, B., 和 Liu, T. 变换器的长度外推：从位置编码的角度进行的综述。
    *CoRR*, abs/2312.17044, 2023。
