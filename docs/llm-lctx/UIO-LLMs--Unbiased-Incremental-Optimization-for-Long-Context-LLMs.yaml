- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:01:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:01:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UIO-LLMs：长上下文LLMs的无偏增量优化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18173](https://ar5iv.labs.arxiv.org/html/2406.18173)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18173](https://ar5iv.labs.arxiv.org/html/2406.18173)
- en: Wenhao Li¹, Mingbao Lin², Yunshan Zhong¹, Shuicheng Yan², Rongrong Ji¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李文浩¹，林明宝²，钟云山¹，严水成²，季荣荣¹
- en: ¹ Xiamen University ² Skywork AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 厦门大学 ² Skywork AI
- en: wenhaoli@stu.xmu.edu.cn, linmb001@outlook.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: wenhaoli@stu.xmu.edu.cn, linmb001@outlook.com
- en: zhongyunshan@stu.xmu.edu.cn, shuicheng.yan@kunlun-inc.com, rrji@xmu.edu.cn Corresponding
    Author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: zhongyunshan@stu.xmu.edu.cn, shuicheng.yan@kunlun-inc.com, rrji@xmu.edu.cn 通讯作者
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Managing long texts is challenging for large language models (LLMs) due to limited
    context window sizes. This study introduces UIO-LLMs, an unbiased incremental
    optimization approach for memory-enhanced transformers under long-context settings.
    We initially conceptualize the process as a streamlined encoder-decoder framework
    where the weights-shared encoder and decoder respectively encapsulate a context
    segment into memories and leverage these memories to predict outputs of the subsequent
    segment. Subsequently, by treating our memory-enhanced transformers as fully-connected
    recurrent neural networks (RNNs), we refine the training process using the Truncated
    Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative
    incremental optimization techniques. These techniques not only diminish time complexity
    but also address the bias in gradient computation through an unbiased optimization
    process. UIO-LLMs successfully handle long context, such as extending the context
    window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters,
    while keeping the inference cost nearly linear as context length increases. Our
    project is at [https://github.com/wenhaoli-xmu/UIO-LLMs](https://github.com/wenhaoli-xmu/UIO-LLMs).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 处理长文本对大型语言模型（LLMs）来说具有挑战性，因为其上下文窗口的大小有限。本研究介绍了UIO-LLMs，一种针对长上下文设置下的记忆增强型变换器的无偏增量优化方法。我们最初将这个过程概念化为一个简化的编码器-解码器框架，其中共享权重的编码器和解码器分别将上下文片段封装到记忆中，并利用这些记忆来预测后续片段的输出。随后，通过将我们的记忆增强型变换器视为全连接的递归神经网络（RNNs），我们利用截断时间反向传播（TBPTT）算法来改进训练过程，该算法结合了创新的增量优化技术。这些技术不仅减少了时间复杂度，还通过无偏优化过程解决了梯度计算中的偏差问题。UIO-LLMs成功处理了长上下文，例如将Llama2-7b-chat的上下文窗口从4K扩展到100K
    tokens，并且仅增加了2%的额外参数，同时在上下文长度增加时保持推理成本几乎线性。我们的项目网址是[https://github.com/wenhaoli-xmu/UIO-LLMs](https://github.com/wenhaoli-xmu/UIO-LLMs)。
- en: '*K*eywords Context compression  $\cdot$ Long-context LLMs'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 上下文压缩  $\cdot$ 长上下文LLMs'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The long-context reasoning capabilities of large language models (LLMs) [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)] are garnering increasing interest. The context
    window of LLMs can be likened to a computer’s memory, with a larger capacity offering
    greater flexibility and possibilities for developers. This enables them to integrate
    techniques such as retrieval-augmented generation (RAG) [[4](#bib.bib4)] and create
    various downstream applications such as question answering and reading comprehension [[5](#bib.bib5)].
    However, limited computational resources make it almost infeasible to pre-train
    models on lengthy texts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的长上下文推理能力[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)] 正在引起越来越多的关注。LLMs的上下文窗口可以比作计算机的内存，容量越大，开发者的灵活性和可能性就越大。这使他们能够集成检索增强生成（RAG）技术[[4](#bib.bib4)]，并创建各种下游应用，例如问答和阅读理解[[5](#bib.bib5)]。然而，有限的计算资源使得在长文本上预训练模型几乎不可行。
- en: Prevalent approach involves initially pretraining models using short texts and
    extending their ability to handle lengthy text through fine-tuning. It has been
    employed in LongChat [[6](#bib.bib6)], LongLora [[7](#bib.bib7)], Positional Interpolation [[8](#bib.bib8)],
    PoSE [[9](#bib.bib9)], Yarn [[10](#bib.bib10)], *etc*. However, the quadratic
    complexity inherent in the attention mechanism continues to pose a challenge to
    efficiency during the inference stage when processing lengthy texts. In addition
    to these fine-tuning-based methods, another strategy involves implementing suitable
    modifications during the inference stage to augment the effective context window
    size of the model. These tactics typically involve attention pruning, such as
    Streaming LLM [[11](#bib.bib11)], which manages the number of tokens by retaining
    only the nearest KV caches and the foremost KV caches. Nonetheless, for these
    pruning-based methods, the information from discarded tokens becomes difficult
    to utilize, resulting in varying degrees of performance decline.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的方法是首先使用短文本对模型进行预训练，然后通过微调扩展其处理长文本的能力。这在LongChat[[6](#bib.bib6)]、LongLora[[7](#bib.bib7)]、Positional
    Interpolation[[8](#bib.bib8)]、PoSE[[9](#bib.bib9)]、Yarn[[10](#bib.bib10)]等中得到了应用。然而，注意力机制固有的二次复杂度在处理长文本时继续对效率构成挑战。除了这些基于微调的方法外，另一种策略是在推理阶段实施适当的修改，以增强模型的有效上下文窗口大小。这些策略通常涉及注意力修剪，如Streaming
    LLM[[11](#bib.bib11)]，通过仅保留最近的KV缓存和最前面的KV缓存来管理令牌的数量。然而，对于这些基于修剪的方法，丢弃的令牌中的信息变得难以利用，导致性能下降的程度各异。
- en: '![Refer to caption](img/39b509f7b84d85cd81610ed103048d4b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39b509f7b84d85cd81610ed103048d4b.png)'
- en: 'Figure 1: Overall encoder-decoder architecture of our UIO-LLMs, initially splitting
    the text into multiple segments of length $l$-length segments, generating memory.
    Then, we merge memory with the next segment, for further processing by the decoder.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们UIO-LLMs的整体编码器-解码器架构，最初将文本拆分为多个长度为$l$的段，生成记忆。然后，我们将记忆与下一个段合并，以便解码器进一步处理。
- en: In this paper, we examine and recognize that transformer models [[12](#bib.bib12)]
    typically maintain a complete set of historical information attributed to the
    attention mechanism; conversely, recurrent neural networks (RNNs) are characterized
    by their retention of distilled historical insights, a consequence of their sequential
    data processing that emphasizes recent information in decision-making processes.
    These two architectures exhibit contrasting features in this respect. Certain
    techniques, such as Performer [[13](#bib.bib13)] and Linear Transformers [[14](#bib.bib14)],
    modify the attention computation sequence by employing kernel approaches [[15](#bib.bib15),
    [16](#bib.bib16)]. They compute the outer products of keys and values, accumulating
    them into a large matrix for data compression. This transforms the transformer
    into an RNN-like model that compresses all past information, weakening its ability
    to handle long-term dependencies. Balancing between storing comprehensive (transformer)
    and condensed (RNN) historical data is possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们考察并认识到，transformer模型[[12](#bib.bib12)]通常保持一整套由注意力机制提供的历史信息；相反，递归神经网络（RNNs）的特点是保留提炼的历史洞察，这源于其顺序数据处理，强调决策过程中的近期信息。这两种架构在这方面表现出对比特征。某些技术，如Performer[[13](#bib.bib13)]和Linear
    Transformers[[14](#bib.bib14)]，通过采用核方法[[15](#bib.bib15), [16](#bib.bib16)]来修改注意力计算序列。它们计算键和值的外积，将其积累到一个大矩阵中以进行数据压缩。这将transformer转变为类似RNN的模型，压缩所有过去的信息，削弱其处理长期依赖的能力。可以在存储全面（transformer）和精简（RNN）历史数据之间取得平衡。
- en: 'In this study, we propose the UIO-LLMs method, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context
    LLMs"), which leverages the decoder-only LLMs as a context compressor. Specifically,
    the context is divided into segments, each of which is appended with multiple
    “” tokens at its end. After the forward propagation through the encoder,
    the activation of the “” tokens have distilled contextual information, effectively
    forming a compact and informative memory representation. This representation can
    be transferred to the decoder as additional KV caches, via a transfer head consisting
    of two projection matrices. To minimize the introduction of additional parameters,
    we leverage LoRA [[17](#bib.bib17)] to fine-tune the encoder and the transfer
    head. This results in a mere 2% increase in parameters for Llama2-7b-chat [[18](#bib.bib18)].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项研究中，我们提出了UIO-LLMs方法，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UIO-LLMs:
    Unbiased Incremental Optimization for Long-Context LLMs")所示，该方法利用仅解码器的LLMs作为上下文压缩器。具体而言，上下文被分为多个片段，每个片段的末尾附加多个“”标记。在通过编码器进行前向传播后，“”标记的激活已经提炼出上下文信息，有效地形成了一个紧凑且信息丰富的记忆表示。这种表示可以通过由两个投影矩阵组成的转移头转移到解码器，作为额外的KV缓存。为了最小化引入额外参数，我们利用LoRA [[17](#bib.bib17)]对编码器和转移头进行微调。这使得Llama2-7b-chat [[18](#bib.bib18)]的参数仅增加了2%。'
- en: Regarding optimization, the interconnection of memory segments forms a structure
    akin to a fully-connected RNN. Consequently, Back Propagation Through Time (BPTT)
    is essential for optimization. However, it incurs a linear time and storage overhead
    that scales with the length of the input text. Hence, our research is centered
    on enhancing the efficiency of the BPTT algorithm. To this end, we introduce an
    incremental TBPTT algorithm, which is an adaptation of the Truncated BPTT method [[19](#bib.bib19)],
    and significantly reduces the time overhead by reordering the computation process
    in an incremental manner. Furthermore, despite the enhanced efficiency of incremental
    TBPTT, the inherent biased gradient estimation problem associated with the localized
    TBPTT window remains a hurdle for learning long-term dependencies. To overcome
    this challenge, we have further developed the Unbiased Incremental Optimization
    algorithm. This algorithm ensures unbiased gradient estimation, facilitating the
    training on texts of up to 100K in length with a constant compression ratio.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化，记忆片段的相互连接形成了类似于全连接RNN的结构。因此，反向传播时间（BPTT）对优化至关重要。然而，它带来了与输入文本长度成线性关系的时间和存储开销。因此，我们的研究集中于提高BPTT算法的效率。为此，我们引入了一种增量TBPTT算法，这是对截断BPTT方法[[19](#bib.bib19)]的改编，通过以增量方式重新排序计算过程，显著减少了时间开销。此外，尽管增量TBPTT提高了效率，但与局部TBPTT窗口相关的固有偏差梯度估计问题仍然是学习长期依赖关系的障碍。为克服这一挑战，我们进一步开发了无偏增量优化算法。该算法确保了无偏的梯度估计，使得可以以恒定的压缩比例对长度达到100K的文本进行训练。
- en: Notably, our UIO-LLMs surpass the performance and efficiency of prior memory-enhanced
    transformers, including RMT [[20](#bib.bib20)], AutoCompressor [[21](#bib.bib21)],
    Gist Tokens [[22](#bib.bib22)], and Activation Beacon [[23](#bib.bib23)]. It surpasses
    AutoCompressor on QA and summarization tasks without compromising long text generation
    quality. As for Activation Beacon, our model reduces trainable parameters, enables
    parallel compression, and lowers training costs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的UIO-LLMs超越了先前增强记忆的变压器模型，包括RMT [[20](#bib.bib20)]、AutoCompressor [[21](#bib.bib21)]、Gist
    Tokens [[22](#bib.bib22)]和Activation Beacon [[23](#bib.bib23)]的性能和效率。在问答和摘要任务中，其性能优于AutoCompressor，同时不影响长文本生成的质量。至于Activation
    Beacon，我们的模型减少了可训练参数，实现了并行压缩，并降低了训练成本。
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Memory-Enhanced Transformers. Recent studies highlight memory-enhanced transformers
    for long text extrapolation. Pioneering work, RMT [[20](#bib.bib20)], combines
    RNN with transformer for segment-level recurrence. AutoCompressor [[21](#bib.bib21)]
    improves this by using a fully-connected RNN, though its LongBench [[5](#bib.bib5)]
    performance can be enhanced. Activation Beacon [[23](#bib.bib23)] introduces two
    key improvements of direct migration of memory activation from the encoder to
    the decoder and a dedicated multi-head attention (MHA) module for memory. The
    BABILong [[24](#bib.bib24)] study shows that the GPT-2 [[25](#bib.bib25)] + RMT
    model outperforms advanced models like GPT-4 [[26](#bib.bib26)] and GPT-3.5 in
    handling extensive contextual information, underscoring the potential of memory-enhanced
    transformers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 增强记忆的变换器。最近的研究强调了用于长文本外推的记忆增强型变换器。开创性工作 RMT [[20](#bib.bib20)] 将 RNN 与变换器结合，实现了分段级别的递归。AutoCompressor
    [[21](#bib.bib21)] 通过使用全连接 RNN 改进了这一方法，尽管其 LongBench [[5](#bib.bib5)] 性能仍可提升。Activation
    Beacon [[23](#bib.bib23)] 引入了两个关键改进：将记忆激活从编码器直接迁移到解码器，以及为记忆设计的多头注意力 (MHA) 模块。BABILong
    [[24](#bib.bib24)] 研究表明，GPT-2 [[25](#bib.bib25)] + RMT 模型在处理广泛上下文信息时优于先进模型如 GPT-4
    [[26](#bib.bib26)] 和 GPT-3.5，突显了记忆增强型变换器的潜力。
- en: Context Distillation. Context distillation has emerged as an effective approach
    for knowledge compression and transfer. Early studies, such as Wingate’s research [[27](#bib.bib27)],
    focus on compressing prompts by replacing them with shorter learnable prompts.
    This method laid the foundation for subsequent research. Gist Tokens [[22](#bib.bib22)]
    advances this concept by training general-purpose summary tokens, allowing prompt
    compression without separate training. We utilize a similar approach with learnable
    prompts for context compression. The ICAE [[28](#bib.bib28)] model builds upon
    Gist Tokens, incorporating LoRA fine-tuning and an auto-encoding task for training.
    With a four-times compression ratio, ICAE demonstrates near-perfect input reconstruction
    accuracy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文蒸馏。上下文蒸馏已成为知识压缩和转移的有效方法。早期研究，如 Wingate 的研究 [[27](#bib.bib27)]，专注于通过用更短的可学习提示替换提示来压缩提示。这一方法为后续研究奠定了基础。Gist
    Tokens [[22](#bib.bib22)] 推进了这一概念，通过训练通用总结令牌，实现了提示压缩而无需单独训练。我们采用类似的方法，用可学习提示进行上下文压缩。ICAE
    [[28](#bib.bib28)] 模型在 Gist Tokens 的基础上进行扩展，结合了 LoRA 微调和自编码任务进行训练。ICAE 具有四倍的压缩比，展示了近乎完美的输入重建准确性。
- en: Unbiased BPTT Approximation. Training RNNs often relies on the resource-intensive
    Back-Propagation Through Time method (BPTT) [[29](#bib.bib29)]. Researchers have
    proposed unbiased approximations like NoBackTrack [[30](#bib.bib30)] and UORO [[31](#bib.bib31)]
    to reduce memory and compute overhead, opening new possibilities for efficient
    sequence model training. ARTBP [[32](#bib.bib32)] mitigates noise by using a flexible
    memory approach and incorporating compensatory factors, maintaining accuracy and
    efficiency for long sequences. While these methods have advanced sequence model
    research, they are not directly applicable to memory-enhanced transformers due
    to their focus on regular RNNs and lack of consideration for specific constraints
    in memory-enhanced transformers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无偏 BPTT 近似。训练 RNN 通常依赖于资源密集型的时间反向传播方法 (BPTT) [[29](#bib.bib29)]。研究人员提出了无偏近似方法，如
    NoBackTrack [[30](#bib.bib30)] 和 UORO [[31](#bib.bib31)]，以减少内存和计算开销，为高效序列模型训练开辟了新可能。ARTBP
    [[32](#bib.bib32)] 通过使用灵活的内存方法和引入补偿因子来减轻噪声，在长序列的准确性和效率方面保持了较高水平。虽然这些方法推动了序列模型研究的进展，但由于其关注于常规
    RNN 并未考虑记忆增强型变换器的特定约束，因此不适用于记忆增强型变换器。
- en: 3 Methodology
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/d67b14f730ffb1ac16556fc56a012355.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d67b14f730ffb1ac16556fc56a012355.png)'
- en: 'Figure 2: We enhance the encoder’s summary ability by using LoRA fine-tuning
    and adding a transfer head to each layer, which aligns the “” in each encoder
    layer with its matching decoder layer.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们通过使用 LoRA 微调和在每一层中添加一个迁移头来增强编码器的总结能力，这使得每个编码器层中的“”与其匹配的解码器层对齐。
- en: 3.1 Overall Framework
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 总体框架
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UIO-LLMs: Unbiased Incremental
    Optimization for Long-Context LLMs") showcases our proposed UIO-LLMs architecture,
    which uses an encoder-decoder framework enhanced with “” tokens to capture
    the preceding text’s essence. Additionally, we introduce a novel algorithm for
    unbiased gradient estimation, enabling efficient training of memory-enhanced transformers
    on long texts without significantly increasing parameters.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UIO-LLMs: Unbiased Incremental Optimization
    for Long-Context LLMs")展示了我们提出的 UIO-LLMs 架构，它使用增强了“”标记的编码器-解码器框架来捕捉前文的本质。此外，我们引入了一种新算法用于无偏梯度估计，实现了对长文本的记忆增强型变换器的高效训练，而不会显著增加参数。'
- en: 3.2 Streamlined Encoder-Decoder Architecture
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 精简编码器-解码器架构
- en: 'Our method features an encoder-decoder structure, allowing for independent
    input handling by the encoder and parallel compression of lengthy texts. By partitioning
    the long text $X$ at every layer:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法具有编码器-解码器结构，允许编码器独立处理输入并并行压缩长文本。通过在每一层对长文本 $X$ 进行分段：
- en: '|  | $\begin{gathered}Q\leftarrow hW_{Q}^{\text{Lora}},\quad K\leftarrow hW_{K},\quad
    V\leftarrow hW_{V}^{\text{Lora}},\quad O\leftarrow\text{MHA}(Q,K,V)W_{O},\end{gathered}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}Q\leftarrow hW_{Q}^{\text{Lora}},\quad K\leftarrow hW_{K},\quad
    V\leftarrow hW_{V}^{\text{Lora}},\quad O\leftarrow\text{MHA}(Q,K,V)W_{O},\end{gathered}$
    |  | (1) |'
- en: 'where $h$, which are then utilized to perform linear transformations on the
    preserved memory activations of each layer. This process culminates in the generation
    of the KV cache:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$，然后利用这些对每一层的保存记忆激活进行线性变换。这个过程最终生成了 KV 缓存：
- en: '|  | $\begin{gathered}h_{\text{ord}},h_{\text{mem}}\leftarrow\text{split}(h),\quad
    K_{\text{mem}}\leftarrow h_{\text{mem}}W_{K}^{\text{Lora*}},\quad V_{\text{mem}}\leftarrow
    h_{\text{mem}}W_{V}^{\text{Lora*}}.\end{gathered}$ |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{gathered}h_{\text{ord}},h_{\text{mem}}\leftarrow\text{split}(h),\quad
    K_{\text{mem}}\leftarrow h_{\text{mem}}W_{K}^{\text{Lora*}},\quad V_{\text{mem}}\leftarrow
    h_{\text{mem}}W_{V}^{\text{Lora*}}.\end{gathered}$ |  | (2) |'
- en: 'To distinguish it from the previous notation, we employ the symbol * in Eq. ([2](#S3.E2
    "In 3.2 Streamlined Encoder-Decoder Architecture ‣ 3 Methodology ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs")), which signifies the use of
    a separate instance of LoRA. Subsequently, we integrate the newly obtained KV
    cache, specifically $K_{\text{mem}}$ of the Llama2-7b-chat model [[18](#bib.bib18)],
    contributing to an efficient and optimized system. Conversely, the Activation
    Beacon [[23](#bib.bib23)] method significantly contributes to a more substantial
    portion of the model’s trainable parameters, accounting for over 33% to fine-tune
    each attention layer.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '为了与之前的符号区分，我们在公式 ([2](#S3.E2 "In 3.2 Streamlined Encoder-Decoder Architecture
    ‣ 3 Methodology ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context
    LLMs"))中使用符号 *，表示使用了一个单独的 LoRA 实例。随后，我们将新获得的 KV 缓存，特别是 Llama2-7b-chat 模型的 $K_{\text{mem}}$
    [[18](#bib.bib18)]，整合进一个高效且优化的系统中。相反，Activation Beacon [[23](#bib.bib23)] 方法显著增加了模型的可训练参数的比例，占比超过
    33%，以微调每个注意力层。'
- en: In the token-by-token generation stage, once the aggregate length of the generated
    sequence $x_{k+1}^{\prime}$ to the encoder for further compression and remove
    the associated KV caches from the decoder.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐词生成阶段，一旦生成的序列 $x_{k+1}^{\prime}$ 的总长度达到一定值，我们将其送回编码器以进行进一步压缩，并从解码器中移除相关的 KV
    缓存。
- en: 3.3 Unbiased Incremental Optimization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 无偏增量优化
- en: 3.3.1 Memory-Enhanced Transformers are Fully-Connected RNNs
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 记忆增强型变换器是全连接的 RNN
- en: '![Refer to caption](img/d098367c902a21da9cc41ad958086049.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d098367c902a21da9cc41ad958086049.png)'
- en: 'Figure 3: Our memory-enhanced transformers can be conceptualized as fully-connected
    RNNs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们的记忆增强型变换器可以被概念化为全连接的 RNN。
- en: 'We realize, as illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3.3.1 Memory-Enhanced
    Transformers are Fully-Connected RNNs ‣ 3.3 Unbiased Incremental Optimization
    ‣ 3 Methodology ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context
    LLMs"), our memory-enhanced transformers are analogous to fully-connected RNNs,
    the general formula of which can be defined as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们意识到，如图 [3](#S3.F3 "Figure 3 ‣ 3.3.1 Memory-Enhanced Transformers are Fully-Connected
    RNNs ‣ 3.3 Unbiased Incremental Optimization ‣ 3 Methodology ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs")所示，我们的记忆增强型变换器类似于全连接的 RNN，其通用公式可以定义为：'
- en: '|  | $J_{t},m_{t}=f_{t}(x_{t},[m_{1},m_{2},...,m_{t-1}]\,&#124;\,\Theta),$
    |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{t},m_{t}=f_{t}(x_{t},[m_{1},m_{2},...,m_{t-1}]\,&#124;\,\Theta),$
    |  | (3) |'
- en: where for each segment $t$, and $m_{t}$ represents all model parameters, encompassing
    those of the encoder, decoder, and transfer head. Specifically, the decoder’s
    parameters are frozen, while the encoder’s parameters and those of the transfer
    head are fine-tuned using LoRA [[17](#bib.bib17)]. Notice the concept of utilizing
    BPTT training for memory-enhanced transformers, treating them as RNNs and initially
    focusing on the last-step memory, was pioneered in RMT [[20](#bib.bib20)]. We
    extend by considering all prior memories, aligning more closely with fully-connected
    RNNs that allow each time step to leverage the complete history of memories.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个段$t$，$m_{t}$表示所有模型参数，包括编码器、解码器和传输头的参数。具体来说，解码器的参数被冻结，而编码器的参数和传输头的参数则使用LoRA进行微调[[17](#bib.bib17)]。值得注意的是，将BPTT训练用于内存增强的变换器，视其为RNN，并最初关注最后一步的记忆，最早由RMT[[20](#bib.bib20)]提出。我们扩展了考虑所有先前的记忆，更加贴近允许每个时间步利用完整记忆历史的全连接RNN。
- en: 'Optimization. To update $\Theta$, we first derive its gradient as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 优化。为了更新$\Theta$，我们首先推导其梯度为：
- en: '|  | $\displaystyle\nabla_{\Theta}=\sum_{t=1}^{T}{\frac{\partial J_{t}}{\partial\Theta}}=\sum_{t=1}^{T}\sum_{s=1}^{t-1}\Bigg{[}\frac{\partial
    J_{t}}{\partial m_{s}}\cdot\frac{\partial m_{s}}{\partial\Theta}$ |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\Theta}=\sum_{t=1}^{T}{\frac{\partial J_{t}}{\partial\Theta}}=\sum_{t=1}^{T}\sum_{s=1}^{t-1}\Bigg{[}\frac{\partial
    J_{t}}{\partial m_{s}}\cdot\frac{\partial m_{s}}{\partial\Theta}$ |  | (4) |'
- en: '|  |  | $\displaystyle+\sum_{\underset{\text{s.t.}\,s” tokens. All LoRA modules have a consistent
    configuration of $r=128$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们使用了Llama2-7b-chat模型[[18](#bib.bib18)]，上下文窗口为1K，压缩比为32和8。更大的窗口提高了性能，但增加了训练和推断的成本。可训练的参数包括编码器和转移头中的LoRA模块，以及“”令牌。所有LoRA模块具有一致的配置，$r=128$。
- en: 'Our training, upon 8 RTX 3090 GPUs, uses a combined dataset: The first, from
    Activation Beacon [[23](#bib.bib23)], comprises RedPajama [[37](#bib.bib37)] and
    LongAlpaca [[38](#bib.bib38)], taking up 95%. The second combines long texts from
    LongData-Corpus [[39](#bib.bib39)] and RedPajama [[37](#bib.bib37)]. For models
    with compression rates of 32 and 8, max token lengths are set to 100K and 25K.
    We use unbiased incremental TBPTT with $S=2$ and store up to 3 time steps. We
    employ Adam with a learning rate of 1e-4 and a cosine scheduler.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练在8台RTX 3090 GPU上进行，使用了一个组合数据集：第一个来自Activation Beacon[[23](#bib.bib23)]，包括RedPajama[[37](#bib.bib37)]和LongAlpaca[[38](#bib.bib38)]，占95%。第二个结合了LongData-Corpus[[39](#bib.bib39)]和RedPajama[[37](#bib.bib37)]的长文本。对于压缩率为32和8的模型，最大令牌长度分别设置为100K和25K。我们使用了无偏增量TBPTT，$S=2$，并存储最多3个时间步。我们使用Adam优化器，学习率为1e-4，并采用余弦调度器。
- en: 4.2 Performance Comparisons
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 性能比较
- en: '![Refer to caption](img/095f9ee54c2e83a894eb46394048b9d6.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/095f9ee54c2e83a894eb46394048b9d6.png)'
- en: 'Figure 5: The pipeline for the auto-encoding task.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：自编码任务的流程图。
- en: 'Figure 6: UIO-LLMs’ results on auto-encoding tasks assessed by BLEU-4 and Rouge-L
    for evaluation.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：UIO-LLMs在自编码任务上的结果，通过BLEU-4和Rouge-L进行评估。
- en: Compression Ratio BLEU-4$\uparrow$ 8 0.9851 0.993 32 0.5948 0.762
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩比 BLEU-4$\uparrow$ 8 0.9851 0.993 32 0.5948 0.762
- en: Auto-Encoding Task. To evaluate UIO-LLMs’ long text compression, we use the
    ICAE auto-encoding task that compresses and reconstructs text [[28](#bib.bib28)].
    Training details follow the standard process, except for using the MiniPile corpus [[40](#bib.bib40)]
    as training data, filtered to 10K 1K-token samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码任务。为了评估UIO-LLMs的长文本压缩，我们使用ICAE自编码任务来压缩和重建文本[[28](#bib.bib28)]。训练细节遵循标准流程，只是在训练数据上使用了MiniPile语料库[[40](#bib.bib40)]，过滤到10K
    1K-token样本。
- en: 'The training pipeline, as depicted in Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Performance
    Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased Incremental Optimization
    for Long-Context LLMs"), involves encoding the 1K-token samples, producing memory
    representation, and reconstructing the 1K-token inputs. We assess reconstruction
    accuracy on a 100-sample test set using BLEU-4 and Rouge-L metrics. Table [6](#S4.F6
    "Figure 6 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs") reports the results. The model
    performs well, with Rouge-L scores of 0.993 and 0.762 for compression ratios of
    8 and 32, respectively. To obtain a more visually comprehensive understanding
    of the reconstruction results, we chose the first sample from the test set and
    displayed the reconstruction outcomes using compression ratios of $8$ in Figure [7](#S4.F7
    "Figure 7 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs"). We can see that our method mostly
    recover the original context, demonstrating the capability of our method for lossless
    compression of long contexts. This significant reconstruction performance lays
    the foundation for utilizing memory to inference.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '训练流程，如图 [6](#S4.F6 "Figure 6 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation
    ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs")所示，包括对1K-token样本进行编码，生成记忆表示，然后重建1K-token输入。我们在100个样本的测试集上使用BLEU-4和Rouge-L指标评估重建准确性。表 [6](#S4.F6
    "Figure 6 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs")报告了结果。模型表现良好，压缩比为8和32时，Rouge-L得分分别为0.993和0.762。为了获得更直观的重建结果，我们从测试集中选择了第一个样本，并在图 [7](#S4.F7
    "Figure 7 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs")中展示了使用压缩比$8$的重建结果。我们可以看到我们的方法大多数情况下恢复了原始上下文，展示了我们方法对长上下文的无损压缩能力。这一显著的重建性能为利用记忆进行推断奠定了基础。'
- en: '![Refer to caption](img/e526ff540eed77f28fb5f5291305c1ed.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e526ff540eed77f28fb5f5291305c1ed.png)'
- en: 'Figure 7: A case study of the auto-encoding task shows near-lossless compression
    at a ratio of 8\. Even with a ratio of 32, reconstructed paragraphs retained meaning
    with minor wording changes.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：自动编码任务的案例研究显示了 8 倍压缩比下接近无损的压缩效果。即使在 32 倍压缩比下，重建的段落仍保留了意义，仅有少量措辞变化。
- en: 'Table 1: Perplexity results on PG19 and Proof-Pile show UIO-LLMs’ strong long-context
    modeling. Context window sizes are 1K or 2K. * marks original paper results. “OOM”
    stands for Out-of-Memory error, which we’ve encountered upon 8 RTX 3090 GPUs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PG19 和 Proof-Pile 上的困惑度结果显示了 UIO-LLMs 在长上下文建模中的强大能力。上下文窗口大小为 1K 或 2K。* 标记为原始论文结果。“OOM”表示内存溢出错误，我们在
    8 个 RTX 3090 GPU 上遇到了该错误。
- en: '| Method | Compression Ratio | PG19$\downarrow$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 压缩比 | PG19$\downarrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 4K | 16K | 25K | 32K | 100K | 4K | 16K | 25K | 32K | 100K |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 4K | 16K | 25K | 32K | 100K | 4K | 16K | 25K | 32K | 100K |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Streaming LLM | 1 | 9.50 | 9.83 | 9.88 | 9.89 | 9.89 | 6.47 | 5.13 | 4.62
    | 4.44 | 3.94 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Streaming LLM | 1 | 9.50 | 9.83 | 9.88 | 9.89 | 9.89 | 6.47 | 5.13 | 4.62
    | 4.44 | 3.94 |'
- en: '| LongChat-7B | 9.93 | 9.49 | OOM | OOM | - | 5.65 | 3.90 | OOM | OOM | - |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7B | 9.93 | 9.49 | OOM | OOM | - | 5.65 | 3.90 | OOM | OOM | - |'
- en: '| LongAlpaca-7B | 9.96 | 9.75 | OOM | OOM | - | 6.31 | 3.97 | OOM | OOM | -
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LongAlpaca-7B | 9.96 | 9.75 | OOM | OOM | - | 6.31 | 3.97 | OOM | OOM | -
    |'
- en: '| Beacon-2K | 32 | 8.67 | 8.41 | 8.42 | 8.41 | 9.24 | 5.70 | 4.13 | 3.67 |
    3.50 | 4.23 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Beacon-2K | 32 | 8.67 | 8.41 | 8.42 | 8.41 | 9.24 | 5.70 | 4.13 | 3.67 |
    3.50 | 4.23 |'
- en: '| Beacon-1K | 8.56 | 8.54 | 8.58 | 8.59 | 8.83 | 5.79 | 4.33 | 3.86 | 3.70
    | 3.33 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Beacon-1K | 8.56 | 8.54 | 8.58 | 8.59 | 8.83 | 5.79 | 4.33 | 3.86 | 3.70
    | 3.33 |'
- en: '| UIO-LLMs | 8.51 | 8.32 | 8.30 | 8.30 | 8.28 | 5.78 | 4.26 | 3.72 | 3.53 |
    3.19 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| UIO-LLMs | 8.51 | 8.32 | 8.30 | 8.30 | 8.28 | 5.78 | 4.26 | 3.72 | 3.53 |
    3.19 |'
- en: '| Beacon-2K | 8 | 8.52 | 8.13 | 9.31 | - | - | 5.52 | 3.87 | 4.44 | - | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Beacon-2K | 8 | 8.52 | 8.13 | 9.31 | - | - | 5.52 | 3.87 | 4.44 | - | - |'
- en: '| Beacon-1K | 8.26 | 8.13 | 8.16 | - | - | 5.41 | 3.91 | 3.47 | - | - |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Beacon-1K | 8.26 | 8.13 | 8.16 | - | - | 5.41 | 3.91 | 3.47 | - | - |'
- en: '| UIO-LLMs | 8.27 | 8.09 | 8.09 | - | - | 5.43 | 3.84 | 3.41 | - | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| UIO-LLMs | 8.27 | 8.09 | 8.09 | - | - | 5.43 | 3.84 | 3.41 | - | - |'
- en: 'Long-Context Language Modeling. We report UIO-LLMs’ long text language modeling
    using PG19 [[35](#bib.bib35)] and a sampled subset of Proof-pile [[36](#bib.bib36)]
    in Table [1](#S4.T1 "Table 1 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation
    ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs"). We test
    compression rates of 8 and 32 with a 1K context window. Compared baselines include
    LongChat-7B-v1.5-32K [[6](#bib.bib6)], LongAlpaca-7B-32K [[7](#bib.bib7)], Streaming
    LLM [[11](#bib.bib11)], and Activation Beacon [[23](#bib.bib23)]. Compared to
    training-free Streaming LLM, UIO-LLMs benefit from longer contexts with a continuous
    decrease in perplexity. Versus Activation Beacon, UIO-LLMs achieve lower perplexity.
    Activation Beacon’s performance declines at 100K tokens, but UIO-LLMs maintain
    strong performance.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '长上下文语言建模。我们报告了 UIO-LLMs 使用 PG19 [[35](#bib.bib35)] 和 Proof-pile 的抽样子集 [[36](#bib.bib36)]
    在表 [1](#S4.T1 "Table 1 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs:
    Unbiased Incremental Optimization for Long-Context LLMs") 中的长文本语言建模。我们测试了 1K 上下文窗口下的
    8 倍和 32 倍压缩率。对比基线包括 LongChat-7B-v1.5-32K [[6](#bib.bib6)]、LongAlpaca-7B-32K [[7](#bib.bib7)]、Streaming
    LLM [[11](#bib.bib11)] 和 Activation Beacon [[23](#bib.bib23)]。与无训练的 Streaming
    LLM 相比，UIO-LLMs 从更长的上下文中受益，困惑度持续下降。与 Activation Beacon 相比，UIO-LLMs 实现了更低的困惑度。Activation
    Beacon 的性能在 100K 令牌下下降，而 UIO-LLMs 维持了强劲的性能。'
- en: 'Table 2: Results on three LongBench tasks: Single-Doc QA (NarrativeQA, QASper,
    MultiFieldQA-en/zh, denoted as 1-1 to 1-4), Multi-Doc QA (HotpotQA, 2WikiMQA,
    Musique, DuReader, denoted as 2-1 to 2-4), and Summarization (GovReport, QMSum,
    Multi-News, VCSUM, denoted as 3-1 to 3-4). Results for Llama2-7b-chat-4K, LongChat-7B,
    and LongAlpaca-7B are from paper [[38](#bib.bib38)].'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：三项 LongBench 任务的结果：单文档问答（NarrativeQA、QASper、MultiFieldQA-en/zh，标记为 1-1 至
    1-4）、多文档问答（HotpotQA、2WikiMQA、Musique、DuReader，标记为 2-1 至 2-4）和摘要（GovReport、QMSum、Multi-News、VCSUM，标记为
    3-1 至 3-4）。Llama2-7b-chat-4K、LongChat-7B 和 LongAlpaca-7B 的结果来自论文 [[38](#bib.bib38)]。
- en: '| Method | Ratio | Single-Doc QA$\uparrow$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 比率 | 单文档问答$\uparrow$ |'
- en: '| 1-1 | 1-2 | 1-3 | 1-4 | 2-1 | 2-2 | 2-3 | 2-4 | 3-1 | 3-2 | 3-3 | 3-4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 1-1 | 1-2 | 1-3 | 1-4 | 2-1 | 2-2 | 2-3 | 2-4 | 3-1 | 3-2 | 3-3 | 3-4 |'
- en: '| Llama2-7B-chat-4K | 1 | 18.7 | 19.2 | 36.8 | 11.9 | 25.4 | 32.8 | 9.4 | 5.2
    | 27.3 | 20.8 | 25.8 | 0.2 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-chat-4K | 1 | 18.7 | 19.2 | 36.8 | 11.9 | 25.4 | 32.8 | 9.4 | 5.2
    | 27.3 | 20.8 | 25.8 | 0.2 |'
- en: '| LongChat-7B | 16.9 | 27.7 | 41.4 | 29.1 | 31.5 | 20.6 | 9.7 | 19.5 | 30.8
    | 22.7 | 26.4 | 9.9 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7B | 16.9 | 27.7 | 41.4 | 29.1 | 31.5 | 20.6 | 9.7 | 19.5 | 30.8
    | 22.7 | 26.4 | 9.9 |'
- en: '| LongAlpaca-7B | 19.8 | 29.1 | 37.15 | 8.48 | 37.01 | 30.26 | 17.14 | 15.25
    | 31.53 | 24.13 | 27.74 | 0.46 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LongAlpaca-7B | 19.8 | 29.1 | 37.15 | 8.48 | 37.01 | 30.26 | 17.14 | 15.25
    | 31.53 | 24.13 | 27.74 | 0.46 |'
- en: '| Beacon | 32 | 21.03 | 17.12 | 21.74 | 19.80 | 34.92 | 26.76 | 14.96 | 16.10
    | 21.33 | 22.01 | 23.06 | 11.73 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Beacon | 32 | 21.03 | 17.12 | 21.74 | 19.80 | 34.92 | 26.76 | 14.96 | 16.10
    | 21.33 | 22.01 | 23.06 | 11.73 |'
- en: '| UIO-LLMs | 20.10 | 18.90 | 21.83 | 23.03 | 32.80 | 31.16 | 15.53 | 17.05
    | 21.57 | 21.41 | 22.08 | 9.66 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| UIO-LLMs | 20.10 | 18.90 | 21.83 | 23.03 | 32.80 | 31.16 | 15.53 | 17.05
    | 21.57 | 21.41 | 22.08 | 9.66 |'
- en: '| Beacon | 8 | 9.6 | 24.46 | 28.25 | 25.02 | 39.09 | 31.07 | 17.38 | 13.48
    | 24.26 | 21.95 | 24.32 | 12.06 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Beacon | 8 | 9.6 | 24.46 | 28.25 | 25.02 | 39.09 | 31.07 | 17.38 | 13.48
    | 24.26 | 21.95 | 24.32 | 12.06 |'
- en: '| UIO-LLMs | 7.64 | 26.84 | 30.12 | 25.45 | 39.35 | 33.57 | 18.03 | 12.65 |
    24.89 | 20.34 | 25.34 | 11.21 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| UIO-LLMs | 7.64 | 26.84 | 30.12 | 25.45 | 39.35 | 33.57 | 18.03 | 12.65 |
    24.89 | 20.34 | 25.34 | 11.21 |'
- en: '| Average Length | 18,409 | 3,619 | 4,559 | 6,707 | 9,151 | 4,887 | 11,214
    | 15,768 | 8,734 | 10,614 | 2,113 | 15,380 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 平均长度 | 18,409 | 3,619 | 4,559 | 6,707 | 9,151 | 4,887 | 11,214 | 15,768 |
    8,734 | 10,614 | 2,113 | 15,380 |'
- en: 'Downstream Long-Text Tasks. We compare the performance of our UIO-LLMs on long-context
    tasks from LongBench with several Llama2-7b/Llama2-7b-chat [[18](#bib.bib18)]
    based long-context models, including RoPE extrapolation based methods LongChat-7B-v1.5-32K [[6](#bib.bib6)]
    and LongAlpaca-7B-32K [[7](#bib.bib7)], as well as Activation Beacon [[23](#bib.bib23)],
    which shares a similar architecture to our models. UIO-LLMs and Activation Beacon
    use a 1K context window and compression ratios of 8 and 32. Table [2](#S4.T2 "Table
    2 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased Incremental
    Optimization for Long-Context LLMs") shows UIO-LLMs’ merits from extra context,
    competitive results with 32K methods using only 1K, and improved performance with
    lower compression. 1) UIO-LLMs excel compared to Llama2-7b-chat-4K and Streaming
    LLM due to their additional context, highlighting the efficacy of our memory mechanism.
    2) Despite a 1K context window, UIO-LLMs achieve competitive results with RoPE-based
    LongChat-7B and LongAlpaca-7B, utilizing a 32K window. 3) UIO-LLMs benefit from
    a reduced compression ratio, showing efficient use of finer-grained memory and
    ability to extract information. Notice UIO-LLMs show comparable results to Beacon
    but with only 2% parameters increase, which contrast sharply with Beacon’s 33%.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '下游长文本任务。我们将我们的UIO-LLMs在LongBench上的长上下文任务表现与几个基于Llama2-7b/Llama2-7b-chat[[18](#bib.bib18)]的长上下文模型进行比较，包括基于RoPE外推的方法LongChat-7B-v1.5-32K[[6](#bib.bib6)]和LongAlpaca-7B-32K[[7](#bib.bib7)]，以及与我们模型架构类似的Activation
    Beacon[[23](#bib.bib23)]。UIO-LLMs和Activation Beacon使用1K的上下文窗口和8与32的压缩比。表[2](#S4.T2
    "Table 2 ‣ 4.2 Performance Comparisons ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs")显示了UIO-LLMs在额外上下文下的优点，使用仅1K却与32K方法竞争的结果，以及在更低压缩比下的性能提升。1)
    与Llama2-7b-chat-4K和Streaming LLM相比，UIO-LLMs表现优异，突显了我们的记忆机制的有效性。2) 尽管上下文窗口为1K，UIO-LLMs在RoPE基础的LongChat-7B和LongAlpaca-7B上取得了竞争性结果，后者使用32K窗口。3)
    UIO-LLMs从减少的压缩比中获益，展示了精细化记忆的高效利用和提取信息的能力。注意，UIO-LLMs与Beacon的结果相当，但参数增加仅2%，而Beacon的增加为33%。'
- en: 4.3 Ablation Studies
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Analysis on Incremental TBPTT. Since incremental TBPTT offers the same gradient
    computation results as TBPTT, our experiments prioritize evaluating its time and
    memory efficiency. To speed up the process, we use a smaller context window of
    16 with a 4$\times$ compression ratio, significantly reducing computational overhead.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 增量TBPTT分析。由于增量TBPTT提供与TBPTT相同的梯度计算结果，我们的实验优先评估其时间和内存效率。为了加快过程，我们使用了一个较小的16上下文窗口和4$\times$压缩比，显著降低了计算开销。
- en: 'Figure 9: Time and memory overhead of our incremental TBPTT compared to TBPTT.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：我们的增量TBPTT与TBPTT的时间和内存开销对比。
- en: 'We train on a text paragraph with 2048 tokens, divided into 128 segments, matching
    our context window size. To simplify, we limit iterations to 10\. We present results
    in Figure [9](#S4.F9 "Figure 9 ‣ 4.3 Ablation Studies ‣ 4 Experimentation ‣ UIO-LLMs:
    Unbiased Incremental Optimization for Long-Context LLMs"). Figure LABEL:fig:time_cost
    shows that incremental TBPTT’s time overhead remains steady as the TBPTT window
    size increases, indicating minimal impact on time complexity. Both incremental
    TBPTT and TBPTT show a linear memory increase with the window size in Figure LABEL:fig:mem_cost.
    However, incremental TBPTT consistently uses half the memory due to our optimized
    implementation, which retains only the encoder’s computational graph. This optimization
    significantly reduces memory usage, making it more feasible for large TBPTT windows.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在包含 2048 个标记的文本段落上进行训练，将其分成 128 个片段，匹配我们的上下文窗口大小。为了简化，我们将迭代次数限制为 10\. 结果如图 [9](#S4.F9
    "Figure 9 ‣ 4.3 Ablation Studies ‣ 4 Experimentation ‣ UIO-LLMs: Unbiased Incremental
    Optimization for Long-Context LLMs")所示。图 LABEL:fig:time_cost 显示增量 TBPTT 的时间开销在
    TBPTT 窗口大小增加时保持稳定，表明对时间复杂度的影响很小。图 LABEL:fig:mem_cost 中的增量 TBPTT 和 TBPTT 都显示出内存随窗口大小线性增加。然而，由于我们优化的实现仅保留了编码器的计算图，增量
    TBPTT 一直使用一半的内存。这种优化显著减少了内存使用，使得大 TBPTT 窗口更具可行性。'
- en: Analysis on Unbiased Incremental TBPTT. To assess the accuracy of unbiased incremental
    TBPTT in gradient estimation, we compare its gradients with vanilla TBPTT’s. We
    expect high similarity, indicating our method’s accuracy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对无偏增量 TBPTT 的分析。为了评估无偏增量 TBPTT 在梯度估计中的准确性，我们将其梯度与普通 TBPTT 的梯度进行比较。我们期望高度相似，这表明我们方法的准确性。
- en: 'Table 3: A statistical analysis compares the L2 norm ratios of gradients from
    two algorithms. Key findings: 1) The compensation factor is vital for accurate
    gradient estimation. 2) Variance decreases with increasing window size $S$, enhancing
    estimation accuracy.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：统计分析比较了两种算法的梯度 L2 范数比率。主要发现：1) 补偿因子对于准确的梯度估计至关重要。2) 方差随着窗口大小 $S$ 的增加而减少，从而提高了估计的准确性。
- en: '| Statistics | w/o Factor | w/ Factor |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 统计数据 | 无因子 | 有因子 |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $S=1$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $S=1$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Mean | 0.676 | 0.818 | 0.935 | 1.000 | 0.994 | 0.980 | 1.024 | 0.999 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.676 | 0.818 | 0.935 | 1.000 | 0.994 | 0.980 | 1.024 | 0.999 |'
- en: '| Variance | 0.112 | 0.041 | 0.008 | 3e-5 | 0.039 | 0.038 | 0.009 | 4e-5 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 方差 | 0.112 | 0.041 | 0.008 | 3e-5 | 0.039 | 0.038 | 0.009 | 4e-5 |'
- en: We use a context window of 128, compression ratio of 8, and compute gradients
    for parameters $\Theta$ for accurate gradient estimation. Even with a TBPTT window
    of 1, using this factor gives more accurate gradients than a larger window, aligning
    with our theoretical findings.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上下文窗口为 128，压缩比为 8，并计算参数 $\Theta$ 的梯度，以获得准确的梯度估计。即使 TBPTT 窗口为 1，使用该因子也能提供比更大窗口更准确的梯度，这与我们的理论发现一致。
- en: 5 Limitation
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 限制
- en: Our proposed UIO-LLMs excels in long-context language modeling and achieves
    lossless compression at a ratio of 8\. However, its performance on downstream
    tasks like NarrativeQA doesn’t improve despite compression ratio reduction, suggesting
    a memory utilization issue. Future work could involve fine-tuning the decoder
    to better utilize memories. Additionally, UIO-LLMs has a limit on context length
    due to memory occupation. Moreover, unbiased incremental TBPTT requires memory
    independence for accurate gradient estimation. If this condition is violated,
    the algorithm can still run but may result in significant gradient estimation
    errors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 UIO-LLMs 在长上下文语言建模中表现出色，并在 8 的比率下实现了无损压缩。然而，尽管压缩比降低，其在下游任务如 NarrativeQA
    上的表现并未改善，表明存在内存利用问题。未来的工作可能涉及对解码器进行微调，以更好地利用内存。此外，由于内存占用，UIO-LLMs 对上下文长度有一定限制。此外，无偏增量
    TBPTT 需要内存独立性以进行准确的梯度估计。如果违反了这一条件，算法仍然可以运行，但可能会导致显著的梯度估计误差。
- en: 6 Conclusion
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: To enhance the ability of LLMs on processing long texts, We proposed UIO-LLMs,
    which employs a streamlined encoder-decoder framework where the weights-shared
    encoder and decoder respectively encapsulate a context segment into memories and
    leverage these memories to predict outputs of the subsequent segment. In order
    to accelerate the training process, we proposed incremental TBPTT, which is an
    efficient optimizing technique dedicated to memory-enhanced transformers and reduces
    the complexity of traditional TBPTT. Based on incremental TBPTT, we further proposed
    its unbiased version, an innovative optimizing technique that ensures unbiased
    gradient approximation of BPTT. Equipped with unbiased incremental TBPTT, UIO-LLMs
    can be trained on texts of 100K tokens, resulting in a strong performance on language
    modeling tasks and comparable performance on downstream tasks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强大规模语言模型（LLMs）处理长文本的能力，我们提出了UIO-LLMs，它采用了一个精简的编码器-解码器框架，其中权重共享的编码器和解码器分别将上下文片段封装为记忆，并利用这些记忆预测后续片段的输出。为了加速训练过程，我们提出了增量TBPTT，这是一种专门用于内存增强型变压器的高效优化技术，降低了传统TBPTT的复杂性。基于增量TBPTT，我们进一步提出了其无偏版本，这是一种创新的优化技术，确保了BPTT的无偏梯度近似。配备了无偏增量TBPTT，UIO-LLMs可以在包含100K标记的文本上进行训练，在语言建模任务中表现强劲，并在下游任务中表现出色。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Anthropic. Introducing 100k context windows, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Anthropic。介绍100k上下文窗口，2023。'
- en: '[2] OpenAI. Function calling and other api updates (longer context)., 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] OpenAI。功能调用及其他API更新（更长上下文）。，2023。'
- en: '[3] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long
    contexts. TACL, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 刘乃生、林凯文、约翰·赫维特、阿什文·帕兰贾佩、米歇尔·贝维拉夸、法比奥·佩特罗尼和佩尔西·梁。迷失在中间：语言模型如何使用长上下文。TACL，2023。'
- en: '[4] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. In NeurIPS, 2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 帕特里克·刘易斯、伊桑·佩雷斯、亚历山德拉·皮克图斯、法比奥·佩特罗尼、弗拉基米尔·卡普欣、纳曼·戈亚尔、海因里希·库特勒、迈克·刘易斯、易文涛、蒂姆·洛克塔谢尔、塞巴斯蒂安·里德尔和道威·基拉。用于知识密集型NLP任务的检索增强生成。在NeurIPS，2020。'
- en: '[5] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,
    Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi
    Li. Longbench: A bilingual, multitask benchmark for long context understanding.
    arXiv, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 白玉石、吕鑫、张家杰、吕洪畅、唐建凯、黄志电、杜正晓、刘晓、曾奥涵、侯磊、董宇霄、唐杰和李娟子。Longbench：一个用于长上下文理解的双语多任务基准。arXiv，2023。'
- en: '[6] Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E.
    Gonzalez, Ion Stoica, Xuezhe Ma, , and Hao Zhang. How long can open-source llms
    truly promise on context length?, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 李大成*、邵如林*、谢安泽、盛颖、郑莲敏、约瑟夫·E·冈萨雷斯、伊昂·斯托伊卡、马雪哲和张浩。开源LLMs在上下文长度方面的真实承诺能持续多久？，2023。'
- en: '[7] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language
    models. arXiv, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 陈宇康、钱盛菊、唐浩天、赖欣、刘志坚、韩松和贾佳雅。Longlora：高效微调长上下文大规模语言模型。arXiv，2023。'
- en: '[8] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 陈守远、汪舍曼、陈亮健和田远东。通过位置插值扩展大规模语言模型的上下文窗口，2023。'
- en: '[9] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian
    Li. Pose: Efficient context window extension of llms via positional skip-wise
    training, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 朱大伟、杨楠、王亮、宋一凡、吴文浩、魏富如和李书健。Pose：通过位置跳跃训练高效扩展LLMs的上下文窗口，2023。'
- en: '[10] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN:
    Efficient context window extension of large language models. In ICLR, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 彭博文、杰弗瑞·凯斯内尔、范洪陆、恩里科·希波尔。YaRN：大规模语言模型的高效上下文窗口扩展。发表于ICLR，2024。'
- en: '[11] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 小光轩、田远东、陈贝迪、韩松、迈克·刘易斯。高效的流媒体语言模型与注意力下沉。arXiv, 2023。'
- en: '[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    In NeurIPS, 2017.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌斯科雷特、利昂·琼斯、艾登·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏金。注意力即你所需。在NeurIPS，2017。'
- en: '[13] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
    Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
    Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention
    with performers. In ICLR, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
    Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
    Kaiser, David Belanger, Lucy Colwell 和 Adrian Weller。用表演者重新思考注意力。发表于 ICLR，2021年。'
- en: '[14] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are
    rnns: Fast autoregressive transformers with linear attention. In ICML, 2020.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Katharopoulos, A. Vyas, N. Pappas 和 F. Fleuret。变换器是 RNN：具有线性注意力的快速自回归变换器。发表于
    ICML，2020年。'
- en: '[15] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel
    machines. In NeurIPS, 2007.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Ali Rahimi 和 Benjamin Recht。大规模核机器的随机特征。发表于 NeurIPS，2007年。'
- en: '[16] Krzysztof Choromanski, Mark Rowland, and Adrian Weller. The unreasonable
    effectiveness of structured random orthogonal embeddings. In NeurIPS, 2017.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Krzysztof Choromanski, Mark Rowland 和 Adrian Weller。结构化随机正交嵌入的非理性有效性。发表于
    NeurIPS，2017年。'
- en: '[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
    models. In ICLR, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang 和 Weizhu Chen。LoRA：大语言模型的低秩适应。发表于 ICLR，2022年。'
- en: '[18] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom。Llama 2：开放基础和微调的聊天模型，2023年。'
- en: '[19] Ronald J. Williams and Jing Peng. An Efficient Gradient-Based Algorithm
    for On-Line Training of Recurrent Network Trajectories. Neural Computation, 1990.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ronald J. Williams 和 Jing Peng。针对递归网络轨迹的高效梯度基础算法。《神经计算》，1990年。'
- en: '[20] Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer.
    In NeurIPS, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Aydar Bulatov, Yuri Kuratov 和 Mikhail Burtsev。递归记忆变换器。发表于 NeurIPS，2022年。'
- en: '[21] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting
    language models to compress contexts. In EMNLP, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Alexis Chevalier, Alexander Wettig, Anirudh Ajith 和 Danqi Chen。将语言模型适应于压缩上下文。发表于
    EMNLP，2023年。'
- en: '[22] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts
    with gist tokens. In NeurIPS, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jesse Mu, Xiang Lisa Li 和 Noah Goodman。学习使用本质令牌压缩提示。发表于 NeurIPS，2023年。'
- en: '[23] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng
    Dou. Soaring from 4k to 400k: Extending llm’s context with activation beacon,
    2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye 和 Zhicheng
    Dou。从 4k 到 400k 的飞跃：通过激活信标扩展 LLM 的上下文，2024年。'
- en: '[24] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin,
    and Mikhail Burtsev. In search of needles in a 10m haystack: Recurrent memory
    finds what llms miss, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin
    和 Mikhail Burtsev。在 10m 干草堆中寻找针：递归记忆发现了 LLMs 遗漏的东西，2024年。'
- en: '[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever, et al. Language models are unsupervised multitask learners. 2019.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
    Sutskever 等。语言模型是无监督的多任务学习者。2019年。'
- en: '[26] OpenAI. Gpt-4 technical report, 2024.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] OpenAI. Gpt-4技术报告，2024年。'
- en: '[27] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression
    and contrastive conditioning for controllability and toxicity reduction in language
    models. In EMNLP, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] David Wingate，Mohammad Shoeybi 和 Taylor Sorensen. 用于语言模型的提示压缩和对比条件化，以实现可控性和毒性减少。发表于EMNLP，2022年。'
- en: '[28] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context
    autoencoder for context compression in a large language model. In ICLR, 2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Tao Ge，Hu Jing，Lei Wang，Xun Wang，Si-Qing Chen 和 Furu Wei. 用于大语言模型中上下文压缩的上下文编码器。发表于ICLR，2024年。'
- en: '[29] Michael C. Mozer. A focused backpropagation algorithm for temporal pattern
    recognition. Complex Systems, 1989.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Michael C. Mozer. 一种用于时间模式识别的聚焦反向传播算法。复杂系统，1989年。'
- en: '[30] Yann Ollivier, Corentin Tallec, and Guillaume Charpiat. Training recurrent
    networks online without backtracking, 2015.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Yann Ollivier，Corentin Tallec 和 Guillaume Charpiat. 在线训练递归网络而不回溯，2015年。'
- en: '[31] Corentin Tallec and Yann Ollivier. Unbiased online recurrent optimization.
    In ICLR, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Corentin Tallec 和 Yann Ollivier. 无偏在线递归优化。发表于ICLR，2018年。'
- en: '[32] Corentin Tallec and Yann Ollivier. Unbiasing truncated backpropagation
    through time, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Corentin Tallec 和 Yann Ollivier. 通过时间修剪反向传播去偏差，2018年。'
- en: '[33] Michael Mozer. A focused backpropagation algorithm for temporal pattern
    recognition. Complex Systems, 1995.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Michael Mozer. 一种用于时间模式识别的聚焦反向传播算法。复杂系统，1995年。'
- en: '[34] Jeffrey S. Vitter. Random sampling with a reservoir. TOMS, 1985.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Jeffrey S. Vitter. 使用储备池的随机采样。TOMS，1985年。'
- en: '[35] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P
    Lillicrap. Compressive transformers for long-range sequence modelling. arXiv,
    2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Jack W Rae，Anna Potapenko，Siddhant M Jayakumar，Chloe Hillier 和 Timothy
    P Lillicrap. 用于长距离序列建模的压缩变换器。arXiv，2019年。'
- en: '[36] Zhangir Azerbayev, Edward Ayers, and B.P. Proof-pile, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Zhangir Azerbayev，Edward Ayers，和 B.P. Proof-pile，2022年。'
- en: '[37] Together Computer. Redpajama: An open source recipe to reproduce llama
    training dataset, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Together Computer. Redpajama：一个开源配方，用于重现 llama 训练数据集，2023年。'
- en: '[38] Yukang Chen, Shaozuo Yu, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Long alpaca: Long-context instruction-following
    models, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Yukang Chen，Shaozuo Yu，Shengju Qian，Haotian Tang，Xin Lai，Zhijian Liu，Song
    Han 和 Jiaya Jia. 长情节：长上下文指令跟随模型，2023年。'
- en: '[39] Yuyi Jiong. Longdata-corpus, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yuyi Jiong. Longdata-corpus，2023年。'
- en: '[40] Jean Kaddour. The minipile challenge for data-efficient language models.
    arXiv, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Jean Kaddour. 数据高效语言模型的minipile挑战。arXiv，2023年。'
- en: Appendix A PyTorch Implementation of Incremental TBPTT
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A PyTorch 实现的增量 TBPTT
- en: 'The incremental TBPTT algorithm performs backpropagation for only two nodes
    at each time step, leading to a computational cost that is independent of the
    TBPTT window size $S$. This streamlined approach significantly reduces the computational
    burden compared to traditional backpropagation methods. Below is an illustrative
    example of how our incremental TBPTT algorithm can be implemented in PyTorch,
    showcasing its efficiency and simplicity:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 增量 TBPTT 算法在每个时间步只进行两个节点的反向传播，从而使计算成本与 TBPTT 窗口大小 $S$ 无关。这种简化的方法显著减少了与传统反向传播方法相比的计算负担。以下是我们的增量
    TBPTT 算法在 PyTorch 中实现的一个示例，展示了其效率和简洁性：
- en: '1def  incremental_tbptt(model:  torch.Module,  x:  List[torch.Tensor],  S:  int):23  mem  =  []4  mem_detach  =  []56  for  xid,  xi  in  enumerate(x):7  #  forward  propagation8  loss_i,  mem_i  =  model.forward(xi,  mem_detach)910  #  first  backward  propagation11  loss_i.backward(retrain_graph=True)1213  mem_i_detach  =  mem_i.detach()14  mem_i_detach.requries_grad_(True)1516  mem.append(mem_i)17  mem_detach.append(mem_i_detach)1819  elim_id  =  xid  -  S20  if  elim_id  >=  0:21  #  second  backward  propagation22  gradient  =  mem_detach[elim_id].grad23  mem[elim_id].backward(gradient=gradient.data)2425  for  elim_id  in  range(max(0,  len(x)  -  S),  len(x)  -  1):26  gradient  =  mem_detach[elim_id].grad27  if  gradient  is  not  None:28  mem[elim_id.backward(gradient=gradient.data)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '1def  incremental_tbptt(model:  torch.Module,  x:  List[torch.Tensor],  S:  int):23  mem  =  []4  mem_detach  =  []56  for  xid,  xi  in  enumerate(x):7  #  前向传播8  loss_i,  mem_i  =  model.forward(xi,  mem_detach)910  #  首次反向传播11  loss_i.backward(retrain_graph=True)1213  mem_i_detach  =  mem_i.detach()14  mem_i_detach.requries_grad_(True)1516  mem.append(mem_i)17  mem_detach.append(mem_i_detach)1819  elim_id  =  xid  -  S20  if  elim_id  >=  0:21  #  第二次反向传播22  gradient  =  mem_detach[elim_id].grad23  mem[elim_id].backward(gradient=gradient.data)2425  for  elim_id  in  range(max(0,  len(x)  -  S),  len(x)  -  1):26  gradient  =  mem_detach[elim_id].grad27  if  gradient  is  not  None:28  mem[elim_id].backward(gradient=gradient.data)'
- en: In the provided implementation, at each time step, the generated memory is detached
    and flagged for gradient computation. This crucial step effectively disconnects
    the computational graphs associated with distinct time steps, preventing unwanted
    dependencies. Once computations for all time steps have been completed, it becomes
    imperative to initiate a dedicated backpropagation pass for any remaining time
    steps within the TBPTT window. This additional step diverges from the standard
    TBPTT methodology and is essential for maintaining the accuracy of the training
    process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的实现中，在每个时间步生成的记忆会被分离并标记用于梯度计算。这个关键步骤有效地断开了与不同时间步相关联的计算图，防止了不必要的依赖。一旦所有时间步的计算完成，就必须为
    TBPTT 窗口内的剩余时间步启动一个专用的反向传播过程。这一步骤与标准 TBPTT 方法有所不同，对于保持训练过程的准确性至关重要。
- en: Appendix B PyTorch Implementation of Unbiased Incremental TBPTT
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B PyTorch 无偏增量 TBPTT 实现
- en: 'Our unbiased incremental TBPTT distinguishes itself from the incremental TBPTT
    by integrating the Reservoir Sampling technique. This integration ensures that
    all historical time steps are preserved with equal probability, rather than solely
    focusing on the most recent ones. Furthermore, the introduction of a compensatory
    factor within our method allows for an unbiased estimation of gradients, enhancing
    the accuracy of the model’s learning process. Additionally, the PyTorch implementation
    of unbiased incremental TBPTT closely resembles that of its incremental counterpart,
    with the key difference lying in the incorporation of the unbiased estimation
    mechanism. The following is an example of how our unbiased incremental TBPTT can
    be implemented in PyTorch:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的无偏增量 TBPTT 通过集成 Reservoir Sampling 技术与增量 TBPTT 区分开来。这种集成确保所有历史时间步的保留概率是相等的，而不仅仅关注最近的时间步。此外，我们的方法引入了一个补偿因子，允许对梯度进行无偏估计，从而提高模型学习过程的准确性。此外，无偏增量
    TBPTT 的 PyTorch 实现与其增量版本非常相似，关键的区别在于纳入了无偏估计机制。以下是如何在 PyTorch 中实现我们的无偏增量 TBPTT
    的示例：
- en: '1def  remove(reservoir,  chunk_id):2  if  chunk_id  in  reservoir:3  reservoir.remove(chunk_id)4  return  reservoir567def  destroy_graph(mem:  List,  mem_detach:  List,  elim_id):8  #  second  backward  progagation9  mem[elim_id].backward(gradient=mem_detach[elim_id].grad.data)10  m.grad  =  m.grad  *  factor  +  gd1112  mem_i_detach  =  mem_i.detach()13  mem_i_detach.requires_grad_(True)1415  mem.append(mem_i)16  mem_detach.append(mem_i_detach)171819def  unbiased_incremental_tbptt(model:  torch.Module,  x:  List[torch.Tensor],  S:int):20  mem  =  []21  mem_detach  =  []22  reservoir  =  []2324  for  xid,  xi  in  enumerate(x):25  #  forward  propagation26  loss_i,  mem_i  =  model.forward(xi,  mem_detach)2728  grads  =  []29  for  m  in  mem_detach:30  grads.append(m.grad)31  m.grad  =  None3233  #  first  backward  progagation34  loss_i.backward(retain_graph=True)3536  for  gd,  m  in  zip(grads,  mem_detach):37  factor  =  max(xid  /  S,  1)38  m.grad  =  m.grad  *  factor  +  gd3940  mem_i_detach  =  mem_i.detach()41  mem_i_detach.requires_grad_(True)4243  mem.append(mem_i)44  mem_detach.append(mem_i_detach)4546  if  xid  <  S:47  reservoir.append(xid)48  else:49  j  =  random.randint(0,  chunk_id)50  if  j  <  S:51  elim_id  =  reservoir[j]52  reservoir[j]  =  xid53  destroy_graph(mem,  mem_detach,  elim_id)54  else:55  destroy_graph(mem,  mem_detach,  xid)5657  for  elim_id  in  reversed(remove(reservoir,  chunk_id)):58  destroy_graph(mem,  mem_detach,  elim_id)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '1def remove(reservoir, chunk_id):2 if chunk_id in reservoir:3 reservoir.remove(chunk_id)4
    return reservoir567def destroy_graph(mem: List, mem_detach: List, elim_id):8 #
    第二次反向传播9 mem[elim_id].backward(gradient=mem_detach[elim_id].grad.data)10 m.grad
    = m.grad * factor + gd1112 mem_i_detach = mem_i.detach()13 mem_i_detach.requires_grad_(True)1415
    mem.append(mem_i)16 mem_detach.append(mem_i_detach)171819def unbiased_incremental_tbptt(model:
    torch.Module, x: List[torch.Tensor], S:int):20 mem = []21 mem_detach = []22 reservoir
    = []2324 for xid, xi in enumerate(x):25 # 前向传播26 loss_i, mem_i = model.forward(xi,
    mem_detach)2728 grads = []29 for m in mem_detach:30 grads.append(m.grad)31 m.grad
    = None3233 # 第一次反向传播34 loss_i.backward(retain_graph=True)3536 for gd, m in zip(grads,
    mem_detach):37 factor = max(xid / S, 1)38 m.grad = m.grad * factor + gd3940 mem_i_detach
    = mem_i.detach()41 mem_i_detach.requires_grad_(True)4243 mem.append(mem_i)44 mem_detach.append(mem_i_detach)4546
    if xid < S:47 reservoir.append(xid)48 else:49 j = random.randint(0, chunk_id)50
    if j < S:51 elim_id = reservoir[j]52 reservoir[j] = xid53 destroy_graph(mem, mem_detach,
    elim_id)54 else:55 destroy_graph(mem, mem_detach, xid)5657 for elim_id in reversed(remove(reservoir,
    chunk_id)):58 destroy_graph(mem, mem_detach, elim_id)'
- en: This code snippet illustrates the practical application of our unbiased incremental
    TBPTT algorithm within the PyTorch environment, emphasizing its enhanced gradient
    estimation capabilities and its similarity to the incremental TBPTT in terms of
    implementation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段展示了我们在 PyTorch 环境中应用无偏增量 TBPTT 算法的实际效果，强调了其改进的梯度估计能力以及其实现与增量 TBPTT 的相似性。
- en: 'Appendix C The Detailed Derivation of Eq. ([11](#S3.E11 "In 3.3.3 Unbiased
    Incremental TBPTT ‣ 3.3 Unbiased Incremental Optimization ‣ 3 Methodology ‣ UIO-LLMs:
    Unbiased Incremental Optimization for Long-Context LLMs"))'
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 C 方程的详细推导 ([11](#S3.E11 "在 3.3.3 无偏增量 TBPTT ‣ 3.3 无偏增量优化 ‣ 3 方法论 ‣ UIO-LLMs:
    用于长上下文 LLMs 的无偏增量优化"))'
- en: 'Reservoir sampling [[34](#bib.bib34)] is a technique that ensures uniformly
    sampling from a sequence of unknown length. In our case, we can analogously apply
    this concept to the TBPTT window. At time step $t$ with high probability. This
    can be interpreted as the correlation between the random variables $\mathbf{Z}_{t1,s}^{(X_{i})}$
    are independent, satisfying the following equation:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 水库抽样 [[34](#bib.bib34)] 是一种确保从未知长度序列中均匀抽样的技术。在我们的案例中，我们可以类比地将这一概念应用于 TBPTT 窗口。在时间步
    $t$ 上具有高概率。这可以解释为随机变量 $\mathbf{Z}_{t1,s}^{(X_{i})}$ 之间的相关性是独立的，满足以下方程：
- en: '|  | $1$2 |  | (13) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (13) |'
- en: Appendix D More Ablations
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 更多消融实验
- en: Analysis on Different TBPTT Window Sizes Increasing $S$ leads to better performance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 TBPTT 窗口大小分析 增加 $S$ 会提高性能。
- en: '![Refer to caption](img/71f096b292ac5412471f5f7e408e6a87.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/71f096b292ac5412471f5f7e408e6a87.png)'
- en: 'Figure 10: Learning curve with various $S$ for better visualization.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 各种 $S$ 下的学习曲线以便于可视化。'
- en: 'Figure 11: Language modeling performance on various TBPTT window $S$.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 各种 TBPTT 窗口 $S$ 下的语言建模性能。'
- en: Perplexity$\downarrow$ PG19 9.84 9.75 9.67 9.63 Proof-Pile 9.90 9.74 9.59 9.44
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度$\downarrow$ PG19 9.84 9.75 9.67 9.63 Proof-Pile 9.90 9.74 9.59 9.44
- en: 'Analysis on Components In order to examine the impact of transfer head and
    unbiased incremental TBPTT on the model’s performance, we use a fixed compression
    ratio of $32$ while keeping other settings unchanged for the ablation study. As
    shown in Table [4](#A4.T4 "Table 4 ‣ Appendix D More Ablations ‣ UIO-LLMs: Unbiased
    Incremental Optimization for Long-Context LLMs"), both transfer head and unbiased
    incremental TBPTT increase the performance.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '组件分析 为了考察转移头和无偏增量 TBPTT 对模型性能的影响，我们使用固定的压缩比 $32$，同时保持其他设置不变进行消融研究。如表 [4](#A4.T4
    "表 4 ‣ 附录 D 更多消融实验 ‣ UIO-LLMs: 用于长上下文 LLMs 的无偏增量优化") 所示，转移头和无偏增量 TBPTT 都提高了性能。'
- en: 'Table 4: Ablation results on transfer head and unbiased incremental TBPTT (UIO-TBPTT).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 转移头和无偏增量 TBPTT (UIO-TBPTT) 的消融结果。'
- en: '| Components | Benchmarks |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 基准 |'
- en: '| --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Transfer Head | UIO-TBPTT | TBPTT | PG19-100K$\downarrow$ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 转移头 | UIO-TBPTT | TBPTT | PG19-100K$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ✓ | ✓ |  | 8.28 | 20.10 | 18.90 | 21.83 | 23.03 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  | 8.28 | 20.10 | 18.90 | 21.83 | 23.03 |'
- en: '| ✓ |  | ✓ | 8.31 | 17.01 | 17.69 | 19.23 | 20.35 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  | ✓ | 8.31 | 17.01 | 17.69 | 19.23 | 20.35 |'
- en: '|  | ✓ |  | 9.29 | 18.70 | 18.65 | 21.47 | 22.71 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |  | 9.29 | 18.70 | 18.65 | 21.47 | 22.71 |'
- en: Appendix E Inference Advantages
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 推理优势
- en: 'In the prefill stage, the model encodes input tokens to generate context representations,
    also known as the KV cache or memories in UIO-LLMs. For Llama2-7b-chat [[18](#bib.bib18)]
    and RoPE extrapolation based methods like LongChat-7B-v1.5-32K [[38](#bib.bib38)],
    a significant challenge in this stage is the computational complexity, which scales
    quadratically with the input token length. This requires a substantial number
    of FLOPs and GPU memory, making prefilling a computationally expensive process.
    However, as shown in Figure [12](#A5.F12 "Figure 12 ‣ Appendix E Inference Advantages
    ‣ UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs"), our approach
    achieves a nearly linear growth in prefilling time and GPU memory overhead as
    the sequence length increases, suggesting a considerable superiority.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '在预填充阶段，模型对输入标记进行编码以生成上下文表示，也称为 KV 缓存或 UIO-LLMs 中的记忆。对于 Llama2-7b-chat [[18](#bib.bib18)]
    和基于 RoPE 外推的方法，如 LongChat-7B-v1.5-32K [[38](#bib.bib38)]，这一阶段的一个显著挑战是计算复杂性，它随着输入标记长度的平方增加。这需要大量的
    FLOPs 和 GPU 内存，使得预填充过程计算代价高。然而，如图 [12](#A5.F12 "图 12 ‣ 附录 E 推理优势 ‣ UIO-LLMs: 用于长上下文
    LLMs 的无偏增量优化") 所示，我们的方法在序列长度增加时实现了几乎线性的预填充时间和 GPU 内存开销增长，表明具有显著优势。'
- en: '![Refer to caption](img/0ed00c95dff784d7e5f4b62d8d85a5a0.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0ed00c95dff784d7e5f4b62d8d85a5a0.png)'
- en: 'Figure 12: The prefilling time and GPU memory comparison of different models
    are presented below. Since the experimental results for LongChat-7B-v1.5-32K and
    LongAlpaca-7B are nearly identical to those of Llama2-7b-chat, we only show the
    results of Llama2-7b-chat for clarity.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：不同模型的预填充时间和GPU内存比较见下文。由于LongChat-7B-v1.5-32K和LongAlpaca-7B的实验结果与Llama2-7b-chat几乎相同，为了清晰起见，我们仅展示Llama2-7b-chat的结果。
